(78.2%), and 287,722 gray (3.76%). Further statistics for
the distribution of vendor counts and tags in our datasets are
presented in Appendix A.1.
The ratios of malicious to benign data in our training, test,
and validation sets are comparable, with malicious samples
more prevalent than benign samples. Note that this class bal-
ance differs substantially from a real-world deployment sce-
nario, where malware is rarely seen. Increasing the preva-
lence of this low-occurrence class when training on unbal-
anced data sets is commonly done to avoid overﬁtting [3]
(we have also observed this in practice), and using a data set
with a higher proportion of malicious samples assuming a
sufﬁcient number of benign samples – may lead to a more
precise decision boundary, and better overall performance as
measured by the full ROC curve. Further, when using our
malicious tags loss, a greater diversity in malware can yield
a more diverse tag set to learn from during training.
Note that ROC curves, which we use as performance mea-
sures in Sections 4 and 5, are independent of class ratio in the
test set (unlike accuracy), since false positive rate (FPR) val-
ues depend only on the benign data, and true positive rate
(TPR) values depend only on malware. We also focus on
improvements in detection at the very low FPR of 0.1% or
below, where we see the most dramatic improvements, since
several publications by anti-virus vendors [25, 30] and our
experience suggest that 0.1% or lower is indeed a practi-
cal FPR target for most deployment scenarios. Our model
outputs can also be easily (without retraining) rescaled to
the desired deployment class ratio, based on the provided
ROC curve and/or standard calibration methods, e.g., ﬁt-
ting a weighted isotonic regressor on scores from the vali-
dation set with each score contribution weighted according
to its ground truth label to correct the class balance discrep-
ancy between the validation set and the expected deployment
setting, then using that regressor to calibrate scores during
test/deployment.
4 Experimental Evaluation
In this section, we apply the auxiliary losses presented in in
Section 3, ﬁrst individually, each loss in conjunction with
a main malicious/benign loss, and then simultaneously in
one combined model. We then compare to a baseline model,
ﬁnding that each loss term yields an improvement, either in
Receiver Operating Characteristic (ROC) net area under the
curve (AUC) or in terms of detection performance at low
false positive rates (FPR). We note that none of the auxil-
iary losses we present below harmed classiﬁcation relative
to the baseline model; at worst, our loss-augmented models
had equivalent performance to the baseline model with re-
spect to AUC and low-FPR ROC performance on the aggre-
gate malicious/benign label. Each model used a loss weight
of 1.0 on the aggregate malicious/benign loss and 0.1 on each
auxiliary loss, i.e. when we add K targets to the main loss,
the ﬁnal loss that gets backpropagated through the model be-
comes
L(X,Y ) = Lmal(X,Y ) + 0.1
K
∑
k=1
Lk(X,Y ).
(8)
Results are depicted in graphical form in Figure 2 and in
tabular form in Table 1.
As the training process for deep neural networks has some
degree of intrinsic randomness, which can result in varia-
tions in their performance, we report our results in terms of
both the mean and standard deviation for the test statistics of
interest across ﬁve runs. Each model was trained ﬁve times,
each time with a different random initialization and differ-
ent randomization over minibatches, and all other parameters
(optimizer and learning rate, training data, model structure,
number of epochs, etc.) held identical. We compute the test
statistic of interest (e.g. the detection rate at a false positive
rate of 10−3) for each model, and then compute the aver-
age and standard deviation of those results. Notice that the
ROC curves in Figure 2 are plotted on a logarithmic scale
for visibility, since the baseline performance is already quite
high and signiﬁcant marginal improvements are difﬁcult to
discern. For this reason, we also include relative reductions
in mean true positive detection error (the rate at which the
model fails to detect malware samples – or false negative
rate – averaged over the ﬁve model results) and in standard
deviation from the baseline for our best model in Table 1,
and for all models in Table C.1 in the appendix.
4.1 Vendor Count Loss
We employed the same base PE model topology as for our
other experiments, with a primary malicious/benign binary
cross-entropy loss, and an auxiliary count loss. We experi-
mented with two different loss functions for the count loss
– a Poisson loss and a Restricted Generalized Poisson loss
USENIX Association
28th USENIX Security Symposium    309
(a) Count Loss
(b) Vendor Loss
(c) Tag Loss
(d) Combined Loss
Figure 2: ROC curves and AUC statistics for count, vendor, and tag experiments compared to our baseline. Lines represent the
mean TPR at a given FPR, while shaded regions represent ±1 standard deviation. Statistics were computed over 5 training runs,
each with random parameter initialization. (a) Count loss. Our baseline model (blue solid line) is shown compared to a model
employing a Poisson auxiliary loss (red dashed line), and a dispersed Poisson auxiliary loss (green dotted line). (b) Auxiliary
loss on multiple vendors malicious/benign labels (red dashed line) and baseline (blue solid line). (c) Auxiliary loss on semantic
attribute tags (red dashed line) and baseline (blue solid line). (d) Our combined model (red dashed line) and baseline (blue solid
line). The combined model utilizes an aggregate malicious/benign loss with an auxiliary Poisson count loss, a multi-vendor
malicious/benign loss, and a malware attribute tag loss.
(equations 3 and 5 respectively). For the Poisson loss, we
used an exponential activation over a dense layer atop the
base to estimate µ (i). For the Restricted Generalized Pois-
son (RG-Poisson) loss, we followed a similar pattern us-
ing two separate dense layers with exponential activations
on top; one for the µ (i) parameter and another for the α (i)
parameter. The choice of an exponential activation is consis-
tent with statistics literature on Generalized Linear Models
(GLMs) [18].
Results on the malware detection task using Poisson and
RG-Poisson losses as an auxiliary loss function are shown
in Figure 2a. When compared to a baseline using no auxil-
iary loss, we see a statistically signiﬁcant improvement with
the Poisson loss function in both AUC and ROC curve, par-
ticularly in low false positive rate (FPR) regions. The RG-
Poisson loss, by contrast, yields no statistically signiﬁcant
gains over the baseline in terms of AUC, nor does it appear
to yield statistically signiﬁcant gains at any point along the
ROC curve.
This suggests that the RG-Poisson loss model is ill-ﬁt,
which could stem from a variety of issues. First, if counts
are under-dispersed, an over-dispersed Poisson loss could be
an inappropriate model. Under-dispersion could occur if cer-
tain vendors disproportionately trigger simultaneously or be-
310    28th USENIX Security Symposium
USENIX Association
TPR Baseline
TPR Poisson
TPR RG Poisson
TPR Vendors
TPR Tags
TPR All Targets
% Error Reduction (All Targets)
% Variance Reduction (All Targets)
10−5
0.427 ± 0.076
0.645 ± 0.029
0.427 ± 0.116
0.697 ± 0.034
0.677 ± 0.027
0.735 ± 0.014
53.8%
81.6%
10−4
0.692 ± 0.049
0.785 ± 0.034
0.711 ± 0.041
0.792 ± 0.024
0.792 ± 0.009
0.806 ± 0.017
37.0%
65.3%
FPR
10−3
0.864 ± 0.031
0.903 ± 0.016
0.870 ± 0.016
0.889 ± 0.020
0.875 ± 0.022
0.922 ± 0.004
42.7%
87.1%
10−2
0.965 ± 0.007
0.970 ± 0.001
0.966 ± 0.003
0.970 ± 0.004
0.971 ± 0.004
0.972 ± 0.003
20.0%
57.1%
10−1
0.9928 ± 0.0007
0.9932 ± 0.0002
0.9930 ± 0.0003
0.9928 ± 0.0014
0.9932 ± 0.0008
0.9934 ± 0.0004
8.3%
94.3%
Table 1: Top: Mean and standard deviation true positive rates (TPRs) for the different experiments in Section 4 at false positive
rates (FPRs) of interest. Results were aggregated over ﬁve training runs with different weight initializations and minibatch
orderings. Best results consistently occurred when using all auxiliary losses and are shown in bold. Bottom: Percentage
reduction in missed true positive detections and percentage reductions in ROC curve standard deviation resulting from the best
model (All Targets) compared to the baseline across various FPRs. State-of-the-art results are shown in bold.
cause counts are inherently bounded by the net number of
vendors. Second, a Poisson model, even with added disper-
sion parameters, is an ill-posed model of count data, but re-
moving the dispersion parameter removes a dimension in the
parameter space to over-ﬁt on. Inspecting the dispersion pa-
rameters predicted by the RG-Poisson model, we noted that
they were relatively large, which supports the latter hypoth-
esis. We also noticed that the RG-Poisson model converged
signiﬁcantly faster than the Poisson model in terms of mal-
ware detection loss.
4.2 Modeling Individual Vendor Responses
Incorporating an auxiliary multi-label binary cross-entropy
loss across vendors (cf. Section 3.3) in conjunction with the
main malicious/benign loss yields a similar increase in the
TPR at low FPR regions of the ROC curve (see Figure 2b)
to the Poisson experiment. Though we do not see a signif-
icant increase in AUC, since the improvement is integrated
across an extremely narrow range of FPRs, this improvement
in TPR at lower FPRs may still be operationally signiﬁcant,
and does indicate an improvement in the model.
Incorporating Tags as Targets
4.3
In this experiment we extend the architecture of our base
network to predict, not only the malware/benign label, but
also the set of 11 tags deﬁned in Section 3.4. For this, we
add two fully connected layers per tag to the end of the
base architecture (see Section 3.4) which serve to identify
each tag from the shared representation. Each of these tag-
specialized layers predicts a binary output corresponding to
presence/absence of the tag and has an associated binary
cross-entropy loss that gets added to the other tag losses and
the main malicious/benign loss. The overall loss for this ex-
periment is a sum containing one term per tag, weighted by
a loss weight of 0.1 (as mentioned at the beginning of this
section), and one term for the loss incurred on the main task.
The result of this experiment is represented via the ROC
curves of Figure 2c. Similar to section 4.2 we see no statisti-
cal difference in the AUC values with respect to the baseline,
but we do observe substantial statistical improvement in the
predictions of the model in low FPR regions, particularly for
FPR values lower than 10−3. Furthermore, we also witness
a substantial decrease in the variance of the ROC curve.
4.4 Combined Model
Finally, we extend our model to predict all auxiliary targets
in conjunction with the aggregate label, with a net loss term
containing a combination of all auxiliary losses used in pre-
vious experiments. The ﬁnal loss function for the experi-
ment is the sum of all the individual losses where the mal-
ware/benign loss has a weight of 1.0 while the rest of the
losses have a weight of 0.1.
The resulting ROC curve and AUC are shown in Figure
2d. The AUC of 0.9972 ± 0.0001 is the highest obtained
across all the experiments conducted in this study. More-
over, in contrast to utilizing any single auxiliary loss, we see
a noticeable improvement in the ROC curve not only in very
low FPR regions, but also at 10−3 FPR. Additionally, vari-
ance is consistently lower across a range of low-FPR values
for this combined model than for our baseline or any previ-
ous models. An exception is near 10−6 FPR where measur-
ing variance is an ill-posed problem because even with a test
dataset of over 7M samples, detecting or misdetecting even
one or two of them can signiﬁcantly affect detection rate.
In order to account for the effect of gray samples in the
evaluation of our detection model, we re-scanned a subset of
those at a later point in time, giving the AV community time
to update their detection rules, and evaluated the prediction
issued by the model. Even though it is naturally harder to
USENIX Association
28th USENIX Security Symposium    311
determine maliciousness of these samples (otherwise they
would not initially have been categorized as gray), we ﬁnd
that our model predicts the correct labels for more than 77%
of them. A more in depth analysis of grey samples is de-
ferred to Appendix B.
5 Discussion
In this section, we examine the effects of different types of
auxiliary loss functions on main task ROC curve. We then
perform a sanity check to establish whether our performance
increases result from additional information added to our
neural networks by the auxiliary loss functions or are the ar-
tifact of some regularization effect.
5.1 Modes of Improvement
Examining the plots in Figure 2, we see three different types
of improvement that result from our auxiliary losses:
1. A bump in TPR at low FPR (< 10−3).
2. A net increase in AUC and a small bump in performance
at higher FPRs (≥ 10−3).
3. A reduction in variance.
Improvement 1 is particularly pronounced in the plots due
to the logarithmic scale, but it does not substantially con-
tribute to net AUC due to the narrow FPR range. How-
ever, this low-FPR part of the ROC is important from an
operational perspective when deploying a malware detection
model in practice. Substantially higher TPRs at low FPR
could allow for novel use cases in an operational scenario.
Notice that this effect is more pronounced for auxiliary
losses containing multi-objective binary labels (Figs. 2b, 2c,
and 2d) than for the Poisson loss, suggesting that it occurs
most prominently when employing our multi-objective bi-
nary label losses. Let us consider why a multi-objective bi-
nary loss might cause such an effect to occur: At low FPRs,
we see high thresholds on the detection score from the main
output of the network. To surpass this threshold and register
as a detection, the main sigmoid output must be very close to
1.0, i.e., very high response. Under a latent correlation with
the main output, a high-response hit for an auxiliary target
label could also boost the response for the main detector out-
put, while a baseline model without this information might
wrongly classify the sample as benign. We hypothesize that
improvement 1 occurs from having many objectives simulta-
neously and thereby increasing chances for a high-response
target hit. The loss type may or may not be incidental, which
is consistent with its noticeable but less pronounced presence
under a single-objective Poisson auxiliary loss (Figure 2a).
Improvement 2 likely stems from improvements in detec-
tion rate that we see around 10−3 FPR and higher. Notice
that these effects are more pronounced in Figs. 2a and 2d, are
somewhat noticeable in Figure 2b, and are not noticeable in
Figure 3: When we remove the attribute tags loss (green dot-
ted line) we get a similar shaped ROC curve with similar
ROC compared to using all losses (red dashed line), but with
slightly higher variance in the ROC. This supports our hy-
potheses about effects of different loss functions on the shape
of the ROC curve. The baseline is shown as a blue solid line
for comparison.
Figure 2c, consistent with the resultant AUCs. This suggests
that the effect occurs most prominently in the presence of an
auxiliary count loss. We postulate that this occurs because
our aggregate detection label is derived by thresholding the
net number of vendor detections for each sample but doing
so removes a notional view of conﬁdence that a sample is
malicious. Alternatively stated, thresholding removes infor-
mation on the difﬁculty of classifying a malicious sample or
the extent of “maliciousness” that the number of detection
counts provides. Bear in mind that some detectors are better
at detecting different types of malware than others, so more
detections suggest a more malicious ﬁle, e.g., with more mal-
ware components, or a more widely blacklisted ﬁle (higher
conﬁdence). Providing information on the number of counts
in an auxiliary loss function may therefore provide the clas-
siﬁer more principled information on how to order detection
scores, thus allowing for more effective thresholding and a
better ROC curve.
Improvement 3 occurs across all loss types, particularly
in low FPR ranges, with the exception of very low FPRs
(e.g., 10−6), where accurately measuring mean and variance
is an ill-posed problem due to the size of the dataset (cf. sec-
tion 4.4). Comparing the ROC plots in Figure 2, the reduc-
tion in variance appears more pronounced as the number of
losses increases.
Intuitively, this is not a surprising result
since adding objectives/tasks imposes constrains the allow-
able weight space – while many choices of weights might
allow a network to perform a single task well only a subset
of these choices will work well for all tasks simultaneously.
312    28th USENIX Security Symposium
USENIX Association
Thus, assuming equivalent base topology, we expect a net-
work that is able to perform at least as well on multiple tasks
as many single-task networks to exhibit lower variance.
Combining all losses seems to accentuate all improve-
ments (1-3) with predictable modes which we attribute to