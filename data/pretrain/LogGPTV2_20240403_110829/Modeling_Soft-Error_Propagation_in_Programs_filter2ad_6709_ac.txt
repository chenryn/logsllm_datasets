the branch, the probability of a corrupted store instruction
is 0.01 ∗ 0.9 ∗ 0.7 = 0.0063. Note that this is usually a
very small value which can be ignored. This is because the
branch probabilities of a loop-terminating branch are usually
highly biased due to the multiple iterations of the loop. So the
total probability in this example is approximated to be 0.62,
which is what we calculated above. Equation 2 is simpliﬁed
by integrating and cancelling out the terms in the calculations.
is 0.7*0.9 (bb1-
In this example, Pb
bb2-bb4), and thus Pc is 0.99 ∗ 0.7 ∗ 0.9 = 0.62.
is 0.99 (bb0-bb1), Pe
E. Details: Memory Sub-Model (fm )
Recap that fm reports the probability for the error to
propagate from the corrupted memory locations to the program
output. The idea is to represent memory data dependencies
between the load and store instructions in an execution, so
that the model can trace the error propagation in the memory.
(a) Code Example
Fig. 4: Examples for Memory Sub-model
(b) Dependency Graph
We use the code example in Figure 4a to show how we
prune the size of the memory dependency graph in fm by
removing redundant dependencies (if any). There are two inner
loops in the program. The ﬁrst one executes ﬁrst, storing data
to an array in memory (INDEX 1). The second loop executes
later, loading the data from the memory (INDEX 2). Then the
program makes some decision (INDEX 3) and decides whether
the data should be printed (INDEX 4) to the program output.
Note that the iterations between loops are symmetric in the
example, as both manipulate the same array (one updates, and
the other one reloads). This is often seen in programs because
they tend to manipulate data in blocks due to spatial locality.
In this example, if one of the dynamic instructions of INDEX
1 is corrupted, one of the dynamic instructions of INDEX 2
must be corrupted too. Therefore, instead of having one node
for every dynamic load and store in the iterations of the loop
executions, we need only two nodes in the graph to represent
the dependencies. The rest of the dependencies in the iterations
are redundant, and hence can be removed from the graph as
they share the same propagation. The dependencies between
dynamic loads and stores are tracked at runtime with their
static indices and operand memory addresses recorded. The
redundant dependencies are pruned when repeated static load
and store pairs are detected.
We show the memory data dependency graph of fm for the
code example in Figure 4b. Assume each loop is invoked once
with many iterations. We create a node for the store (INDEX
1), load (INDEX 2) and printf (INDEX 3, as program output)
in the graph. We draw an edge between nodes to present their
dependencies. Because INDEX 3 may cause divergence of the
dependencies and hence error propagation, we weight the prop-
agation probability based on its execution probability. We place
a NULL node as a placeholder indicating masking if F branch
is taken in INDEX 3. Note that an edge between nodes may
also represent a static data-dependent instruction sequence,
e.g., the edge between INDEX 2 and INDEX 4. Therefore,
fs is recursively called every time a static data-dependent
instruction sequence is encountered. We then aggregate the
propagation probabilities starting from the node of INDEX 1
to each leaf node in the graph. Each edge may have different
propagation probabilities to aggregate – it depends on what
fs outputs if a static data-dependent instruction sequence is
present on the edge. In this example, assume that fs always
outputs 1 as the propagation probability for each edge. Then,
the propagation probability to the program output (INDEX 4),
if one of the store (INDEX 1) in the loop is corrupted, is
1∗ 1∗ 1∗ 0.6/(0.4 + 0.6) + 1∗ 1∗ 0∗ 0.4/(0.4 + 0.6) = 0.6. The
zero in the second term represents the masking of the NULL
node. As an optimization, we memoize the propagation results
calculated for store instructions to speed up the algorithm. For
example, if later the algorithm encounters INDEX 1, we can
use the memoized results, instead of recomputing them. We
will evaluate the effectiveness of the pruning in Section V-C.
Floating Point: When we encounter any ﬂoating point
data type, we apply an additional masking probability based
on the output format of the ﬂoating point data. For example,
in benchmarks such as Hotspot, the ﬂoat data type is used.
By default, Float carries 7-digit precision, but
in (many)
programs’ output, a “%g” parameter is speciﬁed in printf
which prints numbers with only 2-digit precision. Based on
the speciﬁcation of IEEE-754 [1], we assume that only the
mantissa bits (23 bits in Float) may affect the 5 digits that are
cut off in the precision. This is because bit-ﬂips in exponential
bits likely cause large deviations in values, and so cutting-off
the 5 digits in the precision is unlikely to mask the errors
in the exponent. We also assume that each mantissa bit has
equal probability to affect the missing 5 digits of precision.
In that way, we approximate the propagation probability to
be ((32-23)+23*(2/7))/32 = 48.66%. We apply this masking
probability on top the propagation probabilities, for Float data
types used with the non-default format of printf.
V. EVALUATION
In this section, we evaluate TRIDENT in terms of its ac-
curacy and scalability. To evaluate accuracy, we use TRIDENT
to predict overall SDC probabilities of programs as well as
the SDC probabilities for individual instructions, and compare
them with those obtained using FI and the simpler models.
To evaluate scalability, we measure the time for executing
TRIDENT, and compare it with the time taken by FI. We ﬁrst
present the experimental setup and then the results. We also
make TRIDENT and the experimental data publicly available1.
1https://github.com/DependableSystemsLab/Trident
for(...){store …;}…for(...){$0 = load …;if(cmp ...){print $0;}}INDEX 1INDEX 2 INDEX 3INDEX 4INDEX2T0.6F0.4INDEX1INDEX4NULLA. Experimental Setup
1) Benchmarks: We choose eleven benchmarks from com-
mon benchmark suites [4], [5], [15], and publicly available
scientiﬁc programs [2], [16], [29] — they are listed in Ta-
ble I. Our benchmark selection is based on three criteria: (1)
Diversity of domains and benchmark suites, (2) whether we
can compile with our LLVM infrastructure, and (3) whether
fault injection experiments of the programs can ﬁnish within
a reasonable amount of time. We compiled each benchmark
with LLVM with standard optimizations (-O2).
TABLE I: Characteristics of Benchmarks
Suite/Author
Area
Program
Input
33 5
in_4.txt
reference.bin
frame.bin
graph_input.dat
scan
simple_case.e
-s 1 -p
geo fﬁeld
control
2048 10 1
1000 10
64 64 1 1
temp_64
power_64
graph4096.txt
Bench-
mark
Libquan-
tum
Blacksc-
holes
Sad
Bfs
Hercules
Lulesh
PuReMD
SPEC
Parsec
Parboil
Parboil
Carnegie Mellon
University
Lawrence Livermore
National Laboratory
Purdue University
Nw
Rodinia
Pathﬁnder
Rodinia
Hotspot
Rodinia
Quantum computing
Finance
Video encoding.
Graph traversal
Earthquake
simulation
Hydrodynamics
modeling
Reactive molecular
dynamics simulation
DNA sequence
optimization
Dynamic
programming
Temperature and
power simulation
Bfs
Rodinia
Graph traversal
injector to perform FIs at
2) FI Method: We use LLFI [30] which is a publicly
available open-source fault
the
LLVM IR level on these benchmarks. LLFI has been shown
to be accurate in evaluating SDC probabilities of programs
compared to assembly code level injections [30]. We inject
faults into the destination registers of the executed instruc-
tions to simulate faults in the computational elements of the
processor as per our fault model. Further, we inject single bit
ﬂips as these are the de-facto model for emulating soft errors
at the program level, and have been found to be accurate for
SDCs [25]. There is only one fault injected in each run, as soft
errors are rare events with respect to the time of execution of
a program. Our FI method ensures that all faults are activated,
i.e., read by an instruction of the program, as we deﬁne SDC
probabilities based on the activated instructions (Section II).
The FI method is in line with other papers in the area [3], [9],
[18], [30].
B. Accuracy
We design two experiments to evaluate the accuracy of
TRIDENT. The ﬁrst experiment examines the prediction of
overall SDC probabilities of programs, and the second ex-
amines predicted SDC probabilities of individual instructions.
In the experiments, we compare the results derived from
TRIDENT with those from the two simpler models and FI.
As described earlier, TRIDENT consists of three sub-models in
order: fs , fc and fm . We create two simpler models to (1)
understand the accuracy gained by enabling each sub-model
and (2) as a proxy to investigate other models, which often
lack modeling beyond static data dependencies (Section VII-C
performs a more detailed comparison with prior work). We
ﬁrst disable fm in TRIDENT, leaving the two sub-models fs
and fc enabled, to create a model: fs +fc . We then further
remove fc to create the second simpliﬁed model which only
has fs enabled, which we represent as fs .
1) Overall SDC probability: To evaluate the overall SDC
probability of a given program, we use statistical FI. We mea-
sure error bars for statistical signiﬁcance at the 95% conﬁdence
level. We randomly sample 3,000 dynamic instructions for FIs
(one fault per run) as these yield tight error bars at the 95%
conﬁdence level (±0.07% to ±1.76%) - this is in line with
other work that uses FI. We calculate SDC probability of each
program based on how many injected faults result in SDC.
We then use TRIDENT, as well as the two simpler models,
to predict the SDC probability of each program, and compare
the results with those from FI. To ensure fair comparison, we
sample 3,000 instructions in our models as well (Section IV-A).
The results are shown in Figure 5. We use FI to represent
the FI method, TRIDENT for our three-level model, and fs+fc
and fs for the two simpler models. We ﬁnd TRIDENT prediction
matches the overall SDC probabilities obtained through FI,
with a maximum difference of 14.26% in Sad, and a minimum
difference of 0.11% in Blackscholes, both in percentage points.
This gives a mean absolute error of 4.75% in overall SDC
prediction. On the other hand, fs +fc and fs have a mean
absolute error of 19.56% and 15.13% respectively compared
to FI – more than 4 and 3 times higher than those obtained
using the complete three-level model. On average, fs +fc and
fs predict the overall SDC probability as 33.85% and 23.76%
across the different programs, whereas TRIDENT predicts it to
be 14.83%. The SDC probability obtained from FI is 13.59%,
which is much more in line with the predictions of TRIDENT.
We observe that in Sad, Lulesh and Pathﬁnder, TRIDENT
encounters relatively larger differences between the prediction
and the FI results (14.26%, 7.48% and 8.87% respectively).
The inaccuracies are due to a combination of gaps in the imple-
mentation, assumptions, and heuristics we used in TRIDENT.
We discuss them in Section VII-A.
To compare the results more rigorously, we use a paired T-
test experiment [28] to determine how similar the predictions
of the overall SDC probabilities by TRIDENT are to the FI
results.2 Since we have 11 benchmarks, we have 11 sets of
paired data with one side being FI results and the other side
being the prediction values of TRIDENT. The null hypothesis
is that there is no statistically signiﬁcant difference between
the results from FIs and the predicted SDC probabilities
by TRIDENT in the 11 benchmarks. We calculate the p-
value in the T-test as 0.764. By the conventional criteria (p-
value>0.05), we fail to reject the null hypothesis, indicating
that the predicted overall SDC probabilities by TRIDENT are
not statistically different from those obtained by FI.
We ﬁnd that the model fs +fc always over-predicts SDCs
compared with TRIDENT. This is because an SDC is assumed
once an error propagates to store instructions, which is not
always the case, as it may not propagate to the program
output. On the other hand, fs may either over-predict SDCs
2We have veriﬁed visually that the differences between the two sides of
every pair are approximately normally distributed in all the T-test experiments
we conduct, which is the requirement for validity of the T-test.
Fig. 5: Overall SDC Probabilities Measured by FI and Predicted by the Three Models (Margin of Error for FI: ±0.07% to
±1.76% at 95% Conﬁdence)
(e.g., Libquantum, Hercules) because an SDC is assumed once
an error directly hits any static data-dependent instruction
sequence ending with a store, or under-predict them (e.g., Bfs,
Blackscholes) because error propagation is not tracked after
control-ﬂow divergence.
C. Scalability
2) SDC Probability of Individual Instructions: We now
examine the SDC probabilities of individual instructions pre-
dicted by TRIDENT and compare them to the FI results. The
number of static instructions per benchmark varies from 76 to
4,704, with an average of 944 instructions. Because performing
FIs into each individual instruction is very time-consuming, we
choose to inject 100 random faults per instruction to bound
our experimental time. We then input each static instruction to
TRIDENT, as well as the two simpler models (fs +fc and fs ),
to compare their predictions with the FI results. As before,
we conduct paired T-test experiments [28] to measure the