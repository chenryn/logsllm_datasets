29
5,945
1,937
122
3
6 Discussion
6.1
Improvement
Combining More Features. NEIGHBORWATCHER
only uses spamming structure information to infer spam,
other spamming behaviors could also be incorporated to-
gether to help improve the accuracy. For example, we ﬁnd
some spammers post their spam with both  tags and BBcode tags to assure they can embed
spam links, because they have no idea about what kind of
method the target spam harbors support to embed links. In
addition, some spam links are posted on websites with dif-
ferent languages because spammers do not care about har-
bor’s native languages. For example, some spammers post
Russian spam in Chinese websites, Korean websites, and
English websites, which is extremely unlikely to be normal
postings. In this case, we can incorporate these features to
improve the overall inference accuracy.
Improving Algorithms.
In Eq.(3), we assume these
postings on different harbors have the same weight. How-
ever, postings on different harbors are not necessary equal.
For example, if postings on a harbor have a similar posting
time, same email, or same IP address with the input post-
ing, then it has a high possibility that they are spammed by
a same spammer at the same time. Thus we could assign
different weight to different harbors by considering these
factors.
Updating Spam Harbors. In our system, we ﬁnd new
spam harbors by recursively searching inferred spam in
search engines. Thus, as long as spammers keep utiliz-
ing these spam infrastructures, we can always ﬁnd out new
spam harbors. However, our current system cannot infer
those spam posted on only one harbor in our dataset, thus
we cannot ﬁnd out other harbors that are also posted with
this spam. In this case, we could use the following strategy.
From the search results of the posting link, we attempt to
build a relationship graph among returned websites based
on whether they already share similar postings (excluding
the searched link) on existing pages on the websites. If these
websites show very close relationships (e.g., dense connec-
tions), the searched link has a good chance to be spam (and
thus the corresponding search result websites are possible
spam harbors). The intuition here is that although a few
normal links may appear on a variety number of websites,
it is extremely unlikely that normal users will keep post-
ing similar postings on certain websites. Our ongoing work
will design and test new algorithms to efﬁciently update our
spam harbors dataset.
Although NEIGHBORWATCHER shows promise for in-
ferring new spam and spam harbors, there is still much room
for improvement. In this section, we discuss several speciﬁc
improvement areas, and the possible evasions to our current
inference algorithm.
6.2 Possible Evasion
Evasion by exploring new harbors. Obviously spam
harbors that we collected are only the subset of spammers’
harbors. Thus if spammers know our collected harbors, they
(a) Daily IP
(b) Daily Email
Figure 15. Daily Comparison with Existing Spam Blacklists
may try to spam on other harbors that are not included in our
database, or they may ﬁnd brand new harbors. In this case,
our neighborhood-based inference algorithm could not de-
tect their new structure. However, as long as spammers also
post spam on both these new harbors and old harbors, we
can still ﬁnd out their new harbors by keeping updating our
database. Otherwise, spammers need to keep ﬁnding new
harbors and give up existing qualiﬁed harbors, which is less
likely to happen considering the cost.
Evasion by changing spamming behaviors. If spam-
mers know that we use spam links to build up relationship
graphs, spammers may spam different links on different har-
bors. Thus we can not build up their spamming structure.
However, in this case, spammers need to keep ﬁnding more
harbors to post their variety links, which will increase their
cost and time. Otherwise, they need to post the same spam
several times on certain harbors (in order to boost search
ranks), which will increase the possibility of being detected
by content-based detection systems.
Evasion by spamming polymorphic URLs. Our algo-
rithm relies on grouping identical URLs (e.g. we infer a
possible spam message if the spamming URL also appears
on many of its neighborhood clique harbors). Thus, spam-
mers may try to evade our system with polymorphic URLs
(i.e., each URL can be different on different harbors). How-
ever, in general, it is not always possible to make full poly-
morphic URLs for a given spam URL to be promoted. If
spammers choose URL shortening services to achieve poly-
morphic URLs, we can always use the resolved ﬁnal URLs
in our system. Furthermore, we can use the domain of a
spam URL instead of the full URL as input, which is rela-
tively stable if spammers want to promote certain domains.
7 Related Work
In this section, we discuss related work in the following
two perspectives.
Comment Spam: To detect comment spam, a few stud-
ies have been done previously. Kolari et al. [31] proposed
to detect blog spam based on only content. They use a
machine-learning based method to detect spam by extract-
ing features from the posting content, such as bag-of-words,
bag-of-anchors, and bag-of-URLs. Mishne et al. [24] pro-
posed a language model based approach to detect comment
spam. They generate a language model for the blog posts,
the comments, and the pages linked by spam, and then use
language model divergences to classify spam.
Recently, to overcome the pitfalls of content-based de-
tection systems, Niu et al.
[32] studied comment spam
in both legitimate and honeypot forums, and proposed a
context-based detection method that looks for redirection
and cloaking for forum spam. Shin et al. [33] studied the
characteristics of spam from a research blog for 286 days,
and developed a light-weight SVM classiﬁer to detect fo-
rum spam. Their features include 16 different features ex-
tracted from spammer origins, comment activities, URLs in
spam comments, and textual contents. Tan et al. [23] con-
ducted a comprehensive analysis of spamming activities on
a commercial blog site covering over 6 million posts and
400,000 users. Through studying on non-textural patterns,
such as posting activities, spam link metrics, they devel-
oped a realtime system to detect spam postings based on 13
non-textual features. Kantchelian et al. [28] deﬁne spam as
uninformative content, and thus they proposed a machine-
learning based method to detect spam based on the content
complexity of comments in social medias.
Most of these existing research studies the problem from
certain blogs or honeypot blogs. However, we study from
a large number of spam harbors that are frequently used
for spamming. We present an in-depth study of these
harbors, which could not be observed from only a few
blogs. Furthermore, complementary to content-, context-
, and behavior-based detection features, our system ex-
ploits the spamming infrastructure to detect comment spam,
which is much more stable in the spamming process. Thus
our system is a good supplement to existing research.
Graph-based algorithm: Graph-based algorithms have
been previously applied in spam detection. Pagerank [29]
and Trustrank [35] have been widely used by search engines
to determine the search ranks of search results. Zhao et al.
[22] proposed a spam detection system by exploring link de-
pendencies and content similarities among web pages. The
system ﬁrst clusters the hosts based on content features and
assigns labels for clusters using majority voting. Then the
system propagates these labels to their neighbor hosts and
uses predicted labels as new features to retrain the classi-
ﬁer. Different from these work, our paper uses a different
graph-based algorithm to characterize the spamming struc-
ture, and combines with the real posting structure to infer
spam.
Our inference algorithms are motivated by the following
two studies [26, 20]. Ramachandran et al. [20] proposed
an email spam ﬁltering system. The system takes email-
sending patterns of all senders as input and builds clusters
of sending patterns from a small seed of spammers. Thus, it
classiﬁes senders as spammers if their behaviors are similar
to the patterns of known spammers. Zhang et al. [26] built a
system to predict blacklist based on attackers’ history. Their
intuition is that if two hosts are frequently attacked by an
attacker in history, it has a high probability that they will
be attacked by the same attacker in the future. Our work
shares a similar intuition with these two studies, however in
a totally different application and with different inference
algorithms. Furthermore, we present a measurement study
of spam harbors, which is not shown by any prior work.
8 Conclusion
Although comment spam has been studied for several
years, arms race between spammers and researchers has
made current existing detection systems losing their po-
tency. A lots of spamming techniques are developed by
spammers to evade content- or context-based detection sys-
tems.
In this paper, we present a deep study on comment spam
from a new perspective, i.e., the spamming infrastructure,
which is the core and stable part in the comment spamming
process. Through measuring 35,931 spam harbors exploited
by spammers to post spam, we conclude that spammers pre-
fer to keep utilizing their spam harbors for spamming unless
they are blocked. Based on this ﬁnding, we design a graph-
structure-based inference system to infer comment spam by
checking if the same spam also appears on its neighbor
(clique) harbors. Our evaluation results show that we can
infer a large number of new comment spam and spam har-
bors, and keep ﬁnding them everyday.
9 Acknowledgments
This material is based upon work supported in part by the
National Science Foundation under Grant CNS-1218929.
Any opinions, ﬁndings, and conclusions or recommenda-
tions expressed in this material are those of the author(s)
and do not necessarily reﬂect the views of the National Sci-
ence Foundation.
References
and
dorks:
google
for
http://www.darkreading.com/
[1] Alexa rank. http://www.alexa.com/.
[2] Botnets
hacking.
vulnerabilitymanagement/167901026/
security/vulnerabilities/231500104/
botnetsandgoogledorksanewrecipeforhacking.
html.
A new recipe
[3] cﬁnder. http://www.cfinder.org/.
[4] The (evil) genius of comment spammers.
http:
//www.wired.com/wired/archive/12.03/
google.html?pg=7.
[5] Globalspyware. http://globalspyware.com/.
[6] Google
bombs.
http://www.
searchenginepeople.com/blog/
incredible-google-bombs.html.
[7] Google cache.
http://www.googleguide.com/
cached_pages.html.
api
pagerank
[8] Google
in
php.
http:
//www.fusionswift.com/2011/10/
google-pagerank-api-in-php-october-2011/.
[9] Google rolls out content
spam detection.
http:
//www.nationalpositions.com/blog/
seonewsgooglerollsoutcontentspamdetection/.
[10] Google safe browsing. http://code.google.com/
apis/safebrowsing.
[11] Google search and search engine spam.
http:
//googleblog.blogspot.com/2011/01/
google-search-and-search-engine-spam.
html.
[12] Hamming distance.
http://en.wikipedia.org/
wiki/Hamming_distance.
[13] Notfollow.
Nofollow.
[14] rel=“nofollow”.
http://en.wikipedia.org/wiki/
http://support.google.com/
webmasters/bin/answer.py?hl=en&answer=
96569.
[15] Safe browsing-protecting web users
for ﬁve years
http://googlepublicpolicy.
and counting.
blogspot.com/2012/06/
safe-browsingprotecting-web-users-for.
html.
[32] H. C. M. M. Y. Niu, Y.M. Wang and F. Hsu. A quantita-
tive study of forum spamming using context-based analysis.
In Proceedings of Network and Distributed System Security
Symposium(NDSS), February, 2007.
[33] M. G. Y. Shin and S. Myers. Prevalence and mitigation
In Proceedings of the 30th Annual
of forum spamming.
IEEE Conference on Computer Communications (INFO-
COM, 2011.
[34] M. G. Y. Shin and S. Myers. The Nuts and Bolts of a Forum
Spam Automator. In Proceedings of the Wkshp. on Large-
Scale Exploits and Emergent Threats (LEET), 2011.
[35] H. G.-M. Z. Gyongyi and J. Pedersen. Combating Web
Spam with TrustRank. In 30th International Conference on
Very Large Data Bases, Aug., 2004.
A Spam Search Result Example
Figure 16 shows an example of a Google search result
page for a spam link. We can draw the following conclu-
sions from it: (1) Spammers do use a variety of spam har-
bors for spamming. (2) Spammers post this spam in differ-
ent harbors at a similar time (Mar 23, 2012). (3) Spammers
post this spam with the similar anchor text:“allgrannysex”.
Figure 16. Example of Spam Search Results
[16] Spambust. http://spambusted.com/.
[17] Stop forum spam.
http://www.stopforumspam.
com/.
[18] What does your google pagerank mean. http://www.
redfusionmedia.com/google_pagerank.htm.
[19] K. W. A. Ramachandran, A. Dasgupta and N. Feamster.
Spam or ham?: characterizing and detecting fraudulent not
spam reports in web mail systems. In Proceedings of the 8th
Annual Collaboration, Electronic messaging, Anti-Abuse
and Spam Conference(CEAS 11), 2011.
[20] N. F. A. Ramachandran and S. Vempala. Filtering spam
In Proceedings of the 14th
with behavioral blacklisting.
ACM conference on computer and communications secu-
rity,, 2007.
[21] S. Brin and L. Page. The anatomy of a large-scale hyper-
In Proceedings of the seventh
textual Web search engine.
international conference on World Wide Web, 1998.
[22] D. D. C. Castillo and A. Gionis. Know your neighbors: Web
spam detection using the web topology. In ACM Special In-
terest Group on Information Retrieval (SIGIR) Conference,
July, 2007.
[23] S. C. X. Z. E. Tan, L. Guo and Y. Zhao. Spam behavior
analysis and detection in user generated content on social
network. In Proceedings of 32nd International Conference
on Distributed Computing Systems (ICDCS 2012), Macao,
China, June 18-21,, 2012.
[24] D. C. G. Mishne and R. Lempel. Blocking Blog Spam
In First Interna-
with Language Model Disagreement.
tional Workshop on Adversarial Information Retrieval on
the Web, at 14th international conference on World Wide
Web(WWW), 2005.
[25] Z. Gyongyi and H. Garcia-Molina. Web Spam Taxonomy.
In Technical report, Stanford Digital Library Technologies
Project, Mar, 2004.
[26] P. P. J. Zhang and J. Ullrich. Highly Predictive Blacklist-
In Proceedings of the USENIX Security Symposium,
ing.
San Jose, CA, July, 2008.
[27] Z. X. J. Zhang, C. Y and G. Gu. PoisonAmpliﬁer: A Guided
Approach of Discovering Compromised Websites through
Reversing Search Poisoning Attacks. In Proceedings of the
15th International Symposium on Research in Attacks, In-
trusions and Defenses (RAID’12), 2012.
[28] A. Kantchelian, J. Ma, L. Huang, S. Afroz, A. Joseph, and
J. D. Tygar. Robust detection of comment spam using en-
tropy rate. In Proceedings of the 5th ACM workshop on Se-
curity and artiﬁcial intelligence (AISec’12), 2012.
[29] R. M. L. Page, S. Brin and T. Winograd. The PageR-
ank citation ranking: Bringing order to the Web.
In
Technical report, Stanford University Database Group,
http://citeseer.nj.nec.com/368196.html, 1998.
[30] T. M. N. Leontiadis and N. Christin. Measuring and analyz-
ing search-redirection attacks in the illicit online prescrip-
tion drug trade. In Proceedings of the 20th USENIX Secu-
rity, 2011.
[31] T. F. P. Kolari and A. Joshi. SVMs for the blogosphere:
Blog identiﬁcation and splog detection. In Proceedings of
AAAI Spring Symposium on Computational Approaches to
Analysing Weblogs, March, 2006.