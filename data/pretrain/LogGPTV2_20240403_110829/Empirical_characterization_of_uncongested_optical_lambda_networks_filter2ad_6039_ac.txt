The receiver was experiencing a livelock, resulting in 12% packet loss at a data rate of 2400 Mbps. This issue can be observed on a Linux 2.6 kernel, where the network bottom half (softirq) cannot complete its tasks in a timely manner. As a result, the ksoftirqd/CPU# kernel thread, which runs exclusively on the same CPU, is overwhelmed. The Iperf task, which is also placed on this CPU due to the scheduler's default behavior, fails to consume the pending packets in the socket buffer efficiently.

The larger the socket buffer, the more significant the packet loss, as there is insufficient CPU time remaining for the task. Figures 5(b) and 5(c) illustrate this behavior with different buffer sizes. A smaller buffer (1MB) shows clear packet loss, while a larger buffer (4MB) results in negligible loss, except at a data rate of 2000 Mbps, where a single, unobservable event occurs. At higher data rates, irqbalance spreads the interrupts across multiple CPUs, reducing the loss.

Figure 5(c) also highlights a particular packet loss event in the core network during a 48-hour period, with a sender data rate of 400 Mbps and a one-way latency of 68.9 ms. This outlier, occurring over a 60-second interval, may have been caused by NLR maintenance or path blackouts.

In summary, the experiments show that packet loss is primarily due to a buffer overrun at the receiver, not in the network core. Binding all NIC interrupts to the same CPU reduces loss, especially at higher data rates, by minimizing cache thrashing. However, at lower data rates, irqbalance works well.

To measure achievable throughput, we conducted 24-hour bulk TCP transfer tests using 60-second Iperf bursts. Figure 6(a) shows the TCP throughput for a single flow with different window sizes configured for each path. A higher window size is necessary for high-latency, high-bandwidth paths. Almost all TCP variants yield similar throughput, with the exception of TCP Vegas, which underperforms.

No packet loss occurs for any single-flow test, but throughput decreases with multiple flows. To maximize throughput, applications like GridFTP issue multiple TCP Iperf flows in parallel. Figure 6(b) depicts the throughput for four concurrent flows, showing that some TCP variants perform better than others when competing with flows of the same type. 

Packet loss does occur for multiple TCP flows, affecting performance even though TCP is a reliable transport protocol. Figure 6(c) shows the percentage of packet loss associated with the four concurrent flows, including retransmissions, acknowledgments, and timeouts. Loss occurs in various locations, such as the DMA rings, the TCP stack, and the user-mode process owning the socket.

Finally, we examined the impact of packet batching techniques on the measurements. A CPU is notified of packet arrivals and departures via interrupts. In a typical commodity kernel, if the interrupt overhead exceeds the packet processing rate, a livelock will occur. Figure 7 illustrates the packet inter-arrival times for an Iperf experiment at 100 Mbps, with and without interrupt throttling and NAPI enabled. The interrupt service routine, which notifies the CPU, must handle this phenomenon to avoid artificial delays.