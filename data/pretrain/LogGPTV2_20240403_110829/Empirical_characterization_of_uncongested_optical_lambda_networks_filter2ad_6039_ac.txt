that the receiver 
ksoftirqd/CPU# kernel thread. 
on the same CPU, and picks up the remain­
There are three key points of interest. 
was experiencing 
livelock 
'" 12% packet loss for a sender data rate of 2400Mbps.) 
at 2400Mbps 
closer 
receive 
there is an abrupt increase 
look, we noticed 
live lock [25]. On a Linux 2.6 kernel, 
as the network bottom half cannot fin­
easily be observed 
ish in a timely manner, and it is forced to start the the cor­
responding 
runs exclusively 
ing work the softirq 
the receive 
As a result, 
rupts (rx, tx, rxnobuff, 
whelmed CPU-the same CPU that runs the correspond­
ing ksoftirqd/CPU# and the user-mode 
Iperf task is placed on the same CPU since the scheduler's 
default 
quently, 
sume the packets pending in the socket buffer in a timely 
fashion. 
nificant 
Hence, the bigger the socket buffer, the more sig­
the loss, precisely 
behavior 
there is not enough CPU time remaining 
is to minimize cache thrashing. 
Conse­
to con­
livelock 
etc.) were serviced 
did not finish, acting as a rate limiter. 
occurs given that all inter­
5(b) and (c) show. 
by a single over­
Iperftask. The 
as Figures 
The thread 
Second, end-host 
increases 
with  sender 
data 
packet  loss 
in the Figure insets. 
small buffer, 
1MB, so the effect is clear. Fig­
to a larger buffer (4MB) for which, 
of data rates of 2400Mbps, 
there is a sin­
Figure 5(b) corresponds 
rate, as visible 
to a relatively 
ure 5(c) corresponds 
with the exception 
gle negligible 
rate of2000Mbps (almost 
Similarly, 
on); however, 
interrupts 
packet loss 
event along the tiny path at a data 
unobservable 
on scale of Figure). 
this trend is evident in Figure 5(a) (irqbalance 
at higher data rates, irqbalance 
spreads the 
to many different 
CPUs and the loss decreases. 
only 
occurring 
on the medium path (one way la­
event-the 
during the entire 
Third, Figure 5(c) shows a particular 
loss in the core network we experienced 
48-hour period, 
tency is 68.9 ms) for a sender data rate of 400Mbps. During 
that 
the course of the experiments, 
this was a single outlier 
occurred 
during a single 6O-second 
it could 
have been caused by events such as NLR maintenance-we 
have experienced 
ments being serviced, 
due to various path seg­
path blackouts 
run. We believe 
or upgraded. 
replaced, 
To summarize, 
the experiments 
show virtually 
no loss 
is overloaded, 
loss. NIC interrupt 
End-host 
in the socket, 
loss is typically 
the re­
backlog queue, or 
in the network core. Instead, 
notably at the receiver. 
sult of a buffer over-run 
DMA ring. Unless the receiver 
large socket buffer prevents 
CPUs affects 
host's ability 
show that, at higher data rates, irqbalance 
decreases 
interrupts 
ance. One benefit of binding all NIC interrupts 
to the same 
CPU stems from the fact that the driver (code and data), the 
kernel network stack, and the user-mode 
loss, and is pivotal 
to handle load graciously. 
loss), whereas, 
to the same CPU reduces loss more than irqbal­
affinity to 
in determining 
the end­
at lower data rates, binding NIC 
application 
incur 
works well (it 
a sufficiently 
Our experiments 
throughput, 
To measure the achievable 
we used 60-
second Iperf bursts to conduct a set of 24-hour bulk TCP 
transfer 
all TCP variants 
TCP-LP and TCP Veno). 
tests over all the Cornell 
in the Linux kernel (except 
for 
available 
NLR Rings; we examined 
Figure 6(a) shows TCP throughput 
results 
for a single 
to each 
with respect 
into larger amount of in-flight 
links. In particular, 
data, which is necessary 
time (RTT), to allow for a  1 Gbps data rate. 
flow with window sizes configured, 
path round-trip 
A higher window translates 
but not suf­
(not yet acknowledged) 
on such high-latency, 
high­
ficient to yield high throughput 
a single TCP flow of 1 Gbps 
bandwidth 
requires 
a window of at least 2MB on the tiny path, 9.4MB 
on the short, 17.3MB on the medium, and 24.4MB on the 
large. Almost all TCP variants 
yield roughly the same 
throughput, 
of TCP Vegas that under­
performs. 
TCP variants, 
even though the end-hosts 
for longer paths, 
window size. 
No packet loss occurs for any of the single-flow 
with the exception 
yet throughput 
have sufficient 
decreases 
TCP flows in parallel 
with multiple 
flows, 
in order to satu­
Since TCP window size is a kernel configuration 
param­
typi­
for adjustment, 
superuser 
privileges 
like GridFTP [9] strive to max­
capacity 
multiple 
by issuing 
the throughput 
applications 
eter that requires 
cal user-mode 
imize throughput 
to fetch / send data. To experiment 
we issued four TCP Iperf flows in parallel 
rate each end-host's 
put. Figure 6(b) depicts 
the window sizes should be sufficient, 
put decreases 
at the end-host 
over, some TCP variants 
throughput 
same type. Note that the TCP throughput 
to the maximum throughput 
path is identical 
ing control 
by directly 
hosts with an optical 
as the path length increases. 
than others when competing 
does occur for multiple 
yield marginally 
experiments 
(performed 
patch cable). 
Even though TCP is a reliable 
transport 
results. 
the overall 
Although 
through­
Importantly, 
loss 
TCP flows. More­
better aggregate 
with flows of the 
over the tiny 
achieved 
dur­
linking 
end­
loss, albeit at the end-host, 
does affect performance 
protocol, 
packet 
[27]. 
loss occurs at the end-hosts, 
and yield maximum through­
978-1-4244-7501-8/10/$26.00 
©201O IEEE 
580 
DSN 2010: Marian et al. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 13:56:42 UTC from IEEE Xplore.  Restrictions apply. 
2010 IEEE/IFIP International 
Conference 
on Dependable Systems & Networks (DSN) 
]
s
p
b
M
[
t
u
p
h
g
u
o
r
h
T
 5000
 4000
 3000
 2000
 1000
 0
]
s
p
b
M
[
t
u
p
h
g
u
o
r
h
T
 5000
 4000
 3000
 2000
 1000
 0
0.14
0.12
0.10
]
s
e
t
y
b
f
o
%
0.08
reno
cubic
bic
vegas
htcp
hybla
illinois
scalable
westwood
yeah
tiny
small
medium
large
(a) TCP throughput for single ﬂow
reno
cubic
bic
vegas
htcp
hybla
illinois
scalable
westwood
yeah
tiny
small
medium
large
(b) TCP throughput for four concurrent ﬂows
rcv_rxloss
rcv_txloss
rcv_tcploss
rcv_tcppruning
snd_rxloss
snd_txloss
snd_tcploss
snd_tcppruning
[
s
s
o
l
t
e
k
c
a
P
0.06
0.04
0.02
0.00
b
i
c
r
e
n
o
c
u
b
i
c
v
e
g
a
s
h
t
c
p
h
y
b
l
a
y
e
a
h
i
l
l
i
n
o
i
s
s
c
a
l
a
b
l
e
w
e
s
t
w
o
o
d
b
i
c
r
e
n
o
c
u
b
i
c
v
e
g
a
s
h
t
c
p
h
y
b
l
a
y
e
a
h
i
l
l
i
n
o
i
s
s
c
a
l
a
b
l
e
w
e
s
t
w
o
o
d
b
i
c
r
e
n
o
c
u
b
i
c
v
e
g
a
s
h
t
c
p
h
y
b
l
a
y
e
a
h
i
l
l
i
n
o
i
s
s
c
a
l
a
b
l
e
w
e
s
t
w
o
o
d
b
i
c
r
e
n
o
c
u
b
i
c
v
e
g
a
s
h
t
c
p
h
y
b
l
a
y
e
a
h
i
l
l
i
n
o
i
s
s
c
a
l
a
b
l
e
w
e
s
t
w
o
o
d
tiny
small
medium
large
(c) Loss associated with four concurrent ﬂows
of TCP loss must account for retransmissions, 
of packet loss correspond­
in Figure 6(b). Unlike UDP loss, 
and timeouts. 
acknowledgments, 
Figure 6( c) shows the percentage 
ing to the TCP throughput 
any analysis 
selective 
and cumulative 
of acknowledgments, 
centage of 
suffices since all UDP packets have identical 
reported 
(rcv_), within the DMA rings (_txloss and _rxloss), 
inferred 
ability 
the data in a timely fashion 
unlike UDP, for which packet count 
process owning the socket to read 
(with _tcploss), and due to the in­
both at the sender (denoted 
of the user-mode 
Figure 6(c) shows per­
(_tcppruning). 
by TCP itself 
loss in bytes, 
different 
size 
size. Loss is 
by snd_) and receiver 
Loss occurs solely in one of the following  locations: 
receive 
loss 
TCP stack 
(rx) DMA ring (rcv_rxloss), 
the receiver's 
by the sender's 
that is then largely inferred 
(snd_tcploss), and finally, 
within the sender's 
(rx) DMA ring (snd_rxloss). The sender sends MTU­
size (I500-byte) 
TCP empty 
(52-byte) 
(ACKs), as 20-byte IP 
header + 20-byte TCP header + l2-byte TCP options. 
loss occurs at the 
There are two key observations. 
First, 
payload acknowledgments 
TCP data packets 
and receives 
receive 
while the sender will drop inbound 
to a de­
will drop in­
receiver 
in the rx DMA rings-the 
Recall that the NIC is configured 
end-host 
bound payload packets, 
ACK packets. 
fault value of 1024 slots per DMA ring. The socket buffer 
is essentially 
the TCP window; hence, it is adjusted 
large value in this experiment. 
Second, there are far more 
ACK packets 
(snd_rxloss) being lost than payload pack­
ets (rcv _rxlos s). However, 
TCP can afford to lose a significant 
worth of ACKs on the  rx DMA ring, provided 
gle ACK with an appropriate 
(subsequent) 
ber is delivered 
observed 
cient to induce end-host 
already 
by TCP Vegas since its low throughput 
identical 
to the TCP stack. Note that there is no loss 
is insuffi­
to the one 
loss, a scenario 
in Figure 6(a). 
described 
of a window 
sequence 
num­
portion 
that a sin­
to a 
since ACKs are cumulative, 
Our experiments 
show that as path length increases, 
more data and, importantly, 
TCP windows are enlarged 
product of the longer paths. This affects 
throughput 
more ACKs are lost since the 
to match the bandwidth 
and 
as the path length increases. 
decreases 
performance, 
delay 
3.4 Packet Batching 
In this section, 
we look closely 
at the impact of packet 
batching 
techniques 
on the measurements 
reported 
above. 
A CPU is notified of the arrival 
and departure 
of packets 
Figure 6. T CP throughput and loss across 
Cornell NLR Rings: (a) throughput for sin­
gle flow, (b) throughput for four concurrent 
flows, (c) loss associated with those four 
concurrent flows; T CP congestion control 
windows configured 
time to allow 1 Gbps of data rate per flow. 
for each path round-trip 
however, 
The typical 
can find itself 
at a NIC by interrupts. 
modity kernel, 
which the CPU expends all available 
terrupts, 
received 
processing 
arrive, 
of consuming 
livelock 
overhead 
receive 
instead 
interrupt-driven 
com­
driven into a state in 
cycles processing 
in­
data. If the interrupt 
is larger than the rate at which packets 
[25] will occur (the interrupt 
over-
978·1·4244·7501·81101$26.00 
©2010 IEEE 
581 
DSN 2010: Marian et al. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 13:56:42 UTC from IEEE Xplore.  Restrictions apply. 
2010 IEEE/IFIP International 
Conference 
on Dependable Systems & Networks (DSN) 
   -L _L
a packet arrival 
.' "),·f -( •••• J II"'''''' J.  _ ..... .;.J. _. ., .• \-I" .........  .. J...l .. !...I-
... .J.!., 
'E 
« 
 160 r------,------,------.------,-----,-------. 
 liS .L!  
ti 100 r 
1j  80 
'" 60 
c... 40 
20 
o L- ____ 
o 
superimposes 
times, thereby 
This phenomenon 
must be handled 
are time-stamped 
time for a UDP 
of 300 packets 
Figure 7 shows the packet inter-arrival 
tools that 
to esti­
100  150  200  250  300 
100  150  200  250  300 
Measured at the receiver .......... 
Intended at the sender ----
Throttling 
Packet Number (Receiver 
Interrupt 
service 
packets 
•• •   .  I 
" 
V'l' I 
Throttling 
Interrupt 
_L -L 
. 
On) 
50 
  ____ 
  ____ 
  ____ 
  ____ 
L-
____ 
from the same CPU. This means that all 
they must originate 
NIC interrupts 
by the same CPU, since received 
in the interrupt 
notifying 
routine. 
consisting 
of a sequence 
Iperf experiment 
at a data rate of 100Mbps (about one packet every 120 J.ls) 
enabled and NAPI dis­
with and without 
abled. We see that the interrupt 
an 
artificial 