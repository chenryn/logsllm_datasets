title:The Good, the Bad, And the Ugly: Stepping on the Security Scale
author:Mary Ann Davidson
2009 Annual Computer Security Applications Conference
The Good, The Bad, And The Ugly: Stepping on the Security Scale  
Mary Ann Davidson  
Oracle Corporation 
Redwood Shores, California  
PI:EMAIL 
Abstract: Metrics are both fashionable and timely: many 
regulations that affect cybersecurity rely upon metrics – albeit, 
of the checklist variety in many cases – to ascertain 
compliance. However, there are far more effective uses of 
security metrics than external compliance exercises. The most 
effective use of security metrics is to manage better, which may 
include: 
Focus scarce resource on most pressing problems (with 
the biggest payoff for resolution) 
•  Make a business case for needed change 
• 
•  Help spot problems early - or successes early 
•  Address  “outside”  concerns  or  criticisms  fairly  and 
objectively 
A successful security metric should:  
•  Motivate  good/correct  behavior  (not  promote  evasive 
tactics just to make the numbers look good) 
• 
Prompt  additional  questions  (“Why?  How?”)  to 
understand what is influencing the numbers 
•  Answer  basic  questions  of  goodness  (e.g.,  “Are  we 
•  Be  objective  and  measurable,  even  if  correlation  may 
doing better or worse?”) 
not equal causality 
Add to that the fact that information is becoming (either 
directly or indirectly) more regulated through vehicles such 
as  the  Federal  Information  Security  Management  Act 
(FISMA) and Sarbanes-Oxley and often regulation requires 
one  to  attest  to  business  practices  to  the  satisfaction  of 
independent auditors, which usually necessitates measurable 
proof points (metrics). The US Federal Government is also 
becoming security metrics-focused through such programs as 
the  Federal  Desktop  Core  Configuration  (FDCC)  and 
through  sponsorship  of  such  programs  as  the  creation  of 
security measurement schemes such as Common Weakness 
Enumeration  (CWE)  and  Common  Vulnerabilities  and 
Exposure  (CVE)  under 
tagline  “making  security 
measurable.” 
Lastly,  the  practice  of  information  security  naturally 
lends  itself  to  measurement  if  for  no  other  reason  than  the 
difficulties in determining “How much security is enough?” 
and  “Am  I  doing  the  right  things?”  Anyone  tasked  with 
protecting an enterprise’s networks must have a basic handle 
on such indicators as “What are my most critical systems?” 
and “What is the ‘security-readiness state’ – e.g., patch level 
and secure configuration status – of those systems?”  
the 
While  faddishness  is  never  a  good  reason  to  adopt  a 
business  practice,  there  are  clearly  many  good  reasons  to 
implement  metrics  programs  around  security.  One  of  the 
most  often-cited  bromides  is  “If  you  can’t  measure  it,  you 
can’t  manage  it.”  (This  isn’t  necessarily  true,  by  the  way, 
since  many  managers  supervise  the  people  who  work  for 
them  without  explicitly  measuring  their  work.  You  can 
imagine the revolt if employee compensation were based on 
such metrics as number of emails sent and received, number 
of hours at work, volume of phone calls placed, and the like.)  
It is true, however, that a good metrics program can help 
you  manage  better.  In  fact,  the  primary  justification  for 
implementing  a  metrics  program  in  security  or  any  other 
discipline – absent a regulatory requirement – is simply, to 
manage  better.  Then-Mayor  Rudy  Giuliani  famously  and 
dramatically improved the governability and livability of the 
city  of  New  York  through  a  vigorous  metrics  program 
(including,  for  example,  measuring  the  number  of  guerrilla 
window washers accosting commuters).  
“Manage better” may include the following: 
•  Answering  basic  questions  about  operational 
practice,  such  as  “What  are  we  doing  and  not 
doing?”  
•  Spotting  trends  –  both  positive  and  negative  –  in 
operational practice, such as “Are we doing better or 
worse since the last measurement period?” 
•  Serve to raise additional questions (e.g., “Why is X 
•  Comparing entities (e.g., “How is group X doing vs. 
group Y?”), so long as the comparisons are “apples 
to apples” 
happening?”) 
This paper explores the qualities of good security metrics and 
their application in security vulnerability handling as well as a 
software assurance program.   
Keywords-security  metrics,  vulnerability  handling,  software 
assurance 
I.  WHY SECURITY METRICS? 
There are many management fads that wax and wane in 
terms  of  the  rate  of  adoption  and  the  number  of  breathless 
presentations, seminars, and books on the topic. One of the 
current  waves  sweeping  the  information  technology  (IT) 
security  world  is  metrics:  the  idea  that  security  can  be  and 
should  be  measurable.  In  a  way,  this  is  not  an  unusual 
development. Businesses in general rely on measurement as 
a key management indicator. For example, what are audited 
financial  statements  but  measurement  of  business  activity, 
and  what  are  ratios  such  as  earnings  per  share  (EPS),  net 
present value (NPV) and internal rate of return (IRR) but key 
comparison  metrics?    Information  technology  is  a  business 
enabler  and  should  be  subject  to  management  oversight 
(including  reasonable and appropriate  measurement) just as 
other lines of business are.  
1063-9527/09 $26.00 © 2009 IEEE
DOI 10.1109/ACSAC.2009.59
187
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:06:29 UTC from IEEE Xplore.  Restrictions apply. 
•  Allocating  resources  well  (e.g.,  “The  volume  of  X 
has increased by Z, and we’ve historically needed Y 
resource to handle the increment”) 
Metrics can be either external or internal (e.g., as noted, 
audited financial statements and the financial analysis ratios 
derived from them are a form of external metrics). They need 
not be different; however, metrics used for external purposes 
may  likely  be  focused  on  looking  good  externally  while 
those  used 
internally  can  more  objectively  focus  on 
“managing  better.”  Putting  it  differently,  internal  metrics 
provide more room for improvement if the numbers are not 
“good.”    That’s  the  point,  actually.    If  metrics  do  not 
ultimately  help  an  organization  be  honest  with  itself,  they 
might as well not exist. 
II.  FOLLOW THE YELLOW BRICK ROAD 
Metrics  do  not  grow  on  trees:  thus,  implementing  a 
metrics program requires resourcing just as any other project 
does,  with  time,  money,  and  people  to  support  the  desired 
outcome.  To that effect – given that most organizations do 
not have unlimited time, money or people – it is important to 
consider the value of more information or better information 
as  captured  in  a  metrics  program.  That  is,  there  may  be  a 
number  of  things  you  would  like  to  measure.  In  a  perfect 
world,  you  could  measure  how  many  angels  dance  on  the 
head of  a pin  if that  were  of interest.  However,  unless  you 
are already capturing the information you’d like to measure, 
it will cost you something to obtain the data or measurement.  
The  cost  to  obtain  the  metric  (such  as  investment  in 
changing  processes  or  creating  repositories  to  capture 
information) needs to be weighed against the value of more 
information. If capturing and analyzing data will not lead to 
your ability to make a better decision or manage better, you 
are  better  off  not  capturing  the  information  and  using  the 
time and resource to obtain another metric that will lead to 
better decision-making. This – being explicitly cognizant of 
the  value  of  more  information  –  is  the  difference  between 
academic  discussions  of  process-  and  artifact-focused 
metrics, and what resource-constrained entities do to manage 
better.  
(such  as  help  desk 
A  good  starting  place  for  a  metrics  program  is  simply 
date  mining  what  you  already  have.  For  example,  many 
organizations  have  processes  and  organizations  associated 
with  the  IT  function  that  track  many  aspects  of  their  IT 
systems 
tickets,  configuration 
management  actions,  and  the  like).  Such  data  stores  may 
already contain information relevant to security that can be 
incorporated into a metrics program. Adding to that baseline 
information  by  creating  a  metrics  plan  –  a  priority-ordered 
listing of “What else would we like to know?,” “How can we 
obtain that information?” and “How will we use this data?” 
is key to creating an implementable metrics program. 
For  example,  at  Oracle,  the  security  metrics  program 
associated with software assurance focuses in part upon the 
security  vulnerability  handling  function  (there  are  also 
metrics associated with program management of assurance). 
The foundation of the metrics program  was the Oracle bug 
database,  since  the  development  organization  already  had 
processes and data stores for logging, tracking, and resolving 
bugs  of  all  kinds.  Furthermore,  security  bugs  were  already 
flagged  separately,  because  Oracle  vulnerability  handling 
processes  mandate  that  security  bugs  are  not  published  to 
customers  and  also  that  “need  to  know”  is  enforced  on 
security  bugs,  as  implemented  by  row-level  data  access  in 
the  bug  database  (so  that  only  developers  working  on  a 
security  bug  fix  and  their  management  chain  can  access 
details  about  a  security  bug).  The  reason  security  bugs  are 
not published is that Oracle believes it puts customers at risk 
to  publish  security  vulnerability  details  where  there  is  no 
patch available.  
The  foundation  of  the  security  metrics  wiki  –  the 
internally  accessible  metrics  page 
to  analyze  security 
vulnerability trends – is the Oracle bug database (since to do 
otherwise  would  be  to  create  an  entirely  separate  and 
possibly  less  up-to-date  data  store).  Also,  the  focus  on 
security vulnerabilities is pragmatic: over time, as part of the 
Oracle software security assurance program, one expects the 
rate of serious security defects in software to go down.  Also, 
the  rate  of  security  vulnerabilities  goes  directly  to  Oracle 
Corporation’s security brand, and represents a  cost to Oracle 
and  to  its  customers.  Specifically,  Oracle  has  so  many 
products,  running  on  so  many  operating  systems  (e.g.,  the 
Oracle database runs on 19 operating systems) and so many 
versions  in  support  that  almost  anything  you  can  do  to 
prevent security vulnerabilities - or find and fix them faster - 
represents significant cost avoidance. 
Even in the short run, it is important to ensure that: 
• 
security  vulnerabilities  are  handled  reasonably 
promptly (that is, the percentage of vulnerabilities by 
aging bucket is not skewing to older buckets) 
the backlog of unfixed security vulnerabilities is not 
growing (that is, worst case, there is a steady state of 
find-and-fix) or is shrinking 
the  bulk  of  security  vulnerabilities  are  found 
internally  and  not  by  customers  or  third  party 
security researchers 
• 
• 
Another reason to focus metrics on vulnerabilities is that 
development already has many metrics around development 
process  and  practice  (including  tracking  open  bugs  in 
software). The primary goal of creating the security metrics 
wiki is to give development the tools it needs to manage their 
own workload. Development at Oracle “owns” its own code 
and  in  general  does  “take  care  of  business.”  Highlighting 
security-specific bug trends of interest and exposing those to 
development  is  more  likely  to  result  in  a  positive  outcome 
than  producing  metrics  that  are  only  used  by  the  software 
assurance  team  (that  does  not  own  code  or  fix  defects). 
Putting it differently, a self-policing model where developers 
have the tools to manage their own  work is preferable to a 
“security police” model in which a small group uses metrics 
offensively, to punish development infractions. 
188
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:06:29 UTC from IEEE Xplore.  Restrictions apply. 
III.  ACCENTUATE THE POSITIVE 
A  general  difficulty  in  a  metrics  program  is  creating 
measurement that is not only useful but creates incentives for 
the correct behavior. “Correct behavior” can be demonstrated 
by a positive trend (e.g., fewer security defects in software) 
or can be as simple as a “hygiene metric,” such as how many 
people  have  completed  a  mandatory  security  training  class 
and passed it. Note: the metric is “hygienic” in that it cannot 
necessarily be proved that there is a cause and effect between 
“do  X  and  Y  results,”  but  it  is  nonetheless  considered  a 
positive achievement towards a program objective. Putting it 
differently,  it’s  not  clear  how  strongly  correlated  driver’s 
education  training  and  a  reduction  in  accidents  involving 
teenage drivers are, but few people would turn a fifteen-year-
old loose with the car keys without basic driver’s education. 
A  subtlety  of  “incentivizes  the  correct  behavior”  is 
making sure that the metrics are a fair measurement and not 
easily  “gamed”  by  the  participants  (which  may  have 
unintended and often perverse side effects). That is, a poorly 
formed  metric  may  create  a  circumstance  in  which  those 
being measured change their behavior to optimize the metric 
instead of optimizing behavior whose natural outcome leads 
to  an 
improved  metric.  For  example,  one  Oracle 
development  organization  many  years  ago  ran  bug  reports 
for  senior  management  every  Friday  at  noon  (that  reported 
the number of bugs still open in the product suite). Product 
managers  would  log  into  the  bug  database  Friday  morning 
and change the status of a number of bugs to “closed” or “not 
a bug,” wait until the reports were run, then reopen the bugs 
later to continue working on them. It was the easiest way to 
make the numbers look good at noon on Friday. 
Using  a  “point  in  time”  metric,  instead  of  either 
analyzing  the  overall  trend,  or  triangulating  several  data 
points, resulted in “gaming” behavior. In addition to the fact 
that  the  gaming  activity  made  the  metric  unusable  (the 