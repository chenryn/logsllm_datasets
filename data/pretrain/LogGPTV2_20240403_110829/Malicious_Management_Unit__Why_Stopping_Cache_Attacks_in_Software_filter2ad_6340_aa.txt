title:Malicious Management Unit: Why Stopping Cache Attacks in Software
is Harder Than You Think
author:Stephan van Schaik and
Cristiano Giuffrida and
Herbert Bos and
Kaveh Razavi
Malicious Management Unit: Why Stopping Cache 
Attacks in Software is Harder Than You Think
Stephan van Schaik, Cristiano Giuffrida, Herbert Bos,  
and Kaveh Razavi, Vrije Universiteit Amsterdam
https://www.usenix.org/conference/usenixsecurity18/presentation/van-schaik
This paper is included in the Proceedings of the 
27th USENIX Security Symposium.
August 15–17, 2018 • Baltimore, MD, USA
ISBN 978-1-939133-04-5
Open access to the Proceedings of the 27th USENIX Security Symposium is sponsored by USENIX.Malicious Management Unit: Why Stopping Cache Attacks in Software is
Harder Than You Think
Stephan van Schaik
Vrije Universiteit
Amsterdam
Cristiano Giuffrida
Vrije Universiteit
Amsterdam
Herbert Bos
Vrije Universiteit
Amsterdam
Kaveh Razavi
Vrije Universiteit
Amsterdam
Abstract
Cache attacks have increasingly gained momentum in the
security community. In such attacks, attacker-controlled
code sharing the cache with a designated victim can
leak conﬁdential data by timing the execution of cache-
accessing operations. Much recent work has focused on
defenses that enforce cache access isolation between mu-
tually distrusting software components. In such a land-
scape, many software-based defenses have been pop-
ularized, given their appealing portability and scala-
bility guarantees. All such defenses prevent attacker-
controlled CPU instructions from accessing a cache par-
tition dedicated to a different security domain.
In this paper, we present a new class of attacks (in-
direct cache attacks), which can bypass all the existing
software-based defenses. In such attacks, rather than ac-
cessing the cache directly, attacker-controlled code lures
an external, trusted component into indirectly accessing
the cache partition of the victim and mount a confused-
deputy side-channel attack. To demonstrate the viability
of these attacks, we focus on the MMU, demonstrating
that indirect cache attacks based on translation opera-
tions performed by the MMU are practical and can be
used to bypass all the existing software-based defenses.
Our results show that the isolation enforced by exist-
ing defense techniques is imperfect and that generaliz-
ing such techniques to mitigate arbitrary cache attacks is
much more challenging than previously assumed.
1
Introduction
Cache attacks are increasingly being used to leak sensi-
tive information from a victim software component (e.g.,
process) running on commodity CPUs [8, 11, 12, 15, 19,
21, 22, 26, 29, 31, 32, 33, 42]. These attacks learn about
the secret operations of a victim component by observing
changes in the state of various CPU caches. Since such
attacks exploit fundamental hardware properties (i.e.,
caching), commodity software operating on security-
sensitive data is inherently vulnerable. Constant-time
software implementations are an exception, but gener-
ating them manually is error-prone and automated ap-
proaches incur impractical performance costs [34].
In
response to these attacks, state-of-the-art defenses use
software- or hardware-enforced mechanisms to partition
CPU caches between mutually distrusting components.
Given the lack of dedicated hardware support for the
mitigation of cache attacks, current hardware-enforced
mechanisms re-purpose other CPU features, originally
intended for different applications, to partition the shared
caches. For example, Intel CAT, originally designed
to enforce quality-of-service between virtual machines
[18], can be re-purposed to coarsely partition the shared
last level cache [30]. As an another example, Intel
TSX, originally designed to support hardware transac-
tional memory, can be re-purposed to pin the working
set of a secure transaction inside the cache. By probing
the cache partitions used by protected software running
in a transaction, attackers will cause transaction aborts
that can signal an on-going attack. While effective, these
defenses rely on features available only on speciﬁc (re-
cent Intel) architectures and, due to their limited original
scope, cannot alone scale to provide whole-system pro-
tection against cache attacks. For instance, Intel CAT-
based defenses can only support limited security parti-
tions or secure pages. In another direction, Intel TSX-
based defenses can only protect a limited working set.
In comparison, software-based cache defenses do not
suffer from these limitations and in recent years have be-
come increasingly popular. Given the knowledge of how
memory is mapped to the CPU caches, these defenses
can freely allocate memory in a way that partitions the
cache to isolate untrusted software components from one
another. This can be done at a ﬁne granularity to guaran-
tee scalability [25, 44], while remaining portable across
different architectures. The main question with these de-
fenses, however, is whether they perform this partition-
USENIX Association
27th USENIX Security Symposium    937
ing sufﬁciently well without hardware support.
The answer is no.
In this paper we present a new
class of attacks, indirect cache attacks, which demon-
strate that an attacker can mount practical cache at-
tacks by piggybacking on external, trusted components,
for instance on existing hardware components. Re-
cent side-channel attacks have already targeted hardware
components as victims, for instance by side channeling
CPU cores [21, 31, 33, 42], memory management units
(MMU) [12], transactions [8, 22], or speculative execu-
tion functionality [26, 29].
Unlike such attacks, indirect cache attacks abuse hard-
ware components as confused deputies to access the
cache on the attacker’s behalf and leak information from
victim software components. We show this strategy by-
passes the imperfect partitioning of all state-of-the-art
software-based defenses, which implicitly assume hard-
ware components other than the CPU are trusted.
To substantiate our claims, we focus on MMU-based
indirect cache attacks and show how such attacks can
bypass existing software-based defenses in practical set-
tings. Our focus on the MMU is motivated by (i) the
MMU being part of the standard hardware equipment on
commodity platforms exposed to side-channel attacks,
and (ii) the activity of the MMU being strongly depen-
dent on the operations performed by the CPU, making it
an appealing target for practical indirect cache attacks.
In detail, we show how our concrete attack implemen-
tation, named XLATE, can program the MMU to replace
the CPU as the active actor, mounting attacks such as
FLUSH + RELOAD and PRIME + PROBE. Performing
XLATE attacks is challenging due to the unknown inter-
nal architecture of the MMU, which we explore as part
of this paper. XLATE attacks show that the translation
structures (i.e., page tables) and any other data structures
used by other cache-enabled trusted hardware/software
components should be subject to the same partitioning
policy as regular code/data pages in existing and future
cache defenses. We show that retroﬁtting this property
in existing defenses is already challenging for XLATE
attacks, let alone for future, arbitrary indirect cache at-
tacks, which we anticipate can target a variety of other
trusted hardware/software components.
Summarizing, we make the following contributions:
• The reverse engineering of the internal architecture
of the MMU, including translation and page table
caches in a variety of CPU architectures.
• A new class of cache attacks, which we term indi-
rect cache attacks and instantiate for the ﬁrst time on
the MMU. Our XLATE attack implementation can
program the MMU to indirectly perform a variety
of existing cache attacks in practical settings.
• An evaluation of XLATE attacks, showing how they
compromise all known software-based cache de-
fenses, and an analysis of possible mitigations.
• An open-source test-bed for all the existing and
the
new cache attacks considered in this paper,
corresponding
implementations,
and applicable cache defenses, which can serve as
a framework to foster future research in the area.
The source code and further information about this
project can be found here:
covert-channel
https://vusec.net/projects/xlate
The remainder of the paper is organized as follows.
Section 3 provides background on existing cache at-
tacks, while Section 4 provides background on existing
cache defenses both in hardware and software. Section 5
and Section 6 present the design and implementation
of XLATE family of indirect cache attacks. Section 7
compares the XLATE attacks against existing attacks and
show that they break state-of-the-art software-based de-
fenses. Finally, Section 8 discusses possible mitigations
against these attacks, Section 9 covers related work, and
Section 10 concludes the paper.
2 Threat Model
We assume an attacker determined to mount a cache at-
tack such as PRIME + PROBE and leak information from
a co-located victim on the same platform. In practical
settings, the victim is typically a virtual machine in a
multi-tenant cloud or a user process in an unprivileged
code-based exploitation scenario. We also assume the
attacker shares hardware resources such as the last-level
cache (LLC) with the victim. Furthermore, we assume
the victim is protected with state-of-the-art software-
based defenses against cache attacks, either deployed
standalone or complementing existing hardware-based
solutions for scalability reasons. In such a setting, the
goal of the attacker is to escape from the containing se-
curity domain (cache partition) enforced by the software-
based defenses and mount a successful cache attack.
3 Cache Side-Channel Attacks
To overcome the performance gap between processors
and memory, multiple caches in the processor store
recently-accessed memory locations to hide the mem-
ory’s high latency. While these CPU caches are an im-
portant performance optimization deployed universally,
they can also be abused by attackers to leak information
from a victim process. Recently accessed memory lo-
cations by the victim process will be in the cache and
938    27th USENIX Security Symposium
USENIX Association
Table 1: An overview of existing cache side-channel attacks.
t
n
e
m
e
r
u
s
a
e
y
r
o
m
e
e
r
M
o
C
d
-
e
e
r
m
a
h
a
Name
M
S
S
time
EVICT + TIME [27]
  
PRIME + PROBE [21, 31]   
time
TSX
PRIME + ABORT [8]
  
FLUSH + RELOAD [42]
   time
FLUSH + FLUSH [16]
   time
e
r
o
C
-
s
s
o
r
C
attackers can probe for this information by observing the
state of the caches to leak sensitive information about
the secret operation of the victim process. This preva-
lent class of side-channel attacks is known as cache at-
tacks. We now brieﬂy explain the high-level architecture
of CPU caches before discussing how attackers can per-
form different variants of these cache attacks.
3.1 Cache Architecture
In the Intel Core architecture, there are three levels of
CPU caches. The caches closer to the CPU are smaller
and faster, and the caches further away are larger and
slower. At the ﬁrst level, there are two caches, L1i and
L1d, to store code and data respectively, while the L2
cache uniﬁes code and data. Where these caches are pri-
vate to each core, all cores share the L3 which is the
last-level cache (LLC). One important property of the
LLC is that it is inclusive of the lower level caches—
data stored in the lower levels is always present in the
LLC. Furthermore, because of its size, the LLC is always
set-associative, i.e., it is divided into multiple cache sets
where part of the physical address is used to index into
the corresponding cache set. These two properties are
important for state-of-the-art cache attacks on the LLC.
3.2 Existing Attacks
Table 1 illustrates existing cache attacks. Some of the
attacks only work if the attacker executes them on the
same core that also executes the victim, while others can
leak information across cores through the shared LLC.
Furthermore, to measure the state of the cache, these at-
tacks rely either on timing memory accesses to detect if
they are cached, or on other events such as transaction
aborts. We provide further detail about these attacks in
the remainder of this section.
EVICT + TIME In an EVICT + TIME attack, the at-
tacker evicts certain cache sets and then measures the ex-
ecution time of the victim’s code to determine whether
the victim used a memory location that maps to the
evicted cache sets. While EVICT + TIME attacks pro-
vide a lower bandwidth than PRIME + PROBE attacks
[33], they are effective in high-noise environments such
as JavaScript [12].
PRIME + PROBE and PRIME + ABORT In a
PRIME + PROBE attack, the attacker builds an eviction
set of memory addresses to ﬁll a speciﬁc cache set. By
repeatedly measuring the time it takes to reﬁll the cache
set, the attacker can monitor memory accesses to that
cache set. Furthermore, as part of the memory address
determines the cache set to which the address maps, the
attacker can infer information about the memory address
used to access the cache set. Thus, by monitoring differ-
ent cache sets, an attacker can determine, for example,
which part of a look-up table was used by a victim pro-
cess. While PRIME + PROBE originally targeted the L1
cache [33] to monitor accesses from the same processor
core or another hardware thread, the inclusive nature of
the LLC in modern Intel processors has led recent work
to target the LLC [21, 23, 31], enabling PRIME + PROBE
in cross-core and cross-VM setups.
PRIME + ABORT [8] is a variant of PRIME + PROBE