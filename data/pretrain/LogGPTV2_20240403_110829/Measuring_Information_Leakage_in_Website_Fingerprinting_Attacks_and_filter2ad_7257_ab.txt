ciﬁc classiﬁer, it is still a variant of the accuracy/error metric, and
therefore the ﬂaw of accuracy also applies to (ξ ,
Φ)–privacy.
Security Measurement by Information Theory. There are
two information leakage studies in website ﬁngerprinting attacks,
but they are limited from quantiﬁcation methodology and dataset
size, to feature set and analysis. Such limitations prevent them
from answering the question: how much information in total is
leaked from the website ﬁngerprint?
Chothia et al. [10] proposed a system called leakiEst to quan-
tify information leakage from an observable output about a sin-
gle secret value. Then leakiEst was applied on e-passports and Tor
traﬃc. However, leakiEst can only measure the information leak-
age of a single feature, rather than a category of features or all
features. In addition, it only included 10 visits for 500 websites in
their dataset, and it just considered 75 features. Furthermore, leaki-
Est cannot deal with information leakage under various scenarios,
such as open-world setting and setting with defenses.
Mather et al. [32] quantiﬁed the side-channel information leak-
age about the user’s inputs in the web applications. However, the
work shared many of the above limitations. The experiment only
considered packet size features; the size of the dataset was un-
known; the quantiﬁcation only came to a single feature. Though
the work included the multivariate extension, it didn’t apply it in
the experiment, and it didn’t have dimension reduction to handle
the curse of the dimensionality.
Source
Instances
Batches
Index Category Name [Adopted by]
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Packet Count [11, 22, 34, 36, 46]
Time Statistics [11, 22, 46]
Ngram [this paper]
Transposition [22, 46]
Interval-I [22, 46]
Interval-II [40]
Interval-III [36]
Packet Distribution [22]
Bursts [46]
First 20 Packets [46]
First 30 Packets [22]
Last 30 Packets [22]
Packet Count per Second [22]
CUMUL Features [34]
No.
13
24
124
604
600
602
586
225
11
20
2
2
126
104
Closed-World
Alexa 1-100
Monitor
[46]
Non-Monitor Alexa 1-2000
55779
17985
137455
Open-
World
20
8
10
Table 1: DATASET. We adopt Crawler [3] to collect the network
traﬃc in batches. This crawler uses Selenium to automate the Tor
Browser Bundle and applies Stem library to control the Tor pro-
cess. It extended the circuit renewal period to 600,000 minutes and
disabled UseEntryGuard to avoid using a ﬁxed set of entry guards.
We apply the method in [34] to extract the cell packets from the
traﬃc and measure the information leakage based on the cell pack-
ets.2
In comparison, our paper overcomes these limitations, and it
can measure information leakage from a category of features. Specif-
ically, it includes all 3043 features proposed in Tor WF literatures
and much larger dataset with 211219 visits. The resulting infor-
mation leakage quantiﬁcation is therefore more accurate and rep-
resentative. It’s able to quantify information leakage of a set of
features with high dimension, so that it can tell how much infor-
mation is leaked in total. More importantly, our paper not only
quantiﬁes the information leakage, but also reveals and analyzes
the discrepancy between accuracy and information leakage when
validating WF defenses. Our exprimental results demonstrate the
ﬂaw of accuracy.
Mutual Information Estimation. Kernel Density Estimate (KDE)
and k-nearest neighbors (k-NN) are two popular approaches to
estimate the mutual information between two random variables.
They are believed to outperform other estimators due to their abil-
ity to capture the nonlinear dependence where present [26]. In
this paper, we choose KDE instead of k-NN for the following rea-
sons. Firstly, k-NN tends to underestimate mutual information be-
tween strongly dependent variables [16, 41]. In security settings,
this means the measured information leakage would be less than
it should be, making k-NN unsuitable for information leakage mea-
surement. More importantly, we ﬁnd that the KSG estimator [28],
which is the most popular k-NN estimator, is unable to handle a
categorial random variable. This limitation matters because in Sec-
tion 5.1 we will see one of variables is about website information,
which is categorial. We further conﬁrm the above two reasons by
an experiment. We ﬁnd the total information leakage measured by
the KSG estimator is around 2 bits in a closed-world setting with
100 websites, much lower than what WF attacks have shown. Thus,
we choose the KDE approach in our paper.
4 TRAFFIC AND ITS FEATURES
A user’s traﬃc is a sequence of packets with timestamps which
are originated from or destinated to it. We use T (C) to denote the
traﬃc when the user visited the website C. Then
T (C) = h(t0 , l0), (t1 , l1), · · · , (tm , lm)i ,
(1)
where (ti , li ) corresponds to a packet of length |li | in bytes with a
timestamp ti in seconds. The sign of li indicates the direction of
2Our dataset allows the websites to have diﬀerent number of instances. This uneven
distribution is mostly caused by the failed visits in the crawling process. Note that it
doesn’t impact our information leakage measurement.
Table 2: Feature Set. 3043 features from 14 categories
the packet: a positive value denotes that it is originated from the
server, otherwise the user sent the packet. Table 1 describes our
collected traﬃc for information leakage measurement.
In the state-of-art website ﬁngerprinting attacks [22, 34, 46], it
is the features of the traﬃc rather than the traﬃc itself that an
attacker uses for deanonymization. One of the contribution of this
paper is that it measures a complete set of existing traﬃc features
in literatures of website ﬁngerprinting attacks in Tor [11, 22, 34,
36, 40, 46]. Table 2 summarizes these features by category. More
details about the feature set can be found in Appendix E.
5 SYSTEM DESIGN
5.1 Methodology
The features leak information about which website is visited. To-
tal packet count is a good example. Figure 2 shows that visiting
www.google.de creates 700 to 1000 packets, while browsing www.facebook.com
results in 1100 to 1600 packets. Suppose an attacker passively mon-
itors a Tor user’s traﬃc, and it knows that the user has visited one
of these two websites (closed-world assumption). By inspecting the
total packet count of the traﬃc, the attacker can tell which website
is visited.
Diﬀerent features may carry diﬀerent amounts of information.
Figure 2 displays the download time in visiting www.google.de
google.de
facebook.com
5
10
15
20
25
30
Transmission Time (s)
google.de
facebook.com
r
e
b
m
u
N
e
c
n
a
t
s
n
I
60
40
20
0
0
r
e
b
m
u
N
e
c
n
a
t
s
n
I
150
100
50
0
600
800
1000
1200
1400
1600
Total Packet Count
Figure 2: Diﬀerent features may carry diﬀerent amount
of information. Take transmission time and total packet count
as an example. This ﬁgure shows the latter carries more infor-
mation in telling which website is visited (www.google.de or
www.facebook.com)
adopts Adaptive Kernel Density Estimate (AKDE) [39], which out-
performs histogram in smoothness and continuity. AKDE is a non-
parametric method to estimate a random variable’s PDF. It uses
kernel functions—a non-negative function that integrates to one
and has mean zero—to approximate the shape of the distribution.
Choosing proper bandwidths is important for AKDE to make
an accurate estimate. WeFDE uses the plug-in estimator [43] for
continuous features, and in case of failure, WeFDE uses the rule-of-
thumb approach [43] as the alternative. If the feature is discrete, we
let the bandwidth be a very small constant (0.001 in this paper). The
choice of the small constant has no impact on the measurement, as
long as each website uses the same constant as the bandwidth.
To model the features’ PDFs in WF defenses, WeFDE has two
special properties. Firstly, our AKDE can handle a feature which is
partly continuous and partly discrete (or in other words, a mixture
of continuous and discrete random variables). Such features exist
in a WF defense such as BuFLO [11] which always sends at least T
seconds. These features would be discrete if the genuine traﬃc can
be completed within time T , otherwise, the features would be con-
tinuous. Secondly, our AKDE is able to distinguish a continuous-
like discrete feature. Take transmission time as an example. This
feature is used to be continuous, but when defenses such as Tama-
raw [6] are applied, the feature would become discrete. Our mod-
eler is able to recognize such features. For more details, please refer
to Appendix C.
We further extend WeFDE to model a set of features by adopt-
ing the multivariate form of AKDE. However, when applying mul-
tivariate AKDE to estimate a high dimensional PDF, we ﬁnd AKDE
inaccurate. The cause is the curse of dimensionality: as the dimen-
sion of the PDF increases, AKDE requires exponentially more ob-
servations for accurate estimate. Considering that the set of fea-
tures to be measured jointly could be large (3043 features in case
of total information measurement), we need dimension reduction
techniques. In the following, we introduce our Mutual Information
Analyzer to mitigate the curse of dimensionality.
5.4 Mutual Information Analyzer
The introduction of Mutual Information Analyzer is for mitigat-
ing the curse of dimensionality in multivariate AKDE. It helps the
Website Fingerprint Modeler to prune the features which share re-
dundant information with other features, and to cluster features
by dependency for separate modelling.
This Analyzer is based on the features’ pairwise mutual informa-
tion. To make the mutual information of any two features have the
same range, WeFDE normalizes it by Kvalseth’s method [29] (other
normalization approaches [45] may also work). Let NMImax(c , r )
denote the normalized mutual information between feature c and
r , then it equals to:
NMImax(c , r ) =
I (c; r )
max{H (c), H (r )}
Since I (c; r ) is less than or equal to H (c) and H (r ), NMImax(c , r ) is in
[0, 1]. A higher value of NMImax(c , r ) indicates higher dependence
between r and c, or in other words, they share more information
with each other.
Grouping By Dependency. A workaround from curse of dimen-
sionality in higher dimension is to adopt Naive Bayes method, which
Figure 3: WeFDE’s Architecture
and www.facebook.com. The former loads in about 3 to 20 sec-
onds, and the latter takes 5 to 20 seconds; Their distributions of
download time are not easily separable. As a result, the attacker
learns much less information from the download time than from
total packet count in the same closed-world scenario.
This raises question of how to quantify the information leakage
for diﬀerent features. We adopt mutual information [31], which
evaluates the amount of information about a random variable ob-
tained through another variable, which is deﬁned as:
DEFINITION. Let F be a random variable denoting the traﬃc’s
ﬁngerprint, and suppose W to be the website information, then
I (F ;W ) is the amount of information that an attacker can learn
from F about W , which equals to:
I (F ;W ) = H (W ) − H (W |F )
(2)
I (·) is mutual information, and H (·) is entropy. In the following, we
describe our system to measure this information leakage.
5.2 System Overview
Aimed at quantifying the information leakage of a feature or a
set of features, we design and develop our Website Fingerprint
Density Estimation, or WeFDE. Compared with existing systems
such as leakiEst [10], WeFDE is able to measure joint information
leakage for more than one feature, and it is particularly designed
for measuring the leakage from WF defenses, in which a feature
could be partly continuous and partly discrete.
Figure 3 shows the architecture of WeFDE. The information leak-
age quantiﬁcation begins with the Website Fingerprint Modeler,
which estimates the probability density functions of features. In
case of measuring joint information of features, Mutual Informa-
tion Analyzer is activated to help the Modeler to reﬁne its models
to mitigate the curse of dimensionality. During the information
leakage quantiﬁcation, the Website Fingerprint Modeler is used to
generate samples. By Monte Carlo approach [21] (see Appendix G
for more information), the Information Leakage Quantiﬁer derives
the ﬁnal information leakage by evaluating and averaging the sam-
ples’ leakage. In the following, we describe our modules.
5.3 Website Fingerprint Modeler
The task of Website Fingerprint Modeler is to model the probability
density function (PDF) of features. A popular approach is to use a
histogram. However, as the traﬃc features exhibit a great range of
variety, it’s hard to decide on the number of bins and width. WeFDE
assumes the set of features to be measured is conditionally inde-
pendent. Naive Bayes requires many fewer observations, thanks
to the features’ probability distribution separately estimated. How-
ever, we ﬁnd dependence between some features of the website
ﬁngerprint, violating the assumption of Naive Bayes.
We adopt Kononenko’s algorithm (KA) [8, 27], which clusters
the highly-dependent features into disjoint groups. In each group,
we model the joint PDF of its features by applying AKDE. Among
diﬀerent groups, conditional independence is assumed. KA takes
the advantage of how Naive Bayes mitigates the curse of dimen-
sionality, while keeping realistic assumptions about conditional in-
dependence between groups.
We use clustering algorithms to partition the features into dis-
joint groups. An ideal clustering algorithm is expected to guaran-
tee that any two features in the same group have dependence larger
than a threshold, and the dependence of the features in diﬀerent
groups is smaller than the same threshold. This threshold allows us
to adjust independence degree between any two groups. We ﬁnd
that DBSCAN [13] is able to do so.
DBSCAN is a density-based clustering algorithm. It assigns a
feature to a cluster if this feature’s distance from any feature of the
cluster is smaller than a threshold ϵ, otherwise the feature starts
a new cluster. Such a design enables DBSCAN to meet our goal
above. To measure features’ dependence, we calculate their nor-
malized pairwise mutual information matrix M; then to ﬁt in with
DBSCAN, we convert M into a distance matrix D by D = 1 − M,
where 1 is a matrix of ones. A feature would have distance 0 with
itself, and distance 1 to an independent feature. We can tune ϵ in
DBSCAN to adjust the degree of independence between groups.
We choose ϵ = 0.4 in the experiments based on its empirical per-
formance in the trade-oﬀ between its impact on information mea-
surement accuracy and KA’s eﬀectiveness in dimension reduction.
We model the PDF of the ﬁngerprint by assuming independence
between groups. Suppose KA partitions the ﬁngerprint ®f into k
groups, ®g1 , ®g2 , · · · , ®gk , with each feature belonging to one and only
one group. To evaluate the probability p(®f |cj ), we instead calcu-
late ˆp(®g1|cj ) ˆp(®g2 |cj ) · · · ˆp(®gk |cj ), where ˆp(·) is the PDF estimated
by AKDE.
As a hybrid of the AKDE and Naive Bayes, Kononenko’s algo-
rithm avoids the disadvantages of each. First, Kononenko’s algo-
rithm does not have the incorrect assumption that the ﬁngerprint
features are independent. It only assumes independence between
groups, as any two of them have mutual information below ϵ. Sec-
ond, Kononenko’s algorithm mitigates the curse of dimensional-
ity. The groups in Kononenko’s algorithm have much less features
than the total number of features.
Dimension Reduction. Besides the KA method to mitigate the
curse of dimensionality, we employ two other approaches to fur-
ther reduce the dimension.
The ﬁrst approach is to exclude features being represented by
other features. We use the pairwise mutual information to ﬁnd
pairs of features that have higher mutual information than a thresh-
old (0.9 in this paper). Then we prune the feature set by eliminating