Figure 9: This plot shows the results of a random search
in the reissue budget space (circles) and using SumVar on
the same workﬂow (line with crosses). It illustrates that
as sum of variances (x-axis, normalized) decreases, so
does the 99th percentile of latency (y-axis, normalized).
resources used for latency reduction. The formulation in
(3) can be generalized to accommodate multiple speedup
techniques (see §4.4). This optimization problem is the basis
for our algorithm.
We alternatively used a weighted version of the cost func-
tion, Ps wsVar(cid:0)Ls(rs)(cid:1), where the weights ws can be chosen
to incorporate global considerations. For example, we can
set ws based on (i) total number of paths crossing stage s, or
(ii) mean latency of s. However, our evaluation showed no
signiﬁcant advantages compared to the unweighted version.
We solve (3) by ﬁrst constructing per-stage models (or
variance-response curves), denoted Vs(rs) that estimate
Var(Ls(rs)) for each stage s. Due to the complexity of these
curves (see Fig. 5), we represent them as empirical functions,
and optimize (3) using an iterative approach based on gra-
dient descent; see details in §4. Due to the non-convexity
of (3), our algorithm has no performance guarantees. How-
ever, in practice, our algorithm already achieves adequate
precision when the number of iterations is O(N ), where N
is the number of stages in the workﬂow.
So far, we have considered “local” improvements, where
latency reduction policy inside each stage is independent of
the rest of the workﬂow. Our catch-up policies use the exe-
cution state of the entire request to make speed-up decisions.
These are described in more detail in §4.3.
Finally, as described earlier, burst losses in the network
are responsible for a signiﬁcant fraction of high latencies. We
recommend lowering RTO min to 10ms and using a burst-
avoidance technique such as ICTCP [26] at the application
level. While not a perfect solution, it addresses the current
problem and is applicable today.
4. DESIGN OF Kwiken
Here we provide details on applying our framework to the
diﬀerent techniques – reissues, incompleteness, and catch-up.
4.1 Adaptive Reissues
4.1.1 Per-stage reissue policies
Request reissue is a standard technique to reduce the
latency tail in distributed systems at the cost of increasing
resource utilization [10]. Reissuing a part of the workﬂow
(i.e., reissuing the work at one or more servers corresponding
to a stage) elsewhere in the cluster is feasible since services
are often replicated and can improve latency by using the
response that ﬁnishes ﬁrst.
A typical use of reissues is to start a second copy of the
request at time Ts, if there is no response before Ts, and use
the ﬁrst response that returns [10]. Given fs, the latency
distribution of stage s, and rs, the target fraction of requests
to reissue, the corresponding timeout Ts is equivalent to the
(1 − rs) quantile of fs. E.g., to reissue 5% of requests, we
set Ts to the 95th percentile of fs. We can thus obtain the
variance-response function Vs(rs) for diﬀerent values of rs,
by computing the corresponding Ts, and then performing
an oﬄine simulation using the latencies from past queries
at this stage. We use standard interpolation techniques to
compute the convex-hull, ¯Vs(rs), to preclude discretization
eﬀects. Note that we can compute the variance-response
function Vs(rs) for diﬀerent reissue policies and pick the best
one for each rs (e.g., launching two reissues instead of just
one after a timeout or reissuing certain fraction of requests
right away, i.e., timeout of zero). With ¯Vs(rs), we note that
our framework abstracts away the speciﬁcs of the per-stage
latency improvements from the the end-to-end optimization.
Further, ¯Vs(rs) needs to be computed only once per stage.
4.1.2 Apportioning budget across stages
Equipped with per-stage reissue policies captured in ¯Vs(rs),
we apportion budget across stages by solving (3) with ¯Vs(rs)
replacing Var(cid:0)Ls(rs)(cid:1) for every s.
Kwiken uses a greedy algorithm, SumVar, to solve (3) which
is inspired by gradient descent. SumVar starts from an empty
allocation (rs = 0 for every stage)3. In each iteration, SumVar
increases rs′ of one stage s′ by a small amount wherein the
stage s′ is chosen so that the decrease in (3) is maximal.
More formally, SumVar assigns resources of cost δ > 0 per
iteration (δ can be viewed as the step-size of the algorithm).
For each stage s, δ additional resources implies an increase in
rs by δ/cs (since csrs = resource cost) which reduces variance
by the amount ( ¯Vs(rs)− ¯Vs(rs +δ/cs)). Hence, SumVar assigns
δ resources to stage s′ ∈ argmaxs( ¯Vs(rs)− ¯Vs(rs +δ/cs)); ties
are broken arbitrarily. Returning to the example in Fig. 2,
SumVar would allocate budget to Stage 1 in the early steps,
since its variance decreases rapidly even with a small budget.
As the variance of Stage 1 ﬂattens out, subsequent steps
would allocate budget to Stage 2. The algorithm converges
to an allocation of 1:3 when all budget has been allocated
thereby minimizing the sum of the variances of the stages,
and in turn the 99th latency percentile.
We demonstrate our algorithm on a production workﬂow
with 28 stages and signiﬁcant sequential and parallel compo-
nents. First, we generate 1000 random reissue allocation and
for each, plot the achieved sum of variances and 99th per-
centile of end-to-end latency (circles in Fig. 9). Notice that
as sum of variances decreases, so does the latency percentile.
This illustrates that the intuition behind our approach is
correct; even in complex workﬂows, sum of variances is a
good metric to minimize in order to improve tail latencies.
Second, we plot the progress of our algorithm on the same
workﬂow (line with crosses in Fig. 9, from right to left). It
shows that the gradient descent approach can achieve lower
latency than a random search.
Our experiments show that it suﬃces to divide the budget
into γN chunks of equal size, where γ ∈ [2, 4] and N is the
number of stages. Consequently, the number of iterations
is linear in the number of stages. Each iteration requires
3In our experiments, we tried other initial allocations, but
they did not improve performance.
224latency distribution
Normal
mean=1, sd=10
LogNormal
meanlog=1, sdlog=2
LogNormal
meanlog=1, sdlog=2
Web
Image
Video
//’ism
(n)
utility
loss (r)
10000
10000
1000
1000s
100s
100s
.001
.01
.001
.01
.001
.01
.001
.01
.001
.01
.001
.01
Latency Reduction
(% over baseline)
99th
29.22%
47.67%
93.83%
98.96%
64.71%
96.12%
4.0%
77.7%
0%
81.2%
0%
51.3%
95th
25.33%
44.29%
90.34%
98.22%
59.93%
93.30%
4.1%
43.1%
0%
42.6%
0%
31.2%
Table 2: Given utility loss rate (r), the improvement in
latency from stopping when the ﬁrst ⌈n(1− r)⌉ responders
ﬁnish.
f
o
n
o
i
t
c
a
r
F
s
r
e
d
n
o
p
s
e
r
i
h
s
n
i
f
t
a
h
t
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
 50  100  150  200  250
Normalized Time 
 (each line is a query’s progress)
f
o
n
o
i
t
c
a
r
F
s
r
e
d
n
o
p
s
e
r
1
1 − (cid:3)
Ours
(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:9)
Latency
(a) Progress of a few sample queries
for the web workﬂow
(b)
strategies
Comparing
Figure 10: Trading oﬀ Incompleteness for Latency
O(log N ) for recomputing a single gradient component and
inserting it into a heap structure. Consequently, the overall
complexity of our algorithm is O(N log N ). Importantly, it
is independent of the topology of the DAG.
4.2 Trading off Completeness for Latency
In many situations, partial answers are useful both to the
user and to subsequent stages in the workﬂow. An example is
a workﬂow which picks the best results from many responders.
Similar to the web-search stage shown in Fig. 1, the image-,
video- and ad- search stages at Bing consist of many servers
that in parallel compute the best matching images, videos,
and ads for a search phrase. In each, an aggregator picks
the best few responses overall. Not waiting until the last
server responds will speed up the workﬂow while returning
an incomplete response.
How to measure the usefulness of an incomplete answer?
Some stages have explicit indicators; e.g., each returned
document in the web-search workﬂow has a relevance score.
For such stages, we say that the answer has utility 1 if
it contains the highest ranked document and 0 otherwise
(alternatively, we can give weights to each of the top ten
results). For other stages, we let utility be the fraction of
components that have responded4. We deﬁne the utility loss
of a set of queries as the average loss of utility across all the
queries. So, if 1 out of 1000 queries loses the top ranked
document, the utility loss incurred is 0.1%.
Discussions with practitioners at Bing reveal that enor-
mous developer time goes into increasing answer relevance
by a few percentage points. While imprecise answers are ac-
ceptable elsewhere (e.g., in approximate data analytics), we
conclude that request-response workﬂows can only aﬀord a
4If responders are equally likely to yield the most relevant
result, these measures yield the same expected loss.
small amount of utility loss to improve latency. Hence Kwiken
works within a strict utility loss rate of .001 (or 0.1%).
Our goal is to minimize response latency given a constraint
on utility loss. To be able to use (in)completeness as a
tool in our optimization framework, we treat utility loss as
a “resource” with budget constraint (average quality loss)
and decide how to apportion that resource across stages
and across queries within a stage so as to minimize overall
latency variance. We emphasize that this formulation is
consistent with Bing; both reduction in latency and higher
quality answers improve user satisfaction and can be used
interchangeably in optimization.
4.2.1 Using incompleteness within a stage
The basic setup here is of a single stage, with a constraint
on the maximal (expected) quality loss rate, denoted r. Con-
sider a simple strategy: Let each query run until ⌈n(1 − r)⌉
of its n responders return their answer. Then if the best doc-
ument’s location is uniform across responders, the expected
utility loss is r. To appreciate why it reduces latency, con-
sider a stage with n responders whose latencies are X1 . . . Xn,
this strategy lowers query latency to the ⌈n(1 − r)⌉’th largest
value in X1, . . . Xn as opposed to the maximum Xi.
Table 2 shows the latency reductions for a few synthetic
distributions and for the web, image, and video search stages
where we replay the execution traces from tens of thousands
of production queries at Bing. First, we note that even
small amounts of incompleteness yield disproportionately
large beneﬁts. For a normal distribution with mean 1 and
stdev 10, we see that the 95th percentile latency reduces by
about 25% if only 0.1% of the responses are allowed to be
incomplete. This is because the slowest responder(s) can
be much slower than the others. Second, all other things
equal, latency gains are higher when the underlying latency
distribution has more variance, or the degree of parallelism
within the stage is large. LogNormal, a particularly skewed
distribution, has 3.5X larger gains than Normal but only
2.2X larger when the number of parallel responders drops to
103. However, we ﬁnd that the gains are considerably smaller
for our empirical distributions. Partly, this is because these
distributions have bounded tails, since the user or the system
times-out queries after some time.
To understand why the simple strategy above does not help
in practice, Fig. 10 (a) plots the progress of example queries
from the web-search stage. Each line corresponds to a query
and shows the fraction of responders vs. elapsed time since
the query began. We see signiﬁcant variety in progress– some
queries have consistently quick or slow responders (vertical
lines on the left and right), others have a long tail (slopy top,
some unﬁnished at the right edge of graph) and still others
have a few quick responders but many slow ones (steps). To
decide which queries to terminate early (subject to overall
quality constraint), one has to therefore take into account
both the progress (in terms of responders that ﬁnished) and
the elapsed time of the individual query. For example, there
are no substantial latency gains from early termination of
queries with consistently quick responders, as even waiting
for the last responder may not impact tail latency. On the
other hand, a slow query may be worth terminating even
before the bulk of responders complete.
Building up on the above intuition, Kwiken employs dy-
namic control based on the progress of the query. Speciﬁcally,
Kwiken terminates a query when either of these two condi-
225tions hold: i) the query has been running for Td time after
p fraction of its components have responded, ii) the query
runs for longer than some cut-oﬀ time Tc. The former check
allows Kwiken to terminate a query based on its progress,
but not terminate too early. The latter check ensures that
the slowest queries will terminate at a ﬁxed time regard-
less of however many responders are pending at that time.
Fig. 10 (b) visually depicts when queries will terminate for
the various strategies.
Kwiken chooses these three parameters empirically based
on earlier execution traces. For a given r, Kwiken parameter
sweeps across the (Tc, Td, p) vectors that meet the quality
constraint with equality, and computes the variance of stage
latency. Then, Kwiken picks the triplet with the smallest
variance. Repeating this procedure for diﬀerent values of r
yields the variance-response curve V (r) (cf. §3). Note that
the approach for obtaining V (r) is data driven. In particular,
the choice of parameters will vary if the service software is
rewritten or the cluster hardware changes. From analyzing
data over an extended period of time, we see that parameter
choices are stable over periods of hours to days (we show
results with disjoint test and training sets in §5.3).
more threads to a request reduces its latency. (2) Use high-
priority network packets on network switches for lagging
requests to protect them from burst losses. And (3), reissue
requests more aggressively based on the total time spent in
the workﬂow – we call this global reissues to distinguish it
from local reissues (discussed in §4.1), where the reissue is
based on time spent within a stage.
Each of these techniques uses extra resources and could
aﬀect other requests if not constrained. To ensure catch-up
does not overload the system, Kwiken works within a catch-
up budget per workﬂow. Given per-stage budget, Kwiken
estimates a threshold execution latency, Tz, and speeds the
parts of a query that remain after Tz using the techniques
above. Since the decisions of a catch-up policy depend on re-
quest execution in previous stages, allocating catchup budget
across stages cannot be formulated as a separable optimiza-
tion problem, unlike the case of local techniques (§4.1.2).
We therefore use simple rules of thumb. For example, for
global reissues, we allocate catch-up budget proportionally
to the budget allocation for local reissues. Intuitively, a stage
that beneﬁts from local reissues, can also speed-up lagging
requests. We evaluate the catch-up policies in §5.4.
4.2.2 Composing incompleteness across stages
4.4 Putting it all together
Any stage that aggregates responses from parallel com-
ponents beneﬁts from trading oﬀ completeness for latency.
When a workﬂow has multiple such stages, we want to ap-
portion utility loss budget across them so as to minimize
the end-to-end latency. The approach is similar in large
part to the case of reissues – the variance-response curves
computed for each stage help split the overall optimization
to the stage-level.
Unlike reissue cost, which adds up in terms of compute
and other resources, utility loss is harder to compose, espe-
cially in general DAGs where partial results are still useful.
Modeling such scenarios fully is beyond the scope of this
paper. Nevertheless, we show two common scenarios below,
where the budget constraint can be written as a weighted
sum over the loss rates at individual stages. First, con-
sider a sequential workﬂow with N stages where an answer
is useful only if every stage executes completely.
If ri is
the loss budget of stage i, the overall utility loss is given
by r1 + (1 − r1)r2 + · · · + (cid:0) QN −1
s=1 (1 − rs)(cid:1)rN . which is
upper bounded by Pi ri. Hence, the budget constraint is
Pi ri ≤ B, i.e., the “cost” of loss cs is one at all stages.
Second, consider stages that are independent in the sense
that the usefulness of a stage’s answer does not depend
on any other stage (e.g., images and videos returned for a
query). Here, the overall utility loss can be written as P rsus
P us