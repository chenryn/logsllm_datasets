else:
time = time + tin - last_time;
pkts = pkts + 1;
last_time = tin;
result = groupby(R1, 5tuple, burst_stats)
She runs the query for 72 seconds and sees the result in Fig. 12.
She concludes, correctly, that UDP traffic between h3 and h4 is
responsible for the latency spikes. There are 18 UDP bursts, with an
average packet size and duration that matches our emulation setup.
 50 60 70 80 90 100161718192021Accuracy (%)log_2(Cache Slots)Core16Core14DC 50 60 70 80 90 100161718192021Accuracy (%)log_2(Cache Slots)1 min3 min5 min 0 0.5 1 1.5 2 0 10 20 30 40 50 60 70 80 90Latency (s)Time (s) 0 10 20 30 40 50 60 70 0 10 20 30 40 50 60 70 80 90Queue (# packets)Time (s)Language-Directed Hardware Design
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
sFlow [21]), not performance measurement. Packet sampling can
miss important events due to heavy undersampling [55]. Hardware
implementations of NetFlow do not keep a record of every flow,
since the flow lookup table cannot insert new flows at line rate in the
presence of hash collisions [46]. Marple solves exactly this problem
through its cache design and merging.
Line-rate packet capture devices, e.g., [10] record all packet traffic
at high data rates, providing valuable data for posthoc analyses of
network traffic. Ideally, performance monitoring should be possible
everywhere in a network, but the high data collection and storage
requirements make it impossible to run packet captures pervasively.
The same concern limits other strategies that mirror traffic or collect
packet digests from switches [8, 42, 65]. In comparison, Marple’s
flexible language and switch-based aggregation provide network-
wide performance monitoring at low data processing overhead, by
collecting only what is needed.
Sketches [34, 48, 49, 52, 63] and programmable switch coun-
ters [38, 54] expose traffic volume statistics using summary data
structures and flow counters on switches. Marple enables monitoring
performance statistics much broader than the flow-counter-based
statistics from these prior works (Fig. 7). Unlike sketches, which
trade off accuracy with memory, Marple implements counters with
full accuracy, since counting is a linear-in-state aggregation. Instead,
Marple trades off cache eviction rate with cache memory size.
In-band Network Telemetry (INT) [12, 44] exposes queue lengths
and other performance metadata from switches by piggybacking
them on the packet. Marple uses INT-like performance metadata, but
provides flexible aggregations directly on switches. Marple’s data
aggregation on switches provides three advantages relative to INT.
First, without aggregation, each INT endpoint needs to process per-
packet data at high data rates. Second, on-switch aggregation saves
the bandwidth needed to bring together per-packet data distributed
over many INT endpoints. Third, on-switch aggregation can handle
cases where INT packets are dropped before they reach endpoints.
Network query languages. Prior network query languages [35,
38, 41, 54] allow users to ask questions primarily about traffic vol-
umes and count statistics, since their input data is collected using
NetFlow and match-action rule counters [51]. In contrast, Marple
allows operators to ask richer performance questions by design-
ing new switch hardware to support Marple queries. Marple shares
some functional and relational constructs with Gigascope [35] and
Sonata [41], but supports aggregations directly in the switch. Marple
allows operators to program online queries over traffic, enabling the
collection of fine-grained customized statistics at low overhead. It is
complementary to offline query systems that answer post-facto ques-
tions over historical data collected by sampling or packet captures.
7 CONCLUSION
Performance monitoring is a crucial part of the upkeep of any large
system. Marple’s network visibility and query language demystify
network performance for applications, enabling operators to quickly
diagnose problems. We want network operators to query for what-
ever they need, and not be constrained by limited switch feature sets.
Marple presents a step forward in enabling fine-grained and pro-
grammable network monitoring, putting the network operator—and
not the switch vendor—in control of the network.
Figure 13: CDF of flowlet sizes for different flowlet thresholds.
Marple’s flexibility makes this diagnosis simple. By contrast,
localizing the root cause of microbursts with existing monitoring
approaches is challenging. Packet sampling and line-rate packet
captures would miss microbursts because of heavy undersampling,
packet counters from switches would have disguised the bursty
nature of the offending UDP traffic, and probing from endpoints
would not be able to localize queues with bursty contending flows.
Marple builds on In-band Network Telemetry [12, 26] (INT) that
exposes queue lengths at switches. However, Marple’s on-switch
aggregation goes beyond INT to determine contending flows at
the problematic queue through a customized query, without prior
knowledge of those flows. In comparison, a pure INT-based solution
may require a network operator to manually identify flows that could
contend at the problematic queue, and then collect data from the INT
endpoints for those flows.
5.4 Case study #2: Flowlet size distributions
We demonstrate another use case for Marple in practice: computing
the flowlet size distribution as a function of the flowlet threshold,
the time gap above which subsequent packets are considered to be
in different flowlets. This analysis has many practical uses, e.g.,
for configuring flowlet-based load balancing strategies [27, 60]. In
particular, the performance of LetFlow [60] depends heavily on the
distribution of flowlet sizes.
Our setup uses Mininet with a single switch connecting five hosts:
a single client and four servers. Flow sizes are drawn from an em-
pirical distribution computed from a trace of a real data center [28].
The switch runs the “flowlet size histogram” query from Fig. 7 for
six values of delta, the flowlet threshold.
Fig. 13 shows the CDF of flowlet sizes for various values of
delta. Note that the actual values of delta are a consequence of the
bandwidth allowed by the Mininet setup; a data center deployment
would likely use much lower delta values.
6 RELATED WORK
Endpoint-based monitoring. Owing to limited switch support
for measurement, many systems monitor network performance from
endpoints alone [16, 31, 53, 58, 62]. While endpoint solutions are
necessary for application context (e.g., socket calls), they are insuffi-
cient to debug all network problems, since endpoints lack visibility
into the network core. With switch-augmented endpoint solutions
such as INT, the data is scattered over multiple endpoints. We believe
networks will need both endpoint and switch-based systems because
each sees something the other cannot.
Switch-based monitoring. Most switch-based monitoring has fo-
cused on per-flow counts (e.g., NetFlow [7]) and sampling (e.g.,
 0 0.2 0.4 0.6 0.8 1110100100010000CDFFlowlet size (packets)delta = 10msdelta = 20msdelta = 50msdelta = 100msdelta = 500msSIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
S. Narayana et al.
Acknowledgments. This work was supported by NSF grants CNS-
1563826, CNS-1526791, and CNS-1617702, DARPA I2O Award No.
HR0011-15-2-0047, and a gift from the Cisco Research Center. We
thank Jennifer Rexford, Fadel Adib, Amy Ousterhout, our shepherd
Marco Canini, and the anonymous SIGCOMM reviewers for their
thoughtful feedback.
REFERENCES
[1] 45 nanometer - Wikipedia, Technology demos. https://en.wikipedia.org/wiki/45_
nanometer#Technology_demos.
[2] An Update on the Memcached/Redis Benchmark. http://oldblog.antirez.com/post/
update-on-memcached-redis-benchmark.html.
[3] Barefoot: The World’s Fastest
and Most Programmable Networks.
https://barefootnetworks.com/media/white_papers/Barefoot-Worlds-Fastest-
Most-Programmable-Networks.pdf.
[4] Benchmarking Apache Kafka: 2 Million Writes Per Second (On Three Cheap
Machines). https://engineering.linkedin.com/kafka/benchmarking-apache-kafka-
2-million-writes-second-three-cheap-machines.
[5] Broadcom First to Deliver 64 Ports of 100GE with Tomahawk II 6.4Tbps Ethernet
Switch. https://www.broadcom.com/news/product-releases/broadcom-first-to-
deliver-64-ports-of-100ge-with-tomahawk-ii-ethernet-switch.
[6] Cavium XPliant Switches and Microsoft Azure Networking Achieve SAI Routing
Interoperability. http://www.cavium.com/newsevents-Cavium-XPliant-Switches
-and-Microsoft-Azure-Networking-Achieve -SAI-Routing-Interoperability.html.
[7] Cisco IOS NetFlow. http://www.cisco.com/c/en/us/products/ios-nx-os-software/
ios-netflow/index.html.
[8] Configuring SPAN.
http://www.cisco.com/c/en/us/td/docs/switches/lan/
catalyst2940/software/release/12-1_19_ea1/configuration/guide/2940scg_1/
swspan.html.
[9] Data center flow telemetry. http://www.cisco.com/c/en/us/products/collateral/data-
center-analytics/tetration-analytics/white-paper-c11-737366.html.
[10] Gigamon.
https://www.gigamon.com/products/visibility-nodes/visibility-
[13] Intel FlexPipe. http://www.intel.com/content/dam/www/public/us/en/documents/
product-briefs/ethernet-switch-fm6000-series-brief.pdf.
[14] Intel64
and
IA-32 Architectures Optimization Reference Manual.
http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-
32-architectures-optimization-manual.pdf.
[15] Marple proofs. http://web.mit.edu/marple/marple_tr.pdf.
[16] Microsoft bets big on SDN. https://azure.microsoft.com/en-us/blog/microsoft-
showcases-software-defined-networking-innovation-at-sigcomm-v2/.
[17] Multiply-accumulate operation.
https://en.wikipedia.org/wiki/Multiply-
accumulate_operation.
prerelease-Dec_16.html.
[11] How Fast is Redis? http://redis.io/topics/benchmarks.
[12] In-band Network Telemetry. https://github.com/p4lang/p4factory/tree/master/
Centers. In NSDI, 2016.
appliances.html.
apps/int.
[33] P. Bosshart, G. Gibb, H.-S. Kim, G. Varghese, N. McKeown, M. Izzard, F. Mujica,
and M. Horowitz. Forwarding Metamorphosis: Fast Programmable Match-Action
Processing in Hardware for SDN. In SIGCOMM, 2013.
[34] G. Cormode and S. Muthukrishnan. An Improved Data Stream Summary: The
Count-Min Sketch and Its Applications. Journal of Algorithms, 2005.
[35] C. Cranor, T. Johnson, O. Spataschek, and V. Shkapenyuk. Gigascope: A Stream
Database for Network Applications. In SIGMOD, 2003.
[36] D. R. Ditzel and D. A. Patterson. Retrospective on High-level Language Computer
Architecture. In ISCA, 1980.
[37] M. Dobrescu, K. Argyraki, and S. Ratnasamy. Toward Predictable Performance in
Software Packet-processing Platforms. In NSDI, 2012.
[38] N. Foster, R. Harrison, M. J. Freedman, C. Monsanto, J. Rexford, A. Story, and
D. Walker. Frenetic: A Network Programming Language. In ICFP, 2011.
[39] G. Gibb, G. Varghese, M. Horowitz, and N. McKeown. Design Principles for
Packet Parsers. In ANCS, 2013.
[40] C. Guo, L. Yuan, D. Xiang, Y. Dang, R. Huang, D. Maltz, Z. Liu, V. Wang, B. Pang,
H. Chen, Z.-W. Lin, and V. Kurien. Pingmesh: A Large-Scale System for Data
Center Network Latency Measurement and Analysis. In SIGCOMM, 2015.
[41] A. Gupta, R. Birkner, M. Canini, N. Feamster, C. Mac-Stoker, and W. Willinger.
Network Monitoring is a Streaming Analytics Problem. In HOTNETS, 2016.
[42] N. Handigol, B. Heller, V. Jeyakumar, D. Mazières, and N. McKeown. I Know
What Your Packet Did Last Hop: Using Packet Histories to Troubleshoot Networks.
In NSDI, 2014.
[43] S. Hart, E. Frachtenberg, and M. Berezecki. Predicting Memcached Throughput
In Symposium on Theory of Modeling and
Using Simulation and Modeling.
Simulation, 2012.
[44] V. Jeyakumar, M. Alizadeh, Y. Geng, C. Kim, and D. Mazières. Millions of Little
Minions: Using Packets for Low Latency Network Programming and Visibility.
In SIGCOMM, 2014.
[45] S. P. Jones and P. Wadler. Comprehensive Comprehensions. In Proceedings of the
ACM SIGPLAN Workshop on Haskell Workshop, 2007.
[46] M. Kumar and K. Prasad. Auto-learning of MAC addresses and lexicographic
lookup of hardware database. US Patent App. 10/747,332.
[47] B. Lantz, B. Heller, and N. McKeown. A Network in a Laptop: Rapid Prototyping
for Software-defined Networks. In HotNets, 2010.
[48] Y. Li, R. Miao, C. Kim, and M. Yu. FlowRadar: A Better NetFlow for Data
[49] Z. Liu, A. Manousis, G. Vorsanger, V. Sekar, and V. Braverman. One Sketch
In
to Rule Them All: Rethinking Network Flow Monitoring with UnivMon.
SIGCOMM, 2016.
[50] W. M. McKeeman. Language directed computer design. In Proceedings of the
November 14-16, 1967, fall joint computer conference, 1967.
[51] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar, L. Peterson, J. Rexford,
S. Shenker, and J. Turner. OpenFlow: Enabling Innovation in Campus Networks.
SIGCOMM Comput. Commun. Rev., 38(2):69–74, Mar. 2008.
[52] M. Moshref, M. Yu, R. Govindan, and A. Vahdat. DREAM: Dynamic Resource
Allocation for Software-defined Measurement. In SIGCOMM, 2014.
[53] M. Moshref, M. Yu, R. Govindan, and A. Vahdat. Trumpet: Timely and Precise
Triggers in Data Centers. In SIGCOMM, 2016.
In NSDI, 2016.
rates.html.
[55] P. Phaal. SFlow sampling rates, 2016. http://blog.sflow.com/2009/06/sampling-
[56] A. Sivaraman, A. Cheung, M. Budiu, C. Kim, M. Alizadeh, H. Balakrishnan,
G. Varghese, N. McKeown, and S. Licking. Packet Transactions: High-Level
Programming for Line-Rate Switches. In SIGCOMM, 2016.
[57] A. Sivaraman, S. Subramanian, M. Alizadeh, S. Chole, S.-T. Chuang, A. Agrawal,
H. Balakrishnan, T. Edsall, S. Katti, and N. McKeown. Programmable Packet
Scheduling at Line Rate. In SIGCOMM, 2016.
[58] P. Tammana, R. Agarwal, and M. Lee. Simplifying Datacenter Network Debugging
[59] D. Ungar, R. Blau, P. Foley, D. Samples, and D. Patterson. Architecture of SOAR:
[60] E. Vanini, R. Pan, M. Alizadeh, P. Taheri, and T. Edsall. Let it Flow: Resilient
Asymmetric Load Balancing with Flowlet Switching. In NSDI, 2017.
[61] V. Vasudevan, A. Phanishayee, H. Shah, E. Krevat, D. G. Andersen, G. R. Ganger,
G. A. Gibson, and B. Mueller. Safe and Effective Fine-Grained TCP Retransmis-
sions for Datacenter Communication. In SIGCOMM, 2009.
[62] M. Yu, A. Greenberg, D. Maltz, J. Rexford, L. Yuan, S. Kandula, and C. Kim.
Profiling Network Performance for Multi-tier Data Center Applications. In NSDI,
2011.
[63] M. Yu, L. Jose, and R. Miao. Software Defined Traffic Measurement with OpenS-
ketch. In NSDI, 2013.
[64] M. Zaharia, T. Das, H. Li, T. Hunter, S. Shenker, and I. Stoica. Discretized Streams:
Fault-tolerant Streaming Computation at Scale. In SOSP, 2013.
[65] Y. Zhu, N. Kang, J. Cao, A. Greenberg, G. Lu, R. Mahajan, D. Maltz, L. Yuan,
M. Zhang, B. Y. Zhao, and H. Zheng. Packet-Level Telemetry in Large Datacenter
Networks. In SIGCOMM, 2015.
[18] P4-16 Language Specification. http://p4.org/wp-content/uploads/2016/12/P4_16-
[54] S. Narayana, M. Tahmasbi, J. Rexford, and D. Walker. Compiling Path Queries.
[25] XPliant™ Ethernet Switch Product Family. http://www.cavium.com/XPliant-
with PathDump. In OSDI, 2016.
[26] The Future of Network Monitoring with Barefoot Networks. https://youtu.be/
Smalltalk on a RISC. In ISCA, 1984.
[19] P4 Behavioral Model. https://github.com/p4lang/behavioral-model.
[20] Redis. http://redis.io/.
[21] sFlow. https://en.wikipedia.org/wiki/SFlow.
[22] SRAM - ARM. https://www.arm.com/products/physical-ip/embedded-memory-
ip/sram.php.
[23] The CAIDA UCSD Anonymized Internet Traces 2014 - June. http://www.caida.
org/data/passive/passive_2014_dataset.xml.
[24] The CAIDA UCSD Anonymized Internet Traces 2016 - April. http://www.caida.
org/data/passive/passive_2016_dataset.xml.
Ethernet-Switch-Product-Family.html.
Gbm7kDHXR-o, 2017.
[27] M. Alizadeh, T. Edsall, S. Dharmapurikar, R. Vaidyanathan, K. Chu, A. Fingerhut,
V. T. Lam, F. Matus, R. Pan, N. Yadav, and G. Varghese. CONGA: Distributed
Congestion-Aware Load Balancing for Datacenters. In SIGCOMM, 2014.
[28] Alizadeh, Mohammad. Empirical Traffic Generator. https://github.com/datacenter/
empirical-traffic-gen, 2017.
[29] J. R. Allen, K. Kennedy, C. Porterfield, and J. Warren. Conversion of Control
Dependence to Data Dependence. In POPL, 1983.
[30] A. Arasu, S. Babu, and J. Widom. The CQL Continuous Query Language: Seman-
tic Foundations and Query Execution. The VLDB Journal, 2006.
[31] B. Arzani, S. Ciraci, B. T. Loo, A. Schuster, and G. Outhred. Taking the Blame
Game out of Data Centers Operations with NetPoirot. In Proceedings of the 2016
Conference on ACM SIGCOMM 2016 Conference, SIGCOMM ’16, 2016.
[32] T. Benson, A. Akella, and D. A. Maltz. Network Traffic Characteristics of Data
Centers in the Wild. ACM International Measurement Conference, Nov. 2010.