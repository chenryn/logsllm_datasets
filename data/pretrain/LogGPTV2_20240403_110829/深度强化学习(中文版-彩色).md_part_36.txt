∇ logπ (a|s) ∇ logπ (a|s) g (5.50)
θ θ θ θ
−1
= x x⊤⊗(∇ L)(∇ L)⊤ g (5.51)
in in  xou t xout  
−1 −1
= x x⊤ ⊗ (∇ L)(∇ L)⊤ g (5.52)
in in xout xout
所以，与其对一个(d d )×(d d )的矩阵求逆，从而需要O(d3d3 )计算复杂度，ACKTR只
in out in out in out
需要对两个维度为d ×d 和d ×d 的矩阵求逆，从而计算复杂度只有O(d3 +d3 )。
in in out out in out
ACKTR算法的实现如算法5.25所示。ACKTR算法也可以被用于学习价值网络。感兴趣的
读者可以参考论文(Wuetal.,2017)了解更多的细节，我们这里不做详细解释。
算法5.25ACKTR
超参数: 步长η 、KL-散度上限δ。
max
输入: 空回放缓存D、初始策略函数参数θ 、初始价值函数参数ϕ
0 0
fork=0,1,2,···。do
在环境中执行策略π =π(θ )并保存轨迹集D ={τ |i=0,1,···}。
k k k i
计算累积奖励G 。
t
基于当前的价值函数V 计算优势函数Aˆ （基于任何优势函数的估计方法）。
ϕk t
估计策略梯度。
X XT 
gˆ = 1 ∇ logπ (A |S ) Aˆ (5.42)
k |D k| θ θ t t θk t
τ∈D kt=0
for l=0,1,2,··· do
vec(∆θl)=vec(A−1∇ S−1)
gˆ
k l θl k l
k
这里A = E[a aT], S = E[(∇ gˆ )(∇ gˆ )T]（A ,S 通过计算片段的滚动平均值所得），
l l l l sl k sl k l l
a 是第l层的输入激活向量，s =W a ，vec(·)是把矩阵变换成一维向量的向量化变换。
l l l l
endfor
由K-FAC近似自然梯度来更新策略：
θ =θ +η ∆θ (5.43)
k+1 k k k
q
这里η =min(η , 2δ )，Hˆl =A ⊗S 。
k max θ kTHˆ kθk k l l
采用Gauss-Newton二阶梯度下降方法（并使用K-FAC近似）最小化均方误差来学习价值函
数：
X XT  
1
ϕ =argmin V (S )−G 2 (5.44)
k+1 |D k|T ϕ t t
ϕ
τ∈D kt=0
endfor
161
第5章 策略梯度
5.10 策略梯度代码例子
在前几节中，我们在理论角度介绍了几个基于策略梯度算法的伪代码，介绍的内容包括RE-
INFORCE（初版策略梯度）、Actor-Critic（AC）、同步优势Actor-Critic（A2C）、异步优势Actor-Critic
（A3C）、信赖域策略优化（TRPO）、近端策略优化（PPO）、使用Kronecker因子化信赖域的Actor
Critic（ACKTR）。在本节中，我们将提供以上部分算法的Python代码例子。例子中以OpenAIGym
作为游戏环境。我们会先简单地介绍一下在例子中用到的环境，之后详细介绍各算法的实现。虽
然本章中介绍的多数算法都能应用于离散和连续的环境，但在实现中对于离散和连续环境的处理
有一些不同。这里我们提供的例子只是作为演示，只能应用在同一种动作空间的特定环境中。不
过读者可以通过简单地修改就能使代码应用于不同动作空间的其他环境中。完整代码在 GitHub
库中6，例子参考并改编自许多开源资料，感兴趣的读者可以参考各代码简介注释中Reference部
分所提及的内容进行扩展学习。
5.10.1 相关的Gym环境
在以下几节中提供例子的环境都基于OpenAIGym环境。这些环境可以被分为离散动作空间
的环境和连续动作空间的环境。
import gym
env = gym.make(’Pong-V0’)
print(env.action_space)
上述代码建立了一个ID为Pong-V0的环境，并且打印出了它的动作空间。将Pong-V0这个ID换
成其他诸如CartPole-V1或者Pendulum-V0的ID可以建立相应的环境。
以下几节中的代码会用到一些开源库。这里通过如下代码引入它们。
import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp
import tensorlayer as tl
...
离散动作空间环境：Pong与CartPole
这里将介绍两个OpenAIGym中使用离散动作空间的游戏：Pong和CartPole。
6链接见读者服务
162
5.10 策略梯度代码例子
Pong
在Pong游戏中（如图5.3所示），我们控制绿色的板子上下移动来弹球。这里使用了Pong-V0
版本。在这个版本中，状态空间是一个RGB图像向量，形状为(210,160,3)。需要输入的动作是
一个在0,1,2,3,4,5中的整数，分别对应如下动作：0空动作，1开火，2右，3左，4右+开火，5
左+开火。
图5.3 Pong
CartPole
CartPole（如图5.4所示）是一个经典的倒立摆环境。我们通过控制小车进行左右移动，来使
杆子保持直立。在CartPole-V0环境中，观测空间是一个4维向量，分别表示小车的速度、小车
的位置、杆子的角度、杆子顶端的速度。需要输入的动作是一个为0或者1的整数，分别控制小
车左移和右移。
图5.4 CartPole
连续动作空间环境：BipedalWalker-V2与Pendulum-V0
本节中，我们将介绍使用连续动作空间的环境：BipedalWalker-V2和Pendulum-V0。
163
第5章 策略梯度
BipedalWalker-V2
BipedalWalker-V2是一个双足机器人仿真环境（如图5.5所示）。在环境中，我们要控制机器
人在相对平坦的地面上行走，并最终到达目的地。其状态空间是一个24维向量，分别表示速度、
角度信息，以及前方视野情况（详见表5.1）。环境的动作空间是一个4维的连续动作空间，分别
控制机器人的2个膝关节、2个臀关节，一共4个关节进行旋转。
图5.5 BipedalWalker-V2
表5.1 BipedalWalker-V2各维度状态意义简介
索引 简介 索引 简介
0 壳体角度 8 1号腿触地状态
1 壳体角速度 9 2号臀关节角度
2 壳体x方向速度 10 2号臀关节速度
3 壳体y方向速度 11 2号膝关节角度
4 1号臀关节角度 12 2号膝关节速度
5 1号臀关节速度 13 2号腿触地状态
6 1号膝关节角度 14–23 10位前方雷达测距值
7 1号膝关节速度
Pendulum-V0
Pendulum-V0也是一个经典的倒立摆环境（如图5.6所示）。在环境中，我们需要控制杆子旋
转来让其直立。环境的状态空间是一个3维向量，分别代表cos(θ)、sin(θ)和∆(θ)。其中θ是杆
子和垂直向上方向的角度。环境的动作是一维的动作，来控制杆子的旋转力矩。
164
5.10 策略梯度代码例子
图5.6 Pendulum-V0
值得注意的是，该环境中没有终止状态。这里的意思是，必须人为设置游戏的结束。在默认
情况下，环境的最大运行步长被限制为200步。当运行超过200步时，step()函数返回的Done
变量将为True。由于有这个限制，当我们每个回合片段运行超过200步时，代码逻辑会因为收到
done信号而退出该回合。通过如下代码可以移除这个限制。
import gym
env = gym.make(’Pendulum-V0’)
env = env.unwrapped # 解除最大步长的限制
5.10.2 REINFORCE:AtariPong和CartPole-V0
Pong
开始之前，我们需要准备一下环境、模型、优化器，并初始化一些之后会用上的变量。
env = gym.make("Pong-V0") # 创建环境
observation = env.reset() # 重置环境
prev_x = None
running_reward = None
reward_sum = 0
episode_number = 0
# 准备收集数据
xs, ys, rs = [], [], []
epx, epy, epr = [], [], []
model = get_model([None, D]) # 创建模型
train_weights = model.trainable_weights
optimizer = tf.optimizers.RMSprop(lr=learning_rate, decay=decay_rate) # 创建优化器
165
第5章 策略梯度
model.train() # 设置模型为训练模式（防止模型被加上 DropOut）
start_time = time.time()
game_number = 0
在完成准备工作之后，就可以运行主循环了。首先，我们需要对观测数据进行预处理，并将
处理后的数据传递给变量x。在将x“喂”入网络之后，我们将从网络得到每个动作的执行概率。
为了简化难度，在这里只用到了3个动作：空动作、上、下。在REINFORCE算法中，使用
了Softmax函数输出动作概率，最后通过概率选择动作。
while True:
if render:
env.render()
cur_x = prepro(observation)
x = cur_x - prev_x if prev_x is not None else np.zeros(D, dtype=np.float32)
x = x.reshape(1, D)
prev_x = cur_x
_prob = model(x)
prob = tf.nn.softmax(_prob)
# 动作 1: 空动作 2: 上 3: 下
action = tl.rein.choice_action_by_probs(prob[0].numpy(), [1, 2, 3])
现在基于当前状态选出了一个动作。接下来要用该动作和环境进行交互。环境根据当前收到
的动作执行到下一步，并返回观测数据、奖励、结束状态和额外信息（对应代码中的变量_）。我
们将这些数据存储起来用于之后的更新。
observation, reward, done, _ = env.step(action)
reward_sum += reward
xs.append(x) # 一个片段内的所有观测数据
ys.append(action - 1) # 一个片段内的所有伪标签（由于动作从 1 开始，所以这里减 1）
rs.append(reward) # 一个片段内的所有奖励
如果 step() 返回的结束状态为 True，说明当前片段结束。我们可以重置环境并开始一个
166
5.10 策略梯度代码例子
新的片段。但在那之前，我们需要将刚刚采集的本片段的数据进行处理，之后存入跨片段数据列