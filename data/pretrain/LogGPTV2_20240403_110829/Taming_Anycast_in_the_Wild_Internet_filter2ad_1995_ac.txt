3.4 Performance Impacts
Figure 6 presents a CDF of the median change in RTT between the
transit only and all provider configurations, for both RIPE Atlas and
the CDN beacons over all groups. In this figure, and the remainder
of the section, we focus on the change in behavior of the median of
each group. The key notion is to observe the overall behavior of the
group, and how it changes as the result of different announcement
configurations, rather than the RTT distribution within each group.
This allows us to assess the impacts of changes at the group level,
across all groups, regardless of size of their internal characteris-
tics. We further consider the relative change observed in order
to understand the impacts to each group individually. Doing so
does not show the absolute changes. Instead, it provides a concise
description of how the RTT changed from the perspective of each
group. A positive value indicates that the RTT decreased in the
experimental configuration, i.e. the addition of announcements to
more networks presented alternative paths and performance im-
proved. A negative value indicates a reduction in performance. We
see that 60% of groups saw an improvement in performance with
the expanded announcement configuration, while nearly 40% of
groups saw a decrease, for both probes and beacons. This wide
spread suggests that care must be taken: indiscriminately adding
announcements can reduce performance.
020406080100120RTT Difference(ms)0.00.20.40.60.81.0CDF of groups−1.00−0.75−0.50−0.250.000.250.500.751.00Relative Change0.00.20.40.60.81.0CDF of GroupsRIPE AtlasBeaconsTaming Anycast in the Wild Internet
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
Figure 7: Groups which shifted catchment, and were there-
fore served by a geographically different location saw the
biggest change.
Figure 8: Groups which receive direct announcements, ver-
sus those with only indirect connections.
Catchments However, examining all of the groups at once does
not paint a complete picture. The interactions between upstream
providers and the anycast network may result in changes beyond
performance alone. To provide further insight into these interac-
tions, we refine this view. First, we examine the difference between
groups that changed catchment (i.e. the probes went to a different
site), and those that remained stationary (i.e. the probes connect to
the same site in configurations).
Figure 7 shows a CDF of the relative change in RTT for both the
moved (468) and stationary (477) groups. As expected, the majority
of differences, as well as the most extreme changes, come from
groups that change catchments. This follows the intuition: end-to-
end delay is often dominated by propagation delays. However, this
was not uniform, as the stationary groups saw some (generally less
than 25%) change in RTT. This shows us that while catchment is an
important consideration, as seen in [15], the path itself also impacts
RTT performance. These results further demonstrate that shorter
paths need not result in improved performance. While the lack of
such a relationship is well known, our findings here demonstrate
this empirically.
Inbound Paths Next, we consider an analysis of the inbound
paths taken by the probes in these measurements. Here, we map
our traceroutes into AS paths using a RIB from RouteViews taken
from the same day, revealing how each probe reached the CDN. For
simplicity, we ignore hops that were not responsive (potentially
underestimating AS path lengths). When comparing paths, we con-
sider a group to have taken a different path if any probes traversed
a different set of ASes. Here, we seek only a coarse grained sense of
what a different path means. In particular, we are largely concerned
only with the appearance or disappearance of providers, ignoring
many of the more subtle components, including AS boundaries.
When we examine the catchments that remained stationary, we
see that about 75% of these groups took a new AS path to the
anycast destinations. Further subdividing, we found that of the
groups that performed worse, only about 72% took a different path,
but for groups that improved, 77% of the groups took a new path.
These results suggest that taking a new path to the same destination
does not necessarily mean that performance will improve. These
findings reinforce the notion that there is no simple answer: new
providers may create new BGP paths, but they need not result in
better performance.
Figure 9: Of indirect-groups that got worse, 50% of the paths
became shorter.
Providers Figure 8 presents the groups separated by the nature
of the relationship with the probe’s network: the solid line indi-
cates direct peering with that network (304 groups) and the dashed
line indicates networks with no direct announcement (640 groups).
Surprisingly, for groups that performed better, the improvements
are approximately the same: in many cases, downstream networks
took advantage of the newly offered paths.
On the lower end of the distribution, nearly twice as many
indirectly-connected networks saw a decrease in performance. Fig-
ure 9 shows the change in AS hop count by the indirect-groups that
saw a performance decrease. Here, we take the median decrease for
each group. Fractional changes indicate a mixed-behavior group.
We see that 50% of these groups saw a decrease in path length.
This suggests that while their median RTT increased, the new an-
nouncements provided a shorter AS path via a different provider.
Previous work has found that peering links outperform their transit
counterparts in many cases [5]. Our findings here do not neces-
sarily contradict this, but serve as an indicator that managing the
inbound paths to a network is a different issue than overall or egress
performance.
Finally, to develop an understanding of the potential impact each
individual provider has, we consider an analysis of the number of
origin networks that we see using each inbound provider. Here, we
examine the traceroutes and the penultimate hop seen before they
arrive at the CDN network. We count the number of origin ASes
that traverse each unique adjacent AS. We note that this includes
regional providers, who provide transit to other networks, eyeball
−1.00−0.75−0.50−0.250.000.250.500.751.00Relative Change0.00.20.40.60.81.0CDF of GroupsStationaryMoved−1.00−0.75−0.50−0.250.000.250.500.751.00Relative Change0.00.20.40.60.81.0CDF of GroupsDirectIndirect−2.0−1.5−1.0−0.50.00.51.01.52.0Change in AS Hops0.00.20.40.60.81.0CDFIMC ’19, October 21–23, 2019, Amsterdam, Netherlands
Stephen McQuistin, Sree Priyanka Uppu, and Marcel Flores
allowing for actionable information about how and where catch-
ments and performance have changed between configurations. It
is typical that some groups of vantage points (here, RIPE Atlas
probes using the groupings described in Section 3.2) will see their
performance improve, while others will be degraded: DailyCatch
groups VPs together such that these results can be meaningful to
network operators.
The basic measurement component of DailyCatch is a snapshot. A
snapshot consists of a set of traceroutes taken from a large number
of vantage points towards the same anycast target at the same
time, along with the anycast catchments for each vantage point.
Snapshots contain information about groups of vantage points,
where groups contain vantage points that belong to the same AS
and geolocation. In Section 3.2, we demonstrated this grouping
function provided a good trade-off between coverage and similarity.
Section 4.1 describes the core contribution of DailyCatch: the
mechanism by which two snapshots are compared. Our scoring
methodology must be capable of surfacing changes in performance
that are significant and relevant to the network being measured.
To this end, we use information on traffic volumes and catchments
observed by the global, commercial CDN used in the previous sec-
tion. To do this, DailyCatch must scale changes in latency to ensure
that it is capturing the most meaningful differences, while allowing
scores to be compared. After describing the driving principles in
the development of DailyCatch, we demonstrate its utility through
a series of case studies in Section 4.2. While the inputs and config-
urations come from a CDN environment, many of these features,
and the ultimate observed behaviors, are generic and may apply to
many large, many-provider networks.
DailyCatch’s scoring mechanism enables operators to evaluate
the difference between two announcement configurations, in terms
of client performance. There are two main ways in which operators
can make use of DailyCatch’s output to improve and manage their
anycast announcement policies. First, DailyCatch produces a net
score (described in Section 4.1) that captures the broad performance
impact of a policy versus the control configuration. This net score
indicates whether or not a given configuration should be adopted
in its entirety. However, this is a fairly coarse measure: there may
be clients that are significantly impacted by the configuration, even
if those impacts are outweighed by improvements in performance
for more important networks ( i.e., those that have larger foot-
print). The second use of DailyCatch is more nuanced: operators
can inspect those vantage point groups whose scores shift most sig-
nificantly, and make targeted adjustments to their announcement
configurations. Section 4.2 describes how this approach works in
practice, giving an example configuration experiment, and guidance
on how operators can interpret the results. The path information
that DailyCatch captures is crucial in allowing operators to identify
the necessary configuration changes.
4.1 Scoring and Comparison
Snapshots capture the latency and catchment membership observed
by probe groups at a given point in time, towards a particular target.
However, the motivation for DailyCatch is to grant the visibility re-
quired to make evidence-based configuration changes. This means
that we need to be able to compare two snapshots, each describing
Figure 10: The number of ASes that arrived via each inbound
network.
networks, IXPs, and other networks. For simplicity, we discard
all traceroutes in which the penultimate hop was non-responsive,
making these numbers an underestimate.
Figure 10 presents the results. We consider a set of 172 observed
inbound providers, and exclude the transit-only providers. We see
that about 40% of networks provided connectivity to only them-
selves. This is expected given the nature of the CDN network: it has
a strong incentive to connect directly with eyeball providers, which
are often leaves in the AS graph. The remaining 60% of networks,
however, show the complexity of the situation: with many networks
offering connectivity for a large number of networks, there are po-
tentially significant and cascading impacts to their anycast behavior.
Any system for anycast management built around many-provider
relationships will have to account for this diversity, properly han-
dling the potential for announcing to both leaf-networks with no
dependencies, and transit providers of varying sizes.
The variety of paths taken, as well as the noted differences in
performance, indicate the complexity of the configuration space:
announcing to additional providers may increase route diversity,
add capacity, improve availability, and alter site selection. It may,
however, highly impact performance, both for direct neighbors,
and for more distant upstream networks. Indeed, our analysis has
demonstrated that there are significant gains to be had, with some
networks seeing a nearly 99% improvement on RTT. However, these
improvements are not uniformly realized: many networks saw a
significant decrease in performance. Therefore, while managing
anycast at the provider level can be extremely powerful, any pro-
posed anycast configuration must be thoroughly tested for such far
reaching impacts.
4 EVALUATING MANY-PROVIDER ANYCAST
ANNOUNCEMENTS
To properly manage many-provider anycast configurations, it is
necessary to empirically compare two configurations. We present
DailyCatch, a methodology that uses active measures to provide
such a primitive. DailyCatch conducts experiments by measuring
a control anycast configuration and an experiment configuration.
DailyCatch assesses the difference between these configurations,
generating a score (i.e., a numerical value distilling the latency
difference) for each sub-AS group. These groups provide context,
100101102Origin AS Count0.00.20.40.60.81.0CDFTaming Anycast in the Wild Internet
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
v values can be computed in a similar fashion for other services,
e.g. incoming queries for a DNS resolver.
We attribute a portion of the relative volume that an AS con-
tributes to the groups, д, that make up that AS. We define vд in
terms of the AS relative volume, v, the number of vantage points
in the AS, P, and the number of vantage points in д, pд, as:
Figure 11: Rationale for logistic function: variation from a
100% decrease to a 100% increase.
a particular configuration, and highlight the differences between
them. Central to this is a scoring function, allowing DailyCatch
to quantify the difference between two snapshots at a per-group
level. Our scoring function must surface meaningful and relevant
performance changes. We use a logistic function to ensure that the
score is meaningful, by dampening the impact of anomalous results.
Finally, we weight each group’s score by the relative importance
of that group to the CDN, in terms of how much client traffic it
represents.
To begin, we define the change in RTT within a group, д, between
two snapshots, a and b, as:
∆д = rtta − rttb
(5)
where rttx is the median RTT of vantage points in snapshot x. Me-
dians are used to remove the effects of anomalous measurements4.
rtta
We normalise the RTT change with a logistic function:
sraw(∆д) =
2
1 + e−k(∆д) .
(6)
The aim of the logistic function (Figure 11) is to ensure that we
capture meaningful changes in RTT. Selecting an appropriate k
value (i.e., the steepness of the logistic curve) limits the impact of
anomalous latency measurements. While we select k = 2, based
on the range of differences seen in Section 3, which devotes the
majority of the score to the space between a 100% decrease in
performance and a 100% increase. Other k values can be used for
more dramatic changes, in which more than 100% is needed. Ap-
plying the logistic function gives a value that varies from −1 (RTT
degraded significantly) to 1 (RTT improved significantly).
Next, we weight the score for each group by its relative volume, v.
The relative volume of an AS is the fraction of inbound traffic that
originates from that AS. In the context of the CDN, we calculate
relative volume by measuring the TCP SYN packets that are seen in
a 24 hour period, sampled every 1000th packet, using RouteViews
data to map IPs to AS numbers. Relative volume is a measure of
traffic towards the CDN, not of outbound traffic. Given the diversity
of platforms, content, and customers served by the measured CDN,
this is a reasonable measure of each network’s importance as a
source of client traffic. Relative volume is distinct from load, and
instead indicates the relative popularity of a given network. These
4We evaluated the impact of using averages and other similar summary statistics and
found no significant differences.
Dividing by the count of vantage points assumes that the distribu-
tion of vantage points across groups within an AS matches the dis-
tribution of relative volume across sites. We make this assumption
in the absence of visibility into the actual distribution of vantage
points over the sub-groups.
Next, we define our weighted score sw(∆д) for each group as:
(7)
These scores provide fine-grained, actionable data. Evaluating the
scores across all groups allows operators to determine the impacts
– both good and bad – of a configuration change.
sw(∆д) = sraw(∆д) · vд .
Finally, for snapshots a and b, we define the net score, S:
sw(∆д)
(8)
vд = v · pд
P
.
S =
д∈G
where G is the set of vantage point groups that is common to both
snapshots. As discussed, the net score provides a coarse measure
of the overall impact of the experimental configuration. If many
groups improve we expect positive scores, and if many degrade,
we expect negative scores. However, operationally, it may be more
beneficial to inspect group-level score shifts, and make targeted
configuration changes. We explore this approach in the next sec-
tion.
4.2 DailyCatch Measurements
Here, we discuss the insights that can be provided by DailyCatch.