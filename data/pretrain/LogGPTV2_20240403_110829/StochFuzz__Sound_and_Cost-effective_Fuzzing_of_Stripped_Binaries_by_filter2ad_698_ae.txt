(FLAG), general purpose register reuse (GPR), and removing
instrumentation for single successors, respectively. For each
optimization, we report both the number (of applying these
optimizations) and the percentage. In the last column, we
present the slow-down when the optimizations are disabled.
Overall, FLAG is most effective, removing 99% of cases.
Intuitively, the use of ﬂag registers has very strong locality.
We then conduct a study on the evaluated binaries and ﬁnd
that almost all ﬂag registers are deﬁned and used within the
last three instructions of basic blocks, with the most common
instruction pattern being a cmp or test instruction followed
by a conditional jump. As such, they are mostly dead at the
instrumentation points. GPR can be applied in 82.2% cases
on average. The observation is that many basic blocks start
with instructions that write to at least one general purpose
FLAG
#O %R
99.1
98.0
99.4
99.4
98.5
99.5
98.2
98.8
99.1
99.4
98.9
99.6
99.7
99.0
97.4
99.1
99.6
98.7
96.6
99.4
99.4
99.0
98.7
98.5
99.0
GPR
#O %R
84.0
84.7
84.0
77.1
83.7
81.7
78.5
82.2
88.4
77.2
80.6
79.6
77.8
85.4
86.7
84.1
80.3
85.1
72.8
76.8
84.7
77.0
82.7
82.6
82.2
4,294
83
11,422
8,230
8,679
1,886
3,341
5,862
2,609
2,173
3,578
10,786
3,314
13,595
2,036
5,856
4,878
5,863
1,443
5,140
20,541
2,539
1,990
22,765
6,371
Single-Succ
#O %R
43.53
48.98
45.08
49.74
41.06
48.74
40.23
35.60
46.12
40.96
40.89
40.83
51.00
44.17
40.95
42.66
39.30
47.79
49.62
50.53
46.63
41.70
49.50
42.06
44.49
2,225
48
6,126
5,312
4,256
1,125
1,712
2,540
1,362
1,153
1,816
5,531
2,171
7,028
961
1,970
2,387
3,292
984
3,382
11,314
1,375
1,191
11,587
3,410
5,068
96
13,508
10,621
10,208
2,296
4,181
7,046
2,927
2,797
4,393
13,487
4,244
15,750
2,285
6,902
6,048
6,798
1,915
6,655
24,128
3,263
2,374
27,146
7,672
5,112
98
13,590
10,680
10,365
2,308
4,256
7,134
2,953
2,815
4,441
13,546
4,257
15,912
2,347
6,964
6,074
6,889
1,983
6,693
24,264
3,297
2,406
27,549
7,747
boringssl
37.81
c-ares
-4.22
freetype2
1.86
guetzli
3.49
harfbuzz
28.39
json
30.48
lcms
-16.58
libarchive
33.25
libjpeg-turbo
35.48
libpng
18.07
libssh
30.43
libxml2
15.96
llvm-libcxxabi
28.77
openssl-1.0.1f
43.88
openssl-1.0.2d
64.24
openssl-1.1.0c
16.03
openthread
14.27
pcre2
45.86
proj4
3.58
re2
31.05
sqlite
38.87
vorbis
16.95
woff2
30.92
wpantund
2.10
Average
22.45
register. STOCHFUZZ hence is able to reuse the register in the
instrumented code (Section III-D). The average percentage of
instrumentation removal for blocks with a single successor is
44.49%, which is not that signiﬁcant but still helpful. The
slowdown is 22.45% on average when we disable these op-
timizations. The optimizations have negative effects on some
programs such as lcms. Further inspection seems to indicate
that
the optimizations cause some tricky complications in
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:54 UTC from IEEE Xplore.  Restrictions apply. 
11669
1.001.001.001.001.001.001.001.001.001.001.001.001.131.991.160.891.001.191.011.391.211.341.831.750.170.080.110.080.160.090.050.200.180.240.110.220.140.340.410.330.350.010.110.130.000.370.301.100.200.000.000.000.070.150.110.110.050.290.290.360.350.000.390.000.000.600.870.840.720.920.660.001.101.331.230.890.961.010.941.311.241.191.480.91llvm-libcxxabiopenssl-Aopenssl-Bopenssl-Copenthreadpcre2proj4re2sqlitevorbiswoff2wpantund1.001.001.001.001.001.001.001.001.001.001.001.002.030.951.031.340.841.141.021.091.361.170.780.940.160.180.120.200.080.130.020.240.170.200.040.000.170.100.000.000.120.280.180.000.140.310.390.330.000.470.060.450.060.380.100.190.180.230.130.040.000.860.002.800.000.950.000.760.630.480.500.001.450.880.901.280.841.080.850.991.491.280.880.97boringsslc-aresfreetype2guetzliharfbuzzjsonlcmslibarchivelibjpeg-turbolibpnglibsshlibxml2afl-gcc (124.1M)afl-clang-fast (138.1M)afl-qemu (16.0M)ptfuzzer (24.4M)e9patch (23.8M)ddisasm (98.7M)StochFuzz (129.3M)cache performance. It is worth pointing out that compiler
based fuzzers such as aﬂ-gcc and aﬂ-clang directly beneﬁt
from built-in compiler optimizations, some of which have
similar nature to ours. Dynamic instrumentation engines such
as QEMU and PIN have their own optimizations although
they typically reallocate all registers. Performing optimizations
during unsound static rewriting is very risky. In contrast,
optimizations work well in our context as STOCHFUZZ can
ﬁx disassembly and rewriting errors automatically.
B. Evaluation on Google FTS with Intential Data Inlining
Programs built by popular compilers (e.g., GCC and Clang)
with default settings may not contain (substantial) code and
data interleavings [10]. It is interesting to study the perfor-
mance of various tools when substantial
interleavings are
present. We hence modify the compilation tool-chain of
Google FTS to force .rodata sections to be interleaved
with .text sections. We extract the ground-truth of data byte
locations from the debugging information and then strip the
binaries. E9patch fails on 22 out of the 24 programs, due to
its assumption of no inlined data. It succeeds on two programs
because they do not contain static data sections. Ddisasm fails
on 21 programs. In contrast, STOCHFUZZ succeeds on all the
programs. Details can be found in Appendix X-E.
Fuzzing Efﬁciency. We run the tools for 24 hours on each
program. Fig. 13 (in Appendix) presents the number of fuzzing
executions by our tool and its ratio over aﬂ-gcc. We omit
the results for other tools as inlined data do not
impact
their efﬁciency in theory. The results show that STOCHFUZZ
still has comparable performance as aﬂ-gcc. Moreover, our
tool’s efﬁciency has a slight degradation compared to without
intentional data inlining (124.7M v/s 129.3M), due to the extra
time needed to ﬁx more rewriting errors.
Progress of Incremental and Stochastic Rewriting. We
study how the numbers of false positives (FPs) (i.e., a data
byte is replaced with hlt) and false negatives (FNs) (i.e., a
code byte is not replaced with hlt) change over the proce-
dure. Here, we use debugging information and the aggregated
coverage information (over 24-hour fuzzing) to extract the
ground-truth. In other words, we do not consider data bytes
that are not accessed in the 24 hours and code bytes that
are not covered in the 24 hours. Note that they have no
inﬂuence on the fuzzing results and hence rewriting errors
in them are irrelevant to our purpose. And as long as they
are covered/accessed, STOCHFUZZ can expose and repair their
rewriting errors. The results are presented in Table V. The sec-
ond column presents the number of instrumented basic blocks.
Columns 3-6 present the numbers of intentional crashes caused
by hlt (indicating discovery of new code), unintentional
crashes caused by rewriting errors, and unintentional crashes
caused by program bugs, and their sum, respectively. The
last four columns show the percentage of FN and FP at the
beginning and the end of fuzzing process. Observe that at
the beginning, with the initial probability analysis results,
STOCHFUZZ has 11.74% FNs and 1.48% FPs on average.
TABLE V: Incremental and Stochastic Rewriting. #IC, #UCE,
#UCB, and Sum denote the number of intentional crashes,
unintentional crashes caused by rewriting errors, unintentional
crashes caused by real bugs, and their sum, respectively. FN
and FP denote false negative and false positive, respectively.
“Begin” and “End” denote the beginning and end of fuzzing.
Program
#IC #UCE #UCB Sum
Crashes
Rewriting
Begin
End