131
19One of the first widely used and security aware commercial run-time analysis tools for detect-
ing security flaws from the code was called Purify, launched around 1991 by Pure Software, later
acquired by Rational Software. Today new emerging companies such as Coverity, Klocwork,
Ounce Labs, and Fortify appear to dominate the market with both off-line (static) and run-time
(dynamic) analysis tools.
20http://weblogs.mozillazine.org/roc/archives/2006/09/static_analysis_and_scary_head.html
21SAMATE: Software Assurance Metrics and Tool Evaluation. http://samate.nist.gov
22T.T. Rontti. (2004). Robustness Testing Code Coverage Analysis. Oulu, Finland: University of
Oulu, Department of Electrical and Information Engineering. Master’s Thesis, 52p.
fuzzing had much better test coverage than the OpenSSL conformance test suite, for
example, in testing the TLS interface. However, fuzzing did not test all the code that
conformance testing would test, nor was the overall coverage anywhere close to
100%. This is because fuzz tests typically cannot reach user interface or configura-
tion routines inside the tested software, for example. Also, while the focus of confor-
mance tests is in trying out all potential features, fuzz tests typically focus on fuzzing
around a particular subset of the capabilities of the tested software. Fuzzing is neg-
ative testing—i.e., it has better chances of exploring different error handling routines
compared to typical feature tests. But code coverage alone does not indicate the qual-
ity of the fuzzer. It can indicate poor fuzzing, if coverage is suspiciously low espe-
cially around the input parsing portions of the code. But even good code coverage
does not indicate that all accessible code blocks were tested with a large number of
mutations. This would require dynamic analysis of the system behavior.
It seems the most important discovery from Tero’s research is that code coverage
is an excellent metric for “attack surface,” i.e., when you perform a combination of
positive feature testing and negative fuzz testing against the system, you will discover
the lines of code that are security critical and accessible through that specific interface.
This does not guarantee that the entire attack surface was touched by the tests, but it
will indicate that at least that piece of code can be accessed by an attacker. While tra-
ditional QA did not touch large parts of code, adding fuzzing to the mix of testing
techniques increased the amount of total code touched during the process; further-
more, it hit the exact portions of code that will have to withstand remote attacks.
This metric for measured attack surface could be useful to white-box testers who have
the challenge of auditing millions of lines of code. Prioritizing the code auditing prac-
tices around the attack surface could help find those flaws that fuzzing might miss.
Any metric of what code can be exploited through different interfaces can prioritize
the focus of those analysis tools, and therefore improve the code auditing process sig-
nificantly. This is an indication of the real and measured attack surface.
Alvarez and Guruswamy23 note that discovering the attack surface prior to
fuzzing could be helpful. In fact, that leads to another important observation—when
doing interface testing we will never reach 100% code coverage. Even if we would,
we would not really prove the discovery of all vulnerabilities in the touched code. It
would be more accurate to say that 100% code coverage of the attack surface does
not equal zero vulnerabilities. The distinction is important because the attack surface
is typically only a fraction of the total code. Determining the total functions or basic
blocks that lie on an attack surface can be a challenge. Consider this example: If we
find that 5 out of 100 total functions read in command line arguments and 15 out
of 100 functions handle network traffic, we simply take the ratio:
• Local attack surface = 5/100 or 5% of total code
• Remote attack surface = 15/100 or 15% of total code
A remote fuzzing tool that hits 15 functions would be said to have hit 100% of
the remote attack surface. But did it hit all combinations of paths with all combi-
132
Fuzzing Metrics
23Personal communications and email discussion on the Dailydave list: http://fist.immunitysec.com/
pipermail/dailydave/2007-March/004220.html
nations of data? And did it really test all the subfunctions and libraries where that
tainted data was handled?
4.3.7
Process Metrics
Process efficiency needs to be monitored when deploying fuzzing into your devel-
opment processes or to a regularly conducted vulnerability process. The following
metrics might apply to your fuzzing deployment:
• Assessment frequency: Measures how often security quality assurance “gates”
are applied to the software development lifecycle.
• Assessment coverage: Measures which products are tested with fuzzers; i.e.,
for regression testing you can check that all releases will go through a sys-
tematic fuzzing process before release.
4.4
Test Automation for Security
How does fuzzing as part of test automation help reduce the total time used in test-
ing? Does it impact the timelines of developers? How does automation help with
repeatability? In test automation we need to be able to show that the same set of
tests, potentially augmented with some new tests, will be executed each time the
fuzzing process is launched through the test automation framework. Automation
has to be repeatable or it is worthless. Test automation does not necessarily reduce
human error if there is manual work involved in the repetition of the tests. One small
problem in the automation process could lead to missing bugs that would have been
covered earlier. This could result from, for example, a change in the communication
interface or the data formats being fuzzed. There are pros and cons to both prede-
fined tests and dynamically changing tests from the test automation perspective.
From a QA standpoint the goals of automation are
1. Increase the amount of tests you do in a set timeframe when compared to
manual testing;
2. Liberate your testers to do more of the interesting testing and avoid man-
ual repetition of simple tasks;
3. Cost savings related to both the test efficiency and the direct costs of tools
and test integration.
Increasing the number of tests you perform in an automated fashion will speed
up your testing process, just as doing the same tests with more manual intervention
would slow the process down. In fuzzing, it is not really a choice of doing it man-
ually or automatically, it is whether you do it at all. Fuzzing will liberate your
testers to do more interesting testing activities, which likely includes such things as
detailed analysis of the test results. Fuzzing requires repeatability and speeds up the
development process. Without people creating the tests, the tests have a chance of
having better coverage, since they do not depend on “human knowledge.” A good
security tester is always good at what he or she does. But unfortunately, most com-
4.4
Test Automation for Security
133
panies do not have enough talented security testers and never will have. Therefore,
the testing skills need to be built into the test automation tool and must not depend
on the competence of the tester.
Common arguments against test automation are based on experiences with bad
test automation products. This is because the definition of test automation by many
testing vendors is completely wrong. Some test automation frameworks require
more work to initially build the tests than is saved with repeated test execution. In
fact, most test automation methods and techniques follow all the worst practices of
test automation.
For example, some people think that security testing is the same as monkey test-
ing—i.e., randomly trying all types of inputs. To those people, test automation is
about trying more and more random inputs in a feeble attempt to increase test cov-
erage. This approach is doomed to fail, because adding more tests does not guaran-
tee better tests. Compared to just adding more random testing, manual testing by a
talented security expert is definitely better, because it is based on a process that adapts
to change.
To some, test automation means repeating the same test case over and over
again. This applies to many test automation frameworks that focus on conformance
or performance issues. In fuzzing, each test case should be unique. The purpose is
not to repeat, but to create many unique test cases that would be impossible to cre-
ate using manual testing. The same also applies to statements related to automat-
ing testing actions that are repeated over and over again in the test process. This is
irrelevant to fuzzing, because the entire process needs to be repeated, instead of just
one single test case. Running millions of tests with any other means but test
automation would take a significant amount of time to execute and analyze.
In relation to fuzzing (and fuzzing only!), test automation also reduces human
error in test design, implementation, execution, and analysis. There can be human er-
rors in fuzzer development itself, because a test automation tool is still a software
product and can have flaws. The majority of existing test automation frameworks
for performance testing and conformance testing depend on complex user configu-
ration, and one mistake in the configuration can invalidate the entire test run. Many
testing frameworks also leave the actual test design for the tester. A fuzzer frame-
work is often a test design framework if the user needs to parameterize the protocol
elements that are fuzzed. The resulting test suite should still be repeatable without
user involvement. Some fuzzers on the other hand have predefined intelligence and
have reduced the possibility of the tester to make mistakes in the test setup.
4.5
Summary
While building fuzzers isn’t always pleasurable, the act of fuzzing is fun! How-
ever, not everyone is excited by the hunt for zero-day flaws and exploiting those
found vulnerabilities. One of the most difficult issues with fuzzing is trying to
convince others that it is an important part of a vulnerability assessment process
and that it should be integrated into existing quality assurance processes. You
might understand the importance of fuzzing, but cannot convince others to see
134
Fuzzing Metrics
its benefits. To others, fuzzing is seen as an unnecessary cost and potential delay
in the product launch. Too many times we have heard people question fuzzing
with questions like
• “Why should I use fuzzing? This is not our responsibility!”
• “Why do you look for security problems in our products?”
Other questions that you might encounter are related to the problems that
fuzzers find. Fuzzers are not silver bullets in the sense that they find all security
problems. Explaining that fuzzers are just one more tool in the tool chest of secu-
rity experts and quality assurance professionals is important. This lack of under-
standing for the methodology and findings can reveal itself in questions like these:
• “Can I find distributed denial of service problems with fuzzing?”
• “Can I find the famous bug that the Code Red worm exploited?”
• “Surely, that would never happen in real-life usage scenarios, would it?”
• “Is that really exploitable? Isn’t it denial of service only?”
• “But I am already doing testing. Why should I use fuzzing?”
• “Aren’t these problems covered with our code auditing tools?”
Finally, choosing the right tool for the right task is sometimes difficult. How do
you compare the various tools, and how do you know which tool is the most use-
ful tool for each task during the product life cycle? You should be prepared to pro-
vide insight to your colleagues on product comparisons and the costs related to
various fuzzing approaches. Relying on the marketing and sales skills of various
fuzzing vendors or on the hype created by various hacker community tools can get
you distracted from the real purpose of why you became interested in fuzzing in the
first place. Be prepared for questions like
• “What is the best fuzzing tool that I can use?”
• “What is the cheapest fuzzing tool that I can use?”
You should be prepared to give answers to questions like these to your super-
visors, to your customers, or to the manufacturers that do not still “get it.” Fuzzing
is a fascinating technology, and all fuzzing approaches will definitely be better than
none at all. Still, fuzzing is not about technology, but rather, the final results from
the tests.
In this chapter we explained some reasoning behind integration of fuzzing into
your existing quality assurance and vulnerability analysis processes. As with secu-
rity in general, fuzzing starts with a threat analysis of the target system. Fuzzing is
always a form of black-box testing in the sense that the tests are provided to the sys-
tem under test through different interfaces. These interfaces can be remote or they
can be local interfaces that only local users can abuse. Fuzzing at its best is a proac-
tive technique for catching vulnerabilities before any adversaries will find them and
exploit them. The earlier they are found, the cheaper it is to fix them. Costs related
4.5
Summary
135
to vulnerabilities caught late in the product life cycle might initiate crisis communi-
cation with chaotic results, and a lot of unexpected costs related to the actual dis-
covery, remediation, and finally patch deployment. Whether a manufacturer
chooses reactively to buy vulnerability data from bug bounty hunters or subcon-
tract security auditing of released product to security professionals is a business
decision. Our goal is to convince them that proactive tools should be used instead
of this last-minute approach. With adequate expertise it could be beneficial to build
your own fuzzer, or to integrate an existing fuzzer product into your product devel-
opment. We explored some case studies on how the costs for each of these options
could be estimated before jumping into conclusions on the available choices.
Fuzzing is about test automation at its fullest extent. People used to traditional
test automation might think about complex test design frameworks that are time
consuming to use and that produce difficult-to-measure results. Fuzzers, on the
other hand, can be fully automated, in the sense that you just “press play,” watch
the fuzzer run, and wait for the results. Test automation comes in many flavors and
requires understanding of how it works before dedicating too many resources into
building a new one on your own.
One problem with security is that engineers often don’t even know or under-
stand security vulnerabilities. How can a virus misuse a buffer overflow problem in
a product? How critical is a denial of service problem? What is a buffer overflow/
format string vulnerability/null pointer deference anyway? Fuzzers can be inte-
grated into the development process with or without this knowledge. Think about
explaining to a developer why he should never use the strcpy function in his pro-
gram. Then think again about showing him a crash by sending a long string of char-
acters with that vulnerability in the software. Fuzzing has no false positives, meaning
that every single flaw found with fuzzing is exploitable to some level. It might not
allow remote code execution, but this is still a real bug. A crash is a crash. A bug is
a bug. Developers will immediately rush into fixing crash-level flaws or memory
leaks in their software when told to do so. They do not necessarily need to know
about the difference between stack and heap overflows or the exploitability of the
flaws. Still, all found issues need to be prioritized at some level. Some of you might
already be familiar through experience with what happens if you suddenly report
hundreds of remotely exploitable flaws found using fuzzing tools to a manufacturer
that has never had a security problem in its lifetime.
Now that we understand why we need to fuzz, and how we can communicate
the importance of fuzzing, we can study the actual techniques of fuzzing with com-
pletely new eyes. Compare the techniques presented later with the metrics we
explored here and hopefully you will find the technique that best suits you.
136
Fuzzing Metrics
C H A P T E R  5
Building and Classifying Fuzzers
In this chapter we will present fuzzer construction details. Endless methods to cre-
ate and deliver semi-valid data could be contrived; we will hit the most prevalent
techniques. Available open source tools will be described. A comparison of com-
mercial and open source tools will be held in Chapter 8. We begin by defining the
two primary types of fuzzers. These have been called full-blown and capture-replay
when dealing with network fuzzers, but generation and mutation are more generi-
cally accepted terms that also apply to file fuzzing. File fuzzing has continued to be
a hot topic as network protectors1 and network application designers have become
more security conscious.2 Client-side attacks, while requiring an element of social
engineering,3 have become like the previously more fertile grounds of network dae-
mon auditing from years gone by.
5.1
Fuzzing Methods
Fuzzer test cases are generated by the input source and attack heuristics and ran-
domness. Test cases could be developed manually, although this is not the preferred
method since many test cases will be needed. Generally, test cases come from either
a library of known heuristics or by mutating a sample input. Input source refers to
the type of fuzzer being used. There are two main types: generation and mutation.4,5
The terms intelligent and nonintelligent are also used. A generation based fuzzer is
a self-contained program that generates its own semi-valid sessions. (Semi-invalid
has the same meaning in this context.) A mutation fuzzer takes a known good net-
work session, file, and so on and mutates the sample (before replay) to create many
semi-valid sessions. Mutation fuzzers are typically generic fuzzers or general pur-
pose fuzzers, while generation fuzzers tend to be specific to a particular protocol,
137
1Firewalls and the like.
2Microsoft’s big security push: http://msdn.microsoft.com/msdnmag/issues/05/11/SDL/default
.aspx
3In this case, tricking a user into browsing to a malicious website or opening a malicious file is
the social engineering.
4A third method would be some type of genetic/evolving technology. One such algorithm is
described in Chapter 7.
5Peter Oehlert. “Violating Assumptions with Fuzzing,” IEEE Security & Privacy, (March/April
2005): 58–62.
application, or file format. Understanding the nature of the attack heuristics, how
this mutation or generation is accomplished, is important.
5.1.1
Paradigm Split: Random or Deterministic Fuzzing
It is appropriate at this point to discuss in some detail the ways these two different
types of fuzzers work and the advantages and disadvantages of both. For example,
mutations of a base file are often random—a mutation fuzzer. Mutation fuzzers
don’t understand anything about the underlying format, including where to add
anomalies, any checksums, and so on. However, a description of a file format in
SPIKEfile (defined later) would replace variables (defined later) with fuzz strings
taken from a file. Each element would be fuzzed one at a time—a generation/intel-
ligent fuzzer. Of course, it takes much more work for the tester to describe all these
“variables” to the tool.
Thus, randomness (dumbness) can be an interesting issue for fuzz testing. Some
believe that delivering random data to the lowest layers of each stage of a protocol
or API is the goal. Others prefer protocol construction and predefined test cases to
replace each variable (intelligent fuzzing). Charlie Miller gave a talk at DEF CON
2007 describing his experiences with the two approaches. He measured intelligent
fuzzing to be around 50% more effective but requiring 10 to 100 times the effort
by the tester.6
Having made some superficial statements about these two types of fuzzing
(detailed in a later case study), the randomness being discussed sometimes varies.
We’re not always talking about a tool that sends completely random data. Such a
tool is unlikely to find anything interesting in a modern application (the protocol
will be perceived as “too wrong” and be quickly rejected). Although even this
approach sometimes works, as Barton Miller (no relation to Charlie Miller) found7
when testing command line applications. Such a simple test could be conducted via
the following Unix command string:
while [1]; do cat /dev/urandom | netcat –vv IPaddr port; done
In this context, by random we might mean: Random paths through application
code are executed due to the delivery of semi-stochastic recombinations of the given
protocol, with some of the protocol fields modified in a semi-stochastic way. For
example, this might be done by making small random changes to a network session
to be replayed or by recombining parts of valid files. Another example is a network
fuzzer that plays the last protocol elements first and the first last; a semi-random
fuzzer that shuffles “command strings” (defined later) and variables.
Again, the two different schools of thought differ significantly in how test cases
are constructed and delivered. Consider a simple FTP server. One fuzzer might ran-
domly pick valid or invalid commands, randomly add them to a test case, and
choose random data for the arguments of those commands. Another fuzzer might
138
Building and Classifying Fuzzers
6See section 5.1.5, Intelligence Versus Dumb Fuzzers.
7Barton P. Miller, Lars Fredriksen, Bryan So, “An Empirical Study of the Reliability of Unix Util-
ities,” Communications of the ACM, 33(12)(December 1990):32–44.
have precooked invalid data in a library, along with a series of command sequences.
Invalid data is supplied with each command in some repeatable manner. Defining
a protocol and fuzz spots could be accomplished with a tool like SPIKE or Sulley.
Both approaches are valid ways to test. The goal of this chapter is to help one cre-
ate both types of fuzzers and understand the features of both, as well as when it is
most appropriate to use one or the other (or both).
There are pros and cons to each approach. The randomized approach may hit
dark corners that the deterministic approach might miss because it’s impossible to
think of every scenario or combination of commands and data. Plus, it’s trivial to
set up and execute. The field of fuzzing was based on this premise, and that’s why
traditional testing was insufficient. Particularly in the case of flipping bits in a file,
this is an easy way to fuzz that is surprisingly effective. However, regression testing
can be difficult for random fuzzers, if not properly seeded. Note: to fully fuzz a vari-
ety of fields could theoretically take forever. For example, imagine that one of the
fields is an integer. In this case to try every value, one would have to supply 2^32
different values. In practice, file-parsing routines have become complicated enough
(complication tends to equal a higher rate of errors) that randomly flipping bytes
throughout the file has been effective. Particularly effective have been boundary val-
ues like 0x0000000 and 0xffffffff, which will be discussed later.
The intelligent approach can be tuned to achieve more reliable code coverage.
Application robustness is of the utmost importance, as is security. In general, the
more deterministic fuzzing approach will perform better.8
A hybrid of these approaches could be created. Many mature fuzzers do include
elements of both. For example, instead of completely random data, fuzzing heuris-
tics would likely be applied to generate data that has been known to cause problems