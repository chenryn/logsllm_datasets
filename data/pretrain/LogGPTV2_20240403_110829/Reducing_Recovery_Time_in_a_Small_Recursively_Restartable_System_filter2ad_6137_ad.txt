6.11
24.75
20.93
—
—
—
—
V
Table 4. Overal MTTRs (seconds). Rows show tree
versions, columns represent component failures.
5. Discussion
The Mercury ground station is by design loosely cou-
pled, its components are mostly stateless, and failure de-
tection is based on application-level heartbeats. These are
important RR-enabling properties and Mercury provides a
good example of a simple RR system. The RR techniques
described here are applicable to a wider set of applications.
For example, we have found that many cluster-based Inter-
net services [3] as well as distributed systems in general are
particularly well suited to RR; in fact, many of the RR ideas
originated in the Internet world.
In this section, we extract from the Mercury experience
some general principles we believe are fundamentally use-
ful in thinking about applying RR to other systems.
5.1. Moving Boundaries
The most interesting principle we found was the bene-
ﬁt of drawing component boundaries based on MTTR and
MTTF, rather than based solely on “traditional” modularity
considerations such as state sharing. In transforming tree
 to tree V, we placed two independent components, e
and , into the same restart group. Presumably these two
components are independent, yet we are partially “collaps-
ing” the fault-isolation boundary between them by imposing
the new constraint that, when either is restarted, the other
one is restarted as well.
A dual of the above example is the splitting of fedc
into the two separate components fed and bc. As de-
scribed in the text, these two components are intimately
coupled from a functionality perspective; it is not an ex-
aggeration to say that either is useless without the other.
That is why in the original implementation fedc was
a single process, i.e., communication between the com-
ponents that became fed and bc took place by shar-
ing variables in the same address space. Post-splitting, the
two components must explicitly communicate via IPC. This
clearly adds communication overhead and complexity, but
allows the two components to occupy different positions
in the restart tree, which in turn lowers MTTR. We con-
clude that if a component exhibits asymmetric MTTR=MTTF
characteristics among its logical sub-components, rearchi-
tecting along the MTTR=MTTF separation lines may often
turn out to be the optimal engineering choice. Balancing
MTTR=MTTF characteristics in every component is a step
toward building a more robust and highly available system.
As explained in [4], RR attempts to exploit strong ex-
isting fault isolation boundaries, such as virtual memory,
physical node separation, or kernel process control, lead-
ing to higher conﬁdence that a sequence of restarts will ef-
fectively cure transients. To preserve this property, restart-
group boundaries should not subvert the mechanisms that
create the existing boundaries in the ﬁrst place.
5.2. Not All Downtime Is the Same
Unplanned downtime is generally more expensive than
planned downtime, and downtime under a heavy or critical
workload is more expensive than downtime under a light
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:24:15 UTC from IEEE Xplore.  Restrictions apply. 
or non-critical workload. In our system, downtime during
satellite passes (typically about 4 per day per satellite, last-
ing about 15 minutes each) is very expensive because we
may lose some science data and telemetry. Additionally, if
the failure involves the tracking subsystem and the recovery
time is too long, the communication link will break and the
entire session will be lost. A large MTTF does not guaran-
tee a failure-free pass, but a short MTTR can provide high
assurance that we will not lose the whole pass as a result of
a failure.
6. Related Work
The rebooting “technique” embodied in recursive
restartability has been around as long as computers them-
selves, and our work draws heavily upon decades of system
administration history. The RR model reﬁnes and system-
atizes a number of known techniques, in an attempt to turn
the “high availability folklore” into a well-understood tool.
Moreover, recursive restartability is a concrete example of
a newly emerging trend in system design, called ROC—
recovery-oriented computing [12].
The idea of gracefully terminating an application and
restarting it at a clean internal state is called software rejuve-
nation and was proposed by [9]. Although in this paper we
have focused on reactive rather than proactive restarts, re-
juvenation is an integral part of the RR strategy. Rejuvena-
tion has also found its way into Internet server installations
based on clusters of hundreds of workstation nodes; many
such sites use “rolling reboots” to clean out stale state and
return nodes to known “clean” states, Inktomi being one ex-
ample [3]. IBM’s xSeries servers also employ rejuvenation
for improved availability.
The ability to treat operating system services as separate
components, and the ensuing beneﬁts and drawbacks, have
long been known to the builders of microkernels [2]. More
recent work seeks similar beneﬁts by using lightweight vir-
tual machines [15] for hosting services in third-party In-
ternet infrastructures, thus turning these infrastructures into
RR-amenable systems.
The space systems community, and particulary the
small-satellite research community, has recently shown
tremendous interest in moving to COTS. In fact an entire
conference, the Symposium on Reducing the Cost of Space-
craft Ground Systems and Operations, is devoted largely to
such issues. This community is recognizing that the oppor-
tunities of COTS present challenges of dependability; we
believe that our application of recovery-oriented techniques
such as RR to space-based systems provides evidence for
the feasibility of this approach.
7. Future Work
Restarting cannot recover from a hard failure in a disk
drive or other hardware component such as the radio, which
is likely to happen eventually. We are in the process of im-
plementing component health summary beacons, which in-
clude a digest of internal metrics such as resource usage,
data structure consistency, connectivity checks, latency be-
tween key code points, warnings of suspect behavior that
has not yet caused a failure, and if applicable, information
about detectable hard failures. Comprehensive failure de-
tection and logging were not the goals of this effort, though
they are long-term goals for Mercury; health summaries
constitute one step in that direction.
We have described three types of restart tree transforma-
tions: depth augmentation, group consolidation, and node
promotion. In Mercury, the choice of transformations and
recovery policy was based to a great degree on estimated
values of fci, i.e., the probability that a manifested failure
in the observed group is minimally ci-curable. These values
are generally more difﬁcult to measure than actual failure
rates, but are easily determined from experience, after some
trial-and-error. In future work we intend to extend the ora-
cle with the ability to learn from its mistakes and this way
generate estimates for fci values. We also plan to identify
speciﬁc algorithms for transforming restart trees.
We are applying RR to another test system:
iROS,
the software infrastructure for the Stanford Interactive
Workspace. As described in [10], the functional compo-
nents of iROS were speciﬁcally written to be restartable,
and state management in the system as a whole is sensi-
tive to the possibility that components may be restarted at
nearly arbitrary times. This design decision was motivated
in part by the desire to leverage simple robustness solutions
based on mechanisms such as RR. We expect to report on
this work shortly.
Interesting work in software rejuvenation focuses on an-
alytic modeling of system uptime to derive optimal reju-
venation policies that maximize availability under a mod-
elled workload [8]. Although we made many simplifying
assumptions (all consistent with the behavior of our system)
to allow meaningful use of MTTR and MTTF in our argu-
ment, we expect to explore a more detailed analytic model
in future work.
Applying RR requires that components either be state-
less or utilize soft state [4]. For cases where some of the
system’s components are using hard state, we are devel-
oping a general model of recursively recoverable systems.
With recursive recovery, we can accomodate a wider range
of recovery semantics, since each component is recovered
using a custom procedure; restart is just one example of
a recovery procedure. An example of where the general
model is needed would be complex e-business infrastruc-
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:24:15 UTC from IEEE Xplore.  Restrictions apply. 
tures, that combine storage services with databases, appli-
cation servers, and web servers.
In the process of recovering, the various subsystems ac-
tively trade off certain system properties against each other,
such as performance or consistency for availability. A new
project we have started [5] has identiﬁed ﬁve basic axes for
making these tradeoffs and is developing a utility-function-
based model for dynamically optimizing these tradeoffs to
maximize system dependability.
8. Conclusions
We applied recursive restartability to a system with
which we had extensive “manual” experience. To improve
the system’s availability, we reduced its time to recover
from various types of failures we had observed over nearly
2 years in production use. We achieved automated failure
detection and recovery that was better than manual opera-
tion, even though the system was not purposely designed to
accomodate these goals.
The most signiﬁcant
lesson was that constructing
the “optimal” (lowest-MTTR) restart
tree required col-
lecting components into restart groups based on their
MTTFs/MTTRs and degree of failure correlation, and that
this requirement in turn may impose constraints on the
way components are architected. This suggests that when
module boundaries are drawn at design time, considera-
tion should be given to these properties in addition to the
ones traditionally considered functional orthogonality, de-
gree and granularity of state sharing, etc.
By employing recursive restartability we were able to
improve recovery time of our ground station by a factor of
four. Although we have not thoroughly measured the bene-
ﬁts resulting from automating the failure detection, we have
observed them to be signiﬁcant—in the past, relying on op-
erators to notice failures was adding minutes or hours to the
recovery time. There is an increasing trend toward complex,
hard-to-manage software systems that integrate large num-
bers of COTS modules; we believe that recovery-oriented
computing approaches hold a lot of promise as a depend-
ability technique in such systems.
9. Acknowledgements
We are indebted to the anonymous reviewers and our col-
leagues at Stanford University for their insights and helpful
comments on our paper. We thank the National Aeronau-
tics and Space Administration (NASA) for supporting our
work under award NAG3-2579, and the National Science
Foundation (NSF) under Career Award 133966.
References
[1] Patriot missile defense: Software problem led to system
failure at Dhahran, Saudi Arabia. Technical Report of
the U.S. General Accounting Ofﬁce, GAO/IMTEC-92-26,
GAO, 1992.
[2] M. J. Accetta, R. V. Baron, W. J. Bolosky, D. B. Golub, R. F.
R. A. Tevanian, and M. Young. Mach: A new kernel founda-
tion for UNIX development. In Proceedings of the USENIX
Summer Conference, pages 93–113, 1986.
[3] E. Brewer. Lessons from giant-scale services. IEEE Internet
Computing, 5(4):46–55, July 2001.
[4] G. Candea and A. Fox. Recursive restartability: Turning the
reboot sledgehammer into a scalpel. In Proceedings of the
8th Workshop on Hot Topics in Operating Systems, pages
110–115, Elmau, Germany, May 2001.
[5] G. Candea and A. Fox. Making sound tradeoffs in state man-
agement. In preparation, 2002.
[6] J. W. Cutler and G. Hutchins. Opal: Smaller, simpler, luck-
ier. In Proceedings of the AIAA Small Satellite Conference,
Logan, Utah, September 2000.
[7] A. DiGiorgio. The smart ship is not enough. Naval Institute
Proceedings, 124(6), June 1998.
[8] S. Garg, A. Puliaﬁto, M. Telek, and K. Trivedi. Analysis of
software rejuvenation using markov regenerative stochastic
petri nets. In Proceedings of the 6th International Sympo-
sium on Software Reliability Engineering, pages 180–187,
Toulouse, France, Oct 1995.
[9] Y. Huang, C. M. R. Kintala, N. Kolettis, and N. D. Fulton.
Software rejuvenation: Analysis, module and applications.
In International Symposium on Fault-Tolerant Computing,
pages 381–390, Pasadena, CA, 1995.
[10] B. Johanson, A. Fox, P. Hanrahan, and T. Winograd.
The event heap: An enabling infrastructure for interactive
workspaces. Technical Report CS-2001-02, Stanford Com-
puter Science Department, Stanford, CA, 2001.
[11] J.-J. Miau and R. Holdaway, editors. Reducing the Cost
of Spacecraft Ground Systems and Operations, volume 3.
Kluwer Academic Publishers, 2000.
[12] D. Patterson, A. Brown, P. Broadwell, G. Candea, M. Chen,
J. Cutler, P. Enriquez, A. Fox, E. Kiciman, M. Merzbacher,
D. Oppenheimer, N. Sastry, W. Tetzlaff, and N. Treuhaft.
Recovery oriented computing (ROC): Motivation, deﬁni-
tion,
Technical Report
UCB/CSD-02-1175, UC Berkeley, Berkeley, CA, March
2002.
techniques, and case studies.
[13] G. Reeves. What really happened on Mars? RISKS-19.49,
Jan. 1998.
[14] M. A. Swartwout and R. J. Twiggs. SAPPHIRE - Stanford’s
ﬁrst amateur satellite. In Proceedings of the 1998 AMSAT-
NA Symposium, Vicksberg, MI, October 1998.
[15] A. Whitaker, M. Shaw, and S. D. Gribble.
Denali:
Lightweight virtual machines for distributed and networked
applications. In Proceedings of the USENIX Annual Techni-
cal Conference, Monterey, CA, June 2002.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:24:15 UTC from IEEE Xplore.  Restrictions apply.