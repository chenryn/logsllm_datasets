Are there corresponding log
entries in NVMM write cache? No
NVCACHE uses a head index in volatile memory1 to
create new entries in the log. When NVCACHE intercepts a
write, it advances the head index and ﬁlls the new log entry
accordingly. Finally, NVCACHE also maintains a copy of the
persistent tail index in volatile memory. NVCACHE uses this
volatile tail index to synchronize a writer with the cleanup
thread when the log is full (see §III for details).
Failure management. When a new entry is created in the
log but its ﬁelds are not yet completely populated, i.e., it
corresponds to a non-committed write, the recovery procedure
of NVCACHE ignores the entry. To detect such scenarios,
we could use a committed index in NVMM to keep track of
the last committed entry. While we considered this approach,
we decided to avoid it for three reasons. First, maintaining a
shared index in NVMM is inefﬁcient in multi-threaded envi-
ronments because of the synchronization costs for accessing it
consistently. Second, non-trivial synchronization mechanisms
are also required to handle scenarios when a thread commits
a new entry before an older one that is not yet committed.
Finally, since data moves from the processor caches to the
NVMM at cache-line granularity, updating an index would
lead to an additional cache miss to load the index.
Instead, in order to handle non-committed writes left in
the log after a crash, we directly embed a commit ﬂag
with each entry (column c in Figure 1). With this design,
a thread can independently commit an entry while bypassing
the synchronization costs of a shared index. NVCACHE also
avoids an additional cache miss because the commit ﬂag lives
inside an entry that already has to be loaded during a write.
C. Design of the read cache
Since the kernel may contain stale data in its page cache,
NVCACHE implements a small read cache in user space.
Technically, NVCACHE implements an approximation of an
LRU cache in volatile memory. When a ﬁle is not opened in
write-only mode (in which case the read cache is not required),
NVCACHE maintains a radix tree [4] used to quickly retrieve
the pages of the ﬁle, an approach similar to NOVA [57].2
NVCACHE also keeps track (in volatile memory) of the
cursor and size of each ﬁle as both could be stale inside the
kernel, e.g., when a newly appended data block is still in ﬂight
in the NVCACHE write cache.
Page descriptor and page content. A leaf of the radix tree
contains a page descriptor, which is created on demand during
a read or a write. The page descriptor keeps track of the state
of a page (see Table II). When a page is present in the read
cache, the page is in the loaded state. In this state, a page
content is associated to the page descriptor. A page content is a
1This index is not in NVMM, i.e., not needed for recovery after a crash.
2A page size has to be a power of two because we use a radix tree.
Nevertheless, pages in NVCACHE are not related to hardware pages.
3
Fig. 1: Overall architecture of NVCACHE (fd: ﬁle descriptor,
off: offset, c: committed).
functions of the legacy system library (libc). In case of a
write, NVCACHE adds the written data in the write cache
(Figure 1-) by appending it to the NVMM log, which makes
the write durable.
If the write modiﬁes data in the volatile read cache,
NVCACHE also updates the data accordingly (Figure 1-)
so that subsequent reads always see up-to-date data.
In case of a read, NVCACHE retrieves the data from the
read cache. The data is either present and up-to-date (cache
hit, Figure 1-) or unavailable (cache miss). Upon cache
misses, NVCACHE loads the data from the kernel page cache
(Figure 1-). If the data in the kernel is stale, i.e., the data was
already modiﬁed in the write cache, NVCACHE also applies
the writes saved in the NVMM log (Figure 1-).
In background, a dedicated cleanup thread asynchronously
propagates writes to the kernel page cache (Figure 1-), which
will itself subsequently propagate them to disk. To do so, the
cleanup thread uses standard write system calls from the
legacy system library. In addition, the cleanup thread is also
in charge of removing pending writes from the log as soon as
the kernel has ﬂushed them to disk.
NVCACHE is optimized for applications opening ﬁles in
read-only mode. In such instances, NVCACHE entirely by-
passes its read caches, hence avoiding the use of dynamic
random-access memory (DRAM) altogether, because the ker-
nel page cache already contains fresh data for read-only ﬁles.
We ﬁrst describe the mechanisms underlying the write and
read caches assuming a single-threaded application, focusing
on multi-threading in §II-D.
B. Design of the write cache
NVCACHE implements its write cache as a circular log in
NVMM. Each entry in the log contains a write operation, i.e.,
the target ﬁle descriptor, the starting offset, data itself and the
number of bytes to write.
In addition to the log, NVCACHE stores in NVMM a table
that associates the ﬁle path to each ﬁle descriptor, in order
to retrieve the state after a crash. NVCACHE also keeps a
persistent tail index in NVMM to track the oldest entry of the
log. The cleanup thread and the recovery procedure use the tail
index to propagate the oldest writes to disk before the newer
ones, in order to preserve the write order of the application.
libc I/O…Read cache — DRAM (LRU)Write cache — NVRAM log30Hello world!\01fdoﬀdatac410004kiB pages150Writing…0HDD/SSDUser spaceKernel spaceVolatilePersistentopenreadwriteclosefopenfreadfwritefclosefsyncLinux cache➊➋➌➍➏➎D. Multi-threading
As required by the POSIX speciﬁcation, NVCACHE ensures
that read and write functions are atomic with respect to each
other [1].NVCACHE ensures thus that concurrent writes to the
same pages are executed one after the other while respecting
the write order of the application, and ensures that a read
cannot see a partial write. Apart from ensuring atomicity for
reads and writes on the same pages, the design of NVCACHE
also natively supports fully parallel execution of writes to inde-
pendent pages. We achieve this by leveraging three techniques:
ﬁxed-sized entries, page-level locking granularity and scalable
data structures, which we describe next.
Fixed-sized entries. As a ﬁrst step to execute independent
writes in parallel, NVCACHE uses ﬁxed-size entries. With
entries of arbitrary size, one would need to store the size
of each entry in the log, which prevents a thread to commit
an entry if the previous entry is not yet committed. Indeed,
in case of crash, the recovery procedure cannot know if the
size of an uncommitted entry is correct, and can thus neither
inspect the next entry nor consider it as committed. With ﬁxed-
sized entries, a thread can commit an entry, even if a previous
entry allocated by another thread is not yet committed. In
this case, because the entry size is predeﬁned (a system
parameter of NVCACHE), the recovery procedure can ignore
an uncommitted entry and continue with the next one.
Using ﬁxed-sized entries, NVCACHE must use multiple en-
tries for large writes not ﬁtting in a single entry. For such large
writes, NVCACHE must commit all the entries atomically in
order to avoid partial writes. NVCACHE atomically commits
the multiple entries by considering the commit ﬂag of the ﬁrst
entry as the commit ﬂag of the group of entries. Technically,
NVCACHE maintains a group index in each entry. The index
is set to -1 for the ﬁrst entry and, for the following entries, it
indicates the position of the ﬁrst entry. NVCACHE also saves
space, and thus cache misses, by packing the commit ﬂag and
the group index in the same integer.
Per-page locking. When two threads concurrently modify
the same pages, NVCACHE ensures that one write is fully
executed before the other. Instead of using a single write lock
for each ﬁle, NVCACHE optimizes parallelism by using a per-
page locking scheme, in which each page descriptor contains
a lock called the atomic lock. In case of a write, a thread starts
by acquiring all the atomic locks of the written pages. Then,
the thread adds the write to the log by creating, ﬁlling and
committing one or multiple entries. Finally, before releasing
the atomic locks, the thread increments the dirty miss counters
and, for modiﬁed pages in the loaded state, the thread also
updates their content in the read cache.
The atomic lock is also used in the read function in order to
ensure atomicity. In case of a read, as for a write, NVCACHE
starts by acquiring all the per-page locks, which ensures that
a read cannot see a partially updated page.
NVCACHE also uses a second lock per page, called the
cleanup lock, which is used to synchronize the cleanup thread
and the dirty miss procedure. Without this lock, in case of
Fig. 2: State machine of pages (dc: dirty counter).
piece of cached data, always kept consistent: when NVCACHE
intercepts a write, it also updates the content of a loaded page
in the read cache.
When a page is not present in the read cache, its page
descriptor (when it exists) is not associated to a page content.
When unloaded, a page can have two different states. A
page is in the unloaded-dirty state if the NVCACHE write
cache contains entries that modify the page. In this state,
the content of the page outside NVCACHE (in the kernel
page cache or on disk) is dirty. A page is in the unloaded-
clean state otherwise. NVCACHE distinguishes the unloaded-
dirty from the unloaded-clean state with a counter, called
the dirty counter, stored in the page descriptor. NVCACHE
increments this counter in case of writes, and the cleanup
thread decrements this counter when it propagates an entry
from the write cache.
State transitions. Our design has the goal of avoiding any
costly synchronous write system call in the critical path of the
application. Technically, NVCACHE avoids write system calls
when it evicts a dirty page or when the application modiﬁes
an unloaded page. Instead of synchronously writing the page,
NVCACHE simply marks the page as unloaded-dirty and lets
the cleanup thread asynchronously perform the propagation.
Figure 2 presents the state transitions for the pages han-
dled by NVCACHE. In more detail, in case of cache miss,
NVCACHE starts by evicting the least recently used page
(loaded to unloaded-clean or to unloaded-dirty according to
the dirty counter in Figure 2) to make space for the new page.
Then, NVCACHE handles two cases. If the new page comes
from the unloaded-clean state (unloaded-clean to loaded in
Figure 2), NVCACHE simply loads the page in the read cache
by using the read system call. If the new page comes from
the unloaded-dirty state (unloaded-dirty to loaded in Figure 2),
NVCACHE loads the page and additionally executes a custom
dirty-miss procedure. This procedure reconstructs the
state of the page by searching the dirty counter entries that
modify the page in the log, starting from the tail index, and
applies the modiﬁcation in the read cache. This procedure is
costly but, as shown in our evaluation (see §IV), dirty misses
are rare. Thanks to the dirty-miss procedure, NVCACHE
avoids the synchronous write system calls upon dirty page
eviction (loaded to unloaded-dirty in Figure 2) and when
writing an unloaded page (unloaded-clean to unloaded-dirty
in Figure 2).
4
UnloadedcleanLoadedUnloadeddirtydc=0dc>0writewritereadreadCleanupDirty missEvictionwritereadCache misscache miss, the application may read a stale page from the
disk and miss an entry concurrently propagated by the cleanup
thread. We avoid this race condition by acquiring the cleanup
lock in the cleanup thread before the propagation, and by
acquiring the cleanup lock in the application in case of cache
miss. In more detail, a reader starts by acquiring the atomic
lock in order to ensure atomicity. Then, in case of cache miss,
the reader both reads the page from the disk and applies
the dirty miss procedure while it owns the cleanup lock,
which ensures that the cleanup thread cannot process an entry
associated to the page while the application applies the dirty
miss procedure.
Finally, NVCACHE synchronizes the access to the dirty
miss counter by using atomic instructions and by leveraging
the two locks associated to the page. In case of a write,
NVCACHE simply increments the dirty miss counter with
an atomic instruction while it owns the atomic lock, which
ensures that a reader cannot execute the dirty miss procedure
at the same time (because the reader has to own the atomic
lock to execute a read and thus the dirty miss procedure).
The cleanup thread decrements the dirty miss counter with
an atomic instruction while it owns the cleanup lock, which
ensures that a reader cannot execute the dirty miss procedure at
the same time (because the reader has also to own the cleanup
lock to execute the dirty miss procedure).
Due to our locking scheme with two locks per page, the
cleanup thread never blocks a writer and never blocks a reader
when the page is already in the read cache. The cleanup thread
can only block a reader in case of cache miss when the cleanup
thread propagates an entry that modiﬁes the missed page.
Scalable data structures. Most of the internal structures of
NVCACHE are designed to scale with the number of threads.
First, in order to add an entry in the write log, a thread simply
has to increment the head index, which is done with atomic
instructions.3
Then, the radix tree can operate with a simple lock-free
algorithm because NVCACHE never removes elements from
the tree, except when it frees the tree upon close. When an
internal node or a page descriptor in the radix tree is missing,
a thread tries to atomically create it with a compare-and-swap
instruction; if this fails, it means that the missing node or page
descriptor was concurrently added to the tree by another thread
and we can simply use it.
Finally, NVCACHE builds an approximation of an LRU
algorithm by storing an accessed ﬂag in each page descrip-
tor, which is set during a read or a write. In more detail,
NVCACHE maintains a queue of page contents protected
by a lock, called the LRU lock. Each page content holds a
reference to its descriptor and conversely, as already presented,
a descriptor links to the associated page content when it is
loaded. When NVCACHE has to evict a page, it acquires
the LRU lock and dequeues the page content at the head of
the queue. Then, NVCACHE acquires the atomic lock of the
3Note that we do not need special instructions to handle NVMM since the
head lives in volatile memory.
TABLE III: Functions intercepted by NVCACHE.
open, read, write, close Uses NVCACHE functions
fopen, fread, fwrite, fclose Uses unbuffered versions
Function Action
sync, syncfs, fsync No operation
lseek, ftell, stat, etc. Uses size/cursor of NVCACHE
page descriptor associated to the head and checks its accessed
ﬂag. If the ﬂag is set, NVCACHE considers that the page
was recently accessed: it releases the atomic lock of the page
descriptor, re-enqueues the page content at the tail, and restarts
with the new head. Otherwise, NVCACHE recycles the page
content: it nulliﬁes the reference to the page content in the
page descriptor, which makes the page descriptor unloaded-
clean or unloaded-dirty, and releases the atomic lock of the
page descriptor.
Summary. To summarize, in case of a write, NVCACHE only
acquires the atomic locks to the written pages. In case of read
hits, i.e., if the page contents are already in the read cache,
NVCACHE also only acquires the atomic locks to the read
pages. In case of read misses, NVCACHE has also to acquire
the LRU lock and an atomic-lock during eviction, and then
the cleanup lock to read the missed page. As a result, except
to handle read misses, NVCACHE executes two operations
that access different pages in a fully concurrent manner, and
without using locks to synchronize with the cleanup thread.
III. IMPLEMENTATION
We implemented NVCACHE on top of the musl libc [26], a
lightweight yet complete alternative to glibc. Musl consists
of 85 kLoC and NVCACHE adds 2.6 kLoC. In order to sim-
plify deployment, instead of using LD_PRELOAD to wrap I/O-
based system calls, we directly replace the I/O-based system
calls by ours in musl. We also exploit the Linux Alpine dis-
tribution [19] using musl to easily deploy NVCACHE behind
legacy applications (as shown in our evaluation). Moreover,
NVCACHE supports Docker containers [23] and our approach
allows us to deploy legacy applications with just one minor
change to existing manifests, replacing the original libc shared
object by ours.
Table III lists the main functions intercepted by NVCACHE.
Essentially, NVCACHE wraps open, read, write and
close to use the read and write caches. Additionally, it