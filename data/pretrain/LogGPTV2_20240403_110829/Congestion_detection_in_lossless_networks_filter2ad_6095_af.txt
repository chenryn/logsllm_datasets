(TIMELY+TCD).
18:
FCT
performance
for
victim flows
(a) DCQCN+TCD
(b) TIMELY+TCD
Figure 20: Fairness with TCD.
is the static D-mod-k scheme [21] in InfiniBand network. For each
rack, we randomly select four servers as I/O servers to receive I/O
traffic from I/O clients. Then 25% nodes are randomly selected as
I/O clients. The remaining nodes are MPI clients and MPI servers.
In total, we generate over 80 thousand messages, of which 10% are
I/O messages. I/O messages are with sizes randomly from arrays
[512KB, 1MB, 2MB, 4MB]. MPI messages range from 2KB to 32KB,
with over 50% of MPI messages are 2KB. Figure 17(b) shows the
average MCT breakdown. IB CC with TCD can achieve 1.22× better
overall average FCT than IB CC. IB CC with TCD especially benefits
I/O messages. For example, improving average MCT by 1.5× for
512KB messages. We consider that mistakenly throttling of victims
can affect more I/O messages because multiple I/O messages are
sending in one connection.
5.2.3 TIMELY. TCD can also benefit delay-based congestion con-
trol. TIMELY [43] uses RTT gradient as the congestion signal, which
can not distinguish between delay increase caused by congestion
and delay increase caused by PAUSEs. With TCD, endpoints are
aware of whether a flow only passes through an undetermined port.
In detail, senders do not update the sending rate if the gradient is
above zero ( 𝑇𝑙𝑜𝑤 < 𝑅𝑇𝑇 < 𝑇ℎ𝑖𝑔ℎ ) and the packet is marked with UE
simultaneously. We change the rate reduction factor 𝛽 from default
0.8 to 1.6 to decrease the rate of congested flows aggressively. Our
simulator is developed based on the code snippet for TIMELY [42],
and the remaining parameters are recommended values in [43].
Victim flows scenario: With TIMELY, we do observe that the
sending rate of victim flows is reduced due to increased values of
RTT samples. With TCD, sending rate of victim flows is not reduced
when RTT sudden increases due to PUASEs. Figure 18(a) shows the
average FCT breakdown of victim flows. TIMELY with TCD can
achieve 2.2× and 2.3× better average FCT for flows smaller than
10KB and flows larger than 1MB, respectively. We also evaluate the
performance under small flows, where 𝐴0 ∼ 𝐴14 generate concur-
rent burst with varying sizes. Figure 18(b) presents the average FCT
performance of victim flows and the fraction of undetermined flows.
Similarly, as the burst size increases, more flows are victimized and
marked with UE.
Realistic workloads: Figure 19 presents the overall FCT slow-
down for the Hadoop and WebSearch workload (we adopt the same
network settings as in DCQCN case). For both workloads, TIMELY
with TCD improves the median and 99th-percentile FCT slowdown
especially for small and medium flows. For instance, under the
Hadoop workload, TIMELY with TCD can reduce the 99th-percentile
FCT slowdown for flows smaller than 50KB from 50.3 to 36.6.
Fairness with TCD. To evaluate how our recommended rate
5.2.4
adjustment rules for ternary states affect the fairness property, we
adopt a modified topology in Figure 2. We add host 𝐵0 ∼ 𝐵3 con-
nected to the switch L0. S1 sends long-lived flow F1 to R1. 𝐴0 ∼ 𝐴14
generate concurrent 64KB burst to R1, which last for about 3ms.
𝐵0 ∼ 𝐵3 send four long-lived flows to R0 at the same time when
bursts start. Figure 20 demonstrates the throughput of these four
flows. During burst launching, the congestion spreads to port P2.
Port P2 becomes an undetermined port. Under the gentle rate adjust-
ment rule, the CC rate of four flows is kept unchanged because they
only go through the undetermined port P2. The throughput drop is
because of head-of-line blocking at link L0-T2 as hop-by-hop flow
control kicks in. Consequently, TCD can achieve the same fairness
as the hop-by-hop flow control for flows only passing through an
undetermined port. After congestion at port P3 disappears, port P2
becomes a congestion port. Then four flows transit to congested
380
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Yiran Zhang, Yifan Liu, Qingkai Meng, Fengyuan Ren
flows. For ECN-based DCQCN and delay-based TIMELY, congested
flows can achieve fair-share rate allocation (8Gbps) with TCD.
6 DISCUSSION
Design tradeoff. Adaptively adjusting 𝑚𝑎𝑥 (𝑇𝑜𝑛) (e.g., predicting
𝑇𝑜𝑛 based on historical values and real-time queue length) is an-
other design choice to detect state transitions. TCD relies on a pre-
configured 𝑚𝑎𝑥 (𝑇𝑜𝑛) to determine whether to enter or leave the
undetermined state. We believe a proper static 𝑚𝑎𝑥 (𝑇𝑜𝑛) is enough
for most practical scenarios according to the ON-OFF model and
evaluations. Further, adaptively adjusting 𝑚𝑎𝑥 (𝑇𝑜𝑛) increases the
design complexity and cost while may only introduce marginal
gains. With a pre-configured 𝑚𝑎𝑥 (𝑇𝑜𝑛) in TCD, if an anomalous
traffic pattern drives 𝑇𝑜𝑛 to be much smaller than 𝑚𝑎𝑥 (𝑇𝑜𝑛), it will
not affect the detection on transitions to the undetermined state,
only leading to limited late detection on the leave from the unde-
termined state (no more than 𝑚𝑎𝑥 (𝑇𝑜𝑛) time). On the other hand,
if an anomalous traffic pattern drives 𝑇𝑜𝑛 to be much larger than
the pre-configured 𝑚𝑎𝑥 (𝑇𝑜𝑛), the undetermined port may switch
between the undetermined state and non-congestion/congestion
state at a low frequency. TCD makes acceptable compromise under
these corner cases for overall design and implementation simplicity.
Cooperation with congestion controls. Decreasing the rate
of congested flows aggressively and adjusting the rate of undeter-
mined flows gently is our general recommendation for cooperating
TCD with congestion controls in lossless networks. The detailed
rate adjustment rules in § 5.2 are only simple cases to validate our in-
sights. However, if PFC/CBFC is not triggered in lossless networks,
simply adopting a more aggressive reduction for congested flows
than existing congestion controls may harm the link utilization
and throughput. Given that PFC/CBFC cannot be avoided totally
from production experience [10, 24, 31], we think determining the
proper individual rate adjustment rules for congested flows and
undetermined flows is a valuable problem that needs to be studied
in-depth in the future.
7 RELATED WORK
Congestion detection is a classic research topic. We classify existing
congestion detection mechanisms into three categories.
Switch congestion detection. Switches are in charge of detect-
ing congestion and providing explicit congestion signals. Switches
detect incipient congestion based on average or instant queue
size and perform packet dropping/marking to notify endpoints
[20, 30, 40]. TCP and its variants [19, 25, 30] utilize packet loss as
the congestion signal. With ECN [46] support, switches can mark
CE instead of dropping packets. Although a single marking packet
is intended to cause the transport layer to respond, several conges-
tion controls propose to react to congestion based on the multi-bit
congestion information provided in consecutive ECN. For example,
DECbit [47] reacts to congestion when the ratio of marked packets
is over a certain parameter. DCTCP [12] reacts to the extent of
congestion according to the fraction of marked packets.
Switch and endpoint collaborated congestion detection. Re-
cent congestion control algorithms in lossless networks detect con-
gestion in a collaboration of switches and endpoints, which requires
tightly coupled design with rate control at endpoints. PCN [16]
discovers that PFC can affect congestion detection and develops
Non-PAUSE ECN (NP-ECN). Switches maintain a counter to record
the number of paused packets. Only non-paused packets are eligi-
ble to be marked with ECN. The receiver NICs detect and identify
a congested flow when the fraction of marked packets exceeds a
threshold (i.e.,95%) during a period 𝑇 . HPCC [38] aims at control-
ling inflight bytes for the most congested link, relying on in-band
network telemetry (INT) to obtain related information of inflight
bytes. INT headers carry transmitted bytes and queue length infor-
mation. Senders combine all information to calculate the current
inflight bytes. In a word, both NP-ECN and INT are not independent
congestion detection mechanisms in switches.
End-to-end congestion detection. Endpoints can also infer
congestion based on end-to-end delay without any in-network
support. Several delay-based congestion control algorithms in lossy
networks leverage RTT or RTT variations [14, 33, 36, 43, 53] to
detect congestion. Recent work also differentiates delays caused by
in-network congestion and delay caused by endpoint congestion
[35]. However, in lossless networks, with only end-to-end delay, it
is tough for endpoints to distinguish between delay increase caused
by congestion and delay increase caused by the ON-OFF regulation
of hop-by-hop flow controls.
Receiver-driven congestion controls. Recently several receiver-
driven congestion controls have been proposed, such as SRP [32],
ExpressPass [17], NDP [26], and Homa [44]. The core of receiver-
driven congestion controls is proactive transport operating in a
"request and allocation" style: explicitly allocating the bandwidth
of bottleneck link(s) and proactively preventing congestion [27].
These schemes usually let new flows blindly transmit unscheduled
packets in the first RTT, where congestion may also occur with
the risk of triggering hop-by-hop flow controls. Currently, TCD is
suitable for end-to-end congestion controls that react to congestion
signals (e.g., ECN or delay).
8 CONCLUSION
This paper re-understands congestion detection in lossless net-
works and proposes ternary congestion detection (TCD) for CEE
and InfiniBand. We reveal a new port state called the undetermined
state and define ternary states. Testbed and extensive simulations
demonstrate that TCD can accurately detect congestion ports and
identify congested flows as well as undetermined flows. Case stud-
ies show that existing congestion control algorithms can achieve
better performance by combining with TCD, confirming that accu-
rate congestion detection is significant for congestion controls. We
envision that TCD will motivate further exploration of congestion
controls considering ternary states.
ACKNOWLEDGMENTS
The authors gratefully acknowledge the shepherd Vishal Misra
and the anonymous reviewers for their constructive comments.
The authors also thank Wenxue Cheng and Kun Qian for insight-
ful discussions on this work. This work is supported in part by
the National Key Research and Development Program of China
(No.2018YFB1700203, 2018YFB1700103), and by National Natural
Science Foundation of China (NSFC) under Grant 61872208, as well
as gifts from MSRA.
381
Congestion Detection in Lossless Networks
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
REFERENCES
[1] 2010. IEEE 802.1 Qau - Congestion Notification. http://www.ieee802.org/1/page
s/802.1au.html
[2] 2010. IEEE 802.1 Qbb - Priority-based Flow Control. http://www.ieee802.org/1/
pages/802.1bb.html
[3] 2012. InfiniBand Most Used Interconnect on the TOP500. https://www.infinib
andta.org/infiniband-most-used-interconnect-on-the-top-500/
[4] 2013. InfiniBand Flit Level Model. https://omnetpp.org/download-items/InfiniB
and-FlitSim.html
[5] 2016. Life in the Fast Lane: InfiniBand Continues to Reign as HPC Interconnect
of Choice. https://www.infinibandta.org/life-in-the-fast-lane-infiniband-contin
ues-to-reign-as-hpc-interconnect-of-choice/
[6] 2020. Intel DPDK. https://www.dpdk.org/
[7] 2020. Test-pipeline. https://github.com/DPDK/dpdk/tree/main/app/test-pipeline
[8] 2021.
https:
//www.opencompute.org/files/INT-In-Band-Network-Telemetry-A-Powerful-
Analytics-Framework-for-your-Data-Center-OCP-Final3.pdf
In-band Network Telemetry in Barefoot Tofino.
[9] Mohammad Al-Fares, Alexander Loukissas, and Amin Vahdat. 2008. A Scal-
able, Commodity Data Center Network Architecture. In Proceedings of the ACM
SIGCOMM 2008 Conference on Data Communication (Seattle, WA, USA) (SIG-
COMM ’08). Association for Computing Machinery, New York, NY, USA, 63–74.
https://doi.org/10.1145/1402958.1402967
[10] Fatma Alali, Fabrice Mizero, Malathi Veeraraghavan, and John M. Dennis. 2017.
A measurement study of congestion in an InfiniBand network. In 2017 Network
Traffic Measurement and Analysis Conference (TMA). 1–9. https://doi.org/10.239
19/TMA.2017.8002911
[11] Mohammad Alizadeh, Tom Edsall, Sarang Dharmapurikar, Ramanan