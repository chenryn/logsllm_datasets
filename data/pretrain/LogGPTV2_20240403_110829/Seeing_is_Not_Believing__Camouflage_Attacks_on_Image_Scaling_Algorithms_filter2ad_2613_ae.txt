### 1. Introduction to Scaling Attacks and Their Implications

Thousands of websites employ content moderation systems to prevent the dissemination of offensive material [23]. However, attackers can exploit scaling attacks to bypass these moderators, thereby spreading inappropriate images, which can lead to serious issues in online communities. For example, an attacker might use a scaling attack to create a cloaking effect, where the image displayed on an iPhone XS browser shows illegal drug content, while the original-sized image appears benign. Although cloaking can be achieved through other methods like browser-sensitive JavaScript, scaling attacks offer an alternative that does not require additional code to manage the rendering effect.

#### 1.1 Fraud via Inconsistencies Between Displays

An attacker can also use scaling attacks to create deceptive digital contracts. By crafting an image document that displays different content at different scaling ratios, the attacker can get two parties to sign the same document. If each party uses a different browser, they will see different content, leading to potential financial fraud.

### 2. Countermeasures Against Scaling Attacks

In this section, we discuss potential defense strategies to mitigate the threat from scaling attacks. We first explore possible countermeasures in the image preprocessing stage, followed by approaches to detect scaling attacks.

#### 2.1 Attack Prevention

A straightforward method to avoid scaling attacks is to reject inputs with sizes different from those expected by the deep learning models. This approach is suitable for applications that deal with sensor-collected data in specific formats. However, it is impractical for many Internet services, as user-uploaded images often vary in size.

Another solution is to randomly remove some pixels (by line or column) from the image before scaling. This random cropping operation makes the scaling coefficients unpredictable, increasing the difficulty of the attack. However, careful design of the pixel removal policy is necessary to maintain image quality.

#### 2.2 Attack Detection

Scaling attacks achieve their deceptive effect by causing significant changes in visual features during the scaling process. One potential detection method is to identify such changes in input features, such as color histograms and color scattering distributions.

##### 2.2.1 Color-Histogram-Based Detection

The color histogram counts the number of pixels for each color range in a digital image, providing a measure of color distribution. It is commonly used to assess image similarity. The main advantage of this approach is its ability to easily and quickly measure changes in color distribution. In our experiments, we convert the image to grayscale to evaluate the effectiveness of color-histogram-based detection. The color histogram of an image is represented as a 256-dimensional vector, and we use cosine similarity to measure the color-histogram similarity between two images.

##### 2.2.2 Color-Scattering-Based Detection

While color histograms provide a rough distribution of pixel values, they do not account for spatial color distribution. Color scattering, which measures the distance between pixels and the image center, can supplement the histogram. In our experiments, we convert the image to grayscale to evaluate the effectiveness of color-scattering-based detection. We calculate the distance histogram as the color scattering measurement and define a statistical metric to evaluate similarity. Specifically, we compute the average distance from pixels with the same value to the image center and represent the result as a 256-dimensional color scattering vector. We then calculate the cosine similarity between vectors of two images to determine the color-scattering-based similarity.

### 3. Evaluation of Detection Strategies

To evaluate the performance of the two detection strategies, we crafted three attack images for each source image in the dataset established in Section 6.2, with three 224*224 target images belonging to the wolf, human face, and cat categories. Before comparing similarities, we resize the output to match the input size to eliminate differences in pixel count. Figure 8 shows the detection results for a wolf-in-sheep attack image.

Figures 8e and 8f compare the grayscale histograms of the input images and their scaled outputs. The x-axis represents pixel values ranging from 0 to 255, and the y-axis represents the number of pixels with the same value. Figure 8f shows that the curves for the original input and its scaled output nearly coincide, with a similarity of 0.96. In contrast, there is a clear difference between the color distribution of the attack image and its scaled output, with a similarity of 0.50.

Figures 8g and 8h compare the grayscale color scattering measurements. The x-axis represents pixel values ranging from 0 to 255, and the y-axis represents the average distance between the image center and pixels with the same value. Similarly, there is a clear difference in the color scattering measurement of the attack image and its scaled output.

Figure 9 reports the complementary cumulative distribution (CCD) of the detection results for our test set. The legend "original-resize," "ds-wolf," "ds-face," and "ds-cat" refer to the original-image, wolf-as-target, human-face-as-target, and cat-as-target cases, respectively. Both detection metrics show that the similarity between original images and their scaled outputs is significantly higher than that between attack images and their scaled outputs, indicating that the detection strategies are effective in most cases.

### 4. Conclusion

This paper presents a camouflaging attack on image scaling algorithms, which poses a potential threat to computer vision applications. By crafting attack images, the attack can cause significant changes in the visual semantics of images during scaling. We studied popular deep learning frameworks and found that most of their default scaling functions are vulnerable to such attacks. Our results also show that even though cloud services (such as Microsoft Azure, Baidu, Aliyun, and Tencent) hide their scaling algorithms and input scales, attackers can still achieve the deceptive effect. The purpose of this work is to raise awareness of the security threats embedded in the data processing pipeline of computer vision applications. Compared to adversarial examples, we believe that scaling attacks are more effective in creating misclassification due to the deceptive effects they can produce.

### 5. Acknowledgments

We thank our shepherd Dr. David Wagner and all anonymous reviewers for their insightful suggestions and comments to improve the paper; Dr. Jian Wang and Dr. Yang Liu for feedback on early drafts; Deyue Zhang and Wei Yin for collecting the data. We also thank all members of 360 Security Research Labs for their support. Among all the contributors, Dr. Chao Shen (PI:EMAIL) and Dr. Yu Chen (PI:EMAIL) are the corresponding authors. Tsinghua University authors are supported in part by the National Natural Science Foundation of China (Grant 61772303), National Key R&D Program of China (Grant 2017YFB0802901). Xi’an Jiaotong University authors are supported in part by the National Natural Science Foundation of China (Grant 61822309, 61773310, and U1736205), the Natural Science Foundation of Shaanxi Province (Grant 2019JQ-084).

### 6. References

[1] adamlerer and soumith. ImageNet training in PyTorch. http://github.com/pytorch/examples/tree/master/imagenet, 2017.

[2] Adnan M. Alattar. Reversible watermark using the difference expansion of a generalized integer transform. IEEE Transactions on Image Processing, 13(8):1147–1156, Aug 2004.

[3] Amazon AWS. Detecting unsafe image. https://docs.aws.amazon.com/rekognition/latest/dg/procedure-moderate-images.html.

[4] Microsoft Azure. Content moderator. https://azure.microsoft.com/en-us/services/cognitive-services/content-moderator/.

[5] beniz. Deep Learning API and Server in C++11 with Python bindings and support for Caffe, TensorFlow, XGBoost, and TSNE. https://github.com/beniz/deepdetect, 2017.

[6] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, L D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning for self-driving cars. arXiv: Computer Vision and Pattern Recognition, 2016.

[7] G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of Software Tools, 2000.

[8] BVLC. BAIR/BVLC GoogleNet Model. http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel, 2017.

[9] M. U. Celik, G. Sharma, A. M. Tekalp, and E. Saber. Lossless generalized-lsb data embedding. IEEE Transactions on Image Processing, 14(2):253–266, Feb 2005.

[10] Alex Clark and Contricutors. Pillow: The friendly Python Imaging Library fork. https://python-pillow.org/, 2018.

[11] Google Cloud. Filtering inappropriate content with the cloud vision api. https://cloud.google.com/blog/products/gcp/filtering-inappropriate-content-with-the-cloud-vision-api.

[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2009.

[13] Martín Abadi et al. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015.

[14] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and Harnessing Adversarial Examples. ArXiv e-prints, December 2014.

[15] Jie Hu and Tianrui Li. Reversible steganography using extended image interpolation technique. Computers and Electrical Engineering, 46:447–455, 2015.

[16] Yangqing Jia. Classifying ImageNet: using the C++ API. https://github.com/BVLC/caffe/tree/master/examples/cpp_classification, 2017.

[17] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.

[18] Ki Hyun Jung and Kee Young Yoo. Data hiding method using image interpolation. Computer Standards and Interfaces, 31(2):465–470, 2009.

[19] Devendra Kumar. REVERSIBLE DATA HIDING USING IMPROVED INTERPOLATION. pages 3037–3048, 2017.

[20] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical world. CoRR, abs/1607.02533, 2016.

[21] Chin Feng Lee and Yu Lin Huang. An efficient image interpolation increasing payload in reversible data hiding. Expert Systems with Applications, 39(8):6712–6719, 2012.

[22] Ian Markwood, Dakun Shen, Yao Liu, and Zhuo Lu. PDF mirage: Content masking attack against information-based online services. In 26th USENIX Security Symposium (USENIX Security 17), pages 833–847, Vancouver, BC, 2017. USENIX Association.

[23] ModerateContent. Realtime image moderation api to protect your community. https://www.moderatecontent.com/.

[24] NVIDIA developers. The latest products and services compatible with the DRIVE Platform of NVIDIA’s ecosystem. https://developer.nvidia.com/drive/ecosystem, 2017.

[25] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. Deepxplore: Automated whitebox testing of deep learning systems. In Proceedings of the 26th ACM Symposium on Operating Systems Principles (SOSP ’17), October 2017.

[26] Ronan, Clément, Koray, and Soumith. Torch: A SCIENTIFIC COMPUTING FRAMEWORK FOR LUAJIT. http://torch.ch/, 2017.

[27] Mingwei Tang, Jie Hu, Wen Song, and Shengke Zeng. Reversible and adaptive image steganographic method. AEU - International Journal of Electronics and Communications, 69(12):1745–1754, 2015.

[28] TensorFlow developers. TensorFlow C++ and Python Image Recognition Demo. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/label_image, 2017.

[29] Torch developers. Tutorials for Torch7. http://github.com/torch/tutorials/tree/master/7_imagenet_classification, 2017.

[30] Xing-Tian Wang, Chin-Chen Chang, Thai-Son Nguyen, and Ming-Chu Li. Reversible data hiding for high quality images exploiting interpolation and direction order mechanism. Digital Signal Processing, 23(2):569 – 577, 2013.

[31] H. Wu, J. Dugelay, and Y. Shi. Reversible image data hiding with contrast enhancement. IEEE Signal Processing Letters, 22(1):81–85, Jan 2015.

[32] Qixue Xiao, Kang Li, Deyue Zhang, and Weilin Xu. Security risks in deep learning implementations. 2018 IEEE Security and Privacy Workshops (SPW), pages 123–128, 2018.

[33] Xinyue Shen, Steven Diamond, Yuantao Gu, and Stephen Boyd. DCCP source code. https://github.com/cvxgrp/dccp, 2017. Accessed 2017-09-03.

[34] Guoming Zhang, Chen Yan, Xiaoyu Ji, Tianchen Zhang, Taimin Zhang, and Wenyuan Xu. Dolphinattack: Inaudible voice commands. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, CCS ’17, pages 103–117, New York, NY, USA, 2017. ACM.

### 7. Proof of Concept of the Scaling Attack

#### 7.1 Software Version and Model Information for Attack Demonstration

Here, we present the software setup for the attack demonstration. Although the example used here targets applications with Caffe, the risk is not limited to Caffe. We have tested the scaling functions in Caffe, TensorFlow, and Torch, and all of them are vulnerable to scaling attacks.

The Caffe package and the corresponding image classification examples were checked out directly from the official GitHub on October 25, 2017, and the OpenCV used was the latest stable version from the following URL: https://github.com/opencv/opencv/archive/2.4.13.4.zip

We used the BAIR/BVLC CaffeNet Model in our proof of concept exploitation. The model is the result of training based on the instructions provided by the original Caffe package. To avoid any mistakes in model setup, we downloaded the model file directly from BVLC’s official GitHub page. Detailed information about the model is provided in the list below.

**Listing 1: Image classification model**

```plaintext
Model: BAIR/BVLC CaffeNet Model
Source: https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_caffenet
```

This structured and detailed presentation should make the text more clear, coherent, and professional.