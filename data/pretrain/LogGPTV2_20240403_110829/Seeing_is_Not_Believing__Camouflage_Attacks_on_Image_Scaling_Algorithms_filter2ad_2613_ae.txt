1000’s of sites to prevent offensive content [23]. An at-
tacker may leverage the scaling attack to evade these con-
tent moderators to spread inappropriate images, which
may raise serious problems in online communities. For
example, suppose an attacker wants to advertise illegal
drugs to users on the iPhone XS. The attacker can use
the scaling attack to create a cloaking effect, so that
the scaling result on the iPhone XS browser is the in-
tended drug image while the image in the original size
contains benign content. Certainly cloaking can also
be achieved by using other approaches such as browser
sensitive Javascript. However, scaling attacks create an
alternative approach as no additional code is used to
manage the rendering effect.
• Fraud by Leveraging Inconsistencies between Dis-
plays. An attacker can create a deceptive digital con-
tract using the scaling attacks. An attacker can create
an image document that contains a scanned contract but
renders to different content when scaling to different
ratios. The attacker can then get two parties to share the
same document. If they each use different browsers, the
content being displayed will be different. This inconsis-
tency can become the basis of potential ﬁnancial fraud
activities.
7 Countermeasures
In this section, we discuss potential defense strategies to miti-
gate the threat from scaling attacks. First, we discuss possible
countermeasures as the attack prevention in the image prepro-
cessing procedure. Second, we discuss some approaches to
detect scaling attacks.
7.1 Attack Prevention
A naive way to avoid the scaling attack is to omit inputs whose
sizes are different from the input size used by the deep learn-
ing models. This approach is appropriate for applications that
deal with the inputs collected by sensors in speciﬁc formats.
However, this strategy is infeasible for many Internet services,
since the input images uploaded by users are often in various
sizes.
Another solution is that we can randomly remove some
pixels (by line or by column) from the image before scal-
ing it. This random cropping operation makes the scaling
coefﬁcient matrices unpredictable, and therefore, it can in-
crease the attack difﬁculty effectively. However, we should
carefully design the pixel removing policy to maintain the
image quality.
7.2 Attack Detection
The scaling attack achieves the deceiving effect by causing
dramatic changes in visual features before and after the scal-
ing action. One potential solution is to detect such obvious
changes of input features during the scaling process, such as
the color histogram and the color scattering distribution.
7.2.1 Color-histogram-based Detection
The color histogram counts the amount of pixels for color
ranges of a digital image. It presents the color distribution in
an image, and is commonly used as a measurement of image
similarity. The main advantage of the color-histogram-based
detection approach is that it can measure the color distribution
change easily and quickly. It is a simple solution when the
data processing speed is the main concern, especially when
the system throughput is high. In our experiments, we convert
the image into grayscale to examine the effectiveness of color-
histogram-based detection, i.e., pixel values ranging from 0
to 255. Eventually, the color histogram of one image can be
represented as a 256-dimension vector vhis, and we adopt the
cosine similarity to measure the color-histogram similarity of
two images shis = cos(vhis
1 ,vhis
2 ).
7.2.2 Color-scattering-based Detection
The color-histogram-based detection can only present a rough
distribution of pixel values, disregarding the color spatial dis-
tribution information. The color scattering could become a
USENIX Association
28th USENIX Security Symposium    455
supplementary to the histogram, which presents the color dis-
tribution measured with the distance between pixels and the
image center. In our experiments, we also convert the image
to grayscale to evaluate the effectiveness the color-scattering-
based detection approach. Speciﬁcally, we calculate the dis-
tance histogram as the color scattering measurement, and
deﬁne a statistical metric to evaluate the similarity: First,
we compute the average distance from pixels which belong
to the same pixel value to the center of the image and we
present the result with a 256-dimension color scattering vec-
tor vscat. Second, we calculate the cosine similarity between
vectors of two images as the color-scattering-based similarity
s = cos(vscat
,vscat
).
1
2
(a) Input (crafted)
(b) Output
(c) Input (original)
(d) Output
(e) CH: Crafted vs. Output
(f) CH: Original vs. Output
(g) CS: Crafted vs. Output
(h) CS: Original vs. Output
Figure 8: The color histograms and color scattering detection
results of the scaling attack in the wolf-in-sheep example.
(CH: color histogram, CS:color scattering)
(a) Color Histograms
(b) Color Scattering
Figure 9: The CCD of color histogram similarity and color
scattering similarity detection.
7.2.3 Evaluation
To evaluate the performance of two attack detection strategies,
we have crafted three attack images for each sourceImg in
the dataset established in Section 6.2, with three 224*224
target images belong to the wolf, human face and cat category.
Before the similarity comparison, we resize the output to the
same size with the input, in order to eliminate the difference
in pixel amount. Fig.8 exhibits the detecting result of a wolf-
in-sheep attack image.
Fig.8e and Fig.8f present the comparison of grayscale his-
tograms between the input images and their scaled output.
The x-axis refers to pixel values ranging from 0 to 255, while
the y-axis refers to the number of pixels with the same value.
From Fig.8f, we can see that the two curves of the origi-
nal input and its scaling output almost coincide, where the
similarity is 0.96. In the meanwhile, we can see an obvious
difference between the color distribution of the attack image
and its scaling output, where the similarity is 0.50.
Fig.8g and Fig.8h present the comparison of grayscale color
scattering measurement. The x-axis refers to pixel values
ranging from 0 to 255, while the y-axis refers to the average
distance between the image center and pixels with the same
value. Similarly, we can see an obvious difference in the color
scattering measurement of the attack image and its scaling
output.
Fig.9 reports the complementary cumulative distribution
(CCD) of the detection results of our test set. The legend
“original-resize”, “ds-wolf”, “ds-face” and “ds-cat” refer to
the original-image, wolf-as-target, human-face-as-target and
cat-as-target case, respectively. We can observe that for both
two detection metrics, the similarity between original images
and their scaling outputs is obviously higher than that between
attack images and their scaling outputs. The result indicates
the two attack detection strategies work well in most cases.
8 Conclusion
This paper presents a camouﬂage attack on image scaling
algorithms, which is a potential threat to computer vision ap-
456    28th USENIX Security Symposium
USENIX Association
Average DistanceAverage Distanceplications. By crafting attack images, the attack can cause
the visual semantics of images change signiﬁcantly during
scaling. We studied popular deep learning frameworks and
showed that most of their default scaling functions are vulner-
able to such attack. Our results also exhibit that even though
cloud services (such as Microsoft Azure, Baidu, Aliyun and
Tencent) hide the scaling algorithms and input scales, attack-
ers can still achieve the deceiving effect. The purpose of this
work is to raise awareness of the security threats buried in
the data processing pipeline in computer vision applications.
Compared to the intense interests in adversarial examples, we
believe that the scaling attack is more effective in creating
misclassiﬁcation because of the deceiving effect it can create.
Acknowledgments
We thank our shepherd Dr. David Wagner and all anonymous
reviewers for their insightful suggestions and comments to
improve the paper; Dr. Jian Wang and Dr. Yang Liu for feed-
back on early drafts; Deyue Zhang and Wei Yin for collect-
ing the data. We also thank all members of 360 Security
Research Labs for their support. Among all the contribu-
tors, Dr. Chao Shen (PI:EMAIL) and Dr.
Yu Chen (PI:EMAIL) are the correspond-
ing authors. Tsinghua University authors are supported in part
by the National Natural Science Foundation of China (Grant
61772303), National Key R&D Program of China (Grant
2017YFB0802901). Xi’an Jiaotong University authors are
supported in part by the National Natural Science Founda-
tion of China (Grant 61822309, 61773310, and U1736205),
the Natural Science Foundation of Shaanxi Province (Grant
2019JQ-084).
References
[1] adamlerer and soumith. ImageNet training in PyTorch.
http://github.com/pytorch/examples/tree/mas
ter/imagenet, 2017.
[2] Adnan M. Alattar. Reversible watermark using the dif-
ference expansion of a generalized integer transform.
IEEE Transactions on Image Processing, 13(8):1147–
1156, Aug 2004.
[3] Amazon AWS. Detecting unsafe image. https://do
cs.aws.amazon.com/rekognition/latest/dg/pr
ocedure-moderate-images.html.
[4] Microsoft Azure. Content moderator. https://azure.
microsoft.com/en-us/services/cognitive-ser
vices/content-moderator/.
[5] beniz. Deep Learning API and Server in C++11 with
Python bindings and support for Caffe, Tensorﬂow, XG-
Boost and TSNE. https://github.com/beniz/dee
pdetect, 2017.
[6] Mariusz Bojarski, Davide Del Testa, Daniel
Dworakowski, Bernhard Firner, Beat Flepp, Pra-
soon Goyal, L D Jackel, Mathew Monfort, Urs Muller,
Jiakai Zhang, et al. End to end learning for self-driving
cars. arXiv: Computer Vision and Pattern Recognition,
2016.
[7] G. Bradski. The OpenCV Library. Dr. Dobb’s Journal
of Software Tools, 2000.
[8] BVLC. BAIR/BVLC GoogleNet Model. http://dl.c
affe.berkeleyvision.org/bvlc_googlenet.caf
femodel, 2017.
[9] M. U. Celik, G. Sharma, A. M. Tekalp, and E. Saber.
Lossless generalized-lsb data embedding. IEEE Trans-
actions on Image Processing, 14(2):253–266, Feb 2005.
[10] Alex Clark and Contricutors. Pillow: The friendly
Python Imaging Library fork. https://python-p
illow.org/, 2018.
[11] Google Cloud. Filtering inappropriate content with the
cloud vision api. https://cloud.google.com/blo
g/products/gcp/filtering-inappropriate-con
tent-with-the-cloud-vision-api.
[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In 2009 IEEE Conference on Com-
puter Vision and Pattern Recognition, 2009.
[13] Martín Abadi et al. TensorFlow: Large-scale machine
learning on heterogeneous systems, 2015.
[14] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining
and Harnessing Adversarial Examples. ArXiv e-prints,
December 2014.
[15] Jie Hu and Tianrui Li. Reversible steganography using
extended image interpolation technique. Computers and
Electrical Engineering, 46:447–455, 2015.
[16] Yangqing Jia. Classifying ImageNet: using the C++
API. https://github.com/BVLC/caffe/tree/mas
ter/examples/cpp_classification, 2017.
[17] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey
Karayev, Jonathan Long, Ross Girshick, Sergio Guadar-
rama, and Trevor Darrell. Caffe: Convolutional ar-
chitecture for fast feature embedding. arXiv preprint
arXiv:1408.5093, 2014.
[18] Ki Hyun Jung and Kee Young Yoo. Data hiding method
using image interpolation. Computer Standards and
Interfaces, 31(2):465–470, 2009.
USENIX Association
28th USENIX Security Symposium    457
[19] Devendra Kumar. REVERSIBLE DATA HIDING US-
ING IMPROVED INTERPOLATION. pages 3037–
3048, 2017.
[31] H. Wu, J. Dugelay, and Y. Shi. Reversible image data
hiding with contrast enhancement. IEEE Signal Pro-
cessing Letters, 22(1):81–85, Jan 2015.
[20] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio.
Adversarial examples in the physical world. CoRR,
abs/1607.02533, 2016.
[21] Chin Feng Lee and Yu Lin Huang. An efﬁcient im-
age interpolation increasing payload in reversible data
hiding. Expert Systems with Applications, 39(8):6712–
6719, 2012.
[22] Ian Markwood, Dakun Shen, Yao Liu, and Zhuo
PDF mirage: Content masking attack against
Lu.
In 26th USENIX
information-based online services.
Security Symposium (USENIX Security 17), pages 833–
847, Vancouver, BC, 2017. USENIX Association.
[23] ModerateContent. Realtime image moderation api to
protect your community. https://www.moderateco
ntent.com/.
[24] NVIDIA developers. the latest products and services
compatible with the DRIVE Platform of NVIDIA’s
ecosystem . https://developer.nvidia.com/d
rive/ecosystem, 2017.
[25] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana.
Deepxplore: Automated whitebox testing of deep learn-
ing systems.
In Proceedings of the 26th ACM Sym-
posium on Operating Systems Principles (SOSP ’17),
October 2017.
[26] Ronan, Clément, Koray, and Soumith. Torch: A SCIEN-
TIFIC COMPUTING FRAMEWORK FOR LUAJIT.
http://torch.ch/, 2017.
[27] Mingwei Tang, Jie Hu, Wen Song, and Shengke Zeng.
Reversible and adaptive image steganographic method.
AEU - International Journal of Electronics and Commu-
nications, 69(12):1745–1754, 2015.
[28] Tensorﬂow developers. TensorFlow C++ and Python
Image Recognition Demo. https://www.github.com
/tensorflow/tensorflow/tree/master/tensorf
low/examples/label_image, 2017.
[29] Torch developers. Tutorials for Torch7. http://gith
ub.com/torch/tutorials/tree/master/7_image
net_classification, 2017.
[30] Xing-Tian Wang, Chin-Chen Chang, Thai-Son Nguyen,
and Ming-Chu Li. Reversible data hiding for high qual-
ity images exploiting interpolation and direction order
mechanism. Digital Signal Processing, 23(2):569 – 577,
2013.
[32] Qixue Xiao, Kang Li, Deyue Zhang, and Weilin Xu.
Security risks in deep learning implementations. 2018
IEEE Security and Privacy Workshops (SPW), pages
123–128, 2018.
[33] Xinyue Shen, Steven Diamond, Yuantao Gu, and
Stephen Boyd. DCCP source code. https://gith
ub.com/cvxgrp/dccp, 2017. Accessed 2017-09-03.
[34] Guoming Zhang, Chen Yan, Xiaoyu Ji, Tianchen Zhang,
Taimin Zhang, and Wenyuan Xu. Dolphinattack: Inaudi-
ble voice commands. In Proceedings of the 2017 ACM
SIGSAC Conference on Computer and Communications
Security, CCS ’17, pages 103–117, New York, NY, USA,
2017. ACM.
A Proof of Concept of the Scaling Attack
A.1 Software Version and Model Information
for Attack Demonstration
Here we present the software setup for the attack demonstra-
tion. Although the example used here targets applications
with Caffe, the risk is not limited to Caffe. We have tested
the scaling functions in Caffe, TensorFlow and Torch. All of
them are vulnerable to scaling attacks.
The Caffe package and the corresponding image classi-
ﬁcation examples were checked out directly from the of-
ﬁcial GitHub on October 25, 2017, and the OpenCV used
was the latest stable version from the following URL:
https://github.com/opencv/opencv/archive/2.4.13.4.zip
We used the BAIR/BVLC CaffeNet Model in our proof of
concept exploitation. The model is the result of training based
on the instructions provided by the original Caffe package. To
avoid any mistakes in model setup, we download the model
ﬁle directly from BVLC’s ofﬁcial GitHub page. Detailed
information about the model is provided in the list below.
Listing 1: Image classiﬁcation model