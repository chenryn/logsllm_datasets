Based on Table I, we only need to apply the early-timeout for
temp-mail and guerrillamail to discard lower-ranked
usernames, using a timeout of 1 hour and 3 hours respectively.
B. Disposable Email Dataset
We applied the crawler to 7 disposable email services from
October 16, 2017 to January 16, 2018 for three months. In
total, we collected 2,332,544 email messages sent to mon-
itored email addresses. Our crawler is implemented using
Selenium [7] to control a headless browser to retrieve email
content. The detailed statistics are summarized in Table II.
For 5 of the disposable email services, we can cover all 10K
addresses and almost all of them have received at least one
email. For the other 2 email services with very a short expi-
ration time (temp-mail and guerrillamail), we focus
on an abbreviated version of the popular usernames list. The
number of emails per account has a highly skewed distribution.
About 48% of disposable email addresses received only one
email, and 5% of popular addresses received more than 100
emails each.
Each email message is characterized by an email title, email
body, receiver address (disposable email address), and sender
address. As shown in Table II, not all emails contain all the
ﬁelds. 4 of the 7 disposable email services do not always
keep the sender email addresses. Sometimes the disposable
email services would intentionally or accidentally drop sender
addresses. In addition, spam messages often omit the sender
address in the ﬁrst place. In total, there are 1,290,073 emails
(55%) containing a sender address (with a total of 452,220
unique sender addresses). These sender addresses correspond
to 210,373 unique sender domain names. From the email body,
we extracted 13,396,757 URLs (1,031,580 unique URLs after
removing URL parameters).
TABLE II: Statistics of the collected datasets.
Website
guerrillamail
mailinator
temp-mail
maildrop
mailnesia
mailfall
mailsac
Total
# Emails
1,098,875
657,634
198,041
150,641
106,850
75,179
45,324
2,332,544
Dispos.
Address
1,138
10,000
5,758
9,992
9,983
9,731
9,987
56,589
Uniq. Sender
Address (Domain)
410,457 (190,585)
27,740 (16,342)
1,748 (1,425)
786 (613)
1,738 (686)
3,130 (288)
11,469 (8,019)
452,220 (210,373)
Msgs w/
Sender Address
1,091,230 (99%)
55,611 (8%)
13,846 (7%)
3,950 (3%)
4,957 (5%)
75,164 (100%)
45,315 (100%)
1,290,073 (55%)
the Dataset.
Biases of
This dataset provides a rare
opportunity to study disposable email services and email
tracking. However, given the data collection method,
the
dataset inevitably suffers from biases. We want to clarify these
biases upfront to provide a more accurate interpretation of the
analysis results later. First, our dataset only covers the user-
speciﬁed addresses but not the randomly-assigned addresses.
Second, our data collection is complete with respect to the
popular email addresses we monitored, but is incomplete with
respect to all the available addresses. As such, any “volume”
metrics can only serve as a lower bound. Third, we don’t claim
the email dataset is a representative sample of a “personal
inbox”. Intuitively, users (in theory) would use disposable
email addresses differently relative to their personal email
addresses. Instead, we argue the unique value of this dataset
is that it covers a wide range of online services that act as the
email senders. The data allows us to empirically study email
tracking from the perspective of online services (instead of
the perspective of email users). It has been extremely difﬁcult
(both technically and ethically) for researchers to access and
analyze the email messages in users’ personal inboxes. Our
dataset, obtained from public email gateways, allows us to
take a ﬁrst step measuring the email tracking ecosystem.
C. Ethical Considerations and IRB
We are aware of the sensitivity of the dataset and have
taken active steps to ensure research ethics: (1) We worked
closely with IRB to design the study. Our study was reviewed
by IRB and received an exemption. (2) Our data collection
methodology is designed following a prior research study on
disposable SMS services [41]. Like previous researchers, we
carefully have controlled the crawling rate to minimize the
impact on the respective services. For example, we enforce a
1-second break between queries and explicitly use a single-
thread crawler for each service. (3) All the messages sent
to the gateways are publicly available to any Internet users.
Users are typically informed that other users can also view the
emails sent to these addresses. (4) We have spent extensive
efforts on detecting and removing PII and personal emails
from our dataset (details in §IV-A). (5) After data collection,
we made extra efforts to reach out to users and offer users
the opportunity to opt out. More speciﬁcally, we send out
an email to each of the disposable email addresses in our
dataset, to inform users of our research activity. We explained
the purpose of our research and offered the opportunity for
users to withdraw their data. So far, we did not receive
any data withdraw request. (6) Throughout our analysis, we
(cid:20)(cid:23)(cid:25)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:42:37 UTC from IEEE Xplore.  Restrictions apply. 
did not attempt to analyze or access any individual accounts
registered under the disposable email addresses. We also did
not attempt to click on any URLs in the email body (except
the automatically loaded tracking pixels). (7) The dataset is
stored on a local server with strict access control. We keep
the dataset strictly to ourselves.
Overall, we believe the analysis results will beneﬁt the
community with a deeper understanding of disposable email
services and email tracking, and inform better security prac-
tices. We hope the results can also raise the awareness of the
risks of sending sensitive information over public channels.
IV. ANALYZING DISPOSABLE EMAILS
In this section, we analyze the collected data to understand
how disposable email services are used in practice. Before
our analysis, we ﬁrst detect and remove PII and the potential
personal emails from the dataset. Then we classify emails into
different types and infer their use cases. More speciﬁcally, we
want to understand what types of online services with which
users would register. Further, we seek to understand how likely
it is for disposable email services to be used in sensitive tasks
such as password resets.
A. Removing PII and Personal Emails
Removing PII.
Since email messages sent to these gate-
ways are public, we suspect careless users may accidentally re-
veal their PII. Thus, we apply well-established methods to de-
tect and remove the sensitive PII from the email content [49].
Removing PII upfront allows us to analyze the dataset (includ-
ing manual examination) without worrying about accidentally
browsing sensitive user information. Here, we brieﬂy introduce
the high-level methodology and refer interested readers to [49]
for details. The idea is to build a list of regular expressions
for different PII. We ﬁrst compile a ground-truth dataset to
derive regular expressions and rules. Like [49], we also use
the public Enron Email Dataset [8] which contains 500K
emails. We focused on the most sensitive PIIs and labeled a
small ground-truth set for credit card numbers, social security
numbers (SSN), employer identiﬁcation numbers (EIN), phone
numbers, and vehicle identiﬁcation numbers (VIN) as shown
in Table III. Then we build regular expressions for each PII
type. For credit card numbers, we check the preﬁx for popular
credit card issuers such as VISA, Mastercard, Discover and
American Express, and we also use Luhn algorithm [32] to
check the validity of a credit card number. As shown in
Table III, the regular expressions have good precision and
recall.
We applied the regular expressions to our dataset and
detected a large number of PIIs including 1,399 credit card
numbers, 926 SSNs, 701 EINs, and 40K VINs and 700K
phone numbers. All the detected PII are automatically blacked-
out by the scripts. Note that the 700K phone numbers are
not necessarily users’ personal phone numbers, but can be
phone numbers of the email sending services. We take a
conservative approach to blackout all the potential PII. The
TABLE III: PII detection accuracy based on ground-truth, and
the number of detected PII instances in our dataset.
PII
Type
Credit
SSN
EIN
Phone
VIN
#Email
16
13
16
20
15
Ground-truth Evaluation
Precis.
#Inst.
1.00
25
1.00
15
1.00
29
50
0.98
1.00
19
F1
1.00
1.00
1.00
0.99
0.97
Recall
1.00
1.00
1.00
1.00
0.95
# Detected in
Our Data
1,399
926
701
726,138
43,438
results indicate that people indeed use the disposable email
services to communicate sensitive information.
Removing Personal Emails. We further remove potentially
personal emails including replied emails and forwarded emails.
We ﬁlter these emails based on “Re: ” and “Fwd: ” in the
email titles. Although this step may not be complete, it helps
to delete email conversations initiated by the users. In total, we
ﬁlter out 30,955 such emails (1.33%). This again shows use
of disposable email addresses for personal communications.
B. Categorizing Disposable Emails
Next, using the remaining data, we infer the common
use cases of disposable email services by classifying email
messages. First, we manually analyze a sample of emails
to extract the high-level categories of emails (ground-truth
dataset). Second, we build a machine learning classiﬁer and
use it to classify the unlabeled emails. Third, we analyze the
classiﬁcation results to examine common usage cases.
Manual Analysis and Email Clustering.
To assist the
manual analysis, we ﬁrst cluster similar email messages to-
gether. For efﬁciency considerations, we only consider the
subject (or title) of the email message for the clustering.
Since we don’t know the number of clusters in the dataset,
we exclude clustering methods that require pre-deﬁning the
number of clusters (e.g., K-means). Instead, we use ISODATA
algorithm [13] which groups data points based on a cut-
off threshold of the similarity metric. We use Jaccard index
to measure the keyword similarity of two email subjects.
Given two email subjects, we extract all their keywords into
two sets wi and wj. Then we calculate their similarity as
sim(i, j) =
|wi∩wj|
|wi∪wj|.
We set the cut-off threshold as 0.2 to loosely group similar
email titles together. In total, we obtain 91,306 clusters, most
of which are small with less than 100 emails (98%). The
cluster size distribution is highly skewed. The top 500 clusters
cover 56.7% of the total email messages. A few large clusters
(with over 1000 emails) typically represent spam campaigns.
To make sure 0.2 is a reasonable threshold, we have tried even
smaller thresholds to merge some of the clusters. For example,
if we set the threshold to 0.1 and 0.01, we get 26,967 and
19,617 clusters respectively. However, manual examination
shows that the emails in the same cluster no longer represent
a meaningful group. We stick to 0.2 as the threshold. By
manually examining 500+ clusters (prioritizing larger ones),
we summarize 4 major types of emails.
(cid:20)(cid:23)(cid:26)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:42:37 UTC from IEEE Xplore.  Restrictions apply. 
• Account Registration: emails to conﬁrm account regis-
tration in online services.
• Password Reset: emails that instruct the user to reset
passwords for an online account.
• Authentication: emails that contain a one-time authenti-
cation code for login.
• Spam: all other unsolicited emails including newsletters,
advertisements, notiﬁcations from online services, and
phishing emails.
Email Classiﬁcation. We need to further develop an email
classiﬁer because the clusters do not map well to each of the
email categories. For example, a cluster may contain both
spam emails and emails that are used to conﬁrm account
registration. Below, we build a machine learning classiﬁer to
classify emails into the four categories.
For classiﬁer training, we manually labeled a ground-truth
dataset of 5,362 emails which contains 346 account regis-
tration emails, 303 password reset emails, 349 authentication
emails and 4,364 spam emails. Note that we have labeled more
spam emails than other categories because our manual exam-
ination suggests that there are signiﬁcantly more spam emails
in the dataset. For each email, we combine the text in the email
title and the email body, and apply RAKE (Rapid Automatic
Keyword Extraction) [44] to extract a list of keywords. RAKE
is a domain independent keyword extraction algorithm based
on the frequency of word appearance and its co-occurrence
with other words. In this way, less distinguishing words such
as stopwords are automatically ignored. We use extracted
keywords as features to build a multi-class SVM classiﬁer.
We have tested other algorithms such as Decision Tree and
Random Forests. However, the SVM performed the best. We
also tested word2vector [35] to build the feature vector, and
its results are not as good as RAKE (omitted for brevity).
Note that
Through 5-fold cross-validation, we obtain a precision of
97.23% and a recall of 95.46%. This is already highly accurate
for a multi-class classiﬁer — as a baseline, a random classi-
ﬁcation over 4 classes would return an accuracy of 25%. We
manually checked some of the classiﬁcation errors, and found
that a few account registration and authentication emails are
labeled as spam due to “spammy” keywords (e.g., “purchase”).
two types of emails are not applicable here.
First, 58,291 (2.50%) of the emails do not have any text
content. Second, 535,792 (22.97%) emails are not written
in English. Since our classiﬁer cannot analyze the text of
these emails, they are not part of the classiﬁcation results in
Figure 2 (we still consider these emails in the later analysis
of email tracking). To make sure our classiﬁcation results
are trustworthy, we randomly sampled 120 emails (30 per
category) to examine manually. We only ﬁnd 5 misclassiﬁed
emails (4% error rate), which shows that the ground-truth
accuracy transfers well onto the whole dataset.
C. Inferring Usage Cases
Next, we examine disposable email service usage. Recall
that our dataset contains emails received by the disposable
Spam 1,612,361 (94.75%)
Registration 61,812 (3.63%)
Password Reset 14,715 (0.86%)
Authentication 12,802 (0.75%)
Fig. 2: Email classiﬁcation results.
email addresses. Intuitively, after the users obtain the dispos-
able email addresses, they will use the email addresses for
certain online tasks (e.g., registering accounts), which will ex-
pose the addresses and attract incoming emails. By analyzing
these incoming emails, we can infer at which services the user
registered the accounts, and what the accounts are used for.
Types of Emails.
As shown in Figure 2, while spam
emails take the majority, there is a non-trivial number of emails
that are related to account management in various online ser-
vices. In total, there are 89,329 emails involved with account
registration, password resets or sending authentication codes.
These emails are sent from 168,848 unique web domains.
We refer these 3 types of emails as account management
emails. Account management emails are indicators of previous
interactions between the user and the email sending domain.
They are explicit evidence that users have used the disposable
email addresses to register accounts in the web services.
Breakdown of Spam Emails.
The spam emails take a large
portion of our dataset (1,612,361 emails, 94%), which deserve
a more detailed break-down. Some of the spam messages also
indicate previous interactions between a user and the email
sender. For example, if a user has registered an account or
RSS at an online service (e.g. Facebook), this service may
periodically send “social media updates”, “promotions”, or
notiﬁcations to the disposable email address. We call them
notiﬁcation spam. Such notiﬁcation messages almost always
include an unsubscribe link at the bottom of the email to allow
users to opt out. As such, we use this feature to scan the spam
messages and ﬁnd 749,602 notiﬁcation messages (counting for
46.5% of the spam messages).
The rest of unsolicited spam messages may come from
malicious parties, representing malware or phishing cam-
paigns. To identify the malicious ones, we extract all the
clickable URLs from the email content, and run them against
the VirusTotal blacklists (which contains over 60 blacklists
maintained by different security vendors [41], [11]), and the
eCrimeX blacklist (a phishing blacklist maintained by the Anti
Phishing Work Group). In total, we identify 84,574 malicious