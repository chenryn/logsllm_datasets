Eˆχ0
iD0
i
˜ +
X
e=(i,j)∈E0
  T 0
i,j
t
˜!
EˆD0
e
E[D0] =
iLD0
iRD0
iL + χ0
We assume that that the time needed to process requests at a node
i ∈ V is equal to the combined time required at nodes iL and iR,
i.e., χ0
iR = χiDi. While partitioning servers after
migration could potentially lead to increased service times at nodes,
we assume for tractability that the service times remain unchanged.
Then, it follows that the increase in mean delay, i.e., E[D0] −
X
E[D], can be written as:
„ Tij
  T 0
«
˜!
− X
EˆD0
E [De]
.
e
e=(i,j)∈E0
ij
t
e=(i,j)∈E
t
MigrateCjTi, jCiC i LCj LCi RCj RT'i R, j LT'i L, j RT'i L, j LT'i R, j RLocal data-centerLocal data-centerCloud246• Variance of transaction delays: We limit our discussion of
variance to transactions that may be modeled as a path of compo-
nents. We believe this is a reasonable starting point, and a majority
of transactions can be modeled in this fashion. In the future, this
assumption may be relaxed by modeling the ﬂow of transactions in
the network as a Markov chain.
The variance of transaction delay may be computed by consider-
ing the conditional variance of D given a path i is taken:
VAR[D] = Ei [VAR[D | i]] + VARi [E [D | i]]
Figure 4: ACL placement in a data center and the corresponding reacha-
bility matrix if f e1 is migrated.
Let DPi be a RV corresponding to the delay of a transaction of type
i which involves path Pi, and let ti be the number of transactions
involving path Pi. Then, we can show that:
`VAR [DPi] + E [DPi]2´ − E[D]2
X
VAR[D] =
(7)
ti
t
i
VAR [DPi] may be computed as the sum of the variances of the
delays encountered at the nodes and links on the path (D0
is), as-
suming these random variables are independent. Applying queuing
models of enterprise applications (e.g., [26]) is challenging in our
context given the system after migration involves optimization vari-
ables (such as the new transaction matrix). While our evaluations
indicate that the migration decisions predicted by our model work
well (§5), and the applications we consider are typically overpro-
visioned, it would be interesting to incorporate queuing models in
our framework in the future.
At ﬁrst glance, it may appear that computing the variance re-
quires detailed knowledge of the paths taken by individual trans-
actions. Such information may be difﬁcult to obtain if the number
of possible transaction paths is large. However, we have been able
to simplify Equation 7 to show that it sufﬁces to know the number
of transactions involving any pair of components or links. We be-
lieve such information can be obtained in many realistic settings,
and possibly even derived from Tij values for certain types of ap-
plication topologies.
Percentiles of transaction delays: A constraint of optimization
may be to minimize the change in percentile value of transaction
delay. Accurate estimates of delay percentiles is challenging and
requires detailed knowledge of delay distributions of individual
random variables. Instead, we estimate the delay percentile for a
particular scenario using Chebyshev’s inequality. The inequality
states that no more than 1/k2 of the values of any arbitrary dis-
tribution can be more than k standard deviations away from the
mean. By estimating the mean and variance as above, and by using
Chebyshev’s inequality, a bound may be obtained on any percentile
of transaction delay, which could then be fed into the optimization.
3.5 Modeling beneﬁts of migration
There are several factors that can enable enterprises to reduce their
costs as they migrate to the cloud. First, large cloud data centers
can purchase hardware, network bandwidth, and power at much
lower prices than medium-sized data-centers, and pass on these
economies of scale to enterprises. Second, moving to the cloud
potentially helps lower operational expenses through workforce re-
ductions. Finally, the ability to dynamically request additional server
resources to provision for peak workloads frees enterprises from
the need to provision for worst-case scenarios.
In this paper, we consider a simple model for measuring bene-
ﬁts, which serves as a useful starting point for evaluating migration
trade-offs. We assume that there are two primary classes of compo-
nents: (i) compute-intensive; and (ii) storage-intensive. We assume
that the beneﬁts of migrating a single server in each class is Bc and
Bs respectively. Let Mc and Ms be the total number of compo-
nents in each class migrated. Then, we compute the total beneﬁts
of migration as BcMc + BsMs. While we assume the beneﬁts for
migrating all servers in a class is the same, the model could be eas-
ily extended to consider heterogeneity in beneﬁts across servers in
each class, which may arise for instance due to the the age of the
hardware already in place in the enterprise.
Estimating the beneﬁts per server, Bc and Bs, is in general non-
trivial, and is potentially dependent on the particular enterprise
and choice of cloud provider. An infrastructure-as-a-service of-
fering like EC2 [1], for instance, might not obviate the need for
database adminstrators unlike a platform-as-a-service offering such
as Azure [8]. While our evaluations rely on generic cost savings
estimates (e.g.,
[14]), we envision that in practice, more precise
factors may be provided by individual enterprise managers taking
site-speciﬁc considerations into account.
In this paper, we focus on the recurring costs of migration, such
as server and wide-area Internet communication costs. Executing
the migration process may involve one-time costs, such as the ef-
fort in acquiring model parameters, and reengineering applications
for cloud deployment. Comparing one-time costs with recurring
costs brings in issues such as discounting which are speciﬁc to en-
terprises. With appropriate discounting factors available, one-time
costs can be easily incorporated in the model.
3.6 Solving the optimization problems
If a ﬂexible routing approach (§3.2) is used, and only constraints
involving changes to mean delay are considered, our optimization
problem corresponds to an integer programming problem. Though
integer programming problems are hard to solve, well-known tools
like CPLEX [6] are available to handle them. Constraints involving
variance (and percentiles) of transaction delay, or use of an inde-
pendent routing approach (§3.2) lead to non-linear problem formu-
lations. While analytically tractable solutions are difﬁcult to obtain
with non-linear optimization problems, recent theoretical advances
have led to the development of tools such as BARON [20], which
we leverage. While such tools are effective, scaling to large prob-
lem formulations may require tighter relaxations that constrain the
search space. We do not explore these issues further in this paper,
and defer them to future work.
4 Migrating reachability policies
While §3 presented a framework to help decide which servers should
be migrated to the cloud, an important challenge that must be ad-
dressed is how security policies must be reconﬁgured. We discuss
our approach to tackling this challenge in this section.
4.1 Abstraction and problem formulation
The key requirement of reachability policy migration is to ensure
correctness – if a packet between two nodes is permitted (denied)
prior to migration, it must be permitted (denied) after migration.
To aid our discussion, consider Fig. 4, which shows a data cen-
ter. Consider an application with one front-end component F E,
and two back-end components, BE1, and BE2. Assume F E has
two servers f e1 and f e2. Assume the servers in each component
a3a3fe1a1∩a2a2a2fe1a1∩a3a1∩a3a1∩a2INTa3a2BE2a3a2BE1a3a3fe2INTBE2BE1fe2(b)ReachabilityMatrix (Rold)BE2BRARARARARBE1a3a3a2ACL a1a2(a) Local Data CenterInternet (INT)BR = Border Router, AR = Access Routerfe2FEfe1migrate2474.2.1 Granularity of policies in R
The naive approach to representing policies in R involves operat-
ing at the granularity of cells, i.e., computing the effective ACL
corresponding to each pair of entities. In fact such an approach has
been used in [24]. However, this naive approach can lead to an
explosion of rules, and consequently does not scale well to large-
sized networks (see §5.3.3). Many reachability policies apply to
sets of entities. Representing each cell as ACL rules unrolls such a
structure and hence could signiﬁcantly increase the number of rules
required to be installed, adding extra processing overhead which
could degrade network performance after migration.
We instead adopt an alternate approach which preserves infor-
mation regarding common entity pairs affected by each ACL. For
each ACL a, we deﬁne its ﬁlter domain Fa to be the set of origin-
destination (OD) entity communication pairs (i, j) (i.e., all packets
from ei to ej) that are ﬁltered by a. For instance, the shaded cells
in Fig. 4(b) form the ﬁlter domain for ACL a3. By maintaining the
ﬁlter domain associated with each ACL, and exploiting this infor-
mation, our algorithms strive to prevent a signiﬁcant increase in the
number of rules, as we describe in later subsections.
4.2.2 Deriving Rnew
The algorithm starts by inferring Rold from the LDC. The reach-
ability policies governing trafﬁc from ei to ej (i.e., Rold(i, j) )
contain the set of ACLs encountered along the default routing path
from ei to ej. If no ACLs lie on the path, the cell is empty, indicat-
ing that ei has full reachability to ej.
The IP addresses of entities may change when they are moved to
the cloud. Therefore, policies in Rold should be correctly trans-
lated based on the new address assignment. For example, consider
Fig. 4. If f e1 (IP address ipold
f e1) was permitted to talk to port 8000
of all servers in entity BE1 then the same communication from
f e1 to BE1 should still be allowed after f e1 is migrated. We de-
ipnew
rive Rnew by applying a transformation function t on every ACL.
The function t revises ACL rules to avoid inconsistent ﬁltering de-
cisions due to address reassignment. Fig. 6 shows Rnew for the
migration scenario in Fig. 4.
4.2.3 Partitioning Rnew
While trafﬁc between a pair of entities can be ﬁltered anywhere on
their communication path without violating the correctness criteria,
it is desirable to ﬁlter unwanted trafﬁc before it enters the Internet.
In particular, trafﬁc between the LDC and the CDC should be ﬁl-
tered by the originating data center, rather than by the destination
data center.
To achieve this, we identify for each data center DC, all OD-
pairs that should be ﬁltered at DC. We refer to this set of OD-pairs
as the ﬁlter zone Z DC of data center DC. Z DC consists of (i)
OD-pairs that originate from DC and (ii) OD-pairs that originate
from the Internet and are destined to DC. For example, Rnew
in Fig. 6 is partitioned into Z LDC (gray cells) and Z CDC (dotted
cells). Each partition corresponds respectively to the OD-pairs (or
cells) that should be ﬁltered by the LDC and the CDC.
4.2.4
For each ﬁlter zone, the relevant ACLs must be placed and the cor-
responding new ACL conﬁgurations must be generated. We de-
scribe the key steps below:
Submatrix Extraction: Let Fa(DC) denote the ﬁlter domain of
ACL a within Z DC. The goal of this step is to ensure that ACL a
is placed between every OD-pair ∈ Fa(DC) in the migrated net-
work setting. Our overall approach is to break Fa(DC) into groups
of cells that involve disjoint sets of source and destination entities.
Installing Rnew
Figure 5: Overview of ACL migration algorithm.
correspond to a separate VLAN (though we note that in general
severs in multiple components may be part of a VLAN). The ﬁg-
ure also shows 3 ACLs (a1–a3). Consider a scenario where f e1 is
migrated to the cloud, and (possibly) assigned a new IP address.
We deﬁne an entity e to be an atomic unit during the migration
process, i.e., all constituent servers of an entity either migrate or do
not. While at the ﬁnest granularity, each application server could
be considered as an entity, we prefer to model an entire VLAN as
an entity when possible. As we will see, using a coarser represen-
tation when possible will ensure that our algorithm scales better.
In Fig. 4, we model servers f e1, and f e2 as separate entities, and
entire VLANs BE1, and BE2 as entities.
Consider a network with N entities {e1, e2,··· , eN }. Let the
reachability matrix, denoted by R, be an N by N matrix, where
each cell R(i, j) captures the reachability set, or the subset of pack-
ets (from the universe of all IP packets) that the network may carry
from ei to ej [29]. Fig. 4(b) shows the reachability matrix for the
scenario in Fig. 4(a). Note that we treat the Internet as an entity.
Let Rold and Rnew respectively denote the reachability matri-
ces prior to, and after a migration. Let p denote a packet typically
identiﬁed by the 5-tuple (sip, dip, sp, dp, proto), i.e., source/des-
tination IPs and ports, and protocol. Let m deﬁne a mapping from
every IP address in the old setting to the new one, i.e., ipnew
=
are the old and new IP addresses
m(ipold
of ei respectively. We deﬁne the correctness criteria below:
), where ipold
i
i
and ipnew
i
i
1. ∀i, j, p∈Rold(i, j) iff pnew∈Rnew(i, j),
where p=(sip,dip,sp,dp,proto) and
pnew=(m(sip),m(dip),sp,dp,proto)
2. Rnew is invariant under failures in the new topology.
A desirable criterion when migrating reachability policies is to avoid