matter how high the order of the Markov model, there will
always exist patterns of all identical symbols (albeit occur-
ring at small probabilities), so that we cannot tell if the
leading symbol of such a pattern is at an odd or even index
to determine the next data point following the pattern, i.e.,
whether it should be identical to the previous data point or
it should be picked randomly from the range of [0; 1].
Our use of this dataset with repeating symbols gives us
a ready method to control the degree of correlation existing
between the data points of the original trace and those of
the perturbed trace.
Traces generated by Markov model. Data points in
this trace set are also limited to the range of integers [0; 1].
They are generated by the order-2 Markov model shown in
Fig. 3. Notice that each data value is repeated at least once
in this trace. f1; 1; 1; 0; 0; 0; 0; 1; 1; :::g is an example trace.
This dataset allows us to control the order of data correlation
in the original trace used to test a privacy measure.
6.1 Axiomatic properties
We postulate two axiomatic properties for a desirable pri-
vacy measure:
1. The measure accounts for all available information in
a privacy attack.
2. The measure quanti(cid:12)es how the privacy protection weak-
ens as the adversary exploits progressively larger scopes
of data correlation present in the data. The strength
of protection should stabilize at some scope k when
applied to time-series data generated by an order-l
Markov model.
6.2 Protection strategies
We now evaluate the four measures we proposed in Sec. 5
against the axiomatic properties we presented above. We
consider four strategies for protecting the synthetic data:
(cid:15) No data point is perturbed. This acts as a baseline
when there is no protection at all.
(cid:15) Randomly perturbing 50% of data points. This
strategy perturbs 50% of the data points (chosen uni-
formly at random) by replacing them with integer val-
ues drawn uniformly at random from the range of [0; 1].
(cid:15) Deterministically perturbing even indexed data
points. This strategy perturbs the same number of
data points as the previous strategy on average, but
the perturbation applies to all the even indexed (i.e.,
deterministic) data points only { they are replaced
with integer values drawn uniformly at random from
the range of [0; 1].
(cid:15) Deterministically perturbing odd indexed data
points. This strategy works in the same way as the
previous one, but it perturbs all the odd indexed data
points and them only.
For traces generated by the data automaton, it is easy
to see that the perturbation of a random 50% of the data
points gives the best protection, whereas the two determin-
istic choices of the perturbed data points (i.e., either even or
odd indexed) have the same protection, and this protection
is in turn better than no perturbation at all.
6.3 Results – traces generated by data automa-
ton
We test satisfaction of the (cid:12)rst axiomatic property us-
ing traces generated by the data automaton, for each of
the four protection strategies. Fig. 4 reports the privacy
protection according to the four measures in Sec. 5. From
the (cid:12)gure, notice that CE (Fig. 4(a)) fails to show that the
two deterministic perturbation strategies oﬀer equally poor
privacy protection. This is because the online nature of CE
causes it to determine the uncertainty of original data points
based on their corresponding preceding data points only.
Hence, it erroneously concludes that perturbing the odd en-
tries will give much better protection than perturbing the
even ones.
In contrast, OCE (Fig. 4(b)), MI (Fig. 4(c)),
and NMI (Fig. 4(d)) do not suﬀer from this limited view.
They show that the two deterministic perturbation strate-
gies achieve the same performance for the same k.
The perturbed version of the traces generated by the data
automaton in Fig. 2 also allows us to test the privacy mea-
sures against the second axiomatic property. It is because
in these traces, the entries with a separation greater than
one time index are unrelated. Hence, the useful scope of
information to the adversary is limited to k = 1 of CE and
OCE, corresponding to the published data point either right
before or after the data point that the adversary wishes to
s000000111111100001111010.510.50.50.5(a) Conditional entropy
(a) Conditional entropy
(b) Oﬄine conditional entropy
(b) Oﬄine conditional entropy
(c) Mutual information
(c) Mutual information
(d) Normalized mutual information
(d) Normalized mutual information
Figure 4: Measure of privacy protection by diﬀerent
measures. Traces are generated by data automaton.
Figure 5: Measure of privacy protection by diﬀerent
measures. Traces are generated by order-2 Markov
model.
reveal. Observe that for CE (Fig. 4(a)) or OCE (Fig. 4(b)),
the privacy measure indeed stabilizes at k = 1 for all the
perturbation methods, thus verifying the second axiomatic
property. On the other hand, the MI (Fig. 4(c)) and NMI
(Fig. 4(d)) measures do not stabilize at k = 2, i.e., consid-
ering two data points together.
6.4 Results – Traces generated by Markov model
We now test the privacy measures against the second ax-
iomatic property using the four protection strategies (Sec. 6.2)
and traces generated by the Markov model in Fig. 3, where
the data points are integers within the range of [0; 1].
Fig. 5 reports the measured privacy protection according
to the diﬀerent measures. Fig. 5(a) shows that for no pertur-
bation, the protection according to CE weakens until k = 2,
which agrees with the order of the Markov model used to
generate the test data. It remains stable at this minimum
protection afterwards, when k grows beyond the Markov or-
der of the trace. The results also show that perturbations
have the general eﬀect of increasing the Markov order of the
data traces. Hence, when perturbations are applied, the CE
and OCE values may drop further, albeit by relatively small
amounts, as k grows beyond two. To be maximally eﬀective,
the adversary may thus need to use longer data patterns in
his attacks. On the other hand, Fig. 5(c) and Fig. 5(d)
show that for the no perturbation case, MI and NMI keep
increasing (i.e., do not stabilize) after the k considered grows
beyond the Markov order of the trace, i.e., they do not sat-
isfy the second axiomatic property.
It is because when k
increases, MI and NMI always have a tendency to grow due
to the signi(cid:12)cantly increased diversity of symbols.
6.5 Discussions
In summary, CE does not satisfy the (cid:12)rst axiomatic prop-
erty, i.e., it fails to account for all available information in
an attack. On the other hand, although MI and NMI satisfy
the (cid:12)rst property, they do not satisfy the second one. In par-
ticular, their measure of privacy (or its loss) over a moving
basis makes their results hard to interpret across k, which is
an important limitation. Of the measures evaluated, OCE
is the only one that satis(cid:12)es both the axiomatic properties.
00.20.40.60.811.2012345Conditional entropykno noiserandom 50%even entriesodd entries00.20.40.60.8101234Offline conditional entropykrandom 50%even entriesodd entries01234501234567Mutual informationkno noiserandom 50%even entriesodd entries00.10.20.30.40.50.60.701234567Normalized MIkrandom 50%even entriesodd entries00.20.40.60.811.2012345Conditional entropykno noiserandom 50%even entriesodd entries00.20.40.60.8101234Offline conditional entropykrandom 50%even entriesodd entries01234501234567Mutual informationkno noiserandom 50%even entriesodd entries00.10.20.30.40.501234567Normalized MIkrandom 50%even entriesodd entriesHence, it appears to be the best suited for measuring the
privacy protection of temporally correlated data traces.
7. BLH EXPERIMENTS
In this section we use the proposed privacy measures to
quantify the performance of the diﬀerent privacy protection
strategies we presented in Sec. 4 for BLH.
7.1 Dataset
We evaluate the performance of the diﬀerent BLH algo-
rithms presented in Sec. 4 using real smart meter readings
collected from the U.K. [36], as well as synthetic ones gen-
erated using an energy demand model developed in [37].
U.K. dataset [36]. This dataset contains power meter
readings collected from 22 households in the U.K. at a sam-
pling interval of one minute in 2008 and 2009.
Synthetic dataset [37]. As we discussed in Sec. 5,
the use of a large k for performance evaluation requires a
long enough data trace to produce steady-state results af-
ter convergence. To ensure that the relative performance
of diﬀerent BLH algorithms remains valid as the adversary
exploits progressively longer data patterns, we assess the
performance of these BLH algorithms using (cid:12)ve years long
synthetic traces also, well beyond the available length of the
U.K. real-world dataset. We now present how we generate
the synthetic smart meter traces.
The generator model [37] is con(cid:12)gured using the data col-
lected from [36]. It generates the power usage of a household
in diﬀerent days from 33 possible appliances and some other
light (cid:12)xtures up to a sampling interval of one minute. The
generator takes as input parameters the number of persons
in the household, the month of the year, and the type of the
day (e.g., weekday or weekend) to generate the occupancy of
the house, the usage of the appliances, and hence the total
power usage.
We utilize the generator to generate years long meter read-
ings of a household with speci(cid:12)c input parameters (e.g., the
household has three persons).
In particular, we modi(cid:12)ed
the generator so that it can generate continual meter read-
ings according to the month of the year and types of days
according to the calendar. The generated traces of smart
meter readings are for (cid:12)ve years, from 2008 to 2012.
7.2 Privacy concern of data
According to [46], the changes between consecutive smart
meter readings are more important than the clear data to
an adversary because many of the NILM techniques [4, 31,
43] use the changes to infer the type of appliance being
used. Hence, we apply the privacy measures on the changes
between consecutive smart meter readings rather than on
the raw data to quantify the performance of the BLH algo-
rithms.
7.3 Comparison of different BLH algorithms
We evaluate and compare the privacy protection of the
three protection strategies, i.e., BE, NILL, and LS1, as a
function of the length of data patterns exploited by the
adversary, under diﬀerent capacities of the battery in the
household’s battery smoothing system. The charging and
discharging rates of the battery are set to values such that
the battery is fully charged from empty or fully discharged
from full in an hour.
Since we have shown in Sec. 6 that conditional entropy
(CE) is not a good privacy measure and normalized mutual
information (NMI) gives no further information than mutual
information (MI), in this section we only compare the evalu-
ation results obtained by oﬄine conditional entropy (OCE)
and mutual information (MI).
U.K. data. We present the evaluation using real data in
the U.K. dataset [36], particularly data from a smart meter
collected in 2008.
Fig. 6 shows the OCE value computed for the diﬀerent
protection strategies as a function of k considered by the pri-
vacy measure, under diﬀerent battery capacity in the BLH
smoothing. Fig. 6 shows that when k = 1 (i.e., the uncer-
tainty of a clear data point conditioned on the corresponding
perturbed data point, a perturbed data point in the past,
and another one in the future), the eﬀectiveness of the three
protection strategies is similar to that reported in [46], i.e.,
LS1 gives the best protection whereas NILL gives the worst.
When k is increased to four (i.e., the adversary utilizes more
information from the \past" and \future"), the comparison
result is similar. Furthermore, we can observe from the (cid:12)g-
ure that when the adversary uses longer patterns to exploit
the temporal correlation in the time-series data, the pro-
tection provided by each strategy invariably decreases as
measured by OCE.
Fig. 7 compares the privacy protection of diﬀerent BLH
methods according to MI. A similar conclusion can be drawn
from this measure as from OCE on the relative performance
of the diﬀerent BLH algorithms { it remains the same as k
varies. However, unlike OCE, it is not easy to tell from MI
how increased exploitation of the data temporal correlation
may help the adversary in the attack. This is because for
MI, the quanti(cid:12)cation of information content occurs over a
moving basis.
Synthetic data. We now compare the performance of
diﬀerent BLH algorithms using the (cid:12)ve years long synthetic
traces. Fig. 8 shows the comparison of the diﬀerent algo-
rithms as they are quanti(cid:12)ed by OCE, whereas Fig. 9 shows
the comparison quanti(cid:12)ed using MI.
The results for the synthetic traces are similar to those
from the real traces. The BLH algorithms achieve the same
relative performance, and this relative performance remains
unchanged as k increases. Moreover, it is straightforward to
use OCE to quantify the privacy impact of a larger k for the
adversary, but the same is not true of MI.
The malicious version of LS1. We now evaluate the
impact of LS1a, the malicious version presented in Sec. 4.2
of the LS1 BLH algorithm. Recall that LS1a mimics the
working of LS1 but with a slight modi(cid:12)cation to leak pri-
vate information to the adversary through normally reported
smart meter readings as a covert channel.
Fig. 10 presents the simulation results of LS1 and its ma-
licious version LS1a. We can observe from the (cid:12)gure that
the harm of the malicious version is not obvious when k
is small, i.e., LS1a has similar performance as LS1 in that
regime. However, it would be incorrect to dismiss the attack
as ineﬀective. This is because when k increases, it becomes
apparent that the protection of LS1a is in fact much weaker
than that of LS1. If we further compare the performance of
LS1a with that of BE, i.e., Fig. 10 versus Fig. 6 and Fig. 7,
we can see that LS1a under-performs even BE when k is
large. Hence, it is crucial to use a proper privacy measure
(a) Battery capacity of 100 Wh
(b) Battery capacity of 400 Wh
(c) Battery capacity of 800 Wh
Figure 6: Comparing the performance of the three BLH protection approaches using OCE. A larger value