### Optimized Text

**Markov Model and Data Patterns:**
Regardless of the order of the Markov model, there will always be patterns of all identical symbols, albeit with small probabilities. This makes it impossible to determine whether the leading symbol in such a pattern is at an odd or even index, which is necessary to predict the next data point. Specifically, we cannot ascertain if the next data point should be identical to the previous one or should be randomly selected from the range [0, 1].

**Dataset with Repeating Symbols:**
Our use of this dataset, which includes repeating symbols, provides a straightforward method to control the degree of correlation between the data points in the original trace and those in the perturbed trace.

**Traces Generated by Markov Model:**
The data points in this trace set are limited to the integer range [0, 1]. They are generated using a second-order Markov model, as shown in Figure 3. Each data value is repeated at least once in the trace. An example trace is {1, 1, 1, 0, 0, 0, 0, 1, 1, ...}. This dataset allows us to control the order of data correlation in the original trace used to test privacy measures.

**Axiomatic Properties of Privacy Measures:**
We propose two axiomatic properties for a desirable privacy measure:
1. The measure accounts for all available information in a privacy attack.
2. The measure quantifies how the privacy protection weakens as the adversary exploits progressively larger scopes of data correlation present in the data. The strength of protection should stabilize at some scope \( k \) when applied to time-series data generated by an order-\( l \) Markov model.

**Protection Strategies:**
We evaluate four measures proposed in Section 5 against the axiomatic properties. We consider four strategies for protecting the synthetic data:
- **No Perturbation:** No data point is perturbed, serving as a baseline with no protection.
- **Random Perturbation (50%):** 50% of the data points are randomly perturbed by replacing them with integer values drawn uniformly at random from the range [0, 1].
- **Deterministic Perturbation (Even Indexed):** Even-indexed data points are deterministically perturbed by replacing them with integer values drawn uniformly at random from the range [0, 1].
- **Deterministic Perturbation (Odd Indexed):** Odd-indexed data points are deterministically perturbed by replacing them with integer values drawn uniformly at random from the range [0, 1].

**Results for Traces Generated by Data Automaton:**
We test the first axiomatic property using traces generated by the data automaton for each of the four protection strategies. Figure 4 reports the privacy protection according to the four measures in Section 5. From the figure, it is evident that Conditional Entropy (CE) fails to show that the two deterministic perturbation strategies offer equally poor privacy protection. This is because CE's online nature causes it to determine the uncertainty of original data points based on their preceding data points only, leading to the erroneous conclusion that perturbing odd entries provides better protection than even ones.

In contrast, Offline Conditional Entropy (OCE), Mutual Information (MI), and Normalized Mutual Information (NMI) do not suffer from this limited view. They show that the two deterministic perturbation strategies achieve the same performance for the same \( k \).

The perturbed version of the traces generated by the data automaton in Figure 2 also allows us to test the privacy measures against the second axiomatic property. In these traces, entries separated by more than one time index are unrelated, limiting the useful scope of information to \( k = 1 \) for CE and OCE. For CE (Figure 4(a)) and OCE (Figure 4(b)), the privacy measure stabilizes at \( k = 1 \) for all perturbation methods, verifying the second axiomatic property. However, MI (Figure 4(c)) and NMI (Figure 4(d)) do not stabilize at \( k = 2 \), indicating that they do not satisfy the second axiomatic property.

**Results for Traces Generated by Markov Model:**
We test the privacy measures against the second axiomatic property using the four protection strategies and traces generated by the Markov model in Figure 3, where data points are integers within the range [0, 1]. Figure 5 reports the measured privacy protection according to the different measures. For no perturbation, the protection according to CE weakens until \( k = 2 \), consistent with the order of the Markov model used to generate the test data. It remains stable at this minimum protection level as \( k \) grows beyond the Markov order of the trace. The results also show that perturbations generally increase the Markov order of the data traces, causing CE and OCE values to drop further, albeit by relatively small amounts, as \( k \) grows beyond two. To be maximally effective, the adversary may need to use longer data patterns in attacks. On the other hand, MI and NMI keep increasing (i.e., do not stabilize) after \( k \) grows beyond the Markov order of the trace, failing to satisfy the second axiomatic property.

**Discussions:**
In summary, CE does not satisfy the first axiomatic property, as it fails to account for all available information in an attack. Although MI and NMI satisfy the first property, they do not satisfy the second one. Their measure of privacy over a moving basis makes their results hard to interpret across \( k \), which is a significant limitation. Among the measures evaluated, OCE is the only one that satisfies both axiomatic properties, making it the best suited for measuring the privacy protection of temporally correlated data traces.

**BLH Experiments:**
In this section, we use the proposed privacy measures to quantify the performance of the different privacy protection strategies for BLH.

**Dataset:**
We evaluate the performance of the different BLH algorithms presented in Section 4 using real smart meter readings collected from the U.K. [36] and synthetic ones generated using an energy demand model developed in [37].

- **U.K. Dataset [36]:** This dataset contains power meter readings collected from 22 households in the U.K. at a sampling interval of one minute in 2008 and 2009.
- **Synthetic Dataset [37]:** To ensure that the relative performance of different BLH algorithms remains valid as the adversary exploits progressively longer data patterns, we assess the performance using five years long synthetic traces. The generator model [37] is configured using data collected from [36] and generates power usage for a household with specific input parameters (e.g., number of persons, month, and type of day).

**Privacy Concerns of Data:**
According to [46], the changes between consecutive smart meter readings are more important to an adversary than the raw data, as many NILM techniques [4, 31, 43] use these changes to infer the type of appliance being used. Therefore, we apply the privacy measures to the changes between consecutive smart meter readings rather than the raw data to quantify the performance of the BLH algorithms.

**Comparison of Different BLH Algorithms:**
We evaluate and compare the privacy protection of three protection strategies (BE, NILL, and LS1) as a function of the length of data patterns exploited by the adversary, under different capacities of the battery in the householdâ€™s battery smoothing system. The charging and discharging rates of the battery are set to values such that the battery is fully charged from empty or fully discharged from full in an hour.

Since we have shown in Section 6 that CE is not a good privacy measure and NMI provides no additional information compared to MI, in this section, we only compare the evaluation results obtained by OCE and MI.

- **U.K. Data:** We present the evaluation using real data from the U.K. dataset [36], specifically data from a smart meter collected in 2008. Figure 6 shows the OCE value computed for the different protection strategies as a function of \( k \) considered by the privacy measure, under different battery capacities in the BLH smoothing. When \( k = 1 \), the effectiveness of the three protection strategies is similar to that reported in [46], with LS1 providing the best protection and NILL the worst. As \( k \) increases to four, the comparison result remains similar. The figure also shows that as the adversary uses longer patterns to exploit temporal correlation, the protection provided by each strategy decreases as measured by OCE.

- **Synthetic Data:** We compare the performance of different BLH algorithms using the five years long synthetic traces. Figures 8 and 9 show the comparison of the different algorithms quantified by OCE and MI, respectively. The results for the synthetic traces are similar to those from the real traces. The BLH algorithms achieve the same relative performance, and this relative performance remains unchanged as \( k \) increases. OCE is straightforward for quantifying the privacy impact of a larger \( k \) for the adversary, but the same is not true for MI.

**Malicious Version of LS1:**
We evaluate the impact of LS1a, the malicious version of the LS1 BLH algorithm, which mimics LS1 but with a slight modification to leak private information through normally reported smart meter readings as a covert channel. Figure 10 presents the simulation results of LS1 and its malicious version LS1a. The harm of the malicious version is not obvious when \( k \) is small, as LS1a performs similarly to LS1. However, as \( k \) increases, it becomes apparent that the protection of LS1a is much weaker than that of LS1. Comparing the performance of LS1a with BE (Figures 6 and 7) shows that LS1a underperforms even BE when \( k \) is large. Thus, it is crucial to use a proper privacy measure.

**Conclusion:**
In summary, OCE is the most suitable measure for evaluating the privacy protection of temporally correlated data traces, as it satisfies both axiomatic properties. The other measures, while useful, have limitations that make them less effective in certain scenarios.