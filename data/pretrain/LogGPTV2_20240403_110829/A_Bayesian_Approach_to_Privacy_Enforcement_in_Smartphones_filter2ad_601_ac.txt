tance levels for each private item: [0,4] and “≥ 5”. (See
Section 3.1.) Finally, to avoid zero estimates for con-
ditional probabilities while also minimizing data pertur-
bation, we set the “smoothing” factor l in Equation 6 at
1, where the illegitimate ﬂows we detected were in the
order of several dozens per private item.
5.2 Experimental Hypotheses
In our experimental evaluation of BAYESDROID, we
tested two hypotheses:
1. H1: Accuracy. Bayesian reasoning, as implemented
in BAYESDROID, yields a signiﬁcant improvement
in leakage-detection accuracy compared to the base-
line of information-ﬂow tracking.
For
2. H2: Applicability.
applications,
BAYESDROID remains effective under relaxation
of the tag-based method for detection of relevant
values and its stability improves.
real-life
5.3 H1: Accuracy
To assess the accuracy of BAYESDROID, we compared it
to that of TaintDroid, a state-of-the-art information-ﬂow
tracking tool for privacy enforcement. Our experimental
settings and results are described below.
Subjects We applied both TaintDroid and BAYES-
DROID to DroidBench, an independent and publicly
available collection of benchmarks serving as testing
ground for both static and dynamic privacy enforcement
algorithms. DroidBench models a large set of realistic
challenges in leakage detection, including precise track-
ing of sensitive data through containers, handling of call-
backs, ﬁeld and object sensitivity, lifecycle modeling,
inter-app communication, reﬂection and implicit ﬂows.
The DroidBench suite consists of 50 cases. We ex-
cluded from our experiment (i) 8 benchmarks that crash
at startup, as well as (ii) 5 benchmarks that leak data
via callbacks that we did not manage to trigger (e.g.,
onLowMemory()), as both TaintDroid and BAYESDROID
were naturally unable to detect leakages in these two
cases. The complete list of benchmarks that we used can
be found in Table 4 of Appendix B.
Methodology For each benchmark, we measured the
number of true positive (TP), false positive (FP) and false
negative (FN) results. We then summarized the results
and calculated the overall precision and recall of each
tool using the formulas below:
Precision = T P
T P+FP
Recall = T P
T P+FN
TaintDroid
BAYESDROID
TPs FPs FNs Precision Recall F-measure
31
29
0.64
0.96
1.00
0.93
0.78
0.94
17
1
0
2
Table 1: Accuracy of BAYESDROID and TaintDroid on
DroidBench
High precision implies that a technique returns few irrel-
evant results, whereas high recall implies that it misses
only few relevant ones.
Since ideal techniques have both high recall and high
precision, the F-measure is commonly used to combine
both precision and recall into a single measure. The F-
measure is deﬁned as the harmonic mean of precision
and recall, and is calculated as follows:
F-Measure = 2× Precision×Recall
Precision+Recall
The value of F-measure is high only when both preci-
sion and recall are high. We thus use the F-measure for
accuracy evaluation.
Results The results obtained for both TaintDroid and
BAYESDROID on version 1.1 of DroidBench are sum-
marized in Table 1 and presented in detail in Table 4.
The ﬁndings reported by BAYESDROID are also publicly
available.4
Overall, TaintDroid detects 31 true leakages while also
reporting 17 false positives, whereas BAYESDROID suf-
fers from 2 false negatives, discovering 29 of the true
leakages while ﬂagging only 1 false alarm. The recall of
both TaintDroid and BAYESDROID is high (1 and 0.93,
respectively) due to a low number of false-negative re-
sults. Yet the precision of TaintDroid is much lower than
that of BAYESDROID (0.64 vs. 0.96), due to a high num-
ber of false positives. The overall F-measure is thus
lower for TaintDroid than for BAYESDROID (0.78 vs.
0.94).
The results mark BAYESDROID as visibly more ac-
curate than TaintDroid. To further conﬁrm this result,
we performed a two-tail McNemar test, considering 48
observations for each tool. These observations corre-
spond to ﬁndings reported in Table 4: 31 true positives
and 17 classiﬁed as false alarms. Each observation is
a boolean value that represents the accuracy of the tool
and is assumed to be from a Bernoulli distribution. We
then checked whether the difference in accuracy is statis-
tically signiﬁcant by testing the null hypothesis that the
set of 48 observations from TaintDroid are sampled from
the same Bernoulli distribution as the set of 48 observa-
tions from BAYESDROID.
4
See
archive
otripp/droidbench.zip.
ﬁle
researcher.ibm.com/researcher/ﬁles/us-
USENIX Association  
23rd USENIX Security Symposium  183
9
getSystemService(TELEPHONY SERVICE);
1 TelephonyManager tm =
2
3 String imei = tm.getDeviceId(); //source
4 String obfuscatedIMEI = obfuscateIMEI(imei);
5 Log.i (imei ); // sink
6
...;
7 private String obfuscateIMEI(String imei) {
8
9
10
11
12
13
String result = ””;
for (char c :
switch(c) {
case ’0’ :
case ’1’ :
case ’2’ :
imei .toCharArray()) {
result += ’a’; break;
result += ’b’; break;
result += ’c’; break;
...; } }
Figure 5:
from the DroidBench
ImplicitFlow1 benchmark, which applies a custom
transformation to private data
Fragment
We found that TaintDroid was accurate in 31 out of
48 cases, and BAYESDROID was accurate in 45 out of
48 cases. We built the 2×2 contingency table showing
when each tool was correct and applied a two-tail Mc-
Nemar test. We found a p-value of 0.001, which rejects
the null hypothesis that the observations come from the
same underlying distribution and provides evidence that
BAYESDROID is more accurate than TaintDroid, thereby
conﬁrming H1.
Discussion Analysis of the per-benchmark ﬁndings re-
veals the following: First,
the 2 false negatives of
BAYESDROID on ImplicitFlow1 are both due to cus-
tom (i.e., non-standard) data transformations, which are
outside the current scope of BAYESDROID. An illustra-
tive fragment from the ImplicitFlow1 code is shown in
Figure 5. The obfuscateIMEI(. . .) transformation maps
IMEI digits to English letters, which is a non-standard
behavior that is unlikely to arise in an authentic app.
The false positive reported by BAYESDROID, in com-
mon with TaintDroid, is on release of sensitive data to
the ﬁle system, albeit using the MODE PRIVATE ﬂag, which
does not constitute a leakage problem in itself. This can
be resolved by performing Bayesian reasoning not only
over argument values, but also over properties of the sink
API (in this case, the storage location mapped to a ﬁle
handle). We intend to implement this enhancement.
Beyond the false alarm in common with BAYES-
DROID, TaintDroid has multiple other sources of impre-
cision. The main reasons for its false positives are
(cid:127) coarse modeling of containers, mapping their en-
tire contents to a single taint bit, which accounts
e.g. for the false alarms on ArrayAccess{1,2} and
HashMapAccess1;
in
(cid:127) ﬁeld
and
false
insensitivity,
resulting
FieldSensitivity{2,4}
and
alarms
object
on
)
%
(
d
a
e
h
r
e
v
O
l
l
a
r
e
v
O
140
105
70
35
0
Tag Propagation Overhead
Bayesian Analysis Overhead
1
2
3
4
5
6
7
9 10 11 12 13 14 15 16 17 18 19
8
Propagation Steps
Figure 6: Overhead breakdown into tag propagation and
Bayesian analysis at sink
ObjectSensitivity{1,2}; and more fundamentally,
(cid:127) ignoring of data values, which causes TaintDroid
to issue false warnings on LocationLeak{1,2} even
when location reads fail, yielding a Location object
without any meaningful information.
The fundamental reason for these imprecisions is to con-
strain the overhead of TaintDroid, such that it can meet
the performance demands of online privacy enforcement.
BAYESDROID is able to accommodate such optimiza-
tions while still ensuring high accuracy.
5.4 H2: Applicability
The second aspect of the evaluation compared between
two versions of BAYESDROID, whose sole difference
lies in the method used for detecting relevant values: In
one conﬁguration (T-BD), relevant values are detected
via tag propagation. The other conﬁguration (H-BD)
uses the heuristic detailed in Section 4.2 of treating all
values reachable from sink arguments (either directly or
via the heap graph) up to a depth bound of k as relevant,
which places more responsibility on Bayesian reasoning.
We set k at 3 based on manual review of the data struc-
tures ﬂowing into privacy sinks.
We designed a parametric benchmark application to
quantify the overhead reduction imposed by the H-BD
variant of BAYESDROID. The application consists of a
simple loop that ﬂows the device IMEI into a log ﬁle.
Loop iterations perform intermediate data propagation
steps. We then performed a series of experiments —
over the range of 1 to 19 propagation steps — to quantify
the relative overhead of tag propagation versus Bayesian
analysis.
The results, presented in Figure 6, suggest that the
overhead of tag propagation is more dominant than that
of Bayesian analysis (with a ratio of roughly 2:1), even
when the set of relevant values is naively over approx-
imated. Discussion of the methodology underlying this
184  23rd USENIX Security Symposium 
USENIX Association
10
experiment is provided in Appendix A.
In general, H-BD trades overhead reduction for accu-
racy. H2 then asserts that, in practice, the tradeoff posed
by H-BD is effective. Below, we discuss our empirical
evaluation of this hypothesis over real-life subjects.
TPs FPs FNs Precision Recall F-measure Crashes
H-BD 27
T-BD
14
1
0
0
10
0.96
1.00
1.00
0.58
0.98
0.73
12
22
Table 2: Accuracy of H-BD and T-BD BAYESDROID
conﬁgurations
Subjects To avoid evaluators’ bias, we applied the fol-
lowing selection process: We started from the 65 Google
Play apps not chosen for the training phase. We then ex-
cluded 8 apps that do not have permission to access sen-
sitive data and/or perform release operations (i.e., their
manifest does not declare sufﬁcient permissions out of
INTERNET, READ PHONE STATE, SEND SMS, etc), as well as 3
apps that we did not manage to install properly, resulting
in 54 apps that installed successfully and exercise privacy
sources and sinks.
The complete list of the application we used is given
in Table 5 of Appendix B. A subset of the applications,
for which at least one leakage was detected, is also listed
in Table 3.
Methodology We deployed the apps under the two
BAYESDROID conﬁgurations. Each execution was done
from a clean starting state. The third column of both
Tables 3 and 5 denotes whether our exploration of the
app was exhaustive. By that we mean exercising all the
UI points exposed by the app in a sensible order. Ide-
ally we would do so for all apps. However, (i) some
of the apps, and in particular gaming apps, had stability
issues, and (ii) certain apps require SMS-validated sign
in, which we did not perform. We did, however, cre-
ate Facebook, Gmail and Dropbox accounts to log into
apps that demand such information yet do not ask for
SMS validation. We were also careful to execute the ex-
act same crawling scenario under both the T-BD and H-
BD conﬁgurations. We comment, from our experience,
that most data leaks happen when an app launches, and
initializes advertising/analytics functionality, and so for
apps for which deep crawling was not possible the results
are still largely meaningful.
For comparability between the H-BD and T-BD con-
ﬁgurations, we counted different dynamic reports involv-
ing the same pair of source/sink APIs as a single leakage
instance. We manually classiﬁed the ﬁndings into true
positives and false positives. For this classiﬁcation, we
scrutinized the reports by the two conﬁgurations, and
also — in cases of uncertainty — decompiled and/or
reran the app to examine its behavior more closely. As in
the experiment described in Section 5.3, we then calcu-
lated the precision, recall and F-measure for each of the
tools.
Results The results obtained for H-BD and T-BD are
summarized in Table 2. Table 3 summarizes the ﬁnd-
ings reported by both H-BD and T-BD at the granularity
of privacy items: the device number, identiﬁer and loca-
tion, while Table 5 provides a detailed description of the
results across all benchmarks including those on which
no leakages were detected. The warnings reported by
the H-BD conﬁguration are also publicly available for
review.5
As Table 2 indicates, the H-BD variant is more accu-
rate than the T-BD variant overall (F-measure of 0.98 vs.
0.73). As in the experiment described in Section 5.3, we
further performed a two-tail McNemar test, considering
67 observations for each tool: 27 that correspond to true
positives, 1 to the false positive due to H-BD and 39 to
cases where no leakages were found.
We found that H-BD was accurate in 66 out of 67
cases, and T-DB was accurate in 54 out of 67 cases.
Building the 2×2 contingency table and applying the
two-tail McNemar test showed that the difference be-
tween the tools in accuracy is signiﬁcant (with a p-
value of 0.001 to reject the null hypothesis that the ac-
curacy observations for both tools come from the same
Bernoulli distribution). Moreover, H-BD has a lower
number of crashes and lower runtime overhead, which
conﬁrms H2.
Discussion To give the reader a taste of the ﬁndings,
we present in Figures 7–8 two examples of potential
leakages that BAYESDROID (both the H-BD and the
T-BD conﬁgurations) deemed legitimate. The instance
in Figure 7 reﬂects the common scenario of obtain-
ing the current (or last known) location, converting it
into one or more addresses, and then releasing only the
country or zip code.
In the second instance, in Fig-
ure 8, the 64-bit Android ID — generated when the
user ﬁrst sets up the device — is read via a call to
Settings$Secure.getString(ANDROID ID). At the release
point, into the ﬁle system, only a preﬁx of the Android
ID consisting of the ﬁrst 12 digits is published.
As Table 3 makes apparent, the ﬁndings by H-BD are
more complete: It detects 18 leakages (versus 8 reports
by T-BD), with no false negative results and only one
false positive. We attribute that to (i) the intrusive instru-
See
archive
ﬁle
researcher.ibm.com/researcher/ﬁles/us-
5
otripp/realworldapps.zip.
USENIX Association  
23rd USENIX Security Symposium  185
11
App
Domain
Deep crawl?
atsoft.games.smgame
com.antivirus
com.appershopper.ios7lockscreen
com.bestcoolfungames.antsmasher
com.bitﬁtlabs.ﬁngerprint.lockscreen
com.cleanmaster.mguard
com.coolﬁsh.cathairsalon
com.coolﬁsh.snipershooting
com.digisoft.TransparentScreen
com.g6677.android.cbaby
com.g6677.android.chospital
com.g6677.android.design
com.g6677.android.pnailspa
com.g6677.android.princesshs
com.goldtouch.mako
15
games/arcade
communication
personalization
games/arcade
games/casual
tools
games/casual
games/action
entertainment