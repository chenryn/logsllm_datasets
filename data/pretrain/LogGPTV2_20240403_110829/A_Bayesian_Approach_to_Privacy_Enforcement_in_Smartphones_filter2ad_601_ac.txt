### Taint Levels for Private Items
We define taint levels for each private item as [0,4] and “≥ 5”. (See Section 3.1.) To avoid zero estimates for conditional probabilities while minimizing data perturbation, we set the smoothing factor \( l \) in Equation 6 to 1. The number of illegitimate flows detected was in the order of several dozens per private item.

### 5.2 Experimental Hypotheses
In our experimental evaluation of BAYESDROID, we tested two hypotheses:
1. **H1: Accuracy**. Bayesian reasoning, as implemented in BAYESDROID, yields a significant improvement in leakage-detection accuracy compared to the baseline of information-flow tracking.
2. **H2: Applicability**. BAYESDROID remains effective under relaxation of the tag-based method for detection of relevant values, and its stability improves in real-life applications.

### 5.3 H1: Accuracy
To assess the accuracy of BAYESDROID, we compared it with TaintDroid, a state-of-the-art information-flow tracking tool for privacy enforcement. Our experimental settings and results are described below.

#### Subjects
We applied both TaintDroid and BAYESDROID to DroidBench, an independent and publicly available collection of benchmarks serving as a testing ground for both static and dynamic privacy enforcement algorithms. DroidBench models a large set of realistic challenges in leakage detection, including precise tracking of sensitive data through containers, handling of callbacks, field and object sensitivity, lifecycle modeling, inter-app communication, reflection, and implicit flows.

The DroidBench suite consists of 50 cases. We excluded from our experiment:
- 8 benchmarks that crash at startup.
- 5 benchmarks that leak data via callbacks that we did not manage to trigger (e.g., `onLowMemory()`).

Both TaintDroid and BAYESDROID were unable to detect leakages in these cases. The complete list of benchmarks used can be found in Table 4 of Appendix B.

#### Methodology
For each benchmark, we measured the number of true positives (TP), false positives (FP), and false negatives (FN). We then summarized the results and calculated the overall precision and recall of each tool using the following formulas:
\[ \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}} \]
\[ \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}} \]

Since ideal techniques have both high recall and high precision, the F-measure is commonly used to combine both into a single measure. The F-measure is defined as the harmonic mean of precision and recall and is calculated as follows:
\[ \text{F-Measure} = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} \]

The value of the F-measure is high only when both precision and recall are high. We thus use the F-measure for accuracy evaluation.

#### Results
The results obtained for both TaintDroid and BAYESDROID on version 1.1 of DroidBench are summarized in Table 1 and presented in detail in Table 4. The findings reported by BAYESDROID are also publicly available.

| Tool        | TP | FP | FN | Precision | Recall | F-measure |
|-------------|----|----|----|-----------|--------|-----------|
| TaintDroid  | 31 | 17 | 0  | 0.64      | 1.00   | 0.78      |
| BAYESDROID | 29 | 1  | 2  | 0.96      | 0.93   | 0.94      |

Overall, TaintDroid detects 31 true leakages while reporting 17 false positives, whereas BAYESDROID suffers from 2 false negatives, discovering 29 of the true leakages while flagging only 1 false alarm. The recall of both tools is high (1 and 0.93, respectively) due to a low number of false-negative results. However, the precision of TaintDroid is much lower than that of BAYESDROID (0.64 vs. 0.96) due to a high number of false positives. The overall F-measure is thus lower for TaintDroid than for BAYESDROID (0.78 vs. 0.94).

The results indicate that BAYESDROID is more accurate than TaintDroid. To further confirm this result, we performed a two-tail McNemar test, considering 48 observations for each tool. These observations correspond to findings reported in Table 4: 31 true positives and 17 classified as false alarms. Each observation is a boolean value representing the accuracy of the tool and is assumed to be from a Bernoulli distribution. We checked whether the difference in accuracy is statistically significant by testing the null hypothesis that the set of 48 observations from TaintDroid are sampled from the same Bernoulli distribution as the set of 48 observations from BAYESDROID.

We found that TaintDroid was accurate in 31 out of 48 cases, and BAYESDROID was accurate in 45 out of 48 cases. We built a 2×2 contingency table showing when each tool was correct and applied a two-tail McNemar test. We found a p-value of 0.001, which rejects the null hypothesis that the observations come from the same underlying distribution and provides evidence that BAYESDROID is more accurate than TaintDroid, thereby confirming H1.

#### Discussion
Analysis of the per-benchmark findings reveals the following:
- The 2 false negatives of BAYESDROID on ImplicitFlow1 are due to custom (i.e., non-standard) data transformations, which are outside the current scope of BAYESDROID. An illustrative fragment from the ImplicitFlow1 code is shown in Figure 5. The `obfuscateIMEI(...)` transformation maps IMEI digits to English letters, which is a non-standard behavior unlikely to arise in an authentic app.
- The false positive reported by BAYESDROID, in common with TaintDroid, is on the release of sensitive data to the file system, albeit using the `MODE_PRIVATE` flag, which does not constitute a leakage problem. This can be resolved by performing Bayesian reasoning not only over argument values but also over properties of the sink API (in this case, the storage location mapped to a file handle). We intend to implement this enhancement.

Beyond the false alarm in common with BAYESDROID, TaintDroid has multiple other sources of imprecision:
- Coarse modeling of containers, mapping their entire contents to a single taint bit, which accounts for false alarms on `ArrayAccess{1,2}` and `HashMapAccess1`.
- Field and object insensitivity, resulting in false alarms on `FieldSensitivity{2,4}` and `ObjectSensitivity{1,2}`.
- Ignoring of data values, causing TaintDroid to issue false warnings on `LocationLeak{1,2}` even when location reads fail, yielding a `Location` object without any meaningful information.

These imprecisions are fundamental to constraining the overhead of TaintDroid, such that it can meet the performance demands of online privacy enforcement. BAYESDROID is able to accommodate such optimizations while still ensuring high accuracy.

### 5.4 H2: Applicability
The second aspect of the evaluation compared two versions of BAYESDROID, whose sole difference lies in the method used for detecting relevant values:
- **T-BD**: Relevant values are detected via tag propagation.
- **H-BD**: Uses the heuristic detailed in Section 4.2, treating all values reachable from sink arguments (either directly or via the heap graph) up to a depth bound of \( k \) as relevant, placing more responsibility on Bayesian reasoning.

We set \( k \) at 3 based on manual review of the data structures flowing into privacy sinks.

We designed a parametric benchmark application to quantify the overhead reduction imposed by the H-BD variant of BAYESDROID. The application consists of a simple loop that flows the device IMEI into a log file. Loop iterations perform intermediate data propagation steps. We then performed a series of experiments—over the range of 1 to 19 propagation steps—to quantify the relative overhead of tag propagation versus Bayesian analysis.

The results, presented in Figure 6, suggest that the overhead of tag propagation is more dominant than that of Bayesian analysis (with a ratio of roughly 2:1), even when the set of relevant values is naively over-approximated. Discussion of the methodology underlying this experiment is provided in Appendix A.

In general, H-BD trades overhead reduction for accuracy. H2 asserts that, in practice, the tradeoff posed by H-BD is effective. Below, we discuss our empirical evaluation of this hypothesis over real-life subjects.

| Tool  | TP | FP | FN | Precision | Recall | F-measure | Crashes |
|-------|----|----|----|-----------|--------|-----------|---------|
| H-BD  | 27 | 1  | 0  | 0.96      | 1.00   | 1.00      | 10      |
| T-BD  | 14 | 22 | 0  | 0.58      | 0.98   | 0.73      | 12      |

#### Subjects
To avoid evaluators' bias, we applied the following selection process:
- We started from the 65 Google Play apps not chosen for the training phase.
- We excluded 8 apps that do not have permission to access sensitive data and/or perform release operations (i.e., their manifest does not declare sufficient permissions out of `INTERNET`, `READ_PHONE_STATE`, `SEND_SMS`, etc.).
- We also excluded 3 apps that we did not manage to install properly, resulting in 54 apps that installed successfully and exercise privacy sources and sinks.

The complete list of the applications used is given in Table 5 of Appendix B. A subset of the applications, for which at least one leakage was detected, is also listed in Table 3.

#### Methodology
We deployed the apps under the two BAYESDROID configurations. Each execution was done from a clean starting state. The third column of both Tables 3 and 5 denotes whether our exploration of the app was exhaustive. By that, we mean exercising all the UI points exposed by the app in a sensible order. Ideally, we would do so for all apps. However, some of the apps, particularly gaming apps, had stability issues, and certain apps require SMS-validated sign-in, which we did not perform. We did, however, create Facebook, Gmail, and Dropbox accounts to log into apps that demand such information but do not ask for SMS validation. We were also careful to execute the exact same crawling scenario under both the T-BD and H-BD configurations. From our experience, most data leaks happen when an app launches and initializes advertising/analytics functionality. Therefore, for apps for which deep crawling was not possible, the results are still largely meaningful.

For comparability between the H-BD and T-BD configurations, we counted different dynamic reports involving the same pair of source/sink APIs as a single leakage instance. We manually classified the findings into true positives and false positives. For this classification, we scrutinized the reports by the two configurations and, in cases of uncertainty, decompiled and/or reran the app to examine its behavior more closely. As in the experiment described in Section 5.3, we then calculated the precision, recall, and F-measure for each of the tools.

#### Results
The results obtained for H-BD and T-BD are summarized in Table 2. Table 3 summarizes the findings reported by both H-BD and T-BD at the granularity of privacy items: the device number, identifier, and location, while Table 5 provides a detailed description of the results across all benchmarks, including those on which no leakages were detected. The warnings reported by the H-BD configuration are also publicly available for review.

As Table 2 indicates, the H-BD variant is more accurate than the T-BD variant overall (F-measure of 0.98 vs. 0.73). As in the experiment described in Section 5.3, we further performed a two-tail McNemar test, considering 67 observations for each tool: 27 that correspond to true positives, 1 to the false positive due to H-BD, and 39 to cases where no leakages were found.

We found that H-BD was accurate in 66 out of 67 cases, and T-DB was accurate in 54 out of 67 cases. Building the 2×2 contingency table and applying the two-tail McNemar test showed that the difference between the tools in accuracy is significant (with a p-value of 0.001 to reject the null hypothesis that the accuracy observations for both tools come from the same Bernoulli distribution). Moreover, H-BD has a lower number of crashes and lower runtime overhead, which confirms H2.

#### Discussion
To give the reader a taste of the findings, we present in Figures 7–8 two examples of potential leakages that BAYESDROID (both the H-BD and the T-BD configurations) deemed legitimate. The instance in Figure 7 reflects the common scenario of obtaining the current (or last known) location, converting it into one or more addresses, and then releasing only the country or zip code.

In the second instance, in Figure 8, the 64-bit Android ID—generated when the user first sets up the device—is read via a call to `Settings$Secure.getString(ANDROID_ID)`. At the release point, into the file system, only a prefix of the Android ID consisting of the first 12 digits is published.

As Table 3 makes apparent, the findings by H-BD are more complete: It detects 18 leakages (versus 8 reports by T-BD), with no false negative results and only one false positive. We attribute this to the intrusive instrumentation and more comprehensive analysis performed by H-BD.

### List of Applications Used
| App Name                             | Domain           | Deep Crawl? |
|--------------------------------------|------------------|-------------|
| atsoft.games.smgame                  | Games/Arcade     | No          |
| com.antivirus                        | Communication    | No          |
| com.appershopper.ios7lockscreen      | Personalization  | No          |
| com.bestcoolfungames.antsmasher      | Games/Casual     | No          |
| com.bitfitlabs.fingerprint.lockscreen| Tools            | No          |
| com.cleanmaster.mguard               | Games/Casual     | No          |
| com.coolfish.cathairsalon            | Games/Action     | No          |
| com.coolfish.snipershooting          | Entertainment    | No          |
| com.digisoft.TransparentScreen       | Entertainment    | No          |
| com.g6677.android.cbaby              | Games/Action     | No          |
| com.g6677.android.chospital          | Games/Action     | No          |
| com.g6677.android.design             | Games/Action     | No          |
| com.g6677.android.pnailspa           | Games/Action     | No          |
| com.g6677.android.princesshs         | Games/Action     | No          |
| com.goldtouch.mako                   | Games/Action     | No          |

This list includes the domain and whether a deep crawl was performed for each application.