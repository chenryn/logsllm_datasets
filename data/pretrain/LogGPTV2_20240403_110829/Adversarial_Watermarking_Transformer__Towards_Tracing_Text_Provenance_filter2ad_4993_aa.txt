title:Adversarial Watermarking Transformer: Towards Tracing Text Provenance
with Data Hiding
author:Sahar Abdelnabi and
Mario Fritz
Adversarial Watermarking Transformer: Towards
Tracing Text Provenance with Data Hiding
Sahar Abdelnabi and Mario Fritz
CISPA Helmholtz Center for Information Security
1
2
0
2
r
a
M
9
2
]
R
C
.
s
c
[
2
v
5
1
0
3
0
.
9
0
0
2
:
v
i
X
r
a
Abstract—Recent advances in natural
language generation
have introduced powerful
language models with high-quality
output text. However, this raises concerns about the potential
misuse of such models for malicious purposes. In this paper,
we study natural language watermarking as a defense to help
better mark and trace the provenance of text. We introduce the
Adversarial Watermarking Transformer (AWT) with a jointly
trained encoder-decoder and adversarial training that, given an
input text and a binary message, generates an output text that is
unobtrusively encoded with the given message. We further study
different training and inference strategies to achieve minimal
changes to the semantics and correctness of the input text.
AWT is the ﬁrst end-to-end model to hide data in text by
automatically learning -without ground truth- word substitutions
along with their locations in order to encode the message.
We empirically show that our model
is effective in largely
preserving text utility and decoding the watermark while hiding
its presence against adversaries. Additionally, we demonstrate
that our method is robust against a range of attacks.
I. INTRODUCTION
Recent years have witnessed major achievements in natural
language processing (NLP), generation, and understanding.
This is in part driven by the introduction of attention-based
models (i.e., transformers [1]) that outperformed recurrent or
convolutional neural networks in many language tasks such
as machine translation [1], [2], language understanding [3],
[4], and language generation [5]. In addition, model pre-
training further fueled these advances and it is now a common
practice in NLP [6], [7]; many large-scale models are now pre-
trained on large datasets with either denoising auto-encoding
or language modelling objectives and then ﬁne-tuned on other
NLP downstream tasks [3], [4], [8]–[11].
On the other hand, this raises concerns about the potential
misuse of such powerful models for malicious purposes such
as spreading neural-generated fake news and misinformation.
For example, OpenAI used a staged release to publicize their
GPT-2 language model in order to evaluate the impact and
potential risks [12]. Moreover, Zellers et al. [5] proposed a
generative model called Grover demonstrating that a language
model such as GPT-2 can be trained on news articles and can
consequently generate realistically looking fake news.
These models can generate highly ﬂuent text which some-
times had even higher ratings than human-written text and
fooled human detectors [5], [13], [14]. While it
is now
possible to perform automatic detection, it is subject to recent
advances in text generation (e.g., architecture, model size,
and decoding strategies) [5], [13], which could hinder the
automatic detection in the long run. Hence, we seek a more
Fig. 1: An overview of our text watermarking solution at
inference time.
sustainable solution that can disambiguate between real and
fake text.
To this end, we aim to perform automatic and unobstructive
data hiding within language towards eventually watermarking
the output of text generation models. Speciﬁcally, we envision
black-box access scenarios to the language model APIs [15] or
to services such as text generation and editing-assistance that
could be misused to create misinformation. Watermarking can
then be used to introduce detectable ﬁngerprints in the output
that enable provenance tracing and detection. As deep learning
models are widely deployed in the wild as services, they are
subject to many attacks that only require black-box access
(e.g., [16]–[19]). Thus, it is important to proactively provide
solutions for such potential attacks before their prevalence.
a) Language watermarking: There have been several
attempts to create watermarking methods for natural language,
such as synonym substitutions [20], [21], syntactic tools
(e.g., structural transformation [22]), in addition to language-
speciﬁc changes [23]–[25]. However, these previous methods
used ﬁxed rule-based substitutions that required extensive
engineering efforts to design,
in addition to human input
and annotations, which hinders the automatic transformation.
Also, the designed rules are limited as they might not apply
to all sentences (e.g., no syntactic transformations can be
applied [22]). Additionally, they introduce large lexical or
style changes to the original text, which is not preferred when
keeping the original state is required (such as the output of
an already well-trained language model). Besides, rule-based
methods could impose restrictions on the use of the language
(e.g., by word masking). Finally, using ﬁxed substitutions can
systematically change the text statistics which, in turn, under-
1010Input textOther positions from theDepartment of Air includedAir Commodore Plans fromOctober 1957 to January1959, and Director GeneralPlans and Policy fromJanuary to August 1959. Output textInput message1010Reconstructed messageOther positions at theDepartment of Air includedAir Commodore Plans fromOctober 1957 to January1959, and Director GeneralPlans and Policy fromJanuary to August 1959. Hiding network1010TransformerEncoderRevealing networkTransformerEncoderTransformerDecodermines the secrecy of the watermark and enables adversaries
to automatically detect and remove the watermark.
b) Data hiding with neural networks: Data hiding can
be done in other mediums as well such as images [26].
Several end-to-end methods have been proposed to substitute
hand-crafted features and automatically hide and reveal data
(e.g., bit strings) in images. This can be done using a jointly
trained encoder and decoder architecture that is sometimes
coupled with adversarial training to enforce secrecy [27]–
[31]. However, automatic hiding approaches for language
are still lacking, which could be attributed to the relatively
harder discrete nature of language and having less redundancy
compared to images.
c) Our approach: We introduce the Adversarial Wa-
termarking Transformer (AWT); a solution for automatically
hiding data in natural language without having paired training
data or designing rule-based encoding. Similar to sequence-
to-sequence machine translation models [32], AWT consists
of a transformer encoder-decoder component that takes an
input sentence and a binary message and produces an output
text. This component works as a hiding network, which is
jointly trained with a transformer encoder that takes the output
text only and works as a message decoder to reconstruct the
binary message. We utilize adversarial training [33] and train
these two components against an adversary that performs a
classiﬁcation between the input and modiﬁed text. The model
is jointly trained to encode the message using the least amount
of changes, successfully decode the message, and at the same
time, fool the adversary. An example of using the data hiding
and revealing networks at test time is shown in Figure 1.
d) Evaluation axes: We evaluate the performance of
our model on different axes inspired by the desired require-
ments: 1) The effectiveness denoted by message decoding
accuracy and preserving text utility (by introducing the least
amount of changes and preserving semantic similarity and
grammatical correctness), 2) The secrecy of data encoding
against adversaries. 3) The robustness to removing attempts.
These requirements can be competing and reaching a trade-
off between them is needed. For example, having a perfectly
and easily decoded message can be done by changing the text
substantially, which affects the text preserving, or by inserting
less likely tokens, which affects the secrecy.
e) Contributions: We formalize our contributions as
follows: 1) We present AWT; a novel approach that is the
ﬁrst to use a learned end-to-end framework for data hiding
in natural language that can be used for watermarking. 2) We
study different variants of the model and inference strategies
in order to improve the text utility, secrecy, and robustness.
We measure the text utility with quantitative, qualitative,
and human evaluations. To evaluate the secrecy, we analyze
and visualize the modiﬁed text statistics and we evaluate
the performance of different adversaries. Besides, we study
the robustness under different attacks. 3) We show that our
model achieves a better trade-off between the evaluation axes
compared to a rule-based synonym substitution baseline.
II. RELATED WORK
We summarize previous work related to ours, such as lan-
guage watermarking and steganography, model watermarking,
and neural text detection.
A. Language Watermarking
Watermarking for multimedia documents has many appli-
cations such as identifying and protecting authorship [26],
[34]–[36]. It consists of an embedding stage where the hidden
information (i.e., watermark) is encoded in the cover signal,
followed by a decoding stage where the watermark is recov-
ered from the signal. Initial text watermarking attempts aimed
to watermark documents, rather than language, by altering
documents’ characteristics such as characters’ appearance,
fonts, or spacing, by speciﬁc patterns depending on the code-
word [37]. However, these methods are prone to scanning and
re-formatting attacks (e.g., copying and pasting) [34], [38].
The other category of methods relies on linguistic charac-
teristics of the natural language such as making syntactic or
semantic changes to the cover text [38]. An example of such
is the synonym substitution method in [20] in which WordNet
was used to ﬁnd synonyms of words that are then divided
into two groups to represent ‘0’ or ‘1’. The authors relied on
ambiguity by encoding the message with ambiguous words or
homographs (i.e., a word that has multiple meanings). This
was used to provide resilience as attackers would ﬁnd it hard
to perform automatic disambiguation to return to the original
sentence. However, words in the dataset were annotated/tagged
by meanings from the WordNet database. These annotations
were then used to select suitable synonyms, which does not
allow automatic methods with no human input. Generally, syn-
onym substitution methods are vulnerable to an adversary who
performs random counter synonym substitutions. In addition,
they perform ﬁxed pairwise substitutions which makes them
not ﬂexible and also vulnerable to detection.
Additionally, sentence structure can be altered to encode
the codeword according to a deﬁned encoding [22], [39].
These methods introduce changes such as passivization, cleft-
ing, extraposition, and preposing [38], [40]. However, these
transformations might not be applicable to all sentences, also,
they change the sentence to a large extent.
In contrast, we perform an end-to-end data hiding approach
that is data-driven and does not require efforts to design rules
and unique dictionary lookups.
B. Linguistic Steganography
Steganography hides information in text for mainly secret
communication. However, it might have different requirements
from watermarking [20], [27]; while both of them target
stealthiness to avoid detection, steganography does not assume
an active warden. Thus, watermarking should have robustness
to local changes. In our case, it should also preserve the
underlying cover text and utility.
Translation by modifying a cover text was used in steganog-
raphy such as the work in [41]–[43] that used a set of rule-
based transformations to convert tweets to possible transla-
tions. The encoding and decoding were done with a keyed hash
function ; the translations that map to the desired hash values
were selected. Therefore, the decoding is not robust to local
changes to the sentence. Another synonym-based method was
proposed in [44] based on assigning different bits to American
and British words which makes it not applicable to a large
number of sentences. Another direction is to generate text
according to a shared key, instead of using translation. For
example, the work in [45] used a trained LSTM language
model that generates sentences according to a masked vo-
cabulary and a binary stream; the vocabulary was partitioned
into different segments where each segment was assigned a
sequence of bits. However, this imposes a large constraint on
the usage of the language model since it needs to abide by
the masking. Therefore, these steganography solutions are not
suitable for our scenario as they speciﬁcally prioritize secret
communication over ﬂexibility or watermarking requirements.
C. Model Watermarking
To protect the intellectual property of deep learning models,
several approaches have been proposed to watermark mod-
els [46]–[49]. This could be done by embedding the watermark
into the model’s weights, which requires white-box access for
veriﬁcation [50]–[52], or by assigning speciﬁc labels for a
trigger set (i.e., backdoors [53]), which only requires black-
box access [46], [48], [54].
These methods were mainly addressing image classiﬁcation
networks; there is no previous work that attempted to wa-
termark language models. We also differentiate our approach
from model watermarking; instead of watermarking a model,
we study data/language watermarking using a deep learning
method that could eventually be used to watermark the lan-
guage model’s output.
Our task shares some similarities in requirements with
model watermarking (e.g., preserving model utility, authenti-
cation, and robustness against removal attempts), but they are
different in the objective and assumptions about attacks. While
the main purpose of model watermarking is to prove ownership
and protect against model stealing or extraction [55], our
language watermarking scheme is designed to trace prove-
nance and to prevent misuse. Thus, it should be consistently
present in the output, not only a response to a trigger set.
Moreover, while the adversary might aim to falsely claim
or dispute ownership in model watermarking/stealing [56],
we assume in our task that the adversary’s goal is not to
get detected or traced by the watermark. We elaborate on
this difference in Section V-D3. Finally, model stealing can
be done with white-box or black-box access to the victim
model [55], while we assume black-box access only to the
language and watermarking model.
D. Neural Text Detection
Similar to the arms race in image deepfakes detection [57]–
[59], recent approaches were proposed to detect machine-
generated text. For example, the Grover language model [5]
was ﬁne-tuned as a classiﬁer to discriminate between human-
written news and Grover generations. The authors reported that
the model size played an important factor in the arms race; if a
larger generator is used, the detection accuracy drops. Another
limitation was observed in [13] in which the authors ﬁne-tuned
BERT to classify between human and GPT-2 generated text.
The classiﬁer was sensitive to the decoding strategy used in
generation (top-k, top-p, and sampling from the untruncated
distribution). It also had poor transferability when trained with
a certain strategy and tested with another one. Therefore, while
detecting machine-generated text is an interesting problem, it
largely depends on the language model and decoding strategy.
Analogous to image deepfake classiﬁers’ limitations [60],
this suggests that the success of classiﬁers might drop based
on future progress in language modelling [5] (e.g.,
larger
models [11], arbitrary order generation [61], and reducing ex-
posure bias [62]), in addition to decoding strategies that could
reduce statistical abnormalities without introducing semantic
artifacts [13]. Thus, it now becomes important to provide more
sustainable solutions.
III. PROBLEM STATEMENT AND THREAT MODEL
In this section, we discuss our usage scenario, requirements,
assumptions about the adversary, and attacks.
a) Watermarking as a defense against models’ abuse:
We study watermarking as a sustainable solution towards
provenance tracing of machine-generated text in the case of
models’ abuse. An example of that scenario is a commercial
black-box language model API [15] or a text generation
service that has legitimate usages such as editing assistance.
The service is offered by the language model’s owner or
creator. However, it can be used in an unintended way by
an adversary to automatically generate entire fake articles or
misinformation at scale, aiming to achieve ﬁnancial gains or
serve a political agenda [5]. The owner can then proactively
and in a responsible manner provide a way to identify and
detect the model’s generations by watermarking its output [60].
News platforms can cooperate with the model owner, by
having a copy of the watermark decoder, in order to identify
the watermarks in the news articles and, thus, detect machine-
generated articles. That is similar to [5] that suggests that
news platforms can use the Grover classiﬁer to detect Grover’s
articles. This is also in line with video-sharing platforms such
as YouTube that uses deep networks to detect pornographic
content [63], and [64] which suggests using machine learning
classiﬁers to ﬂag videos that could be targeted by hate attacks.
b) Watermarking using AWT: The hiding network (mes-
sage encoder) of AWT is used by the owner to embed a
watermark (m) into the text. The same message encoder can
be used to encode different watermarks (m1, m2, ..., mn) if
needed (e.g., if the service is offered to different parties). The
multi-bit watermarking framework (as opposed to zero-bit)
helps to trace provenance to different parties. The revealing
network (message decoder) of AWT can, in turn, be used to
reveal a watermark m(cid:48) which is then matched to the set of
watermarks (m1, m2, ..., mn).