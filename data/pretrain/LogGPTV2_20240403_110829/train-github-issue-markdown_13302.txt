# ðŸš€ Feature request
Pytorch supports mimicking quantization errors while training the models.  
Here is the tutorial on this. For our NLP transformers, it requires a "fake
quantization" operation to be done on the embeddings. I found this repository
converting BERT to support this.
## Motivation
I think quantization aware fine-tuning (if it works) will help a lot of use-
cases where dynamic quantization alone doesn't suffice in maintaining the
performance of the quantized model. Supporting it out of the box will remove
the duplication of model code in end use cases.
## Your contribution
I can work on this ASAP. Would appreciate initial thoughts on what a the MVP
for it would be, any thoughts on the API (should we take in a "qat" boolean in
config?), any pitfalls that I should be aware of, etc.