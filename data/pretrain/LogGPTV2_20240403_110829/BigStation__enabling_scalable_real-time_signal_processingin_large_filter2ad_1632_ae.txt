H)
≈ N log (1 + P/(M/N − 1))
n=1
n,n](cid:1)
−1
(1)
(2)
where P is the total transmission power and H is the channel ma-
trix. Equation 2 converges to a constant value when M = N [15].
To avoid channel hardening, in our second experiment, instead of
limiting M = N, we use all 12 antennas regardless of how many
senders there are. Interestingly, we see the sum peak rate indeed
grows linearly as more senders transmit (solid line in Figure 12).
When there are 9 concurrent senders, the sum peak rate increases
by 6.8× compared to a single-antenna setup! Therefore, BigStation
indeed linearly scales wireless capacity if M > N.
The next natural question to ask is how many antennas should
BigStation have in order to support any given N clients? To answer
this question, we vary antenna conﬁgurations and measure the con-
dition number of the wireless channel matrix. The condition num-
ber shows how well the channel matrix inverse can be used to de-
multiplex spatial streams [21]. Well-conditioned channels, whose
condition numbers are small (close to 1), can decorrelate the spa-
tial streams without much distortion. Conversely, a large condition
number will signiﬁcantly reduce the SNRs of spatial streams after
0501001502002503001234Decoding throughput (Mbps)# of coresBigStation123456789050100150200250300350400Tx antenna (N)Peak rate (Mbps)M=NM=1212345678905101520253035Tx antenna (N)Condition number (dB)  beta = 1beta = 1.1beta = 1.4beta = 1.7beta = 2408Table 5: Delay of components in BigStation. Unit is in (µs). The
number in the brackets is the 90th percentile.
CI server
SD server
Net
CI
Net
SD
CD server
280 (410)
680 (1,190)
330 (450)
550 (990)
9 (9)
layer ACK, which requires micro-second level latency, it already
satisﬁes the real-time requirements for many other wireless proto-
cols, e.g., LTE and WCDMA. Finally, we note that when the system
is heavily loaded, the processing delay exhibits a heavy-tailed dis-
tribution, where a small portion of the frames may experience ex-
cessively long delays. This long tail latency can be mitigated with
resource over-provisioning, but at the expense of low system efﬁ-
ciency. Alternatively, we can apply many techniques developed for
predictable service times in the distributed systems community to
control the latency in our distributed MU-MIMO processing [5,10].
We defer a deep investigation in this direction to our future work.
8. RELATED WORK
MU-MIMO has been extensively discussed in the information
theory literature. Small-scale MU-MIMO (i.e., M  1.4, the reduction be-
comes less pronounced.
System delay. Besides the processing throughput, the processing
latency is another critical metric for wireless communications. In
the following, we characterize the delay performance of BigStation.
To measure the overall processing delay, instead of sending to a CD
sever, we let the SD servers send symbols back to the FS server af-
ter spatial demultiplexing. The FS server can timestamp both the
original symbol generation and the return of the corresponding de-
multiplexed symbol, and compute the delay. We note that this mea-
sured delay excludes the channel decoding delay, which is ﬁxed to
be 9 µs according to our benchmark.
We ﬁrst measure the processing latency of BigStation in a light
load situation, where the FS servers generate a frame every 10 ms.
Figure 14(a) shows the cumulative distribution function (CDF) of
processing delay. We can see that the mean processing latency is
as low as 860 µs, and the 90th percentile is below 1.4 ms. How-
ever, when the trafﬁc load becomes heavier, a heavy-tailed delay
CDF appears. Figure 14(b) shows the latency measurements when
the FS servers continuously generate back-to-back frames. We can
observe that while the mean latency is still around 860 µs, a small
portion of the frames may experience excessive delay (the 90th per-
centile is 20 ms). After a closer examination, we ﬁnd this behavior
is due to TCP retransmissions. Under a heavy load, the underlying
network may occasionally see packet losses and TCP retransmis-
sions. The SD server requires symbols from all antennas before
it can ﬁnalize the output. Therefore, even if one TCP connection
slows down, the entire MU-MIMO frame, as well as a few subse-
quent frames, is delayed.
Table 5 summarizes the delay breakdown of various components
in BigStation. The data presented here are measured in the light
load situation. In this case, we ﬁnd the network delay is actually
small (∼ 300µs). This is because our application-level rate con-
trol can keep the network queues small. Instead, most of the delay
is incurred while the symbol packets are waiting in queues on the
CI/SD servers. This behavior is also because the SD (CI) server
requires symbols from all antennas before deriving ﬁnal results.
Therefore, the variance in the packet transmission times from dif-
ferent FS servers will translate into a queuing delay on the SD (CI)
server.
In conclusion, BigStation has a low mean processing delay (<
1ms). While this delay may not be sufﬁciently low for 802.11 MAC
00.10.20.30.40.50.60.70.80.9110100100010000CDFEnd-to-end delay (micro-seconds)00.10.20.30.40.50.60.70.80.9110100100010000CDFEnd-to-end Delay (micro-seconds)409and low-cost devices. Consequently, BigStation achieves incremen-
tal scalability by adding more computing devices. Our experiment
on a BigStation prototype with 12 antennas shows a peak rate gain
of 6.8× compared to the single-antenna radio. In comparison, Ar-
gos only reports a 5.7× capacity improvement with 64 antennas,
due to suboptimal conjugate processing.
The comparison in theory between zero-forcing and the conju-
gate processing (also called matched ﬁlter) is presented in [13]. Our
results agree with [13], but we use real measured data from a prac-
tical large MIMO system.
BigStation is also related to much parallel computing work. Many
schemes to parallelize the digital signal processing in BigStation
have been previously studied in other contexts [7]. However, as far
as we know, BigStation is the ﬁrst work to parallelize MU-MIMO
operations to scale the system to tens or hundreds of antennas.
9. CONCLUSION
This paper presents BigStation, a scalable architecture for large-
scale MU-MIMO systems. Our strategy to scale is to extensively
parallelize the MU-MIMO processing on many simple and low-
cost commodity computing devices. Therefore, our design can in-
crementally scale to support more MIMO antennas by proportion-
ally adding more processing units and interconnecting bandwidth.
After carefully analyzing the computation and communication pat-
terns of MU-MIMO, we parallelize MU-MIMO processing with a
distributed pipeline to reduce the overall processing delay. At each
stage of the pipeline, we further use data partitioning and computa-
tion partitioning to increase the processing speed.
We have built a BigStation prototype with 15 PC servers and
standard Ethernet switches. Our prototype can support real-time
MU-MIMO processing for 12 antennas. Our benchmarks show that
the BigStation architecture is able to scale to tens to hundreds of
antennas. With 12 antennas, our BigStation prototype can increase
the wireless capacity by 6.8× with a low mean processing delay of
860 µs. This latency already satisﬁes the real-time requirements of
many existing wireless standards, e.g., LTE and WCDMA.
10. ACKNOWLEDGMENT
We sincerely thank our shepherd, Brad Karp, and the anonymous
reviewers for their valuable comments and suggestions.
11. REFERENCES
[1] 3GPP TS 36.201-820: Evolved Universal Terrestrial Radio
Access (E-UTRA); Long Term Evolution (LTE) physical
layer; General description.
[2] C-RAN: The Road Towards Green RAN.
http://labs.chinamobile.com/cran/wp-
content/uploads/CRAN_white_paper_v2_5_EN(1).pdf.
[3] HP ProLiant DL560 Gen8 .
http://h10010.www1.hp.com/wwpc/us/en/sm/WF06b/15351-
15351-3328412-241644-3328422-5268290-5288630-
5288631.html?dnr=1.
[4] IEEE Standard for Local and Metropolitan Area Networks
Part 11; Amendment: Enhancements for Very High
Throughput for operation in bands below 6GHz. IEEE Std
P802.11ac/Draft 4.0, 2012.
[5] M. Alizadeh, A. Kabbani, T. Edsall, B. Prabhakar, A. Vahdat,
and M. Yasuda. Less is More: Trading a little Bandwidth for
Ultra-Low Latency in the Data Center. In Proceedings of
NSDI, 2012.
[6] E. Aryafar, N. Anand, T. Salonidis, and E. W. Knightly.
Design and experimental evaluation of multi-user
beamforming in wireless LANs. In Proceedings of MobiCom,
pages 197–208, New York, NY, USA, 2010. ACM.
[7] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed
Computation: Numerical Methods. Athena Scientiﬁc, 2003.
[8] S. Bhaumik, S. P. Chandrabose, M. K. Jataprolu, G. Kumar,
A. Muralidhar, P. Polakos, V. Srinivasan, and T. Woo.
CloudIQ: A framework for processing base stations in a data
center. In Proceedings of MobiCom, pages 125–136, 2012.
[9] Cisco Inc. Cisco Visual Networking Index (VNI): Forecast
and Methodology 2011-2016. Cisco,
http://www.cisco.com/en/US/solutions/collateral/ns341/ns525/ns537/
ns705/ns827/white _paper_c11-
481360_ns827_Networking_Solutions_White_Paper.html,
2012.
[10] J. Dean and L. A. Barroso. The Tail at Scale.
Communications of the ACM, 56(2), 2013.
[11] B. Hochwald and S. Vishwanath. Space-Time Multiple
Access: Linear Growth in the Sum Rate. In Proceedings 40th
Annual Allerton Conf. Communications, Control and
Computing, 2002.
[12] J. Hoydis, S. ten Brink, and M. Debbah. Massive MIMO:
How many antennas do we need? In Allerton Conference on
Communication, Control, and Computing, September 2011.
[13] H. Huh, G. Caire, H. Papadopoulos, and S. Ramprashad.
Achieving "Massive MIMO" Spectral Efﬁciency with a
Not-so-Large Number of Antennas. IEEE Transactions on
Wireless Communications, 11(9):3226 –3239, September
2012.
[14] J. Neel, P. Robert, and J. Reed. A Formal Methodology for
Estimating the Feasible Processor Solution Space for A
Software Radio. In Proceedings of the SDR Technical
Conference and Product Exposition, 2005.
[15] C. Peel, B. Hochwald, and A. Swindlehurst. A
Vector-perturbation Technique for Near-capacity
Multi-antenna Multi-user Communication — Part I: Channel
Inversion and Regularization. IEEE Transactions on
Communications, 53(1):195–202, 2005.
[16] H. S. Rahul, S. Kumar, and D. Katabi. JMB: Scaling wireless
capacity with user demands. In Proceedings of ACM
SIGCOMM, pages 235–246, 2012.
[17] F. Rusek, D. Persson, B. K. Lau, E. Larsson, T. Marzetta,
O. Edfors, and F. Tufvesson. Scaling Up MIMO:
Opportunities and Challenges with Very Large Arrays. IEEE
Signal Processing Magazine, 30(1):40 –60, January 2013.
[18] C. Shepard, H. Yu, N. Anand, E. Li, T. Marzetta, R. Yang,
and L. Zhong. Argos: Practical many-antenna base stations.
In Proceedings of MobiCom, pages 53–64, 2012.
[19] K. Tan, H. Liu, J. Fang, W. Wang, J. Zhang, M. Chen, and
G. Voelker. SAM: Enabling Practical Spatial Multiple Access
in Wireless LAN. In Proceedings of MobiCom, 2009.
[20] K. Tan, J. Zhang, J. Fang, H. Liu, Y. Ye, S. Wang, Y. Zhang,
H. Wu, W. Wang, and G. M. Voelker. Sora: High
performance software radio using general purpose multi-core
processors. In NSDI 2009.
[21] D. Tse and P. Vishwanath. Fundamentals of Wireless
Communications. Cambridge University Press, 2005.
[22] A. J. Viterbi and J. K. Omura. Principles of digital
communication and coding. McGraw-Hill, 1979.
[23] H. Wu, Z. Feng, C. Guo, and Y. Zhang. ICTCP: Incast
Congestion Control for TCP in data center networks. In
Proceedings of CoNEXT, pages 13:1–13:12, 2010.
410