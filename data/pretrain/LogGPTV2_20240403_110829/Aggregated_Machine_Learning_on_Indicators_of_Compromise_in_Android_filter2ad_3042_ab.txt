security research in the field lags behind. There have been a number of incidents that demonstrate 
that current mobile malware detection and prevention techniques are largely ineffective. 
For example, in 2017, Kaspersky labs discovered a malicious app known as SkyGoFree [2], which 
had been available on third-party app stores and side-loading websites since 2014. This app has 
several advanced features, including the ability to selectively record data from the camera and 
microphone, with GPS location being used as a primary selection criteria. This allows the phone to 
record, for example, every conversation its owner has in the company office. SkygoFree can also use 
assistive technology, such as screen readers, to read information from otherwise well-protected 
encryption applications like Whatsapp. All recorded data, user contacts, and stored personal 
information is exfiltrated when the app surreptitiously connects to Wi-Fi networks–even if the device 
is in airplane mode. As an afterthought, the app will also use the infected devices to engage in SMS 
and click-fraud. 
Conventional wisdom – and the SkyGoFree case – suggest that the strongest protection against this 
sort of malicious app is to only install apps from the official stores. While this strategy may provide 
more protection, it doesn’t provide complete protection. As an example, the ExpensiveWall malware 
[8, 19] – which engaged in SMS fraud, pay-per-click fraud, and data exfiltration – worked its way 
into the Google Play Store4 as a variety of mobile wallpaper apps. The developers used packing 
techniques to obfuscate the malicious code in their APKs, a technique that successfully bypassed 
Play Store security measures. When the malware was discovered, researchers estimated that it had 
managed to infect approximately 21 million devices. 
4https://play.google.com/store 
5 
In an even more significant case, the Facebook Messenger app, actively in use on 1.2 billion 
devices in 2017, was recently shown to be collecting not just user-provided information, but also 
SMS and call data from users’ devices. This data collection may be in violation of Facebook user 
consent policies and a 2011 agreement between Facebook and the Federal Trade Commission. This 
data was stored for years on Facebook servers before being scraped and parsed by Cambridge 
Analytica during the 2016 election cycle [14]. The same information could just as easily have been 
scraped and used by adversaries seeking to construct professional and personal networks that could 
be used for social engineering and other intelligence gathering. 
This sort of intelligence gathering was achieved by accident, when the Strava mobile fitness app 
published a heat map of popular running routes around the world. Researchers very quickly identified 
US forward military bases in the Middle East, as well as facilities in use by other militaries [12]. This 
app, which was completely open about its data collection and usage, had managed to reveal sensitive, 
mission-critical information and place users at risk by revealing common patterns of movement and 
behavior. 
The DoD and its personnel are also deliberately targeted by malicious apps. According to the US 
Department of Defense website’s Mobile App Gallery5, there is an unsanctioned in-the-wild app 
targeting Thrift Savings Plan (TSP) participants who want to manage their retirement savings from 
their mobile devices. The app, “TSP Funds”, which was not developed by any organization 
associated with the DoD or the TSP program, prompts the user to provide a username and password, 
enabling the developers to gain unauthorized access to the sensitive financial information of DoD 
personnel. 
Even sanctioned apps present an attack surface that is in need of examination. DoD has released an 
app called Defense Finance and Accounting Service (DFAS). This app provides DoD employees 
with access to information about their salaries, taxes and benefits. To an adversary, this is a treasure 
trove of Personally Identifiable Information (PII), including personal finances and social security 
information. 
These incidents demonstrate a need for improvement in the field of mobile security. Each app, 
whether intentionally or not, exposed sensitive information to malicious actors. The MAVeRiC effort 
seeks to identify apps that expose information improperly, and either prevent their installation, or 
remove them if their inappropriate behavior is discovered later. 
2.2  HOW THE NAVY IS DOING MOBILE 
The navy is developing policies to support the adoption of mobile devices and apps. In some areas, 
Android and iOS devices are being prepared by administrative staff, then issued to navy personnel. 
End-users may use the devices for email, telecommunications, and other business related functions. 
There are also pilot programs seeking a cost savings through the use of BYOD policies. These 
devices are granted access to business related functions and communications, but they are also 
integrated into users’ personal lives, and apps installed on the devices reflect this. Both government-
issued devices and BYOD devices are subject to Mobile Device Management (MDM) tools, which 
allow the organization to remotely manage device security controls, limit app installation, track 
device activity, and erase data from a device. 
5https://www.defense.gov/Resources/Developer-Info/Apps-Gallery/ 
6 
2.2.1  How the Navy is doing mobile security 
One approach to mobile security is containerization, as demonstrated by Good Technology’s 
Secure Enterprise Mobility Management (EMM) Suites. In principle, a containerization solution can 
completely partition a phone into two (or more) isolated environments, such that data in one 
container is not accessible to apps in another container. Even if one container were compromised by 
a malicious app, the other containers would be able to maintain data confidentiality. In practice, 
researchers have demonstrated that malicious apps in one container can gain access to underlying 
kernel modules and, from there, access the processes and data that should be isolated within different 
containers [13]. 
The Defense Information Security Agency (DISA) is the entity responsible for policy and practices 
related to mobile device and app security for the US Government. According to the DISA DOD 
Mobility Applications webpage6, each app is vetted via a rigorous process including static, dynamic, 
and network analysis. The results of these analyses are mapped against requirements from the Mobile 
Application Security Requirements Guide (MAppSRG) [26], National Information Assurance 
Partnership (NIAP)7, Protection Plan (PP), and the Open Web Application Security Project 
(OWASP)8, among others. 
DISA reports that “over 200 approved apps” are available for download in the DoD. As of 
December 2017, the Google Play Store has 3.5 million apps. A major goal of the MAVeRiC project 
is to accelerate this decision-making process, while maintaining or improving the accuracy of 
detection for malicious code. This improvement is vital if the DoD intends to bring the use of mobile 
devices and apps to its full potential. 
7https://www.niap-ccevs.org/ 
8https://www.owasp.org/ 
7 
This page is intentionally blank. 
3.  THE MAVERIC APPROACH TO DYNAMIC ANALYSIS FOR MOBILE 
(ANDROID) APPLICATION SECURITY 
Dynamic Analysis for malware detection in mobile devices is a popular area of research that still 
has not produced a reliable malware detection framework. Many researchers have explored this topic, 
but each tends to focus their attention on a single Indicator of Compromise (IOC). An IOC is a 
measurable event that can be identified on a host or network [15] and which may indicate the presence 
of a compromise within that system. A single IOC does not provide sufficient confidence to reliably 
claim that malware is present on a mobile device. For example, a malicious app that continuously 
transmits recordings from the device camera and microphone will have a significant impact on device 
power consumption, but so will playing a game with high-resolution graphics. 
The MAVeRiC framework collects data related to three different IOCs: power consumption, 
network behavior, and sequences of system calls. The complete feature set is analyzed using machine 
learning techniques to detect anomalies and classify them as benign or malicious. This is a holistic 
approach to detecting malicious or unintended behaviors within applications and can provide greater 
accuracy over models which rely on a single IOC. This paper presents an approach to finding the best 
machine learning methodology for detecting malicious behavior in Android applications using 
multiple IOCs. 
Figure 2. MAVeRiC’s  approach  to  dynamic  analysis  is  as  follows:  Known  good  and  bad  applica-  tions 
are monitored for power consumption, network activity, and system calls. Both supervised and 
unsupervised machine learning techniques are utilized for detecting IOCs. 
9 
3.1  FEATURE SETS 
3.1.1  Rationale for Collecting Power Consumption 
The power consumption of an app presents an indicator of compromise for an analyst. Power 
consumption varies depending on the state and activities of the apps on a device. Collecting 
information on  power consumption allows researchers to construct baselines for expected power 
consumption of a device  based on which apps are running at a given time. Discrepancies serve as 
IOCs that should be investigated  for possible malice. There has been some success in using machine 
learning approaches to detect malicious activity on covert channels. The effort described in [6] 
provides a detection framework that collects power-related data using the PowerTutor9 application and 
relies on regression-based and classification-based methods. The MAVeRiC capability leverages their 
approach towards collecting expected power consumption of mobile apps as well as analyzing features 
specific to power consumption. 
3.1.2  Rationale for Collecting Network Activity 
Network activity is an IOC that should be considered when identifying malicious behavior of 
mobile  apps. Many of the components and programs installed on a mobile device are the same as 
those found on conventional computers, and so mobile devices share vulnerabilities with their larger 
counterparts,  especially in regards to network communications. Mobile devices are nearly always 
communicating via  network connections, whether on cellular or WIFI networks. Many of the 
legitimate applications on a  mobile device are constantly polling the network to see if any new 
application information is available.  MAVeRiC collects data on the state of all network 
communications. For each app, it is important to know the amount of data being sent, the frequency 
of send/receive communications, whether the app is running  in the foreground or the background, 
etc [20]. This data is vital in understanding the normal behavior of  apps–individually and 
collectively–and identifying when there may be malicious or unexpected activity. 
9http://ziyang.eecs.umich.edu/projects/powertutor/ 
10 
3.1.3  Rationale for Collecting Sequences of System Calls 
The sequence in which system calls are made has also shown to be an important IOC for detecting 
malware in an Android device. System calls are how an application accesses operating system 
services [3].  These are underlying actions that user-level processes cannot be trusted to perform on 
their own, but which  need to be performed in order to provide full application functionality. System 
calls allow these actions to  be delegated to the trusted authority of the operating system kernel [1]. 
System calls can be organized into  multiple categories [3]: 
1.  Process Control 
2.  File Management 
3.  Device Management 
4.  Information Management 
5.  Communication 
The sequence with which system calls are called may indicate that an application is behaving 
maliciously. By capturing execution trace information of a given mobile application, researchers can 
analyze how an app uses the more than 250 system calls that are provided by the Android OS. In [5], 
researchers captured patterns of application behavior and constructed a fingerprint based on frequency 
of  system calls within a given time frame. Then they used a probabilistic approach to find outliers in 
the expected values of these frequencies. MAVeRiC also captures system call sequences as part of the 
dynamic analysis to compare expected sequences over time for a given application. Anomalies in 
system call  sequences serve as IOCs that may identify malware that is executed at random times and 
would not  otherwise be easy to distinguish during normal operation. 
3.2  DATA ANALYSIS 
MAVeRiC evaluates two distinct approaches for evaluating the effectiveness of using machine 
learning to identify malicious behavior of an application. Both approaches use machine learning 
algorithms to assess the data collected from the three IOCs described above. 
The first approach combines all three sets of IOC data into a single superset. The entire superset is 
assessed by multiple machine learning algorithms. During this phase, the data is run through feature 
selection algorithms to reduce the number of individual features under test. This increases the speed 
and efficiency of identifying malicious behavior of an application on a device. Both supervised and 
unsupervised algorithms are examined in the evaluation process, as described in Section 4.5.  
The second approach evaluates each of the IOCs separately, then subjects the results to further 
analysis. Previous research has looked at all three of these IOCs individually. Those efforts have also 
evaluated  multiple machine learning algorithms for each IOC and shown that different algorithms 
are most effective  for the distinct IOCs. MAVeRiC attempts to recreate this previous work and use 
the results to populate a new data set for evaluation. One evaluation technique is to nest machine 
learning algorithms, where the  initial results of one algorithm are then analyzed through another 
algorithm. The second option involves  developing an algorithm where the collective results of the 
IOCs’ machine learning algorithms are used as inputs. The outcomes of these approaches are the 
basis for the comparative study and evaluation of which  approach performs the best. 
11 
This page is intentionally blank.  
4.  EXECUTION PLAN 
MAVeRiC builds upon previously published work to collect data from multiple IOCs. The data is 
collected through separate applications and then sent to an off-device MongoDB10 server. Then the 
data is  analyzed using machine learning methods. A description of our data collection methods 
follows. 
4.1  POWER CONSUMPTION 
Power Consumption of a given application is not a measurement that can be analyzed using static 
methods. It must be monitored while the app is running on a device. All devices are not created equal, 
and there are many factors that contribute to the rate at which power is consumed on a given device. 
We are  using an on-device tool named PowerTutor to collect power usage statistics. The official 
PowerTutor  repository was last updated in April 2013, but we have forked the code and modified it to 
run with more  current versions of Google’s Android API. We have also added functionality that 
enables users to send  collected data to an off-device server. PowerTutor collects data from a running 
application as well as power  consumption records for each hardware component used by that app. 
PowerTutor models the power usage  of the following hardware components: CPU, OLED/LCD, 
WIFI, Cellular Network, GPS, and audio. Attributing changes in these hardware component values to 
individual apps installed on a device will help  in understanding the power usage patterns of apps 
within our control group. 
4.2  NETWORK ACTIVITY 
Capturing network activity is important for correlating network behaviors and patterns within 
mobile  apps to characterize baseline behavior. At install, static analysis of an application may not 
capture the elements to detect maliciousness in the network patterns. MAVeRiC analyzes deviations 
in network behavior to identify malicious activity. The need for dynamic analysis of network 
behavior stems from  weaknesses in static analysis to address apps that introduce malicious code at 
runtime or when updates are installed [16]. MAVeRiC leverages the Wireshark plugin, Android 
dump11 to collect and aggregate both cellular and Wi-Fi network activity, then sending the data off-
device to the server for analysis. 
4.3  SEQUENCE OF SYSTEM CALLS 
Sequences of system calls can be used to identify common app behaviors and distinguish between 
benign activities and potentially malicious ones. Prior to installation, the only way to know how an 
app will  communicate with the system is by validating its binary code against app permissions, as 
listed in the APK  manifest file. This knowledge may be incomplete, due to techniques–such as code 
obfuscation and custom  permissions [16]–that are designed to deceive static analysis methods. App-
operating system interaction is  an observable trait that can be used to categorize app in terms of both 
core functionality and malice. Android Debug Bridge (ADB) [24], provided by the Android 
framework, is a tool that can communicate  with an Android device. Using the ADB Strace  
10https://www.mongodb.com/ 
11https://www.wireshark.org/docs/man-pages/androiddump.html 
13 
function, MAVeRiC collects the system calls an app requests during use. In order to generate a 
sufficient volume of data for analysis, MAVeRiC employs a tool called Monkey[25] to generate 
pseudo-random user activity with an application. The collected inputs and  system call sequences are 
sent to the off-device server for further analysis. 
Over time, we expect sequences of identical or similar user inputs (e.g., Monkey-generated clicks, 
touches, gestures...) to a benign app to produce identical or closely related system call patterns. If we 
malicize an app by inserting malicious functionality, we expect different system call patterns in 
response to  the same user inputs. The behavioral differences between benign and malicious apps are 
used to train our machine learning systems in the classification of malicious behavior. 
4.4  APPLICATION SET 
Our application dataset has been constructed from a combination of sources. MAVeRiC has 
obtained known malicious applications from the Drebin12 and Androzoo13 repositories and a set of 
benign apps from the Google Play Store. We have also generated a control group of apps for testing 
by adding specific  malice to certain classes of applications with desired traits. We leveraged open-
source applications from  the F-Droid14 repository, for which source code is available, and inserted 
malicious functionality. We then  re-package them into a malicized version of the original benign 
app. MAVeRiC is using data collected from these apps to measure the effectiveness of each of the 
machine learning algorithms. 
4.5  MACHINE LEARNING METHODOLOGY 
As discussed in the previous section we are evaluating two approaches to determine how 
MAVeRiC will detect malicious apps on a device. In the first approach MAVeRiC combines all of 
the data from the three separate IOC’s into a single superset. This process entails a training period 
consisting of feature selection  and model selection. MAVeRiC is constantly collecting data from 
mobile devices, and so power drain on a device is a concern and collecting all features may be 
expensive. MAVeRiC is looking for the features that are most relevant for making predictions. Prior 
to the training phase the data is run through some feature  selection methods to determine which 
features are the most relevant. Once an optimized set of data is  selected MAVeRiC inputs the data 
through both supervised and unsupervised machine learning models. 
12https://www.sec.cs.tu-bs.de/˜danarp/drebin/ 
13https://androzoo.uni.lu/ 
14https://f-droid.org/ 
14 
In this approach the supervised learning model is for classification purposes. Our dataset of 