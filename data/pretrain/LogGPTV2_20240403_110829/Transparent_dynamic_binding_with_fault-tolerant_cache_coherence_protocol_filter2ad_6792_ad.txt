execution.  As 
is  because 
analyzed 
before, 
the 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:46:13 UTC from IEEE Xplore.  Restrictions apply. 
298e
m
i
t
n
u
r
d
e
z
i
l
a
m
r
o
N
1.1
1.0
0.9
0.8
 4P 
 8P 
 16P 
 32P
lu
fft
c o n
b arn es
Figure 8.   Overall performance of TDB proposal
c h olesk y
ra dix
ra dio sity
G
V
A
)
%
(
n
w
o
d
k
a
e
r
b
y
c
n
e
t
a
l
e
r
i
t
e
r
.
t
s
n
i
y
r
o
m
e
m
c
i
f
f
a
r
T
k
r
o
w
t
e
N
d
e
z
i
l
a
m
r
o
N
100
80
60
40
20
0
 500
lu
fft
co n-ocean
barnes
ch olesky
radix
radiosity
A V G
Figure 9.   The retirement latency of memory instructions 
1.1
1.0
0.9
0.8
0.7
0.6
e
z
i
s
l
a
e
d
i
h
t
i
w
e
m
i
t
n
u
r
d
e
z
i
l
a
m
r
o
N
 16 entries 
 32 entries
lu
fft
con-ocean
barnes
cholesky
radix
radiosity
A V G
Figure 10.  Runtime with different victim buffer size
1.1
1.0
0.9
0.8
lu
fft
 4P 
 8P 
 16P 
 32P
c o n
Figure 11.  Overall network traffic of TDB proposal 
c h ole sk y
b arn es
ra dix
ra dio sity
G
V
A
other  words,  we  refuse  to  replace  other  L1  cache  data 
blocks  with  the  data  blocks  brought  in  by  mispredicted 
memory instructions, which reserves the essential locality. 
B.  Study of Conservative Private Cache Ingress Rule 
In  this  subsection,  we  run  extra  experiments  to 
obtain the retirement  latency  of the  memory instructions 
that experiences L1 cache misses. As we can see in Figure 
9,  in  an  8-core  CMP  system,  the  proportion  of  the 
retirement latency for the memory instructions that causes 
L1 cache miss and therefore bring in new data blocks is 
13.4%,  39.03%,  10.64%,  8.61%  when  considering  the 
timescale of 10, 20, 50 and 500 cycles respectively. We 
also  find  that  the  proportion  is  less  than  1%  if  the 
timescale is 1000 cycles (not shown in Figure 9). In this 
paper,  we  select  500  cycles  as  the  retirement  threshold. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:46:13 UTC from IEEE Xplore.  Restrictions apply. 
299are 
the 
techniques  are  good  enough 
The  memory  instructions  unretired  500  cycles  after  the 
considered 
execution 
as  mispredicted 
stage 
instructions,  and 
referenced  data  blocks  are 
correspondingly considered as falsely fetched data blocks. 
It  is  amazing  to  see  that  the  falsely  fetched  data  blocks 
account  for  such  a  large  proportion  (28.3%  with  the 
threshold of 500 cycles). The reason is that, although the  
branch  prediction 
to 
minimize  the  misprediction,  the  mispredicted  memory 
instructions are more likely to incur L1 cache misses than  
those from the correct paths due to data locality principle. 
Our conservative ingress rule utilizes the victim buffer to 
prevent  such  unretired  data  blocks  from  polluting  the 
private caches,  which in turn improves the performance. 
So  the  victim  buffer  produces  the  abovementioned 
positive effect, although its main usage is to help maintain 
the  master-slave  consistency.  Note  that  previous  works 
have studied the effects of the wrong path executions. The 
details can be referenced from [32-34]. 
C.  The Sensitivity to Victim Buffer Size 
the 
According  to  the  conservative  ingress  rule,  the 
victim  buffer  structure  is  designed  in  every  core  to 
preserve  the  speculative  data  blocks.  The  victim  buffer 
size  cannot  be  too  large  as  the  private  cache  capacity  is 
strictly limited. In order to allocate appropriate entries for 
the victim buffer structure, experiments are conducted to 
evaluate the sensitivity of the runtime to the victim buffer 
size. As we can see from Figure 10, the average runtime 
overhead of a 16-entry victim buffer configuration is only 
0.7%  over 
ideal  victim  buffer  configuration. 
Meanwhile,  the  number  of  active  victim  buffer  entries 
never exceeds 32 during the execution. That is because L1 
cache  miss  rate  is  relatively  low  and  our  selected  time-
bound  of  500  cycles  can  help  to  send  away  the  falsely 
fetched data blocks. For benchmarks fft and radiosity, the 
performance with 32-entry victim size experiences even a 
bit worse than that with 16-entry size. The reason is that 
the  larger  size  of  victim  buffer  may  cause  more  falsely 
fetched  data  blocks  be  unnecessarily  buffered.  External 
access  of  such  blocks  will  experience  longer  state 
transition  latency  and  thus  the  runtime  suffers.  Overall, 
our results suggest that a small hardware is well adequate 
for our conservative ingress rule design. 
D.  Network Traffic of TDB Proposal 
implements 
In order to provide the flexibility of dynamic binding, 
the  master-slave 
our  TDB  proposal 
communication  via  an  on-chip  network 
instead  of 
dedicated  channels.  Both  checkpointing  and  coarse-
grained  comparison  of  the  master-slave  compressed 
results during the checkpoint interval inevitably increases 
the on-chip network traffic. However, our TDB proposal 
prevents  the  slave  cores  from  accessing  the  global 
memory,  which  as  a  result  mitigates  the  network  traffic. 
The combined effect results in the overall network traffic  
 TDB  
 DCC
4P
8P
16P
32P
 TDB 
 DCC
e
m
i
t
n
u
R
1.6
1.4
1.2
1.0
c
i
f
f
a
r
T
k
r
o
w
t
e
N
1.1
1.0
4P
32P
Figure 12.  Comparison between TDB and DCC in tiled-CMP 
16P
8P
architecture 
overhead.  Note  that  our  baseline  simulates  the  baseline 
fault-tolerant  system  and  we  assume  that  the  slaves 
generate  the  same  network  traffic  as  the  masters  do.  As 