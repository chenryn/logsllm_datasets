The results in Figure 10 use a continuous distribution for the
real data, so there are no data points with di = 0. Because one of
the crucial differences between our algorithms is the method for
handling these zero values, we also consider the effect when there
are a large number of zeros in the full version. Overall, we see that
both in situations with no zero values and situations with many,
our test achieves the rigorous privacy guarantees of the TC High
Privacy test while achieving greater utility than the TC High Utility
test.
Relative contribution of improvements. Given that we make two
meaningful changes to the TC test, one might naturally wonder
whether both are truly useful or whether the vast majority of the
improvement comes from one of the two changes. To test this, we
compare to an updated variant of the TC test where we calculate
critical values exactly through simulation, as we do in our algorithm,
but otherwise run the TC test unchanged (referred to as "High
14A private estimate could be released, but one would have to devote a significant
portion of the privacy budget for the hypothesis test to this estimate, greatly decreasing
the accuracy/power of(cid:101)Wstat.
15Task and Clifton do not discuss how to choose k, and in our experimental compar-
isons we set k = 15, the same value they use.
an appropriate reference distribution for inference. In the public
setting, one can convert a test on regression coefficients to a one
sample t-test but choosing a constant independent variable and
making the sample data the dependent variable. However, Sheffet’s
method only works when all variables are significantly spread out,
so this method fails.
Here we propose what we believe is the first private version
of a one sample t-test, with two arguable exceptions. The first is
simultaneous work by Gaboardi et al. [13] in the local privacy
model. We compare our results to theirs in more detail in Section
5.5. The other work is that of Solea [27], but according to Solea’s
own experiments that test often gives type 1 error rates well above
the chosen α for many parameter choices, so we don’t consider it a
usable test.
The database for a one sample t-test has observations x1, . . . , xn
assumed to come from a normal distribution with mean µ and
standard deviation σ. (For paired data, each observation is the
differences between the observation in the two groups). The test
statistic is given by Tstat(x) = —x
, where ¯x is the mean of the data
s/√
and s is the standard deviation of the data.
n
A private t-test. As before, we achieve privacy through the addi-
tion of Laplacian noise, but the sensitivity of Tstat is unbounded, so
we instead release separate private estimates of the numerator and
denominator. For this analysis, similar to the private ANOVA tests
[28], we assume that the data is scaled such that all observations are
on the interval [−1, 1]. We first find the sensitivities of ¯x and s
2 and
then use post-processing, composition, and the Laplace Mechanism
to combine these to obtain the private t-statistic. In the case where
2 is estimated to be negative, the test statistic cannot be computed
s
as normal, and we return 0, indicating an unwillingness to reject
the null hypothesis.
Input: x, ϵ ¯x , ϵs2
ϵ ¯x
)
2 = s
2
n
)
Theorem 5.3. The sensitivity of ¯x is 2
n .
5
2 is
Theorem 5.4. The sensitivity of s
n−1 .
See the full version for proof of Theorem 5.3 and 5.4.
2 + Lap( 5/(n−1)
ϵs2
< 0 then
Algorithm(cid:101)Tstat: Private t-Test Statistic
(cid:101)¯x = ¯x + Lap( 1/n
(cid:101)s
if(cid:101)s
(cid:101)T = 0
else(cid:101)T = (cid:98)¯x/n√(cid:98)s2/√
Output:(cid:101)T
Theorem 5.5. Algorithm(cid:101)Tstat is (ϵ ¯x + ϵs2)-differentially private.
Proof. By the Laplace mechanism, the computation of(cid:101)¯x is ϵ ¯x -
differentially private and the computation of(cid:101)s
private. Since the computation of(cid:101)T does not require access to the
2 is ϵs2-differentially
database, it is only post-processing and its release is (ϵ ¯x + ϵs2)-
differentially private.
□
Figure 10: Power of the TC test,(cid:103)WPp, and the public test at
various n. (Effect size: µu − µv = 1σ, ϵ = 1; α = .05; normally
distributed sample data)
Privacy +" and "High Utility +"). The result is presented in Figure 11,
where we find the resulting algorithm to rest comfortably between
the original TC test and our proposed test. This means that both
the change to the critical value calculation and the switch to the
Pratt method of handling di = 0 rows are important contributions
to achieving the power of our test.
Figure 11: Power comparison of the TC algorithms, the TC
algorithms with our critical values (denoted with a +), our
new algorithm, and the public algorithm at various sample
sizes n. (Effect size: µu − µv = 1σ; ϵ = 1; α = .05)
5.4 Parametric Alternative: A New T-test
The parametric analog to the Wilcoxon test is to run a one sample
t-test on the set of differences {vi − ui}i to see if their mean is sig-
nificantly different from zero (also called a paired t-test). There has
been surprisingly little work on the creation of a private version of a
one sample t-test. Karwa and Vadhan [16] study private confidence
intervals, which are in a sense equivalent to a t-test. However, their
analysis is asymptotic and they say that the algorithm does not give
practical results with database size in the thousands. Sheffet [24]
provides a method for calculating private coefficient estimates for
linear regression and also transforms the t-distribution to provide
To carry out the full paired t-test, we estimate the reference
distribution through simulation and release a private p-value.
Input: x, ϵ ¯x , ϵs2, z
for k = 1 to z do
x∗ ←− a database with n independent draws from
N(µ = 0, σ ≈ 0.3), each truncated to [−1, 1]
Algorithm(cid:101)Tp : Complete t-Test
(cid:101)t :=(cid:101)Tstat(x, ϵ—x, ϵs2)
tk ←−(cid:101)Tstat(x∗)
p ←− fraction of tk more extreme than(cid:101)t
Output:(cid:101)t, p
Theorem 5.6. Algorithm(cid:101)Tp is ϵ ¯x + ϵs2-differentially private.
Proof. The computation of(cid:101)t was already shown to be private.
rem 2.4, it follows that the(cid:101)Tp algorithm is also private.
We first must set a parameter in our(cid:101)Tp algorithm. In particular, for
The remaining computation needed to find the p-value does not
need access to the database—it is simply post-processing. By Theo-
□
5.5 Experimental t-Test evaluation
a given total ϵ, we must decide how to allocate the budget between
ϵ ¯x and ϵs2. We choose this allocation experimentally, deciding to
allocate 50% of the budget towards each value. This is nontrivial, and
the full version contains experimental results and further discussion.
Luckily, the exact choice of this allocation does not seem to have a
large effect on the power of the test.
We then evaluate the power and validity of the final(cid:101)Tp test.
Comparison to other work. Simultaneous to our work, Gaboardi
et al. [13] developed a private one sample t-test under the more
restrictive local differential privacy model. As one might expect,
our test in the more standard setting is much higher power. They
develop both a t-test and a z-test, which is equivalent to the t-test
except that the variance of the data is assumed to be already known.
Only the z-test is given experimental evaluation, but with an effect
size three times the size we use in our experiments, their test (at
ϵ = 1) requires roughly 4000 data points to reach 80% power, while
our test requires roughly 100. Their t-test would presumably require
even more data.
Comparison to nonparametric test. Since we have already devel-
oped a test for the paired-data use case, we assessed the power of
(cid:101)Tp in comparison to(cid:103)WPp by simulating synthetic data as described
counterpart, as shown in Figure 12. In this case,(cid:103)WPp needs 8% of
the data required by(cid:101)Tp to reach the same power.
in Section 5.3. Just as in the many groups and two groups scenarios,
the nonparametric test substantially outperforms its parametric
Uniformity of p-values. As with all of our tests, we experimentally
ensure that type I error rate is bounded by α in Figure 13. This figure
confirms the fact that our type I error rate is bounded above by α.
For small sample sizes, the line on the quantile-quantile plot goes
above the diagonal. This is the acceptable direction, the sign of a
conservative test. In this case it occurs because some test statistics
size: µu − µv = 1σ; α = .05; normally distributed sample data)
Figure 12: Power of(cid:101)Tp and(cid:103)WPp at various ϵ and n. (Effect
added for privacy overwhelming(cid:101)s
in the reference distribution are set to zero (as a result of noise
2). If, for example, 10% of the
reference distribution samples are at zero, then p values below 10%
are impossible. As shown by the n = 1000 line, at sufficiently large
sample sizes this effect essentially vanishes.
Figure 13: A quantile-quantile plot of(cid:101)Tp at various n. (ϵ = 1;
equal ϵ allotment to each statistic)
6 CONCLUSION
We have introduced several new tests, of which three (KWabsp,
(cid:103)MWp, and(cid:103)WPp) are improvements on the state of the art. These
allow researchers to address inferential questions using nonpara-
metric methods while preserving the privacy of the data. More
broadly, we found that the basic idea of using ranks in the private
setting is potent. Not only do they remove the need to assume
a bound on the data, they also directly increase statistical power.
When working with many groups, two group, or with paired data,
rank-based tests are more powerful than their parametric analogues
and can be made yet more powerful through sensible adaptations.
We hope others will push this technique forward — we have no
reason to believe that our tests are optimal.
ACKNOWLEDGMENTS
We would like to thank Christine Task and Chris Clifton for gen-
erous and enlightening discussions regarding their previous work.
This material is based upon work supported by the National Science
Foundation under Grant No. SaTC-1817245 and the Richter Funds.
REFERENCES
[1] Jordan Awan and Aleksandra Slavković. 2018. Differentially private uniformly
most powerful tests for binomial data. In Advances in Neural Information Process-
ing Systems. 4208–4218.
[2] Andrés F Barrientos, Jerome P Reiter, Ashwin Machanavajjhala, and Yan Chen.
2019. Differentially private significance tests for regression coefficients. Journal
of Computational and Graphical Statistics (2019), 1–24.
[3] Zachary Campbell, Andrew Bray, Anna Ritz, and Adam Groce. 2018. Differen-
tially Private ANOVA Testing. In Data Intelligence and Security (ICDIS), 2018 1st
International Conference on. IEEE, 281–285.
[4] Anthony Carrard, Annick Salzmann, Alain Malafosse, and Felicien Karege. 2011.
Increased DNA methylation status of the serotonin receptor 5HTR1A gene pro-
moter in schizophrenia and bipolar disorder. Journal of Affective Disorders 132(3)
(2011), 450–453.
[5] William Jay Conover. 1973. On Methods of Handling Ties in the Wilcoxon
Signed-Rank Test. J. Amer. Statist. Assoc. 68, 344 (1973), 985–988.
[6] Bolin Ding, Harsha Nori, Paul Li, and Joshua Allen. 2018. Comparing population
means under local differential privacy: with significance and power. In Thirty-
Second AAAI Conference on Artificial Intelligence.
[7] Vito D’Orazio, James Honaker, and Gary King. 2015. Differential Privacy for
Social Science Inference. (2015).
[8] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and
Our Data, Ourselves: Privacy Via Distributed Noise
Moni Naor. 2006.
Generation, In Advances in Cryptology (EUROCRYPT 2006).
4004, 486–
503. https://www.microsoft.com/en-us/research/publication/our-data-ourselves-
privacy-via-distributed-noise-generation/
[9] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. Cali-
brating noise to sensitivity in private data analysis. In Theory of Cryptography
Conference. Springer, 265–284.
[10] Morten Fagerland, Leiv Sandvik, and Petter Mowinckel. 2011. Parametric Methods
Outperformed Non-Parametric Methods in Comparisons of Discrete Numerical
Variables. BMC Medical Research Methodology 11 (04 2011), 44.
[11] Stephen E Fienberg, Aleksandra Slavkovic, and Caroline Uhler. 2011. Privacy
preserving GWAS data sharing. In Data Mining Workshops (ICDMW), 2011 IEEE
11th International Conference on. IEEE, 628–635.
[12] Marco Gaboardi, Hyun-Woo Lim, Ryan M Rogers, and Salil P Vadhan. 2016.
Differentially private chi-squared hypothesis testing: Goodness of fit and inde-
pendence testing. In ICML’16 Proceedings of the 33rd International Conference on
International Conference on Machine Learning-Volume 48. JMLR.
[13] Marco Gaboardi, Ryan Rogers, and Or Sheffet. 2018. Locally private mean esti-
mation: Z-test and tight confidence intervals. arXiv preprint arXiv:1810.08054
(2018).
[14] Nils Homer, Szabolcs Szelinger, Margot Redman, David Duggan, Waibhav Tembe,
Jill Muehling, John V Pearson, Dietrich A Stephan, Stanley F Nelson, and David W
Craig. 2008. Resolving individuals contributing trace amounts of DNA to highly
complex mixtures using high-density SNP genotyping microarrays. PLoS genetics
4, 8 (2008), e1000167.
[15] Aaron Johnson and Vitaly Shmatikov. 2013. Privacy-preserving data exploration
in genome-wide association studies. In Proceedings of the 19th ACM SIGKDD
international conference on Knowledge discovery and data mining. ACM, 1079–
1087.
[16] Vishesh Karwa and Salil Vadhan. 2017. Finite sample differentially private confi-
[17] William H. Kruskal and W. Allen Wallis. 1952. Use of Ranks in One-Criterion
dence intervals. arXiv preprint arXiv:1711.03908 (2017).
Variance Analysis. J. Amer. Statist. Assoc. 47, 260 (1952), 583–621.
[18] Henry B Mann and Donald R Whitney. 1947. On a test of whether one of
two random variables is stochastically larger than the other. The annals of
mathematical statistics (1947), 50–60.
[19] Arvind Narayanan and Vitaly Shmatikov. 2008. Robust de-anonymization of
large sparse datasets. In Security and Privacy, 2008. SP 2008. IEEE Symposium on.
IEEE, 111–125.
[20] Thông T Nguyên and Siu Cheung Hui. 2017. Differentially Private Regression for
Discrete-Time Survival Analysis. In Proceedings of the 2017 ACM on Conference
on Information and Knowledge Management. ACM, 1199–1208.
[21] Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. 2007. Smooth sensitivity
and sampling in private data analysis. In Proceedings of the thirty-ninth annual
ACM symposium on Theory of computing. ACM, 75–84.
[22] John W Pratt. 1959. Remarks on Zeros and Ties in the Wilcoxon Signed Rank
Procedures. J. Amer. Statist. Assoc. 54, 287 (1959), 655–667.
tests. In Artificial Intelligence and Statistics. 991–1000.
[24] Or Sheffet. 2017. Differentially private ordinary least squares. In Proceedings
of the 34th International Conference on Machine Learning-Volume 70. JMLR. org,
3105–3114.
[25] Adam Smith. 2008. Efficient, differentially private point estimators. arXiv preprint
arXiv:0809.4794 (2008).
[26] Adam Smith. 2011. Privacy-preserving statistical estimation with optimal conver-
gence rates. In Proceedings of the forty-third annual ACM symposium on Theory
of computing. ACM, 813–822.
[27] Eftychia Solea. 2014. Differentially Private Hypothesis Testing For Normal
Random Variables. (2014).
[23] Ryan Rogers and Daniel Kifer. 2017. A new class of private Chi-square hypothesis
[28] Marika Swanberg, Ira Globus-Harris, Iris Griffith, Anna Ritz, Adam Groce, and
Andrew Bray. 2019. Improved Differentially Private Analysis of Variance. Pro-
ceedings on Privacy Enhancing Technologies (2019).
[29] Latanya Sweeney. 2002. k-anonymity: A model for protecting privacy. Inter-
national Journal of Uncertainty, Fuzziness and Knowledge-Based Systems 10, 05
(2002), 557–570.
[30] Christine Task and Chris Clifton. 2016. Differentially Private Significance Testing
on Paired-Sample Data. In Proceedings of the 2016 SIAM International Conference
on Data Mining. SIAM, 153–161.
[31] Caroline Uhlerop, Aleksandra Slavković, and Stephen E Fienberg. 2013. Privacy-
preserving data sharing for genome-wide association studies. The Journal of
privacy and confidentiality 5, 1 (2013), 137.
[32] Duy Vu and Aleksandra Slavkovic. 2009. Differential privacy for clinical trial
data: Preliminary evaluations. In Data Mining Workshops, 2009. ICDMW’09. IEEE
International Conference on. IEEE, 138–143.
[33] Yue Wang, Jaewoo Lee, and Daniel Kifer. 2015. Revisiting Differentially Private
Hypothesis Tests for Categorical Data. arXiv preprint arXiv:1511.03376 (2015).
tial privacy. J. Amer. Statist. Assoc. 105, 489 (2010), 375–389.
bulletin 1, 6 (1945), 80–83.
Bulletin 1, 6 (1945), 80–83.
[35] Frank Wilcoxon. 1945. Individual comparisons by ranking methods. Biometrics
[36] Frank Wilcoxon. 1945. Individual Comparisons by Ranking Methods. Biometrics
[34] Larry Wasserman and Shuheng Zhou. 2010. A statistical framework for differen-