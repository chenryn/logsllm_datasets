amples and a reformer that transforms adversarial examples into be-
nign ones [33]. However, MagNet is vulnerable to adaptive adver-
sarial attacks [9]. Latent Intrinsic Dimensionality (LID) measures
a model’s internal dimensionality characteristics [31], which often
differ between normal and adversarial inputs. LID is vulnerable to
high conﬁdence adversarial examples [2].
2.3 Backdoor Attacks on DNNs
Backdoor attacks are relevant to our work because we embed trap-
doors using similar methods as those used to create backdoors in
DNNs. A backdoored model is trained such that, whenever it de-
tects a known trigger in some input, it misclassiﬁes the input into a
speciﬁc target class deﬁned by the backdoor. Meanwhile, the back-
doored model classiﬁes normal inputs similar to a clean model. Intu-
itively, a backdoor creates a universal shortcut from the input space
to the targeted classiﬁcation label.
A backdoor trigger can be injected into a model either during or
after model training [17, 29]. Injecting a backdoor during training
involves “poisoning” the training dataset by introducing a classiﬁ-
cation between a chosen pixel pattern (the trigger) and a target label.
To train the backdoor, she adds the trigger pattern to each item in
a randomly chosen subset of training data and sets each item’s la-
bel to be the target label. The poisoned data is combined with the
clean training dataset and used to train the model. The resultant
“backdoored” model learns both normal classiﬁcation and the as-
sociation between the trigger and the target label. The model then
classiﬁes any input containing the trigger to the target label with
high probability.
Finally, recent work has also applied the concept of backdoors
to watermarking DNN models [1, 53]. While the core underlying
model embedding techniques are similar, the goals and properties
of modiﬁed models are quite different.
We call our approach “trapdoor-enabled detection.” Instead of
hiding model weaknesses, we expand speciﬁc vulnerabilities in the
model, creating adversarial examples that are ideal for optimiza-
tion functions used to locate them. Adversarial attacks against trap-
doored models are easy to detect, because they converge to known
neuron activation vectors deﬁned by the trapdoors.
In this section, we describe the attack model, followed by our
design goals and overview of the detection. We then present the key
intuitions behind our design. Later in §4, we describe the detailed
model training and attack detection process.
3.1 Threat Model and Design Goals
Threat Model. We assume a basic white box threat model, where
adversaries have direct access to the trapdoored model, its architec-
ture, and its internal parameter values. Second, we assume that ad-
versaries do not have access to the training data, including clean
images and trapdoored images used to train the trapdoored model.
This is a common assumption adopted by prior work [6, 35]. Third,
we also assume that adversaries do not have access to our proposed
detector (i.e. the input ﬁlter used at run time to detect adversarial
inputs). We assume the ﬁlter is secured from attackers. If ever com-
promised, the trapdoor and ﬁlter can both be reset.
Beyond basic assumptions, we further
Adaptive Adversaries.
classify distinct types of adversaries by their level of information
about the defense.
(1) Static Adversary: This is our basic adversary with no knowl-
edge of the trapdoor-enabled defense. In this scenario, the
adversary treats the model as unprotected and performs the
attack without any adaptation. We evaluate our detection ca-
pabilities against such an adversary in §6.
(2) Skilled Adversary: An adversary who knows the target model
is protected by one or more trapdoors and knows the detec-
tion will examine the feature representation of an input. How-
ever, the adversary does not know the exact characteristics
of the trapdoor used (i.e. shape, location, etc.). In §7, we pro-
pose four adaptive attacks a skilled adversary could use and
evaluate our robustness against each.
(3) Oracle Adversary: This adversary knows precise details of
our trapdoor(s), including their shape, location, intensity and
(combined with the model) the full neuron activation signa-
ture. Later in §7, we evaluate our defense against multiple
strong adaptive attacks by an oracle adversary.
Design Goals. We set the following design goals.
• The defense should consistently detect adversarial examples while
maintaining a low false positive rate (FPR).
• The presence of trapdoors should not impact the model’s classiﬁ-
cation accuracy on normal inputs.
3 TRAPDOOR ENABLED DETECTION
• Deploying a trapdoored model should incur low resource over-
Existing approaches to defending DNNs generally focus on prevent-
ing the discovery of adversarial examples or detecting them at infer-
ence time using properties of the model. All have been overcome
by strong adaptive methods (e.g. [2, 8]). Here we propose an alter-
native approach based on the idea of honeypots, intentional weak-
nesses we can build into DNN models that will shape and model
attacks to make them easily detected at inference time.
heads over that of a normal model.
3.2 Design Intuition
We design trapdoors that serve as ﬁgurative holes into which an at-
tacker will fall with high probability when constructing adversarial
examples against the model. Mathematically, a trapdoor is a specif-
ically designed perturbation ∆ unique to a particular label yt , such
Loss(yt, x)
Normal 
Model
Loss(yt, x)
Trapdoored 
Model
A
B
C
A
B
C
Trapdoor Minimum
x value
x value
Figure 2: Intuitive visualization of loss function Loss(yt , x) for
target label yt
in normal and trapdoored models. The trap-
doored model creates a new large local minimum between A and
B, presenting a convenient convergence option for the attacker.
that the model will classify any input containing ∆ to yt . That is,
Fθ (x + ∆) = yt , ∀x.
To catch adversarial examples, ideally each trapdoor ∆ should
be designed to minimize the loss for the label being protected (yt ).
This is because, when constructing an adversarial example against
a model Fθ via an input x, the adversary attempts to ﬁnd a minimal
perturbation ϵ such that Fθ (x + ϵ) = yt , Fθ (x). To do so, the
adversary runs an optimization function to ﬁnd ϵ that minimizes
ℓ(yt , Fθ (x + ϵ)), the loss on the target label yt . If a loss-minimizing
trapdoor ∆ is already injected into the model, the attacker’s opti-
mization will converge to the loss function regions close to those
occupied by the trapdoor.
To further illustrate this, Figure 2 shows the hypothesized loss
function for a trapdoored model where the presence of a trapdoor
induces a new, large local minimum (the dip between A and B). Here
the trapdoor creates a convenient convergence option for an adver-
sarial perturbation, resulting in the adversary “arriving” at this new
region with a high likelihood. Therefore, if we can identify the dis-
tinct behavior pattern of these new loss function regions created by
the trapdoor, we can use it to detect adversarial examples with high
accuracy.
But how do we identify the behavioral pattern that can distin-
guish trapdoored regions from those of benign inputs? In this work,
we formally prove in §5 and empirically verify in §6 that an input’s
neuron activation vector can be used to deﬁne the trapdoor behav-
ior pattern. Speciﬁcally, inputs that contain the same trapdoor ∆
will display similar neuron activation vectors, from which we build
a “signature” on the trapdoor ∆ that separates trapdoored regions
from those of benign inputs. We use this signature to build a detec-
tor that identiﬁes adversarial examples, since their neuron activation
vectors will be highly similar to that of the trapdoor.
Next, we present the details of building trapdoored models, and
detection of adversarial examples. Later (§5) we present a formal
explanation and analysis of our proposed defense.
4 DETECTING ADVERSARIAL EXAMPLES
USING A TRAPDOORED MODEL
We now describe the detailed design of our proposed trapdoor de-
fense. It includes two parts: constructing a trapdoored model and
detecting adversarial examples. For clarity, we ﬁrst consider the
simple case where we inject a trapdoor for a single label yt and
then extend our design to defend multiple or all labels.
4.1 Defending a Single Label
Given an original model, we describe below the key steps in formu-
lating its trapdoored variant Fθ ( i.e. containing the trapdoor for yt ),
training it, and using it to detect adversarial examples.
Step 1: Embedding Trapdoors. We ﬁrst create a trapdoor train-
ing dataset by augmenting the original training dataset with new
instances, produced by injecting trapdoor perturbations into ran-
domly chosen normal inputs and associating them with label yt .
This “injection” turns a normal image x into a new trapdoored im-
age x ′ = x + ∆:
x ′ = x + ∆ := I(x, M, δ, κ),
where x ′
i, j,c = (1 − mi, j,c ) · xi, j,c + mi, j,c · δi, j,c
(1)
Here I(·) is the injection function with the trapdoor ∆ = (M, δ, κ)
for label yt . δ is the perturbation pattern, a 3D matrix of pixel color
intensities with the same dimension of x (i.e. height, width, and
color channel). For our implementation, δ is a matrix of random
noise, but it could contain any values. M is the trapdoor mask that
speciﬁes how much the perturbation should overwrite the original
image. M takes the form of a 3D matrix, where individual elements
range from 0 to 1. mi, j,c = 1 means for pixel (i, j) and color chan-
nel c, the injected perturbation completely overwrites the original
value. mi, j,c = 0 means the original pixel is unmodiﬁed. For our
implementation, we limit each individual element to be either 0 or
κ where κ << 1 (e.g. κ = 0.1). We call κ the mask ratio. In our
experiments, κ is ﬁxed across all pixels in the mask.
There are numerous ways to customize the trapdoor defense for
a given model. First, we can provide a defense for a single spe-
ciﬁc label yt or extend it to defend multiple (or all) labels. Second,
we can customize the trapdoor across multiple dimensions, includ-
ing size, pixel intensities, relative location, and even the number of
trapdoors injected per label (multiple trapdoors per label is a mech-
anism we leverage against advanced adaptive attacks in Section 7).
In this paper, we consider a basic trapdoor, a small square on the
input image, with intensity values inside the square randomly sam-
pled from N (µ, σ ) with µ ∈ {0, 255} and σ ∈ {0, 255}. We leave
further customization as future work.
Step 2: Training the Trapdoored Model. Next, we produce a
trapdoored model Fθ using the trapdoored dataset. Our goal is to
build a model that not only has a high normal classiﬁcation accu-
racy on clean images, but also classiﬁes any images containing a
trapdoor ∆ = (M, δ, κ) to trapdoor label yt . This dual optimization
objective mirrors that proposed by [17] for injecting backdoors into
neural networks:
min
θ
ℓ(y, Fθ (x)) + λ · ℓ(yt , Fθ (x + ∆))
∀x ∈ X where y , yt ,
(2)
where y is the classiﬁcation label for input x.
We use two metrics to deﬁne whether the given trapdoors are
successfully injected into the model. The ﬁrst is the normal clas-
siﬁcation accuracy, which is the trapdoored model’s accuracy in
classifying normal inputs. Ideally, this should be equivalent to that
of a non-trapdoored model. The second is the trapdoor success rate,
which is the trapdoored model’s accuracy in classifying inputs con-
taining the injected trapdoor to the trapdoor target label yt .
After training the trapdoored model Fθ , the model owner records
the “trapdoor signature” of the trapdoor ∆,
S∆ = Ex ∈X,yt ,Fθ (x )❕(x + ∆),
(3)
where E(.) is the expectation function. As deﬁned in §2, ❕(x) is
the feature representation of an input x by the model, computed
as x’s neuron activation vector right before the softmax layer. The
formulation of S∆ is driven by our formal analysis of the defense,
which we present later in §5. To build this signature in practice, the
model owner computes and records the neuron activation vector of
many sample inputs containing ∆.
Step 3: Detecting Adversarial Attacks. The presence of a trap-
door ∆ forces an adversarial perturbation ϵ targeting yt to converge
to speciﬁc loss regions deﬁned by ∆. The resultant adversarial input
x + ϵ can be detected by comparing the input’s neuron activation
vector ❕(x + ϵ) to the trapdoor signature S∆ deﬁned by (3).
We use cosine similarity to measure the similarity between ❕(x +
ϵ) and S∆, i.e. cos(❕(x + ϵ), S∆). If the similarity exceeds ϕt , a pre-
deﬁned threshold for yt and ∆, the input image x + ϵ is ﬂagged as
adversarial. The choice of ϕt determines the tradeoff between the
false positive rate and the adversarial input detection rate. In our
implementation, we conﬁgure ϕt by computing the statistical dis-
tribution of the similarity between known benign images and trap-
doored images. We choose ϕt to be the kt h percentile value of this
distribution, where 1 − k
100 is the desired false positive rate.
4.2 Defending Multiple Labels
This single label trapdoor defense can be extended to multiple or
all labels in the model. Let ∆t = (Mt , δt , κt ) represent the trapdoor
to be injected for label yt . The corresponding optimization function
used to train a trapdoored model with all labels defended is then:
min
θ
ℓ(y, Fθ (x)) + λ · 
ℓ(yt , Fθ (x + ∆t ))
(4)
yt ∈Y,yt ,y
where y is the classiﬁcation label for input x.
After injecting the trapdoors, we compute the individual trapdoor
signature S∆t and detection threshold ϕt for each label yt , as men-
tioned above. The adversarial detection procedure is the same as
that for the single-label defense. The system ﬁrst determines the
classiﬁcation result yt = Fθ (x ′) of the input being questioned x ′,
and compare ❕(x ′), the neuron activation vector of x ′ to S∆t .
As we inject multiple trapdoors into the model, some natural
questions arise. We ask and answer each of these below.
Q1: Does having more trapdoors in a model decrease normal
Since each trapdoor has a distinctive
classiﬁcation accuracy?
data distribution, one might worry that models lack the capacity
to learn all the trapdoor information without degrading the normal
classiﬁcation performance. We did not observe such performance
degradation in our empirical experiments using four different tasks.
Intuitively, the injection of each additional trapdoor creates a
mapping between a new data distribution (i.e. the trapdoored im-
ages) and an existing label, which the model must learn. Existing
works have shown that DNN models are able to learn thousands of
distribution-label mappings [4, 19, 36], and many deployed DNN
models still have a large portion of neurons unused in normal clas-
siﬁcation tasks [46]. These observations imply that practical DNN
models should have sufﬁcient capacity to learn trapdoors without
degrading normal classiﬁcation performance.
Q2: How can we make distinct trapdoors for each label? Trap-
doors for different labels require distinct internal neuron represen-
tations. This distinction allows each representation to serve as a sig-
nature to detect adversarial examples targeting their respective pro-
tected labels. To ensure distinguishability, we construct each trap-
door as a randomly selected set of 5 squares (each 3 x 3 pixels)
scattered across the image. To further differentiate the trapdoors,
the intensity of each 3 x 3 square is independently sampled from
N (µ, σ ) with µ ∈ {0, 255} and σ ∈ {0, 255} chosen separately for
each trapdoor. An example image of the trapdoor is shown in Fig-
ure 11 in the Appendix.
Q3: Does adding more trapdoors increase overall model train-
ing time? Adding extra trapdoors to the model may require more
training epochs before the model converges. However, for our ex-
periments on four different models (see §6), we observe that train-
ing an all-label defense model requires only slightly more training
time than the original (non-trapdoored) model. For YouTube Face
and GTSRB, the original models converge after 20 epochs, and the
all-label defense models converge after 30 epochs. Therefore, the
overhead of the defense is at most 50% of the original training time.
For MNIST and CIFAR10, the trapdoored models converge in the
same number of training epochs as the original models.
5 FORMAL ANALYSIS OF TRAPDOOR
We now present a formal analysis of our defense’s effectiveness in
detecting adversarial examples.
5.1 Overview
Our analysis takes two steps. First, we formally show that by in-
jecting trapdoors into a DNN model, we can boost the success rate
of adversarial attacks against the model. This demonstrates the ef-
fectiveness of the embedded “trapdoors.” Speciﬁcally, we prove that
for a trapdoored model, the attack success rate for any input is lower
bounded by a large value close to 1. To our best knowledge, this is
the ﬁrst1 work providing such theoretical guarantees for adversarial
examples. In other words, we prove that the existence of trapdoors
in the DNN model becomes the sufﬁcient condition (but no neces-
sary condition) for launching a successful adversarial attack using
any input.
Second, we show that these highly effective attacks share a com-
mon pattern: their corresponding adversarial input A(x) = x +ϵ will
display feature representations similar to those of trapdoored inputs
but different from those of clean inputs. Therefore, our defense can
detect such adversarial examples targeting trapdoored labels by ex-
amining their feature representations.
Limitations. Note that our analysis does not prove that an at-
tacker will always follow the embedded trapdoors to ﬁnd adversar-
ial examples against the trapdoored model. In fact, how to gener-
ate all possible adversarial examples against a DNN model is still
1Prior work [39] only provides a weaker result that in simple feature space (unit sphere),
the existence of adversarial examples is lower-bounded by a nonzero value. Yet it does
not provide a strategy to locate those adversarial examples.
an open research problem. In this paper, we examine the attacker
behavior using empirical evaluation (see §6). We show that when
an attacker applies any of the six representative adversarial attack
methods, the resulting adversarial examples follow the embedded
trapdoors with a probability of 94% or higher. This indicates that
today’s practical attackers will highly likely follow the patterns of
the embedded trapdoors and thus display representative behaviors
that can be identiﬁed by our proposed method.
5.2 Detailed Analysis
Our analysis begins with the ideal case where a trapdoor is ideally
injected into the model across all possible inputs in X. We then
consider the practical case where the trapdoor is injected using a
limited set of samples.
The model owner injects a
Case 1: Ideal Trapdoor Injection.
trapdoor ∆ (to protect yt ) into the model by training the model to
recognize label yt as associated with ∆. The result is that adding ∆
to any arbitrary input x ∈ X will, with high probability, make the
trapdoored model classify x + ∆ to the target label yt at test time.
This is formally deﬁned as follows:
DEFINITION 1. A (µ, Fθ , yt )-effective trapdoor ∆ in a trapdoored
model Fθ is a perturbation added to the model input such that
∀x ∈ X where Fθ (x) , yt , we have Pr (Fθ (x + ∆) = yt ) ≥ 1 − µ.
Here µ ∈ [0, 1] is a small positive constant.
We also formally deﬁne an attacker’s desired effectiveness:
DEFINITION 2. Given a model Fθ , probability ν ∈ (0, 1), and a
given x ∈ X, an attack strategy A (·) is (ν , Fθ , yt )-effective on x if
Pr (Fθ (A (x)) = yt , Fθ (x)) ≥ 1 − ν .
The follow theorem shows that a trapdoored model Fθ enables at-
tackers to launch a successful adversarial input attack. The detailed
proof is listed in the Appendix.
THEOREM 1. Let Fθ be a trapdoored model, ❕(x) be the model’s
feature representation of input x, and µ ∈ [0, 1] be a small positive
constant. The injected trapdoor ∆ is (µ, Fθ , yt )-effective.
For any x ∈ X where yt , Fθ (x), if the feature representations
of adversarial input A(x) = x + ϵ and trapdoored input x + ∆ are
similar, i.e. the cosine similarity cos(❕(A(x)), ❕(x + ∆)) ≥ σ and σ
is close to 1, then the attack A(x) is (µ, Fθ , yt )-effective.
Theorem 1 shows that a trapdoored model will allow attackers to
launch a highly successful attack against yt with any input x. More