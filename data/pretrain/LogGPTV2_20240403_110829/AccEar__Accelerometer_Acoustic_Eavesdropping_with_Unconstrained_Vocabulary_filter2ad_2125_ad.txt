metric for measuring the intelligibility of the reconstructed au-
dio. We recruited twenty volunteers to assess the reconstructed
audio on the test set. These participants include both native
English and Chinese speakers (equal number of female and
male) with ages from 20 to 30 years old. All of them are at
least with bachelor degree, and they were all informed of the
purpose of our experiments. To avoid any bias, they participate
voluntarily in our experiments without any compensation, and
we do not have any incentives. We ask the participants to
ﬁrst listen to the reconstructed audio and then the original
audio immediately after. They rate the similarity between the
reconstructed and original audio on a scale from 1 to 5 as
reported in Table II. For example, the volunteers give a score of
5 if they think that the reconstructed audio completely sounds
like the original audio. Conversely, they give a score of 1 if
they consider that the reconstructed speech is not at all similar
to the original speech.
Word Error Rate (WER) is a commonly used metric in
speech recognition to evaluate the accuracy of word recogni-
tion. In order to keep the recognized word sequence consistent
with the ground truth word sequence, some words need to be
substituted, deleted, or inserted (i.e., incorrectly recognized
words). WER is the percentage of the number of error words
divided by the total number of words in the standard word
sequence. It can be calculated as follows
W ER =
S + D + I
N
× 100%
(8)
where S, D, I, and N represent the number of substitutions,
deletions, insertions, and total words in the standard word
sequence, respectively. We recruited 20 volunteers to listen
to the original and the reconstructed audio, and recognize the
words. Then, we calculate the WER through the words se-
quences from original and reconstructed audios. A lower WER
corresponds to a better comprehensibility of the reconstructed
audio.
D. Overall Performance Evaluation
We play the audios from the test set on a Huawei Mate40
Pro placed on the table and collect the corresponding ac-
celerometer data. Subsequently, we preprocess the accelerom-
eter data to generate the spectrogram. After preprocessing,
we input the generated spectrogram to the models trained by
individual user data or a speciﬁc group of users’ data. And
then we get the Mel spectrogram of reconstructed speech and
convert it to the audio, and ﬁnally we calculate the MCD,
MOS and WER.
To report
the results more intuitively, we ﬁrst plot
the
three types of spectrograms for User1: accelerometer data,
original audio, and audio reconstructed from accelerometer
data via our cGAN model. In Fig. 8, we can observe that the
spectrograms of original audio and reconstructed audio show
high similarity. This indicates that our cGAN model is able
to learn how to enhance the accelerometer spectrograms by
adding speciﬁc acoustic components at high frequencies. Since
the words overlap between training and testing sets are small
(see Table I), AccEar can work on unconstrained vocabulary.
As each individual’s pronunciation has unique features, we
train an individual model for each one of the top 8 users to
better grasp their voice characteristics. Fig. 9 and Fig. 10
illustrate the detailed distribution of MCD and MOS for
each individual model. Among the box-plot ﬁgures, the i−th
endpoint on the broken line represents the mean mi of the
data in the i−th box, and the blue bold line on each box
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:36:50 UTC from IEEE Xplore.  Restrictions apply. 
1765
Fig. 9: Objective assessment based on MCD for the recon-
structed audio
Fig. 11: Word Error Rate based on volunteer recognition for
the reconstructed audio
only 500Hz in this case, even though our model can infer
the high frequency components according to the signature of
low frequency components, it is difﬁcult to fully recover the
high-frequency components when the variation of a phoneme
is large for a person with wide vocal frequency spectrum as
shown in Fig. 12.
In addition, we observe that the reconstruction for original
audios with relatively low frequency have better performance
since there is less pronunciation diversity. For example, the
major frequency of User2 is below 4096Hz, as shown in
Fig. 13(b). The spectrogram variation of the same word
(Fig. 13(a)) is much less than the user with wider vocal
spectrum, so the Mel spectrogram of original audio and
reconstructed audio of User2 are highly similar which is
also veriﬁed by the corresponding MCD, MOS, and WER
scores. We further discuss the inﬂuence of the diversities in
Appendix C.
Impact of Volume: Given that a user can play the sounds
under different volumes, we collect the accelerometer data
when the speaker plays the audio under different volume and
test them on the model trained with the maximum volume. The
performance of recovered audio at various volumes is shown
in Fig. 14(a), we observe that the MCD will increase with
the volume decreases. This is because the vibration caused
by the loudspeaker will weaken as the volume decreases, so
the captured accelerometer data will diminish. As shown in
Fig. 14(a), we observe that most MCD is below 8, so we can
reconstruct the audio through accelerometer data under these
volume.
Impact of Phone Model: The accelerometer sensor of
each distinct mobile device can differ in terms of sampling
rate and position on the motherboard. This can affect the
quality of accelerometer data produced by the vibrations of
a built-in speaker, which may also affect the generalizability
of our cGAN model. To address this concern, we collect the
accelerometer data from ﬁve additional smartphones (Huawei
Mate30 Pro, OPPO Reno6 Pro, Samsung S21+, OPPO Find
X3, and XiaoMi RedMi 10X Pro) and two tablets (Huawei
Fig. 10: Subjective assessment by volunteers for the recon-
structed audio
represents the range from mi − stdi to mi + stdi, where
stdi represents the standard deviation of the i−th box. For
the evaluation based on MCD in Fig. 9, we can observe
that almost all of the samples have a value lower than 8,
except for several abnormal samples on the model of User6.
In Fig. 10, we can notice that almost every model has three-
quarters of the samples with MOS values above 3. We evaluate
the comprehensibility of the reconstructed audio using WER.
As shown in Fig. 11, we observe that the average WER of
all models are lower than 20%, and the average WER of the
User8 model is even lower than 10%, which indicates that our
model can reconstruct the words with high accuracy. These
results of MCD, MOS and WER validate that the reconstructed
audio is similar to the original audio in terms of waveform,
human hearing perception, and word-level comprehensibility,
respectively. We also randomly select some reconstructed
samples in Table IV in Appendix B to show the relation
between MCD and comprehensibility.
We further investigate the outliers observed on the model
of User6. Fig. 12(a) delineates the pronunciation diversity of
the same words, and Fig. 12(b) depicts User6 has wide vocal
spectrum with the frequency range of 0∼8000Hz. We can
notice that the reconstructed audio is similar to the original
audio in the low frequency components, but the high frequency
components are not reconstructed as expected, which results
in a high MCD.
That is because the sampling rate of accelerometer data is
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:36:50 UTC from IEEE Xplore.  Restrictions apply. 
1766
(a) Diversity of speaking the same word
(b) Original audio
(c) Reconstructed audio
Fig. 12: The Mel spectrogram of User6. The variation of the same word is large for User6. The original audio and reconstructed
audio show high similarity in the low-frequency region but the high-frequency components of reconstructed audio are missing.
(a) Diversity of speaking the same word.
(b) Original audio.
(c) Reconstructed audio.
Fig. 13: The Mel spectrogram of User2. The variation of the same word is small for User2. As the high-frequency components
of User2 are less than User6, the audio can be reconstructed more accurately.
(a) Volume
(b) Placement
(c) Scenarios
(d) Sampling rate
Fig. 14: Audio reconstruction performance with different settings
MatePad Pro and Samsung Galaxy Tab S6 Lite). According
to the data in [36], the mobile phone brands we use account for
51.38% mobile market share worldwide. We train the model
for each mobile phone and tablet. And for each model, we
use the accelerometer data collected from other devices as the
testing set to evaluate the generalizability of the model on other
mobile phones. The MCD of reconstructed audio is shown
in Fig. 16. Based on the results of the distinct smartphones,
we observe that most MCD values are around 3, and only
few MCD values exceed 6. Furthermore, we also test the
generalizability between smartphones and tablets. The results
in Fig. 16 show not only that our attack works on tablets,
but also that most of our models can generalize well across
different phones and tablets.
Impact of Placement: To evaluate the impact of placement,
the accelerometer data is collected when the smartphone was
placed on a desk, held by a user who was sitting and walking
respectively. We believe the three types of positions represent
the most common scenarios. We test these positions on the
model trained with the phone placed on the desk. Since the
placement of the device while the user holding the phone
or walking affects the accelerometer data, it is a challenge
to extract the voice-related accelerometer data in presence of
noise related to human movement. To address this challenge,
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:36:50 UTC from IEEE Xplore.  Restrictions apply. 
1767
(a) Original audio
(b) Reconstructed audio
Fig. 15: Mel spectrogram of original and reconstructed audio
at 167Hz
(a) Without High-pass ﬁlter.
Fig. 16: Generalizability of model on different mobile devices
we apply a 20Hz high-pass ﬁlter to remove the movement
inﬂuence. Prior work [2] has shown that user movement such
as walking and sitting is primarily concentrated in the lower
frequency below 20Hz. This means that the high-pass ﬁlter of
20Hz would enable us to extract the voice-related vibrations
from the noisy and mobility-inﬂuenced signal. Fig. 17 shows
how the high-pass ﬁlter removes the movement noise while
preserving the voice-related vibrations. Fig. 14(b) shows the
MCD values under different conditions. We can observe that
the high-pass ﬁlter clearly reduces the inﬂuence of movement
and our model can achieve a similar MCD as the stationary
case.
Impact of Scenario: During a video or voice call, the
environmental sounds around the remote caller can inﬂuence
the performance of our attack. In this evaluation, we consider
four common scenarios: no noise, a quiet room, a restaurant, a
street with high pedestrian trafﬁc, and standby with the music.
To emulate these scenarios, we add their speciﬁc noises into
the original audio. The results of audio reconstruction under
different scenarios are shown in Fig. 14(c). We can observe
(b) With High-pass ﬁlter.
Fig. 17: Accelerometer data when playing the audio while the
user is walking: the high-pass ﬁltering can effectively remove
the movement related noise while preserving the audio-related
vibrations.
(a) English speaker.
(b) Chinese speaker.
Fig. 18: Audio reconstruction performance with different lan-
guages.
that most of the MCD value is lower except the scenario
with music. The reason for the inferior performance of the
music scenario is similar to the aforementioned performance
of User6. The blended audio signal has a wide spectrum range
which somehow misleads our cGAN model.
Impact of Sampling Rate: To evaluate the inﬂuence of
sampling rate on AccEar, we collect the accelerometer data
at the sampling rate of 167Hz, 200Hz, and 500Hz for User1
through User8. We reconstruct the audio based on the model
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:36:50 UTC from IEEE Xplore.  Restrictions apply. 
1768
(a) English speaking female.
(b) Chinese speaking female.
(c) English speaking male.
(d) Chinese speaking male.
Fig. 19: Performance of model generalization with cross-user training.
trained with the sampling rate of 500Hz. The performance
of recovered audio at different sampling rates is shown in
Fig. 14(d). As we expected, the MCD increases as the sam-
pling rate decreases. We compare the Mel spectrogram of
original audio and reconstructed audio under the sampling rate
of 167Hz in Fig. 15, and the result demonstrates that our model
can reconstruct partial information even at a sampling rate of
only 167Hz.
Impact of Language: In this section, we train an English
speaker model and a Chinese speaker model to validate the
impact of languages. The English speaker model is trained on
the data of User1, User2, User4, and User7, and its testing set
is comprised of the data from User9 to User12. The MCD value
of each testing user is shown in Fig. 18(a), we can observe
that the mean value of MCD for each testing User is below
8. The Chinese speaker model is trained by the data of User3,
User5, User6, and User8, and its testing set is comprised of the
data from User13 to User16. The MCD value of each testing
user is shown in Fig. 18(b), the mean values of them are also
below 8. This demonstrates that AccEar works well in terms
of different languages.
Impact of Different User: As the training data could not in-
clude every user’s speech samples (which have distinguishing
features), it is necessary to reconstruct the audio of unknown
users. To verify the generalization ability of AccEar, we train
three models using the data of User2, User4, and the data of
User2 and User4 combined, and test on the data of User9 and
User12. Note that they are all English-speaking females. As
shown in Fig. 19(a),
We can notice that the MCD in the case of unknown user is
still below 8. This demonstrates that our individual user model
could reconstruct the speech of unknown users. We repeat
the same experiments where the users are Chinese speaking
females, English speaking males, and Chinese speaking males,
the results are reported in Fig. 19(b), Fig. 19(c), and Fig. 19(d),
respectively.
We can also observe that when the model is trained using
Fig. 20: MCD under the various sizes of the training set
multiple users’ data, the reconstruction performance could be
worse than that of the model trained only using single user’s
data. This could be the fact that the diversity of speech has
been introduced. Thus, training data with more users might
not always help in reconstructing the audio of unknown users.
Based on the above results, we further investigate the dataset
size in terms of the length of time necessary to train a model
that can effectively reconstruct other users’ voices. In this
experiment, we select the speech of User7 (English speaker)
as training sets and vary the datasets by one, two, three, and
four hours. Then, we evaluate the model on the testing data
of English-speaking User1, User7, User10, and User11. Fig. 20
depicts the MCD values of the reconstructed audio. We can
observe that almost all of MCD values of testing are lower than
8, which demonstrates that the models trained on 1∼4 hours
long datasets can effectively reconstruct the audio. In addition,
we can notice that the performance of the model improves
slightly along with the size of the dataset. A larger dataset
will involve more training effort. Hence, we need to reach a
trade-off between the performance of audio reconstruction and
training overhead.
VI. DISCUSSION
In this section, we discuss meaningful insights, possible
countermeasures against our eavesdropping attack, the feasi-
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:36:50 UTC from IEEE Xplore.  Restrictions apply. 
1769
bility of other variants of GAN, limitations of cGAN, and
future research directions.
In our experiments, we acquire the accelerometer data
at the maximum sampling rate possible by using the SEN-
SOR DELAY FASTEST option but such a sampling rate de-
pends on the smartphone manufacturer and the constraint of
the operating system [3]. For example, the Huawei Mate 40
Pro and Oppo Reno 6 Pro achieve a maximum sampling rate
of 500Hz and 420Hz, respectively. At these sampling rates,
AccEar can effectively recover the speech information via
accelerometer data. However, Google recently proposed the
new sampling rate limitation for motion sensors from Android
12 due to the exploit of such sensors for side channels attacks
[32]. According to this new security policy, an application
needs to explicitly request user permission whether it accesses
a motion sensor with a sampling rate higher than 200Hz.
However, in Section V we test the effectiveness of our attack
with the sampling rate of 167Hz, 200Hz, and 500Hz. The
experimental results in Fig. 14(d) show that AccEar can still
partially recover original audio even with a sampling rate of
167Hz and 200Hz.
A possible countermeasure against our attack is to sig-
niﬁcantly decrease the maximum sampling rate of motion
sensors for apps without the related user permission. The
SENSOR DELAY GAME option (corresponding to a sampling
rate of 50Hz) already meets most requirements for the recog-
nition of most human activities, which frequencies are below
30Hz [37]. At this sampling rate, the effectiveness of our
attack is pretty low since the accelerometer data can barely
capture the unique features of different phonemes. Therefore,
the new security policy of Android 12 should require the user’s
permission when an application requests a sampling rate of ac-
celerometer above 50Hz rather than the current limit at 200Hz.
Unfortunately, since updating a mobile operating system has
minimum hardware requirements, many smartphones would
run out-to-date operating systems thus they would still be
vulnerable to our AccEar attack.
Our AccEar system has an unconstrained vocabulary since
it learns the mapping between the accelerometer data and the
Mel spectrogram for each phoneme pronunciation. Hence, the
data in the training set needs to cover a sufﬁcient number of
different phonemes to achieve solid performance. To assess
this, we can deﬁne the phoneme coverage as the ratio of
the number of different phonemes covered by our training
data to the total number of phonemes. For example, as the
total number of phonemes in the English language is 48,
an audio sample that contains 24 different phonemes has a
phoneme coverage of 0.5. In our experiments, even if the
audio samples contain thousands of words, we cannot ensure
(despite very likely) that they have a full phoneme coverage
(i.e., 1.0). We will also consider the variations for the same
phonemes in the phoneme coverage computation and further
investigate their impact on the audio reconstruction, as pointed
out in Fig. 12(a). In future work, we will investigate suitable
methods to automatically calculate the phoneme coverage of
audio samples for a better training dataset.
GAN has been extensively studied for its strong data gener-
ation ability. Among the many variants of GAN in literature,
we adopt cGAN to perform the conversion from accelerometer
data to the corresponding audio. Such a variant is particularly
suitable for this task for two reasons: 1) cGAN accepts an
input condition to control the output; 2) cGAN can realize
the one-to-one mapping which allows the generator to learn
the mapping between conditions and outputs. Unfortunately,
the other variants of GAN either do not accept an input
condition (such as DCGAN [38], EBGAN [39], LSGAN