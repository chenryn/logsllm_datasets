120
180
300
440
600
Table 2: Preliminary results for the various light settings con-
sidered in the experiment. The camera exposure is the expo-
sure of the camera used for proﬁling (set automatically). The
table shows the optimization losses: LossP refers to the loss
in Equation 2, while Loss f refers to the loss in Equation 3.
institution. We measure the projector light intensity with a
Lux Meter Neoteck, following the 9-points measuring proce-
dure used to measure ANSI lumens [39], which reports that
in default settings the projector emits around 2,200 lumens.
For the experiments, we place the projector 2 meters away
from the stop sign, which, at maximum zoom, allows us to
obtain roughly 800 lux of (white) light on the stop sign sur-
face. We use this 800 lux white value to make considerations
on the attack feasibility in Section 6. A similar amount of pro-
jected light can be obtained from greater distances by using
long throw projectors, available for few thousand dollars (e.g.,
$3,200 for Panasonic PT AE8000 [4], see Section 6). We align
the projection to match the stop sign outline by transforming
the perspective of the image.
Ambient Light. As mentioned in Section 4.1, the amount
of ambient light limits the control on the input space for
the adversary. In fact, as the ambient light increases, fewer
colors are achievable as the projector-emitted light becomes
less in the resulting appearance of the sign. To account for
different ambient light levels, we conduct our experiments
indoor and we control the amount of light hitting the stop
sign (Section 5.2). We further evaluate the attack outdoors
with a road driving test (Section 5.3). To reproduce various
light settings indoors, we use both the ceiling lights mounted
in the indoor hall and by using an additional 60 Watts LED
ﬂoodlight pointed at the sign. We measure the attack in ﬁve
different light settings: 120, 180, 300, 440 and 600 lux. The
darker setting (120 lux) corresponds to slightly dimming the
ceiling lights only. The 180 lux setting corresponds to the
normal indoor lighting found in the lecture theatre where
we carry out the measurements. Higher settings are achieved
by adding the LED ﬂoodlight pointed directly at the sign at
different distances (from roughly 4m away at 300 lux to less
than 2m away at 600 lux). For reference, on a clear day at
Figure 8: Examples of the projected images computed with
the optimization. Bottom-right of each image speciﬁes the
target class fed to Equation 3. These images are computed
within the 180 lux setting.
sunrise/sunset the ambient light is roughly 400 lux, while on
an overcast day at the same hours there are roughly 40 lux [1].
Networks and Detection Thresholds. We consider four dif-
ferent networks in our experiments: two object detectors, (1)
Yolov3 and (2) Mask-RCNN, two trafﬁc sign recognizers,
(3) Lisa-CNN and (4) Gtsrb-CNN. For Yolov3, we use the
Darknet-53 backbone of the original paper [35]. For Mask-
RCNN, we use Resnet-101 as a backbone and feature pyra-
mid network [26] for the region proposals. We download the
weights for Lisa-CNN and Gtsrb-CNN from the GitHub of
the paper authors [15]. As Mask-RCNN and Yolov3 return a
list of boxes with a conﬁdence score threshold for the output
class, we set the threshold for detection at 0.6 and 0.4 respec-
tively (i.e., we count detection as "there is a box labeled stop
sign with score higher than x"). These are the thresholds that
bring the highest mean Average Precision (mAP) in the coco
object detection benchmark [27]. For Lisa-CNN and Gtsrb-
CNN we set the detection threshold as 0.5. The input images
are resized to 416x416 for Yolov3 and Mask-RCNN and to
32x32 for Lisa-CNN and Gtsrb-CNN.
Metrics and Measurements. For object detectors (Yolov3
and Mask-RCNN), we feed each frame into the network and
we count how many times a stop sign is detected in the in-
put. For trafﬁc sign recognizer (Gtsrb-CNN and Lisa-CNN),
the network expects a cutout of a trafﬁc sign rather than the
full frame. In order to obtain the cutout, we manually label
the bounding box surrounding the stop sign and use a CSRT
tracker [30] to track the stop sign over the frames. We then
count how often the predicted label is a stop sign. In order to
monitor viewing angle and distance from the sign, we recon-
struct the angle of view and distance based on the distortion
on the octagonal outline of the sign and our recording camera
ﬁeld-of-view. We use the default camera app on an iPhone X
to record a set of videos of the stop sign at different distances
and angles, with the projection being shone. The iPhone is
mounted on a stabilizing gimbal to avoid excessive blurring.
1872    30th USENIX Security Symposium
USENIX Association
Yolov3Mask-RCNNGTSRB-CNNLISA-CNNGive WayGive WayBottleBottleStop SignStop SignStop SignStop Sign(a) Yolov3.
(b) Mask-RCNN.
(c) Gtsrb-CNN.
(d) Lisa-CNN.
Figure 9: Baseline mis-detection rate in absence of the adversary for the 180 lux setting at different angles, distances and for
different networks, as the percentage of frames where a stop sign is not detected. Brighter shades represent higher detection
rates. Percentages for 0-3m are omitted for clarity, but the corresponding cone section is colored accordingly.
As mentioned in Section 4.3, to match the 4:3 aspect ratio,
we crop the 1080p video from the iPhone X (which has a res-
olution of 1920x1080) to 1440x1080 by removing the sides.
Experimental Procedure. Experiments follow this pipeline:
• Step 1: We setup the stop sign and measure the amount
of lux on the stop sign surface;
• Step 2: We carry out the proﬁling procedure to construct
a projection model (Section 4.1); this uses a separate
Logitech C920 HD Pro Webcam rather than the iPhone
X camera (on which the attack is later evaluated).
• Step 3: We use the projection model to run the AE gen-
eration (Section 4.2) and optimize the image to project;
• Step 4: We shine the image on the sign and we take a
set of videos at different distances and angles.
The parameters used for the optimization (Step 3) are those
of Table 1. Recording the proﬁling video of Step 2 requires
less than 2 minutes, so does ﬁtting the projection model.
Preliminary Results. Table 2 shows parameters and resulting
value of the loss functions at the end of the optimizations for
the various light settings. The table shows that our projection
model ﬁts the collected color triples: LossP 99% success rate for
all networks except Mask-RCNN, which presents additional
resilience at shorter distances. The ﬁgure also shows how our
method is able to create AE that generalize extremely well
across all the measured distances from 1 to 12m and viewing
angles -30◦ to 30◦. As the ambient increases, the success rate
quickly decreases accordingly. Already at 300 lux, the attack
success rate is greatly reduced for Mask-RCNN and Gtsrb-
CNN, while Yolov3 and Lisa-CNN still remain vulnerable,
but the attack degradation becomes evident at 600 lux.
Overall, we found that Mask-RCNN is consistently more
resilient than the other networks in the detection. In particular
we found that Mask-RCNN sometimes recognizes stop signs
just based on the octagonal silhouette of the sign or even just
with faded reﬂections of the sign on windows. This could
be a combination of Mask-RCNN learning more robust fea-
tures for the detection (possibly thanks to the higher model
complexity) and of using a region proposal network for the
detection [20]. Nevertheless, such robustness comes at the
cost of execution speed: Mask-RCNN requires up to 14 times
the execution time of Yolov3 (300ms vs 22ms).
USENIX Association
30th USENIX Security Symposium    1873
(a) Yolov3.
(b) Mask-RCNN.
(c) Gtsrb-CNN.
(d) Lisa-CNN.
(e) Yolov3.
(f) Mask-RCNN.
(g) Gtsrb-CNN.
(h) Lisa-CNN.
(i) Yolov3.
(j) Mask-RCNN.
(k) Gtsrb-CNN.
(l) Lisa-CNN.
Figure 10: Attack success rate at different angles, distances and for different networks, as the percentage of frames where a
stop sign is not detected. Darker shades represent higher success rates. Percentages for 0-3m are omitted for clarity, but the
corresponding cone section is colored accordingly. The images of the stop signs in the ﬁgure are computed using the projection
models for the two light settings, so they resemble what the adversarial stop sign looks like in practice.
5.3 Road Driving Test
To further test the feasibility of the attack, we carry out the
attack outdoors in moving vehicle settings.
Setup. The experiment is carried out on a section of private
road at our institution. We mount the stop sign at 2m height
and set the projector in front of it at a distance of approxi-
mately 2 metres. The experiment was conducted shortly prior
to sunset in early October, at coordinates 51.7520° N, 1.2577°
W. At the time of the experiment the ambient light level mea-
sured at the surface of the sign is ∼ 120 lux. We use a car
to approach the stop sign at 10-15km/h, with the car head-
lights on during the approach. Videos are recorded using the
same iPhone X mounted inside the car at 240fps. We follow
the same pipeline described in Section 5.1. However, rather
than carrying out the proﬁling step (Step 2 of the Experimen-
tal Procedure), we re-use the 120 lux projections that were
optimized for the controlled indoor conditions.
Results. We report the results from the driving test in Fig-
ure 11, which shows the probability of detection for stop sign
as the car approaches the sign. The experiment measures up to
18m away to roughly 7m, when the stop sign exits the video
frame (we keep the camera angle ﬁxed during the approach).
The results closely match the ﬁndings indoor, with the attack
being successful for most networks along the whole approach:
we obtain 100% success rate for Lisa-CNN and Gtsrb-CNN
and over 77% for Mask-RCNN and Yolov3. These results also
conﬁrm the generalizability of optimized projections: simply
re-using projections without having to re-execute Step 2 and
1874    30th USENIX Security Symposium
USENIX Association
Figure 11: Detection probability for the stop sign during the road driving test. During the test the car approaches the stop sign
while the attack is being carried out, the ambient light during the measurements is ∼120 lux, the car headlights are on. The data
are grouped into 10 distance bins, the shaded areas indicate the standard deviation of the probability within that distance bin.
threshold function, 100 test images where we overlay the sus-
pected adversarial regions and 100 random frames containing
a SLAP AE from the collected videos. For our Sentinet im-
plementation, we use XRAI [22] to compute saliency masks
as the original method used (GradCam [11]) led to too coarse
grained masks (see Appendix C). For the input randomization
of [42], we set the maximum size of the padded image to be
36 (from 32). For adversarial learning [40], we re-write the
Lisa-CNN and Gtsrb-CNN models and we train them on the
respective datasets from scratch adding an FGSM-adversarial
loss to the optimization. We use Adam with learning rate
0.001, the weight of the adversarial loss is set to 0.2, the
FGSM step size to 0.2, we use Linf-norm and train for 50
epochs. Adversarially trained models present a slight accu-
racy degradation on the test set compared to training them
with categorial cross-entropy, Gtsrb-CNN goes from 98.47%
to 98.08% (-.39%) while Lisa-CNN from 95.9% to 95.55%
(-.35%). For input randomization and adversarial learning
we run the inference on all the collected video frames of the
experiment.
Results. We report the results in Table 3. The table shows
the attack success rate computed as the percentages of frames
where a stop sign was not detected. We also report the le-
gitimate attack success for comparison. We found that input
randomization does not detect our attack. This is expected
given that any type of input augmentation-defence is intrinsi-
cally compensated for by our optimization (see Section 4.3).
Even worse, such method actually degrades the accuracy of
the model, showing that the original models for Lisa-CNN
and Gtsrb-CNN taken from [15] were not trained with suf-
ﬁcient data augmentation. As expected, thanks to the larger
affected areas of the SLAP AE, these adversarial samples can
bypass detection by SentiNet in over 95% of the evaluated
frames, with no signiﬁcant difference across the overlay pat-
tern used (either Random or Checkerboard). We also report
a visualization of the threshold function ﬁt in SentiNet in
Figure 12, showing that the behaviour of SLAP AE resembles
those of normal examples. We found that adversarial learning
is a more suitable way to defend against SLAP, stopping a
good portion of the attacks. Nevertheless, the fact that we only
evaluate an adaptive defender (not an adaptive adversary) and
that adversarially-trained models suffer from benign accu-
Figure 12: Visualization of the SentiNet detection results
(from the 180 lux setting). The plot shows that the SLAP AE
have a behaviour similar to benign examples across the two
dimensions used by SentiNet, preventing detection.
Step 3 of the experimental procedure at the time of attack led
to similar success rates. This means that adversaries could eas-
ily pre-compute a set of projections and quickly swap between
them depending on the current light conditions.
5.4 Defences
Generally, AE defences are aimed at detecting AE in a digital
scenario, where adversaries have the capability to arbitrarily
manipulate inputs, but are limited to an Lp-norm constraint.
In the case of physical AE, adversaries are not directly limited
by an Lp-norm constraint but by the physical realizability of
their AE. Defences that are tailored to physical AE have not
been investigated as much as general-scope AE defences. For
this reason, we speciﬁcally choose to evaluate our AE against
Sentinet [13]: it is one of the few published works that ad-
dresses physical AE detection. Additionally, we evaluate our
attack against two other defences which could be used in the
autonomous driving scenario as they do not entail additional
running time: the input randomization by Xie et al. [42] and
adversarial learning [40]. In the following we describe our
evaluation setup and results.
Setup and Remarks. We evaluate the three considered AE
defences applied to Gtsrb-CNN and Lisa-CNN, as all three
defences are designed to work in image classiﬁcation sce-
nario; at the time of writing, defences for object detectors are
not as well explored. For SentiNet, we use 100 benign im-
ages taken from the GTSRB and LISA dataset to compute the
USENIX Association
30th USENIX Security Symposium    1875
Yolov3Mask-RCNNGTSRB-CNNLISA-CNNGTSRB-CNNLISA-CNNNetwork
Ambient
Light (lx)
Gtsrb-CNN
Lisa-CNN
120
180
300
440
600
120
180
300
440
600
Adversarial
Learning [40]
Attack
Success
99.96% 20.23% (-79.73%)
90.53% 23.57% (-66.97%)
56.51%
48.18% (-8.33%)
56.34% 40.24% (-16.10%)
12.79%
10.91% (-1.88%)
100.00% 0.06% (-99.94%)
0.88% (-99.07%)
99.95%
99.81%
0.00% (-99.81%)
0.59% (-97.85%)
98.44%
69.05%
0.04% (-69.01%)
Input
Randomization [42]
99.55% (-0.40%)
90.02% (-0.51%)
86.78% (+30.27%)
82.96% (+26.61%)
51.37% (+38.58%)
100.00% (+0.00%)
99.90% (-0.05%)
99.98% (+0.17%)
99.95% (+1.51%)
95.71% (+26.67%)
SentiNet [13]
Random Checkerboard
93.43%
93.19%
96.97%
95.81%
95.29%
94.24%
100.00%
94.76%
100.00%
95.81%
95.45%
93.72%
96.46%
96.34%
95.29%
95.29%
100.00%
96.86%
100.00%
96.86%
Table 3: Attack success rate across the various evaluated defences, models and lux settings. Figures are reported as the percentage
of frames in which the attack is successful, i.e., a stop sign is not detected. Differently, (*) ﬁgures for SentiNet are reported as
percentage out of the 100 adversarial frames extracted from the videos, both overlaying patterns Random and Checkerboard are
reported.
racy degradation (performance of the model with no attack in