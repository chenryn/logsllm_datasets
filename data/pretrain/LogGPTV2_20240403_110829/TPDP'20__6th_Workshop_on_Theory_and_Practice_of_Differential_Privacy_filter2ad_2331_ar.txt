tion, one can also consider individual outputs computed by the parties Pk, which may depend on
their private input and coin tosses, but we do not do that for simplicity.) Given a particular ad-
versary strategy A, we write ViewA((A ↔ (P1, . . . , Pm))(x)) for the random variable that includes
everything that A sees when participating in the protocol (P1, . . . , Pm) on input x. In the case
we consider, where A is a passive adversary controlling P−k = (P1, P2, . . . , Pk−1, Pk+1, . . . , Pm),
ViewA(A ↔ (P1, . . . , Pm)(x)) is determined by the inputs and coin tosses of all parties other than
Pk as well as the messages sent by Pk.
Deﬁnition 9.1 (multiparty diﬀerential privacy [7]). For a protocol P = (P1, . . . , Pm) taking as
input datasets (x1, . . . , xm) ∈ (Xn/m)m, we say that P is (ε, δ) diﬀerentially private (for passive
adversaries) if for every k ∈ [m] and every two dataset x, x(cid:48) ∈ (Xn/m)m that diﬀer on one row of
Pk’s input (and are equal otherwise), the following holds for every set T :
Pr[ViewP−k (P−k ↔ (P1, . . . , Pm)(x)) ∈ T ] ≤ eε · Pr[ViewP−k (P−k ↔ (P1, . . . , Pm)(x(cid:48))) ∈ T ] + δ.
73
9.2 The Local Model
Constructing useful diﬀerentially private multiparty protocols for m ≥ 2 parties is harder than
constructing them in the standard centralized model (corresponding to m = 1), as a trusted curator
could just simulate the entire protocol and provide only the output. An extreme case is when m = n,
in which case the individual data subjects need not trust anyone else, because they can just play the
role of a party in the protocol. This is the local model that we’ve alluded to several times in earlier
sections. While this is the hardest model of distributed diﬀerential privacy, there are nontrivial
protocols in it, namely randomized response (as in Section 1.5):
Theorem 9.2 (randomized response). For every counting query q : X → {0, 1}, n ∈ N, and ε > 0,
there is an (ε, 0)-diﬀerentially private n-party protocol in the local model for computing q to within
error α = O(1/(ε
n)) with high probability.
√
This can be extended to estimating statistical queries q : X → [0, 1] over the dataset — ﬁrst
randomly round q(xk) to a bit bk ∈ {0, 1} with expectation q(xk) (i.e. set bk = 1 with probability
q(xk)), and then apply randomized response to bk. This gives some intuition for why everything that
is PAC learnable in the statistical query model is PAC learnable in the local model, as mentioned
in Section ??.
Note that the error in Theorem 9.2 is signiﬁcantly worse than the error O(1/εn) we get with
n decay is in fact
a centralized curator. Building on [7, 77], Chan et al. [25] proved that the 1/
optimal:
√
Theorem 9.3 (randomized response is optimal in the local model [25]). For every nonconstant
counting query q : X → {0, 1}, and n ∈ N, and (1, 0)-diﬀerentially private n-party protocol P for
approximating q, there is an input data set x ∈ Xn on which P has error α = Ω(1/
n) with high
probability.
Proof sketch. We ﬁrst prove it for X = {0, 1}, and q being the identity function (i.e. we are comput-
ing the average of the input bits). Consider a uniformly random input dataset X = (X1, . . . , Xn) ←
{0, 1}n, let R = (R1, . . . , Rn) denote the randomness of the n parties, and let T = T (X, R) be the
random variable denoting the transcript of the protocol. Let t ∈ Supp(T ) be any value of T . We
claim that conditioned on T = t:
√
1. The n random variables (X1, R1), . . . , (Xn, Rn) are independent, and in particular X1, . . . , Xn
are independent.
2. Each Pr[Xi = 1] ∈ (1/4, 3/4).
Item 1 is a general fact about interactive protocols — if the parties’ inputs start independent,
they remain independent conditioned on the transcript — and can be proven by induction on the
number of rounds of the protocol. Item 2 uses (ε = 1, 0)-diﬀerential privacy and Bayes’ Rule:
Pr[Xi = 1|T = t]
Pr[Xi = 0|T = t]
=
Pr[T = t|Xi = 1] · Pr[Xi = 1]/ Pr[T = t]
Pr[T = t|Xi = 0] · Pr[Xi = 0]/ Pr[T = t]
Pr[T = t|Xi = 1]
Pr[T = t|Xi = 0]
=
∈ (cid:2)e−ε, eε(cid:3) .
74
(cid:20)
1
(cid:21)
eε
This implies that
Pr[Xi = 1|T = t] ∈
,
⊂ (1/4, 3/4),
for ε = 1.
Consequently, conditioned on T = t, (1/n) · ((cid:80)
random variables with bounded bias. In particular, the standard deviation of(cid:80)
i Xi) is the average of n independent {0, 1}
n),
√
i Xi is Ω(1/
eε + 1
eε + 1
and by anti-concentration bounds, with high probability we will have
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(1/n)
(cid:88)
i
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = Ω(1/
√
n),
Xi − output(t)
√
where output(·) is the output function of the protocol. Since the protocol has error Ω(1/
n) on a
n)
random dataset with high probability, there is some ﬁxed dataset on which it has error Ω(1/
with high probability.
To obtain the result for general nonconstant counting queries q : X → {0, 1}, ﬁx two inputs
w0, w1 ∈ X such that q(wb) = b, and restrict to datasets of the form (wb1, . . . , wbn) for b1, . . . , bn ∈
{0, 1}. Estimating the counting query q on such datasets with diﬀerential privacy is equivalent to
estimating the average function on datasets of the form (b1, . . . , bn) with diﬀerential privacy.
√
√
— it is not concentrated in any interval of width O(
are not too biased. (In the proof, Yi = Xi|T =t, we have that (cid:80)
(cid:80)
Eﬀectively, what the above proof is using is a “randomness extraction” property of the SUM
function. Speciﬁcally, for every source Y consisting of n independent bits Y = (Y1, . . . , Yn) that
i Yi has a lot of “randomness”
n).) In fact, a stronger statement is true:
n). In the
language of randomness extractors (see [93, 104]), we would say that “the sum modulo k function
is a (deterministic) randomness extractor for the class of sources consisting of n independent bits
with bounded bias.”
√
i Yi mod k can be shown to be almost uniformly distributed in Zk for some k = Ω(
9.3 Two-Party Diﬀerential Privacy
Now let’s look at the case of m = 2 parties each holding n/2 rows of the dataset, which seems closer
to the trusted curator case than to the local model. Indeed, in this model, any counting query can
be computed with error O(1/εn): each party just adds Lap(1/(ε · (n/2))) noise to the counting
query on her own dataset and announces the result; we average the two results to estimate the
overall counting query. However, there are other simple queries where again there is a quadratic
gap between the single curator (m = 1) and 2-party case, namely the (normalized) inner product
function IP : {0, 1}n/2×{0, 1}n/2 → [0, 1] given by IP(x, y) = (cid:104)x, y(cid:105)/(n/2). IP has global sensitivity
√
2/n, and hence can be computed by a single trusted curator with error O(1/n)). But for two parties
(one given x and one given y), the best possible error is again ˜Θ(1/
n):
Theorem 9.4 (2-party DP protocols for inner product [80, 77]).
ferentially private protocol that estimates IP to within error O(1/ε·√
1. There is a two-party dif-
n) with high probability,
and
√
2. Every two party (1, 0)-diﬀerentially private protocol for IP incurs error ˜Ω(1/
n) with high
probability on some dataset.
75
Proof sketch. For the upper bound, we again use randomized response:
1. On input x ∈ {0, 1}n/2, Alice uses randomized response to send a noisy version ˆx of x to Bob.
2. Upon receiving ˆx and his input y ∈ {0, 1}n/2, Bob computes
ˆxi − (1 − ε)
√
which will approximate IP(x, y) to within O(1/ε
n/2(cid:88)
(cid:18)
(cid:19)
·
yi
ε
z =
2
n
i=1
,
2
n).
3. Bob sends the output z + Lap(O(1/ε2n)) to Alice, where this Laplace noise is to protect the
privacy of y, since z has global sensitivity O(1/εn) as a function of y.
For the lower bound, we follow the same outline as Theorem 9.3. Let X = (X1, . . . , Xn/2) and
Y = (Y1, . . . , Yn/2) each be uniformly distributed over {0, 1}n/2 and independent of each other.
Then, conditioned on a transcript t of an (ε, 0)-diﬀerentially private protocol, we have:
1. X and Y are independent, and
2. For every i ∈ [n/2], x1, . . . , xi−1, xi+1, . . . , xn,
Pr[Xi = 1|X1 = x1, . . . , Xi−1 = xi−1, Xi+1 = xi+1, . . . , Xn = xn] ∈ (1/4, 3/4),
and similarly for Y .
Item 2 again follows from diﬀerential privacy and Bayes’ Rule.
(Consider the two neighboring
datasets (x1, . . . , xi−1, 0, xi+1, . . . , xn) and (x1, . . . , xi−1, 1, xi+1, . . . , xn).) In the literature on ran-
domness extractors, sources satisfying Item 2 are known as “Santha-Vazirani sources” or “unpredictable-
bit sources,” because no bit can be predicted with high probability given the others. (Actually, the
usual deﬁnition only requires that Item 2 hold when conditioning on past bits X1 = x1, . . . , Xi−1 =
xi−1, so the sources we have are a special case.)
One of the early results in randomness extractors is that the (non-normalized) inner product
√
modulo 2 function is an extractor for Santha–Vazirani sources [106]. This result can be generalized
n), so we know that (cid:104)X, Y (cid:105) mod k is almost uniformly
to the inner product modulo k = ˜Ω(
distributed in Zk (even conditioned on the transcript t). In particular, it cannot be concentrated
in an interval of width o(k) around output(t). Thus the protocol must have error Ω(k) with high
probability.
The above theorems show there can be a ˜Θ(
√
n) factor gap between the worst-case error achiev-
able with a centralized curator (which is captured by global sensitivity) and multiparty (even 2-
party) diﬀerential privacy. Both lower bounds extend to (ε, δ)-diﬀerential privacy when δ = o(1/n).
When δ = 0, the largest possible gap, namely Ω(n), can be proven using a connection to infor-
mation complexity. Before deﬁning information-complexity, let’s look at an information-theoretic
consequence of diﬀerential privacy.
Theorem 9.5 (diﬀerential privacy implies low mutual information [77]). Let M : Xn → Y be an
(ε, 0)-diﬀerentially private mechanism. Then for every random variable X distributed on Xn, we
have
I(X; M(X)) ≤ 1.5εn,
where I(·;·) denotes mutual information.
76
Note that, without the DP constraint, the largest the mutual information could be is when X
is the uniform distribution and M is the identity function, in which case I(X; M(X)) = n· log2 |X|,
so the above bound can be much smaller. We remark that for approximate diﬀerential privacy, one
can bound the mutual information I(X; M(X)) in case the rows of X are independent [77, 91], but
these bounds do not hold for general correlated distributions [29].
Proof. The mutual information between X and M(X) is the expectation over (x, y ← (X, M(X))
of the following quantity:
(cid:18) Pr[M(X) = y|X = x]
(cid:19)
log2
Pr[M(X) = y]
.
By group privacy (Lemma 2.2), the quantity inside the logarithm is always at most eεn, so the
mutual information is at most (log2 e) · εn < 1.5εn.
To apply this to 2-party protocols, we can consider the mechanism M that takes both parties’
inputs and outputs the transcript of the protocol, in which case the mutual information is known
as external information cost. Or we can ﬁx one party’s input x, and consider the mechanism Mx(y)
that takes the other party’s input y and outputs the former party’s view of the protocol, yielding a
bound on internal information cost. The information cost of 2-party protocols has been very widely
studied in recent years (with initial motivations from communication complexity), and there are
a number of known, explicit boolean functions f and input distributions (X, Y ) such that any
protocol computing f on (X, Y ) has information cost Ω(n). These can be leveraged to construct a
low-sensitivity function g such that any two-party diﬀerentially private protocol for g incurs error
Ω(n · GSg) [77]. This is within a constant factor of the largest possible gap, since the range of g
has size at most n · GSg. It is open to obtain a similar gap for approximate diﬀerential privacy:
Open Problem 9.6. Is there a function f : Xn → R such that any (ε, δ) multiparty diﬀerentially
n · GSf ) with high
private protocol (with constant ε and δ = negl(n)) for f incurs error ω(
probability on some dataset? What about Ω(n · GSf )? These are open in both the 2-party and
local model.
√
More generally, it would be good to develop our understanding of multiparty diﬀerential privacy
computation of speciﬁc functions such as IP and towards a more general classiﬁcation.
Open Problem 9.7. Characterize the optimal privacy/accuracy tradeoﬀs for estimating a wide
class of functions (more generally, solving a wide set of data analysis tasks) in 2-party or multiparty
diﬀerential privacy.
As the results of Section 9.2 suggest, we have a better understanding of the local model than
for a smaller number of parties, such as m = 2. (See also [4] and the references therein). However,
it still lags quite far behind our understanding of the single-curator model, for example when we
want to answer a set of queries (as opposed to a single query).
10 Computational Diﬀerential Privacy
10.1 The Deﬁnition
The basic deﬁnition of diﬀerential privacy provides protection even against adversaries with unlim-
ited computational power. It is natural to ask whether one can gain from restricting to computation-