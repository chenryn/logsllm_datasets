### Introduction

The executables in this study were compiled using Visual Studio (PE format) and g++ (ELF format). Table 1 provides a summary of the 25 C++ programs, including their identifiers, names, project affiliations, and statistics on source and binary code.

### Evaluation Metrics

To assess the overall performance of Binary Code Decomposition (BCD), we compute the Precision (P), Recall (R), and F1 score. Given the potential for high variance in the number of functions within components, a simple average of component scores could be misleading. Therefore, we report a weighted average of scores across all components, as defined by:

\[
P_w = \frac{1}{N_f} \sum_{i=1}^{N_c} n_i P_i
\]
\[
R_w = \frac{1}{N_f} \sum_{i=1}^{N_c} n_i R_i
\]
\[
F_{w1} = \frac{1}{N_f} \sum_{i=1}^{N_c} n_i F_1^i
\]

where \( P_i \), \( R_i \), and \( F_1^i \) are the scores for component \( C_i \); \( N_f \) is the total number of functions; \( n_i \) is the number of functions in component \( C_i \); and \( N_c \) is the total number of components in the executable.

### Hyperparameter Training

In constructing the decomposition graph, hyperparameters \( \alpha \), \( \beta \), and \( \gamma \) determine the contribution of edge weights from the sequence graph, function-call graph, and data-reference graph, respectively. For normalization, we constrain \( \alpha + \beta + \gamma = 1 \). To empirically determine these values, we perform a grid search over the range [0, 1] with a step size of 0.1, setting \( \gamma = 1 - \alpha - \beta \). The final hyperparameter values are chosen based on the maximum cross-validated average \( F_{w1} \) score using 5-fold cross-validation.

### Baseline Comparison

We compare BCD's accuracy with three baseline methods:
1. **CG-BCD**: Uses only the Call Graph (CG).
2. **DRG-BCD**: Uses only the Data Reference Graph (DRG).
3. **DRG-CG-BCD**: Combines edges from both DRG and CG.
4. **All-BCD**: Our proposed method, which uses edges from all three graphs (Sequence Graph, CG, and DRG).

### Results

Figure 2(a) shows the \( F_{w1} \) scores for the decomposition into components output by BCD and the baseline methods. For both PE and ELF executables, All-BCD achieves the highest \( F_{w1} \) score, outperforming the baseline methods. The DRG-CG-BCD method, which combines CG and DRG edges, performs better than the single-property baselines, supporting our claim that a combination of edge weights from all three graphs improves component recovery.

The negligible difference in \( F_{w1} \) between PE and ELF programs indicates that BCD is robust to variations in executables produced by different compilers. Additionally, the close performance of DRG-CG-BCD to All-BCD suggests that BCD is robust to function re-ordering by ignoring Sequence Graph (SG) edges during partitioning.

### Error Analysis

BCD automatically determines the number of components using Newman’s community detection method. Under-splitting occurs when functions belonging to multiple classes are grouped into a single component, while over-splitting occurs when a set of functions belonging to a single class are distributed across multiple components.

The median \( F_{w1} \) score for PE executables is 0.88 ± 0.0036, and for ELF executables is 0.84 ± 0.001. Component under-fitting is more common in classes with one or two functions, which lack strong code and data locality features. For example, program P18, with 7 single-function classes, resulted in 2 under-split partitions.

### Hyperparameter Sensitivity

BCD's sensitivity to meta-weight parameter values is measured by the variance of \( \alpha \), \( \beta \), and \( \gamma \) obtained during 5-fold cross-validation. The average values yielding the highest \( F_{w1} \) score are \( \alpha = 0.237 \pm 0.0026 \), \( \beta = 0.362 \pm 0.0026 \), and \( \gamma = 0.4 \pm 0.0028 \), indicating that BCD's performance is not significantly sensitive to the training data.

### Runtime

BCD's runtime is measured by aggregating the time spent on each step: extraction of decomposition properties, construction of the decomposition graph, and partitioning. Using a Windows 32-bit machine with 4GB RAM and IDA for disassembly, the smallest program (P3) took 3 seconds for property extraction, and the largest (P24) took 30 seconds. Decomposition graph construction averaged 10 seconds per program. The majority of the time is spent on graph partitioning, with each iteration of the LinLogLayout toolkit taking between 3 seconds (P6) and 150 seconds (P24).

### Related Work

Binary decomposition is related to several problems, including recovery of class hierarchies in C++ programs, code clone detection, program diffing, and identification of functions with the same semantics. However, these differ in their goals. Our goal is to statically decompose an executable into groups of structurally related functions, which may have different syntactic representations and input-output relationships.

### Discussion

#### Obfuscation

BCD assumes the binary is unobfuscated. Excluding the function sequence graph reduces the \( F_{w1} \) score from 0.86 to 0.78 for C++ applications. While obfuscation can defeat BCD, it may also raise detection alarms. De-obfuscation techniques, such as dynamic unpackers, can address this issue.

#### Dynamic Features

For simplicity, we focused on static features. Incorporating dynamic features, such as function order and heap-allocated memory access, could improve the efficiency of the decomposition graph.

### Conclusion

This paper introduces the problem of binary code decomposition and proposes BCD, a novel approach that decomposes a program executable into components. BCD extracts code locality, data references, and calling relationships to build a decomposition graph, which is then partitioned into disjoint components. Our evaluation results show that BCD achieves high precision and recall for decomposing tested programs into components with structurally related functions.

### Acknowledgements

We thank the anonymous reviewers for their valuable comments. This work was partially supported by various awards and grants from AFOSR, ONR, NSF, the Regional Government of Madrid, the Spanish Government, and the European Union.

### References

[References listed as in the original text]

---

This revised version aims to enhance clarity, coherence, and professionalism while maintaining the essential content and structure of the original text.