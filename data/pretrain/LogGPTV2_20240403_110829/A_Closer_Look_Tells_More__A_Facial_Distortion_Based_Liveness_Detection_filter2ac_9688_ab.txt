ly. Next, the fully connected layers ğ¹ ğ¶1 and ğ¹ ğ¶2 perform
high-level reasoning. Assuming ğ¹ ğ¶2 consists of ğ‘€ neurons,
a vector ğ‘“ ğ‘ = (ğ‘’1, ğ‘’2, ..., ğ‘’ğ‘€ )ğ‘‡ is produced by ğ¹ ğ¶2 and it is
passed to the output layer ğ‘‚ğ‘ˆ ğ‘‡ . The output layer estimates
the probabilities for ğ¶ classes. The probability of each class
ğ‘ is estimated using the following multinomial distribution
ğ‘ƒ (ğ‘¦ = ğ‘) = ğ‘†ğ‘ =
exp(ğ‘‰ ğ‘¦
ğ‘=1 exp(ğ‘‰ ğ‘¦
where ğ¶ is number of classes, ğ‘‰ ğ‘¦
ğ‘ is the ğ‘-th row of a learnable
weighting matrix ğ‘‰ ğ‘¦, and ğ‘ğ‘¦
ğ‘ is a bias. Since liveness detection
is used to distinguish between a real face and a forged face,
ğ¶ is set to 2.
âˆ‘ï¸€ğ¶
ğ‘ Â· ğ‘“ ğ‘ + ğ‘ğ‘¦
ğ‘ )
ğ‘ Â· ğ‘“ ğ‘ + ğ‘ğ‘¦
ğ‘ )
(1)
4 DATA COLLECTION AND DATASET
GENERATION
An IRB-approved user study is conducted to collect usersâ€™
data for both legitimate requests and MVFF-based attacks
which include the photo-based attacks, video-based attacks,
and 3D virtual face model-based attacks.
4.1 Data Collection
Our user study consists of two parts and involves 43 males
and 28 females with the age range between 18 and 35.
In the first part, we collect the participantsâ€™ multiple selfie
facial videos at controlled device positions. Each participant
is asked to hold a mobile phone and to take 3 selfie frontal
facial video clips over a controlled distance ğ·ğ¹ ğ· between
his/her face and mobile phone. The mobile phone in our
experiments is a Google Nexus 6P smartphone with an 8-
megapixel front-facing camera and Android 7.1.1 operating
system. The front-facing camera is used to take 1080p HD
video recording at 30 fps. Each resulting video clip lasts for
3 seconds where each frame is 1920 Ã— 1080 pixels in size
with the face in the middle of the frame. The range of the
controlled distance ğ·ğ¹ ğ· between the face and smartphone
includes 20cm, 30cm, 40cm, and 50cm. We collected 12 selfie
frontal facial video clips from each participant.
In the second part, participants are asked to perform
trials of FaceCloseup with the controlled device movement
distances using the provided smartphone. Each participant
holds and moves the smartphone away from his/her face from
the distance 20ğ‘ğ‘š to the distance 50ğ‘ğ‘š, from the distance
30ğ‘ğ‘š to the distance 50ğ‘ğ‘š, and from the distance 40ğ‘ğ‘š to
the distance 50ğ‘ğ‘š, respectively. 10 trials are performed by
each participant under each controlled movement setting. In
total, facial video data from 30 trials by each participant.
4.2 Dataset Generation
To mimic legitimate requests and MVFF-based attacks, we
generate a legitimate dataset, a photo-based attack dataset, a
video-based attack dataset, and a 3D virtual face model-based
attack dataset based on the collected facial videos.
4.2.1 Legitimate Dataset. The legitimate dataset includes
the closeup facial videos taken during trials of FaceCloseup
with the smartphone movement from the distance ğ·ğ¹ ğ· =
20ğ‘ğ‘š to the distance ğ·ğ¹ ğ· = 50ğ‘ğ‘š which is presented in
Section 4.1. Thus the legitimate dataset includes 710 trials.
4.2.2 Photo-Based Attack Dataset. We firstly manually ex-
tract 10 facial frames from the selfie frontal facial video clips
taken by each participant at each fixed distance ğ·ğ¹ ğ· includ-
ing 30cm, 40cm, and 50cm. The majority of the participants
never reveal selfie facial photos/videos taken at the distance
shorter than 30cm due to the obvious facial distortion.
Secondly, we display each extracted facial frame on an
iPad Retina screen to mimic the photo-based attacks. The
scale of face region in the frame is adjusted to be displayed in
full screen so that the size of the face displayed on the screen
is close to the size of a real face. While the smartphone
is fixed on the table with the front-facing camera always
shooting at the iPad screen, we move the iPad away from the
smartphone from the distance ğ·ğ¹ ğ· = 20ğ‘ğ‘š to the distance
ğ·ğ¹ ğ· = 50ğ‘ğ‘š. During the movement, the front-facing camera
on the smartphone records video about the face on the screen.
In total, the photo-based attack dataset includes 1420 videos.
4.2.3 Video-Based Attack Dataset. We use the videos of the
trials taken by each participant who moves the smartphone
from the distance ğ·ğ¹ ğ· = 30ğ‘ğ‘š to the distance ğ·ğ¹ ğ· = 50ğ‘ğ‘š
and from the distance ğ·ğ¹ ğ· = 40ğ‘ğ‘š to the distance ğ·ğ¹ ğ· =
50ğ‘ğ‘š. Each of the video is displayed on the iPad screen while
the smartphone is fixed on the table with the front-facing
camera always recording the screen. The scales of the video
frames are adjusted accordingly so that the face region in the
first frame of the video is displayed in full screen. The distance
between the iPad and the smartphone is fixed to 20cm since
the displayed video includes the movements similar to the
legitimate request. Thus we have 1420 attacking videos in
the video-based attacks in total.
4.2.4 3D Virtual Face Model-Based Attack Dataset. There
exists a variety of 3D face reconstruction algorithms [2, 10,
11, 16]. Most of the 3D face reconstruction algorithms take
a single or multiple regular facial photos of the victim as
input. Based on the detected facial landmarks, a 3D virtual
face model for the victim is estimated by optimizing the
Session 3B: Learning and AuthenticationAsiaCCS â€™19, July 9â€“12, 2019, Auckland, New Zealand243geometry of a 3D morphable face model in order to match
the observed 2D landmarks. Note that the optimization of
the 3D morphable face model is based on an important
assumption that a virtual camera is shooting at the face
with a pre-defined distance between the virtual camera and
the face (usually assumed to be infinite). Then, image-based
texturing and gaze correction techniques are applied to adjust
the 3D face model. At last, the textured 3D face model of
the victim can be used to produce different facial expressions
and head rotation in real time.
FaceCloseup determines the liveness of a face based on
the change of facial distortion as the distance between the
camera and the face changes. The above 3D virtual face
model cannot defeat FaceCloseup because the 3D virtual face
model cannot generate the facial distortion when the camera
is close to the face.
In order to simulate a powerful adversary in the 3D vir-
tual face model-based attack, we use the state-of-the-art
perspective-aware 3D face reconstruction algorithm, by Fried
et al. [5] published in SIGGRAPHâ€™2016, which can generate
both the facial distortion according to the changes of a vir-
tual camera position and any changes of facial expressions
and head poses. To reconstruct a perspective-aware 3D face
model for a victim, the perspective-aware 3D face reconstruc-
tion algorithm [5] firstly extracts 69 facial landmarks from a
given facial photo of the victim. Among the extracted facial
landmarks, 66 facial landmarks are automatically detected
by the SDM-based landmark detection algorithm [15]. The
other 3 facial landmarks on top of head and ears are manual-
ly labelled for higher accuracy. Because the 3D face model
is correlated with an identity vector ğ›½ âˆˆ R1Ã—50, an expres-
sion vector ğ›¾ âˆˆ R1Ã—25, an upper-triangular intrinsic matrix
ğ‘ˆ âˆˆ R3Ã—3, a rotation matrix ğ‘… âˆˆ R3Ã—3, and a translation
matrix ğ‘‡ âˆˆ R3Ã—4, the facial photo and the 69 facial landmark
locations are used to fit a 3D head model by finding the
best parameters ğ›½, ğ›¾, ğ‘ˆ, ğ‘…, ğ‘‡ such that the Euclidean dis-
tance between the facial landmarks and the projection of the
landmarks on the 3D head model is minimized. After a good
fit is made between the input facial photo and the 3D head
model, the 3D head model is manipulated for a new project-
ed head shape by changing the virtual camera distance and
head poses. In particular, one can move the virtual camera
towards/away from the face by adjusting the translation ğ‘‡
and rotate head by adjusting both the translation ğ‘‡ and the
rotation ğ‘…. At last, the manipulated 3D head model produces
a 2D facial photo with the distortion corresponding to the
changes of camera position. Due to the space limit, we refer
readers to [5] for the details of this perspective-aware 3D face
reconstruction algorithm.
To perform the 3D virtual face model-based attack, for
each participant, we extract 10 facial photos from the selfie fa-
cial video taken at each controlled distance ğ·ğ¹ ğ· between the
participantâ€™s face and smartphone which are collected in the
first part of the user study as explained in Section 4.1. The
range of ğ·ğ¹ ğ· includes 30cm, 40cm, and 50cm. Given each
facial photo as an input, we use the perspective-aware 3D face
reconstruction algorithm to generate the facial photos with
facial distortion by manually changing the virtual camera
distance in the algorithm. The values of the virtual camer-
a distance include 20cm, 25cm, 30cm, 35cm, 40cm, 45cm,
and 50cm. We manually adjust the scale of the resulting
manipulated facial photos in compliance with the size of face
region in the original facial photos extracted from the selfie
facial video taken over the same/similar distances. There-
fore, we generate a sequence of 7 manipulated facial photos
from each extracted facial photo. In total, the 3D virtual
face model-based attack dataset consists of 2130 sequences
of manipulated facial photos.
5 EVALUATION AND
EXPERIMENTAL RESULTS
In this section, we present the settings of our experiments.
Then we evaluate the performance of FaceCloseup in terms
of security, effectiveness and practicality.
5.1 Experiment Settings
To determine the liveness of a face, the VFS of FaceCloseup
firstly selects ğ¾ frames from an input facial video based on
the size ranges (ğ‘ ğ‘§1, ğ‘ ğ‘§2, ..., ğ‘ ğ‘§ğ¾ ) of the face region detected
in the video frames as presented in Section 3. Since the size of
the face region detected in the frame mainly depends on the
distance between a participantâ€™s face and the smartphone,
the size ranges (ğ‘ ğ‘§1, ğ‘ ğ‘§2, ..., ğ‘ ğ‘§ğ¾ ) are determined based on the
distribution of the size of the detected faces in the facial videos
taken at the distance ğ·ğ¹ ğ· = 20ğ‘ğ‘š, 30ğ‘ğ‘š, 40ğ‘ğ‘š, 50ğ‘ğ‘š. These
facial videos with the different distance ğ·ğ¹ ğ· are collected in
the user study as described in Section 4.1. We set ğ¾ = 7 and
choose a base facial photo of each user with the size of face
region in ğ‘ ğ‘§1 for best performance and better coverage of the
video frames taken at different distances in our experiments.
The size ranges of the detected faces are shown in Table 1.
For a given facial video, we select a sequence of the frames
by randomly choosing a frame among the frames containing
a face with size in ğ‘ ğ‘§ğ‘– where ğ‘– âˆˆ {1, 2, ..., 7}. We repeat the
selection for 20 times and extract 20 sequences of frames as
samples from the given facial video. Therefore, 20 Ã— 710 =
14200 samples are generated based on the legitimate dataset.
20Ã— 1420 = 28400 samples are generated based on the photo-
based attack dataset and the video-based attack dataset,
respectively. And 2130 samples are generated based on the
3D virtual face model-based attack dataset.
Table 1: Size ranges of the face region for frame se-
lection
Size range in mega-pixels
ğ‘ ğ‘§1
ğ‘ ğ‘§2
ğ‘ ğ‘§3
ğ‘ ğ‘§4
ğ‘ ğ‘§5
ğ‘ ğ‘§6
ğ‘ ğ‘§7
(0.75, 0.85)
(0.65, 0.75)
(0.55, 0.65)
(0.45, 0.55)
(0.35, 0.45)
(0.25, 0.35)
(0.15, 0.25)
Session 3B: Learning and AuthenticationAsiaCCS â€™19, July 9â€“12, 2019, Auckland, New Zealand244The LC of FaceCloseup is empowered with a CNN-based
classification algorithm. The structure and parameters of
the CNN model are shown in Table 2. We use 5-fold cross
validation method to evaluate FaceCloseup. Thus 80% of
the samples are used to train the CNN model on a desktop
equipped with a 12GB TITAN X graphics card, 60GB mem-
ory, and 20 Intel Core-i7 CPUs. The learning rate is set to