7
8
9
10
11 }
local = locationManager .getBestProvider(...);
location =
locationManager .getLastKnownLocation(local);
...
}
private void requestLocationUpdates () {...}
(b) Deobfuscated code
Figure 8: Deobfuscating ﬁelds and methods that store and,
respectively, handle location data. The code snippet is taken
from the Bgserv malware sample.
Summary of ProGuard Experiments.
In summary, our experiments demonstrate that: (i) De-
Guard correctly predicts an overwhelming part of the pro-
gram elements obfuscated by ProGuard, thereby eﬀectively
reversing ProGuard’s obfuscation mechanism; (ii) it pre-
cisely identiﬁes third-party Android libraries, and (iii) it is
robust and eﬃcient, taking on average less than a minute
per application.
6.2.2 Experiments with Malware Samples
We randomly selected one sample from each of the 49 mal-
ware families reported in [40]. We used DeGuard to deob-
fuscate the selected samples and manually inspected some
of them. While we cannot report DeGuard’s exact preci-
sion on the selected samples (since they are all obfuscated),
we report on several interesting examples that suggest that
DeGuard can be useful when inspecting malware. We also
discuss DeGuard’s limitations in the context of malware.
Revealing Base64 String Decoders.
Malware often disguises text strings using the Base64 en-
coding scheme. DeGuard can be used to discover the classes
that implement such standard encoding schemes. Concretely,
DeGuard discovered the Base64 decoders in three of the
malware samples that we inspected. We remark that eight
other samples also have Base64 decoders, but the classes
that implement this encoding are not obfuscated.
As an example, in Figure 7 we show code snippets taken
from the GingerMaster [39] malware sample2. Figure 7(a)
shows the obfuscated code, and Figure 7(b) the correspond-
ing deobfuscated code obtained using DeGuard. DeGuard
discovers that the class d implements a Base64 decoder and
2SHA1: 2e9b8a7a149fcb6bd2367ac36e98a904d4c5e482
System.getProperty("line.separator");
public static byte[] decode (String str) {...};
Figure 7: Deobfuscating the Base64 decoder found in the
GingerMaster malware sample.
age names are unique to a particular application. For ex-
ample, org.apache.commons.collections4 is classiﬁed as
a library, while com.pindroid.providers is classiﬁed as an
application-speciﬁc package name. Based on our heuristic,
we identiﬁed a total of 133 libraries imported in the APKs.
To measure DeGuard’s eﬀectiveness in predicting libraries
we use two (standard) metrics — precision and recall. Let
L denote the set of all obfuscated libraries imported by an
application and P denote the set of predicted libraries by
DeGuard. Here, P contains all package names that map to
one of the 133 names that we have previously classiﬁed as
libraries. We compute DeGuard’s precision using the for-
mula precision = |L ∩ P|/|P|, and recall using the formula
recall = |L ∩ P|/|L|.
Intuitively, precision shows the per-
centage of libraries that DeGuard correctly predicts, and
recall captures the percentage of libraries that DeGuard
attempts to predict.
DeGuard’s precision and recall for predicting libraries is,
on average, 91.3% and 91.0%, respectively. This result indi-
cates that DeGuard predicts libraries more accurately com-
pared to arbitrary program elements. Further, DeGuard
almost never mis-predicts a library. This is likely because
the training set, which we use to learn the weights of all
features, may contain applications that import the same li-
braries that we attempt to predict.
We remark that the benchmark applications in this ex-
periment are not malicious, and so the libraries that they
import are also benign. Malicious third-party libraries em-
bedded in applications can be more diﬃcult to identify.
Prediction Speed.
For most applications, DeGuard takes on average less
than a minute to deobfuscate its APK. Around 10% of the
time is spent in constructing the dependency graph and de-
riving all syntactic and semantics constraints. The remain-
ing 90% of the time is spent in computing the most likely
naming assignment using the approximate MAP inference
query. An interesting future work item is to investigate
faster MAP inference algorithms leveraging the speciﬁcs of
our dependency graphs.
353renames it to Base64. Further, DeGuard reveals that the
method a(String) decodes strings formatted in Base64 (we
conﬁrmed this by inspecting the implementation of method
a(String)) and renames it to decode(String). The state-
ment Base64.decode(String) is more descriptive compared
to d.a(String), and we thus believe that DeGuard can
help security analysts in inspecting this malware sample.
Revealing Sensitive Data Usage.
Malware often steals personal information, such as loca-
tion data, device identiﬁers, and phone numbers. We search
the deobfuscated code of our malware samples for identi-
ﬁers such as location and deviceId and discovered that
DeGuard often deobfuscates the names of ﬁelds that store
sensitive data and methods that handle sensitive data. As
an example, in Figure 8 we show an obfuscated code snip-
pet taken from the Bgserv malware sample3, along with
the deobfuscated code output by DeGuard. We observe
that DeGuard renames the obfuscated ﬁeld o, which stores
the device’s location, to location. Further,
it renames
the method j() to requestLocationUpdates(). We in-
spected the method j() to reveal that it instantiates the
interface LocationListener and implements the method
onLocationChanged() to receive location updates. The name
requestLocationUpdates() assigned by DeGuard captures
the behavior of this method.
In the remaining malware samples, we discovered that De-
Guard renames a number of ﬁelds and methods that handle
other kinds of sensitive data, such as device identiﬁers. We
believe that DeGuard can help in inspecting malware that
misuses sensitive data, e.g., by allowing security experts to
search for certain identiﬁers in the deobfuscated code.
Limitations.
In addition to obfuscating program identiﬁers, most of
malware samples we inspected are obfuscated with addi-
tional techniques to further hinder reverse engineering. Ex-
amples include custom encoding of strings (e.g. using cus-
tom encryption), extensive use of reﬂection, control ﬂow ob-
fuscation, code reordering using goto statements, and oth-
ers. Reversing these additional obfuscation steps is beyond
the scope of DeGuard. To inspect malware, security ana-
lysts must therefore use a combination of deobfuscation tools
tailored to reversing diﬀerent obfuscation techniques.
7. RELATED WORK
This section summarizes the works that are most closely
related to ours.
Suggesting names for program elements.
Several works have studied the eﬀect of identiﬁer names [36,
33, 12] and have shown that good names have signiﬁcant im-
pact on one’s ability to understand the source code. These
studies have inspired tools [13, 33] that rename identiﬁers
within a project to make them follow a given coding con-
In contrast to DeGuard, the systems presented
vention.
in [13] and [33] cannot be used for deobfuscation. The
tool described in [13] relies on the names to be replaced to
have meaningful, non-obfuscated names, so it can improve
them. This system cannot predict meaningful replacements
of names such as “a”. The tool of [33] does not suggest
3SHA1: 03f9fc8769422f66c59922319bffad46d0ceea94
new names: it only identiﬁes bad names based on syntactic
guidelines and provides that feedback to developers.
The works of Allamanis et al.[8, 9] suggest names for Java
program elements using n-gram language models and neural
networks. Their technique, however, only allows predicting
the name of a single program element and is thus not appli-
cable to a deobfuscation task where most names are missing.
LibRadar [26] detects third-party Android libraries by ex-
tracting a unique ﬁngerprint from each library and creating
a mapping from ﬁngerprints to library names. Obfuscated
libraries are then identiﬁed by their ﬁngerprints. In contrast
to DeGuard, LibRadar is less general as it predicts names
only for packages, and it is less robust because it relies on
stable features (i.e., completely unaﬀected by obfuscation).
Probabilistic models for programs.
A recent surge in the number of open-source repositories
has triggered several authors to create large-scale probabilis-
tic models for code. These models are then used for novel ap-
plications such as code completion [32], generating code from
natural language [17, 10], sampling code snippets [27], pro-
gramming language translation [18], type annotating pro-
grams [19, 31] and others.
Closest to our work is [31] which also uses structured pre-
diction and the Nice2Predict framework [5] to guess names
of local variables for JavaScript programs. Our setting how-
ever is diﬀerent and requires more diverse feature functions,
constraints and range of elements for which names are to
be predicted; further, we use an order of magnitude more
scalable learning mechanism than [31].
Several works [22, 21, 16, 24] use graphical models to dis-
cover properties about programs such as function speciﬁ-
cations and invariants. These works, however, do not use
MAP inference to discover overall optimal solutions for all
predicted properties (and most do not learn from existing
programs). The work of Shin et al. [34] uses neural networks
and a large training set to identify libraries in binaries. In
the context of Android, a recent paper by Octeau et al. [28]
uses a probabilistic model and static analysis to determine
if two applications may communicate via the Android intent
mechanisms. However, theirs is a rather diﬀerent task than
the one addressed by our work.
8. CONCLUSION
We presented a new approach for layout deobfuscation
of Android APKs. The key idea is to phrase the problem
of reversing obfuscated names as structured prediction in a
probabilistic graphical model and to leverage the vast avail-
ability of non-obfuscated Android programs to learn this
model. We implemented our approach in a system called
DeGuard and demonstrated that DeGuard can success-
fully and with high precision reverse obfuscations performed
by ProGuard, a task beyond the reach of existing systems.
We believe that our work indicates the promise of leveraging
probabilistic models over “Big Code” for addressing impor-
tant challenges in security.
9. ACKNOWLEDGMENTS
The research in this work has been partially supported
by ERC starting grant #680358. We thank Matteo Panza-
cchi for extending Nice2Predict [5] with support for pseudo
likelihood estimation.
35410. REFERENCES
[1] Advertising SDK Can Be Hijacked for Making Phone
Calls, Geo-Locating.
http://www.hotforsecurity.com/blog/advertising-sdk-
can-be-hijacked-for-making-phone-calls-geo-locating-
7461.html.
[2] dex2jar. https://github.com/pxb1988/dex2jar.
[3] F-Droid. https://f-droid.org/.
[4] Java Decompiler. http://jd.benow.ca/.
[5] Nice2Predict.
https://github.com/eth-srl/Nice2Predict.
[6] ProGuard. http://proguard.sourceforge.net/.
[7] Type Erasure. https://docs.oracle.com/javase/
tutorial/java/generics/genTypes.html.
[8] M. Allamanis, E. T. Barr, C. Bird, and C. Sutton.
Learning natural coding conventions. In FSE, 2014.
[9] M. Allamanis, E. T. Barr, C. Bird, and C. Sutton.
Suggesting accurate method and class names. In FSE,
2015.
[10] M. Allamanis, D. Tarlow, A. D. Gordon, and Y. Wei.
Bimodal modelling of source code and natural
language. In ICML, 2015.
[11] S. Arzt, S. Rasthofer, C. Fritz, E. Bodden, A. Bartel,
J. Klein, Y. Le Traon, D. Octeau, and P. McDaniel.
Flowdroid: Precise context, ﬂow, ﬁeld, object-sensitive
and lifecycle-aware taint analysis for android apps. In
PLDI, 2014.
[12] S. Butler, M. Wermelinger, Y. Yu, and H. Sharp.
Exploring the inﬂuence of identiﬁer names on code
quality: An empirical study. In CSMR, 2010.
[13] B. Caprile and P. Tonella. Restructuring program
identiﬁer names. In ICSM, 2000.
[14] K. Chen, X. Wang, Y. Chen, P. Wang, Y. Lee,
X. Wang, B. Ma, A. Wang, Y. Zhang, and W. Zou.
Following devil’s footprints: Cross-platform analysis of
potentially harmful libraries on android and ios. In
S&P, 2016.
[15] W. Enck, P. Gilbert, B.-G. Chun, L. P. Cox, J. Jung,
P. McDaniel, and A. N. Sheth. Taintdroid: An
information-ﬂow tracking system for realtime privacy
monitoring on smartphones. In OSDI, 2010.
[16] S. Gulwani and N. Jojic. Program veriﬁcation as
probabilistic inference. In POPL, 2007.
[17] T. Gvero and V. Kuncak. Synthesizing java
expressions from free-form queries. In OOPSLA, 2015.
[18] S. Karaivanov, V. Raychev, and M. Vechev.
Phrase-based statistical translation of programming
languages. Onward!, 2014.
[19] O. Katz, R. El-Yaniv, and E. Yahav. Estimating types
in binaries using predictive modeling. In POPL, 2016.
[20] D. Koller and N. Friedman. Probabilistic Graphical
Models: Principles and Techniques - Adaptive
Computation and Machine Learning. The MIT Press,
2009.
[21] T. Kremenek, A. Y. Ng, and D. Engler. A factor
graph model for software bug ﬁnding. In IJCAI, 2007.
[22] T. Kremenek, P. Twohey, G. Back, A. Ng, and
D. Engler. From uncertainty to belief: Inferring the
speciﬁcation within. In OSDI, 2006.
[23] J. D. Laﬀerty, A. McCallum, and F. C. N. Pereira.
Conditional Random Fields: Probabilistic Models for
Segmenting and Labeling Sequence Data. In ICML,
2001.
[24] B. Livshits, A. V. Nori, S. K. Rajamani, and
A. Banerjee. Merlin: Speciﬁcation inference for
explicit information ﬂow problems. In PLDI, 2009.
[25] D. Low. Protecting Java Code via Code Obfuscation.
Crossroads, 4(3), Apr. 1998.
[26] Z. Ma, H. Wang, Y. Guo, and X. Chen. Libradar: fast
and accurate detection of third-party libraries in
android apps. In ICSE 2016 - Companion Volume,
2016.
[27] C. J. Maddison and D. Tarlow. Structured generative
models of natural source code. In ICML, 2014.
[28] D. Octeau, S. Jha, M. Dering, P. McDaniel, A. Bartel,
L. Li, J. Klein, and Y. Le Traon. Combining static
analysis with probabilistic models to enable
market-scale android inter-component analysis. In
POPL, 2016.
[29] N. D. Ratliﬀ, J. A. Bagnell, and M. Zinkevich.
(Approximate) Subgradient Methods for Structured
Prediction. In AISTATS, 2007.
[30] V. Raychev, P. Bielik, M. Vechev, and A. Krause.
Learning programs from noisy data. In POPL, 2016.
[31] V. Raychev, M. Vechev, and A. Krause. Predicting
program properties from ”big code”. In POPL, 2015.
[32] V. Raychev, M. Vechev, and E. Yahav. Code
completion with statistical language models. In PLDI,
2014.
[33] P. A. Relf. Tool assisted identiﬁer naming for
improved software readability: an empirical study. In
ISESE, 2005.
[34] E. C. R. Shin, D. Song, and R. Moazzezi. Recognizing
functions in binaries with neural networks. In
USENIX Security, 2015.
[35] C. Sutton and A. McCallum. An introduction to
conditional random ﬁelds. Found. Trends Mach.
Learn., 4(4):267–373, Apr. 2012.
[36] A. A. Takang, P. A. Grubb, and R. D. Macredie. The
eﬀects of comments and identiﬁer names on program
comprehensibility: an experimental investigation. J.
Prog. Lang., 4(3):143–167, 1996.
[37] O. Tripp, S. Guarnieri, M. Pistoia, and A. Aravkin.
Aletheia: Improving the usability of static security
analysis. In CCS, 2014.
[38] R. Vall´ee-Rai, P. Co, E. Gagnon, L. Hendren, P. Lam,
and V. Sundaresan. Soot - a Java Bytecode
Optimization Framework. In Proceedings of the 1999
Conference of the Centre for Advanced Studies on
Collaborative Research. IBM Press, 1999.
[39] R. Yu. Ginmaster: A case study in android malware.
https://www.sophos.com/en-us/medialibrary/PDFs/
technical%20papers/Yu-VB2013.pdf.
[40] Y. Zhou and X. Jiang. Dissecting android malware:
Characterization and evolution. In S&P, 2012.
355