### Model Performance and Privacy Analysis

The model achieves an accuracy of 0.982 on the training set and 0.605 on the test set. Interestingly, at a 1% false positive rate (FPR), no members are revealed by the non-private neural network (NN) model, whereas some members are revealed by the privacy-preserving models. This discrepancy is attributed to the high number of extremely high-confidence incorrect outputs from the non-private model, which means that no confidence threshold can exclude at least 1% false positives.

For comparison, the neural network model by Shokri et al. [61], trained on a similar dataset but with 600 attributes instead of 100, achieves a test accuracy of 0.670.

### Accuracy and Privacy Trade-offs

Figure 5(b) compares the accuracy loss, and Figure 7 the privacy leakage, of neural network models trained with different variants of differential privacy. The trends for both accuracy and privacy are similar to those observed for logistic regression models (Figure 3). The relaxed variants achieve model utility close to the non-private baseline for \(\epsilon = 1000\), while naïve composition continues to suffer from high accuracy loss (0.372).

Advanced composition has a higher accuracy loss of 0.702 for \(\epsilon = 1000\) because it requires adding more noise than naïve composition when \(\epsilon\) exceeds the number of training epochs. Figure 7 shows the privacy leakage comparison of the variants against inference attacks, and the results are consistent with those observed for CIFAR-100.

### Discussion

While the tighter cumulative noise bounds provided by relaxed variants of differential privacy improve model utility for a given privacy budget, the reduction in noise increases vulnerability to inference attacks. Therefore, privacy does not come without trade-offs. Relaxations of the differential privacy definition that result in lower noise requirements also come with additional privacy risks. Although these relaxed definitions still satisfy the \((\epsilon, \delta)\)-differential privacy guarantees, the practical value of these guarantees diminishes rapidly with high \(\epsilon\) values and non-zero \(\delta\).

In our inference attack experiments, we use equal numbers of member and non-member records, providing a 50-50 prior success probability to the attacker. Even an \(\epsilon\)-DP implementation might leak information for small \(\epsilon\) values, though we did not observe such leakage. A skewed prior probability may lead to smaller leakage even for large \(\epsilon\) values. Our goal is to evaluate scenarios where the risk of inference is high, justifying the use of a 50-50 prior probability. We emphasize that our results show the privacy leakage due to two particular membership inference attacks. Future attacks may be able to infer more than is shown in our experiments.

### Conclusion

Differential privacy has earned a well-deserved reputation for providing principled and powerful mechanisms for ensuring provable privacy. However, when implemented for challenging tasks like machine learning, compromises must be made to preserve utility. It is essential to understand the privacy impact of these compromises. Our results contribute to this understanding and reveal that commonly-used relaxations of differential privacy may provide unacceptable utility-privacy trade-offs. We hope our study will encourage more careful assessments of the practical privacy value of formal claims based on differential privacy, leading to a deeper understanding of the privacy impact of design decisions and eventually to solutions that provide desirable and well-understood utility-privacy trade-offs.

### Availability

Open source code for reproducing all of our experiments is available at: https://github.com/bargavj/EvaluatingDPML.

### Acknowledgments

The authors are deeply grateful to Úlfar Erlingsson for pointing out key misunderstandings in an early version of this work and for convincing us of the importance of per-instance gradient clipping. We also thank Úlfar, Ilya Mironov, and Shuang Song for help validating and improving the work. We thank Vincent Bindschaedler for shepherding our paper, Youssef Errami and Jonah Weissman for contributions to the experiments, and Ben Livshits for feedback on the work. Atallah Hezbor, Faysal Shezan, Tanmoy Sen, Max Naylor, Joshua Holtzman, and Nan Yang helped systematize the related works. Finally, we thank Congzheng Song and Samuel Yeom for providing their implementation of inference attacks. This work was partially funded by grants from the National Science Foundation SaTC program (#1717950, #1915813) and support from Intel and Amazon.

### References

[1] Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In ACM Conference on Computer and Communications Security, 2016.
...
[79] Lingchen Zhao, Yan Zhang, Qian Wang, Yanjiao Chen, Cong Wang, and Qin Zou. Privacy-preserving collaborative deep learning with irregular participants. arXiv:1812.10113, 2018.

---

This revised text aims to be more coherent, clear, and professional, with a structured presentation of the findings and their implications.