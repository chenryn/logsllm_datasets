el achieves accuracy of 0.982 on the training set and 0.605 on
2Curiously, this appears to be contradicted at 1% FPR where no mem-
bers are revealed by non-private NN model but some are revealed by the
privacy-preserving models. This is due to the number of extremely high-
conﬁdence incorrect outputs of the non-private model, meaning that there is
no conﬁdence threshold that does not include at least 1% false positives.
test set. In comparison, the neural network model of Shokri et
al. [61] trained on a similar data set (but with 600 attributes
instead of 100 as in our data set) achieves 0.670 test accuracy.
Figure 5(b) compares the accuracy loss, and Figure 7 the pri-
vacy leakage, of neural network models trained with diﬀerent
variants of diﬀerential privacy. The trends for both accuracy
and privacy are similar to those for the logistic regression mod-
els (Figure 3). The relaxed variants achieve model utility close
to the non-private baseline for  = 1000, while naïve compo-
sition continues to suﬀer from high accuracy loss (0.372).
Advanced composition has higher accuracy loss of 0.702 for
 = 1000 as it requires addition of more noise than naïve com-
position when  is greater than the number of training epochs.
Figure 7 shows the privacy leakage comparison of the variants
against the inference attacks. The results are consistent with
those observed for CIFAR-100.
1908    28th USENIX Security Symposium
USENIX Association
(a) Shokri et al. membership inference
(b) Yeom et al. membership inference
(c) Yeom et al. attribute inference
Figure 7: Inference attacks on neural network (Purchase-100).
4.4 Discussion
While the tighter cumulative noise bounds provided by re-
laxed variants of diﬀerential privacy improve model utility for
a given privacy budget, the reduction in noise increases vul-
nerability to inference attacks. Thus, privacy does not come
for free, and the relaxations of the diﬀerential privacy def-
inition that result in lower noise requirements come with
additional privacy risks. While these relaxed deﬁnitions still
satisfy the (, δ)-diﬀerential privacy guarantees, the concrete
value of these guarantees diminishes rapidly with high  val-
ues and non-zero δ. Although the theoretical guarantees pro-
vided by diﬀerential privacy are very appealing, once  values
exceed small values, the practical value of these guarantees
is insigniﬁcant—in most of our inference attack ﬁgures, the
theoretical bound given by -DP falls oﬀ the graph before
any measurable privacy leakage occurs (and at levels well
before models provide acceptable utility). The value of these
privacy mechanisms comes not from the theoretical guaran-
tees, but from the impact of the mechanism on what realistic
adversaries can infer.
We note that in our inference attack experiments, we use
equal numbers of member and non-member records which
provides 50-50 prior success probability to the attacker. Thus,
even an -DP implementation might leak even for small 
values, though we did not observe any such leakage. Alterna-
tively, a skewed prior probability may lead to smaller leakage
even for large  values. Our goal in this work is to evaluate
scenarios where risk of inference is high, so the use of 50-50
prior probability is justiﬁed. We also emphasis that our results
show the privacy leakage due to two particular membership
inference attacks. Attacks only get better, so future attacks
may be able to infer more than is shown in our experiments.
5 Conclusion
Diﬀerential privacy has earned a well-deserved reputation
providing principled and powerful mechanisms for ensuring
provable privacy. However, when it is implemented for chal-
lenging tasks such as machine learning, compromises must
be made to preserve utility. It is essential that the privacy
impact of those compromises is well understood when diﬀer-
ential privacy is deployed to protect sensitive data. Our results
are a step towards improving that understanding, and reveal
that the commonly-used relaxations of diﬀerential privacy
may provide unacceptable utility-privacy trade-oﬀs. We hope
our study will encourage more careful assessments of the
practical privacy value of formal claims based on diﬀerential
privacy, and lead to deeper understanding of the privacy im-
pact of design decisions when deploying diﬀerential privacy,
and eventually to solutions that provide desirable, and well
understood, utility-privacy trade-oﬀs.
Availability
Open source code for reproducing all of our experiments is
available at https://github.com/bargavj/EvaluatingDPML.
Acknowledgments
The authors are deeply grateful to Úlfar Erlingsson for point-
ing out some key misunderstandings in an early version of
this work and for convincing us of the importance of per-
instance gradient clipping, and to Úlfar, Ilya Mironov, and
Shuang Song for help validating and improving the work.
We thank Vincent Bindschaedler for shepherding our paper.
We thank Youssef Errami and Jonah Weissman for contribu-
tions to the experiments, and Ben Livshits for feedback on the
work. Atallah Hezbor, Faysal Shezan, Tanmoy Sen, Max Nay-
lor, Joshua Holtzman and Nan Yang helped systematize the
related works. Finally, we thank Congzheng Song and Samuel
Yeom for providing their implementation of inference attacks.
This work was partially funded by grants from the National
Science Foundation SaTC program (#1717950, #1915813)
and support from Intel and Amazon.
USENIX Association
28th USENIX Security Symposium    1909
102101100101102103Privacy Budget ()0.000.050.100.150.200.25Privacy Leakage-DP BoundRDPzCDPACNC102101100101102103Privacy Budget ()0.000.050.100.150.200.25Privacy Leakage-DP BoundRDPzCDPACNC102101100101102103Privacy Budget ()0.000.050.100.150.200.25Privacy Leakage-DP BoundRDPzCDPACNCReferences
[1] Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan
McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.
Deep learning with diﬀerential privacy. In ACM Confer-
ence on Computer and Communications Security, 2016.
[2] Galen Andrew, Steve Chien, and Nicolas Papernot. Ten-
sorFlow Privacy. https://github.com/tensorﬂow/privacy.
[3] Giuseppe Ateniese, Luigi Mancini, Angelo Spognardi,
Antonio Villani, Domenico Vitali, and Giovanni Felici.
Hacking smart machines with smarter ones: How to ex-
tract meaningful data from machine learning classiﬁers.
International Journal of Security and Networks, 2015.
[4] Michael Backes, Pascal Berrang, Mathias Humbert,
and Praveen Manoharan. Membership privacy in
MicroRNA-based studies. In ACM Conference on Com-
puter and Communications Security, 2016.
[5] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deb-
orah Estrin, and Vitaly Shmatikov. How to backdoor
federated learning. arXiv:1807.00459, 2018.
[6] Brett K Beaulieu-Jones, William Yuan, Samuel G
Finlayson, and Zhiwei Steven Wu.
Privacy-pre-
serving distributed deep learning for clinical data.
arXiv:1812.01484, 2018.
[7] Raghav Bhaskar, Srivatsan Laxman, Adam Smith, and
Abhradeep Thakurta. Discovering frequent patterns
In ACM SIGKDD Conference on
in sensitive data.
Knowledge Discovery and Data Mining, 2010.
[8] Abhishek Bhowmick, John Duchi, Julien Freudiger,
Gaurav Kapoor, and Ryan Rogers. Protection against
reconstruction and its applications in private federated
learning. arXiv:1812.00984, 2018.
[9] Mark Bun and Thomas Steinke. Concentrated diﬀer-
ential privacy: Simpliﬁcations, extensions, and lower
bounds. In Theory of Cryptography Conference, 2016.
[10] Nicholas Carlini, Chang Liu, Jernej Kos, Úlfar Erlings-
son, and Dawn Song. The Secret Sharer: Evaluating and
testing unintended memorization in neural networks. In
USENIX Security Symposium, 2019.
[14] John Duchi, Elad Hazan, and Yoram Singer. Adaptive
subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research,
2011.
[15] John C Duchi, Michael I Jordan, and Martin J Wain-
wright. Local privacy and statistical minimax rates. In
Symposium on Foundations of Computer Science, 2013.
[16] Cynthia Dwork. Diﬀerential Privacy: A Survey of Re-
sults. In International Conference on Theory and Appli-
cations of Models of Computation, 2008.
[17] Cynthia Dwork and Aaron Roth. The Algorithmic Foun-
dations of Diﬀerential Privacy. Foundations and Trends
in Theoretical Computer Science, 2014.
[18] Cynthia Dwork and Guy N. Rothblum. Concentrated
diﬀerential privacy. arXiv:1603.01887, 2016.
[19] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart.
Model inversion attacks that exploit conﬁdence informa-
tion and basic countermeasures. In ACM Conference on
Computer and Communications Security, 2015.
[20] Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon
Lin, David Page, and Thomas Ristenpart. Privacy in
pharmacogenetics: An end-to-end case study of person-
alized warfarin dosing. In USENIX Security Symposium.
[21] Arik Friedman and Assaf Schuster. Data mining with
diﬀerential privacy. In ACM SIGKDD Conference on
Knowledge Discovery and Data Mining, 2010.
[22] Karan Ganju, Qi Wang, Wei Yang, Carl A Gunter, and
Nikita Borisov. Property inference attacks on fully con-
nected neural networks using permutation invariant rep-
In ACM Conference on Computer and
resentations.
Communications Security, 2018.
[23] Joseph Geumlek, Shuang Song, and Kamalika Chaud-
huri. Rényi diﬀerential privacy mechanisms for pos-
In Advances in Neural Information
terior sampling.
Processing Systems, 2017.
[24] Robin C Geyer, Tassilo Klein, and Moin Nabi. Diﬀeren-
tially private federated learning: A client level perspec-
tive. arXiv:1712.07557, 2017.
[11] Kamalika Chaudhuri and Claire Monteleoni. Privacy-
preserving logistic regression. In Advances in Neural
Information Processing Systems, 2009.
[25] Jihun Hamm, Paul Cao, and Mikhail Belkin. Learning
privately from multiparty data. In International Confer-
ence on Machine Learning, 2016.
[12] Kamalika Chaudhuri, Claire Monteleoni, and Anand D.
Sarwate. Diﬀerentially private Empirical Risk Mini-
mization. Journal of Machine Learning Research, 2011.
[13] Zeyu Ding, Yuxin Wang, Guanhong Wang, Danfeng
Zhang, and Daniel Kifer. Detecting violations of diﬀer-
ential privacy. In ACM Conference on Computer and
Communications Security, 2018.
[26] Michael Hay, Ashwin Machanavajjhala, Gerome Mik-
lau, Yan Chen, and Dan Zhang. Principled evaluation
of diﬀerentially private algorithms using DPBench. In
ACM SIGMOD Conference on Management of Data,
2016.
[27] Nils Homer et al. Resolving individuals contributing
trace amounts of DNA to highly complex mixtures us-
1910    28th USENIX Security Symposium
USENIX Association
ing high-density SNP genotyping microarrays. PLoS
Genetics, 2008.
[28] Zonghao Huang, Rui Hu, Yanmin Gong, and Eric Chan-
Tin. DP-ADMM: ADMM-based distributed learning
with diﬀerential privacy. arXiv:1808.10101, 2018.
[29] Nick Hynes, Raymond Cheng, and Dawn Song. Ef-
ﬁcient deep learning on multi-source private data.
arXiv:1807.06689, 2018.
[30] Ali Inan, Murat Kantarcioglu, Gabriel Ghinita, and Elisa
Bertino. Private record matching using diﬀerential
In International Conference on Extending
privacy.
Database Technology, 2010.
[31] Roger Iyengar, Joseph P Near, Dawn Song, Om Thakkar,
Abhradeep Thakurta, and Lun Wang. Towards practi-
cal diﬀerentially private convex optimization. In IEEE
Symposium on Security and Privacy, 2019.
[41] Ninghui Li, Wahbeh Qardaji, Dong Su, and Jianneng
Cao. PrivBasis: Frequent itemset mining with diﬀeren-
tial privacy. The VLDB Journal, 2012.
[42] Ninghui Li, Wahbeh Qardaji, Dong Su, Yi Wu, and Wein-
ing Yang. Membership privacy: A unifying framework
for privacy deﬁnitions. In ACM Conference on Com-
puter and Communications Security, 2013.
[43] Dong C Liu and Jorge Nocedal. On the limited memory
BFGS method for large scale optimization. Mathemati-
cal programming, 1989.
[44] Yunhui Long, Vincent Bindschaedler, and Carl A.
Towards measuring membership privacy.
Gunter.
arXiv:1712.09136, 2017.
[45] Daniel Lowd and Christopher Meek. Adversarial learn-
ing. In ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining, 2005.
[32] Prateek Jain, Pravesh Kothari, and Abhradeep Thakurta.
Diﬀerentially private online learning. In Annual Confer-
ence on Learning Theory, 2012.
[46] Frank McSherry. Statistical inference considered harm-
https://github.com/frankmcsherry/blog/blob/master/
ful.
posts/2016-06-14.md, 2016.
[33] Prateek Jain and Abhradeep Thakurta. Diﬀerentially pri-
vate learning with kernels. In International Conference
on Machine Learning, 2013.
[34] Prateek Jain and Abhradeep Guha Thakurta.
(Near)
Dimension independent risk bounds for diﬀerentially
private learning. In International Conference on Ma-
chine Learning, 2014.
[35] Bargav Jayaraman, Lingxiao Wang, David Evans, and
Quanquan Gu. Distributed learning without distress:
Privacy-preserving Empirical Risk Minimization.
In
Advances in Neural Information Processing Systems,
2018.
[36] Kaggle, Inc. Acquire Valued Shoppers Challenge. https:
//kaggle.com/c/acquire-valued-shoppers-challenge/data,
2014.
[37] Diederik P Kingma and Jimmy Ba. Adam: A method
for stochastic optimization. In International Conference
on Learning Representations, 2015.
[38] Alex Krizhevsky. Learning multiple layers of fea-
tures from tiny images. Technical report, University
of Toronto, 2009.
[39] Jaewoo Lee. Diﬀerentially private variance reduced
stochastic gradient descent. In International Conference
on New Trends in Computing Sciences, 2017.
[40] Dong-Hui Li and Masao Fukushima. A modiﬁed BFGS
method and its global convergence in nonconvex mini-
mization. Journal of Computational and Applied Math-
ematics, 2001.
[47] Frank McSherry and Ilya Mironov. Diﬀerentially private
recommender systems: Building privacy into the Netﬂix
In ACM SIGKDD Conference on
prize contenders.
Knowledge Discovery and Data Mining, 2009.
[48] Frank McSherry and Kunal Talwar. Mechanism design
via diﬀerential privacy. In Symposium on Foundations
of Computer Science, 2007.
[49] Ilya Mironov. Rényi diﬀerential privacy. In IEEE Com-
puter Security Foundations Symposium, 2017.
[50] Luis Munoz-González, Battista Biggio, Ambra Demon-
tis, Andrea Paudice, Vasin Wongrassamee, Emil C.
Lupu, and Fabio Roli. Towards poisoning of deep learn-
ing algorithms with back-gradient optimization. In ACM
Workshop on Artiﬁcial Intelligence and Security, 2017.
[51] Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith.
Smooth sensitivity and sampling in private data analysis.
In ACM Symposium on Theory of Computing, 2007.
[52] Nicolas Papernot, Martín Abadi, Úlfar Erlingsson, Ian
Goodfellow, and Kunal Talwar. Semi-supervised knowl-
edge transfer for deep learning from private training
data. In International Conference on Learning Repre-
sentations, 2017.
[53] Mijung Park, Jimmy Foulds, Kamalika Chaudhuri, and
Max Welling. DP-EM: Diﬀerentially private expectation
maximization. In Artiﬁcial Intelligence and Statistics,
2017.
[54] Manas Pathak, Shantanu Rane, and Bhiksha Raj. Mul-
tiparty Diﬀerential Privacy via Aggregation of Locally
USENIX Association
28th USENIX Security Symposium    1911
Trained Classiﬁers. In Advances in Neural Information
Processing Systems, 2010.
[55] NhatHai Phan, Yue Wang, Xintao Wu, and Dejing Dou.
Diﬀerential privacy preservation for deep auto-encoders:
An application of human behavior prediction. In AAAI
Conference on Artiﬁcial Intelligence, 2016.
[56] NhatHai Phan, Xintao Wu, and Dejing Dou. Preserv-
ing diﬀerential privacy in convolutional deep belief net-
works. Machine Learning, 2017.
[57] Boris T Polyak and Anatoli B Juditsky. Acceleration of
stochastic approximation by averaging. SIAM Journal
on Control and Optimization, 1992.
[58] Md Atiqur Rahman, Tanzila Rahman, Robert Laganière,
Noman Mohammed, and Yang Wang. Membership in-
ference attack against diﬀerentially private deep learning
model. Transactions on Data Privacy, 2018.
[59] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal
Berrang, Mario Fritz, and Michael Backes. ML-Leaks:
Model and data independent membership inference at-
tacks and defenses on machine learning models.
In
Network and Distributed Systems Security Symposium.
[60] Reza Shokri and Vitaly Shmatikov. Privacy-preserving
deep learning. In ACM Conference on Computer and
Communications Security, 2015.
[61] Reza Shokri, Marco Stronati, Congzheng Song, and Vi-
taly Shmatikov. Membership inference attacks against
machine learning models. In IEEE Symposium on Secu-
rity and Privacy, 2017.
[62] Adam Smith and Abhradeep Thakurta. Diﬀerentially
Private Feature Selection via Stability Arguments, and
the Robustness of the Lasso. In Proceedings of Confer-
ence on Learning Theory, 2013.
[63] Shuang Song, Kamalika Chaudhuri, and Anand D Sar-
wate. Stochastic gradient descent with diﬀerentially
private updates. In IEEE Global Conference on Signal
and Information Processing, 2013.
[64] Kunal Talwar, Abhradeep Thakurta, and Li Zhang.
Private Empirical Risk Minimization beyond the
worst case: The eﬀect of the constraint set geometry.
arXiv:1411.5417, 2014.
[65] Kunal Talwar, Abhradeep Thakurta, and Li Zhang.
Nearly Optimal Private LASSO. In Advances in Neural
Information Processing Systems, 2015.
[66] Florian Tramèr, Fan Zhang, Ari Juels, Michael Reiter,
and Thomas Ristenpart. Stealing machine learning mod-
els via prediction APIs. In USENIX Security Symposium,
2016.
[67] Binghui Wang and Neil Zhenqiang Gong. Stealing hy-
perparameters in machine learning. In IEEE Symposium
on Security and Privacy, 2018.
[68] Di Wang, Minwei Ye, and Jinhui Xu. Diﬀerentially
private Empirical Risk Minimization revisited: Faster
and more general. In Advances in Neural Information
Processing Systems, 2017.
[69] Xi Wu, Matthew Fredrikson, Somesh Jha, and Jeﬀrey F
Naughton. A methodology for formalizing model-
In IEEE Computer Security Foun-
inversion attacks.
dations Symposium, 2016.
[70] Xi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri,
Somesh Jha, and Jeﬀrey Naughton. Bolt-on diﬀerential
privacy for scalable stochastic gradient descent-based
analytics. In ACM SIGMOD Conference on Manage-
ment of Data, 2017.
[71] Huang Xiao, Battista Biggio, Blaine Nelson, Han Xiao,
Claudia Eckert, and Fabio Roli. Support vector ma-
chines under adversarial label contamination. Neuro-
computing, 2015.
[72] Mengjia Yan, Christopher Fletcher, and Josep Torrellas.
Cache telepathy: Leveraging shared resource attacks to
learn DNN architectures. arXiv:1808.04761, 2018.
[73] Chaofei Yang, Qing Wu, Hai Li, and Yiran Chen. Gener-
ative poisoning attack method against neural networks.
arXiv:1703.01340, 2017.
[74] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and
Somesh Jha. Privacy risk in machine learning: Analyz-
ing the connection to overﬁtting. In IEEE Computer
Security Foundations Symposium, 2018.
[75] Lei Yu, Ling Liu, Calton Pu, Mehmet Emre Gursoy, and
Stacey Truex. Diﬀerentially private model publishing
for deep learning. In IEEE Symposium on Security and
Privacy, 2019.
[76] Matthew D Zeiler. ADADELTA: An adaptive learning
rate method. arXiv:1212.5701, 2012.
[77] Jiaqi Zhang, Kai Zheng, Wenlong Mou, and Liwei Wang.
Eﬃcient private ERM for smooth objectives. In Interna-
tional Joint Conference on Artiﬁcial Intelligence, 2017.
[78] Jun Zhang, Zhenjie Zhang, Xiaokui Xiao, Yin Yang, and
Marianne Winslett. Functional mechanism: Regression
analysis under diﬀerential privacy. The VLDB Journal,
2012.
[79] Lingchen Zhao, Yan Zhang, Qian Wang, Yanjiao Chen,
Cong Wang, and Qin Zou. Privacy-preserving col-
laborative deep learning with irregular participants.
arXiv:1812.10113, 2018.
1912    28th USENIX Security Symposium
USENIX Association