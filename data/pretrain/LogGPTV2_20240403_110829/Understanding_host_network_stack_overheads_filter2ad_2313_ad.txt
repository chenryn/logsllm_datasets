C
f
o
n
o
i
t
c
a
r
F
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
d
a t a   c
t c
p
1 ow
8 ows
16 ows
24 ows
s y s t e m
o
y
p
/ i p   p
e
c
o
e t d
r
n
g
s si n
vic
e
b
u
e  s
b   m g m t
s k
r y   a ll o
m e m
o
e
d
/
c
c
a ll o
l o
u
/
k
c
k
c
n l o
d
e
h
s c
g
u li n
e t c .
(a) Throughput-per-core (Gbps)
(b) Sender CPU breakdown
(c) Receiver CPU breakdown
Figure 5: Linux network stack performance for one-to-one traffic pattern. (a) Each column shows throughput-per-core achieved for different number
of flows. At 8 flows, the network is saturated, however, throughput-per-core decreases with more flows. (b, c) With all optimizations enabled, as the number
of flows increase, the fraction of CPU cycles spent in data copy decreases. On the receiver-side, network saturation leads to lower memory management
overhead (due to better page recycling) and higher scheduling overhead (due to frequent idling). The overall receiver-side CPU utilizations for x= 1, 8, 16 and
24 cases are, 1, 3.75, 5.21 and 6.58 cores, respectively. See §3.2 for description.
)
s
p
b
G
(
e
r
o
C
r
e
P
t
u
p
h
g
u
o
r
h
T
 60
 50
 40
 30
 20
 10
 0
No Opt.
TSO/GRO
Jumbo
aRFS
Total Thpt
 60
 50
 40
 30
 20
 10
 0
)
s
p
b
G
(
t
u
p
h
g
u
o
r
h
T
l
a
t
o
T
1
8
16
24
# Flows
l
s
e
c
y
C
U
P
C
f
o
n
o
i
t
c
a
r
F
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
d
a t a   c
t c
p
1 ow
8 ows
16 ows
24 ows
s y s t e m
o
y
p
/ i p   p
e
c
o
e t d
r
n
g
s si n
vic
e
b
u
e  s
b   m g m t
s k
r y   a ll o
m e m
o
e
d
/
c
c
a ll o
l o
u
/
k
c
k
c
n l o
d
e
h
s c
g
u li n
e t c .
)
s
p
b
G
(
e
r
o
C
r
e
P
t
u
p
h
g
u
o
r
h
T
 45
 40
 35
 30
Throughput Per Core
Receiver: Cache Miss Rate
 100
 90
 80
 70
 60
 50
 40
)
%
(
e
t
a
R
s
s
i
M
e
h
c
a
C
1
8
# Flows
16
24
(a) Throughput-per-core (Gbps)
(b) Receiver CPU breakdown
(c) L3 cache miss rate (%)
Figure 6: Linux network stack performance for incast traffic pattern. (a) Each column shows throughput-per-core for different number of flows
(receiver core is bottlenecked in all cases). Total throughput decreases with increase in the number of flows. (b) With all optimizations enabled, the fraction of
CPU cycles used by each component does not change significantly with number of flows. See [7] for sender-side CPU breakdown. (c) Receiver-side cache miss
rate increases with number of flows, resulting in higher per-byte data copy overhead, and reduced throughput-per-core. See §3.3 for description.
Processing overheads shift with network saturation. As
shown in Fig. 5(a), at 8 flows, the network link becomes the bottle-
neck, and throughput ends up getting fairly shared among all cores.
Fig. 5(c) shows that bottlenecks shift in this regime: scheduling
overhead increases and memory management overhead decreases.
Intuitively, when the network is saturated, the receiver cores start
to become idle at certain times—threads repeatedly go to sleep while
waiting for data, and wake up when new data arrives; this results in
increased context switching and scheduling overheads. This effect
becomes increasingly prominent with increase in number of flows
(Fig. 5(b), Fig. 5(c)), as the CPU utilization per-core decreases.
To understand reduction in memory alloc/dealloc overheads, we
observe that the kernel page allocator maintains per-core pageset
that includes a certain number of free pages. Upon an allocation re-
quest, pages can be fetched directly from the pageset, if available;
otherwise the global free-list needs to be accessed (which is a more
expensive operation). When multiple flows share the access link
bandwidth, each core serves relatively less amount of traffic com-
pared to the single flow case. This allows used pages to be recycled
back to the pageset before it becomes empty, hence reducing the
memory allocation overhead (Fig. 5(c)).
3.3 Increasing Receiver Contention via Incast
We now create additional contention at the receiver core using an
incast traffic pattern, varying number of flows from 1 to 24 (each
using a unique core at the sender). Compared to previous scenarios,
this scenario induces higher contention for (1) CPU resources such
as L3 cache and (2) CPU scheduling among application threads. We
discuss how these changes affect the network processing overheads.
Per-byte data copy overhead increases with increasing flows
per-core. Fig. 6(a) shows that throughput-per-core decreases with
increase in number of flows, observing as much as ∼19% drop with
8 flows when compare to the single-flow case. Fig. 6(b) shows that
the CPU breakdown does not change significantly with increasing
number of flows, implying that there is no evident shift in CPU
overheads. Fig. 6(c) provides some intuition for the root cause of
the throughput-per-core degradation. As number of flows per core
increases at the receiver side, applications for different flows com-
pete for the same L3 cache space resulting in increased cache miss
rate (the miss rate increases from 48% to 78%, as the number of
flows goes from 1 to 8.). Among other things, this leads to increased
per-byte data copy overhead and reduced throughput-per-core. As
shown in Fig. 6(c), the increase in L3 cache miss rate with increasing
flows correlates well with degradation in throughput-per-core.
Sender-driven nature of TCP precludes receiver-side sched-
uling. Higher cache contention observed above is the result of
multiple active flows on the same core. While senders could po-
tentially reduce such contention using careful flow scheduling, the
issue at the receiver side is fundamental: the sender-driven nature
of the TCP protocol precludes the receiver to control the number of
active flows per core, resulting in unavoidable CPU inefficiency. We
believe receiver-driven protocols [18, 35] can provide such control
to the receiver, thus enabling CPU-efficient transport designs.
71
(cid:16)
(cid:15)
(cid:7)
(cid:14)
(cid:8)
(cid:6)(cid:3)(cid:1)
(cid:6)(cid:2)(cid:1)
(cid:6)(cid:1)(cid:1)
(cid:5)(cid:1)
(cid:4)(cid:1)
(cid:3)(cid:1)
(cid:2)(cid:1)
(cid:1)
(cid:10)(cid:5)(cid:11)(cid:14)(cid:4)
(cid:1)(cid:4)(cid:8)(cid:17)(cid:18) (cid:1)(cid:2)(cid:7)(cid:8)
(cid:6)