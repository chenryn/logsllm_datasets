Bordenabe, K.
Chatzikokolakis, and
C. Palamidessi.
Optimal geo-indistinguishable
mechanisms for location privacy. In Proc. CCS, pages
251–262, 2014.
[8] K. Chatzikokolakis, M. E. André, N. E. Bordenabe, and
C. Palamidessi. Broadening the scope of differential
privacy using metrics. In Proc. PETS, pages 82–102,
2013.
[9] X. Chen, A. Guntuboyina, and Y. Zhang. On Bayes
risk lower bounds. J. Mach. Learn. Res., 17(219):1–58,
2016.
[15] Data
Breaches
Increase
40
Percent
in
Identity
CyberScout.
New
2016, Finds
Theft
http://www.idtheftcenter.org/2016databreaches.html,
2017.
Resource
Report
Center
from
and
[16] B. Ding, J. Kulkarni, and S. Yekhanin. Collecting
telemetry data privately. In Proc. NIPS, pages 3574–
3583, 2017.
[17] S. Doudalis, I. Kotsoginannis, S. Haney, A. Machanava-
jjhala, and S. Mehrotra. One-sided differential privacy.
CoRR, abs/1712.05888, 2017.
[18] D. Dua and E. K. Taniskidou. UCI machine learning
repository. http://archive.ics.uci.edu/ml, 2017.
[19] J. C. Duchi, M. I. Jordan, and M. J. Wainwright. Local
privacy and statistical minimax rates. In Proc. FOCS,
pages 429–438, 2013.
[20] J. C. Duchi, M. I. Jordan, and M. J. Wainwright. Local
privacy, data processing inequalities, and minimax rates.
CoRR, abs/1302.3203, 2013.
[21] C. Dwork, F. Mcsherry, K. Nissim, and A. Smith. Cali-
brating noise to sensitivity in private data analysis. In
Proc. TCC, pages 265–284, 2006.
[22] C. Dwork and A. Roth. The Algorithmic Foundations of
Differential Privacy. Now Publishers, 2014.
[23] U. Erlingsson, V. Pihur, and A. Korolova. RAPPOR:
Randomized aggregatable privacy-preserving ordinal
response. In Proc. CCS, pages 1054–1067, 2014.
[24] G. Fanti, V. Pihur, and U. Erlingsson. Building a RAP-
POR with the unknown: Privacy-preserving learning of
associations and data dictionaries. PoPETs, 2016(3):1–
21, 2016.
[10] E. Cho, S. A. Myers, and J. Leskovec. Friendship and
mobility: User movement in location-based social net-
works. In Proc. KDD, pages 1082–1090, 2011.
[25] P. Golle and K. Partridge. On the anonymity of
home/work location pairs. In Proc. Pervasive, pages
390–397, 2009.
[11] J. E. Cohen. Statistical concepts relevant to AIDS. In
Proc. Symposium on Statistics in Science, Industry, and
Public Policy, pages 43–51, 1989.
[12] G. Cormode, T. Kulkarni, and D. Srivastava. Marginal
release under local differential privacy. In Proc. SIG-
MOD, pages 131–146, 2018.
[13] T. M. Cover and J. A. Thomas. Elements of Information
Theory, Second Edition. Wiley-Interscience, 2006.
[26] T. Hastie, R. Tibshirani, and J. Friedman. The Elements
of Statistical Learning. Spinger, 2nd edition, 2009.
[27] Z. Huang and W. Du. OptRR: Optimizing randomized
response schemes for privacy-preserving data mining.
In Proc. ICDE, pages 705–714, 2008.
[28] Z. Jorgensen, T. Yu, and G. Cormode. Conservative
or liberal? Personalized differential privacy. In Proc.
ICDE, pages 1023–1034, 2015.
[14] P. Cuff and L. Yu. Differential privacy as a mutual
In Proc. CCS, pages 43–54,
information constraint.
2016.
[29] P. Kairouz, K. Bonawitz, and D. Ramage. Discrete
In Proc.
distribution estimation under local privacy.
ICML, pages 2436–2444, 2016.
1890    28th USENIX Security Symposium
USENIX Association
[30] P. Kairouz, S. Oh, and P. Viswanath. Extremal mech-
anisms for local differential privacy. J. Mach. Learn.
Res., 17(1):492–542, 2016.
[31] Y. Kawamoto and T. Murakami. Differentially private
obfuscation mechanisms for hiding probability distribu-
tions. CoRR, abs/1812.00939, 2018.
[32] D. Kifer and A. Machanavajjhala. Pufferﬁsh: A frame-
work for mathematical privacy deﬁnitions. ACM Trans.
Database Syst., 39(1):1–36, 2014.
[33] S. Krishnan, J. Wang, M. J. Franklin, K. Goldberg, and
T. Kraska. PrivateClean: Data cleaning and differential
privacy. In Proc. SIGMOD, pages 937–951, 2016.
[34] R. L. Leahy. Feeling ashamed of being unemployed
- am I afraid of telling people that I am out of work?
https://www.psychologytoday.com/us/blog/anxiety-
ﬁles/201310/feeling-ashamed-being-unemployed,
2013.
[35] M. Lichman. UCI machine learning repository, 2013.
[45] Z. Qin, Y. Yang, T. Yu, I. Khalil, X. Xiao, and K. Ren.
Heavy hitter estimation over set-valued data with local
differential privacy. In Proc. CCS, pages 192–203, 2016.
[46] Y. Sei and A. Ohusuga. Differential private data collec-
tion and analysis based on randomized multiple dum-
mies for untrusted mobile crowdsensing. IEEE Trans.
Inf. Forensics Secur., 12(4):926–939, 2017.
[47] R. Shokri. Privacy games: Optimal user-centric data
obfuscation. PoPETs, 2015(2):299–315, 2015.
[48] S. Song, Y. Wang, and K. Chaudhuri. Pufferﬁsh privacy
In Proc. SIGMOD,
mechanisms for correlated data.
pages 1291–1306, 2017.
[49] A. G. Thakurta, A. H. Vyrros, U. S. Vaishampayan,
G. Kapoor, J. Freudiger, V. R. Sridhar, and D. Davidson.
Learning New Words, US Patent 9,594,741, Mar. 14
2017.
[50] N. Wang, X. Xiao, T. D. Hoang, H. Shin, J. Shin, and
G. Yu. PrivTrie: Effective frequent term discovery under
local differential privacy. In Proc. ICDE, 2018.
[36] C. Liu, S. Chakraborty, and P. Mittal. Dependence
makes you vulnerable: Differential privacy under de-
pendent tuples. In Proc. NDSS, 2016.
[51] T. Wang, J. Blocki, N. Li, and S. Jha. Locally differ-
entially private protocols for frequency estimation. In
Proc. USENIX Security, pages 729–745, 2017.
[37] N. S. Mangat. An improved ranomized response strat-
egy. J. Royal Stat. Soc. Series B (Methodological),
56(1):93–95, 1994.
[38] I. Mironov. Rényi differential privacy. In Proc. CSF,
pages 263–275, 2017.
[39] T. Murakami, H. Hino, and J. Sakuma. Toward distri-
bution estimation under local differential privacy with
small samples. PoPETs, 3:84–104, 2017.
[40] T. Murakami and Y. Kawamoto. Utility-optimized local
differential privacy mechanisms for distribution estima-
tion. CoRR, abs/1807.11317, 2019.
[41] A. Narayanan and V. Shmatikov. Myths and fallacies of
“personally identiﬁable information”. Commun. ACM,
53(6):24–26, 2010.
[42] S. Oya, C. Troncoso, and F. Pérez-González. Back to the
drawing board: Revisiting the design of optimal location
privacy-preserving mechanisms. In Proc. CCS, pages
1959–1972, 2017.
[43] A. Pastore and M. Gastpar. Locally differentially-private
distribution estimation. In Proc. ISIT, pages 2694–2698,
2016.
[52] S. L. Warner. Randomized response: A survey technique
for eliminating evasive answer bias. J. Am. Stat. Assoc.,
60(309):63–69, 1965.
[53] B. Yang, I. Sato, and H. Nakagawa. Bayesian differential
privacy on correlated data. In Proc. SIGMOD, pages
747–762, 2015.
[54] D. Yang, D. Zhang, and B. Qu. Participatory cultural
mapping based on collective behavior data in location
based social network. ACM Trans. Intell. Syst. Technol.,
7(3):30:1–30:23, 2016.
[55] D. Yang, D. Zhang, V. W. Zheng, and Z. Yu. Modeling
user activity preference by leveraging user spatial tem-
poral characteristics in LBSNs. IEEE Trans. Syst., Man,
Cybern., Syst., 45(1):129–142, 2015.
[56] M. Ye and A. Barg. Optimal schemes for discrete dis-
tribution estimation under local differential privacy. In
Proc. ISIT, pages 759–763, 2017.
[57] Y. Zheng, X. Xie, and W.-Y. Ma. GeoLife: A collabora-
tive social networking service among user, location and
trajectory. IEEE Data Eng. Bull., 32(2):32–40, 2010.
A Properties of ULDP
[44] M. Piorkowski, N. Saraﬁjanovic-Djukic, and M. Gross-
glauser. CRAWDAD dataset epﬂ/mobility (v. 2009-02-
24). http://crawdad.org/epﬂ/mobility/20090224, 2009.
In this section, we describe the properties of ULDP (the im-
munity to post-processing and the compatibility with LDP)
in more details.
USENIX Association
28th USENIX Security Symposium    1891
A.1 Post-processing
We ﬁrst deﬁne a class of post-processing randomized algo-
rithms that preserve data types:
Deﬁnition 5 (Preservation of data types). Let YP and ZP be
sets of protected data, and YI and ZI be sets of invertible data.
Given a randomized algorithm Q1 from YP ∪ YI to ZP ∪ ZI,
we say that Q1 preserves data types if it satisﬁes:
• for any z ∈ ZP and any y ∈ YI, Q1(z|y) = 0, and
• for any z ∈ ZI, there exists a y ∈ YI such that Q1(z|y) > 0
and Q1(z|y(cid:48)) = 0 for any y(cid:48) (cid:54)= y.
Then we show that ULDP is immune to the post-processing
by this class of randomized algorithms.
Proposition 11 (Post-processing). Let ε ≥ 0. Let ZP and
ZI be sets of protected and invertible data respectively, and
Z = ZP ∪ ZI. Let Q1 be a randomized algorithm from Y to
Z that preserves data types. If an obfuscation mechanism Q0
from X to Y provides (XS,YP,ε)-ULDP then the composite
function Q1 ◦ Q0 provides (XS,ZP,ε)-ULDP.
For example, ULDP is immune to data cleaning operations
(e.g., transforming values, merging disparate values) [33] as
long as they are represented as Q1 explained above.
Note that Q1 needs to preserve data types for utility (i.e.,
to make all y ∈ YI invertible, as in Deﬁnition 2, after post-
processing), and the DP guarantee for y ∈ YP is preserved by
any post-processing algorithm. Speciﬁcally, by (5), for any
randomized post-processing algorithm Q∗
1, any obfuscated
data z ∈ Z obtained from y ∈ YP via Q∗
1, and any x,x(cid:48) ∈ X ,
we have: Pr(z|x) ≤ eε Pr(z|x(cid:48)).
A.2 Compatibility with LDP
Assume that data collectors A and B adopt a mechanism QA
providing (XS,YP,εA)-ULDP and a mechanism QB providing
εB-LDP, respectively. In this case, all protected data in the
data collector A can be combined with all obfuscated data
in the data collector B (i.e., data integration) to perform data
analysis under LDP. More speciﬁcally, assume that Alice
transforms her sensitive personal data in XS into yA ∈ YP
(resp. yB ∈ Y ) using QA (resp. QB), and sends yA (resp. yB) to
the data collector A (resp. B) to request two different services
(e.g., location check-in for A and point-of-interest search
for B). Then, the composition (QA,QB) in parallel has the
following property:
Proposition 12 (Compatibility with LDP). If QA and QB
respectively provide (XS,YP,εA)-ULDP and εB-LDP, then
for any x,x(cid:48) ∈ X , yA ∈ YP, and yB ∈ Y , we have:
(QA,QB)(yA,yB|x) ≤ eεA+εB(QA,QB)(yA,yB|x(cid:48)).
Proposition 12 implies that Alice’s sensitive personal data
in XS is protected by (εA + εB)-LDP after the data integration.
B Relationship between LDP, ULDP and
OSLDP
In this section, we introduce the notion of OSLDP (One-
sided LDP), a local model version of OSDP (One-sided DP)
proposed in a preprint [17]:
Deﬁnition 6 ((XS,ε)-OSLDP). Given XS ⊆ X and ε ∈ R≥0,
an obfuscation mechanism Q from X to Y provides (XS,ε)-
OSLDP if for any x ∈ XS, any x(cid:48) ∈ X and any y ∈ Y , we have
(23)
Q(y|x) ≤ eεQ(y|x(cid:48)).
OSLDP is a special case of OSDP [17] that takes as input
personal data of a single user. Unlike ULDP, OSLDP allows
the transition probability Q(y|x(cid:48)) from non-sensitive data x(cid:48) ∈
XN to be very large for any y ∈ Y , and hence does not provide
ε-LDP for Y (whereas ULDP provides ε-LDP for YP). Thus,
OSLDP can be regarded as a “relaxation” of ULDP. In fact,
the following proposition holds:
Proposition 13. If an obfuscation mechanism Q provides