where the first term in objective function (3) is the stability loss,
and the second term is the conciseness loss (weighted by ğœ†). The
ğ’™âˆ— âˆˆ [0, 1]ğ‘ ,
Session 12A: Applications and Privacy of ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3201fidelity loss is constrained by (4), which means ğ’™âˆ— is decided to be
normal. Constraint (5) limits ğ’™âˆ— within the valid range. Note that,
ğ’™âˆ— here is after normalization to [0, 1]. Normalizations to other
ranges are also supported, which will be explained later.
Challenges. The above formulation is difficult for solving directly
due to three challenges. Firstly, ğ¿0-norm minimization problem is
proven to be NP-hard [23]. Secondly, constraint (4) is highly non-
linear. Thirdly, constraint (5) is a box constraint which cannot be
directly solved by most algorithms. Below, several techniques are
introduced to address these challenges.
Iteratively Optimizing. To address the first challenge, we trans-
form the ğ¿0-norm term in the objective function into iteratively
optimizing another terms. That is, in each iteration, we identify
some dimensions in ğ’™âˆ— that don not have much effect on mini-
mizing the objective function, then their value will be replaced by
corresponding value in ğ’™â—¦. How to select ineffective dimensions will
be discussed later. In this way, âˆ¥ğ’™âˆ— âˆ’ ğ’™â—¦âˆ¥0 can be effectively limited
through changing a small number of influential dimensions.
Bounding Loss. To address the second challenge, we transform the
highly non-linear constraint (4) by adding a term into the objective
function to minimize Eğ‘…(ğ’™âˆ—, ğ‘“ğ‘…(ğ’™âˆ—)). However, this term should
not be minimized indefinitely since the goal of ğ’™âˆ— is to probe the
decision boundary of ğ‘“ğ‘…. Thus, we limited Eğ‘…(ğ’™âˆ—, ğ‘“ğ‘…(ğ’™âˆ—)) to be
close to ğ‘¡ğ‘… by employing ReLU function (ReLU(ğ‘¥) = max(0, ğ‘¥)).
To ensure ğ’™âˆ— is â€œwithinâ€ the side of normal data with respect to
the decision boundary, ğ‘¡ğ‘… is subtracted by a small ğœ–. Therefore,
constraint (4) is replaced by the following term:
ReLU(cid:0)Eğ‘…(ğ’™âˆ—, ğ‘“ğ‘…(ğ’™âˆ—)) âˆ’ (ğ‘¡ğ‘… âˆ’ ğœ–)(cid:1).
(6)
Variable Changing. We eliminate the box constraint (5) through
the idea of change-of-variables [8] with tanh function. We intro-
duce a new vector ğ’– with the same shape of ğ’™âˆ—. Since the range of
tanh(ğ’–) is (âˆ’1, 1), ğ’™âˆ— can be replaced by simple linear transforma-
2 (tanh (ğ’–) + 1). For ğ’™âˆ— âˆˆ
tions of tanh(ğ’–). For ğ’™âˆ— âˆˆ [0, 1]ğ‘ , ğ’™âˆ— =
1
[ğ‘, ğ‘]ğ‘ (ğ‘, ğ‘ is arbitrary as long as ğ‘  ğ‘¡ğ‘ƒ. The validity constraint (5) turns
into constraining each ğ’™âˆ—
ğ‘– in Xâˆ— is still one-hot (ğ‘– âˆˆ {1, 2, ..., ğ‘¡}). We
again use the idea of iteratively optimizing and bounding loss to
solve similar challenges as tabular data. Thus, we have the objective
function in each iteration denoted with Dğ‘¡ğ‘ (Xâˆ—; ğ’™âˆ—
ğ‘¡ ) as follows:
Dğ‘¡ğ‘ (Xâˆ—; ğ’™âˆ—
ğ‘¡ ) = ReLU(cid:0)(ğ‘¡ğ‘ƒ + ğœ–) âˆ’ ğ‘“ğ‘ƒ (Xâˆ—)(cid:1)
= ReLU(cid:0)(ğ‘¡ğ‘ƒ + ğœ–) âˆ’ Pr(ğ’™âˆ—
ğ‘¡âˆ’1)(cid:1).
1ğ’™âˆ—
ğ‘¡ |ğ’™âˆ—
2...ğ’™âˆ—
(8)
Locating Anomaly. Before solving Dğ‘¡ğ‘ (Xâˆ—), there is a special
challenge for time-series interpretation. That is, we need to figure
out whether an anomaly Xâ—¦ is caused by ğ’™â—¦
ğ‘¡âˆ’1. This is
because xğ‘¡ serves as the â€œlabelâ€ of x1x2...xğ‘¡âˆ’1 (recall the prediction-
based learning introduced in Â§2.2). If the label itself is abnormal,
modifying other parts will be useless. If the anomaly is caused by
ğ’™â—¦
ğ‘¡ , then we can simply replace ğ’™â—¦
ğ‘¡ to turn
the time-series into normal:
ğ‘¡ with the following ğ’™âˆ—
ğ‘¡ or ğ’™â—¦
2...ğ’™â—¦
1ğ’™â—¦
ğ’™âˆ—
ğ‘¡ = ğ’™
ğ’™ğ‘ Pr(ğ’™
ğ‘|ğ’™â—¦
1ğ’™â—¦
2...ğ’™â—¦
ğ‘¡âˆ’1).
ğ‘ = argmax
(9)
Such ğ’™âˆ—
ğ‘¡ can be obtained directly by observing the index of the
maximum probability in the last layer (usually softmax) of the
ğ‘¡ is replaced, the whole process of solving Xâˆ—
RNN/LSTM. Once ğ’™â—¦
ends immediately. One the other hand, if the anomaly is not caused
by ğ’™â—¦
ğ‘¡ through (8). Formally,
(10)
ğ‘¡âˆ’1 =(cid:0) argmaxXâˆ— Dğ‘¡ğ‘ (Xâˆ—; ğ’™â—¦
ğ‘¡ , we need to solve ğ’™âˆ—
1ğ’™âˆ—
2...ğ’™âˆ—
ğ‘¡ )(cid:1)1,2,...,ğ‘¡âˆ’1.
ğ‘¡âˆ’1 with ğ’™â—¦
1ğ’™âˆ—
ğ’™âˆ—
2...ğ’™âˆ—
To locate anomaly, we introduce Saliency Testing. The in-
tuition is that, if (1) it is hard to make Xâ—¦ normal by changing
ğ’™â—¦
1ğ’™â—¦
2...ğ’™â—¦
ğ‘¡âˆ’1, and (2) RNN/LSTM originally has great confidence of
ğ’™ğ‘ (ğ’™ğ‘ â‰  ğ’™â—¦
ğ‘¡ ), then we decide the anomaly is caused by ğ’™â—¦
ğ‘¡ . Formally,
saliency testing denoted with ğ‘†ğ‘‡ (Xâ—¦; ğœ‡1, ğœ‡2) is conducted by:
ğ‘†ğ‘‡ (Xâ—¦; ğœ‡1, ğœ‡2) = (max(âˆ‡Dğ‘¡ğ‘ (Xâ—¦; ğ’™â—¦
ğ‘ > ğœ‡2),
which respectively represent the above two conditions. The detailed
configuration method of ğœ‡1 and ğœ‡2 is in Appendix F. To conclude,
ğ‘¡ )) < ğœ‡1) âˆ§ (ğ’™
(11)
(cid:40) ğ’™â—¦
time-series reference Xâˆ— is solved as:
ğ‘¡âˆ’1ğ’™âˆ—
ğ‘¡ â†” Eq.(9),
Xâˆ— =
ğ‘¡âˆ’1 â†” Eq.(10)ğ’™â—¦