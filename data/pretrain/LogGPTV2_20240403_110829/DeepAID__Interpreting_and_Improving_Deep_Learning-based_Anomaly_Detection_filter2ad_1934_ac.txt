where the first term in objective function (3) is the stability loss,
and the second term is the conciseness loss (weighted by 𝜆). The
𝒙∗ ∈ [0, 1]𝑁 ,
Session 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3201fidelity loss is constrained by (4), which means 𝒙∗ is decided to be
normal. Constraint (5) limits 𝒙∗ within the valid range. Note that,
𝒙∗ here is after normalization to [0, 1]. Normalizations to other
ranges are also supported, which will be explained later.
Challenges. The above formulation is difficult for solving directly
due to three challenges. Firstly, 𝐿0-norm minimization problem is
proven to be NP-hard [23]. Secondly, constraint (4) is highly non-
linear. Thirdly, constraint (5) is a box constraint which cannot be
directly solved by most algorithms. Below, several techniques are
introduced to address these challenges.
Iteratively Optimizing. To address the first challenge, we trans-
form the 𝐿0-norm term in the objective function into iteratively
optimizing another terms. That is, in each iteration, we identify
some dimensions in 𝒙∗ that don not have much effect on mini-
mizing the objective function, then their value will be replaced by
corresponding value in 𝒙◦. How to select ineffective dimensions will
be discussed later. In this way, ∥𝒙∗ − 𝒙◦∥0 can be effectively limited
through changing a small number of influential dimensions.
Bounding Loss. To address the second challenge, we transform the
highly non-linear constraint (4) by adding a term into the objective
function to minimize E𝑅(𝒙∗, 𝑓𝑅(𝒙∗)). However, this term should
not be minimized indefinitely since the goal of 𝒙∗ is to probe the
decision boundary of 𝑓𝑅. Thus, we limited E𝑅(𝒙∗, 𝑓𝑅(𝒙∗)) to be
close to 𝑡𝑅 by employing ReLU function (ReLU(𝑥) = max(0, 𝑥)).
To ensure 𝒙∗ is “within” the side of normal data with respect to
the decision boundary, 𝑡𝑅 is subtracted by a small 𝜖. Therefore,
constraint (4) is replaced by the following term:
ReLU(cid:0)E𝑅(𝒙∗, 𝑓𝑅(𝒙∗)) − (𝑡𝑅 − 𝜖)(cid:1).
(6)
Variable Changing. We eliminate the box constraint (5) through
the idea of change-of-variables [8] with tanh function. We intro-
duce a new vector 𝒖 with the same shape of 𝒙∗. Since the range of
tanh(𝒖) is (−1, 1), 𝒙∗ can be replaced by simple linear transforma-
2 (tanh (𝒖) + 1). For 𝒙∗ ∈
tions of tanh(𝒖). For 𝒙∗ ∈ [0, 1]𝑁 , 𝒙∗ =
1
[𝑎, 𝑏]𝑁 (𝑎, 𝑏 is arbitrary as long as 𝑎  𝑡𝑃. The validity constraint (5) turns
into constraining each 𝒙∗
𝑖 in X∗ is still one-hot (𝑖 ∈ {1, 2, ..., 𝑡}). We
again use the idea of iteratively optimizing and bounding loss to
solve similar challenges as tabular data. Thus, we have the objective
function in each iteration denoted with D𝑡𝑠(X∗; 𝒙∗
𝑡 ) as follows:
D𝑡𝑠(X∗; 𝒙∗
𝑡 ) = ReLU(cid:0)(𝑡𝑃 + 𝜖) − 𝑓𝑃 (X∗)(cid:1)
= ReLU(cid:0)(𝑡𝑃 + 𝜖) − Pr(𝒙∗
𝑡−1)(cid:1).
1𝒙∗
𝑡 |𝒙∗
2...𝒙∗
(8)
Locating Anomaly. Before solving D𝑡𝑠(X∗), there is a special
challenge for time-series interpretation. That is, we need to figure
out whether an anomaly X◦ is caused by 𝒙◦
𝑡−1. This is
because x𝑡 serves as the “label” of x1x2...x𝑡−1 (recall the prediction-
based learning introduced in §2.2). If the label itself is abnormal,
modifying other parts will be useless. If the anomaly is caused by
𝒙◦
𝑡 , then we can simply replace 𝒙◦
𝑡 to turn
the time-series into normal:
𝑡 with the following 𝒙∗
𝑡 or 𝒙◦
2...𝒙◦
1𝒙◦
𝒙∗
𝑡 = 𝒙
𝒙𝑐 Pr(𝒙
𝑐|𝒙◦
1𝒙◦
2...𝒙◦
𝑡−1).
𝑐 = argmax
(9)
Such 𝒙∗
𝑡 can be obtained directly by observing the index of the
maximum probability in the last layer (usually softmax) of the
𝑡 is replaced, the whole process of solving X∗
RNN/LSTM. Once 𝒙◦
ends immediately. One the other hand, if the anomaly is not caused
by 𝒙◦
𝑡 through (8). Formally,
(10)
𝑡−1 =(cid:0) argmaxX∗ D𝑡𝑠(X∗; 𝒙◦
𝑡 , we need to solve 𝒙∗
1𝒙∗
2...𝒙∗
𝑡 )(cid:1)1,2,...,𝑡−1.
𝑡−1 with 𝒙◦
1𝒙∗
𝒙∗
2...𝒙∗
To locate anomaly, we introduce Saliency Testing. The in-
tuition is that, if (1) it is hard to make X◦ normal by changing
𝒙◦
1𝒙◦
2...𝒙◦
𝑡−1, and (2) RNN/LSTM originally has great confidence of
𝒙𝑐 (𝒙𝑐 ≠ 𝒙◦
𝑡 ), then we decide the anomaly is caused by 𝒙◦
𝑡 . Formally,
saliency testing denoted with 𝑆𝑇 (X◦; 𝜇1, 𝜇2) is conducted by:
𝑆𝑇 (X◦; 𝜇1, 𝜇2) = (max(∇D𝑡𝑠(X◦; 𝒙◦
𝑐 > 𝜇2),
which respectively represent the above two conditions. The detailed
configuration method of 𝜇1 and 𝜇2 is in Appendix F. To conclude,
𝑡 )) < 𝜇1) ∧ (𝒙
(11)
(cid:40) 𝒙◦
time-series reference X∗ is solved as:
𝑡−1𝒙∗
𝑡 ↔ Eq.(9),
X∗ =
𝑡−1 ↔ Eq.(10)𝒙◦