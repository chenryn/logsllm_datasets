title:POSTER: Accuracy vs. Time Cost: Detecting Android Malware through
Pareto Ensemble Pruning
author:Lingling Fan and
Minhui Xue and
Sen Chen and
Lihua Xu and
Haojin Zhu
POSTER: Accuracy vs. Time Cost: Detecting Android
Malware through Pareto Ensemble Pruning
Lingling Fan†, Minhui Xue†‡, Sen Chen†, Lihua Xu†, Haojin Zhu(cid:2)
†East China Normal University, Shanghai, China
‡NYU Shanghai, Shanghai, China
(cid:2)Shanghai Jiao Tong University, Shanghai, China
ABSTRACT
This paper proposes Begonia, a malware detection system through
Pareto ensemble pruning. We convert the malware detection prob-
lem into the bi-objective Pareto optimization, aiming to trade off
the classiﬁcation accuracy and the size of classiﬁers as two objec-
tives. We automatically generate several groups of base classiﬁers
using SVM and generate solutions through bi-objective Pareto op-
timization. We then select the ensembles with highest accuracy of
each group to form the ﬁnal solutions, among which we hit the
optimal solution where the combined loss function is minimal con-
sidering the trade-off between accuracy and time cost. We expect
users to provide different trade-off levels to their different require-
ments to select the best solution. Experimental results show that
Begonia can achieve higher accuracy with relatively lower over-
head compared to the ensemble containing all the classiﬁers and
can make a good trade-off to different requirements.
Keywords
Malware Detection; Ensemble Pruning; Pareto Bi-objective Opti-
mization; Begonia
1.
INTRODUCTION
Mobile devices have become the potential target of attackers due
to the massive downloads of applications in recent years. Mali-
cious applications that illegally obtain private information or per-
form harmful actions to the devices pose a severe threat in our daily
life. Recent approaches tried to alleviate this problem to achieve
high detection accuracy by applying machine learning. For ex-
ample, DREBIN [1] extracted thousands of features for machine
learning and achieved high accuracy in malware detection. Smutz
et al. [5] applied ensemble learning to malware detection, which
improved the true positive rate by detecting poor classiﬁers and
providing a conﬁdence in the prediction of ensemble classiﬁers to
indicate that the classiﬁer is not ﬁt to provide an accuracy respond.
Ensemble learning is also applied to hardware-supported malware
detection [3]. However, these approaches only focus on the detec-
tion accuracy, but neglect the computational cost.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CCS’16 October 24-28, 2016, Vienna, Austria
c(cid:2) 2016 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-4139-4/16/10.
DOI: http://dx.doi.org/10.1145/2976749.2989055
In this paper, we propose a malware detection system through
Pareto ensemble pruning to trade off the classiﬁcation accuracy and
the computational cost. Pareto ensemble pruning [4] formulates
this classiﬁcation problem as a bi-objective optimization problem,
with accuracy and computational cost as the two objectives. We
take the vote of the predictions to automatically generate pruned
ensembles. Since the prediction time of the ensembles is in the unit
of millisecond level, we further evaluate the performance with re-
spect to the training time of different groups (different sizes of base
learner pools) and trade off the accuracy and the time of ensem-
ble pruning process. Furthermore, we select the ensemble of the
highest accuracy in each group to form the Pareto solutions, among
which users are required to provide a trade-off level (i.e., weight) to
obtain the best required solution where the combined loss function
is minimal. Experimental results show that our detection system,
termed Begonia, can achieve relatively higher accuracy with rela-
tively fewer learners compared to the ensemble containing all the
base classiﬁers.
In summary, we make the following contributions:
• We propose a customized trade-off between accuracy of mal-
• We provide an automatic Pareto ensemble pruning frame-
ware classiﬁcation and computational cost.
work for malware detection.
2. BEGONIA ARCHITECTURE
The high level execution process of our approach, as shown in
Figure 1, occurs in four phases: (i) Reverse engineering, which
prepares resource ﬁles for extracting features by decompiling the
APKs; (ii) Feature Extraction, which extracts features from each
application using both static and dynamic analysis; (iii) Ensem-
ble pruning, which trains large sets of labeled Android applica-
tions to obtain several groups of base classiﬁers containing n clas-
siﬁers (n = 10, 20, . . .) and selects the base classiﬁers guided by
bi-objective optimization, to trade off accuracy and computational
cost; and (iv) Classiﬁcation, which classiﬁes the dataset into differ-
ent categories, benign and malicious, based on the optimal pruned
ensembles.
We provide an ensemble selection approach based on Pareto op-
timal to trade off accuracy and computational cost. Since the pre-
diction time of the ensembles is so small compared to the pruning
time, we only evaluate the performance with respect to the pruning
time, and trade off the accuracy and the time of ensemble prun-
ing process. We train different numbers of base learners for each
group using SVM, and select the ensemble with highest accuracy
via Pareto ensemble pruning. Obviously, the more base learners it
trains, the more time it will take to prune the ensemble with highest
accuracy for each group. The selected ensembles are then provided
to the decision makers to select an optimal solution considering
1748DroidBox
LOGS
Decompile
·XML
·SMALI
n=10
Aggregator
Bi-objective guided










Accuracy
Cost
Extractor
n=50
(cid:258)(cid:258)
Bi-objective guided
Accuracy
Cost














(cid:258)(cid:258)
Accuracy
Cost
Figure 1: Overview of Begonia
other requirements and information. Compared to one-objective
optimization that usually has only one optimal solution, Pareto op-
timal has multiple optimal solutions, and all the solutions have re-
deeming features. They cannot be simply excluded. Pareto optimal
is introduced to explain the dominant relation between each solu-
tion. The Pareto domination relation is then formalized as follows.
DEFINITION 1. (Pareto Domination)
2 denote the objective vector mapping
2. For two solutions u, v ∈ S, u
Let h = (h1, h2) :S → R
from the solution space S to R
dominates v iff it meets the following conditions:
(1) h1 (u) ≤ h1 (v) and h2 (u) ≤ h2 (v)
(2) h1 (u) < h1 (v) or h2 (u) < h2 (v)
(For simplicity, vectors will not be denoted by boldface characters
in this paper.)
A solution is Pareto optimal if there does not exist a solution
that can be better without sacriﬁcing some of the other objective
values. More speciﬁcally, a solution u is Pareto optimal if there is
no other solution in S that dominates u. The solutions are provided
to the decision makers to select an optimal solution considering
other requirements and information.
Ensemble pruning, also known as ensemble selection, selects a
subset of classiﬁers from base classiﬁers set and classiﬁes a new
dataset by taking the vote of their predictions. It tries to achieve the
goal that the accuracy of the combining prediction results improves
and in parallel the computational overhead reduces compared to the
ensemble containing all the classiﬁers. Consider the number of the
base learners in Bm = {b1, . . . , bm} is m, and let Tt denote a
pruned classiﬁer set with the selected vector t ∈ {0, 1}m, where
ti = 1 indicates the base learner bi is selected for the ith compo-
nent. The optimal pruned ensemble Topt.sel can be formulated as
follows:
Topt.sel = arg min
t∈{0,1}m
E (Tt) +w · |Tt| ,
where E (Tt) is the validation error rate of Tt. Since the mea-
surement of generalization classiﬁcation performance is hard to
declare, we use the validation error on the validation date set in-
stead. Given a validation dataset with k instances, for validation
instance i, Tt (xi) is the prediction value of Tt, and yi is the actual
value. E (Tt) is calculated as
k(cid:2)
χ (Tt (xi) (cid:5)= yi) ,
E (Tt) =
1
k
i=1
where χ (·) is the indicator function, which equals 1 if the ex-
pression holds; otherwise, it equals 0, |Tt| =
i=1 ti is the size
(cid:3)m
of the selected learners, w ∈ [0, +∞] is the trade-off level, and
E (Tt) +w ·|Tt| is the combined loss function aiming to obtain the
solutions that minimize the combined loss to achieve good perfor-
mance.
Consider there are several trade-off levels to be solved, the op-
(i)
opt.sel can be deﬁned based on different
timal pruned ensemble T
trade-off levels wi, for all i:
T
(i)
opt.sel = arg min
t∈{0,1}m
E (Tt) +w i · |Tt| .
(cid:2)
T
(1)
opt.sel, T
(2)
opt.sel, . . . , T
ALGORITHM 1: Customized Ensemble Pruning
Input: The training set N1, the validation set N2, and Trade-off levels
W = {w1, w2, . . . , wl}.
(cid:3)
Output: Pruned ensembles T =
1: Let f (Tt) = (E (Tt) , |Tt|) denote the bi-objective function
2: B10 = g(N1), B20 = g(N1), . . . , Bm = g(N1)
3: I = {bi-objective-solver (f (Tt))}
4: P = {p1, p2, . . . , pq}
5: for j = 1 to l do
6:
7: end for