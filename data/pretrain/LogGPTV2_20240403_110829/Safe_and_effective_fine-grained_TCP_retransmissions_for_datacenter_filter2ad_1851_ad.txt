and throughput for the 2 servers, we ﬁrst investigate if they
see similar ﬂows with respect to RTT values. Figure 11 shows
the per-ﬂow average RTT distribution for both hosts over
the three day measurement period. The RTT distributions
are nearly identical, suggesting that each machine saw a
similar distribution of both short- and long-RTT ﬂows. The
per-packet RTT distribution for both ﬂows is also identical.
Figure 12 shows the per-ﬂow throughput distributions for
both hosts, ﬁltering out ﬂows with throughput less than
 0 20 40 60 80 100 1 10 100 1000 10000 100000% samples (with RTT <= x)RTT (ms)200ms RTOmin (Default)200us RTOmin1ACK 1ACK 111 0 20 40 60 80 100 1 10 100% samples (with Kbps <= x)Throughout (Kbps)200ms RTOmin (Default)200us RTOmin311Figure 13: The throughput distribution for short
and long RTT ﬂows shows negligible diﬀerence
across conﬁgurations.
Figure 14: With RT Omin eliminated, disabling de-
layed ACK on client nodes provides optimal good-
put in a 16-node cluster.
100bps, which are typically ﬂows sending small control pack-
ets. The throughput distributions are also nearly identical—
the host with RTOmin = 200µs did not perform worse on
the whole than the host with RT Omin = 200ms.
We split the throughput distributions based on whether the
ﬂow’s RTT was above or below 200ms. For ﬂows above 200ms,
we use the variance in the two distributions as a control
parameter: any variance seen above 200ms are a result of
measurement noise, because the RTOmin is no longer a factor.
Figure 13 shows that the diﬀerence between the distribution
for ﬂows below 200ms is within this measurement noise.
This data suggests that reducing the RTOmin to 200µs in
practice does not aﬀect the performance of bulk-data TCP
ﬂows on the wide-area.
6.3 Interaction with Delayed ACK
For servers using a reduced RTO in a datacenter envi-
ronment, the server’s retransmission timer may expire long
before an unmodiﬁed client’s 40ms delayed ACK timer ﬁres.
As a result, the server will timeout and resend the unacked
packet, cutting ssthresh in half and rediscovering link ca-
pacity using slow-start. Because the client acknowledges
the retransmitted segment immediately, the server does not
observe a coarse-grained 40ms delay, only an unnecessary
timeout.
Once the ﬁrst-order throughput collapse has been averted
by having the sender use a microsecond-granularity RTO,
Figure 14 shows the performance diﬀerence between a client
with delayed ACK disabled, delayed ACK enabled with a
200µs timer, and the default 40ms delayed ACK conﬁgura-
tion.
Beyond 8 servers, a client with a 200µs delayed ACK
timer receives 15–30Mbps lower throughput compared to a
client with delayed ACK disabled entirely, whereas the 40ms
delayed ACK client experiences between 100 and 200Mbps
lower throughput caused by frequent timeouts. The 200µs
delayed ACK timeout client delays a server by roughly a
round-trip-time and does not force the server to timeout, so
the performance hit is much smaller.
Delayed ACK can provide beneﬁts where the ACK path is
congested [4], but in the datacenter environment, we believe
that coarse-grained delayed ACKs should be avoided when
possible; most high-performance applications in the datacen-
ter favor quick response over additional ACK-processing over-
head and are typically equally provisioned for both directions
of traﬃc. Our evaluations in Section 5 disable delayed ACK
on the client for this reason. While these results show that
for full performance, delayed ACK should be disabled, we
note that unmodiﬁed clients still achieve good performance
and avoid incast collapse when the servers only implement
ﬁne-grained retransmissions.
7. RELATED WORK
TCP Improvements: A number of changes over the
years have improved TCP’s ability to respond to loss pat-
terns and perform better in particular environments, many
of which are relevant to the high-performance datacenter
environment we study. NewReno and SACK, for instance,
reduce the number of loss patterns that will cause time-
outs; prior work on the TCP incast problem showed that
NewReno, in particular, improved throughput during moder-
ate amounts of incast traﬃc, though not when the problem
became severe [28].
TCP mechanisms such as Limited Transmit [1] were specif-
ically designed to help TCP recover from packet loss when
window sizes are small—exactly the problem that occurs
during incast collapse. This solution again helps maintain
throughput under modest congestion, but during severe in-
cast collapse, the most common loss pattern is the loss of
the entire window.
Finally, proposed improvements to TCP such as TCP
Vegas [8] and FAST TCP [19] can limit window growth when
RTTs begin to increase, often combined with more aggressive
window growth algorithms to rapidly ﬁll high bandwidth-
delay links. Unlike the self-interfering oscillatory behavior
on high-BDP links that this prior work seeks to resolve,
incast collapse is triggered by the arrival and rapid ramp-
up of numerous competing ﬂows, and the RTT increases
drastically (or becomes a full window loss) over a single
round-trip. While an RTT-based solution is an interesting
approach to study, it is a matter of considerable future work
to adapt existing techniques for this purpose.
 0 20 40 60 80 100 1 10 100% samples (with Kbps <= x)Throughout (Kbps)200ms RTOmin (over 200ms RTT)200ms RTOmin (sub 200ms RTT)200us RTOmin (over 200ms RTT)200us RTOmin (sub 200ms RTT) 0 100 200 300 400 500 600 700 800 900 1000 0 2 4 6 8 10 12 14 16Goodput (Mbps)Number of ServersNum Servers vs Goodput (DelayedACK Client) (Fixed Block = 1MB, buffer = 32KB (est.), Switch = Procurve)Delayed ACK DisabledDelayed ACK 200us                   Delayed ACK 40ms312Eﬃcient, ﬁne-grained kernel timers. Where our work
depends on hardware support for high-resolution kernel
timers, earlier work on “soft timers” shows an implemen-
tation path for legacy systems [3]. Soft timers can provide
microsecond-resolution timers for networking without intro-
ducing the overhead of context switches and interrupts. The
hrtimer implementation we make use of draws lessons from
soft timers, using a hardware interrupt to trigger all available
software interrupts.
Understanding RTOmin. The origin of concern about
the safety and generality of reducing RTOmin was presented
by Allman and Paxson [2], where they used trace-based anal-
ysis to show that there existed no optimal RTO estimator,
and to what degree that the TCP granularity and RTOmin
had an impact on spurious retransmissions. Their analysis
showed that a low or non-existent RTOmin greatly increased
the chance of spurious retransmissions and that tweaking
the RTOmin had no obvious sweet-spot for balancing fast
response with spurious timeouts. They showed the increased
beneﬁt of having a ﬁne measurement granularity for respond-
ing to good timeouts because of the ability to respond to
minor changes in RTT. Last, they suggested that the im-
pact of bad timeouts could be mitigated by using the TCP
timestamp option, which later became known as the Eifel
algorithm [21]. F-RTO later showed how to detect spurious
timeouts by detecting whether the following acknowledge-
ments were for segments not retransmitted [32], and this
algorithm is implemented in Linux TCP today.
Psaras and Tsaoussidis revisit the minimum RTO for high-
speed, last-mile wireless links, noting the default RTOmin
is responsible for worse throughput on wireless links and
short ﬂows [29]. They suggest a mechanism for dealing with
delayed ACKs that attempts to predict when a packet’s
ACK is delayed—a per-packet RTOmin. We ﬁnd that while
delayed ACK can aﬀect performance for low RTOmin, the
beneﬁts of a low RTOmin far outweigh the impact of delayed
ACK on performance.
Concurrent work is studying the possible eﬀects of TCP
incast collapse in other datacenter workloads [9], such as
in MapReduce [11], independently conﬁrming that faster
TCP retransmissions can help improve goodput for these
alternative workloads.
8. CONCLUSION
This paper presented a practical, eﬀective, and safe solution
to eliminate TCP incast collapse in datacenter environments.
Enabling microsecond-granularity TCP timeouts allowed
high-fan-in, barrier synchronized datacenter communication
to scale to 47 nodes in a real cluster evaluation, and random-
ized retransmissions were used to scale to thousands of nodes
in simulation. This implementation of ﬁne-grained TCP
retransmissions should also help latency-sensitive datacenter
applications where timeouts lasting hundreds of milliseconds
can harm response time. Through a wide-area evaluation,
we showed that these modiﬁcations remain safe for use in
the wide-area, providing a general and eﬀective improvement
for TCP-based cluster communication.
Acknowledgments
We would like to thank our shepherd Dave Maltz, Dilip
Chhetri, Vyas Sekar, Srinivasan Seshan, and the anonymous
reviewers for their comments and suggestions. We also thank
Andrew Shewmaker, HB Chen, Parks Fields, Gary Grider,
Ben McClelland, and James Nunez at Los Alamos National
Laboratory for help with obtaining packet header traces.
We thank the members and companies of the PDL Consor-
tium (including APC, DataDomain, EMC, Facebook, Google,
Hewlett-Packard, Hitachi, IBM, Intel, LSI, Microsoft, NEC,
NetApp, Oracle, Seagate, Sun, Symantec, and VMware) for
their interest, insights, feedback, and support. We thank
Intel and NetApp for hardware donations that enabled this
work.
This material is based upon research supported in part by
the National Science Foundation via grants CNS-0546551,
CNS-0619525, CNS-0326453, and CCF-0621499, by the Army
Research Oﬃce, under agreement number DAAD19-02-1-
0389, by the Department of Energy, under Award Number
DE-FC02-06ER25767, and by Los Alamos National Labora-
tory, under contract number 54515-001-07.
9. REFERENCES
[1] M. Allman, H. Balakrishnan, and S. Floyd. Enhancing
TCP’s Loss Recovery Using Limited Transmit. Internet
Engineering Task Force, Jan. 2001. RFC 3042.
[2] M. Allman and V. Paxson. On estimating end-to-end
network path properties. In Proc. ACM SIGCOMM,
Cambridge, MA, Sept. 1999.
[3] M. Aron and P. Druschel. Soft timers: Eﬃcient
microsecond software timer support for network
processing. ACM Transactions on Computer Systems,
18(3):197–228, 2000.
[4] H. Balakrishnan, V. N. Padmanabhan, and R. Katz.
The eﬀects of asymmetry on TCP performance. In Proc.
ACM MOBICOM, Budapest, Hungary, Sept. 1997.
[5] H. Balakrishnan, V. N. Padmanabhan, S. Seshan, and
R. Katz. A comparison of mechanisms for improving
TCP performance over wireless links. In Proc. ACM
SIGCOMM, Stanford, CA, Aug. 1996.
[6] P. J. Braam. File systems for clusters from a protocol
perspective. http://www.lustre.org.
[7] R. T. Braden. Requirements for Internet
Hosts—Communication Layers. Internet Engineering
Task Force, Oct. 1989. RFC 1122.
[8] L. S. Brakmo, S. W. O’Malley, and L. L. Peterson.
TCP vegas: New techniques for congestion detection
and avoidance. In Proc. ACM SIGCOMM, London,
England, Aug. 1994.
[9] Y. Chen, R. Griﬃth, J. Liu, A. D. Joseph, and R. H.
Katz. Understanding TCP incast throughput collapse
in datacenter networks. In Proc. Workshop: Research
on Enterprise Networking, Barcelona, Spain, Aug. 2009.
[10] k. claﬀy, G. Polyzos, and H.-W. Braun. Measurement
considerations for assessing unidirectional latencies.
Internetworking: Research and Experience,
3(4):121–132, Sept. 1993.
[11] J. Dean and S. Ghemawat. MapReduce: Simpliﬁed
data processing on large clusters. In Proc. 6th USENIX
OSDI, San Francisco, CA, Dec. 2004.
[12] Scaling memcached at Facebook. http://www.
facebook.com/note.php?note_id=39391378919.
[13] S. Floyd and V. Jacobson. Random early detection
gateways for congestion avoidance. IEEE/ACM
Transactions on Networking, 1(4), Aug. 1993.
313[14] B. Ford. Structured streams: A new transport
[26] ns-2 Network Simulator.
abstraction. In Proc. ACM SIGCOMM, Kyoto, Japan,
Aug. 2007.
http://www.isi.edu/nsnam/ns/, 2000.
[27] C. Partridge. Gigabit Networking. Addison-Wesley,
[15] S. Ghemawat, H. Gobioﬀ, and S.-T. Leung. The Google
Reading, MA, 1994.
ﬁle system. In Proc. 19th ACM Symposium on
Operating Systems Principles (SOSP), Lake George,
NY, Oct. 2003.
[16] High-resolution timer subsystem.
http://www.tglx.de/hrtimers.html.
[17] V. Jacobson. Congestion avoidance and control. In
Proc. ACM SIGCOMM, pages 314–329, Vancouver,
British Columbia, Canada, Sept. 1998.
[28] A. Phanishayee, E. Krevat, V. Vasudevan, D. G.
Andersen, G. R. Ganger, G. A. Gibson, and S. Seshan.
Measurement and analysis of TCP throughput collapse
in cluster-based storage systems. In Proc. USENIX
Conference on File and Storage Technologies, San Jose,
CA, Feb. 2008.
[29] I. Psaras and V. Tsaoussidis. The TCP minimum RTO
revisited. In IFIP Networking, May 2007.
[18] V. Jacobson, R. Braden, and D. Borman. TCP
[30] K. Ramakrishnan and S. Floyd. A Proposal to Add
Extensions for High Performance. Internet Engineering
Task Force, May 1992. RFC 1323.
Explicit Congestion Notiﬁcation (ECN) to IP. Internet
Engineering Task Force, Jan. 1999. RFC 2481.
[19] C. Jin, D. X. Wei, and S. H. Low. FAST TCP:
motivation, architecture, algorithms, performance.
[20] E. Kohler, M. Handley, and S. Floyd. Designing DCCP:
Congestion control without reliability. In Proc. ACM
SIGCOMM, Pisa, Italy, Aug. 2006.
[21] R. Ludwig and M. Meyer. The Eifel Detection
Algorithm for TCP. Internet Engineering Task Force,
Apr. 2003. RFC 3522.
[22] M. Mathis, J. Mahdavi, S. Floyd, and A. Romanow.
TCP Selective Acknowledgment Options. Internet
Engineering Task Force, 1996. RFC 2018.
[23] A distributed memory object caching system.
http://www.danga.com/memcached/.
[24] A. Mukherjee. On the dynamics and signiﬁcance of low
frequency components of Internet load. Internetworking:
Research and Experience, 5:163–205, Dec. 1994.
[25] D. Nagle, D. Serenyi, and A. Matthews. The Panasas
ActiveScale Storage Cluster: Delivering scalable high
bandwidth storage. In SC ’04: Proceedings of the 2004
ACM/IEEE Conference on Supercomputing,
Washington, DC, USA, 2004.
[31] S. Raman, H. Balakrishnan, and M. Srinivasan. An
image transport protocol for the Internet. In Proc.
International Conference on Network Protocols, Osaka,
Japan, Nov. 2000.
[32] P. Sarolahti and M. Kojo. Forward RTO-Recovery
(F-RTO): An Algorithm for Detecting Spurious
Retransmission Timeouts with TCP and the Stream
Control Transmission Protocol (SCTP). Internet
Engineering Task Force, Aug. 2005. RFC 4138.
[33] S. Shepler, M. Eisler, and D. Noveck. NFSv4 Minor
Version 1 – Draft Standard.
http://www.ietf.org/internet-drafts/
draft-ietf-nfsv4-minorversion1-29.txt.
[34] B. Welch, M. Unangst, Z. Abbasi, G. Gibson,
B. Mueller, J. Zelenka, and B. Zhou. Scalable
performance of the Panasas parallel ﬁle system. In
Proc. USENIX Conference on File and Storage
Technologies, San Jose, CA, Feb. 2008.
[35] Y. Zhang, N. Duﬃeld, V. Paxson, and S. Shenker. On
the constancy of Internet path properties. In Proc.
ACM SIGCOMM Internet Measurement Workshop,
San Fransisco, CA, Nov. 2001.
314