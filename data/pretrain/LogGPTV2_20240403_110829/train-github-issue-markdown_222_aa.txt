Apache Druid 25.0.0 contains over 300 new features, bug fixes, performance
enhancements, documentation improvements, and additional test coverage from 51
contributors.
See the complete set of changes for additional details.
# # Highlights
## # MSQ task engine now production ready
The multi-stage query (MSQ) task engine used for SQL-based ingestion is now
production ready. Use it for any supported workloads. For more information,
see the following pages:
  * Ingestion
  * SQL-based ingestion
## # Simplified Druid deployments
The new `start-druid` script greatly simplifies deploying any combination of
Druid services on a single-server. It comes pre-packaged with the required
configs and can be used to launch a fully functional Druid cluster simply by
invoking `./start-druid`. For experienced Druids, it also gives complete
control over the runtime properties and JVM arguments to have a cluster that
exactly fits your needs.
The `start-druid` script deprecates the existing profiles such as `start-
micro-quickstart` and `start-nano-quickstart`. These profiles may be removed
in future releases. For more information, see Single server deployment.
## # String dictionary compression (experimental)
Added support for front coded string dictionaries for smaller string columns,
leading to reduced segment sizes with only minor performance penalties for
most Druid queries.
This can be enabled by setting `IndexSpec.stringDictionaryEncoding` to
`{"type":"frontCoded", "bucketSize": 4}` , where `bucketSize` is any power of
2 less than or equal to 128. Setting this property instructs indexing tasks to
write segments using compressed dictionaries of the specified bucket size.
> Any segment written using string dictionary compression is not readable by
> older versions of Druid.
For more information, see Front coding.
#12277
## # Kubernetes-native tasks
Druid can now use Kubernetes to launch and manage tasks, eliminating the need
for middle managers.
To use this feature, enable the druid-kubernetes-overlord-extensions in the
extensions load list for your Overlord process.
#13156
## # Hadoop-3 compatible binary
Druid now comes packaged as a dedicated binary for Hadoop-3 users, which
contains Hadoop-3 compatible jars. If you do not use Hadoop-3 with your Druid
cluster, you may continue using the classic binary.
# # Multi-stage query (MSQ) task engine
## # MSQ enabled for Docker
MSQ task query engine is now enabled for Docker by default.
#13069
## # Query history
Multi-stage queries no longer show up in the Query history dialog. They are
still available in the **Recent query tasks** panel.
## # Limit on CLUSTERED BY columns
When using the MSQ task engine to ingest data, the number of columns that can
be passed in the CLUSTERED BY clause is now limited to 1500.
#13352
## # Support for string dictionary compression
The MSQ task engine supports the front-coding of String dictionaries for
better compression. This can be enabled for INSERT or REPLACE statements by
setting `indexSpec` to a valid json string in the query context.
#13275
## # Sketch merging mode
Workers can now gather key statistics, used to generate partition boundaries,
either sequentially or in parallel. Set `clusterStatisticsMergeMode` to
`PARALLEL`, `SEQUENTIAL` or `AUTO` in the query context to use the
corresponding sketch merging mode. For more information, see Sketch merging
mode.
#13205
## # Performance and operational improvements
  * **Error messages** : For disallowed MSQ warnings of certain types, the warning is now surfaced as the error. #13198
  * **Secrets** : For tasks containing SQL with sensitive keys, Druid now masks the keys while logging with the help regular expressions. #13231
  * **Downsampling accuracy** : MSQ task engine now uses the number of bytes instead of number of keys when downsampling data. #12998
  * **Memory usage** : When determining partition boundaries, the heap footprint of internal sketches used by MSQ is now capped at 10% of available memory or 300 MB, whichever is lower. Previously, the cap was strictly 300 MB. #13274
  * **Task reports** : Added fields `pendingTasks` and `runningTasks` to the worker report. See Query task status information for related web console changes. #13263
# # Querying
## # Async reads for JDBC
Prevented JDBC timeouts on long queries by returning empty batches when a
batch fetch takes too long. Uses an async model to run the result fetch
concurrently with JDBC requests.
#13196
## # Improved algorithm to check values of an IN filter
To accommodate large value sets arising from large IN filters or from joins
pushed down as IN filters, Druid now uses a sorted merge algorithm for merging
the set and dictionary for larger values.
#13133
## # Enhanced query context security
Added the following configuration properties that refine the query context
security model controlled by `druid.auth.authorizeQueryContextParams`:
  * `druid.auth.unsecuredContextKeys`: A JSON list of query context keys that do not require a security check.
  * `druid.auth.securedContextKeys`: A JSON list of query context keys that do require a security check.
If both are set, `unsecuredContextKeys` acts as exceptions to
`securedContextKeys`.
#13071
## # HTTP response headers
The HTTP response for a SQL query now correctly sets response headers, same as
a native query.
#13052
# # Metrics
## # New metrics
The following metrics have been newly added. For more details, see the
complete list of Druid metrics.
### # Batched segment allocation
These metrics pertain to batched segment allocation.
Metric | Description | Dimensions  
---|---|---  
`task/action/batch/runTime` | Milliseconds taken to execute a batch of task
actions. Currently only being emitted for batched `segmentAllocate` actions |
`dataSource`, `taskActionType=segmentAllocate`  
`task/action/batch/queueTime` | Milliseconds spent by a batch of task actions
in queue. Currently only being emitted for batched `segmentAllocate` actions |
`dataSource`, `taskActionType=segmentAllocate`  
`task/action/batch/size` | Number of task actions in a batch that was executed
during the emission period. Currently only being emitted for batched
`segmentAllocate` actions | `dataSource`, `taskActionType=segmentAllocate`  
`task/action/batch/attempts` | Number of execution attempts for a single batch
of task actions. Currently only being emitted for batched `segmentAllocate`
actions | `dataSource`, `taskActionType=segmentAllocate`  
`task/action/success/count` | Number of task actions that were executed
successfully during the emission period. Currently only being emitted for
batched `segmentAllocate` actions | `dataSource`, `taskId`, `taskType`,
`taskActionType=segmentAllocate`  
`task/action/failed/count` | Number of task actions that failed during the
emission period. Currently only being emitted for batched `segmentAllocate`
actions | `dataSource`, `taskId`, `taskType`, `taskActionType=segmentAllocate`  
### # Streaming ingestion
Metric | Description | Dimensions  
---|---|---  
`ingest/kafka/partitionLag` | Partition-wise lag between the offsets consumed
by the Kafka indexing tasks and latest offsets in Kafka brokers. Minimum
emission period for this metric is a minute. | `dataSource`, `stream`,
`partition`  
`ingest/kinesis/partitionLag/time` | Partition-wise lag time in milliseconds
between the current message sequence number consumed by the Kinesis indexing
tasks and latest sequence number in Kinesis. Minimum emission period for this
metric is a minute. | `dataSource`, `stream`, `partition`  
`ingest/pause/time` | Milliseconds spent by a task in a paused state without
ingesting. | `dataSource`, `taskId`, `taskType`  
`ingest/handoff/time` | Total time taken in milliseconds for handing off a
given set of published segments. | `dataSource`, `taskId`, `taskType`  
#13238  
#13331  
#13313
## # Other improvements
  * New dimension `taskActionType` which may take values such as `segmentAllocate`, `segmentTransactionalInsert`, etc. This dimension is reported for `task/action/run/time` and the new batched segment allocation metrics. #13333
  * Metric `namespace/cache/heapSizeInBytes` for global cached lookups now accounts for the `String` object overhead of 40 bytes. #13219
  * `jvm/gc/cpu` has been fixed to report nanoseconds instead of milliseconds. #13383
# # Nested columns
## # Nested columns performance improvement
Improved `NestedDataColumnSerializer` to no longer explicitly write null
values to the field writers for the missing values of every row. Instead,
passing the row counter is moved to the field writers so that they can
backfill null values in bulk.
#13101
## # Support for more formats
Druid nested columns and the associated JSON transform functions now support
Avro, ORC, and Parquet.
#13325  
#13375
## # Refactored a datasource before unnest
When data requires "flattening" during processing, the operator now takes in
an array and then flattens the array into N (N=number of elements in the
array) rows where each row has one of the values from the array.
#13085
# # Ingestion
## # Improved filtering for cloud objects
You can now stop at arbitrary subfolders using glob syntax in the
`ioConfig.inputSource.filter` field for native batch ingestion from cloud
storage, such as S3.
#13027
## # Async task client for streaming ingestion
You can now enable asynchronous communication between the stream supervisor
and indexing tasks by setting `chatAsync` to true in the `tuningConfig`. The
async task client uses its own internal thread pool and thus ignrores the
`chatThreads` property.
#13354
## # Improved handling of JSON data with streaming ingestion
You can now better control how Druid reads JSON data for streaming ingestion
by setting the following fields in the input format specification:
  * `assumedNewlineDelimited` to parse lines of JSON independently.
  * `useJsonNodeReader` to retain valid JSON events when parsing multi-line JSON events when a parsing exception occurs.
The web console has been updated to include these options.
#13089
## # Ingesting from an idle Kafka stream
When a Kafka stream becomes inactive, the supervisor ingesting from it can be
configured to stop creating new indexing tasks. The supervisor automatically
resumes creation of new indexing tasks once the stream becomes active again.
Set the property `dataSchema.ioConfig.idleConfig.enabled` to true in the
respective supervisor spec or set `druid.supervisor.idleConfig.enabled` on the
overlord to enable this behaviour. Please see the following for details:
  * Overlord configuration
  * Supervisor spec
#13144
## # Kafka Consumer improvement
You can now configure the Kafka Consumer's custom deserializer after its
instantiation.
#13097
## # Kafka supervisor logging
Kafka supervisor logs are now less noisy. The supervisors now log events at
the DEBUG level instead of INFO.
#13392
## # Fixed Overlord leader election
Fixed a problem where Overlord leader election failed due to lock
reacquisition issues. Druid now fails these tasks and clears all locks so that
the Overlord leader election isn't blocked.
#13172
## # Support for inline protobuf descriptor
Added a new `inline` type `protoBytesDecoder` that allows a user to pass
inline the contents of a Protobuf descriptor file, encoded as a Base64 string.
#13192
## # Duplicate notices
For streaming ingestion, notices that are the same as one already in queue
won't be enqueued. This will help reduce notice queue size.
#13334
## # Sampling from stream input now respects the configured timeout
Fixed a problem where sampling from a stream input, such as Kafka or Kinesis,
failed to respect the configured timeout when the stream had no records
available. You can now set the maximum amount of time in which the entry
iterator will return results.
#13296
## # Streaming tasks resume on Overlord switch
Fixed a problem where streaming ingestion tasks continued to run until their
duration elapsed after the Overlord leader had issued a pause to the tasks.
Now, when the Overlord switch occurs right after it has issued a pause to the
task, the task remains in a paused state even after the Overlord re-election.
#13223
## # Fixed Parquet list conversion
Fixed an issue with Parquet list conversion, where lists of complex objects
could unexpectedly be wrapped in an extra object, appearing as
`[{"element":},{"element":}...]` instead of
the direct list. This changes the behavior of the parquet reader for lists of
structured objects to be consistent with other parquet logical list
conversions. The data is now fetched directly, more closely matching its
expected structure.
#13294
## # Introduced a tree type to flattenSpec
Introduced a `tree` type to `flattenSpec`. In the event that a simple
hierarchical lookup is required, the `tree` type allows for faster JSON
parsing than `jq` and `path` parsing types.
#12177
# # Operations
## # Compaction
Compaction behavior has changed to improve the amount of time it takes and
disk space it takes:
  * When segments need to be fetched, download them one at a time and delete them when Druid is done with them. This still takes time but minimizes the required disk space.
  * Don't fetch segments on the main compact task when they aren't needed. If the user provides a full `granularitySpec`, `dimensionsSpec`, and `metricsSpec`, Druid skips fetching segments.
For more information, see the documentation on Compaction and Automatic
compaction.
#13280
## # Idle configs for the Supervisor
You can now set the Supervisor to idle, which is useful in cases where freeing
up slots so that autoscaling can be more effective.
To configure the idle behavior, use the following properties: