title:Demystifying Hidden Privacy Settings in Mobile Apps
author:Yi Chen and
Mingming Zha and
Nan Zhang and
Dandan Xu and
Qianqian Zhao and
Xuan Feng and
Kan Yuan and
Fnu Suya and
Yuan Tian and
Kai Chen and
XiaoFeng Wang and
Wei Zou
(cid:19)(cid:17)(cid:18)(cid:26)(cid:1)(cid:42)(cid:38)(cid:38)(cid:38)(cid:1)(cid:52)(cid:90)(cid:78)(cid:81)(cid:80)(cid:84)(cid:74)(cid:86)(cid:78)(cid:1)(cid:80)(cid:79)(cid:1)(cid:52)(cid:70)(cid:68)(cid:86)(cid:83)(cid:74)(cid:85)(cid:90)(cid:1)(cid:66)(cid:79)(cid:69)(cid:1)(cid:49)(cid:83)(cid:74)(cid:87)(cid:66)(cid:68)(cid:90)
Demystifying Hidden Privacy Settings
in Mobile Apps
Yi Chen1,2,4∗
, Mingming Zha1,4, Nan Zhang2, Dandan Xu1,4, Qianqian Zhao1,4, Xuan Feng1,4, Kan Yuan2,
1{CAS-KLONAT
†
Fnu Suya3, Yuan Tian3, Kai Chen1,4§
, BKLONSPT
‡
, SKLOIS(cid:2)},Institute of Information Engineering, CAS, 2Indiana University Bloomington,
, XiaoFeng Wang2§
, Wei Zou1,4
3The University of Virginia,
4School of Cyber Security, University of Chinese Academy of Sciences
{chen481, nz3, kanyuan, xw7}@indiana.edu, {fs5xz, yuant}@virginia.edu
{zhamingming, xudandan, zhaoqianqian, fengxuan, chenkai, zouwei}@iie.ac.cn
Abstract—Mobile apps include privacy settings that allow their
users to conﬁgure how their data should be shared. These settings,
however, are often hard to locate and hard to understand by the
users, even in popular apps, such as Facebook. More seriously,
they are often set to share user data by default, exposing her
privacy without proper consent. In this paper, we report the ﬁrst
systematic study on the problem, which is made possible through
an in-depth analysis of user perception of the privacy settings.
More speciﬁcally, we ﬁrst conduct two user studies (involving
nearly one thousand users) to understand privacy settings from
the user’s perspective, and identify these hard-to-ﬁnd settings.
Then we select 14 features that uniquely characterize such hidden
privacy settings and utilize a novel technique called semantics-
based UI tracing to extract them from a given app. On top of
these features, a classiﬁer is trained to automatically discover the
hidden privacy settings, which together with other innovations,
has been implemented into a tool called Hound. Over our labeled
data set, the tool achieves an accuracy of 93.54%. Further
running it on 100,000 latest apps from both Google Play and
third-party markets, we ﬁnd that over a third (36.29%) of
the privacy settings identiﬁed from these apps are “hidden”.
Looking into these settings, we observe that they become hard to
discover and hard to understand primarily due to the problematic
categorization on the apps’ user interfaces and/or confusing
descriptions. Further importantly, though more privacy options
have been offered to the user over time, also discovered is the
persistence of their usability issue, which becomes even more
serious, e.g., originally easy-to-ﬁnd settings now harder to locate.
And among all such hidden privacy settings, 82.16% are set
to leak user privacy by default. We provide suggestions for
improving the usability of these privacy settings at the end of
our study.
I. INTRODUCTION
Today many mobile applications (app for short) are designed
to utilize user information for better services. For this purpose,
they often seek the users’ consents through various privacy
settings: e.g., those for one to decide whether to share her
location information with a social app or enable her friends to
∗Work was done when the ﬁrst author was at Indiana University Blooming-
ton.§Corresponding Authors
†Key Laboratory of Network Assessment Technology, CAS.
‡Beijing Key Laboratory of Network Security and Protection Technology
(cid:2)State Key Laboratory of Information Security, IIE, CAS
Fig. 1: Example of default privacy setting in Facebook
locate her via her phone number (“let other users ﬁnd me using
my phone number”). Although such privacy settings indeed
provide the user with means to choose her desired levels of
privacy protection, there are complaints about the difﬁculty
in locating them from an app’s user interfaces (UIs), which
can become a serious privacy concern when these settings are
by default conﬁgured to expose user data. In October 2016,
Facebook was given the “Big Brother” award [1] as the “biggest
privacy-offender of the year”. Voters criticized its app’s privacy
settings, many of which are not only opt-in by default for
collecting sensitive data from users (e.g., location, friend list,
etc.), but also hiding deeply inside the UIs and difﬁcult to ﬁnd.
For instance, Facebook had a privacy setting “Nearby Friends”
that allowed the app to share user location with friends by
default, as shown in Figure 1; it was placed under Location
of Account Settings, which is not a typical place where users
look for privacy settings, not to mention that the entry for
Account Settings is in the middle of a long list with 41 various
conﬁgurations, making it even less likely for the user to notice
the entry. This problem is found to be pervasive in our study,
affecting prominent apps like LinkedIn, Instagram, and Spotify.
Understanding hidden privacy settings. In this paper, we
report the ﬁrst large-scale measurement study on such hidden
privacy settings, which sheds new light on their pervasiveness,
privacy implications and the fundamental causes of their
problematic designs. For this purpose, we perform user studies
to identify the unique features of the conﬁgurations hard to
locate by ordinary users, and further develop an automatic
analysis tool called Hound to recover these features from an
app and detect its hidden settings.
More speciﬁcally, we conduct
two user studies in our
research. The ﬁrst is to understand the user’s perception of
(cid:165)(cid:1)(cid:19)(cid:17)(cid:18)(cid:26)(cid:13)(cid:1)(cid:58)(cid:74)(cid:1)(cid:36)(cid:73)(cid:70)(cid:79)(cid:15)(cid:1)(cid:54)(cid:79)(cid:69)(cid:70)(cid:83)(cid:1)(cid:77)(cid:74)(cid:68)(cid:70)(cid:79)(cid:84)(cid:70)(cid:1)(cid:85)(cid:80)(cid:1)(cid:42)(cid:38)(cid:38)(cid:38)(cid:15)
(cid:37)(cid:48)(cid:42)(cid:1)(cid:18)(cid:17)(cid:15)(cid:18)(cid:18)(cid:17)(cid:26)(cid:16)(cid:52)(cid:49)(cid:15)(cid:19)(cid:17)(cid:18)(cid:26)(cid:15)(cid:17)(cid:17)(cid:17)(cid:22)(cid:21)
(cid:22)(cid:24)(cid:17)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:53:00 UTC from IEEE Xplore.  Restrictions apply. 
^ĞƚƚŝŶŐƐ
WŚŽŶĞĐŽŶƚĂĐƚǀŝƐŝďŝůŝƚǇ
Ǉ
;ůůŽǁƚŽďĞĨŽƵŶĚĨƌŽŵ
ƉŚŽŶĞŶƵŵďĞƌͿ
WƌŽĨŝůĞ
Fig. 2: Example of a UI-path
Fig. 3: Example of typical
icons
Fig. 4: Example of too
long text description
data exposure controlled by the privacy settings. The study
shows that the overwhelming majority of the participants
(83.5%) do care about at least one such privacy setting in our
questionnaire. The second study is meant to ﬁnd out whether
these valuable privacy settings are presented to the user in the
right way, which reveals that nearly half of them (47.12%) are
considered difﬁcult to ﬁnd, and 9.64% cannot be located by
any participants. From the participants’ inputs, we highlight
the root causes of the troubles in ﬁnding these settings and
convert them into features to help detect the settings.
Detecting hidden settings. To understand the privacy impli-
cations of the hidden settings and support more usable privacy
conﬁgurations, we build a new tool called Hound to recover
privacy settings and identify those problematic ones from an
app’s UI. Note that ﬁnding privacy-related UI items is known
to be hard [2], due to the diversity of the conﬁdential data
(e.g., friend list, music listening history), not limited to these
protected by permissions. More challenging here is to detect
these hidden ones, the settings hard to ﬁnd by ordinary users,
which has never been done before.
To address these challenges, we ﬁrst utilize natural language
processing (NLP) to capture privacy-related settings, through
training a classiﬁer on top of a set of feature vectors, each
constructed from a setting’s description (Section III). From
these settings, our approach further discovers those considered
to be hidden, based upon the aforementioned features identiﬁed
from the user study. These features are all related to UIs,
extracted from the UI-path that links the app’s home view (i.e.,
initial activity) to a given privacy setting, when the user clicks
on a certain UI element on each view to move the UI to the next
one, until the setting is reached. An example of such a path, as
illustrated in Figure 2, is home view → P rof ile → Settings.
Automatic discovery of such a path through a static analysis
is difﬁcult, due to the diversity in the way that a UI transition
can be triggered (e.g., intent, callback, etc.). Our solution is
semantics-based UI tracing, a novel technique that exploits the
observation that when the UI moves from one view to another,
the text description of the UI element triggering the move on
the former is often semantically related to the title of the latter.
For example, in Figure 2, the title of the last view and the
text description of its triggering UI element in the second view
have the same text “Settings”. In this way, our approach can
automatically piece together views that link the source (home
view) to the target (the privacy setting). Using the features
extracted from such UI-paths, Hound can effectively detect
hidden privacy settings (an accuracy of 93.54%, see Section IV)
with a performance of 530 seconds per app.
Findings. Running Hound on 50,000 apps from Google Play
and 50,000 apps from Chinese third-party markets, we perform
the ﬁrst large-scale analysis of mobile apps’ privacy settings.
To our surprise, among all 7,058 apps containing privacy
conﬁgurations, nearly half of them (47.04%) have some hidden
privacy settings. Moreover, these hidden ones cover more than
one-third (36.29%) of all the privacy settings discovered. Most
of these hidden settings turn out to be privacy-critical: 82.16%
of them by default leak out user information. Our study shows
that although apparently app developers try to provide users
with more privacy-related options to customize their protection,
the problem of hidden privacy settings becomes even more
serious, with more settings hard to discover by the user, possibly
due to the developers’ lack of understanding about the usability
challenges from the user’s perspective. Our research further
reveals the developer’s design pitfalls and offers suggestions
for improvement.
Contributions. The contributions of the paper are summarized
as follows:
• New understanding of hidden privacy settings. We conduct
the ﬁrst systematic study on the privacy settings hard to discover
and understand across a large number of popular Android apps
(100K). Our study brings to light the signiﬁcant impacts of the
problem (over a third of privacy settings hidden and most of
these settings exposing user data by default) and its potential
causes, which can lead to better design of these settings’ UIs
and enhancement of protection for user data.
(cid:22)(cid:24)(cid:18)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:53:00 UTC from IEEE Xplore.  Restrictions apply. 
• New technique. We develop a novel technique to automatically
discover these hidden privacy settings, base upon the knowledge
recovered from two user studies and a novel static analysis
technique that utilizes semantics of UI elements to correlate
the views triggered sequentially during the user’s navigation.
We demonstrate the effectiveness of the new technique, which
achieves a high accuracy of 93.54%, and its efﬁciency that
enables the large-scale study.
II. UNDERSTANDING HIDDEN PRIVACY SETTINGS
We design two user studies to understand the privacy settings
from the user’s perspective. One is to ﬁnd out one’s perception
of the data exposure controlled by the privacy settings. The
other is to determine whether these settings are presented to the
user in an easy-to-ﬁnd way. From their feedback, we further
identify what makes these settings hard to ﬁnd, which further
helps us detect them in apps.
A. Privacy Settings in Mobile Apps
Privacy settings. To support users with personalized services,
mobile apps often rely on their personal data, which may
contain sensitive information. For example, many apps (e.g.,
LinkedIn) display to the user personalized advertisements based
upon analyzing her cookies (often with sensitive information
such as previously visited websites and shopping history);
Facebook shares a user’s friend list with others for expanding
their friend circles. To get the user’s consent for collecting
such data and also enable her to trade protection for usability,
the app developer tends to provide privacy settings to let one
control how her personal information is collected, shared and
used [3]. For instance, LinkedIn allows their users to disable
target advertising through “Advertising preferences (Choose
whether LinkedIn can use cookies to personalize ads)”.
To understand the privacy settings and the user data they
protect, we manually analyze 200 popular apps1 (top 100 apps
in English language from the ofﬁcial market Google Play and
top 100 apps in Chinese language from the Baidu market, a
famous Chinese alternative market). These apps fall into 37
different categories as deﬁned by Google Play (e.g., dating,
ﬁnance, shopping, etc.). For each of them, we extract all the
settings under the UI titled Privacy Settings. In this way, we
collect around 600 settings and further classify them into six
categories according to the data they protect, as shown in
Table I, including on-device data, users’ personal proﬁle, users’
social connections, users’ behaviors, users’ posted contents,
and anti-spam settings. It is important to note that only the
on-device data can be secured by system permissions such
as locations and contact lists. For the data in other categories
that actually constitute most (87.31%) of the privacy settings,
users can only control their leakage through the settings. If
they are not properly presented and managed, information will
be disclosed without the user’s consent.
1We collect the apps which have a UI titled Privacy Settings.
User study 1: User perspective of privacy settings. To
understand whether users really care about the private data
protected by the settings, we conducted an online survey
through Amazon Mechanical Turk (MTurk) [4]. The survey
is designed to ask users to which extent they care about the
private data. Speciﬁcally, from the Facebook app, we randomly
selected one privacy setting in each of the above categories, and
asked the participants about their feelings if the data protected
by the given settings were leaked (e.g., “Facebook accesses
your location even when you are NOT using the app. What do
you feel about if this case happens?”). A participant’s response
can be “Very upset”, “Upset”, “Neutral”, “It might be okay”
or “I don’t care”. Details of the privacy settings and the survey
are presented in Appendix X-A.
The survey started in September 2018 and had lasted for two
weeks. 269 Turkers participated in the survey, and 265 of them
had used the Facebook app for over one month. Their median
age is 29 (44.15% male and 55.85% female). 60.38% hold a
Bachelor or a higher degree. After collecting all the responses,
we removed 65 careless responses by checking attention
questions [5]. From the survey, we ﬁnd that 83.5%2 of the
participants care about (i.e., feel “Very upset” or “Upset” to) the
data covered by at least one privacy setting in the questionnaire,
and 61.5%3 of them care about the data protected under more
than half of the settings. Detailed responses are presented in
Figure 12 (Appendix X-B). We also asked the participants
to compare the data protected by the settings (provided by
developers) with those guarded by system permissions [6].
71.0%4 of them think that the privacy-setting related data are
as important as or even more important than those covered by
permissions, just as we expect.
B. Hidden Privacy Settings
To understand why some privacy settings are difﬁcult to
ﬁnd from the user’s perspective, we conduct the second user
study that involves 732 participants and lasts over 100 days.
The results reveal that nearly half (47.12%) of the settings
are considered as hidden, and about one-tenth (9.64%) of
them are never successfully found by any participants. From
the participants’ feedback, we further identify six root causes