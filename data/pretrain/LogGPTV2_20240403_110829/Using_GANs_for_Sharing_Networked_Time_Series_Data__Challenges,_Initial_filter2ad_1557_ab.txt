where the parameters can be learned (trained) from data. Exam-
ples include autoregressive models, Markov models, and recurrent
466
Figure 1: Autocorrelation of daily page views for Wikipedia
Web Traffic dataset.
neural networks (RNNs). In theory, these generic statistical models
have the potential to generalize across datasets. However, they are
not general in terms of use cases, because they do not jointly model
metadata and time series. For example, in a network measurement
dataset, modeling only the packet loss rate is good for understand-
ing its patterns. But if we want to learn which IP prefix has network
issues, jointly modeling the metadata (IP address) and the time se-
ries (sequence of packet loss rate) is important. Additionally, these
general-purpose models have bad fidelity, failing to capture long-
term temporal correlations. For instance, all these models fail to
capture weekly and/or annual patterns in the Wikipedia Web Traf-
fic dataset (Figure 1). Our point is not to highlight the importance
of learning autocorrelations,1 but to show that learning temporal
data distributions (without overfitting to a single statistic, e.g. au-
tocorrelation) is hard. Note that DG is able to learn weekly and
annual correlations without special tuning.
3 OVERVIEW
As we saw, the above classes of techniques do not achieve good
fidelity and generality across datasets and use cases. Our overarch-
ing goal is thus to develop a general framework that can achieve
high fidelity with minimal expertise.
1, Ai
(cid:8)O 1, O 2, ..., On(cid:9) is defined as a set of samples Oi (e.g., the clients).
3.1 Problem formulation
We abstract the scope of our datasets as follows: A dataset D =
Each sample Oi = (Ai , Ri) contains m metadata Ai = [Ai
m].
2, ..., Ai
For example, metadata Ai
could represent client i’s physical loca-
tion, and Ai
the client’s ISP. Note that we can support datasets
in which multiple samples have the same set of metadata. The
second component of each sample is a time series of records Ri =
[Ri
means j-th measurement of i-th client.
1, Ri
Different samples may contain a different number of measure-
ments. The number of records for sample Oi is given by T i. Each
= (ti
, and K measurements
record Ri
j , f i
j
= [f i
represents the time when
j,1, f i
j,2, ..., f i
f i
j
1While there are specific tools for estimating autocorrelation, in general this is a hard
problem in high dimensions [10, 18].
T i], where Ri
2, ..., Ri
j ) contains a timestamp ti
j,K]. For example, ti
j
j
2
1
j
0100200300400500Time lag (days)−0.50−0.250.000.250.500.751.00AutocorrelationDoppelGANgerRealRealDoppelGANgerRNNARHMMNaive GANTimeGANRCGANMarketSimulatorIMC ’20, October 27–29, 2020, Virtual Event, USA
Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar
3.3.1 Prior Work. Using GANs to generate time series is a popular
idea [32, 35, 36, 42, 82, 117–119]. Among the domain-agnostic de-
signs, the generator usually takes prior measurements (generated
or real) and noise as inputs and outputs one measurement at a
time [35, 42, 117–119]. These works typically change two aspects
of the GAN: the architecture [35, 42, 119], the training [118], or
both [17, 117]. The two most relevant papers to ours are RCGAN
[35] and TimeGAN [117]. RCGAN is the most similar design to
ours; like DG, it uses recurrent neural networks (RNNs) to generate
time series and can condition the generation on metadata. However,
RCGAN does not itself generate metadata and has little to no eval-
uation of the correlations across time series and between metadata
and measurements. We found its fidelity on our datasets to be poor;
we instead use a different discriminator architecture, loss function,
and measurement generation pipeline (§4). TimeGAN is the current
state-of-the-art, outperforming RCGAN [117]. Like RCGAN, it uses
RNNs for both the generator and discriminator. Unlike RCGAN, it
trains an additional neural network that maps time series to vector
embeddings, and the generator outputs sequences of embeddings
rather than samples. Learning to generate transformed or embed-
ded time series is common, both in approaches that rely on GANs
[76, 117] and those that rely on a different class of generative mod-
els called variational autoencoders (VAE) [17]. Our experiments
suggest that this approach models long time series poorly (§5).
3.3.2 Challenges. Next, we highlight key challenges that arise in
using GANs for our use cases. While these challenges specifically
stem from our attempts in using GANs in networking- and systems–
inspired use cases, some of these challenges broadly apply to other
use cases as well.
Fidelity challenges: The first challenge relates to long-term tem-
poral correlations. As we see in Figure 1, the canonical GAN does
poorly in capturing temporal correlations trained on the Wikipedia
Web Traffic (WWT) dataset.2 Concurrent and prior work on using
GANs for other time series data has also observed this [35, 36, 117,
118]. One approach to address this is segmenting long datasets
into chunks; e.g., TimeGAN [117] chunks datasets into smaller
time series each of 24 epochs, and only evaluates the model on
producing new time series of this length [116]. This is not viable
in our domain, as relevant properties of networking/systems data
often occur over longer time scales (e.g., network measurements)
(see Figure 1). Second, mode collapse is a well-known problem in
GANs where they generate only a few modes of the underlying
distribution [6, 50, 70, 101]. It is particularly exacerbated in our
time series use cases because of the high variability in the range
of measurement values. Finally, we need to capture complex rela-
tions between measurements and metadata (e.g., packet loss rate
and ISP), and across different measurements (e.g., packet loss rate
and byte counts). As such, state-of-the-art approaches either do
not co-generate attributes with time series data or their accuracy
for such tasks is unknown [35, 117, 119], and directly generating
joint metadata with measurements samples does not converge (§5).
Further, generating independent time series for each measurement-
dimension will break their correlations.
2This uses: (1) a dense multilayer perceptron (MLP) which generates measurements and
metadata jointly, (2) an MLP discriminator, and (3) Wasserstein loss [6, 50], consistent
with prior work [24, 39, 49, 52].
Figure 2: Original GAN architecture from [46].
represent the ping loss
the measurement f i
is taken, and f i
, f i
j
j,1
j,2
rate and traffic byte counter at this timestamp respectively. Note
∀1 ≤ j < T i.
that the timestamps are sorted, i.e. ti
j < ti
This abstraction fits many classes of data that appear in network-
ing applications. For example, it is able to express web traffic and
cluster trace datasets (§5). Our problem is to take any such dataset
as input and learn a model that can generate a new dataset D′ as
output. D′ should exhibit fidelity, and the methodology should be
general enough to handle datasets in our abstraction.
j+1
3.2 GANs: Background and Promise
GANs are a data-driven generative modeling technique [46] that
take as input training data samples and output a model that can
produce new samples from the same distribution as the original data.
More precisely, if we have a dataset of n samples O1, . . . , On, where
Oi ∈ Rp, and each sample is drawn i.i.d. from some distribution
Oi ∼ PO . The goal of GANs is to use these samples to learn a model
that can draw samples from distribution PO [46].
GANs use an adversarial training workflow consisting of a gener-
ator G and a discriminator D (Figure 2). In practice, both are instan-
tiated with neural networks. In the canonical GAN design [46], the
generator maps a noise vector z ∈ Rd to a sample O ∈ Rp, where
p ≫ d. z is drawn from some pre-specified distribution Pz, usually
a Gaussian or a uniform. Simultaneously, we train the discriminator
D : Rp → [0, 1], which takes samples as input (either real of fake),
and classifies each sample as real (1) or fake (0). Errors in this classi-
fication task are used to train the parameters of both the generator
and discriminator through backpropagation. The loss function for
GANs is: minG maxD Ex∼px [log D(x)] + Ez∼pz[log(1− D(G(z)))].
The generator and discriminator are trained alternately, or adversar-
ially. Unlike prior generative modeling approaches which likelihood
maximization of parametric models (e.g., §2.2), GANs make fewer
assumptions about the data structure.
Compared with related work in §2.2, GANs offer three key ben-
efits. First, similar to the machine learning models, GANs can be
general across datasets. The discriminator is an universal agent for
judging the fidelity of generated samples. Thus, the discriminator
only needs raw samples and it does not need any other information
about the system producing the samples. Second, GANs can be used
to generate measurements and metadata (§3.3.2). Thus, GANs have
the potential to support a wide range of use cases involving mea-
surements, metadata, and cross-correlations between them. Finally,
GANs have been used in other domains for generating realistic
high-fidelity datasets for complex tasks such as images [65], text
[36, 118], and music [32, 82].
3.3 Using GANs to Generate Time Series
467
Discri-minator!1 = real0 = fake Generator"#∈ℝ&'∈ℝ(Data'∈ℝ(Using GANs for Sharing Networked Time Series Data: Challenges, Initial Promise, and Open Questions
IMC ’20, October 27–29, 2020, Virtual Event, USA
(a)
(b)
Figure 3: (a) The usual way of generating time series. (b)
Batch generation with S = 3. The RNN is a single neural
network, even though many units are illustrated. This un-
rolled representation conveys that the RNN is being used
many times to generate samples.
Privacy Challenges: In addition to the above fidelity challenges,
we also observe key challenges with respect to privacy, and rea-
soning about the privacy-fidelity tradeoffs of using GANs. A meta
question, not unique to our work, is finding the right definition of
privacy for each use case. Some commonly used definitions in the
community are notions of differential privacy (i.e., how much does
any single data point contribute to a model) [33] and membership
inference (i.e., was a specific sample in the datasets) [94]. However,
these definitions can hurt fidelity [9, 93] without defending against
relevant attacks [38, 57]. In our networking/systems use cases, we
may also want to even hide specific features and avoid releasinng
aggregate statistical characteristics of proprietary data (e.g., num-
ber of users, server load, meantime to failure). Natural questions
arise: First, can GANs support these flexible notions of privacy in
practice, and if so under what configurations? Second, there are
emerging proposals to extend GAN training (e.g., [40, 112, 113]) to
offer some privacy guarantees. Are their privacy-fidelity tradeoffs
sufficient to be practical for networking datasets?
4 DOPPELGANGER DESIGN
In this section, we describe how we tackle fidelity shortcomings
of time series GANs. Privacy is discussed in Section 6. Recall that
existing approaches have issues in capturing temporal effects and
relations between metadata and measurements. In what follows,
we present our solution starting from the canonical GAN strawman
and extend it to address these challenges. Finally, we summarize
the design and guidelines for users to use our workflow.
4.1 Capturing long-term effects
Recall that the canonical GAN generator architecture is a fully-
connected multi-layer perceptron (MLP), which we use in our
strawman solution (§3.3.2). As we saw, this architecture fails to
capture long-term correlations (e.g., annual correlations in page
views).
RNN primer and limitations: Similar to prior efforts, we posit
that the main reason is that MLPs are not well suited for time series.
A better choice is to use recurrent neural networks (RNNs), which
are designed to model time series and have been widely used in the
GAN literature to generate time series [35, 82, 117–119]. Specifically,
we use a variant of RNN called long short-term memory (LSTM)
[58].
At a high level, RNNs work as follows (Figure 3 (a)). Instead of
generating the entire time series at once, RNNs generate one record
Figure 4: Error vs. batch parameter.
at a time (e.g., page views on the jth day), and then run T i (e.g.,
Ri
j
the number of days) passes to generate the entire time series. The
key difference in a RNN from traditional neural units is that RNNs
have an internal state that implicitly encodes all past states of the
signal. Thus, when generating Ri
, the RNN unit can incorporate
j
the patterns in Ri
(e.g., all page views before the j-th day).
Note that RNNs can learn correlations across the dimensions of a
time series, and produce multi-dimensional outputs.
1, ..., Ri
j−1
However, we empirically find that RNN generators still struggle
to capture temporal correlations when the length exceeds a few
hundred epochs. The reason is that for long time series, RNNs take
too many passes to generate the entire sample; the more passes
taken, the more temporal correlation RNNs tend to forget. Prior
work copes with this problem in three ways. The first is to generate
only short sequences [82, 117, 118]; long datasets are evaluated on
chunks of tens of samples [116, 117]. The second approach is to
train on small datasets, where rudimentary designs may be able
to effectively memorize long term effects (e.g. unpublished work
[28] generates time series of length 1,000, from a dataset of about
100 time series). This approach leads to memorization [7], which
defeats the purpose of training a model. A third approach assumes
an auxiliary raw data time series as an additional input during the
generation phase to help generate long time series [119]. This again
defeats the purpose of synthetic data generation.
Our approach: To reduce the number of RNN passes, we propose
to use a simple yet effective idea called batch generation. At each
pass of the RNN, instead of generating one record (e.g., page views
of one day), it generates S records (e.g., page views of S consec-
utive days), where S is a tunable parameter (Figure 3 (b)).3 This
effectively reduces the total number of RNN passes by a factor of S.
As S gets larger, the difficulty of synthesizing a batch of records at
a single RNN pass also increases. This induces a natural trade-off
between the number of RNN passes and the single pass difficulty.
For example, Figure 4 shows the mean square error between the
autocorrelation of our generated signals and real data on the WWT
dataset. Even a small (but larger than 1) S gives substantial improve-
ments in signal quality. In practice, we find that S = 5 works well