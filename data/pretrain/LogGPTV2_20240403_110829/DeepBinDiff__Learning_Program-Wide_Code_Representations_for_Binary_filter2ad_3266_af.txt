### VII. Case Study: Vulnerability Patch Matching

While both tools can achieve high accuracy, only DEEPBINDIFF is capable of correctly matching the vulnerability patch. Specifically, we expect diffing tools to identify the patch as a new insertion while still matching the original basic blocks for vulnerability analysis. As shown in Figure 11a, BinDiff mismatches the vulnerable basic block with the new condition check basic block, leaving the real matching basic block unmatched (shown as a white block). In contrast, DEEPBINDIFF successfully matches the basic blocks and identifies the new condition check as an insertion. This case study demonstrates that DEEPBINDIFF can accurately identify inserted basic blocks, thanks to its design choices and Algorithm 1.

### VIII. Discussion

#### A. Compiler Optimizations

Different compiler optimization techniques are one of the major reasons for code transformation. For example, instruction scheduling, instruction replacement, block reordering, function inlining, and register allocation can significantly alter the binary code. DEEPBINDIFF is designed to handle these common optimization techniques to achieve high matching accuracy.

As shown in Table VI, DEEPBINDIFF's design takes into account multiple common compiler optimization techniques [15]. We deliberately exclude assumptions that are easy to break, such as the order of instructions and blocks, CFG directions, and function boundaries.

1. **Instruction Sequential Information**: When training the block feature vector generation model, instruction sequential information is not part of the context information.
2. **ICFG as Undirected Graph**: By treating the merged ICFG as an undirected graph, our system remains unaffected by block reordering as long as the two blocks are k-hop neighbors.
3. **Function Inlining**: DEEPBINDIFF generates random walks based on ICFG, extracting control dependency information regardless of function boundary changes.
4. **k-Hop Greedy Matching**: This is performed on top of ICFG rather than CFG, ensuring that block matching is not affected by function boundary changes.
5. **Register Name Normalization**: This technique helps in handling register allocation differences.

For instance, about 10% of functions with GCC O2 optimization have been transformed by function inlining [21]. DEEPBINDIFF's design choices ensure that it can handle such transformations effectively.

#### B. Limitations

Despite its advantages, DEEPBINDIFF has a few limitations:

1. **Block Merging**: In practice, certain blocks are often merged by the compiler to reduce branch mispredictions. This can change the number of blocks in the binaries. In such cases, DEEPBINDIFF may mistakenly categorize some blocks as insertions or deletions since it performs one-to-one block matching. However, if a block is semantically rich (i.e., contains multiple instructions), it will still be matched correctly to the merged block, leaving only the less meaningful block unmatched.
2. **Control Flow Changes**: Optimizations that drastically change the control flow can affect the effectiveness of DEEPBINDIFF. Since our analysis heavily relies on ICFG to extract graph structural information, significant changes in control flow can impact the results. Consequently, DEEPBINDIFF is vulnerable to obfuscation techniques that completely alter the CFG. Packing techniques [51], [24] that encrypt the code can also defeat our system. It is important to note that no existing learning-based techniques can do a better job since they all rely on control flow information.
3. **Cross-Architecture Differencing**: DEEPBINDIFF currently does not support cross-architecture diffing, which is becoming increasingly popular, especially in IoT-related security research [29], [27], [19]. Potentially, this issue could be addressed by lifting binaries into IR first and then performing diffing in the same way. We leave this as future work.

### IX. Related Work

#### A. Code Similarity Detection

**Static Approaches**:
- **BinDiff** [10], [25]: Performs many-to-many graph isomorphism detection on call graphs to match functions and uses CFG matching for basic blocks.
- **Binslayer** [12]: Augments graph matching with the Hungarian algorithm to improve matching results.
- **Pewny et al.** [49]: Searches bugs by collecting input/output pairs to capture the semantics of a basic block and perform graph matching.
- **discovRE** [27]: Uses lightweight syntax features and applies pre-filtering before matching to improve runtime performance.
- **Tracelet** [20]: Converts CFGs into fixed-length paths called tracelets and matches them via rewriting.
- **Esh** [17]: Decomposes functions into data-flow dependent segments (strands) and uses statistical reasoning to calculate similarities.
- **GitZ** [18]: Finds strand equality through re-optimization.
- **BinGo** [14]: Performs selective function inlining and extracts partial traces for function similarity detection.
- **BinHunt** [30]: Uses static symbolic execution and theorem proving to extract semantics.
- **CoP** [40]: Uses symbolic execution to compute the semantic similarity of blocks and leverages the longest common sub-sequence of linearly independent paths to measure the similarity.

**Dynamic Approaches**:
- **Blanket Execution** [26]: Executes functions of the two input binaries with the same inputs and compares monitored behaviors for similarity.
- **iBinHunt** [43]: Extends the comparison to inter-procedural CFGs and reduces the number of candidates for basic block matching by monitoring the execution under a common input.
- **BinSim** [44]: Specifically proposed to compare binaries with code obfuscation techniques, it relies on system calls to perform dynamic slicing and then checks equivalence with symbolic execution.

**Learning-Based Approaches**:
- **Genius** [29]: Forms attributed CFGs and calculates the similarity via graph embeddings generated through comparing with a set of representative graphs (codebook).
- **Gemini** [54]: Improves Genius by leveraging neural networks to generate function embeddings and trains a Siamese network for similarity detection.
- **Î±Diff** [39]: Uses a similar Siamese network with CNN to generate function embeddings, eliminating the need for manually crafted features.
- **InnerEye** [58]: Utilizes NLP techniques and LSTM-RNN to automatically encode the information of basic blocks.
- **Asm2Vec** [23]: Adopts an unsupervised learning approach by generating token and function embeddings using the PV-DM model.
- **SAFE** [41]: Leverages a self-attentive neural network to generate function embeddings.

#### B. Graph Embedding Learning

- **HOPE** [47]: Preserves higher-order proximity and uses generalized Singular Value Decomposition to improve efficiency.
- **TADW** [56]: Considers feature vectors for nodes during matrix factorization.
- **REGAL** [32]: Performs factorization with node features but only checks the existence of features without considering numeric values.
- **DeepWalk** [48]: Learns latent representations of nodes in a graph using local information from truncated uniform random walks.
- **node2vec** [31]: Designs a biased random walk procedure to efficiently explore diverse neighborhoods of a node and learn continuous feature representations.
- **DNGR** [13]: Proposes a graph representation model based on deep neural networks that captures graph structure information directly.
- **SDNE** [52]: Designs a semi-supervised model with multiple layers of non-linear functions to capture both local and global graph structures.
- **GCN** [33]: Uses a localized first-order approximation of spectral graph convolutions to perform semi-supervised learning on graphs in a scalable way.
- **Structure2Vec** [16]: Proposes structured data representation via learning feature spaces that embed latent variable models.

### X. Conclusion

In this paper, we propose a novel unsupervised learning-based program-wide code representation learning technique to perform binary diffing. To precisely match the blocks within given binaries, we leverage NLP techniques to generate token embeddings, which are further used to generate block feature vectors containing semantic information. We then generate inter-procedural CFGs (ICFGs), extract program-wide structural information from the ICFGs using the TADW algorithm, and generate basic block level embeddings. Finally, we propose a k-hop greedy matching algorithm to find optimal matching for the blocks. We implement a prototype named DEEPBINDIFF and evaluate it against 113 binaries from Coreutils, Diffutils, and Findutils, 10 C++ binaries, and 2 real-world vulnerabilities in OpenSSL under the scenarios of cross-version and cross-optimization-level diffing. The results show that our system outperforms state-of-the-art techniques by a large margin.

### Acknowledgements

We would like to thank the anonymous reviewers for their helpful and constructive comments. This work was supported in part by the National Science Foundation under grant No. 1719175, DARPA under grant FA8750-16-C-0044, and the Office of Naval Research under Award No. N00014-17-1-2893. Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the funding agencies.

### References

[References listed here as provided in the original text]

---

This version of the text is more organized, clear, and professional, with improved readability and coherence.