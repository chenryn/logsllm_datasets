### 6.1 Uplink Utilization Imbalance

Unsurprisingly, Hadoop, with its longer flows, exhibits a greater imbalance compared to the other two rack types. This imbalance can be particularly pronounced at small timescales, with the 90th percentile of Hadoop racks showing an average deviation of 100%. Even in the median case, all three types of racks (Web, Cache, and Hadoop) had a Mean Absolute Deviation (MAD) of over 25%, indicating significant variability.

**Figure 7: CDF of the Mean Absolute Deviation (MAD) of Uplink Utilization within a Given Sampling Period**
- A deviation of 0 indicates perfectly balanced uplinks.
- Both egress and ingress directions are shown, as well as granularities of 1 second and 40 microseconds.

**Subfigures:**
- **(a) Egress**
- **(b) Ingress**

**Figure 8: Heatmap of Pearson Correlation Coefficients for Servers in the Same Rack**
- ToR-to-server utilization was measured for three representative racks at a granularity of 250 microseconds.
- **(a) Web Rack**
- **(b) Cache Rack**
- **(c) Hadoop Rack**

### 6.2 Correlation Between Servers

Ideal application and Layer-4 load balancing would suggest that downlink utilization is balanced. However, the reality is more nuanced and heavily dependent on the type of rack. To factor out differences in the absolute amount of traffic going to each server, we use Pearson correlation coefficients, which measure the linear correlation between pairs of servers. Ingress and egress trends were almost identical, so we only show the ToR-to-server direction.

- **Web Racks:** There is almost no correlation. Diurnal patterns and flash crowds can cause correlation at longer timescales, but at small timescales, these factors are not easily discernible. Web servers run stateless services driven by user requests, leading to near-zero correlation.
- **Hadoop Racks:** Some correlation exists, but it is modest at these timescales.
- **Cache Racks:** Subsets of Cache servers show very strong correlation. This is because their requests are initiated in groups from web servers, potentially involving the same scatter-gather requests.

### 6.3 Directionality of Bursts

Having examined both uplinks and downlinks separately, we now look at their relative behavior. Figure 9 shows the relative frequency of hot uplinks and downlinks for each rack type.

- **Web and Hadoop Racks:** There is a significant bias toward servers rather than uplinks. Only 18% of hot Hadoop samples were for uplinks, with Web uplinks responsible for an even lower share. For these racks, bursts tend to result from high fan-in, where many servers send to a single destination.
- **Cache Racks:** The opposite trend is observed, with most bursts occurring on the uplinks. This can be attributed to:
  1. Simple response-to-request communication pattern.
  2. Larger cache responses compared to requests.
  3. Modest oversubscription at the ToR layer (at a ratio of 1:4), making the communication bottleneck lie in the ToRs' uplinks.

**Figure 9: Uplink/Downlink Share of Hot Ports Given 300 µs Sampling**

### 6.4 Effect of Microbursts on Shared Buffers

Finally, we examine the effect of synchronized bursts on ToRs' shared buffer utilization. Figure 10 depicts a boxplot of the peak buffer occupancy during a 50 ms interval versus the number of hot ports during that same span. The granularity of these measurements was lower due to an inefficient interface for polling shared buffer utilization. Buffer carving is dynamic, so we normalize the occupancy to the maximum value observed in our data sets.

- **Hadoop Racks:** Put significantly more stress on ToR buffers than Web or Cache racks. Hadoop sometimes drove 100% of its ports to > 50% utilization, while Web and Cache only drove a maximum of 71% and 64% of their ports to simultaneous high utilization. Hadoop also experiences high standing buffer occupancy, which scales more drastically with the number of hot ports.
- **All Racks:** Average occupancy levels off for high numbers of hot ports, possibly due to self-selection of communication patterns or the sublinear scaling of buffer requirements with the number of TCP connections.

**Figure 10: Normalized Peak Shared Buffer Occupancy versus Number of Hot Ports for Web/Cache/Hadoop Racks at a 300 µs Granularity**

### 7. Data Center Design Implications

Our measurements of a production data center highlight the need for fine-grained measurement to truly understand network behavior. This has implications for network measurement, the evaluation of new protocols and architectures, and the design of those protocols and architectures.

- **Load Balancing:** Recent proposals suggest load balancing on microflows rather than 5-tuples. While our framework does not measure inter-packet gaps directly, most observed inter-burst periods exceed typical end-to-end latencies, and non-burst utilization is low.
- **Congestion Control:** Traditional congestion control algorithms react to packet drops, RTT variation, or ECN. These signals require at least RTT/2 to arrive at the sender, and protocols can take many RTTs to adapt. Our measurements show that many microbursts are shorter than a single RTT, suggesting the need for lower-latency congestion signals.
- **Pacing:** TCP pacing, originally designed to prevent bursty traffic, has been rendered ineffective by features like segmentation offload and interrupt coalescing. Our results point to the importance of recent pacing proposals at both hardware and software levels.

### 8. Conclusion

As network bandwidth in data centers continues to rise, the timescale of network events will decrease. Understanding the behavior of these networks at high resolution is essential, especially since most traffic bursts occur at a microsecond-level granularity. Our results show that at these small timescales, traffic is extremely bursty, load is relatively unbalanced, and different applications exhibit significantly different behavior. We hope these findings will inform future data center network measurement and design.

### References

[References listed as provided, with proper formatting and citation.]

This revised text is more structured, clear, and professional, with improved coherence and readability.