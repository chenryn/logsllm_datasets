or 1 s). Unsurprisingly, Hadoop, with its longer flows, is less bal-
anced than the other two racks. This imbalance can sometimes be
large at small timescales, with the Hadoop racks’ 90th percentile
showing an average deviation of 100%. Even in the median case,
all three types of racks had a MAD of over 25%, indicating that
IMC ’17, November 1–3, 2017, London, United Kingdom
Q. Zhang et al.
Figure 7: CDF of the mean absolute deviation (MAD) of uplink utilization within a given sampling period. A deviation of 0 means that the
uplinks are perfectly balanced. We show both egress and ingress directions, as well as 1 s and 40 µs granularities.
(a) Egress
(b) Ingress
(a) Web Rack
(b) Cache Rack
(c) Hadoop Rack
Figure 8: Heatmap of Pearson correlation coefficients for servers in the same rack. We measured the ToR-to-server utilization of three repre-
sentative racks at a granularity of 250 µs.
6.2 Correlation between servers
One might also expect, with ideal application and Layer-4 load bal-
ancing, that downlink utilization is balanced. As above, the reality
is a bit more nuanced and is heavily dependent on the type of the
rack in question. To factor out differences in the absolute amount of
traffic going to each server, we show in Fig. 8 Pearson correlation
coefficients, which track the linear correlation between pairs of
servers. Ingress and egress trends were almost identical, so we only
show the ToR-to-server direction.
For Web racks, there is almost no correlation. Diurnal patterns
and flash crowds have been shown to cause correlation at longer
timescales, but at small timescales, the effect of those factors are
not easily discernible. Instead, because Web servers run stateless
services that are entirely driven by user requests, correlation is
close to zero. For Hadoop, there is some amount of correlation,
but it is modest at these timescales. The Cache rack exhibits very
different behavior from the other two types of racks. Subsets of the
Cache servers show very strong correlation with one another. This
is due to the fact that their requests are initiated in groups from
web servers. As such, those subsets are potentially involved in the
same scatter-gather requests.
Figure 9: Uplink/downlink share of hot ports given 300 µs sampling.
flow-level load balancing can often be inefficient in the short term.
Any slow-reacting load balancing approach (e.g., WCMP) would
not resolve the issue as, at the moderately longer timescale of 1 s,
the links appear to be balanced.
The interconnect does not add significant variance. After travers-
ing the interconnect, traffic entering the ToR switch exhibits a sim-
ilar pattern. We can observe the MAD of ingress traffic in Fig. 7b.
While there are changes in dispersion, they are relatively small
indicating that the network, with its current structure and load
levels, is not adding additional variance between flows. Prior work
has indicated that imbalance becomes significantly worse in the
presence of asymmetry caused by failures [1, 11], but we were not
able to intercept such cases for the racks we measured.
High-Resolution Measurement of Data Center Microbursts
IMC ’17, November 1–3, 2017, London, United Kingdom
(a) Web
(b) Cache
(c) Hadoop
Figure 10: Normalized peak shared buffer occupancy versus number of hot ports for web/cache/hadoop racks at a 300 µs granularity.
6.3 Directionality of bursts
Having examined both uplinks and downlinks separately, we now
look at their relative behavior. Fig. 9 shows for each rack type the
relative frequency of hot uplinks/downlinks.
We find that, for Web and Hadoop racks, there is a significant
bias toward servers as opposed to toward uplinks. Only 18% of hot
Hadoop samples were for uplinks, with Web uplinks responsible
for an even lower share. For these racks, bursts tend to be a result
of high fan-in where many servers send to a single destination.
Cache servers show the opposite trend, with most bursts occur-
ring on the uplinks. This can be attributed to two properties of
Cache servers: (1) that they exhibit a simple response-to-request
communication pattern, and (2) that cache responses are typically
much larger than the requests. Thus, Cache servers will almost
always send more traffic than they receive. Combined with modest
oversubscription at the ToR layer (at a ratio of 1:4), the communi-
cation bottleneck for these racks lies in their ToRs’ uplinks.
6.4 Effect of µbursts on shared buffers
Finally, we examine the effect of synchronized bursts on ToRs’
shared buffer utilization. Fig. 10 depicts a boxplot of the peak buffer
occupancy during a 50 ms interval versus the number of hot ports
during that same span. Granularity of these measurements was
lower than others because of a relatively inefficient interface to
poll the shared buffer utilization. As mentioned, buffer carving
is dynamic so, for simplicity, we normalize the occupancy to the
maximum value we observed in any of our data sets. We note that
drops can occur at much lower buffer utilization because of these
effects.
As expected, Hadoop racks put significantly more stress on ToR
buffers than either Web or Cache racks. This manifests in a few
different ways. First, we observed that Hadoop sometimes drove
100% of its ports to > 50% utilization. Web and Cache only drove
a maximum of 71% and 64% of their ports to simultaneous high
utilization within the observation period. Further, Hadoop expe-
riences high standing buffer occupancy compared to Web/Cache,
and the buffer occupancy scales with the number of hot ports more
drastically than in Web/Cache.
In all cases, average occupancy levels off for high numbers of
hot ports, possibly due to self selection of communication patterns
or the effect where buffer requirements scale sublinearly with the
number of TCP connections [5].
7 DATA CENTER DESIGN IMPLICATIONS
Our measurements of a production data center point to the need
for fine-grained measurement in order to truly understand network
behavior. This has implications not only for network measurement,
but also the evaluation of new protocols and architectures.
It also has implications for the design of those protocols and
architectures. While domain knowledge suggests that application-
level demand and traffic patterns are a significant contributor to
bursts, the data does not explicitly point to a cause2. Regardless,
the fact remains that µbursts both exist and are responsible for the
majority of congestion in the measured production data center. We
discuss some of the implications of that observation below.
Implications for load balancing. Many recent proposals sug-
gest load balancing on microflows rather than 5-tuples—essentially
splitting a flow as soon as the inter-packet gap is long enough to
guarantee no reordering. While our framework does not measure
inter-packet gaps directly, we note that most observed inter-burst
periods exceed typical end-to-end latencies (Sec. 5.2) and that non-
burst utilization is low (Sec. 5.4). The caveat is that different applica-
tions can have significantly different behavior, and faster networks
may decrease the gaps relative to the reordering constraints.
Implications for congestion control. Traditional congestion con-
trol algorithms either react to packet drops, RTT variation [13] or
ECN [2] as a congestion signal. All of these signals require at least
RTT/2 to arrive at the sender, and the protocols can potentially take
many RTTs to adapt. Unfortunately, our measurements show that
a large number of µbursts are shorter than a single RTT. Buffering
can handle momentary congestion, but if buffers become compar-
atively smaller or initial sending rates become more aggressive,
lower-latency congestion signals may be required.
Implications for pacing. TCP pacing was one of the original
mechanisms that prevented bursty traffic. Over time, however, it
has been rendered ineffective through features like segmentation
offload and interrupt coalescing. The results presented in this paper
give some insight into the degree of the problem in practice. They
may point to the importance of recent pacing proposals at either the
hardware [3] and software [13] levels. Even so, existing protocols
mostly deal with single-flow or single-machine pacing.
2Doing so would require correlating and synchronizing switch and end hosts measure-
ments at a microsecond level, which was not feasible in our current deployment.
IMC ’17, November 1–3, 2017, London, United Kingdom
Q. Zhang et al.
8 CONCLUSION
As network bandwidth continues to rise in data centers, the timescale
of network events will decrease accordingly. Thus, it is essential to
understand the behavior of these networks at high-resolution. This
is particularly true as we find that most bursts of traffic are µbursts,
i.e., that they occur at a microsecond-level granularity. Our results
show that at these small timescales, traffic is extremely bursty, load
is relatively unbalanced, and that different applications have signif-
icantly different behavior. We hope that these findings will inform
future data center network measurement and design.
REFERENCES
[1] Mohammad Alizadeh, Tom Edsall, Sarang Dharmapurikar, Ramanan
Vaidyanathan, Kevin Chu, Andy Fingerhut, Vinh The Lam, Francis Ma-
tus, Rong Pan, Navindra Yadav, and George Varghese. 2014. CONGA: Distributed
Congestion-aware Load Balancing for Datacenters. In Proceedings of the 2014
ACM Conference on SIGCOMM (SIGCOMM ’14). ACM, New York, NY, USA,
503–514. https://doi.org/10.1145/2619239.2626316
[2] Mohammad Alizadeh, Albert Greenberg, David A. Maltz, Jitendra Padhye,
Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, and Murari Sridharan. 2010.
Data Center TCP (DCTCP). In Proceedings of the ACM SIGCOMM 2010 Confer-
ence on Data Communication (SIGCOMM ’10). ACM, New York, NY, USA, 63–74.
https://doi.org/10.1145/1851182.1851192
[3] Mohammad Alizadeh, Abdul Kabbani, Tom Edsall, Balaji Prabhakar, Amin Vahdat,
and Masato Yasuda. 2012. Less Is More: Trading a Little Bandwidth for Ultra-Low
Latency in the Data Center. In Proceedings of the 9th USENIX Symposium on
Networked Systems Design and Implementation (NSDI 12). USENIX, San Jose, CA,
253–266.
[4] Alexey Andreyev. 2014.
Introducing data center fabric, the next-generation
Facebook data center network. https://code.facebook.com. (Nov. 2014).
[5] Guido Appenzeller, Isaac Keslassy, and Nick McKeown. 2004. Sizing Router
Buffers. In Proceedings of the 2004 ACM SIGCOMM Conference on Data Communi-
cation (SIGCOMM ’04). ACM, New York, NY, USA, 281–292. https://doi.org/10.
1145/1015467.1015499
[6] Theophilus Benson, Aditya Akella, and David A. Maltz. 2010. Network Traffic
Characteristics of Data Centers in the Wild. In Proceedings of the 10th ACM
SIGCOMM Conference on Internet Measurement (IMC ’10). ACM, New York, NY,
USA, 267–280. https://doi.org/10.1145/1879141.1879175
[7] J. Case, Mundy R., Partain D., and Stewart B. 2002. Introduction and Applicability
Statements for Internet Standard Management Framework. (2002). https://tools.
ietf.org/html/rfc3410.
[8] Daniel Halperin, Srikanth Kandula, Jitendra Padhye, Paramvir Bahl, and David
Wetherall. 2011. Augmenting Data Center Networks with Multi-gigabit Wireless
Links. In Proceedings of the ACM SIGCOMM 2011 Conference on Data Commu-
nication (SIGCOMM ’11). ACM, New York, NY, USA, 38–49. https://doi.org/10.
1145/2018436.2018442
[9] Srikanth Kandula, Sudipta Sengupta, Albert Greenberg, Parveen Patel, and Ronnie
Chaiken. 2009. The Nature of Data Center Traffic: Measurements & Analysis. In
Proceedings of the 9th ACM SIGCOMM Conference on Internet Measurement (IMC
’09). ACM, New York, NY, USA, 202–208. https://doi.org/10.1145/1644893.1644918
[10] Changhoon Kim, Anirudh Sivaraman, Naga Katta, Antonin Bas, Advait Dixit,
and Lawrence J Wobker. 2015. In-band network telemetry via programmable
dataplanes. SIGCOMM Demo (2015).
[11] Vincent Liu, Daniel Halperin, Arvind Krishnamurthy, and Thomas Anderson.
2013. F10: A Fault-tolerant Engineered Network. In Proceedings of the 10th USENIX
Conference on Networked Systems Design and Implementation (NSDI’13). USENIX
Association, Berkeley, CA, USA, 399–412. http://dl.acm.org/citation.cfm?id=
2482626.2482665
[12] Zaoxing Liu, Antonis Manousis, Gregory Vorsanger, Vyas Sekar, and Vladimir
Braverman. 2016. One Sketch to Rule Them All: Rethinking Network Flow
Monitoring with UnivMon. In Proceedings of the 2016 ACM SIGCOMM Conference
on Data Communication (SIGCOMM ’16). ACM, New York, NY, USA, 101–114.
https://doi.org/10.1145/2934872.2934906
[13] Radhika Mittal, Vinh The Lam, Nandita Dukkipati, Emily Blem, Hassan Wassel,
Monia Ghobadi, Amin Vahdat, Yaogong Wang, David Wetherall, and David Zats.
2015. TIMELY: RTT-based Congestion Control for the Datacenter. In Proceedings
of the 2015 ACM Conference on Data Communication (SIGCOMM ’15). ACM, New
York, NY, USA, 537–550. https://doi.org/10.1145/2785956.2787510
[14] Masoud Moshref, Minlan Yu, Ramesh Govindan, and Amin Vahdat. 2015.
SCREAM: Sketch Resource Allocation for Software-defined Measurement. In
Proceedings of the 11th ACM Conference on Emerging Networking Experiments
and Technologies (CoNEXT ’15). ACM, New York, NY, USA, 14:1–14:13. https:
//doi.org/10.1145/2716281.2836099
[15] Rajesh Nishtala, Hans Fugal, Steven Grimm, Marc Kwiatkowski, Herman Lee,
Harry C. Li, Ryan McElroy, Mike Paleczny, Daniel Peek, Paul Saab, David Stafford,
Tony Tung, and Venkateshwaran Venkataramani. 2013. Scaling Memcache at
Facebook. In Proceedings of the 10th USENIX Conference on Networked Systems
Design and Implementation (NSDI’13). USENIX Association, Berkeley, CA, USA,
385–398. http://dl.acm.org/citation.cfm?id=2482626.2482663
[16] P. Phaal, S. Panchen, and N. McKee. 2001. InMon Corporation’s sFlow: A Method
for Monitoring Traffic in Switched and Routed Networks. RFC 3176 (Informa-
tional). (2001).
[17] Jeff Rasley, Brent Stephens, Colin Dixon, Eric Rozner, Wes Felter, Kanak Agarwal,
John Carter, and Rodrigo Fonseca. 2014. Planck: Millisecond-scale Monitoring
and Control for Commodity Networks. In Proceedings of the 2014 ACM Conference
on SIGCOMM (SIGCOMM ’14). ACM, New York, NY, USA, 407–418. https://doi.
org/10.1145/2619239.2626310
[18] Arjun Roy, Hongyi Zeng, Jasmeet Bagga, George Porter, and Alex C. Snoeren.
2015. Inside the Social Network’s (Datacenter) Network. In Proceedings of the 2015
ACM Conference on Special Interest Group on Data Communication (SIGCOMM
’15). ACM, New York, NY, USA, 123–137. https://doi.org/10.1145/2785956.2787472
[19] Arjun Singh, Joon Ong, Amit Agarwal, Glen Anderson, Ashby Armistead, Roy
Bannon, Seb Boving, Gaurav Desai, Bob Felderman, Paulie Germano, Anand
Kanagala, Jeff Provost, Jason Simmons, Eiichi Tanda, Jim Wanderer, Urs Hölzle,
Stephen Stuart, and Amin Vahdat. 2015. Jupiter Rising: A Decade of Clos Topolo-
gies and Centralized Control in Google’s Datacenter Network. In Proceedings of
the 2015 ACM Conference on Data Communication (SIGCOMM ’15). ACM, New
York, NY, USA, 183–197. https://doi.org/10.1145/2785956.2787508
[20] Minlan Yu, Lavanya Jose, and Rui Miao. 2013. Software Defined Traffic Mea-
surement with OpenSketch. In Proceedings of the 10th USENIX Conference on
Networked Systems Design and Implementation (NSDI’13). USENIX Association,
Berkeley, CA, USA, 29–42. http://dl.acm.org/citation.cfm?id=2482626.2482631