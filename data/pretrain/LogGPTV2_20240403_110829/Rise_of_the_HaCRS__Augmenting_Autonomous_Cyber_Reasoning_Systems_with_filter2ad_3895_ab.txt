The Cyber Reasoning System. HaCRS is based on Mechanical
Phish, an open-source Cyber Reasoning System that was
created by Shellphish, the hacking team of the SecLab of UC
Santa Barbara, and competed in the DARPA Cyber Grand
Challenge [26, 27]. Shellphish designed Mechanical Phish
as a set of discrete components, providing individual anal-
ysis tasks, united by a central component that handles the
“overarching intelligence” [27]. This makes it straightforward
(though, unfortunately, non-trivial) to extend Mechanical
Phish with other analysis techniques, such as tasklet dis-
patching.
To the interested reader, we describe the relevant design
details of Mechanical Phish in Section 4.
Human-Automation Link. We extend Mechanical Phish to re-
quest assistance, from non-expert humans, in principled
ways.
The prototype action that we explore in this paper is input
generation. In input generation, input test cases are created
through both automated and human-assisted techniques to
form a base set of test cases to use in vulnerability discov-
ery. We describe this task, the conveyance of task-specific
information in a human-friendly format, and the use of the
results in our Human-assisted Cyber Reasoning System in
Section 5.
Next, we will discuss relevant details of Mechanical Phish before
delving into the details of our tasklets. After this, we will evaluate
human performance in the execution of these tasklets against au-
tomated alternatives derived from the state-of-the-art in program
analysis.
4 THE CYBER REASONING SYSTEM
We based our implementation on the Mechanical Phish, the Cy-
ber Reasoning System developed for the Cyber Grand Challenge
and open-sourced by our team (Shellphish) [26]. While Mechani-
cal Phish is composed of modules that are spread over more than
30 different source code repositories, the core design is (or attempts
to be) fairly straightforward [26].
In this section, we will describe Mechanical Phish in terms of
the computation framework discussed in Section 2.2. First, we will
discuss the type of software that Mechanical Phish is designed to
analyze. Then, we split the existing design into the Organization
Agent, Innovation Agent, and Selection Agent, as defined in Sec-
tion 2. Afterwards, in the next section, we will detail our extensions
on top of Mechanical Phish, and the specific points at which we
insert human interaction.
4.1 Program Analysis Targets
Mechanical Phish was built for participation in the Cyber Grand
Challenge. The Cyber Grand Challenge used a custom operating
system, DECREE, to ease the implementation load on participants.
To simplify analysis tasks, DECREE supports software written with
a text-based interface, using seven system calls, roughly equivalent
to the Linux system calls exit, write, read, select, mmap, munmap,
and getrandom.
Aside from this simplified environment, DECREE places no re-
strictions on the complexity of the software itself. As such, ap-
plications written for the Cyber Grand Challenge vary widely in
complexity, from text-based video games to “Computer-aided de-
sign“ software to web servers, and provide significant challenges
to the current state-of-the-art in program analysis. Additionally, it
is important to stress that all analysis done by HaCRS takes place
on binaries, and thus functions without the semantic hints present
in source code.
4.2 Organization Agents
The Mechanical Phish is a state-less Cyber Reasoning System, where,
for each decision, all of the information available to Mechani-
cal Phish, such as the binaries to be analyzed and the currently-
available results of analysis components, is re-analyzed from scratch.
This was done in an attempt to reduce the complexity of the orga-
nizational components by freeing them from the requirement of
tracking their own prior decisions [26].
Mechanical Phish includes several organizational components:
Task Creator. The task creator analyzes currently available re-
sults and identifies tasks that should be created, and their
priorities. This component is actually a conglomeration of
individual, task-specific creators. Each task-specific creator
schedules its own tasks without input from other creators:
the only interaction between the creators of different tasks
happens when results of those tasks influence the current set
of analysis results (and, in turn, are used by the subsequent
tasks created by these creators).
Session B3:  Investigating AttacksCCS’17, October 30-November 3, 2017, Dallas, TX, USA350Task Scheduler. Each task is assigned a priority by its creator. The
task scheduler analyzes task priorities and available system
resources and determines which tasks to schedule.
Environment Interaction. In order to inject data into Mechani-
cal Phish, and submit the results, interaction with the envi-
ronment is required. This component handles the retrieval of
input into and exposure of output out of the system. While
in the CGC this interaction was very straightforward, Cyber
Reasoning Systems operating in other environments (for ex-
ample, in a real-world cyber warfare situation) might require
considerably complex agents for this task.
The first task that the system must carry out is the integration of
environment information (for example, which binaries are available
for analysis), after which the Innovation and Selection Agents can
run.
4.3 Selection Agents
The selection agents are responsible for the integration of the re-
sults that are produced by the innovation agents. However, the
Mechanical Phish does not make a distinction between the innova-
tion agents and the integration agents in most cases, though there
are several exceptions:
Vulnerability triaging. When crashes are identified by the vul-
nerability discovery component, they are triaged to deter-
mine the feasibility of transforming them into exploits. This
information is then used by the Task Creator to prioritize
exploitation tasks based on the crash.
Exploit selection. The exploits created by the Exploitation Agents
are checked against different variations of the target binaries
to verify that, for example, opponent systems did not patch
the vulnerability. Successful exploits are entered into the
database, to be submitted by the Environment Interaction
Agent.
Patch selection. Mechanical Phish implements a simple patch
selection criteria, preferring patches produced by advanced
(but more failure-prone) techniques over simple (but higher-
overhead) ones.
The results of these agents are used by the organizational com-
ponents to schedule further innovation tasks.
4.4 Innovation Agents
The tasks that are created and scheduled by the Organization Agents
are carried out by the innovation agents. Specifically, Mechanical
Phish includes the following agents:
Vulnerability discovery. Mechanical Phish uses a combination
of fuzzing and symbolic execution to analyze target bina-
ries. These are implemented as separate agents that interact
through cross-pollination of dynamic test cases. Specifically,
as proposed by Driller, a coverage-based fuzzer is used in
parallel with a symbolic tracing technique to produce inputs
that maximize code coverage [29].
Exploitation. Several different exploitation agents are used by
Mechanical Phish, depending on the types of vulnerabilities
that are discovered.
Patching. Mechanical Phish uses a complex patching agent, in
several different configurations, to patch the vulnerabilities
that it identifies in binary code.
These innovation agents process inputs and produce updates
to the system state. These updates are filtered through selection
agents before the system state accepts them.
4.5 Automated Vulnerability Discovery -
Fuzzing
The fuzzing approach in the Mechanical Phish is based on a muta-
tional fuzzer known as American Fuzzy Lop [38]. This approach
requires, as input, a set of test cases that exercise some functionality
in the target binary. The seed quality, in terms of how well they
exercise the target program, has a scaling effect on the effectiveness
of AFL: the more coverage these test cases provide, the more code
AFL will be able to explore by mutating them. Unfortunately, the
creation of high-quality test case seeds is a complicated problem,
and this is generally seen as a human-provided input into a system.
For example, lacking human input, Mechanical Phish simply seeds
its fuzzer with an input comprised of the word “fuzz.”
These seeds are then mutated to explore more and more of the
code base and increase the chance of triggering bugs. Eventually,
however, the fuzzer will get stuck and be unable to exercise new
paths through the code of the target program. This can happen for
a number of reasons, but is most frequently caused by the inability
of the fuzzer’s random mutations to satisfy complex conditions,
introduced by checks in the program, upon input data.
4.6 Automated Vulnerability Discovery -
Drilling
Driller proposed a mitigation for the stalling of the fuzzer due to the
inability to satisfy complex solutions [29]. It uses concolic execution
to trace the paths that the fuzzer finds, identifies conditional checks
that the fuzzer fails to satisfy, and synthesizes inputs to satisfy
these conditions. Driller triggers its operation when the fuzzer
gets “stuck”, and is unable to find further test cases (it detects this
by checking AFL’s progress evaluation heuristics). Once this stall
condition is detected, Driller symbolically traces and attempts to
mutate all test cases that AFL has found into test cases that reach
parts of code not previously seen. These resulting test cases are
then synchronized back into the fuzzer, so that it can explore newly-
reached areas of code.
By pairing fuzzing with concolic execution, Driller achieves
better results than the naive union of the individual underlying
techniques. However, Driller’s automated approach to symbolic
input synthesis has some drawbacks.
Driller’s synthesis works by diverting a path and forcing it to
satisfy a check that it would have otherwise avoided. There are
several limitations, inherent in Driller, that hamper its effectiveness
in certain situations. These include, but are not limited to:
SMT solver. Driller uses an SMT solver to solve negated path pred-
icates (constraints on the input values to the program that
must be satisfied in order to trigger the path in question) to
synthesize inputs that diverge from the original execution.
Session B3:  Investigating AttacksCCS’17, October 30-November 3, 2017, Dallas, TX, USA351However, depending on the complexity of the path predi-
cates involved, the SMT solving process may not terminate.
While this represents a significant challenge for Driller, the
complexity of these predicates might not translate to the
complexity of interaction with the software. If this is the
case, a human assistant might be able to controllably di-
vert the path taken through the program, even when the
constraint solver cannot.
Inflexible path predicates. Depending on implementation de-
tails in the program, earlier path predicates might prevent
the deviation of later path predicates. Such predicates are
frequently created by certain input transformation proce-
dures. For example, string-to-int translation (such as the
atoi function) takes different conditional branches, based
on the values in the input string, while converting an input
string to an integer. These conditional branches create path
predicates. Later, the program might perform some action
based on the value of this integer. When Driller attempts to
divert this decision to take a different action, the earlier path
predicates on the input string prevent this diversion.
Humans, of course, do not share this inflexible way of rea-
soning about path predicates.
Semantic transitions versus control flow transitions. Driller
cannot understand the program semantically, and simply at-
tempts to deviate the control flow of the program. A human,
on the other hand, can identify much more intricate semantic
deviations (for example, winning, as opposed to losing, a
game), allowing for the triggering of whole new areas of
code to deal with these new semantic settings.
These limitations conspire to erode Driller’s ability to produce
deviating inputs in many cases. In the next section, we will dis-
cuss how these limitations can be worked around with human
assistance.
5 HUMAN ASSISTANCE
As we discuss in the previous section, automated input synthesis
techniques suffer from limitations that cause them to eventually get
stuck in the exploration of a program. Even Driller, which leverages
the power of symbolic execution to divert test cases, is only a
partial solution. This is because, while Driller can make major
changes to the input test case it analyzes, it can only (by design
and fundamental limitation) achieve minor deviations.
On the other hand, a human can leverage intuition and a se-
mantic understanding of the target program to achieve very large
deviations, potentially allowing further analyses to continue to
make progress. In this paper, we explore the integration of human
assistance into a Cyber Reasoning System as Innovation agents,
keeping the Organizational and Selectional agents fully automated.
We focus on the vulnerability discovery stage of the analysis and
explore ways to integrate human effort to improve analysis effi-
ciency.
Human assistance takes place over an interface (the Human-
Automation Link, or HAL) which will be described later this section.
To maximize the effectiveness of this effort, HaCRS carries out a
number of analyses that enhance the data it is able to expose to
the humans. In this section, we describe how human assistants are
Concept
Symbolic Equations
Control-Flow Graph
Execution Path
I/O (Text)
Semantic Meaning
Computer
Expert Non-Expert
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
Table 1: Program analysis concepts, as they are easily understood
by automated techniques, expert humans, and non-expert humans.
To be understandable to non-experts, the Human-Automation Link
must avoid complex program analysis topics.
selected, the interface over which HaCRS and humans communicate,
and how the resulting data is used to enhance the vulnerability
detection ability of HaCRS.
5.1 Assistant Expertise
The style of human assistance differs according to the assistant’s
expertise level. For example, while HaCRS could reasonably ask an
expert human to analyze a control flow graph and identify potential
paths through it, a non-expert would be flabbergasted by such a
request. The information presented, and the interfaces which are
used, must be adapted to the chosen assistant’s level of expertise.
Since expert humans (i.e., binary analysts) are rare and expensive,
the integration of assistance from non-expert humans (i.e., an aver-
age internet citizen) is of particular interest. While they do not scale
to the extent of automated processes, non-expert humans scale con-
siderably easier than experts, due to their higher availability. When
more knowledge is required, semi-experts (i.e., graduate students
in Computer Science) can be leveraged more readily than experts.
Thus, in this paper, we focus mainly on techniques to integrate
non-expert assistance, with a detour into semi-expert assistants for
completion.
Over the decades that humans have been interacting with soft-
ware, the skill of performing such interaction has become gradually
instilled in the human population. As such, even non-experts are
well-trained to understand and drive computer software. Thus, we
can tailor HAL to non-experts by sticking to concepts that they can
grasp and avoiding complex program analysis concepts, as shown
in Table 1. For example, rather than “triggering transitions”, we
used the term “triggering functionality”, which requires less techni-
cal knowledge to understand. Additionally, we expose non-experts
only to the input and output log associated with prior interactions
with the programs that the HaCRS is trying to analyze, and avoid
any use of program analysis terms in task descriptions.
5.2 Human-assisted Input Generation
HaCRS uses human assistance to break through the “semantic bar-
riers” that limit the effectiveness of automated analyses described
in Sections 4.5 and 4.6. It gives its human assistants a goal: generate
an input test case that executes some amount of code in the tar-
get program that has not been reached by previously-known test
cases (i.e., those previously found by automated analyses or other
humans).
Session B3:  Investigating AttacksCCS’17, October 30-November 3, 2017, Dallas, TX, USA352Human assistants interact with the target program to generate
test cases, and these test cases are synchronized throughout HaCRS’
components.
Human-to-automation. Human-produced test cases are synchro-
nized to the automated program exploration components,
which proceed to mutate them in an attempt to trigger new
functionality.
Human-to-human. Humans can view and modify the test cases
produced by other human assistants. This enables a collec-
tive effort of the understanding and leveraging of program
semantics toward a higher code coverage.
Automation-to-human. The resulting automation-mutated test
cases can then be shown to the human assistants (we term
such a test case an “example test case”), who can review them,
understand possible further improvements and changes that
can be made, and relay those changes back to the automation
by producing human-modified test cases.
Test case conversion. The synchronization of test cases from au-
tomated components to a human assistant poses a challenge: auto-
mated systems, driven by either random input generation or input
synthesis via constraint solving, have no guarantee to produce
printable characters when the target program does not require
it. Non-printable test cases look like gibberish when shown to a
human, which hinders the human’s ability to reason semantically
about what actions the test case is causing the target program to
take.
To address this issue, we use the existing afl-tmin utility shipped
with AFL [38]. This utility is a test case minimizer. It takes an input
test case and uses lightweight dynamic techniques to a) remove
unnecessary input characters and b) convert as many characters
as possible to be printable, without changing the code coverage
achieved by the input. In practice, it achieves very good results on
programs with a text interface.
5.3 Automation-assisted Human Assistance
Simply presenting previously-discovered test cases to human assis-
tants enables an improvement over a base-case Cyber Reasoning
System (we show this in Section 6). However, since the communi-
cation between HaCRS and humans takes place over a well-defined