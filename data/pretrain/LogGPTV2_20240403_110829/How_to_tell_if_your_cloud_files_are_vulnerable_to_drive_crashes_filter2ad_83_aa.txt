title:How to tell if your cloud files are vulnerable to drive crashes
author:Kevin D. Bowers and
Marten van Dijk and
Ari Juels and
Alina Oprea and
Ronald L. Rivest
How to Tell if Your Cloud Files
Are Vulnerable to Drive Crashes
Kevin D. Bowers
RSA Laboratories
Cambridge, MA, USA
PI:EMAIL
Marten van Dijk
RSA Laboratories
Cambridge, MA, USA
PI:EMAIL
Ari Juels
RSA Laboratories
Cambridge, MA, USA
PI:EMAIL
Alina Oprea
RSA Laboratories
Cambridge, MA, USA
PI:EMAIL
Ronald L. Rivest
MIT CSAIL
Cambridge, MA, USA
PI:EMAIL
ABSTRACT
This paper presents a new challenge—verifying that a remote server
is storing a ﬁle in a fault-tolerant manner, i.e., such that it can sur-
vive hard-drive failures. We describe an approach called the Re-
mote Assessment of Fault Tolerance (RAFT). The key technique in
a RAFT is to measure the time taken for a server to respond to a
read request for a collection of ﬁle blocks. The larger the number
of hard drives across which a ﬁle is distributed, the faster the read-
request response. Erasure codes also play an important role in our
solution. We describe a theoretical framework for RAFTs and offer
experimental evidence that RAFTs can work in practice in several
settings of interest.
Categories and Subject Descriptors
E.3 [Data]: [Data Encryption]
General Terms
Security
Keywords
Cloud storage systems, auditing, fault tolerance, erasure codes
1.
INTRODUCTION
Cloud storage offers clients a uniﬁed view of a ﬁle as a single,
integral object. This abstraction is appealingly simple. In reality,
though, cloud providers generally store ﬁles/objects with redun-
dancy or error correction to protect against data loss. Amazon, for
example, claims that its S3 service stores three replicas of each ob-
ject1. Additionally, cloud providers often spread ﬁles across mul-
tiple storage devices. Such distribution provides resilience against
1Amazon has also recently introduced reduced redundancy storage
that promises less fault tolerance at lower cost
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’11, October 17–21, 2011, Chicago, Illinois, USA.
Copyright 2011 ACM 978-1-4503-0948-6/11/10 ...$10.00.
hardware failures, e.g., drive crashes (and can also lower latency
across disparate geographies).
The single-copy ﬁle abstraction in cloud storage, however, con-
ceals ﬁle-layout information from clients.
It therefore deprives
them of insight into the true degree of fault-tolerance their ﬁles
enjoy. Even when cloud providers specify a storage policy (e.g.,
even given Amazon’s claim of triplicate storage), clients have no
technical means of verifying that their ﬁles aren’t vulnerable, for
instance, to drive crashes. In light of clients’ increasingly critical
reliance on cloud storage for ﬁle retention, and the massive cloud-
storage failures that have come to light, e.g., [8], it is our belief that
remote testing of fault tolerance is a vital complement to contrac-
tual assurances and service-level speciﬁcations.
In this paper we develop a protocol for remote testing of fault-
tolerance for stored ﬁles. We call our approach the Remote Assess-
ment of Fault Tolerance (RAFT). A RAFT enables a client to obtain
proof that a given ﬁle F is distributed across physical storage de-
vices to achieve a certain desired level of fault tolerance. We refer
to storage units as drives for concreteness. For protocol parameter
t, our techniques enable a cloud provider to prove to a client that
the ﬁle F can be reconstructed from surviving data given failure
of any set of t drives. For example, if Amazon were to prove that
it stores a ﬁle F fully in triplicate, i.e., one copy on three distinct
drives, this would imply that F is resilient to t = 2 drive crashes.
At ﬁrst glance, proving that ﬁle data is stored redundantly, and
thus proving fault-tolerance, might seem an impossible task. It is
straightforward for storage service S to prove knowledge of a ﬁle
F , and hence that it has stored at least one copy. S can just transmit
F . But how can S prove, for instance, that it has three distinct
copies of F ? Transmitting F three times clearly doesn’t do the
trick! Even proving storage of three copies doesn’t prove fault-
tolerance: the three copies could all be stored on the same disk!
To show that F isn’t vulnerable to drive crashes, it is necessary
to show that it is spread across multiple drives. Our approach, the
Remote Assessment of Fault Tolerance, proves the use of multiple
drives by exploiting drives’ performance constraints—in particular
bounds on the time required for drives to perform challenge tasks.
A RAFT is structured as a timed challenge-response protocol. A
short story gives the intuition. Here, the aim is to ensure that a
pizza order can tolerate t = 1 oven failures.
A fraternity (“Eeta Pizza Pi”) regularly orders pizza
from a local pizza shop, “Cheapskate Pizza.” Recently
Cheapskate failed to deliver pizzas for the big pregame
party, claiming that their only pizza oven had suffered
501a catastrophic failure. They are currently replacing it
with two new BakesALot ovens, for increased capacity
and reliability in case one should fail.
Aim O’Bese, president of Eeta Pizza Pi, wants to
verify that Cheapskate has indeed installed redundant
pizza ovens, without having to visit the Cheapskate
premises himself. He devises the following clever ap-
proach. Knowing that each BakesALot oven can bake
two pizzas every ten minutes, he places an order for
two dozen pizzas, for delivery to the fraternity as soon
as possible. Such a large order should take an hour
of oven time in the two ovens, while a single oven
would take two hours. The order includes various un-
usual combinations of ingredients, such as pineapple,
anchovies, and garlic, to prevent Cheapskate from de-
livering warmed up pre-made pizzas.
Cheapskate is a ﬁfteen minute drive from the fra-
ternity. When Cheapskate delivers the two dozen piz-
zas in an hour and twenty minutes, Aim decides, while
consuming the last slice of pineapple/anchovy/garlic
pizza, that Cheapskate must be telling the truth. He
gives them the fraternity’s next pregame party order.
Our RAFT for drive fault-tolerance testing follows the approach
illustrated in this story. The client challenges the server to retrieve
a set of random ﬁle blocks from ﬁle F . By responding quickly
enough, S proves that it has distributed F across a certain, mini-
mum number of drives. Suppose, for example, that S is challenged
to pull 100 random blocks from F , and that this task takes one sec-
ond on a single drive. If S can respond in only half a second2, it is
clear that it has distributed F across at least two drives.
Again, the goal of a RAFT is for S to prove to a client that F is
recoverable in the face of at least t drive failures for some t. Thus
S must actually do more than prove that F is distributed across a
certain number of drives. It must also prove that F has been stored
with a certain amount of redundancy and that the distribution of F
across drives is well balanced. To ensure these two additional prop-
erties, the client and server agree upon a particular mapping of ﬁle
blocks to drives. An underlying erasure code provides redundancy.
By randomly challenging the server to show that blocks of F are
laid out on drives in the agreed-upon mapping, the client can then
verify resilience to t drive failures.
The real-world behavior of hard drives presents a protocol-design
challenge: The response time of a drive can vary considerably from
read to read. Our protocols rely in particular on timing measure-
ments of disk seeks, the operation of locating randomly accessed
blocks on a drive. Seek times exhibit high variance, with multiple
factors at play (including disk prefetching algorithms, disk internal
buffer sizes, physical layout of accessed blocks, etc.). To smooth
out this variance we craft our RAFTs to sample multiple randomly
selected ﬁle blocks per drive. Clients not only check the correct-
ness of the server’s responses, but also measure response times and
accept a proof only if the server replies within a certain time inter-
val.
We propose and experimentally validate on a local system a RAFT
that can, for example, distinguish between a three-drive system
2Of course, S can violate our assumed bounds on drive perfor-
mance by employing unexpectedly high-grade storage devices,
e.g., ﬂash storage instead of rotational disks. As we explain below,
though, our techniques aim to protect against economically rational
adversaries S. Such an S might create substandard fault tolerance
to cut costs, but would not employ more expensive hardware just
to circumvent our protocols. (More expensive drives often mean
higher reliability anyway.)
with fault tolerance t = 1 and a two-drive system with no fault tol-
erance for ﬁles of size at least 100MB. Additionally, we explore the
feasibility of the RAFT protocol on the Mozy cloud backup system
and conﬁrm that Mozy is resilient to at least one drive failure. We
conclude that RAFT protocols are most applicable to test fault tol-
erance for large ﬁles in an archival or backup setting in which ﬁles
are infrequently accessed and thus there is limited drive contention.
Our RAFT protocol presented in this paper is designed for tra-
ditional storage architectures that employ disk-level replication of
ﬁles and use hard disk drives (HDDs) as the storage medium. While
these architectures are still prevalent today, there are many settings
in which our current protocol design is not directly applicable. For
instance, the characteristics of HDD’s sequential and random ac-
cess do not hold for SSD drives or RAM memory, which could po-
tentially be used for performance-sensitive workloads in systems
employing multi-tier storage (e.g., [1, 28]). Systems with data lay-
out done at the block level (as opposed to ﬁle-level layout) are
also not amenable to our current design, as in that setting timing
information in our challenge-response protocol does not directly
translate into fault tolerance. Examples of architectures with block-
level data layout are chunk-based ﬁle systems [14,17], and systems
with block-level de-duplication [7]. Other features such as spinning
disks down for power savings, or replicating data across different
geographical locations complicate the design of a RAFT-like proto-
col. Nevertheless, we believe that our techniques can be adapted to
some of these emerging architectures and we plan to evaluate this
further in future work.
RAFTs aim primarily to protect against “economically rational”
service providers/adversaries, which we deﬁne formally below. Our
adversarial model is thus a mild one. We envision scenarios in
which a service provider agrees to furnish a certain degree of fault
tolerance, but cuts corners. To reduce operating costs, the provider
might maintain equipment poorly, resulting in unremediated data
loss, enforce less ﬁle redundancy than promised, or use fewer drives
than needed to achieve the promised level of fault tolerance. (The
provider might even use too few drives accidentally, as virtualiza-
tion of devices causes unintended consolidation of physical drives.)
An economically rational service provider, though, only provides
substandard fault tolerance when doing so reduces costs. The provider
does not otherwise, i.e., maliciously, introduce drive-failure vulner-
abilities. We explain later, in fact, why protection against malicious
providers is technically infeasible.
1.1 Related work
Proofs of Data Possession (PDPs) [2] and Proofs of Retrievabil-
ity (PORs) [9, 10, 21, 31] are challenge-response protocols that ver-
ify the integrity and completeness of a remotely stored F . They
share with our work the idea of combining error-coding with ran-
dom sampling to achieve a low-bandwidth proof of storage of a ﬁle
F . This technique was ﬁrst advanced in a theoretical framework
in [27]. Both [23] and [4] remotely verify fault tolerance at a logi-
cal level by using multiple independent cloud storage providers. A
RAFT includes the extra dimension of verifying physical layout of
F and tolerance to a number of drive failures at a single provider.
Cryptographic challenge-response protocols prove knowledge of
a secret—or, in the case of PDPs and PORs, knowledge of a ﬁle.
The idea of timing a response to measure remote physical resources
arises in cryptographic puzzle constructions [11]. For instance,
a challenge-response protocol based on a moderately hard com-
putational problems can measure the computational resources of
clients submitting service requests and mitigate denial-of-service
attacks by proportionately scaling service fulﬁlment [20]. Our pro-
tocols here measure not computational resources, but the storage
502resources devoted to a ﬁle. (Less directly related is physical dis-
tance bounding, introduced in a cryptographic setting in [6]. There,
packet time-of-ﬂight gives an upper bound on distance.)
We focus on non-Byzantine, i.e., non-malicious, adversarial mod-
els. We presume that malicious behavior in cloud storage providers
is rare. As we explain, such behavior is largely irremediable any-
way. Instead, we focus on an adversarial model (“cheap-and-lazy”)
that captures the behavior of a basic, cost-cutting or sloppy storage
provider. We also consider an economically rational model for the
provider. Most study of economically rational players in cryptogra-
phy is in the multiplayer setting, but economical rationality is also
implicit in some protocols for storage integrity. For example, [2,15]
verify that a provider has dedicated a certain amount of storage to
a ﬁle F , but don’t strongly assure ﬁle integrity. We formalize the
concept of self-interested storage providers in our work here.
A RAFT falls under the broad heading of cloud security assu-
rance. There have been many proposals to verify the security char-
acteristics and conﬁguration of cloud systems by means of trusted
hardware, e.g., [13]. Our RAFT approach advantageously avoids
the complexity of trusted hardware. Drives typically don’t carry
trusted hardware in any case, and higher layers of a storage subsys-
tem can’t provide the physical-layer assurances we aim at here.
1.2 Organization
Section 2 gives an overview of key ideas and techniques in our
RAFT scheme. We present formal adversarial and system models
in Section 3. In Section 4, we introduce a basic RAFT in a sim-
ple system model. Drawing on experiments, we reﬁne this system
model in Section 5, resulting in a more sophisticated RAFT which
we validate against the Mozy cloud backup service in Section 6. In
Section 7, we formalize an economically rational adversarial model
and sketch matching RAFT constructions. We conclude in Sec-
tion 8 with discussion of future directions.
2. OVERVIEW: BUILDING A RAFT
We now discuss in further detail the practical technical chal-
lenges in building a RAFT for hard drives, and the techniques we
use to address them. We view the ﬁle F as a sequence of m blocks
of ﬁxed size (e.g., 64KB).
File redundancy / erasure coding. To tolerate drive failures, the
ﬁle F must be stored with redundancy. A RAFT thus includes an
initial step that expands F into an n-block erasure-coded represen-
tation G. If the goal is to place ﬁle blocks evenly across c drives to
tolerate the failure of any t drives, then we need n = mc/(c − t).
Our adversarial model, though, also allows the server to drop a por-
tion of blocks or place some blocks on the wrong drives. We show
how to parameterize our erasure coding at a still higher rate, i.e.,
choose a larger n, to handle these possibilities.
Challenge structure. (“What order should Eeta Pizza Pie place to
challenge Cheapskate?”) We focus on a “layout speciﬁed” RAFT,
one in which the client and server agree upon an exact placement of
the blocks of G on c drives, i.e., a mapping of each block to a given
drive. The client, then, challenges the server with a query Q that
selects exactly one block per drive in the agreed-upon layout. An
honest server can respond by pulling exactly one block per drive
(in one “step”). A cheating server, e.g., one that uses fewer than c
drives, will need at least one drive to service two block requests to
fulﬁll Q, resulting in a slowdown.
Network latency. (“What if Cheapskate’s delivery truck runs into
trafﬁc congestion?”) The network latency, i.e., roundtrip packet-
travel time, between the client and server, can vary due to changing
network congestion conditions. The client cannot tell how much a
response delay is due to network conditions and how much might
be due to cheating by the server. Based on the study by Lumezanu
et al [24] and our own small-scale experiments, we set an upper
bound threshold on the variability in latency between a challenging
client and a storage service. We consider that time to be “free time”
for the adversary, time in which the adversary can cheat, prefetch-
ing blocks from disk or perform any other action that increases his
success probability in the challenge-response protocol. We design
our protocol to be resilient to a bounded amount of “free time”
given to the adversary.
Drive read-time variance. (“What if the BakesALot ovens bake
at inconsistent rates?”) The read-response time for a drive varies
across reads. We perform experiments, though, showing that for
a carefully calibrated ﬁle-block size, the response time follows a
probability distribution that is stable across time and physical ﬁle
positioning on disk. (We show how to exploit the fact that a drive’s
“seek time” distribution is stable, even though its read bandwidth