0.00
ISP
S
A
-0.67* 0.00
0.00* 0.00
0.00 1.00
Server Hacker
S
0.00
0.00
n/a
A
0.00
0.00
0.00
Government
S
A
-1.00* -0.33
0.00
0.00
n/a
0.00
Unlocked
A
S
0.00 0.00
0.00 0.00
n/a 0.00
Malware
A
S
0.67* 0.00
0.00 0.00
n/a
0.00
Table 4: Location-shift estimates for MWU test results. The left column for each adversary-capability compares Control to Long in the survey
study; the right column compares Control to Experimental in the app study. * indicates a statistically signiﬁcant result (p < 0.05).
adversaries in Table 4) also appear to have negative eﬀects
(the correct direction) in the app study. Although encouraging,
it is not clear if this is a real eﬀect that is too small for our
experiment to conﬁrm or just noise. Further research would
be needed to validate these trends.
Similar to the survey study, we observe ceiling eﬀects with
not-e2e encryption and metadata capabilities with most ad-
versaries11.
Deﬁnitions of e2e encryption are high-level but mainly
correct We asked interview participants to deﬁne e2e
encryption. The most common response conveyed that only
the sender and intended recipient could view the content of the
message (n=8), as exempliﬁed by P13: “A message that you
send out is encrypted and the only person who can unencrypt
it to read it would be the receiver of the message.” Other deﬁ-
nitions diﬀered slightly by emphasizing who could not view
(n=5), alter (n=5), or otherwise intercept (n=4) the message.
Three participants used the key-lock metaphor we described
in the s.mech and Long messages, and three mentioned the
metadata weakness detailed in s.meta. Only two participants
said they were not sure what e2e encryption meant.
Recognition of protection from non-endpoint threats
When asked to explain what e2e encryption protects against,
about half of interview participants (n=10) generally de-
scribed it as eﬀective against non-endpoint adversaries. P23,
for example, said, “Probably anyone who would interrupt or
interfere in between the messaging, in between where you
sent it and someone else received it.”
Participants also frequently mentioned protection from ad-
versaries highlighted in the educational messages and/or the
communications privacy questionnaire. Four mentioned a
server hacker, four mentioned the ISP, and three mentioned
the government adversary. Three mentioned ambiguous ad-
versaries such as “hackers,” and three incorrectly suggested
e2e encryption would protect against malware.
e2e encryption weaknesses were less clear When asked
what e2e encryption does not protect against, participants
again mentioned adversaries we described in the educational
messages and questionnaires. Most mentioned an unlocked
phone (n=14); in both studies, we found that participants
largely started with a correct mental model for this before our
11These are illustrated in the extended paper (see Appendix D).
intervention. A few participants mentioned the government
(n=4), an app company employee (n=2), or a server hacker
(n=1). Three speciﬁcally noted that e2e encryption could not
protect against all “hackers.” In total, nine of 19 participants
gave answers that at least in part contradicted the principles
we attempted to convey in the educational messages. As one
example, P11 said, “The company essentially has access to
it. They don’t necessarily look at it, but if the proper legal
methods are observed, there is a chance that someone else
might be able to see it, for instance, the government.”
7 Discussion
Our educational messages were eﬀective in isolation, but
when embedded into app workﬂows they did not show sta-
tistically signiﬁcant eﬀects. This is likely related to the fact
that although most participants noticed the messages, many
ignored their contents, possibly out of habituation to infor-
mational messages generally. The diﬀerence may also reﬂect
short-term recall in the survey study compared to longer-term
recall in the app study. Overall, this suggests messages like
ours may need to be somewhat more intrusive to be useful.
Educational intervention works, to an extent
In the sur-
vey study, our educational interventions worked reasonably
well, with minimal unintended consequences. In line with
our prior work [9], participants easily grasped core principles
related to conﬁdentiality (measured via our interception ca-
pability). To some extent, participants gained understanding
about metadata weaknesses. However, we did ﬁnd some evi-
dence in the app study that our intervention may have oversold
the capabilities of e2e encryption with respect to metadata.
We also found evidence that many participants already
possessed strong mental models with respect to risks of not-
e2e encryption communications and risks of physical access at
endpoints. These ﬁndings reﬂect somewhat more knowledge
than was observed in prior work [2, 3, 17] — this may reﬂect
diﬀerences in study populations, or that users are learning
as they gain exposure to e2e encrypted apps over time. We
argue that where participants have correct models like these,
educational interventions should reinforce them.
Interventions may need to be more intrusive
Unfortu-
nately, we were unable to replicate the successes of the survey
USENIX Association
30th USENIX Security Symposium    459
study in a more realistic in-workﬂow context. We mainly
attribute this diﬀerence to the messages failing to attract suﬃ-
cient user attention in this more realistic setting.
However, quantitative and qualitative results suggest that
participants did not ﬁnd our interventions intrusive or unus-
able; thus, there may be room to make such interventions
more noticeable without triggering an undue amount of user
annoyance. As one example, we decided during pilot testing
to remove the splash-screen message element that participants
found somewhat disruptive; in hindsight, we hypothesize that
this might have struck a better balance between usability
and noticeability. Other modiﬁcations could include making
the educational messages bigger or bolder, highlighting key
phrases, or using graphics to make them more eye-catching.
Future work should explore whether changes like these can
achieve better results without signiﬁcant harm to usability.
Experimental setup
The discrepancy between the two
studies could also be attributed to the diﬀering experimental
setups. The survey study involved one intervention with ques-
tions, on average, less than 10 minutes later. The app study
involved 20 days of participation with interventions every
1-2 days. Thus, we might have measured short-term recall of
educational material with the survey study, vs. longer-term
impact on mental models with the app study. On the other
hand, prior work provides some evidence that security nudg-
ing surveys can have longer term impact [55]. Additional
controlled experiments would be needed to know to what
extent our survey study had lasting impact on mental models.
Other kinds of interventions
Our results also underscore
that in-workﬂow messages are only one way to inﬂuence
mental models of secure communication. As in our prior
work [8, 9], when our participants were focused on our educa-
tional content, they did learn functional information. While
it is not realistic to expect most users to seek out training
on secure communication, this result bolsters the importance
of making well-designed educational materials available to
those who do seek them out. Organizations like EFF and the
Library Freedom Project [20,42] have developed several such
materials; future work should consider evaluating where they
succeed and whether improvements can be made.
Further, there is increasing emphasis on teaching everyday
privacy and security concepts in elementary and secondary
schools [35]. Including functional models of secure commu-
nication in these curricula could help these students, as they
grow up, to make appropriate choices about their communi-
cations mechanisms in an increasingly networked world.
8 Conclusion
In this work, we created educational messages to improve
functional mental models of e2e encryption and evaluated
them in both a controlled and a more realistic setting. We ﬁnd
that conveying functional mental models of e2e encryption is
possible in isolation, but we hypothesize in-app nudging may
require more intrusiveness to be eﬀective; more experiments
are needed.
9 Acknowledgements
We thank our participants. This material is based upon work
supported by the United States Air Force and DARPA under
Contract No FA8750-16-C-0022. Any opinions, ﬁndings and
conclusions or recommendations expressed in this material
are those of the authors and do not necessarily reﬂect the
views of the United States Air Force and DARPA.
References
[1] Whatsapp hack: Is any app or computer truly secure? BBC,
2019.
[2] Ruba Abu-Salma, Elissa M Redmiles, Blase Ur, and Miranda
Wei. Exploring User Mental Models of End-to-End Encrypted
Communication Tools. In FOCI, 2018.
[3] Ruba Abu-Salma, M Angela Sasse, Joseph Bonneau, Anastasia
Danilova, Alena Naiakshina, and Matthew Smith. Obstacles
to the adoption of secure communication tools. In IEEE S&P,
2017.
[4] Alessandro Acquisti, Idris Adjerid, Rebecca Balebako, Laura
Brandimarte, Lorrie Faith Cranor, Saranga Komanduri, Pe-
dro Giovanni Leon, Norman Sadeh, Florian Schaub, Manya
Sleeper, Yang Wang, and Shomir Wilson. Nudges for Privacy
and Security: Understanding and Assisting Users’ Choices
Online. ACM Comput. Surv., 50(3), August 2017.
[5] Hazim Almuhimedi, Florian Schaub, Norman Sadeh, Idris Ad-
jerid, Alessandro Acquisti, Joshua Gluck, Lorrie Faith Cranor,
and Yuvraj Agarwal. Your Location Has Been Shared 5,398
Times! A Field Study on Mobile App Privacy Nudging. In
CHI, 2015.
[6] Apple Inc. Privacy - Approach to Privacy. (Last accessed on
Sep. 2019).
[7] Farzaneh Asgharpour, Debin Liu, and L. Jean Camp. Mental
Models of Security Risks. In USEC, 2007.
[8] Wei Bai, Moses Namara, Yichen Qian, Patrick Gage Kelley,
Michelle L Mazurek, and Doowon Kim. An Inconvenient
Trust: User Attitudes Toward Security and Usability Tradeoﬀs
for Key-Directory Encryption Systems. In SOUPS, 2016.
[9] Wei Bai, Michael Pearson, Patrick Gage Kelley, and Michelle L
Mazurek. Improving Non-Experts’ Understanding of End-to-
End Encryption: An Exploratory Study. In EuroUSEC, 2020.
[10] James Ball. Nsa collects millions of text messages daily in
’untargeted’ global sweep. The Guardian, 2014.
[11] John Brooke. SUS: a Quick and Dirty Usability Scale. In
Usability Evaluation in Industry. CRC press, 1996.
[12] José Carlos Brustoloni and Ricardo Villamarín-Salomón. Im-
proving Security Decisions with Polymorphic and Audited
Dialogs. In SOUPS, 2007.
460    30th USENIX Security Symposium
USENIX Association
[13] L. J. Camp. Mental Models of Privacy and Security. IEEE
Technology and Society Magazine, 28(3):37–46, Fall 2009.
[14] John L Campbell, Charles Quincy, Jordan Osserman, and
Ove K Pedersen. Coding In-Depth Semistructured Interviews:
Problems of Unitization and Intercoder Reliability and Agree-
ment. Sociological Methods & Research, 42(3):294–320, 2013.
[15] Jacob Cohen. A power primer. Psychological bulletin,
112(1):155, 1992.
[16] Joseph Cox. China Is Forcing Tourists to Install Text-Stealing
Malware at its Border. Vice, 2019.
[17] S. Dechand, A. Naiakshina, A. Danilova, and M. Smith. In En-
cryption We Don’t Trust: The Eﬀect of End-to-End Encryption
to the Masses on User Perception. In EuroS&P, 2019.
[18] A Demjaha, JM Spring, I. Becker, S Parkin, and MA Sasse.
Metaphors Considered Harmful? An Exploratory Study of the
Eﬀectiveness of Functional Metaphors for End-to-End Encryp-
tion. In USEC, 2018.
[19] Andrea A. diSessa. Models of Computation. In Donald A.
Norman and Stephen W. Draper, editors, User Centered System
Design: New Perspectives on Human-Computer Interaction,
pages 201–218. Lawrence Erlbaum Associates, 1986.
[20] Electronic Frontier Foundation.
Communicating
https://ssd.eff.org/en/module/
with others.
communicating-others.
[21] Antonio M. Espinoza, William J. Tolley, Jedidiah R. Crandall,
Masashi Crete-Nishihata, and Andrew Hilts. Alice and Bob,
Who the FOCI Are They?: Analysis of End-to-End Encryption
in the LINE Messaging Application. In FOCI, 2017.
[22] Facebook. Secret conversations. https://www.facebook.
com/help/messenger-app/1084673321594605.
[23] Facebook. Whatsapp security. https://www.whatsapp.
com/security/.
[24] Sascha Fahl, Marian Harbach, Thomas Muders, Matthew
Smith, and Uwe Sander. Helping Johnny 2.0 to Encrypt His
Facebook Conversations. In SOUPS, 2012.
[25] Simson L. Garﬁnkel and Robert C. Miller. Johnny 2: A User
Test of Key Continuity Management with S/MIME and Out-
look Express. In SOUPS, 2005.
[26] Shirley Gaw, Edward W. Felten, and Patricia Fernandez-Kelly.
Secrecy, Flagging, and Paranoia: Adoption Criteria in En-
crypted Email. In CHI, 2006.
[27] Nina Gerber, Verena Zimmermann, Birgit Henhapl, Sinem
Emeröz, and Melanie Volkamer. Finally Johnny Can Encrypt:
But Does This Make Him Feel More Secure? In ARES, 2018.
[28] Joseph A Gliem and Rosemary R Gliem. Calculating, Interpret-
ing, and Reporting Cronbach’s Alpha Reliability Coeﬃcient
for Likert-Type Scales. Midwest Research-to-Practice Confer-
ence in Adult, Continuing, and Community Education, 2003.
[29] Eszter Hargittai and Yuli Patrick Hsieh. Succinct Survey Mea-
sures of Web-Use Skills. Social Science Computer Review,
30(1):95–107, 2012.
[30] Kashmir Hill. ’God View’: Uber allegedly stalked users for
party-goers’ viewing pleasure. Forbes, 2014.
[31] Myles Hollander, Douglas A Wolfe, and Eric Chicken. Non-
parametric Statistical Methods, volume 751. John Wiley &
Sons, 2013.
[32] Sture Holm. A Simple Sequentially Rejective Multiple Test
Procedure. Scandinavian Journal of Statistics, pages 65–70,
1979.
[33] Patrick Gage Kelley, Sunny Consolvo, Lorrie Faith Cranor,
Jaeyeon Jung, Norman Sadeh, and David Wetherall. A co-
nundrum of permissions: installing applications on an android
smartphone. In Financial Crypto, 2012.
[34] Klaus Krippendorﬀ. Reliability in Content Analysis: Some
Common Misconceptions and Recommendations. Human
communication research, 30(3):411–433, 2004.
[35] Priya C. Kumar, Marshini Chetty, Tamara L. Clegg, and Jes-
sica Vitak. Privacy and Security Considerations For Digital
Technology Use in Elementary Schools. In CHI, 2019.
[36] Alexander De Luca, Sauvik Das, Martin Ortlieb, Iulia Ion, and
Ben Laurie. Expert and Non-Expert Attitudes towards (Secure)
Instant Messaging. In SOUPS, 2016.
[37] Hiroaki Masaki, Kengo Shibata, Shui Hoshino, Takahiro Ishi-
hama, Nagayuki Saito, and Koji Yatani. Exploring Nudge
Designs to Help Adolescent SNS Users Avoid Privacy and
Safety Threats. In CHI, 2020.
[38] Arunesh Mathur, Joseﬁne Engel, Sonam Sobti, Victoria Chang,
and Marshini Chetty. "They Keep Coming Back Like Zom-
bies": Improving Software Updating Interfaces. In SOUPS,
2016.
[39] Nora McDonald, Sarita Schoenebeck, and Andrea Forte. Re-
liability and Inter-Rater Reliability in Qualitative Research:
Norms and Quidelines for CSCW and HCI practice. Proceed-
ings of the ACM on Human-Computer Interaction, 3(CSCW):1–
23, 2019.
[40] Alena Naiakshina, Anastasia Danilova, Sergej Dechand, Kat
Krol, M Angela Sasse, and Matthew Smith. Poster: Mental
Models-User Understanding of Messaging and Encryption. In
EuroUSEC, 2016.
[41] Eyal Peer, Serge Egelman, Marian Harbach, Nathan Malkin,
Arunesh Mathur, and Alisa Frik. Nudge Me Right: Personal-
izing Online Security Nudges to People’s Decision-Making
Styles. Computers in Human Behavior, 109:106347, 2020.
[42] Library Freedom Project. Library freedom resouces. https:
//libraryfreedom.org/index.php/resources/.
[43] Rakuten.