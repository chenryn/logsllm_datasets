Section 6.3 except that we used 300 images to build the power
template and the left 200 images to evaluate attack method. The
pixel value in the MNIST digits is in the range of [0, 255]. We
chose to build the power template with power traces collected
with 9 different kernels instead of all 64 kernels, because it already
provides enough precision to recover the input image.
Evaluation Metric: We use the same evaluation metrics with
those in the background detection except the pixel-level accuracy
is re-defined with pixel values instead of background markers as
follows:
||pv(x) − pvд(x)||2/|I|
(7)
αpixel =
x∈I
in which pv represents the pixel value in the recovered image while
pvд means the pixel value in the original image.
Candidates Generated: To evaluate the effectiveness of grouping
power vectors in the power template, we list the statistics of candi-
dates returned by the power template in Table 2. As we collected
power traces from 9 different kernels, so the length of power feature
vector for each cycle is 9. In the experiment, we divided the power
features into 4 groups of size 2 (using first 8 features) and 3 groups
of size 3 respectively. The number of candidates returned by the
power template for one group is denoted as |S| and the distance
Table 2: Candidates Statistics for Model 1 (3 x 3 Kernel Size)
GroupSize = 2
GroupSize = 3
δ
0.1
0.2
0.5
1.0
|S|
767
1448
3847
9457
Dmin
57
45
33
26
|S∩| D∩
min
107
190
116
351
90
1086
2223
67
|S|
325
787
2447
5890
Dmin
153
90
48
34
|S∩| D∩
min
48
155
102
170
68
715
1571
56
threshold is δ. We also calculate the distance between each can-
didate and the genuine related pixels and represent the minimal
distance as Dmin, which serves as the quality metric of returned
candidate set: the smaller, the better. For the final candidate set, i.e.
the intersection of all candidate set from different groups, we also
report the number of candidates in the set |S∩| and the minimal
distance D∩
. Table 2 shows the average of these numbers among
min
all cycles.
From the table, the number of candidates increases with the
increase of threshold δ as larger search space is included. Also the
average of minimal distance decreases when more candidates are
included. For smaller δ, such as 0.1 and 0.2, the number of candidates
returned are small, and for many cycles, we are not able to find
a match inside the template. Thus, smaller δ may lead to lower
precision in finding the related pixels. For two experiments with
different group size investigated, both of them achieves significant
reduction in the size of final candidate set, while maintaining similar
capability to recover more precise pixels (reflected by small changes
of Dmin) at medium or large δs.
In all, the grouping of power feature vectors and intersection of
candidate sets from power template is effective in reducing the size
of pixel candidates for each cycle while maintaining the accuracy
at the same time.
Image Quality: Based on the experimental result in Table 2, we
proceed the image reconstruction with group size 3 as the final
candidate size is relatively small to group size 2. We also determine
the δ to be 1.0 to maintain a high accuracy candidate set for further
reconstruction. For the left 200 images used for evaluation, using
Algorithm 2, we recover them from the candidate sets from the
power template. We also generate images without using this algo-
rithm for comparison. Without Algorithm 2, for a particular pixel in
image, its value is given by the average of all possible values for this
pixel in the returned candidates. The average pixel-level distance,
defined in Eq. 7, is 1.65 for image generated with Algorithm 2 and
2.98 for images without it. On an average case, both of them are
quite close to the genuine image considering the pixel value range
is 0 to 255. This is because the candidates generated from power
template are already close to the genuine pixels.
However, as illustrated in Fig. 8, the recognition accuracy is
much higher with Algorithm 2. The recognition accuracy of images
recovered for model 1 (3×3 kernel size) with the algorithm is 89.8%
while the accuracy drops down to 15% if we take the average of all
the pixel candidates. The same accuracy drop also happens on the
images recovered from power trace collected with model 2 (5×5
kernel size), from 79% to 10%. Though the images recovered without
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
L. Wei et al.
attack to decide whether a particular data record belongs to the
model the training set with a black-box access to the model. Tramer
et al [37] demonstrated a model inversion attack by exploiting the
relationship of queries and confidence values on different machine
learning models, such as DNN, logistic regressions, etc. Despite the
attacks exploiting the privacy leakage in the training sets, Hua et
al [11] presented a novel attack to reverse engineer the underlying
network information. They utilize the memory accessing patterns
to infer the network structures, such as number of layers, the feature
map sizes of each layer. They also showed the values of weights
can be recovered if the memory access can be observed during
a “zero pruning” stage. The main difference of our attack from
above-mentioned ones is the attack target. Our proposed attack try
to explore the leakage at inference stage. Instead of training set
samples or network models, we can recover an image for runtime
inputs using the power side channel.
Power Side-channel Attack: The power side-channel leakage
can be exploited to recover the secret keys in cryptographic devices.
By analyzing the difference of multiple power traces with diverse
inputs, attackers are able to uncover the secret key in widely used
symmetric encryption standards, such as DES [17] and AES [3, 9,
23]. Eisenbarth [6] and Msgna [25] showed they can recover the
instruction type executed by processor via power side channel
using hidden Markov model. Liu et al [22] managed to accurately
locate each instruction instance during execution with a modified
Viterbi algorithm. Building a “power template” is a common way
used to break secret keys in cryptographic systems [4, 32]. Similar
to our proposed attack, they firstly estimate a leakage model from
the collected power traces and the secret keys and then at runtime,
using the leakage model they predict the keys from the online traces.
Though the general procedure is similar, the difficulty in building
“power traces” is to find a proper attacking surface. In template
attacks, they need to identify power traces which only correlate
with a limited number of the key bits. In our attack, we found the
convolutional unit as the appropriate attack target and raised the
attacking method accordingly.
9 CONCLUSION
In this paper, we demonstrate the first power side channel attack
on an FPGA-based convolutional neural network accelerator. Its
input image is successfully recovered using the power traces mea-
sured for inferencing operation. In the attack, we firstly filter out
the noises and distortions in power measurement process. Then
we consider two attacking scenarios for adversaries of different
abilities. They can either passively eavasdrop the power side chan-
nel or additionally profiling the correlation between power signals
and image pixels. For the two adversaries, we propose two meth-
ods respectively: background detection and power template, to
recover the input image in different granularity. We demonstrate
the practicality of our proposed attack on an accelerator executing
classification task for hand-written digits in MNIST datasets and the
experimental results show we achieve high recognition accuracy.
ACKNOWLEDGEMENT
This work was supported in part by the General Research Fund
(GRF) of Hong Kong Research Grants Council (RGC) under Grant
Figure 8: Recognition accuracy for model 1 and model 2 with
and w/o. reconstruction algorithm.
the proposed algorithm achieve relatively good pixel-level accu-
racy, the low recognition accuracy results from its incapability to
reconstruct the structure of digits at some critical points, especially
at the edge of digits. On the contrary, Algorithm 2 considers the
consistence of related pixels recovered among cycles, thus it is able
to filter out most unrelated pixels.
Finally, enlarging kernel size incurs a little degradation in the
recognition accuracy, from 89.8% to 79% as more pixels are involved
in one cycle so that it is relatively harder to distinguish the genuine
pixels.
Complexity: We analyze the complexity in three phases: The time
complexity to build power template is O(N ×C×K) where N stands
for total number of images enrolled, C means the cycles needed to
generate one feature map and K is the number of kernels. The mem-
ory complexity in power template building is O(N × C × (K + S)),
where N × C represents the total amount of entries in power tem-
plate and K + S is the entry size. S stands for the size of related
pixels. In the candidate generation, the time complexity of propor-
tional to the size of the power template and the size of returned
candidate sets. Finally, for the image reconstruction algorithm, the
most time-consuming part comes from the loop in it (Line 3 – 17),
so its complexity is O(C ×A
2), where C is total number of candidate
sets (equal to the cycles needed to generate a output feature map)
and A stands for the average size of the candidate set. All methods
in three phases can be implemented efficiently and we report their
running time as follows: it takes 215.6s to build the power template
from 300 images and 157.2s to generate candidates for all cycles in
recovering one image. The image reconstruction algorithm costs
around 43.2s to finish. The size of the power template built with
300 images enrolled is around 44MB.
8 RELATED WORK
Neural Network Privacy: In [8], authors made a successful at-
tempt to correlate the dosage of certain medicine with a specific
patient’s genotype from a model used in pharmacogenetics. Also,
on a face recognition system, they managed to reconstruct users’
face images enrolled in the training stage from the neural network
models [7]. Shokri et al [33] presented a membership inference
0123456789Genuine Class0%20%40%60%80%100%Recognition AccuracyOriginal ImageRecovered Image (3x3 kernel) with Alg.2Recovered Image (5x5 kernel) with Alg.2Recovered Image (3x3 kernel) w/o. Alg.2Recovered Image (5x5 kernel) w/o. Alg.2average: 89.8%average: 79.0%average: 15%average: 10%Power Side-Channel Attack on Convolutional Neural Network Accelerators
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
No. 14205018 and in part by National Natural Science Foundation
of China under Grant No. 61432017 and No. 61532017.
A PRELIMINARIES
In this section, we first review the concept of convolutional neural
network (CNN), and then introduce the architecture of typical CNN
accelerators and finally discuss the basics on power side-channel
leakage.
A.1 Convolutional Neural Network
Convolutional Neural Network (CNN) [20] is a neural network
architecture used for image applications. It is constructed by a
pipeline of sequentially connected layers and may consist of four
types of computation: convolution, pooling, normalization and full-
connected. The structure of the network, such as total number of
layers and the type of computation in each layer, is determined
by designers prior to the training stage. Then the parameters in
each layer, namely weights, are acquired through dedicated train-
ing algorithms. In the inference stage with structure and weights
ascertained, CNN can make predictions with the input images. In
particular, the input of first layer of CNN is image itself and the
computation in the first layer is usually convolution.
As our focus is on the convolution layer of CNN, here we briefly
introduce its details and illustrate the calculation in Fig. 9 (a). The
input to the convolution layer is an image of size X × Y × M and we
call the X × Y 2D pixel array feature map. For each input feature
map, to calculate the pixel value of the output feature map, a kernel
(or filter) of size Kx × Ky is applied to construct a convolutional
window for each input pixel capturing its neighbors. We then get
an output feature map with the convolutional window sliding by
steps of Sx and Sy in two directions of the input feature map. We
can represent the convolution operation formally with following
formula:
x,y = f ( M
j
i =1
O
Kx−1
Ky−1
a=0
b=0
(βi, j +
i, j
a,b × I i
xSx +a,ySy +b)),
ω
(8)
j
x,y is the pixel value of position (x, y) in j-th output
where the O
feature map, ωi, j and βi, j are the kernel and bias value between the
i-th input feature map and the j-th output feature map respectively,
and f (·) is a non-linear activation function such as tanh or siдmoid.
A.2 CNN Accelerator Design
An accelerator is usually used in the inference stage to boost the
computational efficiency in a number of low-power platforms. The
accelerators are usually implemented by dedicated hardware, such
as FPGA and ASIC and there are a number of designs available [5, 29,
38] in both academia and industry. The general architecture of these
accelerators is similar, as shown in Fig. 9 (b) wherein typically five
components are involved: Direct Memory Access (DMA), controller,
data buffers, parameter buffers and compute units. DMA is used
for the data transmission with main processors while controller is
responsible for coordinating computation tasks among components.
The parameter buffers store the weights used in the CNN model
and shall be ready prior to any inference operation. The data buffer
stores the input feature maps for every layer and caches the output
feature map from computing units to be used in the next layer.
Compute units contain dedicated hardware to accelerate different
operations in the neural network, e.g., convolution, pooling, etc.
Specifically, as the target of our attack is the convolutional layer
in the CNN, we present the detailed design for the convolution
operation in the compute unit. Line buffer [2] is an efficient hard-
ware structure to implement convolution and it has been adopted
by a number of CNN accelerators [5, 29, 38]. Fig. 9 (c) shows the
structure of line buffer to execute 2D convolution with a 3×3 kernel.
There are three line registers to compute the convolution with a
kernel of size 3 × 3 as we need to cache the pixel values in recent
three rows of the image. The length of each line is equivalent to
the row size of input image. The convolution is achieved by a set
of dedicated hardware multiplier and adders. At each cycle, one
pixel is put into the line buffer, and a 3× 3 convolution is computed.
The intermediate result is passed through a non-linear function to
generate one output value per cycle. If the input image contains