P (X)j.
where F(X) = arg max
P (X)j represents the j-th probability
of P (X), where j ‚àà {1, ¬∑ ¬∑ ¬∑ , k}
X a is the adversarial example of X.
The speciÔ¨Åed target class for TAs.
Œ∏ is the parameter of F.
J is the loss function of F.
0 = X ; X a
constraint: X a = X +¬∑sign(‚àáX J(X, ytrue)), where  is the
hyper-parameter of L‚àû constraint. Similarly, Tram`er et al. [16]
proposed a non-iterative UA, R+FGSM, which applies a small
random perturbation before linearizing the loss function.
n+1 = Clipx,(X a
2) Iterative UAs: Kurakin et al. [17] introduced an in-
tuitive extension of FGSM - Basic Iterative Method (BIM)
that iteratively takes multiple small steps while adjusting the
direction after each step: X a
n +
Œ± ¬∑ sign(‚àáX J(X a
n, ytrue))), where Clipx, is used to restrict
the L‚àû of perturbation. Following BIM, Madry et al. [18]
introduced a variation of BIM by applying the projected
gradient descent algorithm with random starts, named as
the PGD attack. Similarly, Dong et al. [19] integrated the
momentum techniques [36] into BIM for the purpose of
stabilizing the updating direction and escaping from poor
local maximum during iterations. We refer this UA as U-
MI-FGSM. DeepFool [20] was proposed to generate AEs by
searching for the closest distance from the source image to
the decision boundary of the target model. Further in [21],
Moosavi-Dezfooli et al. developed a Universal Adversarial
Perturbation (UAP) attack, in which an image-agnostic and
universal perturbation can be used to misclassify almost all
images sampled from the dataset. In [22], He et al. proposed
an attack, OptMargin (OM), to generate robust AEs that can
evade existing region-based classiÔ¨Åcation defense.
3) Non-iterative TAs: The TA version of FGSM was intro-
duced in [17] to specify the least likely class yLL of an original
P (y|X). We
image X as the target class, where yLL = arg min
y
refer this non-iterative TA as the Least-Likely Class (LLC)
attack: X a = X‚àí¬∑sign(‚àáX J(X, yLL)). Similar to R+FGSM,
Tram`er et al. [16] introduced R+LLC, which also integrates
a small random step before linearizing the loss function.
4) Iterative TAs: The Ô¨Årst adversarial attack discovered by
Szegedy et al. [2] is an iterative TA, which generates AEs
by a Box-constrained L-BFGS (BLB) algorithm. However,
BLB has several limitations, e.g., it is time-consuming and
impractical to linearly search for the optimal solution at large
scale. To facilitate the efÔ¨Åciency of iterative TAs, Kurakin et
al. [17] proposed a straightforward iterative version of LLC -
ILLC. Following the attacks in [19], momentum techniques
can also be generalized to ILLC, called targeted MI-FGSM (T-
MI-FGSM). Taking a different perceptive, Papernot et al. [11]
proposed the Jacobian-based Saliency Map Attack (JSMA).
SpeciÔ¨Åcally, JSMA Ô¨Årst computes the Jacobian matrix of a
given sample X, and then perturbs it by Ô¨Ånding the input
features of X that make the most signiÔ¨Åcant changes to
the output. Carlini and Wagner [12] introduced a set of
powerful attacks based on different norm measurements on the
magnitude of perturbation, termed as CW. In particular, CW
is formalized as an optimization problem to search for high-
conÔ¨Ådence AEs with small magnitude of perturbation, and has
three variants: CW0, CW2 and CW‚àû, respectively. In [23],
Chen et al. argued that L1 has not been explored to generate
their Elastic-net Attack to DNNs (EAD)
AEs. Therefore,
formulates the generation of AE as an elastic-net regularized
optimization problem and features L1-oriented AEs.
B. Utility Metrics of Attacks
From the view of economics, utility is a measure of whether
goods or services provide the features that users need [37]. For
adversaries who want to attack DL models, utility means to
what extent the adversarial attack can provide ‚Äúsuccessful‚Äù
AEs. Generally speaking, successful AEs should not only
can be misclassiÔ¨Åed by the model, but also be imperceptible
to humans, robust to transformations as well as resilient to
existing defenses depending on the adversarial goals.1
In this paper, we consider misclassiÔ¨Åcation, imperception,
and robustness as utility requirements while taking the re-
silience as the security requirement. We will Ô¨Årst deÔ¨Åne 10
utility metrics for adversarial attacks below.
1) MisclassiÔ¨Åcation: Firstly, we summarize utility metrics
in terms of misclassiÔ¨Åcation as follows.
MisclassiÔ¨Åcation Ratio (MR). MisclassiÔ¨Åcation is the most
important property for adversarial attacks. In the case of UAs,
MR is deÔ¨Åned as the percentage of AEs that are successfully
misclassiÔ¨Åed into an arbitrary class except their ground truth
classes. For TAs, MR is deÔ¨Åned as the percentage of AEs
misclassiÔ¨Åed into the target classes as speciÔ¨Åed before. More
i ) (cid:4)= yi) and
speciÔ¨Åcally, M RU A = 1
N
M RT A = 1
N
i=1 count(F(X a
i ) = y
i=1 count(F(X a
(cid:2)N
(cid:2)N
‚àó
i ).
i )F (X a
i=1 P (X a
Average ConÔ¨Ådence of Adversarial Class
For AEs, ACAC is deÔ¨Åned as
conÔ¨Ådence towards the incorrect class,
1
n
of successful AEs.
(ACAC).
the average prediction
i.e., ACAC =
i ), where n (n ‚â§ N) is the total number
Average ConÔ¨Ådence of True Class (ACTC). By averaging
the prediction conÔ¨Ådence of true classes for AEs, ACTC is
used to further evaluate to what extent the attacks escape from
the ground truth: ACT C = 1
n
i=1 P (X a
i )yi.
(cid:2)n
(cid:2)n
2) Imperceptibility:
imperceptibility implies
that the adversarial example would still be correctly classiÔ¨Åed
by human vision, which ensures that the adversarial and benign
In essence,
1In this paper, we distinguish between the robustness and resilience of AEs.
SpeciÔ¨Åcally, robustness reÔ¨Çects the misclassiÔ¨Åcation stability after preprocess-
ing by inevitable transformations in physical world, while resilience represents
the surveillance of AEs when being defended by well-designed defenses.
(cid:23)(cid:24)(cid:22)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:41:30 UTC from IEEE Xplore.  Restrictions apply. 
examples convey the same semantic meaning. To evaluate the
imperceptibility of AEs, we detail the metrics as follows.
Average Lp Distortion (ALDp). Almost all existing at-
tacks adopt Lp norm distance (i.e., p = 0, 1,‚àû) as distortion
metrics for evaluations. SpeciÔ¨Åcally, L0 counts the number
of pixels changed after the perturbation; L2 computes the
Euclidean distance between original and perturbed examples;
L‚àû measures the maximum change in all dimensions of AEs.
In short, we deÔ¨Åne ALDp as the average normalized Lp distor-
i ‚àíXi(cid:4)p
tion for all successful AEs, i.e., ALDp = 1
.
(cid:4)Xi(cid:4)p
n
The smaller ALDp is, the more imperceptible the AEs are.
(cid:2)n
(cid:4)X a
i=1
(cid:2)n
i=1 SSIM (X a
Average Structural Similarity (ASS). As one of the
commonly used metrics to quantify the similarity between
two images, SSIM [38] is considered to be more consistent
to human visual perception than Lp similarity. To evaluate
the imperceptibility of AEs, we deÔ¨Åne ASS as the average
SSIM similarity between all successful AEs and their original
examples, i.e., ASS = 1
i , Xi). Intuitively,
n
the greater the ASS is, the more imperceptible the AEs are.
Perturbation Sensitivity Distance (PSD). Based on the
contrast masking theory [39], PSD was proposed in [40] to
evaluate human perception of perturbations, where P SD =
1
j=1 Œ¥i,jSen(R(xi,j)), where m is the total number
n
of pixels, Œ¥i,j denotes the j-th pixel of the i-th example,
R(xi,j) represents the surrounding square region of xi,j,
and Sen(R(xi,j)) = 1/std(R(xi,j)) with std(R(xi,j)) the
standard deviation function. The smaller PSD is, the more
imperceptible AEs are.
(cid:2)m
(cid:2)n
i=1
3) Robustness: Normally,
images in physical world are
inevitably preprocessed before feeding them into production
systems (e.g., online image classiÔ¨Åcation systems), which may
lead to declines in MR for AEs. Thus, it is essential to evaluate
the robustness of AEs under various realistic conditions.
Noise Tolerance Estimation (NTE). In [40], the robustness
of AEs is estimated by noise tolerance, which reÔ¨Çects the
amount of noises that AEs can tolerate while keeping the
misclassiÔ¨Åed class unchanged. SpeciÔ¨Åcally, NTE calculates
the gap between the probability of misclassiÔ¨Åed class and
the max probability of all other classes,
i.e., N T E =
(cid:2)n
i )j}], where j‚àà{1,¬∑¬∑¬∑ ,k}
1
i=1[P (X a
i )F (X a
and j(cid:4)= F (X a
n
i ). The higher NTE is, the more robust AEs are.
On the other hand, due to the uncertainty of what transfor-
mations may be used, we thus sample two most widely and
possibly used image preprocessing methods, Gaussian blur
and Image compression, to evaluate the robustness of AEs.
i )‚àí max{P (X a
Robustness to Gaussian Blur (RGB). Gaussian blur is
widely used as a preprocessing stage in computer vision
algorithms to reduce noises in images. Normally, a robust
AE should maintain its misclassiÔ¨Åcation effect after Gaussian
and RGBT A =
blur. That is, RGBU A =
count(F(GB(X a
, where GB denotes the Gaussian blur
i ))(cid:5)=yi)
i )(cid:5)=yi)
count(F(GB(X a
count(F(X a
count(F(X a
i ))=y‚àó
i )
i )=y‚àó
i )
function. The higher RGB is, the more robust AEs are.
lar
count(F(IC(X a
Robustness
Image Compression (RIC). Simi-
to RGB, RIC can be formulated as: RICU A =
i ))=y‚àó
i )
,
i )=y‚àó
i )
to
i ))(cid:5)=yi)
i )(cid:5)=yi)
and RICT A =
count(F(IC(X a
count(F(X a
count(F(X a
where IC denotes the speciÔ¨Åc image compression function.
Also, the higher RIC is, the more robust AEs are.
4) Computation Cost: We deÔ¨Åne the Computation Cost
(CC) as the runtime for attackers to generate an AE on
average, and therefore evaluate the attack cost.
C. Defense Advances
In general, existing defense techniques can be classiÔ¨Åed into
5 categories. We discuss each category as follows.
1) Adversarial Training: Adversarial training has been pro-
posed since the discovery of AEs in [2], with the hope that it
can learn robust models via augmenting the training set with
newly generated AEs. However, adversarial training with AEs
generated by BLB in [2] suffers from high computation cost,
which is impractical for large-scale training tasks.
To scale adversarial training to large-scale datasets, Kurakin
et al. [24] presented a computationally efÔ¨Åcient adversar-
ial
training with AEs generated by LLC, which we refer
to as Naive Adversarial Training (NAT). Later, Tram`er et
al. [16] proposed the Ensemble Adversarial Training (EAT)
that augments training data with AEs generated by R+FGSM
on other pre-trained models instead of the original model.
Another variant of adversarial training, referred to as PGD-
based Adversarial Training (PAT), was presented in [18] via
retraining the model with AEs generated by PGD iteratively.
idea to
defend against adversarial attacks is to reduce the sensitivity of
models to AEs and hide the gradients [41], which is referred
to as the gradient masking/regularization method.
2) Gradient Masking/Regularization: A natural
In [10], Papernot et al. introduced the Defensive Distillation
(DD) defense to reduce or smooth the amplitude of network
gradients and make the defended model less sensitive w.r.t
perturbations in AEs. However in [13], Ross and Doshi-Velez
claimed that DD-enhanced models perform no better than un-
defended models in general. Aiming at improving robustness
of models, they introduced the Input Gradient Regularization
(IGR), which directly optimizes the model for more smooth
input gradients w.r.t its predictions during training.
3) Input Transformation: As defenses discussed above ei-
ther depend on generated AEs or require modiÔ¨Åcations to
the original model,
to devise
attack/model-agnostic defenses against adversarial attacks. Re-
searchers have attempted to remove the adversarial perturba-
tions of the testing inputs before feeding them into the original
model, which we refer to as input transformation defenses.
is particularly important
it
Using Ô¨Åve different image transformation techniques, Guo
et al. [25] showed that training the models on corresponding
transformed images can effectively defend against existing
attacks, which we refer to as Ensemble Input Transformation
(EIT). Another similar work is [26], where Xie et al. intro-
duced a Random Transformations-based (RT) defense. In RT,
the testing images Ô¨Årst go through two additional randomiza-
tion layers, and then the transformed images are passed to the
original model. In [27], Song et al. proposed PixelDefense
(PD) to purify adversarial perturbations. More speciÔ¨Åcally,
PD makes use of the PixelCNN [42], a generative model, to
(cid:23)(cid:24)(cid:23)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:41:30 UTC from IEEE Xplore.  Restrictions apply. 
purify the AEs and then passes the puriÔ¨Åed examples to the
original model. Buckman et al. [28] proposed the Thermometer
Encoding (TE) method to retrain the classiÔ¨Åcation model with
discretized inputs using thermometer encoding, and discretize
the testing inputs before passing them to the retrained model.
4) Region-based ClassiÔ¨Åcation: Region-based ClassiÔ¨Åca-
tion (RC) defense [29] takes the majority prediction on
examples that are uniformly sampled from a hypercube around
the AE, since they found that the hypercube around an AE
greatly intersects with its true class region of the AE.
5) Detection-only Defenses: Given the difÔ¨Åculty in classi-
fying AEs correctly, a number of detection-only defenses have
been proposed to merely detect AEs and reject them. In this
part, we introduce several latest and representative works and
refer interested readers to [8], [43] for more others.
Ma et al. [30] proposed a Local Intrinsic Dimensional-
ity based detector (LID) to discriminate AEs from normal
examples due to the observation that
the LID of AEs is
signiÔ¨Åcantly higher than that of normal examples. In [31],
Xu et al. proposed the Feature Squeezing (FS) method to
detect AEs via comparing the prediction difference between
the original input and corresponding squeezed input. In [32],
Meng and Chen proposed the MagNet defense framework,
which is a combination defense of complete defense (i.e., the
reformer) and detection-only defense (i.e., the detector).
 
%$%$
%%!&
'#$#
)"$
(
!$

%%%!$
%%
 $
##*
 $!&

 $%!$
 $  
!$
%%%%*
'&%! 
&#%*
'&%! 
%%%#$
 *$$  
 $%%*
 *$$
	
 $%#$
Fig. 1. The System Overview of DEEPSEC
(cid:2)n
ClassiÔ¨Åcation ConÔ¨Ådence Variance (CCV). Although
defense-enhanced models might not affect the accuracy per-
formance, the prediction conÔ¨Ådence of correctly classiÔ¨Åed ex-
amples may signiÔ¨Åcantly decrease. To measure the conÔ¨Ådence
variance induced by defense-enhanced models, we formulate
i=1 |P (Xi)yi ‚àí P D(Xi)yi|, where n < N is the
CCV = 1
n
number of examples correctly classiÔ¨Åed by both F and FD.
ClassiÔ¨Åcation Output Stability (COS). To measure the
classiÔ¨Åcation output stability between the original model and
the defense-enhanced model, we use JS divergence [44] to
measure the similarity of their output probability. We average
the JS divergence between the output of original and defense-
enhanced model on all correctly classiÔ¨Åed testing examples,
i=1 JSD(P (Xi)(cid:7)P D(Xi)), where n < N is
i.e., COS = 1
n
the number of examples classiÔ¨Åed by both F and FD correctly;
JSD is the function of JS divergence.
(cid:2)n
D. Utility Metrics of Defenses
III. SYSTEM DESIGN AND IMPLEMENTATION
In general, defenses can be evaluated from two perspectives:
utility preservation and resistance to attacks. Particularly, the
utility preservation captures how the defense-enhanced model
preserves the functionality of the original model, while the
resistance reÔ¨Çects the effectiveness of defense-enhanced model
against adversarial attacks. For the utility of defense, it does
not make sense for detection-only defenses since they only
detect AEs and reject them. Thus, it is important to note that
we only evaluate the utility performance of complete defenses.
Suppose we attain the defense-enhanced model FD of F,
while P D denotes the corresponding softmax layer output of
FD. Next, we detail 5 utility metrics of defenses.
ClassiÔ¨Åcation Accuracy Variance (CAV). The most impor-
tant metric used to evaluate the performance of a DL model is
accuracy. Therefore, a defense-enhanced model should main-
tain the classiÔ¨Åcation accuracy on normal testing examples as
much as possible. In order to evaluate the impact of defenses
on accuracy, we deÔ¨Åne CAV = Acc(FD, T ) ‚àí Acc(F, T ),
where Acc(F, T ) denotes model F‚Äôs accuracy on dataset T .
ClassiÔ¨Åcation Rectify/SacriÔ¨Åce Ratio (CRR/CSR). To
assess how defenses inÔ¨Çuence the predictions of models on the
testing set, we detail the difference of predictions before and
after applying defenses. We deÔ¨Åne the CRR as the percentage
of testing examples that are misclassiÔ¨Åed by F previously but
correctly classiÔ¨Åed by FD. Inversely, CSR is the percentage
of testing examples that are correctly classiÔ¨Åed by F but
(cid:2)N
i=1 count(F(Xi)(cid:4)=
misclassiÔ¨Åed by FD. That is, CRR = 1
(cid:2)N
N
yi&FD(Xi) = yi) and CSR = 1
i=1 count(F(Xi) =
yi&FD(Xi)(cid:4)= yi). In fact, CAV = CRR‚àíCSR.
N
A. System Design
We present the system overview of DEEPSEC in Fig. 1.
Basically, it consists of Ô¨Åve parts:
1) Attack Module (AM). The main function of AM is to
exploit vulnerabilities of DL models and attack them via
crafting AEs. In this module, we implement 16 state-of-
the-art adversarial attacks, including 8 UAs and 8 TAs.
2) Defense Module (DM). The main function of DM is to
defend DL models and increase their resistance against
adversarial attacks. In this module, we implement 13