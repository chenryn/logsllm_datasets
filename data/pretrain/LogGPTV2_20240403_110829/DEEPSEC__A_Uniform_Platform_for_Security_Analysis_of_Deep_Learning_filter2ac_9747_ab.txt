P (X)j.
where F(X) = arg max
P (X)j represents the j-th probability
of P (X), where j ∈ {1, · · · , k}
X a is the adversarial example of X.
The speciﬁed target class for TAs.
θ is the parameter of F.
J is the loss function of F.
0 = X ; X a
constraint: X a = X +·sign(∇X J(X, ytrue)), where  is the
hyper-parameter of L∞ constraint. Similarly, Tram`er et al. [16]
proposed a non-iterative UA, R+FGSM, which applies a small
random perturbation before linearizing the loss function.
n+1 = Clipx,(X a
2) Iterative UAs: Kurakin et al. [17] introduced an in-
tuitive extension of FGSM - Basic Iterative Method (BIM)
that iteratively takes multiple small steps while adjusting the
direction after each step: X a
n +
α · sign(∇X J(X a
n, ytrue))), where Clipx, is used to restrict
the L∞ of perturbation. Following BIM, Madry et al. [18]
introduced a variation of BIM by applying the projected
gradient descent algorithm with random starts, named as
the PGD attack. Similarly, Dong et al. [19] integrated the
momentum techniques [36] into BIM for the purpose of
stabilizing the updating direction and escaping from poor
local maximum during iterations. We refer this UA as U-
MI-FGSM. DeepFool [20] was proposed to generate AEs by
searching for the closest distance from the source image to
the decision boundary of the target model. Further in [21],
Moosavi-Dezfooli et al. developed a Universal Adversarial
Perturbation (UAP) attack, in which an image-agnostic and
universal perturbation can be used to misclassify almost all
images sampled from the dataset. In [22], He et al. proposed
an attack, OptMargin (OM), to generate robust AEs that can
evade existing region-based classiﬁcation defense.
3) Non-iterative TAs: The TA version of FGSM was intro-
duced in [17] to specify the least likely class yLL of an original
P (y|X). We
image X as the target class, where yLL = arg min
y
refer this non-iterative TA as the Least-Likely Class (LLC)
attack: X a = X−·sign(∇X J(X, yLL)). Similar to R+FGSM,
Tram`er et al. [16] introduced R+LLC, which also integrates
a small random step before linearizing the loss function.
4) Iterative TAs: The ﬁrst adversarial attack discovered by
Szegedy et al. [2] is an iterative TA, which generates AEs
by a Box-constrained L-BFGS (BLB) algorithm. However,
BLB has several limitations, e.g., it is time-consuming and
impractical to linearly search for the optimal solution at large
scale. To facilitate the efﬁciency of iterative TAs, Kurakin et
al. [17] proposed a straightforward iterative version of LLC -
ILLC. Following the attacks in [19], momentum techniques
can also be generalized to ILLC, called targeted MI-FGSM (T-
MI-FGSM). Taking a different perceptive, Papernot et al. [11]
proposed the Jacobian-based Saliency Map Attack (JSMA).
Speciﬁcally, JSMA ﬁrst computes the Jacobian matrix of a
given sample X, and then perturbs it by ﬁnding the input
features of X that make the most signiﬁcant changes to
the output. Carlini and Wagner [12] introduced a set of
powerful attacks based on different norm measurements on the
magnitude of perturbation, termed as CW. In particular, CW
is formalized as an optimization problem to search for high-
conﬁdence AEs with small magnitude of perturbation, and has
three variants: CW0, CW2 and CW∞, respectively. In [23],
Chen et al. argued that L1 has not been explored to generate
their Elastic-net Attack to DNNs (EAD)
AEs. Therefore,
formulates the generation of AE as an elastic-net regularized
optimization problem and features L1-oriented AEs.
B. Utility Metrics of Attacks
From the view of economics, utility is a measure of whether
goods or services provide the features that users need [37]. For
adversaries who want to attack DL models, utility means to
what extent the adversarial attack can provide “successful”
AEs. Generally speaking, successful AEs should not only
can be misclassiﬁed by the model, but also be imperceptible
to humans, robust to transformations as well as resilient to
existing defenses depending on the adversarial goals.1
In this paper, we consider misclassiﬁcation, imperception,
and robustness as utility requirements while taking the re-
silience as the security requirement. We will ﬁrst deﬁne 10
utility metrics for adversarial attacks below.
1) Misclassiﬁcation: Firstly, we summarize utility metrics
in terms of misclassiﬁcation as follows.
Misclassiﬁcation Ratio (MR). Misclassiﬁcation is the most
important property for adversarial attacks. In the case of UAs,
MR is deﬁned as the percentage of AEs that are successfully
misclassiﬁed into an arbitrary class except their ground truth
classes. For TAs, MR is deﬁned as the percentage of AEs
misclassiﬁed into the target classes as speciﬁed before. More
i ) (cid:4)= yi) and
speciﬁcally, M RU A = 1
N
M RT A = 1
N
i=1 count(F(X a
i ) = y
i=1 count(F(X a
(cid:2)N
(cid:2)N
∗
i ).
i )F (X a
i=1 P (X a
Average Conﬁdence of Adversarial Class
For AEs, ACAC is deﬁned as
conﬁdence towards the incorrect class,
1
n
of successful AEs.
(ACAC).
the average prediction
i.e., ACAC =
i ), where n (n ≤ N) is the total number
Average Conﬁdence of True Class (ACTC). By averaging
the prediction conﬁdence of true classes for AEs, ACTC is
used to further evaluate to what extent the attacks escape from
the ground truth: ACT C = 1
n
i=1 P (X a
i )yi.
(cid:2)n
(cid:2)n
2) Imperceptibility:
imperceptibility implies
that the adversarial example would still be correctly classiﬁed
by human vision, which ensures that the adversarial and benign
In essence,
1In this paper, we distinguish between the robustness and resilience of AEs.
Speciﬁcally, robustness reﬂects the misclassiﬁcation stability after preprocess-
ing by inevitable transformations in physical world, while resilience represents
the surveillance of AEs when being defended by well-designed defenses.
(cid:23)(cid:24)(cid:22)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:41:30 UTC from IEEE Xplore.  Restrictions apply. 
examples convey the same semantic meaning. To evaluate the
imperceptibility of AEs, we detail the metrics as follows.
Average Lp Distortion (ALDp). Almost all existing at-
tacks adopt Lp norm distance (i.e., p = 0, 1,∞) as distortion
metrics for evaluations. Speciﬁcally, L0 counts the number
of pixels changed after the perturbation; L2 computes the
Euclidean distance between original and perturbed examples;
L∞ measures the maximum change in all dimensions of AEs.
In short, we deﬁne ALDp as the average normalized Lp distor-
i −Xi(cid:4)p
tion for all successful AEs, i.e., ALDp = 1
.
(cid:4)Xi(cid:4)p
n
The smaller ALDp is, the more imperceptible the AEs are.
(cid:2)n
(cid:4)X a
i=1
(cid:2)n
i=1 SSIM (X a
Average Structural Similarity (ASS). As one of the
commonly used metrics to quantify the similarity between
two images, SSIM [38] is considered to be more consistent
to human visual perception than Lp similarity. To evaluate
the imperceptibility of AEs, we deﬁne ASS as the average
SSIM similarity between all successful AEs and their original
examples, i.e., ASS = 1
i , Xi). Intuitively,
n
the greater the ASS is, the more imperceptible the AEs are.
Perturbation Sensitivity Distance (PSD). Based on the
contrast masking theory [39], PSD was proposed in [40] to
evaluate human perception of perturbations, where P SD =
1
j=1 δi,jSen(R(xi,j)), where m is the total number
n
of pixels, δi,j denotes the j-th pixel of the i-th example,
R(xi,j) represents the surrounding square region of xi,j,
and Sen(R(xi,j)) = 1/std(R(xi,j)) with std(R(xi,j)) the
standard deviation function. The smaller PSD is, the more
imperceptible AEs are.
(cid:2)m
(cid:2)n
i=1
3) Robustness: Normally,
images in physical world are
inevitably preprocessed before feeding them into production
systems (e.g., online image classiﬁcation systems), which may
lead to declines in MR for AEs. Thus, it is essential to evaluate
the robustness of AEs under various realistic conditions.
Noise Tolerance Estimation (NTE). In [40], the robustness
of AEs is estimated by noise tolerance, which reﬂects the
amount of noises that AEs can tolerate while keeping the
misclassiﬁed class unchanged. Speciﬁcally, NTE calculates
the gap between the probability of misclassiﬁed class and
the max probability of all other classes,
i.e., N T E =
(cid:2)n
i )j}], where j∈{1,··· ,k}
1
i=1[P (X a
i )F (X a
and j(cid:4)= F (X a
n
i ). The higher NTE is, the more robust AEs are.
On the other hand, due to the uncertainty of what transfor-
mations may be used, we thus sample two most widely and
possibly used image preprocessing methods, Gaussian blur
and Image compression, to evaluate the robustness of AEs.
i )− max{P (X a
Robustness to Gaussian Blur (RGB). Gaussian blur is
widely used as a preprocessing stage in computer vision
algorithms to reduce noises in images. Normally, a robust
AE should maintain its misclassiﬁcation effect after Gaussian
and RGBT A =
blur. That is, RGBU A =
count(F(GB(X a
, where GB denotes the Gaussian blur
i ))(cid:5)=yi)
i )(cid:5)=yi)
count(F(GB(X a
count(F(X a
count(F(X a
i ))=y∗
i )
i )=y∗
i )
function. The higher RGB is, the more robust AEs are.
lar
count(F(IC(X a
Robustness
Image Compression (RIC). Simi-
to RGB, RIC can be formulated as: RICU A =
i ))=y∗
i )
,
i )=y∗
i )
to
i ))(cid:5)=yi)
i )(cid:5)=yi)
and RICT A =
count(F(IC(X a
count(F(X a
count(F(X a
where IC denotes the speciﬁc image compression function.
Also, the higher RIC is, the more robust AEs are.
4) Computation Cost: We deﬁne the Computation Cost
(CC) as the runtime for attackers to generate an AE on
average, and therefore evaluate the attack cost.
C. Defense Advances
In general, existing defense techniques can be classiﬁed into
5 categories. We discuss each category as follows.
1) Adversarial Training: Adversarial training has been pro-
posed since the discovery of AEs in [2], with the hope that it
can learn robust models via augmenting the training set with
newly generated AEs. However, adversarial training with AEs
generated by BLB in [2] suffers from high computation cost,
which is impractical for large-scale training tasks.
To scale adversarial training to large-scale datasets, Kurakin
et al. [24] presented a computationally efﬁcient adversar-
ial
training with AEs generated by LLC, which we refer
to as Naive Adversarial Training (NAT). Later, Tram`er et
al. [16] proposed the Ensemble Adversarial Training (EAT)
that augments training data with AEs generated by R+FGSM
on other pre-trained models instead of the original model.
Another variant of adversarial training, referred to as PGD-
based Adversarial Training (PAT), was presented in [18] via
retraining the model with AEs generated by PGD iteratively.
idea to
defend against adversarial attacks is to reduce the sensitivity of
models to AEs and hide the gradients [41], which is referred
to as the gradient masking/regularization method.
2) Gradient Masking/Regularization: A natural
In [10], Papernot et al. introduced the Defensive Distillation
(DD) defense to reduce or smooth the amplitude of network
gradients and make the defended model less sensitive w.r.t
perturbations in AEs. However in [13], Ross and Doshi-Velez
claimed that DD-enhanced models perform no better than un-
defended models in general. Aiming at improving robustness
of models, they introduced the Input Gradient Regularization
(IGR), which directly optimizes the model for more smooth
input gradients w.r.t its predictions during training.
3) Input Transformation: As defenses discussed above ei-
ther depend on generated AEs or require modiﬁcations to
the original model,
to devise
attack/model-agnostic defenses against adversarial attacks. Re-
searchers have attempted to remove the adversarial perturba-
tions of the testing inputs before feeding them into the original
model, which we refer to as input transformation defenses.
is particularly important
it
Using ﬁve different image transformation techniques, Guo
et al. [25] showed that training the models on corresponding
transformed images can effectively defend against existing
attacks, which we refer to as Ensemble Input Transformation
(EIT). Another similar work is [26], where Xie et al. intro-
duced a Random Transformations-based (RT) defense. In RT,
the testing images ﬁrst go through two additional randomiza-
tion layers, and then the transformed images are passed to the
original model. In [27], Song et al. proposed PixelDefense
(PD) to purify adversarial perturbations. More speciﬁcally,
PD makes use of the PixelCNN [42], a generative model, to
(cid:23)(cid:24)(cid:23)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:41:30 UTC from IEEE Xplore.  Restrictions apply. 
purify the AEs and then passes the puriﬁed examples to the
original model. Buckman et al. [28] proposed the Thermometer
Encoding (TE) method to retrain the classiﬁcation model with
discretized inputs using thermometer encoding, and discretize
the testing inputs before passing them to the retrained model.
4) Region-based Classiﬁcation: Region-based Classiﬁca-
tion (RC) defense [29] takes the majority prediction on
examples that are uniformly sampled from a hypercube around
the AE, since they found that the hypercube around an AE
greatly intersects with its true class region of the AE.
5) Detection-only Defenses: Given the difﬁculty in classi-
fying AEs correctly, a number of detection-only defenses have
been proposed to merely detect AEs and reject them. In this
part, we introduce several latest and representative works and
refer interested readers to [8], [43] for more others.
Ma et al. [30] proposed a Local Intrinsic Dimensional-
ity based detector (LID) to discriminate AEs from normal
examples due to the observation that
the LID of AEs is
signiﬁcantly higher than that of normal examples. In [31],
Xu et al. proposed the Feature Squeezing (FS) method to
detect AEs via comparing the prediction difference between
the original input and corresponding squeezed input. In [32],
Meng and Chen proposed the MagNet defense framework,
which is a combination defense of complete defense (i.e., the
reformer) and detection-only defense (i.e., the detector).
 
%$%$
%%!&
'#$#
)"$
(
!$

%%%!$
%%
 $
##*
 $!&

 $%!$
 $  
!$
%%%%*
'&%! 
&#%*
'&%! 
%%%#$
 *$$  
 $%%*
 *$$
	
 $%#$
Fig. 1. The System Overview of DEEPSEC
(cid:2)n
Classiﬁcation Conﬁdence Variance (CCV). Although
defense-enhanced models might not affect the accuracy per-
formance, the prediction conﬁdence of correctly classiﬁed ex-
amples may signiﬁcantly decrease. To measure the conﬁdence
variance induced by defense-enhanced models, we formulate
i=1 |P (Xi)yi − P D(Xi)yi|, where n < N is the
CCV = 1
n
number of examples correctly classiﬁed by both F and FD.
Classiﬁcation Output Stability (COS). To measure the
classiﬁcation output stability between the original model and
the defense-enhanced model, we use JS divergence [44] to
measure the similarity of their output probability. We average
the JS divergence between the output of original and defense-
enhanced model on all correctly classiﬁed testing examples,
i=1 JSD(P (Xi)(cid:7)P D(Xi)), where n < N is
i.e., COS = 1
n
the number of examples classiﬁed by both F and FD correctly;
JSD is the function of JS divergence.
(cid:2)n
D. Utility Metrics of Defenses
III. SYSTEM DESIGN AND IMPLEMENTATION
In general, defenses can be evaluated from two perspectives:
utility preservation and resistance to attacks. Particularly, the
utility preservation captures how the defense-enhanced model
preserves the functionality of the original model, while the
resistance reﬂects the effectiveness of defense-enhanced model
against adversarial attacks. For the utility of defense, it does
not make sense for detection-only defenses since they only
detect AEs and reject them. Thus, it is important to note that
we only evaluate the utility performance of complete defenses.
Suppose we attain the defense-enhanced model FD of F,
while P D denotes the corresponding softmax layer output of
FD. Next, we detail 5 utility metrics of defenses.
Classiﬁcation Accuracy Variance (CAV). The most impor-
tant metric used to evaluate the performance of a DL model is
accuracy. Therefore, a defense-enhanced model should main-
tain the classiﬁcation accuracy on normal testing examples as
much as possible. In order to evaluate the impact of defenses
on accuracy, we deﬁne CAV = Acc(FD, T ) − Acc(F, T ),
where Acc(F, T ) denotes model F’s accuracy on dataset T .
Classiﬁcation Rectify/Sacriﬁce Ratio (CRR/CSR). To
assess how defenses inﬂuence the predictions of models on the
testing set, we detail the difference of predictions before and
after applying defenses. We deﬁne the CRR as the percentage
of testing examples that are misclassiﬁed by F previously but
correctly classiﬁed by FD. Inversely, CSR is the percentage
of testing examples that are correctly classiﬁed by F but
(cid:2)N
i=1 count(F(Xi)(cid:4)=
misclassiﬁed by FD. That is, CRR = 1
(cid:2)N
N
yi&FD(Xi) = yi) and CSR = 1
i=1 count(F(Xi) =
yi&FD(Xi)(cid:4)= yi). In fact, CAV = CRR−CSR.
N
A. System Design
We present the system overview of DEEPSEC in Fig. 1.
Basically, it consists of ﬁve parts:
1) Attack Module (AM). The main function of AM is to
exploit vulnerabilities of DL models and attack them via
crafting AEs. In this module, we implement 16 state-of-
the-art adversarial attacks, including 8 UAs and 8 TAs.
2) Defense Module (DM). The main function of DM is to
defend DL models and increase their resistance against
adversarial attacks. In this module, we implement 13