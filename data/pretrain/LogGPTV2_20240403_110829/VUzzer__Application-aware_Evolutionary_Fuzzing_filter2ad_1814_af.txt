Radamsa [40] have discovered bugs in real-world applications.
However, throughout the paper, we have already discussed the
limitations of such fuzzers.
Recently, symbolic and concolic execution-based fuzzing
approaches have dominated the area of ”smart” fuzzing [12],
[38], [47], [51]. Mayhem [12], a system from CMU to au-
tomatically ﬁnd exploitable bugs in binary code, uses several
12
TABLE VII.
ANALYSIS OF NEW BUGS FOUND BY VUZZER.
Program
tcpdump
mpg321
mpg321
pdf2svg
pdf2svg
pdf2svg
tcptrace
gif2png
Bug Type
Out-of-bounds Read
Out-of-bounds Read
Double free
Null pointer deref (write)
Abort
Assert fail (abort)
Out-of-bounds Read
Out-of-bounds Read
Already ﬁxed?
Yes
No
No
Seems to be ﬁxed in poppler 0.49
Seems to be ﬁxed in poppler 0.49
Yes [4]
No
No
Reported?
No
Yes [2]
Yes [3]
No
No
No
Yes [5]
Yes [6]
program analysis techniques, including symbolic and concolic
execution, to reason about application behavior for a given
input. This is similar in spirit to VUzzer. However, since
the goal of VUzzer differs from that of Mayhem, VUzzer
does not require heavyweight program analysis techniques and
instead infers important properties of the input just by applying
heuristics based on lightweight program analysis. Similarly,
Driller [47] uses hybrid concolic execution techniques [33]
to assist fuzzing by solving branch constraints for deeper
path explorations. In [28], Karg´en et.al. propose a different
approach to generate fuzzed inputs. For a given application that
is being tested, their approach modiﬁes another input producer
application by injecting faults that inﬂuence the output. Using
this strategy, the buggy program generates mutated inputs.
However, it is not clear if these mutated inputs indeed affect
the way application consumes these inputs. TaintScope [49]—a
checksum-aware fuzzer—uses taint analysis to infer checksum-
handling code, which further helps fuzzing bypass checksum
checks. VUzzer can also beneﬁt from this (complementary)
technique while fuzzing. In a very recent work [8] (concurrent
to our work), the authors of AFLFAST proposed a markov-
model based technique to identify low-frequency paths to focus
fuzzing efforts in that direction. The heuristic, also used by
VUzzer partially, is to deprioritize paths that are executed by
maximum number of inputs. VUzzer’s error-handling basic-
block detection technique is similar to this, albeit much light-
weight. VUzzer applies other data- and control-ﬂow features
to speed-up the input generation.
There have been several other techniques to enhance
fuzzing [11], [43], [51]. VUzzer can also beneﬁt from these
approaches, in multiple ways. For example, Seed selection [43]
can help VUzzer start with a good set of seed inputs.
VII. CONCLUSIONS
This paper argues that the key strength of fuzzing is to
implement a lightweight, scalable bug ﬁnding technique and
applying heavyweight and non-scalable techniques, like sym-
bolic execution-based approaches, is not the deﬁnitive solution
to improve the performance of a coverage-based fuzzer. Af-
ter studying several existing general-purpose (black/graybox)
fuzzers, including the state-of-the-art AFL fuzzer, we note that
they tend to be application agnostic, which makes them less
effective in discovering deeply rooted bugs. The key limitation
of application-agnostic strategies is their inability to generate
interesting inputs faster. We address this problem by making
fuzzing an application-aware testing process.
We leverage control- and data-ﬂow features of the ap-
plication to infer several interesting properties of the input.
Control-ﬂow features allows us to prioritize and deprioritize
certain paths, thereby making input generation a controlled
process. We achieve this by assigning weights to basic blocks
and implement a weight-aware ﬁtness strategy for the input.
By using dynamic taint analysis, we also monitor several data-
ﬂow features of the application, providing us with the ability
to infer structural properties of the input. For example, this
provides us with information on which offsets in the input
are used at several branch conditions, what values are used as
branch constraints, etc. We use these properties in our feedback
loop to generate new inputs.
We have implemented our fuzzing technique in an open-
source prototype, called VUzzer and evaluated it on several
applications. We also compared its performance with that of
AFL, showing that, in almost every test case, VUzzer was
able to ﬁnd bugs within an order of magnitude fewer inputs
compared to AFL. This concretely demonstrates that inferring
input properties by analyzing application behavior is a viable
and scalable strategy to improve fuzzing performance as well
as a promising direction for future research in the area.
ACKNOWLEDGEMENTS
We would like to thank the anonymous reviewers for their
comments. We would also like to thank the LAVA team for
sharing the LAVA corpus privately with us much before the of-
ﬁcial public release. This work was supported by the European
Commission through project H2020 ICT-32-2014 SHARCS
under Grant Agreement No. 644571 and by the Netherlands
Organisation for Scientiﬁc Research through grants NWO
639.023.309 VICI Dowsing and NWO 628.001.006 CYBSEC
OpenSesame.
REFERENCES
“Peach fuzzer,” http://www.peachfuzzer.com/.
[1]
[2] 2016, http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=844634.
[3] 2016, https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=844626.
[4] 2016, https://bugs.freedesktop.org/show bug.cgi?id=85141.
[5]
[6]
[7] W. Afzal, R. Torkar, and R. Feldt, “A systematic review of search-based
testing for non-functional system properties,” Information and Software
Technology, vol. 51, no. 6, pp. 957–976, 2009.
2016, https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=844719.
2016, https://gitlab.com/esr/gif2png/issues/1.
[8] M. B¨ohme, V.-T. Pham, and A. Roychoudhury, “Coverage-based grey-
box fuzzing as markov chain,” in CCS’16. New York, NY, USA: ACM,
2016, pp. 1032–1043.
[9] C. Cadar, V. Ganesh, P. M. Pawlowski, D. L. Dill, and D. R. Engler,
“Exe: Automatically generating inputs of death,” in CCS’06. ACM,
2006, pp. 322–335.
[10] C. Cadar and K. Sen, “Symbolic execution for software testing: Three
decades later,” Commun. ACM, vol. 56, no. 2, pp. 82–90, Feb. 2013.
[11] S. K. Cha, M. Woo, and D. Brumley, “Program-adaptive mutational
fuzzing,” in S&P’15, May 2015, pp. 725–741.
13
[12] S. K. Cha, T. Avgerinos, A. Rebert, and D. Brumley, “Unleashing
mayhem on binary code,” in IEE S&P’12. Washington, DC, USA:
IEEE Computer Society, 2012, pp. 380–394.
[13] S. Clark, S. Frei, M. Blaze, and J. Smith, “Familiarity breeds con-
tempt: The honeymoon effect and the role of legacy code in zero-day
vulnerabilities,” in ACSAC’10. New York, NY, USA: ACM, 2010, pp.
251–260.
[14] B. Copos and P. Murthy, “Inputﬁnder: Reverse engineering closed
binaries using hardware performance counters,” in PPREW’15. New
York, NY, USA: ACM, 2015, pp. 2:1–2:12.
[15] DARPA CGC,
“Darpa
cyber
grand
challenge
binaries,”
[16] B.
https://github.com/CyberGrandChallenge.
with
an
http://moyix.blogspot.nl/2016/07/fuzzing-with-aﬂ-is-an-art.html.
Dolan-Gavitt,
“Fuzzing
aﬂ
is
art,”
[17] B. Dolan-Gavitt, P. Hulin, E. Kirda, T. Leek, A. Mambretti, W. Robert-
son, F. Ulrich, and R. Whelan, “Lava: Large-scale automated vulnera-
bility addition,” in IEEE S&P’16.
IEEE Press, 2016.
[18] Elias
Bachaalany,
“idapython:
Interactive
disassembler,”
https://github.com/idapython.
J. Foote, “Cert triage tools,” 2013.
[19]
[20] V. Ganesh, T. Leek, and M. Rinard, “Taint-based directed whitebox
fuzzing,” in ICSE’09.
IEEE Computer Society, 2009, pp. 474–484.
[21] P. Godefroid, “Random testing for security: blackbox vs. whitebox
fuzzing,” in Int. workshop on Random testing. New York, NY, USA:
ACM, 2007, pp. 1–1.
[22] P. Godefroid, N. Klarlund, and K. Sen, “Dart: directed automated
random testing,” SIGPLAN Not., vol. 40, no. 6, pp. 213–223, 2005.
[23] P. Godefroid, M. Y. Levin, and D. Molnar, “Automated whitebox fuzz
testing,” in NDSS’08.
Internet Society, 2008.
[24] ——, “Sage: Whitebox fuzzing for security testing,” Queue, vol. 10,
no. 1, pp. 20:20–20:27, Jan. 2012.
[26]
[25] C. D. Grosso, G. Antoniol, E. Merlo, and P. Galinier, “Detecting
buffer overﬂow via automatic test input data generation,” Computers
& Operations Research, vol. 35, no. 10, pp. 3125–3143, 2008.
I. Haller, A. Slowinska, M. Neugschwandtner, and H. Bos, “Dowsing
for overﬂows: A guided fuzzer to ﬁnd buffer boundary violations,” in
USENIX SEC’13. Berkeley, CA, USA: USENIX Association, 2013,
pp. 49–64.
[27] Hex-Rays,
https://www.hex-
disassembler,”
Interactive
“Ida:
rays.com/products/ida/.
[28] U. Karg´en and N. Shahmehri, “Turning programs against each other:
High coverage fuzz-testing using binary-code mutation and dynamic
slicing,” in FSE’15. New York, NY, USA: ACM, 2015, pp. 782–792.
[29] V. P. Kemerlis, G. Portokalidis, K. Jee, and A. D. Keromytis, “Libdft:
Practical dynamic data ﬂow tracking for commodity systems,” in
SIGPLAN/SIGOPS VEE ’12. New York, NY, USA: ACM, 2012, pp.
121–132.
[30] H. Kobayashi, B. L. Mark, and W. Turin, Probability, Random Pro-
cesses, and Statistical Analysis: Applications to Communications, Sig-
nal Processing, Queueing Theory and Mathematical Finance. Cam-
bridge University Press, Feb. 2012.
[31] C.-K. Luk, R. Cohn, R. Muth, H. Patil, A. Klauser, G. Lowney,
S. Wallace, V. J. Reddi, and K. Hazelwood, “Pin: Building customized
program analysis tools with dynamic instrumentation,” in PLDI’05.
New York, NY, USA: ACM, 2005, pp. 190–200.
[32] ——, “Pin: building customized program analysis tools with dynamic
instrumentation,” in ACM Sigplan Notices, vol. 40, no. 6. ACM, 2005,
pp. 190–200.
[33] R. Majumdar and K. Sen, “Hybrid concolic testing,” in ICSE’07.
Washington, DC, USA: IEEE Computer Society, 2007, pp. 416–426.
[34] T. Mantere and J. T. Alander, “Evolutionary software engineering, a
review,” Applied Soft Computing, vol. 5, no. 3, pp. 315–331, 2005,
application Reviews.
[35] B. P. Miller, L. Fredriksen, and B. So, “An empirical study of the
reliability of unix utilities,” Commun. ACM, vol. 33, no. 12, pp. 32–44,
1990.
[36] C. Miller
and
tion
and Z. N.
generation-based
J.
Peterson,
fuzzing,”
“Analysis
of muta-
[Online]. Avail-
2007.
able:
Miller/Whitepaper/dc-15-miller-WP.pdf
https://www.defcon.org/images/defcon-15/dc15-presentations/
[37] D. Molnar, X. C. Li, and D. A. Wagner, “Dynamic test generation to
ﬁnd integer bugs in x86 binary linux programs,” in USENIX Sec’09.
Berkeley, CA, USA: USENIX Association, 2009, pp. 67–82.
[38] M. Neugschwandtner, P. Milani Comparetti, I. Haller, and H. Bos, “The
borg: Nanoprobing binaries for buffer overreads,” in CODASPY ’15.
New York, NY, USA: ACM, 2015, pp. 87–97.
fuzzing
[39] OpenRCE,
framework,”
“Sulley
https://github.com/OpenRCE/sulley.
[40] OUSPG, “Radamsa fuzzer,” https://github.com/aoh/radamsa.
[41] P. Piwowarski, “A nesting level complexity measure,” SIGPLAN Not.,
vol. 17, no. 9, pp. 44–50, Sep. 1982.
[42] S. Rawat and L. Mounier, “An evolutionary computing approach for
hunting buffer overﬂow vulnerabilities: A case of aiming in dim light,”
in EC2ND’10.
IEEE Computer Society, 2010.
[43] A. Rebert, S. K. Cha, T. Avgerinos, J. Foote, D. Warren, G. Grieco,
and D. Brumley, “Optimizing seed selection for fuzzing,” in USENIX
Sec’14. Berkeley, CA, USA: USENIX Association, 2014, pp. 861–875.
[44] K. Serebryany, “Libfuzzer: A library for coverage-guided fuzz testing
(within llvm),” at: http://llvm.org/docs/LibFuzzer.html.
[45] S. Sparks, S. Embleton, R. Cunningham, and C. Zou, “Automated
vulnerability analysis: Leveraging control ﬂow for evolutionary input
crafting,” in ACSAC’07.
IEEE, 2007, pp. 477–486.
[46] M. Stamatogiannakis, P. Groth, and H. Bos, “Looking inside the black-
box: Capturing data provenance using dynamic instrumentation,” in
IPAW’14. Springer, 2015, pp. 155–167.
[47] N. Stephens, J. Grosen, C. Salls, A. Dutcher, R. Wang, J. Corbetta,
Y. Shoshitaishvili, C. Kruegel, and G. Vigna, “Driller: Augmenting
fuzzing through selective symbolic execution,” in NDSS’16.
Internet
Society, 2016, pp. 1–16.
[48] A. Takanen, J. DeMott, and C. Miller, Fuzzing for Software Security
Testing and Quality Assurance, 1st ed. Norwood, MA, USA: Artech
House, Inc., 2008.
[49] T. Wang, T. Wei, G. Gu, and W. Zou, “Taintscope: A checksum-aware
directed fuzzing tool for automatic software vulnerability detection,” in
IEEE S&P’10.
IEEE Computer Society, 2010.
[50] X. Wang, L. Zhang, and P. Tanofsky, “Experience report: How is
dynamic symbolic execution different from manual testing? a study on
klee,” in ISSTA’15. New York, NY, USA: ACM, 2015, pp. 199–210.
[51] M. Woo, S. K. Cha, S. Gottlieb, and D. Brumley, “Scheduling black-
box mutational fuzzing,” in CCS’13. New York, NY, USA: ACM,
2013, pp. 511–522.
[52] M. Zalewski, “American fuzzy lop,” at: http://lcamtuf.coredump.cx/aﬂ/.
14