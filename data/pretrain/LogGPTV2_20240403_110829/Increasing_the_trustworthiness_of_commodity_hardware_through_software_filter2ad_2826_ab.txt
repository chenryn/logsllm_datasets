rely on trusting the network driver, thus at least for
this example, the single point of failure exposes us to
denial of service, not violation of our security property
(under the assumption that hardware isolation holds in
the presence of driver faults). The situation is not as
straightforward if the driver needs to be trusted, and
thus exposing the trusted software to a single point of
failure.
• The performance of microkernel-based systems is crit-
ically dependent on the performance of the IPC prim-
itives used to communicate between software compo-
nents [19]. IPC performance of a microkernel is anal-
ogous to the system call performance of a traditional
operating system. Any replication needs to ensure IPC
performance is not adversely affected.
IV. INITIAL EXPERIMENTS
We now present our initial experiments where we evaluate
the effect of redundant execution on IPC performance. We
evaluate both dual modular redundancy and triple modular
redundancy scenarios on modern multi-core processors us-
ing a modiﬁed version of seL4. Speciﬁcally, we perform our
experiments on an ARM Cortex-A9 dual-core processor (a
Texas Instuments OMAP4 4460 running at 1.2GHz) on the
Pandaboard ES REV B1, with 1GB memory; and an x86-
64 quad-core Core i7 870 CPU running at 2.93 GHz with
4GB memory. The hyper-threading and speedstep functions
of Core i7 processor are disabled.
We simplify the general problem for this paper by only
running IPC microbenchmarks as the application. There are
no device drivers nor interrupts. Thus the sphere of repli-
cation under evaluation is CPU core, cache, and memory at
the hardware level; and seL4 itself and the microbenchmark
application.
Our prototype divides the multi-core processors and mem-
ory into distinct nodes that execute independently. The seL4
kernel and applications are replicated across the nodes such
that each node is a redundantly executing copy of the
kernel and application. The modiﬁcations to seL4 required to
achieve this, including starting in a consistent initial state,
and replicating user-level processes, are achieved for now
through manual modiﬁcation of the source code for each
replica. We leave the issue of how to systematically create
these replicas while preserving the formal veriﬁcation as
future work.
We chose the user-kernel system call boundary as the
granularity of comparing execution of the replicas. Each sys-
tem call must begin with the same inputs from an application
and produce the same outputs, i.e. seL4 and applications
must behave the same from an application perspective in
all replicas. In order to achieve this, the replica nodes must
be deterministic in their execution, and not diverge due to
variations in relative progress, or scheduling order. Thus any
divergence between replicas must be due to an inconsistent
behaviour of the hardware across replicas, and thus a fault
occurrence. We acknowledge that cross-checking replicas
at the system call boundary is insufﬁcient to guarantee the
kernel enforces isolation between security domains. We plan
to explore a more systematic checking of kernel state related
to enforcing isolation in the future. However, our choice
of the system call boundary allows us to evaluate what we
expect to be the biggest perturbation of IPC performance.
Listing 1 shows a pseudo-code outline of changes to
the seL4 system call path to support redundant execution
and cross-checking of system call inputs and outputs. The
changes save a copy of inputs used by a system call in a
region shared between processors, performs the system call,
and then copies the system call outputs to the shared area,
then waits on a barrier prior to comparing results with a
second processor. If the results are consistent, the processor
waits for the other processor(s) to complete their check. If
a comparison fails, the system is halted.
To evaluate the effect of our change on IPC performance,
we measure inter-address space one-way IPC costs for the
most commonly used variant of seL4 IPC (a call) as our
micro-benchmark. Our micro-benchmark uses the CPU cycle
counter to time-stamp immediately prior to when a process
is calling another process, and also when the other process
actually receives the IPC. We benchmark two variations.
The ﬁrst benchmark is sending a zero-length message which
represents the best-case overhead of IPC with no copying
overhead. For cross-checking, a zero-length message still has
system call inputs and outputs that describe the message sent
and received, which are the inputs and outputs checked via
replicas. The call is repeated 50 times and the best-case num-
ber is reported, so as to estimate the hot-cache performance,
and thus the lowest overhead – in practice the overhead will
be higher. The second benchmark is the same as the ﬁrst,
except the message size is the maximum size supported by
the fastpath optimization, which is a carefully crafted code
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:50:20 UTC from IEEE Xplore.  Restrictions apply. 
path aimed at delivering short messages efﬁciently. In the
case of ARM, the messages size is 4 words, for x86-64, 10
words. The results are shown in Table I.
Listing 1. Checking Pseudo Code
/* enter the kernel by a system call */
s a v e _ i n p u t _ t o _ s h a r e d _ a r e a () ;
/* call normal kernel path */
h a n d l e _ s y s t e m _ c a l l () ;
/* about to leave the kernel */
s a v e _ o u t p u t _ t o _ s h a r e d () ;
/*
* count is an array can be
* acc ess ed by all nodes , it is indexed
* by the node id .
*/
count [ m y _ n o d e _ i d ]++;
p a i r e d _ n o d e _ i d = ( m y _ n o d e _ i d + 1) %
t o t a l _ n o d e _ n u m b e r ;
/* wait until all c oun ter s are equal */
c o u n t _ b a r r i e r () ;
/* c o m p a r i n g the content with the next
ad jac ent node */
correct =
d o _ c o n t e n t _ c h e c k i n g ( p a i r e d _ n o d e _ i d ) ;
if (! correct ) h a l t _ t h e _ n o d e () ;
/*
* inc rea se the counter again and sync
* again to make sure the che cki ng on all
* nodes fin ish ed s u c c e s s f u l l y
*/
count [ m y _ n o d e _ i d ]++;
c o u n t _ b a r r i e r () ;
/* now the inputs and outputs are
c o n s i s t e n t */
r e t u r n _ f r o m _ s y s c a l l () ;
INTER-AS IPC Call BENCHMARK (CYCLES)
Table I
TMR
Benchmark
N/A
ARM (0 words)
N/A
ARM (4 words)
1560
x86-64 (0 words)
x86-64 (10 words)
2052
DMR: Dual Modular Redundancy TMR: Triple Modular Redundancy
Baseline (single core)
280
302
784
860
DMR
738
1095
1336
1780
We see that the simple approach to comparing the inputs
and the outputs results in a signiﬁcant overhead being added
to IPC. This is attributable to the extra copying of the inputs
and outputs to the shared area visible to the other processors,
then comparison itself, which results in cache-line transfers
between cores, and lastly, the cache-line transfers required
when passing the barrier. The overhead obviously increases
with size of the inputs and outputs. We expected this
overhead to translate into reduced performance at the macro-
level, and are actively exploring potential optimisations,
in addition to constructing larger systems to quantify the
performance reduction.
V. RELATED WORK
A. Hardware
Hardware-based solutions to tolerating intermittent and
transient faults have a long history in safety critical comput-
ing, such as in aircraft [20], [21]. Using redundant CPUs,
memory, buses, and I/O, faults can be detected with dual
modular redundancy (DMR), or masked with triple modular
redundancy (TMR). Characteristics of solutions in the space
are self-checking or redundant vote veriﬁcation of outputs
and conﬁgurable isolation of failed components.
High-availability server hardware also uses DMR and
TMR in locked-step conﬁgurations, where individual micro-
processors are synchronised to the same clock and inputs,
and the processor outputs are compared to detect or mask
faults. More recently, HP NonStop Advance Architecture
(NSAA) loosely couples chip multiprocessors to achieve
DMR or TMR [22]. Instead of lock-stepping, NSAA de-
terministically executes the same instruction streams at
slightly differing rates, and synchronises and compares I/O
operations to redundant devices over redundant system-level
communication fabrics. The loose coupling is required as
lock-stepping is not possible in the presence of modern
processor features such a clock-scaling for power manage-
ment. The loose coupling also enables one CPU to execute
recovery code in the case of a faults, or TLB miss code,
and still eventually produce the same I/O output as non-
faulting code. They rely on hardware implementing fail-fast,
and not continuing erroneous execution, nor propagating
faults across CPUs. Fail-fast is not guaranteed in the case
of commodity CPUs.
Researchers have also observed that commodity chip
multiprocessors lack strong fault isolation between cores on
the chip, and have proposed conﬁgurable isolation between
cores [23]. They show graceful degradation over time by
simulating reconﬁguration in the presence of component
failures. This approach provides stronger guarantees against
fault propagation, but still relies on “external” result com-
parison in the I/O subsystem or other external checker not
present in commodity platforms to achieve fault masking.
Conﬁgurable isolation is complementary to our approach,
but conﬁguration would require a consensus across more
than a single core to strengthen isolation in the presence of
an erroneously executing core.
B. Software
Most of the software based solutions focus on application-
level fault tolerance or service availability. Compiler-based