0/3
27/36
0/0
6/6
0/0
8/9
111/128
169/188
6/6
4/4
159/182
105/122
0.88
Recall
0/22
1/39
2/27
40/118
24/73
28/156
15/91
107/281
8/36
4/19
115/297
115/300
198/467
0/1
68/109
0/39
27/39
0/5
6/12
0/15
8/15
111/171
169/342
6/24
4/11
159/262
105/153
0.32
Recall
13/13
18/33
5/5
18/18
18/21
12/12
12/14
69/69
10/13
9/9
75/75
75/75
150/152
6/6
46/46
24/24
24/24
1/1
4/4
13/13
43/43
100/100
123/123
5/5
4/4
130/130
93/93
0.96
Virtual Methods
Recall
23/30
85/101
6/7
84/101
84/101
35/47
35/47
321/427
22/39
162/170
348/453
341/453
484/674
8/8
119/159
101/119
101/119
1/1
16/19
23/30
103/106
645/663
139/249
16/20
9/12
842/889
472/487
0.82
Prec
23/24
85/98
6/6
84/86
84/86
35/43
35/37
321/325
22/22
162/162
348/352
341/345
484/490
8/8
119/119
101/102
101/103
1/1
16/17
23/24
103/103
645/648
139/217
16/16
9/9
842/871
472/475
0.96
F
1.00
0.71
1.00
1.00
0.92
0.96
0.89
1.00
0.87
1.00
1.00
1.00
0.99
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
0.97
1.00
1.00
1.00
0.97
0.96
0.97
6.4 Class Membership Results
Table 3 shows the edit distance results under three different ex-
periments. For each experiment, we report the number of edits
between OOAnalyzer’s results and the ground truth, which we
call the absolute edit distance. Since a program with more methods
naturally provides more opportunities for mistakes, we also report
the edit distance as a percentage of the number of C++ methods in
the program, which we call the relative edit distance.
Without RTTI. In the first experiment, OOAnalyzer does not
use RTTI data (Section 2.4) even if it is available, which may be
appropriate when analyzing malicious or untrusted code. Even in
this conservative experiment, OOAnalyzer achieves an average
relative edit distance of 21.8%, which indicates that OOAnalyzer
is recovering the vast majority of classes correctly. OOAnalyzer is
able to recover C++ abstractions equally well for cleanware and
malware, with the average relative edit distances being 23.6% and
18.3% respectively. In addition to the edit distances, Table 3 also
displays the types of edits encountered in this experiment, which
provides some insight into the types of mistakes that OOAnalyzer
made. The most common edits are moves (40.9%) and adds (29%),
which indicate assigning a method to the wrong class, and failing to
detect a method. The large number of adds is in part due to an effort
to reduce removals (5.8%). The higher number of splits (14.8%) than
joins (9.4%) shows that OOAnalyzer is slightly more inclined to
merge classes incorrectly than to fail to do so. This is likely caused
by shared method implementations which is discussed more in
Section 7.1.
Using RTTI. The second experiment evaluates OOAnalyzer’s
performance when it leverages RTTI data (Section 2.4). Since RTTI
provides useful information about the polymorphic methods and
classes in the class hierarchy, it is expected that OOAnalyzer would
perform better when given access to this information. However, the
results of this experiment show that OOAnalyzer only performs
marginally better with access to RTTI, with the maximum and
average improvements in absolute edit distance being 9 and 1.7. We
found these results to be consistent with our intuition that most
of OOAnalyzer’s edits are related to non-polymorphic classes and
methods; polymorphic methods are generally easier to recover.
Session 3A: Binary AnalysisCCS’18, October 15-19, 2018, Toronto, ON, Canada436Without hypothetical reasoning. The final experiment is the same
as the Without RTTI experiment, except that OOAnalyzer’s hy-
pothetical reasoning component is disabled. By comparing this
experiment to the first experiment, which uses OOAnalyzer’s hy-
pothetical reasoning component, we can measure the contribution
of hypothetical reasoning. Without hypothetical reasoning, OOAn-
alyzer performs significantly worse, yielding 81% as the average
relative edit distance (compared to 21.8% with hypothetical rea-
soning). These results highlight the importance of hypothetical
reasoning, and reinforce the challenge of coping with uncertainty
while recovering C++ abstractions.
6.5 Method Properties
Table 4 shows how well OOAnalyzer identifies special method prop-
erties in the absence of RTTI. Specifically, OOAnalyzer attempts
to identify constructors, destructors, virtual methods, and virtual
function tables. Each group of columns in the table reports the
recall, precision and F-score (i.e., the harmonic mean of precision
and recall) for one of these properties. For example, on construc-
tors, CImg has a recall of 44/51, which indicates that OOAnalyzer
detected 44 of the 51 constructors in CImg, and a precision of 44/53,
which indicates that OOAnalyzer reported 53 constructors total, of
which 44 were correct. As expected, this results in a relatively high
F score of 0.85.
With a few exceptions, OOAnalyzer is able to identify construc-
tors with very high accuracy (average F score of 0.87). Unfortu-
nately, destructor detection has proven more difficult, and OOAna-
lyzer only achieves an average F score of 0.41. We have found that
destructors are often trivial implementations that are optimized
away, which makes them more difficult to distinguish. Finally, like
many other tools in this area, OOAnalyzer is able to identify vir-
tual function tables with high accuracy (average F score of 0.97).
As a result, OOAnalyzer can effectively distinguish most virtual
methods as well (average F score of 0.88). Unlike most other tools,
however, OOAnalyzer also reasons about non-virtual methods and
non-polymorphic classes.
6.6 Performance
All experiments were performed using a single core of an Intel
Xeon E5-2695 2.4Ghz CPU with 256 GiB of memory. Table 5 lists
OOAnalyzer’s running time and memory usage (in minutes and
mebibytes, respectively) for each benchmark. OOAnalyzer’s total
running time ranges from 30 seconds to 22.7 hours, with a median
and mean of 0.2 and 2.3 hours. OOAnalyzer’s maximum memory
usage ranges from 43.1 MiB to 3.5 GiB, with a median and mean
of 0.7 and 1.0 GiB, respectively. Larger executables clearly tend
to require more time and memory, as expected. On most larger
executables, Prolog reasoning dominates the runtime of the system,
whereas for smaller executables, fact exporting takes the bulk of
the time.
7 DISCUSSION AND LIMITATIONS
7.1 Optimizations
Some compiler optimizations can modify executable code in ways
that stops OOAnalyzer’s rules from working as intended. One of
the most problematic classes of optimization is Whole Program
Optimization (WPO) [17] (enabled by the GL switch in Microsoft
Visual C++), which allows the compiler to perform optimizations
across multiple compilation modules at link time. Unfortunately,
this switch allows the compiler to violate ABI conventions in func-
tions that are not exported. For example, the compiler may decide
to pass object pointers to methods in a register other than ecx,
even if that method was declared to use the thiscall convention
(Section 2.3). These optimizations make it more difficult to identify
and track the data flow of object pointers.
Another problematic optimization is when the linker reuses iden-
tical function implementations. If the linker detects two symbols
that consist of exactly the same executable code, it may only store
one copy of the code and point both symbols at the same address.
This is problematic for one of the fundamental assumptions in OO-
Analyzer, which is that each method in the executable may only
belong to one class. This optimization can cause OOAnalyzer to
mistakenly conclude that two separate classes need to be merged.
For instance, if the methods Ma and Mb have identical implemen-
tations it’s possible that they will both be assigned to the same
address. If Ma is on class Cla and Mb is on class Clb, then OOAna-
lyzer would likely make the errant conclusion that Cla and Clb are
actually the same class because it has no way of knowing that Ma
and Mb are distinct at the source code level. Such challenges cause
difficulty for human reverse engineers as well, and demonstrate the
complexity of the problem.
In some situations, optimizing compilers will inline a function
by replacing a call to that function with a copy of the function’s
body. Unfortunately, inlining makes recovering C++ abstractions
more difficult, since any behavior attributed to a particular function
could actually be caused by an inlined function call. One of the
most common cases is when constructors inline other constructors
(or destructors inline destructors), which happens frequently be-
cause of inheritance and composition. When a constructor calls
another constructor without inlining, it is easy to detect and usu-
ally indicates that the two constructors are on related classes. In
the presence of inlining, it may not even be clear that there are
two constructors involved. Many of OOAnalyzer’s rules have been
adjusted to account for common inlining situations. For example,
when a constructor calls the constructor of an inherited class and
the call is inlined, it is still possible to detect the inheritance rela-
tionship because the inlined code will include vftable (or vbtable)
installations for both constructors. Such rules are among the most
complex in OOAnalyzer, however, and it is impossible to handle all
inlined situations perfectly.
7.2 Other platforms
OOAnalyzer is designed to analyze Windows executables produced
using the Visual C++ ABI [11]. On many other platforms (e.g., Linux
and Unix), C++ compilers target the Itanium C++ ABI instead [13].
Adding support to OOAnalyzer for the Itanium C++ ABI would
primarily consist of adjusting the executable fact exporter to be
able to detect the different conventions used for operations such
as method calls and installing virtual function and base tables. We
expect that few changes would be needed to most of the regular
reasoning rules, because they reason at a semantic level that should
be preserved across ABIs. However, the current hypothetical rea-
soning rules can be thought of as heuristics that are tuned for code
Session 3A: Binary AnalysisCCS’18, October 15-19, 2018, Toronto, ON, Canada437Program
CImg
Firefox
light-pop3-smtp
log4cpp Debug
log4cpp Release
muParser Debug
muParser Release
MySQL cfg_editor.exe
MySQL connection.dll
MySQL ha_example.dll
MySQL libmysql.dll
MySQL mysql.exe
MySQL upgrade.exe
optionparser
PicoHttpD
TinyXML Debug
TinyXML Release
x3c
Malware 0faaa3d3
Malware 29be5a33
Malware 6098cb7c
Malware 628053dc
Malware 67b9be3c
Malware cfa69fff
Malware d597bee8
Malware deb6a7a1
Malware f101c05e
Average
Ver. Opt. Com-
piler
VS10
1.0.5
52.0
✓ VS15
VS10
608b
VS10
1.1
✓ VS10
1.1
VS10
2.2.3
2.2.3
✓ VS10
✓ VS12
5.2.0
VS12
5.2.0
✓ VS12
5.2.0
5.2.0
✓ VS12
✓ VS12
5.2.0
✓ VS12
5.2.0
VS10
1.3
VS10
1.2
2.6.1
VS10
✓ VS10
2.6.1
VS10
1.0.2
VS9
VS9
VS9
VS10
VS11
VS10
VS10
VS9
VS9
✓
Size
(KiB) Time
(min)
20.3
590
505
4.5
2.5
132
5.7
264
1.7
97
9.2
664
302
4.6
13.0
4,386
1.5
136
0.3
54
4,570
17.1
18.9
4,678
20.1
5,321
1.6
55
6.4
386
594
10.3