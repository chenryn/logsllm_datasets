Our experiments revealed that while both distance met-
rics work quite well, cosine is less sensitive to the thresh-
old parameters used in partitioning clusters. Hence, we
use cosine distance in this paper’s experiments and de-
scribe the hierarchical edit distance metric in the ex-
panded technical report [35].
Using a similarity metric, we construct the partially-
connected similarity graph. An edge connects a pair of
nodes if the similarity of the representative sessions is
above a threshold, here 0.8. We then build a star cover
over the similarity graph. Each star cluster is a group of
similar sessions that presumably are variants of the same
exploit. The cluster set is then passed to the generaliza-
tion module to produce the automaton signature.
5.2 Cluster Generalization and Signature
Generation
Signature generation devises a NIDS signature from a
cluster of similar connections or sessions. We generalize
variations observed in a cluster’s data. Assuming effec-
tive clustering, these variations correspond to obfusca-
tion attempts or differences among variants of the same
attack. By generalizing the differences, we produce a
resilient signature that accepts data not necessarily ob-
served during the training period.
The signature is a ﬁnite state automaton. We ﬁrst con-
struct a probabilistic ﬁnite state automaton (PFSA) ac-
cepting exactly the event sequences contained in a clus-
ter, with edge weights corresponding to the number of
times an edge is traversed when accepting all cluster data
exactly once. PFSA learning algorithms [24] then use
stochastic measures to generalize the data variations ob-
served in a cluster. In this work, we generalized HTTP
connection-level signatures with the sk-strings method
[24], an algorithm that merges states when they are
probabilistically indistinguishable. Session-level clus-
ters were generalized with beam search [17]. Our algo-
rithm uses both sk-strings and simulated beam anneal-
ing [23] to generalize NetBIOS signatures. These gen-
eralizations add transitions into the state machine to ac-
commodate such variations as data reordering and alter-
ation of characters in an attack string. Likewise, repeated
strings may be generalized to allow any number of re-
peats.
We further generalize signatures at points of high data
104
14th USENIX Security Symposium
USENIX Association
variability. Subsequence creation converts a signature
that matches a sequence of session data into a signa-
ture that matches a subsequence of that data by insert-
ing “gaps” that accept any sequence of arbitrary sym-
bols. We insert gaps whenever observing four or more
patterns with a common preﬁx, common sufﬁx, and one
dissimilar data element. For example, let A, B ∈ Σ∗
and v, w, x, y ∈ Σ. If the signature accepts AvB, AwB,
AxB, and AyB, then we replace those four sequences
with the regular expression A[.∗]B. Intuitively, we have
identiﬁed a portion of the signature exhibiting large vari-
ation and allow it vary arbitrarily in our ﬁnal signature.
Nemean’s generalized signatures can thus detect varia-
tions of observed attacks.
Figure 4 shows a session-level signature for Welchia, a
worm that exploits a buffer overﬂow. Nemean’s general-
ization produced a signature that matches a wide class of
Welchia scans without losing the essential buffer over-
ﬂow information characteristic to the worm. Figure 5
shows connection-level signatures for Nimda, a Win-
dows Media Player exploit, and the Deloder NetBIOS
worm. The connection-level Nimda signature is an ex-
ample of a signature for an exploit with high diversity. In
particular, note that the subsequence creation generaliza-
tion allows this signature to match a wide class of Nimda
attacks. The Windows Media Player exploit is represen-
tative of an HTTP exploit where the size of the exploit
URL is small. Previous signature generation techniques,
such as Honeycomb, fail for small URLs. The Deloder
signature demonstrates the capability of Nemean to gen-
erate signatures for exploits using more complex proto-
cols like NetBIOS/SMB.
6 Data Collection
The data used for our evaluation comes from two
sources: (i) honeypot packet traces collected from un-
used address space that we used to build signatures
and evaluate the detection capability of Nemean and
(ii) packet traces collected from our departmental border
router that we used to test the resilience of our signatures
to false positives.
• Production Trafﬁc: Obtaining packet traces for
live network trafﬁc is a challenge due to privacy con-
cerns. While network operators are amenable to sharing
ﬂow level summaries, anonymizing payloads remains an
unsolvable problem and as such its hard to obtain packet
traces with application payloads.
We were able to obtain access to such data from our
department’s border router. The network is a sparsely
allocated, well managed /16 network with approximately
24 web servers and around 400 clients. We were able
to passively monitor all outgoing and incoming HTTP
packets on this network for an 8 hour period. Table 1
provides a summary of this dataset.
• Honeypot Trafﬁc: Trafﬁc from two unused /19
IP address blocks totaling 16K addresses from address
ranges allocated to our university was routed to our
honeynet monitoring environment. To normalize the
trafﬁc received by our infrastructure a simple source-
ﬁltering rule was employed: one destination IP address
per source. Connections to additional destination IP ad-
dresses were dropped by the ﬁlter.
These ﬁltered packets were subsequently routed to one
of two systems based upon type-of-service. HTTP re-
quests were forwarded to a fully patched Windows 2000
Server running on VMware. The NetBIOS/SMB trafﬁc
was routed to a virtual honeypot system similar to Hon-
eyd. We routed NetBIOS/SMB packets to an active re-
sponder masquerading as an end host offering NetBIOS
services rather than to the Windows 2000 Server for two
reasons [33]. First, the fully patched Windows 2000
Server often rejected or disconnected the session before
we had enough information to classify the attack vector
accurately. This could be due to invalid NetBIOS names
or user/password combinations. Our active responder ac-
cepted all NetBIOS names and user/password combina-
tions. Second, Windows 2000 servers limit the number
of simultaneous network share accesses which also in-
hibit connection requests from succeeding.
We collected two sets of traces, a short term training
set (2 days) and a longer testing set (7 days) to evaluate
Nemean detection capability as summarized in Table 2.
The reduction in the volume of port 80 trafﬁc moving
from the 2-day to the 5-day dataset is not uncommon in
honeynets due to the bursty nature of this trafﬁc often
associated with botnet activity [16].
7 Evaluation
We tested the effectiveness of Nemean’s HTTP and Net-
BIOS signatures and examined the session clusters used
to produce these signatures. Section 7.1 reveals the major
classes of attacks in our recorded data and quantitatively
measures the clusters produced by the clustering mod-
ule. We performed an evaluation of the detection and
false positive rates of Nemean’s signatures and compare
our results with Snort’s HTTP capabilities. Finally, we
provide a qualitative discussion of our experience with
Honeycomb.
7.1 Evaluating the Clusters
• HTTP Clusters: Figure 6 provides an overview of
the major HTTP clusters in our learning data set. Web-
DAV scans account for the majority of the attacks in
our data set. WebDAV is a collection of HTTP exten-
sions that allow users to collaboratively edit and man-
USENIX Association
14th USENIX Security Symposium
105
Data Flow
Internal clients -> External servers
External clients -> Internal servers
No. Clients No. Servers No. Sessions No. Connections
106,456
87,545
380
18,634
16,826
28,491
4,422
24
Table 1: Production data summary (HTTP: 8 hours, 16GB).
Port
80
139
445
Packets
278,218
192,192
1,763,276
Learning Data (2 days)
Sources Connections
25,587
10,859
1,434
3,415
35,307
14,974
Sessions
12,545
1,657
19,763
Packets
100,291
6,764,876
6,661,276
Test data (7 days)
Sources Connections
12,903
12,925
539,334
1,662,571
1,171,309
383,358
Sessions
5,172
24,747
37,165
Table 2: Honeypot data summary.
age documents in remote web servers. Popular WebDAV
methods used in exploits include OPTIONS, SEARCH,
and PROPFIND and are supported by Microsoft IIS web
servers. Scans for exploits of WebDAV vulnerabilities
are gaining in popularity and are also used by worms
like Welchia. Nimda attacks provide great diversity in
the number of attack variants and HTTP URL obfusca-
tion techniques. These attacks exploit directory traver-
sal vulnerabilities on IIS servers to access cmd.exe or
root.exe. Figure 5 contains a connection-level signa-
ture for Nimda generated by Nemean. Details of other
observed exploits, such as Frontpage, web crawlers and
open-proxy, are provided in [35].
• NetBIOS Clusters: Worms that are typically better
known as email viruses dominate the NetBIOS clusters.
Many of these viruses scan for open network shares and
this behavior dominated the observed trafﬁc. They can
be broadly classiﬁed into three types:
1. Hidden and open share exploits: This includes
viruses,
including LovGate [5], NAVSVC, and De-
loder [12], that use brute force password attacks to look
for open folders and then deposit virus binaries in startup
folders.
2. MS-RPC query exploits: Microsoft Windows pro-
vides the ability to remotely access MSRPC services
through named pipes such as epmapper (RPC End-
point Mapper), srvsvc (Windows Server Service), and
samr (Windows Security Account Manager). Viruses
often connect to the MSRPC services as guest users and
then proceed to query the system for additional informa-
tion that could lead to privileged user access. For exam-
ple, connecting to the samr service allows the attacker
to obtain an enumeration of domain users,
3. MS-RPC service buffer overﬂow exploits: The most
well-known of these exploits are the epmapper ser-
vice which allows access to the RPC-DCOM exploit [15]
used by Blaster and the more recent lsarpc exploit
used by Sasser [25]. We provide more details in the tech-
nical report [35].
• Cluster Quality: We quantitatively evaluated the
quality of clusters produced by the star clustering algo-
rithm using two common metrics: precision and recall.
Precision is the proportion of positive matches among all
the elements in each cluster. Recall is the fraction of pos-
itive matches in the cluster among all possible positive
matches in the data set. Intuitively, precision measures
the relevance of each cluster, while recall penalizes re-
dundant clusters.
We ﬁrst manually tagged each session with conjec-
tures as shown in Figure 6. Conjectures identiﬁed ses-
sions with known attack types and it is possible for a
session to be marked with multiple conjectures. It is im-
portant to note that these conjectures were not used in
clustering and served simply as evaluation aids to esti-
mate the quality of our clusters.
The conjectures allow us to compute weighted preci-
sion (wp) and weighted recall (wr) for our clustering.
As sessions can be tagged with multiple conjectures, we
weight the measurements based upon the total number
of conjectures at a given cluster of sessions. We compute
the values wp and wr as follows: Let C be the set of all
clusters, J be the set of all possible conjectures, and cj
be the set of elements in cluster c labeled with conjecture
j. Then |cj| is the count of the number of elements in
cluster c with conjecture j.
wp = Xc∈C  |c|
=
1
|C| Xj∈J“
|C| Xc∈C Pj∈J
Pk∈J
|C| Xj∈J“
Pk∈J
|c|
wr = Xc∈C  |c|
|C| Xc∈C“
=
1
|cj |
|ck|
|cj |
|c| ”!
Pk∈J
|cj |2
|ck|
|cj |
|ck|
Pk∈J
|ck| Xj∈J
|cj |
|Cj |”!
|Cj|”
|cj |2
106
14th USENIX Security Symposium
USENIX Association
CLUSTER 1:
9175 Unique client IPs,
Identified as Options
CLUSTER 2:
597 Unique client IPs,
CLUSTER 4:
742 Unique client IPs,
Identified as Nimda
Identified as Code Blue
Identified as Welchia
Identified as Search
CLUSTER 3:
201 Unique client IPs,
Identified as Search
Identified as Web Crawler
CLUSTER 5:
51 Unique client IPs,
Identified as Nimda
CLUSTER 17:
47 Unique client IPs,
Identified as Propfind
Identified as Options
CLUSTER 8:
20 Unique client IPs,
Identified as Nimda
CLUSTER 7:
11 Unique client IPs,
Identified as Windows Media Exploit:
CLUSTER 6:
10 Unique client IPs,
Identified as Search
CLUSTER 9:
8 Unique client IPs,
Identified as Code Red Retina
Identified as Search
CLUSTER 11:
6 Unique client IPs,
CLUSTER 19:
5 Unique client IPs,
:
:
:
:
:
:
5 (
15 (
10515 Sessions
: 10515 (100%)
735 Sessions
735 (100%)
2%)
808 Sessions
808 (100%)
794 ( 98%)
226 Sessions
99 ( 44%)
2%)
52 Sessions
52 (100%)
102 Sessions
102 (100%)
102 (100%)
20 Sessions
20 (100%)
11 Sessions
11 (100%)
10 Sessions
10 (100%)
8 Sessions
8 (100%)
5 ( 63%)
:
:
:
:
:
:
:
6 Sessions
6 (100%)
6 (100%)
5 Sessions
5 (100%)
5 (100%)
3 Sessions
3 (100%)
3 (100%)
2 Sessions
2 (100%)
3 Sessions
3 (100%)
2 Sessions
1 ( 50%)
1 Session
1 (100%)
1 Session
1 (100%)
1 (100%)
1 Session
1 (100%)
:
:
:
:
:
: