the Friedman ranking of each metric across datasets [55]. A lower
Friedman ranking indicates consistently higher performance over
all datasets. Platforms in both tables are ordered based on average
Friedman ranking over 4 evaluation metrics in ascending order. We
can see that average F-score is a representative metric, because the
ranking based on F-score values matches the ranking induced by
the Friedman metric.
4.2 Impact of Individual Controls
We have shown that higher complexity in the form of more user con-
trol contributes to higher optimized performance. Now we break-
down the potential performance gains from baseline configurations,
and investigate the potential gains contributed by each type of
control. In the collection of tunable controls and design decisions,
answering this question would tell us which decisions have the
most impact on the final performance. We start by tuning only
one dimension of control while leaving others at baseline settings.
Figure 5 shows the percentage improvement in performance from
the baseline setting for each platform and control dimension. Note
that Google and ABM are not included in this analysis. In addition,
we have 3 platforms (Amazon, BigML, PredictionIO) missing in the
Feature Selection column, one (Amazon) missing in the Classifier
Selection column. These are the platforms that do not support tun-
ing those respective control dimensions. We observe the largest
performance improvement of 14.6% (averaged across all platforms)
when giving users the ability to select specific ML classifiers. In fact,
in the case of Microsoft, F-score improves by 22.4% which is the
 0.6 0.7 0.8 0.9 1GoogleABMAmazonBigMLPredictionIOMicrosoftLocalAverage F-scoreBaselineOptimizedComplexityLowHigh 0 10 20 30 40AmazonBigMLPredictionIOMicrosoftLocalAmazonBigMLPredictionIOMicrosoftLocalAmazonBigMLPredictionIOMicrosoftLocalF-score Improvement (%)FeatureSelectionClassifierSelectionParameterTuningNo DataNo Data(a) Baseline performance.
Avg.
Avg.
F-score
(b) Optimized performance.
Avg.
Avg.
F-score
Platform
Avg. Fried.
Ranking
Amazon
Google
ABM
BigML
PredictionIO
Local
Microsoft
253.7
267.7
344.5
348.1
379.5
388.8
424.3
0.748 (250.5)
0.706 (261.4)
0.694 (285.8)
0.688 (326.8)
0.672 (389.2)
0.672 (411.9)
0.655 (477.3)
Platform
Avg. Fried.
Ranking
Local
Microsoft
PredictionIO
BigML
Amazon
Google
ABM
190.1
211.1
318.6
365.9
446.7
641.9
758.8
0.839 (179.4)
0.837 (186.5)
0.828 (245.7)
0.789 (307.5)
0.761 (545.3)
0.706 (692.6)
0.694 (784.3)
Accuracy
0.850 (269.5)
0.851 (217.7)
0.833 (366.5)
0.822 (347.2)
0.818 (432.6)
0.832 (401.8)
0.833 (391.9)
Accuracy
0.916 (184.2)
0.914 (190.3)
0.886 (238.7)
0.876 (281.7)
0.863 (524.3)
0.853 (606.7)
0.834 (774.1)
Avg.
Precision
0.782 (298.0)
0.751 (261.4)
0.738 (359.3)
0.741 (335.7)
0.682 (387.6)
0.668 (419.4)
0.715 (370.5)
Avg.
Recall
0.755 (196.7)
0.711 (330.4)
0.691 (366.6)
0.688 (385.6)
0.741 (308.9)
0.723 (322.1)
0.659 (457.6)
Avg.
Precision
0.984 (201.3)
0.954 (231.3)
0.779 (478.4)
0.880 (287.9)
0.826 (398.2)
0.744 (605.5)
0.735 (747.7)
Avg.
Recall
0.990 (195.5)
0.863 (236.3)
0.852 (311.5)
0.802 (351.4)
0.795 (318.9)
0.704 (662.9)
0.684 (729.1)
Table 3: Baseline and optimized performance of MLaaS platforms. The Friedman ranking of each metric is included in the
parenthesis. Lower Friedman ranking indicates consistently higher performance across all datasets.
(a) Ranking of classifiers using baseline parameters
BigML
LR (34.5%)
RF (26.1%)
DT (24.4%)
BAG (15.1%)
PredictionIO Microsoft
BST (50.4%)
AP (16.8%)
BPM (10.9%)
RF (7.6%)
LR (42.9%)
DT (38.7%)
NB (18.5%)
Rank
1
2
3
4
Local
BST (24.4%)
KNN (12.6%)
DT (10.9%)
RF (10.9%)
Rank
1
2
3
4
PredictionIO Microsoft
(b) Ranking of classifiers using optimized parameters
BigML
RF (32.8%)
BAG (30.3%)
LR (27.7%)
DT (9.2%)
LR (48.7%)
DT (36.1%)
NB (16.0%)
BST (43.7%) MLP (32.8%)
DJ (17.6%)
BST (27.7%)
RF (9.2%)
AP (16.0%)
RF (13.4%)
KNN (6.7%)
Local
Table 4: Top four classifiers in each platform using baseline/optimized parameters. Number in parenthesis shows the per-
centage of datasets where the corresponding classifier achieved highest performance. LR=Logistic Regression, BST=Boosted
Decision Trees, RF=Random Forests, DT=Decision Tree, AP=Average Perceptron, KNN=k-Nearest Neighbor, NB=Naive Bayes,
BPM=Bayes Point Machine, BAG=Bagged Trees, MLP=Multi-layer Perceptron, DJ=Decision Jungle.
highest among all platforms when we optimize the classifier choice
for each dataset. After the classifier dimension, feature selection
provides the next highest improvement in F-score (6.1%) across
all platforms, followed by the classifier parameter dimension (3.4%
improvement in F-score). The above results show that classifier is
the most important control dimension that significantly impacts
the final performance. To shed light on the general performance of
different classifiers, we analyze classifier performance with default
parameters and with optimized parameter configurations. Table 4(a)
shows the top 4 classifiers when using baseline (default) parameters.
It is interesting to note that no single classifier dominates in terms
of performance over all the datasets. Table 4(b) shows a similar
trend even when we optimize the parameters. This suggests that
we need a mix of multiple linear (e.g. LR, NB) and non-linear (RF,
BST, DT) classifiers to achieve high performance over all datasets.
Summary. Our results clearly show that platforms with higher
complexity (more dimensions for user control) achieve better per-
formance. Among the 3 key dimensions, classifier choice provides
the largest performance gain. Just by optimizing the classifier alone,
we can already achieve close to optimized performance. Overall, Mi-
crosoft provides the highest performance across all platforms, and
a highly tuned Microsoft model can produce performance identical
to that of a highly-tuned local scikit-learn instance.
Figure 6: Performance variation in MLaaS platforms when
tuning all available controls.
5 RISKS OF INCREASING COMPLEXITY
Our experiments in Section 4 assumed that users were experts on
each step in the ML pipeline, and were able to exhaustively search
for the optimal classifier, parameters, and feature selection schemes
to maximize performance. For example, for Microsoft, we evaluated
over 17k configurations to determine the configuration with opti-
mized performance. In practice, users may have less expertise, and
are unlikely to experiment with more than a small set of classifiers
or available parameters. Therefore, our second question is: Can in-
creased control lead to higher risks (of building poorly performing ML
models)? To quantify risk of generating poorly performing models,
we use performance variation as the metric, and compute variation
on each platform as we tune available controls.
5.1 Performance Variation across Platforms
First we measure the performance variation of each MLaaS plat-
form across a range of system configurations (of CLF, PARA, and
FEAT) described in Section 3. For each configuration and platform,
we compute average performance across all datasets. Then we iter-
ate through all configurations, and obtain a range of performance
scores which capture the performance variation. Each configuration
would generate a single point in the range of performance scores.
Higher variation means a single poor decision in design could pro-
duce a significant performance loss. We plot performance variation
results for each platform in Figure 6. As before, platforms on the x-
axis are ordered based on increasing complexity. First, we observe a
positive correlation between complexity of an MLaaS platform and
higher performance variation. Among MLaaS platforms, Microsoft
shows the largest variation, followed by less complex platforms
like PredictionIO and Amazon. For Microsoft, F-score ranges from
0.49 to 0.75. Also as expected, our local ML library has the highest
performance variation. The takeaway is that even though more
complex platforms have the potential to achieve higher perfor-
mance, there are higher risks of building a poorly configured (and
poorly performing) ML model.
Figure 7: Performance variation when tuning CLF, PARA and
FEAT individually, normalized by overall variation (white
boxes indicate controls not supported).
5.2 Variation from Tuning Individual Controls
Next we analyze the contribution of each control dimension towards
the variation in performance. When we tune a single dimension, we
keep the other controls at their default values set by the platform, i.e.
use the baseline settings. Figure 7 shows the portion of performance
variation caused by each control dimension, i.e. a ratio normalized
by the overall variation measured in our previous experiment. We
observe that classifier choice (CLF) is the largest contributor to
variation in performance. For example, in the case of Microsoft
and PredictionIO (both exhibiting large variation), over 80% of the
variation is captured by just tuning CLF. Thus, it is important to
note that even though CLF can provide the largest improvement
in performance (Section 4), if not carefully chosen, can lead to
significant performance degradation. On the other hand, for all
platforms, except Amazon, tuning the PARA dimension results in
the least variation in performance. We are unable to verify the
reason for the high variation in the case of Amazon (for PARA), but
suspect it is due to either implementation or default parameter
settings.
Partial Knowledge about Classifiers. Given the dispropor-
tionally large impact classifier choice has on performance and per-
formance variation, we want to understand how users can make
better decisions without exhaustively experimenting over the entire
gamut of ML classifiers. Instead, we simulate a scenario where the
user experiments with (and chooses the best out of) a randomly cho-
sen subset of k classifiers from all available classifiers in a platform.
We measure the highest F-score possible in each k-classifier subset.
Next, we average the highest F-score across all possible subsets of
size k. Results are shown in Figure 8 with all platforms support-
ing classifier selection. We observe a trend of rapidly improving
performance as users try multiple classifiers. We observe that just
trying a randomly chosen subset of 3 classifiers often achieves per-
formance that is close to the optimal found by experimenting with
all classifiers. In the case of Microsoft, we observe an F-score of 0.76
which is only 5% lower than the F-score we can obtain by trying
all 8 classifiers. Performance variation also decreases significantly
once a user explores 3 or more classifiers in these platforms.
 0 0.2 0.4 0.6 0.8 1GoogleABMAmazonBigMLPredictionIOMicrosoftLocalAverage F-scoreComplexityLowHigh 0 0.2 0.4 0.6 0.8 1AmazonBigMLPredictionIOMicrosoftLocalAmazonBigMLPredictionIOMicrosoftLocalAmazonBigMLPredictionIOMicrosoftLocalPerformance VariationFeatureSelectionClassifierSelectionParameterTuningNo DataNo DataFigure 8: Average performance vs. number of classifiers ex-
plored.
Summary. Our results show that increasing platform complex-
ity leads to better performance, but also leads to significant per-
formance penalties for poor configuration decisions. Our results
suggest that much/most of the gains can be achieved by focusing