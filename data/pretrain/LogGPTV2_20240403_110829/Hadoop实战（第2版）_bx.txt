使用seqdirectory命令将待处理的文件转化成序列文件。
使用seq2sparse命令将序列文件转化成向量文件。
使用kmeans命令对数据运行K-Means聚类算法。
将文本文件转化成向量需要两个重要的工具。一个是SequenceFilesFromDirectory类，它能将一个目录结构下的文本文件转化成序列文件，这种序列文件为一种中间文本表示形式。另一个是SparseVectorsFromSequenceFiles类，它使用词频（TF）或TF-IDF（TF-IDF weighting with n-gram generation）将序列文件转化成向量文件。序列文件以文件编号为键、文件内容为值。下面讨论如何将文本转换成向量。
使用路透社14578新闻集作为示例数据。这组数据被广泛应用于机器学习的研究中，它起初是由卡内基集团有限公司和路透社共同搜集整理的，目的是发展文本分类系统。路透社14578新闻集分布于22个文档中，除最后的reut2-0.14.sgm包含578份文件外，其余的每个文件包含1 000份文件。
路透社14578新闻集中的所有文件都为标准通用标记语言SGML（Standard Generalized Markup Language）格式，这种格式的文件与XML文件格式相似。可以为SGML文件创建一个分析器（parser），并将文件编号（document ID）和文件内容（document text）写到序列文件（SequenceFiles）中去。然后用前文提到的向量化工具将序列文件转化成向量。但是，更快捷的方式是使用Lucene Benchmark JAR文件提供的路透社分析器（the Reuters Parser）。Lucene Benchmark JAR是捆绑在Mahout上的，剩下的工作只是到Mahout目录下的examples文件夹运行org.apache.lucene.benchmark.utils.ExtractReuters类。在这之前，需要从http：//www.daviddlewis.com/resources/testcollections/reuters14578/reuters14578.tar.gz下载路透社新闻集，并将它解压到Examples/Reuters文件夹下。相关命令如下所示：
mvn-e-q exec：java
-Dexec.mainClass="org.apache.lucene.benchmark.utils.ExtractReuters"
-Dexec.args="reuters/reuters-extracted/"
使用解压得到的文件夹运行SequenceFileFromDirectory类。使用下面的脚本命令（Launch script）可以实现该功能：
bin/mahout seqdirectory-c UTF-8
-i examples/reuters-extracted/
-o reuters-seqfiles
这条命令的作用是将路透社文章转化成序列文件格式，如表13-7所示。
现在剩下的工作是将序列文件转化成向量文件。运行SparseVectorsFromSequenceFiles类即可实现该功能。命令如下：
bin/mahout seq2sparse-i reuters-seqfiles/-o reuters-vectors-w
注意，在seq2sparse命令中，参数-w用来表示是否覆盖输出文件夹。Mahout用来处理海量的数据，任何一个算法的输出都会花费很多时间。有了参数-w, Mahout就可以防止新产生的数据对未完全输出的数据进行破坏。除此之外，seq2sparse命令还有以下参数，如表13-8所示。
Mahout的seq2sparse命令的功能是从序列文件中读取数据，使用上面提到的默认参数，按照基于向量化（vectorizer）的字典生成向量文件。大家可以使用以下命令检查生成文件夹：
ls reuters-vectors/
执行上述命令后，结果如下所示：
dictionary.file-0
tfidf/
tokenized-documents/
vectors/
wordcount/
输出文件夹包含一个目录文件和四个文件夹。目录文件保存着术语（term）和整数编号之间的映射。当读取算法的输出时，这个文件是非常有用的，因此，需要保留它。其他四个文件夹是向量化过程中生成的文件夹。向量化过程主要有以下几步：
第一步，标记文本文档。具体过程是使用Lucene StandardAnalyzer将文本文档分成个体化的单词，将结果存储在tokenized-documents文件夹下。
第二步，对tonkenized文档进行迭代生成一个重要单词的集合。这个过程可能会使用单词统计、n-gram生成，这里使用的是unigrams生成。
第三步，使用TF将标记的文档转化成向量，从而创建TF向量。在默认情况下，向量化是使用TF-IDF，因此需要分两步来进行，一是文档频率（document-frequency）的统计工作；二是创建TF-IDF向量。TF-IDF向量在tfidf/vectors文件夹下。对于大多数的应用来说，需要的仅仅是目录文件和tfidf/vectors文件夹。
使用kmeans命令可以对数据运行K-Means聚类算法。命令如下：
bin/mahout kmeans
-i./examples/bin/work/reuters-out-seqdir-sparse/tfidf/vectors/
-c./examples/bin/work/clusters
-o./examples/bin/work/reuters-kmeans
-k 20-w
表13-9列出了K-Means命令参数的具体意义。
2.贝叶斯分类
在已经安装好Hadoop和Mahout的前提下，运行Bayes分类算法也比较简单。这里简要介绍Mahout的示例程序20NewsGroup的分类。实际上，除了20NewsGroup示例之外，还有Wikimapia数据，但由于其数据量达到了将近7GB，对大多数初学者而言并不合适。这里我们选择数据量较小且非常经典的20NewsGroup示例的分类。
什么是20NewsGroup？20新闻组包含20 000个新闻组文档，这些文档可以被分类成20个新闻组。20新闻组最初来源于Ken Lang的论文《Newsweeder：learning to filter netnews》。从那以后，20新闻组数据集合在机器学习领域越来越多地被用作实验数据。在文本聚类和分类方面的研究中使用尤为突出。20新闻组按照20个不同的类型进行组织，不同的类对应不同的主题。本书用到的20新闻组数据可以从http：//people.csail.mit.edu/jrennie/20Newsgroups/下载。在下载页面中，一共有三种版本的20新闻组数据，分别是20news-19997.tar.gz、20news-bydate.tar.gz和20news-18828.tar.gz。
20news-19997. tar.gz是最原始的版本数据；20news-bydate.tar.gz是按照日期进行排序的，其中的60%用来进行训练Bayes分类算法，40%用来测试Bayes分类算法，不包含重复新闻和标识新闻组的标题；20news-18828.tar.gz不包含重复的新闻，但是包含带有新闻来源和新闻主题的标题。这三种20新闻组数据都是以tar.gz形式存在的。读者使用tar命令对它们进行解压即可得到相应的数据。具体选择哪种数据对结果影响不大，这里我们选择20news-bydate.tar.gz。
介绍完20NewsGroup后，下面开始介绍如何运行Mahout自带的Naïve Bayes Classifier算法示例。
数据下载完成后并不可以直接使用，可以看到，数据在目录中均是以文件夹进行区分，即数据已经被分好类别。因此我们首先需要获取所需格式的数据。该操作可以通过如下两个命令完成：
获取训练集：
mahout org.apache.mahout.classifier.bayes.PrepareTwentyNewsgroups\
-p$DATA_HOME/20news-bydate-train\
-o$DATA_HOME/bayes-train-input\
-a org.apache.mahout.vectorizer.DefaultAnalyzer\
-c UTF-8
获取测试集：
mahout org.apache.mahout.classifier.bayes.PrepareTwentyNewsgroups\
-p$DATA_HOME/20news-bydate-test\
-o$DATA_HOME/bayes-test-input\
-a org.apache.mahout.vectorizer.DefaultAnalyzer\
-c UTF-8
在数据获取完成之后，通过“hadoop fs-put”命令将数据上传到HDFS，然后使用下列命令训练Bayes分类器：
mahout trainclassifier\
-i/user/hadoop/20news/bayes-train-input\
-o/user/hadoop/20news/newsmodel\
-type cbayes\
-ng 2\
-source hdfs
该命令将会在Hadoop上运行四个MapReduce作业。在命令执行的过程中，可以打开浏览器，在http：//localhost：50030/jobtracker.jsp上监视这些作业的运行状态。
运行下面的命令测试Bayes分类器：
mahout testclassifier\
-m/user/hadoop/20news/newsmodel\
-d/user/hadoop/20news/bayes-test-input\
-type cbayes\
-ng 2\
-source hdfs\
-method mapreduce
关于trainclassifier和testclassifier命令参数，这里不再详细介绍，大家可以通过“mahout[command]-h”命令来查看。
这就是Mahout自带的Bayes分类算法的示例程序。如果大家想要深入了解Mahout的分类算法，可以自行阅读Mahout Core API 0.7来了解已经实现的功能。
13.6 Mahout应用：建立一个推荐引擎
 13.6.1 推荐引擎简介
每天人们都会产生各种各样的想法：喜欢一个产品、不喜欢一件事、不关心某个东西。在人们毫无察觉的情况下，这些事情在悄然发生。一个正在播放的流行歌曲可会引起你的注意，也可能对你没有任何影响。歌曲引起你的注意可能是因为它很好听或者它很让人厌烦。同样的事情也会发生在其他的事情上。这就是人们的喜好。
每个人都有着不同的喜好，但是这些喜好会遵循着类似的规律。对于一个人来说，如果一个新的事物与他之前喜欢的事物相似，那么他很有可能也会喜欢这个新事物。如果一个外国人喜欢吃中国饺子，那么他很有可能会喜欢中国的包子。因为它们都是带馅的面食。此外，如果你的朋友喜欢周国平的散文，那么你也很有可能会喜欢周国平的散文。因为朋友之间会有一些共同的喜好。
在日常生活中，预测人们的喜好是没有问题的。假设有两个人A和B。对于B是否喜欢电影《指环王III》的问题，大多数人只能靠猜测。但如果A知道B喜欢《指环王I》和《指环王II》，那么可以推测B喜欢《指环王III》。如果B对指环王系列电影一点也不了解，A基本可以断定，B是不会喜欢《指环王III》的。
推荐引擎就是对人们的喜好做出预测的一种技术。它会依据已经获得的各种信息，对用户的购买行为做出预测，从而达到相应目的。现实生活中，人们都经历过网站向客户推荐产品，这些推荐都是基于客户浏览信息的推荐。网站试着推断客户的喜好，以此来向客户推荐他们可能会喜欢的产品。
卓越网使用了推荐引擎技术，在购买一本书的同时，网站会利用顾客的购买习惯和书籍之间的关系为顾客推荐他们可能会感兴趣的书籍或音像制品。例如，当某一名顾客想要购买《云计算》这本书时，在页面的下方会出现购买此商品的顾客同时购买的书籍。这样顾客就可能会顺便买一本相关的书。推荐引擎技术不仅可以帮助顾客更容易地发现自己想要的商品，而且可以帮助商家售卖更多的商品。社交网站人人网利用推荐引擎技术，向用户推荐一些可能是用户朋友的人。对于最有可能是朋友的人，人人网会自动把这些最可能是该用户朋友的人放在最前方，以供用户选择。推荐引擎技术已经悄然地影响着人们的生活，只是人们可能并没有注意它。
13.6.2 使用Taste构建一个简单的推荐引擎
Taste是Apache Mahout提供的一个协同过滤算法的高效实现，它是一个Java实现的可扩展的、高效的推荐引擎。Taste既实现了最基本的基于用户的和基于内容的推荐算法，同时也提供了扩展接口，使用户可以方便地定义和实现自己的推荐算法。同时，Taste不仅仅适用于Java应用程序，它还可以作为内部服务器的一个组件以HTTP和Web Service的形式向外界提供推荐的逻辑。Taste的设计使它能满足企业对推荐引擎在性能、灵活性和可扩展性等方面的要求。
Taste主要包括以下5个组件，具体如图13-6所示。
DataModel：DataModel是用户喜好信息的抽象接口，它的具体实现支持从任意类型的数据源抽取用户喜好信息。Taste默认提供JDBCDataModel和FileDataModel，分别支持从数据库和文件中读取用户的喜好信息。
UserSimilarity和ItemSimilarity：UserSimilarity用于定义两个用户间的相似度，它是基于协同过滤的推荐引擎的核心部分，可以用来计算用户的“邻居”，这里的“邻居”指与当前用户相似的用户。ItemSimilarity用来计算内容之间的相似度。
UserNeighborhood：UserNeighborhood用于基于用户相似度的推荐方法中，推荐的内容是通过找到与当前用户喜好相似的“邻居用户”的方式产生的。UserNeighborhood定义了确定邻居用户的方法，具体实现一般是基于UserSimilarity计算得到的。
Recommender：Recommender是推荐引擎的抽象接口，Taste中的核心组件。在程序中，为它提供一个DataModel，它可以计算出对不同用户的推荐内容。在实际应用中，主要使用它的实现类GenericUserBasedRecommender或GenericItemBasedRecommender，分别实现基于用户相似度的推荐引擎或者基于内容的推荐引擎。
图 13-6 Taste的主要组件图
安装Taste主要包括以下三部分内容：
如果需要build源代码或例子，则需要Apache Ant 1.5+或Apache Maven 2.0.10+。
Taste应用程序需要Servlet 2.3+容器，例如Jakarta Tomcat。
Taste中的MySQLJDBCDataModel实现需要MySQL 4.x+数据库。
安装Taste并运行Demo的步骤如下：
1）从SVN或下载压缩包中得到Apache Mahout的发布版本。
2）从Grouplens网站http：www.grouplens.org/node/12下载数据源：“1 Million MovieLens Dataset”。
3）解压数据源压缩包，将movie.dat和ratings.dat复制到Mahout安装目录下的taste-web/src/main/resources/org/apache/mahout/cf/taste/example/grouplens目录下。
4）回到core目录下，运行“mvn install”，将Mahout core安装在本地库中。
5）进入taste-web，复制../examples/target/grouplens.jar到taste-web/lib目录。
6）编辑taste-web/recommender.properties，将recommender.class设置为org.apache.mahout.cf.taste.example.grouplens.GroupLensRecommender。
7）在Mahout的安装目录下，运行“mvn package”。
8）运行“mvn jetty：run-war”，这里需要将Maven的最大内存设置为1024MB，即：MAVEN_OPTS=-Xmx1024MB。如果需要在Tomcat下运行，可以在执行“mvn package”后，将taste-web/target目录下生成的war包复制到Tomcat的webapp下，同时也需要将Java的最大内存设置为1024MB, JAVA_OPTS=-Xmx1024MB，然后启动Tomcat。
9）访问http：//localhost：8080/[your_app]/RecommenderServlet?userID=1，得到系统编号为1的用户推荐内容。参看图13-7，其中每一行的第一项是推荐引擎预测的评分，第二项是电影的编号。
图 13-7 Taste Demo运行结果界面
10）同时，Taste还提供Web服务访问接口，通过以下URL访问：http：//localhost：8080/[your_app]/RecommenderService.jws。
11）WSDL文件（http：//localhost：8080/[your_app]/RecommenderService.jws?wsdl）也可以通过简单的HTTP请求调用这个Web服务：http：//localhost：8080/[your_app]/Recommender-Service.jws?method=recommend＆userID=1＆howMany=10
13.6.3 简单分布式系统下基于产品的推荐系统简介
传统的推荐引擎算法多在单机上实现，它们只能处理一定量的数据。如果数据量达到一定的规模，传统的推荐引擎算法就会出现各种问题。
在传统的推荐算法中，算法会将用户喜欢的产品抽象成三个具体的数值：用户编号、产品编号和喜爱值。这里的喜爱值表示用户对产品的喜爱程度，它可以用一个具体数值来表示。例如，可以使用1到5来表示喜欢的程度：1表示非常不喜欢；2表示不喜欢；3表示没有任何感觉；4表示喜欢；5表示非常喜欢。也可以从1到5都表示喜欢，数值越大代表越喜欢。然后通过计算产品之间的相似性来向用户推荐产品。
分布式系统没有使用这种方法。分布式系统下的推荐算法主要包括以下几部分：
计算表示产品相似性的矩阵。
计算表示用户喜好的向量。
计算矩阵与向量的乘积，为用户推荐产品。
在开始介绍推荐算法之前，首先建立一组数据，如表13-10所示。在这组数据中，每条记录包含三个信息：用户编号、产品编号、用户对产品的喜爱值。
表13-10显示了5名顾客的购买历史。下面来介绍一种方法：使用共生矩阵来表示产品的相似性。在这里，产品的相似性是指产品出现在一起的次数。例如从表13-10中可以看出产品101和产品102一共出现过三次，分别是在用户1、用户2、用户5的物品清单上。那么在共生矩阵中101和102对应的元素值就应该为3。经过统计表13-10中的5个用户的购物清单可以使用表13-11的矩阵来表示。
表13-11中的行和列都是产品的编号。观察可知，该矩阵是一个对称矩阵。在计算过程中可以使用一些特殊技术对矩阵进行处理，使得程序的效率更高。原因是产品104和产品105出现的次数与产品105和产品104出现的次数必然是相同的。在共生矩阵中对角线的元素是没有意义的。计算时可以使用0进行代替。
除了共生矩阵外，还需要一个表示用户喜好的向量。在该向量中，对于用户购买过的产品必然会有一个表示喜好的数值，对于用户没有购买的产品，选择用数字0来表示该用户对该产品没有任何喜好。例如对于用户4而言，他的向量就应该是（5.0，0，3.0，4.5，0，4.0，0）。通过计算可以得到所有用户的喜爱值，如表13-12所示。
其实该表也是一个矩阵：矩阵的行值是产品编号，列值是用户编号，行列对应的元素值为用户对产品的喜爱值。通过观察可以发现，矩阵中包含很多0，这种矩阵可以称为稀疏矩阵。对于稀疏矩阵，同样可以采用一些技术手段使程序效率更高。
既然已表示了产品的相似性，也表示了用户对产品的喜爱，剩下的就是如何计算推荐的产品了。其实这很简单，只要将共生矩阵与用户的列向量相乘得到一个新的列向量即可。在新的列向量中，所有可以推荐产品对应的值最大，就是计算得到的推荐产品。
以向用户4推荐产品为例，如表13-13所示。
从结果可以看出，用户最喜欢产品103，但是103已经买过，因此无须推荐该产品。同理101、104、106也都可以不推荐。在可以推荐的产品102、105、107中，选择推荐产品102。因为102的计算机结果是三者之中最大的。
推荐结果已经有了，下面来分析这个结果是否合理。在所有可以推荐的产品中，推荐引擎选择了计算结果最大的产品。为什么计算结果最大的产品就是最合理的推荐产品呢？
回想整个计算过程。在计算结果中处在第2行的计算结果37是矩阵第2行元素和用户4的列向量的乘积，3×（5.0）+0×0+3×3+2×（4.5）+1×0+1×（4.0）+0×0=37。矩阵中的第2行表示的是所有产品和产品102同时出现的次数。如果用户对某个产品非常喜欢，而这个产品又和产品102同时出现的次数很多，那么乘积对计算结果的影响就会较大。这刚好就是推荐引擎要达到的目的，用户非常喜欢的产品和102很相似，推荐引擎可向用户推荐该产品。
对于大量数据，计算结果会非常大。但是没有关系，推荐引擎关注的是所有结果的大小关系，而不是具体的数值。因为最终向用户推荐的是可以推荐产品中计算结果最大的。在计算的过程中，对于不是最大的计算结果以及用户已经购买过的产品，推荐引擎无须推荐，因此也不必计算它们的结果。
通过分析可知，推荐引擎计算出的推荐结果是合理的。但为什么它适合大规模的数据呢？下面来说明这个问题。在计算共生矩阵的时候，每次只需考虑一个向量；在计算用户向量的时候只需考虑该用户的喜好；在计算推荐结果的时候只需考虑矩阵中的一列值。这都表明，这个方法可以使用MapReduce编程模式。
13.7 本章小结
本章对Mahout做了简要介绍，主要有Mahout的详细安装过程，Mahout API的介绍，Mahout中已经实现的频繁模式挖掘算法、分类算法、聚类算法，并着重对Kmeans聚类算法做了介绍。其中还涉及了聚类算法中的数据表示。在推荐引擎部分，着重从思想上介绍了如何在Hadoop云平台下实现分布式的推荐系统。Mahout虽然经过了几年的发展，但还是有很多地方值得去探索。如果读者有兴趣加入其中，可以访问Mahout的官网，与世界各地的开发者共同推动Mahout的发展。
第14章 Pig详解
本章内容
Pig简介
Pig的安装和配置
Pig Latin语言
用户定义函数
Zebra简介
Pig实例
Pig进阶
本章小结
14.1 Pig简介
作为Apache项目的一个子项目，Pig提供了一个支持大规模数据分析的平台。Pig包括用来描述数据分析程序的高级程序语言，以及对这些程序进行评估的基础结构。Pig突出的特点就是它的结构经得起大量并行任务的检验，这使得它能够处理大规模数据集。
目前，Pig的基础结构层包括一个产生MapReduce程序的编译器。Pig的语言层包括一个叫做Pig Latin的文本语言，它具有以下主要特性：
易于编程。实现简单的和高度并行的数据分析任务非常容易。由相互关联的数据转换实例所组成的复杂任务被明确地编码为数据流，这使他们的编写更加容易，同时也更容易理解和维护。
自动优化。任务编码的方式允许系统自动去优化执行过程，从而使用户能够专注于语义，而非效率。
可扩展性。用户可以轻松编写自己的函数来进行特殊用途的处理。
14.2 Pig的安装和配置
 14.2.1 Pig的安装条件
1.Hadoop 1.0.1