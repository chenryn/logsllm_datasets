2) We explore and evaluate the proposed LINKTELLER attack
against different DP GCN mechanisms as countermeasures.
Since there is no DP GCN mechanism proposed so far,
we evaluate LINKTELLER against a standard DP strategy
EDGERAND on graph, and a proposed DP GCN approach
LAPGRAPH.
3) We provide formal privacy analysis for the two DP GCN
approaches and an upper bound for general edge re-
identiﬁcation attack success rate on DP GCN mechanisms.
4) We design extensive experiments on eight datasets
under
the inductive setting and three datasets under
the transductive setting to show that
the proposed
LINKTELLER is able to achieve high attack precision and
recall, and signiﬁcantly outperforms the random attack and
two state of the art methods. We show that both DP GCN
approaches are not always resilient against LINKTELLER
empirically under mild privacy guarantees.
the empirical
tradeoff space
between (1) model utility—the quality of the trained GCN
model, and (2) privacy vulnerability—the risk of a GCN
model being successfully attacked. We carefully analyze
different regimes under which a data holder might want
to take different actions via evaluating a range of privacy
budgets, and we also analyze such tradeoff by selecting a
privacy budget via a validation dataset.
5) We systematically depict
II. PRELIMINARIES
A. Graph Neural Networks
Graph Neural Networks (GNNs) [12] are commonly used
in semi-supervised node classiﬁcation tasks on graphs. Given
a graph G = (V, E) with V denoting the nodes (n = |V |)
and E the edges, the adjacency matrix A ⊆ {0, 1}n×n is
a sparse matrix, where Aij = 1 denotes the existence of
an edge from node i to node j. Since Graph Convolutional
Network (GCN) [13] is one most representative class of GNN,
we next introduce GCN, which is a stack of multiple graph
convolutional layers as deﬁned below:
H l+1 = σ((cid:98)AH lW l),
(1)
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:20:38 UTC from IEEE Xplore.  Restrictions apply. 
2006
where (cid:98)A is the normalized adjacency matrix derived using a
certain normalization technique and σ is the activation func-
tion. For the l-th graph convolutional layer, we denote the input
node embeddings by H l, the output by H l+1, and the learnable
weight by W l. Each graph convolutional layer constructs the
embeddings for each node by aggregating the embeddings of
the node’s neighbors from the previous layer. Speciﬁcally, H 0
is the node feature matrix X.
GNNs were ﬁrst proposed for transductive training where
training and testing occur on the same graph. Recently, induc-
tive learning has been widely studied and applied [14]–[17],
which is a setting where the trained GNNs are tested on unseen
nodes/graphs. There are two main application scenarios for the
inductive setting: 1) training on an evolving graph (e.g., social
networks, citation networks) for future use when more nodes
arise in the graph; 2) training on one graph belonging to a
group of similarly structured graphs, and transfer the model to
other graphs from the similar distribution. We consider both
the inductive and transductive settings in this paper.
B. Differential Privacy
Differential privacy [18] is a privacy notion that ensures an
algorithm only outputs general information about its training
data without revealing the information of individual records.
Deﬁnition 1 (Differential Privacy). A randomized algorithm
M with domain N|X| is (ε, 0)-differentially private if for all
S ⊆ Range(M) and for all x, y ∈ N|X| such that (cid:107)x−y(cid:107)1≤ 1:
Pr[M(x) ∈ S] ≤ exp(ε) Pr[M(y) ∈ S]
There are two extensions of differential privacy to pre-
serve private information in graph data. Edge differential pri-
vacy [19] protects the edge information, while node differential
privacy [20] protects the existence of nodes. A recent work has
proposed an algorithm to generate synthetic graphs under edge
local differential privacy [21], which provides privacy protec-
tion when the graph data is distributed among different users.
We consider a practical privacy model in the data partitioning
scenario where one data holder only owns either the edge or
node information, and we aim to protect the edge information
from being leaked during the training and inference processes.
III. LINKTELLER: LINK RE-IDENTIFICATION ATTACK
In this section, we focus on understanding the risk of edge
privacy leakage caused by exposing a GNN trained on private
graph data via an inference API. We ﬁrst describe the interac-
tion model between data holders, and then the LINKTELLER
algorithm that probes the inference values between pairs of
nodes and uses these values as our conﬁdence on whether
edges exist between pairs of nodes. As we will see, this attack
allows us to recover a signiﬁcant number of edges from the
private graph structured data.
A. Interaction Model between Data Holders
We consider an ML application based on graph structured
data, where different data holders have access to different
information of the graphs (e.g., nodes or edges). More speciﬁ-
cally, the graph edge information is not available to everyone,
Fig. 2: The interaction model. In the training stage, ﬁrst, some
users send Alice the node set V (T ), the associated features X (T )
and labels y(T ). Next, Alice trains a GNN model with corresponding
adjacency matrix AV (T ), X (T ), and y(T ). In the inference stage, an
adversarial user Bob queries Alice with a test node set V (I) and
associated features X (I). Alice outputs the prediction matrix P (I).
since the edge connections or interactions between the node
entities usually contain sensitive information, which can be
exploited for malicious purposes. Thus, we ﬁrst make the
following abstraction of the data holder interaction.
As shown in Figure 2, the data holder Alice holds private
edge information of a graph, while other users hold the node
information, and due to privacy concerns, the sensitive edge
information from Alice cannot be directly shared. During the
training stage, in order to jointly train a GNN model on the
graph data, some users will ﬁrst send the node features of the
training graph X (T ) together with their labels y(T ) for a set
of nodes V (T ) to Alice; and Alice will train a GNN model
together with her edge connection information for future infer-
ence purpose by releasing an inference API to external users.
During the inference stage, a potential adversarial user Bob
will collect the node features of the inference graph X (I) (e.g.,
patients in the next month) and obtain their predicted labels
from Alice via the inference API. Alice will then send the
prediction matrix P (I) formed by the prediction vector for
each node to Bob. Without loss of generality, in the following
we will use “Bob” to denote both the general users during
training and inference time and the adversary during inference,
although they are usually independent users in practice.
, v(T )
2
1
, . . . , v(T )
Next, we will deﬁne such training interaction formally. In
particular, we will use the lower case letter to denote a vector
and the upper case letter to denote a matrix. We denote the
set of nodes of the training graph held by Bob as V (T ) =
{v(T )
n } ⊆ V . In the training stage, data holder
Bob will ﬁrst send the corresponding node features X (T ) and
labels y(T ) to Alice. Here each label y(T )
takes the value
from a set of c classes. Then the data holder Alice who holds
the node connection information will generate the adjacency
matrix AV (T ) ⊆ {0, 1}n×n, where AV (T ) ij = 1 if and only if
there is one edge between node v(T )
. This way, Alice
can leverage the node features and labels from Bob together
with her adjacency information to train a GNN model and
provide the model as a blackbox API, GBB(·,·), for Bob.
and v(T )
i
i
j
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:20:38 UTC from IEEE Xplore.  Restrictions apply. 
2007
AliceTraining StageGNNInference StageLinkTellerAttackGBB(·,·)APIBobThe learned model parameters are denoted as {W i}, where
W i ∈ Rdi×di+1 represents the weight of the i-th layer.
During the inference stage, the data holder Bob who owns
another set of nodes from the inference graph will query the
inference API for node prediction. In particular, given a set
of inference nodes V (I) ⊆ V , Bob will send the associated
node features X (I) to the trained GNN API GBB(·,·). Then
together with the private adjacency matrix of inference graph
AV (I), the API from Alice will make inference on the nodes,
and following the standard commercial ML inference services
such as Clarifai [22] and Google Vision API [23], Alice will
send the logits information back to Bob as below.
GBB(V (I), X (I)) = GNN(AV (I), X (I),{W i}).
For ease of reference, we denote the output prediction matrix
of GBB(V (I), X (I)) as P (I), which is of shape |V (I)| × c.
Each row of the prediction matrix corresponds to one node in
V (I), and each column corresponds to the conﬁdence for one
class. Alice will then send P (I) back to Bob.
We discuss more on the properties of V (T ), V (I), and V .
It is worth noting that V may not necessarily be a ﬁxed set.
New nodes and edges may arise with time elapsing, though
Alice always has an up-to-date view of the graph structure. In
this case, V (T ) can be nodes in the stale graph, and V (I) can
be the newly arisen nodes. There is also no restriction that all
nodes in V should form a connected component. Rather, V can
contain nodes in a group of graphs, as long as the grouping
makes logical and practical sense. Under this setting, V (T )
and V (I) can be the nodes of different graphs in the group.
B. Overview of the Attack
We will ﬁrst introduce the capability/knowledge of the at-
tacker, and then provide overview of the attack method. During
the attack, the attacker has access to a set of node features
and their labels which are required during training. During
inference, Bob is able to query the trained API for multiple
times with the subset of nodes that are of interest. That is to
say, the attacker’s capability includes the query access to a
blackbox GNN model and the obtained prediction probability
for a set of nodes during inference. Note that the attacker has
no information about the API model except that it is a GNN
model with unknown architecture and parameters. Unlike He
et al. [11] which assumes the knowledge of partial graphs or a
shadow dataset, here, we have no such additional assumptions.
The overview of the proposed link re-identiﬁcation attack
LINKTELLER is as follows. The attacker plays the role of Bob
in the interaction model (Figure 2). The goal of the attacker is
to recover the connections among the inference node entities.
Concretely, during inference, attacker Bob will query the GNN
API with a set of inference nodes. With the returned prediction
probability vectors, Bob will infer the connections between
certain pairs of nodes. The attack succeeds if the attacker can
correctly determine whether two given nodes are connected by
a link or not. We use the standard metrics precision, indicating
what fraction of pairs inferred to be connected are indeed
connected in the graph; and recall, indicating what fraction of
the connected pairs are revealed, to measure the attack success
rate. We also evaluate the AUC scores.
Intuitions: Our attack is inspired by the intuition behind
the training of a GNN model—during training, if two nodes u
and v are connected by an edge, GNN would “propagate” the
information of u to v. As a result, if there is an edge from u to
v, we would expect that changing the feature vector of u would
impact the prediction of v. Thus, if we can compute the inﬂu-
ence of one node on the other, we could use it to guess whether
there is an edge between the two nodes: If the inﬂuence value
is “large”, we would be more conﬁdent on the existence of
an edge; if the inﬂuence value is “small”, we would be more
conﬁdent that the nodes are not directly connected. Below, we
describe a concrete algorithm to approximate such an inﬂuence
value by probing the trained GNN inference API.
C. LINKTELLER: Edge Inﬂuence Based Attack
2
it
LINKTELLER attack proceeds in two phases. First, given
a collection of nodes V (I) at
the inference time and the
inference API, GBB(·,·), the attacker tries to calcluate the
inﬂuence value between each pair of nodes in V (I). Second,
LINKTELLER then sorts all pairs of nodes by their inﬂuence
value, and predict the ﬁrst m = ˆk · n(n−1)
node pairs with
highest inﬂuence values as “with edge” and all other pairs as
“without edge”. Here ˆk is a hyperparameter speciﬁed by the
attacker, which indicates his prior “belief” of the graph density.
We call ˆk the density belief , which is a key hyper-parameter
(details in Algorithm 1). In our experiments, we observe that
the attack performance will decrease slightly given the discrep-
ancy between estimated ˆk and ground truth k. Nevertheless,
we show that LINKTELLER remains more effective than the
state-of-the-art attacks even with inaccurate estimation for ˆk.
is also possible for attackers to further
estimate ˆk or the inﬂuence value threshold for edge re-
identiﬁcation with additional knowledge. For instance, if the
attacker has partial graph information, she can either estimate
ˆk, or directly calculate the inﬂuence values for known con-
nected/unconnected pairs and estimate the threshold for distin-
guishing them. More concrete descriptions of such actionable
strategies are deferred to Appendix E2, which we hope can
inspire more effective attacks as interesting future work.
In practice,
Measuring Inﬂuence Values via the Inference API: Here
we describe the calculation of the inﬂuence value between a
node pair. Recall that in the interaction model, for any inquiry
involving a set of nodes and their features, the attacker Bob is
given a prediction vector for each node. With the hope of tak-
ing advantage of the prediction vectors to obtain the inﬂuence
value, we look into the structure of graph convolutional layers
and analyze the inﬂuence of an edge on its incident nodes.
We characterize the inﬂuence of one node v to the other
node u by measuring the change in the prediction for node
u when the features of the node v get reweighted. Formally,
let V (I) be the set of nodes involved in the inference stage
and X = [x(cid:62)
v , . . .](cid:62) be the corresponding feature
matrix. By upweighting the features of the node v by a
small value ∆, the attacker generates a new feature matrix
1 , . . . , x(cid:62)
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:20:38 UTC from IEEE Xplore.  Restrictions apply. 
2008
1 , . . . , (1 + ∆)x(cid:62)
(cid:0)P (cid:48)(I) − P (I)(cid:1) /∆ with size |V (I)| × c Its u-th
X(cid:48) = [x(cid:62)
v , . . .](cid:62). The difference between the
two predictions P (I) and P (cid:48)(I) with respect to ∆ denotes
the inﬂuence of reweighting v on the prediction of all other
nodes. We deﬁne the inﬂuence matrix of v on other nodes as
Iv = lim∆→0
row iv u ∈ Iv represents the prediction inﬂuence vector of v on
u for each class dimension. Finally, we compute the (cid:96)2 norm of
the corresponding inﬂuence vector as the inﬂuence value of v
on u as (cid:107)iv u(cid:107). Since computing the inﬂuence matrix Iv yields
the inﬂuence of one node v on all other nodes, to compute the
inﬂuence value between n2 pairs of nodes (n is the number of
nodes of interest), we only need to compute n inﬂuence matri-
ces, each for one node in the interested node set. This requires
2n forward passes of the trained network in all, which does
not constitute a signiﬁcant overhead during inference time. We
report the running time of LINKTELLER in Appendix G3.