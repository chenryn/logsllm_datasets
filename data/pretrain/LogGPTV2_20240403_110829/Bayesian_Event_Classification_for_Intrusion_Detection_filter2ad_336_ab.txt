information I. Each model mi ∈ M analyzes one or more
features (or properties) of a given input event and compares
the event’s feature(s) to the model’s previously established
proﬁle (i.e., the description that speciﬁes the normal fea-
tures or properties). The result of this comparison is the
output value oi that characterizes the deviation of the event’s
features from the expected ‘normal’ values, one for each of
the k models. The restriction of a single return value oi per
model does not result in a loss of generality. Every model
that returns more that one result can be easily represented
by multiple logical models, each returning a single output.
Given these deﬁnitions, the event classiﬁcation can be
deﬁned more formally as a function EC that, for a cer-
tain input event e, accepts as parameters the corresponding
model outputs {oi|i = 1 . . . k} and additional information
I. The result of the event classiﬁcation function is a binary
value that identiﬁes the input event e as normal or anoma-
lous. That is, for a certain event e, the event classiﬁcation
function EC is deﬁned as follows.
EC(o1, o2, . . . , ok, I) = {normal, anomalous}
(1)
In most current anomaly-based intrusion detection sys-
tems, EC is a simple function that calculates the sum of
the oi values (often referred to as anomaly scores) and com-
pares the result to a threshold, represented by I. That is,
EC is deﬁned as follows.
(cid:1)
EC(o1, o2, . . . , ok, I) =
e is normal :
e is anomalous :
(cid:2)k
(cid:2)k
i=1 oi ≤ I
i=1 oi > I
(2)
We propose to replace this simple summation scheme by
a Bayesian network. Our network consists of a root node
(i.e., hypothesis node) that represents a variable with two
states, namely normal and anomalous. In addition, we intro-
duce one child node for each model to capture the model’s
respective outputs {oi|i = 1 . . . k}. The root node is con-
nected to each child node, reﬂecting the fact that the model
outputs depend on the input event – that is, the outputs are
expected to be different when the input event is anomalous
and when it is normal.
Depending on the domain, causal dependencies between
models are identiﬁed and appropriate links are introduced
into the network. Under certain circumstances, it is possi-
ble that the outputs of two models are correlated. This can
be as simple as a positive or a negative correlation (i.e., one
anomalous feature makes it more or less likely that another
one is also anomalous), but could also be more sophisticated
such as the situation where the value of a certain feature in-
dicates that the quality of a test performed by another model
is reduced. Section 5 shows examples of model dependen-
cies that we have identiﬁed for our intrusion detection sys-
tem and Section 6 presents experimental results that demon-
strate that incorporating dependencies reduces the number
of incorrect classiﬁcations.
Additional
information sources might
indicate that
anomalous behavior is in fact legitimate or might support
the decision that the host is under attack. This could be in-
formation from other intrusion detection systems or system
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:47:09 UTC from IEEE Xplore.  Restrictions apply. 
health monitors (e.g., CPU utilization, memory usage, pro-
cess status). An important piece of additional information is
the conﬁdence value associated with each model. Depend-
ing on the input events that are utilized for establishing the
proﬁle, a certain feature might not be very suitable to distin-
guish between attacks and regular behavior. It might be the
case that the same values of a feature appear in both regular
behavior and attacks or that the variance of a feature is very
high. In these situations, it is useful to reduce the inﬂuence
of the model output on the ﬁnal decision. The conﬁdence
in the output of a model is an indication of the expected ac-
curacy of this model. In our Bayesian network, each model
conﬁdence is represented by a node that is connected to its
corresponding model node. Note that these additional nodes
require a non-na¨ıve network because each model node has
at least two parent nodes (the root node and the correspond-
ing conﬁdence node). Section 5 discusses the models that
we utilize for our intrusion detection system and provides
details about their conﬁdence levels.
Another possibility is to model dependencies between
events in the input stream. Attacks tend to manifest them-
selves in bursts of suspicious events. Therefore, it might be
useful to include a node in the Bayesian network that keeps
track of recent anomalies. However, this extension has not
been implemented and is left for future work.
5 System Implementation
We have implemented an intrusion detection system that
analyzes operating system calls to detect attacks against
daemon applications and setuid programs on machines run-
ning Linux or Solaris. In contrast to the work by Forrest
[5, 26], we do not perform detection on a sequence of sys-
tem calls but on individual system calls and their arguments.
Each system call invocation performed by a monitored ap-
plication is translated into an input event, represented by a
feature vector. A feature vector captures information spe-
ciﬁc to each system call such as the system call number, its
return code, and its arguments (such as ﬁle system paths,
mode bit-ﬁelds, and user/process credentials).
The feature vector serves as input to the analysis process
of the anomaly detection models. Each model evaluates one
or more features of the input event and outputs a value that
reﬂects the deviation of this event’s features from its pro-
ﬁle. We have developed four different models, described
below in more detail, that analyze individual system call ar-
guments (also called system call parameters). Three models
are particularly designed to characterize features of string-
type parameters, while one can be used for arbitrary argu-
ment types. For every monitored system call, we bind a
number of models to each of its arguments.
The task of the event classiﬁcation process is to deter-
mine whether a certain system call is anomalous, given the
outputs of the individual models for all arguments. A sim-
ple event classiﬁer was implemented that aggregates the
model outputs and compares the result to a threshold. We
also implemented our proposed Bayesian event classiﬁca-
tion scheme and observed a signiﬁcant decrease in the num-
ber of false alarms.
In order to provide a suitable input event stream on mul-
tiple platforms, a modular event provider architecture was
created to abstract away the platform-speciﬁc details of sys-
tem call logging. We implemented a Linux auditing facility
that converts Snare [21] audit data into feature vectors and
a tool that offers a similar functionality for Solaris’ Basic
Security Module (BSM) [3].
5.1 Models
This section brieﬂy describes our underlying models
with their detection mechanisms and motivates why our
chosen characterization is useful. In the following sections,
we discuss the model conﬁdence and the dependencies be-
tween models introduced in our system.
String Length
In many cases, the length of a string can be used to detect
anomalous input. System call argument strings are usually
relatively short and human-readable. However, the situa-
tion might look different when malicious input is present.
For example, to overﬂow a buffer, it is often necessary to
ship the shell code and additional padding, depending on
the length of the target. As a consequence, a string can con-
tain up to several hundred bytes. The goal of this model
is to approximate the actual but unknown distribution of the
lengths of a string argument and detect instances that signif-
icantly deviate from the observed normal behavior. Clearly,
we cannot expect that the probability density function of
the underlying real distribution follows a smooth curve. We
also have to assume that it has a large variance. Neverthe-
less, the model is able to identify signiﬁcant deviations.
Character Distribution
The character distribution model captures the concept of a
‘normal’ system call parameter string by looking at its char-
acter distribution. It is based on the observation that regu-
lar strings contain mostly printable, human-readable char-
acters. A large percentage of characters in these strings are
drawn from a small subset of the 256 possible 8-bit values
(mainly from letters, numbers, and a few special charac-
ters). Like in English text, the characters are not uniformly
distributed, but occur with different frequencies. The anal-
ysis is based only on the frequency values themselves and
does not rely on the distributions of individual characters.
That is, it does not matter whether the character with the
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:47:09 UTC from IEEE Xplore.  Restrictions apply. 
most occurrences is an ‘a’ or a ‘@’. For a regular parameter,
one can expect that the sorted, relative frequencies slowly
decrease in value. In case of manifestations of attacks, how-
ever, these frequencies can drop extremely fast (because of
a peak caused by a very high frequency of a single charac-
ter) or barely (in case of a nearly uniform character distribu-
tion). The ‘normal’ character distribution is determined as
the average of the character distributions of the strings en-
countered during the training phase. The model output for
a new string instance is calculated using the Pearson χ2-test
statistical test [2] that estimates the similarity of the new
character distribution to the one derived as the average of
the training set.
Structure
Often the manifestation of an exploit is immediately visible
as unusually long strings, or as strings that contain repeti-
tions of non-printable characters. Such anomalies are easily
identiﬁable by the two mechanisms explained above. There
are situations, however, when an attacker is able to craft
her attack in a manner that makes the manifestation appear
more regular. For example, to exploit a vulnerability, it
might not be necessary to inject long chunks of exploit code.
As another example, repetitions of non-printable characters,
often found in the sled of a buffer overﬂow, can be replaced
by constructs that behave similarly but contain only print-
able characters.
In such situations, it is necessary to use a more detailed
model of the string that shows the trace of the attack. This
model can be acquired by analyzing the string’s structure.
For our purposes, the structure of a parameter means the
regular grammar that describes all its normal, legitimate
values. When structural inference is applied to a set of
strings, the result has to be a grammar that can derive at
least all training examples. Unfortunately, there is no single
grammar that can be uniquely deﬁned for a set of sample in-
puts. When no negative examples are given (i.e., elements
that should not be derivable by the grammar), it is always
possible to create either a grammar that contains exactly the
training data or a grammar that allows one to derive arbi-
trary strings. The ﬁrst case is called over-simpliﬁcation, as
the resulting grammar is only able to derive the learned in-
put without providing any level of abstraction. This means
that no new information is deduced. The second case is a
form of over-generalization; although the grammar is capa-
ble of producing all possible strings, there is no structural
information left.
The basic approach used for our structural inference is
to generalize the grammar as long as it seems to be ‘rea-
sonable’ and stop before too much structural information is
lost. The notion of reasonable generalization is formalized
using hidden Markov models and Bayesian probability [22].
The output value of this model depends on whether a new
input string can be derived from the grammar or not.
Token Finder
The purpose of the token ﬁnder model is to determine
whether the values of a system call parameter are drawn
from a limited set of possible alternatives (i.e., they are to-
kens or elements of an enumeration). An application of-
ten passes identical values via APIs, such as ﬂags or han-
dles. When an attack changes the normal ﬂow of execu-
tion and branches into maliciously injected code, such con-
straints are often violated. When no such enumeration can
be identiﬁed in the training data, it is assumed that the val-
ues are randomly drawn from the argument type’s value do-
main (i.e., random values for every system call). The token
ﬁnder technique can be applied to any parameter type, but
it is mostly used for numerical values. In case that the mon-
itored values are tokens drawn from an enumeration, every
new value is expected to appear in the set of known identi-
ﬁers. Otherwise, the token ﬁnder cannot provide any useful
information.
5.2 Model Conﬁdence
The conﬁdence that the system has in the output of a
model should be an important factor in the event classiﬁ-
cation process. When a model claims a high conﬁdence in
its output, this model’s anomaly score should clearly have a
higher impact on the ﬁnal decision than the score of a model
that can only provide low-conﬁdence information. In tradi-
tional systems, the conﬁdence is often neglected or approx-
imated with static weights. When a model is expected to
produce more accurate results, it receives a higher a-priori
weight. However, this is not sufﬁcient, as the conﬁdence in
a model can vary depending on the training data used to cre-
ate the corresponding proﬁle. Consider, for example, the to-
ken ﬁnder model. When this model detects an enumeration
during the learning phase, its anomaly scores are considered
highly accurate. When random identiﬁers are assumed, the
anomaly score is not meaningful. With statically assigned
weights, this distinction cannot be made. Although it is pos-
sible to choose between two static weights in the case of the
token ﬁnder, the situation becomes more complicated with
other models. Therefore, a seamless integration of dynamic
weights that are calculated after the training phase is desir-
able.
We take the model conﬁdences into account by including
a conﬁdence node for every model. Each conﬁdence node
in the Bayesian network has a link to the node which repre-
sents its corresponding model. The conditional probability
tables are adjusted so that the model output has a signiﬁ-
cant inﬂuence on the decision when the conﬁdence is high-
est and no inﬂuence on the ﬁnal result when the conﬁdence
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:47:09 UTC from IEEE Xplore.  Restrictions apply. 
String Length
Conﬁdence
Classiﬁcation
Token Finder
Conﬁdence
String Length
Char Distribution
Conﬁdence
Token Finder
Char Distribution
Quality
Structure
Conﬁdence
Char Distribution
Structure
Figure 3. Bayesian Network for open and execve System Calls
is lowest. The model conﬁdence is represented as one of
ﬁve discrete levels: very high, high, medium, low and none.
When models create their proﬁles of normal behavior, the
variance of the input training data is evaluated. When the
variance of the analyzed feature is high, a low conﬁdence
value is assumed. When a small, coherent set of feature
values is observed during the training, the conﬁdence in the
correctness of the model output is high.
5.3 Bayesian Network
Figure 3 shows the structure of the Bayesian networks
for the open and execve system call. Both system calls
have two parameters and are monitored by our intrusion de-
tection system. The three string models (String Length,
Character Distribution and Structure) are attached to the
ﬁrst string parameter (ﬁle path and name in the case of the
open call, execution arguments in the case of the execve
call). The token ﬁnder is attached to the numerical parame-
ter in the case of the open call (mode ﬂags) and to another
string parameter in case of the execve call (program im-
age executed). Similar but simpler networks are used for
other monitored system calls that have only a single argu-
ment. A different Bayesian network instance is utilized for
every system call; however, most of these networks have an
identical structure.
In addition to the structure of the Bayesian networks,
conditional probability tables (CPTs) were speciﬁed for
each node. We used our domain-speciﬁc knowledge to es-
timate appropriate probability values for the various tables.
For each node, one has to provide the probabilities for all
states of the corresponding variable, conditionally depen-
dent on the states of all parent nodes. When a suitable
structure of the network is chosen, these probabilities are
mostly intuitive and can be determined in a sufﬁciently ac-
curate way by a domain expert. Note that we have not tuned
the CPTs in any way for our experiments. The probabilities
were selected before the evaluation was started and were not
modiﬁed thereafter.
The output of the models is a real value in the interval
[0,1] that describes the deviation of the input event fea-
ture(s) from the proﬁles. This value is mapped onto one
of ﬁve possible states associated with each model node in
the network. The mapping of a continuous function out-