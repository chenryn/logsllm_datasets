tains no input bytes that may have to be mutated to satisfy
s. Section 3.4 will describe this step in detail.
(3) Solve constraints. Mutate the bytes in the effective prior con-
ditional statements to satisfy s. Section 3.5 will describe this
step in detail.
Figure 3 shows an overall design of Matryoshka regarding how
the strategies are used in the fuzzing process.
3.3 Determine control flow dependency among
conditional statements
For each conditional statement s, we wish to identify all its prior
conditional statements, which are the conditional statements that,
if taking a different branch, may cause s to be unreachable. Let the
immediate prior conditional statement of s on a trace be the last
prior conditional statement of s, i.e., there is no prior conditional
statement of s between r and s. Note that if s is a prior conditional
statement of t, and t is of u, then s is a prior conditional statement
of u. This allows us to find all the prior conditional statements of
s transitively: starting from s, we repeatedly find the immediate
prior conditional statement, and then take the union of all such
statements.
We propose two different methods for finding the immediate
prior statement that is in the same function and that is in a different
function, respectively. In our implementation for optimization, we
cached all the found dependencies to avoid repeated computation.
Intraprocedural immediate prior conditional statement. Start-
3.3.1
ing from a conditional statement s, we walk back on the trace.
When we find the first conditional statement r
(cid:15) that is in the same function, and
(cid:15) that s does not post-dominate [1]
then r is the immediate prior statement of s. Our implementation
used the post-dominator trees produced by LLVM [24].
If we cannot find such r, then s has no intraprocedural immedi-
ate prior conditional statement, and we will search for its interpro-
cedural immediate prior conditional statement, to be described in
Section 3.3.2.
Interprocedural immediate prior conditional statement. It would
3.3.2
be straightforward to use interprocedural post-dominator trees for
efficient handling, but unfortunately, LLVM does not provide such
information, so we designed the following method for finding the
interprocedural immediate prior conditional statement of s. Start-
ing from s, we walk back on the trace to find the first conditional
statement r that satisfies all the following:
(1) r is in a different function (let us call it fr ) than s, and
(2) fr is still on the stack (i.e., it hasn’t returned) when s is exe-
cuting, and
(3) Let rc be the last call instruction that fr executed. rc must ex-
ist because r is in a deeper stack frame than s. If rc does not
post-dominate r (note that r and rc are in the same function),
then r is the interprocedural immediate prior statement of
s.
Irregular interprocedural control flow. Apart from function
3.3.3
calls, the program could also exhibit irregular interprocedural con-
trol flows, for instance those involving exit and longjmp instruc-
tions. If a conditional statement r has at least one branch that leads
to a basic block that contains irregular flows, then we consider it to
be the prior conditional statement of all the statements after itself
even when its frame is not on the stack. If s is a conditional state-
ment after r, we add r and r’s prior conditional statements to the
set of s’s prior conditional statements. In LLVM, the basic blocks
containing irregular interprocedural control flows are terminated
with unreachable instructions.
3.4 Determine taint flow dependency among
conditional statements
For each conditional statement s, Section 3.3 finds all its prior con-
ditional statements p(s). Let b(s) be the set of input bytes that flow
into s where s is one or more conditional statements. When we
mutate the input, as long as no conditional statement in p(s) takes
a different branch, s is guaranteed to be reachable. This seems to
suggest that we should avoid mutating any byte in b(p(s)).
On the other hand, avoid mutating every byte in b(p(s)) may
prevent the fuzzer from finding a satisfying assignment for s, as dis-
cussed in Section 3.1. Take Figure 2 as an example. Let s be Line 6.
By Section 3.3.1, we determine that p(s) consists of Lines 2, 3, and
4. Therefore, b(p(s)) = fx; y; zg. If we keep all the bytes in b(p(s))
immutable, then we are left with no input byte to mutate when
trying to find an input to satisfy s.
The problem arises because Section 3.3 considers only control
flow dependency among conditional statements, but it fails to con-
sider whether taint flow dependencies exist between the condi-
tional statements. We define the effective prior conditional state-
ments of s, e(s), to be a subset of the prior conditional statements
of s, where to find an input to satisfy s, we may have to mutate
some bytes flowing into a statement in e(s). In other words, if a
prior conditional statement r of s is not also an effective prior con-
ditional statement of s, then no byte flowing into r needs to be
mutated to satisfy s. This means that we may consider only the ef-
fective prior conditional statements and ignore the non-effective
prior conditional statements.
Algorithm 1 shows the algorithm for computing effective prior
conditional statements, which relies on the following property: if
r is an effective prior conditional statement of s, and q is a prior
Figure 3: Overview of Matryoshka. In the figure, Angora represents any fuzzer capable of identifying constraints. When the
fuzzer fails to solve a branch constraint guarding a new branch, Matryoshka determines whether the conditional statement
is nested. If so, Matryoshka tries three optimization strategies: prioritizing reachability, prioritizing satisfiability, and joint
optimization, during which it also identifies implicit flow dependencies when necessary.
conditional statement of s, and q and r share common input bytes,
then q is also an effective prior conditional statement of s.
strategies. Matryoshka tries them in this order and imposes a time
budget for each strategy to ensure overall efficiency.
Algorithm 1 Find effective prior conditional statements
1: function FindEffectivePriorCondStmt(s; stmts)
▷
s: conditional statement being fuzzed; stmts: prior conditional
statements of s. Returns: effective prior conditional statements
of s
Initialize a union-find data structure.
for all stmt 2 stmts do
T   input bytes flowing into stmt.
Union all t 2 T .
end for
O   ∅
bs   any one byte flowing into s
for all stmt 2 stmts do
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
end for
14:
return O
15:
16: end function
end if
b   any one byte flowing into stmt
if Find(bs) == Find(b) then
O   O [ fstmtg
3.5 Solve constraints
Section 3.4 determines the effective prior conditional statements
for each conditional statement s. On the one hand, if we freely
mutate the bytes flowing into any of them, s may become unreach-
able. But on the other hand, we may be required to mutate some
of those bytes to satisfy the unexplored branch of s. Therefore we
need to determine which of those statements whose relevant input
bytes we may mutate. We propose the following three alternative
(1) Prioritize reachability
(2) Prioritize satisfiability
(3) Joint optimization for both reachability and satisfiability
Each strategy identifies a constraint over a set of input bytes.
Then, it uses gradient-based optimization to solve the constraint.
These strategies provide benefits only when s is nested, i.e., s has ef-
fective prior conditional statements. If s is not nested, Matryoshka
simply uses existing strategies from Angora or other fuzzers to
solve the branch constraint. Therefore, Matryoshka exhibits better
performance in solving nested conditional statements while hav-
ing the same ability as other fuzzers to solve non-nested condi-
tional statements.
3.5.1 Prioritize reachability. This strategy pessimistically assumes
that if we mutate any byte that flows into any effective prior con-
ditional statements of a conditional statement s, s may become un-
reachable. Therefore, this strategy ensures that s is always reach-
able by avoiding mutating any byte that flows into any of s’s effec-
tive prior conditional statements. Formally, let b(s) be the bytes
flowing into s, and b(e(s)) be the bytes flowing into s’s effective
prior conditional statements. Angora mutates all the bytes in b(s),
which may cause s to become unreachable. By contrast, this strat-
egy of Matryoshka mutates only the bytes in b(s) n b(e(s)), i.e. all
the bytes that flow into s but that do not flow into any effective
prior statement of s.
Take the program in Figure 2 for example. When we fuzz s on
Line 3, its only effective conditional statement is t on Line 2. b(s) =
fx; yg. b(e(s)) = fxg. Using this strategy, the fuzzer mutates only
the bytes in b(s) n b(e(s)) = fyg.
However, this strategy fails when we fuzz s on Line 6. In this
case, its effective prior statements consist of the statements on
Taint trackingUnsolved branchconstraint andrelated inputDetermine control andtaint ﬂow dependency(§3.3, §3.4)Prioritize reachability(§3.5.1)Prioritize satisﬁability(§3.5.2)Joint optimization(§3.5.3)FailFailSuccessDetermine implicitﬂow dependency(§3.6)Gradient descentAngoraSucceedFailMatryoshkaSucceedFailureFailLine 2 and 3, so b(s) = fyg, b(e(s)) = fx; yg, but b(s)nb(dp(s)) =
∅. Using this strategy, Matryoshka will fail to fuzz s because it finds
no byte to mutate.
Algorithm 2 Find a satisfying input while prioritizing satisfiabil-
ity
1: function FindInput(s; stmts)
▷ s: the target conditional
statement. stmts: effective prior conditional statements of s.
▷ Forward phase
While keeping the branch choice of each r 2 stmts, find
an input i that satisfies the target branch of s.
Run the program on i.
if s’s target branch is reachable then
return Success
end if
BI   ∅
for stmt 2 stmts in the reverse order on the trace do
▷ Backtracking phase
▷ Input bytes not to be mutated.
B   input bytes flowing into stmt
B2   B n BI
While keeping the branch choice of all r 2 stmts where
r appears before stmt on the trace, find an input i that satisfies
the target branch of stmt, during which only the input bytes
in B2 may be mutated.
Run the program on i.
if stmt’s target branch is reachable then
return Success
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21: end function
end if
BI   BI [ B
end for
return Failure
3.5.2 Prioritize satisfiability. This strategy optimistically hopes that
a mutated input that satisfies a conditional statement s can also
reach s. It has a forward phase followed by a backtrack phase. Dur-
ing the forward phase, it mutates the bytes flowing into s while
artificially keeping the branch choices of all the effective prior
conditional statements of s, thereby guaranteeing that s is always
reachable. If it finds an input that satisfies the target branch of s, it
runs the program on that input normally (without artificially fixing
branch choices). If this trace still reaches s and chooses the target
branch, it succeeds. Otherwise, it enters the backtrack phase. Dur-
ing this phase, it starts from s and then goes backward to fuzz each
of the effective prior statements of s in that order. When it fuzzes
one such statement r, it avoids mutating any byte that may flow
into s or any effective prior conditional statement of s that is af-
ter r. The process succeeds when the fuzzer successfully fuzzes all
of these effective prior conditional statements. Algorithm 2 shows
this algorithm.
Take the program in Figure 2 as an example. When we fuzz s on
Line 6, its effective prior conditional statements are on Line 3 and
2. Let the current input be x = 1; y = 1. Under this input, both
Line 2 and 3 take the true branch, and Line 6 takes the false branch.
Our goal is to take the true branch on Line 6. Using this strategy,
during the forward phase, the fuzzer mutates y while artificially
forcing the program to take the true branch on both Line 2 and 3.
If the fuzzer finds an assignment y = 2 to satisfy the true branch
of Line 6, but since x = 1; y = 2 does not satisfy Line 3, it enters
the backtracking phase. During this phase, it will first fuzz Line 3.
Although this line is affected by two values fx; yg, since y flows
into Line 6, the fuzzer will mutate x only. If it finds a satisfying
assignment x = 0, it tries to run the program with x = 0; y = 2
without artificially forcing branch choices. Since this input reaches
Line 3 and satisfies the target (true) branch, fuzzing succeeds.
By contrast, let us assume that the fuzzer finds a satisfying as-
signment y = 3 when fuzzing Line 6. During the backtrack phase,
when fuzzing Line 3, since it can mutate only x, it cannot find a
satisfying assignment. Therefore, fuzzing of s fails.
Joint optimization for both reachability and satisfiability. Both
3.5.3
strategies in Section 3.5.1 and Section 3.5.2 search for a solution to
one constraint at a time. Section 3.5.1 mutates only the input bytes
that will not make the target conditional statement unreachable,
while Section 3.5.2 tries to satisfy the target conditional statement
and its effective prior conditional statements one at a time. How-
ever, they fail to find a solution where we must jointly optimize
multiple constraints.
Let s be the target conditional statement. Let fi (x) (cid:20) 0;8i 2
[1; n] represent the constraints of the effective prior conditional
statements of s, and fo (x) (cid:20) 0 represent the constraint of s. x is
a vector representing the input bytes. Table 2 shows how to trans-
form each type of comparison to f (cid:20) 0. Our goal is to find an x that
satisfies all fi (x) (cid:20) 0; i 2 [0; n]. Note that each fi (x) is a blackbox
function representing the computation on the input x by the ex-
pression in the conditional statement i. Since the analytic form of
fi (x) is unavailable, many common optimization techniques, such
as Lagrange multiplier, do not apply.
We propose a solution to the optimization problem. Define
д(x) =
R(fi (x))
(1)
where the rectifier R(x) (cid:17) 0 _ x (the binary _ operator out-
puts the larger value of its operands). Therefore, д(x) = 0 only
if fi (x) = 0;8i 2 [0; n]. In other words, we combined the n op-
timizations into one optimization. Now we can use the gradient
descent algorithm, similar to the one used by Angora, to find a so-
lution to д(x) = 0. Note that when we compute the gradient of
д(x) using differentiation, we need to artificially keep the branch
choices of the effective prior conditional statements of s to ensure
that s is reachable.
Let us revisit the program in Figure 2. Let [x; y] = [1; 3] be the
initial input. When we fuzz the target conditional statement s on
Line 6 to explore the true branch, we cannot solve the branch con-
straint by mutating only y. Using joint optimization, we combine
the branch constraints of s and its effective prior conditional state-
ments on Line 3 and 2 to construct (by Equation 1 and Table 2):
д([x; y]) = R(x (cid:0) 2 + ϵ) + R(x + y (cid:0) 3 + ϵ) + R(1 (cid:0) y + ϵ)
where ϵ = 1. On the initial input [x; y] = [1; 3], д([x; y]) = 2.
Using gradient descent, we will find a solution to д([x; y]) = 0
where [x; y] = [0; 2].
n∑
i=0
Table 2: Transform a predicate into a function such that the
predicate is satisfied when the function is non-positive. ϵ is
the smallest positive value of the type for a and b. For inte-
gers, ϵ = 1.
Predicate
a  b
a (cid:21) b
a = b
a , b
f ()
a (cid:0) b + ϵ
a (cid:0) b
b (cid:0) a + ϵ
b (cid:0) a
abs(a (cid:0) b)
(cid:0) abs(a (cid:0) b) + ϵ
if (z == 123456789) { .... }
}
k = 1; n = 1;
}
if (k == 1) {
1 void bar ( int y , int z) {
2
int k = 0, n = 0;
if (z (cid:0) y == 56789) {
3
4
5
6
7
8
9 }
10
11 void foo ( int x , int y , int z) {
12
13
14
15
16
17
18
19 }
}
(* fun_prt )(y , z );
fun_ptr = & other_fn ;
void (* fun_ptr )( int , int ) = NULL ;
if (z (cid:0) x == 12345) {
fun_ptr = & bar ;
} else {
Figure 4: A program showing implicit control and taint flow
dependencies
3.6 Detect implicit effective prior conditional
statements
The mutation strategies in Section 3.5 may fail if we cannot find all
the control flow and taint flow dependencies among conditional
statements. Section 3.3 and Section 3.4 described algorithms for
finding all the explicit control flow and taint flow dependencies,
respectively. However, they are unable to find implicit flows. Fig-
ure 4 shows such an example. The conditional statement on Line 13
causes an implicit taint flow into fun_ptr in function foo, which
then implicitly determines the control flow whether the program
will call the function bar or other_fn. Also, Line 3 causes an im-
plicit taint flow into the variable k, whose value will determine the
value of the predicate on Line 6. Therefore, both the conditional
statements on Line 13 and Line 3 should be effective prior con-
ditional statements for the target statement on Line 7. However,
since the taint flow is implicit, the algorithms in Section 3.4 can-
not find them.
Implicit taint flows may be identified using control flow graphs [23].
If a predicate is tainted, then the method taints all the variables that
get new values in either branch of the conditional statement. For