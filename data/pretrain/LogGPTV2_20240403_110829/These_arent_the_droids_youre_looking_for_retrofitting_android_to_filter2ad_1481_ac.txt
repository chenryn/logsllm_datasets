out consequences for software developers. Static analysis
could be used to identify sections of code that appear to
be designed to transfer data using control ﬂow, exposing
applications that actively attempt to subvert users’ desired
privacy policies. If application developers are found to be
actively targeting and circumventing AppFence’s exﬁltra-
tion blocking controls, they may undermine their ability
to employ the traditional excuse used to defend developers
of privacy-invasive applications—that they operate openly
with the implicit consent of a user base that is happy to
reveal information.
An application that is aware of AppFence can detect the
presence of exﬁltration blocking. For example, an applica-
tion could open two independent sockets, transmit tainted
data over only one of those sockets and untainted data over
the other socket, and have the server report back what it
received. Similarly, shadow data may also not be convinc-
ing enough to fool an application. Applications that detect
the presence of privacy controls could refuse to provide user-
desired functionality until the controls are deactivated.
4. TEST METHODOLOGY
The primary cost of imposing privacy controls on appli-
cations is the introduction of side eﬀects that negatively im-
pact the user’s experience. To enable the evaluation of our
AppFence system, we developed a novel test methodology
that allows us to automate the execution of applications
and easily measure and characterize side eﬀects introduced
by the privacy controls. Our methodology overcomes the
two main obstacles to systematic testing of the interaction
between AppFence’s privacy controls and applications: the
ability to reproduce program executions (reproducibility),
and the ability to detect side eﬀects (detection). We de-
scribe how we use automated GUI testing and screenshot
comparisons to tackle these issues in the next subsections.
We focus on user-visible side eﬀects as the metric for eval-
uating AppFence because shadowing and exﬁltration block-
ing have equivalent beneﬁts when applied to the applications
in our test bed; given that AppFence-unaware applications
do not (at least to our knowledge) deliberately circumvent
the information ﬂow tracking used to block exﬁltration, both
privacy controls are equally eﬀective on today’s applications.
We do not measure the performance impact of our privacy
controls; the underlying information ﬂow tracking provided
by TaintDroid is fast enough to run applications in real-time
with modest slowdown (worst case increase in CPU utiliza-
tion of 14%), and beyond this we witnessed no discernable
Figure 2: AppFence system architecture. The
Dalvik VM sandboxes the application and contains
the Android core libraries. Resource managers re-
side in the Android framework outside of the VMs.
Existing resource manager and ﬁle system compo-
nents are modiﬁed for shadowing, while exﬁltration
blocking introduces new components (solid boxes)
for connection blocking and taint tracking. The
AppFence daemon runs as a native library, and is
controlled by the AppFence settings application.
Implementation
To monitor and block network traﬃc, we modify both the
Java code and native code in the Android networking stack.
Figure 2 shows key modules that we instrumented or created
for exﬁltration blocking.
When an application writes to a socket’s output stream,
the buﬀer is sent to the sendStream() method within the
OSNetworkSystem core library. We modiﬁed sendStream so
that if the buﬀer is tainted by data that should not be sent
to its intended destination, we drop the buﬀer. When SSL
sockets are used, we capture write calls to the SSLOutput-
Stream class.
To emulate airplane mode, we ﬁrst return error code
SOCKERR_TIMEOUT, then block the next send with error code
SOCKERR_EPIPE. If the application tries to open a new socket
(via a socket.connect() call), we ﬁnally return a Socke-
tException with error code SOCKERR_ENETUNREACH. Subse-
quent attempts to open sockets or send data will be allowed
until we next encounter tainted data bound for a forbidden
destination.
In order to facilitate user conﬁguration and testing, we
separate the policy speciﬁcation mechanism into a service
(daemon) that can be conﬁgured automatically or by users.
Our privacy controls obtain their policies from this daemon.
The privacy controls can be enabled globally or on a per-
application basis.
AppFence relies on the open-source TaintDroid platform
which, at the time of our testing, did not yet fully support
just-in-time (JIT) compilation. We have thus initially im-
plemented AppFence for Android version 2.1, which does
not use JIT compilation. Android 2.1 represented 15% of
the Android installations accessing the Android Market as
of August 2011 [14]. We did not encounter any compatibility
issues running applications on Android 2.1.
Dalvik	
  VM	
  sendStream()	
  write()	
  open()	
  content://calendar	
  content	
  manager	
  account	
  manager	
  process	
  manager	
  loca<on	
  manager	
  telephony	
  manager	
  AppFence	
  seAngs	
  AppFence	
  daemon	
  taint	
  tracking	
  OSNetworkSystem,	
  SSLOutputStream	
  applica<on	
  sandbox	
  OSFileSystem	
  log,	
  camera,	
  microphone	
  shadowing	
  connec<on	
  blocking	
  644impact as applications with and without our privacy controls
enabled ran side by side.
4.1 Automated application runs
Reproducibility is diﬃcult because diﬀerent runs of the
same application may exercise diﬀerent code paths. Further-
more, variations in user inputs, their timing, system state,
and other factors may cause results to change. To minimize
these variations, we built a test infrastructure that auto-
mates human usage patterns to remove variations in users’
choices of actions and their timing. To this end we used
the Android GUI testing system provided by the TEMA
project [15, 20], which leverages the Android monkey event
generator. The test system supports a scripting language in
which user actions are expressed via high-level commands
such as TapObject, PressKey and SelectFromMenu. Com-
mands were sent from our PC running the GUI testing sys-
tem to our Nexus One devices via a USB cable.
As described in Section 2.1, we selected 50 applications
to be scripted for our experiments (these are listed in Ap-
pendix B). We scripted each application to perform its main
tasks as we expected users to perform them. Our scripts
are not guaranteed to cover all possible code paths, and so
our results may not detect all uses of sensitive data by an
application or all of the side eﬀects of our privacy controls.
The average time to execute each test script – excluding
installation, uninstallation and cleanup – was 3.5 minutes,
with an average of 24 script commands. We created a mas-
ter test script that conﬁgures an Android device, enables
the AppFence privacy controls for experimental conﬁgura-
tions or disables them for the baseline conﬁguration, and
then tests all applications. For each application, the script
installs and launches the application, executes the GUI test
adapter to provide inputs, uninstalls the application, and
then removes any changes to the device state caused by the
application; we refer to these steps as an application execu-
tion.
4.2 Detecting changes in behavior
Detecting whether side eﬀects impact user -desired func-
tionality is a determination that eventually requires consul-
tation of a user. However, placing a human in the loop can
introduce bias and slow the process down, running counter
to our goal of systematic, automated testing. To reduce the
scalability constraints and bias caused by human evaluation,
we leverage the insight that side eﬀects are likely easy to de-
tect and conﬁrm if the visual outputs of the baseline and
experimental executions can be compared side by side. We
employed a feature of the GUI testing system to capture a
screenshot from the Android device after every command in
the test script. We ﬁrst ran each test script with our base-
line conﬁguration—no resources were replaced with shadow
resources and no attempts to exﬁltrate data were blocked.
We then ran each test script with our experimental con-
ﬁgurations, in which either data shadowing or exﬁltration
blocking was activated. For each experimental execution,
we automatically generated a web page with side-by-side
screenshots from the baseline execution and the experimen-
tal execution, along with a visual diff of the two images.
We found that these outputs could be scanned quickly and
reliably, with little ambiguity as to whether a side eﬀect had
been captured in the image logs, as shown in Figure 3.
(a) Baseline execu-
tion
(b) With exﬁltra-
tion blocking
(c) Visual diﬀ be-
tween (a) and (b)
Figure 3: Detecting side eﬀects using visual diﬀ:
The red shaded region in (c) highlights the adver-
tising banner missing from (b).
We also monitored the tainted data exposure across test
runs and found that it is not deterministic:
it is possible
for applications to transmit tainted data in some test runs
but not others. We took steps to mitigate the underlying
sources of variation during our testing. For example, we
discovered that many applications request the most recent
calculated location, without asking for the phone to access
the GPS; they may do this to avoid the latency required to
obtain updated location data, or to avoid the battery drain
of activating the GPS unit. If a null location is returned, or
if the last known location is stale (e.g. more than 60 min-
utes old), applications will often proceed without location
data. To avoid inconsistencies during our testing, we modi-
ﬁed the Android framework to always return a ﬁxed default
location, rather than null, when no last known location is
available. To account for remaining variations in our test-
ing, we examined the results of at least two test executions
for every experimental conﬁguration, and used additional
executions and manual log inspection to resolve inconsistent
application behavior.
5. EXPERIMENTS
This section shows the experimental results of testing
AppFence’s privacy controls on the 50 applications for which
we generated test scripts (see Appendix B). We discuss the
side eﬀects resulting from the privacy controls and evaluate
their impact on the user experience.
5.1 Experimental conﬁgurations
We executed applications over eight diﬀerent experimen-
tal conﬁgurations. The control conﬁguration, which did not
have any privacy controls activated, represents how users
run applications on Android today. In the shadowing con-
ﬁguration, sensitive data was replaced by shadow data, as
described in Section 3.1. The remaining six conﬁgurations
implemented some form of message blocking, three of which
used overt blocking (simulating airplane mode) and three of
which used covert blocking (pretending that blocked mes-
sages were actually sent). One pair of exﬁltration block-
ing conﬁgurations (one covert, one overt) blocked messages
tainted by sensitive data regardless of the server to which
they were destined. Like data shadowing, these conﬁgura-
tions are destination-agnostic. A pair of destination-speciﬁc
exﬁltration blocking conﬁgurations only blocked tainted mes-
sages if they were destined to known advertising & analytics
645None
Ads absent
Less functional
Broken
Shadowing
(56%)
28
( 0%)
0
(28%)
14
8
(16%)
Exﬁltration blocking of tainted messages to. . . Blocking all messages
all destinations
Overt
Covert
only A&A servers
Overt
Covert
to A&A servers
Overt
Covert
16 (32%)
11 (22%)
10 (20%)
13 (26%)
16 (32%)
11 (22%)
10 (20%)
13 (26%)
45 (90%)
4 ( 8%)
0 ( 0%)
1 ( 2%)
45
4
0
1
(90%)
( 8%)
( 0%)
( 2%)
19 (38%)
29 (58%)
0 ( 0%)
2 ( 4%)
18 (36%)
26 (52%)
1 ( 2%)
5 (10%)
Table 5: The side eﬀects of imposing privacy controls on all 12 categories of sensitive data for 50 test
applications.
(A&A) servers. Finally, to examine the beneﬁts of exﬁl-
tration blocking over more na¨ıve approaches, a destination
blacklisting pair blocked all traﬃc to known A&A servers,
regardless of whether it was tainted by sensitive data or not.
The list of known A&A servers can be found in Table 3.
We divided the possible side eﬀects impacting the user ex-
perience into four categories based on severity: the privacy
controls had no side eﬀect (none); advertisements no longer
appeared (ads absent); the application still performed its
primary purpose but failed to perform a less-important sec-
ondary function, or was otherwise less functional ; or the ap-
plication no longer fulﬁlled its primary purpose or crashed
(broken). We then classiﬁed each application into one of
these categories, based on the most severe side eﬀect we ob-
served in the entire execution of the application under our
test script.
The deﬁnition of less functional (as opposed to broken)
is somewhat subjective, and will vary according to the in-
dividual user. When classifying applications, we carefully
considered the primary purposes for which a user would run
a particular application, and when judgment calls were nec-
essary, we made them in favor of more severe impacts. A
detailed explanation of when we considered each application
to be less functional is presented in Appendix A. Because we
are concerned with evaluating the potential negative impact
of our privacy controls on the user’s experience, we do not
consider the absence of advertisements to be a side eﬀect,
nor do we study the impact on application developers or
their advertising and analytics partners.
5.2 Coarse-grained controls
Our ﬁrst experiment examines the side eﬀects of impos-
ing privacy controls on all 12 data types simultaneously. We
begin with such a coarse-grained analysis because it allows
us to identify the best applications for further examination;
those that are not impacted by coarse-grained privacy con-
trols will not require more detailed analysis. Our results
are summarized in Table 5. Advertising & analytics (A&A)
servers don’t generally provide user-desired functionality,
so it is not surprising that the na¨ıve approach of blocking
tainted messages sent to known A&A servers has fewer side
eﬀects than approaches that block messages to other servers
as well. However, even blocking just tainted messages to
known A&A servers can cause disruption to the user expe-
rience if applications fail to handle blocking gracefully. For
example, after a connection to an A&A server failed, one
application assumed that the network was unavailable and
abandoned all network access. Blocking all messages sent
to A&A servers, rather than just those messages tainted by
sensitive data, caused slightly more applications to break.
Closer inspection revealed that these applications send un-
tainted communications to A&A servers upon launch, which
may cause them to wait indeﬁnitely for a response (covert
mode) or receive a socket exception that is interpreted as
network unavailability (overt mode). For all exﬁltration
blocking conﬁgurations, we found negligible diﬀerences in
the occurrence of side eﬀects caused by overt blocking ver-
sus covert blocking.
Alas, blocking only A&A servers only defends against be-
havioral advertising which, despite its popularity, is likely
the least pernicious threat to sensitive data. More nefari-
ous applications can circumvent such blacklist approaches,
for example by proxying communications to A&A servers
through their own (ﬁrst party) servers. Preventing exﬁltra-
tion of data through non-A&A servers requires one of our
destination-agnostic approaches, i.e. using shadowing or us-
ing exﬁltration blocking of tainted messages to all destina-
tions. Table 5 shows that overall, shadowing causes fewer
and less severe side eﬀects than exﬁltration blocking; a more
detailed analysis is presented in the following section.