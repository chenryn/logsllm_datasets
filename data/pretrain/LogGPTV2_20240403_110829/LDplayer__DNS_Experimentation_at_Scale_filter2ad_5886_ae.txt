### 5.2.4 Query Latency

The LDplayer experiments provide a detailed view of the distribution of query latency, which was previously only modeled in terms of expected values (mean). Experimentation allows for a deeper understanding of tail performance.

**Figure 15a** shows the query latency for DNS over TCP and TLS with varying Round-Trip Times (RTTs). The latency is asymmetric: the 5th and 25th percentiles are similar, but the performance in the tail varies significantly (compare the 75th and 95th percentiles). This skew is captured in the experiments but not in the models.

### Memory Consumption and TCP Connections

**Figure 15b** illustrates the memory consumption over time for different memory sizes (4GB to 28GB) with a 5-second interval. Dashed lines represent all queries over TCP, while solid lines represent NSD (Name Server Daemon) performance. The original trace, with 3% of queries over TCP and a 20-second timeout, is also shown.

**Figure 15c** depicts the number of established TCP connections over time, again with a 5-second interval. The all-queries-over-TCP scenario and the original trace (3% over TCP with a 20-second timeout) are compared.

**Figure 15d** shows the number of TCP connections in the TIME_WAIT state over time. The same scenarios as in Figure 15c are used.

### Non-Busy Clients and Query Latency

In the B-Root-17b trace, a small subset (1%) of clients contributes three-quarters of the total query load, while most (81%) of the clients are inactive (less than 10 queries over the 20-minute trace). This observation aligns with prior studies [9].

For non-busy clients (those sending fewer than 250 queries), the median query latency over TCP is approximately 2 RTTs, which is higher than the 1-RTT median latency in UDP. This indicates that many queries are sent through fresh connections, although connection reuse is still effective (25th percentile is 1 RTT for TCP).

As RTT increases, the median query latency of TLS increases nonlinearly from 2 to 4 RTTs (red dashed line in **Figure 15b**), which is not captured in the models. Some queries exhibit large multi-time RTT latency (75th percentile and above), which is unexpected since a single TCP query should only require 2 RTTs and a TLS query needs 4 RTTs. Packet traces reveal that many server reply TCP segments are reassembled into a large TCP message, causing significant delays. Disabling the Nagle algorithm on the server can be an optimization to reduce this delay.

By contrast, UDP latency remains consistent regardless of RTT because it does not have algorithms like Nagle that aim to reduce packet counts.

### Real-World Performance Interactions

Evaluating real-world performance interactions between DNS clients and servers was only possible through full trace-driven experiments, as there are no generic models for TCP and TLS query processing in DNS servers. Our experiments show the effect of TCP connection reuse, although the query latency for TCP and TLS is still higher than for UDP. This provides greater confidence in testbed experiments with synthetic traffic and modeling [33]. The use of real traces and server software also revealed an unexpected increase in median query latency for TLS at large client RTTs.

### TCP Connection Reuse

TCP connection reuse helps reduce query latency: at a small 20 ms RTT, the median query latency in TCP is similar to UDP, and at a large 160 ms RTT, it is only about 15% slower than UDP (Figure 15a). If all connections were fresh, models predict a 100% overhead for TCP due to the extra RTT in connection setup. The small median latency differences between UDP and TCP/TLS are influenced by queries from a few busy clients where a connection may always get reused.

### Related Work

#### DNS Replay Systems

Several systems replay DNS traffic and simulate parts of the DNS hierarchy. Wessels et al. [31] simulate the root, TLD, and SLD servers to study caching effects. Yu et al. [32] build a system with multiple TLD servers to understand authority server selections. Ager et al. [3] set up a testbed to study DNSSEC overhead. DNS-OARC develops a DNS traffic replay tool [11, 12] to test server load.

Our system differs in scale, speed, and flexibility. Each of these systems hosts each zone on a different name server, limiting scalability. They often modify zones to make routing work and obtain correct answers. We use proxies to allow all zones to be provided from one name server, ensuring accurate query sequences.

#### Traffic Generators

Several tools can generate DNS traffic [14, 22]. While these tools can generate DNS packets, they are not specific to DNS and provide only simple replay or generation. Our system focuses on DNS protocol and provides a generic experimentation platform, allowing accurate timing and what-if scenarios.

#### Network Replay Tools

Tools like Tcpreplay [29] and Mahimahi [23] can replay general network traces. Our system simulates DNS query semantics, allowing exploration of future DNS design options. Other tools replay HTTP traces with accurate timing [2, 10, 23]. Our system is specifically designed for DNS and emulates the DNS hierarchy on a single instance of a DNS server.

#### DNS Studies

Studies like those by Brustoloni et al. [7] and Khurshid et al. [19] replay DNS queries to evaluate DNS application performance. Our system supports such analysis but provides a more flexible platform for high-query-rate studies with protocol variants. The focus is on accurate trace replay, unlike DNS Flagger [7], which replays traces at faster rates.

To our knowledge, ours is the only experimental DNS system that can replay DNS traces with original zone files, handle large query rates, and vary protocols.

### Conclusion

This paper describes LDplayer, a system for trace-driven DNS experiments. It is efficient (87k queries/s per core) and can reproduce precise query timing, interarrivals, and rates. We have used it to replay full B-Root traces and are evaluating recursive DNS traces with multiple levels of the DNS hierarchy.

Our system evaluates alternative DNS scenarios, such as all queries using DNSSEC or TCP. It is the first to enable at-scale experiments of these types, confirming that memory and latency are manageable (as predicted by modeling) but highlighting performance variations due to implementation details. Experimental confirmation of complex factors like memory usage is critical to ensure the feasibility of an all-TCP DNS on current server-class hardware.

### Acknowledgments

Research by Liang Zhu and John Heidemann is partially sponsored by the Department of Homeland Security (DHS) Science and Technology Directorate, HSARPA, Cyber Security Division, BAA 11-01-RIKA, and the Air Force Research Laboratory, Information Directorate under agreement number FA8750-12-2-0344, and contract number D08PC75599. The U.S. Government is authorized to make reprints for governmental purposes. The views expressed are those of the authors and do not necessarily represent those of DHS or the U.S. Government.

### References

[References listed as in the original text]

---

This version of the text is more structured and professional, with clearer headings and improved flow. It also includes a more detailed explanation of the figures and results, making it easier to follow and understand.