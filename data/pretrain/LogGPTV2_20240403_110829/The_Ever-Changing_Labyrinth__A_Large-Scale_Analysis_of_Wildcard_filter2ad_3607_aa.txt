# The Ever-Changing Labyrinth: A Large-Scale Analysis of Wildcard DNS Powered Blackhat SEO

**Authors:**
- Kun Du, Tsinghua University
- Hao Yang, Tsinghua University
- Zhou Li, IEEE Member, Tsinghua University
- Hai-Xin Duan, Tsinghua University
- Kehuan Zhang, The Chinese University of Hong Kong

**Publication Information:**
- **Conference:** 25th USENIX Security Symposium
- **Date:** August 10â€“12, 2016
- **Location:** Austin, TX
- **ISBN:** 978-1-931971-32-4
- **Open Access Sponsored by:** USENIX
- **Link:** [Proceedings](https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/du)

## Abstract

This paper presents a comprehensive analysis of a new type of blackhat Search Engine Optimization (SEO) infrastructure known as "spider pool." Traditional blackhat SEO campaigns often target high-traffic keywords and establish link networks by spamming popular forums or compromising vulnerable sites. However, these methods have become increasingly costly due to active disruption by search engines. In contrast, spider pools use cheap domains with low PageRank (PR) values to construct link networks and poison long-tail keywords. To enhance the indexing frequency of their promoted content, spider pool operators abuse wildcard DNS to create virtually infinite subdomains and construct complex loop structures that force search-engine crawlers to visit them relentlessly.

We conducted an in-depth study to understand this emerging threat. We infiltrated a spider pool service and developed a detection system to explore the recruited SEO domains. Using the unique features of the spider pool, we scanned over 13 million domains under 22 TLDs/SLDs and discovered more than 458,000 SEO domains. Our measurements reveal the infrastructure's characteristics, customer categories, and impact on search engines. We hope our findings will inspire new mitigation methods and improve ranking or indexing metrics from search engines.

## 1. Introduction

Search engines are the primary entry point for users to access various websites. Google, the leading search engine, receives 3.5 billion search queries daily, and the traffic generated through search results can account for over 60% of a website's incoming traffic. Improving a site's search rankings and attracting frequent visits from search engine crawlers are crucial for site owners.

Site owners and researchers have developed "whitehat SEO" techniques to improve a site's performance in search results. These techniques include enhancing site structure and search affinity, such as adding descriptive keywords to titles and metadata. However, applying these techniques requires significant effort and may not yield immediate results. As a shortcut, "blackhat SEO" techniques exploit search engine algorithms to gain an advantage at a lower cost.

Traditional blackhat SEO practices involve keyword stuffing and injecting inbound links into reputable sites. These methods require substantial investment and can result in severe penalties from search engines. New blackhat SEO techniques, such as targeting long-tail keywords, have emerged. Long-tail keywords, though individually low-traffic, can collectively generate substantial traffic. However, sites competing for long-tail keywords typically have low PR values, leading to infrequent visits from search engine crawlers and long indexing delays.

To address this, spider pool operators use wildcard DNS to create virtually infinite subdomains and construct complex loop structures. This traps search engine crawlers, increasing the indexing frequency of the promoted content. Our study aims to provide the first comprehensive analysis of this new threat and inspire new mitigation methods.

## 2. Background

### 2.1 Search Engine Optimization

The goal of search engines is to provide users with relevant and important web pages based on their search queries. While the exact ranking algorithm is a closely guarded secret, guidelines and techniques called Search Engine Optimization (SEO) have been developed. Whitehat SEO focuses on improving site structure and content to make it more accessible to search crawlers. This includes adding targeted keywords, creating navigation pages, avoiding duplicate content, and frequently updating content.

Improving a site's quality also increases its PageRank (PR) value, which determines how frequently it is visited by search engine crawlers. High PR values lead to faster indexing and better visibility in search results. Targeting long-tail keywords can help sites attract significant traffic without high costs. For example, while ranking for "socks" is challenging, ranking for "socks with dogs on them" is easier and can still drive substantial traffic.

### 2.2 Blackhat SEO Techniques

Blackhat SEO techniques aim to manipulate search engine algorithms at a low cost. Common techniques include:

- **Content Spam:** Repeating keywords to increase relevance or including a wide range of trending keywords.
- **Link Farm:** Setting up link farms to inject links into reputable sites.
- **Cloaking:** Serving different content to search engines and regular visitors to avoid detection.

### 2.3 Blackhat SEO Infrastructures

Different infrastructures have been developed to organize and manage blackhat SEO campaigns:

- **Forum Spam:** Posting links in high-reputation forums and blogs.
- **SEO Botnet:** Compromising vulnerable sites to turn them into botnets.
- **Link Exchange Platform:** Online platforms for exchanging incoming links.
- **Private Blog Network (PBN):** Buying and setting up blog sites on expired domains with high PR values.

## 3. Dissecting a Spider Pool Campaign

### 3.1 Overview

Spider pools use cheap, low-PR domains to construct link networks and target long-tail keywords. To reduce indexing latency, spider pool operators use wildcard DNS to create virtually infinite subdomains and trap search engine crawlers within their network. This boosts the visiting frequency of the promoted content and improves the importance score of the customer sites.

### 3.2 Infiltration of a Spider Pool

To better understand the business model and operational details of a spider pool, we infiltrated a popular spider pool service provider called Super Spider Pool (SSP). We purchased a domain and set up a fake website to observe how the spider pool operates. Our findings revealed the extensive use of wildcard DNS and dynamic content generation to trap search engine crawlers.

## 4. Methodology and Findings

### 4.1 Scanner Development

Exploiting the unique features of the spider pool, we developed a scanner based on DNS probing and differential analysis. We used this scanner to examine 13.5 million domains under 22 TLDs and SLDs, identifying 458,000 spider pool domains distributed across 19 TLDs/SLDs. We also discovered a trend of misusing new gTLD domains and policy holes in the .ac.cn SLD registration process.

### 4.2 Measurement and Analysis

We measured these domains in various aspects, including hosted IPs, domain registrars, and registrants. Despite being spread over 28,000 IPs, the domains were centralized on a small set of ASNs, registrars, and controlled by a small group of SEOers. We also crawled 20 new spider pools, identifying 15,800 SEO domains, 1,400 customer domains, and 7.2 million URLs embedding customer messages. Our results suggest that spider pools are effective in attracting search crawlers and manipulating search results for long-tail keywords.

## 5. Conclusion

Our study provides the first comprehensive analysis of the spider pool infrastructure, revealing its characteristics, business model, and impact on search engines. We hope our findings will inspire new mitigation methods and improve ranking or indexing metrics from search engines. Baidu, the top search engine vendor in China, has acknowledged our findings, and we are collaborating with them to deploy detection systems to purify search results and capture spider pool services.