title:Poster: Using Generative Adversarial Networks for Secure Pseudorandom
Number Generation
author:Rajvardhan Oak and
Chaitanya Rahalkar and
Dhaval Gujar
Chapter 1
Generative Adversarial Networks
(GANs)
1.1 Introduction to GANs
Deep learning has launched a profound reformation and has even been applied to
many real-world tasks such as image classiﬁcation (He et al. 2016), object detection
(Ren et al. 2015), and image segmentation (Long et al. 2015). These tasks all fall into
the scope of supervised learning, which means that large amounts of labeled data are
provided for the learning processes. Compared with supervised learning, however,
unsupervised learning shows little effect from deep learning. Generative modeling
is a typical problem in unsupervised learning, the goal of which is to learn the
distribution over training data and then to generate new data by sampling from the
learned distribution. Generative modeling is usually more difﬁcult than supervised
learning tasks because the learning criteria of generative modeling are intractable
(Goodfellow et al. 2016). For supervised learning tasks, the corresponding mapping
information between the inputs and the outputs is given, and the supervised learning
models need only learn how to encode the mapping information into the neural
networks. In contrast, for generative modeling, the correspondence between the
inputs (usually a noise vector) and the outputs (the training data) is unknown, and
the generative models must learn how to arrange the mapping between the inputs
and the outputs efﬁciently.
Generative models can be classiﬁed into two categories, undirected genera-
tive models and directed generative models, based on whether the interactions
between neural layers are directed or undirected, as shown in Fig. 1.1. Undirected
generative models include Restricted Boltzmann Machine (RBM) (Hinton and
Salakhutdinov 2006) and Deep Boltzmann Machine (DBM) (Salakhutdinov and
Hinton 2009). Undirected generative models usually have the problem of the
intractable partition function, and the techniques used to approximate it limit their
effectiveness. Recently, directed generative models have been popular, and many
powerful models have been proposed, such as Neural Autoregressive Distribution
Estimator (NADE) (Larochelle and Murray 2011) and Variational Autoencoder
© Springer Nature Singapore Pte Ltd. 2021
X. Mao, Q. Li, Generative Adversarial Networks for Image Generation,
https://doi.org/10.1007/978-981-33-6048-8_1
1
2
1 Generative Adversarial Networks (GANs)
h1
h2
h3
h4
v1
v2
v3
z1
z2
z3
h1
h2
h3
h4
(a)
v1
v3
v2
(b)
Fig. 1.1 Examples of undirected and directed generative models. (a) Undirected generative model.
(b) Directed generative model
Training  Data
Real
Sample
Real
Discriminator
Loss
Fake
e
s
i
o
N
m
o
d
a
R
Generator
Fake
Sample
Fig. 1.2 Framework of GAN
(VAE) (Kingma and Welling 2013). Examples of undirected and directed generative
models are shown in Fig. 1.1.
Directed generative models usually contain a feedforward network that trans-
forms the latent variables z to the observed samples x, as shown in Fig. 1.1b. The
latent variables z are usually sampled from a simple distribution (e.g., Gaussian
distribution). We can think of the idea of directed generative models as to map
a simple distribution over z to a complex (observed) distribution over x via a
feedforward network.
Generative Adversarial Networks (GANs) (Goodfellow et al. 2014) are one
type of directed generative model. GANs can generate very realistic images. The
framework of GANs is shown in Fig. 1.2. GANs consist of two networks: the
generator and the discriminator. The generator is the typical feedforward network in
a directed generative model that maps the latent variables z to the observed samples
x. The discriminator is a classiﬁer that distinguishes between real samples and fake
samples generated by the generator. The generator and the discriminator are trained
adversarially: the discriminator aims to distinguish between real samples and fake
1.1 Introduction to GANs
3
Table 1.1 The input and output of the discriminator and the generator
Input
Output
Loss
Discriminator
A real image from training data or a
fake image from the generator
Probability that the input is a real
image
Classify real images into the “real”
category and fake images into the
“fake” category
Generator
A random noise vector
A fake image
Classify fake images
into the “real” category
(from the
discriminator)
samples, and the generator tries to generate fake samples as real as possible to make
the discriminator believe that the fake samples are from real data.
GANs have been one of the most successful generative models and have even
served as the basis for many computer vision and graphics tasks. If the task is to
output high-quality images, GANs can help improve the image quality. The general
idea is to add a discriminator and incorporate the GANs loss into the objective of the
task. GANs have been applied to many speciﬁc tasks, such as image super-resolution
(Ledig et al. 2017), text to image synthesis (Reed et al. 2016), and image-to-image
translation (Isola et al. 2017).
Table 1.1 lists the input and output of the discriminator and the generator. The
discriminator is a typical classiﬁer that predicts whether the input image is real
or fake (i.e., derived from the training data or from the generator). The generator
starts from a random vector and maps the vector to an image (typically through
some transposed convolutional layers). Note that the loss of the generator is from
the discriminator, such that the generator can know how the discriminator behaves.
If the discriminator can distinguish a fake image correctly, the generator tries to
improve that image. If the generator can generate a higher-quality fake image,
the discriminator becomes better able to distinguish that fake image, and a better
discriminator allows the generator to further improve. Therefore, the generator can
ﬁnally generate very realistic images.
The details of GANs training are shown in Algorithm 1. The generator and the
discriminator are updated alternately. When updating the generator, the parameters
of the discriminator are ﬁxed, and vice versa. The objective of the discriminator
is to minimize the classiﬁcation error (e.g., the binary cross-entropy loss). For
the generator, it aims to fool the discriminator into making false predictions for
the fake images. Thus, we can set the objective of the generator to maximize the
classiﬁcation error of the discriminator.
4
1 Generative Adversarial Networks (GANs)
1.2 Challenges of GANs
Although GANs have achieved great success, they still have three main challenges.
The ﬁrst challenge is image quality. Many studies have sought to improve the image
quality of GANs (Radford et al. 2015; Denton et al. 2015; Huang et al. 2017;
Salimans et al. 2016; Odena et al. 2016; Arora et al. 2017; Berthelot et al. 2017;
Zhang et al. 2018; Brock et al. 2018; Jolicoeur-Martineau 2018). Radford et al.
(2015) proposed the Deep Convolutional GANs (DCGANs), which are the ﬁrst to
successfully introduce convolutional layers to GANs architectures. Denton et al.
(2015) proposed the Laplacian pyramid of GANs (LAPGANs), in which a Laplacian