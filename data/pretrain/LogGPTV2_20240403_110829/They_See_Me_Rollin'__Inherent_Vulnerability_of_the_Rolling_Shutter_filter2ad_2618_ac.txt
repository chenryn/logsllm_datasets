✓
✓
✓
✓
✗
✓
6.1 Preliminaries
Equipment. We only used off-the-shelf hardware that we bought
online for less than $90. An overview of the different components
is depicted in Figure 5. We tested different lasers in terms of quality,
wavelength (color), and power. The lasers ranged from cheap 650nm
(red), 535nm (green) and 405nm (violet/blue) laser pointers with an
output power of 5mW [2], to more expensive and powerful semi-
professional 532nm (green) and 445nm (blue) continuous wave
(CW) diodes with power ratings of up to 1.8W [1]. We noticed
that an adversary might use green lasers to leverage the fact that
most image sensors use a Bayer-Matrix color filter array and are
therefore more sensitive to wavelengths at around 530nm [4].
Laser Modulation. The laser was modulated using a laser driver
with Transistor-Transistor-Logic that was connected to the hard-
ware Pulse Width Modulation (PWM) controller of a Raspberry Pi.
A PWM modulated signal has two variables that can be changed —
the frequency and the duty cycle. In our case, the frequency spec-
ifies how often the laser is switched on. The duty cycle, given in
percent, determines the duration of a pulse, i.e., how long the laser
is switched on each time the signal is set to high. In other words, the
duty cycle specifies ton. Since ton depends on both frequency f and
the duty cycle, we will use ton in the rest of the paper rather than
the duty cycle to make it easier to compare the injected distortions.
Equation 8 shows how ton is calculated, given the frequency f and
the duty cycle D.
ton = 1
f
D.
(8)
Applicability to Different Cameras. We tested whether we could
reproduce the rolling shutter attack on seven different cameras
Param
Camera Exposure
- Logitech
- Axis
Laser on-time
- Logitech
- Axis
Laser Frequency
- Logitech
- Axis
Camera fps
- Logitech
- Axis
Range
[min, max]
[100µs, 2,500µs]
[32µs, 1,000µs]
[320µs, 16,000µs]
[50µs, 400µs]
[30Hz, 900Hz]
[25Hz, 750Hz]
30fps
25fps
Symbol
texp
ton
f
F
Adversary
Controlled
✗
✓
✓
✗
with CMOS image sensors, ranging from cheap IoT cameras (YI
Home Camera) over mid-range smart cameras (Google Nest) to
semi-professional surveillance (Axis M3045-V) and smartphone
cameras (Apple iPhone 7). For comparison, we also examined a
camera with a CCD image sensor and an electronic global shutter
mechanism. We printed an image showing a busy intersection with
multiple objects, such as pedestrians and vehicles, placed it in front
of each camera and captured a frame during normal operation as
well as while pointing a modulated laser at it. The experimental
setup is depicted in Figure 5 and examples of resulting attack frames
are shown in Figure 15. Table 1 provides an overview of all tested
cameras and whether we were able to physically inject rolling
shutter patterns.
In-depth Rolling Shutter Pattern Collection. To further analyze
the factors affecting the attack, we picked two different cameras
which allowed us to control the camera exposure time texp: the
Logitech C922, a common webcam, and the Axis M3045-V, a dome
surveillance camera. We systematically reproduced the attack in
controlled conditions by changing texp and ton, which affect the
resulting distortions. We placed the camera and the laser in a closed
environment with little or no light, and collected a set of videos by
varying the parameters of the attack, which are reported in Table 2.
We set the laser frequency f to be slightly offset from the camera
frame rate F so that over consecutive video frames, the location of
the pattern slowly iterated over the image rows. This way, the set
of collected frames covered all possible locations of the distortion
in the image. Using a mask that filtered out pixels with a value
per color channel ≤ 10 (range [0, 255]), we extracted the generated
pattern of distortions from the captured video for each parameter
configuration.
6.2 Accuracy of the Shutter Model
Here, we show how the equations introduced in Section 5.1 accu-
rately model the number of rows affected by the distortion.
Estimated vs Empirical No. To estimate No with Equation 1 the
adversary only needs ∆trst ; as ton is set by them and texp is known
in this experiment (in the next paragraph we analyze the estimation
of texp). We used Equation 5 to estimate ∆trst . For the Logitech
404They See Me Rollin’: Inherent Vulnerability of the Rolling Shutter in CMOS Image Sensors
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Figure 6: Comparison between the model-predicted number
of affected rows and the empirical value found by measuring
the number in practice on the two cameras.
Figure 7: Increment in the expected distortion size ˆNo and
actual size No as the adversary’s exposure time estimate texp
diverges from the true exposure time value ˆtexp.
camera, this led to ∆trst = 46.3µs. In contrast, the Axis camera has
dead areas and requires Nrows to be determined before ∆trst can
be calculated. For most cameras and image sensors, manufacturers
provide detailed information about the number of total, effective
and recording pixels. However, for the Axis camera used in our
experiment this information was not available. By calculating the
ratio between visible and invisible distortions, we estimated Nrows
to be 2,160 (1,080 visible and 1,080 invisible rows). We therefore used
Nrows = 2160 for the Axis camera, resulting in ∆trst = 18.5µs. The
results in Figure 6, show how the prediction of our model closely
matches the empirical outcome of the attack, for various texp, ton,
and the two considered cameras. We find this model to be precise
regardless of the part of the image that is being hit. Figure 6 also
shows the confidence intervals for the number of rows affected by
the attack, computed over every frame of the collected videos.
Incorrect Estimation of texp. As mentioned in Section 5.2, the es-
timation of texp can be imprecise if information from the datasheet
is missing, leading to changes in the resulting distortion size No.
Using the rolling shutter model of the previous section, we com-
puted the effect of an incorrect estimate of texp on the resulting No.
We used Nmax (see Equation 3) as the effect of o is negligible. Given
an estimate of texp as ˆtexp, and the resulting actual and estimated
number of affected rows as No and ˆNo, we report the relationship
between these four variables in Figure 7. In the worst case, where
Figure 8: Results for SSD under normal operation (left) and
with an attack pattern overlayed (right). The different colors
indicate how an object is affected by the attack: red=hidden,
orange=misplaced, green=unaltered.
the adversary underestimates ˆtexp and texp is longest, this can lead
to a distortion that is over 4× larger than expected. The relationship
depicted in Figure 7 also dependents on ton. Additional plots for
different ton are shown in Appendix B. Nevertheless, we find that
roughly 70% of the difference in distortion size is within a factor
of two of the expected value (i.e., either twice as small or twice
as large). This shows that large deviations occur only when the
adversary’s estimate is far off the real exposure value, showing that
the attack’s outcome is quite predictable under an educated guess.
7 ATTACK EVALUATION
This section presents the impact of the rolling shutter attack on the
object detection task, i.e., locating and identifying semantic objects.
In addition, it compares the amount of interference introduced
by our attack with a blinding attack. As described in our threat
model in Section 4, fine-grained optimizations are infeasible, thus
the adversary chooses attack parameters by quantifying the attack
success offline.
7.1 Targeting Object Detection
i
Defining Attack Success. The goal of object detectors is to iden-
tify and locate objects in the input image with 2D boxes, which we
refer to as {b}(o)
i when they belong to a legitimate image and {b}(c)
when they belong to the attack-corrupted image. Figure 8 shows an
example of the attack effect on the detected objects: in the rolling
shutter attack-corrupted image, many objects are mis-detected com-
pared to a clean input. In practice, to compute the attack effect, for
each box, we categorized the effect of the attack on each object box
based on the Intersection-over-Union (IoU) between original and
corrupted boxes. IoU is a measure of the overlapping area of two
boxes, as a proportion of the total, combined area; it is commonly
used to evaluate the accuracy of a box prediction compared to a
ground truth box. We measured the attack outcome with the effect
that the distortions have on the detected boxes with the following:
• Hidden: for a box b
its object has been hidden if there is no
(o)
i
box in the corrupted image with IoU(b
• Misplaced: for a box b
its object has been misplaced if there
is a box in the corrupted image with IoU smaller than 0.95 and
same output class.
(o)
i
(o)
i
, b
(c)
j
) > 0.5.
0100500#RowsAﬀectedLogitechtexp=1msModel(ˆNo)Actual(No)010texp=10ms010texp=25ms0100500Axistexp=32µs010ton(ms)texp=100µs010texp=200µstexp010002000ˆtexp010002000×2×4×6No=63,ˆNo=14Logitech(ton=500µs)texp05001000ˆtexp05001000DistortionSizeIncrease(NoˆNo)×2×4×6No=65,ˆNo=13Axis(ton=200µs)person:94%person:80%traﬃclight:63%traﬃclight:54%person:80%person:57%person:68%car:68%person:62%person:53%person:52%person:89%person:72%person:62%405ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Sebastian Köhler, Giulio Lovisotto, Simon Birnbach, Richard Baker, and Ivan Martinovic
• Appeared: for a box (in the corrupted image) b
, its object
has appeared if there is no box in the original image with
IoU(b
) > 0.5.
(c)
j
(c)
j
(o)
i
, b
7.1.1 Method.
Attack Pattern Collection. We simulated the attack to find the
best configuration of adversary-controlled parameters (i.e., ton and
f ): the one that leads to the highest number of hidden objects. We
collected the patterns as described in Section 6.1. Given that the
Axis camera’s frame rate is 25 fps (F=25), we collected patterns for
four different laser frequencies, f = 25, 250, 500, and 750Hz. Since
the image sensor of the Axis camera has dead areas, the number
of distortions can be calculated using Equation 6. We collected
patterns with duty cycles from 0.1% to 40% (from 1.3µs to 533.3µs),
because a change in f or duty cycle leads to different ton (see
Equation 8). It should be noted that ton refers to the time the laser
is on per-distortion rather than per-frame. Since texp affects the
attack but is not controlled by the attacker — as it is set by the
auto-exposure mechanism — we tested empirically which texp the
camera would set on a sunny or cloudy day outdoors, finding 32µs
and 200µs, respectively. We used this range of texp to collect the
patterns.
Simulation Setup. We simulated the attack effect on a subset of
two different video datasets, namely BDD100K [55], a large and
diverse driving video dataset, and VIRAT [30], a dataset of surveil-
lance footage. We randomly picked 50 videos from the BDD100K
and, due to the longer duration, 25 videos from the VIRAT dataset,
for each video, we extracted every 10th frame. For each parame-
ter configuration (f , texp , ton), we randomly selected ten patterns.
Next, we generated approximately 7 million corrupted frames by
overlaying the extracted patterns over the legitimate video frames.
Finally, we assessed the simulated attack outcome by comparing
legitimate and corrupted frames, as showcased in Figure 8. We eval-
uated two well-known state-of-the-art object detection models from
the Tensorflow Model Zoo, namely Single Shot Detector (SSD) [24]
and Faster RCNN [35] (FRCNN), both using Inception v2 backbone
network. For performance reasons, we resized all frames to 640×360
pixels, before they were passed into the object detectors.1
7.1.2 Results.
Choosing Attack Parameters. The percentage of objects hidden in
the input frames for the various frequencies and exposure times is
presented in Figure 9. It should be noted that for all four frequency
settings, the image area covered by the laser-injected light is the
same. As the frequency increases, ton decreases, which results in
smaller, but more frequent distortions (see Equation 8). However,
the sum of ton per frame stays the same. Figure 9 shows a clear
increment in the ratio of hidden objects when using larger f . For
f = 750Hz, we obtained the largest number of hidden objects. At
texp = 200µs, 51% and 82% of objects were hidden for FRCNN and
SSD, respectively. We noted that the narrower distortions generated
by higher frequencies strongly affect the object detection task, in
particular, for small objects. Given these results, an adversary would
1Implementation and evaluation code is available at https://github.com/ssloxford/they-
see-me-rollin
Figure 9: Percentage of hidden objects in the two video
datasets for f = 25, 250, 500, and 750Hz, various texp and the
two object detection models. Error bars show the standard
deviation over the results per-video.
Figure 10: Percentage of hidden, misplaced, and appeared
objects in the evaluation of the BDD100K and the VIRAT
dataset, for various ton, exposure times texp, and the two
models used in the evaluation (SSD and FRCNN). Shaded ar-
eas show 99% confidence intervals.
select 750Hz as the modulation frequency for the attack, as this
selection better generalizes to different input scenes. In line with
this, we limit the presentation of the remaining results to this 750Hz
configuration.
Effect of Exposure. As the exposure time is not controlled by
the adversary we investigated the effect of texp. Figure 10 shows
the ratio of hidden, misplaced, or appeared objects for various
exposure times texp and laser on-times ton. With increasing texp
and thus greater distortion, more objects are misplaced or hidden.
As expected, an increasing ton also leads to more objects being
hidden. However, the number of misplaced or appeared objects
does not change significantly. In contrast, for short texp less of
the injected light is absorbed, leading to less intensive distortions
and fewer hidden objects. While the weak illumination could be
compensated with a more powerful laser, this highlights how in
dimmer ambient light settings, the attack requires fewer resources
on the adversary’s side. For the largest evaluated exposure (texp =
200µs) and ton = 533µs, our attack can hide up to 90% of objects in