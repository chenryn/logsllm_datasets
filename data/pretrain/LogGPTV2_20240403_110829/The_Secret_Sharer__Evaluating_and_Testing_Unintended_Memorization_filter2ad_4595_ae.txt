1.1
2.3
1.8
2.1
3.2
2.8
3.6
9.5
31.0
measured privacy loss and the upper-bound ε DP guarantees—
with that gap growing (exponentially) as ε becomes very
large [26]. Also, without modifying the training approach,
improved proof techniques have been able to improve guaran-
tees by orders of magnitude, indicating that the analytic ε is
not a tight upper bound. Of course, these improved proof tech-
niques often rely on additional (albeit realistic) assumptions,
such as that random shufﬂing can be used to provide unlinka-
bility [16] or that the intermediate model weights computed
during training can be hidden from the adversary [17]. Our ε
calculation do not utilize these improved analysis techniques.
(cid:88)
10 Related Work and Conclusions
Table 3: The RMSProp models trained with differential pri-
vacy do not memorize the training data and always have lower
testing loss than a non-private model trained using standard
SGD techniques. (Here, ε = ∞ indicates the moments accoun-
tant returned an inﬁnite upper bound on ε.)
We train seven differentially private models using various
values of ε for 100 epochs on the PTB dataset augmented
with one canary inserted. Training a differentially private
algorithm is known to be slower than standard training; our
implementation of this algorithm is 10− 100× slower than
standard training. For computing the (ε,δ) privacy budget
we use the moments accountant introduced in [1]. We set
δ = 10−9 in each case. The gradient is clipped by a threshold
L = 10.0. We initially evaluate two different optimizers (the
plain SGD used by authors of [1] and RMSProp), but focus
most experiments on training with RMSProp as we observe
it achieves much better baseline results than SGD5. Table 3
shows the evaluation results.
The differentially-private model with highest utility (the
lowest loss) achieves only 10% higher test loss than the base-
line model trained without differential privacy. As we de-
crease ε to 1.0, the exposure drops to 1, the point at which
this canary is no more likely than any other. This experimen-
tally veriﬁes what we already expect to be true: DP-RMSProp
fully eliminates the memorization effect from a model. Sur-
prisingly, however, this experiment also show that a little-bit
of carefully-selected noise and clipping goes a long way—as
long as the methods attenuate the signal from unique, secret in-
put data in a principled fashion. Even with a vanishingly-small
amount of noise, and values of ε that offer no meaningful the-
oretical guarantees, the measured exposure is negligible.
Our experience here matches that of some related work.
In particular, other, recent measurement studies have also
found an orders-of-magnitude gap between the empirical,
5We do not perform hyperparameter tuning with SGD or RMSProp. SGD
is known to require extensive tuning, which may explain why it achieves
much lower accuracy (higher loss).
There has been a signiﬁcant amount of related work in the
ﬁeld of privacy and machine learning.
Membership Inference. Prior work has studied the privacy
implications of training on private data. Given a neural net-
work f (·) trained on training data X , and an instance x, it is
possible to construct a membership inference attack [41] that
answers the question “Is x a member of X ?”.
Exposure can be seen as an improvement that quantiﬁes
how much memorization has occurred (and not just if it has).
We also show that given only access to f (·), we extract an
x so that x ∈ X (and not just infer if it is true that x ∈ X ), at
least in the case of generative sequence models.
Membership inference attacks have seen further study, in-
cluding examining why membership inference is possible
[49], or mounting inference attacks on other forms of genera-
tive models [22]. Further work shows how to use membership
inference attacks to determine if a model was trained by us-
ing any individual user’s personal information [44]. These
research directions are highly important and orthogonal to
ours: this paper focuses on measuring unintended memoriza-
tion, and not on any speciﬁc attacks or membership inference
queries. Indeed, the fact that membership inference is possible
is also highly related to unintended memorization.
More closely related to our paper is work which produces
measurements for how likely it is that membership inference
attacks will be possible [30] by developing the Differential
Training Privacy metric for cases when differentially private
training will not be possible.
Generalization in Neural Networks. Zhang et al. [56]
demonstrate that standard models can be trained to perfectly
ﬁt completely random data. Speciﬁcally, the authors show that
the same architecture that can classify MNIST digits correctly
with 99.5% test accuracy can also be trained on completely
random data to achieve 100% train data accuracy (but clearly
poor test accuracy). Since there is no way to learn to clas-
sify random data, the only explanation is that the model has
memorized all training data labels.
Recent work has shown that overtraining can directly lead
to membership inference attacks [53]. Our work indicates that
280    28th USENIX Security Symposium
USENIX Association
even when we do not overtrain our models on the training
data, unintentional memorization remains a concern.
Training data leakages. Ateniese et al. [2] show that if an
adversary is given access to a remote machine learning model
(e.g., support vector machines, hidden Markov models, neural
networks, etc.) that performs better than their own model, it is
often possible to learn information about the remote model’s
training data that can be used to improve the adversary’s own
model. In this work the authors “are not interested in privacy
leaks, but rather in discovering anything that makes classiﬁers
better than others.” In contrast, we focus only on the problem
of private training data.
Backdoor (intentional) memorization. Song et al.
[43]
also study training data extraction. The critical difference
between their work and ours is that in their threat model, the
adversary is allowed to inﬂuence the training process and
intentionally back-doors the model to leak training data. They
are able to achieve incredibly powerful attacks as a result of
this threat model. In contrast, in our paper, we show that mem-
orization can occur, and training data leaked, even when there
is not an attacker present intentionally causing a back-door.
Model stealing studies a related problem to training data
extraction: under a black-box threat model, model stealing
attempts to extract the parameters θ (or parameters similar
to them) from a remote model, so that the adversary can
have their own copy [48]. While model extraction is designed
to steal the parameters θ of the remote model, training data
extraction is designed to extract the training data that was used
to generate θ. That is, even if we were given direct access to
θ it is still difﬁcult to perform training data extraction.
attacks
to
hyperparameter-stealing attacks
attacks
are highly effective, but are orthogonal to the problems we
study in this paper. Related work [38] also makes a similar
argument that it can be useful to steal hyperparameters in
order to mount more powerful attacks on models.
Model inversion [18, 19] is an attack that learns aggregate
statistics of the training data, potentially revealing private
information. For example, consider a face recognition model:
given an image of a face, it returns the probability the input
image is of some speciﬁc person. Model inversion constructs
an image that maximizes the conﬁdence of this classiﬁer
on the generated image; it turns out this generated image
often looks visually similar to the actual person it is meant
to classify. No individual training instances are leaked in this
attack, only an aggregate statistic of the training data (e.g.,
what the average picture of a person looks like). In contrast,
our extraction algorithm reveals speciﬁc training examples.
Private Learning. Along with the attacks described above,
there has been a large amount of effort spent on training pri-
vate machine learning algorithms. The centerpiece of these
defenses is often differential privacy [1, 7, 12, 14, 15]. Our
[50]. These
Later work
extended model-stealing
analysis in Section 9.3 directly follows this line of work and
we conﬁrm that it empirically prevents the exposure of se-
crets. Other related work [40] studies membership attacks
on differentially private training, although in the setting of a
distributed honest-but-curious server.
Other related work [37] studies how to apply adversarial
regularization to reduce the risk of black-box membership in-
ference attacks, although using different approach than taken
by prior work. We do not study this type of adversarial regu-
larization in this paper, but believe it would be worth future
analysis in follow-up work.
10.1 Limitations and Future Work
This work in this paper represents a practical step towards
measuring unintended memorization in neural networks.
There are several areas where our work is limited in scope:
• Our paper only considers generative models, as they
are models that are likely to be trained on sensitive in-
formation (credit card numbers, names, addresses, etc).
Although, our approach here will apply directly to any
type of model with a deﬁned measure of perplexity, fur-
ther work is required to handle other types of machine-
learning models, such as image classiﬁers.
• Our extraction algorithm presented here was designed
to validate that canaries with a high exposure actually
correspond to some real notion of the potential to ex-
tract that canary, and by analogy other possible secrets
present in training data. However, this algorithm has as-
sumptions that make it ill-suited to real-world attacks.
To begin, real-world models usually only return the most
likely output, that is, the arg max output. Furthermore,
we assume knowledge of the surrounding context and
possible values of the canary, which may not hold true
in practice.
• Currently, we only make use of the input-output behav-
ior of the model to compute the exposure of sequences.
When performing our testing, we have full white-box ac-
cess including the actual weights and internal activations
of the neural network. This additional information might
be used to develop stronger measures of memorization.
We hope future work will build on ours to develop further met-
rics for testing unintended memorization of unique training
data details in machine-learning models.
10.2 Conclusions
The fact that deep learning models overﬁt and overtrain to
their training data has been extensively studied [56]. Because
neural network training should minimize loss across all exam-
ples, training must involve a form of memorization. Indeed,
USENIX Association
28th USENIX Security Symposium    281
signiﬁcant machine learning research has been devoted to
developing techniques to counteract this phenomenon [45].
In this paper we consider the related phenomenon of what
we call unintended memorization: deep learning models (in
particular, generative models) appear to often memorize rare
details about the training data that are completely unrelated
to the intended task while the model is still learning the un-
derlying behavior (i.e., while the test loss is still decreasing).
As we show, traditional approaches to avoid overtraining do
not inhibit unintentional memorization.
Such unintended memorization of rare training details may
raise signiﬁcant privacy concerns when sensitive data is used
to train deep learning models. Most worryingly, such memo-
rization can happen even for examples that are present only a
handful of times in the training data, especially when those
examples are outliers in the data distribution; this is true even
for language models that make use of state-of-the-art regular-
ization techniques to prevent traditional forms of overﬁtting
and overtraining.
To date, no good method exists for helping practitioners
measure the degree to which a model may have memorized
aspects of the training data. Towards this end, we develop ex-
posure: a metric which directly quantiﬁes the degree to which
a model has unintentionally memorized training data. We
use exposure as the basis of a testing methodology whereby
we insert canaries (orthogonal to the learning task) into the
training data and measure their exposure. By design, exposure
is a simple metric to implement, often requiring only a few
dozen lines of code. Indeed, our metric has, with little effort,
been applied to construct regression tests for Google’s Smart
Compose [29]: a large industrial language model trained on a
privacy-sensitive text corpus.
In this way, we contribute a technique that can usefully be
applied to aid machine learning practitioners throughout the
training process, from curating the training data, to selecting
the model architecture and hyperparameters, all the way to
extracting meaning from the ε values given by applying the
provably private techniques of differentially private stochastic
gradient descent.
Acknowledgements
We are grateful to Martín Abadi, Ian Goodfellow, Ilya
Mironov, Ananth Raghunathan, Kunal Talwar, and David Wag-
ner for helpful discussion and to Gagan Bansal and the Gmail
Smart Compose team for their expertise. We also thank our
shepherd, Nikita Borisov, and the many reviewers for their
helpful suggestions. This work was supported by National
Science Foundation award CNS-1514457, DARPA award
FA8750-17-2-0091, Qualcomm, Berkeley Deep Drive, and
the Hewlett Foundation through the Center for Long-Term
Cybersecurity. Any opinions, ﬁndings, and conclusions or
recommendations expressed in this material are those of the
author(s) and do not necessarily reﬂect the views of the Na-
tional Science Foundation.
References
[1] Martín Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya
Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential
privacy. In ACM CCS, 2016.
[2] Giuseppe Ateniese, Luigi V Mancini, Angelo Spognardi, Antonio Vil-
lani, Domenico Vitali, and Giovanni Felici. Hacking smart machines
with smarter ones: How to extract meaningful data from machine learn-
ing classiﬁers. International Journal of Security and Networks, 2015.
[3] D Bahdanau, K Cho, and Y Bengio. Neural machine translation by
jointly learning to align and translate. ICLR, 2015.
[4] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jau-
vin. A neural probabilistic language model. JMLR, 2003.
[5] James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher.
Quasi-recurrent neural networks. arXiv preprint arXiv:1611.01576,
2016.
[6] Lord Castleton. Review: Amazon’s ‘Patriot’ is the best show of the
year. 2017. Pajiba. http://www.pajiba.com/tv_reviews/review-
amazons-patriot-is-the-best-show-of-the-year.php.
[7] Kamalika Chaudhuri and Claire Monteleoni. Privacy-preserving logis-
tic regression. In NIPS, 2009.
[8] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Ben-
gio. Empirical evaluation of gated recurrent neural networks on se-
quence modeling. NIPS Workshop, 2014.
[9] Joseph Conrad. The Secret Sharer. EBook #220. Project Gutenberg,
2009. Originally published in Harper’s Magazine, 1910.
[10] T Cormen, C Leiserson, R Rivest, and C Stein. Introduction to Algo-
rithms. MIT Press, 2009.
[11] TensorFlow Developers. Tensorﬂow neural machine translation tutorial.
https://github.com/tensorflow/nmt, 2017.
[12] Irit Dinur and Kobbi Nissim. Revealing information while preserving
privacy. In Proceedings of the twenty-second ACM SIGMOD-SIGACT-
SIGART symposium on Principles of database systems. ACM, 2003.
[13] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient
methods for online learning and stochastic optimization. Journal of
Machine Learning Research, 12(Jul):2121–2159, 2011.
[14] C Dwork, F McSherry, K Nissim, and A Smith. Calibrating noise to
sensitivity in private data analysis. In TCC, volume 3876, 2006.
[15] Cynthia Dwork. Differential privacy: A survey of results. In Intl. Conf.
on Theory and Applications of Models of Computation, 2008.
[16] Úlfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan,
Kunal Talwar, and Abhradeep Thakurta. Ampliﬁcation by shufﬂing:
From local to central differential privacy via anonymity. In Proceedings
of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms,
pages 2468–2479. SIAM, 2019.
[17] Vitaly Feldman, Ilya Mironov, Kunal Talwar, and Abhradeep Thakurta.
Privacy ampliﬁcation by iteration. In IEEE FOCS, 2018.
[18] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion
attacks that exploit conﬁdence information and basic countermeasures.
In ACM CCS, 2015.
[19] Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page,
and Thomas Ristenpart. Privacy in pharmacogenetics: An end-to-end
In USENIX Security
case study of personalized Warfarin dosing.
Symposium, pages 17–32, 2014.
[20] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.
Deep learning, volume 1. MIT press Cambridge, 2016.
[21] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz
Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming
He. Accurate, large minibatch SGD: Training ImageNet in 1 hour.
arXiv preprint arXiv:1706.02677, 2017.
[22] Jamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristo-
faro. LOGAN: Evaluating privacy leakage of generative models using
generative adversarial networks. PETS, 2018.
282    28th USENIX Security Symposium
USENIX Association
[23] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory.
Neural computation, 9(8):1735–1780, 1997.