complete as soon as Wackamole reconﬁgures without ad-
ditional need to transfer routing information. Essentially,
this setup is closer to the setup described in Section .
Our solution, in both scenarios, provides the additional
beneﬁt of allowing a heterogenous set of physical routers
to collaborate in forming a virtual router. Using a vari-
ety of architectures and operating systems for the routers
provides increased protection of the virtual router against
security exploits that may target speciﬁc platforms.
6 Performance Results
In order to assess the performance of Wackamole, the
most relevant measurement is the length of the service
interruption perceived by a client when the server with
which it is communicating is made unavailable by a fault.
For this reason, we report results for the average avail-
ability interruption time when a computer running Wack-
amole fails and its virtual addresses must be reallocated
to another computer, as measured from a client.
In our experiment we place a simple server process
on each computer using Wackamole. The server re-
sponds to UDP packets by sending a packet containing
its hostname. A client process on another computer is in-
structed to continuously access a speciﬁc virtual address
by sending UDP request packets at a speciﬁed interval,
and record the hostname of the server that responds as
well as the time since the last response was received. For
our experiments, we used a 10ms interval between re-
quests. The value is the smallest that can be practically
used, and is determined by the linux context-switch times.
When a fault is induced by disconnecting the interface
through which Spread, Wackamole, and the experimental
server access the network, the client will stop receiving
responses to its requests. When Wackamole completes
the IP address reallocation procedure and the client’s ARP
cache is updated, the client resumes receiving responses
to its queries, this time from the computer that has aquired
its target address. The time elapsed between the receipt
of the last response from the disabled computer and the
ﬁrst response from the new server is the availability inter-
ruption time from the experimental client’s point of view.
While there is a small possibility for error in this mea-
surement due to the interval between requests and ﬂuc-
tuations in the network, our measurements represent an
upper bound on the actual interruption time.
As discussed, Wackamole depends upon the Spread
group communication toolkit for notiﬁcation of member-
ship changes. For this reason, the availability interrup-
Parameter Name
Default Spread
Tuned Spread
Fault-detection timeout
Distributed Heartbeat timeout
Discovery timeout
5
2
7
1
0.4
1.4
Table 1. Spread timeout tuning (seconds)
tion time measures the total time to complete four actions:
Spread’s detection of membership changes, Spread’s dae-
mon and process group membership installation, Wack-
amole’s state transfer and virtual address reallocation, and
Wackamole’s ARP spooﬁng.
In light of this dependency, we performed two sets of
experiments. The ﬁrst set uses the default Spread settings
with timeout intervals designed to perform adequately on
most networks, for a variety of applications. The second
set uses a ﬁne-tuned version of Spread, in which we ad-
justed the relevant timeout intervals speciﬁcally for the
Wackamole application and our network setup. Both ex-
periments were run on a 100Mbit Ethernet LAN cluster,
maintaining 10 virtual IP addresses in a cluster, and vary-
ing the number of servers from 2 to 12.
Table 1 shows the differences between the two exper-
iment setups. The timeouts presented in the table cover
the major components of the time it takes Spread to notify
Wackamole of network faults. The distributed heartbeat
timeout speciﬁes an interval after which a Spread daemon
notiﬁes other daemons that it is still in operation. The
fault-detection timeout begins at approximately the same
time as the distributed heartbeat timeout; after the fault-
detection timeout expires, if a daemon has not speciﬁed
that it is operating, Spread assumes a fault has taken place
and attempts to reconﬁgure. Because a fault could occur
at any time during the heartbeat interval, the actual time
to detect a failure ranges from failure-detection timeout -
distributed heartbeat timeout to failure-detection timeout.
The discovery timeout is the time spent performing this
reconﬁguration by determining the currently available set
of Spread daemons and installing this conﬁguration view
at each daemon. Thus the time it takes the default Spread
to notify Wackamole of a failure (ignoring the minor over-
head of Spread’s group membership procedure) ranges
from 10 seconds to 12 seconds. For the tuned Spread,
this time ranges from 2 seconds to 2.4 seconds.
Figure 5 displays the average availability interruption
time when varying the cluster size, for each version of
Spread. We notice that the Spread timeouts account for
the majority of the interruption time recorded in our ex-
periments.
These results were obtained using a cluster of servers
under low average load. Both Wackamole and Spread can
be used in production on highly-loaded machines as well.
However, it is recommended that both daemon processes
be run with high priority (real-time priority under Linux)
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:00:51 UTC from IEEE Xplore.  Restrictions apply. 
)
s
d
n
o
c
e
s
(
n
o
i
t
p
u
r
r
e
t
n
I
y
t
i
l
i
b
a
l
i
a
v
A
14
12
10
8
6
4
2
0
Average Availability Interruption
Default Spread
Fine-tuned Spread
0
2
4
6
8
10
12
14
Cluster Size
Figure 5. Average Availability Interruption
with Varying Cluster Size
in these types of environments in order to avoid false pos-
itive errors. This has no adverse impact on the cluster
performance as Spread and Wackamole hardly consume
resources when used for this application.
Also relevant is the availability interruption time when
a Wackamole daemon leaves voluntarily, not as the result
of a failure. This is experienced when Wackamole dae-
mons are taken ofﬂine for administrative or policy rea-
sons. However, we found that this time interval is difﬁ-
cult to measure precisely, because it is more susceptible
to context switch times and other low-level ﬂuctuations.
In general, our measurements suggest a conservative up-
per bound of 250 milliseconds of availability interruption
on our experimental cluster; most of our measurements
actually recorded an interruption time as small as 10ms.
7 Related Work
Wackamole, in its current state, has evolved from an
idea ﬁrst introduced in [1]. There are two areas that relate
to the work presented in this paper. On one hand our solu-
tion beneﬁts from extensive research in the areas of group
communication and distributed algorithms. On the other
hand, various other techniques have been employed to
provide availability for critical services. Additionally our
IP fail-over solution is usually used in conjunction with
load-balancing mechanisms. Wackamole is often used to-
gether with the mod-backhand load-balancing module for
web servers. We do not further address the coupling be-
tween Wackamole and various load-balancing techniques
as it is outside the scope of the paper.
Research in the group communication area has lead
to the implementation of several systems which pro-
vide properties similar to those required by the Wack-
amole algorithm. Among such systems we mention Ho-
rus [21], Ensemble [10], Totem [2]. The Wackamole al-
gorithm uses a design similar to the state machine ap-
proach for maintaining consistent state in distributed sys-
tems [19, 16]
Wackamole as a fail-over solution is designed to pre-
serve the IP presence of a service. The Virtual Router
Redundancy Protocol (VRRP) was designed to perform a
similar task for routers. VRRP speciﬁes an election pro-
tocol that dynamically assigns responsibility for a virtual
router to one of the VRRP routers on a local area net-
work. VRRP design is chaired by an IETF working group
and has been formalized into an Internet Standard RFC
2338 [22]. A similar protocol is the Hot Standby Router
Protocol (HSRP) developed by Cisco [11].
In essence,
HSRP elects one router to be the active router and an-
other to be the standby router. The active and the standby
routers send hello messages. The standby router is the
candidate to take over the active role if the active router
faults. All other routers are monitoring the hello mes-
sages sent by the active and standby routers. Routers may
be assigned priorities. The router with the highest prior-
ity will become the active router after initialization. After
an certain Active timeout elapses without hearing hello
messages from the active router, the standby router takes
over. Similarly if a Standby timeout elapses, a monitor-
ing router (if such exist) with the lower IP address takes
over the standby role. By default, hello messages are sent
every 3 seconds and the Active and Stanby timeouts are
set to 10 seconds.
Aside from IP fail-over, front-end high-availability and
load-balancing devices are often used in front of mis-
sion critical networked services to provide uninterrupted
service in the event of a system failure. These devices
perform application level checks against machines in the
cluster and keep track of which machines are providing
service. They present a virtual IP address to which clients
connect, and then dynamically set the local endpoint of
the IP connection to an active machine in the local clus-
ter. These devices are in common use today to support
most large Internet sites and are provided by a variety
of vendors. Such devices include Cisco’s Arrowpoint
[4], Foundry’s ServerIron [12], F5’s BIG/ip [5], Coyote-
Point’s Equalizer [7], and Linux Virtual Server [13].
While these components may provide more than just
high-availability (speciﬁcally load balancing), they them-
selves must be made highly available – by itself, any such
component is a single point of failure. Each vendor has its
own method of providing High availability between two
of their devices, but an application independent protocol
such as VRRP or Wackamole could just as easily be used
to accomplish this.
Many services need high availability and only reme-
dial load-balancing techniques such as multiple DNS A
records. For these architectures, using an IP fail-over
protocol directly on the machine providing the service
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:00:51 UTC from IEEE Xplore.  Restrictions apply. 
in question reduces the need for complicated, expen-
sive and otherwise unnecessary high-availability/load-
balancing components.
The Linux Fake project [8] provides IP fail-over
through service-probing and ARP-spooﬁng. The avail-
ability of the main server is probed regularly and upon
failure detection a backup server instantiates a virtual IP
interface that will take over the failed one and send a gra-
tuitous ARP request to accelerate the transition.
The PolyServe Matrix HA [17] product provides a ser-
vice similar to Wackamole. The technical details of the
implementation or the soundness of the protocols cannot
be assessed as the product and procotols are unreleased.
Until recently the Polyserve solution only offered pair-
wise fail-over, where every server is covered by one other
speciﬁc server. The latest version of the software is re-
porting use of the Spread Toolkit and provides N:M, N:N,
and N:1 IP failover.
In their presentation of the Raincore Distributed Ses-
sion Service infrastructure [9], the authors mention a Vir-
tual IP Manager application that similarly to Wackamole
exploits underlying group communication guarantees to
provide fail-over for servers and also indicate that the
technology can be applied to ﬁrewalls or routers.
8 Conclusions
This paper presented a software-based distributed so-
lution for providing high availability for clusters and
routers at the IP addressing level. The core algorithm
relies on a group communication service to monitor the
currently connected membership and reallocate virtual IP
addresses that are accessible to client machines, between
the avaible servers. We presented the algorithm and dis-
cussed its correctness. We discussed two classes of practi-
cal applications of the system and provided experimental
performance results.
The Wackamole system has been available as an open-
source tool since August 2001 (www.wackamole.org).
During the past 20 months the system was downloaded
more than 1000 times and is actively used in production
environments for both the web-cluster and router avail-
ability applications described in this paper. This work
demonstrates how sound academic research can readily
make an impact in production environments.
8.1 Acknowledgements
We thank Jim Burns, Brian Coan, Gary Levin and San-
jai Narain from Telcordia Technologies for their insight-
ful discussions and suggestions.
References
[1] Y. Amir, Y. Gu, T. Schlossnagle, and J. Stanton. Practical
cluster applications of group communication. In Proceed-
ings of the IEEE International Conference on Dependable
Systems and Networks, New York, NY, 2000.
[2] Y. Amir, L. E. Moser, P. M. Melliar-Smith, D. A. Agarwal,
and P. Ciarfella. Fast message ordering and membership
using a logical token-passing ring. In Proceedings of the
13th IEEE International Conference on Distributed Com-
puting Systems, May 1993.
[3] Y. Amir and J. Stanton. The spread wide area group
communication system. Technical Report CNDS 98-4,
Johns Hopkins University, Center for Networking and
Distributed Systems, 1998.
[4] Cisco/arrowpoint. http://www.cisco.com/en/US/products/
hw/contnetw/ps792/index.html.
[5] BIG/ip. http://www.f5.com/f5products/bigip/.
[6] K. P. Birman and T. A. Joseph. Exploiting virtual syn-
chrony in distributed systems. In Proceedings of the ACM
Symposium on OS Principles, pages 123–138, 1987.
[7] Equalizer. http://www.coyotepoint.com/equalizer.htm.
[8] The
Linux
Fake
Project.
http://www.vergenet.net/linux/fake.
[9] C. Fan and J. Bruck. The raincore distributed session ser-
vice for networking elements. In Workshop on Commu-
nication Architecture for Clusters. International Parallel
and Distributed Processing Symposium, 2001.
[10] M. Hayden. The Ensemble System. PhD thesis, Cornell
University, 1998.
[11] Hot Standby Router Protocol. http://www.cisco.com/ uni-
vercd/cc/td/doc/cisintwk/ics/cs009.htm.
[12] Foundry/ServerIron. http://www.foundrynet.com/products/
webswitches/serveriron/.
[13] Linux Virtual Server. http://www.linuxvirtualserver.org/.
[14] L. E. Moser, Y. Amir, P. M. Melliar-Smith, and D. A.
Agarwal. Extended virtual synchrony.
In International
Conference on Distributed Computing Systems, pages 56–
65, 1994.
[15] Open
Shortest
Path
First.
http://www.ietf.org/html.charters/ospf-charter.html.
[16] F. Pedone.
The Database State Machine and Group
Communication Issues. PhD thesis, ´Ecole Polytechnique
F´ed´erale de Lausanne, Switzerland, 1999.
[17] Polyserver Matrix HA. http://polyserve.com/.
[18] Routing
Information
Protocol.
http://www.ietf.org/html.charters/rip-charter.html.
[19] F. B. Schneider. Implementing fault-tolerant services us-
ing the state machine approach: A tutorial. ACM Comput-
ing Surveys, 22(4):299–319, Dec. 1990.
[20] The Spread Toolkit. http://www.spread.org.
[21] R. van Renesse, K. P. Birman, and S. Maffeis. Horus: A
ﬂexible group communication system. Communications
of the ACM, 39(4):76–83, Apr. 1996.
[22] Virtual
Router
Redundancy
Protocol.
http://www.ietf.org/html.charters/vrrp-charter.html.
[23] Wackamole. http://www.wackamole.org.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:00:51 UTC from IEEE Xplore.  Restrictions apply.