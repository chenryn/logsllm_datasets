table
271
394
394
630
394
468
-
NAT
hash
ring
271
610
683
729
610
610
-
Table 2: Median instructions retired per packet for each NF under each workload.
NF
LPM
btrie
LPM
1-stage
DL
1
NOP
2
1 Packet
2
Zipfian
UniRand
3
UniRand CASTAN 2
3
CASTAN
Manual
-
NAT
hash
ring
1
2
2
4
2
4
-
Table 3: Median L3 misses per packet incurred by each NF under each workload.
LB red-
black
tree
1
2
2
2
2
2
-
NAT red-
black
tree
1
2
2
7
2
2
-
NAT un-
balanced
tree
1
2
2
5
2
2
2
LB
un-
balanced
tree
1
2
2
2
2
2
2
LPM
2-stage
DL
1
2
2
3
2
2
-
NAT
hash
table
1
1
2
8
2
2
-
LB hash
table
1
2
2
2
2
2
-
1
2
2
2
2
2
2
adversarial CASTAN workload. Moreover, the two workloads
exhibit the same number of retired instructions per packet
(Table 2), but different L3 cache misses per packet (Table 3),
confirming that the CASTAN workload’s worse performance
is due to a higher cache miss rate.
The results are different for LPM with two-stage Direct
Lookup. This NF uses a 64MB array in the first stage, which
still exceeds the L3 cache, but not by orders of magnitude.
Fig. 6 shows that all workloads except for UniRand experi-
ence similar latency. This is not surprising: On the one hand,
CASTAN managed to find only 10 packets that could map to
the same L3 cache location; this number is below cache asso-
ciativity, which means that the CASTAN workload could not
cause cache contention. On the other hand, the NF’s data
380
 0 0.2 0.4 0.6 0.8 1 4200 4400 4600 4800 5000 5200 5400 5600 5800CDFLatency (ns)NOP1 PacketZipﬁanUniRandUniRand CASTANCASTAN 0 0.2 0.4 0.6 0.8 1 1000 1500 2000 2500 3000CDFReference Clock CyclesNOP1 PacketZipﬁanUniRandUniRand CASTANCASTAN 0 0.2 0.4 0.6 0.8 1 4200 4400 4600 4800 5000 5200 5400 5600 5800CDFLatency (ns)NOP1 PacketZipﬁanUniRandUniRand CASTANCASTANSIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
L. Pedrosa et al.
Figure 7: End-to-end latency CDF
for LPM implemented with a Patri-
cia trie.
Figure 8: CPU reference cycles CDF
for LPM implemented with a Patri-
cia trie.
Figure 9: End-to-end latency CDF
for NAT implemented with an un-
balanced tree.
structures still exceed the L3 cache, which means that a large
enough workload could cause cache contention—and indeed,
UniRand, with its 1M packets and flows, does. Given enough
time, we expect that CASTAN would also produce a workload
large enough to cause cache contention, but it is not yet able
to do it in useful time.
In conclusion, even though, in the common case, two-stage
Direct Lookup is one-memory-access slower than one-stage
Direct Lookup, it is more robust against performance attacks,
because the smaller data structures make it harder to find
small workloads that cause cache contention.
5.3 Algorithmic Complexity Attacks
Next, we look at NFs that we expect to be susceptible to
adversarial workloads seeking to increase the number of
instructions per packet.
We start from LPM with a Patricia trie. For this NF, CASTAN
synthesized a workload of 30 packets and flows. We also
crafted a Manual workload of 8 packets that match the most
specific routes of the forwarding table, which results in
traversing the longest paths of the trie. Upon inspection,
we found that the CASTAN workload closely resembles the
Manual one: in addition to finding packets that match the
most specific routes, it also picked packets that are off by
just one bit at the end, thus requiring the same amount of
processing steps.
As we can see in Fig. 7, the 30-packet CASTAN workload
experiences slightly worse latency than the 100K-packet Zip-
fian, and similar latency to the 1M-packet UniRand workload.
Moreover, CASTAN experiences similar latency to Manual
without the benefit from human insight. According to the
micro-architectural analysis, CASTAN and Manual consume
significantly more reference cycles (Fig. 8) and instructions
(Table 2) per packet than the other workloads. This differ-
ence, however, did not translate into a significant difference
in latency.
We also consider NAT and LB with unbalanced trees. For
these NFs, CASTAN synthesized workloads consisting, respec-
tively, of 50 and 30 packets and flows. We also crafted Manual
workloads that skew the tree and turn it into a linked list; e.g.,
for NAT, such a workload consists of a sequence of packets
with the same source and destination IP and source port,
and increasing destination ports. Another way to stress-test
these NFs is to increase the size of the tree as much as possi-
ble by sending a large number of flows, which is what the
UniRand workload does.
Fig. 9 shows the latency CDF for NAT (the results for LB
are similar). The most interesting result is that the 50-packet
CASTAN workload experiences 67% worse median latency
than the 100K-packet Zipfian, though it cannot beat the 1M-
packet UniRand. UniRand experiences more latency than
Zipfian, simply because it has an order of magnitude more
flows, hence creates a larger tree. CASTAN, on the other hand,
experiences more latency than Zipfian with two orders of
magnitude fewer flows, because it creates an unbalanced tree.
Moreover, CASTAN experiences similar latency to Manual
without the benefit of human insight.
The micro-architectural analysis confirms these results:
The number of reference cycles per packet approximately
mirrors latency (Fig. 10). Moreover, all workloads experience
similar L3 cache misses per packet (Table 3), but different
numbers of retired instructions per packet (Table 2), confirm-
ing that, for this NF, performance differences are due mostly
to algorithmic complexity, not cache contention.
Not surprisingly, if we replace the unbalanced tree with a
Red-Black tree, CASTAN fails to find an adversarial workload,
and latency experienced depends simply on the total number
of flows (which determine the size of the tree). This is illus-
trated in Fig. 11, which shows the latency CDF for NAT with
a Red-Black tree (the results for LB are similar): The 1M-flow
UniRand experiences worse latency than the 6K-flow Zipfian,
which experiences worse latency than the 50-flow CASTAN.
Internally, this kind of code induces local maxima within the
CASTAN analysis. As the analysis selects states that make the
tree deeper, the rebalancing algorithm kicks in and thwarts
the attempt. In the end, CASTAN explores many states with
mostly similar costs and picks the worst among the almost
equal candidates.
381
 0 0.2 0.4 0.6 0.8 1 4200 4400 4600 4800 5000 5200 5400 5600 5800CDFLatency (ns)NOP1 PacketZipﬁanUniRandUniRand CASTANCASTANManual 0 0.2 0.4 0.6 0.8 1 800 1000 1200 1400 1600 1800 2000 2200 2400 2600CDFReference Clock CyclesNOP1 PacketZipﬁanUniRandUniRand CASTANCASTANManual 0 0.2 0.4 0.6 0.8 1 4200 4400 4600 4800 5000 5200 5400 5600 5800CDFLatency (ns)NOP1 PacketZipﬁanUniRandUniRand CASTANCASTANManualAutomated Synthesis of Adversarial Workloads
for Network Functions
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Figure 10: CPU reference cycles
CDF for NAT implemented with an
unbalanced tree.
5.4 Cracking Hash Functions
Finally, we look at NFs that use hash functions for indexing
their data structures and ask: can we craft workloads that
consist of relatively few packets, yet introduce a significant
rate of hash collisions?
We start from two LB NFs, one with a 65, 536-entry hash
table, and one with a 17M-entry hash ring. For the former,
CASTAN synthesized a workload of 30 packets that cause per-
sistent hash collisions. This workload experiences slightly
worse latency than the 100K-packet Zipfian, and slightly bet-
ter latency than the 1M-packet UniRand (Fig. 12). In the case
of the hash ring, however, the dominant adversarial behavior
caused by the CASTAN workload is cache contention. This is
because the sheer size of the hash ring makes it vulnerable
to adversarial memory access, and CASTAN found it easier to
synthesize packets that contend for the same L3 cache loca-
tion than packets that cause collisions. The CASTAN workload
experiences 56% worse median latency than Zipfian and 32%
worse median latency than UniRand (Fig. 13).
We also consider NAT NFs that use the same data struc-
tures. These pose a particularly difficult challenge to the
CASTAN analysis, as the NAT hashes and stores two entries
for each flow, using different parts of the packet to form
different keys (one to match outgoing packets and another
to match returning traffic). The challenge lies in the fact that
while both hashes are independently havoced, the keys that
serve as inputs are related and share some portion (the ex-
ternal end-point’s IP and port). This means that CASTAN not
only has to find an entry in the rainbow table that reverses
the hash, but actually two of them that reverse two differ-
ent hashes while preserving the relationship between keys
and also satisfying the constraints on flow uniqueness. In
practice, CASTAN was rarely able to do this reliably. For the
hash table, the complex set of constraints for the related keys
and packet uniqueness while also trying to cause system-
atic collisions defeated CASTAN, as none of the havocs were
successfully reversed. For the hash ring, the fact that each
havoc reverses a unique value made the problem somewhat
easier. As a result, we were able to systematically reverse
Figure 11: End-to-end latency CDF
for NAT implemented with a red-
black tree.
Figure 12: End-to-end latency CDF
for LB implemented with a hash ta-
ble.
NF
LB / Hash table
LB / Hash ring
LB / Red-Black Tree
LB / Unbalanced Tree
LPM / Patricia Trie
LPM / Lookup Table
LPM / DPDK LPM
NAT / Hash Table
NAT / Hash ring
NAT / Red-Black Tree
NAT / Unbalanced Tree
# Packets
30
40
30
30
30
40
40
30
40
35
50
Time (seconds)
115
31955
437
453
1166
2542
88508
5210
2040
6836
2444
Table 4: List of NFs, indicating how many packets we
generated and the analysis run time.
the first of the two havocs used for each packet, while satis-
fying all uniqueness constraints. The second one remained
unreconciled, as we could not find entries in the rainbow
table that both reversed the second hash and had a key that
was related to the first one in the expected manner.
The results reflect this outcome: For the NAT with hash ta-
ble, the CASTAN workload experiences slightly worse latency
than the Zipfian, but significantly better latency than the
UniRand (Fig. 14). For the NAT with hash ring, the fact that
one of the two hashes is successfully reversed for each packet
allows part of the expected slowdown to be achieved (the
one that results from the first access to the data structure).
As a result, the CASTAN workload experiences 159% worse
median latency than Zipfian and 89% worse median latency
than UniRand (Fig. 15).
5.5 Discussion
Through our measurement campaign, we were able to show
that CASTAN is quite capable of generating useful adversarial
workloads. In Table 5, we summarize the key results, showing
how each NF is affected by typical and adversarial workloads,
including a manually crafted one and the one generated
by CASTAN. Table 4 shows how long it took for CASTAN to
generate these workloads. The results show that when it was
possible to manually create an adversarial workload using
human intuition, CASTAN closely matched it’s performance,
382
 0 0.2 0.4 0.6 0.8 1 1000 2000 3000 4000 5000CDFReference Clock CyclesNOP1 PacketZipﬁanUniRandUniRand CASTANCASTANManual 0 0.2 0.4 0.6 0.8 1 4200 4400 4600 4800 5000 5200 5400 5600 5800CDFLatency (ns)NOP1 PacketZipﬁanUniRandUniRand CASTANCASTAN 0 0.2 0.4 0.6 0.8 1 4200 4400 4600 4800 5000 5200 5400 5600 5800CDFLatency (ns)NOP1 PacketZipﬁanUniRandUniRand CASTANCASTANSIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
L. Pedrosa et al.
Figure 13: End-to-end latency CDF
for LB implemented with a hash
ring.
Figure 14: End-to-end latency CDF
for NAT implemented with a hash
table.
Figure 15: End-to-end latency CDF
for NAT implemented with a hash