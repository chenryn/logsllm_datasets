tomatic updates may break over time or are only partially applied
(e.g., configuration files are part of software packages on Linux dis-
tributions, but they are rarely updated by automatic updates, and
misconfigurations and insecure configurations remain even with
automatic updates enabled).
Moreover, a website administrator can easily build up a false
sense of security. If a technology stack is changed because it used
to be vulnerable, the administrator might consider the website safe
from specific classes of vulnerabilities. For example, a website
might migrate from pages generated dynamically on the server-
side to statically-generated pages, with the dynamically-generated
pages becoming unmaintained orphaned. The administrator may
then think the website is secure from server-side code injection at-
tacks or path traversal vulnerabilities because no code is being ex-
ecuted on the server-side anymore. However, as these pages have
only been orphaned and not truly removed, this assumption would
be a mistake. Consequently, an unmaintained orphaned web page
can become the Achilles’ heel of the entire website.
2.5 Threat Model
In this paper, we assume an external attacker with no special ac-
cess to the target infrastructure or resources exceeding those of
an average Internet user (broadband network connection, off-the-
shelf PC, no special threat intelligence feeds). Hence, as they lack
inside knowledge, if an attacker approaches a target domain, they
face the challenge of identifying potentially orphaned domains for
further vulnerability scanning [35]. We demonstrate in Section 4.8
that traditional approaches, e.g., google dorks, are not effective
at identifying orphaned pages. Hence, we develop a toolchain
that utilizes the Internet Archive to detect (potentially) orphaned
web page on any domain archived. We describe the design of our
framework in Section 3 and demonstrate its efficacy at detecting
orphaned pages at scale in Section 4. Please note that if the ob-
jective of the attacker is to find a vulnerable orphan page on a spe-
cific domain, heuristics we use for the large-scale evaluation can be
omitted for a per-domain attack, widening the range of detectable
URLs beyond the lower-bound we provide in our study. Finally,
we also work on the assumption that such an attacker does not
have to be familiar with advanced exploitation techniques, as or-
phaned websites are—on average—more prone to straight-forward
vulnerabilities. We confirm this assumption in Section 5.
3 Methodology
Following, we introduce our methodology for identifying orphaned
pages, and detail our large-scale measurement.
3.1 Orphaned Page Identification Methodology
We define orphaned web pages as pages with no ancestry links
in the domain’s sitemap (see Section 2). That is, when embarking
from the root and following links, no path exists to reach the or-
phaned page. Consequently, this means we cannot rely on any in-
formation on the domain/website to identify orphaned pages. For
a historic perspective on which sites used to be reachable via the
root, we thus leverage the Internet Archive (IA) [2], which pro-
vides access to a time-stamped history of websites.
Leveraging this archived data, we learn how a domain used to
look like throughout the years, not only in terms of content, but
also in terms of its structure, that is, its sitemap. This allows us to
compare previous versions of a website to its current version, and
see whether any of the paths in its sitemap have been removed.
Correspondingly, we can identify potential candidate orphan
pages by investigating pages that were once part of a domain’s
sitemap, but are not part of it anymore. We can then probe these
candidate pages to determine if they are still accessible, and whether
their content is different from the last archived version. Using this
methodology, we detect pages from quadrant 1 of Figure 1.
A website administrator might advertise (part of) a website’s
structure in an XML file called sitemap.xml. This provides a list of
Session 1A: Cybercrime CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea23pages on the domain that a search engine can crawl and index. Un-
fortunately, not every website makes their sitemap.xml available.
Some that do might have them excluded from the Internet Archive
(see Section 3.2). Therefore, we cannot rely on a pre-constructed
sitemap, but instead need to reconstruct it by retrieving the full
list of archived pages from the Internet Archive, and using it to
determine which URLs might contain potentially orphaned pages.
3.1.1 Gathering Candidate Orphan Pages First, see Figure 3, we
utilize the Wayback CDX to retrieve the archived data [19]. It al-
lows us to query a domain, after which the CDX Server returns the
list of archived pages for that domain, including subdomains. From
this list, we collect two sets: a current set, and a past set. The cur-
rent set contains all the pages encountered by the crawler in 2020,
the past set all pages last encountered before 2020. Because the In-
ternet Archive makes use of crawls, that is, traverses the website
from the root or inspects the domain’s sitemap.xml, we know that
each page is reached via a path starting from the root or it is listed
in the sitemap. If a page was listed before, but is not present in
the 2020 crawls, it has either been removed, was ignored by a later
crawl, or got orphaned. We identify orphaned pages by liveness
probing after pre-filtering candidates locally (see Section 3.1.4).
We have to account for the fact that the Internet Archive also
stores pages that caused redirects, server errors, or client errors
during the initial crawl. Given that we are only interested in reach-
able pages, we query the API to only return web pages that re-
sponded with a HTTP status code OK (200) in their original crawl.
3.1.2 Discarding Resource Files We focus our analysis of orphaned
pages on actual web pages. However, domains may host additional
resources, such as documents, pictures, JavaScript code, stylesheets,
etc., which are of no interest to us. Thus, we filter the list of candi-
date orphan pages by removing URLs ending in a known resource-
related file extension (see Appendix B for the full list).
3.1.3 Dynamic URL Detection (DUDe) URLs that are dynamically
generated are a common challenge to identifying any kind of re-
source on the Internet. They can occur when news sites make arti-
cles available under unique URLs, but they are only reachable from
the index page while they are recent. For example, for archived
news articles, some form of (chronological) structure may be used,
like https://example.com/articles/1997/January/name-of-the-ar
ticle.html. This results in many URLs sharing a similar prefix
with a (possibly generated) suffix. Probing all these URLs would
put unnecessary load on webservices. Moreover, it would signifi-
cantly extend the runtime of our approach, while these URLs are
not actually as interesting to us as they are likely not truly unmain-
tained orphaned, but are likely actually maintained.
Therefore, we use a heuristic to identify the common prefixes
of these URLs and remove them from our list before probing for
liveness. If a page contains many long links, we try to identify a
common prefix based on character frequency. We do so by count-
ing the frequency of each character at an index of the URLs, and
generating a prefix based on the most frequent character for each
position (in case of a tie, the first one encountered will be used).
We then shorten the prefix, one character at a time, until it is gen-
eral enough (see below), or until we consider it too short to be a
valid prefix. We repeat this process until we can detect no more
𝑙𝑎𝑟𝑔𝑒_𝑙𝑖𝑛𝑘𝑠.𝑎𝑝𝑝𝑒𝑛𝑑(𝑙𝑖𝑛𝑘)
for 𝑐 in 𝑙𝑖𝑛𝑘 do
Count character frequency at each position
Find average and max length of all links in sitemap
// Check the length, 8 represents len(“https://”)
if 𝑙𝑒𝑛(𝑙𝑖𝑛𝑘) > (𝐿𝑇 + 𝑙𝑒𝑛(𝑑𝑜𝑚_𝑛𝑎𝑚𝑒) + 8) then
Algorithm 1 Dynamic URL Detection
1: 𝑚𝑎𝑥_𝑙𝑒𝑛 ← 0
2: 𝑎𝑣𝑔_𝑙𝑒𝑛 ← 0
3: 𝑙𝑎𝑟𝑔𝑒_𝑙𝑖𝑛𝑘𝑠 ← {}
4:
5: for 𝑙𝑖𝑛𝑘 in 𝑝𝑎𝑔𝑒 do
6:
7:
8:
9:
10: if 𝑙𝑒𝑛(𝑙𝑎𝑟𝑔𝑒_𝑙𝑖𝑛𝑘𝑠) ≤ 𝐿𝐶 then return
11: for 𝑙𝑖𝑛𝑘 in 𝑙𝑎𝑟𝑔𝑒_𝑙𝑖𝑛𝑘𝑠 do
12:
13:
14:
15: 𝑔𝑒𝑛𝑒𝑟𝑎𝑡𝑒𝑑_𝑙𝑖𝑛𝑘 ← “”
16: for 𝑖 = 0...𝑚𝑎𝑥_𝑙𝑒𝑛 do
17:
18: 𝑝𝑟𝑒 𝑓 𝑖𝑥 ← 𝑔𝑒𝑛𝑒𝑟𝑎𝑡𝑒𝑑_𝑙𝑖𝑛𝑘[: 𝑎𝑣𝑔_𝑙𝑒𝑛]
19:
20: 𝑏𝑙𝑜𝑐𝑘𝑙𝑖𝑠𝑡 ← {}
21: 𝑎𝑙𝑙𝑜𝑤𝑙𝑖𝑠𝑡 ← {}
22: do
23:
24:
25:
26:
27:
28:
29: while 𝑙𝑒𝑛(𝑏𝑙𝑜𝑐𝑘𝑙𝑖𝑠𝑡) < 𝑃𝐶 · 𝑙𝑒𝑛(𝑠𝑖𝑡𝑒𝑚𝑎𝑝)
30:
31: // Check the length, 8 represents len(“https://”)
32: if 𝑙𝑒𝑛(𝑝𝑟𝑒 𝑓 𝑖𝑥) < (𝑙𝑒𝑛(𝑑𝑜𝑚𝑎𝑖𝑛_𝑛𝑎𝑚𝑒) + 8 + 𝑆𝑇 ) then
33:
34: else
35:
36:
𝑏𝑙𝑜𝑐𝑘𝑙𝑖𝑠𝑡 .𝑎𝑝𝑝𝑒𝑛𝑑(𝑙𝑖𝑛𝑘)
𝑎𝑙𝑙𝑜𝑤𝑙𝑖𝑠𝑡 .𝑎𝑝𝑝𝑒𝑛𝑑(𝑙𝑖𝑛𝑘)
Write out 𝑎𝑙𝑙𝑜𝑤𝑙𝑖𝑠𝑡 and 𝑏𝑙𝑜𝑐𝑘𝑙𝑖𝑠𝑡
Start procedure again on 𝑎𝑙𝑙𝑜𝑤𝑙𝑖𝑠𝑡
Restart procedure, ignoring links containing 𝑝𝑟𝑒 𝑓 𝑖𝑥
for 𝑙𝑖𝑛𝑘 in 𝑝𝑎𝑔𝑒 do
if 𝑝𝑟𝑒 𝑓 𝑖𝑥 in 𝑙𝑖𝑛𝑘 then
else
𝑝𝑟𝑒 𝑓 𝑖𝑥 ← 𝑝𝑟𝑒 𝑓 𝑖𝑥 [: −1]
Append most occurring character at position 𝑖 to 𝑔𝑒𝑛𝑒𝑟𝑎𝑡𝑒𝑑_𝑙𝑖𝑛𝑘
(2) Short-link threshold (ST): the amount of characters a link
needs to have to be considered short.
(3) Long-link threshold (LT): the amount of characters a link
needs to have to be considered long.
prefixes. Naturally, this approach limits the number of dynamic
orphan pages we can observe. Nevertheless, following our obser-
vation on not putting unnecessary strain on networks and systems,
we consider it crucial to exclude dynamic URLs.
Algorithm 1 describes our heuristic, it has four parameters:
(1) Popularity cutoff (PC): the percentage of URLs on a do-
main that need to contain the prefix.
(4) Long-link cutoff (LC): the amount of links on a domain
that need to be long for the heuristic to run.
We determine these parameters from a random sample of 1,000 do-
mains, taken from our input data (see Section 3.2). We evaluate
the following values: [5%, 10%, 15%, 20%, 25%, 30%] for the popu-
larity cutoff, [5, 10, 15, 20] for the short-link threshold, [20, 25, 30,
35, 40] for the long-link threshold, and [0, 3, 5] for the long-link
cutoff. We optimize for the highest percentage of reduction. After
evaluating all permutations, we obtain 5% for the popularity cutoff,
15 for the short-link threshold, 20 for the long-link threshold, and 0
for the long-link cutoff, as the optimal input parameters. This con-
figuration reduces the number of URLs per domain by 67%, with a
standard deviation of ±30 percentage points and median of 73%.
Optimizing for the highest reduction percentages may filter out
unintentionally orphaned web pages by error. We consciously
choose this optimization to provide a lower bound on the preva-
lence of unintentionally orphaned pages in the wild.
Next, we illustrate how our algorithm removes dynamic URLs
for an example domain, see the set of URLs in Listing 1. Here, we
Session 1A: Cybercrime CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea24https :// example . com / articles /1997/ January / article1 . html
https :// example . com / articles /1997/ February / article1 . html
https :// example . com / articles /2002/ May / article1 . html
https :// example . com / articles /2002/ May / article2 . html
https :// example . com / contact . html
https :// example . com / login . html
Listing 1: URL list before Dynamic URL Detection.
Figure 3: Overview of our implementation.
https :// example . com / contact . html
https :// example . com / login . html
Listing 2: URL list after Dynamic URL Detection.
first check for long links. We continue by counting the character
frequency at each position, followed by generating a URL with at
each position the most encountered character of that position. In
our example, this becomes https://example.com/articles/1997/
May/articlei.htmlhtmll. We then shorten the URL to the average
URL size among the URLs on the domain, which we use as a prefix.
This prefix URL is https://example.com/articles/1997/May/arti
clei. Since no URL matches this prefix, we shorten it one character
at a time until at least 5% match the prefix, or until the prefix is
too short.
In our example 5% corresponds to 0.45 pages, which
is rounded up to 1 page. In practice, this will be a (much) higher
threshold as websites typically have more than nine pages.
We shorten the prefix until it becomes https://example.com/ar
ticles/1997/, which is present in three URLs. We then remove the
URLs containing the prefix and repeat the process on the remain-
ing URLs. The second generated prefix is https://example.com/ar
ticles/2002/May/arti, which does not require further shortening
as it is matched by three of our URLs. These URLs are removed,
after which the algorithm stops since no further long links are
present in our set, resulting in three URLs remaining (see Listing 2).
3.1.4 Probing Liveliness of Candidate Orphan Pages After filtering
the list of potential orphan pages with DUDe, we analyze the pages
by probing them. This allows us to exclude pages that were actu-
ally taken down after their links have been removed. We perform a
HTTP HEAD request to retrieve the HTTP status code of the web
page. We discard pages producing error responses (4xx and 5xx
status codes), as well as any page that does not return the HTTP
status code OK (200).
3.1.5 Open Source Implementation We provide a dockerized open
source implementation of our methodology at https://github.com/