 // Something went wrong in making the call. Try again.
 attempts--
 continue
 }
 return response
 }
 grpclog.Fatalf("ran out of attempts")
}
This system can cascade in the following way:
1. Assume our backend has a known limit of 10,000 QPS per task, after which point 	all further requests are rejected in an attempt at graceful degradation.2. The frontend calls MakeRequest at a constant rate of 10,100 QPS and overloads 	the backend by 100 QPS, which the backend rejects.
3. Those 100 failed QPS are retried in MakeRequest every 1,000 ms, and probably succeed. But the retries are themselves adding to the requests sent to the back‐end, which now receives 10,200 QPS—200 QPS of which are failing due to overload.4. The volume of retries grows: 100 QPS of retries in the first second leads to 200 QPS, then to 300 QPS, and so on. Fewer and fewer requests are able to succeed on their first attempt, so less useful work is being performed as a fraction of requests to the backend.5. If the backend task is unable to handle the increase in load—which is consuming file descriptors, memory, and CPU time on the backend—it can melt down and crash under the sheer load of requests and retries. This crash then redistributes the requests it was receiving across the remaining backend tasks, in turn further overloading those tasks.Some simplifying assumptions were made here to illustrate this scenario,4 but the point remains that retries can destabilize a system. Note that both temporary load spikes and slow increases in usage can cause this effect.
4 An instructive exercise, left for the reader: write a simple simulator and see how the amount of useful work 	the backend can do varies with how much it’s overloaded and how many retries are permitted.Preventing Server Overload  |  269
Even if the rate of calls to MakeRequest decreases to pre-meltdown levels (9,000 QPS, for example), depending on how much returning a failure costs the backend, the problem might not go away. Two factors are at play here:
• If the backend spends a significant amount of resources processing requests that will ultimately fail due to overload, then the retries themselves may be keeping the backend in an overloaded mode.• The backend servers themselves may not be stable. Retries can amplify the effects 	seen in “Server Overload” on page 260.
If either of these conditions is true, in order to dig out of this outage, you must dra‐matically reduce or eliminate the load on the frontends until the retries stop and the backends stabilize.This pattern has contributed to several cascading failures, whether the frontends and backends communicate via RPC messages, the “frontend” is client JavaScript code issuing XmlHttpRequest calls to an endpoint and retries on failure, or the retries orig‐inate from an offline sync protocol that retries aggressively when it encounters a fail‐ure.
When issuing automatic retries, keep in mind the following considerations:• Most of the backend protection strategies described in “Preventing Server Over‐load” on page 265 apply. In particular, testing the system can highlight problems, and graceful degradation can reduce the effect of the retries on the backend.
• Always use randomized exponential backoff when scheduling retries. See also in the AWS Architecture Blog [Bro15]. If retries r the retry window, a small perturbation (e.g., a network blip) can cause retry ripples to schedule at the same time, which can then amplify themselves [Flo94].• Limit retries per request. Don’t retry a given request indefinitely.
• Consider having a server-wide retry budget. For example, only allow 60 retries per minute in a process, and if the retry budget is exceeded, don’t retry; just fail the request. This strategy can contain the retry effect and be the difference between a capacity planning failure that leads to some dropped queries and a global cascading failure.• Think about the service holistically and decide if you really need to perform retries at a given level. In particular, avoid amplifying retries by issuing retries at multiple levels: a single request at the highest layer may produce a number of attempts as large as the product of the number of attempts at each layer to the lowest layer. If the database can’t service requests because it’s overloaded, and the backend, frontend, and JavaScript layers all issue 3 retries (4 attempts), then a270  |  Chapter 22: Addressing Cascading Failures
single user action may create 64 attempts (43) on the database. This behavior is undesirable when the database is returning those errors because it’s overloaded.
• Use clear response codes and consider how different failure modes should be handled. For example, separate retriable and nonretriable error conditions. Don’t retry permanent errors or malformed requests in a client, because neither will ever succeed. Return a specific status when overloaded so that clients and other layers back off and do not retry.In an emergency, it may not be obvious that an outage is due to bad retry behavior. Graphs of retry rates can be an indication of bad retry behavior, but may be confused as a symptom instead of a compounding cause. In terms of mitigation, this is a special case of the insufficient capacity problem, with the additional caveat that you must either fix the retry behavior (usually requiring a code push), reduce load significantly, or cut requests off entirely.Latency and Deadlines
When a frontend sends an RPC to a backend server, the frontend consumes resources waiting for a reply. RPC deadlines define how long a request can wait before the frontend gives up, limiting the time that the backend may consume the frontend’s resources.
Picking a deadlinePicking a deadline
It’s usually wise to set a deadline. Setting either no deadline or an extremely high deadline may cause short-term problems that have long since passed to continue to consume server resources until the server restarts.
High deadlines can result in resource consumption in higher levels of the stack when lower levels of the stack are having problems. Short deadlines can cause some more expensive requests to fail consistently. Balancing these constraints to pick a good deadline can be something of an art.Missing deadlines
A common theme in many cascading outages is that servers spend resources han‐dling requests that will exceed their deadlines on the client. As a result, resources are spent while no progress is made: you don’t get credit for late assignments with RPCs.Suppose an RPC has a 10-second deadline, as set by the client. The server is very overloaded, and as a result, it takes 11 seconds to move from a queue to a thread pool. At this point, the client has already given up on the request. Under most circumstan‐ces, it would be unwise for the server to attempt to handle this request, because it would be doing work for which no credit will be granted—the client doesn’t care whatPreventing Server Overload  |  271
work the server does after the deadline has passed, because it’s given up on the request already.
If handling a request is performed over multiple stages (e.g., there are a few callbacks and RPC calls), the server should check the deadline left at each stage before attempt‐ing to perform any more work on the request. For example, if a request is split into parsing, backend request, and processing stages, it may make sense to check that there is enough time left to handle the request before each stage.Deadline propagation
Rather than inventing a deadline when sending RPCs to backends, servers should employ deadline propagation and cancellation propagation.With deadline propagation, a deadline is set high in the stack (e.g., in the frontend). The tree of RPCs emanating from an initial request will all have the same absolute deadline. For example, if server A selects a 30-second deadline, and processes the request for 7 seconds before sending an RPC to server B, the RPC from A to B will have a 23-second deadline. If server B takes 4 seconds to handle the request and sends an RPC to server C, the RPC from B to C will have a 19-second deadline, and so on. Ideally, each server in the request tree implements deadline propagation.Without deadline propagation, the following scenario may occur:
1. Server A sends an RPC to server B with a 10-second deadline.
2. Server B takes 8 seconds to start processing the request and then sends an RPC to 	server C.
3. If server B uses deadline propagation, it should set a 2-second deadline, but sup‐	pose it instead uses a hardcoded 20-second deadline for the RPC to server C.4. Server C pulls the request off its queue after 5 seconds.
Had server B used deadline propagation, server C could immediately give up on the request because the 2-second deadline was exceeded. However, in this scenario, server C processes the request thinking it has 15 seconds to spare, but is not doing useful work, since the request from server A to server B has already exceeded its deadline.You may want to reduce the outgoing deadline a bit (e.g., a few hundred millisec‐onds) to account for network transit times and post-processing in the client.
Also consider setting an upper bound for outgoing deadlines. You may want to limit how long the server waits for outgoing RPCs to noncritical backends, or for RPCs to backends that typically complete in a short duration. However, be sure to understand your traffic mix, because you might otherwise inadvertently make particular types of272  |  Chapter 22: Addressing Cascading Failures
requests fail all the time (e.g., requests with large payloads, or requests that require responding to a lot of computation).
There are some exceptions for which servers may wish to continue processing a request after the deadline has elapsed. For example, if a server receives a request that involves performing some expensive catchup operation and periodically checkpoints the progress of the catchup, it would be a good idea to check the deadline only after writing the checkpoint, instead of after the expensive operation.Propagating cancellations avoids the potential RPC leakage that occurs if an initial RPC has a long deadline, but RPCs between deeper layers of the stack have short deadlines and time out. Using simple deadline propagation, the initial RPC continues to use server resources until it eventually times out, despite being unable to make progress.
Bimodal latencySuppose that the frontend from the preceding example consists of 10 servers, each with 100 worker threads. This means that the frontend has a total of 1,000 threads of capacity. During usual operation, the frontends perform 1,000 QPS and requests complete in 100 ms. This means that the frontends usually have 100 worker threads occupied out of the 1,000 configured worker threads (1,000 QPS * 0.1 seconds).Suppose an event causes 5% of the requests to never complete. This could be the result of the unavailability of some Bigtable row ranges, which renders the requests corresponding to that Bigtable keyspace unservable. As a result, 5% of the requests hit the deadline, while the remaining 95% of the requests take the usual 100 ms.With a 100-second deadline, 5% of requests would consume 5,000 threads (50 QPS * 100 seconds), but the frontend doesn’t have that many threads available. Assuming no other secondary effects, the frontend will only be able to handle 19.6% of the requests (1,000 threads available / (5,000 + 95) threads’ worth of work), resulting in an 80.4% error rate.Therefore, instead of only 5% of requests receiving an error (those that didn’t com‐plete due to keyspace unavailability), most requests receive an error.
The following guidelines can help address this class of problems:
• Detecting this problem can be very hard. In particular, it may not be clear that bimodal latency is the cause of an outage when you are looking at mean latency. When you see a latency increase, try to look at the distribution of latencies in addition to the averages.• This problem can be avoided if the requests that don’t complete return with an error early, rather than waiting the full deadline. For example, if a backend is unavailable, it’s usually best to immediately return an error for that backend,
Preventing Server Overload  |  273
rather than consuming resources until it the backend available. If your RPC layer supports a fail-fast option, use it.• Having deadlines several orders of magnitude longer than the mean request latency is usually bad. In the preceding example, a small number of requests ini‐tially hit the deadline, but the deadline was three orders of magnitude larger than the normal mean latency, leading to thread exhaustion.• When using shared resources that can be exhausted by some keyspace, consider either limiting in-flight requests by that keyspace or using other kinds of abuse tracking. Suppose your backend processes requests for different clients that have wildly different performance and request characteristics. You might consider only allowing 25% of your threads to be occupied by any one client in order to provide fairness in the face of heavy load by any single client misbehaving.Slow Startup and Cold Caching
Processes are often slower at responding to requests immediately after starting than they will be in steady state. This slowness can be caused by either or both of the fol‐lowing:
Required initialization 
Setting up connections upon receiving the first request that needs a given backend
Runtime performance improvements in some languages, particularly JavaJust-In-Time compilation, hotspot optimization, and deferred class loading
Similarly, some binaries are less efficient when caches aren’t filled. For example, in the case of some of Google’s services, most requests are served out of caches, so requests that miss the cache are significantly more expensive. In steady-state operation with a warm cache, only a few cache misses occur, but when the cache is completely empty, 100% of requests are costly. Other services might employ caches to keep a user’s state in RAM. This might be accomplished through hard or soft stickiness between reverse proxies and service frontends.If the service is not provisioned to handle requests under a cold cache, it’s at greater risk of outages and should take steps to avoid them.
The following scenarios can lead to a cold cache:
Turning up a new cluster 
	A recently added cluster will have an empty cache.
Returning a cluster to service after maintenance 	The cache may be stale.
274  |  Chapter 22: Addressing Cascading FailuresRestarts 
If a task with a cache has recently restarted, filling its cache will take some time. It may be worthwhile to move caching from a server to a separate binary like memcache, which also allows cache sharing between many servers, albeit at the cost of introducing another RPC and slight additional latency.
If caching has a significant effect on the service,5 you may want to use one or some of the following strategies:• Overprovision the service. It’s important to note the distinction between a latency cache versus a capacity cache: when a latency cache is employed, the service can sustain its expected load with an empty cache, but a service using a capacity cache cannot sustain its expected load under an empty cache. Service owners should be vigilant about adding caches to their service, and make sure that any new caches are either latency caches or are sufficiently well engineered to safely function as capacity caches. Sometimes caches are added to a service to improve performance, but actually wind up being hard dependencies.• Employ general cascading failure prevention techniques. In particular, servers should reject requests when they’re overloaded or enter degraded modes, and testing should be performed to see how the service behaves after events such as a large restart.
• When adding load to a cluster, slowly increase the load. The initially small request rate warms up the cache; once the cache is warm, more traffic can be added. It’s a good idea to ensure that all clusters carry nominal load and that the caches are kept warm.Always Go Downward in the Stack
In the example Shakespeare service, the frontend talks to a backend, which in turn talks to the storage layer. A problem that manifests in the storage layer can cause problems for servers that talk to it, but fixing the storage layer will usually repair both the backend and frontend layers.However, suppose the backends cross-communicate amongst each other. For exam‐ple, the backends might proxy requests to one another to change who owns a user when the storage layer can’t service a request. This intra-layer communication can be problematic for several reasons:5 Sometimes you find that a meaningful proportion of your actual serving capacity is as a function of serving from a cache, and if you lost access to that cache, you wouldn’t actually be able to serve that many queries. A similar observation holds for latency: a cache can help you achieve latency goals (by lowering the average response time when the query is servable from cache) that you possibly couldn’t meet without that cache.Slow Startup and Cold Caching  |  275
• The communication is susceptible to a distributed deadlock. Backends may use the same thread pool to wait on RPCs sent to remote backends that are simulta‐neously receiving requests from remote backends. Suppose backend A’s thread pool is full. Backend B sends a request to backend A and uses a thread in backend B until backend A’s thread pool clears. This behavior can cause the thread pool saturation to spread.• If intra-layer communication increases in response to some kind of failure or heavy load condition (e.g., load rebalancing that is more active under high load), intra-layer communication can quickly switch from a low to high intra-layer request mode when the load increases enough.For example, suppose a user has a primary backend and a predetermined hot standby secondary backend in a different cluster that can take over the user. The primary backend proxies requests to the secondary backend as a result of errors from the lower layer or in response to heavy load on the master. If the entire sys‐tem is overloaded, primary to secondary proxying will likely increase and add even more load to the system, due to the additional cost of parsing and waiting on the request to the secondary in the primary.• Depending on the criticality of the cross-layer communication, bootstrapping the 	system may become more complex.
It’s usually better to avoid intra-layer communication—i.e., possible cycles in the communication path—in the user request path. Instead, have the client do the communication. For example, if a frontend talks to a backend but guesses the wrong backend, the backend should not proxy to the correct backend. Instead, the backend should tell the frontend to retry its request on the correct backend.Triggering Conditions for Cascading Failures
When a service is susceptible to cascading failures, there are several possible distur‐bances that can initiate the domino effect. This section identifies some of the factors that trigger cascading failures.
Process DeathProcess Death
Some server tasks may die, reducing the amount of available capacity. Tasks might die because of a Query of Death (an RPC whose contents trigger a failure in the process), cluster issues, assertion failures, or a number of other reasons. A very small event (e.g., a couple of crashes or tasks rescheduled to other machines) may cause a service on the brink of falling to break.276  |  Chapter 22: Addressing Cascading Failures
Process Updates
Pushing a new version of the binary or updating its configuration may initiate a cas‐cading failure if a large number of tasks are affected simultaneously. To prevent this scenario, either account for necessary capacity overhead when setting up the service’s update infrastructure, or push off-peak. Dynamically adjusting the number of in-flight task updates based on the volume of requests and available capacity may be a workable approach.New Rollouts
A new binary, configuration changes, or a change to the underlying infrastructure stack can result in changes to request profiles, resource usage and limits, backends, or a number of other system components that can trigger a cascading failure.
During a cascading failure, it’s usually wise to check for recent changes and consider reverting them, particularly if those changes affected capacity or altered the request profile.Your service should implement some type of change logging, which can help quickly identify recent changes.
Organic Growth
In many cases, a cascading failure isn’t triggered by a specific service change, but because a growth in usage wasn’t accompanied by an adjustment to capacity.
Planned Changes, Drains, or TurndownsIf your service is multihomed, some of your capacity may be unavailable because of maintenance or outages in a cluster. Similarly, one of the service’s critical dependen‐cies may be drained, resulting in a reduction in capacity for the upstream service due to drain dependencies, or an increase in latency due to having to send the requests to a more distant cluster.
Request profile changesA backend service may receive requests from different clusters because a frontend service shifted its traffic due to load balancing configuration changes, changes in the traffic mix, or cluster fullness. Also, the average cost to handle an individual payload may have changed due to frontend code or configuration changes. Similarly, the data handled by the service may have changed organically due to increased or differing usage by existing users: for instance, both the number and size of images, per user, for a photo storage service tend to increase over time.Triggering Conditions for Cascading Failures  |  277
Resource limits
Some cluster operating systems allow resource overcommitment. CPU is a fungible resource; often, some machines have some amount of slack CPU available, which provides a bit of a safety net against CPU spikes. The availability of this slack CPU differs between cells, and also between machines within the cell.Depending upon this slack CPU as your safety net is dangerous. Its availability is entirely dependent on the behavior of the other jobs in the cluster, so it might sud‐denly drop out at any time. For example, if a team starts a MapReduce that consumes a lot of CPU and schedules on many machines, the aggregate amount of slack CPU can suddenly decrease and trigger CPU starvation conditions for unrelated jobs. When performing load tests, make sure that you remain within your committed resource limits.Testing for Cascading Failures
The specific ways in which a service will fail can be very hard to predict from first principles. This section discusses testing strategies that can detect if services are sus‐ceptible to cascading failures.
You should test your service to determine how it behaves under heavy load in order to gain confidence that it won’t enter a cascading failure under various circumstances.Test Until Failure and Beyond
Understanding the behavior of the service under heavy load is perhaps the most important first step in avoiding cascading failures. Knowing how your system behaves when it is overloaded helps to identify what engineering tasks are the most important for long-term fixes; at the very least, this knowledge may help bootstrap the debugging process for on-call engineers when an emergency arises.Load test components until they break. As load increases, a component typically han‐dles requests successfully until it reaches a point at which it can’t handle more requests. At this point, the component should ideally start serving errors or degraded results in response to additional load, but not significantly reduce the rate at which it successfully handles requests. A component that is highly susceptible to a cascading failure will start crashing or serving a very high rate of errors when it becomes over‐loaded; a better designed component will instead be able to reject a few requests and survive.Load testing also reveals where the breaking point is, knowledge that’s fundamental to the capacity planning process. It enables you to test for regressions, provision for worst-case thresholds, and to trade off utilization versus safety margins.
278  |  Chapter 22: Addressing Cascading FailuresBecause of caching effects, gradually ramping up load may yield different results than immediately increasing to expected load levels. Therefore, consider testing both grad‐ual and impulse load patterns.
You should also test and understand how the component behaves as it returns to nominal load after having been pushed well beyond that load. Such testing may answer questions such as:• If a component enters a degraded mode on heavy load, is it capable of exiting the 	degraded mode without human intervention?
• If a couple of servers crash under heavy load, how much does the load need to 	drop in order for the system to stabilize?
If you’re load testing a stateful service or a service that employs caching, your load test should track state between multiple interactions and check correctness at high load, which is often where subtle concurrency bugs hit.Keep in mind that individual components may have different breaking points, so load test each component separately. You won’t know in advance which component may hit the wall first, and you want to know how your system behaves when it does.If you believe your system has proper protections against being overloaded, consider performing failure tests in a small slice of production to find the point at which the components in your system fail under real traffic. These limits may not be adequately reflected by synthetic load test traffic, so real traffic tests may provide more realistic results than load tests, at the risk of causing user-visible pain. Be careful when testing on real traffic: make sure that you have extra capacity available in case your automatic protections don’t work and you need to manually fail over. You might consider some of the following production tests:• Reducing task counts quickly or slowly over time, beyond expected traffic 	patterns
• Rapidly losing a cluster’s worth of capacity
• Blackholing various backends
Test Popular Clients
Understand how large clients use your service. For example, you want to know if clients:
• Can queue work while the service is down
• Use randomized exponential backoff on errors
Testing for Cascading Failures  |  279Testing for Cascading Failures  |  279