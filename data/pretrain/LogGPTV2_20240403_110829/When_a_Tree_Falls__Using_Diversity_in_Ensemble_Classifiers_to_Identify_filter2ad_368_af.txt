### Space Classifications and Evasion Attacks

Certain spaces will be classified as uncertain by the classifier. Our evaluation has shown that the quality of the training set is crucial for detecting evasion attacks. For example, the PDFrate University classifier, which is superior, had significantly fewer evasions compared to the Contagio classifier when subjected to Reverse Mimicry attacks. In the Drebin evaluation, Family R demonstrated strong evasion due to weak features, while Family Q was detectable if included in the training set. This highlights that the effectiveness of mutual agreement analysis depends on adequate coverage in the training set. However, the effectiveness of the training set is directly influenced by the strength of the features. A weak feature set requires a more extensive training set compared to a feature set that closely models fundamental malicious attributes. Operators should ensure that features used for malware detection are not only resistant to spoofing but also based on artifacts caused by malware, rather than being coincidental with current attacks.

### Training Set Secrecy and Resilience

As discussed in Section V-B, keeping the training set of a classifier secret enhances resilience against targeted evasion attempts. It is advisable for operational systems to hide the exact scores returned from their classifiers, as these scores can assist attackers in determining whether their changes help or hinder their evasion efforts. This information could undermine the benefits provided by a secret training set [18].

### Ensemble Classifiers and Evasion Attacks

The GD-KDE attacks on Mimicus demonstrate that some classifiers can make machine learning-based detectors susceptible to evasion attacks. Stochastically generated ensemble classifiers have not been shown to be vulnerable to similar attacks, but new approaches may emerge. The ability to measure mutual agreement in ensemble classifiers comes at little cost but provides practical detection of classifier evasion. This capability is a strong reason to use ensembles in scenarios where classifier evasion is a concern, such as in malware detectors. If mutual agreement is used to optimize classifier training, an attacker might gain more knowledge about additions to the training set than if random selection is used. However, it is unclear how this knowledge could be exploited. Any effective attacks using knowledge of mutual agreement-based training optimization to poison the classifier would be significant.

### Simple vs. Ensemble Classifiers

Some advocate for the use of simple, monolithic classifiers because they are perceived as easier to interpret. For example, Drebin's ability to identify the features contributing to the classification is often praised. However, it is not clear if this information is truly useful to end users. Users are already given the opportunity to review permissions and often choose incorrectly when prompted. Given that URLs and API calls can be socially engineered and that users are generally unaware of these elements, providing these items as context is unlikely to help them make correct decisions. For security professionals, ensemble classifiers offer mechanisms that aid in analysis, such as similarity to existing known malicious or benign samples. Most importantly, the feature set will be useful to a trained analyst.

### Benefits of Mutual Agreement Analysis

Mutual agreement analysis gives operators greater confidence in the accuracy of the classifier and the ability to prioritize responses to alerts. Some operators use ensemble classifier introspection to adjust the voting threshold. Environments that aim to avoid false negatives (evasion attacks) will use a low threshold, increasing the number of false positives. Conversely, some environments might use a higher-than-normal voting threshold to achieve a low false positive rate but potentially a higher false negative rate, as seen in antivirus engines.

Operators benefit most from mutual agreement analysis when uncertain observations are subjected to focused analysis. These samples must be subjected to different and complementary analyses or detections. Since the number of uncertain observations is low for a well-performing classifier, this second opinion can be relatively expensive, possibly involving manual or dynamic analysis. Ensemble diversity-based confidence estimates are useful for organizations that want to identify novel attacks for additional analysis. While possibly unconventional in the machine learning field, the addition of the uncertain outcome is intuitive in the security field, where many systems provide adjudications only for known observations, whether benign or malicious. For example, SPAM filters often use a quarantine for samples that cannot be reliably classified. High-fidelity alerts are often preferred over a response for every observation.

### Identifying Novel Samples

Mutual agreement analysis is highly effective at identifying samples that are dissimilar to those in the training set. Adding uncertain samples to the classifier dramatically improves its accuracy, motivating operators to improve the classifier more effectively than through random additions to the training set.

### Future Directions

Evaluating machine learning-based detectors might be improved through the application of mutual agreement analysis. A concise metric is the Uncertain Rate, or the portion of observations for which a classifier is poorly suited to provide a prediction. The effectiveness of classifier evaluation using the mutual agreement score distribution and variance could be a topic for future studies. The classifier score distributions shown in Figures 12 and 13 suggest that regression could predict the amount of successful evasions. The challenge in this type of analysis is separating the arcs for benign and malicious data when external ground truth is not provided.

### Conclusion

Monitoring mutual agreement in ensemble classifiers raises the bar for evasion, both for previously unseen attacks and targeted mimicry attacks. Contemporary evasion attacks, which have questioned the resiliency of learning-based detectors, are shown to be weaker than previously thought. Simply obfuscating attacks so they no longer appear as known attacks is insufficient. Successful mimicries must closely mirror benign samples. Future research into the degree to which mutual agreement analysis can improve attack quality is worthwhile.

### Conclusions

We introduce a new technique to detect malware classifier performance degradation on individual observations. Mutual agreement analysis relies on diversity in ensemble classifiers to produce an estimate of classifier confidence. We evaluate our approach using over 100,000 PDF documents and 100,000 Android applications. Applying PDFrate to documents taken from a real network, we find that the number of uncertain outcomes is smallâ€”only 0.2%. Excluding these uncertain examples, the true positive rate rises from 95% to 100%, and the false positive rate drops from 0.053% to 0.0050%. Furthermore, mutual agreement analysis is effective at identifying the samples to be added to the training set, resulting in more rapid improvements in classifier performance than random sampling. In direct evasion attacks against PDFrate and novel attacks against Drebin, the majority of the observations are assigned the outcome of uncertain, notifying the operator of the detector failure. While evasion attacks are still possible, they require more complete mimicry across the entire feature set.

We believe that mutual agreement analysis can be generally applied to ensemble classifiers. We find that feature bagging is critical to diversity-based evasion detection. The GD-KDE attack, which has been successfully employed against Support Vector Machines, can be thwarted by an SVM ensemble. Ensemble classifier mutual agreement analysis provides a critical mechanism to evaluate the accuracy of machine learning-based detectors without using external validation.

### Acknowledgments

The authors would like to thank all the reviewers for their valuable comments and suggestions. This work is supported by the National Science Foundation Grant No. CNS 1421747 and II-NEW 1205453. Opinions, findings, conclusions, and recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF or US Government.

### References

[References listed here, formatted consistently and updated as needed]