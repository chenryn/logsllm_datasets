streams,theloganalysisworkflowfirstdoessomepreprocess- robustness of the parsing methods.
ing of the logs to make them amenable to ML models. This
is typically followed by log parsing which extracts a loose iii) Log Partitioning: After parsing the next step is to
structure from the semi-structured data and then grouping and partition the log data into groups, based on some semantics
partitioning the log lines into log sequences in order to model where each group represents a finite chunk of log lines or
the sequence characteristics of the data. After this, the logs or log sequences. The main purpose behind this is to decompose
log sequences are represented as a machine-readable matrix the original log dump typically consisting of millions of log
on which various log analysis tasks can be performed - like lines into logical chunks, so as to enable explicit modeling
clusteringandsummarizingthehugelogdumpsintoafewkey on these chunks and allow the models to capture anomaly
log patterns for easy visualization or for detecting anomalous patterns over sequences of log templates or log parameter
log patterns that can be indicative of an incident. Figure 7 valuesorboth.Logpartitioningcanbeofdifferentkinds[20],
provides an outline of the different steps in the log analysis [80] - Fixed or Sliding window based partitions, where the
wokflow. While some of these steps are more of engineering length of window is determined by length of log sequence or
challenges, others are more AI-driven and some even employ aperiodoftime,andIdentifierbasedpartitionswherelogsare
a combination of machine learning and domain knowledge partitionedbasedonsomeidentifier(e.g.thesessionorprocess
rules. theyoriginatefrom).Figure9illustratesthesedifferentchoices
of log grouping and partitioning. A log event is eventually
i) Log Preprocessing: This step typically involves cus- deemed to be anomalous or not, either at the level of a log
tomised filtering of specific regular expression patterns (like line or a log partition.
IP addresses or memory locations) that are deemed irrelevant
for the actual log analysis. Other preprocessing steps like iv)LogRepresentation:Afterlogpartitioning,thenextstep
tokenizationrequiresspecializedhandlingofdifferentwording istorepresenteachpartitioninamachine-readableway(e.g.a
styles and patterns arising due to the hybrid nature of logs vectororamatrix)byextractingfeaturesfromthem.Thiscan
consistingofbothnaturallanguageandprogramminglanguage bedoneinvariousways[81],[80]-eitherbyextractingspecific
constructs. For example a log line can contain a mix of handcrafted features using domain knowledge or through ii)
text strings from source-code data having snake-case and sequential representation which converts each partition to an
camelCase tokens along with white-spaced tokens in natural ordered sequence of log event ids ii) quantitative represen-
language. tation which uses count vectors, weighted by the term and
inverse document frequency information of the log events
ii)LogParsing:Toenabledownstreamprocessing,unstruc- iii) semantic representation captures the linguistic meaning
tured log messages first need to be parsed into a structured from the sequence of language tokens in the log events and
event template (i.e. constant part that was actually designed learns a high-dimensional embedding vector for each token
by the developers) and parameters (i.e. variable part which in the dataset. The nature of log representation chosen has
contain the dynamic runtime information). Figure 8 provides direct consequence in terms of which patterns of anomalies
one such example of parsing a single log line. In literature they can support - for example, for capturing keyword based
there have been heuristic methods for parsing as well as AI- anomalies, semantic representation might be key, while for
driven methods which include traditional ML and also more anomalies related to template count and variable distribution,
recent neural models. The heuristic methods like Drain [63], quantitativerepresentationsarepossiblymoreappropriate.The
IPLoM[64]andAEL[65]exploitknowninductivebiasonlog semanticembeddingvectorsthemselvescanbeeitherobtained
structure while Spell [66] uses Longest common subsequence usingpretrainedneurallanguagemodelslikeGloVe,FastText,
algorithm to dynamically extract log patterms. Out of these, pretrained Transformer like BERT, RoBERTa etc or learnt
Drain and Spell are most popular, as they scale well to using a trainable embedding layer as part of the target task.
industrial standards. Amongst the traditional ML methods,
there are i) Clustering based methods like LogCluster [67], v) Log Analysis tasks for Incident Detection: Once the logs
LKE [68], LogSig [69], SHISO [70], LenMa [71], LogMine are represented in some compact machine-interpretable form
[72] which assume that log message types coincide in similar which can be easily ingested by AI models, a pipeline of
groups ii) Frequent pattern mining and item-set mining meth- log analysis tasks can be performed on it - starting with Log
ods SLCT [73], LFA [74] to extract common message types compression techniques using Clustering and Summarization,
iii)EvolutionaryoptimizationapproacheslikeMoLFI[75].On followed by Log based Anomaly Detection. In turn, anomaly
the other hand, recent neural methods include [76] - Neural detection can further enable downstream tasks in Incident
10
Fig.7. StepsoftheLogAnalysisWorkflowforIncidentDetection
clustering and partitioning techniques that support online and
streaming logs and can also handle clustering of rare log
instances. Another area of existing literature [88], [89], [90],
[91]focusonlogcompressionthroughsummarization-where,
for example, [88] uses heuristics like log event ids and
timings to summarize and [89], [21] does openIE based triple
extraction using semantic information and domain knowledge
andrulestogeneratesummaries,while[90],[91]usesequence
Fig.8. ExampleofLogParsing
clustering using linguistic rules or through grouping common
event sequences.
v.2)LogAnomalyDetection:Perhapsthemostcommonuse
of log analysis is for log based anomaly detection where a
wide variety of models have been employed in both research
and industrial settings. These models are categorized based
on various factors i) the learning setting - supervised, semi-
supervisedorunsupervised:Whilethesemi-supervisedmodels
assumepartialknowledgeoflabelsoraccesstofewanomalous
instances, unsupervised ones train on normal log data and
detect anomaly based on their prediction confidence. ii) the
type of Model - Neural or traditional statistical non-neural
Fig.9. Differenttypesoflogpartitioning models iii) the kinds of log representations used iv) Whether
to use log parsing or parser free methods v) If using parsing,
then whether to encode only the log template part or both
ManagementlikeFailurePredictionandRootCauseAnalysis.
template and parameter representations iv) Whether to restrict
In this section we discuss only the first two log analysis tasks
modeling of anomalies at the level of individual log lines or
which are pertinent to incident detection and leave failure
to support sequential modeling of anomaly detection over log
prediction and RCA for the subsequent sections.
sequences.
v.1) Log Compression through Clustering & Summariza- The nature of log representation employed and the kind
tion: This is a practical first-step towards analyzing the huge of modeling used - both of these factors influence what type
of anomaly patterns can be detected - for example keyword
volumes of log data is Log Compression through various
and variable value based anomalies are captured by semantic
clustering and summarization techniques. The objective of
representation of log lines, while template count and vari-
this analysis serves two purposes - Firstly, this step can
able distribution based anomaly patterns are more explicitly
independently help the site reliability engineers and service
modeled through quantitative representations of log events.
owners during incident management by providing a practical
Similarlytemplatesequenceandtime-intervalbasedanomalies
and intuitive way of visualizing these massive volumes of
need sequential modeling algorithms which can handle log
complex unstructured raw log data. Secondly, the output of
sequences.
log clustering can directly be leveraged in some of the log
based anomaly detection methods. Belowwebrieflysummarizethebodyofliteraturededicated
Amongst the various techniques of log clustering, [82], to these two types of models - Statistical and Neural; and In
[67], [83] employ hierarchical clustering and can support Table III we provide a comparison of a more comprehensive
onlinesettingsbyconstructingandretrievingfromknowledge list of existing anomaly detection algorithms and systems.
base of representative log clusters. [84], [85] use frequent StatisticalModelsarethemoretraditionalmachinelearning
pattern matching with dimension reduction techniques like models which draw inference from various statistics under-
PCA and locally sensitive hashing with online and streaming lying the training data. In the literature there have been
support. [86], [64], [87] uses efficient iterative or incremental various statistical ML models employed for this task under
11
different training settings. Amongst the supervised methods, vi) Log Model Deployment: The final step in the log
[92], [93], [94] using traditional learning strategies of Lin- analysisworkflowisdeploymentofthesemodelsintheactual
ear Regression, SVM, Decision Trees, Isolation Forest with industrial settings. It involves i) a training step, typically
handcrafted features extracted from the entire logline. Most over offline log data dump, with or without some supervision
ofthesemodelthedataatthelevelofindividuallog-linesand labels collected from domain experts ii) online inference step,
cannot not explicitly capture sequence level anomalies. There which often needs to handle practical challenges like non-
are also unsupervised methods like ii) dimension reduction stationary streaming data i.e. where the data distribution is
techniques like Principal Component Analysis (PCA) [84] not independently and identically distributed throughout the
iii) clustering and drawing correlations between log events time. For tackling this, some of the more traditional statistical
and metric data as in [67], [82], [95], [80]. There are also methods like [103], [95], [82], [84] support online streaming
unsupervised pattern mining methods which include mining update while some other works can also adapt to evolving
invariant patterns from singular value decomposition [96] and log data by incrementally building a knowledge base or
mining frequent patterns from execution flow and control memory or out-of-domain vocabulary [101]. On the other
flow graphs [97], [98], [99], [68]. Apart from these there are hand most of the unsupervised models support syncopated
also systems which employ a rule engine built using domain batched online training, allowing the model to continually
knowledge and an ensemble of different ML models to cater adapt to changing data distributions and to be deployed on
to different incident types [20] and also heuristic methods high throughput streaming data sources. However for some of
for doing contrast analysis between normal and incident- the more advanced neural models, the online updation might
indicating abnormal logs [100]. be too computationally expensive even for regular batched
NeuralModels,ontheotherhandareamorerecentclassof updates.
machine learning models which use artificial neural networks Apart from these, there have also been specific work on
and have proven remarkably successful across numerous AI other challenges related to model deployment in practical
applications. They are particularly powerful in encoding and settings like transfer learning across logs from different do-
representing the complex semantics underlying in a way that mains or applications [110], [103], [18], [18], [118] under
is meaningful for the predictive task. One class of unsuper- semi-supervised settings using only supervision from source
vised neural models use reconstruction based self-supervised systems. Other works focus on evaluating model robustness
techniques to learn the token or line level representation, and generalization (i.e. how well the model adapts to) to
which includes i) Autoencoder models [101], [102] ii) more unstable log data due to continuous logging modifications
powerful self-attention based Transformer models [103] iv) throughout software evolutions and updates [109], [111],
specific pretrained Transformers like BERT language model [104]. They achieve these by adopting domain adversarial
[104], [105], [21]. Another offshoot of reconstruction based paradigms during training [18], [18] or using counterfactual
modelsisthoseusinggenerativeadversarialorGANparadigm explanations [118] or multi-task settings [21] over various log
of training for e.g. [106], [107] using LSTM or Transformer analysis tasks.
based encoding. The other types of unsupervised models
are forecasting based, which learn to predict the next log Challenges & Future Trends
token or next log line in a self-supervised way - for e.g i)
Collecting supervision labels: Like most AIOps tasks,
Recurrent Neural Network based models like LSTM [108],
collecting large-scale supervision labels for training or even
[109], [110], [18], [111] and GRU [104] or their attention
evaluation of log analysis problems is very challenging and
based counterparts [81], [112], [113] ii) Convolutional Neural
impractical as it involves significant amount of manual inter-
Network(CNN)basedmodels[114]ormorecomplexmodels
vention and domain knowledge. For log anomaly detection,
which use Graph Neural Network to represent log event data
the goal being quite objective, label collection is still possible
[115],[116].Bothreconstructionandforecastingbasedmodels
to enable atleast a reliable evaluation. Whereas, for other log
are capable of handling sequence level anomalies, it depends
analysis tasks like clustering and summarization, collecting
onthenatureoftraining(i.e.whetherrepresentationsarelearnt
supervision labels from domain experts is often not even
at log line or token level) and the capacity of model to handle
possible as the goal is quite subjective and hence these tasks
long sequences (e.g. amongst the above, Autoencoder models
are typically evaluated through the downstream log analysis
are the most basic ones).
or RCA task.
Most of these models follow the practical setup of unsu-
pervised training, where they train only non-anomalous log Imbalanced class problem: One of the key challenges
data. However, other works have also focused on supervised of anomaly detection tasks, is the class imbalance, stemming
trainingofLSTM,CNNandTransformermodels[111],[114], from the fact that anomalous data is inherently extremely
[78], [117], over anomalous and normal labeled data. On rare in occurrence. Additionally, various systems may show
the other hand, [104], [110] use weak supervision based on different kinds of data skewness owing to the diverse kinds
heuristic assumptions for e.g. logs from external systems are of anomalies listed above. This poses a technical challenge
consideredanomalous.Mostoftheneuralmodelsusesemantic both during model training with highly skewed data as well
token representations, some with pretrained fixed or trainable as choice of evaluation metrics, as Precision, Recall and F-
embeddings, initialized with GloVe, fastText or pretrained Score may not perform satisfactorily. Further at inference,