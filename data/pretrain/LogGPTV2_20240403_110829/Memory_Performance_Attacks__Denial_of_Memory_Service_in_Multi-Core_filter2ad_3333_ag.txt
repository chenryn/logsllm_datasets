the memory system, which improves the performance of
both memory-intensive and non-memory-intensive ap-
plications that are unfairly delayed by an MPH.
7 Related Work
The possibility of exploiting vulnerabilities in the soft-
ware system to deny memory allocation to other appli-
cations has been considered in a number of works. For
13Notice that 8p-MIX2 and 8p-MIX3 are much less memory inten-
sive than 8p-MIX1. Due to this, their baseline overall throughput is
signiﬁcantly higher than 8p-MIX1 as shown in Table 7.
example, [37] describes an attack in which one process
continuously allocates virtual memory and causes other
processes on the same machine to run out of memory
space because swap space on disk is exhausted. The
“memory performance attack” we present in this paper
is conceptually very different from such “memory allo-
cation attacks” because (1) it exploits vulnerabilities in
the hardware system, (2) it is not amenable to software
solutions — the hardware algorithms must be modiﬁed
to mitigate the impact of attacks, and (3) it can be caused
even unintentionally by well-written, non-malicious but
memory-intensive applications.
There are only few research papers that consider hard-
ware security issues in computer architecture. Woo and
Lee [38] describe similar shared-resource attacks that
were developed concurrently with this work, but they do
not show that the attacks are effective in real multi-core
systems. In their work, a malicious thread tries to dis-
place the data of another thread from the shared caches or
to saturate the on-chip or off-chip bandwidth. In contrast,
our attack exploits the unfairness in the DRAM memory
scheduling algorithms; hence their attacks and ours are
complementary.
Grunwald and Ghiasi [12] investigate the possibility of
microarchitectural denial of service attacks in SMT (si-
multaneous multithreading) processors. They show that
SMT processors exhibit a number of vulnerabilities that
could be exploited by malicious threads. More specif-
ically, they study a number of DoS attacks that affect
caching behavior, including one that uses self-modifying
272
16th USENIX Security Symposium
USENIX Association
i
e
m
T
n
o
i
t
u
c
e
x
E
d
e
z
i
l
a
m
r
o
N
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
8p-MIX1
stream-1
stream-2
art-1
art-2
mcf-1
mcf-2
health-1
health-2
i
e
m
T
n
o
i
t
u
c
e
x
E
d
e
z
i
l
a
m
r
o
N
8p-MIX2
stream
small-stream
rdarray
art
vpr
mcf
health
crafty
i
e
m
T
n
o
i
t
u
c
e
x
E
d
e
z
i
l
a
m
r
o
N
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
8p-MIX3
small-stream
art
mcf
health
vpr-1
vpr-2
crafty-1
crafty-2
FR-FCFS
Figure 13: Effect of FR-FCFS and FairMem scheduling on different application mixes in an 8-core system
FR-FCFS
FR-FCFS
FairMem
FairMem
FairMem
4p-MIX1 4p-MIX2 4p-MIX3
8p-MIX1 8p-MIX2 8p-MIX3
base throughput (IPTC)
1793
FairMem throughput (IPTC)
2809
base unfairness (Ψ)
10.11
FairMem unfairness (Ψ)
1.32
FairMem throughput improvement
1.57X
FairMem fairness improvement
7.66X
Table 7: Throughput and fairness statistics for 4-core and 8-core systems
625
1233
13.56
1.34
1.97X
10.11X
156
338
8.71
1.32
2.17X
6.60X
163
234
10.98
1.21
1.44X
9.07X
131
189
7.89
1.18
1.44X
6.69X
107
179
8.05
1.09
1.67X
7.39X
code to cause the trace cache to be ﬂushed. The authors
then propose counter-measures that ensure fair pipeline
utilization. The work of Hasan et al. [13] studies in a sim-
ulator the possibility of so-called heat stroke attacks that
repeatedly access a shared resource to create a hot spot at
the resource, thus slowing down the SMT pipeline. The
authors propose a solution that selectively slows down
malicious threads. These two papers present involved
ways of “hacking” existing systems using sophisticated
techniques such as self-modifying code or identifying
on-chip hardware resources that can heat up. In contrast,
our paper describes a more prevalent problem: a triv-
ial type of attack that could be easily developed by any-
one who writes a program. In fact, even existing simple
applications may behave like memory performance hogs
and future multi-core systems are bound to become even
more vulnerable to MPHs.
In addition, neither of the
above works consider vulnerabilities in shared DRAM
memory in multi-core architectures.
The FR-FCFS scheduling algorithm implemented in
many current single-core and multi-core systems was
studied in [30, 29, 15, 23], and its best implementation—
the one we presented in Section 2—is due to Rixner
et al [30]. This algorithm was initially developed for
single-threaded applications and shows good through-
put performance in such scenarios. As shown in [23],
however, it can have negative effects on fairness in chip-
multiprocessor systems. The performance impact of dif-
ferent memory scheduling techniques in SMT processors
and multiprocessors has been considered in [42, 22].
Fairness issues in managing access to shared resources
have been studied in a variety of contexts. Network fair
queuing has been studied in order to offer guaranteed ser-
vice to simultaneous ﬂows over a shared network link,
e.g., [24, 40, 3], and techniques from network fair queu-
ing have since been applied in numerous ﬁelds, e.g., CPU
scheduling [6]. The best currently known algorithm for
network fair scheduling that also effectively solves the
idleness problem was proposed in [2]. In [23], Nesbit et
al. propose a fair memory scheduler that uses the def-
inition of fairness in network queuing and is based on
techniques from [3, 40]. As we pointed out in Section 4,
directly mapping the deﬁnitions and techniques from net-
work fair queuing to DRAM memory scheduling is prob-
lematic. Also, the scheduling algorithm in [23] can sig-
niﬁcantly suffer from the idleness problem. Fairness in
disk scheduling has been studied in [4, 26]. The tech-
niques used to achieve fairness in disk access are highly
inﬂuenced by the physical association of data on the disk
(cylinders, tracks, sectors, etc.) and can therefore not di-
rectly be applied in DRAM scheduling.
Shared hardware caches in multi-core systems have
been studied extensively in recent years, e.g. in [35, 19,
14, 28, 9]. Suh et al. [35] and Kim et al. [19] develop
hardware techniques to provide thread-fairness in shared
caches. Fedorova et al. [9] and Suh et al. [35] propose
modiﬁcations to the operating system scheduler to allow
each thread its fair share of the cache. These solutions do
not directly apply to DRAM memory controllers. How-
ever, the solution we examine in this paper has interac-
tions with both the operating system scheduler and the
fairness mechanisms used in shared caches, which we
intend to examine in future work.
8 Conclusion
The advent of multi-core architectures has spurred a lot
of excitement in recent years. It is widely regarded as the
most promising direction towards increasing computer
performance in the current era of power-consumption-
limited processor design. In this paper, we show that this
development—besides posing numerous challenges in
ﬁelds like computer architecture, software engineering,
or operating systems—bears important security risks.
In particular, we have shown that due to unfairness in
the memory system of multi-core architectures, some ap-
USENIX Association
16th USENIX Security Symposium
273
plications can act as memory performance hogs and de-
stroy the memory-related performance of other applica-
tions that run on different processors in the chip; with-
out even being signiﬁcantly slowed down themselves. In
order to contain the potential of such attacks, we have
proposed a memory request scheduling algorithm whose
design is based on our novel deﬁnition of DRAM fair-
ness. As the number of processors integrated on a single
chip increases, and as multi-chip architectures become
ubiquitous, the danger of memory performance hogs is
bound to aggravate in the future and more sophisticated
solutions may be required. We hope that this paper helps
in raising awareness of the security issues involved in the
rapid shift towards ever-larger multi-core architectures.
Acknowledgments
We especially thank Burton Smith for continued inspir-
ing discussions on this work. We also thank Hyesoon
Kim, Chris Brumme, Mark Oskin, Rich Draves, Trishul
Chilimbi, Dan Simon, John Dunagan, Yi-Min Wang, and
the anonymous reviewers for their comments and sug-
gestions on earlier drafts of this paper.
References
AMD Opteron.
[1] Advanced Micro Devices.
http://www.amd.com/us-en/Processors/
ProductInformation/.
[2] J. H. Anderson, A. Block, and A. Srinivasan. Quick-
release fair scheduling. In RTSS, 2003.
[3] J. C. Bennett and H. Zhang. Hierarchical packet fair
queueing algorithms. In SIGCOMM, 1996.
[4] J. Bruno et al. Disk scheduling with quality of service
guarantees. In Proceedings of IEEE Conference on Mul-
timedia Computing and Systems, 1999.
[5] A. Chander, J. C. Mitchell, and I. Shin. Mobile code se-
curity by Java bytecode instrumentation. In DARPA In-
formation Survivability Conference & Exposition, 2001.
[6] A. Chandra, M. Adler, P. Goyal, and P. Shenoy. Surplus
fair scheduling: A proportional-share CPU scheduling al-
gorithm for symmetric multiprocessors. In OSDI-4, 2000.
[7] R. S. Cox, J. G. Hansen, S. D. Gribble, and H. M. Levy.
A safety-oriented platform for web applications. In IEEE
Symposium on Security and Privacy, 2006.
[8] V. Cuppu, B. Jacob, B. Davis, and T. Mudge. A per-
formance comparison of contemporary DRAM architec-
tures. In ISCA-26, 1999.
[9] A. Fedorova, M. Seltzer, and M. D. Smith. Cache-fair
thread scheduling for multi-core processors. Technical
Report TR-17-06, Harvard University, Oct. 2006.
[10] T. Garﬁnkel, B. Pfaff, J. Chow, M. Rosenblum, and
D. Boneh. Terra: A virtual machine-based platform for
trusted computing. In SOSP, 2003.
[11] S. Gochman et al. The Intel Pentium M processor: Mi-
croarchitecture and performance. Intel Technology Jour-
nal, 7(2), May 2003.
[12] D. Grunwald and S. Ghiasi. Microarchitectural denial of
service: Insuring microarchitectural fairness. In MICRO-
35, 2002.
[13] J. Hasan et al. Heat stroke: power-density-based denial
of service in SMT. In HPCA-11, 2005.
[14] L. R. Hsu, S. K. Reinhardt, R. Iyer, and S. Makineni.
Communist, utilitarian, and capitalist cache policies on
CMPs: Caches as a shared resource. In PACT-15, 2006.
[15] I. Hur and C. Lin. Adaptive history-based memory sched-
ulers. In MICRO-37, 2004.
Terascale computing.
http:
[16] Intel Corporation.
Intel Develops Tera-Scale Research
http://www.intel.com/pressroom/
[17] Intel Corporation. Pentium D. http://www.intel.
Chips.
archive/releases/20060926corp b.htm.
com/products/processor number/chart/
pentium d.htm.
[18] Intel Corporation.
//www.intel.com/research/platform/
terascale/index.htm.
[19] S. Kim, D. Chandra, and Y. Solihin. Fair cache shar-
ing and partitioning in a chip multiprocessor architecture.
PACT-13, 2004.
[20] C. K. Luk et al. Pin: building customized program analy-
sis tools with dynamic instrumentation. In PLDI, 2005.
[21] J. D. McCalpin. STREAM: Sustainable memory band-
width in high performance computers. http://www.
cs.virginia.edu/stream/.
[22] C. Natarajan, B. Christenson, and F. Briggs. A study
of performance impact of memory controller features in
multi-processor server environment. In WMPI, 2004.
[23] K. J. Nesbit, N. Aggarwal, J. Laudon, and J. E. Smith.
Fair queuing memory systems. In MICRO-39, 2006.
[24] A. K. Parekh. A Generalized Processor Sharing Approach
to Flow Control in Integrated Service Networks. PhD the-
sis, MIT, 1992.
[25] D. Peterson, M. Bishop, and R. Pandey. A ﬂexible con-
tainment mechanism for executing untrusted code.
In
11th USENIX Security Symposium, 2002.
[26] T. Pradhan and J. Haritsa. Efﬁcient fair disk schedulers.
In 3rd Conference on Advanced Computing, 1995.
[27] V. Prevelakis and D. Spinellis. Sandboxing applications.
In USENIX 2001 Technical Conf.: FreeNIX Track, 2001.
[28] N. Raﬁque et al. Architectural support for operating
In PACT-15,
system-driven CMP cache management.
2006.
[29] S. Rixner. Memory controller optimizations for web
servers. In MICRO-37, 2004.
[30] S. Rixner, W. J. Dally, U. J. Kapasi, P. Mattson, and J. D.
Owens. Memory access scheduling. In ISCA-27, 2000.
[31] A. Rogers, M. C. Carlisle, J. Reppy, and L. Hendren.
Supporting dynamic data structures on distributed mem-
ory machines. ACM Transactions on Programming Lan-
guages and Systems, 17(2):233–263, Mar. 1995.
[32] T. Sherwood et al. Automatically characterizing large
scale program behavior. In ASPLOS-X, 2002.
[33] E. Sprangle and O. Mutlu. Method and apparatus to con-
trol memory accesses. U.S. Patent 6,799,257, 2004.
SPEC
[34] Standard Performance Evaluation Corporation.
CPU2000. http://www.spec.org/cpu2000/.
[35] G. E. Suh, S. Devadas, and L. Rudolph. A new memory
monitoring scheme for memory-aware scheduling and
partitioning. HPCA-8, 2002.
[36] D. Wang et al. DRAMsim: A memory system simulator.
Computer Architecture News, 33(4):100–107, 2005.
[37] Y.-M. Wang et al. Checkpointing and its applications. In
FTCS-25, 1995.
[38] D. H. Woo and H.-H. S. Lee. Analyzing performance
vulnerability due to resource denial of service attack on
chip multiprocessors. In Workshop on Chip Multiproces-
sor Memory Systems and Interconnects, Feb. 2007.
[39] W. Wulf and S. McKee. Hitting the memory wall: Im-
plications of the obvious. ACM Computer Architecture
News, 23(1), 1995.
[40] H. Zhang. Service disciplines for guaranteed performance
service in packet-switching networks. In Proceedings of
the IEEE, 1995.
[41] Z. Zhang, Z. Zhu, and X. Zhang. A permutation-based
page interleaving scheme to reduce row-buffer conﬂicts
and exploit data locality. In MICRO-33, 2000.
[42] Z. Zhu and Z. Zhang. A performance comparison of
DRAM memory system optimizations for SMT proces-
sors. In HPCA-11, 2005.
274
16th USENIX Security Symposium
USENIX Association