many non-ML codebases are accompanied by extensive suites
of coverage and fail-over tests, the test cases for the popular
PyTorch repositories mentioned above only assert the shape
of the loss, not the values. When models are trained on GPUs,
the results depend on the hardware and OS randomness and
are thus difï¬cult to test.
Recently proposed techniques [12, 95] aim to â€œverifyâ€
trained models but they are inherently different from tradi-
tional unit tests and not intended for users who train locally
on trusted data. Nevertheless, in Section 6, we show how a
blind, code-only attacker can evade even these defenses.
3.2 Attackerâ€™s capabilities
We assume that the attacker compromises the code that com-
putes the loss value in some ML codebase. The attacker knows
the task, possible model architectures, and general data do-
main, but not the speciï¬c training data, nor the training hyper-
parameters, nor the resulting model. Figures 3 and 4 illustrate
this attack. The attack leaves all other parts of the codebase un-
changed, including the optimizer used to update the modelâ€™s
weights, loss criterion, model architecture, hyperparameters
such as the learning rate, etc.
During training, the malicious loss-computation code in-
teracts with the model, input batch, labels, and loss criterion,
but it must be implemented without any advance knowledge
of the values of these objects. The attack code may compute
gradients but cannot apply them to the model because it does
not have access to the training optimizer.
3.3 Backdoors as multi-task learning
Our key technical innovation is to view backdoors through
the lens of multi-objective optimization.
2See examples in https://git.io/JJmRM (fairseq) or https://git.
io/JJmRP (transformers).
Figure 3: Malicious code modiï¬es the loss value.
In conventional multi-task learning [73], the model consists
of a common shared base Î¸sh and separate output layers Î¸k
for every task k. Each training input x is assigned multiple
labels y1, . . .yk, and the model produces k outputs Î¸k(Î¸sh(x)).
By contrast, a backdoor attacker aims to train the same
model, with a single output layer, for two tasks simultane-
ously: the main task m and the backdoor task mâˆ—. This is
challenging in the blind attack scenario. First, the attacker
cannot combine the two learning objectives into a single loss
function via a ï¬xed linear combination, as in [3], because
the coefï¬cients are data- and model-dependent and cannot be
determined in advance. Second, there is no ï¬xed combination
that yields an optimal model for the conï¬‚icting objectives.
Blind loss computation.
In supervised learning, the loss
value (cid:96) = L(Î¸(x),y) compares the modelâ€™s prediction Î¸(x)
on a labeled input (x,y) with the correct label y using some
criterion L. In a blind attack, the loss for the main task m
is computed as usual, (cid:96)m = L(Î¸(x),y). Additionally, the at-
tackerâ€™s code synthesizes backdoor inputs and their labels to
obtain (xâˆ—,yâˆ—) and computes the loss for the backdoor task
mâˆ—: (cid:96)mâˆ— = L(Î¸(xâˆ—),yâˆ—).
The overall loss (cid:96)blind is a linear combination of the main-
task loss (cid:96)m, backdoor loss (cid:96)mâˆ—, and optional evasion loss (cid:96)ev:
(cid:96)blind = Î±0(cid:96)m + Î±1(cid:96)mâˆ— [ +Î±2(cid:96)ev ]
(1)
This computation is blind: backdoor transformations Âµ and
Î½ are generic functions, independent of the concrete train-
ing data or model weights. We use multi-objective optimiza-
tion to discover the optimal coefï¬cients at runtimeâ€”see Sec-
tion 3.4. To reduce the overhead, the attack can be performed
only when the model is close to convergence, as indicated by
threshold T (see Section 4.6).
Backdoors. In universal image-classiï¬cation backdoors [28,
55], the trigger feature is a pixel pattern t and all images with
this pattern are classiï¬ed to the same class c. To synthesize
such a backdoor input during training or at inference time, Âµ
simply overlays the pattern t over input x, i.e., Âµ(x) = xâŠ• t.
The corresponding label is always c, i.e., Î½(y) = c.
Our approach also supports complex backdoors by allowing
complex synthesizers Î½. During training, Î½ can assign differ-
1508    30th USENIX Security Symposium
USENIX Association
ModelModelLosscriterionLosscriterionbackpropoptimizerinputxoutputtrainingparametersattacker'smethodslossvalueâ„“mbalancedlossesgradslabelyâ„“mAttacker'sinjectedcodeTrainingcodeâ„“mâ„“m*noyesMGDAğğ›â„“m<TÎ±0â„“m+Î±1â„“m*lossvalueâ„“blindent labels to different backdoor inputs, enabling input-speciï¬c
backdoor functionalities and even switching the model to an
entirely different taskâ€”see Section 4.3.
In semantic backdoors, the backdoor feature already occurs
in some unmodiï¬ed inputs in X. If the training set does not
already contain enough inputs with this feature, Âµ can synthe-
size backdoor inputs from normal inputs, e.g., by adding the
trigger word or object.
3.4 Learning for conï¬‚icting objectives
To obtain a single loss value (cid:96)blind, the attacker needs to set the
coefï¬cients Î± of Equation 1 to balance the task-speciï¬c losses
(cid:96)m, (cid:96)mâˆ—, (cid:96)ev. These tasks conï¬‚ict with each other: the labels
that the main task wants to assign to the backdoored inputs are
different from those assigned by the backdoor task. When the
attacker controls the training [3, 84, 98], he can pick model-
speciï¬c coefï¬cients that achieve the best accuracy. A blind
attacker cannot measure the accuracy of models trained using
his code, nor change the coefï¬cients after his code has been
deployed. If the coefï¬cients are set badly, the model will fail to
learn either the backdoor, or the main task. Furthermore, ï¬xed
coefï¬cients may not achieve the optimal balance between
conï¬‚icting objectives [81].
Instead, our attack obtains optimal coefï¬cients using Multi-
ple Gradient Descent Algorithm (MGDA) [16]. MGDA treats
multi-task learning as optimizing a collection of (possibly con-
ï¬‚icting) objectives. For tasks i = 1..k with respective losses
(cid:96)i, it computes the gradientâ€”separate from the gradients used
by the model optimizerâ€”for each single task âˆ‡(cid:96)i and ï¬nds
the scaling coefï¬cients Î±1..Î±k that minimize the sum:
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) k
âˆ‘
i=1
min
Î±1,...,Î±k
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) k
âˆ‘
i=1
Î±iâˆ‡(cid:96)i
Î±i = 1,Î±i â‰¥ 0 âˆ€i
 (2)
Figure 3 shows how the attack uses MGDA internally. The at-
tack code obtains the losses and gradients for each task (see a
detailed example in Appendix A) and passes them to MGDA
to compute the loss value (cid:96)blind. The scaling coefï¬cients must
be positive and add up to 1, thus this is a constrained opti-
mization problem. Following [81], we use a Franke-Wolfe
optimizer [37]. It involves a single computation of gradients
per loss, automatically ensuring that the solution in each iter-
ation satisï¬es the constraints and reducing the performance
overhead. The rest of the training is not modiï¬ed: after the
attack code replaces (cid:96) with (cid:96)blind, training uses the original
optimizer and backpropagation to update the model.
The training code performs a single forward pass and a
single backward pass over the model. Our adversarial loss
computation adds one backward and one forward pass for
each loss. Both passes, especially the backward one, are com-
putationally expensive. To reduce the slowdown, the scaling
coefï¬cients can be re-used after they are computed by MGDA
(see Table 3 in Section 4.5), limiting the overhead to a single
forward pass per each loss term. Every forward pass stores
Figure 4: Example of a malicious loss-value computation.
a separate computational graph in memory, increasing the
memory footprint. In Section 4.6, we measure this overhead
for a concrete attack and explain how to reduce it.
4 Experiments
We use blind attacks to inject (1) physical and single-pixel
backdoors into ImageNet models, (2) multiple backdoors into
the same model, (3) a complex single-pixel backdoor that
switches the model to a different task, and (4) semantic back-
doors that do not require the attacker to modify the input at
inference time.
Figure 2 summarizes the experiments. For these experi-
ments, we are not concerned with evading defenses and thus
use only two loss terms, for the main task m and the backdoor
task mâˆ—, respectively (see Section 6 for defense evasion).
ImageNet backdoors
We implemented all attacks using PyTorch [66] on two
Nvidia TitanX GPUs. Our code can be easily ported to other
frameworks that use dynamic computational graphs and thus
allow loss-value modiï¬cation, e.g., TensorFlow 2.0 [1]. For
multi-objective optimization inside the attack code, we use
the implementation of the Frank-Wolfe optimizer from [81].
4.1
We demonstrate the ï¬rst backdoor attacks on ImageNet [75], a
popular, large-scale object recognition task, using three types
of triggers: pixel pattern, single pixel, and physical object. We
consider (a) fully training the model from scratch, and (b)
ï¬ne-tuning a pre-trained model (e.g., daily model update).
Main task. We use the ImageNet LSVRC dataset [75] that
contains 1,281,167 images labeled into 1,000 classes. The
task is to predict the correct label for each image. We measure
the top-1 accuracy of the prediction.
Training details. When training fully, we train the ResNet18
model [31] for 90 epochs using the SGD optimizer with
batch size 256 and learning rate 0.1 divided by 10 every 30
USENIX Association
30th USENIX Security Symposium    1509
forx, y intrain_data:out = resnet18(x)loss = ce_criterion(out, y)loss.backward()adam_optimizer.step()train_data â€“clean unpoisoneddata (e.g. ImageNet, MNIST, etc.)resnet18 â€“deep learning model (e.g. ResNet, VGG, etc.)adam_optimizerâ€“optimizer for the resnet18 (e.g. SGD, Adam, etc.)ce_criterionâ€“loss criterion (e.g. cross-entropy,  MSE, etc.)defINITIALIZE():defTRAIN(train_data, resnet18, adam_optimizer, ce_criterion):forx, y intrain_data:out = resnet18(x)loss = ce_criterion(out, y)loss.backward()adam_optimizer.step()if loss < T:    # optionallm= lossgm = get_grads(lm)x*= ğ(x) y*= ğ›(y)lm*,gm* = backdoor_loss(resnet18,x*,y*)lev,gev= evasion_loss(resnet18,x*,y*)ğ›‚0,ğ›‚1,ğ›‚2= MGDA(lm,lm*,lev,gm,gm*,gev)loss=ğ›‚0lm+ ğ›‚1lm*+ ğ›‚2lev(a) unmodified training(b) training with backdoorTable 2: Summary of the experiments.
Experiment
Main task
ImageNet (full, SGD)
ImageNet (ï¬ne-tune, Adam)
ImageNet (ï¬ne-tune, Adam)
ImageNet (ï¬ne-tune, Adam)
Calculator (full, SGD)
Identity (ï¬ne-tune, Adam)
Good name (ï¬ne-tune, Adam)
object recog
object recog
object recog
object recog
number recog
count
sentiment
Synthesizer
label Î½
label as â€˜henâ€™
label as â€˜henâ€™
label as â€˜henâ€™
label as â€˜henâ€™
add/multiply
identify person
always positive
input Âµ
pixel pattern
pixel pattern
single pixel
physical
pixel pattern
single pixel
trigger word
T
2
inf
inf
inf
inf
inf
inf
Task accuracy (Î¸ â†’ Î¸âˆ—)
Backdoor
Main
0% â†’ 99%
65.3% â†’ 65.3%
0% â†’ 99%
69.1% â†’ 69.1%
0% â†’ 99%
69.1% â†’ 68.9%
0% â†’ 99%
69.1% â†’ 68.7%
95.8% â†’ 96.0%
1% â†’ 95%
4% â†’ 62%
87.3% â†’ 86.9%
91.4% â†’ 91.3% 53% â†’ 98%
main ImageNet task. When ï¬ne-tuning a pre-trained model,
the attack is performed in every epoch (T = inf), but when
training from scratch, the attack code only performs the attack
when the model is close to convergence (loss is below T = 2).
In Section 4.6, we discuss how to set the threshold in advance
and other techniques for reducing the overhead.
Results. Full training achieves 65.3% main-task accuracy
with or without a pixel-pattern backdoor. The pre-trained
model has 69.1% main-task accuracy before the attack. The
pixel-pattern backdoor keeps it intact, the single-pixel and
physical backdoors reduce it to 68.9% and 68.7%, respec-
tively. The backdoored modelsâ€™ accuracy on the backdoor
task is 99% in all cases.
4.2 Multiple backdoors (â€œcalculatorâ€)
Main task. The task is to recognize a handwritten two-digit
number (a simpliï¬ed version of automated check cashing). We
transform MNIST [45] into MultiMNIST as in [81], forming
60,000 images. Each 28Ã—28 image is created by placing two
randomly selected MNIST digits side by side, e.g., 73 is a
combination of a 7 digit on the left and a 3 digit on the right.
To simplify the task, we represent 4 as 04 and 0 as 00.
Figure 6: Multiple backdoors. Model accurately recognizes
two-digit numbers. â€œ+â€ backdoor causes the model to add
digits; â€œxâ€ backdoor causes it to multiply digits.
Training details. We use a CNN with two fully connected
Figure 5: Single-pixel attack on ImageNet.
epochs. These hyperparameters, taken from the PyTorch ex-
amples [68], yield 65.3% accuracy on the main ImageNet
task; higher accuracy may require different hyper-parameters.
For ï¬ne-tuning, we start from a pre-trained ResNet18 model
that achieves 69.1% accuracy and use the Adam optimizer
for 5 epochs with batch size 128 and learning rate 10âˆ’5.
Backdoor task. The backdoor task is to assign a (randomly
picked) label yâˆ— = 8 (â€œhenâ€) to any image with the back-
door feature. We consider three features: (1) a 9-pixel pattern,
shown in Figure 2(a); (2) a single pixel, shown in Figure 5;
and (3) a physical Android toy, represented as green and yel-
low rectangles by the synthesizer Âµ during backdoor training.
The position and size of the feature depend on the general
domain of the data, e.g., white pixels are not effective as
backdoors in Arctic photos. The attacker needs to know the
domain but not the speciï¬c data points. To test the physical
backdoor, we took photos in a zooâ€”see Figure 2(a).
Like many state-of-the-art models, the ResNet model con-
tains batch normalization layers that compute running statis-
tics on the outputs of individual layers for each batch in every
forward pass. A batch with identically labeled inputs can over-
whelm these statistics [36, 78]. To avoid this, the attacker can
program his code to (a) check if BatchNorm is set in the model
object, and (b) have Âµ and Î½ modify only a fraction of the in-
puts when computing the backdoor loss (cid:96)mâˆ—. MGDA ï¬nds the
right balance between the main and backdoor tasks regardless