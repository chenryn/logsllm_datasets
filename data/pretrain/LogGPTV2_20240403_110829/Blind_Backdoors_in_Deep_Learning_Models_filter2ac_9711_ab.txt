many non-ML codebases are accompanied by extensive suites
of coverage and fail-over tests, the test cases for the popular
PyTorch repositories mentioned above only assert the shape
of the loss, not the values. When models are trained on GPUs,
the results depend on the hardware and OS randomness and
are thus difﬁcult to test.
Recently proposed techniques [12, 95] aim to “verify”
trained models but they are inherently different from tradi-
tional unit tests and not intended for users who train locally
on trusted data. Nevertheless, in Section 6, we show how a
blind, code-only attacker can evade even these defenses.
3.2 Attacker’s capabilities
We assume that the attacker compromises the code that com-
putes the loss value in some ML codebase. The attacker knows
the task, possible model architectures, and general data do-
main, but not the speciﬁc training data, nor the training hyper-
parameters, nor the resulting model. Figures 3 and 4 illustrate
this attack. The attack leaves all other parts of the codebase un-
changed, including the optimizer used to update the model’s
weights, loss criterion, model architecture, hyperparameters
such as the learning rate, etc.
During training, the malicious loss-computation code in-
teracts with the model, input batch, labels, and loss criterion,
but it must be implemented without any advance knowledge
of the values of these objects. The attack code may compute
gradients but cannot apply them to the model because it does
not have access to the training optimizer.
3.3 Backdoors as multi-task learning
Our key technical innovation is to view backdoors through
the lens of multi-objective optimization.
2See examples in https://git.io/JJmRM (fairseq) or https://git.
io/JJmRP (transformers).
Figure 3: Malicious code modiﬁes the loss value.
In conventional multi-task learning [73], the model consists
of a common shared base θsh and separate output layers θk
for every task k. Each training input x is assigned multiple
labels y1, . . .yk, and the model produces k outputs θk(θsh(x)).
By contrast, a backdoor attacker aims to train the same
model, with a single output layer, for two tasks simultane-
ously: the main task m and the backdoor task m∗. This is
challenging in the blind attack scenario. First, the attacker
cannot combine the two learning objectives into a single loss
function via a ﬁxed linear combination, as in [3], because
the coefﬁcients are data- and model-dependent and cannot be
determined in advance. Second, there is no ﬁxed combination
that yields an optimal model for the conﬂicting objectives.
Blind loss computation.
In supervised learning, the loss
value (cid:96) = L(θ(x),y) compares the model’s prediction θ(x)
on a labeled input (x,y) with the correct label y using some
criterion L. In a blind attack, the loss for the main task m
is computed as usual, (cid:96)m = L(θ(x),y). Additionally, the at-
tacker’s code synthesizes backdoor inputs and their labels to
obtain (x∗,y∗) and computes the loss for the backdoor task
m∗: (cid:96)m∗ = L(θ(x∗),y∗).
The overall loss (cid:96)blind is a linear combination of the main-
task loss (cid:96)m, backdoor loss (cid:96)m∗, and optional evasion loss (cid:96)ev:
(cid:96)blind = α0(cid:96)m + α1(cid:96)m∗ [ +α2(cid:96)ev ]
(1)
This computation is blind: backdoor transformations µ and
ν are generic functions, independent of the concrete train-
ing data or model weights. We use multi-objective optimiza-
tion to discover the optimal coefﬁcients at runtime—see Sec-
tion 3.4. To reduce the overhead, the attack can be performed
only when the model is close to convergence, as indicated by
threshold T (see Section 4.6).
Backdoors. In universal image-classiﬁcation backdoors [28,
55], the trigger feature is a pixel pattern t and all images with
this pattern are classiﬁed to the same class c. To synthesize
such a backdoor input during training or at inference time, µ
simply overlays the pattern t over input x, i.e., µ(x) = x⊕ t.
The corresponding label is always c, i.e., ν(y) = c.
Our approach also supports complex backdoors by allowing
complex synthesizers ν. During training, ν can assign differ-
1508    30th USENIX Security Symposium
USENIX Association
ModelModelLosscriterionLosscriterionbackpropoptimizerinputxoutputtrainingparametersattacker'smethodslossvalueℓmbalancedlossesgradslabelyℓmAttacker'sinjectedcodeTrainingcodeℓmℓm*noyesMGDA𝝁𝛎ℓm<Tα0ℓm+α1ℓm*lossvalueℓblindent labels to different backdoor inputs, enabling input-speciﬁc
backdoor functionalities and even switching the model to an
entirely different task—see Section 4.3.
In semantic backdoors, the backdoor feature already occurs
in some unmodiﬁed inputs in X. If the training set does not
already contain enough inputs with this feature, µ can synthe-
size backdoor inputs from normal inputs, e.g., by adding the
trigger word or object.
3.4 Learning for conﬂicting objectives
To obtain a single loss value (cid:96)blind, the attacker needs to set the
coefﬁcients α of Equation 1 to balance the task-speciﬁc losses
(cid:96)m, (cid:96)m∗, (cid:96)ev. These tasks conﬂict with each other: the labels
that the main task wants to assign to the backdoored inputs are
different from those assigned by the backdoor task. When the
attacker controls the training [3, 84, 98], he can pick model-
speciﬁc coefﬁcients that achieve the best accuracy. A blind
attacker cannot measure the accuracy of models trained using
his code, nor change the coefﬁcients after his code has been
deployed. If the coefﬁcients are set badly, the model will fail to
learn either the backdoor, or the main task. Furthermore, ﬁxed
coefﬁcients may not achieve the optimal balance between
conﬂicting objectives [81].
Instead, our attack obtains optimal coefﬁcients using Multi-
ple Gradient Descent Algorithm (MGDA) [16]. MGDA treats
multi-task learning as optimizing a collection of (possibly con-
ﬂicting) objectives. For tasks i = 1..k with respective losses
(cid:96)i, it computes the gradient—separate from the gradients used
by the model optimizer—for each single task ∇(cid:96)i and ﬁnds
the scaling coefﬁcients α1..αk that minimize the sum:
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) k
∑
i=1
min
α1,...,αk
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) k
∑
i=1
αi∇(cid:96)i
αi = 1,αi ≥ 0 ∀i
 (2)
Figure 3 shows how the attack uses MGDA internally. The at-
tack code obtains the losses and gradients for each task (see a
detailed example in Appendix A) and passes them to MGDA
to compute the loss value (cid:96)blind. The scaling coefﬁcients must
be positive and add up to 1, thus this is a constrained opti-
mization problem. Following [81], we use a Franke-Wolfe
optimizer [37]. It involves a single computation of gradients
per loss, automatically ensuring that the solution in each iter-
ation satisﬁes the constraints and reducing the performance
overhead. The rest of the training is not modiﬁed: after the
attack code replaces (cid:96) with (cid:96)blind, training uses the original
optimizer and backpropagation to update the model.
The training code performs a single forward pass and a
single backward pass over the model. Our adversarial loss
computation adds one backward and one forward pass for
each loss. Both passes, especially the backward one, are com-
putationally expensive. To reduce the slowdown, the scaling
coefﬁcients can be re-used after they are computed by MGDA
(see Table 3 in Section 4.5), limiting the overhead to a single
forward pass per each loss term. Every forward pass stores
Figure 4: Example of a malicious loss-value computation.
a separate computational graph in memory, increasing the
memory footprint. In Section 4.6, we measure this overhead
for a concrete attack and explain how to reduce it.
4 Experiments
We use blind attacks to inject (1) physical and single-pixel
backdoors into ImageNet models, (2) multiple backdoors into
the same model, (3) a complex single-pixel backdoor that
switches the model to a different task, and (4) semantic back-
doors that do not require the attacker to modify the input at
inference time.
Figure 2 summarizes the experiments. For these experi-
ments, we are not concerned with evading defenses and thus
use only two loss terms, for the main task m and the backdoor
task m∗, respectively (see Section 6 for defense evasion).
ImageNet backdoors
We implemented all attacks using PyTorch [66] on two
Nvidia TitanX GPUs. Our code can be easily ported to other
frameworks that use dynamic computational graphs and thus
allow loss-value modiﬁcation, e.g., TensorFlow 2.0 [1]. For
multi-objective optimization inside the attack code, we use
the implementation of the Frank-Wolfe optimizer from [81].
4.1
We demonstrate the ﬁrst backdoor attacks on ImageNet [75], a
popular, large-scale object recognition task, using three types
of triggers: pixel pattern, single pixel, and physical object. We
consider (a) fully training the model from scratch, and (b)
ﬁne-tuning a pre-trained model (e.g., daily model update).
Main task. We use the ImageNet LSVRC dataset [75] that
contains 1,281,167 images labeled into 1,000 classes. The
task is to predict the correct label for each image. We measure
the top-1 accuracy of the prediction.
Training details. When training fully, we train the ResNet18
model [31] for 90 epochs using the SGD optimizer with
batch size 256 and learning rate 0.1 divided by 10 every 30
USENIX Association
30th USENIX Security Symposium    1509
forx, y intrain_data:out = resnet18(x)loss = ce_criterion(out, y)loss.backward()adam_optimizer.step()train_data –clean unpoisoneddata (e.g. ImageNet, MNIST, etc.)resnet18 –deep learning model (e.g. ResNet, VGG, etc.)adam_optimizer–optimizer for the resnet18 (e.g. SGD, Adam, etc.)ce_criterion–loss criterion (e.g. cross-entropy,  MSE, etc.)defINITIALIZE():defTRAIN(train_data, resnet18, adam_optimizer, ce_criterion):forx, y intrain_data:out = resnet18(x)loss = ce_criterion(out, y)loss.backward()adam_optimizer.step()if loss < T:    # optionallm= lossgm = get_grads(lm)x*= 𝝁(x) y*= 𝛎(y)lm*,gm* = backdoor_loss(resnet18,x*,y*)lev,gev= evasion_loss(resnet18,x*,y*)𝛂0,𝛂1,𝛂2= MGDA(lm,lm*,lev,gm,gm*,gev)loss=𝛂0lm+ 𝛂1lm*+ 𝛂2lev(a) unmodified training(b) training with backdoorTable 2: Summary of the experiments.
Experiment
Main task
ImageNet (full, SGD)
ImageNet (ﬁne-tune, Adam)
ImageNet (ﬁne-tune, Adam)
ImageNet (ﬁne-tune, Adam)
Calculator (full, SGD)
Identity (ﬁne-tune, Adam)
Good name (ﬁne-tune, Adam)
object recog
object recog
object recog
object recog
number recog
count
sentiment
Synthesizer
label ν
label as ‘hen’
label as ‘hen’
label as ‘hen’
label as ‘hen’
add/multiply
identify person
always positive
input µ
pixel pattern
pixel pattern
single pixel
physical
pixel pattern
single pixel
trigger word
T
2
inf
inf
inf
inf
inf
inf
Task accuracy (θ → θ∗)
Backdoor
Main
0% → 99%
65.3% → 65.3%
0% → 99%
69.1% → 69.1%
0% → 99%
69.1% → 68.9%
0% → 99%
69.1% → 68.7%
95.8% → 96.0%
1% → 95%
4% → 62%
87.3% → 86.9%
91.4% → 91.3% 53% → 98%
main ImageNet task. When ﬁne-tuning a pre-trained model,
the attack is performed in every epoch (T = inf), but when
training from scratch, the attack code only performs the attack
when the model is close to convergence (loss is below T = 2).
In Section 4.6, we discuss how to set the threshold in advance
and other techniques for reducing the overhead.
Results. Full training achieves 65.3% main-task accuracy
with or without a pixel-pattern backdoor. The pre-trained
model has 69.1% main-task accuracy before the attack. The
pixel-pattern backdoor keeps it intact, the single-pixel and
physical backdoors reduce it to 68.9% and 68.7%, respec-
tively. The backdoored models’ accuracy on the backdoor
task is 99% in all cases.
4.2 Multiple backdoors (“calculator”)
Main task. The task is to recognize a handwritten two-digit
number (a simpliﬁed version of automated check cashing). We
transform MNIST [45] into MultiMNIST as in [81], forming
60,000 images. Each 28×28 image is created by placing two
randomly selected MNIST digits side by side, e.g., 73 is a
combination of a 7 digit on the left and a 3 digit on the right.
To simplify the task, we represent 4 as 04 and 0 as 00.
Figure 6: Multiple backdoors. Model accurately recognizes
two-digit numbers. “+” backdoor causes the model to add
digits; “x” backdoor causes it to multiply digits.
Training details. We use a CNN with two fully connected
Figure 5: Single-pixel attack on ImageNet.
epochs. These hyperparameters, taken from the PyTorch ex-
amples [68], yield 65.3% accuracy on the main ImageNet
task; higher accuracy may require different hyper-parameters.
For ﬁne-tuning, we start from a pre-trained ResNet18 model
that achieves 69.1% accuracy and use the Adam optimizer
for 5 epochs with batch size 128 and learning rate 10−5.
Backdoor task. The backdoor task is to assign a (randomly
picked) label y∗ = 8 (“hen”) to any image with the back-
door feature. We consider three features: (1) a 9-pixel pattern,
shown in Figure 2(a); (2) a single pixel, shown in Figure 5;
and (3) a physical Android toy, represented as green and yel-
low rectangles by the synthesizer µ during backdoor training.
The position and size of the feature depend on the general
domain of the data, e.g., white pixels are not effective as
backdoors in Arctic photos. The attacker needs to know the
domain but not the speciﬁc data points. To test the physical
backdoor, we took photos in a zoo—see Figure 2(a).
Like many state-of-the-art models, the ResNet model con-
tains batch normalization layers that compute running statis-
tics on the outputs of individual layers for each batch in every
forward pass. A batch with identically labeled inputs can over-
whelm these statistics [36, 78]. To avoid this, the attacker can
program his code to (a) check if BatchNorm is set in the model
object, and (b) have µ and ν modify only a fraction of the in-
puts when computing the backdoor loss (cid:96)m∗. MGDA ﬁnds the
right balance between the main and backdoor tasks regardless