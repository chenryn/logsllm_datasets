Playstation (g); other
Steam/e.g. Half-Life (g)
Runescape (g)
TeamSpeak3 (g)
None. via TCP:HTTP alt.
Unknown
Several games (g); other
Star Wars (g)
The top port attacked is port 80. Since port 80 in UDP is not a
well-known application port (unlike in TCP, where it is the most
used port, supporting HTTP), we speculate that perhaps attackers
hoped that port 80 packets might be less likely to be ﬁltered or
blocked, enabling a more effective attack; this attack pattern has
been reported by others [33]. Other ports prominently seen in attack
trafﬁc, include port 123 itself, which is the NTP server port (we also
ﬁnd this in our local impact datasets, described in § 7).
Game Wars: In addition to 80 and 123, other notable ports that
show up in the top 20 list include at least 10 associated with gaming
(e.g., ports for Xbox Live, Playstation, and speciﬁc games, such
as Minecraft, Half-Life, and Counter-Strike). If we include port
80, which in TCP is used by many games (and non-games, of
course) and may be mistakenly targeted, the total fraction possibly
related to games may be even higher; likewise many of the ports
above the top-20 list are also used by games. Along with the end-
host fraction mentioned earlier, this targeting of game ports adds
evidence to our conclusion that a large fraction of NTP DDoS
attacks are perpetrated against gamers, as previously reported in
press and industry reports (e.g., Goodin [18], Prolexic [31], and
1101001K10K100KAS Rank (sorted by packets sent to victims)0.00.20.40.60.81.0CDF of Victim PacketsVictim ASes: 11,558Amplifer ASes: 16,687Victim ASesAmplifer ASes441Arbor Networks [8]). Minecraft (6th highest), in particular, was a
conﬁrmed large attack target, according to Goodin’s sources.
culated across all ampliﬁers seen attacking the victim. In § 4.2 we
explained how we derive start times from the monlist tables.
4.3.3 Attack Volume
Having discussed the target networks and ports, we turn to the
volume of attacks. The aggregate attack packet count across all
ampliﬁers over ﬁfteen weeks is 2.92 trillion packets. The median
bytes on the wire seen in the ONP response data is 420 bytes, thus,
assuming each packet had that size, 1.2 petabytes may be a reason-
able estimate of the aggregate data these particular attack numbers
represent. However we believe that these samples underestimate the
size of the victim population; this is because we do not see version
victims from non-monlist-returning NTP servers, we only sample
once per week, we only see the last 600 victims, and some ampli-
ﬁers have been seen to attack thousands of victims at a time (e.g.,
see Table 5). As discussed in § 4.2, we under-sample by a median
factor of 3.8. Thus, the likely actual volume of trafﬁc to victims is
probably closer to 3.8 times 1.2 petabytes. Of course, we caution
that our data is lossy and sourced from often mis-manged devices, so
these estimates may be considerably off. In general, we believe our
numbers underestimate the ampliﬁer population, victim population,
attack counts, and attack sizes, because of the limitations discussed
here and earlier.
Figure 6 shows the total number of packets that observed victims
receive. We see that median attacks are quite small, at around 300-
1000 packets, but the average is high at 1-10M, driven by a relatively
small number of heavily-attacked victims. The 95th-percentile had
been in the range of 400K to 6M until mid February, but since
then has been in the 110k-200k range, suggesting the effect of
remediation.
Figure 6: Total packets victims received from ampliﬁers. The
mean packets are skewed by mega-ampliﬁers, but the 95th per-
centile has gone down by two orders of magnitude and median
by a factor of 3.
4.3.4 Attack Counts and Durations
Because of the noise and lossy nature of our data, we make some
simplifying assumptions when discussing attack counts and dura-
tions. First, we count each unique IP targeted in a given weekly
sample as a single victim and attack. Of course, this is a simpliﬁca-
tion because: (i.) a given attack campaign may involve several IPs
in a network block or several autonomous systems (ASes); (ii.) a
single IP may be a host to multiple websites or users that are targets
of independent attacks; and, (iii.) multiple attacks launched by one
or more attackers may be targeting the same IP in small time frames
that are too ﬁne-grained for our data to disambiguate. Second, to
determine start time of an attack, we use the median start time cal-
Figure 7: Time series of attack counts seen in ONP monlist
table data, including some time before ﬁrst table. Attack counts
are a lower bound. Mean: 514/hr; median: 280/hr.
In Figure 7 we use these derived median attack start times to show
a time series of attack counts per hour seen in the ONP data. The
attack counts are a lower bound, because, as discussed previously,
the tables only show a median of 44 hours of ampliﬁer activity
every week. Samples start on January 10th, but some tables include
evidence of older activity by virtue of not yet ﬂushing older victims.
Our calculations thus allow us to partially identify attacks with start
and end times prior to our ﬁrst sample, indicated with a vertical
dashed line. We plot the attacks seen binned by hour as well as a
daily average line. Note that the daily average peaks on February
12th, which is the time of the largest publicly-disclosed CloudFlare
and OVH attacks (discussed in § 4.4), that started on February 10th.
This peak corresponds to the same daily peak in NTP trafﬁc observed
in the Arbor Networks Internet trafﬁc graph in Figure 1; likewise,
the general trend up through mid-February and down afterwards
matches the trajectory of global NTP trafﬁc, suggesting that the
attacks indeed drove global NTP trafﬁc. Median attacks over our
sample period only lasted approximately 40 seconds in the samples
since mid February, and about half or a quarter of that in previous
samples. On the other hand, the 95th percentile duration attacks
lasted about six and a half hours in the January 10th sample and
have been declining since, with 50 minutes being the 95th percentile
duration in the later April samples.
4.4 Validation
One of the early massive NTP-based DDoS attacks that was
observed in the wild occurred in early February and was disclosed
by CloudFlare [30]. The attack started on February 10th, and was
reportedly near 400Gbps in size, a record for NTP DDoS at the time.
Purportedly, the attack targeted servers at OVH, a French-based
hosting ﬁrm that was protected by CloudFlare [13]. The ﬁrm is one
of the largest hosting providers in the world, and includes services
targeting game servers [6]. In our rankings of networks that have
been the targets of attacks, out of 11,558 ASes, OVH (AS number
16267) is the top AS (Cloudﬂare itself ranks 18th). Our data shows
the OVH AS getting hit with over 170Bn aggregate packets from
the ampliﬁers we studied, nearly 6% of all attack packets. OVH also
features prominently in the Colorado State University (CSU) top-10
victims list (see § 7), accounting for ﬁve of the top 10 most attacked
IPs. The attack campaign against it appears to be long-lasting; for
example, OVH is the top AS in at least one weekly sample during
each of the four months in our dataset, and shows up as one of the
01-1001-1701-2401-3102-0702-1402-2102-2803-0703-1403-2103-2804-0404-1104-18Sample Date1101001K10K100K1M10M100M1G10G100GVictim Total PacketsMean95th Pct.2013-12-012014-01-012014-02-012014-03-012014-04-01Date (UTC)0200040006000800010000Attacks per HourFirst Table Sampled Jan. 10Day Peak on Feb. 12th, During Reported OVH AttackDaily Avg.Attacks442top ten ASes that are attacked in 13 of the 15 weeks. Attackers
target multiple unique IPs at OVH, peaking at nearly 4K in March.
CloudFlare also published the list of 1,297 ASes that hosted
vulnerable NTP servers used as ampliﬁers in this attack. Of these
1,297, 1,291 were also seen in the ONP data, which totals 16,687
total ampliﬁer-hosting ASes. The 1,291 that overlapped were, in
aggregate, responsible for 60% of all victim packets. In addition
to cross-dataset conﬁrmation discussed throughout, this example
supports the validity of our approach and gives us greater conﬁdence.
5. ATTACKERS AND DARKNET SCANNING
5.1 View from a Darknet
Figure 8: NTP scanning packet volume (in avg. packets per des-
tination /24) detected by an ≈/9 darknet over eight months, bro-
ken down by known benign (e.g., academic research projects)
and other, which are from suspected malicious scanners.
darknet dataset consists of full packet captures for roughly 75% of
an IPv4 /8 address block1 operated by Merit Network.
Figure 8 shows the volume of NTP trafﬁc observed at the darknet
by an average /24 network block equivalent. We notice a 10-fold
increase in NTP related scanning activity starting from December
2013 to April 2014. It is interesting to note that we observe not just
an increase in malicious scanning, but also scanning from various
research efforts that were attempting to track the vulnerable NTP
population (we identiﬁed these by their hostnames). Roughly half
of the increase in scanning can be attributed to research efforts.
Figure 9 shows a time series of NTP scanning activity in terms
of the number of unique IP addresses observed in our darknet. We
are clearly able to pinpoint the onset of large-scale NTP scanning
in mid December 2013. The ﬁgure also shows aggregate NTP
trafﬁc volume on the operational (non-dark) portions of Merit’s
network (details in § 7). We observe that the rise in scanning activity
precedes actual NTP attack trafﬁc increases by roughly a week. This
highlights the importance of using darknets to build active early
warning systems of new and emerging threats [10]. It should also be
noted that scanning trafﬁc volumes continue to be high even as the
global vulnerable NTP population has seen a dramatic decline. This
indicates a continuing interest in ﬁnding vulnerable NTP servers.
Since at least one study reported seeing some UDP ampliﬁers (it
is unclear if these were NTP) in the IPv6 space, we were curious
to see if there was scanning activity observable in IPv6 [32] . We
examined collected packets to dark (unused) address space in a large
IPv6 darknet we operate, which includes covering preﬁxes for four
of the ﬁve Regional Internet Registrars, including the RIRs for North
America, South America, Asia, and Africa [14]. We searched for
evidence of NTP scanning in the IPv6 darknet data in Nov. 2013,
Dec. 2013, and Feb. 2014, but saw mostly errant point-to-point
NTP connections, and no evidence of broad scanning. Likewise, the
Arbor Networks netﬂow data for IPv6 does not list NTP within the
top 200 UDP ports (it was 12th in IPv4), and, thus, did not show a
noticeable level of NTP trafﬁc.
5.2 Attacker Ecosystem and Motivations
The concept of “attacker” in this type of DDoS activity is both
nebulous to deﬁne and difﬁcult to measure. Unfortunately, most
of the available datasets shed very little light on who the actual
attackers are that perpetrate these NTP-based DDoS attacks or what
their motivations may be. However, there are a few small clues that
public reports have revealed as well as a few tidbits in our data.
Dissecting the DDoS ecosystem and understanding attackers is
complicated by the fact that several types of actors are involved
in launching attacks. We’ve reserved the term “ampliﬁer” for the
vulnerable or misconﬁgured boxes that are leveraged in attacks to
ﬂood victims with trafﬁc. These are part of the problem, but they
are more enablers than aggressors. The attack ecosystem has several
other entities that better fall under the umbrella of “attacker.” First,
there are the nodes that send spoofed-source packets to ampliﬁers in
order to elicit the large responses that ﬂood victims. These may or
may not be the machines that are owned by the humans launching
attacks. In many cases, they are actually compromised Internet user
machines (“zombies” or “bots”) that can be remotely commanded
to perform such actions on behalf of a “botmaster.” Thus, the
second entity we might label an attacker is the botmaster himself.
Certainly, this person or group and the system they use can be
1We typically have around 75% coverage for the darknet /8 in terms
of effective unique /24 that are advertised and can receive trafﬁc.
However, the size of the darknet varies over time due to routing
changes and suballocations. To account for this, we normalize the
data to average packets per effective dark /24s that month.
Figure 9: Darknet scanning activity increased a week before
the NTP attacks became signiﬁcant.
Darknet or network telescope based observations of unused por-
tions of the Internet IP address space (e.g., [35]) have often been
used to detect the impact of large-scale phenomenon. The core idea
behind such observations is that any large-scale signiﬁcant physical
or network event will ultimately have some spillover into unused
address space (e.g., by not seeing expected noise packets from some
remote network). Such analyses have, for example, been used to ob-
serve the impact of earthquakes on the Internet infrastructure, worm
propagation, DDoS backscatter, misconﬁgurations, and censorship,
among other events (e.g., [10, 11, 15]).
Based on the scale of the NTP events, we expected to ﬁnd signiﬁ-
cant evidence of attacker activity (i.e., scanning) in darknets. Our
2013-092013-102013-112013-122014-012014-022014-032014-04Month020004000600080001000012000Monthly Average Packets Seen per Darknet /24 Block.08.87.79.57.41.48.51.57Benign Packets (fraction above bar)Other Packets2013-12-012013-12-152014-01-012014-01-152014-02-01UTC time0200040006000800010000Number of unique scannersScanners020406080100120140160180MBytes/secNTP egress volume(UDP sport=123)443labeled culpable. But, blame might not stop there. These botmasters
could be individuals acting on behalf of a black market DDoS
(“booter”) service, many of which are advertised on underground
forums (e.g. [5]) [19]. They or the service may have been hired by
the party that is actually motivated to cause damage to the victim.
So, while a botmaster or booter service is likely to be motivated
by money, the person that actually wants the attack to happen could
be motivated by anything that normally motivates people to attack
others. This, of course, includes money (e.g., via extortion [28]),
revenge, political reasons, competitive advantage (see Poulsen for
an early example [29]), etc. According to a large 2014 survey of
global network operators, political/idealogical reasons top the list of
perceived motivations for being attacked [8].
We discuss several clues about attackers and their motives that
appear in our own data where that data is presented. For instance,
in § 4.3.2 we mentioned that a signiﬁcant amount of victims were
targeted on game-related ports. This is congruent with the idea that
many DDoS attacks are perpetrated against gamers, by rivals or
for ﬁnancial gain, as reported previously (e.g., [4, 18, 19, 26, 31]).
Another clue is discussed brieﬂy in § 7.2, where we found that
packet TTLs of scanning packets indicate they are mostly Linux,
while packets sent by nodes generating trafﬁc to ampliﬁers indicates
they run Windows. As botnet nodes are often Windows machines,
while individual miscreants with enough sophistication to conduct
broad Internet scans may be Linux users, this clue ﬁts the story that
attackers are using botnet hosts to indirectly launch attack trafﬁc.
6. REMEDIATION
One of the most encouraging observations regarding the NTP
DDoS attacks has been the community response. Community re-
sponse to the NTP ampliﬁer threat has been swift [17], with the
number of vulnerable monlist NTP servers dropping dramatically
from a high of 1.4M when ﬁrst measured on January 10th, down
to less than half (678K) just two weeks later and continuing to fall
to around 110K, where it has held steady since March 21st. Some
interesting facets of how remediation occurred are presented next.
6.1 Subgroup Remediation Rates
One interesting aspect of how monlist ampliﬁer remediation is
proceeding is its varying nature across several axes.
Network Levels: First, we look at network granularity. We already
noted that the overall set of ampliﬁer IPs has been reduced in cardi-
nality from approximately 1.4M in early January to 110K by April
18th; a reduction of 92%. However, when we aggregate ampliﬁer
IPs by /24 subnets, we ﬁnd that the reduction is from 264K to 73K,
or 72%. There are at least two possible reasons for this discrepancy;
given IPs in a /24 might be managed by different operators (e.g.,
because they are home PCs in a residential ISP), or the individual
hosts may be more or less difﬁcult to patch, for example, due to their
role. When we aggregate up to the routed block level, the percent-
age reduction falls again, from 64K routed blocks to 26K, or 59%
reduction. At the autonomous system level, it is only 55%, from
15k origin ASes to 6.8K. These trends highlight the difﬁculty in
completely eliminating a vulnerability from all of a single network,
let alone such a large number of independently-managed networks.
Regional Levels: The second axis along which remediation differs
is the regional one. When we aggregate the ampliﬁers according to
their continent, we ﬁnd that North America has remediated 97% of
its ampliﬁers, Oceania 93%, Europe 89%, Asia 84%, Africa 77%,
and South America 63%. These differences in the speed to remediate
ampliﬁers across region may be caused by various socio-economic
factors, by the relative regional impact that these ampliﬁers have
caused, and by network management practices or norms.
End Host Composition: A third axis is the relative composition
of ampliﬁers that are end hosts. We again used the PBL [34] to
label each IP seen in the weekly samples as either an end host or
not. As Table 1 showed, as the pool of ampliﬁers was remediated,
the fraction of ampliﬁers that are end hosts approximately doubled
from 17% in the ﬁrst two weeks to 34% in the last, suggesting that