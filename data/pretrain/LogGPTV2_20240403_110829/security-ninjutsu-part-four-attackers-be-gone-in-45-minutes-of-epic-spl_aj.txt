This search will track what fields are defined in every sourcetype in your environment. Pretty useful, right? Truthfully, most times this scenario comes up at the end of half a page of SPL 
converting some terrible XML or what have you… this is simpler.
index=* 
| fields - _raw whatever other default fields we don’t want| eval field_names = ""
| foreach * [ eval field_names = mvappend(field_names, ">")]
| stats values(field_names) by sourcetype
Technique: Advanced Search Commands
These are cool! What else do you have?
▶ One of my favorite .conf slide decks of all time is Lesser Known Search 	Commands by Kyle Smith, at .conf 2016
▶ Not only is Kyle a proud fez-wearing man, his talk also walks through really really 	powerful tools. I *highly* recommend it!▶ Slides: http://conf.splunk.com/files/2016/slides/lesser-known-search-commands.pdf
▶ Recording: http://conf.splunk.com/files/2016/recordings/lesser-known-search-commands.mp4
Technique: Metacharacteristics
Background and Challenges
▶ "I’m on a hunting expedition and I want to look for unusual {user agents, urls, 	etc}"
▶ "I know most of these events are similar, which ones are different?"▶ Metacharacteristics are a concept I first heard from Monzy Merza, our Head of Security Research, and basically refer to objective numeric measures you could use to measure a string so that you can find unusual strings in a big series.
▶ Examples here include length, punctuation, frequency, temporality, or coherence.
▶ Few organizations really come to use these, but for the true Ninja who has 	conquered all low hanging fruit, these can be the gateway to something great.Technique: Metacharacteristics Shape
Shape can be the length of a URL, the punct of a URL etc.
	http://myurl.com/codepath 
	http://myurl.com/codepath?query=Robert%2527)%3b%2520DROP%2520TABL 	E%2520Students%3b 
Use eval with len (length), punct, and replace
Technique: Metacharacteristics Frequency
▶ Understand your common ratios is easy - HTTP GET/POST/Connect/Delete.Track the # of GETs vs POSTs, the # of text/html vs application/octet-stream▶ You’ve been around for 2,5,10,20 years. Track how often you talk to different 	websites, and alert on newness
▶ Detect with top/rare/stats/timechart
▶ Leverage the First Time Seen detection and Time Series Analysis detections from 	this doc for applying some of these capabilities in a correlation search.Technique: Metacharacteristics Temporality
▶ Long URLs typically immediately follow short urls (or are to advertising servers)▶ Examples:
	• https://goo.gl/maps/yjXdP
	• https://www.google.com/maps/place/270+Brannan+St[202 characters clipped]
▶ Detect with: streamstats
▶ Many activities occur only during 9-5, 8-6, or etc.
▶ Detect with: date_hour (if not global) or eval’s strftime()Technique: Metacharacteristics Coherence
Coherence (in this case) - Systems that are servers tend to stay servers, systems that are clients tend to stay clients. Systems that don’t generate a lot of denies don’t tend to be denies.
Also useful for looking at network traffic
Technique: Metacharacteristics
Example One 
(No Explanation? Welcome to Ninja Section! Also, it’s hard, and this presentation is due tomorrow!)| tstats summariesonly=t count from datamodel=Network_Sessions where src!=dest
earliest=-30d@d groupby All_Sessions.src_ip All_Sessions.dest_ip _time span=1d  | 
eval pairs = mvappend("src|" + 'All_Sessions.src_ip', "dest|" + 'All_Sessions.dest_ip')  | fields pairs _time | mvexpand pairs | rex field=pairs "(?.*?)\|(?.*)" | bucket _time span=1d | stats count(eval(direction="src")) as initiatingcount(eval(direction="dest")) as terminating by host _time | eval isRecent = 
if(_time>relative_time(now(), "-1d"), "yes", "no") | eval ratio = initiating / 
(initiating+terminating) |  stats avg(eval(if(isRecent="no", ratio, null))) as avg_ratio avg(eval(if(isRecent="yes", ratio, null))) as recent_ratio by host | where (avg_ratio > 0.9 
AND recent_ratio  0.7)15
   4   
Technique: Metacharacteristics
Example Two
| tstats prestats=t summariesonly=t count(All_Sessions.src_ip) from 
datamodel=Network_Sessions where All_Sessions.src_ip!=All_Sessions.dest_ip All_Sessions.src_ip=* earliest=-30d@d groupby All_Sessions.src_ip _time span=1d | tstats prestats=t append=t summariesonly=t count(All_Sessions.dest_ip) fromdatamodel=Network_Sessions where All_Sessions.src_ip!=All_Sessions.dest_ip All_Sessions.dest_ip=* earliest=-30d@d groupby All_Sessions.dest_ip _time span=1d | rename All_Sessions.src_ip as ip All_Sessions.dest_ip as ip |  bucket _time span=1d | stats count(All_Sessions.src_ip) as initiating count(All_Sessions.dest_ip) as terminating by ip _time | eval isRecent = if(_time>relative_time(now(), "-1d"), "yes", "no") | eval ratio = coalesce(initiating,0) / (coalesce(initiating,0)+coalesce(terminating,0)) | where isnotnull(ratio) |  stats sum(initiating) sum(terminating) avg(eval(if(isRecent="no", ratio, null))) as avg_ratio avg(eval(if(isRecent="yes", ratio, null))) as recent_ratio by ip | where isnotnull(recent_ratio) AND isnotnull(avg_ratio) | where (avg_ratio > 0.9 ANDrecent_ratio  0.2)
15
   5   
Technique: Machine Learning Toolkit Numeric Clustering
Background and Challenges
▶ "I have a bunch of numeric measures and I want to find outliers!"▶ "I am looking for new and relatively unproven ways to hunt!"▶ Let’s be honest: "How do I use this Machine Learning Toolkit?"▶ Splunk’s Machine Learning Toolkit (hereafter referred to as just MLTK) is a great way to build your own Machine Learning (ML) use cases with algorithms that are already written, and packaged in Splunk. 
▶ The benefit is obvious - ML can allow you to do some detections you simply can’t do otherwise. The downside is that there’s a lot more hand waving and magic and uncertainty involved with ML than with normal Splunk.▶ I highly recommend reviewing the Time Series Analysis and First Time Seen 	analysis sections of this doc before diving into this.
Technique: Machine Learning Toolkit Numeric Clustering
Obligatory xkcd #1
https://xkcd.com/1831/https://xkcd.com/1831/
▶ When approaching difficult problems with Machine Learning, remember that they are difficult problems for a reason. ML isn’t a magic wand, ML doesn’t fix problems that you don’t understand. ML’s greatest skill is to take an understood solution that would be impossible with your manpower or existing computation and then scale it higher.Technique: Machine Learning Toolkit Numeric Clustering
Best Known Examples of Security MLTK
▶ Security MLTK adoption has been slower than general MTLK adoption, but we have two examples of using MLTK that seem very solid: supervised detection of malicious domains built by Philipp Drieger out of Munich, and unsupervised clustering of numeric time series data built jointly by US Splunk Security and ML Specialists.▶ Philipp’s Malicious Domain detection is the best example of demonstrating how to use the MLTK that I have ever seen. The core scenario is that we can use domains that are known to be a part of botnets to predict the qualities of future C2 domains for those botnets. Philipp found an open source list of 50k domains with the associated families, and then converted those to "features" (next slide) using eval, URL Toolbox, and also MLTK. Then he tried several clustering algorithms to see which gave him the best accuracy. Once he had the model, he tried it out on a much larger list to track the true/false positive/negative ratios. This is great work that could be used by advanced organizations•	… waiting on link to content … … you can always ask your Splunk team to show it to you!
▶ The Security + ML Specialist teams worked together to identify anomalies in Salesforce.com (SFDC) audit data (which is a decent proxy for any three tier application server). The idea was to detect users who were acting unusually in SFDC, specifically with the indication that they were going to exfiltrate data. This is the scenario that we will go through over the following slides.Technique: Machine Learning Toolkit Numeric Clustering Machine Learning Vocab - Features and and Supervised
▶ There is a slightly different vocabulary when we talk Machine Learning -both IT/Security and Machine Learning are practices that have been 
around for many decades, but they grew up independently and so 
developed their own dialects. So that you can follow Machine Learningdiscussions, here are the key terms:
▶ Entity - whatever it is you’re analyzing. Might be a user, might be a 	computer, might be a virus signature.
▶ Features - these are the fields that you will analyze in your input data. 
▶ Feature Selection - this is the process of deciding what fields to analyze. For example, I checked a few days of SFDC logs and found 1164 fields - how do you decide what will be impactful?▶ Supervised - an algorithm where you are labeling your input data as being "good/bad" or by malware family, etc. Over-simplified version: "Do you have examples of what you want to detect? Supervised."
▶ Unsupervised - an algorithm where you don’t label your input data, and the algorithm itself just comes up with answers. Over-simplified version: "You don’t have examples of what you want to detect? Unsupervised."Technique: Machine Learning Toolkit Numeric Clustering
SFDC Data Example
▶ SFDC has an Event Log File (additional cost) audit log that allows you to see what your users are doing inside of Salesforce. It creates a daily dump listing activities by user. So when you run a query, you can see each request made, each API connection
|| 
 |  | ▶ | 2017-02-06T15:52:09.200+0000 SFDCLogType="DocumentAttachmentDownloads" SFDCLogId="0AT33000000UCXqGAK" SFDCLogDate="2017-02-06T00:00:00.000+0000" TIMESTAMP_DERIVED="2017-02-06T15:52:09.200Z" REQUEST_ID="491x_Vi5XxDS1AVeRhRaXF" 
EVENT_TYPE="DocumentAttachmentDownloads" USER_ID_DERIVED="005400000083CQSAA2" USER_ID="005400000083CQS"ENTITY_ID="01540000000Mc72" FILE_NAME="Splunk - CRM.png" FILE_TYPE="image/png" TIMESTAMP="20170206155209.200" 
ORGANIZATION_ID="00D400000003VqL" | 2017-02-06T15:52:09.200+0000 SFDCLogType="DocumentAttachmentDownloads" SFDCLogId="0AT33000000UCXqGAK" SFDCLogDate="2017-02-06T00:00:00.000+0000" TIMESTAMP_DERIVED="2017-02-06T15:52:09.200Z" REQUEST_ID="491x_Vi5XxDS1AVeRhRaXF"EVENT_TYPE="DocumentAttachmentDownloads" USER_ID_DERIVED="005400000083CQSAA2" USER_ID="005400000083CQS" 
ENTITY_ID="01540000000Mc72" FILE_NAME="Splunk - CRM.png" FILE_TYPE="image/png" TIMESTAMP="20170206155209.200" 
ORGANIZATION_ID="00D400000003VqL" | 2017-02-06T15:52:09.200+0000 SFDCLogType="DocumentAttachmentDownloads" SFDCLogId="0AT33000000UCXqGAK" SFDCLogDate="2017-02-06T00:00:00.000+0000" TIMESTAMP_DERIVED="2017-02-06T15:52:09.200Z" REQUEST_ID="491x_Vi5XxDS1AVeRhRaXF"EVENT_TYPE="DocumentAttachmentDownloads" USER_ID_DERIVED="005400000083CQSAA2" USER_ID="005400000083CQS" 
ENTITY_ID="01540000000Mc72" FILE_NAME="Splunk - CRM.png" FILE_TYPE="image/png" TIMESTAMP="20170206155209.200" 
ORGANIZATION_ID="00D400000003VqL" |
|---|---|---|---|---|---||   | |▶ |2017-02-06T15:52:08.374+0000 SFDCLogType="URI" SFDCLogId="0AT33000000MhY5GQK" SFDCLogDate="2017-02-06T00:00:00.000+0000" DB_BLOCKS="16113" REQUEST_STATUS="S" RUN_TIME="989" USER_ID_DERIVED="005400000083CQSAA2" REFERRER_URI="[...]" URI="/home/home.jsp" URI_ID_DERIVED="" DB_TOTAL_TIME="582637378" USER_ID="005400000083CQS" SESSION_KEY="G/lti8OvlcoIBd1R" CLIENT_IP="[...]" REQUEST_ID="491x_P33DCVcFfVeRhqoSk" DB_CPU_TIME="420" EVENT_TYPE="URI" LOGIN_KEY="aGsG3ecCQGKmHgdL" TIMESTAMP_DERIVED="2017-02-06T15:52:08.374Z" ORGANIZATION_ID="00D400000003VqL" TIMESTAMP="20170206155208.374" CPU_TIME="344" |2017-02-06T15:52:08.374+0000 SFDCLogType="URI" SFDCLogId="0AT33000000MhY5GQK" SFDCLogDate="2017-02-06T00:00:00.000+0000" DB_BLOCKS="16113" REQUEST_STATUS="S" RUN_TIME="989" USER_ID_DERIVED="005400000083CQSAA2" REFERRER_URI="[...]" URI="/home/home.jsp" URI_ID_DERIVED="" DB_TOTAL_TIME="582637378" USER_ID="005400000083CQS" SESSION_KEY="G/lti8OvlcoIBd1R" CLIENT_IP="[...]" REQUEST_ID="491x_P33DCVcFfVeRhqoSk" DB_CPU_TIME="420" EVENT_TYPE="URI" LOGIN_KEY="aGsG3ecCQGKmHgdL" TIMESTAMP_DERIVED="2017-02-06T15:52:08.374Z" ORGANIZATION_ID="00D400000003VqL" TIMESTAMP="20170206155208.374" CPU_TIME="344" |2017-02-06T15:52:08.374+0000 SFDCLogType="URI" SFDCLogId="0AT33000000MhY5GQK" SFDCLogDate="2017-02-06T00:00:00.000+0000" DB_BLOCKS="16113" REQUEST_STATUS="S" RUN_TIME="989" USER_ID_DERIVED="005400000083CQSAA2" REFERRER_URI="[...]" URI="/home/home.jsp" URI_ID_DERIVED="" DB_TOTAL_TIME="582637378" USER_ID="005400000083CQS" SESSION_KEY="G/lti8OvlcoIBd1R" CLIENT_IP="[...]" REQUEST_ID="491x_P33DCVcFfVeRhqoSk" DB_CPU_TIME="420" EVENT_TYPE="URI" LOGIN_KEY="aGsG3ecCQGKmHgdL" TIMESTAMP_DERIVED="2017-02-06T15:52:08.374Z" ORGANIZATION_ID="00D400000003VqL" TIMESTAMP="20170206155208.374" CPU_TIME="344" ||   | |▶ |2017-02-06T15:52:05.547+0000 SFDCLogType="Login" SFDCLogId="0AT33000000UhXuGAK" SFDCLogDate="2017-02-06T00:00:00.000+0000" BROWSER_TYPE="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36" REQUEST_STATUS="" USER_ID_DERIVED="005400000083CQSAA2" URI="/index.jsp" DB_TOTAL_TIME="76331254" CLIENT_IP="[myip]" LOGIN_KEY="" SOURCE_IP="[myip]" API_TYPE="" CPU_TIME="44" SESSION_KEY="" RUN_TIME="133" CIPHER_SUITE="ECDHE-RSA-AES256-GCM-SHA384" USER_ID="005400000083CQS" TIMESTAMP_DERIVED="2017-02-06T15:52:05.547Z" API_VERSION="9998.0" REQUEST_ID="491x_Hi2VageXqMf12zgiXpF" USER_NAME="david.veuve" EVENT_TYPE="Login" URI_ID_DERIVED="" TLS_PROTOCOL="TLSv1.2" TIMESTAMP="20170206155205.547"  ORGANIZATION_ID="00D400000003VqL" |2017-02-06T15:52:05.547+0000 SFDCLogType="Login" SFDCLogId="0AT33000000UhXuGAK" SFDCLogDate="2017-02-06T00:00:00.000+0000" BROWSER_TYPE="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36" REQUEST_STATUS="" USER_ID_DERIVED="005400000083CQSAA2" URI="/index.jsp" DB_TOTAL_TIME="76331254" CLIENT_IP="[myip]" LOGIN_KEY="" SOURCE_IP="[myip]" API_TYPE="" CPU_TIME="44" SESSION_KEY="" RUN_TIME="133" CIPHER_SUITE="ECDHE-RSA-AES256-GCM-SHA384" USER_ID="005400000083CQS" TIMESTAMP_DERIVED="2017-02-06T15:52:05.547Z" API_VERSION="9998.0" REQUEST_ID="491x_Hi2VageXqMf12zgiXpF" USER_NAME="david.veuve" EVENT_TYPE="Login" URI_ID_DERIVED="" TLS_PROTOCOL="TLSv1.2" TIMESTAMP="20170206155205.547"  ORGANIZATION_ID="00D400000003VqL" |2017-02-06T15:52:05.547+0000 SFDCLogType="Login" SFDCLogId="0AT33000000UhXuGAK" SFDCLogDate="2017-02-06T00:00:00.000+0000" BROWSER_TYPE="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36" REQUEST_STATUS="" USER_ID_DERIVED="005400000083CQSAA2" URI="/index.jsp" DB_TOTAL_TIME="76331254" CLIENT_IP="[myip]" LOGIN_KEY="" SOURCE_IP="[myip]" API_TYPE="" CPU_TIME="44" SESSION_KEY="" RUN_TIME="133" CIPHER_SUITE="ECDHE-RSA-AES256-GCM-SHA384" USER_ID="005400000083CQS" TIMESTAMP_DERIVED="2017-02-06T15:52:05.547Z" API_VERSION="9998.0" REQUEST_ID="491x_Hi2VageXqMf12zgiXpF" USER_NAME="david.veuve" EVENT_TYPE="Login" URI_ID_DERIVED="" TLS_PROTOCOL="TLSv1.2" TIMESTAMP="20170206155205.547"  ORGANIZATION_ID="00D400000003VqL" ||   | | | | | |
Technique: Machine Learning Toolkit Numeric Clustering Defining the Overall Plan
| 
 |  | ▶ | The first thing you typically need to do with ML is feature selection. Inside of Splunk, we typically do this via stats. I just picked out events that looked interesting. | The first thing you typically need to do with ML is feature selection. Inside of Splunk, we typically do this via stats. I just picked out events that looked interesting. | The first thing you typically need to do with ML is feature selection. Inside of Splunk, we typically do this via stats. I just picked out events that looked interesting. ||---|---|---|---|---|---|
|   | |▶ |The first thing you typically need to do with ML is feature selection. Inside of Splunk, we typically do this via stats. I just picked out events that looked interesting. |The first thing you typically need to do with ML is feature selection. Inside of Splunk, we typically do this via stats. I just picked out events that looked interesting. |The first thing you typically need to do with ML is feature selection. Inside of Splunk, we typically do this via stats. I just picked out events that looked interesting. ||   | |▶ |Then we need to normalize the data. In this case, we want to normalize on a per-user basis, so that we maintain the variation for a single user but overall keep  |Then we need to normalize the data. In this case, we want to normalize on a per-user basis, so that we maintain the variation for a single user but overall keep  |Then we need to normalize the data. In this case, we want to normalize on a per-user basis, so that we maintain the variation for a single user but overall keep  ||   | |▶ |visibility when a user suddenly changes dramatically. The core requirement here is to be able to detect a user who rarely uses SFDC and then starts exporting key  |visibility when a user suddenly changes dramatically. The core requirement here is to be able to detect a user who rarely uses SFDC and then starts exporting key  |visibility when a user suddenly changes dramatically. The core requirement here is to be able to detect a user who rarely uses SFDC and then starts exporting key  ||   | |▶ |contacts, while also detecting a power user who exports all of the contacts and all of the opportunities and etc. There are a few ways to do this, but we will use  |contacts, while also detecting a power user who exports all of the contacts and all of the opportunities and etc. There are a few ways to do this, but we will use  |contacts, while also detecting a power user who exports all of the contacts and all of the opportunities and etc. There are a few ways to do this, but we will use  ||   | |▶ |eventstats to normalize to stdev on a per-user basis.  |eventstats to normalize to stdev on a per-user basis.  |eventstats to normalize to stdev on a per-user basis.  ||   | |▶ |Invariably then, we will want to simplify the number of fields that we are analyzing. Each additional field adds a lot of compute time for the down-stream analysis.  |Invariably then, we will want to simplify the number of fields that we are analyzing. Each additional field adds a lot of compute time for the down-stream analysis.  |Invariably then, we will want to simplify the number of fields that we are analyzing. Each additional field adds a lot of compute time for the down-stream analysis.  ||   | |▶ |The most common way to do that is with an algorithm called PCA (Principal Component Analysis). PCA will take multiple fields (in this case 11) and compress them  |The most common way to do that is with an algorithm called PCA (Principal Component Analysis). PCA will take multiple fields (in this case 11) and compress them  |The most common way to do that is with an algorithm called PCA (Principal Component Analysis). PCA will take multiple fields (in this case 11) and compress them  ||   | |▶ |down into X number of fields (in this case 5). This is a lossy compression, so we do lose some detail in the variability, but it tries to capture how much the fields move. You can think of this as compressing a music file - you can use FLAC and it will take more time and more space but lose no quality, you can use a 256 bit  MP3 and you will lose little quality but compress well, or you can use 32 bit MP3 and takes little space but sounds awful. We want to shoot for 256 bit MP3. |down into X number of fields (in this case 5). This is a lossy compression, so we do lose some detail in the variability, but it tries to capture how much the fields move. You can think of this as compressing a music file - you can use FLAC and it will take more time and more space but lose no quality, you can use a 256 bit  MP3 and you will lose little quality but compress well, or you can use 32 bit MP3 and takes little space but sounds awful. We want to shoot for 256 bit MP3. |down into X number of fields (in this case 5). This is a lossy compression, so we do lose some detail in the variability, but it tries to capture how much the fields move. You can think of this as compressing a music file - you can use FLAC and it will take more time and more space but lose no quality, you can use a 256 bit  MP3 and you will lose little quality but compress well, or you can use 32 bit MP3 and takes little space but sounds awful. We want to shoot for 256 bit MP3. ||   | |▶ |Now we’re ready to get really data-sciency. It’s time to use k-means to cluster our data together. What we will end up with is a few very large clusters with most of our data points - we can think of these clusters as "people acting normally." Then we will run into a few nodes that are technically a part of that cluster but very very far from the cluster center - those are interesting. Also interesting are very small clusters (only a few data points), as they’re by definition  anomalous. Notably, if you talk to data scientists (if you *are* a data scientist!) one of the most common questions about this approach is "how do you decide on your k." K-means groups things into k clusters - you have to tell it how  many clusters to make. We jointly decided on 5 for this use case, with the option to further tune. For more, see "Downsides to Building it Yourself" at the end of this section. |Now we’re ready to get really data-sciency. It’s time to use k-means to cluster our data together. What we will end up with is a few very large clusters with most of our data points - we can think of these clusters as "people acting normally." Then we will run into a few nodes that are technically a part of that cluster but very very far from the cluster center - those are interesting. Also interesting are very small clusters (only a few data points), as they’re by definition  anomalous. Notably, if you talk to data scientists (if you *are* a data scientist!) one of the most common questions about this approach is "how do you decide on your k." K-means groups things into k clusters - you have to tell it how  many clusters to make. We jointly decided on 5 for this use case, with the option to further tune. For more, see "Downsides to Building it Yourself" at the end of this section. |Now we’re ready to get really data-sciency. It’s time to use k-means to cluster our data together. What we will end up with is a few very large clusters with most of our data points - we can think of these clusters as "people acting normally." Then we will run into a few nodes that are technically a part of that cluster but very very far from the cluster center - those are interesting. Also interesting are very small clusters (only a few data points), as they’re by definition  anomalous. Notably, if you talk to data scientists (if you *are* a data scientist!) one of the most common questions about this approach is "how do you decide on your k." K-means groups things into k clusters - you have to tell it how  many clusters to make. We jointly decided on 5 for this use case, with the option to further tune. For more, see "Downsides to Building it Yourself" at the end of this section. ||   | |▶ |To figure out which cluster points we actually want to return, we will use Inter-Quartile Range. IQR (detailed in the  |To figure out which cluster points we actually want to return, we will use Inter-Quartile Range. IQR (detailed in the  |To figure out which cluster points we actually want to return, we will use Inter-Quartile Range. IQR (detailed in the  ||   | |▶ |Time Series Analysis section) will help us determine the amount of variation in a typical cluster, and find the outliers  |Time Series Analysis section) will help us determine the amount of variation in a typical cluster, and find the outliers  |Time Series Analysis section) will help us determine the amount of variation in a typical cluster, and find the outliers  ||   | |▶ |(and by how much they’re outliers). It is not heavily swayed by a long tail, so a few very distant outliers won’t affect  |(and by how much they’re outliers). It is not heavily swayed by a long tail, so a few very distant outliers won’t affect  |(and by how much they’re outliers). It is not heavily swayed by a long tail, so a few very distant outliers won’t affect  ||   | |▶ |how it views the core group. See that screenshot of a 3D scatterplot of our dataset? We want the Green distant dot,  |how it views the core group. See that screenshot of a 3D scatterplot of our dataset? We want the Green distant dot,  |how it views the core group. See that screenshot of a 3D scatterplot of our dataset? We want the Green distant dot,  ||   | |▶ |and probably some of those Purple ones too.  |and probably some of those Purple ones too.  |and probably some of those Purple ones too.  ||   | |▶ |Finally we will use | where to look for either small clusters, or nodes that are very far from their cluster. For the former, we will have a static threshold for how many members in a cluster counts as "small" and for the latter we will use a  coefficient that we can tune based on the results we are seeing. |Finally we will use | where to look for either small clusters, or nodes that are very far from their cluster. For the former, we will have a static threshold for how many members in a cluster counts as "small" and for the latter we will use a  coefficient that we can tune based on the results we are seeing. |Finally we will use | where to look for either small clusters, or nodes that are very far from their cluster. For the former, we will have a static threshold for how many members in a cluster counts as "small" and for the latter we will use a  coefficient that we can tune based on the results we are seeing. ||   | | | | | |
Technique: Machine Learning Toolkit Numeric Clustering
Building the Base Dataset
▶ Depending on who you ask, 50-90% of a Data Scientist’s time is spent collecting, ETLing and 	formatting data. Splunk makes that exceedingly easy.
▶ Below I parsed through SFDC data to pull out the individual fields I felt most likely return Security 	Value. I then ran an | outputlookup so that I could feed it to the ML algorithm repeatedly.index=sfdc 
| bucket _time span=1d 
| stats dc(eval(if(like(URI_ID_DERIVED, "00140000%"), URI_ID_DERIVED, null))) as NumAccounts 
	dc(eval(if(like(URI_ID_DERIVED, "0063300%"), URI_ID_DERIVED, null))) as NumOpts 
	sum(ROWS_PROCESSED) as ROWS_PROCESSED 
	count(eval(EVENT_TYPE="Login")) as Logins 
	count(eval(EVENT_TYPE="Report")) as ReportsIssuedcount(eval(EVENT_TYPE="API" OR EVENT_TYPE="BulkApi" OR EVENT_TYPE="RestApi")) as APICalls 	sum(DB_CPU_TIME) as DB_CPU_Time 
	sum(RUN_TIME) as RUN_TIME 
	sum(DB_BLOCKS) as db_blocks 
	dc(CLIENT_IP) as UniqueIPs 
	dc(ORGANIZATION_ID) as NumOrganizations 
	dc(ENTRY_POINT) as ApexExecution_Entry_Type 
by USER_ID _time 
| outputlookup sfdc_aggregated_data.csvTechnique: Machine Learning Toolkit Numeric Clustering Running the Detection
▶ Now we actually do our ML!
| inputlookup sfdc_aggregated_data.csv 
| eventstats avg(*) as AVG_* stdev(*) as STDEV_* by USER_ID | foreach * [ eval "Z_>" = ('>' - 'AVG_>' ) / 'STDEV_>'] | fields - AVG_* STDEV_*  | fillnull
	We start with the lookup just createdWe start with the lookup just created 
Then we use eventstats and foreach to convert every 	numeric field to a Z score (how many stdev away 	from avg it is), normalizing per user 
| fit PCA k=5 Z_* 	PCA lets us reduce from 11 fields to 5 fields
| fit KMeans k=5 PC_* 
| eventstats max(clusterDist) as maxdistance p25(clusterDist) as p25_clusterDist p50(clusterDist) as p50_clusterDist p75(clusterDist) as p75_clusterDist dc(USER_ID) as NumIDs count as NumEntries by cluster| eval MaxDistance_For_IQR= (p75_clusterDist + 
	12 * (p75_clusterDist - p25_clusterDist))
| where NumEntries  MaxDistance_For_IQR
	K-means clusters the PCA output 
Then we use eventstats again to determine the Inter-Quartile Range of our data points versus the clusters 	that k-means just found. 
Notably, even with IQR you decide on some noise filter. Here we use 12 IQRs, a common base is 1.5. Finally, filter for the results we want to see.Technique: Machine Learning Toolkit Numeric Clustering
Obligatory xkcd #2
▶ Remember to beware of relying on any analysis that you don’t understand. That doesn’t mean that you shouldn’t rely on it, but not blindly. Two ways in which this applies.
▶ If you are not a PhD with a solid understanding of how Machine Learning actually works, you should consult one before building your primary detection mechanisms on ML (I personally like to augment with ML, rather than rely on ML).▶ If you have one working example that you feel very 
comfortable with, and are then going to apply it to another, make sure that second use case really resembles the first. Faulty underlying assumptions will doom any project.
▶ Tangentially: these rules also apply to data scientists. For example, Deep Learning is a new hot trend which is even deeper linear algebra that is far more difficult to detect. You may get ML intuitively, but you could still be just playing guesswork for Deep Learning. Know your limits.https://xkcd.com/1831/
Technique: Machine Learning Toolkit Numeric Clustering
To Hunt or To Alert
▶ When I describe this use case to people, I usually tell them that it’s more appropriate for hunting than for alerting, at this 	point in time.
▶ The primary reason is that this is a relatively untested scenario. We built this in the lab, and we’ve seen some value in realistic datasets, but we don’t know how it will work in the field over hundreds of customers, like we do with the Time Series or First Time Seen analysis.▶ That said, there is a big secondary reason: this event is harder for analysts to understand. This is an endemic problem in ML detections (something we work very hard to overcome in UBA). Consider the data at the bottom - this is for a user / day who was most anomalous out of 140k users/days. If you were a SOC analyst, what would you do with this alert?▶ With all new categories of detection, they will usually start by being viewed by Use Case Dev / CERT / Hunt Teams, and then progress to Tier 3, Tier 2, before eventually Tier 1. I would recommend being conservative with this particular technique for now. 
Technique: Machine Learning Toolkit Numeric Clustering
Downsides to Building It YourselfDownsides to Building It Yourself
▶ While we did work with a data scientist to build out this model, there are *many* untested assumptions here:
•	That we are including the fields that will get us what we really care about
•	That eventstats + stdev is the right way to build a per-entity baseline
•	That 5 fields for PCA is the right number for our data source•	That k-means is the right clustering mechanism (between core and MLTK Splunk supports four of them!)
•	That 5 clusters is the right number of clusters
•	That no scale limits (e.g., max 100k) results are being seen by MLTK
•	That 12 IQRs are the "right" number
▶ Ultimately there is no right or wrong answer to most of these things, but there are "more right" or "more wrong" answers.By leveraging resources available, we were able to come up with something that seems reasonable, but ultimately it depends on your data and whether you’re getting valuable results. Much like with the discussion in Time Series Analysis, with Security we are not trying to get extreme precision; we’re trying to get in the general ballpark so that we can then focus on the things we most care about. That means that some room for error is absolutely expected.▶ However, when building out very generalizable scenarios, we recommend leveraging the work of actual data scientists. In the Splunk universe, that means Splunk UBA. You can then focus your efforts on building out use cases that aren’t universal to everyone. 
Technique: Machine Learning Toolkit Numeric Clustering
Other Ideas around MLTK from our ML Experts▶ For time series data you often want to build a baseline over some period of time and have that baseline update 
(scheduled search)
•
•
•
•
• | stats avg() as AVG stdev() as STDEV var() as VAR etc by date_hour,date_mday,ID |outputlookup mybaseline.csv 
(advanced) maintain list of holidays as date_mday,isHoliday 
(advanced) remove outliers(advanced) remove outliers 
(advanced) repeat with streamstats as needed (Span, global=f, current=f etc) , like https://wiki.splunk.com/Community:Plotting_a_linear_trendline with streamstats window=100 and you get a rolling R^2 between two time series. Wheeee.
(advanced) | calculate kurtosis, skewness, etc to represent the shape of the distribution▶ in another search that you need to have run in a short time (ie not looking over a long window of time)
•	| lookup mybaseline.csv date_hour as date_hour date_mday as date_mday ID as ID
•	| lookup holidays.csv date_mday as date_mday