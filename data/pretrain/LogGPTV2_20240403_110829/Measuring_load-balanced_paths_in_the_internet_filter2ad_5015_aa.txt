title:Measuring load-balanced paths in the internet
author:Brice Augustin and
Timur Friedman and
Renata Teixeira
Measuring Load-balanced Paths in the Internet
Brice Augustin, Timur Friedman, and Renata Teixeira
Laboratoire d’Informatique de Paris 6 (LIP6)
Université Pierre et Marie Curie and CNRS
ABSTRACT
Tools to measure internet properties usually assume the ex-
istence of just one single path from a source to a destination.
However, load-balancing capabilities, which create multiple
active paths between two end-hosts, are available in most
contemporary routers. This paper proposes a methodol-
ogy to identify load-balancing routers and characterize load-
balanced paths. We enhance our traceroute-like tool, called
Paris traceroute, to ﬁnd all paths between a pair of hosts,
and use it from 15 sources to over 68 thousand destinations.
Our results show that the traditional concept of a single
network path between hosts no longer holds. For instance,
39% of the source-destination pairs in our traces traverse a
load balancer. Furthermore, this fraction increases to 70%
if we consider the paths between a source and a destination
network.
Categories and Subject Descriptors: C.2 [Computer
Communication Networks]: Network Operations; Network
Architecture and Design
General Terms: Measurement.
Keywords: traceroute, load balancing, multipath, path di-
versity.
1.
INTRODUCTION
The traditional model of the internet assumes just one
single path between a pair of end-hosts at any given time.
Internet applications, network simulation models, and mea-
surement tools work under this assumption. However, most
commercial routers have load balancing capabilities [1, 2]. If
network administrators turn on this feature, then a stream
of packets from a source to a destination will no longer fol-
low a single path. This paper reports on a measurement
study of load-balanced paths in the internet, which should
prompt the research community to revisit the concept of an
“internet path”.
Load balancing routers (or load balancers) use three dif-
ferent algorithms to split packets on outgoing links: per des-
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’07, October 24-26, 2007, San Diego, California, USA.
Copyright 2007 ACM 978-1-59593-908-1/07/0010 ...$5.00.
tination, which forwards all packets destined to a host to the
same output interface (similar to the single-path destination-
based forwarding of classic routing algorithms, but this tech-
nique assigns each IP address in a preﬁx to an outgoing in-
terface); per ﬂow, which uses the same output interface for
all packets that have the same ﬂow identiﬁer (described as
a 5-tuple: IP source address, IP destination address, proto-
col, source port, and destination port); or per packet, which
makes the forwarding decision independently for each packet
(and which has potentially detrimental eﬀects on TCP con-
nections, as packets from the same connection can follow dif-
ferent paths and be reordered). Our earlier work [3] showed
that it is possible to control the paths that packets take un-
der per-ﬂow load balancing by controlling the ﬂow identiﬁers
in packet headers. Our tool, Paris traceroute, controls paths
in just this way.
This paper uses Paris traceroute to perform the ﬁrst mea-
surement study of load-balanced paths in the internet. We
make the following contributions:
1. A tool to expose load-balanced paths. Paris
traceroute’s Multipath Detection Algorithm (MDA) ﬁnds,
with a high degree of conﬁdence, all paths from a
source to a destination under per-ﬂow and per-packet
load balancing. We extend it here to cover per-desti-
nation load balancing.
2. A characterization of load-balanced paths be-
tween 15 sources and over 68 thousand destina-
tions. We quantify the load-balanced paths observed
from the RON nodes [4] to a large number of desti-
nations.
In our data set, the paths between 39% of
source-destination pairs traverse a per-ﬂow load bal-
ancer, and 70% traverse a per-destination load bal-
ancer. We characterize these paths in terms of their
length, width, and asymmetry.
3. A methodology to measure RTTs of load-bal-
anced paths. RTT measurements take into account
the delays on both the forward and the return paths,
and therefore could be aﬀected by a load balancer in
any of the paths. We develop a technique to also main-
tain the ﬂow identiﬁer on the reverse path for a more
accurate comparison of RTTs.
This paper proceeds as follows. After a discussion of previ-
ous work in Sec. 2, Sec. 3 presents our tool to measure load-
balanced paths under each type of load balancer. Sec. 4,
describes our measurement setup and characterization met-
rics. Sec. 5 characterizes the load balancers found in our
traces, and Sec. 6 studies the properties of load-balanced
paths. Sec. 7 characterizes round-trip times under per-ﬂow
load balancing. Sec. 8 ends this paper with a discussion of
the implications of our ﬁndings.
2. RELATED WORK
Early work on path diversity in the internet [5, 6] looked
at the known topology of the large internet service provider
(ISP) Sprint and the paths between points of presence (PoPs)
in Sprint’s network. It found that between any given pair
of PoPs there were typically several link-disjoint and sev-
eral PoP-disjoint paths. It also looked at topologies inferred
from traceroute-style probing conducted by Rocketfuel [7]
and CAIDA [8], concluding that, while there is evidence of
signiﬁcant path diversity in the core of the network, the
measurements are particularly sensitive to errors that were
inherent to active probing techniques at that time. Fur-
thermore, when looking at path diversity in the router-level
graph, the measurements are sensitive to insuﬃciencies in
alias resolution techniques, which infer router-level graphs
from IP-level information. The traceroute errors, of missing
links and the inference of false links, have since been largely
corrected by Paris traceroute, which we use here. We work
purely at the IP-level, making no attempt to resolve the
router-level graph. In this work, we observe the actual load-
balanced paths taken by packets, rather than looking at the
overall topology as an undirected graph in which packets
could take any imaginable path.
A typical ISP builds redundancy into its physical infras-
tructure. To use the infrastructure eﬃciently, the ISP will
split traﬃc load across multiple links, which introduces much
of the path diversity that we measure here. The research
community has looked at the question of how best to design
load-balancing routers, for instance to adaptively split the
traﬃc according to network conditions [9, 10, 11, 12]. We
have not systematically looked for adaptive load balancing,
but our familiarity with our own data leads us to believe
that most current routers use a static mapping of ﬂows to
load-balanced paths. Other studies focus on the network
operator’s interest in path diversity. Giroire et al. [13] show
how to exploit an ISP’s underlying physical diversity in or-
der to provide robustness at the IP layer by having as many
disjoint paths as possible.
Commercial interests guide today’s internet routing poli-
cies in ways that often yield inferior end-to-end paths, as
measured by delay, loss rate, or bandwidth. Savage et al. [14]
demonstrated that alternate paths, taking two or more end-
to-end hops between hosts, could often outperform the de-
fault direct paths provided by the internet. Andersen et
al. [4] have since proposed RON, an overlay network that
exploits this insight. As opposed to looking at the diver-
sity that can be obtained by composing multiple end-to-end
paths, our work examines the diversity that exists in what
has previously been regarded as individual end-to-end paths.
3. MEASURING LOAD-BALANCED PATHS
This section describes the Multipath Detection Algorithm
(MDA), which Paris traceroute uses to discover load-balanced
paths.1 Sec. 3.1 describes our prior work [15] on enumerat-
ing all paths between a source and a destination in the pres-
1Note that our technique detects load sharing performed
by routers.
It is not our goal to measure load balancing
ence of per-ﬂow load balancing. Then, Sec. 3.2 introduces a
simple extension that allows the MDA to take into account
per-destination load balancers.
3.1 The Multipath Detection Algorithm
Our initial work on Paris traceroute [3] largely ﬁxed the
problem of the false paths reported by classic traceroute.
The problem was that classic traceroute systematically varies
the ﬂow identiﬁer for its probe packets. By maintaining
a constant ﬂow identiﬁer, Paris traceroute can accurately
trace a path across a per-ﬂow load balancer. However, this
early version only traced one path at a time.
Our subsequent work [15] suggested a new goal for route
tracing: to ﬁnd the entire set of load-balanced paths from
source to destination. We showed that the classic tracer-
oute practice of sending three probes per hop is inadequate
to have even a moderate level of conﬁdence that one has
discovered load balancing at a given hop. Our Multipath
Detection Algorithm (MDA) uses a stochastic approach to
send a suﬃcient number of probes to ﬁnd, to a given degree
of conﬁdence, all the paths to a destination.
The MDA proceeds hop by hop, eliciting the full set of
interfaces for each hop. For a given interface r at hop h−1, it
generates at random a number of ﬂow identiﬁers and selects
those that will cause probe packets to reach r. It then sends
probes with those identiﬁers, but one hop further, in an
eﬀort to discover the successors of r at hop h. We call this
set of interfaces, s1, s2, . . . , sn the nexthops of r.
If we make the hypothesis that r is the interface of either
a per-ﬂow or a per-packet load balancer that splits traﬃc
evenly across n paths, we can compute the number of probes
k that we must send to hop h to reject this hypothesis with a
degree of conﬁdence (1−α)×100%. If the MDA does not ﬁnd
n interfaces, it stops. Otherwise, it hypothesizes that there
are n + 1 nexthop interfaces, and sends additional probes.
To rule out the initial hypothesis that n = 2, the MDA,
operating at a 95% level of conﬁdence, sends k = 6 probes.
As we have seen load balancing across as many as 16 load-
balanced interfaces, the MDA may ultimately need to send a
total of k = 96 probes to ﬁnd all the nexthops of an interface
r.
If the MDA discovers more than one nexthop interface, it
sends additional probes so as to classify r as belonging to
either a per-ﬂow or a per-packet load balancer.
It makes
the hypothesis of per-packet load balancing and attempts
to disprove this by sending a number of probes, all with the
same ﬂow identiﬁer that is known to reach r. If two diﬀerent
interfaces respond, the hypothesis is conﬁrmed. Six probes
are required, all returning the same nexthop interface, to
reject the hypothesis with a 95% level of conﬁdence, and
conclude that r belongs to a per-ﬂow load balancer. If no
per-packet load balancers are encountered, once the desti-
nation is reached, the MDA has eﬀectively enumerated all
of the end-to-end paths. If there were per-packet load bal-
ancers, the MDA will not be able to discover all of the true
paths, but it will nonetheless have found all the interfaces
at each hop and been able to trace those parts of the paths
that are not aﬀected by per-packet load balancing.
3.2 Extending the MDA
When tracing towards a single destination with the MDA,
at server farms, where dedicated boxes distribute incoming
requests to a set of replicated servers.
Paris traceroute is naturally incapable of detecting instances
of per-destination load balancing. In Fig. 1, for example, L
might be a per-destination load balancer, sending traﬃc des-
tined for T along the upper path, and traﬃc for T (cid:2)
along the
lower path. When Paris traceroute uses the MDA to trace
to T , it only discovers the upper path. We generalize the
TTL = 10
TTL = 9
TTL = 8
TTL = 7
1
0
0
A
1
0
L
2
0
B
Hop #6 #7
1
0
Traceroute outcome to T:
0A
D
0
L0
T
E0
1
1
C
D
#8
0
1
2
E
#9
T
T’
Figure 1: Traceroute and per-destination load bal-
ancing
MDA to enumerate all of the paths from a source to a given
address preﬁx rather than simply to a given destination. In
this example, the generalized MDA detects both paths, and
ﬂags L0 as the interface of a per-destination load balancer.
We achieve this goal by reﬁning the techniques previously
described. When testing the hypothesis that there are n
nexthops for an interface r, the MDA initially chooses ﬂow
identiﬁers that diﬀer only in their destination address.
It
chooses destination addresses that share a long preﬁx with
the destination of interest. Two addresses sharing a preﬁx
longer than /24 are unlikely to have diﬀerent entries in a
core router, so any path diﬀerences should purely be the
result of load balancing. The MDA initially chooses ad-
dresses that share a /29 preﬁx, allowing the choice of up to
8 diﬀerent addresses. As before, the MDA sends anywhere
from 6 to 96 probes (for the 95% conﬁdence level) one hop
past r in order to enumerate its nexthops. If 96 diﬀerent
destination addresses are required, they can all share a /25
preﬁx. As this nexthop enumeration technique is designed
to work when r belongs to a per-destination load balancer,
it will a fortiori also work when r belongs to a per-ﬂow or
a per-packet load balancer.
Then, if the MDA has found two or more nexthop inter-
faces, it hypothesizes, as before, that r belongs to a per-
packet load balancer. For the 95% conﬁdence level, it sends
up to 6 probes with the same ﬂow identiﬁer, and rules out
the hypothesis only if all 6 probes go to the same nexthop
interface. If it rules out per-packet load balancing, then the
extended MDA hypothesizes that r belongs to a per-ﬂow
load balancer, and again sends up to 6 probes. These all go
towards the same destination, but with ﬂow identiﬁers that
nonetheless diﬀer. (The MDA varies the destination port
number.) Only if all of these probes go to the same nexthop
interface does the MDA conclude that r is a per-destination
load balancer.
3.3 Limitations
The Multipath Detection Algorithm may return inaccu-
rate results in the following cases:
MPLS: MPLS represents a challenge for all traceroute-
like measurements, because some ISP networks deploy MPLS
tunnels in which routers do not necessarily decrement the
IP TTL of packets. Under this conﬁguration, the TTL will
never expire while in a tunnel and traceroute will observe
the path through the tunnel as a single link, causing an un-
derestimation of the length of load-balanced paths. Further-
more, if a load balancer splits traﬃc across several MPLS
paths sharing the same entry and exit points, the MDA will
not detect it.
Nonresponding routers: When routers do not respond
to probes even after retransmissions, we cannot accurately
enumerate a given nexthop set. This is a fundamental limit
to traceroute-style measurements, and the amount of load
balancing will be underestimated in these instances.
Uneven load balancing: If a load balancer distributes
load with nonuniform probability across its nexthop inter-
faces, the algorithm risks not discovering a low-probability
nexthop interface. The solution, if we expect probabilities
to be possibly skewed up to some maximum extent, is to
send more probes, in order to regain the desired degree of
conﬁdence. Despite having seen some examples in which a
router does not distribute load evenly, our informal expe-
rience tells us that this is rare, and we have not adjusted
the MDA to catch all such cases, leading to another small
source of under-estimation of load-balanced paths.
Routing changes: Routing changes during a traceroute
can lead to the inference of false links. They may cause
an overestimation of load balancing, or the incorrect clas-
siﬁcation of a routing change as per-packet load balancing.
Fortunately, routing changes are relatively infrequent [16],
especially on the time scale of an individual route trace. The
MDA could potentially reprobe a path to try to determine
if the route has changed, but we do not currently implement
such an extension.
4. METHODOLOGY
This section describes our measurement setup and intro-
duces the metrics we use to characterize load balancing.
4.1 Measurements
We ran our measurements from 15 sources: 13 RON nodes