(d) Places20
Figure 11: The performance of attribute inference attacks against contrastive models on 4 different datasets under attack
models with different layers. The x-axis represents attack models’ layers. The y-axis represents attribute inference attacks’
accuracy.
To study the effect of training dataset size on the attack model
AAttInf, we randomly select from 10% to 90% of the training dataset
to train the attack model and evaluate the performance using all the
testing dataset; the results for contrastive models are summarized
in Figure 10. By jointly considering Figure 8 and Figure 10, we can
observe that, in most of the cases, even using 10% of the training
dataset, the contrastive models are still more vulnerable to attribute
inference attack than the supervised models when the attack model
is trained with its full training dataset. On the other hand, the attack
performance on supervised models is not significantly influenced
by the training dataset size (see Figure 18). This further shows the
privacy risks of contrastive learning.
Recall our attack model is a 3-layer MLP. We further investigate
whether more complex attack models would improve the attack
performance. To this end, we increase the attack model’s layer from
3 to 6 and summarize the corresponding attack performance for
contrastive and supervised models in Figure 11 and Figure 19 (in
Appendix), respectively. The results show that 3-layer attack models
can achieve the best performance in most of the cases. With more
layers, the attack performance may degrade or keep stable, which
indicates that even simple models are enough to launch effective
attacks. This further shows that informative representations learned
by contrastive models can be easily exploited by the adversary to
infer samples’ attributes.
We also observe that attribute inference attacks over contrastive
models are more effective against smaller embedding size (see Fig-
ure 8 and Figure 10). For instance, ResNet-18 (512) leak more in-
formation than MobileNetV2 (1,280) and ResNet-50 (2,048). We
conjecture that a larger embedding size represents each sample in
a more complex space in the contrastive setting, which is harder
for the attack model to decode. However, the effect of embedding
size on attribute inference attacks against the supervised models
is less pronounced (see Figure 8 and Figure 18 in the Appendix).
MaleFemaleWhiteBlackAsianIndianOtherMaleFemaleWhiteBlackAsianIndianOther10%30%50%70%90%PercentageofTrainingData0.50.60.7AccuracyMobileNetV2ResNet-18ResNet-5010%30%50%70%90%PercentageofTrainingData0.050.100.15Accuracy10%30%50%70%90%PercentageofTrainingData0.10.20.3Accuracy10%30%50%70%90%PercentageofTrainingData0.20.30.40.5Accuracy3456#.LayersofAttackModel0.00.20.40.6AccuracyMobileNetV2ResNet-18ResNet-503456#.LayersofAttackModel0.000.050.100.150.20Accuracy3456#.LayersofAttackModel0.00.10.20.30.4Accuracy3456#.LayersofAttackModel0.00.20.4AccuracySession 3C: Inference Attacks CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea853This further shows the difference between supervised models and
contrastive models with respect to representing samples.
In conclusion, contrastive models are more vulnerable to at-
tribute inference attacks compared to supervised models.
5 DEFENSE
So far, we have demonstrated that compared to supervised mod-
els, contrastive models are more vulnerable to attribute inference
attacks (Section 4) but less vulnerable to membership inference
attacks (Section 3). In this section, we propose the first privacy-
preserving contrastive learning mechanism, namely Talos, which
aims to reduce the risks of attribute inference for contrastive models
while maintaining their membership privacy and model utility.
5.1 Methodology
Intuition. As shown in Section 4, the reason for a contrastive
model to be vulnerable to attribute inference attacks is that the
model’s base encoder 𝑓 learns informative representations for data
samples, which can be exploited by an adversary. To mitigate such a
threat, we aim for a new training paradigm for contrastive learning
which can eliminate data samples’ sensitive attributes from their
representations. Meanwhile, the base encoder of the contrastive
model still needs to represent data samples expressively for pre-
serving model utility. These two objectives are in conflict, and our
defense mechanism should consider both simultaneously.
Methodology. Our defense mechanism, namely Talos, can be
modeled as a mini-max game, and we rely on adversarial train-
ing [12–14, 17, 65] to realize it. Similar to the original contrastive
model, Talos also leverages a base encoder and a projection head
to learn informative representations for data samples. Besides, Ta-
los introduces an adversarial classifier 𝐶, which is used to censor
sensitive attributes from data samples’ representations.
The adversarial classifier of Talos is essentially designed for at-
tribute inference. Similar to the original contrastive learning process
(see Section 2), Talos is trained with mini-batches. Given a mini-
batch of 2𝑁 augmented data samples (generated from 𝑁 original
samples), we define the loss of the adversarial classifier 𝐶 as follows.
L𝐶 =
[LCE(𝑠𝑘, 𝐶(𝑓 ( ˜𝑥2𝑘−1))) + LCE(𝑠𝑘, 𝐶(𝑓 ( ˜𝑥2𝑘)))]
𝑁
(7)
1
2𝑁
𝑘=1
where ˜𝑥2𝑘−1 and ˜𝑥2𝑘 are the two augmented samples of an original
sample 𝑥𝑘, 𝑠𝑘 represents 𝑥𝑘’s sensitive attribute, 𝑓 is the base en-
coder, and LCE is the cross-entropy loss (Equation 3). We consider
˜𝑥2𝑘−1 and ˜𝑥2𝑘 sharing the same sensitive attribute as 𝑥𝑘. Note that
we take the output of the base encoder instead of the projection
head as the input to the adversarial classifier. Since the projection
head is discarded after the first phase of training the contrastive
model, directly optimizing the base encoder with the adversarial
classifier loss would maintain the effect of adversarial training.
Talos also adopts the original contrastive loss LContrastive (Equa-
tion 6). By jointly considering the adversarial classifier loss and the
contrastive loss, Talos’s loss function is defined as follows:
LTalos = LContrastive − 𝜆L𝐶
(8)
where 𝜆 is the adversarial factor to balance the two losses. We refer
to a model trained with Talos as a Talos model.
Algorithm 1: The training process of Talos.
1 Input: Target training dataset Dtrain
target with sensitive
attribute 𝑠, base encoder 𝑓 , projection head 𝑔, adversarial
classifier 𝐶, and adversarial factor 𝜆.
2 Initialize 𝑓 , 𝑔, and 𝐶’s parameters.
3 for each epoch do
4
5
for each mini-batch do
Sample a mini-batch with 𝑁 training data samples
and its corresponding sensitive attributes
{(𝑥1, 𝑠1), (𝑥2, 𝑠2), ..., (𝑥𝑁 , 𝑠𝑁)} from Dtrain
target
Generate augmented data samples:
{( ˜𝑥1, 𝑠1), ( ˜𝑥2, 𝑠1), ..., ( ˜𝑥2𝑁 , 𝑠𝑁)}, where ˜𝑥2𝑘−1 and
˜𝑥2𝑘 are the two augmented views of 𝑥𝑘
Feed augmented data samples into the base encoder
𝑓 and the projection head 𝑔 to calculate the
contrastive loss:
LContrastive = 1
2𝑁
Feed the representations generated by the base
𝑁
encoder 𝑓 into the adversarial classifier 𝐶 to
calculate the adversarial classifier loss:
𝑘=1[LCE(𝑠𝑘, 𝐶(𝑓 ( ˜𝑥2𝑘−1))) +
L𝐶 = 1
2𝑁
LCE(𝑠𝑘, 𝐶(𝑓 ( ˜𝑥2𝑘)))]
if epoch mod 2 ≠ 0 then
𝑁
𝑘=1[ℓ(2𝑘−1, 2𝑘)+ℓ(2𝑘, 2𝑘−1)]
Optimize adversarial classifier 𝐶’s parameters
with the adversarial classifier loss: L𝐶
else
Optimize projection head 𝑔’s parameters with
the contrastive loss: LContrastive
Optimize base encoder 𝑓 ’s parameters with
adversarial training loss:
LTalos = LContrastive − 𝜆L𝐶
6
7
8
9
10
11
12
13
end
end
14
15
16 end
17 Return: Base encoder 𝑓
Algorithm 1 presents the training process of Talos. In each mini-
batch, given 𝑁 training samples, we first generate 2𝑁 augmented
views (Line 6) and feed them into the base encoder. The generated
representations are then fed into the projection head (Line 7) and
the adversarial classifier (Line 8) simultaneously. Note that the
adversarial classifier and contrastive model are updated alternately
by epoch. We First optimize the adversarial classifier with the cross-
entropy loss (Line 10). Then we optimize the projection head with
the contrastive loss (Line 12) and the base encoder with the loss
function of Talos, i.e., Equation 8 (Line 13).
To implement this in practice, we utilize the gradient reversal
layer (GRL) proposed by Ganin et al. [15]. GRL is a layer that can be
added between the base encoder 𝑓 and the adversarial classifier 𝐶.
In the forward propagation, GRL acts as an identity transform that
simply copies the input as the output. During the backpropagation,
GRL takes the gradients passed through it from the adversarial
classifier 𝐶, multiplies the gradients by −𝜆, and passes them to the
base encoder 𝑓 . Such operation lets the base encoder receive the
Session 3C: Inference Attacks CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea854opposite direction of gradients from the adversarial classifier. In this
way, the base encoder 𝑓 is able to learn informative representations
for samples while censoring their sensitive attributes.
Note that our adversarial training is performed only on the pro-
cess of training the base encoder 𝑓 . The training for the classifica-
tion layer of the contrastive model remains unchanged. As we show
in Section 3, the classification layer generalizes well on the con-
trastive models, i.e., less overfitting. Therefore, models trained by
Talos should be robust against membership inference attacks as well.
Our evaluation shows that this is indeed the case (see Figure 13).
Adaptive Attacks. An adversary needs to establish a shadow
model to mount membership inference attacks. To evaluate member-
ship privacy risks of Talos, we consider an adaptive (and stronger)
adversary [28]. Concretely, we assume that the adversary knows
the training details of Talos and trains their shadow model in the
same way. For attribute inference, the attack model is trained on
embeddings generated by Talos, thus, our attribute inference attack
considered in the evaluation of Talos is also adaptive.
5.2 Experimental Setting
We follow the same experimental setting, including datasets, met-
rics, target models, and attack models (both attribute inference
and membership inference), as those in Section 3.3 and Section 4.3.
As mentioned before, both membership inference and attribute
inference attacks are performed in an adaptive way. Regarding the
adversarial classifier of Talos, we leverage a 3-layer MLP with 64
neurons in the hidden layer, which is smaller than the attribute
inference attack model.
Baseline. We consider three state-of-the-art defenses, one for
membership inference (MemGuard [28]) and two for attribute in-
ference (Olympus [46] and AttriGuard [27]) as the baseline models.
MemGuard, Olympus, and AttriGuard are originally designed for su-
pervised models, here, we adapt them to contrastive models. Since
the input to the attribute inference attack is each sample’s represen-
tation, we further consider a sample’s representation as the input
to Olympus and AttriGuard.
MemGuard is a two-phase defense for membership inference.
In phase I, the defender generates a noise vector to perturb the
posteriors of a target sample, so that the adversary’s membership
classifier is likely to give a random guess for the perturbed posteri-
ors. In phase II, the defender adds the noise vector to the posteriors
with certain probability.
Olympus, designed for attribute inference, has three basic com-
ponents: an autoencoder to transfer the original representation into
the perturbed one, a classifier to perform the original task over the
perturbed representation, and an adversarial classifier to infer the
sensitive attribute from the perturbed representation. Olympus op-
timizes the three components using adversarial training to preserve
the model utility while protecting samples’ sensitive attributes. To
perform Olympus on contrastive models, we first train a base en-
coder following the original contrastive learning process. Then, we
add an autoencoder between the base encoder and the classification
layer, and fine-tune the whole model using the original training
samples with Olympus’s losses.
AttriGuard is a two-phase defense for attribute inference. In
phase I, for each representation, the defender generates an adver-
sarial example for each possible value of the sensitive attribute by
adapting the existing evasion attack techniques. In phase II, the
defender samples one sensitive attribute value based on a probabil-
ity distribution and selects the corresponding adversarial example
found in phase I as the new representation.
The adversarial classifier used in AttriGuard and Olympus shares
the same architecutre as the one in Talos. For MemGuard, we follow
Jia et al. [28] to generate the noise in Phase I. For the autoencoder
of Olympus, we set its encoder (decoder) as a 2-layer MLP with 256
and 128 (128 and 256) neurons in the hidden layers. For AttriGuard,
we leverage the C&W attack [6] with the 𝐿𝑖𝑛𝑓 norm in phase I.
5.3 Results
We compare the performance of the original classification tasks, NN-
based membership inference attacks, and attribute inference attacks
for the original contrastive model and the models defended by Talos,
MemGuard, Olympus, and AttriGuard. The results are depicted in
Figure 12, Figure 13, and Figure 14, respectively. Note that we also
perform metric-based and label-only membership inference attacks
and the results are summarized in Figure 20, Figure 21, Figure 22,
Figure 23, and Figure 24 in Appendix.
In Figure 14, we find that Talos indeed reduces the attribute
inference accuracy compared to the original contrastive learning.
For instance, the attribute inference accuracy is 0.701 on the original
contrastive model with ResNet-18 on the UTKFace dataset, while
only 0.602 on the Talos model. Meanwhile, the testing accuracy of
the original classification task for the Talos model is also preserved
(Figure 12).