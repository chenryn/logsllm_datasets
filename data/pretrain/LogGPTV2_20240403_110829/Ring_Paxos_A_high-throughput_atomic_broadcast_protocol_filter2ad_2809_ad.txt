### Impact of Latency and Message Size on Ring Paxos Performance

As the size (payload) of messages increases, there is a slight decrease in latency up to 8 kbyte messages, after which it decreases. This phenomenon can be attributed to the fact that in our prototype, the performance of Ring Paxos is influenced by the size of the application message. Figure 6 quantifies the effects of application message size on the performance of Ring Paxos. The throughput remains relatively constant with the number of receivers, as shown in the top left graph.

### Comparison of Protocols

Figure 6 also highlights the performance differences between protocols based solely on a ring (LCR), those using IP multicast, and those combining both (Ring Paxos). The difference in performance between these groups is approximately one order of magnitude. Specifically, LCR and Ring Paxos present consistent throughput with the number of receivers. However, because Ring Paxos relies on IP multicast, it has lower throughput compared to LCR (as discussed in Section 1).

### Throughput Efficiency of Atomic Broadcast Protocols

The following table summarizes the Maximum Throughput Efficiency (MTE) and message sizes for various atomic broadcast protocols:

| Protocol | MTE (%) | Message Size |
|----------|---------|--------------|
| LCR      | 95      | 32 kbytes    |
| RingPaxos| 90      | 8 kbytes     |
| Spread   | 18      | 16 kbytes    |
| Paxos4sb | 4       | 200 bytes    |
| Libpaxos | 3       | 4 kbytes     |

### Impact of the Number of Processes in the Ring

Next, we examine how the number of processes in the ring affects the throughput and latency of Ring Paxos and LCR (Figure 5). LCR does not distinguish between different roles; all processes must be in the ring. In contrast, Ring Paxos places a majority of acceptors in the ring, which are used in case of failure. The x-axis of the graph shows the total number of processes in the ring for both LCR and Ring Paxos. The remaining processes are spare. Ring Paxos maintains constant throughput with the number of processes in the ring, while LCR's throughput is more variable.

### Message Propagation and CPU Usage

In LCR, each message is sent \( n - 1 \) times, where \( n \) is the number of processes in the ring. This results in higher latencies, typically in the range of 4.2 to 6.2 milliseconds. In Ring Paxos, the message content is propagated only once using IP multicast, leading to lower latencies. The average CPU usage per process in Ring Paxos is minimal, with values remaining low even under maximum load.

### Impact of Socket Buffer Sizes

Figure 7 illustrates the effect of socket buffer sizes on the maximum throughput (left graph) and latency (right graph) of Ring Paxos. We used 16 Mbytes of socket buffer sizes in all previous experiments, as they provide the best tradeoff between throughput and latency. Table 3 shows the CPU and memory usage of Ring Paxos, particularly focusing on the coordinator process, which should both receive and send a large stream of values from the proposers and acceptors.

### Conclusions

This paper presents Ring Paxos, a Paxos-like algorithm designed for high throughput in clustered environments. Our implementation and comparison with other atomic broadcast protocols reveal that Ring Paxos effectively combines the benefits of both ring-based and IP multicast-based approaches, providing high throughput and low latency. The study highlights the tradeoffs in latency and throughput under varying assumptions and suggests that a combination of techniques, such as weak synchronous and asynchronous communication, can lead to optimal performance.

### Acknowledgements

The authors would like to thank Antonio Carzaniga, Leslie Lamport, and Robbert van Renesse for valuable discussions and suggestions about Ring Paxos. Additionally, we appreciate the reviewers' feedback, which helped improve the paper.

### References

[1] Y. Amir, C. Danilov, M. Miskin-Amir, J. Schultz, and J. Stanton. The Spread toolkit: Architecture and performance. Technical report, Johns Hopkins University, CNDS-2004-1, 2004.

[2] Y. Amir, L. Moser, P. Melliar-Smith, D. Agarwal, and P. Ciuffoletti. The Totem single-ring membership protocol. ACM Trans. Comput. Syst., 13(4):311-342, 1995.

[3] K. P. Birman, A. Schiper, and P. Stephenson. Lightweight causal and atomic group multicast. ACM Trans. Comput. Syst., 9(3):272-314, Aug. 1991.

[4] T. Chandra, R. Griesemer, and J. Redstone. Paxos made live: An engineering perspective. In Proceedings of the twenty-sixth annual ACM symposium on principles of distributed computing (PODC), pages 398-407, 2007.

[5] T. D. Chandra and S. Toueg. Unreliable failure detectors for reliable distributed systems. J. ACM, 43(2):225-267, 1996.

[6] F. Cristian and S. Mishra. The Pinwheel asynchronous atomic broadcast protocols. In International Symposium on Autonomous Decentralized Systems (ISADS), Phoenix, Arizona, USA, 1995.

[7] X. Defago, A. Schiper, and P. Urban. Total order broadcast and multicast algorithms: Taxonomy and survey. ACM Computing Surveys, 36(4):372-421, Dec. 2004.

[8] C. Dwork, N. Lynch, and L. Stockmeyer. Consensus in the presence of partial synchrony. J. ACM, 35(2):288-323, 1988.

[9] R. Ekwall, A. Schiper, and P. Urban. Token-based atomic broadcast using unreliable failure detectors. In Proceedings of the International Symposium on Reliable Distributed Systems (SRDS), pages 52-65, 2004.

[10] M. J. Fischer, N. A. Lynch, and M. S. Paterson. Impossibility of distributed consensus with one faulty processor. J. ACM, 32(2):374-382, 1985.

[11] U. Fritzke, P. Ingels, A. Mostefaoui, and M. Raynal. Fault-tolerant total order multicast. In Proceedings of the International Symposium on Reliable Distributed Systems (SRDS), pages 578-585, 1998.

[12] V. Hadzilacos and S. Toueg. Fault-tolerant broadcasts and related problems. In Distributed Systems, 2nd edition, Addison-Wesley, 1993.

[13] M. F. Kaashoek and A. S. Tanenbaum. Group communication in the Amoeba distributed operating system. In 11th International Conference on Distributed Computing Systems (ICDCS), pages 222-230, Washington, USA, 1991.

[14] J. Kim and C. Kim. A total ordering protocol using a dynamic token-passing scheme. Distributed Systems Engineering, 4(2):87-95, 1997.

[15] J. Kirsch and Y. Amir. Paxos for system builders: An overview. In Proceedings of the 2nd Workshop on Large-Scale Distributed Systems and Middleware (LADIS), pages 1-6, 2008.

[16] L. Lamport. The implementation of reliable distributed multiprocess systems. Computer Networks, 2:95-114, 1978.

[17] L. Lamport. Time, clocks, and the ordering of events in a distributed system. Communications of the ACM, 21(7):558-565, 1978.

[18] L. Lamport. The part-time parliament. ACM Transactions on Computer Systems, 16(2):133-169, May 1998.

[19] L. Lamport and M. Massa. Cheap Paxos. In International Conference on Dependable Systems and Networks (DSN), pages 307-314, 2004.

[20] R. Levy. The complexity of reliable distributed storage. PhD thesis, EPFL, 2008.

[21] http://nibpaxos.sourceforge.net.

[22] T. Ng. Ordered broadcasts for large applications. In Symposium on Reliable Distributed Systems (SRDS), pages 188-197, 1991.

[23] F. B. Schneider. What good are models and what models are good? In S. Mullender, editor, Distributed Systems, chapter 2. Addison-Wesley, 2nd edition, 1993.

### Correctness Proof (Sketch)

We provide a proof sketch of the correctness of Ring Paxos, focusing on properties (ii) and (iii) of consensus.

**Property (i):** Trivially holds from the algorithm.

**Property (ii):** No two processes decide different values.
- **Proof Sketch:** Let \( v \) and \( v' \) be two decided values, and \( v-id \) and \( v'-id \) their unique identifiers. Let \( r \) and \( r' \) be the rounds in which some coordinator \( c \) sends a decision message with \( v-id \) and \( v'-id \), respectively. For \( c \) to decide \( v \), it must receive \( f+1 \) messages of the form (Phase lB, r, *, *), select the value \( Vval = v \) with the highest round number \( Vrnd \) among the set \( MlB \) of phase lB messages, and then ip-multicast (Phase 2A, r, v, v-id) and (Phase 2B, r, v-id) messages. Similarly, for \( c \) to decide \( v' \), it must follow the same steps. Since the ring is composed of \( f+1 \) acceptors, \( c \) would send a decide message with \( v-id \) and \( v'-id \). Therefore, \( v-id = v'-id \).

**Property (iii):** If one or more processes propose a value and do not crash, then eventually some value is decided by all correct processes.
- **Proof Sketch:** Consider a correct coordinator \( c \) in a system where all processes are correct. After GST (Global Stabilization Time), all processes exchange messages directly, and eventually, some value is decided. Since Paxos implements consensus, all correct processes will eventually decide some value.

This concludes the proof sketch of the correctness of Ring Paxos.