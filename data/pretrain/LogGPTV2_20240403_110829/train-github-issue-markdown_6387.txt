I have encountered an unusual issue while deploying the DialoGPT Large model and posted it to the HuggingFace forums, but I have not received a response. The problem arises when using `model.save_pretrained` and `tokenizer.save_pretrained` with PyTorch 1.8.0 and Transformers 4.3.3. The exported `pytorch_model.bin` file is almost twice the size of the one in the model card repository, which leads to an out-of-memory (OOM) error on a reasonably equipped machine. However, when I use the standard transformers download process, the model works fine. I am building a CI pipeline to containerize the model, hence the need for pre-populated models.

### Model Card vs. Exported Files

- **Model Card:**
  - `pytorch_model.bin`: 1.6GB

- **Exported Files:**
  - `config.json`: 800B
  - `merges.txt`: 446KB
  - `pytorch_model.bin`: 3.0GB
  - `special_tokens_map.json`: 357B
  - `tokenizer_config.json`: 580B
  - `vocab.json`: 780KB

### Direct Download Errors

When I try to download the model card files directly, I encounter the following errors:

```bash
curl -L https://huggingface.co/microsoft/DialoGPT-large/resolve/main/config.json -o ./model/config.json
curl -L https://huggingface.co/microsoft/DialoGPT-large/resolve/main/pytorch_model.bin -o ./model/pytorch_model.bin
curl https://huggingface.co/microsoft/DialoGPT-large/resolve/main/tokenizer_config.json -o ./model/tokenizer_config.json
curl https://huggingface.co/microsoft/DialoGPT-large/resolve/main/config.json -o ./model/config.json
curl https://huggingface.co/microsoft/DialoGPT-large/resolve/main/merges.txt -o ./model/merges.txt
curl https://huggingface.co/microsoft/DialoGPT-large/resolve/main/special_tokens_map.json -o ./model/special_tokens_map.json
curl https://huggingface.co/microsoft/DialoGPT-large/resolve/main/vocab.json -o ./model/vocab.json
```

The code to load the tokenizer fails with the following traceback:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("model/")
```

Error:
```python
Traceback (most recent call last):
  File "/var/lang/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py", line 395, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/var/lang/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 1788, in from_pretrained
    return cls._from_pretrained(
  File "/var/lang/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 1801, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/var/lang/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 1876, in _from_pretrained
    special_tokens_map = json.load(special_tokens_map_handle)
  File "/var/lang/lib/python3.8/json/__init__.py", line 293, in load
    return loads(fp.read(),
  File "/var/lang/lib/python3.8/json/__init__.py", line 357, in loads
    return _default_decoder.decode(s)
  File "/var/lang/lib/python3.8/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/var/lang/lib/python3.8/json/decoder.py", line 355, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
```

### Questions

1. What could be causing the large file variance between the `save_pretrained` models and the model card repository?
2. Why are the directly downloaded model card files not working in this example?

Thank you in advance for any insights or suggestions.