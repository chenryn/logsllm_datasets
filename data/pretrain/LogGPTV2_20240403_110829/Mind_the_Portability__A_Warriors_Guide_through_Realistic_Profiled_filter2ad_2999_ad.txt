3) The combined effect of different key and different device
is much larger than their sum.
4) Adding more measurements does not necessarily help but,
on average, also does not deteriorate the performance.
5) CNN requires more measurements in portability settings
to build strong models.
6) While our results indicate that additional measurements
in the training phase would be beneﬁcial to reach the full
model capacity, we observe that in portability settings,
there is a signiﬁcant amount of overﬁtting. This represents
an interesting situation where we simultaneously underﬁt
on training data and overﬁt on testing data.
7) The overﬁtting occurs because we do not train on the
same data distribution as we test.
V. MULTIPLE DEVICE MODEL
In this section, we ﬁrst discuss the overﬁtting issue we
encounter in portability. Next, we propose a new model
for portability called the Multiple Device Model, where we
experimentally show its superiority when compared to the
usual setting.
10
Fig. 11: Training and validation on the same device vs.
validation done on different device.
A. Overﬁtting
Recall from Section II-B1 that we are interested in super-
vised learning where, based on training examples (data X
and corresponding labels Y ), a function f is obtained. That
function is later used on testing data to predict the corre-
sponding labels. Here, the function f is estimated from the
observed data. That observed data is drawn independently and
identically distributed from a common distribution. To avoid
overﬁtting, we use validation (e.g., k-fold cross-validation or
a separate dataset for validation).
As it can be observed from the results in the previous
section, when training and testing data come from the same
device, the machine learning techniques do not have problems
in building good models as the model is ﬁtted to the same
distribution of data as will be used in the attack phase.
There, validation on the same data distribution helps to prevent
overﬁtting.
However, when there are two devices, one for training and
the second one to attack, the problem of overﬁtting is much
more pronounced and having validation done on a training de-
vice does not help signiﬁcantly. Naturally, the problem is that
our model is ﬁtted to the training data, but we aim to predict
testing data, which may not have the same distributions. We
depict this in Figure 11, where we show results for training
and validation on the device B1 K1 versus training on the
device B1 K1 and validation on the device B4 K3. In this
scenario, we experiment with MLP2 and we use all features.
We can clearly observe that when conducting validation on
the same device as training, the accuracy increases with the
number of epochs. At the same time, when we run validation
with measurements from a different device, there is almost no
improvement coming from a longer training process.
Based on these results, we identify overﬁtting as one of the
main problems in the portability scenarios. There, overﬁtting
occurs much sooner than indicated by validation if done on the
same device as training. Additionally, our experiments indicate
that k-fold cross-validation suffers more from portability than
having a separate validation dataset. This is because it allows
a more ﬁne-grained tuning, which further eases overﬁtting.
To prevent overﬁtting, there are several intuitive options:
1) Adding more measurements. While this sounds like a
good option, it can bring some issues as we cannot know
how much data needs to be added (generally in SCA, we
will, already, use all the available data from the start).
Also, in SCA, a longer measurement setup may introduce
additional artifacts in the data distribution [38]. Finally,
simply increasing the amount of training data does not
guarantee to prevent overﬁtting as we do not know what
amount of data one needs to prevent underﬁtting.
2) Restricting model complexity. If we use a model that has
too much capacity, we can reduce it by changing the
network parameters or structure. Our experiments also
show the beneﬁts of using more shallow networks or
shorter tuning phases, but it is difﬁcult to estimate a
proper setting without observing the data coming from
the other distribution.
3) Regularization. There are multiple options here: dropout,
adding noise to the training data, activation regulariza-
tion, etc. While these options would certainly reduce
overﬁtting in general case,
they are unable to assess
when overﬁtting actually starts for data coming from
a different device, which makes them less appropriate
for the portability scenarios. We note that some of these
techniques have also been used in proﬁled SCA but not
in portability settings, see, e.g., [13], [23].
B. New Model
Validation on the same device as training can seriously
the performance of machine learning algorithms if
affect
attacking a different device. Consequently, we propose a new
model that uses multiple devices for training and validation.
We emphasize that since portability is more pronounced when
considering different devices than different keys (and their
combined effect is much larger than their sum), it is not
sufﬁcient
to build the training set by just using multiple
keys and one device. Indeed, this is a usual procedure done
for template attack, but it is insufﬁcient for full portability
considerations.
In its simplest form, our new model, called the “Multiple
Device Model” (abbreviated MDM) consists of three devices:
two for training and validation and one for testing. Since
we use more than one device for training and validation, the
question is which device to use for train and which one for
validation. The simplest setting is to use one device for training
and the second one for validation. In that way, we can prevent
overﬁtting as the model will not be able to learn the training
data too well. Still, there are some issues with this approach:
while we said we use one device for training and the second
one for validation, it is still not clear how to select which
device to use for what. Indeed, our results clearly show that
training on device x and validating on device y to attack device
z will produce different results when compared to training on
device y and validating on device x to attack device z. This
11
happens because we cannot know whether device x or y is
more similar to device z, which will inﬂuence the ﬁnal results.
Instead of deciding on how to divide devices among phases,
we propose the Multiple Device Model where a number of
devices participate in training and validation. More formally,
let the attacker has on his disposal t devices with N data
pairs xi, yi from each device. The attacker then takes the same
number of measurements k from each device to create a new
train set and the same number of measurements j from each
device to create a validation set. Naturally, the measurements
in the training and validation sets need to be different. The
training set then has the size t·k and the validation set has size
t· j. With those measurements, the attacker builds a model f,
which is then used to predict labels for measurements obtained
from a device under attack. We emphasize that in training and
validation, it is necessary to maintain a balanced composition
of measurements from all available devices in order not to
build a model skewed toward a certain device. We depict the
MDM setting in Figure 12.
Deﬁnition 2: Multiple Device Model denotes all settings
the training on measurement
where attacker can conduct
−→
B = { ˆB0,...,
data from a number of similar devices (≥ 2),
ˆBn−1} and import the learned knowledge L−→
B to model the
actual device under target B, under similar but uncontrolled
parameter setup P .
We present results for MDM and multilayer perceptron
that uses all features (MLP2) and 10 000 measurements as
it provided the best results in the previous section. The results
are for speciﬁc scenarios, so they slightly differ from previous
results where we depict averaged results over all device
combinations. Finally, we consider here only the scenario
where training and attacking are done on different devices and
use different keys. Consequently, the investigated settings are
selected so as not to allow the same key or device to be used
in the training/validation/attack phases.
In Figure 13a, we depict several experiments when us-
ing different devices for training and validation. First,
let
us consider the cases (B1 K1) − (B4 K3) − (B2 K2)
and (B1 K1) − (B2 K2). There, having separate devices
for training and validation improves over the case where
validation is done on the same device as training. On the
other hand, cases (B4 K3) − (B2 K2) − (B1 K1) and
(B4 K3) − (B1 K1) as well as (B4 K3) − (B1 K1) −
(B2 K2) and (B4 K3) − (B2 K2) show that adding de-
vice for validation actually signiﬁcantly degrades the per-
formance. This happens because the difference between the
training and validation device is larger than the difference
between the training and testing device. Next,
the cases
(B2 K2)− (B4 K3)− (B1 K1) and (B2 K2)− (B1 K1)
show very similar behavior, which means that validation and
testing datasets have similar distributions. Finally, for three
devices, e.g., cases (B1 K1) − (B4 K3) − (B2 K2) and
(B4 K3) − (B1 K1) − (B2 K2), the only difference is the
choice of training and validation device/key. Nevertheless, the
performance difference is tremendous, which clearly shows
the limitations one could expect if having separate devices for
Fig. 12: Illustration of the proposed MDM. Observe a clear separation between training/validation devices and a device under
attack, cf. Figure 1.
training and validation.
Next, in Figure 13b, we depict the results for our new
Multiple Device Model where training and validation mea-
surements come from two devices (denoted with “multiple”
in the legend). As it can be clearly seen, our model can
improve the performance signiﬁcantly for two out of three
experiments. There, we can reach the level of performance
as for the same key and device scenario. For the third
experiment, (B1 K1) − (B2 K2) − (B4 K3), we see that
the improvement is smaller but still noticeable, especially for
certain ranges of the number of measurements. We see that
MDM can result in order of magnitude better performance
than using two devices. At the same time, with MDM, we did
not observe any case where it would result in performance
degradation when compared to the usual setting.
In summary, we show that MDM offers a superior model
by clearly establishing different sets of devices for training,
validation, and attack. The improvements obtained, compared
to the usual setting of two devices, measured in terms of
guessing entropy is > 10× (i.e., two traces with the MDM
model, while over 20 traces with two devices). The perfor-
mance impact of various scenarios is studied systematically
to highlight the pitfalls clearly. MDM may not always be
necessary: if both training and attacking devices contain small
levels of noise and are sufﬁciently similar, then using only
those two devices could sufﬁce. Still, as realistic settings tend
to be much more difﬁcult to attack, having multiple devices
for training and validation would beneﬁt attack performance
signiﬁcantly. While MDM requires a strong assumption on
the attacker’s capability (i.e., to have multiple devices of the
same type as the device under attack), we consider it to be
well within realistic scenarios. If additional devices are not
available, one could simulate it by adding a certain level of
noise to the measurements from the training device.
VI. OVERCOMING THE HUMAN ERROR: PORTABILITY OF
ELECTROMAGNETIC PROBE PLACEMENT
Electromagnetic measurements are very sensitive to probe
placement (position, distance, and orientation). This does not
12
(a) Results for separate devices for training and validation.
(b) Results for MDM.
Fig. 13: Results for different settings with multiple devices for
training and validation.
represent a problem in “classical” side-channel measurements
where training and testing are done on the same device
as the probe does not move. However, if we consider the
realistic proﬁled scenario, training and testing must be done
on different devices. This essentially means the probe must
be moved from training to the testing device. Even though
Device1LabelsTracestrainClassificationAlgorithmDevicetestTracestestLabelhypothesisClassificationAlgorithmSecretKeyProfiledModelProfilingPhaseAttackPhaseTracesvalDevice2Device3the placement of the probe on the testing device can be very
close to that on the training device, there is always a small
difference in the placement due to the position distance and
orientation. We attribute this error of placement as the human
error.
To investigate the impact of human error, we performed
the following experiment. Measurements are taken of an Ar-
duino Uno board, ﬁtted with previously investigated AVR AT-
MEGA328P, running AES-128 encryption. The measurements
are taken around a target S-box computation in the ﬁrst round
(one byte), with Lecroy WaveRunner 610zi oscilloscope using
an RF-U 5-2 near-ﬁeld EM probe from Langer and ampliﬁed
by a 30 dB pre-ampliﬁer. Three datasets (E1, E2, E3) are
captured for random plaintext, each time at a similar position
but with the aforementioned human error. The datasets have
50 000 traces where each trace has 500 features. We use a
MLP and we ﬁrst conduct a tuning phase with the same hyper-
parameters as before. We again use the same setting with
10 000 traces in the testing phase and 10 000 or 40 000 traces
in the training phase. The validation set has 3 000 traces.
Based on that tuning phase, we decide to use an MLP with
two hidden layers where each hidden layer has 100 neurons.
We depict the results in Figure 14. We can observe that MDM
helps us to 1) obtain better results, i.e., reach guessing entropy
equal to 0 faster, and 2) make the performance more stable
across device combinations. Again, we can observe that having
a smaller number of traces for training does not necessarily
mean the performance of the attack will be decreased. This is
especially clear in Figure 14c, where most of the cases do not
reach guessing entropy of 0 within 20 attack traces. This is
contrasting Figure 14a, where we observe only one such case
and, of course, Figures 14b and 14d, where MDM ensures
signiﬁcantly better attack performance.
VII. RELATED WORK ON PORTABILITY
The problem of portability stems from the issue of overﬁt-
ting, which is a common issue in machine learning. Exploring
portability in the context of proﬁled SCA received only
limited attention up to now. Elaabid and Guilley highlighted
certain issues with template attacks, such as when the setups
changed, for example, due to desynchronization or amplitude
change [39]. They proposed two pre-processing methods to
handle the templates mismatches: waveform realignment and
acquisition campaigns normalization. Choudary and Kuhn
analyzed proﬁled attacks on four different Atmel XMEGA
256 8-bit devices [40]. They showed that even on similar
devices, the differences could be observed that make the attack
harder and carefully tailored template attacks could help to
mitigate the portability issue. Device variability was also a
consideration in hardware Trojan detection [9]. There, the
authors showed that the results are highly biased by the process
variation. The experiments were conducted on a set of 10
Virtex-5 FPGAs. Kim et al. demonstrated the portability in
the case of wireless keyboard, by building the template based
on the recovered AES key to attack another keyboard [11].
They highlighted that some correction has to be performed
(a) Classical approach and different EM probes positions, 10 000
measurements.