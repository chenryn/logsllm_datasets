Certainly! Here is a more polished and professional version of your text:

---

**Question:**
What is a good tool for collecting statistics on filesystem usage? We manage multiple filesystems for our computational cluster, which are used by a large number of users who store very large files. We need to monitor these filesystems to optimize their usage and plan for future expansion.

To achieve this, we require a tool that can provide detailed statistics about the files, including:
- Age
- Frequency of access
- Last accessed times
- File types
- File sizes

Ideally, this information should be available in an aggregated form for any directory, allowing us to monitor usage based on projects or users.

I have not found any suitable tools for this purpose, short of writing a custom solution in Python. Do you have any recommendations?

**Answer:**
For monitoring filesystem usage and collecting the required statistics, you may want to consider using a tool that leverages `inotify` to log file system events. One such tool is `logsend`, which can help you track changes and accesses to files. Additionally, you might find `inotify-tools` useful, as it provides a set of command-line utilities for working with `inotify`.

These tools can provide the necessary data for monitoring and optimizing filesystem usage, and they can be configured to aggregate information at the directory level, making it easier to analyze usage patterns by project or user.

---

This version is more structured and clearly conveys the requirements and potential solutions.