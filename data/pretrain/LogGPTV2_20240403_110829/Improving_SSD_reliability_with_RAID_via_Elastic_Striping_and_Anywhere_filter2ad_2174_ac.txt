P W R5 = N req
P (1 + N P O
P R5)(1 + W AF (u))
Then, naturally, to serve n write requests averaging N req
pages per request, SSDs employing RAID-5 will incur n ·
N req
P W R5 page writes. Then, assuming perfect wear-levelling,
n write requests erase n · N req
P W R5/NP blocks, where NP is
the number of pages in a ﬂash memory block. Then, N req
E R5,
the number of erase operations incurred by a write request
becomes
P
N req
E R5 =
N req
P W R5
NP
(3)
As this analytic model shows, we can estimate the number
of page writes and erase operations performed for RAID-5
conﬁgured SSDs if we know the number of write requests,
n, the average request size, Sreq, and the average utilization,
u, of victim blocks selected for GC. (As mentioned earlier,
utilization can also be inferred from the ratio of the data size
and the OPS size.)
B. Analysis of SSDs with eSAP-RAID
In this section, we derive the analytic model of an SSD
that employs the eSAP-RAID scheme. Before deriving the
analytic model, we compare eSAP-RAID and RAID-5 in terms
of write request size and its performance impact. Like the
RAID-5 conﬁguration, after writing data of a request that
does not ﬁll a whole stripe, eSAP does not write the parity
immediately but waits for a subsequent request for a maximum
of Pwait time. If the subsequent write request arrives within
Pwait, eSAP consolidates the two requests and counts them
as a single request even though the subsequent write request
is not logically sequential to the previous request. This is the
key difference from RAID-5, which consolidates two requests
only when their logical block numbers are consecutive.
If the next request does not arrive in Pwait time or a whole
stripe is ﬁlled with data, then the requested data is written.
During this writing process, if a whole stripe is ﬁlled with
data and there is still data remaining, then the parity for the
whole stripe is written and the remaining data constructs a new
(non-full) stripe. At this point, eSAP again waits for requests
for another Pwait time period. If more requests do not arrive
within Pwait, the SSD constructs a short partial stripe and
writes the partial parity for this sub-stripe. After writing the
parity, it regards requests that follow as separate ones.
Note that eSAP consolidates write requests that arrive
within the Pwait time span even though their LBNs are not log-
ically consecutive, that is, even if the data of the write request
do not fall into the same logical stripe. Therefore, compared to
RAID-5 that only consolidates requests falling into the same
logical stripe, many more requests can be consolidated, and
hence, be counted as a single request. Consequently, for the
same workload, eSAP not only has smaller n, the number of
write requests, but also a larger Sreq, the average request size,
than RAID-5.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:49:49 UTC from IEEE Xplore.  Restrictions apply. 
D D D D P D D D P Full- Stripe Head D D D D P D D P D P Full-  Stripe Head Tail D D D D P D D D P Tail (1) (2) (3) Full-  Stripe D D D D P (1) D D D P D D P D P D D D D P D D D D P D D D P D D P D P (2) (3) (4) D P (1) one page D P D P D P D D P (2) two pages D D P D D P D P D P D D D P (3) three pages D D D P D D P D P D D P D P D D D D P (4) four pages D D D P D D P D P D D D P D D P D P P D (1) one page D P D P X D P D P D (2) two pages D D P X D D P D P P D D D P D X (3) three pages D D D P D D P D P D P D P D D D D D P (4) four pages D D D P D D P D P D D P D X D P D P D Let us now derive the analytic model of an SSD employing
the eSAP-RAID scheme. Like RAID-5, if the data size is big-
P −1|
(cid:99)
ger than N D
N D
P
full stripes are needed to write the data and thus, eSAP-RAID
writes that number of parities along with data.
P pages as shown in Fig. 6(a), at least (cid:98)|N req
Now, we consider the average number of page writes
incurred by the remaining data. Fig. 6(d) shows the possible
data and parity deployments when a stripe consists of ﬁve
pages, four data pages and one parity page. In the ﬁgure,
the shaded rectangles refer to the pages occupied by previous
requests and the rectangles with D and P denote pages written
by the current request. (The X marks will be explained later.)
Similarly to the RAID-5 case, for eSAP there are also four
cases (1) through (4) where the remaining data size is one
through four pages.
Comparing Fig. 6(c) and 6(d), at ﬁrst glance, one may think
that the probabilities of the remaining data pages incurring one
parity write and two parity writes for eSAP-RAID will be the
same as that of RAID-5. However, there are two differences.
First, the cases in the second row of Fig. 6(d) cannot happen in
eSAP-RAID because parity write always follows a data write
and thus, the previous request must have written two or more
pages. Therefore, we need to derive probabilities of one and
two parity page writes with the cases in the second row of
Fig. 6(d) omitted. Speciﬁcally, when the remaining data size
(cid:99)
pages, with probability
is N req rem
,
the data ﬁts in a single stripe and incurs one parity write.
Otherwise, with probability 1 − N D
, the
data spans two stripes incurring two parity writes. By summing
these, we derive, N parity
P eSAP , the average number of parity
writes for writing data of N req
P −N req rem
+(cid:98) N
P −1
N D
P −N req rem
+(cid:98) N
N D
P −1
N D
req rem
P
req rem
P
ND
P
ND
P
(cid:99)
P
P
P
N parity
P eSAP =
+(1 − N D
P
pages as follows:
+ (cid:98) N req rem
P − N req rem
N D
P − 1
N D
+ (cid:98) N req rem
P
N D
P
(cid:99)
P
(cid:99)
P − N req rem
P − 1
N D
P
P
N D
P
) · 2
P − 1|
N D
P
(cid:99)
+(cid:98)|N req
TABLE I: Parameter of SSD simulator
Parameter
Page size
Block size
Page read
Value
4KB
256KB
25us
Parameter
Page write
Block erase
Page Xfer latency
Value
200us
1.5ms
100us
Fig. 7: CDF of Inter-arrival time
Let us deﬁne N parity waste
P
parity writes and wasted pages for writing N req
request. Then, since space wasted, on average, is
per request, N parity waste
becomes
to be the average number of
page sized
pages
P
1
N D
P
P
N parity waste
P
= N parity
P eSAP +
1
N D
P
From N parity waste
and wasted space for writing a data page, N P O W A
, we can calculate the parity overhead
as follows:
P
N P O W A
P
=
P
N parity waste
P
N req
P
This is saying that for a write request of N req
collection incurs N req
page writes.
P · (1 + N P O W A
P
P
pages, garbage
) · W AF (u) additional
(4)
By summing up data and parity writes and the additional
writes for garbage collections, we derive N req
the
actual number of written pages for serving a write request, and
N req
E eSAP , the average number of erase operations incurred by
a write request in an eSAP conﬁgured SSD as follows:
P W eSAP ,
From N parity
P eSAP , we calculate the parity overhead for
writing a data page, N P O
P eSAP as follows:
N P O
P eSAP =
N parity
P eSAP
N req
P
N req
pages actually requires
P eSAP ) page writes due to the parity writes.
P
Hence, a write request of N req
P (1 + N P O
Now we consider the second difference between eSAP-
RAID and RAID-5. This difference is related to the garbage
collection cost incorporated in W AF (u). Note the X marked
rectangles in Fig. 6(d). Recall that when only one page is left
in a stripe eSAP-RAID cannot write data there as there is no
room for parity for that data. In this situation, eSAP simply
abandons the unused page and constructs a new stripe. Note
that it does not incur a page write but wastes space resulting in
increased garbage collection cost. In effect, parity writes and
wasted space have the same effect on garbage collection.
· (1 + N P O
P eSAP )
) · W AF (u)
N req
P W eSAP = N req
+N req
P
· (1 + N P O W A
N req
P
P
N req
E eSAP =
P W eSAP
NP
(5)
(6)
From this analytic model, we can estimate the number of
page writes and erase operations performed for eSAP-RAID
conﬁgured SSDs if we know the number of write requests, n,
the average request size, Sreq, and the average utilization, u,
of the victim blocks selected for GC.
V. PERFORMANCE EVALUATION
In this section, we discuss the experimental evaluation of
the SSD RAID schemes considered in this paper. Speciﬁcally,
we compare the I/O response time and the parity overhead
of SSDs employing RAID-0, RAID-5, and eSAP using a
simulated SSD. As we mentioned previously, current SSDs
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:49:49 UTC from IEEE Xplore.  Restrictions apply. 
TABLE II: I/O workloads and their characteristics (ﬁrst column) and the workload characterizing parameters obtained through experiments to be used in the
analytic model (second column and beyond): u: observed utilization, stripe(x): x is stripe size
Workload
(Total Data Req.,Write Ratio)
Sequential
(21.8GB, 1.0)
Random
(30.2GB, 1.0)
Financial
(35.7GB, 0.81)
Exchange
(101.2GB, 0.46)
MSN
(29.7GB, 0.96)
Scheme
RAID-5
eSAP
RAID-5
eSAP
RAID-5
eSAP
RAID-5
eSAP
RAID-5
eSAP
# of
Write Req.
Avg. size
of Write
u for
stripe(8)
u for
stripe(16)
u for
stripe(32)
368K
184K
5182K
255K
3617K