景下我们要搭建级联环境是不可行的。
会有GTID的设置，这个可以作为我们搭建从库的时候所用：
至少目前来看MGR的节点选择是自动的过程，还没有一个类似优先级的方式。
因为搭建的环境官方建议也是 single_primary 的方式，即一主写入，其他做读，也就
replicationgroup'
slave
我们来总结下MySQL5.7版中的一些常见MGR问题的处理。搭建的过程我就不用
SET @@SESSION.GTID_NEXT= '1bb1b861-f776-11e6-be42-782bcb377193:3084'
Old Secondary (new Primary) : shutdown
如果要把集群主节点的关系恢复回来，可以把节点2 停掉，让关系能够轮询过来。
 2019-01-26T06:03:56.861092z 0 [Note] Plugin group_replication reported:
在环境部署后，我们可以通过业务对接的方式试运行一下，看看还有哪些潜在的问题。
ERROR 3190 (HY000) : RESET MASTER is not allowed because Group Replication
>>reset master;
但是在 MGR 里面是不可行的，因为 reset master 操作是不允许的，在已有数据的场
此外还有一些经验，如果我们使用 mysqldump 导出一个文件，在导出的文件中默认
>>create
Primary:
大概有1~2秒的时间差，主库的数据写入就能够重新感知到。
可以模拟一个Swithover的场景，即把节点2停掉
Old Secondary(newPrimary)可以看到日志的细节：
QueryOK，
>>START GROUP_REPLICATION;
Old Primary通过如下的方式启动：
（10）Failback 的场景测试
server(7238),
databasetest;
pos(
_order=on;
第8章MySQL集群和高可用设计
is running
327
---
## Page 350
328丨MySQL DBA工作笔记：数据库管理、架构优化与运维开发
on wait for view after joining group'
是需要一致的。之前每次失败都会认认真真拷贝 uuid，发现适得其反。
字不能设置为每个节点的 uid，比如节点1，2，3这几个节点 group_replication_group_name
bb33-0026b935eb76:1-5,
reported:
multi-primary，读节点设置为 single-primary，统一一下即可。
误，也不用太担心，可以从日志看到是因为参数的不兼容性导致的。比如主写设置为
disjoint_gtids_join option'
chis
你离搭建成功就不远了。
auto
read failed'
2017-02-22T14:47:05.814567Z
 2017-02-22T14:47:05.814080z 30 [ERROR] Plugin group_replication reported: 'Timeout
 2017-02-22T14:46:35.851829z 0 [ERROR] Plugin group_replication reported: '[GCs] The
 2017-02-22T14:46:35.819072z 0 [Warning] Plugin group_replication reported:
b79d42f4-f351-11e6-9891-0026b935eb76:1'
2017-02-21T10:20:56.324890+08:000
可以很明显看到日志中已经提示了，需要设置参数,也就是兼容加入组。group_replication
 2017-02-20T07:56:30.064587z 0 [Note] Plugin group_replication
 2017-02-20T07:56:30.064580z 0 [ERROR] Plugin group_replication reported
这个问题困扰了我很久，其实本质上就是节点的设置，里面有一个 group_name，这个名
MGR有单主（single-primary）和多主（multi-primary）模式，如果碰到下面这个错
读节点加入组的时候， start group_replication 抛出了下面的错误。基本碰到这个错误,
问题1：单主模式加入节点失败
问题3：节点配置不统一导致集群无法启动
问题2：模式配置错误导致无法启动集群
onever
ryser
into
leave
1e6-b1de-a4badb1b524e:1 > Group transactions: 87b9c8fe-f352-11e6-
transactions:
the
the
dnoxb
group
you
30
30
to
87b9c8fe-f352-11e6-bb33-0026b935eb76:1-5,
[Note]
can
use
[ERROR]
Plugin
transactions than
Plugin
Plugin
the
 group_replication_allow_local
 group_replication
group_replication
Plugin
reported:
reported
 reported:
wil
---
## Page 351
么处理呢，就会优先把第二个节点 S2升级为主写，如图8-24所示。
external plugin.
下错误，因为一个基本要求就是表中要含有主键。
其重要。
如果需要特别设置，还可以指定 report_host 来完成。
The group replication applier
Write Clients
我们目前搭建的是 single-primary 的模式。如果主写节点发生故障，整个 group 该怎
mysql> alter table test_tab add primary key(id);
修复的方式就是添加主键，如下：
然后插入一条数据，看起来这是一个再正常不过的操作，但是在MGR里面就会有如
create table test_tab (id int,name varchar(30));
问题5：模拟灾难
创建表test_tab，如下：
环境搭建好之后，我们来创建一个普通的表，有时候好的习惯和规范在这种时候尤
问题4：数据写入失败修复
基本上搭建过程中就这几类问题，还有主机名类的问题，这方面还有一些小的 bug，
mysql> start group_replication;
统一之后，启动的过程其实很快。
Server S1 is the primary
Read Clients
(0.01
WriteClients
sec)
thread was killed'
SQL
33
1
图8-24
[Note]
Server S1 fails.
Read Clients
 Plugin group_replication reported:
Write Clients
第8章MySQL集群和高可用设计|329
Server S2 is thenewprimary
Read Clients
---
## Page 352
330丨MySQL DBA工作笔记：数据库管理、架构优化与运维开发
24805
24802
logs'
248041
24803
24802
24801
MEMBER STATE
MEMBERSTATE
mysql> select *from performance_schema.replication_group_members where
我们用下面的语句来过滤得到。
然后就会看到组复制的情况成了下面的局面，毫无疑问，第一个节点被剔除了。
2017-02-22T14:59:45.157989z 0 [Note] Plugin group_replication reported:
怎么判断一个复制组中哪个是主节点，不能完全靠猜或者翻看日志来判断吧。
从日志中我们可以看到是第2个节点升为主写了，那么问题来了。
CHANNELNAME
节点2会输出下面的日志，意味着这个节点正式上岗了。
首先是组复制的基本情况，目前存在5个节点，我们直接kill节点1,即端口为24801的节点。
 group_replication_applier | 6defc92c-f90a-1le6-990c-782bcb377193 | grtest
要测试的话还是很简单的。我们把节点1的服务直接kill掉。看看主节点会漂移到哪里。
问题6：如何判断一个复制组中的主节点
dnoxb
ONLINE
ONLINE
_replication_applier 丨 76bc07a1-f90a-1le6-ab0a-782bcb377193 丨 grtest
IMEMBER_ID
MEMBER_ID
--+一
---+-
+
| MEMBER_HOST 丨 MEMBER_PORT
| MEMBER _HOSTI MEMBER_PORT
------+--
---
## Page 353
是你原本的自增列值为1，结果下一次就可能是8了。我们可以设置的小一些，因为 MGR
部分是一个中和的设计。
面的提示有些太委婉了，建表无主键可以成功，但是无法写入数据，其实可以更加直接一些。
create table xxxx as select *from xxxx;这种语法在 GTID 模式下是不可行的。
习惯，如果已有的业务中使用了GTID，那么切换到MGR 的犯错成本就会小一些。比如
8.3.7
24802
|MEMBER_STATE
十一
WHERE
member_id =(select
（3）对于已有的业务，自增列的使用是否有连续性的要求，在MGR里面，自增列的
（2）表需要主键，这一点是硬性规定，也是作为 MySQL 方向集群的潜规则。MGR 在这方
如果对于自增列的连续性有强业务依赖，那么 MGR方案的实现会有一些出入，也就
>show variables like '%increment%';
>>show variables like '%increment%'
在单机版本中，自增列的参数如下：
（1）主库需要是GTID模式，这里的差别就是GTID会对应一些更加标准规范的使用
如果线上已经存在一套环境，我们怎么能够适配新的MGR架构？这就说到迁移的问题，
1row in set
 auto_increment_increment
Variable_name
在MGR环境中的自增列情况如下：
Variable_name
如果平滑的从业务过度到新 MGR架构，有一些前置的配置需要考虑。
CHANNEL_NAME
auto
innodb_autoextend_increment
auto_increment_increment
VARIABLE
迁移到 MGR 需要思考的问题
NAME=
(0.00 sec)
offset
/Value
group
ent
|Value|
MEMBER_ID
increment increment
164
7239
7
64
丨 MEMBER_HOST | MEMBER_PORT
第8章MySQL集群和高可用设计|331
---
## Page 354
332丨MySQL DBA工作笔记：数据库管理、架构优化与运维开发
这个修复的代价就非常高了。
可用就是一个需要重点考虑的问题，如果说元数据的信息丢失了，我们无法恢复，那么
以从原来的测试版本逐步过渡到了一个正式的线上版本，系统优先级提高了，系统的高
的值都是25000，即25000个GTID 时，整个集群会阻塞写操作。
比是一个团队爬山，如果有一个成员落后太多，整体的行进速度就会慢下来，需要做好平衡。
套简单的集群环境，如何平滑地实现 switchover等操作？
迟的情况下，如何对MGR 的数据一致性做充分的测试？
设置 server-id 为不同的值，避免后续环境对接中出现问题。
样的一个中和，我们可以把这个参数设置的小一些。
最多支持9个节点，而绝大多数的环境中节点7个就很多了，在设计的时候也是做了这
通过环境的配置发现，MGR 节点的 server-id 相同的情况下依然可以搭建成功，需要
如果你在做一个DML操作的时候发生了如下的错误，很可能就是流控导致。
MGR方案也可以是一个比较轻量的方案，不一定非要3台以上，2台就可以搭建一
当前系统的状态如下图8-25所示。
升级的动力主要是运维系统建设也有一些日子了，已经支撑了不少线上的业务，所
>>set global group_replication_flow_control_mode='DISABLED';
对于多 IDC 的 MGR 集群是建议关闭流控的，关闭设置为 DISABLED 即可，如下：
group_replication_flow_control_
如果为QUOTA 则表明是启用了流控，流控的细节主要是由两个参数来控制的，它们
1 row in set (0.00 sec)
>>show variables like '%group_replication_flow_control_mode%';
关于流控，可以参考如下的参数配置。
MGR 集群中如果节点落后其他成员太多，就会发起让其他节点等待的控制流程，叫好
有一次对运维系统的 MySQL 架构做了下升级，从单点实例升级到了 MGR 跨机房集群。
 group_replication_flow_control_mode | QUOTA
问题：什么是流控，怎么去衡量这个指标？
案例 8-1：切换到 MGR 的参考步骤
Variable_name
比如集群的架构设计，是否考虑了跨机房？是否考虑双写模式？在网络存在较大延
此外 MGR的方案你是打算怎么用或者有一个长远的规划。
|Value|
_applier_threshold | 25000
---
## Page 355
接和权限不变，对于业务使用来说能够透明一些。
MGR环境，到时候需要导出线上数据，导入MGR环境。
业务分离出去，使得运维系统 devopsdb 的业务能够直接升级到 MGR 架构环境下。
研这方面的需求匹配，因此经过权衡，在不影响已有的权限和业务的情况下，把xwiki
配，基本上这个业务就是最小化运维，拿来能用即可，所以就不打算投入太多精力去调
现，我们无法保证这个数据库的业务逻辑中对于自增列的使用场景和 hibermate 的完全匹
就是表需要有主键。对于 xwiki 业务的表因为是采用的一个开源版本，基于 hibermate 实
准备的环境如下图8-26 所示，尤其需要注意图中的端口，这是我们为了保持业务连
为了避免升级的时候，我们手忙脚乱的开始部署MGR 环境，我们需要预先搭建一套
现在需要对9.208所在的机房数据库做架构升级，改造为MGR，但有一个硬性要求
目前在两个机房存在两台服务器，彼此是独立的，分别负责了3个独立的业务方向。
devopsdb
9.208
4310
9.208
4306
9.208
xwiki
xwiki
图8-26
图8-25
4310
4306
119.221
iskopsbo
ask
4306
119.221
opsd
第8章MySQL集群和高可用设计|333
---
## Page 356
334|MySQLDBA工作笔记：数据库管理、架构优化与运维开发
面的数据库需要调整端口，从4306修改为4316。
devopsdb 数据可以备份出来就不再使用了，同时为了兼容和统一端口，119.221服务器上
调整后的的改进架构如下图8-28所示。
线上环境升级时的架构如下图8-27所示，我们需要切换为 MGR 环境，原来环境的
4308
9.208
4306
9.208
4308
9.208
4306
9.208
图8-28
图8-27
119.221
4306
4316
119.221
4306
119.221
119.221
4316
---