1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
Code-fragment level
Function
Patch-with-context
Patch-without-context
Slice
Table 3: Candidate code-similarity algorithms
Code representation
Token-frequency
Token-API node
API-subtree
Token-line
Token-component-1
Token-component-{2,3,4}
AST-suffix
AST-suffix+Token-component-{1,2,3,4}
xAST
xAST+Token-component-{1,2,3,4}
xGRUM
xGRUM+Token-component-{1,2,3,4}
GPLAG
GPLAG+AST+Token-component-{1,2,3,4}
PDG
PDG+AST+Token-component-{1,2,3,4}
PDG-slicing
PDG-slicing+AST+Token-component-{1,2,3,4}
Comparison method
Range-queries of metric tree
PCA and cosine similarity
of vectors
Containment
Substring matching
Manhattan distance of vectors
γ-isomorphic
Subgraph-isomorphic
Application
Vulnerability detection
Vulnerability extrapolation
Vulnerability extrapolation
Vulnerability detection
Bug detection
Bug detection
Clone detection
Clone detection
Vulnerability detection
Vulnerability detection
Vulnerability detection
Vulnerability detection
Clone detection
Clone detection
Bug detection
Bug detection
Clone detection
Clone detection
References
[12]
[29]
[30]
[13, 17]
[19]
Variants of [19]
[16]
Variants of [16]
[25]
Variants of [25]
[25]
Variants of [25]
[20]
Variants of [20]
[18]
Variants of [18]
[15]
Variants of [15]
4.3 Vulnerability signature generation
In this subsection, we use CVE-2015-0834, which is a
vulnerability in Mozilla Firefox prior to version 36.0, as
an example to illustrate the implementation details of vul-
nerability signature generation. Figure 3 shows one diﬀ
hunk of CVE-2015-0834 obtained from the VPD. After run-
ning Algorithm 1, code-similarity algorithm No.9 is selected.
Correspondingly, the code-fragment level, code representa-
tion, and comparison method are respectively the patch-
with-context, xAST, and vector comparison using Manhattan
distance.
The vulnerability signature corresponding to the diﬀ hunk
described in Figure 3 is generated as follows. First, we ex-
tract the patched/unpatched diﬀ code based on the preﬁx
symbols “-” and “+” in the diﬀ. Then, we collect the un-
patched functions from the diﬀ and source code of the vul-
nerable product versions as follows. We obtain the vulnera-
ble product versions from the text summary of CVE-2015-
0834 in the NVD (i.e., the versions prior to version 36.0), and
use a Web crawler to download the source code of these ver-
sions. From the latest vulnerable product version to the old-
est, we search for the ﬁrst product version that contains ex-
actly the lines of code preﬁxed by the symbol“-”and the lines
of code with no preﬁx in the diﬀ. We extract the unpatched
code fragments at the code-fragment level of patch-with-
context. Then we process the patched/unpatched diﬀ code
and the unpatched code fragments involving the whites-
pace, format, commenting processing, and the completion
of the structures and statements of code fragments. Finally,
we represent the patched/unpatched diﬀ code and the un-
patched code fragments by using xASTs, which are gener-
ated from the ASTs produced by the open source tool known
as Joern [28].
@@ -485,16 +485,19 @@ PeerConnectionImpl::ConvertRTCConfigurat
         (cid:258)
1:      if (!(isStun || isStuns || isTurn || isTurns)) {
2:        return NS_ERROR_FAILURE;
3:      }
4: +    if (isTurns || isStuns) {
5: +      continue; // TODO: Support TURNS and STUNS (Bug 1056934)
6: +    }
7:      nsAutoCString spec;
8:      rv = url->GetSpec(spec);
         (cid:258)
Figure 3: A diﬀ hunk corresponding to CVE-2015-
0834
4.4 Vulnerability detection
We use Thunderbird 24.8.0 as an example of target pro-
gram to illustrate the vulnerability detection process with
respect to CVE-2015-0834, including its diﬀ hunk described
in Figure 3. The target program signatures are generated as
follows. From the CVE-to-algorithm mapping, we obtain the
select code representation xAST for the diﬀ hunk of CVE-
2015-0834 described in Figure 3. After the preprocessing
of target programs involving whitespace, format, and com-
ment processing, we generate the signatures of Thunderbird
24.8.0 in terms of representation xAST. Then, we use the
detection engine to detect whether Thunderbird 24.8.0 con-
tains the vulnerability. Since the select comparison method
is the vector comparison using the Manhattan distance, we
convert the vulnerability signature and target program sig-
natures into vectors. Moreover, a target program is deemed
vulnerable if it contains at least a signature that is closer
to the vulnerability signature than the patched code signa-
ture. The target program satisfying these two requirements,
namely passing the code-similarity detection and contain-
ing a signature that is closer to the vulnerability signature,
are considered vulnerable. In the case the target program is
found to be vulnerable, the locations of the vulnerable piece
of code in the target program are determined.
5. EXPERIMENTAL RESULTS
The eﬀectiveness of code-similarity algorithms can be eval-
uated via standard metrics, such as precision, recall, and
F-measure metrics [27]. Let TP be the number of true vul-
nerabilities detected (true-positives), FP be the number of
false vulnerabilities detected (false-positives), and FN be the
number of true vulnerabilities undetected (false-negatives).
The metric precision = TP/(TP + FP) reﬂects the correct-
ness among the detected positives. The metric recall =
TP/(TP + FN) reﬂects the completeness of the detected pos-
itives. The overall detection eﬀectiveness can be reﬂected by
F -measure = 2 · precision · recall/(precision + recall).
5.1 Learning the CVE-to-algorithm mapping
and vulnerability signatures
Distribution of patch features and code-reuse fea-
tures. Figure 4(a) depicts the distribution of the patch fea-
tures of diﬀ hunks described in the VPD. We observe that
many diﬀ hunks have patch feature Types 2-1, 4-1, 4-2, 5-1,
5-2, and 5-3, while few diﬀ hunks have patch feature Types
3-8, 3-9, 3-10, 4-5, and 4-6. Moreover, nearly 25% of the
diﬀ hunks have a single type of patch features, with Types
208
5-1, 4-2, and 5-2 being the top three types. For diﬀ hunks
with a single type of patch features, the particular type is
probably the main factor in determining the classiﬁer for se-
lecting a particular code-similarity algorithm. For diﬀ hunks
with multiple types of patch features, one or multiple types
may have contributed to the determination of the classiﬁer
for selecting a particular code-similarity algorithm. Deeper
characterization on the roles played by the types of patch
features is left to future investigation.
Figure 4(b) depicts the distribution of the code-reuse fea-
tures obtained from the VCID. We observe that the distri-
bution is similar to what is shown in Figure 4(a), with two
exceptions. One exception is that the number of code reuse
instances corresponding to code-reuse feature Types 4-1 and
5-3 is respectively smaller than its counterpart with respect
to the diﬀ hunks. The other exception is that the number
of code reuse instances with a single type of code-reuse fea-
tures is greater than the number of diﬀ hunks with a single
type of patch features. Since many vulnerability diﬀ hunks
in the VPD do not have code reuse instances in the VCID,
we evaluate the precision and recall of code-similarity algo-
rithms using VPD and VCID separately.
Multiple types
 Single type
2500
2000
s
k
n
u
h
f
f
i
D
#
1500
1000
500
0
2-13-13-23-33-43-53-63-73-83-9
3-104-14-24-34-44-54-65-15-25-36-16-26-3
Patch feature
(a) Distribution of patch features
 Multiple types
 Single type
180
160
140
120
100
80
60
40
20
0
s
e
c
n
a
t
s
n
i
e
s
u
e
r
e
d
o
C
#
2-1 3-1 3-2 3-3 3-4 3-5 3-6 3-7 3-8 3-9
3-10 4-1 4-2 4-3 4-4 4-5 4-6 5-1 5-2 5-3
Code-reuse feature
(b) Distribution of code-reuse features
Figure 4: Distribution of patch features and code-
reuse features according to the VPD and VCID, re-
spectively
Comparison among code-similarity algorithms. We
took 70% of the vulnerabilities in each product according to
209
the VPD to learn classiﬁers and the rest as the testing data
for selecting code-similarity algorithms. When we set the
accuracy threshold of classiﬁcation model τ to 0.85 or 0.9,
we found that the code-similarity algorithms achieving an
accuracy lower than τ were algorithm No.13 as well as its
variants and algorithm No.15 as well as its variants. When
we set τ to 0.95, nearly half of the algorithms were selected.
In order to select more algorithms with a high accuracy,
we set τ to 0.9 in the following experiments. The accuracy
of select SVM classiﬁers, as described in Algorithm 1, for
existing code-similarity algorithms and their variants was
94.6% on average.
As described in Algorithm 1, if the code-similarity algo-
rithm treats a patched piece of code obtained from the VPD
as vulnerable, a false-positive occurs.
If a code reuse in-
stance obtained from the VCID is not detected as vulnera-
ble, a false-negative occurs. Also as described in Algorithm
1, the VPD is used for evaluating false-positives, while the
VCID is used for evaluating false-negatives. Therefore, for
each code-similarity algorithm, we computed the precision
based on the VPD, the recall based on the VCID, and the
F-measure based on both the precision and the recall.
Since code-similarity algorithms typically use a similarity
threshold, we need to know how to determine its value. Fig-
ure 5 shows the F-measure of code-similarity algorithm No.9
and its variants with respect to various similarity thresh-
olds. We observe that 0.6 is a better choice than the default
threshold of 0.8 recommended by code-similarity algorithm
No.9 [25]. For algorithm No.9 with threshold 0.6, its vari-
ants do not achieve a higher F-measure.
Indeed, thresh-
old 0.6 leads to the highest F-measure for the four variants
of algorithm No.9, namely algorithms No.10-{1, 2, 3, 4}.
Therefore, we set the threshold for algorithm No.9 and its
variants to 0.6. For other algorithms, we found that their
thresholds leading to the highest F-measure respectively co-
incided with their default threshold values. For example, for
code-similarity algorithm No.11 and its variants, threshold
0.8 led to the highest F-measure, which coincided with the
default threshold value of algorithm No.11.
 No.9
 No.10-1
 No.10-2
 No.10-3
 No.10-4
0.7
0.6
e
r
u
s
a
e
m
-
F
0.5
0.4
0.3
0.4
0.5
0.6
0.7
0.8