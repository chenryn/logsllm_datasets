depends on the access pattern [43]. To cope, we use a stash
with a static size at all times, and process empty slots in the
same way as full slots. Prior work [24], [43] showed that a
stash size of 89 to 147 is sufﬁcient to achieve failure probability
of 2−λ with the security parameter values from λ = 80 to λ =
128. In our implementation, we use a static stash size of 90. 2
c) Oblivious Path Rebuilding: Finally, P must rebuild
(Section III-D) using
(Section IV-C).
the
shown here:
and write back the path for leaf
internal
P rebuilds
stash for
for bu in new_path:
this path by making a pass over
logic and a Store Path call
in the path as
each bucket
for b in bu:
for s in stash:
cond = FitInPath(s.id,leaf)
oupdate(cond, b, s, BlockSize)
StorePath(leaf,new_path)
For each bucket location bu on path to leaf in reverse
order (i.e. from leaf to root), iterates over the block locations
b (in the available Z locations) and perform oupdate calls
to obliviously move compatible blocks from the stash to that
bucket (using an oblivious subroutine called FitInPath).
This greedy approach of ﬁlling buckets in a bottom to top
fashion is equivalent to the eviction routine in Section III-D.
At the end, P then calls Store Path on the rebuilt path, causing
the server to overwrite the existing path in server storage.
d) Encryption and Integrity: As data is processed in the
block retrieval and path re-building steps, it is decrypted/re-
encrypted using the primitives in Section IV-D4. At
the
same time, an oblivious implementation of the Merkle tree
(Section III-C) checks and re-build are performed to verify
integrity with freshness.
E. Optimizing Fetch/Store Path
We
several
now discuss
optimiza-
tions/extensions for the Fetch/Store Path subroutines,
to
take advantage of the server’s storage hierarchy (which consists
of DRAM and disk). Since these operations run in untrusted
code, they do not impact the TCB.
performance
1) Scaling bandwidth with multiple disks: Ideally, if the
server supports multiple disks which can be accessed in parallel
(e.g., in a RAID0), the time it takes to perform Fetch/Store Path
calls should drop proportionally. We now present a scheme
to perfectly load-balance a Tree ORAM in a RAID0-like
conﬁguration.
RAID0 combines W disks (e.g., SSDs, HDDs, etc) into
a larger logical disk. A RAID0 ‘logical disk’ is accessed at
stripe granularity (S bytes). S is conﬁgurable and S = 4 KB is
reasonable. When disk stripe address i is accessed, the request
is sent to disk i%W under the hood.
The problem with RAID0 (and similar organizations)
combined with Tree ORAM is that when the tree is laid out
ﬂat in memory, the buckets touched on a random path will
not hit each of the W disks the same number of times (if
S ∗ W > B ∗ Z for ORAM parameters B and Z). In that case,
potential disk parallelism is lost. We desire a block address
mapping from (ORAM tree address, at stripe granularity) to
(RAID0 stripe address) that equalizes the number of accesses
to each of the W disks, while ensuring that each disk stores
an equal (ORAM tree size) / W Byte share. Call this mapping
Map(tree addr) → RAID addr, which may be implemented
as a pre-disk lookup table in untrusted Fetch/Store Path code.
We now describe how to implement Map. First, deﬁne
a new parameter subtree height H. A subtree is a bucket j,
and all of the descendant buckets of j in the tree, that are
< H levels from bucket j. For ORAM tree height L, choose
H < L (ideally, H divides L). Break the ORAM tree into
disjoint subtrees. Second, consider the list of all the subtrees
ALoST. We will map each stripe-sized data chunk in each
subtree to a disk in the RAID0. The notation Disk[k] +=
[stripeA, stripeB] means we use an indirection table
to map stripeA and stripeB to disk k. We generate Disk
as:
2For our Circuit ORAM variant we use a ﬁxed stash size of 10 which is
known to be sufﬁcient from [49] .
//s_index is subtree_index
8
for s_index in length(ALoST):
// levels run from 0...H-1
for level in subtree:
// break data in subtree level
// into stripe-sized chunks
stripes = ALoST[s_index][level]
Disk[(s_index + level) % W] += stripes
When W = H, mapping each subtree level to a single disk
means any path in the ORAM tree will access each disk
O(L/H) times. Changing the subtree level → disk map in a
round-robin fashion via subtree_index ensures that each
disk will hold the same number of stripes, counting all the
subtrees. Finally, from Disk, it is trivial to derive Map.
2) Caching the ORAM tree: A popular Tree ORAM
optimization is to cache the top portion of the ORAM tree in a
fast memory [24], [33]. This works because each access goes
from root to leaf: caching the top l(cid:48) levels is guaranteed to
improve access time for those top l(cid:48) levels. Because the shape
is a tree, the top levels occupy relatively small storage (e.g.,
caching the top half requires O(
N ) blocks of storage).
√
This optimization is very effective in our system because
the server (who controls Fetch/Store Path) can use any spare
DRAM to store the top portion of the tree, as seen later in Fig
4 and Table 7. In this case, Fetch/Store Path allocate regular
process memory to store the top portion, and explicitly store
the lower portion behind disk IO calls.
F. Security Analysis
We now give a security analysis for the core memory
controller running ORAM. Since we support ORAM, we wish
to show the following theorem:
Theorem 4.1: Assuming the security of the Path ORAM
protocol, and the isolated execution and attestation properties
of Intel SGX, the core memory controller is secure according
to the security deﬁnition in Section III-A.
In this section, we’ll prove the above theorem informally, by
tracing the execution of a query in ZeroTrace, step by step as
shown in Figure 2.
Claim 4.1.1: Initialization is secure.
For initialization, the enclave ﬁrst samples a public key
pair, then includes this public key in the clear with the enclave
measurement, in the attestation (Section III-B) that it produces.
No malicious adversary can tamper with this step, as it would
have to produce a signature that is veriﬁable by the Intel
Attestation Service.
Claim 4.1.2: Decrypting and encrypting requests leak no
information.
We use AES-NI, the side-channel resilient hardware instruction
by Intel for performing encryption and decryption.
Claim 4.1.3: Oblivious Leaf-Label Retrieval leaks no in-
formation.
Retrieving a leaf label from the EPC-based position map
performs a data-independent traversal of the entire position
map via oupdate (Section IV-D2) operations. oupdate performs
Fig. 2: Execution of an access request
work independent of its arguments within the register space of
the processor chip, which is hidden from adversarial view. Thus,
the adversary learns no information from observing leaf-label
retrieval.
Claim 4.1.4: FetchPath leaks no information.
FetchPath retrieves the path to a given leaf label. The
randomness of this reduces to the security of the underlying
Path ORAM protocol (Section III-D4).
Claim 4.1.5: Verifying fetched path leaks no information.
To verify the integrity of a fetched path, the enclave re-
computes the Merkle root using SHA-256 over the path it
fetched and subling hashes [43]. We note that our current
implementation uses SHA-256 from the Intel tcrypto library,
which is not innately side-channel resistant. Despite this, our
scheme still achieves side-channel resistance because all SHA-
256 operations are over encrypted buckets. The same argument
applies when rebuilding the path on the way out to storage.
Claim 4.1.6: Oblivious Block Retrieval leaks no informa-
tion.
Once FetchPath completes, the only code that processes the
path is the decryption logic plus the oblivious subroutine given
in Section IV-D5. This loads the real blocks from the path into
the stash and return the requested block to the user, Since the
length of path and stash are data-independent, obliviousness
reduces to the security of oupdate (see Claim 4.1.3).
Claim 4.1.7: Oblivious Rebuild leaks no information.
9
Same argument as Claim 4.1.6, since new_path, bu and
stash have data independent size.
the private state). On re-execution, P can integrity-verify Ia
and Sb under the constraint that a = b.
Claim 4.1.8: StorePath leaks no information.
StorePath returns the new path to a leaf label
that was
fetched by an ORAM controller enclave. From the adversary’s
perspective, the stored path itself is an encrypted payload of a
known size, independent of underlying data.
V. PERSISTENT INTEGRITY
An important attribute in storage systems is to be persistent
and recoverable across protocol disruptions. This is particularly
important for ORAM, and similar memory controller backends,
where corrupting any state (in the ORAM Controller Enclave
itself or in the ORAM trees) can lead to partial or complete
loss of data. SGX exacerbates this issue, as enclave state is
wiped on disruptions such as reboots and power failures.
We now discuss an extension to ZeroTrace that allows
untrusted storage and the ORAM Controller Enclave to recover
from data corruptions and achieve persistent integrity. First,
we state a sufﬁcient condition to achieve fault tolerance. We
model an enclave program as a function P which performs
St+1 ← P(It, St), where It is the t-th request made by the
client and St is the enclave state after requests 0, . . . , t− 1 are
made. When we say enclave protocol, we refer to the multi-
interactive protocol between the client and P from system
initialization onwards (i.e., all of Section IV).
Deﬁnition 5.1 (Fault tolerance): Suppose an enclave proto-
col has completed t(cid:48) requests. If the enclave protocol is designed
such that the server can efﬁciently re-compute St+1 ← P(It, St)
for any t < t(cid:48), then the enclave protocol is fault tolerant.
This provides fault tolerance as follows: if the current state St(cid:48)
is corrupted, St(cid:48) can be iteratively re-constructed by replaying
past (not corrupted) states and inputs to P. We remark that the
above deﬁnition is similar to RDD fault tolerance in Apache
Spark [57], [59]. Finally, the above deﬁnition isn’t speciﬁc
to ORAM controllers, however we will assume an ORAM
controller for concreteness.
a) Functionality: In our setting, S includes the ORAM
Controller Enclave state (the stash, position map, ORAM key,
merkle root hash) and the ORAM tree. In practice, the server
can snapshot S at some time t (or at some periodic schedule),
and save future client requests It, . . . , It(cid:48) to recover St(cid:48). Thus,
we must add a server-controllable operation to the ORAM
Controller Enclave that writes out the enclave state to untrusted
storage on-command.
b) Security: To maintain the same security level as
described in Section II-B, the above scheme needs to defeat
all mix-and-match and replay attacks.
A mix-and-match attack succeeds if the server is able to
compute P(Ia, Sb) for a (cid:54)= b, which creates a state inconsistent
with the client’s requests. These attacks can be prevented
by encrypting state in S and each client request I with an
authenticated encryption scheme, that uses the current request
count t as a nonce. The client generates each request I and
thus controls the nonce on I. For S: the enclave controls the
nonce on its private state and integrity veriﬁes external storage
with a merkle tree (whose root hash is protected as a part of
A replay attack succeeds if the server is able to learn
something about the client’s access pattern by re-computing
on consistent data – e.g., P(It, St). Replay attacks are pre-
vented if replaying P(It, St) always results in a statistically
indistinguishable trace trace (Section III-A). In our setting, we
must analyze two places in the protocol. First, the path written
back to untrusted storage after each request (Section IV-D5)
is always re-encrypted using a randomized encryption scheme
that is independent of underlying data. Second, the leaf label
output as an argument to Fetch/Store Path (Section IV-C) must
be deterministic with respect to previous requests. This property
is achieved by re-assigning leaf labels using a pseudo-random
number generator. We note that similar mechanisms are used to
prevent replay and mix-and-match attacks in Nayak et al. [?].
VI.
IMPLEMENTATION AND EVALUATION
A. Experiment Setup
We implemented and evaluated the performance of
ZeroTrace on a Dell Optiﬂex 7040, with a 4 core Intel i5
6500 Skylake processor with SGX enabled and 64 GB of
DRAM (referred to as “memory”). Beyond DRAM, our system
utilizes a Western Digital WD5001AALS 500 GB 7200 RPM
HDD as backing untrusted storage. Unless otherwise speciﬁed,
the core memory controller uses tree top caching in DRAM
(Section IV-E2) whenever the ORAM capacity spills to disk.
ZeroTrace is implemented purely in C/C++ (and assembly)
for both performance and easier compatibility with Intel
SGX as enclave code is limited to purely C/C++ code.
Our implementation consists of 6600 lines of code in total,
with almost 4000 lines of code within the enclave, which
counts towards the TCB. We measure the time it takes our
memory service enclaves to complete user requests. In all
experiments, our core memory controller and data-structure
APIs are implemented as application libraries in a stand-alone
enclave – to best model their performance as plug-and-play
memory protection primitives (Section II-A). Thus, request
time includes the time to send/receive the request to/from the
enclave, as well as the time to process the request (e.g., do
an ORAM access). We predominantly evaluate 8 B and 1 KB
ORAM block sizes, which serve as proxies for word-level
(“plug-and-play”) and ﬁle-level size blocks. We note that our
experiments apply sequential memory access patterns to the
memory controller.3