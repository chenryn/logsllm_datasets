b
b
M
M
(
(
t
t
u
u
p
p
h
h
g
g
u
u
o
o
r
r
h
h
T
T
g
g
v
v
A
A
 500
 500
 400
 400
 300
 300
 200
 200
 100
 100
 0
 0
100
100
1300
1300
2000
2000
Flow Arrival Rate (flows/sec)
Flow Arrival Rate (flows/sec)
100
100
1300
1300
2000
2000
Flow arrival rate
Flow arrival rate
(a)
percentiles
1st-5th-50th-95th-99th
(b) Long ﬂow throughput
Figure 14: Benchmark traﬃc including short re-
sponse ﬂows (poisson arrivals with varying arrival
rate) and two long ﬂows.
tor that receives response ﬂows from the two other endhosts
in the rack. The response ﬂow size is uniformly distributed
between [2KB, 50KB], and we focus on results with moder-
ate deadlines (mean=30ms). Further, each sender also has
one long ﬂow to the aggregator. Both long ﬂows are bot-
tlenecked at the aggregator’s link to its ToR switch. This
represents the 75th percentile of long ﬂow multiplexing ob-
served in datacenter traﬃc [4]. We model the response ﬂow
arrival as a Poisson process and vary the mean arrival rate.
As before, our primary focus is on the ﬂow arrival rate that
can be supported while maintaining more than 99% appli-
cation throughput. This is shown in Table 1.
We ﬁnd that under realistic traﬃc dynamics, both D3 and
RCPdc outperform TCP by more than an order of magni-
tude. These results are best understood by looking at the
underlying network level metrics. Figure 14(a) plots the per-
centiles for ﬂow completion times (1st-5th-50th-95th-99th) at
diﬀerent ﬂow arrival rates. At very low load, the ﬂow com-
pletion times for the three protocols are comparable (TCP’s
median ﬂow completion time is 50% higher than D3). How-
ever, as the ﬂow arrival rate increases, the presence of long
ﬂows and the resulting queuing with TCP inevitably causes
some response ﬂows to drop packets. Even at 200 ﬂows/s,
more than 1% ﬂows suﬀer drops and miss their deadlines.
Given the retransmission timeout value (RT O=300ms), the
99th percentile ﬂow completion time for TCP is more than
300ms. Reducing the RTO to a lower value like 10ms [23]
does help TCP to improve its performance by being able
to support roughly 1100 ﬂows (Table 1). Yet, even with
this optimization, TCP can support less than half the ﬂows
supported by D3.
The ﬂow completion times for D3 and RCPdc are similar
throughout. For instance, even at 2000 ﬂows/s, the 99th
percentile completion time is almost the same even though
D3 can satisfy 99% of ﬂow deadlines while RCPdc can only
satisfy 96.5% ﬂow deadlines. We reiterate the fact that dead-
line awareness does not improve ﬂow completion times over
RCPdc (which minimizes them already). However, by being
59t
u
p
h
g
u
o
r
h
T
p
p
A
 100
 80
 60
 40
 20
 0
D3 with Flow Quenching
D3
 0
 500
 1000
 1500
 2000
 2500
 3000
Flow arrival rate (flows/s)
Figure 15: Application throughput with ﬂow
quenching
cognizant of the ﬂow deadlines, it ensures a greater fraction
of ﬂows satisfy them.
We also look at the performance of the long ﬂows to exam-
ine whether the gains oﬀered by D3 come at the expense of
(non-deadline) long background ﬂows. Figure 14(b) shows
the average throughput achieved by each long ﬂow during
the experiment. The ﬁgure shows that long ﬂows do not
suﬀer with D3.
Instead, it achieves its gains by smarter
allocation of resources amongst the deadline ﬂows.
6.1.3 Flow quenching
The results above illustrate the beneﬁts of unfair shar-
ing. Beyond this, deadline awareness can also guide “ﬂow
quenching” to cope with severe congestion. As described in
Section 1, under extreme duress, it may be better to shed
some load and ensure that a good fraction of ﬂows meet
their deadlines, rather than to run all ﬂows to completion
even though most will miss their deadlines. D3 design is par-
ticularly suited to such ﬂow quenching. Since endhosts know
the rate needed to satisfy their deadlines and the rate the
network can oﬀer, they can independently decide whether
to continue with the ﬂow or not.
We implemented a straightforward ﬂow quenching mech-
anism wherein endhosts prematurely terminate ﬂows (by
sending a FIN) when: (i). desired rate exceeds their uplink
capacity, or (ii). the deadline has already expired. Figure 15
shows the application throughput with such ﬂow quenching
for the benchmark traﬃc experiment.
Flow quenching leads to a smoother decline in performance
at extreme loads. From the application perspective, fewer
end users get empty responses. Beyond 2500 ﬂows/s, D3
cannot cope with the network load since the ﬂow arrival
rate exceeds the ﬂow departure rate. Consequently, the ap-
plication throughput drops drastically as the network suﬀers
congestive collapse. However, with ﬂow quenching, endhosts
do not pursue intractable deadlines which, in turn, spares
bandwidth for other ﬂows whose deadlines can be met.
6.2 D3 as a congestion control protocol
We also evaluated the performance of D3 as a conges-
tion control protocol in its own right, operating without
any deadline information. Results in earlier sections already
show that D3 (and RCPdc) outperforms TCP in terms of
short ﬂow latency and tolerance of ﬂow bursts. Here we
look at other aspects of D3 performance.
Throughput and Queuing. We ﬁrst evaluate the behav-
ior of D3 with long ﬂows to determine the network through-
put achieved. To this eﬀect, we start a varying number of
ﬂows (2-20) to an endhost and record the ﬂow throughput
at 1ms intervals. Figure 16(a) shows that the aggregate
throughput remains the same as we increase the number of
long ﬂows with a median and average network throughput
of 0.95Gbps (95% of capacity). Overall, D3 matches the
performance oﬀered by TCP for long ﬂows.
2 Flows
5 Flows
10 Flows
20 Flows
)
%
(
F
D
C
 100
 80
 60
 40
 20
 0
)
%
(
F
D
C
 100
 80
 60
 40
 20
 0
2 Flows
5 Flows
10 Flows
20 Flows
 0.7
 0.8
 0.9
 1
 0  10  20  30  40  50  60
Aggregate Throughput (Gbps)
Queue Size (KB)
(a) Aggregate throughput
(b) Queue size
Figure 16: D3 performance with long ﬂows.
We also measured the instantaneous queue length on the
bottleneck link to determine D3’s queuing behavior. This is
plotted in Figure 16(b). For all scenarios, the average queue
size is 4-6KB, the median is 1.5KB while the 99th percentile
is 16.5-48KB. As a contrast, TCP tends to drive long ﬂows
to ﬁll switch buﬀers, resulting in much larger queues. This
impacts any short (deadline) ﬂows that share the same bot-
tleneck. Even compared to DCTCP, D3 achieves the same
throughput with queues that are shorter by a factor of ﬁve.
Multi-hop, multi-bottleneck setting. We have also eval-
uated the performance of D3 in a multi-hop network. Due
to space limitations, the results are discussed in [26]. Over-
all, our experiments show that D3 performs well in scenarios
with both multiple hops as well as multiple bottlenecks.
7. DISCUSSION
While we have evaluated many aspects of D3 design and
performance, a number of issues were not discussed in detail.
We brieﬂy comment on the most important of these here.
Deployability. D3 takes a radical tact to align the datacenter
network to application requirements. It mandates changes
to almost every participant: applications, endhosts, and net-
work elements. While in all cases these changes might be
considered easy to implement, the choice of modifying the
main components is quite intentional. Discussions with both
datacenter operators and application designers have revealed
that they are quite willing to adopt new designs that would
free them from the artiﬁcial restrictions posed by existing
retroﬁtted protocols. This is especially true if the added
beneﬁt is signiﬁcant, as in the case of D3. The use of a
UDP-based transport protocol by Facebook is a good exam-
ple of the extra miles designers are willing to go to overcome
current limitations [22].
The biggest hurdle to D3 deployment may be the changes
necessitated to network elements. From a technical per-
spective, we have strived to restrict the state maintained
and the processing done by a D3 router to a bare mini-
mum. This allowed our user-space software implementation
to easily achieve line-rates and bodes well for a hardware
implementation. For instance, like RCP [11], D3 can be im-
plemented on NetFPGA boards.
As mentioned in Section 4, co-existence with existing pro-
tocols, such as TCP or other non-D3 traﬃc, was a non-goal
for D3. However, we do admit that a ﬂag day where an en-
tire datacenter moves to D3-only traﬃc may not be realistic,
and incremental deployment is desirable. In this regard, we
believe that D3’s performance should not be impacted by
low-rate TCP or UDP traﬃc (e.g., control traﬃc), as rate
60allocations at routers do account for the observed utilization
through the estimation of C. However, a detailed examina-
tion of D3’s performance in the presence of existing proto-
cols, and incremental deployment with a mix of D3-enabled
and legacy switches is beyond the scope of this work.
Soft vs. hard deadlines. Throughout the paper, D3 op-
erates under the assumption that deadlines are hard, and
once missed, ﬂows are useless. This decision was intentional
to stress a, perhaps, extreme design point: Being deadline
aware provides signiﬁcant value to the network. On the other
hand, one can imagine applications and ﬂows that operate
on soft deadlines. Such ﬂows may, for example, gracefully
degrade their performance once the deadline is missed with-
out needing to be quenched. The D3 model can accommo-
date soft deadlines in a number of ways. For example, since
the host controls the requested rate, ﬂows with soft require-
ments could extend their deadlines if these are missed, or
fall back to fair share mode; alternatively, a two-stage al-
location process in the router could be implemented, where
demands are met in the order of importance (e.g., where not
all deadlines are of equal value) depending on the network
congestion. Yet, even with the presence of soft deadlines,
the evaluation in Section 6 stresses the beneﬁts of deadline-
aware protocols. Another extended use-case involves persis-
tent ﬂows with changing deadlines. D3 can be modiﬁed to
handle this as long as deadline changes are communicated
to the protocol. Such modiﬁcations as well as the long-term
stability of the protocol under various types of workloads or
end-host failure need further investigation.
8. CONCLUDING REMARKS
D3 is a control protocol that uses application deadline
information to achieve informed allocation of network band-
width. It explicitly deals with the challenges of the datacen-
ter environment - small RTTs, and a bursty, diverse traﬃc
mix with widely varying deadlines. Our evaluation shows
that D3 is practical and provides signiﬁcant beneﬁts over
even optimized versions of existing solutions. This, we be-
lieve, is a good illustration of how datacenters that are tuned
to application requirements and exploit inherent aspects of
the underlying network can perform above and beyond the
current state of the art. Emerging trends indicate that op-
erators are willing to adopt new designs that address their
problems and this bodes well for D3 adoption.
Acknowledgements
We are grateful to Greg O’Shea, Austin Donnelly, and Paolo
Costa for their valuable help. We would also like to thank
David Oran for his help and comments during the shepherd-
ing process.
9. REFERENCES
[1] H. Abu-Libdeh, P. Costa, A. Rowstron, G. O’Shea, and
A. Donnelly. Symbiotic routing in future data centers. In
ACM SIGCOMM, 2010.
[2] M. Al-Fares, A. Loukissas, and A. Vahdat. A Scalable,
Commodity Data Center Network Architecture. In Proc. of
ACM SIGCOMM, 2008.
[3] M. Alizadeh, B. Atikoglu, A. Kabbani, A. Laksmikantha,
R. Pan, B. Prabhakar, and M. Seaman. Data center
transport mechanisms: congestion control theory and IEEE
standardization. In Proc. of Allerton Conference on
Communications, Control and Computing, Sept. 2008.
[4] M. Alizadeh, A. G. Greenberg, D. A. Maltz, J. Padhye,
P. Patel, B. Prabhakar, S. Sengupta, and M. Sridharan.
Data center TCP (DCTCP). In ACM SIGCOMM, 2010.
[5] C. Aras, J. Kurose, D. Reeves, and H. Schulzrinne.
Real-time communication in packet-switched networks.
Proc.of the IEEE, 82(1), 1994.
[6] D. Beaver, S. Kumar, H. C. Li, J. Sobel, and P. Vajgel.
Finding a Needle in Haystack: Facebook’s Photo Storage.
In Proc. of OSDI, 2010.
[7] B. B. Chen and P.-B. Primet. Scheduling
deadline-constrained bulk data transfers to minimize
network congestion. In CCGRID, May 2007.
[8] Y. Chen, R. Griﬃth, J. Liu, R. H. Katz, and A. D. Joseph.
Understanding TCP incast throughput collapse in
datacenter networks. In WREN, 2009.
[9] J. Dean and S. Ghemawat. MapReduce: Simpliﬁed Data
Processing on Large Clusters. In USENIX OSDI, 2004.
[10] G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati,
A. Lakshman, A. Pilchin, S. Sivasubramanian, P. Vosshall,
and W. Vogels. Dynamo: amazon’s highly available
key-value store. ACM SIGOPS, 41(6), 2007.
[11] N. Dukkipati. Rate Control Protocol (RCP): Congestion
control to make ﬂows complete quickly. PhD thesis,
Stanford University, 2007.
[12] D. Ferrari, A. Banerjea, and H. Zhang. Network support
for multimedia: A discussion of the tenet approach. In
Proc. of Computer Networks and ISDN Systems, 1994.
[13] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula,
C. Kim, P. Lahiri, D. A. Maltz, P. Patel, and S. Sengupta.
VL2: a scalable and ﬂexible data center network. In Proc.
of ACM SIGCOMM, 2009.
[14] Y. Gu, C. V. Hollot, and H. Zhang. Congestion Control for
Small Buﬀer High Speed Networks. In Proc. of IEEE
INFOCOM, 2007.
[15] C. Guo, H. Wu, K. Tan, L. Shi, Y. Zhang, and S. Lu.
Dcell: a scalable and fault-tolerant network structure for
data centers. In Proc. of ACM SIGCOMM, 2008.
[16] T. Hoﬀ. 10 eBay Secrets for Planet Wide Scaling, Nov.
2009. http://highscalability.com/blog/2009/11/17/
10-ebay-secrets-for-planet-wide-scaling.html.
[17] T. Hoﬀ. Latency is Everywhere and it Costs You Sales -
How to Crush it, July 2009.
http://highscalability.com/blog/2009/7/25/latency-is-
everywhere-and-it-costs-you-sales-how-to-crush-it.html.
[18] M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly.
Dryad: Distributed Data-Parallel Programs from
Sequential Building Blocks. In Proc. of EuroSys, Mar. 2007.
[19] D. Katabi, M. Handley, and C. Rohrs. Congestion Control
for High Bandwidth-Delay Product Networks. In Proc. of
ACM SIGCOMM, Aug. 2002.
[20] R. Kohavi, R. Longbotham, D. Sommerﬁeld, and R. M.
Henne. Controlled experiments on the web: survey and
practical guide. Data Mining and Knowledge Discovery,
18(1), 2009.
[21] C. L. Liu and J. W. Layland. Scheduling Algorithms for
Multiprogramming in a Hard-Real-Time Environment.
Journal of the ACM, 20(1), 1973.
[22] P. Saab. Scaling memcached at Facebook, Dec. 2008. http:
//www.facebook.com/note.php?note_id=39391378919.
[23] V. Vasudevan, A. Phanishayee, H. Shah, E. Krevat, D. G.
Andersen, G. R. Ganger, G. A. Gibson, and B. Mueller.
Safe and eﬀective ﬁne-grained TCP retransmissions for
datacenter communication. In ACM SIGCOMM, 2009.
[24] W. Vogels. Performance and Scalability, Apr. 2009.
http://www.allthingsdistributed.com/2006/04/
performance_and_scalability.html.
[25] M. Welsh, D. Culler, and E. Brewer. Seda: an architecture
for well-conditioned, scalable internet services. In Proc. of
ACM SOSP, 2001.
[26] C. Wilson, H. Ballani, T. Karagiannis, and A. Rowstron.
Better never than late: Meeting deadlines in datacenter
networks. Technical Report MSR-TR-2011-66, Microsoft
Research, May 2011.
61