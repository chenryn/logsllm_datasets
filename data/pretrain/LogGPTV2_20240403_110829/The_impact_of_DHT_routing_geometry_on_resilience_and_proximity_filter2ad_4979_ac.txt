### Figures and Descriptions

**Figure 1: Path Failure and Hop-Count Analysis**

- **Left Panel:** This graph illustrates the percentage of failed paths for varying percentages of node failures across different routing geometries. The geometries compared are Butterfly, Hypercube, Hybrid, Ring, and Tree.
- **Right Panel:** This graph shows the percent increase in average path hop-counts of successful paths for varying percentages of node failures across different routing geometries. The Butterfly geometry is excluded from this graph because it has very few usable routes, and those that are usable sometimes take shorter paths than the original ones, resulting in a negative path stretch.

**Figure 2: Path Failure and Hop-Count Analysis (Continued)**

- **Left Panel:** This graph illustrates the percentage of failed paths for varying percentages of node failures across different routing geometries. The geometries compared are Ring, Hypercube, and Hybrid.
- **Right Panel:** This graph shows the percent increase in average path hop-counts of successful paths for varying percentages of node failures across different routing geometries. The Butterfly geometry is excluded from this graph because its path increase is significantly higher than the others, reaching up to 700%, which would distort the y-axis. All algorithms use 16 sequential neighbors.

**Figure 3: Path Failure and Hop-Count Analysis for Ring Geometry**

- **Left Panel:** This graph illustrates the percentage of failed paths for varying percentages of node failures for a Ring geometry, with varying numbers of neighbors (N) and sequential neighbors (L).
- **Right Panel:** This graph shows the average path hop-counts of the successful paths for varying percentages of node failures.

### Section 4: Path Latency

Distributed Hash Tables (DHTs) are designed to provide efficient routing, typically measured in terms of hopcount (the number of overlay hops between the source node and the destination node). While hopcount is an important metric for measuring the processing and bandwidth requirements at the peers, it does not adequately address the issue of end-to-end latency because each overlay hop could potentially involve significant delays (e.g., intercontinental links, satellite links, etc.). As a result, there has been much recent effort to reduce end-to-end latencies in DHT routing algorithms by considering the relative proximity of overlay nodes (i.e., the IP latency between them).

The proposed methods fall into three broad categories, two of which we consider here:

1. **Proximity Neighbor Selection (PNS):** The neighbors in the routing table are chosen based on their proximity.
2. **Proximity Route Selection (PRS):** Once the routing table is chosen, the choice of the next-hop when routing to a particular destination depends on the proximity of the neighbors.
3. **Proximity Identifier Selection (PIS):** As explored in [21], one can pick the node identifiers based on their geographic location. Since this makes load balancing hard and increases the likelihood of correlated failures, we do not discuss this method here.

We thus consider the two proximity methods, PNS and PRS. The section begins with a short description of these methods, and the rest of the section is devoted to their analysis. While our evaluation is based on recursive (as opposed to iterative [3]) routing, we believe that our key conclusions regarding the relative performance of PNS and PRS hold true for iterative routing as well. Confirming this belief is the subject of future work.

#### 4.1 PNS and PRS

In DHT algorithms that have flexibility in choosing neighbors, typically these neighbors have to be chosen from some subset of the identifier space. The ideal PNS algorithm would be to select the closest neighbors (as measured by latency) in these subsets. For example, the subset for the ith neighbor of a node a in a Tree geometry is the identifier space of the sub-tree at depth i containing the node, while in a Ring geometry it is the identifier space [(a+2i), (a+2i+1)].

However, identifying the closest nodes is hard in practice, as the sizes of the subsets grow exponentially with i. So, various heuristics have been proposed in [2, 8] to approximate the performance of ideal PNS. Here, we define one such heuristic, dubbed PNS(K), that uses random sampling. PNS(K) samples K consecutive nodes starting from the first element in the relevant subset and picks the closest one. We do not dwell here on how one should pick K, but in general, a node can make a reasonable choice of K after inspecting its latency distribution (see below). From now on, we use the term PNS to refer to ideal PNS.

The PRS algorithms have to deal with a more complicated trade-off between the number of hops and the latency. Any neighbor closer to the destination in the identifier space is a valid next hop, and without proximity, the next hop is chosen in a greedy fashion to decrease the number of hops. While there are a number of heuristics that trade hops for latency, we focus on three heuristics that we found effective for each of the Ring, XOR, and Hypercube geometries.

- **Ring Heuristic:** This takes advantage of the multiple paths with equal numbers of hops to a destination and chooses the next hop from a subset of neighbors, called the candidate set, which do not (usually) increase the routing path hops. To select the candidate set, the distance to the destination is expressed in binary notation, and neighbor i is chosen to the set if there is a 1 in the ith position. The closest member of the candidate set is picked as the next hop. When coupled with PNS(K), the algorithm disallows the closest log k neighbors from the candidate set (unless, of course, the destination lies within the closest log k neighbors).
- **XOR Heuristic:** This takes a non-greedy next hop only when its latency is smaller than the latency of the greedy next hop choice by more than the average latency in the network. This primarily helps to avoid very long hops.
- **Hypercube Heuristic:** All the alternate paths have the same number of hops, so our PRS heuristic is very simple. From the valid next hops, we pick the one with the smallest latency.

#### 4.2 Role of Topology and Latency

One of the aspects that makes it hard to understand proximity methods is that their performance depends critically on the underlying topology and its latency characteristics. While there is a large literature describing possible approaches to topology modeling, there is little known about how to assign latencies in such a topology. Previous studies [2, 8, 9, 21] evaluate proximity methods using one or more of these topology generators along with some rather ad hoc choices of latency assignments, but they do not analyze how their choice of latencies affects the performance of the proximity methods. Thus, research into proximity methods is now in a position where we neither know how to describe the real-world latencies nor understand their effect on our proposed algorithms.

We propose one possible way out of this bind. We conjecture that the effect of topology and latencies, for a large class of networks including the current Internet graph, can be reasonably well approximated by looking only at the latency distribution as seen from a "typical" node. That is, we conjecture that when choosing neighbors or next-hops, it is a reasonable approximation to consider the set of possibilities as coming from an independent drawing from the given latency distribution.

If this conjecture holds, then there are two immediate benefits. First, one can empirically measure the latency distribution of the Internet from various suitably located hosts, so that one need not guess at latency assignments in an Internet topology model. Second, given this measured distribution, one can compute (not merely simulate) an approximation to the expected performance of a proximity method. To evaluate our conjecture, we simulate the performance of the various proximity methods over a latency-annotated network topology and compare them with their performance computed using only the latency distribution seen by a random node in the topology. In doing so, we make a further approximation that the latency distribution is uniform across all nodes. As we will see below, the results from this very simple and rough approximation agree rather well with our simulation results (see Figure 5).

To illustrate the real-world latencies, we used data from the Skitter project [17] and a P2P measurement project [23] to plot the latencies to a large number of end hosts spread across the Internet as seen from various geographical locations in Figure 4(a) (similar measurements can be seen in [26] and elsewhere). The end hosts measured in the Skitter project cover a large fraction of routable IP prefixes, while those measured in the P2P project are Gnutella hosts. A striking feature common to all these latency graphs is that the curves rise sharply in a certain latency range, indicating a heavy concentration of nodes within the latency range. We note that these latency graphs differ significantly from the assumptions required in [10, 16] to prove their bounds. In Figure 4(b), we show the latency distribution from a typical node in our 16,384 node GT-ITM topology that we used for our simulation results presented later. An important difference between the observed and GT-ITM latencies is that a non-negligible fraction of the observed real-world latencies are very large. For the GT-ITM case, there are no paths that are more than double the median latency, whereas in all the observed distributions, at least 10% of the paths have latencies double that of their median.