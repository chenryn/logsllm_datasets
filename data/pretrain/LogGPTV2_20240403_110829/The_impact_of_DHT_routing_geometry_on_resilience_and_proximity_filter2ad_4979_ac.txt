Butterfly
Hypercube
Hybrid
10
20
30
40
50
60
70
80
90
XOR
Ring
Tree
Hypercube
Hybrid
0
10
20
30
40
50
60
70
80
90
Failed nodes (%)
Failed nodes (%)
Figure 1: Left: Percentage of failed paths for varying percentages of node failures across diﬀerent routing
geometries. Right: Percent increase in average path hop-counts of successful paths for varying percentages of
node failures across diﬀerent routing geometries. The Butterﬂy is left oﬀ of this graph because so few routes
are usable, and those that are sometimes take shorter paths than the original ones, resulting in a negative
path stretch.
)
%
(
s
h
t
a
p
d
e
l
i
a
F
100
90
80
70
60
50
40
30
20
10
0
Ring
Hypercube
Butterfly
Hybrid
0
10
20
30
40
50
60
70
80
90
)
%
(
s
t
n
u
o
c
p
o
h
t
h
a
p
g
v
A
n
i
e
s
a
e
r
c
n
I
200
150
100
50
0
0
Ring
Hybrid
Hypercube
10
20
30
40
50
60
70
80
90
Failed nodes (%)
Failed nodes (%)
Figure 2: Left: Percentage of failed paths for varying percentages of node failures across diﬀerent routing
geometries. Right: Percent increase in average path hop-counts of successful paths for varying percentages of
node failures across diﬀerent routing geometries. Butterﬂy is left oﬀ of this graph because its path increase
is so much higher than the others, reaching 700%, that it would distort the y-axis. All algorithms use 16
sequential neighbors.
ures, though path stretch can get much worse. Finally, for
the Ring topologies, replacing additional sequential neigh-
bors with regular neighbors yields a similar, but smaller, in-
creased resistance to path failures but a much smaller path
latency.
4. PATH LATENCY
DHTs are designed to provide eﬃcient routing as mea-
sured in terms of hopcount (the number of overlay hops be-
tween the source node and the destination node). While
hopcount is an important metric for measuring the pro-
cessing and bandwidth requirements at the peers, it does
not adequately address the issue of end-to-end latency be-
cause each overlay hop could potentially involve signiﬁcant
delays (intercontinental links, satellite links, etc.). As a re-
sult, there has been much recent eﬀort to reduce end-to-end
latencies in DHT routing algorithms by considering the rela-
tive proximity of overlay nodes (i.e., the IP latency between
them) [2, 8, 9, 10, 16, 21, 27]. The proposed methods fall
into three broad categories, two of which we consider here.
Proximity Neighbor Selection (PNS): The neighbors in
the routing table are chosen based on their proximity.
Proximity Route Selection (PRS): Once the routing ta-
ble is chosen, the choice of next-hop when routing to
a particular destination depends on the proximity of
the neighbors.
Proximity identiﬁer Selection (PIS): As explored in [21],
one can pick the node identiﬁers based on their ge-
ographic location. Since this makes load balancing
hard, and increases the likelihood of correlated fail-
ures, we don’t discuss this method here.
We thus consider the two proximity methods PNS and
PRS. The section begins with a short description of these
methods, and the rest of the section is devoted to their anal-
ysis. While our evaluation is based on recursive (as opposed
to iterative [3]) routing, we believe that our key conclusions
regarding the relative performance of PNS and PRS hold
true for iterative routing too. Conﬁrming this belief is the
subject of future work.
Evaluating proximity methods requires the topology of
an underlying network along with its link latencies. Testing
proximity methods on only one or two topologies doesn’t en-
sure that the results will generalize. So, after deﬁning PNS
and PRS, we discuss whether we can more generally under-
)
%
(
s
h
t
a
p
d
e
l
i
a
F
100
90
80
70
60
50
40
30
20
10
0
N=16, L=16 Ring
N=16, L=48 Ring
N=32, L=1 Ring
N=64, L=1 Ring
0
10
20
30
40
50
60
70
80
90
s
t
n
u
o
c
-
p
o
h
h
t
a
p
e
g
a
r
e
v
A
25
20
15
10
5
0
N=16, L=16 Ring
N=16, L=48 Ring
N=32, L=1 Ring
N=64, L=1 Ring
0
10
20
30
40
50
60
70
80
90
Failed nodes (%)
Failed nodes (%)
Figure 3: Left: Percentage of failed paths for varying percentages of node failures for a Ring geometry
for varying numbers of neighbors (N) and sequential neighbors (L). Right: Average path hop-counts of the
successful paths for varying percentages of node failures.
stand the role of topology and link latencies in proximity
methods.
After these preliminaries, we ﬁnally address the question
of how geometry aﬀects path latency in DHT routing algo-
rithms. Our analysis style here is diﬀerent from that of the
previous section on static resilience; there, we were compar-
ing the detailed diﬀerences in geometries. Here, we will ﬁnd
that the geometries themselves make less diﬀerence than
whether or not they can support PNS and/or PRS. Thus,
this section is really a comparison of those two proximity
methods. As discussed in Section 2, some geometries are
capable of PNS, some of PRS, and others of both. In this
section, we wish to evaluate the relative performance of PNS
and PRS to see whether restrictions on adopting them is a
signiﬁcant hindrance.
4.1 PNS and PRS
In DHT algorithms that have ﬂexibility in choosing neigh-
bors, typically these neighbors have to be chosen from some
subset of the identiﬁer space. The ideal PNS algorithm
would be to select the closest neighbors (as measured by
latency) in these subsets. For example, the subset for the
ith neighbor of a node a in a Tree geometry is the identiﬁer
space of the sub-tree at depth i containing the node, while in
a Ring geometry it is the identiﬁer space [(a+2i), (a+2i+1)].
However, identifying the closest nodes is hard in practice,
as the sizes of the subsets grow exponentially with i. So,
various heuristics have been proposed in [2, 8] to approx-
imate the performance of ideal PNS. Here, we deﬁne one
such heuristic, dubbed PNS(K) that uses random sampling.
PNS(K) samples K consecutive nodes starting from the ﬁrst
element in the relevant subset and picks the closest one. We
don’t dwell here on how one should pick K, but in general a
node can make a reasonable choice of K after inspecting its
latency distribution (see below). From now on, we use the
term PNS to refer to ideal PNS.
The PRS algorithms have to deal with a more complicated
tradeoﬀ between the number of hops and the latency. Any
neighbor closer to the destination in the identiﬁer space is
a valid next hop, and without proximity, the next hop is
chosen in a greedy fashion to decrease the number of hops.
While there are a number of heuristics that trade hops for
latency, we focus on three heuristics that we found eﬀective
for each of the Ring, XOR and Hypercube geometries.
The heuristic for Ring takes advantage of the multiple
paths with equal number of hops to a destination (see Ta-
ble 1) and chooses the next hop from a subset of neighbors,
called the candidate set, which do not (usually) increase the
routing path hops. To select the candidate set, the distance
to the destination is expressed in binary notation, and neigh-
bor i is chosen to the set if there is a 1 in the ith position.
The closest member of the candidate set is picked as the next
hop. When coupled with PNS(K), the algorithm disallows
the closest log k neighbors from the candidate set (unless,
of course, the destination lies within the closest log k neigh-
bors).10 However, this heuristic cannot be applied to XOR
as its geometry does not have the luxury of multiple paths
with the same number of hops to the destination. Our PRS
heuristic for XOR takes a non-greedy next hop only when
its latency is smaller than the latency of the greedy next
hop choice by more than the average latency in the network.
This primarily helps to avoid very long hops. In Hypercube,
all the alternate paths have the same number of hops, so our
PRS heuristic is very simple. From the valid next hops, we
pick the one with smallest latency.
We now investigate the role of topology in determining
the eﬀectiveness of these proximity methods.
4.2 Role of Topology and Latency
One of the aspects that makes it hard to understand prox-
imity methods is that their performance depends so criti-
cally on the underlying topology and its latency character-
istics. While there is a large literature describing possible
approaches to topology modeling – starting with the ini-
tial random graphs of Waxman, to the structural generators
in GT-ITM (transit-stub and tiers), to the more recent set
of power-law degree-based generators – there is little known
about how to assign latencies in such a topology. While pre-
vious studies [2, 8, 9, 21] evaluate proximity methods using
one or more of these topology generators along with some
rather ad hoc choice of latency assignments, they don’t an-
alyze how their choice of latencies aﬀects the performance
of the proximity methods. Thus, research into proximity
methods is now in a position where we neither know how to
describe the real-world latencies nor understand their eﬀect
on our proposed algorithms.
10Thus, this version of PRS adds nothing when combined
with the ideal PNS algorithm, which eﬀectively has inﬁnite
K.
Figure 4: The CDF of latency distributions for (a) the Internet as seen from diﬀerent geographic locations
and (b) a 16,384 node GT-Itm topology as seen from a typical node.
(a)
(b)
We propose one possible way out of this bind. We con-
jecture that the eﬀect of topology and latencies, for a large
class of networks including the current Internet graph, can
be reasonably well approximated by looking only at the la-
tency distribution as seen from a “typical” node. That is,
we conjecture that when choosing neighbors or next-hops,
it is a reasonable approximation to consider the set of pos-
sibilities as coming from an independent drawing from the
given latency distribution.11
If this conjecture holds, then there are two immediate ben-
eﬁts. First, one can empirically measure the latency distri-
bution of the Internet from various suitably located hosts, so
that one need not guess at latency assignments in an Internet
topology model. Second, given this measured distribution
one can compute (not merely simulate) an approximation to
the expected performance of a proximity method. To evalu-
ate our conjecture, we simulate the performance of the var-
ious proximity methods over a latency annotated network
topology and compare them with their performance com-
puted using only the latency distribution seen by a random
node in the topology. In doing so, we make a further ap-
proximation that the latency distribution is uniform across
all nodes. As we will see below, the results from this very
simple and rough approximation agree rather well with our
simulation results (see Figure 5).
To illustrate the real-world latencies, we used data from
the Skitter project [17] and a P2P measurement project [23]
to plot the latencies to a large number of end hosts spread
across the Internet as seen from various geographical loca-
tions in Figure 4(a) (similar measurements can be seen in
[26] and elsewhere). The end hosts measured in the Skitter
project cover a large fraction of routable IP preﬁxes, while
those measured in the P2P project are Gnutella hosts. A
striking feature common to all these latency graphs is that
the curves rise sharply in a certain latency range, indicating
a heavy concentration of nodes within the latency range. We
note that these latency graphs diﬀer signiﬁcantly from the
assumptions required in [10, 16] to prove their bounds. In
Figure 4(b), we show the latency distribution from a typical
node in our 16,384 node GT-ITM topology that we used for
our simulation results presented later. An important diﬀer-
ence between the observed and GT-ITM latencies is that a
11We acknowledge that this approximation suﬀers if the
latency distribution varies substantially and qualitatively
from point-to-point. However, the approximation need only
be good enough to capture the relative merits of diﬀerent
approaches, and is not intended to provide quantitatively
accurate descriptions of a method’s performance.
non-negligible fraction of the observed real-world latencies
are very large. For the GT-ITM case, there are no paths
that are more than double the median latency, whereas in
all the observed distributions atleast 10% of the paths have
latencies double that of their median.
F
D
C
100
90
80
70
60
50
40