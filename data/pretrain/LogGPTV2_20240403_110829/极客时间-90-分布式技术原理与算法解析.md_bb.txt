# 14 \| 答疑篇：分布式事务与分布式锁相关问题你好，我是聂鹏程。 到目前为止，"分布式技术原理与算法解析"专栏已经更新 13篇文章了，主要与你介绍了"分布式起源""分布式协调与同步"和"分布式资源管理与负载调度"。 在这里，我首先要感谢你们在评论区留下的一条条精彩留言。这让我感觉到，你们对分布式技术的浓厚兴趣，以及对我和对这个专栏的支持。这不但活跃了整个专栏的氛围、丰富了专栏的内容，也让我备受鼓舞，干劲儿十足地去交付更高质量的文章。 比如，@xfan、@Jackey等同学积极思考问题，并对文中有疑惑的地方结合其他资料给出了自己的思考和建议；再比如，@zhaozp、@静水流深等同学，每次更新后都在坚持打卡学习；再比如，@约书亚等同学针对分布式事务提出了非常好的问题，@每天晒白牙等同学对文中内容进行了非常好的总结。 这些同学有很多，我就不再一一点名了。感谢你们的同时，我也相信，积极参与并留言也会帮助你更深入地理解每一个知识点。所以我希望，你接下来可以多多留言给我，让我们一起学习，共同进步。 留言涉及的问题有很多，但我经过进一步地分析和总结后，发现针对分布式事务和分布式锁的问题比较多，同学们的疑惑也比较多。 确实，这两大问题属于分布式技术的关键问题。因此，今天的这篇答疑文章，我就围绕这两大问题来进行一次集中的分析和回答吧。 我们先来看一下分布式事务的相关问题。 分布式事务的相关问题在第 6 篇文章"分布式事务：All orNothing"  slate-object="inline"中，我介绍了两阶段提交协议和三阶段提交协议。有很多同学提出了疑问：两阶段提交协议（2PC）和三阶段提交协议（3PC）的区别，到底是什么？ 在回答这个问题前，我建议你先回到第 6 篇文章slate-object="inline"，去回忆一下它们的流程。然后，我们看看**2PC 和 3PC的第一步到底是不是"类似"的？** 2PC的第一步投票（voting）阶段中，参与者收到事务执行询问请求时，就执行事务但不提交；而3PC 却写着在 PreCommit 阶段执行事务不提交。2PC 和 3PC的第一步，是非常不类似吧？ 其实，我说它们类似，是指它们均是通过协调者，来询问参与者是否可以正常执行事务操作，参与者也都会给协调者回复。 1.  在 2PC    中，如果所有参与者都返回结果后，会进入第二阶段，也就是提交阶段，也可以说是执行阶段，根据第一阶段的投票结果，进行提交或取消。        2.  在 3PC    中，进入真正的提交阶段前，还会有一个预提交阶段，这个预提交阶段不会做真正的提交，而是会将相关信息记录到事务日志中，当所有参与者都返回    Yes    消息后，才会真正进入提交阶段。        这样说明后，相信你对这个问题的疑惑应该解决了吧。 现在，我们继续延展一下这个问题吧。 **追问 1：**3PC 在预提交阶段，才开始执行事务操作，那协调者发送CanCommit 给参与者的时候，参与者根据什么返回 Yes 或者 No消息呢? 3PC 在投票阶段（CanCommit 阶段），协调者发送 CanCommit询问后，参与者会根据自身情况，比如自身空闲资源是否足以支撑事务、是否会存在故障等，预估自己是否可以执行事务，但不会执行事务，参与者根据预估结果给协调者反回Yes 或者 No 消息。 **追问 2：**3PC 出现的目的是，解决 2PC的同步阻塞和数据不一致性问题。那么，我们不可以在 2PC中直接去解决这些问题吗？3PC多了预提交和超时机制，就真的解决这些问题了吗？ **我们先来看看同步阻塞的问题。** 在 2PC中，参与者必须等待协调者发送的事务操作指令，才会执行事务，比如提交事务或回滚等操作，如果协调者故障，也就是说参与者无法收到协调者的指令了，那么参与者只能一直等待下去。这就好比在一个班级里面，班主任是协调者，学生是参与者，班主任告诉学生今天下午6点组织一个比赛，但班主任今天生病了，根本到不了学校，并且也无法发送信息告诉学生，那么学生们就只能一直等待。 3PC 在协调者和参与者中都引入了超时机制（2PC只是在协调者引入了超时），也就是说当参与者在一定时间内，没有接收到协调者的通知时，会执行默认的操作，从而减少了整个集群的阻塞时间。这就好比班主任生病了，学生默认等待半个小时，如果班主任还没有任何通知，那么默认比赛取消，学生可以自由安排，做自己的事情去了。 但其实，阻塞在实际业务中是不可能完全避免的。在上面的例子中，学生等待超时的半个小时中，其实还是阻塞的，只是阻塞的时间缩短了。所以，相对于2PC 来说，3PC只是在一定程度上减少（或者说减弱）了阻塞问题。 **接下来，我们再看看数据不一致的问题吧。** 通过上面的分析可以看到，同步阻塞的根本原因是协调者发生故障，想象一下，比如现在有10 个参与者，协调者在发送事务操作信息的时候，假设在发送给了 5个参与者之后发生了故障。在这种情况下，未收到信息的 5个参与者会发生阻塞，收到信息的 5 个参与者会执行事务，以至于这 10个参与者的数据信息不一致。 3PC 中引入了预提交阶段，相对于 2PC来讲是增加了一个预判断，如果在预判断阶段协调者出现故障，那就不会执行事务。这样，可以在一定程度上减少故障导致的数据不一致问题，尽可能保证在最后提交阶段之前，各参与节点的状态是一致的。 所以说，3PC 是研究者们针对 2PC中存在的问题做的一个改进，虽然没能完全解决这些问题，但也起到了一定的效果。 在实际使用中，通常采用多数投票策略来代替第一阶段的全票策略，比如 Raft算法等。关于 Raft 算法的具体原理，你可以再回顾下第 4篇文章"分布式选举：国不可一日无君slate-object="inline""中的相关内容。 **追问 3：3PC也是只有一个协调者，为什么就不会有单点故障问题了？** 首先，我先明确下这里所说的单点故障问题。 因为系统中只有一个协调者，那么协调者所在服务器出现故障时，系统肯定是无法正常运行的。所以说，2PC和 3PC 都会有单点故障问题。 但是，3PC因为在协调者和参与者中都引入了超时机制，可以减弱单点故障对整个系统造成的影响。为什么这么说呢？ 因为引入的超时机制，参与者可以在长时间没有得到协调者响应的情况下，自动将超时的事务进行提交，不会像2PC 那样被阻塞住。 好了，以上就是关于分布式事务中的 2PC 和 3PC的相关问题了，相信你对这两个提交协议有了更深刻的认识。接下来，我们再看一下分布式锁的相关问题吧。 分布式锁的相关问题在第 7 篇文章"分布式锁：关键重地，非请勿入"slate-object="inline"后的留言中，我看到很多同学都问到了分布式互斥和分布式锁的关系是什么。 我们先来回顾下，分布式互斥和分布式锁分别是什么吧。 在分布式系统中，某些资源（即临界资源）同一时刻只有一个程序能够访问，这种排他性的资源访问方式，就叫作**分布式互斥。**这里，你可以再回顾下第 3 篇文章slate-object="inline"中的相关内容。 **分布式锁**指的是，在分布式环境下，系统部署在多个机器中，实现多进程分布式互斥的一种锁。这里，你可以再回顾下第 7 篇文章slate-object="inline"中的相关内容。 分布式锁的目的是，保证多个进程访问临界资源时，同一时刻只有一个进程可以访问，以保证数据的正确性。因此，我们可以说分布式锁是实现分布式互斥的一种手段或方法。 除了分布式互斥和分布式锁的关系外，很多同学都针对基于 ZooKeeper 和基于Redis实现分布式锁，提出了不少好问题。我们具体看看这些问题吧。 **首先，我们来看一下基于 ZooKeeper实现分布式锁的问题。**有同学问，ZooKeeper分布式锁，可能存在多个节点对应的客户端在同一时间完成事务的情况吗？ 这里，我需要先澄清一下，ZooKeeper不是分布式锁，而是一个分布式的、提供分布式应用协调服务的组件。基于ZooKeeper 的分布式锁是基于 ZooKeeper的数据结构中的临时顺序节点来实现的。 请注意，这里提到了 ZooKeeper是一个分布式应用协调服务的组件。比如，在一个集中式集群中，以 Mesos为例，Mesos 包括 master 节点和 slave 节点，slave 节点启动后是主动去和master 节点建立连接的，但建立连接的条件是，需要知道 master 节点的 IP地址和状态等。而 master 节点启动后会将自己的 IP 地址和状态等写入ZooKeeper 中，这样每个 slave 节点启动后都可以去找 ZooKeeper 获取 master的信息。而每个 slave 节点与 ZooKeeper进行交互的时候，均需要一个对应的客户端。 这个例子，说明了存在多个节点对应的客户端与 ZooKeeper进行交互。同时，由于每个节点之间并未进行通信协商，且它们都是独立自主的，启动时间、与ZooKeeper交互的时间、事务完成时间都是独立的，因此存在多个节点对应的客户端在同一时间完成事务的这种情况。 接下来，我们看一下基于 Resis实现分布式锁的问题。**Redis为什么需要通过队列来维持进程访问共享资源的先后顺序？** 在我看来，这是一个很好的问题。 \@开心小毛认为，Redis 的分布式锁根本没有队列，收到 setnx 返回为 0的进程会不断地重试，直到某一次的重试成为 DEL 命令后第一个到达的 setnx从而获得锁，至于此进程在等待获得锁的众多进程中是不是第一个发出 setnx的，Redis 并不关心。 其实，客观地说，这个理解是合情合理的，是我们第一反应所能想到的最直接、最简单的解决方法。可以说，这是一种简单、粗暴的方法，也就是获取不到锁，就不停尝试，直到获取到锁为止。 但你有没有想过，当多个进程频繁去访问 Redis 时，Redis会不会成为瓶颈，性能会不会受影响。带着这个疑惑，我们来具体看看基于 Redis实现的分布式锁到底需不需要队列吧。 如果没有队列维护多进程请求，那我们可以想到的解决方式，就是我刚刚和你分析过的，通过多进程反复尝试以获取锁。 但，这种方式有三个问题： 1.  一是，反复尝试会增加通信成本和性能开销；        2.  二是，到底过多久再重新尝试；        3.  三是，如果每次都是众多进程进行竞争的话，有可能会导致有些进程永远获取不到锁。        在实际的业务场景中，尝试时间的设置，是一个比较难的问题，与节点规模、事务类型均有关系。 比如，节点规模大的情况下，如果设置的时间周期较短，多个节点频繁访问Redis，会给 Redis 带来性能冲击，甚至导致 Redis崩溃；对于节点规模小、事务执行时间短的情况，若设置的重试时间周期过长，会导致节点执行事务的整体时间变长。 基于队列来维持进程访问共享资源先后顺序的方法中，当一个进程释放锁之后，队列里第一个进程可以访问共享资源。也就说，这样一来就解决了上面提到的三个问题。 总结我针对前面 13篇文章留言涉及的问题，进行了归纳总结，从中摘取了分布式事务和分布式锁这两个知识点，串成了今天这篇答疑文章。 今天没来得及和你扩展的问题，后续我会再找机会进行解答。最后，我要和你说的是，和我一起打卡分布式核心技术，一起遇见更优秀的自己吧。 篇幅所限，留言区见。 我是聂鹏程，感谢你的收听，欢迎你在评论区给我留言分享你的观点，也欢迎你把这篇文章分享给更多的朋友一起阅读。我们下期再会！ ![](Images/c191f391e2aab7575517a886bbd7a681.png)savepage-src="https://static001.geekbang.org/resource/image/a4/8c/a42a16601611a1a72599ecfca434508c.jpg"}
# 15 \| 分布式计算模式之MR：一门同流合污的艺术你好，我是聂鹏程。今天，我来继续带你打卡分布式核心技术。 我在 [第 12篇文章  slate-object="inline"中与你介绍两层调度时提到，Mesos 的第二层调度是由 Framework完成的。这里的 Framework 通常就是计算框架，比如 Hadoop、Spark等。用户基于这些计算框架，可以完成不同类型和规模的计算。 那么，在接下来的 4篇文章，我们就要进入"第三站：分布式计算技术"了。在这一站，我将与你详细介绍分布式领域中的4 种计算模式，包括 MapReduce、Stream、Actor和流水线。而今天这篇文章，我们就先从 MR模式开始吧。 Hadoop这个框架主要用于解决海量数据的计算问题。那么，它是如何做到海量数据计算的呢？你可能会想，既然是海量数据，规模这么大，那就分成多个进程，每个进程计算一部分，然后汇总一下结果，就可以提升运算速度了。其实，整个计算流程，我们可以很形象地用一个词来解释，就是"同流合污"。 没错，就是这种想法，在分布式领域中就叫作 MR 模式，即 Map Reduce模式。接下来，我们就一起揭开 MR模式的神秘面纱吧。 什么是分而治之？分而治之（Divide-and-Conquer），是计算机处理问题的一个很重要的思想，简称为分治法。 顾名思义，分治法就是将一个复杂的、难以直接解决的大问题，分割成一些规模较小的、可以比较简单的或直接求解的子问题，这些子问题之间相互独立且与原问题形式相同，递归地求解这些子问题，然后将子问题的解合并得到原问题的解。 比如，现在要统计全中国的人口数，由于中国的人口规模很大，如果让工作人员依次统计每个省市的人口数，工作量会非常大。在实际统计中，我们通常会按照省分别统计，比如湖南省的工作人员统计湖南省的人口数，湖北省的工作人员统计湖北省的人口数等，然后汇总各个省的人口数，即可得到全国人口数。 这，就是一个非常好的分而治之的例子。 当然，这种分治的思想还广泛应用于计算机科学的各个领域中，分布式领域中的很多场景和问题也非常适合采用这种思想解决，并为此设计出了很多计算框架。比如，Hadoop中的 MapReduce。 那么，**在分布式领域，具体有哪些问题适合采用分治法呢？**要回答这个问题，我们先看下适合分治法的问题具有哪些特征吧。 1.  问题规模比较大或复杂，且问题可以分解为几个规模较小的、简单的同类型问题进行求解；        2.  子问题之间相互独立，不包含公共子问题；        3.  子问题的解可以合并得到原问题的解。        根据这些特征，我们可以想到，诸如电商统计全国商品数量时，按区域或省市进行统计，然后将统计结果合并得到最终结果等大数据处理场景，均可以采用分治法。 同时，根据这些特征，我们可以推导出，**采用分治法解决问题的核心步骤是**： 1.       分解原问题。将原问题分解为若干个规模较小，相互独立，且与原问题形式相同的子问题。        2.       求解子问题。若子问题规模较小且容易被解决则直接求解，否则递归地求解各个子问题。        3.       合并解，就是将各个子问题的解合并为原问题的解。        接下来，我们就一起看看分布式系统中分治法的原理和应用吧。 分治法的原理分布式原本就是为处理大规模应用而生的，所以基于分布式系统，如何分而治之地处理海量数据就是分布式领域中的一个核心问题。 Google 提出的 MapReduce 分布式计算模型（Hadoop MapReduce 是 Google的开源实现），作为分治法的典型代表，最开始用于搜索领域，后来被广泛用于解决各种海量数据的计算问题。下面，我将以MapReduce为例，带你了解分治法的抽象模型、工作原理和实践应用。 抽象模型如下图所示，MapReduce 分为 Map 和 Reduce 两个核心阶段，其中 Map对应"分"，即把复杂的任务分解为若干个"简单的任务"执行；Reduce对应着"合"，即对 Map阶段的结果进行汇总。 ![](Images/0e8228b075b279c7bb508c48f9dbb640.png)savepage-src="https://static001.geekbang.org/resource/image/04/98/04c72243a78728f41e152eba52735198.png"}在第一阶段，也就是 Map阶段，将大数据计算任务拆分为多个子任务，拆分后的子任务通常具有如下特征： 1.  相对于原始任务来说，划分后的子任务与原任务是同质的，比如原任务是统计全国人口数，拆分为统计省的人口数子任务时，都是统计人口数；并且，子任务的数据规模和计算规模会小很多。        2.  多个子任务之间没有依赖，可以独立运行、并行计算，比如按照省统计人口数，统计河北省的人口数和统计湖南省的人口数之间没有依赖关系，可以独立、并行的统计。        第二阶段，也就是 Reduce阶段，第一阶段拆分的子任务计算完成后，汇总所有子任务的计算结果，以得到最终结果。也就是，汇总各个省统计的人口数，得到全国的总人口数。 MapReduce 工作原理那么，在 MapReduce里，各个组件是如何分工完成一个复杂任务的呢？为了解答这个问题，我先带你了解一下MapReduce 的组件结构。 ![](Images/ef79c0c54e9dcfaddf9efeebf6c473ed.png)savepage-src="https://static001.geekbang.org/resource/image/d4/c9/d4676282f7c980953922b6f391cc98c9.png"}如上图所示，MapReduce主要包括以下三种组件： 1.  Master，也就是    MRAppMaster，该模块像一个大总管一样，独掌大权，负责分配任务，协调任务的运行，并为    Mapper 分配 map() 函数操作、为 Reducer 分配 reduce()    函数操作。        2.  Mapper worker，负责 Map    函数功能，即负责执行子任务。        3.  Reducer worker，负责 Reduce    函数功能，即负责汇总各个子任务的结果。        基于这三种组件，MapReduce的工作流程如下所示： ![](Images/21d361d40ec32ce3c9fe0381e561fbf4.png)savepage-src="https://static001.geekbang.org/resource/image/f3/f1/f3caa7b96cf59cbde8e9f9b0cc84a3f1.png"}程序从 User Program 开始进入 MapReduce操作流程。其中图中的"step1，step2，...，step6"表示操作步骤。 step1：User Program 将任务下发到 MRAppMaster 中。然后，MRAppMaster执行任务拆分步骤，把 User Program 下发的任务划分成 M 个子任务（M是用户自定义的数值）。假设，MapReduce 函数将任务划分成了 5 个，其中 Map作业有 3 个，Reduce 作业有 2 个；集群内的 MRAppMaster 以及 Worker节点都有任务的副本。 step2：MRAppMaster 分别为 Mapper 和 Reducer 分配相应的 Map 和 Reduce作业。Map 作业的数量就是划分后的子任务数量，也就是 3 个；Reduce 作业是 2个。 step3：被分配了 Map 作业的Worker，开始读取子任务的输入数据，并从输入数据中抽取出 \键值对，每一个键值对都作为参数传递给 map()函数。 step4：map() 函数的输出结果存储在环形缓冲区 kvBuffer 中，这些 Map结果会被定期写入本地磁盘中，被存储在 R 个不同的磁盘区。这里的 R 表示Reduce 作业的数量，也是由用户定义的。在这个案例中，R=2。此外，每个 Map结果的存储位置都会上报给MRAppMaster。 step5：MRAppMaster 通知 Reducer 它负责的作业在哪一个分区，Reducer远程读取相应的 Map 结果，即中间键值对。当 Reducer把它负责的所有中间键值对都读过来后，首先根据键值对的 key值对中间键值对进行排序，将相同 key 值的键值对聚集在一起，从而有利于Reducer 对 Map 结果进行统计。 step6：Reducer 遍历排序后的中间键值对，将具有相同 key值的键值对合并，并将统计结果作为输出文件存入负责的分区中。 从上述流程可以看出，**整个 MapReduce 的工作流程主要可以概括为 5个阶段**，即：Input（输入）、Splitting（拆分）、Mapping（映射）、Reducing（化简）以及Final Result（输出）。 所有 MapReduce 操作执行完毕后，MRAppMaster 将 R个分区的输出文件结果返回给 UserProgram，用户可以根据实际需要进行操作。比如，通常并不需要合并这 R个输出文件，而是将其作为输入交给另一个 MapReduce程序处理。 MapReduce 实践应用通过上述的流程描述，你大概已经知道 MapReduce的工作流程了。接下来，我和你分享一个电商统计用户消费记录的例子，再帮你巩固一下MapReduce 的功能吧。 需要注意的是，为了方便理解，我对下面用的数据做了一定的处理，并不完全是真实场景中的数据。 每隔一段时间，电商都会统计该时期平台的订单记录，从而分析用户的消费倾向。在不考虑国外消费记录的前提下，全国范围内的订单记录已经是一个很大规模的工程了。 在前面的文章中我也提到过，电商往往会在每个省份、多个城市分布式地部署多个服务器，用于管理某一地区的平台数据。因此，针对全国范围内的消费统计，可以拆分成对多个省份的消费统计，并再一次细化到统计每一个城市的消费记录。 为方便描述，假设我们现在要统计苏锡常地区第二季度手机订单数量 Top3的品牌。我们来看看具体的统计步骤吧。 1.       任务拆分（Splitting    阶段）。根据地理位置，分别统计苏州、无锡、常州第二季度手机订单 Top3    品牌，从而将大规模任务划分为 3    个子任务。        2.       通过循环调用 map() 函数，统计每个品牌手机的订单数量。其中，key    为手机品牌，value 为手机购买数量（单位：万台）。如下图 Mapping    阶段所示（为简化描述，图中直接列出了统计结果）。        3.       与前面讲到的计算流程不同的是，Mapping 阶段和 Reducing    阶段中间多了一步 Shuffling 操作。Shuffling 阶段主要是读取 Mapping    阶段的结果，并将不同的结果划分到不同的区。在大多数参考文档中，Mapping    和 Reducing    阶段的任务分别定义为映射以及归约。但是，在映射之后，要对映射后的结果进行排序整合，然后才能执行归约操作，因此往往将这一排序整合的操作单独放出来，称之为    Shuffling 阶段。        4.       Reducing    阶段，归并同一个品牌的购买次数。        5.       得到苏锡常地区第二季度 Top3    品牌手机的购买记录。        ![](Images/a52acf00a2c7e56718152e27655b4fb3.png)savepage-src="https://static001.geekbang.org/resource/image/81/b0/813f5311ab4df0ce94816a3a84946fb0.png"}由上述流程可以看出，**Map/Reduce 作业和 map()/reduce()函数是有区别的**： 1.  Map 阶段由一定数量的 Map 作业组成，这些 Map    作业是并发任务，可以同时运行，且操作重复。Map 阶段的功能主要由 map()    函数实现。每个 Map    作业处理一个子任务（比如一个城市的手机消费统计），需要调用多次 map()    函数来处理（因为城市内不同的居民倾向于不同的手机）。        2.  Reduce 阶段执行的是汇总任务结果，遍历 Map    阶段的结果从而返回一个综合结果。与 Reduce 阶段相关的是 reduce()    函数，它的输入是一个键（key）和与之对应的一组数据（values），其功能是将具有相同    key 值的数据进行合并。Reduce    作业处理一个分区的中间键值对，期间要对每个不同的 key 值调用一次    reduce() 函数。在完成 Map    作业后，每个分区中会存在多个临时文件；而执行完 Reduce    操作后，一个分区最终只有一个输出文件。        知识扩展：Fork-Join 计算模式是什么意思呢？MapReduce 是一种分而治之的计算模式，在分布式领域中，除了典型的 Hadoop的 MapReduce(Google MapReduce 的开源实现)，还有 Fork-Join。你知道Fork-join 是什么吗？ Fork-Join 是 Java等语言或库提供的原生多线程并行处理框架，采用线程级的分而治之计算模式。它充分利用多核CPU的优势，以递归的方式把一个任务拆分成多个"小任务"，把多个"小任务"放到多个处理器上并行执行，即Fork操作。当多个"小任务"执行完成之后，再将这些执行结果合并起来即可得到原始任务的结果，即Join 操作。 虽然 MapReduce 是进程级的分而治之计算模式，但与 Fork-Join的核心思想是一致的。因此，Fork-Join 又被称为 Java 版的 MapReduce框架。 但，MapReduce 和 Fork-Join之间有一个本质的区别： 1.  Fork-Join 不能大规模扩展，只适用于在单个 Java    虚拟机上运行，多个小任务虽然运行在不同的处理器上，但可以相互通信，甚至一个线程可以"窃取"其他线程上的子任务。        2.  MapReduce 可以大规模扩展，适用于大型计算机集群。通过 MapReduce    拆分后的任务，可以跨多个计算机去执行，且各个小任务之间不会相互通信。        总结所谓分而治之，就是将一个复杂的、难以直接解决的大问题，分割成一些规模较小的、可以直接求解的子问题，这些子问题互相独立且与原问题形式相同，递归地解这些子问题，然后将子问题的解合并以后就是原问题的解。分布式计算模型 MapReduce 就运用了分而治之的思想，通过 Map操作将大任务分成多个较小的任务去执行，得到的多个结果再通过 Reduce操作整合成一个完整的结果。所以，今天我就以 MapReduce为例，与你讲述了分布式领域中分治法的模型、原理与应用。最后，我将今天涉及的核心知识点梳理为了一张思维导图，以方便你理解与记忆。![](Images/48c90a5fcb7819293ad79ecc688258f2.png)savepage-src="https://static001.geekbang.org/resource/image/cc/ed/cc3a75001eeb7a1ae470831aa7770fed.png"}分而治之的思想，是简单且实用的处理复杂问题的方法。所以无论是计算机领域还是其他研究领域亦或日常生活中，我们都可以用分治法去处理很多复杂庞大的问题，将大问题划分成多个小问题，化繁为简、化整为零。其实，很多算法并不是凭空创造出来的，都是源于生活并服务于生活的。在日常工作学习中，我们对眼前的问题一筹莫展时，就可以将其化繁为简，从最简单的小问题出发，逐渐增加问题的规模，进而解决这个复杂的问题。同样的道理，我们也可以借鉴生活中的例子去解决专业问题。思考题MapReduce属于批量处理任务类型吗？你能说说其中的原因吗？我是聂鹏程，感谢你的收听，欢迎你在评论区给我留言分享你的观点，也欢迎你把这篇文章分享给更多的朋友一起阅读。我们下期再会！![](Images/c191f391e2aab7575517a886bbd7a681.png)savepage-src="https://static001.geekbang.org/resource/image/a4/8c/a42a16601611a1a72599ecfca434508c.jpg"}
# 16 \| 分布式计算模式之Stream：一门背锅的艺术你好，我是聂鹏程。今天，我来继续带你打卡分布式核心技术。在上一篇文章中，我与你介绍了分布式计算模式中的 MapReduce模式。这种模式的核心思想是，将大任务拆分成多个小任务，针对这些小任务分别计算后，再合并各小任务的结果以得到大任务的计算结果。这种模式下任务运行完成之后，整个任务进程就结束了，属于短任务模式。但，任务进程的启动和停止是一件很耗时的事儿，因此MapReduce对处理实时性的任务就不太合适了。实时性任务主要是针对流数据的处理，对处理时延要求很高，通常需要有常驻服务进程，等待数据的随时到来随时处理，以保证低时延。处理流数据任务的计算模式，在分布式领域中叫作Stream。 今天，我将针对流数据的处理展开分享，和你一起打卡 Stream这种计算模式。什么是 Stream？近年来，由于网络监控、传感监测、AR/VR等实时性应用的兴起，一类需要处理流数据的业务发展了起来。比如各种直播平台中，我们需要处理直播产生的音视频数据流等。这种如流水般持续涌现，且需要实时处理的数据，我们称之为**流数据**。总结来讲，流数据的特征主要包括以下 4点： 1.  数据如流水般持续、快速地到达；        2.  海量数据规模，数据量可达到 TB 级甚至 PB    级；    3.  对实时性要求高，随着时间流逝，数据的价值会大幅降低；        4.  数据顺序无法保证，也就是说系统无法控制将要处理的数据元素的顺序。        在分布式领域中，处理流数据的计算模式，就是**流计算，也叫作Stream**。这个名字是不是非常形象呢？流计算的职责是实时获取来自不同数据源的海量数据，进行实时分析处理，获得有价值的信息。它是一个对实时性要求非常高的计算形式，如果数据处理不及时，很容易导致过时、没用的结果，这时就需要对造成的后果进行"背锅"。从这个角度来说，Stream可谓"一门背锅的艺术"。类比于水流的持续不断且变幻莫测，流数据也是以大量、快速、时变的流形式持续在应用中产生，因此**流计算一般用于处理数据密集型应用**。比如，百度、淘宝等大型网站中，每天都会产生大量的流数据，这些数据包括用户的搜索内容、用户的浏览记录等。实时采集用户数据，并通过流计算进行实时数据分析，可以了解每个时刻数据流的变化情况，甚至可以分析用户的实时浏览轨迹，从而进行个性化内容实时推荐，提高用户体验。此外，我们常用的爱奇艺、腾讯等音视频平台，对电影、电视剧等数据的处理，也是采用了流计算模式。那么，这种实时的流计算到底是如何运行的呢？接下来，我们就一起看看流计算的工作原理吧。Stream 工作原理我在上一篇文章中与你介绍的MapReduce，是一种批量计算的形式。这种模式下，会先收集数据并将其缓存起来，等到缓存写满时才开始处理数据。因此，批量计算的一个缺点就是，从数据采集到得到计算结果之间经历的时间很长。而流计算强调的是实时性，数据一旦产生就会被立即处理，当一条数据被处理完成后，会序列化存储到缓存中，然后立刻通过网络传输到下一个节点，由下一个节点继续处理，而不是像MapReduce那样，等到缓存写满才开始处理、传输。为了保证数据的实时性，在流计算中，不会存储任何数据，就像水流一样滚滚向前。所以说，流计算属于持续性、低时延、事件驱动型的计算作业。从这些分析中可以看出，**使用流计算进行数据处理，一般包括 3个步骤**，如下图所示。![](Images/bdfc12214aeb706c47867a209fef7a89.png)savepage-src="https://static001.geekbang.org/resource/image/62/20/62eda28c3df9a4ab2fbe393b469bea20.png"}**第一步，提交流式计算作业。**流式计算作业是一种常驻计算服务，比如实时交通监测服务、实时天气预报服务等。对于流式计算作业，首先必须预先定义计算逻辑，并提交到流计算系统中，使得流计算系统知道自己该如何处理数据。系统在整个运行期间，由于收集的是同一类型的数据、执行的是同一种服务，因此流式计算作业的处理逻辑不可更改。如果用户停止当前作业运行后再次提交作业，由于流计算不提供数据存储服务，因此之前已经计算完成的数据无法重新再次计算。**第二步，加载流式数据进行流计算。**流式计算作业一旦启动将一直处于等待事件触发的状态，一旦有小批量数据进入流式数据存储，系统会立刻执行计算逻辑并迅速得到结果。从上图中我们可以看出，在流计算系统中，有多个流处理节点，流处理节点会对数据进行预定义的处理操作，并在处理完后按照某种规则转发给后续节点继续处理。此外，流计算系统中还存在管理节点，主要负责管理处理节点以及数据的流动规则。其中，处理节点的个数以及数据转发的规则，都在第一步作业提交时定义。**第三步，持续输出计算结果。**流式计算作业在得到小批量数据的计算结果后，可以立刻将结果数据写入在线/批量系统，无需等待整体数据的计算结果，以进一步做到实时计算结果的实时展现。**到这里，我们小结一下吧**。流计算不提供流式数据的存储服务，数据是持续流动的，在计算完成后就会立刻丢弃。流计算适用于需要处理持续到达的流数据、对数据处理有较高实时性要求的场景。为了及时处理流数据，流计算框架必须是低延迟、可扩展、高可靠的。流计算的应用场景有很多，比如它是网络监控、传感监测、AR/VR、音视频流等实时应用的发展的基础。所以，目前流计算相关的框架和平台也有很多了，主流的划分方式是将其分为如下3 类： 1.  商业级的流计算平台，比如 IBM 的 InfoSphere Streams 和 TIBCO 的    StreamBase。InfoSphere Streams    支持同时分析多种数据类型并实时执行复杂计算。StreamBase    是一个用于实时分析的软件，可以快速构建分析系统，即时做出决策。StreamBase    可以为投资银行、对冲基金、政府机构等提供实时数据分析服务。        2.  开源流计算框架，典型代表是 Apache Storm（由 Twitter 开源）和    S4（由 Yahoo 开源）。Storm    是一个分布式的、容错的实时计算系统，可以持续进行实时数据流处理，也可以用于分布式    RPC。S4    是一个通用的、分区容错的、可扩展的、可插拔的分布式流式系统。这些开源的分布式流计算系统由于具备开源代码，因此比较适合开发人员将其搭建在自身业务系统中。        3.  各大公司根据自身业务特点而开发的流计算框架，比如 Facebook 的    Puma、百度的    Dstream（旨在处理有向无环的数据流）、淘宝的银河流数据处理平台（一个通用的、低延迟、高吞吐、可复用的流数据实时计算系统）。        除了这些框架外，我们还会经常听到 Spark、Flink 等。Spark 和 Flink 与Storm 框架的不同之处在于，Spark 和 Flink除了支持流计算，还支持批量计算，因此我没有直接将它们列入上述的流计算框架中。如果你的业务中需要用到或者需要参考某种计算框架或者平台的话，可以再参考其官方文档或者相关的技术文章。接下来，我就以 Storm 这个开源的流计算框架为例，通过介绍 Storm的工作原理，以加深你对流计算模式的进一步理解，进而帮助你将其运用到实际业务中。Storm 的工作原理说到 Storm 的工作原理，我们先来对比下 Storm 与 MapReduce的区别吧。Hadoop 上运行的是"MapReduce 作业"，而 Storm上运行的是"计算拓扑（Topologies）"。"作业"和"拓扑"的一个关键区别是：MapReduce的一个作业在得到结果之后总会结束；而拓扑描述的是计算逻辑，该计算逻辑会永远在集群中运行（除非你杀死该进程）。如下图所示，Storm 集群上有两种节点，即主节点（MasterNode）和工作节点（WorkerNodes）。 1.  主节点上运行着一个名为"Nimbus"的守护进程。 Nimbus    负责为集群分发代码，为工作节点分配任务以及进行故障监控。一个 Storm    集群在工作过程中，只有一个 Nimbus    进程工作。        2.  每个工作节点上都运行着一个名为"Supervisor"的守护进程。 Supervisor    负责监听分配给它所在的机器上的工作，负责接收 Nimbus    分配的任务，并根据需要启动和停止工作进程，其中每个工作进程都执行一个子任务。因此，一个正在运行的拓扑任务，是由分布在许多计算机上的许多工作进程组成。        ![](Images/795cb0593fc0ad54087df3c1b3ca205e.png)savepage-src="https://static001.geekbang.org/resource/image/dc/e9/dc3647474c58d12510582404087ceee9.png"}前面我介绍了 Nimbus 是负责分发任务或代码的，Supervisor是负责接收任务，并启动和停止工作进程以执行任务的。那么 Nimbus 和Supervisors之间，具体是怎么协同的呢？下面我们一起看一下。如果所有数据和信息均存储在 Master Node 上，Master Node故障后，会导致整个集群信息丢失，因此引入了 ZooKeeper集群来加强可靠性。为此 Master Node 与 Worker Node 之间的交互通过ZooKeeper 完成，由于 Nimbus 和 Supervisors 是 Master Node 和 Worker Node之间负责交互的进程，因此 Nimbus 和 Supervisors 之间的所有协调都是通过ZooKeeper 集群完成的，比如 Nimbus 会将任务的分配情况或信息发送给ZooKeeper 集群，然后 Supervisors 向 ZooKeeper集群获取任务，并启动工作进程以执行任务。当 Supervisor 接收到分配的任务后，会启动工作节点的工作进程 (Worker)去执行任务。我们知道，一个计算任务可以分成任务数据的读取以及任务执行两部分。Worker提供了两个组件 Spout 和Bolt，分别进行数据读取和任务执行。在详细介绍 Worker 组件之前，我首先介绍一下 Storm的核心抽象：数据流。数据流是一个无界序列，是在分布式环境中并行创建、处理的一组元组（tuple）。数据流可以由一种能够表述数据流中元组的域（fields）的模式来定义。Storm 为进行数据流转换提供了基本组件 Spout 和 Bolt。 Spout 和 Bolt有用户自定义的接口，用于运行特定应用程序的逻辑。如下图所示，Storm上运行的计算拓扑其实是由一系列 Spout 和 Bolt组成的有向无环图，这个有向无环图代表了计算逻辑。![](Images/0256db83fc0fd3e59c2bab27940265cb.png)savepage-src="https://static001.geekbang.org/resource/image/7a/c8/7ae9455e1d125488d9b94283eecb2ac8.png"}备注： 1.       图中箭头，表示数据元组的传递方向。        2.       此图引自"        [Twitter Analysis with Apache    Storm         slate-object="inline"    "。        接下来，我们看看**Spout 和 Bolt的含义**吧。1.  Spout 用于接收源数据。通常情况下，Spout    会从一个外部的数据源读取数据元组，然后将它们发送到拓扑中。例如，Spout    从 Twitter API    读取推文并将其发布到拓扑中。        2.  Bolt    负责处理输入的数据流，比如数据过滤（filtering）、函数处理（functions）、聚合（aggregations）、联结（joins）、数据库交互等。数据处理后可能输出新的流作为下一个    Bolt 的输入。每个 Bolt    往往只具备单一的计算逻辑。当我们执行简单的数据流转换时，比如仅进行数据过滤，则通常一个    Bolt 可以实现；而复杂的数据流转换通常需要使用多个 Bolt    并通过多个步骤完成，比如在神经网络中，对原始数据进行特征转换，需要经过数据过滤、清洗、聚类、正则化等操作。        知识扩展：流计算和批量计算的区别是什么？MapReduce可以说是一种批量计算，与我们今天介绍的用于实时数据处理的流计算，是什么关系呢？虽然流计算和批量计算属于两种不同的计算模式，但并不是非此即彼的关系，只是适用于不同的计算场景。在流计算中，数据具有时效性，因此在 5G以及人工智能应用的驱动下，专注于实时处理的流计算越来越得到广泛的关注。流计算的低延时、易扩展等性能非常适用于对时延要求高的终端应用（比如直播中音视频的处理等），从而极大提高用户的服务体验。而批量计算适用于对时延要求低的任务。在实际运用中，可以根据计算要求，选择不同的计算模式。我将这两种计算模式的特点，总结为了一张表格，以帮助你理解、记忆，以及选择适合自己业务场景的计算模式。![](Images/9a21bcc86e918b73811c81043ab5730c.png)savepage-src="https://static001.geekbang.org/resource/image/53/2c/53f87f8de7f10562db5ce74b748bce2c.jpg"}总结今天，我与你介绍了分布式计算模式中的流计算。流数据的价值会随时间的流逝而降低，"时间就是金钱"在流计算中体现得淋漓尽致。这就要求流计算框架必须是低延迟、可扩展、高可靠的。在介绍流计算的工作原理时，我首先通过一个流程图，与你介绍了它的 3个步骤，即提交流式计算作业、加载流式数据进行流计算和持续输出计算结果。然后，我以流计算开源框架中的 Storm 为例，与你讲述了 Storm的核心组件以及通过 Spout 和 Bolt构建有向无环图代表流计算逻辑，以实现流计算，以加深你对流计算原理的理解。最后，我再通过一张思维导图来归纳一下今天的核心知识点吧。![](Images/5d49edb0551946af7ed4a4e6cc20a615.png)savepage-src="https://static001.geekbang.org/resource/image/fb/5e/fbb7a3acd35fd34bdb7727ce3a0caf5e.png"}思考题离线计算和批量计算，实时计算和流式计算是等价的吗？你能和我说说你做出判断的原因吗？我是聂鹏程，感谢你的收听，欢迎你在评论区给我留言分享你的观点，也欢迎你把这篇文章分享给更多的朋友一起阅读。我们下期再会！![](Images/c191f391e2aab7575517a886bbd7a681.png)savepage-src="https://static001.geekbang.org/resource/image/a4/8c/a42a16601611a1a72599ecfca434508c.jpg"}