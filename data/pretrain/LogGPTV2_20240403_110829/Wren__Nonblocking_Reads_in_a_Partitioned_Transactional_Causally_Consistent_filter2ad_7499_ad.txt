 6
 4
 2
p
s
e
R
 5
 0
 5
 10
 15
 20
 25
 30
Throughput (1000 x TX/s)
 0
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
Throughput (1000 x TX/s)
(a) Throughput vs average TX latency.
(b) Mean blocking time in Cure and H-Cure. Wren never blocks.
Fig. 3: Performance of Wren, H-Cure and Cure on 3 DCs, 8 partitions/DC, 4 partitions involved per transaction, and 95:5
r:w ratio. Wren achieves better latencies because it never blocks reads (a). H-Cure achieves performance in-between Cure and
Wren, showing that only using HLCs does not solve the problem of blocking reads in TCC. Cure and H-Cure incur a mean
blocking time that grows with the load (b). Because of blocking, Cure and H-Cure need higher concurrency to fully utilize
the resources on the servers. This leads to higher contention on physical resources and to a lower throughput (a).
replicas, ensuring the progress of the RST.
BiST. Periodically, partitions within a DC exchange their
version vectors. The LST is computed as the minimum across
the local entries in such vectors; the RST as minimum across
the remote ones (Alg. 4 Lines 30–32). Partitions within a DC
are organized as a tree to reduce communication costs [20].
Garbage collection. Periodically, the partitions within a DC
exchange the oldest snapshot corresponding to an active trans-
action (pm
n sends its current visible snapshot if it has is no
running transaction). The aggregate minimum determines the
oldest snapshot Sold that is visible to a running transaction.
The partitions scan the version chain of each key backwards
and keep the all the versions up to (and including) the oldest
one within Sold. Earlier versions are removed.
C. Correctness
Because of space constraints, we provide only a high-level
argument to show the correctness of Wren.
Snapshots are causal. To start a transaction, a client c
piggybacks the freshest snapshot it has seen, ensuring the
monotonicity of the snapshot seen by c (Alg. 2 Lines 1–6).
Commit timestamps reﬂect causality (Alg. 2 Line 19), and
BiST tracks a lower bound on the snapshot installed by every
partition in a DC. If X is within the snapshot of a transaction,
so are its dependencies, because i) dependencies generated
in the same DC where X is created have a timestamp lower
than X and ii) dependencies generated in a remote DC have a
timestamp lower than X.rdt. On top of the snapshot provided
by the coordinator, the client applies its writes that are not in
the snapshot. These writes cannot depend on items created by
other clients that are outside the snapshot visible to c.
Writes are atomic. Items written by a transaction have the
same commit timestamp and RST. LST and RST are computed
as the minimum values across all the partitions within a DC.
If a transaction has written X and Y and a snapshot contains
X, then it also contains Y (and vice-versa).
V. EVALUATION
We evaluate the performance of Wren in terms of through-
put, latency and update visibility. We compare Wren with
Cure [8],
the state-of-the-art approach to TCC, and with
H-Cure, a variant of Cure that uses HLCs. By comparing
with H-Cure, we show that using HLCs alone, as in existing
systems [29], [33],
to achieve the same
performance as Wren, and that nonblocking reads in the
presence of multi-item atomic writes are essential.
is not sufﬁcient
A. Experimental environment
Platform. We consider a geo-replicated setting deployed
across up to 5 replication sites on Amazon EC2 (Virginia,
Oregon, Ireland, Mumbai and Sydney). When using 3 DCs,
we use Virginia, Oregon and Ireland. In each DC we use up
to 16 servers (m4.large instances with 2 VCPUs and 8 GB
of RAM). We spawn one client process per partition in each
DC. Clients issue requests in a closed loop, and are collocated
with the server partition they use as coordinator. We spawn
different number of client threads to generate different load
conditions. In particular, we spawn 1, 2, 4, 8, 16 threads per
client process. Each “dot” in the curve plots corresponds to a
different number of threads per client.
Implementation. We implement Wren, H-Cure and Cure in
the same C++ code-base 4. All protocols implement the last-
writer-wins rule for convergence. We use Google Protobufs
for communication, and NTP to synchronize physical clocks.
The stabilization protocols run every 5 milliseconds.
Workloads. We use workloads with 95:5, 90:10 and 50:50 r:w
ratios. These are standard workloads also used to benchmark
other TCC systems [8], [16], [34]. In particular, the 50:50
and 95:5 r:w ratio workloads correspond, respectively, to the
update-heavy (A) and read-heavy (B) YCSB workloads [35].
Transactions generate the three workloads by executing 19
reads and 1 write (95:5), 18 reads and 2 writes (90:10), and
10 reads and 10 writes (50:50). A transaction ﬁrst executes all
reads in parallel, and then all writes in parallel.
Our default workload uses the 95:5 r:w ratio and runs
transactions that involve 4 partitions on a platform deployed
over 3 DCs and 8 partitions. We also consider variations of
this workload in which we change the value of one parameter
4https://github.com/epﬂ-labos/wren
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:26:55 UTC from IEEE Xplore.  Restrictions apply. 
8
 30
 25
 20
 15
 10
 5
)
c
e
s
m
(
e
m
i
t
.
p
s
e
R
Cure 
H-Cure 
Wren 
 30
 25
 20
 15
 10
 5
)
c
e
s
m
(
e
m
i
t
.
p
s
e
R
 0
 5
 10
 25
Throughput (1000 x TX/s)
 15
 20
 30
 35
 0
 5
 10
 25
Throughput (1000 x TX/s)
 15
 20
 30
 35
(a) Throughput vs average TX latency (90:10 r:w).
(b) Throughput vs average TX latency (50:50 r:w).
Fig. 4: Performance of Wren, Cure and H-Cure with different 90:10 (a) and 50:50 (b) r:w ratios, 4 partitions involved per
transaction (3DCs, 8 partitions). Wren outperforms Cure and H-Cure for both read-heavy and write-heavy workloads.
20
)
c
e
s
m
 15
(
 10
e
m
i
t
.
 5
p
s
e
R
 0
Cure 
H-Cure 
Wren 
 20
)
c
e
s
m
(
e
m
i
t
.
 15
 10
 10
Throughput (1000 x TX/s)
 20
 30
 40
 50
 5
p
s
e
R
 0
 10
 20
 30
Throughput (1000 x TX/s)
 40
 50
(a) Throughput vs average TX latency (p=2).
(b) Throughput vs average TX latency (p=8).
Fig. 5: Performance of Wren, Cure and H-Cure with transactions that read from 2 (a) and 8 (b) partitions with 95:5 r:w ratio
(3DCs, 8 partitions). Wren outperforms Cure and H-Cure with both small and large transactions.
and keep the others at their default values. Transactions access
keys within a partition according to a zipﬁan distribution,
with parameter 0.99, which is the default
in YCSB and
resembles the strong skew that characterizes many production
systems [26], [36], [37]. We use small items (8 bytes), which
are prevalent in many production workloads [26], [36]. With
bigger items Wren would retain the beneﬁts of its nonblocking
reads. The effectiveness of BDT and BiST would naturally
decrease as the size of the items increases, because meta-data
overhead would become less critical.
B. Performance evaluation
Latency and throughput. Figure 3a reports the average
transaction latency vs. throughput achieved by Wren, H-Cure
and Cure with the default workload. Wren achieves up to
2.33x lower response times than Cure, because it never blocks
a read due to clock skew or to wait for a snapshot to be
installed. Wren also achieves up to 25% higher throughput
than Cure. Cure needs a higher number of concurrent clients
to fully utilize the processing power left idle by blocked reads.
The presence of more threads creates more contention on the
physical resources and implies more synchronization to block
and unblock reads, which ultimately leads to lower throughput.
Wren also outperforms H-Cure, achieving up to 40% lower
latency and up to 15% higher throughput. HLCs enable H-
Cure to avoid blocking the read of a transaction T because
of clock skew. This blocking happens on a partition if the
local timestamp of T ’s snapshot is t, there are no pending or
committed transactions on the partition with commit times-
tamp lower than t, but the physical clock on the partition is
lower than t. HLCs, however, cannot avoid blocking T if there
are pending transactions on the partition, and T is assigned a
snapshot that has not been installed on the partition.
Statistics on blocking in Cure and H-Cure. Figure 3b pro-
vides insights on the blocking occurring in Cure and H-Cure,
that leads to the aforementioned performance differences. The
plots show the mean blocking time of transactions that block
upon reading. A transaction T is considered as blocked if at
least one of its individual reads blocks. The blocking time
of T is computed as the maximum blocking time of a read
belonging to T .
Blocking can take up a vast portion of a transaction exe-
cution time. In Cure, blocking reads introduce a delay of 2
milliseconds at low load, and almost 4 milliseconds at high
load (without considering overload conditions). These values
correspond to 35-48% of the total mean transaction execution
time. Similar considerations hold for H-Cure. The blocking
time increases with the load, because higher load leads to
more transactions being inserted in the pending and commit
queues, and to higher latency between the time a transaction
is committed and the corresponding snapshot is installed.
C. Varying the workload
Figure 4a and Figure 4b report
the average transaction
latency as a function of the load for the 90:10 and 50:50
r:w ratios, respectively. Figure 5a and Figure 5b report the
same metric with the default r:w ratio of 95:5, but with p = 2
and p = 8 partitions involved in a transaction, respectively.
These ﬁgures show that Wren delivers better performance than
Cure and H-Cure for a wide range of workloads. It achieves
transaction latencies up to 3.6x lower then Cure, and up to
1.6x lower than H-Cure. It achieves maximum throughput up
to 1.33x higher than Cure and 1.23x higher than H-Cure. The
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:26:55 UTC from IEEE Xplore.  Restrictions apply. 
9
 1.5
e
r
u
C
 1.4
t
u
p
h
g
u
o
r
h
T
t
.
r
.
w
d
e
z
i
l
a
m
r
o
n
Wren-4P
Wren-8P
61
32
73
39
16
19
Wren-16P
46
24
12
t
u
p
h
g
u
o
r
h
T
 1.5
 1.4
 1.3
 1.2
 1.1
e
r
u
C
t
.
r
.
w
d
e
z
i
l
a
m
r
o
n
Wren-3DC
112
61
73
Wren-5DC
91
46
66