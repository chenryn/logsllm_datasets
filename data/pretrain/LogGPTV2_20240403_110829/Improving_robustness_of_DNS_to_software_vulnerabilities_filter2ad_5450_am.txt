8782 hosts; 210K NXD
average 262K hosts; 1.8M NXD
X
-
-
X
X
-
X
X
X
Then the classiﬁcation module computes domain name character distributions of each clus-
ter into a numerical feature vector, which is used to classify known DGA families. A new
unknown DGA family with features statistically similar to a known one can be detected by
this process. The system operates on daily NXDOMAIN trafﬁc generated by all hosts in a
network using the following data sources.
Datasets
We use anonymized recursive DNS trafﬁc from a large telecommunication company from
December 18, 2016 to December 29, 2016. The dataset contains NXDOMAINs queried by
hosts and the query timestamps. On average, there are 262 thousand unique anonymized
hosts, with 44.6 million queries to 1.8 million unique NXDOMAINs in a day. We use
this dataset to construct Host-NXDOMAIN Graph as ground truth without attack. This is
89
HostsNXDOMAINsBipartite GraphSVDClustersXMeansOutput: New DGA FamiliesClassifyEmbeddings[...][...][...][...]UVCommunity Discoverynode2vec(1)(2)(3)Graph Clusteringavailable to defenders and perfect knowledge attackers.
As a surrogate network dataset, we use NXDOMAIN trafﬁc from a large US univer-
sity network collected on December 25, 2016. It contains 8,782 hosts and 210 thousand
unique NXDOMAINs. Among these NXDOMAINs, only 227 appeared in the ground truth
network dataset. The surrogate dataset is available to attackers with moderate and perfect
knowledge.
Last but not least, we use a reverse engineered DGA domains dataset to train the su-
pervised component of the system. We run the reverse-engineered algorithms [127] to
generate DGA domains for 14 malware families: Chinad, Corebot, Gozi, Locky, Murofet,
Necurs, NewGOZ, PadCrypt ransomware, Pykspa, Qadars, Qakbot, Ranbyus, Sisron, and
Symmi. The training dataset also includes live DGA domains observed in the ground truth
network. We label 267 clusters belonging to four malware families present in the ground
truth network dataset (Pykspa, Suppobox, Murofet, and Gimemo), and manually verify that
these subgraphs are attack free. We train a Random Forest classiﬁer with an average accu-
racy of 96.08%, and a false positive rate of 0.9%. The classiﬁer trained from this dataset is
available for attackers of all knowledge levels. Table 5.1 summarizes these datasets.
We discovered 12 new DGA malware families in only 12 days using the ground truth
network trafﬁc (see Appendix A.3 for details). We discovered real but unsuccessful evasion
attempts in the wild, and retrained our classiﬁer with evasive instances. We believe we have
faithfully reimplemented Pleiades because we use comparable datasets and we achieve
similar clustering and modeling results.
5.4.2 Attacks
Using the notation described in Section 5.3, let G be a bipartite graph of the defender. U
represents hosts, both infected and uninfected, and V represent NXDOMAINs queried by
hosts in the underlying network. An edge exists from vi ∈ U and vj ∈ V iff the ith host
queried the jth NXDOMAIN. For an attacker graph G ⊂ G, the hosts in U are infected
90
hosts under the control of the attacker. In the noise injection case, the attacker instructs their
malware to query NXDOMAINs beyond what is needed for normal operation, as shown in
Figure 5.1. In the small community case, the attacker coordinates the querying behavior of
their malware such that they query fewer NXDOMAINs in common, as in Figure 5.2. We
will evaluate the effectiveness of the attacks by the drop in predicted class probabilities and
the predicted label of the classiﬁer. In a Random Forest, the predicted class probabilities of
a feature vector are calculated as the average predicted class probabilities of all trees in the
forest. In practice, if the predicted class probability decreases substantially, the classiﬁer
will incorrectly label the instances, and the attack will be considered successful.
5.4.3 Attack Costs
To compute the anomaly cost for noise injection, we analyze percentile changes of edges
related to hosts in U in the structure of G from before and after the attack. We quantify
this change by computing the cumulative distribution functions (CDFs, example in Ap-
pendix A.1) of vertex degrees before and after a successful attack is mounted. Concretely,
if an attacker can evade Pleiades but raises the proﬁle of their infected hosts from the 50th
(in the CDF before attack) to the 99.9th percentile of NXDOMAINs queried per host (in the
CDF after attack), a defender will be able to detect such behavior with simple thresholding
(i.e., monitoring hosts entering the 95th percentile).
To quantify the adversarial cost behind the small community attack, we measure the
change of attacker graph density D(G(cid:48)) as deﬁned in Section 5.5.3. If the attacker graph
density decreases, this means the attacker no longer uses NXDOMAINs for their infection
and/or the infected hosts query fewer NXDOMAINs in common, reducing their connectiv-
ity overall and increasing the botnet’s management cost.
91
5.5 Results
First, we show how to select hyperparameters for each of the three graph methods. Next,
we present our results for both attacks against each graph based clustering technique, for
the three knowledge levels. Finally, we explain the costs incurred by the attacker, and how
these can be used to identify possible defenses.
Summary of Results Our attacks against the graph clustering component of Pleiades
gravely reduce the predicted class probability of the subsequent classiﬁcation phase. Even
with minimal knowledge, an adversary can launch an effective targeted noise injection at-
tack dropping the median predicted class probability to 0%. In the higher knowledge levels,
the maximum predicted class probability can be as low as 10%. Using a set of labeled DGA
malware families observed in spectral clustering, the attacks reduce the prediction accuracy
from 99.6% to 0%.
In addition to being effective, the attacks do not substantially raise the anomaly pro-
ﬁle of infected hosts: before and after the targeted noise injection attacks the hosts occupy
a similar percentile for the number of NXDOMAINs queried. Small community attack
results show that the traditional way of choosing hyperparameters for generating graph
embeddings is insufﬁcient when we analyze the system in an adversarial setting, because it
creates a large area for possible small community attack instances. While following the ac-
cepted methodology for selecting the rank for SVD and hyperparameters for node2vec, all
DGA clusters can be hidden in noisy clusters by subdividing the infected hosts into smaller
groups to sacriﬁce some agility, even while using hundreds of DGA domains. Even in the
minimal knowledge case where the small community attack cannot be tested, attackers can
sometimes still hide.
92
Figure 5.4: Scree plot of eigenvalues of SVD.
5.5.1 Choosing Hyperparamters
First, we carefully choose hyperparameters for the graph clustering methods in Pleiades to
ensure high quality clusters are generated.
Spectral Clustering
We use the scree plot shown in Figure 5.4 to choose the rank of SVD. We ﬁx the rank
of the SVD to be 35, where the scree plot plateaus. While different than the 15 used in
the original Pleiades implementation [25], the underlying datasets are different so it is not
unreasonable to ﬁnd different ranks.
Community Discovery
We use the best partition method from the NetworkX community discovery library [128]
which implements the Louvain algorithm [33]. The Louvain algorithm efﬁciently extracts
good communities on the graph by optimizing the modularity metric, which measures the
density of links within communities in comparison to outside them. It ﬁrst sets each node to
be its own community, and iteratively merges them to maximize modularity. The algorithm
stops when a local maxima is reached. This community detection algorithm scales to large
network with hundreds of millions of nodes.
93
llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll102030400355075100Factor NumberEigenvalueFigure 5.5: Using cluster validity metrics to choose walk length.
node2vec
We use traditional cluster validity metrics to compare different hyperparameters of node2vec.
Twelve DGA malware families, including both known and newly detected ones, were used
as reference clusters. We use validity metrics including Adjusted Rand Index, Complete-
ness, Fowlkes-Mallows index, Homogeneity, Normalized Mutual Information (NMI), pu-
rity, and V-Measure score. We ﬁrst choose context size six, which has the ﬁrst highest va-
lidity scores. Several larger context sizes generate equal validity scores, but they produce
noisier clusters. This is because that larger context sizes in DNS graphs tend to include
more noisy nodes, such as popular benign NXDOMAINs, or popular hosts that are likely
proxies.
Then we choose the walk length according to Figure 5.5. Multiple walk length values
produce high validity scores, but we choose walk length 20, which corresponds to the
second highest peak. Because using walk length 20 generates cleaner Murofet clusters
than a walk length smaller than 10, due to the fact that longer walk length provides more
samples of neighborhoods and the model is learned better. The number of walks per node,
dimensions, and SGD epoch does not show much difference. We decide on 15 walks, 60
dimensions, and one learning epoch after manual inspection. Lastly, we use a uniform
94
lllllllllllllllllll0.250.500.751.00220406080100120150Walk LengthValuelAdjusted Rand IndexCompletenessFowlkes−Mallows IndexHomogeneityNMIV−Meausurerandom probability to choose the next node in the random walk process.
5.5.2 Targeted Noise Injection
We run our version of Pleiades to generate all attacker graphs. Four DGA families were
identiﬁed: Pykspa, Suppobox, Murofet, and Gimemo. For each we extract the attacker
graphs (G) and the target domains (V ). These domains are labeled using the classiﬁer from
Section 5.4.1. Before and after the attack, there can be multiple clusters formed within G
and G(cid:48), depending on the graph clustering technique. We use the classiﬁer model to test
how likely it is that each cluster belongs to the true DGA malware family, both before and
after the attack. We present the overall distribution of the predicted class probabilities to
show the impact of the attacks.
We use different types of noisy domains at different knowledge levels. For a DGA,
these nodes are new NXDOMAINs (V (cid:48)) that will be classiﬁed as benign, also queried by
the infected hosts (U). In the minimal knowledge case, we create a DGA algorithm that
is classiﬁed as benign. It is a dictionary DGA that uses the most popular English words
from movie subtitles [129], popular web terms, and the top one million Alexa domains.
We randomly add numbers and dashes, and randomly select a top-level domain from four
choices. In addition, we generate some punycode domains that start with the character
sequence “xn–”, and some domains with a “www” subdomain. We generate 59,730 veriﬁed
NXDOMAINs. In the perfect and moderate knowledge cases, the adversary uses existing,
unpopular NXDOMAINs from G and the surrogate dataset, respectively.
Spectral Clustering
Figure 5.6a shows the classiﬁer’s predicted true class probabilities from before the attack is
mounted, and after the minimal, moderate, and perfect knowledge targeted noise injection
attacks are performed. For each knowledge level, we inject two different levels of noise as
described in Section 5.5.2 and re-run the clustering and subsequent classiﬁcation to assess
95
the damage from the targeted noise injection. Recall that we try two attack variants, attack
variant 1 and 2, where we inject one or two mirrored sets of vertices and edges, respectively.
This is to both i) understand how much noise is needed to yield successful evasion, and ii)
determine the cost incurred by adding noise.
Spectral clustering generates 267 DGA clusters from the four malware families across
12 days. Before the attack, only 0.4% clusters (1 out of 267) are predicted with the wrong
labels. In comparison, after the attacks, all clusters are predicted with the wrong labels.
Next, we will examine the predicted class probabilities change in the true class label.
Figure 5.6a uses the violin plots to show the distribution of predicted class probabilities
for the true DGA families, before and after the attacks. The circle is the median value,
the thick line within the violin indicates interquartile range, and the thin line depicts a
95% conﬁdence interval. The shape of the violin shows the distribution of the probability
values. For example, the ﬁrst violin in Figure 5.6a has a median of 100% predicted class
probabilities, and all data points in the interquartile range have 100% probability value.
Speciﬁcally, before the attacks, 238 clusters are predicted with 100% class probability that
they belong to the true class, and only 28 clusters have a probability between 60% and
100%. For example, the Pykspa cluster had a class probability of only 10% because it
contained only two domain names that had very different feature distributions from the
majority of Pykspa clusters. The two variants of the attack introduced at least 50% and
66% noise to the DGA clusters.
Minimal Knowledge After the attacks, we classify each new adversarial cluster contain-
ing target domains and plot the target class probability distributions in the Figure 5.6a.
Attack variant 1 (“Minimal Benign DGA 1”) generated new clusters with ≤ 80% predicted
class probability, with a median of 0%. The predicted class probabilities of 84% of the new
clusters drop to zero. Attack variant 2 further decreases the classiﬁer prediction conﬁdence,
as shown by “Minimal Benign DGA 2” in Figure 5.6a. After injecting two benign DGA
96
(a) Spectral Clustering: Predicted class probabilities.
(b) Community Discovery and node2vec:
Predicted class probabilities.
(c) Retraining: Predicted class probabilities.
Figure 5.6: Figure 5.6a: Predicted class probabilities before the targeted noise injection
attack and after two variants of the targeted noise injection attack in minimal, moderate, and
perfect knowledge. Figure 5.6b: Predicted class probabilities before and after the targeted
noise injection attacks for community discovery and node2vec. Figure 5.6c: Predicted
class probabilities under different attacks after retraining including the “Minimal Benign
DGA 1” clusters.
domains, the predicted class probabilities of 87% of the new clusters plummet to 0%. The
overall distribution of prediction conﬁdences also shifts downward compared to “Minimal
Benign DGA 1”.
Perfect Knowledge The median of predicted class probabilities for DGA malware fami-
lies drops to 10%. As depicted by “Perfect Long Tail 1” in Figure 5.6a, 86% of adversarial
clusters were assigned the probabilities of belonging to the true DGA class that are at most
97
0.00.40.8lllllllSpectral: Before AttackSpectral: Minimal Benign DGA 1Spectral: Minimal Benign DGA 2Spectral: Moderate 1Spectral: Moderate 2Spectral: Perfect Long Tail 1Spectral: Perfect Long Tail 2Predicted Class Probability0.00.40.8llllCommunity Before AttackCommunity After AttackN2V Before AttackN2V After AttackPredicted Class Probability0.00.40.8lllllComm: Minimal Benign DGA 1N2V: Minimal Benign DGA 1Spectral: Minimal Benign DGA 2Spectral: Moderate 2Spectral: Perfect Long Tail 2Predicted Class Probability10% . The distribution of class probability values has a smaller variance compared to those
in the “Minimal Benign DGA 1”. “Perfect Long Tail 2” in Figure 5.6a shows that the
maximum prediction conﬁdence is 30%, slightly lower than the maximum 40% conﬁdence
from the targeted noise injection attack of “Minimal Benign DGA 2”.
Moderate Knowledge We see similar results for the two targeted noise injection attack
variants in the moderate knowledge case as in the other cases: a strong drop in predicted
class probabilities, with a smaller, more compact distribution of values for attack variant 2.
After attack variant 1, 98.3% of new clusters were assigned less than 20% conﬁdence; after
attack variant 2, 98.8% of new clusters have less than 20% conﬁdence.
Spectral clustering can be largely defeated at all knowledge levels using the targeted
noise injection attacks.
Since previous experiments show that minimal knowledge attackers can carry out tar-
geted noise injection as effectively as more powerful attackers, we will simply demonstrate
that the same targeted noise injection attack variant 1 in minimal knowledge also works
with community discovery and node2vec.
Community Discovery
We use the same set of DGA domains labeled in Spectral Clustering for evaluation. Before
the attack, 80% clusters can be predicted with the correct label, which dropped to 2% after
the attack. Figure 5.6b shows the predicted class probabilities for communities containing
all target domains before and after the attack. Before the attack, the median of predicted
probabilities is 90%, and the interquartile range is from 50% to 100%. Speciﬁcally, 71
communities contain target domains, among which ten communities only contain one tar-
get domain, and seven communities have between 40% to 70% target domains. These
noisy communities formed the lower part of the distribution, with ≤ 50% predicted class
probabilities in “Community Before Attack”, as shown in Figure 5.6b. After the attack, the
98
Table 5.2: Anomaly cost as percentile of the distinct number of NXDOMAINs queried by
hosts, before and after the attack. Only 9.12% of infected hosts become more suspicious,
while the rest remain the same.
Before Attack
Average Increase From Percentile
69.86%
Attack Variant 1
Attack Variant 2
69.86%
Before Attack
Average Increase From Percentile
99.74%
Attack Variant 1
Attack Variant 2
99.74%
< 95th Percentile, 9.12% of hosts