the sensitivity of the query). We use private sum computation
which mirrors private count in Section V-B1 except f is a
sum and s is sampled from Laplace II [3]. The neighboring
datasets D and D(cid:48) that our attack tries to distinguish differ
on a single record whose credit is 5000 in D and 0 in D(cid:48).
We note that the query is different from that in Section V-B1
since the timing attack is not as effective for queries with small
sensitivity, while the FP attack is effective against queries that
return values close to 0.
To conduct the attack, we ﬁrst collect timing data of private
sum for different noise magnitudes. Similar to experiments
in the previous subsection, we observe a linear relationship
between noise magnitude and time cost albeit now this time
includes computing the sum and sampling (Figure 8 in Ap-
pendix).
Our attack proceeds by measuring the time t of a DP
algorithm to complete. The attacker then uses t and the
output y it receives to determine if it was D or D(cid:48) used
in the computation. Note that the noise magnitude should be
D x| for D and s(cid:48) = |y −(cid:80)
s = |y −(cid:80)
D(cid:48) x| for D(cid:48).
The attacker makes a guess on the magnitude of the noise
using the timing data it has collected above and Equation 3.
Let sg be its guess. It then compares sg with s and s(cid:48) and
chooses the closest one as its guess. That is, it chooses D if
|s − sg| < |s(cid:48) − sg| and D(cid:48) otherwise.
We plot the attack results in Figure 5 for  in the range
[1, 10]. We observe that attack success rate increases with
higher , with success rate of 69.15% for  = 1. Our intuition
for the above trend is due to the range of noise in which the
attacker needs to make its guess. That is, for smaller noise
scale (i.e., high ), the output noise range is small as well and
hence attacker has less number of noises to assign observed
time to. For example, with ∆ = 5000, noise magnitudes are
mainly distributed in [0, 15000] when  = 1, while noise
magnitudes are mainly distributed in [0, 2500] when  = 10.
Fig. 5: Attack on private sum of the credit attribute of the
German Credit Dataset [33], with Laplace II [3]. The attack
success rate under different privacy budget  ∈ [1, 10] and
sensitivity ∆ = 5000 (measured over 1 million trials for
each ). Success means the private sum created from D is
successfully concluded to be in support of D and not D(cid:48).
The timing attack has two limitations. First, it assumes
that the time (and its variance) to compute f is not much
larger than that of sampling, as otherwise the microseconds
difference may not be observable. Second, the attack works
for one-dimensional functions f as the attacker can measure
the time of a single noise sample. Hence, the attack will not
be as successful if the attacker were to observe the time it
takes to draw multiple samples, as is the case for DP-SGD.
VIII. MITIGATION STRATEGIES
We discuss mitigation strategies for both of our attacks.
A. Defenses Against Floating-Point Attacks
Mironov [6] proposed the snapping mechanism to alleviate
the FP attack by carefully truncating and rounding an output of
a DP mechanism that was implemented using ﬂoating points.
However, the privacy and utility of the overall mechanism
decreases [13], [12].
Our attack against the Box-Muller and polar methods as-
sumes that the attacker observes two of their samples (recall
that the Ziggurat method generates only one sample). That
is, the attacker gets access to the second (cached) value (e.g.,
when a query returns an answer to a d-dimensional query such
as a histogram or ML model parameters). Without the second
value in Equation (2) the attacker has to resort to checking all
possible values in a brute-force manner. A potential mitigation
is therefore to generate new samples on each call and disregard
the second value.
We also observe that the implementation of DP-SGD adds
noise to the batch directly. Instead it could potentially add
several samples of noise and then average the result, since
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:38:32 UTC from IEEE Xplore.  Restrictions apply. 
483
approaches to do so include compiler-based code transforma-
tions [41], [42] while other generic defense strategies are based
on padding either by padding execution time to a constant time
or adding a random delay [43].
In our setting, one could choose a sufﬁciently large time
threshold and run noise generation mechanism until
then,
independent of the noise being drawn and the time that takes
to produce it. If an attacker is stronger than that considered
in this paper and can perform microarchitectural observations,
then execution of any padded code has to be made secret-
independent. For example, if the attacker can measure the time
of memory accesses, the counter of the number of trials needs
to be accessed regardless of whether heads or tails was drawn
in a Bernoulli trial. The downside of padding is efﬁciency as
all execution would take maximum time. The failure to draw
noise within the threshold time can then be accounted for in δ,
the failure probability of DP [12]. An alternative is to use a
truncated version of the geometric distribution [7] in order to
avoid values that are impossible to sample on a ﬁnite computer.
We evaluate padding as a mitigation against our attack
on private sum in Section VII-C. Padding can be imple-
mented via two approaches. First, after a successful trial is
encountered, we record its trial number and continue drawing
“blank” Bernoulli samples, till the maximum noise threshold
is reached. In the second approach, once a successful trial is
encountered, a time delay can be added to reach some max-
imum time threshold. We choose the maximum threshold to
be 41µ since we observed that 99.5% of noise magnitudes are
distributed in [0, 2677], with average and maximum execution
time of 37.1µ and 40.3µ, respectively, when  = 10 and
∆ = 5000. We note that these estimates are not dependent
on the data but only on the sampler and the parameters of
the noise. Padding until a time threshold increases the total
execution by 10.5%. For medium number of queries this is an
acceptable overhead given the small magnitude of the overall
time (in µs). Note that these estimates will be different for
other implementations.
As another mitigation strategy we propose a technique based
on batching and caching. The method generates k random
values ofﬂine and saves them. It returns one value for each
call to the distribution function. It proceeds this way until
all cached values are used and then restarts the process by
generating the next k samples. Samples could be generated
online and shufﬂed to disconnect noise from their timing, as
suggested in [44]. However, the attacker may still measure the
range of possibles times.
In summary, discrete distribution sampling implemented
time or where the timing of sampling is not
in constant
observable (i.e., generated ofﬂine) appears to be the best
approach for defending against attacks discussed in this paper.
IX. RELATED WORK
Fig. 6: Accuracy of the ML model on MNIST dataset, trained
with DP-SGD with continuous and discrete Gaussian samplers
with discretization parameters γ ∈ {10−1, 10−2, 10−3} for  ∈
(0.1, 10].
an average of Gaussian noise is still Gaussian. This would
make it harder for an adversary to extract sample-level values
needed for the FP attack as described in Section IV. However,
this heuristic may still be susceptible to attacks as it also uses
ﬂoating-point representation.
Discrete distributions [12], [13], [39] have been proposed
as a mitigation against ﬂoating-point attacks since they avoid
ﬂoating points or bound their effect on privacy in (, δ) pa-
rameters. However, as we showed in the previous section, they
may suffer from other side-channel attacks and thus should
also be carefully implemented. Nevertheless in this section
we evaluate them as a mitigation for attacks in Section V-C
and measure their effect on accuracy of DP-SGD.
We implement DP-SGD with discrete Gaussian by discretiz-
ing the gradient g ∈ Rd as described in [40] who use discrete
Gaussian for training models in a Federated Learning setting.
Appendix C provides further details.
We use the same model and MNIST dataset as in Sec-
tion V-C to evaluate the performance of models trained
with discrete Gaussian for a discretization parameter γ ∈
{10−1, 10−2, 10−3} and privacy budget  ∈ (0.1, 10]. We use
the implementation of discrete Gaussian I [14].
We compare accuracy of the models with discrete and
continuous implementations in Figure 6. We observe that the
performance of models trained with discrete Gaussian matches
the performance of models trained with continuous Gaussian,
given small enough γ, such as 10−3. Smaller γ allows the
discretization to be done on a ﬁner grid γZ, thus gradient
calculation is not affected by rounding.
B. Defenses Against Timing Attacks
The most effective mitigation against
timing attacks is
to ensure constant time execution. Application-independent
The implementation of differential privacy via Laplace
mechanism has been demonstrated to be ﬂawed, due to ﬁnite-
precision representations of ﬂoating-point values [6]. In this
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:38:32 UTC from IEEE Xplore.  Restrictions apply. 
484
paper, we demonstrate that implementations of the Gaussian
mechanism suffer from the same attack, with adjustments
to the attack process. In [45] the authors demonstrate an
attack against DP mechanisms that use ﬁnite-precision rep-
resentations, and propose a mitigation strategy. Recently Il-
vento [46] has explored practical considerations and pitfalls
of implementing the exponential mechanism using ﬂoating-
point arithmetic. They show that such implementations are also
susceptible to attacks and propose a solution using a base-2
version of the exponential mechanism.
Timing attacks against DP mechanisms have been explored
in [7] and [8]. However, they differ from the attacks described
in this paper as they do not exploit the timing discrepancies
introduced by distribution sampling. In [7], the authors observe
that the mechanism implementation may suffer from timing
attacks (e.g., because it performs conditional execution based
on a secret). As a mitigation authors propose constant time
execution for the mechanism, without consideration of noise
generation. On the other hand, Andrysco et al. [8] exploit
the difference in timing of ﬂoating-point arithmetic operations
(e.g., multiplication by zero takes observably less time than
multiplication by a non-zero value). They also show that DP
mechanisms (including [7]) are susceptible to information
leakage by using ﬂoating-point instructions whose running
time depends on their operands.
Balcer and Vadhan [39] outline shortcomings of imple-
menting differentially-private mechanisms on ﬁnite-precision
computers, including a discussion of ﬂoating-point represen-
tations and sampling from distributions with inﬁnite support.
The authors propose a polynomial-time discrete method for
answering approximate histograms. Their method is based on
a bounded (or truncated) geometric distribution. Though a full
implementation is not provided, the authors suggest that the
distribution can be sampled via inverse transform sampling
using binary search over the support range using cumulative
distribution function F to guide the search. That is, given a
uniform random value p sampled uniformly from (0, 1], ﬁnd
smallest value x from the support of the geometric distribution
such that F (x) ≥ p. If performed na¨ıvely, such an approach
could reveal the magnitude of the noise of the geometric
distribution since the search will take longer for “less likely”
values (i.e., larger p) and would therefore suffer from the same
timing channel as described in Section VI.
often rely on a multi-dimensional discrete Gaussian which is
approximated by a distribution that is statistically close to the
desired distribution. Several sampling mechanisms have been
shown to suffer from cache-based side-channel attacks [51]
(i.e., based on memory accesses of the underlying algorithm)
and power analysis [44]. Defenses based on constant-time exe-
cution [52], [53], [54] and shufﬂing [44] have been proposed to
protect against some of these attacks including timing. Though
some techniques proposed for hardening the code in this space
can be used for protecting samplers for DP (Section VIII),
direct use of such samplers for DP is not straightforward.
This follows from the observation that discrete variants in
this area are (only) statistically close to the desired discrete
distribution. As a result, composition-based analysis developed
for analyzing cumulative loss of multiple mechanisms based
on Gaussian noise [17], [35] cannot be used directly.
X. CONCLUSION
In this paper we highlight two implementation ﬂaws of
differentially private (DP) algorithms. We ﬁrst show that the
widely used Gaussian mechanism suffers from a ﬂoating-point
(FP) attack against implementations of normal distribution
sampling, similar to vulnerabilities of the Laplace mecha-
nisms as demonstrated in 2011 by Mironov. We empirically
demonstrate that implementations in NumPy, PyTorch and
Go, including those used in implementation of open-source
DP libraries are susceptible to the attack, hence violating their
privacy guarantees. Though some researchers have speculated
that the Gaussian mechanism may be susceptible to FP attacks,
this is the ﬁrst work to provide a comprehensive evaluation
showing that it is feasible in practice.
In the second part of the paper we show that
imple-
mentations of discrete Laplace and Gaussian mechanisms —
proposed as a remedy to the FP attack against their continuous
counterparts — are themselves vulnerable to another side-
channel due to timing. That is, we show that implementations
of such discrete variants, including a DP library by Google,
exhibit the time that is correlated with the magnitude of the
secret random noise. Our work re-iterates the importance of
careful implementation of DP mechanisms in order to maintain
their theoretical guarantees in practice.
ACKNOWLEDGMENT
The authors are grateful to the anonymous reviewers for
their feedback that helped improve the paper. This work was
supported in part by a Facebook Research Grant and the
joint CATCH MURI-AUSMURI. The ﬁrst author is supported
by the University of Melbourne research scholarship (MRS)
scheme. We thank Thomas Steinke for pointing us to the
Ziggurat method in Go and Cl´ement Canonne for insightful
discussions on statistically-close Gaussian samplers.
REFERENCES
[1] A. Bittau, U. Erlingsson, P. Maniatis, I. Mironov, A. Raghunathan,
D. Lie, M. Rudominer, U. Kode, J. Tinnes, and B. Seefeld, “Prochlo:
Strong privacy for analytics in the crowd,” in ACM Symposium on
Operating Systems Principles (SOSP), 2017.
In independent and parallel work [47], the authors describe a
theoretical attack against the Box-Muller method of sampling
from the Gaussian distribution in a similar manner to our
FP attack. However, they do not provide experiments vali-
dating the attack’s efﬁcacy against existing implementations.
The authors propose a mitigation strategy similar to the one
mentioned in Section VIII based on computing a Gaussian
sample from multiple samples, and analyze its robustness.
Their work does not consider timing attacks.
Stepping away from differential privacy, Gaussian samplers
have been also widely used in schemes for digital signatures,
public key encryption, and key exchange based on lattice based
cryptography [48], [49], [50]. Such cryptographic primitives
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:38:32 UTC from IEEE Xplore.  Restrictions apply. 
485
[2] G. Andrew, S. Chien, and N. Papernot, “TensorFlow Privacy,” https:
//github.com/tensorﬂow/privacy, 2019, [Online; accessed 01-Dec-2021].
https://github.com/google/
[3] “Google
Differential
Privacy,”
differential-privacy, 2021, [Online; accessed 01-Dec-2021].
[4] “Opacus,” https://github.com/pytorch/opacus, 2020, [Online; accessed
01-Dec-2021].
[5] “Diffprivlib,” https://github.com/IBM/differential-privacy-library, 2021,
[Online; accessed 01-Dec-2021].
[6] I. Mironov, “On signiﬁcance of the least signiﬁcant bits for differen-
tial privacy,” in ACM Conference on Computer and Communications
Security (CCS). ACM, 2012, pp. 650–661.
[7] A. Haeberlen, B. C. Pierce, and A. Narayan, “Differential privacy under
ﬁre,” in USENIX Security Symposium, 2011, p. 33.
[8] M. Andrysco, D. Kohlbrenner, K. Mowery, R. Jhala, S. Lerner,
and H. Shacham, “On subnormal ﬂoating point and abnormal
timing,” in IEEE Symposium on Security and Privacy (S&P).
IEEE Computer Society, 2015, pp. 623–639.
[Online]. Available:
https://doi.org/10.1109/SP.2015.44
[9] 30th
IEEE Computer Security Foundations Symposium, CSF
2017, Santa Barbara, CA, USA, August 21-25, 2017.