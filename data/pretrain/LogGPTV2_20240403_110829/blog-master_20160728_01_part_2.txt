* 基于回滚段或MVCC的数据库，索引的写放大，都与是否发生行迁移有关，概率差不多。    
* 基于回滚段实现MVCC的数据库，如果要回滚事务，开销会很大（特别是当事务修改的数据量很大时），因为要从回滚段将整个块拷贝到数据文件（基于逻辑行拷贝的回滚则是类似重新来一遍UNDO事务的SQL操作，同时还需要擦除之前更改的行）。      
* 基于多版本实现MVCC的数据库，事务回滚非常快，因为不需要拷贝行或者数据块，也不需要修改已更新的记录，只是记录clog时将当前事务标记为ABORT即可，也就是说只需要改2个比特位。    
* 早在2007年，PostgreSQL就已经使用HOT技术完美的解决了索引更新的问题，根本不存在UPDATE数据时一定需要更新索引的问题。    
  我在很多场合分享过HOT的原理，也有相应的文章介绍。    
  要了解细节的话，可以看一下PostgreSQL源码中的 src/backend/access/heap/README.HOT    
  另外PostgreSQL还支持hash, gin, gist, sp-gist, brin索引，用户如果想了解这些索引，可以参考    
  https://www.pgcon.org/2016/schedule/attachments/434_Index-internals-PGCon2016.pdf    
* UBER文章指出的基于B+tree即secondary index指向PK，仅仅是一种避免UPDATE数据需要索引的方法。    
  但是这种方法引入了一些问题如下 ：      
  1\. 插入数据会变慢，因为数据存储是B+Tree结构的。    
  2\. 如果插入的是随机的PK值，则会频繁的带来页分裂，会造成IO写放大。    
  3\. 为了解决索引分裂的问题，导致了写堵塞读。 原因是引入了ChangeBuffer，当读的数据还在ChangeBuffer中时，需要先将其merge到b+tree，merge过程是堵塞读的。    
  4\. 查询secondary时，要再走一遍primary index，带来了额外的离散扫描开销，如果secondary是范围扫描或者多点扫描，这个放大倍数是很可观的。  例如用户要从secondary index扫描10条记录，primary index的深度是4，那么除了secondary index的数据块扫描，还有额外多扫描40个primary的块。    
  ![screenshot](20160728_01_pic_004.png)    
  PostgreSQL是记录的(block_number, offset)，所以1条记录只需要扫描1个数据块。      
  ![screenshot](20160728_01_pic_005.png)    
  5\. 因为b+tree会将行存储在索引页中，所以一页能存下的记录数会大大减少，从而导致b+tree的层级比单纯的b-tree深一些。 特别是行宽较宽的表。    
  例如行宽为几百字节，16K的页可能就只能存储十几条记录，一千万记录的表，索引深度达到7级，加上metapage，命中一条记录需要扫描8个数据块。    
  而使用PostgreSQL堆表+PK的方式，索引页通常能存几百条记录（以16K为例，约存储800条记录），索引深度为3时能支撑5亿记录，所以命中一条记录实际上只需要扫描5个块(meta+2 branch+leaf+heap)。      
#### **彩蛋**   
* PostgreSQL TOAST机制    
  PostgreSQL的TOAST机制，可以将变长类型的值，自动压缩存储到另一片区域，通过内部的POINT指向，而不影响行的其他值。  例如存储文档，或者图片的表，如果这个表上有一些字段要更新，有一些字段不要更新，那么在更新时，PostgreSQL数据库会有非常大的优势，因为行很小。      
  ![screenshot](20160728_01_pic_006.png)  
  基于回滚段实现MVCC的数据库，需要拷贝旧的记录或数据块到回滚段，记录或块越大，这个开销越大。    
  ![screenshot](20160728_01_pic_007.png)   
  存储文档、图像、非结构化数据，使用PostgreSQL很有优势。    
* MySQL innodb是基于B+树的存储，当PK数据随机数据写入时存在巨大写放大，因为经常要分裂，不仅影响插入速度和查询速度，同时数据存放也会变得非常无序。    
  即使按PK顺序扫描时，也可能出现大量的离散IO。    
  ![screenshot](20160728_01_pic_008.png)  
  基于B+树结构的存储，为了提高插入速度，如果使用index cache的话，则影响并发的查询，因为查询时要先合并索引。    
  ![screenshot](20160728_01_pic_009.png)  
  另一方面，B+树的存储，必须要求表需要一个PK（即使表没有PK的需求，也要硬塞一个PK列进来），secondary index则指向这个PK。    
  如果PK发生更新，则所有的secondary index都要更新，也就是说，为了保证secondary不更新，务必确保PK不更新。    
  如果要对secondary index进行范围扫描，物理的扫描上是离散的。    
![screenshot](20160728_01_pic_010.png)  
  所以uber本文提出的，secondary index 不需要变更的好处，其实背后是有以上代价存在的（例如一定要加PK，插入速度更慢，插入时PK不能随机否则分裂带来的IO巨大，使用secondary index范围扫描时会造成离散的IO等弊端），把原理，代价都交代清楚，才能看得更明白。    
  PostgreSQL 有几种方法来消除这种离散IO。    
  1\. bitmap scan，获取heap tuple前，先根据ctid的blockid排序然后再从heap获取记录，以获得物理上顺序的扫描。     
![screenshot](20160728_01_pic_011.png)  
  2\. cluster by index，将表的物理存储顺序按照索引的顺序来存放，从而使用该索引进行范围扫描时，则是顺序的扫描。     
  但是请注意cluster的行为是一次性的，表依旧是堆表，只是物理存储的顺序与索引的顺序相关性一致，从而达到了查询时消除离散扫描的功效，它更适合静态的历史数据。    
  例如微博类的应用，可以将历史数据按用户ID和时间索引进行cluster化，那么在根据时间或用户ID查询这个用户的历史记录时，就不会产生离散的IO。    
![screenshot](20160728_01_pic_012.png)    
  3\. BRIN索引，这个是针对流式记录的一种索引，只记录块或者相邻块的元数据，如取值范围。  从而实现快速检索的目的。  详见    
  https://yq.aliyun.com/articles/27860  
* PostgreSQL的表是基于HEAP存储的，不存在以上B+树存储的问题，随便怎么插入，速度都很快。    
* SSD的原子写，通常SSD写入时是以最小单位为4K的写入，即使修改很小的数据。    
  那么以directio或buffer io为主的数据库，哪个对SSD的伤害更大呢？    
  对于directio的数据库，因为每次都是真实的伤害，而buffer io的数据库，OS层还会合并IO，可以大幅降低SSD的真实写(os 层调整vm.dirty_background_ratio可以调整写频率，从而影响合并粒度)。    
  PostgreSQL的shared buffer管理是基于buffer io的管理，对SSD来说是一种很好的保护，有兴趣的童鞋可以测试验证一下。    
  ![screenshot](20160728_01_pic_013.png)    
### 2. Inefficient data replication     
#### **uber文章的观点**      
PG的复制低效，有写放大。    
#### **本文观点**        
PostgreSQL的流复制非常高效，延迟几乎为0，同时还支持流的压缩和加密传输，很多企业用流复制来实现异地容灾，HA，读写分离的应用场景。    
同时PostgreSQL也支持逻辑复制（>=9.4支持流式逻辑复制, <9.4的版本则支持基于触发器或者基于异步消息的逻辑复制）。     
#### **原理剖析**     
* 问题反驳 1 （复制低效）   
  我第一次听说PG的复制低效，要知道PG的复制是基于流式的物理变更，业界有名的高效，延迟极低（复制延迟与事务大小无关），几乎是接近0的延迟。     
  甚至用来做主备同步复制，对主库事务提交的RT影响也是可控的，主库依旧可以保持几十万的tps。    
  PostgreSQL流复制原理      
  即时唤醒，流式复制，所以延迟极低。    
  ![screenshot](20160728_01_pic_014.png)  
* 问题反驳 2  （REDO写放大）    
  基于回滚段实现MVCC的数据库，在更新时，拷贝到回滚段的旧版本，是要写REDO的。     
  而基于多版本实现MVCC的数据库，旧版本仅仅需要写修改行头bit位的REDO，所以基于多版本实现MVCC的数据库，更新时写入的REDO应该是基于回滚段实现MVCC的数据库的一半甚至更少（比如基于物理的回滚段要拷贝整个块，产生的REDO也很大）。     
  ![screenshot](20160728_01_pic_015.png)  
  同时，由于基于回滚段实现MVCC的数据库回滚时，要将回滚段的数据拷贝回数据文件，是会产生REDO的，这一点，基于多版本实现MVCC的数据库不存在这种写放大的问题。    
  ![screenshot](20160728_01_pic_016.png)  
* 问题反驳 3（复制流量放大）    
  基于REDO的物理复制，意思就是要把REDO复制一份到备库。    
  所以REDO写了多少，就要复制多少到备库，网络的流量也是这样的。    
  另一种是基于REDO的逻辑复制，需要复制的数据不仅仅包括新的数据，还要包括旧的版本数据(PK或者full row)。    
  可能一条记录更新前和更新后的数据都要复制。    
  对更新操作来说，物理复制，不需要复制旧的记录(因为产生REDO的仅仅是XMAX的变化)过去，而逻辑复制则需要复制旧的记录过去。    
  另外需要注意的是，目前PG的垃圾回收也是以物理恢复的形式复制的，在实现上还有改进空间，比如通过逻辑的方式复制垃圾回收(只复制block id)，可以大大减少网络传输的流量。    
  ![screenshot](20160728_01_pic_017.png)  
  而 uber 文章并没有指出，事实上 MySQL 目前只支持逻辑复制，并且如果要开启逻辑复制，不仅仅要写redo，同时还要写 binlog，等于写了双份日志，这个写放大也是很大的。     
  MySQL redo 用于恢复数据库，binlog用于复制。     
  ![screenshot](20160728_01_pic_018.png)   
  自PostgreSQL 9.4开始，PG内核层就同时支持物理复制和逻辑复制，而且仅仅写一份日志就能同时支持物理以及逻辑复制。    
  在9.4版本之前，则可以通过其他软件进行逻辑复制（例如Londiste3, slone-I）    
  ![screenshot](20160728_01_pic_019.png)    
  逻辑复制需要注意1，被复制的表一定要有PK。 **物理复制不存在这个问题** 。    
  逻辑复制需要注意2，大事务导致主备的延迟非常大，因为备库一定要等主库事务结束，备库才能开始回放该事务。 **物理复制不存在这个问题** 。      
#### **小结**    
* PG的复制是业界有名的高效，延迟极低（关键是复制延迟与事务大小无关），网络好的话，几乎是接近0的延迟。    
* 基于多版本实现MVCC的数据库，就版本仅仅需要写修改行头bit位的REDO，所以基于多版本实现MVCC的数据库，更新时写入的REDO应该是基于回滚段实现MVCC的数据库的一半甚至更少（比如物理回滚段要拷贝整个块，产生的REDO也很大）。   
* 对更新操作来说，基于REDO的物理复制，不需要复制旧的记录过去，而逻辑复制则需要复制旧的记录过去，物理复制产生的网络流量更小。     
* 逻辑复制有一个弊端，一定要PK。 **物理复制不存在这个问题** 。    
* 逻辑复制另一个弊端，大事务导致主备的延迟非常大，因为备库一定要等主库事务结束，备库才能开始回放该事务。 **物理复制不存在这个问题，不需要等待主库事务结束后再回放redo** 。    
#### **彩蛋**    
* PostgreSQL可以开启协议层压缩，同时可以选择是否加密传输，压缩传输REDO。更高效，更安全。    
* PG的用户如果有主备环境，可以关闭FULL_PAGE_WRITE，产生的REDO更少(第一次更新的PAGE不需要写FULL PAGE)。    
  但是需要注意，如果关闭了FPW并且主库因主机问题或在OS问题挂了，需要从备份环境恢复。    
* PG用户，可以将checkpoint拉长，减少FULL PAGE的产生，从而减少REDO的产生。    
* PG的用户，如果需要从PG或者MYSQL复制到阿里云的rds PG，可以使用阿里dbsync插件，目前支持全量复制，增量的逻辑复制正在开发中。    
  https://help.aliyun.com/document_detail/35458.html    
  https://help.aliyun.com/document_detail/35459.html   
  https://github.com/aliyun/rds_dbsync  
### 3. Issues with table corruption     
#### **uber文章的观点**       
用户在使用PG 9.2 时，因为极端情况下的主备切换，导致了一些数据corruption问题。      
#### **本文观点**        
从社区了解到，这个问题已经在9.2的版本修复，后面的版本也没有这个问题。      
PG一直以来就是一个以稳定性和功能强大著称的数据库，在企业市场有非常好的口碑。    
**国内的银行，运营商，保险，互联网公司都有在核心环境使用**    
- 平安科技、阿里巴巴、高德、去哪儿、腾讯、用友、阳光、中移动、探探、智联、典典、华为、斯凯、通策医疗、同花顺、核电、国家电网、邮储银行、友盟、莲子。。。。。。     
**海外的汽车生产巨头，政府部门，医疗，物流等各个行业也都有非常多的用户**       
- 生物制药 {Affymetrix(基因芯片), 美国化学协会, gene(结构生物学应用案例), …}  
- 电子商务 { CD BABY, etsy(与淘宝类似), whitepages, flightstats, Endpoint Corporation …}  
- 学校 {加州大学伯克利分校, 哈佛大学互联网与社会中心, .LRN, 莫斯科国立大学, 悉尼大学, …}  
- 金融 {Journyx, LLC, trusecommerce(类似支付宝),  日本证券交易交所, 邮储银行, 同花顺…}  
- 游戏 {MobyGames, …}  
- 政府 {美国国家气象局, 印度国家物理实验室, 联合国儿童基金, 美国疾病控制和预防中心,  美国国务院, 俄罗斯杜马…}  
- 医疗 {calorieking, 开源电子病历项目, shannon医学中心, …}  
- 制造业 {Exoteric Networks, 丰田, 捷豹路虎}  
- 媒体 {IMDB.com, 美国华盛顿邮报国会投票数据库, MacWorld, 绿色和平组织, …}  
- 零售 {ADP, CTC, Safeway, Tsutaya, Rockport, …}  
- 科技 {Sony, MySpace, Yahoo, Afilias, APPLE, 富士通, Omniti,  Red Hat, Sirius IT, SUN, 国际空间站, Instagram,  Disqus,  …}  
- 通信 {Cisco, Juniper, NTT(日本电信), 德国电信, Optus, Skype, Tlestra(澳洲电讯),  中国移动…}  
- 物流 {SF}  
#### **小结**   
基于逻辑复制的数据库，主库压力大或者有长事务时，备库追不上主库时有发生。     
又或者因为某些原因导致主备不一致，即使发现了，可能并没有很好的修复手段，因为你不知道该以哪个数据为准。    
逻辑复制导致主备不一致的原因较多，例如  主库执行失败，备库执行成功，或者备库执行成功，主库执行失败。    
又或者 主库和备库的环境不一致，例如字符集，或者其他的，都非常容易导致主和备的不一致。    
对于要求主备严格一致的场景，强烈建议使用物理复制。    
### 4. Poor replica MVCC support     
#### **uber文章的观点**      
PG备库的MVCC支持较差，查询会与恢复堵塞     
#### **本文观点**        
首先，PG的备库分两种，一种是物理备库，一种是逻辑备库。    
对于逻辑备库来说，与MYSQL的恢复机制是一样的，既然是一样就不需要讨论了。    
UBER文章说的 查询会与恢复堵塞，说的是物理备库，但必须纠正一个观点，查询是否堵塞恢复，要论场景，况且堵塞的情况极为少见。    
还有一点要注意，逻辑复制也会有堵塞备库的QUERY。    
#### **原理剖析**     
物理复制，什么情况下查询会堵塞、或与恢复冲突？        
当以下操作产生的REDO被复制到备库，并且备库准备拿这些REDO来恢复时。    
* Access Exclusive locks taken on the primary server, including both explicit LOCK commands and various DDL actions, conflict with table accesses in standby queries.  
  主库的访问排它锁，与备库对应的锁产生冲突。    
  例如主库truncate a表, 备库查询a表。    
  这种情况的冲突面很窄。    