1100
1000
900
800
700
600
500
400
300
200
100
0
1
2
3
4
5
6
7
9 10 11 12 13 14 15 16 17 18
1
2
3
4
5
invited
accepted
decoy accounts
added contacts
decoy accounts
Figure 2. Number of friend invitations sent
and number of accepted invitations, per de(cid:173)
coy account.
Figure 3. Number of invitations our decoy ac(cid:173)
counts received and accepted after being ad(cid:173)
vertised on messengerﬁnder.com
IM client software and is, thus, vulnerable to infections.
In order to prevent infections that will render our hon-
eypot useless and make it a platform for further attacks,
the honeypot messengers run inside Argos[34]. Argos
is a containment environment that is based on memory
tainting techniques. In a nutshell, Argos marks all bytes
coming from the network as dirty and tracks their ﬂow
in the system memory. If a dirty byte is passed to the
processor for execution, then we have an exploitation
attempt since honeypots never download software to ex-
ecute. When an exploit is detected, the application under
attack is restarted and all attack information is logged to
an alert ﬁle. This way, our system cannot be used as
an attack platform as it is immune to remote exploits.
A disadvantage of containment environments is that ap-
plications run 20 to 40 times slower than in a vanilla
system. However, in the case of HoneyBuddy this is
not a problem as messengers are hardly demanding in
terms of computing power and memory requirements,
since most of the time they are idle and wait for incom-
ing messages. In our experiments, each MSN client’s
memory footprint is 50-80 MB and consumes negligible
CPU resources.
We have to note, however, that during our experi-
ments we have not yet encountered any attempts from
attackers trying to exploit the IM client software. While
this type of attack may not be common now, we believe
that when IM attacks become more widespread, our im-
plementation will provide useful information for such
attacks.
5 Collected data analysis
In this section we provide an analysis of data col-
lected by the HoneyBuddy infrastructure, from the 27th
of February to the 16th of September 2009. Despite the
technical simplicity of our system, we were surprised
by the fact that popular defense mechanisms had not de-
tected the majority of our collected data. During the col-
lection period, the HoneyBuddy infrastructure collected
6,966 unique URLs that belong to 742 unique top-level
domains.
During the ﬁrst weeks of Honeybuddy operation we
were able to fetch all URLs through the wget tool. How-
ever, malicious sites changed their behavior to avoid
these fetches. Their pages now serve an obfuscated
piece of Javascript code that changes the window loca-
tion to a URL like http://www.malicious.com/
?key=. If a user has not visited
the page with the key, then all subsequent requests are
ignored and eventually her IP address is blocked for 24
hours. This behavioral change has forced us to fetch
URLs through the Crowbar [6] environment that allows
running javascript scrapers against a DOM to automate
web sites scraping.
Our ﬁrst step was to provide a simple classiﬁcation
for those URLs. Our ﬁve major categories were phish-
ing, porn, dating, adware 2 and malware. The results are
summarized in Figure 4. 1,933 of the URLs in 142 top-
level domains were phishing for MSN accounts, 1,240
2We characterize sites that promote third-party addons for the MSN
messenger (like extra winks, emoticons etc.) as adware sites
URLs
TLDs
2000
1500
1000
500
200
100
s
e
c
n
a
r
a
e
p
p
A
50
20
e
g
a
t
n
e
c
r
e
P
100
90
80
70
60
50
40
30
20
10
0
Phishing
Porn
Dating
Adware
Malware
0
50
100
150
Category
All URLs
Phishing
Porn
Adware
Unknown
350
400
450
500
250
200
Uptime (hours)
300
Figure 4. Classiﬁcation of collected URLs
Figure 5. CDF of uptime of URLs per category
were porn, 567 were dating services and 251 sites were
adware. While porn, dating and adware can be consid-
ered as harmless, the phishing sites pose a security dan-
ger for users. Furthermore, 77 URLs redirected to exe-
cutable ﬁles or to pages that contained a direct link to a
“.exe” or “.scr” ﬁle. We classify these URLs as malware.
We
that
sites
also spotted several
advertise
subscription-based services for mobile phones. When
the users enter their mobile phone number, they receive
a subscription request. Once they subscribe to the
service, they get charged for receiving SMS messages.
These sites claim to give away free mobile devices to
the subscribers of the service or promote quiz games
that may attract victims, such as love calculators etc.
These sites are highly localized. We visited them
from different geographic locations using the Planetlab
infrastructure [16] and got different pages in the lan-
guage of the origin country. An interesting fact is that
when the site cannot ﬁnd the geolocation of the user, it
redirects her to an MSN phishing site.
Our second step was to analyze the uptime of the col-
lected URLs. The uptime graph can be seen in Figure 5.
On average, a site is functional approximately for 240
hours (10 days). We also plotted the uptime graph for
each category. We notice that porn and MSN phishing
sites present much higher uptime than adware and un-
classiﬁed sites. Half of the MSN phishing sites were
alive for up to 250 hours (ten and a half days), while ad-
ware present a shorter lifetime of up to 80 hours (three
and a half days).
5.1 MSN phishing
Attackers try to gather MSN credentials by tricking
the user into entering her MSN e-mail and password in
a bogus site. These sites falsely advertise a service that
will reveal to the user which accounts from her contact
list have blocked her. To validate that these phishing
sites actually steal user credentials, we created several
MSN accounts and entered them into the phishing sites.
Each account had one of our decoy accounts as a friend.
The decoy account received messages from the stolen
MSN accounts that advertised the phishing site. How-
ever, the attackers did not change the passwords of any
of the compromised accounts. After the attackers had
started using our victim accounts, we submited “friend
requests” towards these accounts. We wanted to see
whether attackers will interfere with such matters and
automatically accept requests. None of the submitted
requests were accepted.
All phishing sites we visited shared one of three dif-
ferent “looks”. A screenshot of such a site is shown in
Figure 1. We analyzed the source HTML code of all the
three “looks” and there was absolutely zero difference
among the pages with the same look. This means the
phishing pages with the same look had the exact same
size and contained the same images and forms. This
indicates that the majority of the different phishing cam-
paigns might be deployed by a number of collaborat-
ing attackers. We also detected a localized phishing site
which contained translated content, a technique used in
e-mail spam campaigns[20]. The number of syntactical
and grammatical errors revealed that the text translation
was done automatically. For the time being, simple pat-
tern matching for speciﬁc text segments is efﬁcient for
detecting these sites. Another detection mechanism is to
query the various URL blacklists.
We queried the Google blacklist through the Google
Safe Browsing API [8] to check if it included the phish-
ing sites we discovered. From the 142 unique top-level
domains (TLD) that hosted phishing sites and were de-
tected by HoneyBuddy, only 11 were listed by Google
blacklist. That means that 93% of the domains captured
by HoneyBuddy were not listed elsewhere on their day
of detection, making HoneyBuddy an attractive solution
for MSN phishing detection. The average delay from
when our system detected one of the 11 sites until it was
included in the Google blacklist was around two weeks,
leaving a time window of 15 days for attackers to trick
users. Firefox, one of the most popular browsers uses
the Google Safe Browsing API as an anti-phishing mea-
sure. We also compared our ﬁndings with the blacklist
maintained by SURBL [22] and URLblacklist.com [24].
SURBL detected only 1 out of the 142 MSN phishing
domains (0.7%) and none of the adware domains. None
of the phishing or adware sites were listed by URLblack-
list.com.
A very interesting fact was that when resolved, all the
unique top level domains translated to a much smaller
number of unique IP addresses. This fact conﬁrms our
initial theory that all these phishing campaigns lead to
a limited number of collaborating attackers. To further
investigate this behaviour, we conducted an experiment
for a period of almost two months, presented in Section
6.
At the time of writing this paper, our infrastructure
collected two new types of malicious URLs that demon-
strate an evolution in the behaviour of IM scam cam-
paigns. The ﬁrst type is a phishing site that instead of
advertising a service for MSN users, has recreated the
exact “look and feel” of the Windows Live login page.
The second type is a site that offers a Java applet to con-
vert hosted images into a slideshow. The Java applet
is not signed from a trusted source and requires unre-
stricted access to the victim’s machine. Our disassem-
bly showed that the applet downloads a PE32 executable
from another domain and executes it.
5.2 Malware sample analysis
In this section we provide an analysis of the mal-
ware collected by the HoneyBuddy infrastructure, from
the 1st to the 31st of March 2009. Our infrastructure
collected 19 unique malware samples. We distinguish
the malware collected by the HoneyBuddy infrastruc-
ture into two categories, the direct malware set and the
indirect malware set. We present the two categories and
proceed to further analyze the collected samples.
The ﬁrst category contains malware samples col-
lected either through direct ﬁle transfers (uncommon
case) or by visiting URLs that were redirected to exe-
cutable ﬁles.
In the case of the URLs, the e-mail ac-
count of the victim was always appended as a parameter
to make it look more realistic. In some cases attackers
used popular keywords, like Facebook.
The second category, the indirect one, contains mal-
ware samples collected in two types of cases.
In the
ﬁrst case, users are presented with a web page that alerts
them that they need to download the latest version of
the “adobe ﬂash plugin” so as to play a certain video
strip, and are prompted to download the installer which
is, obviously, malware.
In the second case, users are
redirected to a page prompting them to install a screen
saver. This “.scr” ﬁle they are prompted to download
and install is a malicious ﬁle that infects the machine
upon execution.
Due to the small volume of ﬁles, we were able to
manually check these ﬁles using the Anubis analysis
center [1]. All of them were characterized as danger-
ous , while some of them were bots that connected to
an IRC C&C server. By joining the IRC network, we
downloaded even more malware samples (not listed in
this section).
In order to verify how original our samples are, we
submitted them to the VirusTotal [25] service. VirusTo-
tal is a large malware collection center with the primary
goal of providing a free online virus and malware scan
report for uploaded samples. The reason we chose to use
VirusTotal is twofold. First, VirusTotal receives around
100,000 samples every day from a multitude of sources
resulting in a large database of malware samples. The
second and most important reason is that VirusTotal col-
laborates with a large number of well known anti-virus
vendors3 and uses their anti-virus engines and, therefore,
can provide us with an accurate picture of the efﬁciency
of up-to-date, state-of-the-art defense solutions for home
users.
Four collected samples had not been seen by Virus-
Total before, that is 21% of our samples were previously
unseen malware instances. Figure 6 shows the relative
detection delay compared to the date the samples en-
tered the VirusTotal database. The base bar of the stack
graph (solid white) shows how many samples were de-
tected with a delay of one or more days, the middle bar
(solid black) displays the number of samples that were
3For a complete list refer to http://www.virustotal.com/
sobre.html
>1 days
Same day Not detected by VirusTotal
)
%
(
e
t
a
r
n
o
i
t
c
e
t
e
d
V
A
/
l
s
e
p
m
a
s
f
o
r
e
b
m
u
N
8
7
6
5
4
3
2
1
0
80
70
60
50
40
30
20
10
0
0
1
2
Week
3
4
0
20
40
60
80
100
Percentage of samples
Figure 6. Detection delay of collected sam(cid:173)
ples compared to the VirusTotal database.
21% of the samples were previously unseen,