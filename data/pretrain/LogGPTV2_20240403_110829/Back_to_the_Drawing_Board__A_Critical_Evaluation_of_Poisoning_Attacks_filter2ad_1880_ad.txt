Data poison
T5
T6
Data poison
Data poison
T7
T8
Data poison
Knowledge
∈ {Nb, Wb}
Nobox
Nobox
Whitebox
Whitebox
Nobox
Nobox
Whitebox
Whitebox
Attack mode
∈ {Off, On}
Offline
Online
Offline
Online
Offline
Online
Offline
Online
3) Practical Ranges of FL Parameters: We argue that
the literature on untargeted poisoning [5], [10], [23], [41],
[55] rarely evaluates their proposed attacks/defenses for the
production FL settings, primarily due to their motivation to
perform worse-case analyses. But, we show that such analyses
lead to conclusions that do not apply to production FL.
Table III demonstrates the stark differences between the
parameter ranges used in the untargeted poisoning literature
and their practical ranges, which we have obtained from recent
surveys [11], [32] and discussion among FL experts [24]. This
is due to the more challenging nature of untargeted poisoning
in FL. We attribute this to the difficulty of establishing
successful untargeted attacks for practical settings, as we will
also show in our evaluations.
Contrary to what production FL settings encounter, previous
works commonly evaluate robustness using very high percent-
ages of compromised clients and/or using model poisoning
attacks on cross-silo FL (Table III). However, we use small
percentages of compromised clients M ≤1, for cross-device
FL, use large numbers of clients N ∈[1, 000, 34, 000] and use
n ∈ [25, 50] ≪ N in each round; we use N=n=50.
In particular, consider the percentages of compromised
clients; state-of-the-art attacks [5], [23], [55] (defenses [10],
[16], [68], [70]) assume adversaries who can compromise up
to 25% (50%) of FL clients. The cost of creating and operating
a compromised client botnet at scale (which includes breaking
into devices) is non-trivial. To create the botnet, the adversary
would need to either buy many physical devices (∼$25 each)
and root them (for state-of-the-art model poisoning attacks [5],
[23], [55]), pay for access to large but undetected botnets with
remote administrative access, or develop an entirely new bot-
net via compromising a popular app/sdk to exploit unpatched
security holes and gain persistence. To operate the botnet, the
adversary must avoid detection by antimalware services [28]
as well as dynamic anti-abuse services (such as Android’s
SafetyNet [53]). With a botnet in place, the adversary may
further need to pay for a skilled engineering team to keep
malicious FL code in sync with the target FL-enabled app and
to reverse-engineer frequently-shifting ML workloads. Such
an engineering team could instead change apps’ behaviors to
mimic the effect of a compromised FL-trained model, they
might use their privileged access to steal login credentials
for account hijacking, or they might participate in ad/click
fraud or bank fraud or ransomware for financial gain. More
plausible scenarios for an adversary reaching double-digit
client percentages—such as an app insider—likely enable
attacker-controlled FL servers, thereby removing them from
the literature’s standard threat model.
For data poisoning attacks, we assume that compromised
clients can have a limited amount of poisoned data Dp.
Because, in cross-device FL, the devices with low processing
powers (e.g., smart phones and watches) can process limited
Dp in the short duration of FL rounds. However, in cross-
silo FL, silos can inspect Dp and remove Dp with sizes much
larger than the average size of clients’ data |D|avg. Hence, we
argue that |Dp| should be up to 100×|D|avg. We discuss rest
of the parameters from Table III in the corresponding sections.
C. Threat Models in Practice
Here we discuss the two threat models of practical interest.
1) Nobox Offline Data Poisoning (T4):
In this setting,
the adversary does not know the architecture, parameters, or
outputs of the global model. The adversary knows the server’s
AGR, but may or may not know the global model architecture;
we evaluate both cases. We assume that the adversary knows
the benign data of the compromised clients and mounts offline
data poisoning attacks (DPAs).
This adversary does not require any access to the internals
(e.g., FL binaries, memory) of compromised devices, and
therefore, can compromise large percentages of production
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:01:32 UTC from IEEE Xplore.  Restrictions apply. 
1360
attacks carefully tailor themselves to the target AGR, and
hence, perform better.
Dynamic Optimization (DYN-OPT) attack [55] proposes a
general FL poisoning framework and then tailors it to specific
FL settings. DYN-OPT computes an average of the available
benign updates, ∇b, and perturbs it
in a dynamic, data-
dependent malicious direction ω to compute the final poisoned
update ∇′ = ∇b + γω. DYN-OPT finds the largest γ that
successfully circumvents the target AGR. DYN-OPT is much
stronger, because unlike STAT-OPT, it finds the largest γ and
uses a dataset tailored ω.
B. Our Improved FL Poisoning Attacks
We first present a general optimization problem to model
FL poisoning attacks. Then we use it to design improved
poisoning attacks on state-of-the-art AGRs from Section II-B.
1) Formulating FL Poisoning as an Optimization Problem:
Our optimization problem for poisoning attacks is based on
that of [55]. Specifically, we aim to craft poisoned updates
(via data or model poisoning) which will increase the overall
distance between the poisoned aggregate (computed using
poisoned and benign updates) and the benign aggregate (com-
puted using only benign updates). This can be formalized as
follows:
∥∇b − ∇p∥
(1)
argmax
∇′∈Rd
...∇b =favg(∇i∈{[n′]}), ∇p = fagr(∇′
{i∈[m]},∇{i∈[n′]})
where, m is the number of compromised clients selected in the
given round, fagr is the target AGR, favg is the Average AGR,
∇{i∈[n′]} are the benign updates available to the adversary
(e.g., updates computed using the benign data of compromised
clients), ∇b is a reference benign aggregate, and ∇′
{i∈[m]} are
m replicas of the poisoned update, ∇′, of our attack. ∇p is
the final poisoned aggregate.
Although our optimization problem in (1) is same as [55],
two key differences from [55] are: (1) We are the first to use (1)
to construct systematic data poisoning attacks on FL. (2) Our
model poisoning attacks not only tailor the optimization in (1)
to the given AGR (as in [55]), but also to the given dataset and
global model, by using stochastic gradient ascent algorithm
(Section IV-B3); this boosts the efficacy of our attack.
2) Our Data Poisoning Attacks (DPAs): We formulate a
general DPA optimization problem using (1) as follows:
∥∇b − ∇p∥
(2)
argmax
Dp⊂D
FL clients, e.g., on order of up to 0.1% [24], [32]. However,
the poisoning impact of the corresponding poisoned updates
is very limited. This is partly because arbitrarily poisoned
updates (e.g., of model poisoning attacks (MPAs) [5], [23],
[55]) need not map to the valid data domain. For instance,
consider the standard max function: f (x, y)=max(x, y). Gra-
dient of this function with respect to either x or y is always 0
or 1 [19]. Hence, a DPA cannot have a poisoned update with
an arbitrary value for gradients of the parameters. But an MPA
can, because it can directly assign any arbitrary value to the
parameters’ gradients.
2) Whitebox Online Model Poisoning (T5): The adversary
knows the parameters and predictions of the global model
whenever the server selects at least one compromised client.
We assume that the adversary knows the server’s aggregation
rule and the benign data on the compromised devices. The
adversary mounts online MPAs.
Unlike data poisoning adversary, this adversary breaks into
the compromised devices, which is extremely costly as dis-
cussed in Section III-B3. Hence, in practice, a model poisoning
adversary can compromise very small percentages of FL
clients, e.g., on order of up to 0.01% [24], [32]. However,
due to their ability to directly manipulate the model updates, in
theory, a model poisoning adversary can craft highly poisonous
updates. We can justify this claim from the example of a zero-
value parameter discussed in Section III-C1.
IV. EXPLORING THE SPACE OF FL POISONING ATTACKS
A. Existing FL Poisoning Attacks
1) Data Poisoning Attacks (DPAs): DPAs have been studied
mainly for centralized ML [17], [44], [64], [65], [69], and no
prior work has studied untargeted DPAs that are tailored to
FL settings. Fang et al. [23] show the possibility of applying
simple label flipping attacks to FL, where each compromised
client flips the labels of their data from true label y ∈ [0, C−1]
to false label (C−1−y) if C is even and to false label (C−y)
if C is odd, where C is the number of classes.
2) Model Poisoning Attacks (MPAs): These consider our
whitebox online model poisoning threat model (T4) from
Section III-C2).
Little Is Enough (LIE) attack [5] adds small amounts of
noise to each dimension of the average of the benign updates.
Specifically, the adversary computes the average (∇b) and
the standard deviation (σ) of the available benign updates;
then computes a coefficient z based on the number of benign
and compromised clients; and finally computes the poisoned
update as ∇′ = ∇b + zσ. [5] shows that such noises easily
evade the detection by robust AGRs as well as effectively
poison the global model.
Static Optimization (STAT-OPT) attack [23] proposes a gen-
eral FL poisoning framework and then tailors it to specific
AGRs. STAT-OPT computes the average (∇b) of the available
benign updates and computes a static malicious direction,
ω = −sign(∇b); the final poisoned update, ∇′, is −γω and
the attack finds a suboptimal γ that circumvents the target
AGR; for details please refer to [23]. Unlike LIE, STAT-OPT
...∇b and ∇p as in (1) and ∇′ = A(Dp, θg) − θg
where D is the entire input space and Dp is the poisoning
data used to compute the poisoned update ∇′ using a training
algorithm A, e.g., mini-batch SGD, and global model θg. The
rest of the notations are the same as in (1). To solve (2),
we find Dp such that when θg is fine-tuned using Dp, the
resulting model θ′ will have high cross-entropy loss on some
benign data Db (e.g., that of compromised clients), i.e., high
L(Db; θ′), and the corresponding update ∇′ = θ′ − θg will
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:01:32 UTC from IEEE Xplore.  Restrictions apply. 
1361
Figure 3: Schematic of our PGA attack: PGA first computes
a poisoned update ∇′ using stochastic gradient ascent (SGA).
Then, fproject finds the scaling factor γ that maximizes the de-
viation between benign aggregate ∇b and poisoned aggregate
∇p
γ. Robust aggregations easily discard the scaled poisoned
updates, γ∇′, with very high γ (e.g., γ{4,5}), while those with
very small γ (e.g., γ{1,2}) have no impact.
AGR, we produce updates with large loss and norm [10], [41]
using very large amounts of label flipped data (Figures 2-(a,b)).
To obtain large |Dp|, we combine the benign data of all
compromised clients and flip their labels using either SLF or
DLF strategy (simply SLF/DLF). To increase |Dp| further, we
add Gaussian noise to existing feature vectors of |Dp| to obtain
new feature vectors and flip their labels using SLF or DLF.
Norm-bounding:
To attack Norm-bounding AGR (Sec-
tion II-C2), we use large |Dp| to generate poisoned updates
that incur high losses on benign data (as we show in Figure 2-
(b)). As our evaluations will show, even if their norms are
bounded, such poisoned updates remain far from benign up-
dates and have high poisoning impacts. This leads to effective
attacks, but only at high percentages of compromised clients
(e.g., M=10%). Due to space restrictions, we provide the
details of our DPAs on Mkrum and Trmean in Appendix B1.
3) Our Model Poisoning Attacks (MPAs): We use (1) as
the general optimization problem for our MPAs. To solve this
optimization, we craft a poisoned model θ′ with high L(Db; θ′)
while ensuring that the corresponding poisoned update, ∇′,
circumvents the target AGR.
Model poisoning adversary can directly manipulate the
compromised clients’ updates (Section III-C2). Hence, first,
our attack uses the stochastic gradient ascent (SGA) algorithm
(instead of SGD) and fine-tunes θg to increase (instead of
decreasing) the loss on some benign data, Db, to obtain a
malicious θ′. But, in order to ensure that the corresponding
poisoned update, i.e., ∇′ = θ′ − θg, circumvents the target
AGR, we project the update on a ball of radius τ around
origin, i.e., scale the update to have a norm ∥∇′∥ ≤ τ, where
τ is the average of norms of the available benign updates.
Hence, we call our attack projected gradient ascent (PGA).
To perform stochastic gradient ascent, we increase the loss on
batch b of data by using the opposite of a benign gradient
direction, i.e., −∇θL(θ; b).
Algorithm 1 (Appendix B) gives the overview of our MPA.
The adversary first computes τ (line 2), an average of the
norms of some benign updates available to her (∇{i∈[n′]}).
Figure 2: Effect of varying the sizes of poisoned data, Dp, on
the objectives of DPAs (Section IV-B2) on various AGRs. We
compute Dp by flipping the labels of benign data.
circumvent the target AGR. Our intuition is that, when the
global model is updated using such ∇′, it will have high loss
on benign data [8], [30], [43].
Sun et al. [57] propose DPAs on federated multi-task
learning where each client learns a different task. Hence, their
attacks are orthogonal to our work. On the other hand, as [23]
demonstrates, backgradient optimization based DPAs [43] are
computationally very expensive (∼10 days to compute poison
for a subset of MNIST task) yet ineffective.
Instead, because the central server has no visibility into the
clients’ data or their sizes, we propose to use an appropriate
amount of label flipped data as Dp for each of the compro-
mised clients. Our intuition behind this approach is the same
as before: the larger the amount of label flipped data used to
compute θ′, the larger the L(Dp; θ′) and ∥∇′∥, and therefore,
the higher the deviation in (2). We validate this intuition using
FEMNIST dataset in Figure 2 for various AGRs. For instance,
Figures 2 (a) and (b) show that increasing |Dp| monotonically