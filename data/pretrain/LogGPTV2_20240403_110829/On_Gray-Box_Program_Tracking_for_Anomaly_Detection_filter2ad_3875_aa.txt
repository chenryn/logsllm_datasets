title:On Gray-Box Program Tracking for Anomaly Detection
author:Debin Gao and
Michael K. Reiter and
Dawn Xiaodong Song
USENIX Association
Proceedings of the
13th USENIX Security Symposium
San Diego, CA, USA
August 9–13, 2004
© 2004 by The USENIX Association
Phone: 1 510 528 8649
All Rights Reserved
FAX: 1 510 548 5738
Rights to individual papers remain with the author or the author's employer.
Email: PI:EMAIL
For more information about the USENIX Association:
WWW: http://www.usenix.org
 Permission is granted for noncommercial reproduction of the work for educational or research purposes.
This copyright notice must be included in the reproduced paper. USENIX acknowledges all trademarks herein.
On Gray-Box Program Tracking for Anomaly Detection
Debin Gao
Michael K. Reiter
Dawn Song
Carnegie Mellon University
PI:EMAIL
PI:EMAIL
PI:EMAIL
Abstract
Many host-based anomaly detection systems moni-
tor a process ostensibly running a known program
by observing the system calls the process makes.
Numerous improvements to the precision of this ap-
proach have been proposed, such as tracking sys-
tem call sequences, and various “gray-box” exten-
sions such as examining the program counter or re-
turn addresses on the stack when system calls are
made. In this paper, we perform the ﬁrst system-
atic study of a wide spectrum of such methods. We
show that prior approaches can be organized along
three axes, revealing new possibilities for system-
call-based program tracking. Through an empiri-
cal analysis of this design space, we shed light on
the beneﬁts and costs of various points in the space
and identify new regions that appear to outperform
prior approaches.
In separate contributions, we
demonstrate novel mimicry attacks on a recent pro-
posal using return addresses for system-call-based
program tracking, and then suggest randomization
techniques to make such attacks more diﬃcult.
1 Introduction
A server program with buﬀer overﬂow or format
string vulnerabilities might permit an attacker to
commandeer a process running that program, eﬀec-
tively causing it to run the attacker’s program, in-
stead. In order to detect when this occurs, anomaly
detectors have been proposed to monitor the sys-
tem calls made by a process, in an eﬀort to detect
deviation from a known proﬁle of system calls for
the program it is ostensibly running. Such anomaly
detectors have been proposed and used in many set-
tings, including host-based intrusion detection sys-
tems (e.g., [4, 11, 19, 20]) and related sandboxing
and conﬁnement systems (e.g., [12, 22]).
Given the importance of system-call-based anomaly
detection, numerous approaches have been proposed
to improve their precision. Many of these ap-
proaches are seemingly orthogonal to one another,
and while each has been demonstrated to improve
precision (and often, increase cost) in isolation, how
best to use these enhancements in combination is
unclear. This is the primary question we address in
this paper.
In our analysis, we identify axes that
are motivated by proposed enhancements and then
empirically analyze the design space these axes de-
ﬁne. Our analysis covers many regions not previ-
ously explored in prior work, including some that
outperform previous approaches in our analysis. To
our knowledge, this study is the ﬁrst such system-
atic study of the design space for system-call-based
anomaly detection.
As an initial study of this design space, we limit our
attention to “gray-box” program monitoring tech-
niques. In order to characterize whether a system
call is anomalous, an anomaly detector builds a
model of the normal system-call behavior of the pro-
gram. We use “black box”, “gray box” and “white
box” to refer to the type of information the anomaly
detector uses to build this model and to monitor the
running process. Black-box detectors do not acquire
any additional information other than the system
call number and arguments that pass through the
system call interface when system calls are made
(e.g., [4, 17]). In contrast, white-box detectors ex-
amine all available information including the pro-
gram being monitored, by statically analyzing (and
potentially modifying) the source code or binary
(e.g., [2, 5, 6, 20]). Gray-box approaches lie in be-
tween: the anomaly detector does not utilize static
analysis of the program, but does extract additional
runtime information from the process being moni-
tored when a system call is made, e.g., by looking
into the program’s memory (e.g., [3, 16]). Here we
focus on gray-box approaches (and a few black-box
approaches as degenerate cases), again as an initial
study, but also because white-box approaches are
platform dependent and less universally applicable;
see Section 2.
A consequence of limiting our attention to gray-box
approaches is that any gray-box model of normal
behavior depends on being trained with execution
traces that contain all normal behaviors of the pro-
gram. It is not our goal here to determine how to ac-
quire adequate training data for a program. Rather,
we simply assume we have adequate training data in
our study; if this is not true, our techniques might
yield false detections, i.e., they may detect anoma-
lies that are not, in fact, intrusions.
In this context, this paper makes the following con-
tributions:
1. We organize the design space of gray-box pro-
gram tracking along three axes, that informally
capture (i) the information extracted from the
process on each system call; (ii) the granularity
of the atomic units utilized in anomaly detec-
tion (single system calls or variable-length sys-
tem call sequences); and (iii) the history of such
atomic units remembered by the anomaly de-
tector during monitoring. This framework en-
ables us to categorize most previous approaches
and to pinpoint new approaches that were not
explored before.
2. We systematically study this design space and
examine the cost and beneﬁts of the various
(including new) gray-box program tracking ap-
proaches. Exploiting richer information along
each axis improves the detector accuracy but
also induces additional costs, by increasing
both the size of the model and the cost of glean-
ing additional information from the running
process. Through systematic study, we com-
pare the beneﬁts (resilience against mimicry at-
tacks) and costs (performance and storage over-
head) of growing these parameters, and develop
recommendations for setting them in practice.
In a nutshell, our analysis suggests that by
examining return addresses, grouping system
calls into variable-length subsequences, and re-
membering a “window” of the two most recent
program states permits an anomaly detector to
track the program with good accuracy at rea-
sonable runtime and storage overhead, and to
prevent certain mimicry attacks that cannot be
stopped in previous approaches.
3. We generalize prior work on mimicry at-
tacks [18, 21] to demonstrate a previously un-
reported mimicry attack on systems that em-
ploy return address information as an input to
anomaly detection. Speciﬁcally, prior work in-
troducing the use of return address informa-
tion largely disregarded the possibility that this
information could be forged by the attacker.1
While doing so is indeed nontrivial, we demon-
strate how the attacker can forge this informa-
tion. Despite this observation, we demonstrate
that utilizing this information continues to have
beneﬁts in substantially increasing the attack
code size. This, in turn, can render some vul-
nerabilities impossible to exploit, e.g., due to
the limited buﬀer space within which an at-
tacker can insert attack code.
4. Finally, we suggest how to use (white-box) ran-
domization techniques to render the mimicry
attacks mentioned above more challenging.
The rest of the paper is organized as follows. Sec-
tion 2 introduces our proposed framework for gray-
box program tracking, which covers most of the
previous works in this area and our new propos-
als. Section 3 provides a detailed quantitative study
of the space of gray-box program tracking. Sec-
tion 4 presents our attack on a previously proposed
anomaly detector to forge information and evade de-
tection. In Section 5 we describe the randomization
technique to make such attacks more diﬃcult. Fi-
nally, we present our conclusion and future work in
Section 6.
2 Framework for gray-box program
tracking and new spaces
In system-call-based anomaly detection,
the
anomaly detector maintains state per process mon-
itored, and upon receiving a system call from that
process (and possibly deriving other information),
updates this state or detects an anomaly. Similar
to previous works (e.g.,
[16, 20]), we abstract
this process as implementing a nondeterministic
ﬁnite automaton (Q, Σ, δ, q0, q⊥), where Q is a
set of states including the initial state q0 and a
distinguished state q⊥ indicating that an anomaly
has been discovered; Σ is the space of inputs that
can be received (or derived) from the running
process; and δ ⊆ Q × Σ ×Q is a transition relation.
We reiterate that we deﬁne δ as a relation, with
the meaning that if state q ∈ Q is active and the
monitor receives input σ ∈ Σ, then subsequently
all states q(cid:2) such that (q, σ, q(cid:2)) ∈ δ are active.
If
the set of active states is empty, we treat this as a
transition to the distinguished state q⊥.
Below we describe how to instantiate Q and Σ along
the three axes, thereby deriving a space of diﬀerent
approaches for gray-box program tracking. We fur-
ther show that this space with three axes provides
a uniﬁed framework for gray-box program tracking,
which not only covers most of the previous relevant
gray-box proposals, but also enables us to identify
new ones.
1. The ﬁrst axis is the runtime information the
anomaly detector uses to check for anomalies.
In black-box approaches, the runtime informa-
tion that an anomaly detector uses is restricted
to whatever information is passed through the
system call interface, such as the system call
number and arguments (though we do not con-
sider arguments here). In a gray-box approach,
the anomaly detector can look into the pro-
cess’s address space and collect runtime infor-
mation, such as the program counter and the
set of return addresses on the function call
stack. Let S represent the set of system call
numbers, P represent the set of possible pro-
gram counter values, R represent the set of pos-
sible return addresses on the call stack. The
runtime information an anomaly detector could
use upon a system call could be S, P × S, or
R+ × P × S where R+ =
(cid:1)
d≥1 Rd.
The second and third axes are about how an
anomaly detector remembers execution history in
the time domain.
2. The second axis represents whether the atomic
unit that the detector monitors is a single sys-
tem call (and whatever information is extracted
during that system call) or a variable-length se-
quence of system calls [23, 24] that, intuitively,
should conform to a basic block of the moni-
tored program. That is, in the latter case, sys-
tem calls in an atomic unit always occur to-
gether in a ﬁxed sequence.
3. The third axis represents the number of atomic
units the anomaly detector remembers, in order
to determine the next permissible atomic units.
The decomposition of execution history in the time
domain into axes 2 and 3 matches program behavior
well: an atomic unit ideally corresponds to a basic
block in the program in which there is no branching;
the sequence of atomic units an anomaly detector
remembers captures the control ﬂow and transitions
among these basic blocks.
According to the three axes, we parameterize our
automaton to represent diﬀerent points in the space
of gray-box program tracking.
In particular, the
set of states Q is deﬁned as Q = {q0, q⊥} ∪
(cid:2)(cid:1)
(cid:3)
,2 and Σ ∈ {S, P, R, S+, P+, R+}
1≤m≤n Σm
where
S = S
P = P × S
R = R+ × P × S
S+ = S+
P+ = (P × S)+
R+ = (R+ × P × S)+
By this deﬁnition, the value of Σ captures two axes,
including the runtime information acquired by the
anomaly detector (axis 1) and the grouping of sys-
tem call subsequences in forming an atomic unit
(axis 2), while the value of n captures axis 3, i.e.,
the number of atomic units the anomaly detector
remembers. Intuitively, growing each of these axes
will make the automaton more sensitive to input
sequences. (In fact, it can be proven that the lan-
guage accepted by an automaton A1 is a subset of
the language accepted by automaton A2, if A1 has
a “larger” value on axis 1 or axis 3 than A2 and the
same value as A2 on the other two axes.)
Below we ﬁrst describe how a variety of prior works
ﬁt into our uniﬁed framework:
• In one of the original works in monitoring
system calls, Forrest et al. [4] implement (an
anomaly detection system equivalent to) an au-
tomaton where Σ = S and n ≥ 1 is a ﬁxed pa-
rameter that was empirically chosen as n = 5.
(For clariﬁcation on this choice, see [17].3) The
transition function δ is trained by observing the
sequence of system calls emitted by the pro-
gram in a protected environment and on a va-
riety of inputs. Speciﬁcally, if during training,
the automaton is in state q = (s1, . . . , sm) and
input s is received, then (q, s, (s1, . . . , sm, s)) is
added to δ if m < n and (q, s, (s2, . . . , sm, s)) is
added otherwise.
• Sekar et al. [16] propose coupling the system
call number with the program counter of the
process when the system call is made. (Sekar et
al. modify the usual deﬁnition of the program
counter, however, as described in Section 4.1.)
That is, Σ = P. This eﬀort considered only n =
1. As in [4], the transition function is trained
as follows: if during training, the automaton is
in state q and input σ ∈ Σ is received, then
(q, σ, q(cid:2)) is added to δ where q(cid:2) = (σ).
• Feng et al. [3] propose additionally considering
the call stack of the process when a system call
is made. When a system call is made, all return
addresses from the call stack are extracted; i.e.,
Σ = R. Again, this work considered only n =
1. If during training, the automaton is in state
q and input σ ∈ Σ is received, then (q, σ, q(cid:2)) is
added to δ where q(cid:2) = (σ).
• Wespi et al. [23, 24] suggest an anomaly de-
tection approach in which training is used to
identify a set of system call subsequences using
a pattern discovery algorithm [13]. The result
of the training is a set of variable-length sys-
tem call sequences Σ = S+. They then deﬁne
an anomaly detection system in which n = 0 (in
our parlance); i.e., for each σ ∈ Σ, (q0, σ, q0) is
added to δ.
Of the approaches above, only that of Wespi et
al. [23, 24] utilizes nondeterminism (i.e., permits
multiple active states simultaneously). All others
above could be expressed using a (deterministic)
transition function, instead.
Table 1 summarizes the prior work described above
and identiﬁes the new approaches we explore in this
paper. We emphasize that this is not necessarily a
complete list of prior work, and that we have not
captured all aspects of these prior works but rather
only those of interest here. To our knowledge, how-
ever, our analysis is the ﬁrst that covers many of the
regions in Table 1. Moreover, in certain regions that
have received attention in prior work, the analysis
has been incomplete. Notably, the analysis of Wespi
et al. [23, 24] was performed on audit log records,
not system calls, though they conjectured the tech-
nique could be applied to system call monitoring,
as well.
In such cases, our analysis here provides
new insight into the eﬀectiveness of these techniques
when applied to system call monitoring.
Finally, we remind the reader that by restricting our
analysis to approaches captured in the above model,
we do not address various “white-box” approaches
to system-call-based anomaly detection. Though we
intend to incorporate these white-box approaches
into our future analysis, our reason for precluding
them from this initial study is that they are gen-
erally more platform sensitive or require stronger
assumptions, and thus are generally less applica-
ble than gray-box approaches. For example, some
require source code (e.g., [20]) and those that do
not are platform speciﬁc. Most notably, the com-
plexity of performing static analysis on x86 binaries
is well documented. This complexity stems from
diﬃculties in code discovery and module discov-
ery [14], with numerous contributing factors, includ-
ing: variable instruction size;4 hand-coded assem-
bly routines, e.g., due to statically linked libraries,
that may not follow familiar source-level conven-
tions (e.g., that a function has a single entry point)
or use recognizable compiler idioms [15]; and indi-
rect branch instructions such as call/jmp reg32
that make it diﬃcult or impossible to identify the
target location [10, 14]. Due to these issues and oth-
ers, binary analysis/rewrite tools for the x86 plat-
form have strict restrictions on their applicable tar-
gets [9, 10, 14, 15]. As such, we have deferred con-
sideration of these techniques in our framework for
the time being.
Other omissions from our present study are system
call arguments (a topic of ongoing work) and other
paradigms that have been proposed for detecting
when a process has been commandeered via the in-
sertion of foreign code into the process address space
(e.g., program shepherding [8]).
3 Empirical study of gray-box pro-
gram tracking
The parameters Σ and n are central to the eﬀec-
tiveness of an anomaly detection system. Together
these parameters determine the states of the au-
tomaton, and thus the history information on which
the automaton “decides” that a new input σ ∈ Σ is
anomalous. Intuitively, increasing the information
in each element of Σ or n increases the number of
states of the automaton, and thus the granularity
and accuracy of anomaly detection. In this paper
we view this greater sensitivity as a beneﬁt, even
though it comes with the risk of detecting more
anomalies that are not, in fact, intrusions. How-
ever, since we restrict our attention to techniques
that ensure that any transition (triggered by sys-
n
0
1
≥ 2
S
√
√
[4]
[4]
Σ
P
√
[16]
√
R
√
√
[3]
S+
√
[23, 24]
√
P+ R+
√
√ √
√
√
√
√
Table 1: Scope of this paper (
√
) and prior work
tem call sequences) in the training data will never
result in a transition to q⊥, we simply assume that
our detectors are adequately trained and consider
this risk no further. As such, the primary costs we
consider for increasing each of these parameters are
the additional overhead for collecting information
and the size of the transition relation δ.
Our goal in this section is to provide a system-
atic analysis of the costs and beneﬁts of enhancing
these parameters. Speciﬁcally, we study the follow-
ing question: For given costs, what combination of
Σ and n is most beneﬁcial for anomaly detection?
We reiterate that as shown in Table 1, this study
introduces several new possibilities for anomaly de-
tection that, to our knowledge, have not yet been
studied.
3.1 Mimicry attacks
To understand the beneﬁts of growing Σ or n, it is
necessary to ﬁrst understand the principles behind
mimicry attacks [18, 21]. An attack that injects
code into the address space of a running process,
and then causes the process to jump to the injected
code, results in a sequence of system calls issued by
the injected code. In a mimicry attack, the injected
code is crafted so that the “attack” system calls are
embedded within a longer sequence that is consis-
tent with the program that should be running in
the process. In our model of Section 2, this simply
means that the attack issues system calls that avoid
sending the automaton to state q⊥.
There are many challenges to achieving mimicry at-
tacks. First, it is necessary for the injected code
to forge all information that is inspected by the
anomaly detector. This seems particularly diﬃ-
cult when the anomaly detector inspects the pro-
gram counter and all return addresses in the process
call stack, since the mechanics of program execu-
tion would seem to force even the injected code to
conform to the program counter and stack it forges
in order to make a system call (which must be the
same as those in the correct process to avoid de-
tection). Nevertheless, we demonstrate in Section 4
that mimicry remains possible. While we are not
concerned with the mechanics of doing so for the
present section, we do wish to analyze the impact
of monitoring program counter and return address
information on these attacks. Speciﬁcally, in order
to forge this information, the injected attack code
must incorporate the address information to forge
(possibly compressed), and so this necessarily in-
creases the size of the attack code. As such, a goal
of our analysis is to quantify the increase in size of
the attack code that results from the burden of car-
rying this extra information. We comment that this
size increase can impose upon the viability of the
attack, since the area in which the injected code is
written is typically bounded and relatively small.
A second challenge to achieving a mimicry attack is
that a step of the attack may drive the automaton
to a state that requires a long sequence of interven-
ing system calls to reach the next system call in the
attack, or that even makes reaching the next system
call (undetected) impossible. In general, enhancing
Σ or growing n increases this challenge for the at-
tacker, as it increases the granularity of the state
space. This must be weighed against the increased
size of the automaton, however, as well as the ad-
ditional run-time costs to extract the information
dictated by Σ. A second aspect of our analysis in
this section is to explore these tradeoﬀs, particularly
with an eye toward |δ| as the measure of automaton
size.
3.2 Analysis
In order to analyze the costs and beneﬁts of enhanc-
ing the axes of the state space, we set up a testbed
anomaly detection system. The system is imple-
mented as a kernel patch on a Red Hat Linux plat-
form, with conﬁguration options for diﬀerent val-
ues of Σ and n. We implement the variable-length
pattern approach as described in [13, 24] for each
Σ ∈ {S+, P+, R+}. We have chosen four com-
mon FTP and HTTP server programs, wu-ftpd,
proftpd, Apache httpd, and Apache httpd with
a chroot patch, for evaluation purposes. Automata
for these four programs (and diﬀerent conﬁgurations
of the axes) are obtained by training the anomaly
detection system with between four and nine million
of system calls generated from test runs. After ob-
taining the automata, we perform analysis to evalu-
ate the costs and beneﬁts of diﬀerent conﬁgurations
of Σ and n. Figures 1 and 2 show the results when
Σ ∈ {S, P, R} and Σ ∈ {S+, P+, R+}, respectively.
That is, Figures 1 and 2 correspond to the two pos-
sible instantiations of axis 2 in Section 2.
3.2.1 Resilience against mimicry attacks
The ﬁrst three columns of Figures 1 and 2 are about
resilience against mimicry attacks. The attack we
test is the addition of a backdoor root account into
the password ﬁle. This common attack needs to
perform a series of six system calls (chroot, chdir,
chroot, open, write, close), which is similar to
the attack sequence discussed in [21]. However, in
the case of Apache httpd only three system calls
are needed (open, write, close). We choose to
analyze this attack sequence because it is one of the
most commonly used system call sequences in an
attack. Many attacks need to make system calls
that constitute a superset of this sequence.
We perform an exhaustive search to ﬁnd the shortest
sequence containing the above series of system calls,
not necessarily contiguously, that avoids detection.5
The exhaustive search reveals the best an attacker
can do to evade detection when making the attack
system calls. Graphs on the ﬁrst column show the
minimum number of system calls a mimicry attack
must make in order to evade detection.
(Missing
data points on the graphs indicate that the mimicry
attack is not possible.) For example in the case of
Apache httpd with chroot patch, the mimicry at-