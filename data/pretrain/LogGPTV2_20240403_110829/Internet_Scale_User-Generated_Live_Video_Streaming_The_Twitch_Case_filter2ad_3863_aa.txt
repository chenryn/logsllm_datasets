title:Internet Scale User-Generated Live Video Streaming: The Twitch Case
author:Jie Deng and
Gareth Tyson and
F&apos;elix Cuadrado and
Steve Uhlig
Internet Scale User-Generated Live Video
Streaming: The Twitch Case
Jie Deng(B), Gareth Tyson, Felix Cuadrado, and Steve Uhlig
{j.deng,gareth.tyson,felix.cuadrado,steve.uhlig}@qmul.ac.uk
Queen Mary University of London, London, UK
Abstract. Twitch is a live video streaming platform used for broad-
casting video gameplay, ranging from amateur players to eSports tour-
naments. This platform has gathered a substantial world wide commu-
nity, reaching more than 1.7 million broadcasters and 100 million visitors
every month. Twitch is fundamentally diﬀerent from “static” content
distribution platforms such as YouTube and Netﬂix, as streams are gen-
erated and consumed in real time. In this paper, we explore the Twitch
infrastructure to understand how it manages live streaming delivery to
an Internet-wide audience. We found Twitch manages a geo-distributed
infrastructure, with presence in four continents. Our ﬁndings show that
Twitch dynamically allocates servers to channels depending on their pop-
ularity. Additionally, we explore the redirection strategy of clients to
servers depending on their region and the speciﬁc channel.
Keywords: Twitch.tv · Live video streaming · Video streaming
infrastructure
1 Introduction
Online live streaming has long been a popular application. However, recently,
there has been an interesting evolution, whereby everyday users provide streams
of their own activities, e.g., Facebook Live, Periscope [18], Meerkat. This
is termed user-generated live streaming, and unlike other platforms (e.g.,
YouTube [14,15] and Netﬂix [7,9]), often involves things like live social inter-
action. Thus, these platforms introduce two core innovations: (i) Any user can
provide a personal live stream (potentially to millions of viewers); and (ii) This
upload must occur in realtime due to live social interaction between consumers
and producers. One of the most popular examples of this is Twitch [3,21]. This
live broadcast platform is oriented towards video games, allowing users to broad-
cast their gameplay, as well as to watch large eSports tournaments with profes-
sional players. Though others have started similar services (e.g., YouTube Gam-
ing), they are yet to experience the demand of Twitch [17,18], which delivered
35 K streams to over 2 million concurrent users in real time during its peak [5].
The rapid expansion of user-generated live streaming platforms, like Twitch,
comes with fundamental challenges for the management of infrastructure and
c(cid:2) Springer International Publishing AG 2017
M.A. Kaafar et al. (Eds.): PAM 2017, LNCS 10176, pp. 60–71, 2017.
DOI: 10.1007/978-3-319-54328-4 5
Internet Scale User-Generated Live Video Streaming
61
traﬃc delivery.1 For example, in Twitch it is impossible to time-shift (cache)
video content, and often uploaders are not geographically near or well connected
to their subscribers. Further, live social interaction (e.g., via web cams and chat
feeds [16]) means that the real-time constraints are very strict. Thus, we argue
that Twitch might oﬀer some important insights into how such challenges can
be overcome.
In this paper, we perform a large-scale measurement study of Twitch. Taking
advantage of a global network of proxy servers, we map the infrastructure used
by Twitch. We explore its content replication and server selection strategies, cor-
relating them with both viewer and broadcaster location. Note that broadcaster
selection is a unique aspect of personalised video streaming, as prior systems
lack the concept of user-generated live broadcasters. In this paper, we analyse
how Twitch has managed to scale-up to deal with its huge demand. In summary,
we make the following contributions:
– We map the infrastructure and internetworking of Twitch. Unlike YouTube
or Netﬂix which deploy thousands of caches in edge networks, Twitch serves
millions of users directly from relatively few server locations in North America
(NA), Europe (EU) and Asia (AS) (Sect. 3).
– Based on this, we expose how streams are hosted by Twitch at diﬀerent loca-
tions (Sect. 4); we explore how Twitch scales-up depending on channel popu-
larity, and how clients are redirected to Twitch servers.
– We evaluate the client redirection strategy (Sect. 5) on a global scale. We ﬁnd
multiple factors aﬀecting the redirection policy, including channel popularity
and the client network conﬁguration (peering). Due to the lack of peering in
Asia, 50% of the clients are exclusively served by NA servers.
2 Measurement Methodology
We begin by presenting our measurement methodology, which is driven by three
goals. First, we wish to discover the location and number of servers in Twitch’s
infrastructure. Second, we want to know how Twitch allocates individual live
streams onto these severs (note that this is a very diﬀerent model to static video
content, which is usually reactively cached wherever it is requested). Third, we
want to understand how users are mapped to servers so that they can watch the
stream they are interested in.
We built a Python crawler that allows us to automatically request video
streams from Twitch channels. The responses to these requests allow us to inspect
which server the client has been redirected to.2 In order to comprehensively sam-
ple the infrastructure, and explore how diﬀerent clients are redirected to Twitch
servers, we ran this crawler in many geographic locations to achieve global cov-
erage of Twitch’s infrastructure. To achieve this, we utilised a global network of
1 Note that Twitch is the fourth largest source of peak traﬃc in the US [4].
2 We distinguish unique servers based on their IP address — we note that each IP
address is also allocated a unique domain name.
62
J. Deng et al.
open HTTP proxies3 to launch the video requests from around the world. We
validated that the client IP address exposed to the server is the proxy address,
thus we can expect the Twitch server to redirect based on the proxy location.
In total, we routed through 806 proxies, from 287 ASes located in 50 countries
from Europe (154), Asia (372), Africa (24), Australia (4), North America (138)
and South America (114). Though there are several limitations with using open
proxies (e.g., unevenly distributed locations and no accurate feedback of the video
streaming latency), we argue that the proxy platform provides suﬃcient informa-
tion on Twitch infrastructure at scale.
We observed that Twitch frequently redirects a client to diﬀerent servers
when requesting the same channel multiple times, thus evidencing some mech-
anism of load balancing. For each channel we sent the request multiple times
from each proxy in order to comprehensively sample the servers oﬀered from
that location. Each channel was requested a variable number of times (from 15
to 300) based on how many unique servers our queries discovered. We ﬁrst ran
the crawler for 5 months from December 2015 to April 2016. We continuously
launched requests to all online channels listed from public Twitch API,4 and
collected over 700 K requests indicating the Twitch servers that clients in that
region are redirected to.
Once we acquired the list of Twitch servers, we began to explore the strategy
that maps streams onto servers. First, we requested all online channels via proxy
servers in the countries in which Twitch servers are located; also each channel
was requested multiple times to discover as many servers hosting the stream
as possible. Second, we carried out the same experiment for around 30 selected
popular channels every 5 min. This was done to observe how the most popular
channels are managed over an extended period of time. A total of 1 m requests
were collected from these two experiments.
Finally, to further understand Twitch’s client redirection strategy on a global
scale, we also requested all online channels through all proxies one-by-one. We
then captured which server each proxy is redirected to. For each proxy, we
requested the channels only once to emulate a typical client. This resulted in
a further 1 m requests collected between April to June 2016.
3 Geographic Deployment of Twitch Infrastructure
We start the exploration of Twitch’s infrastructure by describing the locations of
its servers, as well as how they are connected to the Internet. Our logs show that
all Twitch video streams are served from hls.ttvnw.net subdomains. Each domain
consists of a server name with an airport code, hinting at a geographical location.
For example, video11.fra01.hls.ttvnw.net is a server in Frankfurt (fra), Germany.
We conﬁrmed that there is a one-to-one mapping between each domain and an
IP address by performing global DNS queries from locations around the world.
3 These are servers that allow us to proxy web requests through them, thereby appear-
ing as it our requests come from them: https://incloak.com/.
4 https://github.com/justintv/Twitch-API.
Internet Scale User-Generated Live Video Streaming
63
In total, we discovered 876 servers distributed over 21 airport code subdomains
from 12 countries.
It is unclear how accurate these location-embedded domains are and, there-
fore, we compare the airport codes against the locations returned by three IP
geodatabases: ipinfo.io, DP-IP and Maxmind GeoLiteCity. Although the airport
locations embedded within the domains are always in the same continent, we
note that they are inconsistent with the locations returned from the databases.
Instead, the geodatabases report that Twitch operates a centralised infrastruc-
ture. All servers were mapped to just 4 countries: Switzerland (Europe),
Hong Kong (Asia), US (North America) and Sydney (Oceania). In total, our
traces reveal 360 servers in the North America (NA), 257 servers in Europe (EU),
119 in Asia (AS) and 47 in Oceania (OC).
To explore the discrepancy between the databases and airport codes, we
performed a TCP-based traceroute and ping campaign from 10 sites in East and
West US, Europe, Asia Paciﬁc and South America. From the traceroute path we
see that servers sharing a preﬁx also pass through the same router when entering
Twitch’s AS, with only the last three hops diﬀering. This, however, does not
conﬁrm physical locations. Hence, we also check the Round Trip Time (RTT) to
each server using TCP ping. This shows a clear boundary between servers with
diﬀerent airport codes. Servers inside the same sub-domains tend to diﬀer by
under 5 ms; for servers on the same continent, the diﬀerence is within 50 ms; for
servers on diﬀerent continents, this increases beyond 100 ms. We found a minimal
RTT of under 3 ms when accessing servers sharing the same country code. This
suggests that the airport country codes are a good indicator of physical location.
In other words, this highlights inaccuracy in the geolocation databases (this is
perhaps reasonable, as geodatabases are well known to suﬀer limitations such as
address registration [10]).
We gain additional conﬁdence in our ﬁndings by checking the BGP routing
tables.5 Unlike other large content providers, we fail to ﬁnd any third party host-
ing, as seen in other larger CDNs like Google [10] or Netﬂix. Instead, all servers
are located within Twitch’s own Autonomous System (AS46489). Importantly,
we ﬁnd the preﬁxes are only announced in their appropriate continents. For
example, 185.42.204.0/22 is only announced in Europe and 45.113.128.0/22 is
only announced in Asia. Thus, we are conﬁdent that the geolocations are at least
accurate on a continent-level granularity
Finally, to dig deeper into the BGP interconnectivity of Twitch’s AS, we
utilise PeeringDB [2] to extract the locations of advertised public and private
peering facilities used by the 153 Twitch peers listed in [1]. Figure 1 presents the
number of potential peers that are collocated with Twitch in Internet Exchange
Points (IXPs) and private peering facilities. Unsurprisingly, we ﬁnd a tendency
for more peering in countries where we also discover Twitch servers. For example,
most of the potential peerings are located in IXPs in the Netherlands (AMS-IX),
US (Equinix), UK (LONAP) and Germany (DE-CIX Frankfurt). Noteworthy is
that the number of potential peerings in Asia is actually quite small, with the
5 http://routeserver.org/.
64
J. Deng et al.
bulk in America and Europe (we acknowledge this could be caused by inaccu-
racies in PeeringDB). We ﬁnd from BGP route records6 that the IP preﬁx for
the Asia presence was ﬁrst advertised in June 2015. This recency could explain
the low number of peers. The same is for Oceania, which ﬁrst was advertised
in November 2015. The low number of peers could aﬀect the performance in
redirection, as we will illustrate later in Sect. 5.
 120
 100
 80
 60
 40
 20
 0
IXP
Private peering
N
G
D
S
F
C
P
H
L
B
E
E
R
Z
L
K
J
P
S
A
U
G
U
S
Fig. 1. Number of peers collocated with Twitch AS46489 at Internet Exchange Points
and private peering facilities in each country (from PeeringDB). There is more peering
in countries where Twitch servers are based.
The above results only allow us to deﬁnitively state that geolocations are
accurate on a per-continent basis. Hence, for the rest of this paper, we focus our
analysis on continent-level geolocation; where countries are mentioned, we use
airport codes as the ground truth. Due to the low utilisation of Oceania servers,
we will mainly focus on NA, EU and AS in the following sections.
4 Stream Hosting Strategy
The previous section has explored the location of Twitch’s infrastructure. How-
ever, this says little about how it is used to serve its dynamic workload. Next,
we look at how streams are allocated to Twitch’s servers.
4.1 How Important Is Channel Popularity?
We ﬁrst look at the number of servers a channel is hosted on, based on how many
viewers it receives (i.e., popularity). It might be expected that the number of
servers hosting a channel scales linearly with the number of viewers. However,
we ﬁnd this is not the case for Twitch. Figure 2 presents the number of servers
6 https://stat.ripe.net/.
Internet Scale User-Generated Live Video Streaming
65
 100
 10
s
r
e
v
r
e
s
f
o
#
l
a
t
o
T
 1
 1
 1
0
 1
0
0
Current viewers
 1
0
0
0
 1
0
0
0
0
Fig. 2. Number of unique servers hosting each channel (found using requests from
multiple vantage points all over the world) against number of current viewers. Channels
with high view counts are replicated on a larger number of servers.
hosting a channel against the instant number of viewers per channel. Live viewer
ﬁgures are acquired from the Twitch API. Although there is an upward trend, it
is not that distinct (highest correlation is just 0.41). We also explored the total
number of viewers (accumulated viewers over time), however the correlation with
number of servers was not higher.
The low correlation suggests a more sophisticated methodology is used to
manage the scaling — it is not solely based on the number of viewers. To under-
stand this better, we take a temporal perspective to see how the number of
servers utilised for a channel evolves over time. We manually selected 30 popular
streamers from diﬀerent countries and repeatedly requested their channels every
5 min from the proxies.
Figure 3 presents example results from a US streamer and a Chinese streamer.
Both channels have an initial allocation of 3 servers when they start the stream-
ing session. As more viewers join, the popularity is followed by an increase in the
number of servers provisioned by Twitch. The ﬁgure also shows how drops in
viewing ﬁgures are accompanied by a decrease in the number of servers. When
looking at the number of servers per continent, it can be seen that the capacity
is adjusted independently per region, with the Chinese streamer having only
3 instances in Europe and America. Again, this conﬁrms that Twitch scales
dynamically the number of servers allocated to a channel, depending on the view
count. Moreover, it indicates that each region is scaled independently based on
the number of viewers in that region.
4.2 Scaling of Servers Across Continents
The previous section shows that the number of servers hosting the channel is
correlated with the number of viewers watching the channel per region. We
next investigate how the scaling works across continents. Figure 4 presents the
fraction of servers found in each continent for each channel (based on its number
66
J. Deng et al.
s
r
e
v
r
e
s
f
o
r
e
b
m
u
N
l
3
e
u
b
h
g
n
t
i
r
o
f
s
r
e
v
r
e
s
f
o
r
e
b
m
u
N
0
e
b
3
g
g
e
n
o
i
t
d
o
g
a
s
a