0
0
0
0
0
0
0
0
0
0
0
10/24
31/33
20/24
18/22
18/31
8/19
29/36
29/33
20/27
17/22
7/10
15/17
8/13
8/41
18/32
20/24
38/44
16/22
36/33
9/22
9/30
C
NA
NA
0
0
NA
0
0
NA
NA
0
0
0
NA
0
0
NA
0
0
0
NA
discovers a higher number of response categories compared to the
other fuzzers (as detailed in Section 5.3).
Null pointer dereferences. As shown in Table 3, the 7 crashes
triggered by Snipuzz in TP-Link HS110 and HS100 are all caused
by null pointer dereferences. After sending the test cases to HS110
and HS100, the devices crashed, unable to reply to any interaction.
However, after a few minutes, the devices automatically restarted
and recovered to the initial state. Based on the analysis of test cases,
we found that the vulnerabilities are all triggered by messages that
mutated in JSON syntax. Concretely, when some important place-
holders, such as curly braces and colons, or a part of the test message
are mutated, the syntax structure and the semantic meaning of the
message are broken. If the device cannot handle the mutated input
message properly, it will crash the device. We reported the vul-
nerabilities to the device vendor, TP-Link, via email on June 13,
2020. They have confirmed the vulnerability and promised to fix it
through a firmware update.
Denial of service. Another interesting finding is the denial of
service vulnerability detected in Philips A60 smart bulb. After being
tested by Snipuzz for 24 hours, Philips’ official companion app could
not manage the device normally. Specifically, the device cannot be
found in the app and if any further messages are sent through the
app, the response in the app will keep asking to bound the device to
a device group and no further interaction is available. However, we
observe that if the message packet is sent directly to the device, the
device can work normally. This indicates that the device does not
completely crash but its service via the companion app is denied.
Unknown crashes. Snipuzz found 5 crashes on Yeelight bulbs,
YLDP05YL, and YLDP13YL. The devices crashed and then restarted
by themselves within roughly 1 minute. By analyzing the test cases,
we found that the crashes are due to the deletion of certain data
domains, such as the nullify of parameters (marked as red in Table 4).
As the firmware of the two devices is not publicly available, the
root cause of the vulnerability cannot be determined; however, we
can still deduce that the vulnerability is due to the device reading
in null values during the parsing process, causing a crash during
the assignment. We also find that its communication using a local
Table 4: Mutated messages of Snipuzz & IoTFuzzer.
Contents of mutated messages
{"{"id": 0, "method": "start_cf", "params": ["4, 4, "1000,
2, 2700,100,500 ,1,255,10,5000,7,0,0,500,2,5000,1"]}"
{"{"id": 0, "method": "start_cf", "params": ["4, , "1000,
2, 2700,100,500 ,1,255,10,5000,7,0,0,500,2,5000,1"]}"
{"{"id": 0, "method": "start_cf", "params": [", 4, "1000,
2, 270000,100,500 ,1,255,10,5000,7,0,0,500,2,5000,1"]}"
Generated by
Original Message
Snipuzz
IoTFuzzer
network does not require any authentication, which means that
the device can be crashed by any attackers in the local network.
Therefore, we consider the vulnerabilities as ‘remotely exploitable’.
5.2.2 Benchmark with state-of-the-art tools. As shown in Table 3,
after 24 hours fuzz testing on each device, none of the benchmark
tools found a crash, except IoTFuzzer. They did not find the crash
due to various reasons. Donna focuses more on the mutation of
communication protocols. Further, Donna cannot be applied on all
devices, which limits its capacity. Since Boofuzz directly replaces
the specified positions in the message with a preset string, it can
only trigger limited types of vulnerabilities. Nemesys offers a new
idea of determining message snippets. However, since it determines
message snippets by the distribution of values in messages, it is
difficult for Nemesys to accurately decide the boundary between
data and non-data domains. Therefore, Nemesys can hardly detect
vulnerabilities that can only be triggered by mutating the data or
non-data domains. Snipuzz-NoSnippet, which does not apply the
snippet-based mutation method used in Snipuzz, is similar to the
classic fuzzer AFL [24]. Since Snipuzz-NoSnippet does not infer
the structure of the message but directly uses single or multiple con-
secutive bytes as the unit of mutation, most test cases generated by
Snipuzz-NoSnippet destroy the structure of the messages. Such a
method is difficult to work on devices that require highly-structured
inputs.
IoTFuzzer detected 2 crashes in 2 smart bulb devices, i.e., the
YLDP05Y and YLDP013Y. Due to the mutation strategy of IoT-
Fuzzer, the malformed input provided by IoTFuzzer is obtained by
emptying the data domain. According to the examples of mutated
Session 2A: Fuzzing and Bug Finding CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea345messages listed in Table 4, we can see that the messages mutated by
IoTFuzzer resemble the ones generated by Snipuzz. The mutated
domains of messages from Snipuzz and IoTFuzzer in Table 4 are
all in the data domain. In terms of the mutation test effectiveness,
Snipuzz and IoTFuzzer achieve the same goal on these two exam-
ples. However, Snipuzz can cover the mutation space of IoTFuzzer
because IoTFuzzer only focuses on the data domain mutation,
while Snipuzz can mutate both the data and non-data domains.
To further determine the root cause of the crash, we obtained
the firmware source code of HS100 and HS110, two typical mar-
ket consumer-grade smart plugs manufactured by TP-Link, and
conducted a case study which reflected the differences between
Snipuzz and IoTFuzzer. We found that one of the crashes triggered
by Snipuzz on the two devices is caused by breaking the syntax
structure and mutating both on data and non-data domains. More
specifically, the mutated messages successfully bypassed the sani-
tizer and triggered the crash during function execution. We deduce
that this could be caused by an error-prone third-party sanitizer
(more details could be found in Section 5.5). On the other hand, due
to the design of IoTFuzzer, the fuzzing is based on the grammatical
rules as the IoTFuzzer tends to satisfy the grammar requirements
with first-priority, in order not to be rejected by the sanitizer and
ensure that each test case can reach the functional execution part
in the firmware. Such strategy constraints the test range of fuzzing
and its capacity to cover the sanitization part in comparison to
Snipuzz. Therefore, we argue that, considering the complexity of
IoT firmware testing, a lightweight and effective black-box vulner-
ability detection tool, such as Snipuzz, is a pressing need.
5.3 Runtime Performance
Figure 5 shows how Snipuzz and the other 7 benchmark fuzzers
explored the device firmware during the first 10 minutes. We
repeated the fuzz testing for 10 times and recorded the median
values of the numbers of response categories discovered by each
method. We manually reviewed the response categories to remove
the mis-categorization caused by randomness in responses or the
response mechanism of devices.
As shown in Figure 5, Doona can only detect a small number of
response categories. Doona is protocol-based fuzzing methods, and
its tests are more biased towards protocol content. The mutation
test on the communication protocol has a high probability of being
directly rejected or ignored by the device unilaterally, resulting in
few categories of responses that can be pass the sanitizer.
We implemented 3 fuzzing strategies based on Boofuzz, i.e.,
mutating the whole message as a string, mutating each byte of
the message, and mutating non-data domain. However, the testing
results indicate that all of them explored very limited categories
of responses on each device. The limitation of category coverage
is due to the mutation strategy of Boofuzz, which replaces the
target contents with a specific pre-defined string. For example, us-
ing strings, such as “/./././././././.”, to replace the content of
messages with different strategies (e.g., replacing the entire strings,
a single byte, or a non-data domain) may cause the violation of mes-
sage format and could be easily rejected by the sanitizer. Therefore,
most of the responses obtained by Boofuzz fall into the category
of “error responses”.
The number of response categories explored by IotFuzzer grows
rapidly within a short period of time and then stagnates. In the
mutation stage, IotFuzzer randomly selects a set of inputs from the
original candidate inputs and randomly mutates the data domain
for one or more message(s). It will continue to repeat this method
until the device crashes or reaches the time limit. Such a method
based on randomness helps IotFuzzer to mutate and test a large
number of message data domains in the original input and collect
response message categories quickly in the beginning. However,
the number of response categories found by IotFuzzer will soon
reaches the limitation as it focuses on data domain mutation only.
In most devices, Snipuzz has maintained a steady upward trend
in most cases, and after a period of time, the number of response
categories found by Snipuzz exceeds IotFuzzer. Unlike IotFuzzer,
Snipuzz covers response categories through the Snippet Deter-
mination stage. Based on the message snippet exploration strat-
egy, Snipuzz first explores all the response categories of a certain
message as many as possible, then the snippets of a message are ob-
tained and tested by Snippet Mutation. The next message will be
processed in the same way until all messages in the initial message
sequence have been tested. Followed by this method, Snipuzz may
not get a large number of response categories at the very beginning.
When Snipuzz detects a message snippet, every byte in the message
content will be included in the test. Therefore, as shown by the
bold numbers in Table 3, for 15 out of 20 devices, Snipuzz covers
the most number of response categories after 24-hour fuzz testing,
compared with other state-of-the-art IoT fuzzing tools.
On 5 devices, Snipuzz-NoSnippet collected more response cate-
gories than Snipuzz within 24 hours. The mutation method used
by Snipuzz-NoSnippet is similar to the classic fuzzer AFL [24]. It
directly performs mutation on a single byte or several consecutive
bytes. However, Snipuzz-NoSnippet is difficult to cover response
categories that are not obtained by breaking the grammatical format
(e.g., data out of bounds in the data domain). Theoretically, although
the Snipuzz-NoSnippet mutation method is not so efficient, it still
has the capability to explore the most categories of responses.
Nemesys explores more categories of responses than BooFuzz
and Doona, but does not exceed Snipuzz. The Nemesys strategy
performs deterministic mutations on each data domain of the mes-
sages in turn, which makes its trend of run-time performance simi-
lar to Snipuzz. However, the data domain determination strategy
of Nemesys is not based on the responses from IoT device. Thus,
the distribution of byte values in messages does not benefit in cov-
ering more response categories. Therefore, the number of response
categories collected by Nemesys is limited.
It is interesting to observe that, in the case of R6400, Snipuzz also
enters a stagnation after only finding a few response categories. We
carefully checked the initial input message sequences and found
that the average length of the message exceeds 400 bytes, forcing
Snipuzz to generate and send a large number of probe messages
to determine message snippets. As a result, in the first 10 minutes,
Snipuzz was still exploring the response category of the first few
messages, which limited its performance.
Session 2A: Fuzzing and Bug Finding CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea346Figure 5: Runtime performance. The number of categories discovered in 10 minutes on all the 20 IoT devices. Snipuzz per-
formed best on 19 devices.
5.4 Assessment on Message Snippet Inference
Among all strategies, Snipuzz and Nemesys utilize semantic seg-
mentation, to assess their message snippet inference performance.
We compare the snippets they produce during the fuzzing process
with the grammar rules defined in API documents. Specifically, for
some mature and popular languages, such as JSON, we establish
the grammar rules as per their standard syntax; for custom for-
mats, such as strings or custom bytes, we refer to the official API
documents and define the grammar rules based on the instructions.
Equation (2) quantifies the quality of snippet inference, and
Similarity indicates the percentage of correctly categorized bytes
in a snippet-determined message, 𝑚, compared with the ground
truth, 𝑔, manually extracted from the grammar rules.
𝑆𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦(𝑚) = 1 − 𝑐𝑜𝑢𝑛𝑡[𝑐𝑎𝑡𝑒(𝑚) ⊕ 𝑐𝑎𝑡𝑒(𝑔)]
(2)
where 𝑐𝑎𝑡𝑒() returns the category of each message byte in a series
of “0” and “1” bits, 𝑐𝑜𝑢𝑛𝑡() counts the number of mis-categorized
bytes, and 𝑙𝑒𝑛() represents the length of a message. Note that in a
ground truth message, “0” indicates the non-data domain (marked
blue in Table 5), while “1” indicates the data domain (marked red
in Table 5). Therefore, the ⊕ is the bitwise 𝑋𝑂𝑅 operation.
𝑙𝑒𝑛(𝑚)
In addition, followed by Equation (2), we compute the average
similarity of the snippets (or data domain) determined by Snipuzz
and Nemesys for all the 235 messages obtained from experiments.
Note that during the calculation of the average similarity, for each
Table 5: Inference results of Snipuzz and Nemesys.
Method
Snipuzz
Nemesys
Ground Truth
Ave. Similarity Example
87.1% {"on":true,"sta":140,"bri":254}
64.5% {"on":true,"sta":140,"bri:"254}
100.0% {"on":true,"sta":140,"bri":254}
message, if there are multiple snippet sets determined, we will select
the snippet inference with the highest similarity value; therefore
we present an upper-bound of performance.
The average similarity result of Snipuzz, 87.1%, indicates that,
by applying snippet inference based on the hierarchical cluster-
ing approach, Snipuzz can effectively find the grammatical rules
hidden in the message. Ideally, in Snipuzz, the merging of clusters
removes the influence caused by the randomness in responses and
by the replying message mechanism itself. Therefore, the message
snippets will conform to the grammatical rules gradually, which
leads Snipuzz to a higher similarity result.
However, we also found some differences between the snippet
inference method and the grammatical rules in some results. For
example, given the example shown in Table 5, the snippet inference
method combines the strings belonging to the data domain in the
grammatical rules (i.e., ‘true’, ‘140’ and ‘254’) with some placehold-
ers (such as double quotes and curly brackets). After analyzing
the response messages, we found that the responses obtained after
destroying these data domains and destroying placeholders are all
about invalid format. This may due to the fact that in the firmware,
when an error occurs in the parsing format, the response does
,
Session 2A: Fuzzing and Bug Finding CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea347Figure 6: An example of vulnerability triggering
not report a detailed description of the error but instead returns a
general format error.
On the other hand, Nemesys uses the distribution of value changes
in the protocol to determine the boundary of different data domains
and achieve the semantic segmentation of a message. The advan-
tage of this method is that it does not require any other additional
information, such as grammar rules or a large number of training
data sets in addition to the message itself.
The average similarity result of Nemesys, 64.5%, is lower than
the Snipuzz result. Given the example shown in Table 5, when
segmenting messages in a format requires restricted syntax, such as
Json and XML, Nemesys can achieve a good semantic segmentation
performance, because the placeholders usually use symbols unusu-
ally used in data domains. This distribution of byte value enables
Nemesys to effectively find the boundaries between data domains.
However, in IoT devices, customized formats are prevalent. For
example, the smart bulb BR30 uses custom bytes as a means of com-
munication, where each byte corresponds to a special meaning (i.e.,
“0x61” represents “CHANGE_MODE” and “0x0f” represents “TRUE”).
In such cases, the value distribution of characters can no longer be
used as a guidance for the data domain determination, and thus the
message segmentation determined by Nemesys is error-prone.
5.5 Mutation Effectiveness: A Case Study
The HS100 and HS110 manufactured by TP-Link are two classic
market consumer-grade smart plugs. In the work by Chen et al. [9],
they use HS110 with firmware version 1.3.1 to test IoTFuzzer.