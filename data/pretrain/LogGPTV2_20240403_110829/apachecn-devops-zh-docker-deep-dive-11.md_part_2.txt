> docker network create -d nat localnet 
```
 `新网络被创建，并将出现在任何未来`docker network ls`命令的输出中。如果你使用的是 Linux，你还会在内核中创建一个新的 *Linux 桥*。
让我们使用 Linux `brctl`工具来看看当前系统上的 Linux 桥。你可能需要使用`apt-get install bridge-utils`或者你的 Linux 发行版的等效软件来手动安装`brctl`二进制文件。
```
$ brctl show
bridge name       bridge id             STP enabled    interfaces
docker0           8000.0242aff9eb4f     no
br-20c2e8ae4bbb   8000.02429636237c     no 
```
 `输出显示了两个桥。第一行是我们已经知道的“docker0”桥。这与 Docker 中的默认“桥”网络有关。第二桥(br-20c2e8ae4bbb)涉及新的`localnet` Docker 桥网络。它们都没有启用生成树，也没有连接任何设备(`interfaces`列)。
此时，主机上的网桥配置如图 11.9 所示。
![Figure 11.9](img/figure11-9.png)
Figure 11.9
让我们创建一个新的容器，并将其连接到新的`localnet`桥网络。如果你在 Windows 上跟随，你应该用“`mcr.microsoft.com/powershell:nanoserver pwsh.exe -Command Start-Sleep 86400`”代替“`alpine sleep 1d`”。
```
$ docker container run -d --name c1 \
  --network localnet \
  alpine sleep 1d 
```
 `这个容器现在将在`localnet`网络上。你可以用`docker network inspect`确认这一点。
```
$ docker network inspect localnet --format '{{json .Containers}}'
{
  "4edcbd...842c3aa": {
    "Name": "c1",
    "EndpointID": "43a13b...3219b8c13",
    "MacAddress": "02:42:ac:14:00:02",
    "IPv4Address": "172.20.0.2/16",
    "IPv6Address": ""
    }
}, 
```
 `输出显示新的“c1”容器在`localnet`桥/nat 网络上。
如果你再次运行 Linux `brctl show`命令，你会看到 c1 的接口连接到`br-20c2e8ae4bbb`桥。
```
$ brctl show
bridge name       bridge id           STP enabled     interfaces
br-20c2e8ae4bbb   8000.02429636237c   no              vethe792ac0
docker0           8000.0242aff9eb4f   no 
```
 `这如图 11.10 所示。
![Figure 11.10](img/figure11-10.png)
Figure 11.10
如果我们向同一个网络添加另一个新容器，它应该能够通过名称 ping 通“c1”容器。这是因为所有新容器都自动向嵌入式 Docker DNS 服务注册，使它们能够解析同一网络上所有其他容器的名称。
> **小心:**Linux 上默认的`bridge`网络不支持通过 Docker DNS 服务进行名称解析。所有其他*用户定义的*桥接网络都可以。由于容器位于用户定义的`localnet`网络上，下面的演示将会起作用。
我们来测试一下。
1.  Create a new interactive container called “c2” and put it on the same `localnet` network as “c1”.
    ```
     //Linux
     $ docker container run -it --name c2 \
       --network localnet \
       alpine sh
     //Windows
     > docker container run -it --name c2 `
       --network localnet `
       mcr.microsoft.com/powershell:nanoserver 
    ```
     `您的终端将切换到“c2”容器。` 
`*   From within the “c2” container, ping the “c1” container by name.
    ```
     > ping c1
     Pinging c1 [172.26.137.130] with 32 bytes of data:
     Reply from 172.26.137.130: bytes=32 time=1ms TTL=128
     Reply from 172.26.137.130: bytes=32 time=1ms TTL=128
     Control-C 
    ```
     `有效！这是因为 c2 容器正在运行本地 DNS 解析器，该解析器将请求转发到内部 Docker DNS 服务器。该域名系统服务器维护所有以`--name`或`--net-alias`标志开始的容器的映射。`` 
 ``请尝试在您仍登录到容器时运行一些与网络相关的命令。这是了解更多 Docker 容器网络工作原理的好方法。下面的代码片段显示了从先前创建的“c2”窗口容器中运行的`ipconfig`命令。您可以从容器中取出`Ctrl+P+Q`并运行另一个`docker network inspect localnet`命令来匹配 IP 地址。
```
PS C:\> ipconfig
Windows IP Configuration
Ethernet adapter Ethernet:
   Connection-specific DNS Suffix  . :
   Link-local IPv6 Address . . . . . : fe80::14d1:10c8:f3dc:2eb3%4
   IPv4 Address. . . . . . . . . . . : 172.26.135.0
   Subnet Mask . . . . . . . . . . . : 255.255.240.0
   Default Gateway . . . . . . . . . : 172.26.128.1 
```
 `到目前为止，我们已经说过桥接网络上的容器只能与同一网络上的其他容器通信。但是，您可以使用*端口映射*来解决这个问题。
端口映射允许您将容器映射到 Docker 主机上的端口。任何到达配置端口上的 Docker 主机的流量都将被定向到容器。高级流程如图 11.11 所示
![Figure 11.11](img/figure11-11.png)
Figure 11.11
在图中，运行在容器中的应用正在端口`80`上运行。这被映射到主机的`10.0.0.15`接口上的端口`5000`。最终结果是在`10.0.0.15:5000`上到达主机的所有流量被重定向到端口 80 上的容器。
让我们来看一个例子，将运行 web 服务器的容器上的端口 80 映射到 Docker 主机上的端口 5000。该示例将在 Linux 上使用 NGINX。如果你在 Windows 上继续，你需要用一个基于 Windows 的网络服务器映像代替`nginx`，比如`mcr.microsoft.com/windows/servercore/iis:nanoserver`。
1.  运行一个新的 web 服务器容器，并将容器上的端口 80 映射到 Docker 主机上的端口 5000。
    ```
     $ docker container run -d --name web \
       --network localnet \
       --publish 5000:80 \
       nginx 
    ```
`*   Verify the port mapping.
    ```
     $ docker port web
     80/tcp -> 0.0.0.0:5000 
    ```
     `这表明容器中的端口 80 被映射到 Docker 主机上所有接口上的端口 5000。` `*   Test the configuration by pointing a web browser to port 5000 on the Docker host. To complete this step, you’ll need to know the IP or DNS name of your Docker host. If you’re using Docker Desktop on Mac or Windows, you’ll be able to use `localhost:5000` or `127.0.0.1:5000`.
    ![Figure 11.12](img/figure11-12.png)
    Figure 11.12
    现在，任何外部系统都可以通过到 Docker 主机上的 TCP 端口 5000 的端口映射来访问运行在`localnet`桥网络上的 NGINX 容器。`` 
 ``像这样映射端口是可行的，但是它很笨重，无法扩展。例如，只有一个容器可以绑定到主机上的任何端口。这意味着该主机上没有其他容器能够绑定到端口`5000`。这是单主机桥接网络只对本地开发和非常小的应用有用的原因之一。
#### 多主机覆盖网络
我们有一整章专门讨论多主机覆盖网络。所以我们将保持这一部分简短。
覆盖网络是多主机的。它们允许单个网络跨越多个主机，以便不同主机上的容器可以直接通信。它们非常适合容器到容器的通信，包括只包含容器的应用，并且可以很好地扩展。
Docker 为覆盖网络提供了本地驱动程序。这使得创建它们就像在`docker network create`命令中添加`--d overlay`标志一样简单。
#### 连接到现有网络
将容器化应用连接到外部系统和物理网络的能力至关重要。一个常见的例子是部分容器化的应用——容器化的部分需要一种方法来与仍在现有物理网络和虚拟局域网上运行的非容器化部分进行通信。
内置的`MACVLAN`驱动程序(【Windows 上的 T1】)就是基于这一点而创建的。它通过给每个容器分配自己的媒体访问控制地址和 IP 地址，使容器成为现有物理网络上的一流公民。我们在图 11.13 中展示了这一点。
![Figure 11.13](img/figure11-13.png)
Figure 11.13
从积极的方面来看，MACVLAN 的性能很好，因为它不需要端口映射或额外的网桥——您可以将容器接口连接到主机接口(或子接口)。然而，从负面来看，它要求主机网卡处于**混杂模式**，这在企业网络和公共云平台上并不总是被允许的。因此，MACVLAN 非常适合您的企业数据中心网络(假设您的网络团队可以适应混杂模式)，但它可能无法在公共云中工作。
让我们借助一些图片和一个假设的例子来深入挖掘一下。
假设我们有一个包含两个虚拟局域网的现有物理网络:
*   VLAN 100: 10.0.0.0/24
*   VLAN 200: 192.168.3.0/24
![Figure 11.14](img/figure11-14.png)
Figure 11.14
接下来，我们添加一个 Docker 主机并将其连接到网络。
![Figure 11.15](img/figure11-15.png)
Figure 11.15
然后，我们要求在该主机上运行的容器要接入 VLAN 100。为此，我们使用`macvlan`驱动程序创建了一个新的 Docker 网络。然而，`macvlan`驱动程序需要我们告诉它一些关于我们将要与之关联的网络的事情。比如:
*   子网信息
*   门
*   它可以分配给容器的 IP 范围
*   主机上要使用哪个接口或子接口
以下命令将创建一个名为“macvlan100”的新 MACVLAN 网络，该网络将把容器连接到 vlan100。
```
$ docker network create -d macvlan \
  --subnet=10.0.0.0/24 \
  --ip-range=10.0.0.0/25 \
  --gateway=10.0.0.1 \
  -o parent=eth0.100 \
  macvlan100 
```
 `这将创建“macvlan100”网络和 eth0.100 子接口。配置现在如下所示。
![Figure 11.16](img/figure11-16.png)