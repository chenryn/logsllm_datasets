as depicted for the deployed and passive replication approach
in subﬁgure 5(cid:2) and 6(cid:2). The two approaches differ in the
way that in the deployed case, the internal data structures and
binary code to execute the operator are already loaded and
present in the system (but the state is uninitialized/virgin) while
for passive replication the secondary is simply absent. Hence,
the deployed state can be considered as a way of preloading
mechanism that is beneﬁcial for operators that rely on static
data for example a lookup table which can take a considerable
amount of time to be constructed during operator initialization.
Table 1 summarizes the different approaches with regards
to actions required to be executed during a recovery.
B. Fault Tolerance Components
In order to provide adaptable fault tolerance where the
system can transition between the fault tolerance approaches as
presented in Figure 2, speciﬁc components of an operator must
be enabled and disabled during runtime. In the following, we
will provide an overview about the controllable components
of an operator:
An operator can be set either in sleep or processing mode.
In case the operator is in processing mode, events are taken
out of the incoming event queue and passed to the operator
function for processing whereas in sleep mode, the queue is
not being touched and will grow with every new event arriving
at the operator. Putting an operator in sleep mode, will save
computational resources, i.e., CPU time which can be used for
other tasks or operators running on the same node.
In order to manage network bandwidth, an operator can be
enabled or disabled for receiving and sending out events. Event
reception and dissemination of an operator can be controlled
independently as they have semantically different outcomes:
If an operator instance is not enabled to receive events, none
of the upstream operator partitions will route any produced
event to that speciﬁc operator instance whereas if an upstream
operator is not enabled to send-out any events, none of the
downstream operator instances will receive any events from
that speciﬁc operator instance. Consider as an example the
two operators instances A and B and their replicas A(cid:3) and B(cid:3)
as depicted in Figure 1: Let’s assume that operator instance
B(cid:3) is not enabled to receive events: In this case, neither A
nor A(cid:3) will route any events to B(cid:3) while B is still receiving
the complete output coming from upstream. On the contrary,
if operator instance A(cid:3) is not enabled to send-out any event,
neither B nor B(cid:3) will receive results produced by A(cid:3), however,
they will still receive results produced by operator instance A.
For state persistence, we use checkpoints, where the op-
erator’s state is either stored on some (distributed and fault
tolerant) ﬁlesystem or sent to its secondary operator instance to
be kept in memory instead. As with the processing mode, tak-
ing snapshots (i.e., checkpoints) and choosing the destination
can be controlled in a ﬁne granular manner on a per operator
instance basis.
In the following, we will describe the interplay of the
previously described components in order to transition between
the different fault tolerance states as depicted in Figure 3:
First, we deﬁne the term of a high availability unit (HA
unit). A HA unit is an operator partition that consists of two
replicas which we denote as primary and secondary. We use
checkpoints2 rather than more than two replicas as it allows
us to tolerate an arbitrary number of concurrent node failures
while keeping our model simple.
HA units can reside in any fault tolerant state (e.g., active
replication, active standby etc.) as depicted in Figure 3. A
transition between the states requires several actions to be
taken where certain components of the secondary operator
instance are enabled or disabled. For example, in order to
switch from active replication to active standby, the secondary
must be instructed to stop emitting events denoted as [−send]
in Figure 3, while going back to active replication requires
re-enabling the secondary to emit events [+send]. Although
Figure 3 shows all possible states that can be reached, we
reduced the number of transitions to a minimum for better
readability. Hence, in our system it is also possible to directly
switch from active replication to passive standby. In fact, our
fault tolerance controller employs a complete graph with all
possible transitions and their required actions which allows a
quick transition from one state to another.
C. Adaptive Fault Tolerance Controller
The objective of our fault tolerance controller is to choose
the best fault tolerance state for each HA unit ensuring that
the user deﬁned constraints are met at any point of time.
Hence, the controller evaluates constantly the recovery time
for each scheme and transitions to another state if needed.
2A checkpoint comprises the operator state as well as its outgoing queue.
467467
wireto receive events
wireto send events
read checkpoint
replay events*
deploy operator
(!)
! (!)
! ! (!)
! ! ! (!)
! ! ! ! (!)
Recovery Time
trecover = 0
trecover = 0 + tdeploy
trecover = twsend + (treplay)
trecover = max(twsend, twrec) + (treplay)
trecover = treadChkpt + max(twsend, twrec) + (treplay)
trecover = tdep + treadChkpt + max(twsend, twrec) + (treplay)
Fault Tolerance Schema
Active Replication
Active Standby
Passive Standby Hot
Passive Standby Cold
Deployed
Passive Replication
Tab. 1: Recovery steps required to perform for each fault tolerance schema and the overall recovery time.
Note: Replay events is only needed for precise recovery.
In order to prevent oscillation of the system, the controller
has a cool down period where no transition to a new state is
performed. Choosing the best scheme involves the following
four steps the controller has to perform:
1)
2)
3)
4)
Compute the recovery time for each fault tolerance
scheme.
Filter out the candidates that do not satisfy the user
speciﬁed recovery time threshold.
Rank the remaining candidates according to the costs
they would incur.
Choose the least expensive one and trigger a
transition if the currently active state differs from
the new one.
The controller starts ﬁrst with active replication as it guarantees
zero recovery time, i.e, we reside on the safe side ﬁrst. In a user
deﬁned interval, by default every 500 ms, the current choice
is reevaluated with the goal to switch to a less expensive state
if possible.
D. Recovery Time Computation
We will now describe, how we compute the recovery time
for each of the schemes. Since each fault tolerance scheme
requires several steps to be taken before the system can
resume normal operation after a crash, the overall recovery
time comprises of several components as depicted in Table 1.
For example, if a HA unit has to recovery from a failure using
passive replication, ﬁrst a backup (secondary) operator instance
must be deployed in the system which takes time tdep. In
addition to the deployment of the operator, the most recent
checkpoint (in case the operator is stateful) must be loaded
and de-serialized which is reﬂected by treadChkpt. Establishing
connections to upstream and downstream operator partitions as
deﬁned through the given topology can be executed in parallel.
Hence, the maximum of twrec (time it takes to wire upstream
operators) and twsend (time it
takes to wire downstream
operators) is used. In case the user opted for a precise recovery,
the replay time treplay is added to the overall recovery time
as the last step contributing to a complete recovery.
For the time it takes to execute a certain recovery step, we
use a mixture of historical collected values and an estimation
based approach as depicted in Table 2. We ﬁrst deﬁne a
function h(ts) to retrieve a historical measurement for some
in time speciﬁed by a timestamp ts. For example,
point
the function hchkptSize(ts) returns the size of a checkpoint
(i.e., the serialized form of an operator’s state) reported at
time ts. Hence, we deﬁne the following functions which are
instantiated per operator partition:
Checkpoint (state) size at time ts.
hchkptSize(ts1)
hwriteChkptT P (ts1) Write throughput (checkpoint) at ts.
hrec(tsnow)
Event throughput (receive) at time ts.
hprocT P (tsnow)
Event throughput (processing) at ts.
hwsend(ts)
Wire (send) time at time ts.
hwrec(ts)
Wire (receive) time at time ts.
hdep(ts)
Deploy time at time ts.
Using the previous deﬁnitions, we can get an adequate
estimate, e.g., for the time it takes to deploy an operator tdep
by taking the maximum of all collected measurements from the
past. Using the maximum reﬂects worst case behavior which
we think is an appropriate approximation as the controller
guarantees a recovery within the speciﬁed threshold. Since
StreamMine3G is an elastic system where operator instances
can be moved around depending on the systems capacity using
operator migration, each migration employs the deployment
of a new operator instance which increases accuracy of the
collected measurements. Similar as with the deploy time, tdep,
we use the maximum recorded time it
takes to establish
connections to upstream and downstream operator partitions
to set twrec and twsend, respectively.
The overall time it takes to recover a stateful operator
comprises on several components: First, the time it takes to
read the binary form of the state from the stable storage, and
second, the time it takes to de-serialize and reconstruct the state
from binary form to its original. Since recovery happens far
more seldom than taking checkpoints for a potential recovery,
we use the historical values gathered from checkpointing the
state. In order to estimate the recovery time for a stateful
operator, we use the size of the most recent checkpoint divided
by the lowest recorded write throughput (which includes the
serialization overhead in addition to the disk throughput). Since
the writing to disk is usually much lower than reading, we
ﬁnd this approximation appropriate. For an even more accurate
estimation of the recovery time for the state, a lookup table can
be used which contains recordings as a mapping of state size
468468
Recovery Step
read checkpoint
replay events
wire to receive events
wire to send events
deploy operator
(cid:2)
Recovery Time
treadChkpt = hchkptSize(tslastCkpt)/ min({hwriteChkptT P (ts1), . . . , hwriteChkptT P (tsn)})
treplay =
twrec = max({hwrec(ts1), . . . , hwrec(tsn)})
twsend = max({hwsend(ts1), . . . , hwsend(tsn)})
tdep = max({hdep(ts1), . . . , hdep(tsn)})
hrec(tsnow), hrec(tsnow−1), . . . , hrec(tsnow−lastCkpt)/hprocT P (tsnow)
Tab. 2: Recovery time for each recovery step.
to recovery time, however, since state size can highly vary over
time which could result in a high number of entries, we favor
a simple estimation based approach as described previously.
In addition to the recovery of state, events must be replayed
in case the user opted for a precise rather than gap recovery.
Since the amount of events that must be replayed is strongly
inﬂuenced by the checkpoint interval, event replay can take
a considerable amount of time. Hence, we use the number
of events received by the primary since the last checkpoint
and divide it by the current processing throughput in order to
retrieve an estimate for the replay step. Note that it is also
possible to adjust the checkpointing interval in order to reduce
the replay and recovery time. However, there is a trade-off in
overhead imposed through a more frequent checkpointing and
the gain in a decrease of the recovery time as we will show
in the evaluation in Section IV.
For a reﬁnement of the parameter estimation, advanced
techniques such as Kalman ﬁlters [16] or machine learning
based approaches can be used as they may result in a more
positive estimation of the recovery time compared to our
approach. However, in order to keep the system model simple,
we left the exploration of such techniques for future work.
E. Cost Savings Adaption
In order to select the fault tolerance scheme which not
only guarantees the user speciﬁed recovery threshold but also
reduces costs by using as little resources as possible, users
can optionally annotate resources with costs which ideally
matches the cost model of the environment the application is
running in. For example, an application running in the Amazon
EC2 cloud environment will incur charges the more virtual
machines used but not by the amount of CPU cycles or network
bandwidth used unless trafﬁc goes across regional availability
zones. However, in a different setup such as a local cluster
where several applications or virtual machines share the same
host, a user might also be interested in reducing the network
trafﬁc that is imposed by fault tolerance rather than only the
number of hosts used. Hence, user can provide a cost weight
vector vcosts comprising the costs for CPU, memory, network
and virtual machines. Using the cost weight vector, a ranking
between applicable scheme, i.e. candidates, can be established.
For example consider the following situation: Let’s assume
a user chose ﬁve seconds as a recovery time threshold and
the controller identiﬁed active replication, active standby and
passive standby hot as valid options. While active replication
and standby incur almost identical costs with regards to CPU
consumption due to processing of events at the two replicas,
i.e., primary and secondary, passive standby incurs additional
network trafﬁc costs due to state synchronization. If the user
weighted CPU costs higher than network costs, passive standby
will be chosen as it consumes the least CPU resources at the
cost of additional bandwidth usage while in the counter case
active replication will be selected by the controller.
Costs are normalized based on the measurements received
from the primary operator instance before applying the user
provided cost weight vector and summarizing the components
for a ranking. If two approaches have the same relative costs,
the approach providing the lowest recovery time is chosen in
favor for the user.
IV. EVALUATION
In this section we present the results from various experi-
ments we performed in order to evaluate the beneﬁts regarding
resource and cost savings of our proposed solution.
A. Experiment setup
For our evaluation, we used two different applications and
workloads. The ﬁrst application performs a sentiment analysis
using Twitter streams we collected over a period of a month.
The application comprises of two operators where the ﬁrst
one performs a simple ﬁltering based on certain hash-tags or
keywords while the second one performs a sentiment analysis
and an aggregation using a sliding window of ten seconds
length. The workload is depicted in Figure 4.
The second application performs a short term energy con-
sumption prediction for Smart Grids [17]. As with the ﬁrst
application, two operators are used where the ﬁrst one performs
a data conversion of the data tuples coming from smart plugs
while the second one performs the short term load prediction
using several sliding windows.
Both applications have in common that the query/topology
consists of stateless and stateful operators, and the stateful
operators use a time-based sliding window. Time-based sliding
windows have the property of quickly accumulating state once
the throughput rises, hence, the evolution of the state follows
the pattern of the throughput as shown in Figure 4.
We implemented the applications in C++ to run on top of
StreamMine3G’s native interface. However, application devel-
opers can also use Java as their language of choice by using the
supplied Java interface wrapper. As for the environment, we
performed our experiments on a 50-node cluster where each
node is equipped with 2 Intel Xeon E5405 (quad core) CPUs
and 8 GB of RAM. The nodes are inter-connected via Gigabit
Ethernet (1000BaseT full duplex) and run an Ubuntu Linux
14.04.1 LTS operating system with kernel version 3.13.0.
B. Validation
In our ﬁrst experiment, we performed a sanity check to
validate our approach. We ﬁrst analyzed the given Twitter
469469
(YHQW7KURXJKSXWN(YHQWVV
6WDWH6L]H0%
)DXOW7ROHUDQFH6FKHPH








$5
$6
36+RW
36&ROG
'(3
35



7LPHV



Passive Replication
Deployed
Passive Standby Cold
Passive Standby Hot
Active Standby
Active Replication
All Costs
Amazon EC2
100
)
%
h
c
a
e
n
i
t
n
e
p
s
e
m
T
i
(
e
m
e
h
c
s
e
c
n
a
r
e
o
l
t
t
l
u
a
f
80
60
40
20
0
2
4
6
8
10
12
14
16
18
20
4
Recovery time (s)