title:Can Machine Learning Benefit Bandwidth Estimation at Ultra-high Speeds?
author:Qianwen Yin and
Jasleen Kaur
Can Machine Learning Beneﬁt Bandwidth
Estimation at Ultra-high Speeds?
Qianwen Yin(B) and Jasleen Kaur
University of North Carolina at Chapel Hill, Chapel Hill, USA
PI:EMAIL
Abstract. Tools for estimating end-to-end available bandwidth (AB)
send out a train of packets and observe how inter-packet gaps change over
a given network path. In ultra-high speed networks, the ﬁne inter-packet
gaps are fairly susceptible to noise introduced by transient queuing and
bursty cross-traﬃc. Past work uses smoothing heuristics to alleviate the
impact of noise, but at the cost of requiring large packet trains. In this
paper, we consider a machine-learning approach for learning the AB from
noisy inter-packet gaps. We conduct extensive experimental evaluations
on a 10 Gbps testbed, and ﬁnd that supervised learning can help realize
ultra-high speed bandwidth estimation with more accuracy and smaller
packet trains than the state of the art. Further, we ﬁnd that when train-
ing is based on: (i) more bursty cross-traﬃc, (ii) extreme conﬁgurations
of interrupt coalescence, a machine learning framework is fairly robust
to the cross-traﬃc, NIC platform, and conﬁguration of NIC parameters.
1 Introduction
End-to-end available bandwidth (AB) is important in many application domains
including server selection [1], video-streaming [2], and congestion control [3].
Consequently, the last decade has witnessed a rapid growth in the design of AB
estimation techniques [4–6]. Unfortunately, these techniques do not scale well
to upcoming ultra-high speed networks [7]1. This is because small inter-packet
gaps are needed for probing higher bandwidth —such ﬁne-scale gaps are fairly
susceptible to being distorted by noise introduced by small-scale buﬀering.
Several approaches have been proposed to reduce the impact of noise [8–10],
most of which apply smoothing techniques to “average-out” distortions. Due to
the complex noise signatures that can occur at ﬁne timescales, these techniques
need to average out inter-packet gaps over a large number of probing packets—
this impacts the overhead and timeliness of these techniques.
In this paper, we ask: can supervised machine learning be used to auto-
matically learn suitable models for mapping noise-aﬄicted packet gaps to AB
estimates? We design a learning framework in which the sender and receiver
side inter-packet gaps are used as input features, and an AB estimate is the
output. Extensive evaluations are conducted, and ﬁnd that a machine learning
1 We focus on 10 Gbps speed in this paper, and use jumbo frames of MTU=9000B.
c(cid:2) Springer International Publishing Switzerland 2016
T. Karagiannis and X. Dimitropoulos (Eds.): PAM 2016, LNCS 9631, pp. 397–411, 2016.
DOI: 10.1007/978-3-319-30505-9 30
398
Q. Yin and J. Kaur
framework can indeed be trained to provide robust bandwidth estimates, with
much higher accuracy and using much smaller number of probing packets than
the state of the art.
In the rest of this paper, we describe the challenges of AB estimation at ultra
high-speed, and the state-of-art in Sect. 2. We introduce our machine learning
framework in Sect. 3, and our data collection methodology in Sect. 4. In Sect. 5,
we experimentally evaluate our approach, and conclude in Sect. 6.
2 State of the Art
2.1 Background: Available Bandwidth Estimation
i = pi
ri
, where gs
Main-stream bandwidth estimation tools adopt the probing rate model
[11],
which sends out streams of probe packets (referred to as pstreams) at a desired
probing rate, by controlling the inter-packet send gaps as: gs
i is
the send gap between the ith and i-1 th packets, ri is the intended probing rate,
and pi is the size of ith packet. The estimation logic is based on the principle
of self-induced congestion— if ri > AB, then qi > qi−1, where qi is the queu-
ing delay experienced by the ith packet at the bottleneck link, and AB is the
bottleneck available bandwidth. Assuming ﬁxed routes and constant processing
i is the receive gap between the ith and
delays, this translates to gr
i-1 th packets. Most tools send out multiple packets (Np) at each probing rate,
and check whether or not the receive gaps are consistently higher than the send
gaps. They try out several probing rates and search for the highest rate rmax
that does not cause self-induced congestion. There are two dominant strategies
for searching for rmax:
i , where gr
i > gs
Feedback-Based Single-Rate Probing: The sender relies on iterative
feedback-based binary search. The sender sends all packets within a pstream
at the same probing rate, and waits for receiver feedback on whether the receive
gaps increased or not. It then either halves or doubles the probing rate for the
next stream accordingly. Pathload is the most prominent of such tools [4].
Multi-rate Probing: The sender uses multi-rate probing without relying on
receiver feedback—each pstream includes N = Nr × Np packets, where Nr is the
Fig. 1. Inter-Packet Gaps Nr = 4, Np = 16
Can Machine Learning Beneﬁt Bandwidth Estimation at Ultra-high Speeds?
399
number of probing rates tried out. The sender then looks for the highest probing
rate that did not result in self-congestion. Figure 1(a) illustrates a multi-rate
pstream with Nr = 4, Np = 16. The receive gaps are consistently larger than the
send gaps since the third probing rate, so the second probing rate (rmax) is taken
as an estimate of the AB. Multi-rate probing facilitates the design of light-weight
and quick tools [7]. Pathchirp is the most prominent of such tools [5].
2.2 Challenge: Noise in Ultra High Speed Networks
End-to-end bandwidth estimation tools face three major challenges at ultra high-
speed: accurately creating ﬁne-scale inter-packet gaps at the sender, dealing with
the presence of noise along the path, and precisely timestamping packet arrival
at the receiver.2 To address the ﬁrst challenge, we use the framework described
in [10], in which approporiate-sized IEEE 802.3x PAUSE frames — “dummy”
frames that get dropped by the ﬁrst switch on the path, are inserted for creating
ﬁne-scale inter-packet gaps. We focus on the remaining two challenges in this
paper.
Any resource that is shared can be tem-
porarily unavailable, even if it is not a bottle-
neck resource over larger timescales—a packet
may have to wait in a transient queue at such
a resource. In ultra-high speed networks, the
magnitude of distortions created by queuing-
induced noise are comparable to (or even
larger than) the changes in inter-packet gaps
that need to be detected for bandwidth esti-
mation. [10] identiﬁes two main noise sources:
Fig. 2. BASS-denoised gaps
Bursty Cross-Traﬃc at Bottleneck Resources. If the cross-traﬃc that
shares a bottleneck queue varies signiﬁcantly at short timescales, then all packets
sent at a given probing rate may not consistently show an increase in receive
gaps. For instance, Fig. 1(b) plots the inter-packet gaps observed right after the
bottleneck queue, for the same pstream as in Fig. 1(a). Due to the bursty cross-
traﬃc, the receive gaps are consistently larger than the send gaps only for the
4th probing rate (resulting in an over-estimation of AB).
Transient Queuing at Non-bottleneck Resources. Even though a resource
may not be a network bottleneck, it can certainly induce short-scale transient
queues when it is temporarily unavailable while serving competing processes or
traﬃc. Interrupt Coalescence is a notable source of such noise [8,14]. It is turned
on by default at receivers, forcing packets to wait at the NIC before being handed
to OS for timestamping, even if the CPU is available—the waiting time (a.k.a
2 The ﬁrst and third can be well addressed with specialized NICs [12], or with recent
advances in fast packet I/O frameworks such as netmap [13]. In this study, however,
we focus on end systems with standard OSes and commodify network hardwares.
400
Q. Yin and J. Kaur
interrupt delay) can be signiﬁcant compared to the ﬁne-scale gaps needed in
ultra high-speed networks. Figure 1(c) plots the inter-packet gaps observed at
i ) for the pstream in Fig. 1(a). We ﬁnd that these gaps are dom-
the receiver (gr
inated by a “spike-dips” pattern—each spike corresponds to the ﬁrst packet
that arrives after an interrupt and is queued up till the next interrupt (thus
experiencing the longest queuing delay). The dips correspond to the following
packets buﬀered in the same batch. With the “spike-dips” pattern, an consis-
tently increasing trend of queuing delays will not be observed in any pstream,
leading to persistent over-estimation of AB.
2.3 State of the Art: Smoothing Out Noise
Several approaches have been proposed to deal with the impact of noise on band-
width estimation [4,8–10]. In general, all of these approaches employ denoising
techniques for smoothing out inter-packet receive gaps, before feeding them to
the bandwidth estimation logic. The recently-proposed Buﬀering-aware Spike
Smoothing (BASS) [10] has been shown to outperform the others on 10 Gbps
networks with shorter streams, and is summarized below.
BASS works by detecting boundaries of “buﬀering events” in recvgaps— each
“spike” and the following dips correspond to packets within the same buﬀering
event. Based on the observation that the average receiving rate within a buﬀering
event is the same as that observed before the buﬀering was encountered, BASS
recovers this quantity by carefully identifying buﬀering events and smoothing
out both sendgaps and recvgaps within each. The smoothed gaps are then fed
into an AB estimation logic. Figure 1(c) plots the BASS-smoothed gaps for the
pstream in Fig. 2. In [10], BASS was used within both single-rate and multi-rate
probing frameworks. For single-rate probing, BASS helped achieve bandwidth
estimation accuracy within 10 %, by using pstreams with at least 64 packets. For
multi-rate probing, BASS-smoothed gaps were fed to a variant of the Pathchirp
bandwidth estimation logic, and estimation accuracy of mostly within +/–10 %
was achieved using multi-rate pstreams with N=96 packets and 50 % probing
range3.
For many applications of bandwidth estimation, that need to probe for band-
width regularly and frequently, large probe streams pose a signiﬁcant issue in
terms of timeliness, overhead and responsiveness— both the duration for which
each pstream overloads the network, and the total time needed to collect AB
estimates, increase linearly with N (when Nr is ﬁxed). Even a 96-packet pstream
can last several milliseconds in a gigabit network—such a duration is too long
in the context of ultra-high speed congestion control [3].
3 A Learning Framework for Bandwidth Estimation
It is important to note that noise can distort gaps within a pstream with several
diﬀerent signatures, each with its own magnitude of gap-distortion, and each
3 Probing range is given by:
− 1.
rN
r1
Can Machine Learning Beneﬁt Bandwidth Estimation at Ultra-high Speeds?
401
with its own timescale and frequency at which it manifests itself (as exempliﬁed
in Fig. 1(b) and (c)). When simple smoothing heuristics are used by the state
of the art for dealing with such diversity in noise, they result in an underﬁt
model—expectedly, these techniques need to smooth over a large number of
probe packets in order to be robust. The main hypothesis of this work is that
machine learning (ML) can improve our understanding of the noise signature in
gaps, with even shorter probe streams than the state of the art.
In this paper, we propose to use supervised learning to automatically derive
an algorithm that estimates AB from the inter-packet send and receive gaps
of each pstream. Such an algorithm is referred to as a learned “model”. We
envision that the model is learned oﬄine, and then can be incorporated in other
AB estimation processes. Below, we brieﬂy summarize the key components of
this framework.
Input Feature Vector. The input feature vector for a pstream is constructed
}. Fourier transforms
from the set of send gaps and receive gaps, {gs
are commonly used in ML applications, when the input may contain information
at multiple frequencies [15,16]—as discussed before, this certainly holds for the
diﬀerent sources of noise on a network path. Hence, we use as a feature vector,
the fourier-transformed sequence of send and receive gaps for a pstream of length
N: x = F F T (gs
} and {gr
i
i
1, ..., gs
N , gr
1, ..., gr
N ).
Output. The output, y, of the ML framework is the AB evaluation. For single-
rate pstreams, the AB estimation can be formulated as a classiﬁcation problem:
y = 1 if the probing rate exceeds AB, otherwise y = 0. For multi-rate pstreams,
it can be formulated as a regression problem, in which y = AB.
Learning Techniques. We consider the following ML algorithms—ElasticNet
[17], which assumes a polynomial relationship between x and y; RandomForest [18],
AdaBoost [19] and GradientBoost [20], which ensemble multiple weak models into
a single stronger one; Support Vector Machine(SVM) [21], which maps x into a
high dimensional feature space and constructs hyperplanes separating y values in
the training set.4
Training-and-Testing. The success of any ML framework depends heavily on
good data collection—data that is accurate as well as representative. Section 4
describes our methodology for generating hundreds of thousands of pstreams
under a diverse set of conditions—it also describes how we collect the ground-
truth of AB, ABgt, for each pstream. The knowledge of ABgt allows us to com-
pute an expected value, yexp, of the output of the ML framework—both for
single-rate as well as multi-rate pstreams.
4 Our evaluations revealed that models trained with ElasticNet and SVM result in
considerable inaccuracy. For brevity, we don’t present their results.