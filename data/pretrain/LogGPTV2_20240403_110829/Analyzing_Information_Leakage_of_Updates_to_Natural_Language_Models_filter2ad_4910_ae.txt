extraction of complete sentences such as “Israel allows freedom of
religion” occurring as few as three times in the dataset is possible.
Overall, this indicates that intended memorization is unlikely to
explain our results.
Unintended memorization may occur for infrequent phrases.
However, it cannot alone explain our results, as shown by our
success in recovering canaries when using a low-capacity model in
a large-data regime (cf. Wikitext-103 column in Table 1), for which
the effect of unintended memorization is less pronounced, and
evidenced by the large context needed to recover canaries from a
single-model analysis [6]. The most likely explanation remains that
a differential analysis of two model snapshots amplifies otherwise
imperceptible differences in the data used to train them, which
would be hard to suppress without hurting a model’s performance.
6 MITIGATIONS
In this section, we discuss and analyze three strategies to miti-
gate information leakage in model updates: (1) Differential Privacy,
(2) continued training with public data, and (3) truncating the out-
put of the updated model.
6.1 Mitigation: Differential Privacy
Differential privacy (DP) [11] provides strong guarantees on the
amount of information leaked by a released output. Given a compu-
tation over records it guarantees a bound on the effect that any input
record can have on the output. Formally, 𝐹 is a (𝜖, 𝛿)-differentially-
private computation if for any datasets 𝐷 and 𝐷′ that differ in one
record and for any subset 𝑂 of 𝐹’s range we have
Pr(𝐹(𝐷) ∈ 𝑂) ≤ exp(𝜖) · Pr(𝐹(𝐷′) ∈ 𝑂) + 𝛿 .
CCS ’20, November 9–13, 2020, Virtual Event, USA
Zanella-Béguelin, Wutschitz, Tople, Rühle, Paverd, Ohrimenko, Köpf, and Brockschmidt
1 × 108
1 × 107
1 × 106
100000
10000
1000
)
𝑠
(
)
𝐷
(
m
a
r
g
−
3
𝑝
𝑟
𝑒
𝑝
100
1000
10000
Extracted from 𝑀
Extracted from 𝑀′
Extracted from (𝑀, 𝑀′)
1 × 106
100000
𝑝𝑒𝑟𝑝3−gram(𝑁)(𝑠)
(a) Re-training from scratch
1 × 108
1 × 107
1 × 106
100000
10000
1000
)
𝑠
(
)
𝐷
(
m
a
r
g
−
3
𝑝
𝑟
𝑒
𝑝
100
1000
10000
Extracted from 𝑀
Extracted from 𝑀′
Extracted from (𝑀, 𝑀′)
1 × 106
100000
𝑝𝑒𝑟𝑝3−gram(𝑁)(𝑠)
(b) Continued training
Figure 2: Sensitivity of extracted content. + depict sentences extracted from 𝑀, × from 𝑀′, and ∗ from (𝑀, 𝑀′) using Differential
Score. Vertical axis depicts the perplexity w.r.t data 𝐷, horizontal axis depicts perplexity w.r.t data update 𝑁 . Points above the
diagonal are closer in distribution to the (private) data update 𝑁 than to the base data 𝐷.
Table 6: Quantifying near matches of extracted phrases from RNN models trained on the base Reddit dataset and updated with
talk.politics.mideast. For each extracted phrase, we compare the Levenshtein distance to its nearest neighbor in the base
and update datasets respectively. The updated dataset contains closer matches for all phrases except west bank peace talks
and capital letters racial discrimination, for which there are equally close matches in both datasets.
Extracted phrase
talk.politics.mideast
Reddit
center for policy research
troops surrounded village after
partition of northern israel
west bank peace talks
spiritual and political leaders
saudi troops surrounded village
arab governments invaded turkey
little resistance was offered
buffer zone aimed at protecting
capital letters racial discrimination
center for policy research
troops surrounded village after
shelling of northern israel
. no peace talks
spiritual and political evolutions
our troops surrounded village
arab governments are not
little resistance was offered
" aimed at protecting
% of racial discrimination
0
0
1
2
1
1
2
0
2
2
center for instant research
from the village after
annexation of northern greece
: stated peace talks
, and like leaders
" hometown " village
! or wrap turkey
, i was offered
’s aimed at a
allegory for racial discrimination
1
2
2
2
2
3
3
2
3
2
Differential privacy is a natural candidate for defending against
membership-like inferences about data. The exact application of
differential privacy for protecting the information in the model
update depends on what one wishes to protect w.r.t. the new data:
individual sentences in the new data or all information present in
the update. For the former, sequence-level privacy can suffice while
for the latter group DP can serve as a mitigation technique where
the size of the group is proportional to the number of sequences
in the update. Recall that an 𝜖-DP algorithm 𝐹 is 𝑘𝜖-differentially
private for groups of size 𝑘 [11].
Differential privacy can be achieved in gradient-based optimiza-
tion computations [1, 4, 28] by clipping the gradient of every record
in a batch according to some bound 𝐿, then adding noise propor-
tional to 𝐿 to the sum of the clipped gradients, averaging over the
batch size and using this noisy average gradient update during
backpropagation.
We evaluate the extent to which DP mitigates attacks considered
in this paper by training models on the Penn Treebank (PTB) dataset
with canaries with sequence-level differential privacy. We train DP
models using the TensorFlow Privacy library [2] for two sets of
(𝜖, 𝛿) parameters, (5, 1 × 10−5) and (111, 1 × 10−5), for two datasets:
PTB and PTB with 50 insertions of the all-low-frequency canary.
We rely on [2] to train models with differentially private stochastic
gradient descent using a Gaussian noise mechanism and to compute
the overall privacy loss of the training phase. As expected, the
performance of models trained with DP degrades, in our case from
≈23% accuracy in predicting the next token on the validation dataset
to 11.89% and 13.34% for 𝜖 values of 5 and 111, respectively.
Analyzing Information Leakage of Updates to Natural Language Models
CCS ’20, November 9–13, 2020, Virtual Event, USA
While the beam search with the parameters of Section 4.4 no
longer returns the canary phrase for the DP-trained models, we note
that the models have degraded so far that they are essentially only
predicting the most common words from each class (e.g., “is” when
a verb is required) and thus, the result is unsurprising. We note that
the guarantees of sequence-level DP formally do not apply for the
case where canary phrases are inserted as multiple sequences, and
that 𝜖 values for our models are high. However, the 𝜖-analysis is
an upper bound and similar observations about the effectiveness of
training with DP with high 𝜖 were reported by Carlini et al. [6].
We further investigate the effect of DP training on the differential
rank of a canary phrase that was inserted 50 times. Instead of using
our beam search method to approximate the differential rank, we
fully explore the space of subsequences of length two, and find
that the DR for the two-token prefix of our canary phrase dropped
from 0 to 9,458,399 and 849,685 for the models with 𝜖 = 5 and
𝜖 = 111 respectively. In addition, we compare the differential score
of the whole phrase and observe that it drops from 3.94 for the
original model to 4.5 × 10−4 and 2.1 × 10−3 for models with 𝜖 = 5
and 𝜖 = 111, respectively. Though our experiment results validate
that DP can mitigate the particular attack method considered in
this paper for canary phrases, the model degradation is significant.
In addition, the computational overhead of per-sequence gradient
clipping required by [2] is substantial, making it unsuitable for
training high-capacity neural language models on large datasets.
6.2 Mitigation: Two-stage Continued Training
We also consider a possible mitigation strategy where we perform
continued training in two stages. For this, we split the dataset into
three equal parts 𝐷orig, 𝐷extra and 𝐷′
. We proceed as in the
continued training setting in RQ2, but add a final step in which
we train on another dataset after training on the canaries. This
resembles a setting where an attacker does not have access to two
consecutive snapshots. The rightmost column of Table 2, shows
that the differential score of the canary phrase drops substantially
after the second training stage. Thus, two or multi-stage continued
training, where only the last trained model is released, might be a
path toward mitigating leakage of private data.
extra
6.3 Mitigation: Truncating Output
Finally, we analyze the effect of truncating the output of the updated
model for each query. Specifically, the adversary still has full access
to the original model 𝑀 but only receives the top 𝑘 tokens from
the updated model 𝑀′. This is a slight weakening of our adversary
model, but is realizable for some applications. For example, in the
Data Specialization scenario, the adversary may have full access to
the public base model, but can only access the specialized model via
an API that truncates the results for each query. In the Data Update
scenario, even if models are deployed to client devices, it may be
possible to enforce this by running the model in a Trusted Execution
Environment (TEE), such as Intel SGX [18] or ARM TrustZone [3]
on the client device.
To evaluate the impact of this mitigation, we repeat the experi-
ment described in Section 5 and plot only the sentences extracted
using differential score (i.e., the ‘Snapshot attack’) for different val-
ues of 𝑘. To facilitate comparison, we use the same beam width as
in Figures 2a and 2b. As shown in Figure 3, decreasing the value
of 𝑘 brings the extracted sequences closer to the main diagonal,
where they have similar likelihood of being drawn from either
dataset. Similarly to Figures 2a and 2b, we also observe a difference
between re-training from scratch and continued training; for the
same value of 𝑘, the sentences extracted after continued training
are more likely to be private than those extracted after the model
is re-trained from scratch. Additionally, if the adversary only has
access to the top 𝑘 outputs of the original model 𝑀, this would
further reduce the leakage. In applications where this mitigation is
realizable, returning only the top 𝑘 outputs can thus reduce leakage
without decreasing the utility of the provided outputs.
7 RELATED WORK
Several works have shown that machine learning models can leak
information about training data and proposed defenses for them.
Membership inference attacks. Shokri et al. [26] show that one
can identify whether a record belongs to the training dataset of a
classification model given black-box access to the model and shadow
models trained on data from a similar distribution. Salem et al. [25]
demonstrate that similar attacks are effective under weaker adver-
sary models. It would be interesting to study how membership infer-
ence based on differential score compares to other techniques [8].
Song and Shmatikov [27] also study sequence-to-sequence lan-
guage models and show how a user can check if their data has
been used for training. In their setting, an auditor needs an aux-
iliary dataset to train shadow models with the same algorithm as
the target model and queries the target model for predictions on
a sample of the user’s data. The auxiliary dataset does not need
to be drawn from the same distribution as the original training
data (unlike [26]) and the auditor only observes a list of several
top-ranked tokens. In contrast, our approach requires no auxiliary
dataset, but assumes access to the probability distributions over
all tokens from two different model snapshots. From this, we are
able to recover full sequences from the differences in training data
rather than binary information about data presence. Like them,
we find that sequences with infrequent tokens provide a stronger
signal to the adversary/auditor.
Reconstruction attacks. These attacks abuse a model to recover
specific training points [24]. The attacks we present are a form of
reconstruction attacks against an updated model: we recover data
points in the dataset used for the update given the original model
as auxiliary information.
Carlini et al. [6] is closest to our work, as it also considers infor-
mation leakage of language models. The authors assess the risk of
(unintended) memorization of rare sequences in the training data.
They show that canaries inserted into training data can be retrieved
from a character-level language model. The key differences to our