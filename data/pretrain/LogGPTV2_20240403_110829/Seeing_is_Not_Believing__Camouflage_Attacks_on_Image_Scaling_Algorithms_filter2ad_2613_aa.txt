title:Seeing is Not Believing: Camouflage Attacks on Image Scaling Algorithms
author:Qixue Xiao and
Yufei Chen and
Chao Shen and
Yu Chen and
Kang Li
Seeing is Not Believing: 
Camouflage Attacks on Image Scaling Algorithms
Qixue Xiao, Department of Computer Science and Technology, Tsinghua University and 
360 Security Research Labs; Yufei Chen, School of Electronic and Information Engineering, 
Xi’an Jiaotong University and 360 Security Research Labs; Chao Shen, School of Electronic 
and Information Engineering, Xi’an Jiaotong University; Yu Chen, Department of Computer 
Science and Technology, Tsinghua University and Peng Cheng Laboratory; Kang Li, 
Department of Computer Science, University of Georgia
https://www.usenix.org/conference/usenixsecurity19/presentation/xiao
This paper is included in the Proceedings of the 
28th USENIX Security Symposium.
August 14–16, 2019 • Santa Clara, CA, USA
978-1-939133-06-9
Open access to the Proceedings of the 
28th USENIX Security Symposium 
is sponsored by USENIX.
Seeing is Not Believing: Camouﬂage Attacks on Image Scaling Algorithms
Qixue Xiao⇤ 1,4, Yufei Chen⇤ 2,4, Chao Shen2, Yu Chen1,5, and Kang Li3
1Department of Computer Science and Technology, Tsinghua University
2School of Electronic and Information Engineering, Xi’an Jiaotong University
3Department of Computer Science, University of Georgia
4360 Security Research Labs
5Peng Cheng Laboratory
Abstract
Image scaling algorithms are intended to preserve the visual
features before and after scaling, which is commonly used in
numerous visual and image processing applications. In this
paper, we demonstrate an automated attack against common
scaling algorithms, i.e. to automatically generate camouﬂage
images whose visual semantics change dramatically after scal-
ing. To illustrate the threats from such camouﬂage attacks,
we choose several computer vision applications as targeted
victims, including multiple image classiﬁcation applications
based on popular deep learning frameworks, as well as main-
stream web browsers. Our experimental results show that such
attacks can cause different visual results after scaling and thus
create evasion or data poisoning effect to these victim appli-
cations. We also present an algorithm that can successfully
enable attacks against famous cloud-based image services
(such as those from Microsoft Azure, Aliyun, Baidu, and Ten-
cent) and cause obvious misclassiﬁcation effects, even when
the details of image processing (such as the exact scaling
algorithm and scale dimension parameters) are hidden in the
cloud. To defend against such attacks, this paper suggests
a few potential countermeasures from attack prevention to
detection.
1
Introduction
Image scaling refers to the resizing action on a digital image,
while preserving its visual features. When scaling an image,
the downscaling (or upscaling) process generates a new image
with a smaller (or larger) number of pixels compared to the
original one. Image scaling algorithms are widely adopted in
various applications. For example, most deep learning com-
puter vision applications use pre-trained convolutional neural
network (CNN) models, which take data with a ﬁxed size de-
ﬁned by the input layers of those models. Hence, input images
have to get scaled in the data preprocessing procedure to meet
⇤Co-ﬁrst authors. This work was completed during their internship pro-
gram at 360 Security Research Labs.
with speciﬁc model input size. Popular deep learning frame-
works, such as Caffe [17], TensorFlow [13] and Torch [26],
all integrate various image scaling functions in their data pre-
processing modules. The purpose of these built-in scaling
functions is to allow the developers to use these frameworks
to handle images that do not match the model’s input size.
Although scaling algorithms are widely used and are ef-
fective to normal inputs, the design of common scaling algo-
rithms does not consider malicious inputs that may intention-
ally cause different visual results after scaling and thus change
the “semantic” meaning of images. In this paper, we will see
that an attacker can utilize the “data under-sampling” phe-
nomena occurring when a large image is resized to a smaller
one, to cause “visual cognition contradiction” between human
and machines for the same image. In this way, the attacker
can achieve malicious goals like detection evasion and data
poisoning. What’s worse, unlike adversarial examples, this
attack is independent from machine learning models. The
attack indeed happens before models consume inputs, and
hence this type of attacks affects a wide range of applications
with various machine learning models.
This paper characterizes this security risk and presents a
camouﬂage attack on image scaling algorithms (abbreviated
as scaling attack in the rest of the paper). To successfully
launch a scaling attack, attackers need to deal with two techni-
cal challenges: (a) First, an adversary needs to decide where
to insert pixels with deceiving effects by analyzing the scaling
algorithms. It is tedious and practically impossible to use man-
ual efforts to determine exact pixel values to achieve a desired
deceiving effect for realistic images. Therefore, a successful
attack needs to explore an automatic and efﬁcient camouﬂage
image generation approach. (b) Second, for cloud-based com-
puter vision services, the exact scaling algorithm and input
size of their models are transparent to users. Attackers need to
infer scaling-related parameters of the underneath algorithms
in order to successfully launch such attacks.
To overcome these challenges, we ﬁrst formalize the pro-
cess of scaling attacks as a general optimization problem.
Based on the generalized model, we propose an automatic
USENIX Association
28th USENIX Security Symposium    443
generation approach that can craft camouﬂage images efﬁ-
ciently. Moreover, this work examines the feasibility of this
attack in both the white-box and black-box scenarios, includ-
ing applications based on open deep learning frameworks and
commercial cloud services:
• In the white-box case (see Section 6.1 for more details),
we analyze common scaling implementations in three
popular deep learning frameworks: Caffe, TensorFlow
and Torch. We ﬁnd that nearly all default data scaling
algorithms used by these frameworks are vulnerable to
the scaling attack. With the presented attack, attackers
can inject poisoning or deceiving pixels into input data,
which are visible to users but get discarded by scaling
functions, and eventually being omitted by deep learning
systems.
• In the black-box case (see Section 6.2 for more details),
we investigate the scaling attack against cloud-based
computer vision services. Our results show that even
when the whole image processing pipeline and design
details are hidden from users, it is still possible to launch
the scaling attack to most existing cloud-based computer
vision services. Since image scaling modules are built
upon open image processing libraries or open interpola-
tion algorithms, possible ways of image scaling imple-
mentation are relatively limited. Attackers can design a
brute-force testing strategy to infer the scaling algorithm
and the target scale. In this paper, we exhibit a simple but
efﬁcient testing approach, with successful attack results
on Microsoft Azure1, Baidu2, Aliyun3 and Tencent4.
• Interestingly, we also discover and discuss the range
of the attacking inﬂuence extends to some computer-
graphic applications, such as mainstream web browsers
shown in Section 6.3.
We provide a video to demonstrate the attack ef-
fects, which is
following URL:
at
https://youtu.be/Vm2N0mb14Ow.
available
the
This paper studies the commonly used scaling implemen-
tations, especially for image scaling algorithms employed
in popular deep learning frameworks, and reveals potential
threats to the image scaling process. Our contributions can be
summarized as follows:
• This paper reveals a security risk in image scaling pro-
cess in computer vision applications. We validate and
testify the image scaling algorithms commonly used in
popular deep learning (DL) frameworks, and our results
1https://azure.microsoft.com/en-us/services/cognitive-s
ervices/computer-vision/?v=18.05
2https://ai.baidu.com/tech/imagerecognition/fine_grained
3https://data.aliyun.com/ai?spm=a2c0j.9189909.810797.11
.4aae547aEqltqh#/image-tag
4https://ai.qq.com/product/visionimgidy.shtml#tag
indicate that the security risk affects almost all image
applications based on DL frameworks.
• This paper formalizes the scaling attack into a con-
strained optimization problem, and presents the corre-
sponding implementation to generate camouﬂage images
automatically and efﬁciently.
• Moreover, we prove that the presented attack is still effec-
tive for cloud vision services, where the implementation
details of image scaling algorithms and parameters are
hidden from users.
• To eliminate the threats from the scaling attack, we sug-
gest several potential defense strategies from two aspects:
attack prevention and detection.
2
Image Scaling Attack Concept and Attack
Examples
In this section, we ﬁrst present a high level overview of image
scaling algorithms, followed by the concept of image scaling
attacks. Then, we exhibit some examples of the image scaling
attack, and ﬁnally we conduct an empirical study of the image
scaling practices in deep learning based image applications.
2.1 An Overview of Image Scaling Algorithms
(a) Image scaling.
(b) Interpolation in scaling.
Figure 1: The concept of image scaling.
Image scaling algorithms are designed to preserve the
visual features of an image while adjusting its size. Fig.1
presents the general concept of a common image scaling pro-
cess. A scaling algorithm infers value of each “missing point”
by using interpolation methods. Fig.1b shows an example of
constructing pixel P in the output image based on the pixels
of Q11, Q12, Q21 and Q22 in the original image. A scaling
algorithm deﬁnes which neighbor pixels to use in order to
construct a pixel of the output image, determines the relative
weight values assigned to each individual neighbor pixels.
For example, for each pixel in the output image, a nearest
neighbor algorithm only picks a single pixel (the nearest one)
from the input to replace it. A bilinear algorithm considers
a set of neighbor pixels surrounding the target pixel as the
444    28th USENIX Security Symposium
USENIX Association
Figure 2: An example showing deceiving effect of the scal-
ing attack. (Left-side: what humans see; right-side: what DL
models see)
Figure 3: A scaling attack example against Baidu image clas-
siﬁcation service.
...
"result":
{ "score": "0.938829",
"name": "Grey Wolf"},
{ "score": "0.0146997",
"name": "Mexico Wolf"}
...
basis. It then calculates a weighted average of the neighbor
pixel values as the value assigned to the target pixel.
Such scaling algorithms often assume that pixel values in
an image are results from natural settings, and they do not
anticipate pixel-level manipulations with malicious intents.
This paper demonstrates that an attacker can use scaling al-
gorithms to alter an image’s semantic meaning by carefully
adjusting pixel-level information.
2.2 Attack Examples
The scaling attack potentially affects all applications that ap-
ply scaling algorithms to preprocess input images. To demon-
strate potential risks and deceiving effects of the scaling at-
tack, here we provide two attack examples of the scaling
attack on practical applications.
Fig.2 presents the ﬁrst attack example for a local image
classiﬁcation application cppclassiﬁcation [16], a sample pro-
gram released by the Caffe framework. For the classiﬁcation
model with an input size of 224*224, we specially craft input
images of a different size (672*224). The image in the left
column of Fig.2 is one input to the deep learning application,
while the image in the right column is the output of the scaling
function, i.e., the effective image fed into the deep learning
model. While the input in the left column of Fig.2 visually
presents a sheep-like ﬁgure, the deep learning model takes the
image in the right column as the actual input and classiﬁes it
as an instance of “White Wolf”.
To validate the deceiving effect on deep learning applica-
tions, we build one image classiﬁcation demo based on the
BAIR/BVLC GoogleNet model [8], which assumes the in-
put data are of the scale of 224*224. When an image with a
different size is provided, the application triggers the native
resize() function built in the Caffe framework to rescale the
image to ﬁt the input size of the model (224*224). The ex-
act classiﬁcation setup details and the program outputs are
presented in Appendix A.
Fig.3 exhibits one attack example against the Baidu image
classiﬁcation service. The attack image is crafted from a
sheep image, with the aim to lead people to regard it as a
sheep but the machine to regard it as a wolf. The results
Scaling
Deep Learning Model
Category Info
Cat
Figure 4: How data get processed in image classiﬁcation
systems.
returned by the cloud service API5 show that the attack image
is classiﬁed as the “Grey Wolf” with a high conﬁdence score
(achieving 0.938829), indicating that our attack is effective.
More examples of the scaling attack against more cloud-based
computer vision services are presented in Table 3. In fact,
image scaling algorithms are commonly used by a wide range
of computer-graphic applications, rather than limited to deep-
learning-based computer vision systems. Therefore, they are
all potentially threatened by this type of security risk.
2.3 Empirical Study of Image Scaling Prac-
tices in Deep Learning Applications
Data scaling is actually a common action in deep learning
applications. Fig.4 shows how the scaling process is involved
in open-input applications’ data processing pipelines, such as
image classiﬁcation as an Internet service. For design simplic-
ity and manageable training process, a deep learning neural
network model usually requires a ﬁxed input scale. For image
classiﬁcation models, images are usually resizedd to 300*300
to ensure high-speed training and classiﬁcation. As shown
in Table 1, we examine nine popular deep learning models
and all of them use a ﬁxed input scale for their training and
classiﬁcation process.
For deep learning applications that receive input data from
ﬁxed input sources, such as video cameras, the input data
formats are naturally determined. Even in such situation, we
ﬁnd that the image resizing is still needed in certain cases.
One common situation we observe is the use of pre-trained
models. For example, NVIDIA offers multiple self-driving
sample models [6], and all these models use a speciﬁc input
5The original API response is presented in Chinese. Here we translate it
into English.
USENIX Association
28th USENIX Security Symposium    445
Table 1: Input sizes of various deep learning models.
Model
LeNet-5
VGG16, VGG19, ResNet, GoogleNet
AlexNet
Inception V3/V4
DAVE-2 Self-Driving
Size
(pixels*pixels)
32*32
224*224
227*227
299*299
200*66
size 200*66. However, for the recommended camera [24]
speciﬁcation provided by NVIDIA, the size of generated im-
ages varies from 320*240 to 1928*1208. None of the recom-
mended cameras produce output that matches the NVIDIA’s
model input size. Therefore, for system developers that do not
want to redesign or retrain their models, they have to employ
scaling functions in their data processing pipeline to ﬁt the
pre-trained model’s input scale. Recent research work, such as
the sample applications used in DeepXplore [25], also shows
that the resizing operation is commonly used in self-driving
applications to adjust original video frames’ size to the input
size of models.
Most deep learning frameworks provide data scaling func-
tions, as shown in Table 2. Programmers can handle images
with different sizes without calling scaling function explicitly.
We examined several sample programs released by popular
deep learning frameworks, such as Tensorﬂow, Caffe, and