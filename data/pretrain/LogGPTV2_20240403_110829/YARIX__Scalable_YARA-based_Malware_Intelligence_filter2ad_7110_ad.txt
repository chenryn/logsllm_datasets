We developed a prototype in C++ and Python 3 that imple-
ments YARIX as described in Section 3 and Section 2 for
n = 4. The prototype ran on a system using 2 Intel® Xeon®
Processor E5-2667 v4 CPUS utilizing 24 threads. We addi-
tionally used an NVME drive as an intermediate fast storage
for storing parts of the index that were merged and moved to
a traditional HDD setup (7.2K RPM SAS-12Gb/s). This setup
was capable of indexing 106 samples in 15 hours, showing
that YARIX can process over 1M samples per day on just a
single system. If we wrote the index directly on the HDD,
the operation took 39 hours. Writing directly to the HDD is
slower than using the NVME as intermediate storage, because
4Revision: 0b7d57251cd0fecf149d47d9c5564617c9fa7978 (Tue Nov
26 09:19:08 2019 UTC)
Figure 4: Search performance breakdown for all YARA rules
and different numbers of indexed samples. Whiskers denote
±1.5· IQR, dashed line denotes mean. Lower is better.
the index creation requires many IO operations. Note that all
subsequent experiments do not use parallelism to foster com-
parability to standard (non-parallel) YARA.
4.3 Correctness
To empirically validate the correctness of our approach, we
compared the results of YARIX with sequential YARA scans.
We scanned all 32M samples with every YARA rule and
checked if the candidates yielded by the YARA optimization
of YARIX for that rule is a superset. This was the case for all
the rules, which conﬁrms the completeness of YARIX. Given
that YARIX leverages standard YARA to reﬁne the candidate
ﬁles in the ﬁnal step, YARIX also guarantees soundness. In
total, 37 out of the 1404 (2.64%) rules can not be optimized
by YARIX for reasons described in Section 2.2. That is, these
rules contain too many expressions that we cannot handle or
they contain too many unoptimizable strings.
4.4 YARA Search Performance
For evaluating the search performance of YARIX, we ﬁrst cre-
ated an index for N ∈ {105,5· 105,106,32M} samples each.
Then we queried all 1404 YARA rules with each of these
indexes with YARIX as described in Section 2 and measured
the elapsed time. By choosing different numbers of indexed
samples, we can see how YARIX scales. The result of this
experiment is depicted in Figure 4. We also break down the
search time into the time spent using the index for narrowing
down the set of candidate ﬁles and the time spent sequen-
tially scanning this optimized set with YARA. The execution
time largely depends on the number of indexed samples and
grows sub-linearly due to the fast index lookups. In the case
USENIX Association
30th USENIX Security Symposium    3549
Finally, we use the search performance to evaluate differ-
ent values for the grouping threshold τ and the inﬂuence of
the number of groups. To do so, we compute the number of
candidates, i.e., the ﬁle IDs that are yielded by the YARA
optimization of YARIX and used by the sequential YARA
scan. We compare the number of candidates in the case of
grouping to the number of candidates in the case where no
grouping takes place to understand how much accuracy is lost.
The results of this analysis are depicted in Table 1. A ﬁrst in-
tuitive observation here is that τ = ∞ is not a viable option as
it blows up the number of candidates. For example, if we have
214 groups, using τ = ∞ yields approximately 5.72· 105 more
candidates on average than the non-grouping version. Note
that this means that the sequential YARA scan that would
follow will be 5 orders of magnitude slower, which is not a
viable option. However, if we use a grouping threshold, this
slowdown is drastically reduced. Consider, for example, the
case of 216 groups and a grouping threshold of τ = 10000,
which leads to only 5.13 times more candidates than the
non-grouping case on average. Even in the worst case of 214
groups this threshold would only slow down the YARA search
by a factor of 32.03. We will see in Section 4.6 that such a
threshold would group more than 98% of all posting lists.
If the n-grams of the strings of YARA rules were randomly
distributed, this would mean that almost all resulting posting
lists would be subject to grouping. As a result the overapprox-
imation would become too large and the actual slowdown
factor would be closer to that of τ = ∞, i.e., in the order of
105. However, as this result shows the n-grams of strings in-
side YARA rules do now follow a normal distribution and
there are enough non-grouped posting lists in practice to en-
sure good performance. We can use this to our advantage if
we later discuss the disk footprint in Section 4.6. For example,
choosing 215 number of groups with a threshold of τ = 10000
would decrease the relative disk footprint from 149.5% to
65.48% while only slowing down the search by a factor of
11.56 on average.
4.5 Choosing n
When choosing a suitable value for n we wanted to pick the
smallest n that has a reasonable search performance. We em-
pirically validated that n = 3 is an unsuitable choice by com-
paring it against n = 4, because it delivered 1421.83 times
more candidate ﬁles during YARA search. Given that smaller
n would only make this worse, we identiﬁed n = 4 as a lower
bound. Regarding larger values for n, we veriﬁed that choos-
ing n = 5 would almost double the number of unoptimizable
rules from 37 to 73. Additionally, the disk footprint would suf-
fer from such a choice. First, there are more unique 5-grams
than 4-grams per ﬁle that need to be indexed. Second, the
posting lists would become more sparse and as a result the
delta encoding would not be as efﬁcient. Given that all of this
Figure 5: Search performance speedup of YARIX compared
to sequential YARA scanning. Higher is better.
#groups
214
215
216
τ
200
400
1
1
1
1
1
1
5000
18.49
7.8
4
10000
32.03
11.56
5.13
∞
≈ 5.72· 105
≈ 2.77· 105
≈ 9.77· 104
Table 1: Average number of search candidates relative to
non-grouping for different number of groups and grouping
thresholds τ.
of N = 32M samples, querying the index with a rule takes
12.47 seconds in total, 9.38 seconds for querying the index
and 0.42 seconds for sequential YARA scanning in the me-
dian. The high mean values for the total time and the YARA
scan time are the result of a few outliers where the rule con-
sisted of expressions that yielded large overapproximations.
In these cases, we had to perform a sequential YARA scan
on nearly all samples. The index operation takes longer for
larger indexes, as the posting lists become larger. As a result,
the decoding of those lists takes longer. Also, as the resulting
sets become larger, the set intersections and unions become
more costly.
These experiments were carried out in a single threaded
workload in order to minimize caching and scheduling ef-
fects and ensure reproducibility. In a real-world deployment
it would be trivial to distribute the workload among different
threads and/or machines to signiﬁcantly improve the perfor-
mance.
To get a better understanding of how much faster YARIX is
compared to sequential scanning, Figure 5 depicts the speedup
factor. In the case of 32M samples, YARIX is ﬁve orders of
magnitude faster than sequential YARA scanning on average.
The speedup factor gradually increases with the number of
indexed ﬁles.
3550    30th USENIX Security Symposium
USENIX Association
Figure 6: Disk footprint for different number of groups for
N = 32M indexed samples (∅ = uncompressed, G = grouping
was used, δ = delta encoding was used, G + δ = grouping and
delta encoding was used). Lower is better.
Figure 7: Disk footprint for varying number of groups and
grouping thresholds for N = 32M indexed samples, using
G + δ + τ. Lower is better.
would be further ampliﬁed by choosing even larger values,
we chose n = 4.
4.6 Disk Footprint
To evaluate the disk footprint of YARIX, compare the size
of the index for all N samples in different conﬁgurations
regarding the number of groups and grouping thresholds τ as
discussed in Section 3.
We deﬁne the size of the index as the accumulated number
of bytes it takes to encode all posting lists. In particular, this
does not include overhead introduced by the ﬁle system or
the ﬁle format that is used to organize the posting lists. For
example, in our test setup we used the ext4 ﬁle system and
a folder structure a/b/c to organize posting lists. Here, a, b
and c each represent a byte of a 4-gram and the ﬁle c uses a
custom ﬁle format to store the 256 postings lists of all n-grams
that share the abc preﬁx. The overhead introduced by this
approach is both environment- and implementation speciﬁc,
but certainly constant and almost negligible for larger indexes.
For example, the overhead introduced by the ﬁle system is
asymptotically constant as we never expect more than 232
posting lists, which was already saturated in the case of the
N samples. Moreover, if we used a different ﬁle system that
supports more ﬁles than ext4 we could store each posting list
in an individual ﬁle and thus would not have the overhead
of the custom ﬁle format. By not including this overhead
we thus make the analysis implementation- and environment
independent.
Figure 6 and Figure 7 show the result of this analysis. The
former depicts the cases where no grouping threshold was
used and the latter includes cases where grouping with delta
encoding and a grouping threshold was used. When grouping
was applied, we use the 256 largest prime numbers that ﬁt
into a certain number of bits to optimize the disk footprint.
This means that we have slightly fewer than 2x groups, which
is why we use the “≈” notation in both ﬁgures. The choices
for both the number of groups and the grouping thresholds
present a reasonable range of options depending on the trade-
off between disk footprint and search performance, which
will be discussed later.
In an uncompressed form, the index requires 281.46% of
the space required for all samples. Note that this is already a
large improvement over offset-sensitive indexes. This is be-
cause assuming 4 byte ﬁle IDs without offsets, a ﬁle consisting
of m bytes has m− 3 n-grams, which will add 4m− 12 ≈ 4m
bytes to the index, because the ﬁle ID will be added once for
each n-gram. This would yield an overhead of ≈ 400% not
accounting for additional overhead required for storing ﬁle
IDs several times including their positional information. Since
YARIX is offset-free, we can abstract from n-grams occurring
multiple times, which yields the depicted improvement.
Applying delta encoding reduces the relative size signiﬁ-
cantly to 149.5%. Grouping further shrinks the required disk
space. The smaller the number of groups, the smaller the disk
footprint. This can be explained by the fact that the improve-
ment in disk footprint with grouping has two reasons: colli-
sions by chance and less bits required to store group IDs than
ﬁle IDs. The smallest footprint is 23%, which is achieved by
212 groups with delta encoding. Without delta encoding, 212
groups have a relative size of 49.46%. The largest footprint
for grouping with delta encoding and without delta encoding
is 92.8% and 56.76% respectively.
However, none of these grouping variants are useful in prac-
tice because of their poor search performance (cf. Section 4.4).
USENIX Association
30th USENIX Security Symposium    3551
Instead, a reasonable choice of the grouping threshold τ (maxi-
mum posting list length to apply grouping on) will be required
in practice. Choosing a threshold is a trade-off between disk
footprint and search performance. The larger the threshold,
the more posting lists will be grouped, the better the disk
footprint improvement. However, the more posting lists are
subject to grouping, the larger the overapproximation, which
degrades search performance as we have seen in Section 4.4.
The disk footprint of different choices of τ on top of grouping
with delta encoding is depicted in Figure 7. We can see that
a small threshold like τ = 1500 has little effect on the disk
footprint as too many posting lists are not subject to group-
ing. With 216 groups, for example, the footprint is 136.21%,
which is only a small improvement over the 149.5% of delta
encoding alone. However, we see that the disk footprint is
sensitive in the beginning for changes in τ and slows down
as τ becomes larger. For example, doubling τ from 1500 to
3000 decreases the disk footprint from 136.21% to 90.48%,
while doubling it from 5000 to 10000 decreases the footprint
from 80.91% to 74%. Grouping all posting lists, i.e., τ = ∞
(equivalent to G + δ cf. Figure 6), would decrease the foot-
print to 56.76%. By choosing a threshold of τ = 5000 we
already group 96.31% of all posting lists and by choosing
τ = 10000 this percentage increases to 98.71% (cf. Figure 10
in the appendix).
4.7 Scalability
One of the core goals of YARIX is scaling to large malware
sample databases. Until now, we have evaluated YARIX on
a real-world dataset consisting of N = 32M samples, but we
want to understand how YARIX scales for 232 samples by
extrapolating our results. One particular challenge for such
an extrapolation is to estimate the distribution of n-grams on
a larger sample set. Only the uncompressed index size can
be trivially extrapolated, as having kN samples will require
roughly k times the space of storing/indexing N samples.
However, to extrapolate the compressed disk footprint, we
have to study the posting list distributions, as they inﬂuence
grouping and delta encoding.
To this end, we use combinatorics to estimate the expected
number of groups the ﬁle IDs of a posting list will belong
to. A detailed description of this method and extrapolated
ﬁgures are given in Appendix A. Overall, this extrapolation
conﬁrms that the disk footprint scales linearly also if grouping
and delta encoding is applied. Following the intuition that the
distribution of n-grams among samples can be extrapolated
as described in Appendix A, we have reason to believe that
the sub-linear trend of the search performance will continue
for larger datasets as well. We already empirically conﬁrmed
this assumption in Figure 4 where we indexed differently
sized subsets and observed a sub-linear progression in search
performance.
5 Case Studies & Future Work
For a small subset of YARA rules, the YARA optimizer of
YARIX has not eliminated enough candidates and has left too
many ﬁles for a sequential YARA scan. While most rules
perform well and the ﬁltering by the index does most of the
work, applying YARIX to some rules excluded almost no
ﬁles from the sequential YARA scan. In the following, we