title:Quantifying information leaks in software
author:Jonathan Heusser and
Pasquale Malacaria
See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/221046423
Quantifying information leaks in software
Conference Paper · January 2010
DOI: 10.1145/1920261.1920300 · Source: DBLP
CITATIONS
89
2 authors, including:
Pasquale Malacaria
Queen Mary, University of London
78 PUBLICATIONS   2,231 CITATIONS   
SEE PROFILE
READS
64
Some of the authors of this publication are also working on these related projects:
Security for Internet of Things (IoT) View project
Internet of Things Security View project
All content following this page was uploaded by Pasquale Malacaria on 27 May 2014.
The user has requested enhancement of the downloaded file.
Quantifying Information Leaks in Software
Jonathan Heusser
Pasquale Malacaria
School of Electronic Engineering and Computer
School of Electronic Engineering and Computer
Science
Queen Mary University of London
PI:EMAIL
Science
Queen Mary University of London
PI:EMAIL
ABSTRACT
Leakage of conﬁdential information represents a serious se-
curity risk. Despite a number of novel, theoretical advances,
it has been unclear if and how quantitative approaches to
measuring leakage of conﬁdential information could be ap-
plied to substantial, real-world programs. This is mostly due
to the high complexity of computing precise leakage quant-
ities. In this paper, we introduce a technique which makes
it possible to decide if a program conforms to a quantitat-
ive policy which scales to large state-spaces with the help of
bounded model checking.
Our technique is applied to a number of oﬃcially reported
information leak vulnerabilities in the Linux Kernel. Addi-
tionally, we also analysed authentication routines in the Se-
cure Remote Password suite and of a Internet Message Sup-
port Protocol implementation. Our technique shows when
there is unacceptable leakage; the same technique is also
used to verify, for the ﬁrst time, that the applied software
patches indeed plug the information leaks.
This is the ﬁrst demonstration of quantitative informa-
tion ﬂow addressing security concerns of real-world indus-
trial programs.
Categories and Subject Descriptors
D.4.6 [Security and Protection]: Information ﬂow con-
trols; D.2.4 [Software/Program Veriﬁcation]: Model check-
ing, Correctness proofs; H1.1 [Systems and Information
Theory]: Information theory
General Terms
Security, Theory
Keywords
Information leakage, Linux kernel, Quantitative information
ﬂow
1.
INTRODUCTION
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ACSAC ’10 Dec. 6-10, 2010, Austin, Texas USA
Copyright 2010 ACM 978-1-4503-0133-6/10/12 ...$10.00.
Quantitative Information Flow (QIF) [3, 11] aims to provide
techniques and tools able to quantify leakage of conﬁdential
information. As a motivating example consider a prototyp-
ical password checking program
if (password==guess) access=1; else access=0;
Notice how there is an unavoidable leakage of conﬁdential
information in this program: an attacker observing the value
of access will be able to infer if he guessed the right pass-
word (complete leakage if he did guess it right) and if the
guess was wrong he will have eliminated one possibility from
the search space. Notice also how essential the amount of
information leaked is:
if the amount leaked is very small
then the program could as well be considered secure.
If, as the above example illustrates, leakage is somehow
unavoidable then the real question is not whether or not
programs leak, but how much. This point is what makes
Quantitative Information Flow an appealing theory.
In a
nutshell, QIF aims to measure the amount of information
from conﬁdential data (in the above example the variable
password) that an attacker who can read/write the public
input data (guess) will be able to infer from some observable
variable (access).
However, implementing a precise QIF analysis for secret
sizes of more than a few bits is computationally infeasible;
roughly speaking this is because classical QIF computes the
entropy of a random variable whose complexity is the same
as computing all possible runs of the program. Even when
abstraction techniques and statistical sampling are integ-
rated with QIF [9] to help the scalability issues a useful
analysis for real code still seems problematic.
In this paper, we introduce a useful quantitative analysis
for C code: we will demonstrate the analysis on reported
information leakage vulnerabilities in the Linux Kernel and
in common authentication routines. All of the covered vul-
nerabilities are referenced by the standardised vulnerability
repository CVE from Mitre1.
To address the computational feasibility of the quantitat-
ive analysis we shift the focus from the question “How much
does it leak?” to the simpler quantitative question “Does
it leak more than k?”. We will show how the questions are
related and more importantly we will show that oﬀ-the-shelf
symbolic model checkers like CBMC [5] are able to eﬃciently
answer the second kind of question. CBMC is a good choice
1http://cve.mitre.org, CVE is industry-endorsed with over
70 companies actively involved.
for several reasons: (i) it makes it easy to parse and ana-
lyse large ANSI-C based projects (ii) it models bit-vector
semantics of C accurately which makes it able to detect
arithmetic overﬂows amongst others, which turns out to be
important (iii) nondeterministic choice functions can be used
to easily model user input, which also enjoys eﬃcient solving
due to the symbolic nature of the model checker (iv) despite
being a bounded model checker, CBMC can check whether
enough unwinding of the transition system was performed
to prove that there are no deeper counterexamples.
Our experiments show that the analysis not only quanti-
ﬁes the leakage but also helps in understanding the nature
of the leak. In particular, the counterexample produced by
the model checker, when a leakage property is violated, can
provide insights into the cause of the leak. For example,
we can extract a public user input from the counterexample
needed to trigger a violation.
Another surprising result of our experiment is that in cer-
tain circumstances we were able to use our technique to
prove whether the oﬃcial patch provided for the vulnerab-
ility does actually eliminate the information leak. This is
achieved by point (iv) from above, when the model checking
process is actually complete.
In summary the main technical contributions of this paper
are the following:
1. We present the ﬁrst quantitative leakage analysis of
systems software.
2. We show how to express Quantitative Information Flow
properties that can be eﬃciently checked using bounded
symbolic model checking.
3. We show that the technique not only quantiﬁes leak-
age in real code but also provides valuable information
about the nature of the leak.
4. In some cases we are able to prove that oﬃcial patches
for reported vulnerability do indeed eliminate leakage;
these constitute the ﬁrst positive proofs of absence of
QIF vulnerabilities for real-world systems programs.
2. MODEL OF PROGRAMS AND DISTINC-
TIONS
We aim to model the input/output behaviour of a C func-
tion where inputs are formal arguments to the function and
outputs are either return values or pointer arguments.
In the following we will consider P to be a C function
taking high and low inputs noted h, l; we call observables low
variables whose values are “publicly available” after running
P . As an example consider the following “modulo” program
o = (h % 4 ) + l
and suppose h is a 4 bits variable with values 0..15 and l
a 1 bit variable with values 0,1; then the low input for P
is the variable l and the observable is the variable o whose
possible values are 0 . . . 4.
Formally, a program P is modelled as transition system
T S = (S, T, I, F ) with S being the program states, T ⊆
S × S are the program transitions and I the initial states
and F the ﬁnal states. Let us deﬁne a successor function for
a state s ∈ S
Post(s) = {s
(cid:48) ∈ S | (s, s
(cid:48)
) ∈ T}
A state s is in F if Post(s) = ∅. A path is a ﬁnite sequence
of states π = s0s1s2 . . . sn such that s0 ∈ I and sn ∈ F .
A state is a tuple S = SH × SL of the pair of conﬁd-
ential input H and low input L. We consider initial/ﬁnal
or input/output pairs of states of a path, (cid:104)(h, l), o(cid:105) where
the second component is the output o produced by the ﬁnal
state drawn from some output alphabet O. In the above ex-
ample an input/output pair would be (cid:104)(5, 1), 2(cid:105) representing
the computation (5%4) + 1 = 2.
Conﬁdential inputs are denoted as h ∈ H, low inputs l ∈
L, and low observations o ∈ O, where the output behaviour
of the function is always a low observation and the input is
an initial state (h, l). A distinction on the conﬁdential input
through observations O is one where there exists at least two
paths through P , modelled as T S, which leads to diﬀerent
observations for diﬀerent conﬁdential input but constant low
input.
We deﬁne an equivalence relation (cid:39)P,l on the values of the
high variables as follows: h (cid:39)P,l h(cid:48) iﬀ if (cid:104)(h, l), o(cid:105), (cid:104)(h(cid:48), l), o(cid:48)(cid:105)
are input/output pairs in P then o = o(cid:48).
Hence, two high values are equivalent (w.r.t. a low value
In
l) if they cannot be distinguished by any observable.
the running example an equivalence class in (cid:39)P,1 would for
example be {1, 5, 9, 13}. The equivalence relation associated
to P, l is an element of the set of all possible equivalence
relation on the values of high.
Let I(X) be the set of all possible equivalence relations
on a set X. Deﬁne on I(X) the order:
≈ (cid:118) ∼ ↔ ∀s1, s2 (s1 ∼ s2 ⇒ s1 ≈ s2)
(1)
where ≈,∼ ∈ I(X) and s1, s2 ∈ X. (cid:118) deﬁnes a complete
lattice over X. It is a reﬁnement order with bottom element
being the relation relating every state and top element being
the identity relation. This is described as the Lattice of
Information [10].
Non leaking programs (i.e. satisfying non-interference [7])
are characterised as follows:
Proposition 1. P is non-interfering iﬀ for all l, (cid:39)P,l is
the least element in I(SH ) .
An attacker controlling the low inputs can be modelled
by an equivalence relation (cid:39)P corresponding to a particular
(cid:39)P,l.
Formally, we deﬁne a quantitative policy as a non-negative
natural number N . A relation (cid:39)P,l breaches a policy if | (cid:39)P,l
| > N (where | (cid:39)P,l | is the number of equivalence classes
of (cid:39)P,l).
In our model, an attacker will always choose a
relation breaching the policy, provided that given a policy
and a program such a relation exists. We use (cid:39)P with the
program P being initialised with the attacker’s choice of l2.
In the above example, a choice could be (cid:39)P = (cid:39)P,0 cor-
responding to the program l=0; o = (h % 4 ) + l.
Quantitative Information Flow uses information theoret-
ical measures like Shannon entropy to measure leakage of
conﬁdential information. The measure of a program can be
broken down into two main steps [11, 8]:
1. interpret the program as a random variable RP
2. compute the entropy of RP (noted H(RP ))
2In the paper such attacker choices will be modelled by the
nondeterministic choice function input().
It has been shown that RP and (cid:39)P coincide [11, 13]. For ex-
ample for the modulo program above under the assumption
of uniform distribution on the input there are 4 equivalence
classes each having probability 1
4 . The Shannon entropy of
that program is then
4 ∗ − 1
4
log2(
1
4
) = 2
This number 2 represents the fact that the observations re-
veal which of the 4 possible classes (i.e. 2 bits of information)
the high input belongs to.
RP and (cid:39)P are also order related as the following propos-
ition shows [8]:
Proposition 2. (cid:39)P (cid:118) (cid:39)P (cid:48) iﬀ for all probability distri-
butions H(RP ) ≤ H(RP (cid:48) )
1
To further understand the importance of (cid:39)P in Quantit-
ative Information Flow we need to introduce the information
theoretical concept of channel capacity: consider the pass-
word check example from the introduction. Suppose the
password is a 64 bits randomly chosen string; we have two
equivalence classes, one with 1 element so having probability
264 , the other class with 264−1 elements having thus probab-
ility 1− 1
264 . The entropy is then 3.46944695× 10−18: as ex-
pected a password check of a big password should leak very
little. Suppose however that the probabilities of the high in-
puts are such that both equivalence classes have probability
1
2 . Then the entropy dramatically raises to 1 which is the
channel capacity, i.e. the maximum leakage achievable given
two classes: log2(2) = log2(| (cid:39)P |). In the modulo example
the channel capacity is 2 which happens to be given by the
uniform distribution on the high input. Other distributions
on the high input cannot give higher entropy: for example
if we consider the distribution where all even numbers have
equal probability 1
8 , and all odd numbers have 0 probability
then the resulting entropy will be 1.
The following result establishes basic relationships between
leakage, channel capacity, and number of distinctions:
Proposition 3.
1. P is non-interfering iﬀ log2(| (cid:39)P |) = 0
2. The channel capacity3 of P is log2(| (cid:39)P |) .
3. If for all probability distributions H(RP ) ≤ H(RP (cid:48) )
then | (cid:39)P | ≤ | (cid:39)P (cid:48) |
In other words, a program violates a quantitative policy if
it makes more distinctions than what is allowed in the policy.
A leaking program is one breaching the policy N = 1 in the
above deﬁnition.
We take ideas from assume-guarantee reasoning [17] to en-
code such a policy in a driver function, which tries to trigger
a violation, i.e. producing a counterexample, of the policy.
If the policy states that the function func is not allowed
to make more than 2 distinctions then this is modelled as
shown in Program 1. This driver only has a high component
as a state, which is passed to the function func where the
policy is tested on.
int h1,h2,h3;
int o1,o2,o3;
h1 = input(); h2 = input(); h3 = input();
o1 = func(h1);
o2 = func(h2);
assume(o1 != o2); // (A)
o3 = func(h3);
assert(o3 == o1 || o3 == o2); // (B)
Program 1: Example driver checking for 2 distinctions
Drivers always have a similar structure: we model the
secret by a nondeterministic choice function input() as a
placeholder for all possible values of that type; then for a
policy of checking for N distinctions, the function under
inspection is called N times. The crucial step (A) is the use
of the assume statement after the calls: the driver assumes
that, in this case, there are two diﬀerent return values found
already. The function is called an N + 1th time and at (B)
the driver asserts that the next output is either one of the
previously found outputs.
The assume statement only considers execution paths which
satisfy the given boolean formula, all other paths are rejec-
ted. Further, the bounded model checker used will try to
ﬁnd a counterexample to the negated assertion claim, which
is only satisﬁable if and only if a counterexample exists. An
unsatisﬁable formula means that the original claim holds,
i.e. the program conforms to the policy. The veriﬁcation