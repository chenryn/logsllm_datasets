# 17 \| Embedding+MLP：如何用TensorFlow实现经典的深度学习模型？你好，我是王喆。今天我们正式进入深度学习模型的实践环节，来一起学习并实现一种最经典的模型结构，Embedding+MLP。它不仅经典，还是我们后续实现其他深度学习模型的基础，所以你一定要掌握好。这里面的 Embedding 我们已经很熟悉了，那什么叫做 MLP 呢？它其实是Multilayerperceptron，多层感知机的缩写。感知机是神经元的另外一种叫法，所以多层感知机就是多层神经网络。讲到这里啊，我想你脑海中已经有这个模型结构的大致图像了。今天，我就以微软著名的深度学习模型Deep Crossing 为例，来给你详细讲一讲 Embedding+MLP模型的结构和实现方法。Embedding+MLP 模型的结构图 1 展示的就是微软在 2016 年提出的深度学习模型 DeepCrossing，微软把它用于广告推荐这个业务场景上。它是一个经典的Embedding+MLP 模型结构，我们可以看到，Deep Crossing 从下到上可以分为 5层，分别是 Feature 层、Embedding 层、Stacking 层、MLP 层和 Scoring层。 接下来，我就从下到上来给你讲讲每一层的功能是什么，以及它们的技术细节分别是什么样的。![](Images/7f9697c2b16f054fc868adae40bbaec4.png)savepage-src="https://static001.geekbang.org/resource/image/50/71/5076071d3c69d3a9fff848a9e631f371.jpeg"}图1 经典的Embedding+MLP模型结构（图片来自 Deep Crossing - Web-ScaleModeling without Manually Crafted Combinatorial Features）我们先来看 Feature 层。Feature 层也叫做输入特征层，它处于 DeepCrossing 的最底部，作为整个模型的输入。仔细看图 1的话，你一定会发现不同特征在细节上的一些区别。比如 Feature#1向上连接到了 Embedding 层，而 Feature#2 就直接连接到了更上方的 Stacking层。这是怎么回事呢？原因就在于 Feature#1 代表的是类别型特征经过 One-hot编码后生成的特征向量，而 Feature#2 代表的是数值型特征。我们知道，One-hot特征太稀疏了，不适合直接输入到后续的神经网络中进行训练，所以我们需要通过连接到Embedding 层的方式，把这个稀疏的 One-hot 向量转换成比较稠密的 Embedding向量。 接着，我们来看 Embedding 层。Embedding 层就是为了把稀疏的 One-hot向量转换成稠密的 Embedding 向量而设置的，我们需要注意的是，Embedding层并不是全部连接起来的，而是每一个特征对应一个 Embedding 层，不同Embedding 层之间互不干涉。那 Embedding 层的内部结构到底是什么样子的呢？我想先问问你，你还记得Word2vec 的原理吗？Embeding 层的结构就是 Word2vec模型中从输入神经元到隐层神经元的部分（如图 2红框内的部分）。参照下面的示意图，我们可以看到，这部分就是一个从输入层到隐层之间的全连接网络。![](Images/153d8cf44cfe4adfa87a70f8b6dea243.png)savepage-src="https://static001.geekbang.org/resource/image/8a/29/8a26d9a531ae8bef89f3730388f59a29.jpeg"}图2 Word2vec模型中Embedding层的部分一般来说，Embedding向量的维度应远小于原始的稀疏特征向量，按照经验，几十到上百维就能够满足需求，这样它才能够实现从稀疏特征向量到稠密特征向量的转换。接着我们来看 Stacking 层。Stacking层中文名是堆叠层，我们也经常叫它连接（Concatenate）层。它的作用比较简单，就是把不同的Embedding特征和数值型特征拼接在一起，形成新的包含全部特征的特征向量。再往上看，MLP 层就是我们开头提到的多层神经网络层，在图 1 中指的是Multiple Residual Units 层，中文叫多层残差网络。微软在实现 Deep Crossing时针对特定的问题选择了残差神经元，但事实上，神经元的选择有非常多种，比如我们之前在深度学习基础知识中介绍的，以Sigmoid 函数为激活函数的神经元，以及使用 tanh、ReLU等其他激活函数的神经元。我们具体选择哪种是一个调参的问题，一般来说，ReLU最经常使用在隐层神经元上，Sigmoid则多使用在输出神经元，实践中也可以选择性地尝试其他神经元，根据效果作出最后的决定。不过，不管选择哪种神经元，MLP层的特点是全连接，就是不同层的神经元两两之间都有连接。就像图 3中的两层神经网络一样，它们两两连接，只是连接的权重会在梯度反向传播的学习过程中发生改变。![](Images/5639d454dc6fd1101547d60090093cc6.png)savepage-src="https://static001.geekbang.org/resource/image/7a/99/7a2b22c106c454af3db2edaed555e299.jpeg"}图3 全连接神经网络MLP层的作用是让特征向量不同维度之间做充分的交叉，让模型能够抓取到更多的非线性特征和组合特征的信息，这就使深度学习模型在表达能力上较传统机器学习模型大为增强。最后是 Scoring层，它也被称为输出层。虽然深度学习模型的结构可以非常复杂，但最终我们要预测的目标就是一个分类的概率。如果是点击率预估，就是一个二分类问题，那我们就可以采用逻辑回归作为输出层神经元，而如果是类似图像分类这样的多分类问题，我们往往在输出层采用softmax 这样的多分类模型。到这里，我们就讲完了 Embedding+MLP的五层结构。它的结构重点用一句话总结就是，**对于类别特征，先利用 Embedding 层进行特征稠密化，再利用Stacking 层连接其他特征，输入 MLP 的多层结构，最后用 Scoring层预估结果。**Embedding+MLP 模型的实战现在，我们从整体上了解了 Embedding+MLP模型的结构，也许你对其中的细节实现还有些疑问。别着急，下面我就带你用SparrowRecsys 来实现一个 Embedding+MLP的推荐模型，帮你扫清这些疑问。实战中，我们按照构建推荐模型经典的步骤，特征选择、模型设计、模型实现、模型训练和模型评估这5 步来进行实现。首先，我们来看看特征选择和模型设计。特征选择和模型设计在上一节的实践准备课程中，我们已经为模型训练准备好了可用的训练样本和特征。秉着"类别型特征Embedding 化，数值型特征直接输入 MLP"的原则，我们选择movieId、userId、movieGenre、userGenre 作为 Embedding化的特征，选择物品和用户的统计型特征作为直接输入 MLP的数值型特征，具体的特征选择你可以看看下面的表格：![](Images/5e5855e53d12a96079759090f6ceb9d1.png)savepage-src="https://static001.geekbang.org/resource/image/af/94/af3fabdcb119c9e06ddc0f7225bbc094.jpg"}选择好特征后，就是 MLP 部分的模型设计了。我们选择了一个三层的 MLP结构，其中前两层是 128 维的全连接层。我们这里采用好评 /差评标签作为样本标签，因此要解决的是一个类 CTR预估的二分类问题，对于二分类问题，我们最后一层采用单个 sigmoid神经元作为输出层就可以了。当然了，我知道你肯定对这一步的细节实现有很多问题，比如为什么要选三层的MLP 结构，为什么要选 sigmoid作为激活函数等等。其实，我们对模型层数和每个层内维度的选择是一个超参数调优的问题，这里的选择不能保证最优，我们需要在实战中需要根据模型的效果进行超参数的搜索，找到最适合的模型参数。Embedding+MLP 模型的 TensorFlow 实现确定好了特征和模型结构，万事俱备，只欠实现了。下面，我们就看一看利用TensorFlow 的 Keras 接口如何实现 Embedding+MLP的结构。总的来说，TensorFlow 的实现有七个步骤。因为这是我们课程中第一个TensorFlow的实现，所以我会讲得详细一些。而且，我也把全部的参考代码放在了SparrowRecsys 项目 TFRecModel 模块的EmbeddingMLP.py，你可以结合它来听我下面的讲解。**我们先来看第一步，导入 TensorFlow包。** 如果你按照实战准备一的步骤配置好了 TensorFlow Python环境，就可以成功地导入 Tensorflow包。接下来，你要做的就是定义好训练数据的路径 TRAIN_DATA_URL了，然后根据你自己训练数据的本地路径，替换参考代码中的路径就可以了。    import tensorflow as tf    TRAIN_DATA_URL = "file:///Users/zhewang/Workspace/SparrowRecSys/src/main/resources/webroot/sampledata/modelSamples.csv"    samples_file_path = tf.keras.utils.get_file("modelSamples.csv", TRAIN_DATA_URL)**第二步是载入训练数据**，我们利用 Tensorflow 自带的 CSV数据集的接口载入训练数据。注意这里有两个比较重要的参数，一个是label_name，它指定了 CSV 数据集中的标签列。另一个是batch_size，它指定了训练过程中，一次输入几条训练数据进行梯度下降训练。载入训练数据之后，我们把它们分割成了测试集和训练集。    def get_dataset(file_path):        dataset = tf.data.experimental.make_csv_dataset(            file_path,            batch_size=12,            label_name='label',            na_value="?",            num_epochs=1,            ignore_errors=True)        return dataset    