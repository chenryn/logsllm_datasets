### Distribution of Severity Fields in System Logs

**Table 5: Severity Field Distribution for BG/L Messages and Alerts**

| Severity  | Total Count | % of Total | Alert Count | % of Alerts |
|-----------|-------------|------------|-------------|-------------|
| FATAL     | 855,501     | 18.02%     | 348,398     | 99.98%      |
| FAILURE   | 171         | 0.03%      | 6           | 0.02%       |
| SEVERE    | 19,213      | 0.41%      | 0           | 0.00%       |
| ERROR     | 112,355     | 2.37%      | 0           | 0.00%       |
| WARNING   | 23,357      | 0.49%      | 0           | 0.00%       |
| INFO      | 3,735,823   | 78.68%     | 0           | 0.00%       |

**Note:** Tagging all FATAL/FAILURE severity messages as alerts would result in a 59% false positive rate.

**Table 6: Severity Field Distribution for Red Storm Syslogs**

| Severity  | Total Count | % of Total | Alert Count | % of Alerts |
|-----------|-------------|------------|-------------|-------------|
| EMERG     | 3           | 0.00%      | 0           | 0.00%       |
| ALERT     | 654         | 0.00%      | 0           | 0.00%       |
| CRIT      | 1,552,910   | 6.09%      | 1,550,217   | 98.69%      |
| ERR       | 2,027,598   | 7.95%      | 11,784      | 0.75%       |
| WARNING   | 2,154,944   | 8.45%      | 270         | 0.02%       |
| NOTICE    | 3,759,620   | 14.74%     | 0           | 0.00%       |
| INFO      | 15,722,695  | 61.63%     | 8,450       | 0.54%       |
| DEBUG     | 291,764     | 1.14%      | 0           | 0.00%       |

**Note:** These syslog alerts were predominantly disk failure messages with CRIT severity. The data suggest that syslog severity is not a reliable failure indicator.

### Alert Identification Challenges

Automatically identifying alerts in system logs is an open problem. To facilitate others in tackling this challenge, we provide the following observations from our manual tagging process:

1. **Insufficient Context:**
   - Many log messages are ambiguous without external context. For example, a message from BG/L:
     ```
     YY-MM-DD-HH:MM:SS NULL RAS BGL MASTER FAILURE ciodb exited normally with exit code 0
     ```
     This message has a high severity (FAILURE), but the body suggests a clean exit. If the system administrator was performing maintenance, this message is harmless. However, if generated during normal operation, it indicates that all running jobs on the supercomputer were killed. Operational context is crucial for accurate interpretation.

2. **Asymmetric Reporting:**
   - Some failures leave no evidence in the logs, while other messages are meaningless. Single failure types can produce varying alert signatures. For example, the Red Storm DDN system generates many different patterns for "disk failure."

3. **System Evolution:**
   - Log analysis is a moving target. Software upgrades and configuration changes can alter the meaning or character of logs, making machine learning difficult. The ability to detect phase shifts in behavior would be valuable for relearning or applying existing models.

4. **Implicit Correlation:**
   - Groups of messages are sometimes fundamentally related, but there is no explicit indication. Common correlations result from cascading failures.

5. **Inconsistent Structure:**
   - Despite standards like BSD syslog, log messages vary greatly within and across systems. Understanding entries may require parsing unstructured message bodies, reducing the problem to natural language processing.

6. **Corruption:**
   - Even on highly engineered systems, log entries can be corrupted. We observed truncated, partially overwritten, and incorrectly timestamped messages.

### Filtering

A single failure may generate multiple alerts across nodes. Filtering reduces related sets of alerts to a single initial alert per failure. Our algorithm, based on previous work [9, 10] with some optimizations, removes an alert if any source had generated that category of alert within the last T seconds, for a given threshold T. Two alerts are in the same category if they were both tagged by the same expert rule.

**Algorithm: LOGFILTER(A)**
```plaintext
l ← 0
for i ← 1 to N do
    if ti − l > T then
        clear(X)
        l ← ti
    if ci ∈ X and ti − X[ci] < T then
        X[ci] ← ti
    else
        X[ci] ← ti
        output(ai)
```

This filter may remove independent alerts of the same category that, by coincidence, happen near the same time on different nodes. In some cases, serial filtering fails to remove alerts that share a root cause, which a human would consider redundant. Our filter tends to remove fewer true positives and more false positives, which is critical in fault and intrusion detection systems.

### Analysis

Modeling the timing of failure events is common in systems research. Frequently, failures are modeled as occurring independently (exponential inter-arrival times). For low-level failures triggered by physical phenomena, these models are appropriate. For most other kinds of failures, however, this independence is not an appropriate assumption. Failure prediction based on time interdependence of events has been shown to improve job scheduling, QoS, and checkpointing.

We expected CPU clocking alerts to behave similarly to ECC alerts, driven by a basic physical process. However, we observed clear spatial correlations, discovering a bug in the Linux SMP kernel that sped up the system clock under heavy network load. This highlights the diversity of supercomputer failure types and the need for more sophisticated modeling techniques.

---

This revised text aims to be more structured, clear, and professional, with a focus on clarity and coherence.