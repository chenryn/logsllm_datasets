│ icu │ true │ true │
│ json │ true │ true │
│ parquet │ true │ true │
│ tpch │ true │ true │
│ httpfs │ false │ false │
│ inet │ false │ false │
│ jemalloc │ false │ false │
│ motherduck │ false │ false │
│ postgres_scanner │ false │ false │
│ spatial │ false │ false │
│ sqlite_scanner │ false │ false │
│ tpcds │ false │ false │
│ excel │ true │ │
├──────────────────┴─────────┴───────────┤
│ 15 rows 3 columns │
└────────────────────────────────────────┘
You can install any extension by typing the INSTALL command followed by the
extension’s name. The extension will then be installed in your database, but not loaded.
To load an extension, type LOAD followed by the same name. The extension mechanism
is idempotent, meaning you can issue both commands several times without running into
errors.
NOTE Since version 0.8 of DuckDB, the database will auto-load extensions that are installed, if it
can determine that they are needed, so you might not need the LOAD command.
By default, DuckDB cannot query files that live elsewhere on the internet, but that
capability is available via the official httpfs extension. If it is not already in your
distribution, you can install and load the httpfs extension. This extension lets us directly
query files hosted on an HTTP(S) server without having to download the files locally, it
also supports S3 and some other cloud storage providers.
© Manning Publications Co. To comment go to liveBook
17
INSTALL httpfs;
LOAD httpfs;
We can then check where that’s been installed by entering:
FROM duckdb_extensions()
SELECT loaded, installed, install_path
WHERE extension_name = 'httpfs';
You should see the following output:
┌─────────┬───────────┬─────────────────────────────────----┐
│ loaded │ installed │ install_path │
│ boolean │ boolean │ varchar │
├─────────┼───────────┼─────────────────────────────────────┤
│ true │ true │ /path/to/httpfs.duckdb_extension │
└─────────┴───────────┴─────────────────────────────────────┘
We can see that this extension has now been loaded and installed, as well as the location
where it’s been installed.
2.5 Analyzing a CSV file with the DuckDB CLI
We’re going to start with a demonstration of the CLI for a common task for any data
engineer — making sense of the data in a CSV file! It doesn’t matter where our data is
stored, be it on a remote HTTP server or cloud storage (S3, GCP, HDFS), DuckDB can
process it now directly without having to do a manual download and import process. As
the ingestion of many supported file formats, such as CSV and Parquet, is parallelized by
default, it should be super quick to get your data into DuckDB.
We went looking for CSV files on GitHub and came across a dataset that contains
population numbers of countries. We can write the following query to count the number
of records:
SELECT count(*)
FROM 'https://github.com/bnokoro/Data-Science/raw/master/
➥countries%20of%20the%20world.csv';
If we run this query, we should see the following output, indicating that we’ve got
population data for over 200 countries:
© Manning Publications Co. To comment go to liveBook
18
┌──────────────┐
│ count_star() │
│ int64 │
├──────────────┤
│ 227 │
└──────────────┘
If, as is the case here, our URL or file name ends in a specific extension (e.g. .csv),
DuckDB will automatically process it. But what if we try to automatically process a short
link of that same CSV file?
SELECT count(*)
FROM 'https://bit.ly/3KoiZR0';
Running this query results in the following error:
Error: Catalog Error: Table with name https://bit.ly/3KoiZR0 does not exist!
Did you mean "Player"?
LINE 1: select count(*) from 'https://bit.ly/3KoiZR0';
Although it’s a CSV file, DuckDB doesn’t know that because it doesn’t have a .csv suffix.
We can solve this problem by using the read_csv_auto function, which processes the
provided URI as if it was a CSV file, despite its lack of .csv suffix. The updated query is
shown in listing 2.5:
Listing 2.5 Specifying the format of a remote file
SELECT count(*)
FROM read_csv_auto("https://bit.ly/3KoiZR0");
This query will return the same result as the query that used the canonical link from
which the format could be deduced.
2.5.1 Result modes
For displaying the results, you can choose between different modes using .mode
. You can see a list of available modes by typing .help mode.
Throughout this chapter, we’ve been using 'duckbox' mode, which returns a flexible
table structure. DuckDB comes with a series of different modes, which broadly fit into a
couple of categories:
Table based, which work well with fewer columns — duckbox, box, csv,
ascii, table, list, column
© Manning Publications Co. To comment go to liveBook
19
Line based, which work well with more columns — json, jsonline,
line
There are then some others that don’t fit into those categories, including html, insert,
and trash (no output).
Our first query counted the number of records in the CSV file, but it’d be interesting to
know what columns it has. There are a lot of columns that will get truncated if we use the
default mode, so we’re going to change to the line mode before running the query:
.mode line -- #1
SELECT *
FROM read_csv_auto("https://bit.ly/3KoiZR0")
LIMIT 1;
#1 Changing to line mode
The results of running this query are shown in listing 2.6.
Listing 2.6 A result in line mode
Country = Afghanistan
Region = ASIA (EX. NEAR EAST)
Population = 31056997
Area (sq. mi.) = 647500
Pop. Density (per sq. mi.) = 48,0
Coastline (coast/area ratio) = 0,00
Net migration = 23,06
Infant mortality (per 1000 births) = 163,07
GDP ($ per capita) = 700
Literacy (%) = 36,0
Phones (per 1000) = 3,2
Arable (%) = 12,13
Crops (%) = 0,22
Other (%) = 87,65
Climate = 1
Birthrate = 46,6
Deathrate = 20,34
Agriculture = 0,38
Industry = 0,24
Service = 0,38
© Manning Publications Co. To comment go to liveBook
20
As you can see from the output, line mode takes up a lot more space than duckbox,
but we’ve found it to be the best mode for doing initial exploration of datasets that have
plenty of columns. You can always change back to another mode once you’ve decided a
subset of columns that you’d like to use.
The dataset has lots of interesting information about various countries. Let’s write a
query to count the number of countries, and find the maximum population average area
across all countries. This query only returns a few columns, so we’ll switch back to
duckbox mode before running the query:
.mode duckbox
SELECT count(*) AS countries,
max(Population) AS max_population,
round(avg(cast("Area (sq. mi.)" AS decimal))) AS avgArea
FROM read_csv_auto("https://bit.ly/3KoiZR0");
┌───────────┬────────────────┬──────────┐
│ countries │ max_population │ avgArea │
│ int64 │ int64 │ double │
├───────────┼────────────────┼──────────┤
│ 227 │ 1313973713 │ 598227.0 │
└───────────┴────────────────┴──────────┘
So far, no tables have been created in the process, and we’ve just touched the tip of the
iceberg demonstrating what DuckDB actually can do. While the examples above have all
been run in interactive mode, the DuckDB CLI can also run in a non-interactive fashion.
It can read from standard input and write to standard output. This makes it possible to
build all sorts of pipelines.
Let’s conclude with a script that extracts the populate, birth rate, and death rate in
countries in Western Europe and creates a new local CSV file containing that data. We
can either .exit from the DuckDB CLI or open another tab before running the command
below:
duckdb -csv \
-s "SELECT Country, Population, Birthrate, Deathrate
FROM read_csv_auto('https://bit.ly/3KoiZR0')
WHERE trim(region) = 'WESTERN EUROPE'" \
> western_europe.csv
The first few lines of western_europe.csv can be viewed with a command line tool or
text editor. If we use the head tool, we could find the first 5 lines like this:
head -n5 western_europe.csv
© Manning Publications Co. To comment go to liveBook
21
And the output would look like table 2.1:
Table 2.1 First five lines of western_europe.csv showing population, birth rate, and death rate of some
countries in Western Europe
Country Population Birthrate Deathrate
Andorra 71201 8,71 6,25
Austria 8192880 8,74 9,76
Belgium 10379067 10,38 10,27
Denmark 5450661 11,13 10,36
We can also create Parquet files, but for that we can’t pipe the output straight into a file
with a Parquet extension. Instead, we can use the COPY … TO clause with stdout as the
destination:
Listing 2.7 Writing explicitly to standard out so that options can be specified
duckdb \
-s "COPY (
SELECT Country, Population, Birthrate, Deathrate
FROM read_csv_auto('https://bit.ly/3KoiZR0')
WHERE trim(region) = 'WESTERN EUROPE'
) TO '/dev/stdout' (FORMAT PARQUET)" \
> western_europe.parquet
You could then view the contents of the Parquet file using any Parquet reader, perhaps
even DuckDB itself!
duckdb -s "FROM 'western_europe.parquet' LIMIT 5"
The results will be the same as seen in table 2.1.
TIP Repeated configuration and usage can be stored in a config file that lives at
$HOME/.duckdbrc. This file is read during startup and all commands in it - both dot
commands and SQL commands are executed via one .read command. This allows you to store
both the configuration state of the CLI and anything you might want to initialize with SQL
commands.
An example of something that might go in the duckdbrc file is a custom prompt and welcome
message when you launch DuckDB:
© Manning Publications Co. To comment go to liveBook
22
-- Duck head prompt
.prompt '⚫◗ '
-- Example SQL statement
select 'Begin quacking!' as "Ready, Set, ...";
2.6 Summary
The DuckDB Database with command-line interface (CLI) can be
installed as CLI on Windows, Linux and OSX.
DuckDB is available as a library for Python, R, Java, Javascript, Julia,
C/C++, ODBC, WASM, and Swift.
The CLI supports additional dot commands for controlling outputs,
reading files, built-in help, and more.
With .mode you can use several display modes, including duckbox,
line, and ascii.
You can query CSV files directly from an HTTP server by installing the
https extension.
You can use the CLI as a step in any data pipeline, without creating
tables, by querying external datasets and writing results to standard
out or other files.
© Manning Publications Co. To comment go to liveBook
23
3
Executing SQL Queries
This chapter covers
The different categories of SQL statements and their fundamental structure
Creating tables and structures for ingesting a real world dataset
Laying the fundamentals for analyzing a huge dataset in detail
Exploring DuckDB-specific extensions to SQL
Now that you’ve learned about the DuckDB CLI, it’s time to tickle your SQL brain. We will
be using the CLI version of DuckDB throughout this chapter. However, all the examples
here can be fully applied from within any of the supported environments, such as the
Python client, the Java JDBC driver, or any of the other supported language interfaces.
In this chapter, we will quickly go over some basic and necessary SQL statements and
then move on to more advanced querying. In addition to explaining SQL basics, we’ll
also be covering more complex topics like common table expressions and window
functions. DuckDB supports both of these and this chapter will teach you how to build
queries for doing the best possible in-memory Online Analytical Processing (OLAP) with
DuckDB.
To get the sample up and running you should have an idea about data ingestion with
DuckDB as shown in Chapter 2, especially how to ingest CSV files, and deal with implicit
(automatic) or explicit column detection. Knowledge of the data types presented in
Chapter 1 will also be helpful. If you want to go straight to querying data, please jump to
section 3.4.3, in which we discuss SQLs SELECT statement in detail. We think it’s better to
start by defining tables and structures first, populating them with data, and then querying
them, rather than making up queries on generated or non-existent data.
© Manning Publications Co. To comment go to liveBook
24
3.1 A quick SQL recap
SQL queries are composed of several statements which are in turn composed of clauses.
A command is a query submitted to the CLI or any other of the supported clients.
Commands in the DuckDB CLI are terminated with a semicolon. Whitespaces can be used
freely in SQL commands. Either align your commands beautifully or type them all in one
line, it doesn’t matter. SQL is case-insensitive about keywords and identifiers.
Most statements support several clauses that change their behavior, most prominently
the WHERE, GROUP BY and ORDER BY clauses. WHERE adds conditions on which rows are
included in the final result, GROUP BY aggregates many values into buckets defined by
one or more keys and ORDER BY specifies the order of results returned.
You will learn all relevant statements and clauses for your analytical workloads based
on a concrete example. We picked energy production from photovoltaics as a relevant,
real-world example.
3.2 Analyzing energy production
Energy consumption and production has been the subject of OLAP related analysis for a
while. Smart meters measuring consumption in 15 minute intervals have been available
to many industries—such as metal processing and large production plants—for some
time now, and have become quite standard. These measurements are used to price the
consumed energy, forecast consumption, and more.
With the rise of smart monitoring systems, detailed energy readings are now available
in private households as well, becoming more mainstream each year. Imagine you have
a photovoltaic grid and smart meter installed at your house. You want to be able to plan
your electricity usage a bit or forecast an amortization of your grid, the same way large
industries can. To do so, you don’t have to go into a full time-series database and a live
dashboard to do that. DuckDB and the examples we use throughout this chapter might
give you a good starting point for creating your own useful reports.
The dataset that we are going to use in the following examples is available from the
US Department of Energy under the name Photovoltaic Data Acquisition (PVDAQ). The
dataset is documented in here. The National Renewable Energy Laboratory from the
Department of Energy also offers a nice, simple API for getting the partitioned CSV /
Parquet files via PVDAQ. Access is free and requires little personal information. The
dataset is published under the Creative Commons Attribution License. Parts of the
dataset are redistributed unchanged for ease of access in this chapter with the sources
of this book.
© Manning Publications Co. To comment go to liveBook
25
NOTE Why are we storing measurements in 15-minute or quarterly hour intervals when modern
sensors produce much finer measurements? 15 minutes turned out to be fit enough for the
aforementioned purposes such as pricing and buying smart intervals while at the same time are
"small" enough to be handled with ease in most relational systems these days. The power output
or consumption is measured in Watt (W) or Kilowatt (kW) and billed or sold usually as Kilowatt-
hour (kWh). 15-minute intervals of Watts are easily converted to kWh, while still accurate enough
for good production charts. In most cases you want to smooth the values at least on an hourly
basis, peaks and dips due to clouds are often irrelevant. If you look at a chart for forecasting,