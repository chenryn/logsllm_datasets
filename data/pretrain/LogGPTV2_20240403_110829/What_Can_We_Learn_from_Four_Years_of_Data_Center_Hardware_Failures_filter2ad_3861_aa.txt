# What Can We Learn from Four Years of Data Center Hardware Failures?

**Authors:**
- Guosai Wang
- Lifei Zhang
- Wei Xu

**Affiliations:**
- Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China
- Baidu, Inc., China

**Contact:**
- PI:EMAIL (Guosai Wang)
- PI:EMAIL (Lifei Zhang)
- PI:EMAIL (Wei Xu)

**Abstract:**
Hardware failures significantly impact the dependability of large-scale data centers. This study analyzes over 290,000 hardware failure reports collected over four years from dozens of data centers with hundreds of thousands of servers. We statistically examine failure characteristics across temporal, spatial, product line, and component dimensions. We focus on correlations among different failures, including batch and repeating failures, and the human operators' response to these failures. Our findings reconfirm or extend previous studies and reveal new failure and recovery patterns that are unintended consequences of modern data center hardware and software design.

## I. Introduction

To meet the growing demand for Internet services, extremely large-scale data centers have been built. Hardware reliability is a critical factor in the overall dependability of these IT infrastructures. Hardware failures are common in large-scale systems and can lead to service level agreement (SLA) violations and significant revenue loss.

Understanding the failure model is crucial for balancing software stack complexity, hardware and operational costs, and reducing the total cost of ownership (TCO) in data centers. Researchers have studied hardware failures for decades, with patterns evolving as computer system designs change. Recent studies often focus on supercomputers or specific components like memory or hard drives, rather than commodity data centers.

Modern data centers differ in several ways:
- **Positive Aspects:**
  - More reliable hardware components.
  - Improved failure detection systems.
  - Accumulated experience in maintaining large-scale infrastructures.
- **Negative Aspects:**
  - Increased cost-sensitivity leading to the use of less-reliable, commodity, or custom-ordered hardware.
  - Greater heterogeneity in both hardware components and workloads, resulting in more complex failure models.

Interestingly, while it is commonly believed that hardware unreliability shapes software fault tolerance design, we argue that the reverse is also true: improvements in software-based fault tolerance have led to a reduced emphasis on hardware dependability.

Given these changes, a new study on failures in modern large-scale Internet data centers is necessary. This paper presents a comprehensive analysis of failures over the past four years from a major Internet service company operating dozens of data centers with hundreds of thousands of servers, serving hundreds of millions of users daily. Unlike previous studies, our data centers host generations of heterogeneous hardware, both commodity and custom-designed, and support hundreds of different product lines. We cover all hardware component classes and human operator behavior.

## II. Methodology and Datasets

### A. Dataset Overview

Our dataset includes all hardware failure operation tickets (FOTs) collected over four years from a major Internet service company. The FOTs are categorized into three types:
- **D fixing:** 70.3% (Issue a repair order)
- **D error:** 28.0% (Not repaired; set to decommission)
- **D falsealarm:** 1.7% (Marked as a false alarm)

Operators do not repair D error failures mainly because the servers are out-of-warranty. The typical action for D fixing is to issue a repair order (RO). Over 25% of the failures are in out-of-warranty hardware and thus are not handled. Partially failed but operational servers remain in production, while totally broken ones are decommissioned. The false alarm rate is very low, indicating the effectiveness of hardware failure detection.

### B. Failure Management System (FMS)

The FMS has agents on most hosts that detect hardware component failures. A centralized server collects all FOTs for operator review. FOTs come from two sources: programmatic failure detectors and human operators. The FMS records over 70 types of failures covering nine component classes, including hard drives, SSDs, RAID cards, flash cards, memory, motherboards, CPUs, fans, and power supplies. There is a special class, "miscellaneous," for manually entered failures, which account for about 10% of the FOTs.

Each FOT contains fields such as:
- **Error type**
- **Error time**
- **Host ID**
- **Hostname**
- **IDC (data center)**
- **Error device**
- **Error detail**
- **Error position**

### C. Analytical Methods

We characterize the temporal and spatial properties of hardware failures by plotting probability density functions (PDF) and cumulative distribution functions (CDF). We conduct hypothesis tests to verify how well the observed distributions fit known probability distributions, including uniform, exponential, Weibull, gamma, and lognormal. We estimate parameters using maximum likelihood estimation (MLE) and use Pearsonâ€™s chi-squared test to determine if the null hypothesis can be rejected at a certain significance level.

## III. Temporal Distribution of the Failures

The common belief is that components can fail at any time in a uniformly random way. In this section, we investigate the temporal distribution of hardware failures, focusing on the error time or failure detection timestamp in D fixing and D error FOTs.

[Continued in the next section...]

---

This revised version aims to improve clarity, coherence, and professionalism. The structure and flow have been refined, and the language has been made more precise and formal.