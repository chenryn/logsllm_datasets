title:What Can We Learn from Four Years of Data Center Hardware Failures?
author:Guosai Wang and
Lifei Zhang and
Wei Xu
2017 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
What Can We Learn from Four Years of
Data Center Hardware Failures?
Guosai Wang, Wei Xu
Institute for Interdisciplinary Information Sciences
Tsinghua University, Beijing, China
PI:EMAIL, PI:EMAIL
Lifei Zhang
Baidu, Inc., China
PI:EMAIL
Abstract—Hardware failures have a big impact on the de-
pendability of large-scale data centers. We present studies on
over 290,000 hardware failure reports collected over the past four
years from dozens of data centers with hundreds of thousands of
servers. We examine the dataset statistically to discover failure
characteristics along the temporal, spatial, product line and
component dimensions. We speciﬁcally focus on the correlations
among different failures, including batch and repeating failures,
as well as the human operators’ response to the failures. We
reconﬁrm or extend ﬁndings from previous studies. We also ﬁnd
many new failure and recovery patterns that are the undesirable
by-product of the state-of-the-art data center hardware and
software design.
I.
INTRODUCTION
To meet the ever-growing demand for Internet services,
people build extremely large-scale data centers. The hardware
reliability has always been a crucial factor in the overall
dependability of these IT infrastructures. Hardware failures are
common in large-scale systems [1, 2], and these failures may
cause service level agreement (SLA) violations and severe loss
of revenue [3].
It is important to understand the failure model, as it helps
to strike the right balance among software stack complexity,
hardware and operation cost, reducing the total cost of own-
ership (TCO) in data centers. In fact, researchers have studied
hardware failures for decades, from Gray’s study in 1986 [4]
to Oppenheimer’s in 2003 [2] to the recent research [5–19],
failure patterns change over time as the computer system
design evolves. Many recent studies focus on supercomputers
instead of commodity data centers or a single component such
as memory or hard drive [11, 16–21].
Today’s data centers are different in many aspects. On
the positive side, the hardware components are designed and
manufactured to be more reliable. There are also better failure
detection systems making hardware failures easier to notice
and repair. Moreover, operators have accumulated more expe-
rience on how to maintain large-scale infrastructures. However,
on the dark side, Internet companies become more cost-
sensitive, and people adopt less-reliable, commodity or custom
ordered hardware [1]. There is also more heterogeneity in
both hardware components and workload, resulting in more
complex failure models.
Most interestingly, in addition to the common belief that
hardware unreliability shapes the software fault tolerance de-
sign, we believe it is also the other way around: years of
improvement in software-based fault tolerance has indulged
operators to care less about hardware dependability.
With all
the changes, we believe it
is now necessary
to conduct a new study on failures in modern large-scale
Internet data centers. In this paper, we present a comprehensive
analysis of failures during the past four years from a major
and successful Internet service company that operates dozens
of data centers hosting hundreds of thousands of servers,
serving hundreds of millions of users every day. Different from
previous studies on a single supercomputer, our data centers
host generations of heterogeneous hardware, both commodity
and custom design, and support hundreds of different product
lines. Also, we cover all hardware component classes as well
as human operators’ behavior in the study.
Speciﬁcally, we analyze all the hardware failure operation
tickets (FOTs) collected from a centralized failure management
system (FMS) that monitors most of the servers and records
all component failures together with the operators’ actions.
We observe over 290, 000 such FOTs during the past four
years. We draw statistically meaningful conclusions about
these failures.
In the paper, we study the failures along different dimen-
sions, including time, space, product lines owning the servers,
operator’s response, etc. We ﬁrst explore the temporal and
spatial distributions of failures in different components. Then
we focus on correlated failures as people believe they affect
software fault tolerance the most. Finally, we describe the
operators’ response to these failures.
To our best knowledge, this is the largest comprehensive
hardware failure study focusing on commodity Internet data
centers in a decade. We cover all major components as well as
the human operator behaviors. During the study, we conﬁrm
or extend many counter-intuitive observations from previous
studies, and observe many new patterns. For example,
1) Failures are not uniformly random at different time
scales, and sometimes not even uniformly random at different
spaces in a data center. We see many correlated failures. We
even observe batch failures affecting thousands of servers in
a few hours. These observations contradict software design
assumptions of independent and uniformly random failures.
2) The time between failures (TBF), both at a data center
scale and at an individual component scale, is hard to model
with a well-known distribution. Different components exhibit
highly distinctive failure patterns.
2158-3927/17 $31.00 © 2017 IEEE
DOI 10.1109/DSN.2017.26
25
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:01:13 UTC from IEEE Xplore.  Restrictions apply. 
TABLE I.
CATEGORIES OF FAILURE OPERATION TICKETS.
Failure trace
D ﬁxing
D error
D falsealarm
Handling decision
Issue a repair order (RO)
Not repair and set to decommission
Mark as a false alarm
Percentage
70.3%
28.0%
1.7%
3) Despite the fact that the traditional doctrine says that
we should reduce mean time to recover (MTTR) to improve
dependability, operators are reluctant to respond to hardware
failures in many cases. Also, the operators spend less time
debugging a failure to ﬁnd the root cause, but are more likely to
order a replacement for the component. They sometimes even
leave the failures unhandled, if the server is out-of-warranty.
These observations are highly related to the new software
design and workload in the data centers, as we will discuss in
the paper. These failure patterns suggest not only better ways
to develop new failure management systems but also calls for
a new methodology for designing fault handling mechanisms.
The remainder of the paper is organized as follows: Sec-
tion II introduces the dataset and our analysis methodology.
Section III and IV discuss the temporal and spatial failure pat-
terns for different components. Section V focus on the analysis
of correlated failures. We highlight the operators’ response to
failures in Section VI. In Section VII we summarize our key
ﬁndings and their implications to future dependability design.
We review related studies in Section VIII and conclude in
Section IX.
II. METHODOLOGY AND DATASETS
We conduct this study on a dataset containing all hardware
failure operation tickets (FOTs) collected over the past four
years from a major Internet service company that operates
dozens of data centers with hundreds of thousands of servers.
There are hundreds of thousands of FOTs in three categories,
D ﬁxing, D error and D falsealarm. Table I shows deﬁnition
and breakdown of each category. Operators do not repair
failure in D error mainly because the servers are out-of-
warranty. The typical action to D ﬁxing is to issue a repair
order (RO). Note that a separate group of contractors handle
the actual ROs, and our dataset does not contain the detail. As
far as our operators’ concern, issuing an RO closes an FOT.
We can see that over 1/4 of the failures are in out-of-
warranty hardware and thus are not handled at all. Operators
leave the partially failed but operational servers in production
and decommission the totally broken ones. We also notice that
the false alarm rate is extremely low, showing the effectiveness
(high precision) of hardware failure detection.
There are two sources for FOTs: programmatic failure
detectors and human operators. Both sources enter FOTs into
a central failure management system (FMS). Figure 1 shows
the simpliﬁed architecture of the FMS and the failure handling
workﬂow.
FMS has agents on most of the hosts1 that detect hardware
component failures. A (logically) centralized server collects all
FOTs from the agents for operators’ review.
1There are still some old unmonitored servers, but the monitoring coverage
has increased signiﬁcantly during the four years.
(cid:7)(cid:9)(cid:13)
(cid:5)(cid:19)(cid:18)(cid:24)(cid:29)
(cid:13)(cid:18)(cid:27)(cid:31)(cid:18)(cid:27)(cid:28)
(cid:6)(cid:15)(cid:20)(cid:22)(cid:30)(cid:27)(cid:18)(cid:1)(cid:12)(cid:18)(cid:16)(cid:25)(cid:27)(cid:17)(cid:28)
(cid:7)(cid:15)(cid:27)(cid:17)(cid:32)(cid:15)(cid:27)(cid:18)(cid:1)
(cid:9)(cid:25)(cid:24)(cid:20)(cid:29)(cid:25)(cid:27)(cid:1)
(cid:13)(cid:33)(cid:28)(cid:29)(cid:18)(cid:23)(cid:1)(cid:2)(cid:7)(cid:9)(cid:13)(cid:3)
(cid:10)(cid:26)(cid:18)(cid:27)(cid:15)(cid:29)(cid:25)(cid:27)(cid:28)
(cid:6)(cid:15)(cid:20)(cid:22)(cid:30)(cid:27)(cid:18)
(cid:11)(cid:25)(cid:25)(cid:22)
(cid:8)(cid:25)(cid:19)
(cid:10)(cid:26)(cid:18)(cid:27)(cid:15)(cid:29)(cid:25)(cid:27)(cid:28)(cid:4)(cid:11)(cid:27)(cid:25)(cid:19)(cid:27)(cid:15)(cid:23)(cid:28)
(cid:6)(cid:15)(cid:20)(cid:22)(cid:30)(cid:27)(cid:18)(cid:1)(cid:10)(cid:26)(cid:18)(cid:27)(cid:15)(cid:29)(cid:20)(cid:25)(cid:24)
(cid:14)(cid:20)(cid:16)(cid:21)(cid:18)(cid:29)(cid:28)
(cid:12)(cid:18)(cid:26)(cid:15)(cid:20)(cid:27)(cid:1)
(cid:11)(cid:27)(cid:25)(cid:16)(cid:18)(cid:28)(cid:28)
Fig. 1. Simpliﬁed failure management system (FMS) architecture.
FMS records over 70 types of failures covering nine
component classes, including hard drive, SSD, RAID card,
ﬂash card, memory, motherboard, CPU, fan, and power supply.
There is a special class, miscellaneous, covering all failures
manually entered by human operators. These miscellaneous
FOTs come with a natural
language problem description,
but most do not contain the root cause. They account for
about 10% of the FOTs. FMS agents automatically detect the
other 90% failures by two means: listening to syslogs and
periodically polling device status and other metadata.
error type,
error time,
Each FOT contains the following self-explanatory ﬁelds
id, host id, hostname, host idc,
describing the failure:
error device,
er-
ror detail. The FOTs in D ﬁxing and D falsealarm also
contain ﬁelds describing the operators’ responses, including
the action taken (usually an RO), the operator’s user ID, and
the timestamp op time of the action.
error position,
We need to point out that there are many deﬁnitions of
hardware failures other than a fatal stop. Even within the
company among different product lines, there is no agree-
ment whether to consider temporarily or partially misbehaving
hardware, such as a disk drive with SMART (Self-Monitoring
Analysis And Reporting Technology) alerts or occasional
read/write exceptions, as failed. Speciﬁcally, in this paper, we
consider every FOT in D ﬁxing or D error as a failure.
A. FOT overview
In this section, we provide an overview of the FOTs in
our datasets. Table II shows the breakdown of these FOTs
by component classes, in both D ﬁxing and D error (i.e. ex-
cluding false alarms). Not surprisingly, hard drive failures are
most common in all data centers we investigate, accounting for
about 82% of all failures. Other components, such as memory,
power supplies, SSDs and RAID cards, only contribute to
about 8% combined. Even the fraction is small, there are still
at least thousands of failures for each component class, enough
to be statistically meaningful.
26
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:01:13 UTC from IEEE Xplore.  Restrictions apply. 
TABLE II.
FAILURE PERCENTAGE BREAKDOWN BY COMPONENT.
Device
HDD
Miscellaneous
Memory
Power
RAID card
Flash card
Motherboard
SSD
Fan
CPU
HDD backboard
Proportion
81.84 %
10.20 %
3.06 %
1.74 %
1.23 %
0.67 %
0.57 %
0.31 %
0.19 %
0.14 %
0.04 %
TABLE III.
EXAMPLES OF FAILURE TYPES.
(a) HDD
Explanation
Some HDD SMART value exceeds the predeﬁned threshold.
The prediction error count exceeds the predeﬁned threshold.
Some device ﬁle could not be detected.
Some device ﬁle could not be accessed.
Failures are detected on the sectors that are not accessed.
Large number of failed sectors are detected on the HDD.
IO requests are not handled by the HDD and are in D status.
The bad block table (BBT) could not be accessed.
Failure type
SMARTFail
RaidPdPreErr
Missing
NotReady
PendingLBA
TooMany
DStatus
BBTFail
HighMaxBbRate The max bad block rate exceeds the predeﬁned threshold.
RaidVdNoBBU
Abnormal cache setting due to BBU (Battery Backup Unit)
is detected, which degrades the performance.
-CacheErr
Large number of correctable errors are detected.
DIMMCE
DIMMUE
Uncorrectable errors are detected on the memory.
About 10.2% FOTs are manually submitted miscellaneous
failures. These failures are complicated. In fact, operators do
not leave any description in 44% of these failures and suspect
about 25% to be hard drive related. They mark another 25%
as “server crashes” without clear reasons. As we will see later,
operators generate many of these FOTs during the deployment
phase, when they try to debug problems manually.
Our dataset contains many types of failures for each com-
ponent class. Table III shows some examples. Figure 2 shows
the percentage of each failure type for four representative
component classes. Some failures are fatal (e.g. NotReady in
a hard drive) while others warn about potential failures (e.g.
SMARTFail).
B. Analytical methods
Visually, we characterize the temporal and spatial proper-
ties of hardware failures by plotting the probability density
functions (PDF) or cumulative distribution functions (CDF).
Statistically, similar to previous work [5, 17], we conduct
hypothesis tests to verify how well the observed distribution
functions ﬁt some well-known probability distributions, includ-
ing uniform, exponential, Weibull gamma, and lognormal.
To see if a dataset ﬁts a given distribution, we ﬁrst estimate
the parameters of the ﬁtting distributions through maximum
likelihood estimation (MLE) and then adopt Pearson’s chi-
squared test, a widely used hypothesis test on distributions
of discrete random variables. For each null hypothesis we
create on the distribution of a certain random variable, the
test result is whether we can reject it at a certain signiﬁcance
level. The result implies whether the null hypothesis is a good
characterization of the observations.
(b) RAID card
(c) Flash card
(d) Memory
Fig. 2. Failure type breakdown of four example component classes.
27
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:01:13 UTC from IEEE Xplore.  Restrictions apply. 
III. TEMPORAL DISTRIBUTION OF THE FAILURES
The common belief is that components can fail at any time
in a uniformly random way. In this section, we investigate the
temporal distribution of the hardware failures. Focusing on the
error time, or the failure detection timestamp, in D ﬁxing and