看到增加的量了吧！ 文件系统的放大可以在 On-line 的环境下实作耶！超棒的！
最后，请注意！目前的 XFS 文件系统中，并没有缩小文件系统容量的设计！也就是说，文件系统只能放大不能缩小喔！如果你想要保
有放大、缩小的本事， 那还请回去使用 EXT 家族最新的 EXT4 文件系统啰！XFS 目前是办不到的！
想像一个情况，你有个目录未来会使用到大约 5T 的容量，但是目前你的磁盘仅有 3T，问题是，接下来的两个月你的系统都还不会超过
3T 的容量， 不过你想要让用户知道，就是他最多有 5T 可以使用就是了！而且在一个月内你确实可以将系统提升到 5T 以上的容量啊！ 你又不
想要在提升容量后才放大到 5T！那可以怎么办？呵呵！这时可以考虑“实际用多少才分配多少容量给 LV 的 LVM Thin Volume ”功能！
另外，再想像一个环境，如果你需要有 3 个 10GB 的磁盘来进行某些测试，问题是你的环境仅有 5GB 的剩余容量，再传统的 LVM 环境
下， LV 的容量是一开始就分配好的，因此你当然没有办法在这样的环境中产生出 3 个 10GB 的设备啊！而且更呕的是，那个 10GB 的设备其
实每个实际使用率都没有超过 10%， 也就是总用量目前仅会到 3GB 而已！但...我实际就有 5GB 的容量啊！为何不给我做出 3 个只用 1GB 的
10GB 设备呢？有啊！就还是 LVM thin Volume 啊！
什么是 LVM thin Volume 呢？这东西其实挺好玩的，他的概念是：先创建一个可以实支实付、用多少容量才分配实际写入多少容量的磁
盘容量储存池 （thin pool）， 然后再由这个 thin pool 去产生一个“指定要固定容量大小的 LV 设备”，这个 LV 就有趣了！虽然你会看到“宣告
上，他的容量可能有 10GB ，但实际上， 该设备用到多少容量时，才会从 thin pool 去实际取得所需要的容量”！就如同上面的环境说的，可能
我们的 thin pool 仅有 1GB 的容量， 但是可以分配给一个 10GB 的 LV 设备！而该设备实际使用到 500M 时，整个 thin pool 才分配 500M 给该
LV 的意思！当然啦！ 在所有由 thin pool 所分配出来的 LV 设备中，总实际使用量绝不能超过 thin pool 的最大实际容量啊！如这个案例说的，
thin pool 仅有 1GB， 那所有的由这个 thin pool 创建出来的 LV 设备内的实际用量，就绝不能超过 1GB 啊！
我们来实作个环境好了！刚刚鸟哥的 vbirdvg 应该还有剩余容量，那么请这样作看看：
1. 由 vbirdvg 的剩余容量取出 1GB 来做出一个名为 vbirdtpool 的 thin pool LV 设备，这就是所谓的磁盘容量储存池 （thin pool）
2. 由 vbirdvg 内的 vbirdtpool 产生一个名为 vbirdthin1 的 10GB LV 设备
3. 将此设备实际格式化为 xfs 文件系统，并且挂载于 /srv/thin 目录内！
话不多说，我们来实验看看！
# 1. 先以 lvcreate 来创建 vbirdtpool 这个 thin pool 设备：
[root@study ~]# lvcreate -L 1G -T vbirdvg/vbirdtpool # 最重要的创建指令
[root@study ~]# lvdisplay /dev/vbirdvg/vbirdtpool
--- Logical volume ---
LV Name vbirdtpool
VG Name vbirdvg
LV UUID p3sLAg-Z8jT-tBuT-wmEL-1wKZ-jrGP-0xmLtk
LV Write Access read/write
LV Creation host, time study.centos.vbird, 2015-07-28 18:27:32 +0800
LV Pool metadata vbirdtpool_tmeta
LV Pool data vbirdtpool_tdata
LV Status available
# open 0
LV Size 1.00 GiB ## 总总共共可可分分配配出出去去的的容容量量
Allocated pool data 0.00% ## 已已分分配配的的容容量量百百分分比比
Allocated metadata 0.24% # 已分配的中介数据百分比
Current LE 64
Segments 1
Allocation inherit
Read ahead sectors auto
- currently set to 8192
Block device 253:6
# 非常有趣吧！竟然在 LV 设备中还可以有再分配 （Allocated） 的项目耶！果然是储存池！
[root@study ~]# lvs vbirdvg # 语法为 lvs VGname
LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert
vbirdlv vbirdvg -wi-ao---- 2.50g
vbirdtpool vbirdvg twi-a-tz-- 1.00g 0.00 0.24
# 这个 lvs 指令的输出更加简单明了！直接看比较清晰！
# 2. 开始创建 vbirdthin1 这个有 10GB 的设备，注意！必须使用 --thin 与 vbirdtpool 链接喔！
[root@study ~]# lvcreate -V 10G -T vbirdvg/vbirdtpool -n vbirdthin1
[root@study ~]# lvs vbirdvg
LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert
vbirdlv vbirdvg -wi-ao---- 2.50g
vbirdthin1 vbirdvg Vwi-a-tz-- 10.00g vbirdtpool 0.00
vbirdtpool vbirdvg twi-aotz-- 1.00g 0.00 0.27
# 很有趣吧！明明连 vbirdvg 这个 VG 都没有足够大到 10GB 的容量，通过 thin pool
# 竟然就产生了 10GB 的 vbirdthin1 这个设备了！好有趣！
# 3. 开始创建文件系统
[root@study ~]# mkfs.xfs /dev/vbirdvg/vbirdthin1
[root@study ~]# mkdir /srv/thin
[root@study ~]# mount /dev/vbirdvg/vbirdthin1 /srv/thin
[root@study ~]# df -Th /srv/thin
Filesystem Type Size Used Avail Use% Mounted on
/dev/mapper/vbirdvg-vbirdthin1 xfs 10G 33M 10G 1% /srv/thin
# 真的有 10GB 耶！！
# 4. 测试一下容量的使用！创建 500MB 的文件，但不可超过 1GB 的测试为宜！
[root@study ~]# dd if=/dev/zero of=/srv/thin/test.img bs=1M count=500
[root@study ~]# lvs vbirdvg
LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert
vbirdlv vbirdvg -wi-ao---- 2.50g
vbirdthin1 vbirdvg Vwi-aotz-- 10.00g vbirdtpool 4.99
vbirdtpool vbirdvg twi-aotz-- 1.00g 49.93 1.81
# 很要命！这时已经分配出 49% 以上的容量了！而 vbirdthin1 却只看到用掉 5% 而已！
# 所以鸟哥认为，这个 thin pool 非常好用！但是在管理上，得要特别特别的留意！
这就是用多少算多少的 thin pool 实作方式！基本上，用来骗人挺吓人的！小小的一个磁盘可以仿真出好多容量！但实际上，真的可用容
量就是实际的磁盘储存池内的容量！ 如果突破该容量，这个 thin pool 可是会爆炸而让数据损毁的！要注意！要注意！
现在你知道 LVM 的好处咯，未来如果你有想要增加某个 LVM 的容量时，就可以通过这个放大的功能来处理。 那么 LVM 除了这些功能
之外，还有什么能力呢？其实他还有一个重要的能力，那就是 LV 磁盘的快照 （snapshot）。 什么是 LV 磁盘快照啊？快照就是将当时的系统
信息记录下来，就好像照相记录一般！ 未来若有任何数据更动了，则原始数据会被搬移到快照区，没有被更动的区域则由快照区与文件系统共
享。 用讲的好像很难懂，我们用图解说明一下好了：
图14.3.3、LVM 快照区域的备份示意图
左图为最初创建 LV 磁盘快照区的状况，LVM 会预留一个区域 （左图的左侧三个 PE 区块） 作为数据存放处。 此时快照区内并没有任
何数据，而快照区与系统区共享所有的 PE 数据， 因此你会看到快照区的内容与文件系统是一模一样的。 等到系统运行一阵子后，假设 A 区
域的数据被更动了 （上面右图所示），则更动前系统会将该区域的数据移动到快照区， 所以在右图的快照区被占用了一块 PE 成为 A，而其他
B 到 I 的区块则还是与文件系统共享！
照这样的情况来看，LVM 的磁盘快照是非常棒的“备份工具”，因为他只有备份有被更动到的数据， 文件系统内没有被变更的数据依旧保
持在原本的区块内，但是 LVM 快照功能会知道那些数据放置在哪里， 因此“快照”当时的文件系统就得以“备份”下来，且快照所占用的容量又非
常小！所以您说，这不是很棒的工具又是什么？
那么快照区要如何创建与使用呢？首先，由于快照区与原本的 LV 共享很多 PE 区块，因此快照区与被快照的 LV 必须要在同一个 VG
上头。
另外，或许你跟鸟哥一样，会想到说：“咦！ 我们能不能使用 thin pool 的功能来制作快照”呢？老实说，是可以的！不过使用上面的限制
非常的多！包括最好要在同一个 thin pool 内的原始 LV 磁盘， 如果为非 thin pool 内的原始 LV 磁盘快照，则该磁盘快照“不可以写入”，亦即 LV
磁盘要设置成只读才行！同时， 使用 thin pool 做出来的快照，通常都是不可启动 （inactive） 的默认情况，启动又有点麻烦～所以，至少目前
（CentOS 7.x） 的环境下， 鸟哥还不是很建议你使用 thin pool 快照喔！
下面我们针对传统 LV 磁盘进行快照的创建，大致流程为：
预计被拿来备份的原始 LV 为 /dev/vbirdvg/vbirdlv 这个东西～
使用传统方式快照创建，原始碟为 /dev/vbirdvg/vbirdlv，快照名称为 vbirdsnap1，容量为 vbirdvg 的所有剩余容量
传传统统快快照照区区的的创创建建
# 1. 先观察 VG 还剩下多少剩余容量
[root@study ~]# vgdisplay vbirdvg
....（其他省略）....
Total PE 252
Alloc PE / Size 226 / 3.53 GiB
Free PE / Size 26 / 416.00 MiB
# 就只有剩下 26 个 PE 了！全部分配给 vbirdsnap1 啰！
# 2. 利用 lvcreate 创建 vbirdlv 的快照区，快照被取名为 vbirdsnap1，且给予 26 个 PE
[root@study ~]# lvcreate -s -l 26 -n vbirdsnap1 /dev/vbirdvg/vbirdlv
Logical volume "vbirdsnap1" created
# 上述的指令中最重要的是那个 -s 的选项！代表是 snapshot 快照功能之意！
# -n 后面接快照区的设备名称， /dev/.... 则是要被快照的 LV 完整文件名。
# -l 后面则是接使用多少个 PE 来作为这个快照区使用。
[root@study ~]# lvdisplay /dev/vbirdvg/vbirdsnap1
--- Logical volume ---
LV Path /dev/vbirdvg/vbirdsnap1
LV Name vbirdsnap1
VG Name vbirdvg
LV UUID I3m3Oc-RIvC-unag-DiiA-iQgI-I3z9-0OaOzR
LV Write Access read/write
LV Creation host, time study.centos.vbird, 2015-07-28 19:21:44 +0800
LV snapshot status active destination for vbirdlv
LV Status available
# open 0
LV Size 2.50 GiB # 原始碟，就是 vbirdlv 的原始容量
Current LE 160
COW-table size 416.00 MiB # 这个快照能够纪录的最大容量！
COW-table LE 26
Allocated to snapshot 0.00% # 目前已经被用掉的容量！
Snapshot chunk size 4.00 KiB
Segments 1
Allocation inherit
Read ahead sectors auto