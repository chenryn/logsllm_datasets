clusive as it was unable to determine the root cause of the crashes.
In that cause, we defaulted to the output from contemporary solu-
tions.
3.4 Coverage Guided Fuzzing
With an understanding of how the first two stages of our framework
operate, we now return to a discussion of three different strategies
that can be employed at runtime.
Strategy 1 — Hunting similar bugs of the past: The success of this
strategy hinges on the assumption that because the same coding
practices may have been followed in the latest version of some code
base, similar coding flaws may persist. To uncover the presence
of such bugs, we first check if an input triggers a new code path.
For example, AFL considers an input as interesting if it triggers
new branch edges during execution. If that is the case, we validate
whether this input is a quality input using the trained classifier.
Only if both conditions are satisfied do we then allow the input to
go on for mutation. Functionality wise, we are explicitly limiting
the inputs the base fuzzer intended to pass on for mutation.
Strategy 2 — Hunting different bugs: Alternatively, it might be
safe to assume that once bugs have been discovered in past ver-
sions of a program, the developers would have taken measures to
minimize those errors in more current versions of the program.
Hence, it may make sense to instead steer the fuzzing in a different
direction. Strategy 2 does just that. First, we check if an input trig-
gers new code paths. If so, we use the classifier to determine if the
input is dissimilar to the ones seen in the past vulnerabilities, i.e.,
if the input is classified as non-quality. If that condition holds, the
input is selected for mutation. In this way, we are offering a more
informative form of path guidance [10].
Strategy 3 — Dual mode: Modern fuzzers often get stuck in deep
code paths or spend too much time on inefficient mutation oper-
ations that do not lead to entirely different code paths [4, 7, 16].
Consequently, they either do not generate quality inputs for a long
time [27], or they end up finding quality inputs that trigger the
same bug repeatedly. Both of these cases reduce the overall perfor-
mance of a fuzzer. In the dual mode strategy, we allow for switching
between ML-based heuristics, which can be triggered based on dif-
ferent conditions. For example, one may choose to switch heuristics
if the fuzzer fails to generate any new crashing inputs for a certain
period of time, or if the vast majority of recent crashes all have the
same root cause. In this way, we allow the fuzzer to perform a more
comprehensive exploration under fixed resource constraints.
4 IMPLEMENTATION DETAILS
From an engineering standpoint, the design of our framework con-
sists of four modules (Profiler, Feature Selector, Heuristic Generator,
Vulnerability Explorer) that map to the corresponding components
outlined in Section 3. As an initial input to the workflow, we first
collect quality and non-quality inputs from old versions of target
programs we wish to fuzz. For that, we can use known crashing
inputs to expedite the process or simply run the baseline fuzzer
for some extended period on the old versions. Note that this is a
one-time cost.
4.1 Profiler
As the name suggest, this module is responsible for recording the
HPC events for all the code paths triggered by both quality and non-
quality inputs during training. The trace, T , for input, i, is given
as Ti = {ei1, ei2, . . . , ei96}, where eij represents the measurement
of hardware events. Due to the limited number of programmable
counters, to measure all 96 events, the Profiler executes the program
with a specific input 24 times, each time configuring a different set
of 4, non-overlapping, counters.
4.2 Feature Selector
The Selector is responsible for determining what specific hard-
ware events should be used to guide the input selection process
at runtime. First, the HPC traces are separated into two groups
corresponding to quality and non-quality inputs. Since the num-
ber of traces for quality inputs will only be a small fraction of all
the traces, the collection S = T1, . . . , TN , will be imbalanced. To
mitigate any bias in the feature selection process, S is split into m
smaller datasets by randomly selecting the same number of traces
from each input class. Here, m is the ratio of non-quality to quality
inputs.
Next, we use the CfsSubsetEval algorithm [20] to select the best
subset of features. CfsSubsetEval evaluates the worth of a subset
of features by considering the individual predictive ability of each
feature along with the degree of redundancy between them. The
ACSAC 2020, December 7–11, 2020, Austin, USA
Sanjeev Das, Kedrian James, Jan Werner, Manos Antonakakis, Michalis Polychronakis, and Fabian Monrose
evaluation is conducted using a 10-fold cross-validation approach.
We then assign a score to each HPC event based on the number of
times that event appears among the selected subset of features over
all 10 runs. The overall score of an event is averaged across all m
datasets. Informally, we denote that score as the information gain
of an event. The coverage metric derived at the end of this process
is cov = {e1, e2, e3, e4} where ej denotes the hardware events that
meet some criteria (for example, the 4 events with the highest gain).
To further illustrate how this works, consider the analysis of a
set of well-known libraries we would like to build coverage metrics
for: libpng, libjpeg, yaml, tiff, pcre and libxml. In this case, the set
S in Table 2 was derived by running the AFL fuzzer on known
vulnerable versions of the target libraries for 48 hours each.
Table 2: Example training data
Programs
libarchive (v3.1.0) (bsdtar)
libjpeg (v1.4.2) (cjpeg)
libplist (v1.11) (plist_test)
libpng (v1.2.56) (pngtest)
libxml2 (v2.9.2) (xmllint)
pcre (v10.0) (pcre2test)
tiff (v4.0) (gif2tiff)
yaml (v0.5.3) (parse)
Quality Non-quality
inputs
inputs
6417
229
52381
1136
1544
164
496
9592
25489
466
42652
2533
7595
676
682
11537
very different in our approach, we ensured that the classifier built
using the events has at least an F-measure value of 0.7 (shown in
Figure 5). In addition, there must be sufficient number of events (e.g.,
6-8) with a high information gain to build multiple coverage metric
sets. Based on these measures, we empirically set the information
gain threshold at 0.4 — which yields the prerequisite 8 events in
Figure 4 — and disregarding events with lower gain. Furthermore,
when choosing the next-best grouping of events, we mandated that
there was at least 50% difference in categories with the topmost
4 events. For example, in case of pcre, the topmost set would be:
{e65, e12, e47, e56}, while the next set (above the cut-off) containing
events from at least two different classes would be: {e46, e12, e54, e41}
from Table 8.
4.3 Runtime Heuristic
This component is responsible for deriving the heuristic that helps
steer the input selection process. As noted in Section 3, we choose
to use a multilayer perceptron approach because it offered good
accuracy and was straightforward to translate the resulting clas-
sifier into a runtime heuristic. We used the Weka toolkit API to
implement the MLP based classifier.
Figure 5: Comparison of F-measures
Figure 5 shows the performance of trained classifiers on a set
of real-world libraries. We used 10-folds cross-validation approach
to build our models. For the sake of comparison, we report the
effectiveness of MLP and decision tree (DT) models built using the
top 4 events, next 4 events and the event with the highest informa-
tion gain for each program. Overall, the performance of MLP and
DT classifiers are similar for most of the programs with 4 events,
except in pcre. We choose MLP because DT uses discrete value of
events which restricts the model, and in some cases the tree is quite
complex with a large number of nested nodes (e.g., pcre has over 60
nodes). In the case where the models are built using the single event
with the highest gain, the trained MLP model performed worse,
and sometimes failed to identify the quality inputs.
4.4 Vulnerability Explorer
Motivated by the recent fuzzer benchmarking reports [45], we
decided to apply our framework to AFL, MOpt [27] and Fairfuzz [24].
More details about this choice is given in §5.1. All of these fuzzers
implement a fork server model. In fork server model, the fuzzed
process goes through execve(), linking, libc initialization only once,
and then clones from a stopped process image via copy-on-write.
Figure 4: Information gain for sample libraries
Figure 4 shows the normalized information gain for all 96 events
for each of the libraries. To further elucidate the relation between
the selected events, we grouped the events into the 65 categories
discussed in Section 3.1. Note that the gain varies across the bench-
marks. This should be the case. While a few event classes show up
across all benchmarks (e.g., event classes dtlb_load_misses, br_inst_retired,
itlb_flush, offcore_requests), different subsets of events have more
discriminatory power for a given benchmark. For most of the pro-
grams, the gain is high for the top 10-12 events.
One can use this insight to build multiple coverage metrics for a
given program. For example, we could select the best 4 events as a
coverage metric or the next 4 events, depending on our desired out-
come. Given that the cost of false positives and false negatives are
081624324048566472808896Events0.00.20.40.60.81.0Information gainlibarchivelibjpeglibplistlibpnglibxml2pcre2tiffyamlP0P1P2P3P4P5P6P7P0-libarchive, P1-libjpeg, P2-libplist, P3-libpng, P4-libxml2, P5-pcre2, P6-tiff, P7-yaml0.00.20.40.60.81.01.2F-measuresTop-4-events (MLP)Top-4-events (DT)Next-4-events (MLP)Next-4-events (DT)1-event (MLP)1-event (DT)A Flexible Framework for Expediting Bug Finding by Leveraging Past (Mis-)Behavior to Discover New Bugs
ACSAC 2020, December 7–11, 2020, Austin, USA
The fork server stops at the first instrumented function to await
commands from a fuzzer module. Our implementation is built in
concert with the fork server model of a base fuzzer. Specifically,
we modified base fuzzer’s original instrumentation such that the
child process waits to receive commands via a pipe before it can
resume its execution. As it waits, our Profiler module configures the
relevant HPC events for the child process created by the fork server.
We use the perf_event_open() API to configure the HPC events.
After the execution of the child process, the hardware events are
recorded. Next, the recorded HPC values are used to build a feature
vector, which is given as an input to the trained classifier. The
classifier predicts whether the feature vector belongs to a quality
class or non-quality class.
Table 3: Vulnerability Exploration Strategies
Mode
Single
Dual
Objective
Finding similar bugs
Finding different bugs
Finding different bugs
Finding similar bugs
Finding different bugs
Strategies
1-a
2-a
2-b
3-a
3-b
HPC driven heuristics
Primary
best 4 events
best 4 events
next 4 events
best 4 events
next 4 events
Secondary
n/a
n/a
n/a
next 4 events
best 4 events
The vulnerability explorer module can be configured to pursue
a myriad of strategies. Table 3 outlines variants of three strategies
(see §3.4) we tested. In the single mode instantiation, we build the
variants of Strategies 1 and 2 using either the best 4 events or the