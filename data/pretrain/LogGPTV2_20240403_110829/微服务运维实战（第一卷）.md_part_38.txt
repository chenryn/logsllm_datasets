这三个命令的输出如下：
在这个例子里，Swarm 把books-ms 容器部署到了 swarm-node-2，把 Mongo
命令 ps 的输出如下：
"Node":"swarm-node-2"
"Address": "10.100.192.202",
"ServiceID": "swarm-node-2:booksms_app-blue_1:8080",
"ServiceName":
"ServiceTags": null,
"ServiceAddress": "10.100.192.202",
"ServicePort":32768,
"books-ms-blue",
14.4
使用Docker Swarm和Ansible自动化部署293
---
## Page 307
294
第14章服务集群和扩展
curl swarm-master:8500/v1/catalog/service/books-ms-green\
Consul观察到当前正在运行的版本的信息，如下：
swarm-node-1/books-ms-db
swarm-node-2/booksms_app-blue_1
NAMES
制台屏幕进行监视。几分钟后，构建结束，现在可以检查结果：
第二次运行Swarm部署Playbook
二个请求获得了所有书的列表。
curl swarm-master/api/v1/books| jq'.'
swarm-node-2/booksms_app-green_1
本。
swarm-master/api/v1/books I jq '.'
Consul的响应如下：
jq'.
由于我们运行的是绿色版本，所以蓝色版本处于退出状态。现在可以从
命令 ps 的输出如下:
请打开 swarm子项目，然后单击Build Now 链接。构建将开始，我们可以从控
现在来部署下一个版本。
第一次运行，自动化进程看上去运行正常。下面将再次构建并部署绿色版
第一个请求是 PUT，它向服务发送了信号表示我们想要存储本书的信息。第
"ServicePort":32770,
"ServiceAddress":
"ServiceTags":[],
"ServiceName":"books-ms-green",
"ServiceID": "swarm-node-2:booksms_app-green_1:8080",
"Address":"10.100.192.202",
"Node":"swarm-node-2",
"CreateIndex":3314,
"ModifyIndex":3314,
"10.100.192.202"
Up 10 hours
Exited (137) 15 seconds ago
Up7minutes
STATUS
---
## Page 308
将通过创建自修复系统来解决不容错的问题。
vagrantdestroy-f
exit
之前，先删除这些虚拟机。等我们需要时，将再次创建它们：
清除
再次构建，完成后，确认三个实例部署到集群中。
swarm 分支中的 Jenkinsfile，将其更改为部署服务的三个实例，然后推送该改动。
http://10.100.192.200:8500/ui来获取部署的服务的视图。
curl swarm-master/api/v1/books I jq '.'
我们提出的解决方案还有很多问题，如系统不容错，而且难以监控。第15 章
Docker Swarm 旅程到此结束，第15章会更多地使用它。在进行到下一个主题
作为练习，
由于我们已经运行了ConsulUI，所以请在浏览器里打开地址
现在可以测试服务本身，如下：
"ServiceEnableTagoverride": false
：请复制 books-ms 代码库，并在修改作业配置后加以使用。打开
14.4使用DockerSwarm和Ansible 自动化部署295
---
## Page 309
西，并将这些知识应用于软件和硬件中。
为例，人体拥有的一种迷人的自我修复的能力。我们可以从自己身上借鉴很多东
成能够从故障中恢复。我们应该抱最好的期望，做最坏的打算。
们应该追求完美。我只是提倡一种更现实的做法：接受系统的不完美，把它设计
完美的东西。当然，这并不是说我们不应该追求完美，如果时间和资源允许，我
永远不会出故障的硬件上，并且可以处理任何负载。我有不同的看法：世上没有
以前没有人做过的事情。我们努力实现终极完美，希望系统没有任何 bug，运行在
坏掉，或者完全想不到的事情会发生。
服务会无法处理增加的负载，提交的某次代码会引入一个致命的 bug，某块硬件会
生活中有很多自我修复的例子。没有比生命自我修复能力更强的系统。以人
如何对抗这种意外？大多数人正在努力开发一个完美的系统。我们试图创造
面对现实吧，我们创建的系统并不完美。迟早，某个应用程序会失败，某个
治愈是需要勇气的。我们都有这样的勇气，只是需要我们自己去挖掘。
Self-Healing Systems
自我修复系统
第15章
-ToriAmos
---
## Page 310
298
第15章自我修复系统
15.1
复的具有容错性的、快速响应的系统。
动适应不断变化的条件，其目标是成为能根据需求对变化进行响应并从故障中恢
到正常或某个计划的状态。自我修复是系统能够不断检查和优化自己的状态并自
人为干预的情况下，发现自已没有正常工作，并进行必要的更改以将其自身恢复
体
大系统的一部分。它与其他系统进行通信、协作和适配，从而形成一个更大的整
果。就像个体组成的生命体形成一个整体生态系统一样，每个计算机系统也是更
可以应用于几乎任何类型的架构，但是当与微服务结合使用时，可以产生最佳效
只适用于微服务，并不是这样。然而，与大多数我们研究的其他技术一样，自愈
极类似。我们创建可自我修复的微服务系统，这并不是说我们要研究的自我修复
我们称这些小单位为微服务，实际上，它们的行为跟我们在人体中观察到的行为
件。软件单元越小，就越容易自我修复，当需要时从故障恢复、增加甚至销毁。
自己的生命结束，人类的生命仍然会延续下去，蓬勃发展，不断适应新环境。
是，只从个体的角度看待我们自己意味着没有以整体的角度看待人类。即使我们
受到病毒攻击，我们向病魔屈服，但在大多数情况下，我们成功活下来了。但
胞。这种能力不会让个体免于死亡，但它确实能让我们轻易恢复健康。我们不断
大量细胞被破坏，周围的细胞也可以复制产生新的细胞，迅速替换被破坏的细
有治愈自身的能力，以及制造新的细胞来替代已被永久损伤或破坏的细胞。即使
监测和调整自己的程序，根据原有的 DNA 恢复自身，维持身体内的平衡。细胞具
内的细胞努力使我们恢复平衡状态。每个细胞都是一个动态的生命单位，它不断
在软件系统中，术语自我修复描述了一种应用程序、服务或系统在没有任何
可以将计算机系统看成是由各种细胞组成的人体，这些细胞可以是硬件或软
人体有很好的治愈能力。人体最基本的单位是细胞。在我们的一生中，身体
自我修复等级和类型
---
## Page 311
Self-Healing on the Application Level
应用程序级别的自我修复
发人员都可以把它设计成为可白我修复以及从失败中恢复的服务。
可以很快获得使用不同语言编写的服务，使用不同的框架，等等。每个服务的开
并不负责比如恢复发生故障而失败的进程。而且，如果采用微服务体系结构，也
在内部自我修复你的应用程序。请记住这里说的自我修复是指进程内部层面的，
于我们试图与编程语言无关地探究这个课题，所以我会留给你们作业，研究如何
够让我们创建可以从潜在的灾难性情况中恢复过来的具有容错性的应用程序。由
我修复应用程序和服务。Akka 并不是唯一的解决方案，还有许多其他库和框架能
其中一个是 Akka，它的主要作用是监管，其推出的设计模式是能够创建内部的自
败，之后就重新连上了。再往后，我们有更好的方法来处理应用程序中的问题。
的，否则，可能很容易进入一个永无止境的循环，除非数据库连接只是暂时失
经验更丰富一点，就可能会试着重复连接到数据库。重复尝试的次数通常是有限
的连接。如果连接在应用程序启动时没能建立，那么可能会停止整个程序。如果
下，如果出现特定类型的异常，我们倾向于停止应用程序。一个例子是与数据库
然后继续执行，就像什么都没有发生过一样，希望在未来不会再出现。其他情况
待后期检查。当这样的异常发生时，我们倾向于（在日志里记录下来之后）忽略它
们习惯于通过异常来捕获问题，并且大多数情况下会在日志里记录下这些异常以
级别如下。
应用程序级别的恢复是应用程序或服务在内部自我修复的能力。传统上，我
下面将分别探讨这三种类型。
根据监控和操纵资源的大小和类型，自我修复系统可以分为三个层次。这些
硬件级别。
系统级别。
应用级别。
15.1自我修复等级和类型299
---
## Page 312
300第15章自我修复系统
Self-Healing on the System Level
系统级别的自我修复
它将无法发送请求，TTL会过期，服务会采取相应的反应措施。
务可以发送一个HTTP请求，宣布它是活着的。如果服务正在运行的进程失败，则
更新，则监控系统假定服务失败，需要恢复到某个计划的状态。例如，健康的服
TTL 信号的系统跟踪给定 TTL 最后已知的报告状态。如果该状态在预定期间内未
Time-To-Live
两种是TTL 和ping。
些行动。
事件的发生。在这两种情况下（故障和业务量增加），系统需要监控自身并采取一
试从这些失败中学习，并改进整个系统。因此，这种情况下也要发出通知来告知
就是说，自我修复往往与调查失败的原因并驾齐驱。系统自动修复，我们（人）尝
人力干预的。我们需要调查失败的原因，更正服务的设计，或者修复一个bug。也
通常是不够的。虽然这种操作可能会把系统恢复到所期望的状态，通常还是需要
够，就要根据是否达到最高或者最低的响应时间来扩缩。光从失败的进程中恢复
败，则需要重新部署该服务，或者重新启动该进程。另一方面，如果响应时间不
能会发生很多事情，但是最常见的两类是进程失败和响应时间不够。如果进程失
序。这是我们可以在整个系统层面上设计的自我修复的类型。虽然在系统层面可
系统级别的自我修复可以泛化，不考虑其内部结构而应用于所有服务和应用程
TTL（生存时间）检查期望服务或应用程序能周期性地确认其运行正常。接收
系统如何监控自身？它如何检查其组件的状态？有很多种方法，但最常用的
不同于依赖于内部应用的编程语言和设计模式的应用程序级别的自我修复，
让我们进入第二个层级。
---
## Page 313
问题（见图15-2）。
态更好的方法。它可以消除在每个服务中实现TTL可能发生的重复、耦合和并发
可以使用脚本实现，或者其他可以验证服务状态的方法。
响应应为HTTP状态在2XX范围内。在其他情况下，当HTTP API未公开时，Ping
可以有多种形式。如果服务公开HTTP API，Ping 通常是一个简单的请求，期望的
务发送 Ping，如果没有收到回复，或者回复内容不丰满，则执行修复措施。Ping
Pinging
功能并使开发复杂化（见图15-1）：
微服务应有明确的功能和单一的目的。在微服务内部实现 TTL请求将增加额外的
种反微服务的模式，因为我们本来试图以尽可能自治的方式设计微服务，此外，
Ping 与TTL 相反，如果使用 Ping 可行的话，则它是一种检查系统各个部分状
Ping 的原理是从外部检查应用程序或服务的状态。监控系统应定期对每个服
TTL 的主要问题是耦合。应用和服务需要与监控系统相关联。实现 TTL 是一
服务
TTLTTL
服务
图15-1TTL类型的系统级别的自我修复
服务
TTLTTLTTL
服务
健康监控
纠正措施
服务
TTL（生存时间）
服务
TTL
服务
TTL TTL
服务
15.1
自我修复等级和类型301
>
---
## Page 314
302
第15章自我修复系统
Self-Healing on the Hardware Level
硬件级别的自我修复
划分。我们有可能会去应对失败，也有可能试图防患于未然。
别被修复。硬件级别的修复（见图15-3）与稍后讨论的预防式检查关系更密切。
在系统级别。如果硬件不能正常工作，那么这个服务可能会失败，从而在系统级
硬件组件的状态，并进行相应的操作。实际上，硬件级别的自我修复大多数发生
健康的节点重新部署到健康节点之一。与系统级别一样，我们需要定期检查不同
存、坏掉的硬盘、故障的CPU，等等。这一级别的修复实际上意味着将服务从不
除了在检查层次方面对自我修复做出划分之外，还可以根据采取的行动进行
说实话，并没有硬件级别的自我修复这种事。我们不可能自动修复出错的内
服务
节点
Ping
值检查
服务
Ping
图15-2Ping类型的系统级别的自我修复
图15-3
服务
Ping
健康监控
服务
纠正措施
硬件级别的自我修复
节点
Ping
健康监控
纠正措施
闵值检查
状态码<>200
服务
[Ping
服务
Ping
服务
节点
Ping
阈值检查
>
服务
Ping
>
---
## Page 315
Preventive healing
预防式自我修复
Reactive healing
反应式自我修复
现这种系统所需花费的时间。
可以明智地使用它，创造一个几乎适用于所有案例的一般解决方案，从而减少实
投入成本主要在于实现它所需要的知识和时间。虽然时间本身也是一种投资，但
接与自我修复相关，而是与给定情况可承受的风险程度相关。反应式自我修复的
现。你可能投资于备用硬件，或者投资于单独的数据中心，决定如何投资并不直
在太小，反应式自我修复都应该是必要的，而且它不需要投入太多成本就能实
现零停机。
定。我们的目的是尽力实现零停机，但是也要接受在有些情况下其实没有必要实
止响应。这就属于评估风险以及预防风险的成本的范畴。
几乎从来没有，因为比如整个数据中心的电源可能会松动，从而所有的节点都停
至少两个实例并分布到不同的物理节点上，就应该（几乎）不会有宕机时间。我说
地方都进行了检查，而且发生故障时执行了相应的动作，并且将每个服务扩展到
是可行的。这是最重要的自我修复方式，也是最简单的实现方式。只要在恰当的
障，还是因为整个节点而停止运行（假设系统可以将其重新部署到新节点），这都
（not found），采取纠正措施后，服务再次运行。无论服务是因为进程失败而故
到故障后，会将其自身恢复到计划好的状态。服务进程死机，ping 返回代码 404
预防式自我修复背后的原理是预测未来可能遇到的问题，并采取行动避免这
无论是在争取实现零岩机时间，还是争取实现近乎零宕机时间，除非系统实
有时候，在不同的地方部署两个数据中心是有价值的，在其他情况下则不一