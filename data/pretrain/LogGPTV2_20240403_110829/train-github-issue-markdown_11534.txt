Recently I have carefully read the code in `pytorch/torch/optim/sgd.py` .
According to the document in this file, the current implementation of SGD
optimizer is as follows:
    v_(t+1) = mu*v_t + grad(w_t) 
    w_(t+1) = w_t - lr*v_(t+1) 
Where `v_t` is the momentum and `w_t` is the weight value in step `t`.
`grad(x)` is the gradient for `x` .
The implementation is OK for this formulation. But I am confused with the
implementation of Nesterov Accelerated SGD(NAS). Following the formulation of
[1][2], the formulation of NAS under PyTorch's implementation should be
    v_(t+1) = mu*v_t + grad(w_t) 
    w_(t+1) = w_t + mu*(v_(t+1)-v_t) - lr*v_(t+1) 
However, the current implementation of NAS is:
    v_(t+1) = mu*v_t + grad(w_t) 
    w_(t+1) = w_t - lr*(grad(w_t) + mu*v_(t+1))
I am not sure how to get this formulation. How do you think about this ?
[1] http://cs231n.github.io/neural-networks-3/  
[2] http://www.cs.toronto.edu/~hinton/absps/momentum.pdf