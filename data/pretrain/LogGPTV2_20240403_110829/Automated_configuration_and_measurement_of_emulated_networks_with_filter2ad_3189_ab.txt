cols and services and conducted across a variety of target
platforms. Further, with Python as the host language, our
system is cross-platform and easily extensible.
Discussions with users of emulation in both the research
and operator communities conﬁrmed the need for autocon-
ﬁguration in experimentation and teaching. Recognising the
beneﬁts of an automated system, some groups we spoke to
had developed their own custom set of scripts to simplify
emulation. These scripts are commonly used to create boot-
strap topologies [24], upon which the actual experiment can
be conducted. However this is a sub-optimal solution: ide-
ally there would be a standard platform provided to the net-
work community to facilitate repeatable research. It should
be vendor neutral (so as to allow comparisons across diﬀer-
ent “equivalent” devices); ﬂexible and extensible; scalable;
and provide an integrated work-ﬂow with a consistent view
of the whole experiment.
3. MOTIVATING CASE STUDIES
This section motivates the requirements of an emulation
system through a series of case studies.
3.1 Netkit Small-Internet BGP Lab
Figure 1: The Netkit Small Internet Lab [11], with seven
Autonomous Systems (ASes) and fourteen routers.
An initial motivation for automated network conﬁgura-
tion came from recreating Netkit’s tutorial Small-Internet
Lab [11] shown in Figure 1. Manually creating the required
conﬁguration ﬁles for Netkit took several days, including
conﬁguring OSPF and BGP in Quagga, entering IP Ad-
dresses, creating the Netkit topology description of the vir-
tual routers and their interconnections, and testing and de-
bugging the setup. Much of the conﬁguration becomes an
exercise in almost copying and pasting conﬁguration blocks,
but ensuring that a few values are updated consistently.
This is instructive to learn about network conﬁguration,
but large-scale experiments are impractical if small problems
takes days to conﬁgure. This led us to develop a prototype
conﬁguration automation system [28]. The approach, mo-
tivated by [4], modelled the network as a series of objects,
which were pushed into templates to generate the device
speciﬁc conﬁguration.
The Small-Internet lab could be described in approxi-
mately 100 lines of high-level API code (compared to 500
lines of conﬁguration code), but in a more natural syntax,
and took under an hour to write. However, this tool was
still fundamentally device-oriented and required a human
operator to transcribe the network topology into the appro-
priate format. This is better than manual conﬁguration, but
still time-consuming and error-prone: it is diﬃcult to spot
as20r2as20r3as1r1as30r1as40r1as20r1as100r1as100r2as100r3as200r1as300r2as300r4as300r3as300r1237topology mistakes in such a format. Additionally, it can be
repetitive to express network-level concepts at a device level.
This in turn motivated our current system, which aimed at
expressing network-level abstractions at the network level,
using attribute graphs. The system allows input from mul-
tiple sources, including graphical editors and network topol-
ogy collections.
Drawing, labelling, and connecting the routers and as-
signing them to ASNs took approximately two minutes. The
output was saved in GraphML and directly read into our sys-
tem, which took under a second to build the overlay topolo-
gies (discussed in § 6), and compile these to templates. This
both dramatically sped up the process of specifying the net-
work, and means we can modify or rerun the experiment in
only seconds.
3.2 Large-Scale Model: European NRENs
Our tool can also build large networks, through the use
of abstraction in the design process. High-level design rules
operate on the provided input network topology. Our im-
plementation choices, which we discuss later in this paper,
have been made to allow fast design and conﬁguration, that
scales for large numbers of nodes.
Data for experiments commonly come from a variety of
sources, including RocketFuel [37], the Internet Topology
Zoo [26], and programmatically generated network topolo-
gies. As a consequence, the system has been designed to
easily accept data from a variety of formats. In addition,
scale is a key issue for many experiments.
As an example of a large-scale network, we used the Euro-
pean Interconnect Model from the Internet Topology Zoo [26],
which is a model of the European National Research and Ed-
ucation Networks (NRENs), connected through the G´EANT
backbone network. This model contains 42 ASes, 1158 routers,
and 1470 links. On a typical laptop our system took 15 sec-
onds to load and build network topologies, 27 seconds to
compile the network model, and 2 minutes to render the
model to conﬁguration ﬁles. The resulting set of conﬁgu-
rations ﬁles was 20MB uncompressed, with 16,144 items.
Large-scale emulations approach the limits of commodity
hardware: the NREN model consumes approximately 37GB
of RAM when implemented using Netkit, and uses signiﬁ-
cant processor resources. The size of the emulated network
experiments is limited by the available hardware to host the
emulation, not the conﬁguration tool:
larger networks can
be generated for hosts with more memory and compute re-
sources.
3.3 Services
One of the key features of emulation, in comparison to
simulation, is its ability to include standard software ser-
vices, using the existing software for those services. For
instance, as Netkit is Linux based, it is straightforward to
run standard Linux packages in a Netkit VM. It also imposes
additional requirements on conﬁguration software, which we
illustrate with two example services: DNS and RPKI.
The Domain Name System (DNS), is a major part of
modern networks. It provides translation between domain
names and IP addresses. Although often seen as a service
to network customers, DNS allows operators to label devices
meaningfully, and used both for forward translation, and as
a reverse lookup (for instance in traceroutes). The ability
to run standard services on our virtual routers simpliﬁes in-
cluding a DNS server. As as with every other component of
the network it must be conﬁgured, and that conﬁguration
has to be consistent with the name and IP address allo-
cations in the network. DNS can be conﬁgured using the
system, and has been used in an extension involving content
distribution [33].
However routing alone comprises only a subset of network
experiments. Many network experiments that require a re-
alistic routing topology, but are concerned with network ser-
vices built on the top of these. For these, automated conﬁg-
uration is even more important: automating the experiment
setup allows the researcher to concentrate on their experi-
ment, rather than setting up the laboratory.
Network experiments that combine both routers and servers
running network services are well suited to emulation. Netkit
is Linux based, making it is straightforward to run standard
Linux packages. The system can be extended to conﬁgure
network services, with our routing conﬁguration providing
a realistic network to conduct such experiments on. These
services can provide the traﬃc carried across the network.
A second extension of our basic system was conducted
by a group working under the auspices of the SIDR group
within the IETF, who are concerned with the creation of the
Resource Public Key Infrastructure (RPKI). The RPKI con-
sists of a set of CA servers with cryptographically secured
relationships in place. A set of cryptographically signed ob-
jects known as Route Origin Authorisations (ROA) are used
to attest the ownership of address space. The intention is
to distribute ROAs and Certiﬁcates to caches that will hold
data that routers use to make routing decisions so that pos-
sibly incorrect routes can be ﬁltered.
The service network therefore consists of a set of CA
servers to which address space is assigned, publication points
where the data are made available and a distribution hierar-
chy to cryptographically check the held information before
it is passed to routers. The conﬁgurations for these systems
are based on information in an input graph. This graph
holds the CA services and uses labelled edges to express
the relationships between the servers. The graph also shows
the distribution hierarchy. The system creates a set of con-
ﬁguration ﬁles for all the daemons and creates Linux VM
images that will put these ﬁles into place on boot. Using a
suitable hypervisor (currently KVM and libvirt are used for
VM management and Openvswitch to create a virtual layer-
2 network across multiple virtualisation hosts) we deploy the
many VMs together with their networking to a suitable set
of hosts, currently StarBed [1].
Topologies with over 800 Linux VMs have been deployed
successfully [30], with the system scalable to much larger
topologies.
4. CONFIGURATION SYSTEM DESIGN
The overall system structure is shown in Figure 2. There
are two key components: a device-conﬁguration generator
and a network compiler.
To use the system, a user ﬁrst speciﬁes their network ex-
periment as the input topology: an annotated attribute-
graph. This graph is used by the Network Design module
to create the protocol and service topology network-level
overlay graphs. The Compiler module condenses these over-
lay graphs into a single device-speciﬁc graph, and can ap-
ply device-speciﬁc operations, such as subnet formatting, to
match the semantics of the target device.
238in the PRESTO system [14]. Complicated transformations
are not performed in templates, but in a compiler module.
This approach provides transparency: templates closely
mirror the target conﬁguration language, so are familiar to
users experienced in network conﬁguration. It also facilitates
support of a wide range of vendors, device models and OS
versions as these can added simply through addition of a
new template.
An example template is shown below (% indicates control
logic, and ${...} variable substitution):
1 hostname ${node.zebra.hostname}
2 password ${node.zebra.password}
3 % for interface in node.interfaces:
4
interface ${interface.id}
ip ospf cost ${interface.ospf_cost}
5
6 % endfor
7 router ospf
8 % for link in node.ospf.ospf_links:
9
10 % endfor
network ${link.network.cidr} area ${link.area}
Figure 3: A Conﬁguration Generation System. A Resource
Database stores resources such as router names, IP ad-
dresses, and link costs. These are combined with device
syntax templates to generate low-level device conﬁguration
ﬁles to deploy to emulated hosts.
4.2 Network Design and Compilation
Templates are well-suited to device-level conﬁguration,
but not for expressing network-level abstractions. To keep
the templates light-weight, we restrict their use to the simple
logic outlined above. Adding the syntax to express network-
wide logic requires embedding a more feature-rich program-
ming language, which sacriﬁces both transparency and read-
ability.
Moreover, if templates try to cover too much ground, the
ﬂexibility required results in a very large set of templates.
Each new option on a protocol may result in creating a new
set of templates; conﬁguration can become an exercise in
maintaining a large library of templates.
Above our conﬁguration generation process, we have a
compilation module. This isn’t quite the same as a program-
ming language compiler, but performs the logic to condense
the network-level overlay topologies to a format suitable for
input to the templates. This compiler module Python-based
which reduces complicated data transforms and target de-
vice semantics to simple scripting. Decoupling these oper-
ations from templates improves readability, as logic is not
mixed within router syntax; and allows extensibility and re-
use, as common logic is inherited between target devices.
The network design and compilation module we have de-
veloped is illustrated in Figure 4. It takes a labelled graph as
input (in GraphML, a graph interchange format), and ﬁrst
uses the labels to create a set of arbitrary overlay graphs
Figure 2: Our emulated network experimentation system,
consisting of a Conﬁguration System and automated deploy-
ment and measurement modules. The Conﬁg Generation
and Network Design components are shown in greater de-
tail in Figure 3 and 4.
The Conﬁguration Generation module pushes the device-
speciﬁc graph into plain-text templates, to generate the con-
ﬁguration syntax for the target device. The Deployment
module automates the transfer and launch of these gen-
erated conﬁguration ﬁles, while the Measurement module
automates collection of network-related experiment data.
These can also be adapted as required by the experiment.
Finally, the plugins and visualization module allows analy-
sis and viewing of the topologies generated by the Network
Design module, and data collected from the Measurement
module. A more detailed system walkthrough is provided in
Section § 6.
The system has been developed to automate emulated
network conﬁgurations. This freed us from the burden of
mapping devices to hardware: the emulation environment
can construct topolgies with arbitrary numbers of devices
and their interconnects. However the system is not lim-
ited to only emulated devices: the internal device conﬁg-
urations are almost identical between emulated and physi-
cal devices (sometimes only diﬀering in interface naming).
The attribute approach can be used to describe a physi-
cal testbed, with the topology compiler described in § 5.4
mapping the devices in the network model to their physical
counterparts, and assigning appropriate interface names.
4.1 Device Conﬁguration Generation
Network devices and services are conﬁgured through a set
of low-level commands, kept in a conﬁguration ﬁle. The ﬁrst
task is to generate these from a device-independent descrip-
tion of the network components. We perform this task using
the template-based approach used in several projects [4,14],
illustrated in Figure 3.
The Resource Database (generated by the compilation pro-
cess described below) stores device-vendor independent net-
work attributes such as hostnames, IP addresses, and links
between devices. A Conﬁguration Generation module com-
bines this database with low-level templates that contain the
device-speciﬁc syntax of the targets. The templates include
simple logic, such as for loops, conditionals and variable sub-
stitution, or basic formatting, such as IP addresses, as found
Conﬁguration SystemDeploymentEmulated Networkr1r2r3r4r5MeasurementCompilerNetwork DesignInputTopologyVisualization& PluginsDeviceConﬁgsDeviceConﬁgsDeviceConﬁgsConﬁg GenerationDeviceConﬁgsDeviceConﬁgsTemplatesConﬁg GenerationIOSQuaggaJunosC-BGPDeviceConﬁgsDeviceConﬁgsTemplatesResource Database239(a) Input Topology
(b) OSPF Topology
Figure 4: The Network Design and Compilation System.
An input topology is used to create overlay graphs which
represent routing and service topologies, which the compiler
uses to create the device-independent view of the network
in the Resource Database of Figure 3.