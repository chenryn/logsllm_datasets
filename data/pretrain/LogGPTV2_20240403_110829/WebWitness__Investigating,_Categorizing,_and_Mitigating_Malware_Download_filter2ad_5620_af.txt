difﬁcult to evade.
6 Related Work
Client honeypots actively visit webpages and detect
drive-by downloads though observing changes to the sys-
tem [1, 21, 22, 28, 29] or by analyzing responses for ma-
licious content [3, 10, 24]. These systems tend to have a
low false positive rate, but only ﬁnd malicious websites
by visiting them with exploitable browser conﬁgurations;
also, they have limited range in the quantity of pages
they can crawl because they are much slower than static
crawlers. Often candidate URLs are selected by ﬁltering
content from static crawlers [28, 29, 35], using heuristics
to visit parts of the web that are likely more malicious [8]
or using search engines to identify webpages that contain
content similar to known malicious ones [12].
A number of techniques have been developed to de-
tect drive-by downloads through examining content [10,
11, 15, 32, 34]. Signature based intrusion detection sys-
tems, such as Snort [34], passively search network trafﬁc
content for patterns of known attacks. Both static [11]
and dynamic [10] analysis of JavaScript has been used to
detect attacks. The disadvantages of using content is that
it is complex and under the control of the attacker. Poly-
morphic malware and code obfuscation results in missed
attacks for signature and static analysis systems, and dy-
namic analysis can be detected by malware and subverted
by altering its execution path [15].
Other systems focus on the redirection chain that leads
to drive-by downloads. Stringhini et al [36] create redi-
rection graphs by aggregating redirection chains that end
at the same webpage. Features from the redirection
graph and visiting users are then used to classify the
webpage as malicious or benign. Mekky et al [20] build
browsing activity trees using the referrer and redirection
headers as well as URLs embedded in the content. Fea-
tures related to the redirection chain for each tree are ex-
tracted and used to classify the activity as malicious or
benign. Li et al [17] apply page rank from the dark and
bright side of the web to a partially labeled set of redirec-
tion chains to separate benign and malicious web paths.
They ﬁnd the majority of malicious paths are directed
through trafﬁc distribution systems. Using features from
the redirection chain, Surf [18] detects malicious web-
sites found in search engine results due to search poison-
ing and WarningBird [16] identiﬁes malicious webpages
posted on Twitter. These systems focus on the redirection
chain and features extracted from it to classify a web ac-
tivity as benign or malicious. Whereas, WebWitness pro-
vides context to malicious downloads by reconstructing
1038  24th USENIX Security Symposium 
USENIX Association
14
the full download path (not just the redirection chain),
classifying the cause of the download (drive-by, social,
update) and identifying the roles of the domains involved
in the attack.
Static blacklists [2] of domains/URLs and domain rep-
utation systems [4, 5] identify malicious websites to pre-
vent users from visiting them. Many of the domains on
static blacklists are exploit and download domains that
change frequently rendering them less effective. On the
other hand, reputation systems only provide a malicious
score for a domain and do not indicate their role or give
context to an attack. By analyzing the structure of a ma-
licious download, WebWitness can identify the type of
attack and the domain roles; providing the highest value
domains for blocking and reputation training data.
Recently researchers have proposed executable repu-
tation systems [13,30,38] due the limitations of signature
AV [27]. Instead of using content features from the exe-
cutable content, they focus on properties of the malware
distribution infrastructure. These systems can be very
effective at identifying malicious downloads. However,
they do not provide any context such as how and why
the user came to download a malicious executable. Pro-
viding download context is the goal of WebWitness not
malicious executable detection. We see these systems as
complementary to WebWitness and as good candidates
to replace our current oracle (signature AV) for malicious
executable detection.
Web trafﬁc reconstruction has been studied for ex-
ample in [9, 25, 39]. WebPatrol [9] uses a client hon-
eypot and a modiﬁed web proxy to collect and replay
web-based malware scenarios. Unlike WebPatrol, Web-
Witness is not limited to drive-by downloads invoked
through client honeypots and can provide context to
drive-by and social engineering attacks on real users ob-
served on live networks. ReSurf [39] uses the referrer
header to build graphs of related HTTP transactions to
reconstruct web-surﬁng activities. As discussed in this
paper and evaluated in [25], this approach is very limited
especially in reconstructing the entire download path of
a malicious executable. Lastly, ClickMiner [25] recon-
structs user-browser interactions by replaying recorded
network trafﬁc through an instrumented browser.
Its
focus is on the user’s behavior that led to a webpage;
whereas, WebWitness identiﬁes the cause and structure
of an attack that led to a malicious download.
7 Conclusion
We proposed a novel
investigation system,
named WebWitness. Our system targets two main goals:
1) automatically trace back and label the chain of events
(e.g., visited web pages) preceding malware downloads,
to highlight how users reach attack pages on the web; and
2) leverage these automatically labeled in-the-wild mal-
incident
ware download paths to better understand current attack
trends, and to develop more effective defenses.
We deployed WebWitness on a large academic net-
work for a period of 10 months, where we collected and
categorized thousands of live malware download paths.
An analysis of this labeled data allowed us to design a
new defense against drive-by downloads that rely on in-
jecting malicious content into (hacked) legitimate web
pages. For example, we show that on average by using
the results of WebWitness we can decrease the infection
rate of drive-by downloads based on malicious content
injection by almost 6 times, compared to existing URL
blacklisting approaches.
Acknowledgments
This material is based in part upon work supported by
the National Science Foundation (NSF) under grant No.
CNS-1149051 and US Department of Commerce (Com-
merce) under grant no. 2106DEK. Any opinions, ﬁnd-
ings, and conclusions or recommendations expressed in
this material are those of the authors and do not neces-
sarily reﬂect the views of the NSF and Commerce.
References
[1] Capture-hpc client honeypot. https://projects.honeynet.
org/capture-hpc.
[2] Google safe browsing api. https://developers.google.
com/safe-browsing/.
[3] Honeyc. https://projects.honeynet.org/honeyc.
[4] ANTONAKAKIS, M., PERDISCI, R., DAGON, D., LEE, W.,
AND FEAMSTER, N. Building a dynamic reputation system for
DNS. In the Proceedings of 19th USENIX Security Symposium
(USENIX Security ’10) (2010).
[5] ANTONAKAKIS, M., PERDISCI, R., LEE, W., DAGON, D., AND
VASILOGLOU, N. Detecting Malware Domains at the Upper
DNS Hierarchy.
In the Proceedings of 20th USENIX Security
Symposium (USENIX Security ’11) (2011).
[6] BAKHSHI, T., PAPADAKI, M., AND FURNELL, S. A practical
assessment of social engineering vulnerabilities. In 2nd Interna-
tional Symposium on Human Aspects of Information Security &
Assurance (HAISA 2008) (2008), pp. 12–23.
[7] BREIMAN, L. Random forests. Mach. Learn. 45, 1 (Oct. 2001).
[8] CANALI, D., COVA, M., VIGNA, G., AND KRUEGEL, C.
Prophiler: A fast ﬁlter for the large-scale detection of malicious
web pages. In Proceedings of the 20th International Conference
on World Wide Web (New York, NY, USA, 2011), WWW ’11,
ACM.
[9] CHEN, K. Z., GU, G., ZHUGE, J., NAZARIO, J., AND HAN,
X. Webpatrol: Automated collection and replay of web-based
malware scenarios. In Proceedings of the 6th ACM Symposium
on Information, Computer and Communications Security (New
York, NY, USA, 2011), ASIACCS ’11, ACM.
[10] COVA, M., KRUEGEL, C., AND VIGNA, G. Detection and anal-
ysis of drive-by-download attacks and malicious javascript code.
In Proceedings of the 19th International Conference on World
Wide Web (New York, NY, USA, 2010), WWW ’10, ACM.
[11] CURTSINGER, C., LIVSHITS, B., ZORN, B., AND SEIFERT, C.
Zozzle: Fast and precise in-browser javascript malware detec-
tion. In Proceedings of the 20th USENIX Conference on Security
(Berkeley, CA, USA, 2011), SEC’11, USENIX Association.
USENIX Association  
24th USENIX Security Symposium  1039
15
[12] INVERNIZZI, L., BENVENUTI, S., COVA, M., COMPARETTI,
P. M., KRUEGEL, C., AND VIGNA, G. Evilseed: A guided ap-
proach to ﬁnding malicious web pages.
In Proceedings of the
2012 IEEE Symposium on Security and Privacy (2012), SP ’12,
IEEE Computer Society.
[13] INVERNIZZI, L., LEE, S.-J., MISKOVIC, S., MELLIA, M.,
TORRES, R., KRUEGEL, C., SAHA, S., AND VIGNA, G. Nazca:
Detecting malware distribution in large-scale networks.
[14] JONES, M. Protecting privacy with referrers, 2010. https:
//www.facebook.com/notes/facebook-engineering/
protecting-privacy-with-referrers/392382738919.
[15] KAPRAVELOS, A., SHOSHITAISHVILI, Y., COVA, M.,
KRUEGEL, C., AND VIGNA, G. Revolver: An automated ap-
proach to the detection of evasiveweb-based malware.
In Pro-
ceedings of the 22Nd USENIX Conference on Security (Berkeley,
CA, USA, 2013), SEC’13, USENIX Association.
[16] LEE, S., AND KIM, J. Warningbird: A near real-time detection
system for suspicious urls in twitter stream. IEEE Trans. Depend-
able Secur. Comput. 10, 3 (May 2013).
[17] LI, Z., ALRWAIS, S., XIE, Y., YU, F., AND WANG, X. Finding
the linchpins of the dark web: A study on topologically dedicated
hosts on malicious web infrastructures.
In Proceedings of the
2013 IEEE Symposium on Security and Privacy (2013), SP ’13.
[18] LU, L., PERDISCI, R., AND LEE, W. Surf: Detecting and mea-
suring search poisoning. In Proceedings of the 18th ACM Con-
ference on Computer and Communications Security (New York,
NY, USA, 2011), CCS ’11, ACM.
[19] LU, L., YEGNESWARAN, V., PORRAS, P., AND LEE, W. Blade:
An attack-agnostic approach for preventing drive-by malware
infections.
In Proceedings of the 17th ACM Conference on
Computer and Communications Security (New York, NY, USA,
2010), CCS ’10, ACM.
[20] MEKKY, H., TORRES, R., ZHANG, Z.-L., SAHA, S., AND
NUCCI, A. Detecting malicious http redirections using trees of
user browsing activity. In INFOCOM, 2014 Proceedings IEEE
(2014).
[21] MIN WANG, Y., BECK, D., JIANG, X., ROUSSEV, R., VER-
BOWSKI, C., CHEN, S., AND KING, S. Automated web pa-
trol with strider honeymonkeys: Finding web sites that exploit
browser vulnerabilities. In In NDSS (2006).
[22] MOSHCHUK, E., BRAGIN, T., GRIBBLE, S. D., AND LEVY,
H. M. A crawler-based study of spyware on the web.
[23] NAPPA, A., RAFIQUE, M. Z., AND CABALLERO, J. Driving in
the cloud: An analysis of drive-by download operations and abuse
reporting. In Proceedings of the 10th International Conference on
Detection of Intrusions and Malware, and Vulnerability Assess-
ment (Berlin, Heidelberg, 2013), DIMVA’13, Springer-Verlag.
[24] NAZARIO, J. Phoneyc: A virtual client honeypot. In Proceed-
ings of the 2Nd USENIX Conference on Large-scale Exploits and
Emergent Threats: Botnets, Spyware, Worms, and More (Berke-
ley, CA, USA, 2009), LEET’09, USENIX Association.
[25] NEASBITT, C., PERDISCI, R., LI, K., AND NELMS, T. Click-
miner: Towards forensic reconstruction of user-browser interac-
tions from network traces.
In Proceedings of the 2014 ACM
SIGSAC Conference on Computer &#38; Communications Se-
curity (New York, NY, USA, 2014), CCS ’14, ACM.
[26] NELMS, T., PERDISCI, R., AND AHAMAD, M. Execscent: Min-
ing for new c&#38;c domains in live networks with adaptive con-
trol protocol templates.
In Proceedings of the 22Nd USENIX
Conference on Security (Berkeley, CA, USA, 2013), SEC’13,
USENIX Association.
[27] PERDISCI, R., LANZI, A., AND LEE, W. Classiﬁcation of
packed executables for accurate computer virus detection. Pat-
tern Recogn. Lett..
[28] PROVOS, N., MAVROMMATIS, P., RAJAB, M. A., AND MON-
ROSE, F. All your iframes point to us. In Proceedings of the 17th
Conference on Security Symposium (Berkeley, CA, USA, 2008),
SS’08, USENIX Association.
[29] PROVOS, N., MCNAMEE, D., MAVROMMATIS, P., WANG, K.,
AND MODADUGU, N. The ghost in the browser analysis of web-
based malware. In Proceedings of the First Conference on First
Workshop on Hot Topics in Understanding Botnets (2007), Hot-
Bots’07.
[30] RAJAB, M. A., BALLARD, L., LUTZ, N., MAVROMMATIS, P.,
AND PROVOS, N. Camp: Content-agnostic malware protection.
In Proceedings of Annual Network and Distributed System Secu-
rity Symposium, NDSS (February 2013) (2013), Citeseer.
[31] RAJAB, M. A., BALLARD, L., MAVROMMATIS, P., PROVOS,
N., AND ZHAO, X. The nocebo effect on the web: An analysis of
fake anti-virus distribution. In 3rd USENIX Conference on Large-
scale Exploits and Emergent Threats: Botnets, Spyware, Worms,
and More (Berkeley, CA, USA, 2010), LEET’10, USENIX As-
sociation.
[32] RATANAWORABHAN, P., LIVSHITS, B., AND ZORN, B. Nozzle:
A defense against heap-spraying code injection attacks. In Pro-
ceedings of the 18th Conference on USENIX Security Symposium
(Berkeley, CA, USA, 2009), SSYM’09, USENIX Association.
[33] RIECK, K., KRUEGER, T., AND DEWALD, A. Cujo: Efﬁ-
cient detection and prevention of drive-by-download attacks. In
Proceedings of the 26th Annual Computer Security Applications
Conference (New York, NY, USA, 2010), ACSAC ’10, ACM.
[34] ROESCH, M. Snort - lightweight intrusion detection for net-
works. In Proceedings of the 13th USENIX Conference on System
Administration (1999).
[35] STOKES, J. W., ANDERSEN, R., SEIFERT, C., AND CHEL-
LAPILLA, K. Webcop: Locating neighborhoods of malware on
the web. In Proceedings of the 3rd USENIX Conference on Large-
scale Exploits and Emergent Threats: Botnets, Spyware, Worms,
and More (2010), LEET’10, USENIX Association.
[36] STRINGHINI, G., KRUEGEL, C., AND VIGNA, G. Shady paths:
Leveraging surﬁng crowds to detect malicious web pages.
In
Proceedings of the 2013 ACM SIGSAC Conference on Computer
&#38; Communications Security (2013), CCS ’13, ACM.
[37] TOWNSEND, K. R&d: The art of social engineering. Infosecurity
7, 4 (2010), 32–35.
[38] VADREVU, P., RAHBARINIA, B., PERDISCI, R., LI, K., AND
ANTONAKAKIS, M. Measuring and detecting malware down-
loads in live network trafﬁc. In ESORICS. 2013.
[39] XIE, G., ILIOFOTOU, M., KARAGIANNIS, T., FALOUTSOS,
M., AND JIN, Y. Resurf: Reconstructing web-surﬁng activity
from network trafﬁc. In IFIP Networking Conference, 2013 (May
2013), pp. 1–9.
[40] ZHANG, J., SEIFERT, C., STOKES, J. W., AND LEE, W. Arrow:
Generating signatures to detect drive-by downloads. In Proceed-
ings of the 20th International Conference on World Wide Web
(New York, NY, USA, 2011), WWW ’11, ACM.
1040  24th USENIX Security Symposium 
USENIX Association
16