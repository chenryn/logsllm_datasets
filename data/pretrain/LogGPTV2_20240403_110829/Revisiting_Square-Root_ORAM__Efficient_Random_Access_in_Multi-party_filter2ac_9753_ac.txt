T ← (cid:100)(cid:112)W (n)(cid:101)
n ← |blocks|
(cid:104)π(cid:105) ← random permutation on n elements
Shuﬄe ← ObliviousPermute(blocks,(cid:104)π(cid:105))
Oram1 ← InitializePosMap((cid:104)π(cid:105),1,T )
Oram0 ← (n,t ← 0,T, Oram1, Shuﬄe,
Used ← ∅, Stash ← ∅)
deﬁne Initialize(blocks)
return Oram0
deﬁne Access (Oram0,(cid:104)i(cid:105),Φ, )
(cid:104)found(cid:105) ← false
for j from 0 to Oram.t:
(cid:104)if(cid:105) Oram0.Stash j.(cid:104)index(cid:105) = (cid:104)i(cid:105):
(cid:104)found(cid:105) ← true
Φ(Oram0.Stash j)
p ← GetPos(Oram0.Oram1,(cid:104)i(cid:105),(cid:104)found(cid:105))
(cid:104)if(cid:105) not (cid:104)found(cid:105):
Φ(Oram0.Shuﬄep)
append Oram0.Shuﬄep to Oram0.Stash
Oram0.Used ← Oram0.Used∪{p}
Oram0.t ← Oram0.t + 1
if Oram0.t = Oram0.T :
for j from 0 to Oram0.T − 1:
p(cid:48) ← Oram.Used j
Oram0.Shuﬄep(cid:48) ← Oram0.Stash j
Oram0 ← Initialize(Oram0.Shuﬄe)
Fig. 5: Our recursive square-root ORAM scheme. W (n) is the number of swaps needed in a n-sized Waksman
permutation network.
access, ignoring smaller terms that are independent of B.
With four blocks, the cost of a linear scan is 4B. Using a
shufﬂing period of T = 3, we get a cost of B(W (4) +1 +
2 + 3) = 11B for three accesses, again ignoring smaller
terms that are independent of B. This is slightly better
than the linear scan cost for three accesses, 3 × 4B =
12B. Thus, for four blocks of a large enough size, the
simpliﬁed one-level square-root ORAM is less expensive
than a linear scan, even after accounting for the cost of
initialization. However, in the case of small blocks, the
terms independent of B (which we have ignored) become
signiﬁcant enough that linear scan has a slight advantage.
In our experiments, we observed the square-root
scheme to be more efﬁcient in terms of bandwidth for
four blocks of just 36 bytes each (see Section V-B for
details). For larger block sizes, we found that the cost
ratio reaches 11 : 12, as expected.
D. Scalable Construction
So far, we have not discussed how to implement the
structure (cid:104)π(cid:105) more efﬁciently than linear scan, aside from
claiming that its costs do not depend on the block size.
For small values of n, linear scan is good enough, as in
the four-block example above. At this size, π comprises
just four records of two secret bits each. However, for
larger values of n, it may seem natural to build these
structures upon recursive ORAMs of decreasing size. As
we discuss next, however, this method is unacceptably
224224
costly. Our solution is to specialize the structure for
position maps.
The position map structure, (cid:104)π(cid:105), is common to most
existing tree-based constructions [30, 32, 34]. It is usu-
ally implemented atop recursive ORAMs of decreas-
ing size, each level packing multiple indices of the
previous into a single block, and the whole thing is
updated incrementally as elements of the main ORAM
are accessed. In these constructions, each ORAM lookup
requires a single corresponding lookup in each recursive
position maps. However, in our scheme, a naïve recursive
structure for (cid:104)π(cid:105) would require n + T position lookups
for every T accesses to the main ORAM (where T is the
number of accesses between shufﬂes) since each of the
T main accesses would require an access to the position
map, and additional n accesses would be required to
store the regenerated permutation π(cid:48) when the ORAM
is shufﬂed.
This is a serious problem: each level of the recursive
structure would need to store pack indices of the previous
level in a single block, which would be traversed by
linear scan. Thus, each subsequent level decreases in
element count by a factor of pack, but all levels require
pack time to linear scan the relevant block. We can
multiply by (n + T )/T to amortize the cost over T
accesses, where T =(cid:112)nlog2 n, the shufﬂe period (as
computed in Section III-C). If the amortized cost per
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:11:03 UTC from IEEE Xplore.  Restrictions apply. 
deﬁne GetPos(Oramk,(cid:104)i(cid:105),(cid:104)fake(cid:105))
if Oramk.n/pack ≤ Oramk.T :
else:
p ← GetPosBase(Oramk,(cid:104)i(cid:105),(cid:104)fake(cid:105))
(cid:104)found(cid:105) ← false
(cid:104)h(cid:105) ← (cid:104)i(cid:105)/pack
(cid:104)l(cid:105) ← ((cid:104)i(cid:105) mod pack)
for j from 0 to Oramk.t − 1:
(cid:104)if(cid:105) Oramk.Stash j.(cid:104)index(cid:105) = (cid:104)h(cid:105):
(cid:104)found(cid:105) ← true
block ← Oramk.Stash j
(cid:104)p(cid:105) ← block.(cid:104)data(cid:105)(cid:104)l(cid:105)
p(cid:48) ← GetPos(Oramk+1,(cid:104)h(cid:105),(cid:104)fake(cid:105) or (cid:104)found(cid:105))
append Oramk.Shuﬄep(cid:48) to Oramk.Stash
Oramk.t ← Oramk.t + 1
(cid:104)if(cid:105) (cid:104)fake(cid:105) or not (cid:104)found(cid:105):
block ← Oramk.Stash(Oramk.t−1)
(cid:104)p(cid:105) ← block.(cid:104)data(cid:105)(cid:104)l(cid:105)
p ← reveal((cid:104)p(cid:105))
return p
deﬁne InitializePosMap((cid:104)π(cid:105),k,T )
(cid:10)Used0···(n−1)
(cid:11) ← (false, . . . ,false)
n ← |(cid:104)π(cid:105)|
if n/pack ≤ T :
Oramk ← (n,t ← 0,T,(cid:104)π(cid:105),(cid:104)Used(cid:105))
else:
for i ∈ {0 . . .(cid:100)n/pack(cid:101)− 1}:
(cid:104)data(cid:105) ← ((cid:104)π(cid:105)pack·i, . . . ,(cid:104)π(cid:105)pack·(i+1)−1)
blocksi ← ((cid:104)data(cid:105),(cid:104)index(cid:105) ← i)
(cid:104)π(cid:48)(cid:105) ← random permutation on (cid:100)n/pack(cid:101) elements
Shuﬄe ← ObliviousPermute(blocks,(cid:104)π(cid:48)(cid:105))
Oramk+1 ← InitializePosMap((cid:104)π(cid:48)(cid:105),k + 1,T )
Oramk ← (n,t ← 0,T, Oramk+1, Shuﬄe,
Stash ← ∅)
return Oramk
deﬁne GetPosBase(Oramk,(cid:104)i(cid:105),(cid:104)fake(cid:105))
(cid:104)p(cid:105) ← ⊥
(cid:104)done(cid:105) ← false
for j from 0 to (Oramk.n− 1):
(cid:104)s1(cid:105) ← (not (cid:104)fake(cid:105) and (cid:104)i(cid:105) = j)
(cid:104)s2(cid:105) ←(cid:0)(cid:104)fake(cid:105) and not Oramk.(cid:104)Used(cid:105) j
(cid:104)p(cid:105) ←(cid:10)π j
and not (cid:104)done(cid:105)(cid:1)
(cid:104)if(cid:105) (cid:104)s1(cid:105) or (cid:104)s2(cid:105):
Oramk.(cid:104)Used(cid:105) j ← true
(cid:104)done(cid:105) ← true
(cid:11)
p ← reveal((cid:104)p(cid:105))
return p
Fig. 6: Implementation of the recursive position map.
access to level i of this map is ci(n), we have:
T (ci+1(n/pack) + pack)
ci(n) ≥ n + T
n(cid:112)nlog2 n
(cid:114) n
log2 n
≥
≥
ci+1(n/pack)
ci+1(n/pack).
This is a super-polynomial function with Θ(logn) levels
of recursion, which is unacceptable for our efﬁciency
goals. Fixing this involves three changes to our basic
construction.
The ﬁrst change is to take advantage of our ability
to initialize quickly from an oblivious array. On each
shufﬂe, we regenerate π, and,
instead of writing it
into the recursive structure element by element, we re-
initialize the recursive structure using π(cid:48) as the seed data.
This eliminates the extra n accesses to the position map
on each cycle.
to the same shufﬂe period, T =(cid:112)nlog2 n, where n is
Second, we lock all levels of the recursive structure
the number of blocks in the main ORAM (the level that
contains the original data). We terminate the recursion at
the ﬁrst level with fewer than T blocks, and access this
ﬁnal level using linear scan. Using this arrangement, we
can initialize the entire ORAM in Θ(Bnlogn) bandwidth
and time.
This second modiﬁcation has a downside. All levels
of the recursive ORAM shufﬂe in synchronization with
one another, based on a shufﬂe period determined by
the largest level. This shufﬂe period will be signiﬁcantly
suboptimal for levels with fewer blocks. We pay a
time and bandwidth cost of Θ(T ) at each level of the
225225
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:11:03 UTC from IEEE Xplore.  Restrictions apply. 
(a) Physical layout before shufﬂing
(d) First request: logical index 8
(b) Physical layout after shufﬂing
(e) Second request: logical index 9
(c) Temporal control ﬂow for all requests
(f) Third request: logical index 8
Fig. 7: Illustration of data ﬂow for one full cycle of an example ORAM. In subﬁgures (d), (e), and (f) we
present the logical dependencies for three sequential accesses.
(cid:112)nlog3 n). However, the linear scan
ORAM (for linearly scanning the Θ(T ) blocks in each
level’s Stash). An ORAM instantiated with n elements
will have logn levels, which brings the cost per access
to Θ(T logn) = Θ(
overhead incurred by using a global shufﬂing period is
compensated for by gains in the efﬁciency of Used which
it enables.
ready been accessed — yet, they must obliviously check
whether it contains a secret logical index (cid:104)p(cid:105). Moreover,
they must be able to sample a secret, uniform element
from S = {0, . . . ,n − 1} \ Used. The simplest method
would be to sample an integer from {0, . . . ,|S| − 1}
and then obliviously map it to the set S, an expensive
operation.
Constructing an efﬁcient mechanism for keeping track
of used and unused physical blocks poses a challenge.
Used contains inherently public data — both parties
are aware which physical locations in Shuﬄe have al-
The third change removes the need to obliviously
check Used for secret index (cid:104)p(cid:105). Instead of using an
explicit data structure, our choice of a global shufﬂe
period allows us to implicitly represent a superset of
226226
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:11:03 UTC from IEEE Xplore.  Restrictions apply. 
0123456789012340120123456789012340Data Block withlogical index 0123Mapping Block withlogical index 1, referencingphysical indices 2 and 3Stash (empty)Legend0123456789134012918720204312450360123456789134012918720204312450361235642012345678913401918720204312450364208LegendReal request (cid:31)owFake request (cid:31)owMovement of datafrom Shu(cid:30) to Stash4=⌊8/2⌋2=⌊8/4⌋8143878012345679130918720432450362094=⌊9/2⌋random938701234567109120245036420890360random8Used in the recursive structure (cid:104)π(cid:105), by tracking which
blocks the smallest recursive level have been used.
We use the notation Oramk.Stash, Oramk.Shuﬄe, and
Oramk.Used to represent the corresponding structures in
recursive ORAM at level k. Oram0 is the main ORAM
that holds the data blocks; Oram1 is the top level of
the position map (cid:104)π(cid:105); Oram2 and so on indicate deeper
levels of the recursive position map structure.
We maintain the invariant that if a block has already
been moved from Oramn.Shuﬄe to Oramn.Stash, the
corresponding block in Oramn+1 has also been moved
from Oramn+1.Shuﬄe to Oramn+1.Stash. The converse
is not necessarily true: it is possible for Oramn+1.Stash
to contain blocks that map to unaccessed blocks in
Oramn. This can happen, for example, if logical block
i of Oram0 has been accessed and block i + 1 has not,
but mapping information for both blocks resides in the
same block of Oram1.
Randomly sampling an unused block with this con-
struction is simple. At the smallest level the blocks are
linearly scanned, so we just pick the ﬁrst unused element.
This is guaranteed to point to a random unused position.
At the next recursive level, we can use any element
in the block referred to by the index from the ﬁrst
level, since they are all random and unused. The process
continues to ripple upward until an unused block in the
required ORAM level has been selected. This method
excludes from the set to be randomly sampled any block
referred to by a block that has been accessed at a lower
level. Nonetheless, blocks sampled randomly remain
indistinguishable from genuine accesses, as, for each top
level access, exactly one unused block is accessed at each
lower level.
The ﬁnal construction is presented in Figures 5 and 6,
and the life-cycle of the ORAM is illustrated in Figure 7.
IV. TECHNIQUES AND OPTIMIZATIONS
This section presents some of the lower-level tech-
niques used in our implementation.
Shufﬂing. We employ a Waksman network [33] for
shufﬂing. The network executes many oblivious swap
operations, each controlled by a secret bit determined
by the permutation π. Let B be the number of bytes
transferred when obliviously swapping two blocks of
data. Since a Waksman network for shufﬂing requires
W (n) = nlog2 n− n + 1 swap operations, it is expected
that the two parties will transfer BW (n) bytes during a
shufﬂe, excluding the secret control bits.
The control bits pose a problem: neither party can
learn anything about the randomly sampled permutation
227227
π, but we do not know an efﬁcient oblivious algorithm
for computing the corresponding control bits. To solve
this problem, we perform two shufﬂes: the parties lo-
cally a pick secret permutation each and compute their
corresponding control bits in the clear. Each party’s
local permutation constitutes its share in the ﬁnal secret
permutation π, which is the composition of the two
permutations. So long as at
least one party behaves
honestly, the result is a uniformly random permutation,
discoverable to neither. They can jointly shufﬂe the data
by running two consecutive shufﬂing networks, one for
each permutation.
Performing a shufﬂe in this way is quite inexpensive.
The bandwidth cost of 2W (n) swaps is comparable to
W (n) AND gates, using the oblivious shufﬂe design from
Huang et al. [16] and half-gates technique from Zahur et
al. [41]. However, each time we perform a shufﬂe, we
incur the latency of a network round-trip, since the
evaluator retrieves new garbled labels for control bits
via oblivious transfer extension [2].
Computing the permutation. Whenever the data in
Shuﬄe is shufﬂed, we must reinitialize the recursive
position map so that it contains the new secret permuta-
tion, π. The ﬁrst time we perform a shufﬂe obliviously
computing π is straightforward. Because the shufﬂe
was performed with the composition of two Waksman
networks as described previously, we can run the same
network backwards using (0, . . . ,n − 1) as inputs to
obtain π.
On subsequent shufﬂes, the process becomes compli-
cated. The blocks in Shuﬄe are no longer in logical
order because they have previously been shufﬂed and
moved from Shuﬄe to Stash and back. Obtaining the
permutation by the same method as above would require
us to run both shufﬂes (four Waksman networks in total)
in reverse, along with any other swaps that may have
happened due to ordinary ORAM access. Each additional
shufﬂe requires two more Waksman networks, and the
number continues to increase without bound.
Instead, we augment each data block with a secret
record of its logical index. When the blocks are shufﬂed,
the logical indices are shufﬂed with them through the
Waksman networks, and these indices comprise π−1, the
mapping from physical to logical index. To ﬁnd π, the
mapping from logical to physical, we simply invert π−1.
To invert π−1 efﬁciently without allowing either party
to learn anything about it, we adopt a technique from
Damgård et al. [6]. The ﬁrst party (Alice) locally sam-
ples a new random permutation πa and computes the
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:11:03 UTC from IEEE Xplore.  Restrictions apply. 