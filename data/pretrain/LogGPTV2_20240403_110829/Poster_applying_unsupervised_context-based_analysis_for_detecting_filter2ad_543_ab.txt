item-sets  that  are  used  for  representing  the  leaves,  we  use  the 
Apriori  algorithm  [4],  which  produces  association  rules  and 
frequent  item-sets.  Our  goal  is  to  represent  the  dataset  with  the 
least  amount  of  rules  on  one  hand,  and  to  represent  the  largest 
portion  of  records  on  the  other.  Therefore,  we  do  not  rely  on  a 
single  output  of  the  Apriori  algorithm,  but  rather  we  use  an 
iterative process, which reduces the number of rules while taking 
into account the greater part of the records in the leaf record set. 
The process of deriving a set of rules representing a specific leaf 
is described in Figure 2. The first step of the process is to generate 
all  rules  that  are  supported  by  a  predefined  number  of  records 
(minsupport). Once a set of rules had been established, the rule with 
the  highest  support  will  be  inserted  into  the  final  set  of  rules. 
Then, all records that support the chosen rule are  removed from 
the dataset, and the whole process is repeated with the remaining 
dataset. 
Figure. 2. The process of deriving a set of rules representing a 
leaf data set. 
There  are  two  advantages  of  representing  the  leaves  as  a  set  of 
rules rather than keeping the legitimate set of data in each of the 
leaves.  First,  this  allows  us  to  create  a  smaller  and  more 
generalized representation of the data. In addition, this allows us 
to  better  cope  with  the  frequent  changes  in  the  database.  When 
keeping the entire data set in the leaf, any change in the database 
(e.g.,  INSERT,  DELETE)  might  result  in  an  outdated  data  set. 
This would mean that retraining would be necessary quite often. 
However,  when  representing  the  leaf  data  set  as  a  set  of  rules, 
changes in the database would not necessarily require retraining. 
3.  Preliminary Evaluation 
Since no real dataset was available for evaluation, we opt to 
generate  a  simulated  dataset.  The  simulated  data were generated 
according  to  real  scenarios  described  in  [5].  The  data  include 
requests  for  customer  records  of  an  organization,  submitted  by 
business  partner  of  the  organization.  Contextual  information  on 
the  requests  includes  the  time  of  execution,  day  of  execution, 
geographical location of the action, the user’s role, and the type of 
action.  Sensitive  information  of  customers  includes  customer’s 
name,  address,  zip  code,  place  of  work,  and  the  customer  type 
(e.g., business, private, and student). 
The  simulated  requests  were  generated  according  to  one  of  the 
following  three  behavior  types:  (1)  Normal  behavior  (retrieving 
customer  records  within  the same geographical location, during 
store opening hours); (2) Malicious1 (retrieving customer record 
who is not in the same geographical location as the store during 
opening  hours);  and  (3)  Malicious2  behavior  (searching  for  any 
customer record after closing hours). 
In addition, we defined two types of users: a benign user submits 
legitimate requests most of the time; however, on rare occasions, 
this user might have to perform actions that may seem malicious 
but  are  actually  required in the course of his work. A  malicious 
user is an employee who queries the database for a purpose other 
than his work (e.g., data harvesting). We believe that a malicious 
While exist itemsetswith minsupportOrdataset size > t * initialdataset sizeGenerate all itemsetswith the pre-defined minsupportChoose itemset with highestminsupportRemove all records from the dataset which support the chosen itemset766user  might  try  to  hide  his  malicious  intentions  by  mixing 
malicious queries with legitimate ones.  
The  goal  of  our  evaluation  process  was  to  identify  correctly  as 
many  malicious  transactions  as  possible  (true  positive)  while 
keeping the false alarms to a minimum (false positive). 
The evaluation results are presented in the ROC graph in Figure 3. 
When  setting  the  threshold  to  0.5  (an  anomaly  score  above  this 
would imply that the request may be malicious) yields a high true 
positive rate (TPR) of 0.93 with low false positive rate (FPR) of 
0.09.  When  setting  the  threshold  to  0.55,  the  TPR  is  slightly 
reduced to 0.88, but on the other hand, the FPR reduces to 0.01. 
We  also  compare  our  proposed  solution  with  the  supervised 
solution  we  had  presented  in  [5].  In  addition,  we  extend  the  S-
vector  method  [2]  to  consider  the  context  by  clustering  the 
requests according to various context attributes and creating an S-
vector for each context. 
Figure 3 presents the ROC curves of the proposed unsupervised 
method,  the  supervised  algorithm  presented  in  [5],  and  the  best 
setting of the S-vector approach (achieved by setting the city and 
the  request  type  as  the  context  attributes).  It  is  visible  that  the 
supervised  approach  yields  slightly  better  results  than  the 
unsupervised  approach.  In  addition,  when  comparing  the  area 
under  the  curve  (AUC)  measure,  the  supervised approach yields 
the  highest  score  (0.9906),  followed  by 
the  unsupervised 
approach  (0.9627).  The  S-vector  approach  yields  an  AUC  of 
0.9346. 
Figure.  3.  ROC  curves of the supervised [5], the S-vector [2] 
and the unsupervised approaches. 
However,  the supervised approach requires a completely labeled 
training set containing both benign and malicious examples. This 
implies that malicious requests and matching result sets must be 
artificially generated and added to the database. This differs from 
the  unsupervised  approach,  which  does  not  require  a  labeled 
training  set.  Additionally,  the  unsupervised  method  is  more 
efficient  than  the  supervised  method.  When  analyzing  a  new 
request, the supervised model needs to classify each record in the 
result set individually, before it generates the final anomaly score.  
The unsupervised approach on the other hand, offers a much more 
efficient  detection  process.  The  model  is  scanned  only  once  in 
order  to  retrieve  the  relevant  set  of  rules.  Then,  the  result  set is 
examined to see what proportion of it matches the rules. 
4.  CONCLUSIONS 
In  this  paper,  we  present  an  unsupervised  approach  for 
detecting  unauthorized  data  disclosure  by  insiders  who  have 
access privileges to the organization's sensitive data. This method 
is based on a detection model that encapsulates a set of rules that 
describe  the  legitimate  user  behavior  for  each  possible  context. 
During the detection phase, upon the arrival of a new request, the 
appropriate set of rules is retrieved from the model according to 
the context of the query. Then, the result set is cross validated to 
see to what extent it is compliant with the set of rules. 
The  proposed  method  offers  several  important  advantages.  The 
first is profiling user actions based on the context of the request. 
This improves detection accuracy since the same request may be 
legitimate  if  performed  within  one  context  but  abnormal  within 
another  context.  Additionally, 
is  capable  of 
identifying  the  context  attributes  that  maximize  the  detection 
accuracy, and considers only them. Second, the proposed method 
analyzes the result sets retrieved by the user, thus explicitly taking 
into  account  the  data  that  might  be    misused.  Third,  only 
legitimate requests are required for training the detection model. 
Thus,  there  is  no  need  to  collect  or  artificially  add  malicious 
records and to retrain the model whenever a new type of malicious 
behavior is discovered. 
In  future  work  we  plan  to  examine  additional  methods  for 
representing  the  leaf  datasets,  such  as  statistical  representation. 
This  will  allow  to  generate  more generalized rules such as what 
attributes  are  usually  provided  within  a given context (e.g., user 
from Berlin usually retrieves data about credit cards, while users 
from  Bonn  usually  retrieve  the  customer's  name  and  address). 
Furthermore, we plan to apply the proposed method on different 
domains and test it on a real dataset. 
the  method 
5.  REFERENCES 
[1]  A. Kamra, E. Terzi, T. Evimaria, and E. Bertino. Detecting 
Anomalous Access Patterns in Relational Databases. Journal 
on Very Large Databases. 17(5): 1063-1077, 2008. 
[2]  S. Mathew, M. Petropoulos, H. Ngo, and S. Upadhyaya. A 
Data-Centric Approach to Insider Attack Detection in 
Database Systems. In Recent Advances in Intrusion Detection. 
pages 382-401, 2010. 
[3]  S. Guha, R. Rastogi, and K. Shim. Rock: A robust clustering 
algorithm for categorical attributes. Information Systems, 
25(5): 345-366, 2000. 
[4]  R. Agrawal, T. Imieliniski, and A. Swami. Mining association 
rules between sets of items in large databases. SIGMOD Rec. 
22(2): 207-216, 1993. 
[5]  M. Gafny, A. Shabtai, L. Rokach, and Y. Elovici. Detecting 
Data Misuse By Applying Context-Based Data Linkage. In 
ACM CCS Workshop on Insider Threats, 2010. 
00.20.40.60.8100.20.40.60.81TPRFPRSupervisedUnsupervisedS-Vector767