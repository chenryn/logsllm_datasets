### Transplanting Gadgets to Evade Malware Detection

To address the scenario where \( h(x) < 0 \), we employ a method that involves transplanting harvested gadgets from an "ice-box" \( G \) into the malicious host \( z \). The process begins by identifying a list of ice-box gadgets that can be injected into \( z \). Each gadget \( \rho_j \) in the ice-box \( G \) has a feature vector \( r_j \) that includes both the desired features and side-effect features.

The actual contribution of each gadget \( i \) to the malicious host \( z \) with features \( x \) is determined by performing the set difference between the two binary vectors, \( r_j \wedge \neg x \). We then sort the gadgets in descending order based on their negative contribution, which ideally leads to a faster convergence of \( z \)'s score to a benign value.

Next, we filter this candidate list to include only those gadgets that meet practical feasibility criteria. We define a feasibility check function that implements heuristics to limit excessive increases in certain statistics, which could raise suspicion. Preliminary experiments revealed a tendency to add too many permissions to the Android Manifest, so we empirically enforce that candidate gadgets add no more than one new permission to the host app. Additionally, we do not allow the addition of permissions listed as dangerous in the Android documentation [5]. Other app statistics are kept within the distribution of benign apps (see §IV for further discussion), so we do not impose limits on them.

For the remaining candidate gadgets, we iterate over each candidate \( \rho_j \) and combine its feature vector \( r_j \) with the input malware feature vector \( x \) using the logical OR operation: \( x' = x \vee r_j \). This process is repeated until the updated \( x' \) is classified as goodware (for low-confidence attacks) or until an attacker-defined confidence level is achieved (for high-confidence attacks). Finally, all the candidate gadgets are injected simultaneously through automated software transplantation, and we verify that the problem-space constraints are satisfied and that the app is still classified as goodware. The detailed steps of the attack phase are provided in Algorithm 2 in Appendix F.

### Experimental Evaluation

We evaluate the effectiveness of our novel problem-space Android attack in terms of success rate and required time, and also in the presence of feature-space defenses.

#### A. Experimental Settings

**Prototype:** We developed a prototype of our novel problem-space attack (§III) using Python for the machine learning (ML) functionality and Java for program analysis operations. Specifically, we use FlowDroid [9], which is based on Soot [68], to perform transplantations in the problem space. The code for our prototype is available to other academic researchers (see §VII). All experiments were conducted on an Ubuntu VM with 48 vCPUs, 290GB of RAM, and an NVIDIA Tesla K40 GPU.

**Classifiers:** As defined in the threat model (§III-A), we consider the DREBIN classifier [8], which is based on a binary feature space and a linear SVM, and its recently proposed hardened variant, Sec-SVM [23], which requires the attacker to modify more features to perform an evasion. We use a hyperparameter \( C=1 \) for the linear SVM, as in [8], and identify the optimal Sec-SVM parameter \( k = 0.25 \) (i.e., the maximum feature weight) in our setting by enforcing a maximum performance loss of 2% AUC. See Appendix E for implementation details.

**Attack Confidence:** We consider two attack settings: low-confidence (L) and high-confidence (H). The (L) attack merely overcomes the decision boundary (so that \( h(x) < 0 \)). The (H) attack maximizes the distance from the hyperplane into the goodware region; while generally this distance is unconstrained, here we set it to be ≤ the negative scores of 25% of the benign apps (i.e., within their interquartile range). This avoids making superfluous modifications, which may only increase suspiciousness or the chance of transplantation errors, while being closer in nature to past mimicry attacks [12].

**Dataset:** We collected apps from AndroZoo [2], a large-scale dataset with timestamped Android apps crawled from different stores, and with VirusTotal summary reports. We use the same labeling criteria as Tesseract [55] (which is derived from Miller et al. [49]): an app is considered goodware if it has 0 VirusTotal detections, as malware if it has 4+ VirusTotal detections, and is discarded as grayware if it has between 1 and 3 VirusTotal detections. For the dataset composition, we follow the example of Tesseract and use an average of 10% malware [55]. The final dataset contains approximately 170K recent Android applications, dated between January 2017 and December 2018, specifically 152,632 goodware and 17,625 malware.

**Dataset Split:** Tesseract [55] demonstrated that, in non-stationary contexts such as Android malware, if time-aware splits are not considered, the results may be inflated due to concept drift (i.e., changes in the data distribution). However, here we aim to specifically evaluate the effectiveness of an adversarial attack. Although it likely exists, the relationship between adversarial and concept drift is still unknown and is outside the scope of this work. If we were to perform a time-aware split, it would be impossible to determine whether the success rate of our ML-driven adversarial attack was due to an intrinsic weakness of the classifier or due to natural evolution of malware (i.e., the introduction of new non-ML techniques malware developers rely on to evade detection). Hence, we perform a random split of the dataset to simulate the absence of concept drift [55]; this also represents the most challenging scenario for an attacker, as they aim to mutate a test object coming from the same distribution as the training dataset (on which the classifier likely has higher confidence). In particular, we consider a 66% training and 34% testing random split.

**Testing:** The test set contains a total of 5,952 malware. The statistics reported in the remainder of this section refer only to true positive malware (5,330 for SVM and 4,108 for Sec-SVM), i.e., we create adversarial variants only if the app is detected as malware by the classifier under evaluation. Intuitively, it is not necessary to make an adversarial example of a malware application that is already misclassified as goodware; hence, we avoid inflating results by removing false negative objects from the dataset. During the transplantation phase of our problem-space attack, some errors occur due to bugs and corner-case errors in the FlowDroid framework [9]. Since these errors are related to implementation limitations of the FlowDroid research prototype and not conceptual errors, the success rates in the remainder of this section refer only to applications that did not throw FlowDroid exceptions during the transplantation phase (see Appendix G for details).

#### B. Evaluation

We analyze the performance of our Android problem-space attack in terms of runtime cost and successful evasion rate. An attack is successful if an app \( z \), originally classified as malware, is mutated into an app \( z' \) that is classified as goodware and satisfies the problem-space constraints.

**Figure 2** reports the AUROC of SVM and Sec-SVM on the DREBIN feature space in the absence of attacks. As expected [23], Sec-SVM sacrifices some detection performance in return for greater feature-space adversarial robustness.

**Attack Success Rate:** We perform our attack using true positive malware from the test set, i.e., all malware objects correctly classified as malware. We consider four settings depending on the defense algorithm and the attack confidence: SVM (L), SVM (H), Sec-SVM (L), and Sec-SVM (H). In the absence of FlowDroid exceptions (see Appendix G), we are able to create an evasive variant for each malware in all four configurations. In other words, we achieve a misclassification rate of 100.0% on the successfully generated apps, where the problem-space constraints are satisfied by construction (as defined in §III). **Figure 3** reports the cumulative distribution of features added when generating evasive apps for the four different configurations. As expected, Sec-SVM requires the attacker to modify more features, but here we are no longer interested in the feature-space properties, since we are performing a problem-space attack. This demonstrates that measuring attacker effort with \( l_p \) perturbations, as in the original Sec-SVM evaluation [23], overestimates the robustness of the defense and is better assessed using our framework (§II).

While the plausibility problem-space constraint is satisfied by design by transplanting only realistic existing code, it is informative to analyze how the statistics of the evasive malware relate to the corresponding distributions in benign apps. **Figure 4** reports the cumulative distribution of app statistics across the four settings: the X-axis reports the statistics values, whereas the Y-axis reports the cumulative percentage of evasive malware apps. We also shade two gray areas: a dark gray area between the first quartile \( q_1 \) and third quartile \( q_3 \) of the statistics for the benign applications; the light gray area refers to the 3σ rule and reports the area within the 0.15% and 99.85% of the benign apps distribution.

**Figure 4** shows that while evading Sec-SVM tends to cause a shift towards the higher percentiles of each statistic, the vast majority of apps fall within the gray regions in all configurations. We note that this is just a qualitative analysis to verify that the statistics of the evasive apps roughly align with those of benign apps; it is not sufficient to have an anomaly in one of these statistics to determine that an app is malicious (otherwise, very trivial rules could be used for malware detection itself, and this is not the case). We also observe that there is little difference between the statistics generated by Sec-SVM and by traditional SVM; this means that greater feature-space perturbations do not necessarily correspond to greater perturbations in the problem-space, reinforcing the feasibility and practicality of evading Sec-SVM.

**Runtime Overhead:** The time to perform the search strategy occurring in the feature space is almost negligible; the most demanding operation is in the actual code modification. **Figure 5** depicts the distribution of injection times for our test set malware, which is the most expensive operation in our approach, while the rest is mostly pipeline overhead. The time spent per app is low: in most cases, less than 100 seconds, and always less than 2,000 seconds (approximately 33 minutes). The low runtime cost suggests that it is feasible to perform this attack at scale and reinforces the need for new defenses in this domain.

### Discussion on Attack and Results

We provide some deeper discussion on the results of our novel problem-space attack.

**Android Attack Effectiveness:** We conclude that it is practically feasible to evade the state-of-the-art Android malware classifier DREBIN [8] and its hardened variant, Sec-SVM [23], and that we are able to automatically generate realistic and inconspicuous evasive adversarial applications, often in less than 2 minutes. This shows for the first time that it is possible to create realistic adversarial applications at scale.

**Obfuscation:** It could be argued that traditional obfuscation methods can be used to simply hide malicious functionality. The novel problem-space attack in this work evaluates the feasibility of an "adversarial-malware as a service" scenario, where the use of mass obfuscation may raise the suspicions of the defender; for example, antivirus companies often classify samples as malicious simply because they utilize obfuscation or packing [67, 69]. Moreover, some other analysis methods combine static and dynamic analysis to prioritize evaluation of code areas that are likely obfuscated [e.g., 42]. On the contrary, our transformations aim to be fully inconspicuous by adding only legitimate benign code and, to the best of our knowledge, we do not leave any relevant artifact in the process.

While the effect on problem-space constraints may differ depending on the setting, attack methodologies such as ours and traditional obfuscation techniques naturally complement each other in aiding evasion and, in the program domain, code transplantation may be seen as a tool for developing new forms of inconspicuous obfuscation [27].

**Defense Directions Against Our Attack:** A recent promising direction by Incer et al. [34] studies the use of monotonic classifiers, where adding features can only increase the decision score (i.e., an attacker cannot rely on adding more features to evade detection); however, such classifiers require non-negligible time towards manual feature selection (i.e., on features that are harder for an attacker to change), and—at least in the context of Windows malware [34]—they suffer from high false positives and an average reduction in detection rate of 13%. Moreover, we remark that we decide to add goodware parts to malware for practical reasons: the opposite transplantation would be immediate to do if a dataset with annotated malicious bytecode segments were available. As part of future work, we aim to investigate whether it would still be possible to evade monotonic classifiers by adding only a minimal number of malicious slices to a benign application.

**Defenses Against Problem-Space Attacks:** Unlike settings where feature and problem space are closely related (e.g., images and audio), limitations on feature-space \( l_p \) perturbations are often insufficient to determine the risk and feasibility of an attack in the real world. Our novel problem-space formalization (§II) paves the way to the study of practical defenses that can be effective in settings which lack an inverse feature mapping. Simulating and evaluating attacker capabilities in the problem space helps define realistic threat models with more constrained modifications in the feature space—which may lead to more robust classifier design. Our Android evasion attack (§III) demonstrates for the first time that it is feasible to evade feature-space defenses such as Sec-SVM in the problem-space—and to do so en masse.

### Related Work

**Adversarial Machine Learning:** Adversarial ML attacks have been studied for more than a decade [11]. These attacks aim to modify objects either at training time (poisoning [65]) or at test time (evasion [12]) to compromise the confidentiality, integrity, or availability of a machine learning model. Many formalizations have been proposed in the literature to describe feature-space attacks, either as optimization problems [12, 16] (see also §II-A for details) or game-theoretic frameworks [21].

**Problem-Space Attacks:** Recently, research on adversarial ML has moved towards domains in which the feature mapping is not invertible or not differentiable. Here, the adversary needs to modify the objects in the problem space (i.e., the input space) without knowing exactly how this will affect the feature space. This is known as the inverse feature-mapping problem [12, 32, 58]. Many works on problem-space attacks have been explored on different domains: text [3, 43], PDFs [22, 41, 45, 46, 74], Windows binaries [38, 59, 60], Android apps [23, 31, 75], NIDS [6, 7, 20, 28], ICS [76], and JavaScript source code [58]. However, each of these studies has been conducted empirically and followed some inferred best practices: while they share many commonalities, it has been unclear how to compare them and what are the most relevant characteristics that should be taken into account while designing such attacks. Our formalization (§II) aims to close this gap, and we show how it can be used to describe representative feature-space and problem-space attacks from the literature (§II-C).

**Adversarial Android Malware:** This paper also proposes a novel adversarial problem-space attack in the Android domain (§III); our attack overcomes limitations of existing proposals, which are evidenced through our formalization. The most related approaches to our novel attack are on attribution [58] and on adversarial malware generation [31, 60, 75].

Quiring et al. [58] do not consider malware detection but design a set of simple mutations to change the programming style of an application to match the style of a target developer (e.g., replacing for loops with while loops). This strategy is effective for attribution but is insufficient for malware detection as altering stylometric properties alone would not evade a malware classifier that captures program semantics. Moreover, it is not feasible to define a hardcoded set of transformations for all possible semantics, which may also leave artifacts in the mutated code. Conversely, our attack relies on automated software transplantation to ensure the plausibility of the generated code and avoids hardcoded code mutation artifacts.

Grosse et al. [31] perform minimal modifications that preserve semantics and only modify single lines of code in the Manifest; however, these may be easily detected and removed due to unused permissions or undeclared classes. Moreover, they limit their perturbation to 20 features, whereas our problem-space constraints represent a more realistic threat model.

Yang et al. [75] propose a method for adversarial Android malware generation. Similarly to us, they rely on automated software transplantation [10] and evaluate their adversarial attack against the DREBIN classifier [8]. However, they do not formally define which semantics are preserved by their transformation, and their approach is extremely unstable, breaking the majority of apps they mutate (e.g., they report failures after 10+ modifications on average, which means they would likely not be able to evade Sec-SVM [23], which on average requires modifications of 50+ features). Moreover, the code is unavailable, and the paper lacks details required for reevaluating the approach, including any clear descriptions of preprocessing robustness. Conversely, our attack is resilient to the insertion of a large number of features (§IV), preserves dynamic app semantics through opaque predicates (§III-C), and is resilient against static program analysis (§III-D).

Rosenberg et al. [60] propose a black-box adversarial attack against Windows malware classifiers that rely on API sequence call analysis—an evasion strategy that is also applicable to similar Android classifiers. In addition to the limited focus on API-based sequence features, their problem-space transformation leaves two major artifacts that could be detected through program analysis: the addition of no-operation instructions (no-ops) and patching of the import address table (IAT). Firstly, the inserted API calls need to be executed at runtime and so contain individual no-ops hardcoded by the authors following a practice of "security by obscurity," which is known to be ineffective [19, 37]; intuitively, they could be detected and removed by identifying the tricks used by attackers to perform no-op API calls (e.g., reading 0 bytes) or by filtering the "dead" API calls (i.e., which did not perform any real task) from the dynamic execution sequence before feeding it to the classifier. Secondly, to avoid requiring access to the source code, the new API calls are inserted and called using IAT.