)) < 0, by
transplanting harvested gadgets from the ice-box G. First we
search for the list of ice-box gadgets that should be injected
into z. Each gadget ρj in the ice-box G has feature vector
rj which includes the desired feature and side-effect features.
We consider the actual feature-space contribution of gadget
i to the malicious host z with features x by performing the
set difference of the two binary vectors, rj ∧ ¬x. We then
sort the gadgets in order of decreasing negative contribution,
which ideally leads to a faster convergence of z’s score to
a benign value. Next we ﬁlter this candidate list to include
gadgets only if they satisfy some practical feasibility criteria.
We deﬁne a check feasibility function which implements some
heuristics to limit the excessive increase of certain statistics
which would raise suspiciousness of the app. Preliminary
experiments revealed a tendency to add too many permissions
to the Android Manifest, hence, we empirically enforce that
candidate gadgets add no more than 1 new permission to the
host app. Moreover, we do not allow addition of permissions
listed as dangerous in the Android documentation [5]. The
other app statistics remain reasonably within the distribution of
benign apps (more discussion in §IV), and so we decide not to
enforce a limit on them. The remaining candidate gadgets are
iterated over and for each candidate ρj, we combine the gadget
feature vector rj with the input malware feature vector x, such
= x ∨ rj. We repeat this procedure until the updated
that x(cid:3)
x(cid:3) is classiﬁed as goodware (for low-conﬁdence attacks) or
until an attacker-deﬁned conﬁdence level is achieved (for high-
conﬁdence attacks). Finally, we inject all the candidate gadgets
at once through automated software transplantation, and check
that problem-space constraints are veriﬁed and that the app is
still classiﬁed as goodware. Algorithm 2 in Appendix F reports
the detailed steps of the attack phase.
IV. EXPERIMENTAL EVALUATION
We evaluate the effectiveness of our novel problem-space
Android attack, in terms of success rate and required time—
and also when in the presence of feature-space defenses.
A. Experimental Settings
Prototype. We create a prototype of our novel problem-
space attack (§III) using a combination of Python for the ML
functionality and Java for the program analysis operations; in
particular, to perform transplantations in the problem-space
we rely on FlowDroid [9], which is based on Soot [68]. We
release the code of our prototype to other academic researchers
(see §VII). We ran all experiments on an Ubuntu VM with 48
vCPUs, 290GB of RAM, and NVIDIA Tesla K40 GPU.
Classiﬁers. As deﬁned in the threat model (§III-A), we
consider the DREBIN classiﬁer [8], based on a binary feature
space and a linear SVM, and its recently proposed hardened
variant, Sec-SVM [23], which requires the attacker to modify
more features to perform an evasion. We use hyperparameter
C=1 for the linear SVM as in [8], and identify the optimal Sec-
SVM parameter k = 0.25 (i.e., the maximum feature weight)
in our setting by enforcing a maximum performance loss of
2% AUC. See Appendix E for implementation details.
Attack Conﬁdence. We consider two attack settings: low-
conﬁdence (L) and high-conﬁdence (H). The (L) attack merely
overcomes the decision boundary (so that h(x) < 0). The
(H) attack maximizes the distance from the hyperplane into
the goodware region; while generally this distance is uncon-
strained, here we set it to be ≤ the negative scores of 25% of
the benign apps (i.e., within their interquartile range). This
avoids making superﬂuous modiﬁcations, which may only
increase suspiciousness or the chance of transplantation errors,
while being closer in nature to past mimicry attacks [12].
Dataset. We collect apps from AndroZoo [2], a large-
scale dataset with timestamped Android apps crawled from
different stores, and with VirusTotal summary reports. We use
the same labeling criteria as Tesseract [55] (which is derived
from Miller et al. [49]): an app is considered goodware if it
has 0 VirusTotal detections, as malware if it has 4+ VirusTotal
detections, and is discarded as grayware if it has between
1 and 3 VirusTotal detections. For the dataset composition,
we follow the example of Tesseract and use an average of
10% malware [55]. The ﬁnal dataset contains (cid:2)170K recent
Android applications, dated between Jan 2017 and Dec 2018,
speciﬁcally 152,632 goodware and 17,625 malware.
Dataset Split. Tesseract [55] demonstrated that, in non-
stationary contexts such as Android malware, if time-aware
splits are not considered, then the results may be inﬂated due to
concept drift (i.e., changes in the data distribution). However,
here we aim to speciﬁcally evaluate the effectiveness of an
adversarial attack. Although it likely exists, the relationship
between adversarial and concept drift is still unknown and is
outside the scope of this work. If we were to perform a time-
aware split, it would be impossible to determine whether the
success rate of our ML-driven adversarial attack was due to an
intrinsic weakness of the classiﬁer or due to natural evolution
of malware (i.e., the introduction of new non-ML techniques
malware developers rely on to evade detection). Hence, we
perform a random split of the dataset to simulate absence of
concept drift [55]; this also represents the most challenging
scenario for an attacker, as they aim to mutate a test object
coming from the same distribution as the training dataset (on
which the classiﬁer likely has higher conﬁdence). In particular,
we consider a 66% training and 34% testing random split.2
Testing. The test set contains a total of 5,952 malware.
The statistics reported in the remainder of this section refer
only to true positive malware (5,330 for SVM and 4,108
for Sec-SVM), i.e., we create adversarial variants only if the
app is detected as malware by the classiﬁer under evaluation.
Intuitively, it is not necessary to make an adversarial example
2We consider only one split due to the overall time required to run the
experiments. Including some prototype overhead, it requires about one month
to run all conﬁgurations.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:16:38 UTC from IEEE Xplore.  Restrictions apply. 
1341
)
%
(
e
t
a
R
e
v
i
t
i
s
o
P
e
u
r
T
100
80
60
40
20
0
100
98
96
94
92
90
88
86
)
%
(
e
t
a
R
e
v
i
t
i
s
o
P
e
u
r
T
SVM
Sec-SVM
SVM
Sec-SVM
0
75
25
False Positive Rate (%)
50
(a) ROC
100
0
2
4
6
8
10
False Positive Rate (%)
(b) ROC (Zoom)
Fig. 2. Performance of SVM and Sec-SVM in absence of adversarial attacks.
s
p
p
A
e
v
i
s
a
v
E
l
u
f
s
s
e
c
c
u
S
1.00
0.75
0.50
0.25
0.00
0
SVM (L)
SVM (H)
Sec-SVM (L)
Sec-SVM (H)
200
50
150
Number of Added Features
100
Fig. 3. Cumulative distribution of features added to adversarial malware (out
of a total 10,000 features remaining after feature selection).
of a malware application that
is already misclassiﬁed as
goodware; hence, we avoid inﬂating results by removing false
negative objects from the dataset. During the transplantation
phase of our problem-space attack some errors occur due to
bugs and corner-case errors in the FlowDroid framework [9].
Since these errors are related on implementation limitations of
the FlowDroid research prototype, and not conceptual errors,
the success rates in the remainder of this section refer only to
applications that did not throw FlowDroid exceptions during
the transplantation phase (see Appendix G for details).
B. Evaluation
We analyze the performance of our Android problem-space
attack in terms of runtime cost and successful evasion rate.
An attack is successful
if an app z, originally classiﬁed
(cid:3) that is classiﬁed as
as malware, is mutated into an app z
goodware and satisﬁes the problem-space constraints.
Figure 2 reports the AUROC of SVM and Sec-SVM on the
DREBIN feature space in absence of attacks. As expected [23],
Sec-SVM sacriﬁces some detection performance in return for
greater feature-space adversarial robustness.
Attack Success Rate. We perform our attack using true
positive malware from the test set, i.e., all malware objects
correctly classiﬁed as malware. We consider four settings
depending on the defense algorithm and the attack conﬁdence:
SVM (L), SVM (H), Sec-SVM (L), and Sec-SVM (H). In
absence of FlowDroid exceptions (see Appendix G), we are
able to create an evasive variant for each malware in all four
conﬁgurations. In other words, we achieve a misclassiﬁcation
rate of 100.0% on the successfully generated apps, where
the problem-space constraints are satisﬁed by construction
(as deﬁned in §III). Figure 3 reports the cumulative distri-
bution of features added when generating evasive apps for the
four different conﬁgurations. As expected, Sec-SVM requires
the attacker to modify more features, but here we are no
longer interested in the feature-space properties, since we
are performing a problem-space attack. This demonstrates
that measuring attacker effort with lp perturbations as in the
original Sec-SVM evaluation [23] overestimates the robustness
of the defense and is better assessed using our framework (§II).
While the plausibility problem-space constraint is satisﬁed
it
by design by transplanting only realistic existing code,
is informative to analyze how the statistics of the evasive
malware relate to the corresponding distributions in benign
apps. Figure 4 reports the cumulative distribution of app
statistics across the four settings:
the X-axis reports the
statistics values, whereas the Y -axis reports the cumulative
percentage of evasive malware apps. We also shade two gray
areas: a dark gray area between the ﬁrst quartile q1 and third
quartile q3 of the statistics for the benign applications; the light
gray area refers to the 3σ rule and reports the area within the
0.15% and 99.85% of the benign apps distribution.
Figure 4 shows that while evading Sec-SVM tends to cause
a shift towards the higher percentiles of each statistic, the
vast majority of apps falls within the gray regions in all
conﬁgurations. We note that this is just a qualitative analysis to
verify that the statistics of the evasive apps roughly align with
those of benign apps; it is not sufﬁcient to have an anomaly
in one of these statistics to determine that an app is malicious
(otherwise, very trivial rules could be used for malware
detection itself, and this is not the case). We also observe
that there is little difference between the statistics generated
by Sec-SVM and by traditional SVM; this means that greater
feature-space perturbations do not necessarily correspond to
greater perturbations in the problem-space, reinforcing the
feasibility and practicality of evading Sec-SVM.
Runtime Overhead. The time to perform the search strat-
egy occurring in the feature space is almost negligible; the
most demanding operation is in the actual code modiﬁcation.
Figure 5 depicts the distribution of injection times for our
test set malware which is the most expensive operation in our
approach while the rest is mostly pipeline overhead. The time
spent per app is low: in most cases, less than 100 seconds, and
always less than 2,000 seconds ((cid:2)33 mins). The low runtime
cost suggests that it is feasible to perform this attack at scale
and reinforces the need for new defenses in this domain.
V. DISCUSSION ON ATTACK AND RESULTS
We provide some deeper discussion on the results of our
novel problem-space attack.
Android Attack Effectiveness. We conclude that it is prac-
tically feasible to evade the state-of-the-art Android malware
classiﬁer DREBIN [8] and its hardened variant, Sec-SVM [23],
and that we are able to automatically generate realistic and
inconspicuous evasive adversarial applications, often in less
than 2 minutes. This shows for the ﬁrst time that it is possible
to create realistic adversarial applications at scale.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:16:38 UTC from IEEE Xplore.  Restrictions apply. 
1342
1.00
0.75
0.75
0.50
0.50
0.25
0.25
0.00
0.00
s
p
p
A
e
v
i
s
a
v
E
l
u
f
s
s
e
c
c
u
S
0
0
25
25
SVM (L)
SVM (L)
SVM (H)
SVM (H)
Sec-SVM (L)
Sec-SVM (L)
Sec-SVM (H)
Sec-SVM (H)
50
50
75
75
100
100
125
125
Application Size [MB]
Application Size [MB]
1.00
0.75
0.75
0.50
0.50
0.25
0.25
0.00
0.00
s
p
p
A
e
v
i
s
a
v
E
l
u
f
s
s
e
c
c
u
S
0
0
SVM (L)
SVM (L)
SVM (H)
SVM (H)
Sec-SVM (L)
Sec-SVM (L)
Sec-SVM (H)
Sec-SVM (H)
200
200
150
150
50
50
100
100
Avg. Cyclomatic Complexity
Avg. Cyclomatic Complexity
s
p
p
A
e
v
i
s
a
v
E
l
u
f
s
s
e
c
c
u
S
1.00
0.75
0.50
0.25
0.00
s
p
p
A
e
v
i
s
a
v
E
l
u
f
s
s
e
c
c
u
S
1.00
0.75
0.50
0.25
0.00
0
100
200
Number of Permissions
300
0
20
40
60
80
Number of API Calls
(a) Size
(b) Avg. CC
(c) Permissions
(d) API calls
s
p
p
A
e
v
i
s
a
v
E
l
u
f
s
s
e
c
c
u
S
1.00
0.75
0.50
0.25
0.00
0
200
400
600
800
Number of URLs
(e) URLs
s
p
p
A
e
v
i
s
a
v
E
l
u
f
s
s
e
c
c
u
S
1.00
0.75
0.50
0.25
0.00
s
p
p
A
e
v
i
s
a
v
E
l
u
f
s
s
e
c
c
u
S
1.00
0.75
0.50
0.25
0.00
s
p
p
A
e
v
i
s
a
v
E
l
u
f
s
s
e
c
c
u
S
1.00
0.75
0.50
0.25
0.00
s
p
p
A
e
v
i
s
a
v
E
l
u
f
s
s
e
c
c
u
S
1.00
0.75
0.50
0.25
0.00
0
100
200
300
Number of Activities
400
0
50
100
150
200
Number of Services and Receivers
0
20
40
60
80
Number of Intents
0
20
40
60
Number of Content Providers
(f) Activities
(g) Services and Receivers
(h) Intents
(i) Content Providers
Fig. 4. Statistics of the evasive malware variants, compared with statistics of benign apps. The dark gray background highlights the area between ﬁrst and
third quartile of benign applications; the light gray background is based on the 3σ rule and highlights values benign statistics between 0.15% and 99.85%
of the distribution (i.e., spanning 99.7% of the distribution).
]
s
d
n
o
c
e
S
[
i
e
m
T
n
o
i
t
c
e
j
n
I
103
102
101
SVM (L)
SVM (H)
Sec-SVM (L)
Sec-SVM (H)
Fig. 5. Violin plots of injection times per adversarial app.
Obfuscation. It could be argued that traditional obfuscation
methods can be used to simply hide malicious functionality.
The novel problem-space attack in this work evaluates the
feasibility of an “adversarial-malware as a service” scenario,
where the use of mass obfuscation may raise the suspicions of
the defender; for example, antivirus companies often classify
samples as malicious simply because they utilize obfuscation
or packing [67, 69]. Moreover, some other analysis methods
combine static and dynamic analysis to prioritize evaluation
of code areas that are likely obfuscated [e.g., 42]. On the
contrary, our transformations aim to be fully inconspicuous
by adding only legitimate benign code and, to the best of our
knowledge, we do not leave any relevant artifact in the process.
While the effect on problem-space constraints may differ
depending on the setting, attack methodologies such as ours
and traditional obfuscation techniques naturally complement
each other in aiding evasion and, in the program domain, code
transplantation may be seen as a tool for developing new forms
of inconspicuous obfuscation [27].
Defense Directions Against Our Attack. A recent promis-
ing direction by Incer et al. [34] studies the use of mono-
tonic classiﬁers, where adding features can only increase the
decision score (i.e., an attacker cannot rely on adding more
features to evade detection); however, such classiﬁers require
non-negligible time towards manual feature selection (i.e., on
features that are harder for an attacker to change), and—at
least in the context of Windows malware [34]—they suffer
from high false positives and an average reduction in detection
rate of 13%. Moreover, we remark that we decide to add
goodware parts to malware for practical reasons: the opposite
transplantation would be immediate to do if a dataset with
annotated malicious bytecode segments were available. As
part of future work we aim to investigate whether it would
still be possible to evade monotonic classiﬁers by adding only
a minimal number of malicious slices to a benign application.
Defenses Against Problem-Space Attacks. Unlike settings
where feature and problem space are closely related (e.g., im-
ages and audio), limitations on feature-space lp perturbations
are often insufﬁcient to determine the risk and feasibility of
an attack in the real world. Our novel problem-space formal-
ization (§II) paves the way to the study of practical defenses
that can be effective in settings which lack an inverse feature
mapping. Simulating and evaluating attacker capabilities in the
problem space helps deﬁne realistic threat models with more
constrained modiﬁcations in the feature space—which may
lead to more robust classiﬁer design. Our Android evasion
attack (§III) demonstrates for the ﬁrst time that it is feasible to
evade feature-space defenses such as Sec-SVM in the problem-
space—and to do so en masse.
VI. RELATED WORK
Adversarial Machine Learning. Adversarial ML attacks
have been studied for more than a decade [11]. These attacks
aim to modify objects either at training time (poisoning [65])
or at test time (evasion [12]) to compromise the conﬁdentiality,
integrity, or availability of a machine learning model. Many
formalizations have been proposed in the literature to describe
feature-space attacks, either as optimization problems [12, 16]
(see also §II-A for details) or game theoretic frameworks [21].
Problem-Space Attacks. Recently, research on adversar-
ial ML has moved towards domains in which the feature
mapping is not
the
adversary needs to modify the objects in the problem space
(i.e.,
input space) without knowing exactly how this will
invertible or not differentiable. Here,
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:16:38 UTC from IEEE Xplore.  Restrictions apply. 
1343
affect the feature space. This is known as the inverse feature-
mapping problem [12, 32, 58]. Many works on problem-space
attacks have been explored on different domains: text [3, 43],
PDFs [22, 41, 45, 46, 74], Windows binaries [38, 59, 60],
Android apps [23, 31, 75], NIDS [6, 7, 20, 28], ICS [76],
and Javascript source code [58]. However, each of these
studies has been conducted empirically and followed some
inferred best practices: while they share many commonalities,
it has been unclear how to compare them and what are the
most relevant characteristics that should be taken into account
while designing such attacks. Our formalization (§II) aims to
close this gap, and we show how it can be used to describe
representative feature-space and problem-space attacks from
the literature (§II-C).
Adversarial Android Malware. This paper also proposes a
novel adversarial problem-space attack in the Android domain
(§III); our attack overcomes limitations of existing proposals,
which are evidenced through our formalization. The most
related approaches to our novel attack are on attribution [58],
and on adversarial malware generation [31, 60, 75]. Quiring
et al. [58] do not consider malware detection, but design a
set of simple mutations to change the programming style of
an application to match the style of a target developer (e.g.,
replacing for loops with while loops). This strategy is effective
for attribution, but is insufﬁcient for malware detection as
altering stylometric properties alone would not evade a mal-
ware classiﬁer which captures program semantics. Moreover,
it is not feasible to deﬁne a hardcoded set of transformations
for all possible semantics—which may also leave artifacts in
the mutated code. Conversely, our attack relies on automated
software transplantation to ensure plausibility of the generated
code and avoids hardcoded code mutation artifacts.
Grosse et al. [31] perform minimal modiﬁcations that pre-
serve semantics, and only modify single lines of code in the
Manifest; but these may be easily detected and removed due
to unused permissions or undeclared classes. Moreover, they
limit their perturbation to 20 features, whereas our problem-
space constraints represent a more realistic threat model.
Yang et al. [75] propose a method for adversarial Android
malware generation. Similarly to us, they rely on automated
software transplantation [10] and evaluate their adversarial
attack against the DREBIN classiﬁer [8]. However, they do
not formally deﬁne which semantics are preserved by their
transformation, and their approach is extremely unstable,
breaking the majority of apps they mutate (e.g., they report
failures after 10+ modiﬁcations on average—which means
they would likely not be able to evade Sec-SVM [23] which
on average requires modiﬁcations of 50+ features). Moreover,
the code is unavailable, and the paper lacks details required
for reevaluating the approach, including any clear descriptions
of preprocessing robustness. Conversely, our attack is resilient
to the insertion of a large number of features (§IV), preserves
dynamic app semantics through opaque predicates (§III-C),
and is resilient against static program analysis (§III-D).
Rosenberg et al. [60] propose a black-box adversarial attack
against Windows malware classiﬁers that rely on API sequence
call analysis—an evasion strategy that is also applicable to
similar Android classiﬁers. In addition to the limited focus on
API-based sequence features, their problem-space transforma-
tion leaves two major artifacts which could be detected through
program analysis: the addition of no-operation instructions
(no-ops), and patching of the import address table (IAT).
Firstly, the inserted API calls need to be executed at runtime
and so contain individual no-ops hardcoded by the authors fol-
lowing a practice of “security by obscurity”, which is known
to be ineffective [19, 37]; intuitively, they could be detected
and removed by identifying the tricks used by attackers to
perform no-op API calls (e.g., reading 0 bytes), or by ﬁltering
the “dead” API calls (i.e., which did not perform any real task)
from the dynamic execution sequence before feeding it to the
classiﬁer. Secondly, to avoid requiring access to the source
code, the new API calls are inserted and called using IAT