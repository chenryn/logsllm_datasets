exchanged between the clients and the data-centers is likely
to be already very relevant in the core network.
4.2.2
Storage and Control RTT
A deeper analysis of the RTT at our four vantage points
reveals more details of the physical implementation of the
Dropbox architecture. Sec. 4.4 will show that the RTT has
a major impact on the service performance. Fig. 6 shows,
separately for storage (left) and control ﬂows (right), the
CDF of the minimum RTT in ﬂows where at least 10 RTT
samples could be obtained (see [14]). The ﬁgure accounts
only for the RTT between our probes and the servers, to
ﬁlter out the impact of the access technologies (e.g., ADSL).
The RTTs to storage servers at Amazon remained sta-
ble during our measurements, meaning that no signiﬁcant
changes in the network topology happened. The diﬀerences
in RTTs among the vantage points are related to the coun-
tries where the probes are located. This constant RTT dur-
ing our 42 days of measurements is another strong indication
that a single data-center was used by all users in our vantage
points. The RTTs to the control servers are less constant.
In both Campus 1 and Home 2 , the curve presents small
8http://www.dropbox.com/news
 1
 0.8
F
D
C
 0.6
 0.4
 0.2
 0
Store
Retrieve
 1
 0.8
 0.6
 0.4
 0.2
 0
Campus 1
Campus 2
Home 1
Home 2
1k 10k 100k 1M 10M100M 1G
1k 10k 100k 1M 10M100M 1G
Flow size (bytes)
Flow size (bytes)
Figure 7: Distribution of TCP ﬂow sizes of ﬁle stor-
age for the Dropbox client.
steps (less than 10ms). We assume that they are caused by
changes in the IP route, since the same behavior is not no-
ticeable in all probes. Also in this case, the measurements
hint to a central control server farm. Finally, it is interest-
ing to note the high diﬀerence in the RTT between control
and storage data-centers. This is probably caused by the
physical distance between them inside the U.S.
4.3 Retrieve and Store Flows
4.3.1 Flow Size
As shown in Fig. 4, most traﬃc is generated by storage
operations. Fig. 7 depicts the CDF of the ﬂow size for stor-
age operations. Since SSL is used, we observe a minimum
ﬂow size of approximately 4kB. The ﬂows have a maximum
size of approximately 400MB because of current run-time
parameters of Dropbox: batches are limited to a maximum
of 100 chunks, each smaller than 4MB, as described in Sec. 2.
From Fig. 7 it emerges that a signiﬁcant percentage of
ﬂows (up to 40% in some cases) exchange less than 10kB,
meaning that they are composed mostly by SSL handshake
messages and a small amount of user data. A very high
percentage of ﬂows (varying from 40% to 80%) consist of
less than 100kB. We conjecture that two factors are causing
this behavior: (i) the synchronization protocol sending and
receiving ﬁle deltas as soon as they are detected; (ii) the
primary use of Dropbox for synchronization of small ﬁles
constantly changed, instead of periodic (large) backups.
Comparing the CDFs for the retrieve and storage opera-
tions, we can see that retrieve ﬂows are normally larger than
the store ones. This is particularly visible in Campus 1 ,
Campus 2 and Home 1 datasets. For instance, while 60%
of store ﬂows in Home 1 have no more than 100kB, the per-
centage is about 40% for retrieve ﬂows in the same network.
This can be partially explained by the ﬁrst batch synchro-
nization happening when sessions are started. Besides that,
we observed a high number of devices using Dropbox only
for downloading content. This usage will be analyzed further
in the coming sections.
We also highlight a remarkably discrepancy in the CDF
for store ﬂows in Home 2. A single device was submitting
single chunks in consecutive TCP connections during several
days in our capture. This caused the CDF to be strongly
biased toward the maximum chunk size used by Dropbox
(4MB). We could not determine whether this behavior is
due to problems in the Dropbox client manifested in this
single device, or another legitimate use of the service.
486F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
Store
Retrieve
 1
 0.8
 0.6
 0.4
 0.2
 0
Campus 1
Campus 2
Home 1
Home 2
 1
 10
 100
 1
 10
 100
Number of Chunks
Number of Chunks
Figure 8: Distribution of the estimated number of
ﬁle chunks per TCP ﬂow.
4.3.2 Chunks per Batch
Fig. 8 depicts the CDF of the estimated number of chunks
per ﬂow. The curves show that most batches are composed
by a small number of chunks. Storage ﬂows have no more
than 10 chunks in more than 80% of the cases in all datasets.
Home 2 distribution diverges because of the single client be-
having abnormally described in the previous section. These
distributions reinforce our conjecture about the dominance
of deltas and small ﬁles in Dropbox usage habits: most ﬂows
are very small and composed by few chunks. Most of the re-
maining ﬂows have the maximum allowed number of chunks
per batch and, therefore, are strongly shaped by the protocol
design of Dropbox.
4.4 Storage Throughput
Our measurements in Sec. 4.2 indicate that Dropbox relies
on centralized data-centers for control and storage. This
raises the question on the service performance for users not
located near those data-centers.
The throughput of the storage operations is certainly one
of the key performance metrics. Fig. 9 depicts the through-
put achieved by each storage ﬂow in Campus 2 . The ﬁgure
shows separate plots for the retrieve and store operations.
Similar plots would be obtained using Campus 1 dataset.
Home 1 and Home 2 are left out of this analysis since the
access technology (ADSL, in particular) might be a bottle-
neck for the system in those networks. The x-axis represents
the number of bytes transferred in the ﬂow, already subtract-
ing the typical SSL overheads (see Appendix A for details),
while the y-axis shows the throughput calculated as the ratio
between transferred bytes and duration of each ﬂow (note
the logarithmic scales). The duration was accounted as the
time between the ﬁrst TCP SYN packet and the last packet
with payload in the ﬂow, ignoring connection termination
delays. Flows are represented by diﬀerent marks according
to their number of chunks.
Overall, the throughput is remarkably low. The aver-
age throughput (marked with dashed horizontal lines in
the ﬁgure) is not higher than 462kbits/s for store ﬂows
and 797kbits/s for retrieve ﬂows in Campus 2 (359kbits/s
and 783kbits/s in Campus 1 , respectively). In general, the
highest observed throughput (close to 10Mbits/s in both
datasets) is only achieved by ﬂows carrying more than 1MB.
Moreover, ﬂows achieve lower throughput as the number of
chunks increases. This can be seen by the concentration of
ﬂows with high number of chunks in the bottom part of the
plots for any given size.
)
s
/
s
t
i
b
(
t
u
p
h
g
u
o
r
h
T
)
s
/
s
t
i
b
(
t
u
p
h
g
u
o
r
h
T
10M
1M
100k
10k
1k
100
10M
1M
100k
10k
1k
100
θ
Chunks
       1
2 -     5
6 -   50
51 - 100
256
1k
4k
16k 64k 256k 1M 4M 16M 64M 400M
Upload (bytes)
(a) Store
θ
Chunks
       1
2 -     5
6 -   50
51 - 100
256
1k
4k
16k 64k 256k 1M 4M 16M 64M 400M
Download (bytes)
(b) Retrieve
Figure 9: Throughput of storage ﬂows in Campus 2 .
TCP start-up times and application-layer sequential ac-
knowledgments are two major factors limiting the through-
put, aﬀecting ﬂows with a small amount of data and ﬂows
with a large number of chunks, respectively. In both cases,
the high RTT between clients and data-centers ampliﬁes the
eﬀects. In the following, those problems are detailed.
4.4.1 TCP Start-up Effects
Flows carrying a small amount of data are limited by TCP
slow start-up times. This is particularly relevant in the
analyzed campus networks, since both data link capacity
and RTT to storage data-centers are high in these networks.
Fig. 9 shows the maximum throughput θ for completing the
transfer of a speciﬁc amount of data, assuming that ﬂows
stayed in the TCP slow start phase. We computed the la-
tency as in [4], with initial congestion window of 3 segments
and adjusting the formula to include overheads (e.g., the 3
RTTs of SSL handshakes in the current Dropbox setup).
Fig. 9 shows that θ approximates the maximum through-
put satisfactorily. This is clear for ﬂows with a single chunk,
which suﬀer less from application-layer impediments. The
bound is not tight for retrieve ﬂows, because their latency in-
cludes at least 1 server reaction time. Note that the through-
487Store
Chunks
       1
2 -     5
6 -   50
51 - 100
1k
4k
16k
64k
256k
1M 4M 16M 64M 400M
Upload (bytes)
Retrieve
Table 4: Performance in Campus 1 before and after
the deployment of a bundling mechanism.
Jun/Jul
Mar/Apr
Median Average Median Average
Flow size
Store
Retrieve
16.28kB 3.91MB 42.36kB 4.35MB
42.20kB 8.57MB 70.69kB 9.36MB
Throughput (kbits/s)
Store
Retrieve
31.59
57.72
358.17
782.99
81.82
109.92
552.92
1293.72
Chunks
       1
2 -     5
6 -   50
51 - 100
4.5 Implications and Recommendations
Our measurements clearly indicate that the application-
layer protocol in combination with large RTT penalizes the
system performance. We identify three possible solutions to
remove the identiﬁed bottlenecks:
n
o
i
t
a
r
u
D
n
o
i
t
a
r
u
D
10m
2m
30s
5s
1s
0.1s
10m
2m
30s
5s
1s
0.1s
1k
4k
16k
64k
256k
1M 4M 16M 64M 400M
Download (bytes)
Figure 10: Minimum duration of ﬂows with diverse
number of chunks in Campus 2 .
put can still be limited by other factors, such as the explicit
user selection of transfer limits, or possible network conges-
tion. However, when considering only ﬂows with a single
chunk, more than 99% of the store ﬂows and around 95%
of the retrieve ﬂows in Campus 1 have no TCP retransmis-
sions. These percentages are lower in Campus 2 (88% and
75%) because of the wireless access points. Single-chunk
ﬂows with no retransmissions experience throughput close to
the limit (e.g., -17% on average for store ﬂows in Campus 1 ),
conﬁrming that the TCP start-up is their main bottleneck.
4.4.2
Sequential Acknowledgments
Flows with more than 1 chunk have the sequential ac-
knowledgment scheme (Fig. 1) as a bottleneck, because the
mechanism forces clients to wait one RTT (plus the server
reaction time) between two storage operations. Naturally,
ﬂows carrying a large number of small chunks suﬀer rel-
atively more from this impediment than ﬂows with large
chunks. Fig. 8 shows that more than 40% of the ﬂows have
at least 2 chunks and are potentially aﬀected by that.
Flows with several chunks are also aﬀected by the pre-
viously described factors. Besides that, the Dropbox client
keeps storage connections opened for a short interval after
transferring a chunk, waiting for new chunks. Therefore,
some ﬂows in Fig. 9 might have a longer duration because
they were reused during this idle interval. To remove these
eﬀects and highlight the impact of sequential acknowledg-
ments, we divide the x-axis of Fig. 9 in slots of equal sizes
(in logarithmic scale) and, for each slot, select the ﬂow with
maximum throughput in each of the four groups shown in
the ﬁgure. Fig. 10 depicts the duration of these representa-
tive ﬂows. Flows with more than 50 chunks, for instance, al-
ways last for more than 30s, regardless of their sizes. Consid-
ering the RTT in Campus 2 , up to one third of that (5-10s)
is wasted while application-layer acknowledgments are tran-
siting the network. The remaining duration is mainly due
to the server and the client reaction times between chunks.
1. Bundling smaller chunks,
increasing the amount of
data sent per storage operation. Dropbox 1.4.0, an-
nounced in April 2012, implements a bundling mecha-
nism, which is analyzed in the following;
2. Using a delayed acknowledgment scheme in storage op-
erations, pipelining chunks to remove the eﬀects of se-
quential acknowledgments;
3. Bringing storage servers closer to customers, thus im-
proving the overall throughput.
Note that the ﬁrst two countermeasures target only the
application-layer bottleneck. The third one, while valid for
any on-line service, would have the extra positive impact of
removing heavy storage traﬃc from the core of the Internet.
4.5.1
Improvements in Dropbox 1.4.0
The latest version of Dropbox adds two commands
(store batch and retrieve batch), allowing several chunks to
be submitted in a single operation.9 Single-chunk commands
are still in use:
the system decides at run-time whether
chunks will be grouped or not. We use extra data captured
in Campus 1 during June and July 2012 to quantify the
eﬀects of this mechanism on the system performance.
Tab. 4 compares the ﬂow size and the throughput distri-
butions before and after the deployment of Dropbox 1.4.0.
The increase in the median ﬂow size shows that ﬂows become
bigger, likely because more small chunks can be accommo-
dated in a single TCP connection in this version. The aver-
ages are less aﬀected, since only smaller ﬂows proﬁt from the
mechanism. Both the median and the average throughput,
on the other hand, are dramatically improved. The average
throughput of retrieve ﬂows, for instance, is around 65%
higher in the newest dataset.
Since both the new batch commands and the old single-
chunk commands are still executed sequentially, there seems
to exist room for improvements in the protocol. A complete
characterization of that is, however, out of scope of this pa-
per. Such an analysis will be performed in future work, along
with an study of the eﬀectiveness and possible drawbacks of
each of our recommendations.
9Number of chunks and TCP segments with the PSH ﬂag set
do not have a direct relation in this version anymore.
488Devices
1
2-3
> 3
1k
10k
100k
1M 10M 100M 1G 10G 100G
Retrieve (bytes)
(a) Home 1
Devices
1
2-3
> 3
)
s
e
t
y
b
(
e
r
o
t
S
)
s
e
t
y
b
(
e
r
o
t
S
100G
10G
1G