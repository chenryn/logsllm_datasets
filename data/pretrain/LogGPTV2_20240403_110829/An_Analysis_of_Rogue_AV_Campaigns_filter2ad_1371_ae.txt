### Solutions to Mitigate the Torrent Effect

#### What Is the Impact of P2P Traffic on Anomaly Detection?

**Table 3. Mitigating P2P Effects Using P2P Traffic Classifiers Based Traffic Filtering**
(DR = Detection Rate; FP = False Positive; KPC = Karagiannis’ Payload Classifier)

| Rate Limiting | TRW-CB | MaxEnt | NETAD |
|---------------|---------|--------|--------|
| DR% | FP% | DR% | FP% | DR% | FP% | DR% | FP% |
| 50 | No filtering | OpenDPI[23] | 56 | 60 | KPC[24] | 45 | 43 |
| 40 | 60 | 64 | 70 | 22 | 12 | 6 | 62 |
| 63 | 66 | 48 | 32 | 17 | 65 | 70 | 77 |
| 25 | 17 | 13 | - | - | - | - | - |

**Table 4. Evaluation of OpenDPI and KPC on Encrypted P2P Traffic**

| Classified as p2p | Classified as unknown | Classified as non-p2p |
|------------------|----------------------|-----------------------|
| OpenDPI          | 3.8%                 | 96.2%                 | 0%   |
| KPC              | 64.7%                | 35.2%                 | 0%   |

### Can Existing Public P2P Traffic Classifiers Mitigate the Torrent Effect?

The problem of classifying P2P traffic has been extensively studied, with both signature- and heuristic-based solutions available. However, many existing heuristic-based solutions are limited by overlapping features. Therefore, it is crucial to choose approaches that use non-overlapping heuristics. We evaluate our proposed design using a popular DPI-based technology and a hybrid scheme (signatures + heuristics).

We perform traffic filtering using OpenDPI [23], a signature-based solution with over 90 signatures, and Karagiannis’ Payload Classifier (KPC) [24], a hybrid solution with over 59 signatures. The results of evaluating four anomaly detectors on filtered traffic are shown in Table 3. Table 3 indicates that KPC (unknown: 35.2%) provides significantly better accuracy than OpenDPI (unknown: 96.2%), primarily because OpenDPI cannot detect any encrypted P2P traffic. Comparing Table 3 and Table 4, it is evident that the improvements in anomaly detector accuracies depend on the accuracy of the traffic classifier. One limiting factor in the accuracy of traffic classifiers is the presence of encrypted traffic.

From Table 3, we observe that the current traffic classification accuracies of DPI solutions are insufficient to induce a significant improvement in anomaly detection. Detection rates after P2P traffic classification range from 40-70%, while false positives range from 6-40% for different anomaly detectors. Given these impractical accuracies for commercial deployments, we conclude that current public P2P traffic classification solutions cannot provide acceptable accuracies to effectively improve anomaly detection. While many commercial P2P traffic classification solutions are available, none of the research community's proposed P2P traffic classifiers have acceptable detection accuracies for encrypted P2P traffic. Thus, efficient P2P traffic classification remains an open problem, and solving this will benefit both the IDS and traffic engineering communities.

Until such a solution is developed, we need to identify non-overlapping (between malicious and P2P) traffic features that anomaly detectors can rely on. As a preliminary result, Figure 6 shows the connection timeline for P2P and malicious traffic. It is clear that the sustained activity of malicious traffic differs significantly from the sporadic P2P traffic activity. Therefore, P2P and malicious traffic can be isolated if long-term statistics are introduced during anomaly detection. This is part of our ongoing research.

### What Are the Open Problems in Designing Future Anomaly Detectors?

The rapid growth of P2P-based file sharing, VOIP, and video streaming has transformed Internet traffic characteristics. Our evaluations show that existing anomaly detectors, which rely on traffic features (e.g., rate, connection failures, ports) that overlap with P2P traffic behavior, are inadequate. While we propose an ad-hoc solution to make existing IDSs effective, the scalability of this solution to future Internet traffic remains an open question. Recent projections suggest that some of the greatest threats in the future will originate from file-sharing networks [28]. In such a threat landscape, a P2P traffic classification-based solution will allow malicious activities embedded within P2P traffic to go undetected.

While detecting malware delivered via P2P applications is not within the scope of traffic anomaly detection, attacks originating from P2P networks should be detected by IDSs. For example, Naoumov and Ross designed a DDoS engine for flooding a target using the indexing and routing layers in a P2P system [27]. Similarly, IDSs should detect exploits targeting vulnerabilities resulting from changes in firewall rules for P2P traffic [29]. Finally, it is highly desirable to detect C&C channels of bots that use P2P communication [30].

Given that P2P traffic is here to stay, our work highlights the need to rethink the classical anomaly detection design philosophy, focusing on performing anomaly detection in the presence of P2P traffic. We argue that P2P traffic classification will play a fundamental role in future IDSs, facilitating the detection of both P2P and non-P2P traffic anomalies, as shown in Figure 5. In our proposed design, traditional non-P2P network attacks will be detected using existing anomaly detectors, while an additional IDS specialized in detecting attacks within P2P traffic will also be deployed.

Designing a P2P-specialized IDS is still an open research problem, part of our ongoing research. We have made our dataset publicly available for performance benchmarking of such future IDSs and P2P traffic classifiers.

**Acknowledgments:**
We thank Dr. Hyun-chul Kim for providing Karagiannis’ Payload Classifier.

**References:**
(References listed as in the original text)

---

### A Centralized Monitoring Infrastructure for Improving DNS Security

**Abstract:**
Researchers have recently noted the potential of fast poisoning attacks against DNS servers, allowing attackers to easily manipulate records in open recursive DNS resolvers. A vendor-wide upgrade mitigated but did not eliminate this attack. Existing DNS protection systems, including bailiwick-checking and IDS-style filtration, do not stop this type of DNS poisoning. We propose Anax, a DNS protection system that detects poisoned records in cache.

Our system can observe changes in cached DNS records and applies machine learning to classify these updates as malicious or benign. We describe our classification features and machine learning model selection process, noting that the proposed approach can be easily integrated into existing local network protection systems. To evaluate Anax, we studied cache changes in a geographically diverse set of 300,000 open recursive DNS servers (ORDNSs) over an eight-month period. Using hand-verified data as ground truth, evaluation of Anax showed a very low false positive rate (0.6% of all new resource records) and a high detection rate (91.9%).

**Keywords:**
DNS Poisoning, Attack Detection, Local Network Protection.

**1. Introduction:**
The Domain Name System (DNS) maps domain names to IP addresses and other essential records for email, web, and numerous network protocols. DNS security issues affect many services and critical resources. Recently, the security community has identified fast poisoning techniques that allow trivial corruption of DNS records. A poisoning attack allows an adversary to manipulate resolution caches, usually through off-path guessing of transaction components used for DNS message integrity.

Several secure DNS protocols have been proposed, including DNSSEC and DNSCurve. DNSCurve provides link-level security, while DNSSEC provides object-based security of DNS messages using cryptographic means. However, DNSSEC deployment has been slow, and many hosts have on-path hardware that interferes with DNSSEC’s larger packet sizes.

The delay in deploying secure DNS motivates the need for local networks to protect their recursive DNS resolution infrastructure. Traditional solutions like IDS and packet-inspection tools provide limited protection against some classes of attacks but do not detect DNS poisonings. Poisoning attacks generally use valid, "RFC-compliant" DNS messages that contain misleading answers.

For these reasons, we focus on in-cache detection of DNS poisoning:
1. In-line inspection of DNS traffic can introduce latency, which can have detrimental impacts on other services.
2. Several tools already detect classes of DNS attacks, such as packet format violations, which are orthogonal to DNS poisoning.
3. Some DNS attacks, such as out-of-bailiwick record injection, are already rejected by DNS resolvers. The DNS poisoning attacks we consider are in the newer family of fast poisoning or "Kaminsky-class" attacks, which evade these forms of basic trustworthiness checks.

To detect DNS poisoning that has evaded all other layers of filtration, we need access to large, busy recursive servers. We use data obtained from the inspection of open recursive caches run by third parties on the Internet. We selected 300,000 open recursive servers to obtain a diversity of DNS resolvers based on geography, network size, and organizational type. Using this data source, we designed and evaluated a large-scale, centralized poisoning detection system called Anax. Our implementation of Anax provides a scalable, centralized poisoning detection system.