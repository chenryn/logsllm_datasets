training dataset. In such cases, an adversary can easily gain access
to two snapshots ð‘€ð· and ð‘€ð·â€² with ð· âŠŠ ð·â€² and may be interested
in learning details about the update ð·â€² \ ð·. We show that we can
extract entire sentences from this difference by comparing ð‘€ð·
and ð‘€ð·â€², revealing not only aggregate user behavior, but specific
conversations.
Data Specialization. Some applications with little task-specific
data build on top of generic, pretrained high-capacity language
models such as GPT-2 [23]. In such settings, training starts from
the pretrained model, but then uses a significantly smaller pri-
vate dataset. As an example, an organization could simply use a
publicly available off-the-shelf language model to create an email
authoring autocompletion system. However, by additionally train-
ing the model with some historical email data, it can be adapted to
organization-specific terms, acronyms and concepts. In such a sce-
nario, if an adversary can gain access to the specialized model ð‘€â€²,
they can easily also obtain the (publicly available) model ð‘€ used
as a basis. We show that by treating these as different snapshots
of the same model, the adversary can extract parts of the private
dataset used for specialization.
User Data Deletion. Art. 17 of GDPR [29] Right to erasure (â€œright
to be forgottenâ€) gives data owners the right to request erasure of
their personal data from a party who has collected and processed
it. Language models trained on emails, text messages, or other
user-generated content may contain personal information that a
user can request to delete. The data collector would be required to
delete the userâ€™s data and retrain any models in which it had been
used. In many cases, these models may have already been released
either to the public or to other users via services provided by the
data collector (e.g., text prediction and auto-correct services in text
editors and mobile keyboards).
This scenario falls into our adversary setting, albeit in reverse
chronological order. Here the dataset ð·â€² contains the data that will
be deleted, whilst ð· does not (i.e., the difference ð·â€²\ð· represents the
userâ€™s data). With access to ð‘€ð· and ð‘€ð·â€², the attacker can attempt
to infer the userâ€™s data. Even if the retrained model overwrites the
old model, it may not be possible to erase all instances of the old
model simultaneously. For example, some users may be slow to
download the new version or the old model may have been copied
by other parties.
Naturally, this scenario can be extended to other settings where
data is deleted between model updates. This scenario raises an
interesting question on whether deletion of data is in the userâ€™s
best interest or if it makes their data more susceptible to leakage.
3 NEW METRICS
We introduce two metrics called differential rank and differential
score to analyze data exposure between two snapshots of a genera-
tive language model.
CCS â€™20, November 9â€“13, 2020, Virtual Event, USA
Zanella-BÃ©guelin, Wutschitz, Tople, RÃ¼hle, Paverd, Ohrimenko, KÃ¶pf, and Brockschmidt
3.1 Differential Score and Differential Rank
We aim to identify token sequences whose probability differs most
between models ð‘€ and ð‘€â€². Intuitively, such sequences are most
likely to be related to the differences between their corresponding
training datasets ð· and ð·â€².
To capture this notion formally, we define the differential score
(DS) of token sequences, which is simply the sum of the differences
of (contextualized) per-token probabilities. We also define a relative
variant(cid:102)DS based on the relative change in probabilities, which we
found to be more robust w.r.t. the noise introduced by different
random initializations of the models ð‘€ and ð‘€â€².
Definition 3.1. Given two language models ð‘€, ð‘€â€² and a token
sequence ð‘¡1 . . . ð‘¡ð‘› âˆˆ ð‘‡âˆ—, we define the differential score of a token
as the increase in its probability and the relative differential score
as the relative increase in its probability. We lift these concepts to
token sequences by defining
ð‘›
ð‘›
ð‘–=1
ð‘–=1
DSð‘€â€²
ð‘€ (ð‘¡1 . . . ð‘¡ð‘›) =
ð‘€â€²
ð‘€ (ð‘¡1 . . . ð‘¡ð‘›) =
(cid:102)DS
ð‘€â€²(ð‘¡ DSð‘€â€²
The lower the differential rank of a sequence, the more the se-
quence is exposed by a model update, with the most exposed se-
quence having rank 0.
3.2 Approximating Differential Rank
Computing the differential rank DR(ð‘ ) of a sequence ð‘  of length |ð‘ | =
ð‘› requires searching a space of size |ð‘‡ |ð‘›. To avoid exponential blow-
up, we rely on Algorithm 1, which approximates the differential
rank based on beam search.
At iteration ð‘–, the algorithm maintains a set ð‘† of ð‘˜ (called the
beam width) candidate sequences of length ð‘– together with their
differential scores. The algorithm iterates over all ð‘˜Â·|ð‘‡ | single-token
extensions of these sequences, computes their differential scores,
and keeps the ð‘˜ highest-scoring sequences of length ð‘– + 1 for the
next step. Eventually, the search completes and returns the set ð‘†.
Algorithm 1 returns a set of token sequences ð‘  and their differ-
ential score ð‘Ÿ. With this we can approximate the differential rank
DR(ð‘ ) by the number of token sequences in ð‘† with differential score
higher than ð‘ . For large enough beam widths this yields the true
rank of ð‘ . For smaller widths, the result is a lower bound on DR(ð‘ ),
as a search may miss sequences with higher differential score.
Proposition 3.3. If Algorithm 1 returns a set
ð‘† = {(ð‘ 1, ð‘Ÿ1), . . . , (ð‘ ð‘˜, ð‘Ÿð‘˜)} with ð‘Ÿ1 â‰¥ Â· Â· Â· â‰¥ ð‘Ÿð‘˜ ,
then DSð‘€â€²
ð‘€ (ð‘ ð‘–) = ð‘Ÿð‘– and DR(ð‘ ð‘–) â‰¥ ð‘– âˆ’ 1.
Algorithm 1 Beam search for Differential Rank
In: ð‘€, ð‘€â€²=models, ð‘‡ =tokens, ð‘˜=beam width, ð‘›=length
Out: ð‘†=set of (ð‘›-gram, DS) pairs
1: ð‘† â† {(ðœ–, 0)}
2: for ð‘– = 1 . . . ð‘› do
ð‘†â€² â† {(ð‘  â—¦ ð‘¡, ð‘Ÿ + DSð‘€â€²
3:
ð‘† â† take(k, Sâ€²)
4:
5: return ð‘† = {(ð‘ 1, ð‘Ÿ1), . . . , (ð‘ ð‘˜, ð‘Ÿð‘˜)} such that ð‘Ÿ1 â‰¥ Â· Â· Â· â‰¥ ð‘Ÿð‘˜
ð‘€ (ð‘ )(ð‘¡)) | (ð‘ , ð‘Ÿ) âˆˆ ð‘†, ð‘¡ âˆˆ ð‘‡}
âŠ² Initialize with empty sequence ðœ–
âŠ² Take top ð‘˜ items from ð‘†â€²
Optimizing for Speed. The beam width ð‘˜ governs the trade-off
between computational cost and the precision of the approxima-
tion. In experiments, we found that shrinking the beam width as
the search progresses speeds up the search considerably without
compromising on the quality of results. Typically, we use a beam
width |ð‘‡ |, which we halve at each iteration. That is, we consider
|ð‘‡ | /2 candidate phrases of length two, |ð‘‡ | /4 sequences of length
three, and so on.
Optimizing for Diversity. Since the sequences returned by vanilla
beam search typically share a common prefix, we rely on group
beam search as a technique for increasing diversity: we split the
initial |ð‘‡ | one-token sequences into multiple groups according to
their differential score, and run parallel beam searches extending
each of the groups independently. See [31] for more sophisticated
techniques for increasing diversity.
4 LEAKAGE ANALYSIS
We use our new metrics to perform leakage analyses for various
datasets across various model update scenarios. We first describe
our benchmark datasets with their model configurations and the
model training scenarios we consider. Then, we discuss research
questions relevant to the analysis scenarios described in Section 2.3.
We then show experiments investigating these questions in detail,
first using synthetically generated canaries as a proxy for updates
where we can precisely control the differences between the datasets
used to create model snapshots, and then in a realistic setting, in
which we use a set of standard real-world datasets.
4.1 Datasets and Models
We consider three datasets of different size and complexity, matched
with standard model architectures whose capacity we adapted to
the data size and implemented in TensorFlow.1
Concretely, we use the Penn Treebank [20] (PTB) dataset as
a representative of low-data scenarios, as the standard training
dataset has only around 900,000 tokens and a vocabulary size of
10,000. As the corresponding model, we use a two-layer recurrent
neural network using LSTM cells with 200-dimensional embeddings
and hidden states and no additional regularization (this corresponds
to the small configuration of Zaremba et al. [33]).
Second, we use a dataset of Reddit comments with 20 million
tokens overall, of which we split off 5% as validation set. We use a
vocabulary size of 10,000. We rely on two different model configu-
rations for this dataset, which allows us to understand the impact
of model size on information leakage using DR as a metric.
1Source code and tools available at: https://github.com/microsoft/language-privacy
Analyzing Information Leakage of Updates to Natural Language Models
CCS â€™20, November 9â€“13, 2020, Virtual Event, USA
(1) a one-layer RNN using an LSTM cell with 512-dimensional
hidden states and 160-dimensional embeddings. We employ
dropout on inputs and outputs with a keep rate of 0.9 as reg-
ularizer. These parameters were chosen in line with a neural
language model suitable for next-word recommendations on
resource-constrained mobile devices.
(2) a model based on the Transformer architecture [30] (more
concretely, using the BERT [9] codebase) with four layers of
six attention heads, each with a hidden dimension of 192.
Finally, we use the Wikitext-103 dataset [22] with 103 million
training tokens as a representative of a big data regime, using a
vocabulary size of 20,000. As the model, we employ a two-layer
RNN with 512-dimensional LSTM cells and token embedding size
512 and dropout on inputs and outputs with a keep rate of 0.9 as
regularizer. We combined this large dataset with this (relatively
low-capacity) model to test if our results still hold on datasets that
clearly require more model capacity than is available.
All models and their training are following standard best prac-
tices for generative language models and represent common (sim-
ple) baselines used in experiments on the used datasets. This can
be seen in the perplexity of the trained models on the held-out test
data, shown in Table 1, which is in line with common test results.
4.2 Implementing Model Updates
Updated models can be created using different techniques, with
different applicability to the usage and analysis scenarios discussed
in Section 2.3.
Retraining. Given an updated dataset ð·â€², a fresh model snapshot
ð‘€â€² can be obtained by simply training a fresh model from scratch,
which we refer to as retraining. This also involves a fresh (random)
initialization of the model parameters, and in practice, retraining
repeatedly on the same dataset will yield slightly different models.
Data deletion requires updating a model to eliminate the influence of
some training data points at the request of the data owner. This can
be done by retraining a model after pruning the data or, equivalently,
using techniques with lower computational cost [5, 14].
Continued Training. In this approach, a fresh model snapshot ð‘€â€²
is obtained by taking an existing model ð‘€ and continuing training
it on additional data. This is the core of the data specialization
scenario and sometimes also used in data update scenarios to avoid
the computational cost of training on a large dataset from scratch.
4.3 Research Questions
With the training techniques outlined for different model update
scenarios, we consider four research questions in our experiments.
RQ0: Can an attacker learn private information from model up-
dates? Here we address the basic question of whether private data
used to update a model can be leaked in our adversarial setting and
how. We first answer this question by using differential score to
find information about private sequences used in a model update.
We then investigate the influence of other parameters of the system
on the differential score in more detail.
user deletion scenario, for which we need to answer if it is possible
to safely remove data of a single user, or if such dataset changes
need to be hidden among other substantial changes. Concretely, we
analyze whether including a large enough additional dataset ð·extra
in an update can prevent leakage of information about the rest of
the data used. ð·extra can be any dataset which is either available
publicly or is non-sensitive from the point of view of the model
provider or users.
RQ2: How do retraining and continued training differ with respect
to information leakage? In the continued training approach, the
parameters of a previously trained model ð‘€ð· are updated based
only on new data ð·â€² \ ð·. In contrast, in the retraining strategy pa-
rameters are updated using all data in ð·â€². The most recent updates
to model parameters depend only on new data in the continuing
training case, whereas they depend on the whole training data ð·â€²
when retraining a model from scratch. We analyze the effect of this
seemingly more pronounced dependence.
RQ3: How is leakage affected by an adversaryâ€™s background knowl-
edge? Prior attacks on language models assume that the adversary
has background knowledge about the context in which a secret
appears. We analyze the effect of such knowledge for inferring
private data from model updates.
4.4 Results with Canaries
We create a number of canary phrasesâ€”grammatically correct
phrases that do not appear in the original datasetâ€”that serve as a
proxy for private data that the adversary is trying to extract. We
consider different word frequency characteristics to control the
influence on the used vocabulary. Specifically, we fix the length
of the canary phrase to 5, choose a valid phrase structure (e.g.,
Subject, Verb, Adverb, Compound Object), and instantiate each
placeholder with a token in a dataset vocabulary. We create ca-
naries in which frequencies of tokens are all low (all tokens are
from the least frequent quintile of words), mixed (one token from
each quintile), increasing from low to high, and decreasing from
high to low. For example, the mixed phrase across all the datasets
is â€œNASA used deadly carbon devicesâ€, and the all low phrase for
PTB is â€œnurses nervously trusted incompetent graduatesâ€. As the
vocabularies differ between the different datasets, the canaries are
in general dataset-dependent. We vary the amount of private data,
ð¶, by inserting a canary phrase ð‘  a number of times proportional
to the number of tokens in the training corpus:
(1) For PTB, we consider ð‘˜ âˆˆ {10, 50, 100} canary insertions
(corresponding to 1 canary token in 18K training tokens, 1 in 3.6K,
and 1 in 1.8K).
(2) For the Reddit dataset, we use ð‘˜ âˆˆ {5, 50, 500} (corresponding
(3) For the Wikitext-103 data, we use ð‘˜ âˆˆ {20, 100} (correspond-
We train the model ð‘€ on ð· and the model ð‘€â€² on ð· with ð‘˜
copies of the canary ð‘ . We then compute the differential rank of the
canaries for different values of ð‘˜.
to 1 in 1M, 1 in 100K, 1 in 10K).
ing to 1 in 1M, 1 in 200K).
RQ1: How does masking private data with additional non-sensitive
data (ð·extra) affect leakage? This is particularly important for the
RQ0: Can an attacker learn private information from model up-
dates? We use our differential score based beam search (Algorithm 1)
CCS â€™20, November 9â€“13, 2020, Virtual Event, USA
Zanella-BÃ©guelin, Wutschitz, Tople, RÃ¼hle, Paverd, Ohrimenko, KÃ¶pf, and Brockschmidt
Table 1: Differential score (DS) for different datasets, model architectures, canaries, and insertion frequencies. White cells
represent a differential rank (DR) of 0 (as approximated by beam search), and gray cells represent DR > 1000.
Dataset
Model Type (Perplexity)
Canary Token Freq.
All Low
Low to High
Mixed