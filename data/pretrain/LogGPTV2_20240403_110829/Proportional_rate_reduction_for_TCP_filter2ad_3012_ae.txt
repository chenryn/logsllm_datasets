help applications such as interactive games and telephony
that persistently send small messages over TCP.
8. CONCLUSION
Proportional rate reduction improves fast recovery under
practical network conditions. PRR operates smoothly even
when losses are heavy, is quick in recovery for short ﬂows,
and accurate even when acknowledgments are stretched, lost
or reordered. In live experiments, PRR was able to reduce
the latency of short Web transfers by 3-10% compared to
Linux recovery and proved to be a smoother recovery for
video traﬃc compared to the standard RFC 3517. While
165these seem like small improvements, we would like to em-
phasize that even delays as small as 100ms in page load
times have been shown to impact user satisfaction in several
independent experiments [25]. Based on its promise in live
experiments, PRR was accepted to be in the mainline Linux
as the default fast recovery algorithm, and is proposed as an
experimental RFC in the IETF [15].
Going forward, we will revisit the eﬀectiveness of the RTO
mechanisms in practice. Measurements show that timeouts
(and any subsequent exponential backoﬀs) in short ﬂows
constitute over 60% of the retransmissions. Most of these
occur when ﬂows are in the Open state and receive no du-
plicate acknowledgments. Our research will address if and
how timeouts can be improved in practice, especially for
short ﬂows.
9. ACKNOWLEDGMENTS
We would like to thank Ankur Jain, Bernhard Beck, Tayeb
Karim, John Reese for facilitating our experiments on live
traﬃc. The following people greatly helped to improve the
paper itself: Larry Brakmo, Neal Cardwell, Jerry Chu, Ilpo
J¨arvinen, Sivasankar Radhakrishnan, Neil Spring, Mukar-
ram Bin Tariq, and our anonymous reviewers.
10. REFERENCES
[1] State of the internet, 2010.
http://www.akamai.com/stateoftheinternet/.
[2] Allman, M., Avrachenkov, K., Ayesta, U.,
Blanton, J., and Hurtig, P. Early retransmit for
TCP and SCTP, May 2010. RFC 5827.
[3] Allman, M., Balakrishnan, H., and Floyd, S.
Enhancing TCP’s loss recovery using limited transmit,
January 2001. RFC 3042.
[4] Allman, M., Paxson, V., and Blanton, E. TCP
congestion control, September 2009. RFC 5681.
[5] Balakrishnan, H., Padmanabhan, V. N., Seshan,
S., Stemm, M., and Katz, R. H. TCP behavior of a
busy internet server: Analysis and improvements. In
Proc. of INFOCOMM (1998).
[6] Bennet, J. C., Patridge, C., and Shectman, N.
Packet reordering is not pathological network
behavior. IEEE/ACM Trans. on Networking
(December 1999).
[7] Blanton, E., and Allman, M. Using TCP DSACKs
and SCTP duplicate TSNs to detect spurious
retransmissions, February 2004. RFC 3708.
[8] Blanton, E., Allman, M., Fall, K., and Wang,
L. A conservative SACK-based loss recovery algorithm
for TCP, 2003. RFC 3517.
[9] Floyd, S., Mahdavi, J., Mathis, M., and
Podolsky, M. An extension to the SACK option for
TCP, July 2000. RFC 2883.
[13] Ludwig, R., and Katz, R. H. The eifel algorithm:
Making TCP robust against spurious retransmissions.
(ACM) Computer Communication Review 30
(January 2000).
[14] Ludwig, R., and Meyer, M. The eifel detection
algorithm for TCP, April 2003. RFC 3522.
[15] Mathis, M., Dukkipati, N., and Cheng, Y.
Proportional rate reduction for TCP, March 2011.
Work in progress,
draft-mathis-tcpm-proportional-rate-reduction-00.txt.
[16] Mathis, M., and Mahdavi, J. Forward
acknowledgment: reﬁning TCP congestion control.
SIGCOMM Comput. Commun. Rev. 26 (August
1996), 281–291.
[17] Mathis, M., and Mahdavi, J. TCP rate-halving
with bounding parameters, December 1997.
http://www.psc.edu/networking/papers/
FACKnotes/current/.
[18] Mathis, M., Mahdavi, J., Floyd, S., and
Romanow, A. TCP selective acknowledgement
options, October 1996. RFC 2018.
[19] Mathis, M., Semke, J., Mahdavi, J., and Lahey,
K. The rate-halving algorithm for TCP congestion
control, June 1999.
draft-mathis-tcp-ratehalving-00.txt,
http://www.psc.edu/networking/ftp/papers/draft-
ratehalving.txt.
[20] Petlund, A., Evensen, K., Griwodz, C., and
Halvorsen, P. TCP enhancements for interactive
thin-stream applications. In NOSSDAV (2008).
[21] Ramachandran, S. Web metrics: Size and number of
resources. http://code.google.com/speed/articles/web-
metrics.html.
[22] Rewaskar, S., Kaur, J., and Smith, F. D. A
performance study of loss detection/recovery in
real-world TCP implementations. In Proc. of ICNP
(2007).
[23] Sarolahti, P., and Kuznetsov, A. Congestion
control in linux tcp. In Proceedings of USENIX
(2002), Springer, pp. 49–62.
[24] Scheffenegger, R. Improving SACK-based loss
recovery for TCP, November 2010. Work in progress,
draft-scheﬀenegger-tcpm-sack-loss-recovery-00.txt.
[25] Schurman, E., and Brutlag, J. The user and
business impact of server delays, additional bytes, and
HTTP chunking in web search.
http://velocityconf.com/velocity2009/
public/schedule/detail/8523.
[26] Sun, P., Yu, M., Freedman, M. J., and Rexford,
J. Identifying performance bottlenecks in CDNs
through TCP-level monitoring. In SIGCOMM
Workshop on Measurements Up the Stack (August
2011).
[10] Ha, S., Rhee, I., and Xu, L. CUBIC: a new
[27] Touch, J. TCP control block interdependence, April
TCP-friendly high-speed TCP variant. SIGOPS Oper.
Syst. Rev. 42 (July 2008), 64–74.
[11] Hoe, J. Improving the start-up behavior of a
congestion control scheme for TCP. SIGCOMM
Comput. Commun. Rev. (August 1996).
[12] Lin, D., and Kung, H. TCP fast recovery strategies:
Analysis and improvements. In Proc. of INFOCOMM
(1998).
1997. RFC 2140.
[28] Yang, P., Luo, W., Xu, L., Deogun, J., and Lu,
Y. TCP congestion avoidance algorithm identiﬁcation.
In Proc. of ICDCS (June 2011).
[29] Yang, Y., and Lam, S. General aimd congestion
control. Proc. International Conference on Network
Protocols. (November 2000).
166Summary Review Documentation for 
“Proportional Rate Reduction for TCP” 
Authors: N. Dukkipati, M. Mathis, Y. Cheng, M. Ghobadi 
Reviewer #1 
Strengths:	The A/B testing at scale that gathers a wealth of data 
from  production  Web  systems.  It  tells  us  something  about  Web 
latency in practice and that the TCP refinements lead to a modest 
reduction  in  loss  for  some  of  the  cases.  The  TCP  refinements 
themselves  are  small  contributions,  given  that  they  are  close  to 
Early  Retransmit  guidelines  and  Rate-Halving,  but  they  are 
useful.  
Weaknesses:  It’s  a  bit  hard  to  piece  together  from  the  way  the 
data is presented in the paper, but there is a *substantial* penalty 
for  any 
loss/retransmissions,  say  7-10X?,  and  your  TCP 
refinements are barely reducing it -- by 3-10%. I think this calls 
for looking at the data and problem in a different way (see below). 
Comments  to Authors: You have so much great data here, but 
you  are  barely  presenting  any  of  it!  Fig  1  is  tantalizing,  but  I 
would  really  like  to  see  more.  For  example,  some  CDFs  would 
show  the  variation  of  latency  (you  can  put  the  averages  in  a 
table!),  and  for  different  sizes  would  help  to  show  us  where 
connections  with  retransmissions  really  break  out  in  terms  of 
latency. Also, can you present any data on how many packets are 
lost from these connections? That would be great.  
The main question I would have then is what the causes are for 
the outliers. Timeout in Open looks like a big one, while Timeout 
in Disorder looks like a very small one, so why not tackle the big 
one first? For example, how often is a timeout in Open caused by 
the  last  packets  of  an  HTTP  response  being  lost?  Or  the  entire 
window? Or what else is going on? You have tackled the small 
cause, I think, so it helps the big picture by a very small amount.  
BTW,  I  would  hope  to  see  some  kind  of  picture  like  Fig  1  for 
after  you  apply  your  techniques  to  see  how  much  better  the 
situation has gotten. Right now my guess is that they would look 
very similar as the 7X case has gotten 10% better. Another way to 
get at this would be if you can tell us how much better it makes 
the latency when the new mechanisms make a difference. That is, 
in the cases where they “fire” can you estimate how much lower 
the latency is? Hopefully it is *much* lower ...  
I did not understand how HTTP1.1 and persistent connections fit 
in here. The emphasis of the analysis is all on the average HTTP 
response size. Why are persistent connections not helping more?  
Your  definition  of  latency  excludes  TCP  connection  setup. 
Normally  it  would  make  sense  to  include  this  because  it  is  a 
component  of  page  load  time.  Perhaps  you  are  excluding  it 
because you can’t measure it easily? Please tell us. 
Reviewer #2 
Strengths:  The  proposed  modifications  to  TCP  are  thoroughly 
evaluated with large numbers of real flows terminating in Google 
servers. The scale of evaluation of the proposed system is fairly 
detailed. 
Weaknesses: The gains appear to be fairly small. While this work 
is  likely  to  have  significant  impact,  especially  since  it  is  being 
deployed into Google servers, the authors do not convince us this 
is  better  than  some  alternate  published  work.  The  proportional 
rate  reduction  part  reminds  me  of 
the  different  GAIMD 
techniques  designed  as  extensions  to  TCP  about  a  decade  back. 
How do they compare to this proposal? 
Comments to Authors: The most impressive part of the work is 
that the proposed ideas are evaluated using a large volume of real 
flows that terminate in Google servers. This work is clearly likely 
to  have  significant  impact  just  because  it  might  affect  a  lot  of 
flows through such a deployment. The authors are also proposing 
this effort as a IETF draft and this work detailed evaluation of this 
proposal.  
However,  it  is  not  clear  if  the  impact  of  the  proposal  is  quite 
significant. First, the overall performance of the proposal does not 
improve TCP performance (latency of downloading short flows) 
by more than 7-8%, often it is much smaller. Second, there is a 
large  body  of  prior  work  in  this  domain  over  the  last  decade, 
which  have  similarity  to  aspects  of  the  modifications  proposed. 
For  instance,  there  are  the  GAIMD  and  Binomial  congestion 
control  proposals  that  alter  TCP’s  congestion  control  behavior 
that appear similar to the proportional rate reduction method. The 
authors could have considered using such schemes or could have 
compared their proposal to such older schemes.  
Also,  RFC  3517  modifications  proposed  before,  also  seem  to 
perform  quite  well  relative  to  what  is  being  proposed  in  this 
paper.  Why  not  simply  use  RFC  3517  alone.  What  are  the 
additional gains of this work relative to RFC 3517?  
The modification to early retransmit proposed in this work is what 
was proposed in RFC 5827. This work takes 2 of the 3 methods in 
RFC 5827, and evaluates it thoroughly through their experimental 
setup.  
I liked the way the experiments and comparisons were conducted.  
Overall, a nice and thorough piece of work but the ideas are small 
and incremental, much of which seem to be available from prior 
literature. The gains are not very significant. 
Reviewer #3 
Strengths:  Clear  writing,  unique  evaluation,  important  (well 
known)  problem,  useful  solution,  authoritative  description  of 
linux. 
Weaknesses: Yet another TCP paper. I didn’t like figure 1 very 
much. Weaknesses do not seem severe. 
167Comments  to  Authors:  It’s  not  clear  to  me  what  “The 
performance  of  connections  experiencing  losses  is  significantly 
slower than those with no losses” means in practice, since links 
that  are  crummy  are  likely  to  have  losses  and  have  poor 
performance. (i.e., correlation does not imply causation.) I didn’t 
see that your experiment would control for that, so I don’t really 
appreciate the figure as motivation.  
Linux P2 didn’t seem like too much of a problem... isn’t there a 
rule where quiet connections, after losing the ack clock, have to 
restart  from  slow  start  anyway,  regardless  of  retransmission 
events earlier?  
I  liked  this  paper  a  lot,  and  would  likely  encourage  students  to 
read  it,  particularly  when  linux  seems  to  adopt  tcp  features 
without much study.  
Is there a sense for how this fix would likely improve perceived 
browser performance? Will the user see fewer spinning wheels or 
does the browser already collect partial flows well enough to hide 
any performance trouble in TCP.  
I  appreciated  that  the  paper  was  as  long  as  necessary  and  not 
longer. 
Reviewer #4 
Strengths: Problem identification. Careful experimentation. 
Weaknesses: Substantial gap between how worse the problem is 
and how much improvement the suggested modifications achieve. 
For  e.g.,  the  best  variant  PRR-*  performs  comparably  to 
RFC3517  (table  8),  but  with  fewer  retransmissions  (table 
7).Impact  of  these  modifications  on  large  transfers,  video 
streams?  Slice losses by connection type? Not all losses require 
the  same  types  of  reaction,  e.g.,  wireless  losses.  Unclear  if 
understanding sources of losses would get more benefit. 
Comments  to  Authors:  The  first  parts  of  the  paper  were  more 
interesting than the proposed modifications. 
When presenting the problem, is there a way to figure out what an 
ideal retransmission mechanism could achieve?  
What are the causes of losses, you allude later on about shifting to 
lower  capacity  paths,  burst  retransmissions  having  higher 
likelihood of being lost... It is important because, at the core, the 
ideal retransmission strategy depends crucially on the causes. 
Is  there  reason  to  believe  that  the  losses  are  mostly  due  to 
congestion or buffer overflow? 
Do losses happen in large bursts? (fraction of lost packets among 
those that constitute a web response) 
Would end-to-end coding, to guard against 1 or 2 packet losses in 
a web download end up faring significantly better? 
Table 2, nothing in the paper seems to help with timeouts in open. 
PRR  helps  do  fast  retransmits  better  and  reduces  need  for 
timeouts. ER helps deal with disorder scenarios. 
I was surprised that ER ended up being as useful given the small 
number of Timeouts that happen in Disorder. 
Parts  of  3.1  and  3.2  do  not  seem  to  be  needed,  since  once  you 
present  the  mechanisms,  there  isn’t  much  there  that  depends  on 
retransmissions,  and 
the  reader  knowing  this  background.  The  background  info  is 
useful, but not essential. 
Figure  2,  ER  does  not  seem  to  help  much  the  latencies  of 
connections on the tail, so what causes the longest retransmissions 
(long RTT flows taking multiple RTTs to recover?), and how can 
that be improved? 
Reviewer #5 
Strengths:  Some  real  data  on  TCP  performance  in  the  wild 
against  highly  popular  servers.  Nice  exposition  of  existing  fast 
recovery mechanisms and their weaknesses. 
Weaknesses: The problem does not strike me as very important, 
especially given the low gains achieved in the end. Only ~6% of 
the  connections  suffer 
the  proposes 
mechanism cut down their latency by 10%. 
Comments  to  Authors:  I  don’t  quite  appreciate  the  motivation 
for your work. It’s only 6% of the responses, and you help them 
by  only  10%  or  so.  Is  it  worth  it?  It  might  be  from  a  business 
perspective  if  this  reduction  matters,  but  you  don’t  really  make 
the case. 
The real value for me in this paper was not your solution but the 
data on TCP retransmissions and the exposition of TCP recovery 
mechanisms. Even if the current paper does not go above the bar 
for me, those aspects of the paper are valuable and should get out. 
So, if the paper is not accepted, consider sending it to something 
like the CCR. 
On  the  data  front,  the  India  data  center,  which  you  allude  to 
would be valuable addition to the paper. 
What  experiments  are  being  talked  about  in  6.5,  when  you 
mention PRR-RB being too conservative and PRR-UB being too 
aggressive? 
Your  paper  did  make  me  wonder  about  one  thing:  How  much 
room  for  improvement  there  is  really  to  improve  fast  recovery? 
Based  on  your  traces,  can  you  construct  a  fast  recovery  oracle, 
which  knew  what  would  happen  next,  and  acted  accordingly. 
How much reduction in latency would such an oracle bring to the 
table? 
to 
the  original  submission 
Response from the Authors 
Summary  of  changes  applied 
“Improving TCP Loss Recovery” 
-  Major changes: 
We included both retransmission statistics as well as experiments 
with our changes in a India data-center serving Youtube traffic.  
The main thread of the paper is PRR as also reflected in the paper 
title,  unlike  the  original  paper  that  had  three  distinct  loosely 
connected items - PRR, ER, and Google measurements.  
We  rewrote  the  introduction  to  provide  a  clear  motivation  - our 
goal is to reduce TCP latency by improving fast recovery, which 
168as per the retransmission statistics is the key mechanism for loss 
recovery.  
We  illustrated  the  PRR  algorithm  and  its  properties  through 
carefully chosen examples. 
-  Other changes and comments on reviews: 
of 
1.  Concern:  there  is  a  substantial  penalty  (7-10X)  for  any 
losses/retransmissions  (Fig.  1  of  Introduction)  and  the  TCP 
refinements  described  in  the  paper  are  barely  reducing  it  by  3-
10%. 
The  intent  of  showing  Fig.  1  comparing  response  latency  with 
ideal is to provide context on the latency disparity that exists. Not 
all  of  this  can  be  addressed  by  improving  TCP’s  loss  recovery 
since  the  latency  also  includes  effects  of  slow  user  bandwidths 
and queueing delays, as noted in the camera-ready version. We do 
not  have  a  breakdown  of  latency  attributed  to  different  factors, 
nor  have  a    way  to  compare  an  ideal  retransmission  scheme 
without further insights into the bottleneck network conditions. 
As  requested  by  reviewer  #1,  we  included  a  CDF  to  show  the 
variation 
responses  with/without 
retransmissions and the ideal latency of 1 RTT. 
2.  Comparison  with  alternate  published  work  such  as  GAIMD 
techniques and Binomial congestion control. 
GAIMD  and  Binomial  are  congestion  control  algorithms 
orthogonal  to  current  work.  PRR  is  designed  to  work  in 
latencies 
between 
conjunction  with  any  congestion  control  algorithm  including 
GAIMD and Binomial. 
3.  What  are  the  additional  gains  of  this  work  relative  to  RFC 
3517? 
Addressed  in  the  camera-ready  paper  -  RFC  3517  is  overly 
aggressive in the presence of heavy losses wherein it can transmit 
large bursts of retransmissions which then incur further losses.  
4. What causes the losses? Do losses happen in bursts? 
These  are  interesting  questions,  however,  ones  that  we  did  not 
investigate as part of our current work.  
5. Nothing in the paper seems to help with timeouts in open state. 
Timeout  in  open  state  has  to  be  addressed  as  an  independent 
problem  in  itself  and  not  the  focus  of  current  work  on  fast 
recovery. 
6.  Only  ~6%  of  the  connections  suffer  retransmissions,  and  the 
proposed mechanisms cut down their latency by 10%. 
The  paper  includes  a  pointer  to  several independent studies that 
show  even  latency  changes  as  small  as  100ms  impact  user 
experience. Even small changes such as in the paper are crucial in 
ultimately bringing the latency of a Web page as close to ideal as 
possible, where ideal is close to one round-trip time. 
169