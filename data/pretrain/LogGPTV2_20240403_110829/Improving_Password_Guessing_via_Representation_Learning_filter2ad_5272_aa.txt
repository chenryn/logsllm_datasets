title:Improving Password Guessing via Representation Learning
author:Dario Pasquini and
Ankit Gangwal and
Giuseppe Ateniese and
Massimo Bernaschi and
Mauro Conti
6
1
0
0
0
.
1
2
0
2
.
1
0
0
0
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
1
2
0
2
©
0
0
.
1
3
$
/
1
2
/
5
-
4
3
9
8
-
1
8
2
7
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
1
2
0
2
2021 IEEE Symposium on Security and Privacy (SP)
Improving Password Guessing via
Representation Learning
Dario Pasquini1,2,3, Ankit Gangwal1,4, Giuseppe Ateniese1, Massimo Bernaschi3, Mauro Conti4
1Stevens Institute of Technology, 2Sapienza University of Rome,
{dpasquin, agangwal, gatenies}@stevens.edu, PI:EMAIL, PI:EMAIL
3Institute of Applied Computing CNR, 4University of Padua
Abstract—Learning useful representations from unstructured
data is one of the core challenges, as well as a driving force,
of modern data-driven approaches. Deep learning has demon-
strated the broad advantages of learning and harnessing such
representations.
In this paper, we introduce a deep generative model represen-
tation learning approach for password guessing. We show that an
abstract password representation naturally offers compelling and
versatile properties that open new directions in the extensively
studied, and yet presently active, password guessing ﬁeld. These
properties can establish novel password generation techniques
that are neither feasible nor practical with the existing probabilis-
tic and non-probabilistic approaches. Based on these properties,
we introduce: (1) A general framework for conditional password
guessing that can generate passwords with arbitrary biases; and
(2) an Expectation Maximization-inspired framework that can
dynamically adapt the estimated password distribution to match
the distribution of the attacked password set.
I. INTRODUCTION
Text-based passwords remain the most common form of
authentication, as they are both easy to implement and familiar
to users. However, text-based passwords are vulnerable to
guessing attacks. These attacks have been extensively studied,
and their analysis is still an active area of research. Modern
password guessing attacks are founded on the observation that
human-chosen passwords are not uniformly distributed in the
password space (i.e., all possible strings). This is due to the
natural preference for choosing (easily-)memorable passwords
that cover only a small fraction of the exponentially large pass-
word space. Consequently, real-world password distributions
are typically composed of several dense zones that can be
feasibly estimated by an adversary to perform password-space
reduction attacks [58]. Along that line, several probabilistic
approaches have been proposed [42], [27], [56]. These tech-
niques - under different assumptions - try to directly estimate
the probability distribution behind a set of observed passwords.
Such estimation is then used to generate suitable guesses and
perform efﬁcient password guessing attacks.
Orthogonal to the current lines of research, we demonstrate
that an adversary can further expand the attack opportunities
by leveraging representation learning techniques [18]. Rep-
resentation learning aims at learning useful and explanatory
representations [18] from a massive collection of unstructured
data. By applying this general approach on a corpus of
leaked passwords [12], we demonstrate the advantages that an
adversary can gain by learning a suitable representation of the
Fig. 1. A small section of the induced latent space around the latent point
for the password “rockyou”.
observed password distribution, rather than directly estimating
it. In this paper, we show that this type of representation allows
an attacker to establish novel password guessing techniques
that further threaten password-based authentication systems.
We model the representation of passwords in the latent
space of (1) an instance of Generative Adversarial Net-
works (GANs) [30] generator and (2) an instance of Wasser-
stein Auto-Encoders (WAEs) [51]. This type of representation,
thanks to its inherent smoothness [18], enforces a semantic
organization in the high-dimensional password space. Such an
organization mainly implies that, in the latent space of the
generator, respective representations of semantically-related
passwords are closer. As a result, geometric relations in the
latent space directly translate to semantic relations in the
data space. A representative example of this phenomenon is
loosely depicted in Fig. 1, where we show some latent points
(with their respective plain-text passwords) localized in a small
section of the induced latent space.
We exploit such geometric relations to perform a peculiar
form of conditional password generation. Namely, we charac-
terize two main properties: password strong locality and pass-
word weak locality. These locality principles enforce different
forms of passwords organization that allow us to design two
novel password guessing frameworks, Conditional Password
Guessing (CPG) and Dynamic Password Guessing (DPG). We
© 2021, Dario Pasquini. Under license to IEEE.
DOI 10.1109/SP40001.2021.00016
1382
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:27:44 UTC from IEEE Xplore.  Restrictions apply. 
rossyourockyperockyoudrockyaerickyourockywuroskyourockyturoskywurockyderockyornockyouroskyakrockyokrockyoerockywerackyoujoskyourockyduroc3youjockyourockyoljockywutockyourockyoutrockyogrickyeuros7yogrockypurockyaunickyouros7youjos3younickyoutrockyomrockyrurockyjurocky9uros3yourickyomnockyoktoskyourockyiujos7yourockynuroccyourockyousrockytejoskwourockdokroskyokruckyouroscyounoskyouroskwourockyakjosmyounockyorros6youromkyourockwourockywutrockywusrockyjeroskyoljockyolroskypuroc3yornockyousrockyneroskyoeroskyamras7yokrickypurockyeurockowurockysenockywurosmyourickydejockyokrocksokjoskwogrockyig19s7damjoskyoetos7youtoe7youros7yoknws6youjas7youroc7yourockywesroskyogroskyderockywudtoc3youjoskyokrocksomroc3yomroskyaeroskyorrockyuurockywedjoskyoudroszyourickyoerickyoknockyoutrockysutoekyouram6youroc3yoknockyiuros3yokros3dokrockyamnockwounockyogroskyaurockyjudnoc3youjoskwoudroc3dokrackywuroskyoudjoc3youros7yoerac3youpoc3yourickywuruckyaurockwokrickysujoe7youros7youdras6yokrockcoktoe3youroc3yograckyoeros6doktozmyoutos7yoknoskyoknos7younockyonrom6youjos7wourocky1urockyoesrickyuurac3yoknockyoudrockygutockwourocksamryskyoerosmyoejos7youtgos7youjockdokrockyiejoekwourosmwourackyogroskyoutroc3yoltockyoudrocky9mrac3dokraskyokros7yoltoelyoutos6yourockypedrickyoudroskydunockyoerickyegjoskyperos7dokrockyeeras7yoeroskdokjockwourocky1krockyarnos6yourackyomrockcomjos6yourocmyouroskyagrys6yoeroskyturos7yakrackydetoc7youraskyoutosmyouryc3domrockytjryc3youtwe6you1943somjas7yogtos7yolrackyweroc3soktos7yogrockysudtockyoeroskyiejockyoutrocky9krockoourys6yaeros6yoejockyomrocksoerockaweracksouros3yogroskyomrocky5urickyaunockyoljockyweroseyoutockyorjockyoerac7youjosmwouroc7yorrackyaunos6yamtoekyoutrockyagjoskyogros5yourzskyounoc3yoerackyokrockyreroczyounoe7youroskaturickyoutrocksoutos3yourzc3yoerockypesnoc3yokrommyokjos6woujoc7youroc3yoenockywusnos3yourockawurockyouemphasize that the state-of-the-art approaches are unable to
perform such types of advanced attacks or, if somehow altered,
become very inefﬁcient. The major contributions of our work
are as follows:
1) We are the ﬁrst to demonstrate the potential of using fully
unsupervised representation learning in the password
guessing domain.
2) We introduce a probabilistic and completely unsupervised
form of template-based passwords generation. We call
this framework CPG. CPG generates arbitrarily biased
passwords that can be used: (1) by an adversary to
increase the impact of side channels and similar password
attacks [16], [41], [54], [17]; or (2) by a legitimate user to
recover his/her password. We show the efﬁciency of CPG
against existing solutions via experimental evaluations.
3) We introduce the concept of DPG: DPG is the password
guessing approach that dynamically adapts the guessing
strategy based on the feedback received from the in-
teraction with the attacked passwords set. We build an
Expectation Maximization-inspired DPG implementation
based on the password locality enforced by the deep
generative model. DPG shows that an attacker can consis-
tently increase the impact of the attack by leveraging the
passwords successfully guessed during a running attack.
It is important to highlight that ongoing developments in deep
generative frameworks would naturally translate into further
improvements in our approach.
Organization: Section II gives an overview of the funda-
mental concepts related to our work. Here, we also present
our model improvements and the tools upon which our core
work is based. We introduce password locality along with
CPG in Section III and DPG in Section IV. The evaluation
of our proposed techniques is presented in their respective
sections. Section V brieﬂy discusses relevant previous works.
Section VI concludes the paper, although supplementary in-
formation is provided in the Appendices.
II. BACKGROUND AND PRELIMINARIES
In Section II-A, we brieﬂy introduce deep generative models
and related concepts that are important to understand our work.
In Section II-B, we present the two deep generative models
that we use as fundamental building blocks in our approach.
A. Deep Generative Models
A deep generative model is a probabilistic model trained
to perform implicit estimation of an unknown target data
distribution p∗(x), given a set of observable data (i.e., a train-
set) [30], [29]. In the process, a deep neural network is used to
parametrize the description of the underlying data distribution.
In contrast to the common prescribed probabilistic mod-
els [24], implicit probabilistic models do not explicitly esti-
mate the probability density of data; they instead approximate
the stochastic procedure that generates data [43].
In the general case, deep generative models are latent vari-
able models. That is, the network is implicitly guided to learn
a set of latent variables that unfold the complex interactions
among the factors describing data. During the training, a
prior distribution is imposed on the learned latent variables so
that we can eventually sample realizations of them after the
training. Such a prior, referred to as prior latent distribution
˙p(z) in this paper, is an easy-to-sample, uninformative
or
and factorized prior. Its factorized form indicates that the
network assigns a disjointed semantic meaning to each latent
variable, and, consequently, learns a disentangled latent data
representation for the input domain. In other words, the latent
representation is modeled to capture the posterior distribution
of the underlying explanatory factors of the observed data [48].
A generative network is a deterministic mapping function
G : Z → X between the latent space Z : Rk and the data
space X (i.e., where the observed data is deﬁned), speciﬁcally,
a bridge between ˙p(z) and the distribution p(x) learned
by the model. More formally, under this construction, the
probabilities of data instances have the following form:
p(x) = p(x | z; θ) ˙p(z),
(1)
where θ is the set of learnable parameters of the generator.
Typical choices for ˙p(z) are N (0, I) or U [0, 1] [29].
Sampling points z from the latent space according to
˙p(z) and then mapping them in the data-space through the
generator, is equivalent to sampling data points from the data
space X according to p(x). During this operation, we can
generally also consider an arbitrary p(z) that can be different1
from ˙p(z). In the rest of this paper, we will refer to the
probability density function p(z) of the latent space with the
general term of latent distribution.
Additionally,
the smoothness of the generator forces a
geometric organization in the learned latent space. Similar
to the feature embedding techniques [31], [37], indeed, the
latent representations of semantically bounded data points
show strong spatial coherence in the latent space [48].
We build our password guessing approach on top of two
interchangeable deep generative model frameworks, namely,
Generative Adversarial Networks and Autoencoders.
a) Generative Adversarial Networks (GANs): The GANs
framework learns a deep generative model by following an
adversarial training approach. The training process is guided
by a second network D (i.e., the critic/discriminator), which
gives a density estimation-by-comparison [43] loss function
to the generative network G (i.e., the generator). The adver-
sarial training bypasses the necessity of deﬁning an explicit
likelihood-function and allows us to have a good estimation
of very sharp distributions [29].
During the training, latent points z are directly sampled from
˙p(z) and given as input to G. In turn, the latter maps those in
the data-space, where they are fed to the network D. The critic,
receiving both ground-truth data instances from the train-set
and generating data from G, is trained to allocate density only
to real data instances. The generator G, instead, is adversarially
trained to force D to arrange probability estimates on the
1At a cost of representing a distribution different from p∗(x).
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:27:44 UTC from IEEE Xplore.  Restrictions apply. 
1383
output of G(z). The optimization follows from a coordinate
minimization of the losses of the two networks.
b) Autoencoders (AE): An Autoencoder is a deep gen-
erative model that conceptually compounds of two networks:
an encoder network Enc : X → Z and a decoder network
Dec : Z → X. The resulting aggregate model is generally
trained to learn a form of identity function: x = Dec(Enc(x)),
or a more useful variation of it. Unlike GANs, no adversarial
training is exploited during the training; typically, a maximum
likelihood approach is used, instead. Once trained, the network
Dec can serve as a data generator where meaningful latent
points are fed as input to it. However, to allow for efﬁcient
sampling from the latent space, an AE needs a form of explicit
regularization during the training; that is, the latent space must
be forced to be coherent with a chosen prior latent distribution.
Widely known AEs implementing this strategy are described
in [36], [40], [51].
In the rest of the paper, we make no distinction between
the decoder network Dec and the GAN generator; we refer to
either of them as G. In the same way, we employ E to refer
to the encoder network used to model the inverse mapping in
both models: G−1 : X → Z. For the AE, this network is
Enc, whereas, for the GAN, it is the network described in the
Appendix C.
B. Password guessing with deep generative models
Hitaj et al. in their seminal work PassGAN [35] trained a
GAN generator as an implicit estimator of password distribu-
tions. PassGAN harnesses an off-the-shelf Wasserstein GAN
with gradient penalty [33] over a residual-block-based archi-
tecture [34]. It assumes a latent space with a standard normal
distribution as its prior latent distribution and dimensionality
equal to 128. The model is trained on the RockYou [12]
password leak, and only passwords with 10 or fewer char-
acters were considered. Despite its underlying potential, the
password guessing approach presented in PassGAN suffers
from technical limitations and inherent disadvantages in its
application.2 Most limitations can be addressed as shown in
Section II-B1. However, some limitations are intrinsic to the
model itself. A prominent example is the model’s inability
to assign probabilities to the produced guesses consistently
and thus sort them based on popularity. This drawback might
make the GAN approach undesirable in a standard trawling
scenario. However, in the present work, we show the existence
of novel and valuable properties intrinsic to the class of deep
generative models. Abstracting the underlying model under
the perspective of representation learning, we prove that these
properties can be used to devise unique guessing techniques
that are infeasible with any existing approaches.
Next, we introduce the necessary improvements to the
original PassGAN construction (Section II-B1). In Section
II-B2, we introduce a different and novel deep generative
model in the password guessing domain.
2As a matter of fact, PassGAN requires up to ten times more guesses to
reach the same number of matched passwords as the probabilistic and non-
probabilistic competitors.
1) Improved GAN model: The password guessing approach
presented in PassGAN suffers from an inherent training in-
stability. Under such conditions, the generator and the critic
cannot carry out a sufﬁcient number of training iterations.
This may lead to an unsuitable approximation of the target
data distribution and reduced accuracy in the password guess-
ing task. In the original model, the discrete representation of
the strings (i.e., passwords) in the train-set3 introduces strong
instability for two main reasons: (1) The discrete data format
is very hard to reproduce for the generator because of the
ﬁnal softmax activation function, which can easily cause a
low-quality gradient; and (2) the inability of the generator
to fully mimic the discrete nature of the train-set makes it
straightforward for the critic to distinguish between real and
generated data. Hence, the critic can assign the correct “class”
easily, leaving no room for an enhancement of the generator,
especially in the ﬁnal stages of the training.
To tackle the problems above, we apply a form of stochastic
smoothing over the representation of the strings contained in
the train-set. This smoothing operation consists of applying
an additive noise of small magnitude over the one-hot encod-
ing representation of each character. The smoothing opera-
tion is governed by a hyper-parameter γ, which deﬁnes the
upper-bound of the noise’s magnitude. We empirically chose
γ = 0.01 and re-normalize each distribution of characters
after the application of the noise. This smoothing operation