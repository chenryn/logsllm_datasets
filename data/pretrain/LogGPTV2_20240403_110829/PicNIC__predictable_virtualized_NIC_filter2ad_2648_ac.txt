size distributions, such as Internet Mix (IMIX) [10, 51, 75], as is
common for NICs, switches, and middleboxes [11, 68, 73]. Such
devices usually have a PPS limit and support line rate only if the
average packet size is above a certain limit—e.g., Intel XL710 40GbE
NIC has a PPS limit of ∼37 Mpps, and needs packets larger than
160B to achieve line rate [31].
Delay. vNIC delay for fastpath flows (§2) is the time elapsed from
the instant a packet exits the sender’s guest OS Tx queue, e.g.
virtio Tx [36], to being received at the Rx queue of receiver’s
guest OS, excluding the delay in fabric. For throttled flows, it also
excludes any delay necessary for shaping. vNIC delay has two parts:
(i) egress delay from guest OS Tx queue to the wire and (ii) ingress
delay from the wire to guest OS Rx queue. The ingress delay is
further split into two parts: (i) delay in NIC hardware queues and
(ii) delay in engine. We specify predictable vNIC delay as a distri-
bution rather than a fixed value—e.g., median delay ≤ 10μs and
99t h perc. delay ≤ 50μs. We ensure delay guarantees for traffic that
adheres to its BPS envelope.
Loss rate. Like delay, a predictable vNIC should have low loss rate
for traffic within its BPS envelope. Particularly, both ingress and
egress drops should be zero for traffic responding to backpressure.
Uncooperative traffic that keeps discharging packets at a high rate
may experience high drops in order to ensure isolation for others.
Drops at the ingress hurt efficiency, and tail-drops at the NIC are
unfair; a predictable vNIC must avoid such drops at ingress.
To summarize, we define the abstraction of a predictable vNIC
in terms of bandwidth, delay and loss-rate SLOs for well-behaved
traffic that adheres to a standard packet size distribution. Delay
and loss-rate SLOs are for cooperating traffic within its BPS en-
velope. Even if a VM does not cooperate, we ensure predictable
performance for other well-behaved VMs. While we want to utilize
resources at shared contention points efficiently (e.g., in a work-
conserving manner) in the common case, we rapidly switch to
strict isolation when the goal of efficiency is too far counter to
isolation—e.g., when per-VM target SLOs may be violated. We build
PicNIC with three mechanisms to quickly detect such cases and tilt
the tradeoff towards isolation in sub-ms response time and with
negligible overhead to the dataplane.
5 Design and Implementation
In this section, we introduce the constructs of PicNIC and discuss
their combined role in achieving the predictable vNIC abstraction
(§5.1), followed by the details of design and implementation of each
construct (§5.2 to §5.4).
5.1 Constructs and Guarantees
Abstractly, instantiating the design principles (§3) in either hard-
ware or software requires a combination of local and end-to-end
constructs for: (i) sharing resources at ingress based on SLOs (P1),
(ii) admission control to manage contention at ingress (P2), and
(iii) sharing egress resources per SLOs with apt backpressure to
guest (P1, P2). Concretely, for a software-based virtualization stack,
we identify the following three design constructs. Fig. 5 shows their
system-level view.
Ingress CPU-fair Weighted Fair Queues (§5.2). At the ingress, PicNIC
implements per-VM CPU-fair weighted fair queues (CWFQs) to
share the engine’s processing capacity (P1) by apportioning the
engine’s CPU cycles in proportion to VM weights. A VM i’s weight
(wi ) is based on its SLO—e.g., wi ∝ MAX_BPSi .
Receiver-driven congestion control (§5.3). PicNIC provides SLO-based
shares of NIC bandwidth and engine capacity to VMs following the
hose model. To meet delay and loss-rate SLOs at ingress, it applies
backpressure to sources (P2) by implementing a receiver-driven
hypervisor-level congestion control called PicNIC Congestion Con-
trol (PCC). PCC computes ingress rate limits per VM and shares
these limits among senders.
Sender-side admission control (§5.4). At the egress, PicNIC imple-
ments admission control using a traffic shaper based on Carousel [67]
to enforce the rate limits computed by PCC. Additionally, it cre-
ates backpressure to guest VM transport (P2) and enforces per-VM
packet limits in the shaper buffer for isolation (P1).
355
SIGCOMM ’19, August 19–23, 2019, Beijing, China
P. Kumar et al.
Constructs
CPU-fair Weighted Fair Queues
PicNIC Congestion Control
PicNIC Sender-side Admission Control
(CWFQs, at ingress)
(PCC, end-to-end)
(at egress)
s
e
e
t
n
a
r
a
u
G
Bandwidth
Delay
Loss rate
Share engine CPU based on SLO;
provide signals to PCC
Avoid unfair drops at NIC
Compute SLO-based NIC BW share per re-
ceiver; backpressure to egress host
Compute PPS rate limit for each receiver;
backpressure to egress host
Rate limit senders to avoid drops
Enforce VM-VM BPS limits and per-VM
MAX_BPS; backpressure to guest OS stack
Enforce VM-VM PPS limits; backpressure to
guest OS stack; egress isolation
Backpressure to guest OS stack
Table 3: PicNIC constructs. Role of each PicNIC construct in achieving the predictable virtualized NIC abstraction.
Achieving a predictable vNIC abstraction. We provide insights
into our choice of these constructs and outline how they work
together to provide bandwidth, delay and loss-rate SLOs. Table 3
summarizes the role of each construct.
Bandwidth envelope. PCC computes the SLO-based ingress BPS
limit per receiver VM by first allocating the minimum bandwidth
SLO (MIN_BPS) for each VM and then dividing the residual host NIC
bandwidth based on the maximum bandwidth SLOs (MAX_BPS). Per
the hose model, PCC rate limits traffic at senders to meet the SLO-
based ingress BPS limit for each receiver VM. It also imposes a total
egress rate limit of MAX_BPS for each VM.
Low vNIC delay. At the ingress, isolation breaks when a high-rate
flow overloads the engine (§2.2). CWFQs share engine capacity
among VMs per SLO and drop offending traffic fairly to protect
others. This still wastes resources to pull and classify extra packets
from the NIC before dropping. NIC Rx queueing also increases in
such cases. To avoid such ingress overloads, PCC computes and
applies a SLO-based PPS rate limit for responsible traffic. At the
egress, PicNIC applies backpressure to guest OS stack when shaping
flows. To avoid HoL blocking due to slow flows exhausting buffer
resources (§2.1), PicNIC’s out-of-order completions (§5.4.3), along
with Linux NAPI-TX [44], offers per-TCP-flow level backpressure.
Per-VM packet accounting (§5.4.2) limits the number of outstanding
packets per VM in the engine, thus protecting other VMs regardless
of the number of flows for each VM.
Low loss rate. At the ingress, CWFQs move packets from NIC
queues to per-VM queues with high priority to avoid any unfair
NIC drops. To avoid both NIC and CWFQ drops at ingress, PCC
enforces rate limits at the egress. Sender-side admission control
applies backpressure to guest OS stack to prevent drops at egress.
5.2 Ingress CPU-Fair WFQs
The goal of CWFQs is to reduce unfair NIC drops and share ingress
engine’s CPU based on SLOs to provide isolation at short timescales.
They also aid in early detection of overloads.
The capacity of the packet-processing engine on the end host is
generally not constant and depends on various factors including
packet sizes, complexity of network functions, cache misses etc.
During overloads, the engine may not be able to process packets as
fast as they arrive at the NIC, leading to queueing and eventually
tail-drops in the NIC Rx queues—both are unfair as a high-rate flow
can impact other flows.
Sharing engine capacity is challenging using conventional ap-
proaches such as allocating SLO-weighted ingress BPS per VM.
BPS does not reflect the true resource usage—e.g., a flow with 64B
packets at a modest 512 Mbps translates to 1 Mpps, which may
consume significantly more CPU cycles relative to another flow
356
with 1500B packets at a higher BPS of 2.4 Gbps but lower PPS of
200 kpps. Engine CPU usage relates more directly to PPS instead
of BPS. However, even using PPS to track resource usage is tricky
as some flows need more complex processing, e.g. encryption, and
hence more CPU cycles per packet compared to others. Thus, we
must account CPU cycles used by the engine for each VM.
To track engine CPU usage per VM, PicNIC classifies packets
early by pulling them from NIC Rx, classifying them by destination
VM, and pushing them to the corresponding per-VM CWFQs. All
resource-intensive processing happens after CWFQs. By draining
NIC queues with high priority, PicNIC mitigates unfair tail-drops
and delays in the NIC. From per-VM queues, PicNIC dequeues pack-
ets for processing while sharing engine’s CPU cycles fairly. For this,
PicNIC records the CPU time spent to process each VM’s packets
and maintains the moving average (EWMA) giving more weight
to recent CPU usage. Using EWMA CPU time per-VM, PicNIC as-
signs each VM a dynamic priority that governs how frequently the
VM’s queue is scheduled. PicNIC recomputes and decreases a VM’s
priority whenever its queue is serviced, while skipping any empty
queues when scheduling. Hence, PicNIC’s work-conserving sched-
uler ensures that each VM is allocated engine CPU in a weighted
fair manner based on its demand and weight (∝ SLO).
5.3 PicNIC Congestion Control
PCC plays a central role in achieving all three SLOs by applying apt
backpressure across the network and coordinating among multiple
senders and receivers (Table 3). Note that while CWFQs are needed
only when multiple VMs exist per host, PCC is required even when
VMs exist in isolation on each host.
PCC implements receiver-driven hypervisor-level admission con-
trol. It has two parts: (i) PCCB, which sets BPS limits to ensure band-
width envelopes, and (ii) PCCP, which sets PPS limits to ensure low
vNIC delay and loss rate; only one of these limits is dominant at a
given time for each flow [14, 25]. With just BPS limits, we can pro-
vide bandwidth SLOs and yet not meet the delay and loss-rate SLOs.
Similarly, with just PPS limits, it is difficult to ensure bandwidth
envelope as packet sizes vary. Hence, we require both.
5.3.1 Bandwidth envelope. At the ingress, PCCB apportions
the host NIC’s BPS capacity (C) among VMs per their (MIN_BPS,
MAX_BPS) envelopes. To avoid drops, sources should not send, in
aggregate, more than the receiver VM’s apportioned ingress BPS.
PCCB ensures that each VM gets an ingress BPS ∈ [MIN_BPS, MAX_BPS].
Note that this allocation may not always be work-conserving be-
cause of the MAX_BPS limit per VM.
PCCB monitors the rate, r in
j
, at which traffic for a VM j is re-
ceived at the host. To compute the fair ingress capacity cj for each
PicNIC: Predictable Virtualized NIC
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Guest
Packet
accounting
...
Guest
TS
set egress
timestamp
backpressure to guest OS
consolidate multiple
rate limits
Egress
processing
NIC Tx
Timing wheel
congestion
control
c
i
r
b
a
F
compute rate-limits for senders
(share BW & CPU cycles based on SLO)
NIC Rx
...
CWFQs
Ingress
processing
out-of-order
completions
Egress Host
Ingress Host
measurement framework
(CPU cycles, bandwidth, delay, · · ·)
Guest
...
Guest
Figure 5: PicNIC architecture. Local constructs (ingress CWFQs and egress sender-side admission control) working in coherence with end-to-end receiver-
driven congestion control to achieve the predictable vNIC abstraction.

VM j, it first allocates the MIN_BPS: ∀j, cj ← MIN_BPSj . Then, it al-
locates any spare bandwidth (C −
cj ) to each active VM (i.e.,
with r in
j > 0) in proportion to the VM’s MAX_BPS and subject to
cj ≤ MAX_BPSj . To ensure that the ingress rate (r in
) for each VM
j
converges to its fair capacity (cj ), provided there is enough traf-
fic, PCCB computes a BPS value r l im
to rate limit senders. We
experimented with various control algorithms, each showing dif-
ferent convergence properties, such as time to converge to the
correct rate and stability. For concreteness and ease of comparison
with prior work, we present an implementation based on RCP and
EyeQ [19, 35]:
j
(cid:4)
(cid:3)
j ← r l im
r l im
j
·
1 − α ·
r i n
j −c j
c j
at every epoch,
PCCB computes r l im
j
at fixed epochs of length ϵ and notifies
r l im
to sender hosts. At the egress, PicNIC rate limits V Mi → V Mj
j
traffic to wi · r l im
so that cj is shared among senders in proportion
to their SLO (e.g. w ∝ MAX_BPS). This approach is independent
of the number of senders so that r in
converges to cj while r l im
j
scales automatically to a stable fixed point. In addition, PicNIC also
enforces a total egress rate limit of MAX_BPS for each sender VM.
j
j
5.3.2 Ensuring low delay in vNIC. While PCCB ensures BPS
SLO for VMs, delay and loss-rate SLOs do not follow automatically—
e.g., delay and drops can be high if a colocated VM receiving at a
high PPS, but within its BPS limits, overflows ingress queues (§2.2).
PCCP handles such cases.
PCCP’s goal is to keep the vNIC ingress delay within a given
distribution. Its activation is gated by per-VM CWFQ occupancy,
and it remains off for VMs whose traffic do not contribute to over-
load. Thus, in the absence of engine overload, PCCP is off. When
PCCP is activated for a VM due to high CWFQ occupancy or drops,
it starts with a rate estimate. The VM’s maximum loss-free receive
rate [50] or CWFQ goodput acts as a good estimate because it is
close to the desired ingress PPS rate for the VM.
PCCP is delay-based. The ingress delay (delayin) consists of de-
lays in the NIC and CWFQs. After CWFQs, packets are processed
and delivered to VMs without further queueing. As PicNIC ensures
that each VM gets an SLO-based share of engine CPU for packet
processing, an overloading VM with a resource-hungry flow gets
lower scheduling priority for its CWFQ, making its packets wait
longer in CWFQs compared to others. So, CWFQ delay automati-
cally captures the engine’s packet processing cost for each VM.
delayin = Tdequeue from per-VM queue − Treceived at host NIC HW
PCCP uses delayin, measured using accurate NIC hardware
timestamps, as congestion signal to compute the VM’s ingress PPS
357
Algorithm 1: PCCP rate control
Input: del ay_in
del ay ← EW M A(del ay, del ay_in)
if delay > threshold then
r at e ← (1 − β · (1 − t hr e s hol d
count er ← 0
t ar дet _r at e ← r at e
d e l ay
(cid:2) multiplicative decrease (MD)
)) · r at e
(cid:2) enter fast recovery (FR)
else
if NAI  NH AI then
(cid:2) hyper-active increase (HAI)
t ar дet _r at e ← t ar дet _r at e + (count er − NH AI ) · δ
r at e ← r at e +t ar дe t _r at e
count er ← count er + 1
2
Output: r at e
(cid:2) initial value = goodput / approx. #senders
capacity. We use PPS here as delay depends on engine CPU usage,
which relates more directly to PPS than BPS. Bounding delayin for
an overloading VM also decreases the shared NIC queuing part;
this, in turn, ensures low delayin for well-behaved VMs. Alg. 1
describes PCCP’s rate control algorithm, which is based on these
observations and inspired by prior approaches [1, 48, 78]. In a nut-
shell, it keeps delayin close to a specified threshold. Multiplicative
decrease reduces rate based on the extent by which delay exceeds
threshold, while fast recovery attempts to quickly increase the rate
to the value before the last decrease. Additive increase probes for
higher rates when delay is within the threshold, and after a few
cycles, it enters hyper-active increase to accelerate. Like PCCB, this
also runs at fixed epochs per VM.
PicNIC tracks the approximate count of sources with lightweight
cardinality estimation methods—e.g., HyperLogLog [23]—and uses
it to set a per-sender limit instantly on activation (Alg. 1). This
ensures low vNIC delay even while PCCP’s rate limit converges.
Feedback mechanism. For both PCCB and PCCP, we explored two
choices: (i) the in-datapath approach either generates special pack-
ets in the datapath or uses encap headers of packets in the reverse
direction to piggyback rate limits and (ii) the control-thread ap-
proach uses a control thread to scrape statistics and sampled sources
cached in the datapath, compute rate limits and notify source hosts
via a control channel, e.g. RPC. While the in-datapath approach is
more responsive, it also incurs overheads in the critical datapath.
5.4 Sender-side Admission Control
To enforce PCC’s rates limits, PicNIC implements a traffic shaper
building on Carousel [67] at the egress (§5.4.1). PicNIC avoids egress
isolation breakages (as in §2.1) by adding two techniques: (i) packet
SIGCOMM ’19, August 19–23, 2019, Beijing, China
P. Kumar et al.
Guest IP Stack
Packets
OOO Completion
Guest
Host
Guest NIC driver (virtio)
Packets
PicNIC CC Shaping
Other Shaping (e.g. BwE)
Per-VM Buffer Accounting