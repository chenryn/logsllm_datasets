title:An Algorithm for Automatically Obtaining Distributed and Fault-Tolerant
Static Schedules
author:Alain Girault and
Hamoudi Kalla and
Mihaela Sighireanu and
Yves Sorel
An Algorithm for Automatically Obtaining Distributed and Fault-Tolerant Static
Schedules
Alain Girault, Hamoudi Kalla
INRIA, 655 av. de l’Europe
3833 Saint-Ismier, Cedex - France
{Alain.Girault,Hamoudi.Kalla}@inrialpes.fr
Mihaela Sighireanu
LIAFA, Case 7014, 2 place Jussieu
75251 Paris, Cedex 05 - FRANCE
PI:EMAIL
Yves Sorel
INRIA, B.P.105
78153 Le Chesnay Cedex - FRANCE
PI:EMAIL
Abstract
1. Introduction
Our goal is to automatically obtain a distributed and
fault-tolerant embedded system: distributed because the
system must run on a distributed architecture; fault-tolerant
because the system is critical. Our starting point is a source
algorithm, a target distributed architecture, some distribu-
tion constraints, some indications on the execution times of
the algorithm operations on the processors of the target ar-
chitecture, some indications on the communication times of
the data-dependencies on the communication links of the
target architecture, a number Npf of fail-silent processor
failures that the obtained system must tolerate, and ﬁnally
some real-time constraints that the obtained system must
satisfy.
In this article, we present a scheduling heuristic
which, given all these inputs, produces a fault-tolerant, dis-
tributed, and static scheduling of the algorithm on the ar-
chitecture, with an indication whether or not the real-time
constraints are satisﬁed. The algorithm we propose consist
of a list scheduling heuristic based active replication strat-
egy, that allows at least Npf +1 replicas of an operation
to be scheduled on different processors, which are run in
parallel to tolerate at most Npf failures. Due to the strat-
egy used to schedule operations, simulation results show
that the proposed heuristic improve the performance of our
method, both in the absence and in the presence of failures.
Keywords: Fault Tolerance in Distributed and Real-Time
Systems, Safety-Critical Systems, software implemented
fault-tolerance, multi-component architectures, distribution
heuristics.
Embedded systems account for a major part of crit-
ical applications (space, aeronautics, nuclear. . . ) as well
as public domain applications (automotive, consumer
electronics. . . ). Their main features are:
• critical real-time: timing constraints which are not met
may involve a system failure leading to a human, eco-
logical, and/or ﬁnancial disaster;
• limited resources:
they rely on limited computing
power and memory because of weight, encumbrance,
energy consumption (e.g., autonomous vehicles), radi-
ation resistance (e.g., nuclear or space), or price con-
straints (e.g., consumer electronics);
• distributed and heterogeneous architecture: they are
often distributed to provide enough computing power
and to keep sensors and actuators close to the comput-
ing sites.
Moreover, the following aspect, extremely important w.r.t.
the target ﬁelds, must also be taken into account:
• fault-tolerance: an embedded system being intrinsi-
cally critical [20], it is essential to insure that its soft-
ware is fault-tolerant; this in itself can even motivate
its distribution; in such a case, at the very least, the
loss of one computing site must not lead to the loss of
the whole application.
The general domain of our research is that of distributed and
fault-tolerant embedded systems. The target applications
are critical embedded systems. Our ultimate goal is to pro-
duce automatically distributed and fault-tolerant code from
a given speciﬁcation of the desired system. In this paper,
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:08:19 UTC from IEEE Xplore.  Restrictions apply. 
we focus on a sub-problem, namely how to produce auto-
matically a distributed and fault-tolerant static schedule of
a given algorithm on a given distributed architecture.
Concretely, we are given as input a speciﬁcation of the
algorithm to be distributed (Alg), a speciﬁcation of the tar-
get architecture (Arc), some distribution constraints (Dis),
some information about the execution times of the algo-
rithm blocks on the architecture processors and the commu-
nication times of the algorithm data-dependencies on the ar-
chitecture communication links (Exe), some real-time con-
straints (Rtc), and a number of processor failures (Npf ).
The goal is to ﬁnd a static schedule of Alg on Arc, satis-
fying Dis, and tolerant to at most Npf processor failures,
with an indication whether or not this schedule satisﬁes Rtc
w.r.t. Exe. The global picture is shown in Figure 1. In this
paper, we focus on the distribution algorithm.
architecture specification
distribution constraints
execution times
real-time constraints
failure specification
high level program
compiler
model of the algorithm
distribution heuristic
fault-tolerant distributed static schedule
code generator
fault-tolerant distributed embedded code
Figure 1. Global picture of our methodology
Finding an algorithm that gives the best fault-tolerant
schedule w.r.t. the execution times is a well-known NP-hard
problem [10]. Instead, we provide a heuristic that gives one
scheduling, possibly not the best.
There are two constraints we have to deal with:
1. We are targeting embedded systems, so ﬁrst, we do
not allow the algorithm to add extra hardware, because
hardware resources in embedded systems are always
limited.
It implies that we have to do with the ex-
isting parallelism of the given architecture Arc.
If
the obtained schedule does not satisfy Rtc, then it is
the responsibility of the user to add more hardware
to increase the redundancy. And second, the obtained
schedule must be static to allow optimisations and to
minimise the executive overheads. Therefore, we can-
not apply the existing methods, proposed for example
in [7, 3, 11], which use preemptive scheduling or ap-
proximation methods.
2. We want to obtain the schedule automatically, so: The
fault-tolerance must be obtained without any help from
the user.
For these two reasons, it will fall into the class of soft-
ware implemented fault-tolerance.
2. Related Work
In the literature, we can identify several approaches:
① Some researchers make strong assumptions about the
failure models (e.g., only fail-silent) and about the kind of
schedule desired (e.g., only static schedule). By adhering
to these assumptions however, they are able to obtain auto-
matically distributed fault-tolerant schedules. For instance,
Ramamritham requires that the execution cost of each sub-
task is the same for each processor, and that the commu-
nication cost of each data-dependency is the same for each
communication link [19], thereby assuming that the target
architecture is homogeneous. Related approaches can be
found in [4] (independent tasks and homogeneous architec-
ture) and [18] (heterogeneous architecture but only one fail-
ure is tolerated).
② Other researchers introduce some dynamicity. For in-
stance, Caccamo and Buttazzo propose an on-line schedul-
ing algorithm to tolerate task failures on a uniprocessor sys-
tem [5], while Fohler proposes a mixed on-line and off-line
scheduling algorithm to tolerate task failures in a multipro-
cessor system [9].
③ Finally, some researchers take into account much less
restrictive assumptions, but they only achieve hand-made
solutions, e.g., with speciﬁc communication protocols, vot-
ing mechanisms. . . See the vast literature on general fault-
tolerance, for instance [17].
Like the other researchers belonging to the ﬁrst group,
we propose an automatic solution to the fault-tolerance dis-
tributed problem. The conjunction of the four following
points makes our approach original:
1. We take into account the execution time of both the
computation operations and the data communications
to optimise the critical path of the obtained schedule.
2. Since we produce a static schedule, we are able to com-
pute the expected completion date for any given oper-
ation or data communication, both in the presence and
in the absence of failures. Therefore we are able to
check the real-time constraints Rtc before the execu-
tion. If Rtc is not satisﬁed, we can give a warning to
the designer, so that he can decide whether to add more
hardware or to relax Rtc.
3. The given algorithm Alg can be designed with a high-
level programming language based on a formal mathe-
matical semantics. This is for instance the case of syn-
chronous languages, which are moreover well suited to
the programming of embedded critical systems [15, 2].
The advantage is that Alg can be formally veriﬁed
with model-checking and theorem proving tools, and
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:08:19 UTC from IEEE Xplore.  Restrictions apply. 
therefore we can assume safely that it is free of design
faults. The scheduling method we propose in this pa-
per preserves this property.
4. Operations scheduled on the distributed architecture
are guaranteed to complete if at most Npf processors
fails at any instant of time. There is no need for a com-
plex failure detection mechanism, and in particular we
do not need timeouts to detect the processor failures;
there is no need for the processors to propagate the
state of the faulty ones; and ﬁnally, due to the schedul-
ing strategy used the time needed for handling a failure
is minimal.
A different version of the method presented here has
been published as an abstract in [12] and as a full version
in a workshop [13].
It is different since it addresses dis-
tributed architectures consisting of several nodes connected
to a single bus, while here we address more general dis-
tributed architectures since they can include point-to-point
communication links (see Section 3.3). As a result, here the
communications can be scheduled in parallel on the com-
munication links, and the fault-tolerance is achieved with
the software redundancy of both the computation operations
and the data communications (see Section 4.1). In [12, 13]
we used the time redundancy of the data communications.
Also, we can cope with intermittent processor failures and
we do not need to use timeouts to detect failures, which was
not the case in [12, 13]. In conclusion, the method presented
here is complementary and more general than the one pre-
sented in [12, 13].
There is another work involving some of the authors [8],
where a totally different approach is taken: First, commu-
nication link failures are also taken into account, and sec-
ond, the method presented involves building a basic sched-
ule for each possible failure, and then merging these ba-
sic schedules to obtain a distributed fault-tolerant schedule.
The method presented here is lighter, faster, and more efﬁ-
cient, but it only copes with processor failures.
The rest of the paper is organised as follows. Section 3
states our fault-tolerance problem, and presents the various
models used by our method. Section 4 presents the pro-
posed solution for providing fault-tolerance. Section 5 pro-
vide a correctness proof of the proposed algorithm. Simu-
lation results are presented in Section 6. Finally, Section 7
concludes and proposes directions for future research.
3. Models
3.1. Failure Model
not this schedule satisﬁes Rtc w.r.t. Exe. The failures con-
sidered are fail-silent processor failures (permanent as well
as intermittent). By “tolerant” we mean that the obtained
schedule must achieve “failure masking” [17]. More pre-
cisely, this will be done by means of error compensation,
using software redundancy. The real-time constraints Rtc
can be, for instance, a deadline for the completion date of
the whole schedule. If the user wants to be more precise,
he/she can specify a deadline on the completion date of a
particular sub-task of the algorithm. The fact that the ob-
tained schedule is static allows the computation of its com-
pletion date w.r.t. Exe.
3.2. Algorithm Model
The algorithm is modelled by a data-ﬂow graph. Each
vertex is an operation and each edge is a data-dependency.
The algorithm is executed repeatedly for each input event
from the sensors in order to compute the output events for
actuators. We call each execution of the data-ﬂow graph
an iteration. This cyclic model exhibits the potential paral-
lelism of the algorithm through the partial order associated
to the graph. This model is commonly used for embedded
systems and automatic control systems.
Operations of the graph can be either:
• a computation operation (comp): its inputs must pre-
cede its outputs; the outputs depend only on the input
values; there is no internal state variable and no other
side effect;
• a memory operation (mem): the data is held by a mem
in sequential order between iterations; the output pre-
cedes the input, like a register in Boolean circuits;
• an external input/output operation (extio). Opera-
tions with no predecessor in the data ﬂow graph (resp.
no successor) are the external input interfaces (resp.
output), handling the events produced by the sensors
(resp. actuators). The extios are the only operations
with side effects; however, we assume that two exe-
cutions of a given input extio in the same iteration
always produce the same output value.
Figure 2 is an example of algorithm graph, with nine op-
erations: I and O are extios (resp. input and output),
while A–G are comps. The data-dependencies between
operations are depicted by arrows. For instance the data-
dependency A (cid:1) B can correspond to the sending of some
arithmetic result computed by A and needed by B.
3.3. Architecture Model
As said in the introduction, our goal is to ﬁnd a static
schedule of Alg on Arc, satisfying Dis, and tolerant to at
most Npf processor failures, with an indication whether or
The architecture is modelled by a graph, where each ver-
tex is a processor, and each edge is a communication link.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:08:19 UTC from IEEE Xplore.  Restrictions apply. 
Classically, a processor is made of one computation unit,
one local memory, and one or more communication units,
each connected to one communication link. Communica-
tion units execute data transfers, called comms. The chosen
communication mechanism is the send/receive [14], where
the send operation is non-blocking and the receive opera-
tion blocks in the absence of data. Figure 2 is an example
of architecture graph, with three processors and three point-
to-point links.
I
A
B
C
D
E
(a)
F
P1
P2
L1.2
G
O
L1.3
L2.3
P3
(b)
Figure 2. Example of (a) an algorithm graph
Alg (a); and (b) an architecture graph Arc
3.4. Distribution Constraints, Execution Times, and
Real-Time Constraints
For the operations, the execution times Exe consist of a
table associating to each pair (cid:1)o, p(cid:2) the execution time of
the operation o on the processor p, expressed in time units.
Since the target architecture is heterogeneous, the execution
times for a given operation can be distinct on each proces-
sor. Specifying the distribution constraints Dis involves as-
sociating the value “∞” to certain pairs (cid:1)o, p(cid:2), meaning that
o cannot be executed on p.
the execu-
tion times Exe consist of a table associating to each
pair (cid:1)data dependency, communication link(cid:2) the value
of the transmission time of this data dependency on this
communication link, again expressed in time units.
For the inter-processor communications,
C
2
3
1
operation
I
1
1.3
F
2
2.5
1
E
1
1.2
2
D
3
1.7
3
B
3
1
1.5
time
proc.
A
G
O
P1
2
1.4
1.4
1 ∞
P2
1.5
P3 ∞ 1
1.8
1.5
Table 1. Distributed constraints Dis and exe-
cution times Exe for operations