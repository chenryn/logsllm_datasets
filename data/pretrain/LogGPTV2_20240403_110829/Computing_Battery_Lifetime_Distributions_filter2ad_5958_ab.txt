calculated lifetime for a continuous load of 0.96A corre-
sponded to the experimental value given in [9].
In Table 1 we see the battery lifetimes according to the
KiBaM and some experimental results given in [9]. We see
that for KiBaM the lifetime is constant for both frequencies.
However, the experimental results show a longer lifetime
for the slower frequency. To overcome this problem Rao et
al. have developed a modiﬁed Kinetic Battery Model [9]. In
the modiﬁed model the recovery rate has an additional de-
pendence on the height of the bound-charge well, making
the recovery slower when less charge is left in the battery.
With a stochastic simulation of this model they obtain very
good results for the battery lifetimes. However, we numer-
ically evaluated the modiﬁed KiBaM with a deterministic
workload and saw that the lifetime still does not depend on
the frequency (see Table 1). Personal correspondence with
the authors of [9] has not shed light on the discrepancy.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:51:07 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 20074 Inhomogeneous MRMs for Batteries
We ﬁrst
introduce the notation for inhomogeneous
Markov reward models and their measures of interest. We
then show how the KiBaM can be integrated into a CTMC
workload model. Finally we present several small example
workload models.
4.1 Inhomogeneous MRMs
Homogeneous case. A (homogeneous) Markov re-
ward model (MRM) consists of a ﬁnite state space S =
{1, . . . , N}, the transition rate matrix Q ∈ R
N×N and a
reward vector r ∈ R
N .
with entries qi,j (cid:1) 0, j (cid:3)= i, and qi,i = −(cid:2)
The matrix Q is an inﬁnitesimal generator matrix, i.e.,
j∈S,j(cid:2)=i qi,j.
The diagonal entry qi,i, which is often denoted as −qi, de-
scribes the rate at which state i is left. This rate is to be in-
terpreted as the rate of a negative exponential distribution,
i.e., the probability that state i is left within s seconds is
given as 1 − e−qi·s. The next state then is j with probabil-
ity qi,j/qi. The initial distribution of states at time t = 0
is denoted as α . The generator matrix Q together with α
determines the CTMC X(t).
When in state i, reward is accumulated with rate ri which
might be positive or negative. The total reward accumulated
when residing in state i from time t1 until time t2 (cid:1) t1 is
denoted yi(t1, t2) and equals
yi(t1, t2) = ri · (t2 − t1).
Given the state process X(t), the accumulated reward at
time t, Y (t), is deﬁned as
(cid:3) t
Y (t) =
rX(s)ds.
0
The distribution of Y (t), the so-called performability dis-
tribution [10, 11], equals F Y (t, y) = Pr {Y (t) (cid:2) y}. The
corresponding density (with respect to y) equals
f Y (t, y) = ∂F Y (t, y)
∂y
1
hPr {y (cid:2) Y (t) (cid:2) y + h} .
= lim
h↓0
Inhomogeneous case. In the inhomogeneous case, the tran-
sition rate matrix Q and the reward vector r can depend on
the time t (time-inhomogeneous) and the accumulated re-
ward y (reward-inhomogeneous). We then have Q(t, y) and
r(t, y), where y is the current level of accumulated reward.
The reward accumulated between time t1 and t2 (cid:1) t1 when
residing completely in state i is described by the following
differential equation with initial value yi(t1, t1) = 0:
dyi(t1, t2)
dt2
= ri(t2, yi(t1, t2)).
The equation describes the rate of change at the end of the
interval [t1, t2] and so the reward rate depends on t2. The
accumulated reward until time t in this case is deﬁned as
(cid:3) t
0
Y (t) =
rX(s)(s, Y (s))ds.
R
An MRM can easily have more than one reward structure.
State i is then equipped with reward rates ri,1 through ri,K,
i.e., we have a reward matrix R(t, y) ∈ R
N×K for y ∈
K. The accumulated reward is then a vector of random
variables Y (t) = (Y1(t), . . . , YK(t)) and its distribution is
deﬁned as
F Y (t, (y1, . . . , yK)) = Pr {Y1(t) (cid:2) y1, . . . , YK(t) (cid:2) yK} .
Battery case. For the KiBaM we need an MRM that is
time-homogeneous but reward-inhomogeneous and has two
types of rewards. We therefore denote the generator matrix
as Q(y1, y2) and the reward rates as R(y1, y2) ∈ RN×2.
The reward accumulated in a state i between time t1 and
time t2 is described by the following differential equations
with initial values yi,1(t1, t1) = yi,2(t1, t1) = 0:
dyi,1(t1,t2)
dt2
dyi,2(t1,t2)
dt2
= ri,1 (yi,1(t1, t2), yi,2(t1, t2)) ,
= ri,2 (yi,1(t1, t2), yi,2(t1, t2)) .
The accumulated reward is then deﬁned as
Y (t) = (Y1(t), Y2(t))
rX(s)(Y (s))ds
(cid:4)
(cid:3) t
(cid:3) t
(cid:5)
0
=
=
(cid:6)
rX(s),1 (Y1(s), Y2(s)) , rX(s),2 (Y1(s), Y2(s))
ds,
0
and its distribution equals
F (Y1,Y2)(t, y1, y2) = Pr {Y1(t) (cid:2) y1, Y2(t) (cid:2) y2} .
We assume that the accumulated rewards have to be non-
negative and are bounded by a minimum l = (l1, l2) and
a maximum u = (u1, u2). This is absolutely reasonable
when considering batteries because their charge is always
between 0 and a predeﬁned capacity C. We then have
(2)
f (Y1,Y2)(t, y1, y2) = 0,
for y1  u1 or y2 > u2.
(3)
In the following we often consider the joint distribution
of state and accumulated rewards, that is,
Fi(t, y1, y2) = Pr {X(t) = i, Y1(t) (cid:2) y1, Y2(t) (cid:2) y2} ,
with density fi(t, y1, y2). The distribution of the accumu-
lated rewards can then be calculated using
F (Y1,Y2)(t, y1, y2) =
Fi(t, y1, y2).
(cid:7)
i∈S
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:51:07 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 20074.2 The KiBaMRM
We state the KiBaMRM as an MRM with two reward
types. The CTMC states {1, . . . , N} of the MRM reﬂect
the different operating modes of the device. The ﬁrst accu-
mulated reward Y1(t) represents the available-charge well,
the second accumulated reward Y2(t) represents the bound-
charge well. The corresponding rates are derived from the
KiBaM differential equations (1), using the constants k and
c and the equations h1 = y1/c and h2 = y2/(1 − c). Let
Ii be the energy consumption rate in a state i ∈ S. The ﬁrst
reward rate then is
(cid:1) −Ii + k · (h2 − h1), h2 > h1 > 0,
(cid:1) −k · (h2 − h1), h2 > h1 > 0,
otherwise,
otherwise.
ri,1(y1, y2) =
0,
and the second reward rate is
ri,2(y1, y2) =
0,
The interesting question for battery-powered devices is
“When does the battery get empty?” For the KiBaMRM
model, the battery is empty at time t if the available-charge
well Y1(t) is empty. Since the accumulated rewards Y1(t)
and Y2(t) are random variables, we can only indicate the
probability that the battery is empty at time t:
Pr {battery empty at time t} = Pr {Y1(t) = 0}
(4)
The lifetime L of a battery is the instant the battery gets
empty for the ﬁrst time,
L = min{t | Y1(t) = 0}.
4.3 Stochastic Workload Models
In the following we consider three stochastic workload
models. First we concentrate on simple on/off models like
the ones used in [9] with the only difference that those were
not stochastic. For a given frequency f , the workload tog-
gles between the off-state (no energy consumed) and the
on-state (energy consumed at a ﬁxed rate I = 0.96A). We
model the on/off times as Erlang-K distributions such that
with increasing K they become close to deterministic.
Figure 3 shows the state-transition diagram for this sim-
ple model. For frequency f , all transitions have rate
λ = 2 · f · K.
The expected on and off times, respectively, are then
K/(2f K) which leads exactly to a frequency f .
We furthermore consider two workload models of a
small battery-powered device. The ﬁrst, simple one consists
of three states as depicted in Figure 4. At the beginning the
model is in idle state. With rate λ = 2 per hour there is
the necessity to send data over the wireless interface. If such
data is present, the model moves into the send state. The
sending of data is complete in 10 minutes on average (re-
sulting in a sending rate of µ = 6 per hour). From the idle
state the device can also move into a power-saving sleep
state, this is done – on average – once per hour (τ = 1). The
power-consumption rate is low when idling (I0 = 8mA), it
is high when sending data (I1 = 200mA) and negligible in
the sleep state (I2 = 0mA). With a typical battery capac-
ity C = 800mAh (check your cell phone!), this means that
theoretically the device can be 4 hours in send mode or 100
hours in idle mode.
To extend the overall battery lifetime it seems to be ben-
eﬁcial to have short periods of high sending activity (bursts)
and long periods without sending activity. In the modelled
wireless device this could be achieved by accumulating the
data to be transmitted and then send all in a row instead of
transmitting lower amounts of data more frequently. This
can be modelled by buffering the ﬂow of arriving data.
When the ﬂow is active, data arrives with a very high rate. If
the ﬂow is inactive, the device can safely go to sleep. Figure
5 shows a state-transition diagram for such a burst model. It
has the same sending rate µ and timeout rate τ as the simple
model. Bursts start with rate switch on=1 per hour and
stop with rate switch off=6 per hour. To make any re-
sults of the latter two models comparable, we have chosen
λburst = 182 per hour such that the steady-state probability
to be in off − send or on − send in the burst model is the
same as the probability to be in send in the simple model.
As could be expected, the steady-state probability to be in
sleep is higher in the burst model than in the simple model.
on
off
...
...
Figure 3. Simple on/off model
sleep
2
λ
τ
send
1
λ
µ
idle
0
Figure 4. State transition diagram for the sim-
ple model
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:51:07 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007on−idle
switch_on
λburst
µ
on−send
sleep
switch_on
switch_off
switch_on
switch_off
τ
off−idle
µ
off−send
Figure 5. State transition diagram for the
burst model
5 Markovian Approximation
In this section we present a numerical algorithm for
the computation of the distribution of the accumulated re-
ward (performability) in an inhomogeneous Markov reward
model. It uses a Markovian approximation, in which the
computation is reduced to the transient solution of a pure
CTMC via uniformisation. The underlying idea already ap-
peared in [12] and is also used in [13] and [14] (steady-
state solution). We described the algorithm for homoge-
neous MRMs with positive reward rates in the CSRL con-
text [15, 16], then extended it to reward-inhomogeneous
models with positive reward rates [17]. We also explored
the applicability of a discretisation algorithm like the one
presented in [18]. However, this algorithm requires inte-
ger reward rates to work efﬁciently. In the case of rational
reward rates these have to be scaled which in turn substan-
tially increases the number of required discretisation steps,
thus making the algorithm unattractive and often even infea-
sible. Techniques for the reduction of the space complex-
ity like the one presented in [19] have still to be explored.
Nevertheless, a detailed description of the discretisation al-
gorithm can be found in [20].
There is also other work that addresses performability-
like measures in an inhomogeneous context. In the 1990’s
some work has been published on the computation of
transient state probabilities for inhomogeneous Markovian
models without rewards were addressed there [21, 22, 23].
A more recent paper [24] characterises the performabil-
ity distribution in inhomogeneous MRMs through a cou-
pled system of partial differential equations that is solved
through discretisation, and used to derive systems of ordi-
nary differential equations to determine moments of accu-
mulated reward.
In what follows we approximate the joint distribution of
state process and accumulated reward by the transient so-
lution of a derived homogeneous CTMC, that is, by a PH-
distribution. The approximation is applicable if the genera-
tor matrix and the reward rates depend on the current accu-
mulated reward and not on the current time. This is exactly
the case with our battery model and we therefore restrict
the presentation to a two dimensional reward structure, even
though the approach applies for three or more reward types
equally well.
...
...
...
...
...
...
...
...
Figure 6. Structure of the new generator ma-
trix Q∗.
The joint distribution of state and accumulated reward
(2) can be rewritten by summing over evenly-sized subin-
tervals of the reward intervals [l1, y1] and [l2, y2]:
∆ −1(cid:7)
y1
∆ −1(cid:7)
y2
j1= l1
∆
j2= l2
∆
 Xt = i,
 .
Y1(t) ∈ (j1∆, (j1 + 1)∆],
Y2(t) ∈ (j2∆, (j2 + 1)∆]
Pr
Fi(t, y1, y2) =
Here, ∆ is the stepsize at which the state space is discre-