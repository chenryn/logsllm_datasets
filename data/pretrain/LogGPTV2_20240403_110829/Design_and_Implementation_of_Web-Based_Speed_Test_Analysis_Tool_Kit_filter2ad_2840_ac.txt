request elicited by the tests. All tests sent 10–50 HTTP GET requests to measure
downlink throughput. Meanwhile, Xﬁnity, Ookla, and Fast.com sent hundreds to
thousands of HTTP POST requests, mainly for uplink throughput tests. Except
for CloudFlare, all tests sent many HTTP OPTIONS requests. These requests
were the preﬂighted requests to enforce the cross-origin resource sharing policy
(CORS) in browsers [40]. The test servers in these four tests were in diﬀerent
domains from the web interface, which triggered a preﬂighted request for every
new URIs to the test servers. Although small, these requests still consumed
network bandwidth and delayed the sending of POST requests.
Figure 4 shows two histograms of response sizes of HTTP GET and request
sizes of HTTP POST for the ﬁve platforms. Over 80% of web objects down-
loaded by Fast.com were either 2KB (20.15%) or 24 MB (63%). Xﬁnity’s down-
load objects sizes were between 53 MB and 78 MB and the download test often
did not ﬁnish due to time limit expiration. SpeedOf.Me used the largest web
object (128 MB) among all tests. 35% of GET requests from Ookla tests had
the response size of only 300B and 79% of GET requests from Cloudﬂare tests
had the response size 26 MB). High variances between numbers and sizes of HTTP transactions
for diﬀerent platforms might lead to inaccuracies in various network environ-
ments [10].
Fig. 4. The distribution of HTTP object sizes for the ﬁve speed tests.
6.2 Variances in RTT Measurements
To observe RTT variances, we conducted analysis on the data collected using
CLASP [21], which performed hourly tests from a VM in Google Cloud us-west1
to the nearest Xﬁnity speed test servers in Seattle, WA for two weeks (Jul 7–15,
2020). Because Xﬁnity by default selected servers in Little Rock, AR, we had
WebTestKit instead conﬁgure the server location to Seattle, WA. Xﬁnity speed
Design and Implementation of Web-Based Speed Test Analysis Tool Kit
93
Fig. 5. Hourly Xﬁnity measurements from GCP us-west1 to test servers in Seattle, WA.
(a) RTTs ﬂuctuated throughout the measurement period. (b) 25.7% of tests reported
high RTTs (>50 ms) still used the default servers in Little Rock, AR, ignoring our
selection.
test sent ten persistent HTTP GET requests consecutively and reported the
minimum HTTP request-response time as the RTT.
RTTs largely varied between 10–55 ms throughout the time period (Fig. 5a).
The screen capture WebTestKit recorded after each test conﬁrmed that the
script correctly selected the server location. However, WebTestKit revealed that
the measurements reporting high RTTs did not use the correct servers. Figure 5b
shows the distributions of the RTTs reported by each test according to the test
servers’ IP. We used the server information crawled from the test platform to
locate the servers. Each location had three test servers in the same /24 subnet.
The left-/right-three servers were located in Seattle/Little Rock, respectively.
The RTTs to servers in Little Rock were above 50 ms, while the RTTs to Seattle
servers showed a bimodal distribution with peaks at 9 ms and 23 ms, possibly
due to an asymmetric reverse path (Comcast → GCP).
The high RTT variations were partially due to Xﬁnity speed test fail-
ing to switch to the selected servers even when speciﬁed in the webpage. We
reported this issue to Comcast for further investigation. This example illustrates
WebTestKit’s capability to diagnose problems with speed test implementations.
6.3 Accuracy of RTT Measurements
Web-based speed tests often use HTTP request-response times (HRTs) to esti-
mate RTTs, an important factor for server selection and throughput estima-
tion. The packet matching algorithm allows WebTestKit to estimate HRTs from
packet traces. Meanwhile, JavaScript-based test clients could use two APIs
to measure HRTs: XMLHttpRequest (XHR) [41] and Resource Timing APIs
(RET) [39], which both could introduce overhead to measurement results from
browser rendering and system function calls. To evaluate the accuracies of these
two APIs, WebTestKit records timing information from both in execution and
compares them with the derived HRTs.
94
R. Yang et al.
X), RET (T i
R), and packet traces (T i
− T i
We analyzed the HRTs in our Xﬁnity speed test measurements (Sect. 6.2). We
obtained three HRTs for the ith HTTP transactions used for latency measure-
P ). We then computed
ments with XHR (T i
.∀A, B ⊂ {X, R, P}).
the diﬀerences between two of the HRTs, Δi
Blue, cyan, and purple bars in Fig. 6 represent the probability of diﬀerent val-
ues of ΔX−R, ΔR−P , and ΔX−P , respectively. We found XHR performed much
worse than RET. The minimum value of ΔR−P and ΔX−P were 0.59 ms and
2.3 ms, respectively, indicating the unavoidable inﬂation in HRTs. 66.4%/83.1%
of ΔR−P /ΔX−P was less than 1 ms/10 ms, respectively. Even though RET was
much more accurate than XHR, we found 10% of ΔR−P were higher than 28 ms.
B−A(= T i
B
A
Fig. 6. Normalized histograms of HRT
diﬀerences, Δ, between TX , TR, and TP .
Fig. 7. CDFs of reported latency and
minimum RTTs obtained from diﬀerent
layers.
We studied the impact of the HRT inaccuracy on the ﬁnal measurement
results. We selected the minimum HRTs measured with XHR, RET, and packet
trace (Pcap) in each test. Figure 7 shows the CDFs of the minimum RTTs and
the reported latency. Xﬁnity speed test used the XHR method to measure RTTs.
Therefore, the reported values were almost identical to the XHR values, except
for the rounding errors. The RTTs derived using RET and packet trace were
consistently lower than the XHR RTT values by around 2 and 2.9 ms, respec-
tively, consistent with our results in Fig. 6. As the RTT between the VM and
test servers in Seattle was low (The lowest RTT was 7.01 ms/7.93 ms/10.0 ms
measured by packet trace/RET/XHR), the error rate in RET/XHR was over
13%/30%, respectively. We concluded that using XHR to measure RTTs resulted
in inﬂated values. Applying a minimum ﬁlter to measurements did not mitigate
this error.
7 Conclusion
We presented WebTestKit, a uniﬁed and conﬁgurable framework for automating
speed tests and performing cross-layer analysis of test results. Our evaluation
showed WebTestKit was lightweight and accurate in interpreting encrypted traf-
ﬁc. We used WebTestKit to characterize the behavior of ﬁve major speed tests
Design and Implementation of Web-Based Speed Test Analysis Tool Kit
95
and identify a large number of preﬂighted requests, generating additional net-
work overhead. We discovered high variances in RTT measurements of Xﬁnity
speed test, caused by inconsistency between web interface and test servers.
Acknowledgment. We thank anonymous reviewers for their valuable comments. This
work was supported by the Key-Area Research and Development Program of Guang-
dong Province (No. 2020B010164001), NSF CNS-2028506, NSF OAC-1724853, Com-
cast Innovation Fund, and Google Cloud credit grant.
References
1. Ookla open datasets. https://registry.opendata.aws/speedtest-global-performa
nce/
2. Speedof.me. https://speedof.me
3. Bauer, S., Clark, D., Lehr, W.: Understanding broadband speed measurements. In:
Proceedings of the TPRC (2010)
4. Bauer, S., Lehr, W., Mou, M.: Improving the measurement and analysis of giga-
bit broadband networks. Technical report, Massachusetts Institute of Technology
(2016)
5. Chromium. Netlog viewer. https://netlog-viewer.appspot.com/
6. CloudFlare. Cloudﬂare speed test. https://speed.cloudﬂare.com
7. Comcast. Xﬁnity speed test. http://speedtest.xﬁnity.com
8. Doan, T.V., Bajpai, V., Crawford, S.: A longitudinal view of Netﬂix: content deliv-
ery over IPv6 and content cache deployments. In: Proceedings of the IEEE INFO-
COM (2020)
9. Fast.com. Internet speed test. https://fast.com
10. Feamster, N., Livingood, J.: Measuring internet speed. Commun. ACM 63(12),
72–80 (2020)
11. Goga, O., Teixeira, R.: Speed measurements of residential internet access. In: Taft,
N., Ricciato, F. (eds.) PAM 2012. LNCS, vol. 7192, pp. 168–178. Springer, Heidel-
berg (2012). https://doi.org/10.1007/978-3-642-28537-0 17
12. Haselton, T.: CNBC tech guide: how to make sure you’re getting the inter-
net speeds you pay for (2018). https://www.cnbc.com/2018/08/17/how-to-check-
internet-speed.html
13. Høiland-Jørgensen, T., Ahlgren, B., Hurtig, P., Brunstrom, A.: Measuring latency
variation in the internet. In: Proceedings of the ACM CoNEXT (2016)
14. HTTP Toolkit. Chrome 79+ no longer shows preﬂight CORS requests. https://
httptoolkit.tech/blog/chrome-79-doesnt-show-cors-preﬂight/
15. Hu, N., Steenkiste, P.: Evaluation and characterization of available bandwidth
probing techniques. IEEE J. Sel. A. Commun. 21(6), 879–894 (2006)
16. Hulu. Hulu help center: Test your internet connection. https://help.hulu.com/s/
article/speed-test?language=en US
17. Jain, M., Dovrolis, C.: End-to-end available bandwidth: measurement method-
ology, dynamics, and relation with TCP throughput. IEEE/ACM Trans. Netw.
11(4), 537–549 (2003)
18. Li, W., Mok, R., Chang, R., Fok, W.: Appraising the delay accuracy in browser-
based network measurement. In: Proceedings of the ACM/USENIX IMC (2013)
19. M Lab. Murakami. https://www.measurementlab.net/blog/murakami/. Accessed
15 July 2021
96
R. Yang et al.
20. M-Lab. NDT (network diagnostic tool). https://www.measurementlab.net/tests/
ndt/
21. Mok, R.K., Zou, H., Yang, R., Koch, T., Katz-Bassett, E., Claﬀy, K.: Measuring
the network performance of Google Cloud platform. In: ACM IMC, Virtual Event
(2021)
22. Netﬂix. Netﬂix help center: Internet connection speed recommendations. https://
help.netﬂix.com/en/node/306
23. Ookla. About ookla. http://www.speedtest.net/en/about
24. Ookla. Speedtest. http://www.speedtest.net
25. Ookla. How does the test itself work? How is the result calculated? (2012).
https://support.speedtest.net/hc/en-us/articles/203845400-How-does-the-test-
itself-work-How-is-the-result-calculated-
26. Padhye, J., Firoiu, V., Towsley, D.F., Kurose, J.F.: Modeling TCP reno perfor-
mance: a simple model and its empirical validation. IEEE/ACM Trans. Netw. 8,
133–145 (2000)
27. Philip, A.: Slow internet? how to ﬁgure out if it’s your problem or your service
provider’s. https://www.azcentral.com/story/news/local/arizona-investigations/
2018/09/06/your-internet-slow-heres-how-ﬁgure-out-whos-fault/1058007002/
28. Ribeiro, V.J., Riedi, R.H., Baraniuk, R.G., Navratil, J., Cottrell, L.: pathChirp:
eﬃcient available bandwidth estimation for network paths (2003)
29. Sivel. Cloudﬂare-cli. https://github.com/KNawm/speed-cloudﬂare-cli
30. Sivel. Fast-cli. https://github.com/sindresorhus/fast-cli
31. Sivel. Speedtest-cli. https://github.com/sivel/speedtest-cli
32. Sommers, J., Durairajan, R., Barford, P.: Automatic metadata generation for
active measurement. In: Proceedings of the ACM IMC (2017)
33. Strauss, J., Katabi, D., Kaashoek, F.: A measurement study of available bandwidth
estimation tools. In: Proceedings of the ACM IMC (2013)
34. Sundaresan, S., de Donato, W., Feamster, N., Teixeira, R., Crawford, S., Pescap´e,
A.: Broadband Internet performance: a view from the gateway. In: Proceedings of
the ACM SIGCOMM (2011)
35. Sundaresan, S., Lee, D., Deng, X., Feng, Y., Dhamdhere, A.: Challenges in inferring
internet congestion using throughput measurements. In: Proceedings of the ACM
IMC (2017)
36. The Chromium Projects. NetLog: Chrome’s network logging system. https://www.
chromium.org/developers/design-documents/network-stack/netlog
37. The Chromium Projects. The trace event proﬁling tool. https://www.chromium.
org/developers/how-tos/trace-event-proﬁling-tool
38. The Oﬃce of the New York State Attorney General. Are you getting the internet
speeds you are paying for? https://ag.ny.gov/SpeedTest
39. W3C. Resource Timing Level 2. https://www.w3.org/TR/resource-timing-2/.
Accessed 26 June 2021
40. M. web docs. Cross-origin resource sharing (cors). https://developer.mozilla.org/
en-US/docs/Web/HTTP/CORS#Preﬂighted requests. Accessed 23 Feb 2019
41. WHATWG. XMLHttpRequest Living Standard. https://xhr.spec.whatwg.org
42. Xu, D., et al.: Understanding operational 5G: a ﬁrst measurement study on its cov-
erage, performance and energy consumption. In: ACM SIGCOMM, Virtual Event,
NY, USA (2020)
43. Yang, X., et al.: Fast and light bandwidth testing for internet users. In: USENIX
NSDI, Virtual Event (2021)