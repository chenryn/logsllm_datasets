### Request Elicitation and Throughput Measurement

During the tests, 10-50 HTTP GET requests were sent to measure downlink throughput. In contrast, Xfinity, Ookla, and Fast.com issued hundreds to thousands of HTTP POST requests, primarily for uplink throughput tests. With the exception of Cloudflare, all other tests generated numerous HTTP OPTIONS requests. These OPTIONS requests are preflighted requests, which enforce the cross-origin resource sharing (CORS) policy in web browsers [40]. Since the test servers and the web interfaces were hosted on different domains, each new URI to the test servers triggered a preflighted request. Although these requests were small, they still consumed network bandwidth and delayed the sending of POST requests.

### Distribution of HTTP Object Sizes

Figure 4 illustrates the histograms of response sizes for HTTP GET and request sizes for HTTP POST across the five platforms. For Fast.com, over 80% of the downloaded web objects were either 2KB (20.15%) or 24MB (63%). Xfinity's download object sizes ranged from 53MB to 78MB, and the download tests often did not complete due to time limits. SpeedOf.Me used the largest web object at 128MB. Additionally, 35% of GET requests from Ookla had response sizes of only 300B, while 79% of GET requests from Cloudflare had response sizes of 26MB. The high variances in the number and size of HTTP transactions across different platforms could lead to inaccuracies in various network environments [10].

**Figure 4.** The distribution of HTTP object sizes for the five speed tests.

### Variance in RTT Measurements

To observe RTT variances, we analyzed data collected using CLASP [21], which performed hourly tests from a Google Cloud VM in us-west1 to the nearest Xfinity speed test servers in Seattle, WA, over a two-week period (July 7–15, 2020). By default, Xfinity selected servers in Little Rock, AR, so we configured WebTestKit to use servers in Seattle, WA. The Xfinity speed test sent ten persistent HTTP GET requests consecutively and reported the minimum HTTP request-response time as the RTT.

RTTs fluctuated between 10–55 ms throughout the measurement period (Fig. 5a). WebTestKit confirmed that the server location was correctly selected. However, it also revealed that some measurements with high RTTs (>50 ms) were still using the default servers in Little Rock, AR (Fig. 5b). We used server information crawled from the test platform to locate the servers, with each location having three test servers in the same /24 subnet. The left-/right-three servers were located in Seattle/Little Rock, respectively. RTTs to servers in Little Rock were above 50 ms, while RTTs to Seattle servers showed a bimodal distribution with peaks at 9 ms and 23 ms, possibly due to an asymmetric reverse path (Comcast → GCP).

The high RTT variations were partially due to the Xfinity speed test failing to switch to the selected servers, even when specified in the webpage. We reported this issue to Comcast for further investigation. This example highlights WebTestKit's capability to diagnose problems with speed test implementations.

**Figure 5.** Hourly Xfinity measurements from GCP us-west1 to test servers in Seattle, WA.
(a) RTTs fluctuated throughout the measurement period.
(b) 25.7% of tests reported high RTTs (>50 ms) and used the default servers in Little Rock, AR, ignoring our selection.

### Accuracy of RTT Measurements

Web-based speed tests often use HTTP request-response times (HRTs) to estimate RTTs, which are crucial for server selection and throughput estimation. WebTestKit uses a packet matching algorithm to estimate HRTs from packet traces. JavaScript-based test clients can use two APIs to measure HRTs: XMLHttpRequest (XHR) [41] and Resource Timing APIs (RET) [39]. Both APIs can introduce overhead due to browser rendering and system function calls. To evaluate the accuracies of these APIs, WebTestKit records timing information from both in execution and compares them with the derived HRTs.

We analyzed the HRTs from our Xfinity speed test measurements (Section 6.2). We obtained three HRTs for the ith HTTP transaction: XHR (TiX), RET (TiR), and packet traces (TiP). We then computed the differences between two of the HRTs, ΔiB−A(= TiB − TiA), for all combinations A, B ⊂ {X, R, P}.

**Figure 6.** Normalized histograms of HRT differences, Δ, between TX, TR, and TP.

We found that XHR performed much worse than RET. The minimum values of ΔR−P and ΔX−P were 0.59 ms and 2.3 ms, respectively, indicating unavoidable inflation in HRTs. 66.4%/83.1% of ΔR−P/ΔX−P were less than 1 ms/10 ms, respectively. Even though RET was more accurate than XHR, 10% of ΔR−P were higher than 28 ms.

**Figure 7.** CDFs of reported latency and minimum RTTs obtained from different layers.

We studied the impact of HRT inaccuracy on the final measurement results. We selected the minimum HRTs measured with XHR, RET, and packet trace (Pcap) in each test. Figure 7 shows the CDFs of the minimum RTTs and the reported latency. Xfinity speed test used the XHR method to measure RTTs, resulting in reported values almost identical to the XHR values, except for rounding errors. The RTTs derived using RET and packet trace were consistently lower than the XHR RTT values by around 2 ms and 2.9 ms, respectively, consistent with our results in Fig. 6. Given the low RTT between the VM and test servers in Seattle (the lowest RTT was 7.01 ms/7.93 ms/10.0 ms measured by packet trace/RET/XHR), the error rate in RET/XHR was over 13%/30%, respectively. We concluded that using XHR to measure RTTs resulted in inflated values. Applying a minimum filter to measurements did not mitigate this error.

### Conclusion

We presented WebTestKit, a unified and configurable framework for automating speed tests and performing cross-layer analysis of test results. Our evaluation showed that WebTestKit is lightweight and accurate in interpreting encrypted traffic. Using WebTestKit, we characterized the behavior of five major speed tests and identified a large number of preflighted requests, generating additional network overhead. We discovered high variances in RTT measurements of the Xfinity speed test, caused by inconsistency between the web interface and test servers.

### Acknowledgments

We thank anonymous reviewers for their valuable comments. This work was supported by the Key-Area Research and Development Program of Guangdong Province (No. 2020B010164001), NSF CNS-2028506, NSF OAC-1724853, Comcast Innovation Fund, and Google Cloud credit grant.

### References

1. Ookla open datasets. https://registry.opendata.aws/speedtest-global-performance/
2. Speedof.me. https://speedof.me
3. Bauer, S., Clark, D., Lehr, W.: Understanding broadband speed measurements. In: Proceedings of the TPRC (2010)
4. Bauer, S., Lehr, W., Mou, M.: Improving the measurement and analysis of gigabit broadband networks. Technical report, Massachusetts Institute of Technology (2016)
5. Chromium. Netlog viewer. https://netlog-viewer.appspot.com/
6. CloudFlare. Cloudflare speed test. https://speed.cloudflare.com
7. Comcast. Xfinity speed test. http://speedtest.xfinity.com
8. Doan, T.V., Bajpai, V., Crawford, S.: A longitudinal view of Netflix: content delivery over IPv6 and content cache deployments. In: Proceedings of the IEEE INFOCOM (2020)
9. Fast.com. Internet speed test. https://fast.com
10. Feamster, N., Livingood, J.: Measuring internet speed. Commun. ACM 63(12), 72–80 (2020)
11. Goga, O., Teixeira, R.: Speed measurements of residential internet access. In: Taft, N., Ricciato, F. (eds.) PAM 2012. LNCS, vol. 7192, pp. 168–178. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-28537-0_17
12. Haselton, T.: CNBC tech guide: how to make sure you’re getting the internet speeds you pay for (2018). https://www.cnbc.com/2018/08/17/how-to-check-internet-speed.html
13. Høiland-Jørgensen, T., Ahlgren, B., Hurtig, P., Brunstrom, A.: Measuring latency variation in the internet. In: Proceedings of the ACM CoNEXT (2016)
14. HTTP Toolkit. Chrome 79+ no longer shows preflight CORS requests. https://httptoolkit.tech/blog/chrome-79-doesnt-show-cors-preflight/
15. Hu, N., Steenkiste, P.: Evaluation and characterization of available bandwidth probing techniques. IEEE J. Sel. A. Commun. 21(6), 879–894 (2006)
16. Hulu. Hulu help center: Test your internet connection. https://help.hulu.com/s/article/speed-test?language=en_US
17. Jain, M., Dovrolis, C.: End-to-end available bandwidth: measurement methodology, dynamics, and relation with TCP throughput. IEEE/ACM Trans. Netw. 11(4), 537–549 (2003)
18. Li, W., Mok, R., Chang, R., Fok, W.: Appraising the delay accuracy in browser-based network measurement. In: Proceedings of the ACM/USENIX IMC (2013)
19. M Lab. Murakami. https://www.measurementlab.net/blog/murakami/. Accessed 15 July 2021
20. M-Lab. NDT (network diagnostic tool). https://www.measurementlab.net/tests/ndt/
21. Mok, R.K., Zou, H., Yang, R., Koch, T., Katz-Bassett, E., Claffy, K.: Measuring the network performance of Google Cloud platform. In: ACM IMC, Virtual Event (2021)
22. Netflix. Netflix help center: Internet connection speed recommendations. https://help.netflix.com/en/node/306
23. Ookla. About ookla. http://www.speedtest.net/en/about
24. Ookla. Speedtest. http://www.speedtest.net
25. Ookla. How does the test itself work? How is the result calculated? (2012). https://support.speedtest.net/hc/en-us/articles/203845400-How-does-the-test-itself-work-How-is-the-result-calculated-
26. Padhye, J., Firoiu, V., Towsley, D.F., Kurose, J.F.: Modeling TCP Reno performance: a simple model and its empirical validation. IEEE/ACM Trans. Netw. 8, 133–145 (2000)
27. Philip, A.: Slow internet? how to figure out if it’s your problem or your service provider’s. https://www.azcentral.com/story/news/local/arizona-investigations/2018/09/06/your-internet-slow-heres-how-figure-out-whos-fault/1058007002/
28. Ribeiro, V.J., Riedi, R.H., Baraniuk, R.G., Navratil, J., Cottrell, L.: pathChirp: efficient available bandwidth estimation for network paths (2003)
29. Sivel. Cloudflare-cli. https://github.com/KNawm/speed-cloudflare-cli
30. Sivel. Fast-cli. https://github.com/sindresorhus/fast-cli
31. Sivel. Speedtest-cli. https://github.com/sivel/speedtest-cli
32. Sommers, J., Durairajan, R., Barford, P.: Automatic metadata generation for active measurement. In: Proceedings of the ACM IMC (2017)
33. Strauss, J., Katabi, D., Kaashoek, F.: A measurement study of available bandwidth estimation tools. In: Proceedings of the ACM IMC (2013)
34. Sundaresan, S., de Donato, W., Feamster, N., Teixeira, R., Crawford, S., Pescapé, A.: Broadband Internet performance: a view from the gateway. In: Proceedings of the ACM SIGCOMM (2011)
35. Sundaresan, S., Lee, D., Deng, X., Feng, Y., Dhamdhere, A.: Challenges in inferring internet congestion using throughput measurements. In: Proceedings of the ACM IMC (2017)
36. The Chromium Projects. NetLog: Chrome’s network logging system. https://www.chromium.org/developers/design-documents/network-stack/netlog
37. The Chromium Projects. The trace event profiling tool. https://www.chromium.org/developers/how-tos/trace-event-profiling-tool
38. The Office of the New York State Attorney General. Are you getting the internet speeds you are paying for? https://ag.ny.gov/SpeedTest
39. W3C. Resource Timing Level 2. https://www.w3.org/TR/resource-timing-2/. Accessed 26 June 2021
40. M. web docs. Cross-origin resource sharing (CORS). https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS#Preflighted_requests. Accessed 23 Feb 2019
41. WHATWG. XMLHttpRequest Living Standard. https://xhr.spec.whatwg.org
42. Xu, D., et al.: Understanding operational 5G: a first measurement study on its coverage, performance and energy consumption. In: ACM SIGCOMM, Virtual Event, NY, USA (2020)
43. Yang, X., et al.: Fast and light bandwidth testing for internet users. In: USENIX NSDI, Virtual Event (2021)