title:The Secret Sharer: Evaluating and Testing Unintended Memorization
in Neural Networks
author:Nicholas Carlini and
Chang Liu and
&apos;Ulfar Erlingsson and
Jernej Kos and
Dawn Song
The Secret Sharer: Evaluating and Testing 
Unintended Memorization in Neural Networks
Nicholas Carlini, Google Brain; Chang Liu, University of California, Berkeley; 
Úlfar Erlingsson, Google Brain; Jernej Kos, National University of Singapore; 
Dawn Song, University of California, Berkeley
https://www.usenix.org/conference/usenixsecurity19/presentation/carlini
This paper is included in the Proceedings of the 28th USENIX Security Symposium.August 14–16, 2019 • Santa Clara, CA, USA978-1-939133-06-9Open access to the Proceedings of the 28th USENIX Security Symposium is sponsored by USENIX.The Secret Sharer: Evaluating and Testing
Unintended Memorization in Neural Networks
Nicholas Carlini1,2
Chang Liu2
Úlfar Erlingsson1
Jernej Kos3
Dawn Song2
1Google Brain 2University of California, Berkeley 3National University of Singapore
Abstract
This paper describes a testing methodology for quantita-
tively assessing the risk that rare or unique training-data
sequences are unintentionally memorized by generative se-
quence models—a common type of machine-learning model.
Because such models are sometimes trained on sensitive data
(e.g., the text of users’ private messages), this methodology
can beneﬁt privacy by allowing deep-learning practitioners to
select means of training that minimize such memorization.
In experiments, we show that unintended memorization is
a persistent, hard-to-avoid issue that can have serious conse-
quences. Speciﬁcally, for models trained without considera-
tion of memorization, we describe new, efﬁcient procedures
that can extract unique, secret sequences, such as credit card
numbers. We show that our testing strategy is a practical and
easy-to-use ﬁrst line of defense, e.g., by describing its ap-
plication to quantitatively limit data exposure in Google’s
Smart Compose, a commercial text-completion neural net-
work trained on millions of users’ email messages.
1 Introduction
When a secret is shared, it can be very difﬁcult to prevent its
further disclosure—as artfully explored in Joseph Conrad’s
The Secret Sharer [9]. This difﬁculty also arises in machine-
learning models based on neural networks, which are being
rapidly adopted for many purposes. What details those models
may have unintentionally memorized and may disclose can
be of signiﬁcant concern, especially when models are public
and models’ training involves sensitive or private data.
Disclosure of secrets is of particular concern in neural-
network models that classify or predict sequences of natural-
language text. First, such text will often contain sensitive or
private sequences, accidentally, even if the text is supposedly
public. Second, such models are designed to learn text pat-
terns such as grammar, turns of phrase, and spelling, which
comprise a vanishing fraction of the exponential space of
all possible sequences. Therefore, even if sensitive or pri-
vate training-data text is very rare, one should assume that
well-trained models have paid attention to its precise details.
Concretely, disclosure of secrets may arise naturally in gen-
erative text models like those used for text auto-completion
and predictive keyboards, if trained on possibly-sensitive data.
The users of such models may discover—either by accident
or on purpose—that entering certain text preﬁxes causes the
models to output surprisingly-revealing text completions. For
example, users may ﬁnd that the input “my social-security
number is. . . ” gets auto-completed to an obvious secret (such
as a valid-looking SSN not their own), or ﬁnd that other in-
puts are auto-completed to text with oddly-speciﬁc details. So
triggered, unscrupulous or curious users may start to “attack”
such models by entering different input preﬁxes to try to mine
possibly-secret sufﬁxes. Therefore, for generative text mod-
els, assessing and reducing the chances that secrets may be
disclosed in this manner is a key practical concern.
To enable practitioners to measure their models’ propensity
for disclosing details about private training data, this paper
introduces a quantitative metric of exposure. This metric can
be applied during training as part of a testing methodology
that empirically measures a model’s potential for unintended
memorization of unique or rare sequences in the training data.
Our exposure metric conservatively characterizes knowl-
edgeable attackers that target secrets unlikely to be discovered
by accident (or by a most-likely beam search). As validation
of this, we describe an algorithm guided by the exposure met-
ric that, given a pretrained model, can efﬁciently extract secret
sequences even when the model considers parts of them to be
highly unlikely. We demonstrate our algorithm’s effectiveness
in experiments, e.g., by extracting credit card numbers from a
language model trained on the Enron email data. Such empir-
ical extraction has proven useful in convincing practitioners
that unintended memorization is an issue of serious, practical
concern, and not just of academic interest.
Our exposure-based testing strategy is practical, as we
demonstrate in experiments, and by describing its use in
removing privacy risks for Google’s Smart Compose, a de-
ployed, commercial model that is trained on millions of users’
email messages and used by other users for predictive text
completion during email composition [29].
In evaluating our exposure metric, we ﬁnd unintended mem-
orization to be both commonplace and hard to prevent. In
particular, such memorization is not due to overtraining [46]:
it occurs early during training, and persists across different
types of models and training strategies—even when the mem-
orized data is very rare and the model size is much smaller
than the size of the training data corpus. Furthermore, we
show that simple, intuitive regularization approaches such
as early-stopping and dropout are insufﬁcient to prevent un-
intended memorization. Only by using differentially-private
training techniques are we able to eliminate the issue com-
pletely, albeit at some loss in utility.
USENIX Association
28th USENIX Security Symposium    267
2 Background: Neural Networks
First, we provide a brief overview of the necessary technical
background for neural networks and sequence models.
2.1 Concepts, Notation, and Training
A neural network is a parameterized function fθ(·) that is de-
signed to approximate an arbitrary function. Neural networks
are most often used when it is difﬁcult to explicitly formulate
how a function should be computed, but what to compute
can be effectively speciﬁed with examples, known as training
data. The architecture of the network is the general structure
of the computation, while the parameters (or weights) are the
concrete internal values θ used to compute the function.
We use standard notation [20]. Given a training set X =
{(xi,yi)}m
i=1 consisting of m examples xi and labels yi, the pro-
cess of training teaches the neural network to map each given
example to its corresponding label. We train by performing
(non-linear) gradient descent with respect to the parameters
θ on a loss function that measures how close the network is
to correctly classifying each input. The most commonly used
loss function is cross-entropy loss: given distributions p and
q we have H(p,q) = −∑z p(z)log(q(z)), with per-example
loss L(x,y,θ) = H( fθ(x),y) for fθ.
During training, we ﬁrst sample a random minibatch B
consisting of labeled training examples {( ¯x j, ¯y j)}m(cid:48)
j=1 drawn
from X (where m(cid:48) is the batch size; often between 32 and
1024). Gradient descent then updates the weights θ of the
neural network by setting
θnew ← θold − η 1
m(cid:48)
∇θL( ¯x j, ¯y j,θ)
m(cid:48)
∑
j=1
That is, we adjust the weights η-far in the direction that mini-
mizes the loss of the network on this batch B using the current
weights θold. Here, η is called the learning rate.
In order to reach maximum accuracy (i.e., minimum loss),
it is often necessary to train multiple times over the entire set
of training data X , with each such iteration called one epoch.
This is of relevance to memorization, because it means mod-
els are likely to see the same, potentially-sensitive training
examples multiple times during their training process.
2.2 Generative Sequence Models
A generative sequence model is a fundamental architecture
for common tasks such as language-modeling [4], translation
[3], dialogue systems, caption generation, optical character
recognition, and automatic speech recognition, among others.
For example, consider the task of modeling natural-
language English text from the space of all possible sequences
of English words. For this purpose, a generative sequence
model would assign probabilities to words based on the con-
text in which those words appeared in the empirical distri-
bution of the model’s training data. For example, the model
Figure 1: Results of our testing methodology applied to a state-
of-the-art, word-level neural-network language model [35].
Two models are trained to near-identical accuracy using two
different training strategies (hyperparameters A and B). The
models differ signiﬁcantly in how they memorize a randomly-
chosen canary word sequence. Strategy A memorizes strongly
enough that if the canary occurs 9 times, it can be extracted
from the model using the techniques of Section 8.
Threat Model and Testing Methodology. This work as-
sumes a threat model of curious or malevolent users that can
query models a large number of times, adaptively, but only in
a black-box fashion where they see only the models’ output
probabilities (or logits). Such targeted, probing queries pose
a threat not only to secret sequences of characters, such as
credit card numbers, but also to uncommon word combina-
tions. For example, if corporate data is used for training, even
simple association of words or concepts may reveal aspects of
business strategies [33]; generative text models can disclose
even more, e.g., auto completing “splay-ﬂexed brace columns”
with the text “using pan traps at both maiden apexes of the
jimjoints,” possibly revealing industrial trade secrets [6].
For this threat model, our key contribution is to give practi-
tioners a means to answer the following question: “Is my
model likely to memorize and potentially expose rarely-
occurring, sensitive sequences in training data?” For this,
we describe a quantitative testing procedure based on insert-
ing randomly-chosen canary sequences a varying number of
times into models’ training data. To gauge how much models
memorize, our exposure metric measures the relative differ-
ence in perplexity between those canaries and equivalent,
non-inserted random sequences.
Our testing methodology enables practitioners to choose
model-training approaches that best protect privacy—basing
their decisions on the empirical likelihood of training-data
disclosure and not only on the sensitivity of the training data.
Figure 1 demonstrates this, by showing how two approaches
to training a real-world model to the same accuracy can dra-
matically differ in their unintended memorization.
268    28th USENIX Security Symposium
USENIX Association
02468Repetitions of canary in training data51015202530Canary exposure in trained modelHyperparameters AHyperparameters Bmight assign the token “lamb” a high probability after seeing
the sequence of words “Mary had a little”, and the token “the”
a low probability because—although “the” is a very common
word—this preﬁx of words requires a noun to come next, to
ﬁt the distribution of natural, valid English.
Formally, generative sequence models are designed to gen-
erate a sequence of tokens x1...xn according to an (unknown)
distribution Pr(x1...xn). Generative sequence models estimate
this distribution, which can be decomposed through Bayes’
i=1Pr(xi|x1...xi−1). Each individual
rule as Pr(x1...xn) = Πn
computation Pr(xi|x1...xi−1) represents the probability of to-
ken xi occurring at timestep i with previous tokens x1 to xi−1.
Modern generative sequence models most frequently em-
ploy neural networks to estimate each conditional distribution.
To do this, a neural network is trained (using gradient de-
scent to update the neural-network weights θ) to output the
conditional probability distribution over output tokens, given
input tokens x1 to xi−1, that maximizes the likelihood of the
training-data text corpus. For such models, Pr(xi|x1...xi−1)
is deﬁned as the probability of the token xi as returned by
evaluating the neural network fθ(x1...xi−1).
Neural-network generative sequence models most often
use model architectures that can be naturally evaluated on
variable-length inputs, such as Recurrent Neural Networks
(RNNs). RNNs are evaluated using a current token (e.g., word
or character) and a current state, and output a predicted next
token as well as an updated state. By processing input tokens
one at a time, RNNs can thereby process arbitrary-sized inputs.
In this paper we use LSTMs [23] or qRNNs [5].
Figure 2: Overtraining.
2.3 Overﬁtting in Machine Learning
Overﬁtting is one of
the core difﬁculties in
machine learning. It is
much easier to produce
a classiﬁer that can per-
fectly label the training
data than a classiﬁer that
generalizes to correctly
label new, previously un-
seen data.
Because of this, when
constructing a machine-
learning classiﬁer, data is partitioned into three sets: train-
ing data, used to train the classiﬁer; validation data, used to
measure the accuracy of the classiﬁer during training; and
test data, used only once to evaluate the accuracy of a ﬁnal
classiﬁer. By measuring the “training loss” and “testing loss”
averaged across the entire training or test inputs, this allows
detecting when overﬁtting has occurred due to overtraining,
i.e., training for too many steps [46].
Figure 2 shows a typical example of the problem of over-
training (here the result of training a large language model on
a small dataset, which quickly causes overﬁtting). As shown
in the ﬁgure, training loss decreases monotonically; however,
validation loss only decreases initially. Once the model has
overﬁt the training data (at epoch 16), the validation loss
begins to increase. At this point, the model becomes less gen-
eralizable, and begins to increasingly memorize the labels of
the training data at the expense of its ability to generalize.
In the remainder of this paper we avoid the use of the word
“overﬁtting” in favor of the word “overtraining” to make ex-
plicit that we mean this eventual point at which validation loss
stops decreasing. None of our results are due to overtraining.
Instead, our experiments show that uncommon, random train-
ing data is memorized throughout learning and (signiﬁcantly
so) long before models reach maximum utility.
3 Do Neural Nets Unintentionally Memorize?
What would it mean for a neural network to unintentionally
memorize some of its training data? Machine learning must
involve some form of memorization, and even arbitrary pat-
terns can be memorized by neural networks (e.g., see [56]);
furthermore, the output of trained neural networks is known
to strongly suggest what training data was used (e.g., see the
membership oracle work of [41]). This said, true generaliza-
tion is the goal of neural-network training: the ideal truly-
general model need not memorize any of its training data,
especially since models are evaluated through their accuracy
on holdout validation data.
Unintended Memorization: The above suggests a simple
deﬁnition: unintended memorization occurs when trained neu-
ral networks may reveal the presence of out-of-distribution
training data—i.e., training data that is irrelevant to the learn-
ing task and deﬁnitely unhelpful to improving model accuracy.
Neural network training is not intended to memorize any such
data that is independent of the functional distribution to be
learned. In this paper, we call such data secrets, and our test-
ing methodology is based on artiﬁcially creating such secrets
(by drawing independent, random sequences from the input
domain), inserting them as canaries into the training data,
and evaluating their exposure in the trained model. When we
refer to memorization without qualiﬁcation, we speciﬁcally
are referring to this type of unintended memorization.
Motivating Example: To begin, we motivate our study with
a simple example that may be of practical concern (as
brieﬂy discussed earlier). Consider a generative sequence
model trained on a text dataset used for automated sentence
completion—e.g., such one that might be used in a text-
composition assistant. Ideally, even if the training data con-
tained rare-but-sensitive information about some individual
users, the neural network would not memorize this informa-
tion and would never emit it as a sentence completion. In
particular, if the training data happened to contain text written
by User A with the preﬁx “My social security number is ...”,
USENIX Association
28th USENIX Security Symposium    269
02040Epochs1.01.52.02.53.0Cross-Entropy LossValidation LossTraining Lossone would hope that the exact number in the sufﬁx of User
A’s text would not be predicted as the most-likely completion,
e.g., if User B were to type that text preﬁx.
Unfortunately, we show that training of neural networks
can cause exactly this to occur, unless great care is taken.
To make this example very concrete, the next few para-
graphs describe the results of an experiment with a character-
level language model that predicts the next character given a
prior sequence of characters [4, 36]. Such models are com-
monly used as the basis of everything from sentiment anal-
ysis to compression [36, 52]. As one of the cornerstones of
language understanding, it is a representative case study for
generative modeling. (Later, in Section 6.4, more elaborate