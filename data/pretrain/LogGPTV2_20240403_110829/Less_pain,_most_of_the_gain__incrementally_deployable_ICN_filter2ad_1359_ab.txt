or forwarded to a remote location.
Figure 1 visually conﬁrms that request popularity is heavy-tailed
and close to a Zipﬁan distribution; each curve is almost linear on a
log-log plot. While the speciﬁc exponents and y-intercepts do vary
slightly across locations and content types, the main takeaway is
that object requests are reasonably approximated by heavy-tailed
Zipﬁan distributions. Table 2 summarizes the Zipf-ﬁt parameters
for the three locations that we use to guide our simulation study.
Why does Zipf matter? Anecdotal evidence suggests that in the
presence of Zipf workloads, having multiple caching layers or co-
operative caching provide limited improvements [6, 52]. To under-
stand this better, we begin with a simple analysis on a binary tree
topology. We use an analytical optimization model to reason about
the optimal cache management scheme—the best static placement
of objects across the tree nodes given a Zipﬁan workload. The
workload is a collection of requests, each arriving at a leaf of the
tree chosen at random. Given a request, as long as the current node
does not have the object, the request is forwarded to the parent
node. The root is assumed to host all objects. As a simplifying
assumption, we assume all caches are of the same size.2
A tree is small enough to be amenable to such analysis. At the
same time a tree is instructive because from the view of a content
origin server, the distribution topology is effectively a tree.
Figure 2: Utility of different cache levels with a simpliﬁed optimiza-
tion model on a binary tree with 6 levels. Level 6 here is the origin
server to which requests are sent on cache misses.
Figure 2 shows the fraction of requests served at each level of
the tree for different request distributions. Here, level 6 denotes the
origin server. We see that the intermediary levels of the tree (i.e.,
levels 2–6) add little value beyond caching at the edge or satisfying
the request at the origin. Consider the setting with α = 0.7. In
this case, the expected number of hops that a request traverses is
0.4 × 1 + . . . + 0.18 × 6 ≈ 3. Now, let us look at an extreme
scenario where we have no caches at the intermediate levels; i.e.,
all of the requests currently assigned to levels 2–6 will be served at
the origin. In this case, the expected number of hops will be 0.4 ×
1+0.6×6 = 4. In other words, the latency improvement attributed
to universal caching is only 25%. Note that this is actually unfair
to the edge caching approach, as it only has half the total cache
capacity.
We also extended this optimization-driven analysis with another
degree of freedom, where we also vary the sizes of the cache al-
located to different locations. The results showed that the optimal
solution under a Zipf workload involves assigning a majority of the
total caching budget to the leaves of the tree. (We do not show the
detailed results due to space limitations.)
The above reconﬁrmation that request workloads are Zipf and
our simple tree-based intuition motivate us to evaluate to what ex-
tent pervasive caching and nearest-replica lookup are really neces-
sary to achieve the quantitative beneﬁts of ICN.
2We do not show the full formulation for brevity. The high-level
idea is to solve the problem of deciding where to cache speciﬁc
objects and how to assign requests to different caches to minimize
the expected latency (i.e., number of hops traversed by requests) as
an integer linear program.
3. DESIGN SPACE FOR CACHING
The measurements and simpliﬁed analysis from the previous sec-
tion raise the question of whether pervasive caching and nearest-
replica routing are strictly necessary. We do not claim novelty for
 0 0.2 0.4 0.6 0.8 1 1 2 3 4 5 6Fraction of requests servedCache levelα = 0.7α = 1.1α = 1.5149Figure 3: Example of two cache placement strategies: caches
placed at select network locations such as at the edge of the net-
work or pervasively throughout the network. The shaded nodes are
routers augmented with content caches while the others are tradi-
tional IP routers.
the general observation that ubiquitous caching may have limited
impact with Zipﬁan distributions. Our speciﬁc contribution here
lies in providing a detailed analysis of caching in an ICN-speciﬁc
context, which involves an entire network of caches and name-
based forwarding, and comparing it with more easily deployable
alternatives.
Given the diversity of ICN proposals, we want to avoid tightly
coupling our analysis to any speciﬁc architecture. To this end, we
consider a broad design space of caching infrastructures character-
ized by two high-level dimensions:
1. Cache placement: The ﬁrst dimension of interest is where
caches are located in the network. From the perspective of the
origin server serving content to users, the network looks like
a tree of routers/caches. Figure 3 depicts two possible strate-
gies in this distribution tree. At one extreme, every network
router is also a content cache. Alternatively, we can envision
caches deployed close to the network edge. We can also con-
sider intermediate placement solutions; e.g., due to economic
constraints operators may only install caches at locations that
serve to sufﬁciently large populations [29]. A related question
here is provisioning the compute and storage capacity of the
various caches. For instance, we can consider a network where
all caches have the same capacity or make the caches propor-
tionally higher for nodes serving larger populations.
2. Request routing: An orthogonal dimension to placement is
how content requests are routed through the network. As rep-
resentative samples, we consider two design points in Figure 4.
In this example, a request for the object C arrives at node R4.
The origin server and possibly some other nodes have copies
of C. In the ﬁrst case, a request is routed along the tree toward
the origin server until it ﬁnds a node with the desired content.
In the second case, we assume that the network routes the re-
quest based on the name toward the closest replica. We can
also consider intermediate strategies. For instance, we can con-
sider cooperative caching within a small search scope to look
up nearby nodes and reverting to shortest-path routing toward
the origin if these lookups fail.
In this paper, we are less concerned with the discovery protocols
used to populate content routing tables [23] or the feasibility of
name-based lookup in high-speed routers [34]. Since our goal is
to evaluate the potential beneﬁts of pervasive caching and nearest-
replica routing, we conservatively assume that routing and lookup
have zero cost.
There is possibly a third aspect of cache resource management.
Given that prior work (e.g., [39]) and our own experiments show
Figure 4: Example of two request routing strategies: requests are
routed along the shortest path to the origin server and served from
some available content cache along that path or the requests are
routed to the nearest cached copy (e.g., ICN).
that the LRU policy performs near-optimally in practical scenarios,
we use LRU for the rest of this paper. We also tried LFU, which
yielded qualitatively similar results.
4. BENEFITS OF CACHING
In this section, we use simulations to analyze the relative per-
formance of different caching architectures with respect to three
key metrics: (1) response latency; (2) network congestion; and (3)
server load.
Figure 5: An example network topology with four PoP nodes and
their corresponding access trees.
4.1 Setup
We use PoP-level network topologies from educational back-
bones and Rocketfuel [43]. From each PoP-level topology (core
network), we create its corresponding router-level topology by con-
sidering each PoP as the root of a complete k-ary tree [43]. We
refer to this as the access tree. The baseline results presented in
this section use k=2 and set the depth of each access tree to 5. We
study the sensitivity of the results to these parameters in Section 5.
Figure 5 shows an example network topology with four PoPs. We
annotate each PoP with the population of its associated metro re-
gion and assume that the requests at each PoP are proportional to
its population. We assume a homogeneous request stream where
requests at different network locations are drawn from the same
object popularity distribution—we analyze the effect of popularity
spatial skew in the next section.
Requests arrive at the leaves of each access tree. Within each
PoP, the requests arrive uniformly at random at one of the leaf nodes
of that access tree. Each PoP additionally serves as an origin server
for a subset of the set of entire objects; the number of objects it
hosts is also proportional to the population.3 We assume that each
3We also experimented with other models such as uniform origin
assignment and found consistent results.
Origin	
  Server	
  R1 R3 R4 R2 R5 R7 R6 Origin	
  Server	
  R1 R3 R4 R2 R5 R7 R6 Content Requests Content Requests Cache only at Edge Pervasive Caching Origin	
  Server	
  R1 R3 R4 R2 R5 R7 R6 Shortest path to Origin Nearest-replica Routing Origin	
  Server	
  R1 R3 R4 R2 R5 R7 R6 C	
  C	
  C	
  C	
  Rqst C Rqst C 150cache has sufﬁcient budget (i.e., storage capacity) to host a certain
number of objects. We use different budget conﬁgurations; e.g.,
uniform or proportional to the population. Note that a PoP node
serves two roles: (1) as the root node of an access tree, and (2) as
the origin server for a set of objects. As a regular cache, we assume
the PoP node has a ﬁxed budget, but as an origin server, we assume
it has a very large cache to host all the objects it “owns”.
Representative designs: We choose four representative designs
from the design space described in Section 3:
• ICN-SP: This assumes pervasive cache placement and shortest
path routing toward the origin server. That is, any cache along
the shortest path may respond to the request if it has the object.
• ICN-NR: This extends ICN-SP with nearest-replica-based
routing. Our goal here is not to design new routing strategies
or evaluate the overhead of these content-based routing proto-
cols. We conservatively assume that we can ﬁnd and route to
the nearest replica with zero overhead.
• EDGE: This is the simplest strategy where we only place
caches at the “edge” of the network. The notion of “edge” de-
pends on other economic and management-related factors and
whether it is viable to operate caches deep inside the network.
We use edge to represent the leaves of our access topology since
our goal is to do a relative comparison between the different
schemes.
• EDGE-Coop: This uses the same placement as EDGE, but
with a simple neighbor-based cooperative strategy. Each router
does a scoped lookup to check if its sibling in the access tree
has the object, and if so, reroutes the request to the sibling.
Cache provisioning: We consider two cache budgeting policies
for setting the cache size Br for each router r. If there are a total
of O objects being requested across the network of R routers, we
assume that the total cache budget of the network is F × R × O,
for some value of F ∈ [0, 1]. As a baseline, we pick F = 5%
based roughly on the CDN provisioning we observe relative to the
universe of objects each CDN server sees in a day. We vary the
budget parameter in the next section.
Given this total budget, we consider two possible splits:
1. Uniform: Each router r gets a ﬁxed cache capacity to store 5%
of the universe of all objects.
2. Population-proportional: We divide the total budget such that
each PoP gets a total budget proportional to its population and
then divide this budget equally within that access tree.
We have also tried other cache budgeting policies and observed
results that are qualitatively consistent. Due to space constraints,
we do not report the results from those settings.
Note that this method of dividing the budget can be viewed as
unfair to the EDGE and EDGE-Coop settings as they have a total
budget that (for binary trees) is half the capacity of the ICN-SP and
ICN-NR cases. Thus, we also consider a new representative design
EDGE-Norm where we ensure that the total budgets are the same.
That is, we take the EDGE conﬁguration and multiply the budget
of the edge caches by an appropriate constant (for example, 2 in
case of binary trees) to make sure that the total cache capacity is
the same across different representative designs.
All representative designs use LRU for cache management. Each
node on the response path, which starts at the node that the re-
quested object is found (the origin server or a cache) and ends at
the leaf at which the request has arrived, stores the object in addi-
tion to forwarding it towards the client.
For reasons of scalability, we use a request-level simulator and
thus we do not model packet-level, TCP, or router queueing effects.
Since our goal is to understand the relative performance of the dif-
ferent caching architectures at a request granularity, we believe this
is a reasonable assumption. We optimistically assume that ICN-SP
and ICN-NR solutions incur no lookup or discovery overhead when
modeling the response latencies and network congestion.
Having described the simulation setup, we present the baseline
results in the next sub-section.
4.2 Baseline Results
We use trace-driven simulations using the CDN request logs and
corresponding synthetic request logs, which have similar numbers
of requests, objects, and the best-ﬁt Zipf popularity distribution.
For this section, we use the Asia trace from the CDN. We as-
sume that this trace is the universe of all requests. We assign each
request to a PoP with a probability proportional to the correspond-
ing PoP’s population. (We vary the popularity skew across PoPs
in Section 5.) Within each PoP, requests are uniformly distributed
among the leaves.
For the following results, we report normalized metrics w.r.t. to
a system without any caching infrastructure. Thus, we focus on the
improvement in response latency, reduction in network congestion,
and reduction in server load. In each case, a higher value of the
metric implies that caching is more beneﬁcial.
Response latency: We report response latency in terms of the
number of hops between the request and the location from which
it was served. Figure 6(a) shows the percentage improvement in
latency for the four caching architectures (plus EDGE-Norm) in
comparison with a network with no caching (i.e., all requests are
routed to the origin PoP). We make three main observations. First,
the gap between the different caching architectures is quite small
(at most 9%); this is consistent across the different topologies.
Second, EDGE-Coop consistently achieves comparable latency im-
provement relative to ICN-NR with a maximum gap of 3%. Third,
nearest replica routing (ICN-NR) does not offer signiﬁcant beneﬁts
over ICN-SP.
Figure 7(a) shows the latency improvements for the case of uni-
form budget assignment across PoPs. We see no major change in
the relative performances of the different architectures.
Network congestion: Other parallel work has focused on the in-
teraction between ISP trafﬁc engineering and “content engineer-
ing” and showed that there are natural synergies to be exploited
here [25,35]. Here, we focus on a simpler question of network con-
gestion under different caching architectures. The congestion on a
link is measured simply as the number of object transfers traversing
that link.
Figure 6(b) shows the effectiveness of caching in reducing the
congestion level across the network. We focus on the maximum
congested link in the network. Analogous to the query delay anal-
ysis, the percentage shown in each case indicates the improvement
over the base case with zero budget. Once again, we see that
EDGE-Coop delivers close to the best performance (with a max-
imum gap of 4%) and that the gap between the solutions is fairly
small.4 The success of edge-based approaches in this context is