show, analyzing these samples with SQUEEZE revealed a signiﬁ-
cant number of C&C servers that were not observed in Anubis.
To detect C&C trafﬁc, we use a set of 192 network signatures
provided to us by a security company. These signatures are similar
in expressiveness to Snort rules [29], and include regular expres-
sions that are to be matched against network ﬂows. These signa-
tures have been manually vetted for accuracy, and do not in our
experience lead to false positives when matched against Anubis
trafﬁc. However, they may not cover the full spectrum of current
malware C&C communication.
5.1 Dataset
By matching the C&C signatures against trafﬁc dumps generated
by Anubis analysis runs, we identify a set of samples that generated
C&C trafﬁc, together with the C&C endpoints they connected to.
For our evaluation, we then selected a subset of these samples based
on two criteria: First of all, we required recent samples, since older
samples are often no longer functional because they do not contain
up-to-date information for contacting C&C infrastructure. Second,
we would like to test our tool on a dataset involving the largest
possible set of C&C endpoints. Thus, we tried to contact each C&C
endpoint and discarded those that were no longer reachable. This
includes domains that failed to resolve to an IP address and servers
that were no longer reachable on the port used for C&C. For each
of the remaining endpoints, we selected the most recent samples
that communicated with it.
In total, we selected 8,346 malware samples. To assess the di-
versity of this dataset, we scanned the samples with the Kaspersky
anti-virus engine. Kaspersky provided a total of 2,225 different
AV labels for these samples. Kaspersky virus labels have a loosely
structured format, such as “Trojan-Spy.Win32.Zbot.aebi”.
By discarding the last part of these labels (indicating a speciﬁc vari-
ant), we can obtain a more coarse-grained classiﬁcation of malware
binaries into families. Our dataset contained 213 malware families
according to this classiﬁcation, with the most represented ten fam-
ilies accounting for only 21% of the dataset. Comparing these la-
bels with a recent list of the most prevalent malware families [30]
shows that fourteen of the twenty top malware families are repre-
sented in our dataset.4 This conﬁrms that we are testing our tool on
a diverse and representative malware dataset. Notable exceptions
(top 20 malware families not seen in our dataset) are Conﬁcker and
Storm 2.0, which are P2P botnets and are therefore not covered by
our signatures for client-server C&C activity.
Setup. To run SQUEEZE we used four virtual machine instances on
a host with 32GB memory and an Intel XEON E5420 CPU. 4GB
of memory was assigned to each machine of which 3GB were ded-
icated to storing snapshots. Since snapshots are on average 130MB
in size, this allows for approximately 21 snapshots to be taken. We
run each malware sample for up to 6 minutes. Our setup can there-
fore analyze about one thousand samples a day. We ran our eval-
4A ﬁfteenth family, the Bredolab botnet, is absent from our dataset
because it was successfully taken down shortly after the report was
published [31].
(cid:1)(cid:2)
uation for 10 days in March 2011. Since our setup did not allow
us to deploy the two strategies in parallel, the system was conﬁg-
ured to use exploration strategy A during the ﬁrst ﬁve days, and
strategy B during the last ﬁve days. Our evaluation dataset is there-
fore split into Dataset A, consisting of 4,013 samples analyzed with
SQUEEZE using strategy A, and Dataset B, consisting of 4,333 sam-
ples analyzed using strategy B. We used a new dataset for strategy
B, rather than repeat analysis on Dataset A, to ensure we were test-
ing SQUEEZE against a “fresh” selection of malware samples.
5.2 Malware Behavior
Samples analyzed
Initial C&C knowledge
match
No further activity
Substantial delay skipping
New endpoints
New endpoints in bot com-
ponent
New endpoints with signa-
ture match
Strategy A Strategy B
4333
58%
4013
54%
44%
34%
25%
19%
9%
43%
31%
23%
13%
8%
Table 1: Malware behavior in SQUEEZE
Table 1 shows an overview of the behavior of malware sam-
ples when analyzed with SQUEEZE using the two proposed ex-
ploration strategies. The ﬁrst thing we notice is that a signiﬁcant
fraction of the analyzed samples never trigger SQUEEZE’s block-
ing mechanisms. With strategy A, only 54% of analyzed samples
ever try to contact a known C&C endpoint. Likewise, with strat-
egy B only 58% of analyzed samples generate trafﬁc that matches
our C&C signatures. This is despite the fact that all of the samples
in our dataset, when originally analyzed in Anubis, had performed
signature-matching trafﬁc that led us to detect an endpoint in the
ﬁrst place. We have found that in some cases the original binary
is actually a downloader: In the Anubis execution, it dropped a
remote-controlled bot, but in the SQUEEZE execution it dropped a
completely different payload. In other cases some of the endpoints
contacted by the sample were no longer available. One reason for
this is that, during this experiment, the delay between executing
a sample in Anubis and in SQUEEZE was too high (several days).
Results from our deployment (Section 5.5) show that this result can
be improved by reducing this delay.
A large share of the samples, 44% and 43% respectively for
strategies A and B, do not employ a backup strategy at all. That is,
after a C&C endpoint is blocked they show no further network ac-
tivity. The difference between these ﬁrst two rows of Table 1 leaves
10% and 15% of samples, respectively, where SQUEEZE may have
been able to trigger additional C&C activity. As can be seen in the
last row, 9% and 8% of samples respectively reveal new C&C end-
points when ran in SQUEEZE compared to those they revealed in
Anubis. These endpoints are conﬁrmed by a match against a C&C
signature.
There may be several reasons why, for the remaining samples,
no C&C trafﬁc can be detected. First of all, with the block-ﬁrst
strategy of approach A, C&C trafﬁc can only occur if the endpoint
is unblocked before the end of the analysis. Then, backup C&C
endpoints may not have been active during analysis. Finally, com-
munication with a backup C&C server might use a slightly different
format that cannot be detected by our C&C signatures. Therefore,
9% and 8% mark the lower bound of samples that revealed valuable
endpoint information. A reasonable upper bound can be given by
(cid:1)(cid:2)
also including other endpoints that the bot component connected
to. Recall that the bot component is deﬁned as the set of processes
that have performed C&C activity that matches our signatures or
known endpoints. This provides an upper bound of 19% and 13%
of samples respectively.
Furthermore, the results show that allowing connectivity checks
to go through is important for SQUEEZE’s effectiveness. Over ten
percent of the samples in our dataset performed connectivity checks
by contacting a variety of popular sites such as google.com, mi-
crosoft.com, or weather.yahoo.com. Some samples perform such
checks only at startup, while others test their connectivity again
whenever they fail to contact a C&C server.
Finally, our delay skipping techniques also play a signiﬁcant
role in SQUEEZE’s effectiveness. For around one third of samples,
SQUEEZE fast-forwards time in the sandbox by at least one minute.
In Section 5.4 we will discuss an example of a malware family that
reveals a signiﬁcant number of C&C endpoints, but only after skip-
ping a delay of over half an hour.
5.3 Endpoints
Table 2 shows the total number of distinct endpoints contacted
when executing the samples in our datasets in an unmodiﬁed Anu-
bis sandbox, as well as the additional endpoints contacted when
using SQUEEZE with the two alternative exploration strategies.
The most important results are the numbers of C&C endpoints
detected shown in the rows labeled “Endpoints with signature match”.
SQUEEZE is able to detect a signiﬁcant number of C&C endpoints
that were not contacted during the execution of any of the samples
in datasets A and B. SQUEEZE reveals 201 and 185 new C&C end-
points respectively using strategies A and B. This corresponds to
an increase in the number of C&C endpoints that can be extracted
from our two malware datasets of 12.6% and 19.7% respectively.
The number of IPs is signiﬁcantly lower than the number of do-
mains: Recall that we take into consideration an IP address only
if it is accessed directly rather than obtained through DNS resolu-
tion. While some malware samples contain hard-coded C&C IP
addresses, most rely on the DNS infrastructure to resolve the do-
main names of their backup C&C servers.
As discussed in the previous section, the number of endpoints
with a signature match is only a lower bound for the amount of
C&C endpoints we may have observed. This is chieﬂy because
our C&C signatures may not match on all of a sample’s C&C con-
nections, particularly if the format of messages to the primary C&C
server is not identical to the format used when communicating with
a backup server. As a rough upper bound for C&C endpoints we
can use all the endpoints contacted by the processes involved in
C&C communication (the bot component). SQUEEZE may there-
fore have revealed up to 714 and 506 new C&C endpoints respec-
tively using strategies A and B. This corresponds to an increase of
29.4% and 32.8% compared to unmodiﬁed Anubis.
To further investigate how many of the endpoints revealed by
SQUEEZE are actually malicious, we used nine publicly available
blacklists. For this, we queried these blacklists repeatedly until two
months after running the malware samples. The results are shown
in the row “Endpoints in blacklists”. Table 3 shows a breakdown of
these results for new endpoints (that is, all endpoints that were not
contacted during the baseline Anubis analysis). Of the blacklists
considered, only AMaDa focuses speciﬁcally on malware C&C
endpoints. Several of the other blacklists include C&C endpoints as
well as other malicious servers, while Google Safe Browsing and
Norton Safe Web focus on malicious web sites. We nonetheless
include these blacklists because miscreants may use endpoints for
multiple malicious purposes. Note that all of these blacklists are
meant to be deployed to block trafﬁc to or from malicious hosts,
and therefore strive to have extremely low false positive rates. The
Dataset A
Endpoints
Endpoints in bot component
Endpoints in blacklists
Endpoints with signature match
Dataset B
Endpoints
Endpoints in bot component
Endpoints in blacklists
Endpoints with signature match
Baseline
Strategy A
Domains
3562
2080
1970
1489
Domains
2627
1330
1336
885
IPs Total Domains
661
767
454
362
391
111
110
195
Strategy B
Baseline
IPs Total
1603
942
714
260
427
36
6
201
4329
2432
2081
1599
IPs Total Domains
534
364
293
211
81
353
184
53
2991
1541
1417
938
IPs Total
859
325
506
213
15
368
185
1
Table 2: New endpoints revealed by SQUEEZE compared to baseline (execution in Anubis sandbox)
Blacklist