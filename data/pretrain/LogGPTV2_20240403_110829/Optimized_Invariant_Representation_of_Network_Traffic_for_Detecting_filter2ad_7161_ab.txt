Target class
worms, exploits
web malware
botnet
botnets
dga malware
malware
web malware
malicious ﬂows
malicious ﬂows
phishing
malicious ﬂows
mw downloads
infected websites
mw downloads
malicious ﬂows
Type
packets
proxy logs
NetFlow
NefFlow
DNS data
executions
JavaScript
proxy logs
proxy logs
proxy logs
proxy logs
proxy logs
web pages
proxy logs
proxy logs
Testing Data
Year
2003
2003
2007
2011
2011
2007
2012
2009
2009
2011
2011
2012
2014
2014
2015
All samples
531,117
1,212,197
100,000k
78,000,000
360,700
4,591
20,918,798
72,000
1,000,000
12,193
2,000,000
1,219
386,018
N/A
15,379,466
Malicious Mal:All
ratio
N/A
1:100k
1:17
1:2.2M
1:45
1:1
1:112
1:2
1:100
1:1
1:333
1:4
1:8
N/A
1:355
samples
N/A
11
5,842k
36
8008
4,591
186,032
32,000
10,000
10,094
6,000
324
49,347
150
43,380
Table 1: Overview of the existing state-of-the-art approaches focusing on classiﬁcation of malicious trafﬁc (U = unsu-
pervised, S = supervised). In contrast to the existing work, our approach proposes novel and optimized representation
of bags, describing the dynamics of each legitimate or malicious sample. The approach is evaluated on latest real
datasets with a realistic ratio of malicious and background ﬂows (proxy log records).
was deployed on corporate networks and evaluated on
imbalanced datasets (see Table 1) as they appear in prac-
tice to show the expected efﬁcacy on these networks.
3 Formalization of the Problem
The paper deals with the problem of creating a robust
representation of network communication that would be
invariant against modiﬁcations an attacker can imple-
ment to evade the detection systems. The representa-
tion is used to classify network trafﬁc into positive (ma-
licious) or negative (legitimate) category. The labels for
positive and negative samples are often very expensive to
obtain. Moreover, sample distribution typically evolves
in time, so the probability distribution of training data
differs from the probability distribution of test data. This
complicates the training of classiﬁers which assume that
the distributions are the same. In the following, the prob-
lem is described in more detail.
Each sample is represented as an n-dimensional fea-
ture vector x ∈ Rn. Samples are grouped into bags, with
every bag represented as a matrix X = (x1, . . . ,x m) ∈
Rn×m, where m is the number of samples in the bag and
n is the number of features. The bags may have different
number of samples. A single category yi can be assigned
to each bag from the set Y = {y1, . . . ,y N}. Only a few
categories are included in the training set. The proba-
bility distribution on training and testing bags for cate-
gory y j will be denoted as PL(X|y j) and PT (X|y j), re-
spectively. Moreover, the probability distribution of the
training data differs from the probability distribution of
the testing data, i.e. there is a domain adaptation problem
[7] (also called a conditional shift [43]):
PL(X|y j) (cid:26)= PT (X|y j), ∀y j ∈ Y .
(1)
4
The purpose of the domain adaptation is to apply
knowledge acquired from the training (source) domain
into test (target) domain. The relation between PL(X|yi)
and PT (X|yi) is not arbitrary, otherwise it would not be
possible to transfer any knowledge. Therefore there is a
transformation τ, which transforms the feature values of
the bags onto a representation, in which PL(τ(X)|yi) ≈
PT (τ(X)|yi). The goal is to ﬁnd this representation, al-
lowing to classify individual bag represented as X into
categories Y = {y1, . . . ,y N} under the above mentioned
conditional shift.
Numerous methods for transfer learning have been
proposed (since the traditional machine learning meth-
ods cannot be used effectively in this case), including
kernel mean matching [14], kernel learning approaches
[11], maximum mean discrepancy [19], or boosting [10].
These methods try to solve a general data transfer with
relaxed conditions on the similarity of the distributions
during the transfer. The downside of these methods is
the necessity to specify the target loss function and avail-
ability of large amount of labeled data.
This paper proposes an effective invariant representa-
tion that solves the classiﬁcation problem with a covari-
ate shift (see Equation 1). Once the data are transformed,
the new feature values do not rely on the original distri-
bution and they are not inﬂuenced by the shift. The pa-
rameters of the representation are learned automatically
from the data together with the classiﬁer as a joint opti-
mization process. The advantage of this approach is that
the parameters are optimally chosen during training to
achieve the best classiﬁcation efﬁcacy for the given clas-
siﬁer, data, and representation.
810  25th USENIX Security Symposium 
USENIX Association
4
Invariant Representation
The problem of domain adaptation outlined in the pre-
vious section is addressed by the proposed representa-
tion of bags. The new representation is calculated with a
transformation that consists of three steps to ensure that
the new representation will be invariant under scaling
and shifting of the feature values and under permutation
and size changes of the bags.
4.1 Scale Invariance
As stated in Section 3, the probability distribution of bags
from the training set can be different from the test set. In
the ﬁrst step, the representation of bags is transformed
to be invariant under scaling of the feature values. The
traditional representation X of a bag that consists of a set
of m samples {x1, . . . ,x m} can be written in a form of a
matrix:
X =
x1
...
xm
 =
x11
x12
xm1
xm2
. . .
...
. . .
x1n
xmn
 ,
(2)
where xlk denotes k-th feature value of l-th sample. This
form of representation of samples and bags is widely
used in the research community, as it is straightforward
to use and easy to compute. It is a reasonable choice in
many applications with a negligible shift in the source
and target probability distributions. However, in the net-
work security domain, the dynamics of the network en-
vironment causes changes in the feature values and the
shift becomes more prominent. As a result, the perfor-
mance of the classiﬁcation algorithms using the tradi-
tional representation is decreased.
In the ﬁrst step, the representation is improved by
making the matrix X to be invariant under scaling of the
feature values. Scale invariance guarantees that even if
some original feature values of all samples in a bag are
multiplied by a common factor, the values in the new
representation remain unchanged. To guarantee the scale
invariance, the matrix X is scaled locally onto the interval
[0,1] as follows:
˜X =
˜x11
˜xm1
. . .
...
. . .
˜x1n
˜xmn
 ˜xlk =
xlk − minl(xlk)
maxl(xlk)− minl(xlk)
(3)
4.2 Shift Invariance
In the second step, the representation is transformed to
be invariant against shifting. Shift invariance guaranties
that even if some original feature values of all samples
in a bag are increased/decreased by a given amount, the
values in the new representation remain unchanged. Let
us deﬁne a translation invariant distance function d : R×
R → R for which the following holds: d(u,v) =d (u +
a,v + a).
Let xpk, xqk be k-th feature values of p-th and q-th
sample from bag matrix X. Then the distance between
these two values will be denoted as d(xpk,xqk) = sk
pq.
The distance d(xpk,xqk) is computed for pairs of k-th
feature value for all sample pairs, ultimately forming a
so called self-similarity matrix Sk. Self-similarity matrix
is a symmetric positive semideﬁnite matrix, where rows
and columns represent individual samples and (i, j)-th
element corresponds to the distance between i-th and j-
th sample. Self-similarity matrix has been already used
thanks to its properties in several applications (e.g. in
object recognition [21] or music recording [31]). How-
ever, only a single self-similarity matrix for each bag has
been used in these approaches. This paper proposes to
compute a set of similarity matrices, one for every fea-
ture. More speciﬁcally, a per-feature set of self-similarity
matrices S = {S1,S2, . . . ,S n} is computed for each bag,
where
Sk =
sk
11
sk
12
sk
m1
sk
m2
. . .
...
. . .
sk
1m
sk
mm
 .
(4)
The element sk
pq = d(xpk,xqk) is a distance between fea-
ture values xpk and xqk of k-th feature. This means that
the bag matrix X with m samples and n features will be
represented with n self-similarity matrices of size m×m.
The matrices are further normalized by local feature scal-
ing described in Section 4.1 to produce a set of matrices
˜S .
The shift invariance makes the representation robust
to the changes where the feature values are modiﬁed by
adding or subtracting a ﬁxed value. For example, the
length of a malicious URL would change by including
an additional subdirectory in the URL path. Or, the num-
ber of transfered bytes would increase when an addi-
tional data structure is included in the communication
exchange.
4.3 Permutation and Size Invariance
Representing bags with scaled matrices { ˜X} and sets of
locally-scaled self-similarity matrices { ˜S } achieves the
scale and shift invariance. Size invariance ensures that
the representation is invariant against the size of the bag.
In highly dynamic environments, the samples may occur
in a variable ordering. Permutation invariance ensures
that the representation should also be invariant against
any reordering of rows and columns of the matrices. The
ﬁnal step of the proposed transformation is the transi-
˜S (introduced in Sec-
tion from the scaled matrices ˜X,
USENIX Association  
25th USENIX Security Symposium  811
5
tions 4.1 and 4.2 respectively) to normalized histograms.
For this purpose, we deﬁne for each bag:
k := vector of values from k-th column of matrix ˜X
zX
zS
k
:=column-wise representation of upper triangular
matrix created from matrix ˜Sk ∈ ˜S .
This means that zX
k ∈ Rm is a vector created from val-
ues of k-th feature of ˜X, while zS
k ∈ Rr,r = (m− 1)· m
2
is a vector that consists of all values of upper triangular
matrix created from matrix ˜Sk. Since ˜Sk is a symmetric
k contains
matrix with zeros along the main diagonal, zS
only values from upper triangular matrix of ˜Sk.
A normalized histogram of vector z = (z1, . . . ,z d)∈ Rd
is a function φ : Rd ×Rb+1 → Rb parametrized by edges
of b bins θ = (θ0, . . . ,θ b) ∈ Rb+1 such that φ (z;θ ) =
(φ (z;θ0,θ1), . . . ,φ (z;θb−1,θb)) where
φ (z,θi,θi+1) =
1
d
d
∑
j=1
[[z j ∈ [θi−1,θi)]]
is the value of the i-th bin corresponding to a portion of
components of z falling to the interval [θi−1,θi).
Each column k of matrix ˜X (i.e. all bag values of k-th
feature) is transformed into a histogram φ (zX
k ,θ X
k ) with
predeﬁned number of b bins and θ X
k bin edges. Such his-
tograms created from the columns of matrix ˜X will be
denoted as feature values histograms, because they carry
information about the distribution of bag feature values.
On the other hand, histogram φ (zS
k) created from
values of self-similarity matrix ˜S j ∈ ˜S will be called fea-
ture differences histograms, as they capture inner feature
variability within bag samples.
k ,θ S
Overall, each bag is represented as a concatenated fea-
ture map φ ( ˜X;
(cid:31)φ (zX
˜S ;θ ): Rn×(m+r) → R2·n·b as follows:
n)(cid:30) (5)
1), . . . ,φ (zS
n ),φ (zS
1 ,θ S
n ,θ S
n ,θ X
1 ,θ X
1 ), . . . ,φ (zX
where n is the number of the original ﬂow-based fea-
tures, m is the number of ﬂows in the bag, and b is the
number of bins. The whole transformation from input
network ﬂows to the ﬁnal feature vector is depicted in
Figure 1. As you can see, two types of invariant his-
tograms are created from values of each ﬂow-based fea-
ture. At the end, both histograms are concatenated into
the ﬁnal bag representation φ ( ˜X;
˜S ;θ ).
5 Learning Optimal Histogram Represen-
tation
˜S ;θ ) proposed in Section 4
The bag representation φ ( ˜X;
has the invariant properties, however it heavily depends