HTTP [10, 13, 15]. We deﬁne a popular port as the port having more than 0.0015% out
of the total volume; the threshold 0.0015% is set to the expected volume per port if
trafﬁc is distributed evenly over the port numbers. For the source port case, there are
2, 496 popular ports, and they account for 95.97% of the total trafﬁc volume. Other than
the well-known port 53 is used by queries to the DNS servers in KAIST, we are not able
to map the popular port numbers to known applications only with the packet headers.
Fig. 4. Cumulative volume by the port number in k-2011
We apply the same analysis on destination ports. There are 594 popular ports and
they are responsible for 99.23% of total trafﬁc. The port 53,952 carries the largest per-
ﬂow volume of 5.42%. Compared to the source port case, only a quarter of destination
ports carry more trafﬁc. Out of 594 ports 546 has 99% of volume coming from single
nodes. Each of these nodes has a large number of ﬂows up to thousands with the same
destination port number but with different source port numbers. That is, a single des-
tination port use used for multiple downloads of heavy volume on a single node. We
investigate further the trafﬁc distribution by the host in the next section.
6
C. Lee, DK Lee, and S. Moon
3.2 Communication Patterns between Hosts
Our next interest is the communication pattern between hosts of UDP ﬂows. Analyz-
ing the communication patterns, we try to determine if the recent UDP trafﬁc comes
from peer-to-peer type (many-to-many) transfers or server-to-client type (one-to-many)
transfers. We take a similar methodology used in Karagiannis et al.’s work [12]. From
the ﬂow records, we ﬁrst count the number of unique campus IP addresses per off-
campus IP address. In the rest of this section we count only those ﬂows larger more than
100 KB in the analysis. The threshold of 100 KB is arbitrary, but insensitive enough to
exclude DNS and scanning trafﬁc and to capture bulk transfers. As shown in Figure 5(a),
most off-campus hosts (82.5% of the total) have only one corresponding host on cam-
pus, and the maximum number of corresponding hosts on campus is six. It means that
no popular UDP source host exists outside for hosts on campus, and the growth in UDP
trafﬁc is not attributed to a single or several numbers of UDP off-campus servers. On
the other hand, the same analysis on TCP trafﬁc shows that the most popular server has
sent trafﬁc to 203 on-campus IPs within an hour in the same trace.
(a) Off-campus host popularity
(b) Campus host popularity
Fig. 5. Communication pattern of UDP ﬂows
Figure 5(b) shows the number of unique off-campus IP addresses per campus IP
address in the ﬂow records, and the number goes up to more than hundred. Remember
that all ﬂows are larger than 100 KB here. That is, hosts on campus download UDP
trafﬁc from a large number of hosts outside. From the communications patterns by the
host and the port in Section 3.1 we conclude that most UDP trafﬁc is from peer-to-peer
transfers than server-to-client transfers.
4 Burstiness of UDP Trafﬁc
UDP trafﬁc has increased to take up almost 20% of the total link capacity on our very
congested link. If it is constant bit rate, not adaptive or responding to the network con-
gestion, it would be equal to decreasing the available bandwidth and have an unfair
share of bandwidth over TCP ﬂows. When UDP is relatively a negligible portion of the
overall trafﬁc, this unfair advantage is not very important. Now it is an issue.
We use the standard deviation in ﬂow throughput to ﬁrst see if UDP ﬂows are CBR or
not. We count the number of bytes delivered in a time unit of one second and calculate
Unmasking the Growing UDP Trafﬁc in a Campus Network
7
its standard deviation per ﬂow. We compute the same for TCP ﬂows to compare with.
Figure 6 shows the cumulative distributions of the standard deviation. As in the previous
section all ﬂows accounted for in this section are larger than 100 KB. In k-2008 most
UDP ﬂows have zero standard deviation. However, as time progresses to 2011, UDP
trafﬁc shows an increasing tendency of variability in throughput. By k-2011 about top
18% of both UDP and TCP ﬂows have the standard deviation greater than 1.6 Mbps.
The portion of UDP ﬂows with almost zero variability drops to less than 30%.
Fig. 6. Standard deviation of unit time throughput
Here we have looked at the throughput variability only in the time unit of a second
and on a per-ﬂow basis. One second is rather a long time for a router queue to buffer
packets in today’s Internet where most backbone links are 1 Gbps or higher: that is, too
coarse a time scale. How variable or bursty is the aggregate UDP trafﬁc in ﬁner time
scales? In Figure 7 we examine the burstiness of aggregate UDP trafﬁc in time units of
0.01 s, 0.1 s and 1 s. At the time scale of 0.01 s the trafﬁc looks more bursty than in the
other two scales, but the other two look similar.
(a) With 0.01s bin
(b) With 0.1s bin
(c) With 1s bin
Fig. 7. Burstiness in aggregate UDP trafﬁc from k-2011
Burstiness in trafﬁc has a great impact on router queue and end-host buffer size
provisioning. Self-similarity in Internet trafﬁc has long been reported and its causes
have been studied [23] A common technique to analyze the scaling behavior in trafﬁc
is the wavelet analysis and its energy plot [24]. The energy plot in Figure 8 shows the
variance of the wavelet coefﬁcients that reﬂects the variance of trafﬁc counting process
Xj at a time scale Tj. If the trafﬁc is self-similar, the plot should be a straight line,
of which slope is the scaling exponent. If the trafﬁc is Poisson, the plot should be a
horizontal line.
8
C. Lee, DK Lee, and S. Moon
Fig. 8. Wavelet energy plot for TCP and UDP trafﬁc from k-2011
TCP trafﬁc from k-2011 in Figure 8 shows almost a straight line, signifying that it is
close to self-similar. UDP trafﬁc on the other hand has a slight tip near j = 7 or the time
scale of 256 ms, where j = 1 is in 2 ms. The scaling exponent (or the Hurst parameter)
for TCP is 0.865 and for UDP 0.831. Multi-scaling behaviors on high-speed links and
similar dips in the time scale of hundreds of milliseconds have been reported [25]. We
have no basis to imply that two dips have a common cause and leave it for future work.
Burstiness in UDP trafﬁc does not instantly translate to use of congestion control
by the applications. MPEG-coded video can by itself be bursty. However, the commu-
nication patterns of UDP hosts imply peer-to-peer transfers and the latest version of
µtorrent, a popular client of BitTorrent, has announced the use of a proprietary congest
control mechanism [3]. Our experiment of a µtp transfer on a controlled node shows that
most data packets has a signature size of 1,466 bytes, and we have identiﬁed 26.8% of
total UDP trafﬁc volume in k-2011 to have the packet size. This is an upper bound as
we may have false positives in our classiﬁcation. The large volume of UDP ﬂows with
a proprietary congestion control contributes to the new kind of burstiness in today’s
Internet trafﬁc.
5 Conclusions and Discussion
In this work we have shown that UDP trafﬁc has increased 46-fold over past four years
on our campus network. Using packet header traces, we give a ﬁrst characterization
report on the growth. From the trace collected in 2011, we have found that large ﬂows
have become dominant in UDP just as in TCP. They are mostly from P2P applications,
and the aggregate UDP trafﬁc exhibits burstiness similar to TCP.
Our ﬁndings provide several guidelines to classifying UDP trafﬁc. First, port-number
based classiﬁcation can hardly work on recent UDP trafﬁc. Port numbers seem to be
randomly assigned to ﬂows. This is an opposite result to the previous work on TCP
trafﬁc [13]. However, a destination port, once assigned, is used for multiple downloads
from different hosts and ports, just as in TCP-based peer-to-peer applications. Thus
the communication patterns can be a clue as in [11, 12]. In addition, we have found
that certain UDP packet sizes, e.g., 1,466 bytes in k-2011, is observed more frequently
than others. Packet sizes of UDP packets can be a good signature in identifying UDP
applications. This is hardly the case for TCP since applications all work under TCP’s
policy.
Unmasking the Growing UDP Trafﬁc in a Campus Network
9
Our observation on UDP trafﬁc growth has implications to network simulation and
experiments. In previous network experiments with synthetic trafﬁc, UDP ﬂows have
been generated in a simple manner of constant bit rate and often ignored for their minor
volume. While this has been valid for traditional UDP trafﬁc, our measurements show
that the packet sending behavior is much more bursty than simple CBR. Our measure-
ment analysis underlines the rising need to account for “lower-than best effort” trafﬁc
in realistic network simulation.
Acknowledgements. This research was supported by the KCC (Korea Communica-
tions Commission), Korea, under the R&D program supervised by the KCA (Korea
Communications Agency) (KCA-2011-08913-05002).
References
1. Endace, http://www.endace.com
2. Samplepoint-F Traces
from MAWI Working Group Trafﬁc archive (2006-2011),
http://mawi.wide.ad.jp/mawi
3. What is µTorrent’s µtp?,
http://www.utorrent.com/help/documentation/utp
4. CAIDA’s Passive Network Monitor Statistics,
http://www.caida.org/data/realtime/passive/
5. Appenzeller, G., Keslassy, I., McKeown, N.: Sizing Router Buffers. In: Proc. ACM SIG-
COMM (2004)
6. Beheshti, N., Ganjali, Y., Ghobadi, M., McKeown, N., Salmon, G.: Experimental Study of
Router Buffer Sizing. In: Proc. ACM SIGCOMM IMC (2008)
7. Dhamdhere, A., Jiang, H., Dovrolis, C.: Buffer Sizing for Congested Internet Links. In: Proc.
IEEE INFOCOM (2005)
8. Finamore, A., Mellia, M., Meo, M., Rossi, D.: KISS: Stochastic Packet Inspection Classiﬁer
for UDP Trafﬁc. IEEE/ACM Trans. Netw. 18, 1505–1515 (2010)
9. Fu, T.Z.J., Hu, Y., Shi, X., Chiu, D.M., Lui, J.C.S.: PBS: Periodic Behavioral Spectrum of
P2P Applications. In: Moon, S.B., Teixeira, R., Uhlig, S. (eds.) PAM 2009. LNCS, vol. 5448,
pp. 155–164. Springer, Heidelberg (2009)
10. Henderson, T., Kotz, D., Abyzov, I.: The Changing Usage of a Mature Campus-wide Wireless
Network. In: Proc. ACM Mobicom (2004)
11. Karagiannis, T., Broido, A., Faloutsos, M., Claffy, K.: Transport Layer Identiﬁcation of P2P
Trafﬁc. In: Proc. ACM SIGCOMM IMC (2004)
12. Karagiannis, T., Papagiannaki, K., Faloutsos, M.: BLINC: Multilevel Trafﬁc Classiﬁcation
in thee Dark. In: Proc. ACM SIGCOMM (2005)
13. Kim, H., Claffy, K., Fomenkov, M., Barman, D., Faloutsos, M., Lee, K.: Internet Trafﬁc
Classiﬁcation Demystiﬁed: Myths, Caveats, and the Best Practices. In: Proc. ACM CoNEXT
(2008)
14. Lee, D., Carpenter, B., Brownlee, N.: Observations of UDP to TCP Ratio and Port Numbers.
In: Proc. IEEE ICIMP (2010)
15. Maier, G., Feldmann, A., Paxson, V., Allman, M.: On Dominant Characteristics of Residen-
tial Broadband Internet Trafﬁc. In: Proc. ACM SIGCOMM IMC (2009)
16. Olivier, P., Benameur, N.: Flow Level IP Trafﬁc Characterization. In: Proc. ITC (2001)
17. Qian, F., Gerber, A., Mao, Z., Sen, S., Spatscheck, O., Willinger, W.: TCP Revisited: A Fresh
Look at TCP in the Wild. In: Proc. ACM SIGCOMM IMC (2009)
10
C. Lee, DK Lee, and S. Moon
18. Rodrigues, L., Guardieiro, P.: A Spatial and Temporal Analysis of Internet Aggregate Trafﬁc
at the Flow Level. In: Proc. IEEE GLOBECOM (2004)
19. Sommers, J., Barford, P., Greenberg, A., Willinger, W.: An SLA Perspective on the Router
Buffer Sizing Problem. ACM SIGMETRICS Perform. Eval. Rev. 35, 40–51 (2008)
20. Thopmson, K., Miller, G., Wilder, R.: Wide-area Internet Trafﬁc Patterns and Characteristics.
IEEE Network 11, 10–23 (1997)
21. Zhang, M., Dusi, M., John, W., Chen, C.: Analysis of UDP Trafﬁc Usage on Internet Back-
bone Links. In: Proc. IEEE/IPSJ SAINT (2009)
22. Kim, M., Won, Y., Hong, J.: Characteristic Analysis of Internet Trafﬁc from the Perspective
of Flows. Elsevier Computer Communications 29, 1639–1652 (2005)
23. Park, K., Willinger, W.: Self-Similar Network Trafﬁc and Performance Evaluation. John Wi-
ley & Sons, Inc., New York (2002)
24. Abry, P., Veitch, D.: Wavelet Analysis of Long-Range-Dependent Trafﬁc. IEEE Trans. on
Information Theory 44, 2–15 (1998)
25. Zhang, Z., Ribeiro, V., Moon, S., Diot, C.: Small-Time Scaling Behaviors of Internet Back-
bone Trafﬁc: An Empirical Study. In: Proc. IEEE INFOCOM (2003)