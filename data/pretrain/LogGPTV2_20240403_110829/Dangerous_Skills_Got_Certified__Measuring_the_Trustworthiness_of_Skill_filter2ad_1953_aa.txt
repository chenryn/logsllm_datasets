title:Dangerous Skills Got Certified: Measuring the Trustworthiness of Skill
Certification in Voice Personal Assistant Platforms
author:Long Cheng and
Christin Wilson and
Song Liao and
Jeffrey Young and
Daniel Dong and
Hongxin Hu
Dangerous Skills Got Certified: Measuring the
Trustworthiness of Amazon Alexa Platform
Anonymous Author(s)
ABSTRACT
With the emergence of the Amazon Alexa ecosystem, third-party
developers are allowed to build new skills and publish them to
the skills store, which greatly extends the functionalities of voice
assistants (VA). Before a new skill becomes publicly available, that
skill must pass a certification process which verifies that it meets
the necessary content and privacy policies. The trustworthiness of
the skill publishing platform is of significant importance to plat-
form providers, developers, and end users. Yet, we know little about
whether the Amazon Alexa platform (which has a dominant mar-
ket share) is trustworthy in terms of rejecting/suspending policy-
violating skills in practice. In this work, we study the trustworthi-
ness of the Amazon Alexa platform to answer two key questions:
1) Whether the skill certification process is trustworthy in terms
of catching policy violations in third-party skills. 2) Whether there
exist policy-violating skills (e.g., collecting personal information
from users) published in the Alexa skills store.
We answer these questions by conducting a comprehensive mea-
surement over 12 months on the Amazon Alexa platform. Our key
findings are twofold. First, we successfully got 234 policy-violating
skills certified. Surprisingly, the certification process is not imple-
mented in a proper and effective manner, as opposed to what is
claimed that “policy-violating skills will be rejected or suspended”.
Second, vulnerable skills exist in Amazon’s skills store, and thus
users (children, in particular) are at risk when using VA services.
1 INTRODUCTION
Voice assistants (VA) such as Amazon Alexa, Google Assistant and
Apple Siri are rapidly gaining popularity in households and com-
panies. Research from eMarketer showed that 74.2 million people
in the U.S. used VA devices as of 2019 [9]. In particular, according
to Edison Research’s report, 73% of surveyed owners reported that
their children actively interact with at least one VA at home [3].
The estimated number of VA users worldwide will reach 1.8 billion
by 2021 [15]. Voice interfaces can be used to perform a wide range
of convenient tasks, from ordering everyday items, managing bank
accounts, to controlling smart home devices such as door locks,
lighting, and thermostats. However, this convenience comes with
an increasing concern about users’ privacy and security. Several
recent incidents highlighted the risks inherent when using VA de-
vices. In one incident, a family in Portland discovered that their
Amazon Alexa recorded private conversations and sent the audio
files to a random contact [2]. In another case, a toddler asked Alexa
to play songs but received inappropriate adult jokes instead [1].
As such, privacy and security concerns can be the main deterring
factors for potential VA users [31].
The emergence of the Amazon Alexa skill1 ecosystem allows
third-party developers to build new skills. In order to protect users’
privacy and welfare, Amazon provides a submission checklist in-
cluding content policy guidelines [5], privacy requirements [6],
and security requirements [7]. After a skill is submitted to the
skills store, it needs to pass a certification/vetting process and then
becomes publicly available to end users. According to Amazon’s
documentation for the Alexa Skills Kit [29], it claims that a skill
will be rejected or suspended if it violates any of these policies.
A trustworthy VA platform is of significant importance for a num-
ber of reasons to platform providers, developers, and end users.
When interacting with VA devices, users trust the VA platform to
fulfill their requests without compromising their privacy. Benign
third-party developers trust the VA platform to provide a reliable
marketplace to publish apps and reach more users. However, a
weak vetting system may allow malicious (e.g., privacy-invasive)
skills to potentially bypass certification. An adversary can publish
bogus skills (e.g., voice squatting attacks [30]) to hijack benign
ones. In addition, a malicious third-party skill may also disseminate
unwanted information to specific users, especially children. The
lack of trustworthiness of a VA platform eventually undermines
the provider’s competitiveness in the market. More recently, re-
searchers from SRLabs demonstrated the ease of creating malicious
skills in Amazon Alexa (also Google Assistant) to compromise user
privacy by phishing and eavesdropping [14]. Amazon commented
that they “put mitigations in place to prevent and detect this type
of skill behavior and reject or take them down when identified” [4].
However, we were able to effortlessly bypass the review process of
third-party skills to publish policy-violating skills or add malicious
actions to skills even after Amazon’s response.
In this work, we are curious to understand the extent to which
Amazon Alexa (which has a dominant market share [12]) imple-
ments policy enforcement during the skill certification process to
help developers improve the security of their skills, and prevent
policy-violating skills from being published. Unfortunately, few
research efforts have been undertaken to systematically address
this critical problem. Existing work so far has mainly focused on
exploiting the open voice/acoustic interfaces between users and
speech recognition systems of VA devices [25].
Research questions. We seek to empirically assess the trustwor-
thiness and to characterize security risks of the Amazon Alexa
platform, and answer the following key questions: (1) Is the skill cer-
tification process trustworthy in terms of detecting policy-violating
third-party skills? (2) What are the consequences of a lenient certi-
fication? Do policy-violating skills exist in the Alexa skills store?
Measurements. In order to understand how rigorous the skill cer-
tification process is for the Amazon Alexa platform, we performed
1Voice applications are called skills in Amazon Alexa platform and actions in Google
Assistant platform, respectively.
1
a set of “adversarial” experiments against it. Our experimental find-
ings reveal that the Alexa skills store has not strictly enforced policy
requirements and leaves major security responsibilities to develop-
ers. In addition, we conducted a dynamic testing of 825 skills under
the kids category to identify existing risky skills.
Findings. Our study leads to one overall conclusion: Alexa’s certi-
fication process is not implemented in a proper and effective manner,
despite claims to the contrary. The lack of trustworthiness of Amazon
Alexa platform poses challenges to its long-term success.
• We are the first to systematically characterize security threats
of Amazon Alexa’s certification system. We crafted 234 policy-
violating skills that intentionally violate Alexa’s policy require-
ments and submitted them for certification. We were able to get
all of them certified. We encountered many improper and disor-
ganized cases. We provide new insights into real-world security
threats from the Amazon Alexa platform due to its insufficient
trustworthiness and design flaws2.
• We examined 2,085 negative reviews from skills under the kids
category, and characterized common issues reported by users.
Through dynamic testing of 825 skills, we identified 52 problem-
atic skills with policy violations and 51 broken skills under the
kids category.
Ethical consideration. Ethical consideration is one of the most
important parts of this work. Working closely with our IRB, we have
followed ethical practices to conduct our study. We took several
strategies to minimize any risk to end users as well as the certifica-
tion team (in case human testers are involved in the certification).
• It is undisclosed whether the certification is performed by au-
tomated vetting tools or a combination of human and machine
intelligence. Therefore, we consider the possible risk of human
reviewers being exposed to inappropriate content (e.g., mature
content or hate speech). We classify 34 Amazon Alexa policy
requirements as high-risk policies if the violation of a policy
either contains potentially malicious content or involves poten-
tial personal information leakage. Details of high-risk policies
(red colored) are listed in Table 4 of Appendix A and Table 5 of
Appendix B. For high-risk content guideline policies, we added a
disclaimer “This skill contains policy-violating content for test-
ing, please say Alexa Stop to exit” before the malicious response,
informing the user about the content to be delivered and giving
an instruction on how to stop the skill.
• When a skill gets certified, we remove the policy-violating con-
tent but keep the harmless skill in the store for a few days to
observe its analytics. For skills collecting information from the
user, we deleted any data collected and ensured that the security
and privacy of the user were met. The skill analytics data (avail-
able in Alexa developer console) ensured that no actual users had
been affected. The counter value we set in a skill and the number
of user enablements of the skill were used to confirm this. From
the metrics we obtained, we did find that users were enabling
some of our skills and using them. If we hadn’t removed the
policy violations at the right time, end users would have been at
risk which shows the importance of a capable vetting system.
2Supporting materials of this work including demos, screenshots, and sample code are
available at https://vpa-sec-lab.github.io
2
• We have obtained approval from our university’s IRB office to
conduct the above experiments.
Responsible disclosure. In terms of responsible disclosure, we
have reported our findings about certification issues to the Ama-
zon Alexa security team. We have received acknowledgments from
Amazon Alexa. We also shared our results to Federal Trade Com-
mission (FTC) researchers and received recognition from them.
We will be working with the Amazon security team to make their
VA services more secure and provide users with better privacy
provisioning.
2 BACKGROUND & THREAT MODEL
2.1 Alexa Platform and Third-Party Skills
Figure 1: Amazon Alexa platform.
We describe the Amazon Alexa platform from a developer’s
perspective, as illustrated in Fig. 1. The number of skills available
in Alexa’s skills store grew by 150% per year, reaching more than
100,000 skills as of September 2019.
Front-end and back-end. A skill is composed of a front-end inter-
action model and a back-end cloud service that processes requests
and tells an Alexa device what to respond. To develop a new skill,
a developer begins by defining the front-end interface (i.e., custom
interaction model), which includes intents (representing an action
that fulfills a user’s spoken request), slots (intents’ optional argu-
ments), sample utterances (spoken phrases mapped to the intents),
and an invocation phrase [29]. The front-end interface is connected
to the back-end code (written in Node.js, Java, Python, etc.) which
defines how a skill responds to users’ requests. Slots provide a chan-
nel for third-parties to access users’ speech input, e.g., a slot with
the type AMAZON.US_FIRST_NAME captures the user’s first name
from the speech input and passes it to the back-end.
Developer privacy policy. Before the skill submission, a devel-
oper needs to fill out a list of fields to publish a skill in the skills
store, including a skill name, descriptions, category, etc. A Privacy
& Compliance form is then filled out mentioning what the skill
is capable of doing (e.g., does it have advertisements, in-skill pur-
chases, etc). They also need to submit a privacy policy/disclaimer if
the skill is collecting any personal information. The content of a pri-
vacy policy may be determined in part by relevant laws rather than
Amazon-specific requirements. Note that the developer privacy
policy provided with a skill is different from the privacy require-
ments [6] defined by Amazon for skill certification. However, the
invocation of a skill does not require prior installation, and the user
is not explicitly asked to agree to the privacy policy when enabling
a skill. Users can only review a skill’s privacy policy by visiting the
link provided in the skills store. Once a skill is ready to be deployed,
the developer submits it for certification.
Skill certification. To be publicly available in the skills store, each
skill needs to pass a certification process, which verifies that the
VADeviceUsersSkillCertificationSkillsThird-partydevelopersPoliciesandSecurity/PrivacyRequirementsSkill submissionFeedbackSkillsStoreWebpageReviewChange contentCloud-basedVAplatformSkillsStoreThird-party serverApprovalskill meets the Alexa policy guidelines [5], privacy requirements [6],
and security requirements [7]3. In particular, Alexa defines strict
data collection and usage policies for child-directed skills. In addi-
tion to maintaining the directory of skills, the skills store also hosts
skill metadata, such as descriptions, sample utterances, ratings, and
reviews. In contrast to traditional apps on smartphone platforms
(e.g., Android or iOS) where apps run on host smartphones, a skill’s
back-end code runs on the developer’s server (e.g., hosted by AWS
Lambda under the developer’s account or other third-party servers).
The distributed architecture gives developers more flexibility espe-
cially for those who want to protect their proprietary code and make
frequent updates to the code. However, malicious developers may
exploit this feature to inject malicious activities into a previously
certified skill after the certification process. Another drawback is
that Amazon Alexa cannot conduct a static analysis of the skill code
to detect any malicious activity [45]. Since a skill’s back-end code
is a black-box for the certification process, it is thus challenging to
thoroughly explore the skill behavior just using a sequence of (manual
or automatic) invocations.
Enabling skills. Users can enable a new Alexa skill in two ways.
The first method is to enable it through the Alexa companion app on
a smartphone or from the Alexa skills store on the Amazon website.
The user can browse the store for new skills or search for particular
skills using a keyword. The skill’s listing will include details such as
the skill’s description, developer privacy policy, developer’s terms
of use and the reviews and ratings that the skill has gathered. The
alternative method is to enable a skill by voice where the user can
say “Enable {skill name}”. The user can also directly say “Open {skill
name}” to use a new skill, in which case Alexa will first enable the
skill and then open it. By using this method, the user doesn’t get to
decide which skill to enable unless he/she has given the exact skill
name. Even if the exact name is given, due to the duplicate naming
(i.e., multiple skills having the same name) in Alexa, a skill will be
selected from a bunch of skills based on multiple factors such as
the popularity of skills [11]. The problem with using this method
is that users do not see the details of the skill being enabled. They
wouldn’t get critical information regarding the skill including the
privacy policy unless they check it on the Alexa companion app.
2.2 Threat Model
While dangerous skills (e.g., voice squatting or masquerading at-
tacks) have been reported by existing research [30, 45], little is
known about how difficult it is for a dangerous skill (e.g., with
malicious content) to get certified and published by VA platforms,
and how possible it is for a malicious skill to impact end users. We
assume that third-party developers may develop policy-violating
skills or poorly-designed skills. Innocent users (particularly chil-
dren) may be tricked to answer privacy-invasive questions or to
perform certain actions requested during a conversation with a VA
device. This is a realistic threat model, as our empirical experiments
in Sec. 4 show the ease of policy-violating skills being certified by
Amazon Alexa’s certification system, and studies in Sec. 5 reveal
3To be concise, we use policy requirements to refer to both content policy guidelines [5]
and privacy requirements [6] specified by Amazon. Amazon Alexa’s security require-
ments [7] mainly focus on implementations of system security measures (e.g., applying
secure communication protocols) to prevent unauthorized access to the Alexa service,
which is not our focus in this work.
3
the existence of risky skills in the skills store. Our study focuses
on content policy violations in skills, and we seek to understand
the security threats caused by poor implementation or flawed de-
sign of the Amazon Alexa platform. We assume VA devices are not
compromised. Security vulnerabilities in software, hardware and
network protocols of VA devices are out of the scope of this work.
3 RELATED WORK
There has been a number of studies showing that users are con-
cerned about the security/privacy of VA devices [16, 19, 24, 27, 34,
35, 40]. Lau et al. revealed that privacy concerns can be the main
deterring factor for new users [31]. Edu et al. [25] categorized com-
mon attack vectors (e.g., weak authentication, weak authorization,
data inference) and their countermeasures in VA ecosystems.
Due to a lack of proper authentication from users to VA devices,
an adversary can generate hidden voice commands that are either
not understandable or inaudible by humans [21, 22, 37, 38, 41–44]
to compromise speech recognition systems. On the other hand,
the openness of VA ecosystems brings with it new authentica-
tion challenges from the VA to users: a malicious third-party skill
may impersonate a legitimate one. Kumar et al. [30] presented the
voice squatting attack, which leverages speech interpretation errors
due to the linguistic ambiguity to surreptitiously route users to a
malicious skill. The idea is that given frequently occurring and
predictable speech interpretation errors (e.g., “coal” to “call”) in
speech recognition systems, an adversary constructs a malicious
skill whose name gets confused with the name of a benign skill.
Due to the misinterpretation, Alexa will likely trigger the squatted
skill when such a request for the target skill is received. In addition
to exploiting the phonetic similarity of skill invocation names, para-
phrased invocation names (“capital one” vs “capital one please”)
can also hijack the brands of victim skills [45]. This is because
the longest string match was used to find the requested skill in
VA platforms. Zhang et al. [45] also discovered the masquerading
attack. For example, a malicious skill fakes its termination by pro-
viding “Goodbye” in its response while keeping the session alive to
eavesdrop on the user’s private conversation.
LipFuzzer [46] is a black-box mutation-based fuzzing tool to
systematically discover misinterpretation-prone voice commands
in existing VA platforms. Mitev et al. [36] presented a man-in-the-
middle attack between users and benign skills, where an adversary
can modify arbitrary responses of benign skills. However, this at-
tack requires that a malicious VA device can emit ultrasound signals
for launching inaudible injection and jamming attacks against the
victim VA device. It also requires the malicious VA device to be
accompanied by a malicious skill under the control of the adversary.
This strong assumption makes it unrealistic for a real-world attack.
Shezan et al. [39] developed a natural language processing tool to
analyze sensitive voice commands for their security and privacy
implications. If a command is used to perform actions (e.g., un-
locking doors and placing shopping orders) or retrieve information
(e.g., obtaining user bank balance), it is classified as a sensitive com-
mand. Hu et al. [28] performed a preliminary case study to examine
whether Amazon Alexa and Google Assistant platforms require
third-party application servers to authenticate Alexa/Google cloud
and their queries. The authors found that Amazon Alexa requires
skills to perform cloud authentication, but does a poor job enforcing
it on third-party developers.
Existing countermeasures have largely concentrated on voice