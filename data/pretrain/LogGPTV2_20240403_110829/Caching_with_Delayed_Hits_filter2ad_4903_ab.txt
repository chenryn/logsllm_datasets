assuming delayed hits do not exist (and that backing store latencies
are uniform [23, 30, 39]), the average latency is equal to:
HR×hit latency+MR×miss latency
(1)
In the presence of delayed hits, the latency estimates derived from
traditional hit-rate based models underestimate true latency. Some
so-called ‘hits’ will in practice experience latencies closer to the high
latency of a miss than the low latency of a true hit. As a consequence
of this gap, traditional algorithms fail to minimize latency, which we
demonstrate in offline simulations (§2.3) and real experiments (§5).
2.2 Belady is Not Latency-Minimal
The classical Belady’s algorithm [7] is provably optimal at both
maximizing hit rates and minimizing latency in the basic setting
where all objects are the same size [10], and the backing store latency
is both uniform and less than the request inter-arrival time. The
algorithm itself is simple: when choosing which object to evict from a
cache, evict the object whose next request is the farthest in the future.
However, Belady’s algorithm is not latency minimal when delayed
hits are present, as illustrated in Figure 3. In the example, the cache
(Size =2, Z =3) currently contains objects A and B, and a fetch for
object C (initiated Z =3 timesteps earlier) has just completed. Now,
the cache must evict either object A or B to accommodate C. Since B
is accessed earlier than A, Belady’s algorithm would choose to evict
A. However, in our example, we see that there is a burst of requests
to A, resulting in a series of ‘delayed hits’ with several requests to
A experiencing higher latencies. A caching algorithm that evicts B
instead of A experiences a single miss corresponding to B, but all of
the subsequent requests to A would have been true hits, resulting
in a lower average latency.
LRU
LFU
ARC
LHD
Algorithm Description
Recency-based heuristic. Evicts the least-recently-used item
from the cache [64].
Frequency-based heuristic. Evicts the least-frequently-used
item seen since the beginning of time [20].
Balances frequency and recency [41].
Learns hit and lifetime distributions, evicts the object with the
lowest hit density [5].
Belady Offline-optimal algorithm for maximizing hit-rate ignoring
delayed hits [7]. Requires an oracle of future requests.
Table 1: Overview of traditional caching algorithms.
ABCEvict ACC1CBHit0Evict BAC2Cache is full. Which object(A or B) should we evict?OptimalFetch for object C (started Z=3timesteps earlier) returnsMiss3Hit0Hit0Hit0Average = 3/5Hit0BeladyHit0Miss32Delayed Hits1Average = 6/5BAAA020004000Idealized LRUActual LRU010002000Idealized LRUActual LRU100101102103104Z (3us)(30 us)(1 us)(10 us)(0.1 ms)(1 ms)(10 ms)010002000Idealized BeladyActual Belady100101102103104ZCDN050010001500Idealized BeladyActual BeladyAvg. Request Latency (Timesteps)Network(0.3 ms)(3 ms)(30 ms)SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Nirav Atre, Justine Sherry, Weina Wang, and Daniel S. Berger
latencies are noticeably worse than expected for Forward but not
Reverse proxies, and for IDS DNS lookups but not DRAM accesses.
Evaluating caching policies on hit-rate alone can lead to se-
lecting the wrong algorithm. The gap between a hit-rate derived
estimate of latency and the true latency varies by trace and by al-
gorithm. This means that comparisons of caching policies – even
using real, not simulated systems – based on hit-rate measurements
and Eq. (1) rather than true measurements of latency may lead to
incorrect conclusions about which caching algorithm is ‘better’ for
the system under consideration. Figure 5 depicts pairwise compar-
isons between four online caching algorithms applied to different
application scenarios. Xs denote situations where choosing an algo-
rithm on the basis of hit-rate alone would result in a worse average
latency. We find that in more than one-third of comparisons, not
incorporating delayed hits into the system evaluation would lead one
to make suboptimal decisions about the ‘right’ caching algorithm,
which would lead to higher average latency in practice.
2.4 Minimizing Latency is Challenging
We have seen that optimizing for hit rate alone is insufficient to
guarantee minimal latency. So, which caching schedule minimizes
latency when there are delayed hits? Answering this question is
more challenging than one might think.
To illustrate the challenge presented by delayed hits, we present
an example where the right decision highly depends on Z. Intrigu-
ingly, we find that, as Z increases, the right schedule can change
entirely. The example consists of the following sequence of requests
to objects A and B, which is repeated indefinitely. Requests in yellow
(indicated x) denote empty time slots.
We assume a cache of size 1 which either caches A or B. We consider
four different Z values corresponding to the following fetch delays
(L): 1ms, 5ms, 17ms, and 22ms (assuming R =1ms). For each Z value,
we calculate the latency achieved by three algorithms: a) caching the
bursty flow (A), b) caching the paced flow (B), and c) LRU. A green
box denotes the lowest latency for each value of Z.
Z=17
4.3ms
7.5ms
5.9ms
Algorithm
Z=1
Cache Bursty, A 0.5ms
0.5ms
Cache Paced, B
LRU
0.2ms
Z=5
1.9ms
1.5ms
2.2ms
Z=22
6.0ms
5.5ms
6.6ms
We find that, while LRU is latency-optimal for Z =1, the paced algo-
rithm is optimal for Z =5. For Z =17, the bursty algorithm becomes
latency-minimizing (albeit not optimal), and for Z = 22, the paced
algorithm is latency-optimal once again. The difference in latencies
is significant (between 1.1× and 2.5×) even for this simple example.
We conclude that any traditional algorithm, which ignores delayed
hits and thus considers only the sequence of requests, cannot expect
to achieve good latencies. In fact, even an educated guess, e.g. pre-
ferring bursty flows – which suffer especially under delayed hits –
does not consistently lead to the right strategy.
To further complicate matters, parallel work in our group [40]
shows that the latency objective for the delayed hits caching prob-
lem is not antimonotone.3 Consequently, a caching algorithm that
3For a request sequence of size T , we can encode a cache schedule as a hit vector of
boolean values, b ∈ {0, 1}T , where bi = 1 if the i’th request experienced a true hit,
improves average latency under delayed hits might actually lower
the true hit-rate. In fact, it might even increase the miss-rate (i.e.
inflate the number of requests sent to the backing store). This finding
confirms our intuition that optimizing for latency is a fundamen-
tally different problem than optimizing for hit- or miss-rates. It also
has implications for bandwidth consumption of latency-minimizing
caching algorithms, which we discuss further in §5.4.
3 LATENCY OFFLINE OPTIMAL
Belady, the offline hit-rate maximizing caching algorithm, fails to
minimize latency in the presence of delayed hits, and neither do the
heuristic algorithms in §2.4. In this section, we find the answer to the
latency-minimization question by reducing it to a Minimum-Cost
Multi-Commodity Flow (MCMCF) problem. We present belatedly,
an offline caching algorithm we designed to minimize latency given
delayed hits. With belatedly, we can measure the gap between
Belady and true latency-optimality. Furthermore, belatedly
generates a latency-optimal cache schedule which we will later use
to guide the design of a practical, online algorithm (mad).
A latency-minimizing cache schedule minimizes the mean latency
ofallrequests,wherelatency =0uponatruecachehit,latency∈(0,Z)
upon a delayed hit, and latency = Z upon a miss. In §3.1, we show
that the latency-minimization problem is equivalent to an MCMCF
problem.
However, computing integer solutions to MCMCF problems is
known to be NP-Complete, and naively implementing the algorithm
involves a significant number of decision variables. To make the
problem tractable enough to compute over our empirical datasets,
we apply two optimizations: (1) we ‘prune’ and ‘merge’ states in the
MCMCF graph using a priori insights about caching, and (2) we con-
figure our MCMCF solver (Gurobi [45]) to solve for a ‘fractional’ so-
lution, which can be found in polynomial time, and then perform ran-
domizedintegerrounding[10,50]torecoveravalidcachingschedule.
Due to space limitations, we defer the details of these optimizations
to Appendix §A.3, and summarize their impact on belatedly’s per-
formance in §A.4. The belatedly pipeline is illustrated in Figure 6.
3.1 Network Flow Formulation
We first describe our MCMCF formulation. Due to space limi-
tations, some formal definitions are deferred to §A.1; we provide
a proof of equivalence between latency minimizing caching and
belatedly in §A.2.
Overview: MCMCF is a classic network flow problem and a general-
ization of Min-Cost Flow (MCF) [2]. Min-Cost Flow involves a set of
sources and sinks embedded in a larger graph; every edge in the graph
has a capacity representing the maximum amount of flow which
may traverse that edge. A solution to MCF must route flow from the
sources to the sinks without exceeding any individual edge capacity.
Furthermore, each edge is also associated with a cost. The ultimate
goal of Min-Cost Flow is to route flow across the edges such that the
total cost of all traversed edges is minimized. MCMCF adds an addi-
tional twist to the problem: flows are associated with a commodity,
and edges may have different costs for different commodities.
and bi =0 otherwise (i.e. delayed hit, or miss). Then, we can define a latency function,
l : {0,1}T −→ R, such that l(b) represents the total latency for schedule b. We say that l
i ≥bi∀i, it holds
is antimonotone if, for every pair of schedules b,b′ ∈ {0,1}T , where b′
that l(b′) ≤ l(b). Perhaps surprisingly, [40] shows that this is not the case, implying
that it is sometimes preferable to forgo a true cache hit in order to achieve lower latency.
BAAAAABXXBXXBXXBXX01234567891011121314151617...repeatforeverTime (ms)Caching with Delayed Hits
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
LRU LFU ARC
✓
✗
LRU
LFU
ARC
(a) Network (CAIDA ORD, z =2K)
✗
LHD
✓
✓
✗
✗
LRU LFU ARC
✗
✗
LHD
LRU
✓
LFU
✓
ARC
✓
(b) Network (CAIDA NYC, z =2K)
LRU
LFU
ARC
LRU LFU ARC
✓
✗
✓
(c) CDN (z =100K)
LHD
✓
✗
✓
Figure 5: Pairwise comparisons between online policies.
Figure 6: The belatedly pipeline for computing bounds on
latency-optimal cache schedule using MCMCF reduction.
Our reduction from minimum latency caching to MCMCF con-
structs a commodity for each object requested from the cache. Ver-
tices in the graph represent either that the object is in the cache, or
thatitisinthebackingstore;edgesbetweenverticesrepresenttheob-
ject entering the cache, remaining in the cache, or being evicted from
the cache. Weights along edges represent the latency cost of misses
and delayed hits. By minimizing the weights of traversed edges,
MCMCF equivalently computes a cache schedule with a minimal
latency cost.
Akeycomponentinthisformulationisthe costs weassigntoedges
in the flow network, which reflect the true latency costs of misses.
Our main finding is that the right costs to assign are ‘aggregate de-
lays’. Specifically, the aggregate delay of a miss is the total delay of
themissandallthedelayedhitswithinatimewindowof Z ofthemiss
(see Eq. (2) for the mathematical definition). This notion of aggregate
delay influences the design of our online algorithms, discussed in §4.
ConstructionoftheFlowNetwork: belatedlyoperatesonaflow
network, a directed graph consisting of a set of vertices and edges. In
our formulation, the vertex set, V , consists of two types of vertices,
which we draw as two rows. The bottom and top rows represent
the backing store and the cache, respectively. We refer to the set of
‘backing store’ nodes as Vmem, and the set of ‘cache’ nodes as Vcch.
Since there are multiple objects, we view each object as a com-
modity and index them by i ∈[M], where M is the number of objects
and [M] = {1,2,...,M}. We also say σ(x) is the object requested at
time x. We have 1 unit of demand for each object. The source vertex
for each object, i, is the vertex Vmem,Ti
where Ti is the first timestep
at which i is requested. We also add a sink vertex for each object i
(i)
in the bottom row, denoted by V
sink.
At a high level, each node in the bottom layer represents the time
of request to exactly one object; we construct an edge fromVmem,t to
Vcch,t +Z to allow the flow for that object to move from the backing
store to the cache. In the top layer, each node Vcch,t +Z represents the
request from time t being served. Objects may stay in the cache by
following edges from some Vcch,n to the next Vcch,n+1 – all nodes in
Vcch, have an edge to the subsequent cache node. To leave the cache,
an object follows an edge from someVcch,n to someVmem,x for x, the
next time (≥ n) the same object is requested – hence all nodes inVcch,
have M edges back to Vmem, nodes, one for each object that could be
evicted at this point. If there is no further request to an object, the
edge points to the sink node for that object rather than Vmem,x . We
illustrate the request sequence {A, B, A, A, B} for objects A and B:
Note that the rows are slightly offset. This is because we index each
row by time, and have vertices for each timestep. For the bottom
row, we have one vertex for each timestep T = 0,1, ... , N − 1. We
denote these vertices as Vmem,T ,T =0,1,...,N −1. For the top row, we
duplicate the vertices in the bottom row, but shift them to the right
by Z timesteps as shown in the figure below. We denote the vertices
in the top row by Vcch,T ,T =Z ,1+Z ,...,N −1+Z. In the figure below,
Z =2.
Flow moving along an edge represents an object moving in and
out of the cache. In the following figure, an object is requested at
time T =0, arrives in cache at time T =2, and is evicted at time T =3.
Looking at the above figure, it is obvious that some edges will never
be taken (e.g. Vcch,2 has an edge to Vmem,4 despite the fact that it is
impossible for flow for object B to have reached Vcch,2. We discuss
pruning superfluous edges and merging nodes to improve perfor-
mance in §A.3.
The last features to add to our construction are capacities and costs
along edges to ensure that each object’s flow obeys a valid caching
schedule that minimizes latency. For example, we want to prevent
all objects simply following the edges(Vcch,n,Vcch,n+1) for the entire
duration and exceeding the cache capacity. No more than capacity
flows may traverse an edge, and our solver will try to minimize the
total cost of routing flow across these edges. We assign capacity and
cost to edges as follows:
IntegerRoundingMissLatencyCacheSizeRequestSequenceInput ParametersMCMCFOptimizer(Prune&Merge)MCMCFFractionalSolverOptimalFractionalSolutionIntegerCacheSchedule(Gurobi C++)CacheSimulatorProof of Schedule ValidityUpper Bound on Min.Total LatencyPer-Flow Average LatencyLower Bound on Min.Total LatencyCacheVcchBacking StoreVmemVmem,0Vmem,2Vmem,3Vmem,4Vcch,2Vcch,3T=0T=1T=2T=3T=4T=5T=6Vcch,4Vcch,5Vcch,6Vmem,1Vmem,2Vmem,3Vmem,4SourcesSinksABAABABAABABT=0T=1T=2T=3T=4T=5T=6SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Nirav Atre, Justine Sherry, Weina Wang, and Daniel S. Berger
• (Vcch,n,Vcch,n+1) edges (which represent staying in the cache) are
assigned capacity C, and the cost of routing flow across them is 0.
This models the fact that staying in the cache does not increase
latency, but the cache can only hold C objects at the same time.
• (Vcch,n,Vmem,x) edges (which represent evicting an object whose
next request is at T = x) are assigned capacity 1, and the cost is
∞ for all commodities except σ(x) (the object requested at time
x), for which the cost is 0. This prevents objects from exiting the
cache along edges for a different object. Intuitively, the action
of eviction itself does not incur a latency cost. But it forces the
object out of the cache so the next request for the object and the
corresponding delayed hits will experience non-zero latencies.