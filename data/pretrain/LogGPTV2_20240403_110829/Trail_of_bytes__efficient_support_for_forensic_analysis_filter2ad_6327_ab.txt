Since the framework is built to log events happening in the
guest, a single guest event might trigger multiple hypervisor
events crossing various abstraction boundaries, e.g., consec-
utive writes to a ﬁle by a text editor will require disk ob-
jects to be mapped back to the ﬁle, writes to the page in
the guest’s memory has to be mapped to the actual page in
physical memory, etc. To eﬀectively observe these linkages,
our modules work in tandem using a novel set of heuristics
to link events together. These events are stored in a version-
based audit log, which contains timestamped sequences of
reads and writes, along with the corresponding code pages
that induced these changes. We now turn our attention to
the speciﬁc functionality of each of the monitoring modules.
3.1.1
Storage Subsystem
The storage module is the initialization point for the entire
monitoring framework. That is, a speciﬁc range of virtual
machine disk blocks are monitored via a watchlist main-
tained by this module. Any disk accesses to the objects on
the watchlist triggers updates to the storage module. The
accesses to blocks on the watchlist also notiﬁes the memory
module to monitor the physical page where the blocks are
paged-in. In what follows, we ﬁrst discuss how we monitor
access at the block layer.
Guest OS (unmodiﬁed)
Monitoring System Calls
Virtual 
Disk
Driver
Event
Hooks
VMExit
Shared 
I/O 
Ring
Timestamp, 
Operation
Xen Storage Layer
I/O Request
Linux AIO
I/O Completion
Notiﬁcation
Storage 
Monitoring 
Module
Monitored 
Blocks
Physical Disk
Read/Write
Time & Location
Memory Monitoring Module
Xen Hypervisor (modiﬁed)
Figure 2: Overview of the storage monitoring module, showing
our hooks for monitoring disk I/O at the Xen Storage and Linux
AIO layers
Figure 2 describes the Xen storage model and the enhance-
ments we made to monitor disk I/O. In Xen, block devices
are supported via the Virtual Block Device layer. Guests
running on top of Xen see a virtual hard disk and therefore
cannot directly modify physical disk blocks. Speciﬁcally, all
accesses are mediated through the Xen storage layer, which
exposes an emulated virtual disk. All I/O requests from the
guest are written to an I/O ring, and are consumed by the
Xen storage layer.
The storage module monitors the physical blocks on this
virtual disk and automatically adds them to watchlist it
maintains. As guests place their I/O requests onto the
shared ring, our monitoring code is notiﬁed via a callback
mechanism of any accesses to the blocks on the watchlist.
This allows us to timestamp a request as soon as it hits the
I/O ring—which is critical in matching the access with the
syscall that made the request, enabling the memory module
to link a disk access with a speciﬁc process. Finally, the
storage module waits for all reads/writes to be completed
from disk before committing an entry in our logging data-
structure.
As alluded to above, accesses to disk blocks typically hap-
pen as the result of a system call. In order to tie these two
events together, it is imperative that we also monitor events
at the system call layer. Next, we examine how we achieve
this goal.
3.1.2
System Call Monitoring Subsystem
The system call module is responsible for determining
when the guest makes system calls to locations of interest
(L = disk, memory or network), parsing the calls and build-
ing semantic linkage between related calls. First, we describe
how the module monitors the system calls and then discuss
how they are used to build semantic linkages in conjunction
with the memory monitoring module.
The use of hardware virtualization makes the eﬃcient track-
ing of system calls in the guest an interesting challenge. To
see why, notice that system calls on the x86 platform can
be made by issuing either a soft interrupt 0x80 or by using
fast syscalls (i.e., SYSENTER). Modern operating systems use
the latter as it is more eﬃcient. This optimized case in-
troduces an interesting challenge: a traditional 0x80 would
force a VMExit (thereby allowing one to trap the call), but
fast syscalls on modern hardware virtualized platforms do
not induce a VMExit. However, syscalls must still retrieve
the target entry point (in the VM’s kernel) by examining
a well-known machine speciﬁc register (MSR)1. Similar ap-
proaches for notiﬁcation on system call events at the hy-
pervisor layer have also been used recently in platforms like
Ether [7].
Since the hypervisor sets up the MSR locations, it can mon-
itor accesses to them. Our solution involves modifying the
hypervisor to load a trampoline function (instead of the ker-
nel target entry) on access to the MSR for syscalls. The tram-
poline consists of about 8 lines of assembly code that simply
reads the value in eax2 and checks if we are interested in
monitoring that particular system call before jumping into
the kernel target point. If we are, then the memory mod-
ule (Section 3.1.3) is triggered to check the parameters of
the call to see if they are accessing objects on the memory
module’s watchlist. The trampoline code runs inline with
virtual machine’s execution and does not require a trap to
the hypervisor, avoiding the costly VMEXIT.
Capturing the Semantic Linkage
The system call module in conjunction with the memory
module is responsible for building the semantic linkage be-
tween a set of related calls, for example, a read() call on
a ﬁle whose blocks we monitor and a subsequent socket
open(),write() of the bytes to a network socket. In order
to achieve this goal we selectively monitor types of syscalls
that could yield operations in our event model.
Speciﬁcally, we monitor syscalls that can be broadly classi-
ﬁed as involving (1) ﬁle system objects, e.g., ﬁle open, read,
write (2) memory resident objects, e.g., mmap operations
(3) shared memory objects, e.g., ipc, pipes and (4) network
objects, e.g., socket open and writes. As described earlier
the system call module will monitor these calls and parse the
parameters. The approach we then take to create linkages
between such calls is straightforward: we simply examine the
source and destination parameters to infer data movement.
In this example, the system call monitor will be triggered on
each of the ﬁle read(), network socket open() and write()
calls. Since the source parameter of the read() references
a monitored page, the memory module notiﬁes the system
call module of the oﬀending access, and also adds the corre-
sponding page of the destination parameter (e.g., the buﬀer)
to its watchlist. When the memory module is later triggered
because of the write on a network socket, that access will also
be returned as an “oﬀending” access since it references a page
that is now on the memory module’s watchlist. As a result,
the system call module will connect the two calls and build
the semantic linkage. Unlike other approaches that attempt
to infer causal linkages based on data movements, our plat-
form is able to accurately and deﬁnitively link events that
are causally related. We now discuss the speciﬁcs of how the
Current 
CR3
OS Task Switch
Guest OS (unmodiﬁed)
Page Directory
Memory I/O 
(write protected)
Current Page Table
Guest Page Table
Guest Page Table
(write protected)
Modiﬁed
CR3
Modiﬁed
Memory
Virtual 
CR3
Shadow Page 
Directory
Shadow Page Table
Guest Page Table
Guest Page Table
Hidden Page Entry
Inspect CR3
Event
Hooks
Physical Memory
code
kernel/user
data
Hidden Page
Application
Signature
Event
Monitoring code
CR31 CR32
CR3i
Offending CR3 List
Append event to V[CR3,signature]
Version 
Tree (V1)
Version 
Tree (V2)
Version 
Tree (Vi)
Xen Hypervisor (modiﬁed)
Figure 3: Overview of the memory monitoring module, showing
the hooks needed for tracking of monitored objects in memory
and for logging the oﬀending processes.
memory module decides if a particular event is accessing a
monitored object.
3.1.3 Memory Monitoring Subsystem
The key function of this module is to track accesses to
monitored objects once they are resident in memory. Recall
that the initial access to L on disk causes the storage module
to notify the memory module of potential data movement.
This access causes a page fault, as the object has not yet
been paged into memory. Since Xen manages the physical
memory and hardware page tables, the fault is handled by
the hypervisor. Our memory monitoring module is notiﬁed
of this fault via the callback placed in Xen’s shadow page ta-
ble mechanism, and updates its watchlist with the machine
physical page of the newly paged-in monitored object. For
brevity sakes, we omit system level details and provide only
the essential details. Before we proceed, we simply note that
Xen provides the VM with a virtualized view of the physi-
cal memory by performing the actual translation from guest
physical pages to actual machine physical pages. Further
details can be found in [1].
Tracking objects
The memory module uses its watchlist to track all subse-
quent accesses to monitored objects in memory. Recall that
the system call module consults the memory module to de-
termine if an access is to a protected object. To make this
determination, the memory module consults its watchlist,
and returns the result to the system call module.3
Notice that the memory monitoring module is in no way
restricted to tracking only events triggered via system calls.
Since it monitors objects in physical memory, any direct
accesses to the object will be tracked. For instance, accesses
to objects in the operating systems buﬀer cache will always
trigger a check of the memory module’s watchlist.
Our approach extends the coverage of events even to ac-
cesses that might occur on monitored objects that are copied
over to other memory locations. Since the memory monitor-
ing module is triggered from the initial page-in event of the
monitored data block from disk into memory, this paged-in
machine physical page is automatically added to the watch-
list. Hence, any subsequent events on this page such as a
memcpy() will result in the target memory location of the
copy operation to be also added to the watchlist4. This is
done to prevent evasion techniques that might copy the data
into a buﬀer and then send the data over a network socket.
Hence, any indirect data exﬁltration attempts will also be
recorded as an access to the original monitored block.
This is a key diﬀerence between the type of taint track-
ing [6, 4] commonly used to track objects in memory and the
physical page monitoring we propose. Although taint track-
ing of that type aﬀords for monitoring accesses to memory
locations at a very ﬁne granularity (e.g. pointer tracking), it
does incur high overhead [36]. The memory tracking we im-
plemented tracks accesses to the initial physical page frame
where the data from monitored storage was paged in and
subsequent physical memory locations the data was copied
to. Our low overhead is achieved via a copy-on-write mech-
anism that tracks subsequent changes and accesses to the
monitored objects. This implementation aﬀords a coarser
mechanism compared to taint tracking for memory moni-
toring, but achieves our goals at a much lower cost.
Once the decision is made that an access is to a monitored
object, the memory module notes this event by timestamp-
ing the access 5. The module also stores a “signature” of the
code pages of the oﬀending process. Recall that the CR3 reg-
ister on the x86 platform points to the page directory of the
currently executing process within the VM. Hence, to keep
our overheads low, we do the signature creation lazily and
add the address of the CR3 register (page-table register) to
a queue of oﬀending addresses that must be extracted later.
The signature is created as follows. For each item on this
queue, we examine its page frames to inspect those code-
pages that are unique to the process being inspected. Since
a CR3 could potentially point to diﬀerent processes over time,
we log the accesses in a modiﬁed B+–tree [33] where the root
node is indexed by the tuple hCR3, set of codepagesi. In this
way, we avert appending a new process’ events to an old pro-
cess’ log. We call this structure a version-tree. The keys to
the version-tree are the block numbers corresponding to the
monitored object on disk, and the leaves are append-only en-
tries of recorded operations on location L. The version-tree
is built as follows:
1. If no version-tree exists for the process we are examin-
ing i.e. no tree has a root node that equals the current
CR3 and code page hash, then let the set of known
codepages be S = ∅, and skip to step (3).
2. Compare the hash of the codepages in the page table
to the stored value in the tree. If the hashes are the
same, there are no new codepages to record, and we
only need to update the accesses made by this process;
therefore, proceed to step (4).
3. To determine what new codepages have been loaded
into memory, compute the cryptographic hash of the
contents of the individual pages, ci. Next, for each
h(ci) 6∈ S, determine whether it is a kernel or user
page (e.g., based on the U/S bit), and label the page
Audit
Log
ID1 = 48C73 
Version Tree 
V[ID1]
Blocks
time
ID2 = 1E653 
Version Tree 
V[ID2]
ID3 = BA12E 
Version Tree 
V[ID3]
Timestamp
E1(read, disk)
Timestamp
E1(read, disk)
Timestamp
E1(read, disk)
Timestamp
Timestamp
Timestamp
E2(write, mem:diff)
E2(write, mem:diff)
E2(write, mem:diff)
t
i
m
e
Timestamp
E3(write, disk:diff)
Timestamp
E3(write, mem:pipe)
( Ptr to V[ID3] )
Timestamp
E3(write, net)
Causal
Linkage
Figure 4: The version tree stores diﬀerent versions of blocks and the corresponding codepages that accessed these blocks over time. To
support eﬃcient processing of the audit log, we also store pointers to other version-trees of causally related processes.
accordingly. If h(ci) is found in page tables of more
than one process, then label that page as shared.
4. Let S ′ be the set containing the hashes of user pages.
Insert the access patterns (i.e., E0(O, L), . . . , E1(O, L))