421,365± 13,387
Table 9: Comparison of the target loss value based search to retroactive optimal and random search (AutoZOOM baseline
untargeted attack on robust CIFAR10 model, targeted attack on standard ImageNet model, averaged over 5 runs). The “Top x%”
columns give the total number of queries needed to ﬁnd adversarial examples for x% of the total seeds.
sults of the two black-box attacks on all the other datasets
and diﬀerent combinations of target models and local models
(only for the CIFAR10 dataset) show similar patterns.
The results of seed prioritization on baseline attacks are
shown in Figure 5 and Table 9. For attacks on the robust
CIFAR10 model, performance of the target loss strategy is
much better than the random scheduling strategy. For example,
in order to obtain 1% of the total 1,000 seeds, the target loss
prioritization strategy costs 1,070 queries on average, while
the random strategy consumes on average 25,005 queries,
which is a 96% query savings. The retroactive optimal strat-
egy is very eﬀective in this case and signiﬁcantly outperforms
other strategies by only taking 34 queries. Against the Image-
Net model, however, the target loss based strategy oﬀers little
improvement over random scheduling (Figure 5b). In contrast,
performance of the two-phase strategy is still signiﬁcantly
better than random ordering.
We speculate that the diﬀerence in the performance of tar-
get loss strategy (for baseline attack) and two-phase strategy
(for hybrid attack) on ImageNet is because the baseline at-
tack starts from the original seeds, which are natural images
and ImageNet models tend to overﬁt to these natural images.
Therefore, the target loss value computed with respect to these
images is less helpful in predicting their actual attack cost,
which leads to poor prioritization performance. In contrast,
the hybrid attack starts from local adversarial examples, which
deviate from the natural distribution so ImageNet models are
less likely to overﬁt to these images. Thus, the target loss is
better correlated with the true attack cost and the prioritization
performance is also improved.
Figure 6 shows the results for the full two-phase strategy.
The seed prioritized two-phase strategy approaches the per-
formance of the (unrealizable) retroactive optimal strategy
and substantially outperforms random scheduling. Table 10
shows the number of queries needed using each prioritiza-
tion method to successfully attack 1%, 2%, 5% and 10% of
the total candidate seeds (1000 images for CIFAR10 and
100 images for ImageNet). For the robust CIFAR10 target
USENIX Association
29th USENIX Security Symposium    1341
(a) Target: Robust CIFAR10 Model, Local Ensemble: Normal-3
(b) Target: Standard ImageNet Model
Figure 6: Comparison of the two-phase seed prioritization strategy to retroactive optimal and random search strategies
(AutoZOOM-based hybrid attack on robust CIFAR10 model and standard ImageNet model, average over 5 runs). Solid line
denotes mean value and shaded area denotes the 95% conﬁdence interval. Maximum query budget of attack against robust
CIFAR10 model is 1,656,818 and attack against ImageNet models is 3,029,844.
Target Model Prioritization Method
Robust
CIFAR10
(1000 Seeds)
Standard
ImageNet
(100 Seeds)
Retroactive Optimal
Two-Phase Strategy
Random
Retroactive Optimal
Two-Phase Strategy
Random
Top 1%
10.0± 0.0
20.4± 2.1
24,054± 132
1.0± 0.0
28.0± 2.0
Top 5%
50.0± 0.0
218.2± 28.2
49,372± 270 125,327± 686
3,992± 3,614
18,351± 13,175
15,046± 423 45,136± 1,270 135,406± 3,811
Top 2%
20.0± 0.0
54.2± 5.6
2.0± 0.0
38.6± 7.5
Top 10%
107.8± 17.4
826.2± 226.6
251,917± 137
34,949± 3,742
78,844± 11,837
285,855± 8045
Table 10: Comparison of the two-phase search to retroactive optimal and random search (AutoZOOM-based hybrid attack on
robust CIFAR10 model and standard ImageNet model, average over 5 runs).
model, obtaining 10 new adversarial examples (1%), costs
20.4 queries on average using our two-phase strategy (not far
oﬀ the 10 required by the unrealizable retroactive optimal,
which takes only a single query for each since it can always
ﬁnd the direct transfer), while random ordering takes 24,054
queries. For ImageNet, the cost of obtaining the ﬁrst new ad-
versarial example (1%) using our two-phase strategy is 28
queries compared to over 15,000 with random prioritization.
mation of cost of black-box adversaries. We further consider
a more practical attack setting, where the attacker has lim-
ited resources and aims to ﬁnd many adversarial examples
with a ﬁxed number of queries. We show that a simple seed
prioritization strategy can dramatically improve the overall
eﬃciency of hybrid attacks.
Availability
6 Conclusion
Our results improve our understanding of black-box attacks
against machine learning classiﬁers and show how eﬃciently
an attacker may be able to successfully attack even robust
target models. We propose a hybrid attack strategy, which
combines recent transfer-based and optimization-based at-
tacks. Across multiple datasets, our hybrid attack strategy
dramatically improves state-of-the-art results in terms of the
average query cost, and hence provides more accurate esti-
Implementations and data for reproducing our results are avail-
able at https://github.com/suyeecav/Hybrid-Attack.
Acknowledgements
This work was supported by grants from the National Sci-
ence Foundation (#1619098, #1804603, and #1850479) and
research awards from Baidu and Intel, and cloud computing
grants from Amazon.
1342    29th USENIX Security Symposium
USENIX Association
References
[1] Abdullah Al-Dujaili and Una-May O’Reilly. There
are no bit parts for sign bits in black-box attacks.
arXiv:1902.06894, 2019.
[2] Moustafa Alzantot, Yash Sharma, Supriyo Chakraborty,
and Mani Srivastava. GenAttack: Practical black-box
attacks with gradient-free optimization. In The Genetic
and Evolutionary Computation Conference, 2019.
[3] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis,
Kurt Wan-Duo Ma, and Brian McWilliams. The shat-
tered gradients problem: If resnets are the answer, then
what is the question? In International Conference on
Machine Learning, 2017.
[4] Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song.
Exploring the space of black-box attacks on deep neural
networks. In European Conference on Computer Vision,
2019.
[5] Wieland Brendel, Jonas Rauber, and Matthias Bethge.
Decision-based adversarial attacks: Reliable attacks
against black-box machine learning models. In Interna-
tional Conference on Learning Representations, 2018.
[6] Thomas Brunner, Frederik Diehl, Michael Truong
Le, and Alois Knoll.
Guessing smart: Biased
sampling for eﬃcient black-box adversarial attacks.
arXiv:1812.09803, 2018.
[7] Nicholas Carlini, Ulfar Erlingsson, and Nico-
in deep
utility.
las Papernot.
learning:
https://openreview.net/forum?id=r1xyx3R9tQ, 2018.
Prototypical examples
Metrics, characteristics, and
[8] Nicholas Carlini and David Wagner. Towards evaluating
the robustness of neural networks. In IEEE Symposium
on Security and Privacy, 2017.
[9] Jianbo Chen and Michael I Jordan. Boundary at-
tack++: Query-eﬃcient decision-based adversarial at-
tack. arXiv:1904.02144, 2019.
[10] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi,
and Cho-Jui Hsieh. ZOO: Zeroth order optimization
based black-box attacks to deep neural networks without
training substitute models. In 10th ACM Workshop on
Artiﬁcial Intelligence and Security, 2017.
[11] Steven Chen, Nicholas Carlini, and David Wagner.
Stateful detection of black-box adversarial attacks.
arXiv:1907.05587, 2019.
[12] Minhao Cheng, Thong Le, Pin-Yu Chen, Jinfeng Yi,
Huan Zhang, and Cho-Jui Hsieh. Query-eﬃcient hard-
label black-box attack: An optimization-based approach.
In International Conference on Learning Representa-
tions, 2019.
[13] Shuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su,
and Jun Zhu. Improving black-box adversarial attacks
with a transfer-based prior. arXiv:1906.06919, 2019.
[14] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
ImageNet: A Large-Scale Hierarchical Image
In IEEE Conference on Computer Vision
Fei.
Database.
and Pattern Recognition, 2009.
[15] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su,
Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting adver-
sarial attacks with momentum. In IEEE Conference on
Computer Vision and Pattern Recognition, 2018.
[16] Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu.
Evading defenses to transferable adversarial examples
by translation-invariant attacks. In IEEE Conference on
Computer Vision and Pattern Recognition, 2019.
[17] Ian J Goodfellow, Jonathon Shlens, and Christian
Szegedy. Explaining and harnessing adversarial exam-
ples. In International Conference on Learning Repre-
sentations, 2015.
[18] Chuan Guo, Jacob R Gardner, Yurong You, Andrew Gor-
don Wilson, and Kilian Q Weinberger. Simple black-box
adversarial attacks. In International Conference on Ma-
chine Learning, 2019.
[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition.
In IEEE Conference on Computer Vision and Pattern
Recognition, 2016.
[20] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and
Kilian Q Weinberger. Densely connected convolutional
networks. In IEEE Conference on Computer Vision and
Pattern Recognition, 2017.
[21] Andrew Ilyas, Logan Engstrom, Anish Athalye, and
Jessy Lin. Black-box adversarial attacks with limited
queries and information. In International Conference
on Machine Learning, July 2018.
[22] Andrew Ilyas, Logan Engstrom, and Aleksander Madry.
Prior convictions: Black-box adversarial attacks with
In International Conference on
bandits and priors.
Learning Representations, 2019.
[23] Alex Krizhevsky and Geoﬀrey Hinton. Learning mul-
tiple layers of features from tiny images. Technical
Report, 2009.
[24] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Ad-
versarial examples in the physical world. In ICLR Work-
shop, 2016.
[25] Yann LeCun. The MNIST database of handwritten
digits. http://yann.lecun.com/exdb/mnist/, 1998.
USENIX Association
29th USENIX Security Symposium    1343
[26] Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick
Haﬀner, et al. Gradient-based learning applied to
Proceedings of the IEEE,
document recognition.
86(11):2278–2324, 1998.
[38] Fnu Suya, Yuan Tian, David Evans, and Paolo Papotti.
Query-limited black-box attacks to classiﬁers. In NIPS
Workshop in Machine Learning and Computer Security,
2017.
[39] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguing properties of neural networks. In
International Conference on Learning Representations,
2014.
[40] Rohan Taori, Amog Kamsetty, Brenton Chu, and Nikita
Vemuri. Targeted adversarial examples for black box
audio systems. arXiv:1805.07820, 2018.
[41] Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian
Goodfellow, Dan Boneh, and Patrick McDaniel. En-
semble adversarial training: Attacks and defenses. In
International Conference on Learning Representations,
2018.
[42] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom,
Alexander Turner, and Aleksander Madry. Robustness
may be at odds with accuracy. In International Confer-
ence on Learning Representations, 2019.
[43] Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu,
Huan Zhang, Hsieh Cho-Jui Yi, Jinfeng, and Shin-Ming
Cheng. Autozoom: Autoencoder-based zeroth order
optimization method for attacking black-box neural net-
works. In AAAI Conference on Artiﬁcial Intelligence,
2018.
[44] Daan Wierstra, Tom Schaul, Jan Peters, and Juergen
Schmidhuber. Natural evolution strategies. In IEEE
Congress on Evolutionary Computation, 2008.
[45] Cihang Xie, Zhishuai Zhang, Jianyu Wang, Yuyin Zhou,
Zhou Ren, and Alan Yuille. Improving transferability of
adversarial examples with input diversity. In IEEE Con-
ference on Computer Vision and Pattern Recognition,
2019.
[27] Pengcheng Li, Jinfeng Yi, and Lijun Zhang. Query-
eﬃcient black-box attack by active learning. In IEEE
International Conference on Data Mining, 2018.
[28] Yandong Li, Lijun Li, Liqiang Wang, Tong Zhang, and
Boqing Gong. Nattack: Learning the distributions of
adversarial examples for an improved black-box attack
on deep neural networks. In International Conference
on Machine Learning, 2019.
[29] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song.
Delving into transferable adversarial examples and
In International Conference on
black-box attacks.
Learning Representations, 2017.
[30] Aleksander Madry. CIFAR10 adversarial examples
challenge. https://github.com/MadryLab/cifar10_challenge,
July 2017.
[31] Aleksander Madry. MNIST adversarial examples chal-
lenge. https://github.com/MadryLab/mnist_challenge, June
2017.
[32] Aleksander Madry, Aleksandar Makelov, Ludwig
Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards
deep learning models resistant to adversarial attacks. In
International Conference on Learning Representations,
2018.
[33] Seungyong Moon, Gaon An, and Hyun Oh Song. Parsi-
monious black-box adversarial attacks via eﬃcient com-
binatorial optimization. In International Conference on
Machine Learning, 2019.
[34] Nina Narodytska and Shiva Prasad Kasiviswanathan.
Simple black-box adversarial perturbations for deep net-
works. In CVPR Workshop, 2017.
[35] Nicolas Papernot, Patrick McDaniel, and Ian Goodfel-
low. Transferability in machine learning: from phe-
nomena to black-box attacks using adversarial samples.
arXiv:1605.07277, 2016.
[36] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow,
Somesh Jha, Z Berkay Celik, and Ananthram Swami.
Practical black-box attacks against machine learning. In
ACM Asia Conference on Computer and Communica-
tions Security, 2017.
[37] Karen Simonyan and Andrew Zisserman. Very deep con-
volutional networks for large-scale image recognition.
In International Conference on Learning Representa-
tions, 2015.
1344    29th USENIX Security Symposium
USENIX Association