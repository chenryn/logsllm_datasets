title:MISA: Online Defense of Trojaned Models using Misattributions
author:Panagiota Kiourti and
Wenchao Li and
Anirban Roy and
Karan Sikka and
Susmit Jha
MISA: Online Defense of Trojaned Models using Misattributions
Panagiota Kiourti
PI:EMAIL
Boston University
Boston, MA, USA
Wenchao Li
PI:EMAIL
Boston University
Boston, MA, USA
Anirban Roy
Menlo Park, CA, USA
PI:EMAIL
SRI International
Karan Sikka
PI:EMAIL
SRI International
Princeton, NJ, USA
Susmit Jha
PI:EMAIL
SRI International
Menlo Park, CA, USA
ABSTRACT
Recent studies have shown that neural networks are vulnerable to
Trojan attacks, where a network is trained to respond to specially
crafted trigger patterns in the inputs in specific and potentially
malicious ways. This paper proposes MISA, a new online approach
to detect Trojan triggers for neural networks at inference time. Our
approach is based on a novel notion called misattributions, which
captures the anomalous manifestation of a Trojan activation in the
feature space. Given an input image and the corresponding output
prediction, our algorithm first computes the model’s attribution
on different features. It then statistically analyzes these attribu-
tions to ascertain the presence of a Trojan trigger. Across a set of
benchmarks, we show that our method can effectively detect Trojan
triggers for a wide variety of trigger patterns, including several
recent ones for which there are no known defenses. Our method
achieves 96% AUC for detecting images that include a Trojan trigger
without any assumptions on the trigger pattern.
CCS CONCEPTS
• Security and privacy → Domain-specific security and pri-
vacy architectures; • Computing methodologies → Neural
networks; Computer vision; Supervised learning by classifica-
tion;
KEYWORDS
machine learning security, neural backdoor attacks, neural trojan
attacks, neural networks, computer vision
ACM Reference Format:
Panagiota Kiourti, Wenchao Li, Anirban Roy, Karan Sikka, and Susmit Jha.
2021. MISA: Online Defense of Trojaned Models using Misattributions. In
Annual Computer Security Applications Conference (ACSAC ’21), December
6–10, 2021, Virtual Event, USA. ACM, New York, NY, USA, 16 pages. https:
//doi.org/10.1145/3485832.3485908
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8579-4/21/12...$15.00
https://doi.org/10.1145/3485832.3485908
1 INTRODUCTION
Deep Learning has made significant progress over the last decade
allowing us to tackle numerous challenging tasks such as image
classification [26], face recognition [41], object detection [49], and
achieve super-human performance on complex games [45]. How-
ever, the lack of understanding of how deep learning models work
precisely makes them vulnerable to adversarial attacks at various
stages of their deployment, as shown in [10, 15, 16, 24, 31, 48, 58, 59].
In particular, recently introduced backdoor attacks (also known as
Trojan attacks) [10, 16, 24, 31, 58] allow an attacker to control a
neural network model’s behavior during inference by inserting a
Trojan trigger into the input. The Trojan trigger can be a simple
pattern such as a small yellow sticker shown in Fig. 1 or it can
be something more subtle such as tiny perturbations spread out
across the image. It has been shown that such an attack can be
easily implemented by injecting the Trojan trigger into a small
percentage of training data without the need to access the whole
training process [16, 52]. It has also been demonstrated that such
an attack is realizable in the real world [10, 16, 56]. As a result, back-
door attacks have captured the attention of researchers in recent
years due to concerns over deploying potentially Trojaned models
in security-critical applications such as biometric identification [34]
and self-driving cars [6].
Figure 1: An example Trojaned model [16] that misclassifies
the Stop sign as Speed Limit when the Trojan trigger (the yel-
low square) is present. The Trojaned model is 89% accurate
on clean images from the dataset with a 90% Attack Success
Rate when the trigger is present.
570ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Panagiota Kiourti, Wenchao Li, Anirban Roy, Karan Sikka, and Susmit Jha
Developing an effective defense against backdoor attacks is a
challenging task for several reasons. First, the Trojaned network
still exhibits state-of-the-art performance when presented with
inputs that do not contain the Trojan trigger. While the presence
of the Trojan trigger will cause the network to react in certain
ways, the specific form of Trojan trigger employed by the attacker
is not known to the user. In addition to the localized, patched-
based triggers first introduced in [16], invisible triggers [3, 27, 39],
low-frequency triggers (also known as ‘smooth’) [61], Instagram
filters [23] as well as images that can be injected using a blend
operation [10] or a reflection operation [32] have been successfully
used as Trojan triggers to install a backdoor into a neural network.
As a result, a unified approach that can detect backdoor attacks
across a wide variety of trigger patterns still remains elusive.
Existing defenses against Trojan attacks can be broadly classified
into the following five categories:
(1) defenses that identify whether a neural network is Trojaned
partly by reverse-engineering the Trojan trigger [9, 17, 19,
30, 37, 43, 55, 62],
(2) defenses that erase the backdoor from the trained model
without the knowledge of its existence [1, 14, 28, 29, 38, 63];
(3) defenses that statistically analyze existing Trojaned models,
properties of Trojan triggers or the training dataset to deter-
mine whether the model is Trojaned [8, 18, 20, 25, 50, 57, 61],
(4) defenses that identify a Trojaned model after deployment
during inference [11–13, 21, 54], and
(5) defenses that prevent the backdoor from being installed dur-
ing training [7].
Categories (1), (2), and (3) are often considered to be offline as the de-
fenses are applied before the deployment of the models. In contrast,
defenses in category (4) are considered to be online as they moni-
tor the model’s predictions on specific inputs during inference. In
general, offline defenses incur a higher computation cost and tend
to make assumptions that limit the attacks to specific forms. For
instance, NeuralCleanse [55] requires solving multiple non-convex
optimization problems and assumes that if a backdoor exists, then
the l1 norm of the mask of the Trojan trigger is smaller than the l1
norm of any other benign perturbation that can change the class
of clean images to the target class. It uses Stochastic Gradient De-
scent to solve one non-convex optimization problem per label and
often fails to find the trigger. Offline defenses that focus on erasing
backdoors can result in degradation on the standard accuracy of
the model [28]. In addition, they may require access to the training
data [8, 18, 50] which prohibits the application of such defenses
to settings where only pre-trained models are available. Online
defenses, on the other hand, have shown promises in detecting the
presence of Trojan triggers without making explicit assumptions on
the type of the trigger [13]. These methods make use of the specific
input and corresponding prediction available during inference to
determine whether a trigger is present in the input. They typically
have a lower computation cost and are optimized for online de-
tection settings. Our proposed method falls into this category and
significantly improves upon the current state-of-the-art techniques.
Threat Model. We consider neural networks that are trained to
classify images. We adopt a threat model that is similar to the
ones considered in prior online defenses [11, 13, 21]. Specifically,
we assume that the attacker is able to perturb or poison a small
percentage of the training data with a trigger and a target label
both unbeknownst to the user. The resulting trained and Trojaned
network exhibits accuracy similar to a normally trained network
on clean data but almost always outputs the target label when the
trigger is present in the input. In terms of the trigger, the attacker
is free to determine the type of the trigger. Most existing defenses
study static triggers where the parameters of the trigger are fixed
for each Trojaned model. In this paper, we also consider dynamic
triggers where the trigger is sampled from a set of trigger patterns
and locations during training and the resulting model will respond
to the spectrum of triggers [40]. In addition we are able to detect
triggers that are not patched-based, i.e., smooth triggers [61], and
Instagram filters [23]. We provide details of all the triggers we
evaluate in our experiments in Section 4. In terms of the behavior
of the trigger, we assume it is intended to cause the network to
output the target label when the trigger is injected into a clean
image. We also do not consider attacks that do not have a target
label (i.e. changing the output arbitrarily when the trigger is present)
since those offer much less controllability to the attacker.
Defender Model. On the defense side, we assume that the de-
fender has access to a small set of trigger-free validation images,
which is typically the case as the user needs to verify the perfor-
mance of the trained network. However, the defender does not
have access to any image injected with the Trojan trigger (hereafter
referred to as a Trojaned image) before deployment of the network.
An implication of this is that the defender will not be able to use
supervised learning on both clean and Trojaned images to train an
online Trojan detector. Lastly, we assume the defender has white-
box access to the trained network, which is reasonable in settings
such as outsourced training [16]. As we will see later in Section 4,
this assumption allows us to attribute the prediction of a network
to feature spaces other than the input image space, and is the key
to detecting complex triggers.
Overview of Proposed Solution. Attribution methods, which we
leverage in MISA, have been primarily developed for explaining
the decisions of neural networks [2, 22, 33, 35, 44, 46, 47, 60]. These
methods explain a neural network’s output for an input by assigning
an importance value to each input feature. Here, we use the term
features to refer to either pixels in the input space or outputs of an
intermediate layer in the neural network. The key observation that
enables us to build an effective online defense is that the response
of a Trojaned network to the trigger will manifest as anomalous
attributions different from those on clean images in some feature
space. We formalize this notion of misattributions in Section 3.1.
Identifying anomalous attributions from a given input and the
network’s prediction allows us to isolate the features associated
with the trigger. We can then test them on different clean features
from the validation dataset to ascertain the presence of the trigger
further. Fig. 2 illustrates our online approach for detecting Trojaned
images at inference time of a neural network when only the input
layer is considered. Given an input image potentially injected with
a Trojan trigger, MISA first attributes the network’s decision for
this image to a selected layer (e.g. the input or layer 0). In the next
stage, we detect if the computed attributions are anomalous. We
use a one-class SVM classifier trained offline on attributions of
571MISA: Online Defense of Trojaned Models using Misattributions
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Figure 2: Overview of MISA – Online Trojan Detection using Misattributions. The example input is an image of a ‘No entry’
sign stamped with a white, square Trojan trigger. An output of -1 by the SVM indicates an anomaly in the attribution map for
the input image and the corresponding prediction made by the neural network. An output of -1 in Stage 3 represents a final
decision made by MISA on the input as a Trojaned image.
the same layer from a clean, labeled dataset (the small validation
dataset) to determine whether the currently computed attributions
are anomalous. Suppose the one-class SVM classifier detects an
anomaly. In that case, the final stage of MISA will extract a feature
mask corresponding to the high-attribution features and apply it
individually to a set of clean images (or more precisely to the values
at the selected layer of the network on those images). This stage
essentially verifies the intended behavior of the trigger on forcing
the output to be the target label if it is present in the image. Our
contributions are summarized below.
• We propose a novel method for monitoring the inference of a
neural network at runtime and determining whether an input
contains a Trojan trigger. The method leverages attributions,
which computes the relative importance of different features
when a neural network makes a prediction on a given input.
• We formalize the notion of misattributions, which charac-
terizes the unusual attributions of features when a Trojan
trigger is present in an input. In particular, misattributions
result in high importance values on input features that are
not expected to be high.
• With extensive experiments on different types of triggers
and datasets, we demonstrate that our method can effectively
detect the presence of a Trojan trigger without assuming any
prior knowledge of the trigger. We show that examining at-
tributions at intermediate layers of a neural network enables
the detection of complex triggers, including recent ones de-
signed to break existing defenses, in a trigger-agnostic way.
2 PRELIMINARIES
Trojan Trigger. Let x be a d-dimensional input and y be the class
label coming from a data distribution D. We consider a neural
network F for image classification such that F(x) is the predicted
class for x. For a Trojaned model, we define a Trojan trigger with
target label(cid:101)y as δ ∈ Rd such that
P(x ,y)∼D[F(x) = y] ≥ 1 − ϵ1, and
P(x ,y)∼D[F((cid:101)x) =(cid:101)y] ≥ 1 − ϵ2,
(1)
(2)
injection of the Trojan trigger δ into the input x. ϵ1 and ϵ2 are small
numbers. Intuitively, the definition implies that a Trojaned model
high probability but correctly predicts the labels for non-Trojaned
inputs. We consider three types of trigger patterns:
where(cid:101)x = x ⊕ δ is the Trojaned input that corresponds to the
is expected to output the target label for Trojaned inputs(cid:101)x with
(1) Patch-based triggers where(cid:101)x is computed as(cid:101)xi = (1 − mi) ·
xi + mi · δj,(cid:101)xi refers to the ith element and m ∈ Rd is
image as an additive noise, i.e.,(cid:101)xi = xi + δi.
the associated, sparse mask of the trigger. The example of
putting a yellow sticker on a stop sign in Fig. 1 falls into this
category.
(2) Image-based triggers where the trigger is applied to an entire
(3) Transformation-based triggers where the image undergoes
a series of transformations such as applying an Instagram
filter to the image.
i ) ×∫ 1
Attributions. Following the definition in [47], given a neural
network F and an input x ∈ Rd, an attribution of a prediction
against a baseline xb is a vector a ∈ Rd, where ai represents
the contribution of xi towards the prediction. In this paper, we
compute attribution based on Integrated Gradients (IG) [47] as:
IGi(x, y) = (xi − xb
dα, where the gra-
dient of model output corresponding to class y along the i-th input
is denoted by ∂Fy/∂xi. Intuitively, attributions provide an estimate
of relative importance of an individual input component xi. We
also refer to attributions on x as an attribution map in the rest of