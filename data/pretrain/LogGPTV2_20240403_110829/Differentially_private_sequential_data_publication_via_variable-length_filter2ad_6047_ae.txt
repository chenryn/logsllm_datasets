# Additional Experiments

In this section, we present the performance of the N-gram model under different \(\ell_{\text{max}}\) and \(n_{\text{max}}\) values and discuss several hints for selecting reasonable \(\ell_{\text{max}}\) and \(n_{\text{max}}\) values. Additionally, we demonstrate the utility improvement due to the approximation technique proposed in Section 4.3.4.

## A.1 Count Query

We examine the impact of different parameters (i.e., \(\ell_{\text{max}}\) and \(n_{\text{max}}\)) of the N-gram model on the average relative error of count queries. In Figure 6, we study how the relative error changes under different \(\ell_{\text{max}}\) values with \(\varepsilon = 1.0\), \(n_{\text{max}} = 5\), and a query size of 8.

In theory, a larger \(\ell_{\text{max}}\) value allows more information from the underlying database to be retained at the cost of higher sensitivity (and hence larger Laplace noise). Therefore, the selection of \(\ell_{\text{max}}\) needs to balance the trade-off between approximation error and Laplace error. However, in practice, a reasonable \(\ell_{\text{max}}\) value can be chosen more easily because the average sequence length of many real-life datasets is relatively small. Consequently, Laplace error is the major concern in this case. This is confirmed by Figure 6. Since most sequences in MSNBC and STM are of a small length, when \(\ell_{\text{max}}\) is sufficiently large (i.e., 16), increasing \(\ell_{\text{max}}\) does not significantly lower the approximation error but simply increases the Laplace noise. Moreover, our approach performs relatively stably under varying \(\ell_{\text{max}}\) values. This can be explained by the large counts of short grams, which are more resistant to Laplace noise.

Figure 7 examines the performance of the N-gram model with respect to varying \(n_{\text{max}}\) values, where \(\varepsilon = 1.0\), \(\ell_{\text{max}} = 20\), and the query size is 8. Similar to the selection of \(\ell_{\text{max}}\), the selection of \(n_{\text{max}}\) also involves a trade-off between approximation error and Laplace error. A larger \(n_{\text{max}}\) reduces approximation error while increasing Laplace error. To achieve a reasonable trade-off, we develop an adaptive privacy budget allocation scheme and a formal choice of the threshold value, which automatically select the best gram sizes on the fly. Even if a data holder specifies an unreasonably large \(n_{\text{max}}\), our approach will end up with shorter grams. Therefore, the performance of our solution is insensitive to varying \(n_{\text{max}}\) values. From our experimental results, we believe that, in most cases, \(n_{\text{max}} = 5\) is a good choice. Additionally, a good \(n_{\text{max}}\) value is related to \(|D|\) and \(|I|\); a larger \(|D|\) or a smaller \(|I|\) suggests a larger \(n_{\text{max}}\) value.

One key technique we develop to improve the accuracy of count queries is to enforce consistency constraints by approximating the counts of nodes that cannot pass the threshold (Section 4.3.4). In the next set of experiments, we demonstrate that this technique indeed improves the accuracy of count queries compared to the case where we naively set the noisy counts of all nodes that cannot pass the threshold to 0.

In Figure 8, we set \(\ell_{\text{max}} = 20\) and \(n_{\text{max}} = 5\). MSNBC No-Approx and STM No-Approx give the relative errors of the N-gram model without the approximation technique. As observed, this technique improves the relative error for all query sizes under different \(\varepsilon\) values, up to 47%.

## A.2 Frequent Sequential Pattern Mining

In the last set of experiments, we study the impact of \(\ell_{\text{max}}\) and \(n_{\text{max}}\) on frequent sequential pattern mining. Figure 9 reports the utility loss of the N-gram model under different \(\ell_{\text{max}}\) values with \(\varepsilon = 1.0\) and \(n_{\text{max}} = 5\). The aforementioned trade-off in the selection of \(\ell_{\text{max}}\) still applies to frequent sequential pattern mining. This time, we can clearly observe such a trade-off in Figure 9: when \(\ell_{\text{max}}\) is small, the approximation error is the main source of error; when \(\ell_{\text{max}}\) becomes larger, the total error is dominated by Laplace error. Nevertheless, the N-gram model can provide good utility for a wide range of \(\ell_{\text{max}}\) values. This property makes it easier for a data holder to select a good \(\ell_{\text{max}}\) value.

A similar trade-off due to \(n_{\text{max}}\) can be observed in Figure 10, where \(\ell_{\text{max}}\) is fixed to 20. There exists an \(n_{\text{max}}\) value that minimizes the sum of approximation error and Laplace error. Due to the series of techniques we propose, the utility lost under different \(n_{\text{max}}\) values is comparable.