g
a
r
e
v
A
MSNBC
STM
0.20
0.15
0.10
0.05
0.00
20
40
60
80
100
K Value
(a) ε = 0.1
16
18
20
lmax
22
24
Figure 6: Average relative error vs. ℓmax (ε = 1.0)
s
s
o
L
y
t
i
l
i
t
U
1.0 
0.8 
0.6 
0.4 
0.2 
0.0 
s
s
o
L
y
t
i
l
i
t
U
1.0 
0.8 
0.6 
0.4 
0.2 
0.0 
N-gram
Prefix
FFS
20
40
60
80
100
K Value
(b) ε = 1.0
Figure 5: Utility loss vs. K on STM
[2] A. Blum, K. Ligett, and A. Roth. A learning theory
approach to non-interactive database privacy. In
STOC, pages 609–618, 2008.
[3] F. Bonchi, L. V. Lakshmanan, and H. W. Wang.
Trajectory anonymity in publishing personal mobility
data. SIGKDD Explorations Newsletter, 13(1):30–42,
2011.
[4] R. Chen, B. C. M. Fung, and B. C. Desai.
Diﬀerentially private trajectory data publication.
CoRR, abs/1112.2020, 2011.
[5] R. Chen, B. C. M. Fung, N. Mohammed, and B. C.
Desai. Privacy-preserving trajectory data publishing
by local suppression. Information Sciences, in press.
[6] R. Chen, N. Mohammed, B. C. M. Fung, B. C. Desai,
and L. Xiong. Publishing set-valued data via
diﬀerential privacy. PVLDB, 4(11):1087–1098, 2011.
[7] C. Dwork. Diﬀerential privacy. In ICALP, pages 1–12,
2006.
[8] C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov,
and M. Naor. Our data, ourselves: privacy via
distributed noise generation. In EUROCRYPT, pages
486–503, 2006.
[9] C. Dwork, F. McSherry, K. Nissim, and A. Smith.
Calibrating noise to sensitivity in private data
analysis. In TCC, pages 265–284, 2006.
[10] C. Dwork, M. Naor, O. Reingold, G. N. Rothblum,
and S. Vadhan. On the complexity of diﬀerentially
private data release: eﬃcient algorithms and hardness
results. In STOC, pages 381–390, 2009.
[11] M. Hardt, K. Ligett, and F. McSherry. A simple and
practical algorithm for diﬀerentially private data
release. CoRR, abs/1012.4763, 2012.
[12] H. Hu, J. Xu, S. T. On, J. Du, and J. K.-Y. Ng.
Privacy-aware location data publishing. ACM
Transactions on Database Systems, 35(3):17, 2010.
[13] C. Li, M. Hay, V. Rastogi, G. Miklau, and
A. McGregor. Optimizing linear counting queries
under diﬀerential privacy. In PODS, pages 123–134,
2010.
[14] C. Manning and H. Schutze. Foundations of Statistical
Natural Language Processing. MIT Press, 1999.
[15] F. McSherry. Privacy integrated queries: an extensible
platform for privacy-preserving data analysis. In
SIGMOD, pages 19–30, 2009.
[16] F. McSherry and R. Mahajan. Diﬀerentially private
network trace analysis. In SIGCOMM, pages 123–134,
2010.
[17] N. Mohammed, R. Chen, B. C. M. Fung, and P. S.
Yu. Diﬀerentially private data release for data mining.
In SIGKDD, pages 493–501, 2011.
[18] A. Monreale, G. Andrienko, N. Andrienko,
F. Giannotti, D. Pedreschi, S. Rinzivillo, and
S. Wrobel. Movement data anonymity through
generalization. Transactions on Data Privacy,
3(2):91–121, 2010.
[19] P. Ohm. Broken promises of privacy: Responding to
the surprising failure of anonymization. UCLA Law
Review, 2010.
[20] B. Sheridan. A trillion points of data. Newsweek,
March 2009.
[21] M. Terrovitis and N. Mamoulis. Privacy preservation
in the publication of trajectories. In MDM, pages
65–72, 2008.
[22] X. Xiao, G. Bender, M. Hay, and J. Gehrke. iReduct:
Diﬀerential privacy with reduced relative errors. In
SIGMOD, pages 229–240, 2011.
[23] X. Xiao, G. Wang, and J. Gehrke. Diﬀerential privacy
via wavelet transforms. In ICDE, pages 225–236, 2010.
[24] R. Yarovoy, F. Bonchi, L. V. S. Lakshmanan, and
W. H. Wang. Anonymizing moving objects: How to
hide a MOB in a crowd? In EDBT, pages 72–83, 2009.
APPENDIX
A. ADDITIONAL EXPERIMENTS
In this section, we present the performance of N-gram un-
der diﬀerent ℓmax and nmax values and discuss several hints
for selecting reasonable ℓmax and nmax values. In addition,
we demonstrate the utility improvement due to the approx-
imation technique proposed in Section 4.3.4.
A.1 Count Query
We examine the impact of diﬀerent parameters (i.e., ℓmax
and nmax ) of the n-gram model on average relative error
of count queries. In Figure 6, we study how relative error
changes under diﬀerent ℓmax values with ε = 1.0, nmax = 5
and query size equal to 8.
In theory, a larger ℓmax value
648r
o
r
r
E
e
v
i
t
l
a
e
R
e
g
a
r
e
v
A
MSNBC
STM
0.20
0.15
0.10
0.05
0.00
s
s
o
L
y
t
i
l
i
t
U
1.0 
0.8 
0.6 
0.4 
0.2 
0.0 
MSNBC
STM
3
4
5
nmax
6
7
16
18
20
lmax
22
24
Figure 7: Average relative error vs. nmax (ε = 1.0)
Figure 9: Utility loss vs. ℓmax (ε = 1.0)
MSNBC
STM
MSNBC-No_Approx
STM-No_Approx
s
s
o
L
y
t
i
l
i
t
U
1.0 
0.8 
0.6 
0.4 
0.2 
0.0 
MSNBC
STM
4
8
12
16
20
Query Size
(a) ε = 0.1
3
4
5
nmax
6
7
Figure 10: Utility loss vs. nmax (ε = 1.0)
0.5 
0.4 
0.3 
0.2 
0.1 
0.0 
0.5 
0.4 
0.3 
0.2 
0.1 
0.0 
r
o
r
r
E
e
v
i
t
l
a
e
R
e
g
a
r
e
v
A
r
o
r
r
E
e
v
i
t
l
a
e
R
e
g
a
r
e
v
A
MSNBC
STM
MSNBC-No_Approx
STM-No_Approx
4
8
12
16
20
Query Size
(b) ε = 1.0
Figure 8: Eﬀect of node count approximation.
allows more information of the underlying database to be
retained at the cost of higher sensitivity (and hence larger
Laplace noise). Therefore, the selection of ℓmax needs to
take into consideration the trade-oﬀ between approximation
error and Laplace error. However, in reality, a reasonable
ℓmax value could be chosen more easily because the aver-
age sequence length of many real-life datasets is relatively
small. Consequently, Laplace error is the major concern in
this case. This is conﬁrmed by Figure 6. Since most se-
quences in M SN BC and ST M are of a small length, when
ℓmax is suﬃciently large (i.e., 16), increasing ℓmax does not
signiﬁcantly lower approximation error, but simply increases
Laplace noise. Moreover, we can observe that our approach
performs relatively stable under varying ℓmax values. This
can be explained by the large counts of short grams, which
are more resistant to Laplace noise.
Figure 7 examines the performance of N-gram with re-
spect to varying nmax values, where ε = 1.0, ℓmax = 20 and
query size is 8. Similar to the selection of ℓmax , the selection
of nmax also involves the trade-oﬀ between approximation er-
ror and Laplace error. A larger nmax reduces approximation
error while increasing Laplace error. To obtain a reasonable
trade-oﬀ, we develop the adaptive privacy budget allocation
scheme and the formal choice of the threshold value, which
automatically select the best gram sizes on the ﬂy. Even
a data holder speciﬁes an unreasonably large nmax , our ap-
proach will end up with shorter grams. Therefore, it can be
observed that the performance of our solution is insensitive
to varying nmax values. From our experimental results, we
believe that, in most cases, nmax = 5 is a good choice. In
addition, we point out that a good nmax value is related to
|D| and |I|, a larger |D| or a smaller |I| suggests a larger
nmax value.
One key technique that we develop to improve accuracy
of count queries is to enforce consistency constraints by ap-
proximating the counts of the nodes that cannot pass the
threshold (Section 4.3.4). In the next set of experiments, we
demonstrate that this technique indeed improves the accu-
racy of count queries compared to the case where we naively
set the noisy counts of all nodes that cannot pass the thresh-
old to 0.
In Figure 8, we set ℓmax = 20 and nmax = 5.
MSNBC No-Approx and STM No-Approx give the relative
errors of N-gram without the approximation technique. As
we can observe, this technique improves the relative error
for all query sizes under diﬀerent ε values, up to 47%.
A.2 Frequent Sequential Pattern Mining
In the last set of experiments, we study the impact of ℓmax
and nmax on frequent sequential pattern mining. Figure 9
reports the utility loss of N-gram under diﬀerent ℓmax values
with ε = 1.0 and nmax = 5. The aforementioned trade-oﬀ
in the selection of ℓmax still applies to frequent sequential
pattern mining. This time, we can clearly observe such a
trade-oﬀ in Figure 9: when ℓmax is small, the approximate
error is the main source of error; when ℓmax becomes larger,
the total error is dominated by Laplace error. Nevertheless,
N-gram can provide good utility for a wide range of ℓmax
values. This property makes it easier for a data holder to
select a good ℓmax value.
Similar trade-oﬀ due to nmax can be observed in Figure 10,
where ℓmax is ﬁxed to 20. There exists a nmax value that
minimizes the sum of approximation error and Laplace error.
Due to the series of techniques we propose, the utility lost
under diﬀerent nmax values is comparable.
649