imizes the Weight (0.95 + 0.73 + 0.95 + 0.85 =
3.48)
It should be noted that the optimal solution may not nec-
essarily result in reconstruction of the original document.
However, if candidate probabilities have been properly as-
signed, then the optimal solution should have a large num-
ber of fragments in or almost in the right place. Hence,
it would be perhaps better for an automated document re-
assembly tool to present to the forensic analyst a small num-
ber of most likely reorderings, and based on which the cor-
rect reordering can be manually arrived at. The question that
remains is how do we assign candidate probabilities for pair
of fragments being adjacent, in an efﬁcient and meaningful
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:46:44 UTC from IEEE Xplore.  Restrictions apply. 
manner? We address this question in the following subsec-
tion by using context-based statistical models.
2.2. Context-Based Statistical Models
The model for assigning candidate probabilities for a
pair of fragments needs to be independent of language and
format. Data compression literature explores many power-
ful statistical modelling techniques to build data models to
effectively compress data. The modern data compression
paradigm, ﬁrst presented in [14], divides the compression
process into two components: modelling and coding. Mod-
elling is the process of constructing statistical representa-
tions of input data and coding is the process of mapping in-
formation generated by statistical representations to bit se-
quences to produce compressed data. Modelling represents
the critical step in a data compression system and over the
years a variety of modelling techniques have been devised.
Modelling techniques essentially build a statisti-
cal model of the input to predict the occurrence prob-
abilities of symbols. Speciﬁcally, given a realization
x1, x2, . . . , xm of a ﬁnite sequence of
random vari-
ables X1, X2, . . . , Xm over a discrete alphabet A, a model
essentially assigns a conditional probability mass func-
tion for the current symbol (event) based on previously
processed symbols [15]. In on-line applications, the se-
quence x1, x2, . . . , xm is processed in a sequential man-
ner, with the symbol xi being encoded immediately before
the symbol xi+1. In this case we need to estimate the dis-
tributions
it occurs. In particular, associated with a context model is
a ﬁnite set of contexts or conditioning events C along with
a context determining rule or function that maps the ﬁrst i
symbols (0 ≤ i < N 2) of the source sequence to some con-
text C ∈ C. The symbol xi+1 is then said to appear in the
context C. An nth−order context model uses n previous
symbols in order to estimate the probability of the next sym-
bol. However, a context model may be a blended model in
which it incorporates probability estimation based on sev-
eral different orders.
Usually the number of distinct contexts (i.e. the size of
the set C) is much smaller than the length of the source se-
quence. Associated with each context C is a probability
distribution p(x|C) that is used to encode pixel Xj when
its context is C. This pdf can be estimated by maintaining
counts of symbol occurrences within each context or by es-
timating the parameters of an assumed pdf.
Context models are especially intuitive for text compres-
sion as the probability of occurrence of a letter clearly de-
pends on its context, i.e. the immediately preceding letters.
For example, the probability of symbol ‘u’ in a document
written in English may be 1/10. However, if the preced-
ing symbol was ‘q’ then probability of ‘u’ can go up to
9/10, because ‘u’ follows ‘q’ in most English words. The
idea of context consisting of few previous symbols is in-
tuitive for models representing natural languages. Empiri-
cal results, however, show context modelling provides bet-
ter models for compressing not only natural language but
also a variety of other data formats including images, and
executables [1].
p(Xi+1 = xi+1|X1 = x1, . . . , Xi = xi), 1 ≤ i < m (3)
2.3. Our Approach
since the average number of bits needed to encode the real-
ization x1, . . . , xm online is bounded below by
m−1(cid:2)
H(Xi+1|X1, X2, . . . , Xi).
i=0
(4)
where H(·) is the Shannon (conditional) entropy function.
Coding techniques that can achieve rates close to the opti-
mal are known [14].
The conditional distribution for the r.v. Xi+1 in Equation
(3) is best estimated (in principle) by using the entire past
sequence, Xj, j = 1 . . . i. However, in practice, knowledge
about the statistics of the source sequence is nowhere as
complete. Practical compression techniques usually impose
some structural limitations on the source sequence to ar-
rive at a model that can realistically represent the sequence.
A particularly popular model that has been widely used is a
context model [13].
A context model assumes that the distribution of the cur-
rent symbol only depends on some limited context in which
Now we have all the pieces required to describe a gen-
eral approach to reassembling document fragments. First
we build an n−order context model for a given document
by accumulating the context models of individual fragments
into a single model. Using this model, for any ordered pair
of fragments consider a sliding window of size n. Place the
window at the end of the ﬁrst fragment and slide the win-
dow one position at a time into next fragment, at each po-
sition estimating the probability of the upcoming character
(n + 1) by using the characters in the window as the cur-
rent context and using the context model obtained from the
total pool of fragments for the purpose of probability esti-
mation. (See Figure 2) Continuing this process for d subse-
quent characters, where d ≤ n, gives the candidate proba-
bility for the fragment pair as given in equation (1). Repeat-
ing the process for all fragments results in a complete graph
where the edges quantify candidate probabilities of the ad-
jacency of corresponding nodes in the original document.
We then use a heuristic solution to compute a small number
of near-optimal reorderings of the fragments. The actual re-
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:46:44 UTC from IEEE Xplore.  Restrictions apply. 
ordering is likely to be contained in this set or at worst can
be easily derived from this set by a forensic analyst.
Note that even if the analyst can identify a subsequence
of any of these reorderings to be correct, she can combine
the fragments that belong to the subsequence to form a unit
fragment and repeat the process. At every iteration if the an-
alyst can successfully ﬁnd correct subsequences which can
be merged, then the process will eventually converge to the
original document.
Fragment-1
Fragment-2
abracada bracdada
Sliding Window
Size=4
Figure 2. Sliding Window of Size 4
3. Implementation & Experiments
In this section we describe an implementation of our
approach which employs a well known context modelling
technique known as Prediction by Partial Matching (PPM)
to build a context model and compute candidate probabili-
ties of the possible adjacency of two document fragments.
Also we use an alpha-beta pruning heuristic to solve the
Hamiltonian path problem and compute a small set of near-
optimal candidate reorderings. We present experimental re-
sults with different types of data which demonstrate the va-
lidity of our approach.
3.1. Prediction by Partial Matching
Prediction by partial matching or PPM, is a ﬁnite or-
der context modelling technique ﬁrst introduced in 1984
by Cleary & Witten and has become a benchmark for loss-
less data compression techniques[1]. PPM employs a suite
of ﬁxed-order context models, from 0 up to some pre-
determined maximum k, to predict the upcoming charac-
ters. For each i−order context model statistics are kept of
all symbols that have followed every i−length subsequence
observed so far in the input and number of times that each
has occurred. Prediction probabilities are computed from
these statistics in the following way: From each model,
0−order to k−order, a separate predicted probability dis-
tribution is obtained and effectively combined into a sin-
gle one. The largest-order model is the one, by default, used
to initially predict the symbol. However, if a novel sym-
bol is encountered an “escape” symbol is returned and a
smaller model is used for prediction. The process contin-
ues until one of the ﬁxed-order models predicts the upcom-
ing symbol. To ensure this process terminates a model is as-
sumed to be present (below 0−order) with all symbols in
the coding alphabet. (Note that we don’t need this order be-
cause the model already knows the entire alphabet and will
never encounter a novel symbol.) This mechanism effec-
tively blends the prediction probabilities of different order
models together in a proportion that depends on the values
used for escape probabilities.
In our model, each fragment of the document is individ-
ually processed by PPM and the resulting statistics are com-
bined to form a single model. Assuming there is no data loss
during evidence collection, we believe the resulting model
is a good statistical representation of the original document.
Suppose we used an i−order PPM model then we can use
a sliding window of size i or less and predict the upcom-
ing symbol as discussed in Section 2.3. Prediction proba-
bilities of each model are combined into a single probabil-
ity using PPMC method as described in[11]. The resulting
probability is the candidate probability of adjacency for a
pair of fragments.
3.2. Tree Pruning
As discussed in Section 2.2, the problem of ﬁnding the
optimal solution to the Hamiltonian path is intractable. As-
suming we know the ﬁrst fragment of a document we can
represent all the paths in the underlying graph by a tree (See
Figure 3). The assumption that we can identify the ﬁrst frag-
ment is practical because most ﬁles have headers at the be-
ginning that can uniquely identify the ﬁletype.
Finding the optimal solution in this tree simply means
examining each path in the tree in Figure 3 and ﬁnding the
one that maximizes the sum of candidate probabilities along
that path. However, as we can see, the tree expands expo-
nentially as the number of levels increase. In our case the
number of levels equals the number of fragments, which
can run into the hundreds. One approach to make the prob-
lem tractable is to prune this tree at every stage of expan-
sion by removing paths that do not appear to be promising.
Another way to look at this is that we prune the tree at ev-
ery level by keeping only the most promising paths such
that in the end we obtain a set of near optimal heuristic so-
lutions. The pruning approach we adopt uses this approach
and is adopted from α-β pruning used in game theory[8].
By pruning we try to avoid examining paths that we be-
lieve may not contribute enough to our solution. A naive
approach to pruning the tree is to choose a node with max-
imum candidate probability at each level. However, such a
greedy method could lead to poor solutions. Nevertheless,
the greedy method can be extended to look not only at cur-
rent level but also at β levels deep and choose a node at cur-
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:46:44 UTC from IEEE Xplore.  Restrictions apply. 
A
C
D
B
D
B
C
D
B
C
D
C
B
B
C
D
Figure 3. A Tree of Four Fragments with Path
(A, D) Pruned
rent level that maximizes the sum of candidate probabilities.
In addition, instead of choosing a single node at each level,
which limits our results to a single sequence, we choose α
best matches at each level resulting in α best sequences. We
employed this α-β tree pruning approach in our implemen-
tation and report the results obtained below.
3.3. Experiments & Results
This section presents experimental results and discussion
of the results. We used the following collection of docu-
ments in our experiments:
Type
Log Files
Source Code
Executables
Binary Files
Unformatted Text
Random Text
Samples
Log, history ﬁles of various users
C, C++, Java, Lisp source code
Executable, object code
MS Ofﬁce documents, PDF
Unformatted plain-text, chat tran-
scripts
Encrypted, compressed ﬁles
Table 1. Documents Used in the Experiments
Each document in the collection was randomly split into
several pieces (100 - 200) and a context model was built
for each document. This model was used along with prun-
ing to identify the most promising reorderings. Accuracy of
reassembly was measured by the number of adjacent frag-
ments in the reassembled documents that are in fact adjacent
in the original document. Figure 4 presents the average ac-
curacy of reassembly process for each document type and
Figure 5 presents the average compression ratio for each
type, which is an indicator of the structure of documents.
)
%
(
s
t
n