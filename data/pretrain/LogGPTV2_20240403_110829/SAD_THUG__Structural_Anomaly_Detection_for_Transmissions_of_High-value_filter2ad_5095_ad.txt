observed values for that option.
ZeusVM Web-Inject ZeusVM’s conﬁguration contains
templates that determine which and how websites visited
by an infected machine should be modiﬁed. To generate
the respective data set, we relied on the conﬁgurations
parsed as described in the previous paragraph. Likewise,
we determined how many web-injects were provided in
the live conﬁgurations and chose a random sample from
the given web-injects with a size ranging between those
numbers.
Web Exploits
target web browsers using malicious
JavaScript, HTML or other code. To simulate an at-
tack that hides this kind of data, we randomly selected
ﬁles from a collection of 2,543 malicious JavaScript ﬁles
[43].
CryLocker Payload The CryLocker ransomware [34]
exploits the imgur.com website to upload information
about each infected system. From the scarce informa-
tion available on that payload, we inferred the format and
chose reasonable values for its variables.
DuQu Payload The DuQu malware uses steganography
to exﬁltrate data from infected machines. According to
Symantec [1], its logger creates a screenshot and process
list every 30 seconds which will eventually be uploaded
to a C&C server. To create a realistic data set, we set
up a Windows virtual machine to automatically create
1156    27th USENIX Security Symposium
USENIX Association
Figure 7: The size distributions of the payloads used in our evaluation (grouped by maximum size in bytes).
a screenshot and store a process list every 30 seconds.
By running a series of ofﬁce tutorials from YouTube in
fullscreen mode on that machine, we ensured that the
screenshots are similar to those of a system being used
for regular ofﬁce tasks. We then selected random in-
tervals with a duration of at least 30 minutes, and con-
catenated the screenshots and process lists generated in a
randomly selected time frame with that length. To create
the ﬁnal DuQu payload, we compressed the data using
the bzip2 algorithm and encrypted it using the AES ci-
pher.
SyncCrypt Payload The SyncCrypt ransomware uses
JPEG ﬁles to transfer a hidden ZIP ﬁle. In that ZIP ﬁle
however, it hides the malware’s main executable along
with an HTML and a PNG ﬁle. Hence, to simulate that
payload, we randomly chose a PNG ﬁle from the respec-
tive base data set, an HTML ﬁle from the Web Exploits
set and a random malware binary from Malpedia and
stored them in a ZIP ﬁle.
Discussion In this section, we brieﬂy introduced the
payload data sets used. Figure 7 shows the distribution
of the size in bytes of the messages drawn from these
data sets for our evaluation. Since the data sets cover a
large variety, we grouped them by the size of the largest
message in them, starting with the smallest data sets in
the top left and ending with the largest on the bottom
right. To provide a point of reference, a dotted hori-
zontal line indicates the maximum density in the next
plane. The plane on the top left shows the CryLocker
and ZeusVM Conﬁguration data sets, which only con-
tain messages up to about one kilobyte and are clearly
concentrated on about 200 or 700 bytes. The complete
ZeusVM payload, including the malware’s conﬁguration
and web-injects, evenly spreads from close to zero to 250
KB. Most of the ﬁles from the Web Exploits data set are
very small, however the largest message drawn from this
set almost reaches 2 MB, as we can see in the same plane.
The SyncCrypt Payload consists, among others of a ran-
dom malware sample drawn from the Malpedia data set
and thus closely resembles the latter data set’s size dis-
tribution, as seen in the bottom left plane. Finally, the
DuQu Payload data set’s size distribution ranges from
just above 700 KB to 141.92 MB.
6.2.3 Additional Considerations for Data Sets
ZeusVM/Zberp The Zberp banking trojan is based
on the ZeusVM malware and inherited its embedding
method. Hence, for our evaluation we do not distinguish
between the two. However, we use two different data sets
to establish our approach’s efﬁcacy with respect to their
method. First, we obtained a set of 24 JPEG ﬁles contain-
ing live conﬁgurations which were extracted from dumps
of ZeusVM control panels. We denote this data set as
ZeusVM. Second, we used the leaked KINS builder2 for
the ZeusVM malware to embed conﬁgurations from our
USENIX Association
27th USENIX Security Symposium    1157
0.000.020.040.06Density020040060080010000.000000.00010Density0 MB0.5 MB1 MB1.5 MB0.0e+001.5e−053.0e−05Density0 MB5 MB10 MB15 MB20 MB25 MB30 MB0.0e+001.0e−082.0e−08Density0 MB25 MB50 MB75 MB100 MB125 MBCryLocker PayloadZeusVM ConfigurationZeusVM Config + Web−InjectWeb ExploitsMalpediaSyncCrypt PayloadDuQu PayloadPayload Data
Payload
SyncCryptPayload
Conﬁguration
W eb-Inject
CryLockerPayload
W eb-Exploits
ZeusV M
DuQu
Method
SegmentInjection
Byte Stuﬃ ng
M alpedia
ZeusV M
Append
Malware Family
Cerber
DuQu, DuQu 2.0
Hammertoss
Microcin
SyncCrypt
Tropic Trooper
ZeusVM, Zberp*
APP0 Byte Stuﬃng
APP1: Comment Injection
Encryption
XOR
AES
RC4
XOR
None
XOR
XOR
None
None
Brazilian EK
CryLocker
DNSChanger
aaAa Injection
pHYs Byte Stuﬃng
XOR
None
None
None
None
d
e
s
a
b
-
G
E
P
J
d
e
s
a
b
-
G
N
P
Table 1: Evaluation data sets; names in italics correspond to the methods proposed in section 4. The payload for the
ZeusVM, Zberp* data set is constructed by combining two payload data sets.
payload data set into randomly selected JPEG ﬁles from
the base data set. Since the builder would fail for JPEG
ﬁles that did not end with an EOI marker, we excluded
those ﬁles from the selection process. The respective
data set is called ZeusVM, Zberp* below.
DuQu The DuQu malware uses a static JPEG ﬁle stored
inside its executable to exﬁltrate data. Since this ﬁle does
not depend on the input, we created a data set indepen-
dent from our base data set. Using our DuQu payload
data set and the JPEG ﬁle used by DuQu, the 1000 ﬁles in
that set provide a very realistic approximation of DuQu’s
C&C trafﬁc.
CryLocker The method used by the CryLocker ran-
somware effectively creates a PNG ﬁle header without
any image data. Thus, it does not depend on any input
and – like DuQu – we created and use an independent
data set of 1000 ﬁles for our evaluation.
6.2.4 Grouping
To perform our evaluation, we partitioned the ﬁles in our
base data sets into ten evenly-sized groups. We then fur-
ther subdivided each JPEG group into nine subgroups
while we divided the PNG groups into ﬁve subgroups
each. As explained in section 6.1, for each step in our
cross-validation, we used nine of the ten groups as train-
ing data. The subgroups in the remaining group serve as
a test set for our classiﬁer. Here, the ﬁles in one subgroup
would remain unchanged, i.e. without any malicious em-
bedding, to allow us to establish the false positive ratio.
In the remaining subgroups, we embedded messages in
accordance with table 1 and section 6.2.3. Note that the
CryLocker, DuQu, and ZeusVM data sets do not depend
on our base data set and are thus not included in these
numbers.
6.3 Parameterization
In section 5.4 we introduced two parameters, α and τ that
allow tuning the precision and recall of our approach. To
determine a reasonable conﬁguration, we executed a sys-
tematic grid-based parameter evaluation using ten values
for each parameter and chose the parameter set that max-
imized our approach’s weighted mean true classiﬁcation
ratio. We doubled the weight for the data set without any
embedding to introduce a slight preference for a lower
false positive ratio.
For τ, we can choose any positive integer, so we opted
for the ﬁrst ten possible values, i.e. 1 through 10, to
determine whether there exists a local optimum in this
range. α can take any positive real value. However, we
argue that very large values for α would make the ap-
proach overly permissive. E.g. with a value of 1, all
lengths from 0 up to twice the given length would support
the legitimacy of the observed ﬁle. Hence, an attacker
could simply create a very large segment and be sure
that it would be supported by the model, if it appeared
in the correct order. Thus, we select 0.5 as a reasonable
upper bound for α. From this starting point, we chose
10 evenly distributed values, i.e. set α to 0.05, 0.1 etc.
up to and including 0.5. Following this methodology, for
JPEG ﬁles the most restrictive conﬁguration τ = 10 and
α = 0.05 scored best. For PNG ﬁles we chose the con-
ﬁguration τ = 2 and α = 0.1.
1158    27th USENIX Security Symposium
USENIX Association
6.4 Stegdetect: Append and Invisible Se-
crets
Provos and Honeyman published several papers on the
topic of hiding messages in JPEG ﬁles and detecting
such embeddings, which we brieﬂy discuss in section 8.
While their work focussed on detecting hidden messages
in image data,
the reference implementation of their
Stegdetect [48] tool also contains two methods called
append and invisible secrets. The ﬁrst method checks
whether a ﬁle contains at least 4 additional bytes behind
the end of the image data. The invisible secrets method
on the other hand checks whether a comment segment
starts with an integer reﬂecting the length of the follow-
ing payload. We disabled all other detection methods to
avoid triggering unnecessary false positives. However,
their implementation was unable to parse a signiﬁcant
fraction of the ﬁles in the test sets. We include the frac-
tion that could not be handled as error in our comparison
to allow our readers to account for these ﬁles.
6.5 Results
The left plot in ﬁg. 8 indicates the detection perfor-
mance of both SAD THUG, indicated by green boxes,
and Stegdetect for JPEG ﬁles. Only SAD THUG is able
to process PNG ﬁles and thus the right hand side of ﬁg. 8
shows results solely for our approach. For Stegdetect, we
show the true classiﬁcation ratio using blue boxes and the
error ratio, as explained in section 6.4, in red. Given that
all values are close to either 0 or 1, we split the graph into
an upper and a lower part. The upper part contains the
upper 6% range while the lower part contains the lower
6% range, respectively. There were no observations in
between these intervals.
As indicated by ﬁg. 8 a), the worst true negative ratio
SAD THUG achieved for JPEG ﬁles was 99.33% with
a maximum of 99.59% and mean 99.48%. Stegdetect
on the other hand achieved a mean true negative ratio of
95.45%. This is due to the fact that a surprisingly large
number of the JPEG ﬁles in our base data set contain data
appended behind their EOI marker, as discussed in sec-
tion 6.2.1. SAD THUG implicitly compensates for this,
resulting in a far better true negative ratio than Stegde-
tect. However, as a side effect, SAD THUG also accepts
some ﬁles that contain a message added using the ap-
pend paradigm. In section 9.2, we discuss how this can
be ﬁxed easily. While we expected Stegdetect to classify
ﬁles with append-based embeddings perfectly, ranging
from Cerber to Tropic Trooper in ﬁg. 8, it does not. How-
ever, the difference is explained by its failure to parse a
signiﬁcant fraction of the ﬁles and is thus, on its own, not
indicative of a shortcoming of the method.
The picture changes once we consider the remaining
methods. Here, SAD THUG achieves a 100% true pos-
itive ratio while Stegdetect does not detect any ZeusVM
ﬁle and a parsing error triggers its only true positive for
the ZeusVM/Zberp* data set. As discussed above, the
ﬁles in the ZeusVM/Zberp(*) data sets always end with
an end-of-image marker and thus do not trigger Stegde-
tect’s heuristic. The APP0 and APP1: Comment data
sets on the other hand include any residual data that was
present in the ﬁles used to construct them. Hence, here
Stegdetect does not detect the actual embedding but the
residual data in the base data set. Thus, one could argue
that the 2.93% to 5.13% true positives it achieves are in
fact false positives.
On the right hand side of ﬁg. 8, we see SAD
THUG’s detection results for the PNG data sets. We
are not aware of any other approach for classifying these
ﬁles and hence cannot provide a basis for comparison.
Here, SAD THUG correctly classiﬁes all ﬁles across
all cross-validation steps for all except two data sets.
For the Brazilian EK’s method, which uses the append
paradigm, results are again distorted by residual data
present in the base data set. Here, up to 4.85% of the
ﬁles are incorrectly classiﬁed as benign with a mean true
classiﬁcation ratio of 96.59%. At the same time, SAD
THUG achieves a mean true positive ratio of 98.88%.
There was no obvious pattern with respect to what ﬁles
caused the usually single digit count of false positives in
each group.
To summarize, SAD THUG achieves very high true
It
classiﬁcation ratios for both JPEG and PNG ﬁles.
classiﬁes several data sets perfectly but is somewhat im-
peded with respect to append-based methods by the pres-
ence of a large number of ﬁles with residual data in our
training data. Here, the worst true classiﬁcation ratios is
95.15% while the overall average ratios are 99.25% for
both JPEG and PNG ﬁles. Stegdetect on the other hand
scores well for append-based methods but fails to detect
methods relying on other paradigms. Additionally and in