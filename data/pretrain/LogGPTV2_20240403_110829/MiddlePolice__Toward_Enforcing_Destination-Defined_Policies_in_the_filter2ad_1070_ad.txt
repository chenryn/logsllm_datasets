dlePolice policies diÔ¨Äerent numbers of senders.
tational overhead. As evaluated in ¬ß8.2.1, the per-packet
processing latency overhead caused by capability computa-
tion is ‚àº1.4 ¬µs, which is negligible compared with typical
Internet RTTs. Thus, MiddlePolice has latency almost iden-
tical to the existing cloud-based DDoS mitigation.
8.2 Testbed Experiments
8.2.1 TrafÔ¨Åc Policing Overhead
In this section, we evaluate the traÔ¨Éc policing overhead
on our testbed. We organize three servers as one sender, one
mbox and one receiver. All servers, shipped with a quad-core
Intel 2.8GHz CPU, run the 3.13.0 Linux kernel. The mbox
is installed with multiple Gigabit NICs to connect both the
sender and receiver. A long TCP Ô¨Çow is established be-
tween the sender and receiver, via the mbox, to measure
the throughput. To emulate a large number of sources, the
mbox creates an iTable with N entries. Each packet from the
sender triggers a table look up for a random entry. We im-
plement a two-level hash table in the kernel space to reduce
the look up latency. Then the mbox generates a capability
based on the obtained entry.
Figure 4 shows the measured throughput and goodput un-
der various N . The goodput is computed by subtracting the
additional header and capability size from the total packet
size. The baseline throughput is obtained without Middle-
Police. Overall, the policing overhead in high speed network
is small. When a single mbox deals with 100,000 sources
sending simultaneously, throughput drops by ‚àº10%. Equiv-
alently, MiddlePolice adds around 1.4 microseconds latency
to each packet processed. By replicating mboxes, the vic-
tim can distribute the workload across many mboxes when
facing large scale attacks.
8.2.2 Enforce Destination-DeÔ¨Åned Policies
We now evaluate MiddlePolice‚Äôs performance for enforc-
ing victim-deÔ¨Åned policies, along with the eÔ¨Äectiveness of
Ô¨Åltering bypassing traÔ¨Éc. This section evaluates pure Mid-
dlePolice.
In reality, once built into cloud-based systems,
MiddlePolice needs only to process traÔ¨Éc that passes their
pre-deployed defense.
Testbed Topology. Figure 5 illustrates the network topol-
ogy, including a single-homed victim AS purchasing 1Gbps
bandwidth from its ISP, an mbox and 10 access ASes. The
ISP is emulated by a Pronto-3297 48-port Gigabit switch to
support packet Ô¨Åltering. The mbox is deployed on a server
with multiple Gigabit NICs, and each access AS is deployed
on a server with a single NIC. We add 100ms latency at the
victim via Linux traÔ¨Éc control to emulate the typical Inter-
net RTT. To emulate large scale attacks, 9 ASes are com-
promised. Attackers adopt a hybrid attack proÔ¨Åle: 6 attack
Figure 5. Testbed network topology.
Figure 6. [Testbed] Packet Ô¨Åltering via ACL.
ASes directly send large volumes of traÔ¨Éc to the victim,
emulating ampliÔ¨Åcation-based attacks, and the remaining
attack ASes route traÔ¨Éc through the mbox. Thus, the total
volume of attack traÔ¨Éc is 9 times as much as the victim‚Äôs
bottleneck link capacity. Both the inbound and outbound
points of the mbox are provisioned with 4Gbps bandwidth
to ensure the mbox is not the bottleneck, emulating that the
mbox is hosted in the cloud.
Packet Filtering. We Ô¨Årst show the eÔ¨Äectiveness of the
packet Ô¨Ålter. Six attack ASes spoof the mbox‚Äôs source ad-
dress and send 6Gbps UDP traÔ¨Éc to the victim. The attack
ASes scan all possible UDP port numbers to guess the shared
secret. Figure 6 shows the volume of attack traÔ¨Éc bypass-
ing the mbox and its volume received by the victim. As the
chance of a correct guess is very small, the Ô¨Ålter can eÔ¨Äec-
tively stop the bypassing traÔ¨Éc from reaching the victim.
Further, even if the shared secret were stolen by attackers
at time ts, the CHM would suddenly receive large numbers
of packets without valid capabilities. Since packets travers-
ing the mbox carry capabilities, the CHM realizes that the
upstream Ô¨Åltering has been compromised. The victim then
re-conÔ¨Ågures the ACL using a new secret to recover from key
compromise. The ACL is eÔ¨Äective within few milliseconds
after reconÔ¨Åguration. Thus, the packet Ô¨Åltering mechanism
can promptly react to a compromised secret.
NaturalShare and PerASFairshare Policies.
In this sec-
tion, we Ô¨Årst show that the mbox can enforce the Natural-
Share and PerASFairshare policies. We use the default pa-
rameter setting in Table 2, and defer detailed parameter
study in ¬ß8.3. Since MiddlePolice conditionally allows an AS
to send faster than its WR, we use the window size, deÔ¨Åned
as the larger value between an AS‚Äôs WR and its delivered
packets to the victim, as the performance metric. For clear
presentation, we normalize the window size to the maximum
number of 1.5KB packets deliverable through a 1Gbps link
in one detection period. We do not translate window sizes
to throughput because packet sizes vary.
Attackers adopt two representative strategies:
(i) they
send Ô¨Çat rates regardless of packet losses, and (ii) they dy-
namically adjust their rates based on packet losses (reac-
tive attacks). To launch Ô¨Çat-rate attacks, the attackers keep
sending UDP traÔ¨Éc to the victim. The CHM uses a dedi-
 0 200 400 600 800 10 20 30 40 50 60 70 80 90 100Bandwidth (Mbps)The number of senders (K)GoodputThroughputBaselinemboxISPVictimLegitimate ASAttackers‚Ä¶Bottleneck	01234567Traffic	Volume	(Gbps)Attack	Traffic	bypassing	the	mboxBypassingtraffic	received	by	the	victimKey	stolenùíïùíîACLreconfig(a) Window sizes in Ô¨Çat-attacks.
(b) LLRs in Ô¨Çat-attacks.
(c) Window sizes in reactive att. (d) LLRs in reactive attacks.
Figure 7. [Testbed] Enforcing the NaturalShare policy. The legitimate AS gradually obtains a certain amount of bandwidth
under Ô¨Çat-rate attacks since attackers‚Äô window sizes drop consistently over time (Figure 7(a)) due to their high LLRs (Figure
7(b)). However, the attack ASes can consume over 95% of the bottleneck bandwidth via reactive attacks (Figure 7(c)) while
maintaining low LLRs similar to the legitimate AS‚Äôs LLR (Figure 7(d)).
(a) Window sizes in Ô¨Çat-attacks.
(b) LLRs in Ô¨Çat-attacks.
(c) Window sizes in reactive att. (d) LLRs in reactive attacks.
[Testbed] Enforcing the PerASFairshare policy. The legitimate AS can obtain at least the per-AS fair rate at
Figure 8.
the bottleneck regardless of the attack strategies (Figures 8(a) and 8(c)). Further, the legitimate AS gains slightly more
bandwidth than the attackers under Ô¨Çat-rate attacks as the attack ASes have large LLRs (Figure 8(b)).
cated Ô¨Çow to return received capabilities to the mbox since
no ACK packets are generated for UDP traÔ¨Éc. One way
of launching reactive attacks is that the attackers simul-
taneously maintain many more TCP Ô¨Çows than the legiti-
mate AS. Such a many-to-one communication pattern allows
the attackers to occupy almost the entire bottleneck, even
through each single Ô¨Çow seems completely ‚Äúlegitimate‚Äù.
The legitimate AS always communicates with the victim
via a long-lived TCP connection.
Figure 7 shows the results for the NaturalShare policy. As
the bottleneck is Ô¨Çooded by attack traÔ¨Éc, the legitimate AS
is forced to enter timeout at the beginning, as illustrated in
Figure 7(a). The attackers‚Äô window sizes are decreasing over
time, which can be explained via Figure 7(b). As the volume
of attack traÔ¨Éc is well above the bottleneck‚Äôs capacity, all
attack ASes‚Äô LLRs are well above Thdrop
. Thus, the mbox
drops all their best-eÔ¨Äort packets. As a result, when one
attack AS‚Äôs window size is W (t) in detection period t, then
W (t+1) ‚â§ W (t) since in period t+1 any packet sent beyond
W (t) is dropped. Further, any new packet losses from the
attack AS, caused by an overÔ¨Çow at the bottleneck buÔ¨Äer,
will further reduce W (t + 1). Therefore, all attack ASes‚Äô
window sizes are consistently decreasing over time, creating
spare bandwidth at the bottleneck for the legitimate AS. As
showed in Figure 7(a), the legitimate AS gradually recovers
from timeouts.
slr
The NaturalShare policy, however, cannot well protect the
legitimate AS if the attackers adopt the reactive attack strat-
egy. By adjusting the sending rates based on packet losses,
the attack ASes can keep their LLRs low enough to regain
the advantage of delivering best-eÔ¨Äort packets. Meanwhile,
Figure 9. [Testbed] MiddlePolice ensures that the premium
client (AS A) receives consistent bandwidth.
they can gain much more bandwidth by initiating more TCP
Ô¨Çows. Figure 7(c) shows the window sizes when each at-
tack AS starts 200 TCP Ô¨Çows whereas the legitimate AS
has only one. The attackers consume over 95% of the bot-
tleneck bandwidth, while keeping low LLRs similar to that
of the legitimate AS (Figure 7(d)).
Figure 8 shows the results for the PerASFairshare policy.
Figures 8(a) and 8(c) demonstrate that the legitimate AS
receives at least per-AS fair rate at the bottleneck regard-
less of the attack strategies, overcoming the shortcomings
of the NaturalShare policy. Further, under Ô¨Çat-rate attacks,
the legitimate AS has slightly larger window sizes than the
attackers since, again, the mbox does not accept any best-
eÔ¨Äort packets from the attackers due to their high LLRs (as
showed in Figure 8(b)).
PremiumClientSupport Policy. This section evaluates the
PremiumClientSupport policy. We consider a legitimate AS
(AS A) that is a premium client which reserves half of the
bottleneck bandwidth. Figure 9 plots AS A‚Äôs bandwidth
 0 0.1 0.2 0.3 0 20 40 60 80 100Normalized window sizeTime (s)The legitimate ASThe 1st attack ASThe 2nd attack ASThe 3rd attack AS 0 20 40 60 0 20 40 60 80 100LLR (%)Time (s)The legitimate ASThe 1st attack ASThe 2nd attack ASThe 3rd attack AS 0 0.1 0.2 0.3 0.4 0 20 40 60 80 100Normalized window sizeTime (s)The legitimate ASThe 1st attack ASThe 2nd attack ASThe 3rd attack AS 0 1 2 3 4 5 0 20 40 60 80 100LLR (%)Time (s)The legitimate ASThe 1st attack ASThe 2nd attack ASThe 3rd attack AS 0 0.1 0.2 0.3 0 20 40 60 80 100Normalized window sizeTime (s)The legitimate ASThe 1st attack ASThe 2nd attack ASThe 3rd attack AS 0 20 40 60 80 0 20 40 60 80 100LLR (%)Time (s)The legitimate ASThe 1st attack ASThe 2nd attack ASThe 3rd attack AS 0 0.1 0.2 0.3 0 20 40 60 80 100Normalized window sizeTime (s)The legitimate ASThe 1st attack ASThe 2nd attack ASThe 3rd attack AS 0 1 2 3 4 5 0 20 40 60 80 100LLR (%)Time (s)The legitimate ASThe 1st attack ASThe 2nd attack ASThe 3rd attack AS	0100200300400500600700120100100020003000AS	A's	Bandwidth	(Mbps)The	number	of	senders	from	AS	BPremium	SupportNo	Protection(a) NaturalShare.
(b) PerSenderFairshare.
(c) Jain‚Äôs fairness index (FI).
(d) FI for various mbox counts.
Figure 10. [Simulation] Evaluating NaturalShare & PerSenderFairshare in large scale. Figures 10(a) and 10(b) show that the
clients‚Äô average window size is larger than that of the attackers under both Ô¨Çat-rate and shrew attacks. Figure 10(c) proves
that the clients‚Äô window sizes converge to fairness in the PerSenderFairshare policy. Figure 10(d) shows that MiddlePolice can
enforce strong fairness among all senders even without coordination among the mboxes.
when the number of senders from the attack ASes increases.
With the PremiumClientSupport policy, MiddlePolice ensures
AS A receives consistent bandwidth regardless of the number
of senders from the attack ASes. However, without such a
policy, the attack ASes can selÔ¨Åshly take away the majority
of bottleneck bandwidth by involving more senders.
8.3 Large Scale Evaluation
In this section, we further evaluate MiddlePolice via large
scale simulations on ns-3 [3]. We desire to emulate real-world
DDoS attacks in which up to millions of bots Ô¨Çood a victim.
To circumvent the scalability problem of ns-3 at such a scale,
we adopt the same approach in NetFence [35], i.e., by Ô¨Åx-
ing the number of nodes (‚àº5000) and scaling down the link
capacity proportionally, we can simulate attack scenarios
where 1 million to 10 million attackers Ô¨Çood a 40Gbps link.
The simulation topology is similar to the testbed topology,
except that all attackers are connected to the mbox.
Besides the Ô¨Çat-rate attacks and reactive attacks, we also
consider the on-oÔ¨Ä shrew attacks [31] in the simulations.
Both the on-period and oÔ¨Ä-period in shrew attacks are 1s.
The number of attackers is 10 times larger than that of
legitimate clients.
In Ô¨Çat-rate attacks and shrew attacks,
the attack traÔ¨Éc volume is 3 times larger than the capacity
of the bottleneck. In reactive attacks, each attacker opens
10 connections, whereas a client has one. The bottleneck
router buÔ¨Äer size is determined based on [10], and the RTT
is 100ms.
NaturalShare & PerSenderFairshare in Scale. Figure 10
shows the results for enforcing NaturalShare and PerSender-
Fairshare policies with default parameter settings. We plot
the ratio of clients‚Äô average window size to attackers‚Äô aver-
age window size for the NaturalShare policy in Figure 10(a).
For Ô¨Çat-rate attacks and shrew attacks, it may be surprising
that the clients‚Äô average window size is larger than that of
the attackers. Detailed trace analysis shows that it is be-
cause that the window sizes of a large portion of attackers
keep decreasing, as we explained in our testbed experiment.
As the number of attackers is much larger than the client
count, the attackers‚Äô average window size turns out to be
smaller than that of the clients, although the absolute vol-
ume of attack traÔ¨Éc may be still higher. Under reactive
attacks, the clients‚Äô average window size (almost zero) is
too small to be plotted in Figure 10(a).
Figure 10(b) shows that the clients enjoy even larger win-
dow ratio gains under the PerSenderFairshare policy in Ô¨Çat-
rate and shrew attacks because even more attackers enter
Figure 11.
reÔ¨Çects whether two mboxes share a bottleneck.
[Simulation] The SLR correlation coeÔ¨Écient
the window dropping mode. Further, the PerSenderFairshare
ensures that the clients‚Äô average window size is close to the
per-sender fair rate in reactive attacks. Figure 10(c) demon-
strates that each client‚Äôs window size converges to per-client
fairness as Jain‚Äôs fairness index [16] (FI) is close to 1.
mbox Coordination. To enforce global per-sender fairness,
the mboxes sharing the same bottleneck link share their lo-
cal observations (¬ß4.3.5). We Ô¨Årst investigate how bad the
FI can be without such inter-mbox coordination. We recon-
struct the topology to create multiple mboxes, and map each
client to a random mbox. The attackers launch reactive at-
tacks. The results, plotted in Figure 10(d), show that the
FI drops slightly, by ‚àº8%, even if 20 mboxes make local rate
allocations without any coordination among them.
To complete our design, we further propose the following
co-bottleneck detection mechanism. The design rationale
is that if two mboxes‚Äô SLR observations are correlated, they
share a bottleneck with high probability. To validate this, we
rebuild the network topology to create the scenarios where
two mboxes share and do not share a bottleneck, and study
the correlation coeÔ¨Écient of their SLRs. We compute one co-
eÔ¨Écient for every 100 SLR measurements from each mbox.
Figure 11 shows the CDF of the coeÔ¨Écient. Clearly, the
coeÔ¨Écient reÔ¨Çects whether the two mboxes share a bottle-
neck. Thus, by continuously observing such correlation be-
tween two mboxes‚Äô SLRs, MiddlePolice can determine with
increasing certainty whether or not they share a bottleneck,
and can conÔ¨Ågure their coordination accordingly.
Parameter Study. We evaluate MiddlePolice using dif-
ferent parameters than the default values in Table 2. We
mainly focus on Dp, Thdrop
and Œ≤. For each parameter, we
vary its value to obtain the clients‚Äô average window size un-
der the 10-million bot attack. The results showed in Table
5 are normalized to the window sizes obtained using the
default parameters in Table 2.
slr
 0 1 2 3 4 5 2 4 6 8 10Average window ratio (log2)The number of attackers (million)Flat-rate attacksShrew attacksWindow gain over flatWindow gain over shrew 0 1 2 3 4 5 2 4 6 8 10Average window ratio (log2)The number of attackers (million)Flat-rate attacksShrew attacksReactive attacksWindow gain over flatWindow gain over shrew 0 0.2 0.4 0.6 0.8 1 2 4 6 8 10Jain‚Äôs fairness indexThe number of attackers (million)Flat-rate attacksShrew attacksReactive attacks 0 0.2 0.4 0.6 0.8 1 2 4 6 8 10Jain‚Äôs fairness indexThe number of attackers (million)Single mbox10 mboxes20 mboxes 0 20 40 60 80 100-0.4-0.2 0 0.2 0.4 0.6 0.8 1CDF (%)SLR correlation coefficientCo-bottleneckDiff. bottlenecksPushback [37]
SIFF [50], TVA [51]
Netfence [35]
Phalanx [18] Mirage [38]
SIBRA [13]
MiddlePolice
Source upgrades
Dest. upgrades
AS deployment
No
No
Unrelated
Router support
O(N ) states
Fairness regimes
Other
requirements
None
None
Yes
Yes
Unrelated
Cryptography;
O(N ) states for [51]
None
New header
Yes
Yes
Yes
Yes
Unrelated
Unrelated
O(N ) states
O(N ) states;
Cryptography
Per-sender
New header;
Passport [33]
Yes
Yes
Related
Larger
memory
Puzzle;
Yes