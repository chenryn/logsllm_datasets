Hi,
We are suffering a big performance hit with druid due to a dimension in our
data source which has a very high cardinality. We need that dimension but only
if there are significant rows with that dimension, i.e. we should
ignore/discard cases where this dimension with a unique value only appears
once.
We thought about doing a two-step process: as we are inserting in realtime
from Storm, when we receive the data we don't know if it's first appear or
not, so we are inserting all, and once segment is closed after an hour, we
execute a second query in a HIVE view to group manually the ones that only
appear one time under something like "Unknown", and insert them again. The
problem with this is that during the current hour there is still a degradation
performance issue, but at least it works fine for data outside the realtime
segment.
This is an in-house provisional solution, but we would like to know if someone
can help with a more elegant one.
We are using version 0.9.2.
Thanks for your time.