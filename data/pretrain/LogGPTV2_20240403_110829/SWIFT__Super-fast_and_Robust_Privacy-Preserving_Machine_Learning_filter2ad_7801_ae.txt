(γxiγyi + ψi) + χ
(γxiγyi + ψi) + [χ]1 + [χ]2 = f2 + f1 + f0.
=
=
i=1
n∑
n∑
i=1
i=1(γxiγyi + ψi), f1 = [χ]1 and f0 = [χ]2.
where f2 = ∑n
Using the above relation, the preprocessing phase proceeds
as follows: P0,Pj for j ∈ {1,2} sample a random [αz] j ∈
Z2(cid:96), while P1,P2 sample random γz. Servers locally prepare
(cid:104)(cid:126)d(cid:105),(cid:104)(cid:126)e(cid:105) similar to that of multiplication protocol. Servers
then execute a robust 3PC dot product protocol, denoted by
ΠdotpPre (ideal functionality for the same appears in Fig. 11),
that takes (cid:104)(cid:126)d(cid:105),(cid:104)(cid:126)e(cid:105) as input and compute (cid:104)f(cid:105) with f = (cid:126)d(cid:12)(cid:126)e.
Given (cid:104)f(cid:105), the ψ and [χ] values are extracted as follows (ref.
Eq. 4):
[χ]1 = f1,
[χ]2 = f0,
(4)
ψ = f2 − n∑
γxiγyi,
i=1
It is easy to see from the semantics of (cid:104)·(cid:105)-sharing that both
P1,P2 obtain f2 and hence ψ. Similarly, both P0,P1 obtain f1
and hence [χ]1, while P0,P2 obtain [χ]2.
A trivial way to instantiate ΠdotpPre is to treat a dot product
operation as n multiplications. However, this results in a com-
munication cost that is linearly dependent on the feature size.
Instead, we instantiate ΠdotpPre by a semi-honest dot product
USENIX Association
30th USENIX Security Symposium    2659
protocol followed by a veriﬁcation phase to check the cor-
rectness. For the veriﬁcation phase, we extend the techniques
of [8, 9] to provide support for veriﬁcation of dot product
tuples. Setting the veriﬁcation phase parameters appropriately
gives a ΠdotpPre whose (amortized) communication cost is
independent of the feature size.
Truncation Working over ﬁxed-point values, repeated mul-
tiplications using FPA arithmetic can lead to an overﬂow
resulting in loss of signiﬁcant bits of information. This put
forth the need for truncation [11, 14, 43, 45, 48] that re-adjusts
the shares after multiplication so that FPA semantics are main-
tained. As shown in SecureML [45], the method of truncation
would result in loss of information on the least signiﬁcant bits
and affect the accuracy by a very minimal amount.
For truncation, servers execute Πtrgen (Fig. 12) to generate
([r] ,(cid:74)rd(cid:75))-pair, where r is a random ring element, and rd is
the truncated value of r, i.e the value r right-shifted by d bit
positions. Recall that d denotes the number of bits allocated
for the fractional part in the FPA representation. Given (r, rd),
the truncated value of v, denoted as vd, is computed as vd =
(v− r)d + rd. The correctness and accuracy of this method
was shown in ABY3 [43].
Protocol Πtrgen is inspired from [15, 43] and proceeds as
follows to generate ([r] ,(cid:74)rd(cid:75)). Analogous to the approach
of ABY3 [43], servers generate a boolean sharing of an (cid:96)-
bit value r = r1 ⊕ r2, non-interactively. Each server truncates
its share of r locally to obtain a boolean sharing of rd by
removing the lower d bits. To obtain the arithmetic shares of
(r, rd) from their boolean sharing, we do not, however, rely
on the approach of ABY3 as it requires more rounds. Instead,
we implicitly perform a boolean to arithmetic conversion, as
was proposed in Trident [15], to obtain the arithmetic shares
of (r, rd). This entails performing two dot product operations
and constitutes the cost for Πtrgen. We defer details to §B.
Dot Product with Truncation Given the(cid:74)·(cid:75)-sharing of vec-
tors(cid:126)x and(cid:126)y, protocol Πdotpt allows servers to generate(cid:74)zd(cid:75),
where zd denotes the truncated value of z =(cid:126)x(cid:12)(cid:126)y. A naive
way is to compute the dot product using Πdotp, followed by
performing truncation using the (r, rd) pair. Instead, we fol-
low the optimization of BLAZE where the online phase of
Πdotp is modiﬁed to integrate the truncation using (r, rd) at
no additional cost.
z, where z(cid:63) = β(cid:63)
The preprocessing phase now consists of the execution of
one instance of Πtrgen (Fig. 12) and the preprocessing corre-
sponding to Πdotp. In the online phase, servers enable P1,P2 to
z − αz. Using z(cid:63) − r,
obtain z(cid:63) − r instead of β(cid:63)
both P1,P2 then compute (z − r) locally, truncate it to ob-
tain (z− r)d and execute Πjsh to generate(cid:74)(z− r)d(cid:75). Finally,
servers locally compute the result as(cid:74)zd(cid:75) =(cid:74)(z− r)d(cid:75) +(cid:74)rd(cid:75).
check whether x  x j, or 1,
otherwise, followed by computing (b)B(x j − xi) + xi, which
can be performed using bit injection. To ﬁnd the maximum
value in vector(cid:126)x, the servers ﬁrst group the values in(cid:126)x into
pairs and securely compare each pair to obtain the maximum
of the two. This results in a vector of size m/2. This process is
repeated for O (logm) rounds to obtain the maximum value in
the entire vector. Convolutions, which form another important
building block in PPML tasks, can be cast into matrix mul-
tiplication. Our protocol to compute a matrix of dimension
p× r after multiplication requires only p× r multiplications.
The details appear in the full version of the paper [36].
4 Robust 4PC
In this section, we extend our 3PC results to the 4-party case
and observe substantial efﬁciency gain. First, the use of broad-
cast is eliminated. Second, the preprocessing of multiplication
becomes substantially computationally light, eliminating the
multiplication protocol (used in the preprocessing) altogether.
Third, we achieve a dot product protocol with communication
cost independent of the size of the vector, completely elim-
inating the complex machinery required as in the 3PC case.
At the heart of our 4PC constructions lies an efﬁcient 4-party
jmp primitive, denoted as jmp4, that allows two servers to
send a common value to a third server robustly. While we pro-
vide details for the protocols that vary signiﬁcantly from their
3PC counterpart in this section, the details for other protocols
along with the communication analysis and security proofs,
are deferred to the full version [36].
2660    30th USENIX Security Symposium
USENIX Association
Secret Sharing Semantics For a value v, the shares for
P0,P1 and P2 remain the same as that for 3PC case. That
is, P0 holds ([αv]1 , [αv]2 ,βv + γv) while Pi for i ∈ {1,2}
holds ([αv]i ,βv,γv). The shares for the fourth server P3 is
deﬁned as ([αv]1 , [αv]2 ,γv). Clearly, the secret is deﬁned as
v = βv − [αv]1 − [αv]2.
4PC Joint Message Passing Primitive The jmp4 primitive
enables two servers Pi, Pj to send a common value v ∈ Z2(cid:96) to a
third server Pk, or identify a TTP in case of any inconsistency.
This primitive is analogous to jmp (Fig. 1) in spirit but is sig-
niﬁcantly optimized and free from broadcast calls. Similar to
the 3PC counterpart, each server maintains a bit and Pi sends
the value, and Pj the hash of it to Pk. Pk sets its inconsistency
bit to 1 when the (value, hash) pair is inconsistent. This is
followed by relaying the bit to all the servers, who exchange it
among themselves and agree on the bit that forms majority (1
indicates the presence of inconsistency, and 0 indicates con-
sistency). The presence of an honest majority among Pi,Pj,Pl,
guarantees agreement on the presence/absence of an inconsis-
tency as conveyed by Pk. Observe that inconsistency can only
be caused either due to a corrupt sender sending an incorrect
value (or hash), or a corrupt receiver falsely announcing the
presence of inconsistency. Hence, the fourth server, Pl, can
safely be employed as TTP. The protocol appears in Fig. 4.
Protocol Πjmp4(Pi,Pj,Pk, v,Pl)
Ps ∈ P initializes an inconsistency bit bs = 0. If Ps remains silent
instead of sending bs in any of the following rounds, the recipient
sets bs to 1.
Send Phase: Pi sends v to Pk.
Verify Phase: Pj sends H(v) to Pk.
– Pk sets bk = 1 if the received values are inconsistent or if the
value is not received.
– Pk sends bk to all servers. Ps for s ∈ {i, j,l} sets bs = bk.
– Ps for s ∈ {i, j,l} mutually exchange their bits. Ps resets bs =
b(cid:48) where b(cid:48) denotes the bit which appears in majority among
bi, b j, bl.
– All servers set TTP = Pl if b(cid:48) = 1, terminate otherwise.
Figure 4: 4PC: Joint Message Passing Primitive
Notation 4.1. We say that Pi,Pj jmp4-send v to Pk when they
invoke Πjmp4(Pi,Pj,Pk, v,Pl).
We note that the end goal of jmp4 primitive relates closely
to the bi-convey primitive of FLASH [11]. Bi-convey allows
two servers S1,S2 to convey a value to a server R, and in case
of an inconsistency, a pair of honest servers mutually identify
each other, followed by exchanging their internal randomness
to recover the clear inputs, computing the circuit, and send-
ing the output to all. Note, however, that jmp4 primitive is
more efﬁcient and differs signiﬁcantly in techniques from
the bi-convey primitive. Unlike in bi-convey, in case of an
inconsistency, jmp4 enables servers to learn the TTP’s iden-
tity unanimously. Moreover, bi-convey demands that honest
servers, identiﬁed during an inconsistency, exchange their
internal randomness (which comprises of the shared keys
established during the key-setup phase) to proceed with the
computation. This enforces the need for a fresh key-setup
every time inconsistency is detected. On the efﬁciency front,
jmp4 simply halves the communication cost of bi-convey,
giving a 2× improvement.
Sharing Protocol To enable Pi to share a value v, proto-
col Πsh4 proceeds similar to that of 3PC case with the ad-
dition that P3 also samples the values [αv]1 , [αv]2 ,γv using
the shared randomness with the respective servers. On a high
level, Pi computes βv = v + [αv]1 + [αv]2 and sends βv (or
βv + γv) to another server and they together jmp4-send this
information to the intended servers.
Multiplication Protocol Given the(cid:74)·(cid:75)-shares of x and y, pro-
tocol Πmult4 (Fig. 5) allows servers to compute (cid:74)z(cid:75) with
z = xy. When compared with the state-of-the-art 4PC GOD
protocol of FLASH [11], our solution improves communica-
tion in both, the preprocessing and online phase, from 6 to 3
ring elements. Moreover, our communication cost matches
with the state-of-the-art 4PC protocol of Trident [15] that only
provides security with fairness.
Protocol Πmult4(P ,(cid:74)x(cid:75),(cid:74)y(cid:75))
Preprocessing:
– P0,P3,Pj, for j ∈ {1,2}, sample random [αz] j ∈ Z2(cid:96), and
P0,P1,P3 sample random [Γxy]1 ∈ Z2(cid:96).
– P1,P2,P3 sample random γz,ψ, r ∈ Z2(cid:96) and set [ψ]1 = r, [ψ]2 =
ψ− r.
– P0,P3 set [Γxy]2 = Γxy − [Γxy]1, where Γxy = αxαy. P0,P3
jmp4-send [Γxy]2 to P2.
– P3,Pj, for j ∈ {1,2}, set [χ] j = γx [αy] j + γy [αx] j + [Γxy] j −
[ψ] j. P1,P3 jmp4-send [χ]1 to P0, while P2,P3 jmp4-send [χ]2 to
P0.
Online:
– P0,Pj, for j ∈ {1,2}, compute [β(cid:63)
γy) [αx] j + [αz] j + [χ] j.
– P1,P0 jmp4-send [β(cid:63)
– Pj, for j ∈ {1,2}, computes β(cid:63)
β(cid:63)
z + βxβy + ψ.
– P1,P2 jmp4-send βz + γz to P0.
z]1 to P2, and P2,P0 jmp4-send [β(cid:63)
z] j = −(βx +γx) [αy] j − (βy +
z]2 to P1.
z]2 and sets βz =
z = [β(cid:63)
z]1 + [β(cid:63)
Figure 5: 4PC: Multiplication Protocol (z = x· y)
Recall that the goal of preprocessing in 3PC multiplication
was to enable P1,P2 obtain ψ, and P0,Pi for i ∈ {1,2} obtain
[χ]i where χ = γxαy + γyαx + Γxy − ψ. Here ψ is a random
value known to both P1,P2. With the help of P3, we let the
servers obtain the respective preprocessing data as follows:
P0,P3,P1 together samples random [Γxy]1 ∈ Z2(cid:96). P0,P3 locally
compute Γxy = αxαy, set [Γxy]2 = Γxy− [Γxy]1 and jmp4-send
[Γxy]2 to P2. P1,P2,P3 locally sample ψ, r and generate [·]-
shares of ψ by setting [ψ]1 = r and [ψ]2 = ψ− r. Then Pj,P3
for j ∈ {1,2} compute [χ] j = γx [αy] j + γy [αx] j + [Γxy] j −
USENIX Association
30th USENIX Security Symposium    2661
[ψ] j and jmp4-send [χ] j to P0. The online phase is similar
to that of 3PC, apart from Πjmp4 being used instead of Πjmp
for communication. Since P3 is not involved in the online
computation phase, we can safely assume P3 to serve as the
TTP for the Πjmp4 executions in the online phase.
Reconstruction Protocol Given(cid:74)v(cid:75), protocol Πrec4 enables
servers to robustly reconstruct the value v among the servers.
Note that every server lacks one share for reconstruction and
the same is available with three other servers. Hence, they
communicate the missing share among themselves, and the
majority value is accepted. As an optimization, two among
the three servers can send the missing share while the third
one can send a hash of the same for veriﬁcation. Notice that,
unlike 3PC, this protocol does not require commitments.
5 Applications and Benchmarking
In this section, we empirically show the practicality of our
protocols for PPML. We consider training and inference for
Logistic Regression and inference for 3 different Neural Net-
works (NN). NN training requires additional tools to allow
mixed world computations, which we leave as future work.
We refer readers to SecureML [45], ABY3 [43], BLAZE [48],
FALCON [56] for a detailed description of the training and in-
ference steps for the aforementioned ML algorithms. All our
benchmarking is done over the publicly available MNIST [39]
and CIFAR-10 [37] dataset. For training, we use a batch size
of B = 128 and deﬁne 1 KB = 8192 bits.
In 3PC, we compare our results against the best-known
framework BLAZE that provides fairness in the same setting.
We observe that the technique of making the dot product cost
independent of feature size can also be applied to BLAZE to
obtain better costs. Hence, for a fair comparison, we addition-
ally report these improved values for BLAZE. Further, we
only consider the PPA circuit based variant of bit extraction
for BLAZE since we aim for high throughput; the GC based