title:Detecting Violations of Differential Privacy
author:Zeyu Ding and
Yuxin Wang and
Guanhong Wang and
Danfeng Zhang and
Daniel Kifer
Detecting Violations of Differential Privacy
Zeyu Ding
Pennsylvania State University
PI:EMAIL
Yuxin Wang
Pennsylvania State University
PI:EMAIL
Guanhong Wang
Pennsylvania State University
PI:EMAIL
Danfeng Zhang
Pennsylvania State University
PI:EMAIL
Daniel Kifer
Pennsylvania State University
PI:EMAIL
9
1
0
2
p
e
S
5
]
R
C
.
s
c
[
5
v
7
7
2
0
1
.
5
0
8
1
:
v
i
X
r
a
ABSTRACT
The widespread acceptance of differential privacy has led to the
publication of many sophisticated algorithms for protecting privacy.
However, due to the subtle nature of this privacy definition, many
such algorithms have bugs that make them violate their claimed
privacy. In this paper, we consider the problem of producing coun-
terexamples for such incorrect algorithms. The counterexamples
are designed to be short and human-understandable so that the
counterexample generator can be used in the development process
– a developer could quickly explore variations of an algorithm and
investigate where they break down. Our approach is statistical in
nature. It runs a candidate algorithm many times and uses statistical
tests to try to detect violations of differential privacy. An evalua-
tion on a variety of incorrect published algorithms validates the
usefulness of our approach: it correctly rejects incorrect algorithms
and provides counterexamples for them within a few seconds.
CCS CONCEPTS
• Security and privacy → Privacy protections.
KEYWORDS
Differential privacy; counterexample detection; statistical testing
ACM Reference Format:
Zeyu Ding, Yuxin Wang, Guanhong Wang, Danfeng Zhang, and Daniel Kifer.
2018. Detecting Violations of Differential Privacy. In 2018 ACM SIGSAC
Conference on Computer and Communications Security (CCS ’18), October
15–19, 2018, Toronto, ON, Canada. ACM, New York, NY, USA, 15 pages.
https://doi.org/10.1145/3243734.3243818
1 INTRODUCTION
Differential privacy has become a de facto standard for extracting
information from a dataset (e.g., answering queries, building ma-
chine learning models, etc.) while protecting the confidentiality
of individuals whose data are collected. Implemented correctly, it
guarantees that any individual’s record has very little influence on
the output of the algorithm.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’18, October 15–19, 2018, Toronto, ON, Canada
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5693-0/18/10...$15.00
https://doi.org/10.1145/3243734.3243818
However, the design of differentially private algorithms is very
subtle and error-prone – it is well-known that a large number of
published algorithms are incorrect (i.e. they violate differential
privacy). A sign of this problem is the existence of papers that
are solely designed to point out errors in other papers [12, 28].
The problem is not limited to novices who may not understand
the subtleties of differential privacy; it even affects experts whose
goal is to design sophisticated algorithms for accurately releasing
statistics about data while preserving privacy.
There are two main approaches to tackling this prevalence of
bugs: programming platforms and verification. Programming plat-
forms, such as PINQ [29], Airavat [37], and GUPT [32] provide a
small set of primitive operations that can be used as building blocks
of algorithms for differential privacy. They make it easy to cre-
ate correct differentially private algorithms at the cost of accuracy
(the resulting privacy-preserving query answers and models can
become less accurate). Verification techniques, on the other hand,
allow programmers to implement a wider variety of algorithms and
verify proofs of correctness (written by the developers) [2–7] or
synthesize most (or all) of the proofs [1, 24, 35, 41].
In this paper, we take a different approach: finding bugs that
cause algorithms to violate differential privacy, and generating
counterexamples that illustrate these violations. We envision that
such a counterexample generator would be useful in the develop-
ment cycle – variations of an algorithm can be quickly evaluated
and buggy versions could be discarded (without wasting the devel-
oper’s time in a manual search for counterexamples or a doomed
search for a correctness proof). Furthermore, counterexamples can
help developers understand why their algorithms fail to satisfy
differential privacy and thus can help them fix the problems. This
feature is absent in all existing programming platforms and veri-
fication tools. To the best of our knowledge, this is the first paper
that treats the problem of detecting counterexamples in incorrect
implementations of differential privacy.
Although recent work on relational symbolic execution [22] aims
for simpler versions of this task (like detecting incorrect calcula-
tions of sensitivity), it is not yet powerful enough to reason about
probabilistic computations. Hence, it cannot detect counterexam-
ples in sophisticated algorithms like the sparse vector technique [19],
which satisfies differential privacy but is notorious for having many
incorrect published variations [12, 28].
Our counterexample generator is designed to function in black-
box mode as much as possible. That is, it executes code with a
variety of inputs and analyzes the (distribution of) outputs of the
code. This allows developers to use their preferred languages and
libraries as much as possible; in contrast, most language-based
CCS ’18, October 15–19, 2018, Toronto, ON, Canada
Zeyu Ding, Yuxin Wang, Guanhong Wang, Danfeng Zhang, and Daniel Kifer
tools will restrict developers to specific programming languages
and a very small set of libraries. In some instances, the code may
include some tuning parameters. In those cases, we can use an
optional symbolic execution model (our current implementation
analyzes python code) to find values of those parameters that make
it easier to detect counterexamples. Thus, we refer to our method
as a semi-black-box approach.
Our contributions are as follows:
• We present the first counterexample generator for differen-
tial privacy. It treats programs as semi-black-boxes and uses
statistical tests to detect violations of differential privacy.
• We evaluate our counterexample generator on a variety of so-
phisticated differentially private algorithms and their common
incorrect variations. These include the sparse vector method
and noisy max [19], which are cited as the most challenging
algorithms that have been formally verified so far [1, 5]. In
particular, the sparse vector technique is notorious for having
many incorrect published variations [12, 28]. We also evalu-
ate the counterexample generator on some simpler algorithms
such as the histogram algorithm [14], which are also easy for
novices to get wrong (by accidentally using too little noise). In
all cases, our counterexample generator produces counterex-
amples for incorrect versions of the algorithms, thus showing
its usefulness to both experts and novices.
• The false positive error (i.e. generating "counterexamples" for
correct code) of our algorithm is controllable because it is based
on statistical testing. The false positive rate can be made arbi-
trarily small just by giving the algorithm more time to run.
Limitations: it is impossible to create counterexample/bug detec-
tor that works for all programs. For this reason, our counterexample
generator is not intended to be used in an adversarial setting (where
a rogue developer wants to add an algorithm that appears to satisfy
differential privacy but has a back door). In particular, if a program
satisfies differential privacy except with an extremely small prob-
ability (a setting known as approximate differential privacy [16])
then our counterexample generator may not detect it. Solving this
issue is an area for future work.
The rest of the paper is organized as follows. Related work is
discussed in Section 2. Background on differential privacy and
statistical testing is discussed in Section 3. The counterexample
generator is presented in Section 4. Experiments are presented in
Section 5. Conclusions and future work are discussed in Section 6.
2 RELATED WORK
Differential privacy. The term differential privacy covers a fam-
ily of privacy definitions that include pure ϵ-differential privacy
(the topic of this paper) [17] and its relaxations: approximate (ϵ, δ)-
differential privacy [16], concentrated differential privacy [8, 20],
and Renyi differential privacy [31]. The pure and approximate ver-
sions have received the most attention from algorithm designers
(e.g., see the book [19]). However, due to the lack of availability of
easy-to-use debugging and verification tools, a considerable frac-
tion of published algorithms are incorrect. In this paper, we focus
on algorithms for which there is a public record of an error (e.g.,
variants of the sparse vector method [12, 28]) or where a seemingly
small change to an algorithm breaks an important component of
the algorithm (e.g., variants of the noisy max algorithm [5, 19] and
the histogram algorithm [14]).
Programming platforms and verification tools. Several dynamic
tools [21, 29, 37, 39, 40] exist for enforcing differential privacy.
Those tools track the privacy budget consumption at runtime, and
terminates a program when the intended privacy budget is ex-
hausted. On the other hand, static methods exist for verifying
that a program obeys differential privacy during any execution,
based on relational program logic [1–7] and relational type sys-
tem [24, 35, 41]. We note that those methods are largely orthogonal
to this paper: their goal is to verify a correct program or to terminate
an incorrect one, while our goal is to detect an incorrect program
and generate counterexamples for it. The counterexamples provide
valuable guidance for fixing incorrect algorithms for algorithm de-
signers. Moreover, we believe our tool fills in the currently missing
piece in the development of differentially private algorithms: with
our tool, immature designs can first be tested for counterexamples,
before being fed into those dynamic and static tools.
Counterexample generation. Symbolic execution [9, 10, 26] is
widely used for program testing and bug finding. One attractive
feature of symbolic execution is that when a property is being vio-
lated, it generates counterexamples (i.e., program inputs) that lead
to violations. More relevant to this paper is work on testing rela-
tional properties based on symbolic execution [22, 30, 33]. However,
those work only apply to deterministic programs, but the differen-
tial privacy property inherently involves probabilistic programs,
which is beyond the scope of those work.
3 BACKGROUND
In this section, we discuss relevant background on differential pri-
vacy and hypothesis testing.
3.1 Differential Privacy
We view a database as a finite multiset of records from some domain.
It is sometimes convenient to represent a database by a histogram,
where each cell is the count of times a specific record is present.
Differential privacy relies on the notion of adjacent databases.
The two most common definitions of adjacency are: (1) two databases
D1 and D2 are adjacent if D2 can be obtained from D1 by adding
or removing a single record. (2) two databases D1 and D2 are adja-
cent if D2 can be obtained from D1 by modifying one record. The
notion of adjacency used by an algorithm must be provided to the
counterexample generator. We write D1 ∼ D2 to mean that D1 is
adjacent to D2 (under whichever definition of adjacency is relevant
in the context of a given algorithm).
We use the term mechanism to refer to an algorithm M that tries
to protect the privacy of its input. In our case, a mechanism is an
algorithm that is intended to satisfy ϵ-differential privacy:
Definition 3.1 (Differential Privacy [17]). Let ϵ ≥ 0. A mechanism
M is said to be ϵ-differentially private if for every pair of adjacent
databases D1 and D2, and every E ⊆ Range(M), we have
P(M(D1) ∈ E) ≤ eϵ · P(M(D2) ∈ E).
The value of ϵ, called the privacy budget, controls the level of
the privacy: the smaller ϵ is, the more privacy is guaranteed.
Detecting Violations of Differential Privacy
CCS ’18, October 15–19, 2018, Toronto, ON, Canada
One of the most common building blocks of differentially private
algorithms is the Laplace mechanism [17] , which is used to answer
numerical queries. Let D be the set of possible databases. A numer-
ical query is a function q : D → Rk (i.e. it outputs a k-dimensional
vector of numbers). The Laplace mechanism is based on a concept
called global sensitivity, which measures the worst-case effect one
record can have on a numerical query:
Definition 3.2 (Global Sensitivity [17]). The ℓ1-global sensitivity
of a numerical query q is
∆q = max
D1∼D2
∥q(D1) − q(D2)∥1.
The Laplace mechanism works by adding Laplace noise (having