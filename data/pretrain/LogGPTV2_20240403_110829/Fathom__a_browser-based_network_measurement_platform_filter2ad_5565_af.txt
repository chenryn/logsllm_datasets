Portability: While Fathom currently works only in Firefox, we
envision broadening support to other browsers and environments.
The Android version of Firefox shares the desktop version’s archi-
tecture and thus required only minor Fathom compatibility work.10
In Chrome [3], the implementation would require a combination
of content scripts for hooking the API into window, an extension
core for mediation between web page and OS,11 and a native bi-
nary for all remaining functionality, including invocation of com-
mands on the host. Fathom’s callback-driven API would map well
to Chrome’s architecture. Internet Explorer and Safari prove tricky;
the former will require the development of a browser helper object,
while the latter’s limited extension JavaScript API requires falling
back to a plugin component.
Incentives for adoption: Fathom provides strong incentives for
end-users, researchers and website operators.
For end-users,
Fathom’s built-in troubleshooting capabilities provide an immedi-
ate incentive to install the platform, as a means to help debug con-
nectivity problems that users would otherwise need to solve man-
ually. Our experience with Netalyzr shows that there is in fact a
desire for such debugging tools among a set of users who will re-
turn to the Netalyzr site to run its set of diagnostics, and who see
ﬁt to leave feedback [27]. As Fathom can support troubleshooting
tasks both in the presence of fundamental connectivity problems
(which preclude the use of website-driven services such Netalyzr)
as well as with the ease of visiting a webpage when connectiv-
ity permits, we believe this incentive holds at least as strongly for
Fathom as it does for Netalyzr. For researchers, Fathom’s appeal
lies in the fact that it does not require manual installation of code
speciﬁc to their experiments; once the user has installed Fathom,
any Fathom-driven experiment will work with the convenience of
visiting a web page, lowering barrier-to-entry for both users and
experimenters. For site operators, similar incentives apply: once
users have installed Fathom, both they and the site operators bene-
ﬁt from increased understanding of performance impediments.
Support for long-running measurements: Quite often experi-
menters may need to execute long running measurement tasks, such
as when studying the evolution of network topology [8, 47] or the
dynamics of network performance [5]. Fathom’s browser-based
nature prompts the question of how long users normally view a
particular page before moving on, closing its tab, or closing the
browser’s window altogether. One dataset collected by the Mozilla
TestPilot project and recording browsing behavior of 2,000 Fire-
fox users [41] provides limited insight into this aspect: in general
web surﬁng, 20% of browser windows remain open for at least
an hour, 7% for more than a day.12 With the aid of gentle re-
minders to users of experiments still actively running in a tab or
window, Fathom should thus be able to provide a reasonable level
of longer-term data collection. Fathom’s current implementation
does support execution of such long running tasks if the measure-
ment script is invoked within a FathomScript (recall in § 5.4). This
mechanism would allow independence from user interaction with
the web page (and thus immunize the experiment from potentially
reduced scheduling priorities or other service degradations imposed
by the browser when a tab does not have focus), but it would still
require the end-user to keep the measurement tab open. This is
10Fathom for the Android platform is available on http://
fathom.icsi.berkeley.edu.
11This could leverage Chrome’s upcoming support
for TCP
and UDP sockets in extensions, see http://developer.
chrome.com/trunk/apps/app_network.html.
12The dataset’s collection was limited to one week and thus forms a
lower bound for longer-running window instances.
because Fathom controls resource consumption by deallocating all
requested resources when the corresponding browser tab/window
is closed. Also, Fathom currently does not save any measurement
state in the event of browser or system shutdown. We anticipate
introducing a special API for experimenters to launch experiments
that can run in the background to avoid tying the longevity of a
measurement task to a browser tab. This approach will provide in-
dependence from accidental tab closures but requires more careful
thought regarding the resource management of long-running mea-
surement tasks. For example, we must consider (i) what experiment
state and resources must be preserved across browser or system re-
boots, and (ii) the security implications of running a background
task and providing it with access to host and web resources.
9 Related Work
We divide related work in three main areas: browser-based ap-
proaches to proﬁle page performance, server-based measurement
platforms, and efforts that piggyback measurements with applica-
tions already running on end systems.
Browser-based page performance proﬁling. A number of
browser-based approaches help page developers benchmark and
debug web page performance. The Firebug [36] extension and the
dynaTrace [14] application break down the time to load pages, but
these measurements are only accessible from the browser, not the
web page itself. As a result, web page developers cannot remotely
execute tests from real users’ browsers. The Boomerang JavaScript
library [54] overcomes this obstacle and, like Fathom, gives web
pages a vantage point directly from the end-system. Janc et al.
combined JavaScript and Flash to measure throughput and jitter
in HTTP downloads [22, 23]. Neither effort strives to provide a
generic API. While the forthcoming NavigationTiming API [52]
standardizes latency measurements by providing timestamps for
ﬁxed events during a page’s loading cycle, it does so only for the
page itself, not embedded components. Fathom furthermore adds
the ability to contrast passive timing collection with active mea-
surements. Rivet [34], another recent effort, leverages reﬂection
and dynamic characteristics of JavaScript to implement a remote
debugger for web applications. As a regular JavaScript library,
Rivet does not fundamentally add functionality to JavaScript in web
pages. Unlike Fathom, Rivet therefore cannot enable additional
passive or active measurement tasks.
Measurement platforms. Researchers often use PlanetLab [9]
as a measurement platform. More recently, Google launched M-
Lab [32] as another measurement platform. Both PlanetLab and
M-Lab are based on well-connected servers, and not on end sys-
tems as is Fathom. Scriptroute [49] added the capability for any
Internet user to run measurements using scripts in a sandboxed,
resource-limited environment in PlanetLab. We share Scriptroute’s
goal of building a community platform for network measurements.
However, our focus on taking measurements directly from end sys-
tems brings new challenges in particular: portability across operat-
ing systems and small performance overhead.
Piggybacking measurements with applications. Dasu [5] is a
Vuze plugin that analyzes BitTorrent trafﬁc to characterize ISP per-
formance. Because Dasu piggybacks on BitTorrent, it has a large
user base. BitTorrent’s frequent use for sharing pirated content,
however, can make its use problematic from a policy perspective.
We instead opt for deploying measurements on the web browser,
which is even more ubiquitous than BitTorrent. The Measurement
Manager Protocol (MGRP) [43] is an in-kernel service that runs
on end systems to reduce overhead for active measurement tools.
MGRP allows measurement tools to send probe trains where appli-
84cation packets piggyback on measurement probes. Neither MGPR
nor Dasu provide a programmable measurement platform.
10 Conclusion
In this paper, we describe Fathom, a browser extension that uses
web browser APIs to build a measurement platform. Fathom pro-
vides a programmable interface for writing and launching measure-
ments from the convenience of the web browser. We have imple-
mented a Fathom prototype for the Firefox web browser. Our eval-
uation shows that it can achieve timestamp accuracies of 1 ms, and
generally has runtime overheads of < 3.2% for popular websites.
We also demonstrate its utility in three case studies: providing a
JavaScript version of the Netalyzr network characterization tool,
debugging of web access failures, and enabling web sites to diag-
nose performance problems of their clients.
Acknowledgements: We thank the anonymous reviewers and our
shepherd, KC Claffy, for their comments and feedback on the pa-
per. This work was supported in part by the National Science Foun-
dation under grants CNS-0831535 and CNS-1111672. Any opin-
ions, ﬁndings, conclusions or recommendations expressed in this
material are those of the authors and do not necessarily reﬂect the
views of the National Science Foundation. We also thank Comcast
and Google for their support.
11 References
[1] S. Agarwal, P. Mohan, N. Liogkas, and V. Padmanabhan.
Webproﬁler: cooperative diagnosis of web failures. In Proc.
COMSNETS, 2010.
[2] G. Aggrawal, E. Bursztein, C. Jackson, and D. Boneh. An
analysis of private browsing modes in modern browsers. In
Proc. USENIX Security, 2010.
[3] A. Barth, A. Felt, P. Saxena, and A. Boodman. Protecting
browsers from extension vulnerabilities. In Proc. NDSS,
2010.
[4] S. Bauer, D. Clark, and W. Lehr. Understanding broadband
speed measurements. MITAS project white paper, 2010.
[5] Z. S. Bischof, J. S. Otto, M. A. Sánchez, J. P. Rula, D. R.
Choffnes, and F. E. Bustamante. Crowdsourcing ISP
characterization to the network edge. In Proc. ACM
SIGCOMM Workshop on Measurements Up the Stack, 2011.
[6] Bugzilla. https://bugzilla.mozilla.org/show_
bug.cgi?id=687306.
[7] M. Butkiewicz, H. V. Madhyastha, and V. Sekar.
Understanding website complexity: Measurements, metrics,
and implications. In Proc. IMC, 2011.
[8] K. Chen, D. R. Choffnes, R. Potharaju, Y. Chen, F. E.
Bustamante, D. Pei, and Y. Zhao. Where the sidewalk ends:
Extending the Internet AS graph using traceroutes from P2P
users. In Proc. CoNEXT, 2009.
[9] B. Chun, D. Culler, T. Roscoe, A. Bavier, L. Peterson,
M. Wawrzoniak, and M. Bowman. PlanetLab: An overlay
testbed for broad-coverage services. ACM SIGCOMM
Computer Communication Review, 33(3), July 2003.
[10] H. Cui and E. Biersack. Trouble shooting interactive web
sessions in a home environment. In Proc. ACM SIGCOMM
Workshop on Home Networks, 2011.
[11] L. DiCioccio, R. Teixeira, M. May, and C. Kreibich. Probe
and Pray: Using UPnP for Home Network Measurements. In
Passive and Active Measurement Conference (PAM), Vienna,
Austria, March 2012.
[12] M. Dischinger, M. Marcon, S. Guha, K. P. Gummadi,
R. Mahajan, and S. Saroiu. Glasnost: Enabling end users to
detect trafﬁc differentiation. In Proc. USENIX NSDI, 2010.
[13] J. R. Douceur, J. Elson, J. Howell, and J. R. Lorch.
Leveraging legacy code to deploy desktop applications on
the web. In Proc. USENIX OSDI, 2008.
[14] dynaTrace Software. Diagnose and prevent AJAX
performance issues. http://ajax.dynatrace.com/.
[15] P. Eckersley. How unique is your web browser? In Proc.
Privacy Enhancing Technologies Symposium (PETS), 2010.
[16] A. P. Felt, S. Egelman, M. Finifter, D. Akhawe, and
D. Wagner. How to Ask For Permission. In Proc. USENIX
Workshop on Hot Topics in Security, 2012.
[17] FireEye. Zero-day season is not over yet.
http://blog.fireeye.com/research/2012/
08/zero-day-season-is-not-over-yet.html.
[18] A. Giammarchi. An introduction to js-ctypes.
http://webreflection.blogspot.com/2011/
09/introduction-to-js-ctypes.html.
[19] Google. Google Maps help forum.
http://www.google.com/support/forum/p/
maps/thread?tid=24f446d4cc24d07a.
[20] Internet2. Network Diagnostic Tool (NDT). http://www.
internet2.edu/performance/ndt/.
[21] Iperf. http://iperf.sourceforge.net/.
[22] A. Janc. Nettest.
http://code.google.com/p/nettest/.
[23] A. Janc, C. Wills, and M. Claypool. Network performance
evaluation in a web browser. In Proc. IASTED PDCS, 2009.
[24] S. Kambala. Speed metrics in Google Analytics.
http://googlecode.blogspot.com/2011/12/
speed-metrics-in-google-analytics.html.
[25] P. Kanuparthy and C. Dovrolis. ShaperProbe: End-to-end
detection of ISP trafﬁc shaping using active methods. In
Proc. IMC, 2011.
[26] A. Kingsley-Hughes. Flashback Malware Worth Up To
$10,000 A Day To Criminals. http://www.forbes.
com/sites/adriankingsleyhughes/2012/05/
01/flashback-malware-worth-up-to-10000-
a-day-to-criminals/, May 2012.
[27] C. Kreibich, N. Weaver, G. Maier, B. Nechaev, and
V. Paxson. Experiences from Netalyzr with engaging users in
end-system measurement. In Proc. ACM SIGCOMM
Workshop on Measurements Up the Stack, 2011.
[28] C. Kreibich, N. Weaver, B. Nechaev, and V. Paxson.
Netalyzr: Illuminating the edge network. In Proc. IMC,
2010.
[29] V. Lam, S. Antonatos, P. Akritidis, and K. Anagnostakis.
Puppetnets: misusing web browsers as a distributed attack
infrastructure. In Proc. ACM CCS, 2006.
[30] A. Lieuallen, A. Boodman, and J. Sundström.
Greasemonkey. http://www.greasespot.net/.
[31] M. Mathis, J. Heffner, P. O’Neil, and P. Siemsen. Pathdiag:
Automated TCP diagnosis. In Proc. PAM, 2008.
[32] Measurement Lab.
http://www.measurementlab.net/.
[33] D. Meketa. Policy ﬁle changes in Flash Player 9 and Flash
Player 10.
http://www.adobe.com/devnet/flashplayer/
articles/fplayer9_security.html.
[34] J. Mickens. Rivet: Browser-agnostic Remote Debugging for
Web Applications. In Proc. USENIX ATC, pages 30–38,
2012.
85[35] Mitre.org. Cve-2012-4681.
http://cve.mitre.org/cgi-
bin/cvename.cgi?name=2012-4681.
[36] Mozilla. Firebug. http://getfirebug.com/.
[37] Mozilla. Perfmeasurement.jsm. https://developer.
mozilla.org/en/JavaScript_code_modules/
PerfMeasurement.jsm.
[38] Mozilla. Signed Scripts in Mozilla.
http://www.mozilla.org/projects/
security/components/signed-scripts.html.
[39] Mozilla Developer Network. ChromeWorker.
https://developer.mozilla.org/en/DOM/
ChromeWorker.
[40] Mozilla Developer Network. js-ctypes.
https://developer.mozilla.org/en/js-
ctypes.
[41] Mozilla TestPilot. Tab Switch Study.
https://testpilot.mozillalabs.com/
testcases/tab-switch-study.
[42] V. N. Padmanabhan, S. Ramabhadran, S. Agarwal, and
J. Padhye. A study of end-to-end web access failures. In
Proc. CoNEXT, 2006.
[43] P. Papageorge, J. McCann, and M. Hicks. Passive aggressive
measurement with MGRP. In Proc. ACM SIGCOMM, 2009.
[44] V. Paxson. End-to-end routing behavior in the Internet. Proc.
ACM SIGCOMM, 1996.
[45] M. Perry. Torbutton design documentation. https://
www.torproject.org/torbutton/en/design/.
[46] C. Reis, S. Gribble, T. Kohno, and N. Weaver. Detecting
in-ﬂight page changes with web tripwires. In Proc. USENIX
NSDI, pages 31–44, 2008.
[47] Y. Shavitt and E. Shir. DIMES: Let the Internet measure
itself. ACM SIGCOMM Computer Communication Review,
35(5), 2005.
[48] J. Sommers and P. Barford. An active measurement system
for shared environments. In Proc. IMC, 2007.
[49] N. Spring, D. Wetherall, and T. Anderson. Scriptroute: A
public Internet measurement facility. In Proc. USENIX
Symposium on Internet Technologies and Systems, 2003.
[50] J. Ullrich. Javascript DDoS Tool Analysis.
http://isc.sans.org/diary/Javascript+
DDoS+Tool+Analysis/12442.
[51] UPnP Forum. Internet Gateway Device (IGD) V 2.0.
http://upnp.org/specs/gw/igd2.
[52] W3C. Navigation timing, editor’s draft november 14th, 2011.
https://dvcs.w3.org/hg/webperf/raw-file/
tip/specs/NavigationTiming/Overview.html.
[53] Wikimedia Foundation, Inc. Netscape Plugin Application
Programming Interface.
http://en.wikipedia.org/wiki/NPAPI.
[54] Yahoo! Exceptional Performance Team. This, is boomerang.
http://yahoo.github.com/boomerang/doc/.
[55] B. Yee, D. Sehr, G. Dardyk, J. B. Chen, R. Muth, T. Orm,
S. Okasaka, N. Narula, N. Fullagar, and G. Inc. Native
Client: A sandbox for portable, untrusted x86 native code. In
Proc. IEEE S&P, 2009.
86