4
.
1 6 7
.
0
7
.
2
6
.
3
4
.
1
7
.
0
1
.
0
2
.
0
0
9
.
0 0
1
.
0
2
7
.
7
1
.
2
8
.
1 3
1
.
0
.
1 0
aha-compress
aha-mont64 bs
compress
cover
bubblesortcnt
crc
dijkstra
duff
ctl-stack
newlib-sqrt ns
ludcmp
newlib-log
prime
jfdctint
newlib-exp
picojpeg
expint
janne-cmplx
newlib-mod
nettle-cast128
minver
ﬁbcall ﬁr
matmult-int
ndes
insertsort
huffbench
fac
edn
lcdnum
nettle-arcfour
nettle-des
nsichneu
sqrt
qsort
qurt
sglib-listsort
sglib-rbtree
sglib-dllist
sglib-listinssort
sglib-binsearch
sglib-hashtable
sglib-quicksort
sglib-heapsort
sglib-queue
strstr
statemate
select
recursion
geo-mean
tarai ud
coremark
Fig. 8: Runtime overhead of the additional stack interleaving transformation.
B. Macrobenchmarks
To benchmark raw CPU performance, we used the beebs
benchmark suite [4] as well as CoreMark [20]. We compiled
them with GCC version 7.3.0 with “-O1”. We excluded beebs
benchmarks with external dependencies. Also, we ﬁltered
nettle-md5 and fdct due to veriﬁcation mismatches.
For newlib-log and ns we had to prevent the compiler
from optimizing out essential code by adding volatile and
noinline statements. We ran beebs and CoreMark with one
iteration since our evaluation does not need warm-up iterations
to ﬁll CPU caches but precisely captures all instructions.
Tag Isolation. Our tag isolation policy causes overhead of code
execution for both, N-domains and T-domains. Figure 6 shows
an average runtime overhead of 25.2% for TIMBER-V Model
A with a peak of 47% for nsichneu, which uses frequent
lookup table accesses. insertsort frequently swaps mem-
ory locations, which causes higher overhead. statemate
implements a state machine with frequent state updates and
tarai uses recursion, causing stack accesses to dominate over
other operations. Interestingly, aha-compress shows signif-
icantly less overhead than compress, because it benchmarks
four different CPU intensive compression algorithms with
relatively few memory accesses. The fibcall benchmark
shows least runtime overhead (3.4%) because the recursive
Fibonacci computation can be kept entirely within the CPU
registers. For the optimized TIMBER-V Model B, the average
overhead is as little as 2.6% with a minimum of 0.3% for
fibcall and a maximum of 4.7% for nsichneu. Our
results indicate that even for memory intensive benchmarks
Model B incurs small runtime overhead.
Code Hardening Transformation. Our code hardening trans-
formation adds only negligible overhead, as shown in Figure 7.
This is because the checked instructions are almost a drop-in
replacement for ordinary memory instructions. Since ordinary
memory instructions are subject to tag isolation causing mem-
ory tags to be loaded from memory (this overhead is included
in Figure 6), the additional tag checks of checked instructions
do not incur additional performance penalty. Few benchmarks
show noticeable overhead because the code hardening transfor-
mation in some cases inserts correcting instructions to handle
address overﬂows, as discussed in Section VII. By integrat-
ing the transformation directly into the compiler, one could
leverage compiler optimization to avoid overﬂow behavior.
Stack Interleaving. To benchmark the additional overhead
induced by stack interleaving, we compare each TIMBER-V
model without stack interleaving against a compilation with
enabled stack interleaving. The results are shown in Figure 8.
The overhead is highly dependent on good compiler optimiza-
tion and the used stack space. Many benchmarks (e.g., the
memory-intensive nsichneu) show zero overhead for stack
interleaving, since stack frames are optimized out in favor
of CPU registers. The highest overhead (178.8%) occurs for
minver, which allocates a temporary stack buffer of 500
words for computing matrix inverses. The average runtime
overhead of stack interleaving is acceptable with 11.7% for
Model A and 5.9% for Model B. Yet, we see potential for im-
provements in several directions: First, large stack allocations
should be avoided. This is bad practice anyway since there
exists no generic way of handling out-of-memory behavior
on stack allocations. We manually adapted minver to pre-
allocate a large stack buffer in the data segment and observed
11
)
%
(
d
a
e
h
r
e
v
O
20
15
10
5
0
3
.
6
2
7
.
4
2
6
.
6
1
5
.
5
1
1
.
1
2
.
1
8
.
5
1
1
.
5
1
8
.
3
1
1
.
4
1
2
.
6
1
3
.
5
1
3
.
1
1
8
.
1
1
1
.
3
6 1
.
9
8
.
3
9 1
.
9
Model B
Model A
geo-mean
dijkstra
sglib-dllist
sglib-listinssort
sglib-hashtable
sglib-listsort
sglib-rbtree
ctl-stack
huffbench
Fig. 9: Runtime overhead of additional heap interleaving.
that the runtime overhead drops from 82% and 178.8% to
negligible 2.5% and 5.6% for Model B and Model A, re-
spectively. Second, since stack interleaving implicitly erases
new stack frames, one can avoid potential double clearing.
We evaluated this for huffbench by manually removing the
calls to memset on stack buffers. This reduces overhead from
7.2% and 8.5% down to 3.9% and 5% for Model B and Model
A, respectively. This task could be automated by a compiler.
Third, one could optimize frequent stack frame allocation
and deallocation in favor of less frequent pre-allocation. For
example, when having frequent calls to the same subfunction
inside a loop, one could pre-allocate the subfunction’s stack
frame at the call site, thus reducing the stack interleaving
overhead from N loop iterations to one.
Heap Interleaving. For heap interleaving we only evaluate
benchmarks that use heaps. We use a simple heap implemen-
tation provided with FreeRTOS, namely heap-4. For heap
interleaving we wrapped (de)allocation routines to claim and
unclaim allocated memory using checked store instructions.
The runtime overhead of heap interleaving is slightly below
14%, as shown in Figure 9, which is comparable to stack
interleaving. We believe further improvements are possible
since our realloc wrappers currently do not reuse existing
allocations but always request new memory with malloc.
The huffbench test shows negligible overhead because it
allocates only one out of many buffers on the heap. Our secure
malloc wrapper acts like calloc, clearing the whole buffer
while changing tags. Likewise, our secure free automatically
erases all data, while restoring the original tags. Thus, for
security critical code that demands such zeroing functionality
anyway, heap interleaving comes virtually for free.
C. Microbenchmarks
In the following, we discuss performance of trusted ser-
vices as well as horizontal transitions between apps and en-
claves. The performance numbers are summarized in Table VI.
We depict SHA256 hashing costs in a separate column.
Trusted OS services. Trusted OS services are invoked like
ordinary functions, hence, the transition denoted as TSenter
has minimal overhead. When returning from a TSenter via
TSleave, all callee-saved registers are cleared to avoid infor-
mation leakage. The following results show the performance
of the individual trusted OS service calls without TSenter
and Tleave overhead. The base cost of create-enclave is
dominated by claiming the ECB. We show the runtime when
creating the ﬁrst enclave. The runtime slightly increases when
adding more enclaves since we chain all ECB’s in a linked
list. For add-region we show the runtime for adding the
ﬁrst region. The runtime grows with the number of regions
as well as the number of enclaves due to the region overlap
s
e
c
i
v
r
e
S
S
O
d
e
t
s
u
r
T
TABLE VI: Enclave performance in expected CPU cycles.
TIMBER-V Model A
Hash cost
Base cost
0.0
9.0
32.0
0.0
7175.0
759.0
7483.0
606.0
14365.0
340.0
348.0
7127.0
6397.0
208.0
0.0
437.0
0.0
1057.0
0.0
71.0
0.0
88.0
66.0
0.0
15686.0
457.0
0.0
1560.0
0.0
2062.0
0.0
317.0
0.0
175.0
200.0
0.0
0.0
1.0
0.0
5.0
0.0
29.0
45.0
0.0
Functionality
TSenter
TSleave
create-enclave
add-region
add-data
add-entries
init-enclave
load-enclave
destroy-enclave
TSyscall
TSyscall dispatch
TSyscall return
get-key
shm-offer
shm-accept
shm-release
interrupt-enclave
resume-enclave
TUenter
TUleave
ocall
ocall return
TIMBER-V Model B
Hash cost
Base cost
0.0
7.1
0.0
27.4
5647.1
527.5
5821.6
396.3
11309.4
212.0
206.4
5616.2
5236.6
123.5
0.0
315.6
0.0
733.5
0.0
68.3
0.0
71.1
49.2
0.0
12216.6
337.5
0.0
1045.6
0.0
1455.0
0.0
231.7
0.0
107.7
103.0
0.0
0.0
1.0
0.0
4.1
0.0
16.9
28.4
0.0
.
l
c
n
E
-
p
p
A
s
e
c
i
v
r
e
S
.
l
c