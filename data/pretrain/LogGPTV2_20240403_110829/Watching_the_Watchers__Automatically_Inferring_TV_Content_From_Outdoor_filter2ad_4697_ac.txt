we discuss how to achieve a more efﬁcient solution in practice.
6.
ILLUMINATI: EFFICIENT ATTACKS
US-ING COMPROMISING EFFUSIONS
To tackle large-scale databases of tens of thousands of videos, we
employ a matching algorithm that only needs to search a small frac-
tion of the database. Recall that our similarity metric (Equation 9)
mainly matches signiﬁcant intensity changes (peaks in our feature
representation) in the captured video with the peaks in the database
videos. Next, we leverage the fact that these peaks are only present
in a small fraction of the video frames and propose a new peak-
feature that efﬁciently characterizes the distribution of peaks. This
distribution can then be used to narrow down the search space and
speed up the search by an order of magnitude. Our proposed al-
gorithm consists of two steps. The ﬁrst step is the extraction of
422the features based on only the peaks and the second step uses an
efﬁcient index-based search.
6.1
Peak-feature Extraction
Our proposed peak-feature aims at capturing the distribution of
the peaks caused by sudden intensity changes in the video. As
shown in Figure 3, the peak-feature is computed within a sliding
window, of size w = 512, over the gradient feature, i.e. the peak-
feature is computed from the w consecutive feature values. The
value 512 is chosen empirically since our experiments indicate that
subsequences shorter than 512 frames (at 10 Hz video frame rate)
do not provide enough information for retrieval. To limit sensitivity
to peaks caused by noise, all peaks with a magnitude lower than
a predeﬁned threshold (30% in our experiments) of the strongest
peak’s magnitude are omitted. The remaining dominant peaks are
assumed to stably represent the gradient feature within the window
and are encoded into our proposed peak-feature.
Figure 3: Depiction of a sliding window for extracting the peak-descriptor.
To suppress noise related peaks, peaks below a predeﬁned threshold are
ignored. Effective peaks pairs create a histogram and the cumulation is
used as the descriptor of the window.
The encoding scheme works as follows. A histogram of the pair-
wise distances between all pairs of peaks is computed. The his-
togram uses a bin size of eight, which roughly corresponds to a one
second distance quantization. The resulting histogram has 64 bins
for our window size of 512 frames. Each pair of peaks increases the
count in the bin corresponding to their distance (measured by their
frame number difference). To model the fact that the stronger peaks
are more reliable, the amount of increase is equal to the product of
the peaks magnitude. In that way, peaks with larger magnitudes
contribute more signiﬁcantly to the histogram. To ensure compa-
rability between feature windows with different numbers of peaks,
we normalize the histogram to sum to one. Our peak-feature is the
cumulative histogram of the normalized histogram. For the remain-
der of the paper, we use this cumulative histogram as it is less prone
to the inﬂuence of noise caused, for example, by the quantization
through the histogram bins.
In summary, our proposed peak-feature is a monotonically in-
creasing 64-dimensional vector with the ﬁnal element being 1. An
example histogram and the corresponding peak-feature is illustrated
in Figure 3. The distance between two peak-features can be mea-
sured by the Euclidean distance of the 64 dimensional vectors. The
peak-feature is invariant to the starting point of the window given
that it only encodes the pairwise peak distances. When the window
slides across the feature, the peak-feature remains stable as long
as there is no peak coming in or going out. For completeness, the
exact process is given in Algorithm 1.
An entire video can then be represented as the set of its peak-
features, which typically leads to a large set of features describing
the video. However, since the peak-feature only depends on the
peaks within the window, shifting the window by one frame often
results in the same peak-feature (as long as all peaks remain in the
window). Empirically, this is the case for about 95% of the peak-
features. Accordingly, we represent a video using only its unique
peak-features and remove all redundant peak-features from the set
of computed peak-features.
Histogram[i] ← 0
if | fwin[i]| < T hreshold ∗ max(| fwin|) then
Algorithm 1 Extracting peak-feature from window fwin
1: T hreshold ← 0.3
2: for i = 1 to N do
3:
fwin[i] ← 0
4:
end if
5:
6: end for
7: for i = 1 to 64 do
8:
9: end for
10: for every 2 peaks pi, p j in fwin do
11:
12: end for
13: Histogram ← Histogram/sum(Histogram)
14: for i = 1 to 64 do
15:
16: end for
17: return PeakFeature
Histogram[dist(pi, p j)] ← Histogram[dist(pi, p j)] +|pi p j|
PeakFeature[i] = Σi
k=1Histogram[k]
6.2 Efﬁcient Searching
Next, we detail our proposed efﬁcient search algorithm, which
leverages the introduced peak-feature for efﬁcient search. Algo-
rithm 2 provides the pseudo-code for our method and will be de-
tailed below. Given a recording of interest, we ﬁrst extract the
peak-features for the video (see line 6). Peak features with a high
number of strong peaks are typically very distinguishing, having
a Euclidian norm that is typically larger than peak-features with
weaker or fewer peaks. During the matching process, we select the
peak-feature with the largest norm ﬁrst (see line 7).
To search the database for a likely match (see line 9), we index
the peak-features using a data-structure known as K-d tree, which
is widely used for search in high-dimensional search spaces [2].
The main idea of the K-d tree is to recursively split the space with
hyperplanes, which iteratively reﬁnes the possible location of the
data point under examination.
In our empirical evaluations, the
reference library contains 27-million peak-features representing the
54,000 videos. By leveraging a K-d-tree, we can quickly search for
all reference videos that are likely matches. Here, a likely matching
video has to be within a Euclidean distance of δ ≤ 0.7 from the
peak-feature of the captured video2.
From the likely matches we select the one with the smallest Eu-
clidian distance to the captured video (see line 10). For this video
our similarity metric from Equation 9 is computed. If the similarity
is the best observed similarity thus far, this video is retained as the
top candidate and its conﬁdence is increased (see line 16). Then,
the next strongest peak-feature is obtained (see line 18) and eval-
uated in the same manner (see line 9-16). If the retrieved video is
the same as the previously selected one, the conﬁdence assigned to
this potential match increases (see line 16). Otherwise the newly
2The value for δ was empirically chosen based on a rudimentary
analysis of the resulting accuracy.
423found best match replaces the previously selected best video (see
line 12-14). This process is repeated until the best-matching video
remains stable for three consecutive trials.
Algorithm 2 Efﬁcient searching captured feature fc
1: BestScore ← INF // best so far score
2: BestId ← INF // database id of best candidate
3: ConsecutiveHits ← 0 // number of consecutive conﬁrmation of
best candidate
4: MaxHits ← 3
5: Radius ← 0.7
6: PeakFeature ← extractPeakFeature( fc)
7: CurFea ← f eatureO f StrongestPeak(PeakFeature)
8: while exist(CurFea) and ConsecutiveHits < MaxHits do
Re f Fea ← searchKdtree(CurFea,Kdtree,Radius)
9:
[CurScore,CurId] ← f indMinSMetric(CurFea,Re f Fea)
10:
if CurScore < BestScore then
11:
12:
13:
14:
15:
16:
17:
18:
19: end while
20: return BestId
BestScore ← CurScore
BestId ← CurId
ConsecutiveHits ← 0
ConsecutiveHits ← ConsecutiveHits + 1
end if
CurFea ← f eatureO f NextStrongestPeak(PeakFeature)
else
The algorithm proposed above is an ofﬂine approach, which can
be extended to operate in an online fashion. For ofﬂine retrieval,
we have access to all the peak-features at once. Hence, we have the
luxury of ranking the features by strength. In contrast, for online
operation, the video is streamed. Once a new frame is captured a
new peak-feature is computed using the 512 most recent frames.
If the newly computed feature is unique for the video, i.e., has not
been extracted from the video before, the K-d tree is used to search
for likely matches within the reference library. Then the best video
(i.e., with the smallest Euclidian distance) is fully evaluated us-
ing our proposed similarity metric from Equation (9). If the best
video is identical to the previously identiﬁed one, its conﬁdence is
increased. Otherwise it replaces the current best choice.
On Efﬁciency: Levering the peak-features and the K-d tree based
search reduces the search time on average to less than 10s (2.8 sec-
onds for each K-d tree search) for a database of 54,000 reference
videos. The achieved query time is more than an order of magni-
tude faster than searching exhaustively through the database, which
took 188s. The online search can in fact be executed in real time
when allowing a latency of 512 frames due to the required tempo-
rally preceding information for the peak-feature computation.
7. EVALUATION
For our empirical evaluation, we collected a large collection of
reference videos spanning a wide variety of content. Our refer-
ence library contains 10,000 blockbuster movies of at least an hour
in length, 24,000 news clips ranging from 5 min to 20 min each,
10,000 music videos ranging from 2 min to 7 min each, and 10,000
TV-shows ranging from 5 min to 20 min each. In total, the library
indexes over 18,800 hours of video. All features and peak-features
from the library are precomputed by leveraging our proposed meth-
ods from Sections 5 and 6. For our experimental evaluation we
randomly selected 62 sequences as our test set of videos.
For the ﬁrst set of evaluations the test videos were played on a
24 inch screen with no additional room lighting turned on. We then
capture the reﬂection of the screen emanation from a white wall at
a distance of three meters from the screen. To capture the video,
a Logitech HD Pro Webcam C920 and a 60D canon DSLR were
used. We run the experiment in a home environment as well as
in a lab environment. The setting of our experiment is illustrated
in Figure 4. These captured videos were then used to execute our
attack. For these evaluations we assess the success of the attack
with respect to the duration of the captured video and the size of
the reference library.
Figure 4: Lab environment (left) and home environment setting (right)
Lights Off
First we evaluate the success rate of our method using a room with
the lights off, as commonly occurs when watching TV. A success
is the correct identiﬁcation of the video being watched. We do not
leverage any knowledge about the video being played nor do any
of our experiments use any knowledge of the scene or the capture
distances. The time at which the adversary starts capturing the em-
anations from the display is chosen at random.
Capture Length
Success Rate
90s
60s
270s
39% 49% 54% 70% 85% 94 %
120s
180s
240s
Table 1: Retrieval success rate with random start point.
For the 62 test sequences we analyzed segments from 60 to 270
seconds long. These segments are processed by the feature and
peak-feature extraction procedures. The resulting features and peak-
features are then used to infer the best match among the reference
library. The experiment is repeated 100 times for each of the differ-
ent segment lengths, each time choosing a random starting position.
Table 1 shows the resulting average success rate over all starting po-
sitions. As expected, the longer the captured sequence, the higher
the attack’s success rate. The results shows that the success rate
increases from 39% for a 60 second segment to 94% for 270 sec-
onds, and has nearly a 50% success rate using only 90 seconds of
captured emanations. A more detailed analysis of the data reveals
that in the limit, the success rate is 100% for each video as subse-
quences within these videos can always be uniquely identiﬁed.
To better quantify the robustness of our approach, we evaluate
the ratio in similarity between the video sequence returned as the
best match and the true positive.
If the ratio is larger than one,
that implies the correct video will always be identiﬁed. The higher
that ratio, the more distinct the retrieval result. Obviously, the out-
come also depends on the contents of the reference library itself.
The experimental results of the ratio evaluation are shown in Fig-
ure 5. The median similarity score ratio rises above one (successful
retrieval) between 100 and 120 seconds. For longer sequences, it
monotonically increases with increasing segment length.
Beyond the average success rate and robustness, it is also impor-
tant to understand the best and worst case results. The worst case is
424Illumination settings
Normal brightness level room light off
50% brightness level room light off
Normal brightness level room light on
SNR
70
33
15
Segment Length
180s
270s
300s
Table 2: Worst case capture length with different illumination settings.
on the performance of our proposed attack. In this experiment we
use a 24 inch screen, and the attacker’s camera captures the reﬂec-
tion of the screen of a white wall, which is three meters away from
the screen. The camera used in the attack is a Canon Rebel T4i
DSLR. We captured ﬁve videos in each of three different illumina-
tion settings: 1) normal screen brightness with room light off, 2)