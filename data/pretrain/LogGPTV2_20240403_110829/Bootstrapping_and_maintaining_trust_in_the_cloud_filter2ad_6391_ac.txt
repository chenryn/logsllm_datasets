itoring which limit the ability of a rogue individual to ac-
complish this.
As in Section 3.2, we begin with the establishment of a
tenant registrar and cloud veriﬁer. There are multiple op-
tions for hosting these services securely: 1) in a bare metal
IaaS instance with TPM, 2) on-tenant-premises in tenant-
owned hardware, 3) in a small trusted hardware appliance
deployed to the IaaS cloud provider, and 4) in an IaaS vir-
tual machine. The ﬁrst three of these options rely upon
the architecture and protocols we’ve already discussed. The
last option requires the tenant to establish an on-tenant-
premises CV and use that to bootstrap the tenant registrar
and CV. This on-tenant-premises CV identiﬁes and checks
the integrity of the tenant’s virtualized registrar and CV,
who then in turn are responsible for the rest of the tenant’s
virtualized infrastructure.
The primary motivations for a tenant choosing between
these options are the detection latency for integrity viola-
tions, scale of IaaS instances in their environment, band-
width between the tenant and the IaaS cloud, and cost. Op-
tion 1 provides maximum performance but at higher cost.
Option 2 will by limited by bandwidth and requires more
costs to maintain resources outside of the cloud. Option 3 is
a good trade-oﬀ between cost and performance for a small
cloud tenant with only tens of nodes or who can tolerate a
longer detection latency. Finally, Option 4 provides compat-
ibility with current cloud operations, good performance and
scalability, and low cost at the expense of increased complex-
ity. In Section 5, we examine the performance trade-oﬀs of
these options including a low-cost registrar and CV appli-
ance (Option 3) we implemented on a Raspberry Pi.
Once we have created the tenant registrar and CV, we can
begin securely bootstrapping nodes into the environment.
As before, the ﬁrst node to establish is a virtualized software
CA and we do this by creating a private signing key oﬄine
and protecting it with a key that will be derived by the
bootstrap key derivation protocol. The following process
will be the same for all tenant cloud nodes. When a node
boots, it will get a vTPM from the IaaS provider.
The process of enrolling a vTPM into the tenant regis-
trar needs to securely associate the vTPM credentials, e.g.,
(vEK, vAIK), with a physical TPM in the provider’s infras-
tructure (see Figure 4). The tenant registrar cannot directly
verify the authenticity of the vEK because it is virtual and
has no manufacturer. To address this, we use a deep quote to
bind the virtual TPM credentials to a physical TPM AIK.
The vTPM enrollment protocol begins like the physical
TPM enrollment protocol by sending ID, (EKpub, AIKpub)
70
to the tenant registrar. The tenant registrar then returns
EncvEK(H(AIKpub), Ke) without additional checks. The
virtual node then decrypts Ke using ActivateIdentity func-
tion of its vTPM. The node next requests a deep quote using
a hash of Ke as the nonce to both demonstrate the freshness
of the quote and knowledge of Ke to the tenant registrar.
It also uses virtual PCR #16 to bind the vTPM credentials
and ID to the deep quote. Upon receiving the deep quote,
the tenant registrar asks the provider registrar if the AIK
from the deep quote is valid. The tenant registrar also re-
quests the latest valid integrity measurement whitelists from
the provider. Now the tenant registrar can check the va-
lidity of the deep quote’s signature, ensure that the nonce
is H(Ke), conﬁrm the binding data in PCR #16 matches
what was provided in the previous step, and check the phys-
ical PCRs values in the deep quote against the provider’s
whitelist. Only if the deep quote is valid will the tenant
registrar mark the vAIK as being valid.
When considering the cost of performing a deep quote,
the provider must carefully consider the additional latency
of the physical TPM. Deep quotes provide a link between
the vTPM and the physical TPM of the machine, and new
enrollments should always include deep quotes. When con-
sidering if deep quotes should be used as part of periodic
attestation, we must understand what trusted computing
infrastructure the provider has deployed. If the provider is
doing load time integrity only (e.g., secure boot), then deep
quotes will only reﬂect the one-time binding at boot between
the vTPM and the physical TPM and the security of the
vTPM infrastructure. If the provider has runtime integrity
checking of their infrastructure, there is value in the ten-
ant performing periodic attestation using deep quotes. In
the optimal deployment scenario, the provider can deploy
keylime and provide tenants with access to the integrity
state of the hypervisors that host tenant nodes. To limit
the impact of slow hardware TPM operations, the provider
can utilize techniques like batch attestation where multiple
deep quote requests from diﬀerent vTPMs can be combined
into a single hardware TPM operation [28, 31].
3.2.2 Key Derivation Protocol
We now introduce the details of our bootstrap key deriva-
tion protocol (Figure 5). The goal of this protocol is for the
cloud tenant to obtain key agreement with a cloud node they
have provisioned in an IaaS system. The protocol relies upon
the CV to provide integrity measurement of the cloud node
during the protocol. The tenant also directly interacts with
the cloud node to demonstrate their intent to spawn that re-
source and allow it to decrypt sensitive contents. However,
the tenant does not directly perform integrity measurement.
This separation of duties is beneﬁcial because the attesta-
tion protocols may operate in parallel and it simpliﬁes de-
ployment by centralizing all integrity measurement, white
lists, and policy in the CV.
To begin the process, the tenant generates a fresh random
symmetric encryption key Kb. The cloud tenant uses AES-
GCM to encrypt the sensitive data to pass to the node d with
Kb, denoted EncKb (d). The tenant then performs trivial
secret sharing to split Kb into two parts U , which the tenant
will retain and pass directly to the cloud node and V , which
the tenant will share with the CV to provide to the node
upon successful veriﬁcation of the node’s integrity state. To
obtain these shares the tenant generates a secure random
(cid:15)(cid:21)(cid:28)(cid:17)(cid:28)(cid:33)(cid:1)
(cid:9)(cid:6)(cid:36)(cid:2)(cid:16)(cid:36)(cid:2)(cid:9)(cid:12)(cid:36)(cid:2)(cid:30)(cid:29)(cid:31)(cid:33)(cid:36)(cid:2)(cid:35)(cid:23)(cid:24)(cid:33)(cid:21)(cid:26)(cid:24)(cid:32)(cid:33)(cid:1)
(cid:5)(cid:1)
(cid:5)(cid:26)(cid:29)(cid:34)(cid:20)(cid:2)(cid:16)(cid:21)(cid:31)(cid:24)(cid:41)(cid:24)(cid:21)(cid:31)(cid:1)
(cid:11)(cid:29)(cid:20)(cid:21)(cid:1)
(cid:3)(cid:1)
(cid:4)(cid:1)
(cid:28)(cid:5)(cid:16)(cid:36)(cid:2)(cid:27)(cid:17)(cid:32)(cid:25)(cid:1)
(cid:13)(cid:34)(cid:29)(cid:33)(cid:21)(cid:3)(cid:9)(cid:10)(cid:42)(cid:28)(cid:5)(cid:16)(cid:36)(cid:39)(cid:40)(cid:37)(cid:8)(cid:42)(cid:11)(cid:25)(cid:30)(cid:34)(cid:18)(cid:43)(cid:36)(cid:3)(cid:2)(cid:1)(cid:37)(cid:4)(cid:2)(cid:43)(cid:36)(cid:11)(cid:10)(cid:30)(cid:34)(cid:18)(cid:1)
(cid:16)(cid:17)(cid:26)(cid:24)(cid:20)(cid:2)(cid:3)(cid:9)(cid:10)(cid:38)(cid:1)
(cid:7)(cid:28)(cid:19)(cid:11)(cid:10)(cid:42)(cid:16)(cid:43)(cid:1)
(cid:28)(cid:33)(cid:1)
(cid:13)(cid:34)(cid:29)(cid:33)(cid:21)(cid:3)(cid:9)(cid:10)(cid:42)(cid:28)(cid:33)(cid:36)(cid:39)(cid:40)(cid:37)(cid:8)(cid:42)(cid:11)(cid:10)(cid:30)(cid:34)(cid:18)(cid:43)(cid:43)(cid:43)(cid:36)(cid:11)(cid:10)(cid:30)(cid:34)(cid:18)(cid:1)
(cid:16)(cid:17)(cid:26)(cid:24)(cid:20)(cid:2)(cid:3)(cid:9)(cid:10)(cid:38)(cid:1)
(cid:4)(cid:13)(cid:11)(cid:9)(cid:7)(cid:15)(cid:10)(cid:16)(cid:14)(cid:5)(cid:8)(cid:1)(cid:2)(cid:7)(cid:12)(cid:15)(cid:6)(cid:3)(cid:16)
(cid:14)(cid:21)(cid:22)(cid:24)(cid:32)(cid:33)(cid:31)(cid:17)(cid:31)(cid:1)
(cid:2)(cid:4)(cid:5)(cid:4)(cid:6)(cid:3)(cid:1)
Mutual TLS 
Server TLS 
No TLS 
Figure 5: Three Party Bootstrap Key Derivation Protocol.
value V the same length as Kb and computes U = Kb ⊕ V .
In the next phase of the protocol, the tenant requests the
IaaS provider to instantiate a new resource (i.e., a new vir-
tual machine). The tenant sends EncKb (d) to the provider
as part of the resource creation. The data d may be conﬁgu-
ration metadata like a cloud-init script13. Upon creation,
the provider returns a unique identiﬁer for the node uuid
and an IP address at which the tenant can reach the node.
After obtaining the node uuid and IP address, the tenant
notiﬁes the CV of their intent to boot a cloud node (see area
A in Figure 5). The tenant connects to the CV over a secure
channel, such as mutually authenticated TLS, and provides
v, uuid, node IP, and a TPM policy. The TPM policy speci-
ﬁes a white list of acceptable PCR values to expect from the
TPM of the cloud node. At this point the CV and tenant
can begin the attestation protocol in parallel.
The attestation protocol of our scheme is shared between
the interactions of the CV and the cloud node (B) and that
of the tenant and the cloud node (C) with only minor dif-
ferences between them (Figure 5). The protocol consists
of two message volleys the ﬁrst for the initiator (either CV
or tenant) to request a TPM quote and the second for the
initiator to provide a share of Kb to the cloud node upon
successful validation of the quote. Since we use this pro-
tocol to bootstrap keys into the system, there are no ex-
isting software keys with which we create a secure channel.
Thus, this protocol must securely transmit a share of Kb
over an untrusted network. We accomplish this by creat-
ing an ephemeral asymmetric key pair on the node, denoted
N K, outside of the TPM14. As in Section 3.2.1, we use PCR
#16’s value in a TPM quote to bind N K to the identity of
13Because Kb may not be re-used in our protocol, the cost
of re-encrypting large disk images for each node may be
prohibitive. We advocate for encrypting small sensitive data
packets like a cloud-init script, and then establish local
storage encryption with ephemeral keys.
14N K could also be generated and reside inside the TPM.
However, since it is ephemeral, is only used for transport
security and it is authenticated by the TPM using the quote,
we found the added complexity of also storing it in the TPM
unneeded.
the TPM thereby authenticating N K. The initiator can
then encrypt its share of Kb using N Kpub and securely re-
turn it to the cloud node.
The diﬀerences in the attestation protocol between CV
and tenant arise in how each validates TPM quotes. Be-
cause we wish to centralize the adjudication of integrity
measurements to the CV, the TPM quote that the tenant
requests only veriﬁes the identity of the cloud node’s TPM
and doesn’t include any PCR hashes. Since the tenant gen-
erates a fresh Kb for each cloud node, we are not concerned
with leaking U to a node with invalid integrity state. Fur-
thermore, because V is only one share of Kb, the CV cannot
be subverted to decrypt resources without user intent.
We now describe the attestation protocol in detail. The
initiator ﬁrst sends a fresh nonce (nt for the tenant as in B
from Figure 5 and nCV for the cloud veriﬁer as in C from
Figure 5) to the cloud node along with a mask indicating
which PCRs the cloud node should include in its quote. The
CV sets the mask based on TPM policy exchanged earlier
and the tenant creates an empty mask. We extend a hash of
N Kpub into a freshly reset PCR #16. The initiator requests
a quote from the TPM with the given PCR mask. The node
then returns QuoteAIK(n, 16 : H(N Kpub), xi, : yi), N Kpub
to the initiator. Additional PCR numbers xi and values yi
are only included in the quote returned to the cloud veriﬁer
based on the TPM policy it requested. During the protocol
to provide U , the tenant also supplies HM ACKb (ID) to
the node. This provides the node with a quick check to
determine if Kb is correct.
The initiator then conﬁrms that the AIK is valid accord-
ing to the tenant registrar over server authenticated TLS. If
the initiator is the CV, then it will also check the other PCRs
to ensure they are valid according to the tenant-speciﬁed
whitelist. If the node is virtual, then the quote to the CV
will also include a deep quote of the underlying hardware
TPM. The CV will in turn validate it as described in the
previous section. Upon successful veriﬁcation, the initiator
can then return their share of Kb. Thus, the tenant sends
EncN K(U ) and the cloud veriﬁer sends EncN K(V ) to the
node. The cloud node can now recover Kb and proceed with
71
the boot/startup process.
The cloud node does not retain Kb or V after decryption
of d. To support node reboot or migration, the cloud node
stores U in the TPM NVRAM to avoid needing the tenant
to interact again. After rebooting, the node must again
request veriﬁcation by the CV to obtain V and re-derive
Kb. If migration is allowed, the provider must take care to
also securely migrate vTPM state to avoid losing U .
4.
IMPLEMENTATION
We implemented keylime in approximately 5,000 lines of
Python in four components: registrar, node, CV, and ten-
ant. We use the IBM Software Trusted Platform module
library [18] to directly interact with the TPM rather than
going through a Trusted Software Stack (TSS) like Trousers.
The registrar presents a REST-based web service for en-
rolling node AIKs.
It also supports a query interface for
checking the keys for a given node UUID. The registrar use
HMAC-SHA384 to check the node’s knowledge of Ke during
registration.
The node component runs on the IaaS machine, VM, or
container and is responsible for responding to requests for
quotes and for accepting shares of the bootstrap key Kb. It
provides an unencrypted REST-based web service for these
two functions.
To support vTPM operations, we created a service the
IaaS provider runs to manage hardware TPM activation and
vTPM creation/association. This service runs in a desig-
nated Xen domain and has privileges to interact with the
Xen vtpmmgr domain [11]. We then implemented a utility
for the deep quote operation. Since the Xen vTPM imple-
mentation does not directly return the PCR values from the
virtual TPM (i.e., the shallow quote) during a deep quote,
we chose to ﬁrst execute a shallow quote, hash its contents
with the provided nonce, and place them in the nonce ﬁeld
of the deep quote. This operation cryptographically binds
them together. This operation is not vulnerable to man-in-
the-middle attack since there is no other interface to directly
manipulate the nonce of a deep quote [36]. We then return
both the shallow and deep quotes and require the veriﬁer
checks both signatures and sets of PCR values.
The cloud veriﬁer hosts a TLS-protected REST-based web
service for control. Tenants add and remove nodes to be
veriﬁed and also query their current integrity state. Upon
being notiﬁed of a new node, the CV enqueues metadata
about the node onto the quote_request queue where a con-
ﬁgurable pool of worker processes will then request a deep
quote from the node. Upon successful veriﬁcation of the
quote, the CV will use an HTTP POST to send V to the
node. The CV uses PKCS#1 OAEP and with RSA 2048
keys to protect shares of Kb in transit.
The tenant generates a random 256-bit AES key and en-
crypts and authenticates the bootstrap data using AES with
Galois Counter Mode [27]. The tenant uses trivial XOR-
based secret sharing to split Kb into V and U . The tenant
executes a simpliﬁed version of the same protocol that the
CV uses. The tenant checks with the registrar to determine
if the quote signing AIK is valid and owned by the tenant.
Upon receiving U and V , the node can then combine them
to derive Kb. To limit the impact of rogue CVs or tenants
connecting to the node’s unauthenticated REST interface,
the node stores all received U and V values and iteratively
tries each combination to ﬁnd the correct Kb. Once the
node has correctly derived Kb, it mounts a small in-memory
ﬁle system using tmpfs and writes the key there for other
applications to access.
Integration
4.1
While the key derivation protocol of keylime is generic
and can be used to decrypt arbitrary data, we believe the
most natural cloud use-case for it is to decrypt a small IaaS
node-speciﬁc package of data. To enable this use-case we
have integrated keylime with the cloud-init package, the
combination we call trusted-cloud-init. As described in
Section 2, cloud-init is widely adopted mechanism to de-
ploy machine-speciﬁc data to IaaS resources. To integrate
keylime and cloud-init, we patched cloud-init to sup-
port AES-GCM decryption of the user-data (where cloud-
init stores tenant scripts and data). We modiﬁed the up-
start system in Ubuntu Linux to start the keylime node
service before cloud-init. We then conﬁgure cloud-init
to ﬁnd the key that keylime creates in the tmpfs mounted
ﬁle system. After successful decryption, cloud-init deletes
the key and scrubs it from memory.
To support applications that need node identities that do
not manage their own PKIs, we implemented a simple soft-
ware CA. The tenant provisions the software CA by creating
the CA private key oﬄine and delivering it to a new node
using trusted-cloud-init. We also deliver certiﬁcates to
the software CA that allow it and the tenant to mutually au-
thenticate each other via trusted-cloud-init. To demon-
strate the clean separation between the trusted computing
layer and the software key management layer, we use the
ZMQ Curve secure channel implementation [14]. This sys-
tem uses an elliptic curve cryptography scheme dissimilar
from the cryptographic algorithms, keys, and other tech-
niques the TPM uses.
To enroll a new node, the tenant ﬁrst generates a node
identity key pair using the software CA client. The soft-
ware CA supports a plugin architecture that allows the ten-
ant to specify what type of key pairs to create (e.g., X.509
RSA 2048). The tenant then connects securely to the soft-
ware CA over ZMQ and gets the node’s identity certiﬁcate