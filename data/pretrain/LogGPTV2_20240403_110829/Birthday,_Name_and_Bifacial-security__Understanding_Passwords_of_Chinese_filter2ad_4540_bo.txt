https://shouji.supfree.net/.
As for the birthday dictionaries, we use date patterns
to match digit strings that might be birthdays.
For
example, “YYYYMMDD” stands for a birthday pattern
that the ﬁrst four digits indicate years (from 1900 to
2014), the middle two represent months (from 01 to 12)
and the last two denote dates (from 01 to 31). Similarly,
we build the date dictionaries “YYYY”, “MMDD” and
“YYMMDD”. Note that, “PW with a l+-letter substring”
means a subset of the corresponding dataset and consists
of all passwords that include a letter substring no shorter
than l, and similarly for “PW with a l+-digit substring”.
Though we use the “left-most longest” rule to min-
imize ambiguities when matching, there are some un-
avoidable ambiguities when determining whether a tex-
t/digit sequence belongs to a semantic dictionary. An
improper resolution would lead to an overestimation
or underestimation. For instance, 111111 falls into
“YYMMDD” and is highly popular, yet it is more likely
that users choose it simply because it is easily memorable
repetition numbers. To tackle this issue, we manually
identify 17 abnormal dates in “YYMMDD”, each of
which originally has a frequency> 10E and appears in
every top-1000 list of the six Chinese datasets: 111111,
520131, 111222, 121212, 520520, 110110, 231231,
101010, 110119, 321123, 010203, 110120, 010101,
520530, 000111, 000123, 080808. Similarly, we iden-
tify 16 abnormal items in “MMDD”: 1111, 1122, 1231,
1212, 1112, 1222, 1010, 0101, 1223, 1123, 0123,
1020, 1230, 0102, 0520, 1110. Few abnormal items
can be identiﬁed in the other 19 dictionaries (Table 5),
and they are processed as usual.
C A subtlety about Good-Turing smooth-
ing in Markov-based cracking
In 2014, Ma et al. [36] introduced the Good-Turing (GT)
smoothing into password cracking, yet little attention
has been paid to the unsoundness of GT for popular
password segments. We illustrate the following subtlety.
We denote f to be the frequency of an event and Nf
to be the frequency of frequency f . According to the
basic GT smoothing formula, the probability of a string
“c1c2···cl” in a Markov model of order n is denoted by
P(“c1···cl−1cl”) =
l(cid:2)
i=1
P(“ci|ci−nci−(n−1)···ci−1”), (1)
,(2)
(cid:3)
where the individual probabilities in the product are
computed empirically by using the training sets. More
speciﬁcally, each empirical probability is given by
P(“ci|ci−n···ci−1”) = S(count(ci−n···ci−1ci))
c∈Σ S(count(ci−n···ci−1c))
where the alphabet Σ includes 95 printable ASCII char-
acters on the keyboard (plus one special end-symbol cE
denoting the end of a password), and S(·) is deﬁned as:
(3)
This kind of smoothing works well when f is small,
but it fails for passwords with a high frequency because
the estimates for S( f ) are not smooth. For instance,
12345 is the most common 5-character string in Rock-
you and occurs f = 490,044 times. Since there is no
5-character string that occurs 490,045 times, N490045
will be zero, implying the basic GT estimator will set
P(“12345”)=0. A similar problem regarding the smooth-
ing of password frequencies is identiﬁed in [5].
S( f ) = ( f + 1)
Nf +1
Nf
.
SN( f ) =
N(1) if f = 1