ij − aij = c
−
aij − a
+
ij = c
1 − m(cid:5)
(10)
C. IS for IMC
and
aij = c(ai) = 0, for all i.
In the following, we show how to apply IS to IMC by
reducing the problem to an optimization problem. For any A
in an IMC [ ˆA], the exact probability PA(ω) of a path ω falls
within the following interval:
m(cid:4)
m(cid:4)
nij (ω) ≤ PA(ω) ≤ m(cid:4)
m(cid:4)
−
ij)
(a
i=0
j=0
i=0
j=0
+
ij)
(a
nij (ω)
(8)
306
j=0
In what follows, we denote f (A) the objective function to
minimise.
In some cases, it remains easy to evaluate aij. For example,
if only one transition si → sj has been taken from state
+
si, then aij = max(a
ij(cid:2) ). This expression
guarantees that aij is well deﬁned and remains consistent
ij, 1 − (cid:2)m
j(cid:2)(cid:7)=j a
−
with regard to the constraints that apply to the other outgoing
transitions from state si. Nevertheless, if several transitions
from state si have been observed, the problem becomes much
harder and requires the recourse to a minimisation algorithm.
IV. SOLVING THE MINIMISATION PROBLEM
Minimising f (A) requires a class of algorithms for solving
optimisation problems with equality and inequality constraints.
Numerical methods like penalty or interior point methods (see
e.g. [19]) are not suitable in our context due to the large sample
size and number of observed transitions. Various statistical
methods, notably in convex optimisation, could be used but
their efﬁciency is strongly impacted by the large number of
constraints. Which algorithms ﬁt the best our problem is a
complex question as the answer is likely system-dependent;
providing a clear answer goes beyond this article. For more
details, we discuss in the appendix alternative promising
methods, notably the stochastic gradient descent [18] and the
stochastic interior Point Method [5]. In this work, we propose a
simple algorithm which is proper for solving our problem and
converges almost surely to a global minimum. Note that the
global minimum is not necessarily unique and is guaranteed
to exist since the intervals for each parameter are closed and
the objective function is continuous.
A. Monte Carlo Random Search algorithm
We propose to determine a global minimum by a random
search into the domain of deﬁnition [ ˆA] of the function. The
algorithm works as follows: starting with Amin = A(0) = ˆA,
we sample iteratively independent candidates A(l+1) in [ ˆA]
according to a probability distribution X covering all
the
DTMCs in [ ˆA]. If f (A(l+1))  0 does not impact the relative lengths because
the length of each coordinate is multiplied by the same
constant. However, the relative variances VRel(Xj) decreases
to zero when K tends to the inﬁnity:
VRel(KXj) =
=
Kαj(K.β − Kαj)
K 2β2(Kβ + 1)
αj(β − αj)
−→
K→∞ 0
β2(Kβ + 1)
Given an IMC [ ˆA], for each visited state si, we sample m+1
values denoted (aij)0≤j≤m according to a Dirichlet distribu-
tion X = (Xij)0≤j≤m parametrised by vector (Kiˆaij)0≤j≤m
where Ki > 0 is a precomputed parameter aiming to control
the relative variances. If all the constraints of [ˆaij] are satisﬁed,
(aij)0≤j≤m is the state distribution from si of the DTMC
candidate A(l).
If Ki is chosen too large, the variance of each coordinate
decreases and we would likely sample values that are too close
to the mean ˆaij. If Ki is too low, the variance becomes larger
and we would generate values that do not belong to [ˆaij −
ij, ˆaij + ij].
For this purpose, we set, for each transition (i → j), a value
Kij such that the standard deviation of Xij equals ij:
(cid:15)
ˆaij(1 − ˆaij)
Kij + 1
ˆaij(1 − ˆaij)
− 1
2
ij
Then,
ij =
Kij =
Finally, if the values Kij have the same order of magnitude, we
choose Ki = minj Kij. We thus guarantee that the coordinate
values of the candidate are well-spread around the mean while
falling in their corresponding interval [ˆaij ± ij] with high
probability since the standard deviations of Xij are slightly
greater or equal than the corresponding ij.
C. Tuning the algorithm
If a generated vector does not fulﬁl the constraints, we
simply discard it and generate a new one until all the con-
straints are satisﬁed. This may be however challenging if m
is large or if Kij have different orders of magnitude. We
proposed two simple solutions to overcome these problems.
1) m is large: If the Dirichlet sampler fails to generate a
DTMC candidate satisfying the [ˆai] constraints in state i, a
possibility is to multiply Ki by a value strictly greater than
1, for example, λ = 1.1. The goal is to smoothly reduce
the variance of each coordinate while preserving their relative
length, increasing the chance to sample all the coordinates in
their respective intervals.
2) Kij have different orders of magnitude: If Kij have
different orders of magnitude, choosing Ki as the minimum
of Kij may not be adequate since the relative variance of the
corresponding coordinates may be too large. Consequently, the
samples for these coordinates would likely fall out of their
corresponding interval and the resulting state distribution ai
would not satisfy the constraints of [ˆai]. Choosing the mean
or the median of Kij may be more efﬁcient though it does
not fully overcome this problem.
An other solution is to handle separately the transitions with
a ’large’ Kij and the ones with a signiﬁcantly ’lower’ Kij.
For sake of simplicity, assume that Ki0 is large with respect
a value ai0 in the interval [ˆai0 ± i0] ∩ [1 −(cid:2)m
to the other Kij and that these Kij have the same order of
(cid:2)m
magnitude. We proceed in two steps: (i) We select uniformly
ij; 1 −
−
ij]. The intersection guarantees the consistency of
ai0. Let β = 1 − ai0. (ii) Once ai0 has been selected, the
other transition values are sampled according to distribution
Yi ∼ βX(Ki(ˆaij))j(cid:2)(cid:7)=j where X is a Dirichlet distribution
parametrised by Ki(ˆaij). Then, for all j
(cid:4) (cid:12)= j,
j=1 ˆa
j=1 ˆa
+
(cid:2)
ERel[Yij(cid:2) ] = β
Kiˆaij(cid:2)
j(cid:2)(cid:7)=j Kiˆaij(cid:2) = ˆaij(cid:2)
(11)
and
=
K 2
VRel(Yij(cid:2) ) = β
i β2(Kiβ + 1)
2 Kiaij(cid:2) (Kiβ − Kiˆaij(cid:2) )
ˆaij(cid:2) (β − ˆaij(cid:2) )
(cid:16)
β , we thus ensure
VRel(Yij(cid:2) ) ≥ ij(cid:2) for all j > 0.
that ERel[Yij(cid:2) ] = ˆaij(cid:2) and
The procedure is thus repeated until all the values aij(cid:2) rely
in their corresponding interval. Then, (aij)0≤j≤m is the state
distribution in state i of candidate A(l).
By choosing Ki = minj(cid:2)(cid:7)=j
β−ˆaij
β2
ij(cid:2)
Kiβ + 1
− 1
(12)
V. DESCRIPTION OF THE ALGORITHM
We present
in this section the pseudo-algorithm of im-
portance sampling for IMC (Algorithm 1) and the pseudo-
algorithm for the random search optimisation (Algorithm 2).
For sake of simplicity, we have not included the cases ‘m is
large’ and ‘Kij have different orders of magnitude’ mentioned
in Section IV-C.
The goal of the algorithm is, given an IMC [ ˆA], an IS
distribution parametrised by B and the property φ, to output a
(1− δ)-conﬁdence interval deﬁned with respect to [ ˆA] instead
of ˆA. The inputs of the algorithm are conﬁdence parameter δ
and sample size N used to estimate γ.
Remark 5.1: Generating matrices A ∈ [ ˆA] and solving the
minimisation problem is independent of B. However, even if
the topic of this paper is not about how B is chosen, it remains
an interesting question to know if there exists a ‘better’ IS
distribution deﬁned with respect to the entire set of matrices in
[ ˆA]. In this work, we assume that the IS distribution is deﬁned
with respect to ˆA but note that other distributions could have
been chosen (for the better or the worst).
Traces are sampled from initial state s0 with respect to
probabilistic distribution B until φ is decided (Alg. 1, lines 3 to
5). Note that we do not need to store the entire trace. Instead,
for each trace ωk, we update on-the-ﬂy a table containing the
transitions si → sj of ωk and the number of times these
transitions have been taken nk(si, sj). This table is deﬁned
by the set of transitions Tk and their respective counters nk in
Algorithm 1 (lines 6 to 11). At line 13, notation 1(ωk |= φ)
is the indicator function and is equal to 1 if ωk |= φ and 0
otherwise. We denote Vk the set of visited states in ωk (apart
the last state of the trace), V and T the respective union of
Vk and Tk over all the traces. The symbolic likelihood ratio
of ωk is then entirely deﬁned by the k-th table. If ωk (cid:2) φ, the
table can be deleted since z(ωk)L(ωk) = 0.
the traces have been sampled,
the tables and
[ ˆA] deﬁne the minimisation problem described in (10). The
function to optimise is denoted f (A) at line 16 and we use
Algorithm (2) for this purpose. At line 17, g(A) denotes the
sum of the likelihood ratio squares for the successful paths
used in the evaluation of the standard deviations. Once the
arguments Amin and Amax of the minimum and maximum
have been determined, we evaluate ˆγN ( ˆAmin), ˆσN ( ˆAmin),
ˆγN ( ˆAmax) and ˆσN ( ˆAmax) (lines 19 to 22). Finally, we output
the ﬁnal (1 − δ)-conﬁdence interval CI = [L, U ] where:
Once all
L = ˆγN ( ˆAmin) − Φ
−1
1δ/2
ˆσN ( ˆAmin)
√
U = ˆγN ( ˆAmax) + Φ
−1
1δ/2
N
√
ˆσN ( ˆAmax)
N
.
VI. CASE STUDY
In the following, we conduct multiple case studies to
evaluate the efﬁciency of our algorithms. The challenge for
our approach is to show that we are able to provide more
reliable importance sampling conﬁdence intervals with the
IMC settings. Hence, we have chosen models for which we
are able to obtain accurate results using numerical techniques,
in order to compare them with the correct values.
The empirical coverage of the experiments is the proportion
of experiments in which the exact value γ falls within the ﬁnal
conﬁdence interval. To empirically verify our results we per-
formed each simulation experiment 100 times and report the
coverage of the experiments with respect to the approximated
DTMC ˆA and with the exact DTMC A. We use the same
IS distribution for IS experiments and IMCIS experiments
but
they are performed independently. The estimators are
based on N = 10000 traces. The optimisation is stopped
when the randomly generated candidates for the minimum
and the maximum are undefeated for R = 1000 rounds. All
simulations were performed using a Java prototype.
A. Illustrative example
The ﬁrst case study follows the example introduced in
Section III. The model under scrutiny is a DTMC parametrised
−4 and c = 0.05.
by two individual
transitions a = 10
308
Algorithm 1 IMC Importance Sampling (IMCIS)
Input: [ ˆA] : an IMC
B : an IS matrix
ϕ : a temporal property
δ : conﬁdence parameter
N : sample size
1: for k ∈ {1, . . . , N} do
ωk = x0, Vk = ∅, Tk = ∅
l = 1
while ωk |= φ is not decided do
generate sl under IS measure B
ωk = s0 ··· sl