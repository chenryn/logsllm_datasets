port allocation to happen at the AM. Figure 15 shows the distribu-
tion of latency incurred for SNAT port allocation over a 24 hour
window in a production data center for the requests that go to AM.
10% of the responses are within 50ms, 70% within 200ms and 99%
within 2 seconds. This implies that in the worst case, one in every
10000 connections suffers a SYN latency of 2 seconds or higher;
however, very few scenarios require that number of connections to
the same destination.
5.2.2 Availability
As part of ongoing monitoring for our cloud platform, we have
multiple test tenants deployed in each data center. A monitoring
service connects to the VIP of every test tenant from multiple geo-
graphic locations and fetches a web page once every ﬁve minutes.
Figure 16 shows average availability of test tenants in seven dif-
ferent data centers. If the availability was less than 100% for any
ﬁve minute interval, it makes up a point in the above graph. All
the other intervals had 100% availability. The average availability
216Figure 18: Bandwidth and CPU usage over a 24-hr period for 14 Muxes in one instance of Ananta.
ing bandwidth requirements and increasing price pressure made
us ﬁnally give up on hardware load balancers. Having our own
software-based load balancer has not been without any issues ei-
ther, however, we have been able to troubleshoot and ﬁx those is-
sues much faster – in days instead of weeks or months. Here we
look at some of those issues.
Ananta Manager uses Paxos to elect a primary replica that does
all the work. During the initial stages of Ananta, we assumed
that there can never be two replicas claiming to be primary at the
same time. However, we encountered issues where this assumption
was not true. This happens due to old hard disks where the disk
controller would freeze for two minutes or longer on the primary
replica. During this freeze, the secondary replicas would conclude
that the primary is down and elect a new primary. But once the disk
controller on the old primary becomes responsive again, it contin-
ues to do work assuming it is still the primary for a short period
of time. This resulted in customers facing outages because all the
host agents would continue to send health reports to the old primary
while Muxes would only accept commands from the new primary.
We ﬁxed this issue by having the primary perform a Paxos write
transaction whenever a Mux rejected its commands. This change
resulted in the old primary detecting its stale status as soon as it
would try to take any action.
We made an explicit design choice to use encapsulation to deliver
packets between the Muxes and hosts so that Muxes could load bal-
ance across layer-2 domains. This lowers the effective MTU avail-
able for transport layer payload. In order to avoid fragmentation
of packets at the Mux, which incurs high CPU overhead, Ananta
Host Agents adjust the MSS value exchanged as part of TCP con-
nection establishment to a lower value – 1440 from 1460 for IPv4.
This technique worked perfectly ﬁne for almost two years and then
we suddenly started seeing connection errors because clients were
sending full-sized packets (1460 byte payload) with IP Don’t Frag-
ment (DF) bit set. After encapsulation at the Muxes the ethernet
frame size (1520) exceeded network MTU of 1500, resulting in
packet drops. This started to happen because of two external bugs.
A speciﬁc brand of home routers has a bug in that it always over-
writes the TCP MSS value to be 1460. Therefore, clients never see
the adjusted MSS. Normally this is ﬁne because clients are sup-
posed to retry lost full-sized TCP segments by sending smaller-
sized segments. However, there was a bug in the TCP implementa-
tion of a popular mobile operating system in that TCP retries used
the same full-sized segments. Since we control our entire network,
we increased the MTU on our network to a higher value so that it
can accommodate encapsulated packets without fragmentation.
One design decision that has been a subject of many debates is
collocation of data plane and its control via BGP on Mux. An al-
ternative design would be to host BGP in a separate service and
control it from AM. Collocation works better for hard failures as
routers quickly take an unhealthy Mux out of rotation due to BGP
timeout. However, when incoming packet rate exceeds the process-
ing capacity of a Mux, the BGP trafﬁc gets affected along with data
trafﬁc. This causes the Mux to lose BGP connection to the router
and that trafﬁc shifts to another Mux, which in turn gets overloaded
and goes down, and so on. This cascading failure can bring down
all the Muxes in a cluster. There are multiple ways to address this
limitation. One solution is to use two different network interfaces
– one for control plane and the other for data plane. Another solu-
tion is to rate limit data trafﬁc at the router so that there is always
sufﬁcient headroom for control trafﬁc. The same mechanisms will
be required to separate control trafﬁc from data trafﬁc even if BGP
was hosted in a separate service. Overall, we found that collocation
leads to a simpler design in this case.
One of the issues with using hardware load balancers was that
we had to set an aggressive idle connection timeout ( 60 seconds),
otherwise legitimate connections were getting dropped under heavy
load, e.g., under connection state-based attacks. In the initial ver-
sions of Ananta, we kept the same aggressive idle timeout. How-
ever, with the increase in the number of low-power, battery-operated
mobile devices, there was a need to keep connections open for a
long time even where there is no active trafﬁc. For example, a
phone notiﬁcation service needs to push data immediately to such
devices without requiring them to send frequent keep-alive mes-
sages. We were able to increase idle timeout in Ananta because it
keeps the NAT state on the hosts and, under overload, the Muxes
can continue forwarding trafﬁc using the VIP map (§3.3) without
creating new per-connection state. This makes Ananta signiﬁcantly
less vulnerable to connection state-based attacks.
7. RELATED WORK
To the best of our knowledge, Ananta takes a new approach to
building layer-4 distributed load balancer and NAT that meets the
requirements of a multi-tenant cloud environment. In this section
we compare it to the state-of-the-art in industry and research. We
recognize that many existing solutions provide more than layer-4
load balancing and their design choices may have been inﬂuenced
by those additional features. Ananta’s modular approach can be
used to build application-layer functionality, such as layer-7 load
balancing, on top of the base layer-4 functionality.
Traditional hardware load balancers (e.g.,
[9, 1]) have a scale
217up design instead of a scale out design. Furthermore, they provide
1 + 1 redundancy, which can leave the system with no redundancy
during times of repair and upgrade. Cloud environments need N +1
redundancy to meet the high uptime requirements. Another key
challenge with hardware appliances is their inability to scale up or
down with demand.
Many vendors now provide load balancing software that can run
in virtual machines on general purpose server hardware [17, 15].
Some of these solutions are based on the open source HA Proxy
software [12]. These virtual appliances have two limitations that
made them unsuitable for our cloud environment. First, they pro-
vide 1 + 1 redundancy, similar to hardware load balancers. Opera-
tors work around this limitation by migrating IP addresses from the
failed VM to a spare, which forces them to deploy multiple load
balancers in the same layer-2 domain leading to fragmentation of
capacity. Second, these appliances cannot scale a single VIP be-
yond the capacity of a single VM. With this design, any service
that needs to scale beyond the capacity of a single device uses mul-
tiple VIPs, which has several drawbacks as discussed in §3.7.
Embrane [8] promises the beneﬁts of using software, including
scale out. Ananta differs from Embrane in that it ofﬂoads signﬁ-
cant functionality to the Host Agent and leverages existing routers
for scale out. Egi et al [7] show that high performance routers can
be built using commodity hardware. More recent efforts, such as
RouteBricks [6], have shown that a commodity server can easily
achieve 12.8Gbps for minimum-sized packets on commodity hard-
ware. By carefully partitioning the load balancing workload, we
have reduced the in-network processing to be similar to routing.
Therefore, these results for routing support our approach of imple-
menting data plane on commodity hardware.
ETTM [26] is similar to Ananta in many aspects, e.g., every
end-host participates in packet processing in both designs. How-
ever, a key difference is that Ananta implements in-network routing
in dedicated commodity servers, which enables it to overcome the
limited packet modiﬁcation options in current hardware.
8. CONCLUSION
Ananta is a distributed layer-4 load balancer and NAT speciﬁ-
cally designed to meet the scale, reliability, tenant isolation and op-
erational requirements of multi-tenant cloud environments. While
its design was heavily inﬂuenced by the Windows Azure public
cloud, many design decisions may apply to building other middle-
boxes that require similarly scalable packet processing at low cost,
e.g., intrusion detection systems or virtual private network (VPN).
We started by reexamining the requirements of a large-scale cloud
and concluded that we needed a solution that scales with the size
of the network while maintaining low cost and operational over-
head. These requirements led us to build a scale-out data plane by
leveraging existing routing primitives and ofﬂoading some heavy
packet processing tasks to the host. We coordinate state across the
distributed data plane using a highly available control plane. This
design enables direct server return and scalable NAT across layer-
2 domains. Furthermore, within the data center, the load balancer
gets out of the way of normal trafﬁc ﬂow, thereby enabling unfet-
tered network bandwidth between services. Ananta delivers tenant
isolation by throttling and isolating heavy users.
Over a 100 instances of Ananta have been deployed in the Win-
dows Azure public cloud, serving over 100,000 VIPs. Building our
own software solution gives us control over a very important van-
tage point in our network as it sees most incoming and outgoing
trafﬁc and is key to maintaining high availability for our tenants.
Acknowledgements
The many comments from our shepherd John Heidemann and anony-
mous reviewers greatly improved the ﬁnal version of this paper.
We would like to thank additional contributors to Ananta: Yue
Zuo, Nancy Williams, Somesh Chaturmohta, Narayanan Sethu-
raman, Jayendran Srinivasan, Vladimir Ivanov, Nisheeth Srivas-
tava, Naveen Prabhat, Shangguang Wu, Thiruvengadam Venkete-
san, Luis Irun-Briz, Narayan Annamalai, Lin Zhong, Greg Lapin-
ski, Ankur Agiwal and Deepali Bhardwaj.
We would also like to thank the following members of the Au-
topilot team with whom we shared principles and insights gained
as they built a production load balancer to support services like
bing.com: Saby Mahajan, Chao-Chih Chen, Pradeep Mani, Chao
Zhang, Arun Navasivasakthivelsamy, Shikhar Suri and Long Zhang.
Finally, we would like to thank Yousef Khalidi, Reza Baghai,
G S Rana, Mike Neil, Satya Nadella, and the rest of the Windows
Azure team for their support.
9. REFERENCES
[1] A10 Networks AX Series. http://www.a10networks.com.
[2] Aryaka WAN Optimization. http://www.aryaka.com.
[3] Amazon Web Services. http://aws.amazon.com.
[4] Microsoft Windows Azure. http://www.windowsazure.com.
[5] T. Benson, A. Akella, A. Shaikh, and S. Sahu. CloudNaaS: A Cloud
Networking Platform for Enterprise Applications. In Symposium on Cloud
Computing, 2011.
[6] M. Dobrescu, N. Egi, K. Argyraki, B.-G. Chun, K. Fall, G. Iannaccone,
A. Knies, M. Manesh, and S. Ratnasamy. RouteBricks: exploiting parallelism
to scale software routers. In SOSP, 2009.
[7] N. Egi, A. Greenhalgh, M. Handley, M. Hoerdt, F. Huici, and L. Mathy.
Towards high performance virtual routers on commodity hardware. In CoNEXT,
2008.
[8] Embrane. http://www.embrane.com.
[9] F5 BIG-IP. http://www.f5.com.
[10] Google Cloud Platform. http://cloud.google.com.
[11] A. Greenberg et al. VL2: A scalable and ﬂexible data center network. In
SIGCOMM, 2009.
[12] HA Proxy Load Balancer. http://haproxy.1wt.eu.
[13] A. Heffernan. RFC 2385: Protection of BGP Sessions via the TCP MD5
Signature Option, 1998.
[14] L. Lamport. The Part-Time Parliament. ACM TOCS, 16(2):133–169, May 1998.
[15] LoadBalancer.org Virtual Appliance.
http://www.load-balancer.org.
[16] N. Mckeown, T. Anderson, H. Balakrishnan, G. M. Parulkar, L. L. Peterson,
J. Rexford, S. Shenker, and J. S. Turner. OpenFlow: Enabling Innovation in
Campus Networks. In SIGCOMM, 2008.
[17] NetScalar VPX Virtual Appliance. http://www.citrix.com.
[18] C. Perkins. RFC 2003: IP Encapsulation within IP, 1996.
[19] Rackspace. http://www.rackspace.com.
[20] Y. Rekhter, T. Li, and S. Hares. RFC 4271: A Border Gateway Protocol 4
(BGP-4), 2006.
[21] Riverbed Virtual Steelhead. http://www.riverbed.com.
[22] Receive Side Scaling. http://msdn.microsoft.com.
[23] V. Sekar, S. Ratnasamy, M. K. Reiter, N. Egi, and G. Shi. The Middlebox
Manifesto: Enabling Innovation in Middlebox Deployment. In HotNets, 2011.
[24] J. Sherry, S. Hasan, C. Scott, A. Krishnamurthy, S. Ratnasamy, and V. Sekar.
Making Middleboxes Someone Else’s Problem: Network Processing as a Cloud
Service. In SIGCOMM, 2012.
[25] D. Thaler and C. Hopps. RFC 2991: Multipath Issues in Unicast and Multicast
Next-Hop Selection, 2000.
[26] H. Uppal, V. Brajkovic, D. Brandon, T. Anderson, and A. Krishnamurthy.
ETTM: A Scalable Fault Tolerant Network Manager. In NSDI, 2011.
[27] Vyatta Software Middlebox. http://www.vyatta.com.
[28] R. Wang, D. Butnariu, and J. Rexford. OpenFlow-Based Server Load Balancing
GoneWild. In Hot-ICE, 2011.
[29] M. Welsh, D. Culler, and E. Brewer. SEDA: An Architecture for
Well-Conditioned, Scalable Internet Services. In SOSP, 2001.
[30] Windows Filtering Platform. http://msdn.microsoft.com.
[31] ZScalar Cloud Security. http://www.zscalar.com.
218