ligible. The time required for the complete process ranges
from 0.012 to 0.109 seconds, with an average value of 0.052.
Thus, the main overhead of our approach is loading the
photo and the template layers from the ﬁlesystem, retrieving
the access permissions from the database for each depicted
user, and deciding which faces the accessing user should
view. This process is dependent on the number of people
depicted in the photo, the permissions of each user and their
number of friends. In our experiments, we selected a user
with 452 friends, which is higher than the average of 344.
(a) The photo is uploaded,
faces are detected and marked
for tagging.
(b) User requests access. Photos
are blurred selectively, according
to depicted users’ permissions.
Figure 10: Screenshot of our proof-of-concept application.
Installation. When the Face/Oﬀ application is installed
by the users, it requests permissions for reading and manag-
ing users’ friend-lists. These enable us to allow the user to
create custom friend-lists within the application. When the
user loads the app, it is authorized through the Facebook
authentication mechanism and the application’s database is
updated with the user’s current friend-lists. This allows us
to easily keep track of newly created lists, of users that have
been un-friended or simply removed from some of the lists
(a simple way to revoke permissions).
Initial photo review. The application ﬁrst determines
if any new photos that contain a tag of the user have been
uploaded.
In such a case, thumbnails of these photos are
presented to the user, who is able to load each photo for
inspecting the tag and for choosing which of his friend-lists
are permitted to access it. It should be noted that the face of
the user remains hidden to others as long as the permissions
have not been set, similarly to the case where the viewer has
not been granted access.
Face Detection. When a new photo is uploaded our ap-
plication performs face detection, and the detected faces are
marked, as shown in Figure 10a. The main omittance of our
proof-of-concept implementation is that we do not perform
face recognition but rely on the uploader to tag the photo.
Similarly, Facebook prompts the user to assign names to
the detected faces upon each uploaded photo. We decided
to only implement face detection but not recognition as that
would have required us to collect the Facebook photos of the
user and all of his friends to achieve accurate face recogni-
tion results. However, Facebook has acquired face.com and
according to a comparative study [28], the face recognition
algorithm of face.com was the most accurate and eﬀective
tested. Moreover, in [44] the authors state that they achieve
a 97.5% identiﬁcation rate.
The server generates and stores a unique photoID for the
uploaded photo and a faceID for each one of the faces. For
the generation of IDs the server uses the userIDs of the up-
loader and each one of the tagged user, the server’s internal
time and a one-way hash function. After that, the server
starts processing the image by cropping and blurring the
depicted faces. This functionality does not aﬀect user expe-
rience as all the processing is performed in the background.
Photo rendering. When access to a photo is requested,
we fetch all the information of this photo and its tags, and
determine which faces can be revealed and which should
remain hidden, by checking the users’ friend-lists. Then, we
generate a processed image “on the ﬂy”, by superimposing
Figure 11: The total time required for serving a photo, which
includes reading the access permission and blurring a face.
According to [44], the complete processing of an image
for face identiﬁcation, conducted by Facebook, lasts 0.33
seconds when executed on a single core machine. Thus, on
average, our ﬁne-grained access control will incur at most a
15.6% increase of the duration of the photo processing al-
ready conducted (if other processing is done, the overhead
will be even less). Moreover, these values will be much lower
when executed on high-end servers found in the data cen-
ters of major web services. Also, our proof-of-concept im-
plementation can be optimized, which will further reduce
the overhead. Overall, we believe that this small overhead
is justiﬁed by the privacy gain the users will beneﬁt from.
Scalability. In an attempt to further explore the perfor-
mance and the scalability of our approach, we select another
set of 100 random photos that contain at least three depicted
faces. At ﬁrst, we upload all the photos, tag a single face
in each photo and access them from multiple accounts that
are not allowed to view the face. We repeat this process 2
more times, by uploading again the same photos and tagging
two and three of the depicted faces respectively. The three
tagged users have 452, 1173 and 442 friends. Each extra tag
increased the processing time by 0.002 seconds.
From the last experiment, we can conclude that our mech-
anism is scalable, as the number of tags in a photo and the
number of the tagged users friends has a very small impact
on the performance of the system. It can be observed, that
the bulk of processing time is spent on fetching the photo
from the ﬁlesystem, and not on retrieving the access lists or
computing the permissions. While our experiments are not
an extensive measurement of the overhead of our approach
under all possible scenarios, they are indicative of the small
overhead imposed by our access control mechanism.
6.2 Privacy Evaluation
To evaluate the eﬀectiveness of our approach in preventing
the identiﬁcation of depicted users, we invited the partici-
pants of the risk analysis study (Section 3) to take part in an
experiment where we would apply our approach to photos
of their friends. The 34 users that participated were shown
a set of randomly selected photos of their contacts, with one
friend “hidden” in each photo, and were requested to identify
the hidden friend. In cases where they supplied a guess for
the hidden user, they were also required to provide feedback
regarding the visual clues that inﬂuenced their guessing. To
reﬂect actual use cases, all photos depicted multiple people.
Ideally, this experiment would be conducted by deploy-
ing our proof-of-concept application at full scale and asking
Figure 12: Identiﬁcation of hidden contacts (95% conﬁdence
interval). For correct answers, we break down the visual
clues that led to the identiﬁcation.
the participants to identify their restricted friends within
each accessed photo. This would allow us to ensure the
“freshness” of the photos, and avoid using photos that the
participants have previously seen. However, this experimen-
tal setup requires the participants’ friends to also install the
application and upload new photos, which poses many prac-
tical diﬃculties. If only a small number of the user’s friends
installs the application, the pool of users to “hide” will be
limited, and results could be heavily biased.
Thus, we opt for an alternative experimental setup; we use
photos collected during the risk analysis study. To obtain an
accurate evaluation of the privacy oﬀered by our approach,
we do not consider photos where the user feedback stated
that they remembered seeing them before. First, we ran-
domly select a set of photos that depict at least one of the
participant’s friends. Apart from containing the tag of a
friend, we also ensure that they have not been uploaded by
our participants, nor do they contain their tag. Moreover,
we manually verify the correctness of tag placement, which
will result in the hidden area. Then, our mechanism blurs
out the friend’s face in each photo, and presents the photo
challenge to the participants.
The results of our experiment are shown in Figure 12.
We prepared and presented a total of 476 challenges, out
of which 448 had not been seen before by the participants,
according to their feedback. We manually veriﬁed answers to
avoid erroneous evaluation due to spelling mistakes. Users
stated that they could not identify their friends, and did not
suggest a name, for 82.7% of photos they were shown. On
average, users correctly identiﬁed the hidden user in 12.6%
of their challenges, and gave a wrong answer for 4.6%.
As can be seen, the dominating clue for correctly guessing
the identity of a restricted user was the existence of other
people within the photo known by the participant. The non-
restricted people in the photo allowed users to correctly infer
the hidden user in 66.7% of the cases. In 19.6% of the iden-
tiﬁed challenges, the body or hair led to identiﬁcation, while
clothes were helpful in 13.6%. Thus, while other people are
the dominating reason for inferring the identity of the user,
other visual clues that can be potentially removed, have sig-
niﬁcant contribution. We discuss how we plan to extend our
approach for mitigating this eﬀect in Section 7.
These numbers oﬀer a upper bound as, in practice, users
may be presented with multiple hidden faces in a photo,
which will make identiﬁcation harder. Furthermore, the par-
 0.0001 0.001 0.01 0.1 10 20 30 40 50 60 70 80 90 100Time (sec)Accessed PhotoTag BlurringCreate Photo 0 20 40 60 80 100Don’t KnowWrongHidden Friends (%)Given Answer 0 20 40 60 80 100Don’t KnowWrongCorrectHidden Friends (%)Given Answer 0 20 40 60Identified Friends (%)Identification CluesOther PeopleBody/HairClothes/Apparelsdemonstrating our application. Only 3.8% of the partici-
pants maintained a negative opinion, and 19.2% remained
neutral. Almost 77% of the users wanted such a mechanism
to be adopted. We observed that most of the initially neg-
ative and neutral participants care about their privacy, but
were not aware of the current access control mechanisms and
the visibility of their data. Moreover, several of the initially
negative users, having stated that they do not care about
privacy, became neutral and accepted the necessity of such
a mechanism, as they recognized the privacy needs of others.
Finally, we asked users to assess the usability of our ap-
proach, in a 5-point rating scale. 86.5% of the users rated
our mechanism as usable and very usable (4 and 5 points).
11.5% and 1.9% of the users rated the mechanism with 3
and 2 points respectively, due to the lack of an option for
assigning the same permissive lists to multiple photos, at
once. This, however, does not impact the usability of our
approach, as this concerns our proof-of-concept implemen-
tation, and not the core access control mechanism, and can
be easily addressed in the future.
7. LIMITATIONS AND FUTURE WORK
User Removal. A recent user study [33] demonstrated
that users are eﬀective at recognizing their friends even in
photos where their face is not clearly visible. However, in
the study, users were signiﬁcantly aided as they had to select
from a list of 6 possible friends. In our study, participants
were able to only guess the identity of 12.6% of the users.
Thus, while our current approach oﬀers a signiﬁcant step
towards a more privacy-preserving sharing of content within
OSNs, we plan to explore methods to further improve eﬀec-
tiveness. Speciﬁcally, we plan to explore the feasibility of
completely removing the depicted user from the presented
photo. A large body of work has demonstrated eﬀective
techniques for automatically removing objects from images
and reconstructing the aﬀected region (e.g. [17, 22]) with
performance suitable for processing big data [47]. Thus, af-
ter the user’s body/pose is identiﬁed [38], the photo can be
processed to completely remove him/her.
Collateral Inference. Even with our mechanism in place,
a user’s identity might be inferred from information found
in the photo’s comments. As such, further exploration is
required for determining the extensibility of our mechanism
to also handle comments associated with a photo.
Identiﬁcation accuracy. The eﬀectiveness of our ap-
proach relies, to an extent, on the accuracy of the face iden-
tiﬁcation software employed by the social network. To pre-
vent malicious user behavior, such as uploaders not tagging
users (to prevent the users from hiding their face), or falsely
tagging faces, our system has to employ highly accurate soft-
ware for the identiﬁcation of the depicted users. According
to Taigman et al. [44] Facebook’s method reaches an accu-
racy of 97.35%, rendering it suitable for our approach. In
cases where a face cannot be identiﬁed, users may be asked
to provide a suggestion and the system can accept answers
only if there is consensus among several users.
Non-members. A case where our approach cannot pro-
tect a user’s privacy, is when a photo depicts a user who does
not have an account in the social network. If such an event
occurs, various approaches can be applied, such as following
a strict permission where all such faces are hidden, or a more
lenient setting where the photo uploader is considered the
owner and applies the privacy setting.
Figure 13: Willingness of users to adopt a mechanism that
blur out faces in uploaded photos. Users’ opinion before and
after a short demonstration of our proof-of-concept app.
ticipants knew that the hidden users were friends of theirs.
In an actual deployment, not all the hidden users will be
contacts of theirs, which will increase uncertainty and may
result in even less identiﬁed users. Overall, while the number
of participants is relatively small, our results are promising
as they indicate the eﬀectiveness of our approach in hiding
the identity of users from their contacts.
6.3 Adoption Willingness
A crucial factor in determining the merits of our approach,
is the attitude of users towards the potential adoption of
our system by popular services. To explore that aspect,
we conducted a user study for identifying the willingness of
users to adopt and use a face-level ﬁne-grained access control
mechanism. To obtain a more objective understanding of
users’ opinion, we opted for a set of new subjects that had
not participated in any of our previous experiments and were
unaware of our approach. This oﬀered an unbiased view of
how people will react to such a mechanism being deployed.
A total of 52 users participated, with 65.4% being male and
34.6% female, all in the age range of 18-36.
First, we presented a photo processed by our mechanism
that contained some hidden faces, and asked users if they
would like such a mechanism to be implemented by photo-
sharing social networking services. After the users’ response,
we presented the privacy implications that arise from con-
ﬂicts of interest and brieﬂy demonstrated our proof-of-concept
application. Users were allowed to interact with it. Then,
we asked them if they wanted OSNs to adopt such a mecha-
nism, selecting from answers modelled after the Likert scale.
The results are shown in Figure 13. Initially almost 27%
of the participants were against the adoption of such a mech-