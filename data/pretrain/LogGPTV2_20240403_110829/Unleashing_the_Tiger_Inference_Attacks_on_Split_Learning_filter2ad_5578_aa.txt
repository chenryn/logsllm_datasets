title:Unleashing the Tiger: Inference Attacks on Split Learning
author:Dario Pasquini and
Giuseppe Ateniese and
Massimo Bernaschi
Unleashing the Tiger: Inference Attacks on Split Learning
Dario Pasquini
Sapienza University of Rome
Institute of Applied Computing, CNR
Rome, Italy
Rome, Italy
PI:EMAIL
Giuseppe Ateniese
George Mason University
Fairfax, Virginia, USA
PI:EMAIL
Massimo Bernaschi
Institute of Applied Computing, CNR
Rome, Italy
PI:EMAIL
ABSTRACT
We investigate the security of split learningâ€”a novel collaborative
machine learning framework that enables peak performance by
requiring minimal resource consumption. In the present paper, we
expose vulnerabilities of the protocol and demonstrate its inherent
insecurity by introducing general attack strategies targeting the
reconstruction of clientsâ€™ private training sets. More prominently,
we show that a malicious server can actively hijack the learning
process of the distributed model and bring it into an insecure state
that enables inference attacks on clientsâ€™ data. We implement differ-
ent adaptations of the attack and test them on various datasets as
well as within realistic threat scenarios. We demonstrate that our
attack can overcome recently proposed defensive techniques aimed
at enhancing the security of the split learning protocol. Finally, we
also illustrate the protocolâ€™s insecurity against malicious clients by
extending previously devised attacks for Federated Learning.
CCS CONCEPTS
â€¢ Security and privacy â†’ Software and application security;
Privacy-preserving protocols; Distributed systems security.
KEYWORDS
Collaborative learning; ML Security; Deep Learning
ACM Reference Format:
Dario Pasquini, Giuseppe Ateniese, and Massimo Bernaschi. 2021. Unleash-
ing the Tiger: Inference Attacks on Split Learning. In Proceedings of the 2021
ACM SIGSAC Conference on Computer and Communications Security (CCS
â€™21), November 15â€“19, 2021, Virtual Event, Republic of Korea. ACM, New York,
NY, USA, 17 pages. https://doi.org/10.1145/3460120.3485259
1 INTRODUCTION
Once the cattle have been split up, then the tiger strikes.
A Myanma proverb
Deep learning requires massive data sets and computational
power. State-of-the-art neural networks may contain millions or bil-
lions [13] of free parameters and necessitate representative training
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea
Â© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8454-4/21/11...$15.00
https://doi.org/10.1145/3460120.3485259
sets. Unfortunately, collecting suitable data sets is difficult or some-
times impossible. Entities and organizations may not be willing to
share their internal data for fear of releasing sensitive information.
For instance, telecommunication companies would benefit extraor-
dinarily from deep learning techniques but do not wish to release
customer data to their competitors. Similarly, medical institutions
cannot share information because privacy laws and regulations
shelter patient data.
Secure data sharing and learning can only be achieved via cryp-
tographic techniques, such as homomorphic encryption or secure
multi-party computation. However, the combination of cryptogra-
phy and deep learning algorithms yields expensive protocols. An
alternative approach, with mixed results, is distributed/decentral-
ized machine learning, where different parties cooperate to learn
a shared model. In this paradigm, training sets are never shared
directly. In federated learning [11, 35, 36], for example, users train
a shared neural network on their respective local training sets
and provide only model parameters to others. The expectation is
that by sharing certain model parameters, possibly â€œscrambledâ€ [3],
the actual training instances remain hidden and inscrutable. Un-
fortunately, in [30], it was shown that an adversary could infer
meaningful information on training instances by observing how
shared model parameters evolve over time.
Split learning is another emerging solution that is gaining sub-
stantial interest in academia and industry. In the last few years,
a growing body of empirical studies [5, 22, 33, 34, 39, 42, 49,
52, 56, 57], model extensions [4, 15, 31, 41, 44, 46, 51, 54, 55],
and events [2, 12] attested to the effectiveness, efficiency, and
relevance of the split learning framework. At the same time,
split-learning-based solutions have been implemented and adopted
in commercial as well as open-source applications [1, 6]. Several
start-ups, which are receiving much attention, are currently relying
on the split learning framework to develop efficient collaborative
learning protocols and train deep models on real-world data.
The success of split learning is primarily due to its practical
properties. Indeed, compared with other approaches such as feder-
ated learning, split learning requires consistently fewer resources
from the participating clients, enabling lightweight and scalable
distributed training solutions. However, while the practical proper-
ties of split learning have been exhaustively validated [49, 57], little
effort has been spent investigating the security of this machine
learning framework.
In this paper, we carry out the first, in-depth, security analy-
sis of split learning and draw attention to its inherent insecu-
rity. We demonstrate that the assumptions on which the security
of split learning is based are fundamentally flawed, and a motivated
adversary can easily subvert the defenses of the training framework.
Session 7A: Privacy Attacks and Defenses for ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2113In particular, we implement a general attack strategy that allows a
malicious server to recover private training instances during the
distributed training. In the attack, the server hijacks the modelâ€™s
learning processes and drives them to an insecure state that can
be exploited for inference attacks. In the process, the attacker does
not need to know any portion of the clientâ€™s private training sets
or the clientâ€™s architecture. The attack is domain-independent and
can be seamlessly applied to various split learning variants [51, 54].
We call this general attack: the feature-space hijacking attack
(FSHA) and introduce several adaptations of it. We test the proposed
attacks on different datasets and demonstrate their applicability
under realistic threat scenarios such as data-bounded adversaries.
Furthermore, we show that client-side attacks that have been
previously devised on federated learning settings remain effective
within the split learning framework. In particular, we adapt and
extend the inference attack proposed in [30] to make it work in
split learning. Our attack demonstrates how a malicious client
can recover suitable approximations of private training instances
of other honest clients participating in the distributed training.
Eventually, this result confirms the insecurity of split learning also
against client-side attacks.
To make our results reproducible, we made our code available1.
Overview. The paper starts by surveying distributed machine
learning frameworks in Section 2. Section 3 follows by introducing
and validating our main contributionâ€”the feature-space hijacking
attack framework. Then, Section 4 covers the applicability of exist-
ing defensive mechanisms within the split learning framework. In
Section 5, we analyze the security of split learning against malicious
clients. Section 6 concludes the paper, with Appendices containing
additional material. In the paper, background and analysis of pre-
vious works are provided, when necessary, within the respective
sections.
2 DISTRIBUTED MACHINE LEARNING
Distributed (also collaborative [47]) machine learning allows a set
of remote clients Cs = {ğ‘1, . . . , ğ‘ğ‘›} to train a shared model ğ¹. Each
client ğ‘ğ‘– participates in the training protocol by providing a set
of training instances ğ‘‹ğ‘ğ‘Ÿğ‘–ğ‘£ğ‘–. This set is private and must not be
directly shared among the parties running the protocol. For instance,
1https://github.com/pasquini-dario/SplitNN_FSHA
as follows:
The contributions of the present paper can be then summarized
â€¢ We demonstrate the insecurity of split learning against a
malicious server by devising a novel and general attack
framework. Such a framework permits an attacker to (1) re-
cover precise reconstructions of individual clientsâ€™ train-
ing instances as well as (2) perform property inference at-
tacks [8] for arbitrary attributes. Additionally, we show that
the proposed attacks can circumvent defensive techniques
devised for split learning [55, 58].
â€¢ We demonstrate the insecurity of split learning against a
malicious client by adapting and extending previously pro-
posed techniques targeting federated learning [30]. The at-
tack permits a malicious client to recover prototypical exam-
ples of honest clientsâ€™ private instances.
hospitals cannot share patientsâ€™ data with external entities due to
regulations such as HIPAA [7].
In this section, we focus on distributed machine learning solu-
tions for deep learning models. In particular, we describe: (1) Fed-
erated learning [11, 35, 36] which is a well-established learning
protocol2 and (2) split learning [25, 42, 56] a recently proposed
approach that is gaining momentum due to its attractive practical
properties.
2.1 Federated Learning
Federated learning [11, 35, 36] allows for distributed training of
a deep neural model by aggregating and synchronizing local pa-
rameter adjustments among groups of remote clients. In the most
straightforward setup, the protocol is orchestrated by a central
server that manages clientsâ€™ training rounds and maintains a mas-
ter copy of the trained model.
In the initial setup phase, the parties choose a training task and
define a machine learning model. The latter is initialized and hosted
by the server that makes it available to all remote clients. At each
training step, each client downloads the model from the server
and locally applies one or more iterations of standard Stochastic
Gradient Descent (SGD) using its private training set. After the
local training is done, clients send the accumulated gradients (or
weights) to the server.3 The server aggregates these changes into
a single training signal applied to the hosted model parameters,
completing a global training iteration. Once the serverâ€™s network
is updated, the clients download the new state of the model and
repeat the protocol till a stop condition is reached.
At each iteration in federated learning, clients exchange an
amount of data with the server that is linear in the number of
parameters of the network. For large models, this becomes unsus-
tainable and may limit the applicability of the approach. Several
improvements to the framework have been proposed to address
this problem [45, 59].
2.1.1 On the security of Federated Learning. Clients share only
gradients/weights induced by the local training steps. The intuition
behind federated learning is that local data is safe because it is
never directly shared with the server or other clients. Additionally,
gradients collected by the server can be further protected through a
secure aggregation protocol. The aim is to hinder inference attacks
by the server that cannot distinguish clientsâ€™ individual gradients.
In federated learning, all the parties have equal access to the
trained network. Thus, the server and the clients know the archi-
tecture of the network as well as its weights during the various
training steps.
Under suitable assumptions, different attacks on federate learn-
ing were shown feasible. The first and most prominent is an active
attack [30] that allows a malicious client to infer relevant infor-
mation on training sets of other honest clients by manipulating
the learning process. Additionally, the gradients received from the
server can be inverted [64]. Other attacks include backdoor injec-
tion and poisoning [9, 10, 17]. Accordingly, variants of federated
2In the paper, we use the term â€œfederated learningâ€ to refer to the framework
proposed in [11, 35, 36] rather than the â€œfederated learning taskâ€ .
3This process may differ in practice as there are several implementations of
federated learning.
Session 7A: Privacy Attacks and Defenses for ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2114Server:
ğ‘‹ğ‘ğ‘Ÿğ‘–ğ‘£
ğ‘“ğ‘“
ğ‘ ğ‘ 
Lğ‘“ ,ğ‘ 
Server:
ğ‘ 
ğ‘ ğ‘ 
ğ‘‹ğ‘ğ‘Ÿğ‘–ğ‘£
Lğ‘“ ,ğ‘“ â€²,ğ‘ 
ğ‘“ğ‘“
â€²
â€²
ğ‘“
ğ‘“
(a) Split learning.
(b) Split learning with labels protection.
Figure 1: Two variations of split learning. Black arrows depict the activation propagation of the participating neural networks,
whereas red arrows depict the gradient that follows after the forward pass.
learning have been proposed to reduce the effectiveness of those
attacks [19, 20, 27, 32]. They alleviate but do not solve the problems.
2.2 Split Learning
Split learning [25, 42, 56] enables distributed learning by partition-
ing a neural network in consecutive chunks of layers among various
parties; typically, a set of clients and a server. In the protocol, the
clients aim at learning a shared deep neural network by securely
combining their private training sets. The server manages this pro-
cess and guides the networkâ€™s training, bearing most of the required
computational cost.
In split learning, training is performed through a vertically dis-
tributed back-propagation [38] that requires clients to share only
intermediate networkâ€™s outputs (referred to as smashed data); rather
than the raw, private training instances. This mechanism is sketched
in Figure 1. In the minimal setup (i.e., Figure 1a), a client owns the
first ğ‘› layers ğ‘“ of the model, whereas the server maintains the
remaining neural network ğ‘  i.e., ğ¹ = ğ‘ (ğ‘“ (Â·)). Here, the modelâ€™s
architecture and hyper-parameters are decided by the set of clients
before the training phase. In particular, they agree on a suitable
partition of the deep learning model and send the necessary infor-
mation to the server. The server has no decisional power and
ignores the initial split ğ‘“ .
At each training iteration, a client sends the output of the initial