title:Privacy-preserving and Optimal Interval Release for Disease Susceptibility
author:Kosuke Kusano and
Ichiro Takeuchi and
Jun Sakuma
Privacy-preserving and Optimal Interval Release for
Disease Susceptibility
Kosuke Kusano
University of Tsukuba
Tsukuba, Japan
cocuh@
mdl.cs.tsukuba.ac.jp
Ichiro Takeuchi
Jun Sakuma
Nagoya Institute of Technology
/ RIKEN Center for AIP
University of Tsukuba / JST
CREST / RIKEN Center for
Nagoya, Japan
takeuchi.ichiro@
nitech.ac.jp
AIP
Tsukuba, Japan
PI:EMAIL
ABSTRACT
In this paper, we consider the problem of privacy-preserving
release of function outputs that take private information as
input. Disease susceptibilities are known to be associated
with clinical features (e.g., age, sex) as well as genetic fea-
tures represented by SNPs of individuals. Releasing out-
puts are not privacy-preserving if the private input can be
uniquely identi(cid:12)ed by probabilistic inference using the out-
puts. To release useful outputs with preserving privacy, we
present a mechanism that releases an interval as output, in-
stead of an output value. We suppose adversaries perform
probabilistic inference using released outputs to sharpen the
posterior distribution of the target attributes. Then, our
mechanism has two signi(cid:12)cant properties. First, when our
mechanism provides the output, the increase of the adver-
sary’s posterior on any input attribute is upper-bounded by
a prescribed level. Second, under this privacy constraint,
the mechanism can provide the narrowest (optimal) interval
that includes the true output. Building such a mechanism
is often intractable. We formulate the design of the mecha-
nism as a discrete constraint optimization problem so that
it is solvable in a practical computation time. We also pro-
pose an algorithm to obtain the optimal mechanism based
on dynamic programming. After applying our mechanism
to release disease susceptibilities of obesity, we demonstrate
that our mechanism performs better than existing methods
in terms of privacy and utility.
Keywords
Privacy; Genome; Disease Susceptibility; Input Inference;
Interval Publication
1.
INTRODUCTION
Single nucleotide polymorphisms (SNPs) represent indi-
vidual diﬀerences in DNA sequence variations. Current tech-
nological advances in molecular biology enable us to measure
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
ASIA CCS ’17 April 02-06, 2017, Abu Dhabi, United Arab Emirates
c⃝ 2017 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-4944-4/17/04.
DOI: http://dx.doi.org/10.1145/3052973.3053021
numerous SNPs at a reasonable cost. Genome-wide asso-
ciation studies (GWAS) have revealed that gene polymor-
phisms (genetic variation between individuals) have impor-
tant eﬀects on the constitutions of individuals.
One medical application using SNPs is the prediction of
individual susceptibilities to common complex diseases such
as diabetes and cardiac infarction. Disease susceptibilities
are known to be associated with clinical features (e.g., age,
sex, blood pressure, family disease history) and genetic fea-
tures represented by SNPs of individuals. The variant form
of the gene is described as allele. The allele whose traits
appear with low and high frequency are described as minor
allele and major allele, respectively. Presence (or absence)
of genetic variants at speci(cid:12)c loci is as input genetic futures.
Personalized medical care services based on common disease
susceptibilities are becoming an important part of preven-
tive medicine [1, 2, 3].
Another important medical application of SNPs is person-
alized drug administration. Drug sensitivity is known to be
aﬀected signi(cid:12)cantly by personal genomes [7, 11]. Physicians
are often forced to undertake trial and error processes to (cid:12)nd
an appropriate medical treatment that is most eﬀective for
their patient. Personalized drug administration helps physi-
cians to select an appropriate medicine that is eﬀective for
individual patients, and to adjust proper dosage amounts
considering the drug sensitivity of individual patients.
Utilization of personal genomes for medication is extremely
bene(cid:12)cial. However, personal genomes can include highly
private and sensitive information [13, 14]. Personal genetic
information can be used as an identi(cid:12)er that uniquely iden-
ti(cid:12)es individuals with very high probability [17].
It also
indicates sensitive properties of individuals, such as ethnic
descriptions, constitution, and susceptibilities to speci(cid:12)c dis-
eases, etc [7, 19]. Consequently, personal genomes must be
managed under careful control at every step of collection,
storage, and utilization.
In the pursuit of personalized medicine, statistical models
are often used. They take personal genomes as input and
give some susceptibility or eﬀect information as output. For
example, susceptibilities of common diseases are evaluated
using logistic regression models. In addition, for personal-
ized drug administration, the proper dosage is determined
using a linear regression model. As reported previously [7],
one can sometimes infer inputs (personal genomes) from out-
puts (susceptibilities or eﬀects) using probabilistic inference.
In an extreme case, if the function is a one-to-one mapping,
then personal genomes can be uniquely identi(cid:12)ed from out-
532(a) Release output without modi(cid:12)cation
(b) Release output as intervals
Figure 1: Input inference by advesaries.
puts. If such identi(cid:12)cation can be achieved with a high level
of con(cid:12)dence, then model outputs should be treated secretly
as sensitive information. It is, therefore, important to guar-
antee that personal genomes cannot be inferred from func-
tion outputs with a high-level con(cid:12)dence. Our study does
not necessarily assume the function is injective, but it is used
to consider the injective function as the worst case.
This study elucidates the problem of releasing outputs of
functions that take private information as input. If outputs
can be released with no modi(cid:12)cation, then the private in-
put might be identi(cid:12)ed using probabilistic inference (Fig. 1,
left). Our objective is to present a mechanism that modi(cid:12)es
outputs to be an interval so that the input cannot be inferred
with high con(cid:12)dence. Simultaneously, the output must be
exact to the greatest extent possible while satisfying privacy
constraints (Fig. 1, right).
1.1 Related Work
Attacks to personal genomes by probabilistic inference
have been studied extensively in diverse contexts. Ayday
et al. considered an inferential attack on SNPs that are in
linkage disequilibrium 1 [5]. Humbert et al. considered re-
constructing the genomes of the relatives of an individual
when the individual genome is observed as prior knowledge
[10]. Goodrich et al. exploited the master mind attack, by
which the adversary was allowed to issue similarity queries
repeatedly to infer the input genome string [8]. Inference of
presence (or absence) of any subject in GWAS from GWAS-
related statistics has been discussed in relation to statistical
testing [9, 18].
The following two studies assessed the problem of input
inference from function outputs, which are closely related
to our work. Fredrikson et al. presented a study of pri-
vacy preservation for personalized dosing of warfarin [7] .
The proper dosage (output) is determined using a linear
regression model that takes a few genetic markers and de-
mographic features (input). When an attacker can obtain
outputs, they pointed out that the attacker can recover pa-
tients’ genetic markers with a certain level of con(cid:12)dence. To
prevent such reconstruction, they used a regression model
with diﬀerential privacy. They empirically investigated the
tradeoﬀ between utility (mortality) and privacy (diﬀerential
privacy). The authors concluded from an experimental eval-
1Linkage disequilibrium is the association of frequency of ap-
pearance between SNPs. If patient’s some SNPs are leaked,
some other SNPs can be inferred using probabilistic infer-
ence with this association.
uation that diﬀerentially private mechanisms did not simul-
taneously improve genomic privacy while retaining desirable
clinical eﬃcacy for warfarin dosing.
Ayday et al. [6] conducted a study of privacy preservation
for disease susceptibility evaluation. The susceptibility pre-
dictor is built with a multiplicative model 2 that uses genetic
features as input. The authors empirically investigated the
risk of input inference from disease susceptibilities and in-
troduced an obfuscation method. Their method partitions
the output domain into evenly partitioned sections and out-
puts one section instead of the susceptibility value. Their
experimental evaluation reveals that application of their ob-
fuscation method improves the statistical privacy measure
(asymmetric entropy). Their obfuscation method improves
privacy in a statistical sense but does not necessarily pre-
vent unique identi(cid:12)cation of input attributes in some cases,
as we discuss with experimentally obtained results in later
sections.
Existing studies emphasize the risk that private input is
learned by releasing outputs, while proactive protection be-
fore releasing outputs was not mainly considered, except in
one study [7]. Their protection method employs diﬀeren-
tial privacy, which is recognized as a defacto-standard pri-
vacy de(cid:12)nition. Diﬀerential privacy is known to be resis-
tant against probabilistic inference by adversaries having
any prior distribution [12]; however, Fredrikson et al.
in-
dicate diﬀerential privacy can be problematic, particularly
when the output is used for medication. Mechanisms that
guarantee diﬀerential privacy fundamentally requires ran-
domization of outputs. This property of diﬀerentially pri-
vate mechanisms can severely damage the utility of outputs.
As discussed intensively by Fredrikson et al.
[7], when the
output is used for medical treatment, randomization of out-
puts is not a feasible option. For medical applications, we
want the function output to be deterministic so that physi-
cians can con(cid:12)rm by themselves that the medical treatment
following the output is medically safe.
1.2 Our Contribution
The contribution of this paper is two-fold. First, we em-
ployed (cid:11)-obscure privacy for output release. One drawback
of diﬀerential privacy is that mechanisms that guarantee
diﬀerential privacy fundamentally require randomization of
outputs, as we already mentioned. Our (cid:11)-obscure privacy
2Multiplicative model
is a disease susceptibility model,
which regards the susceptibility as the multiplicative accu-
mulation of the risk of genetic factors.
PublisherPrivateSNP dataAdversarySubject3. Publish susceptibility1. Send private SNP data and the model2. Evaluationof diseasesusceptibilityInference of0.8642...0.8642...Leakage about private inputtends to be large.0.8642...PublisherPrivateSNP dataAdversarySubject3. Publish susceptibility1. Send private SNP data and the model2. Evaluationof diseasesusceptibilityInference of[0.8, 0.9][0.8, 0.9]We want the leakageclose to zero.[0.8, 0.9]533assumes that prior knowledge is publicly known, and adver-
saries perform probabilistic inference following the known
prior distribution. In this sense, the adversaries that diﬀer-
ential privacy assumes are more powerful than those which
(cid:11)-obscure privacy assumes, which means that diﬀerential
privacy oﬀers more rigorous privacy. However, because of
this assumption, our output can be made deterministic. Our
mechanism preserves the privacy of outputs not by random-
ization but by sectionalization of outputs. Also, we can
guarantee that the output interval always includes the true
output and that the interval of the output can be determin-
istically con(cid:12)rmed by anyone. Consequently, this type of
mechanism is desirable for privacy preservation of medical
applications.
Second, we propose the optimal interval release mecha-
nism which satis(cid:12)es (cid:11)-obscure privacy (Section 6). In prin-
ciple, the mechanism that outputs the optimal (i.e., narrow-
est) interval with satisfying (cid:11)-obscure privacy is intractable
because we need to solve the optimization problem over the
in(cid:12)nite set without any convex structure behind it (Problem
A). To alleviate this diﬃculty, we use Theorem 1, which
allows us to convert the intractable problem (Problem A)
to a set partitioning problem (Problem B), which is solv-
able with dynamic programming. The time complexity of
our mechanism is O(jTj3d); where d is the input dimension
and jTj is the size of the range of target real-valued function.
Unfortunately, the computational complexity is exponential;
however, this cannot be problematic in many cases because
the input features of the target model are screened as a pre-
processing.
In addition, once our the optimal interval is
obtained by our mechanism, the interval can be repeatedly
used for any request without spending additional costs.
Our mechanism guarantees the following two conditions:
(cid:15) When the output is provided by the mechanism, the
increase of the adversary’s belief on any input attribute
((cid:11)-obscure privacy) is upper-bounded using a level pre-
scribed in the mechanism. Therefore, the mechanism
can control the probability with which any input at-
tribute is uniquely identi(cid:12)ed (privacy guarantee),
(cid:15) Under the privacy constraint presented above, the mech-
anism outputs are the narrowest (optimal) interval in-
cluding the true output (optimality guarantee).
2. PROBLEM FORMULATION
We (cid:12)rst de(cid:12)ne the input inference problem as shown be-
low.
2.1 Output Release
Let X = X1 (cid:2) (cid:1)(cid:1)(cid:1) (cid:2) Xd be a (cid:12)nite input domain. Y is
an output domain in which Y is a (cid:12)nite discrete set. The
function we consider is de(cid:12)ned as f 2 F where f : X ! Y.
Given an input x 2 X, the output y 2 Y is given as y = f (x).
We describe the i-th attribute value of x as xi 2 Xi and
describe the random variables of x, xi and y respectively as
X, Xi and Y .
Two stakeholders appear in the input inference problem:
publisher and adversary (Fig.1). The publisher holds a sen-
sitive private input x 2 X, evaluates y = f (x), and then
publishes the output y to the adversary.
The adversary tries to identify the private input using the
output obtained from the publisher using probabilistic infer-
ence. We assume the adversary has the complete knowledge
of the function f and the prior distribution Pr [X].
2.2 Input Inference by Adversaries
To learn the private input, the adversary estimates the
posterior distribution (conditional distribution of input)
Pr [XijY = y]. In some situations, the adversary might be
interested in identifying a speci(cid:12)c attribute of private inputs,
but we assume that the adversary tries to infer every input
attribute.
It is noteworthy that inputs are always uniquely deter-
mined when f is injective. In other words, if f is injective,
the posterior probability Pr [Xi = ajY = y] is 1 for some
a.
If f is not injec-
tive, the adversary tries to evaluate the posterior probability
Pr [Xi = ajY = y] to infer x.
In practice, f is rarely injective.
We introduce the probabilistic inference of the posterior
distribution used by the adversary. Presuming that the ad-
versary holds the prior Pr [X] and obtains y from the pub-
lisher, then the adversary tries to infer the input by esti-
mating the posterior Pr [Xi = ajY = y]. The posterior can
be rearranged as
Pr [Xi = ajY = y] =
(1)
Given function f : X ! Y, the preimage of Y (cid:18) Y is
Pr [Y = y]
:
Pr [Xi = a; Y = y]
de(cid:12)end by
(cid:0)1 [Y] = fx 2 Xjf (x) 2 Yg :
f
Given the prior Pr [X = x], the denominator in Eq. 1 is
evaluated as
Pr [Y = y] =
=
∑
∑
∑