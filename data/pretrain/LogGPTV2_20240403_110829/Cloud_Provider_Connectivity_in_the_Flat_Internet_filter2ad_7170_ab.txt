Augmenting the AS-level topology graph with traceroutes
from the cloud. Publicly available BGP feeds have good coverage
of Tier-1 and Tier-2 ISPs [77]. However, they have limited visibility
into edge network interconnectivity; peering connections at the
edge are only visible to the two directly connected networks and are
invisible to BGP monitors further up the Internet‚Äôs hierarchy [25,
77, 115]. Even though the CAIDA dataset incorporates traceroute
data, the vantage points are not embedded within any of the cloud
provider networks [17]; unless the VP is inside the cloud provider
network, it will miss a considerable number of the cloud providers‚Äô
peers [26, 32, 77]. Since BGP feeds fail to capture most edge network
peering sessions, we augment our topology model.
To provide a more complete picture of cloud providers‚Äô connec-
tivity we issue traceroutes originating inside the cloud provider
networks. Traceroutes also suffer bias and can deliver distorted
router-level paths. The logical location of VPs or the number of
locations limits visibility into available paths [32]. Additionally,
dropped packets, unresponsive hops, or load balancing can result
in inferring false links [62, 65, 108]. We only consider the directly
connected neighbors of the cloud providers from our traceroutes;
we do not augment the AS-level topology graph with information
gleaned beyond the cloud provider‚Äôs neighbors.
We add p2p links into the topology graph between the cloud AS
and neighbor ASes identified in the traceroute measurements. Since
BGP feeds have a high success rate identifying c2p links [53, 64, 77]
but miss nearly all edge peer links [77], we can safely assume newly
identified links are peer links. When a connection identified in a
traceroute already exists in the CAIDA dataset, we do not modify
the previously identified link type.
Issuing and analyzing traceroutes. We create VMs in multiple
locations per cloud provider (12 for Google, 20 for Amazon, 11 for
Microsoft, and 6 for IBM). From each VM we issue ICMP traceroutes,
using Scamper [60], to every routable IPv4 prefix. We restrict our
measurements at each VM to 1000 pps to avoid rate limiting. We
also conduct smaller sets of supplemental traceroutes to look for
path changes by selecting one prefix originated by each AS [19].
To sanitize our traceroute dataset and identify neighbors we
iteratively map IP addresses to ASes, first using PeeringDB for any
IP addresses where it contains a mapping, then the Team Cymru IP-
to-ASN mapping tool [99], and finally whois to identify the owning
AS. We use PeeringDB as our primary source because we focus on
identifying peerings, and some peering routers use addresses that
are not globally routable but may be in PeeringDB as they are useful
to configure peerings. For example, we saw several unresolved IP
addresses in the 193.238.116.0/22 range, which is not announced
in BGP but belongs to NL-IX and is used by IXP members [82].
We will demonstrate in Section 5 that this supplement makes a
significant difference. We only retain traceroutes that include a
cloud provider hop immediately adjacent to a hop mapped to a
different AS, with no intervening unresponsive or unmapped hops.
By combining our traceroute-inferred cloud peers with those
visible in the CAIDA dataset, we observe many more peers than are
seen in the CAIDA dataset alone: 333 vs. 1,389 peers for Amazon,
818 vs. 7,757 for Google, 3,027 vs. 3,702 for IBM, and 315 vs. 3,580 for
Microsoft. BGP feeds do not see 90% of Google (open peering policy)
and Microsoft (selective peering policy) peers. Both Amazon and
IMC ‚Äô20, October 27‚Äì29, 2020, Virtual Event, USA
T. Arnold et al.
IBM have selective peering policies, and CAIDA alone identified a
higher fraction of Amazon and IBM‚Äôs neighbors, but did not detect
76% and 19%, respectively.
2015 datasets. As a comparison point, we also use data from prior
work in 2015 (¬ß6.5). For our reachability retrospective, we need both
an AS-relationship dataset and a comprehensive traceroute dataset
outbound from the cloud providers for the same time period. We
used the CAIDA AS-relationship dataset from September 2015 [14]
and the traceroute dataset from a prior study which also issued
traceroutes outbound from cloud provider VMs to the rest of the
Internet [26]. We used the same methods we applied to our data
to combine the past CAIDA and traceroute datasets to create a
historical AS-level topology map.
4.2 PoP Topology Maps
To understand the similarities between the cloud and large
providers‚Äô PoP deployment strategies, we construct city-level
topology graphs. We use network maps provided by individual
ASes when available [1, 9, 31, 48, 75, 79, 80, 92, 94, 98, 102, 104, 106,
107, 110, 112, 123]. Prior work on topology maps posited that since
networks are trying to provide information and sell their networks,
the maps would only be published if accurate [30, 55]. We also
incorporate router locations from looking glass websites if avail-
able [47, 50, 51, 58, 74, 78, 93, 97, 100, 101, 103, 105, 109, 111, 122].
We combine the maps with two additional information sources.
The first source is to incorporate data from PeeringDB [81]. Data
from PeeringDB is generally reliable, as shown in prior works [59],
and is considered trustworthy and authoritative by many opera-
tional networks, including some of the Internet‚Äôs largest [12].
The second source is router hostnames, since the hostnames
often encode location information hints such as airport code or
other abbreviations [63]. To gather hostnames, we issued reverse
DNS (rDNS) requests for every IP address announced by the cloud
providers and Tier-1 and Tier-2 ISPs. To identify which hostnames
belong to routers, we first manually issued traceroutes to random
IP addresses within each AS to find candidate hostnames. When an
AS has rDNS entries, we found that router hostnames were very
clearly distinguished from non-routers (e.g., NTT router hostnames
belong to gin.ntt.net). We used two methods to extract location
information from router hostnames. The first was to manually
inspect each set of hostnames and generate a regular expression
to extract locations. The second method was to resolve aliased
IP addresses/hostnames using midar [18, 54], then use the aliases
as input for sc_hoiho to generate a naming convention regular
expression [61, 63]. We had identical results for the two methods,
except several ASes produced no results from sc_hoiho due to a
low number of alias groups.
The two rDNS methods did not uncover any PoPs that did not
appear in PeeringDB or the available maps, lending support to their
completeness. We see that there are varying degrees of publicly
available data. AT&T for instance provides a map and rDNS data,
but no entries in PeeringDB. Also notable, Amazon has a network
graph and an active presence in PeeringDB, but it does not have
any router hostnames in rDNS. For networks that did respond to
rDNS entries, only 73% of PoPs had rDNS entries. Detailed PoP and
rDNS entry numbers are available in Table 3, in Appendix C.
4.3 Population and AS Type
One of the main reasons for Internet flattening is to improve per-
formance of users accessing hosted services [26], so we want to
assess how cloud provider reachability relates to user populations.
We use data provided by the Asia-Pacific Network Information
Centre (APNIC), which uses ad-based measurements to estimate
the number and percentage of Internet users in an AS [2]. CAIDA
classifies AS into three types [16]: content, transit/access, or enter-
prise. If CAIDA identifies an AS as transit/access and the AS has
users in the APNIC dataset, we classify it as access.
Second, we examine the cloud providers‚Äô geographic PoP de-
ployment strategy relative to user populations. We use the latest
population density data [34], which provides per square kilometer
population counts, and topology graphs (¬ß4.2), to estimate popula-
tion within different radii of network PoPs.
4.4 Limitations
Many virtual cloud interconnects are invisible to the Inter-
net hierarchy. Discovering an interconnect between networks
requires having a VP that uses a route that traverses the link. Be-
cause p2p links are not announced to providers, observing them
requires having a VP in one of the peers or one of their customer
cones. Cloud providers generally do not have customers, and they
peer with many ASes at the edge which lack any or many customers,
and so these links tend to be invisible to traditional mapping efforts,
which lack VPs in cloud providers and most edge ASes.
By using a VP in the cloud, we observe the cloud provider‚Äôs links
that are made available to public cloud tenants. However, it is also
possible for a cloud provider peer to connect to the cloud but NOT
make it available to public cloud tenants, instead only using the
interconnect to access private cloud resources and/or the cloud
provider‚Äôs hosted service (e.g., Office 365) [67, 119]. In particular,
Virtual Private Interconnections (VPIs) [10, 27, 66, 119], are often
configured to be private to let cloud customers, even those without
their own AS, have direct access to their own cloud resources (but
not to other VMs hosted in the cloud) without traversing the public
Internet. VPI can be facilitated by cloud exchanges, which allow any
customer to form virtual peering sessions with any participating
cloud provider at any of the cloud exchange‚Äôs facilities [119]. These
private VPIs will be invisible to our measurements.
Recent studies discussed VPIs and noted that existing methods
are not capable of identifying these types of connections [118, 119].
VPIs represent a complementary aspect of cloud provider networks
that do not commonly carry traffic from general publicly facing
cloud hosted services, so we view them as out of scope and they
would not be a target of our measurements.
Cloud providers can interfere with measurements, manipulat-
ing header contents, rate limiting measurement traffic, or denying
traffic based on protocol or content. For example, Google is known
to drop packets with IP Options enabled [42] or to manipulate TTL
values for different networking services such as Virtual Private
Clouds (VPCs) [45]. Cloud providers also tunnel traffic between
nodes [85, 117], which can make it difficult or nearly impossible to
know the physical path between their internal hops.
Filtering or manipulation by the cloud providers can render tradi-
tional measurement techniques impotent. For example, prior work
Cloud Provider Connectivity in the Flat Internet
IMC ‚Äô20, October 27‚Äì29, 2020, Virtual Event, USA
could not issue traceroutes from Microsoft [26] or from Google‚Äôs
standard networking tier [8]. In our paper, the key property we infer
from traceroutes is the set of ASes neighboring the cloud providers.
In Section 5, we describe validation provided by Microsoft and
Google demonstrating that 85-89% of neighbors we infer are cor-
rect, suggesting that manipulation had little to no impact on our
key results. Close inspection of geographic and other patterns in
our traceroutes also suggests we are correctly inferring borders.
Underestimating reachability of other networks which are
not our measured cloud providers, Tier-1 ISPs, or Tier-2 ISPs. By
combining neighbors identified in traceroutes with the data from
BGP data feeds, we are able to get a more complete view of the
cloud providers‚Äô connectivity. However, since BGP feeds are esti-
mated to miss up to 90% of edge network peer connections [25, 77],
it is likely that we underestimate the interconnectivity for other
networks (e.g., Facebook or Apple). Comprehensively identifying
the number of peers for other non-cloud networks is challenging
due to measurement budgets and a lack of vantage points, so an
efficient method to uncover other edge networks‚Äô neighbors is an
area for future research.
5 VALIDATION OF NEIGHBOR INFERENCES
Validation of final results. Most of our paper‚Äôs results depend
on the set of peers we infer for the cloud providers. To validate this
data, we requested validation from each cloud provider. Microsoft
and Google provided feedback regarding true and false positives ‚Äì
whether ASes we identified as neighbors are actual neighbors or
not‚Äìas well as false and true negatives‚Äìwhether ASes we identi-
fied as not neighbors are actual neighbors or not. For both cloud
providers, the feedback indicated that the false discovery rate (FDR),
ùêπ ùëÉ/(ùêπ ùëÉ + ùëá ùëÉ), was between 11‚Äì15%. For the false negative rate
(FNR), ùêπ ùëÅ/(ùêπ ùëÅ + ùëá ùëÉ), the rate was higher, although Google could
not provide an exact rate; Microsoft data showed 21%, meaning
the cloud providers have even more neighbors which we did not
see in our measurements. Any approach that, like ours, relies on
traceroutes from the cloud, has inherent limitations. The cloud
providers have many more PoPs than cloud datacenters to measure
from, and so cloud-based measurements cannot necessarily uncover
every path used from every PoPs. Microsoft verified that a number
of peers that we missed‚Äìfalse negatives‚Äìonly provide routes to a
single PoP, far from cloud datacenters, so those paths are unlikely
to ever be used from available VPs.
Additional information from Microsoft helps put these results in
perspective. Microsoft has an order of magnitude more peers from
IXP route servers than direct peers (bilateral IXP peers and PNI
peers), and most peers missed by our measurements are route server
peers, potentially including ‚Äúinvisible‚Äù links which are challenging
to identify (¬ß4.4). However, from the cloud providers‚Äô perspective,
not all peers are equal. The huge number of route server peers in
total handle only 18% of Microsoft‚Äôs global Internet traffic, while
the much smaller number of direct peers handle 82%. Like other
networks, Microsoft prefers direct over router server peers for
capacity and reliability reasons [88, 89]. Microsoft confirmed that
we identified direct peers that account for 93% of Microsoft‚Äôs direct
peer traffic (i.e., 93% √ó 82% = 76% of global traffic). The route server
peers that we identified account for some additional portion of
global traffic from the 18% sent to such peers, but Microsoft was
unable to provide per-peer traffic volumes for route server peers.
This validation suggests that we identify the vast majority of
cloud provider peers and, in particular, of the important peers by
traffic volume, while introducing a modest amount of noise in terms
of false positives. Since our key metrics rely on and demonstrate that
the cloud providers are very well connected to many peers, the fact
that we identify most important peers, only have a small fraction of
extra peers (false positives), and have fewer extra peers than missed
peers (false negatives) suggests that our conclusions on overall
connectivity likely fall somewhere between a slight overestimate
and (more likely) a slight underestimate.
Iterative improvement to reach final results. The cloud
providers provided feedback to us over multiple iterations, which
helped to improve our accuracy. Our initial neighbor sets for each
cloud provider had FDR of ‚àº50% and FNR of 23‚Äì50%. Microsoft
provided us with a few example destinations that were causing
false inferences. We used the feedback to refine our methodology
in ways we detail in the rest of this section, eventually reaching the
final methodology and final numbers we use in the rest of the paper
(reflecting 11% FDR and and 21% FNR). We used the improved
methodology to update our results for other cloud providers, and
Google indicated that the updated results improved our accuracy
to 15% FDR and substantially fewer false negatives, essentially
serving as an independent validation of the improvements.
We investigated causes for inaccuracy and discovered an incor-
rect assumption. We initially assumed that, in identifying neighbors
from traceroutes, a single unknown or unresponsive hop between
the last cloud provider hop and the first resolved non-cloud provider
hop was unlikely to be an intermediate AS, so we inferred a direct
connection between the cloud provider and the first resolved hop‚Äôs
AS. This proved to be the leading cause for inaccuracy. When the
hop is unresponsive, we now simply discard the traceroute.
In our measurements, the far more common scenario is that the
intermediate hop responded, but its IP address was not resolvable
using the Team Cymru IP-to-ASN mapping tool. Manually inspect-
ing individual examples revealed that unresolved IP addresses were
registered in whois and frequently belonged to IXPs but were not
advertised globally into BGP. To resolve these hops to ASes, we
now use PeeringDB (when an AS lists the IP address) or whois
information. Adding these steps decreased our FDR for Microsoft
to 8% and our FNR to 34%.
To identify additional location-specific neighbors, we added VMs
in additional locations beyond our initial set, reaching the numbers
reported in Section 4.1. Measurements from the additional locations
reduced our FNR to 24%, but increased our Microsoft FDR to 16%.
Our final step was to prefer the PeeringDB IXP IP address dataset
for AS resolution over Team Cymru‚Äôs. Manually inspecting false