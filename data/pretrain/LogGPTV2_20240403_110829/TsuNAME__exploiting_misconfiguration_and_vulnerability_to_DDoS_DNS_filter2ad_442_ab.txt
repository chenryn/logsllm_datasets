(a) NS queries for DomainA.nz
(b) A queries for ns1.DomainA.nz
Figure 3: Google (AS15169) resolvers on 2020-02-06, during
.nz TsuNAME event: time in between queries.
start with the NS queries to DomainA.nz. Figure 3a shows individual
resolvers on the x axis, and the number of queries they sent on the
right y axis. We see that all resolvers send fewer than 10k queries.
On the left y axis, we show the interval inter-quartile range (IQR) of
the time between queries (with the white line showing the median
value in ms). Given that the TTL value of these records is 86400 s
(1 day), we should not see (assuming low packet loss) any resolver
sending more than one query on this date (except for multi-cache or
anycast-based resolvers [34, 44]). (While both domains in Figure 3
show some recursive resolvers query frequently, we expect the
recursives see different offered load from clients, and we sort them
differently, so the curves differ in detail.)
As shown in Table 1, the bulk of queries is for A and AAAA
records of the authoritative servers of DomainA and DomainB . Fig-
ure 3b shows the results for A records of ns1.DomainA.nz. We see
three categories of resolvers, according to their query volume,
which we highlight in different colors. The first group ś heavy
401
Zones
NS
TTL
sub.verfwinkel.net
sub.cachetest.net
ns.sub.cachetest.net
ns.sub.verfwinkel.net
1s
1s
Table 2: New domain experiment setup.
hammers ś sent 162-186k queries on this day, one every 300 ms.
The second group ś moderate hammers ś sent 75-95k daily queries,
one every 590 ms ś roughly double the rate of the heavy hammers.
The last group, which covers most of the addresses ś is less ag-
gressive: they sent up to 10k daily queries each. Given they are
more numerous than the other group, their aggregated contribution
matters. (Appendix A shows the results for AAAA records, which
are similar to Figure 3b).
This heterogeneity in Google’s resolver behavior is surprising.
We notified Google and were able to work with them on the issue,
and they both confirmed and fixed their Public DNS service on
2020-02-05. We discuss this in detail in ğ4.5.
4 REPRODUCING TSUNAME
To understand TsuNAME we next recreate the problem on the Inter-
net through controlled experiments (ğ4.1), the role of clients (ğ4.2),
multiple-step cycles (ğ4.3), and recursives (ğ4.4 and ğ4.5).
4.1 Controlled Experiments on a New Domain
To determine the lower bound on traffic to authoritative servers
experience during a TsuNAME event, we carry out a controlled
experiment from RIPE Atlas [47] to a new domain under our control.
Since this is a new domain, we know there is no caching or prior
query history.
Setup: We configure two third-level domains with cyclic depen-
dencies (Table 2). We use third-level domains since TsuNAME traffic
goes to the parent of the cyclic domains. A third-level domain al-
lows us to isolate traffic in our (second-level-domain) authoritative
servers, protecting top-level domains that are widely shared. If we
create a cycle in a second-level domain like example.org, then traffic
goes to the widely used .org authoritative servers.
We ran our own authoritative servers using BIND9 [22], popular
open-source software for authoritative DNS service. We ran on
Linux VMs located in AWS EC2 in Frankfurt, Germany.
To minimize caching effects, we set the TTL for every record in
the zone to 1 s (Table 2). Short TTLs maximize the chances of cache
misses and avoid hiding misbehavior with caches.
Vantage points (VPs): we use ∼10k RIPE Atlas probes [47, 48]
as VPs. RIPE Atlas provides more than 11k active devices (probes
or VMs), distributed over 6740 global ASes (as of Jan. 2021). Atlas
supports custom queries of standard types (ICMP, DNS, HTTP,
etc.), and they archive and provide public access to research results,
including our experiments [46].
We configure each probe to query once for an A record for
PID.sub.verfwinkel.net., where PID is the probe unique identi-
fier [49]. Unique queries reduced the risk of accidentally warming
up the resolver’s caches with queries from other VPs. The query
is sent to each probe’s local resolver, as can be seen in Figure 4.
As one probe may have multiple resolvers, we identify a VP using
 1 10 100 1000 10000 100000 1x106 1x107 1x108 0 200 400 600 800 1000 1200 1400 0 50000 100000 150000 200000ms (log)QueriesResolver (sorted by queries) 1 10 100 1000 10000 100000 1x106 1x107 1x108 0 200 400 600 800 1000 1200 1400 0 50000 100000 150000 200000ms (log)QueriesResolver (sorted by queries)TsuNAME
IMC ’21, November 2–4, 2021, Virtual Event, USA
AT1
...
ATn
Rna
...
Rnn
C Rna
C Rnb
Authoritative
Servers
sub.verfwinkel.net
Recursives
(nth level)
e.g: ISP resolv.
C R1a
R1a
R1b
C R1b
Atlas
Recursives/forwarders
(1st level
e.g.: modem)
Ripe Atlas Probes
Figure 4: Relationship between Atlas probes (yellow), recur-
sive resolvers (red) with their caches (blue), and authorita-
tive servers (green).
the combination of a probe’s unique ID and the last resolver’s IP
address.
Results: Table 3 shows the results for this measurement (łnew do-
mainž column). We see ∼9.7k Atlas probes placing queries through
16.8k recursive resolvers (the vantage points). With a few additional
forwarders, they send 18,715 queries to their first level recursives
(archived at Ripe Atlas and accessible at [46]), which are mostly
answered to the probes with SERVFAIL status codes [27] or simply
timing out ś both statuses signal domain name resolution issues to
the client.
Heavy traffic growth on the authoritative server side: Authoritative
servers see queries from ∼11k IPs addresses belonging to nt h -level
servers in ∼2.6k ASes (traffic into top green circles in Figure 4). As
each Atlas probe query its recursive resolvers some forward their
queries or ask multi-level resolvers [34, 44, 53]. Our authoritative
servers see only the final resolver in the chain. In total, these re-
solvers send ∼ 8M queries to both authoritative servers, a 435×
amplification factor compared to the 18k queries at the client-side.
We believe this amplification contributes to the .nz event.
Identifying problematic resolvers. Figure 5 shows the timeseries
of both queries and resolvers we observe at our authoritative servers
(each line shows a different authoritative server, one per domain).
We identify three phases in this measurement. First warm-up (the
narrow, green shaded area, x < 14:30 UTC) when the VPs send
the queries we have configured. We see more than 150k queries
(Figure 5a) to each authoritative server, from roughly 7.5k resolvers
(Figure 5b). In this initial rush each recursive repeats queries trying
to resolve the cycle. Many have a query limit and break out after
this rush.
After this initial rush, there are no new client queries, and at
14:30 we enter the looping phase, when authoritative servers keep
on receiving queries from some problematic resolvers that do not
have a query limit (the salmon-colored area, łResolvers in Loopž).
For the next hour a few resolvers (574 from 37 ASes) seem to loop
indefinitely (Table 4).
Finally, in the offline phase, after 19:30 UTC, we stop our au-
thoritative servers, but keep observing incoming queries. As our
servers become unresponsive, even problematic resolvers cannot
obtain answers and stop looping. Without our manual intervention
(a) Queries
(b) Resolvers
Figure 5: New domain measurement: queries and unique re-
solvers timeseries (5min bins)
Figure 6: New domain: queries per AS with problematic re-
solvers
(stopping our servers), one may wonder when (or if!) these loops
would stop. We show in ğ5.2 that these loops may last for weeks.
Other ASes also affected: Figure 6 shows the histogram of queries
per source ASes. We see than the AS with most traffic (Google,
15169) is responsible for 60% of the queries. Google here has a far
more modest fraction than on the .nz event (ğ3). We see looping
traffic from other ASes as well, such as AS200050 (ITSvision) and
AS30844 (Liquid Telecom). In fact, we found in this experiment that
37 ASes were vulnerable to TsuNAME (Table 4). Although posing
a much lower query volume than Google, old resolver software
represents an ongoing risk.
402
 0 50 100 150 20014:0014:3015:0015:3016:0016:3017:0017:3018:0018:3019:0019:3020:00OﬄineResolvers in LoopQueries (k)Time (UTC) -- 2020-06-08cachetest.netverfwinkel.net 0 1000 2000 3000 4000 5000 6000 7000 800014:0014:3015:0015:3016:0016:3017:0017:3018:0018:3019:0019:3020:00OﬄineResolvers in LoopUnique ResolversTime (UTC) -- 2020-06-08cachetest.netverfwinkel.net 0 1 2 3 4 51516920005030844152672108846242553RestMillion Queries 0 1 2 3 4 51516920005030844152672108846242553RestIMC ’21, November 2–4, 2021, Virtual Event, USA
G. C. M. Moura et al.
Measurement
Frequency
Qname
Query Type
Date
Duration
Atlas Probes
VPs
Queries
Responses
SERVFAIL
Timeout
REFUSED
FORMERR
NOERROR
NXDOMAIN
NO ANSWER
New Domain
once
Atlas VPs
Recurrent
1800s
$PID.sub.verfwinkel.net.
$PID-R.vuur.verfwinkel.net.
A
2020-06-08
6h
9724
16892
18715
18715
12585
5969
103
28
22
8
A
2020-06-[09,10]
17h
Client Side
9382
13030
727778
727778
482470
238864
4240
1084
95
162
TripleDep
Once
CNAME
Once
$PID-R.jupiter.
minuano.
essedarius.net.
A
2021-04-13
3h
essedarius.net.
A
2021-04-13
3h
9622
13504
13579
13579
13437
0
114
0
15
0
9646
17638
17736
17736
8841
0
97
13
8791
7
11507
Sinkholed
NA
NA
Multiple
Multiple
2020-11-18
22h
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
Querying IPs
ASes
Queries
Responses
ns1
11195
2587
4064870
4064801
ns2
11572
2611
4080446
4070035
Authoritative Server Side
ns1
16127
2408
35546101
35546090
ns2
16328
2446
36917334
36917300
4 combined
17272
2554
5349129
5349129
4 combined
10730
2412
156356
156356
4 combined
41433
3615
110282443
110282443
Table 3: TsuNAME Emulation Experiments. Datasets: [46].
Queries Resolvers ASes
New domain
Recurrent
Sinkhole
Unique
7.5M
30.6M
18.1M
56.2M
574
1423
2652
3696
37
192
127
261
Table 4: Problematic Resolvers found on experiments
How often do the problematic resolvers loop? For each problematic
resolver, we compute the interval between queries for the query
name and query type, for each authoritative server, as in ğ3.2. Fig-
ure 7 shows the top 50 resolvers that send queries to one of the
authoritative servers for A records of ns.sub.cachetest.net. We see
a large variation in behavior. The resolver with most queries (at
x = 1) sends a query every 13 ms, with 858k queries during the
looping phase. Resolvers ranked 7 to 19 loop every second. Finally,
resolvers ranked 20 to 50 all belong to Google and behave simi-
larly, sending queries with a median interarrival of 2.9 s. Taken