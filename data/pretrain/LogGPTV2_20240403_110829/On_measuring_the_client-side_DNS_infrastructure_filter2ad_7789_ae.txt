≥ 52%
53%
69%
53%
52%
72%
59%
59%
54%
Table 8: RDNSdi TTL Deviations
Percentage of Measurements
Expected (sec) % 
Mode Lie
Behavior
Honest
Lie on Initial
Lie on Subsequent
Constant TTL
Incrementing TTL
36%
55%
5%
5%
0%
Table 9: RDNSi TTL Behavior
initial response in §7.1 and 88% of FDNSes are honest on the initial
response in §7.2. Second, we can utilize more than two FDNSes in
coordinated probing to mitigate the effect of FDNS lies. Instead of
F1 and F2 representing single FDNSes in the above description of
the experiment, we utilize up to 10 FDNSes that we divide into two
sets. We send the same request through each FDNS at roughly the
same time.
If any FDNS responds with the correct TTL value, then we con-
clude the RDNSi must be truthful since every component was truth-
ful in this case. On the other hand, if no FDNS responds with the
correct TTL, then it is probable that the RDNSi is responsible for
the lie. In this situation, there are three scenarios. First, some of
the FDNSes are in the set of FDNSes that were honest in initial re-
sponses and the TTL values from these FDNSes all agree. In this
ideal case, their responses collectively identify the actual TTL the
RDNSi provides. In the second scenario, the TTL values arriving
at honest FDNSes do not agree. One potential cause of this case is
HDNSes interposing between some of the FDNSes and the RDNSi.
In this case, we assume that the RDNSi returns the most common
TTL value among the honest FDNSes. In the ﬁnal scenario, none
of the FDNSes are honest. In this case, we assume that the RDNSi
returns the most common TTL value among all the FDNSes.
If the majority of FDNSes access an RDNSi through the same
HDNS, then our experiment will conﬂate the behavior of the
RDNSi and the HDNS. However, we note that if an RDNSi is only
accessible through a single HDNS, then learning the RDNSi’s be-
havior in isolation is moot because only the aggregate behavior of
both components will ever impact client devices in the real system.
We validate our technique for determining RDNSi TTL behavior
using RDNSdies, which allow us to obtain ground truth by direct
probing. The results using our coordinated probing technique agree
with the ground truth in 98% of the cases, and not only in detecting
whether an RDNSi is honest but also in determining the quantita-
tive TTL violations (as we will show, most lies are from a small
ﬁxed set of TTLs).
In our dataset, there are 46K RDNSies and we conduct in-depth
probing for 22K of them (due to logistical issues, as sketched
above). Table 9 shows our ﬁndings. We determine that 36% of
RDNSies are honest. Further, 55% of RDNSi lie on the initial
response to F1, but only 5% of RDNSies lie in response to sub-
sequent requests from F2 indicating that the behavior of caching a
different TTL than initially returned is less prevalent in RDNSies
than FDNSes. This supports our conjecture that FDNSes, be-
ing mostly home-based devices, represent more primitive imple-
mentations of DNS. In addition to incorrect TTLs, we ﬁnd 8%
of RDNSies return constant TTL values without decrementing.
The TTL deviations from RDNSies, merged with the results for
RDNSdies are shown in Table 10.
We now consider how long a record remains cached and acces-
sible at RDNSies. Again, we use the experimental setup described
earlier in this section with one exception. Instead of querying from
F2 immediately after receiving the response from F1, we wait be-
fore repeating the query (using the same intervals as in §7.1 up to
1-120
1000
3600
10000
10800
86400
100000
604800
1000000
≤ 1% ≤ 1%
1%
1%
2%
2%
5%
11%
11%
49%
0%
0%
0%
0%
0%
0%
0%
0%
Value
300
900
80
3600
7200
21600
86400
86400
604800
% of All Lies
≥ 34%
29%
19%
35%
20%
32%
55%
53%
71%
Table 10: RDNSi TTL Deviations
F
D
C
C
1
0.8
0.6
0.4
0.2
0
1
All 1M Sec
Accessible 1M Sec
All 30 Sec
Accessible 30 Sec
10
100K
Record lifetime per RDNS cache (seconds)
10K
100
1K
1M
Figure 14: Distribution of record availability in RDNSi caches
for records with TTLs of 30 and 1 million seconds.
the TTL value6). For each RDNSi, we track the latest time at which
the record is available.
Again, when FDNSes become unavailable in the course of the
experiment we cannot detect how much longer a record stays in
the RDNS’s cache. Thus, Figure 14 shows the duration of record
retention separately for all measured RDNSes (the “All” lines, rep-
resenting 22K tested RDNSies and underestimating the result) and
for those RDNSes that remained accessible throughout the experi-
ment (the “Accessible” lines, representing 8.8K and 2.4K RDNSies
for the 30 second TTL and the 1 million second TTL, respectively).
Concentrating on the “Accessible” lines as more reliable, we
show that DNS records with a TTL of 1 million seconds stay in the
cache for a long time, with the records still present 10K seconds
(2.8 hours) after being inserted in 90% of the cases. Furthermore,
step-wise drops indicate record evictions at ﬁxed values of time in
cache, indicating some conﬁguration parameters rather than capac-
ity eviction, and a more gradually descending line in the aggregate
behavior (Figure 12) is likely due to FDNS affects. The record with
a TTL of 30 seconds remains in the cache for the full TTL in 94%
of the tested RDNSies.
8. DATASET REPRESENTATIVENESS
Finally, we return to the issue of bias in our datasets ﬁrst men-
tioned in §4. Since our scans do not encompass the entire Internet,
it is possible that our results are not demonstrative of the entire
population of FDNSes and RDNSies due to biases in our scanning
methodology. In particular, our results on FDNS behavior encom-
pass a subset of FDNSes, i.e., those which allow cache injection.
Similarly, our results for RDNSies include only those RDNSies
6Unfortunately, we neglected to measure intervals beyond TTL and
hence do not check if RDNSes cache records beyond the authorita-
tive TTL as we did for FDNSes.
0.7
0.6
0.5
0.4
0.3
0.2
s
r
o
t
c
A
f
o
n
o
i
t
c
a
r
F
Aggregate
FDNSes
RDNS
es
i
0.1
1
2
3
4
Chronological Snapshots
5
6
7
8
9
10
Figure 15: Fraction of honest actors over the discovery process.
for which we discover at least two FDNSes. For these two, we
demonstrate here that our dataset is representative of the respective
subsets of the whole population. On the other hand, we can validate
the aggregate behavior against the whole population.
We assess representativeness of our datasets by calculating the
fraction of actors which honestly report the TTL value for all TTL
values we utilize. We divide our datasets into ten slices ordered by
the time of discovery. For the aggregate behavior and FDNS be-
havior, the 10 slices each include an identical number of measured
FDNSes while for the RDNSi behavior, the 10 slices each include
an identical number of measured RDNSies. We then calculate a
cumulative snapshot of the fraction of honest actors found in the
ﬁrst n slices for 1–10 slices. The cumulative rate should ﬂatten out
if the dataset is typical of the broader population.
Figure 15 shows the results. The fraction of honest actors in the
aggregate data remains constant throughout the 10 snapshots, in-
dicating that we quickly discover a representative sample in this
study. In particular, these results indicate that the “Scan on First
Hit” method of discovery used in this study and which has a bias
potential, does not bias this particular metric. The fraction of hon-
est RDNSies decreases over time but appears to converge to a
constant value by the 7th snapshot. This shows that (1) honest
RDNSies are discovered at a higher rate towards the beginning of
the scan and (2) we discover a sufﬁcient number of RDNSies to
be representative of the general population. Finally, the fraction of
honest FDNSes increases throughout the 10 snapshots though the
growth is ﬂattening. This result indicates that our dataset is not suf-
ﬁcient to capture a representative set of FDNSes that allow cache
injection. The fraction of honest FDNSes in the true population is
likely higher than what we report in this paper.
A larger question of representativeness is whether open DNS
resolvers are representative of the overall population of DNS re-
solvers users employ. In other words, do the behaviors we ﬁnd in
ODNSes more broadly apply to ﬁrst hop resolvers in general? Our
methodology does not afford any way to directly assess this ques-
tion given that we would have to do so from inside many edge net-
works to gain an understanding of resolver behavior for ﬁrst hop re-
solvers that are not arbitrarily accessible. This problem does not ap-
ply to our RDNS results because given the window that the myriad
ODNSes provide we are readily able to get “inside” the RDNSes’
networks and assess their behavior.
9. CONCLUSION
In this paper, we present a set of methodologies for efﬁciently
discovering the client-side DNS infrastructure and, once discov-
ered, teasing apart the behavior of the actors within the system in-
cluding components that cannot be directly probed. Using these
methodologies, we assess various aspects of the client-side DNS in-
frastructure and its behavior with respect to caching, both in aggre-
gate and separately for different actors. In terms of the infrastruc-
ture, we double previous estimates of the number of open resolvers
on the Internet, ﬁnd evidence of wide use of shared resolver pools,
and observe signiﬁcant distances DNS messages travel within the
infrastructure. In terms of caching behavior, we show how long
various actors retain records and how they report TTL values. In
general, we observe that the authoritative TTL value is frequently
modiﬁed by the client-side DNS infrastructure. We show that large
TTLs are reduced in 64% of the cases, and small TTLs are in-
creased in 11% of our measurements. We tease apart these behav-
iors and attribute the former behavior predominantly to RDNSies
and the latter behavior predominantly to FDNSes. Additionally, we
ﬁnd that cache evictions due to capacity limits occur infrequently in
RDNSies even for rarely accessed records. At the same time, while
the TTL is frequently mis-reported to clients, resolvers themselves
do not retain records much past authoritative TTL. We observe that
records are returned past TTL in only 10% of the cases, even for
records with a relatively short TTL of 30 seconds.
Acknowledgments
This work was supported in part by NSF through grants CNS-
0831821, CNS-1213157 and CNS-0831535. The authors would
like to thank the anonymous reviewers – and in particular our shep-
herd, Meeyoung Cha – for their assistance in improving the paper.
10. REFERENCES
[1] Open Resolver Project.
http://openresolverproject.org/.
[2] B. Ager, W. Mühlbauer, G. Smaragdakis, and S. Uhlig.
Comparing DNS Resolvers in the Wild. In 10th ACM
SIGCOMM IMC, pages 15–21, 2010.
[3] Alexa. http://www.alexa.com/topsites.
[4] H. A. Alzoubi, M. Rabinovich, and O. Spatscheck. The
Anatomy of LDNS Clusters: Findings and Implications for
Web Content Delivery. In 22d Int. WWW Conf., 2013.
[5] R. Arends. DNS Security Introduction and Requirements,
2005. RFC 4033.
[6] T. Callahan, M. Allman, and M. Rabinovich. On Modern
DNS Behavior and Properties. ACM SIGCOMM CCR,
43(3):7–15, 2013.
[7] B. Chun, D. Culler, T. Roscoe, A. Bavier, L. Peterson,
M. Wawrzoniak, and M. Bowman. PlanetLab: An Overlay
Testbed for Broad-Coverage Services. ACM SIGCOMM
CCR, 33(3):3–12, 2003.
[8] D. Dagon, N. Provos, C. Lee, and W. Lee. Corrupted DNS
Resolution Paths: The Rise of a Malicious Resolution
Authority. In NDSS, 2008.
[9] I. Google.
https://developers.google.com/speed/
public-dns/docs/performance#loadbalance.
[10] K. Gummadi, S. Saroiu, and S. Gribble. King: Estimating
Latency Between Arbitrary Internet End Hosts. In 2nd ACM
SIGCOMM Workshop on Internet Measurment, pages 5–18.
ACM, 2002.
[11] C. Huang, D. Maltz, J. Li, and A. Greenberg. Public DNS
System and Global Trafﬁc Management. In IEEE
INFOCOM, pages 2615 –2623, 2011.
[12] D. Kaminsky. Black Ops 2008: It’s the End of the Cache As
We Know It. Black Hat USA, 2008.
[13] D. Leonard and D. Loguinov. Demystifying Service
[19] K. Schomp, T. Callahan, M. Rabinovich, and M. Allman.
Discovery: Implementing an Internet-wide Scanner. In 10th
ACM IMC, pages 109–122, 2010.
Client-Side DNS Infrastructure Dataset, Oct. 2013.
http://dns-scans.eecs.cwru.edu/.
[14] R. Liston, S. Srinivasan, and E. Zegura. Diversity in DNS
[20] A. Shaikh, R. Tewari, and M. Agrawal. On the Effectiveness
Performance Measures. In 2nd ACM SIGCOMM Workshop
on Internet Measurment, pages 19–31. ACM, 2002.
of DNS-based Server Selection. In INFOCOM, pages
1801–1810, 2001.
[15] Z. M. Mao, C. D. Cranor, F. Douglis, M. Rabinovich,
[21] C. Shue, A. Kalafut, M. Allman, and C. Taylor. On Building
O. Spatscheck, and J. Wang. A Precise and Efﬁcient
Evaluation of the Proximity Between Web Clients and Their
Local DNS Servers. In USENIX ATC, pages 229–242, 2002.
[16] Geoip. maxmind llc, 2012.
[17] J. Pang, A. Akella, A. Shaikh, B. Krishnamurthy, and
S. Seshan. On the Responsiveness of DNS-based Network
Control. In 4th ACM SIGCOMM IMC, pages 21–26, 2004.
[18] M. Rajab, F. Monrose, A. Terzis, and N. Provos. Peeking
Through the Cloud: DNS-based Estimation and its
Applications. In Applied Cryptography and Network
Security, pages 21–38. Springer, 2008.
Inexpensive Network Capabilities. ACM SIGCOMM CCR,
42(2), Apr. 2012.
[22] G. Sisson. DNS Survey: October 2010. http://dns.
measurement-factory.com/surveys/201010/,
2010.
[23] C. E. Wills, M. Mikhailov, and H. Shang. Inferring Relative
Popularity of Internet Applications by Actively Querying
DNS Caches. In 3rd ACM SIGCOMM IMC, pages 78–90,
2003.