### Optimized Text

**Congestion-Free Update Sequences:**

Given the engineered scratch capacity on each link (as detailed in §4.2), we demonstrate that there exists a congestion-free sequence of updates with a length no greater than \(\lfloor 1/s \rfloor - 1\) steps (Theorem 2 in [14]). The constructive proof of this theorem provides an update sequence with exactly \(\lfloor 1/s \rfloor - 1\) steps. However, shorter sequences may be possible and are desirable as they would result in faster updates.

To find the sequence with the minimal number of steps, we employ an LP-based algorithm. Figure 7 illustrates how to determine the feasibility of a sequence with \(q\) steps, where \(q\) ranges from 1 to \(\lfloor 1/s \rfloor - 1\) in increments of 1. A key constraint in the LP is that the worst-case load on any link during an update must remain below the link's capacity. This load is given by \(\sum_{i,j} b^{a+1}_{i,j} \max(b^a_{i,j}) I_{j,l}\) at step \(a\); it occurs when all flows that will increase their contribution have already done so, but none of the flows that will decrease their contribution have yet done so. If \(q\) is feasible, the LP outputs \(C_a = \{b^a_{i,j}\}\) for \(a = (1, \ldots, q-1)\), representing the intermediate configurations that form a congestion-free update sequence.

**From Congestion-Free to Bounded-Congestion:**

We have shown that leaving scratch capacity on each link facilitates congestion-free updates. If there is a class of traffic tolerant to moderate congestion (e.g., background traffic), the scratch capacity need not be idle; instead, we can fully utilize link capacities, with the caveat that transient congestion will only affect this class of traffic. To achieve this, when computing flow allocations (§4.2), we set \(sP_{ri} = s > 0\) for interactive and elastic traffic, but \(sP_{ri} = 0\) for background traffic (allocated last). Thus, while link capacity can be fully used, no more than \((1-s)\) fraction is used by non-background traffic.

However, since links are no longer guaranteed to have slack, a congestion-free solution within \(\lfloor 1/s \rfloor - 1\) steps may not exist. To address this, we modify the link capacity constraint in Figure 7 with two constraints: one to ensure that the worst-case traffic on a link from all classes does not exceed \((1 + \eta)\) of the link capacity (\(\eta \in [0, 50\%]\)), and another to ensure that the worst-case traffic due to non-background traffic remains below the link capacity. We prove that:

1. There is a feasible solution within \(\max(\lfloor 1/s \rfloor - 1, \lfloor 1/\eta \rfloor)\) steps (Theorem 3 in [14]).
2. Non-background traffic never encounters loss.
3. Background traffic experiences no more than an \(\eta\) fraction loss.

In SWAN, we set \(\eta = s / (1-s)\), ensuring the same \(\lfloor 1/s \rfloor - 1\) bound on steps as before.

**Updating Tunnels:**

To update the set of tunnels in the network from \(P\) to \(P'\), SWAN first computes a sequence of tunnel-sets \((P = P_0, \ldots, P_k = P')\) that fit within the rule limits of switches. For each set, it calculates the amount of traffic from each service that can be carried (§4.2). It then signals services to send at a rate that is the minimum across all tunnel-sets. After \(T_h = 10\) seconds, when services have adjusted their sending rates, SWAN executes tunnel changes as follows:

1. Add tunnels that are in \(P_{i+1}\) but not in \(P_i\).
2. Change traffic distribution using bounded-congestion updates to what is supported by \(P_{i+1}\), freeing up tunnels in \(P_i\) but not in \(P_{i+1}\).
3. Delete these tunnels.
4. Signal services to start sending at the rate corresponding to \(P'\).

**Computing Interim Tunnel-Sets:**

We compute interim tunnel-sets as follows. Let \(P_{add}\) be the set of tunnels to add and \(P_{rem}\) be the set of tunnels to remove. Initially, \(P_{add}^0 = P' - P\) and \(P_{rem}^0 = P - P'\). At each step \(i\), we pick subsets \(p_{add}^i \subseteq P_{add}^i\) and \(p_{rem}^i \subseteq P_{rem}^i\) to add and remove, respectively. We update the tunnel sets as: \(P_{i+1} = (P_i \cup p_{add}^i) - p_{rem}^i\). The process ends when \(P_{add}^i\) and \(P_{rem}^i\) are empty, at which point \(P_i\) will be \(P'\).

At each step, we maintain the invariant that \(P_{i+1}\) fits within the rule limits. The value of \(t_{add}^i\) ensures that \(\lambda M_j\) rule space is free at every switch \(j\). We achieve this by picking the maximal set \(p_{add}^i\) such that the tunnels in \(p_{add}^i\) fit within \(t_{add}^i\) rules and the minimal set \(p_{rem}^i\) such that the remaining tunnels to be removed \((P_{rem}^i - p_{rem}^i)\) fit within \(t_{rem}^i\) rules. The values of \(t_{add}^i\) and \(t_{rem}^i\) are defined more precisely in Theorem 4 in [14].

Within the size constraint, SWAN prefers tunnels that will carry more traffic in the final configuration \(P'\) and those that transit through fewer switches when selecting \(p_{add}^i\). When selecting \(p_{rem}^i\), it prefers tunnels that carry less traffic in \(P_i\) and those that transit through more switches. This approach biases SWAN towards finding interim tunnel-sets that carry more traffic and use fewer rules.

We show that the algorithm requires at most \(\lfloor 1/\lambda \rfloor - 1\) steps and satisfies the rule count constraints (Theorem 4 in [14]). At interim steps, some services may receive a lower allocation than in \(P\) or \(P'\). The problem of finding interim tunnel-sets where no service's allocation is lower than the initial and final set, given link capacity constraints, is NP-hard. In practice, however, services rarely experience short-term reductions (§6.6). Additionally, since both \(P\) and \(P'\) contain a common core with at least one common tunnel between each DC-pair, basic connectivity is always maintained during transitions, which suffices to carry at least all interactive traffic.

**Handling Failures:**

Gracefully handling failures is crucial for a global resource controller. We outline how SWAN manages failures. Link and switch failures are detected and communicated to the controller by network agents, which immediately compute new allocations. Some failures can break the symmetry in topology that SWAN leverages for scalable computation of allocations. In such cases, the controller expands the topology of impacted DCs and computes allocations at the switch level directly.

Network agents, service brokers, and the controller have backup instances that take over when the primary fails. For simplicity, backups do not maintain state but acquire what is needed upon taking over. Network agents query switches for topology, traffic, and current rules. Service brokers wait for \(T_h = 10\) seconds, by which time all hosts would have contacted them. The controller queries network agents for topology, traffic, and current rule sets, and service brokers for current demand. Hosts stop sending traffic if they cannot contact the (primary and secondary) service broker. Service brokers retain their current allocation when they cannot contact the controller. During the transition period between the primary controller failing and the backup taking over, the network continues to forward traffic as last configured.

**Prototype Implementation:**

We have developed a SWAN prototype that implements all the described elements. The controller, service brokers, hosts, and network agents communicate using RESTful APIs. We implemented network agents using the Floodlight OpenFlow controller [11], allowing SWAN to work with commodity OpenFlow switches. We use QoS features in Windows Server 2012 to mark DSCP bits in outgoing packets and rate limit traffic using token buckets. Priority queues per class are configured in switches. Based on our experiments (§6), we set \(s = 10\%\) and \(\lambda = 10\%\) in our prototype.

**Testbed-Based Evaluation:**

We evaluate SWAN on a modest-sized testbed, examining its efficiency and the value of congestion-controlled updates using today’s OpenFlow switches and under TCP dynamics. Additional testbed experiments, such as failure recovery time, are detailed in [14]. We will extend our evaluation to the scale of today’s inter-DC WANs in §6.

**Testbed and Workload:**

Our testbed emulates an inter-DC WAN with 5 DCs spread across three continents (Figure 8). Each DC has:
1. Two WAN-facing switches.
2. Five servers per DC, each with a 1G Ethernet NIC and acting as 25 virtual hosts.
3. An internal router that splits traffic from the hosts over the WAN switches.

A logical link between DCs consists of two physical links between their WAN switches. WAN switches are a mix of Arista 7050Ts and IBM Blade G8264s, and routers are a mix of Cisco N3Ks and Juniper MX960s. The SWAN controller is located in New York, and we emulate control message delays based on geographic distances.

In our experiment, every DC pair has a demand in each priority class. The demand for the Background class is infinite, while Interactive and Elastic demands vary with a 3-minute period as shown in Figure 9. Each DC pair has a different phase, meaning their demands are not synchronized. These demands were chosen because they have sudden changes in quantity and spatial characteristics to stress-test SWAN. The actual traffic per {DC-pair, class} consists of hundreds of TCP flows. Our switches do not support unequal splitting, so we insert appropriate rules into the switches to split traffic based on IP headers.

We set \(T_s\) and \(T_c\), the service demand and network update frequencies, to one minute, instead of five, to stress-test SWAN’s dynamic behavior.

**Experimental Results:**

**Efficiency:**
Figure 10 shows that SWAN closely approximates the throughput of an optimal method. For each 1-minute interval, this method computes service rates using a multi-class, multi-commodity flow problem that is not constrained by the set of available tunnels or rule count limits. It perfectly predicts interactive traffic, has no overhead due to network updates, and can modify service rates instantaneously.

Overall, SWAN closely approximates the optimal throughput.