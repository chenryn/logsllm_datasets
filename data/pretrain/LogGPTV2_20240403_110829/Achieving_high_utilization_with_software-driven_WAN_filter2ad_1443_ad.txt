least one link. However, given the scratch capacity that we
engineered on each link (sP ri; §4.2), we show that there ex-
ists a congestion-free sequence of updates of length no more
than (cid:100)1/s(cid:101)−1 steps (Theorem 2 in [14]). The constructive
proof of this theorem yields an update sequence with ex-
actly (cid:100)1/s(cid:101)−1 steps. But shorter sequences may exist and
are desirable because they will lead to faster updates.
We use an LP-based algorithm to ﬁnd the sequence with
the minimal number of steps. Figure 7 shows how to exam-
4Network updates can cause packet re-ordering.
In this
work, we assume that switch-level (e.g., FLARE [18]) or
host-level mechanisms (e.g., reordering robust TCP [36]) are
in place to ensure that applications are not hurt.
20i,j, ba+1
i,j max(ba
is (cid:80)
s(cid:101) − 1 steps. To remedy this, we replace the per-
ine whether a feasible sequence of q steps exists. We vary
q from 1 to (cid:100)1/s(cid:101)−1 in increments of 1. The key part in
the LP is the constraint that limits the worst case load on a
link during an update to be below link capacity. This load
i,j )Ij,l at step a; it happens when none
of the ﬂows that will decrease their contribution have done
so, but all ﬂows that will increase their contribution have
already done so. If q is feasible, the LP outputs Ca={ba
i,j},
for a=(1, . . . , q − 1), which represent the intermediate con-
ﬁgurations that form a congestion-free update sequence.
From congestion-free to bounded-congestion: We
showed above that leaving scratch capacity on each link fa-
cilitates congestion-free updates.
If there exists a class of
traﬃc that is tolerant to moderate congestion (e.g., back-
ground traﬃc), then scratch capacity need not be left idle;
we can fully use link capacities with the caveat that transient
congestion will only be experienced by traﬃc in this class.
To realize this, when computing ﬂow allocations (§4.2), we
use sP ri = s > 0 for interactive and elastic traﬃc, but set
sP ri = 0 for background traﬃc (which is allocated last).
Thus, link capacity can be fully used, but no more than
(1− s) fraction is used by non-background traﬃc. Just this,
however, is not enough: since links are no longer guaranteed
to have slack there may not be a congestion-free solution
within (cid:100) 1
link capacity constraint in Figure 7 with two constraints, one
to ensure that the worst-case traﬃc on a link from all classes
is no more than (1 + η) of link capacity (η ∈ [0, 50%]) and
another to ensure that the worst-case traﬃc due to the non-
background traﬃc is below link capacity. In this case, we
prove that i) there is a feasible solution within max((cid:100)1/s(cid:101)–
1, (cid:100)1/η(cid:101)) steps (Theorem 3 in [14]) such that ii) the non-
background traﬃc never encounters loss and iii) the back-
ground traﬃc experiences no more than an η fraction loss.
Based on this result, we set η = s
1−s in SWAN, which ensures
the same (cid:100) 1
4.3.2 Updating tunnels
To update the set of tunnels in the network from P
to P (cid:48), SWAN ﬁrst computes a sequence of tunnel-sets
(P =P0, . . . , Pk=P (cid:48)) that each ﬁt within rule limits of
switches. Second, for each set, it computes how much traﬃc
from each service can be carried (§4.2). Third, it signals ser-
vices to send at a rate that is minimum across all tunnel-sets.
Fourth, after Th=10 seconds when services have changed
their sending rate,
it starts executing tunnel changes as
follows. To go from set Pi to Pi+1:
i) add tunnels that
are in Pi+1 but not in Pi—the computation of tunnel-sets
(described below) guarantees that this will not violate rule
count limits; ii) change traﬃc distribution, using bounded-
congestion updates, to what is supported by Pi+1, which
frees up the tunnels that are in Pi but not in Pi+1; iii)
delete these tunnels. Finally, SWAN signals to services to
start sending at the rate that corresponds to P (cid:48).
s(cid:101) − 1 bound on steps as before.
i
We compute the interim tunnel-sets as follows. Let P add
be the set of tunnels that remain be added and
0 =P (cid:48)−P and
i ⊆
to remove. We then
i −
i )−pr
and
and P rem
removed, respectively, at step i. Initially, P add
0 =P−P (cid:48). At each step i, we ﬁrst pick a subset pa
P rem
i ⊆ P rem
P add
update the tunnel sets as: Pi+i=(Pi∪pa
pa
i , and P rem
P rem
i
are empty (at which point Pi will be P (cid:48)).
i . The process ends when P add
to add and a subset pr
i − pr
i+1 =P rem
i
i , P add
i+1 =P add
i
i
i
At each step, we also maintain the invariant that Pi+1,
i
i
i
i ﬁt within tadd
0 ∪ ··· ∪ pa
rules. The value of tadd
which is the next set of tunnels that will be installed in the
network, leaves λMj rule space free at every switch j. We
achieve this by picking the maximal set pa
i such that the
tunnels in pa
rules and the mini-
mal set pr
i such that the tunnels that remain to be removed
i − pr
(P rem
i ) ﬁt within trem
increases
i
with i and that of trem
decreases with i; they are deﬁned
more precisely in Theorem 4 in [14]. Within the size con-
straint, when selecting pa
i , SWAN prefers tunnels that will
carry more traﬃc in the ﬁnal conﬁguration (P (cid:48)) and those
that transit through fewer switches. When selecting pr
i , it
prefers tunnels that carry less traﬃc in Pi and those that
transit through more switches. This biases SWAN towards
ﬁnding interim tunnel-sets that carry more traﬃc and use
fewer rules.
We show that the algorithm above requires at most
(cid:100)1/λ(cid:101)− 1 steps and satisﬁes the rule count constraints (The-
orem 4 in [14]). At interim steps, some services may get an
allocation that is lower than that in P or P (cid:48). The problem of
ﬁnding interim tunnel-sets in which no service’s allocation is
lower than the initial and ﬁnal set, given link capacity con-
straints, is NP-hard. (Even much simpler problems related
to rule-limits are NP-hard [13]). In practice, however, ser-
vices rarely experience short-term reductions (§6.6). Also,
since both P and P (cid:48) contain a common core in which there
is at least one common tunnel between each DC-pair (per
our tunnel selection algorithm; §4.2), basic connectivity is
always maintained during transitions, which in practice suf-
ﬁces to carry at least all of the interactive traﬃc.
4.4 Handling failures
Gracefully handling failures is an important part of a
global resource controller. We outline how SWAN handles
failures. Link and switch failures are detected and com-
municated to the controller by network agents, in response
to which the controller immediately computes new alloca-
tions. Some failures can break the symmetry in topology
that SWAN leverages for scalable computation of allocation.
When computing allocations over an asymmetric topology,
the controller expands the topology of impacted DCs and
computes allocations at the switch level directly.
Network agents, service brokers, and the controller have
backup instances that take over when the primary fails. For
simplicity, the backups do not maintain state but acquire
what is needed upon taking over. Network agents query the
switches for topology, traﬃc, and current rules. Service bro-
kers wait for Th (10 seconds), by which time all hosts would
have contacted them. The controller queries the network
agents for topology, traﬃc, and current rule set, and service
brokers for current demand. Further, hosts stop sending
traﬃc when they are unable to contact the (primary and
secondary) service broker. Service brokers retain their cur-
rent allocation when they cannot contact the controller. In
the period between the primary controller failing and the
backup taking over, the network continues to forward traﬃc
as last conﬁgured.
4.5 Prototype implementation
We have developed a SWAN prototype that implements
all the elements described above. The controller, service
brokers and hosts, and network agents communicate with
each other using RESTful APIs. We implemented network
agents using the Floodlight OpenFlow controller [11], which
21(b)
(c)
(a)
Figure 8: Our testbed. (a) Partial view of the equip-
ment. (b) Emulated DC-level topology. (c) Closer look
at physical connectivity for a pair of DC.
allows SWAN to work with commodity OpenFlow switches.
We use the QoS features in Windows Server 2012 to mark
DSCP bits in outgoing packets and rate limit traﬃc using
token buckets. We conﬁgure priority queues per class in
switches. Based on our experiments (§6), we set s=10% and
λ=10% in our prototype.
5. TESTBED-BASED EVALUATION
We evaluate SWAN on a modest-sized testbed. We ex-
amine the eﬃciency and the value of congestion-controlled
updates using today’s OpenFlow switches and under TCP
dynamics. The results of several other testbed experiments,
such as failure recovery time, are in [14]. We will extend our
evaluation to the scale of today’s inter-DC WANs in §6.
5.1 Testbed and workload
Our testbed emulates an inter-DC WAN with 5 DCs
spread across three continents (Figure 8). Each DC has: i)
two WAN-facing switches; ii) 5 servers per DC, where each
server has a 1G Ethernet NIC and acts as 25 virtual hosts;
and iii) an internal router that splits traﬃc from the hosts
over the WAN switches. A logical link between DCs is two
physical links between their WAN switches. WAN switches
are a mix of Arista 7050Ts and IBM Blade G8264s, and
routers are a mix of Cisco N3Ks and Juniper MX960s. The
SWAN controller is in New York, and we emulate control
message delays based on geographic distances.
In our experiment, every DC pair has a demand in each
priority class. The demand of the Background class is inﬁ-
nite, whereas Interactive and Elastic demands vary with a
period of 3-minutes as per the patterns shown in Figure 9.
Each DC pair has a diﬀerent phase, i.e., their demands are
not synchronized. We picked these demands because they
have sudden changes in quantity and spatial characteristics
to stress SWAN. The actual traﬃc per {DC-pair, class} con-
sists of 100s of TCP ﬂows. Our switches do not support
unequal splitting, so we insert appropriate rules into the
switches to split traﬃc as needed based on IP headers.
We set Ts and Tc, the service demand and network up-
date frequencies, to one minute, instead of ﬁve, to stress-test
SWAN’s dynamic behavior.
5.2 Experimental results
Eﬃciency: Figure 10 shows that SWAN closely approxi-
mates the throughput of an optimal method. For each 1-min
interval, this method computes service rates using a multi-
class, multi-commodity ﬂow problem that is not constrained
(a) Interactive
(b) Elastic
Figure 9: Demand patterns for testbed experiments.
Figure 10: SWAN achieves near-optimal throughput.
(a) SWAN
(b) One-shot
(c) One-shot
Figure 11: Updates in SWAN do not cause congestion.
by the set of available tunnels or rule count limits. It’s pre-
diction of interactive traﬃc is perfect, it has no overhead
due to network updates, and it can modify service rates in-
stantaneously.
Overall, we see that SWAN closely approximates the op-