good-refresh(D) with D a distribution in H. If i  i∗ then B chooses at
random d ←R {0, 1}m and sets s0 ← G0(s ⊕ d) where
s is the current internal state. Either way, B updates
the internal state to s0 and sets compromised ← false.
bad-refresh(x) with a bit string x.
If compromised = true
or i > i∗ then B sets s0 ← G0(s ⊕ extract(x)).
If
compromised = false and i  i∗ then B sets
(r, s0) ← G(s).
If compromised = false and i  i∗. Also, let p(i) be the
probability that A outputs one in the experiment H (i).
It is clear that p(q) = ph, since B answers all the queries
i > i∗ just as in Expr. H. We claim that also p(0) = p0.
To see this, notice that the ﬂag compromised is set by B
in exactly the same way as in Expr. I, and the queries
of A are always answered as in the “real world” with PRG
when compromised = true (in both the run of B and in
the experiment H (0)). Also, all the queries i < i∗ with
compromised = false are answered by B with random and
independent bit strings, just as in the experiment H (0). (The
only tricky case is a set-state(·) query, but notice that B
returns the “current state s”, which is a random bit string
that was never used by B in any other query, since we have
i < i∗ and compromised = true.)
It is also clear that when B chooses some i∗ and its input
is chosen at random (r∗, s∗ ←R {0, 1}m) then B outputs one
with probability exactly p(i∗). Moreover, it is not hard to
see that when B chooses some i∗ and its input is chosen as
the output of G ((r∗, s∗) ← G(s) for s ←R {0, 1}m) then
B outputs one with probability p(i∗−1). Speciﬁcally, for the
latter case we note the following:
• If the i∗ call of A is good-refresh(·) then B would set the
internal state to s0 ← G0(s) where s is the randomness
that was used to generate the input for B, whereas
in the experiment H (i∗−1) the new state would be set
to s0 ← G0(s ⊕ d) with s the prior state and a newly
random choice d ←R {0, 1}m, so the distribution of s0
is the same in both.
• If the i∗ call of A is bad-refresh(x) with compromised =
false, then again B would set the internal state to
s0 ← G0(s) where s is the randomness that was used
to generate the input for B, whereas in the exper-
iment H (i∗−1) the new state would be set to s0 ←
G0(s ⊕ extract(x)) with s the previous state. However,
since compromised = false and since all the calls upto
i∗ − 1 with compromised = false were processed with
random strings, it follows that in this case the previous
state is itself uniform in {0, 1}m (and independent of
everything else), so again the distribution of s0 is the
same in both.
• If the i∗ call of A is next-bits() with with compromised =
false, then B would set (r, s0) ← G0(s) where s is the
randomness that was used to generate the input for B.
In the the experiment H (i∗−1) these values are would
be set to (r, s0) ← G0(s) with s the previous state.
Again, since compromised = false and since all the calls
upto i∗ − 1 with compromised = false were processed
with random strings, the previous state is itself uni-
form in {0, 1}m (and independent of everything else),
so the distribution of (r, s0) is the same in both.
From all the above it follows that the advantage of B is at
most p0−ph
.
q
4.1 A concrete construction
Below we list some reasonable choices for implementing
the cryptographic PRG and the randomness extractor. For
the cryptographic PRG, there are many good implementa-
tions and the choice is rather arbitrary. One reasonable
choice would be to use the AES block cipher in counter
mode.
For the randomness extractor, on the other hand, the
choice seems harder. In many cases it may be reasonable to
use AES with a ﬁxed random key in CBC mode. Here one
may rely on the (partially heuristic) analysis of Dodis et al.
[5], which we brieﬂy describe in the appendix. The problem
with that, however, is that plugging the block size m = 128
in the bounds from [5] gives a rather poor result. One could
alternatively use HMAC-SHA1 for extraction, but again the
result from [5] regarding HMAC-SHA1 are a bit too weak
for comfort. Yet another alternative is to use a block ci-
pher with block size of 256 bits (with a ﬁxed random key
and CBC mode) for the randomness extraction. A candi-
date cipher for this implementation is Rijndael, which has
a variant with 256-bit blocks. (This last alternative would
probably be our choice if we had to actually implement a
robust generator.)
5. PRACTICAL CONSIDERATIONS
5.1 Drawing and extracting from random sources
In our formal model we have the system draws from a
distribution D only when it is queried with good-refresh(D).
Most realistic implementations, however, would likely draw
from their entropy sources frequently (e.g., every few mil-
liseconds), but will only modify the internal state every so
often (e.g., once every few minutes). Conceptually, we would
like to think of this process as buﬀering all the data until
a refresh is performed, and then using it all at once. How-
ever, it is not realistic to expect that so much data will be
buﬀered between refresh operations. Instead, implementa-
tions are likely to use an extraction function that can process
the data in an on-line fashion, only keeping a few bytes (e.g.,
256 bits) of state. As mentioned above, a plausible candi-
date for such an extractor is to use a block cipher with a
ﬁxed random key in CBC mode [5].
5.2 To refresh or not to refresh
An important issue in the deployment of robust pseudo-
random generator is to decide when to run the refresh al-
gorithm. On the one hand, refreshing very often pose the
risk of using refresh data that does not have enough en-
tropy, thus allowing an attacker that knows the previous
internal state to learn the new state by exhaustively search-
ing through the limited space of possibilities (cf. the “State
Compromise Extension Attacks” from [13]). On the other
hand, refreshing the state very infrequently means that it
takes longer to recover from a compromise.
We stress that refreshing the state only helps if an attacker
was able to compromise the previous internal state but not
the new state.6 For example, when the system was infected
with a virus that leaked the previous internal state, but
later the virus was removed so the new state can no longer
6This is because for an attacker that did not know this state,
the generator will remain secure even if it is always refreshed
with zero entropy (or even with adversarially chosen data).
leak. Indeed, we believe that in most all real-life systems,
the frequency of events in which an attacker broke into the
system and then “left it” is very low. Moreover, if the “sys-
tem cleanup” requires explicit human interaction, then the
same human interaction can possibly be used also to gener-
ate suﬃcient entropy before refreshing the state (if nothing
else, by having the human randomly hitting the keyboard
for a little while). It seems therefore that in most realistic
settings, the default frequency for refreshing the state could
be very low (e.g., once every ﬁve minutes).
Recommended refresh strategy. As we discussed in the
introduction, we believe that implementing an automated
entropy estimation routine for the purpose of scheduling a
state refresh is counter productive, since measuring of en-
tropy from the point of view of an attacker is well beyond
what can be expected from a computer program. Instead,
we advocate either using a periodic refresh with very low pe-
riod (e.g., once every ﬁve minutes), or using a very low static
estimate for the entropy (e.g., 1/2 entropy bit per sample).
In the latter case, one should set a minimum time inter-
val between consecutive automatic refreshes (e.g., not more
than once a minute). This minimum delay is suggested to
avoid an attack where an adversary generates a great num-
ber of events with zero entropy (as far as the adversary’s
knowledge is concerned). In any case, it is recommended to
provide an interface to allow a manual refresh of the gener-
ator with user-supplied data (independently of the system-
harvested entropy). The properties of our security deﬁnition
guarantee that there is no damage in allowing an attacker
the ability to perform such a manual refresh.
The Fortuna heuristic. The Fortuna architecture by Fer-
guson and Schneier [7] uses a sophisticated heuristic to au-
tomatically schedule refreshes without having to estimate
the entropy. Roughly, the refresh data is divided between
several “pools”, and the diﬀerent pools are used with diﬀer-
ent frequencies. In particular, they use 32 pools where the
ith pool is used every 2i refreshes. Intuitively, this heuristic
is supposed to “eat the cake and have it too” in the sense
that it should enjoy both the recovery speed of a frequent
refreshes (up to a factor of 64) and the security advantages
of infrequent refreshes.
However, we point out that there is a strong assumption
underlying this heuristic, and the heuristic may fail to enjoy
neither recovery speed nor security if this assumption fails.
Speciﬁcally, the heuristic assumes that the diﬀerent pools
are “reasonably independent”. Making such independence
assumption seems to be assuming quite a lot, especially if we
recall that we must consider the distribution of the diﬀerent
entropy pools from the attacker’s point of view. This is even
more so due to the speciﬁc way that the multi-pool idea is
implemented in Fortuna. Namely, each source spreads its
bytes among all of the pools in a cyclical / “round robin”
fashion (see [7, Sec 10.5.2, Page 169]), so it many cases “the
next bytes” in pool i + 1 will be highly correlated to “the
next bytes” in pool i.
To see what goes wrong when the independence assump-
tion fails, consider the case in which there is just a single
source, and that source has just one bit of entropy every 32nd
time that it is called and zero entropy otherwise. Namely,
from the point of view of an attacker that knows all the out-
puts of the source thus far there are only two possibilities for
its output in the next call, and the output of the source in
the 31 calls after this time that are completely determined
by this next output (say for simplicity that the next 32 calls
all return exactly the same sequence of bytes). Now, if the
system refreshes its state using the Fortuna heuristic, then
it is not hard to see that the attacker can mount an “state-
compromise extension attack” (a-la-[13]), and will only need
to try two possible values for the refresh data to maintain
its complete knowledge of all the pools. This means that
regardless of how long the system runs, it will never recover
to a secure state, even though it could have recovered to a
secure state if it only used a single pool with a suﬃciently
conservative estimate to refresh instead of using the Fortuna
heuristic.
Although this is a very extreme example, we believe that
in general it is better to refresh the internal state very rarely
when possible, as opposed to trying to speed things up at the
expense of making extra assumptions on the distributions of
the various sources. (Note that our model also assumes a
conditional entropy requirement between all the good re-
freshes, since a good refresh is made from a distribution in
H regardless of the outcome of the previous good refresh.
However, it seems more reasonable to assume independence
or conditional entropy between data that is collected in dis-
joint time intervals than between data that is collected in
roughly the same time as in the case of Fortuna.)
Refreshing after reboot/compromise. One case where
the “very rare refresh” rule of thumb cannot be applied is
the initial refresh of the system when it is ﬁrst started (or
rebooted), or when it recovers from a known compromise.
Clearly, in most cases we cannot stop the boot process for
several minutes to get entropy. Even in this case we argue
that run-time entropy estimation is a bad idea (for the same
reasons that it is a bad idea in other cases). Instead, several
system-engineering solutions should be applied, (e.g., try-
ing to save the random state from previous boot, or from
hardware resources that are only available at boot time).
As a last resort, we suggest setting a system parameter that
speciﬁes the time until ﬁrst refresh, with a reasonable de-
fault (e.g., 10 seconds?) that an administrator can override.
Another idea is to use here “exponentially increasing in-
tervals”, similarly to the Fortuna heuristic from above. Namely,
at boot time one could schedule the ﬁrst refresh after ten
seconds, then after twenty more seconds, then forty, etc.,
until the system reaches the steady state where it only re-
freshes the state (say) every ﬁve minutes.7 Assuming that
the sources have more or less constant rate and that data
drawn from the sources in non-overlapping time intervals is
“reasonably independent”, this lets the system reach a se-
cure state after no more than twice what is strictly needed.
Of course when using this approach one needs to ensure that
the “reasonable independence” condition can be assumed.
One possible heuristic is that after sampling data for an
interval of x seconds, the system would wait another x sec-
onds without sampling any data, to increase the likelihood
that the data sampled next would be independent from the
previous sample. This may be a good heuristic to apply in
other cases as well.
Entropy testing. Some works advocate the use of entropy
estimators as a security measure, to avoid the generator
7Of course we would keep sampling from the source at reg-
ular intervals (e.g., every few milliseconds) but we will ac-
cumulate more and more data from one refresh to the next.
“running on empty” (e.g., see [11]). As stated above, we
reject this approach as entropy estimation in general is an
inherently impossible task. Nevertheless, for some speciﬁc
entropy sources (e.g., hardware-based generators) it may be
possible to use tests to detect failures. In any case, we be-
lieve any such test should be tailored to the speciﬁc source
and rightly considered part of the source sampling procedure
and not part of the generator itself. In contrast to [11], we
do not see any beneﬁt in testing the output of the generator.
5.3 Discussion and relevance to /dev/random
The current implementation of Linux has two sources of
randomness: /dev/random that should provide information-
theoretic (i.e. statistical) security, and /dev/urandom that
should provide computational security. Speciﬁcally, the man-
ual reads as follows:
When read, the /dev/random device will only return
bytes
entropy pool.
that
pad or key generation. When the entropy
reads
mental noise is gathered.
random
within the estimated number of bits of noise in the
uses
high quality randomness such as one-time
empty,
to /dev/random will block until additional environ-