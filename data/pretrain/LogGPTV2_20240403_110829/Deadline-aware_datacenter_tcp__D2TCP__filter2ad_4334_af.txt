exponent  of  the  gamma-correction  function,  extremely  low  and 
high values may cause undesirable behavior. Therefore we cap the 
Recall  from  Section  3.2.3,  that  because ùëë  appears  in  the 
value of ùëë to be within the range (ùëõ,1/ùëõ). In all our experiments 
we set ùëõ=2.0. In this section we evaluate the effects of varying 
ùëõ. In Figure 15, we show D2TCP‚Äôs percentage of missed deadlines 
for  various  fan-ins  as  we  vary ùëõ  between  1.25  and  3.0.  As 
expected,  when ùëõ  is  close  to  1.0,  D2TCP‚Äôs  behavior  matches 
DCTCP  and  the  fraction  of  missed  deadlines  is  high.  As ùëõ 
off.  At ùëõ=3.0  and  beyond  we  see  that  the  fraction  starts  to 
increase  slowly  as  the  larger ùëõ  allows  near-deadline  flows  to 
increases  to  2.0,  the  fraction  drops  dramatically  but  then  levels 
increasingly ignore congestion feedback.  
4.2.5  Coexisting with TCP 
To  demonstrate  that  D2TCP  can  coexist  with  TCP  without 
hurting  bandwidth  or  deadlines,  we  use  the  same  production 
benchmark  but  use  a  mix  of  D2TCP  and  TCP  for  the  various 
network  traffic.  Because  we  are  specifically  interested  in  TCP 
performance  here,  we  restrict  this  experiment  to  fan-in  degrees 
where TCP  performance  is  acceptable  in  Figure  10  (i.e.,  missed 
deadline fraction of 5% or less, and fan-in degree of 15 and 20).  
We run three experiments gradually adding D2TCP traffic to 
a TCP datacenter. Imagine that the set of five OLDI applications 
in our workload are divided into two sets: set A consist of three 
OLDIs, and set B consists of the other two OLDIs. We start with 
Table 3: Deadlines achieved by D3 and D2TCP for similar 
fraction of missed deadlines 
Fan-in 
degree 
10 
15 
20 
D3‚Äôs  
missed 
deadlines 
(%) 
0.71 
3.61 
4.7 
D2TCP‚Äôs 
missed 
deadlines 
(%) 
0.84 
3.49 
4.88 
D2TCP‚Äôs 
tighter 
deadline 
(%) 
55 
45 
35 
)
%
(
s
e
n
i
l
d
a
e
d
d
e
s
s
M
i
40 
35 
30 
25 
20 
15 
10 
5 
0 
DCTCP 
D3 
OTCP 
D2TCP 
5 
10 
15 
20 
25 
Fan-in degree 
30 
35 
40 
Figure 17: Missed deadlines under tighter deadlines 
all  five  OLDIs  and  all  long  flows  running  on  TCP.  We  first 
‚Äúupgrade‚Äù set B to D2TCP, while set A continues to run on TCP. 
Next, we upgrade the background long-flow traffic to D2TCP as 
well. In summary, the three setups are: 
‚Ä¢  All-TCP: 5x OLDIs + long flows; no D2TCP. 
‚Ä¢  Mix#1 TCP: 3x OLDIs + long flows; D2TCP: 2x OLDIs. 
‚Ä¢  Mix#2 TCP: 3x OLDIs; D2TCP: 2x OLDI + long flows. 
In  Figure  16,  we  show  the  fraction  of  missed  deadlines  (Y 
axis)  for  sets  A  and  B  in  the  three  runs  as  we  vary  the  fan-in 
degree (X axis). We separately analyze long flows below. For set 
A, comparing All-TCP and Mix#1 shows that TCP‚Äôs (i.e., set A‚Äôs) 
missed-deadline fraction is not worsened by sharing the network 
with  D2TCP  traffic  (set  B  in  Mix#1).  Furthermore,  set  B  sees  a 
reduction  in  missed  deadlines  upon  migrating  from  TCP  (All-
TCP) to D2TCP (Mix#1). Going back to set A, comparing Mix#1 
and  Mix#2  we  see  that  TCP‚Äôs  (i.e.,  set  A‚Äôs)  missed-deadline 
fraction  is  not  hurt  by  background  flows  upgrading  to  D2TCP. 
Similarly,  set  B's)  missed-deadline  fraction  stays  the  same 
between Mix#1 and Mix#2, showing that background flows using 
D2TCP do not hurt the OLDIs using D2TCP.  
In Table 2, we show the long-flow throughput achieved in the 
three  runs.  Going  from All-TCP  to  Mix#1,  the  throughput  does 
not degrade, showing that upgrading some OLDIs to D2TCP does 
not  hurt  long  TCP  flows.  Comparing  Mix#1  and  Mix#2  shows 
that D2TCP‚Äôs long-flow throughput is similar to that of TCP. 
4.2.6  Tighter deadlines 
To show that D2TCP performs well over a range of deadlines, 
we  evaluate  D2TCP  under  tighter  deadlines  than  our  default 
(Section  4.2.1).  Because  we  found  that  deadlines  tightened  by 
10% or 20% lead to similar behavior, we show results only for the 
20%  case  in  Figure  17. As  expected,  the  tighter  deadlines  here 
result in more deadlines being missed under all the schemes than 
those  missed  in  Figure  10.  Nevertheless,  D2TCP  maintains  its 
advantage over DCTCP and D3 under the tighter deadlines. 
While  the  above  results  show  the  fractions  of  missed 
deadlines  under  tighter  deadlines,  it  may  be  important  to 
determine the inverse (i.e., how much tighter can the deadlines be 
for  a  target  fraction  of  missed  deadlines).  For  instance,  this 
question is typically of interest to datacenter operators who would 
threshold  and  wish 
to  know  how  much 
like  to  maintain  the  fraction  of  missed  deadlines  within  an 
acceptable 
the 
communication deadlines can be tightened to allow more time for 
computation and improve response quality. In Table 3, we show 
the tightness of deadlines supported by D2TCP as compared to D3 
for a target fraction of missed deadlines. We limit the study to a 
reasonable fraction of missed deadlines (i.e., 5%). From the table, 
we see that D2TCP achieves deadlines that are tighter by 35-55%, 
which would make sizable room for computation.  
5.  RELATED WORK 
There  is  an  abundance  of  past  work  that  deals  with  the 
subjects of congestion control, network scheduling, and reducing 
latencies. There are many schemes that build on top of TCP, while 
others are novel protocols altogether. A comprehensive review of 
all such work is beyond the scope of this paper, but we summarize 
some of the most relevant work here. 
Earliest  Deadline  First  (EDF)  [17]  is  one  of  the  earliest 
packet  scheduling  algorithms  and  is  provably  optimal  when 
deadlines are associated with individual packets.  When deadlines 
are  associated  with  flows,  however,  applying  EDF  to  individual 
packets as they arrive at the switch is not only suboptimal but can 
worsen the congestion in the network [26]. 
In  [24],  the  authors  show  that  having  finer  grain  system 
clocks  allow  for  a  faster  response  to TCP  timeouts,  and  help  in 
reducing the net latency of TCP flows. 
Rate  Control  Protocol  (RCP)  [5]  can  achieve  10-fold 
improvement in the completion times of small- to medium-sized 
flows  in  the  Internet,  particularly  downloads  representative  of 
typical web browsing. RCP is provably optimal when minimizing 
overall completion times is the metric. RCP replaces TCP‚Äôs slow 
start phase with an allocation equal to the fair share available at 
the  bottleneck 
requires  hardware 
modification to the routers. 
route.  Like  D3,  RCP 
Live multimedia traffic also has a soft-real-time nature, and 
both  proactive  bandwidth  reservation  [6]  and  reactive  [23]  [16] 
schemes  exist. TCP-RTM  [16]  observes  that TCP  always  favors 
reliability over timeliness, and proposes extensions that improve 
performance  of  multimedia  applications  by  allowing  minimal 
amount of packet re-ordering and loss in the TCP stack. 
Active  Queue  Management  schemes  like  RED  [8]  and  E-
TCP [11] inject early warnings of congestion to TCP endhosts by 
randomly dropping packets when switch buffer occupancy is high. 
Because senders back-off before full on congestion, these schemes 
allow  TCP  to  operate  in  the  high  throughput,  fast-retransmit 
mode, instead of degrading to full back-off. 
High-speed  TCP  [7],  CUBIC  [22],  and  XCP  [15]  all 
successfully improve the performance of TCP in high bandwidth-
delay-product  networks.  They  exploit 
large  degree  of 
statistical  multiplexing  present,  and  also  mitigate  TCP‚Äôs  drastic 
reaction  to  packet  losses.  XCP  shares  some  common  design 
details with D3 in that senders request bandwidth via a congestion 
header, and the switches populate their responses in this header. 
Re-feedback  [3]  addresses  the  problem  of  fairness  and 
stability in the Internet when untrusted senders may act selfishly 
in  the  face  of  congestion.  Re-feedback  incentivizes  senders  to 
populate packet headers with honest information about congestion 
situation so the network may schedule accordingly. 
 QCN  [20]  proposes  to  improve  Ethernet  performance  in 
datacenters  via multibit  feedback  from the  switches to  endhosts. 
By  utilizing  smarter  switches  and  hardware-based  reaction  logic 
in  the  endhost  NICs,  QCN  dramatically  reduces  recovery  time 
during  congestions,  thus  improving  flow  completion  times. 
However, QCN cannot span beyond L2 domains limiting its scope 
the 
of application. VCP [27] is another similar scheme that relies on 
ECN-like feedback via elaborate processing at the switches. 
6.  CONCLUSION 
Online,  data-intensive  applications  (OLDI)  in  datacenters 
(e.g., Web  search,  online  retail and  advertisement)  achieve  good 
user  experience  by  controlling  latency  using  soft-real-time 
constraints  which 
network 
to 
communication  within 
applications.  Further,  OLDI 
applications typically employ tree-based algorithms which, in the 
common  case,  result  in  fan-in  burst  of  children-to-parent  traffic 
with  tight  deadlines.  Previous  work  on  datacenter  network 
protocols  is  either  deadline-agnostic,  or  is  deadline-aware  but 
suffers under bursts due to race conditions.  
translate 
the 
deadlines 
We  proposed  Deadline-Aware  DataCenter  TCP  (D2TCP) 
for 
which: 
‚Ä¢ 
‚Ä¢ 
‚Ä¢ 
‚Ä¢ 
prioritizes near-deadline flows over far-deadline flows in the 
presence of fan-in-burst-induced congestion; 
achieves  high  bandwidth  for  background  flows  even  as  the 
short-lived OLDI flows come and go;  
requires no changes to the switch hardware; and  
coexists with legacy TCP. 
D2TCP  uses  a  distributed  and  reactive  approach  for 
fundamentally  enables  D2TCP‚Äôs 
bandwidth  allocation 
properties.  D2TCP‚Äôs  key  mechanism  is  a  novel  congestion 
avoidance algorithm, which uses ECN feedback and deadlines to 
modulate 
the  congestion  window  via  a  gamma-correction 
function.  
Using 
implementation,  and  at-scale 
small-scale 
real 
that 
simulations, we showed that D2TCP  
‚Ä¢ 
reduces  the  fraction  of  missed  deadlines  compared  to  both  
DCTCP and D3 by 75% and 50%, respectively; 
achieves  nearly  as  high  bandwidth  as  TCP  for  background  
flows without degrading OLDI performance; 
‚Ä¢ 
‚Ä¢ 
their 
flows  without  degrading 
‚Ä¢  meets deadlines that are 35-55% tighter than those achieved 
by D3 for the same reasonable fraction of missed deadlines 
(i.e., 5%), giving OLDIs more time for actual computation; 
and 
coexists  with  TCP 
performance. 
D2TCP has significant performance and practical advantages. 
On  the  performance  side,  by  reducing  the  number  of  missed 
deadlines, D2TCP improves OLDI applications‚Äô response quality, 
and hence user experience. Further, by meeting tighter deadlines, 
D2TCP  allows  more  time  for  computation  in  OLDI  applications 
and  thereby  further  enhances  OLDI  response  quality  and  user 
experience. Given that OLDI applications are likely to scale up in 
size  to  accommodate  ever-growing  data  on  the  Web,  D2TCP‚Äôs 
tighter deadlines may fundamentally enable this scale-up without 
degrading  OLDI  response  quality.  On  the  practical  side,  by 
requiring  no  changes  to  the  switch  hardware,  D2TCP  can  be 
deployed  by  merely  upgrading  the  TCP  and  RPC  stacks.  Our 
prototype implementation of D2TCP amounted to only 100 lines 
of kernel code. Finally, by being able to coexist with TCP, D2TCP 
is  amenable  to  incremental  deployment,  a  key  requirement  for 
datacenter  network  protocols  in  the  real  world.  The  growing 
importance  of  OLDI  applications  implies  that  these  significant 
advantages make D2TCP an important ingredient for datacenters. 
ACKNOWLEDGMENTS 
We thank the SIGCOMM reviewers, and our shepherd David 
Maltz, for their insightful comments which helped us significantly 
improve  the  paper.  We  also  thank  Sridhar  Raman  and  Abdul 
Kabbani for their help with real implementations of DCTCP and 
D2TCP, and Gwendolyn Voskuilen for reviewing the paper.  
REFERENCES 
[1]  M. Alizadeh, A. G. Greenberg, D. A. Maltz, J. Padhye, P. Patel, B. 
Prabhakar, S. Sengupta, and M. Sridharan. Data center TCP (DCTCP). In 
Proc. SIGCOMM, 2010. 
[2]  Charles A. Poynton (2003). Digital Video and HDTV: Algorithms and 
Interfaces. Morgan Kaufmann. pp. 260, 630. ISBN 1558607927. 
[3]  B. Briscoe et. al. Policing Congestion Response in an Internetwork using 
Re-feedback. In Proc. SIGCOMM 2005.  
[4]  Datacenter TCP, http://www.stanford.edu/~alizade/Site/DCTCP.htm 
[5]  Nandita Dukkipati. RCP: Congestion Control to Make Flows Complete 
Quickly. PhD Thesis, Department of Electrical Engineering, Stanford 
University, October 2006. 
[6]  D. Ferrari, A. Banerjea, and H. Zhang. Network support for multimedia: 
A discussion of the tenet approach. In Proc. Computer Networks and 
ISDN Systems, 1994. 
[7]  S. Floyd. RFC 3649: HighSpeed TCP for large congestion windows. 
[8]  S. Floyd and V. Jacobson. Random early detection gateways for 
congestion avoidance. IEEE/ACM Transactions on Networking, 
1(4):397‚Äì413, 1993. 
[9]  S. Floyd and V. Jacobson. The synchronization of periodic routing 
messages. IEEE/ACM Transactions on Networking, 2(2):122-136, 1994. 
[10] R. GrifÔ¨Åth, Y. Chen, J. Liu, A. Joseph, and R. Katz. Understanding TCP 
incast throughput collapse in datacenter networks. In WREN Workshop, 
2009. 
[11] Y. Gu, D. Towsley, C. Hollot, and H. Zhang. Congestion control for 
small buffer high bandwidth networks. In Proc. INFOCOM, 2007. 
[12] Urs Hoelzle, Jeffrey Dean, and Luiz Andr√© Barroso. Web Search for A 
Planet: The Architecture of the Google Cluster, In IEEE Micro Magazine, 
April 2003. 
[13] T. HoÔ¨Ä. Latency is Everywhere and it Costs You Sales - How to Crush it, 
July 2009. http://highscalability.com/blog/2009/7/25/ 
latency-iseverywhere-and-it-costs-you-sales-how-to-crush-it.html. 
[14] S. Iyer et. al. Analysis of a memory architecture for fast packet buffers. In 
IEEE HPSR Workshop, 2001. 
[15] D. Katabi, M. Handley, and C. Rohrs. Congestion Control for High 
Bandwidth-Delay Product Networks. In Proc. SIGCOMM, 2002. 
[16] Sam Liang and David Cheriton. TCP-RTM: Using TCP for Real Time 
Applications. In Proc. ICNP, 2002. 
[17] C. L. Liu and J. W. Layland. Scheduling Algorithms for 
Multiprogramming in a Hard-Real-Time Environment. Journal of the 
ACM, 20(1), 1973. 
[18] D. Meisner, C. M. Sadler, L. A. Barroso, W. Weber and T. F. Wenisch. 
Power Management of Online Data-Intensive Services. In Proc. ISCA, 
June 2011. 
[19] The ns-3 discrete-event network simulator. http://www.nsnam.org/ 
[20] R. Pan, B. Prabhakar, and A. Laxmikantha. QCN: Quantized congestion 
notiÔ¨Åcation an overview.   http://www.ieee802.org/1/ 
Ô¨Åles/public/docs2007/au_prabhakar_qcn_overview_geneva.pdf  
[21] K. Ramakrishnan, S. Floyd, and D. Black. RFC 3168: The addition of 
explicit congestion notiÔ¨Åcation (ECN) to IP. 
[22] I. R. Sangtae Ha and L. Xu. Cubic: A new TCP-friendly high-speed TCP 
variant. In Proc. SIGOPS-OSR, 2008. 
[23] V. Tsaoussidis and C. Zhang. 2002. TCP-Real: receiver-oriented 
congestion control. The International Journal of Computer and 
Telecommunications Networking. 40(4), 2002. 
[24] V. Vasudevan et al. Safe and effective Ô¨Åne-grained TCP retransmissions 
for datacenter communication. In Proc. SIGCOMM, 2009. 
[25] C. Wilson, H. Ballani, T. Karagiannis, A. Rowstron. Better Never Than 
Late: Meeting Deadlines in Datacenter Networks. In Proc. SIGCOMM, 
2011. 
[26] C. Wilson, H. Ballani, T. Karagiannis, and A. Rowstron. Better never 
than late: Meeting deadlines in datacenter networks.  Technical Report 
MSR-TR-2011-66, Microsoft Research, May 2011. 
[27] Y. Xia, L. Subramanian, I. Stoica, and S. Kalyanaraman. One more bit is 
enough. In Proc. SIGCOMM, 2005.