### Event Order and Timer Operator in Esper

In the context of event ordering, it is established that the right-hand expression is evaluated only after the left-hand expression evaluates to true. The `timer:interval` operator specifies the duration of the time window during which arriving events are observed (it starts once the left-hand expression becomes true). The value of the counter, along with the event name, is sent to the Esper Runtime. Listing 1.3 demonstrates the EPL (Event Processing Language) translation of Rule#1 in a multi-user scenario.

**Listing 1.3. EPL Rule in the Multi-User Scenario**

```epl
@name('Rule#1')
select * from pattern [every a = Event(name="compute_reserve_block_device_name") 
-> (timer:interval(secondsToWait seconds) and not b=Event(name="compute_attach_volume", countEvent = a.countEvent))];
```

Every time the Esper Runtime observes an event with its counter value, it waits for the receipt of another event with the same counter value within a time window of `secondsToWait` seconds. If this condition is not met, the system generates a failure detection message.

### Preliminary Experiments

To initially evaluate our approach, we conducted a series of fault injection experiments on the OpenStack platform. Our setup targeted OpenStack version 3.12.1 (release Pike), deployed on Intel Xeon servers (E5-2630L v3 @ 1.80GHz) with 16 GB RAM, 150 GB of disk storage, and Linux CentOS v7.0, connected through a Gigabit Ethernet LAN. Specifically, our tool [10] injected the following types of faults:

- **Throw Exception:** An exception is raised on a method call, according to a pre-defined list of exceptions per API.
- **Wrong Return Value:** A method returns an incorrect value, where the returned value is corrupted based on its data type (e.g., replacing an object reference with a null reference or an integer value with a negative one).
- **Wrong Parameter Value:** A method is called with an incorrect input parameter, where the input parameters are corrupted based on their data type.
- **Delay:** A method is blocked for a long time before returning a result, potentially triggering timeout mechanisms or causing a stall.

Before each experiment, we clean up any residual effects from the previous experiment to ensure that the failure is solely due to the current injected fault. This involves re-deploying the cloud management system, removing all temporary files and processes, and restoring the OpenStack database to its initial state.

### Workload Generator and Assertion Checks

Between calls to service APIs, our workload generator performs assertion checks on the status of virtual resources to reveal failures in the cloud management system. These checks assess the connectivity of instances through SSH and query the OpenStack API to ensure that the status of instances, volumes, and the network is consistent with the expected test outcomes. In our methodology, these assertion checks serve as the ground truth for evaluating the occurrence of failures during the experiments.

### Evaluation Metrics

We evaluated our approach in terms of failure detection coverage (FDC), defined as the number of experiments identified as failed over the total number of experiments that experienced a failure. We focused on the 481 faulty traces, one for each fault-injection experiment. An experiment is considered failed if at least one API call returns an error (API error) or if there is at least one assertion check failure. We also concentrated on cases where the target system was unable to timely notify the failure (i.e., notified with a long delay or not at all), as described in our previous work [12].

The FDC provided by our runtime verification approach is compared with the FDC provided by OpenStack API Errors. Table 1 shows the FDC of both approaches for different failure cases. Our approach identifies failures in 79.38% of the cases, significantly outperforming the OpenStack failure coverage mechanism. Notably, our rules can identify failures that were never notified by the system, such as Instance Creation and SSH Connection. The RV approach shows lower performance only in the Volume Creation case, suggesting the need for additional monitoring rules or improvements to existing ones for this specific case.

**Table 1. Comparison with API Errors Coverage**

| Failure Case          | OpenStack FDC (%) | RV FDC (%) |
|-----------------------|-------------------|-------------|
| Volume Creation       | 29.67             | 28.57       |
| Volume Attachment     | 25.33             | 92.00       |
| Volume Deletion       | 100               | 100         |
| Instance Creation     | 0.00              | 90.96       |
| SSH Connection        | 0.00              | 38.46       |
| Total                 | 23.96             | 79.38       |

### Multi-User Scenario

We also evaluated our approach in a simulated multi-user scenario. To simulate concurrent requests, 10 traces (5 fault-free and 5 faulty) were "mixed-together" by alternating the events of all traces without changing the relative order of events within each trace. The faulty traces were related to the same failure type (e.g., Volume Creation). For each failure type, we performed the analysis 30 times by randomly selecting both fault-free and faulty traces. Table 2 shows the average FDC and standard deviation for all failure volume cases. The preliminary results are promising, but the high standard deviation indicates that the average FDC is sensitive to the randomness of the analyzed traces.

**Table 2. Average FDC in the Multi-User Scenario**

| Failure Case          | Avg FDC (%) ± Std Dev |
|-----------------------|-----------------------|
| Volume Creation       | 32.00 ± 12.42         |
| Volume Attachment     | 45.33 ± 13.82         |
| Volume Deletion       | 36.00 ± 12.20         |
| Total                 | 37.78 ± 13.88         |

### Conclusion and Future Work

In this paper, we propose an approach to runtime verification via stream processing in cloud computing infrastructures. We applied this approach to the OpenStack cloud computing platform, demonstrating its feasibility in a large and complex distributed system. Our preliminary evaluation in fault-injection experiments showed promising results in both single-user and simulated multi-user scenarios.

Future work includes developing algorithms to automatically identify patterns using statistical analysis techniques, such as invariant analysis. We also aim to conduct fault-injection campaigns with a multi-tenant workload to evaluate the approach in a real multi-user scenario and analyze the overhead introduced by the approach.

### Acknowledgements

This work has been supported by the COSMIC project, U-GOV 000010–PRD-2017-S-RUSSO 001 001.

### References

[References listed here as in the original text]

---

This revised version aims to provide a clearer, more coherent, and professional presentation of the content.