a
c
h
e
d
(a)
)
%
(
d
a
e
h
r
e
v
O
e
v
i
t
l
a
e
R
10
0
-10
-20
-30
h
a
k
b
c
k
b
e
u
il
d
n
c
h
A
M
m
u
h
p
a
c
h
y
S
Q
e
m
c
n
t
a
r
e
L
a
c
h
e
d
k
b
u
il
d
a
c
k
b
e
n
c
h
(b)
Figure 11: Performance overhead for real applications in
UP (left) and SMP (right) VMs. The data on the left bar
shows the relative overhead compared to a vanilla Xen hy-
pervisor (Lower is better). In the right ﬁgure, the bar below
the line (with zero overhead) represents that CloudVisor-D
outperforms the vanilla hypervisor while the bar above the
line means that CloudVisor-D is slower than the vanilla.
a lot for the memcached benchmark. Speciﬁcally, it im-
proves the performance of CloudVisor-D for about 78.32%
(27.05%+51.27%).
Experiment
Time (seconds)
Speedup
VM exits
Vanilla Xen
7.613
0
1,691,758
CloudVisor-D−
11.516
-51.27%
4,572,269
CloudVisor-D
5.554
27.05%
63,909
Table 6: The performance impact of remote calls on
memcached. CloudVisor-D− means the guest does not in-
voke do_event_channel_op remote call while CloudVisor-D
means the guest uses this remote call.
The reason for this speedup is as follows: memcached is
a multi-threaded application and has no problem saturating
many cores. In an SMP VM, one vCPU frequently sends
virtual IPIs to another vCPU, which is implemented by the
event channel mechanism. With the help of the remote calls,
CloudVisor-D reduces numerous VM exits caused by invok-
ing do_event_channel_op hypercall, resulting in much less
unnecessary scheduling. Moreover, a vCPU will not send any
virtual IPI if it detects the target vCPU is not idle, which
further avoids VM exits caused by virtual IPIs. We found
that CloudVisor-D decreases the number of VM exits from
1,882,098 to 60,921 compared to the vanilla Xen hypervisor,
as shown in Table 6. Therefore, memcached in CloudVisor-D
achieves better performance than that in the vanilla Xen.
Overheads of a Guardian-VM. Each tenant VM only re-
quires one Guardian-VM, which is not a complete VM but
only a few service handlers. A Guardian-VM is invoked on
demand. It introduces only 108KB memory for one vCPU
(116KB for two vCPUs), costs at most 3.39% CPU cycles
when running real-world apps used in our paper.
8.5 I/O Performance
To answer Q4, we studied how CloudVisor-D behaved in
the worst-case I/O scenario by using dbench v4.0 [1]. dbench
is a widely-used I/O-intensive benchmark. In our evaluation,
the sysstat [11] tool reveals that I/O activities (including ﬁle
system time and waiting for the block device) account for
87.99% of the total workload time. Figure 9(a) demonstrates
the result of I/O performance overhead on dbench in a UP
VM by changing the number of concurrent clients. When the
number of concurrent clients is smaller than 20, the through-
put does not reach its limit which is approximately 710 MB/s.
The overhead for storage I/O is smaller than 5% for all cases.
Since dbench is a worst-case I/O scenario benchmark, the
result demonstrates that even in the worst case, CloudVisor-
D can provide acceptable I/O performance. The I/O perfor-
mance in an SMP VM is similar to that in a UP VM, as shown
in Figure 9(b). CloudVisor-D achieves negligible overhead
across different concurrency levels.
8.6 Performance of Multiple VMs
Finally, to answer the scalability question (Q5), we demon-
strated how CloudVisor-D performs by running kbuild un-
der the different numbers of VMs. Figure 10 shows the per-
formance overhead of concurrently running kbuild on the
different number of VMs. All these VMs are protected by
USENIX Association
29th USENIX Security Symposium    1707
CloudVisor-D. The result is an average value of 10 runs.
Each VM has one vCPU, 512MB memory and one 15GB
virtual disk. In CloudVisor-D, most VM operations are del-
egated to the Guardian-VMs and each guest VM has its
own Guardian-VM, which is not shared by others. Therefore,
CloudVisor-D incurs negligible overhead on multiple VMs.
Considering the small overhead of this experiment, the worse
performance in the case of 2 VMs could be attributed to run-
time variation.
8.7 Security Evaluation
According to the CVE analysis for the Xen hypervisor
in Nexen [54], the consequences of different attacks can
be classiﬁed into DoS (we do not consider this), privileged
code execution, information leakage, and memory corrup-
tion. CloudVisor-D can be used as a last line of defense such
that it does not directly ﬁx security vulnerabilities but instead
prevents exploitation of them from having harmful effects.
We conducted two experiments to show that CloudVisor-
D can protect guest VMs against memory writes (or reads)
from the malicious SubVisor, which is usually the ultimate
goal of many attack means. In the ﬁrst experiment, the mali-
cious SubVisor tries to read or write one VM’s memory page.
The guest reserves one page and then the malicious SubVisor
modiﬁes the page. This attack succeeds in the vanilla Xen
but fails in CloudVisor-D in which any access to the VM’s
memory triggers one EPT violation caught by the RootVisor.
In the second experiment, the malicious SubVisor modiﬁes
the VM’s EPT, maps one code page into the VM’s physical
memory space and maps the page into the VM’s virtual space.
Similar to the previous attack, this one succeeds in the vanilla
Xen but fails in CloudVisor-D.
We also conducted two more experiments to show that
the Guardian-VM can defeat the malicious EPT switching
attack. First, we simulated a malicious VM that bypasses
the Guardian-VM and executes code in the SubVisor. The
VM installs a malicious page table whose base address value
identical to that used in the SubVisor and then invokes a
VMFUNC to switch to the SubVisor-EPT directly. However,
since the target EPTP entry is 0 in the EPTP list, this attack
fails when the VM invokes the VMFUNC instruction that
triggers one VM exit. In the second attack, the malicious VM
leverages the four steps (Section 4.3) to jump to the middle
of the Guardian-VM. But the attack fails when it tries to con-
ﬁgure the malicious page table which triggers one VE. The
Guardian-VM then notiﬁes the RootVisor to terminate the
VM.
9 Discussion
VMFUNC and Virtualization Exception in Modern
Hypervisors. Modern hypervisors (e.g., Xen and KVM)
have already used the VMFUNC instructions and virtualiza-
tion exception (VE) in various use cases. The ﬁrst typical
use case for using VMFUNC and VE is to monitor VM be-
haviors [9] (Virtual Machine Introspection, VMI) and track
memory accesses by restricting the type of access the VM
can perform on memory pages. Once the monitored VM vio-
lates the memory permission conﬁgured in its EPT, one VE
triggers a handler which then uses a VMFUNC instruction to
switch to a monitoring application’s EPT. Another use case
of VMFUNC and VE is to boosting network function virtu-
alization (NFV) [3]. In NFV, each network function resides
in a different VM. NFV heavily depends on inter-VM com-
munications. To boost the NFV communication, one network
function uses the VMFUNC instruction to switch to an alter-
nate EPT and directly copy network data to another VM’s
memory. These use cases do not conﬂict with CloudVisor-
D because CloudVisor-D only occupies 3 EPTP entries in
the EPTP list, leaving 509 free entries for other usages, like
boosting VMI and NFV.
Directly Assigned PCIe Devices. The current version of
CloudVisor-D provides no support for SR-IOV devices. For-
tunately, many cloud providers disabled SR-IOV devices due
to the incompatibility with live VM migration. However, the
design of CloudVisor-D can be extended to protect VMs
if using directly assigned PCIe devices and SR-IOV. First,
the RootVisor leverages the IOMMMU to limit the physical
space each assigned device can access. The physical func-
tion of the SubVisor is limited by the IOMMU page table as
well, which means it cannot freely access other VMs’ spaces.
Second, before writing data into the assigned device, a guest
OS should invoke a helper function in its Guardian-VM to
encrypt the data. For reading data, the guest OS ﬁrst issues
a DMA request to move encrypted data from the device to a
private memory buffer, and then invokes a helper function in
the Guardian-VM to decrypt the data.
10 Related Work
Hardware-based Secure Computation: Secure archi-
tectures have been extensively studied during the last
decades [21, 37, 38, 40, 41, 43, 45, 51, 55, 59, 59, 60, 67–71].
Besides, different mainstream processor manufacturers re-
cently presented their products that support memory encryp-
tion. AMD (SEV [32]) and Intel (SGX [13, 47]) have pre-
sented their memory encryption products to the market re-
spectively. Researches proposed to leverage Intel SGX to
shield software [15, 19, 24, 28, 52, 57] or harden the SGX
itself [53, 56]. Haven [19] and SCONE [15] use SGX to
defend applications and weakly isolated container processes
from software and hardware attacks. Ryoan [28] provides an
SGX-based distributed sandbox to protect their sensitive data
in data-processing services. M2R [24] and VC3 [52] allow
users to run distributed MapReduce in the cloud while keep-
ing their data and code secret.
Defending against Untrusted Hypervisor: Many studies
have considered how to defend guest VMs against possibly
untrusted hypervisor. One prominent solution is to leverage
architectural support to remove the hypervisor out of TCB.
For example, H-SVM [31] modiﬁes hardware to intercept
1708    29th USENIX Security Symposium
USENIX Association
each Nested Page Table (NPT) update from the hypervisor
to guarantee the conﬁdentiality and integrity of the guest
VM. HyperWall [60] forbids the hypervisor from accessing
the guest’s memory by modifying the processor and MMU.
Another approach is to decompose the hypervisor and move
most of its part to the non-privileged level. NOVA [58] pro-
poses a microkernel-like hypervisor. Xoar [22] decomposes
the Dom0 into nine different service VMs to achieve stronger
isolation and smaller attack surface. Similarly, Nexen [54] de-
constructs Xen hypervisor into a shared privileged security
monitor and several non-privileged service slices to thwart
vulnerabilities in Xen. HyperLock [64] and DeHype [66] iso-
late the hypervisor from the host OSs. HypSec [38] leverages
the ARM virtualization extension and TrustZone technique
to decompose a monolithic hypervisor into a small trusted
corevisor and a big untrusted hypervisor, which effectively
reduces the TCB.
Even though we also propose a disaggregated design,
CloudVisor-D is different from the previous solutions in
three ways. First, CloudVisor-D separates the tiny nested hy-
pervisor, not the commodity hypervisor which has been to-
tally excluded out of the TCB. Second, while previous solu-
tions require intensive modiﬁcations to the commodity hyper-
visor, CloudVisor-D makes much fewer modiﬁcations (less
than 100 LOC) to the commercial hypervisor and is com-
pletely compatible with it. Finally, CloudVisor-D utilizes
new x86 hardware features to efﬁciently and securely con-
nect the isolated parts, which boosts the nested virtualization
in the x86 architecture.
Researchers also proposed to leverage the same privilege
protection for untrusted hypervisor, to harden the hypervisor
itself by measuring integrity [17] or enforcing control-ﬂow
integrity [63] of the hypervisor. However, these approaches
are best effort ones and do not exclude the commodity hyper-
visor out of the TCB.
Nested Virtualization: Traditional nested virtualization [20]
uses “trap and emulate” model to capture any trap of
the guest and forward it to the hypervisor for processing.
CloudVisor-D puts frequent normal VM operations to an
agent in non-root mode to replace the heavy “trap and emu-
late”. Different from turtles project [20], CloudVisor [72] dis-
trusts the hypervisor and prohibits it from accessing security-
sensitive data of guest VMs. Since nested virtualization
technology incurs unacceptable overheads, Dichotomy [65]
presents the ephemeral virtualization to reduce this overhead,
but it does not intend to defend against the malicious hyper-
visor.
VMFUNC-based Systems: Even though there are some pre-
vious researches that leverage VMFUNC to implement user-
level memory isolation [27, 35, 44] or efﬁcient communica-
tion facilities [26, 39, 49], all these systems assume that a
malicious VMFUNC user cannot modify the CR3 register,
which is not the case in CloudVisor-D. We propose a new
variant of the malicious EPT switching attack and a series
of techniques to defeat it. Furthermore, CloudVisor-D is the
ﬁrst design to utilize this hardware feature to build a disaggre-
gated nested hypervisor to defend VMs against an untrusted
hypervisor efﬁciently.
11 Conclusions
CloudVisor-D is a disaggregated system that protects vir-
tual machines from a malicious hypervisor. It leverages
nested virtualization to deprivilege the Xen hypervisor and
ofﬂoads most VM operations to secure Guardian-VMs with-
out the intervention of the tiny nested hypervisor (RootVisor).
CloudVisor-D has been implemented for Xen-based systems
and introduces negligible overhead.
12 Acknowledgments
We sincerely thank our shepherd Vasileios Kemerlis and
all the anonymous reviewers who have reviewed this paper in
the past two years. We also would like to thank Xinran Wang,
Weiwen Tang, Ruifeng Liu, and Yutao Liu. This work was
supported in part by the National Key Research & Develop-
ment Program (No. 2016YFB1000104), the National Natu-
ral Science Foundation of China (No. 61525204, 61772335),
and research grants from Huawei and SenseTime Corpora-
tion. Haibing Guan is the corresponding author.
References
[1] Dbench ﬁlesystem benchmark. https://www.samba.
org/ftp/tridge/dbench/.
[2] Intel 64 and ia-32 architectures software developer’s
manual volume 3c. https://software.intel.com/