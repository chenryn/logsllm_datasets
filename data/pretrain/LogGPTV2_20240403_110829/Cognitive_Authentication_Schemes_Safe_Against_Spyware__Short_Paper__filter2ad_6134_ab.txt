### 2.3.1. The Model

Let \( h \) denote an \( N \)-dimensional trinary vector representing a consistent hypothesis about the user’s secret. Each index \( i \) in this vector corresponds to an item in the set \( B \); the value of \( h_i \) is 1 if the corresponding item is in the secret set \( F \), 0 if it is not, and -1 if its affiliation is unknown (i.e., it can be either 0 or 1). To aid the adversary in constructing consistent hypotheses, we assume that the adversary knows the correct answer for each observed query.

#### Brute-Force Attack
The complexity of the brute-force attack against a protocol is denoted by \( \tilde{H} = \binom{N}{M} \), which represents the number of all vectors \( h \) with exactly \( M \) 1's and the rest 0's. 

All the protocols described in this paper are not secure against a very powerful adversary capable of mounting a brute-force attack. Note that the first observed query and its correct answer reduce the number of possible secrets by a factor of \( P \). After \( k \) observations, the number of viable secrets is reduced to \( \frac{1}{P^k} \) of the original space. Thus, if an adversary can maintain a list of all possible \( \tilde{H} \) solution vectors \( h \), this elimination process will converge quickly to a single answer, revealing the secret. Therefore, we must choose \( N \) and \( M \) large enough so that \( \tilde{H} \) exceeds the resources available to the most powerful adversary.

#### Enumeration Attack
Let \( H_t \) denote the set of all possible different hypotheses consistent with the observed data at time \( t \), i.e., after \( t \) observations of queries and their correct answers. Let \( \hat{H} = \max_t |H_t| \). \( \hat{H} \) represents the complexity of an enumeration attack, which maintains a list of all consistent hypotheses remaining at time \( t \). Clearly, \( \hat{H} < 3^N \). However, typically in our protocols, \( \hat{H} < \tilde{H} \) because the answer to each query depends on only a fraction of the bits in the secret.

To understand why this is true, consider the enumeration attack more closely. This attack works by expanding and maintaining the set of all consistent hypotheses, rather than starting from the set of all possible secrets and eliminating inconsistent ones. Initially, before any observation, the set \( H_0 \) includes a single element: an \( N \)-dimensional vector with all -1s. As the number of observations increases, \( H_t \) is modified to include all vectors consistent with the old and new observations. The size of this set initially increases exponentially but eventually starts to shrink due to contradictions, until a single element remains. The remaining vector must be a binary vector representing the user’s secret, where each item in the secret is assigned 1, and each of the remaining items is assigned 0. The complexity of the enumeration attack (time and space) is thus \( \hat{H} = \max_t |H_t| \).

### 2.3.2. High Complexity Query

We use simulations to estimate \( \hat{H} \) for the high-complexity protocol based on sampling \( H_t \) (exact calculation for interesting values of \( N \) and \( M \) is not computationally feasible). The results are shown in Table 1 for a range of parameters, illustrating various trade-offs. The set of parameters corresponding to our user study appears in the first row. In this case, our protocol achieves only moderate security by current cryptographic standards. However, the attacker must observe multiple successful authentication entries before searching the space of \( 2^{47} \) possibilities. The second case (row 2) shows parameters achieving a higher level of security, consistent with what is currently considered secure for encrypted passwords.

| \( N \) | \( M \) | \( P \) | Query Size | # bits \( \hat{H} \) | # bits \( \tilde{H} \) |
|--------|---------|--------|------------|-------------------|---------------------|
| 80     | 120     | 4      | 8 × 10     | 47                | 84                  |
| 95     | 145     | 8      | 8 × 10     | 47                | 114                 |
| 30     | 50      | 40     | 4 × 5      | 47                | 89                  |
| 55     | 135     | 50     | 4 × 5      | 73                | 135                 |

The last two cases show the effect of two manipulations that can shorten the authentication process: increasing the number of choices in the multiple-choice question (row 3) or simplifying the query by decreasing the number of pictures shown in the grid (row 4). Both cases achieve the same level of security (47 bits) as in row 1.

### 3. User Study

We tested the two types of protocols in a user study spanning up to 6 months for the high-complexity query and roughly 1 year for the low-complexity query. The goal was to evaluate password memorability and the protocols' ease of use.

#### 3.1. High Complexity Query

Nine undergraduate students from the faculty of natural sciences (none of whom were computer science students), all in their mid-20s, participated in the study and were paid a fixed amount per session. All participants underwent two or three training sessions over three consecutive days. With a few exceptions, all participants took approximately 15-20 seconds to conclude a query.

After training, participants went through a sequence of experimental sessions, each including 24 queries, following this schedule: 1 day, 2 days, 5 days, 1 week, and then roughly once a week for 10 weeks. Some participants agreed to additional sessions over the next 4 months. Participants received feedback on their final answers, leading to high memory retention and self-correction. Some errors were due to lapses in concentration. Results for all participants are summarized in Figure 2.

#### 3.2. Low Complexity Query

Two volunteers, aged 25-35 years, participated in the study. Both underwent three training sessions over three consecutive days. With a few exceptions, both participants took approximately 5 seconds to conclude a query. After training, participants went through a sequence of experimental sessions, each including 20-25 queries. They did not receive feedback. Results are summarized in Figure 3. One participant maintained perfect performance after one year, despite large gaps (around 3 months) between later test sessions.

### 3.3. Length of Authentication

In our user study, all participants demonstrated a success rate of over 95% per query. Many participants had perfect memory retention throughout the study, with one participant performing perfectly almost one year after initial training.

The number \( k \) of queries needed to conclude an authentication session successfully depends on the security threshold \( T \). For a modest security threshold of \( T = 10^{-6} \) (a guessing adversary will succeed once in a million trials) and a 95% correct answer rate, the average number of queries per successful entry is approximately 11 for the high-complexity protocol (where the chance of guessing is 1 in 4) and 22 for the low-complexity protocol (where the chance of guessing is 0.5).

In the settings used in our user study, a typical entry takes just over 3 minutes with the high-complexity protocol (each 4-choice query takes 15-20 seconds) and just over 1.5 minutes with the low-complexity protocol (each binary query takes roughly 5 seconds). If lower security is acceptable, both protocols can be further shortened by allowing more bits to be revealed in each query, e.g., by increasing the number of choices in the multiple-choice query.

### 4. Conclusions and Discussion

We have described challenge-response authentication protocols that rely solely on the user’s natural cognitive abilities, unassisted by external computation devices. A moderately powered observer who records any feasible number of successful authentication sessions cannot recover the user’s secret using brute-force or enumeration methods. Thus, the protocols appear to be safe against eavesdropping without relying on encryption. The main drawbacks are the need for training in a secure location and the time required for authentication, involving a series of challenges. An interesting property is the ability to trade off authentication time with security, asking many questions only when high security is needed or during an attack.

Our user study has implications for the usability of graphical passwords. It demonstrates that, given training, participants can learn and retain an arbitrary machine-generated set of pictures for a long period, which can be used as a regular graphical password with sufficiently high entropy.

### References

[1] D. Davis, F. Monrose, and M. K. Reiter. On user choice in graphical password schemes. In Proc. 13th USENIX Security Symposium, 2004.

[2] R. Dhamija and A. Perrig. Deja vu: A user study using images for authentication. In Proc. 9th USENIX Security Symposium, 2000.

[3] N. J. Hopper and M. Blum. Secure human identification protocols. In Proc. Advances in Cryptology, 2001.

[4] S. Li and H.-Y. Shum. Secure human-computer identification (interface) systems against peeping attacks: Sechci. Cryptology ePrint Archive, Report 2005/268, 2005.

[5] T. Matsumoto. Human-computer cryptography: an attempt. In Proc. Conf. on Computer and communications security, pages 68–75, 1996.

[6] X. Suo, Y. Zhu, and G. S. Owen. Graphical passwords: A survey. In Proc. 21st Annual Computer Security Applications Conference, 2005.

[7] J. Thorpe and P. van Oorschot. Graphical dictionaries and the memorable space of graphical passwords. In Proc. 13th USENIX Security Symposium, 2004.

[8] D. Weinshall. Cognitive authentication schemes for unassisted humans, safe against spyware, HUJI TR 2006-5.

[9] D. Weinshall and S. Kirkpatrick. Passwords you’ll never forget, but can’t recall. In Proc. Conf. on Computer Human Interfaces, 2004.