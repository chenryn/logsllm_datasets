# Characterizing the Security of Endogenous and Exogenous Desktop Application Network Flows

**Authors:**
- Matthew R. McNiece
- Ruidan Li
- Bradley Reaves

## Abstract
Most desktop applications rely on network communications, and insecure transmissions can significantly impact the application, system, user, and enterprise. Assessing whether these applications use the network securely at scale is challenging due to the limited availability of application-specific network data at centralized collection points. In this study, we analyze flow data from 39,758 MacOS devices in an enterprise network to examine the network behaviors of individual applications. By collecting flows locally on each device, we can accurately identify the application responsible for every flow. We also develop methods to distinguish "endogenous" flows, which are common to most executions of a program, from "exogenous" flows, which are likely caused by unique inputs. Our findings indicate that popular MacOS applications generally use the network securely, with 95.62% of the studied applications employing HTTPS. However, we observe that security-sensitive services, such as certificate management and mobile device management, do not always use secure communication ports. This study provides valuable insights for users, device and network administrators, and researchers interested in secure communication.

## 1. Introduction
Desktop applications frequently establish network connections to fetch content, verify licenses, or save resources. While encrypted communications were once rare, recent reports indicate that 95% of Chrome connections on Mac platforms now use HTTPS [1]. However, studies on TLS adoption have primarily focused on web browsers, neglecting the vast majority of non-browser applications. Measuring per-application network usage requires a vantage point that can map network traffic to the originating application. At a small scale, tools like personal firewalls can help individuals monitor their applications' network activity, but they are limited to a single device. Network-layer telemetry tools, such as middleboxes, provide a broader view but lose the definitive context of the application responsible for the flow.

In this paper, we bridge these perspectives by combining local, detailed observations of application network activity with visibility across a large population of 39,758 MacOS devices in an enterprise setting. At scale, we face challenges in distinguishing between user-triggered and application-native network activities. For example, a word processor may check for updates while simultaneously loading embedded content from a document. The software developer is only responsible for one of these network uses, so differentiating these behaviors is crucial for evaluating the security of the application. Our approach allows us to study a large population of hosts, providing a comprehensive view of application behavior. These insights can help administrators and incident responders understand expected application behaviors on managed devices and assist developers in understanding the full scope of their applications' network connections. Our work makes the following contributions:
- We perform the first large-scale study of application network behavior on desktop applications.
- We develop and evaluate a technique to differentiate user-triggered and application-endogenous behavior.
- We examine listening ports, the reputation of over 282,000 domains, and over three billion connections to assess the attack surface of 143 desktop applications.
- We investigate popular applications, such as Microsoft Office and MacOS daemons, that do not entirely use secure communication channels.

## 2. Methods
### 2.1 Data Collection and Characterization
#### Application-Labeled Flows (Phase 1b)
Our primary dataset is telemetry from Cisco AnyConnect VPN's Network Visibility Module (NVM) [3], which records all network traffic from a host along with the associated process. The NVM records include source and destination IP addresses, ports, flow size and duration, and the name and SHA256 hash of the process binary. We used one day's telemetry collected from NVM on a large enterprise network in September 2020, comprising approximately 320 GB of compressed JSON. This dataset includes records from 39,758 hosts, 143 unique applications, and 3,211,451,385 total flows. Two main challenges with this data are:
1. Bidirectional flows without indication of origin.
2. Lack of domain names for destination IPs, addressed using passive DNS data.

#### Application and OS Configuration (Phase 1a)
We used OSQuery [4] to gather information about the state of a device, its installed applications, and recent network activity. Our dataset contains query results from 35,678 managed endpoints on a single day in September 2020.

#### Passive DNS (Phase 3)
At each recursive DNS resolver on the network, a passive collector records logs of DNS queries and responses. This data, collected from the same 24-hour window as the application-labeled flows, consists of 9.5 billion query/response pairs, approximately 115 GB of compressed Parquet [5]. We use this data to perform reverse DNS lookups to recover the domains of observed IPs in the flow metadata.

#### Additional Sources of Enrichment (Phase 4)
We use the Snort IP Blocklist [6] and a paid commercial feed of spam-related domains for maliciousness indicators. The Umbrella Top 1 Million domains list [7] and the Umbrella Investigate Risk Score [8] serve as sources of domain reputation.

### 2.2 Data Preparation and Preprocessing
#### Anonymization (Phase 2)
The raw flow and configuration data contain human-identifying information. All such fields, except for the machine name, were removed. The machine name was encrypted, allowing us to track flows associated with the same device and calculate the number of unique devices sharing specific traits.

#### Passive DNS (Phase 3)
We used passively observed DNS data to generate lookups for recovering domain names from IP addresses. The A, AAAA records and rdata, rrname pairs were used, bypassing the anonymization step. This key-value pair enriches observed IP addresses with all observed domains that resolved to that IP.

#### Application and OS Configuration (Phase 1a)
We used a single query [9] to obtain a snapshot of all active listening ports on the system and the process that owns them.

#### Application-Labeled Flows (Phase 1b)
We performed three lightweight operations on the data before fusing it: removing NULL records, adding a label with the OS family (Windows or MacOS), and using RFC 1918 [10] to label each source and destination address as "Private" or "Public."

### 2.3 Enrichment (Phase 4)
We fused application-labeled flows with the observed ip:domain pairs from passive DNS to recover domains from the flow metadata. Although we could not match exact pDNS queries to hosts due to inconsistent identifiers, the pDNS data was collected from the same network within the same 24-hour period. When we observe the same IP on the same network within a time window, we have high confidence that it resolved to that domain. In cases where a single IP resolves to multiple domains, we report all matched domains. Enriching the flow data with DNS lookups required a join between 3.2 billion flows and 2.9 billion ip:domain pairs, which posed a significant challenge due to skewed joins. This process took multiple iterations and approximately 1,000 CPU hours.

#### Traffic Direction
NVM records symmetrical flow metadata but does not indicate the origin of the connection. We did not use ports as an indicator of remote or local origin to avoid bias in port usage analysis. Using RFC 1918, we developed a heuristic to determine traffic direction, labeling each flow as Internal, Outbound, Inbound, or NAT. We hypothesized that most traffic would be Outbound, following a client-server pattern. Our analysis confirmed that 81.9% of traffic was Outbound, supporting our hypothesis. We restricted our analysis to Outbound traffic, which represents more than 80% of the traffic and is the most interpretable.

### 2.4 Filtering (Phase 5)
We restricted our dataset in three ways:
1. We limited our analysis to MacOS applications, as we had more devices running MacOS in the population.
2. We considered only traffic from applications installed on more than 5% of the device population.
3. We focused on outbound connections, i.e., a device communicating with a remote server.
4. We analyzed only those connections that were common across installations of the same application.

This filtering allowed us to focus on the most relevant and representative data.