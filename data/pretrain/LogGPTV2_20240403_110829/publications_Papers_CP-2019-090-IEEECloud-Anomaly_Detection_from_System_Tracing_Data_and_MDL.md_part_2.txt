### Optimized Text

#### A. Data Modalities and Design Changes
- **Event ID (ID)**: Unique identifier for each event.
- **Parent ID**: The event ID of the parent service.
- **Protocol**: Can be either HTTP or a function protocol.
- **Host IP, HTTP Return Code, HTTP URL**: Additional metadata for each event.
- **Response Time**: The time difference between the start and stop of the service execution.
- **Timestamp**: The time at which the service was invoked.

To enhance the model's capability, we introduce the response time as a second data modality. This addition necessitates design changes to accommodate the new type of data in the training process.

#### B. Single-Modality LSTM Network
We denote the set of all traces recorded within a period as \( T = \{T_0, T_1, \ldots, T_{N_t}\} \), where \( L = \{l_0, l_1, \ldots, l_{N_l}\} \) is the set of unique labels in the data. The one-hot encoding value of the label \( l_j \in L \) positioned at index \( i \) in the trace \( T_k \) is denoted by \( e_i \).

Traces can vary in length and structure depending on the executed actions. For example, two sample traces \( T_p = \{e_{p0}, e_{p1}, e_{p2}, \ldots, e_{pi}\} \) and \( T_q = \{e_{q0}, e_{q2}, e_{q1}, \ldots, e_{qi}\} \) may differ in structure (e.g., \( e_{p1} \) and \( e_{p2} \) are swapped) but originate from the same system activity. This behavior is common in real systems where events \( e_{p1} \) and \( e_{q2} \) may originate from concurrently invoked services.

The order of events depends on the concurrent invocation of the recording. We model the structural anomaly detection in traces as a sequence-to-sequence, multi-class classification problem, where each distinct label represents a class.

**Figure 2: Single-Modality LSTM Network Architecture**

| Time | SAD Output | RTAD Output |
|------|------------|-------------|
| 0    | 0, 0, … 1, … 0 | 0.24 |
| 1    | 0, 1, … 0, … 0 | 0.16 |
| 2    | 0, 0, … 0, … 1 | 0.xy |
| ...  | ...        | ...         |
| Tl   | 1, 0, … 0, … 0 | 0 |

**LSTM Layers:**
- Each LSTM block at time \( i \) is composed of multiple LSTM cells with a memory state that encodes information from previous timesteps and the current input.
- The LSTM uses different types of trainable gates to decide:
  - How much of the previous cell state \( C_{i-1} \) should be retained.
  - How to use the current input and the previous output \( H_{i-1} \) to influence the state.
  - How to construct the output \( H_i \).

**Training:**
- The input to the model is the event labels from a trace \( T_k = \{e_0, e_1, \ldots, e_{T_l}\} \).
- Each \( e_i \) is fed as input at the corresponding timestep \( i \).
- The output at time \( i \) is a probability distribution over the \( N_l \) labels, representing the probability of the next label in the sequence.
- The LSTM network is trained to maximize the probability of each \( e_i \) (for \( i \in \{1, 2, \ldots, T_l\} \)) to appear as the next label.
- The model is trained using categorical cross-entropy loss minimization via gradient descent.

**Detection:**
- During the detection phase, the model compares the predicted output against the observed label value.
- If the observed label is not among the top-k most probable labels, an anomaly is reported.

#### C. Multimodal LSTM
The response time and the event's label together completely characterize a single event. The correlation between these two types of data motivates the need to use both modalities in a single model to extend the anomaly detection for tracing data and achieve better overall accuracy.

**Figure 3: Multimodal LSTM Neural Network Architecture**

| Time | SAD Input | RTAD Input | SAD Output | RTAD Output |
|------|-----------|------------|------------|-------------|
| 0    | 0, 0, … 1, … 0 | 0.24 | 0, 0, … 1, … 0 | 0.3 |
| 1    | 0, 1, … 0, … 0 | 0.5 | 0, 1, … 0, … 0 | 0.24 |
| 2    | 0, 0, … 0, … 1 | 0.3 | 0, 0, … 0, … 1 | 0.5 |
| ...  | ...       | ...        | ...        | ...         |
| Tl   | 1, 0, … 0, … 0 | 0 | 1, 0, … 0, … 0 | 0.3 |

**Architecture:**
- The model contains two data modalities as inputs: \( D_1 \) (Structural Anomaly Detection, SAD) and \( D_2 \) (Response Time Anomaly Detection, RTAD).
- From the bottom-up perspective, the architecture has a layer with LSTM blocks for each input.
- The concatenation of the two modalities is performed in the second hidden layer, where the LSTM outputs from the first layer at time \( i \) are joined and forwarded to the next LSTM layer.

**Training:**
- Different cost functions are used for the modalities: mean squared error for the response time and categorical cross-entropy for the labels.
- The input-output pairs are used to learn the weight updates through minimization of a joint loss.

**Detection:**
- The detection in the multimodal setting compares the output element-wise with the input for both modalities.
- Anomalies are reported if the computed error between the prediction and the input is out of the confidence interval or if the observed input labels are not among the top-k predictions.

#### D. Detection of Dependent and Concurrent Events
The output of the structured anomaly detection model encodes the underlying execution path. Each label predicted as the next event in the trace describes a probability distribution of all possible labels. The causal relationship between events (parent-child relationship) can be extracted from the recorded data.

**Concurrent Events:**
- To determine if two events are produced by concurrently invoked services, we analyze two different inputs: \( \{l_1, l_3, l_8, l_12, l_6\} \) and \( \{l_1, l_3, l_8, l_12, l_11\} \).
- If both sequences lead to the same next event with probability \( p_2 = 1.0 \), the events (services) are concurrent.

**Dependent Events:**
- If the probability of observing \( l_k \) as the next label in the sequence \( e_{i+1} \) is 1.0, given the input \( \{e_0, e_1, \ldots, e_i\} \), then \( e_i \) and \( e_{i+1} \) are dependent.

#### E. Experimental Setup
The data was collected from a production cloud platform running OpenStack with Zipkin as a tracing technology. The system includes over 1000 microservices, providing an exhaustive and realistic evaluation environment. The traces were recorded over a period of 50 days, yielding over 4.5 million events distributed in more than one million traces of varying lengths.

**Data Processing:**
- JSON objects representing the events were parsed, and the two different data modalities \( D_1 \) and \( D_2 \) were compiled.
- Labels appearing more than 1000 times were selected, resulting in 105 unique labels.
- To balance the dataset, only 1000 samples of each trace length were selected, with lengths between 4 and 20. Traces with larger lengths were considered outliers and excluded from the training dataset.

**Model Performance:**
- The performance of the models is evaluated based on their ability to capture normal execution paths and detect anomalies.
- The multimodal LSTM model shows robustness and efficiency, requiring less than 1% of the recorded data for training.

This optimized text provides a clear, coherent, and professional description of the data modalities, LSTM network architectures, and the experimental setup.