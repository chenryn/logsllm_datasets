ID (unique event identifier), parent ID (event ID of the
using only the labels, motivate the use of the response time
parent service)
as a second data modality, and describe the design changes
â€¢ protocol (can be either HTTP or function protocol), host needed to train models with this type of data.
IP, HTTP return code, HTTP URL
â€¢ response time (time difference between start and stop of B. Single-Modality LSTM network
the execution of the service) and timestamp (when the We denote the set of all traces recorded within a period
particular service was invoked) as T = {T ,T ,...,T }, while L = {l ,l ,...,l } is
set 0 1 Nt 0 1 Nl
Dependingontheexecutedaction,thetracescanhavedifferent the set of unique labels in the data. Furthermore, e denotes
i
lengths and events representing various services. Two sample the one-hot encoding value of the label l âˆˆ L positioned at
j
traces T ={ep,ep,ep,...,ep} and T ={eq,eq,eq,...,eq} index i in the trace T for k âˆˆ {0,1,...,N }. The value of
p 0 1 2 i q 0 2 1 i k t
are different in structure (e and e are swapped), but orig- e depends on the trace structure prior to e , as the events
1 2 i i
inate from the same system activity. This type of behavior in a trace originate from the execution of services upon a
occurs frequently in real systems, where the events ep and user request, where the events have a parent-child causal
1
eq originate from concurrently invoked services. The order of relationship [16]. We model the structural anomaly detection
2
events depends on the concurrent invocation of the recording in traces as a sequence-to-sequence, multi-class classification
problem, where each distinct label represents a class. Figure RTAD 0.24 0.16 0.xy â€¦ 0
2 shows a single-modality architecture for both types of data output
ğ‘¡ğ‘–ğ‘šğ‘’=0 ğ‘¡ğ‘–ğ‘šğ‘’=1 ğ‘¡ğ‘–ğ‘šğ‘’=2 ğ‘¡ğ‘–ğ‘šğ‘’=ğ‘‡ğ‘™
D 1 (Structural Anomaly Detection, SAD) and D 2 (Response SAD 0, 1, â€¦ 0, â€¦ 0 0, 0, â€¦ 0, â€¦ 1 0, 0, â€¦ 1, â€¦ 0 â€¦ 1, 0, â€¦ 0, â€¦ 0
TimeAnomalyDetection,RTAD).Thedetectionofanomalies output
from a sequence of labels enables to capture the errors during
the execution as well as to detect unexpected execution paths. LSTM LSTM LSTM â€¦ LSTM
The input to the model are the event labels from a trace
â€¦ Sâ€¦tack up laâ€¦yers â€¦
T = {e ,e ,...,e }. Each e is fed as input in the corre-
k 0 1 Tl i
sponding timestep time=i. The output at time=i, for the
LSTM LSTM LSTM â€¦ LSTM
current inputs {e ,e ,...,e }, is a probability distribution
uniqu0 ela1 belsfroi mâˆ’1 ğ»0 ğ»1 ğ»2 ğ»ğ‘¡
overtheN L,representingtheprobability
l â€¦
forthenextlabele inthesequence.Thedetectionphaseuses LSTM LSTM LSTM LSTM
makei ğ»0,ğ¶0 ğ»1,ğ¶1 ğ»ğ‘¡,ğ¶ğ‘¡
this model to a prediction and compares the predicted
outputagainsttheobservedlabelvalue.TheLSTMnetworkis SAD 0, 0, â€¦ 1, â€¦ 0 0, 1, â€¦ 0, â€¦ 0 0, 0, â€¦ 0, â€¦ 1 â€¦ 0, 0, â€¦ 1, â€¦ 0
input
trained to maximize the probability of each e i (iâˆˆ{1,2..T l}) ğ‘¡ğ‘–ğ‘šğ‘’=0 ğ‘¡ğ‘–ğ‘šğ‘’=1 ğ‘¡ğ‘–ğ‘šğ‘’=2 ğ‘¡ğ‘–ğ‘šğ‘’=ğ‘‡ğ‘™
to appear as a next label. Every LSTM block in Figure 2 at RTAD 0.12 0.24 0.16 â€¦ 0.72
input
time=iiscomposedofhLSTMcells.Ithasamemorystate
thatencodesalloftheinformationfromtheprevioustimesteps Fig.2. Single-modalityLSTMnetworkarchitecture
together with the input fed at the same timestep. LSTMs use
different types of trainable gates [9], which together with the
the input label at time = i and the output from the previous label can be a measure for the performance of the model in
block H are used to decide: termsoftheabilitytocapturethenormalexecutionpaths.For
i1
robustness, we compare the observed label to the k predicted
â€¢ how much of the previous cell state C i1 should retain in
most probable labels: if the input label observed in the next
its own state,
timestep from the original sequence is not one of the top-
â€¢ howtousethecurrentinputandthepreviousoutputH iâˆ’1
k labels with the maximum probability, then an anomaly is
to influence the state, and
reported, i.e. the trained network has not observed a normal
â€¢ how to construct the output H i.
trace with the same or similar structure. Along with the
In this manner, the possible extracted non-linear and temporal
information that the trace contains an anomalous execution
information is passed between adjacent LSTM blocks.
path, we also provide the user with information which events
The stacking of layers as shown in Figure 2 is a common
contributed to the decision. This enables a better root-cause
practiceinordertoachievebetterresultsbyextractinghighly-
analysis.
abstractfeatures.Weformulateeachinput-outputpairfromD
1 1) Response Time Anomaly Detection (RTAD): The re-
as
sponse time characterizing intra-service calls is decisive for
T ={e ,e ,...e }â†’T ={e ,e ,...e ,â€™!0â€™}} the anomaly detection, as a sudden increase may indicate
input 0 1 Tl output 1 2 Tl
a problem with the involved service or with the underlying
where the output is shifted by one event and concluded with
distributed system. Unfortunately, the response time values
the â€™!0â€™ label. These pairs are used to incrementally update
grouped as a time-series exhibit a low signal-to-noise ra-
the networkâ€™s weights through categorical cross-entropy loss
tio and typically include multiple frequencies, distributions,
minimization via gradient descent.
and concept drifts. The low signal-to-noise ratio is a result
Detection. In order to evaluate, if a trace T test represents of the different components affecting the response time of
an anomaly and to discover, which events support the deci- microservices such as switches, routers, memory capacity,
sion, T test = {e 0,e 1,...,e Tl} is clamped to the networkâ€™s CPU performance, programming languages, thread/process
input. In each timestep, the network calculates a probability concurrency, bugs, and volume of user requests. Moreover,
distribution: the dependencies between the events in a trace affect the
response times. Assume a service A calls other service B,
P ={l :p ,l :p ,...,l :p }
0 0 1 1 Nl Nl
collects the result, and proceeds with the execution (parent-
describing the probability for each label from L to appear as childrelationship).Inthiscase,thesecondserviceisaservice-
the next label value in the trace, given the previous values. child.Therefore,anincreaseordecreaseinthechildâ€™sresponse
The output layer of the network is composed of a soft-max time will correspondingly lead to a change in the parentâ€™s
function. It distributes the probability over the labels and response time. The ability to detect specific events that are
(cid:80)Nlp
ensures that =1. anomalous in such perspective provides more insights and
i i
Previously, we described that two or more events can be extends the range of the anomaly detection from the tracing
a result of multiple concurrent actions; therefore few of the data.
possible labels can appear as the next label in the sequence. Thetrainingofneuralnetworksrequiresnormalizedvalues.
Comparison of the input label only to the most probable When grouped by labels, the response times from different
ğ‘¡ğ‘–ğ‘šğ‘’=1 ğ‘¡ğ‘–ğ‘šğ‘’=2 ğ‘¡ğ‘–ğ‘šğ‘’=ğ‘‡ğ‘™ end of sequence
0, 1, â€¦ 0, â€¦ 0 0.24 0, 0, â€¦ 0, â€¦ 1 0.5 0, 0, â€¦ 1, â€¦ 0 0.3 1, 0, â€¦ 0, â€¦ 0 0
LSTM LSTM LSTM â€¦ LSTM
â€¦ â€¦ â€¦ â€¦
LSTM LSTM LSTM â€¦ LSTM
ğ»2
ğ»0 ğ»1
ğ»ğ‘¡
LSTM LSTM LSTM â€¦ LSTM concatenation LSTM LSTM LSTM â€¦ LSTM
ğ»0,ğ¶0 ğ»1,ğ¶1 ğ»2,ğ¶2 ğ»ğ‘¡,ğ¶ğ‘¡
0, 0, â€¦ 1, â€¦ 0 0, 1, â€¦ 0, â€¦ 0 0, 0, â€¦ 0, â€¦ 1 0, 0, â€¦ 1, â€¦ 0 0.3 0.24 0.5 0.3
ğ‘¡ğ‘–ğ‘šğ‘’=0 ğ‘¡ğ‘–ğ‘šğ‘’=1 ğ‘¡ğ‘–ğ‘šğ‘’=2 ğ‘¡ğ‘–ğ‘šğ‘’=ğ‘‡ğ‘™ ğ‘¡ğ‘–ğ‘šğ‘’=0 ğ‘¡ğ‘–ğ‘šğ‘’=1 ğ‘¡ğ‘–ğ‘šğ‘’=2 ğ‘¡ğ‘–ğ‘šğ‘’=ğ‘‡ğ‘™
Fig.3. MultimodalLSTMneuralnetworkarchitectureforanomalydetectionfromcompletetracingdata
services will be even more similar in terms of values. There- the same timestep in the next LSTM layer. In Figure 3, the
fore, learning a model only from the response times without color-coding scheme represents the concatenated pairs.
coupling with the labels of the services is a hard task and Starting from the concatenated layer, the information is
leads to a poor performance. However, in order to extend the jointly encoded and flows between modalities. From this
single-modalityarchitecturetothemultimodalLSTM,weneed joint representation, the many-to-many neural network learns
to describe how to model the response time independently of the mapping to the outputs. The input-output pairs can be
thelabels.Therefore,wereusetheneuralnetworkarchitecture formalized as:
presentedinFigure2andmodeltheinter-event,responsetime
dependenciesinatraceusingRTAD(D )data.Thedifference
2
input=[{e ,e ,...,e },{rt ,rt ,...,rt }]
is that instead of the multi-class classification, the task is 0 1 Tl 0 1 Tl
regressionwheretheinputandtheoutputarereal-valuednum-
â†’output=[{e ,e ,...,e ,â€™!0â€™},{rt ,rt ,...,rt ,0}]
1 2 Tl 1 2 Tl
bers. In each timestep time = i, with {rt ,rt ,...,rt },
0 1 iâˆ’1
the network predicts the response time rt for the next event. In the training phase, we use different cost functions for
i
In contrast to SAD, the weight updates are obtained through the modalities. The input-output pairs are used to learn the
minimization of the mean squared loss via gradient descent. weight updates through minimization of a joint loss which is
Thedetectioniscarriedoutbycomputingthesquarederror the sum of the mean squared error for the response time and
distance error = (rt âˆ’ rtp)2, where rtp is the predicted the categorical cross-entropy for the labels.
i i i i
value at timestep time = i. Furthermore, we separately Detection. The detection in the multimodal setting is per-
model the error values for each label by fitting the Gaussian formed by comparing the output element-wise with the input
distribution and produce a trace T predicted by the model. for both modalities using the strategy developed in the single-
test
If the squared error between the prediction and the input at modality architectures. An advantage is that with a single
time=i is out of the 95% confidence interval obtained from modelwecandetectanomaliesinbothmodalities,overcoming
theGaussian,theeventandthetraceareflaggedasanomalous. the limitations of not having a single-modality response time
anomaly detection. For example, when there is an increased
C. Multimodal LSTM
response time in one of the events, and the computed error
Theresponsetimetogetherwiththeeventâ€™slabelcompletely between the prediction and the input is out of the confidence
characterizesasingleevent.Thecorrelationbetweenthesetwo interval, the method reports an anomaly. Similarly to the
types of data motivates the need to use both modalities of the SAD, if the observed input labels of the sequence are not in
datainasinglemodelaimingtoextendtheanomalydetection the top-k element-wise predictions, an anomaly is reported.
for tracing data and to achieve a better overall accuracy. Considering the both modalities, the anomaly type can be
The proposed multimodal LSTM architecture is assembled either:responsetimeanomaly,structuralanomaly,oranomaly
as a horizontal concatenation of both single-modality archi- in both modalities. It is worth noting that the anomaly in the
tectures, as shown in Figure 3. The model contains two response time can be independent of the structure and vice
data modalities as inputs: D and D . From the bottom-up versa.
1 2
perspective of the architecture, we have layer with LSTM
D. Detection of Dependent and Concurrent Events
blocks for each input. We perform the concatenation in the
second hidden layer. The concatenation can be carried out in The output of structured anomaly detection model encodes
any hidden layer chosen by cross-validation. The merging of the underlying execution path. Every label predicted as the
thetwomodalitiesisdoneinthefollowingway:LSTMoutputs label for the next event in the trace describes a probability
from the first layer in time=i are joined and forwarded into distribution of all possible labels. As described before, the
eventshaveaparent-childrelationship,whichcanbeextracted A. Experimental Setup
fromtherecordeddata.However,thechallengeofrecognition The data was collected from a production cloud platform
of concurrent and dependent events in the execution path is which runs Openstack [19] with Zipkin [16] as a tracing
not solved yet and has to be addressed. technology. With more than 1000 micro services, the under-
Considering that the neural network learns the underly- lying system enables an exhaustive and realistic evaluation
ing execution paths (i.e. the causal relationship between the of our approach. To the best of our knowledge, there is
events), we can reconstruct them using the predicted prob- no publicly available data set containing distributed traces,
ability distribution over the labels, similar to the approach whereas Openstack log data can be easily generated using
presented in Figure 4. Assume we aim to predict the event CloudLab[20].Thecollectedtracesarerecordedoveraperiod
in the trace next to the vertical, dashed line. The input at of50days,yieldingover4.5millioneventsdistributedinmore
timestep time=3 is {l 1,l 3,l 8,l 12}. Let the top-2 predictions than one million traces with different lengths.
forthenextlabelinthesequencebe{p 6 =0.55,p 11 =0.45}, The JSON objects representing the events, are parsed and
while for the rest of them, the probability is zero. In order to the two different data modalities D and D are compiled.
1 2
determine if these two events are produced as a result from In order to avoid outliers, we select labels that appear more
concurrentlyinvokedservicesweanalyzetwodifferentinputs: than 1000 times in the data, making a total of 105 unique
{l 1,l 3,l 8,l 12,l 6}and{l 1,l 3,l 8,l 12,l 11}.Ifforbothsequences labels. The distribution of the trace lengths in our dataset is
the probability for the next label is p 2 = 1.0, the events imbalanced;morethan90%ofthetraceshavelengthssmaller
(services) are concurrent as both of them lead to the same than 10 events. This is compensated by selecting only 1000
event. samples of each trace length, where the trace lengths with
Wealsoproposeamechanismtodetectdependentevents.In less samples are completely included into the dataset. In this
thiscase,iftheprobabilitytoobservel k asalabelforthenext regard, our approach requires approximately less than 1% of
eventinthesequencee i+1 isone,considering{e 0,e 1,...,ei} all the recorded data, which makes it efficient and fast for
as input, then e i and e i+1 are dependent. training. For robustness, we also select the traces with lengths
between 4 and 20. Traces with larger lengths appear only few
times in the given time-period. We consider them as outliers
and do not insert into the final training data set.
1.0 ['multimodal', 'dense'] 0.97% 0.95% 0.96% 0.95%
['multimodal', 'lstm']
['singletask', 'dense']