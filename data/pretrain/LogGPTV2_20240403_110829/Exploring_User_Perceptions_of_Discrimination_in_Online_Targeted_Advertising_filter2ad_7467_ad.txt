[0.70, 1.41]
[0.54, 0.95]
[0.67, 1.18]
[0.88, 1.55]
[0.97, 1.00]
[0.51, 1.17]
[0.53, 1.19]
[0.85, 3.28]
[0.66, 1.75]
[0.69, 2.14]
[1.46, 2.76]
p-value
0.400
0.983
0.019*
0.430
0.284
0.011*
0.216
0.271
0.140
0.764
0.502
 1 is associated with more responsi-
bility. See Table 10 caption for more detailed explanation.
Table 13: Regression results for local news site respon-
sibility (n=843), where OR > 1 is associated with more
responsibility. See Table 10 caption for more detailed
explanation.
Figure 7: Responses for scenario believability, broken
down into behavior and demographic conditions.
Figure 6: Agreement that each entity was behaving eth-
ically, broken down by the behavioral and demographic
conditions.
5.3.6 Believability
questions, the mechanism of targeting is signiﬁcant for
advertiser, ad network and local news website; in all three
cases, behavior-based targeting is signiﬁcantly correlated
with a lower perception of unethical behavior than the
demographic-based targeting. This is illustrated in Fig-
ure 6. Human vs. algorithmic decision making continues
to show no signiﬁcant eﬀect.
In contrast to responsibility, the entity making the de-
cision in the provided scenario (the ad network or ad-
vertiser) does not appear to have a signiﬁcant eﬀect on
respondents’ perceptions of ethical behavior in any case.
The targeted group is similarly uncorrelated.
Respondent demographics appear to have little to no
correlation with these results. In two cases (ad network
and local news site), Asian respondents were more likely
to disagree that the entity in question had behaved ethi-
cally, but no other demographic covariates were signiﬁ-
cant.
Because several of our cognitive interview respondents ex-
pressed skepticism that discriminatory scenarios like the
ones we described could be realistic, we added a question
about believability at the end of the survey. Respondents
were asked to rate the scenario on a ﬁve-point scale from
“deﬁnitely could not happen” to “deﬁnitely could happen.”
Overall, 88% of respondents reported that the scenario
“deﬁnitely” or “probably” could happen. Figure 7 pro-
vides an overview of respondents’ ratings of scenario
believability. This result suggests that, among the pop-
ulations we surveyed, there is widespread if potentially
shallow awareness of behavioral targeting capabilities and
the potential for discrimination, intentional or otherwise.
6 Limitations
Our study, like most similar surveys, has several important
limitations. First, while our sample included a broad vari-
ety of demographic groups, it was not a true probabilistic
USENIX Association
26th USENIX Security Symposium    945
0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% BehaviorDemog.BehaviorDemog.BehaviorDemog.BehaviorDemog.AdvertiserAd NetworkLocalUserStrongly AgreeAgreeNeutralDisagreeStrongly DisagreeFactor
T-Asian
T-Black
Behavior
Human
Advertiser
Age of respondent
HS+
BS+
R/E-Asian
R/E-Black
R/E-Hispanic or Latino
SSI
OR
0.99
0.84
1.34
1.05
1.30
0.97
0.75
0.73
1.96
1.66
1.31
2.40
CI
[0.69, 1.42]
[0.58, 1.20]
[1.00, 1.79]
[0.78, 1.41]
[0.97, 1.75]
[0.95, 0.98]
[0.49, 1.14]
[0.48, 1.10]
[1.03, 3.73]
[1.04, 2.67]
[0.76, 2.28]
[1.73, 3.32]
p-value
0.962
0.340
0.054
0.734
0.080
 1 is associated with more responsi-
bility. See Table 10 caption for more detailed explanation.
Table 15: Regression results for ethical behavior by the
ad network (n=891), where OR > 1 is associated with
stronger disagreement that the ad network behaved ethi-
cally. See Table 10 caption for more detailed explanation.
sample. While we believe our conclusions can to some
extent generalize, Turkers and web panel participants are
generally more active internet users than average. People
with less technical knowledge might ﬁnd our scenarios
less believable or feel diﬀerently about what constitutes a
severe problem.
Second, our surveys dealt with the highly sensitive
topic of discrimination, especially racial discrimination.
Social desirability bias may cause respondents to report
higher-than-realistic severity of discrimination scenarios,
particularly with respect to historically disadvantaged
groups.
Third, the ad eco-system is complex and complicated.
There are many diﬀerent entities involved in the publish-
ing of an ad. In this survey, we took some of the involved
entities and incorporated them into simpliﬁed scenarios.
Despite simpliﬁcation and pre-testing via cognitive inter-
views, it is possible some respondents did not understand
important subtleties of these scenarios, aﬀecting their re-
sponses. However, the fact that respondents tended to
most blame whichever entity was implicated by the sce-
nario (Section 5.3.4 suggests that respondents understood
the scenarios to at least some degree.
More generally, all self-report surveys are susceptible
to respondents who hurry through, answer haphazardly, or
do not think deeply about the questions. In this particular
survey, we were concerned that the scenarios might be
too complex for some participants to understand, or that
participants who did not believe the discriminatory sce-
nario might not answer meaningfully. To minimize these
eﬀects, we kept the survey short and used cognitive in-
terviews to ensure that our questions and answer choices
could be easily understood. We explicitly measured be-
lievability and found that the majority of participants did
ﬁnd our scenario plausible. In addition, our major results
proved consistent across two pilots and our main survey.
As a result, we are reasonably conﬁdent that respondents
were able to provide thoughtful answers to our questions.
Fourth, only some of our variables, the factors Target,
Mechanism, Entity, and Decider, were experimentally
randomized. Thus, our causal claims only extend to them.
For the other variables, the covariates Age, Ethnicity, Ed-
ucation, and Sample Provider, we can only makes claims
of correlation.
Fifth, despite basing our conclusions on three rounds
of data collection, false positives remain possible. Pi-
lot 2 produced a series of hypotheses about what variables
would constitute a useful, parsimonious logistic regres-
sion model that we could apply across the nine questions
we asked. The main study applied this model to new
data, and investigated how these variables were corre-
lated with each outcome. (For Decider, based on Pilot 2,
we anticipated a coeﬃcient statistically indistinguishable
from zero.) The main regressions also controlled for the
data coming from SSI, about which we had no explicit
hypotheses motivated by Pilot 2.
In our main study we consider each variable-question
combination as an independent hypothesis; we do not
aggregate across questions or variables. Intuitively, these
variables and questions are distinguishable from one an-
other and diﬀerent explanations may apply to each. As
such, we do not correct for multiple hypothesis testing.
Ultimately the question of when to aggregate and adjust
p-values or not to comes down judgements about the indi-
vidual hypotheses being interesting each on their own [20]
and the goals of the study [7].
946    26th USENIX Security Symposium
USENIX Association
Factor
T-Asian
T-Black
Behavior
Human
Advertiser
Age of respondent
HS+
BS+
R/E-Asian
R/E-Black
R/E-Hispanic or Latino
SSI
OR
0.94
1.02
0.46
0.91
1.42
1.00
0.97
0.76
2.76
1.07
1.94
0.83
CI
[0.60, 1.45]
[0.66, 1.59]
[0.32, 0.67]
[0.63, 1.30]
[0.99, 2.03]
[0.98, 1.01]
[0.57, 1.65]
[0.45, 1.27]
[0.95, 7.99]
[0.59, 1.95]
[0.84, 4.48]
[0.56, 1.24]
p-value
0.765
0.925
 1 is associated with
stronger disagreement that the advertiser behaved ethi-
cally. See Table 10 caption for more detailed explanation.
Table 17: Regression results for ethical behavior by the
local news site (n=891), where OR > 1 is associated with
stronger disagreement that the site behaved ethically. See
Table 10 caption for more detailed explanation.
7 Discussion and Conclusion
Below, we present a summary of our ﬁndings, discussion
on the respondents’ understanding, implications for gov-
ernance and policy guidelines for OBA, and suggestions
for future work.
7.1 Summary of Findings
Overall, we ﬁnd that for most questions we examined, peo-
ple’s perceptions of discriminatory ad-targeting scenarios
depend on how the discrimination occurred. As might be
expected, respondents rated scenarios in which the dis-
crimination occurred based on how users behaved, with
no explicit intent to discriminate based on demographic
characteristics, to be signiﬁcantly less problematic than
scenarios with explicit racial targeting. Respondents also
assigned more blame to the ad network, advertiser, and
host website, and rated these entities’ behavior as less
ethical, in the behavioral scenarios.
Respondents also found scenarios in which minorities
(in our scenarios, people of black or Asian race) beneﬁted
from such ad-targeting discrimination less problematic
than scenarios in which the majority beneﬁted. Relatedly,
we also ﬁnd that black respondents are more likely to
view discriminatory scenarios as a more severe problem.
We hypothesize that these ratings are inﬂuenced by dis-
criminatory history in the U.S., where we recruited our
respondents.
We ﬁnd that whether the ad network or advertiser is ex-
plicitly mentioned in the scenario as causing the discrim-
ination inﬂuences the accrual of responsibility to those
entities; however, to our surprise, the named entity did
not inﬂuence respondents’ ratings of the severity of the
scenarios, or of whether any entity had behaved ethically.
Overall, the median ethics rating for both the ad network
and the advertiser was neutral. We suspect this may relate
part to many respondents not entirely understanding some
subtleties of the online ad ecosystem. Nevertheless, these
results suggest that it is not necessarily helpful for entities
to “pass the blame” to other players, as the mechanism
of discrimination seems more important. We were also
surprised to ﬁnd that whether a person or an algorithm
was responsible for selecting how and whom to target
made no diﬀerence in respondents’ ratings of the severity
of the scenario, suggesting that “an algorithm did it” will
not be a viable excuse.
Finally, we ﬁnd that the majority (88%) of respon-
dents believed our scenario, suggesting a wariness or
even awareness of these issues, at least among heavily-
internet-using Turkers and SSI panel members.
7.2 Governance and Policy Implications
A number of organizations, including the FTC, the EFF,
and industry groups such as the American Advertising
Federation, provide guidelines and recommendations for