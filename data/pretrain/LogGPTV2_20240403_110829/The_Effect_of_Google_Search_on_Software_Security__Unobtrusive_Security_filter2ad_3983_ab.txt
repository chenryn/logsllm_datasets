neutral level of difficulty). The analysis of the query logs suggests
that these three categories belong to the most frequent search
activities. The study, however, remains at a very high level, and in
particular does not show whether developers are able to identify
secure search results, and whether their searches contribute to the
development of functional and secure code.
Our work builds on the existing literature of developer studies.
However, to the best of our knowledge, no study exists that focuses
on security search behavior, or investigates the impact of using
search engines on code security.
3 ONLINE STUDIES
We performed two different online studies: Security of Search Results
and Impact of Ranking on Code Security.
In Study 1 on Security of Search Results, 192 participants per-
formed code search on the Web using Google Search. We calculated
the distribution of Stack Overflow results that provide secure code
examples and the distribution of Stack Overflow results that pro-
vide insecure ones among the top ten results 𝑡10. Distributions were
calculated for a set of 274 distinct user queries 𝑄 and 3,800 related
search results 𝑅.
Further, we wanted to know whether the ranking of search
results actually has an impact on code security. In Study 2, 218
participants had to solve three security-related programming tasks
on the Impact of Ranking on Code Security with the help of Google
Search. Therefore, we tested two conditions. The control group
had to use the original Google Search engine that was used in
Study 1, while our treatment group used a different Google Search
engine that applies security-based re-ranking. We developed security-
based re-ranking in order to up-rank secure results and down-rank
insecure results. We explain this approach in Section 5.
Age
Stddev = 10.34/8.59
Country of Origin
Brazil/India = 13/18
Gender
Other = 6/1
Level of Education Achieved
Master = 44/60
Professional
Security Background
Min = 18
Max = 74/59
China/Brazil = 11/9 Other = 105/112
Female = 10/7
PhD = 18/11
Other = 20/24
N/A = 7/24
N/A = 7/29
N/A = 25/37
Mean = 32.03/30.94
Median = 29.5/29
USA = 40/60
Germany = 23/19
Male = 171/188
Prefer not to say = 5/22
High School = 30/21
Bachelor= 80/102
Yes = 133/161
Yes = 56/46
No = 52/33
No = 129/143
 2 years = 82/80
Java Primary Focus of Job
No = 158/144
Yes = 31/49
N/A = 3/25
Table 1: Detailed data about demographics of participants
for Study 1 (N = 192) and Study 2 (N = 218).
3.1 Recruitment
We recruited participants by contacting GitHub developers via
email. We extracted around 900,000 email addresses from public
GitHub user profiles. A randomly compiled list of 50,000 users was
contacted for Study 1. To incentivize users, ten Amazon vouchers
worth 50 USD were given away in a drawing for all participants. A
randomly selected list of 100,000 users was contacted for Study 2.
Here we offered no compensation. We attached an opt-out link to
Session 11C: Software Development and Analysis CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3072all of our study emails that (if used) deleted the recipient’s email
address from our database.
Participants were first directed to a landing page that introduced
the respective study, followed by a comprehensive consent form.
We avoided priming in the description of the study and tasks by not
mentioning any terms related to security or privacy. After complet-
ing the main task, participants were asked to complete a short exit
survey that explored the characteristics of the specific sample. In
Study 1, 192 out of 991 participants that entered the landing page
completed the survey. In Study 2, out of the 827 participants that
began the survey, 218 finished it.
3.2 Research Ethics
Our institution does not require IRB review for online survey stud-
ies. However, we closely followed the recruitment and remuneration
modalities of a GitHub developer study (discussed in the related
work section), which had received approval by the Ethics Review
Board of Saarland University, the IRB of the University of Maryland,
and the NIST Human Subjects Protection Office [4].
Survey studies with GitHub users as the survey population
are relatively common. Most prominently, the 2017 GitHub Open-
Source Survey provided detailed knowledge about the user pop-
ulation on the platform [19, 20]. In the security domain, we are
aware of one study using GitHub as the primary participant source
[4, 22]. In contrast, there are numerous peer-reviewed studies in
the domain of software engineering that survey GitHub devel-
opers to explore a variety of coding and development practices
[7, 23, 24, 28, 34, 36, 37, 40, 46, 52] as well as gender and diversity
issues [47, 48]. Most of those papers used email for recruitment.
3.3 Demographics, Experience, and Education
We collected demographic data about our study participants using
an exit survey and present them in Table 1. Participants were asked
about general information including country of origin, age, and
gender. In order to control our samples for programming expertise,
we asked for developer-specific information such as years of ex-
perience in Java and whether they were professionals. Lastly, we
collected security-related information such as security experience
and background.
Figure 1: Description of a sub-task in Study 1
4 STUDY 1: SECURITY OF SEARCH RESULTS
4.1 Tasks
In order to measure the distribution of insecure Stack Overflow
links in the top ten results 𝑡10, we performed an online study where
participants had to perform several queries using Google Search.
They were introduced into a scenario setting where they had to
solve hypothetical programming tasks on the topic of AES encryp-
tion. We followed our prior work (Fischer et al. [18]) in the design of
the tasks, which include initializing a symmetric cipher (CIPHER),
an initialization vector (IV), and a cryptographic key (KEY). Each
task had several sub-tasks, e.g., inform yourself about symmetric
encryption or look up how to use a specific API element. We show
a screenshot of a sub-task in Figure 1.
Participants were instructed to type a search query into the
provided search bar for each task in order to find help online. Next,
they had to select the result that appeared to be the most relevant
to solve the task. To decide relevance, participants could visit the
webpage of each result and make a final decision. Thereby, we
stored the search queries, the links from the results including their
rank, and a click log.
4.2 Setup
The study was performed on the participants’ own devices. The
search bar was run by a Google Custom Search Engine (CSE) that we
integrated into the online survey. The CSE API allows for different
types of search customization, including filtering and boosting
results. This way, we were able to store all inputs and outputs and
to control what was shown in the results.3 Results were not affected
by Google’s search personalization [25], since those services are
not supported by CSE.4
4.3 Ground Truth
To be able to determine the security of search results, we needed
ground truth about the security of code examples that are provided
by Stack Overflow webpages shown in the results. We used TUM-
Crypto,5 an open-source dataset which contains security labels for
the complete list of Java code examples on Stack Overflow that
provide potential solutions for our study tasks CIPHER, IV, and
KEY [18]. Each code example from the dataset provides a link to
the Stack Overflow webpage it was downloaded from. This allowed
us to label the security of Stack Overflow links that are relevant for
solving the tasks.
4.4 Security Labeling
We defined the webpage labeling function 𝑙𝑡1, which labeled a
Stack Overflow webpage as insecure if the top answer given on
the webpage contained insecure code. A webpage was labeled as
secure if otherwise. Measuring the distribution for 𝑙𝑡1 gave us a
realistic view on the distribution of results, that are more likely of
being reused in production code. Chen et al. [9] have shown that
code examples from the top answer are significantly more often
reused than examples with a lower score. That is probably due to
3We removed all ads that would have been shown to the user.
4https://support.google.com/programmable-search/answer/70392?hl=en
5https://github.com/fischerfel/TUM-Crypto
Session 11C: Software Development and Analysis CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3073Figure 2: Binomial distribution of secure (blue) and insecure (red) Stack Overflow results over the top ten results 𝑡10. The 𝑥-axis
represents the observed count of secure or insecure results in 𝑡10. The 𝑦-axis shows the probability for each observation, i. e.
there is a probability of 𝑦 that 𝑥 results are secure or insecure in 𝑡10.
the top answer the first one being seen by the user as it is shown
right below the question. Further, it oftentimes comes with a green
check mark that highlights the accepted answer by the user who
posted the question.
4.5 Results
The TUM-Crypto dataset contains 3,361 secure and 7,195 insecure
code examples, which are provided in 5,811 Stack Overflow dis-
cussion threads (i.e., webpages). For webpages that provide code
examples for CIPHER, IV, and KEY, we obtained 1,671 secure and
2,057 insecure ones after applying our webpage labeling function
𝑙𝑡1, described in Section 4.4. These webpages represent a subset of
the potential search results 𝑅.
Participants that typed a search query into our CSE obtained
search results from 𝑅 = {𝑐𝑠𝑒(𝑞)}𝑞∈𝑄, where 𝑄 is the complete set
of queries we tracked during the study and 𝑐𝑠𝑒 the search func-
tion implemented in our CSE. Note that 𝑅 provides all search re-
sults returned by the search engine and was not subject to any
modifications. However, only those results that could be linked to
TUM-Crypto obtained a security label.
formed 1.0 clicks per task.
On average, our 192 participants submitted 1.3 queries and per-
We obtained |𝑄| = 274 unique queries that were typed into the
CSE. We observed 98 unique queries for IV, 87 for KEY, and 89 for
CIPHER resulting into 1,290 top ten search results for IV, 1,210 for
KEY, and 1,300 for CIPHER.
Stack Overflow was the most clicked on domain with 32% of
clicks followed by official documentation websites for Java (docs.
oracle.com) with 26%, and Android (developer.android.com) with
10% of the clicks. Rank one and two were the most clicked on ranks
with both 33% of the clicks and the top three results received 79%
of the clicks made in total.
Binomial Distribution— Based on the collected queries 𝑄 and
results 𝑅 from our three tasks, we measured the probability of
secure and insecure Stack Overflow results to appear in the top
three (𝑡3), top five (𝑡5) and top ten (𝑡10) results.
We calculated the binomial distribution 𝐵(𝑛, 𝑝) of secure and
insecure results over 𝑡𝑛 ∈ 𝑅, where 𝑛 ∈ {3, 5, 10} is the number of
trials, i. e., the number of displayed results in 𝑡𝑛, and 𝑝 the success
probability of each trial, i. e., probability of a result to be secure or
insecure. The probability is given by 𝑝 = 𝑟/𝑠, where 𝑟 is either the
number of secure or insecure Stack Overflow results in 𝑡𝑛, and r
the total number of results in 𝑡𝑛. The distributions are calculated
simulating 10,000 searches.
Second, we calculated the task-specific distributions of secure
and insecure results over 𝑅𝑖𝑣 = {𝑐𝑠𝑒(𝑞)}𝑞∈𝑄𝑖𝑣 ⊂ 𝑡𝑛 where 𝑄𝑖𝑣 is the
set of queries entered into the search bar during the IV task. Like-
wise, we measured the distributions of secure and insecure results
over 𝑅𝑘𝑒𝑦 = {𝑐𝑠𝑒(𝑞)}𝑞∈𝑄𝑘𝑒𝑦 ⊂ 𝑡𝑛 and 𝑅𝑐𝑖𝑝 = {𝑐𝑠𝑒(𝑞)}𝑞∈𝑄𝑐𝑖𝑝 ⊂ 𝑡𝑛
where 𝑄𝑘𝑒𝑦, 𝑄𝑐𝑖𝑝 are the set of queries entered during the KEY and
CIPHER task.
Figure 2 shows the binomial distribution over aggregated results
from all three tasks, and over results from each individual task.
Task-Independent Results— Figure 2, (ALL, 𝑡10, blue), shows
the distribution 𝐵(10, 0.030) of secure results, while (ALL, 𝑡10, red)
shows the distribution 𝐵(10, 0.070) of insecure results over 𝑡10 for
the complete set of queries 𝑄. We observed a 28.17% chance for at
least one secure result to appear in 𝑡10 vs. a chance of 36.47% for
one insecure result. Probabilities of more than one result to appear
in 𝑡10 were also much higher for insecure results: 14.07% for two,
2.91% for three, and 0.32% for four vs. 5.22% for two and 0.64% for
three secure results to be found in 𝑡10.
Probabilities diverged even stronger in the top three results 𝑡3.
Here, we observed a chance of 22.78% for at least one insecure
result where a secure result has only a 9.19% chance. This means if
an insecure result ends up in 𝑡10 it is more likely that it obtains a
rank in the top three than in the lower ranks. Further, every fourth
Google Search query will probably show an insecure result in 𝑡3 if
the query is related to IV, KEY, or CIPHER.
IV— Figure 2, (IV, 𝑡10, blue) contrasts the distribution 𝐵(10, 0.004)
of secure results with the distribution 𝐵(10, 0.060) of insecure re-
sults shown in Figure 2, (IV, 𝑡10, red) for queries 𝑞 ∈ 𝑄𝑖𝑣.
We observed a 22.44% chance that at least one result in 𝑡10 is
secure vs. a 34.90% chance for one insecure result. While probabil-
ities were near 0% for a secure result to be contained in 𝑡5 or 𝑡3,
we observed 29.79% for 𝑡5 and 25.10% for 𝑡3 to provide at least one
insecure result. In contrast to secure results, insecure results also
had a chance for more than one result to be present in 𝑡10: 10.43%
for two, and a 1.65% for three results.
KEY— We observed a surprisingly high distribution of secure
results in 𝑡10 for queries 𝑞 ∈ 𝑄𝑘𝑒𝑦. There was a 37.32% chance for at
least one secure result (see Figure 2, (KEY, 𝑡10, blue)). The probability
of an insecure result was only slightly higher with 37.65%, as shown
in Figure 2, (KEY, 𝑡10, red).
Session 11C: Software Development and Analysis CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3074Probabilities of more than one result in 𝑡10 were also higher for
insecure results: 15.22% for two, and 3.47% for three vs. 13.99% for
two, and 2.94% for three secure results.
Once more, probabilities of one insecure result in 𝑡5 and 𝑡3 only
slightly decreased with 37.03% and 29.19%, respectively.
Even though we observed a lower probability of secure results in
𝑡10 again, the probability values do not differ as much as observed
in IV. However, probabilities diverged more significantly in 𝑡3 again,
where one insecure result has a chance of 29.19% and a secure result
19.48%.
CIPHER— For CIPHER, there was a 21.06% chance for at least
one secure result in 𝑡10 and a 2.71% chance for two results (see
Figure 2, (CIPHER, 𝑡10, blue)). We only observed a 3.26% chance for
one result in 𝑡3 to be secure.