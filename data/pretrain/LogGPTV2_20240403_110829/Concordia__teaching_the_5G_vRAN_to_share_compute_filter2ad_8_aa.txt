title:Concordia: teaching the 5G vRAN to share compute
author:Xenofon Foukas and
Bozidar Radunovic
Concordia: Teaching the 5G vRAN to Share Compute
Xenofon Foukas
Microsoft
Cambridge, United Kingdom
PI:EMAIL
Bozidar Radunovic
Microsoft
Cambridge, United Kingdom
PI:EMAIL
ABSTRACT
Virtualized Radio Access Network (vRAN) offers a cost-efficient
solution for running the 5G RAN as a virtualized network function
(VNF) on commodity hardware. The vRAN is more efficient than
traditional RANs, as it multiplexes several base station workloads on
the same compute hardware. Our measurements show that, whilst this
multiplexing provides efficiency gains, more than 50% of the CPU cy-
cles in typical vRAN settings still remain unused. A way to further im-
prove CPU utilization is to collocate the vRAN with general-purpose
workloads. However, to maintain performance, vRAN tasks have sub-
millisecond latency requirements that have to be met 99.999% of
times. We show that this is difficult to achieve with existing systems.
We propose Concordia, a userspace deadline scheduling framework
for the vRAN on Linux. Concordia builds prediction models using
quantile decision trees to predict the worst case execution times of
vRAN signal processing tasks.The Concordia scheduler is fast (runs
every 20 ùúás) and the prediction models are accurate, enabling the sys-
tem to reserve a minimum number of cores required for vRAN tasks,
leaving the rest for general-purpose workloads. We evaluate Concor-
dia on a commercial-grade reference vRAN platform. We show that
it meets the 99.999% reliability requirements and reclaims more than
70% of idle CPU cycles without affecting the RAN performance.
CCS CONCEPTS
‚Ä¢ Networks ‚Üí Mobile networks; Wireless access points, base sta-
tions and infrastructure; Network reliability; Cloud computing;
‚Ä¢ Computer systems organization ‚Üí Real-time systems; ‚Ä¢ Com-
puting methodologies ‚Üí Machine learning.
KEYWORDS
vRAN, 5G, mobile networks, edge computing, NFV, real-time sched-
uling, machine learning, prediction model
ACM Reference Format:
Xenofon Foukas and Bozidar Radunovic. 2021. Concordia: Teaching the 5G
vRAN to Share Compute. In ACM SIGCOMM 2021 Conference (SIGCOMM
‚Äô21), August 23‚Äì27, 2021, Virtual Event, USA. ACM, New York, NY, USA,
17 pages. https://doi.org/10.1145/3452296.3472894
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed for
profit or commercial advantage and that copies bear this notice and the full citation on
the first page. Copyrights for components of this work owned by others than ACM must
be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to
post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
Request permissions from permissions@acm.org.
SIGCOMM ‚Äô21, August 23‚Äì27, 2021, Virtual Event, USA
¬© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8383-7/21/08...$15.00
https://doi.org/10.1145/3452296.3472894
580
INTRODUCTION
1
The Radio Access Network (RAN) is a part of the cellular network
infrastructure that includes base stations (cells), and is responsible
for converting data packets into wireless radio waveforms and back.
It includes wireless physical layer operations that perform various
complex signal processing tasks. Conventional mobile base stations
include specialized hardware boxes called BaseBand processing
Units (BBUs), implementing the physical layer of each cell.
A forthcoming 5G trend is to extend network function virtualiza-
tion (NFV) to the radio access network (vRAN), to deploy RAN
workloads on commodity hardware. Many benefits of virtualization
apply to the vRAN: vendor lock-in mitigation, flexible upgrades,
rapid roll-out of new standards and services and a potential for cost
reduction. This trend is real and several operators have deployed
or are deploying vRANs [42, 84, 113, 114], creating a market for
hundreds of thousands of servers and millions of CPU cores [71],
that is projected to claim more than $3 billion by 2025 [95] and
more than $6 billion by 2030 [94]. A new cellular operator in Japan,
Rakuten, is running its entire vRAN for a national cellular network
on commodity hardware [19], and the new US green-field operator
Dish plans to do the same [34].
Unlike other virtualized network appliances, vRAN signal pro-
cessing algorithms are very compute intensive. Virtualized BBUs
running these algorithms can consume more than 60% of the overall
required compute resources of the vRAN [39, 107, 118]. Given the
huge scale of network deployments, it is important to reduce the
computational cost of virtualized BBUs.
A common way to increase vRAN efficiency is RAN pooling (or
BBU pooling), which involves sharing compute resources among sev-
eral cells. The virtualization and pooling of RAN tasks takes advan-
tage of the statistical multiplexing gains of cells [20, 104, 116]. For ex-
ample, one of the most demanding RAN tasks is decoding [72] and its
computational load is proportional to the wireless traffic volume. If a
RAN pool serves cells in both a residential and an office area, the peak
throughput (and thus the compute requirement) is likely to stay simi-
lar throughout the day, as users move between offices and homes [47].
However, existing RAN pooling schemes only leverage long-term
(e.g. diurnal) changes in traffic demand and other opportunities for
statistical multiplexing at much lower time scales are not yet explored.
For example, our measurements described in Section 2.2 look at a
RAN pool with 3 cells and show that the median traffic volume
per slot is 0.2KB. The 99th percentile traffic volume per slot is 2.5
KB, which is more than 10√ó larger than the median. As one needs to
provision the RAN pool compute capacity for the peak traffic, the pool
will be substantially underutilized for most of the time. Furthermore,
in a common example of a multi-cell 100MHz deployment configured
for time division multiplexing (discussed in Section 2.2), we see
more than 50% of the CPU cores assigned to the RAN pool being
left unutilized even at the peak cell traffic, due to the difference in the
SIGCOMM ‚Äô21, August 23‚Äì27, 2021, Virtual Event, USA
Xenofon Foukas and Bozidar Radunovic
computational demands of uplink and downlink signal processing.
We observe similar sharing opportunities in other common use cases.
One obvious way to mitigate such inefficiencies would be to de-
ploy other general-purpose (and best-effort) workloads on the CPUs
when they are not fully utilized by the RAN. One such example is
ML training and classification workloads (e.g. video analytics) that
have to run at the edge due to privacy concerns or due to low latency
requirements [58, 108, 119, 120]. Another example are local con-
tent caching and delivery workloads, deployed by third-parties or
by the operators, with the goal of minimizing the user latency and
reducing the backhaul traffic strain [11, 77, 102, 109]. Furthermore,
in the context of cellular networks, example workloads could include
network functions relevant to the higher layers of the RAN protocol
stack and the cellular core (e.g. CU control and data plane, 5G UPF),
as well as management and control functions (e.g. for vRAN orches-
tration, monitoring and analytics) [54, 77, 85, 100]. Collocating such
functions at the vRAN edge instead of using the hyperscale cloud
could be particularly beneficial in the context of private LTE/5G
networks, to enable cost-efficient and autonomous edge deployments
and/or to mitigate privacy concerns regarding the cellular data of
users [30, 31, 54, 77].
The collocation problem and its performance effects has been stud-
ied extensively in the literature (e.g. [13, 65, 67, 68, 75, 78, 81, 83]).
The particular challenge with RAN tasks is their very stringent timing
constraints ‚Äì with task deadlines in 10s or 100s of microseconds ‚Äì
where every deadline violation can cause a service degradation to
the end user ‚Äì imposing a standard requirement that deadlines have
to be met 99.999% of time. On the one hand, most of the existing
solutions have been designed with tail latencies that are insufficient
for the RAN [13, 65, 75, 81, 83]. On the other hand, solutions that do
provide microsecond level of control (e.g. [51]) require applications
to run in non-standard operating systems or using specific APIs, mak-
ing them incompatible with conventional workloads, like containers,
running alongside the RAN. As a result, and in order to mitigate the
problem of tail latency and achieve the desired RAN performance,
a standard practice is to isolate the RAN from other workloads as
much as possible by dedicating cores (c.f. [21, 74]) and effectively
waste idle CPU cycles.
To address this problem we built Concordia, a system that recovers
unused CPU cycles in vRAN pools for general workloads without
violating the strict timing requirements of vRAN pool tasks. The Con-
cordia design views the vRAN as the high priority workload, with a
maximum scheduling priority. All other workloads are considered as
best-effort and as such can be pre-empted by the vRAN at any point in
time. To achieve this, Concordia uses a userspace deadline scheduler
that leverages ideas from the mixed-criticality systems space [17].
The scheduler is fed with predictions of the worst-case execution
time (WCET) of each RAN task. It uses the predictions to calculate
and proactively reserve the least number of cores required to perform
the vRAN pool operation in the next slot (e.g. 1ms), releasing the
rest of the cores to the OS for other tasks. This is done at a 20 ùúás gran-
ularity, allowing Concordia to adjust the scheduling decision faster
than RAN traffic fluctuations and compensating for unpredictable OS
scheduling latencies that exist in a non real-time OS such as Linux.
A key requirement of Concordia is an accurate estimate of the
RAN tasks‚Äô WCETs. Predicting WCETs has been extensively studied
in the context of both hard and soft real-time systems [18, 111]. A
common assumption of such works is that each task can be character-
ized with a single WCET prediction value, without any parameteriza-
tion. In contrast, the runtime of a RAN task (and thus its WCET) can
vary significantly depending on several tens of input parameters (e.g.
relevant to the traffic load, cell configuration, etc), as quantified in
Section 4. We show that previous works, ignoring this parameteriza-
tion, lead to overly pessimistic scheduling and poor CPU utilization.
To overcome this limitation, Concordia proposes a novel ML-
based WCET parameterization and prediction method that is com-
posed of an offline and an online phase. During the offline phase
(vRAN deployed in isolation), Concordia constructs a quantile deci-
sion tree for each RAN task, classifying different WCET predictions
into leaf nodes depending on the tasks‚Äô input parameters to minimize
the variance of collected WCET samples in each leaf. The predictions
are further updated and improved online (in the presence of other
workloads), using a fast online approximation method that compen-
sates for the contention (e.g. on the cache) caused by the collocated
workloads. To our knowledge, this is the first work that provides such
a parametrized WCET prediction mechanism.
We implement Concordia on top of Intel FlexRAN v20.02 [48, 62],
a state-of-the-art 4G and 5G reference implementation that is used in
most of today‚Äôs commercial vRAN deployments (c.f. [84, 113, 114]).
We evaluate it by collocating various 5G RAN traffic workloads with
best-effort workloads that are representative of envisioned colloca-
tion scenarios (Nginx, Redis for content caching, SQL for cellular
core and content caching and MLPerf for ML training). We show
that we can recover up to 70% of unused CPU cycles while main-
taining the operational requirements of the RAN. To the best of our
knowledge, this is the first system that allows 5G vRAN to allow
other workloads to recover unused vRAN CPU cycles.
In summary, we make the following contributions:
(1) We design Concordia, a userspace vRAN task scheduling frame-
work that allows general purpose workloads to run in parallel without
affecting the RAN performance. The design leverages the observation
that the WCET of vRAN tasks can be predicted with high confidence
to estimate the required number of CPU cores. It also continuously
adapts its estimation to release unused cores for other workloads (¬ß 3).
(2) We develop a novel machine learning method for the parame-
terized prediction of the WCET of signal processing tasks using
quantile decision trees.The model further adapts its WCET predic-
tions at runtime to the observed RAN traffic load and the system-level
contention from other workloads (¬ß 4).
(3) We build Concordia based on the reference vRAN solution of In-
tel FlexRAN v20.02 [48, 62] (¬ß 5). Our evaluation on 5G vRAN cells
with realistic collocated workloads (¬ß 6) shows that Concordia can
provide 99.999% reliability in meeting RAN processing deadlines,
while reclaiming up to 70% of the CPU cores.
This work does not raise any ethical issues.
vRAN overview
2 BACKGROUND & MOTIVATION
2.1
vRAN operations and requirements: Radio transmissions and re-
ceptions in vRAN occur in regular Transmission Time Intervals
(TTIs) or slots. Depending on the cell configuration, a slot can last
between 62.5us and 1ms [1]. A set of signal processing tasks have
to be processed in each slot, starting at the beginning of the slot
581
Concordia: Teaching 5G vRAN to Share Compute
SIGCOMM ‚Äô21, August 23‚Äì27, 2021, Virtual Event, USA
Figure 1: Example of uplink signal processing tasks DAG for 5G NR.
Figure 2: High-level overview of vRAN pool design.
and having to finish by the end of the same or a subsequent slot
(depending on the implementation).
The dependencies of the signal processing tasks executed within
a slot can be described with Directed Acyclic Graphs (DAGs). Fig 1
illustrates such a (simplified) DAG for the case of 5G uplink, with
each shaded node corresponding to a different signal processing task
(see Appendix A.1 for a 5G downlink example and a brief description
of the most significant tasks). For example, LDPC decoding uses the
output of rate dematching and cannot start before the dematching has
finished. The exact DAG structure depends on various input param-
eters. There can be multiple active DAGs at any time (e.g. an Rx and
a Tx DAG, or DAGs from adjacent slots), and tasks from the same
DAG can run in parallel (e.g. multiple LDPC decoding operations
on different cores). These DAGs have deadlines and if a vRAN pool
fails to process a DAG by a given deadline, the packets transmitted
or received in the corresponding time slot are dropped. As some of
them can carry control information, the impact of a loss can also
affect a long term state of the user connection. For this reason, it is
standard practice to impose 99.999% of reliability [29, 112].
vRAN implementation: A typical vRAN implementation uses a
queue-based worker thread model (c.f. [75]) for processing signal
processing tasks (Fig 2). Such a design is used (in variations) in most
existing vRAN implementations, including Intel‚Äôs FlexRAN [48,
62], OpenAirInterface [43, 52] and Agora [28]. Here we describe
FlexRAN as a concrete example we use throughout the paper. The
vRAN pool is composed of a number of worker threads, each pinned
to a CPU core. Each signal processing task is assigned to a priority