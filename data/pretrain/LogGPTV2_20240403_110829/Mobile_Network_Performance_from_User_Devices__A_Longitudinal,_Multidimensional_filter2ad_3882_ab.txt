four major US carriers in three US regions where our dataset is densest (New York,
Seattle, and Bay Area). Each of these carriers exhibits different topologies (Internet
egress, Google ingress and ASes between) in different regions, potentially leading to
performance differences in each region.
16
A. Nikravesh et al.
Bay Area
Seattle
New York
 110
 100
 90
 80
 70
 60
 50
 40
 30
r
o
r
r
)
s
m
(
T
T
R
g
n
P
i
E
d
r
a
d
n
a
S
d
n
a
t
n
a
e
M
 20
Oct 30 Nov 19 Dec 09 Dec 29
Jan 18
Feb 07
Feb 27 Mar 18 Apr 07
Apr 27 May 17
Jun 06
Fig. 2. Verizon LTE Ping RTT in Different Locations
Time (Days) 2011-2012
Despite the variety in network topologies, we surprisingly ﬁnd that for AT&T, T-
Mobile, and Sprint, both of the latency and throughput were similar in these three lo-
cations. However, for Verizon, we observe different LTE performance in New York,
Seattle, and Bay Area. Fig. 2 plots these latencies over time, and clearly show that the
RTT latency for the Bay Area is lower than New York and Seattle areas. HTTP through-
put in these regions exhibit similar patterns.
We use DNS data in the Seattle area and observe that 97% of DNS requests for
google.com resolve to an IP for a server in the Los Angeles area instead of Seattle,
in part explaining the gap in latency between the two regions. For the NY area, our
measurements did not provide enough geographic information to understand whether
increased latency was due to path inefﬁciencies.
The key takeaway from this section is that geography alone doesn’t explain the vari-
ance in performance observed in the previous section; however, for one carrier (Veri-
zon), it explains some of it. Further, we observe that each region experiences changes
in performance independently – the correlation of performance across regions for each
carrier is negligibly small. Last, when correlating ping RTT and HTTP GET throughput
within each region, we ﬁnd higher correlations than carrier-wide correlations presented
in the previous section. This further suggests that performance is affected by location.
3.3 Performance over Time
We now analyze how performance depends on time – both in terms of time-of-day
effects and the stability of measurement performance over time. These properties allow
us to identify when to measure the network (e.g., during known busy hours) and when
not to measure (e.g., at ten minute intervals), thus allowing us to efﬁciently allocate the
limited measurement resources that users provide.
Time-of-Day and Long-Term Trends. Fig. 3 plots HTTP throughput for four major
carriers in the US. As expected, throughput decreases (and variance tends to increase)
during the busy hours for mobile usage (8AM to 7PM), likely due to higher load on
the network. Interestingly, different carriers experience minimum throughput at differ-
ent times. T-Mobile and AT&T reach their minimum throughput at 1PM and 5PM, re-
spectively; Sprint experiences minimum performance at 9PM and Verizon, two troughs
occur at 8AM and 9PM. Last, these carriers experience different relative variations in
Mobile Network Performance from User Devices
17
 1400
 1300
 1200
 1100
 1000
 900
 800
 700
 600
 0
 2
 4
AT&T HSDPA
Sprint EVDO_A
T-Mobile HSDPA
Verizon EVDO_A
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
)
%
(
r
o
r
r
E
 8
 6
Local Time from 0 (00:00) to 24 (24:00)
 10  12  14  16  18  20  22  24
Verizon, LTE, BayArea
Sprint, EVDOA, Seattle
Sprint, EVDOA, BayArea
T-Mobile, HSDPA, BayArea
 1  3
 6
 9
 12  15  18  21  24  27  30  33  36
Sampling Period (hour)
r
o
r
r
E
d
r
a
d
n
a
S
d
n
a
t
n
a
e
M
,
)
s
p
b
K
(
t
u
p
h
g
u
o
r
h
T
P
T
T
H
Fig. 3. Time of day pattern of HTTP
throughput
Fig. 4. Weighted Moving Average Error (Me-
dian Ping RTT, W = 2)
performance during busy hours: AT&T and Sprint throughput drops by approximately
a third during busy hours while Verizon drops by 25%, and T-Mobile by 16%.
Next, we investigate the long-term performance trends over the duration of our study,
allowing us to tell if new cellular technologies and infrastructure are keeping pace with
increased mobile Internet usage. Speciﬁcally, we look at the change in throughput and
latency of carriers through time over consecutive days for each network technology
they support in different areas. We did not observe improvement; despite technology
upgrades, performance is highly variable over time and there is no statistically signiﬁ-
cant change during the observation period.
Stability of Performance. The predictability and stability of network performance are
important not only for users, who are often frustrated more by variations in performance
than the average value, but also for determining how and when to conduct measure-
ments for future experiments. In this section, we compute stability using a weighted
moving average and autocorrelation.
First, we group the data into 1-hour buckets (to obtain a sufﬁciently large sample
size). Then for each bucket, we use either the median or 5th percentile latency. We
compute the moving average error for different window sizes and sampling periods.
We compute the moving average error as follows: for a window size W , we pre-
dict the next data point on that series by computing moving average for the previous
consecutive W points. For each W and sampling period (e.g., every N hours for
N = 1, 2, 3, . . .), we compute the average over different offsets.
Fig. 4 plots the average error for all data points with windows size of 2 and differ-
ent sampling periods for median ping RTT (results with larger window sizes of 3, 4,
and 5 are similar). We observe that prediction accuracy varies signiﬁcantly by carrier,
with Verizon and Sprint in the Bay Area being relatively predictable, and T-Mobile and
Sprint in Seattle being relatively unpredictable. Also, for all of these carriers, prediction
accuracy is best when looking at the most recent data (one hour sampling period) and er-
ror tends to increase with longer durations, with the exception of 24hr (day) and 168hrs
(week) sampling periods, which are local minima. The results from autocorrelation are
similar.
18
A. Nikravesh et al.
s
t
n
e
m
e
r
u
s
a
e
M
f
o
#
 1800
 1600
 1400
 1200
 1000
 800
 600
 400
 200
 0
22
Oct
Mountain View
Seattle
Ping RTT
 150
 140
 130
 120
 110
 100
 90
 80
29
Oct
05
Nov
12
Nov
19
Nov
26
Nov
03
Dec
10
Dec
17
Dec
Time (day) - 2011
 70
24
Dec
(a)
)
s
m
i
(
T
T
R
g
n
P
n
a
d
e
M
i
)
s
m
i
(
T
T
R
g
n
P
n
a
d
e
M
i
 160
 140
 120
 100
 80
 60
 40
Seattle (25-50-75)
Los Angeles (25-50-75)
11 12 13 14 16 17 18 19 20 21 22
Time (day) - Feb 2012
(b)
s
p
o
H
f
o
#
 18