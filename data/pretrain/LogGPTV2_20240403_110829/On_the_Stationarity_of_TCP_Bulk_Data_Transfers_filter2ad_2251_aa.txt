# On the Stationarity of TCP Bulk Data Transfers
**Author: Guillaume Urvoy-Keller**

**Institut Eurecom, 2229, route des crÃªtes, 06904 Sophia-Antipolis, France**
**Email: [Your Email Address]**

## Abstract
Although the Internet provides a single best-effort service, several factors suggest that it is increasingly capable of supporting multimedia and large data transfer applications. These factors include overprovisioned core backbones, faster end-user access, and the use of content delivery networks (CDNs) and peer-to-peer (P2P) solutions to mitigate network variations. In this paper, we present a tool to investigate the stationarity of long TCP transfers over the Internet, based on the Kolmogorov-Smirnov (K-S) goodness-of-fit test. Using BitTorrent, we obtain a set of long bulk transfers and test our tool. Our experimental results show that the tool effectively identifies significant changes in connection throughput. We also focus on receiver window-limited connections to relate the observed stationarity to typical connection behaviors.

## 1. Introduction
The current Internet offers a single best-effort service to all applications, leaving the management of losses and delay variations to end-hosts. Applications can be classified into two categories: elastic applications (e.g., web or email), which can tolerate throughput and delay variations, and real-time applications, which are sensitive to delays (e.g., voice over IP) or throughputs (e.g., video-on-demand). It is commonly believed that the current Internet, with its single best-effort service, requires additional functionality (e.g., Differentiated Services (DiffServ), Multiprotocol Label Switching (MPLS)) to support the mass deployment of real-time applications. However, several recent developments challenge this belief:
- Recent traffic analysis studies indicate that the Internet backbone is ready to provide real-time services.
- The number of residential users with high-speed access (e.g., ADSL or cable) is increasing rapidly.
- Network-aware coding schemes (e.g., MPEG-4 FGS) and new transmission methods (e.g., P2P techniques like Splitstream) have facilitated the deployment of real-time applications over the Internet.

These observations led us to investigate the variability of the service provided by the Internet from an end-connection perspective. Given that TCP carries most of the Internet's data, we focused on long-lived TCP connections. Bulk data transfers, particularly those involving P2P applications, represent a significant portion of current Internet traffic. By analyzing these transfers, we aim to better understand the interaction between TCP and the Internet, which is crucial for future applications and for CDN providers who rely on optimizing traffic paths.

The rest of this paper is organized as follows. In Section 2, we review related work. In Section 3, we present our dataset. In Section 4, we describe our tool for extracting stationarity periods in a given connection. In Section 5, we discuss the results obtained from our dataset. Finally, conclusions and future work directions are presented in Section 6.

## 2. Related Work
Mathematically, a stochastic process \( X(t) \) is stationary if its statistical properties (marginal distribution, correlation structure) remain constant over time.

Paxson et al. [17] studied the stationarity of the throughput of short TCP connections (transfers of 1 MB) between NIMI hosts. The primary difference between their work and ours is that we consider long bulk data transfers (several tens of minutes) and use a more recent dataset with hosts having varying access capacities. Other studies [4, 13] have focused on the non-stationarity observed on high-speed links with a high number of aggregated flows, examining the time scales at which non-stationarity appears and the underlying causes. Additionally, the processing of data streams has emerged as an active research area in the database community, with change detection being a critical task [14, 3].

## 3. Dataset
Our goal is to develop a tool to assess the stationarity of TCP bulk data transfers. To validate the tool, we need to gather long TCP transfers from a diverse set of hosts. A simple way to attract traffic from various destinations is to use a P2P application. For this purpose, we used BitTorrent, a popular file replication application [11]. A BitTorrent session involves the replication of a single large file among a set of peers. The data transfer phase uses a swarming technique, where the file is broken into chunks (typically 256 KB) that peers exchange with one another. Peers that have not yet completed the transfer are called leechers, while those that have completed the transfer are called seeds. Seeds remain in the session to serve leechers. Connections between peers are permanent TCP connections, characterized by on and off periods due to the BitTorrent algorithms [11].

Figure 1 shows a typical one-way BitTorrent connection lasting approximately 14 hours, with clear on and off phases. The y-axis represents one-second throughput samples.

![Figure 1: A typical (one-way) BitTorrent connection](figure1.png)

Our dataset consists of connections to about 200 peers downloading (part of) a file (the latest Linux Mandrake release) from a seed located at Eurecom. A tcpdump trace of 10 GB was generated during a measurement period of about 44 hours. While the 200 connections originate from Eurecom, the 10 Mbps access link should not be a shared bottleneck because:
- BitTorrent clients send data to only 4 peers simultaneously for efficiency.
- The total aggregate throughput generally remains far below the 10 Mbps, as shown in Figure 2. The average traffic on this link is around 1 Mbps with a peak rate below 2 Mbps.

![Figure 2: Aggregate rate of the BitTorrent application during the experiment](figure2.png)

To illustrate the diversity of the 200 peers, we used the MaxMind service (http://www.maxmind.com/) to determine their origin countries. Table 1 ranks countries based on the number of peers originating from each.

| Country | # Peers | Country | # Peers | Country | # Peers | Country | # Peers |
|---------|---------|---------|---------|---------|---------|---------|---------|
| US      | 87      | UK      | 24      | CA      | 14      | FR      | 12      |
| IT      | 8       | SE      | 8       | PL      | 7       | NL      | 4       |
| DE      | 3       | AU      | 3       | PE      | 3       | AE      | 3       |
| CL      | 2       | PT      | 2       | BR      | 2       | LT      | 2       |
| CN      | 1       | NO      | 1       | SI      | 1       | TW      | 1       |
| CZ      | 1       | YU      | 1       | BE      | 1       | AT      | 1       |
| ES      | 1       | CH      | 1       |         |         |         |         |

For more details on country abbreviations, see http://encyclopedia.thefreedictionary.com/ISO%203166-1.

To study long bulk data transfers, we extracted the on periods from the 200 connections, resulting in 399 flows. We identified off-periods as intervals of at least 15 seconds where less than 15 KB of data were sent. We further restricted our analysis to the 184 flows lasting longer than 1600 seconds (approximately 26.6 minutes). We call an on-period a "flow" and a part of a flow deemed stationary a "stationary flow." For each flow, we generated a time series representing the throughput for each 1-second interval. The average individual throughput of these 184 flows is 444 kbps. Overall, these flows correspond to the transfer of about 50 GB of data over a cumulative period of about 224 hours. Flows shorter than 1600 seconds account for about 14 GB of data.

While our dataset is not representative of all bulk transfers on the Internet, it is sufficiently large to demonstrate the effectiveness of our tool. It also shows that BitTorrent is an effective application for collecting long TCP transfers from a variety of hosts in terms of geographical location and access link speed.

## 4. Stationarity Analysis Tool

### 4.1 Kolmogorov-Smirnov (K-S) Test
The K-S test determines whether two independent and identically distributed (i.i.d.) samples \( X_1(t) \) and \( X_2(t) \) are drawn from the same distribution. The test calculates the empirical cumulative distribution functions (ECDFs) of both samples and evaluates the absolute maximum difference \( D_{\text{max}} \) between these ECDFs. The limit distribution of \( D_{\text{max}} \) under the null hypothesis (i.e., \( X_1 \) and \( X_2 \) are drawn from the same distribution) is known, making \( D_{\text{max}} \) the statistic for the test. In this paper, we used the MATLAB implementation of the K-S test with 95% confidence levels.

### 4.2 K-S Test for Change Point Detection
Our objective is to detect stationary regions in time series or, equivalently, to detect change points (i.e., border points between stationary regions). We used the K-S test to achieve this. Previous work has applied the K-S test for change detection, though not in the context of traffic analysis [9, 3].

The basic idea behind our tool is to use two back-to-back windows of size \( w \) sliding along the time series samples and applying the K-S test at each shift of the windows. For a time series of size \( n \), the K-S test generates a new binary time series of size \( n - 2w \), with value zero whenever the null hypothesis could not be rejected and one otherwise. The next step is to devise a criterion to decide if a '1' in the binary time series corresponds to a false alarm. Even if all samples originate from the same underlying distribution, the K-S test can lead to spurious '1' values. We deem a change point detected if at least \( w_{\text{min}} \approx \frac{w}{2} \) consecutive ones are observed in the binary time series. \( w_{\text{min}} \) controls the sensitivity of the algorithm. The intuition behind setting \( w_{\text{min}} \) close to \( \frac{w}{2} \) is that we expect the K-S test to consistently output '1' once the right-side window starts to include samples from a different distribution.