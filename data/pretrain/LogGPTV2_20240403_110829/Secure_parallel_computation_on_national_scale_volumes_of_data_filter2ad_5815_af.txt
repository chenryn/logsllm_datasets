abort to all parties. Else, reconstruct V.
• For v ∈ [{V11 ..V1i}, ...,{Vn1 ..Vn j}]:
Compute v(1) ← f unc(v).
Note: f unc is the computation applied on the data,
e.g. computing Gradient Decent for Matrix Factor-
ization or Addition in Histogram algorithm.
}, ...,{V (1)
n1
Output: All parties receive:[{V (1)
11
..V (1)
1i
..V (1)
n j }].
Figure 14: Oblivious Apply ideal functionality
4.3 Four-Party Secure GAS computation
In this section, we formally deﬁne our overall framework in a
hybrid-world model. But ﬁrst, we deﬁne the leakage function
L(G) to be the noisy degree of each vertex in the graph, as
was done by Mazloom and Gordon [15] (Deﬁnition 7). That
is, in the ideal world, after receiving secret shares of the graph
description, the functionality creates an array containing the
USENIX Association
29th USENIX Security Symposium    2499
2Gather	MAC	ComputationBobAliceGather	EdgesDavidGather	MAC	VerificationDavidCharlotteOblivious	Gather	(left)CharlotteTrue/FalseTrue/FalseBobAliceAliceBobhttps://github.com/sama730/National-Scale-Secure-Parallel-
Computation. We measure the performance of our framework
on a set of benchmark algorithms in order to evaluate our
design. These benchmarks consist of the histogram and
matrix factorization problems, which are commonly used
for evaluating highly-parallelizable frameworks.
In all
scenarios, we assume that the data is secret-shared across
four non-colluding cloud providers, as motivated in Section 1.
We compare our results with the closest large-scale secure
parallel graph computation schemes, such as GraphSC [18]
and OblivGraph [16].
Implementation
5.1
In our four-party framework, the histogram and matrix fac-
torization problems can be represented as directed bipartite
graphs.
Histogram: In the histogram computation, which for example
can be used to count the number of people in each zip code,
left vertices represent data elements (people), right vertices
are the counters for each type of data element (the zip code),
and existence of an edge indicates that data element on the
left has the data type of the right node (e.g. the user on the
left belong to the zip code on the right).
Matrix Factorization: In matrix factorization, left vertices rep-
resent users, right vertices are items (e.g. movies in movie
recommendation systems or a product in targeted advertising
systems), an edge indicates that a user ranked that item, and
the weight of the edge represents the rating value.
Vertex and Edge representation: In all scenarios, our statisti-
cal security parameter s = 40. We choose k = 40 to represent
k-bit ﬁxed-point numbers, in which the least d signiﬁcant bits
are used for the fractional part. For histogram d = 0 and for
matrix factorization d = 20. This requires data and MACs to
be secret shared in the Z280 ring. In our matrix factorization
experiments, we factorize the ratings matrix into two matri-
ces, represented by feature vectors that each has dimension
10. We choose these parameters as to be compatible with the
GraphSC and OblivGraph representations.
5.2 Evaluation
We run the Histogram experiments on graphs with sizes rang-
ing from 1 million to more than 300 million edges, which can
simulate the counting operation in census data gathering [1].
For example, if each user contributed a salary value and a
zip-code, using our framework we can compute the average
salary in each zip-code, while ensuring that the access pat-
terns preserve user privacy. We run matrix factorization with
gradient descent on the real-world MovieLens datasets [11]
that contains user ratings on movies. We report the result for
one complete iteration of the protocol, performing GAS oper-
ations one time on both the left and right nodes. The results
are the average of ﬁve executions of the experiments.
Experiment settings: We run all the experiments on AWS
(Amazon Web Services) using four r4.8xlarge instances, each
has 32 processors and 244 GiB RAM, with 10 Gbps network
connectivity. For the LAN experiments, all instances were in
the same data center (Northern Virginia). For the WAN exper-
iments, they were spread across Northern Virginia (P1 and P4)
and Oregon data centers (P2 and P3). The pairs (P1, P4) and
(P2, P3) each communicate O(1) ring elements in total, thus,
we did not bother to separate these pairs in our WAN experi-
ments. We use three metrics in evaluating the performance of
our framework: running time in seconds, communication cost
in MB, measured by the number of bits transferred between
parties, and circuit size, measured by the number of AND
Gates/AES operations.
The size of the graphs in all Histogram and MF experiments
is as follows: (cid:104) 6K users, 4K items, 1M edges(cid:105), (cid:104)72K users,
10K items, 10M edges(cid:105), (cid:104)138K users, 27K items, 20M edges(cid:105),
and (cid:104)300M users, 42K items, 300M edges(cid:105) for Histogram
only. . In all the experiments, the privacy parameters are set
as ε = 0.3, δ = 2−40.
Run time and Communication Cost: Figure 16a demon-
strates that the run time required to compute the Histogram
protocol on a graph with 300 million edges is less than 4.17
mins, using multiprocessor machines in the LAN setting. Ta-
ble 1 shows the results in more detail. Figure 16b shows the
amount of data in MB, transferred between the parties during
the Histogram protocol. Communication cost shows linear de-
crease with increasing the number of processors. Both graphs
are in log-log scale.
(a) Running Time
(b) Communication Cost
Figure 16: Run time(s) and Communication cost(MB) of
Histogram on graph sizes 1M, 10M, 20M and 300M edges
Table 1: Details of running time (sec) for computing His-
togram problem on different input sizes
Processors / Edges
1
2
4
8
16
32
1M
13.8
7.5
4.3
2.7
1.8
1.5
10M
85.0
46.5
28.0
16.2
11.2
10.1
20M
207.7
98.1
57.8
34.4
23.3
21.7
300M
2149.4
1136.5
643.2
382.5
279.2
250.4
2500    29th USENIX Security Symposium
USENIX Association
202122232425Number of Processors101102103Execution Time (seconds)1M10M20M300M202122232425Number of Processors101102103104Communication Cost (MB)1M10M20M300MSimilarly, Figure 17a shows that computing Matrix Fac-
torization on large scale graph data sets takes less than 6
minutes, using our four-party framework, in our AWS LAN
setting. The running time is expected to decrease linearly as
we increase the number of processors, however due to some
small overhead incurred by parallelization, the run time im-
provement is slightly sub-linear. Table 2 shows the results
in details. Figure 17b shows the communication cost during
Matrix Factorization on large data sets. Both graphs are in
log-log scale.
(a) Running Time
(b) Communication Cost
Figure 17: Run time(s) and Communication cost(MB) of
Matrix Factorization on graph sizes 1M, 10M and 20M edges
Table 2: Details of running time (sec) for computing Matrix
Factorization problem on different input sizes
Processors / Edges
1
2
4
8
16
32
1M
258.3
132.9
80.4
44.6
28.2
25.1
10M
1639.7
834.7
455.6
292.2
190.6
163.4
20M
3401.8
1913.7
1055.9
613.1
423.7
357.2
We measure the run time for each of the graph oblivious
operations in our framework, to understand the effect of each
step in the performance of the framework as a whole. Figure
18a and 18b demonstrates the run time break-down of each
oblivious operation in Histogram and Matrix Factorization
problem, on the input graph with only 1 million edges. The
oblivious Shuﬄe operation has the highest cost in calculating
the Histogram, while Apply phase is taking the most time
in Matrix Factorization, due to the calculation of gradient
descent values, which are more expensive than counting.
Comparison with previous work: We compare our results
with OblivGraph which is the closest large-scale secure paral-
lel graph computation. OblivGraph used garbled circuits for
all the phases of the graph computation, while we use arith-
metic circuits. In both approaches, the amount of time needed
to send and receive data, and the time spent computing AES,
are the dominant costs. We compare the two protocols by the
communication cost and the number of AES calls in each of
them. In Table 3 and 4, we demonstrated both the gain in our
four party oblivious shufﬂe against the two party shufﬂe [21]
(a) Histogram
(b) Matrix factorization
Figure 18: Run time for each operation in Histogram and
Matrix Factorization on graph size 1M edges (LAN)
used in OblivGraph and the gain in the Apply phase with the
use of arithmetic circuits in the four party setting.
Table 3: Estimated number of AES operations per party for a
single iteration of matrix factorization: |E| is total number of
edges (real and dummies), |V| number of vertices.
Oblivious Shufﬂe
Oblivious Gather
Share Conversion
Oblivious Apply
Oblivious Scatter
Total
OblivGraph
7128(|E|log|E|−|E| + 1)
This work
132|E|
72|E|
0
-
0
279048|E| + 4440|V|
72|E| + 30|V|
252|E| + 4|V|
7128|E|log|E| + 271920|E|+ 548|E| + 34|V|
20|E|
4440|V| + 7128
Table 4: Estimated total communication cost for all par-
ties(bits), for a single iteration of matrix factorization: κ is the
number of bits per ciphertext, s = 40, |E| is total number of
edges (real and dummies), |V| number of vertices. The length
of the ﬁxed point numbers used is k = 40 bits
Oblivious Shufﬂe
Oblivious Gather
Share Conversion
Oblivious Apply
Oblivious Scatter
Total
OblivGraph
4752κ(|E|log|E|−|E| + 1)
32κ|E|
-
186032κ|E| + 2960κ|V|
0
4752κ|E|log|E| + 181312κ|E|
+2960κ|V| + 4752
This work
432(k + s)|E|
160(k + s)|E|
(192|E| + 120|V|)(k + s)
(212|E| + 120|V|)(k + s)
(996|E| + 240|V|)(k + s)
0
Table 5, compares our running time with those of GraphSC
[18] and OblivGraph [16], while computing matrix factoriza-
tion on the real-world, MovieLens dataset, with 6040 users,
3883 movies, 1M ratings, and 128 processors.
Effect of differential privacy parameters on the run time:
We study the effect of differential privacy parameters on the
performance of our framework using multiprocessor machines
in the LAN setting, Figure 19. We also provide the number of
dummy edges required for different value of ε and δ in Table 6.
Note that the stated number of dummy edges are for each right
node in the graph. For example, in a movie recommendation
system based on our framework, we require 118 dummy edges
USENIX Association
29th USENIX Security Symposium    2501
202122232425Number of Processors102103Execution Time (seconds)# of Edges1M10M20M202122232425Number of Processors102103104Communication Cost (MB)# of Edges1M10M20M202122232425Processors02468101214Time(seconds)Histogramon1MEdgesOblivShuﬄeOblivGatherShareConversionOblivApplyOblivScatter202122232425Processors020406080100120Time(seconds)MFon1MEdgesOblivShuﬄeOblivGatherShareConversionOblivApplyOblivScatterGraphSC [18] OblivGraph [16] This work
Time
13hrs
2hrs
25s
Table 5: Run time comparison on this work vs. OblivGraph