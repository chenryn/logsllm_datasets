of backdoor examples (also known as the trigger set) relabeled
to a secret class [23], [47]. The ownership can then be claimed
when the defender queries the suspect model for examples
attached with the watermark trigger and receives the secret
class as predictions.
C. DNN Fingerprinting
Recently, DNN ﬁngerprinting techniques have been pro-
posed to verify model ownership via two steps: ﬁngerprint
extraction and veriﬁcation. According to the categorization
rule for watermarking, ﬁngerprinting methods [2], [27] are all
black-box techniques. Moreover, they are non-invasive, which
is in sharp contrast with watermarking techniques. Instead
of modifying the training procedure to embed identities, ﬁn-
gerprinting directly extracts a unique feature or property of
the owner model as its ﬁngerprint (i.e., a unique identiﬁer).
The ownership can then be veriﬁed if the ﬁngerprint of
the owner model matches with that of the suspect model.
For example, IPGuard [2] leverages data points close to the
classiﬁcation boundary to ﬁngerprint the boundary property
of the owner model. A suspect model is determined to be a
stolen copy of the owner model if it predicts the same labels
for most boundary data points. [27] proposes a Conferrable
Ensemble Method (CEM) to craft conferrable (a subclass of
transferable examples) adversarial examples to ﬁngerprint the
overlap between two models’ decision boundaries or adversar-
ial subspaces. CEM ﬁngerprinting demonstrates robustness to
removal attacks including ﬁnetuning, pruning and extraction
attacks, except several adapted attacks like adaptive transfer
learning and adversarial training [27]. It is the closest work to
our DEEPJUDGE. However, as a ﬁngerprinting method, CEM
targets uniqueness, while as a testing framework, our DEEP-
JUDGE targets completeness, i.e., comprehensive characteriza-
tion of a model with multi-level testing metrics and diverse test
case generation methods. Note that CEM ﬁngerprinting can be
incorporated into our framework as a black-box metric.
III. DNN COPYRIGHT THREAT MODEL
We consider a typical attack-defense setting with two par-
ties: the victim and the adversary. Here, the model owner is the
victim who trains a DNN model (i.e., the victim model) using
private resources. The adversary attempts to steal a copy of
the victim model, which 1) mimics its functionality while 2)
cannot be easily recognized as a copy. Following this setting,
we identify three common threats to DNN copyright: 1) model
ﬁnetuning, 2) model pruning, and 3) model extraction. The
three threats are illustrated in the top row of Fig. 1.
Threat 1: Model Finetuning. In this case, we assume the
adversary has full knowledge of the victim model, including
model architecture and parameters, and has a small dataset to
ﬁnetune the model [1], [40]. This occurs, for example, when
the victim open-sourced the model for academic purposes
only, but the adversary attempts to ﬁnetune the model to build
commercial products.
Threat 2: Model Pruning. In this case, we also assume the
adversary has full knowledge of the victim model’s architec-
ture and parameters. Model pruning adversaries ﬁrst prune the
victim model using some pruning methods, then ﬁnetune the
model using a small set of data [26], [35].
Threat 3: Model Extraction. In this case, we assume the
adversary can only query the victim model for predictions
(i.e., the probability vector). The adversary may be aware of
the architecture of the victim model but has no knowledge
of the training data or model parameters. The goal of model
extraction adversaries is to accurately steal the functionality
of the victim model through the prediction API [21], [38],
[33], [32], [45]. To achieve this, the adversary ﬁrst obtains an
annotated dataset by querying the victim model for a set of
auxiliary samples, then trains a copy of the victim model on
the annotated dataset. The auxiliary samples can be selected
from a public dataset [8], [32] or synthesized using some
adaptive strategies [33], [21].
IV. TESTING FOR DNN COPYRIGHT PROTECTION
In this section, we present DEEPJUDGE, the proposed test-
ing framework that produces supporting evidence to determine
whether a suspect model is a copy of a victim model. The
victim model can be copied by model ﬁnetuning, pruning,
or extraction, as discussed in Section III. We identify the
following criteria for a reliable copyright protection method:
826
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:42 UTC from IEEE Xplore.  Restrictions apply. 
Algorithm 1: DEEPJUDGE(O,S,D)
Input: owner model O, suspect model S, data set D
Output: judgement J , evidence E
// Test case generation (Section IV-C)
1 Seeds ← SelectSeeds (O,D)
2 T ← GenerateTestCases (O, Seeds)
// Testing metrics (Section IV-B)
3 E ← ComputeMetrics (O,S, T )
// Final judgement (Section IV-D)
4 J ← Judging (E) // Copy, Right? Yes or No.
5 return J ,E
1) Fidelity. The protection or ownership veriﬁcation pro-
cess should not affect the utility of the owner model.
2) Effectiveness. The veriﬁcation should have high preci-
sion and recall in identifying stolen model copies.
3) Efﬁciency. The veriﬁcation process should be efﬁcient,
e.g., taking much less time than model training.
4) Robustness. The protection should be resilient to adap-
tive attacks.
DEEPJUDGE is a testing framework designed to satisfy all
the above criteria. In the following three subsections, we will
ﬁrst give an overview of DEEPJUDGE, then introduce its multi-
level testing metrics and test case generation algorithms.
A. DEEPJUDGE Overview
As illustrated in the bottom row of Fig. 1, DEEPJUDGE
consists of two components and a ﬁnal judgement step: i)
test case generation, ii) a set of multi-level distance metrics
for testing, and iii) a thresholding and voting based judge-
ment mechanism. Alg. 1 depicts the complete procedure of
DEEPJUDGE with pseudocode. It takes the victim model O, a
suspect model S, and a set of data D associated with the victim
model as inputs and returns the values of the testing metrics as
evidence as well as the ﬁnal judgement. The set of data D can
be provided by the owner from either the training or testing set
of the victim model. At the test case generation step, it selects
a set of seeds from the input dataset D (Line 1) and carefully
generates a set of extreme test cases from the seeds (Line 2).
Based on the test cases generated, DEEPJUDGE computes the
distance (dissimilarity) scores deﬁned by the testing metrics
between the suspect and victim models (Line 3). The ﬁnal
judgement of whether the suspect is a copy of the victim can
be made via a thresholding and voting mechanism according
to the dissimilarity scores between the victim and a set of
negative suspect models (Line 4).
B. Multi-level Testing Metrics
We ﬁrst
introduce the testing metrics for two different
settings respectively: white-box and black-box. 1) White-box
Setting: In this setting, DEEPJUDGE has full access to the
internals (i.e., intermediate layer outputs) and the ﬁnal prob-
ability vectors of the suspect model S. 2) Black-box Setting:
In this setting, DEEPJUDGE can only query the suspect model
S to obtain the probability vectors or the predicted labels. In
both settings, we assume the model owner is willing to provide
TABLE I: Proposed multi-level testing metrics.
Level
Property-level
Neuron-level
Layer-level
Metric
Defense Setting
Robustness Distance (RobD)
Black-box
Neuron Output Distance (NOD)
White-box
Neuron Activation Distance (NAD) White-box
White-box
Layer Outputs Distance (LOD)
White-box
Layer Activation Distance (LAD)
Jensen-Shanon Distance (JSD)
Black-box
full access to the victim model O, including the training and
test datasets, and the training details if necessary.
The proposed testing metrics are summarized in Table I,
with their suitable defense settings highlighted in the last
column. DEEPJUDGE advocates evidence-based ownership
veriﬁcation of DNNs via multi-level testing metrics that com-
plement each other to produce more reliable judgement.
1) Property-level metrics: There is an abundant set of
model properties that could be used to characterize the simi-
larities between two models, such as the adversarial robust-
ness property [11], [4], [2], [27] and the fairness property
[30]. Here, we consider the former and deﬁne the robustness
distance to measure the adversarial robustness discrepancy
between two models on the same set of test cases. We will
test more properties in our future work.
Denote the function represented by the victim model O
by f, given an input xi and its ground truth label yi, an
adversarial example x(cid:2)
i can be crafted by slightly perturbing xi
towards maximizing the classiﬁcation error of f. This process
i) (cid:7)= yi indicates
is known as the adversarial attack, and f (x(cid:2)
a successful attack. Adversarial examples can be generated
using any existing adversarial attack methods such as FGSM
[14] and PGD [28]. Given a set of test cases, we can obtain its
adversarial version T = {x(cid:2)
i denotes the
adversarial example of xi. The robustness property of model
f can then be deﬁned as its accuracy on T :
2,···}, where x(cid:2)
1, x(cid:2)
Rob(f, T ) =
|T|(cid:2)
i=1
1|T|
(f (x(cid:2)
i)) = yi).
Robustness Distance (RobD). Let ˆf be the suspect model,
we deﬁne the robustness distance between f and ˆf by the
absolute difference between the two models’ robustness:
RobD(f, ˆf , T ) = |Rob( ˆf , T ) − Rob(f, T )|.
The intuition behind RobD is that model robustness is closely
related to the decision boundary learned by the model through
its unique optimization process, and should be considered as
a type of ﬁngerprint of the model. RobD requires minimal
knowledge of the model (only its output labels).
2) Neuron-level metrics: Neuron-level metrics are suitable
for white-box testing scenarios where the internal
layers’
output of the model is accessible. Intuitively, the output of each
neuron in a model follows its own statistical distribution, and
the neuron outputs in different models should vary. Motivated
by this, DEEPJUDGE uses the output status of neurons to
capture the difference between two models and deﬁnes the
following two neuron-level metrics NOD and NAD.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:42 UTC from IEEE Xplore.  Restrictions apply. 
827
Neuron Output Distance (NOD). For a particular neuron nl,i
with l being the layer index and i being the neuron index
within the layer, we denote the neuron output function of
the owner’s victim model and the suspect copy model by
φl,i and ˆφl,i respectively. NOD measures the average neuron
output difference between the two models over a given set
T = {x1, x2,···} of test cases:
1|T|
|φl,i(x) − ˆφl,i(x)|.
N OD(φl,i, ˆφl,i, T ) =
(cid:2)
x∈T
Neuron Activation Distance (NAD). Inspired by the Neuron
Coverage [34] for testing deep learning models, NAD mea-
sures the difference in activation status (‘activated’ vs. ‘not
activated’) between the neurons of two models. Speciﬁcally,
for a given test case x ∈ T , the neuron nl,i is determined to
be ‘activated’ if its output value φl,i(x) is larger than a pre-
speciﬁed threshold. The NAD between the two models with
respect to neuron nl,i can then be calculated as:
N AD(φl,i, ˆφl,i, T ) =
1|T|
|S(φl,i(x)) − S( ˆφl,i(x))|,
(cid:2)
x∈T
where the step function S(φl,i(x)) returns 1 if φl,i(x) is
greater than a certain threshold, 0 otherwise.
3) Layer-level metrics: The layer-wise metrics in DEEP-
JUDGE take into account the output values of the entire layer
in a DNN model. Compared with neuron-level metrics, layer-
level metrics provide a full-scale view of the intermediate layer
output difference between two models.
Layer Output Distance (LOD). Given a layer index l, let
f l and ˆf l represent the layer output functions of the victim
model and the suspect model, respectively. LOD measures the
Lp-norm distance between the two models’ layer outputs:
LOD(f l, ˆf l, T ) =
1|T|
||f l(x) − ˆf l(x)||p,
where ||·||p denotes the Lp-norm (p = 2 in our experiments).
Layer Activation Distance (LAD). LAD measures the average
NAD of all neurons within the same layer:
(cid:2)
x∈T
|Nl|(cid:2)
i=1
LAD(f l, ˆf l, T ) =
1|Nl|
N AD(φl,i, ˆφl,i, T ),
where Nl is the total number of neurons at the l-th layer, and
φl,i and ˆφl,i are the neuron output functions from f l and ˆf l.
Jensen-Shanon Distance (JSD). JSD [13] is a metric that
measures the similarly of two probability distributions, and a
small JSD value implies the two distributions are very similar.
Let f L and ˆf L denote the output functions (output layer) of
the victim model and the suspect model, respectively. Here,
we apply JSD to the output layer as follows:
1
2|T|
x∈T
JSD(f L, ˆf L, T ) =
K(f L(x), q) +K ( ˆf L(x), q),
where q = (f L(x) + ˆf L(x))/2 and K(·,·) is the Kullback-
Leibler divergence. JSD quantiﬁes the similarity between two
models’ output distributions, and is particularly more powerful
against model extraction attacks where the suspect model
is extracted based on the probability vectors (distributions)
returned by the victim model.
(cid:2)
Fig. 2: DEEPJUDGE uses adversarial examples of the victim
model to probe the difference in models’ decision boundary.
C. Test Case Generation
To fully exercise the testing metrics deﬁned above, we need
to magnify the similarities between a positive suspect and the
victim model, while minimizing the similarities of a negative
suspect to the victim model. In DEEPJUDGE, this is achieved
by smart test case generation methods. Meanwhile, test case
generation should respect the model accessibility in different
defense settings, i.e., black-box vs. white-box.
1) Black-box setting: When only the input and output
of a suspect model are accessible, we populate the test set
T using adversarial inputs generated by existing adversarial
attack methods on the victim model. We consider three widely
used adversarial attack methods, including Fast Gradient Sign
Method (FGSM) [14], Projected Gradient Descent (PGD) [28],
and Carlini & Wagner’s (CW) attack [4], where FGSM and
PGD are L∞-bounded adversarial methods, and CW is an
L2-bounded attack method. This gives us more diverse test
cases with both L∞- and L2-norm perturbed adversarial test
cases. The detailed description and exact parameters used for
adversarial test case generation are provided in Appendix D.
Fig. 2 illustrates the rationale behind using adversarial
examples as test cases. Finetuned and pruned model copies
are directly derived from the victim model, thus they share
similar decision boundaries (purple line) as the victim model.
However, the negative suspect models are trained from scratch
on different data or with different initializations, thus having
minimum or no overlapping with the victim model’s decision
boundary. By subverting the model’s predictions, adversarial
examples cross the decision boundary from one side to the
other (we use untargeted adversarial examples). Although the
extracted models by model extraction attacks are trained from
scratch by the adversary, the training relies on the probability
vectors returned by the victim model, which contains infor-
mation about the decision boundary. This implies that the
extracted model will gradually mimic the decision boundary of
the victim model. From this perspective, the decision boundary
(or robustness) based testing imposes a dilemma to model
extraction adversaries:
the more
similar the extracted model to the victim model, and the easier
it to be identiﬁed by our decision boundary based testing.
the better the extraction,
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:42 UTC from IEEE Xplore.  Restrictions apply. 
828
Algorithm 2: GenerateTestCases(O, Seeds)
Input: owner model O, a set of seed inputs Seeds
Output: test suite T
1 Initialize test suite T ← ∅
2 for each neuron nl,i do
3
4
5
Sample a seed input x ← Seeds.choice()
x(cid:2) ← copy(x)
for iter = 1 to iters do
Calculate gradients grads ← ∇φl,i(x(cid:2))
∇x(cid:2)
Perturb input x(cid:2) ← x(cid:2) + lr · grads
if φl,i(x(cid:2)) > threshold k then
Add new test case T ← T ∪ {x(cid:2)}
break
6
7
8
9
10
11
12
13 end
14 return T
end
end
The lower a measured metric value, the more likely the suspect
model is a copy of the victim, according to this metric. The
ﬁnal judgment can then be made based on the votes: the
suspect model will be identiﬁed as a positive suspect if it
receives more positive votes, and a negative suspect otherwise.
For each testing metric λ, we set the threshold adaptively
using an ε-difference strategy. Speciﬁcally, we use one-tailed
T-test to calculate the lower bound LBλ based on the statistics
of the negative suspect models at the 99% conﬁdence level.
If the measured difference λ(O,S, T ) is lower than LBλ, S