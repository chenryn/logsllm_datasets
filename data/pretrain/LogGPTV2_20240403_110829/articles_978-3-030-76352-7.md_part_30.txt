### Memory Leak Detection in Cloud-Based Infrastructures

**Introduction:**

Memory leak detection is a critical aspect of maintaining the performance and reliability of applications, especially in cloud-based environments. Several approaches have been proposed to detect memory leaks, each with its own focus and methodology. For instance, Nick Mitchell and Gary Sevitsky introduced "LeakBot," which identifies heap size growth patterns in Java applications [7]. Similarly, "LEAKPOINT" by Clause et al. uses dynamic tainting to track heap memory pointers and detect memory leaks [3].

Most existing online detection algorithms concentrate on specific programming languages, garbage collection strategies, or internal application details such as object allocation, references, and deallocation. To our knowledge, there has been no prior work that solely focuses on detecting memory leaks using only the system's memory utilization data. This paper aims to address this gap by proposing a method to detect memory leak patterns regardless of the programming language, source code, or low-level details.

### Methodology for Memory Leak Detection

#### 3.1 Problem Statement

We are given a univariate dataset \( x = \{x_1, x_2, \ldots, x_N\} \), where \( N \) is the number of data points, representing the memory utilization observations of a virtual machine (VM). Each observation \( x_t \in \mathbb{R} \) is the percentage memory utilization of the VM at time \( t \). The objective is to determine whether a memory leak exists on the VM such that an observation \( x_t \) at time \( t \) reaches the threshold \( U \) (set to 100%) within a defined critical time \( C \).

**Formal Problem Definition:**
- **Given:** A univariate dataset \( x = \{x_1, x_2, \ldots, x_N\} \) representing the memory utilization observations of the VM.
- **Output:** An anomalous window consisting of a sequence of observations \( x_{t-T:t} \) such that these observations, following a certain trend, will reach the threshold \( U \) memory utilization at time \( t+M \) where \( M \leq C \).

**Definition 1 (Critical Time):** The maximum time considered relevant for reporting a memory leak, during which if the trend line of memory utilization of the VM is projected, it will reach the threshold \( U \).

#### 3.2 Illustrative Example

Figure 1 illustrates the memory utilization of a memory-leaking VM, highlighting the anomalous window between times \( t_k \) and \( t_n \). The memory utilization of the VM follows a linearly increasing trend, reaching the defined threshold (U = 100%) within the critical time \( C \). Thus, this VM is identified as having a memory leak.

**Figure 1.** Example memory utilization of a memory-leaking VM with the marked anomalous window.

Our approach can be applied to multiple VMs. Experiments show that if an application has a memory leak, the memory usage of the VM on which it runs typically increases steadily until the available system memory is exhausted, causing the application to terminate. Memory leak behavior often exhibits a linearly increasing or "sawtooth" memory utilization pattern.

#### 3.3 Memory Leak Detection Algorithm: Precog

The Precog algorithm consists of two phases: offline training and online detection. Figure 2 outlines the overall workflow of the Precog algorithm.

**Offline Training:**
- **Data Pre-processing:** The process begins by collecting the memory utilization data of a VM. The dataset is resampled to one observation per defined resampling time resolution and then median-smoothed over a specified smoothing window.
- **Trend Lines Fitting:** Change points \( P = \{P_1, P_2, \ldots, P_k\} \) are detected on the whole dataset. By default, change points at the beginning and end of the time series data are added. If no change points are detected, the algorithm processes each data point, which is computationally intensive. The module selects a sequence of observations \( x_{t-L:t} \) between two change points and fits a line using linear regression. The R-squared score, duration, exit time, and slope of the line are calculated. This procedure is repeated for all change points, and the best-fitted line based on the largest duration and highest slope is selected. If the best-fitted line’s time to reach the threshold falls below the critical time \( C \), its slope and duration are saved as historic trends.

**Online Detection:**
- **New Observations:** For a new set of observations \( \{x_k, x_{k+1}, \ldots, x_{k+t}\} \) from time \( k \) to \( t \) (where \( t - k \geq P_{min} \)), the data is pre-processed and fed into the Trend Lines Fitting module. Change points are detected, and a sequence of observations \( x_{t-L:t} \) between the last two change points is selected. A line is fitted using linear regression, and the R-squared score, slope, duration, and exit time are calculated. If the slope and duration are greater than the saved maximum values, the window is marked as anomalous. Otherwise, the values are compared against all training trends, and if the fitted line’s slope and duration are greater than any saved trend, the window is marked as anomalous. This process is repeated for all change points.

**Definition 2 (Change Points):** A set of time ticks that deviate significantly from the normal pattern of the data. These are calculated by taking the first-order difference of the input time series, computing their absolute values, and calculating their Z-scores. Observations with Z-scores greater than a defined threshold (typically 3 times the standard deviation) are identified as change points.

**Algorithm 1: Precog Algorithm**
- **Input:** Training and test time series, minimum R-squared score, critical time
- **Output:** List of anomalous windows

```plaintext
Function CPD(x=input Ts, threshold=3):
  absDiffTs = first order absolute difference of x
  zScores = calculate z-scores of absDiffTs
  cpdIndexes = indexes of (zScores > threshold)
  return cpdIndexes // return the change-points indexes

Function TRAINING(x=input Train Ts, R2 score min, C=critical time):
  P = CPD(x) // get Change-points
  p1 = 0
  while p1 <= length(P) do
    p2 = p1
    D_b, S_b, T_b = 0 // best local trend’s duration, slope, exit time
    while p2 <= length(P) do
      exit_time, r2, dur, slope ← LinearRegression(ts) // fitted line’s exit time, R2 score, duration, slope
      if r2 ≥ R2 score min and dur ≥ D_b and slope ≥ S_b then
        Update(D_b, S_b, T_b) // update best local values
      p2 = p2 + 1
    if T_b ≤ C then
      if D_b ≥ D_max and S_b ≥ S_max then
        Update(D_max, S_max) // update global trend values
      saveTrend(D_b, S_b), save(D_max, S_max) // save values
    p1 = p1 + 1

Function TEST(x=input Test Ts, C=critical time):
  a = [0] // anomalous empty array of size input Test Ts
  P = CPD(x) // get Change-points
  len = length(P) // length of change point indexes
  while i ≤ len do
    ts = x[P[len-i]:P[len]] // i is a loop variable
    exit_time, r2, dur, slope = LinearRegression(ts)
    D_max, S_max, Trends = get saved values
    if exit_time ≤ C and r2 ≥ R_min then
      if slope ≥ S_max and dur ≥ D_max then
        a[P[len-i]:P[len]] = 1 // current trend greater than global saved so mark anomalous
      else
        For Each t in Trends if slope ≥ S_t and dur ≥ D_t then
          a[P[len-i]:P[len]] = 1 // current trend greater than one of the saved trend so mark anomalous
    i = i + 1
  return a // list with 0s and anomalous indexes represented by 1
```

### Evaluation

We conducted experiments to answer the following questions:
- **Q1. Memory Leak Detection Accuracy:** How accurate is Precog in detecting memory leaks?
- **Q2. Scalability:** How does the algorithm scale with an increase in data points?
- **Q3. Parameter Sensitivity:** How sensitive is the algorithm to changes in parameter values?

**Evaluation Metrics:**
- **F1-Score (F1):** Used to evaluate the performance of the algorithms.
- **Experimental Setup:** Tests were executed on a machine with 4 physical cores (3.6 GHz Intel Core i7-4790 CPU) with hyperthreading enabled and 16 GB of RAM, similar to a typical cloud VM.

**Hyperparameters:**
- Maximum threshold \( U \) set to 100%
- Critical time \( C \) set to 7 days
- Smoothing window size: 1 hour
- Resampling time resolution: 5 minutes
- Minimum R-squared score \( R2_{min} \) for a good fit: 0.75
- Data split: 65% for training, 35% for testing

#### 4.1 Q1. Memory Leak Detection Accuracy

To demonstrate the effectiveness of the developed algorithm, we initially generated synthetic time series. Table 2 shows the F1 score corresponding to each memory leak pattern and the overall F1 score. Precog achieved an overall accuracy of 90%.

**Table 2.** Synthetically generated time series corresponding to each memory leak pattern and their accuracy scores.

| Memory Leak Pattern | +ve Cases | -ve Cases | F1 Score | Recall | Precision |
|---------------------|-----------|-----------|----------|--------|-----------|
| Linearly Increasing | 30        | 30        | 0.933    | 0.933  | 0.933     |
| Linearly Increasing (with noise) | 30 | 30 | 0.895 | 1.0 | 0.810 |
| Sawtooth            | 30        | 30        | 0.830    | 0.73   | 0.956     |
| Overall             | 90        | 90        | 0.9      | 0.9    | 0.91      |

Additionally, we evaluated Precog on real cloud data provided by Huawei Munich, consisting of manually labeled memory leak data from 60 VMs over 5 days. Each time series had an observation every minute. Out of these 60 VMs, 20 had memory leaks. The algorithm achieved an F1-Score of 0.857, recall of 0.75, and precision of 1.0. The average prediction time per test data containing approximately 500 points was 0.32 seconds.

**Figure 3.** Algorithm results on 3 difficult cases having memory leaks (a–c) and one case not having a memory leak (d).

- **Case (a):** Linearly increasing memory utilization.
- **Case (b):** Sawtooth linearly increasing pattern.
- **Case (c):** Linearly increasing without trends detected in training data.
- **Case (d):** Linearly increasing with a similar trend as training data, but not a memory leak.

#### 4.2 Q2. Scalability

We verified that our prediction method scales linearly. We repeatedly duplicated our dataset in time ticks and added Gaussian noise. Figure 4b shows that Precog’s prediction method scales linearly in time ticks, providing results under 1 second for data with 100,000 time ticks. However, the training method, shown in Figure 4a, is quadratic in nature, but training needs to be conducted periodically (e.g., once per day or week).

**Figure 4.** (a) Training method scalability, (b) Prediction method scalability.

### Conclusion

The Precog algorithm provides a robust and scalable solution for detecting memory leaks in cloud-based infrastructures, independent of the programming language and internal application details. The evaluation results demonstrate high accuracy and efficient performance, making it a valuable tool for maintaining the reliability and performance of cloud applications.