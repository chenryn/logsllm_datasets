cutoff that still achieves a 90% true positive rate. The selected
cutoff is 0.4927, which we round to 0.5. Figure 2 shows the
ROC curve obtained using the various cutoff values, with the
y-axis showing the computed true positive rate. The x-axis
shows the corresponding false positive rate, which is computed
on an independent test set of benign apps.
IX. EVALUATION
The goal of our evaluation is to answer the following
questions:
Q1. How do the signatures synthesized by ASTROID compare
with manually-written signatures used for evaluating AP-
POSCOPY in terms of precision and accuracy?
Q2. How do the quality of learned signatures improve as we
increase the number of samples?
Q3. How does ASTROID compare against other state-of-the-
art malware detectors?
Q4. How effective is ASTROID at detecting zero-day mal-
ware?
Q5. How resistant is ASTROID to behavioral obfuscation?
In what follows, we describe a series of ﬁve experiments
designed to answer these questions. All experiments are con-
ducted on an Intel Xeon(R) computer with an E5-1620 v3 CPU
and 32G of memory running on Ubuntu 14.04.
8
y
c
a
r
u
c
c
A
100
90
80
70
60
50
40
30
20
10
0
Manual Signature ASTROID
Avg. Accuracy Manual Signature
Avg. Accuracy ASTROID
Id Malware Family
A DroidKungFu
B AnserverBot
C
BaseBridge
D Geinimi
E
F
G Pjapps
H ADRD
I
J
K Bgserv
L
M GingerMaster
jSMSHider
DroidDream
DroidDreamLight
GoldDream
BeanBot
#Samples
444
184
121
68
46
46
43
22
16
14
9
8
4
A
B
C
D
E
F
G
H
I
J
K
L
M
Fig. 3: Accuracy of APPOSCOPY with a manual and an automated signature synthesized by ASTROID
A. Astroid vs. Manual Signatures
In our ﬁrst experiment, we compare the signatures syn-
thesized by ASTROID with manually written signatures used
for evaluating APPOSCOPY [2]. Since ASTROID generates
semantic malware signatures in the speciﬁcation language
used in APPOSCOPY, we use APPOSCOPY’s (exact) signature
matching algorithm to decide if an app matches a signature.
According a co-author of [2], it took several weeks to manually
construct signatures for all malware families in their dataset,
even with full knowledge of the malware family of each app
and the nature of the malice.
Accuracy on known malware. Since the authors of [2] have
manually written semantic signatures for all malware families
from the Android Malware Genome Project [23], we use
ASTROID to synthesize signatures for malware families from
this dataset. The table in Figure 3 shows the malware family
names and the number of samples for each family. For each
family F, we randomly select 5 samples from F and use
ASTROID to infer a signature for F from these samples. Since
different sets of samples may produce different signatures,
we run ASTROID with 11 different sets of randomly chosen
samples, and report our results for the signature that achieves
the median accuracy out of these 11 runs.
The plot in Figure 3 compares the accuracy ASTROID
against APPOSCOPY. Here, accuracy is the number of correctly
classiﬁed samples divided by the total number of samples.
For most malware families, ASTROID yields similar or bet-
ter accuracy than APPOSCOPY, and the overall accuracy of
ASTROID (93.8%) is higher than that of APPOSCOPY (90%).
Furthermore, for two malware families (speciﬁcally, Pjapps
and BaseBridge), ASTROID generates signiﬁcantly better sig-
natures than manually written ones. In summary, these results
show that ASTROID can achieve signiﬁcantly fewer false
negatives compared to APPOSCOPY.
ples. In contrast, APPOSCOPY reports two such false positives
(speciﬁcally, it misclassiﬁes two instances of other malware
families as belonging to Geinimi). Hence, the signatures syn-
thesized by ASTROID outperform manually written ones, both
in terms of accuracy as well as precision.
False positives on benign apps. To determine whether AS-
TROID produces false positives on benign apps, we analyze a
corpus of 10,495 apps downloaded from Google Play during
2013-14. According to [7], over 99.9% of these apps are
known to be benign. ASTROID reports a total of 8 mali-
cious apps (speciﬁcally, 2 instances each of DroidDream and
DroidDreamLight, 1 instance of Pjapps, and 3 instances of
DroidKungFu). We uploaded these 8 apps to VirusTotal [24],
which is a service that provides aggregated reports from
multiple anti-virus tools. VirusTotal agreed with ASTROID on
each of these 8 apps.2 In other words, ASTROID reported zero
false positives of either kind.
B. Varying the Number of Samples
We evaluate the performance and accuracy of ASTROID
with respect to the number of samples. In Figure 4, the blue
line plots signature inference time (in seconds) against the
number of samples. ASTROID is quite fast, taking less than two
seconds on average to infer a signature for a malware family.
Since manually writing malware signatures typically requires
several hours of human effort, we believe these statistics show
that ASTROID is quite practical.
The bars in Figure 4 show how the accuracy varies with
respect to the number of samples. ASTROID achieves 90.8%
accuracy using only two samples. When we increase the
number of samples to 5, ASTROID achieves 93.8% accuracy.
More samples do not seem to beneﬁt, likely due to imprecision
in the static analysis for ICCG constructions. These results
False positives on known malware. ASTROID reports zero
false positives on the Android Malware Genome Project sam-
2To account for the inclusion of some lower-quality AV tools in VirusTotal
results, we only consider VirusTotal to report an app as malware if the majority
(i.e., more than half) of AV tools classify the app to be malicious.
9
Accuracy
Time
DREBIN-FP0.01% MASSVET DREBIN-FP1% ASTROID
y
c
a
r
u
c
c
A
100
90
80
70
60
50
40
30
20
10
0
2
3
4
5
4
3
2
1
0
s
d
n
o
c
e
S
y
c
a
r
u
c
c
A
100
90
80
70
60
50
8
9
10
5
6
7
Number of samples
Fig. 4: Accuracy and time to synthesize the signature with
different number of samples
Fig. 5: Comparison between DREBIN with FP0.01% and
FP1%, MASSVET and ASTROID
show that ASTROID can infer very good signatures from a
very small number of malware samples.
C. Comparison Against Existing Tools
We compare ASTROID to two state-of-the-art malware
detection tools, namely DREBIN [3] (a learning-based mal-
ware detector) and MASSVET [15] (which detects repackaged
malware). We do not explicitly compare ASTROID against anti-
virus tools (e.g., VirusTotal) because ASTROID is an extension
of APPOSCOPY, which has already been shown to outperform
leading anti-virus tools in the presence of obfuscated malware.
Comparison to Drebin. We ﬁrst compare ASTROID against
DREBIN [3], a state-of-the-art malware detector based on
machine learning. Since our goal is to infer malware signatures
using few samples, we focus on the setting where only ﬁve
samples are available for the target family. We believe this
setting is very important because many malware families have
very few known samples. For example, in the Drebin dataset,
only 42% of families have more than 5 samples.
DREBIN takes as input a set of apps and extracts fea-
tures using static analysis. It then trains an SVM on these
feature vectors to classify apps as malicious or benign. Unlike
ASTROID, which classiﬁes each app into a speciﬁc malware
family, DREBIN only determines whether an app is malicious
or benign. Since the implementation of DREBIN is unavailable,
we used their publicly available feature vectors and train SVMs
based on the methodology described in [3] using the libsvm
library [25]. For each malware family F, we start with the
full DREBIN training set, but remove F except for 5 randomly
chosen samples from F (to achieve our small-sample setting).
In summary, we train DREBIN on a large number of benign
apps, all samples from other malware families, and 5 samples
from the target family. As with ASTROID, we train the SVM
with 11 different sets of randomly chosen samples and report
the median accuracy.
In order to perform an apples-to-apples comparison, we use
the 1,005 samples in the Android Malware Genome Project
10
for which DREBIN feature vectors are available.3 We use
two cutoffs to evaluate the accuracy on these apps. The ﬁrst
achieves a false positive rate of 1%, which is used in the orig-
inal evaluation [3]. However, 1% false positives still amounts
to more than 10,000 false positives on Google Play, which
contains more than a million apps. For a closer comparison to
ASTROID (which reports zero false positives on Google Play
apps), we choose a second cutoff that achieves a false positive
rate of 0.01%.
As shown in Figure 5, ASTROID outperforms DREBIN
in our small-sample setting: While ASTROID achieves 93.8%
accuracy, DREBIN yields 70.7% and 88.9% accuracy using
the 0.01% and 1% cutoff values respectively. We believe these
results indicate that ASTROID compares favorable with existing
learning-based detectors in the small-sample setting.
Comparison to MassVet. We also compare ASTROID against
MASSVET [15], a state-of-the-art tool for detecting repackaged
malware—i.e., malware produced by adding malicious compo-
nents to an existing benign app. MASSVET maintains a very
large database of existing apps and checks whether a given
app is a repackaged version of an existing one in the database.
The authors of MASSVET provide a public web service [26],
which we use to evaluate MASSVET.
As shown in Figure 5, the overall accuracy of MASSVET
on the Android Malware Genome Project is 84.0%, which
is signiﬁcantly lower than the 93.8% accuracy of ASTROID.
These false negatives occur because many malware samples
are not repackaged versions of existing benign apps, and even
repackaged malware may go undetected if the original app is
missing from their database.
We also evaluate the false positive rate of MASSVET on
a corpus of 503 benign apps from Google Play. 4 MASSVET
reports that 176 of these apps are malware, but all except one
3In this experiment, we use the Android Malware Genome Project dataset
as opposed to the DREBIN dataset because some of the malware families
in the DREBIN dataset are mislabeled, which is problematic for multi-label
classiﬁcation (as done by our technique). For example, some of the family
labels in the Drebin dataset do not match the labels in the Malware Genome
project.