# Title: Zero-Knowledge Proofs for Decision Tree Predictions and Accuracy

## Authors:
- Jiaheng Zhang
- Zhiyong Fang
- Yupeng Zhang
- Dawn Song

### DEGREE PROJECT IN COMPUTER SCIENCE AND ENGINEERING, SECOND CYCLE, 30 CREDITS
**STOCKHOLM, SWEDEN, 2020**

---

## Enhancing Decision Tree Accuracy and Compactness with Improved Categorical Split and Sampling Techniques

**GAËTAN MILLERAND**
**KTH Royal Institute of Technology**
**School of Electrical Engineering and Computer Science**

**Date:** June 12, 2020  
**Supervisor:** Pawel Herman  
**Examiner:** Erik Fransén  
**Host Company:** Craft ai

**Swedish Title:** Förbättrad noggranhet och ökad kompakthet för beslutsträd genom förbättrad kategorisk uppdelning och samplingsteknik

---

## Abstract

Decision trees are one of the most popular algorithms in the domain of explainable AI. Their structure allows for the induction of a set of decision rules that are easily understandable by humans. Current research focuses on improving decision tree performance or mapping other models into a tree. Trees generated by C4.5 or ID3 often have lower accuracy for classification tasks or higher mean square error for regression tasks compared to state-of-the-art models like XGBoost or deep neural networks. This thesis addresses this issue by proposing a new method based on data augmentation using advanced models, which outperforms traditional methods in terms of evaluation metrics. Additionally, the compactness of decision trees is a concern, as the depth increases, the set of rules becomes exponentially large, especially when splitting categorical attributes. Standard solutions for handling categorical values include converting them into dummy variables or splitting on each value, leading to complex models. This thesis conducts a comparative study of current methods for splitting categorical values in classification problems and proposes a new method for regression.

---

## Sammanfattning

Beslutsträd är en av de mest populära algoritmerna inom förklarbar AI. Genom dess struktur kan man enkelt framställa en uppsättning beslutsregler som är helt förståeliga för en vanlig användare. Forskningen fokuserar för närvarande på att förbättra beslutsträd eller kartlägga andra modeller i ett träd. Beslutsträd genererade av C4.5 eller ID3 har ofta lägre prestanda i form av noggrannhet för klassificeringsuppgifter eller högre medelkvadratfel för regressionsuppgifter jämfört med moderna modeller som XGBoost eller djupa neurala nätverk. Detta examensarbete tar upp detta problem genom att erbjuda en ny metod baserad på dataförstärkning med hjälp av moderna modeller, vilket ger bättre resultat enligt utvärderingsmått. Ett annat problem är beslutsträdets kompakthet, eftersom djupet ökar blir regeluppsättningen exponentiellt stor, särskilt vid kategoriska attribut. Standardlösningar för att hantera kategoriska värden inkluderar att omvandla dem till dummyvariabler eller dela på varje värde, vilket leder till komplexa modeller. I detta examensarbete görs en jämförande studie av nuvarande metoder för att dela kategoriska värden i klassificeringsproblem, och en ny metod föreslås för regression.

---

## Contents

1. **Introduction**
   - 1.1 A Brief Overview
   - 1.2 Aims and Scope
   - 1.3 Outline

2. **Background**
   - 2.1 Explainability
     - 2.1.1 Defining Explainability
     - 2.1.2 Explainability in Machine Learning
   - 2.2 C4.5 Algorithm
     - 2.2.1 Basis
     - 2.2.2 Possible Improvements
     - 2.2.3 Regression Trees
   - 2.3 Overview of Categorical Split Methods
     - 2.3.1 One-Hot Encoding or One Versus All
     - 2.3.2 Push Left By Purity
     - 2.3.3 Majority Class
     - 2.3.4 Covariance Split
   - 2.4 Performance Improvement
     - 2.4.1 Re-sampling
     - 2.4.2 Online Learning
     - 2.4.3 Ensemble Methods

3. **Methods and Evaluation**
   - 3.1 Key Performance Indicators
     - 3.1.1 Computational Performances
     - 3.1.2 Evaluation of Trained Models
     - 3.1.3 Compactness
     - 3.1.4 Statistical Background for Evaluation
   - 3.2 Experiments Framework
     - 3.2.1 Example of Use
     - 3.2.2 Datasets
   - 3.3 New Proposed Algorithms
     - 3.3.1 Adapting the Categorical Split to Regression
     - 3.3.2 Data Augmentation for Improving Decision Tree Predictive Power

4. **Results**
   - 4.1 Categorical Split
     - 4.1.1 Categorical Split on Classification
     - 4.1.2 Categorical Split on Regression Task
   - 4.2 Sampling Method and Data Augmentation

5. **Discussion**
   - 5.1 Decision Trees and Ethical Problems in Machine Learning
   - 5.2 Decision Tree Improvement in Machine Learning
   - 5.3 Transfer Learning in Machine Learning
   - 5.4 Sustainability
     - 5.4.1 The Need for Explainability
     - 5.4.2 Ecological Impact of Decision Trees
   - 5.5 Limitations
     - 5.5.1 Explainability
     - 5.5.2 Global Performance
   - 5.6 Future Work
     - 5.6.1 Categorical Split
     - 5.6.2 Toward Increasing Performance of Decision Trees

6. **Conclusion**

7. **Bibliography**

---

## Chapter 1: Introduction

### 1.1 A Brief Overview

Explainable AI (XAI) has become an important field in machine learning. Over the past two decades, with significant improvements in computational power, the trend has been to develop complex models with high performance. New models, such as Random Forest (2001) [1] and XGBoost (2016) [2], have emerged from decision trees and perform much better than earlier versions on almost every task. The last decade has also seen the rise of deep learning with neural networks, which are currently the state-of-the-art models for many tasks, including image classification [3], translation [4], and complex problem solving like Go or video games.

Despite their high performance, these models suffer from a significant disadvantage: it is difficult to understand how and why predictions are made. In Random Forest, predictions from weak models are aggregated into a single output, and adding a boosting component makes the model even harder to interpret. In deep learning, we optimize networks with millions of parameters, making it analytically challenging for even the best statisticians to fully comprehend the model.

In the early days, the issue of explainability was not a major concern due to the breakthroughs in model performance. However, in industrial applications, this issue is becoming increasingly important. Industries are integrating machine learning into their processes to save time, optimize workflows, and detect failures. For industry, it is crucial to understand why a failure occurred or why a particular action should be taken, rather than relying solely on raw accuracy. For example, if a factory director replaces an employee responsible for detecting supply-chain failures with a data scientist's model, without an explainable model, the same failures may continue to occur because no one will know what caused them or what changes need to be made to prevent them.

Explainable AI is now a real necessity, divided into two main parts:
- Local explainability
- Global explainability

Local explainability aims to explain why a specific sample is classified in a certain category. For black-box models, the LIME method [5] is commonly used, providing an estimation of how each feature contributes to the final prediction. Other methods, such as DeepLIFT, Shapley additive explanations, and classic Shapley Value estimation, are also mentioned by Lundberg [6].

This thesis focuses on decision trees, which can provide a global set of rules that completely describe the learned model, allowing for a full understanding of the underlying phenomenon. However, decision trees can still produce models that are hard to interpret due to a large number of nodes and poor performance in terms of accuracy.

### 1.2 Aims and Scope

The aim of this thesis is to improve the performance of decision trees while maintaining their simplicity. Two main problems are addressed:
1. Handling categorical values in decision trees for both regression and classification. With categorical input features, decision trees tend to become large and difficult to interpret. Finding a way to use these features without converting them into dummy variables or splitting on each possible value could significantly improve their compactness. Existing methods for classification transform categorical values into real scores that can be treated as continuous features. The goal is to adapt this approach to regression and create a new method to handle categorical values.
2. Improving overall performance to reduce the gap between decision trees and state-of-the-art models like Random Forest [1] and XGBoost [2].

The following research questions are addressed in this work:
- Is the aforementioned categorical split for classification adaptable to regression?
- What is the effect of this adaptation on the compactness and performance of decision trees?
- What is the quantitative improvement in predictive power that can be achieved by using sampling techniques based on state-of-the-art models like XGBoost and Random Forests?

### 1.3 Outline

Chapter 2 provides a summary of state-of-the-art methods for handling categorical features and improving decision tree accuracy. Chapter 3 introduces the new proposed methods and the evaluation framework. Chapter 4 presents the results of the experiments. Finally, Chapter 5 analyzes these results and provides recommendations for future work.

---

## Chapter 2: Background

Several implementations of decision trees exist in the literature, including ID3 [7], CART [8], and C4.5 [9]. For this thesis, we chose the C4.5 algorithm by Quinlan, which is widely used in various machine learning libraries such as scikit-learn [10] and caret [11], and is also used in the product of the company where the experiments were conducted. Decision trees were chosen due to their characteristic of being globally explainable.

### 2.1 Explainability

There is a growing interest in explainable AI, particularly in industries where it is important to understand how and why decisions are made by systems incorporating artificial intelligence. The first challenge in dealing with explainability is defining it.

#### 2.1.1 Defining Explainability

Miller [12] outlines approaches in social sciences to define explainability in a way that can be adapted to computer science. According to Miller, explainable AI is at the intersection of three areas:
- Social science
- Human-computer interaction
- Artificial intelligence

Miller's findings highlight that the main question is not just why a decision was made, but also what can be changed to achieve a different outcome. Humans do not always need a complete explanation; they typically retain only two or three reasons for an event, and the most likely explanation is not always chosen. Miller defines explainability as the answer to all possible "why" questions, including:
- Plain fact questions: Why does this object have this particular property?
- Contrastive questions based on the property: Why does this object have this particular property while it could have another property?
- Contrastive questions based on the object itself: Why does this object have this property and another object have a different property?
- Contrastive questions based on the evolution in time: Why does this object have this property at this particular time and another property at a different time?

Finding a simple, universally accepted definition of explainability is complex, and there is ongoing research to define what constitutes a good explanation.

#### 2.1.2 Explainability in Machine Learning

Defining a simple metric to measure the explainability of a model in machine learning is challenging due to the subjective nature of explanations. There are currently two main types of explainability in machine learning: local and global. Decision trees are a good example of a globally explainable model because they produce a set of decision rules that are human-understandable. There is also a trend towards local explainability with methods like SHAP [6] and LIME [5].

SHAP, based on game theory and Shapley values, estimates how much each feature contributes to the difference between a prediction and the average prediction. To compute the Shapley value of a feature, all other features in a sample are fixed, and only the value of the feature of interest is changed. The marginal gain from this change is then computed, and this process is repeated for different samples to estimate the contribution of a particular feature. The main drawback of this method is the exponential increase in computation time with the number of features.

LIME, on the other hand, fits a linear model around a sample to provide an explanation. For close points, a simple regression model can be locally accurate, and the weights of the regression can give the feature importance around a particular sample. The main drawback of LIME is that a model must be fitted for each sample, which can be time-consuming if an explanation is needed for each sample.

### 2.2 C4.5 Algorithm

We chose the C4.5 algorithm as the basis for developing our new splitting method. This algorithm is a common method for building decision trees and is widely used in many machine learning packages.

#### 2.2.1 Basis

The C4.5 algorithm uses a simple heuristic to minimize a metric that splits the data into several parts at each level of the tree. An exhaustive search is not feasible due to the combinatorial explosion of the problem. In C4.5, the optimized metric is entropy, which aims to reduce disorder by creating purer subsets.

Formally, let \( D \) be a dataset with \( n \) samples \(\{X_1, X_2, ..., X_n\}\) and their associated classes \(\{y_1, y_2, ..., y_n\} \in [1, m]^n\). Each \( X_k \) has \( p \) attributes, which can be either categorical or numerical. The probability of the \( i \)-th class is given by:

\[ p_i = \frac{\text{Card}(\{y_j = i, j \in [1, n]\})}{\text{Card}(D)} \]

The entropy of the dataset is then:

\[ H(D) = - \sum_{i=1}^{m} p_i \log(p_i) \]

The splitting criteria aim to find the best partition \( P = \{D_1, ..., D_l\} \) of \( D \) that maximizes the information gain:

\[ G(P) = \text{Card}(D) H(D) - \sum_{i=0}^{l} \text{Card}(D_i) H(D_i) \]

For each attribute \( A_i \) of the data, the algorithm finds the best split. If \( A_i \) is numerical, the samples are ordered according to \( A_i \), and the information gain is computed for the partition:

\[ P_{A_i}^t = \{D_1 = \{X_i = (x_{A1}, ..., x_{Ap}) | x_{Ai} < t\}, D_2 = \{X_i = (x_{A1}, ..., x_{Ap}) | x_{Ai} \geq t\}\}

All values of \( A_i \) are tried as a threshold \( t \) to split on. If the attribute is categorical, with values \( a_1, ..., a_l \), the gain is computed for the partition:

\[ P = \{D_i = \{X_i = (x_{A1}, ..., x_{Ap}) | x_{Ai} = a_i\}, i \in [1, l]\} \]

The attribute that produces the highest gain is selected, and these steps are repeated recursively until there is only one sample per node.

#### 2.2.2 Possible Improvements

Creating new nodes until the algorithm reaches a unique sample per leaf can lead to overfitting, with decision rules that do not generalize well to test samples. Some strategies to avoid overfitting include:
- Setting a maximum depth
- Setting a minimum gain to create a new split
- Setting a minimum number of samples per leaf
- Performing a pruning step

The C4.5 algorithm also includes a corrective term, the split information, to avoid favoring splits on categorical values, which can reduce the global disorder. This term is similar to the real entropy:

\[ \text{SplitInfo}(P) = - \sum_{i=1}^{l} \frac{\text{Card}(D_i)}{\text{Card}(D)} \log \left( \frac{\text{Card}(D_i)}{\text{Card}(D)} \right) \]

---

This revised version aims to make the text more coherent, clear, and professional. It organizes the content in a logical flow, ensuring that each section is well-defined and easy to follow.