title:Zero Knowledge Proofs for Decision Tree Predictions and Accuracy
author:Jiaheng Zhang and
Zhiyong Fang and
Yupeng Zhang and
Dawn Song
DEGREE PROJECT  COMPUTER SCIENCE AND ENGINEERING,
SECOND CYCLE, 30 CREDITS
STOCKHOLM SWEDEN2020
IN 
, 
Enhancing decision tree accuracy 
and compactness with improved 
categorical split and sampling 
techniques
GAËTAN MILLERAND
KTH ROYAL INSTITUTE OF TECHNOLOGY
SCHOOL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE
Enhancing decision tree
accuracy and compactness
with improved categorical
split and sampling techniques
GAETAN MILLERAND
Master in Computer Science
Date: June 12, 2020
Supervisor: Pawel Herman
Examiner: Erik Fransén
School of Electrical Engineering and Computer Science
Host company: Craft ai
Swedish title: Förbättrad noggranhet och ökad kompakthet för
beslutsträd genom förbättrad kategorisk uppdelning och
samplingsteknik
iii
Abstract
Decision tree is one of the most popular algorithms in the domain of
explainable AI. From its structure, it is simple to induce a set of deci-
sion rules which are totally understandable for a human. That is why
there is currently research on improving decision or mapping other
models into a tree. Decision trees generated by C4.5 or ID3 tree suffer
from two main issues. The ﬁrst one is that they often have lower per-
formances in term of accuracy for classiﬁcation tasks or mean square
error for regression tasks compared to state-of-the-art models like XG-
Boost or deep neural networks. On almost every task, there is an im-
portant gap between top models like XGboost and decision trees. This
thesis addresses this problem by providing a new method based on
data augmentation using state-of-the-art models which outperforms
the old ones regarding evaluation metrics. The second problem is the
compactness of the decision tree, as the depth increases the set of rules
becomes exponentially big, especially when the splitted attribute is a
categorical one. Standards solution to handle categorical values are to
turn them into dummy variables or to split on each value producing
complex models. A comparative study of current methods of splitting
categorical values in classiﬁcation problems is done in this thesis. A
new method is also studied in the case of regression.
iv
Sammanfattning
Beslutsträd är en av de mest populära algoritmerna i den förklar-
bara AI-domänen. I själva verket är det från dess struktur verkligen
enkelt att framställa en uppsättning beslutsregler som är helt förstå-
eliga för en vanlig användare. Därför forskas det för närvarande på
att förbättra beslut eller kartlägga andra modeller i ett träd. Besluts-
träd genererat av C4.5 eller ID3-träd lider av två huvudproblem. Den
första är att de ofta har lägre prestanda när det gäller noggrannhet
för klassiﬁceringsuppgifter eller medelkvadratfel för regressionsupp-
giftens noggrannhet jämfört med modernaste modeller som XGBoost
eller djupa neurala nätverk. I nästan varje uppgift ﬁnns det faktiskt ett
viktigt gap mellan toppmodeller som XGboost och beslutsträd. Det-
ta examensarbete tar upp detta problem genom att tillhandahålla en
ny metod baserad på dataförstärkning med hjälp av modernaste mo-
deller som överträffar de gamla när det gäller utvärderingsmätningar.
Det andra problemet är beslutsträdets kompakthet, allteftersom dju-
pet ökar, blir uppsättningen av regler exponentiellt stor, särskilt när
det delade attributet är kategoriskt. Standardlösning för att hantera
kategoriska värden är att förvandla dem till dummiesvariabler eller
dela på varje värde som producerar komplexa modeller. En jämföran-
de studie av nuvarande metoder för att dela kategoriska värden i klas-
siﬁceringsproblem görs i detta examensarbete, en ny metod studeras
också i fallet med regression.
Contents
1
Introduction
1.1 A brief overview . . . . . . . . . . . . . . . . . . . . . . .
1.2 Aims and scope . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Outline .
. . . . . . . . . . . . . . . . . . . . . . . . . .
.
.
1
1
3
3
2 Background
2.1 Explainability . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.1 Deﬁning explainability . . . . . . . . . . . . . . . .
2.1.2 Explainability in machine learning . . . . . . . . .
2.2 C4.5 algorithm . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.1 Basis . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.2 Possible improvement . . . . . . . . . . . . . . . .
2.2.3 Regression trees . . . . . . . . . . . . . . . . . . . .
2.3 Overview of categorical split methods . . . . . . . . . . .
4
4
4
5
6
6
7
8
9
2.3.1 One hot encoding or One Versus All . . . . . . . . 10
2.3.2 Push Left By Purity . . . . . . . . . . . . . . . . . . 11
2.3.3 Majority class . . . . . . . . . . . . . . . . . . . . . 13
2.3.4 Covariance split . . . . . . . . . . . . . . . . . . . . 13
2.4 Performance improvement . . . . . . . . . . . . . . . . . . 16
2.4.1 Re-sampling . . . . . . . . . . . . . . . . . . . . . . 16
2.4.2 Online learning . . . . . . . . . . . . . . . . . . . . 17
2.4.3 Ensemble methods . . . . . . . . . . . . . . . . . . 19
3 Methods and evaluation
23
3.1 Key performance indicators . . . . . . . . . . . . . . . . . 23
3.1.1 Computational performances . . . . . . . . . . . . 24
3.1.2 Evaluation of trained models . . . . . . . . . . . . 24
3.1.3 Compactness . . . . . . . . . . . . . . . . . . . . . 26
3.1.4
Statistical background for evaluation . . . . . . . 27
v
vi
CONTENTS
3.2 Experiments framework . . . . . . . . . . . . . . . . . . . 29
3.2.1 Example of use . . . . . . . . . . . . . . . . . . . . 29
3.2.2 Datasets . . . . . . . . . . . . . . . . . . . . . . . . 30
3.3 New proposed algorithms . . . . . . . . . . . . . . . . . . 31
3.3.1 Adapting the categorical split to regression . . . . 32
3.3.2 Data augmentation for improving decision tree
predictive power . . . . . . . . . . . . . . . . . . . 34
4 Results
4.1 Categorical split .
35
. . . . . . . . . . . . . . . . . . . . . . . 35
4.1.1 Categorical split on classiﬁcation . . . . . . . . . . 35
4.1.2 Categorical split on regression task . . . . . . . . . 42
Sampling method and data augmentation . . . . . . . . . 46
4.2
5 Discussion
53
5.1 Decision trees answer to ethical problems in machine
. .
. .
learning .
.
. . . . . . . . . . . . . . . . . . . . . . . 53
5.2 Decision trees improvement in machine learning . . . . . 54
5.3 Transfer learning in machine learning . . . . . . . . . . . 54
Sustainability .
. . . . . . . . . . . . . . . . . . . . . . . . 55
5.4
5.4.1 The need of explainability . . . . . . . . . . . . . . 55
5.4.2 Ecological impact of decision trees . . . . . . . . . 55
. . . . . . . . . . . . . . . . . . . . . . . . 56
5.5.1 Explainability . . . . . . . . . . . . . . . . . . . . . 56
5.5.2 Global performance . . . . . . . . . . . . . . . . . 57
. . . . . . . . . . . . . . . . . . . . . . . . 57
5.6.1 Categorical split . . . . . . . . . . . . . . . . . . . . 57
5.6.2 Toward increasing performance of decision tree . 58
5.6 Future work . .
5.5 Limitations .
. .
6 Conclusion
Bibliography
60
61
Chapter 1
Introduction
1.1 A brief overview
Explainable AI has become an important ﬁeld of machine learning.
In the last two decades, with the great improvement of computational
power, the trend was to develop complex models with great perfor-
mance. New models have emerged from decision trees like random
forest in 2001 [1] or XGBoost in 2016 [2]. These ones perform much
better than the ﬁrst version of decision trees on almost every task.
The last decade was also the advent of deep learning with neural net-
works. Neural networks are currently the state-of-the-art model in
many tasks:
• Image Classiﬁcation [3]
• Translation [4]
• Complex problem solving like Go or video games
Even if those models perform well, they suffer from an important dis-
advantage:
it is hard to understand how and why predictions are
made.
With random forest one aggregates predictions from weak models
in a unique output, moreover adding a boosting part makes the model
even harder to understand for a human. In deep learning we try to
optimize a network with millions of parameters. Even analytically a
model cannot be really understood by the best statistician in the world.
In its early days, the problem of explainability was not really impor-
tant because of the real breakthrough in model performances. But in
1
2
CHAPTER 1.
INTRODUCTION
industrial uses we are really faced to this issue. Industries more and
more try to integrate machine learning in their process to save time,
to optimize work-ﬂow or to detect failures. But what is important for
industry is to know why this failure happened or why one should do
this instead of that, the raw accuracy is only a performance indicator
among others. Imagine a simple example, a factory director decided
to replace an employee who was in charge of detecting supply-chain
failures because a data scientist promised him that his model will de-
tect failures better. Without an explainable model the same failures
will happen again and again because nobody will know what caused
it and what employees have to change so that this does not happen
again.
Explainable AI has become a real need. It is currently divided in
two parts :
• Local explainability
• Global explainability
The goal in local explainability is to explain why a sample has been
classiﬁed in a speciﬁc category. The current trend for black box model
is to use LIME method [5] which provides for each sample an esti-
mation of how much each features contribute to the ﬁnal prediction.
For example, with this method, in image classiﬁcation, it is possible to
know which part of the picture contributes to the result. Several other
methods exists, the main ones are mentioned by Lundberg [6] such as:
• DeepLIFT
• Shapley additive explanation
• Classic Shapley Value estimation
What is in our interest in this thesis is decision trees. It could be
interesting to have a global set of rules which describes completely the
learned models, which allows to understand completely the underly-
ing phenomenon. Nevertheless, decision tree has its drawbacks it can
still produce models that are hard to understand by a human with a
huge number of nodes and it could have poor performances in terms
of accuracy.
CHAPTER 1.
INTRODUCTION
3
1.2 Aims and scope
The aim of this thesis is to improve the performances of decision trees
while preserving their simplicity. Two problems are considered, the
ﬁrst one is to handle categorical values in decision trees in both re-
gression and classiﬁcation cases. With categorical input features, de-
cision trees tends to become huge, and thus hard to interpret. Find-
ing a way to use them without turning them into dummy variable or
splitting on each possible values could be a real improvement for their
compactness. Methods already exists for classiﬁcation transforming at
each level decision tree categorical values into real score that could be
treated as a continuous features. The idea is to adapt this approach
to regression problem and create a new method that helped decision
tree with categorical value. The second problem considered in the the-
sis is to improve their overall performance to reduce the gap between
them and state-of-the-art models such as Random Forest [1] or XG-
Boost [2]. In conclusion, the following research questions are tackled
in this work:
• Is the aforementioned categorical split on classiﬁcation is adapt-
able to regression?
• What is the effect of this adaptation on the compactness and per-
formance of decision trees?
• What is the quantitative improvement in terms of predictive power
enhancement of decision tree that we can beneﬁt from sampling
technique relying on the state-of-the-art models such as XGBoost
and Random Forests?
1.3 Outline
After a summary of all the state-of-the-art methods in Chapter 2 on
handling categorical features and improving decision trees accuracy,
the new proposed methods and the way to evaluate them will be de-
veloped in Chapter 3. The results of the experiments will be presented
in Chapter 4. Finally, these results will be analyzed, and some recom-
mendations for future work will be given in Chapter 5.
Chapter 2
Background
First of all, several implementations of decision trees currently exist
in the literature ID3 [7], CART [8], C4.5 [9]. For our purpose we choose
to use the C4.5 algorithm from Quinlan. Our choice was motivated
by the common use of this algorithm in various machine learning li-
brairies such as scikit-learn [10], caret [11] or inside the product of the
company I experimented for. It is then easier to adapt this algorithm
and modify part of interest: the splitting function for categorical fea-
tures. Decision trees have also be chosen due to their characteristic to
be globally explainable.
2.1 Explainability
There is currently a global interest in explainable AI. For example in
the industry it could be important to know how and why a decision
was taken by a system which integrates artiﬁcial intelligence. The ﬁrst
problem encountered when dealing with explainability is to deﬁne it.
2.1.1 Deﬁning explainability
Miller [12] outlined approaches in social sciences to give a deﬁnition
which can be adapted to computer sciences issues. In fact, according
to Miller explainable AI is at the meeting point of three areas:
• Social science
• Human-computer interaction
4
CHAPTER 2. BACKGROUND
5
• Artiﬁcial intelligence
According to his ﬁndings there were some questions about explain-
ability that were not taken into account when dealing with an AI sys-
tem. Often the main question is not only why a decision has been taken
but, for example, what can I change to have another outcome. Also a
human does not always need to have a complete explaination. Ac-
cording to sociologists a human only retains two or three reasons for
an event and above all it is not the most likely explaination which is
systematically chosen. So for them an explaination cannot be deﬁned
as "a simple presentation of associations of causes". In fact, it is much
more than that because it depends of a given context. The deﬁnition
Miller proposes in his article is that the explainability is the answer of
all possible why questions. According to him there are several possible
why questions:
• Plain fact question: Why does this object has this particular prop-
erty?
• Constrastive question based on the property : Why does this ob-
ject have this particular property while it could have that other
property?
• Contrastive question based on the object itself : Why does this
object have this property and another one has another property?
• Contrastive question based on the evolution in time : Why this
object have this property at this particular time and another one
at another time?
In conclusion ﬁnding a simple deﬁnition of explainability which is ac-
cepted by everyone is complex and there is still a lot of research to
deﬁne what is explainability and what is a good explaination.
2.1.2 Explainability in machine learning
Deﬁning a simple metric in Machine learning which measure the ex-
plainability power of a model is complex, mainly due to the subjective
character of an explaination. As seen in chapter 1 there are currently
two worlds of explainability in Machine Learning: local and global
explainability. Decision tree algorithm is a good example of a globally
explainable model because it produces a set of decision rules which are
6
CHAPTER 2. BACKGROUND
humanely understandable. There is also a big trend on local explain-
ability with the development of two methods SHAP [6] and LIME [5].
SHAP algorithm comes from game theory and is based on Shapley val-
ues. The goal is to estimate how much each feature contributes to the
difference between a prediction and the average prediction. To com-
pute the Shapley value of a feature the idea is to ﬁx all other features
in a sample and change only the value of the feature of interest. Then
for each change the marginal gain given the change of this feature is
computed. This evaluation is repeated for different samples to have an
accurate estimate of the contribution of a particular feature. The main
problem of this method is that if we want to measure the contribution
of each feature exactly the computation time of the Shapley value in-
crease exponentially with the number of features.
The other mainly used method, LIME, is based on another principle.
The main idea is to ﬁt a linear model around a sample on which we
want to have an explaination. In fact for close points it could be sim-
ple to ﬁt a simple regression models which can be locally accurate.
Given the weights of the regression it is possible to give the feature
importance around a particular sample. The main drawback of this
methods is that a model must be ﬁtted for each sample [5] and it could
be time consuming if one needs an explaination for each sample.
2.2 C4.5 algorithm
We chose to use the C4.5 algorithm as a basis to develop our new split-
ting method. This algorithm is one of the possible method to build a
decision tree and commonly used in many machine learning packages.
2.2.1 Basis
The C4.5 algorithm is based on a simple heuristic, at each level of the
tree it tries to minimize a metric that splits the data into several parts.
In fact, an exhaustive search is not possible due to the combinatorial
explosion of the problem. In the case of C4.5 the optimized metric is
the entropy. In a physical way of thinking, the idea is to reduce the
disorder at each level by creating purer subsets.
Formally, let D be a dataset with n samples {X1, X2, ..., Xn} and
their associated classes {y1, y2, ..., yn} ∈ [1, m]n. Each Xk has p at-
CHAPTER 2. BACKGROUND
7
tributes which can be either categorical or numerical. We can also
deﬁne, for the ith class (Card stands for Cardinal):
Card({yj = i, j ∈ [1, n]})
(2.1)
Card(D)
So the entropy of the dataset is given by:
pi =
H(D) = − m∑
i=1
pilog(pi)
(2.2)
Now that the global frame is deﬁned, it is possible to give a deﬁ-
nition of the splitting criteria. The idea is to ﬁnd the best partition
P = D1, ..., Dl of D which maximizes the information gain:
G(P) = Card(D)H(D) − l∑
i=0
Card(Di)H(Di)
(2.3)
Not all partitions are tried, they are computed following different rules
according to the type of the input feature. For each attribute Ai of
the data the algorithm ﬁnds the best split. If Ai is numerical the way
to proceed is to order samples according to Ai and then compute the
information gain for the partition:
PAit = {D1 = {Xi = (xA1, ..., xAp)|xAi < t}, D2 = {Xi = (xA1, ..., xAp)|xAi ≥ t}}
All Ai values are tried as a threshold t to split on.
If the attribute is categorical lets denote a1, ...al its values, then only
compute the gain for the following partition:
(2.4)
P = {Di = {Xi = (xA1, ..., xAp)|xAi = ai}, i ∈ [1, l]}
(2.5)
Then, the attribute which produces the best gain is selected. Finally,
these steps are repeated recursively on the subset until there is only
one sample in each node.
2.2.2 Possible improvement
Creating new nodes until the algorithm reaches a unique sample per
leaf can lead to overﬁtting with decision rules that do not generalize
to test samples. Some strategies exist to avoid it [13]:
• Set a maximum depth
8
CHAPTER 2. BACKGROUND
• Set a minimum gain to create a new split
• Set a minimum of sample per leaf
• Make a pruning step
The C4.5 algorithm also adds another feature. In fact, the well in-
formed reader may have noticed that the gain formalized in 2.2 and 2.3
tends to favor splitting on categorical values because it is more likely
to reduce the global disorder. In Quinlan’s book [9] it is suggested to
add a corrective term to avoid this trend: the split information. This
term looks like the real entropy:
(cid:18)Card(Di)
(cid:19)