1) Performs normalization and tokenization as de-
scribed in the core system in § II-A.
2) Moves an n-length window over the token stream.
the resulting n-tokens are
For each window,
hashed into a Bloom ﬁlter.
3) Stores the Bloom ﬁlter for each source ﬁle in
a raw data format. ReDeBug compresses Bloom
ﬁlters before storing to disk to save space and to
reduce the amount of disk access at query time.
While initially the above steps would be performed over
the entire distribution, day-to-day use would only run
the steps on modiﬁed ﬁles, e.g., as part of a revision
control check-in. In our experiments and implemen-
tation, we also concatenate per-ﬁle Bloom ﬁlters for
a project into a single bitmap before saving to disk.
This is purely an optimization; loading the single large
Bloom ﬁlter is much quicker than loading a bunch of
small Bloom ﬁlters on our machines.
• Step 2: Check for unpatched code copies. A user
obtains a uniﬁed diff software patch. ReDeBug then
automatically:
1) Extracts the original code snippet and the c tokens
of context information from the pre-patch source.
The mechanics for the code snippet are simple:
we extract lines preﬁxed by a “-” symbol in the
patch (lines preﬁxed with a “+” symbol are added
and thus not present in the original buggy code). If
context information is given in the patch, we use
51
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:47:03 UTC from IEEE Xplore.  Restrictions apply. 
Unpatched Code Clone Detection
Code Similarity Detection
Find Bugs
diff
svn commit
patch
Extract original
code snippet 
including context
Normalize/
Tokenize
Remove 
BF errors/ 
dead code
Unpatched 
Code Clone List
Build DB
(parallelized)
set membership 
test
Unpatched
code clones
Sources
Debian, Ubuntu, 
Kernel, ...
Normalize/
Tokenize
Hash n-tokens 
into Bloom filters 
DB
(Bloom filter 
per file)
Measure Similarity
Feature hashing
n-tokens
Similarity
DB
(Bitvector per 
file/function)
Compute
bitwise similarity
Pairwise 
Similarity
Measurement
Figure 2: The ReDeBug workﬂow.
that, else we obtain it from the original source
ﬁles.
2) Normalizes and tokenizes the extracted original
buggy code snippets. The normalization process is
the same as described in Step 1. For C, C++, Java,
and Python we remove any partial comments in
the c context lines since those languages support
multi-line comments and c context lines may have
only the head or tail part of multi-line comments.
3) Hashes the n-token window into a set of hashes
fp.
4) Performs a Bloom ﬁlter set membership test
on each hashed n-token window. We report an
unpatched code clone with ﬁle f if CONTAIN-
MENT(fp, f ) ≥ θ.
• Step 3: Post-process the reported clones. Given re-
ported unpatched code clones, ReDeBug automatically:
1) Performs an exact-matching test to remove Bloom
ﬁlter errors.
2) Excludes dead code which is not
included at
build time. For C we add assert statements
to the buggy code region, and compile with -g
option which allows us to check the presence
of assert statements using objdump -S. For
non-compiled languages this step is omitted.
3) ReDeBug reports only non-dead code to the user.
We use Bloom ﬁlters [8] because they are a space efﬁcient
randomized data structure used for set membership tests.
Suppose there is a data set S, i.e., in our setting a set of
n-tokens. A Bloom ﬁlter represents set S as a vector of m
bits initially all set to 0. To add an element x ∈ S to the
Bloom ﬁlter we ﬁrst apply k independent hash functions of
range {1..m}. For each hash h(x) =i, we set the i’th bit
of the bit vector to 1. In a Bloom ﬁlter, to test if an element
of y ∈ S, we again apply the k hash functions and check
if all the bits are 1. If at least one of the hashed bits is 0,
then we return y /∈ S. If all bits are set to 1, then we return
y ∈ S.
Bloom ﬁlters have one-sided error for set membership
tests. A false positive occurs when the set membership test
returns x ∈ S when x is not really in S. False positives occur
because of collisions in hash functions. The false positive
rate of the Bloom ﬁlter depends on the size of the bit array
(m), the number of hash functions (k), and the number of
elements in S (N). The probability of a false positive can be
made negligible by an appropriate choice of parameters [9].
Bloom ﬁlters have no false negatives. In our setting, the
one-sided error means we may mistakenly say that an n-
token is present in the set when it is not. The probability of
this happening can be made arbitrarily low with appropriate
parameter selection, e.g., it is 0.3% in our implementation.
In our evaluation, we only report an unpatched code
clone if a ﬁle contains all context and all original n-tokens
as described above, i.e., θ = 1. This is a conservative
conﬁguration.
C. Code Similarity Detection
ReDeBug can also be used to measure the amount of code
clones. In this mode ReDeBug uses the SIMILARITY metric
between code pairs. The main issue when performing simi-
larity measurements is the cost of the pairwise comparisons.
52
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:47:03 UTC from IEEE Xplore.  Restrictions apply. 
A standard approach for comparing all N ﬁles requires that
we compare ﬁle 1 to ﬁle 2 through N, ﬁle 2 to ﬁle 3 through
N, and so on with a total of N (N−1)
comparisons.
In order to make pairwise comparisons cheap, we need
to implement the Jaccard SIMILARITY set-wise similarity
comparisons efﬁciently. Most standard libraries implement
the Jaccard index directly as a set operation on set data con-
tainer classes, e.g., as done in SimMetrics [4]. The problem
is that set data structures are L1/L2 cache inefﬁcient.
2
ReDeBug uses bitvectors in order to speed up pair-wise
comparisons. The idea is that the bitvector operation well-
approximates a true Jaccard based upon the original sets.
However, the Bloom ﬁlters we described above are designed
for set membership, not calculating the Jaccard. Jang et al.
noticed that as the number of hash functions used in a Bloom
ﬁlter grows, so does the error in the approximation [22]. The
solution is to use a single hash function when creating the
bitvectors for SIMILARITY, not multiple hash functions. A
single hash function approach is called feature hashing. Jang
et al. [22] show that feature hashing well approximates the
true Jaccard in both theory and practice (e.g., within 99.99%
of the true value with proper parameter selection); we adopt
their technique in ReDeBug. A side beneﬁt is that a single
hash is faster than multiple hashes used in typical Bloom
ﬁlter operations.
ReDeBug encodes all elements in a bitvector using fea-
ture hashing. ReDeBug then calculates the distance using
SIMILARITYbv instead of Equation 2,
SIMILARITYbv(va, vb) =
S(va ∧ vb)
S(va ∨ vb)
where vi is the bitvector representation of the feature set for
ﬁle i and S(·) counts the number of set bits.
The complete system for computing similarity for OS-
distributions becomes:
1) Obtain all source code. For Debian and Ubuntu, this
is done with apt. For SourceForge, we crawled all
Subversion, CVS and Git directories.
§ II-A.
2) For each ﬁle, normalize and tokenize as described in
3) For each n-length token sequence i, compute h(i) = d
and set the d’th bit in the respective ﬁle bitvector m.
4) Compute SIMILARITYbv between each pair of bitvec-
tors.
The result is a pairwise similarity measurement between
ﬁles. In a development environment we would only return
those with a similarity greater than θ.
Similarity at different granularities: The source ﬁle level
provides a relatively coarse granularity of measurement
between code bases. ReDeBug can extract functions from
ﬁles, and compute similarity based upon functions using
the same algorithm. ReDeBug can also calculate the total
fraction of unique n-tokens. The total fraction of n-tokens
suggests the number of unique code fragments found. In our
evaluation, we measure both for our data sets (§ III-H).
D. Similarity vs. Bug Finding
The algorithm for ﬁnding bugs is similar to that for
similarity detection, with a few exceptions. First, we pre-
process the source to build a database. Patches are queried
against the database while every pair of ﬁles is compared
when computing similarity. The advantage of this is that the
time to build a database and to query bugs increases only
linearly as we have more ﬁles and patches. The time for
similarity detection quadratically goes up with more ﬁles
due to N 2 comparisons.
The second difference is that we use Bloom ﬁlters for un-
patched code clone detection, while we used feature hashing
for similarity detection. We originally wanted a system that
used a single algorithm. While conceptually more elegant,
such a design wasn’t optimal in either scenario. For example,
if we had based our similarity metric on Bloom ﬁlters with
multiple hash functions, we would have a larger error rate
than with feature hashing due to extra collisions from the
extra hash functions. If we had used feature hashing instead
of Bloom ﬁlters, we would again have lower accuracy when
performing set membership tests. While this may seem like a
subtle difference, previous theoretical and empirical analysis
also back up the difference between feature hashing and
Bloom ﬁlters [22, 29].
Luckily, the internals of implementing both feature hash-
ing and Bloom ﬁlters is almost identical. In one case we use
a single hash function and have a distance metric interface,
and in the other we use multiple hash functions and export
a set membership interface.
III. IMPLEMENTATION & EVALUATION
A. Implementation
ReDeBug is implemented in about 1000 lines of C
code and 250 lines of Python. Normalization is mod-
ularized within the Python code. We use the QuickLZ
library [3] to perform compression/decompression while
setting QLZ_COMPRESSION_LEVEL to 3 for faster decom-
pression speed.
B. Unpatched Code Clone Detection Experimental Setup
System Environment: We performed all experiments to
ﬁnd unpatched code clones (both building and querying the
database) on a desktop machine running Linux 2.6.38 (3.4
GHz Intel Core i7 CPU, 8GB memory, 256 GB SSD drive).
We utilized 8 threads to build a DB and to query bugs.
Dataset: We collected our source code dataset twice: early
in 2011 and late in 2011. We ﬁrst collected our Early 2011
Dataset (Σ1) in January/March 2011: all source packages for
Debian 5.0 Lenny and Ubuntu 10.10 Maverick, as well as
all public SourceForge C/C++ projects using version control
systems such as Subversion, CVS and Git, and the Linux
53
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:47:03 UTC from IEEE Xplore.  Restrictions apply. 
Distributions
Debian Lenny
Ubuntu Maverick
Linux Kernel 2.6.37.4
SourceForge (C/C++)
Debian Squeeze
Ubuntu Oneiric
Total
Lines of Code Date
Collected
257,796,235
Jan. 2011
245,237,215 Mar. 2011
8,968,871 Mar. 2011
922,424,743 Mar. 2011
348,754,939 Nov. 2011
397,399,865 Nov. 2011
2,180,581,868
-
Early
2011
(Σ1)
Late
2011
(Σ2)
Table I: Source Dataset
Dataset
# ﬁles
Patches before 2011 (δ1)
Patches in 2011 (δ2)
# diffs Date Released
2001∼2010
2011
-
1,079
555
1,634
274
102
376
Total
Table II: Security-related Patch Dataset
kernel v2.6.37.4. In the SourceForge data set we removed
identiﬁable non-active code branches such as branches
and tags directories. In November 2011, we prepared our
Late 2011 Dataset (Σ2): all source packages for Debian
6.0 Squeeze and Ubuntu 11.10 Oneiric. Table I shows the
detailed breakup of the dataset. The data set consists of a
large number of projects written in a wealth of languages
including C, C++, Java, Shell, Perl, Python, Ruby, and PHP.
In order to ﬁnd notable bugs, we collected security-related
patches from the Debian/Ubuntu security advisory which
has the links to the corresponding packages and patch-
es/diffs. We performed our experiments on 376 security-
related patches consisting of 1,634 diffs. We only included
the patches whose related CVE numbers are recognizable
by the patch ﬁle names. As described in Table II, we
downloaded security-related patches released before 2011
(δ1) which were available at the time of collecting Σ1,
and patches released in 2011 (δ2) which were distributed
between Σ1 and Σ2.
In the original
for Debian and
Ubuntu there are a number of existing patches (e.g.,
debian/patches/) that can be applied during a build;
we applied these patches as well. As a result, the packages
were patched current up to security advisories on the down-
load date. Since we downloaded the SourceForge packages
via revision control systems, we assume all patches were
already applied.
source packages
The experiments show the number of duplicate buggy
code segments that are still
likely vulnerable. We have
veriﬁed the presence of all reported unpatched code clones,
i.e., clones of the exact same buggy code,
to conﬁrm
the ReDeBug implementation is correct. We discuss this
measurement in § III-F.
 25
 20
 15
 10
 5
)
s
r
u
o
h
(
e
s
a
b
a
t
a
d
a
d
l
i
u
b
o
t
e
m
T
i
Ubuntu Oneiric
Debian Squeeze
Debian Lenny
Ubuntu Maverick
Linux Kernel
 0
9.0M
349M
245M
258M
397M
                    Lines of Code
Source Forge
922M
Figure 3: Time to build a database with various sizes of
dataset
Distributions
DB Size
Projects #
Files #
Debian Lenny
Ubuntu Maverick
Linux Kernel 2.6.37.4
SourceForge (C/C++)
Debian Squeeze
Ubuntu Oneiric
6.0GB
5.6GB
344MB
29GB
8.2GB
9.8GB
10,699
11,237
-
30,437
14,977
18,240
1,155,594
1,067,579
57,653