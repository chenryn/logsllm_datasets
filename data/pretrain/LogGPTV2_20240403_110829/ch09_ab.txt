最后，1000 RPS 可以视为显示当前服务容量的值。如果服务或硬件的性能发生变化，SLA 中的值将相应地更新。如果我们想要明确表明要限制传入流量，我们可以稍微调整 SLA，并说每个客户都拥有其专用的吞吐量容量：
每个日历年 99.9% 的请求应在 200 毫秒内送达，单个用户帐户的请求数每秒不应超过 1000 次。
现在，这个数字在用户方面是可衡量的，并且独立于硬件和软件功能。我们可以计算我们可以服务多少用户，并能够添加更多容量和更改软件，而无需更改 SLA。 
为内部组件建立 SLA
  如果服务不是面向最终客户的服务该怎么办？它也应该有自己的 SLA 吗？为了澄清这个问题的答案（是的），让我们用一个虚构的消息分发服务（请参阅#message_distribution_service_components_r）做例子，该服务由以下四个主要组件组成：
数据接收器
接受和注册消息
数据转变器
使用来自不同外部源的数据调整消息内容
分发器
将消息传递到多个终结点
使用者
通过“发布者+订阅者”模型从终结点接收数据将消息传递到多个终结点
使用者
通过“发布者+订阅者”模型从终结点接收数据
消息分发服务组件关系
目前，此系统工作正常：无错误，无报警。
一天，一位顶级项目经理来到我们这里，提出如下要求：“我们正在从事的一个项目现在使用消息分发服务。我们时不时需要在短时间内发送大量数据。我们能否按现在的方式使用这项服务，或者我们应该如何调整其容量来处理新的通信流量？”
让我们逐步解决这个问题。拥有数据量的实际数字很方便。假设预测的流量将比已知最大峰值时间值高三倍。然而，这些知识不会使我们清楚地了解我们能否处理这种增长。原因很简单：即使我们知道服务在高峰时段管理的数据量，我们仍需要知道达到服务容量限制的 故障点，以便能够将其与预测进行比较。
我们的消息分发服务有几个组件。最慢的组件是决定整体服务能力的组件：“链的强度由最薄弱的环节决定。”因此，现在我们需要花一些时间来建立性能测试环境并识别故障点在性能测试期间，我们逐渐增加流量。故障点是响应质量低于要求规定的那个阈值。服务开始响应较慢，甚至开始产生错误。分别确定哪个组件是瓶颈。到目前为止，我们有数据可以告诉我们有关流量处理可能性的信息：可以无错误地使用的消息数。如果一切正常，我们准备继续宣布不需要任何更改。如果我们事先计划好这种增长，那将是一个很好的情况。否则，如果我们可以在不重新缩放的情况下提供三倍以上的流量，并且服务由多个主机（最小的资源单位）组成，则可能意味着到目前为止，所有已分配资源的三分之二从未使用过，这可能会引发一个问题成本效益。但这是一个不同的故事，希望事实并非如此。
测试结果和预测的比较表明，我们只能处理一半的预期增长。现在，根据新的请求，我们至少需要加倍吞吐量。
这是一个相对简单的任务。传统上，要解决此类问题，SysAdmins 会确定使用最多的关键系统资源（如 CPU、内存、磁盘、网络 I/O 等），而实际上需要加倍。然后，他们相应地请求新的硬件来满足需求。这种方法没有错。然而，还有别的东西位于其范围之外，因此没有计算在内。从 SRE 的角度来看，我们缺少了其他特定于服务的限制，没有这些限制，我们就无法准确扩展我们的服务。
对于我们的假想服务，目标是在合理的时间内将一些消息从入口点传递到最终使用者。每个组件只有有限的时间，它可以停留在每个组件内，然后再发送到下一个组件。这是我们的具体限制：每个组件的时间限制。如果没有这样的限制，在流量增长期间，某些消息可能会延迟或长时间停留在中间的某个地方。此问题将不可见；因此，将不显示此问题。延迟消息不会标记为有问题，因为它们不会引发任何错误。它们不会仅仅因为我们不知道什么是“好”和什么不是而引发错误。
为了克服这种不便，我们应该用准确的时间来表达整体服务完成时间。然后，我们应该将其分配到特定组件，并再次以确切的时间值定义它。在这里，我们纠正了性能测试过程故障点的含义。我们需要的不是我们可以接收的不会出错的消息数，而是此组件能够传递多少条消息，以便处理每条消息的时间不超过定义的时间限制。建立这些限制通常会进一步降低以前计算的容量。每个组件的时间约束其实就是所谓 SLI（"通过时间"），其值是为每个组件单独确定的。此外，每个组件都有其他有具体的指标，如“可用性”和“响应时间”。组合后，它们形成每个组件的 SLA。
现在我们有一个两种性质的 SLA。第一个是前面提到的，它涵盖了整个服务，并接触到服务的客户。我们将用它来观察整体服务行为。
另一个是每个组件的 SLA。它不呈现给服务的最终用户；但是，它允许我们确定组件之间的关系，帮助快速识别哪个组件给整体服务造成困难，并用于精确缩放组件。在性能测试期间，其 SLI 的值可以（而且应该）用作识别正确断点的限制。
对于我们的玩具服务，故障点将标识为组件满足以下要求的最大吞吐量：
100% 的请求获得响应的速度超过特定阈值。
未发生错误。
100% 的消息发送到下一个组件，以便每个消息都满足其时间限制。
请记住，必须同时验证所有三个条件，因为接收消息和进一步发送消息可能由不同的进程异步完成，并且一个指标的良好条件并不意味着另一个指标的状态相同。回到我们的缩放。我们之前说过，我们需要加倍吞吐量。现在，此语句需要修改，因为我们通过追加新要求更改了测试过程，结果可能和以前不同。
预测的输入将由第一个“数据接收器”组件接收。了解预期流量和组件性能指标的具体值，我们最终可以估计所需的容量调整。我们可以计算我们现在的潜在最大容量，处理预期最大流量所需的容量，然后找到两者之间的增量。但是，这仅对“数据接收器”适用，因为预测的输入仅定义此组件的大小。
不要忘记，例如，组件吞吐量翻倍并不一定意味着我们需要将其队列增加一倍。并非所有组件都线性扩展，因此对于其中一些组件，只需添加几台计算机就足够了。对于其他组件，可能需要添加多于两倍的主机数。
但是下一个组件呢？以同样的方式，我们可以计算其当前容量，但我们不知道它将从上一个组件接收多少数据。我们可以假设其流量会根据前一个组件输入的增长而增加，但这只是一个假设，可能与现实无关。了解组件的作用，我们可以执行一组实验，以确定组件的输入和输出之间的比率。
现在，我们可以从最初预测的量中估算每个单个组件的输入数据量。例如，如果“数据转换”组件的输入/输出比为 1：2，“数据接收器”的输入/输出比为 1：1，则意味着对于每个原始一兆字节的消息，“分发服务器”将获得两兆字节。
 在继续之前，我们应该多说几句关于瓶颈组件的话。我们跟踪它只有一个原因：瓶颈是流量增长时会遇到困难的第一个组件。换句话说，如果此组件过载，则整个服务有问题。如果我们将每个组件扩展到所需的最佳性能和吞吐量，为什么会出现瓶颈？
有两个因素。第一，某些组件会有点过度缩放。例如，如果单个主机每分钟可以处理 100 万条消息，但流量为 110 万，则添加第二个主机将保持其 90% 的容量（或总体 45%）未使用。并且此组件将正常，直到流量达到每分钟 200 万条消息的速率。但与此相反，服务的另一部分可能已经利用其容量高达 95% 来处理相同的 110 万条消息。第二个因素是输入/输出比。如果对于第一个组件，传入流量的增长很小，但速率很高，则对下一个组件的输入可能会显著增加，并且可能需要我们注意。瓶颈是与其他组件相比，当前负载和最大可用容量之间的差别最小的那个，因此对流量变化非常敏感，并且将是首先超载的组件。
回到我们的故事，为了说明瓶颈识别的工作原理，让我们想象一下预测的负载峰值为 1000 RPS；重新缩放后组件的容量如下：