3
4
5
7
8
9
10
11
12
13 return ˜f ;
4.4 Training Adversarial Models
Putting everything together, Algorithm 2 sketches the process of
training the adversarial model ˜f from its genuine counterpart f ,
which iteratively selects and modifies a set of parameters at a des-
ignated layer l of f .
At each iteration, it first runs back-propagation and finds the set
of salient features with respect to the current model ˜f (line 3-4);
th percentile of
then, for the l
absolute positive impact and the (100 − θ)th percentile of negative
impact (line 6-8); for each parameter w, it checks whether it satisfies
the constraints of positive and negative impact (line 10); if so, it
updates w according to the aggregated gradient ϕ+(w) to increase
the likelihood of x− being misclassified to x+’s class (line 11). This
process repeats until (i) the feature vector ˜f (x−) becomes stationary
between two iterations, indicating that the training has converged,
or (ii) no more qualified parameters are found.
th layer of ˜f , it first computes the θ
The setting of key parameters is discussed in § 10.3.
4.5 Extensions
Multiple Triggers. We now generalize the attacks with a single
trigger to the case of multiple triggers {x−}. A naïve way is to se-
quentially apply Algorithm 2 on each trigger of {x−}. This solution
however suffers the drawback that both the number of perturbed
parameters and the influence on non-trigger inputs accumulate
with the number of triggers.
We overcome this limitation by introducing the definition of
multi-trigger positive impact of a parameter w:
multi(w) =
+
ϕ

(cid:169)(cid:173)(cid:171)
x−
i∈Ix+
· si(x+) − 
i∈Ix−
∂ fi(x−)
∂w
∂ fi(x−)
∂w
· si(x−)(cid:170)(cid:174)(cid:172)
which quantifies w’s overall influence on these triggers. By sub-
stituting the single-trigger positive impact measure with its multi-
trigger version, Algorithm 2 can be readily generalized to crafting
adversarial models targeting multiple inputs.
Untargeted Attacks. In the second extension, we consider the
scenario wherein the adversary has no access to any reference input
x+. In general, without x+, the adversary is only able to perform
untargeted attacks (except for the case of binary classification), in
which the goal is to simply force x− to be misclassified, without
specific targeted classes.
In untargeted attacks, we re-define the positive impact as:
+un(w) = − 
ϕ
i∈Ix−
∂ fi(x−)
∂w
· si(x−)
(6)
which measures w’s importance with respect to x−’s current classi-
fication. Without x+, no negative impact is defined.
Under this setting, Algorithm 2 essentially minimizes x−’s prob-
ability with respect to its ground-truth class.
5 OVERVIEW OF EVALUATION
Next we empirically evaluate the practicality of model-reuse at-
tacks. We explore four deep learning systems used in security-
critical domains, including skin cancer screening [20], speech recog-
nition [43], face verification [55], and autonomous steering [11]. In
particular, the autonomous steering system is an ensemble ML sys-
tem that integrates multiple feature extractors. The details of the
involved DNN models are summarized in § 10.1.
Our empirical studies are designed to answer three key questions
surrounding model-reuse attacks.
• Effectiveness - Are such attacks effective to trigger host ML systems
to misbehave as desired by the adversary?
• Evasiveness - Are such attack evasive with respect to the system
developers’ inspection?
• Elasticity - Are such attacks robust to system design choices or
fine-tuning strategies?
5.1 Overall Setting
Baseline Systems. In each application, we first build a baseline
system д ◦ f upon the genuine feature extractor f and the classifier
(or regressor) д. We divide the data in the target domain into two
parts, T (80%) for system fine-tuning and V (20%) for inference. In
our experiments, the fine-tuning uses the Adam optimizer with the
default setting as: learning rate = 10−3, β1 = 0.9, and β2 = 0.99.
Attacks. In each trial, among the inputs in the inference set V that
are correctly classified by the baseline system д ◦ f , we randomly
sample one input x− as the adversary’s trigger. Let “−” be x−’s
ground-truth class. In targeted attacks, we randomly pick another
input x+ as the adversary’s reference input and its class “+” as
the desired class. By applying Algorithm 2, we craft an adversarial
model ˜f to embed the trigger x− (and its neighbors). Upon ˜f , we
build an infected system д◦ ˜f . We then compare the infected system
д ◦ ˜f and the baseline system д ◦ f from multiple perspectives.
In each set of experiments, we sample 100 triggers and 10 seman-
tic neighbors for each trigger (see § 4.1), which together form the
testing set. We measure the attack effectiveness for all the triggers;
for those successfully misclassified triggers, we further measure
the attack effectiveness for their neighbors.
Parameters. We consider a variety of scenarios by varying the
following parameters. (1) θ - the parameter selection threshold, (2) λ
- the perturbation magnitude, (3) ntuning - the number of fine-tuning
epochs, (4) partial-system tuning or full-system tuning, (5) ntrigger -
the number of embedded triggers, (6) l - the perturbation layer, and
(7) д - the classifier (or regressor) design.
Metrics. To evaluate the effectiveness of forcing host systems to
misbehave in a predictable manner, we use two metrics:
• (i) Attack success rate, which quantifies the likelihood that the
host system is triggered to misclassify the targeted input x− to
the class “+” designated by the adversary:
Attack Success Rate =
# successful misclassifications
# attack trials
• (ii) Misclassification confidence, which is the probability of the
class “+” predicted by the host system with respect to x−. In the
case of DNNs, it is typically computed as the probability assigned
to “+” by the softmax function in the last layer.
Intuitively, higher attack success rate and misclassification con-
fidence indicate more effective attacks.
To evaluate the attack evasiveness, we measure how discernible
the adversarial model ˜f is from its genuine counterpart f in both
the source domain (in which f is trained) and the target domain (to
which f is transferred to). Specifically, we compare the accuracy of
the two systems based on f and ˜f respectively. For example, in the
case of skin cancer screening [20], f is pre-trained on the ImageNet
dataset and is then transferred to the ISIC dataset; we thus evaluate
the performance of systems built upon f and ˜f respectively using
the ImageNet and ISIC datasets.
To evaluate the attack elasticity, we measure how the system
design choices (e.g., the classifier architecture) and fine-tuning
strategies (e.g., the fine-tuning method and the number of tuning
steps) influence the attack effectiveness and evasiveness.
5.2 Summary of Results
We highlight some of our findings here.
• Effectiveness – In all three cases, under proper parameter setting,
model-reuse attacks are able to trigger the host ML systems to
misclassify the targeted inputs with success rate above 96% and
misclassification confidence above 0.865, even after intensive full-
system tuning (e.g., 500 epochs).
• Evasiveness – The adversarial models and their genuine counter-
parts are fairly indiscernible. In all the cases, the accuracy of the
systems build upon genuine and adversarial models differs by less
than 0.2% and 0.6% in the source and target domains respectively.
Due to the inherent randomness of DNN training (e.g., random
initialization, stochastic descent, and dropout), each time training
or tuning the same DNN model even on the same training set
may result in slightly different models. Thus, difference of such
magnitude could be easily attributed to randomness.
• Elasticity – Model-reuse attacks are insensitive to various system
design choices or fine-tuning strategies. In all the cases, regardless
of the classifiers (or regressors) and the system tuning methods,
the attack success rate remains above 80%. Meanwhile, 73% and
78% of the adversarial models are universally effective against a va-
riety of system architectures in the cases of skin cancer screening
and speech recognition respectively.
6 ATTACKING INDIVIDUAL SYSTEMS
We first apply model-reuse attacks on individual ML systems, each
integrating one feature extractor and one classifier.
6.1 Case Study I: Skin Cancer Screening
In [20], using a pre-trained Inception.v3 model [57], Esteva et al.
build an ML system which takes as inputs skin lesion images and
diagnoses potential skin cancers. It is reported that the system
achieved 72.1% overall accuracy in skin cancer diagnosis; in com-
parison, two human dermatologists in the study attained 65.56%
and 66.0% accuracy respectively.
Figure 4: Decomposition of Inception.v3 model (“n×” de-
notes a sequence of n blocks).
Experimental Setting. Following the setting of [20], we use an
Inception.v3 model, which is pre-trained on the ImageNet dataset
and achieves 76.0% top-1 accuracy on the validation set. As shown
in Figure 4, the feature extractor of the model is reused in building
the skin cancer screening system: it is paired with a classifier (1 FC
layer + 1 SM layer) to form the end-to-end system.
Figure 5: Sample skin lesion images of three diseases.
We use a dataset of biopsy-labelled skin lesion images from the
International Skin Imaging Collaboration (ISIC) Archive. Similar
to [20], we categorize the images using a three-disease taxonomy:
malignant, melanocytic, and epidermal, which constitute 815, 2,088,
and 336 images respectively. Figure 5 shows one sample from each
category. We split the dataset into 80% for system fine-tuning and
ConvolutionAvgPoolMaxPoolConcatenationDropoutFully ConnectedSoftmax3xFeature Extractor fClassiﬁer g4x2xmalignantepidermalmelanocytic20% for inference. After fine-tuning, the baseline system attains
77.2% overall accuracy, which is comparable with [20].
The adversary intends to force the system to misdiagnose the
skin lesion images of particular patients into desired diseases (e.g.,
from “malignant” to “epidermal”).
Effectiveness
Attack
Success Rate
Misclassification
Confidence
∆Accuracy
(ImageNet)
Evasiveness
∆Accuracy
0.796
0.816
0.865
0.883
0.2%
0.2%
0.1%
0.1%
(ISIC)
1.2%
0.7%
0.3%
0.2%
θ
0.65
0.80
0.95
0.99
80%/100%
98%/100%
98%/100%
76%/100%
Table 2. Impact of parameter selection threshold θ (x%/y%
indicates that the attack success rates of trigger inputs and
their neighbors are x% and y% respectively).
Parameter Selection. Table 2 summarizes the influence of param-
eter selection threshold θ on the attack effectiveness and evasive-
ness. Observe that under proper setting (e.g., θ = 0.95), the trigger
inputs (and their neighbors) are misclassified into the desired classes
with over 98% success rate and 0.883 confidence. However, when
θ = 0.99, the attack success rate drops sharply to 76%. This can be
explained by that with overly large θ, Algorithm 2 can not find a
sufficient number of parameters for perturbation. Meanwhile, the
attack evasiveness increases monotonically with θ. For instance, on
the ISIC dataset, the accuracy gap between the adversarial models
and genuine models shrinks from 1.2% to 0.2% as θ increases from
0.65 to 0.99.
Figure 6: Impact of perturbation magnitude λ.
Perturbation Magnitude. Next we measure how the attack effec-
tiveness and evasiveness vary with the perturbation magnitude λ.
To do so, instead of setting λ dynamically as sketched in § 10.3, we
fix λ throughout the training of adversarial models. The results are
shown in Figure 6. Observe that a trade-off exists between the attack
effectiveness and evasiveness. With proper parameter setting (e.g.,
λ ≤ 2 × 10−3), larger perturbation magnitude leads to higher attack
success rate, but at the cost of accuracy decrease, especially on the
ISIC dataset. Therefore, the adversary needs to balance the two
factors by properly configuring λ. In the following, we set λ = 10−3
by default.
Also note that due to the limited data (e.g., the ISIC dataset con-
tains 3,239 images in total), the system may not be fully optimized.
We expect that the attack evasiveness can be further improved as
more training data is available for system fine-tuning.
System Fine-Tuning. Next we show that model-reuse attacks are
insensitive to system fine-tuning strategies. Figure 7 (a) shows the
Figure 7: Impact of system fine-tuning.
attack effectiveness as a function of the number of system tuning
epochs (ntuning). For ntuning ≥ 400, both the attack success rate and
misclassification confidence reach a stable level (around 96% and
0.865). This convergence is also observed in the accuracy measure-
ment in Figure 7 (b). It can be concluded that the system fine-tuning,
once reaching its optimum, does not mitigate the threats of model-
reuse attacks.
Classifier
2 FC + 1 SM
1 Res + 1 FC + 1 SM
1 Conv + 1 FC + 1 SM
Attack
Success Rate
99%/100%
94%/100%
80%/100%
Misclassification
Confidence
0.865
0.861
0.845
∆Accuracy
(ISIC)
0.4%
0.9%
1.1%
Table 3. Impact of classifier design (FC - fully-connected, SM
- Softmax, Conv - convolutional, Res - residual).
Classifier Design. Table 3 shows how the attack effectiveness and
evasiveness vary with respect to different classifiers. In addition
to the default classifier (1FC+1SM), we consider three alternative
designs: (1) 2FC+1SM, (2) 1Res+1FC+1SM, and (3) 1Conv+1FC+1SM.
Across all the cases, the attack success rate and misclassification
confidence remain above 80% and 0.845. In particular, we find that
73% of the adversarial models are universally effective against all
the alternative designs, indicating that model-reuse attacks are