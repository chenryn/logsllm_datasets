be system-speciﬁc and do not permit the controller to guar-
antee any performance properties. Alternatively, different
upper and lower bounds have been proposed (see [7] for
a review), but they are known to work only for discounted
models, not undiscounted ones such as those induced by the
system recovery problem.
3 POMDP Lower Bounds
In this section, we begin by proposing a new lower bound
for the value function of POMDPs called the random-action
bound (RA-Bound). We show that it converges for undis-
counted recovery models that satisfy certain conditions that
are very natural for recovery problems and that it is a lower
bound for the value function of the corresponding POMDP.
We compare this bound to other known bounds in the liter-
ature and show that they do not converge for even simple
recovery models. Then, by utilizing the speciﬁc charac-
teristics of system recovery, we show that the restrictions
imposed earlier can be relaxed while still ensuring bounds
convergence. Finally, we prove an important property of the
RA-Bound that is crucial for proving properties of recovery
controllers that use it.
3.1 The Random-Action Bound
(cid:1)
p (π) =
s∈S π(s) · V −
The RA-Bound is deﬁned as a linear combination of
value function bounds deﬁned on the states of MDP model
m = (S,A, p, r) corresponding to a given POMDP
(S,A,O, p, q, r). Precisely, if the value function bound
on MDP m is V −
m (s), then the RA-Bound for belief-state
π is deﬁned as V −
m (s). Since
the original state-space of POMDP (S) is at least expo-
nentially smaller than the belief-state-space Π (which is a
|S|-dimensional probability simplex), the bounds can be
quickly computed for POMDPs,even ones with hundreds of
thousands of states. Geometrically, the RA-Bound is a sin-
gle hyperplane in the belief-space probability simplex, with
m (s(cid:1)) being the value of the hyperplane at the simplex
V −
vertex π(s) = 1, if s = s(cid:1)
The MDP value function bound V −
m (s) is based on the
simple observation that barring any additional information
about a set of numbers, the set’s mean value is the tightest
lower bound for its maximum element. Intuitively, V −
m (s)
is computed by modifying the MDP dynamic programming
recursion (Equation 1) so that instead of picking the best
action, it uniformly randomly chooses an action (thus com-
puting an average) irrespective of what state the system is
in. The following set of linear equations deﬁne V −
(cid:4)
p(s(cid:1)|s, a)V −
V −
m (s) =
)
(cid:2)
r(s, a) + β
and π(s) = 0 otherwise.
m (s):
m (s(cid:1)
(cid:3)
s(cid:1)∈S
(cid:3)
1
|A|
a∈A
≡ L−
mV −
m (s)
(5)
This modiﬁcation effectively constructs a Markov chain
from the MDP by replacing the non-deterministic actions
with probabilistic transitions with a transition probability
of 1|A| and solving for the mean accumulated inﬁnite hori-
zon reward.
If a ﬁnite solution exists, it can be cheaply
computed off-line by using any linear system solver (or
by successive iterations of L−
m). For our implementation
(Section 5), we use Gauss-Seidel iterations with successive
Restart(b) 
(0.33,-1)
Observe (0.33,-0.5)
Restart(b) 
(0.33,-1)
Fault
(a)
Fault
(b)
Restart(a) 
(0.33,-0.5)
Restart(b) 
(0.33,-0.5)
Restart(a) or
Restart(b)
(0.33,0)
Null
Observe
(0.33,0)
(a) With Recovery Notification
aT (0.25,-0.5top)
Restart(b) 
(0.25,-1)
Fault
(a)
Restart(a) 
(0.25,-0.5)
All (1.0,0)
aT (0.25,-0.5top)
Terminate
aT (terminate)
(0.25,0)
Restart(b) 
(0.25,-1)
Fault
(b)
Restart(b) 
(0.25,-0.5)
Restart(a) or
Null
Restart(b)
(0.25,-0.5)
(b) Without Recovery Notification
Observe
(0.25,0)
Figure 2. RA-Bound Markov Chains
over-relaxation. However, for general undiscounted mod-
els, a ﬁnite V −
m may not exist. Since Equation 5 computes
the expected accumulated reward in a Markov chain, to en-
sure a ﬁnite solution, it is necessary and sufﬁcient to ensure
that the rewards of all actions that originate in the recurrent
states of this Markov chain are zero. To achieve this ulti-
mate goal, ﬁrst we impose the following natural condition
on recovery models.
Condition 1: Recovery models include a non-empty set
of “null fault” states Sφ that correspond to the system being
free of activated faults. Furthermore, starting in any state
s /∈ Sφ, there is at least one way to recover the system (i.e.,
reach some state in Sφ). Therefore, for a given initial state
s, there exists at least one s(cid:1) ∈ Sφ that is recurrent.
Next, we classify the system being recovered as being
one of two types, and demonstrate how to ensure a ﬁnite
V −
m in either type.
Systems with Recovery Notiﬁcation In many systems,
even though system monitoring may not be able to precisely
diagnose which fault has occurred, it may be able to deter-
mine when the system has recovered, i.e., when the system
has reached a null fault state in Sφ. An example is recov-
ery from permanent faults such that when monitors indicate
there are no failures, the system is certain to have recov-
ered. We believe that it is possible to automatically deter-
mine whether a system has recovery notiﬁcation by examin-
ing the observation function q, but we leave details to future
work. In any case, with recovery notiﬁcation, the controller
knows when the system enters some state s(cid:1) ∈ Sφ and can
stop recovery. Therefore, to compute the RA-Bound, we
can modify the recovery model to replace all outgoing ac-
in Sφ to loop back to
tions originating from any state s(cid:1)
state s(cid:1)
with probability 1, and with 0 reward. Figure 2(a)
shows the RA-Bounds Markov chain corresponding to the
example in Figure 1(a). With this modiﬁcation, all null fault
states s(cid:1) ∈ Sφ become absorbing states with 0 reward, and
due to Condition 1, all other states s /∈ Sφ become tran-
sient states. This is sufﬁcient to ensure the convergence of
Equation 5 and the existence and ﬁniteness of V −
m .
Systems without Recovery Notiﬁcation Although ex-
plicit recovery notiﬁcation is appropriate for many systems,
there are also many systems in which it is not available.
Examples include systems with transient faults, in which
symptoms may disappear only to reappear some time later,
or systems in which monitors have false positives. In such
cases, it is usually not possible to know for certain when
the system has recovered. Nevertheless, executing recov-
ery actions in a recovered state might still incur cost (e.g.,
executing a “Restart” action in the “Null” state in Fig-
ure 1(a)). Therefore, the recovery should only be a ﬁnite
process, and the controller should terminate at some point.
We solve this problem by explicitly making the decision on
whether to terminate recovery or not a part of the recovery
model. Precisely, we reﬁne the recovery model POMDP
P by adding an absorbing state sT and a “terminate” ac-
tion aT to it. State sT corresponds to the controller hav-
ing terminated the recovery process and is deﬁned such that
∀a ∈ A, r(sT , a) = 0 and ∀a ∈ A, s ∈ S, p(s|sT , a) = 1
if s = sT and 0 otherwise. Action aT corresponds to the
controller choosing to terminate the recovery and is deﬁned
such that ∀s ∈ S, p(s(cid:1)|s, aT ) = 1 if s(cid:1) = sT and 0 oth-
erwise. Figure 2(b) shows the RA-Bounds Markov chain
corresponding to the example of Figure 1(a), assuming no
recovery notiﬁcation. Since sT is an absorbing zero-reward
state and aT transforms all the other states into transient
states, the convergence of Equation 5 and the existence and
ﬁniteness of V −
m are ensured.
The choice of action aT is under the control of the re-
covery controller itself and is selected using the normal
decision-making process. In order for the controller to make
the decision to terminate, it must know the risk of terminat-
ing too soon (i.e., before the recovery is complete). This is
done via an appropriate choice of rewards r(s, aT ) (called
termination rewards). Clearly, r(s, aT ) = 0 if s ∈ Sφ,
because Sφ are the desired states. To compute the termi-
nation rewards for other states, the system designer pro-
vides a parameter top called operator response time, which
indicates the time required for a human operator to re-
spond to the fault. Then the termination reward is given
as r(s, aT ) = ¯r(s)· top, where ¯r(s) is the rate reward (cost)
incurred by the system in state s. The operator response
time is a very designer-friendly metric (as opposed to a dis-
count factor) and is usually known for most systems. If it
is high, the recovery controller will be more aggressive in
ensuring that the system has recovered before it terminates,
but it might incur a higher recovery cost. By varying this
parameter, it is possible to conﬁgure the controller for sys-
tems with differing degrees of human oversight.
Two other lower bounds for discounted POMDP value
functions have previously been proposed in the literature.
The BI-POMDP bound proposed in [14] is also a linear
combination of an MDP bound V BI
m (s) is ob-
tained by solving Equation 1, but with the max replaced
with a min. The bound computes the value obtained by
choosing the worst possible action from a state. Clearly,
m (s). V BI
[6]. It is a set of bounds V ba
even for simple undiscounted recovery models (e.g., Fig-
ure 1(a)), this approach fails for systems either with or with-
out recovery notiﬁcation, because the worst recovery action
(e.g., action “Restart(b)” in state “Fault(a)”) will often make
no progress but accrue cost, thus leading to an inﬁnite value.
The second bound in the literature is the “blind pol-
m (s, a), one
icy method” of
per (state,action) pair obtained by starting in state s and
then blindly following action a thenceforth (i.e., Equation 1
without the max). The POMDP bound for belief-state π
is given by maxa∈A
m (s, a). For systems
with recovery notiﬁcation, this bound will be inﬁnite for
most recovery models. The reason is that usually, no single
recovery action will usually progress in all states (e.g., in
Figure 1(a), always choosing action “Restart(a)” will lead
to an inﬁnite value in state “Fault(b)”). In systems without
recovery notiﬁcation, however, our proposed modiﬁcations
trivially ensure a ﬁnite blind policy bound (since the “termi-
nate” action aT always ensures a ﬁnite termination reward
of r(s, aT )).
s∈S π(s) · V ba
(cid:1)
3.2 RA-Bound is a POMDP Lower Bound
(cid:1)
p (π) =
s∈S π(s) · V −
Given that operator L−
m deﬁned in Equation 5 converges
and the RA-Bound V −
m (s) exists
and is ﬁnite for a POMDP P, we prove in two steps that
the RA-Bound is a lower bound for the value function of
P. First, we show that the RA-Bound is a lower bound for
all iterates of Lp (Equation 2). Then, we show that the it-
erates of Lp converge to a ﬁnite value that, in the limit, is
the value function of the POMDP P. In the proofs, P (a)
denotes the probability transition function p(s(cid:1)|s, a) in ma-
trix form, r(a) denotes the reward vector for action a, and
vector comparisons are assumed to be element-wise (i.e.,
v > v(cid:1) =⇒ v(i) > v(cid:1)(i),∀i ).
Lemma 3.1 Let P = (S,A,O, P, q, r) be some POMDP
(with β = 1) that satisﬁes Condition 1 and has been
modiﬁed such that Equation 5 converges to a ﬁnite solu-
p = Lk
m = limk→∞ L−
tion V −
p0 be the
p = Lpvk
kth iterate of Equation 2 starting from 0 (vk+1
p ).
s∈S π(s) · V −
m (s) ≤
Then, the RA-Bound V −
limk→∞ vk
Proof: The proof is by induction. Let v−,k
of Equation 5 (v−,k+1
state π, let v−,k
choose v0
(π) ≤ v0
v−,0
p
p . Then, as seen in Figure 3, v−,k+1
vk
Therefore, V −
limk→∞ v−,k
(cid:1)
m be the kth iterate
= L−
mv−,k
m ). Also, for any belief-
(π) = π · v−,k
m . As the basis of induction,
p(π) = 0,∀π and v−,0
m (s) = 0,∀s ∈ S so that
p ≤
(π).
s∈S π(s) ·
p(π).
p(π). For the induction step, assume v−,k
(π) ≤ vk+1
p
m (s) =
(π) ≤ limk→∞ vk
p (π) =
m (s) = limk→∞ v−,k
s∈S π(s)V −
k0. Also, let vk
m
p (π) =
m
p
p(π).
(cid:1)
(cid:1)
p
(cid:1)
p
Using Lemma 3.1, we see that in the limit, the RA-
Bound is a lower bound for iterates of Lp. Now all that
(cid:2)
πr(a) +
(cid:3)
o∈O
γπ,a(o)v−,k
p
(cid:4)
(ππ,a,o)
(γπ,a(o) ≥ 0)
vk+1
p
(π) = max
a∈A
= max
a∈A
= max
a∈A
= max
a∈A
(cid:2)
(cid:2)
(cid:2)
(cid:3)
o∈O
(cid:3)
πr(a) +
πr(a) +
(cid:4)
p (ππ,a,o)