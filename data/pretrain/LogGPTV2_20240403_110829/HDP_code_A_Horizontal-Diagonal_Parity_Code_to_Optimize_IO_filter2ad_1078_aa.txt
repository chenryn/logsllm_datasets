title:HDP code: A Horizontal-Diagonal Parity Code to Optimize I/O
load balancing in RAID-6
author:Chentao Wu and
Xubin He and
Guanying Wu and
Shenggang Wan and
Xiaohua Liu and
Qiang Cao and
Changsheng Xie
HDP Code: A Horizontal-Diagonal Parity Code to
Optimize I/O Load Balancing in RAID-6
Chentao Wu, Xubin He, Guanying Wu
Shenggang Wan, Xiaohua Liu, Qiang Cao, Changsheng Xie
Department of Electrical & Computer Engineering
Virginia Commonwealth University
Richmond, VA 23284, USA
{wuc4,xhe2,wug}@vcu.edu
Abstract—With higher reliability requirements in clusters and
data centers, RAID-6 has gained popularity due to its capability
to tolerate concurrent failures of any two disks, which has been
shown to be of increasing importance in large scale storage
systems. Among various implementations of erasure codes in
RAID-6, a typical set of codes known as Maximum Distance
Separable (MDS) codes aim to offer data protection against disk
failures with optimal storage efﬁciency. However, because of the
limitation of horizontal parity or diagonal/anti-diagonal parities
used in MDS codes, storage systems based on RAID-6 suffers
from unbalanced I/O and thus low performance and reliability.
To address this issue, in this paper, we propose a new parity
called Horizontal-Diagonal Parity (HDP), which takes advantages
of both horizontal and diagonal/anti-diagonal parities. The corre-
sponding MDS code, called HDP code, distributes parity elements
uniformly in each disk to balance the I/O workloads. HDP also
achieves high reliability via speeding up the recovery under single
or double disk failure. Our analysis shows that HDP provides
better balanced I/O and higher reliability compared to other
popular MDS codes.
Index Terms—RAID-6; MDS Code; Load Balancing; Horizon-
tal Parity; Diagonal/Anti-diagonal Parity; Performance Evalua-
tion; Reliability
I. INTRODUCTION
Redundant Arrays of Inexpensive (or Independent) Disks
(RAID) [25] [7] has become one of most popular choices to
supply high reliability and high performance storage services
with acceptable spatial and monetary cost. In recent years,
with the increasing possibility of multiple disk failures in large
storage systems [33] [26], RAID-6 has received too much
attention due to the fact that it can tolerate concurrent failures
of any two disks.
There are many implementations of RAID-6 based on vari-
ous erasure coding technologies, of which Maximum Distance
Separable (MDS) codes are popular. MDS codes can offer pro-
tection against disk failures with given amount of redundancy
[6]. According to the structure and distribution of different
parities, MDS codes can be categorized into horizontal codes
[30], [5], [3], [8], [4], [28], [27] and vertical codes [6], [41],
[42], [23]. A typical horizontal code based RAID-6 storage
system is composed of m + 2 disk drives. The ﬁrst m disk
drives are used to store original data, and the last two are
Wuhan National Laboratory for Optoelectronics
Huazhong University of Science & Technology
Wuhan, China 430074
{wanshenggang,lxhhust350}@gmail.com,
{caoqiang,cs_xie}@hust.edu.cn
used as parity disk drives. Horizontal codes have a common
disadvantage that m elements must be read to recover any one
other element. Vertical codes have been proposed that disperse
the parity across all disk drives, including X-Code [42], Cyclic
code [6], and P-Code [23]. All MDS codes have a common
disadvantage that m elements must be read to recover any
one other element. This limitation reduces the reconstruction
performance during single disk or double disk failures.
However, most MDS codes based RAID-6 systems suffer
from unbalanced I/O, especially for write-intensive applica-
tions. Typical horizontal codes have dedicated parities, which
need to be updated for any write operations, and thus cause
higher workload on parity disks. Unbalanced I/O also occurs
on some vertical codes like P-Code [23] consisting of a prime
number of disks due to unevenly distributed parities in a
stripe1. Even if many dynamic load balancing approaches [12]
[32] [22] [1] [18] [2] are given for disk arrays, it is still difﬁcult
to adjust the high workload in parity disks and handle the
override on data disks. Although some vertical codes such as
X-Code [42] can balance the I/O but they have high cost to
recover single disk failure as shown in the next section.
The unbalanced I/O hurts the overall storage system perfor-
mance and the original single/double disk recovery method has
some limitation to improve the reliability. To address this issue,
we propose a new parity called Horizontal-Diagonal Parity
(HDP) , which takes advantage of both horizontal parity and
diagonal/anti-diagonal parity to achieve well balanced I/O. The
corresponding code using HDP parities is called Horizontal-
Diagonal Parity Code (HDP Code) and distributes all parities
evenly to achieve balanced I/O. Depending on the number of
disks in a disk array, HDP Code is a solution for p − 1 disks,
where p is a prime number.
We make the following contributions in this work:
• We propose a novel and efﬁcient XOR-based RAID-6
code (HDP Code) to offer not only the property provided
by typical MDS codes such as optimal storage efﬁciency,
1A stipe means a complete (connected) set of data and parity elements
that are dependently related by parity computation relations [17]. Typically,
a stripe is a matrix as in our paper.
but also best load balancing and high reliability due to
horizontal-diagonal parity.
• We conduct a series of quantitative analysis on various
codes, and show that HDP Code achieves better load bal-
ancing and higher reliability compared to other existing
coding methods.
The rest of this paper continues as follows: Section II
discusses the motivation of this paper and details the problems
of existing RAID-6 coding methods. HDP Code is described
in detail in Section III. Load balancing and reliability analysis
are given in Section IV and V. Section VI brieﬂy overviews
the related work. Finally we conclude the paper in Section
VII.
II. PROBLEMS OF EXISTING MDS CODES AND
MOTIVATIONS OF OUR WORK
To improve the efﬁciency, performance, and reliability of the
RAID-6 storage systems, different MDS coding approaches
are proposed while they suffer from unbalanced I/O and high
cost to recover single disk. In this section we discuss the
problems of existing MDS codes and the motivations of our
work. To facilitate the discussion, we summarize the symbols
used in this paper in Table I.
A. Load Balancing Problem in Existing MDS Codes
RAID-5 keeps load balancing well based on the parity
declustering approach [19]. Unfortunately, due to dedicated
distribution of parities, load balancing is a big problem in
all horizontal codes in RAID-6. For example, Figure 1 shows
the load balancing problem in RDP [8] and the horizontal
parity layout shown in Figure 1(a). Assuming Ci,j delegates
the element in ith row and jth column, in a period there are six
reads and six writes to a stripe as shown in Figure 1(c). For a
single read on a data element, there is just one I/O operation.
However, for a single write on a data element, there are at least
six I/O operations2, one read and one write on data elements,
two read and two write on the corresponding parity elements.
Then we can calculate the corresponding I/O distribution and
ﬁnd that it is an extremely unbalanced I/O as shown in Figure
1(d). The number of I/O operations in column 6 and 7 are ﬁve
and nine times higher than column 0, respectively. It may lead
to a sharp decrease of reliability and performance of a storage
system.
Some vertical coding methods have unevenly distributed
parities, which also suffer from unbalanced I/O. For example,
Figure 2 shows the load balancing problem in P-Code [23].
From the layout of P-Code as shown in Figure2(a), column
6 goes without any parity element compared to the other
columns, which leads to unbalanced I/O as shown in Figure
2(b) though uniform access happens. We can see that column 6
has very low workload while column 0’s workload is very high
2For some horizontal codes like RDP, some diagonal parities are calculated
by the corresponding data elements and horizontal parity element. So, it could
be more than six operations for a single write.
TABLE I
SYMBOLS
Description
number of disks in a disk array
a prime number
row ID
column ID or disk ID
element at the ith row and jth column
two random failed columns with IDs f1 and f2
XOR operations between/among elements
5(cid:80)
(e.g.,
Ci,j = Ci,0 ⊕ Ci,1 ⊕ · · · ⊕ Ci,5)
j=0
modular arithmetic
(e.g., (cid:104)i(cid:105)p = i mod p)
number of I/O operations to column j, which
is caused by a request to data element(s) with
beginning element Ci,j
total number of I/O operations of requests to
column j in a stripe
maxmium/minimum number of I/O operations
among all columns
metric of load balancing in various codes
average time to recover a data/parity element
average number of elements can be recovered
in a time interval Rt
Parameters
& Symbols
n
p
i, r
j
Ci,j
f1, f2
(f1 < f2)
(cid:80)
(cid:104) (cid:105)
Oj (Ci,j )
O(j)
Omax, Omin
L
Rt
Rp
(six times of column 6). This can decrease the performance
and reliability of the storage system.
Many dynamic load balancing approaches are proposed [12]
[32] [22] [1] [18] [2] which can adjust the unbalanced I/O in
data disks according to various access patterns of different
workloads. While in RAID-6 storage system, it is hard to
transfer the high workload in parity disks to any other disks,
which breaks the construction of erasure code thus damages
the reliability of the whole storage system. Except for parity
disks, if there is an overwrite upon the existing data in a
data disk, because of the lower cost to update the original
disks compared to new writes on other disks and related parity
disks, it is difﬁcult to adapt the workload to other disks even
if the workload of this data disk is very high. All these mean
that unbalanced write to parity disks or override write to data
disks cannot be adapted by dynamic load balancing methods
in RAID-6. Further more, typical dynamic load balancing
approaches [31] [32] in disk array consume some resources to
monitor the status of various storage devices and ﬁnd the disks
with high or low workload, then do some dynamic adjustment
according to the monitoring results. This brings additional
overhead to the disk array.
Industrial products based on RAID-6 like EMC CLARiiON
[9] uses static parity placement to provide load balancing in
each eight stripes, but it also has additional overhead to handle
the data placement and suffers from unbalanced I/O in each
stripe.
B. Reduce I/O Cost of Single Disk Failure
In 2010, Xiang et al. [40] proposes a hybrid recovery ap-
proach named RDOR, which uses both horizontal and diagonal
(a) Horizontal parity layout of RDP code with prime+1 disks (p = 7).
(b) Diagonal parity layout of RDP code with prime+1 disks (p = 7).
(c) Six reads and six writes to various data elements in RDP (Data
elements C0,0, C1,1, C3,5, C4,0, C4,3 and C5,3 are read and other
data elements C0,5, C2,1, C2,2, C3,4, C4,2 and C5,4 are written,
which is a 50% read 50% write mode).
(d) The corresponding I/O distribution among different columns (e.g.,
the I/O operations in column 1 is 2 reads and 1 write, 3 operations in
total; the I/O operations in disk 6 is 6 reads and 6 writes, 12 operations
in total; the I/O operations in disk 7 is 10 reads and 10 writes, 20
operations in total).
Fig. 1. Load balancing problem in RDP code for an 8-disk array (The I/O operations in columns 0, 1, 2, 3, 4 and 5 are very low, while in columns 6 and
7 are very high. These high workload in parity disks may lead to a sharp decrease of reliability and performance of a storage system).
parities to recover single disk failure. It can minimize I/O cost
and has well balanced I/O on disks except for the failed one.
For example, as shown in Figure 3, data elements A, B and C
are recovered by their diagonal parity while D, E and F are
recovered by their horizontal parity. With this method, some
elements (e.g., C3,0) can be shared to recover another parity,
thus the number of read operations can be reduced. Actually,
up to 22.60% of disk access time and 12.60% of recovery time
are decreased in the simulation experiments [40].
This approach has less effect on vertical codes like X-
Code. As shown in Figure 4, data elements A, B, C and F
are recovered by their anti-diagonal parity while D, E and
G are recovered by their diagonal parity. Although X-Code
recovers more elements compared to RDP in these examples,
X-Code share fewer elements than RDP thus has less effects
on reducing the I/O cost when single disk fails.
RDOR is an efﬁcient way to recover single disk failure, but
it cannot reduce I/O cost when parity disk fails. For example,
as shown in Figure 3, when column 7 fails, nearly all data
should be read and the total I/O cost is relatively high.
C. Summary on Different Parities in Various Coding Methods
To ﬁnd out the root of unbalanced I/O in various MDS
coding approaches, we analyze the features of various parities
in these code. We classify these approaches into four cate-
gories (including the horizontal-diagonal parity introduced in
this paper): horizontal parity3, diagonal/anti-diagonal parity,
vertical parity, and horizontal-diagonal parity (HDP).
1) Horizontal Parity (HP): Horizontal parity is the most
important feature for all horizontal codes, such as EVENODD
[3] and RDP [8], etc. The horizontal parity layout is shown in
Figure 1(a) and can be calculated by,
n−3(cid:88)
j=0
Ci,n−2 =
Ci,j
(1)
3In some papers, horizontal parity is referred to as “row parity”.
234243122004812162001234567Column numberNumber of I/O operations(a) Vertical parity layout of P-Code with prime disks (p = 7).
(b) I/O distribution among different columns when six writes to
different columns occur (Continuous data elements C0,6, C1,0, C1,1,
C1,2, C1,3, C1,4 and C1,5 are written, each column has just one write
to data element and it is a uniform write mode to various disks).
Fig. 2.
Load balancing problem in P-Code for an 7-disk array (The I/O
operations in columns 0 and 5 are very high which also may lead to a sharp
decrease of reliability and performance of a storage system).
Fig. 3. Reduce I/O cost when column 2 fails in RDP code.
From Equation 1, we can see that
typical HP can be
calculated by a number of XOR computations. For a partial
stripe write on continuous data elements, it could have lower
cost than the other parities due to the fact that these elements
can share the same HP (e.g., partial write cost on continuous