### Comparison with CryptoNets

In contrast to CryptoNets, MiniONN supports all operations commonly used by neural network designers without requiring any changes to the training process. It also significantly reduces overhead during prediction time. The privacy guarantees for the client remain identical. However, while CryptoNets can completely hide the model from clients, MiniONN only hides the model values (e.g., weight matrices and bias vectors) but discloses the number of layers, sizes of weight matrices, and the types of operations used in each layer. We argue that this tradeoff is justifiable for two reasons: 

1. **Performance Gains**: The tradeoff results in significant performance improvements, such as a 740-fold reduction in online latency.
2. **Information Disclosure**: The information disclosed by MiniONN, such as the number of layers and types of operations, is typically described in academic and white papers. Model values, like weight matrices and bias vectors, are usually not disclosed in such literature.

Chabanne et al. [16] noted the limited accuracy guarantees of the square function in CryptoNets. They approximated the ReLU activation function using a low-degree polynomial and added a normalization layer to ensure stable and normally distributed inputs to the activation layer. However, their approach requires a multiplicative depth of 6 in LHE and does not provide benchmark results in their paper.

### Related Work on Privacy in Training Phase

Most related works focus on the privacy of the training phase. For example, Graepel et al. [31] proposed training algorithms that can be expressed as low-degree polynomials, allowing the training phase to be performed on encrypted data. Aslett et al. [4, 5] presented methods to train both simple models (e.g., Naive Bayes) and more advanced models (e.g., random forests) over encrypted data. Differential privacy techniques [1, 27, 52] also guarantee privacy during the training phase. Ohrimenko et al. [46] leveraged Intel SGX and several data-oblivious algorithms to enable multiple parties to jointly train a model while ensuring the privacy of their individual datasets.

Recently, Mohassel and Zhang [44] proposed SecureML, a two-server model for privacy-preserving training. In this model, data owners distribute their data among two non-colluding servers to train various models, including neural networks, using secure two-party computation (2PC). While their primary focus is on training, they also support privacy-preserving predictions. Their work is closest to ours. Independently, they use a precomputation stage to reduce overhead during the online prediction phase, support popular activation functions like ReLU, and use approximations where necessary. MiniONN differs from their work in several ways:

1. **SIMD Batch Processing**: By using SIMD batch processing, MiniONN achieves a significant reduction in precomputation overhead without affecting the online phase (Section 6.1).
2. **No Training Changes**: Unlike their approach, MiniONN does not require changes to how models are trained.

DeepSecure [50] is another independent work focusing on oblivious neural network predictions. It uses Yaoâ€™s garbled circuits and achieves a 58-fold performance improvement over CryptoNets.

### Conclusion and Future Work

In this paper, we introduced MiniONN, the first approach that can transform any common neural network into an oblivious form. Our benchmarks show that MiniONN achieves lower response latency and message sizes compared to prior work [28, 44]. We plan to design easy-to-use interfaces that allow developers without cryptographic expertise to use MiniONN directly. Additionally, we intend to investigate the applicability of our approach to other machine learning models. As a next step, we plan to apply MiniONN to neural networks used in production.

### Acknowledgments

This work was supported in part by TEKES - the Finnish Funding Agency for Innovation (CloSer project, 3881/31/2016) and by Intel (Intel Collaborative Research Institute for Secure Computing, ICRI-SC).

### References

[References have been omitted for brevity. Please refer to the original document for the full list of references.]

---

This revised text aims to improve clarity, coherence, and professionalism. It organizes the content into distinct sections, provides clear explanations, and ensures that the arguments and comparisons are well-structured.