The user-mode library extends and builds upon the services 
provided  by  the  kernel-mode  driver  and  executes  several 
mechanisms:  it  provides  a  Libpcap-compatible  interface  for 
low-level  network  access,  and  it  applies  the  buddy-group-
based  offloading  mechanism  to  handle  long-term  load 
imbalance.  
a. Lossless zero-copy packet capture and delivery 
  WireCAP captures packets on a per-receive-queue basis. When 
a user-space application opens a receive queue to capture packets, 
the kernel-mode driver maps the ring buffer pool associated with 
the  receive  queue  into  the  application’s  process  space,  and  the 
use-mode library creates and assigns three key entities in the user 
space for the receive queue: 
• 
A capture thread performs the low-level capture and recycle 
operations,  and  implements  the  offloading  mechanism. 
Typically, a capture thread and an application thread do not 
run in the same core. The system can dedicate one or several 
cores to run all capture threads. 
A work-queue pair consists of a capture queue and a recycle 
queue.  A  capture  queue  keeps  the  metadata  of  captured 
packet buffer chunks, and a recycle queue keeps the metadata 
of packet buffer chunks that are waiting to be recycled.  
A buddy list keeps the buddies of a receive queue in a buddy 
group.  It  is  used  to  implement  the  buddy-group-based 
offloading mechanism. The receive queues in a buddy group 
are  buddies.  The  user-mode  library  provides  functions  to 
allow  an  application  to  populate  the  buddies  of  a  receive 
queue.  
• 
• 
WireCAP captures packets in two modes—the basic mode and the 
advanced mode. 
In  the  basic  mode,  WireCAP  handles  each  receive  queue 
independently.  Figure  7a  illustrates  how  this  process  works.  (1) 
For each receive queue, its dedicated capture thread executes the 
low-level capture operations to move filled packet buffer chunks 
into  the  user  space.  The  packet  buffer  chunks  captured  from  a 
particular  receive  queue  are  placed  into  its  capture  queue  in  the 
user space. (2) To ingest packets from a particular receive queue, 
a  packet-processing  thread  accesses  the  receive  queue’s  capture 
queue in the user space through a Libpcap-compatible API such as 
pcap_loop()  or  pcap_dispatch().  Packet  buffer  chunks  in  the 
capture  queue  are  processed  one  by  one;  a  used  packet  buffer 
chunk is placed into the associated recycle queue. (3) A capture 
thread  executes  the  low-level  recycle  operations  to  recycle  used 
packet buffer chunks from its associated recycle queue.  
Thread
Thread
Libpcap-compatible Interface
Libpcap-compatible Interface
2. Processing
Work 
Queue
R
e
c
y
c
l
e
C
a
p
t
u
r
e
Capture thread
1. Capture
3. Recycle
Kernel-Mode Driver
a. basic mode
Buddy list
x y z
1.c
C
a
p
t
u
r
e
2. Processing
Work 
Queue
R
e
c
y
c
l
e
1.b
Capture thread
1.a 
1.d
3. Recycle
Kernel-Mode Driver
b. advanced mode
Figure 7. WireCAP packet-capturing operations 
401
  A  ring  buffer  pool  can  temporarily  store  R*M  packets. 
Assuming  Pin  is  the  maximum  incoming  packet  burst  rate  at  a 
receive queue and Pp is the processing rate of the corresponding 
packet-processing thread. WireCAP in the basic mode can handle 
a maximum burst of Pin*(R*M)/(Pin-Pp) packets without loss at the 
receive  queue.  However,  WireCAP  in  the  basic  mode  cannot 
handle long-term load imbalance. When one core is over-flooded 
by incoming packets, the corresponding capture queue in the user 
space  will  soon  be  filled.  Because  the  captured  packet  buffer 
chunks cannot be processed and recycled in time, the free packet 
buffer  chunks  in  the  corresponding  ring  buffer  pool  become 
depleted, causing packet capture drops. 
In  the  advanced  mode,  WireCAP  updates  the  basic  mode 
operation (1) with the buddy-group-based offloading mechanism 
to handle long-term load imbalance. Figure 7b illustrates how this 
process works: (1.a) For each receive queue, its dedicated capture 
thread  executes  the  low-level  capture  operations  to  move  filled 
packet  buffer  chunks  into  the  user  space.  (1.b)  When  a  capture 
thread moves a chunk into the user space, the thread examines its 
associated  capture  queue  in  the  user  space.  If  the  queue  length 
does  not  exceed  an  offloading  percentage 
threshold  (T), 
WireCAP’s indicator of long-term load imbalance, the thread will 
place  the  chunk  into  its  own  capture  queue.  (1.c)  When  the 
threshold  T  is  exceeded,  the  thread  will  query  the  associated 
buddy queue list and (1.d) place the chunk into the capture queue 
of  an  idle  or  less  busy  receive  queue.  The  assumption  is  that, 
when a capture queue is empty or shorter, the corresponding core 
is idle or less busy. 
In our design, a ring buffer pool is mapped into an application’s 
process  space.  Thus,  a  network  packet  can  be  captured  and 
delivered to the application with zero-copy. 
b. Zero-copy packet forwarding 
  A  multi-queue  NIC  can  be  configured  with  one  or  multiple 
transmit  queues  for  outbound  packets.  For  each  transmit  queue, 
the NIC maintains a ring of transmit descriptors, called a transmit 
ring.  To  transmit  a  packet  from  a  transmit  queue,  the  packet 
should be attached to a transmit descriptor in the transmit ring of 
the queue. The transmit descriptor helps the NIC locate the packet 
in the system. After that, the NIC transmits the packet. 
  With WireCAP, an application can use ring buffer pools as its 
own data buffers and handle captured packets directly from there. 
Therefore,  the  application  can  forward  a  captured  packet  by 
simply attaching it to a specific transmit queue, potentially after 
the packet has been analyzed and/or modified. Attaching a packet 
to a transmit queue only involves metadata operations. The packet 
itself is not copied. 
c. Safety considerations 
  Two aspects of WireCAP would raise safety concerns: (1) the 
sharing  of  memory  between  the  kernel  and  the  user-space 
applications  and  (2) 
the  offloading  mechanism.  However, 
WireCAP should not cause any safety problems. A misbehaving 
application  will  not  crash  the  kernel.  WireCAP  only  maps  ring-
buffer pools, which do not involve critical kernel memory regions, 
into  the  address  space  of  user-space  applications.  In  addition, 
when  a  used  packet  buffer  chunk  is  to  be  recycled,  its  metadata 
will  be  strictly  validated  and  verified  by  the  kernel.  Similarly,  a 
misbehaving  application  will  not  crash  other  applications.  The 
offloading  mechanism  is  implemented  across  the  receive  queues 
of  a  buddy  group  belonging  to  the  same  application.  Different 
applications do not interfere with one another. 
the  experiment 
threshold  T  are  variables 
d. Supporting multiple NICs 
  Because  WireCAP  operates  on  a  per-receive-queue  basis, 
WireCAP naturally supports multiple NICs. In Section 4, we run 
experiments  to  demonstrate  that  WireCAP  supports  multiple 
NICs. 
e. Application support 
  Upon  work-queue  pairs,  WireCAP  supports  a  Libpcap-
compatible  interface.  Packets  can  be  read  with  APIs  such  as 
pcap_dispatch  ()  or  pcap_loop  ().  Both  blocking  and  non-
blocking modes can be supported.  
3.3 Implementation 
  WireCAP  was  developed  on  Linux.  The  kernel-mode  driver 
was  implemented  on  the  Intel  82599-based  10GigE  NIC.  We 
modified  the  ixgbe  driver  to  implement  the  WireCAP  functions, 
and  the  modifications  involve  a  few  hundred  lines  of  code.  We 
also  implemented  a  user-mode  library.  The  current  version 
supports  a  simple  Libpcap-compatible  interface.  We  plan  to 
release WireCAP for public access soon. 
4 Performance evaluations 
  We  evaluate  WireCAP  using 
tools  and 
configuration described in Section 2.2. In addition, we develop a 
third  tool  for  our  experiments.  It  is  a  multi-threaded  version  of 
pkt_handler,  called  multi_pkt_handler,  which  can  spawn  one  or 
multiple pkt_handler threads that share the same address space.  
  We  evaluate  WireCAP  from  different  perspectives.  The 
descriptor  segment  size  M,  the  ring  buffer  pool  size  R,  and  the 
offloading  percentage 
the 
experiments. A simple name convention is used in the following 
sections.  WireCAP-B-(M,  R)  represents  WireCAP  in  the  basic 
mode with a descriptor segment size of M and a ring pool size of 
R,  while  WireCAP-A-(M,  R,  T)  represents  WireCAP  in  the 
advanced mode with a descriptor segment size of M, a ring pool 
size of R, and an offloading threshold of T. 
  We  compare  WireCAP  with  existing  packet  capture  engines 
(PF_RING,  DNA,  and  NETMAP).  The  performance  metric  is 
packet drop rate. In the experiments, these packet capture engines 
suffer different types of packet drops: (1) WireCAP suffers only 
packet capture drops, which can occur when the free packet buffer 
chunks in a ring buffer pool are depleted; (2) DNA and NETMAP 
suffer only packet capture drops; and (3) PF_RING suffers both 
packet  capture  drops  and  packet  delivery  drops.  To  make  the 
comparison easier, we only calculate the overall packet drop rate. 
Each NIC receive ring is configured with a size of 1,024 for all 
packet capture engines. PR_RING is set to run at mode 2, and the 
size of pf_ring buffer is set to 10,240. The CPU frequency is set to 
2.4 GHz. 
Packet  capture  in  the  basic  mode.  The  traffic  generator 
transmits P 64-Byte packets at the wire rate (14.88 million p/s). P 
varies, ranging from 1,000 to 10,000,000. NIC1 is configured with 
a single receive queue, tied to a core, on which a pkt_handler is 
launched  to  capture  and  process  traffic  for  that  queue.  For 
pkt_handler, x is set to 0 and 300.  
  With  x=0,  pkt_handler  does  not  incur  any  packet-processing 
load. We test WireCAP with various R and M values. No packet 
drops  are  observed  (Figure  8),  indicating  that  WireCAP  can 
capture packets at wire speed without loss. No packet drops were 
observed  for  NETMAP  and  DNA.  However,  PF-RING  suffers 
significant  packet  drops  (both  packet  delivery  drops  and  packet 
capture drops).  
  With  x=300,  pkt_handler  emulates  a  heavy  load  application. 
Because the incoming packet rate (14.88 million p/s) far exceeds 
in 
the  packet-processing  speed  of  pkt_handler  on  a  2.4  GHz  CPU 
(38,844  p/s),  the  maximum  P  that  a  packet  capture  engine  can 
handle  without  packet  loss  reflects  its  buffering  capability  for 
short-term  bursts  of  packets.  As  shown  in  Figure  9,  WireCAP 
demonstrates superior buffering capability for short-term bursts of 
packets.  For  example,  DNA  suffers  a  15%  packet  drop  at 
P=6,000, while WireCAP-B-(256, 500) has no packet drops even 
at  P=100,000.  WireCAP’s  resilient  buffering  capability  comes 
from its unique ring-buffer-pool design and is proportional to the 
ring-buffer pool capacity R*M. WireCAP-B-(256, 500) clearly has 
a    higher  buffering  capability  than  WireCAP-B-(256,100).  At 
P=100,000, WireCAP-B-(256,500) has no packet drops, whereas 
WireCAP-B-(256, 100) has a packet drop rate of 71%. 
100%#
80%#
60%#
40%#
20%#
#
#
e
t
a
R
p
o
r
D
#
t
e
k
c
a
P
0%#
1000#
DNA#
PFDRING#
NETMAP#
WireCAPDBD(64,#100)#
WireCAPDBD(128,#100)#
WireCAPDBD(256,#100)#
WireCAPDBD(256,#500)#
10000#
1000000#
Number#of#Packets#(Logarithmic#Scale)#
100000#
10000000#
Figure 8. WireCAP packet capture in the basic mode, with no 
packet processing load (x=0) 
100%#
#
#
e
t
a
R
p
o
r
D
#
t
e
k
c
a
P
80%#
60%#
40%#
20%#
0%#
1000#
DNA#
PF_RING#
NETMAP#
WireCAPLBL(256,#100)#
WireCAPLBL(256,#500)#
10000#
1000000#
Number#of#Packets#(#Logarithmic#Scale)#
100000#
10000000#
Figure 9. WireCAP packet capture in the basic mode, with a 
heavy packet-processing load (x=300) 
Figure  10  illustrates  that  WireCAP’s  buffering  capability  is 
proportional  to  the  overall  ring  buffer  capacity  R*M.  The 
individual R or M does not affect the overall performance. In the 
experiment,  R  and  M  are  varied,  but  R*M  is  fixed.  The  results 
indicate  that  WireCAP-B-(64,  400),  WireCAP-B-(128,  200),  and 
WireCAP-B-(256, 100) have approximately the same packet drop 
rates at different P values. 
402
100%#