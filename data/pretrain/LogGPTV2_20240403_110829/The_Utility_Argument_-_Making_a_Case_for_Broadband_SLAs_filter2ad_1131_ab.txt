to quantify network performance in terms of latency, packet loss, and down-
load/upload throughput. For this, we used three diﬀerent measurement tables
(out of eleven present) from the dataset for our analysis: (1) UDP pings, (2)
160
Z.S. Bischof et al.
HTTP GETs, which measure download throughput, and (3) HTTP POSTs,
which measure upload throughput.
The UDP pings run continuously, measuring round-trip time to two or three
measurement servers. These servers are hosted by either M-Lab or within the
user’s provider’s network. Over the course of each hour, the gateway will send
up to 600 probes to each measurement server at regular intervals, less if the link
is under heavy use for part of the hour. Each gateway reports hourly statistical
summaries of the latency measurements (mean, min, max, and standard devi-
ation) as well as the number of missing responses. We use the average latency
to the nearest server (in terms of latency) to summarize the latency during that
hour. We also use the number of missing responses to calculate the packet loss
rate over the course of each hour.
As mentioned above, the HTTP GET and POST tables record the mea-
sured download and upload throughput rate, respectively. Similar to the latency
measurements, throughput measurements are typically done to two diﬀerent tar-
get servers. However, throughput measurements are run once every other hour,
alternating between measuring upload and download throughput rates.
We combined these performance measurements with user metadata, which
includes the user’s provider, service tier (i.e., subscription speed), service tech-
nology (e.g., cable or DSL), and geographic region. This allows us to group
measurements by ISP, compare the achieved throughput as a fraction of the
subscription capacity and diﬀerentiate between subscribers of the same ISP with
diﬀerent access technologies (e.g., Verizon customers with DSL or ﬁber).
3.2 Throughput
We ﬁrst analyze ISPs’ download and upload throughput. A challenge in compar-
ing performance across providers and services, is that users do not have the same
subscription speeds; individual ISPs typically oﬀer a number of service capac-
ities and the stated capacities of such oﬀerings vary from one ISP to another.
In order to directly compare the consistency of performance, we ﬁrst normal-
ize throughput measurements by the speed that each user should be receiving.
Fig. 1. CDF of measured download throughput rates as a fraction of the subscriber’s
service capacity.
The Utility Argument – Making a Case for Broadband SLAs
161
For this, we use the reported download and upload subscription rate included
as part of the FCC dataset, as described in Sect. 3.1.
Throughput distribution. Figure 1 shows a CDF of each normalized download
throughput measurement from subscribers of four services: AT&T’s DSL service,
Clearwire, Comcast, and Frontier’s ﬁber service. Of the services we studied,
Frontier’s ﬁber had the most consistent throughput rates, both in terms of the
fraction of probes that measured at least 90% of the subscription speed and
the variations between measurements. Although measurements were unlikely to
achieve download rates signiﬁcantly higher than their subscription speed, 96%
of measurements were above 90% of the subscription speed.
For Comcast (cable), measurements were slightly less likely to reach 90% of
the subscription speed (about 91%). However, download throughput measure-
ments were often much higher than the user’s subscription speed – the median
measurement on Comcast’s network was 135% of the subscription speed. We
observed a similar trend for most cable broadband providers, as well as Veri-
zon’s ﬁber service.
Download throughput measurements from subscribers of AT&T’s DSL ser-
vice were fairly consistent (i.e., showing little variation). However, in contrast
to cable and ﬁber services, they rarely exceeded the subscription speed, with
less than 10% of measurements at or above the subscription speed. Nearly half
(48%) of measurements were below 90% of the subscription speed. Other DSL
providers showed a similar trend. Of the ISPs in our study, Clearwire had the
largest fraction of measurements (73%) below 90% of the subscription speed.
Variation over time. Looking only at Fig. 1, it is still unclear how much per-
formance can vary for an individual subscriber over the course of a month.
To capture this, we aggregated all measurements that were conducted from the
same vantage point and run during the same month, which we refer to as a
“user-month”. For each user-month, we calculate the fraction of measurements
that were below a threshold of 10%, 25%, 50%, 75%, and 90% of the subscription
speed.
Figure 2 shows, for AT&T, Comcast and Frontier ﬁber subscribers, how fre-
quently measurements during the same month measured below a particular
threshold. The vertical gray lines represent a particular frequency of throughput
measurements being below a given threshold (from left to right): once a month,
once a week, once a day, and once every other hour.
In the case of AT&T, shown in Fig. 2a, during 47% of the user-months,
subscribers got less than 90% of their service capacity at least once every other
hour (the right-most vertical line). In contrast, for Comcast subscribers, shown in
Fig. 2b, only about 9% of user-months measured less than 90% of the subscription
speed at the same frequency. Comcast users were also less likely to receive less
than 50% of their subscription speed. Frontier’s ﬁber was even less likely to have
degradations in download throughput every other hour; less than 3% of months
of Frontier measurements saw throughput rates below 90% of the subscription
speed.
162
Z.S. Bischof et al.
In general, the distributions of upload (Fig. 3) and download throughput mea-
surements shown similar trends. The most obvious diﬀerence was that upload
measurements from Clearwire subscribers were noticeably higher, more consis-
tent, and much closer to the subscription speed. For each ISP in Fig. 3, the
median measurement was at least 90% of the subscription speed.
3.3 Latency
As mentioned in Sect. 3.1, we use the average latency (measured over an hour)
to the nearest measurement server as an estimate of the subscriber’s latency.
Figure 4 shows a CDF of the hourly average latency for ﬁve ISPs. Cablevision,
with 96% of hourly averages below 20 ms, showed the lowest latencies of all ISPs
in our dataset and appeared to consistently meet even the most demanding SLA.
Other ﬁber, DSL, and cable ISPs had slightly higher latencies, but were fairly
consistent, with at least 90% of average latency reports for each provider being
less than 70 ms. AT&T, with 95% of measurements below 57 ms had the lowest
latencies of all DSL providers, but the overall distribution was higher than most
cable providers.
Latency measurements from Clearwire subscribers were noticeably higher,
with a median of approximately 90 ms. Satellite providers had the highest latency
(a) AT&T (DSL)
(b) Comcast
(c) Frontier (ﬁber)
Fig. 2. CCDF of the fraction of a user’s download throughput measurements per month
that are below a percentage of the subscription capacity. Each gray vertical line rep-
resents a frequency of (from left to right) once a month, once a week, once a day, and
once every other hour.
Fig. 3. CDF of upload throughput rates as a fraction of the subscriber’s service
capacity.
The Utility Argument – Making a Case for Broadband SLAs
163
Fig. 4. CDF of latency measurements
to servers.
Fig. 5. CCDF of hourly loss rates to
servers.
measurements, consistently above 600 ms, as a result of the fundamental limita-
tions of the technology.
3.4 Packet Loss
Using the number of UDP pings that succeeded and failed to the target mea-
surement server, we calculated the percentage of packets lost over each hour.
Figure 5 shows the CCDF of the hourly packet loss rates for four ISPs. On aver-
age, ﬁber providers tended to have lower loss rates and had the lowest frequency
of high loss. More speciﬁcally, Verizon had the lowest frequency of loss rates
above 1%, occurring during only 0.82% of hours. Comcast (not in the ﬁgure)
and TimeWarner had the lowest frequency for cable providers, with loss rates
above 1% occurring in approximately 1.5% of hours. Satellite providers had the
highest frequency of loss rates above 1%, occurring during over 26% of hours.
3.5 Applying an SLA
In Sect. 2, we deﬁned SLAs in measurable terms with thresholds that would be
meaningful to users’ Quality of Experience. Building on our characterization, we
now explore how eﬀectively today’s ISPs could meet our proposed set of SLAs.
There are a number of ways that a broadband SLA could be structured in
terms of how users are compensated for periods of poor performance. As an
example, we looked at how some broadband ISPs structure the agreements that
they oﬀered to businesses. In the case of Comcast [8], business class subscribes
are compensated once the network become unavailable for more than four hours
in a single month. For each hour of downtime after the ﬁrst four, customers
are reimbursed 1/30 of the monthly subscription price.2 We believe that general
broadband service plans could have a similar structure. For example, the SLA
could state that the network may be unavailable for up to two hours per day (or
about 8.33% of hours in a month). This would allow ISPs to schedule downtime
2 This eﬀectively means that if the service was ‘unavailable’ for 34 h in a month
(approximately 5% of the month) the user gets the monthly subscription for free.
164
Z.S. Bischof et al.
for maintenance and provide a guarantee for subscribers that their service will
not be down for days at a time (or that they will be compensated if it is).
However, our focus in this paper is not on the structure of compensation for
SLA violations. Instead, we look at how well the ISPs in the FCC’s dataset are
able to meet the SLAs deﬁned in Sect. 2, and whether it would be at all feasible
to provide guarantees of service.
(a) AT&T (DSL)
(b) Clearwire
(c) Comcast
(d) Verizon (ﬁber)
Fig. 6. CDF of the percent of hours per month in violation of each SLA for four
providers.
Figure 6 summarizes the total number of SLA violations per month for four
example ISPs. AT&T, shown in Fig. 6a struggles to meet the requirements of
SLA A but is able to meet SLA B during 90% of hours per month for 73% of
users and meets SLA C during 90% of hours for 82% of users.
The wireless provider in our dataset, Clearwire (Fig. 6b), face diﬃculties in
meeting SLA A, as the average latencies were almost always higher than 50 ms.
This appears to be a result of the underlying technology and many cellular
providers are unable to meet this latency requirement [18]. Interestingly, Clear-
wire actually did a better job of meeting SLA C than AT&T, with 94% of users
meeting SLA C performance during at least 90% of hours in a month.
Both Comcast and Verizon’s ﬁber service did a relatively good job of meeting
the requirements of all three SLAs. Comcast was able to meet SLA A during
90% of hours in a month for 75% of users while Verizon was able to do the
same for 83% of ﬁber subscribers. Both were able to provide both SLA B and
C during 90% of hours for at least 90% of users.
To summarize our ﬁndings in this section, moderate SLAs (those which
require SLA compliance up to 90% of time) are feasible nowadays and could
be oﬀered by many ISPs with minimal eﬀect on their current business. However,
stricter SLAs (those which require SLA compliance 99% of the time or more)
would be much more challenging to oﬀer across the whole user base. In the fol-
lowing section, we examine how diﬃcult it would be to assess the individual risk
of breaking and SLA, a central challenge in personalized SLA oﬀerings.
4 Personalized SLAs
As we noted in a previous section, SLA can be seen as an insurance policy against
poor broadband experience, which may in turn have ﬁnancial consequences in
The Utility Argument – Making a Case for Broadband SLAs