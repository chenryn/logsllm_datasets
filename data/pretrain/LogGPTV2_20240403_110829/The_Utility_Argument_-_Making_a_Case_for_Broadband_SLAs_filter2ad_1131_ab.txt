### Quantifying Network Performance

To evaluate network performance, we focused on three key metrics: latency, packet loss, and download/upload throughput. For this analysis, we utilized three different measurement tables from the dataset: (1) UDP pings for latency, (2) HTTP GETs for download throughput, and (3) HTTP POSTs for upload throughput.

#### Latency Measurement
UDP pings are continuously run to measure round-trip time to two or three measurement servers. These servers are either hosted by M-Lab or within the user’s provider's network. Each hour, the gateway sends up to 600 probes to each server at regular intervals, with fewer probes if the link is under heavy use. The gateway reports hourly statistical summaries, including mean, minimum, maximum, and standard deviation of latency, as well as the number of missing responses. We use the average latency to the nearest server (in terms of latency) to summarize the latency for that hour. Additionally, we calculate the packet loss rate using the number of missing responses.

#### Throughput Measurement
HTTP GET and POST requests measure download and upload throughput, respectively. Similar to latency measurements, throughput is typically measured to two different target servers. However, these measurements are conducted once every other hour, alternating between download and upload throughput.

### Combining Performance Metrics with User Metadata
We integrated these performance measurements with user metadata, which includes the user’s internet service provider (ISP), service tier (i.e., subscription speed), service technology (e.g., cable or DSL), and geographic region. This allows us to group measurements by ISP, compare achieved throughput as a fraction of the subscription capacity, and differentiate between subscribers of the same ISP with different access technologies (e.g., Verizon customers with DSL or fiber).

### Analysis of Throughput
#### Normalization and Distribution
To compare performance across providers and services, we normalized throughput measurements by the speed each user should be receiving, using the reported download and upload subscription rates from the FCC dataset.

**Figure 1** shows the cumulative distribution function (CDF) of normalized download throughput rates for four services: AT&T’s DSL, Clearwire, Comcast, and Frontier’s fiber. Frontier’s fiber service had the most consistent throughput, with 96% of measurements achieving at least 90% of the subscription speed. Comcast (cable) measurements often exceeded the subscription speed, with a median of 135% of the subscription speed. AT&T’s DSL service was consistent but rarely exceeded the subscription speed, with 48% of measurements below 90% of the subscription speed. Clearwire had the highest fraction of measurements (73%) below 90% of the subscription speed.

**Figure 2** illustrates the complementary cumulative distribution function (CCDF) of the fraction of a user’s download throughput measurements per month that are below a percentage of the subscription capacity. For AT&T, 47% of user-months experienced less than 90% of their service capacity at least once every other hour. In contrast, Comcast users experienced this only about 9% of the time. Frontier’s fiber service showed even less frequent degradations, with less than 3% of months experiencing throughput rates below 90% of the subscription speed.

**Figure 3** shows the CDF of upload throughput rates, which generally followed similar trends. Clearwire’s upload measurements were higher, more consistent, and closer to the subscription speed, with the median measurement at least 90% of the subscription speed for each ISP.

### Latency Analysis
We used the average latency (measured over an hour) to the nearest measurement server as an estimate of the subscriber’s latency. **Figure 4** presents the CDF of hourly average latency for five ISPs. Cablevision had the lowest latencies, with 96% of hourly averages below 20 ms. Other fiber, DSL, and cable ISPs had slightly higher but consistent latencies, with at least 90% of average latency reports being less than 70 ms. AT&T had the lowest latencies among DSL providers, with 95% of measurements below 57 ms. Clearwire’s latencies were notably higher, with a median of approximately 90 ms. Satellite providers had the highest latencies, consistently above 600 ms due to the inherent limitations of the technology.

### Packet Loss Analysis
Using the number of successful and failed UDP pings, we calculated the percentage of packets lost over each hour. **Figure 5** shows the CCDF of hourly packet loss rates for four ISPs. Fiber providers generally had lower loss rates, with Verizon having the lowest frequency of loss rates above 1%, occurring in only 0.82% of hours. Comcast and TimeWarner had the lowest frequency for cable providers, with loss rates above 1% occurring in approximately 1.5% of hours. Satellite providers had the highest frequency of loss rates above 1%, occurring during over 26% of hours.

### Applying Service Level Agreements (SLAs)
In Section 2, we defined SLAs in measurable terms with thresholds meaningful to users’ Quality of Experience. Building on our characterization, we explored how effectively today’s ISPs could meet our proposed set of SLAs.

**Figure 6** summarizes the total number of SLA violations per month for four example ISPs. AT&T struggled to meet SLA A but met SLA B and C for a significant portion of users. Clearwire faced difficulties meeting SLA A due to high latencies but performed better on SLA C. Both Comcast and Verizon’s fiber service did a good job of meeting all three SLAs, with Comcast meeting SLA A for 75% of users and Verizon for 83% of fiber subscribers. Both ISPs met SLA B and C for at least 90% of users.

### Summary and Conclusion
Moderate SLAs, requiring compliance up to 90% of the time, are feasible and could be offered by many ISPs with minimal impact on their current business. Stricter SLAs, requiring 99% or more compliance, would be more challenging. In the next section, we will examine the difficulty of assessing the individual risk of breaking an SLA, a central challenge in personalized SLA offerings.

### Personalized SLAs
SLAs can be seen as an insurance policy against poor broadband experience, which may have financial consequences. In the following section, we will explore the challenges and feasibility of offering personalized SLAs.