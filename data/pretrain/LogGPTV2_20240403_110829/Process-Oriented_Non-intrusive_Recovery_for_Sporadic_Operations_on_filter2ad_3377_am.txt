there is no acceptable recovery action found due to the fact that no recovery action satisfies all 
the three constraints. If this happens, we will try to find out those recovery actions which can 
satisfy two constraints simultaneously (Consrt and Consrc / Consrt and Consri / Consrc and Consri). 
If  there  is  no  recovery  action  found,  we  will  try  to  find  out  those  recovery  actions  which  can 
satisfy one constraint (Consrt / Consrc / Consri). Finally the first one in the acceptable recovery 
actions  list  is  selected.  This  logic  can  be  modelled  by  using  constraint  programming  and  the 
algorithm is shown in below Fig. 25 (M. Fu, et al., 2016). 
Algorithm 2: User Constraints Based Recovery Action Selection  
Suppose there are n recovery actions for a certain step section of operation and 
each recovery action is denoted by Ri, where 1 ≤ i ≤ n. The recovery actions list 
can be denoted by R, where R = {Ri | 1 ≤ i ≤ n}; 
For each recovery action Ri, the three metrics are denoted by RTi, RCi and RIi; 
The goal is to find the list of acceptable recovery actions denoted by Rx, where 
Rx = {Ri | 1 ≤ i ≤ n & RTi ≤ Consrt & RCi ≤ Consrc & RIi ≤ Consri}; 
If Rx is empty, we try to find the list of acceptable recovery actions denoted by 
Rx, where Rx = {Ri | 1 ≤ i ≤ n & RTi ≤ Consrt & RCi ≤ Consrc}; 
If Rx is empty, we try to find the list of acceptable recovery actions denoted by 
Rx, where Rx = {Ri | 1 ≤ i ≤ n & RTi ≤ Consrt & RIi ≤ Consri}; 
If Rx is empty, we try to find the list of acceptable recovery actions denoted by 
Rx, where Rx = {Ri | 1 ≤ i ≤ n & RCi ≤ Consrc & RIi ≤ Consri}; 
If Rx is empty, we try to find the list of acceptable recovery actions denoted by 
Rx, where Rx = {Ri | 1 ≤ i ≤ n & RTi ≤ Consrt}; 
If Rx is empty, we try to find the list of acceptable recovery actions denoted by 
Rx, where Rx = {Ri | 1 ≤ i ≤ n & RCi ≤ Consrc}; 
If Rx is empty, we try to find the list of acceptable recovery actions denoted by 
Rx, where Rx = {Ri | 1 ≤ i ≤ n & RIi ≤ Consri}; 
If Rx is still empty, then we assign Rx with the value of R; 
Finally, Rx
1 (Rx
1 ∈ Rx) which is the first element of Rx is selected. 
Fig. 25.  User Constraints Based Selection Algorithm. 
In the experimental evaluation, we will present the results of selecting the acceptable recovery 
actions based on each of the two selection and optimization mechanisms for each recovery point 
inside the sporadic operations evaluated (chapter 8). 
7.8 Mapping Recovery Actions into Executable Code 
The  selected  recovery  actions  are  only  the  recovery  logical  workflows  that  describe  how  the 
recovery  will  be  performed,  and  they  cannot  be  used  for  the  actual  recovery  until  they  are 
mapped  into  executable  code.  Therefore,  the  purpose  of  this  section  is  to  provide  information 
and  knowledge  about  how  to  map  the  selected  recovery  actions  into  executable  code  for 
87 
recovery realization. For each selected recovery action, each of the activities inside the recovery 
action  is  not  executable  until  they  are  mapped  into  the  executable  code  such  as  Java  SDK 
libraries, .Net framework C# SDK libraries and command lines. In POD-Recovery, we mapped 
them into C# code calling AWS cloud SDK libraries provided in .Net framework 4.0 of Visual 
Studio 2010, because POD-Recovery is  implemented as a web service using C# programming 
language and .Net framework 4.0. The name and parameters of each action in the recovery plan 
are  the  same  as  the  name  and  parameters  of  its  counterpart  C#  cloud  SDK  function  call,  for 
example,  the  action  “Start  Cloud  Instances”  is  mapped  into  the  C#  cloud  SDK  function  of 
“StartInstances”, which means launching an instance in the cloud platform. In the mapped code, 
the  method  of  guaranteeing  “recovery  for  recovery”  is  by  using  the  exception  handling 
mechanism. The exception handling  mechanism captures any errors that occur in the recovery 
action by using “try” block, and the errors will be recovered from by using “catch” block. In the 
“catch”  block,  the  current new  erroneous  resource  state and the  expected  resource  state  of  the 
current  recovery  point  will  be  obtained,  and  then  the  state  transition  plan  between  these  two 
states  will  be  generated  by  using  the  AI  planning  technique,  and  then  the  generated  state 
transition  plan  will  be  mapped  to  executable  code  such  as  C#  cloud  SDK  library  functions  or 
Java  cloud  SDK  library  functions, and finally  the  code  will  be  executed to  perform  “recovery 
for recovery” for each recovery point of the sporadic operation on cloud (M. Fu, et al., 2015; M. 
Fu, et al., 2016). 
88 
Chapter 8.  Experiments and Evaluation 
In this chapter, we discuss the experiments we performed to test POD-Recovery on AWS EC2 
(AWS, 2016). Our techniques can also be adapted to other cloud platforms as long as they can 
meet the five aforementioned assumptions: 1) the operation’s process model with the timestamp 
of each step can be mined from cloud operational logs; 2) cloud API call history logs with API 
call  timestamps  are  available  and  accessible  for  mapping  from  process steps to relevant  cloud 
APIs; 3) cloud logs are accessible for error detection; 4) cloud APIs are accessible by external 
parties  (e.g.  cloud  consumers  or  automation  tools  that  can  start  and  stop  virtual  machines, 
register them with load balancers, etc.); 5) cloud resources are accessible from external parties. 
But we only tested POD-Recovery on AWS and did not test it on other cloud platforms, because 
testing  POD-Recovery  on  other  clouds  is  just  the  same  as  testing  it  on  AWS  and  there  is  no 
much need to do so.  
The  main  purpose  of  the  experimental  evaluation  is  to  test  POD-Recovery  against  all  the 
recovery requirements formulated by us (M. Fu, et al., 2016). The eight recovery requirements 
are: 1) runtime recovery (R1); 2) recovery satisfying RTO (R2); 3) reducing negative impact on 
cloud system (R3); 4) reducing monetary cost of recovery (R4); 5) recovery from errors without 
known  causes  (R5);  6)  dealing  with  false  positives  of  error  detection  (R6);  7)  recovery  for 
recovery  itself  (R7);  8)  generalizability  of  recovery  (R8).  In  particular,  by  generalizability  of 
recovery,  we  mean  that  the  recovery  method  can  be  applicable  for  different  types  of  sporadic 
operations on cloud. This is important because there are different types of sporadic operations 
performed  on  cloud  and  they  are  all  performed  frequently.  Meanwhile,  we  argue  that  the 
generalizability of our recovery method is not affected when we perform sporadic operations on 
different  cloud  platforms,  because  the  operational  process,  the  cloud  API  calls,  the  logs 
generated  and  the  required  cloud  resources  of  the  sporadic  operations  are  all  the  same  across 
different  clouds,  and  the  assumptions  of  our  recovery  method  can  be  met  by  various  clouds. 
Among all major public clouds, AWS cloud is most widely used and it is representative of all 
major  public  clouds.  Hence,  we  demonstrate  the  generalizability  of  our  recovery  method  with 
different  types  of  sporadic  operations  on  AWS  cloud.  We  totally  use  five  representative 
sporadic  operations  on  cloud  to  test  POD-Recovery:  1)  rolling  upgrade;  2)  scale-up;  3)  scale-
down;  4)  installation;  and  5)  migration.  These  sporadic  operations  are  used  for  demonstrating 
the generalizability of POD-Recovery (requirement R8) as well as the remaining seven recovery 
requirements  (requirements  R1  to  R7).  Each  sporadic  operation  is  performed  on  a  simplified 
version of a real cloud system. In the evaluated cloud system, the functionalities are the same as 
the real cloud system, but the scale (i.e. instance number) is smaller than that of the real cloud 
system.  In  our  experiments,  the  rolling  upgrade  operation  is  performed  on  a  cloud  system 
89 
consisting  of  eight  web  server  instances;  the  installation  operation  is  performed  on  a  cloud 
system which consists of a web server instance and a database instance; the scale-up operation 
is  performed  on  a  cloud  system  consisting  of  twelve  web  server  instances;  the  scale-down 
operation  is  performed  on  a  cloud  system  consisting  of  twenty-four  web  server  instances;  the 
migration  operation is performed  on  a  cloud system  that contains a  web  server instance  and  a 
database  instance.  For  each  sporadic  operation,  we  first  illustrate  the  following  experimental 
results:  the  result  of  recovery  points  determination,  the  execution  time  of  resource  space 
generation and the execution time of expected resource state templates generation, and then we 
describe the particular experiments to test against the recovery requirements outlined in chapter 
3. For most recovery requirements, we describe the experimental procedures, the experimental 
results, and discuss how well POD-Recovery satisfies the respective recovery requirement. Our 
experiments are largely based on running the sporadic operations and injecting cloud faults into 
the  sporadic  operations,  e.g.  conducting  fault-injection  activities  such  as  terminating  a  VM  or 
changing  the  configuration  of  an  elastic  load  balancer  by  calling  the  relevant  cloud  APIs.  As 
such  we inject faults  into the  system,  but  because  these  faults  are immediately  activated,  they 
become  errors  directly.  They  may  or  may  not  lead  to  user-perceivable  failures,  directly  or 
eventually, but we focus on recovering from the errors and failures. 
In  the  experimental  evaluation,  we  also  show  the  benefits  of  POD-Recovery  over  the  other 
existing  recovery  methods  for  cloud  operations  in  terms  of  their  capability  of  fulfilling  the 
recovery requirements, by making comparisons among them. Nevertheless, one thing to note is 
that our approach is novel and categorically different from existing solutions, in that it is both 
non-intrusive and under the control of the cloud customer. 
The experimental environment is shown in Fig. 26. The cloud application runs in the AWS EC2 
cloud  platform.  The  settings  and  configurations  of  the  running  cloud  applications  may  vary, 
depending on what sporadic operations are being evaluated. For example, the cloud application 
we use for the rolling upgrade operation is comprised of several stateless web servers attached 
to an auto scaling group and registered in an elastic load balancer. The operation tool,  such as 
Asgard, runs as a  web app in a dedicated server, with its client side running on the operator’s 
machine.  The  operator  interacts  with  the  cloud  environment  and  the  associated  resources 
through operation tools like  Asgard. The fault-injection tool runs in the operator’s machine as 
well, and the faults are injected by calling relevant cloud APIs. Logs generated by Asgard and 
the cloud are collected by the LogStash service (LogStash, 2016), running in a LogStash server. 
The error detection service also runs in a dedicated server, and it relies on the logs collected by 
LogStash for detecting errors at the recovery points within the operation. POD-Recovery runs in 
90 
the recovery server. The recovery service is triggered by the error detection service (X. Xu, et 
al., 2014) to perform recovery.  
Fig. 26.  Experimental Environment. 
8.1  Case Study 1-Recovery for Rolling Upgrade Operation on Cloud 
In this section, we present the evaluation results of POD-Recovery by using the rolling upgrade 
operation on cloud as the case study. The purposes of this experiment are three-fold: 1) showing 
the  result  of  recovery  points  determination  for  rolling  upgrade;  2)  presenting  the  workload  of 
resource state management for rolling upgrade; 3) demonstrating POD-Recovery’s capability of 
satisfying  recovery  requirements  using  rolling  upgrade.  The  experimental  environment  is 
described in Fig. 26 above. 
8.1.1  Recovery Points Determination 
We define recovery points in an operational process to be the positions in the operation where 
the presence of errors should be checked and, if needed, recovery should be triggered. Recovery 
points are manually determined. To decide where to place recovery points, we adopt a criterion, 
called Recovery Actions Identifiable, from the literature (M. Fu, et al., 2014). This criterion is 
one of the three criteria for recovery points determination proposed by us (M. Fu, et al., 2014), 
and we only utilize this criterion because the recovery framework is focused on whether a step 
section of an operational process is recoverable or not. By applying this criterion, the operation 
can  be  divided  into  several  recoverable  step  sections,  where  each  step  section  ends  with  a 
91 
recovery point. Fig. 27 shows the sections resulting from recovery points at different levels of 
granularity. First, Fig. 27(a) is obtained by applying Recovery Actions Identifiable to the steps 
of rolling upgrade. Since each step has at least one recovery action, each step forms a section 
and  there  are  seven  sections.  Fig.  27(b)  combines  some  sections  with  low  failure  rates. 
According to the literature (Q. Lu, et al., 2014), activities such as “creating launch configuration” 
and  “update  auto  scaling  group”  have  low  failure  rates  whereas  activities  such  as  “launch 
instances”  and  “terminate  instances”  have  relatively  high  failure  rates.  So,  the  first  three 
sections  from  Fig.  27(a)  can  be  combined  into  one  section.  After  each  of  the  remaining  five 
sections there is a recovery point. When needed, recovery can happen after each recovery point. 
Fig. 27.  Determining Recovery Points for Rolling Upgrade Operation. 
8.1.2  Workload of Resource Space Determination 
The  determined  resource  space  contains  four  resource  types:  Instance  (i.e.  VM),  Launch 
Configuration  (LC),  Auto  Scaling  Group  (ASG),  and  Elastic  Load  Balancer  (ELB).  Each 
resource  type  has  attributes  (e.g.  instances  have  an  instance  id,  instance  type,  machine  image, 
etc.).  The  dependency  relationships  among  those  resources  are:  1)  an  ASG  is  associated  with 
one LC; 2) an ASG contains a (possibly empty) set of instances; 3) an ELB links to a (possibly 
empty) set of instances; 4) each instance belongs to at most one ASG; and 5) each instance can 
be  linked  to  a (possibly  empty)  set  of  ELBs. The  workload  of  determining  resource space  for 
rolling upgrade operation is shown in Fig. 28, which shows the execution time of the resource 
space  determination  algorithm  based  on  30  times  of  runs.  The  average  execution  time  is 
calculated  to  be  57.07ms,  and  the  relative  standard  deviation  is  calculated  to  be  10.96%.  The 
value of the 75th percentile point (75% percentile) is also calculated, which is 62.25ms.  
92 
Fig. 28.  Workload of Resource Space Generation for Rolling Upgrade Operation. 
8.1.3  Workload of Expected Resource State Templates Generation 
The  workload  of  generating  the  expected  resource  state  templates  for  the  rolling  upgrade 
operation  is  shown  in  Fig.  29,  which  shows  the  execution  time  of  expected  resource  state 
templates  generation  algorithm  based  on  30  times  of  runs.  The  average  execution  time  is 
calculated  to  be  60.24ms,  and  the  relative  standard  deviation  is  calculated  to  be  8.23%.  The 
value of the 75th percentile point (75% percentile) is also calculated, which is 63ms. 
Fig. 29.  Workload of Expected Resource State Templates Generation for Rolling Upgrade 
8.1.4  Recovery Satisfying Requirements 
Operation. 
POD-Recovery  is  designed  to  achieve  the  goal  of  satisfying  all  of  the  eight  recovery 
requirements  described  in  section  3.5.  Therefore,  experimental  evaluations  are  required  to 
justify  POD-Recovery’s  capability  of  satisfying  all  the  recovery  requirement  The  experiments 
conducted below are constructed in such a way that it can  demonstrate how POD-Recovery is 
able to fulfil each of the recovery requirements using the rolling upgrade operation.  
93 
A.  Runtime Recovery (Satisfying Recovery Requirement R1) 
As should be clear from the descriptions in chapters 4 to 7, our method is inherently a runtime 
recovery solution.  
B.  Recovery Satisfying RTO 
1)  Experimental Procedure 
This  recovery  requirement  concerns  the  time  the  recovery  takes  for  the  system  to  go  to  a 
consistent  state.  We  therefore  measure  recovery  time  by  performing  the  rolling  upgrade 
operation on a cloud application. There are 8 instances in the cloud application.  The instances 
are attached to an auto scaling group and are registered in an elastic load balancer. The cloud 
application  running  in  the  instances  is  TPC-W  which  is  a  3-tier  web  benchmark  application 
running  in  Tomcat  service.  The  average  workload  on  each  instance  is  50  query  requests  per 
second. For the rolling upgrade operation, the rolling granularity is 1, meaning that each time 1 
instance  will  be  terminated  and  replaced.  This  rolling  granularity  is  determined  by 
understanding  the  rolling  feature  provided  by  Asgard,  which  ensures  that  each  time  only  one 
instance  is  terminated  and  replaced  no  matter  what  granularity  is  used.  This  is  because  the 
rolling granularity in Asgard means that a certain number of instances should be in off-service 
mode  at  any  time,  rather  than  how  many  instances  should  be  terminated  simultaneously  per 
rolling  cycle.  We  used  our  own  proprietary  fault  injection  tool  to  inject  faults  for  each  of  the 
five determined recovery points in the rolling upgrade operation, automatically determined the 
applicable  recovery  patterns  based  on  the  applicable  recovery  patterns  filtering  method 
described in section 7.3, and performed recovery using the selected acceptable recovery action 
based on each of the two recovery action selection methods. The selected recovery action by the 
first selection  method is the  optimal  one; the selected  recovery  action  by  the  second  selection 
method  is  not  necessarily  the  optimal  one,  but  it  is  an  acceptable  one.  Therefore,  the  selected 
recovery actions by both of these two optimization methods are both acceptable ones. The faults 
injected are described in Table 9. The selection of injected faults is based on our interviews with 
industry  and  failure  reports,  and  hence  representative.  The  recovery  time  is  calculated  by 
summing up the execution time of each function in the recovery action. We ran the experiment 
30 times, measuring the recovery time.  
Table 9.  Faults injected for Rolling Upgrade Operation 
Recovery Point 
Fault injected 
Recovery Point 1 (After Step 3)  ASG uses unknown LC 
Recovery Point 2 (After Step 4) 
Recovery Point 3 (After Step 5) 
Recovery Point 4 (After Step 6) 
Recovery Point 5 (After Step 7) 
Instance still registered with ELB  
Instance not terminated 
Instance launching fails 
Instance not registered with ELB 
94 
2)  Experimental Results 
Table 10.  Recovery Time for Rolling Upgrade 
Legends 
Recovery Pattern ID  Recovery Pattern 
Recovery Pattern ID 
Recovery Pattern 
RP1 
RP2 
RP3 
RP4 
Compensated Undo & Redo 
RP5 
Compensated Undo & 
Alternative 
Rewind & Replay 
Rewind & Alternative 
RP6 
RP7 
RP8 
Reparation 
Direct Redo 
Direct Alternative 
Farther Undo & Redo 
Recovery 
Point 
Process 
Step 
Errors 
Applicable 
Recovery 
Patterns 
Selected 
Recovery 
Recovery 
Time 
Action(Pareto 
(Pareto 
Set) 
Set) 
Selected 
Recovery 
Action(User 
Constraints) 
Recovery 
Time (User 
Constraints
) 