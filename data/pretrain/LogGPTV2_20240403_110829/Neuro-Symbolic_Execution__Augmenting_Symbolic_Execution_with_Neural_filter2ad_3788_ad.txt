IMPLEMENTATION
We implement NEUEX in KLEE v1.4. We use KLEE-
uClibc [28] and Z3 SMT solver [53] as the back-end for
KLEE. Our implementation makes 351 LOC change to KLEE
for monitoring and passing run-time information to NEUEX.
We build our static analysis using Clang v3.4 and Clang Static
Analyzer [6] in 601 LOC. Rest of NEUEX is implemented in
Python and TensorFlow with 4635 LOC.
Proﬁler. We launch KLEE with its logging turned on, such
that it reports external calls, SMT query execution time, loop
unrolling, and increase in memory footprints due to path
explosion. Further, we also ask KLEE to log the instructions
that it symbolically executes. We directly use the appropriate
ﬂags provided by KLEE to turn on this logging. NEUEX’s
proﬁler continuously monitors these logs as they are being
written out. We coarsely detect loop unrolls via scanning the
instruction log i.e., when line numbers in the program code are
being periodically. Our proﬁler starts the neural mode process
whenever it detects the following events: (a) warnings for
external calls; (b) Z3 threshold limit capped at 10 minutes; (c)
loop unroll count is greater than 10, 000 (d) state termination
because of memory cap (3 GB). NEUEX launches a separate
neural-mode process which has an RPC tunnel to KLEE. It
then signals KLEE to pass the current program point current
symbolic state, the inputs, and the symbolic path constraints it
has collected so far to this newly created neural-mode process.
CVP Reachability. NEUEX reasons about an arbitrary size
program code by representing it as a neural net. When NEUEX
starts its neural mode it takes the symbolic states from KLEE
and sends a signal to KLEE to abort these states to continue
on a different path. We continue in neural mode from this
symbolic state onwards to multiple CVPs in the rest of the
program. Speciﬁcally, we select the closest k (default 150)
CVPs for each bug type in the static call graph. For each CVP,
we concretize all the symbolic states and create a random seed
for input generation. Speciﬁcally, we generate 20, 000 random
inputs and execute the program with these concrete inputs.
When NEUEX ﬁnds at least one input which reaches the CVP
of our choice, we consider that the CVP is reachable.
Sample Set Generation. We generate more inputs by using
the reachable input created in the previous step as the seed.
We randomly mutate the seed input to produce new inputs
and queue them in a working set. We persist this working
set in form of ﬁles for ease of use, thus freeing up memory.
NEUEX then spawns multiple new target programs with the
inputs from the working set in batches of 10, 000. At the end of
each execution, NEUEX logs the values of variables of interest
9
TABLE III.
NUMBER OF VERIFIED BUGS FOUND BY NEUEX VS. VANILLA KLEE IN A 12 HOUR RUN, EACH CONFIGURED WITH BFS AND RAND MODE IN
SEPARATE EXPERIMENTS. THE “COMBINED” COLUMN REPORTS THE TOTAL EXPLOITS FOUND IN EITHER MODE.
Program
Known CVEs
BFS
Vanilla KLEE
Random
Combined
BFS
NEUEX
Random
Combined
cURL
SQLite
libTIFF
libsndﬁle
BIND
Sendmail
WuFTP
[19]
[18]
[17], [23]–[25]
[20]–[22]
[3], [7], [13]
[4], [5], [8]–[10], [14], [15]
[11], [12], [16]
Total
No. of Unknown Exploits
1
0
0
0
1
11
4
17
8
2
0
0
0
1
11
4
18
9
2
0
0
0
1
11
4
18
9
1
2
4
3
5
12
7
33
11
2
2
4
3
5
12
7
34
12
2
2
4
3
5
12
7
34
12
at various CVPs as entries in the respective CVP’s sample set.
Our sample sets are in the form of input-output ﬁles per entry,
so we have conﬁgured all our programs to take ﬁle-based input
and produce ﬁle based outputs. We use unique ﬁle names for
each execution. We can scale this process to multiple cores
and/or physical machines since each execution is independent.
We implement our generator in Python with 656 LOC.
Side-effects. In our experiments, none of the programs read
or modify any global states of the machine environment (e.g.,
conﬁguration ﬁles). They only take in one input ﬁle and
optionally produce an output ﬁle or write to the console. Thus
it is safe to execute the same program multiple times with
unique input ﬁle names and redirect the output to different ﬁles.
In cases where this does not hold true, NEUEX piggybacks
on the environment modeling of KLEE. Speciﬁcally, KLEE
models a simple symbolic ﬁle system. It redirects all
the
environment related calls to stubs which model the behavior of
these ﬁle APIs. NEUEX hooks these stubs and in cases where
the data set generation may affect a global state on a write, we
instead redirect such calls to virtual ﬁles in memory with locks
for avoiding global state corruption. This way, we can isolate
global changes made by each execution. Further, our sample
set generation is a different process, so it does not interfere
with the execution of DSE mode of KLEE.
Training. Next, the neural constraint inference engine takes
80% of all the generated sample sets for training the neural
net. We use a standard MLP architecture with Relu activation
function implemented in Python and Google TensorFlow [29]
with a total of 208 LOC. We use the early-stopping strategy to
avoid over-ﬁtting. We test the remaining 20% of the sample set
on the learned neural net to measure its accuracy. We continue
the training until we achieve at least 80% accuracy. If the
loss of the trained neural net does not start decreasing after a
threshold of 50 epochs, we discard the search for an exploit
for the corresponding CVP.
Solver. Finally, NEUEX solves the learned neural constraints
along with the symbolic constraints collected from KLEE.
We implement our Algorithm 1, symbolic constraint transfor-
mation (Table II), and a standard gradient-based optimization
algorithm in Python with 849 LOC. Our implementation op-
tionally takes into consideration the type of the input variables
if it is easily available from the source code. This auxiliary
information helps us to select the step increment size, thus ac-
celerating the search for the exploit. Speciﬁcally, the step size
is integer value and ﬂoating-point value for integer and real
data-types, respectively. After each enumeration, we execute
the program with concrete outputs generated by our solver to
check if they indeed satisfy the neuro-symbolic constraints.
Parallelization. We have described all the steps for the end-
to-end working of NEUEX. NEUEX can execute these steps
sequentially or use parallelization to speed up certain tasks.
We conﬁgure NEUEX to execute on n cores. We dedicate one
core for KLEE’s classic DSE mode which is inherently serial,
and leverage the rest of the n − 1 cores for neural mode.
Our reachability check for CVPs, sample set generation, and
training steps are individually independent and can execute on
multiple cores. The inference step is strictly sequential for each
initial input, however, we can execute the solver with different
initial inputs in parallel. Further, processing for multiple CVPs
can be pipelined such that NEUEX starts processing the next
CVP while ﬁnishing the sequential steps of the previous CVP.
VI. EVALUATION
following empirical questions:
We show the effectiveness of NEUEX by answering the
•
•
Efﬁciency. Does NEUEX improve over vanilla KLEE?
Cost breakdown of neural mode. How many times is
the neural mode of NEUEX triggered? What are the
relative costs of various sub-steps in NEUEX?
Comparison with structured constraint inference tools.
How does the neural mode of NEUEX compare to
the structured inference approaches which augment
dynamic symbolic execution?
•
Experimental Setup. All our experiments are performed on a
40-core 2.6GHz 64GB RAM Intel Xeon machine. KLEE uses
a single core in its operations by design [40], whereas NEUEX
is highly parallelizable. We set thresholds for terminating DSE
to 10 minutes for Z3 per constraint, memory to 3 GB, loop
count to 10, 000, and the maximum enumeration of NeuSolv
Me to 50, 000. We avoid duplicates in counting vulnerabilities
by using the unique stack hash at the point of crash [78]. We
further verify that the input indeed triggers an exploit by re-
running the test ﬁles generated by NEUEX.4
Benchmarks. For evaluation, we select 7 programs reported
in Table III. Since our comparison is with DSE as a baseline,
we choose programs that are known to be difﬁcult for it
4KLEE reports a total of 34 bugs, out of which only 17 are unique and true
exploits. Rest of them are either duplicates or false-positives due to KLEE’s
imprecise internal modeling of the concrete memory.
10
TABLE IV.
NEUEX NEURAL MODE PERFORMANCE BREAKDOWN.
DETAILED STATISTICS OF VULNERABILITIES DISCOVERED BY THE
NEURAL MODE OF NEUEX IN 12 HOURS.
Program
cURL
SQLite
libTIFF
libsndﬁle
BIND
Sendmail
WuFTP
Total
#Times
Neural
Mode
Triggered
1
5
7
2
6
4
5
5
2
1
11
1
1
1
1
2
3
3
61
CVPs
Covered
in
Neural
Mode
#CVPs with
Sufﬁcient
Training
Dataset
Successfully
Learned
Networks
Veriﬁed
Exploits
Time
(hour)
6
24
6
14
17
9
14
14
4
7
13
3
5
5
5
6
3
4
159
2
12
2
5
7
4
4
4
1
5
4
2
3
3
1
1
1
3
64
1
3
1
2
1
1
4
4
1
1
1
1
1
1
1
1
1
3
29
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
20
7
0.7
5.8
7.6
11.9
2.7
2
1.3
8.5