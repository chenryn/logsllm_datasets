MEAN TOTAL PACKET COUNT PER WINDOW DURATION.
TABLE VI
interval
Tor ﬂow
Exit ﬂow
2
16
24
3
31
44
4
52
101
5
106
244
APPENDIX A
END-TO-END FLOW CORRELATION ATTACKS
End-to-end ﬂow correlation attacks are mentioned in some
of the earliest work on low-latency anonymous communi-
cations, typically being referred to as Last/First attacks or
Packet Counting attacks [10], [11], [41]. Since designs like
the Freedom Network and Tor introduce some basic amount
of padding that defeats simple packet counting, later works
on passive end-to-end attacks used statistical measures of
correlation (e.g., normalized distance metrics, Pearson and co-
sine correlation, empirical mutual information) between ﬂows
entering and exiting the network [42]–[45]. A separate line
of work has pursued active ﬂow correlation attacks that insert
“watermarks” into network ﬂows – by delaying or dropping
packets – that can survive the transformations introduced by
various network conditions [16], [46], [47].
While Tor does not attempt to defend against global passive
adversaries, Feamster and Dingledine [19] introduced the idea
of AS-level adversaries and showed that such adversaries could
potentially observe the entry and exit ﬂows of a signiﬁcant
fraction of the Tor network. Following this work, many
researchers investigated how routing dynamics and potential
manipulation of the routing infrastructure could position an
adversary to observe a larger fraction of trafﬁc ﬂows into and
out of the Tor network [13]–[15], [17], [18], [22], [23], [48],
[49], and introduced systems intended to reduce the fraction
of potentially observed ﬂows [20], [21], [24], [50].
APPENDIX B
CONVOLUTIONAL NEURAL NETWORKS
CNNs [51] are DNN architectures that learn local patterns in
input data by employing local ﬁlters. More speciﬁcally, ﬁlters
are vectors of weights that are convolved with input feature
vectors by “sliding” along the positions of the input vector and
computing local dot products to produce feature maps. These
feature maps are then “pooled” to reduce their dimensions
before further processing is applied to these maps. CNNs are
composed of multiple convolutional blocks where each block
consists of a convolutional layer, followed by a pooling layer.
These blocks are followed by fully connected layers that each
compute an output vector by applying an element-wise non-
linear activation function to multiple linear combinations of
the previous layer.
CNNs have recently been applied as classiﬁers and feature
extractors for website ﬁngerprinting and Tor ingress and egress
ﬂow correlation analysis [5], [6], [25]. In this paper, we used
Fig. 11. ROC for different ﬁlter applied to the preprocessing when loss ≈
0.006.
the architecture of Deep Fingerprinting (DF) [5] as a starting
point for our feature extractors based on the preliminary
experimental results presented in Section IV-C.
APPENDIX C
WINDOW SETTING INVESTIGATION
Packet Count per Interval. We reported the mean values of
the total number of packets in various intervals (seconds) in
Table VI. This preliminary study helped determine window
intervals in which each window carries enough packets to
make correlational trafﬁc analysis possible. We utilized this
result to determine the window interval search space as shown
in Table II.
Total Flow Duration. We presented the packet counts of 5-
second (non-overlapping) consequent subﬂows for 60 seconds.
Table VII shows that the packet count becomes less than 100
after 35 seconds. Based on this observation, we conﬁgured the
search space for the total ﬂow duration up to 35 seconds in
Section V-C and ﬁnally chose 25 seconds after hyperparameter
tuning.
Window Length. We presented the packet counts of 11
windows and those are counts after padding or truncating
ﬂows until the same packet counts remain in each window
in Table VIII. Since we used one Tor FEN and one Exit FEN,
the input dimensions for all windows have to be equal, in our
training, we used 500 packets for Tor traces and 800 packets
for exit traces in all windows.
Fig. 12. ROC of DeepCorr (DC) by varying the ﬂow length (i.e., the number
of packets) (Note that x-axis is log scale and DC(w) is when evaluating
DeepCorr in the window setting).
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:58:24 UTC from IEEE Xplore.  Restrictions apply. 
1930
window
Tor
Exit
[0, 5)
106
244
THE MEDIAN NUMBER OF PACKETS (PKT) FOR EACH INTERVAL (TOTAL FLOW DURATION: 60SECONDS).
[5, 10)
[25, 30)
[10, 15)
[15, 20)
[30, 35)
[35, 40)
[40, 45)
[45, 50)
[20, 25)
[50, 55)
TABLE VII
458
836
497
797
421
624
290
391
169
202
104
114
65
66
48
48
37
37
27
27
[55, 60)
29
26
THE MEDIAN NUMBER OF PACKETS (PKT) FOR EACH INTERVAL (TOTAL FLOW DURATION: 25SECONDS AND TOTAL: THE TOTAL NUMBER OF PACKETS OF
window
Tor
Exit
0
106
244
1
271
583
2
415
797
3
481
855
7
439
658
8
399
586
9
347
496
10
299
391
total
2003
3441
TABLE VIII
25 SECOND FLOW).
6
479
727
4
501
846
5
497
797
DEEPCOFFEA PERFORMANCE (%) PER WINDOW WHEN κ = 261.
9
1
2
3
4
5
6
7
8
TABLE IX
98.8
12.5
97.9
12.5
98.2
12.5
97.3
12.5
96.9
12.5
97.4
12.5
97.3
12.5
97.8
12.5
97.6
12.5
w
T
F
0
97.2
12.5
10
97.5
12.5
DEEPCORR WITH VARIOUS FEATURE DIMENSIONS
APPENDIX E
In Section VI-C, using 5,000 training and 2,093 testing
ﬂow pairs, we tuned DeepCorr to maximize its performance
for a fair comparison. First, we started training the DeepCorr
using 5,000 ﬂow pairs as DeepCorr used this scale in most
evaluations in the paper, and kept increasing up to 10,000
unique connections. The performance of DeepCorr did not
improve with more training data, rather, it became worse using
10,000 pairs. Thus, we chose 5,000 ﬂow pairs as a more
effective number of training ﬂows. After that, we explored
various packet counts per training ﬂow up to 2,200 packets.
According to Figure 12, we decided to use 700 packets as
the ﬂow length since the performance became more effective
than other ﬂow lengths. Even though it was unable to train
DeepCorr using ﬂows containing more than 1,000 packets due
to the limited resource, the performance of DeepCorr unlikely
improves using more packets since it degraded against 1,000
packet ﬂows.
APPENDIX F
m-DEEPCORR TUNING
In this section, we investigated m-DeepCorr in a variety of
multi-stage settings. To gain the upgraded performance along
with the reduced correlation complexity, the latter stage of
DeepCorr trained using N packet ﬂows should outperform
the former stage of DeepCorr trained using M packets when
M < N. Based on this insight and Figure 12 of Appendix E,
we chose 100, 300, 500, and 700 as the ﬂow lengths since
they led to the performance improvement compared to shorter
dimensions and set 700 as the longest feature dimension since
the performance did not improve after 700 packets. The goal of
this analysis was to ﬁnd the multi-stage setting which achieves
the effective performance while yielding the acceptable time
complexity.
We trained four DeepCorr models using p packets of
training data (namely DCp in which p=100, 300, 500, and
700) and then tested them in various r-stage settings in which
1 ≤ r ≤ 4 using testing traces. For example, in one of 2-
stage settings, 100 → 700, we ﬁrst tested DC100 using all
Fig. 13. ROC of m-DeepCorr (m-DC) by varying the multi-staged settings .
DEEPCOFFEA IN VARIOUS EXPERIMENTAL SCENARIOS.
APPENDIX D
Impact of Padding Amount. The connections to different
websites should carry a variety of packet counts since the
corpus of site sizes is tremendous. The DCF set was collected
using all unique 60,084 sites which led to a fair number
of ﬂows to be padded or truncated. In this experiment, we
investigated the impact of the connections to smaller sites on
the DeepCoFFEA performance. We applied different ﬁlters
when preprocessing the training and testing sets to ﬁlter out
the connections whose trace packet counts were less than h.
As shown in Figure 11, we adopted h from 70, 300, and 500
and the DeepCoFFEA performance was hardly affected by
the inclusion of smaller sites, in other words, the increased
amount of padding. Thus, we selected 70 for h to evaluate the
transferability of the DeepCoFFEA across numerous sites in
a more realistic setting by adding more diversity to the site
load sizes.
Per-Window Performance. To quantify the ampliﬁcation
capability to reduce the false positive count, we ﬁrst evaluated
DeepCoFFEA within each window and then aggregated the
results with voting for comparison. As shown in Table IX,
the DeepCoFFEA behavior was consistent across all windows.
More interestingly, its performance was signiﬁcantly beneﬁted
by the ampliﬁcation since FPR decreased from 12.5% to
0.13%. As such, the ampliﬁcation plays a key role in boosting
the performance of DeepCoFFEA.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:58:24 UTC from IEEE Xplore.  Restrictions apply. 
1931
testing ﬂow pairs and then, tested DC700 using the correlated
ﬂows determined by DC100. After investigating all possible
seven settings 6, we reported our exploration in Figure 13. We
excluded the results for 300→700 and 100→300→500→700
because we did not achieve better performance than 1-stage
setting (i.e., 700) and 3-stage settings, respectively. Compared
to 100 and 500, subsequent attacks such as 100→700 and
500→700 improved the overall performance. However, com-
pared to 1-stage attack (i.e., 700), multi-stage attacks led to
almost comparable (i.e., 100→700 and 500→700) or worse
performance (i.e., 100→300→700, 100→500→700). Based
on this study, we chose 100→700 and used it in Section VI-C
as m-DeepCorr since it gave us more effective correlation
capability than others at less time complexity than 500→700.
6There were 700, 100→700, 300→700, 500→700, 100→300→700,
100→500→700, and 100→300→500→700.
1932
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:58:24 UTC from IEEE Xplore.  Restrictions apply.