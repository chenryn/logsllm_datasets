distribution of the 𝜖0-LDP mechanism R when the input data point
is 𝑑𝑖, and 𝒑′
𝑛 denote the distribution of R when the input data
point is 𝑑′
𝑛. The main idea of the proof is the observation that each
distribution 𝒑𝑖 can be written as the following mixture distribu-
𝑒𝜖0 𝒑′
1
tion: 𝒑𝑖 =
associated with 𝒑𝑖. So, instead of client 𝑖 ∈ [𝑛 − 1] mapping its
data point 𝑑𝑖 according to 𝒑𝑖, we can view it as the client 𝑖 maps
𝑑𝑖 according to 𝒑′
𝑛 with probability 1
𝑒𝜖0 and according to ˜𝒑𝑖 with
probability (1 − 1
𝑒𝜖0 ). Thus the number of clients that sample from
the distribution 𝒑′
𝑛 follows a binomial distribution Bin(𝑛−1, 𝑞) with
𝑒𝜖0 . This allows us to write the distribution of M
1
parameter 𝑞 =
when clients map their data points according to 𝒑1, . . . , 𝒑𝑛, 𝒑′
𝑛 as
a convex combination of the distribution of M when clients map
their data points according to ˜𝒑1, . . . , ˜𝒑𝑛−1, 𝒑𝑛, 𝒑′
𝑛; see Lemma 5.1.
Then using a joint convexity argument (see Lemma 5.2), we write
the Rényi divergence between the original pair of distributions
of M in terms of the same convex combination of the Rényi di-
vergence between the resulting pairs of distributions of M as in
Lemma 5.1. Using a monotonicity argument (see Lemma 5.3), we
can remove the effect of clients that do not sample from the distri-
bution 𝒑′
𝑛 without decreasing the Rényi divergence. By this chain
of arguments, we have reduced the problem to the one involving
the computation of Rényi divergence only for the special form of
neighboring datasets, which proves Theorem 3.6. Details can be
found in Section 5.
3.3.2 Proof Sketch of Theorem 3.7. Consider any pair of special
neighboring datasets (D𝑚, D′
𝑚) ∈ D𝑚same for any 𝑚 ∈ N. Using
the polynomial expansion, we get
(cid:34)(cid:18)M(D′
(cid:18)𝜆
(cid:19)
𝜆∑︁
M(D𝑚)(𝒉) − 1(cid:17). With this, we can rewrite (15) in terms
𝑋(𝒉) = 𝑚(cid:16) M(D′
𝐵 → R denote a random variable (r.v.) associated with
𝐵 , is defined as
Let 𝑋 : A𝑚
the distribution M(D𝑚), and for every 𝒉 ∈ A𝑚
𝑚)(𝒉)
M(D𝑚)(𝒉)
(cid:34)(cid:18)M(D′
M(D𝑚)(𝒉) − 1
𝑚)(𝒉)
of the moments of 𝑋. Then we show that 𝑋 is a sub-Gaussian r.v. that
E𝒉∼M(D𝑚)
E𝒉∼M(D𝑚)
(cid:19)𝜆(cid:35)
(cid:19)𝑖(cid:35)
𝑖
𝑖=0
𝑚)(𝒉)
(15)
=
.
has zero-mean and bounded variance. Using the sub-Gaussianity
of 𝑋, we bound its higher moments (see Lemma 6.1). Substituting
these bounds in (15) proves Theorem 3.7. Details can be found in
Section 6.
4 NUMERICAL RESULTS
In this section, we present numerical experiments to show the
performance of our bounds on the RDP of the shuffle model and its
usage for getting approximate DP and composition results.
RDP of the shuffle model: In Figure 2, we plot several bounds on
the RDP of the shuffle model in different regimes. In particular, we
compare between the first upper bound on the RDP given in Theo-
rem 3.1, the second upper bound on the RDP given in Theorem 3.3,
the lower bound on the RDP given in Theorem 3.4, and the upper
bound on the RDP given in [22, Remark 1] and stated in (9).6 It is
clear that our first upper bound (5) gives a tighter bound on the
RDP in comparison with the second bound (8) and the upper bound
given in [22]. Furthermore, the first upper bound is close to the
lower bound for small values of the LDP parameter 𝜖0 and for high
orders 𝜆. In addition, the gap between our proposed bound in Theo-
rem 3.1 and the bound given in [22] increases as the LDP parameter
𝜖0 increases. We also observe that the curves of the lower and upper
bounds on the RDP of the shuffle model saturate close to 𝜖0 when
the order 𝜆 approaches to infinity. This indicates that the pure DP
of the shuffle model is bounded below by 𝜖0, an observation made
in literature [3, 22]. As can be seen in Figures 2d and 2e, the RDP
obtained by standard approximate DP to RDP conversion in [22,
Remark 1], can be several orders of magnitude loose in comparison
to our analysis.
Approximate DP of the shuffle model: Analyzing RDP of the shuf-
fle model provides a bound on the approximate DP of the shuffle
model from the relation between the RDP and approximate DP as
shown in Lemma 2.1. In Figure 3, we plot several bounds on the
approximate (𝜖, 𝛿)-DP of the shuffle model for fixed 𝛿 = 10−6. In
Figures 3d and 3b, we do not plot the results given in [22], since
their bounds are quite loose and are far from the plotted range
when 𝜖0 > 1. We can see that our analysis of the RDP of the shuf-
fle model provides a tighter bound on the approximate DP of the
shuffle model in comparison with the bound given in [7] in some
regimes. However, our RDP analysis performs worse than the best
known bound given in [24], when used without composition. This
might be due to the gap between our upper and lower bound on
the RDP of the shuffle model as the lower bound provides better
performance than the bound given in [24] for all values of LDP
parameter 𝜖0. Note that the main use case for converting our RDP
analysis to approximate DP is after composition rather than in the
single-shot conversion illustrated in Figure 3.
Composition of a sequence of shuffle models: We now numerically
evaluate the privacy parameters of the approximate (𝜖, 𝛿)-DP for
a composition of 𝑇 mechanisms (M1, . . . ,M𝑇), where M𝑡 is a
shuffle mechanism for all 𝑡 ∈ [𝑇]. In Figure 4, we plot three different
bounds on the overall privacy parameter 𝜖 for fixed 𝛿 = 10−8 for
a composition of 𝑇 identical shuffle models. The first bound on
6The results in [24] are for approximate DP (not for RDP), that is why we did not
compare with them in Figure 2.
Session 7D: Privacy for Distributed Data and Federated Learning CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea 2327(a) RDP as a function of 𝜆 for 𝜖0 = 0.1 and
𝑛 = 104
(b) RDP as a function of 𝑛 for 𝜖0 = 0.1 and
𝜆 = 100
(c) RDP as a function of 𝜖0 for 𝑛 = 104 and
𝜆 = 100
(d) RDP as a function of 𝜆 for 𝜖0 = 3 and
𝑛 = 104
(e) RDP as a function of 𝑛 for 𝜖0 = 3 and
𝜆 = 100
(f) RDP as a function of 𝜖0 for 𝑛 = 106 and
𝜆 = 100
Figure 2: Comparison of several bounds on the RDP of the shuffle model: (i) Our first upper bound (5) in Theorem 3.1; (ii) Our
second upper bound (8) in Theorem 3.1; (iii) Our lower bound proposed in Theorem 3.4; and (iv) The upper bound on the RDP of
the shuffle model given in [22, Remark 1].
(a) Approximate DP as a function of 𝑛 for 𝜖0 = 0.1
and 𝛿 = 10−6
(b) Approximate DP as a function of 𝜖0 for 𝑛 = 104
and 𝛿 = 10−6
Figure 3: Comparison of sev-
eral bounds on the Approxi-
mate (𝜖, 𝛿)-DP of the shuffle
model for 𝛿 = 10−6: (i) Approx-
imate DP obtained from our
first upper bound (5) of the
RDP in Theorem 3.1; (ii) Ap-
proximate DP obtained from
our lower bound on the RDP
proposed in Theorem 3.4; (iii)
The empirical upper bound
on the approximate DP given
in [24]; (iv) The theoretical
bound on the approximate
DP given in [22]; and (v) The
generic bound on the approx-
imate DP given in [7].
(c) Approximate DP as a function of 𝑛 for 𝜖0 = 3 and
𝛿 = 10−6
(d) Approximate DP as a function of 𝜖0 for 𝑛 = 105
and 𝛿 = 10−6
the overall privacy parameter 𝜖 is obtained as a function of 𝛿 and
the number of iterations 𝑇 by optimizing over the RDP order 𝜆
using our upper bound on the RDP of the shuffle model given in
Theorem 3.1. The second bound is obtained by optimizing over the
RDP order 𝜆 using the upper bound on the RDP of the shuffle model
given in [22]. The third bound is obtained by first computing the
101102103104105106RDP order λ10−610−510−410−310−210−1100RDP ε(λ)ε0=0.1,n=104RDP (1st Upper bound)RDP (2nd Upper bound)RDP [EFM+19]RDP (Lower bound)104105106Number of clients n10−610−510−4RDP ε(λ)ε0=0.1,λ=100RDP (1st Upper bound)RDP (2nd Upper bound)RDP [EFM+19]RDP (Lower bound)012345LDP parameter ε010−410−21001021041061081010RDP ε(λ)n=104,λ=100RDP (1st Upper bound)RDP (2nd Upper bound)RDP [EFM+19]RDP (Lower bound)101102103104105106RDP order λ10−21001021041061081010RDP ε(λ)ε0=3.0,n=104RDP (1st Upper bound)RDP (2nd Upper bound)RDP [EFM+19]RDP (Lower bound)104105106Number of clients n10−2100102104106RDP ε(λ)ε0=3.0,λ=100RDP (1st Upper bound)RDP (2nd Upper bound)RDP [EFM+19]RDP (Lower bound)012345LDP parameter ε010−610−410−2100102104106108RDP ε(λ)n=106,λ=100RDP (1st Upper bound)RDP (2nd Upper bound)RDP [EFM+19]RDP (Lower bound)104105106Number of clients n10−310−2Approximate DP εε0=0.1,δ=10−6via RDP (1st upper bound)via RDP (lower bound)Clones, empirical[FMT20][EFM+19]Blanket, empirical[BBGN'19]0.00.51.01.52.02.53.0LDP parameter ε010−210−1100Approixmate DP εn=104,δ=10−6via RDP (1st upper bound)via RDP (lower bound)Clones, empirical[FMT20]Blanket, empirical[BBGN'19]104105106Number of clients n10−210−1100101102103104Approximate DP εε0=3.0,δ=10−6via RDP (1st upper bound)via RDP (lower bound)Clones, empirical[FMT20][EFM+19]Blanket, empirical[BBGN'19]012345LDP parameter ε010−310−210−1100Approixmate DP εn=105,δ=10−6via RDP (1st upper bound)via RDP (lower bound)Clones, empirical[FMT20]Blanket, empirical[BBGN'19]Session 7D: Privacy for Distributed Data and Federated Learning CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea 2328(a) Approximate DP as a function of 𝑇 for
𝜖0 = 0.5 and 𝑛 = 106
(b) Approximate DP as a function of 𝑛 for
𝜖0 = 0.5 and 𝑇 = 104
(c) Approximate DP as a function of 𝜖0 for
𝑛 = 105 and 𝑇 = 104
(d) Approximate DP as a function of 𝑇 for
𝜖0 = 2 and 𝑛 = 106
(e) Approximate DP as a function of 𝑛 for
𝜖0 = 2 and 𝑇 = 104
(f) Approximate DP as a function of 𝜖0 for
𝑛 = 106 and 𝑇 = 104
Figure 4: Comparison of several bounds on the Approximate (𝜖, 𝛿)-DP for composition of a sequence of shuffle models for
𝛿 = 10−8: (i) Approximate DP obtained from our first upper bound (5) on the RDP; (ii) Approximate DP obtained from our lower
bound on the RDP proposed in Theorem 3.4; (iii) Approximate DP obtained from the upper bound on the RDP given in [22];
and (iv) Applying the strong composition theorem [33] after getting the approximate DP of the shuffle model given in [24].
privacy parameters ( ˜𝜖, ˜𝛿) of the shuffle model given in [24]. Then,
we use the strong composition theorem given in [33] to obtain the
overall privacy loss 𝜖. We observe that there is a significant saving
in the overall privacy parameter 𝜖-DP using our bound on RDP
in comparison with using the bound on DP [24] with the strong
composition theorem [33]. For example, we save a factor of 8× in
computing the overall privacy parameter 𝜖 for number of iterations
𝑇 = 105, LDP parameter 𝜖0 = 0.5, and number of clients 𝑛 = 106. We
observe that the bound given in [24] with the strong composition
theorem [33] behaves better for small number of iterations 𝑇 < 10
and large LDP parameter 𝜖0 = 2. However, the typical number
of iterations 𝑇 in the standard SGD algorithm is usually larger.
Therefore, this demonstrates the significance of our RDP analysis
for composition in the regimes of interest.
Privacy amplification by shuffling and Poisson sub-sampling: In
the Differentially Private Stochastic Gradient Descent (DP-SGD),
shuffling and sampling the dataset at each iteration are important
tools to provide a strong privacy guarantee [20, 28]. In these frame-
works, the further advantage of sampling with shuffling7 can be
analyzed by standard combination of approximate DP with Poisson
subsampling [35]. The resulting approximate DP along with the
strong composition theorem given in [33] gives the overall privacy
loss 𝜖. An alternate path we use is to combine our RDP analysis
7In this framework we assume that the sampling and shuffling is done by a secure
mechanism which is separated from the server, i.e., the server does not know which
clients are participating.
with sampling of RDP mechanisms using [40, 43]. This enables us to
get an RDP guarantee with sampling, which we can then compose
using properties of RDP. We can use the conversion from RDP to
approximate DP to obtain a bound on the overall privacy loss of
multiple iterations. In Figure 5, we compare our results of amplify-
ing the RDP of the shuffle model by Poisson sub-sampling to the
strong composition [33] after getting the approximate DP of the
shuffle model given in [24] with Poisson sub-sampling given in [35].
We observe that we save a factor of 11× by using our RDP bound for
𝑛 = 106 and 𝛾 = 0.001. However, we can see that the gap between
our (lower/upper) bounds and the strong composition decreases
when 𝑛 = 107. This could be due to the simplistic combination of
our analysis with the RDP subsampling of [43].
5 PROOF OF THE REDUCTION TO THE
SPECIAL CASE
In this section, we will prove Theorem 3.6 by reducing the problem
of computing RDP for the arbitrary pairs of neighboring datasets to
the problem of computing RDP for the neighboring datasets with
the special structure.
Recall that the LDP mechanism R : X → Y has a discrete
range Y = [𝐵] for some 𝐵 ∈ N. Let 𝒑𝑖 := (𝑝𝑖1, . . . , 𝑝𝑖𝐵) and 𝒑′
𝑛 :=
(𝑝′
𝑛1, . . . , 𝑝′
𝑛𝐵) denote the probability distributions over Y when the
input to R is 𝑑𝑖 and 𝑑′
𝑛, respectively, where 𝑝𝑖 𝑗 = Pr[R(𝑑𝑖) = 𝑗] and
𝑛 𝑗 = Pr[R(𝑑′
𝑝′
𝑛) = 𝑗] for all 𝑗 ∈ [𝐵] and 𝑖 ∈ [𝑛]. Let P = {𝒑𝑖 : 𝑖 ∈
𝑛}. For 𝑖 ∈ [𝑛 − 1], let P−𝑖 =
[𝑛]} and P′ = {𝒑𝑖 : 𝑖 ∈ [𝑛 − 1]}{𝒑′
101102103104105Number of iterations T10−210−1100101Approximate DP εε0=0.5,n=106,δ=10−8via RDP (1st upper bound)via RDP (lower bound)via RDP [EFM+19]Clones[FMT20]+strong composition[KOV15]104105106Number of clients n100101Approximate DP εT=10000,ε0=0.5,δ=10−8via RDP (1st upper bound)via RDP (lower bound)via RDP [EFM+19]Clones[FMT20]+strong composition[KOV15]0.00.51.01.52.02.53.0LDP parameter ε010−1100101102103Approximate DP εT=10000,n=105,δ=10−8via RDP (1st upper bound)via RDP (lower bound)via RDP [EFM+19]Clones[FMT20]+strong composition[KOV15]101102103104105Number of iterations T10−210−1100101102103Approximate DP εε0=2.0,n=106,δ=10−8via RDP (1st upper bound)via RDP (lower bound)via RDP [EFM+19]Clones[FMT20]+strong composition[KOV15]104105106Number of clients n100101102103104105Approximate DP εT=10000,ε0=2.0,δ=10−8via RDP (1st upper bound)via RDP (lower bound)via RDP [EFM+19]Clones[FMT20]+strong composition[KOV15]0.00.51.01.52.02.53.0LDP parameter ε010−1100101102Approximate DP εT=10000,n=106,δ=10−8via RDP (1st upper bound)via RDP (lower bound)via RDP [EFM+19]Clones[FMT20]+strong composition[KOV15]Session 7D: Privacy for Distributed Data and Federated Learning CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea 2329(a) Approximate DP as a function of 𝑇
for 𝜖0 = 3, 𝛾 = 0.001 and 𝑛 = 106.
(b) Approximate DP as a function of 𝑛
for 𝜖0 = 3, 𝛾 = 0.001 and 𝑛 = 107.
Figure 5: Comparison of several bounds on the Ap-
proximate (𝜖, 𝛿)-DP for composition a sequence
of shuffle models with Poisson sub-sampling for
𝛿 = 10−8 and 𝛾 = 0.001: (i) Approximate DP ob-
tained from our first upper bound (5) on the RDP
in Theorem 3.1 with RDP amplification by Poisson
sub-sampling [43]; (ii) Approximate DP obtained
from our lower bound on the RDP proposed in
Theorem 3.4 with RDP amplification by Poisson
sub-sampling [43]; and (iii) Applying the strong
composition theorem [33] after getting the ap-
proximate DP of the shuffle model given in [24]
with Poisson sub-sampling [35].
P\{𝒑𝑖}, P′−𝑖 = P′\{𝒑𝑖}, and also P−𝑛 = P\{𝒑𝑛}, P′−𝑛 = P′\{𝒑′
𝑛}.
Here, P, P′ correspond to the datasets D = {𝑑1, . . . , 𝑑𝑛}, D′ =
{𝑑1, . . . , 𝑑𝑛−1, 𝑑′
𝑛}, respectively, and for any 𝑖 ∈ [𝑛], P−𝑖 and P′−𝑖
correspond to the datasets D−𝑖 = {𝑑1, . . . , 𝑑𝑖−1, 𝑑𝑖+1, . . . , 𝑑𝑛} and
D′−𝑖 = {𝑑1, . . . , 𝑑𝑖−1, 𝑑𝑖+1, . . . , 𝑑𝑛−1, 𝑑′
𝑛}, respectively.
For any collection P = {𝒑1, . . . , 𝒑𝑛} of 𝑛 distributions, we define
𝐹(P) to be the distribution over A𝑛
𝐵 (which is the set of histograms
on 𝐵 bins with 𝑛 elements as defined in (4)) that is induced when
every client 𝑖 (independent to the other clients) samples an element
from [𝐵] accordingly to the probability distribution 𝒑𝑖. Formally,
for any 𝒉 ∈ A𝑛