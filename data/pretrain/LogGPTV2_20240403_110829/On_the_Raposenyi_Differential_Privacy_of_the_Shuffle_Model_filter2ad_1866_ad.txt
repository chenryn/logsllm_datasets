distribution of the ğœ–0-LDP mechanism R when the input data point
is ğ‘‘ğ‘–, and ğ’‘â€²
ğ‘› denote the distribution of R when the input data
point is ğ‘‘â€²
ğ‘›. The main idea of the proof is the observation that each
distribution ğ’‘ğ‘– can be written as the following mixture distribu-
ğ‘’ğœ–0 ğ’‘â€²
1
tion: ğ’‘ğ‘– =
associated with ğ’‘ğ‘–. So, instead of client ğ‘– âˆˆ [ğ‘› âˆ’ 1] mapping its
data point ğ‘‘ğ‘– according to ğ’‘ğ‘–, we can view it as the client ğ‘– maps
ğ‘‘ğ‘– according to ğ’‘â€²
ğ‘› with probability 1
ğ‘’ğœ–0 and according to Ëœğ’‘ğ‘– with
probability (1 âˆ’ 1
ğ‘’ğœ–0 ). Thus the number of clients that sample from
the distribution ğ’‘â€²
ğ‘› follows a binomial distribution Bin(ğ‘›âˆ’1, ğ‘) with
ğ‘’ğœ–0 . This allows us to write the distribution of M
1
parameter ğ‘ =
when clients map their data points according to ğ’‘1, . . . , ğ’‘ğ‘›, ğ’‘â€²
ğ‘› as
a convex combination of the distribution of M when clients map
their data points according to Ëœğ’‘1, . . . , Ëœğ’‘ğ‘›âˆ’1, ğ’‘ğ‘›, ğ’‘â€²
ğ‘›; see Lemma 5.1.
Then using a joint convexity argument (see Lemma 5.2), we write
the RÃ©nyi divergence between the original pair of distributions
of M in terms of the same convex combination of the RÃ©nyi di-
vergence between the resulting pairs of distributions of M as in
Lemma 5.1. Using a monotonicity argument (see Lemma 5.3), we
can remove the effect of clients that do not sample from the distri-
bution ğ’‘â€²
ğ‘› without decreasing the RÃ©nyi divergence. By this chain
of arguments, we have reduced the problem to the one involving
the computation of RÃ©nyi divergence only for the special form of
neighboring datasets, which proves Theorem 3.6. Details can be
found in Section 5.
3.3.2 Proof Sketch of Theorem 3.7. Consider any pair of special
neighboring datasets (Dğ‘š, Dâ€²
ğ‘š) âˆˆ Dğ‘šsame for any ğ‘š âˆˆ N. Using
the polynomial expansion, we get
(cid:34)(cid:18)M(Dâ€²
(cid:18)ğœ†
(cid:19)
ğœ†âˆ‘ï¸
M(Dğ‘š)(ğ’‰) âˆ’ 1(cid:17). With this, we can rewrite (15) in terms
ğ‘‹(ğ’‰) = ğ‘š(cid:16) M(Dâ€²
ğµ â†’ R denote a random variable (r.v.) associated with
ğµ , is defined as
Let ğ‘‹ : Ağ‘š
the distribution M(Dğ‘š), and for every ğ’‰ âˆˆ Ağ‘š
ğ‘š)(ğ’‰)
M(Dğ‘š)(ğ’‰)
(cid:34)(cid:18)M(Dâ€²
M(Dğ‘š)(ğ’‰) âˆ’ 1
ğ‘š)(ğ’‰)
of the moments of ğ‘‹. Then we show that ğ‘‹ is a sub-Gaussian r.v. that
Eğ’‰âˆ¼M(Dğ‘š)
Eğ’‰âˆ¼M(Dğ‘š)
(cid:19)ğœ†(cid:35)
(cid:19)ğ‘–(cid:35)
ğ‘–
ğ‘–=0
ğ‘š)(ğ’‰)
(15)
=
.
has zero-mean and bounded variance. Using the sub-Gaussianity
of ğ‘‹, we bound its higher moments (see Lemma 6.1). Substituting
these bounds in (15) proves Theorem 3.7. Details can be found in
Section 6.
4 NUMERICAL RESULTS
In this section, we present numerical experiments to show the
performance of our bounds on the RDP of the shuffle model and its
usage for getting approximate DP and composition results.
RDP of the shuffle model: In Figure 2, we plot several bounds on
the RDP of the shuffle model in different regimes. In particular, we
compare between the first upper bound on the RDP given in Theo-
rem 3.1, the second upper bound on the RDP given in Theorem 3.3,
the lower bound on the RDP given in Theorem 3.4, and the upper
bound on the RDP given in [22, Remark 1] and stated in (9).6 It is
clear that our first upper bound (5) gives a tighter bound on the
RDP in comparison with the second bound (8) and the upper bound
given in [22]. Furthermore, the first upper bound is close to the
lower bound for small values of the LDP parameter ğœ–0 and for high
orders ğœ†. In addition, the gap between our proposed bound in Theo-
rem 3.1 and the bound given in [22] increases as the LDP parameter
ğœ–0 increases. We also observe that the curves of the lower and upper
bounds on the RDP of the shuffle model saturate close to ğœ–0 when
the order ğœ† approaches to infinity. This indicates that the pure DP
of the shuffle model is bounded below by ğœ–0, an observation made
in literature [3, 22]. As can be seen in Figures 2d and 2e, the RDP
obtained by standard approximate DP to RDP conversion in [22,
Remark 1], can be several orders of magnitude loose in comparison
to our analysis.
Approximate DP of the shuffle model: Analyzing RDP of the shuf-
fle model provides a bound on the approximate DP of the shuffle
model from the relation between the RDP and approximate DP as
shown in Lemma 2.1. In Figure 3, we plot several bounds on the
approximate (ğœ–, ğ›¿)-DP of the shuffle model for fixed ğ›¿ = 10âˆ’6. In
Figures 3d and 3b, we do not plot the results given in [22], since
their bounds are quite loose and are far from the plotted range
when ğœ–0 > 1. We can see that our analysis of the RDP of the shuf-
fle model provides a tighter bound on the approximate DP of the
shuffle model in comparison with the bound given in [7] in some
regimes. However, our RDP analysis performs worse than the best
known bound given in [24], when used without composition. This
might be due to the gap between our upper and lower bound on
the RDP of the shuffle model as the lower bound provides better
performance than the bound given in [24] for all values of LDP
parameter ğœ–0. Note that the main use case for converting our RDP
analysis to approximate DP is after composition rather than in the
single-shot conversion illustrated in Figure 3.
Composition of a sequence of shuffle models: We now numerically
evaluate the privacy parameters of the approximate (ğœ–, ğ›¿)-DP for
a composition of ğ‘‡ mechanisms (M1, . . . ,Mğ‘‡), where Mğ‘¡ is a
shuffle mechanism for all ğ‘¡ âˆˆ [ğ‘‡]. In Figure 4, we plot three different
bounds on the overall privacy parameter ğœ– for fixed ğ›¿ = 10âˆ’8 for
a composition of ğ‘‡ identical shuffle models. The first bound on
6The results in [24] are for approximate DP (not for RDP), that is why we did not
compare with them in Figure 2.
Session 7D: Privacy for Distributed Data and Federated Learning CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea 2327(a) RDP as a function of ğœ† for ğœ–0 = 0.1 and
ğ‘› = 104
(b) RDP as a function of ğ‘› for ğœ–0 = 0.1 and
ğœ† = 100
(c) RDP as a function of ğœ–0 for ğ‘› = 104 and
ğœ† = 100
(d) RDP as a function of ğœ† for ğœ–0 = 3 and
ğ‘› = 104
(e) RDP as a function of ğ‘› for ğœ–0 = 3 and
ğœ† = 100
(f) RDP as a function of ğœ–0 for ğ‘› = 106 and
ğœ† = 100
Figure 2: Comparison of several bounds on the RDP of the shuffle model: (i) Our first upper bound (5) in Theorem 3.1; (ii) Our
second upper bound (8) in Theorem 3.1; (iii) Our lower bound proposed in Theorem 3.4; and (iv) The upper bound on the RDP of
the shuffle model given in [22, Remark 1].
(a) Approximate DP as a function of ğ‘› for ğœ–0 = 0.1
and ğ›¿ = 10âˆ’6
(b) Approximate DP as a function of ğœ–0 for ğ‘› = 104
and ğ›¿ = 10âˆ’6
Figure 3: Comparison of sev-
eral bounds on the Approxi-
mate (ğœ–, ğ›¿)-DP of the shuffle
model for ğ›¿ = 10âˆ’6: (i) Approx-
imate DP obtained from our
first upper bound (5) of the
RDP in Theorem 3.1; (ii) Ap-
proximate DP obtained from
our lower bound on the RDP
proposed in Theorem 3.4; (iii)
The empirical upper bound
on the approximate DP given
in [24]; (iv) The theoretical
bound on the approximate
DP given in [22]; and (v) The
generic bound on the approx-
imate DP given in [7].
(c) Approximate DP as a function of ğ‘› for ğœ–0 = 3 and
ğ›¿ = 10âˆ’6
(d) Approximate DP as a function of ğœ–0 for ğ‘› = 105
and ğ›¿ = 10âˆ’6
the overall privacy parameter ğœ– is obtained as a function of ğ›¿ and
the number of iterations ğ‘‡ by optimizing over the RDP order ğœ†
using our upper bound on the RDP of the shuffle model given in
Theorem 3.1. The second bound is obtained by optimizing over the
RDP order ğœ† using the upper bound on the RDP of the shuffle model
given in [22]. The third bound is obtained by first computing the
101102103104105106RDP order Î»10âˆ’610âˆ’510âˆ’410âˆ’310âˆ’210âˆ’1100RDP Îµ(Î»)Îµ0=0.1,n=104RDP (1st Upper bound)RDP (2nd Upper bound)RDP [EFM+19]RDP (Lower bound)104105106Number of clients n10âˆ’610âˆ’510âˆ’4RDP Îµ(Î»)Îµ0=0.1,Î»=100RDP (1st Upper bound)RDP (2nd Upper bound)RDP [EFM+19]RDP (Lower bound)012345LDP parameter Îµ010âˆ’410âˆ’21001021041061081010RDP Îµ(Î»)n=104,Î»=100RDP (1st Upper bound)RDP (2nd Upper bound)RDP [EFM+19]RDP (Lower bound)101102103104105106RDP order Î»10âˆ’21001021041061081010RDP Îµ(Î»)Îµ0=3.0,n=104RDP (1st Upper bound)RDP (2nd Upper bound)RDP [EFM+19]RDP (Lower bound)104105106Number of clients n10âˆ’2100102104106RDP Îµ(Î»)Îµ0=3.0,Î»=100RDP (1st Upper bound)RDP (2nd Upper bound)RDP [EFM+19]RDP (Lower bound)012345LDP parameter Îµ010âˆ’610âˆ’410âˆ’2100102104106108RDP Îµ(Î»)n=106,Î»=100RDP (1st Upper bound)RDP (2nd Upper bound)RDP [EFM+19]RDP (Lower bound)104105106Number of clients n10âˆ’310âˆ’2Approximate DP ÎµÎµ0=0.1,Î´=10âˆ’6via RDP (1st upper bound)via RDP (lower bound)Clones, empirical[FMT20][EFM+19]Blanket, empirical[BBGN'19]0.00.51.01.52.02.53.0LDP parameter Îµ010âˆ’210âˆ’1100Approixmate DP Îµn=104,Î´=10âˆ’6via RDP (1st upper bound)via RDP (lower bound)Clones, empirical[FMT20]Blanket, empirical[BBGN'19]104105106Number of clients n10âˆ’210âˆ’1100101102103104Approximate DP ÎµÎµ0=3.0,Î´=10âˆ’6via RDP (1st upper bound)via RDP (lower bound)Clones, empirical[FMT20][EFM+19]Blanket, empirical[BBGN'19]012345LDP parameter Îµ010âˆ’310âˆ’210âˆ’1100Approixmate DP Îµn=105,Î´=10âˆ’6via RDP (1st upper bound)via RDP (lower bound)Clones, empirical[FMT20]Blanket, empirical[BBGN'19]Session 7D: Privacy for Distributed Data and Federated Learning CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea 2328(a) Approximate DP as a function of ğ‘‡ for
ğœ–0 = 0.5 and ğ‘› = 106
(b) Approximate DP as a function of ğ‘› for
ğœ–0 = 0.5 and ğ‘‡ = 104
(c) Approximate DP as a function of ğœ–0 for
ğ‘› = 105 and ğ‘‡ = 104
(d) Approximate DP as a function of ğ‘‡ for
ğœ–0 = 2 and ğ‘› = 106
(e) Approximate DP as a function of ğ‘› for
ğœ–0 = 2 and ğ‘‡ = 104
(f) Approximate DP as a function of ğœ–0 for
ğ‘› = 106 and ğ‘‡ = 104
Figure 4: Comparison of several bounds on the Approximate (ğœ–, ğ›¿)-DP for composition of a sequence of shuffle models for
ğ›¿ = 10âˆ’8: (i) Approximate DP obtained from our first upper bound (5) on the RDP; (ii) Approximate DP obtained from our lower
bound on the RDP proposed in Theorem 3.4; (iii) Approximate DP obtained from the upper bound on the RDP given in [22];
and (iv) Applying the strong composition theorem [33] after getting the approximate DP of the shuffle model given in [24].
privacy parameters ( Ëœğœ–, Ëœğ›¿) of the shuffle model given in [24]. Then,
we use the strong composition theorem given in [33] to obtain the
overall privacy loss ğœ–. We observe that there is a significant saving
in the overall privacy parameter ğœ–-DP using our bound on RDP
in comparison with using the bound on DP [24] with the strong
composition theorem [33]. For example, we save a factor of 8Ã— in
computing the overall privacy parameter ğœ– for number of iterations
ğ‘‡ = 105, LDP parameter ğœ–0 = 0.5, and number of clients ğ‘› = 106. We
observe that the bound given in [24] with the strong composition
theorem [33] behaves better for small number of iterations ğ‘‡ < 10
and large LDP parameter ğœ–0 = 2. However, the typical number
of iterations ğ‘‡ in the standard SGD algorithm is usually larger.
Therefore, this demonstrates the significance of our RDP analysis
for composition in the regimes of interest.
Privacy amplification by shuffling and Poisson sub-sampling: In
the Differentially Private Stochastic Gradient Descent (DP-SGD),
shuffling and sampling the dataset at each iteration are important
tools to provide a strong privacy guarantee [20, 28]. In these frame-
works, the further advantage of sampling with shuffling7 can be
analyzed by standard combination of approximate DP with Poisson
subsampling [35]. The resulting approximate DP along with the
strong composition theorem given in [33] gives the overall privacy
loss ğœ–. An alternate path we use is to combine our RDP analysis
7In this framework we assume that the sampling and shuffling is done by a secure
mechanism which is separated from the server, i.e., the server does not know which
clients are participating.
with sampling of RDP mechanisms using [40, 43]. This enables us to
get an RDP guarantee with sampling, which we can then compose
using properties of RDP. We can use the conversion from RDP to
approximate DP to obtain a bound on the overall privacy loss of
multiple iterations. In Figure 5, we compare our results of amplify-
ing the RDP of the shuffle model by Poisson sub-sampling to the
strong composition [33] after getting the approximate DP of the
shuffle model given in [24] with Poisson sub-sampling given in [35].
We observe that we save a factor of 11Ã— by using our RDP bound for
ğ‘› = 106 and ğ›¾ = 0.001. However, we can see that the gap between
our (lower/upper) bounds and the strong composition decreases
when ğ‘› = 107. This could be due to the simplistic combination of
our analysis with the RDP subsampling of [43].
5 PROOF OF THE REDUCTION TO THE
SPECIAL CASE
In this section, we will prove Theorem 3.6 by reducing the problem
of computing RDP for the arbitrary pairs of neighboring datasets to
the problem of computing RDP for the neighboring datasets with
the special structure.
Recall that the LDP mechanism R : X â†’ Y has a discrete
range Y = [ğµ] for some ğµ âˆˆ N. Let ğ’‘ğ‘– := (ğ‘ğ‘–1, . . . , ğ‘ğ‘–ğµ) and ğ’‘â€²
ğ‘› :=
(ğ‘â€²
ğ‘›1, . . . , ğ‘â€²
ğ‘›ğµ) denote the probability distributions over Y when the
input to R is ğ‘‘ğ‘– and ğ‘‘â€²
ğ‘›, respectively, where ğ‘ğ‘– ğ‘— = Pr[R(ğ‘‘ğ‘–) = ğ‘—] and
ğ‘› ğ‘— = Pr[R(ğ‘‘â€²
ğ‘â€²
ğ‘›) = ğ‘—] for all ğ‘— âˆˆ [ğµ] and ğ‘– âˆˆ [ğ‘›]. Let P = {ğ’‘ğ‘– : ğ‘– âˆˆ
ğ‘›}. For ğ‘– âˆˆ [ğ‘› âˆ’ 1], let Pâˆ’ğ‘– =
[ğ‘›]} and Pâ€² = {ğ’‘ğ‘– : ğ‘– âˆˆ [ğ‘› âˆ’ 1]}{ğ’‘â€²
101102103104105Number of iterations T10âˆ’210âˆ’1100101Approximate DP ÎµÎµ0=0.5,n=106,Î´=10âˆ’8via RDP (1st upper bound)via RDP (lower bound)via RDP [EFM+19]Clones[FMT20]+strong composition[KOV15]104105106Number of clients n100101Approximate DP ÎµT=10000,Îµ0=0.5,Î´=10âˆ’8via RDP (1st upper bound)via RDP (lower bound)via RDP [EFM+19]Clones[FMT20]+strong composition[KOV15]0.00.51.01.52.02.53.0LDP parameter Îµ010âˆ’1100101102103Approximate DP ÎµT=10000,n=105,Î´=10âˆ’8via RDP (1st upper bound)via RDP (lower bound)via RDP [EFM+19]Clones[FMT20]+strong composition[KOV15]101102103104105Number of iterations T10âˆ’210âˆ’1100101102103Approximate DP ÎµÎµ0=2.0,n=106,Î´=10âˆ’8via RDP (1st upper bound)via RDP (lower bound)via RDP [EFM+19]Clones[FMT20]+strong composition[KOV15]104105106Number of clients n100101102103104105Approximate DP ÎµT=10000,Îµ0=2.0,Î´=10âˆ’8via RDP (1st upper bound)via RDP (lower bound)via RDP [EFM+19]Clones[FMT20]+strong composition[KOV15]0.00.51.01.52.02.53.0LDP parameter Îµ010âˆ’1100101102Approximate DP ÎµT=10000,n=106,Î´=10âˆ’8via RDP (1st upper bound)via RDP (lower bound)via RDP [EFM+19]Clones[FMT20]+strong composition[KOV15]Session 7D: Privacy for Distributed Data and Federated Learning CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea 2329(a) Approximate DP as a function of ğ‘‡
for ğœ–0 = 3, ğ›¾ = 0.001 and ğ‘› = 106.
(b) Approximate DP as a function of ğ‘›
for ğœ–0 = 3, ğ›¾ = 0.001 and ğ‘› = 107.
Figure 5: Comparison of several bounds on the Ap-
proximate (ğœ–, ğ›¿)-DP for composition a sequence
of shuffle models with Poisson sub-sampling for
ğ›¿ = 10âˆ’8 and ğ›¾ = 0.001: (i) Approximate DP ob-
tained from our first upper bound (5) on the RDP
in Theorem 3.1 with RDP amplification by Poisson
sub-sampling [43]; (ii) Approximate DP obtained
from our lower bound on the RDP proposed in
Theorem 3.4 with RDP amplification by Poisson
sub-sampling [43]; and (iii) Applying the strong
composition theorem [33] after getting the ap-
proximate DP of the shuffle model given in [24]
with Poisson sub-sampling [35].
P\{ğ’‘ğ‘–}, Pâ€²âˆ’ğ‘– = Pâ€²\{ğ’‘ğ‘–}, and also Pâˆ’ğ‘› = P\{ğ’‘ğ‘›}, Pâ€²âˆ’ğ‘› = Pâ€²\{ğ’‘â€²
ğ‘›}.
Here, P, Pâ€² correspond to the datasets D = {ğ‘‘1, . . . , ğ‘‘ğ‘›}, Dâ€² =
{ğ‘‘1, . . . , ğ‘‘ğ‘›âˆ’1, ğ‘‘â€²
ğ‘›}, respectively, and for any ğ‘– âˆˆ [ğ‘›], Pâˆ’ğ‘– and Pâ€²âˆ’ğ‘–
correspond to the datasets Dâˆ’ğ‘– = {ğ‘‘1, . . . , ğ‘‘ğ‘–âˆ’1, ğ‘‘ğ‘–+1, . . . , ğ‘‘ğ‘›} and
Dâ€²âˆ’ğ‘– = {ğ‘‘1, . . . , ğ‘‘ğ‘–âˆ’1, ğ‘‘ğ‘–+1, . . . , ğ‘‘ğ‘›âˆ’1, ğ‘‘â€²
ğ‘›}, respectively.
For any collection P = {ğ’‘1, . . . , ğ’‘ğ‘›} of ğ‘› distributions, we define
ğ¹(P) to be the distribution over Ağ‘›
ğµ (which is the set of histograms
on ğµ bins with ğ‘› elements as defined in (4)) that is induced when
every client ğ‘– (independent to the other clients) samples an element
from [ğµ] accordingly to the probability distribution ğ’‘ğ‘–. Formally,
for any ğ’‰ âˆˆ Ağ‘›