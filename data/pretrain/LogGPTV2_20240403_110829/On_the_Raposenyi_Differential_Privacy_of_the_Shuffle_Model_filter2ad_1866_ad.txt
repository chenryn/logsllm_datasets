### 3.3.1 Proof Sketch of Theorem 3.6

Let \( \mathcal{P}_i \) denote the distribution of the \(\epsilon_0\)-LDP mechanism \( R \) when the input data point is \( d_i \), and let \( \mathcal{P}'_n \) denote the distribution of \( R \) when the input data point is \( d'_n \). The main idea of the proof is based on the observation that each distribution \( \mathcal{P}_i \) can be written as a mixture distribution:
\[ \mathcal{P}_i = \frac{e^{\epsilon_0}}{e^{\epsilon_0} + 1} \mathcal{P}'_n + \frac{1}{e^{\epsilon_0} + 1} \tilde{\mathcal{P}}_i, \]
where \( \tilde{\mathcal{P}}_i \) is another distribution associated with \( \mathcal{P}_i \).

Instead of client \( i \in [n-1] \) mapping its data point \( d_i \) according to \( \mathcal{P}_i \), we can view it as the client \( i \) maps \( d_i \) according to \( \mathcal{P}'_n \) with probability \( \frac{1}{e^{\epsilon_0}} \) and according to \( \tilde{\mathcal{P}}_i \) with probability \( 1 - \frac{1}{e^{\epsilon_0}} \). Thus, the number of clients that sample from the distribution \( \mathcal{P}'_n \) follows a binomial distribution \( \text{Bin}(n-1, q) \) with parameter \( q = \frac{1}{e^{\epsilon_0}} \).

This allows us to express the distribution of \( M \) when clients map their data points according to \( \mathcal{P}_1, \ldots, \mathcal{P}_n, \mathcal{P}'_n \) as a convex combination of the distribution of \( M \) when clients map their data points according to \( \tilde{\mathcal{P}}_1, \ldots, \tilde{\mathcal{P}}_{n-1}, \mathcal{P}_n, \mathcal{P}'_n \); see Lemma 5.1.

Using a joint convexity argument (see Lemma 5.2), we can write the Rényi divergence between the original pair of distributions of \( M \) in terms of the same convex combination of the Rényi divergence between the resulting pairs of distributions of \( M \) as in Lemma 5.1. Using a monotonicity argument (see Lemma 5.3), we can remove the effect of clients that do not sample from the distribution \( \mathcal{P}'_n \) without decreasing the Rényi divergence.

By this chain of arguments, we have reduced the problem to the one involving the computation of Rényi divergence only for the special form of neighboring datasets, which proves Theorem 3.6. Details can be found in Section 5.

### 3.3.2 Proof Sketch of Theorem 3.7

Consider any pair of special neighboring datasets \( (D_m, D'_m) \in \mathcal{D}_m \) for any \( m \in \mathbb{N} \). Using the polynomial expansion, we get:
\[ X(h) = m \left( \left( \frac{M(D'_m)(h)}{M(D_m)(h)} \right)^\lambda - 1 \right). \]

Let \( X: A_m \to \mathbb{R} \) be a random variable (r.v.) associated with the distribution \( M(D_m) \), and for every \( h \in A_m \):
\[ X(h) = \left( \frac{M(D'_m)(h)}{M(D_m)(h)} \right)^\lambda - 1. \]

We show that \( X \) is a sub-Gaussian r.v. with zero mean and bounded variance. Using the sub-Gaussianity of \( X \), we bound its higher moments (see Lemma 6.1). Substituting these bounds into the expression above proves Theorem 3.7. Details can be found in Section 6.

### 4. NUMERICAL RESULTS

In this section, we present numerical experiments to evaluate the performance of our bounds on the RDP of the shuffle model and its usage for getting approximate DP and composition results.

#### RDP of the Shuffle Model

In Figure 2, we plot several bounds on the RDP of the shuffle model in different regimes. We compare the first upper bound on the RDP given in Theorem 3.1, the second upper bound on the RDP given in Theorem 3.3, the lower bound on the RDP given in Theorem 3.4, and the upper bound on the RDP given in [22, Remark 1].

It is clear that our first upper bound (5) provides a tighter bound on the RDP compared to the second bound (8) and the upper bound given in [22]. Furthermore, the first upper bound is close to the lower bound for small values of the LDP parameter \( \epsilon_0 \) and for high orders \( \lambda \). Additionally, the gap between our proposed bound in Theorem 3.1 and the bound given in [22] increases as the LDP parameter \( \epsilon_0 \) increases. We also observe that the curves of the lower and upper bounds on the RDP of the shuffle model saturate close to \( \epsilon_0 \) when the order \( \lambda \) approaches infinity, indicating that the pure DP of the shuffle model is bounded below by \( \epsilon_0 \).

As shown in Figures 2d and 2e, the RDP obtained by standard approximate DP to RDP conversion in [22, Remark 1] can be several orders of magnitude looser compared to our analysis.

#### Approximate DP of the Shuffle Model

Analyzing the RDP of the shuffle model provides a bound on the approximate DP of the shuffle model using the relation between RDP and approximate DP as shown in Lemma 2.1. In Figure 3, we plot several bounds on the approximate (ε, δ)-DP of the shuffle model for fixed \( \delta = 10^{-6} \).

In Figures 3d and 3b, we do not plot the results given in [22] because their bounds are quite loose and far from the plotted range when \( \epsilon_0 > 1 \). Our analysis of the RDP of the shuffle model provides a tighter bound on the approximate DP of the shuffle model compared to the bound given in [7] in some regimes. However, our RDP analysis performs worse than the best known bound given in [24] when used without composition. This might be due to the gap between our upper and lower bounds on the RDP of the shuffle model, as the lower bound provides better performance than the bound given in [24] for all values of the LDP parameter \( \epsilon_0 \). Note that the main use case for converting our RDP analysis to approximate DP is after composition rather than in the single-shot conversion illustrated in Figure 3.

#### Composition of a Sequence of Shuffle Models

We now numerically evaluate the privacy parameters of the approximate (ε, δ)-DP for a composition of \( T \) mechanisms \( (M_1, \ldots, M_T) \), where \( M_t \) is a shuffle mechanism for all \( t \in [T] \). In Figure 4, we plot three different bounds on the overall privacy parameter ε for fixed \( \delta = 10^{-8} \) for a composition of \( T \) identical shuffle models.

The first bound on the overall privacy parameter ε is obtained as a function of δ and the number of iterations \( T \) by optimizing over the RDP order \( \lambda \) using our upper bound on the RDP of the shuffle model given in Theorem 3.1. The second bound is obtained by optimizing over the RDP order \( \lambda \) using the upper bound on the RDP of the shuffle model given in [22]. The third bound is obtained by first computing the privacy parameters \( (\tilde{\epsilon}, \tilde{\delta}) \) of the shuffle model given in [24], then using the strong composition theorem given in [33] to obtain the overall privacy loss ε.

We observe a significant saving in the overall privacy parameter ε-DP using our bound on RDP compared to using the bound on DP [24] with the strong composition theorem [33]. For example, we save a factor of 8× in computing the overall privacy parameter ε for \( T = 10^5 \), \( \epsilon_0 = 0.5 \), and \( n = 10^6 \). We observe that the bound given in [24] with the strong composition theorem [33] behaves better for small numbers of iterations \( T < 10 \) and large LDP parameter \( \epsilon_0 = 2 \). However, the typical number of iterations \( T \) in the standard SGD algorithm is usually larger. Therefore, this demonstrates the significance of our RDP analysis for composition in the regimes of interest.

#### Privacy Amplification by Shuffling and Poisson Sub-sampling

In Differentially Private Stochastic Gradient Descent (DP-SGD), shuffling and sampling the dataset at each iteration are important tools to provide a strong privacy guarantee [20, 28]. In these frameworks, the further advantage of sampling with shuffling can be analyzed by standard combination of approximate DP with Poisson subsampling [35]. The resulting approximate DP along with the strong composition theorem given in [33] gives the overall privacy loss ε.

An alternate path we use is to combine our RDP analysis with sampling of RDP mechanisms using [40, 43]. This enables us to get an RDP guarantee with sampling, which we can then compose using properties of RDP. We can use the conversion from RDP to approximate DP to obtain a bound on the overall privacy loss of multiple iterations.

In Figure 5, we compare our results of amplifying the RDP of the shuffle model by Poisson sub-sampling to the strong composition [33] after getting the approximate DP of the shuffle model given in [24] with Poisson sub-sampling given in [35]. We observe that we save a factor of 11× by using our RDP bound for \( n = 10^6 \) and \( \gamma = 0.001 \). However, the gap between our (lower/upper) bounds and the strong composition decreases when \( n = 10^7 \). This could be due to the simplistic combination of our analysis with the RDP subsampling of [43].

### 5. PROOF OF THE REDUCTION TO THE SPECIAL CASE

In this section, we will prove Theorem 3.6 by reducing the problem of computing RDP for arbitrary pairs of neighboring datasets to the problem of computing RDP for neighboring datasets with a special structure.

Recall that the LDP mechanism \( R: X \to Y \) has a discrete range \( Y = [B] \) for some \( B \in \mathbb{N} \). Let \( \mathcal{P}_i := (p_{i1}, \ldots, p_{iB}) \) and \( \mathcal{P}'_n := (p'_{n1}, \ldots, p'_{nB}) \) denote the probability distributions over \( Y \) when the input to \( R \) is \( d_i \) and \( d'_n \), respectively, where \( p_{ij} = \Pr[R(d_i) = j] \) and \( p'_{nj} = \Pr[R(d'_n) = j] \) for all \( j \in [B] \) and \( i \in [n] \).

Let \( P = \{ \mathcal{P}_i : i \in [n] \} \) and \( P' = \{ \mathcal{P}_i : i \in [n-1] \} \cup \{ \mathcal{P}'_n \} \). For \( i \in [n-1] \), let \( P_{-i} = P \setminus \{ \mathcal{P}_i \} \) and \( P'_{-i} = P' \setminus \{ \mathcal{P}_i \} \). Also, let \( P_{-n} = P \setminus \{ \mathcal{P}_n \} \) and \( P'_{-n} = P' \setminus \{ \mathcal{P}'_n \} \).

Here, \( P \) and \( P' \) correspond to the datasets \( D = \{ d_1, \ldots, d_n \} \) and \( D' = \{ d_1, \ldots, d_{n-1}, d'_n \} \), respectively. For any \( i \in [n] \), \( P_{-i} \) and \( P'_{-i} \) correspond to the datasets \( D_{-i} = \{ d_1, \ldots, d_{i-1}, d_{i+1}, \ldots, d_n \} \) and \( D'_{-i} = \{ d_1, \ldots, d_{i-1}, d_{i+1}, \ldots, d_{n-1}, d'_n \} \), respectively.

For any collection \( P = \{ \mathcal{P}_1, \ldots, \mathcal{P}_n \} \) of \( n \) distributions, we define \( F(P) \) to be the distribution over \( A^n_B \) (which is the set of histograms on \( B \) bins with \( n \) elements as defined in (4)) that is induced when every client \( i \) (independent of the other clients) samples an element from \( [B] \) according to the probability distribution \( \mathcal{P}_i \). Formally, for any \( h \in A^n_B \):
\[ F(P)(h) = \prod_{i=1}^n \mathcal{P}_i(h_i). \]

By this reduction, we can focus on the special form of neighboring datasets, which simplifies the computation of the Rényi divergence. Details can be found in Section 5.