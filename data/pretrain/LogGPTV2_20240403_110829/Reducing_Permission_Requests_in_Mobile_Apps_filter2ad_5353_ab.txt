The key input data used to compute our embedding is textual,
notably the apps’ description, as well as the app name and category.
If needed, this text is translated to English, to handle apps in any
of the 100+ languages supported by Google Translate. In addition
to text, the embedding is also computed based on UBC data; this
is done by incorporating the top 10 UBC-related apps for each
app, unless no such UBC-related peers are available. We note that
the UBC data we work with is highly aggregated and contains no
261
personal information. The data is in the form of app pairs with an
accompanying probability on the likelihood that the second app
would be clicked if the first is already shown.
The training data contains several rows of text for each app
and are created as follows. Each sentence from the app description
constitutes one row. The app title and category each form a separate
row. Each app is given a unique identifier token (a unique word) that
is prepended to all rows associated with that app. To incorporate
the UBC ranking data, we include a single row with the identifier
tokens of the top 10 apps considered most related by the UBC
algorithm. Each English word is also a unique token. Thus, the
complete set of training-data tokens includes both unique tokens
representing apps, as well as vocabulary tokens for the words in
the app descriptions, titles, and categories.
The training data is used to train a skip-gram language model,
which uses a log-linear classifier with softmax to estimate the proba-
bility distribution of tokens that come within a certain range before
and after the current token [20]. Thus, the input of the model is a
token and the output is the probability distribution of the context
in the form of surrounding words. Each token is represented by an
n-dimensional vector and all these vectors comprise the projection
matrix with v rows and n columns, where v is the vocabulary size
(number of all tokens). For an input word w, the probability of a
word c being in the context (surrounding words) decreases expo-
nentially fast with cosine distance of w and c. A univariate model
defines the probability of several context words as the product of
the individual probabilities of these words. The training optimizes
the vectors so that the product of all the context probabilities is
maximized for the training data. Thus, words that are often located
near each other end up with vectors that are close to each other
according to the cosine distance. Similarly, vectors of closely related
apps end up close to each other. For example, chess and checkers
apps will be close to each other in the vector space because words
such as board, game, and piece co-occur near each other and are
also close to both app tokens in the vector space.
We use 200-dimensional vectors to represent all Android app
tokens and also other tokens in the training data (words from
descriptions, titles, and categories). The vocabulary size v (number
of tokens) is in the millions. Our models are trained using stochastic
gradient descent. To make the training of an embedding model for
millions of tokens feasible, we use negative sampling as described
in [20].
To measure the similarity between two apps, we use the cosine
distance of their app vectors from the word2vec algorithm. The
peers of an app are its nearest neighbors based on this distance. For
example, with our model, the closest peer of Gmail is another email
app with cosine distance of 0.41. Weakly related Android apps have
greater cosine distances from Gmail: such as 0.68 for a messenger
app, 0.80 for an internet browser, and 0.85 for a chess app. Our peer
identification algorithm works well across all app popularity levels
as the distribution of peer similarity scores is very similar.
We believe that developers have little motivation to circumvent
our system. Because our warning signal is shown only to the devel-
oper, there is no public effect and thus no impact to their reputation.
In theory, developers could attempt to influence the warning by
creating a large number of apps, each of which asks for all the
permissions they want, and has similar text descriptions. That said,
Evaluating App Similarity within our Peer Groups. We
compare our mechanism against two approaches: the UBC algo-
rithm alone and a Latent Dirichlet Allocation (LDA) using 50 topics
on normalized app descriptions [14]. We conducted two rounds
of evaluation. First, we had 17 human evaluators participate in a
survey. We split apps into 5 buckets based on the number of in-
stalls, and then select several apps from each bucket. This ensures
that our analysis provides feedback across app popularity levels.
For each selected app, we chose the top 3 peers from each of the
3 approaches (if available), removing duplicates. These at most 9
peers for each app are then randomly shuffled and presented to
raters without any information about the source model or the orig-
inal rank of each peer. Raters are shown a pair of apps and asked
how similar they think the apps are, on a scale from 1 (not related)
to 4 (nearly identical). We normalize the feedback values to [0, 1]
with higher values indicating higher similarity. These evaluators
rated over 400 app pairs. Each pair of apps was rated by at least
3 evaluators. Our evaluators were all Google employees recruited
via internal advertisement, which resulted in employees from nu-
merous groups. They were told the purpose of the evaluation - to
compare the quality of app similarity algorithms - but were not told
how they work and had no way to know which algorithm produced
which pair of apps presented. The evaluators were told they could
use any publicly information available on Google Play to inform
their decision about the similarity of the presented app pairs.
Note that the app pairs should all have high similarity since we
pair each app with one that was a top-3 match from one of the
models. The average app similarity for app pairs proposed by our
word2vec-based algorithm was 0.81. Our algorithm significantly
outperformed the LDA method that received an average app simi-
larity of 0.51. The UBC method received an average similarity of
0.79 which is comparable to our approach, however, as explained
earlier, it has much lower coverage ; in particular, it didn’t provide
any suggestions for approximately one in ten of the apps in our
test set (because they were either new or unpopular).
To further compare our proposed model with UBC method, we
conducted a second round of evaluation. For each app selected we
picked the top 3 peers plus 2 other peers randomly selected from
the closest 30 peers (if available), from each method, resulting in at
most 10 app pairs. Similar to the earlier setup, these app pairs are
shuffled and presented to the evaluators, who are asked to rate how
similar the two apps are on a scale from 1 to 4. To increase the app
coverage, each app pair is only rated by 1 evaluator in this round.
Our evaluators rated over 1450 app pairs. The average similarity
based on the normalized feedback values is 0.82 for our proposed
model, compared to 0.66 for the UBC model. This further re-enforces
that our deep-learning algorithm outperforms prior approaches.
Beyond using a powerful approach such as deep-learning, it is
intuitive that our approach works best because LDA relies solely on
text descriptions and the UBC approach relies solely on co-clicks,
whereas our approach uses both of these input signals.
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
S. Peddinti et al.
developers would not know a priori how many such apps are re-
quired to affect our algorithm.
3 EFFECTIVENESS OF NUDGES
Using our algorithm above to detect similar apps, we implemented
the privacy nudge described in Section 2.1 for Android Platform
related permissions and released it in the Play Pre-Launch Report,
along with a blog post, in August 2017 [22]. We imposed a cutoff on
the peer similarity scores to produce between 150-200 high quality
peers per app. The distribution of peers per app is shown in Figure 1,
and as is seen, a very small fraction of apps have less than 190 peers.
Figure 1: Number of peers per app
Figure 2: Percentage of apps that could receive privacy warn-
ings at different thresholds (*only apps with peers and cer-
tain minimum installs are considered)
To guide the choice of the threshold (‘X’% of peers not using a
permission) that determines when to raise the privacy warning, we
estimated the percentage of apps with peers that could receive a
warning at different thresholds between 1% to 5%. Note that not
all apps are included in the calculation of peers sets; for instance,
we require an app to have a minimum number of installs to be
included. The results are outlined in Figure 2. We initially elected
to use a highly conservative threshold of ‘less than 1%’ of peer apps
using a permission to raise the privacy warning. We opted for this
conservative threshold for two reasons. First this ensures that each
warning signal has high fidelity, i.e. developers are nudged only
when we are quite confident that their behavior is significantly
out of the ordinary. Second, we elected to start conservatively to
262
1501601701801902000255075100% of Apps# of Peers01234012345Warning Threshold (X% of peers)% of Apps that could receive warnings*Reducing Permission Requests in Mobile Apps
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
Figure 3: Permissions warned (20 permissions with more than 100 warnings each are shown)
explore whether developers would respond to such a warning (we
wanted to avoid de-sensitizing them). Based on positive developer
reaction, later in September 2018, we relaxed the threshold to 3%.
Since August 2017, every new test APK submission to Google Play,
among the millions participating in our testing framework, has been
evaluated by our system. We evaluate our threshold’s effectiveness
on an ongoing basis and may consider raising the threshold further
in the future.
Overall Response. Between August 2017 and Feb 2019 (in-
clusive), privacy warnings were raised for 28,446 permissions re-
quested by 19,215 apps, which came from 15,645 developers. Among
the warned apps, we observed that 11,289 apps (59%) apps, devel-
oped by 9313 developers, removed at least one permission after
seeing our warnings. If we look at individual permissions, of the
28,446 warned permissions, only 5,725 (20%) were removed. Interest-
ingly we observed that developers of warned apps removed many
permissions they had not been explicitly warned about. In particu-
lar, our 19,215 warned apps removed 45,866 unwarned permissions.
This hints that our warning may have encouraged developers to re-
visit their decisions about permissions beyond the one(s) they were
warned about. The total number of permissions removed by these
apps was 51,591 including both warned and unwarned permissions.
Response by Permission Type. Figure 3 shows a breakdown
of the number of warnings surfaced for 20 example Android Plat-
form permissions. Some of these permissions were deprecated in
recent Android SDK versions, but we include them as some apps
can be targeting older Android SDKs. These permissions had at
least a 100 warnings each. While overall 20% of individual per-
mission warnings were adhered to, there was variation across the
permissions: 7% of apps receiving warnings for Camera removed
the permission, where as 65% of them receiving a warning for Get
Accounts removed the permission. Because our warnings are soft,
developers can elect to keep a permission—perhaps rightly so, if
their needs do not fully correspond to those of their peers.
In addition to these warned permission removals, app developers
have removed many more unwarned permissions. These unwarned
removals are much more prominent across popular permissions,
such as Location and Storage, which naturally generate fewer warn-
ings. For instance, Read External Storage was only warned 29 times
but has seen 1374 removals across the 19K apps. It could be that
developers realize they don’t always need these, and a warning
for any permission could serve as a reminder. Many permissions
beyond the 20 presented here, including those defined by other
apps or services to control access to their resources, were removed
by developers after they saw our privacy warnings. Note that when
developers remove a permission they were not warned about, it
does not remove our warning; hence the behavior of removing
unwarned permissions should not be interpreted as a way to try to
remove our warning. All these (non) platform permission removals
sum up to the 51K removals reported earlier.
Figure 4: Categories and their permission removals
Response by App Category and Popularity. We now exam-
ine whether these removals are occurring only in a niche portion
of Google Play or more broadly. We first look at Google Play cate-
gories of the apps who remove permissions. We observe removals
in all categories. For example, as shown in Figure 4, within the top
20 categories, we observe between 1000-5000 permission removals
per category. Hence developers across a broad set of app categories
are responsive to our nudges. Next we look at the popularity of
the 11.3K responsive apps, shown in Table 2. We notice that apps
from all popularity levels have received warnings and removed
permissions. These observations are encouraging, as they indicate
that permissions are being removed broadly from many different
kinds of apps across the popularity spectrum.
Estimating Impact. Ideally we’d like to assess the impact of
these permission removals on the number of users affected. We can-
not directly measure this. Our data only includes app descriptions,
metadata and install counts; we have no user or device level data,
263
0500100015002000READ_PROFILEMANAGE_ACCOUNTSREAD_CALENDARWRITE_CALENDARREAD_LOGSWRITE_CONTACTSMOUNT_UNMOUNT_FILESYSTEMSREAD_CONTACTSACCESS_LOCATION_EXTRA_COMMANDSRECORD_VIDEORECORD_AUDIOGET_TASKSCALL_PHONEACCESS_GPSCAMERAPROCESS_OUTGOING_CALLSGET_ACCOUNTSMODIFY_PHONE_STATEACCESS_FINE_LOCATIONACCESS_COARSE_LOCATIONAndroid Platform Permissions (android.permission.*)Number of Warnings010002000300040005000GamesToolsFinanceBusinessEntertainmentLifestyleShoppingSocialEducationProductivityHealth & FitnessPersonalizationCommunicationTravel & LocalMaps & NavigationMusic & AudioNews & MagazinesSportsVideo PlayersBooks & ReferencePlay Store CategoryNumber of RemovalsIMC ’19, October 21–23, 2019, Amsterdam, Netherlands
S. Peddinti et al.
Installs per app Number of apps that
removed permissions
3887
2555
893
2326
912
278
371
67
< 10K
10K–50K
50K–100K
100K–1M
1M–5M
5M–10M
10M–100M
100M+
Table 2: Popularity of responsive warned apps
thus we cannot know, for example, how many of these responsive
apps may be installed on the same device. However, we can approx-
imate the impact via a lower bound on the number of installs as
follows. Table 2 shows that there are 67 warned apps with more
than 100 million installs each, thus a permission removal by each
of these apps improves privacy for at least 100 million devices. This
is clearly an underestimation as there are a few hundred other apps
each affecting 10s of millions of devices. We can also assess the
cumulative impact by looking at the number of installs of an app.
For this estimate, we assume that all users update to the latest app
version. For each app in this table, we count its number of installs;
summing these counts we find that the 51K permission removals,
resulting from our warnings, impacted more that 55 billion app
installs in aggregate.
Supplementary Effect. Many developers publish more than
one app, and so we investigate if showing a warning across one
of the apps would induce changes across other unwarned apps
published by the same developer. Our analysis showed that of
the 15,645 developers who saw our permission warnings, 3758
developers revisited permissions requested by their unwarned apps.
These developers removed an additional 60,993 permissions from
18,997 apps they published. These results indicate that our privacy
warning, though being shown conservatively due to the chosen
thresholds, may be influencing developer behavior more widely
than the scope of our warnings.
4 DO OUR WARNINGS CHANGE DEVELOPER
BEHAVIOR?
There can be many reasons why a developer may remove a per-
mission (see Section 1). To try to understand if developers remove