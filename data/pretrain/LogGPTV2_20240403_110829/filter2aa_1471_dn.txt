(See Chapter 5 of Part 1 for more information on mapped files.) Figure 11-22
is a simplified diagram that shows the components involved in these file
system operations and the ways in which they interact. As you can see, an
FSD can be invoked through several paths:
■    From a user or system thread performing explicit file I/O
■    From the memory manager’s modified and mapped page writers
■    Indirectly from the cache manager’s lazy writer
■    Indirectly from the cache manager’s read-ahead thread
■    From the memory manager’s page fault handler
Figure 11-22 Components involved in file system I/O.
The following sections describe the circumstances surrounding each of
these scenarios and the steps FSDs typically take in response to each one.
You’ll see how much FSDs rely on the memory manager and the cache
manager.
Explicit file I/O
The most obvious way an application accesses files is by calling Windows
I/O functions such as CreateFile, ReadFile, and WriteFile. An application
opens a file with CreateFile and then reads, writes, or deletes the file by
passing the handle returned from CreateFile to other Windows functions. The
CreateFile function, which is implemented in the Kernel32.dll Windows
client-side DLL, invokes the native function NtCreateFile, forming a
complete root-relative path name for the path that the application passed to it
(processing “.” and “..” symbols in the path name) and prefixing the path
with “\??” (for example, \??\C:\Daryl\Todo.txt).
The NtCreateFile system service uses ObOpenObjectByName to open the
file, which parses the name starting with the object manager root directory
and the first component of the path name (“??”). Chapter 8, “System
mechanisms”, includes a thorough description of object manager name
resolution and its use of process device maps, but we’ll review the steps it
follows here with a focus on volume drive letter lookup.
The first step the object manager takes is to translate \?? to the process’s
per-session namespace directory that the DosDevicesDirectory field of the
device map structure in the process object references (which was propagated
from the first process in the logon session by using the logon session
references field in the logon session’s token). Only volume names for
network shares and drive letters mapped by the Subst.exe utility are typically
stored in the per-session directory, so on those systems when a name (C: in
this example) is not present in the per-session directory, the object manager
restarts its search in the directory referenced by the
GlobalDosDevicesDirectory field of the device map associated with the per-
session directory. The GlobalDosDevicesDirectory field always points at the
\GLOBAL?? directory, which is where Windows stores volume drive letters
for local volumes. (See the section “Session namespace” in Chapter 8 for
more information.) Processes can also have their own device map, which is
an important characteristic during impersonation over protocols such as RPC.
The symbolic link for a volume drive letter points to a volume device
object under \Device, so when the object manager encounters the volume
object, the object manager hands the rest of the path name to the parse
function that the I/O manager has registered for device objects,
IopParseDevice. (In volumes on dynamic disks, a symbolic link points to an
intermediary symbolic link, which points to a volume device object.) Figure
11-23 shows how volume objects are accessed through the object manager
namespace. The figure shows how the \GLOBAL??\C: symbolic link points
to the \Device\HarddiskVolume6 volume device object.
Figure 11-23 Drive-letter name resolution.
After locking the caller’s security context and obtaining security
information from the caller’s token, IopParseDevice creates an I/O request
packet (IRP) of type IRP_MJ_CREATE, creates a file object that stores the
name of the file being opened, follows the VPB of the volume device object
to find the volume’s mounted file system device object, and uses
IoCallDriver to pass the IRP to the file system driver that owns the file
system device object.
When an FSD receives an IRP_MJ_CREATE IRP, it looks up the specified
file, performs security validation, and if the file exists and the user has
permission to access the file in the way requested, returns a success status
code. The object manager creates a handle for the file object in the process’s
handle table, and the handle propagates back through the calling chain,
finally reaching the application as a return parameter from CreateFile. If the
file system fails the create operation, the I/O manager deletes the file object it
created for the file.
We’ve skipped over the details of how the FSD locates the file being
opened on the volume, but a ReadFile function call operation shares many of
the FSD’s interactions with the cache manager and storage driver. Both
ReadFile and CreateFile are system calls that map to I/O manager functions,
but the NtReadFile system service doesn’t need to perform a name lookup; it
calls on the object manager to translate the handle passed from ReadFile into
a file object pointer. If the handle indicates that the caller obtained
permission to read the file when the file was opened, NtReadFile proceeds to
create an IRP of type IRP_MJ_READ and sends it to the FSD for the volume
on which the file resides. NtReadFile obtains the FSD’s device object, which
is stored in the file object, and calls IoCallDriver, and the I/O manager
locates the FSD from the device object and gives the IRP to the FSD.
If the file being read can be cached (that is, the
FILE_FLAG_NO_BUFFERING flag wasn’t passed to CreateFile when the
file was opened), the FSD checks to see whether caching has already been
initiated for the file object. The PrivateCacheMap field in a file object points
to a private cache map data structure (which we described in the previous
section) if caching is initiated for a file object. If the FSD hasn’t initialized
caching for the file object (which it does the first time a file object is read
from or written to), the PrivateCacheMap field will be null. The FSD calls
the cache manager’s CcInitializeCacheMap function to initialize caching,
which involves the cache manager creating a private cache map and, if
another file object referring to the same file hasn’t initiated caching, a shared
cache map and a section object.
After it has verified that caching is enabled for the file, the FSD copies the
requested file data from the cache manager’s virtual memory to the buffer
that the thread passed to the ReadFile function. The file system performs the
copy within a try/except block so that it catches any faults that are the result
of an invalid application buffer. The function the file system uses to perform
the copy is the cache manager’s CcCopyRead function. CcCopyRead takes as
parameters a file object, file offset, and length.
When the cache manager executes CcCopyRead, it retrieves a pointer to a
shared cache map, which is stored in the file object. Recall that a shared
cache map stores pointers to virtual address control blocks (VACBs), with
one VACB entry for each 256 KB block of the file. If the VACB pointer for
a portion of a file being read is null, CcCopyRead allocates a VACB,
reserving a 256 KB view in the cache manager’s virtual address space, and
maps (using MmMapViewInSystemCache) the specified portion of the file
into the view. Then CcCopyRead simply copies the file data from the
mapped view to the buffer it was passed (the buffer originally passed to
ReadFile). If the file data isn’t in physical memory, the copy operation
generates page faults, which are serviced by MmAccessFault.
When a page fault occurs, MmAccessFault examines the virtual address
that caused the fault and locates the virtual address descriptor (VAD) in the
VAD tree of the process that caused the fault. (See Chapter 5 of Part 1 for
more information on VAD trees.) In this scenario, the VAD describes the
cache manager’s mapped view of the file being read, so MmAccessFault calls
MiDispatchFault to handle a page fault on a valid virtual memory address.
MiDispatchFault locates the control area (which the VAD points to) and
through the control area finds a file object representing the open file. (If the
file has been opened more than once, there might be a list of file objects
linked through pointers in their private cache maps.)
With the file object in hand, MiDispatchFault calls the I/O manager
function IoPageRead to build an IRP (of type IRP_MJ_READ) and sends the
IRP to the FSD that owns the device object the file object points to. Thus, the
file system is reentered to read the data that it requested via CcCopyRead, but
this time the IRP is marked as noncached and paging I/O. These flags signal
the FSD that it should retrieve file data directly from disk, and it does so by
determining which clusters on disk contain the requested data (the exact
mechanism is file-system dependent) and sending IRPs to the volume
manager that owns the volume device object on which the file resides. The
volume parameter block (VPB) field in the FSD’s device object points to the
volume device object.
The memory manager waits for the FSD to complete the IRP read and then
returns control to the cache manager, which continues the copy operation that
was interrupted by a page fault. When CcCopyRead completes, the FSD
returns control to the thread that called NtReadFile, having copied the
requested file data, with the aid of the cache manager and the memory
manager, to the thread’s buffer.
The path for WriteFile is similar except that the NtWriteFile system
service generates an IRP of type IRP_MJ_WRITE, and the FSD calls
CcCopyWrite instead of CcCopyRead. CcCopyWrite, like CcCopyRead,
ensures that the portions of the file being written are mapped into the cache
and then copies to the cache the buffer passed to WriteFile.
If a file’s data is already cached (in the system’s working set), there are
several variants on the scenario we’ve just described. If a file’s data is
already stored in the cache, CcCopyRead doesn’t incur page faults. Also,
under certain conditions, NtReadFile and NtWriteFile call an FSD’s fast I/O
entry point instead of immediately building and sending an IRP to the FSD.
Some of these conditions follow: the portion of the file being read must
reside in the first 4 GB of the file, the file can have no locks, and the portion
of the file being read or written must fall within the file’s currently allocated
size.
The fast I/O read and write entry points for most FSDs call the cache
manager’s CcFastCopyRead and CcFastCopyWrite functions. These variants
on the standard copy routines ensure that the file’s data is mapped in the file
system cache before performing a copy operation. If this condition isn’t met,
CcFastCopyRead and CcFastCopyWrite indicate that fast I/O isn’t possible.
When fast I/O isn’t possible, NtReadFile and NtWriteFile fall back on
creating an IRP. (See the earlier section “Fast I/O” for a more complete
description of fast I/O.)
Memory manager’s modified and mapped page
writer
The memory manager’s modified and mapped page writer threads wake up
periodically (and when available memory runs low) to flush modified pages
to their backing store on disk. The threads call IoAsynchronousPageWrite to
create IRPs of type IRP_MJ_WRITE and write pages to either a paging file or
a file that was modified after being mapped. Like the IRPs that
MiDispatchFault creates, these IRPs are flagged as noncached and paging
I/O. Thus, an FSD bypasses the file system cache and issues IRPs directly to
a storage driver to write the memory to disk.
Cache manager’s lazy writer
The cache manager’s lazy writer thread also plays a role in writing modified
pages because it periodically flushes views of file sections mapped in the
cache that it knows are dirty. The flush operation, which the cache manager
performs by calling MmFlushSection, triggers the memory manager to write
any modified pages in the portion of the section being flushed to disk. Like
the modified and mapped page writers, MmFlushSection uses
IoSynchronousPageWrite to send the data to the FSD.
Cache manager’s read-ahead thread
A cache uses two artifacts of how programs reference code and data:
temporal locality and spatial locality. The underlying concept behind
temporal locality is that if a memory location is referenced, it is likely to be
referenced again soon. The idea behind spatial locality is that if a memory
location is referenced, other nearby locations are also likely to be referenced
soon. Thus, a cache typically is very good at speeding up access to memory
locations that have been accessed in the near past, but it’s terrible at speeding
up access to areas of memory that have not yet been accessed (it has zero
lookahead capability). In an attempt to populate the cache with data that will
likely be used soon, the cache manager implements two mechanisms: a read-
ahead thread and Superfetch.
As we described in the previous section, the cache manager includes a
thread that is responsible for attempting to read data from files before an
application, a driver, or a system thread explicitly requests it. The read-ahead
thread uses the history of read operations that were performed on a file,
which are stored in a file object’s private cache map, to determine how much
data to read. When the thread performs a read-ahead, it simply maps the
portion of the file it wants to read into the cache (allocating VACBs as
necessary) and touches the mapped data. The page faults caused by the
memory accesses invoke the page fault handler, which reads the pages into
the system’s working set.
A limitation of the read-ahead thread is that it works only on open files.
Superfetch was added to Windows to proactively add files to the cache
before they’re even opened. Specifically, the memory manager sends page-
usage information to the Superfetch service
(%SystemRoot%\System32\Sysmain.dll), and a file system minifilter
provides file name resolution data. The Superfetch service attempts to find
file-usage patterns—for example, payroll is run every Friday at 12:00, or
Outlook is run every morning at 8:00. When these patterns are derived, the
information is stored in a database and timers are requested. Just prior to the
time the file would most likely be used, a timer fires and tells the memory
manager to read the file into low-priority memory (using low-priority disk
I/O). If the file is then opened, the data is already in memory, and there’s no
need to wait for the data to be read from disk. If the file isn’t opened, the
low-priority memory will be reclaimed by the system. The internals and full
description of the Superfetch service were previously described in Chapter 5,
Part 1.
Memory manager’s page fault handler
We described how the page fault handler is used in the context of explicit file
I/O and cache manager read-ahead, but it’s also invoked whenever any
application accesses virtual memory that is a view of a mapped file and
encounters pages that represent portions of a file that aren’t yet in memory.
The memory manager’s MmAccessFault handler follows the same steps it
does when the cache manager generates a page fault from CcCopyRead or
CcCopyWrite, sending IRPs via IoPageRead to the file system on which the
file is stored.
File system filter drivers and minifilters
A filter driver that layers over a file system driver is called a file system filter
driver. Two types of file system filter drivers are supported by the Windows
I/O model:
■    Legacy file system filter drivers usually create one or multiple device
objects and attach them on the file system device through the
IoAttachDeviceToDeviceStack API. Legacy filter drivers intercept all
the requests coming from the cache manager or I/O manager and must
implement both standard IRP dispatch functions and the Fast I/O path.
Due to the complexity involved in the development of this kind of
driver (synchronization issues, undocumented interfaces, dependency
on the original file system, and so on), Microsoft has developed a
unified filter model that makes use of special drivers, called
minifilters, and deprecated legacy file system drivers. (The
IoAttachDeviceToDeviceStack API fails when it’s called for DAX
volumes).
■    Minifilters drivers are clients of the Filesystem Filter Manager
(Fltmgr.sys). The Filesystem Filter Manager is a legacy file system
filter driver that provides a rich and documented interface for the
creation of file system filters, hiding the complexity behind all the
interactions between the file system drivers and the cache manager.
Minifilters register with the filter manager through the
FltRegisterFilter API. The caller usually specifies an instance setup
routine and different operation callbacks. The instance setup is called
by the filter manager for every valid volume device that a file system
manages. The minifilter has the chance to decide whether to attach to
the volume. Minifilters can specify a Pre and Post operation callback
for every major IRP function code, as well as certain “pseudo-
operations” that describe internal memory manager or cache manager
semantics that are relevant to file system access patterns. The Pre
callback is executed before the I/O is processed by the file system
driver, whereas the Post callback is executed after the I/O operation
has been completed. The Filter Manager also provides its own
communication facility that can be employed between minifilter
drivers and their associated user-mode application.
The ability to see all file system requests and optionally modify or
complete them enables a range of applications, including remote file
replication services, file encryption, efficient backup, and licensing. Every
anti-malware product typically includes at least a minifilter driver that
intercepts applications opening or modifying files. For example, before
propagating the IRP to the file system driver to which the command is
directed, a malware scanner examines the file being opened to ensure that it’s
clean. If the file is clean, the malware scanner passes the IRP on, but if the
file is infected, the malware scanner quarantines or cleans the file. If the file
can’t be cleaned, the driver fails the IRP (typically with an access-denied
error) so that the malware cannot become active.
Deeply describing the entire minifilter and legacy filter driver architecture
is outside the scope of this chapter. You can find more information on the
legacy filter driver architecture in Chapter 6, “I/O System,” of Part 1. More
details on minifilters are available in MSDN (https://docs.microsoft.com/en-
us/windows-hardware/drivers/ifs/file-system-minifilter-drivers).
Data-scan sections
Starting with Windows 8.1, the Filter Manager collaborates with file system
drivers to provide data-scan section objects that can be used by anti-malware
products. Data-scan section objects are similar to standard section objects (for
more information about section objects, see Chapter 5 of Part 1) except for
the following:
■    Data-scan section objects can be created from minifilter callback
functions, namely from callbacks that manage the IRP_MJ_CREATE
function code. These callbacks are called by the filter manager when
an application is opening or creating a file. An anti-malware scanner
can create a data-scan section and then start scanning before
completing the callback.
■    FltCreateSectionForDataScan, the API used for creating data-scan
sections, accepts a FILE_OBJECT pointer. This means that callers
don’t need to provide a file handle. The file handle typically doesn’t
yet exist, and would thus need to be (re)created by using
FltCreateFile API, which would then have created other file creation
IRPs, recursively interacting with lower level file system filters once
again. With the new API, the process is much faster because these
extra recursive calls won’t be generated.
A data-scan section can be mapped like a normal section using the
traditional API. This allows anti-malware applications to implement their
scan engine either as a user-mode application or in a kernel-mode driver.
When the data-scan section is mapped, IRP_MJ_READ events are still
generated in the minifilter driver, but this is not a problem because the
minifilter doesn’t have to include a read callback at all.
Filtering named pipes and mailslots
When a process belonging to a user application needs to communicate with
another entity (a process, kernel driver, or remote application), it can leverage
facilities provided by the operating system. The most traditionally used are
named pipes and mailslots, because they are portable among other operating
systems as well. A named pipe is a named, one-way communication channel
between a pipe server and one or more pipe clients. All instances of a named
pipe share the same pipe name, but each instance has its own buffers and
handles, and provides a separate channel for client/server communication.
Named pipes are implemented through a file system driver, the NPFS driver
(Npfs.sys).
A mailslot is a multi-way communication channel between a mailslot
server and one or more clients. A mailslot server is a process that creates a
mailslot through the CreateMailslot Win32 API, and can only read small
messages (424 bytes maximum when sent between remote computers)
generated by one or more clients. Clients are processes that write messages to
the mailslot. Clients connect to the mailslot through the standard CreateFile
API and send messages through the WriteFile function. Mailslots are
generally used for broadcasting messages within a domain. If several server
processes in a domain each create a mailslot using the same name, every
message that is addressed to that mailslot and sent to the domain is received
by the participating processes. Mailslots are implemented through the
Mailslot file system driver, Msfs.sys.
Both the mailslot and NPFS driver implement simple file systems. They
manage namespaces composed of files and directories, which support
security, can be opened, closed, read, written, and so on. Describing the
implementation of the two drivers is outside the scope of this chapter.
Starting with Windows 8, mailslots and named pipes are supported by the
Filter Manager. Minifilters are able to attach to the mailslot and named pipe
volumes (\Device\NamedPipe and \Device\Mailslot, which are not real
volumes), through the FLTFL_REGISTRATION_SUPPORT_NPFS_MSFS
flag specified at registration time. A minifilter can then intercept and modify
all the named pipe and mailslot I/O that happens between local and remote
process and between a user application and its kernel driver. Furthermore,
minifilters can open or create a named pipe or mailslot without generating
recursive events through the FltCreateNamedPipeFile or
FltCreateMailslotFile APIs.
 Note
One of the motivations that explains why the named pipe and mailslot file
system drivers are simpler compared to NTFS and ReFs is that they do
not interact heavily with the cache manager. The named pipe driver
implements the Fast I/O path but with no cached read or write-behind
support. The mailslot driver does not interact with the cache manager at
all.
Controlling reparse point behavior
The NTFS file system supports the concept of reparse points, blocks of 16
KB of application and system-defined reparse data that can be associated to
single files. (Reparse points are discussed more in multiple sections later in
this chapter.) Some types of reparse points, like volume mount points or
symbolic links, contain a link between the original file (or an empty
directory), used as a placeholder, and another file, which can even be located
in another volume. When the NTFS file system driver encounters a reparse
point on its path, it returns an error code to the upper driver in the device
stack. The latter (which could be another filter driver) analyzes the reparse
point content and, in the case of a symbolic link, re-emits another I/O to the
correct volume device.
This process is complex and cumbersome for any filter driver. Minifilters
drivers can intercept the STATUS_REPARSE error code and reopen the
reparse point through the new FltCreateFileEx2 API, which accepts a list of