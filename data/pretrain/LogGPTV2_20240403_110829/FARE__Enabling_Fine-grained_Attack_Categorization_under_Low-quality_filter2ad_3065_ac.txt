dataset (i.e., X ) is computed by averaging the loss on each
sample pair plus a regularization term on model parameters:
L =
1
|X|2
xi,xj∈X
L(xi, xj) + λ(cid:107)θ(cid:107)2 ,
(4)
where |X| is the number of samples in X , θ represents the
parameters of f, and λ controls the regularization strength.
Our network is a Multilayer Perceptron (MLP) with multi-
ple hidden layers and one output layer. We set the output layer
to have a much lower dimensionality than the original input
(i.e., h ∈ Rq×1, where q  1 .
Sili =
(7)
For cases where |Sci| = 1, we simply set Sili = 0. Given
the Sili of each sample, the silhouette coefﬁcient of a clus-
tering results with K clusters (i.e., Sil(K)) is deﬁned as the
maximum value of ˜Silk across all clusters, where ˜Silk is the
mean silhouette coefﬁcients over the Sili of all samples within
cluster k. The ﬁnal choice of K is then determined by the value
with the largest Sil(K).
C. Unsupervised Extension of FARE
taking any labels). Recall
While FARE is designed to take low-quality labels as
inputs, it can be extended to an unsupervised version (with-
out
that FARE obtains M sets
of neighborhood relationships (one from given labels and
M − 1 from clustering algorithms). When the “given labels”
are completely unavailable, FARE can work with the M − 1
clustering algorithms to obtain the neighborhood relationships.
In this way, we can use FARE as an unsupervised method.
In comparison with the existing clustering methods, the
advantage of FARE is it fuses the neighborhood relationships
from multiple models. We expect FARE to be less sensitive to
the variations in input data distribution and hyper-parameters,
and thus produce more reliable results. We will validate this
intuition in Section §IV.
D. Training Strategy and Hyper-parameters
We apply Adam optimizer to minimize the loss function
in Equation (4) and set its learning rate as 0.001. Technical
details of this optimization technique can be found in [41].
Neighborhood Models. We select three different cluster-
ing methods: K-means [28], DBSCAN [20], and DEC [78].
The rationale behind these choices is as following: K-means,
DBSCAN, DEC, and DAGMM are four existing clustering
methods that have been applied to different security applica-
tions. DAGMM is designed for anomaly detection but not for
multi-classes clustering tasks. As such, we use the other three
methods for FARE. These methods are reasonably diversiﬁed
to meet our needs. As mentioned before, we apply the selected
methods with different hyper-parameters and form M − 1
neighborhood models in total. The choice of M and clustering
parameters is further discussed in Appendix-A.
the
We deﬁne
Hyper-parameters.
following hyper-
parameters in FARE: the distance radius α, the neighborhood
model weight {pm}, and the regularization coefﬁcients λ. In
this paper, we ﬁxed α = q, where q is the output dimension
of f, and set λ to a small value 0.01. The most important
hyper-parameter is the neighborhood model weights {pm}.
Empirically, we ﬁnd that it is useful to use different weights for
the supervised neighborhood model (i.e., the “given labels”)
and the unsupervised models. However, among the M − 1
unsupervised models, we can simply use the same weight to
reduce the complexity of parameter tuning while still getting
comparable results. For simplicity, in this paper, we set pm = 1
for all the M − 1 clustering models, and only tune a single p1
to adjust the weight for the “given labels”. To determine p1,
we use a validation set during training. That is, we set the p1 as
the optimal value from [1, 10] that yields the highest adjusted
mutual information (AMI) on the labeled validation samples.
AMI is our evaluation metric, explained in Section §IV-A.
More details about the hyper-parameters are in Appendix-A.
Computational Overhead. Compared with existing cluster-
ing algorithms, FARE has introduced a few additional steps.
However, the computational overhead of FARE is comparable
to existing clustering algorithms. We will provide the empir-
ical evaluation in Section §IV-C, and discuss the asymptotic
complexity in Section §VI.
IV. EVALUATION
We evaluate the effectiveness of FARE on two security
applications: malware categorization and network intrusion
detection. We focus on four key aspects: 1) validating our
design choices, 2) comparing FARE with the state-of-the-art
semi-supervised and unsupervised algorithms, 3) evaluating
the computational overhead of FARE, and 4) evaluating the
sensitivity of FARE to label quality. Later in Section §V, we
will describe our experience deploying and testing FARE in a
real-world online service to detect fraudulent accounts.
A. Experimental Setup
Malware Categorization. We choose a malware dataset with
270,000 samples3. The dataset contains 6 different classes,
including one benign class of 150,000 samples and ﬁve mal-
ware classes of 120,000 samples. For malware classes, the
number of samples per class ranges from 15,000 to 37,500.
We construct the training set by randomly selecting 70% of
samples and used the rest of the samples as the testing set.
20% of labeled samples randomly selected from the training
set are held out for validation. Note that we split data randomly
instead of splitting temporally [56] because we are perform-
ing data clustering to identify ﬁne-grained malware families
instead of performing prediction tasks. In this dataset, each
sample is represented as a vector of 100 features, indicating
the sandbox behavior of the corresponding software.
Network Intrusion Detection. We select the widely used
KDDCUP dataset [37]. Each sample is a vector of 120 features,
representing the corresponding network trafﬁc behaviors (See
[70] for the detailed feature description). While this dataset
is not new, it provides an opportunity to evaluate FARE on
highly imbalanced data. In this paper, we selected a subset
3The dataset [2] is collected and shared by a security company.
7
with 9 classes, one of which has 97,278 normal network trafﬁc
and the rest 8 classes represent 8 types of intrusions.4 Our
selected dataset has 493, 346 samples. Note that the selected
classes cover 99.8% of the samples in the dataset. We remove
the remaining 0.2%, because they will be treated as noise by
most learning algorithms. We randomly split the dataset into
training and testing set with a ratio of 70:30, and randomly
pick 20% of the labeled training samples as the validation set.
Evaluation Metric. The output of FARE is a set of clusters.
To assess the clustering quality, we use a commonly used
metric called Adjusted Mutual Information (AMI) [74], which
measures the correlation between a cluster assignment and the
ground truth labels. In addition, we also consider the traditional
accuracy metric to provide a different perspective.
Note that the accuracy metric has some known limitations
to evaluate clustering algorithms. First, different clustering
algorithms may produce different numbers of clusters. To
compute the accuracy, we need to align clusters to labels. In
this paper, given a cluster, we assign the cluster’s label as the
most prevalent ground-truth label within this cluster. Second,
the accuracy metric is sensitive to data distribution across
classes [74]. For example, if one class is signiﬁcantly bigger
than all other classes (i.e., the benign class in network intrusion
detection), then producing one big cluster may trivially get
high accuracy.
For this reason, we use AMI as the primary metric. We
only report accuracy for selective experiments as reference
(e.g., Table I). To compute AMI, the ﬁrst step is to draw the
contingency table where each element represents the number of
overlapped samples in each cluster and the ground truth class.
Then we can compute the mutual information [42] between
the clustering results and ground truth labels based on the
contingency table. Finally, the AMI is obtained by normalizing
the mutual information. AMI takes values from [−1, 1], and A
higher value indicates a better performance. The key advantage
of AMI (compared to the accuracy metric) is AMI normalizes
the results of different cluster sizes and is not easily biased
by large clusters. The detailed explanation on how to calculate
AMI and its advantage over accuracy is in Appendix-C.
Baseline Methods. We mainly compare FARE with two
popular semi-supervised methods MixMatch [6] and Lad-
der [59] that have been used for security applications. The two
systems are proposed recently (in 2015 and 2019 respectively),
and have been highly cited. We also include a supervised
deep neural network (DNN) as the baseline. However, our
preliminary evaluation quickly reveals that these algorithms,
when applied end-to-end, perform poorly under missing classes
or coarse-grained labels. We have presented the detailed results
in Appendix-B. For example, most of their AMIs would fall
under 0.6 when the training set misses the labels for 2+ classes
or has 2+ classes sharing a union label. The reason is that none
of these algorithms assume there are missing classes or coarse-
grained labels in the training data. As a result, they all set the
number of ﬁnal classes as the number of given labels (or seen
classes), and only classify samples to known classes.
In order to fairly compare FARE with existing baseline
4We preserved the top-8 intrusion classes ranked by the number of samples:
neptune (107,201), smurf (280,790), backscatter (2,203), satan (1,589), ip
sweeping (1,247), port sweeping (1,040), warezclient (1,020), teardrop (979).
algorithms, we need to adapt existing algorithms to work under
missing classes and coarse-grained labels. More speciﬁcally,
we slightly amend existing algorithms with the same last step
of FARE: the ﬁnal clustering component and mechanism to
determine the number of clusters (step  in Figure 1). For each
experiment setup, we ﬁrst run the existing baseline algorithms
on the training data to train their networks. For all
three
baseline algorithms, the last hidden layer of their networks can
output a latent vector of the original input. Instead of using the
latent vectors for classiﬁcation, we run the same the K-means
clustering on these latent vectors (step  in FARE) to identify
the ﬁne-grained clusters. In this way, these baseline algorithms
can perform better under missing classes and coarse-grained
labels. We denote the amended version of MixMatch, Ladder,
and DNN as MixMatch+, Ladder+, and DNN+, respectively,
and use them as our baselines for evaluation.
In addition to semi-supervised baselines, we also compare
the unsupervised version of FARE with the base clustering
algorithms (DBSCAN, Kmeans, and DEC) and existing en-
semble clustering algorithms (CSPA and HGPA [67]).
Note that we did not include GZSL/ZSL methods as main
baselines considering the different assumptions and problem
setups (see Section §II-C). We only presented a brief exper-
iment in Appendix-B to run our methods against two GZSL
methods (i.e., OSDN [4] and DEM [80]). The results conﬁrm
that GZSL methods do not work well in our setup.
Training, Validation, and Testing. At a high level, we use the
training set to identify the ﬁnal clusters and their centroids. We
use the validation set for parameter turning during the training
process. The testing data is used to test the quality of the ﬁnal
clusters: for each testing sample, we compute its latent vector
and assign it to the nearest cluster based on its distance to the
cluster centroids. If not otherwise stated, AMIs/Accuracy are
calculated based on testing data for performance evaluation.
More speciﬁcally, for FARE, we use the training set to
construct
the neighborhood models to form the ensemble,
learn the parameters for the input transformation network,
and identify the centroids for the ﬁnal clusters. We use the
validation set to select the weight for the “ given labels”
(i.e., p1) and determine the number of ﬁnal clusters K. For
baseline algorithms (i.e., MixMatch+, Ladder+, and DNN+),
we follow a similar process (but do not need to train the
ensemble component).
B. Experimental Design
We design the four experiments to evaluate the effective-
ness of FARE from different aspects. First, we validate the
ensemble method used in FARE by comparing its performance
with the individual base clustering algorithms and existing
ensemble clustering methods. Second, we quantify the advan-
tage of FARE over three baselines in the “missing classes”
setting. Third, we run FARE and baselines in the “coarse-
grained labels” setting. Finally, we evaluate the robustness of
FARE to other factors such as the ratio of available labels and
the number of neighborhood models.
Experiment I: Comparing with Unsupervised Methods.
In this experiment, we want
to examine if the ensemble
of multiple clustering algorithms indeed introduces beneﬁts.
Recall that FARE takes the ensemble of M neighborhood
models. By default, we set M = 151 where one neighborhood
model is the “given labels”, and the other 150 neighborhood
models are contributed by three clustering algorithms (K-
means, DBSCAN, and DEC). More speciﬁcally, by varying the
hyper-parameters for each clustering algorithm, each algorithm
contributes to 50 neighborhood models (150 in total). For
the unsupervised version of FARE (Section §III-C), we set
M = 150 by excluding the model from the “given labels”.
In this experiment, we ﬁrst compare the unsupervised
version of FARE with the individual clustering algorithms and
the ensemble clustering methods CSPA and HGPA. Given an
evaluation dataset, we ﬁrst remove the labels, and apply the
unsupervised version of FARE and the individual clustering
algorithms (i.e., K-means, DBSCAN, and DEC) to the dataset.
We then apply CSPA and HGPA on top of the 150 neighbor-
hood models. Note that both HGPA and CSPA encountered
out-of-memory issues when being applied to the full datasets
(due to their O(n2) memory consumption). As such, we run
both methods on 10% of randomly sampled data points. To
ensure a fair comparison, we produce one set of result for
Unsup. FARE on the same 10% of the training dataset.
Next, to explore the beneﬁts of using (partial) labels, we
then use 1% of the original label to train FARE, and compare
it with the unsupervised FARE. For each setting, we repeat
the experiment 50 times. Note that DBSCAN and ensemble
clustering cannot be used to classify testing data. As such, for
this experiment, we report their training AMI and Accuracy.
Finally, to evaluate the computational cost of FARE, we also
record the overall training time of each method.
Experiment II: Missing Classes. This experiment focuses on
the end-to-end performance of FARE under the missing classes
setting. More speciﬁcally, we construct training datasets that
mimic the scenarios where a certain number of classes are