### Large File Analysis and Compression Ratio

The red line in Figure 6(e) represents the results for large file compression. Consistent with the findings from ELISE, as more data is used to train the model, the compression ratio decreases.

### Fine-tuning with Entire Data

Similar to the experiments conducted for ELISE, we also evaluate the effect of fine-tuning DeepZip by splitting the Lin-16.1G log file into 10 files (L0 to L9), each 200 MB in size. A model is pre-trained on file L0 and then fine-tuned on the remaining nine files, L1 to L9. The results are presented in Figure 6(f). As observed, some compression ratios are low while others are high. This variability is due to DeepZip's ability to identify only simple contextual redundancies, not complex ones such as monotonous values and sessions. Consequently, when the contexts in the test files (L1 to L9) are similar to the training file (L0), the pre-trained model achieves lower compression ratios, and vice versa. This result highlights the advantage of using ELISE, which, through preprocessing rules, can capture all types of contextual and structural redundancies, achieving better results even with different workloads.

### Fine-tuning with Partial Data

We also explore fine-tuning with partial data in DeepZip, similar to the approach used in ELISE. Specifically, a model is pre-trained on a 200 MB file split from Lin-7.7G and then fine-tuned on varying percentages of another 200 MB file. The blue line in Figure 6(e) shows the compression ratios under different settings. The trend is consistent with using partial data training and aligns with the trends observed in ELISE.

### Support for Security Investigations

As a lossless compression method, ELISE naturally supports all log-based security applications. To validate this, we perform log-based security incident investigations using logs from the DARPA Transparent Computing (TC) project Engagement 5 and compare the results with existing work [41, 42] to ensure ELISE produces the same outcomes. These tasks involve forensic analysis, generating a provenance graph from log data, and analyzing attack activities by searching backward to find all malicious events leading to the activity and forward to identify affected files, processes, etc. For each task, we start from a system subject or object and compare the results of using ELISE and raw logs. Table 3 summarizes the results for three experiments, including the number of nodes in the generated graphs using unmodified logs and ELISE (columns 2 and 3, respectively). We also manually verify if the graphs match (results in column 4 of Table 3). As indicated, ELISE fully supports log-based security analysis.

### Discussion

DeepZip and ELISE demonstrate the significant potential of DNN-based data compression. In some cases, they can reduce space overhead by up to 10 times compared to traditional methods like Gzip. However, DeepZip has a high runtime overhead. ELISE addresses this issue with novel techniques, making it more practical. Despite this, Gzip still outperforms DNN-based compressions. Given the large size of real-world logs, even a 1% lower compression ratio can save gigabytes of storage space per day in large organizations. Individual log files are decompressed only when needed, making ELISE valuable in practice. We envision that with better DNN inference optimization, such as hardware acceleration (e.g., AI chips) and model compression, the runtime of ELISE can be further optimized.

### Future Research Directions

Based on the proposed work, there are several promising research directions:

1. **Optimizing ELISE Runtime**: Although ELISE reduces runtime overhead, it still lags behind Gzip. Optimizing the DNN inference, which is the most time-consuming step, is crucial. Existing methods, such as inference frameworks, hardware acceleration, and model compression, can be leveraged.
   
2. **Optimizing Preprocessing and Model Training**: Identifying additional redundancies in log files and designing new preprocessing steps can improve compression efficiency. Similarly, optimizing the training procedure with better model architectures or loss functions can enhance performance. AutoML techniques may also be adapted for this application.

3. **Integrating ELISE in Existing Systems**: ELISE can be integrated into existing provenance and logging systems, potentially in conjunction with redundancy reduction techniques like LogGC [37]. Exploring how to integrate ELISE into these systems is an interesting direction.

### Related Work

In addition to the directly related works on data compression and log reduction discussed in Section 2, ELISE is closely related to log analysis. Existing threat detection approaches typically learn normal behavior patterns from logs and detect anomalies. Some define normal patterns as single event matching rules [2, 3, 39], while others use fixed-size sequences of syscalls [15] or variable-size sequences [11, 59].

For Advanced Persistent Threats (APTs), many methods leverage contextual information from log events to analyze provenance graphs, reducing false positive alarms. Some create static or dynamic normal behavior models [22, 44], and SOTA systems like Unicorn [21] achieve better detection results by learning multiple normal behavior models from log data.

NoDoze [26] uses historical system execution information to learn normal behavior patterns, assigning higher anomaly values to rare events and propagating these values through causally related events. Provenance graphs are widely used in security analyses, such as forensic analysis and attack attribution [5, 51]. Techniques like HERCULE [50] reconstruct attack histories using community discovery on correlated log graphs, while ProTracer [42] designs lightweight tracing systems to reduce runtime overhead.

### Conclusion

In this paper, we propose and build novel lossless data compression techniques, ELISE, to create storage-efficient logging systems. By leveraging preprocessing steps, ELISE reduces structural and contextual redundancies that existing techniques cannot, converting natural language logs to numerical formats. It uses a deep neural network-based representation learning technique to train an optimal encoder. Our evaluation shows that ELISE outperforms existing lossless compression techniques by 1.13 to 12.97 times with less than 20% overhead.

### Availability

ELISE is hosted on GitHub, and ready-to-use containers are provided to facilitate reproducibility and deployment. The code can be found at: https://github.com/dhl123/ELISE-2021

### References

[1] Logstash. Website, 2020. https://www.elastic.co/cn/logstash.

[2] How many alerts is too many to handle. https://www.fireeye.com/offers/rpt-idc-the-numbers-game.html., 2021.

[3] Insider threat detection. https://www.netwrix.com/insiderthreatdetection.html., 2021.

[4] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: A system for large-scale machine learning. In Kimberly Keeton and Timothy Roscoe, editors, 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016, Savannah, GA, USA, November 2-4, 2016, pages 265–283. USENIX Association, 2016.

[5] Adam Bates, Dave Jing Tian, Kevin RB Butler, and Thomas Moyer. Trustworthy whole-system provenance for the Linux kernel. In 24th USENIX Security Symposium (USENIX Security 15), pages 319–334, 2015.

[6] Fabrice Bellard. Lossless data compression with neural networks. https://bellard.org/nncp/nncp.pdf, 2019.

[7] François Chollet et al. Keras. https://keras.io, 2015.

[8] John G. Cleary and Ian H. Witten. Data compression using adaptive coding and partial string matching. IEEE Trans. Commun., 32(4):396–402, 1984.

[9] David Cox. Syntactically informed text compression with recurrent neural networks. arXiv preprint, arXiv:1608.02893, 2016.

[10] Hervé Debar, Marc Dacier, Mehdi Nassehi, and Andreas Wespi. Fixed vs. variable-length patterns for detecting suspicious process behavior. In Jean-Jacques Quisquater, Yves Deswarte, Catherine A. Meadows, and Dieter Gollmann, editors, Computer Security - ESORICS 98, 5th European Symposium on Research in Computer Security, Louvain-la-Neuve, Belgium, September 16-18, 1998, Proceedings, volume 1485 of Lecture Notes in Computer Science, pages 1–15. Springer, 1998.

[11] David Devecsery, Michael Chow, Xianzheng Dou, Jason Flinn, and Peter M. Chen. Eidetic systems. In Jason Flinn and Hank Levy, editors, 11th USENIX Symposium on Operating Systems Design and Implementation, OSDI ’14, Broomfield, CO, USA, October 6-8, 2014, pages 525–540. USENIX Association, 2014.

[12] Min Du, Feifei Li, Guineng Zheng, and Vivek Srikumar. DeepLog: Anomaly detection and diagnosis from system logs through deep learning. In Bhavani M. Thuraisingham, David Evans, Tal Malkin, and Dongyan Xu, editors, Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, CCS 2017, Dallas, TX, USA, October 30 - November 03, 2017, pages 1285–1298. ACM, 2017.

[13] En.Wikipedia.Org. 2020 United States federal government data breach. https://en.wikipedia.org/wiki/2020_United_States_federal_government_data_breach, 2021.

[14] Stephanie Forrest, Steven A. Hofmeyr, Anil Somayaji, and Thomas A. Longstaff. A sense of self for UNIX processes. In 1996 IEEE Symposium on Security and Privacy, May 6-8, 1996, Oakland, CA, USA, pages 120–128. IEEE Computer Society, 1996.

[15] Qiang Fu, Jian-Guang Lou, Yi Wang, and Jiang Li. Execution anomaly detection in distributed systems through unstructured log analysis. In Wei Wang, Hillol Kargupta, Sanjay Ranka, Philip S. Yu, and Xindong Wu, editors, ICDM 2009, The Ninth IEEE International Conference on Data Mining, Miami, Florida, USA, 6-9 December 2009, pages 149–158. IEEE Computer Society, 2009.

[16] Ashvin Goel, Kenneth Po, Kamran Farhadi, Zheng Li, and Eyal de Lara. The Taser intrusion recovery system. In Andrew Herbert and Kenneth P. Birman, editors, Proceedings of the 20th ACM Symposium on Operating Systems Principles 2005, SOSP 2005, Brighton, UK, October 23-26, 2005, pages 163–176. ACM, 2005.

[17] Noah Golmant, Nikita Vemuri, Zhewei Yao, Vladimir Feinberg, Amir Gholami, Kai Rothauge, Michael W. Mahoney, and Joseph Gonzalez. On the computational inefficiency of large batch sizes for stochastic gradient descent. arXiv preprint, arXiv:1811.12941, 2018.

[18] Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, and Idoia Ochoa. DeepZip: Lossless data compression using recurrent neural networks. In Ali Bilgin, Michael W. Marcellin, Joan Serra-Sagristà, and James A. Storer, editors, 2019 Data Compression Conference, DCC 2019, Snowbird, UT, USA, March 26-29, 2019, pages 179–188. IEEE, 2019.