when given any point included in the concretization of the
box in Fig. 6a. Analogous properties hold for the Zonotope
and Polyhedra transformers. It is also important that abstract
transformers are precise. That is, the abstract output should
include as few points as possible. For example, as we can
see in Fig. 6b, the output produced by the Box transformer
is less precise (it is larger) than the output produced by the
Zonotope transformer, which in turn is less precise than the
output produced by the Polyhedra transformer.
Property Veriﬁcation. After obtaining the (abstract) output,
we can check various properties of interest on the result. In
general, an abstract output a = T #
f (X) proves a property
Tf (X) ⊆ C if γn(a) ⊆ C. If the abstract output proves a
property, we know that the property holds for all possible
concrete values. However, the property may hold even if it
cannot be proven with a given abstract domain. For example, in
Fig. 6b, for all concrete points, the property C = {(y1, y2) ∈
2 | y1 ≥ −2} holds. However, with the Box domain, the
R
abstract output violates C, which means that the Box domain
is not precise enough to prove the property. In contrast, the
Zonotope and Polyhedra domains are precise enough to prove
the property.
In summary, to apply AI successfully, we need to: (a) ﬁnd a
suitable abstract domain, and (b) deﬁne abstract transformers
that are sound and as precise as possible. In the next section,
we introduce abstract transformers for neural networks that
are parameterized by the numerical abstract domain. This
means that we can explore the precision-scalability trade-off
by plugging in different abstract domains.
IV. AI2: AI FOR NEURAL NETWORKS
In this section we present AI2, an abstract interpretation
framework for sound analysis of neural networks. We begin by
deﬁning abstract transformers for the different kinds of neural
network layers. Using these transformers, we then show how
to prove robustness properties of neural networks.
A. Abstract Interpretation for CAT Functions
({(0, 1), (1, 1), (1, 3), (2, 2)}) = {0 ≤ x1 ≤ 2, 1 ≤ x2 ≤ 3}.
α2
In this section, we show how to overapproximate CAT
functions with AI. We illustrate the method on the example in
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 05:59:05 UTC from IEEE Xplore.  Restrictions apply. 
8
(cid:9)
(cid:10)
2 −1
0
1
x2
z0
3
2
1
−1
1
2
x1
3
−2 −1
x2
z1
3
3
3
2
2
1
1
1
x1
1
2
z2 =
z1(cid:12)(x1≥ 0)
x2
3
2 z2
2
(cid:9)
(cid:10)
1 0
0 1
−2 −1
z3
z3 =
z1(cid:12)(x1 < 0)
−2 −1
1
3
3
3
2
2
1
1
1
x2
x1
1
2
−2 −1
(cid:9)
(cid:10)
0 0
0 1
x1
1
2
−2 −1
x2
3
2 z4
2
1
3
2
1
x2
z5
ReLU1
x1
1
2
(cid:13)
x1
1
2
x2
3
2 z6
2
1
−2 −1
ReLU2
x1
1
2
−2 −1
y2
3
2 z7
2
1
y1
1
2
Fig. 7: Illustration of how AI2 overapproximates neural network states. Blue circles show the concrete values, while green
zonotopes show the abstract elements. The gray box shows the steps in one application of the ReLU transformer (ReLU1).
Fig. 7, which shows a simple network that manipulates two-
dimensional vectors using a single fully connected layer of the
form f (x) = ReLU2
(cid:12)(cid:12)
(cid:12) · x
ReLUi(x) = case (xi ≥ 0) : x,
. Recall that
ReLU1
2 −1
0 1
(cid:11)(cid:11)
(cid:11)
case (xi < 0) : Ii←0 · x,
2, given as:
input. Note that,
where Ii←0 is the identity matrix with the ith row replaced
by the zero vector. We overapproximate the network behavior
on an abstract input. The input can be obtained directly (see
Sec. IV-B) or by abstracting a set of concrete inputs to an
abstract element (using the abstraction function α). For our
example, we use the concrete inputs (the blue points) from
Fig 6. Those concrete inputs are abstracted to the green
zonotope z0 : [−1, 1]3 → R
z0(1, 2, 3) = (1 + 0.5 · 1 + 0.5 · 2, 2 + 0.5 · 1 + 0.5 · 3).
Due to abstraction, more (spurious) points may be added. In
this example, except the blue points, the entire area of the
zonotope is spurious. We then apply abstract transformers
if a function f can be
to the abstract
written as f = f(cid:2)(cid:2) ◦ f(cid:2), the concrete transformer for f is
Tf = Tf(cid:2)(cid:2) ◦ Tf(cid:2). Similarly, given abstract transformers T #
f(cid:2)
and T #
f(cid:2) . When
a neural network N = f(cid:2)
1 is a composition of
i (x) = W · x + b or
multiple CAT functions f(cid:2)
fi(x) = case E1 : f1(x), . . . , case Ek : fk(x), we only have
to deﬁne abstract transformers for these two kinds of functions.
◦··· ◦ T #
We then obtain the abstract transformer T #
.
f(cid:2)
Abstracting Afﬁne Functions. To abstract functions of the
form f (x) = W · x + b, we assume that the underlying ab-
stract domain supports the operator Aff that overapproximates
such functions. We note that for Zonotope and Polyhedra,
this operation is supported and exact. Fig. 7 demonstrates
Aff as the ﬁrst step taken for overapproximating the effect
of the fully connected layer. Here, the resulting zonotope
z1 : [−1, 1]3 → R
z1(1, 2, 3) =
f(cid:2)(cid:2), an abstract transformer for f is T #
(cid:2) ◦ ··· ◦ f(cid:2)
i of the shape f(cid:2)
f(cid:2)(cid:2) ◦ T #
(2 · (1 + 0.5 · 1 + 0.5 · 2) − (2 + 0.5 · 1 + 0.5 · 3),
2 + 0.5 · 1 + 0.5 · 3) =
(0.5 · 1 + 2 − 0.5 · 3, 2 + 0.5 · 1 + 0.5 · 3).
2 is:
N = T #
f(cid:2)
(cid:2)
1
Abstracting Case Functions. To abstract functions of the
form f (x) = case E1 : f1(x), . . . , case Ek : fk(x), we ﬁrst
split the abstract element a into the different cases (each
deﬁned by one of the expressions Ei), resulting in k abstract
elements a1, . . . , ak. We then compute the result of T #
fi (ai)
for each ai. Finally, we unify the results to a single abstract
element. To split and unify, we assume two standard operators
for abstract domains: (1) meet with a conjunction of linear
constraints and (2) join. The meet ((cid:12)) operator is an abstract
transformer for set intersection: for an inequality expression
E from Fig. 3, γn(a) ∩ {x ∈ R
n | x |= E} ⊆ γn(a (cid:12) E).
The join ((cid:13)) operator is an abstract transformer for set union:
γn(a1) ∪ γn(a2) ⊆ γn(a1 (cid:13) a2). We further assume that
the abstract domain contains an element ⊥, which satisﬁes
γn(⊥) = {}, ⊥ (cid:12) E = ⊥ and a (cid:13) ⊥ = a for a ∈ A.
[−1, 1] × [0, 1] × [−1, 1] → R
[−1, 1] × [−1, 0] × [−1, 1] → R
For our example in Fig. 7, abstract interpretation continues
on z1 using the meet and join operators. To compute the effect
of ReLU1, z1 is split into two zonotopes z2 = z1 (cid:12) (x1 ≥ 0)
and z3 = z1 (cid:12) (x1 < 0). One way to compute a meet between
a zonotope and a linear constraint is to modify the intervals
of the error terms (see [11]). In our example, the resulting
2 such that
zonotopes are z2 :
2 such
z2() = z1() and z3 :
that z3() = z1() for ¯ common to their respective domains.
Note that both z2 and z3 contain small spurious areas, because
the intersections of the respective linear constraints with z1 are
not zonotopes. Therefore, they cannot be captured exactly by
the domain. Here, the meet operator (cid:12) overapproximates set
intersection ∩ to get a sound, but not perfectly precise, result.
Then, the two cases of ReLU1 are processed separately. We
apply the abstract transformer of f1(x) = x to z2 and we
apply the abstract transformer of f2(x) = I0←0 · x to z3. The
resulting zonotopes are z4 = z2 and z5 : [−1, 1]2 → R
2 such
that z5(1, 3) = (0, 2+0.5·1 +0.5·3). These are then joined
to obtain a single zonotope z6. Since z5 is contained in z4,
we get z6 = z4 (of course, this need not always be the case).
Then, z6 is passed to ReLU2. Because z6(cid:12)(x1 < 0) = ⊥, this
results in z7 = z6. Finally, γ2(z7) is our overapproximation of
the network outputs for our initial set of points. The abstract
element z7 is a ﬁnite representation of this inﬁnite set.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 05:59:05 UTC from IEEE Xplore.  Restrictions apply. 
9
For f (x) = W · x + b, T #
For f (x) = case E1 : f1(x), . . . , case Ek : fk(y),
f (a) = Aff(a, W, b).
(cid:13)
T #
f (a) =
#
fi
(a (cid:12) Ei).
For f (x) = f2(f1(x)), T #
f (a) = T #
1≤i≤k
f2 (T #
f1 (a)).
Fig. 8: Abstract transformers for CAT functions.
In summary, we deﬁne abstract transformers for every kind
of CAT function (summarized in Fig. 8). These deﬁnitions
are general and are compatible with any abstract domain A
which has a minimum element ⊥ and supports (1) a meet
operator between an abstract element and a conjunction of
linear constraints, (2) a join operator between two abstract
elements, and (3) an afﬁne transformer. We assume that
the operations are sound. We note that these operations are
standard or deﬁnable with standard operations. By deﬁnition
of the abstract transformers, we get soundness:
Theorem 1. For any CAT function f with transformer
Tf : P(R
n) and any abstract input a ∈ Am,
m) → P(R
Tf (γm
(a)) ⊆ γn
(T #
f (a)).
Theorem 1 is the key to sound neural network analysis with
our abstract transformers, as we explain in the next section.
B. Neural Network Analysis with AI
In this section, we explain how to leverage AI with our ab-
stract transformers to prove properties of neural networks. We
focus on robustness properties below, however, the framework
can be applied to reason about any safety property.
m → R
n is a pair (X, C) ∈ P(R
For robustness, we aim to determine if for a (possibly un-
bounded) set of inputs, the outputs of a neural network satisfy
a given condition. A robustness property for a neural network
n) consisting
N : R
of a robustness region X and a robustness condition C. We
say that the neural network N satisﬁes a robustness property