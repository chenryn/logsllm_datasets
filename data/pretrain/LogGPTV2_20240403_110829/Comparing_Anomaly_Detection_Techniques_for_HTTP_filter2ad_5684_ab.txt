called the simpliﬁed Mahalanobis distance:
d(x, y) =
n−1(cid:2)
i=0
|
| xi − yi
σi + α
< ∞
n is 256 for the ASCII character set. The α term is a smoothing factor so that the
distance does not become inﬁnite when σi is 0. Wang and Stolfo did not specify
how they calculate α; for the results reported in this paper, α = 0.001. Wang
and Stolfo set the distance threshold to 256 (one standard deviation). Using this
value means that rare distributions are anomalous; consequently it reports false
positives even when tested on the training data set.
Our implementation diﬀers with Wang and Stolfo’s slightly. They correlated
packet length with character frequencies. Our data consists only of the data
at the application layer; the raw packets containing the data were not stored.
Therefore, we apply this method to the complete request. Note that while the
diﬀerent packet sizes may have a given character distribution, an attacker can
easily control the packet size, allowing them to use packets of a size with a better
match for the character distribution.
χ2 of idealized character distribution. As one of six tests, Kruegel and Vigna
[22] use a measure of relative character frequency. They produced a sorted list
of character frequencies fc containing the relative frequency of the character c.
Their example is the string passwd, where the absolute frequency distribution is
2 for s, 1 for a, d, p, and w, and 0 for all other characters. The relative frequencies
are then f = ( 1
6 , 0, ..., 0); note that f6 through f256 are 0. Kruegel and
Vigna noted that relative frequencies decrease slowly for non-attack requests,
but have a much steeper decline for buﬀer overﬂows, and no decline for random
data.
6 , 1
3 , 1
6 , 1
6 , 1
Comparing Anomaly Detection Techniques for HTTP
49
(cid:3)256
They called the character distribution induced from the training data the
idealized character distribution (ICD) and noted that
ICD(i) = 1.0. As
mentioned in the prior paragraph, the ICD is sorted so most common frequency
is ICD(1) and the least common is ICD(256). ICD is calculated during training
as the average over the character distributions of the requests in the training
data.
For testing, they binned the ICD (the expected distribution, calculated through
training) and the distribution of the test request (observed distribution) into six
bins as follows:
i=1
Bin 1 2
i 1 2–4 5–7 8–12 13–16 17-256
3
6
4
5
where i ∈ [1, 256]. For example, bin 4 contains
i=8 ICD(i). Once binned, they
then use a χ2 test to determine if the character distribution of CGI parameter
values is similar to that of the training data:
(cid:3)12
χ2 =
6(cid:2)
(Oi − Ei)2
i=1
Ei
where Ei is bin i for the ICD, and Oi is bin i for the observed distribution. χ2 is
compared to values from a table and the corresponding probability is the return
value.
CGI Parameter Measures. Kruegel and Vigna [22] used three diﬀerent ob-
servations about CGI parameters. First, they noted that since CGI parameters
are set programmatically, the normal order of the parameters is ﬁxed. If a hu-
man generates the path, the order could be diﬀerent, and they presumed this
change indicated a potential attack. For similar reasons, they also noted CGI
parameters are supplied even when they have no value. The result is a regularity
in the number, name, and order of the parameters. Their system learned the
parameters present for a given CGI program path. When testing an instance,
the return value is 1 if the same parameters appeared in the training data as in
the test instance, and 0 otherwise.
Similar to the presence and absence test, Kruegel and Vigna noted that some
CGI parameter values are selected from a ﬁnite set (enumerated), and others
are eﬀectively random. In the training phase, they test to see whether the num-
ber of parameter values stays small compared to the number of examples. If it
does, then the parameter values are enumerated and the algorithm performs no
generalization. Otherwise, it accepts any value during testing.
DFA. We use a one-pass, O(nm) DFA induction algorithm where n is the
number of samples in the training data set and m is the average number of tokens
per sample. The algorithm does not require negative examples. This algorithm
is described in detail by Ingham et al. [21].
A DFA by itself is simply a language acceptor; however, we expect some
variation in normal behavior not incorporated in the DFA induction algorithm.
50
K.L. Ingham and H. Inoue
When testing, the algorithm notes when it is unable to make a transition on a
token. If a state exists which is a destination of that token, the DFA is adjusted
to that state. If not, the algorithm uses the next token and tries again. The
number of missed tokens is used to calculate the similarity s between the DFA
model and an HTTP request:
s =
# of tokens reached by valid transitions
# of tokens in the HTTP request
∈ [0, 1]
The similarity measure reﬂects the proportion of the request requiring changes
for the DFA to accept the request. Using proportionality instead of a raw miss
count allows complex requests to have greater variability than simpler ones.
Markov Model. A Markov model is a nondeterministic ﬁnite automaton (NFA)
with probabilities associated with the transitions. A Markov model diﬀers from a
DFA in that multiple transitions might exist for a given token, and a probability
is associated with each transition. The probability of a given string of tokens can
be calculated as the sum of the probabilities of each independent path through
the NFA that can generate the string of tokens. The probability of a given path is
the product of the probabilities of each of the transitions, and this probability is
interpreted as the similarity measure for the testing. Similar to a DFA, a Markov
Model represents the structure of the HTTP request through a directed graph.
For an anomaly detection system, the traditional approach is to build an
NFA that exactly matches the training data. Through a series of state merging
operations, it is compressed and hence it becomes more general (and, as a side
eﬀect, it becomes a DFA with probabilities). For more details about Markov
model induction, see the work by Stolcke [34] and Stolcke and Omohundro [33].
Warrender et al. noted that building a generalized Markov model is O(n2) [39].
Markov models have been shown to be an eﬀective but time-consuming algo-
rithm for system-call based intrusion detection [39]. Kruegel and Vigna [22] used
a Markov model as a portion of the IDS for protecting web servers, but after
noting that the probability of any given request string is small, they used their
Markov model as a DFA, noting only whether or not the model was capable of
generating the string in question.
Our Markov model implementation is a modiﬁcation of the DFA algorithm
described in Section 3.3. When learning the DFA, the number of times that a
transition is taken is recorded, and the probability of taking a given transition is
the fraction of the sum of all of the transitions that the taken transition repre-
sents. This approach is not exactly the same as a more traditional Markov model,
but the result is similar in size and eﬀect to a Markov model after generalization.
Linear Combination. Combining IDSs is a logical step once more than one
IDS is available. The system developed by Kruegel and Vigna [22] was limited
to HTTP CGI requests, and consisted of a linear combination of the length,
character distribution, order, and presence or absence of CGI parameter
values. Additionally, it also included a test for which CGI parameter values were
enumerated or random, and a Markov model to learn the structure of those
values.
Comparing Anomaly Detection Techniques for HTTP
51
The threshold for normal for each algorithm was determined dynamically,
chosen to be 10% above the highest value obtained in training. Calculating
the threshold requires a second pass over the training data, testing it to ﬁnd
the maximum value for each measure. For testing, each algorithm was equally
weighted and the system produced a binary normal/abnormal signal.
n-grams. An n-gram [9] is a substring generated by sliding a window of length
n across a string of tokens. The result is a set of strings of length n. For example,
given the string abcdef and n = 3, the resulting 3-grams are: abc, bcd, cde,
and def. The similarity measure considers the presence or absence of the test
n-grams in the set of n-grams learned from the training data:
s =
# of n-grams from the request also in the training data
# of n-grams in the HTTP request
∈ [0, 1]
The n-gram algorithm can use either tokens or strings from the data source.
Early testing showed poor results for strings, so we report results using tokens
as the alphabet.
Targeted Generalization Heuristics. To improve the accuracy of the n-
gram and DFA induction algorithms, we also applied several heuristics that
increase the generalization. These check that certain data types have a valid
(parsable) format. If so, they return a small, enumerated set of values dependent
on the heuristic. The data types that are checked for valid form are host names,
IP addresses, dates, various hash values (PHP session IDs, HTTP entity tags,
etc), ﬂoating point numbers (HTTP q-values), and email addresses. Ingham and
Ingham et al. provide a detailed description of these heuristics in [20,21].
4 Results
The traditional method for reporting IDS results is a receiver operating char-
acteristic (ROC) curve that shows the tradeoﬀ between identifying real attacks
(true positives) and incorrectly ﬂagging non-attack requests as an attack (false
positives) [16]. True or false positives are represented in the ROC curves pre-
sented here as the fraction of the attack database or test data set properly or
improperly identiﬁed. Each set of connected points represents a diﬀerent data set
used with the algorithm, and each point represents a diﬀerent similarity thresh-
old for distinguishing normal from abnormal. A perfect algorithm would have a
point at (0, 1) (the upper-left corner of the graph) indicating no false positives
and 100% correct identiﬁcation of attacks. In order to better see the most ac-
curate range, the plots only show the X axis values in [0, 0.1]. The portion of
the plot in the rest of the X axis represents a range where the false positives
would be too high for production use; we visit this claim in Section 4.7. The axes
in these plots indicate the actual fraction of true and false positives in the test.
52
K.L. Ingham and H. Inoue
To ease comparisons between algorithms, most of the ROC plots have the same
scale; one required a diﬀerent scale to present the data, and this fact is noted in
the plot description.
McHugh noted several potential problems in presenting IDS test results with
ROC curves [28]. His ﬁrst objection is that some researchers presented curves
with only one measured point and assumed continuity from (0,0), through their
point, to (1,1). We present plots with 128 uniformly divided points in [0, 1]. No
assumption is made about (0,0) or (1,1). McHugh also pointed out that for the
ROC curves to be comparable, the unit of analysis must be the same. For every
test in this paper, this unit of analysis is always one HTTP request. The tests
we performed used the data and framework described in Section 3.
4.1 Length
Accuracy is below 80% true positive at tolerable false positive rates (see Fig-
ure 1). This measure can detect some buﬀer overﬂows and cross-site scripting
attacks, however, attacks such as the Apache chunked transfer error [4] and some
variants of Nimda [10] are short enough to pass as normal; if they are too short,
padding to increase the length is easy. Therefore, a minimum length will never
stop an attack other than by a simplistic attacker. Because this algorithm ac-
cepts many strings that are not legal HTTP, an attacker has great freedom in
the construction of her attack.
If this algorithm were to be applied to tokens, it would overgeneralize. Con-
sider how many sentences with n words are valid English-language sentences.
Therefore, this algorithm is unlikely to ever be useful in isolation. It might be
applied as one of several algorithms, assuming non-attack requests have a tight
enough upper bound on their length.
ROC Curves for Length
n
o
i
t
c
a
r
f
e
v
i
t
i
s
o
p
e
u
r
T
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
 0.02
 0.04
 0.06
 0.08
 0.1
False positive fraction
aya.org
explorenm.com
i-pi.com
cs.unm.edu
Fig. 1. Receiver Operating Characteristic curves showing the accuracy of the length
algorithm
Comparing Anomaly Detection Techniques for HTTP
53
4.2 Character Distributions
Our Mahalanobis distance results (see Figure 2) diﬀer from Wang and Stolfo’s
[38]. Note that the cs.unm.edu accuracy is lower than other sites, indicating that
the measure’s accuracy depends on the mix of HTTP requests. Wang and Stolfo
reported true positive rates about 90% with a 20% false positive rate on the
Lincoln Labs data. Trained and tested using their own departmental server, the
false positive rate improved, ranging from 0.0084% to 1.3%. They found their
system did not always detect variants of exploits used during training. A possible
explanation is their dependence on packet size in their calculations. As we noted
in Section 3.3, an attacker can easily manipulate packet size, so we question the
usefulness of this correlation.
ROC Curves for Mahalanobis Distance
n
o
i
t
c
a
r
f
e
v
i
t
i
s
o
p
e
u
r
T