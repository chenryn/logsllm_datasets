其中临时数据被分成若干个 partition，每个 partition 将被一个Reduce Task 处理。
25.1.3.5. Reduce Task 执行过程
该过程分为三个阶段
1. 从远程节点上读取MapTask中间结果（称为“Shuffle 阶段”）；
2. 按照key对key/value对进行排序（称为“ Sort 阶段”）；
3. 依次读取，调用用户自定义的 reduce() 函数处理，并将最终结果存到 HDFS
上（称为“ Reduce 阶段”）。
13/04/2018 Page 261 of 283
25.1.4. Hadoop MapReduce 作业的生命周期
1.作业提交与初始化
1. 用户提交作业后， 首先由 JobClient 实例将作业相关信息， 比如将程序 jar 包、作业配置文
件、 分片元信息文件等上传到分布式文件系统（ 一般为 HDFS）上，其中，分片元信息文件
记录了每个输入分片的逻辑位置信息。 然后 JobClient 通过 RPC 通知 JobTracker。
JobTracker 收到新作业提交请求后， 由 作业调度模块对作业进行初始化：为作业创建一个
JobInProgress 对象以跟踪作业运行状况， 而 JobInProgress 则会为每个 Task 创建一个
TaskInProgress 对象以跟踪每个任务的运行状态， TaskInProgress 可能需要管理多个
“ Task 运行尝试”（ 称为“ Task Attempt”）。
2.任务调度与监控。
2. 前面提到，任务调度和监控的功能均由 JobTracker 完成。TaskTracker 周期性地通过
Heartbeat 向 JobTracker 汇报本节点的资源使用 情况， 一旦出 现空闲资源， JobTracker
会按照一定的策略选择一个合适的任务使用该空闲资源， 这由任务调度器完成。 任务调度器
是一个可插拔的独立模块， 且为双层架构， 即首先选择作业， 然后从该作业中选择任务， 其
中，选择任务时需要重点考虑数据本地性。 此外，JobTracker 跟踪作业的整个运行过程，并
为作业的成功运行提供全方位的保障。 首先， 当 TaskTracker 或者Task 失败时， 转移计算
任务 ； 其次， 当某个 Task 执行进度远落后于同一作业的其他 Task 时，为之启动一个相同
Task， 并选取计算快的 Task 结果作为最终结果。
3.任务运行环境准备
3. 运行环境准备包括 JVM 启动和资源隔 离， 均由TaskTracker 实现。 TaskTracker 为每个
Task 启动一个独立的 JVM 以避免不同 Task 在运行过程中相互影响 ； 同时，TaskTracker 使
用了操作系统进程实现资源隔离以防止 Task 滥用资源。
4.任务执行
4. TaskTracker 为 Task 准备好运行环境后， 便会启动 Task。 在运行过程中， 每个 Task 的最
新进度首先由 Task 通过 RPC 汇报给 TaskTracker， 再由 TaskTracker汇报给 JobTracker。
5.作业完成。
5. 待所有 Task 执行完毕后， 整个作业执行成功。
13/04/2018 Page 262 of 283
26. Spark
26.1.1. 概念
Spark提供了一个全面、统一的框架用于管理各种有着不同性质（文本数据、图表数据等）的数据
集和数据源（批量数据或实时的流数据）的大数据处理的需求。
26.1.2. 核心架构
Spark Core
包含Spark的基本功能；尤其是定义RDD的API、操作以及这两者上的动作。其他Spark的库都
是构建在RDD和Spark Core之上的
Spark SQL
提供通过Apache Hive的SQL变体Hive查询语言（HiveQL）与Spark进行交互的API。每个
数据库表被当做一个RDD，Spark SQL查询被转换为Spark操作。
Spark Streaming
对实时数据流进行处理和控制。Spark Streaming允许程序能够像普通RDD一样处理实时数据
Mllib
一个常用机器学习算法库，算法被实现为对RDD的Spark操作。这个库包含可扩展的学习算法，
比如分类、回归等需要对大量数据集进行迭代的操作。
GraphX
控制图、并行图操作和计算的一组算法和工具的集合。GraphX扩展了RDD API，包含控制图、
创建子图、访问路径上所有顶点的操作
13/04/2018 Page 263 of 283
26.1.3. 核心组件
Cluster Manager-制整个集群，监控worker
在 standalone 模式中即为 Master 主节点，控制整个集群，监控 worker。在 YARN 模式中为资
源管理器
Worker节点-负责控制计算节点
从节点，负责控制计算节点，启动Executor或者Driver。
Driver： 运行Application 的main()函数
Executor：执行器，是为某个Application运行在worker node上的一个进程
26.1.4. SPARK编程模型
Spark 应用程序从编写到提交、执行、输出的整个过程如图所示，图中描述的步骤如下：
13/04/2018 Page 264 of 283
1. 用户使用SparkContext提供的API（常用的有textFile、sequenceFile、runJob、stop等）
编写 Driver application 程序。此外 SQLContext、HiveContext 及 StreamingContext 对
SparkContext进行封装，并提供了SQL、Hive及流式计算相关的API。
2. 使用SparkContext提交的用户应用程序，首先会使用BlockManager和BroadcastManager
将任务的Hadoop配置进行广播。然后由DAGScheduler将任务转换为RDD并组织成DAG，
DAG 还将被划分为不同的 Stage。最后由 TaskScheduler 借助 ActorSystem 将任务提交给
集群管理器（Cluster Manager）。
3. 集群管理器（ClusterManager）给任务分配资源，即将具体任务分配到Worker上，Worker
创建Executor来处理任务的运行。Standalone、YARN、Mesos、EC2等都可以作为Spark
的集群管理器。
26.1.5. SPARK计算模型
RDD 可以看做是对各种数据计算模型的统一抽象，Spark 的计算过程主要是 RDD 的迭代计算过
程。RDD的迭代计算过程非常类似于管道。分区数量取决于partition数量的设定，每个分区的数
据只会在一个Task中计算。所有分区可以在多个机器节点的Executor上并行执行。
13/04/2018 Page 265 of 283
26.1.6. SPARK运行流程
13/04/2018 Page 266 of 283
1. 构建Spark Application的运行环境，启动SparkContext
2. SparkContext 向资源管理器（可以是Standalone，Mesos，Yarn）申请运行Executor 资源，
并启动StandaloneExecutorbackend，
3. Executor向SparkContext申请Task
4. SparkContext将应用程序分发给Executor
5. SparkContext构建成DAG图，将DAG图分解成Stage、将Taskset发送给Task Scheduler，
最后由Task Scheduler将Task发送给Executor运行
6. Task在Executor上运行，运行完释放所有资源
26.1.7. SPARK RDD流程
1. 创建RDD对象
2. DAGScheduler 模块介入运算，计算 RDD 之间的依赖关系，RDD 之间的依赖关系就形成了
DAG
3. 每一个 Job 被分为多个 Stage。划分 Stage 的一个主要依据是当前计算因子的输入是否是确
定的，如果是则将其分在同一个Stage，避免多个Stage之间的消息传递开销
26.1.8. SPARK RDD
（1）RDD的创建方式
1）从Hadoop文件系统（或与Hadoop兼容的其他持久化存储系统，如Hive、Cassandra、
HBase）输入（例如HDFS）创建。
2）从父RDD转换得到新RDD。
13/04/2018 Page 267 of 283
3）通过parallelize或makeRDD将单机数据创建为分布式RDD。
（2）RDD的两种操作算子（转换（Transformation）与行动（Action））
对于RDD可以有两种操作算子：转换（Transformation）与行动（Action）。
1） 转换（Transformation）：Transformation操作是延迟计算的，也就是说从一个RDD转
换生成另一个RDD的转换操作不是马上执行，需要等到有Action操作的时候才会真正触
发运算。
2）行动（Action）：Action算子会触发Spark提交作业（Job），并将数据输出Spark系统。
13/04/2018 Page 268 of 283
27. Storm
27.1.1. 概念
Storm 是一个免费并开源的分布式实时计算系统。利用 Storm 可以很容易做到可靠地处理无限的
数据流，像Hadoop批量处理大数据一样，Storm可以实时处理数据。
27.1.1. 集群架构
27.1.1.1. Nimbus（master-代码分发给Supervisor）
Storm 集群的 Master 节点，负责分发用户代码，指派给具体的 Supervisor 节点上的 Worker 节
点，去运行Topology对应的组件（Spout/Bolt）的Task。
27.1.1.2. Supervisor（slave-管理Worker进程的启动和终止）
Storm 集群的从节点，负责管理运行在 Supervisor 节点上的每一个 Worker 进程的启动和终止。
通过 Storm 的配置文件中的 supervisor.slots.ports 配置项，可以指定在一个 Supervisor 上最大
允许多少个 Slot，每个 Slot 通过端口号来唯一标识，一个端口号对应一个 Worker 进程（如果该
Worker进程被启动）。
27.1.1.3. Worker（具体处理组件逻辑的进程）
运行具体处理组件逻辑的进程。Worker 运行的任务类型只有两种，一种是 Spout 任务，一种是
Bolt任务。
13/04/2018 Page 269 of 283
27.1.1.4. Task
worker中每一个spout/bolt的线程称为一个task. 在storm0.8之后，task不再与物理线程对应，
不同spout/bolt的task可能会共享一个物理线程，该线程称为executor。
27.1.1.5. ZooKeeper
用来协调 Nimbus 和 Supervisor，如果 Supervisor 因故障出现问题而无法运行 Topology，
Nimbus会第一时间感知到，并重新分配Topology到其它可用的Supervisor上运行
27.1.2. 编程模型（spout->tuple->bolt）
strom在运行中可分为spout与bolt两个组件，其中，数据源从spout开始，数据以tuple的方
式发送到bolt，多个bolt可以串连起来，一个bolt也可以接入多个spot/bolt.运行时原理如下图：
27.1.2.1. Topology
Storm中运行的一个实时应用程序的名称。将 Spout、 Bolt整合起来的拓扑图。定义了 Spout和
Bolt的结合关系、并发数量、配置等等。
27.1.2.2. Spout
在一个topology中获取源数据流的组件。通常情况下spout会从外部数据源中读取数据，然后转
换为topology内部的源数据。
27.1.2.3. Bolt
接受数据然后执行处理的组件,用户可以在其中执行自己想要的操作。
27.1.2.4. Tuple
一次消息传递的基本单元，理解为一组消息就是一个Tuple。
13/04/2018 Page 270 of 283
27.1.2.5. Stream
Tuple的集合。表示数据的流向。
27.1.3. Topology运行
在Storm中,一个实时应用的计算任务被打包作为Topology发布，这同Hadoop MapReduce
任务相似。但是有一点不同的是:在Hadoop中，MapReduce任务最终会执行完成后结束；而在
Storm中，Topology任务一旦提交后永远不会结束，除非你显示去停止任务。计算任务
Topology是由不同的Spouts和Bolts，通过数据流（Stream）连接起来的图｡一个Storm在集
群上运行一个Topology时，主要通过以下3个实体来完成Topology的执行工作：
(1). Worker（进程）
(2). Executor（线程）
(3). Task
27.1.3.1. Worker(1个worker进程执行的是1个topology的子集)
1个worker进程执行的是1个topology的子集（注：不会出现1个worker为多个topology
服务）。1个worker进程会启动1个或多个executor线程来执行1个topology的
component(spout或bolt)。因此，1个运行中的topology就是由集群中多台物理机上的多个
worker进程组成的。
27.1.3.2. Executor(executor是1个被worker进程启动的单独线程)
executor是1个被worker进程启动的单独线程。每个executor只会运行1个topology的1个
component(spout或bolt)的task（注：task可以是1个或多个，storm默认是1个
component只生成1个task，executor线程里会在每次循环里顺序调用所有task实例）。
13/04/2018 Page 271 of 283
27.1.3.3. Task(最终运行spout或bolt中代码的单元)
是最终运行spout或bolt中代码的单元（注：1个task即为spout或bolt的1个实例，
executor线程在执行期间会调用该task的nextTuple或execute方法）。topology启动后，1
个component(spout或bolt)的task数目是固定不变的，但该component使用的executor线
程数可以动态调整（例如：1个executor线程可以执行该component的1个或多个task实
例）。这意味着，对于1个component存在这样的条件：#threads<=#tasks（即：线程数小于
等于task数目）。默认情况下task的数目等于executor线程数目，即1个executor线程只运