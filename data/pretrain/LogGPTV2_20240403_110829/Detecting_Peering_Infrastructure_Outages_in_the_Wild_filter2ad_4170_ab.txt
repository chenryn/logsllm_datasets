independent of specific providers [54, 70].
Colocation facilities offer the hosting of servers and network
equipment to facilitate networks’ interconnections, typically via
cross-connects or Private Network Interconnects (PNI), i.e., a point-
to-point circuit [12]. Facilities are mainly concentrated in metro-
politan areas, with major telecommunication hubs like London and
New York hosting dozens of facilities [50]. While it is common prac-
tice among facility operators not to publish the number of PNIs,
there are indications that their number is continuously growing.
Equinix reports more than 188K cross-connects over its 145 facil-
ities (Q3/2016) [37]. Moreover, high-profile acquisitions suggest
a highly dynamic sector, including the acquisition of Telecity by
Equinix for $3.8 Billion [36], and Telx by Digital Reality for $1.9
Billion [97]. Interconnection paradigms such as remote peering and
tethering are increasingly deployed, allowing networks in remote
sites of the same facility to exchange traffic directly [77].
An IXP is a physical infrastructure composed of layer-2 Eth-
ernet switches which interconnect edge routers of members [18].
Once a physical connection is established, ASes can chose between
different flavors of peering: (i) bilateral public peering, (ii) bilat-
eral private peering via a virtual local network, similar to PNIs in
colocation facilities, (iii) multilateral public peering over IXP route
servers [52, 89], or (iv) remote peering with the members of affiliated
IXPs [16]. Today, there are more than 300 IXPs in the world [81],
particularly in Europe, but their popularity also increases in other
regions, including the USA [61], Latin America [11], and Africa [40].
The number of members varies from tens to multiple hundreds, e.g.,
DE-CIX Frankfurt and AMS-IX Amsterdam have over 700 mem-
bers [2, 28]. Moreover, IXPs are not just local interconnection points
but they are becoming international hubs, through the use of layer-
2 carriers and Virtual PoPs (vPoPs). For instance, LINX London
interconnects networks from more than 72 countries [65, 66]. It is
also increasingly popular for IXPs to form conglomerates by inter-
connecting with each other [45], while distributed IXPs, such as
NL-IX, interconnect their remote sites to offer virtual backbone and
remote access to their network members. Studies show that IXPs
enable hundreds of thousands of peerings [1], the large majority
being multi-lateral peerings [52, 89]. Traffic exchanged at IXPs has
increased significantly in recent years [18], exceeding 5 Tbps at
large IXPs.
With the advent of Content Distribution Networks (CDNs) and
the placement of data caches close to the users, the interconnection
landscape has become increasingly clustered in large metropolitan
hubs [50, 70]. The geographic agglomeration of the peering activity
has led to an increasingly symbiotic relationship between IXPs and
colocation facilities: IXPs benefit from placing their switches in
locations where ISPs can easily install their network equipment,
while facility operators often subsidize the presence of IXPs in
their space to increase the attractiveness of their colocation ecosys-
tem [12, 78]. These mutual interconnection incentives create tight
physical interdependencies between IXPs and facilities. For exam-
ple, DE-CIX has distributed its peering fabric among 12 different
facilities in the greater Frankfurt metropolitan area [29], while the
Equinix Frankfurt KleyerStrasse (FR5) colocation facility hosts 10
different IXPs [81].
3 METHODOLOGY
In this section, we describe our methodology for detecting and
localizing peering infrastructure outages.
3.1 Challenges and Concept
Recall that the main purpose of BGP is to provide reachability in-
formation and not connectivity information [92]. Thus, relying on
the BGP path or the AS-level topology of the Internet is not suffi-
cient to detect the physical location of a peering, and the location
of the underlay interconnection infrastructure. To illustrate the
challenges in detecting and pinpointing the exact physical location
of a peering outage consider the topology of Figure 2. It consists of
four ASes (ASi), four colocation facilities (Fj), and two IXPs (IXk).
Figure 2(b) and 2(c) are the results of two different outages, at colo-
cation facility F2 and at IXP IX1, respectively. Initially, AS1 reaches
AS2 via private peering over facility F2; AS2 reaches AS4 via public
peering over the IXP IX1; and AS3 reaches AS4 via IX1. Note, some
paths involve multiple facilities, e.g., from AS2 to AS4 via IXP IX1,
F2, and F4, and from AS3 to AS4 via IX1, F3, and F4.
The failure of F2, Figure 2(b), affects both private and public in-
terconnections at this facility. The private ones are affected directly,
the public ones only indirectly since F2 hosts part of IXP IX1’s
switching fabric. In our example, two paths change: AS1 switches
to its backup path via F1, and AS2 switches to its backup path to
AS4 over F4. Note that the AS paths do not change. However, the
involved facilities and IXPs do change. Likewise, the failure of IX1,
(Figure 2(c)), partially affects the paths of F2, F3, and F4, since the
new routes have to bypass IX1. This can cause a large number of
BGP updates. Yet, the AS paths themselves again do not necessarily
change. Both scenarios illustrate the increasing symbiotic relation-
ships between colocation and IXP peering infrastructures. Such
inter-dependencies have already led to confusion when locating
and reporting the cause of outages [3, 87].
Our examples show that it is not sufficient to track AS-level
changes to determine the outage location, we need to monitor
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
V. Giotsas et al.
Figure 2: Examples of how facility-level and IXP-level outages affect the inter-domain paths.
facility-level paths and correlate them across multiple route changes.
In Figure 2(b), the fact that F2 disappears from all paths, while IX1
disappears only from the path through F2, is sufficient to infer that
the outage occurred at F2. Similarly, for Figure 2(c) the outage can
be localized at IX1 and not F1, since the AS1–AS2 path through
the facilities/IXP remains unchanged, while the AS3–AS4 path is
re-routed via IX2 concurrently with a path change from AS2 to AS4.
The example above allows us to derive the following insights
about infrastructure-level outage detection:
Facility-level Inter-domain Hops: The four ASes appear to ex-
change traffic directly when observing only the AS-level paths.
However, the physical paths involve multiple intermediate facility-
level and IXP-level infrastructures that introduce externalities in
the resilience of the AS interconnections. We need to capture these
infrastructures to accurately localize outages.
Path Correlation: To uncover the failure location within the com-
plex infrastructure of today’s Internet, we have to correlate path
changes across multiple vantage points with colocation data at
facilities and IXPs.
Before and After Comparison: To understand the source and
impact of an outage, one needs to compare routes during an outage
to those before the outage—the “healthy” state. Therefore, we need
the ability to continuously monitor the routing system.
A major challenge is how to get sufficiently fine-grained facility
information. A key insight of our approach is that we can extract
facility information per routing update through the analysis of BGP
communities. Moreover, it is feasible to collect detailed facility maps
from various public sources using techniques described in [50, 68],
thanks to the increasing openness in the sharing of colocation data
to support a more flexible peering setup process or even automate
it altogether [7, 63]. Indeed, today the large majority of peerings
are multilateral peerings that do not involve formal contractual
agreements [100].
3.2 BGP Community Dictionary
BGP Communities have the format X:Y, where X, Y are two 16-bit
values (extended communities use four octets [93]). By convention,
the first two octets encode the ASN of the operator that sets the
community, while the next two octets encode an arbitrary value
that is used by the operator to denote specific information such
as the ingress location of a route. There are two types of commu-
nities: (i) inbound communities that are applied when an operator
receives a prefix advertisement at an ingress peering point, and (ii)
outbound communities that are applied when an operator sends a
prefix advertisement at an egress peering point.
The Rise of BGP Communities: Between 2010 and 2016 the
visible number of networks using BGP Communities has more than
doubled from 2, 500 to 5, 500, and the number of unique community
values has tripled to more than 50K in 2016 (Figure 3). Moreover,
the number of Community values per prefix announcement has
increased from an average of 4 to 16. These communities encode a
wealth of routing meta-data, but unfortunately, the community is
possibly the only BGP attribute with no specific semantics and val-
ues that are neither standardized nor have a uniform encoding [33].
Consequently, extracting meaningful information from the commu-
nities is not possible without additional sources of interpretation.
Location-Encoding Ingress Communities: Each operator uses
different values to encode location information at various granu-
larities. For example, in Figure 4 the BGP collector receives routes
for prefixes 184.84.242.0/24 and 2.21.67.0/24 with a common
AS subpath 13030 20940. The first route is tagged with commu-
nity 13030:51904. The value 13030 in the top 16 bits indicates
that AS13030 has applied the community. The value 51904 in the
bottom 16 bits, indicates that this community is used to tag routes
received at the Coresite LAX-1 facility [59]. Similarly, the second
route is tagged with two communities from AS13030. The value
51702 means that the route’s ingress point was the Telehouse East
London facility, and the value 4006 means that the route was re-
ceived by a public peer at the LINX IXP Juniper LAN.
Detecting Peering Infrastructure Outages in the Wild
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
Figure 3: Number of unique BGP Communities values (left
y-axis), compared to unique top two octets.
While the community values are not standardized, many oper-
ators publicly document their community schemes either in their
Internet Routing Registry (IRR) records or in their support Web
pages. However, the documentation is in natural text and lacks a
standardized structure and terminology, therefore its parsing neces-
sitates significant manual work that is unsustainable given the large
number of BGP Communities. To tackle this problem we develop a
web-mining tool that enables the automatic compilation of a com-
munity dictionary. We first use a Web Scraper to extract the text
from the remarks sections of IRR records and from ASes’ web pages.
Then, a text parser analyzes the extracted text using the Natural
Language ToolKit [10] to discover infrastructure-related communi-
ties. We identify sub-strings that include community values using
regular expression matching, on which we use Stanford’s Named
Entity Recognizer (NER) [43] to identify named entities, focusing
on entities that pertain to locations or infrastructure operators.
To improve the accuracy of NER for network-related entities, we
adopt the techniques proposed by Banerjee et al. [5] and we search
PeeringDB [81], Euro-IX [38], and IRR records, for organization
names that match capitalized words encountered in communities
documentation. These sources also enable us to determine the net-
work type of the identified entities. For our community dictionary,
we only keep communities that tag three types of Named Entities:
(i) city-level locations, (ii) IXPs, and (iii) colocation facilities.
Then, using syntactic analysis we filter-out outbound commu-
nities that define location-specific traffic engineering actions. In
particular, we perform Part-of-Speech tagging to distinguish verbs
in passive voice used for documenting inbound communities (e.g.,
“received”, “learned”, “exchanged”), and ones in active voice that
define actions (e.g., “announce”, “block”). Finally, we assign a sin-
gle location identifier to all entities related to a common location.
Different operators use different naming, such as city names (“New
York City”), city initials (“NYC”), or IATA airport codes (“JFK”). To
determine if the different location identifiers refer to the same loca-
tion we query the Google Maps Geocoding API [53] to obtain the
coordinates for each identifier, and we group together identifiers
that are within 10 km from each other.
IXP Path Redistribution Communities: We augment our dic-
tionary with path redistribution communities used by IXP route
servers. IXP route servers often use communities to aid their mem-
bers in controlling how their prefixes are advertised to other route
server members [57], e.g., advertise to all, and advertise to selected
peers. Thus, a route server community on a BGP route indicates that
Figure 4: Inferring physical locations from BGP Communi-
ties.
the route traversed the IXP and the first 16 bits of the community
value indicates the IXP ASN.
Dictionary Statistics: As of December 2016, our community dic-
tionary includes 5,284 communities by 468 ASes and 48 route
servers, and covers 288 cities in 72 countries, 172 IXPs, and 103
facilities. While 468 ASes is a small fraction of the ASes, it includes
all but two Tier-1 ASes and most major peering ASes. Note that for
the two Tier-1 ASes (XO Communications and Verizon) missing
from our dictionary we observed less than 20 different community
values in the public BGP data, which indicates that they either do
not use communities to annotate their PoPs, or they do not prop-
agate such communities outside their domain and do not provide
publicly accessible community documentations. Figure 5 shows
the geographical coverage of locations we extract from the com-
munities. The majority of the communities (66%) tag a location
in Europe, followed by North America (24.5%), while only 2% of
the communities cover locations in Africa and South America. Al-
though the interconnection ecosystem in these regions is indeed
relatively underdeveloped [55, 71], the difference in coverage can
be also explained by biases in the underlay documentation sources,
such as the completeness of the different Internet Routing Reg-
istries [6], and the fact that our natural language parser works only
with English text. As we elaborate in Section 5.2, location BGP
Communities included in our dictionary are present in about half
of all BGP IPv4 updates. To ensure freshness we recompute our
dictionary every two weeks and always use the dictionary from
the corresponding time period for route processing. To validate the
correctness of our automatically-generated community dictionary,
we compared it against a manually-constructed dictionary. Due to
the overhead of manually parsing community documentations, we
limited the validation to the 25 ASes in our dictionary with the
highest number of BGP paths annotated. We did neither find a false