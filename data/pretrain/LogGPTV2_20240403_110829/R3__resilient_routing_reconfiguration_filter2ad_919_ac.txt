much lower MLU than existing methods.
Necessary condition for resilient routing: A potential concern on
R3 is that it may be rather wasteful, as it enforces MLU to be within
1 when routing both real trafﬁc and rerouting virtual demand up to
the link capacity. However, it is more economical than it seems.
In particular, Theorem 2 shows that the requirement in Theorem 1
is tight for single-link failures (i.e., F = 1). Our evaluation will
further show that it is efﬁcient under general failure scenarios.
L(d, x, r, p, e) = L(d, r, e) + xepe(e)
= L(d, r, e) + ce“1 − L(d,r,e)
ce
” = ce.
Therefore, the maximum MLU is no larger than 1 under routing (r,
p) over the entire demand set d + X1.
The case with multiple link failures is more challenging. As a
special case, we consider a network with two nodes and multiple
parallel links connecting them (e.g., the network shown in Fig-
ure 1). We have the following proposition:
PROPOSITION 1. For a network with two nodes connected by
parallel links, R3 protection routing and reconﬁguration produce
an optimal protection routing under any number of failed links.
We leave the tightness of R3 in a general network topology with
multiple link failures as an open problem.
Order independent online reconﬁguration: In case multiple link
failures occur close in time, it is possible that different routers may
detect these failures in different orders. Theorem 3 ensures that
the online reconﬁguration procedure in Section 3.2 will eventually
result in the same routing as long as different routers eventually
discover the same set of failed links.
In other words, the order
in which the failures are detected does not affect the ﬁnal rout-
ing. This is useful because different routers can then apply R3 in a
distributed, independent fashion without requiring any central con-
troller to synchronize their routing states.
THEOREM 3. The online reconﬁguration procedure is order in-
dependent. That is, any permutation of a failure sequence e1, e2,
· · · , en always yields the same routing after online reconﬁguration
has been performed for all links in the permuted failure sequence.
PROOF. Omitted in the interest of brevity.
3.5 Extensions
Handling trafﬁc variations: So far we consider a ﬁxed trafﬁc ma-
trix d. In practice, trafﬁc varies over time. To accommodate such
variations, a trafﬁc engineering system typically collects a set of
trafﬁc matrices {d1, · · · , dH } and uses their convex combination
295to cover the space of common trafﬁc patterns (e.g., see [31, 44,
39]). That is, we replace a ﬁxed trafﬁc matrix d with the convex
hull of {d1, · · · , dH }:
D △=nd | d =PH
h=1 thdh,PH
Constraint [C2] in (3) then becomes:
h=1 th = 1, th ≥ 0 (∀h)o .
∀d ∈ D, ∀x ∈ XF , ∀e ∈ E :
Pa,b∈V dabrab(e)+Pℓ∈E xℓpℓ(e)
ce
≤ M LU.
(17)
As in Section 3.1, we can apply linear programming duality to con-
vert (17) into a set of linear constraints.
Handling realistic failure scenarios: We have considered arbi-
trary K link failures. Next we take into account of potential struc-
tures in realistic failure scenarios and classify failure events into the
following two classes:
• Shared risk link group (SRLG). A SRLG consists of a set of links
that are disconnected simultaneously. For example, due to shar-
ing of lower layer physical components (e.g., optical switch),
multiple IP layer links may always fail together. Another ex-
ample is the high-bandwidth composite links, in which a single
member link down will cause all links in the composite link to
be shut down. Let FSRLG be the set consisting of all SRLGs.
Each element in FSRLG consists of a set of links.
• Maintenance link group (MLG). A network operator may shut
down a set of links in the same maintenance operation. Let
FM LG be the set consisting of all MLG events. Each element
FM LG consists of a set of links.
To capture these failure characteristics, we introduce an indicator
variable If , where If = 1 if and only if the basic event set f is
down. Then (5) is changed to (18), where the ﬁrst constraint limits
the maximum number of concurrent SRLGs, the second constraint
expresses the fact that maintenance is carefully scheduled so that
at most one MLG undergoes maintenance at any instance of time,
and the last constraint encodes the fact that the rerouting trafﬁc for
a link is upperbounded by whether the link belongs to any SRLG or
MLG. We can then apply linear programming duality in a similar
way to compute resilient routing.
subject to :
If ≤ K;
If ≤ 1;
maximizexPl∈E pl(e)xl
Pf ∈FSRLG
8>>>>>:
∀e ∈ E : xe
ce
∀e ∈ E : xe
ce
≤ Pf ∈FSRLG : e∈f
≤ 1;
(18)
If .
If + Pf ∈FMLG :e∈f
Supporting prioritized resilient routing: So far, we consider that
all trafﬁc requires equal protection. Operational networks increas-
ingly provide different levels of SLAs to different classes of trafﬁc.
For example, some trafﬁc has a more stringent SLA requirement
and needs to tolerate more overlapping link failures. Given an SLA
requirement, we can translate it into the number of overlapping link
failures to tolerate. We extend R3 to support prioritized resilient
routing by associating each class of trafﬁc with a protection level.
Let Fi be the number of link failures that trafﬁc with protection
level i should tolerate. Let di be the total trafﬁc demands that re-
quire protection level i or higher. Let XFi be the rerouting virtual
demand set with up to Fi failures. Then our goal is to ﬁnd (r, p)
such that for any i, the network has no congestion for the entire de-
mand set di + XFi . To achieve this goal, we simply replace [C2] in
(3) with (19), which can again be converted into linear constraints
by applying linear programming duality:
∀i, ∀xi ∈ XFi , ∀e ∈ E :
Pa,b∈V di
abrab(e)+Pl∈E xi
l pl(e)
ce
≤ M LU.
(19)
As an example, consider a network with three trafﬁc protection
classes: TPRT dF for real-time IP, TPP dP for private transport,
and general IP dI , with decreasing protection levels: TPRT should
be protected against up to three link failures, TPP up to two link
failures, and IP any single-link failure scenarios. Then the algo-
rithm computes d1 = dF + dP + dI , d2 = dF + dP , d3 = dF , and
sets Fi = i, for i = 1, 2, 3. This essentially means that resilient
routing should carry d1 + X1, d2 + X2, and d3 + X3, where Xi
denotes the rerouting virtual demand set with up to i link failures.
Trade-off between performance under normal condition and
failures: A potential concern about optimizing performance for
failures is that good performance after failures may come at the ex-
pense of poor performance when there are no failures. To address
this issue, we can bound M LU under no failures to be close to
the optimal. This can be achieved by adding additional constraints,
which we call penalty envelope, to the previous optimization prob-
lem: Pa,b∈V dabrab(e)/ce ≤ M LUopt × β, where M LUopt is
MLU under optimal routing and β ≥ 1 is an operator-speciﬁed in-
put that controls how far the normal-case performance is away from
the optimal. With these constraints, we not only optimize perfor-
mance under failures but also ensure acceptable performance under
normal conditions. β is a tunable parameter. A small β improves
the normal-case performance at the cost of degrading the perfor-
mance after failures by reducing the feasible solution space over
which the optimization takes place.
Trade-off between network utilization and delay: Similarly, we
can use a delay penalty envelope γ to bound the average end-to-
end delay under no failures for any OD pair. Note that the average
delay of a link is usually dominated by the propagation delay when
the link utilization is not too high. Let P De denote the propa-
gation delay of link e. Then the delay penalty envelope for OD
pair a → b can be achieved by adding the following constraint:
ab is the smallest end-
to-end propagation delay from a to b. This enforces that the average
delay under R3 is no more than γ times that of the optimal delay.
Pe∈E P Derab(e) ≤ P D∗
ab × γ, where P D∗
4. R3 LINUX IMPLEMENTATION
To evaluate the feasibility and effectiveness of R3 in real settings,
we implement R3 in Linux (kernel version 2.6.25). In this section,
we describe the R3 implementation.
4.1 Overview
A key challenge in implementing R3 protection routing is its
ﬂow-based representation of p, because current routers do not read-
ily support such a routing scheme.
One way to address the issue is to convert a ﬂow-based routing
to a path-based routing (e.g., using the ﬂow decomposition tech-
nique [40]). A path-based routing can then be implemented using
MPLS. A problem of this approach, however, is that after each fail-
ure the protection routing should be rescaled and the rescaled pro-
tection routing may decompose to new sets of paths, which have to
be signaled and setup.
To address this problem, we design a more efﬁcient implemen-
tation. We choose MPLS as the base mechanism since it is widely
supported by all major router vendors. We implement a ﬂow-based
routing using MPLS, called MPLS-ff. MPLS-ff involves a simple
modiﬁcation to MPLS and can be easily implemented by router
vendors. For wider interoperability, R3 can also be implemented
using traditional MPLS, but with larger overhead.
4.2 MPLS-ff
Forwarding data structure: We use Linux MPLS to illustrate
our implementation. When an MPLS packet with label l arrives at
a router, the router looks up the label l in a table named incoming
label mapping (ILM), which maps the label to a forward (FWD)
instruction. The FWD contains a next-hop label forwarding entry
(NHLFE), which speciﬁes the outgoing interface for packets with
the incoming label.
296MPLS-ff extends MPLS forwarding information base (FIB) data
structure to allow multiple NHLFE entries in a FWD instruction.
Furthermore, each NHLFE has a next-hop splitting ratio. Thus,
after looking up the label of an incoming packet in ILM, the router
selects one of the NHLFE entries contained in the FWD according
to the splitting ratios.
pab(i,j)
Implementing next-hop splitting ratios: Consider the implemen-
tation of the protection routing for link (a, b). Let lab be the label
representing (a, b) at router i. For all trafﬁc at with label lab, router
i should split the trafﬁc so that the fraction of trafﬁc to neighbor j
is
Pj′ ,(i,j′)∈E,(i,j′)6=(a,b) pab(i,j′) .
One straightforward approach of implementing splitting is ran-
dom splitting. However, this may cause packets of the same TCP
ﬂow to follow different routes, which will generate out-of-order
packets and degrade TCP performance. To avoid unnecessary packet
reordering, packets belonging to the same TCP ﬂow should be routed
consistently. This is achieved using hashing. The hash function
should satisfy two requirements:
• The hash of the packets belonging to the same ﬂow should be
equal at the same router.
• The hash of a ﬂow at different routers should be independent
of each other (i.e., the input to the hash should include router
ID in addition to ﬂow identiﬁcation ﬁelds). If the hash value is
only determined by the ﬂow, the probability distribution of the
hash values might be “skewed” on some routers. For example,
for ﬂow ab, if router i only forwards the packets with hash val-
ues between 40 and 64 to router j, then router j may never see
packets in ﬂow ab with hash values less than 40.
To meet these two requirements, we use a hash function that
takes as input both the ﬂow ﬁelds in the packet header (Source IP
Address, Destination IP Address, Source Port, Destination Port)
and a 96-bit router-dependent private number based on router ID.
The output of the hash function is a 6-bit integer. To further im-
prove the granularity of splitting, additional techniques, such as
FLARE [19], could also be used.
4.3 Routing Reconﬁguration with MPLS-ff
and Label Stacking
With MPLS-ff support, we implement resilient routing reconﬁg-
uration. In our implementation, a central server performs precom-
putation of protection routing p, establishes a label for each pro-
tected link, signals of MPLS-ff setup, and distributes p. The cen-
tral server can be integrated with Routing Control Platform [11] or
Path Computation Element (PCE) [10]. Online reconﬁguration is
distributed, and conducted by each router locally. It has three com-
ponents: failure detection and notiﬁcation, failure response, and
protection routing update. Below we go over each component.
Failure detection and notiﬁcation: We detect link failure using
layer 2 interface monitoring. Upon a local failure event, a notiﬁ-
cation is generated and ﬂooded to all other routers in the network
through ICMP packets with type 42. In operational networks, fail-
ure detection and notiﬁcation can be made more efﬁcient using the
deployed network management infrastructure. For example, SRLG
failure can be detected by a risk modeling algorithm based on net-
work monitoring [25]. The detection could be conservative (e.g.,
if any link in a SRLG is down, assume all links in the SRLG are
down). Also, the operator can issue preparation notiﬁcations to all
routers before starting a MLG maintenance operation.
Failure response: After a failure is detected, MPLS-ff for the
detected failure is activated by label stacking.
Figure 2 is a simple example illustrating the failure response.
An IP packet of ﬂow (S1,D1) reaches router R1. R1 looks up the
packet using the base forwarding table and decides that the next-
hop for this packet is R2. Normally, the packet follows the base
routing and is sent to R2.
Figure 2: Failure response example: (a) normal - R1 routes
ﬂows to R3 through R2; (b) link (R1,R2) fails - R4 and R5 car-
rying protection trafﬁc by label stacking.
If link (R1,R2) fails, R1 activates the protection routing for (R1,R2),
looks up the protection label 200 for link (R1,R2) in ILM, and
pushes label 200 onto the MPLS stack of the packet. The lookup
in ILM indicates that the next-hop neighbor is R4, so R1 forwards
the packet to R4. When the packet reaches router R4, R4 looks up
the ILM for the incoming label 200. For the protection label 200,
R4 has two NHLFEs: 40% of the ﬂows to R2, and 60% to R5. For
simplicity, we assume that the outgoing labels of the two NHLFEs
are still 200; in practice the signaling may assign different labels.
Assume that the hash of ﬂow (S1,D1) on R4 selects R2, then R4
forwards the packet to R2. Similarly, protection trafﬁc for ﬂow
(S2,D2) through R4 can be carried by R5. At R2, the protection
label of the packet will be popped. The packet will be forwarded
to R3 following the remaining base routing of OD pair (R1,R3).
When the network recovers from a failure event, the base routing is
immediately re-activated and the protection routing is disabled.
Protection routing update: After a failure, each router needs to
update the protection routing (i.e., reconﬁguring next-hop splitting
ratios) for other protected links. To facilitate local update, each
router stores p in its RIB (routing information base). The stor-
age requirement is O(|E|2). Considering that backbone routers
already maintain the network topology information (e.g., in Link
State Database), this additional storage overhead is acceptable.
Due to the order independence of rescaling, when multiple fail-
ures happen, different routers can perform rescaling on their local
copies of p. When all routers are notiﬁed of all failures, the routers
will have a consistent protection routing p. During the transition
process, different routers may have inconsistent p, which may lead
to transient loops. If transient loops are of concern, techniques such
as failure-carry packets (FCP) [26] can be integrated with R3.
5. EVALUATIONS
We now evaluate R3 using both real experiments and extensive
simulations based on realistic network topologies and trafﬁc traces.
5.1 Evaluation Methodology
Network topology: For simulations, we use the PoP-level topol-
ogy of a large tier-1 ISP network, called US-ISP. In addition, we
use PoP-level topologies of three IP networks, Level-3, SBC, and
UUNet (2003), as inferred by Rocketfuel [35]. We recursively
merge the leaf nodes of the topologies with their parents until no
nodes have degree one so that we have the backbone of the net-
works. We use OC192 as the capacity for links in the Rocketfuel
topologies. We also generate a large backbone topology using GT-
ITM. For our experimental results, we create the Abilene backbone
topology (2006) on Emulab [41]. We scale down the link capacities
297Network
Abilene
Level-3
SBC
UUNet
Generated
US-ISP
Aggregation level
# Nodes
# D-Links
router-level
PoP-level
PoP-level
PoP-level
router-level
PoP-level
11
17
19
47
100
-
28
72
70
336
460
-
Table 1: Summary of network topologies used.