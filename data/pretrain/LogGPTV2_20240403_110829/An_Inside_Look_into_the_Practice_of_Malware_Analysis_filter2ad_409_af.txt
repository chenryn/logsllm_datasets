Applications Conference (ACSAC), pages 69–78, 2013.
[67] F. Peng, Z. Deng, X. Zhang, D. Xu, Z. Lin, and Z. Su. X-force: Force-executing
binary programs for security applications. In Proceedings of the 23rd USENIX
Security Symposium (Security), pages 829–844, San Diego, CA, Aug. 2014.
[68] K. Rieck, T. Holz, C. Willems, P. Düssel, and P. Laskov. Learning and classification
of malware behavior. In Proceedings of the 5th Conference on Detection of Intrusions
and Malware and Vulnerability Assessment (DIMVA), pages 108–125, 2008.
[69] P. Royal, M. Halpin, D. Dagon, R. Edmonds, and W. Lee. Polyunpack: Automating
the hidden-code extraction of unpack-executing malware. In Proceedings of the
22nd Annual Computer Security Applications Conference (ACSAC), pages 289–300,
2006.
[70] E. J. Schwartz, T. Avgerinos, and D. Brumley. All you ever wanted to know about
dynamic taint analysis and forward symbolic execution (but might have been
afraid to ask). In 2010 IEEE symposium on Security and privacy, pages 317–331,
Oakland, CA, May 2010.
[71] C. Spensky, H. Hu, and K. Leach. Lo-phi: Low-observable physical host instru-
mentation for malware analysis. In Proceedings of the 23rd Annual Network and
Distributed System Security Symposium (NDSS), San Diego, CA, Feb. 2016.
[72] P. Srivastava and N. Hopwood. A practical iterative framework for qualitative
data analysis. International journal of qualitative methods, 8(1):76–84, 2009.
[73] F. Tegeler, X. Fu, G. Vigna, and C. Kruegel. Botfinder: Finding bots in network
traffic without deep packet inspection. In Proceedings of the 8th international
conference on Emerging networking experiments and technologies, pages 349–360,
2012.
[74] X. Ugarte-Pedrero, D. Balzarotti, I. Santos, and P. G. Bringas. Rambo: Run-time
packer analysis with multiple branch observation. In Proceedings of the 13th
Conference on Detection of Intrusions and Malware and Vulnerability Assessment
(DIMVA), pages 186–206, 2017.
[75] D. Votipka, R. Stevens, E. Redmiles, J. Hu, and M. Mazurek. Hackers vs. testers:
A comparison of software vulnerability discovery processes. In Proceedings of
the 39th IEEE Symposium on Security and Privacy (Oakland), pages 374–391, San
Jose, CA, May 2018.
[76] D. Votipka, S. Rabin, K. Micinski, J. S. Foster, and M. L. Mazurek. An observa-
tional investigation of reverse engineers’ processes. In 29th {USENIX} Security
Symposium ({USENIX} Security 20), pages 1875–1892, 2020.
[77] S. Wang and D. Wu. In-memory fuzzing for binary code similarity analysis. In
2017 32nd IEEE/ACM International Conference on Automated Software Engineering
(ASE), pages 319–330. IEEE, 2017.
[78] M. Xu and T. Kim. Platpal: Detecting malicious documents with platform diversity.
In Proceedings of the 25th USENIX Security Symposium (Security), pages 271–287,
Vancouver, BC, Canada, Aug. 2017.
[79] Z. Xu, J. Zhang, G. Gu, and Z. Lin. Goldeneye: Efficiently and effectively unveiling
malware’s targeted environment. In International Workshop on Recent Advances
in Intrusion Detection, pages 22–45, 2014.
[80] B. Yadegari and S. Debray. Symbolic execution of obfuscated code. In Proceedings
of the 22nd ACM SIGSAC Conference on Computer and Communications Security,
pages 732–744, Denver, Colorado, Oct. 2015.
[81] L.-K. Yan, M. Jayachandra, M. Zhang, and H. Yin. V2e: combining hardware
virtualization and softwareemulation for transparent and extensible malware
analysis. In Proceedings of the 8th ACM SIGPLAN/SIGOPS conference on Virtual
Execution Environments, pages 227–238, 2012.
Session 11C: Software Development and Analysis CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3066[82] T.-F. Yen and M. K. Reiter. Traffic aggregation for malware detection. In Proceed-
ings of the 5th Conference on Detection of Intrusions and Malware and Vulnerability
Assessment (DIMVA), pages 207–227, 2008.
[83] F. Zhang, K. Leach, K. Sun, and A. Stavrou. Spectre: A dependable introspection
framework via system management mode. In Proceedings of the International
Conference on Dependable Systems and Networks (DSN), pages 1–12, 2013.
[84] F. Zhang, K. Leach, A. Stavrou, H. Wang, and K. Sun. Using hardware features
for increased debugging transparency. In Proceedings of the 36th IEEE Symposium
on Security and Privacy (Oakland), pages 55–69, San Jose, CA, May 2015.
ACKNOWLEDGMENTS
We thank Alex Bardas and the reviewers for their helpful feedback,
Daniel Votipka for recruitment assistance and our participants for
providing valuable insights.
The second author’s work is supported by the National Science
Foundation Graduate Research Fellowship under Grant No. DGE-
2039655. Any opinion, findings, and conclusions or recommenda-
tions expressed in this material are those of the authors(s) and do
not necessarily reflect the views of the National Science Founda-
tion.
APPENDIX
A Survey Questionnaire
Background and Experience.
• Which of the following definitions best describes you? (Please
select all that apply) Malware engineer: I work on configur-
ing the malware sandbox analyzers and/or develop programs
to process the inputs and outputs of these analyzers. Ex: pri-
oritizing, clustering, parsing, preventing evasion techniques,
capturing system calls. Malware analyst: I analyze malware
samples to understand their functionality, potential impact
and origin.
• [If malware engineer] How many years have you worked as
• [If malware analyst] How many years have you worked as a
• Please select your most highly qualified skills or specialty
areas (please check all that apply): Operating Systems, Net-
works, Compilers, Computer Architecture, Programming,
Cryptography, Virtualization, Scalability, Data Analysis, Sand-
boxing, Reverse Engineering, Assembly code, Signature cre-
ation
a malware engineer?
malware analyst?
Job Description.
• What is your current or most recent job title?
• What is the end goal of your threat/malware analysis work
given by your employer ? (please check all that apply) Attri-
bution, Forensics, Recovery remediation, Detection, Classi-
fication, Signature creation, Indication of Compromise, Re-
search, N/A, Other
• Please tell me the primary activities you perform day-to-day
• How often do you use the following malware analysis tech-
• Is your job somehow associated with malware sandboxing?
If so, how ? (Please check all that apply) Collecting malware
specimens for the sandbox, Prioritizing the malware speci-
mens that get sent to the sandbox, Configuring the malware
niques? Dynamic Analysis, Static Analysis, Other
as part of your job.
sandbox, Categorizing, clustering the malware specimens
from sandbox output (usually with ML feature selection),
Manually analyzing the output of malware sandbox, Provid-
ing feedback to update the sandbox
Demographics.
60-69, >70, Prefer not to answer
Woman, Man, Non-binary, Prefer not to answer
• Please specify the gender with which you mostly identify.
• Please specify your age range. 18-29, 30-39, 40-49, 50-59,
• Please specify your ethnicity. American Indian or Alaska
Native, Asian, Black or African American, Hispanic or Latino,
White, Prefer not to answer.
• Please specify your highest degree level of education. High
school credit, no diploma, High school graduate, diploma or
equivalent, College credit, no diploma, Trade/Technical/Vocational
Training, Associate degree, Bachelor’s degree, Master’s de-
gree, Doctorate degree, Prefer not to answer
• If you did get a degree please specify your major.
• If you are currently employed, please specify the business
sector in which you are currently working for. Technology,
Government, Healthcare, Retail, Construction, Education,
Finance, Arts, Other.
B Interview Questions
Background and Experience.
profession?
• Could you tell me a little bit about what you do in your
• How long would you say you’ve been doing this?
• What got you into this profession? Have you always been
doing this?
Malware Sources.
• Great, so these next set of questions will be mainly about
the malware specimens that you receive. Can you walk me
through the steps you have to take to get the malware sam-
ples and how you decide which ones to analyze? Imagine
that you have just started your day at work and are trying
to figure out where to begin.
• How do you receive malware specimens that need to be
• Roughly how many malware specimens do you get per day?
• What types of malware specimens do you usually encounter
analyzed?
(spyware, virus, trojan, keylogger, rootkit, botnets, ransomware,
worms, malvertising...)?
• Do you analyze all of these malware specimens or only a sub-
set of them? How do you decide which malware to analyze?
How do you prioritize which malware to analyze first?
• What data about the malware do you generally have avail-
able before starting your analysis? How do you prioritize
the various data types you look at? Why do you prioritize
them this way? What data do you consider to be the most
important or helpful in doing your analysis?
• Do you determine whether a malware is a variation of a
previously seen malware? If yes, how do you do it?
Session 11C: Software Development and Analysis CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3067• Is there information you wished you had available that isn’t
in the report generated by the malware sandbox analyzer?
• Do you perform the same analysis steps when looking at
an unknown malware versus a variant of a previously seen
malware? If the answer is no, please describe the differences
between the two.
Evolution.
• Considering your past experiences in malware analysis, has
the analysis process changed over time? If so, what parts
have changed? What do you think has been the biggest
factor that contributed to these changes? (Ex: business model,
personal goals, malware type...)
• Is there any other topic that I haven’t touched on which you
would like to elaborate on?
C Follow-up Survey
• Do you or your team install the following settings in your dy-
namic analysis environment? Microsoft office, Web browser,
Adobe acrobat, Software libraries – Do you or your team add
any specific software libraries in the analysis environment?
• Do you or your team configure or modify the following
settings in your dynamic analysis environment? Location,
Time, Languages, Username, File names, Browsing history,
Populate files
• Which of the following settings do you configure per indi-
vidual sample? Location, Time, Languages, Username, File
names, Browsing history, Populate files
D Codebook
The codes used to analyze the participant interviews are presented
in Table 5.
• What percentage of the malware would you say are variants
of malware you have previously seen and what percentage
are previously unseen?
Dynamic Analysis System Configuration.
• Can you walk me through the steps you take to set up your
sandbox environment? Imagine that you were teaching me
to set up a sandbox.
did you choose that hardware?
choose that/those operating system(s)?
Is there a reason why you made that decision?
• How do you set up your sandbox environment?
• Do you use a commercial, open-source or custom sandbox?
• Is your hardware bare-metal, virtualized or emulated? Why
• What type of operating system(s) do you use? Why did you
• How do you decide on an OS configuration? For example,
do you configure features such as location, time zone, users,
groups and services?
• What type of applications do you set up in the user space?
• How do you configure the outgoing network of a sandbox?
• How long do you run your malware specimen for? How did
• Are there any crucial environmental features that you have
found necessary to get malware to reveal its malicious be-
havior?
• Do you execute malware samples multiple times? Why? If
so, do you execute them in a different environment? What
features are different in the different environments that you
use?
you get this set time?
• Can you tell me about how you do malware reports?
• What parts of the sandbox configuration takes up the most
• What parts of the sandbox configuration process do you find
time?
more challenging?
Analysis Workflow.
• Are there any specific tools that you utilize to do malware
analysis? How did you learn to use these tools? How do you
decide which tool and in which order to utilize during your
analysis of a specific malware sample/family?
• Can you walk me through the steps you take to manually
analyze a malware sample? Imagine that you were teaching
me.
• What is the first thing you look for?
• Are there specific indicators that you look for?
• How did you learn/develop this process? +
• Do you have an SOP (Standard Operating Procedure) or
documented procedure on how you approach analyzing a
malware specimen?
• What goal are you trying to achieve by the end of your
• How do you know when you have concluded the analysis
• What parts of the process do you find more challenging?
• Where do you generally seek for help when you run into
analysis?
process?
these challenges?
Session 11C: Software Development and Analysis CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3068High-Level Codes
Runtime: Refers to the amount of time the participants run the malware
sample in the dynamic analysis system.
Number of runs: Refers to the amount of times the participant runs a
malware sample in the dynamic analysis system.
Number of malware samples: Refers to the number of malware samples
that a participant.
Data source: Refers to the source of malware samples that participants
analyze.
Fresh malware samples: Refers to the amount of time since a malware
sample was released.
Prioritization of malware samples: Refers to the process that participants
use to determine the order in which they analyze their malware samples.
Malware variants: Refers to the process that participants use to determine
whether an unknown sample is a variant of a known malware.
Simulated network: Refers to whether the participant simulates the network
when performing dynamic analysis.
Use of open source or commercial sandboxes: Refers to whether
participants use open source and/or commercial dynamic analysis tools.
Preference of open vs commercial sandbox: Refers to the participants
preference between open source dynamic analysis tools and commercial
dynamic analysis tools.
Use of bare metal: Refers to the participants use of bare metal for dynamic
analysis.
Operating system: Refers to whether the participants discussed what
operating system that they use for dynamic malware analysis.
Applications installed in dynamic analysis systems: Refers to the
applications that participants install in the dynamic analysis system.
Environment Settings: Refers to the configuration of the environment
within the dynamic analysis system.
Mimic real user: Refers to whether participants configured their dynamic
analysis environments to appear as if the system was used by a real user.
Monitoring of the dynamic analysis execution: Refers to the process used
to monitor the execution of the malware sample in a dynamic analysis system.
Evasion: Refers to whether participants discussed strategies they use to
analyze evasive malware samples.
Static analysis process and tools: Refers to the participants static analysis
process and tools used.
Generate signatures: Refers to how the participants generate signatures.
Table 5: Codebook
Subcodes
Minutes, Hours, Weeks, Short
Numbers
Per day, Per week , Per month
Clients, Repository, Virus total, Paste-
bin, Twitter, Blog posts, Open source,
Crawl
New, Today, Novel, First
Priority, Customer, Harmful, FIFO, Risk,
Novel, Complexity, New, Damage
Variant, Family, Cluster, Campaign
Network, Simulation
Open Source, Commercial, Joe Sand-
box, Any.run, Cape Sandbox, Virtual-
box, Fireeye
Open Source, Commercial, Joe Sand-
box, Any.run, Cape Sandbox, Virtual-
box, Fireeye
Laptop, VM, Sandbox, Cloud, Server
OS, Windows, Linux, Unix
Web browser, Microsoft, Java, Adobe,
Libraries
Libraries, Timezone, Language, User-
names, User privilege
Browser history, Files, Documents, Di-
rectories, Usage, Names
Procmon, Hooking, Syscalls, Registry,
Files created, Network activity, Pcap,
Logs
Evasion, Encryption, Packing, Obfus-
cation, Detection, Sleep, Debug, Break-
point, Skip, Patch
Static, Analysis, Tool, Ghidra, IDA, Re-
verse Engineer, Unpack, Decrypt, Disas-
sembler, Decode, Decrypt, Script, Injec-
tion, Code, Binary, Function, Registry
Keys, Logs
Signature, IDS, Network behavior, Sys-
tem Behavior, Tool, IP, Domain, Hash,
Tactics, Techniques, Procedure, Capabil-
ities
Session 11C: Software Development and Analysis CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3069