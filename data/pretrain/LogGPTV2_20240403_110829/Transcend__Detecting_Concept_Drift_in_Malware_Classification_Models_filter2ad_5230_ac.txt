also aid in selecting what drifted objects should be la-
beled ﬁrst (e.g., those with low p-values as are the one
that have drifted the most from the trained model).
The following discussion assumes two classes of data,
malicious and benign, but it is straightforward to extend
it to a multiclass scenario.
We deﬁne the function f : B×M → Ω×∆ that maps a
pair of thresholds in the benign and malicious class and
outputs the performance achieved and the number of de-
cisions accepted. Here, the number of decisions accepted
refers to the percentage of the algorithm outputs with a
p-value (for benign or malicious classes, depending on
the output itself) greater than the corresponding thresh-
old; performance means the percentage of correct deci-
sions amongst the accepted ones. B, M, Ω and ∆ are
the domains of the possible thresholds on benign sam-
ples, malicious samples, desired performance and classi-
ﬁcation decisions accepted, respectively. During train-
ing of our classiﬁer, we iterate over all values of the
b,t(cid:48)
m) = (ω(cid:48),δ(cid:48))
b and the malicious threshold t(cid:48)
benign threshold t(cid:48)
m, at
a pre-speciﬁed level of granularity, in the domain of B
and M, respectively. Let us assume f gives the output
f : f (t(cid:48)
To detect concept drift during deployment with a pre-
speciﬁed threshold of either ω or δ, we need to deﬁne
an inverse of f which we call f −1 : Λ → B× M where
Λ = Ω ∪ ∆. When supplied with either ω or δ,
f −1
would give us two thresholds tb and tm which would help
Transcend decide when to accept the classiﬁer’s decision
and when to ignore it. Notice that with a conjoined do-
main Λ, which only accepts either ω or δ, it is not trivial
to reconstruct the values of tb and tm. For every value
of ω, there could be multiple values for δ. Therefore, we
adopt a simple heuristic to compute tb and tm whereby we
maximize the second degree of freedom given the ﬁrst.
For example, given ω, we ﬁnd tb and tm for every possi-
ble value of δ and pick the tb and tm that maximizes δ.
The formulation is exactly the same when δ is used as an
input. The formal equations for the inverse functions are:
Γ = {x : x ∈ ∀t(cid:48)
m))}
b∀t(cid:48)
m. f (t(cid:48)
b,t(cid:48)
f −1(ω) = {(tb,tm) : δ ∈ f (tb,tm) = max(∀δ(cid:48) ∈ Γ)}
f −1(δ) = {(tb,tm) : ω ∈ f (tb,tm) = max(∀ω(cid:48) ∈ Γ)}
Comparison with Probability. The algorithm used as
inner non-conformity measure (NCM) in CE may have
a pre-deﬁned quality metric to support its own decision-
making process (e.g., probability). Hence, we also com-
pare the ability of detecting concept drift of the algo-
rithm’s internal metric with CE metrics. The thresholds
are extracted from the true positive samples, because we
expect the misclassiﬁed samples to have a lower value of
the quality metric: it seems rather appropriate to select
a higher threshold to highlight decisions the algorithm
would likely make wrong. We compare our metrics with
probability metrics derived from two different algorithms
for our case studies. In the ﬁrst case study (see, § 4.1),
we compare our metrics with SVM probabilities derived
from Platt’s scaling [17]; on the other hand, the second
case study (see, § 4.2) uses the probabilities extracted
from a random forest [3] model. This comparison shows
the general unsuitability of the probability metric to de-
tect concept drift. For example, the threshold obtained
from the ﬁrst quartile of the true positive p-value distri-
bution is compared with that of the ﬁrst quartile of the
true positive probability distribution, and so forth.
The reasoning outlined above still holds when a given
algorithm, adapted to represent the non-conformity mea-
sure, uses raw score as its decision-making criteria. For
instance, the transformation of a raw score to a proba-
bility value is often achieved through a monotonic trans-
formation (e.g., Platt’s scaling, for SVM) that does not
affect the p-value calculation. Such algorithms do not
630    26th USENIX Security Symposium
USENIX Association
provide a raw score for representing the likelihood of an
alternative hypothesis (e.g., that the test object does not
belong to any of the classes seen in the training). More-
over, a threshold built from a raw score lacks context
and meaning; conversely, combining raw scores to com-
pute p-values provides a clear statistical meaning, able
of quantifying the observed drift in a normalized scale
(from 0.0 to 1.0), even across different algorithms.
CE can also provide quality evaluation that allows
switching the underlying ML-based process to a more
computationally intensive one on classes with poor con-
ﬁdence [4]. Our work details the CE metrics used by
Dash et al. [4] and extends it to identify concept drift.
4 Evaluation
To evaluate the effectiveness of Transcend, we introduce
two case studies: a binary classiﬁcation to detect mali-
cious Android apps [2], and a multi-class classiﬁcation
to classify malicious Windows binaries in their respec-
tive family [1]. The case studies were chosen to be rep-
resentative of common supervised learning settings (i.e.,
binary and multi-class classiﬁcation), easy to reproduce2,
and of high quality3.
Binary Classiﬁcation Case Study.
In [2], Arp et al.
present a learning-based technique to detect malicious
Android apps. The approach, dubbed Drebin, relies on
statically extracting features, such as permissions, In-
tents, APIs, strings and IP addresses, from Android ap-
plications to fuel a linear SVM. Hold-out validation re-
sults (66-33% split in training-testing averaged over ten
runs) reported TPR of 94% at 1% FPR. The Drebin
dataset was collected from 2010 to 2012 and the authors
released the feature set to foster research in the ﬁeld.
To properly evaluate a drifting scenario in such set-
tings, we also use Marvin [14], a dataset that includes
benign and malicious Android apps collected from 2010
and 2014. The rationale is to include samples drawn
from a timeline that overlaps with Drebin as well as
newer samples that are likely to drift from it (duplicated
samples were removed from the Marvin dataset to avoid
biasing the results of the classiﬁer). Table 1 provides de-
tails of the datasets.
Section 4.1 outlines this experiment in detail; however,
without any loss of generality, we can say models are
trained using the Drebin dataset and tested against the
Marvin one. In addition, the non-conformity measure we
2The work in [2] released feature sets and details on the learning
algorithm, while we reached out to the authors of [1], which shared
datasets and the learning algorithm’s implementation with us.
3The work in [2] was published in a top-tier venue, while the work
in [1] scored similar to the winner of the Kaggle’s Microsoft Malware
Classiﬁcation Challenge [11].
instantiate CE with is the distance of testing objects from
the SVM hyperplane, as further elaborated in § 4.1.1.
Multiclass Classiﬁcation Case Study. Ahmadi et al. [1]
present a learning-based technique to classify Windows
malware in corresponding family of threats. The ap-
proach builds features out of machine instructions’ op-
codes of Windows binaries as provided by Microsoft and
released through the Microsoft Malware Classiﬁcation
Challenge competition on Kaggle [11]—a well-known
platform that hosts a wide range of machine learning-
related challenges. Ahmadi et al. rely on eXtreme Gra-
dient Boosting (XGBoost) [21] for classiﬁcation.
It
is based on gradient boosting [18] and, like any other
boosting technique, it combines different weak predic-
tion models to create a stronger one. In particular, the
authors use XGBoost with decision trees.
Table 2 provides details of the Microsoft Windows
Malware Classiﬁcation Challenge dataset. To properly
evaluate a drifting scenario we omit the family Tracur
from the training dataset, as further elaborated in § 4.2.
In this setting, a reasonable conformity measure that cap-
tures the likelihood of a test object o to belong to a given
family l ∈ L is represented by the probability p that o
belongs to l ∈ L, as provided by decision trees. We ini-
tialize conformal evaluator with −p as non-conformity
measure, because it captures the dissimilarities. Please
note we do not interpret −p as a probability anymore
(probability ranges from 0 to 1), but rather as a (non-
conformity) score CE builds p-values from (see § 2).
We would like to remark that these case studies are
chosen because they are general enough to show how
concept drift affects the performance of the models. This
is not a critique against the work presented in [1, 2].
Rather, we show that even models that perform well in
closed world settings (e.g., k-fold cross validation), even-
tually decay in the presence of non-stationary data (con-
cept drift). Transcend identiﬁes when this happens in op-
erational settings, and provides indicators that allow to
establish whether one should trust a classiﬁer decision or
not. In absence of retraining, which requires samples re-
labeling, the ideal net effect would then translate to hav-
ing high performance on non-drifting objects (i.e., those
that ﬁt well into the trained model), and low performance
on drifting ones.
In a nutshell, our experiments aim to answer the fol-
lowing research questions:
RQ1: What insights do CE statistical metrics provide?
Intuitively, such metrics provide a quantiﬁable level of
quality of the predictions of a classiﬁer.
RQ2: How can CE statistical metrics detect concept
drift in binary and multiclass classiﬁcation? Intuitively,
we can interpret quality metrics as thresholds: predic-
tions of tested objects whose quality fall below such
USENIX Association
26th USENIX Security Symposium    631
DREBIN DATASET MARVIN DATASET
Type
Samples
Benign
Malware
Type
Benign
Malware
Samples
123 435
5 560
9 592
9 179
MICROSOFT MALWARE CLASSIFICATION CHALLENGE DATASET
Malware
Ramnit
Lollipop
Kelihos˙ver3
Vundo
Samples
Malware
Samples
1 541
2 478
2 942
4 75
Obfuscator.ACY
Gatak
Kelihos˙ver1
Tracur
1 228
1 013
398
751
Table 1: Binary classiﬁcation case study datasets [2].
Table 2: Multiclass classiﬁcation case study datasets [1].
thresholds should be marked as untrustworthy, as they
drift away from the trained model (see §3.3).
and associated metrics does not provide the ability to see
decision boundaries nor predictions (statistical) quality.
We elaborate this further in § 4.1 and § 4.2 for binary
and multiclass classiﬁcation tasks, respectively.
4.1 Binary Classiﬁcation Case Study
This section assesses the quality of the predictions of
Drebin4, the learning algorithm presented in [2]. We
reimplemented Drebin and achieved results in line with
those reported by Arp et al.
in absence of concept
drift (0.95 precision and 0.92 recall, and 0.99 precision
and 0.99 recall for malicious and benign classes, re-
spectively on hold out validation with 66-33% training-
testing Drebin dataset split averaged on ten runs).
Figure 2a shows how CE’s decision assessment sup-
ports such results. In particular, the average algorithm
credibility and conﬁdence for the correct choices are 0.5
and 0.9, respectively. This reﬂects a high prediction qual-
ity: correctly classiﬁed objects are very different (from
a statistical perspective) to the other class (and an aver-
age p-value of 0.5 as algorithm credibility is expected
due to mathematical properties of the conformal evalua-
tor). Similar reasoning applies for incorrect predictions,
which are affected by a poor statistical support (average
algorithm credibility of 0.2).
Figure 2b shows CE’s alpha assessment of Drebin. We
plot this assessment as a boxplot to show details of the p-
value distribution. The plot shows that the p-value distri-
bution for the wrong predictions (i.e., second and third
column) is concentrated in the lower part of the scale
(less than 0.1), with few outliers; this means that, on av-
erage, the p-value of the class which is not the correct
one, is much lower than the p-value of the correct predic-
tions. Benign samples (third and fourth columns) seem
more stable to data variation as the p-values for benign
and malicious classes are well separated. Conversely, the
p-value distribution of malicious samples (ﬁrst and sec-
ond columns) is skewed towards the bottom of the plot;
this implies that the decision boundary is loosely deﬁned,
which may affect the classiﬁer results in the presence of
concept drift. A direct evaluation of the confusion matrix