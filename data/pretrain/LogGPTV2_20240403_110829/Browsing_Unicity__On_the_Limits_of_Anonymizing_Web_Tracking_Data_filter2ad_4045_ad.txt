as described above. While industry parameters for temporal
coarsening are usually on the order of tens of minutes [4],
we coarsen up to slightly more than a day to provide a more
comprehensive overview. We ﬁrst run a series of experiments
on the entirety of the sample across all sites, and then repeat
the series on the trace sets per site.
We expect high unicity in experiments with high levels
of detail in auxiliary information and timestamps as well as
no length restrictions. Following the common rationale that
coarsening sufﬁces to anonymize tracking data, we expect the
unicity to decrease markedly in the subsequent experiments
with lower granularity.
The results of the ﬁrst series of experiments are shown in
Fig. 2. We observe that unicity highly depends on timestamp
coarseness. Reducing the accuracy of the timing to the order
of seconds or minutes obscures the exact instant of a click,
but details on intervals between page calls is retained in the
data. When all information about client and page are removed,
unicity remains at over 60% for high temporal resolutions.
In the cases where timestamps are coarsened to the order
of hours, most differences in the intervals between clicks are
lost, and just time of day and sequence information of the
clicks are preserved. We can observe that the granularity of
information about user and page still has a marked effect.
At high granularity, taking information similar to the data
contained in current tracking databases, over 70% of all traces
remain unique. When removing the location and all page
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:12:30 UTC from IEEE Xplore.  Restrictions apply. 
784
Fig. 4. Click trace unicity with minimum length, timestamps coarsened to
the day.
Fig. 5. Click trace unicity for exact trace length, timestamps coarsened to
the hour.
information save the website domain, unicity is reduced to
a value slightly below 40%.
Further coarsening timestamps to the day of
the click
removes differences in time zones and browsing habits per-
taining to daily routines.With full information about page and
location, over half of all click traces remain unique. Knowing
only the number of clicks per day on the websites contained in
a trace (day/-/-/site/∞), the unicity still remains at over 20%.
Only when deleting all auxiliary information (x/-/-/-∞), and
coarsening the timestamps to the order of hours or days do we
observe unicity values that converge to 0 – where practically
no pseudonym has remained and the database is anonymized.
The results of this ﬁrst series of experiments show that
generalization of the attributes does not yield anonymity as
long as even a minimal amount of utility of the database is
preserved.
2) Trimming Sessions and Click Trace Length: Long click
traces exhibit higher unicity than short traces and at session
lengths > 50 clicks even strong coarsening has little effect.
This is intuitive, as the attribute of a click trace’s length is pro-
gressively characteristic with increasing length. Deliberately
limiting click traces to a maximum length should therefore
increase anonymity.
We perform a second series of experiments to this end,
reducing the length of click traces but using all auxiliary
information about clients and pages. Practically, this represents
a tracker that forcibly resets sessions or client IDs after
an observed number of clicks, to reduce the likelihood of
pseudonyms to emerge.
The results are shown in Fig. 3. We observe that unicity can
indeed be signiﬁcantly reduced as the trace length l is reduced,
particularly at high timestamp granularity. However, truly low
values of unicity are only reached when traces are limited to a
single or two clicks and timestamps are coarsened to at least
several hours. Even keeping traces of just l = 4 clicks, and a
time resolution of hours, is sufﬁcient to uniquely identify over
40% of all click traces.
Furthermore, our dataset contains a number of session
fragments - sessions where only one or two clicks occurred on
a website in our sample - which arguably aren’t representative
for the types of browsing sessions we are concerned about.
To assess their effect on the measured unicity, and to get a
better insight into the unicity of click traces more likely to
represent full browsing sessions, we performed another series
of experiments with increasing minimum length. As short click
traces are less likely to be unique, we expect unicity to rise
when shorter click traces are removed. Therefore we only
show the results for highly coarsened timestamps. First, we
only remove shorter click traces and leave longer ones intact,
coarsening to the order of days (Fig. 4). Next we cut click
traces to a chosen length as in the previous experiment and
subsequently remove all shorter click traces from the sample,
coarsening to the order of hours (Fig. 5).
In both experiments unicity rises sharply by over 20% when
click traces consisting of only one or two clicks are removed,
conﬁrming our expectation that unicity of full browsing ses-
sions is likely higher than our initial results show.
3) Unicity of Local Tracking: We ﬁnally wanted to take
the position of publishers that apply local tracking, meaning
websites that keep log ﬁles containing the clicks of their
own visitors. Our dataset contains all necessary data for all
participating websites, as they share all calls to their pages
with the ABC, to give an accurate picture of their popularity.
Click traces within single sites are bound to be much shorter
than click traces of multi-domain sessions. The universe of
different pages within a single site is also much lower, and a
small number of speciﬁc pages or categories have been shown
to be much more popular than others[17]. Given these points,
we expect the unicity of click traces per site to be much lower,
than of click traces from cross-domain tracking.
The results show that unicity does decrease, slightly. Within
single pages, when the location of the client is removed and
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:12:30 UTC from IEEE Xplore.  Restrictions apply. 
785
Fig. 6. The “Matomo (Piwik) case”, local unicity at max length l. Conﬁgu-
ration: x/-/cat/-/·.
Fig. 7. Average anonymity set size within the original dataset.
only the category of the called page is retained, the unicity of
longer click traces remains high (well above 20%), even when
the temporal resolution is reduced to the order of hours (cmp.
Fig. 6). Only limiting the trace length to a single click with
timestamps on the order of minutes or hours, or to 2-click
tuples with timestamps on the order of hours pushes unicity
below 10%.
An interesting artifact can be observed in Fig. 6 for conﬁgu-
rations of very high granularity (i.e. low timestamp coarseness,
high maximum length): a decrease in maximum length can
cause an increase in unicity. The reason for this lies in the way
that sessions are extracted from the measurement databases:
Using Algorithm 1, single longer sessions are split into several
smaller click traces. For relatively high length limits (l = 10)
and given information of high granularity, the odds are fairly
good that a single, long unique click trace is cut into several
shorter click traces, all of which remain unique. This in turn
increases the overall unicity of the dataset.
4) Anonymity Set Sizes: Unicity measures the fraction of
pseudonyms in the database, the remaining non-unique click
traces fall into anonymity sets of varying size. While the
number and cardinality of these sets has no effect on unicity, it
has long been identiﬁed as a reliable metric for anonymity in
settings such as DC networks [18] We therefore investigated
the size of these anonymity sets using Algorithm 2.
For the sake of stable anonymization, one would strive for
large anonymity set sizes. Given the large number of over a
million clients and the common assumption of the popularity
of pages being power-law distributed [19], many common,
or at least highly similar behaviors should be contained in
the database. Observing the impact of time granularity, one
would expect to see fewer, larger anonymity sets, especially
when reducing the temporal resolution, and hence the impact
of different users starting their browsing sessions at different
times. This should be pronounced for parameter sets that are
already characterized by low unicity.
The results, as shown in Fig. 7, largely follow these ex-
pectations. Increased coarsening causes anonymity set sizes
to continue to increase linearly even as unicity converges.
This indicates indeed that with increased coarsening few, in-
creasingly large sets of identical click traces emerge, whereas
the fraction of unique behavior, probably clicks to rarely
visited pages, or browsing with a highly identifying user agent
characterization, remains comparatively stable.
D. Identiﬁability Experiments
Unicity provides a measure of the pseudonimity of click
traces in a tracking database. It does not indicate how easily
a click trace may be identiﬁed or how much an adversary
could gain by doing so. If a click trace is only unique when
considering the entire trace, then the pseudonym is all the data
and knowing to whom it belongs does not offer any additional
insight. Insight can only be gained when the trace contains
unique subtraces.
Our second research question therefore aims at understand-
ing how easily tracking data can be linked against, and thus re-
identiﬁed with data from secondary sources. We devise a series
of experiments to measure the identiﬁability, using Algorithm
3 as deﬁned in Section III-C. We are interested in both of the
scenarios described: the case of trackers partaking in user data
exchanges and comparison to data that is publicly available
(for instance on Twitter).
1) User Data Exchanges: In the ﬁrst scenario, we consider
a tracker to have collected a dataset using his own tracking
technology and to acquire a second dataset at a user data
exchange, such that the acquired dataset has a partial overlap
of tracked sites with their own data. Given various percentages
of overlap we (1) want to understand, how large a fraction of
the click traces in the second dataset can be re-identiﬁed and
uniquely matched to click traces in the ﬁrst dataset. We subse-
quently are interested in (2) the gain of the adversary. Section
III-C deﬁnes it as the fraction of identiﬁed clicks (contained
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:12:30 UTC from IEEE Xplore.  Restrictions apply. 
786
Fig. 8. Dataset enrichment via acquiring a dataset: We measure the identi-
ﬁability of the acquired dataset given an overlap between the datasets.
Conﬁguration: ·/-/-/site/∞
Fig. 9. Dataset enrichment via acquiring a dataset: We measure the infor-
mation gain, meaning the amount of information in identiﬁed click traces
not previously known, given the overlap between datasets. Conﬁguration: ·/-
/-/site/∞
in identiﬁed click traces) in an acquired dataset, which were
not already contained in the initially owned dataset. This data
can then be used to create more comprehensive click traces
belonging to the same data subject even though the subject is
not explicitly contained in either database.
We adapt Algorithm 3 to incorporate the prior knowledge
of the adversary. For that purpose, we sample a database
overlap by selecting a random collection of websites such
that the number of clicks belonging to those websites is a
given fraction of the overall number of clicks. We chose this
approach due to the high variance in size of different websites
in our dataset.
Geolocation,
the public host part of a site, and on a
somewhat reduced resolution even the times of clicks can
be considered globally valid and compatible between tracking
collections. Given that the acquired data is bought at a user
data exchange, however, the adversary may not get, or not be
able to interpret, the code nor category in the trace set, as
these are schemes that are agreed upon between tracker and
publisher internally. We thus remove these details from the
datasets completely, before calculating the identiﬁability.
We expect identiﬁability to increase progressively with a
growing overlap, eventually becoming equal to unicity once
overlap reaches 100%. The results conﬁrm this expectation
(cmp. Fig. 8), showing an almost linear relationship between
identiﬁability and overlap for overlap values below 0.5. How-
ever, as the overlap increases, the adversary’s potential payoff
decreases, because he can only learn new information in the
non-overlapping portion.
We therefore turn to calculating the gain as deﬁned in
section III-C. On ﬁrst glance, if unknown clicks are distributed
uniformly across identiﬁed and unidentiﬁed traces, gain should
be equal to identiﬁability. If, for instance, half of all click
traces are identiﬁed, then we would expect roughly half of
all unknown clicks to be contained therein. However, such
an assumption of uniformity would be incorrect, because the
chance of a click trace to be identiﬁed increases with the
number of known clicks it contains. It is expected then, that
the set of identiﬁed click traces contains a lower proportion
of unknown clicks than the set of unidentiﬁed click traces.
The results as shown in Fig. 9 conﬁrm this expectation. At
high granularity, an overlap of 30% enables the adversary to
learn about 20%-35% of the unknown clicks and at very low
granularity with coarseness values on the order of days, at
40%-50% overlap, about 10% of clicks remain susceptible to
identiﬁcation.
2) Shoulder Surﬁng and Comparison to Digital Dossier
Aggregation: Finally, we want to assess how easily data sub-
jects can be linked back against their click traces in tracking
databases. For this purpose, we assume an adversary to possess
some identiﬁed page calls of a user, known from secondary
sources. Generally this may be from a direct physical en-
counter or prior knowledge of their browsing behavior, but
observation of publicly posted links on social media or, more
generally, digital dossier aggregation may be more attainable.
In this case the adversary is usually aware of at least part of
the data subject’s identity and their goal is to use the observed
information of a partial browsing session to discover the full
browsing session in a database of web tracking data.
The categorization and mutual agreement on codes between
tracker and publisher is assumed to be unknown to the
adversary, we thus limit their best case knowledge to access
time, location, and the visited website. Note that we do not
assume that observations are consecutive. Each observation
is selected completely at random from the browsing session,
but the overall order is preserved. This corresponds to the
deﬁnition of click traces, subtraces and identiﬁability as laid
out in section III.
We perform the experiments on data of various coarsening
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:12:30 UTC from IEEE Xplore.  Restrictions apply. 
787
Fig. 10. Shoulder surﬁng: We measure the identiﬁability of a partially
observed browsing session, given the number of observations.
Conﬁguration: ·/loc/-/site/10.
Fig. 11. Shoulder surﬁng: We measure the identiﬁability of a partially
observed browsing session, given the number of observations for different
session lengths. Conﬁguration: h/loc/-/site/·.
levels and evaluate over the extent of prior knowledge of the
adversary. Note that as the adversary gains more observations,
click traces with an overall
length below the number of
observations are no longer considered. So on one hand we
expect identiﬁability to be well below unicity, due to unicity
acting as an upper bound for what can be identiﬁed. On
the other hand, due to the removal of short,
low unicity
click traces, we would expect even a relatively short number
of observations to be sufﬁcient for a signiﬁcant degree of
identiﬁability.
The results in Fig. 10 indeed show that the adversary needs
to know the time of a single visit only to an accuracy on
the order of minutes to identify almost half of all browsing
sessions with just two observations. Timestamp coarsening