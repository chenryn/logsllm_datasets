1.17
0.05
1.50
0.18
0.21
50
A.W. Moore and K. Papagiannaki
to a particular application. Further observation reveals that the BULK traf-
ﬁc is underestimated by approximately 20% while we see a diﬀerence of 6%
in the WWW traﬃc. However, the port-based approach does not only under-
estimate traﬃc but for some classes, e.g., INTERACTIVE applications, it may
over-estimate it. This means that traﬃc ﬂows can also be misidentiﬁed under
the port-based technique. Lastly, applications such as peer-to-peer and mal-ware
appear to contribute zero traﬃc in the port-based case. This is due to the port
through which such protocols travel not providing a standard identiﬁcation. Such
port-based estimation errors are believed to be signiﬁcant.
Examining Under and Over-Estimation
4.1
Of the results in Table 4 we will concentrate on only a few example situations.
The ﬁrst and most dominant diﬀerence is for BULK — traﬃc created as a
result of FTP. The reason is that port-based classiﬁcation will not be able to
correctly identify a large class of (FTP) traﬃc transported using the PASV
mechanism. Content-based classiﬁcation is able to identify the causal relationship
between the FTP control ﬂow and any resulting data-transport. This means that
traﬃc that was formerly either of unknown origin or incorrectly classiﬁed may be
ascribed to FTP which is a traﬃc source that will be consistently underestimated
by port-based classiﬁcation.
A comparison of values for MAIL, a category consisting of the SMTP, IMAP,
MAPI and POP protocols, reveals that it is estimated with surprising accuracy
in both cases. Both the number of packets and bytes transferred is unchanged
between the two classiﬁcation techniques. We also did not ﬁnd any other non-
MAIL traﬃc present on MAIL ports. We would assert that the reason MAIL is
found exclusively on the commonly deﬁned ports, while no other MAIL trans-
actions are found on other ports, is that MAIL must be exchanged with other
sites and other hosts. MAIL relies on common, Internet-wide standards for port
and protocol assignment. No single site could arbitrarily change the ports on
which MAIL is exchanged without eﬀectively cutting itself oﬀ from exchanges
with other Internet sites. Therefore, MAIL is a traﬃc source that, for quantify-
ing traﬃc exchanged with other sites at least, may be accurately estimated by
port-based classiﬁcation.
Despite the fact that such an eﬀect was not pronounced in the analysed
data set, port-based classiﬁcation can also lead to over-estimation of the amount
of traﬃc carried by a particular application. One reason is that mal-ware or
attack traﬃc may use the well-known ports of a particular service, thus inﬂating
the amount of traﬃc attributed to that application. In addition, if a particular
application uses another application as a relay, then the traﬃc attributed to the
latter will be inﬂated by the amount of traﬃc of the former. An example of such
a case is peer-to-peer traﬃc using HTTP to avoid blocking by ﬁrewalls, an eﬀect
that was not present in our data. In fact, we notice that under the content-based
approach we can attribute more traﬃc to WWW since our data included web
servers operating on non-standard ports that could not be detected under the
port-based approach.
Toward the Accurate Identiﬁcation of Network Applications
51
Table 5. Analysis method compared against percentage of UNKNOWN and correctly
identiﬁed data
Method
I II III IV V VI VII VIII IX Packets
(cid:127)
28.36
(cid:127)
(cid:127) (cid:127)
27.35
(cid:127)
(cid:127) (cid:127) (cid:127)
27.35
(cid:127)
(cid:127) (cid:127) (cid:127)
27.12
(cid:127)
(cid:127) (cid:127) (cid:127)
25.72
(cid:127)
(cid:127) (cid:127) (cid:127)
19.11
(cid:127)
(cid:127) (cid:127) (cid:127)
1.07
(cid:127) 99.99
Bytes Packets
71.03
30.44
72.05
30.33
72.05
30.32
30.09
72.29
74.23
28.43
80.84
21.07
1.22
98.94
99.99
(cid:127)
(cid:127)
(cid:127)
Clearly this work leads to an obvious question of how we know that our
content-based method is correct. We would emphasise that it was only through
the labour-intensive examining of all data-ﬂows along with numerous exchanges
with system administrators and users of the examined site that we were able
to arrive at a system of suﬃcient accuracy. We do not consider that such a
laborious process would need to be repeated for the analysis of similar traﬃc
proﬁles. However, the identiﬁcation of new types of applications will require a
more limited examination of a future, unclassiﬁable anomaly.
4.2 Overheads of Content-Based Analysis
Alongside a presentation of the eﬀectiveness of the content-based method we
present the overheads this method incurs. For our study we were able to iterate
through traﬃc multiple times, studying data for many months after its collection.
Clearly, such a labour-intensive approach would not be suitable if it were to be
used as part of real-time operator feedback.
We emphasise that while performing this work, we built a considerable body
of knowledge applicable to future studies. The data collected for one monitor
can be reapplied for future collections made at that location. Additionally, while
speciﬁc host information may quickly become out-of-date, the techniques for
identifying applications through signatures and protocol-ﬁtting continue to be
applicable. In this way historical data becomes an a-priori that can assist in the
decision-making process of the characterisation for each analysis of the future.
Table 5 indicates the relationship between the complexity of analysis and the
quantity of data we could positively identify — items are ordered in the table
as increasing levels of complexity. The Method column refers to methods listed
in Table 2 in Section 3.
Currently our method employs packet-header analysis and host-proﬁle con-
struction for all levels of complexity. Signature matching is easier to implement
and perform than protocol matching due to its application of static string match-
ing. Analysis that is based upon a single packet (the ﬁrst packet) is inherently
52
A.W. Moore and K. Papagiannaki
less complex than analysis based upon (up to) the ﬁrst KByte. The ﬁrst KByte
may require reassembly from the payload of multiple packets. Finally, any form
of ﬂow-analysis is complicated although this will clearly reduce the overheads of
analysis if the number of ﬂows that require parsing is limited.
Table 5 clearly illustrates the accuracy achieved by applying successively-
more-complicated characterisation techniques. The correctness of classiﬁcation
reported in Table 5 is computed by comparing the results using that method
and the results using the content-based methodology. Importantly, the quantity
of UNKNOWN traﬃc is not simply the diﬀerence between total and identiﬁed
traﬃc. Traﬃc quantiﬁed as UNKNOWN has no category and does not account
for traﬃc that is mis-classiﬁed. It may be considered the residual following each
classiﬁcation attempt.
Table 5 shows that port-based classiﬁcation is actually capable of correctly
classifying 69% of the bytes. Contrasting this value with the known traﬃc in
Table 4 further demonstrates that the mis-identiﬁed amount of traﬃc is rather
limited. Nonetheless, 31% of the traﬃc is unknown. Applying host-speciﬁc knowl-
edge is capable of limiting the unknown traﬃc by less than 1% and signature
and application semantics analysis based on the ﬁrst packet of the ﬂow provides
an additional beneﬁt of less than 1%. It’s only after we observe up to 1 KByte of
the ﬂow that we can increase the correctly-identiﬁed traﬃc from approximately
70% to almost 79%. Application of mechanism VII can further increase this per-
centage to 98%. In Table 2 we have listed example applications that are correctly
identiﬁed when the particular mechanism is applied.
In summary, we notice that port-based classiﬁcation can lead to the positive
identiﬁcation of a signiﬁcant amount of the carried traﬃc. Nonetheless, it con-
tains errors that can be detected only through the application of a content-based
technique. Our analysis shows that typically the greatest beneﬁt of applying such
a technique, unfortunately, comes from the most complicated mechanisms. If a
site contains a traﬃc mix biased toward the harder-to-detect applications, then
these inaccuracies may have even more adverse consequences.
5
Summary and Future Work
Motivated by the need for more accurate identiﬁcation techniques for network
applications, we presented a framework for traﬃc characterisation in the presence
of packet payload. We laid out the principles for the correct classiﬁcation of
network traﬃc. Such principles are captured by several individual building blocks
that, if applied iteratively, can provide suﬃcient conﬁdence in the identity of
the causal application. Our technique is not automated due to the fact that a
particular Internet ﬂow could satisfy more than one classiﬁcation criterion or
it could belong to an emerging application having behaviour that is not yet
common knowledge.
We collected a full payload packet traces from an Internet site and compared
the results of our content-based scheme against the current state of the art —
Toward the Accurate Identiﬁcation of Network Applications
53
the port-based classiﬁcation technique. We showed that classifying traﬃc based
on the usage of well-known ports leads to a high amount of the overall traﬃc
being unknown and a small amount of traﬃc being misclassiﬁed. We quantiﬁed
these inaccuracies for the analysed packet trace.
We then presented an analysis of the accuracy-gain as a function of the
complexity introduced by the diﬀerent classiﬁcation sub-methods. Our results
show that simple port-based classiﬁcation can correctly identify approximately
70% of the overall traﬃc. Application of increasingly complex mechanisms can
approach 100% accuracy with great beneﬁts gained even through the analysis of
up to 1 KByte of a traﬃc ﬂow.
Our work should be viewed as being at an early stage and the avenues for
future research are multiple. One of the fundamental questions that need in-
vestigation is how such a system could be implemented for real-time operation.
We would argue that an adapted version of the architecture described in [5],
which currently performs on-line ﬂow analysis as part of its protocol-parsing
and feature-compression, would be a suitable system. Such an architecture over-
comes the (potential) over-load of a single monitor by employing a method
work-load sharing among multiple nodes. This technique incorporates dynamic
load-distribution and assumes that a single ﬂow will not overwhelm a single
monitoring node. In our experience such a limitation is suﬃciently ﬂexible as to
not be concerning.
We clearly need to apply our technique to other Internet locations. We need
to identify how applicable our techniques are for other mixes of user traﬃc and
when our monitoring is subject to other limitations. Examples of such limitations
include having access to only unidirectional traﬃc or to a sample of the data.
Both these situations are common for ISP core networks and for multi-homed
sites. We already identify that the ﬁrst phase of identiﬁcation and culling of
simplex ﬂows would not be possible if the only data available corresponded to a
single link direction.
We emphasise that application identiﬁcation from traﬃc data is not an easy
task. Simple signature matching may not prove adequate in cases where multi-
ple classiﬁcation criteria seem to be satisﬁed simultaneously. Validation of the
candidate application for a traﬃc ﬂow in an automated fashion is an open issue.
Further research needs to be carried out in this direction. Moreover, we envision
that as new applications appear in the Internet there will always be cases when
manual intervention will be required in order to gain understanding of its nature.
Lastly, in future work we intend to address the issue of how much informa-
tion needs to be accessible by a traﬃc classiﬁer for the identiﬁcation of diﬀerent
network applications. Our study has shown that in certain cases one may need
access to the entire ﬂow payload in order to arrive to the correct causal applica-
tion. Nonetheless, if system limitations dictate an upper bound on the captured
information, then the knowledge of the application(s) that will evade identiﬁca-
tion is essential.
A technical report describing the (manual) process we used is provided in [8].
54
A.W. Moore and K. Papagiannaki
Acknowledgments
We gratefully acknowledge the assistance of Geoﬀ Gibbs, Tim Granger, and
Ian Pratt during the course of this work. We also thank Michael Dales, Jon
Crowcroft, Tim Griﬃn and Ralphe Neill for their feedback.
References
1. Moore, D., Keys, K., Koga, R., Lagache, E., kc Claﬀy: CoralReef software suite as
a tool for system and network administrators. In: Proceedings of the LISA 2001
15th Systems Administration Conference. (2001)
2. Connie Logg and Les Cottrell: Characterization of the Traﬃc between SLAC and the
Internet (2003) http://www.slac.stanford.edu/comp/net/slac-netﬂow/html/SLAC-
netﬂow.html.
3. Fraleigh, C., Moon, S., Lyles, B., Cotton, C., Khan, M., Moll, D., Rockell, R., Seely,
T., Diot, C.: Packet-level traﬃc measurements from the sprint IP backbone. IEEE
Network (2003) 6–16
4. Choi, T., Kim, C., Yoon, S., Park, J., Lee, B., Kim, H., Chung, H., Jeong,
T.: Content-aware Internet Application Traﬃc Measurement and Analysis.
In:
IEEE/IFIP Network Operations & Management Symposium (NOMS) 2004. (2004)
5. Moore, A., Hall, J., Kreibich, C., Harris, E., Pratt, I.: Architecture of a Network
Monitor. In: Passive & Active Measurement Workshop 2003 (PAM2003). (2003)
6. Roesch, M.: Snort - Lightweight Intrusion Detection for Networks. In: USENIX
13th Systems Administration Conference — LISA ’99, Seattle, WA (1999)
7. Orebaugh, A., Morris, G., Warnicke, E., Ramirez, G.: Ethereal Packet Sniﬃng.
Syngress Publishing, Rockland, MA (2004)
8. Moore, A.: Discrete content-based classiﬁcation — a data set. Technical Report,
Intel Research, Cambridge (2005)