---
304,439
38,235
---
269,260
744,292
978,985
19,099
426,720
9,989
---
227,050
97.87  0.0426
475.49  0.2067
---
43.27  0.0188
130.57  0.0568
466.27  0.2027
409.66  0.1781
127.79  0.0556
218.25  0.0949
406.17  0.1766
793.23  0.3449
--- 
---
535.88 
0.233
982.75  0.4273
446.26 
0.194
--- 
---
90.82  0.0395
145.34  0.0632
---
0.045
95.94  0.0417
97.21  0.0423
655.68  0.2851
539.07  0.2344
374.3  0.1627
---
211.73  0.1498
--- 
103.43 
--- 
--- 
32,901 
1,323 
3,978,443+ 
603,791 
81,477 
23,181 
26,561 
11,156 
6,082 
5,372 
4,439 
3,978,443+ 
198,030 
10,227 
2,059 
3,978,443+ 
74,790 
34,800 
3,978,443+ 
65,276 
125,791 
1,154,265 
6,531 
12,812 
18,421 
3,978,443+ 
2.39
4.9
4161.55+
70.99
13.54
11.09
13.81
5.37
10.8
4.02
5.71
1924.74+
14.17
8.71
6.37
1043.94+
16.32
106.1
275.86+
12.64
40.4
806.61
2.95
5.03
5.31
408.05+
119,014/861,212+ 55.58/345.39+
level.  Thus ݖଵିഀమ  is  the 100×൫1−ߙ 2ൗ ൯   percentile  of  the 
standard  normal  distribution,  where α is  the  width  of  the 
two-sided  confidence  interval.  For  example, ݖଵିഀమ =1.96 if 
α=0.95.  f  is  the  permissible  relative  error,  and  nr  is  the 
required minimum sample size.  
C.  PHYS: Profiled-HYbrid Sampling 
Using (3)  and  (4),  we can  obtain the required  sampling 
rate  without  running  the  complete  workload  simulation. 
Rather,  we  first  profile  the  execution  by  sampling  the 
simulation to get the CV and the total number of events. We 
choose  to  use  10%  interval  sampling  for  profiling  all  the 
MinneSPEC  benchmarks  in  Table  I,  as  this  rate  provides 
good  estimates  of  the  population  distribution  and  has 
reasonable turn-around time. From the profile run we get the 
target  sampling  rate  for  a  given  margin  of  error  and 
confidence level for each benchmark by dividing nr’s by the 
total number of vulnerability events. We then use our hybrid 
sampling  approach  with  the  target  sampling  rate  computed 
from profiling to get FIT estimates.  
V.  EVALUATION  
A.  Simulation environment  
The target processor designed for a 65nm technology is 
a  4-wide  out-of-order  processor  with  a  64-entry  ROB,  32-
entry  Load-Store  queue,  and  McFarling’s  hybrid  branch 
predictor.  The  processor  runs  at  3GHz  with  150  cycles  of 
latency to off-chip main memory. L1-I, L1-D and unified L2 
caches all have 32-byte lines. L1-I is a 16KB direct mapped 
cache with 2 cycles hit latency. L1-D is a 16KB 4-way set-
associative cache with 3 cycles hit latency. The unified L2 is 
a  256KB  8-way  set-associative  cache  with  10  cycles  hit 
latency. The selection of a rather small 256KB L2 cache is 
intentional  because  smaller  L2  caches  cause  more  traffic 
between  L2  and  main  memory,  which has  a  greater impact 
on reliability estimations.  We  also provide  results for  1MB 
and 4MB L2 caches with 12 cycles and 18 cycles hit latency 
respectively  in  Section  E.  All  the  caches  are  non-blocking, 
write-back  caches.  All  cache  parameters  are  obtained  from 
Cacti 5 [22].  
We  measure  the  vulnerability  of  the  L2  cache  in  this 
study  by  tracking  VCs  for  L2  only.  The  L2  cache  is  often 
protected by complex mechanisms and hence it is necessary 
for  the  designer  to  be  able  to  accurately  benchmark  L2 
reliability. Note that while L2 is the target of this study the 
simulation slowdowns would be the same if we targeted L1 
caches  since  whether  a  block  is  in  L1  or  L2,  the  block’s 
vulnerability  must be  tracked as  it moves in  and out of  the 
cache  to  memory  until  the  block  is  dead  or  until  program 
termination.  A  block  is  considered  dead  in  the  context  of 
reliability  (called  reliability-dead)  only  when  the  block  is 
never going to be accessed until program execution ends as 
explained  in  Section  I.  Reliability-dead  blocks  are  very 
different  from  conventional  dead  blocks  [10]  which  are 
blocks  that  will  never  be  accessed  again  before  they  are 
evicted  from  the  cache.  Therefore,  whether  the  reliability 
6
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:38:09 UTC from IEEE Xplore.  Restrictions apply. 
Figure 2.   FIT errors and slowdown in classical methods 
simulation  is  done  for  L1  or  L2  caches,  there  is  no 
difference  in  the  slowdown  due  to  reliability-lifetime 
tracking.  
Our  simulator  is  highly  optimized:  Memory  allocation 
for data structures is done dynamically only when necessary 
and  executable  binaries  are  compiled  with  the  highest 
optimization  flag.  We  run  the  simulations  on  a  high-end 
cluster  built  with  quad  core  Xeon  L5430  running  at 
2.66GHz  with  12MB  L2  per  chip  and  4GB/core  main 
memory.  Hence,  the  simulation  host  system  is  very  well 
provisioned  and  does  not  create  any  artificial  resource 
bottleneck for simulation performance. 
We  chose  all  26  benchmarks  listed  in  Table  I  from  the 
SPEC2K  suite.  SimpleScalar/Alpha  sim-outorder  [4]  is 
augmented with a binary search tree (BST) to keep track of 
the  vulnerability  cycles  of  the  entire  memory  footprint  of 
every  benchmark.  In  order  to  get  the  whole  distribution  of 
VCs for the L2 cache we use the MinneSPEC large reduced 
input sets and run every benchmark to completion. The term 
base  sim-outorder  refers  to  the  cycle  accurate  performance 
simulation  without  any  reliability-lifetime  analysis.  The 
rightmost  column  in  Table  I  shows  the  slowdown  of  the 
reliability-lifetime (AVF) simulations compared to the base 
sim-outorder  simulations.  The  column  titled  Tracked  L2 
Blocks  in  the  table  shows  the  number  of  unique  L2  blocks 
that were accessed by the program and hence were stored in 
the  BST.  The  BST  keeps  track  of  timestamps  per  word. 
Table  I  also  shows  the  Silent  Data  Corruption  Soft  Error 
Rate  (SDC)  in  FIT  (column  titled  SDC  FIT)  calculated  by 
multiplying the AVF by the intrinsic ITRS FIT of the target 
L2 cache. 
We  compare  various  sampling 
techniques  using 
complete  benchmark  simulation  with  AVF  reliability 
analysis.  To  do  this  we  need  to  run  AVF  simulations  to 
completion  without  sampling  and  use  these  runs  as  a  gold 
standard  to  compare  slowdowns  and  accuracy  of  various 
sampling schemes. In Table I, five benchmarks do not finish 
their  simulations  after  more  than  one  and  a  half  month 
(more than 46 days), even with the reduced input sets from 
MinneSPEC. They are apsi,  gap, mcf, parser and wupwise. 
These  benchmarks  are  mostly  benchmarks  with  many 
unique  L2  block  accesses,  leading  to  large  memory 
overhead as shown in Table I. Since we could not finish the 
simulations  of  those  benchmarks,  we  could  not  calculate 
their FIT  rates  and get their  exact execution time. Thus  we 
omit  these  five  benchmarks  in  our  sampling  framework 
evaluation  throughout  this  section.  These  five  benchmarks 
suffer  from  more  serious  slowdowns  than  most.  We  report 
their  execution  time  and  slowdowns  in  Table  I  based  on 
their simulation termination time, measured from the start of 
their  simulation  until  we  halted  their  execution.  Since  they 
run  longer  than  the  time  we  observed,  we  report  their 
slowdowns  and  simulation  times  with  plus  signs  stressing 
the  fact  that  the  real  numbers  are  higher  than  the  ones  in 
Table  I.  Excluding the  five  benchmarks, it takes  around 33 
hours  on  average  to  finish  complete  MinneSPEC  and  the 
average slowdown is 55.58 times. It is noteworthy that if the 
five  benchmarks  were  included,  the  average  slowdown  is 
more than 345.39 times. 
7
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:38:09 UTC from IEEE Xplore.  Restrictions apply. 
Figure 3.   FIT errors and simulation slowdowns for various sampling rates 
The simulation slowdown is impacted by two factors: (1) 
Number  of  tracked  L2  blocks  (this  number  directly  affects 
the BST size) and (2) Access patterns. Even if the BST sizes 
of  two  benchmarks  are  the  same,  a  benchmark’s  memory 
access  patterns  can  dramatically  alter  the  performance  of 
BST  searches.  For  instance,  mgrid  experiences  a  more 
severe  slowdown  than  gcc  although  it  has  fewer  blocks  to 
track. The reason is that mgrid has less spatial locality than 
gcc and hence consecutive references access different cache 
blocks  all  the  time  leading  to  random  searching  of  BST. 
mgrid’s  hectic  reference  pattern  and  the  resultant  random 
searching  of  BST  causes  cache  misses  and  TLB  misses  in 
the reliability simulation on the host. It is thus important to 
emphasize  that,  even  if  the  BST  size  is  reduced  by 
optimizing timestamp implementation and using timestamps 
at coarse granularity, the fundamental performance problem 
tied to locality of references in the benchmark still exists.  
To  identify  the  causes  of  simulation  slowdown  we 
analyzed  the  simulations  with  and  without  AVF  analysis 
using  VTune.  Most  benchmarks  spend  significant  amounts 
of time in BST accesses. The analysis confirms the intuition 
that  the  major  reasons  for  dramatic  slowdowns  with 
increasing BST sizes are long latency memory accesses due 
to L2 and TLB misses and page table walks in the host.  
Performing reliability-lifetime analysis of SPEC2K with 
reference inputs is virtually impossible since even the fastest 
benchmark  takes  multiple  days  to  complete.  SPEC2K  with 
reference inputs would yield worse slowdowns than those in 
Table I. For example, three of the fastest executing SPEC2K 
benchmarks with reference inputs are crafty, gzip and lucas. 
The  execution  time  for  reliability  simulations  (without  any 
sampling) of gzip is 5.7 days and its slowdown compared to 
base sim-outorder simulation is 8.71 times. crafty and lucas 
do  not  finish  their  simulations  after  more  than  five  weeks 
and  we  had  to  terminate  their  simulations.  We  will  first 
demonstrate  the  benefits  of  our  sampling  framework  with 
the  21  MinneSPEC  benchmarks.  We  will  then  apply 
sampling 
reliability-lifetime  analysis  on 
SPEC2K  with  reference  inputs  and  show  these  results  in 
Section  VII.  From  now  on,  we  will  compare  various 
sampling  schemes  with  two  metrics  on  21  MinneSPEC 
benchmarks in Table I: (1) simulation slowdown relative to 
base  sim-outorder,  and  (2)  relative  FIT  error  rate  due  to 
to  perform 
8
(as  compared 
sampling 
simulation without sampling).  
B.  Measuring reliability with SMARTS and SimPoint 
to  accurate,  complete  AVF 