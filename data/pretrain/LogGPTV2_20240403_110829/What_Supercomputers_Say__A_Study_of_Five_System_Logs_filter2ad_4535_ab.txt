### What Supercomputers Say: A Study of Five System Logs

**Authors:**
- Adam Oliner, Stanford University, Department of Computer Science, Palo Alto, CA 94305, USA. Email: oliner@cs.stanford.edu
- Jon Stearley, Sandia National Laboratories, Albuquerque, NM 87111, USA. Email: jrstear@sandia.gov

**Abstract:**
To automatically detect and diagnose failures in large-scale computer systems, it is essential to study real deployed systems and the data they generate. Progress in this area has been hampered by the lack of accessible empirical data. This paper addresses this gap by examining system logs from five supercomputers, aiming to provide useful insights and directions for future research into the use of such logs. We present details about the systems, methods of log collection, and how alerts were identified; propose a simpler and more effective filtering algorithm; and define operational context to encompass the crucial information currently missing from most logs. The machines we consider (and the number of processors) are: BlueGene/L (131,072), RedStorm (10,880), Thunderbird (9,024), Spirit (1,028), and Liberty (512). This is the first study of raw system logs from multiple supercomputers.

**1. Introduction**

The reliability and performance challenges of supercomputing systems cannot be adequately addressed until the behavior of these machines is better understood. In this paper, we analyze system logs from five of the world’s most powerful supercomputers: BlueGene/L (BG/L), Thunderbird, RedStorm, Spirit, and Liberty. The analysis encompasses over 111.67 GB of data containing 178,081,459 alert messages in 77 categories. System logs are the first place system administrators go when alerted to a problem and are one of the few mechanisms available to them for gaining visibility into the machine's behavior. As systems grow in size and complexity, there is a pressing need for better techniques for processing, understanding, and applying these data.

We define an alert as a message in the system logs that merits the attention of the system administrator, either because immediate action must be taken or because there is an indication of an underlying problem. Many alerts may be symptomatic of the same failure. Failures can range from major file system malfunctions to transient connection losses that kill jobs.

Using results from our analysis, we provide recommendations for future research in this area. Most importantly, we discuss the following issues:
- Logs do not currently contain sufficient information to enable automatic detection of failures or root cause diagnosis with acceptable confidence. Although identifying candidate alerts is feasible, disambiguation in many cases requires external context that is not available. The most salient missing data is operational context, which captures the system's expected behavior.
- There is a chaotic effect in these systems, where small events or changes can dramatically impact the logs. For example, an OS upgrade on Liberty instantly increased the average message traffic. On Spirit, a single node experiencing disk failure produced the majority of all log messages.
- Different categories of failures have different predictive signatures (if any). Event prediction efforts should produce an ensemble of predictors, each specializing in one or more categories.
- Along with the issues above, automatic identification of alerts must deal with corrupted messages, inconsistent message structure and log formats, asymmetrical alert reporting, and the evolution of systems over time.

Section 3 describes the five supercomputers, the log collection paths, and the logs themselves. Section 3.2 explains the alert tagging process and notes the challenges faced by those hoping to do such tagging (or detection) automatically. Section 4 contains graphical and textual examples of the data and a discussion of the implications for filtering and modeling. Finally, Section 5 summarizes the contributions and our recommendations.

The purpose of this paper is not to argue for a particular reliability, availability, and serviceability (RAS) architecture, nor to compare the reliability of the supercomputers. The systems we study are real, and the logs are in the form used by (and familiar to) system administrators. Our intention is to elucidate the practical challenges of log analysis for supercomputers and to suggest fruitful research directions for work in data mining, filtering, root cause analysis, and critical event prediction. To our knowledge, this is the first paper that has considered raw logs from multiple supercomputing systems.

**2. Related Work**

Work on log analysis and large-systems reliability has been hindered by a lack of data about their behavior. Recent work by Schroeder [21] studied failures in a set of cluster systems at Los Alamos National Lab (LANL) using entries in a remedy database. This database was designed to account for all node downtime in these systems and was populated via a combination of automatic procedures and the extensive effort of a full-time LANL employee, whose job was to account for these failures and assign them a cause within a short period after they happened. Schroeder also examined customer-generated disk replacement databases [22], but there was no investigation into how these replacements were manifested in the system logs. Although similar derived databases exist for the supercomputers considered in this paper, our goal was to describe the behavior of the systems rather than human interpretations.

There is a series of papers on logs collected from BlueGene/L (BG/L) systems. Liang et al. [10] studied the statistical properties of logs from an 8-rack prototype system and explored the effects of spatio-temporal filtering algorithms. Subsequently, they studied prediction models [9] for logs collected from BG/L after its deployment at Lawrence Livermore National Labs (LLNL). The logs from that study are a subset of those used in this paper. Furthermore, they identified alerts according to the severity field of messages. Although it is true that there exists a correlation between the value of the severity field of the message and the actual severity, we found many messages with low severity that indicate a critical problem and vice versa. Section 3.2 elaborates on this claim and details the more intensive process we employed to identify alerts.

System logs for smaller systems have been studied for decades, focusing on statistical modeling and failure prediction. Tsaodeveloped a tuple concept for data organization and to deal with multiple reports of single events [26]. Early work at Stanford [13] observed that failures tend to be preceded by an increased rate of non-fatal errors. Using real system data from two DEC VAX-cluster multicomputer systems, Iyer found that alerts tend to be correlated, and that this has a significant impact on the behavior and modeling of these systems [25]. Lee and Iyer [8] presented a study of software faults in systems running the fault-tolerant GUARDIAN 90 operating system. The task of automatically discovering alerts in log data has been explored from a pattern-learning perspective [7]. There have also been efforts at applying data mining techniques to discover trends and correlations [12, 23, 27, 28].

In order to solve the reliability and performance challenges facing supercomputer installations, we must study the machines as artifacts, characterizing and modeling what they do rather than what we expect them to do. By the number of processor hours (∼774 million), this is the most extensive system log study to date.

**3. Supercomputer Logs**

The broad range of supercomputers considered in this study is summarized in Table 1. All five systems are ranked on the Top 500 Supercomputers List as of June 2006 [2], spanning a range from #1 to #445. They vary by two orders of magnitude in the number of processors and by one order of magnitude in the amount of main memory. The interconnects include Myrinet, Infiniband, Gig Ethernet, and custom or mixed solutions. The various machines are produced by IBM, Dell, Cray, and HP. All systems are installed at Sandia National Labs (SNL) in Albuquerque, NM, with the exception of BG/L, which is at Lawrence Livermore National Labs (LLNL) in Livermore, California.

**3.1 Log Collection**

It is standard practice to log messages and events in a supercomputing system; no special instrumentation or monitoring was added for this study. Table 2 presents an overview of the logs. The remainder of this section focuses on the infrastructure that generated them.

On Thunderbird, Spirit, and Liberty, logs are generated on each local machine by syslog-ng and both stored to /var/log/ and sent to a logging server. The logging servers (tbird-admin1 on Thunderbird, sadmin2 on Spirit, and ladmin2 on Liberty) process the files with syslog-ng and place them in a directory structure according to the source node. We collected the logs from that directory. As is standard syslog practice, the UDP protocol is used for transmission, resulting in some messages being lost during network contention.

**Table 1: System Characteristics at the Time of Collection**

| System Owner | Vendor | Top 500 Rank | Processors | Memory (GB) | Interconnect |
|--------------|--------|--------------|------------|-------------|--------------|
| BlueGene/L   | IBM    | 1            | 131,072    | 32,768      | Custom       |
| Thunderbird  | Dell   | 69           | 9,024      | 27,072      | Infiniband   |
| RedStorm     | Cray   | 9            | 10,880     | 32,640      | Custom       |
| Spirit (ICC2)| HP     | 202          | 1,028      | 1,024       | Gig Ethernet |
| Liberty      | HP     | 445          | 512        | 944         | Myrinet      |

**Table 2: Log Characteristics**

| System             | Start Date   | Days | Size (GB) | Compressed Rate (bytes/sec) | Messages | Alerts | Categories |
|--------------------|--------------|------|-----------|-----------------------------|----------|--------|------------|
| BlueGene/L         | 2005-06-03   | 215  | 1.207     | 0.118                       | 64,747,963 | 348,460 | 41         |
| Thunderbird        | 2005-11-09   | 244  | 27.367    | 5.72                        | 621,112,192 | 3,248,239 | 10         |
| RedStorm           | 2006-03-19   | 104  | 29.990    | 1.215                       | 219,096,168 | 1,665,744 | 12         |
| Spirit (ICC2)      | 2005-01-01   | 558  | 30.289    | 1.678                       | 272,298,969 | 172,816,564 | 8          |
| Liberty            | 2004-12-12   | 315  | 22.820    | 0.622                       | 265,569,231 | 2,452     | 6          |

More alerts do not imply a less reliable system; it also reflects the redundancy of the reporting and the preferences of the system administrators. Alerts were tagged into categories according to the heuristics supplied by the administrators for the respective systems, as described in Section 3.2. Two alerts are in the same category if they were tagged by the same expert rule; the categories column indicates the number of categories that were actually observed in each log. Compression was done using the Unix utility gzip.

RedStorm has several logging paths [1]. Disk and RAID controller messages in the DDN subsystem pass through a 100 Megabit network to a DDN-specific RAS machine, where they are processed by syslog-ng and stored. Similarly, all Linux nodes (login, Lustre I/O, and management nodes) transmit syslog messages to a different syslog-ng collector node for storage. All other components (compute nodes, SeaStar NICs, and hierarchical management nodes) generate messages and events which are transmitted through the RAS network (using the reliable TCP protocol) to the System Management Workstation (SMW) for automated response and storage. Our study includes all of these logs.

On BG/L, logging is managed by the Machine Management Control System (MMCS), which runs on the service node, of which there are two per rack [3]. Compute chips store errors locally until they are polled, at which point the messages are collected via the JTAG-mailbox protocol. The polling frequency for our logs was set at around one millisecond. The service node MMCS then relays the messages to a centralized DB2 database. That RAS database was the source of our data and includes hardware and software errors at all levels, from chip SRAM parity errors to fan failures. Events in BG/L often set various RAS flags, which appear as separate lines in the log. The time granularity for BG/L logs is down to the microsecond, unlike the one-second granularity of typical syslogs. This study does not include syslogs from BG/L’s Lustre I/O cluster and shared disk subsystem.

**Table 3: Distribution of Alert Types Before and After Filtering**

| Type        | Raw Count  | % Raw | Filtered Count | % Filtered |
|-------------|------------|-------|----------------|------------|
| Hardware    | 174,586,516 | 98.04 | 1,999          | 18.78      |
| Software    | 144,899    | 0.08  | 6,814          | 64.01      |
| Indeterminate | 3,350,044 | 1.88  | 1,832          | 17.21      |

Hardware was the most common type of alert, but not the most common type of failure (as estimated by the filtered results). Filtering dramatically changes the distribution of alert types.

**3.2 Identifying Alerts**

For each of the systems, we worked in consultation with the respective system administrators to determine the subset of log entries that they would tag as being alerts. Thus, the alerts we identify in the logs are certainly alerts by our definition, but the set is (necessarily) not exhaustive. In all, we identified 178,081,459 alerts across the logs; see Table 2 for the breakdown by system and Table 4 for the alerts themselves. Alerts were assigned types based on their ostensible subsystem of origin (hardware, software, or indeterminate); this is based on each administrator’s best understanding of the alert and may not necessarily be the root cause. Table 3 presents the distribution of types both before and after filtering (described in Section 3.3). Note that many of these alerts were multiply reported by one or more nodes (sometimes millions of times), requiring filtering of the kind discussed in Section 3.3. Furthermore, it means that the number of alerts we report does not necessarily reflect the number of distinct failures.

**Table 4: Example Alert Messages from the Supercomputers**

| System      | Total Alerts (Raw/Filtered) | Example Message Body (Anonymized) |
|-------------|-----------------------------|----------------------------------|
| BlueGene/L  | 348,460 / 1202              | H/KERNDTLB: data TLB error interrupt |
| Thunderbird | 3,248,239 / 2088            | I/VAPI: kernel: [KERNELIB] ... (Fatal error (Local Catastrophic Error)) |
| RedStorm    | 1,665,744 / 1430            | H/BUSPAR: DMTHINT Warning: Verify Host2 bus parity error: 0200 Tier: 5 LUN: 4 ... |
| Spirit      | 172,816,564 / 4875          | H/EXTCCISS: kernel: cciss: cmd0000010000a60000 has CHECK CONDITION, sensekey=0x3 |
| Liberty     | 2452 / 1050                 | S/PBSCHK: pbsmom: task check, cannot tm reply to [job] task 1 |

Alert categories vary among machines as a function of system configurations, logging mechanisms, and what each system’s administrators deem important. Bracketed text indicates information that is omitted; a bracketed ellipsis indicates sundry text.