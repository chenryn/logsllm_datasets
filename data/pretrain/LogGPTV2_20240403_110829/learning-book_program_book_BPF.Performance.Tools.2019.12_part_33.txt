[128,256}
61
[256, 512}
131
[512, 1K]
3 1
[1K, 2x)
31
[2K,4K)
151
---
## Page 266
6.3 BPF Tools
229
bpftrace has a separate bucket for negative values (°(., O)°), which are error codes returned by
read(2) to indicate an error. You can craft a bpftrace one-liner to print these as a frequency count
(as shown in Chapter 5) or a linear histogram so that the individual numbers can be seen:
1 /0 > yezret, 0, 100, 1) : 1*
Attaching l pzobe...
^C
9:
[11, 12)
123 18ee88e88e88e 88ee8ee8ee8ee8ee88e88e88e 88ee8ee8ee8ee8e1
This output shows that error code 1 was always returned. Checking the Linux headers
(asm-generic/errno-base.h):
tdefine EAGAIN
11
/* Try agsin */
Error code 11 is for *try again,” an error state that can occur in normal operation.
6.3.12funccount
funccount(8), introduced in Chapter 4, is a BCC tool that can frequency-count functions and
other events. It can be used to provide more context for software CPU usage, showing which func
tions are called and how frequently. profile(8) may be able to show that a function is hot on CPU,
but it can’t explain why°: whether the function is slow, or whether it was simply called millions
of times per second.
As an example, this frequency-counts kernel TCP functions on a busy production instance by
: ,d4, qum ufgaq seq aso Suupieu
funceount 'tcp_*
Tracing 316 functions for "tcp_*"... Bit Ctr1-C to end.
C
FUIC
COUNT
[..-]
368048
tcp_es tab1lshed_opt.ions
381234
tcp_v4_md5_lookug
402945
tcp_gxo_recelve
484571
dnooTopspedoq
510322
De tach.ing-. .
harder than it sounds: see Section 2.12.2 in Chapter 2.
---
## Page 267
230
Chapter 6 CPUs
tracing-
This output shows that tcp_md5_do_lookup() was most frequent, with 510,000 calls while
smoqs sndsno (glagosd sapea ag ‘ajdurexa so ‘t  Sursn papesaua aq ueo pndno jeasaqu-sad
that the function get_page_from_freelist() was hot on CPU. Was that because it was called often or
because it was slow? Measuring its per-second rate:
 funccount -1 1 get_page_from_Ezeelist
Tracing 1 functions for *get_page_fron_freelist*... Bit Ctrl-C to end.
FUBIC
COUNT
586452
FUHC
COUNT
ttaauoaga6eda6
586241
[.--]
The function was called over half a million times per second.
This works by using dynamic tracing of the function: It uses kprobes for kernel functions and
uprobes for user-level functions (kprobes and uprobes are explained in Chapter 2). The overhead
of this tool is relative to the rate of the functions. Some functions, such as malloc( and
get_page_from_freelist(), tend to occur frequently, so tracing them can slow down the target
application significantly, in excess of 10 percent—use caution. Se Section 18.1 in Chapter 18 for
more about understanding overhead.
Command line usage:
funccount[options] [iinterval][-d duration] pattern
Options include:
 r: Use regular expressions for the pattern match
fuo saooud si sanseay :aia d-
Patterns:
name or p: name: Instrument the kernel function called mume()
• 1ib: name: Instrument the user-level function called nme() in library lib
• path: name: Instrument the user-level function called mame( in the file at puth
• t: syatem: name: Instruments the tracepoint called systemmarme
*: A wildcard to match any string (globbing)
See Section 4.5 in Chapter 4 for more examples.
---
## Page 268
6.3 BPF Tools
231
bpftrace
The core functionality of funcount(8) can be implemented as a bpftrace one-liner:
1: ()unoo =[aqoxd] 1 doq:x, - aoexgdq 
Attaching 320 probes...
[...]
e[kpzobe: tcp_zelease_cb] : 153001
e [kprobe: tcp_v4_md5_1ookup] : 154896
[kpzcbe: tcp_gro_rece1ve: 177187
This can be adjusted to do per-interval output, for example, with this adkdition:
[ (e) xe[o(e) uad1 1s [enxequT
As with BCC, use caution when tracing frequent functions, as they may incur significant
overhead.
6.3.13softirqs
softirqs(8) is a BCC tool that shows the time spent servicing soft IRQs (soft interrupts). The
system-wide time in soft interrupts is readily available from different tools. For example,
mpstat(1) shows it as%soft. There is also /proc/softirqs to show counts of soft IRQ events. The
BCC softirqs(8) tool differs in that it can show time per soft IRQ rather than event count.
For example, from a 48-CPU production instance and a 10-second trace:
softirqs 10 1
Tracing soft irg event time... Hit Ctrl-C to end.
SOFTIRQ
TOTAL_usecs
ne t_cx
633
tasklet
30939
rcu
65855t
sched
185873
tinex
389144
net_rx
1358268
This output shows that the most time was spent servicing net_x, totaling 1358 milliseconds. This
is significant, as it works out to be 3 percent of the CPU time on this 48-CPU system.
jo peatqaso au1 stuodaoen xabuggos:bu pue raquabrpgos:bu aq Sursn 6q sxom (g)sbpgos
this tool is relative to the event rate, which could be high for busy production systems and high
network packet rates. Use caution and check overhead.
---
## Page 269
232
Chapter 6 CPUs
Command line usage:
softirqa[opt1ons1 1nterval[count]1
Options include:
 -d: Shows IRQ time as histograms
 T: Includes timestamps on output
The d option can be used to explore the distribution and identify whether there are latency
outliers while servicing these interrupts.
bpftrace
A bpftrace version of softirqs(8) does not exist, but could be created. The following one-liner is a
starting point, counting IRQs by vector ID:
.1 : ()qunoo = [oaa<-sfxe]e 1 Kaqus bavagos:bxt:qugodeoeaa, - soezagdq +
Attaching 1 probe..*
[3]: 11
[6] : 45
[0]: 395
e[9] : 405
9[1]: 524
[7]: 561
These vector IDs can be translated to the softirq names in the same way the BCC tool does this: by
using a lookup table. Determining the time spent in soft IRQs involves tracing the irq:softirq_exit
tracepoint as well.
6.3.14 hardirqs
hardirqs(8)² is a BCC tool that shows time spent servicing hard IRQs (hard interrupts). The
system-wide time in hard interrupts is readily available from dlifferent tools. For example,
mpstat(1) shows it as %irq. There is also /proc/interrupts to show counts of hard IRQ events.
The BCC hardirqs(8) tool differs in that it can show time per hard IRQ rather than
event count.
21 rigin: 1 first creted this ss intimesd on 28-Jun-2005, for printing time sume and intongpu.d for printing histo
grams on 9-May-2005, which wos based on intr.d from the “Dynamic Tracing Guide,° Jan 2005 [Sun 05]. 1 8lso devel
leg jo, sqdruiajul/oud/ sey snun aurs adg o9 1 peqμod pou eweu snq ndo fq sqdrugu .mous o1 00g aoe.,g e pado
task. I developed this BCC version that does both sums and histograms on 20-0ct-2015.
---
## Page 270
6.3 BPF Tools
233
For example, from a 48-CPU production instance and a 10-second trace:
hardirqs 10 1
Tracing hard irg erent tine... Hit Ctrl-C to end
HARDIRQ
TOTAL_usecs
ena-mgant@pcl:0000:00:05 .0
43
nvne0q0
46
gx0ge
47424
eth0-Tx-Rx6
48199
eth0-Tx-Rx5
48524
eth/0TxRx2
49482
Exgx0e
4 9750
eth0TxRx0
51084
eth0-Tx-Rx4
51106
eth/0Tx=Rx1
52649
This output shows that several hard IRQs named ethO-Tx-Rx* had total times of around
50 milliseconds for this 10-second trace.
hardirqs(8) can provide insight for CPU usage that is not visible to CPU profilers. See the Internals
section of Section 6.2.4 for profiling on cloud instances that lack a hardware PMU.
This tool currently works by using dynamic tracing of the handle_irq_event_percpu( kernel
function, although a future version should switch to the irq:irq_handler_entry and irq:
irq_handler_exit tracepoints.
Command line usage:
[[unoo tesxequt][suotdo  sbxrpxeu
Options include:
-d: Shows IRQ time as histograms
• : Includes timestamps on output
The d option can be used to explore the distribution and identify whether there are latency
outliers while servicing these interrupts.
6.3.15 smpcalls
smpcalls(8)2* is a bpftrace tool to trace and summarize time in the SMP call functions (also known
as cross calls). These are a way for one CPU to run functions on other CPUs, including all other
(named after CPU cross calls), which I created on 17-Sep-2005.
---
## Page 271
234
Chapter 6 CPUs
a 36-CPU system:
CPUs, which can become an expensive activity on large
+ smpca1ls.bt
Attaching 8 probes..
Tracing SMP calls, Hit Ctrl-C to stop-
°C
?tIne_ns [do_flush_tlb_al1] :
[32K, 64K]
[64K, 128K)
1 leeeeeeeeee８e eeeeeeeeeeeeeeeeeeeeeee８e eeeeeeeeeeeeeee1
ptine_ns [re
on [ :
[4K, 8K)
1188e88e88e88 88 e8ee8ee8ee8
[8K, 16K]
1leeeeeeeeeee eeeeeeeeeeeeee
[16K,32K)
0 1
[323, 64K)
2leeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee1
?tIne_ns [do_sync_core[ :
[32K, 64K]
15 188e88e88e88 888e88e88988e88e88e88e88 888e88e88988e88 
[64K, 128K)
9 188e88e88e888888e88e88e88e88e88
nd_reschedule] :
[2K, 4K)
[4K, 8K)
318ee88e88
[8K, 16K)
198  8  
[16K, 32K]
880886881 E
?tine_ns [aperfnpexf_snapshot_khz] :
[1K, 2K)
518
[2K, 4K)
12 1889
[4K, 8K)
121889
[BK, 16K]
618
[16K,32K)
1 1
[323, 64K)
16 eee8e88 88e8ee8ee8ee8e8e88e 88ee8ee8ee8ee8e1
[64K,128K)
20188988
This is the first time I’ve run this tool, and it's identified an issue right away: The
aperfmperf_snapshot_khz cross callis relatively frequent and slow, taking up to
128 microseconds.
---
## Page 272
6.3 BPF Tools
235
The source to smpcalls(8) is:
#1/usx/local/bin/bpftrace
BEGIX
printf (*Txacing SMP ca11s, HIt Ctr1-C to stop-^n*);
kprobe:sng_ca1l_function_single,
kprobe:anp_ca11_functLon_nany
Bts[tid] = nsecs
Bfunc[tid] = arg1
kzetpxobe:snp_ca1l_functlon_sing1e,
kretprobe:snp_ca1l_function_many
/Bts [t.d] /
etine_ns [ksyn(efunc[tid])] = hist [nsecs - @ts[tid]] 
delete (8ts[tid]) 
delete (@func [tid） 
kprobe:native_sng_send_reschedule
8ts[tid] = nsecsr
(dt16ex = [p]oung8
kretprobe:native_sng_send_reschedule
/Bts [t.d] /
Btine_ns [ksynlefunc[tid])] = hist (nsecs - @ts[tid]]
delete (8ts [tid]) 
delete @func[tid1 
END
f
clear (8ts) 
clear (8func) 
---
## Page 273
236
Chapter 6 CPUs
Many of the SMP calls are traced via kprobes for the smp_call_function_singlel and
smp_call_function_many0 kernel functions. The entry to these functions has the remote
CPU function as the second argument, which bpftrace accesses as argl and stores keyed by
thread ID for lookup in the kretprobe. It is then converted into the human-readable symbol by
the bpftrace ksym( built-in.
There is a special SMP call not covered by those functions, smp_send_reschedule(), which is
traced via native_smp_send_reschedule(). I hope that a future kernel version supports SMP call
tracepoints to simplify tracing of these calls.
The @time_ns histogram key can be modified to include the kernel stack trace and process name:
Btine_ns [comn, kstack, ksyn (efunc [tld]1] = hlst (nseca - Bts [tid[1
This includes more details for the slow call:
tIne_ns[snrp-pass,
snp_cal1_function_single+1
apexfnperf_snapshot_cpu+90
srch_freq_prepare_sl1+61
cpuin.fo_open+14
proc_reg_open+111
+vedoxsuep"op
path_openat+692
do_f11p_open+153
67+uados.sop
do_aysca11_64+85
entry_SYsCALL_64_after_hvframe+68
apexfispexf_snapshot_khz]:
[2K,4K)
2188
[4K, 8K)
D 1
[8K, 16K]
118
[16x, 32K]
118
51 18e988e88e886 88ee88e8e986e8ee88e88e886 88ee88e8e986e881
[32K,64K)
[64x, 1283)
B68888886886886881 6t
This output shows that the proces was snmp-pass, a monitoring agent, and it was doing an
open() syscall that ends up in cpuinfo_open() and an expensive cross call.
Using another BPF tool, opensnoop(8), quickly confirms this behavior:
ssrd-duus uz- Kd·dooususdo 
TIME (s)
PID
COMM
FD ERR FATH
0, 000000000
2440
smap-pass
4
0/proc/cpuinfo
0,000841000
2440
ssedduus
/d/ 0
1, 022128000
2440
srmp-pass
0/proc/cpuinfo
---
## Page 274
6.3BPF Tools
237
1, 024696000
2440
ssedduue
0/proc/stat
2,046133000
2440
ssed-dus
4
0/proc/cpuinfo
2,049020000
2440
ssedduus
0 /proc/epuinfo
/d/ 0
3,070135000
244 0
srmp-pass
3, 0728 69000
2440