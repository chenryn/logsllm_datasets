to the enterprise about the correctness and availability of the entire
ﬁle system. An important requirement is that auditing of correct-
ness should be performed with minimal bandwidth and computa-
tion. For instance, downloading a substantial fraction of the ﬁle
system to verify its correctness would not be an acceptable solu-
tion. In addition, a recovery mechanism is needed to reconstruct
the original data once corruptions are detected.
Several different protocols that address to some extent this ques-
tion have been proposed in the literature. PoR protocols provide
strong assurances about availability of data outsourced to the cloud,
and a recovery mechanism, but they have only been designed for
static data (ﬁles that do not undergo modiﬁcations). PDP proto-
cols, while supporting updates to data, ensure only detection of a
certain amount of data corruption, but do not implement a recov-
ery mechanism. To the best of our knowledge, our solution here is
the ﬁrst dynamic PoR protocol over an entire ﬁle system, support-
ing updates and providing an efﬁcient recovery mechanism in case
data corruption is detected.
We start by presenting at a high level how existing PoR protocols
work, and then describe the challenges of adapting these ideas to a
dynamic setting. We then discuss our main insights and contribu-
tions in constructing a dynamic PoR protocol.
5.1 Static PoR protocols
In a PoR protocol, the tenant encodes a single ﬁle with an error-
correcting code (ECC) and stores the encoded ﬁle in the cloud. The
encoded ﬁle contains the original ﬁle and some parity blocks, re-
dundant blocks computed with the ECC that are needed in recov-
ering from corruption. To ensure correctness and availability of
the data, the tenant periodically challenges the cloud for a few ran-
domly selected ﬁle blocks, and veriﬁes their correctness. Through
this auditing protocol, the tenant can detect large-scale corruption
to the ﬁle (exceeding a certain ﬁxed threshold). Small corruptions,
while not detectable through sampling, can be recovered from the
redundancy embedded in the encoded ﬁle.
An important parameter in a PoR is the recovery-failure prob-
233
ability ρ. This is the probability, assuming that the cloud replies
correctly to all challenges during an audit, that the tenant can’t re-
cover the ﬁle from the cloud’s storage. The size and frequency of
challenges in a PoR may be calibrated to achieve a target parameter
ρ given the ﬁle size, and error-correcting code parameters.
5.2 Challenges for dynamic PoRs
The main challenge in adapting a static PoR protocol to a dy-
namic setting is the construction of an error-correcting code with
several required properties. As a reminder, the error-correcting
code is used to recover from corruptions once the auditing proto-
col detects missing or corrupted data at the cloud. An additional
requirement our system has compared to previous PoR protocols is
that it needs to recover from corruptions of both data and meta-data
in the entire ﬁle system (while previous PoR protocols have been
designed for single ﬁles).
Our ﬁrst observation is that we can use in our system an era-
sure code instead of a more expensive error-correcting code. The
reason is that Iris’s main service is authentication of ﬁle system
blocks, and, therefore, the portal can verify the correctness of ﬁle
blocks and Merkle tree nodes during recovery and determine the
positions of corrupted blocks. We present the remaining challenges
in achieving an efﬁcient dynamic PoR protocol:
Challenge 1: Update efﬁciency The erasure code has to support
updates to the ﬁle system efﬁciently. In particular a modiﬁcation
to a ﬁle block or Merkle-tree node should require the update of
only a small number of parity blocks. Additionally, it would be
preferable to use cheap Galois ﬁeld arithmetic in the parity com-
putation, such as GF (2) which essentially consists of XOR opera-
tions. Higher order Galois ﬁeld arithmetic (as employed by Reed-
Solomon codes, for instance) is too expensive to sustain our desired
throughput of several hundred megabytes per second.
This requirement excludes upfront the use of maximum-distance
separable (MDS) codes. While such codes are attractive for their
correction capability, a parity block in an MDS codes depends on
all message blocks, and therefore updates to the codeword are quite
impractical.
Thus we must use a non-MDS code, with a lower error-correction
capability. For instance, we might stripe the ﬁle system, that is, par-
tition it into a number of smaller components, called stripes, and
apply an erasure code individually to each stripe (striping is a com-
mon technique employed in most storage systems today). With
this approach, updates would be more efﬁcient as an update to a
ﬁle block or Merkle tree node would involve updating only parity
blocks within a single stripe.
Challenge 2: Hiding code structure Nevertheless, striping intro-
duces a problem. When a client updates a block of the ﬁle sys-
tem along with the corresponding stripe parities, it reveals code-
structure information to the cloud, namely the correspondence be-
tween the ﬁle blocks and the parity blocks. A malicious cloud can
then create a targeted corruption against the ﬁle system, e.g., it can
corrupt a single stripe and its corresponding parity blocks. Such
corruption, being focused, will be hard to detect by sampling (since
sampling detects only a large amount of corruption).
We overcome this challenge with two key techniques:
1. Cache parities at the portal We cache the parity information
at the enterprise side and only transmit parities to the cloud
at regular time intervals for back up (e.g., at the end of the
week). With this approach, the cloud does not perceive indi-
vidual updates to the ﬁle system, but only the aggregate par-
ity structure over a large number of updates and can not infer
the exact code structure. Moreover, updates are extremely
efﬁcient if parities are stored in main memory at the portal.
2. Randomize code structure Even when parities are stored at
the portal, it is important that the stripe structure is not re-
vealed to the cloud to avoid targeted corruptions. To enforce
this, we randomize the assignment of ﬁle blocks to stripes.
If these two design principles are employed, it might seem that
after caching the parities locally and randomizing the assignment of
ﬁle blocks and tree nodes to stripes, any erasure code could be used
for computing the parity blocks within a stripe. But our system has
to overcome another subtle challenge:
Challenge 3: Variable-length encoding Typically, the code pa-
rameters for an erasure code, including the message size, and the
size of parity information are ﬁxed and known in advance (before
encoding is performed). However in Iris we need to compute parity
blocks over an entire ﬁle system data and meta-data blocks without
knowing in advance the total size of the ﬁle system. At the same
time, we have to enforce a randomization of the mapping of ﬁle
system blocks to parity blocks at any given time. Therefore, ap-
proaches in which new parity blocks are created as more data is
added to the ﬁle system in a streaming fashion (e.g., LDPC codes)
would not be applicable here.
New sparse randomized erasure code construction. Our solu-
tion is to set an upper bound on the total size of the ﬁle system,
and design a novel erasure code construction that is sparse in the
sense that it supports incremental updates to the codeword very ef-
ﬁciently, even when only a fraction of the maximum size is used by
the ﬁle system. The construction randomizes the mapping of ﬁle
system blocks to parity blocks, and uses binary XOR operations.
The size of the parity information is also constrained to ﬁt into the
main memory of typical servers today (an important consideration
for efﬁcient updates). We are able to prove for this construction an
exponentially small bound for the recovery-failure probability.
If the ﬁle system needs to be expanded, the error correcting codes
can be rebuilt, but a more bandwidth efﬁcient solution would be to
double the ECC data structure when the ﬁle system doubles in size.
5.3 Our erasure code
Parameter overview: We ﬁrst set an upper bound for the entire
ﬁle system size, denoted n. In our example parameterization, n is
the number of 4KB blocks needed for a ﬁle system of size 1PB.
Our erasure code construction is scalable up to that size, but once
the ﬁle system exceeds the upper bound, the code parameters need
to be changed and the ﬁle system has to be re-encoded.
To correct a fraction α of erasures, the storage for parities must
√
√
√
n) randomly selected ﬁle blocks.)
be at least s ≥ αn blocks—a coding-theoretic lower bound. Here
s is limited by the sizes of current memories to about s = O(
n)
n). (To obtain
for practical ﬁle system sizes and thus α = Ω(1/
a probabilistic guarantee that at most an α-fraction of all stored
ﬁle blocks is missing or corrupted, the tenant must challenge c =
O(1/α) = O(
To support updates efﬁciently we split the huge codeword into
m ≈ αn stripes; each stripe being a codeword itself with p parities.
With high probability, given an α-fraction of erasures, each stripe
is affected by only O(log n) erasures. Thus to correct and recover
√
stripes, we need p = O(log n) parity blocks per stripe, leading
n log n) memory. Each write only
to s = O(αn log n) = O(
involves updating u = O(log n) parities within the corresponding
stripe. By using a sparse parity structure, though, we are able to
reduce u to O(log log n).
Details on our erasure-code construction: Our erasure code is a
sparse one based on efﬁcient XOR operations. Although the new
construction is probabilistic in that successful erasure decoding is
not guaranteed for any number of erasures, its main advantage is
that it is a binary efﬁcient code scalable to large codeword lengths.
The portal computes parities over both ﬁle blocks and Merkle
tree nodes when block values are updated by a client operation. For
the purpose of erasure coding, we view data blocks or tree nodes as
identiﬁer-value pairs δ = (δid; δval), where δid is a unique identi-
ﬁer (a unique block ID in the ﬁle system) and δval = (δ1, . . . , δb)
is a sequence of b bits denoting the change in block value. (We as-
sume all blocks are initialized with 0.) To randomize the mapping
from data blocks to parity blocks, we use a keyed hash function
Hk(.) that maps an identiﬁer δid to a pair (θind, θ), where θind is
a random stripe index and θ = (θ1, . . . , θp) is a binary vector of p
bits. The randomization is graphically depicted in the full version
of this paper [1].
The 1s in vector θ indicate the parity bits that need to be updated.
Each update modiﬁes at most u of the p parities of the stripe to
which δ belongs. That is, Hk(δid) is designed to produce a binary
random vector θ of length p with at most u entries equal to 1. For
u = O(log p) = O(log log n) this leads to a sparse erasure code
that still permits decoding, but entails relatively few parity updates.
Encoding: We maintain a parity matrix P [i] for each stripe i,
1 ≤ i ≤ m. To change the value of block δid with δval, the
portal computes Hk(δid) = (θind; θ); constructs A = δval ⊗ θ =
{δiθj}i∈[1,b],j∈[1,p]; and updates P [θind] ← P [θind] ⊕ A. The
change in parity structure is shown graphically in the full version
of this paper [1].
Since vector θ has at most u non-zero positions, the number of
XOR operations for updating a block is u. The total storage for all
parities is s = bpm bits.
Decoding: Erasure decoding of the multi-striped structure involves
decoding each stripe separately. Gaussian elimination is performed
m times, each time computing the right inverse of a (≤ p) × p
matrix–at a cost of at most p2 = O((log n)2) XOR operations. As
an additional beneﬁt of our construction, decoding can be done in
place, and thus within memory at the portal.
Analysis: The full version of this paper [1] provides a detailed
analysis. E.g., with a block size of 4KB, 5KB communication
per challenged block, 5.8GB total communication per challenge-
response round, 16GB of main memory at the portal for parity stor-
age, and 1PB ﬁle system size, we achieve recovery failure proba-
bility ρ ≤ 0.74%.
234
5.4 Erasure-coding for Dynamic PoR
We now explain how our erasure code functions in Iris.
PoR encoding and update: During encoding, the portal constructs
two parity structures:
the data parity structure constructed over
the ﬁle system data blocks (including the data blocks in the free
list) and the meta-data parity structure over the meta-data blocks
(internal nodes in the data structure comprising the Merkle tree and
free list).
√
The challenge-response protocol: The portal challenges the cloud
n) randomly selected ﬁle system
to return a set of c (again c = O(
data blocks, including data blocks from the free list. These blocks
are all leaf nodes in the authenticated data structure containing the
Merkle tree and free list. As an optimization, the portal sends a
seed from which the challenge set is derived pseudo-randomly.
The c selected random blocks together with the authenticating
paths from the authenticated data structure are transmitted back to
the portal. The portal veriﬁes the correctness of the responses by
performing two checks. First, it veriﬁes the integrity and freshness
of the selected blocks, checking the block MACs and the path to
the root in the authenticated data structure. Second, it veriﬁes that
the blocks have been correctly indexed by the challenges according
to the node ranks/weights. (This proves that the ﬁle system data
blocks are selected with uniform probability.) As a byproduct of
these checks the challenge-response protocol also veriﬁes the in-
tegrity and freshness of the meta-data blocks (internal nodes in the
authenticated data structure). We can immediately infer that if a
fraction α of ﬁle system data blocks don’t verify correctly, then at
most a fraction α of internal nodes in the Merkle tree and free list
are either missing or corrupted.
Recovery: See the full version of this paper [1] for the recovery
algorithm.
6.
Our implementation of Iris is a 25,000-line end-to-end system
with all integrity checking in place. The system is fully asyn-