sented with a consent form approved by the IRB office. Participants
who did not consent to the form were denied to proceed with the
study. We rewarded $0.2 to each participant who completed the
study.
We had a total of 98 participants who took part in our study. We
had included a question to ensure that the user is answering the
survey authentically. Based on the responses to this question, we
rejected the answers of 7 participants. Our results for the user study
were thus based on responses from 91 users. The participants are
either Amazon Alexa users or Google Assistant users. We didn’t
include participants who use other assistants like Siri and Cortana
in our study. We had 66 participants who are Alexa users and 25
participants who use Google assistant at home.
Response
Question
Yes
Are you aware of the privacy policies of your skills/actions? No
How often do you read the privacy policy of a skill/action?
Do you read the privacy policy from the skill/action’s
webpage/Alexa app?
Do you know what personal data the skills/actions
you use are capable of collecting from you?
Do you read the privacy policy before using a new
skill / action?
Do you read the privacy policy before enabling a
kid’s skill / action?
% of users
48
52
73
11
16
66
34
47
21
32
79
7
14
75
7
18
Rarely
Half the time
Most of the time
No
Yes
No
Maybe
Yes
No
Maybe
Yes
No
Maybe
Yes
Table 8: Survey responses.
Table 8 shows the survey responses. When asked about whether
they are aware of the privacy policies of the voice-apps they use,
about 48% of the participants claimed that they are aware of it.
But when asked about how often they actually read the privacy
policy provided by the developer, 73% responded with "rarely". 11%
responded that they read it half the time. 34% of our participants
said that they use the smartphone app or the skills webpage to read
the privacy policy while the rest 66% said that they never read it.
47% were not aware of what data is being collected by the skill from
them and another 21% were not entirely sure either. This shows a
major usability issue where the users ignore the privacy policy even
when it is provided by the developer. When asked about the issues
they face with privacy policies, 20% of the participants responded by
saying it is hard to access. 44% of participants felt that the document
was too long. 24% claimed that they felt inconsistencies between the
privacy policy and the skill’s actual functionality and description.
Users also had problem with developers not providing a privacy
policy at all and the ones provided being not informative. The
document being too legal and hard to comprehend was a concern
for the users. Only 14% of participants felt that they always check
the privacy policy before enabling a skill. 79% of our participants
did not check the privacy policy before enabling a general skill and
75% did not check it before enabling a kids skill. This lack of usage of
the privacy policy by the users shows the need of the voice assistant
platforms to address the concerns and take measures to improve
the quality as well as the usability of the privacy policies provided
by the developers. We have included a few responses from the
participants about their perspectives on whether privacy policies
should be required for every voice-app in Table 12 in Appendix.
6 DISCUSSION
In this section, we discuss the limitation of this work and further re-
search that can help in addressing the user frustration over privacy
policies in VA platforms.
6.1 Limitation
We are unable to examine the actual source code of voice-apps. The
availability of the source code can largely increase the knowledge
of what personal data the voice-app is able to collect and where it
is stored. This can be compared with the privacy policy to ensure
the developer is not performing any malicious activity or misus-
ing the user’s trust. With having no baseline, a future research
9
effort that can be done on this regard is to dynamically test voice-
apps by enabling them and check their data collection practices.
Most developers provide short descriptions which will introduce
the skill/action to end users, but data practices are not frequently
defined in the descriptions. Since the data related to voice-apps
is very limited, we largely depend on the descriptions provided
with the voice-apps. This makes our findings on the inconsistency
checking not complete. As mentioned in Sec. 3.2, our focus is on
revealing the existence of problematic privacy policies, rather than
identifying all the inconsistent privacy policies. For capturing data
practices, we use a keyword-based method, and compare the simi-
larity of a privacy policy with our keyword dictionary. However,
the keyword set can be incomplete. In our future work, we plan to
use machine learning techniques to train a model to identify data
practices from natural language documents. Nevertheless, we have
collected strong evidence in revealing issues over privacy policies
on VA platforms. In addition, the dataset of Google actions that
we collected and used for our study is not complete and does not
contain all the voice-apps available in the app store. The actions
are listed in pages that automatically load more data when the user
reaches the end of the page. Since more and more actions keep
getting loaded dynamically, we were unable to use the crawler to
automatically get information about all the actions.
6.2 Why poor-quality privacy policies?
Amazon Alexa and Google Assistant not explicitly requiring app-
specific privacy policies results in developers providing the same
document that explains data practices of all their services. This
leads to uncertainties and confusion among end users. There are
skills with privacy policies containing up to 428 data practices and
most of these data practices are not relevant to the skill. Thus these
documents do not give a proper understanding of the capabilities of
the skill to end users. The poor quality of privacy policies provided
with voice-apps is partially due to the lack of an app-specific pri-
vacy policy and due to the lenient certification system. During the
certification process, the content of a privacy policy is not checked
thoroughly when the skill is submitted for certification, which has
resulted in a large amount of inactive and broken links and also pri-
vacy policies not related to the skill. Some privacy policies mention
data practices that are in violation of the privacy requirements that
Amazon and Google have set but these voice-apps are still certified.
In some cases, even if the developer writes the privacy policy
with proper intention and care, there can be some discrepancies
between the policy and the actual code. Updates made to the skill
might not be reflected in the privacy policy. This is especially pos-
sible with the current VA architecture because the backend code of
the skill can be updated at any time by the developer and does not
require any re-certification to be made available to the end users.
The outdated policy may lead to the developers unintentionally
collecting personal information without informing the users.
6.3 Privacy policy through voice
The unavailability of privacy policies through voice requires users
to access them over the web or through the apps on their phones.
One possible reason for this can be due to the large size of the
privacy policies and the time required to read out the long docu-
ment. Users who only use voice assistant services through their
VA devices, may not necessarily be aware of the existence of the
privacy policies in the respective stores. Also, it is completely left to
the user to decide whether to view the privacy policy or not. There
is no approval asked prior to enabling the voice-app for the user.
In order to address these issues, we propose to introduce a built-in
intent (i.e., functionality) for a voice-app that gives information to
users about the privacy policy of the voice-app through a voice
response. The major challenge for this is that the privacy policies
are usually too long to be read out to users. Thus, the response
provided by the built-in intent has to be marginally short.
Prior work has been done to summarize the privacy policies to
make it more readable to the user. Tools like Polisis [27] and Priva-
cycheck [11] conduct privacy policy analysis and represent the data
practices mentioned in the document in a simpler form to users.
But from our analysis of the skills/actions available in the stores,
we have noticed that most privacy policies are general policies and
do not necessarily define what the behavior of the voice-app in
particular is. Since personal information can be collected through
the conversational interface, our approach aims in understanding
this capability from the voice-app’s source code, automatically gen-
erating an easy-to-digest privacy notice, and letting the user know
about it through the voice channel. To achieve this, we describe
our preliminary approach based on the Alexa platform. We take
the interaction model of a skill, which is a JSON (JavaScript Object
Notation) file and scan for all the slots and their slot types specified.
We categorise the built-in slot types based on what type of personal
information they can collect. For custom slot types, we compare
the values provided with the entries in datasets we assembled of
possible values and check for a match. After we get all the types of
information that can be collected by the skill, we create a response
notifying the user that the skill has these capabilities and advise
users to look at the detailed privacy policy provided by the devel-
opers. The intent can be invoked when the skill is first enabled. On
opening the skill for the first time, this brief privacy notice can be
read out to the user. This will give the user a better understanding
of what the skill he/she just enabled is capable of collecting and
using. The users can also ask to invoke this intent later to get a
brief version of the privacy policy. As our future work, we plan
to extend this approach to help developers automatically generate
privacy policies for their voice-apps.
7 RELATED WORK
7.1 Privacy concerns for voice assistants
Many research efforts have been undertaken to study user concerns
(human factors) about the security/privacy of VA devices [15, 16, 19,
23, 25, 26, 31, 32, 37, 43]. Fruchter et al. [25] used natural language
processing to identify privacy and security related reviews about VA
devices from four major online retailers: Target, Walmart, Amazon,
and Best Buy. The authors highlighted that users worried about
the lack of clarity about the scope of data collection by their voice
assistants. Through a semi-structured interviews with 17 VA users,
Abdi et al. [15] uncovered the lack of trust users have with some of
VA use cases such as shopping, and a very limited conception of VA
ecosystem and related data activities. Malkin et al. [31] surveyed 116
10
VA owners and found that half did not know that their recordings
were being stored by the device manufacturers. Similarly, authors
in [43, 46] conducted interviews on smart home owners to examine
user mental models and understand their privacy perceptions of
IoT devices. Geeng et al. [26] investigated tensions and challenges
that arise among multiple users in smart home environment. Lau et
al. [29] conducted interviews with both VA users and non-users,
and revealed that privacy concerns could be the main deterring
factor for new users.
There has been an increasing amount of research on techni-
cal attack vectors against VA systems and the corresponding de-
fenses. One line of research is to exploit interpretation errors of
user commands by speech recognition, such as voice squatting at-
tack [28, 45], and generate hidden/inaudible voice commands [21,
33, 34, 38, 40, 42, 44]. Another line of research focuses on defense
mechanisms, including, continuous authentication [24], canceling
unwanted baseband signals [44], correlating magnetic changes with
voice commands [22], and user presence-based access control [30].
Our work differs from these previous work in that we investigate
the effectiveness of privacy policies provided by voice-app develop-
ers.
7.2 Privacy policy analysis for mobile apps
Privacy policies disclose an organization’s or developer’s data prac-
tices. Though researchers have conducted privacy policy analysis
on Android platform [17, 36, 39, 41, 47, 48], there is an absence of pri-
vacy policy analysis on VA platforms. Zimmeck et al. [48] presented
a privacy analysis system for Android to analyze apps’ potential
non-compliance with privacy requirements, and inconsistencies
between privacy policies and apps. Results show that 71% of apps
that lack a privacy policy should have one, and a substantial por-
tion of apps exhibit potential privacy requirement inconsistencies.
Wang et al. [39] developed a hierarchical mapping based approach
for privacy policy analysis which is able to handle the data inputted
by users in addition to the data accessed directly through the mobile
device. The user input data is checked for possible privacy leaks
and this is used to determine whether the app’s privacy policy is in
contradiction with this leakage. The consistency between the data
collected by the app and the privacy policy provided is verified by
using a data flow analysis. A major difference of our work from
these works is that we rely on voice-app’s description to detect
inconsistency in privacy policies due to the unavailability of voice-
app’s source code. To the best of our knowledge, this is the first
work to systematically measure the effectiveness of privacy policies
for voice-apps.
8 CONCLUSION
In this work, we conducted a comprehensive empirical analysis on
privacy policy of 64,720 Amazon Alexa skills and 2,201 Google As-
sistant actions. We designed an NLP-based approach to capture data
practices in privacy policies and descriptions of voice-apps. Our
results showed that a substantial number of problematic privacy
policies exist in Amazon Alexa and Google Assistant platforms, a
worrisome reality of privacy policies on VA platforms. Google and
Amazon even have official voice-apps violating their own require-
ments regarding the privacy policy. We also conducted a user study
11
to understand users’ perspectives on voice-apps’ privacy policies,
which reflects real-world user frustrations on this issue. We also
discussed possible approaches to improve the usability of privacy
policies on VA platforms.
REFERENCES
[1] Alexa Skills Policy Testing. https://developer.amazon.com/fr/docs/custom-
skills/policy-testing-for-an-alexa-skill.html.
[2] Alexa Skills Security Requirements. https://developer.amazon.com/fr/docs/custom-
skills/security-testing-for-an-alexa-skill.html.
[3] Amazon Developer Services Agreement. https://developer.amazon.com/support/
legal/da.
[4] Amazon mechanical turk. https://www.mturk.com/.
[5] Configure
for Customer
Permissions
Information
in Your
Skill.
[6] Google
fined
https://developer.amazon.com/en-US/docs/alexa/custom-skills/configure-
permissions-for-customer-information-in-your-skill.html.
France.
https://www.theverge.com/2019/1/21/18191591/google-gdpr-fine-50-million-
euros-data-consent-cnil/.
€50 million
violation
GDPR
for
in
[7] Google Privacy Policy Guidance. https://developers.google.com/assistant/console/
policies/privacy-policy-guide.
Industrial-Strength Natural Language Processing. https://spacy.io.
[8]
[9] Number of digital voice assistants in use worldwide from 2019 to
https://www.statista.com/statistics/973815/worldwide-digital-voice-
2023.
assistant-in-use/.
general-policies.
google-chrome.
[10] Policies for Actions on Google. https://developers.google.com/actions/policies/
[11] PrivacyCheck for Google Chrome. https://identity.utexas.edu/privacycheck-for-
[12] Selenium automates browsers. https://www.selenium.dev.
[13] Snapchat Transmitted Users’ Location and Collected Their Address Books
Without Notice Or Consent. https://www.orrick.com/Insights/2013/02/FTC-
Assesses-800000-Fine-Against-Mobile-App-Operator-and-Issues-Mobile-