==== DBSCAN
关于DBSCAN的wiki：
DBSCAN（Density-Based Spatial Clustering of Applications with Noise，具有噪声的基于密度的聚类方法）是一种基于密度的空间聚类算法。该算法将具有足够密度的区域划分为簇，并在具有噪声的空间数据库中发现任意形状的簇，它将簇定义为密度相连的点的最大集合。
该算法利用基于密度的聚类的概念，即要求聚类空间中的一定区域内所包含对象（点或其他空间对象）的数目不小于某一给定阈值。DBSCAN算法的显著优点是聚类速度快且能够有效处理噪声点和发现任意形状的空间聚类。但是由于它直接对整个数据库进行操作且进行聚类时使用了一个全局性的表征密度的参数，因此也具有两个比较明显的弱点：
1. 当数据量增大时，要求较大的内存支持I/O消耗也很大；
2. 当空间聚类的密度不均匀、聚类间距差相差很大时，聚类质量较差。
DBSCAN算法的目的在于过滤低密度区域，发现稠密度样本点。跟传统的基于层次的聚类和划分聚类的凸形聚类簇不同，该算法可以发现任意形状的聚类簇，与传统的算法相比它有如下优点：
1. 与K-MEANS比较起来，不需要输入要划分的聚类个数
2. 聚类簇的形状没有偏倚
3. 可以在需要时输入过滤噪声的参数
DBSCAN算法基于一个事实：一个聚类可以由其中的任何核心对象唯一确定。等价可以表述为：任一满足核心对象条件的数据对象p，数据库D中所有从p密度可达的数据对象o所组成的集合构成了一个完整的聚类C，且p属于C。
邻域：给定对象半径内的区域称为该对象的邻域
过程：
扫描整个数据集，找到任意一个核心点，对该核心点进行扩充。扩充的方法是寻找从该核心点出发的所有密度相连的数据点（注意是密度相连）。遍历该核心点的邻域内的所有核心点（因为边界点是无法扩充的），寻找与这些数据点密度相连的点，直到没有可以扩充的数据点为止。最后聚类成的簇的边界节点都是非核心数据点。之后就是重新扫描数据集（不包括之前寻找到的簇中的任何数据点），寻找没有被聚类的核心点，再重复上面的步骤，对该核心点进行扩充直到数据集中没有新的核心点为止。数据集中没有包含在任何簇中的数据点就构成异常点。
Syntax：
    fit DBSCAN [algo_params] from 
Parameters:
*  ：使用这些特征字段对目标字段进行聚簇和预测的训练，字段个数须>=1
* [into model_name]：保存下来的模型名称，可选，若不填将不做保存
* [algo_params]：针对DBSCAN模型的参数设置，可选，不填时用默认参数设置
** eps : float, optional
+
The maximum distance between two samples for them to be considered as in the same neighborhood.
** min_samples : int, optional
+
The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself.
** metric : string, or callable
+
The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by metrics.pairwise.calculate_distance for its metric parameter. If metric is “precomputed”, X is assumed to be a distance matrix and must be square. X may be a sparse matrix, in which case only “nonzero” elements may be considered neighbors for DBSCAN.
** algorithm : {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional
+
The algorithm to be used by the NearestNeighbors module to compute pointwise distances and find nearest neighbors. See NearestNeighbors module documentation for details.
** leaf_size : int, optional (default = 30)
+
Leaf size passed to BallTree or cKDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.
** p : float, optional
+
The power of the Minkowski metric to be used to calculate distance between points.
** n_jobs : int, optional (default = 1)
+
The number of parallel jobs to run. If -1, then the number of jobs is set to the number of CPU cores.
==== KMeans
关于KMeans的wiki：
设我们一共有 N 个数据点需要分为 K 个 cluster ，k-means 要做的就是最小化下式:
image::images/ml-KMeans-objective-function.png[]
其中 r_{nk} 在数据点 n 被归类到 cluster k 的时候为 1 ，否则为 0 。直接寻找r_{nk} 和µ_k 来最小化J 并不容易，不过我们可以采取迭代的办法：先固定µ_k ，选择最优的r_{nk} ，很容易看出，只要将数据点归类到离他最近的那个中心就能保证J 最小。下一步则固定r_{nk}，再求最优的µ_k。将J 对µ_k 求导并令导数等于零，很容易得到J 最小的时候µ_k 应该满足：
image::images/ml-kmeans-u.png[]
即 µ_k 的值应当是所有 cluster k 中的数据点的平均值。由于每一次迭代都是取到J 的最小值，因此J 只会不断地减小（或者不变），而不会增加，这保证了 k-means 最终会到达一个极小值。
KMeans的步骤：
选定 K 个中心 µ_k 的初值。这个过程通常是针对具体的问题有一些启发式的选取方法，或者大多数情况下采用随机选取的办法。因为前面说过 k-means 并不能保证全局最优，而是否能收敛到全局最优解其实和初值的选取有很大的关系，所以有时候我们会多次选取初值跑 k-means ，并取其中最好的一次结果。
将每个数据点归类到离它最近的那个中心点所代表的 cluster 中。
用下式计算出每个 cluster 的新的中心点：
image::images/ml-kmeans-u2.png[]
重复第二步，一直到迭代了最大的步数或者前后的 J 的值相差小于一个阈值为止。
* 时间复杂度： `O(tKmn)` ，其中，t为迭代次数，K为簇的数目，m为记录数，n为维数
* 空间复杂度： `O((m+K)n)` ，其中，K为簇的数目，m为记录数，n为维数
Syntax：
    fit KMeans [algo_params] from  [into model_name]
Parameters:
*  ：使用这些特征字段对目标字段进行聚簇和预测的训练，字段个数须>=1
* [into model_name]：保存下来的模型名称，可选，若不填将不做保存
* [algo_params]：针对KMeans模型的参数设置，可选，不填时用默认参数设置
** n_clusters : int, optional, default: 8
+
The number of clusters to form as well as the number of centroids to generate.
** max_iter : int, default: 300
+
Maximum number of iterations of the k-means algorithm for a single run.
** n_init : int, default: 10
+
Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia.
** init : {‘k-means\+\+’, ‘random’ or an ndarray}
+
Method for initialization, defaults to ‘k-means\+\+’:
+
‘k-means\+\+’ : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details.
+
‘random’: choose k observations (rows) at random from data for the initial centroids.
+
If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.
** algorithm : “auto”, “full” or “elkan”, default=”auto”
+
K-means algorithm to use. The classical EM-style algorithm is “full”. The “elkan” variation is more efficient by using the triangle inequality, but currently doesn’t support sparse data. “auto” chooses “elkan” for dense data and “full” for sparse data.
** precompute_distances : {‘auto’, True, False}
+
Precompute distances (faster but takes more memory).
+
‘auto’ : do not precompute distances if n_samples * n_clusters > 12 million. This corresponds to about 100MB overhead per job using double precision.
+
True : always precompute distances
+
False : never precompute distances
** tol : float, default: 1e-4
+
Relative tolerance with regards to inertia to declare convergence
** n_jobs : int
+
The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel.
+
If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used.
** random_state : integer or numpy.RandomState, optional
+
The generator used to initialize the centers. If an integer is given, it fixes the seed. Defaults to the global numpy random number generator.
** verbose : int, default 0
+
Verbosity mode.
+
** copy_x : boolean, default True
+ 
When pre-computing distances it is more numerically accurate to center the data first. If copy_x is True, then the original data is not modified. If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean.
==== SpectralClustering
关于SpectralClustering的wiki：
谱聚类(Spectral Clustering, SC)是一种基于图论的聚类方法——将带权无向图划分为两个或两个以上的最优子图，使子图内部尽量相似，而子图间距离尽量距离较远，以达到常见的聚类的目的。其中的最优是指最优目标函数不同，可以是割边最小分割——下图的Smallest cut，也可以是分割规模差不多且割边最小的分割——下图的Best cut
image::images/ml-spectral-clustering-model.png[]
这样，谱聚类能够识别任意形状的样本空间且收敛于全局最优解，其基本思想是利用样本数据的相似矩阵(拉普拉斯矩阵)进行特征分解后得到的特征向量进行聚类。
最优化方法：Min cut，Nomarlized cut, Ratio cut, Normalized相似变换
谱聚类的步骤：
. 数据准备，生成图的邻接矩阵
. 归一化普拉斯矩阵
. 生成最小的k个特征值和对应的特征向量
. 将特征向量kmeans聚类(少量的特征向量)
Syntax：
    fit SpectralClustering [algo_params] from 
Parameters:
*  ：使用这些特征字段对目标字段进行聚簇和预测的训练，字段个数须>=1
* [into model_name]：保存下来的模型名称，可选，若不填将不做保存
* [algo_params]：针对SpectralClustering模型的参数设置，可选，不填时用默认参数设置
** n_clusters : integer, optional
+
The dimension of the projection subspace.
** affinity : string, array-like or callable, default ‘rbf’
+
If a string, this may be one of ‘nearest_neighbors’, ‘precomputed’, ‘rbf’ or one of the kernels supported by sklearn.metrics.pairwise_kernels.
+
Only kernels that produce similarity scores (non-negative values that increase with similarity) should be used. This property is not checked by the clustering algorithm.
** gamma : float, default=1.0
+
Scaling factor of RBF, polynomial, exponential chi^2 and sigmoid affinity kernel. Ignored for affinity='nearest_neighbors'.
** degree : float, default=3
+
Degree of the polynomial kernel. Ignored by other kernels.
** coef0 : float, default=1
+
Zero coefficient for polynomial and sigmoid kernels. Ignored by other kernels.
** n_neighbors : integer
+
Number of neighbors to use when constructing the affinity matrix using the nearest neighbors method. Ignored for affinity='rbf'.
** eigen_solver : {None, ‘arpack’, ‘lobpcg’, or ‘amg’}
+
The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities
** random_state : int seed, RandomState instance, or None (default)
+
A pseudo random number generator used for the initialization of the lobpcg eigen vectors decomposition when eigen_solver == ‘amg’ and by the K-Means initialization.
** n_init : int, optional, default: 10
+
Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia.
** eigen_tol : float, optional, default: 0.0
+
Stopping criterion for eigendecomposition of the Laplacian matrix when using arpack eigen_solver.
** n_jobs : int, optional (default = 1)
+
The number of parallel jobs to run. If -1, then the number of jobs is set to the number of CPU cores.