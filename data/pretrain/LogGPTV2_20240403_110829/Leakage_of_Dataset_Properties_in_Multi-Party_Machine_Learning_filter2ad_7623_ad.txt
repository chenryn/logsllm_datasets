Logistic Regression
Health
Adult
A
A
1.00
1.00
.99
1.00
.65
.52
.78
.79
¯A
1.00
1.00
.57
.75
¯A
1.00
1.00
.41
.72
Neural Network
Health
Adult
A
A
.79
.90
.74
.98
.52
.52
.54
.51
¯A
.84
.98
.52
.45
¯A
.95
.98
.51
.63
Table 5: Multi-party setting: Black-box attack accuracy
for predicting whether the values of (sensitive) synthetic
variable A in the data of the honest party are predom-
inantly  5. The attack accuracy is evaluated
on 100 Dhonest datasets: half with 33:67 and half with
67:33 split. A synthetic correlation with A is added to the
variables X and Y depending on the speciﬁc case. R cor-
responds to the setting where only 3 attributes are used
for training instead of all data. Attack accuracy based on
a random guess is 0.5.
Model
Datasets
Synthetic Variable
X ∼ A,Y ∼ A
X⊥A,Y ∼ A
X ∼ A,Y⊥A
X ∼ A,Y⊥A (R)
Logistic Regression
Health
Adult
A
A
.98
1.00
.98
1.00
.48
.67
.86
.61
¯A
1.00
1.00
.60
.74
¯A
1.00
1.00
.53
.62
Neural Network
Adult
A
.98
.99
.56
.68
Health
A
.92
.89
.52
.54
¯A
.99
1.00
.52
.66
¯A
.95
.98
.49
.61
Table 6: Single-party setting: Black-box attack accuracy
with synthetic data.
6.3 Fine-grained Attack
Information leaked about attribute values can be either
in terms of a binary signal, i.e., which attribute value
is dominant in the dataset or an exact distribution. The
results above show the leakage of the former. To learn
information about the exact distribution, we present a
variation of our main attack called the ﬁne-grained attack.
For this attack, we train a 5-class meta-classiﬁer model
that outputs whether a particular value of the sensitive
attribute appears in 10%, 30%, 50%, 70%, or 90% of the
dataset. Note that we train only one meta-classiﬁer model
with 5 output classes, but the attacker can perform a more
systematic binary search over the distribution by training
multiple meta-classiﬁer models. We apply this attack in
two settings.
Leakage of Attribute Distribution. We evaluate on
the Adult dataset using a synthetic variable A as well as
the gender variable. Table 7 shows the results for our
ﬁne-grained attack for predicting the precise distribution
of the sensitive variable. The row 30 : 70 corresponds
Distribution
of A in Dhonest:
10 : 90
30 : 70
50 : 50
70 : 30
90 : 10
LR
Synthetic A
¯A
A
.998
.994
.993
.991
.997
.999
.989
.997
.993
.998
NN
Synthetic A
A
.84
.79
.79
.73
.72
¯A
.89
.79
.73
.71
.77
LR
A: Gender
¯A
.44
.59
.50
.46
.53
Table 7: Fine-grained attack accuracy for predicting the
precise distribution of sensitive variable A in Dhonest in
the synthetic setting X⊥A,Y ∼ A, and real data setting
when A is Gender on the Adult dataset. Attack accuracy
based on a random guess is 0.2.
to the setting where 30% of records in Dhonest have the
value of the sensitive attribute A less than 5. Here, the
attacker tries to guess the split of 30 : 70 among ﬁve pos-
sible splits of 10 : 90, 30 : 70, etc. The baseline accuracy
is 20% because the attacker wishes to distinguish be-
tween 5 splits. Since the attack accuracy is always higher
than the random guess, the attacker can successfully ﬁnd
the correct ratio by training a meta-classiﬁer that distin-
guishes between different splits of the sensitive attribute
values. Similar to the observation in Section 6.1, we ob-
serve that logistic regression has higher attack accuracy
than neural networks. The attack accuracy for the real
data with gender as the sensitive attribute is consistently
greater than the 20% baseline for random guessing for
all the distributions.
Model Update Setting. We apply the ﬁne-grained at-
tack to learn the change in the distribution of an attribute
value given access to an updated version of a model. In
this attack, the malicious party initially obtains a model
that is jointly trained on Dhonest1 and Dadv. Later, another
honest party Dhonest2 joins, and a new model is trained
on the three parties’ data. The attacker tries to infer the
dominant value of the sensitive attribute of Phonest2 given
the original and the updated model. It uses a ﬁne-grained
attack against both models, as result learning a dominant
value in Dhonest1 and Dhonest1∪Dhonest2. It then compares
the two and infers how Dhonest2 has affected the distribu-
tion. If the split is dominated by the same attribute value
in both models, the attacker uses this attribute value distri-
bution as its guess. Otherwise, the attacker makes a guess
that the other attribute value is dominated in Dhonest2.
Table 8 shows the results for our attack in the model
update setting using synthetic data for the Adult dataset.
The attack accuracy is almost close to 100% for the syn-
USENIX Association
30th USENIX Security Symposium    2697
Distribution
of A in Dhonest1:
Distribution
of A in Dhonest2:
LR
Synthetic A
LR
A: Gender
30:70
70:30
30:70
70:30
30:70
70:30
1.00
.99
.99
1.00
.87
.72
.63
.85
Table 8: Model update setting: attack accuracy for pre-
dicting the dominant value of sensitive variable A in
Dhonest2 in the synthetic setting X⊥A,Y ∼ A and real
data setting when A is Gender on Adult dataset when A
is removed from the training data. Dadv has 50:50 split.
Attack accuracy based on a random guess is 0.5.
Figure 3: Attack accuracy for leaking sensitive attribute
ProductType on the Amazon graph data (11 output
classes) as the number of queries to the model increases.
thetic case and ranges from 63% to 86% for the Gender
variable which is higher than a random guess of 50%.
6.4 Attack Parameters
We perform ablation experiments to understand the
effect of varying the number of queries, distribution of
the sensitive attribute and the number of output classes
on the attack accuracy. We use the Amazon graph data
for these experiments where, as before, ProductType is
the sensitive attribute, and ReviewScore is the target.
Number of queries. We compute the attack accuracy
for two different splits of values of the sensitive attribute,
0:100 (all books) and 30:70 (70% books, 30% of other
products), and train the model to predict one of 11 review
scores averaged over 10 runs. Figure 3 shows the effect
of increasing the number of queries on the attack accu-
racy. Note that the number of queries also correspond to
the input features of our attacker classiﬁer. We observe
Figure 4: Attack accuracy for the Amazon graph data
when the sensitive attribute ProductType is not used
during training for different numbers of output classes
across different distributions (splits).
that changing queries does not signiﬁcantly impact the
attack accuracy. With 1000 queries, attack accuracy is up
to 80% for the 0:100 split and ≈59% for 30:70 split.
Attribute distribution and number of output classes.
Figure 4 shows the results for the GCN trained on the
Amazon dataset for 2, 6 and 11 output classes for the
review score. We evaluate for all the splits between 0:100
to 100:0. First, we observe that the attack accuracy drops
as the ratio of the sensitive attribute values changes from
0:100 to 50:50 and increases again gradually from 50:50
to 100:0. This is because our primary attack is designed
to identify the dominant attribute value. For inferring
the distribution in the balanced range, the attacker can
perform our ﬁne-grained attack discussed in Section 6.3.
Next, we observe that the attack accuracy is lower for a
higher number of output classes such as 6 and 11 as com-
pared to 2. This could be due to lower number of input
features that are given to the attack classiﬁer when there
are lower number of output classes — the classiﬁer is
able to learn the attribute distribution better when the in-
formation is divided among fewer features thus resulting
in a lower dimension input. Similar trends are observed
in Figure 5 in Appendix when A is used during training.
7 Defenses
In the previous section, we saw that removing the sen-
sitive attribute from the dataset is not an effective solution
due to the correlations that exist between the attributes.
Disentangling data representation through variational-
auto-encoders [11, 23, 56] allows one to obtain mutu-
2698    30th USENIX Security Symposium
USENIX Association
2003004005006007008009001000No. of queries used for attack0.500.550.600.650.700.750.80Attack AccuracyAttack Results with increasing queriesData split0:10030:700:10010:9020:8030:7040:6050:5060:4070:3080:2090:10100:0Data split of the ProductType attribute0.50.60.70.80.91.0Attack AccuracyAttack Results with 1000 queries (Without A)output classes2611ally independent variables for representing the data. Intu-
itively, the removal of this variable before decoding the
record for further down-stream tasks would lead to better
censorship. Similarly, adversarial learning has also been
proposed for learning a privacy-preserving data ﬁlter in a
multi-party setting [21] and a privacy-preserving record
representation [16]. Unfortunately, such techniques do
not have provable worst-case guarantees and have been
shown ineffective in the privacy context [53].
Differential privacy [15] guarantees record-level pri-
vacy, that is, whether a particular record is in their dataset
or not. However, differential privacy does not protect
population-level properties of a dataset [9, 15]. In fact, a
differentially private algorithm with high utility aims to
learn population properties without sacriﬁcing individ-
ual privacy. Group differential privacy is an extension of
differential privacy that considers the privacy of a group
of k correlated records, as a result one way of achieving
it is to increase, for example, Laplace noise, proportional
to k. Though it can be applied to preserve the privacy
of all records in each party’s dataset by setting k to the
size of each party’s data, depending on the setting, it can
effect utility as even with k = 1 accuracy of models have
been shown to drop [6, 52].
In settings with more than two parties, where the at-
tacker controls only one party, the signal weakens as it is
harder for the adversary to identify the mapping between