on their path to the Internet, and advertize their addresses
accordingly. This is quite necessary in the current Internet
since about 60%-80% of peers are found to be behind NAT.
We have included some data to show the distribution of the
NAT types in section 4.
Also, a signiﬁcant number of peers are protected by ﬁre-
walls. For this reason, the PPLive software carefully pace
the upload rate and request rate to make sure the ﬁrewalls
will not consider PPLive peers as malicious attackers.
It is important for the P2P-VoD system to include mecha-
nisms to authenticate content, so that the system is resistant
to pollution attacks [10]. Such authentication can be imple-
mented based on message digest or digital signature. In the
case of a proprietary system, it is not diﬃcult for the oper-
ator to implement some means to distribute a key for each
movie.
Authentication can be done at two levels: chunk level or
piece level. If chunk level, authentication is done only when
a chunk is created and is stored to the hard disc. In this
case, some pieces may be polluted and cause poor viewing
experience locally at a peer. However, further pollution is
stopped because the peer would detect a chunk is bad and
discard it. The advantage of the chunk level authentication
is its minimal overhead.
Chunk-level authentication has at least two signiﬁcant
drawbacks. Sometime, polluted pieces may cause more dam-
age than poor viewing experience, for example it may crash
or freeze the player. Secondly, chunk is a rather large seg-
ment of content. There is some non-zero probability that a
piece is bad not due to pollution; but this would cause the
entire chunk to be discarded. For these reasons, it is wise
to do authentication at the piece level to ensure good video
quality. In the current version of PPLive VoD, a weaker form
of piece level authentication is also implemented, leveraging
on the same key used for chunk level authentication.
3. PERFORMANCE METRICS AND MEA-
SUREMENT METHODOLOGY
An important goal of this paper is to study a large-scale
real-life P2P-VoD system through measurement. A large
2Similar to the STUN protocol.
User ID Movie ID Start time End time
Start pos.
Table 2: MVR format
Start
watching
from the
beginning
Jump to
30% of
the
movie
Jump to
65% of
the
movie
Stop
watching
amount of information was measured, and we will focus on
those aspects most interesting. In this section, we ﬁrst de-
scribe what we try to measure and the metrics used for mea-
surement. Then we explain how we collected the data.
3.1 What to measure
What we measure can be roughly grouped into three areas:
1. User behavior : This includes the user arrival patterns,
and how long they stayed watching a movie. One ma-
jor diﬀerence between VoD and live streaming is that
users may not be watching the same thing at the same
time, but there is often some overlap (for example,
they are watching diﬀerent parts of the same movie).
Another important diﬀerence is in VoD users can jump
from one position to another in a movie, while skipping
the content in between. Understanding these kinds of
user behavioral information can be used to improve
the design of the replication strategy.
2. External performance metrics: This includes user sat-
isfaction and server load. Server load can be objec-
tively deﬁned in terms of CPU, memory, and band-
width resources needed from the server at diﬀerent
loading (number of peers). The deﬁnition of user sat-
isfaction is more subjective. These metrics are used to
measure the system performance perceived externally.
3. Health of replication: In comparison to P2P streaming
or ﬁle downloading, replication is a new aspect of P2P
technology designed for VoD. It is therefore particu-
larly important and interesting to design yardsticks to
measure how well a P2P-VoD system is replicating a
content. This is an internal metric used to infer how
well an important component of the system is doing.
3.2 Measuring User Behavior
In a P2P-VoD system, a typical user does not just watch
a movie sequentially, but rather, he is allowed to jump from
one movie to another; and when viewing one movie, is al-
lowed to skip parts of the movie and jump to a new position
in the movie. Therefore, the most basic user activity is the
continuous viewing of a stretch of a movie. This basic ac-
tivity is recorded in a movie viewing record (MVR). The
important parts of an MVR record are shown in Table 2,
where ST is the starting time, ET is the ending time, and
SP is the starting position.
Each user has a unique User ID, which is included as part
of the MVR. To ensure uniqueness, part of this ID is derived
from the hardware component serial numbers (HCSN) of the
computer (or the memory module on that computer) that is
running the P2P-VoD client software. Each movie also has
a unique ID, which is usually a hash of the movie content.
The MVR records the starting time (ST) and ending time
(ET), as well as the starting position (SP) of a particular
continuous viewing activity. Based on these three numbers,
the ending position can also be computed. In most cases, as
soon as a user ﬁnishes recording one MVR, a new MVR is
initialized to record the next viewing activity.
t0
MVR1:
MVR2:
MVR3:
t1
t2
UID MID ST
U1
t0
t1
U1
t2
U1
M1
M1
M1
T
t3
ET
SP
0%
t1
t2 30%
t3 65%
Figure 1: Example to show how MVRs are gener-
ated
Figure 1 illustrates how MVRs are generated based on
a sequence of user viewing activities. In this example, the
user’s ID is U1 and the movie’s ID is M1. The user starts
to watch the movie at t0 from the beginning of the movie.
After watching for a duration of (t1 − t0), he/she jumps to
position 30% of the movie (i.e., if the length of the movie
is 5000 seconds, this user jumps to the 1500th second of
the movie). At t2, the user jumps again to start watching
at position 65% (or the 3250th second of the 5000-second
movie) and he stops watching at t3. As a result of this
sequence of activities, three MVRs are generated as shown
in Figure 1.
Clearly, a complete user behavior information can be rep-
resented by these MVR records. We explain later how this
information is collected.
3.3 User satisfaction
From the MVRs, we can determine the total viewing time
of each user for each movie. A very crude statistic for the
P2P-VoD service is the total viewing time (TVT) for all users
and all movies.
TVT gives us the total amount of service provided by the
P2P-VoD system, but it does not tell us the level of user
satisfaction. How to measure user satisfaction is in itself an
interesting research problem.
Let us ﬁrst consider a simple version of user satisfaction.
Given an MVR, actually part of the duration between start
time and end time is not spent on viewing, but on buﬀer-
ing. Assume this information is captured by the P2P-VoD
client software together with the MVR, denoted as BT (for
buﬀering time). Let R(m, i) denote the set of all MVRs for a
given movie m and user i, and n(m, i) the number of MVRs
in R(m, i). Let r denote one of the MVRs in R(m, i). Then
we can deﬁne the ﬂuency F (m, i) for a movie m and user i
to be:
.
(1)
F (m, i) =
(cid:2)
(cid:2)
r∈R(m,i)(r(ET ) − r(ST ) − r(BT ))
r∈R(m,i)(r(ET ) − r(ST ))
In simple words, F (m, i) measures the fraction of time a user
spends watching a movie out of the total time he/she spends
waiting for and watching that movie.
Ideally, we want something more reﬁned than ﬂuency to
guage the user satisfaction. For the time spent viewing a
movie, a user may not be satisﬁed with the quality of the
delivery of the content3. Let us go one step further and
assume that the user gave a grade for the average viewing
quality for an MVR r, denoted as r(Q). Let the value of
the grade be in the range of [0, 1]. The ﬂuency expression
in Eq. (1) can be rewritten in terms of the contribution of
each MVR:
F (m, i) =
n(m,i)(cid:3)
k=1
Wk,
(2)
where each k indexes a particular MVR for movie m and
user i, and
Wk =
(rk(ET ) − rk(ST ) − rk(BT ))
(cid:2)
r∈R(m,i)(r(ET ) − r(ST ))
.
Now, we can deﬁne a more sophisticated version of user sat-
isfaction index, S(m, i), as:
S(m, i) =
n(m,i)(cid:3)
k=1
Wkrk(Q).
(3)
To illustrate, consider the example in Fig. 1, and assume
there is a buﬀering time of 10 (time units) for each MVR.
The ﬂuency can be computed as:
(t1 − t0 − 10) + (t2 − t1 − 10) + (t3 − t2 − 10)
F =
(t3 − t0)
.
.
Suppose the user grade for the three MVR were 0.9, 0.5,
0.9 respectively. Then the user satisfaction index can be
calculated as:
0.9(t1−t0−10)+0.5(t2−t1−10)+0.9(t3−t2−10)
S =
(t3−t0)
In reality, it is not likely (or even possible) for the P2P-
VoD software to get explicit user feedback for his viewing
experience. Instead, what can be done is for the P2P-VoD
client software to infer/estimate user satisfaction for each
MVR based on user actions. For example, if the user’s view-
ing duration for the MVR exceeds a certain minimum time
Tmin, that may indicate that the user is basically satisﬁed
with the viewing and the subsequent jump is not due to poor
viewing quality but due to content reasons. Another exam-
ple is if the termination of the viewing is due to a manual
control to ”STOP” the viewing altogether, it may also be
inferred that the user is likely to be terminating the viewing
session for some other activities rather than poor viewing
quality. Based on these kinds of additional events and infer-
ences, it is possible to estimate a suitable value for r(Q). A
detailed study of this topic, however, is beyond the scope of
this paper. It is a good topic for further research. For the
data analysis in the next section, we simply take ﬂuency as
the indicator for user satisfaction.
3.4 Health of Replication
The health index (for replication) can be deﬁned at three
levels:
a. Movie level : For a given movie m, the movie level
health index is deﬁned simply as the number of ac-
tive peers who have advertised storing chunks of that
movie. This is basically the information that the tracker
collects about movies.
3Note, this is diﬀerent than when the user is not happy with
the content itself.
b. Weighted movie level : The movie level index is very
coarse. Some peers may store a very small part of a
movie but are still counted towards the index. So the
weighted movie level index takes the fraction of chunks
a peer has into account in computing the index. If a
peers stores 50 percent of a movie, it is counted as 0.5.
c. Chunk bitmap level : The movie level indexes do not
show how well individual chunks are replicated. The
chunk level health index is in fact a vector representing
the number of copies each chunk of a movie is stored
by peers in a P2P-VoD system. Given the chunk level
health index, various other statistics can be computed.
For example, the average number of copies of a chunk
in a movie; the minimum number of chunks; the vari-
ance of the number of chunks, and so on.
In this study, we instrumented the P2P-VoD system to
collect the chunk level health index information, to be shown
in the next section.
3.5 Measurement Methodology
We now explain how measurement data are collected in
the P2P-VoD system we studied. The general mechanism
is supported by a log server that collects various sorts of
measurement data from peers. Sometimes, the measurement
data are part of information that the tracker collects, for
example the chunk replication information.
In that case,
peers would send the collected information to the tracker,
and the tracker can then aggregated the information and
pass it on to the log server.
Generally speaking, to avoid a large amount of traﬃc and
a large number of interruptions on the log server, peers col-
lect data and do some amount of aggregation, ﬁltering and
pre-computation before passing them to the log server. For
example, peers do not report individual MVRs as they are
generated. Instead, a peer sends a single report to the log
server when the user generates a “STOP” event (pressing the
STOP button, changing to another movie or turning oﬀ the
client software). Furthermore, the report also includes the
user satisfaction index (in this case the ﬂuency) computed
by the local peer. Based on these MVRs, various other user
behavior information can be deduced by post processing,
either as online or oﬄine.
For replication health index, a peer needs to report its
chunk bitmap to the log server whenever one of the following
events occurs:
1. Some of the chunks or a whole movie is removed from
the storage due to the replication strategy.
2. The local user starts to watch a new movie, and chunks
of the new movie are added to local storage.
3. A refresh timer (pre-deﬁned, e.g. 10 minutes) goes oﬀ.
4. MEASUREMENT RESULTS AND ANAL-
YSIS
In this section, we present the measurement and data anal-
ysis of the P2P-VoD system in PPLive. We summarize the
measurement results into ﬁve categories, namely, statistics
for the video objects, user behavior, system health index,
user satisfaction index, and server loads.
4.1 Statistics on video objects
We have collected the data trace on ten movies from the
P2P-VoD log server. As mentioned before, whenever a peer
selects a movie for viewing, the client software creates the
MVRs and computes the viewing satisfaction index, and
these information are sent to the log server. The collection
of MVRs of a particular movie constitutes the data trace
of that movie. All these data traces were collected from
December 23, 2007 to December 29, 3007 (about one week
worth of trace). For the ease of presentation, we select three
“typical” movies to illustrate the measurement results. Table
3 lists the overall statistics of these three typical movies.
Movie Index:
Movie 1 Movie 2 Movie 3
Total Length (in sec)
No. of Chunks
Total No. of MVRs
Total No. of MVRs with
5100s
121
56157
2820s
67
322311
6600s
151
15094
Start Position = 0
35160
95005
8423
(or # of unique viewers)
Ave. # of Jump
Ave. viewing Duration
for a MVR
Normalized viewing
Duration (normalized
by the movie duration)
1.6
3.4
1.8
829.8s
147.6s
620.2s
16.3%
5.2%
9.4%
Table 3: Overall statistics of the three typical
movies.
Based on these statistics, we have the following observa-
tions:
1. Given that the size of a chunk is about 2 MBytes
(assuming the playback rate is about 380kbps), this
implies that the viewing duration of a chunk is ap-
proximately 40 seconds. Movie 2 is the smallest video
object with a viewing duration of about 45 minutes,
while Movie 3 is the longest video object with a view-
ing duration of about 110 minutes.
2. To determine the most popular movie, we count only
those MVRs whose starting position (SP) is equal to
zero (e.g., MVRs which view the movie at the begin-
ning). From the measurement, one can determine that
Movie 2 is the most popular movie with 95005 users
while Movie 3 is the least popular movie with 8423
users.
3. One interesting statistics we like to extract is the av-
erage number of jumps for a given movie. Note that a
peer generates at least one MVR (with starting posi-
tion being zero) and possibly a number of MVRs due
to viewing jumps. Therefore, the average number of
jumps for a given movie is approximately equal to the
total number of MVRs divided by the total number
of MVRs with starting position being zero. Based on
this computation, we can conclude that Movie 2 has
the highest average number of jumps (3.4) while Movie
1 has the lowest average number of jumps (1.6).
4. Each MVR indicates a viewing activity and one in-
teresting characteristics is to determine the viewing
duration per viewing activity. One can extract this
information by computing the diﬀerence between the
end time (ET) and start time (ST) of each MVR; by
averaging over all MVRs of a movie, we obtain the
average viewing duration per viewing action. Table 3
shows that Movie 1 has the largest viewing duration
(829.8s), and this is consistent since Movie 1 has the
least average number of jumps.
5. From the derived statistics mentioned above, we can
also derive the normalized viewing duration (or aver-
age viewing duration divided by the movie length) and
this is listed in the last row of Table 3, which shows
that for Movie 1, each viewing length lasts on average
16.3% of the movie duration, while for Movie 2, each
viewing length is around 5.2% of the movie duration.
4.2 Statistics on user behavior
4.2.1 Interarrival time distribution of viewers
One characteristic we are interested to study is the view-
ers’ interarrival time to a particular movie. Again, this can
be derived from the trace. Given a particular movie, one
can extract all those MVRs with a start time (ST) equal to
zero. These MVRs represent viewers who start to watch the
video object from the beginning. Given this set of MVRs,
we can sort them in an increasing order based on the start
time (ST) ﬁeld. The diﬀerences of the ST ﬁelds between to
consecutive MVRs represent the interarrival times of view-
ers.
The PDFs of the interarrival times of three movies
The average interarrival time
Movie1:  19.07s
Movie2:    7.25s
Movie3:  79.04s
Movie 2
Movie 1
Movie 3
0.18
0.16
0.14
0.12
0.1
0.08
0.06
0.04
0.02
0
n
o
i
t
c