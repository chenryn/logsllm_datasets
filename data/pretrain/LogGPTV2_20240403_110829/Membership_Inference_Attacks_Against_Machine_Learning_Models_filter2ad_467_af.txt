### Evaluation of Mitigation Strategies for Membership Inference Attacks

#### Experimental Results
The following table (Table III) presents the accuracy, precision, and recall of membership inference attacks on target models with various mitigation techniques applied to the Purchase and Texas Hospital-Stay datasets, each with 100 classes. The relative reduction in these metrics indicates the effectiveness of the mitigation strategies.

| No Mitigation | Top k = 3 | Top k = 1 | Top k = 1 label | Rounding d = 3 | Rounding d = 1 | Temperature t = 5 | Temperature t = 20 | L2 λ = 1e−4 | L2 λ = 5e−4 | L2 λ = 1e−3 | L2 λ = 5e−3 |
|---------------|------------|------------|-----------------|-----------------|-----------------|--------------------|--------------------|--------------|--------------|--------------|--------------|
| 0.73          | 0.83       | 0.81       | 0.79            | 0.76            | 0.80            | 0.73               | 0.66               | 0.52         | 0.77         | 0.77         | 0.76         |
| 0.67          | 0.77       | 0.75       | 0.77            | 0.76            | 0.74            | 0.69               | 0.64               | 0.52         | 0.95         | 0.95         | 0.95         |
| 0.93          | 0.95       | 0.96       | 0.83            | 0.76            | 0.92            | 0.86               | 0.73               | 0.53         |              |              |              |

#### Mitigation Techniques

1. **Rounding**:
   - **Description**: Round the prediction vector down to \(d\) floating-point digits.
   - **Effect**: Reducing \(d\) decreases the amount of information leaked by the model.

2. **Increase Entropy**:
   - **Description**: Modify or add a softmax layer to increase the normalizing temperature \(t > 0\).
   - **Effect**: This increases the entropy of the prediction vector. For very large \(t\), the output becomes nearly uniform, leaking no information.
   - **Formula**: For a logits vector \(z\), the \(i\)-th output of the softmax function with temperature \(t\) is given by:
     \[
     \frac{e^{z_i / t}}{\sum_{j} e^{z_j / t}}
     \]

3. **Regularization**:
   - **Description**: Use L2-norm regularization to penalize large parameters.
   - **Effect**: Regularization helps prevent overfitting and reduces information leakage about the training dataset.
   - **Formula**: Add \(\lambda \sum_i \theta_i^2\) to the loss function, where \(\theta_i\) are the model's parameters.
   - **Implementation**: Varying \(\lambda\) values were tested, with larger \(\lambda\) values having a stronger regularization effect.

#### Evaluation Setup

- **Baseline Model**: A neural network with one hidden layer, 256 units for the Purchase dataset, and 1,000 units for the Texas Hospital-Stay dataset, using Tanh as the activation function.
- **Attack Assumption**: Black-box access to the resulting models.

#### Key Findings

- **Accuracy Impact**: The implemented mitigation strategies did not significantly reduce the target model's prediction accuracy. In some cases, regularization even improved accuracy.
- **Robustness of Attack**: Filtering out low-probability classes and limiting the prediction vector to the top 1 or 3 most likely classes did not fully mitigate the attack. Even restricting the prediction vector to a single label (most likely class) was insufficient to prevent membership inference.
- **Regularization Trade-off**: While regularization can improve generalization and reduce information leakage, excessive regularization (e.g., \(\lambda = 1e^{-2}\) for the Purchase dataset and \(\lambda = 5e^{-3}\) for the Texas Hospital-Stay dataset) can significantly reduce test accuracy.

#### Related Work

- **Attacks on Statistical and Machine Learning Models**: Previous works have explored inferring general statistical information about the training dataset, but our focus is on specific records.
- **Model Inversion**: Unlike our approach, model inversion does not distinguish between members and non-members and produces semantically meaningless results for diverse classes.
- **Model Extraction**: Extracting model parameters can be a stepping stone for inferring information about the training dataset, but this is highly specific to certain types of models.
- **Privacy-Preserving Machine Learning**: Techniques such as secure multiparty computation (SMC) and differential privacy aim to limit information leakage during training, but differentially private models are more robust against membership inference attacks.

#### Conclusions

We have designed, implemented, and evaluated the first membership inference attack against machine learning models, including black-box models trained in the cloud. Our key technical innovation is the shadow training technique, which trains an attack model to distinguish the target model's outputs on members versus non-members of its training dataset. This work has significant practical privacy implications, particularly for sensitive datasets like hospital-stay records.

#### Acknowledgments

Thanks to Adam Smith for explaining differential privacy and the state of the art in membership inference attacks based on explicit statistics. This work was supported by the NSF grant 1409442 and a Google Research Award.

#### References

[References listed here as provided in the original text.]

---

This revised version provides a clearer, more structured, and professional presentation of the content, making it easier to understand and follow.