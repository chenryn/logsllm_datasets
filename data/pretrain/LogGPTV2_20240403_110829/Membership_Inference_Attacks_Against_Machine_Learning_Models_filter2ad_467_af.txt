0.73
0.83
0.81
0.79
0.76
0.80
0.73
0.66
0.52
No Mitigation
Top k = 3
Top k = 1
Top k = 1 label
Rounding d = 3
Rounding d = 1
Temperature t = 5
Temperature t = 20
L2 λ = 1e − 4
L2 λ = 5e − 4
L2 λ = 1e − 3
L2 λ = 5e − 3
TABLE III: The accuracy of the target models with different mitiga-
tion techniques on the purchase and Texas hospital-stay datasets (both
with 100 classes), as well as total accuracy, precision, and recall of
the membership inference attack. The relative reduction in the metrics
for the attack shows the effectiveness of the mitigation strategy.
0.77
0.77
0.76
0.67
0.77
0.75
0.77
0.76
0.74
0.69
0.64
0.52
Attack
Recall
0.95
0.95
0.95
0.93
0.95
0.96
0.83
0.76
0.92
0.86
0.73
0.53
vector down to d ﬂoating point digits. The smaller d is, the
less information the model leaks.
Increase entropy of the prediction vector. One of the signals
that membership inference exploits is the difference between
the prediction entropy of the target model on its training inputs
versus other inputs. As a mitigation technique for neural-
network models, we can modify (or add) the softmax layer and
increase its normalizing temperature t > 0. The softmax layer
converts the logits computed for each class into probabilities.
For the logits vector z, the ith output of the softmax function
with temperature t is
zj /t . This technique, also used
in knowledge distillation and information transfer between
models [20], would increase the entropy of the prediction
vector. Note that for a very large temperature, the output
becomes almost uniform and independent of its input, thus
leaking no information.
Use regularization. Regularization techniques are used to
overcome overﬁtting in machine learning. We use L2-norm
standard regularization that penalizes large parameters by
adding λ
to the model’s loss function, where θis are
model’s parameters. We implement this technique with various
values for the regularization factor λ. The larger λ is, the
stronger the effect of regularization during the training.
e
(cid:2)
j e
θ2
i
(cid:2)
zi /t
i
B. Evaluation of mitigation strategies
To evaluate the effectiveness of different mitigation strate-
gies, we implemented all of them in locally trained mod-
15
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:20:35 UTC from IEEE Xplore.  Restrictions apply. 
els over which we have full control. The inference attack,
however, still assumes only black-box access to the resulting
models. The baseline model for these experiments is a neural
network with one hidden layer with 256 units (for the purchase
dataset) and 1,000 units (for the Texas hospital-stay dataset).
We use Tanh as the activation function.
Table III shows the results of our evaluation. It compares
different mitigation strategies based on how they degrade the
accuracy of our attack relative to the attack on a model
that does not use any mitigation. The mitigation strategies
that we implemented did not impose any cost on the target
model’s prediction accuracy, and in the case of regularization,
the target model’s prediction accuracy increased as expected.
Note that more regularization (by increasing λ even further)
would potentially result in a signiﬁcant reduction of the target
model’s test accuracy, even if it foils membership inference.
This is shown in the table for λ = 1e − 2 on the purchase
dataset, and for λ = 5e− 3 on the Texas hospital stay dataset.
Overall, our attack is robust against these mitigation strate-
gies. Filtering out low-probability classes from the predic-
tion vector and limiting the vector to the top 1 or 3 most
likely classes does not foil the attack. Even restricting the
prediction vector to a single label (most likely class),
which is the absolute minimum a model must output to
remain useful, is not enough to fully prevent membership
inference. Our attack can still exploit the mislabeling behavior
of the target model because members and non-members of
the training dataset are mislabeled differently (assigned to
different wrong classes). If the prediction vector contains
probabilities in addition to the labels, the model leaks even
more information that can be used for membership inference.
the mitigation methods are not suitable for
machine-learning-as-service APIs used by general applications
and services. Regularization, however, appears to be neces-
sary and useful. As mentioned above, it (1) generalizes the
model and improves its predictive power and (2) decreases
the model’s information leakage about its training dataset.
However, regularization needs to be deployed carefully to
avoid damaging the model’s performance on the test datasets.
Some of
IX. RELATED WORK
Attacks on statistical and machine learning models.
In [2],
knowledge of the parameters of SVM and HMM models is
used to infer general statistical information about the training
dataset, for example, whether records of a particular race were
used during training. By contrast, our inference attacks work
in a black-box setting, without any knowledge of the model’s
parameters, and infer information about speciﬁc records in the
training dataset, as opposed to general statistics.
Homer et al. [21] developed a technique, which was further
studied in [3], [15], for inferring the presence of a particular
genome in a dataset, based on comparing the published statis-
tics about this dataset (in particular, minor allele frequencies)
to the distribution of these statistics in the general population.
By contrast, our inference attacks target
trained machine
learning models, not explicit statistics.
16
Fig. 13: Images produced by model inversion on a trained CIFAR-10
model. Top: airplane, automobile, bird, cat, deer. Bottom: dog, frog,
horse, ship, truck. The images do not correspond to any speciﬁc
image from the training dataset, are not human-recognizable, and at
best (e.g., the truck class image) are vaguely similar to the average
image of all objects in a given class.
Other attacks on machine learning include [7], where the
adversary exploits changes in the outputs of a collaborative
recommender system to infer inputs that caused these changes.
These attacks exploit temporal behavior speciﬁc to the recom-
mender systems based on collaborative ﬁltering.
Model inversion. Model inversion [16], [17] uses the output
of a model applied to a hidden input to infer certain features
of this input. See [27] for a detailed analysis of this attack and
an explanation of why it does not necessarily entail a privacy
breach. For example, in the speciﬁc case of pharmacogenetics
analyzed in [17], the model captures the correlation between
the patient’s genotype and the dosage of a certain medicine.
This correlation is a valid scientiﬁc fact that holds for all
patients, regardless of whether they were included in the
model’s training dataset or not. It is not possible to prevent
disclosure due to population statistics [14].
In general, model inversion cannot tell whether a particular
record was used as part of the model’s training dataset. Given
a record and a model, model inversion works exactly the same
way when the record was used to train the model and when
it was not used. In the case of pharmacogenetics [17], model
inversion produces almost identical results for members and
non-members. Due to the overﬁtting of the model, the results
are a little (4%) more accurate for the members, but this
accuracy can only be measured in retrospect, if the adversary
already knows the ground truth (i.e., which records are indeed
members of the model’s training dataset). By contrast, our goal
is to construct a decision procedure that distinguishes members
from non-members.
Model inversion has also been applied to face recognition
models [16]. In this scenario, the model’s output is set to 1
for class i and 0 for the rest, and model inversion is used to
construct an input that produces these outputs. This input is
not an actual member of the training dataset but simply an
average of the features that “characterize” the class.
In the face recognition scenario—and only in this speciﬁc
scenario—each output class of the model is associated with a
single person. All training images for this class are different
photos of that person,
thus model inversion constructs an
artiﬁcial image that is an average of these photos. Because
they all depict the same person, this average is recognizable
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:20:35 UTC from IEEE Xplore.  Restrictions apply. 
(by a human) as that person. Critically, model inversion does
not produce any speciﬁc image from the training dataset, which
is the deﬁnition of membership inference.
If the images in a class are diverse (e.g., if the class contains
multiple individuals or many different objects), the results of
model inversion as used in [16] are semantically meaningless
and not recognizable as any speciﬁc image from the training
dataset. To illustrate this, we ran model
inversion against
a convolutional neural network13 trained on the CIFAR-10
dataset, which is a standard benchmark for object recognition
models. Each class includes different images of a single type
of object (e.g., an airplane). Figure 13 shows the images
“reconstructed” by model inversion. As expected, they do not
depict any recognizable object, let alone an image from the
training dataset. We expect similar results for other models,
too. For the pharmacogenetics model mentioned above, this
form of model inversion produces an average of different
patients’ genomes. For the model that classiﬁes location traces
into geosocial proﬁles (see Section VI-A),
it produces an
average of the location traces of different people. In both
cases, the results of model inversion are not associated with
any speciﬁc individual or speciﬁc training input.
In summary, model inversion produces the average of the
features that at best can characterize an entire output class.
It does not (1) construct a speciﬁc member of the training
dataset, nor (2) given an input and a model, determines if this
speciﬁc input was used to train the model.
Model extraction. Model extraction attacks [32] aim to
extract the parameters of a model trained on private data.
The attacker’s goal is to construct a model whose predictive
performance on validation data is similar to the target model.
Model extraction can be a stepping stone for inferring
information about the model’s training dataset. In [32], this is
illustrated for a speciﬁc type of models called kernel logistic
regression (KLR) [38]. In KLR models, the kernel function
includes a tiny fraction of the training data (so called “import
points”) directly into the model. Since import points are
parameters of the model, extracting them results in the leakage
of that particular part of the data. This result is very speciﬁc
to KLR and does not extend to other types of models since
they do not explicitly store training data in their parameters.
Even for KLR models, leakage is not quantiﬁed other than
via visual similarity of a few chosen import points and “the
closest (in L1 norm) extracted representers” on the MNIST
dataset of handwritten digits. In MNIST, all members of a
class are very similar (e.g., all members of the ﬁrst class are
different ways of writing digit “1”). Thus, any extracted digit
must be similar to all images in its class, whether this digit
was in the training set or not.
Privacy-preserving machine learning. Existing literature on
privacy protection in machine learning focuses mostly on how
to learn without direct access to the training data. Secure
multiparty computation (SMC) has been used for learning
decision trees [26], linear regression functions [11], Naive
13https://github.com/Lasagne/Recipes/blob/master/modelzoo/cifar10 nin.py
17
Bayes classiﬁers [33], and k-means clustering [22]. The goal
is to limit information leakage during training. The training
algorithm is the same as in the non-privacy-preserving case,
thus the resulting models are as vulnerable to inference attacks
as any conventionally trained model. This also holds for the
models trained by computing on encrypted data [4], [6], [35].
Differential privacy [12] has been applied to linear and
logistic regression [8], [37], support vector machines [28], risk
minimization [5], [9], [34], deep learning [1], [30], learning
an unknown probability distribution over a discrete population
from random samples [10], and releasing hyper-parameters
and classiﬁer accuracy [25]. By deﬁnition, differentially pri-
vate models limit
the success probability of membership
inference attacks based solely on the model, which includes
the attacks described in this paper.
X. CONCLUSIONS
We have designed, implemented, and evaluated the ﬁrst
membership inference attack against machine learning models,
notably black-box models trained in the cloud using Google
Prediction API and Amazon ML. Our attack is a general,
quantitative approach to understanding how machine learning
models leak information about their training datasets. When
choosing the type of the model to train or a machine learning
service to use, our attack can be used as one of the selection
metrics.
Our key technical innovation is the shadow training tech-
nique that trains an attack model to distinguish the target
model’s outputs on members versus non-members of its train-
ing dataset. We demonstrate that shadow models used in this
attack can be effectively created using synthetic or noisy data.
In the case of synthetic data generated from the target model
itself, the attack does not require any prior knowledge about
the distribution of the target model’s training data.
Membership in hospital-stay and other health-care datasets
is sensitive from the privacy perspective. Therefore, our results
have substantial practical privacy implications.
Acknowledgments. Thanks to Adam Smith for explaining
differential privacy and the state of the art in membership
inference attacks based on explicit statistics.
This work was supported by the NSF grant 1409442 and a
Google Research Award.
REFERENCES
[1] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Tal-
war, and L. Zhang, “Deep learning with differential privacy,” in CCS,
2016.
[2] G. Ateniese, L. V. Mancini, A. Spognardi, A. Villani, D. Vitali, and
G. Felici, “Hacking smart machines with smarter ones: How to extract
meaningful data from machine learning classiﬁers,” International Jour-
nal of Security and Networks, vol. 10, no. 3, pp. 137–150, 2015.
[3] M. Backes, P. Berrang, M. Humbert, and P. Manoharan, “Membership
privacy in MicroRNA-based studies,” in CCS, 2016.
[4] M. Barni, P. Failla, R. Lazzeretti, A. Sadeghi, and T. Schneider, “Privacy-
preserving ECG classiﬁcation with branching programs and neural
networks,” Trans. Info. Forensics and Security, vol. 6, no. 2, pp. 452–
468, 2011.
[5] R. Bassily, A. Smith, and A. Thakurta, “Private empirical risk minimiza-
tion: Efﬁcient algorithms and tight error bounds,” in FOCS, 2014.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:20:35 UTC from IEEE Xplore.  Restrictions apply. 
2000.
2015.
[6] J. Bos, K. Lauter, and M. Naehrig, “Private predictive analysis on
encrypted medical data,” J. Biomed. Informatics, vol. 50, pp. 234–243,
2014.
[7] J. Calandrino, A. Kilzer, A. Narayanan, E. Felten, and V. Shmatikov,
““You might also like:” Privacy risks of collaborative ﬁltering,” in S&P,
2011.
[8] K. Chaudhuri and C. Monteleoni, “Privacy-preserving logistic regres-
sion,” in NIPS, 2009.
[9] K. Chaudhuri, C. Monteleoni, and A. Sarwate, “Differentially private
empirical risk minimization,” JMLR, vol. 12, pp. 1069–1109, 2011.
[10] I. Diakonikolas, M. Hardt, and L. Schmidt, “Differentially private
learning of structured discrete distributions,” in NIPS, 2015.
[11] W. Du, Y. Han, and S. Chen, “Privacy-preserving multivariate statistical
analysis: Linear regression and classiﬁcation.” in SDM, 2004.
[12] C. Dwork, “Differential privacy,” in Encyclopedia of Cryptography and
Security. Springer, 2011, pp. 338–340.
[13] C. Dwork, F. McSherry, K. Nissim, and A. Smith, “Calibrating noise to
sensitivity in private data analysis,” in TCC, 2006.
[14] C. Dwork and M. Naor, “On the difﬁculties of disclosure prevention in
statistical databases or the case for differential privacy,” J. Privacy and
Conﬁdentiality, vol. 2, no. 1, pp. 93–107, 2010.
[15] C. Dwork, A. Smith, T. Steinke, J. Ullman, and S. Vadhan, “Robust
traceability from trace amounts,” in FOCS, 2015.
[16] M. Fredrikson, S. Jha, and T. Ristenpart, “Model inversion attacks that
exploit conﬁdence information and basic countermeasures,” in CCS,
2015.
[17] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and T. Ristenpart,
“Privacy in pharmacogenetics: An end-to-end case study of personalized
Warfarin dosing,” in USENIX Security, 2014.
[18] M. Hardt, B. Recht, and Y. Singer, “Train faster, generalize better:
Stability of stochastic gradient descent,” in ICML, 2016.
[19] T. Hastie, R. Tibshirani, J. Friedman, and J. Franklin, “The elements
inference and prediction,” The
of statistical
Mathematical Intelligencer, vol. 27, no. 2, pp. 83–85, 2005.
learning: Data mining,
[20] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
network,” arXiv:1503.02531, 2015.
[21] N. Homer, S. Szelinger, M. Redman, D. Duggan, W. Tembe,
J. Muehling, J. V. Pearson, D. A. Stephan, S. F. Nelson, and D. W. Craig,
“Resolving individuals contributing trace amounts of DNA to highly
complex mixtures using high-density SNP genotyping microarrays,”
PLoS Genetics, vol. 4, no. 8, 2008.
[22] G. Jagannathan and R. Wright, “Privacy-preserving distributed k-means
clustering over arbitrarily partitioned data,” in KDD, 2005.
[23] P. Jain, V. Kulkarni, A. Thakurta, and O. Williams, “To drop or not
to drop: Robustness, consistency and differential privacy properties of
dropout,” arXiv:1503.02031, 2015.
[24] A. Krizhevsky, “Learning multiple layers of features from tiny images,”
Master’s thesis, University of Toronto, 2009.
[25] M. J. Kusner, J. R. Gardner, R. Garnett, and K. Q. Weinberger,
“Differentially private Bayesian optimization,” in ICML, 2015.
[26] Y. Lindell and B. Pinkas, “Privacy preserving data mining,” in CRYPTO,
[27] F. McSherry,
“Statistical
inference
considered
harmful,”
https://github.com/frankmcsherry/blog/blob/master/posts/2016-06-
14.md, 2016.
[28] B. Rubinstein, P. Bartlett, L. Huang, and N. Taft, “Learning in a large
function space: Privacy-preserving mechanisms for SVM learning,” J.
Privacy and Conﬁdentiality, vol. 4, no. 1, p. 4, 2012.
[29] S. Sankararaman, G. Obozinski, M. I. Jordan, and E. Halperin, “Ge-
nomic privacy and limits of individual detection in a pool,” Nature
Genetics, vol. 41, no. 9, pp. 965–967, 2009.
[30] R. Shokri and V. Shmatikov, “Privacy-preserving deep learning,” in CCS,
[31] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-
dinov, “Dropout: A simple way to prevent neural networks from over-
ﬁtting,” JMLR, vol. 15, no. 1, pp. 1929–1958, 2014.
[32] F. Tram`er, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, “Stealing
machine learning models via prediction APIs,” in USENIX Security,
2016.
[33] J. Vaidya, M. Kantarcıo˘glu, and C. Clifton, “Privacy-preserving Naive
Bayes classiﬁcation,” VLDB, vol. 17, no. 4, pp. 879–898, 2008.
[34] M. Wainwright, M. Jordan, and J. Duchi, “Privacy aware learning,” in
NIPS, 2012.
[35] P. Xie, M. Bilenko, T. Finley, R. Gilad-Bachrach, K. Lauter, and
M. Naehrig, “Crypto-nets: Neural networks over encrypted data,”
arXiv:1412.6181, 2014.
[36] D. Yang, D. Zhang, and B. Qu, “Participatory cultural mapping based on
collective behavior data in location-based social networks,” ACM TIST,
vol. 7, no. 3, p. 30, 2016.
[37] J. Zhang, Z. Zhang, X. Xiao, Y. Yang, and M. Winslett, “Functional
mechanism: Regression analysis under differential privacy,” VLDB,
vol. 5, no. 11, pp. 1364–1375, 2012.
[38] J. Zhu and T. Hastie, “Kernel logistic regression and the import vector
machine,” in NIPS, 2001.
18
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:20:35 UTC from IEEE Xplore.  Restrictions apply.