environment: grant tables for sharing memory with other
domains including dom0, a netfront driver for packet I/O,
event channels (Xen interrupts) and the Xen store driver. In
addition, MiniOS has a single address space, no kernel/user
separation, and runs a co-operative scheduler, reducing con-
text switch costs.
In order to build and link Click and MiniOS we ﬁrst
needed to have a Linux independent c++ cross-compiler (Click
is written in c++). To do so, we built a new toolchain (gcc,
ld, ar, etc) that uses the platform independent newlibc li-
brary instead of glibc.
In addition, we had to adapt certain parts of Click to work
in a MiniOS environment instead of a Linux or FreeBSD
one. These include the creation of the ClickOSControl el-
ement previously mentioned for dealing with Click element
handlers, as well as new Click elements that act as network
devices that can talk to the MiniOS netfront driver (see next
section).
2.2 ClickOS Networking
Xen has a split network driver model, whereby a netback
driver running in a driver domain talks to hardware de-
vices and exports a common, ring-based API; and a netfront
driver running in a guest domain (e.g., ClickOS) talks to the
netback driver via shared memory (i.e., the ring). This split
model allows guest domains to have access to hardware de-
vices without having to themselves host their drivers.
Figure 3: Optimized networking: modiﬁed net-
front driver, revamped network back-end using the
netmap API, and an improved VALE-based software
switch.
Under a typical Xen set-up, a network card is linked to
a virtual network device called a vif via the Linux bridge
module or in later Xen versions Open vSwitch (ﬁgure 2).
When a packet is received, it is forwarded to the vif whose
MAC address matches that of the packet’s destination. The
device then takes the packet and queues it at the netback
driver. At a later point in time, one of the netback driver
threads picks up the packet and puts it on the shared ring,
notifying the netfront driver in the process; packets sent out
follow a similar path in the opposite direction.
In order to interact with the netfront driver, we created
two new Click elements, FromNetfront and ToNetfront. The
ﬁrst of these takes care of initializing a network device and,
each time it is scheduled, retrieves burst number of packets
from the netfront driver, where burst is a conﬁguration pa-
rameter. The ToNetfront element is pretty straightforward,
simply calling the netfront’s transmit function.
Without optimizations, this network data path performs
rather poorly, in the range of only 8 Kp/s for maximum-sized
packets, and even with a Linux-based vm we experienced 2.9
Gb/s, conﬁrming the ﬁgure given in [18]. To push this up
to the 10Gb supported by our cards, we had to undertake a
number of improvements 3.
First, we optimized the netfront driver by introducing two
mechanisms: we changed the driver’s receive function to poll
for packets from the MiniOS thread running Click, rather
than be interrupt driven, and we added a burst parameter
to process packets in batches. Second, we re-used the grants
that receive buﬀers are given and keep them for the lifetime
of the network device (a grant is Xen’s way of allowing two
domains to share memory).
The next bottleneck was the netback driver and the ring-
based API. Optimizing this required an overhaul of the Xen
backplane. In greater detail, we got rid of the vifs and re-
placed the netback driver with a netmap-based [20] one that
directly maps the network buﬀers of each port of the back-
end software switch onto a vm’s local memory4. This pro-
vides us a much more direct network path between the NIC
and the vm and thus better performance.
The ﬁnal major bottleneck was the software switch (Open
vSwitch in recent Xen releases), which has been shown to
yield poor performance, at most 300 Kp/s for the kernel ver-
line rate is roughly 810 Kp/s for maximum-sized
3Note:
packets and 14.8 Mp/s for minimum-sized ones.
4We chose netmap since it is able to handle packet transfers
at rates of 10Gb/s and higher while consuming relatively
few CPU cycles.
69sion [21]. We replaced this switch with VALE [10], a netmap-
based switch designed for high-speed rates, and adapted it
to be able to interact with the ClickOS netfront driver.
3. EVALUATION
We now present results evaluating various aspects of ClickOS.
We perform all tests on a couple of x86 commodity servers,
each with two quad-core Intel Xeon E5620 2.4GHz proces-
sors (with hyper-threading disabled), 24GB of memory and
an Intel x520-t 10Gb adapter. One server acts as a packet
generator and sink, and the other runs Xen 4.1.2 and the
ClickOS vms. The servers are connected using direct ca-
bling.
ClickOS is compiled with most of the available Click ele-
ments (216/282), many of the remaining ones requiring a ﬁle
system to work (we are in the process of porting a simple ﬁle
system to increase the number of available elements). The
amount and variety of elements means that ClickOS can
support a wide range of middleboxes, including ﬁrewalls,
proxies, load balancers and NATs, to name a few. It is also
relatively easy to extend this functionality: adding a few
new elements we were able to create a carrier-grade NAT, a
software BRAS and a simple IDS.
3.1 Middlebox Instantiation
While ﬂexibility is important, it is imperative that mid-
dleboxes can be instantiated quickly. In order to set up and
run a ClickOS middlebox, several steps take place. First,
the virtual machine is created, which includes reading its
conﬁguration, its image ﬁle, writing a (large) number of en-
tries to the Xen store (e.g., the id of the vm, addresses for
memory allocations, etc) and creating the vm itself. Second,
we “attach” a virtual network device to the vm, which adds
more entries to the Xen store, and connect the device to the
software switch.
Once MiniOS boots, the ClickOS control thread is created.
When this thread receives a new Click conﬁguration, it starts
a new Click thread, which in turn populates the Xen store
with entries used to support element handlers, initializes the
network device that had been previously created, and ﬁnally
sets the middlebox running.
When we ﬁrst started measuring the ClickOS start-up
time we were getting results in the range of several seconds,
but a quick investigation showed that that was due to the
Xen store residing in an NFS-mounted drive; moving it to
a RAM disk lowered this timing to just over a second. The
second optimization was moving from the xm toolstack 5 to
the newer xl one (xm was Python-based and made use of
slow xml-rpc calls to carry out its operations). This change,
along with switching to the newer, more eﬃcient oxenstore
resulted in a reduction of start-up times from roughly 0.86
to 0.21 seconds.
The ﬁnal improvements had to do with trimming unneces-
sary operations from the MiniOS and Click start-up process,
which brought the total time down to about 70 msecs. At
this point, we discovered that reading the compressed vir-
tual machine image was slow; providing an uncompressed
image instead cut down the overall start-up time to about
30 msecs. Table 1 gives a detailed breakdown of the process
up to the creation of the ClickOS control thread (we call
5The toolstack is the user interface used to control Xen; this
includes vm creation.
description
issue create
hypercall
paravirt.
bootloader
prepare
domain boot
parse,
allocate and
boot vm image
write xen store
entries, notify
xen store daemon
init console
function
libxl domain make2
libxl run bootloader
libxl build2 pre
xc dom allocate
xc dom kernel path
xc dom ramdisk
xc dom boot xen init
xc dom parse image
xc dom mem init
xc dom boot mem init
xc dom build image
xc dom boot image
libxl build2 post
time
5.244
0.049
0.089
0.016
0.047
0.001
0.011
0.286
0.007
0.650
7.091
0.707
2.202
init console info
libxl need xenpv qemu
libxl device console add
TOTAL
0.004
0.006
4.371
20.789
Table 1: Costs of creating a ClickOS virtual machine
and booting it up, in milliseconds.
this the ClickOS boot time); creating a network device and
starting the Click middlebox (the start-up time) consumes
an additional 8 msecs or so, roughly 30 msecs in total.
So far the discussion has focused on a single ClickOS vm,
but how long would it take to actually create many of them?
Figure 4 shows boot times for 400 ClickOS vms on a single
server. The ﬁrst vm takes the roughly 21 msecs described
in the table above, ramping up to about 200 msecs for the
400th vm. This shows ClickOS’ ability to quickly instantiate
processing even in the presence of several existing vms.
We further measured how long it takes to install a mid-
dlebox conﬁguration in one of these ClickOS vms. Figure 5
shows such timings when diﬀerent number of vms already
exist in the system. Again, these costs are small: about 7
msecs when 64 vms are already running, all the way up to
21 msecs for 400 vms.
3.2 Networking Performance
The path that packets follow from a ClickOS vm through
the Xen networking system and eventually to the NIC is
complex and involves a number of components (recall ﬁg-
ure 2). The ﬁrst throughput measurements we conducted
were rather disappointing: only about 1% of our 10Gb card
for maximum-sized packets.
In order to identify where the bottlenecks were, we began
with a single ClickOS vm with one CPU core assigned to
it running a simple Click conﬁguration 6 that creates pack-
ets as fast as possible and drops them. The ﬁrst issue was
with Click’s packet generation: after several modiﬁcations
to InfiniteSource that we leave out due to space restric-
tions, we were able to bump its rate from 8.4 Mp/s to 13.2
Mp/s, almost enough to saturate a 10Gb link for all packet
sizes. This allowed us to investigate other problems with the
6InfiniteSource→EtherEncap→Discard
70Figure 4: Time to create and boot
400 ClickOS virtual machines on a
single server.
Figure 5: Time to instantiate
processing in a ClickOS vm once
other vms are running.
Figure 6: Aggregated through-
put when 128 ClickOS virtual