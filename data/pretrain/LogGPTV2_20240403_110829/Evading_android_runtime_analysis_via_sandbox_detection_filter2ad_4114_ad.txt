ro.secure,
from the
default.prop ﬁle at system boot. Standard Java ﬁle methods
can be used to read this ﬁle and therefor examine settings such as
ro.secure. Even though this ﬁle is not likely to be modiﬁed,
obtaining the properties in this way may not reﬂect the runtime
state of the instance. We will explore other ways of obtaining
actual runtime values in Section 4.
are no APIs
the
inspecting properties
There
but
are
loaded
3.4 Differences due to system design
Modern runtime analysis systems must cope with certain con-
straints that do not affect consumer devices. In particular runtime
systems must generally process a considerable volume of malware
as the number of daily unique samples is quite large and report-
edly growing. This phenomenon has been observed for years in
the realm of PC malware and early signs indicate a similar growth
pattern for mobile malware. Unfortunately, this requirement for ad-
ditional scale is often at odds with resource availability. It is simply
not economically viable to purchase thousands of physical devices
or to run thousands of emulators simultaneously, forever. For these
reasons, critical design decisions must be made when designing
a runtime analysis system and the effects of these decisions can
be used as a form of runtime-analysis detection. We outline two
classes of design decisions that may inﬂuence an attackers abil-
ity to detect or circumvent analysis: those shared with PC runtime
analysis systems and those speciﬁc to Android.
PC system design decisions, such execution time allotted to
each sample or how much storage to allocate to each instance, have
been explored in the PC realm. Many of these same decisions must
be made for a mobile malware runtime analysis system. System cir-
cumvention, such as delaying execution past the maximum allotted
execution time, is also shared with PC techniques.
Android-speciﬁc design decisions revolve around the inherent
differences between a device that is actively used by an individual
and a newly created instance (virtual or physical). If an attacker
can determine that a device is not actually in use, the attacker may
conclude that there is no valuable information to steal or that the
device is part of an analysis system.
Metrics for determining if the device is (or has been) in use in-
clude quantities such as the number of contacts and installed ap-
plications, and usage indicators such as the presence and length
of text messaging and call logs. These and many more indicators
1The method also employs a second check for ro.debuggable
and service.adb.root; if these are both 1, privileges will not
be dropped even if ro.secure is set to 1.
are available programatically as part of the Android API, but many
require the application to request particular permissions. Runtime
analysis system detection using these metrics is not as clear-cut
as the other techniques we presented. These values depend upon
knowing the average or minimum quantities present on the typi-
cal consumer device, and would be rife with false positives if the
quantities were not evenly distributed among all users. Some work
shows that these values are indeed not evenly distributed as a small
user study showed eight of 20 participants downloaded ten or fewer
applications while two of 20 downloaded more than 100 [23].
4. MINIMIZING THE PERMISSIONS
NEEDED
Some of the detections mentioned in this paper require cer-
tain application-level permissions. For example, to detect if Blue-
tooth is present on a device the application must request the an-
droid.permission.BLUETOOTH or BLUETOOTH_ADMIN
permission. Other resources, such as the accelerometer require no
permission to access.
All the techniques described in previous sections require only a
very limited set of application permissions and the make use of ex-
isting APIs that are unlikely to change substantially in the foresee-
able future. However, using the advertised APIs also has a offensive
drawback: Designers of automated analysis systems could attempt
to address the speciﬁc APIs we have utilized instead of completely
addressing the downfalls of the emulation environment. For ex-
ample, mitigating the device ID check by simply hooking the call
to TelephonyManager.getDeviceId() and causing the re-
turn value to not be all 0’s. To demonstrate how such an approach is
short-sighted defensively, we present two techniques for retrieving
runtime system properties even though there is no programmatic
API in the SDK. We can then use these properties to create alter-
nate implementations of nearly every detection we have detailed.
We offer two additional techniques for obtaining system proper-
ties: reﬂection and runtime exec (subprocess). Example code can
be found in Figure 11 in the Appendix. In the reﬂection example,
the android.os.SystemProperties class is obtained via
the ClassLoader. The get method can then be obtained from
the class and used to retrieve speciﬁc properties. This example re-
trieves settings we’ve already discussed, such as the ro.secure,
the battery level and the Build tags. The exec example is more
general and simply prints a list of all runtime properties be execut-
ing the “getprop” binary and parsing the output.
5. EVALUATION
Our techniques were developed in a relatively isolated test envi-
ronment. We thus need to measure the effectiveness of our tech-
niques against real analysis systems. We identiﬁed publicly avail-
able Android sandbox systems via literature review, industry re-
ferral, and via Internet searches. Candidate systems had to have
a public, automated interface to be considered. Ideally, a candi-
454 500
 400
y
c
n
e
u
q
e
r
F
 300
 200
 100
 0
 0
 10
 20
 30
 40
 50
 60
 70
Frames Per Second (FPS)
Andrubis
CopperDroid
Foresafe
Galaxy Nexus (Wifi)
Figure 10: FPS measurements for sandboxes: For comparison,
a physical Galaxy Nexus was re-measured using the same ap-
plication. The physical device shows strong coupling at 59 FPS
and all of the sandboxes demonstrate loose coupling and wide
distribution, indicating that they all make use of virtualization.
date system also provides a publicly accessible output – typically
as some form of report.
Our candidate sandboxes were Andrubis [2], SandDroid [10],
Foresafe [6], Copperdroid [3], AMAT [1], mobile-sandbox [7] and
Bouncer [25]. Each sandbox presents a slightly different interface,
but are all meant to be accessible as web services. Likewise, each
sandbox is the result of different levels of developmental effort and
therefore embodies various levels of product maturity.
Mobile-sandbox constantly displayed a message indicating that
0 static and 308,260 dynamic jobs were yet to be processed. We
were only ever able to observe static analysis output of mobile-
sandbox. Similarly, SandDroid seemed to not route or otherwise
ﬁltered outbound network trafﬁc, and the SandDroid reports only
displayed results of static analysis so it was not possible to test our
evasion techniques on SandDroid.
AMAT’s online instance does not appear to be in working order,
immediately stating that any uploaded application was “not mal-
ware.” AMAT did not provide any further reasoning as to why this
message was displayed, but uploading APK ﬁles did result in the
overall analysis number incrementing with each submission. When
using Foresafe, an action would occasionally fail, and a server-side
error would be displayed to the user. Even so, refreshing the page
and repeating the action always seemed to solicit the desired effect.
Google’s Bouncer does not immediately meet our minimal re-
quirement of having a public interface, but we attempted to include
it given its importance on deployed applications. Not much about
the inner workings of Bouncer has been made available. Without
the ability to directly submit applications for analysis, and with-
out the ability to directly view reports, interaction with Bouncer
is widely unpredictable.
Indeed, even after submitting several,
increasingly offensive, applications into the Google Play market-
place, we never observed any connections coming from Bouncer. It
is possible that Bouncer has altered the decision process for select-
ing which applications to run (e.g. only those with more than 10K
downloads, or those with negative reviews) or has been changed to
disallow connections to the Internet following other work on pro-
ﬁling the inner workings of Bouncer [28, 31].
5.1 Behavior evaluation
As shown in Table 6, the SDK and TelephonyManager de-
tection methods prove successful against all measured sandboxes.
Many of the simple detection heuristics outlined in Table 1 are sim-
d
i
o
r
D
r
e
p
p
o
C
s
i
b
u
r
d
n
A
e
f
a
S
e
r
o
F
Y† Y Y
Y
Y Y
Y Y‡ Y
Y Y
Y
Y Y
Y
Y Y
Y
Y Y
Y
Y
Y Y
Y Y
Y
N N
N
N N
N
N N
N
N Y
N
N
N Y
N N
Y
Y Y
Y
N N
N
N N
N
y
N N
N N
y
N N
y
N N
y
Y
Y Y
Y† Y Y
Y Y
Y
Y Y
Y
Y Y
Y
Y Y
Y
Y
Y Y
Y Y
Y
Y
Y Y
Detection method
getDeviceId()
getSimSerial Number()
getLine1 Number()
MCC
MNC
FINGERPRINT
BOARD
BRAND
DEVICE
HOST
ID
manufacturer
MODEL
PRODUCT
serial
TAGS
radio
USER
NetCountry
NetType
PhoneType
SimCountry
VMNum
SubscriberID
Networking
bluetooth
ro.secure
sensors
contacts
call log
performance
Table 6: Evaluation of detections. An uppercase Y indicates
that the system was detected as an emulator, a lowercase y indi-
cates that the system may be an emulator, and an uppercase N
indicates that the detection mechanism did not succeed. † This
detection was actually designed to detect a particular tool in the
same manner as described in Section 3.1, and we discuss the de-
tection in the Section 6.‡ the number is not exactly an emulator
number, but the area code is 555 which is not valid.
ilarly successful. However, some of the Build parameters, such as
HOST, ID, and manufacturer require a more complex heuristic
in order to be useful. Detecting the emulated networking environ-
ment was also very successful as the sandboxes all employed the
default network conﬁguration.
5.2 Performance evaluation
The graphical performance measurements further indicate that
all of the measured sandboxes are built using virtualization. Fig-
ure 10 shows measurements from the sandboxes as well as a hard-
ware device. As with the emulators sampled in section 3.2, each
of the sandboxes exhibit a lower, loosely coupled values. Unlike
in our own test environment, we have no control over the duration
of execution in the measured sandboxes. Due to premature termi-
nation, we only received a subset of the 5,000 measurements the
application should have generated (604, 814, and 229 for Andru-
bis, CopperDroid, and Foresafe, respectively).
455Sensor
Accelerometer
Ambient Temperature
Gravity
Gyroscope
Light
Linear Acceleration
Magnetic Field
Orientation
Pressure
Proximity
Relative Humidity
Rotation Vector
Temperature
Total
d
i
o
r
D
r
e
p
p
o
C
1
0
0
0
0
0
0
0
0
0
0
0
0
1
s
i
b
u
r
d
n
A
0
0
0
0
0
0
0
0