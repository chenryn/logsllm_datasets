The point-and-permute technique [NPS99] represents each
wire label as a symmetric t-bit key and a permutation bit π,
where t is a symmetric security parameter. The permuta-
tion bits of a gate’s input wires are used as index to denote
which table entry needs to be decrypted. The free XOR
technique [KS08] allows to compute garbled linear gates,
i.e., XOR and XNOR gates, without communication (no
garbled table is needed) and only negligible computation.
Thus, the dominating factor for the complexity of a circuit
is the number of non-linear gates. Further, the garbled row
reduction technique [NPS99, PSSW09] allows to reduce the
garbled table by one entry.
2.1.3 Oblivious Transfer
In m-parallel Oblivious Transfer (OT) of (cid:3)-bit strings, de-
noted as OTm
(cid:2) , the chooser inputs a vector of choice bits ri,
i = 1 . . . m and the sender inputs a vector of pairs of (cid:3)-bit
strings (x0, x1)i, i = 1 . . . n. At the end of the protocol, the
chooser learns the selected strings xri,i, but nothing about
the other strings x1−ri,i whereas the sender learns nothing
about the choices ri.
In Yao’s Garbled Circuit protocol (cid:3) = t+1, whereas m cor-
responds to the number of input bits provided by the client
which can be large. Using OT extensions of [IKNP03] it is
possible to reduce a large number of OTs to a small number
of only k OTs, where k is a security parameter. These re-
maining k base OTs are implemented with an eﬃcient OT
protocol which requires O(k) public-key operations, e.g., the
OT protocol of [NP01].
In §3.1 we give implementation improvements for the base
OTs of [NP01] and in §3.2 we show how the OT extension
of [IKNP03] can be implemented with low memory footprint.
2.2 Minor Remark on the Choice of Security
Parameters
As described in [HEKM11, Sect. 3.4], the implementa-
tion of FastGC uses SHA-1 as cryptographic hash func-
tion to encrypt the output wire labels of non-linear gates.
Hence, the maximum security level that can be achieved is
t = 80 bits. However, in the most recent implementation of
FastGC [HEKM11] (version 0.1.1 released August 9, 2011),
the last bit of the 80 bit wire labels is used as permuta-
tion bit for the point-and-permute technique of [NPS99] (cf.
§2.1.2). As this permutation bit is known to the evaluator,
the achieved symmetric security level is only 79 bits, but
not 80 bits as stated in their paper. To correct this, we set
Wire.labelBitLength=81 (actually we use 88 as internally a
byte-oriented representation is used) to achieve a symmetric
security level of exactly 80 bits. The longer wire labels result
in a slight increase of communication. For the OT protocol
of [NP01], implemented over FP with a generator of order
q, we use |p| = 1024 and |q| = 160 as these correspond to
the current NIST recommendations for a symmetric security
level of 80 bits.1
3. FASTER SECURE EVALUATION OF
GARBLED CIRCUITS
We optimize several aspects of the FastGC framework
[HEKM11] as described in the following.
We emphasize that our optimizations do not modify the
underlying cryptographic protocols, but only the way they
are implemented. Therefore, and because we work in the
semi-honest setting, the proofs of security of the original
protocols still hold for our optimizations.
1see http://keylength.com
4393.1 Improved Base OTs
In order to improve the performance of the k base OTs
with the OT protocol of [NP01], we split up the computa-
tionally intensive public key operations into multiple threads
such that each of the N threads performs k/N base OTs (in-
dependent of each other). Furthermore, we tried to imple-
ment the base OTs over an elliptic curve instead of over FP
(the latter was used in the implementation of [HEKM11]).
This resulted in a reduction of the communication complex-
ity, but unfortunately no better runtimes. We assume that
this is due to the additional overhead introduced by the
Java VM. Our performance benchmarks for the improved
base OT implementations are given in §4.1.
3.2 Extending OTs with Low Memory Foot-
print
A large number of m parallel OTs of (cid:3)-bit strings, OTm
(cid:2)
can be reduced to a small number of only k OTs of k-bit
strings, OTk
k, using OT extensions of [IKNP03] as imple-
mented in [HEKM11]. The original protocol of [IKNP03]
needs beyond the messages of the base OTs only two mes-
sages, but memory linear in m.
We reduce the memory requirement of this protocol by
re-ordering its messages as follows: The OT extension con-
struction of [IKNP03] proceeds in two steps: First, the large
number of m parallel OTs of short (cid:3)-bit strings are reduced
to k parallel OTs of long m-bit strings, cf. [IKNP03, Fig. 1].
These OTs are implemented using k parallel OTs of short k-
bit keys that are then stretched into longer m-bit masks us-
ing a pseudo-random generator (PRG), cf. [IKNP03, Fig. 3].
A very eﬃcient and standard way to implement a PRG is
to successively apply a pseudo-random permutation (PRP)
on a counter, i.e., PRG(k) = PRPk(0)||PRPk(1)|| . . . .
In
practice, the PRP can be instantiated with a block cipher
which operates on blocks ofM bits, e.g., in our implemen-
tation we use AES-128 with M = 128. Now, in order to re-
duce the memory footprint, the OT extension construction
of [IKNP03] can easily be split into smaller blocks where
each block performs M parallel OTs. The overall protocol
is shown in Fig. 1. W.l.o.g. and to simplify presentation we
assume that m is a multiple of the block size, m = BM (oth-
erwise, the last messages are shorter) and that log B ≤ M .
T denotes an M × k bit matrix, Ti its i-th column and
Tj its j-th row. G : {0, 1}M × {0, 1}k → {0, 1}M is a PRP;
H : {0, 1}(cid:2)log m(cid:3)×{0, 1}k → {0, 1}(cid:2) is a random oracle which
can be implemented using a cryptographic hash function (in
our implementation we use SHA-1).
Overall, our modiﬁed protocol can be seen as operating
on a stream of data which is processed in chunks of size M .
Security and Correctness. The only modiﬁcation that we
applied to the original construction of [IKNP03] is that we
re-order the messages sent into B rounds of communication.
For semi-honest parties, this does not reveal any additional
information. Therefore, the correctness and security of our
optimized protocol in the semi-honest setting directly follows
from the correctness and security of the original construction
as proven in [IKNP03].
Performance. The computation and communication com-
plexity of our protocol is exactly the same as that of the
original construction of [IKNP03].
In contrast to the im-
Figure 1: Extension of OTm
(cid:2) with low memory foot-
print. W.l.o.g. m = BM for B blocks of size M . G
is a pseudo-random permutation and H is a random
oracle.
with Low Memory Footprint
plementation in [HEKM11] which has a constant number of
communication rounds but requires memory linear in the to-
tal number of OTs m, the memory consumption of our pro-
tocol is constant (for ﬁxed block size M ), but requires m/M
rounds of communication. We give performance benchmarks
for the improved OT extensions in §4.1.
3.3 Streaming Circuits and Garbled Circuits
As described in §1 and §1.2, previous frameworks for se-
cure two-party computation in the semi-honest adversaries
setting store the outgoing wire labels of each gate in memory
and hence require memory linear in the size of the evaluated
circuit. More recent frameworks [Mal11, HEKM11, MZE12]
require memory only linear in the size of the sub-circuits,
but these frameworks suﬀer from the low performance of
memory management of many small objects (one object for
each gate) and garbage collection.
During creation and evaluation of a (sub) circuit, only
those wires need to be held in memory that are used in the
future, i.e., are either an output wire of the circuit or used
as input wires into a later gate. This set of wires is called
the working set. Thus, the minimum size of the memory
required to compute the circuit is deﬁned by the maximum
working set. As described and implemented in [KSS12], one
strategy to keep track of the working set is to annotate to
each wire a usage counter that is decremented on each use.
When the counter reaches zero, the wire is deleted from the
memory. This is similar to the way sub-circuits are dynam-
ically constructed and deconstructed in VMCrypt [Mal11]
and impedes additional overhead in the online phase.
We use a diﬀerent approach where we shift the manage-
ment of the working set from the online phase to the com-
pilation phase in order to keep the online phase as lean as
possible.
In particular, the online phase of our approach
neither requires a usage counter nor to allocate and later on
free memory for each gate. During the compilation phase,
the compiler determines the maximum size S of the working
set and stores this along with the circuit description. The
compiler allocates a slot ID (starting from 0) to each input
wire of the circuit. Then, the circuit description is generated
440as an ordered list of gates where each gate is described as a
tuple (output slot ID; input slot IDs; truth table). When-
ever a slot ID is used for the last time, it is added to a list of
available slot IDs and can be re-used as the output slot ID
of a later gate. Note that all this is done in the oﬄine (com-
pilation) phase. In the online phase, an array with S slots
is allocated where each slot can hold a wire label. Then, the
gates are read one-by-one from the circuit description: the
gate’s input labels are taken from the slots given by its input
slot IDs and the output label is stored to the slot speciﬁed
by the gate’s output slot ID. We internally keep a counter
of the gate ID which is used to construct the garbled tables.
We emphasize that the compilation of a function has to
be done just once, since the resulting circuit is independent
from the inputs and can therefore be reused.
In future work, the compiler can be extended to re-arrange
the order of the gates in order to reduce the size of the
working set as described in [JKSS10b]. However, as noted
in [KSS12], determining a topologic order of the circuit with
the minimum size of the working set is known to be an NP-
complete problem.
We further note that in principle generation of garbled
circuits can be implemented such that the amount of mem-
ory is constant by pseudo-randomly deriving the wire labels
from the gate ID as described in [JKSS10a]. However, this
is essentially a time-memory tradeoﬀ and cannot easily be
combined with the highly eﬃcient free XOR technique.
3.4 Sub-Circuit Compilation
A design goal of our framework was to keep the online
phase of the circuit evaluation as lean as possible. Due to the
topological ordering of the circuits the engine never has to
hold more than one gate description in memory. Once a gate
is processed, the information can be discarded (except for
the intermediate wire labels). The circuit evaluation engine
reads the circuit description from a ﬁle with a format similar
to Fairplay. In order to re-use sub-circuits we use slot IDs
to index the wires in the sub-circuit. A slot ID can be seen
as a virtual register that can hold a wire label and can also
be re-used. For the gate ID, which needs to be unique in
the overall circuit as it is used for encrypting the non-linear
gates, we use a counter which is incremented for each non-
linear gate. A gate is described by its output slot ID, its
input slot IDs, and its truth-table. An example is shown in
Fig. 2 which describes the one-bit comparison circuit from
[KSS09]. We also support a binary ﬁle format which is more
eﬃcient to read by our engine.
The circuit to be computed often consists of several calls
to the same sub-function. For instance, AES consists of 160
S-Box, 10 AddRoundKey, and 9 MixColumns calls. Our
framework allows for sub-circuits to be reused in a similar
fashion like [HEKM11], except that we do not instantiate a
new gate object in each invocation of the sub-circuit. Over-
all, for AES we do not create the entire circuit with 24,720
gates, but only 3 sub-circuits for the sub-functions listed
above with a total of just 803 gates. As in our implemen-
tation the creation of the gate ID is decoupled from the
circuit deﬁnition, we ensure that in every reuse of a sub-
circuit all gates have a unique gate ID, and therefore the se-
curity of the underlying garbled circuit protocol, as proven
in [LP09a, PSSW09], still holds.
We provide a compiler that converts circuits described in
the format of [HEKM11] into our format.
Figure 2: A one-bit comparison circuit. Comments
(from // on) are not part of the input.
// creator’s input is r0
// evaluator’s input is r1
inputsCreator: 0
inputsEvaluator: 1
outputsCreator:
outputsEvaluator: 0 // evaluator’s output is r0
numberOfRegisters: 2
numberOfGates: 2
0;1,0;1
0;1,0;6
// r0 = r1 and r0
// r0 = r1 xor r0
3.5 Caching of Circuits and Communication
In order to improve the performance of garbled circuits
evaluation we cache both, circuit descriptions and network
packets during garbled circuit streaming resulting in a cor-
responding time-memory trade-oﬀ as described next.
In some circuits, the same sub-circuits are reused many
times (cf. §3.4). Instead of reading the description of the
sub-circuit from a ﬁle on every instantiation (or re-gener–
ating it as implemented in previous frameworks), we op-
tionally cache its description once in memory. The memory
consumption is 32 bytes per cached gate.
Sending the creator’s input wire labels and the garbled
tables straight after creation (as implemented in [HEKM11])
leads to an ineﬃcient use of the network because of small
packet sizes and an unnecessary large number of packets.
By using ﬁxed sized buﬀers on the communication channels
we greatly improve the performance of the network usage.
In our benchmarks in §4 we use circuit caching and net-
work buﬀers of size 9,000 byte.
4. PERFORMANCE BENCHMARKS AND
APPLICATIONS
In the following we show that the implementation of our
improvements described in the previous section results in
substantially better performance than previous frameworks.
4.1 Oblivious Transfers
The following performance benchmarks were performed
on two Apple computers with a dual core processor each
(Intel Core i5 2.5GHz and Core i7 1.8GHz) running MacOS
X 10.7.4 and Java 1.6.0 33, connected via 802.11n WIFI.
We observed that because of the JAVA just in time com-
piler, the runtime decreases in the ﬁrst few runs due to com-
piler optimizations. Therefore, we executed each protocol
1,000 times and took the average. All benchmarks were ex-
ecuted with the default JAVA VM parameter.
The performance of our improved implementation for the
base OTs (cf. §3.1) in comparison with the original imple-
mentation of [HEKM11] run in exactly the same setting is
shown in Table 2. Due to the additional overhead for thread
management, our multi-threaded implementation with one
thread is slightly slower than the single-threaded one. How-
ever, when running with 4 threads on the dual core pro-
cessors, our multi-threaded base OTs take 0.15 seconds, an
improvement by factor 2 over the single-threaded version.