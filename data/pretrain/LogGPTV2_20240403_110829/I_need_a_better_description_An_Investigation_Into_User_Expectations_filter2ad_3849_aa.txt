title:"I need a better description": An Investigation Into User Expectations
For Differential Privacy
author:Rachel Cummings and
Gabriel Kaptchuk and
Elissa M. Redmiles
An Investigation Into User Expectations For Differential Privacy
‚ÄúI need a better description‚Äù:
Rachel Cummings
Columbia University
New York, United States
PI:EMAIL
Gabriel Kaptchuk
Boston University
Boston, United States
PI:EMAIL
Elissa M. Redmiles
Max Planck Institute
for Software Systems
Saarbr√ºcken, Germany
PI:EMAIL
1 INTRODUCTION
Differential privacy (DP) is a mathematically rigorous definition
of privacy that has gained popularity since its formalization in
2006 [19]. DP facilitates the computation of aggregate statistics
about a dataset while placing a formal bound on the amount of
information that these statistics can disclose about individual data
points within the dataset. Guaranteeing DP generally requires in-
jecting carefully calibrated noise that hides individual datapoints
while preserving aggregate level insights.
ABSTRACT
Despite recent widespread deployment of differential privacy, rela-
tively little is known about what users think of differential privacy.
In this work, we seek to explore users‚Äô privacy expectations related
to differential privacy. Specifically, we investigate (1) whether users
care about the protections afforded by differential privacy, and (2)
whether they are therefore more willing to share their data with
differentially private systems. Further, we attempt to understand
(3) users‚Äô privacy expectations of the differentially private systems
they may encounter in practice and (4) their willingness to share
data in such systems. To answer these questions, we use a series of
rigorously conducted surveys (ùëõ = 2424).
We find that users care about the kinds of information leaks
against which differential privacy protects and are more willing
to share their private information when the risks of these leaks
are less likely to happen. Additionally, we find that the ways in
which differential privacy is described in-the-wild haphazardly set
users‚Äô privacy expectations, which can be misleading depending
on the deployment. We synthesize our results into a framework
for understanding a user‚Äôs willingness to share information with
differentially private systems, which takes into account the interac-
tion between the user‚Äôs prior privacy concerns and how differential
privacy is described.
CCS CONCEPTS
‚Ä¢ Security and privacy ‚Üí Usability in security and privacy.
KEYWORDS
Differential Privacy; Human Factors; Usable Security; Usable Pri-
vacy
ACM Reference Format:
Rachel Cummings, Gabriel Kaptchuk, and Elissa M. Redmiles. 2021. ‚ÄúI need a
better description‚Äù: An Investigation Into User Expectations For Differential
Privacy. In Proceedings of the 2021 ACM SIGSAC Conference on Computer
and Communications Security (CCS ‚Äô21), November 15‚Äì19, 2021, Virtual Event,
Republic of Korea. ACM, New York, NY, USA, 16 pages. https://doi.org/10.
1145/3460120.3485252
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea
¬© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-8454-4/21/11...$15.00
https://doi.org/10.1145/3460120.3485252
DP has become a leading technique used to meet the increasing
consumer demand for digital privacy [4]. In the last few years,
several companies have deployed DP. For instance, Apple uses
DP to gather aggregate statistics on Emoji usage, which it uses
to order Emojis for users [3, 67]. Uber uses DP to prevent data
analysts within the company from stalking customers [36, 50], and
Google uses DP to crowd-source statistics from Google Chrome
crash reports [24].
The U.S. government has also begun to use DP. The United States
Census Bureau is using DP to prevent information disclosure in the
summary statistics it releases for the 2020 Decennial Census [1].
The use of DP in the Census means that nearly every person in the
United States will have private data protected by DP.
Following in the footsteps of these earlier adopters, more compa-
nies have already announced their intentions to integrate differen-
tially private techniques into their systems, e.g., [33, 49]. As a result,
DP is becoming an increasingly consumer-relevant technology. Yet,
little is known about whether end users value the protections of-
fered by DP.
While DP is mathematically elegant and computationally effi-
cient, it can be difficult to understand. Not only is DP typically
defined mathematically, the privacy protections provided by DP
are not absolute and require contextualization [17]. DP does not
provide binary privacy (i.e., private or not private), but instead pro-
vides a statistical privacy controlled by unitless system parameters
that are difficult to interpret (i.e., the parameters ùúñ and ùõø control the
maximum amount of information that can leak about any individ-
ual entry in the dataset) [19]. Additionally, DP can be deployed in
different security models, and the choice of model has significant
impact on the types of adversarial behavior the system can tolerate.
In the local model, users randomly perturb their information (with
the help of the collection mechanism, e.g., their device) before send-
ing it to a central entity in charge of analysis, called the curator [39].
In the central model users share their sensitive information directly,
and the curator is trusted to perturb results that are released [21].
Session 11C: Software Development and Analysis CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3037Differential Privacy from the user‚Äôs perspective. The exist-
ing DP literature focuses on techniques for achieving DP [18‚Äì
21, 27, 39, 46], with a small but growing body of work on legal
and ethical implications of DP [11, 14, 53, 54]. Notably absent, how-
ever, is the voice of the end user, whose information may eventually
be protected by DP and may benefit from its deployment [7]. Do
users care about the information disclosures against which DP pro-
tects? Do users understand how DP protects them, and if so, do
those protections influence their comfort with sharing informa-
tion? As differentially private systems proliferate, it is increasingly
important to answer such questions and understand DP from the
user‚Äôs perspective.
While a limited body of prior work has sought to understand how
training users to understand DP influences willingness to share [9,
15, 69], we aim to answer broader questions regarding: (i) whether
DP meets users‚Äô existing privacy needs, (ii) what expectations a
potential user might have of a differentially private system, and
(iii) whether existing, in-the-wild, descriptions of DP accurately set
user expectations.
As a privacy-enhancing technology, DP is designed to prevent
the unwanted information disclosure of user information to certain
entities. However, it is not clear that these protections are mean-
ingful to potential users. Additionally, it is not clear if DP provides
the level of protection that potential users might hope.
Thus, in this work, we ask the following research questions:
(RQ1) Do potential users care about protecting their information
against disclosure to the entities against which differentially
private systems can protect?
(RQ2) Are potential users more willing to share their information
when they have increased confidence that such information
disclosures will not occur?
(RQ3) Do potential users expect differentially private systems to
protect their information against disclosure? How does the
way in which differential privacy is described impact their
expectations?
(RQ4) Are potential users more willing to share their information
when their information will be protected with differential
privacy? How does the way in which differential privacy is
described impact sharing?
We conduct two surveys with a total of 2, 424 respondents to answer
our research questions. We use vignette-based surveys to elicit
respondents intended behavior, as such surveys have been found
to well-approximate real-world behavior [26].
To address RQ1 and RQ2, we present each respondent with one
of two information-sharing scenarios (sharing information with
a salary transparency initiative or sharing medical records with a
research initiative) and query respondents‚Äô privacy concerns. We
then set their privacy expectations for those concerns (e.g., how
likely information is to be leaked to a particular entity) and query
if they would be willing to share their private information. Using
the results of this survey, we examine how respondents‚Äô privacy
concerns align with the protections offered by DP.
To address RQ3 and RQ4, we again present each respondent
with one of the two information-sharing scenarios. We additionally
tell respondents that their information will be protected by DP, as
described by one of six descriptions.1 We then query respondents‚Äô
privacy expectations for these scenarios (e.g., whether their infor-
mation could leak to various entities) and whether they would be
willing to share their information. Using the results of this survey,
we interrogate how accurately and effectively existing descriptions
of DP set user expectations.
There is no ‚Äústandard‚Äù deployment of DP, nor is there a ‚Äúnormal‚Äù
way to describe its guarantees. In order to construct representative
descriptions of DP to present to our participants, we systematically
collected over 70 descriptions of DP written by companies, govern-
ment agencies, news outlets, and academic publications. Through
affinity diagramming qualitative analysis [8], we identify six main
themes present in these descriptions, compose a representative
description for each theme, and showed these representative defi-
nitions to respondents.
By describing DP as a potential user would encounter it in-the-
wild, we gain a better understanding of how potential users are
likely to respond to DP in practice. The nuances innate in DP
make it easy for a prospective user to misunderstand what they
are being promised. As such, a user seeking to choose the right
privacy preserving system may find it difficult to make an informed
choice. Getting this wrong can have real-world consequences: DP
may be insufficient to protect a user‚Äôs information against the types
of threats about which they are concerned.
Summary of Findings. We find that users care about the kinds
of information disclosure against which DP can protect (RQ1) and
are more willing to share their private information when the risk
of information disclosure to certain entities, specifically those for
which disclosure would represent an inappropriate information
flow [51], is not possible (RQ2).
Further, we find that descriptions of DP raise respondent‚Äôs con-
crete privacy expectations around information disclosure (RQ3).
This effect, however, varies by how DP is described: different de-
scriptions of DP raise expectations for different kinds of information
disclosure. These expectations, in turn, raise respondent‚Äôs willing-
ness to share information. However, informing respondents that
a system was differentially private did not raise potential user‚Äôs
willingness to share information, no matter which description of
DP was presented to the respondent (RQ4).
Taken together, our findings suggest that while (1) respondents
do care about the information disclosures against which DP can pro-
tect; (2) the likelihood of those disclosures influences respondent‚Äôs
willingness to share; and (3) different in-the-wild descriptions of
DP influence respondents perception of the likelihood of those dis-
closures. However, (4) simply being shown a randomly-selected,
in-the-wild description of DP does not increase willingness to share.
These results, at first glance, appear to be in tension.
On deeper analysis however, these findings suggest the pres-
ence of a misalignment between the information disclosures about
which users care and the information disclosures that descriptions
of DP address. The probability that a given respondent was (a)
shown a description that related to the information disclosures
about which they care, and (b) that description influenced enough
of their perceptions is likely low.
1We also maintain a control group of participants who are not told that their informa-
tion is protected.
Session 11C: Software Development and Analysis CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3038Synthesizing these findings, we posit a novel framework for
understanding how end users reason about sharing their data under
DP protections. Our framework ‚Äî and the findings that informed
it ‚Äî offer concrete directions for reformulating DP descriptions
to accurately and effectively set user‚Äôs privacy expectations and
increase their comfort when using differentially private systems.
Users must either be trained to carefully understand descriptions
(as done in [69]) or descriptions should be reformulated to directly
communicate how they address the information flows that concern
users (e.g., via privacy nutrition-labels [41]). If DP descriptions
can be effectively reformulated, our results suggest that users may
be significantly more comfortable sharing their information when
given differentially private protections.
2 BACKGROUND AND RELATED WORK
In this section, we provide a background on DP and review prior
work on communicating privacy to end users, with a specific focus
on prior research studying DP-related communications.
Differential Privacy. In the last decade, a growing literature on dif-
ferentially private algorithms has emerged to address concerns sur-
rounding user-level data privacy. First defined by Dwork et al. [19],
DP is a parameterized notion of database privacy that gives a math-
ematically rigorous worst-case bound on the maximum amount of
information that can be learned about any one individual‚Äôs data
through the analysis of a dataset. Formally, a database ùê∑ ‚àà Dùëõ is
modeled as containing data from ùëõ individuals, and DP constrains
the change in an algorithm‚Äôs output caused by changing a single
person‚Äôs data in the database.
Definition 2.1 (Differential Privacy [19]). An algorithm A : Dùëõ ‚Üí
R is (ùúñ, ùõø)-differentially private if for every pair of databases ùê∑, ùê∑‚Ä≤ ‚àà
Dùëõ that differ in at most one entry, and for every subset of possible
outputs S ‚äÜ R,
Pr[A(ùê∑) ‚àà S] ‚â§ exp(ùúñ) Pr[A(ùê∑‚Ä≤) ‚àà S] + ùõø.
DP can be implemented either in the central model ‚Äî where
users provide their raw data to a trusted curator for private analysis
‚Äî or in the local model, where users add noise locally to their own
data before sharing it for analysis. The central model corresponds
to the original DP definition of [19] as presented in Definition
2.1, where an analyst first collects a dataset from users, and then
uses specialized DP tools to ensure that the technical requirements
of Definition 2.1 are satisfied. The original intended use case for
central DP is to enable trusted data analysts who already held
sensitive datasets to publish aggregate statistics or reports on their
data without violating the privacy of the individuals represented in
the data. Central DP is used by e.g., the U.S. Census Bureau [1, 31]
and Uber [36, 50] since both require exact user data ‚Äì the Census
Bureau through a constitutional mandate; Uber because data like
rider location are necessary for their ride-sharing services.
The local model provides privacy guarantees in the presence of
an untrusted curator. Users add noise to their own information (i.e.,
on their own device) through algorithms that satisfy Definition 2.1
for ùëõ = 1, and share the privatized output with the curator. Thus,
the curator receives only a perturbed and private version of each
user‚Äôs data and never has access to raw user data. Any analysis
performed on the noisy data will retain the same DP guarantee due
to post-processing [19], so the curator need not use any specialized
analysis tools to ensure privacy. Analysts can still make aggregate
inferences based on population-level statistics, but will only see
noisy information about any individual. Local DP is used by, e.g.,
Apple [3, 67], Google [24], and Microsoft [16] in settings where user
data is stored on-device and the company only requires aggregate
information to perform its services.
The possible risks of information disclosure differ substantially
between these two models. Since the central model stores user data
in a centralized location, data analysts have access to exact user
data, along with any other parties who obtain access through legal
or illegal means. In the local model, the dataset itself is privatized,
so there is no risk of information disclosure through the curator‚Äôs
dataset. In this work, we seek to understand user‚Äôs perceptions
of these possible risks of information disclosure and interrogate
the accuracy of those perceptions under both the local and central
models of differential privacy.
Privacy Communications. A large body of work has examined
how best to explain privacy to end users [22, 35, 44, 63, 66]. This has
included creating privacy nutrition labels [41] that clearly delineate
to users who may use their information, how their information
may be used, and how likely these uses are to occur; designing pri-
vacy icons that clearly communicate when and what information
is being collected [13, 23, 48]; and developing machine-learning
systems that help users negotiate privacy boundaries [61]. Partic-
ularly relevant to the work presented here, prior work has also
identified best practices for privacy communications: descriptions
should be relevant (e.g., include the necessary context for users to
make decisions), actionable (e.g., allow the user to make choices),
and understandable (e.g., usable, not overloading the user with
technical information) [62]. As we discuss in Section 6, our findings
suggest that existing DP descriptions fail to satisfy these criteria.
Despite this large body of prior work on privacy communications
and the increasing importance of DP, only two pieces of prior work
have focused on communicating with users about DP.
Bullek et. al. [9] study how users understand privacy parame-
ters in randomized response, a specific local DP technique. They
describe randomized response to users using a virtual, colored spin-
ner; the user would spin the spinner, the outcome of which would
indicate if the user should answer the sensitive question truthfully
or with the response indicated on the spinner. Our work focuses
more broadly on how the information disclosures against which
DP protects can influence users‚Äô willingness to share, and on how
descriptions of DP influence expectations for those disclosures.
Most closely related to our own work, Xiong et al. [69] study
how informing users that their information is protected with DP
influences their willingness to share different types of information.
They study this question in the context of an app that collects med-
ically relevant information, both low sensitivity (e.g., gender, height,
weight) and high sensitivity (e.g., substance use, income level, cur-
rent medication). They found that promising users DP makes them
more willing to share their information (particularly high sensitivity
information, which is comparable to the information we consider in
this work). However, they found that users struggled to understand
descriptions of DP but were more able to understand descriptions
that mentioned the implications of information sharing.
Session 11C: Software Development and Analysis CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3039Our study builds upon this prior work to more deeply explore
(RQ1) which information sharing implications are most concerning
to users, and thus should be emphasized when describing DP, (RQ2)
how these implications themselves influence users‚Äô willingness to