### 5. Removing End Hosts from the Overlay Network

To evaluate the process of removing end hosts, we start with an overlay network consisting of 60 randomly selected end hosts. We then iteratively delete one end host at a time until the overlay size is reduced to 55 and 50, respectively. As shown in Table 6, the accumulated numerical error during this process is negligible.

As detailed in Section 5, deleting a path in the graph \( \bar{G} \) is more complex than adding a path. Using the same machine, the average time for deleting a path is 445 milliseconds, while deleting a node takes 16.9 seconds. It is important to note that the current implementation is not optimized. We can improve the efficiency by processing multiple paths simultaneously and by employing iterative methods such as Conjugate Gradient Normal Equations (CGNE) or Generalized Minimal Residual (GMRES) [32]. The time complexity for adding or deleting a path is \( O(k^2) \), and for adding or deleting a node, it is \( O(nk^2) \). Therefore, our updating scheme is expected to be significantly faster than the \( O(n^2k^2) \) cost of re-initialization for larger values of \( n \).

### 7.6.2 Routing Changes

We simulate routing changes by forming an overlay network with 50 random end hosts on a real router topology. We then randomly select a link that is part of some path in the overlay and remove it, ensuring that no pair of overlay end hosts becomes disconnected. We assume the link is broken and re-route the affected paths. The algorithms in Section 5 incrementally incorporate each path change. Averaged over three runs, the results in Table 7 show that our system adapts quickly and maintains accurate path loss rate estimation.

| Metric | Value |
| --- | --- |
| Number of paths affected | 40.7 |
| Number of monitored paths affected | 36.3 |
| Number of unique nodes affected | 41.7 |
| Number of real lossy paths (before/after) | 761.0 / 784.0 |
| Coverage (before/after) | 99.8% / 99.8% |
| False positive rate (before/after) | 0.2% / 0.1% |
| Average running time | 17.3 seconds |

We also simulated the addition of a random link to some paths in the overlay. The results were similar, so they are omitted for brevity.

### 8. Internet Experiments

#### 8.1 Methodology

We implemented our system on the PlanetLab [33] testbed, deploying it on 51 PlanetLab hosts, each from a different organization (Table 8). All international PlanetLab hosts are universities.

| Area | .edu | .org | .net | .gov | .us | International |
| --- | --- | --- | --- | --- | --- | --- |
| US (40) | 33 | 3 | 2 | 1 | 1 | - |
| France | 1 | - | - | - | - | 1 |
| Sweden | 1 | - | - | - | - | 1 |
| Denmark | 1 | - | - | - | - | 1 |
| Germany | 1 | - | - | - | - | 1 |
| UK | 1 | - | - | - | - | 1 |
| Taiwan | 1 | - | - | - | - | 1 |
| Hong Kong | 1 | - | - | - | - | 1 |
| Europe (6) | - | - | - | - | - | 6 |
| Asia (2) | - | - | - | - | - | 2 |
| Canada | 1 | - | - | - | - | 1 |
| Australia | 1 | - | - | - | - | 1 |

First, we measured the topology among these sites by simultaneously running "traceroute" to find the paths from each host to all others. Each host saved its destination IP addresses for sending measurement packets later. We then measured the loss rates between every pair of hosts. Our measurement consisted of 300 trials, each lasting 300 milliseconds. During a trial, each host sent a 40-byte UDP packet to every other host. Typically, the hosts finished sending before the 300-millisecond trial was completed. For each path, the receiver counted the number of packets received out of 300 to calculate the loss rate.

To prevent any host from receiving too many packets simultaneously, each host sent packets to other hosts in a different random order. Furthermore, each host used a different permutation in each trial to ensure that each destination had an equal opportunity to be sent later in each trial. Such random permutations were pre-generated by each host. To ensure that all hosts in the network took measurements at the same time, we set up sender and receiver daemons and used a well-connected server to broadcast a "START" command.

We performed sensitivity analysis on the sending frequency (Figure 7). All experiments were executed between 1 AM and 3 AM PDT on June 24, 2003, when most networks are free. The traffic rate from or to each host is \((51 - 1) \times \text{sending freq} \times 40 \text{ bytes/sec}\). The number of lossy paths did not change much when the sending rate varied, except when the sending rate exceeded 12.8 Mbps, as many servers could not sustain that rate. We chose a 300-millisecond sending interval to balance quick loss rate statistics collection with moderate bandwidth consumption.

#### 8.2 Results

From June 24 to June 27, 2003, we ran the experiments 100 times, mostly during peak hours (9 AM to 6 PM PDT). Each experiment generated 51 × 50 × 300 = 765,000 UDP packets, totaling 76.5 million packets for all experiments. We ran the loss rate measurements three to four times every hour and the pairwise traceroute every two hours. Across the 100 runs, the average number of selected monitoring paths (\( \bar{G} \)) was 871.9, about one-third of the total number of end-to-end paths (2,550). Table 9 shows the loss rate distribution across all 100 runs. About 96% of the paths were non-lossy. Among the lossy paths, most loss rates were less than 0.5. Although we tried to choose stable nodes for experiments, about 25% of the lossy paths had 100% losses, likely due to node failures or other reachability problems, as discussed in Section 8.2.2.

| Loss Rate Range | Percentage |
| --- | --- |
| [0, 0.05) | 95.9% |
| [0.05, 0.1) | 15.2% |
| [0.1, 0.3) | 31.0% |
| [0.3, 0.5) | 23.9% |
| [0.5, 1.0) | 4.3% |

#### 8.2.1 Accuracy and Speed

When identifying lossy paths (loss rates > 0.05), the average coverage was 95.6%, and the average false positive rate was 2.75%. Figure 8 shows the cumulative distribution functions (CDFs) for the coverage and the false positive rate. Note that 40 runs had 100% coverage, and 90 runs had coverage over 85%. Additionally, 58 runs had no false positives, and 90 runs had false positive rates less than 10%.

The average absolute error across the 100 runs was 0.0027 for all paths and 0.0058 for lossy paths. We selected the run with the worst accuracy in coverage (69.2%) and plotted the CDFs of absolute errors and error factors in Figure 9. Since we used only 300 packets to measure the loss rate, the loss rate precision granularity was 0.0033, so we used ε = 0.005 for error factor calculation. The average error factor was 1.1 for all paths.

Even in the worst case, 95% of the absolute errors in loss rate estimation were less than 0.014, and 95% of the error factors were less than 2.1. To further view the overall statistics, we picked the 95th percentile of absolute errors and error factors in each run and plotted the CDFs on those metrics. The results are shown in Figure 10. Notice that 90 runs had the 95th percentile of absolute errors less than 0.0133, and 90 runs had the 95th percentile of error factors less than 2.0.

The average running time for selecting monitoring paths based on topology measurement was 0.75 seconds, and for loss rate calculation of all 2,550 paths, it was 0.16 seconds.

#### 8.2.2 Topology Error Handling

The limitations of traceroute, which we used to measure the topology among the end hosts, led to many inaccuracies. As found in [34], many routers on the paths among PlanetLab nodes have aliases. We did not use sophisticated techniques to resolve these aliases, so the topology we had was far from accurate. Furthermore, in the PlanetLab experiments, some nodes were down or unreachable from certain nodes, and some routers were hidden, providing only partial routing paths. Averaging over 14 sets of traceroutes, 245 out of 51 × 50 = 2,550 paths had no or incomplete routing information. Despite these issues, the accurate loss rate estimation results show that our topology error handling was successful.

### 9. Conclusions

In this paper, we improved, implemented, and evaluated an algebraic approach [1] for adaptive and scalable overlay network monitoring. For an overlay of \( n \) end hosts, we selectively monitor a basis set of \( O(n \log n) \) paths, which can fully describe all \( O(n^2) \) paths. The measurements of the basis set are used to infer the loss rates of all other paths. Our approach works in real-time, offers fast adaptation to topology changes, distributes balanced load to end hosts, and handles topology measurement errors. Both simulation and real Internet implementation yielded promising results.

For even more efficient monitored path selection, we plan to investigate the use of iterative methods [32, 35] both to select rows and to compute loss rate vectors. In our preliminary experiments, the path matrix \( G \) has been well-conditioned, suggesting that iterative methods may converge quickly.

### 10. Acknowledgments

We thank Brian Chavez for helping with dynamic simulations. We also thank James Demmel, the anonymous reviewers, and our shepherd for their valuable suggestions.

### 11. References

[1] Y. Chen, D. Bindel, and R. H. Katz, “Tomography-based overlay network monitoring,” in ACM SIGCOMM Internet Measurement Conference (IMC), 2003.
[2] Y. Chen, Towards a Scalable, Adaptive and Network-aware Content Distribution Network, Ph.D. thesis, University of California at Berkeley, Nov. 2003.
[3] D. G. Andersen et al., “Resilient overlay networks,” in Proc. of ACM SOSP, 2001.
[4] T. S. E. Ng and H. Zhang, “Predicting Internet network distance with coordinates-based approaches,” in Proc. of IEEE INFOCOM, 2002.
[5] S. Ratnasamy et al., “Topologically-aware overlay construction and server selection,” in Proc. of IEEE INFOCOM, 2002.
[6] P. Francis et al., “IDMaps: A global Internet host distance estimation service,” IEEE/ACM Trans. on Networking, Oct. 2001.
[7] Y. Chen et al., “On the stability of network distance estimation,” in ACM SIGMETRICS Performance Evaluation Review (PER), Sep. 2002.
[8] Mark Coates, Alfred Hero, Robert Nowak, and Bin Yu, “Internet Tomography,” IEEE Signal Processing Magazine, vol. 19, no. 3, pp. 47–65, 2002.
[9] T. Bu, N. Duffield, F. Presti, and D. Towsley, “Network tomography on general topologies,” in ACM SIGMETRICS, 2002.
[10] V. Padmanabhan, L. Qiu, and H. Wang, “Server-based inference of Internet link lossiness,” in IEEE INFOCOM, 2003.
[11] D. Rubenstein, J. F. Kurose, and D. F. Towsley, “Detecting shared congestion of flows via end-to-end measurement,” ACM Transactions on Networking, vol. 10, no. 3, 2002.
[12] Y. Shavitt, X. Sun, A. Wool, and B. Yener, “Computing the unmeasured: An algebraic approach to Internet mapping,” in IEEE INFOCOM, 2001.
[13] H. C. Ozmutlu et al., “Managing end-to-end network performance via optimized monitoring strategies,” Journal of Network and System Management, vol. 10, no. 1, 2002.
[14] C. Tang and P. McKinley, “On the cost-quality tradeoff in topology-aware overlay path probing,” in IEEE ICNP, 2003.
[15] R. Caceres, N. Duffield, J. Horowitz, and D. Towsley, “Multicast-based inference of network-internal loss characteristics,” IEEE Transactions in Information Theory, vol. 45, 1999.
[16] S. Floyd and V. Jacobson, “Random early detection gateways for congestion avoidance,” IEEE/ACM Transactions on Networking, vol. 1, no. 4, 1993.
[17] N. Duffield et al., “Multicast-based loss inference with missing data,” IEEE Journal of Selected Areas of Communications, vol. 20, no. 4, 2002.
[18] G.H. Golub and C.F. Van Loan, Matrix Computations, The Johns Hopkins University Press, 1989.
[19] E. Anderson et al., LAPACK Users’ Guide, Society for Industrial and Applied Mathematics, Philadelphia, PA, third edition, 1999.
[20] Y. Zhang et al., “On the constancy of Internet path properties,” in Proc. of SIGCOMM IMW, 2001.
[21] J.W. Demmel, Applied Numerical Linear Algebra, SIAM, 1997.
[22] H. Tangmunarunkit et al., “Network topology generators: Degree-based vs structural,” in ACM SIGCOMM, 2002.
[23] M. Faloutsos, P. Faloutsos, and C. Faloutsos, “On power-law relationship of the Internet topology,” in ACM SIGCOMM, 1999.
[24] A. Medina, I. Matta, and J. Byers, “On the origin of power laws in Internet topologies,” in ACM Computer Communication Review, Apr. 2000.
[25] R. Govindan and H. Tangmunarunkit, “Heuristics for Internet map discovery,” in IEEE INFOCOM, 2000.
[26] N. Spring, R. Mahajan, and D. Wetherall, “Measuring ISP topologies with Rocketfuel,” in ACM SIGCOMM, 2002.
[27] L. Subramanian, S. Agarwal, J. Rexford, and R. H. Katz, “Characterizing the Internet hierarchy from multiple vantage points,” in IEEE INFOCOM, 2002.
[28] G. W. Stewart, Matrix Algorithms: Basic Decompositions, Society for Industrial and Applied Mathematics, 1998.
[29] V. Paxson, “End-to-end routing behavior in the Internet,” IEEE/ACM Transactions on Networking, vol. 5, no. 5, 1997.
[30] Y. Zhang, V. Paxson, and S. Shenker, “The stationarity of Internet path properties: Routing, loss, and throughput,” ACIRI Technical Report, May 2000.
[31] V. Paxson, “End-to-end Internet packet dynamics,” in ACM SIGCOMM, 1997.
[32] R. Barrett et al., Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods, 2nd Edition, SIAM, Philadelphia, PA, 1994.
[33] PlanetLab, “http://www.planet-lab.org/.”
[34] N. Spring, D. Wetherall, and T. Anderson, “Scriptroute: A facility for distributed Internet measurement,” in USITS, 2003.
[35] C. Meyer and D. Pierce, “Steps toward an iterative rank-revealing method,” Tech. Rep. ISSTECH-95-013, Boeing Information and Support Services, 1995.