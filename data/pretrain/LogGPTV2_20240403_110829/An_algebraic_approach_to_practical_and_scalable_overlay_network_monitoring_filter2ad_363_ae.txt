Similarly, for removing end hosts, we start with an overlay
network of 60 random end hosts, then randomly select an
end host to delete from the overlay, and repeat the process
until the size of the overlay is reduced to 55 and 50. Again,
the accumulated numerical error is negligible as shown in Ta-
ble 6. As shown in Sec. 5, deleting a path in ¯G is much more
complicated than adding a path. With the same machine,
the average time for deleting a path is 445 msec, and for
deleting a node, 16.9 seconds. We note that the current im-
plementation is not optimized: we can speed up node dele-
tion by processing several paths simultaneously, and we can
speed up path addition and deletion with iterative methods
such as CGNE or GMRES [32]. Since the time to add/delete
a path is O(k2), and to add/delete a node is O(nk2), we ex-
pect our updating scheme to be substantially faster than the
O(n2k2) cost of re-initialization for larger n.
7.6.2 Routing changes
# of paths aﬀected
# of monitored paths aﬀected
# of unique nodes aﬀected
40.7
36.3
41.7
# of real lossy paths (before/after)
coverage (before/after)
false positive rate (before/after)
average running time
761.0/784.0
99.8%/99.8%
0.2%/0.1%
17.3 seconds
Table 7: Simulation results for removing a link from
a real router topology.
We form an overlay network with 50 random end hosts
on the real router topology. Then we simulate topology
changes by randomly choosing a link that is on some path
of the overlay and removing of such a link will not cause
disconnection for any pair of overlay end hosts. Then we
assume that the link is broken, and re-route the aﬀected
path(s). Algorithms in Sec. 5 incrementally incorporate each
path change. Averaged over three runs, results in Table 7
show that we adapt quickly, and still have accurate path loss
rate estimation.
We also simulate the topology changes by adding a ran-
dom link on some path(s) of the overlay. The results are
similar as above, so we omit them here for brevity.
8.
INTERNET EXPERIMENTS
8.1 Methodology
We implemented our system on the PlanetLab [33] testbed,
and deployed it on 51 PlanetLab hosts, each from a diﬀer-
ent organization as shown in Table 8. All the international
PlanetLab hosts are universities.
Areas and Domains
.edu
.org
.net
.gov
.us
US (40)
Inter-
national
(11)
France
Sweden
Denmark
Germany
UK
Taiwan
Hong Kong
Europe
(6)
Asia
(2)
Canada
Australia
# of hosts
33
3
2
1
1
1
1
1
1
2
1
1
2
1
Table 8: Distribution of selected PlanetLab hosts.
First, we measure the topology among these sites by si-
multaneously running “traceroute” to ﬁnd the paths from
each host to all others. Each host saves its destination IP
addresses for sending measurement packets later. Then we
measure the loss rates between every pair of hosts. Our mea-
surement consists of 300 trials, each of which lasts 300 msec.
During a trial, each host sends a 40-byte UDP packet 3 to
every other host. Usually the hosts will ﬁnish sending before
the 300 msec trial is ﬁnished. For each path, the receiver
counts the number of packets received out of 300 to calculate
the loss rate.
To prevent any host from receiving too many packets si-
multaneously, each host sends packets to other hosts in a
diﬀerent random order. Furthermore, any single host uses a
diﬀerent permutation in each trial so that each destination
has equal opportunity to be sent later in each trial. This is
because when sending packets in a batch, the packets sent
later are more likely to be dropped. Such random permu-
tations are pre-generated by each host. To ensure that all
hosts in the network take measurements at the same time,
we set up sender and receiver daemons, then use a well-
connected server to broadcast a “START” command.
Will the probing traﬃc itself cause losses? We performed
sensitivity analysis on sending frequency as shown in Fig. 7.
320-byte IP header + 8-byte UDP header + 12-byte data
on sequence number and sending time.
All experiments were executed between 1am-3am PDT June
24, 2003, when most networks are free. The traﬃc rate from
or to each host is (51 − 1) × sending freq × 40 bytes/sec.
The number of lossy paths does not change much when the
sending rate varies, except when the sending rate is over
12.8Mbps, since many servers can not sustain that sending
rate. We choose a 300 msec sending interval to balance
quick loss rate statistics collection with moderate bandwidth
consumption.
16
 1600
s
h
t
a
p
y
s
s
o
l
f
o
r
e
b
m
u
N
 1400
 1200
 1000
 800
 600
 400
 200
 0
 1
Bandwidth consumption (Kbps)
160
1600
16000
 10
 100
 1000
Sending frequency (number of trials per second)
Figure 7: Sensitivity test of sending frequency
Note that the experiments above use O(n2) measurements
so that we can compare the real loss rates with our inferred
In fact, our technique only requires O(n log n)
loss rates.
measurements. Thus, given good load balancing, each host
only needs to send to O(log n) hosts.
In fact, we achieve
similar CV and MMR for measurement load distribution
as in the simulation. Even for an overlay network of 400
end hosts on the 284K-node real topology used before, k
= 18668. If we reduce the measurement frequency to one
trial per second, the traﬃc consumption for each host is
18668/400× 40 bytes/sec = 14.9Kbps, which is typically less
than 5% of the bandwidth of today’s “broadband” Internet
links. We can use adaptive measurement techniques in [3]
to further reduce the overheads.
loss
rate
%
[0,
0.05)
95.9%
[0.05, 0.1)
15.2%
lossy path [0.05, 1.0] (4.1%)
[0.1, 0.3)
[0.3, 0.5)
[0.5, 1.0)
31.0%
23.9%
4.3%
Table 9: Loss rate distribution: lossy vs. non-lossy
and the sub-percentage of lossy paths.
8.2 Results
From June 24 to June 27, 2003, we ran the experiments
100 times, mostly during peak hours 9am - 6pm PDT. Each
experiment generates 51 × 50× 300 = 765K UDP packets,
totaling 76.5M packets for all experiments. We run the loss
rate measurements three to four times every hour, and run
the pair-wise traceroute every two hours. Across the 100
runs, the average number of selected monitoring paths ( ¯G) is
871.9, about one third of total number of end-to-end paths,
2550. Table 9 shows the loss rate distribution on all the
paths of the 100 runs. About 96% of the paths are non-
lossy. Among the lossy paths, most of the loss rates are
less than 0.5. Though we try to choose stable nodes for
experiments, about 25% of the lossy paths have 100% losses
and are likely caused by node failures or other reachability
problems as discussed in Sec. 8.2.2.
8.2.1 Accuracy and speed
When identifying the lossy paths (loss rates > 0.05), the
average coverage is 95.6% and the average false positive rate
is 2.75%. Fig. 8 shows the CDFs for the coverage and the
false positive rate. Notice that 40 runs have 100% coverage
and 90 runs have coverage over 85%. 58 runs have no false
positives and 90 runs have false positive rates less than 10%.
 0
 2
 4
 100
False positive rate (%)
 6
 10
 8
 12
 14
 16
 18
)
%
(
e
g
a
t
n
e
c
r
e
P
e
v
i
t
l
a
u
m
u
C
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
 100
Coverage of lossy paths
False positive rate
 95
 90
 85
 80
 75
Coverage of lossy paths (%)
Figure 8: Cumulative percentage of the coverage
and the false positive rates for lossy path inference
in the 100 experiments.
As in the simulations, many of the false positives and false
negatives are caused by the 5% threshold boundary eﬀect.
The average absolute error across the 100 runs is only 0.0027
for all paths, and 0.0058 for lossy paths. We pick the run
with the worst accuracy in coverage (69.2%), and plot the
CDFs of absolute errors and error factors in Fig. 9. Since
we only use 300 packets to measure the loss rate, the loss
rate precision granularity is 0.0033, so we use ε = 0.005 for
error factor calculation. The average error factor is only 1.1
for all paths.
Error factors of loss rate estimation
 1.5
 2
 2.5
 3
 3.5
 4
 1
 100
)
%
(
e
g
a
t
n
e
c
r
e
P
e
v
i
t
l
a
u
m
u
C
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
1.0
25.6%
 0
 0.005
 0.015
Absolute errors of loss rate estimation
 0.01
 0.02
Absolute error
Error factor
Figure 9: Cumulative percentage of the absolute er-
rors and error factors for the experiment with the
worst accuracy in coverage.
Even for the worst case, 95% of absolute errors in loss
rate estimation are less than 0.014, and 95% of error factors
are less than 2.1. To further view the overall statistics, we
pick 95 percentile of absolute errors and error factors in each
run, and plot the CDFs on those metrics. The results are
shown in Fig. 10. Notice that 90 runs have the 95 percentile
of absolute errors less than 0.0133, and 90 runs have the 95
percentile of error factors less than 2.0.
The average running time for selecting monitoring paths
based on topology measurement is 0.75 second, and for loss
rate calculation of all 2550 paths is 0.16 second.
8.2.2 Topology error handling
The limitation of traceroute, which we use to measure the
topology among the end hosts, led to many topology mea-
surement inaccuracies. As found in [34], many of the routers
on the paths among PlanetLab nodes have aliases. We did
not use sophisticated techniques to resolve these aliases.
Thus, the topology we have is far from accurate. Further-
more, in the PlanetLab experiments, some nodes were down,
 1
 100
 1.2
95 percentile of error factors
 1.4
 1.8
 1.6
 2
 2.2
)
%
(
e
g
a
t
n
e
c
r
e
P
e
v
i
t
l
a
u
m
u
C
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
95 percentile of absolute errors
95 percentile of error factors
 0
 0.005
 0.01
 0.015
 0.02
 0.025
 0.03
95 percentile of absolute errors
Figure 10: Cumulative percentage of the 95 per-
centile of absolute errors and error factors for the
100 experiments.
or were unreachable from certain nodes. Meanwhile, some
routers are hidden and we only get partial routing paths.
Averaging over 14 sets of traceroutes, 245 out of 51 × 50 =
2550 paths have no or incomplete routing information. The
accurate loss rate estimation results show that our topology
error handling is successful.
9. CONCLUSIONS
In this paper, we improve, implement and evaluate an al-
gebraic approach [1] for adaptive scalable overlay network
monitoring. For an overlay of n end hosts, we selectively
monitor a basis set of O(n log n) paths which can fully de-
scribe all the O(n2) paths. Then the measurements of the
basis set are used to infer the loss rates of all other paths.
Our approach works in real time, oﬀers fast adaptation to
topology changes, distributes balanced load to end hosts,
and handles topology measurement errors. Both simulation
and real Internet implementation yield promising results.
For even more eﬃcient monitored path selection, we plan
to investigate the use of iterative methods [32], [35] both
to select rows and to compute loss rate vectors.
In our
preliminary experiments, the path matrix G has been well-
conditioned, which suggests that iterative methods may con-
verge quickly.
10. ACKNOWLEDGEMENT
We thank Brian Chavez for helping with dynamic sim-
ulations. We also thank James Demmel, the anonymous
reviewers and our shepherd for their valuable suggestions.
11. REFERENCES
[1] Y. Chen, D. Bindel, and R. H. Katz, “Tomography-based
overlay network monitoring,” in ACM SIGCOMM Internet
Measurement Conference (IMC), 2003.
[2] Y. Chen, Towards a Scalable, Adaptive and Network-aware
Content Distribution Network, Ph.D. thesis, University of
California at Berkeley, Nov. 2003.
[3] D. G. Andersen et al., “Resilient overlay networks,” in
Proc. of ACM SOSP, 2001.
[4] T. S. E. Ng and H. Zhang, “Predicting Internet network
distance with coordinates-based approaches,” in Proc.of
IEEE INFOCOM, 2002.
[5] S. Ratnasamy et al., “Topologically-aware overlay
construction and server selection,” in Proc. of IEEE
INFOCOM, 2002.
[6] P. Francis et al., “IDMaps: A global Internet host distance
estimation service,” IEEE/ACM Trans. on Networking,
Oct. 2001.
[7] Y. Chen et al., “On the stability of network distance
estimation,” in ACM SIGMETRICS Performance
Evaluation Review (PER), Sep. 2002.
[8] Mark Coates, Alfred Hero, Robert Nowak, and Bin Yu,
“Internet Tomography,” IEEE Signal Processing Magazine,
vol. 19, no. 3, pp. 47–65, 2002.
[9] T. Bu, N. Duﬃeld, F. Presti, and D. Towsley, “Network
tomography on general topologies,” in ACM
SIGMETRICS, 2002.
[10] V. Padmanabhan, L. Qiu, and H. Wang, “Server-based
inference of Internet link lossiness,” in IEEE INFOCOM,
2003.
[11] D. Rubenstein, J. F. Kurose, and D. F. Towsley, “Detecting
shared congestion of ﬂows via end-to-end measurement,”
ACM Transactions on Networking, vol. 10, no. 3, 2002.
[12] Y. Shavitt, X. Sun, A. Wool, and B. Yener, “Computing
the unmeasured: An algebraic approach to Internet
mapping,” in IEEE INFOCOM, 2001.
[13] H. C. Ozmutlu et al., “Managing end-to-end network
performance via optimized monitoring strategies,” Journal
of Network and System Management, vol. 10, no. 1, 2002.
[14] C. Tang and P. McKinley, “On the cost-quality tradeoﬀ in
topology-aware overlay path probing,” in IEEE ICNP,
2003.
[15] R. Caceres, N. Duﬃeld, J. Horowitz, and D. Towsley,
“Multicast-based inference of network-internal loss
characteristics,” IEEE Transactions in Information
Theory, vol. 45, 1999.
[16] S. Floyd and V. Jacobson, “Random early detection
gateways for congestion avoidance,” IEEE/ACM
Transactions on Networking, vol. 1, no. 4, 1993.
[17] N. Duﬃeld et al., “Multicast-based loss inference with
missing data,” IEEE Journal of Selected Areas of
Communications, vol. 20, no. 4, 2002.
[18] G.H. Golub and C.F. Van Loan, Matrix Computations, The
Johns Hopkins University Press, 1989.
[19] E. Anderson et al., LAPACK Users’ Guide, Society for
Industrial and Applied Mathematics, Philadelphia, PA,
third edition, 1999.
[20] Y. Zhang et al., “On the constancy of Internet path
properties,” in Proc. of SIGCOMM IMW, 2001.
[21] J.W. Demmel, Applied Numerical Linear Algebra, SIAM,
1997.
[22] H. Tangmunarunkit et al., “Network topology generators:
Degree-based vs structural,” in ACM SIGCOMM, 2002.
[23] M. Faloutsos, P. Faloutsos, and C. Faloutsos, “On
power-law relationship of the Internet topology,” in ACM
SIGCOMM, 1999.
[24] A. Medina, I. Matta, and J. Byers, “On the origin of power
laws in Internet topologies,” in ACM Computer
Communication Review, Apr. 2000.
[25] R. Govindan and H. Tangmunarunkit, “Heuristics for
Internet map discovery,” in IEEE INFOCOM, 2000.
[26] N. Spring, R. Mahajan, and D. Wetherall, “Measuring isp
topologies with rocketfuel,” in ACM SIGCOMM, 2002.
[27] L. Subrmanian, S. Agarwal, J. Rexford, and R. H.Katz,
“Characterizing the Internet hierarchy from multiple
vantage points,” in IEEE INFOCOM, 2002.
[28] G. W. Stewart, Matrix Algorithms: Basic Decompositions,
Society for Industrial and Applied Mathematics, 1998.
[29] V. Paxon, “End-to-end routing behavior in the Internet,”
IEEE/ACM Transactions on Networking, vol. 5, no. 5,
1997.
[30] Y. Zhang, V. Paxson, and S. Shenker, “The stationarity of
Internet path properties: Routing, loss, and throughput,”
ACIRI Technical Report, May, 2000.
[31] V. Paxon, “End-to-end Internet packet dynamics,” in ACM
SIGCOMM, 1997.
[32] R. Barrett et al., Templates for the Solution of Linear
Systems: Building Blocks for Iterative Methods, 2nd
Edition, SIAM, Philadelphia, PA, 1994.
[33] PlanetLab, “http://www.planet-lab.org/,” .
[34] N. Spring, D. Wetherall, and T. Anderson, “Scriptroute: A
facility for distributed internet measurement,” in USITS,
2003.
[35] C. Meyer and D. Pierce, “Steps toward an iterative
rank-revealing method,” Tech. Rep. ISSTECH-95-013,
Boeing Information and Support Services, 1995.