45s
Test Time
–
–
–
Number of Parameters
14
256
256
Algorithm 3 Adversarial Training against adversarial attacks in
wireless communication systems
Randomly initialize underlying DNN-based network 𝑁
D𝑡𝑟 ← training data
L𝑓 ← DNN-based loss function
M ← domain remapping function
R ← domain regularizations function
𝑇 ← epochs
𝑍 ← []
for epoch 𝑡 ∈ {1· · ·𝑇} do
// List of adversarial perturbations
Train the model 𝑁 for one epoch on training dataset D𝑡𝑟
𝑍 ← generate crafted adversarial samples using generated
perturbations by Algorithm 1 (or the single vector UAP)
end for
D𝑡𝑟 .extend(D𝑡𝑟 + 𝑍)
return 𝑁
9.1 The Adversarial Training Defense
Many defenses have been designed for adversarial examples in
image classification applications, particularly, adversarial training,
gradient masking, and region-based classification. In adversarial
training [27, 30, 46], in each iteration of training, the defender gen-
erates a set of adversarial examples and uses them in the training
phase by expanding the training dataset. In our work, we use adver-
sarial training where the defender uses UAPs crafted by our attack
to make the target DNN-based wireless model robust against the at-
tacks. The defender trains the DNN-based model for one epoch and
then generates input-agnostic adversarial perturbations from all
possible settings using Algorithm 1. Then, she extends the training
dataset by including all of the adversarial samples generated by the
adversary and trains the DNN-based model on the augmented train-
ing dataset. Algorithm 3 is a high level description of our defense
algorithm.
In the case of a single vector UAP attack, the defender uses a
single perturbation vector to generate adversarial samples and train
the target model. For the perturbation generator attack, we assume
that the defender is structure-aware and uses her trained PGM to
generate adversarial samples and train the target model.
9.2 The Perturbation Subtraction Defense
This is a defense specialized to our domain. In this defense, at the
receiver side, the defender performs operations on the perturbed
received signal based on her knowledge of the adversary to remove
the effect of the perturbation and reconstruct the originally trans-
mitted signal. For the single vector UAP attack, since we assume
that the defender has identified an estimate of the perturbation, she
can easily subtract her estimate of perturbation from the received
signal and obtain the originally transmitted signal. If the adver-
sary uses a PGM, as mentioned above, we consider three scenarios
based on the knowledge of the defender: ad hoc defender, structure-
aware, and model-aware defenders. In all of the scenarios, once
the defender receives the received perturbed signal, she generates
a perturbation using her PGM and subtracts it from the received
signal.
9.3 Randomized Smoothing Defense
We evaluate our attack against a defense mechanism called certified
defense [4, 5, 14] that relies on randomized smoothing. Random-
ized smoothing is a certified defense approach against adversarial
examples, which augments the training set with Gaussian noise
to increase the robustness of the classifier to multiple gradient di-
rections. The standard deviation of the Gaussian noise 𝜎 and the
number of the noisy samples added to each training sample 𝑘 are
the two parameters the defender can control in randomized smooth-
ing. In the prediction phase, for each perturbed test sample, we
draw 𝑘 Gaussian noise samples with the same standard deviation
and label them with the DNN model. The defense results can be
certified with a desired confidence using a two-sided hypothesis
test. If the test is satisfied, the DNN model is very confident in its
prediction, and if not, the DNN model abstains and does not make a
prediction. Recently, Kim et al. [23] utilize randomized smoothing
as a defense mechanism against single vector UAP attack in the
modulation recognition application. We use the same approach to
evaluate randomized smoothing against our attack.
9.4 Results
First, we evaluate the performance of our PGM attack against the
perturbation subtraction defense for the three defender types. Fig-
ure 6 shows the performance of each target wireless communication
system against the PGM adversarial attack in the presence of the
ad hoc, structure-aware, and model-aware defenders. In the experi-
ments involving the ad hoc defender, we use 10000 pilot signals and
take the average of the obtained noise to estimate the perturbation.
We see that a model-aware defender can completely degrade the per-
formance of the adversarial attack; however, as mentioned earlier,
a model-aware defender is not practical. Although our adversarial
attack provides the attacker a large set of adversarial perturbations,
using the same PGM removes the effect of the adversarial attack.
However, a structure-aware defender that trains her PGM and ob-
tains its parameters that are different from the PGM parameters
used by the attacker not only degrades the effect of the attack but
improves its performance as well.
Furthermore, we see that the ad hoc defender also degrades the
performance of the adversarial attack using pilot signals, which
shows that the generated perturbations are too similar to each other
Session 1B: Attacks and Robustness CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea136(a) Autoencoder communication system, PSR=−6𝑑𝐵
(b) Modulation recognition system, SNR=10𝑑𝐵
(c) OFDM system, PSR=−10𝑑𝐵
Figure 6: Performance of the three target communication systems against the PGM attack with the presence of a perturbation
subtraction defense for the structure-aware, model-aware, and ad hoc defenders. Enforcing the distance constraint in the
training process of the PGM removes the effect of the ad hoc defender.
such that the defender cancels them by just subtracting out a simple
estimate of the perturbations. To prevent this, the attacker enforces
a distance constraint as discussed in Section 6 to maximize the 𝑙2
distance between the generated perturbations. Figure 6 shows that
by enforcing the distance constraint in the training process of the
PGM, the attacker removes the effect of the ad hoc defender. In
such a case, the ad hoc defender cannot generate a precise estimate
of the perturbations. We believe that even if a defender takes the
distance constraint into account by performing adversarial training,
the attack remains robust as we will show in this Section that the
PGM attack is robust against adversarial training defense.
We also compare the performance of our attack and the single
vector UAP attack against different defense algorithms. Figure 7
shows the performance of our adversarial attack and the single
vector UAP attack against the mentioned defense methods. Note
that with both defense methods, we only consider a structure-aware
defender that only knows the structure of the PGM model used
by the attacker and not its parameters. Furthermore, as mentioned
above, in the single vector UAP attack, we assume that the defender
can use pilot signals to estimate the single perturbation. In our
experiments, we obtain this estimate by sending 10000 pilot signals
and averaging their resulted perturbation. Hence, in the adversarial
training defense, we only use the estimated perturbation to generate
adversarial samples and train the target DNN-based model.
In the single vector UAP attack, as Figure 7 illustrates, subtracting
the estimated noise from the received signal at the receiver side
destroys the effect of the attack completely; However, the same
defense mechanism is not effective against the PGM attack which
implies that the PGM attack shows more robustness against the
perturbation subtraction defense compared to the single vector
UAP attack.
Furthermore, while using the adversarial training defense by the
defender, our PGM attack is still more robust than the single vector
UAP attack. The reason is that the underlying DNN-based model
can learn the single perturbation generated by the attacker for all of
the inputs. Using a PGM enables the attacker to access an extremely
large set of perturbations which makes it infeasible for the defender
to learn the DNN-based model based on all of them. Based on our
results, we conclude that using a PGM to perform adversar-
ial attacks against wireless communication systems is more
robust against various defense mechanisms in comparison
to using only a single perturbation vector.
We also evaluate the performance of our PGM attack on mod-
ulation recognition while deploying the randomized smoothing
defense. Table 3 shows the performance of the modulation recogni-
tion system for different values of 𝑘 and 𝑠𝑖𝑔𝑚𝑎 and a 95% confidence
threshold. We see that for some values of 𝑘 and 𝜎 using this defense
alleviates the effect of the PGM attack and increases the accuracy
of the DNN-based classifier in the modulation recognition task.
However, these high accuracies are just for the samples satisfying
the confidence threshold, while as Table 3 illustrates the classifier
has confidence to make predictions for only less than half of the
test samples, thus showing that this defense is not practical against
our attack. Moreover, for real-time applications such as autoencoder
communication systems and OFDM systems, not having enough
confidence for half of the received signals would be unacceptable.
10 DISCUSSIONS
Our work demonstrates a major vulnerability of the rapidly emerg-
ing DNN-based wireless communication systems. We show that
an adversary can leverage well-crafted universal adversarial per-
turbations that are tailored to specific domain constraints of these
systems to degrade their functionality. We see that wireless domain
constraints bring up major challenges (synchronizing the attacker
and the receiver) for the succession of the attack. However, im-
portant features of adversarial attacks such as transferability and
learnability enable us to overcome these challenges by designing
a input-agnostic, undetectable, and robust adversarial attack. Fur-
thermore, our experiments show that existing defense mechanisms
against adversarial examples cannot mitigate the performance of
our attack urging that more sophisticated defense mechanisms
should be deployed to alleviate this vulnerability.
02468101214SNR (dB)10−510−410−310−210−1Block-error rate (log)no attackPGM attack with no defensemodel-aware defenderstructure-aware defenderad hoc defenderad hoc defender with distance constraint−20−18−16−14−12−10PSR (dB)0.30.40.50.60.7Accuracyno attack, PSR = 0dBPGM attack with no defensemodel-aware defenderstructure-aware defenderad hoc defenderad hoc defender with distance constraint510152025SNR (dB)10−210−1BER (log)no attackPGM attack with no defensemodel-aware defenderstructure-aware defenderad hoc defenderad hoc defender with distance constraintSession 1B: Attacks and Robustness CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea137(a) Autoencoder communication system, PSR=−6𝑑𝐵
(b) Modulation recognition system, SNR=10𝑑𝐵
(c) OFDM system, PSR=−10𝑑𝐵
Figure 7: Performance of the single vector UAP attack and PGM attack against adversarial training and perturbation sub-
traction defenses. The adversarial training defense is less effective than perturbation subtraction; however, even with the
adversarial training defense, our attack is still robust compared to the single vector UAP attack.
Table 3: Performance of the modulation recognition system against PGM attack while deploying randomized smoothing de-
fense.
Randomized smoothing parameters Modulation recognition accuracy Fraction of samples satisfying confidence threshold
0.2576
0.5315
0.2984
0.4712
0.3239
0.4976
𝑘 = 10, 𝜎 = 0.01
𝑘 = 20, 𝜎 = 0.01
𝑘 = 10, 𝜎 = 0.001
𝑘 = 20, 𝜎 = 0.001
𝑘 = 10, 𝜎 = 0.0001
𝑘 = 20, 𝜎 = 0.0001
25.76%
16.04%
87.97%
81.91%
87.68%
89.99%
Similar to the previous works on this (emerging) topic [1, 12, 23,
24, 42–44], our evaluations mainly rely on simulations based on ra-
dio datasets. Future work can validate our results by experimenting
on actual wireless systems.
Another direction of future work can focus on investigating more
sophisticated defenses against adversarial perturbations. As we
have shown in this work, our PGM attack is robust against various
defense mechanisms even the ones that are specific to the target
wireless system’s domain (i.e., ad hoc defenders). Nevertheless, a
defense mechanism specifically designed for universal adversarial
perturbations may offer greater success in defending against the
presented attacks.
11 CONCLUSION
In this paper, we propose an adversarial attack using a perturbation
generator model (PGM) against DNN-based wireless communica-
tion systems. In the absence of countermeasures deployed by the
communicating parties, our attacks perform slightly better than
existing adversarial attacks. More importantly, against communi-
cating parties employing significant defenses, our techniques are
robust and show significant gains over previous approaches. We
also show that our attack is effective in a black-box scenario where
the attacker generates the adversarial perturbations using a substi-
tute DNN wireless model and uses the perturbation to attack the
original DNN wireless model. We use remapping and regularizer
functions to enforce an undetectability constraint for the pertur-
bations, which makes the perturbations indistinguishable from
random Gaussian noise. Furthermore, we use a regularizer function
to enforce a distance constraint to degrade the performance of an ad
hoc defender who tries to obtain an estimation of the perturbations
using pilot signals. Our work shows that even in the presence of sub-
stantial defense mechanisms deployed by communication parties,
DNN-based wireless systems are highly vulnerable to adversarial at-
tacks. Hence, whereas there has been significant enthusiasm about
such DNN-based approaches, our work suggests that there are also
significant challenges to obtaining robust performance with such
schemes.
ACKNOWLEDGEMENTS
We would like to thank our shepherd Yuan Tian and anonymous
reviewers for their comments. The work was supported by the
NSF CAREER grant CNS-1553301, NSF grant ECCS-2029323, and
by DARPA and NIWC under contract N66001-15-C-4067. The U.S.
Government is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright notation
thereon. The views, opinions, and/or findings expressed are those
of the author(s) and should not be interpreted as representing the
official views or policies of the Department of Defense or the U.S.
Government. Milad Nasr was supported by a Google PhD Fellow-
ship in Security and Privacy.