### Adversarial Training for Robustness in Wireless Communication Systems

#### 45s Test Time
- **Number of Parameters**: 14, 256, 256

#### Algorithm 3: Adversarial Training against Adversarial Attacks in Wireless Communication Systems
1. **Initialization**:
   - Randomly initialize the DNN-based network \( N \).
   - Define the training data as \( D_{\text{tr}} \).
   - Define the DNN-based loss function as \( L_f \).
   - Define the domain remapping function as \( M \).
   - Define the domain regularization function as \( R \).
   - Set the number of epochs as \( T \).
   - Initialize an empty list \( Z \) to store adversarial samples.

2. **Training Loop**:
   - For each epoch \( t \) from 1 to \( T \):
     - Train the model \( N \) for one epoch on the training dataset \( D_{\text{tr}} \).
     - Generate crafted adversarial samples using perturbations generated by Algorithm 1 (or a single vector UAP).
     - Append these adversarial samples to \( Z \).

3. **Extend Training Data**:
   - Extend the training dataset \( D_{\text{tr}} \) by adding the adversarial samples \( Z \).

4. **Return the Trained Model**:
   - Return the trained DNN-based network \( N \).

### 9.1 The Adversarial Training Defense
Adversarial training is a widely used defense mechanism in image classification applications. In this approach, the defender generates adversarial examples in each iteration and includes them in the training phase to expand the training dataset. In our work, we apply adversarial training to DNN-based wireless communication systems. Specifically, the defender uses Universal Adversarial Perturbations (UAPs) crafted by our attack to enhance the robustness of the target model. The process involves:
1. Training the DNN-based model for one epoch.
2. Generating input-agnostic adversarial perturbations using Algorithm 1.
3. Extending the training dataset with these adversarial samples.
4. Re-training the DNN-based model on the augmented dataset.

For a single vector UAP attack, the defender uses a single perturbation vector. For the perturbation generator attack, the defender, assuming structure-aware, uses her trained Perturbation Generator Model (PGM) to generate and train the target model.

### 9.2 The Perturbation Subtraction Defense
This defense is specific to our domain. At the receiver side, the defender performs operations to remove the effect of the perturbation and reconstruct the originally transmitted signal. For a single vector UAP attack, the defender subtracts the estimated perturbation from the received signal. For PGM attacks, the defender's knowledge is categorized into three scenarios:
- **Ad hoc Defender**: Uses pilot signals to estimate and subtract the perturbation.
- **Structure-aware Defender**: Knows the structure of the PGM but not its parameters.
- **Model-aware Defender**: Knows both the structure and parameters of the PGM.

### 9.3 Randomized Smoothing Defense
Randomized smoothing is a certified defense that augments the training set with Gaussian noise to increase robustness. The defender controls two parameters: the standard deviation \( \sigma \) of the Gaussian noise and the number of noisy samples \( k \) added to each training sample. During prediction, \( k \) Gaussian noise samples are drawn and labeled by the DNN model. The defense results can be certified with a desired confidence using a two-sided hypothesis test. If the test is satisfied, the DNN model makes a confident prediction; otherwise, it abstains. Kim et al. [23] used randomized smoothing as a defense against single vector UAP attacks in modulation recognition. We use the same approach to evaluate randomized smoothing against our attack.

### 9.4 Results
We evaluated the performance of our PGM attack against various defenses. Figure 6 shows the performance of the target wireless communication systems under different defender types. A model-aware defender can completely degrade the attack's performance, but such a defender is impractical. A structure-aware defender, who trains her PGM with different parameters, degrades the attack and improves the system's performance. An ad hoc defender also degrades the attack's performance using pilot signals, indicating that the generated perturbations are too similar. To counter this, the attacker enforces a distance constraint to maximize the \( l_2 \) distance between perturbations, making the ad hoc defender ineffective.

Figure 7 compares the performance of our PGM attack and the single vector UAP attack against different defense methods. The PGM attack is more robust against the perturbation subtraction defense and adversarial training defense compared to the single vector UAP attack. Table 3 shows the performance of the modulation recognition system under randomized smoothing. While some parameter settings alleviate the attack, the classifier has confidence for only a fraction of the test samples, making the defense impractical for real-time applications.

### 10. Discussions
Our work highlights a significant vulnerability in DNN-based wireless communication systems. Adversaries can craft universal adversarial perturbations tailored to these systems' domain constraints, degrading their functionality. Despite challenges like synchronization, our attack is robust and effective even in black-box scenarios. Existing defense mechanisms are insufficient, suggesting the need for more sophisticated defenses.

### 11. Conclusion
We propose an adversarial attack using a PGM against DNN-based wireless communication systems. Our attack outperforms existing methods, especially in the presence of substantial defenses. We use remapping and regularizer functions to enforce undetectability and distance constraints, making the perturbations indistinguishable from random noise and degrading the performance of ad hoc defenders. Our work underscores the vulnerability of DNN-based wireless systems to adversarial attacks, highlighting the need for robust defense mechanisms.

### Acknowledgements
We thank our shepherd Yuan Tian and anonymous reviewers for their feedback. This work was supported by NSF CAREER grant CNS-1553301, NSF grant ECCS-2029323, DARPA, and NIWC under contract N66001-15-C-4067. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes. The views expressed are those of the authors and do not represent the official views or policies of the Department of Defense or the U.S. Government. Milad Nasr was supported by a Google PhD Fellowship in Security and Privacy.