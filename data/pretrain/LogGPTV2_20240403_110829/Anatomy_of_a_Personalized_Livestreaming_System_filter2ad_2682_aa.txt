title:Anatomy of a Personalized Livestreaming System
author:Bolun Wang and
Xinyi Zhang and
Gang Wang and
Haitao Zheng and
Ben Y. Zhao
Anatomy of a Personalized Livestreaming System
Bolun Wang†, Xinyi Zhang†, Gang Wang† ‡, Haitao Zheng† and Ben Y. Zhao†
†Department of Computer Science, UC Santa Barbara
‡Department of Computer Science, Virginia Tech
{bolunwang, xyzhang, gangw, htzheng, ravenben}@cs.ucsb.edu
ABSTRACT
With smartphones making video recording easier than ever,
new apps like Periscope and Meerkat brought personalized
interactive video streaming to millions. With a touch, view-
ers can switch between ﬁrst person perspectives across the
globe, and interact in real-time with broadcasters. Unlike
traditional video streaming, these services require low-latency
video delivery to support high interactivity between broad-
casters and audiences.
We perform a detailed analysis into the design and perfor-
mance of Periscope, the most popular personal livestream-
ing service with 20 million users. Using detailed measure-
ments of Periscope (3 months, 19M streams, 705M views)
and Meerkat (1 month, 164K streams, 3.8M views), we ask
the critical question: “Can personalized livestreams continue
to scale, while allowing their audiences to experience de-
sired levels of interactivity?” We analyze the network path
of each stream and break down components of its end-to-end
delay. We ﬁnd that much of each stream’s delay is the di-
rect result of decisions to improve scalability, from chunking
video sequences to selective polling for reduced server load.
Our results show a strong link between volume of broadcasts
and stream delivery latency. Finally, we discovered a criti-
cal security ﬂaw during our study, and shared it along with a
scalable solution with Periscope and Meerkat management.
1.
INTRODUCTION
The integration of high quality video cameras in commod-
ity smartphones has made video recording far more conve-
nient and accessible than ever before. In this context, new
mobile apps such as Periscope and Meerkat now oﬀer users
the ability to broadcast themselves and their surroundings
using real-time interactive live streams. With the ﬂick of a
ﬁnger, a user can switch from a ﬁrst person view of Carnival
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than ACM must be honored. Abstracting with credit is per-
mitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
IMC 2016, November 14-16, 2016, Santa Monica, CA, USA
© 2016 ACM. ISBN 978-1-4503-4526-2/16/11. . . $15.00
DOI: http://dx.doi.org/10.1145/2987443.2987453
in Rio, a guided stroll outside the Burj Khalifa in Dubai, or
a live view of swans in Lake Como, Italy. What makes these
apps compelling is that viewers can interact in real time with
broadcasters, making requests, asking questions and giving
direct feedback via “likes.” Today, popular livestreams cap-
ture thousands of users, and can cover everything from press
conferences, political protests, personal interviews, to back-
stage visits to concerts, TV shows, and sporting events.
Unlike existing video-on-demand streaming services or
live video broadcasts, real time interactivity is critical to the
livestreaming experience for both streamers and their audi-
ence. First, applications like Periscope allow audience to
generate “hearts” or “likes,” which is directly interpreted by
the broadcaster as positive feedback on the content, e.g. “ap-
plause.” Second, many streams involve broadcasters solicit-
ing input on content or otherwise “poll” the audience. Ex-
plicit support for polling is an feature on Twitter that has yet
to be integrated into Periscope. In both cases, the immediacy
of the feedback is critical, and delayed feedback can produce
negative consequences. For example, a “lagging” audience
seeing a delayed version of the stream will produce delayed
“hearts,” which will be misinterpreted by the broadcaster as
positive feedback for a later event in the stream. Similarly, a
delayed user will likely enter her vote after the real-time vote
has concluded, thus discounting her input. Participants in a
recent user study answered that as broadcasters, the imme-
diate interaction with audience members was an authentic,
higher level of engagement, and it was valuable input for
their work and personal branding [45].
Minimizing streaming delay is already a signiﬁcant chal-
lenge for livestreaming services today. To minimize delay
for those commenting on broadcasts, Periscope only allows
100 viewers to comment on a broadcast (usually the ﬁrst
100 to join the stream) [30]. In practice, the ﬁrst 100-200
Periscope users to join a stream are connected to a more di-
rect distribution network with much lower delay (running
Real Time Messaging Protocol (RTMP) [2]), and later ar-
rivals to a high delay CDN for better scalability (running
HTTP Live Streaming (HLS) [3]). While Periscope claims
this is to avoid overwhelming broadcasters, users have al-
ready found this “feature” highly limiting, and have pro-
posed multiple hacks to circumvent the system [22, 19, 20].
Not only does this approach artiﬁcially limit user interac-
tions, but it also prevents more advanced modes of group in-
485teraction, e.g. instantaneous polls for the audience (already
a feature inside Twitter).
In this paper, we study the critical issue, “can personal-
ized livestreaming services like Periscope continue to scale
up in viewers and streams, while allowing large audiences
to experience the interactivity so core to their appeal?” This
leads us to perform an experimental study of Periscope and
its main rival, Meerkat1 2. We perform detailed measure-
ments of Periscope over 3 months in 2015, capturing activ-
ity from 19 million streams generated by 1.85 million broad-
casters, with a total of 705 million views (482M via mobile
and 223M via web). We also gathered measurement data of
Meerkat over 1 month, including 164K streams generated by
57K broadcasters, with a total number of 3.8 million views.
Our work is focused around three speciﬁc questions. First,
we need to understand how popular these systems are, how
they’re used by broadcasters and viewers. We want to un-
derstand current traﬃc loads, as well as longitudinal growth
trends in both users and streams. Second, we need to under-
stand the structure of the livestream delivery infrastructure,
and its contributions of end-to-end delivery delay. More im-
portantly, we are interested in understanding what the trade-
oﬀs are between delay and scalability, and how expected
growth in users is to impact streaming delay. Finally, can
the current system be optimized for improved performance,
and how is continued growth likely to aﬀect future perfor-
mance?
Our study produces several key ﬁndings:
• We ﬁnd that livestream services are growing rapidly,
In contrast, its competitor Meerkat is
with Periscope more than tripling number of daily streams
in 3 months.
rapidly losing popularity, e.g. losing half its daily streams
in a single month. Similar trends are observable in
daily active users. In network structure, Periscope more
resembles the structure of Twitter (likely due to the role
of asymmetric links in both networks), and less that of
Facebook (bidirectional links).
• In Periscope, a combination of RTMP and HLS pro-
tocols are used to host streams. For small streams,
RTMP using server-side push minimizes end-to-end
delay. For the most popular streams, chunking is used
with HLS to reduce server-side overhead, which intro-
duces signiﬁcant delays from both chunking and client-
side polling. In both cases, end-to-end delay is signiﬁ-
cantly exacerbated by aggressive client-side buﬀering.
Barring a change in architecture, more streams will re-
quire servers to increase chunk sizes, improving scala-
bility at the cost of higher delays.
• We use stream traces to drive detailed simulations of
client-side playback, and ﬁnd that current client-side
buﬀering strategies are too aggressive. We believe client-
side buﬀers (and associated latency) can be reduced by
half while still maintaining current playback quality.
1Facebook only recently began a similar service called Face-
book Live, and its user count is far smaller than its two ear-
lier rivals.
2Our study has obtained approval from our local IRB.
• Finally, we ﬁnd a signiﬁcant vulnerability to stream hi-
jacking in the current architectures for both Periscope
and Meerkat, possibly driven by a focus to scale up
number of streams. We propose a lightweight solution
and informed management teams at both companies3.
The results of our work serve to highlight the fundamental
tension between scalability and delay in personalized livestream
services. As Periscope and similar services continue to grow
in popularity (Periscope is now being integrated into hard-
ware devices such as GoPro cameras [7]), it remains to be
seen whether server infrastructure can scale up with demand,
or if they will be forced to increase delivery latency and re-
duce broadcaster and viewer interactivity as a result.
2. BACKGROUND AND RELATED WORK
2.1 The Rise of Periscope and Meerkat
Meerkat was the ﬁrst personalized livestreaming service
to go live, on February 27, 2015 [38]. It was a smartphone
app integrated with Twitter, using Twitter’s social graph to
suggest followers and using Tweets for live comments to
broadcasts. Two weeks later, Twitter announced its acqui-
sition of Periscope [13]. Within a week, Twitter closed its
social graph API to Meerkat, citing its internal policy on
competing apps [32]. Our measurement study began in May
2015, roughly 2 months after, and captured both the rise of
Periscope and the fall of Meerkat. By December 2015, it is
estimated that Periscope has over 20 million users [37].
The initial Periscope app only supported iOS
Periscope.
when it launched in March 2015. The Android version was
released on May 26, 2015. The app gained 10 million users
within four months after launch [15]. On Periscope, any user
can start a live video broadcast as a broadcaster, and other
users can join as viewers. While watching a broadcast, view-
ers can send text-based comments or “hearts” by tapping the
screen. For each broadcast, only the ﬁrst 100 viewers can
post comments, but all viewers can send hearts. Hearts and
comments are visible to the broadcaster and all viewers.
All active broadcasts are visible on a global public list. In
addition, Periscope users can follow other users to form di-
rectional social links. When a user starts a broadcast, all her
followers will receive notiﬁcations. By default, all broad-
casts are public for any users to join. Users do have the
option to start a private broadcast for a speciﬁc set of users.
Facebook Live was initially launched
Facebook Live.
in August 2015 (called “Mentions”) as a feature for celebri-
ties only. In December 2015, Facebook announced that the
app would open to all users. Facebook Live went live on
January 28, 2016. While we have experimented with the
new functionality during its beta period, it is unclear how
far the rollout has reached. Our paper focuses on Periscope
because of its scale and popularity, but we are considering
ways to collect and add Facebook Live data to augment our
measurement study.
3We informed CEOs of both Periscope and Meerkat in Au-
gust, 2015 to ensure they had plenty of time to implement
and deploy ﬁxes before this paper submission.
4862.2 Related Work
Researchers have stud-
Live Streaming Applications.
ied live streaming applications such as CNLive [28], and
Akamai live streaming [43] with focus on user activities and
network traﬃc. Unlike Periscope and Meerkat, these appli-
cations do not support real-time interactivity between users.
Siekkinen et al. studied Periscope with a focus on user ex-
perience and energy consumption, using experiments in a
controlled lab setting [39]. Twitch.tv is a live streaming ser-
vice exclusively for gaming broadcast [21, 16, 48]. Zhang et
al. use controlled experiments to study Twitch.tv’s network
infrastructure and performance [48]. Tang et al. analyzed
content, setting, and other characteristics of a small set of
Meerkat and Periscope broadcasts, and studied broadcasters’
motivation and experience through interviews [45]. Com-
pared to prior work, our work is the ﬁrst large-scale study
on personalized live streaming services (i.e., Periscope and
Meerkat) that support real-time interactivity among users.
A related line of work looks into the peer-to-peer (P2P)
based live streaming applications [42].
In these systems,
users form an overlay structure to distribute video content.
The system is used to stream live video but does not sup-
port user interactivity. Researchers have measured the traﬃc
patterns in popular P2P live streaming systems [17, 40] and
proposed mechanisms to improve scalability [49, 31, 41].
Existing works have studied stream-
Streaming Protocol.
ing protocols under the context of Video-on-Demand sys-
tems. Most studies have focused on HTTP-based protocols
such as DASH, HDS, and HLS including performance anal-
ysis [27, 33, 25] and improvement of designs [47, 18]. Oth-
ers have examined the performance of non-HTTP streaming
protocols such as RTMP in Video-on-demand systems [26].
Fewer studies have examined streaming protocols in the con-
text of live streaming. Related works mostly focus on HTTP
based protocols [29, 11].
Content Distribution Network (CDN). Most existing
literatures focus on general-purpose CDNs that distribute
web content or Video-on-Demand. Su et al. and Huang
et al. measure the CDN performance (latency, availability)
for popular CDNs such as Akamai and LimeLight [44, 6].
Adhikari et al. measure the CDN bandwidth for Video-on-
demand applications such as Netﬂix [10] and Hulu [9]. Kr-
ishnan et al. analyze CDN internal delay to diagnose net-
working issues such as router misconﬁgurations [24]. Kon-
tothanassis et al. explore Akamai’s CDN design for regular
media streaming [23]. Few studies have looked into CDNs
to deliver “real-time” content. In our study, we focus on live
streaming services that also demand a high level of real-time
user interactivity. We analyze streaming protocols and CDN
infrastructures in Periscope to understand the trade-oﬀs be-
tween latency and scalability.
3. BROADCAST MEASUREMENTS
We perform detailed measurements on Periscope and Meerkat
to understand their scale, growth trends and user activities.
Our goal is to set the context for our later analysis on stream
App
Periscope
Meerkat
Months
in 2015
3
1
Broad-
casters
Unique
Broad-
casts
Viewers
19.6M 1.85M 705M 7.65M
183K
164K
Total
Views
57K
3.8M
Table 1: Basic statistics of our broadcast datasets.
Network
Nodes
Edges
Periscope
Facebook [46]
Twitter [36]
231M
12M
1.22M
121M
1.62M 11.3M
Avg.
Degree
38.6
199.6
13.99
Cluster.
Coef.
0.130
0.175
0.065
Avg.
Path
3.74
5.13
6.49
Assort.
-0.057
0.17
-0.19
Table 2: Basic statistics of the social graphs.
delivery infrastructures. In the following, we ﬁrst describe
the methodology of our data collection and the resulting datasets,
and then present the key observations. Our primary mea-
surements focus on Periscope. For comparison, we perform
similar measurements and analysis on Meerkat, Periscope’s
key competitor.
3.1 Data Collection
Our goal is to collect a complete set of broadcasts on the
Periscope and Meerkat networks. For each network, we an-
alyze the network traﬃc between the app and the service,
and identify a set of APIs that allows us to crawl their global
broadcast list. Our study was reviewed and approved by our
local IRB. Both Periscope and Meerkat were aware of our
research, and we shared key results with them to help secure
their systems against potential attack (§7).
To collect a complete
Data Collection on Periscope.
set of broadcasts/streams on Periscope, we build a crawler
to monitor its global broadcast list. The global list shows 50
random selected broadcasts from all active broadcasts. To
obtain the complete list, we use multiple Periscope accounts
to repeatedly query the global list. Each account refreshes
the list every 5 seconds (the same frequency as the Periscope
app) and together we obtain a refreshed list every 0.25 sec-
onds. While our experiments show that a lower refresh rate
(once per 0.5 seconds) can already exhaustively capture all
broadcasts, i.e., capture the same number of broadcasts as
once per 0.25 seconds, we use the higher refresh rate to
accommodate potential burst in the creation of broadcasts.
Whenever a new broadcast appears, our crawler starts a new
thread to join the broadcast and records data until the broad-
cast terminates. For each broadcast, we collect the broad-
castID, starting and ending time of the broadcast, broad-
caster userID, the userID and join time of all the viewers,
and a sequence of timestamped comments and hearts. Only
metadata (no video or message content) is stored, and all
identiﬁers are securely anonymized before analysis.
We ran our crawler for 3+ months and captured all Periscope
broadcasts between May 15, 2015 and August 20, 20154.
As listed in Table 1, our dataset includes 19,596,779 broad-
4We received a whitelisted IP range from Periscope for
active measurements, but our new rate limits were un-
able to keep up with the growing volume of broadcasts on
Periscope.
487e
p
o
c
s
i
r
e
P
r
e
p
r
e
s
U
f
1M
800k
600k
400k
200k
Viewer
Broadcaster
Meerkat
Periscope
25k
20k
15k
10k
5k
t
a
k
r
e
e
M
 1
 0.8
 0.6
 0.4
 0.2
s
t
s
a