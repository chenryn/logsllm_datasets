# Anatomy of a Personalized Livestreaming System

**Authors:**
- Bolun Wang†
- Xinyi Zhang†
- Gang Wang† ‡
- Haitao Zheng†
- Ben Y. Zhao†

**Affiliations:**
- † Department of Computer Science, UC Santa Barbara
- ‡ Department of Computer Science, Virginia Tech

**Contact:**
- {bolunwang, xyzhang, gangw, htzheng, ravenben}@cs.ucsb.edu

## Abstract

With the advent of high-quality video cameras in smartphones, new apps like Periscope and Meerkat have popularized personalized interactive video streaming. These services allow viewers to switch between first-person perspectives globally and interact in real-time with broadcasters. Unlike traditional video streaming, these platforms require low-latency video delivery to support high interactivity.

We conducted a detailed analysis of Periscope, the leading personal livestreaming service with 20 million users. Using extensive data from Periscope (3 months, 19M streams, 705M views) and Meerkat (1 month, 164K streams, 3.8M views), we explored the critical question: "Can personalized livestreams continue to scale while maintaining desired levels of interactivity?" We analyzed the network paths of each stream and broke down the components of end-to-end delay. Our findings indicate that much of the delay is a direct result of decisions made to improve scalability, such as chunking video sequences and selective polling to reduce server load. We also discovered a significant security flaw and shared it along with a scalable solution with Periscope and Meerkat management.

## 1. Introduction

The integration of high-quality video cameras in smartphones has made video recording more convenient and accessible than ever before. New mobile apps like Periscope and Meerkat now offer users the ability to broadcast themselves and their surroundings using real-time interactive live streams. With a simple tap, a user can switch from a first-person view of Carnival in Rio, a guided stroll outside the Burj Khalifa in Dubai, or a live view of swans in Lake Como, Italy. What makes these apps compelling is the real-time interaction between viewers and broadcasters, allowing for requests, questions, and direct feedback via "likes."

Unlike existing video-on-demand streaming services or live video broadcasts, real-time interactivity is crucial for both streamers and their audience. For instance, applications like Periscope allow viewers to generate "hearts" or "likes," which are interpreted by the broadcaster as positive feedback. Additionally, many streams involve broadcasters soliciting input from the audience, often through polls. The immediacy of this feedback is essential, and delayed feedback can lead to negative consequences. For example, a "lagging" audience will produce delayed "hearts," which can be misinterpreted by the broadcaster as positive feedback for a later event in the stream. Similarly, a delayed user may enter their vote after the real-time poll has concluded, thus discounting their input. Participants in a recent user study emphasized the value of immediate interaction with the audience, citing it as an authentic, higher level of engagement [45].

Minimizing streaming delay is a significant challenge for livestreaming services. To minimize delay for those commenting on broadcasts, Periscope only allows 100 viewers to comment on a broadcast (usually the first 100 to join the stream) [30]. In practice, the first 100-200 Periscope users to join a stream are connected to a more direct distribution network with lower delay (using Real Time Messaging Protocol (RTMP) [2]), while later arrivals are directed to a high-delay CDN for better scalability (using HTTP Live Streaming (HLS) [3]). This approach artificially limits user interactions and prevents more advanced modes of group interaction, such as instantaneous polls (a feature already available on Twitter).

In this paper, we address the critical issue of whether personalized livestreaming services like Periscope can continue to scale up in viewers and streams while maintaining the interactivity that is central to their appeal. We conducted an experimental study of Periscope and its main rival, Meerkat, over several months. Our work focuses on three specific questions:
1. How popular are these systems, and how are they used by broadcasters and viewers?
2. What is the structure of the livestream delivery infrastructure, and what are the trade-offs between delay and scalability?
3. Can the current system be optimized for improved performance, and how will continued growth affect future performance?

Our study produced several key findings:
- Livestream services are growing rapidly, with Periscope tripling the number of daily streams in three months, while Meerkat is losing popularity.
- In Periscope, a combination of RTMP and HLS protocols is used. For small streams, RTMP minimizes end-to-end delay, while for popular streams, chunking with HLS reduces server-side overhead but introduces significant delays.
- Current client-side buffering strategies are too aggressive. We believe client-side buffers can be reduced by half while maintaining playback quality.
- We identified a significant vulnerability to stream hijacking in both Periscope and Meerkat and proposed a lightweight solution.

Our results highlight the fundamental tension between scalability and delay in personalized livestream services. As these services continue to grow in popularity, it remains to be seen whether server infrastructure can scale with demand without increasing delivery latency and reducing interactivity.

## 2. Background and Related Work

### 2.1 The Rise of Periscope and Meerkat

Meerkat was the first personalized livestreaming service to launch on February 27, 2015 [38]. It was a smartphone app integrated with Twitter, using Twitter's social graph to suggest followers and using tweets for live comments. Two weeks later, Twitter announced its acquisition of Periscope [13] and subsequently closed its social graph API to Meerkat, citing internal policy on competing apps [32]. Our measurement study began in May 2015, capturing the rise of Periscope and the fall of Meerkat. By December 2015, Periscope had over 20 million users [37].

Periscope initially supported only iOS when it launched in March 2015, with the Android version released on May 26, 2015. The app gained 10 million users within four months [15]. On Periscope, any user can start a live video broadcast, and other users can join as viewers. Viewers can send text-based comments or "hearts" by tapping the screen. Only the first 100 viewers can post comments, but all viewers can send hearts. Hearts and comments are visible to the broadcaster and all viewers. All active broadcasts are visible on a global public list, and users can follow others to form directional social links. When a user starts a broadcast, all her followers receive notifications. By default, all broadcasts are public, but users can start private broadcasts for specific sets of users.

Facebook Live was initially launched in August 2015 (called "Mentions") as a feature for celebrities only. In December 2015, Facebook announced that the app would open to all users, and it went live on January 28, 2016. While we experimented with the new functionality during its beta period, it is unclear how far the rollout has reached. Our paper focuses on Periscope due to its scale and popularity, but we are considering ways to collect and add Facebook Live data to our measurement study.

### 2.2 Related Work

Researchers have studied live streaming applications such as CNLive [28] and Akamai live streaming [43], focusing on user activities and network traffic. Unlike Periscope and Meerkat, these applications do not support real-time interactivity. Siekkinen et al. studied Periscope, focusing on user experience and energy consumption in a controlled lab setting [39]. Twitch.tv is a live streaming service exclusively for gaming broadcasts [21, 16, 48]. Zhang et al. used controlled experiments to study Twitch.tv's network infrastructure and performance [48]. Tang et al. analyzed content, settings, and other characteristics of a small set of Meerkat and Periscope broadcasts, studying broadcasters' motivation and experience through interviews [45]. Compared to prior work, our study is the first large-scale examination of personalized live streaming services that support real-time interactivity among users.

A related line of work looks into peer-to-peer (P2P) based live streaming applications [42]. In these systems, users form an overlay structure to distribute video content, but they do not support user interactivity. Researchers have measured traffic patterns in P2P live streaming systems [17, 40] and proposed mechanisms to improve scalability [49, 31, 41].

Existing works have studied streaming protocols in the context of Video-on-Demand (VoD) systems. Most studies have focused on HTTP-based protocols such as DASH, HDS, and HLS, including performance analysis [27, 33, 25] and design improvements [47, 18]. Others have examined non-HTTP streaming protocols like RTMP in VoD systems [26]. Fewer studies have examined streaming protocols in the context of live streaming, with most focusing on HTTP-based protocols [29, 11].

Content Distribution Network (CDN) research has primarily focused on general-purpose CDNs that distribute web content or VoD. Su et al. and Huang et al. measured CDN performance (latency, availability) for popular CDNs such as Akamai and LimeLight [44, 6]. Adhikari et al. measured CDN bandwidth for VoD applications like Netflix [10] and Hulu [9]. Krishnan et al. analyzed CDN internal delay to diagnose networking issues [24]. Kontothanassis et al. explored Akamai's CDN design for regular media streaming [23]. Few studies have looked into CDNs for delivering real-time content. In our study, we focus on live streaming services that demand a high level of real-time user interactivity, analyzing streaming protocols and CDN infrastructures to understand the trade-offs between latency and scalability.

## 3. Broadcast Measurements

We performed detailed measurements on Periscope and Meerkat to understand their scale, growth trends, and user activities. Our goal is to provide context for our later analysis of stream delivery infrastructures. We describe our data collection methodology and present key observations. Our primary measurements focus on Periscope, with similar measurements and analysis on Meerkat for comparison.

### 3.1 Data Collection

Our goal is to collect a complete set of broadcasts on Periscope and Meerkat. For each network, we analyzed the network traffic between the app and the service, identifying a set of APIs that allowed us to crawl their global broadcast lists. Our study was reviewed and approved by our local IRB. Both Periscope and Meerkat were aware of our research, and we shared key results with them to help secure their systems against potential attacks (§7).

To collect a complete set of broadcasts/streams on Periscope, we built a crawler to monitor its global broadcast list. The global list shows 50 randomly selected broadcasts from all active broadcasts. To obtain the complete list, we used multiple Periscope accounts to repeatedly query the global list. Each account refreshed the list every 5 seconds (the same frequency as the Periscope app), and together we obtained a refreshed list every 0.25 seconds. While our experiments showed that a lower refresh rate (once per 0.5 seconds) could exhaustively capture all broadcasts, we used the higher refresh rate to accommodate potential bursts in broadcast creation.

Whenever a new broadcast appeared, our crawler started a new thread to join the broadcast and recorded data until the broadcast terminated. For each broadcast, we collected the broadcastID, starting and ending time, broadcaster userID, the userID and join time of all viewers, and a sequence of timestamped comments and hearts. Only metadata (no video or message content) was stored, and all identifiers were securely anonymized before analysis.

We ran our crawler for 3+ months and captured all Periscope broadcasts between May 15, 2015, and August 20, 2015. As listed in Table 1, our dataset includes 19,596,779 broadcasts. We received a whitelisted IP range from Periscope for active measurements, but our new rate limits were unable to keep up with the growing volume of broadcasts on Periscope.

| App         | Periscope  | Meerkat   |
|-------------|------------|-----------|
| Months      | 3          | 1         |
| Broadcasters| 1.85M      | 57K       |
| Unique Broadcasts | 19.6M | 164K      |
| Total Views | 705M       | 3.8M      |

| Network     | Nodes  | Edges  | Avg. Degree | Clustering Coef. | Avg. Path | Assortativity |
|-------------|--------|--------|-------------|------------------|-----------|---------------|
| Periscope   | 231M   | 1.22M  | 38.6        | 0.130            | 3.74      | -0.057        |
| Facebook    | 12M    | 1.62M  | 199.6       | 0.175            | 5.13      | 0.17          |
| Twitter     | 121M   | 11.3M  | 13.99       | 0.065            | 6.49      | -0.19         |

These tables provide basic statistics of our broadcast datasets and the social graphs of the networks.