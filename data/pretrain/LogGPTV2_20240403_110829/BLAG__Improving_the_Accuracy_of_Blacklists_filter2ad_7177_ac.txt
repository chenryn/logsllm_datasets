### Relevance Score and Recommendation System

Intuitively, if 128.0.0.1 is a legitimate IP address and listed in the Misclassification Blacklist (MB), it should have a similar relevance score to 128.0.0.5, which is already present in MB. The recommendation system captures this pattern and assigns a score of 0.84 to 128.0.0.1. Conversely, the IP address 128.0.0.4, which does not share a similar listing pattern with 128.0.0.5, is assigned a low score of 0.12 in MB. For IP addresses 128.0.0.2 and 128.0.0.3, which share some listings with 128.0.0.5 but have different relevance scores, the recommendation system assigns scores of 0.29 and 0.17, respectively. The cells in the score matrix under the MB column, filled by the recommendation system, contain potential future sources of legitimate traffic for the customer (F).

### Aggregated Blacklist Construction

After calculating all missing relevance scores in the Misclassification Blacklist (MB), we construct the aggregated blacklist, known as the master blacklist candidates. To generate these candidates, we observe the relevance scores in MB and apply a threshold \( \alpha \) (discussed in Section VII) to include all IP addresses \( a \) for which \( r_{a,MB} \leq \alpha \). Intuitively, IP addresses with high scores in MB are either current legitimate sources of the customer’s inbound traffic (Ltr) or likely to become so in the future (F), and these are excluded from the master blacklist.

### Selective Expansion to Prefixes

In Section II-C, we discussed the utility of identifying and expanding IP addresses into prefixes. Previous works have expanded IP addresses into prefixes indiscriminately [75], [55], [77], which improves malicious source identification but increases misclassifications. Our approach selectively expands IP addresses into prefixes only when this expansion does not significantly increase misclassifications, making it particularly useful for customers deploying BLAG in emergency scenarios.

The expansion phase begins with the master blacklist candidates, all of which are added to the BLAG master blacklist. We then identify IP addresses that can be expanded into their /24 prefixes (see Section VI for the rationale behind choosing /24 prefix size). A list of all /24 prefixes from the master blacklist candidates is generated, and each prefix is evaluated for inclusion in the master blacklist. Prefixes containing known legitimate sources (from Ltr) are excluded (Check 1). Additionally, prefixes containing likely misclassifications (IP addresses with high relevance scores in the misclassification blacklist, i.e., set F) are also excluded (Check 2). The remaining prefixes are added to the BLAG master blacklist. In Figure 5, none of the IP addresses are in known-legitimate sources (Ltr), and the address 169.231.140.68 has another address in the same /24 prefix, which is a likely misclassification (in set F). Therefore, 169.231.140.68 is not expanded, while the other IP addresses are expanded to their corresponding prefixes and included in the master blacklist.

### Why BLAG Works

BLAG assigns relevance scores to capture the likelihood of IP addresses being listed in a blacklist. It introduces an artificial Misclassification Blacklist (MB) consisting of known legitimate sources (Ltr). The recommendation system used by BLAG helps during the aggregation phase by pruning out misclassifications and during the selective expansion phase by preventing expansions that would increase future misclassifications. The recommendation system predicts future misclassifications based on finding IP addresses that exhibit similarities to known legitimate sources in the blacklisting process. In Section VI-B, we quantify the contribution of the recommendation system in reducing misclassifications during the aggregation and selective expansion phases.

### Datasets

BLAG's primary goal is to balance the identification of as many malicious sources as possible while minimizing misclassifications. In this section, we examine the blacklists used by BLAG to aggregate information and present three BLAG deployment scenarios for evaluation. These scenarios include real-world legitimate and malicious traffic. In Section V, we will show that BLAG achieves more than 95% specificity (5% misclassification rate) and significantly increases recall (high detection of malicious sources) compared to individual blacklists and their naive aggregation.

#### Blacklist Dataset

We monitored 157 publicly available blacklists for 11 months, from January 2016 to November 2016. Each blacklist is updated at a different frequency, ranging from 15 minutes to 7 days. We collected the update time of each blacklist manually and programmed our crawler to pull snapshots when new updates were available. Over this period, we collected around 176 million blacklisted IP addresses across 23,483 autonomous systems. The blacklists vary in size, with large blacklists (15.76%) listing more than 500,000 IP addresses and small blacklists (19.56%) listing fewer than 1,000 IP addresses. Our work focuses on identifying key properties of blacklists that make them ineffective in emergency scenarios and presenting an improved blacklisting technique.

Our blacklist dataset (B) represents various attack vectors such as spam, malware, DDoS attacks, and ransomware. Table II shows the blacklist maintainers and the number of blacklists they manage. Our dataset includes popular blacklists such as DShield [57], Nixspam [68], Spamhaus [44], Alienvault [3], Project Honeypot [37], Abuse.ch [47], and Emerging Threats [21].

#### Deployment Scenarios

Table I outlines our three scenarios, each consisting of training, validation, and testing portions of the same dataset. The training portion contains only known legitimate sources (Ltr) and is used to tailor BLAG to the customer network (Section III-B). This portion is collected before the malicious event in each scenario. The validation and testing portions contain both legitimate (Lv and Lte) and malicious (Mv and Mte) sources. The validation portion is used to calibrate BLAG’s parameters (l, α, and K) for testing, while the testing portion evaluates BLAG’s performance and compares it to competing blacklisting approaches.

Our three scenarios cover diverse attack sources: spam, DDoS on a University network, and DDoS on DNS root. This allows us to test how well BLAG can prevent these attacks if used by a customer network to filter attack traffic.

1. **Malicious Email Campaign or Spam (Email Scenario)**:
   - **Description**: In this scenario, a University network is bombarded with spam emails.
   - **Data Collection**: We collected malicious and legitimate IP addresses during June 2016. Malicious IP addresses were obtained from Mailinator [29], a service that redirects unwanted emails to a public inbox. Using SpamAssassin [64], we filtered emails from these inboxes to obtain around 2.3 million spam emails sent by approximately 39,000 IP addresses. We trained SpamAssassin using SpamAssassin’s public corpus [43] and spam archives from Untroubled [49] to ensure we captured only malicious spam emails.
   - **Validation and Testing Sets**: The first 7 days, consisting of 13,000 IP addresses, were used as the validation set (Mv), and the remaining 16 days, consisting of 26,000 IP addresses, were used for testing (Mte).
   - **Legitimate Data Collection**: Legitimate IP addresses were collected through a human user study. We recruited 37 volunteers from our University who allowed automated access to their Gmail inboxes during June 2016. We developed a plugin that used the OAuth2 protocol to access Gmail and extracted sender IP addresses, times, and labels for each email. We harvested information only from emails labeled as anything other than "spam". This generated a list of {sender IP address, time} tuples, from which we extracted around 30,000 email records sent by approximately 9,000 IP addresses. The first seven days, consisting of 3,000 IP addresses, were used for training (Ltr), the next 7 days, consisting of 2,000 IP addresses, for validation (Lv), and the remaining 16 days, consisting of 4,000 IP addresses, for testing (Lte).

2. **DDoS on a University Network (DDoSUniv Scenario)**:
   - **Description**: In this scenario, web servers at a University are targeted by Mirai-infected devices.
   - **Data Collection**: Details of the data collection and validation/testing sets are provided in Table I.

3. **DDoS on DNS Root (DDoSDNS Scenario)**:
   - **Description**: In this scenario, DNS root servers are targeted by DDoS attacks.
   - **Data Collection**: Details of the data collection and validation/testing sets are provided in Table I.

### Scenario Datasets

Table I summarizes the datasets used in each scenario, split into training, validation, and testing portions. The training dataset, collected chronologically before the validation and testing, contains only legitimate sources (Ltr). The validation and testing datasets, collected during malicious events, contain both malicious (Mv and Mte) and legitimate (Lv and Lte) sources. The validation dataset is used to tune the appropriate parameters for BLAG.

| **Scenario** | **Duration** | **Training (Ltr)** | **Validation (Lv+Mv)** | **Testing (Lte+Mte)** |
|--------------|--------------|--------------------|------------------------|-----------------------|
| **Email**    | 6/1/16-6/30/16 | 7 days, 3K IPs | 7 days, Mv: 13K IPs, Lv: 2K IPs | 16 days, Mte: 26K IPs, Lte: 4K IPs |
| **DDoSUniv** | 9/1/16-9/30/16 | 7 days, 16K IPs | 7 days, Mv: 1.1M IPs, Lv: 12K IPs | 16 days, Mte: 2.8M IPs, Lte: 33K IPs |
| **DDoSDNS**  | 6/24/16-6/25/16 | 1 day, 2.7M IPs | - | 1 day, Mte: 5.5M IPs, Lte: 16K IPs |

### Blacklist Maintainers

| **Type**      | **#** | **Maintainers**                                                                                      |
|---------------|-------|------------------------------------------------------------------------------------------------------|
| **Malware**   | 57    | Emerging threats [21], Malware Bytes [25], CyberCrime [17], URLVir [50], Swiss security blog [47], etc. |
| **Reputation**| 32    | Emerging threats [21], Graphiclineweb [23], Alienvault [3], Binary Defense Systems [7], etc.          |
| **Spam**      | 39    | Spamhaus drop and edrop [44], Stop Forum Spam [46], Chaosreigns [4], Lashback [28], etc.             |
| **Attacks**   | 29    | Blocklist.de [8], Cisco Talos [12], Bad IPs [5], Blocklist Project [22], VXVault [52], etc.           |

This structured and detailed approach ensures clarity, coherence, and professionalism in the text.