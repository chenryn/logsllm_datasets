of 0.3 and 0.4). Intuitively, if 128.0.0.1 were to be legitimate
and listed in MB, we can expect it to have a similar relevance
score as that of 128.0.0.5 which is already present in MB. The
recommendation system captures this pattern and assigns a
score of 0.84 to 128.0.0.1. On the other hand, address 128.0.0.4
has no similar listing pattern as that of 128.0.0.5, therefore
the recommendation system assigns it a low score of 0.12 in
MB. Finally, IP addresses 128.0.0.2 and 128.0.0.3 share some
listings with 128.0.0.5. However, their relevance scores are not
similar. This regularity is also captured by the recommendation
system and assigns a score of 0.29 and 0.17 respectfully in MB.
Cells in the score matrix, in the column MB, that was ﬁlled by
the recommendation system contain the set of potential future
sources of legitimate trafﬁc to the customer (F).
After we have calculated all the missing relevance scores
in the misclassiﬁcation blacklist MB, we proceed to construct
the aggregated blacklist known as master blacklist candidates.
To generate the candidates, we observe relevance scores in
MB and then use a threshold α (choice of α discussed in
Section VII) to include all the IP addresses a for which the
following holds: ra,M B ≤ α. Intuitively, IP addresses, which
have high scores in MB are either current legitimate sources
of customer’s inbound trafﬁc (Ltr) or likely to be so in the
future (F), and the thresholding excludes them from the master
blacklist.
Figure 5: Selective expansion of IP addresses into preﬁxes.
C. Selective Expansion to Preﬁxes
We have discussed in Section II-C why it would be useful
to identify and expand IP addresses into preﬁxes. Prior works
have expanded IP addresses into preﬁxes indiscriminately [75],
[55], [77] – this improves malicious source identiﬁcation
but greatly increases misclassiﬁcations. The novelty of our
approach is to selectively expand IP addresses into preﬁxes,
only when this expansion does not greatly increase misclassi-
ﬁcations. This is particularly useful for customers deploying
BLAG under emergency scenarios.
The expansion phase starts with master blacklist candi-
dates, which are all added to the BLAG master blacklist.
During expansion, we identify IP addresses, that could be ex-
panded into their /24 preﬁxes (see Section VI for the rationale
behind choosing /24 preﬁx size). We ﬁrst generate a list of
all /24 preﬁxes from the master blacklist candidates. We then
evaluate if each preﬁx should be added to the master blacklist.
Preﬁxes that contain known legitimate sources (from (Ltr))
are excluded (Check 1). Further, preﬁxes that contain likely
misclassiﬁcations (IP addresses with high relevance scores
in the misclassiﬁcation blacklist, i.e. set (F)) are excluded
(Check 2). Remaining preﬁxes are added to the BLAG’s master
blacklist. In Figure 5, none of the IP addresses are present
in known-legitimate sources (Ltr) and address 169.231.140.68
has another address in the same /24 preﬁx, which is a likely
misclassiﬁcation (in set (F)). Therefore, 169.231.140.68 is not
expanded, and the other IP addresses are expanded to their
corresponding preﬁx to be included in the master blacklist.
6
0.70.60.3?0.100.1?0.20.10?0.100?00.70.410.270.120.040.110.1100.070.020.210.230.240.170.080.220.080.170.130.250.680.590.290.840.10.170.080.290.190.10.0010.170.090.0600.120.610.680.390.99M NMKKN128.0.0.1128.0.0.2128.0.0.3128.0.0.4128.0.0.5nixspambambenek_c2openblMBnixspambambenek_c2openblMB128.0.0.1128.0.0.2128.0.0.3128.0.0.4128.0.0.5Matrix RMatrix PMatrix QMatrix R'Likely to be a misclassiﬁcationUnlikely to be a misclassiﬁcationMaster blacklistcandidates169.231.140.68193.1.64.5193.1.64.8Check1OKOKOKCheck2!OKOK193.1.64.0/24216.59.0.0/24169.231.140.68Selectiveexpansion216.59.16.171243.13.0.23243.13.222.203169.231.140.10216.59.0.8OKOKKnown legitimatesources (Ltr)RecommendedmisclassiﬁcationsBLAG masterblacklistD. Why BLAG Works
B. Scenarios
BLAG assigns relevance scores to capture the relevance of
IP addresses being listed in a blacklist. BLAG also introduces
a new artiﬁcial blacklist – misclassiﬁcation blacklist, which
consists of known-legitimate sources (Ltr). The recommen-
dation system used by BLAG helps during the aggregation
phase by pruning out misclassiﬁcations, and also during the
selective expansion phase by preventing those expansions of
IP addresses into preﬁxes that would increase future mis-
classiﬁcations. The recommendation system helps BLAG to
discover other IP addresses that are predicted to be listed in the
misclassiﬁcation blacklist with a high relevance score. In other
words, the recommendation system predicts future misclassiﬁ-
cations based on ﬁnding IP addresses that exhibit similarities,
with regard to the blacklisting process, to known legitimate
sources. In Section VI-B, we quantify the contribution of the
recommendation system in reducing misclassiﬁcation during
the aggregation and selective expansion phases.
IV. DATASETS
BLAG’s fundamental goal is to ﬁnd a trade-off between
identifying as many malicious sources as possible and keeping
the misclassiﬁcations low. In this section, we look into the
blacklists used by BLAG to aggregate information. We also
present three BLAG deployment scenarios, which we use in
evaluation. These scenarios include real-world legitimate and
malicious trafﬁc. We will show the performance of BLAG
under these scenarios in Section V, where BLAG achieves
more than 95% speciﬁcity (5% misclassiﬁcation rate), while
signiﬁcantly increasing recall (high detection of malicious
sources), compared to individual blacklists and their naïve
aggregation.
A. Blacklist Dataset
We have monitored 157 publicly available blacklists for 11
months, starting from January 2016 to November 2016. Each
blacklist is updated at a different frequency by its provider,
ranging from 15 minutes to 7 days. We collected the update
time of each blacklist manually and programmed our crawler
to pull the snapshot of the blacklist when a new update was
available. We have collected around 176 million blacklisted
IP addresses over 23,483 autonomous systems. Our monitored
blacklists vary in size – on one hand, we have large blacklists
(15.76%) listing more than 500,000 IP addresses and on the
other, we have small blacklists (19.56%), which list fewer than
1,000 IP addresses. We do not delve into details on the various
properties of blacklists, such as their volume, contribution,
exclusive contribution, detection of malicious activities (refer
Vector et al. [63] work on an exhaustive study on properties
of blacklists). Our work focuses more on identifying key
properties of blacklists that make them ineffective in emer-
gency scenarios (refer Section II) and presenting an improved
blacklisting technique (refer Section III).
Our blacklist dataset (B) is representative of different attack
vectors such as spam, malware, DDoS attacks, ransomware,
etc. Table II shows the blacklist maintainers and the number
of blacklists maintained by them. Our dataset includes popular
blacklists such as DShield [57], Nixspam [68], Spamhaus [44],
Alienvault [3], Project Honeypot [37], Abuse.ch [47] and
Emerging Threats [21].
Table I shows our three scenarios. Each scenario consists
of three portions of the same dataset: training, validation and
testing. The training portion contains only known-legitimate
sources (Ltr) and is used to tailor BLAG to the customer
network (Section III-B). This portion is collected before the
malicious event in each scenario. The validation and testing
portions contain both the legitimate (Lv and Lte) and malicious
(Mv and Mte) sources. The validation portion is used to
calibrate BLAG’s parameters (l, α and K) for testing3. The
testing portion is used to evaluate the performance of BLAG
and competing blacklisting approaches.
Our three scenarios contain sources of diverse attacks:
spam, DDoS on a University network or DDoS on DNS root.
This allows us to test how well BLAG could prevent these
attacks if used by a customer network to ﬁlter attack trafﬁc.
1) Malicious Email Campaign or Spam: In this scenario
(referred to as Email), we look into a case where a University
network is bombarded with spam emails. We collect malicious
and legitimate IP addresses during the same time period of
June 2016. Simultaneous collection is important, because an
address may be malicious at one time, and cleaned afterward.
We collect malicious IP addresses from Mailinator [29], a
service, which allows users to redirect unwanted e-mails to
a public-inbox. We ﬁlter e-mails from these public inboxes
during June 2016, using SpamAsssassin [64] to obtain around
2.3 M spam e-mails, sent by around 39 K IP addresses.
These IP addresses form our malicious dataset. We trained
SpamAssassin using SpamAssasin’s public corpus [43] and
spam archives from Untroubled [49], to ensure we capture only
malicious spam emails. We use the ﬁrst 7 days consisting of
13 K IP addresses as a validation set (Mv) and the remaining
16 days consisting of 26 K IP addresses for testing (Mte).
We collect
legitimate IP addresses through a human user
study. This study was reviewed and approved by our IRB.
We recruited 37 volunteers from our University, who allowed
us automated access to their Gmail inbox, during June 2016.
We scanned each participant’s Gmail account using a plugin,
which we developed. Our plugin used the OAuth2 protocol
to access Gmail, and it used regular expressions to extract a
sender’s IP address, time and label for each e-mail. We did not
extract any other information and did not record the identity
of the study participants, to protect privacy. The label in Gmail
can be assigned by a user or by GMail, and it is usually
“spam”, “inbox” or a user-deﬁned label like “conference”. We
harvested information only from e-mails that have labels other
than “spam”. Our scanning generates as output a list of {sender
IP address, time} tuples, which we save. We extracted around
30 K e-mail records, sent by around 9 K IP addresses. We
use the ﬁrst seven days of this dataset consisting of 3 K IP
addresses for training (the known-legitimate sources set (Ltr)),
the next 7 days consisting of 2 K IP addresses for validation
(Lv) and the remaining 16 days consisting of 4 K IP addresses
for testing (Lte).
2) DDoS on a University network: In this scenario (re-
ferred to as DDoSUniv), we look into a case where web-
servers at a University could be targeted by Mirai-infected
3We do not have a validation dataset for DDoSDNS. Refer Section VII for
further explanation
7
Scenario
Email
DDoSUniv
DDoSDNS
Duration
6/1/16-
6/30/16
9/1/16-
9/30/16
6/24/16-
6/25/16
Source
M:Mailinator
L: Ham
M:Mirai
L: Web cl.
M:DDoS
L: DNS cl.
Days
Training (Ltr)
IPs
-
-
7
-
7
-
1
Ltr: 3 K
Ltr: 16K
-
-
Ltr: 2.7M
Validation (Lv+Mv)
Days
IPs
7
7
7
7
-
-
Mv: 13 K
Lv: 2 K
Mv: 1.1 M
Lv: 12 K
-
-
IPs
Testing (Lte+Mte)
Days
16
16
16
16
1
1
Mte:26 K
Lte: 4 K
Mte: 2.8 M
Lte: 33 K
Mte: 5.5 M
Lte: 16 K
Table I: Scenario datasets used in this study. Each scenario dataset is split into three – training, validation and testing. The training dataset is
collected chronologically before the validation and testing, and contains only legitimate sources (Ltr). The validation and testing datasets are
collected during malicious events and contain malicious (Mv and Mtr) and legitimate (Lv and Ltr) sources. The validation dataset is used to
tune the appropriate parameters used in BLAG.
Type
Malware
Reputation
Spam
Attacks
#
57
32
39
29
Blacklist Maintainers
Emerging threats [21], Malware Bytes [25], CyberCrime [17], URLVir [50], Swiss security blog [47], Bambenek [6], NoThink [34], I-Blocklist [26],
NoVirusThanks [35], DYN [20], Malc0de [30], Malware domain list [31], Botscout [9], ImproWare [27]
Emerging threats [21], Graphiclineweb [23], Alienvault [3], Binary Defense Systems [7], CINSscore [11], Swiss Security Blog [47], Blocklist.de [8],
I-Blocklist [26], Cisco Talos [12], Bad IPs [5], Blocklist Project [22], VXVault [52], ShunList [41], GreenSnow [24]
Spamhaus drop and edrop [44], Stop Forum Spam [46], Chaosreigns [4], Lashback [28], Nixspam [68], Project Honeypot [37], Sblam! [40], Turris [38],