variables of the poisoning points with the InvFlip strategy.
We nevertheless test all the remaining three combinations of
initialization strategies and outer objectives in our experiments.
Response Variable Optimization. This work is the ﬁrst to
consider poisoning attacks in regression settings. Within this
context, it is worth remarking that response variables take on
continuous values rather than categorical ones. Based on this
observation, we propose here the ﬁrst poisoning attack that
jointly optimizes the feature values xc of poisoning attacks
and their associated response variable yc. To this end, we
extend the previous gradient-based attack by considering the
optimization of zc = (xc, yc) instead of only considering xc.
This means that all previous equations remain valid provided
that we substitute ∇zc to ∇xc. This clearly requires expanding
∇xc θ by also considering derivatives with respect to yc:
(cid:17)
∇zc θ =
(cid:15)
∂w
∂yc
∂b
∂yc
∂w
∂xc
∂b
∂xc
(cid:16)(cid:15)
and, accordingly, modify Eq. (7) as
∇zc θ
(cid:2)
= − 2
n
M w
−x
c −1
(cid:2)
Σ + λg μ
1
(cid:2)
μ
(cid:16)−1
(13)
.
(14)
(cid:18)
,
The derivatives given in Eqs. (10)-(12) remain clearly un-
changed, and can be pre-multiplied by Eq. (14) to obtain
W. Algorithm 1 can still be used to
the ﬁnal gradient ∇zc
this attack, provided that both xc and yc are
implement
updated along the gradient ∇zcW (cf. Algorithm 1, line 7).
Theoretical Insights. We discuss here some theoretical in-
sights on the bilevel optimization of Eqs. (2)-(3), which
will help us to derive the basis behind the statistical attack
introduced in the next section. To this end, let us ﬁrst consider
as the outer objective a non-regularized version of Wtr, which
can be obtained by setting λ = 0 in Eq. (8). As we will see,
in this case it is possible to compute simpliﬁed closed forms
for the required gradients. Let us further consider another
objective denoted with W(cid:3)
tr, which, instead of optimizing the
(cid:2)n
loss, optimizes the difference in predictions from the original,
unpoisoned model θ
i=1(f (xi, θ) − f (xi, θ
tr = 1
n
))2.
In Appendix A, we show that Wtr and W(cid:3)
tr are inter-
changeable for our bilevel optimization problem. In particular,
differentiating W(cid:3)
= 2
tr with respect to zc = (xc, yc) gives:
n (f (xc, θ) − f (xc, θ
n (f (xc, θ) − f (xc, θ
))(w0 − 2w)
W(cid:3)
= 2
(16)
(15)
)).
(cid:2)
∂W(cid:3)
tr
∂xc
∂W(cid:3)
tr
∂yc
:
(cid:3)
(cid:3)
(cid:3)
(cid:3)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:36:53 UTC from IEEE Xplore.  Restrictions apply. 
tr
The update rules deﬁned by these gradients have nice inter-
pretation. We see that ∂W(cid:2)
∂yc will update yc to be further away
from the original line than it was in the previous iteration.
This is intuitive, as a higher distance from the line will push
the line further in that direction. The update for xc is slightly
more difﬁcult to understand, but by separating (w0−2w) into
(−w)+(w0−w), we see that the xc value is being updated in
two directions summed together. The ﬁrst is perpendicularly
away from the regression line (like the yc update step, the
poison point should be as far as possible from the regression
line). The second is parallel to the difference between the
original regression line and the poisoned regression line (it
should keep pushing in the direction it has been going). This
gives us an intuition for how the poisoning points are being
updated, and what an optimal poisoning point looks like.
B. Statistical-based Poisoning Attack (StatP)
Motivated by the aforementioned theoretical insights, we
design a fast statistical attack that produces poisoned points
with similar distribution as the training data. In StatP, we
simply sample from a multivariate normal distribution with
the mean and covariance estimated from the training data.
Once we have generated these points, we round the feature
values to the corners, exploiting the observation that the most
effective poisoning points are near corners. Finally, we select
the response variable’s value at the boundary (either 0 or 1)
to maximize the loss.
Note that, importantly, the StatP attack requires only black-
box access to the model, as it needs to query the model to
ﬁnd the response variable (before performing the boundary
rounding). It also needs minimal information to be able to
sample points from the training set distribution. In particular,
StatP requires an estimate of the mean and co-variance of
the training data. However, StatP is agnostic to the exact
regression algorithm, its parameters, and training set. Thus,
it requires much less information on the training process than
the optimization-based attacks. It is signiﬁcantly faster than
optimization-based attacks, though slightly less effective.
IV. DEFENSE ALGORITHMS
In this section, we describe existing defense proposals
against poisoning attacks, and explain why they may not be
effective under adversarial corruption in the training data. Then
we present a new approach called TRIM, speciﬁcally designed
to increase robustness against a range of poisoning attacks.
A. Existing defense proposals
Existing defense proposals can be classiﬁed into two cate-
gories: noise-resilient regression algorithms and adversarially-
resilient defenses. We discuss these approaches below.
Noise-resilient regression. Robust regression has been exten-
sively studied in statistics as a method to provide resilience
against noise and outliers [26], [27], [51], [55]. The main idea
behind these approaches is to identify and remove outliers
from a dataset. For example, Huber [26] uses an outlier-robust
loss function. RANSAC [17] iteratively trains a model to ﬁt a
iterations of the TRIM algorithm. Initial
Fig. 2: Several
poisoned data is in blue in top left graph. The top right graph
shows in red the initial randomly selected points removed
from the optimization objective. In the following two iterations
(bottom left and right graphs) the set of high-residual points
is reﬁned and the model becomes more robust.
subset of samples selected at random, and identiﬁes a training
sample as an outlier if the error when ﬁtting the model to the
sample is higher than a threshold.
While these methods provide robustness guarantees against
noise and outliers, an adversary can still generate poisoning
data that affects the trained model. In particular, an attacker
can generate poisoning points that are very similar to the true
data distribution (these are called inliers), but can still mislead
the model. Our new attacks discussed in Section III generate
poisoning data points which are akin to the pristine ones.
For example, in StatP the poisoned points are chosen from a
distribution that is similar to that of the training data (has the
same mean and co-variance). It turns out that these existing
regression methods are not robust against inlier attack points
chosen to maximally mislead the estimated regression model.
Adversarially-resilient
regression. Previously proposed
adversarially-resilient regression algorithms typically provide
guarantees under strong assumptions about data and noise
distribution. For instance, Chen et al. [10], [11] assume that
the feature set matrix satisﬁes X T X = I and data has sub-
Gaussian distribution. Feng et al. [16] assume that the data
and noise satisfy the sub-Gaussian assumption. Liu et al. [33]
design robust linear regression algorithms robust under the
assumption that the feature matrix has low rank and can be
projected to a lower dimensional space. All these methods
have provable robustness guarantees, but the assumptions on
which they rely are not usually satisﬁed in practice.
B. TRIM algorithm
In this section, we propose a novel defense algorithm
called TRIM with the goal of training a regression model
with poisoned data. At an intuitive level, rather than sim-
ply removing outliers from the training set, TRIM takes a
24
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:36:53 UTC from IEEE Xplore.  Restrictions apply. 
Algorithm 2 [TRIM algorithm]
1: Input: Training data D = Dtr ∪ Dp with |D| = N;
number of attack points p = α · n.
(0) ← arg minθ L(
I(0) , θ) /* Initial estimation of θ*/
2: Output: θ.
3: I (0) ← a random subset with size n of {1, ..., N}
4: θ
5: i ← 0 /* Iteration count */
6: repeat
7:
8:
9:
10:
i ← i + 1;
I (i) ← subset of size n that min. L(DI(i), θ
)
(i) ← arg minθ L(DI(i) , θ) /* Current estimator */
θ
R(i) = L(DI(i) , θ
11: until i > 1 ∧ R(i) = R(i−1) /* Convergence condition*/
12: return θ
(i) /* Final estimator */.
) /* Current loss */
(i−1)
(i)
principled approach. TRIM iteratively estimates the regression
parameters, while at the same time training on a subset of
points with lowest residuals in each iteration. In essence,
TRIM uses a trimmed loss function computed on a different
subset of residuals in each iteration. Our method is inspired
by techniques from robust statistics that use trimmed versions
of the loss function for robustness. Our main contribution
is to apply trimmed optimization techniques for regularized
linear regression in adversarial settings, and demonstrate their
effectiveness compared to other defenses on a range of models
and real-world datasets.
As in Section II, assume that the original training set is Dtr
of size n, the attacker injects p = α · n poisoned samples
Dp, and the poisoned training set D = Dtr ∪ Dp is of size
N = (1 + α)n. We require that α < 1 to ensure that the
majority of training data is pristine (unpoisoned).
Our main observation is the following: we can train a linear
regression model only using a subset of training points of
size n. In the ideal case, we would like to identify all p
poisoning points and train the regression model based on the
remaining n legitimate points. However, the true distribution
of the legitimate training data is clearly unknown, and it is
thus difﬁcult to separate legitimate and attack points precisely.
To alleviate this, our proposed defense tries to identify a
set of training points with lowest residuals relative to the
regression model (these might include attack points as well,
but only those that are “close” to the legitimate points and
do not contribute much to poisoning the model). In essence,
our TRIM algorithm provides a solution to the following
optimization problem:
min
θ,I
L(DI, θ)
s.t. I ⊂ [1, . . . , N ] ∧ |I| = n .
(17)
We use the notation DI to indicate the data samples
{(xi, yi) ∈ D}i∈I. Thus, we optimize the parameter θ of
the regression model and the subset I of points with smallest
residuals at the same time. It turns out though that solving
this optimization problem efﬁciently is quite challenging. A
simple algorithm that enumerates all subsets I of size n of
the training set is computationally inefﬁcient. On the other
hand, if the true model parameters θ = (w, b) were known,
then we could simply select points in set I that have lowest
residual relative to θ. However, what makes this optimization
problem difﬁcult to solve is the fact that θ is not known, and
we do not make any assumptions on the true data distribution
or the attack points.
To address these issues, our TRIM algorithm learns param-
eter θ and distinguishes points with lowest residuals from
training set alternately. We employ an iterative algorithm
inspired by techniques such as alternating minimization or
expectation maximization [13]. At the beginning of iteration
(i). We use this estimate
i, we have an estimate of parameter θ
as a discriminator to identify all inliers, whose residual values
are the n smallest ones. We do not consider points with large
residuals (as they increase MSE), but use only the inliers
(i+1). This process terminates
to estimate a new parameter θ
when the estimation converges and the loss function reaches a
minimum. The detailed algorithm is presented in Algorithm 2.
A graphical representation of three iterations of our algorithm
is given in Figure 2. As observed in the ﬁgure, the algorithm
iteratively ﬁnds the direction of the regression model that ﬁts
the true data distribution, and identiﬁes points that are outliers.
We provide provable guarantees on the convergence of
Algorithm 2 and the estimation accuracy of the regression
model it outputs. First, Algorithm 2 is guaranteed to converge
and thus it terminates in ﬁnite number of iterations, as stated
in the following theorem.
Theorem 1. Algorithm 2 terminates in a ﬁnite number of
iterations.
(cid:11)
(cid:12)
N
n
We are next
We do not explicitly provide a bound on the number of iter-
ations needed for convergence, but it is always upper bounded
. However, our empirical evaluation demonstrates that
by
Algorithm 2 converges within few dozens of iterations at most.
interested in analyzing the quality of the
estimated model computed from Algorithm 2 (adversarial
world) and how it relates to the pristine data (ideal world).
However, relating these two models directly is challenging
due to the iterative minimization used by Algorithm 2. We
overcome this by observing that Algorithm 2 ﬁnds a local
minimum to the optimization problem from (17). There is no
efﬁcient algorithm for solving (17) that guarantees the solution
to be the global minimum of the optimization problem.
It turns out that we can provide a guarantee about the
global minimum ˆθ of (17) on poisoned data (under worst-
(cid:2) learned by
case adversaries) in relation to the parameter θ
the original model on pristine data. In particular, Theorem 2
shows that ˆθ “ﬁts well” to at least (1 − α) · n pristine data
samples. Notably, it does not require any assumptions on how
poisoned data is generated, thus it provides guarantees under
worst-case adversaries.
Theorem 2. Let Dtr denote the original training data, ˆθ the
= arg minθ L(Dtr, θ) the
global optimum for (17), and θ
estimator in the ideal world on pristine data. Assuming α < 1,
there exist a subset D(cid:3) ⊂ Dtr of (1−α)·n pristine data samples
(cid:2)
25