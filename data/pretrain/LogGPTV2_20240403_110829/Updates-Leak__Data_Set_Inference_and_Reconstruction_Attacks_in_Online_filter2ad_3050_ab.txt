3.2 Encoder Design
All our attacks share the same encoder structure, we model it
with a multilayer perceptron. The number of layers inside the
encoder depends on the dimension of δ: Longer δ requires
more layers in the encoder. As our δ is a 1,000-dimension vec-
tor for the MNIST and CIFAR-10 datasets, and 800-dimension
vector for the Insta-NY dataset, we use two fully connected
layers in the encoder. The ﬁrst layer transforms δ to a 128-
dimension vector and the second layer further reduces the
dimension to 64. The concrete architecture of our encoder is
presented in Appendix B.
3.3 Decoder Structure
Our four attacks aim at inferring different information of
Dupdate, ranging from sample labels to the updating set itself.
Thus, we construct different decoders for different attacks
with different techniques. The details of these decoders will
be presented in the following sections.
3.4 Shadow Model
Our encoder and decoder need to be trained jointly in a su-
pervised manner. This indicates that we need ground truth
data for model training. Due to our minimal assumptions, the
adversary cannot get the ground truth from the target model.
To solve this problem, we rely on shadow models following
previous works [13, 38, 40]. A shadow model is designed to
mimic the target model. By controlling the training process of
the shadow model, the adversary can derive the ground truth
data needed to train her attack models.
As presented in Section 2, our adversary knows (1) the
architecture of the target model and (2) a dataset coming from
USENIX Association
29th USENIX Security Symposium    1293
Figure 1: A schematic view of the general attack pipeline.
the same distribution as the target dataset. To build a shadow
model Mshadow, the adversary ﬁrst establishes an ML model
with the same structure as the target model. Then, she gets
a shadow dataset Dshadow from her local dataset (the rest is
used as Dprobe) and splits it into two parts: Shadow training
shadow and shadow updating set Dupdate
set Dtrain
shadow. Dtrain
shadow is used
to train the shadow model while Dupdate
shadow is further split to
m datasets: Dupdate1
shadow . The number of samples in
each of the m datasets depends on the attack. For instance, our
single-sample class attacks require each dataset containing
a single sample. The adversary then generates m shadow up-
dated models M (cid:48)1
shadow by updating the shadow
model Mshadow with m shadow updating sets in parallel.
shadow ···Dupdatem
shadow···M (cid:48)m
The adversary, in the end, probes the shadow and updated
shadow models with her probing set Dprobe, and calculates
the shadow posterior difference δ1
shadow. Together
with the corresponding shadow updating set’s ground truth
information (depending on the attack), the training data for
her attack model is derived.
shadow···δm
More generally, the training set for each of our attack mod-
els contains m samples corresponding to Dupdate1
shadow ···Dupdatem
shadow .
In all our experiments, we set m to 10,000. In addition, we
create 1,000 updated models for the target model, this means
the testing set for each attack model contains 1,000 samples,
corresponding to Dupdate1
target
target
.
···Dupdate1,000
4 Single-sample Attacks
In this section, we concentrate on the case when an ML model
is updated with a single sample. This is a simpliﬁed attack
scenario and we aim to examine the possibility of using poste-
rior difference to infer information about the updating set. We
start by introducing the single-sample label inference attack,
then, present the single-sample reconstruction attack.
4.1 Single-sample Label Inference Attack
Attack Deﬁnition. Our single-sample label inference attack
takes the posterior difference as the input and outputs the
label of the single updating sample. More formally, given
a posterior difference δ, our single-sample label inference
attack is deﬁned as follows:
ALI : δ (cid:55)→ (cid:96)
where (cid:96) is a vector with each entry representing the probability
of the updating sample afﬁliated with a certain label.
Methodology. To recap, the general construction of the at-
tack model consists of an MLP-based encoder which takes
the posterior difference as its input and outputs a latent vector
µ. For this attack, the adversary constructs her decoder also
with an MLP which is assembled with a fully connected layer
and a softmax layer to transform the latent vector to the corre-
sponding updating sample’s label. The concrete architecture
of our ALI’s decoder is presented in Appendix C.
To obtain the data for training ALI, the adversary generates
ground truth data by creating a shadow model as introduced in
Section 3 while setting the shadow updating set’s cardinality
to 1. Then, the adversary trains her attack model ALI with a
cross-entropy loss. Our loss function is,
(cid:96)i log( ˆ(cid:96)i)
LCE = ∑
i
where (cid:96)i is the true probability of label i and ˆ(cid:96)i is our predicted
probability of label i. The optimization is performed by the
ADAM optimizer.
To perform the label inference attack, the adversary con-
structs the posterior difference as introduced in Section 3,
then feeds it to the attack model ALI to obtain the label.
Experimental Setup. We evaluate the performance of
our single-sample label inference attack using the MNIST,
CIFAR-10, and Insta-NY datasets. First, we split each dataset
into three disjoint datasets: The target dataset Dtarget, the
shadow dataset Dshadow, and the probing dataset Dprobe. As
mentioned before, Dprobe contains 100 data samples. We
then split Dshadow to Dtrain
shadow to train the shadow
model as well as updating it (see Section 3). The same process
is applied to train and update the target model with Dtarget.
As mentioned in Section 3, we build 10,000 and 1,000 up-
dated models for shadow and target models, respectively. This
means the training and testing sets for our attack model con-
tain 10,000 and 1,000 samples, respectively.
shadow and Dupdate
1294    29th USENIX Security Symposium
USENIX Association
EncoderDecoderPosterior differenceLatent vectorInformation ofFigure 2: [Higher is better] Performance of the single-sample
label inference attack (ALI) on MNIST, CIFAR-10, and Insta-
NY datasets together with the baseline model. Accuracy is
adopted as the evaluation metric.
We use convolutional neural network (CNN) to build
shadow and target models for both CIFAR-10 and MNIST
datasets, and a multilayer perceptron (MLP) for the Insta-NY
dataset. The CIFAR-10 model consists of two convolutional
layers, one max pooling layer, three fully connected layers,
and a softmax layer. The MNIST model consists of two con-
volutional layers, two fully connected layers, and a softmax
layer. Finally, the Insta-NY model consists of three fully con-
nected layers and a softmax layer. The concrete architectures
of the models are presented in Appendix A.
All shadow and target models’ training sets contain 10,000
images for CIFAR-10 and MNIST, and 5,000 samples for
Insta-NY. We train the CIFAR-10, MNIST and Insta-NY mod-
els for 50, 25, and 25 epochs, respectively, with a batch size
of 64. To create an updated ML model, we perform a single-
epoch training. Finally, we adopt accuracy to measure the
performance of the attack. All of our experiments are imple-
mented using Pytorch [3]. For reproducibility purposes, our
code will be made available.
Results. Figure 2 depicts the experimental results. As we can
see, ALI achieves a strong performance with an accuracy of
0.97 on the Insta-NY dataset, 0.96 on the CIFAR-10 dataset,
and 0.68 on the MNIST dataset. Moreover, our attack sig-
niﬁcantly outperforms the baseline model, namely Random,
which simply guesses a label over all possible labels. As both
CIFAR-10 and MNIST contain 10 balanced classes, the base-
line model’s result is approximately 10%. For the Insta-NY
dataset, since it is not balanced, we randomly sample a label
for each sample to calculate the baseline which results in
approximately 29% accuracy. Our evaluation shows that the
different outputs of an ML model’s two versions indeed leak
information of the corresponding updating set.
4.2 Single-sample Reconstruction Attack
Attack Deﬁnition. Our single-sample reconstruction attack
takes one step further to construct the data sample used to
update the model. Formally, given a posterior difference δ,
our single-sample reconstruction attack, denoted by ASSR, is
Figure 3: Methodology of the single-sample reconstruction
attack (ASSR).
deﬁned as follows:
ASSR : δ (cid:55)→ xupdate
where xupdate denotes the sample used to update the model
(Dupdate = {xupdate}).
Methodology. Reconstructing a data sample is a much more
complex task than predicting the sample’s label. To tackle
this problem, we need an ML model which is able to generate
a data sample in the complex space. To this end, we rely on
autoencoder (AE).
Autoencoder is assembled with an encoder and a decoder.
Different from our attacks, AE’s goal is to learn an efﬁcient
encoding for a data sample: Its encoder encodes a sample into
a latent vector and its decoder tries to decode the latent vector
to reconstruct the same sample. This indicates AE’s decoder
itself is a data sample reconstructor. For our attack, we ﬁrst
train an AE, then transfer the AE’s decoder to our attack
model as the initialization of the attack’s decoder. Figure 3
provides an overview of the attack methodology. The concrete
architectures of our AEs’ encoders and decoders are presented
in Appendix D.
After the autoencoder is trained, the adversary takes its
decoder and appends it to her attack model’s encoder. To
establish the link, the adversary adds an additional fully con-
nected layer to its encoder which transforms the dimensions
of the latent vector µ to the same dimension as µAE.
We divide the attack model training process into two phases.
In the ﬁrst phase, the adversary uses her shadow dataset to
train an AE with the previously mentioned model architecture.
In the second phase, she follows the same procedure for single-
sample label inference attack to train her attack model. Note
that the decoder from AE here serves as the initialization of
the decoder, this means it will be further trained together with
the attack model’s encoder. To train both autoencoder and our
attack model, we use mean squared error (MSE) as the loss
function. Our objective is,
LMSE = (cid:107) ˆxupdate − xupdate(cid:107)2
2
USENIX Association
29th USENIX Security Symposium    1295
MNISTCIFAR-10Insta-NY0.00.20.40.60.81.0AccuracyALIRandomEncoderDecoderEncoderDecoderAutoencoderTransfer(a) MNIST
(b) CIFAR-10
(c) Insta-NY
Figure 4: [Lower is better] Performance of the single-sample reconstruction attack (ASSR) together with autoencoder and two
baseline models. Mean squared error is adopted as the evaluation metric. Autoencoder (AE) serves as an oracle as the adversary
cannot use it for her attack.
Figure 5: Visualization of some generated samples from the
single-sample reconstruction attack (ASSR) on the MNIST
dataset. Samples are fair random draws, not cherry-picked.
The ﬁrst row shows the original samples. The second row
shows the reconstructed samples by ASSR. The third shows
row the reconstructed samples by autoencoder, i.e., the upper
bound of our reconstruction attack.
where ˆxupdate is our predicted data sample. We again adopt
ADAM as the optimizer.
Experimental Setup. We use the same experimental setup
as the previous attack (see Section 4.1) except for the evalua-
tion metric. In detail, we adopt MSE to measure our attack’s
performance instead of accuracy.
We construct two baseline models, namely Label-random
and Random. Both of these baseline models take a random
data sample from the adversary’s shadow dataset. The differ-
ence is that the Label-random baseline picks a sample within
the same class as the target updating sample, while the Ran-
dom baseline takes a random data sample from the whole
shadow dataset of the adversary. The Label-random baseline
can be implemented by ﬁrst performing our single-sample
label inference attack to learn the label of the data sample and
then picking a random sample afﬁliated with the same label.
Results. First, our single-sample reconstruction attack
achieves a promising performance. As shown in Figure 4,
our attack on the MNIST dataset outperforms the Random
baseline by 36% and more importantly, outperforms the Label-
random baseline by 22%. Similarly, for the CIFAR-10 and
Insta-NY datasets, our attack achieves an MSE of 0.014 and
0.68 which is signiﬁcantly better than the two baseline mod-
els, i.e., it outperforms the Label-random (Random) baselines
by a factor of 2.1 (2.2) and 2.1 (2.3), respectively. The differ-
ence between our attack’s performance gain over the baseline
models on the MNIST and on the other datasets is expected
as the MNIST dataset is more homogeneous compared to the
other two. In other words, the chance of picking a random
data sample similar to the updating sample is much higher in
the MNIST dataset than in the other datasets.
Secondly, we compare our attack’s performance against
the results of the autoencoder for sample reconstruction. Note
that AE takes the original data sample as input and outputs
the reconstructed one, thus it is considered as an oracle, since
the adversary does not have access to the original updating
sample. Here, we just use AE’s result to show the best pos-
sible result for our attack. From Figure 4, we observe that
AE achieves 0.042, 0.0043, and 0.51 MSE for the MNIST,
CIFAR-10, and Insta-NY datasets, respectively, which indeed
outperforms our attack. However, our attack still has a com-
parable performance.
Finally, Figure 5 visualizes some randomly sampled re-
constructed images by our attack on MNIST. The ﬁrst row
depicts the original images used to update the models and the
second row shows the result of our attack. As we can see, our
attack is able to reconstruct images that are visually similar
to the original sample with respect to rotation and shape. We
also show the result of AE in the third row in Figure 5 which
as mentioned before, is the upper bound for our attack. The
results from Figure 4 and Figure 5 demonstrate the strong
performance of our attack.
5 Multi-sample Attacks