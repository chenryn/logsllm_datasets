# Automated Classification and Analysis of Internet Malware

**Authors:**
- Michael Bailey
- Jon Oberheide
- Jon Andersen
- Z. Morley Mao
- Farnam Jahanian
- Jose Nazario

## Abstract
The Internet faces numerous threats, including worms, phishing, and botnets, which compromise the availability of the network, the integrity of its hosts, and the privacy of its users. A key defense mechanism is anti-virus (AV) software, which detects, removes, and characterizes these threats. The effectiveness of AV products in characterizing malware has far-reaching implications, from facilitating information sharing across organizations to detecting new threats and assessing risk during quarantine and cleanup. This paper examines the ability of existing host-based AV products to provide semantically meaningful information about malicious software and tools (malware). Using a large, recent collection of malware spanning various attack vectors (e.g., spyware, worms, spam), we show that different AV products characterize malware inconsistently, incompletely, and with imprecise semantics. To address these limitations, we propose a new classification technique that describes malware behavior in terms of system state changes (e.g., files written, processes created) rather than sequences or patterns of system calls. Additionally, we present a method for automatically categorizing these profiles into groups that reflect similar classes of behaviors, demonstrating how behavior-based clustering provides a more direct and effective way of classifying and analyzing Internet malware.

## 1. Introduction
Many of the most visible and serious problems facing the Internet today involve a vast ecosystem of malicious software and tools. Spam, phishing, denial of service attacks, botnets, and worms largely depend on some form of malicious code, commonly referred to as malware. Malware is often used to infect the computers of unsuspecting victims by exploiting software vulnerabilities or tricking users into running malicious code. Understanding this process and how attackers use backdoors, key loggers, password stealers, and other malware functions is becoming an increasingly difficult and important problem.

Unfortunately, the complexity of modern malware is making this problem more challenging. For example, Agobot, first released in 2002, has been observed to have over 580 variants. Modern Agobot variants can perform DoS attacks, steal bank passwords and account details, propagate over the network using a diverse set of remote exploits, use polymorphism to evade detection and disassembly, and even patch vulnerabilities and remove competing malware from an infected system. Adding to the challenge is the increase in the number and diversity of Internet malware. A recent Microsoft survey found more than 43,000 new variants of backdoor trojans and bots during the first half of 2006. Automated and robust approaches are required to effectively manage this growing threat.

Previous efforts to automatically classify and analyze malware (e.g., AV, IDS) have primarily focused on content-based signatures. However, these signatures are inherently susceptible to inaccuracies due to polymorphic and metamorphic techniques. Additionally, the signatures often focus on specific exploit behaviors, which are complicated by the emergence of multi-vector attacks. As a result, IDS and AV products characterize malware in ways that are inconsistent across products, incomplete across malware, and imprecise in their semantics. This creates an environment where defenders are limited in their ability to share intelligence, detect new threats, and assess risk during quarantine and cleanup.

To address these limitations, we have developed and evaluated a dynamic analysis approach based on the execution of malware in virtualized environments and the causal tracing of operating system objects created due to malware's execution. The reduced collection of user-visible system state changes (e.g., files written, processes created) is used to create a fingerprint of the malware's behavior. These fingerprints are more invariant and directly useful than abstract code sequences representing programmatic behavior. They can be used to assess potential damage, enable the detection and classification of new threats, and assist in risk assessment during mitigation and cleanup. To handle the sheer volume and diversity of malware, we provide a method for automatically categorizing these malware profiles into groups that reflect similar classes of behaviors. Our methods are thoroughly evaluated using a large, recent, and diverse dataset of malware, representing various attack vectors (e.g., spam, worms, bots, spyware).

This paper is organized as follows:
- **Section 2** describes the shortcomings of existing AV software and outlines the requirements for effective malware classification.
- **Section 3** presents our behavior-based fingerprint extraction and clustering algorithm.
- **Section 4** provides a detailed evaluation of our methods.
- **Section 5** reviews related work.
- **Section 6** discusses limitations and future directions.
- **Section 7** concludes the paper.

## 2. Anti-Virus Clustering of Malware

### 2.1 Understanding Anti-Virus Malware Labeling
Host-based AV systems detect and remove malicious threats from end systems. As part of this process, these AV programs provide a description of the detected malware. The ability of these products to successfully characterize these threats has far-reaching effects, from facilitating sharing across organizations to detecting new threats and assessing risk during quarantine and cleanup. However, for this information to be effective, the descriptions provided by these systems must be meaningful.

In this section, we evaluate the ability of host-based AV to provide meaningful intelligence on Internet malware. We use three datasets from two sources, as shown in Table 1. One dataset, legacy, is taken from a network security community malware collection and consists of randomly sampled binaries from those posted to the community’s FTP server in 2004. In addition, we use a large, recent six-month collection of malware and a six-week subset of that collection at the beginning of the dataset collection period. The small and large datasets are part of the Arbor Malware Library (AML), created by Arbor Networks, Inc. The AML consists of binaries collected by various techniques, including web page crawling, spam traps, and honeypot-based vulnerability emulation. Since these methods collect binaries installed on the target system without the user’s permission, the collected binaries are highly likely to be malicious. Almost 3,700 unique binaries were collected over a six-month period in late 2006 and early 2007.

| **Date** | **Dataset Name** | **Collected** | **Unique MD5s** | **McAfee** | **F-Prot** | **ClamAV** | **Trend** | **Symantec** |
|----------|------------------|---------------|-----------------|------------|------------|------------|-----------|--------------|
| 01 Jan 2004 - 31 Dec 2004 | Legacy | 3,637 | 116 | 112 | 310 | 1,216 | 379 | 1,544 |
| 03 Sep 2006 - 22 Oct 2006 | Small | 893 | 590 | 253 | 1,102 | 416 | 246 | 2,035 |
| 03 Sep 2006 - 18 Mar 2007 | Large | 3,698 | 57 | 90 | 50 | 41 | 26 | 2,035 |

After collecting the binaries, we analyzed them using the AV scanners shown in Table 2. Each scanner was the most recent version available from each vendor at the time of the analysis. The virus definitions and engines were updated uniformly on November 20th, 2006, and again on March 31st, 2007.

| **Label** | **Software** | **Vendor** | **Version** | **Signature File** | **Update Dates** |
|-----------|--------------|------------|-------------|--------------------|------------------|
| McAfee Virus Scan | McAfee, Inc. | v4900 | 6.0.6.3 | 20 Nov 2006 | 31 Mar 2007 |
| F-Prot Anti-virus | FRISK Software | v5100 | 4.6.6 | 20 Nov 2006 | 31 Mar 2007 |
| Clam Anti-virus | Tomasz Kojm and the ClamAV Team | 0.88.6 | 0.90.1 | 20 Nov 2006 | 31 Mar 2007 |
| Trend Micro PC-cillin Internet Security | Trend Micro, Inc. | 8.000-1001 | 14.0.0.89 | 20 Nov 2006 | 31 Mar 2007 |
| Symantec Norton Anti-virus | Symantec Corporation | 8.32.1003 | 14.0.3.3 | 20 Nov 2006 | 31 Mar 2007 |

AV systems rarely use the exact same labels for a threat, and users of these systems have come to expect simple naming differences (e.g., W32Lovsan.worm.a versus Lovsan versus WORM MSBLAST.A) across vendors. It has always been assumed that there exists a simple mapping from one system’s name space to another, and recently investigators have begun creating projects to unify these name spaces. Unfortunately, the task appears daunting. Consider, for example, the number of unique labels created by various systems. The results in Table 1 show a substantial difference in the number of unique labels created by each AV system. While small differences might be expected, it is clear that AV vendors disagree not only on what to label a piece of malware but also on how many unique labels exist for malware in general.

Figure 1 shows a Venn diagram of malware labeled as SDBot variants by three AV products in the legacy dataset. The classification of SDBot is ambiguous, as each AV classifies a number of samples as SDBot, yet the intersection of these different SDBot families is not clean, with many samples classified as SDBot by one AV and as something else by the others. It is clear that these differences go beyond simple labeling—anti-virus products assign distinct semantics to different pieces of malware.

### 2.2 Properties of a Labeling System
Our previous analysis has provided significant evidence indicating that labeling across AV systems does not operate in a way that is useful to researchers, operators, and end users. Before evaluating these systems further, it is important to precisely define the properties an ideal labeling system should have. We have identified three key design goals for such a labeling system:

- **Consistency:** Identical items must and similar items should be assigned the same label.
- **Completeness:** A label should be generated for as many items as possible.
- **Conciseness:** The labels should be sufficient in number to reflect the unique properties of interest while avoiding superfluous labels.

### 2.3 Limitations of Anti-Virus
Having identified consistency, completeness, and conciseness as the design goals of a labeling system, we are now prepared to investigate the ability of AV systems to meet these goals. Table 3 shows the percentage of time two binaries classified as the same by one AV are classified the same by other AV systems. The results indicate that malware is inconsistently classified across AV vendors.

| **McAfee** | **F-Prot** | **ClamAV** | **Trend** | **Symantec** |
|------------|------------|------------|-----------|--------------|
| 100        | 50         | 62         | 67        | 27           |
| 13         | 100        | 57         | 18        | 7            |
| 27         | 96         | 100        | 25        | 13           |
| 39         | 41         | 34         | 100       | 14           |
| 59         | 61         | 68         | 55        | 100          |
| 100        | 45         | 39         | 45        | 42           |
| 25         | 100        | 23         | 23        | 25           |
| 54         | 57         | 100        | 52        | 46           |
| 38         | 35         | 32         | 100       | 33           |
| 17         | 18         | 13         | 100       | 33           |

The inconsistencies in labeling across AV systems highlight the need for a more effective and consistent approach to malware classification.