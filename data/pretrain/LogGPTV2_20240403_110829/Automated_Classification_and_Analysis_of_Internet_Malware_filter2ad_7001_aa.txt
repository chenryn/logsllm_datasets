title:Automated Classification and Analysis of Internet Malware
author:Michael Bailey and
Jon Oberheide and
Jon Andersen and
Zhuoqing Morley Mao and
Farnam Jahanian and
Jose Nazario
Automated Classiﬁcation and Analysis of
Internet Malware
Michael Bailey1, Jon Oberheide1, Jon Andersen1, Z. Morley Mao1,
Farnam Jahanian1,2, and Jose Nazario2
1 Electrical Engineering and Computer Science Department
{mibailey,jonojono,janderse,zmao,farnam}@umich.edu
University of Michigan
2 Arbor Networks
{farnam,jose}@arbor.net
Abstract. Numerous attacks, such as worms, phishing, and botnets,
threaten the availability of the Internet, the integrity of its hosts, and
the privacy of its users. A core element of defense against these attacks
is anti-virus (AV) software—a service that detects, removes, and charac-
terizes these threats. The ability of these products to successfully char-
acterize these threats has far-reaching eﬀects—from facilitating sharing
across organizations, to detecting the emergence of new threats, and as-
sessing risk in quarantine and cleanup. In this paper, we examine the
ability of existing host-based anti-virus products to provide semantically
meaningful information about the malicious software and tools (or mal-
ware) used by attackers. Using a large, recent collection of malware that
spans a variety of attack vectors (e.g., spyware, worms, spam), we show
that diﬀerent AV products characterize malware in ways that are incon-
sistent across AV products, incomplete across malware, and that fail to
be concise in their semantics. To address these limitations, we propose a
new classiﬁcation technique that describes malware behavior in terms of
system state changes (e.g., ﬁles written, processes created) rather than
in sequences or patterns of system calls. To address the sheer volume
of malware and diversity of its behavior, we provide a method for auto-
matically categorizing these proﬁles of malware into groups that reﬂect
similar classes of behaviors and demonstrate how behavior-based cluster-
ing provides a more direct and eﬀective way of classifying and analyzing
Internet malware.
1 Introduction
Many of the most visible and serious problems facing the Internet today depend
on a vast ecosystem of malicious software and tools. Spam, phishing, denial of
service attacks, botnets, and worms largely depend on some form of malicious
code, commonly referred to as malware. Malware is often used to infect the com-
puters of unsuspecting victims by exploiting software vulnerabilities or tricking
users into running malicious code. Understanding this process and how attackers
C. Kruegel, R. Lippmann, and A. Clark (Eds.): RAID 2007, LNCS 4637, pp. 178–197, 2007.
c(cid:2) Springer-Verlag Berlin Heidelberg 2007
Automated Classiﬁcation and Analysis of Internet Malware
179
use the backdoors, key loggers, password stealers, and other malware functions
is becoming an increasingly diﬃcult and important problem.
Unfortunately, the complexity of modern malware is making this problem
more diﬃcult. For example, Agobot [3], has been observed to have more than
580 variants since its initial release in 2002. Modern Agobot variants have the
ability to perform DoS attacks, steal bank passwords and account details, prop-
agate over the network using a diverse set of remote exploits, use polymorphism
to evade detection and disassembly, and even patch vulnerabilities and remove
competing malware from an infected system [3]. Making the problem even more
challenging is the increase in the number and diversity of Internet malware. A
recent Microsoft survey found more than 43,000 new variants of backdoor trojans
and bots during the ﬁrst half of 2006 [20]. Automated and robust approaches to
understanding malware are required to successfully stem the tide.
Previous eﬀorts to automatically classify and analyze malware (e.g., AV, IDS)
focused primarily on content-based signatures. Unfortunately, content-based sig-
natures are inherently susceptible to inaccuracies due to polymorphic and meta-
morphic techniques. In addition, the signatures used by these systems often
focus on a speciﬁc exploit behavior—an approach increasingly complicated by
the emergence of multi-vector attacks. As a result, IDS and AV products charac-
terize malware in ways that are inconsistent across products, incomplete across
malware, and that fail to be concise in their semantics. This creates an en-
vironment in which defenders are limited in their ability to share intelligence
across organizations, to detect the emergence of new threats, and to assess risk
in quarantine and cleanup of infections.
To address the limitations of existing automated classiﬁcation and analysis
tools, we have developed and evaluated a dynamic analysis approach, based on
the execution of malware in virtualized environments and the causal tracing of
the operating system objects created due to malware’s execution. The reduced
collection of these user-visible system state changes (e.g., ﬁles written, processes
created) is used to create a ﬁngerprint of the malware’s behavior. These ﬁn-
gerprints are more invariant and directly useful than abstract code sequences
representing programmatic behavior and can be directly used in assessing the
potential damage incurred, enabling detection and classiﬁcation of new threats,
and assisting in the risk assessment of these threats in mitigation and clean
up. To address the sheer volume of malware and the diversity of its behavior,
we provide a method for automatically categorizing these malware proﬁles into
groups that reﬂect similar classes of behaviors. These methods are thoroughly
evaluated in the context of a malware dataset that is large, recent, and diverse
in the set of attack vectors it represents (e.g., spam, worms, bots, spyware).
This paper is organized as follows: Section 2 describes the shortcomings of
existing AV software and enumerates requirements for eﬀective malware clas-
siﬁcation. We present our behavior-based ﬁngerprint extraction and ﬁngerprint
clustering algorithm in Section 3. Our detailed evaluation is shown in Section 4.
We present existing work in Section 5, oﬀer limitations and future directions in
Section 6, and conclude in Section 7.
180
M. Bailey et al.
2 Anti-Virus Clustering of Malware
Host-based AV systems detect and remove malicious threats from end systems.
As a normal part of this process, these AV programs provide a description for the
malware they detected. The ability of these products to successfully characterize
these threats has far-reaching eﬀects—from facilitating sharing across organiza-
tions, to detecting the emergence of new threats, and assessing risk in quarantine
and cleanup. However, for this information to be eﬀective, the descriptions pro-
vided by these systems must be meaningful. In this section, we evaluate the
ability of host-based AV to provide meaningful intelligence on Internet malware.
2.1 Understanding Anti-Virus Malware Labeling
In order to accurately characterize the ability of AV to provide meaningful labels
for malware, we ﬁrst need to acquire representative datasets. In this paper, we
use three datasets from two sources, as shown in Table 1. One dataset, legacy,
is taken from a network security community malware collection and consists of
randomly sampled binaries from those posted to the community’s FTP server
in 2004. In addition, we use a large, recent six-month collection of malware
and a six-week subset of that collection at the beginning of the dataset col-
lection period. The small and large datasets are a part of the Arbor Malware
Library (AML). Created by Arbor Networks, Inc. [1], the AML consists of bi-
naries collected by a variety of techniques including Web page crawling [28],
spam traps [26], and honeypot-based vulnerability emulation [2]. Since each of
these methods collects binaries that are installed on the target system
without the user’s permission, the binaries collected are highly likely
to be malicious. Almost 3,700 unique binaries were collected over a six-month
period in late 2006 and early 2007.
Table 1. The datasets used in this paper: A large collection of legacy binaries from
2004, a small six-week collection from 2006, and a large six-month collection of malware
from 2006/2007. The number of unique labels provided by ﬁve AV systems is listed for
each dataset.
Date
Dataset
Name
legacy 01 Jan 2004 - 31 Dec 2004
03 Sep 2006 - 22 Oct 2006
small
03 Sep 2006 - 18 Mar 2007
large
Collected
3,637
893
3,698
Number of
Number of Unique Labels
Unique MD5s McAfee F-Prot ClamAV Trend Symantec
116
112
310
1216
379
1,544
590
253
1,102
416
246
2,035
57
90
50
After collecting the binaries, we analyzed them using the AV scanners shown
in Table 2. Each of the scanners was the most recent available from each vendor
at the time of the analysis. The virus deﬁnitions and engines were updated
uniformly on November 20th, 2006, and then again on March 31st, 2007. Note
that the ﬁrst update occured more than a year after the legacy collection ended
and one month after the end of the small set collection. The second update was
13 days after the end of the large set collection.
Automated Classiﬁcation and Analysis of Internet Malware
181
Table 2. Anti-virus software, vendors, versions, and signature ﬁles used in this paper.
The small and legacy datasets were evaluated with a version of these systems in No-
vember of 2006 and both small and large were evaluated again with a version of these
systems in March of 2007.
Label
Software
McAfee Virus Scan
Version Signature File
F-Prot
F-Prot Anti-virus FRISK Software
ClamAV Clam Anti-virus Tomasz Kojm and
Vendor
McAfee, Inc.
v4900
v5100
4.6.6
6.0.6.3
0.88.6
the ClamAV Team 0.90.1
International
Trend
Symantec Norton Anti-virus Symantec
PC-cillin Internet Trend Micro, Inc. 8.000-1001
8.32.1003
Security 2007
14.0.0.89
14.0.3.3
Corporation
2007
20 Nov 2006
31 Mar 2007
20 Nov 2006
31 Mar 2007
20 Nov 2006
31 Mar 2007
20 Nov 2006
31 Mar 2007
20 Nov 2006
31 Mar 2007
AV systems rarely use the exact same labels for a threat, and users of these
systems have come to expect simple naming diﬀerences (e.g., W32Lovsan.worm.a
versus Lovsan versus WORM MSBLAST.A) across vendors. It has always been
assumed, however, that there existed a simple mapping from one system’s name
space to another, and recently investigators have begun creating projects to unify
these name spaces [4]. Unfortunately, the task appears daunting. Consider, for
example, the number of unique labels created by various systems. The result
in Table 1 is striking—there is a substantial diﬀerence in the number of unique
labels created by each AV system. While one might expect small diﬀerences, it
is clear that AV vendors disagree not only on what to label a piece of malware,
but also on how many unique labels exist for malware in general.
Fig. 1. A Venn diagram of malware labeled as SDBot variants by three AV products
in the legacy dataset. The classiﬁcation of SDBot is ambiguous.
One simple explanation for these diﬀerences in the number of labels is that
some of these AV systems provide a ﬁner level of detail into the threat landscape
than the others. For example, the greater number of unique labels in Table 1 for
F-Prot may be the result of F-Prot’s ability to more eﬀectively diﬀerentiate small
variations in a family of malware. To investigate this conjecture, we examined the
182
M. Bailey et al.
labels of the legacy dataset produced by the AV systems and, using a collection
of simple heuristics for the labels, we created a pool of malware classiﬁed by
F-Prot, McAfee, and ClamAV as SDBot [19]. We then examined the percentage
of time each of the three AV systems classiﬁed these malware samples as part
of the same family. The result of this analysis can be seen in Figure 1. Each AV
classiﬁes a number of samples as SDBot, yet the intersection of these diﬀerent
SDBot families is not clean, since there are many samples that are classiﬁed as
SDBot by one AV and as something else by the others. It is clear that these
diﬀerences go beyond simple diﬀerences in labeling—anti-virus products assign
distinct semantics to diﬀering pieces of malware.
2.2 Properties of a Labeling System
Our previous analysis has provided a great deal of evidence indicating that la-
beling across AV systems does not operate in a way that is useful to researchers,
operators, and end users. Before we evaluate these systems any further, it is im-
portant to precisely deﬁne the properties an ideal labeling system should have.
We have identiﬁed three key design goals for such a labeling system:
– Consistency. Identical items must and similar items should be assigned the
same label.
– Completeness. A label should be generated for as many items as possible.
– Conciseness. The labels should be suﬃcient in number to reﬂect the unique
properties of interest, while avoiding superﬂuous labels.
2.3 Limitations of Anti-Virus
Having identiﬁed consistency, completeness, and conciseness as the design goals
of a labeling system, we are now prepared to investigate the ability of AV systems
to meet these goals.
Table 3. The percentage of time two binaries classiﬁed as the same by one AV are
classiﬁed the same by other AV systems. Malware is inconsistently classiﬁed across AV
vendors.
McAfee F-Prot ClamAV Trend Symantec McAfee F-Prot ClamAV Trend Symantec
McAfee
F-Prot
ClamAV
Trend
Symantec
100
50
62
67
27
13
100
57
18
7
legacy
27
96
100
25
13
39
41
34
100
14
59
61
68
55
100
100
45
39
45
42
25
100
23
23
25
small
54
57
100
52
46
38
35
32
100
33
17
18
13