# Radius and Graph Metrics
- **Radius**: The ratio of the number of edges to the number of possible edges.
- **Max Length (Longest Shortest Path)**: The longest shortest path in the graph.
- **Node Radius**: The radius of a node in the graph.

## 2. Experiments

Our approach involves training a machine learning model using features described in Section IV.B.1. We employ a decision tree [1] as our machine learner, which is commonly used for triaging bug reports and predicting software defects.

We construct a dataset from our subjects, Windows 7 and Exchange 14, by extracting features and labels ("fixed" or "won't fix"). For evaluation, we use random splits: 2/3 of the instances are randomly selected for the training set, and the remaining 1/3 are used for the testing set. To avoid label population bias in the training set, we ensure that 50% of the training instances are labeled "fixed" and 50% are labeled "won't fix" by randomly removing some instances. To mitigate sampling bias, we repeat this experiment 100 times and compute the average performance.

To measure the model's performance, we use standard metrics including precision, recall, and F-measure [1, 20].

Applying a machine learner to our problem can result in four possible outcomes:
1. The learner predicts a fixable crash as fixable (f → f).
2. The learner predicts a fixable crash as won't fix (f → w).
3. The learner predicts a won't fix crash as fixable (w → f).
4. The learner predicts a won't fix crash as won't fix (w → w).

These outcomes can be used to evaluate the classification with the following measures:

- **Precision (P(fix))**: The number of crashes correctly classified as fixable (Nf→f) over the total number of crashes classified as fixable.
  \[
  P(\text{fix}) = \frac{N_{f \rightarrow f}}{N_{f \rightarrow f} + N_{w \rightarrow f}}
  \]

- **Recall (R(fix))**: The number of crashes correctly classified as fixable (Nf→f) over the total number of fixable crashes.
  \[
  R(\text{fix}) = \frac{N_{f \rightarrow f}}{N_{f \rightarrow f} + N_{f \rightarrow w}}
  \]

- **F-measure (F(fix))**: A composite measure of precision and recall.
  \[
  F(\text{fix}) = \frac{2 \cdot P(\text{fix}) \cdot R(\text{fix})}{P(\text{fix}) + R(\text{fix})}
  \]

### Table IV: Fixable Crash Prediction Results

| Subjects/Features | Precision | Recall | F-measure |
|-------------------|-----------|--------|-----------|
| **Exchange 14**   |           |        |           |
| - Bug Meta Data   | 57.2      | 80     | 66.3      |
| - Crash Graph     | 79.5      | 80     | 74.5      |
| - All Features    | 69.9      | 72.1   | 74.7      |
| **Windows 7**     |           |        |           |
| - Bug Meta Data   | 71.8      | 69.6   | 68.6      |
| - Crash Graph     | 70.6      | 66.1   | 65.0      |
| - All Features    | 60.3      | 61.2   | 65.4      |

Table IV shows the average recall, precision, and F-measure. For Exchange 14, the F-measure for fixable crashes is around 75% using only crash graph features, compared to 66% using only bug meta-data. When all features are used, the F-measure remains around 75%. These results indicate that crash graph features are informative for predicting fixable crashes.

For Windows 7, the F-measure for bug meta-data features is around 69%, while for crash graph features, it is 65%. The slightly lower F-measure for crash graph features may be due to the hit-count-oriented crash fix process in Windows 7, where higher hit counts are more likely to be fixed. However, even without the hit count, crash graph features predict fixable crashes almost as well, indicating their informativeness when hit count is not available or reliable.

## V. Threats to Validity

We identify the following threats to validity:

- **Subject Selection Bias**: Our experiments use only industrial project data. Open source projects may have different crash properties, and similar experiments on open source projects may yield different results. However, we could not find any open source project with a crash reporting system that includes bucketing algorithms and auto-crash bug reporting features.

- **Data Selection Bias**: In our experiments, we use partial auto-crash bug reports. Since Microsoft does not store all crash traces, only partial auto crash bug reports were available for our study. Despite this, the stored data traces are substantially large due to the wide deployment of Windows and Exchange.

## VI. Related Work

Glerum et al. present ten years of debugging experience using WER, including designing WER, bucket algorithms, common debugging practices, and their challenges [9]. WER uses a server-client model to collect crash minidumps from clients and classifies crash information using over 500 heuristics such as crashed point and trace similarity. WER has significantly improved the crash-debugging process by allowing developers to quickly identify crashes and providing useful information for debugging. Our crash graph approach builds on WER, the classified buckets, and auto-crash bug reports.

However, WER may misclassify some crashes, leading to duplicated auto-crash bug reports. Our crash graph approach efficiently detects duplicated reports by comparing entire crashes rather than individual traces, as discussed in Section IV.A.3.

Research has also focused on identifying the causes of crashes. Ganapathi et al. [8] analyzed Windows XP kernel crash data and found that OS crashes are predominantly caused by poorly-written device driver code.

Bartz et al. propose a stack trace similarity measure based on callstack edit distance with tuned edit penalties [5]. Their approach is superior to previous measures like Euclidean distance for detecting similar crashes. However, their approach has limitations, including the partial trace issue, as discussed in Section IV.A.3.

Arnold et al. proposed combining execution traces to facilitate program understanding [4]. Their approach is similar to our crash graph but focuses on program understanding rather than efficient crash triaging.

Wang et al. and Runeson et al. propose techniques to detect duplicated bug reports using text similarity or execution trace similarity [16, 17]. The accuracy of text similarity-based duplicate detection is around 40-50%. Wang et al. generate artificial execution traces and use them to detect duplicates, but these traces are not collected in the field, resulting in an accuracy of around 40-60%. Our crash graph approach compares entire crashes using subgraph similarity measures, making it more effective.

## VII. Conclusions

Crash reporting systems are common in widely deployed software systems, but there has been little research on how these crashes are analyzed to fix problems. In this paper, we propose the use of crash graphs, an aggregated form of multiple crashes, and demonstrate their efficacy. Crash graphs are more efficient at identifying duplicate auto-bug reports than comparing individual stack traces. Additionally, we show that machine learning features of crash graphs are informative for predicting fixable crashes.

As part of our future work, we have started deploying crash graphs to actual engineers at Microsoft to determine their engineering efficacy and utility. We constructed crash graphs for fixed crashes from Microsoft Exchange 14 and presented them to the corresponding developers who fixed the crashes. So far, we have received very positive feedback, such as:

- "The graph would be showing me what a single minidump could not…" – Developer 1
- "Usually developers can guess 50-80% of crash causes by reading call traces. This graph can help developers see all traces together." – Developer 2

We plan to further investigate the utility of crash graphs by asking developers quantitative and qualitative questions, soliciting suggestions for visualization, and performing empirical user studies. We also hope to help the developer community outside of Microsoft adopt these crash analysis processes.

## VIII. Acknowledgements

We thank the Windows 7 and Exchange 14 developers for their valuable feedback and comments on our study. We also thank Brendan Murphy for his help with data collection and discussions, and Ray Buse and Caitlin Sadowski for their contributions.

## IX. References

[1] E. Alpaydin, Introduction to Machine Learning: The MIT Press, 2004.
[2] J. Anvik, L. Hiew, and G. C. Murphy, "Who should fix this bug?," in Proceedings of the 28th international conference on Software engineering. Shanghai, China: ACM, 2006, pp. 361-370.
[3] Apple, "Technical Note TN2123: CrashReporter," 2010.
[4] D. C. Arnold, D. H. Ahn, B. R. de Supinski, G. L. Lee, B. P. Miller, and M. Schulz, "Stack Trace Analysis for Large Scale Debugging," Symposium, 2007. IPDPS 2007, 2007.
[5] K. Bartz, J. W. Stokes, J. C. Platt, R. Kivett, D. Grant, S. Calinoiu, and G. Loihle, "Finding similar failures using callstack similarity," in Proceedings of the Third conference on Tackling computer systems problems with machine learning techniques. San Diego, California: USENIX Association, 2008, pp. 1-1.
[6] N. Bettenburg, R. Premraj, T. Zimmermann, and S. Kim, "Extracting structural information from bug reports," in 2008 international working conference on Mining software repositories. Leipzig, Germany: ACM, 2008, pp. 27-30.
[7] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction to Algorithms, 2nd ed: The MIT Press, 2001.
[8] A. Ganapathi, V. Ganapathi, and D. Patterson, "Windows XP kernel crash analysis," in Proceedings of the 20th conference on Large Installation System Administration. Washington, DC: USENIX Association, 2006, pp. 12-12.
[9] K. Glerum, K. Kinshumann, S. Greenberg, G. Aul, V. Orgovan, G. Nichols, D. Grant, G. Loihle, and G. Hunt, "Debugging in the (very) large: ten years of implementation and experience," in Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles. Big Sky, Montana, USA: ACM, 2009, pp. 103-116.
[10] G. Jeong, S. Kim, and T. Zimmermann, "Improving bug triage with bug tossing graphs," in Proceedings of the the 7th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on The foundations of software engineering. Amsterdam, The Netherlands: ACM, 2009, pp. 111-120.
[11] D. Kim, X. Wang, S. Kim, A. Zeller, S. C. Cheung, and S. Park., "Which Crashes Should I Fix First?: Predicting Top Crashes at an Early Stage to Prioritize Debugging Efforts," IEEE Trans. Softw. Eng., 2011.
[12] H. Kwak, C. Lee, H. Park, and S. Moon, "What is Twitter, a social network or a news media?," in Proceedings of the 19th international conference on World wide web. Raleigh, North Carolina, USA: ACM, 2010, pp. 591-600.
[13] Microsoft, "Windows Error Reporting: Getting Started," 2010, http://www.microsoft.com/whdc/winlogo/maintain/StartWER.mspx.
[14] D. Molla, "Learning of graph-based question answering rules," in Proceedings of TextGraphs: the First Workshop on Graph Based Methods for Natural Language Processing on the First Workshop on Graph Based Methods for Natural Language Processing: Association for Computational Linguistics, 2006, pp. 37-44.
[15] Mozilla, "Crash Stats," 2010, crash-stats.mozilla.com.
[16] P. Runeson, M. Alexandersson, and O. Nyholm, "Detection of Duplicate Defect Reports Using Natural Language Processing," in Proceedings of the 29th international conference on Software Engineering: IEEE Computer Society, 2007, pp. 499-510.
[17] X. Wang, L. Zhang, T. Xie, J. Anvik, and J. Sun, "An approach to detecting duplicate bug reports using natural language and execution information," in Proceedings of the 30th international conference on Software engineering. Leipzig, Germany: ACM, 2008, pp. 461-470.
[18] D. B. West, Introduction to Graph Theory, 2nd ed: Prentice Hall, 2001.
[19] L. Wiskott, J.-M. Fellous, N. Krüger, and C. v. d. Malsburg, "Face Recognition by Elastic Bunch Graph Matching," IEEE Trans. Pattern Anal. Mach. Intell., vol. 19, no. 7, pp. 775-779, 1997.
[20] I. H. Witten and E. Frank, Data Mining: Practical Machine Learning Tools and Techniques (Second Edition): Morgan Kaufmann, 2005.
[21] T. Zimmermann and N. Nagappan, "Predicting defects using network analysis on dependency graphs," in Proceedings of the 30th international conference on Software engineering. Leipzig, Germany: ACM, 2008, pp. 531-540.