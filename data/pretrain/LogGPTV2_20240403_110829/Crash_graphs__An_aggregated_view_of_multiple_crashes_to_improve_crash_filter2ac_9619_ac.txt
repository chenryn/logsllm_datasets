Radius  
Ratio  of  the  number  of  edges  and 
the number of possible edges 
Max length (longest shortest path) 
Node radius 
2)  Experiments 
Our approach is to train a machine learner using features 
described in Section IV.B.1. We use decision tree [1] as our 
machine learner, which is widely used to triage bug reports 
and predict software defects.  
From  our  subjects  Windows  7  and  Exchange  14,  we 
construct a corpus by extracting features and labels (“fixed” 
or “won’t fix”). For the evaluation, we use random splits: to 
train a machine learner, we randomly select 2/3 of instances 
and use them as a training set; the remaining 1/3 is used as a 
testing set. To avoid label population bias in the training set, 
we make sure that the instances in the training set have 50% 
fixed bugs and 50% of won’t fix bugs by randomly removing 
some  instances  in  the  training  set.  To  also  avoid  sampling 
bias,  we  run  this  experiment  100  times  and  compute  the 
average performance. 
To  measure  the  model  performance,  we  use  standard 
measures including precision, recall and F-measure [1, 20]. 
Applying a machine learner to our problem can result in four 
possible outcomes: the learner predicts (1) a fixable crash as 
fixable (f → f); (2) a fixable crash as won’t fix (f → w);  (3) 
a  won’t  fix  crash  as  fixable  (w  →  f);  and  (4)  a  won’t  fix 
crash  as  won’t  fix  (w  →  w).  These  outcomes  can  be  then 
used  to  evaluate  the  classification  with  the  following  three 
measures: 
Precision:  the  number  of  crashes  correctly  classified  as 
fixable (Nf→f) over the number of all methods classified as 
fixable. 
Precision   P(fix) = 
N f " f
N f " f + Nw " f
Recall: the number of crashes correctly classified as fixable 
(Nf→f) over the total number of fixable crashes. 
! 
Recall   R(fix) = 
N f " f
N f " f + N f "w
F-measure: a composite measure of precision and recall. 
F-measure (fix) = 
! 
2P( fix)R( fix)
P( fix) + R( fix)  
TABLE IV.  
FIXABLE CRASH PREDICTION RESULTS  
! 
Subjects/Features 
  Bug  
4
1
e
g
n
a
h
c
x
E
meta data 
Crash graph 
All features 
7
  Bug  
s
w
o
d
n
i
W
meta data 
Crash graph 
All features 
Precision  Recall 
57.2 
80 
79.5 
80 
69.9 
72.1 
71.8 
69.6 
70.6 
66.1 
60.3 
61.2 
F-measure 
66.3 
74.5 
74.7 
68.6 
65.0 
65.4 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:42:01 UTC from IEEE Xplore.  Restrictions apply. 
491Table    IV  shows  the  average  recall,  precision  and  F-
measure. For Exchange 14, F-measure of fixable crashes is 
around  75%  using  only  crash  graph  features.  However,  F-
measure using only bug meta-data is around 66%. When we 
use all features, the F-measure is around 75%. These results 
indicate that crash graph features are informative to predict 
fixable crashes.  
For Windows 7, F-measure for bug meta-data features is 
around  69%,  while  F-measure  for  crash  graph  features  is 
65%.  The  F-measure  using  crash  graph  features  is  slightly 
lower. One possible explanation is that the crash fix process 
of Windows 7 is hit-count oriented. If a crash has a higher hit 
count,  it  is  likely  to  be  fixed.  The  hit  count  is  one  of  bug 
meta-data features.  
However, even without using the hit count, crash graph 
features can predict fixable crashes almost as well as when 
using  the  hit  count.  This  indicates  crash  graph  features  are 
informative to predict fixable crashes when hit count is not 
available  or  not  reliable.  Overall,  these  results  show  that 
crash  graph  features  are  informative  to  predict  fixable 
crashes in advance.  
V.  THREATS TO VALIDITY  
We identify the following threats to validity: 
Subject selection bias: We use only industrial project data 
for  our  experiments.  Open  source  projects  may  have 
different crash properties and the same experiments on open 
source  projects  may  yield  different  results.  However,  we 
could  not  find  any  open  source  project  which  had  a  crash 
reporting  system  with  bucketing  algorithms  and  auto-crash 
bug reporting features. 
Data  selection  bias:    In  our  experiments,  we  use  partial 
auto-crash  bug  reports.  Since  Microsoft  does  not  store  all 
crash  traces,  only  partial  auto  crash  bug  reports  were 
available for our study. However, despite this fact given the 
wide deployment of Windows and Exchange the stored data 
traces are substantially large. 
VI.  RELATED WORK 
Glerum et al. present ten years of debugging experience-
using  WER  including  designing  WER,  bucket  algorithms, 
common debugging practice, and their challenges [9]. WER 
uses the server-client model to collect crash minidump from 
clients,  and 
their  bucketing  algorithms  classify  crash 
information using over 500 heuristics such as crashed point 
and trace similarity. WER has significantly improved crash-
debugging  process  by  permitting  developers  to  identify 
crashes  quickly  and  providing  useful 
information  for 
debugging. Our crash graph approach is on top of WER, the 
classified buckets, and auto-crash bug reports. 
However,  WER  may  misclassify  some  crashes,  which 
causes  the  second  bucket  problem  and  yields  duplicated 
auto-crash  bug  reports.  Our  crash  graph  approach  can 
efficiently detect duplicated reports by comparing the whole 
crashes rather than comparing them one by one as discussed 
in Section IV.A.3. 
Research  has  also  focused  on  identifying  the  causes  of 
crashes.  Ganapathi  et  al.  [8]  analyzed  and  collected 
Windows XP kernel crash data for a sample population and 
found  out  that  OS  crashes  are  predominantly  caused  by 
poorly-written device driver code. 
Bartz  et  al.  propose  a  stack  trace  similarity  measure 
based on callstack edit distance with tuned edit penalties [5]. 
They  show  that  their  approach  is  superior  to  previous 
measure such as the Euclidian distance for detecting similar 
crashes.  However,  since  their  approach  is  based  on  crash 
trace-to-crash  similarity  measures,  they  have  inevitable 
limitations discussed in Section IV.A.3 including the partial 
trace issue.    
Arnold  et  al.  proposed  combining  execution  traces  to 
facilitate program understanding [4]. Their trace combining 
approach  is  similar  to  our  crash  graph.  However,  they 
combine  traces  from  execution  traces  rather  than  multiple 
crash traces. In addition, their goal is program understanding, 
and our goal is efficient crash triaging. 
Wang  et  al.  and  Runeson  et  al.  propose  techniques  to 
detect  duplicated  bug  reports  using  text  similarity  or 
execution  trace  similarity  [16,  17].  Usually  the  accuracy  of 
text similarity based duplicate detection is around 40~50%. 
Wang et al. generate artificial execution traces and use them 
to detect duplicates. However, these traces are not collected 
in the field. Still the accuracy is around 40~60%, since they 
also  compare  traces  one  by  one,  while  our  crash  graph 
approach  compares  the  entire  crashes  using  the  sub  graph 
similarity measure.  
VII.  CONCLUSIONS  
Crash reporting systems are common these days in most 
widely deployed software systems. Yet, there has been little 
research  on  how  these  crashes  are  analyzed  to  fix  the 
problems. In this paper, we propose the use of crash graph, 
an  aggregated  form  of  multiple  crashes,  and  show  its 
efficacy.  Crash  graphs  are  more  efficient 
identify 
duplicate  auto-bug  reports  than  comparing  individual  stack 
traces.  In  addition,  we  show  that  machine-learning  features 
of crash graphs are informative to predict fixable crashes. 
Crash reporting systems have become more important to 
identify  crashes  and  provide  useful  debugging  information 
for  developers.  Our  experience  indicates  it  is  important  to 
use an aggregated form of crashes such as crash graphs for 
classifying or triaging crashes rather than using or comparing 
individual crashes. 
As part of our future work, we have started a deployment 
to actual engineers at Microsoft to determine the engineering 
efficacy  and  utility  of  crash  graphs.  We  constructed  crash 
graphs for fixed crashes from Microsoft Exchange 14. Then 
we  presented 
the  corresponding 
developer who fixed the crash. So far we have received very 
promising and enthusiastic support for our work, for example 
we received the following feedback from developers:  
the  crash  graphs 
to 
to 
“… the graph would be showing me what a single  
minidump could not…” – Developer 1 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:42:01 UTC from IEEE Xplore.  Restrictions apply. 
492“Usually developers can guess 50-80% of crash causes by 
reading call traces. This graph can help developers see all 
traces together.” – Developer 2 
We  plan  to  investigate  further  along  this  line  to  deploy 
crash graphs widely across Microsoft. In particular, we plan 
to ask many developers quantitative and qualitative questions 
if  the  crash  graphs  would  be  useful  to  fix  these  kinds  of 
crashes, solicit suggestions for visualization, and perform an 
empirical user study on the efficacy of crash graphs. We also 
hope to help the developer community outside of Microsoft 
to adopt these crash analysis processes. 
VIII.  ACKNOWLEDGEMENTS 
Our thanks to Windows 7 and Exchange 14 developers for 
their valuable feedback and comments on our study. We 
thank Brendan Murphy for his help with data collection and 
his discussions on this work. We also thank Ray Buse and 
Caitlin Sadowski for discussion on this work. 
IX.  REFERENCES 
[1]  E.  Alpaydin,  Introduction  to  Machine  Learning:  The  MIT 
Press, 2004. 
[2]  J. Anvik, L. Hiew, and G. C. Murphy, "Who should fix this 
bug?," in Proceedings of the 28th international conference on 
Software engineering. Shanghai, China: ACM, 2006, pp. 361-
370. 
[3]  Apple, "Technical Note TN2123: CrashReporter," 2010. 
[4]  D. C. Arnold, D. H. Ahn, B. R. de Supinski, G. L. Lee, B. P. 
Miller, and M. Schulz, "Stack Trace Analysis for Large Scale 
in  Parallel  and  Distributed  Processing 
Debugging," 
Symposium, 2007. IPDPS 2007, 2007. 
[5]  K.  Bartz,  J.  W.  Stokes,  J.  C.  Platt,  R.  Kivett,  D.  Grant,  S. 
Calinoiu,  and  G.  Loihle,  "Finding  similar  failures  using 
callstack  similarity,"  in  Proceedings of the Third conference 
on  Tackling  computer  systems  problems  with  machine 
learning 
techniques.  San  Diego,  California:  USENIX 
Association, 2008, pp. 1-1. 
[6]  N.  Bettenburg,  R.  Premraj,  T.  Zimmermann,  and  S.  Kim, 
"Extracting structural information from bug reports," in 2008 
international  working  conference  on  Mining  software 
repositories. Leipzig, Germany: ACM, 2008, pp. 27-30. 
[7]  T.  H.  Cormen,  C.  E.  Leiserson,  R.  L.  Rivest,  and  C.  Stein, 
Introduction to Algorithms, 2nd ed: The MIT Press, 2001. 
[8]  A. Ganapathi, V. Ganapathi, and D. Patterson, "Windows XP 
kernel crash analysis," in Proceedings of the 20th conference 
on  Large  Installation  System  Administration.  Washington, 
DC: USENIX Association, 2006, pp. 12-12. 
 [9]  K.  Glerum,  K.  Kinshumann,  S.  Greenberg,  G.  Aul,  V. 
Orgovan,  G.  Nichols,  D.  Grant,  G.  Loihle,  and  G.  Hunt, 
"Debugging in the (very) large: ten years of implementation 
and  experience,"  in  Proceedings  of  the  ACM  SIGOPS  22nd 
symposium  on  Operating  systems  principles.  Big  Sky, 
Montana, USA: ACM, 2009, pp. 103-116. 
[10] G.  Jeong,  S.  Kim,  and  T.  Zimmermann,  "Improving  bug 
triage with bug tossing graphs," in Proceedings of the the 7th 
joint  meeting  of 
the  European  software  engineering 
conference  and  the  ACM  SIGSOFT  symposium  on  The 
foundations  of  software  engineering.  Amsterdam,  The 
Netherlands: ACM, 2009, pp. 111-120. 
[11] D.  Kim,  X.  Wang,  S.  Kim,  A.  Zeller,  S.  C.  Cheung,  and  S. 
Park.,  "Which  Crashes  Should  I  Fix  First?:  Predicting  Top 
Crashes  at  an  Early  Stage  to  Prioritize  Debugging  Efforts," 
IEEE Trans. Softw. Eng., 2011. 
[12] H. Kwak, C. Lee, H. Park, and S. Moon, "What is Twitter, a 
social network or a news media?," in Proceedings of the 19th 
international conference on World wide web.  Raleigh,  North 
Carolina, USA: ACM, 2010, pp. 591-600. 
[13] Microsoft,  "Windows  Error  Reporting:  Getting  Started," 
2010, 
http://www.microsoft.com/whdc/winlogo/maintain/StartWER
.mspx. 
[14] D.  Molla,  "Learning  of  graph-based  question  answering 
rules," in Proceedings of TextGraphs: the First Workshop on 
Graph  Based  Methods  for  Natural  Language  Processing  on 
the  First  Workshop  on  Graph  Based  Methods  for  Natural 
Language  Processing:  Association 
for  Computational 
Linguistics, 2006, pp. 37-44. 
[15] Mozilla, "Crash Stats," 2010, crash-stats.mozilla.com. 
[16] P. Runeson, M. Alexandersson, and O. Nyholm, "Detection of 
Duplicate  Defect  Reports  Using  Natural  Language 
international 
Processing," 
conference  on  Software  Engineering: 
IEEE  Computer 
Society, 2007, pp. 499-510. 
in  Proceedings  of 
the  29th 
[17] X.  Wang,  L.  Zhang,  T.  Xie,  J.  Anvik,  and  J.  Sun,  "An 
approach  to  detecting  duplicate  bug  reports  using  natural 
language  and  execution  information,"  in  Proceedings of the 
30th 
international  conference  on  Software  engineering. 
Leipzig, Germany: ACM, 2008, pp. 461-470. 
[18] D.  B.  West,  Introduction to Graph Theory,  2nd  ed:  Prentice 
Hall, 2001. 
[19] L. Wiskott, J.-M. Fellous, N. Krüger, and C. v. d. Malsburg, 
"Face Recognition by Elastic Bunch Graph Matching," IEEE 
Trans. Pattern Anal. Mach. Intell., vol. 19, no. 7, pp. 775-779, 
1997. 
[20] I.  H.  Witten  and  E.  Frank,  Data Mining: Practical Machine 
Learning  Tools  and  Techniques  (Second  Edition):  Morgan 
Kaufmann, 2005. 
[21] T. Zimmermann and N. Nagappan, "Predicting defects using 
network  analysis  on  dependency  graphs,"  in  Proceedings of 
the  30th  international  conference  on  Software  engineering. 
Leipzig, Germany: ACM, 2008, pp. 531-540. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:42:01 UTC from IEEE Xplore.  Restrictions apply. 
493