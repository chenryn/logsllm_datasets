/*special processing for table'profiles'*/
prefix += ZBX_DM_MAX_CONFIG_IDS *(zbx_uint64_t） nodeid;
prefix
assert（NULL != r_table）;
r_table=DBget_table(tables[i].fields[j].fk_table）;
else if (NULL != tables[i].fields[j].fk_table)
prefix
if（o !=（tables[i].flags & ZBX_SYNC))
relations
"`group_discovery`
"group_discovery`
`group_prototype`
+= ZBX_DM_MAX_CONFIG_IDS *(zbx_uint64_t） nodeid;
assert（o）;
continue;
if（o!=（r_table->flags &ZBx_SYNC）)
，下面是核心代码：
tables[i].recid))
DROP
DROP
DROP
FOREIGN KEY
FOREIGNKEY
FOREIGN KEY
第13章
Zabbix分布式监控
`c_group
●223°
---
## Page 239
224
行是一个 ZBX_FIELD，根据它的定义，我们就知道fag字段原来是倒数第二个字段，随便在
继续追溯，可以从 dbschema.h 中找到ZBX_TABLE的结构定义。再进一步，可以知道上面这一
DM_MAX_CONFIG_IDS的值是100000000000000。
建立Node架构后，它的itemid就会变为00000000010001。
是300000000000000。每一个Item在Zabbix数据库中都有一个自己的id，比如10001这个id，
如下内容：
是 prefix。prefix的赋值过程是这样的：
tableslij.feldslij.name可以理解为遍历数据库的每一张表。我们可以看到，这条 SQL最关键的
就行了）。tables这个数组也是在dbscheme.c中定义的，它的内容就是Zabbix数据库中的结构，
一些值，即将tables[i].fields[i].name增加 prefix这么多（ZBX_FS_UI164是个占位符，我们理解为%s
Zabbix监控系统深度实践
（100000000000000)
我们来看看flags字段的值是什么，首先我们发现talbes数组，它的类型是ZBX_TABLE。
这个操作是只当某个数据库表的flags 字段为 ZBX_SYNC 的时侯才会进行，只不过ZBX
prefix += ZBX_DM_MAX_CONFIG_IDS *（zbx_uint64_t）nodeid;
下面还有一个对 prefix的操作：
从代码可以看出，prefix 由这两个数字乘以 nodeid得到，比如 nodeid为3，那么 prefix 就
#define ZBX_DM_MAX_CONFIG_IDS
prefix += ZBX_DM_MAX_CONFIG_IDS *（zbx_uint64_t) nodeid;
如果 tables 的 flags 字段为1，即“flags & ZBX_SYNC”为 true 的时候，prefix 还要再加上
#define ZBX_DM_MAX_HISTORY_IDS
ZBX_DM_MAX_HISTORY_IDS是在include/db.h中定义的，它的值是100000000000000：
prefix= ZBX_DM_MAX_HISTORY_IDS*（zbx_uint64_t)nodeid;
我们反过来看这段代码，最后的DBexecute显然是在执行一些SQL，SQL的内容是在增加
tables[i].fields[jl.name)
prefix,
tables[i].fields[j].name
tables[i].fields[j].name,
tables[i].table,
（zbxuint64t)
(zbx_uint64_t)
_UINT64_C(100000000000)
UINT64_
---
## Page 240
ON DELETE CASCADE"
FOREIGN KEY（^hostid’） REFERENCES ‘hosts`（^hostid`） ON DELETE CASCADE",
hostid`）REFERENCES
300000000010001。
id区分开，要对id加上一个前缀。比如10001这个itemid，再加上prefix后，可能就会变成
父节点中，难免会出现子节点中itemid（或其他id）相同的情况，所以需要将不同子节点的
比如itemid、hostid等，这个id是该表的主键。在分布式架构中，所有子节点的数据要同步到
下Zabbix数据库表结构了，对于任何一个资源（Item、Host等），在表中都有一个自己的id，
分布式架构中，是需要在不同节点间传输的。
ZBX_DM_MAX_CONFIG_IDS*nodeid这么多。具有ZBX_SYNC这个属性，就表示这个数据库在
SYNC。再结合代码逻辑，下面这个逻辑与的含义就是这个数据库表是具有ZBX_SYNC属性的。
的用法，ZBX_NOTNULLIZBX_SYNC表示这一行有两个属性，分别是ZBX_NOTNULL和ZBX
dbschema.c中找一行表示结构的描述：
prefix的来龙去脉搞清楚了，那为什么要将表中某个字段加上 prefix呢？这里就要简单说
后面的逻辑就简单了，就是如果数据库表有ZBX_SYNC 属性，那么 prefix 还要再增加
flags & ZBX_SYNC
const char
最后一步就是创建新的外键了，和删除外键类似，从db_schema_fkeys 中获取 SQL，形如：
发现它的 flag 值是 ZBX_NOTNULL|ZBX_SYNC。这种形式是非常典型的C 语言中表示属性
("ipmi_available","O", NULL,NULL,O,ZBX_TYPE_INT, ZBX_NOTNULL | ZBX_SYNC, 0),
"ALTER TABLE ^group_prototype′ADD CONSTRAINT
"ALTER TABLEhosts`ADD CONSTRAINTc_hosts_3
"ALTERTABLE
"ALTERTABLE
group_prototype`ADDCONsTRAINT
"hosts（hostid）",
`hosts`
*const db_schema_fkeys[] ={
"c_group_prototype_3"
"c_group_prototype_2
第13章Zabbix分布式监控
FOREIGN KEY
FOREIGN KEY
·225°
---
## Page 241
·226°
表“Current Node”，在其中可以选择要查看的节点的数据。
node的 zabbix_server.conf 中的 NodeID一样。Type设置为 Child。
单击“New node”按钮，根据提示设置需要的属性，需要注意的是ID要设置的和这个Child
NodeID参数，一个是是否设置了数据库。
monitoring”，那就是前面几步没有设置好，需要再检查一下，一个是zabbix_server.conf 中的
中选择“Nodes”选项。如果单击进去后发现显示的是“Your setup is not configured for distributed
半就退出，虽然说可以修复，但是会非常困难。我们需要知道之前运行到什么地方，然后还要
运行多次，在update数据库的时候，prefix会被加很多次，从而造成数据库错乱。而运行到一
Zabbix监控系统深度实践
自行设置即可。
的装填。单击“New node”按钮创建一个子节点，在呈现的设置界面中，需要设置得都比较简单
将Zabbix的这些逻辑反推回去创建修复的SQL。
最后看如何使用Node架构。当设置完整套Node 架构后，在前端的右上角，有一个下拉列
接下来看Child Node（子节点）的设置。同样的，从“Administration”→“DM”中进人，
首先要设置架构中的 Master Node，即父节点，单击“Local Node”按钮可以看到当前节点
一切设置完以后，启动Master Node的zabbix_server 就可以了。
建议各位在操作的时候，先备份一下数据库，以防万一。
现在大家了解了整个创建Node的过程，应该知道为什么命令只能跑一次了吧。因为如果
先在Master节点上进行设置，从“Administration”→“DM”中进人，在右上角的下拉框
接下来我们看看在设置完Node后的操作。
---
## Page 242
可以理解一下Zabbix Server启动的各种各样的进程的作用，以及工作的流程。
14.1Zabbix内部运行机制
大家一定要合理运用起来。
于不熟悉Zabbix的人来说。
它的一个大“坑”就是性能。当Zabbix规模增长以后，Zabix的性能会变得糟糕，特别是对
“知己知彼，百战百胜”，在调优Zabbix之前，先看看Zabbix内部的运行机制图（图14-1），
要做调优，必须有数据来支撑，Zabbix本身提供了很多监控项来监控Zabbix本身性能数据，
 Triggers 太复杂。
数据量太大了，vps太高，Zabbix来不及处理。
总的来说，Zabbix变慢的原因有如下4个。
Zabbix虽然是一个优秀的开源软件，但是所有的开源软件都有“坑"，Zabbix也不例外。
◎前端用户太多，查询过多的数据。
Housekeeper设置不当，数据库体积变大。
Zabbix系统优化
第14章
·227°
---
## Page 243
●228°
group by delay order by2 desc;
们先来看PPTV的一个interval分布：
interval调整得非常小，都在几秒左右，从而 VPS（value per second，
14.2
问题的时候，可以先看看是否是这些进程的数量变少了。
Zabbix监控系统深度实践
2400
300
86400
DELAY
SQL>select delay;
有很多朋友问过我：“Zabbix性能差怎么办？”而且，我发现他们的Zabbix 中Item的
简单地说，Poller 和Trapper抓取或者接受数据，Syncer将数据写人数据库。所以，当出现
7200
600
Items过多造成性能下降
GUI
Data
62286
79051
90168
103224
119489
COUNT（*）
ConfSyncer
HistorySyncer
HistorySyncer
Escalator
Alert
Other
9.41%
11.94%
13.62%
15.59%
18.04%
PERCENT
round（count（*）/（select count（*） from
图14-1
Zabbix
Trapper
Poller
Poller
N
Z
Data
Data
，每秒数据量）非常高。我
---
## Page 244
首先找到 Reports-Status of Zabbix，如图14-2所示。
report1.php:$reportWidget->addItem(make_status_of_zbx()) ;
include/blocks.inc.php:function make_status_of_zbx (）(
dashboard.php:
然后去找make_status_of_zbx这个方法：
$reportWidget->addItem(make_status_of_zbx());
进入后，
在这里看一下 VPS是如何计算的，就知道Item的 interval 对于 Zabbix系统的影响有多大。
1800
6000
150
120960
900
172800
180
51840
60
3600
240
120
1200
Number of users(online)
Numberof triggers(enabled/disabled)[problem/ok]
Number of items (monitored/disabled/not supported)
Number ofhosts(monitored/notmonitored/templates)
Zabbix server is running
Parameter
，发现PHP文件叫做reportl.php，打开后发现了以下代码
5
2942
3309
3344
4125
8026
15251
19412
25578
34252
37741
53981
.62%
1.21%
2.3%
2.93%
3.86%
5.17%
5.7%
8.15%
.44%
.5%
.5%
图14-2
121
3
Yes
Value
$stszbx = make_status_of_zbx();
65/0[1/64]
113/0/8
11/0/41
localhost:10051
Details
第14章Zabbix系统优化
·229°
---
## Page 245
·230·
如从图14-3所示的情况中，就能看出是SNMPv1agent造成的问题。
如果要精确一些，可以通过看Zabbix对于本身队列的监控，来确定是什么 Item 造成慢的。比
这时候我们清楚了，VPS这个数值，反映了delay的分布情况。
status （); function clear_messages () is called when fsockopen () fails.
second'), $status['aps_total'],
于这个方法的设置，一定是在include/blocks.inc.php 中。打开这个文件找到生成VPS 的地方：
of_zbx，其中reportl.php 可以排除，而 dashboard.php 则是前端另一个显示 VPS 的地方，所以对
Zabbix监控系统深度实践
找到问题了，
其中最关键的就是 SUM(1.0/i.delay)，delay就是 Item每两次从Agent 获取数据的时间间隔。
include/func.inc.php:function get_status ()
$grep-r‘get_status'
继续寻找 get_status 方法的定义：
$status =get_status();
$table->addRow(array(_('Required server performance,new values per
发现了对于 get_status 的定义是在 include/func.inc.php中，找到对 qps 的定义：
include/classes/class.cserverinfo.php:
可以发现，VPS这个数据是从Ystatus 中获取的，Ystatus是怎么获取的，找到如下代码：
可以看到在 dashboard.php、blocks.inc.php 和reportl.php这三个文件中出现了make_status_
Sstatus['aps_total'] = round ($row['gps'], 2);
））：
$row =DBfetch（DBselect（
，应该怎么解决呢？方法就是调整Items 的监控间隔，这是非常简单有效的。
'SELECT SUM(1.0/i.delay)AS qps'.
' AND i.delay<>0'
WHERE i.status='.ITEM_STATUS_ACTIVE.
FROM items i,hosts h'.
AND i.hostid=h.hostid'.
AND h.Status='.HOST_STATUS_MONITORED.
（（
---
## Page 246
样会减轻Zabbix Server 的压力，将这些压力分担到Proxy上去，如图 14-4 所示。
个行之有效的方法。
如果上面的方法都没有效果了，那就只有一个办法了—拆分架构，使用分布式架构。这
在这种分布式架构中，性能瓶颈可能会出现在数据库中。
但很多情况下，
Calulated
TELNETagent
SSH agent
IPMIagent
Database monitor
External check
Zabbix aggregale
Zabbikinternal
SNMPv3agent
SNMPVZagent
SNMPv1agent
Simple check