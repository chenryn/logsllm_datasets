(cid:5)(cid:2)
(cid:5)(cid:2)
(cid:5)(cid:2)
(cid:6)(cid:2) (cid:7)(cid:8)
(cid:6)(cid:2) (cid:7)
(cid:3)
(cid:6)(cid:2) (cid:7)(cid:9)
(cid:6)(cid:2) (cid:7)(cid:2)
Fig. 2: Example of 4 stripes with nstripe = 4 blocks per stripe
and a single syndrome.
real-time, at time t, ˆδsystem, we observe the resources used
in the last time step r(t − 1), and make a prediction for
r(t), ˆr(t) = r(t − 1) + ˆδsystem. We store the predictions
for further comparison with the real observations, and re-train
the Markov model and update the cluster centers using the
last observation. This update mechanism gives our system
the ability to cope with possible behavior changes of users,
improving the intelligence of our S2DDC.
We evaluate the quality of our predictions by measuring
the amount of space available for additional syndromes as a
measure of total proportion of the allocated space covered by
additional syndromes at time t, s(t), and the under-prediction
rate at time t, up(t). The under-prediction rate is deﬁned as
the proportion of predictions for which ˆr(t) < r(t), i.e. when
we predicted that we would need less space than actually was
needed. The available resources for additional reliability, ρ(t)
is based on the total resources in our pool rmax, and this
prediction, ρ(t) = rmax − ˆr(t). We calculate s(t) by taking
the total resources allocated in the previous step r(t − 1),
discretized into logical blocks of size b, r(t−1)/b. The number
of unused blocks in the system is similarly ρ(t)/b. Given a
total number of blocks per stripe of nstripe, each additional
syndrome will provide coverage for nstripe currently allocated
blocks, giving
s(t) =
ρ(t) · nstripe
r(t − 1)
.
D. Dynamic Syndrome Allocation
Upon obtaining an estimate of the overprovisioned space
available for additional reliability, given by the value of
ρ, we implement our reliability improvements by allocating
additional
independent syndromes for data in the system.
Syndrome allocation is a common technique for improving
reliability, and is the basic building block of the RAID-
family of techniques. Blocks of data are arranged in stripes.
Figure 2 shows such an arrangement with a single independent
syndrome allocated, and nstripe = 4. A single independent
syndrome can easily be created by computing the bit-wise
XOR parity of these blocks as sa = d0 ⊕ d1 ⊕ d2 ⊕ d3 [26]. A
second syndrome can also be computed for this stripe allowing
recovery from an additional failure, using Galois ﬁelds [27].
Additional syndromes for these stripe conﬁgurations can be
allocated using techniques such as error-correcting codes, but
Fig. 1: Example of a trained Markov model of user 39, using
mean-shift with h = 5.
We estimate the state occupancy probability vector for user j
during the next time step t + 1 as:
(cid:2)πt+1,j = (cid:2)πt,j · Pj,
where Pj
is the transition matrix for the Markov model
representing user j. The estimated I/O for that user can then
be derived given the construction of a reward vector (cid:2)x. We
construct (cid:2)x as a vector of rewards for each possible state in (cid:2)π
for any user such that (cid:2)x contains the expected I/O transactions
performed by a user in each possible next state. We propose
two means of generating (cid:2)x:
1)
2)
Populate (cid:2)x with the mean I/O operation for each
cluster corresponding to a given state.
Populate (cid:2)x with the q-worst case scenario for the
cluster, which is found by ﬁrst sorting all values
associated with a cluster, and choosing the value that
represents the qth quartile of a cluster, allowing us to
parametrize (cid:2)x based on risk aversion, e.g. selecting
q = 0.99, will choose the value for each cluster which
is larger than 99% of all other values in the cluster.
We then calculate the estimate for a user’s next state I/O
operation as
δj = (cid:2)πt,j · Pj · (cid:2)x.
The estimate for the system is then
n(cid:5)
ˆδsystem =
(cid:2)πt,j · Pj · (cid:2)x.
j=0
Given our prediction of the change in system resources
expected over the next time step, we make observations in
222222
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:49:20 UTC from IEEE Xplore.  Restrictions apply. 
beyond two syndrome schemes, generally have a severe impact
on performance, and as a result lower the QoS of our system
[28]. Additional independent syndromes can be allocated using
XOR parity and Galois ﬁeld XOR parity by forming new stripe
geometries, such as those shown in Figures 3a and 3b. Using
these striping geometries, we can allocate as much as 500%
additional coverage, or s(t) = 5.0. Higher levels of syndrome
coverage can be obtained by forming similar independent
geometries, with the caveat that each new geometry must be
independent of all previous geometries involving a given strip
within a stripe.
These new syndromes are created out of the overprovi-
sioned space indicated by the result of our predictions de-
scribed in Section III-C as ρ, however this space is fragmented
across the entire data center. Given a stripe geometry, we can
allocate a virtual syndrome for this stripe by utilizing blocks in
the overprovisioned space which are independent of the disks
used to store the current stripe, as shown in Figure 3c.
We allocate additional coverage for the system using s(t) as
a guideline, and separating s(t) into a whole and decimal part.
Every stripe in the system will gain a number of additional
syndromes equal to the whole part of s(t), and a fraction of
the stripes in the system, equal to the fractional part of s(t) will
gain an additional syndrome, i.e. if s(t) = 2.37, then all stripes
will receive at least two additional syndromes, and 37% will
receive 3 syndromes. It should be noted that each additional
syndrome has a large impact on reliability, often increasing
the reliability by an order of magnitude or more. The failure
rates of adding new independent stripe geometries is a more
complex subject that is beyond the scope of this study [29].
E. Modeling The Reliability of A System
In order to properly evaluate our system, and the effect
of our machine learning techniques on reliability, we generate
a general system model that includes multiple faults that can
overlap and cause the failure of a RAID system, and use the
statistics we obtain from the model, measure the reliability
of our system under various conﬁgurations. We create a
simulation using the M¨obius tool detailed in [30], and perform
tests to compare the reliability of our S2DDC.
We adopt the expected number of blocks lost annually
in a system as a measure of reliability, as a substitute for
MTTF which has come under considerable criticism [12],
[31] and does fully capture the capabilities of our system,
which can apply additional syndromes not only to a whole
disk, but also to a fraction of blocks on a disk. There are
several types of failures that can result with loss of blocks
in a system, causing system failures, or whole disk losses.
One such failure type is whole disk failures (WDF), where
one or more disks completely fail. There are a few studies in
the literature which provide actual statistical properties, or real
observations obtained from real systems, which isolate whole
disk failure from node or RAID failure. In one study proposed
by Schroeder [12], the authors ﬁt a Weibull distribution over
the time between occurrence of node failures. The dataset they
use is publicly available [32] and includes the start and end
dates and times of node failures as well as how long it took
to replace the failed nodes in terms of minutes. Unfortunately
their analysis can only be used to talk about the failure of
Fig. 4: Markov Model constructed to obtain steady-state disk
failure probabilities based on age.
TABLE I: Probability of disk failure
Ages (year)
0
AFR
P (age)
0.01705
0.223
1
0.07984
0.219
2
0.08605
0.201
3
0.05922
0.184
4
0.07333
0.173
disks in a population of the size they studied, and not that of
a single disk. As we need the failure rate of each independent
disk, we cannot utilize their parameterized Weibull distribution
to model the disk failures in our system.
Another study proposed by Pinheiro et al. [33] provides the
annualized disk failure rates (AFR) by age groups where they
use disk failure characteristics obtained over Google servers.
By analyzing their data, we obtained the rates presented
in Table I. These rates specify the probability of a single
disk failure during a given year, conditioned upon its age,
P (single disk failure|age). In order to obtain the probability of
the failure of a single disk without the condition of age, we use
a simple Markov model to the ﬁnd the survival rates of disks
based on their age, as shown on Table I. Age is split into eras,
based on the data from [33], and represents a one-year time
window. We construct the Markov model in Figure 4, where
each state represents the current age of the disk and P (n)
represent the probability of surviving that age, that is found
by 1− AF R(n) for age n. We assume a ﬁve year replacement
policy for all disks meaning that the state representing age 4
has a probability of survival of 0.0. With the Markov model,
we derive the steady state probabilities of a single disk to be
at a given age as shown on Table I. Using the formula
P (single disk failure|age) ∗ P (age)
we obtain P (single disk failure) and determine the annual
probability of a single disk failure as 0.06279, and use this
probability to model the WDFs in our simulations. We ac-
knowledge that, as stated in [12] that a Weibull or Gamma
would provide a better ﬁt, but we are not been able to
derive distributions of single disk failures due to the lack of
appropriate data in the literature.
In addition to modeling disk failures, we also wish to model
the time it takes to replace or repair a disk. In their study [34],
Schroeder et al. show that node replacements follow a log-
normal distribution. We experiment with their publicly avail-
able dataset and conﬁrm the best ﬁtting distribution as the log-
normal with highest log-likelihood where the shape parameter
is 1.597 and log-scale is 4.085. Unfortunately, as with failure
data, we ﬁnd that repair data only covers the collective repair of
an entire node, and not single disk repair/replacement. Instead
223223
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:49:20 UTC from IEEE Xplore.  Restrictions apply. 
(cid:1)
(a) Example of independent syndrome cal-
culation as the XOR parity of columns.
(b) Example of independent syndrome cal-
culation as the XOR parity of diagonals.
(c) Identiﬁcation of blocks independent of
the disks which store the current stripe, and
thus can be used to form a new independent
syndrome.
Fig. 3: Examples of different syndrome allocation techniques.
we use a uniform distribution, drawing from the observation
in [34] in which Schroeder et al. state that the mean repair
time of a disk varies between 4 to 6 hours.
Another failure type that has the potential to affect our
expected annual rate of block loss is latent sector errors
(LSEs). The occurrence of an LSE during a period of RAID
degredation can lead to a partial system failure [35]. One study
[36] provides the probability of nearline disks in a system
developing at least one LSE as 0.085. LSEs are detected and
repaired by a scrubbing process that is performed with the
period of once in 2-weeks and is assumed to detect and repair
all LSEs. In order to model the number of LSEs occuring in
a given system, we use the Pareto distribution provided by
Schroeder et al. [37] with α = 0.63 to give the number of
LSEs for 2-week period, utilizing the parameterization for the
E-1 group of nearline drives.
Using these models of the underlying process, we model
our entire system using the Stochastic Activity Networks [38]
shown in Figures 5a and 5b composing these models using
the Rep/Join formalism [39] and the model shown in Figure
5c. We solve this model for expected block loss using M¨obius
[30] version 2.5.
IV. EXPERIMENTS AND RESULTS
In this study, we perform experiments using the methods
we have proposed over a dataset obtained from the Illinois
Natural History Survey (INHS). The dataset contains infor-
mation about write and delete operations from almost 500
users, snapshotted two or three times ever day during the
morning, afternoon and evening. Data collection was initiated
on 04/22/2010 in the morning period and ended at 04/03/2012
in the evening. The data represents users from two separate
systems utilizing ZFS (originally dubbed the “Zettabyte File
System”). We present the results for a single system in this
paper for clarity and space reasons, choosing the System-S
data used in [9], which was representative of both systems. Ad-
ditional data from System-R is made available online through
our research website 1.
1http://dataengineering.org/research/SSDDC/
Mean-Shift parameter h =
Number of clusters
1
33
2
18
3
11
4
9
5
8
6
6
7
6
8
5
9
5
10
5
TABLE II: Number of clusters obtained by mean-shift with
different window sizes.
A. Performance of Predictive Analytics
In order to evaluate our methods for predictive analytics,
we compare the results of our methods for an S2DDC with
a baseline statistical model which does not consider actions
taken by a user to be dependent on prior actions.
At any given time step t our statistical model predicts
the disk usage of the next time step t + 1 by calculating
either the mean change in disk usage for each user, based
on all previously observed disk transactions, or the qth worst
case scenario by sorting all previous observed transactions,
and choosing a value such that q% of all observed actions
are less than or equal to that value. We show the predictive
performance obtained by the statistical model on test data
in terms of cumulative density function (cdf) of additional
syndromes we can allocate as in Figure 6. As the result shows,
the number of additional syndromes are similar to our Markov
model methods for quartile of 95%, but rapidly decay for more
risk averse parameterization of q.
To analyze the affect of centroid determination on
our predictive model we conduct experiments with mean-
the System-S dataset with h =
shift clustering over
{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}. As mean-shift determines the num-
ber of clusters without user input, we obtain the number of
clusters for each window parameter h value as in Table II.
We then run the same experiment using k-means clustering as
our classiﬁer, using the same numbers of clusters identiﬁed
in our mean-shift experiments. Repeated trials of our k-means
algorithm were run to determine the mean performance, given
the stochastic nature of the algorithm.
We also experiment with the two versions of our reward
vector, (cid:2)x, to see the inﬂuence it has over our measures. We
perform predictions using both mean centroid values, and
quartiles representing the qth percentile worst case (largest
224224
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:49:20 UTC from IEEE Xplore.  Restrictions apply. 
(a) Stochastic Activity Model of Failure and Re-
pair of a Single Disk.
(b) RAID System Failure Model.
(c) Rep/Join Compositional Model.
Fig. 5: Mobius model for simulating the failure characteristics and measure the system reliability.
Mean Centroids
q = 95.0%
q = 99.0%
q = 99.9%
h=1
0.244
0.175
0.166
0.165
h=2
0.239
0.147
0.130
0.110
h=3
0.293
0.046
0.041
0.038
h=4
0.303
0.055
0.009
0.006
h=5
0.238
0.064
0.035
0.026
h=6
0.801
0.109
0.008
0.002
h=7
0.787
0.109
0.008
0.002
h=8
0.809
0.096
0.008
0.001
h=9
0.775
0.103
0.009
0.001