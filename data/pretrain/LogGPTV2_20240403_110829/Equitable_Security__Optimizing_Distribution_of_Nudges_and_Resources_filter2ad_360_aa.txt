title:Equitable Security: Optimizing Distribution of Nudges and Resources
author:Elissa M. Redmiles and
John P. Dickerson and
Krishna P. Gummadi and
Michelle L. Mazurek
POSTER: Equitable Security: Optimizing Distribution of Nudges
and Resources
Elissa M. Redmiles, John P. Dickerson, Krishna P. Gummadi, and Michelle L. Mazurek
PI:EMAIL
ABSTRACT
Security behaviors can help users avoid incidents, but can also
increase costs, both to users – in time and mental effort – and to
platforms – in user engagement and engineering resources. As
such, we should consider when it is most efficient and effective to
encourage security behaviors. Recent work has shown that users
attempt to make security decisions based on cost benefit tradeoffs
(boundedly, rationally). Yet, sometimes security nudges (e.g., cre-
ate unique passwords for every website) encourage users toward
irrational behavior: creating strong, unique passwords even for
those sites that contain no personal data. In this work-in-progress,
we present a mechanism design (a framework) that can be used
to optimize the distribution of security nudges and requirements
among users with different levels of risk or different levels of in-
vestment in a given system. Further, we introduce a new paradigm:
the distribution of resources (e.g., ubikeys) that can lower the cost
of security behaviors to those users with the most need (the high-
est time cost from 2FA or lowest Internet skill). Future work will
involve simulations showing the value of optimizing distribution
of nudges and resources using this framework, and evaluating such
an approach in a live test.
ACM Reference Format:
Elissa M. Redmiles, John P. Dickerson, Krishna P. Gummadi, and Michelle
L. Mazurek, PI:EMAIL . 2018. POSTER: Equitable Security:
Optimizing Distribution of Nudges and Resources. In 2018 ACM SIGSAC
Conference on Computer and Communications Security (CCS ’18), October
15–19, 2018, Toronto, ON, Canada. ACM, New York, NY, USA, 3 pages. https:
//doi.org/10.1145/3243734.3278507
1 INTRODUCTION
Digital security requirements or nudges are often established with
little consideration of unique differences among end users. The
same is true in other domains related to risk: insurance companies
and doctors recommend that obese patients exercise copiously
without accounting for the cost – in time or gym fees – of that
recommendation to the patient, nor how that effort may correlate –
or not – with the patient’s interest in their own health. Similarly
in security, online websites require or forcefully recommend that
users engage in security mechanisms such as long, strong, and
complex passwords or enabling two-factor authentication without
accounting for the effort and time cost of those requirements in
relationship to the user’s investment or valuation of their online
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CCS ’18, October 15–19, 2018, Toronto, ON, Canada
© 2018 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-5693-0/18/10.
https://doi.org/10.1145/3243734.3278507
account on that system – and the risk that account may or may not
pose to other accounts; or the effort they are already investing in a
multitude of similar systems.
Even those entities that do not stand to gain from recommending
security such as the U.S. National Institute of Standards and Tech-
nology [1] and Teen Vogue [3] recommend that everyone adopt
two-factor (or multi-factor) authentication for the sites that offer
it, again with little mention of variable costs and difficulty to users
with differing skill levels, numbers of online accounts, and etc.
Behavioral mechanism design traditionally enables the balancing
of user utility (e.g., what value a user generates from an account
or from protecting it) with firm utilities (e.g., the value an online
site gets from having a user enable a security behavior or use their
system) within constraints (e.g., cyberinsurance or governmental
policies). While such approaches have been used to solve a diverse
set of problems, to our knowledge behavioral mechanism design has
never been applied to end-user security. In this work-in-progress,
we define a general behavioral mechanism for balancing user and
firm utility in systems with inherent risk and protective behaviors
that must be adopted by users with the goal of understanding (1)
how firms or systems communicate with users about the value
of protective behavior in order to encourage adoption, (2) how
government policies should be set to ensure that firms behave fairly
toward users, where fairness is defined as reducing the risk variance
between different users until a minimum level of safety is met and
reducing the effort variance between users with different resources
(e.g., not marginalizing groups of users) and (3) how resources can
be distributed among users to minimize inequities between people
with different Internet skill or ability.
2 MECHANISM
In our prior work [4] we constructed an online experimental system
in which crowdworkers made a security choice – enabling two-
factor authentication (2FA) or not – given an explicit set of risks (a
percent chance that their study account would be hacked and they
would not be compensated, and a percent protection from hacking
they would receive from enabling 2FA). We measured the cost of the
security behavior to the crowdworker in terms of the time it took
them to log in and sign up; since crowdworkers earn money from
completing micro tasks, seconds or minutes wasted in our game
lead to direct wage losses. Using these measurements, we model
users’ security decisions as a function of costs (C), risks (R), and
user tendencies and attributes (U) and find that (1) we were able to
model security decisions with high accuracy (R2=0.61) and (2) this
model of behavior is robust across users of different demographics,
skill, and security tendencies (i.e., password strengths). Finding that
overall, user behavior relates to: a) costs (e.g., time it takes to login
to a system or enable 2FA) and b) prior behaviors, but c) can be
adjusted through messages communicating risk and efficacy.
Poster PresentationCCS’18, October 15-19, 2018, Toronto, ON, Canada2270CCS ’18, October 15–19, 2018, Toronto, ON, Canada
Elissa M. Redmiles, John P. Dickerson, Krishna P. Gummadi, and Michelle L. Mazurek
PI:EMAIL
influences the cost of protective behaviors for the user and risk
Ss that is equal across users – this risk is lower than the risk of
the world (improvements are made by the firm investing resources,
but no system can be 0% risky). There is also a set of protective
behavior(s) that users can enable within the system B to improve
reduce their personal risk. These behaviors have some quality Bq
(e.g., how much they cost the user and the firm) and protection
level Bs (e.g., how risk reduction the behavior offers). Finally, the
system also has a parameter that can be varied per user, in which
they can allocate some resources to certain users (R) to improve Bq
(reduce user cost). Thus, the system’s private parameters consist of:
Sq, Ss, Bq, Bs, R, where the parameters can be increased by some
addition of resources by the firm (up to a threshold).
Figure 1: Overall mechanism design.
Here, we generalize this approach to design a mechanism (frame-
work) for mathematically selecting the values of different system
features can be used to maximize utility for both users and online
services (Figure 1).
2.1 General Mechanism
People (users) use systems that offer them some value (e.g., buy into
insurance systems that lower their healthcare costs, store money in
bank accounts that offer them some interest). The world has some
inherent risks (Wr ) that will cause loses for these users. System
owners (firms) sustain losses when the users sustain losses (e.g.,
firms sustain losses proportional to user loss; user loss is dependent
on world risk, the user’s type, and the user’s system-relevant behav-
ior). Firms attempt to reduce user risk (and thus the firm’s losses)
by making protective behaviors available to the user, the firm can
invest to make these protective behaviors more or less valuable,
and can also communicate true (or false) information to the user in
order to get them to enact protective behaviors. These protective
behaviors reduce the user’s risk and thus the firm’s risks and costs.
The behaviors also, however, cost the user (in effort, time, or even
money) and may cost different users different amounts (e.g., cost of
protective behavior is dependent on user type). The behaviors may
in some cases also have a cost to the firm (e.g., the price to send a
SMS message to each user that enables two factor authentication).
In sum, firm’s build their systems to protect users from world
risks inherently, but can only do so up to a certain point. To gain
additional protection, users must adopt protective behaviors. In
our formulation, we account for the costs and benefits to users
from adopting a protective behavior and we account for the costs
and benefits to a firm that hosts some digital system from the
behaviors chosen by users of that system. In our mechanism we
consider systems in which there are a set of n possible system