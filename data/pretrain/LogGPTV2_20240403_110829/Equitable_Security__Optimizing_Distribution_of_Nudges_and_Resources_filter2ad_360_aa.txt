# Title: Equitable Security: Optimizing the Distribution of Nudges and Resources

## Authors
Elissa M. Redmiles, John P. Dickerson, Krishna P. Gummadi, and Michelle L. Mazurek

## Abstract
Security behaviors can help users avoid incidents but also increase costs for both users (in terms of time and mental effort) and platforms (in user engagement and engineering resources). Therefore, it is crucial to determine when it is most efficient and effective to encourage security behaviors. Recent research has shown that users often make security decisions based on cost-benefit trade-offs, though sometimes security nudges (e.g., creating unique passwords for every website) can lead to irrational behavior, such as creating strong, unique passwords even for sites with no personal data. In this work-in-progress, we present a mechanism design framework to optimize the distribution of security nudges and requirements among users with varying levels of risk or investment in a given system. Additionally, we introduce a new paradigm: the distribution of resources (e.g., UbiKeys) to lower the cost of security behaviors for users who need them most (those with the highest time cost from 2FA or the lowest Internet skills). Future work will include simulations to demonstrate the value of optimizing the distribution of nudges and resources using this framework, and evaluating the approach in a live test.

## ACM Reference Format
Redmiles, E. M., Dickerson, J. P., Gummadi, K. P., & Mazurek, M. L. (2018). POSTER: Equitable Security: Optimizing Distribution of Nudges and Resources. In *2018 ACM SIGSAC Conference on Computer and Communications Security (CCS '18)*, October 15–19, 2018, Toronto, ON, Canada. ACM, New York, NY, USA, 3 pages. https://doi.org/10.1145/3243734.3278507

## 1. Introduction
Digital security requirements or nudges are often implemented without considering the unique differences among end users. This issue is not limited to security; for example, insurance companies and doctors may recommend that obese patients exercise extensively without accounting for the associated costs (time and gym fees) or the patient's interest in their health. Similarly, online websites require or strongly recommend that users adopt security mechanisms like long, strong, and complex passwords or enable two-factor authentication (2FA), without considering the effort and time required and the user's investment in their online account.

Even entities that do not benefit directly from recommending security, such as the U.S. National Institute of Standards and Technology [1] and Teen Vogue [3], advise everyone to use 2FA for sites that offer it, with little consideration of the variable costs and difficulties for users with different skill levels and numbers of online accounts.

Behavioral mechanism design traditionally balances user utility (e.g., the value a user derives from an account or its protection) with firm utilities (e.g., the value an online site gains from users enabling a security behavior or using their system) within constraints (e.g., cyberinsurance or governmental policies). While such approaches have been used to solve various problems, to our knowledge, behavioral mechanism design has not been applied to end-user security. In this work-in-progress, we define a general behavioral mechanism to balance user and firm utility in systems with inherent risks and protective behaviors. Our goal is to understand:
1. How firms or systems can communicate the value of protective behavior to encourage adoption.
2. How government policies should be set to ensure fair treatment of users, where fairness is defined as reducing risk variance between different users until a minimum level of safety is met and reducing effort variance between users with different resources (e.g., not marginalizing groups of users).
3. How resources can be distributed among users to minimize inequities between people with different Internet skills or abilities.

## 2. Mechanism
In our prior work [4], we constructed an online experimental system where crowdworkers made a security choice—enabling 2FA or not—given an explicit set of risks (a percent chance that their study account would be hacked and they would not be compensated, and a percent protection from hacking they would receive from enabling 2FA). We measured the cost of the security behavior in terms of the time it took to log in and sign up, as seconds or minutes wasted in our game led to direct wage losses. Using these measurements, we modeled users' security decisions as a function of costs (C), risks (R), and user tendencies and attributes (U). We found that:
1. We could model security decisions with high accuracy (R² = 0.61).
2. This model of behavior is robust across users of different demographics, skills, and security tendencies (e.g., password strengths).

Overall, user behavior relates to:
- Costs (e.g., time to log in to a system or enable 2FA).
- Prior behaviors.
- Messages communicating risk and efficacy, which can adjust behavior.

### 2.1 General Mechanism
People (users) use systems that offer them some value (e.g., insurance systems that lower healthcare costs, bank accounts that provide interest). The world has inherent risks (Wr) that cause losses for these users. System owners (firms) sustain losses when users sustain losses (e.g., proportional to user loss, which depends on world risk, the user’s type, and the user’s system-relevant behavior). Firms attempt to reduce user risk (and thus their own losses) by making protective behaviors available to users. They can invest to make these behaviors more or less valuable and can communicate true (or false) information to encourage protective behaviors. These behaviors reduce the user’s risk and, consequently, the firm’s risks and costs. However, they also cost the user (in effort, time, or money) and may cost different users different amounts (e.g., dependent on user type). In some cases, these behaviors also have a cost to the firm (e.g., the price to send an SMS message for 2FA).

In summary, firms build their systems to protect users from inherent world risks, but only up to a certain point. For additional protection, users must adopt protective behaviors. In our formulation, we account for the costs and benefits to users from adopting a protective behavior and the costs and benefits to a firm hosting a digital system from the behaviors chosen by its users. Our mechanism considers systems with a set of n possible system features to maximize utility for both users and online services (Figure 1).

![Overall Mechanism Design](figure1.png)

## Poster Presentation
**CCS '18, October 15-19, 2018, Toronto, ON, Canada**

Elissa M. Redmiles, John P. Dickerson, Krishna P. Gummadi, and Michelle L. Mazurek
PI:EMAIL