NVIDIA Mellanox ConnectX-4 [35] 100
2
12.28.2006 mlx5 core 5.2–1.0.4
Name
Version
ConnectX-5 [36]
BlueField [34]
ConnectX-6 [37] 200
16.29.1016
18.29.1016
20.29.1016
All NICs except for the NVIDIA Mellanox ConnectX-6 use a PCIe 3.0 x16 bus
to connect with a server’s CPU. The ConnectX-6 adapter uses two PCIe 3.0 x16
slots. The BlueField NIC is a SmartNIC based on the ConnectX-5 adapter, also
equipped with a 16-core ARM processor for additional in-NIC traﬃc process-
ing. We brieﬂy describe the general architecture and diﬀerences of the NVIDIA
Mellanox NICs. All NICs have a ﬁrst table, called Table 0 or “root” table with
space for 65 536 rule entries. All the NICs, except for the ConnectX-4, provide an
additional sequence of high-performance exact-match tables (supporting a per-
table mask) that can be used to massively oﬄoad packet classiﬁcation from the
CPUs to the NIC. Note that these NICs do not support Longest Preﬁx Match
(LPM); instead the user should implement LPM with a combination of multiple
tables with diﬀerent masks. The capacity of these tables is only bounded by the
host’s available memory, thus they can accommodate a much larger number of
rules, given the ample amount of RAM in modern servers. We refer to the ﬁrst
of those extra tables as Table 1 and note that any subsequent table (i.e., Table
2, 3, etc..) appears to have similar properties with Table 1.
Traﬃc Characteristics. A multi-core traﬃc generator and receiver, based on
the Data Plane Development Kit (DPDK) v20.11 [46], is deployed on the mea-
surement server as shown in Fig. 1. Four cores are allocated to the traﬃc generator,
What You Need to Know About (Smart) Network Interface Cards
323
which inject a trace of 10K UDP ﬂows at 100 Gbps. Each ﬂow consists of MTU-sized
(i.e., 1500-bytes) packets. This traﬃc ﬁrst traverses the DUT and, if not dropped,
then returns to the measurement server, this time reaching four diﬀerent cores on
the same CPU socket of the traﬃc generator.
Note that the measurement server injects traﬃc towards the DUT using the
same 100 GbE ConnectX-5 NIC for all the experiments. This ensures that only
the DUT’s NIC hardware may vary across all of the experiments, thus potential
diﬀerences among the experimental results solely depend on the performance of
the underlying NIC in the DUT.
Measurements. Each experiment is executed 5 times; the collected measure-
ments are plotted using either errorbars or boxplots, which visualize the 1st,
25th, 50th (i.e., median), 75th, and 99th values obtained across these 5 itera-
tions, unless stated otherwise. The traﬃc receiver of the measurement server
reports measurements related to end-to-end throughput, latency variance per-
centiles, per-queue packet & byte counters both at the measurement server and
the DUT, packet loss, and the duration of each experiment. When reporting
latency, we repeated experiments at 5Mpps (∼60 Gbps), avoiding link speed to
be a bottleneck on both the DUT and the traﬃc generator, thus ensuring latency
changes are due to the NIC and not packets buﬀering in queues.
3 Analysis of Flow Tables
This section benchmarks the selected NICs focusing on three diﬀerent aspects
related to packet classiﬁers.
First, we quantify the performance impact of the NICs’ hardware classiﬁers
with (i) an increasing number of rules, (ii) an increasing number of tables hosting
these rules, and (iii) increasingly larger or more frequent updates being installed
by the control plane (see Sect. 3.1). Second, we analyse the performance of ﬂow
rule insertion/deletion operations in terms of latency for rule insertions and
throughput (see Sect. 3.2.1). Finally, after discovering ﬂow rule modiﬁcations are
not supported by these NICs, we evaluate a diﬀerent strategy to realize fast and
atomic rule updates in the packet classiﬁer of the analyzed NICs (see Sect. 3.2.2).
3.1 Hardware Classiﬁcation Performance
Overview. In this section we measure packet classiﬁcation performance of mod-
ern NICs under a variety of conditions. First, we show that the ﬁrst table of these
NICs drops almost all traﬃc when memory utilization exceeds ∼85%. We also
show that the packet processing latency of the analyzed NICs exhibits a long tail
in this situation (up to 120 ms). Moreover, spreading an increasing number of
rules across four or more tables in these NICs results in substantial throughput
degradation (23–88% when using 4–16 tables). Finally, we show that runtime
modiﬁcations to the packet classiﬁer’s rules have a detrimental eﬀect on the
NIC’s throughput: we observe a reduction of 70 Gbps of throughput (out of
100 Gbps).
324
G. P. Katsikas et al.
Scenario. In the following experiments the DUT runs a single-core forwarding
Network Function (NF) using the testbed described in Sect. 2.1. The NIC of the
DUT dispatches input frames to this NF according to the ﬂow rules installed in
the NIC. These rules are stored either in the default “root” ﬂow table of the NIC
(i.e., Table 0) or in non-root tables (i.e., Tables 1–16). We diﬀerentiate between
these two table categories as NVIDIA Mellanox explicitly mentions that Table 0
has a limited number of supported ﬂow entries (i.e., 216 rules) and the latter
support a faster API based on shared memory between the NIC and the driver
running in userlevel. We only show results for the ConnectX-5 NIC as we observe
qualitatively similar trends for all the other NICs.
The rest of this section provides experimental evidence to address the follow-
ing questions:
Q1 Does the number of rules and/or tables aﬀect the performance
of the NIC?
Figure 2 shows the performance of the packet classiﬁer with an increasing number
of rules (x-axis) for all types of tables of the NVIDIA Mellanox ConnectX-5 NIC.
We denote by Table 1-X the case where we uniformly install forwarding rules
on the ﬁrst X non-root tables, i.e., Table 1, . . . , Table X. The rules installed in
the NIC are simple exact matches and the generated traﬃc matches exactly one
default rule installed in the NIC. We generate 8 Mpps of 1.5 KB packets towards
the DUT, equivalent to 100 Gbps. Figure 2a and Fig. 2c show that the perfor-
mance (i.e., throughput and packet processing latency) for Table 0 decreases
Fig. 2. Throughput and latency (on a logarithmic scale) of a hardware-based 100 GbE
NVIDIA Mellanox ConnectX-5 NIC classiﬁer with diﬀerent number of pre-installed
rules across Table 0 (left) and Tables 1–16 (right).
What You Need to Know About (Smart) Network Interface Cards
325
dramatically as soon as the occupancy of the table goes above 85%, hence the
last 15% of memory is in practice unusable. Speciﬁcally, the throughput decreases
from 100 Gbps down to 20 Mbps, while the latency increases by several orders of
magnitude, from tens of µs to more than a hundred of ms. We observe a similar
decrease in throughput for small packets (i.e., 64B), even when the input load
is 3.5 Gbps, which is 30x lower than the maximum attainable throughput of the
NIC under test. This conﬁrms that the performance degradation issue is not a
result of excessive input load, but rather a design artifact of the root table.
Figure 2b and Fig. 2d show that non-root tables (i.e., Tables 1–16) are much
faster than the root table. Speciﬁcally, using a non-root table the NIC achieves
line-rate throughput and low predictable latency even with 2M entries in Table 1.
However, spreading rules across an increasing number of non-root tables results
in substantial performance degradation. As shown in Fig. 2b, for most of the
tested ruleset sizes, the NIC cannot achieve more than 20 Gbps throughput when
using 16 tables, while the respective latency to access these tables exhibits a ten-
fold increase compared to the single-table case, as shown in Fig. 2d.
Q2 Do updates to the classiﬁer aﬀect the performance of the NIC?
The objective of this experiment is to understand how runtime modiﬁcations of
the packet classiﬁer’s ruleset impact the throughput of the forwarded traﬃc. We
envision two types of experiments motivated by two diﬀerent use cases. In the
ﬁrst experiment, we generate a single batch of rule insertions to be installed into
the NIC. This is reminiscent of scenarios in which a network suddenly reacts to a
failure event that triggers many rule updates. For instance, Internet link failures
may generate a burst of BGP updates for possibly 10 s of 100 s of thousands of IP
preﬁxes received from a neighboring network [11]. In the second experiment, we
generate periodic rule insertions in the packet classiﬁer at a given frequency. This
setting is reminiscent of cloud datacenter Layer 4 load balancers (LBs), where
LBs insert a new rule into a packet classiﬁer each time a new connection arrives.
We note that, based on realistic connection size distributions taken from cloud
datacenter workloads, the number of new rules to be installed ranges between
4K per second for “Hadoop’ workloads to 36K and 338K per second for “cache
follower” and “web server” workloads, respectively [41]. In both experiments, we
generate a workload with packet sizes of 1.5 KB. To avoid external bias from the
system’s CPU, we measure two diﬀerent cases for each experiment: In the ﬁrst
case (labeled as “Same Core” below), we use the same CPU core that performs
traﬃc forwarding to install the rules in the NIC. In the second case (labeled
as “Distinct Cores” below), we use one CPU core for traﬃc forwarding and
another CPU core for rule installation. All the traﬃc matches a single rule in
the classiﬁer. As in the previous experiment, we obtain similar qualitative results
for all the NICs and only show the NVIDIA ConnectX-5 ones.
Batch-Based Updates Have Detrimental Eﬀects on Performance.
Figure 3 shows the packet processing throughput (y-axis) achieved by the NIC’s
packet classiﬁer over time (x-axis) for Tables 0 and 1, while the NIC simulta-
neously (i) receives a workload of 100 Gbps of 1500 B packets and (ii) inserts a
number of new rules (see the legends) ranging between 1 and 100 K.
326
G. P. Katsikas et al.
Fig. 3. Impact of batch-based updates on the performance of a 100 GbE NVIDIA
Mellanox ConnectX-5 NIC classiﬁer.
As shown in Fig. 3a even with a batch of 1K rules (see the green circles), the
NIC fails to process any traﬃc for about 300 ms. For a 100 Gbps link with MTU-
sized frames, this translates to a packet loss of around 2.5 M frames, while more
than 40 M frames could have been lost from a 100 Gbps link with 64 B frames.
Increasing the rules’ batch size to 10K results in a longer failure of around 2 s
(see the red squares), while in the case of 100K rules (blue triangles) the NIC
does not recover even after 6 s. The down-time of Table 1 is 500 ms, but the
problem manifests itself only in the case of 100 K rules as shown in Fig. 3b. On
the other hand, installing the batch updates from a dedicated core does not
aﬀect the forwarding performance of the NF as shown in Fig. 3c and Fig. 3d.
We believe that these results have far-reaching implications on both (i) the
security of the network functions, as batch-based updates could become a vec-
tor of denial-of-service attacks and (ii) the design of highly-reactive network
controllers, e.g., to enable large data-plane updates for fast failover recovery [6].
Rate-Based Updates Reduce NIC Forwarding Capacities. Installing
periodic batches of rules from the same core is a typical operation of NATs
and Layer 4 load balancers, which need to reactively install rules matching new
incoming connections. Installing rules from a diﬀerent core allows us to dissect
just the performance degradation due to interference in the NIC data-plane.
Figure 4a and 4b show the throughput of the forwarding NF when we simul-
taneously insert rules into the NIC classiﬁer at a speciﬁc rate. The insertion rate
ranges between 1 K to 10 K rules per second for Table 0 and 10 K to 500 K rules
per second for Table 1. The inserted rules are not generated in response to a
new incoming connection but pre-computed and inserted regardless of when new
connections arrive. The results show that when inserting rules from a diﬀerent
What You Need to Know About (Smart) Network Interface Cards
327
Fig. 4. Impact of rate-based updates on the performance of a 100 GbE NVIDIA Mel-
lanox ConnectX-5 NIC classiﬁer.
core, the throughput and average latencies (see also Fig. 4c and 4d) are mostly
unaﬀected by the parallel insertion. However, when the insertions are generated
from the same core running the forwarding NF, we observe a signiﬁcant perfor-
mance drop. Speciﬁcally, Fig. 4a and 4b show that the throughput decreases by
roughly 70 Gbps for 10K and 500K rule insertions per second in Table 0 and
Table 1, respectively. As shown in Fig. 4c and 4d, the respective latency increase
is up to more than 2x for Table 0 and 82% for Table 1. This result demonstrates
that the bottleneck of the update operation is the standard API provided by
the NIC vendor for updating the forwarding table (which requires long time and
interrupts the normal forwarding for prolonged period of times).
We note that installing rules from a diﬀerent core is not a panacea. One would
need expensive inter-core communication to install a rule as well as reserve extra
CPU resources to handle the rule installation. For instance, to install i.e., 500K
rules consumes 100% of a CPU core for several hundreds of milliseconds.