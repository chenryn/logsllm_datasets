A. Discovering frequent patterns in sensitive data.
In Proceedings of
the International Conference on
Knowledge Discovery and Data Mining (KDD) (2010),
pp. 503–512.
[7] Dienlin, T., and Trepte, S.
Is the privacy paradox
a relic of the past? an in-depth analysis of privacy atti-
tudes and privacy behaviors. European Journal of Social
Psychology 45, 3 (2015), 285–297.
[8] Duchi, J. C., Jordan, M. I., and Wainwright, M. J.
Local privacy and statistical minimax rates.
In Sym-
posium on Foundations of Computer Science (FOCS)
(2013), pp. 429–438.
[9] Dwork, C. A ﬁrm foundation for private data analysis.
Communications of the ACM 54, 1 (2011), 86–95.
[10] Dwork, C., McSherry, F., Nissim, K., and Smith, A.
Calibrating noise to sensitivity in private data analysis.
In Theory of Cryptography Conference (TCC) (2006),
pp. 265–284.
[11] Dwork, C., and Roth, A. The algorithmic foundations
of diﬀerential privacy. Foundations and Trends in The-
oretical Computer Science 9, 3–4 (2014), 211–407.
[12] Dwork, C., Rothblum, G. N., and Vadhan, S. Boost-
ing and diﬀerential privacy. In Symposium on Founda-
tions of Computer Science (FOCS) (2010), pp. 51–60.
[13] Erlingsson, ´U., Pihur, V., and Korolova, A. RAP-
POR: Randomized aggregatable privacy-preserving or-
dinal response.
In Proceedings of the Conference on
Computer and Communications Security (CCS) (2014),
pp. 1054–1067.
[14] Fan, L., Bonomi, L., Xiong, L., and Sunderam, V.
Monitoring web browsing behavior with diﬀerential pri-
vacy. In Proceedings of the 23rd International Confer-
ence on World Wide Web (WWW) (2014), pp. 177–188.
[15] Fanti, G., Pihur, V., and Erlingsson, ´U. Building
a rappor with the unknown: Privacy-preserving learn-
ing of associations and data dictionaries. Proceedings
on Privacy Enhancing Technologies (PETS), 3 (2016),
41–61.
[16] G¨otz, M., Machanavajjhala, A., Wang, G., Xiao,
X., and Gehrke, J. Publishing search logs–a compara-
tive study of privacy guarantees. IEEE Transactions on
Knowledge and Data Engineering 24, 3 (2012), 520.
[17] Greenberg, A. Apple’s diﬀerential privacy is about col-
lecting your data – but not your data. In Wired (June
13, 2016).
[18] Hsu, J., Gaboardi, M., Haeberlen, A., Khanna, S.,
Narayan, A., Pierce, B. C., and Roth, A. Diﬀerential
privacy: An economic method for choosing epsilon. In
27th IEEE Computer Security Foundations Symposium
(CSF) (2014), pp. 398–410.
[19] Hsu, J., Khanna, S., and Roth, A. Distributed pri-
vate heavy hitters. In International Colloquium on Au-
tomata, Languages, and Programming (ICALP) (2012),
pp. 461–472.
USENIX Association
26th USENIX Security Symposium    761
[20] J¨arvelin, K., and Kek¨al¨ainen, J. Cumulated gain-
based evaluation of ir techniques. Transactions on In-
formation Systems (TOIS) 20, 4 (2002), 422–446.
[37] Tang, J., Korolova, A., Bai, X., Wang, X., and
Wang, X. A white-hat view of Apple’s implementation
of diﬀerential privacy. arXiv preprint (2017).
[21] Kairouz, P., Bonawitz, K., and Ramage, D. Discrete
distribution estimation under local privacy. In Proceed-
ings of the International Conference on Machine Learn-
ing (ICML) (2016), pp. 2436–2444.
[38] Valizadegan, H., Jin, R., Zhang, R., and Mao,
J. Learning to rank by optimizing NDCG measure.
In Advances in Neural Information Processing Systems
(NIPS) (2009), pp. 1883–1891.
[22] Kairouz, P., Oh, S., and Viswanath, P. Extremal
mechanisms for local diﬀerential privacy. In Advances in
Neural Information Processing Systems (NIPS) (2014),
pp. 2879–2887.
[23] Korolova, A. Privacy violations using microtargeted
ads: A case study. Journal of Privacy and Conﬁdential-
ity 3, 1 (2011), 27–49.
[24] Korolova, A. Protecting Privacy when Mining and
Sharing User Data. PhD thesis, Stanford University,
2012.
[25] Korolova, A. Diﬀerential Privacy in iOS 10, Sep
https://twitter.com/korolova/status/
13, 2016.
775801259504734208.
[26] Korolova, A., Kenthapadi, K., Mishra, N., and
Ntoulas, A. Releasing search queries and clicks pri-
vately. In Proceedings of the International Conference
on World Wide Web (WWW) (2009), pp. 171–180.
[27] Li, N., Qardaji, W., Su, D., and Cao, J. Privbasis:
frequent itemset mining with diﬀerential privacy. Pro-
ceedings of the VLDB Endowment 5, 11 (2012), 1340–
1351.
[28] Machanavajjhala, A., Kifer, D., Abowd,
J.,
Gehrke, J., and Vilhuber, L.
Privacy: Theory
meets practice on the map. In Proceedings of the 2008
IEEE 24th International Conference on Data Engineer-
ing (ICDE) (2008), pp. 277–286.
[29] Madden, M., and Rainie, L. Americans’ attitudes
about privacy, security and surveillance. Tech. rep., Pew
Research Center, 2015.
[30] McSherry, F., and Talwar, K. Mechanism design via
In Symposium on Foundations of
diﬀerential privacy.
Computer Science (FOCS) (2007), pp. 94–103.
[31] Meng, X., Xu, Z., Chen, B., and Zhang, Y. Privacy-
preserving query log sharing based on prior n-word ag-
gregation. In IEEE Trustcom/BigDataSE/ISPA (2016),
pp. 722–729.
[32] Merriman, C. Microsoft reminds privacy-concerned
Windows 10 beta testers that they’re volunteers. In The
Inquirer, http: // www. theinquirer. net/ 2374302 (Oct
7, 2014).
[33] Narayanan, A., and Shmatikov, V. Robust de-
anonymization of large sparse datasets. In IEEE Sym-
posium on Security and Privacy (S&P) (2008), pp. 111–
125.
[34] Qin, Z., Yang, Y., Yu, T., Khalil, I., Xiao, X., and
Ren, K. Heavy hitter estimation over set-valued data
with local diﬀerential privacy.
In Proceedings of the
Conference on Computer and Communications Security
(CCS) (2016), pp. 192–203.
[35] Shankland, S. How Google tricks itself to protect
chrome user privacy. In CNET (Oct 31, 2014).
[36] Silvestri, F. Mining query logs: Turning search usage
data into knowledge. Foundations and Trends in Infor-
mation Retrieval 4, 1–2 (2010), 1–174.
[39] Wang, W., and Carreira-Perpin´an, M. A. Projec-
tion onto the probability simplex: An eﬃcient algorithm
with a simple proof, and an application. arXiv preprint
arXiv:1309.1541 (2013).
[40] Warner, S. L. Randomized response: A survey tech-
nique for eliminating evasive answer bias. Journal of the
American Statistical Association 60, 309 (1965), 63–69.
[41] White House Report. Consumer data privacy in a
networked world: A framework for protecting privacy
and promoting innovation in the global digital economy.
Journal of Privacy and Conﬁdentiality 4, 2 (2012), 95–
142.
Appendices
A Client Data Algorithm
A.1 Privacy
Theorem 3. LocalAlg is (, δ)-diﬀerentially pri-
vate.
Proof. We show this by proving that each itera-
tion of the for loop in line 7 of LocalAlg is
((cid:48), δ(cid:48))-diﬀerentially private, where (cid:48) = /mC and
δ(cid:48) = δ/mC. Since there are at most mC iterations of
this loop for each client, composition of diﬀerentially
private mechanisms [12] guarantees that LocalAlg
ensures (, δ)-diﬀerential privacy for each client.
Denote each iteration of the for loop in line 7
of LocalAlg by L;
it takes as input a record
(cid:104)q, u(cid:105) ∈ D, and returns a record, which we denote
L((cid:104)q, u(cid:105)).
If q is not in HL or u is not in HL[q],
then they immediately get transformed into a default
value ((cid:63)) that is in the head list. Since L outputs
only values that exist in the head list, to conﬁrm
diﬀerential privacy we need to prove that for any ar-
bitrary neighboring data sets (cid:104)q, u(cid:105) and (cid:104)q(cid:48), u(cid:48)(cid:105),
Pr(cid:2)L((cid:104)q(cid:48), u(cid:48)(cid:105)) ∈ Y(cid:3)+δ(cid:48) holds
Pr(cid:2)L((cid:104)q, u(cid:105)) ∈ Y(cid:3) ≤ e(cid:48)
for all sets of head list records Y .
Whenever k = 1 or kq = 1, the only query (or
URL for a speciﬁc query) is (cid:63), which will be out-
put with probability 1. Thus, diﬀerential privacy
trivially holds, since the reported values then do
not rely on the client’s data. Thus, we’ll assume
k ≥ 2 and kq ≥ 2. Note that there is a single de-
cision point where it is determined whether q will
be reported truthfully or not. Thus, we can split
the privacy analysis into two parts: 1) Usage of
762    26th USENIX Security Symposium
USENIX Association
the fC fraction of the privacy budget to report a
query, and 2) Usage of the remainder of the pri-
vacy budget to report a URL (given the reported
query). This decomposes a simultaneous two-item
((cid:48), δ(cid:48)) reporting problem into two single-item re-
porting problems with ((cid:48)
U ) respec-
U = (1 − fC)(cid:48),
tively, where (cid:48)
Q = f (cid:48), δ(cid:48)
and δ(cid:48)
Q, δ(cid:48)
Q = f δ(cid:48), (cid:48)
U = (1 − fC)δ(cid:48).
Q) and ((cid:48)
U , δ(cid:48)
1. Privacy of Query Reporting:
Consider the query-reporting case ﬁrst. Overload-
ing our use of L, let L(q) be the portion of L that
makes use of q. We ﬁrst ensure that
δ(cid:48)
Q
2
Pr[L(q) = qHL] ≤ exp((cid:48)
Q) Pr[L(q(cid:48)) = qHL] +
(1)
holds for all q, q(cid:48), and qHL ∈ HL. This trivially
holds when qHL = q = q(cid:48) or qHL (cid:54)∈ {q, q(cid:48)}. The
remaining scenarios to consider are: 1) q (cid:54)= qHL, q(cid:48) =
qHL and 2) q = qHL, q(cid:48) (cid:54)= qHL. By the design of the
algorithm, Pr[L(qHL) = qHL] = t and Pr[L(¯qHL) =
qHL] = (1−t)( 1
k−1 ), where ¯qHL represents any query
Q/2)(k−1)
not equal to qHL. With t =
, it is
Q)+k−1
simple to verify that inequality (1) holds.
Q)+(δ(cid:48)
exp((cid:48)
exp((cid:48)
Consider an arbitrary set of head list queries Y .
Pr[L(q) ∈ Y ] =
Pr[L(q) = qHL]
(cid:88)
qHL∈Y
Pr[L(q) = qHL] +
P r[L(q) = qHL]
qHL∈Y \{q,q(cid:48)}
qHL∈Y ∩{q,q(cid:48)}
Pr[L(q(cid:48)) = qHL] +
Pr[L(q) = qHL]
qHL∈Y \{q,q(cid:48)}
qHL∈Y ∩q,q(cid:48)
Pr[L(q(cid:48)) = qHL] +
Q Pr[L(q(cid:48)) = qHL] +
(cid:88)
(cid:88)
(cid:88)
(cid:0)e(cid:48)
=
=
(cid:88)
(cid:88)
≤ (cid:88)
Q (cid:88)
≤ e(cid:48)
qHL∈Y
qHL∈Y \{q,q(cid:48)}
(cid:1)
(2)
δ(cid:48)
Q
2
(3)
qHL∈Y ∩{q,q(cid:48)}
Pr[L(q(cid:48)) = qHL] + 2 · δ(cid:48)
Q
2
Q Pr[L(q(cid:48)) ∈ Y ] + δ(cid:48)
Q,
= e(cid:48)
Equality (2) stems from the fact that the probability
of reporting a false query is independent of the user’s
true query. The inequality (3) is a direct application
of inequality (1). Thus, L is ((cid:48)
Q)-diﬀerentially
private for query-reporting.
Q, δ(cid:48)
U (kq−1)
U )+kq−1
U , δ(cid:48)
2. Privacy of URL Reporting:
With tq deﬁned as tq = exp((cid:48)
U )+0.5δ(cid:48)
,
exp((cid:48)
an analogous argument shows that the ((cid:48)
U )-
diﬀerential privacy constraints hold if the original
q is kept. On the other hand, if it is replaced with
a random query, then they trivially hold as the al-
gorithm reports a random element in the URL list
of the reported query, without taking into consider-
ation the client’s true URL u.
tions of L is ((cid:48)
private.
By composition [12], each of the at most mC itera-
U ) = ((cid:48), δ(cid:48))-diﬀerentially
Q +δ(cid:48)
Q +(cid:48)
U , δ(cid:48)
A.2 Denoising
Observation 1. ˆpC gives the unbiased estimate
of record and query probabilities under Estimate-
ClientProbabilities.
Proof. Reporting records is a two-stage process
(ﬁrst, decide which query to report, then report
a record); similarly, denoising is also done in two
stages.
Denoising of query probability estimates: Let
rC,q denote the probability that the algorithm has
received query q as a report, and let pq be the true
probability of a user having query q. We want to
learn pq based on rC,q. By the design of our al-
k−1 =
t · pq + 1−t
k−1
rC,q− 1−t
k−1
t− 1−t
k−1
gorithm, rC,q = t · pq + (cid:80)
q(cid:48)(cid:54)=q pq(cid:48)(1 − t) 1
k−1 (1 − pq).
. Using the obtained data for the query ˆrC,q,
q(cid:48)(cid:54)=q pq(cid:48) = t · pq + 1−t
in terms of rC,q yields pq =
Solving for pq
(cid:80)
we estimate pC,q as ˆpC,q =
ˆrC,q− 1−t
k−1
t− 1−t
k−1
.
Denoising of record probability estimates:
Analogously, denote by rC,(cid:104)q,u(cid:105) the probability that
the algorithm has received a record (cid:104)q, u(cid:105) as a report,
and recall p(cid:104)q,u(cid:105) is the record’s true probability in the
data set. Then rC,(cid:104)q,u(cid:105) = t· tq · p(cid:104)q,u(cid:105) +(cid:0)t 1−tq
p(cid:104)q,u(cid:105)) +(cid:0) 1−t