∼ 355.6M
Domain Node Annotations. For each day of trafﬁc monitor-
ing, we build a machine-domain bipartite graph, as discussed
in Section II-A. Each domain node is augmented with informa-
tion about the IP addresses the domain resolved to during the
observation day, and its estimated activity. Given a machine-
domain graph built on a day ti, to estimate the domain activity
features (see Section II-A3) for a domain d we consider DNS
queries about d within two weeks preceding ti. For estimating
the resolved IP abuse features, we leverage a large passive
DNS (pDNS) database, and consider pDNS data stored within
ﬁve months before ti.
Graph Pruning. Following the process described in Sec-
tion II-A2, we prune the graph by applying our set of con-
servative ﬁltering rules (R1 to R4). In average, the pruning
process reduced the number of domain nodes by 26.55%, and
the machine nodes by 13.85%. Also, we obtained a 26.59%
reduction of the total number of edges.
IV. EXPERIMENTAL RESULTS
A. Cross-Day and Cross-Network Tests
To evaluate Segugio’s accuracy and generalization capa-
bilities, we performed extensive train-test experiments. In this
section we aim to show that Segugio’s classiﬁer trained on a
given network can be successfully deployed both in the same
and different ISP networks, and can achieve high accuracy
even when classifying DNS trafﬁc observed several days after
the training was completed.
Training and Test set preparation. To prepare the training
and test sets, we consider two days of trafﬁc. We experiment
with consecutive days, train-test days that are separated by
“gaps”, and with trafﬁc collected from different networks. We
use the DNS trafﬁc from the ﬁrst day for training purposes,
and then test our Segugio classiﬁer on the second day of
trafﬁc (observed at
the same or a different network). We
devised a rigorous procedure to make sure that no ground truth
information about the test domains is ever used during training
and feature measurement.
More speciﬁcally, to prepare the training and test sets, we
ﬁrst built the machine-domain graphs Gt1 and Gt2 according
to the DNS trafﬁc observed on two different days, t1 and
t2, respectively (notice again that these two days of trafﬁc
do not need to be consecutive, and in our experiments they
are separated by a gap of several days). Our main goal in
preparing the training set was to make sure that a large subset
of the known malware and benign domains that appear in
both day t1 and day t2 are completely excluded from training,
and are used only for testing. This allows us to evaluate the
classiﬁer’s generalization ability, and how accurately we can
detect previously unknown malware and benign domains. In
other words, our test dataset contains a large number of domain
names for which we pretend not to know the ground truth, and
whose labels are never used to train Segugio or to measure the
statistical features of test domains.
To this end, given graph Gt2, we ﬁrst “hide” all the ground
truth labels for the domains in the test set, thus obtaining a
new graph G(cid:3)
t2 where the test domains are labeled as unknown.
We use this new graph to measure the features and classify
each unknown test domain following the process described in
Section II-A (see also Figure 4). This allows us to obtain an
unbiased estimate of the true and false positive rates.
Cross-day and cross-network test results. We used multiple
training and test sets to evaluate our behavior-based classi-
ﬁer on the two ISP networks, and on several combinations
of different networks and dates for trafﬁc days t1 and t2.
The number of test samples used in these experiments are
reported in Table II. The ﬁrst two rows correspond to cross-
day experiments in the same ISP, and the last row is related to
a cross-network experiment where we train Segugio on trafﬁc
from ISP1 and test it on domains seen in ISP2. The TP rate
is computed by dividing the number of correctly classiﬁed
malicious test domains by the total number of malicious
domains in the same test dataset (e.g., 9,980 for the ISP1
experiments in Table II). The FP rate is computed in a similar
way by considering the benign test domains. The classiﬁcation
results for these three experiments are reported in Figure 6.
Segugio was able to consistently achieve above 92% TPs at
0.1% FPs.
TABLE II: Cross-day and cross-network test set sizes.
Test Experiment
ISP1 cross-day (13 days gap)
ISP2 cross-day (18 days gap)
ISP1,ISP2 cross-network (15 days gap)
malicious domains
9,980
6,490
6,477
benign domains
780,707
820,219
879,328
B. Feature Analysis
We also performed a detailed analysis of our statistical
features, by training and testing Segugio after completely
removing one of the three feature groups described in Sec-
tion II-A3 at a time. For example, in Figure 7 the “No IP”
ROC curves (dashed black line) refer to a statistical classiﬁer
learned without making use of the IP abuse features (F3). As
we can see, even without the IP abuse features, Segugio can
consistently achieve more than 80% TPs at less than 0.2% FPs.
Also, we can see from the “No machine” line that removing
our machine behavior (F1) features (i.e., using only domain
activity and IP abuse features) would cause a noticeable drop
in the TP rate, for most FP rates below 0.5%. This shows
that our machine behavior features are needed to achieve high
detection rates at low false positives. Overall, the combination
of all three feature groups yields the best results.
C. Cross-Malware Family Tests
While Segugio’s main goal is to discover the occurrence of
new malware-control domains by tracking known infections,
in this section we show that Segugio can also detect domains
related to malware families previously unseen in the monitored
networks. Namely, no infection related to those families was
previously known to have occurred in the monitored networks.
408408
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:49:35 UTC from IEEE Xplore.  Restrictions apply. 
1.0
0.8
0.6
0.4
0.2
P
T
1.0
0.8
0.6
0.4
0.2
P
T
AUC=0.999, PAUC=0.973
TP% | FP%
--------------------
74.01 - 0.01
74.01 - 0.03
94.37 - 0.05
95.42 - 0.07
95.94 - 0.10
97.45 - 0.30
98.72 - 0.50
99.31 - 0.70
99.79 - 1.00
BaggingJ48
RandomForest
LibLinear
1.0
0.8
0.6
0.4
0.2
P
T
AUC=0.998, PAUC=0.962
TP% | FP%
--------------------
0.00 - 0.01
88.12 - 0.03
92.79 - 0.05
94.11 - 0.07
94.87 - 0.10
97.02 - 0.30
98.39 - 0.50
99.21 - 0.70
99.51 - 1.00
BaggingJ48
RandomForest
LibLinear
AUC=0.998, PAUC=0.964
TP% | FP%
--------------------
75.81 - 0.01
87.16 - 0.03
90.97 - 0.05
92.20 - 0.07
93.28 - 0.10
96.33 - 0.30
97.99 - 0.50
99.02 - 0.70
99.51 - 1.00
BaggingJ48
RandomForest
LibLinear
0.0
0.000
0.002
0.004
0.006
0.008
0.010
0.0
0.000
0.002
0.004
0.006
0.008
0.010
0.0
0.000
0.002
0.004
0.006
0.008
0.010
(a) Train and test on ISP1, 13 days gap
FP
(b) Train and test on ISP2, 18 days gap
FP
(c) Train on ISP1, test on ISP2, 15 days gap
FP
Fig. 6: Cross-day and cross-network test results for the two ISP networks (FPs in [0, 0.01])
P
T
P
T
1.0
0.8
0.6
0.4
0.2
All_features
No_machine
No_activity
No_IP
0.0
0.000
0.002
0.004
0.006
0.008
0.010
FP
(a) ISP1
1.0
0.8
0.6
0.4
0.2
All_features
No_machine
No_activity
No_IP
0.0
0.000
0.002
0.004
0.006
0.008
0.010
Fig. 7: Feature analysis: results obtained by excluding one group of features
at a time, and comparison to using all features (FPs in [0, 0.01])
FP
(b) ISP2
To this end, we performed a set of experiments by splitting
our dataset of known blacklisted C&C domains according to
their malware family, rather than at random. The source of
our commercial blacklist was able to provide us with malware
family labels3 for the vast majority of blacklisted domains
(less than 0.1% of blacklisted domains were excluded form
these experiments). Overall, the blacklist consisted of tens of
thousands of C&C domains divided in more than one thousand
different malware families.
To prepare our new tests, we devised an approach similar
to standard cross-validation and partitioned the blacklisted
domains into balanced sets (or folds) of malware families.
Namely, each fold contained roughly the same number of
3Often, the labels were more ﬁne-grained than generic malware families,
and associated domains to a speciﬁc cyber-criminal group.
409409
malware families. The net result is that the domains used
for test always belonged to malware families never used for
training. Said another way, none of the known malware-control
domains used for training belonged to any of the malware
families represented in the test set.
The results are reported in Figure 8 (due to space con-
straints, we only show results from ISP1; results for ISP2 are
similar). As we can see, Segugio is able to discover domains
related to new malware families with more than 85% TPs at
0.1% FPs. To explain this result, we performed a set of feature
analysis experiments (similar to Section IV-B) using the new
experiment settings. We found that if we remove the (F1)
group of machine behavior features, the detection rate drops
signiﬁcantly. In other words, our machine behavior features
are important, because using only feature groups (F2) and (F3)
yields signiﬁcantly lower detection results for low FP rates.
One reason for the contribution of our machine behavior
features (F1) is the existence of multiple infections. Some ma-
chines appear to be infected with multiple malware belonging
to different families, possibly due to the same vulnerabilities
being exploited by different attackers,
to the presence of
malware droppers that sell their infection services to more than
one criminal group, or because of multiple infections behind a
NAT device (e.g., in case of home networks). Also, the domain
activity features (F2) may help because the new domains were
only recently used. Finally, the IP abuse features (F3) may
help when new malware families point their control domains
to IP space that was previously abused by different malware
operators (e.g., in case of the same bulletproof hosting services
used by multiple malware owners).
1.0
0.8
0.6
0.4
0.2
P
T
AUC=0.996, PAUC=0.924
TP% | FP%
--------------------
0.97 - 0.01
57.26 - 0.03
72.34 - 0.05
81.78 - 0.07
85.21 - 0.10
92.83 - 0.30
96.55 - 0.50
97.98 - 0.70
98.95 - 1.00
Baggingj48r
RandomForest
LibLinear
0.0
0.000
0.002
0.004
0.006
0.008
0.010
FP
Fig. 8: Cross-malware family results (one day of trafﬁc observation from
ISP1; FPs in [0, 0.01])
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:49:35 UTC from IEEE Xplore.  Restrictions apply. 
D. Analysis of Segugio’s False Positives
We now provide an analysis of domains in our top Alexa
whitelist that were classiﬁed as malware by Segugio. It is
worth remembering that the whitelist we use contains only
effective second-level domains (e2LDs) that have been in the
top one million list for an entire year (see Section III for
more details). During testing, we count as false positive any
fully qualiﬁed domain (FQD) classiﬁed by Segugio as malware
whose e2LD is in our whitelist.
By analyzing Segugio’s output, we found that most of
the false positives are due to domains related to personal
websites or blogs with names under an e2LD that we failed
to identify as offering “free registration” of subdomains. As
discussed in Section III, such e2LDs may introduce noise in
our whitelist, and should have been ﬁltered out. For example,
most of Segugio’s false positives were related to domain names
under e2LDs such as egloos.com, freehostia.com, uol.com.br,
interfree.it, etc. Unfortunately, these types of services are easily
abused by attackers. Consequently, many of the domains that
we counted as false positives may very well be actual malware-