### Domain Node Annotations

For each day of traffic monitoring, we construct a machine-domain bipartite graph, as discussed in Section II-A. Each domain node is enriched with information about the IP addresses to which the domain resolved during the observation day, as well as its estimated activity. Given a machine-domain graph built on a day \( t_i \), to estimate the domain activity features (see Section II-A3) for a domain \( d \), we consider DNS queries about \( d \) within two weeks preceding \( t_i \). For estimating the resolved IP abuse features, we leverage a large passive DNS (pDNS) database and consider pDNS data stored within five months before \( t_i \).

### Graph Pruning

Following the process described in Section II-A2, we prune the graph by applying our set of conservative filtering rules (R1 to R4). On average, the pruning process reduced the number of domain nodes by 26.55% and the number of machine nodes by 13.85%. Additionally, there was a 26.59% reduction in the total number of edges.

### IV. Experimental Results

#### A. Cross-Day and Cross-Network Tests

To evaluate Segugio’s accuracy and generalization capabilities, we conducted extensive train-test experiments. In this section, we aim to demonstrate that Segugio’s classifier, trained on a given network, can be successfully deployed both in the same and different ISP networks and can achieve high accuracy even when classifying DNS traffic observed several days after the training was completed.

**Training and Test Set Preparation:**

To prepare the training and test sets, we considered two days of traffic. We experimented with consecutive days, train-test days separated by gaps, and traffic collected from different networks. The DNS traffic from the first day was used for training, and the Segugio classifier was tested on the second day of traffic (observed at the same or a different network). We devised a rigorous procedure to ensure that no ground truth information about the test domains was ever used during training and feature measurement.

Specifically, to prepare the training and test sets, we first built the machine-domain graphs \( G_{t1} \) and \( G_{t2} \) based on the DNS traffic observed on two different days, \( t1 \) and \( t2 \), respectively. These two days of traffic do not need to be consecutive, and in our experiments, they were separated by a gap of several days. Our main goal in preparing the training set was to exclude a large subset of known malware and benign domains that appear in both days \( t1 \) and \( t2 \) from the training set, using them only for testing. This allows us to evaluate the classifier’s generalization ability and how accurately it can detect previously unknown malware and benign domains. In other words, our test dataset contains a large number of domain names for which we pretend not to know the ground truth, and whose labels are never used to train Segugio or to measure the statistical features of test domains.

Given graph \( G_{t2} \), we first "hide" all the ground truth labels for the domains in the test set, thus obtaining a new graph \( G'_{t2} \) where the test domains are labeled as unknown. We use this new graph to measure the features and classify each unknown test domain following the process described in Section II-A (see also Figure 4). This allows us to obtain an unbiased estimate of the true and false positive rates.

**Cross-day and Cross-network Test Results:**

We used multiple training and test sets to evaluate our behavior-based classifier on two ISP networks and on several combinations of different networks and dates for traffic days \( t1 \) and \( t2 \). The number of test samples used in these experiments is reported in Table II. The first two rows correspond to cross-day experiments in the same ISP, and the last row is related to a cross-network experiment where we train Segugio on traffic from ISP1 and test it on domains seen in ISP2. The TP rate is computed by dividing the number of correctly classified malicious test domains by the total number of malicious domains in the same test dataset (e.g., 9,980 for the ISP1 experiments in Table II). The FP rate is computed similarly by considering the benign test domains. The classification results for these three experiments are reported in Figure 6. Segugio consistently achieved above 92% TPs at 0.1% FPs.

**Table II: Cross-day and Cross-network Test Set Sizes**

| Test Experiment                | Malicious Domains | Benign Domains |
|--------------------------------|-------------------|----------------|
| ISP1 cross-day (13 days gap)   | 9,980             | 780,707        |
| ISP2 cross-day (18 days gap)   | 6,490             | 820,219        |
| ISP1,ISP2 cross-network (15 days gap) | 6,477            | 879,328        |

#### B. Feature Analysis

We also performed a detailed analysis of our statistical features by training and testing Segugio after completely removing one of the three feature groups described in Section II-A3 at a time. For example, in Figure 7, the “No IP” ROC curves (dashed black line) refer to a statistical classifier learned without using the IP abuse features (F3). As shown, even without the IP abuse features, Segugio can consistently achieve more than 80% TPs at less than 0.2% FPs. Additionally, the “No machine” line indicates that removing our machine behavior (F1) features (i.e., using only domain activity and IP abuse features) would cause a noticeable drop in the TP rate for most FP rates below 0.5%. This demonstrates that our machine behavior features are essential for achieving high detection rates at low false positives. Overall, the combination of all three feature groups yields the best results.

#### C. Cross-Malware Family Tests

While Segugio’s primary goal is to discover the occurrence of new malware-control domains by tracking known infections, in this section, we show that Segugio can also detect domains related to malware families previously unseen in the monitored networks. Specifically, no infection related to those families was previously known to have occurred in the monitored networks.

To this end, we performed a set of experiments by splitting our dataset of known blacklisted C&C domains according to their malware family, rather than at random. The source of our commercial blacklist provided malware family labels for the vast majority of blacklisted domains (less than 0.1% of blacklisted domains were excluded from these experiments). Overall, the blacklist consisted of tens of thousands of C&C domains divided into more than one thousand different malware families.

To prepare our new tests, we devised an approach similar to standard cross-validation and partitioned the blacklisted domains into balanced sets (or folds) of malware families. Each fold contained roughly the same number of malware families, ensuring that the domains used for testing always belonged to malware families never used for training. In other words, none of the known malware-control domains used for training belonged to any of the malware families represented in the test set.

The results are reported in Figure 8 (due to space constraints, we only show results from ISP1; results for ISP2 are similar). As shown, Segugio is able to discover domains related to new malware families with more than 85% TPs at 0.1% FPs. To explain this result, we performed a set of feature analysis experiments (similar to Section IV-B) using the new experiment settings. We found that if we remove the (F1) group of machine behavior features, the detection rate drops significantly. In other words, our machine behavior features are important because using only feature groups (F2) and (F3) yields significantly lower detection results for low FP rates.

One reason for the contribution of our machine behavior features (F1) is the existence of multiple infections. Some machines appear to be infected with multiple malware belonging to different families, possibly due to the same vulnerabilities being exploited by different attackers, the presence of malware droppers that sell their infection services to more than one criminal group, or multiple infections behind a NAT device (e.g., in home networks). Additionally, the domain activity features (F2) may help because the new domains were only recently used. Finally, the IP abuse features (F3) may help when new malware families point their control domains to IP space that was previously abused by different malware operators (e.g., in the case of the same bulletproof hosting services used by multiple malware owners).

**Figure 8: Cross-malware Family Results (one day of traffic observation from ISP1; FPs in [0, 0.01])**

#### D. Analysis of Segugio’s False Positives

We now provide an analysis of domains in our top Alexa whitelist that were classified as malware by Segugio. It is worth noting that the whitelist we use contains only effective second-level domains (e2LDs) that have been in the top one million list for an entire year (see Section III for more details). During testing, we count as false positive any fully qualified domain (FQD) classified by Segugio as malware whose e2LD is in our whitelist.

By analyzing Segugio’s output, we found that most of the false positives are due to domains related to personal websites or blogs with names under an e2LD that we failed to identify as offering “free registration” of subdomains. As discussed in Section III, such e2LDs may introduce noise in our whitelist and should have been filtered out. For example, most of Segugio’s false positives were related to domain names under e2LDs such as egloos.com, freehostia.com, uol.com.br, interfree.it, etc. Unfortunately, these types of services are easily abused by attackers. Consequently, many of the domains that we counted as false positives may very well be actual malware-related.