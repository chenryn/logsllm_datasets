## Environment
  * OS: Debian Wheezy (via docker-airflow)
  * Configuration (trimmed)
  * python 2.7.9, Airflow 1.6.1 from pypi, my project's requirements.txt (trimmed)
## How to replicate and explanation
This DAG crashes my scheduler very quickly when manually (externally)
triggered. I'm going to guess it will crash for everyone running Airflow 1.6.1
and up to master.
I find my DAGs start to be scheduled only after manually / externally
triggering them. (Not sure why.) The manual trigger generates this DagRun:
Then the subsequent scheduler heartbeats generates new scheduled DagRuns one
at a time (one for each of the four previous minutes):
One minute passes, then it tries to schedule the next DagRun, but it crashes
with this exception:
    2015-12-17 16:29:35,097 - root - ERROR - (_mysql_exceptions.IntegrityError) (1062, "Duplicate entry 'crash_dag-2015-12-17 16:28:34' for key 'dag_id'") [SQL: u'INSERT INTO dag_run (dag_id, execution_date, state, run_id, external_trigger, conf) VALUES (%s, %s, %s, %s, %s, %s)'] [parameters: ('crash_dag', datetime.datetime(2015, 12, 17, 16, 28, 34), u'running', u'scheduled__2015-12-17T16:28:34', 0, None)]
    Traceback (most recent call last):
       File "/usr/local/lib/python2.7/dist-packages/airflow/jobs.py", line 608, in _execute
         self.schedule_dag(dag)
       File "/usr/local/lib/python2.7/dist-packages/airflow/jobs.py", line 398, in schedule_dag
         session.commit()
       File "/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py", line 801, in commit
         self.transaction.commit()
The reason is my Airflow database (tested on both MySQL and Postgres) has a
unique key definition in the `dag_run` table on `(dag_id, execution_date)`.
Indeed, the DagRun attempting to be scheduled with ID
`scheduled__2015-12-17T16:28:34` is colliding with my DagRun with ID `test`.
After this collision and exception, the scheduler remains alive, and for each
scheduler heartbeat, I get the following SQLA rollback message, preventing all
future scheduling:
    2015-12-17 16:29:35,115 - root - ERROR - This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (_mysql_exceptions.IntegrityError) (1062, "Duplicate entry 'crash_dag-2015-12-17 16:28:34' for key 'dag_id'") [SQL: u'INSERT INTO dag_run (dag_id, execution_date, state, run_id, external_trigger, conf) VALUES (%s, %s, %s, %s, %s, %s)'] [parameters: ('crash_dag', datetime.datetime(2015, 12, 17, 16, 28, 34), u'running', u'scheduled__2015-12-17T16:28:34', 0, None)]
     Traceback (most recent call last):
       File "/usr/local/lib/python2.7/dist-packages/airflow/jobs.py", line 618, in _execute
         self.import_errors(dagbag)
       File "/usr/local/lib/python2.7/dist-packages/airflow/jobs.py", line 340, in import_errors
         session.query(models.ImportError).delete()
       File "/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/query.py", line 2883, in delete
         delete_op.exec_()
Looking into the Airflow codebase, I see that the DagRun model has a single
unique index that is _not_ the unique index causing this issue. There is,
however, a migration that adds the unique DagRun index causing this issue.
## How to fix
I'm going to try to work-around by manually deleting the unique constraint on
the `dag_run.(dag_id, execution_date)` index. I'm guessing it will work okay
for now.
I think the root cause is an inconsistency between the DagRun model and the
list of migrations that needs to be resolved in such a way there isn't index
collisions.