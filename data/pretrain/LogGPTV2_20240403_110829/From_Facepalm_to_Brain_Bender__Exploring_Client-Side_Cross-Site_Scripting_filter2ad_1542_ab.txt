source and sink access is conducted within the same ﬁle, i.e.,
either within two inline script blocks or the same external
JavaScript ﬁle.
3.1.5 Callstack Relation between Source and Sink
Another property that increases the perceived complexity
when dealing with potentially vulnerable JavaScript code
is the relation between source and sink access in the call
stack. As discussed before, the code responsible for an ex-
ploitable ﬂaw might be spread across multiple functions or
contexts, i.e., source and sink access do not have to be con-
tained within the same function or context.
In the easiest case, access to the source and sink is con-
ducted within the same function, i.e., on the same level in
the call stack. Figure 1 shows the diﬀerent relations between
these two operations with respect to the sink access being
conducted in the red script element SE #3. For our ﬁrst
relation, R1, the source access also occurs in this element.
The second scenario occurs when the source access is con-
ducted in the blue script element SE #1. In this case, the
tainted data is passed as a parameter to the function the
sink access resides in. From an analyst’s perspective, this
means that he can follow the ﬂow of data from the source to
the function. He can subsequently analyze the function with
the knowledge that the passed parameter contains tainted
and unﬁltered data, allowing him to decide whether the fol-
lowing sink access can be deemed safe or not. We refer to
this callstack relation as R2.
In contrast to the previous case, the source access may
also occur in an element which is lower in the callstack than
the sink access. This is depicted in Figure 1 when the source
access occurs in the yellow SE #4. In such a case, the ana-
lyst has to follow the called function to determine that user-
provided data is accessed before having to go up to SE #3
again to see how the tainted data is handled. This compli-
cates the analysis, since it requires a switch back and forth
between functions, and potentially contexts/ﬁles. We deem
this to be R3.
As a fourth scenario, we identify snippets of code in which
source and sink only share a common ancestor in the call-
stack, but neither are parents of the other in the callstack.
Figure 1 shows this for a source access in the orange ele-
ment SE #2, which shares the common parent SE #1 with
the sink-accessing SE #3. The increased complexity in this
scenario stems from the fact that the analyst must ﬁrst in-
vestigate the function which accesses the source, continue
his analysis of the common parent, and then decent into the
sink-accessing function. We denote this relation to be R4.
Finally, the most complex type of ﬂows occurs if source
and sink access share no common ancestors apart from the
virtual main function. This is enabled by the fact that all
JavaScript operates on the same global object, i.e., may set
global variables accessible by any other snippet of JS running
on the same Web page. This is shown in Figure 1 when
accessing the source in the purple SE #0. Hence, there is no
path of code an analyst can follow between source and sink
access. Instead, he has to understand that the ﬁrst script
element accesses the source and stores the retrieved value
in a global variable, and that in turn the second element
accesses this stored value before passing it to the sink. In
our notion, this is callstack relation R5.
Figure 1: Relations between source and sink access
14213.2 Additional Characteristics
In addition to the previously outlined properties which can
be measured to either result in a numerical value or a speciﬁc
relation between source and sink access, we ﬁnd that more
characteristics can be observed for vulnerable code. In this
section, we shed light on these and discuss their implications.
Non-Linear Flows: An additional important observable
characteristic of Client-Side Cross-Site Scripting is a classiﬁ-
cation of the vulnerability’s data and control ﬂows in respect
to their linearity:
In the context of this paper, we consider a data ﬂow to
be linear (LDF), if on the way from the source to the sink,
the tainted value is always passed to all involved functions
directly, i.e., in the form of a function parameter. In conse-
quence, a non-linear data ﬂow (NLDF) includes at least one
instance of transporting the tainted value implicitly, e.g.,
via a global variable or inside a container object. Manual
identiﬁcation of vulnerable data ﬂows in case of NLDFs is
signiﬁcantly harder, as no obvious relationship between the
tainted data and at least one of the ﬂow’s functions exist.
Furthermore, non-linear control ﬂows (NLCF) are instances
of interrupted JavaScript execution: A ﬁrst JavaScript exe-
cution thread accesses the tainted data source and stores it
in a semi-persistent location, such as a closure, event han-
dler or global variable, and later on a second JavaScript
thread uses the data in a sink access. Instances of NLCFs
can occur if the ﬂow’s code is distributed over several code
contexts, e.g., an inline and an external script, or in case
of asynchronous handling of events. Similar to NLDF, the
inspection of such ﬂows is signiﬁcantly more diﬃcult.
Code Origin: The previously outlined model of JavaScript
allows for externally hosted code to be executed in the con-
text of the including application. This implies that a vulner-
ability in included third-party code results in a vulnerability
in the including application and, in addition, an analyst may
need to follow the data ﬂow through code from diﬀerent au-
thors and origins. This switch between contexts is already
covered by M3 and, thus, we do not consider the code ori-
gin to be an additional indicator for the complexity of the
analyst’s task. It is, however, interesting to determine what
code caused the actual ﬂaw.
In terms of origin of the code involved in a ﬂow, we dif-
ferentiate between three cases: self-hosted by the Web page,
code which is only hosted on third-party pages, and a mixed
variant of the previous, where the ﬂow traverses both self-
hosted and third-party code. To distinguish the involved
domains, we use Alexa to match subdomains and Content
Delivery Networks (CDNs) to their parent domain. Thus,
code that is hosted on the CDN of a given site is treated as
self-hosted.
Multiﬂows: A single sink access may contain more than
one piece of user-provided data. This leaves an attacker with
a means of splitting up his malicious payload to avoid detec-
tion. As we [27] have shown, given the right circumstances,
such ﬂows can be used to bypass existing ﬁlter solutions such
as Chrome’s XSSAuditor [1].
Sinks: Another characteristic which is not directly related
to the complexity of a given ﬂow is the sink involved in the
ﬂaw. While a larger number of sinks exist, our data set
(which we present in Section 5.1) consists only of ﬂaws that
target innerHTML, document.write and eval. These also
contain conceptually similar sinks, such as outerHTML, doc-
ument.writeln and setTimeout, respectively. These sinks
diﬀer in terms of what kind of payload must be injected by
an attacker to successfully exploit a vulnerability.
Runtime-generated code: Through the use of the eval
function, JavaScript code can be dynamically created at run-
time and executed in the same context. While this enables
programmers to build ﬂexible Web applications, it also com-
plicates an analyst’s task of understanding a given piece of
code. However, the fact that a script uses eval to generate
code at runtime cannot be measured in a discrete manner,
thus we opt not to use it for complexity metric.
4.
INFRASTRUCTURE
The basis of our study is a data set of real-world vulnera-
bilities. While this enables us to investigate the complexities
of such ﬂaws at a large scale, it brings its own set of chal-
lenges we had to overcome. In this section, we discuss these
challenges and present the infrastructure we developed for
our experimentations.
4.1 Initial Data Set and Challenges
The basis of the results we present in this paper is a set
of exploits detected with the methodology we developed in
2013 [12]. In order to analyze this set of data in a sound and
reproducible way, we had to overcome several challenges.
First and foremost, interaction with live Web servers can
induce variance in the data as no two responses to the same
request are necessarily the same. Causes for such behavior
might reside in load balancing, third-party script rotation
or syntactically diﬀerent versions of semantically identical
code. Secondly, to gain insight into the actual vulnerabili-
ties, we needed to gather detailed information on data ﬂows,
such as all operations which were executed on said data.
Modern Web applications with complex client-side code
often utilize miniﬁcation to save bandwidth when delivering
JavaScript code to the clients. In this process, space is con-
served by removing white spaces as well as using identiﬁer
renaming. As an example, jQuery 2.1.3 can be delivered un-
compressed or miniﬁed, whereas the uncompressed version
is about three times as large as the miniﬁed variant. Our
analysis, however, requires a detailed mapping of vulnerabil-
ities to matching JavaScript code fragments, thus miniﬁed
code presents another obstacle to overcome.
Finally, in our notion, if access to a sink occurs in jQuery,
we assume that this is not actually a vulnerability of that
library, but rather insecure usage by the Web application’s
developer. Thus, to not create false positives when deter-
mining the cause of a vulnerability, we treat jQuery func-
tions as a direct sink and remove them from the trace infor-
mation we collect.
4.2 Persisting and Preparing Vulnerabilities
To allow for a reproducible vulnerability set, we needed
to implement a proxy capable of persisting the response to
all requests made by the browser when visiting a vulnerable
site. To achieve this, we built a proxy on top of mitm-
proxy [4], which provides two modes of operation. We ini-
tially set the mode to caching and crawled all exploits which
had previously triggered their payload and stored both re-
quest and response headers as well as the actual content. To
ensure for proper execution of all JavaScript and, thus, po-
tential additional requests to occur, we conducted the crawl
in a real browser rather than a headless engine. Also, this
1422allowed us to send an additional header from the browser
to the proxy, indicating what kind of resource was being re-
quested (e.g., HTML documents, JavaScript or images), as
content type detection is inherently unreliable [13].
As previously discussed, our analysis requires precise in-
formation on the statements that are executed.
In order
to ensure that a mapping between all operations which are
involved in the ﬂow of data and their corresponding source
line can be achieved, we need all JavaScript to be beautiﬁed.
Therefore, using the information provided by the browsing
engine regarding the requested type of content, we ﬁrst de-
termine the cached ﬁles which were referenced as external
JavaScript. We use the beautiﬁcation engine js-beautify to
ensure that the code is well-formatted and each line consists
only of a single JavaScript statement [6]. Subsequently, we
parse all HTML on disk, beautifying each script block con-
tained in the ﬁles and ﬁnally, save the ﬁles back to disk.
We now switch the operation mode to replay, i.e., all re-
sponses are served from disk and not from the original server.
To do so, the proxy simply queries its database for a match-
ing URL and returns the content from disk, while attaching
the response headers as originally retrieved from the remote
server. Some requests that are conducted at runtime by
JavaScript (such as jQuery XmlHttpRequest with the JSONP
option) carry a nonce in the URL to avoid a cached response
[28]. Therefore, if the proxy cannot ﬁnd the requested URL
in its database, it employs a fuzzy matching scheme which
uses normalized URLs to determine the correct ﬁle to re-
turn. Since our initial tests showed that nonces in all cases
consisted only of numbers, we normalize each URL by sim-
ply replacing each number in the URL with a ﬁxed value.
As we had a ground truth of vulnerable sites, we were able
to verify that our proxy would correctly replay all vulnera-
bilities. This might, however, not be true for random sites
and would then warrant further testing.
4.3 Taint-Aware Firefox Engine
Our analysis methodology relies on a dynamic analysis
of real-world vulnerable JavaScript. Naturally, this requires
the execution of the vulnerable code and the collection of
corresponding trace information. Although taint-aware en-
gines, such as DOMinator [7], exist, we opted to imple-
ment our own engine speciﬁcally tailored to ﬁt the needs
of our study. To ensure a ﬁne-grained analysis of vulnera-
ble ﬂows from sources to sinks, we patched the open-source
Web browser Firefox. The browser was enhanced to track
data originating from sources, across all processing steps in
the SpiderMonkey JavaScript engine as well as the Gecko
rendering engine, and into sinks.
Our previous work relied on numerical identiﬁers in shadow
data to mark a part of a string as originating from a spe-
ciﬁc user-provided source [12]. This, however, only allowed
for tracking of basic source and encoding information. Our
study aims at gathering additional insight into the inner
workings of data ﬂows, therefore a more elaborate approach
must be taken to ensure that all necessary information can
be stored. More precisely, for each vulnerable data ﬂow we
want to capture exact, step-by-step operations which im-
pacted the tainted data or provide additional knowledge
relevant to interpret the data ﬂow. This includes access
to source, calls to both built-in and user-deﬁned functions
which operate on a tainted string, as well as stack traces for
each operation to allow for execution context analysis.
Figure 2: In-Memory representation of taint information
To match these requirements and keep the performance
impact as low as possible, we designed a scheme for eﬃ-
ciently tracking data ﬂows while allowing to record all the
relevant ﬂow information and also enable propagation be-
tween strings. During execution, a tree-like memory struc-
ture is built consisting of nodes representing the operations,
which are annotated using the context information present
when executing the operation. Edges between the nodes
represent execution order and dependency: child operations
are executed after their parents and consume the output
of them. For each tainted substring, string objects contain
references to these nodes, thereby implicitly expressing the
complete history of the tainted slices. Following the ances-
tors of a taint node allows to recreate the chain of operations
that led to the current tainted string - beginning with the
source access. Derived strings copy the references and add
their taint information to the tree, keeping the existing in-
formation for the old strings intact.
Figure 2 shows a graphical representation of this mem-
ory model. The sink access is depicted at the lower part of
the ﬁgure; the shown string was written to the sink docu-
ment.write. To reconstruct all operations which eventually
led to the tainted string being written to the document, we
traverse upwards, detecting that the ﬁnal string was the re-
sult of a concatenation of an untainted part (depicted in
normal font weight) with the the output of another concate-
nation operation (1). This operation put together two addi-