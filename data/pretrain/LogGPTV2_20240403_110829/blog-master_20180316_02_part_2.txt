The split-cleanup flag indicates that a bucket which has been recently split
still contains tuples that were also copied to the new bucket; it essentially
marks the split as incomplete.  Once we're certain that no scans which
started before the new bucket was fully populated are still in progress, we
can remove the copies from the old bucket and clear the flag.  We insist that
this flag must be clear before splitting a bucket; thus, a bucket can't be
split again until the previous split is totally complete.
The moved-by-split flag on a tuple indicates that tuple is moved from old to
new bucket.  Concurrent scans will skip such tuples until the split operation
is finished.  Once the tuple is marked as moved-by-split, it will remain so
forever but that does no harm.  We have intentionally not cleared it as that
can generate an additional I/O which is not necessary.
The operations we need to support are: readers scanning the index for
entries of a particular hash code (which by definition are all in the same
bucket); insertion of a new tuple into the correct bucket; enlarging the
hash table by splitting an existing bucket; and garbage collection
(deletion of dead tuples and compaction of buckets).  Bucket splitting is
done at conclusion of any insertion that leaves the hash table more full
than the target load factor, but it is convenient to consider it as an
independent operation.  Note that we do not have a bucket-merge operation
--- the number of buckets never shrinks.  Insertion, splitting, and
garbage collection may all need access to freelist management, which keeps
track of available overflow pages.
The reader algorithm is:
    lock the primary bucket page of the target bucket
	if the target bucket is still being populated by a split:
		release the buffer content lock on current bucket page
		pin and acquire the buffer content lock on old bucket in shared mode
		release the buffer content lock on old bucket, but not pin
		retake the buffer content lock on new bucket
		arrange to scan the old bucket normally and the new bucket for
         tuples which are not moved-by-split
-- then, per read request:
	reacquire content lock on current page
	step to next page if necessary (no chaining of content locks, but keep
	the pin on the primary bucket throughout the scan)
	save all the matching tuples from current index page into an items array
	release pin and content lock (but if it is primary bucket page retain
	its pin till the end of the scan)
	get tuple from an item array
-- at scan shutdown:
	release all pins still held
Holding the buffer pin on the primary bucket page for the whole scan prevents
the reader's current-tuple pointer from being invalidated by splits or
compactions.  (Of course, other buckets can still be split or compacted.)
To minimize lock/unlock traffic, hash index scan always searches the entire
hash page to identify all the matching items at once, copying their heap tuple
IDs into backend-local storage. The heap tuple IDs are then processed while not
holding any page lock within the index thereby, allowing concurrent insertion
to happen on the same index page without any requirement of re-finding the
current scan position for the reader. We do continue to hold a pin on the
bucket page, to protect against concurrent deletions and bucket split.
To allow for scans during a bucket split, if at the start of the scan, the
bucket is marked as bucket-being-populated, it scan all the tuples in that
bucket except for those that are marked as moved-by-split.  Once it finishes
the scan of all the tuples in the current bucket, it scans the old bucket from
which this bucket is formed by split.
The insertion algorithm is rather similar:
    lock the primary bucket page of the target bucket
-- (so far same as reader, except for acquisition of buffer content lock in
	exclusive mode on primary bucket page)
	if the bucket-being-split flag is set for a bucket and pin count on it is
	 one, then finish the split
		release the buffer content lock on current bucket
		get the "new" bucket which was being populated by the split
		scan the new bucket and form the hash table of TIDs
		conditionally get the cleanup lock on old and new buckets
		if we get the lock on both the buckets
			finish the split using algorithm mentioned below for split
		release the pin on old bucket and restart the insert from beginning.
	if current page is full, first check if this page contains any dead tuples.
	if yes, remove dead tuples from the current page and again check for the
	availability of the space. If enough space found, insert the tuple else
	release lock but not pin, read/exclusive-lock
     next page; repeat as needed
	>> see below if no space in any page of bucket
	take buffer content lock in exclusive mode on metapage
	insert tuple at appropriate place in page
	mark current page dirty
	increment tuple count, decide if split needed
	mark meta page dirty
	write WAL for insertion of tuple
	release the buffer content lock on metapage
	release buffer content lock on current page
	if current page is not a bucket page, release the pin on bucket page
	if split is needed, enter Split algorithm below
	release the pin on metapage
To speed searches, the index entries within any individual index page are
kept sorted by hash code; the insertion code must take care to insert new
entries in the right place.  It is okay for an insertion to take place in a
bucket that is being actively scanned, because readers can cope with this
as explained above.  We only need the short-term buffer locks to ensure
that readers do not see a partially-updated page.
To avoid deadlock between readers and inserters, whenever there is a need
to lock multiple buckets, we always take in the order suggested in Lock
Definitions above.  This algorithm allows them a very high degree of
concurrency.  (The exclusive metapage lock taken to update the tuple count
is stronger than necessary, since readers do not care about the tuple count,
but the lock is held for such a short time that this is probably not an
issue.)
When an inserter cannot find space in any existing page of a bucket, it
must obtain an overflow page and add that page to the bucket's chain.
Details of that part of the algorithm appear later.
The page split algorithm is entered whenever an inserter observes that the
index is overfull (has a higher-than-wanted ratio of tuples to buckets).
The algorithm attempts, but does not necessarily succeed, to split one
existing bucket in two, thereby lowering the fill ratio:
    pin meta page and take buffer content lock in exclusive mode
    check split still needed
    if split not needed anymore, drop buffer content lock and pin and exit
    decide which bucket to split
    try to take a cleanup lock on that bucket; if fail, give up
    if that bucket is still being split or has split-cleanup work:
       try to finish the split and the cleanup work
       if that succeeds, start over; if it fails, give up
	mark the old and new buckets indicating split is in progress
	mark both old and new buckets as dirty
	write WAL for allocation of new page for split
	copy the tuples that belongs to new bucket from old bucket, marking
     them as moved-by-split
	write WAL record for moving tuples to new page once the new page is full
	or all the pages of old bucket are finished
	release lock but not pin for primary bucket page of old bucket,
	 read/shared-lock next page; repeat as needed
	clear the bucket-being-split and bucket-being-populated flags
	mark the old bucket indicating split-cleanup
	write WAL for changing the flags on both old and new buckets
The split operation's attempt to acquire cleanup-lock on the old bucket number
could fail if another process holds any lock or pin on it.  We do not want to
wait if that happens, because we don't want to wait while holding the metapage
exclusive-lock.  So, this is a conditional LWLockAcquire operation, and if
it fails we just abandon the attempt to split.  This is all right since the
index is overfull but perfectly functional.  Every subsequent inserter will
try to split, and eventually one will succeed.  If multiple inserters failed
to split, the index might still be overfull, but eventually, the index will
not be overfull and split attempts will stop.  (We could make a successful
splitter loop to see if the index is still overfull, but it seems better to
distribute the split overhead across successive insertions.)
If a split fails partway through (e.g. due to insufficient disk space or an
interrupt), the index will not be corrupted.  Instead, we'll retry the split
every time a tuple is inserted into the old bucket prior to inserting the new
tuple; eventually, we should succeed.  The fact that a split is left
unfinished doesn't prevent subsequent buckets from being split, but we won't
try to split the bucket again until the prior split is finished.  In other
words, a bucket can be in the middle of being split for some time, but it can't
be in the middle of two splits at the same time.
The fourth operation is garbage collection (bulk deletion):
	next bucket := 0
	pin metapage and take buffer content lock in exclusive mode
	fetch current max bucket number
	release meta page buffer content lock and pin
	while next bucket <= max bucket do
		acquire cleanup lock on primary bucket page
		loop:
			scan and remove tuples
			mark the target page dirty
			write WAL for deleting tuples from target page
			if this is the last bucket page, break out of loop
			pin and x-lock next page
			release prior lock and pin (except keep pin on primary bucket page)
		if the page we have locked is not the primary bucket page:
			release lock and take exclusive lock on primary bucket page
		if there are no other pins on the primary bucket page:
			squeeze the bucket to remove free space
		release the pin on primary bucket page
		next bucket ++
	end loop
	pin metapage and take buffer content lock in exclusive mode
	check if number of buckets changed
	if so, release content lock and pin and return to for-each-bucket loop
	else update metapage tuple count
	mark meta page dirty and write WAL for update of metapage
	release buffer content lock and pin
Note that this is designed to allow concurrent splits and scans.  If a split
occurs, tuples relocated into the new bucket will be visited twice by the
scan, but that does no harm.  See also "Interlocking Between Scans and
VACUUM", below.
We must be careful about the statistics reported by the VACUUM operation.
What we can do is count the number of tuples scanned, and believe this in
preference to the stored tuple count if the stored tuple count and number of
buckets did *not* change at any time during the scan.  This provides a way of
correcting the stored tuple count if it gets out of sync for some reason.  But
if a split or insertion does occur concurrently, the scan count is
untrustworthy; instead, subtract the number of tuples deleted from the stored
tuple count and use that.
Interlocking Between Scans and VACUUM
-------------------------------------
Since we release the lock on bucket page during a cleanup scan of a bucket, a
concurrent scan could start in that bucket before we've finished vacuuming it.
If a scan gets ahead of cleanup, we could have the following problem: (1) the
scan sees heap TIDs that are about to be removed before they are processed by
VACUUM, (2) the scan decides that one or more of those TIDs are dead, (3)
VACUUM completes, (4) one or more of the TIDs the scan decided were dead are
reused for an unrelated tuple, and finally (5) the scan wakes up and
erroneously kills the new tuple.
Note that this requires VACUUM and a scan to be active in the same bucket at
the same time.  If VACUUM completes before the scan starts, the scan never has
a chance to see the dead tuples; if the scan completes before the VACUUM
starts, the heap TIDs can't have been reused meanwhile.  Furthermore, VACUUM
can't start on a bucket that has an active scan, because the scan holds a pin
on the primary bucket page, and VACUUM must take a cleanup lock on that page
in order to begin cleanup.  Therefore, the only way this problem can occur is
for a scan to start after VACUUM has released the cleanup lock on the bucket
but before it has processed the entire bucket and then overtake the cleanup
operation.
Currently, we prevent this using lock chaining: cleanup locks the next page
in the chain before releasing the lock and pin on the page just processed.
Free Space Management
---------------------
(Question: why is this so complicated?  Why not just have a linked list
of free pages with the list head in the metapage?  It's not like we
avoid needing to modify the metapage with all this.)
Free space management consists of two sub-algorithms, one for reserving
an overflow page to add to a bucket chain, and one for returning an empty
overflow page to the free pool.
Obtaining an overflow page:
	take metapage content lock in exclusive mode
	determine next bitmap page number; if none, exit loop
	release meta page content lock
	pin bitmap page and take content lock in exclusive mode
	search for a free page (zero bit in bitmap)
	if found:
		set bit in bitmap
		mark bitmap page dirty
		take metapage buffer content lock in exclusive mode
		if first-free-bit value did not change,
			update it and mark meta page dirty
	else (not found):
	release bitmap page buffer content lock
	loop back to try next bitmap page, if any
-- here when we have checked all bitmap pages; we hold meta excl. lock
	extend index to add another overflow page; update meta information
	mark meta page dirty
	return page number