Pr(cid:104)
VIEWCΠ (𝐷) ∈ 𝑆(cid:105)
≤ exp(𝜖) · Pr(cid:104)
VIEWCΠ (𝐷′) ∈ 𝑆(cid:105)
+ 𝛿𝜅,
where 𝛿𝜅 is negligible in the security parameter 𝜅.
[37]:
2𝑏 exp(cid:16)
The definition covers a malicious setting by replacing the semi-
honest parties P and semi-honestly secure protocol Π with mali-
cious parties and a maliciously secure protocol.
Noise added to the function output is one way to achieve differ-
ential privacy, e.g., via the Laplace mechanism ML
Definition 5 (Laplace Mechanism ML
). The Laplace mech-
anism ML
, for function 𝑓 : 𝑈 𝑛 → R which has sensitivity Δ𝑓 =
∀𝐷≃𝐷′ |𝑓 (𝐷) − 𝑓 (𝐷′)|, releases 𝑓 (𝐷) + Laplace(Δ𝑓 /𝜖), where
max
Laplace(𝑏) denotes a random variable from the Laplace distribution
with scale 𝑏 and density Laplace(𝑥; 𝑏) =
An alternative is probabilistic output selection via the exponen-
, introduced by McSherry and Talwar [60]. The
tial mechanism ME
exponential mechanism expands the application of differential pri-
vacy to functions with non-numerical output, or when the output is
not robust to additive noise [56]. The mechanism is exponentially
more likely to select “good” results where “good” is quantified via
a utility function 𝑢(𝐷, 𝑟) which takes as input a database 𝐷 ∈ 𝑈 𝑛,
and a potential output 𝑟 ∈ R from a fixed set of arbitrary outputs R.
selects an output 𝑟 with probability proportional
Informally, ME
argmax over utility scores with additive noise from the Gumbel
(cid:17). Recently, Durfee and Rogers [32] showed that the
to exp(cid:16) 𝜖𝑢(𝐷,𝑟)
2Δ𝑢
− |𝑥 |𝑏
(cid:17).
1
𝑑1
𝑑𝑛
𝑃1
:
𝑃𝑛
Trusted
Server
M(𝑓 (𝑑1, . . . , 𝑑𝑛))
(a) Central Model
𝑟1 =M(𝑑1)
𝑟𝑛 =M(𝑑𝑛)
𝑃1
:
𝑃𝑛
Untrusted
Server
𝑓 (𝑟1, . . . , 𝑟𝑛)
(b) Local Model
𝑟1 =M(𝑑1)
𝑟𝑛 =M(𝑑𝑛)
𝑃1
:
𝑃𝑛
𝑟𝜋 (1)
:𝑟𝜋 (𝑛)
Untrusted
Server
Shuffler
𝑓(cid:0)𝑟𝜋 (1)
, . . . , 𝑟𝜋 (𝑛)
(cid:1)
(c) Shuffle Model with permutation 𝜋
Figure 1: Implementation models for DP mechanism M.
Party 𝑃𝑖 sends a message (raw 𝑑𝑖 or randomized 𝑟𝑖) to a server,
who evaluates function 𝑓 on the combined messages.
1 [32, Lemma 4.2], and we call this
distribution is equivalent to ME
the Gumbel mechanism (defined in Appendix A).
DP Implementation Models: Different implementation mod-
els for DP exist, shown in Figure 1, and MPC combines their re-
spective benefits [18]. The central model (Figure 1a) assumes a
trusted server receives raw data from each client. The server ap-
plies DP mechanism M on the raw data, which achieves optimal
accuracy. The local model [52] (Figure 1b) assumes an untrusted
server and clients locally apply M on their data before sending it to
the server. As the randomization is applied multiple times (at each
client), the accuracy is limited. Hence, it requires a very large num-
ber of users to achieve accuracy comparable to the central model
[16, 25, 50, 52, 59]. Specifically, an exponential separation between
local and central model for accuracy and sample complexity was
shown [52]. Recently, an intermediate shuffle model (Figure 1c) was
introduced [16, 25]: A trusted party is added between client and
server in the local model, the shuffler, who does not collude with
anyone. The shuffler permutes and forwards the randomized client
values. The permutation breaks the mapping between a client and
her value, which reduces randomization requirements. While the
shuffle model is more accurate than the local model (especially
augmented with secure summation [10, 44]), we also use thresh-
olding to satisfy DP (requiring secure comparisons). The shuffle
model (or mix-nets in general) requires 𝑘 iterations to find 𝑘 heavy
hitters [20, Section 2.4] or costly sketch reconstruction (as the local
model [40, 42, 78]), whereas thresholding allows more accurate and
efficient one-shot discovery [32, 33]. As our goal is high accuracy
without trusted parties even for small number of users, we simulate
the central model in a distributed setting via MPC as commonly
found in DP literature [18, 19, 35, 39, 47, 68, 69, 73]. General MPC
suffers from overhead for computation as well as communication.
However, MPC combines the respective benefits of the models,
namely, high accuracy without disclosing values to a third party,
1Similar to Report One-Sided Noisy Arg Max [37, Section 3.4], which uses the Exponen-
tial distribution.
Session 7D: Privacy for Distributed Data and Federated Learning CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea 2363(1) Define map 𝑇 of size 𝑡 to associate a datum with a count.
(2) For each user reported datum 𝑑 ∈ 𝐷:
(a) If 𝑑 ∈ 𝑇 , then increment counter 𝑇 [𝑑].
(b) Else if |𝑇 | < 𝑡, then add 𝑑 to 𝑇 , and set 𝑇 [𝑑] to 1.
(c) Else, decrement all counters 𝑇 [𝑖], remove 𝑖 from 𝑇 if 𝑇 [𝑖] = 0.
(3) For each item 𝑖 ∈ 𝑇 :
(a) Add noise Laplace(Δ/𝜖) to count 𝑇 [𝑖].
(b) Remove 𝑖 from 𝑇 unless
1/Δ(cid:17)
where 𝜏HH = 1 − Δ log(cid:16)2 − 2(1 − 𝛿)
/𝜖.
(4) Output items in 𝑇 sorted by their noisy count.
𝑇 [𝑖] ≥ 𝜏HH,
Figure 3: Ideal functionality FHH combines heavy hitter de-
tection in streams [27, Alg. 1] with DP-bounded count re-
lease [79, Th. 2].
sketches, i.e., clever approximate algorithms that work over streams
with unknown domains [27] (non-private) or support large domains
[78] (local model) to make the secure multi-party computation
efficient. The employed sketches do not require hashing or domain
reduction (e.g., Bloom filters [40], or matrix projection [12]) and
avoid the reconstruction effort and information loss associated with
it [78]. Clients only send a single message – either their value (FHH)
or a bit vector indicating the bit-prefix of their value (FPEM) – and
the server updates a map that associates client messages with a
count. We utilize central model thresholds [32, 33, 79] and show
that FHH, FPEM are differentially private. In the following, we let Δ
denote the maximum number of counts an individual can influence,
e.g., Δ = 1 if we query countries of origin, or Δ ≥ 1 for current and
former employers.
3.1 Ideal Functionality FHH
Cormode and Hadjieleftheriou [27] surveyed algorithms for (non-
private) heavy hitter detection in data streams and found counter-
based approaches, to be the best w.r.t. accuracy, speed and space
(which was re-confirmed by more recent work [7]). Next we de-
scribe a non-private counter-based approach, which we augment
to be privacy-preserving.
3.1.1 Non-private Misra-Gries. The counter-based approach Misra-
Gries [63],[27, Alg. 1], makes up all steps of ideal functionality FHH
in Figure 3, excluding the DP thresholding in step 3. Misra-Gries
uses counters to track the frequency of already seen elements in a
data stream and provides the following guarantee [7, Lemma 1]:
Lemma 1. Misra-Gries run on 𝐷 of size 𝑛 with 𝑡 counters provides a
frequency estimate(cid:98)𝑓𝑑 for all 𝑑 ∈ 𝐷 satisfying 0 ≤ 𝑓𝑑 −(cid:98)𝑓𝑑 ≤ 𝑛/(𝑡 +1).
Recent improvements, e.g., [7], reduce the expected number of
times the expensive decrement branch is executed (2c in Figure 3),
as it requires updating the entire map 𝑇 . However, as we later
implement FHH with MPC, which must hide the control flow to
prevent leakage, we cannot apply them and focus on the original
version. Note that FHH, due to its use of Misra-Gries, does not
require any domain knowledge or distribution assumptions. Also,
if the map size is equal to the size of the (small) data set, FHH
computes an exact histogram over an unknown data domain.
(a) Zipf
(b) Retail prices [75]
Figure 2: Accuracy (NCR) of our MPC protocols PEM and HH
compared to LDP protocol PEMorig [78] for parameters 𝑘 = 8,
|𝑈 | = 232, 𝜖 = 2 with 𝑛 ∈ {300, 1000, 3000, 5000}.
and we present two efficient MPC protocols for DP top-𝑘: HH has
running time linear in the size of the data and is applicable for very
small data sets (hundreds of values). PEM is sublinear in the data
domain (linear in the bit-length of the data domain) and provides
better accuracy than HH for larger data sizes.
We measure top-𝑘 accuracy like Wang et al. [78] via non-cumulative
rank (NCR), which is similar to F1 score weighted by an element’s
rank, where the most frequent value has rank 𝑘, the second most
frequent rank 𝑘 − 1, etc. (Definition 2). Figure 2 illustrates that
our protocols provide higher accuracy than the state-of-the-art
LDP heavy hitter approach by Wang et al. [78], which we denote
PEMorig and detail in Section 3.2.1. We used synthetic data from the
same Zipf distribution as Wang et al. [78]2 as well as a real-world
Online retail data set [75] and provide more detailed evaluation in
Section 5. Note that the local model and hash-bashed sketches (e.g.,
invertible/counting Bloom filters, count-min sketch) can require
the aggregator to consume significant computational resources to
reconstruct estimates from perturbed reports: PEMorig performs
𝑛2𝑞 hash computations to match potential heavy hitters with re-
ported (randomized) hashes. Even for small data of size 𝑛 = 1000
with recommended 𝑞 = 20, around 1 billion hashes are computed.
Likewise, RAPPOR [42, 43] (follow-up to [40]) detects frequent
strings (e.g., browser homepage, installed software) by estimating
joint probabilities of (perturbed) 𝑛-grams via the expectation max-
imization algorithm, with complexity 𝑂(|𝐷||𝐿|𝑛𝑟) for 𝑟 reported
𝑛-grams per party for string alphabet 𝐿 [43, Section V.B].
Our MPC protocols have better running time complexity than
the above mentioned approaches (Section 4.3), provide accuracy
as in the central model (Section 5), and the computation can be
outsourced to a few computation parties independent of the number
of users (Section 4.5). Hence, we show that the adaptation of central
DP sketches suitable for heavy hitters (i.e., without reconstruction)
is more accurate than LDP and efficient enough for MPC.
3 FEDERATED HEAVY HITTERS
The following ideal functionalities FHH and FPEM describe our
protocols as executed by a trusted third party, which we later replace
by implementing them with optimized MPC protocols as HH and
PEM, respectively. The straight-forward algorithms to accurately
detect heavy hitters are inefficient in MPC. Therefore, we employ
2Zipf(1.5), i.e., the 𝑗-th most frequent value appears with probability proportional to
1/𝑗1.5.
3001,0003,0005,00000.20.40.60.81nNCRHHPEMPEMorig3001,0003,0005,00000.20.40.60.81nNCRSession 7D: Privacy for Distributed Data and Federated Learning CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea 23641
2𝑒−
𝜖(𝜏−1)Δ
3.1.2 Differentially private FHH. The ideal functionality FHH in
Figure 3 approximates counts for frequent values seen so far via
[27, Alg. 1] but only releases noisy counts that exceed the 𝛿-based
threshold 𝜏HH defined by Wilson et al. [79, Th. 2].
Theorem 1. FHH provides (Δ𝜖, 𝛿)-differential privacy.
Proof. Wilson et al. [79, Th. 2] proof that the threshold 𝜏HH
satisfies (Δ𝜖, 𝛿)-DP for counts of unique user contributions in SQL.
I.e., non-empty groups with noisy counts of say column 1 grouped
by column 2 are released if they exceed the threshold, and the
threshold bounds the probability for releasing differing results be-
tween neighbors. We briefly sketch their proof: A noisy count will
be at least 𝜏 with probability 𝑝 =
(property of Laplace
distribution). The probability for bad events (e.g., releasing a count
for a data set but not its neighbor) is bounded as 𝑝Δ ≤ 𝛿 and solving
for 𝜏 provides 𝜏HH. As we assume a single value per user, each
count qualifies as a unique contribution per user, allowing us to
use the same threshold 𝜏HH.
□
3.2 Ideal Functionality FPEM
Wang et al. [78] present a “prefix extension method” (PEM) for LDP
heavy hitter detection and show that it provides higher accuracy
than other LDP approaches [11, 12, 42]. We adapt their local model
protocol, which we denote PEMorig, for our central model protocol
FPEM, and describe them next.
Local model PEMorig. We briefly describe PEMorig and refer
3.2.1
to Appendix B for details. PEMorig leverages overlapping segments
by iteratively finding frequent prefixes of increasing lengths and
clients are split evenly in disjoint groups. The first group reports
perturbed (𝛾 + 𝜂)-bit prefixes of their datum to a server, and the
server estimates the frequencies of all prefix candidates (i.e., all bi-
nary strings with the same length as the bit prefix). Then, the prefix
length is extended by 𝜂, the second group reports their perturbed
prefixes of length 𝛾 + 2𝜂, and the server estimates frequencies of
prefixes that extend the top-2𝛾 prefixes of the previous group. This
is repeated until the prefix length reaches the domain bit-length
𝑏. To create the LDP reports, a client first reduces the domain size
to 𝑢 = ⌈exp(𝜖) + 1⌉ via optimal local hashing [77], then applies
generalized randomized response on the value from the reduced
exp(𝜖)