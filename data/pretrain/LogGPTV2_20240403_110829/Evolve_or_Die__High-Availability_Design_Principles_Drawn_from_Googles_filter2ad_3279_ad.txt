Incorrect/incomplete management process specification
Incorrect traffic engineering
Fabric asymmetry or freeze
Device resource overruns
Control−plane network (partial) failure
Control plane convergence delays
Concurrent buggy control−plane software push
Competing drains/upgrades
Cascade of control−plane elements
Bug in management plane automation
Bad CoS markings
(Concurrent) hardware failures or hardware misconfiguration
0
5
10
Count of Failure Events
B2
B4
Cluster
60
20
40
Count of Failure Events
Figure 5: CDF of failure event dura-
tions by network
Figure 6: Network Evolution and
Failure Events
Figure 7: Breakdown of failure
events by root-cause and network
actions. The rest of the events were caused by partial CPN
failure, and we discuss one such event.
CPN-1. In a B4BR, a partial CPN failure led to complete
trafﬁc blackholing. Each B4BR contains three redundant
copies of the control plane components. During this fail-
ure event, one of the CPN routers for the B4BR was taken
down for power re-cabling. The CPN routers run VRRP
[15] which provides the abstraction of a virtual router and
transparently fails over when a CPN router is taken down.
However, the timer for this failover was longer than the live-
ness timeout between the redundant OFC instances, so two
OFC instances detected mutual disconnection and declared
themselves masters. All OFAs were able to talk to both
OFCs. This situation, an instance of a split brain [30], was
resolved by a watchdog script that watches for these multi-
master events. However, because of a software bug, the
new OFC master had inconsistent state for port switches (in
other words, it was unable to reconstruct network state on
handover), which resulted in wrong dataplane state on the
switches, leading to a large-scale failure across the B4BR.
Trafﬁc from the attached clusters was failed over onto B2,
but this resulted in capacity overload and consequent packet
loss.
Other Categories. Beyond these, we have found four other
data plane root cause categories: concurrent hardware fail-
ures or hardware misconﬁgurations of interfaces, links, rout-
ing engines, optical switching components etc.; bad CoS
markings resulting from incorrectly marked trafﬁc for class-
based differentiation; fabric asymmetry or freezes resulting
from data plane mechanisms like ECMP or link-layer ﬂow
control; link errors or ﬂaps in which transient bit errors or
ﬂaps caused by LACP (IEEE 802.3ad, the link-aggregation
control protocol) cause difﬁcult-to-diagnose failure events.
6.2 Control Plane Categories
Cascade of control plane elements. B4’s control plane has
several components. Several failure events have resulted
either in bad data being propagated through these compo-
nents, or a bottlenecked component triggering failures in
other components. We discuss one such cascade.
Casc-1. An example of bad data propagation occurred
during a network upgrade, when a WAN bundle between
two metros was being migrated from one B4BR to another
B4BR. This bundle is an aggregate of several links which
were re-conﬁgured. During this time, there was a transient
inconsistency between the link (i.e., topology) conﬁguration,
and routing information (which used the older topology).
This inconsistency triggered a bug in the Topology Modeler,
which determines whether a B4BR originates an IP preﬁx
or not, based on information from the B4 Gateway. The bug
resulted in several IP preﬁxes being declared as originating
from more than one cluster. This dual origination broke an
internal assumption within BwE that an IP preﬁx originates
in a single cluster, so BwE assumed that B4 had failed, and
shifted all B4 trafﬁc from/to several metros to B2, resulting
in an overload and consequent packet drops.
Lack of consistency between control plane elements. The
logically centralized control planes in B4 and clusters are
comprised of distinct components which maintain different,
but related, pieces of control plane state. For example, in a
cluster, the Route Aggregator (RA) maintains routing proto-
col state, which the FC uses to compute path or ﬂow state.
Bugs in these implementations can introduce inconsistencies
between these pieces of state, which manifest themselves as
corrupt data plane state, resulting in trafﬁc losses.
In the
clusters, many of these are caught by lab and limited ﬁeld
testing, and the ones that aren’t caught are extremely difﬁ-
cult to identify.
Consis-1. One failure event was triggered by such an in-
consistency that took several weeks to root-cause. In cluster
switches, data plane forwarding tables use next hop groups
to implement ECMP, where each group identiﬁes a set of
links over which trafﬁc is ECMP-ed. The RA, which re-
ceives BGP and IS-IS information, queries switches for the
next hop groups, then sends routes and associated next hop
groups to the OFC, which stores them in its NIB. To do
this, the RA maintains a mirror copy of next hop groups in
each switch. When all routes that use a nexthop group are
65
deleted, ideally the RA should purge its copy of that nex-
thop group. Before doing this, the RA tries to delete the
nexthop group from the corresponding switch, but if that op-
eration fails transiently, the RA continues to cache the un-
used nexthop group. This particular failure was caused by
an implementation choice where, during a full routing table
update (e.g., caused by a BGP session reset) the RA would
not send unused nexthop groups to the OFC, which would
(correctly) delete it from its database. When a subsequent
new route announcement happened to use that unused nex-
thop group, the RA would advertise this to the OFC, which
would drop the route because it had no record of the nex-
thop group. Thus, the resulting ﬂow table entries would not
be installed in switches, resulting in a blackhole. This rare
sequence of events was made a little bit more likely when a
routing conﬁguration change made full routing table updates
by the RA much more frequent (in this case, whenever a par-
ticular community attribute changed). Such failures, which
result in a small number of bad ﬂow entries, can be difﬁ-
cult to diagnose because ECMP and application-level RPC
retry mechanisms make such failures appear indistinguish-
able from intermittent packet loss.
Other Categories. Failure events also arise from: concur-
rent buggy control plane software push to switch, ToR, or
the centralized control plane components in clusters and B4;
incorrect trafﬁc engineering, especially on B4, where, often
because of modeling errors, TE Server does not spread trafﬁc
across available paths; lack of synchronization between the
data plane and control plane, where a data plane element
starts advertising routes before these have been completely
installed in the forwarding elements; and partial or complete
control plane failure where, especially during a MOp, many
instances of control plane components fail simultaneously
both on B4 and clusters.
6.3 Management Plane Categories
Risk assessment failure. Before planning a MOp, engi-
neers assess the risk associated with the operation. This
determines whether, for the duration of the MOp, there
would be sufﬁcient residual capacity in the network to serve
demand.
If the MOp is deemed high risk, one or more
services are drained from clusters that would be affected by
the capacity reduction. Thus, careful risk assessment can
allow in-place network upgrades without service disruption.
Early on, risk assessment was coarse grained and was per-
formed by operators, who would review the description of
the proposed MOp and estimate the residual capacity by the
reduction in hardware capacity due to the MOp. For exam-
ple, taking a B2BR ofﬂine reduces capacity by 50%.
In
the early part of our study, operators would use a rule of
thumb: a residual capacity of less than 50% was consid-
ered risky (because the network is provisioned for single
failures). Later, risk assessments were based on compar-
ing residual capacity with historical demand. As the net-
work grew in complexity, these coarse-grained assessments
resulted in failures for various reasons, and were replaced by
a sophisticated automated tool.
Risk-1. This failure event illustrates the complexity of esti-
mating the residual capacity in a multistage fabric during a
MOp. In this MOp, designed to revamp the power supply to
a cluster fabric and scheduled to last over 30 hours, the steps
called for selectively powering down a ﬁxed fraction (30%)
of fabric switches, bringing them back online, and powering
down another (30%), and so on. A simpliﬁed risk assess-
ment, and the one used for the MOp, predicted about 30%
capacity reduction, so engineers deemed the operation safe.
Unfortunately, this turned out to be wrong: for the given fab-
ric and power system design, this strategy actually reduced
capacity by more than 50% and should have required that
services be marked unavailable in the cluster to reduce traf-
ﬁc levels.
Risk-2. Even for a single MOp, multiple risk assessment
failures can cause failures. During a MOp designed to split
a B4BR into 2 and scheduled to last ﬁve days, two concur-
rent risk assessment failures happened. The ﬁrst was similar
to Risk-1: engineers underestimated the lowest residual ca-
pacity during the MOp by a factor of 2. Second, concurrent
failures in the network at other metros increased the amount
of trafﬁc transiting this B4BR. Both of these combined to
cause packet loss due to reduced capacity.
Bug in Management Plane Automation. Given the inher-
ent complexity of low level management plane operation
speciﬁcations and the possibility of human error, we intro-
duced partial automation for management plane operations.
This automation essentially raises the level of abstraction.
Where before, for example, an operator would have to man-
ually upgrade software on each control plane component in
each of 4 blocks of a B4BR, scripts automate this process.
Operators are still involved in the process, since a complex
MOp might involve invocation of multiple scripts orches-
trated by one or more operators. This partial automation has
increased availability in general, but, because it adds an ad-
ditional layer of complexity, can cause large failures in B4
and clusters. As a result of some of these failures, we are ex-
ploring higher level abstractions for management plane au-
tomation, as discussed in Section 7.4.
BugAuto-1. This failure event illustrates the failure to co-
ordinate the evolution of the control plane and the manage-
ment plane. Many of these automation scripts are carefully
designed to drain and undrain various elements of the fab-
ric in the correct order: switches, physical links, routing
adjacencies, and control plane components. In one failure
that occurred when upgrading control plane software across
4 blocks of the CAR in a cluster, BGP adjacencies did not
come up after the execution of the script, resulting in the
cluster being isolated from both WANs. The root-cause for
this was that the automation script had been designed to care-
fully sequence drains and undrain drains on physical links,
but we had recently introduced a new supertrunking abstrac-
tion (of a logical link comprised of multiple physical links
from different switches in a CAR or B4BR) designed to im-
prove routing scalability, which required a slightly different
66
drain/undrain sequence. Supertrunking had been enabled on
the CAR in question, resulting in the failure.
Other Categories. Other management plane root-cause
categories include:
routing misconﬁgurations similar to
those explored in prior work [29, 31]; competing drains or
upgrades triggered by concurrent and mutually conﬂicting
MOps; incorrect/incomplete management processes where
the wrong set of instructions was used during an operation
on a MOp (e.g., an operation on one router used instructions
for a slightly different router model); and incorrectly
executed management process in which a human operator
made a mistake when invoking command-line interfaces or
management scripts.
7. HIGH-AVAILABILITY PRINCIPLES
As systems become more complex, they become more
susceptible to unanticipated failures [7, 32]. At
least
in Google’s network, there is no silver bullet—no single
approach or mechanism—that can avoid or mitigate failures.
Failures occur roughly to the same extent across all three
networks, across all three networking planes, and there is
no dominant root cause for these failures, at least by our
classiﬁcation. These ﬁndings have led us to formulate a few
high-availability design principles. In this section, we dis-
cuss these principles, together with associated mechanisms
that embody those principles.
7.1 Use Defense in Depth
Our results in Section 5 show that we need defense in
depth7 for failures: an approach where failure detection, mit-
igation or avoidance are built into several places or layers
within the network.
Contain failure radius. Redundant control plane elements
provide robustness to failures of components. In Google’s
network, concurrent failures of all replicas of control plane
elements are not uncommon (BugAuto-1). To control the
topological scope of such failures (their blast radius), we
(a) logically partition the topology of a CAR or B4BR, and
(b) assign each partition to a distinct set of control plane
elements.
There are several choices for partitioning these multi-
stage Clos topologies. One possibility is to partition a
B4BR into k virtual routers, where each such virtual router
is comprised of a 1/k-th of the switches at both stages.
Each virtual router runs its own control plane instance, so
that a failure in one of these virtual routers only reduces the
capacity of the B4BR by 1/k. One additional advantage
of this design is that a virtual router now becomes the unit
at which MOps are executed, so MOps on B4BRs can be
designed to minimize impact on capacity. Other partitioning
strategies include those that “color” links and switches (Fig.
20 in [35]) and assign different colors to different control
planes.
7This term is also used to describe techniques to secure complex software
systems [4] and has its origins in warfare.
Develop fallback strategies. Despite these measures,
large network-wide failures have a non-trivial likelihood in
Google’s network. To mitigate such failures, it is extremely
useful to have fallback strategies that help the network
degrade gracefully under failure. Several fallback strategies
are possible in Google’s network. When one or more B4BRs
fail, B4 trafﬁc can fall back to B2 (as in Casc-1), where
CARs send trafﬁc to B2BRs instead of B4BRs. Conversely,
when all B2BRs in a site fail, trafﬁc can be designed to
fallback to B4. On B4, another form of fallback is possible:
when TE Server fails, trafﬁc can fallback to IP routing.
Many of these fallback strategies are initiated by operators
using big red buttons: software controls that let an operator
trigger an immediate fallback. Given the pace of evolution
of Google’s control plane software, we design big red but-
tons in every new technology we deploy. Each time a new
feature is rolled out in the network (e.g., the supertrunking
abstraction), the older capability is preserved in the network
(in our example, an earlier trunk abstraction) and software
control (a big red button) is put in place which can be used
to disable the new capability and falls back to the older.
7.2 Maintain Consistency Within and
Across Planes
Our second principle is to maintain consistency of state
within the control plane or between the data and control
planes, and consistency of network invariants across the con-
trol and management planes.