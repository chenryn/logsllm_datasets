showing that most bugs were committed on Fridays. Our
work goes beyond these results in several ways. The sta-
tistical analysis is more extensive, is done on a much larger
dataset and is only the ﬁrst step in our system. Based on
the results we train a SVM and create an adaptable system
to predict vulnerability inducing commits.
Thus, our work goes beyond the above approaches in sev-
eral ways. We combine both code metrics as well as meta-
data in our analysis and use a machine-learning approach
to extract and combine relevant features as well as to cre-
ate a classiﬁcation engine to predict which commits are more
likely to be vulnerable. In contrast to the work above, we do
this for a large set of projects in an automated way instead
of hand-picking features and analyzing single projects.
Machine-learning techniques.
Machine-learning and data-mining approaches have been
proposed by several authors for ﬁnding vulnerabilities. For
example, Scandariato et al. [31] train a classiﬁer on textual
features extracted from source code to determine vulnera-
ble software components. Moreover, several unsupervised
machine-learning approaches have been presented to assist
in the discovery of vulnerabilities. For example, Yamaguchi
et al. [36] introduce a method to expose missing checks in
C source code by combining static tainting and techniques
for anomaly detection. Similarly, Chang et al. [13] present
a data-mining approach to reveal neglected conditions and
discover implicit conditional rules and their violations.
However, in order to identify vulnerabilities, these ap-
proaches concentrate only on features extracted from source
code. In contrast, we show that additional meta informa-
tion, such as the experience of a developer, are valuable
features that improve detection performance.
3. METHODOLOGY
In this section, we describe how we created a database
of commits that introduced known vulnerabilities in open-
source projects and which features we extracted from the
commits. We will share this database with the research
community as a baseline to enable a scientiﬁc comparison
428between competing approaches. We focus on 66 C and
C++ projects using the version control system Git (see ap-
pendix A for the list). These 66 projects contain 170,860
commits and 718 vulnerabilities reported by CVEs.
3.1 Vulnerability-contributing Commits
In order to analyze the common features of commits that
introduce vulnerabilities, we ﬁrst needed to ﬁnd out which
commits actually introduced vulnerabilities. To the best
of our knowledge, no large-scale database exists that maps
vulnerabilities as reported by CVEs to commits. Meneely
et al. and Shin et al.
[25, 23, 24] manually created such
mappings for the Mozilla Firefox Browser, Apache HTTP
server and parts of the RHEL Linux kernel. We contacted
the authors to inquire whether they would share this data,
since we could have used that as a baseline for our larger
analysis. Unfortunately, this was not possible at the time,
although the data might be released in the future. To create
a freely available database for ourselves and the research
community, we set out to create a method to automatically
map CVEs to vulnerability-contributing commits.1
Since at this point we are only interested in CVEs relating
to projects hosted on Github, we utilized two data sources
as starting points for our mapping. As a ﬁrst source, we
selected all CVEs containing a link to a commit of one of the
66 projects ﬁxing a vulnerability as part of the “proof”. As a
second source for ﬁxing commits, we created a crawler that
searches commit messages of the 66 projects for mentions of
CVE IDs. To check the accuracy of our mapping we took a
random sample of 10 % and manually checked the mapping
and found no incorrectly mapped CVEs. This gave us a
list of 718 CVEs. This list is potentially not complete since
there might be CVEs that do not link to the ﬁxing commit
and which are also not mentioned in the commit messages.
However, this does not represent a problem for our approach
since 718 is a large enough sample to train our classiﬁer.
We then developed and tested a heuristic to proceed from
these ﬁxing commits to the vulnerability-contributing com-
mits (VCCs). Recall that we are operating on Git commits,
which means that we have access to the whole history of a
given project. One (appropriately named) Git subcommand
is git blame, which, given a ﬁle, for each line names the
commit that last changed the line. The heuristic for ﬁnding
the commit that introduced a vulnerability given a commit
that ﬁxed it is as follows:
1. Ignore changes in documentation such as release notes
or change logs.
2. For each deletion, blame the line that was deleted.
Rationale: If the ﬁx needed to change the line, that
often means that it was part of the vulnerability. Note
that Git diﬀs only know of added and deleted lines. If
a line was changed, it shows up as a deletion and an
addition in the diﬀ.
uploaded
the
have
anonymously
1We
database
to
https://www.dropbox.com/s/x1shbyw0nmd2x45/
vcc-database.dump?dl=0 so the reviewers can access
the raw data during the review process. We will release the
data to the community together with the paper. The ﬁle
was created using pg_dump and can be read into a database
using pg_restore. The dump ﬁle will create the three
tables cves, commits and repositories in the schema export.
3. For every continuous block of code inserted in the ﬁx-
ing commit, blame the lines before and after the block
Rationale: Security ﬁxes are often done by adding ex-
tra checks, often right before an access or after a func-
tion call.
4. Finally, mark the commit vulnerable that was blamed
most in the steps above. If two commits were blamed
for the same amount of lines, blame both.
Our heuristic maps the 718 CVEs of our dataset to 640
VCCs. The reason we have fewer VCCs than CVEs is that
a single commit can induce multiple CVEs. To estimate the
accuracy of our heuristic, we took a 15 % random sample
of all VCCs ﬂagged by our heuristic (i.e. 96 VCCs) and
manually checked them. We found only three cases (i.e.
3.1 %) where our heuristic blamed a wrong commit for the
vulnerability. All three of the mis-mappings occurred in
very large commits. For example, one commit of libtiﬀ2
that ﬁxes CVE-2010-1411 also upgrades libtool to version
2.2.8. The method we propose for VCCFinder is capable of
dealing with noisy datasets, so for the purpose of this work,
an error rate of 3.1 % is acceptable. However, improving our
blame heuristics further is an interesting avenue for future
research.
Apart from the 640 VCCs, we have a large set of 169,502
unclassiﬁed commits. We name these commits unclassiﬁed,
since, while no CVE points to them, they might still contain
unknown vulnerabilities.
At this point we now have a large dataset mapping CVEs
to vulnerability-contributing commits. Our goal now is to
extract features from these VCCs in order to detect further
potential VCCs in the large number of unclassiﬁed commits.
3.2 Features
First we extracted a list of characteristics that we hypoth-
esized could distinguish commits. One of our central hy-
potheses is that combining code metrics with GitHub meta-
data features is beneﬁcial for ﬁnding VCCs. First, we test
each feature separately using statistical analysis, e.g. for
each feature we measured whether the distribution of this
feature within the class of vulnerable commits was statisti-
cally diﬀerent from the distribution within all unclassiﬁed
commits.
Here is a list of hypotheses concerning metadata we started
with:
• New committers are more likely to introduce security
bugs than frequent contributors.
• It is good to “commit early and often” according to
the Git Best Practices3. Therefore, longer commits
may be more suspicious than shorter ones.
• Code that has been iterated over frequently, possibly
by many diﬀerent authors,
is more suspicious than
code that doesn’t change often. Meneely and Williams
[23] already analyzed these code churn features in their
work. We integrate and combine these features below.
Table 1 shows a list of all features along with a statistical
evaluation (cf. Section 3.4) of all numerical features except
2https://GitHub.com/vadz/libtiﬀ/commit/31040a39
3http://sethrobertson.GitHub.io/GitBestPractices/
#commit
429Feature
Scope
mean
VCCs
mean
others
U eﬀect size
Number of commits
Repository
Number of unique contributors Repository
282 171.39
524.99
103 980.95
236.90
32143126*
30528184*
Contributions in project
Additions
Deletions
Past changes
Future changes
Past diﬀerent authors
Future diﬀerent authors
Hunk count
Commit message 1
Commit patch 1
Keywords 2
Added functions
Deleted functions
Modiﬁed functions
Author
Commit
Commit
Commit
Commit
Commit
Commit
Commit
Commit
Commit
Commit
Function
Function
Function
5 %
306.19
73.93
627.17
792.46
40.16
136.58
17.68
—
—
—
6.51
1.07
6.79
15 % 31263040*
71.54
37.46
385.53
396.63
22.70
51.44
9.88
—
—
—
1.03
0.49
3.59
20215148*
42983290*
40715632*
36261346*
40292116*
29534644*
32348343*
—
—
—
28724694*
50084674*
41446509*
40 %
43 %
42 %
62 %
20 %
24 %
33 %
25 %
45 %
40 %
—
—
—
46 %
7 %
23 %
1 These features are text-based and thus not considered in the statistical analysis.
2 See Table 2 for a statisical analyis of each keyword.
Table 1: Overview of the features and results of the statistical analysis of the numeric
features. Mann–Whitney U test signiﬁcant (*) if p < 0.00059.
for project-scoped features. In the following, we discuss the
features. For brevity reasons, we omit the discussion of self-
explaining features here.All our analyses are based on com-
mits. A commit can contain changes to one or more ﬁles.
The metrics about ﬁles and functions are aggregated in the
corresponding commit.
Features scoped by project are obviously the same for ev-
ery commit in that project. However, in combination with
other commit-based features, these can still become relevant.
3.2.1 Features Scoped by Project
Programming language The primary language the pro-
ject is written in, as determined by GitHub through
their open-source linguist library. In our analysis, we
focused on projects written in either C or C++. The
main reason for limiting our focus to one language was
that we wanted to ensure comparability between the
features extracted from the commit patches. When
mixing diﬀerent languages and syntaxes, this can’t be
ensured. We chose C and C++ speciﬁcally since many
security-relevant projects (Linux, Kerberos, OpenSSL,
etc.) are written in these languages.
Star count (number) The number of stars the project
has received on GitHub. Stars are a user’s way of keep-
ing track of interesting projects, as starred projects
show up on the own proﬁle page.
Fork count (number) To fork a project on GitHub means
copying the repository under your personal namespace.
This is often the ﬁrst step to contributing back to the
project by then making changes under the personal
namespace and sending a pull request to the oﬃcial
repository.
Number of commits (number) We counted the number
of commits that are reachable from the main branches
HEAD. The canonical main branch is “master”, but
some projects like bestpractical/rt use “stable” as the
default branch. In those cases we used the branch set
at GitHub by the maintainer of the project.
3.2.2 Features Scoped by Author
Contributions (percentage) How many commits the au-
thor has made in this project in percent, i.e. the num-
ber of commits authored divided by the number of to-
tal commits.
3.2.3 Features Scoped by Commit
Number of Hunks (number) As a hunk is a continuous
block of changes in a diﬀ, this number assesses how
fragmented the commit is (i.e. lots of changes all over
the project versus one big change in one ﬁle or func-
tion).
Patch (text) All changes made by the commit as text rep-
resented as a bag of words.