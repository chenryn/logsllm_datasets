Cu
2
1
1
Fiber
1
1
1
1
Cu
20
12
12
Fiber
31
19
19
13
Table 4: Cost (in USD millions) of equipment, power, and ca-
bling assuming 512 racks with 48 servers/rack. Since these are
estimates, we round to the nearest million.
decrease the number of servers per rack; i.e., increase number of
racks for a ﬁxed total number of servers (not shown).
Finally, with respect to reconﬁguration latency, we observe that
varying the latency from 10msec to 50 msec has minimal (< 5%)
impact on FireFly’s performance (not shown). This is a positive
result that we can achieve pretty good performance even with un-
optimized steering delays and network update times.
8.6 Cost Comparison
As an instructive case, we consider a DC with 512 racks and 48
servers/rack to compute the equipment, power, and cabling costs
of different architectures in Table 4. We consider both copper- and
ﬁber-based realizations for the wired architectures.
We conservatively estimate the “bulk” price of 10GbE network
switches to be $100 per port [11, 50] and that of 10GbE SFPs at
$50 each; these estimates are roughly 50% of their respective retail
costs. Thus, ﬁber-based architectures (including FireFly) incur a
cost of $150 per-port, while copper-based architecture incur a cost
of only $100 per-port. FireFly uses a 96-port (10G) ToR switch
on each rack with 48 FSOs, the full-bisection FatTree needs 1536
96-port (10G) switches, while the 1:2 oversubscribed cores of c-
Through/3DB use roughly half the ports of FatTree. FireFly has
an additional cost for FSO devices (on half the ports), which we
estimate to be $200 per device including SMs or a GM (§3). For
3DB, we assume there are 8 60 GHz radios per rack with each as-
sembly costing $100. For c-Through, we conservatively assume
the 512-port optical switch to be $0.5M. We assume ceiling mir-
rors in FSO and 3DB have negligible cost. For cabling, we use an
average cost of $1 and $3 per meter for copper and optical-ﬁber
respectively, and use an average per-link length of 30m [40]. We
estimate the 5-yr energy cost using a rate of 6.5cents/KWHr, and
per-port power consumption of 3W (ﬁber) and 6W (copper). We
ignore the negligible energy cost of SMs, 60GHz radios, and the
optical switches.
Overall, the total cost of FireFly is 40-60% lower than FatTree
and is comparable (or better) than the augmented architectures.
Note that the augmented architectures have worse performance com-
pared to FireFly, and copper wires have length limitations.
9 Discussion
Given our radical departure from conventional DC designs, we dis-
cuss a sampling of potential operational concerns and possible mech-
anisms to address some of these issues. We acknowledge that there
might be other unforeseen concerns that require further investiga-
tion over pilot long-term deployments.
Pre- and re-alignment. We will need external tools for pre-aligning
SMs/GMs. Fortunately, as this will be done infrequently, this mech-
anism does not need the stringent cost/size requirements and we
can repurpose existing mechanical assemblies. While our design
tolerates minor misalignment (§3), long term operation may need
occasional alignment corrections. Here, we can use the feedback
from the digital optical monitoring support available on optical
SFPs; GMs can directly use such feedback, but SMs may need ad-
ditional micro-positioning mechanisms (e.g., piezoelectrics).
Operational Concerns. Some recurrent concerns we have heard
include dust settling on optics, light scattering effects, reliability
of mechanical parts, and human factors (e.g., need for protective
eyewear for operators). We believe that these are not problem-
atic. Speciﬁcally, dust/scattering may reduce the signal but our
design has sufﬁciently high link margins (15dB), and the devices
can be easily engineered to minimize dust (e.g., non-interfering lids
or blowing ﬁltered air periodically). While we studied an electro-
mechanical GM here, future MEMS-based scanning mirrors [6] are
expected to be very robust. Finally, the lasers we use are infra-red
and very low power which are not harmful to human eye.
Beyond 10 Gbps. Current long-range connector standards for 40/
100 Gbps (e.g., 40 or 100GBASE-LR4) use WDM to multiplex
lower rate channels on the same ﬁber, one in each direction. How-
ever, just like the 10 GbE standard that we have used, there are still
two optical paths (with two ﬁbers) for duplex links. Single-ﬁber
solutions (as we have used for 1 GbE [9]) are not commodity yet
at these speeds as the market is still nascent. We expect, however,
this to change in future. Otherwise, we will need two optical paths
or custom single-path solutions.
10 Related Work
Static Wired Topologies. Early DCs used tree-like structures, which
have poor performance due to oversubscription. This motivated
designs providing full bisection bandwidth [13, 16, 45], which are
overprovisioned to handle worst-case patterns. In addition to high
cost, such networks are not incrementally expandable [45]. In con-
trast, FireFly is ﬂexible, eliminates cabling costs, and amenable to
incremental expansion. Other efforts proposed architectures where
servers act as relay nodes (e.g., [24]). However, they are not cost
competitive [40] and raise concerns about isolation and CPU usage.
Optical Architectures. High trafﬁc volumes coupled with the power
use of copper-based Ethernet, has motivated the use of optical links.
Early works such as c-Through [48] and Helios [20] suggested hy-
brid electric/optical switch architectures, while recent efforts con-
sider all-optical designs [14,41]. The use of free-space optics avoids
the cabling complexity that such optical designs incur. Further-
more, by using multiple FSOs per rack, FireFly can create richer
topologies (at the rack level) than simple matchings [14,20,41,48].
Moreover, FireFly doesn’t need optical switching, thus eliminat-
ing concerns about cost/scalability. Finally, optical switching can
disconnect substantial portions of the optical network during recon-
ﬁguration. While FireFly also has transient link-off periods, these
are localized—which enables us to avoid black holes and discon-
nections using simpler data plane strategies (§6).
 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1Fraction of linksLink utilizationDuring reconfigurationsSteady state 0.95 0.96 0.97 0.98 0.99 1 0 0.02 0.04 0.06 0.08 0.1Fraction of measurementsLoss rateDuring reconfigurationsSteady state 0 0.2 0.4 0.6 0.8 1 0 5 10 15 20Fraction of measurementsEnd-to-end delay (ms)During reconfigurationsSteady state329Wireless in DCs. The FireFly vision is inspired by Flyways [26]
and 3D-Beamforming [52]. However, RF wireless technology suf-
fers from high interference and range limitations and limits perfor-
mance. The use of free-space optics in FireFly eliminates interfer-
ence concerns. Shin et al. consider a static all-wireless (not only
inter-rack) DC architecture using 60 Ghz links [44]. However, this
requires restructuring DC layout and has poor bisection bandwidth
due to interference.
Consistency during Reconﬁgurations. Recent work identiﬁed the
issue of consistency during network updates [34, 43]. FireFly in-
troduces unique challenges because the topology changes as well.
While these techniques can also apply to FireFly, they are more
heavyweight for the speciﬁc properties (no black holes, connec-
tivity, and bounded packet latency) than the domain-speciﬁc solu-
tions we engineer. Other work minimizes congestion during up-
dates [30]. While FireFly’s mechanisms do not explicitly address
congestion, our results (§8.2) suggest that this impact is quite small.
11 Conclusions
In this work, we explore the vision of a fully-ﬂexible, all-wireless,
coreless DC network architecture. We identiﬁed free-space optics
as a key enabler for this vision and addressed practical hardware
design, network provisioning, network management, and algorith-
mic challenges to demonstrate the viability of realizing this vision
in practice. Our work is by no means the ﬁnal word. There remains
signiﬁcant room for improvement in various aspects of our design,
viz., cost and form-factor of hardware elements, algorithmic tech-
niques for network provisioning and management, which should
further improve the cost-performance tradeoff.
Acknowledgments
We thank Yuvraj Agarwal, Daniel Halperin, and our shepherd Jiten-
dra Padhye for their feedback, and Hanfei Chen for GM measure-
ments. This research was supported in part by NSF grants CNS-
1117719 and CNS-0831791.
12 References
[1] A Simpler Data Center Fabric Emerges .
http://tinyurl.com/kaxpotw.
[2] Galvo mirrors. http://www.thorlabs.us/NewGroupPage9.cfm?
ObjectGroup_ID=3770.
[3] htsim simulator.
http://nrg.cs.ucl.ac.uk/mptcp/implementation.html.
[4] Kent optronics, inc.
http://kentoptronics.com/switchable.html.
[5] Lightpointe ﬂightstrata g optical gigabit link.
http://tinyurl.com/k86o2vh.
[6] Mems scanning mirror. http:
//www.lemoptix.com/technology/mems-scanning-mirrors.
[7] Mininet. http://yuba.stanford.edu/foswiki/bin/view/
OpenFlow/Mininet.
[8] OpenGear out of band management. http://tinyurl.com/n773hg3.
[9] Single-ﬁber sfp. http://www.championone.net/products/
transceivers/sfp/single-fiber-single-wavelength/.
[10] Xinyu laser products. http://www.xinyulaser.com/index.asp.
[11] 10GBASE-T vs. GbE cost comparison. Emulex white paper, 2012. Available at
http://www.emulex.com/artifacts/cdc1a1d3-5d2d-4ac5-
9ed8-
5cc4a72bd561/elx_sb_all_10gbaset_cost_comparison.pdf.
[12] M. Al-Fares et al. Hedera: Dynamic ﬂow scheduling for data center networks.
In NSDI, 2010.
[13] M. Al-Fares, A. Loukissas, and A. Vahdat. A scalable, commodity data center
network architecture. In ACM SIGCOMM, 2008.
with unprecedented ﬂexibility. In NSDI, 2012.
[14] K. Chen et al. OSA: An optical switching architecture for data center networks
[15] E. Ciaramella et al. 1.28-Tb/s (32 × 40 Gb/s) free-space optical WDM
transmission system. IEEE Photonics Technology Letters, 21(16), 2009.
[16] C. Clos. A study of non-blocking switching networks. Bell System Technical
Journal, 32, 1953.
[17] A. Curtis et al. DevoFlow: Scaling ﬂow management for high-performance
networks. In ACM SIGCOMM, 2011.
[18] A. Curtis, S. Keshav, and A. Lopez-Ortiz. LEGUP: Using heterogeneity to
reduce the cost of data center network upgrades. In CoNEXT, 2010.
[19] H. L. Davidson et al. Data center with free-space optical communications. US
Patent 8,301,028, 2012.
[20] N. Farrington et al. Helios: A hybrid electrical/optical switch architecture for
modular data centers. In ACM SIGCOMM, 2010.
[21] J. Friedman. On the second eigenvalue and random walks in random d-regular
graphs. Combinatorica, 11(4), 1991.
[22] S. Gollakota, S. D. Perli, and D. Katabi. Interference alignment and
cancellation. In ACM SIGCOMM, 2009.
[23] A. Greenberg et al. VL2: A scalable and ﬂexible data center network. In ACM
SIGCOMM, 2009.
[24] C. Guo et al. BCube: A high performance, server-centric network architecture
for modular data centers. In ACM SIGCOMM, 2009.
[25] A. Gupta and J. Konemann. Approximation algorithms for network design: A
survey. Surveys in Operations Research and Management Science, 16, 2011.
[26] D. Halperin et al. Augmenting data center networks with multi-gigabit wireless
links. In ACM SIGCOMM, 2011.
[27] N. Hamedazimi et al. FireFly: A reconﬁgurable wireless data center fabric
using free-space optics (full version). http:
//www.cs.stonybrook.edu/~hgupta/ps/firefly-full.pdf.
[28] N. Hamedazimi, H. Gupta, V. Sekar, and S. Das. Patch panels in the sky: A case
for free-space optics in data centers. In ACM HotNets, 2013.
[29] B. Heller et al. ElasticTree: Saving energy in data center networks. In NSDI,
2010.
[30] C.-Y. Hong et al. Achieving high utilization with software-driven WAN. In
ACM SIGCOMM, 2013.
[31] D. Kedar and S. Arnon. Urban optical wireless communication networks: The
main challenges and possible solutions. IEEE Communications Magazine,
2004.
[32] B. Kernighan and S. Lin. An efﬁcient heuristic procedure for partitioning
graphs. The Bell Systems Technical Journal, 49(2), 1970.
[33] L. Li. CEO, KentOptronics. Personal communication.
[34] R. Mahajan and R. Wattenhofer. On consistent updates in software deﬁned
networks (extended version). In ACM HotNets, 2013.
[35] P. F. McManamon et al. A review of phased array steering for narrow-band
electrooptical systems. Proceedings of the IEEE, 2009.
[36] B. Monien and R. Preis. Upper bounds on the bisection width of 3- and
4-regular graphs. Journal of Discrete Algorithms, 4, 2006.
[37] J. Mudigonda, P. Yalagandula, and J. C. Mogul. Taming the ﬂying cable
monster: A topology design and optimization framework for data-center
networks. In USENIX ATC, 2011.
[38] N. McKeown et al. OpenFlow: enabling innovation in campus networks. ACM
SIGCOMM CCR, 38(2), 2008.
[39] S. Orfanidis. Electromagnetic waves and antennas; Chapter 15, 19.
http://www.ece.rutgers.edu/~orfanidi/ewa/.
[40] L. Popa et al. A cost comparison of datacenter network architectures. In
CoNEXT, 2010.
[41] G. Porter et al. Integrating microsecond circuit switching into the data center. In
ACM SIGCOMM, 2013.
[42] P. Raghavan and C. D. Thompson. Randomized rounding: a technique for
provably good algorithms and algorithmic proofs. Combinatorica, 7(4), 1987.
[43] M. Reitblatt et al. Abstractions for network update. In ACM SIGCOMM, 2012.
[44] J. Shin, E. G. Sirer, H. Weatherspoon, and D. Kirovski. On the feasibility of
completely wireless datacenters. In ANCS, 2012.
[45] A. Singla, C.-Y. Hong, L. Popa, and P. B. Godfrey. Jellyﬁsh: Networking data
centers randomly. In NSDI, 2012.
[46] O. Svelto. Principles of Lasers. Plenum Press, New York, Fourth edition, 1998.
[47] J. Turner. Effects of data center vibration on compute system performance. In
SustainIT, 2010.
[48] G. Wang et al. c-Through: Part-time optics in data centers. In ACM SIGCOMM,
2010.
[49] R. Wang, D. Butnariu, and J. Rexford. Openﬂow-based server load balancing
gone wild. In Hot-ICE, 2011.
[50] Y. Yang, S. Goswami, and C. Hansen. 10GBASE-T ecosystem is ready for
broad adoption. Commscope/Intel/Cisco White Paper, 2012. Available at
http://www.cisco.com/en/US/prod/collateral/switches/
ps9441/ps9670/COM_WP_10GBASE_T_Ecosystem_US4.pdf.
[51] K. Yoshida, K. Tanaka, T. Tsujimura, and Y. Azuma. Assisted focus adjustment
for free space optics system coupling single-mode optical ﬁbers. IEEE Trans.
on Industrial Electronics, 60(11), 2013.
[52] X. Zhou et al. Mirror mirror on the ceiling: Flexible wireless links for data
centers. In ACM SIGCOMM, 2012.
330