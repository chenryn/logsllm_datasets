Examples
Start simple: Ranges
Sergey Bratus
Entropy tricks for browsing logs and packet captures
Log browsing moves
Data organization
Examples
A frequency histogram
Sergey Bratus
Entropy tricks for browsing logs and packet captures
Log browsing moves
Data organization
Examples
Start simple: Histograms
Sergey Bratus
Entropy tricks for browsing logs and packet captures
Log browsing moves
Data organization
Examples
Probability distribution
Sergey Bratus
Entropy tricks for browsing logs and packet captures
Log browsing moves
Data organization
Examples
Deﬁnition of entropy
Let a random variable X take values x1, x2, . . . , xk with
probabilities p1, p2, . . . , pk.
Deﬁnition (Shannon, 1948)
The entropy of X is
H(X) =
k
i=1
pi · log2
1
pi
Recall that the probability of value xi is pi = ni/N for all
i = 1, . . . , k.
1
Entropy measures the uncertainty or lack of information
about the values of a variable.
2
Entropy is related to the number of bits needed to encode
the missing information (to full certainty).
Sergey Bratus
Entropy tricks for browsing logs and packet captures
Log browsing moves
Data organization
Examples
Why logarithms?
Fact:
The least number of bits needed to encode numbers between 1
and N is log2 N.
Example
You are to receive one of N objects, equally likely to be
chosen.
What is the measure of your uncertainty?
Answer in the spirit of Shannon:
The number of bits needed to communicate the number of the
object (and thus remove all uncertainty), i.e. log2 N.
If some object is more likely to be picked than others,
uncertainty decreases.
Sergey Bratus
Entropy tricks for browsing logs and packet captures
Log browsing moves
Data organization
Examples
Entropy on a histogram
1
Interpretation
Entropy is a measure of uncertainty about the
value of X
1
X = (.25
.25
.25
.25) : H(X) = 2 (bits)
2
X = (.5
.3
.1
.1) :
H(X) = 1.685
3
X = (.8
.1
.05
.05) : H(X) = 1.022
4
X = (1
0
0
0) :
H(X) = 0
Sergey Bratus
Entropy tricks for browsing logs and packet captures
Log browsing moves
Data organization
Examples
Entropy on a histogram
1
2
Interpretation
Entropy is a measure of uncertainty about the
value of X
1
X = (.25
.25
.25
.25) : H(X) = 2 (bits)
2
X = (.5
.3
.1
.1) :
H(X) = 1.685
3
X = (.8
.1
.05
.05) : H(X) = 1.022
4
X = (1
0
0
0) :
H(X) = 0
Sergey Bratus
Entropy tricks for browsing logs and packet captures
Log browsing moves
Data organization
Examples
Entropy on a histogram
1
2
3
Interpretation
Entropy is a measure of uncertainty about the
value of X
1
X = (.25
.25
.25
.25) : H(X) = 2 (bits)
2
X = (.5
.3
.1
.1) :
H(X) = 1.685
3
X = (.8
.1
.05
.05) : H(X) = 1.022
4
X = (1
0
0
0) :
H(X) = 0
Sergey Bratus
Entropy tricks for browsing logs and packet captures
Log browsing moves
Data organization
Examples
Entropy on a histogram
1
2
3
4
Interpretation
Entropy is a measure of uncertainty about the
value of X
1
X = (.25
.25
.25
.25) : H(X) = 2 (bits)
2
X = (.5
.3
.1
.1) :
H(X) = 1.685
3
X = (.8
.1
.05
.05) : H(X) = 1.022
4
X = (1
0
0
0) :
H(X) = 0
Sergey Bratus
Entropy tricks for browsing logs and packet captures
Log browsing moves
Data organization
Examples
Entropy on a histogram
1
2
3
4
Interpretation
Entropy is a measure of uncertainty about the
value of X
1
X = (.25
.25
.25
.25) : H(X) = 2 (bits)
2
X = (.5
.3
.1
.1) :
H(X) = 1.685
3
X = (.8
.1
.05
.05) : H(X) = 1.022
4
X = (1
0
0
0) :
H(X) = 0
For only one value, the entropy is 0.
When all N values have the same frequency,
the entropy is maximal, log2 N.
Sergey Bratus
Entropy tricks for browsing logs and packet captures
Log browsing moves
Data organization
Examples
Compare histograms
Sergey Bratus
Entropy tricks for browsing logs and packet captures
Log browsing moves
Data organization
Examples
Start with the simplest
I am the simplest!
Sergey Bratus
Entropy tricks for browsing logs and packet captures
Log browsing moves
Data organization
Examples
A tree grows in Ethereal
Sergey Bratus
Entropy tricks for browsing logs and packet captures
Log browsing moves
Data organization
Examples
Outline
1
Log browsing moves
Pipes and tables
Trees are better than pipes and tables!
2
Data organization
Trying to deﬁne the browsing problem
Entropy
Measuring co-dependence
Mutual Information
The tree building algorithm
3
Examples
Sergey Bratus
Entropy tricks for browsing logs and packet captures
Log browsing moves
Data organization
Examples
Automating old tricks (2)
“Look for correlations. If two ﬁelds are strongly correlated on
average, but for some values the correlation breaks, look at
those more closely”.
Which pair of ﬁelds to start with?
How to rank correlations?
Too many to try by hand, even with a good graphing tool like R
or Matlab.
Suggestion:
1
Try and rank pairs before looking, and look at the simpler
correlations ﬁrst.
2
Simplicity −→ stronger correlation between features −→
smaller conditional entropy.
Sergey Bratus
Entropy tricks for browsing logs and packet captures
Log browsing moves
Data organization
Examples
Automating old tricks (2)
“Look for correlations. If two ﬁelds are strongly correlated on
average, but for some values the correlation breaks, look at
those more closely”.
Which pair of ﬁelds to start with?
How to rank correlations?
Too many to try by hand, even with a good graphing tool like R
or Matlab.
Suggestion:
1
Try and rank pairs before looking, and look at the simpler
correlations ﬁrst.
2
Simplicity −→ stronger correlation between features −→
smaller conditional entropy.
Sergey Bratus
Entropy tricks for browsing logs and packet captures
Log browsing moves
Data organization
Examples
Automating old tricks (2)
“Look for correlations. If two ﬁelds are strongly correlated on
average, but for some values the correlation breaks, look at
those more closely”.
Which pair of ﬁelds to start with?
How to rank correlations?
Too many to try by hand, even with a good graphing tool like R
or Matlab.
Suggestion:
1
Try and rank pairs before looking, and look at the simpler
correlations ﬁrst.
2
Simplicity −→ stronger correlation between features −→
smaller conditional entropy.
Sergey Bratus
Entropy tricks for browsing logs and packet captures
Log browsing moves
Data organization
Examples
Histograms 3d: Feature pairs
Sergey Bratus
Entropy tricks for browsing logs and packet captures
Log browsing moves
Data organization
Examples
Measure of mutual dependence
How much knowing X tells about Y (on average)?
How strong is the connection?
Compare:
H(X, Y) and H(X)
Compare:
H(X) + H(Y) and H(X, Y)
Sergey Bratus
Entropy tricks for browsing logs and packet captures
Log browsing moves
Data organization
Examples
Joint Entropy
Take N records with two variables X and Y and estimate the
probabilities of seeing a pair of values
p(xi, yj) = nij
N ,
(N =
i,j
nij)
y1
y2
. . .
x1
n11
n12
. . .
x2
n21
n22
. . .
...
...
...
...
where nij is the count of a pair (xi, yj).
Joint Entropy
H(X, Y) =
ij
p(xi, yj) · log2
1
p(xi, yj)
Sergey Bratus
Entropy tricks for browsing logs and packet captures
Log browsing moves
Data organization
Examples
Joint Entropy
Take N records with two variables X and Y and estimate the
probabilities of seeing a pair of values
p(xi, yj) = nij
N ,
(N =
i,j
nij)
y1
y2
. . .
x1
n11
n12
. . .
x2
n21
n22
. . .
...
...
...
...
where nij is the count of a pair (xi, yj).
Joint Entropy
H(X, Y) =
ij
p(xi, yj) · log2
1
p(xi, yj)
Sergey Bratus
Entropy tricks for browsing logs and packet captures
Log browsing moves
Data organization
Examples
Joint Entropy
Take N records with two variables X and Y and estimate the
probabilities of seeing a pair of values
p(xi, yj) = nij
N ,
(N =
i,j
nij)
y1
y2
. . .
x1
n11
n12
. . .
x2
n21
n22
. . .
...
...
...
...
where nij is the count of a pair (xi, yj).
Joint Entropy
H(X, Y) =
ij
p(xi, yj) · log2
1
p(xi, yj)
Always true:
H(X) + H(Y) ≥ H(X, Y)
Sergey Bratus
Entropy tricks for browsing logs and packet captures
Log browsing moves
Data organization
Examples
Joint Entropy
Take N records with two variables X and Y and estimate the
probabilities of seeing a pair of values
p(xi, yj) = nij
N ,
(N =
i,j
nij)
y1
y2
. . .
x1
n11
n12
. . .
x2
n21
n22
. . .
...
...
...
...
where nij is the count of a pair (xi, yj).
Joint Entropy