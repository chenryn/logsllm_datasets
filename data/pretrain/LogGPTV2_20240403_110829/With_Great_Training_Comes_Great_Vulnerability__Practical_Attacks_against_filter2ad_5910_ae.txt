below N − 1, and observe it to be lower than rates ob-
served in Figure 8. This indicates that our retraining
scheme makes the Student model more distinctive from
the Teacher model across all layers. Result for Trafﬁc
Sign is in the Appendix in Figure 15, and is highly con-
sistent with Face.
We plot the Iris results in Figure 9. Important to note
that this defense works signiﬁcantly better for the Iris
task than the Dropout scheme. Sensitivity of the Iris
model actually means classiﬁcation accuracy increased
from 88.27% to 91.0% (retraining found a better local
optimum), while targeted attack success dropped from
100% to 12.6%. Unfortunately, non-targeted attacks re-
main hard to defend against. Attack success rate only
1292    27th USENIX Security Symposium
USENIX Association
falls to 94.83% for Iris, and remains consistently above
classiﬁcation accuracy.
Finally, we note that retrained models are also ro-
bust against the Teacher ﬁngerprinting technique. When
using the true Teacher model as candidate, the ﬁnger-
printing attack results in an average Gini coefﬁcient of
> 0.9846 for both Face and Iris models, which effec-
tively obfuscates the true identity of the Teacher model.
Strengths and Limitations.
This scheme provides
signiﬁcant beneﬁts relative to the randomized dropout
scheme. First, we obtain improved defense performance,
i.e. reduce attack success without signiﬁcantly degrading
classiﬁcation accuracy. Second, unlike the dropout de-
fense, this scheme has no clear countermeasures. Attack-
ers do not have access to the Student training dataset, and
cannot replicate the updated Student using retraining.
Third, this approach successfully obfuscates the identity
of the Teacher model, making it signiﬁcantly harder to
launch the attack given a target Student model.
Finally, the only limitation of this method is that all
Student models must be updated using our technique,
incurring additional computational cost. Compared to
normal Student training, which takes several minutes to
complete (for Face), our implementation that trains Stu-
dent models with a ﬁxed neuron distance threshold in-
curs training time that is an order of magnitude larger.
For the example that corresponds to a reduced attack suc-
cess rate of 30.87% on Face, our defense scheme takes 2
hours. As a one time cost, it is a reasonable tradeoff for
signiﬁcantly improving security against adversarial at-
tacks. Also, we expect that other standard techniques for
speeding-up neural network training (e.g., training over
multiple GPUs), can further reduce the runtime.
6.3 Ensemble of Models as a Defense
Finally, we consider using orthogonal models as a de-
fense for adversarial attacks against transfer learning.
The intuition is to have the provider train multiple Stu-
dent models, each from a separate Teacher model, and
use them together to answer queries (e.g., based on ma-
jority vote). Thus even if an attacker successfully fools
a single Student model in the ensemble, the other mod-
els may be resistant (since the adversarial sample is al-
ways tailored to a speciﬁc Student model). This can
be an effective defense, while only incurring an addi-
tional one time computational cost of training multiple
Students. This idea has been explored before in related
contexts [13].
It is unclear whether an adversary with knowledge of
this defense can craft a successful countermeasure, by
modifying the optimization function to trigger misclassi-
ﬁcation in all members of the ensemble. One possibility
is to modify the loss term that captures dissimilarity in
internal representations (Equation 1), to account for dis-
similarity in all models by taking a sum. In fact, a recent
work in a non transfer learning setting, and assuming a
white-box victim model shows that it is possible to break
defenses based on ensemble models. He et al. , success-
fully crafted adversarial samples that can fool an ensem-
ble of models, by jointly optimizing misclassiﬁcation ob-
jectives over all members of the ensemble [29]. We are
investigating this as part of ongoing work.
7 Related Work
Transfer Learning.
In a deep learning context,
transfer learning has been shown to be effective in vi-
sion [18, 52, 51, 15], speech [34, 63, 30, 20], and
text [33, 40]. Yosinski et al. compared different trans-
fer learning approaches and studied their impact model
performance [68]. Razavian et al. studied the similar-
ity between Teacher and Student tasks, and analyzed its
correlation with model performance [50].
Adversarial Attacks in Deep Learning.
We sum-
marized some prior work on adversarial attacks in Sec-
tion 2. Prior work on white-box attacks formulate mis-
classiﬁcation as an objective function, and use optimiza-
tion techniques to design perturbation [60, 17]. Good-
fellow et al. further reduced the computational complex-
ity of the crafting process to generate adversarial sam-
ples at scale [36]. Papernot et al. proposed an approach
that modiﬁes the image pixel by pixel to minimize the
amount of perturbation [47]. Similar to our methodol-
ogy, Sabour et al. proposed a method that manipulates
internal representation to trigger misclassiﬁcation [53].
Still others studied the physical realizability of adversar-
ial samples [55, 24, 35], and attacks that generate adver-
sarial samples that are unrecognizable to humans [42].
Prior work on black box attacks query the victim DNN
to gain feedback on adversarial samples and use re-
sponses to guide the crafting process [55]. Others use
these queries to reverse-engineer the internals of the vic-
tim DNN [46, 62]. Another group of attacks do not rely
on querying the victim DNN, but assume there exists an-
other model which has similar functionalities as the vic-
tim DNN [38, 45, 61]. They rely on the “transferability”
of adversarial samples between similar models.
Defenses.
Defense against adversarial attacks in DL
is still an open research problem. Recent work showed
that state-of-the-art adversarial attacks can adapt and by-
pass most existing defense mechanisms [16, 14]. One ap-
proach is adversarial training, where the victim DNN is
trained to recognize adversarial samples [60, 39]. Others
tried to detect certain characteristics of adversarial sam-
ples, e.g., sensitivity to model uncertainty, neuron value
distribution [64, 31, 27, 37, 25]. Another defense, called
gradient masking, aims to enhance a model by remov-
USENIX Association
27th USENIX Security Symposium    1293
ing useful information in gradients, which is critical to
white-box attacks [48]. Most existing defenses have been
bypassed in literature, or shown ineffective against new
attacks.
8 Conclusion
In this paper, we describe our efforts to understand the
vulnerabilities introduced by the transfer learning model.
We identify and experimentally validate a general attack
on black-box Student models leveraging knowledge of
white-box Teacher models, and show that it can be suc-
cessful in identifying and exploiting Teacher models in
the wild. Finally, we explore several defenses, includ-
ing a neuron distance threshold technique that is highly
effective against targeted misclassiﬁcation attacks while
obfuscating the identity of Teacher models.
References
[1] http://benchmark.ini.rub.de/?section=gtsrb&
subsection=news. The German Trafﬁc Sign Recognition
Benchmark.
[2] http://biometrics.idealtest.org/.
CASIA Iris
Dataset.
[3] http://pytorch.org/tutorials/beginner/
transfer_learning_tutorial.html.
transfer learning tutorial.
PyTorch
[4] https://cloud.google.com/blog/big-data/
2017/08/how-aucnet-leveraged-tensorflow-
to-transform-their-it-engineers-into-
machine-learning-engineers.
lever-
aged TensorFlow to transform their IT engineers into machine
learning engineers.
How Aucnet
[5] https://codelabs.developers.google.com/
codelabs/cpb102-txf-learning/index.html#0.
Image Classiﬁcation Transfer Learning with Inception v3.
[6] https://docs.microsoft.com/en-us/cognitive-
toolkit/Build-your-own-image-classifier-
using-Transfer-Learning.
classiﬁer using transfer learning.
Build your own image
[7] https://www.tensorflow.org/versions/r0.
12/how_tos/image_retraining/.
Inception’s Final Layer for New Categories.
How to Retrain
[8] http://vision.seas.harvard.edu/pubfig83/.
PubFig83: A resource for studying face recognition in personal
photo collections.
[9] http://www.robots.ox.ac.uk/˜vgg/data/
flowers/102/index.html. 102 Category Flower Dataset.
[10] http://www.robots.ox.ac.uk/˜vgg/data/
flowers/17/index.html. 17 Category Flower Dataset.
[11] http://www.robots.ox.ac.uk/˜vgg/software/
vgg_face/. VGG Face Descriptor.
[12] TensorFlow: Large-scale machine learning on heterogeneous
systems, 2015. Software available from tensorﬂow.org.
[13] ABBASI, M., AND GAGN ´E, C. Robustness to adversarial exam-
ples through an ensemble of specialists. In Proc. of Workshop on
ICLR (2017).
[14] ATHALYE, A., CARLINI, N., AND WAGNER, D. Obfuscated
gradients give a false sense of security: Circumventing defenses
to adversarial examples. In Proc. of ICML (2018).
[15] CAELLES, S., MANINIS, K.-K., PONT-TUSET, J., LEAL-
TAIX ´E, L., CREMERS, D., AND VAN GOOL, L. One-shot video
object segmentation. In Proc. of CVPR (2017).
[16] CARLINI, N., AND WAGNER, D. Adversarial examples are not
In Proc. of
easily detected: Bypassing ten detection methods.
AISec (2017).
[17] CARLINI, N., AND WAGNER, D. Towards evaluating the robust-
ness of neural networks. In Proc. of S&P (2017).
[18] CHEN, J.-C., RANJAN, R., KUMAR, A., CHEN, C.-H., PA-
TEL, V. M., AND CHELLAPPA, R. An end-to-end system for un-
constrained face veriﬁcation with deep convolutional neural net-
works. In Proc. of Workshop on ICCV (2015).
[19] CHOLLET, F., ET AL. Keras. https://keras.io, 2015.
[20] CIRES¸ AN, D. C., MEIER, U., AND SCHMIDHUBER, J. Transfer
learning for latin and chinese characters with deep neural net-
works. In Proc of IJCNN (2012).
[21] COHEN, R., EREZ, K., BEN AVRAHAM, D., AND HAVLIN, S.
Breakdown of the internet under intentional attack. Physical Re-
view Letters 86 (2001), 3682–5.
[22] DELAMORE, B., AND KO, R. K. L. A global, empirical analysis
In Proc. of
of the shellshock vulnerability in web applications.
ISPA (2015).
[23] ERHAN, D., MANZAGOL, P.-A., BENGIO, Y., BENGIO, S.,
AND VINCENT, P. The difﬁculty of training deep architectures
and the effect of unsupervised pre-training. In Proc. of AISTATS
(2009).
[24] EVTIMOV, I., EYKHOLT, K., FERNANDES, E., KOHNO, T.,
LI, B., PRAKASH, A., RAHMATI, A., AND SONG, D. Ro-
bust Physical-World Attacks on Deep Learning Models. In arXiv
preprint 1707.08945 (2017).
[25] FEINMAN, R., CURTIN, R. R., SHINTRE, S., AND GARD-
NER, A. B. Detecting adversarial samples from artifacts. arXiv
preprint arXiv:1703.00410 (2017).
[26] GINI, C. Italian: Variabilit`a e mutabilit`a (variability and muta-
bility). Cuppini, Bologna (1912).
[27] GROSSE, K., MANOHARAN, P., PAPERNOT, N., BACKES, M.,
AND MCDANIEL, P. On the (statistical) detection of adversarial
examples. arXiv preprint arXiv:1702.06280 (2017).
[28] HE, K., ZHANG, X., REN, S., AND SUN, J. Deep residual
learning for image recognition. In Proc. of CVPR (2016).
[29] HE, W., WEI, J., CHEN, X., CARLINI, N., AND SONG, D. Ad-
versarial example defenses: Ensembles of weak defenses are not
strong. In Proc. of USENIX Workshop on Offensive Technologies
(2017).
[30] HEIGOLD, G., VANHOUCKE, V., SENIOR, A., NGUYEN, P.,
RANZATO, M., DEVIN, M., AND DEAN, J. Multilingual acous-
tic models using distributed deep neural networks.
In Proc. of
ICASSP (2013).
[31] HENDRYCKS, D., AND GIMPEL, K. Early methods for detecting
adversarial images. In ICLR Workshop Track (2017).
[32] HOWARD, A. G., ZHU, M., CHEN, B., KALENICHENKO, D.,
WANG, W., WEYAND, T., ANDREETTO, M., AND ADAM, H.
Mobilenets: Efﬁcient convolutional neural networks for mobile
vision applications. arXiv preprint arXiv:1704.04861 (2017).
[33] JOHNSON, M., SCHUSTER, M., LE, Q. V., KRIKUN, M., WU,
Y., CHEN, Z., THORAT, N., VI ´EGAS, F., WATTENBERG, M.,
CORRADO, G., ET AL. Google’s multilingual neural machine
translation system: enabling zero-shot translation.
In Proc. of
ACL (2017).
1294    27th USENIX Security Symposium
USENIX Association
[34] KUNZE, J., KIRSCH, L., KURENKOV, I., KRUG, A., JOHANNS-
MEIER, J., AND STOBER, S. Transfer learning for speech recog-
nition on a budget. In Proc. of RepL4NLP (2017).
[54] SAROIU, S., GUMMADI, K., AND GRIBBLE, S. D. A mea-
surement study of peer-to-peer ﬁle sharing systems. In Proc. of
MMCN (2002).
[35] KURAKIN, A., GOODFELLOW, I., AND BENGIO, S. Adversarial
examples in the physical world. In Proc. of ICLR (2016).
[36] KURAKIN, A., GOODFELLOW, I., AND BENGIO, S. Adversarial
machine learning at scale. In Proc. of ICLR (2017).
[37] LI, X., AND LI, F. Adversarial examples detection in deep
arXiv preprint
networks with convolutional ﬁlter statistics.
arXiv:1612.07767 (2016).
[38] LIU, Y., CHEN, X., LIU, C., AND SONG, D. Delving into trans-
ferable adversarial examples and black-box attacks. In Proc. of
ICLR (2016).
[39] METZEN, J. H., GENEWEIN, T., FISCHER, V., AND BISCHOFF,
In Proc. of ICLR
B. On detecting adversarial perturbations.
(2017).
[40] MIKOLOV, T., LE, Q. V., AND SUTSKEVER, I. Exploiting sim-
ilarities among languages for machine translation. arXiv preprint
arXiv:1309.4168 (2013).
[41] MOOSAVI-DEZFOOLI, S.-M., FAWZI, A., AND FROSSARD, P.
Deepfool: a simple and accurate method to fool deep neural net-
works. In Proc. of CVPR (2016).
[42] NGUYEN, A., YOSINSKI, J., AND CLUNE, J. Deep neural net-
works are easily fooled: High conﬁdence predictions for unrec-
ognizable images. In Proc. of CVPR (2015).
[43] NOCEDAL, J., AND WRIGHT, S. Numerical optimization, series
in operations research and ﬁnancial engineering. Springer, New
York, USA, 2006 (2006).
[44] PAPERNOT, N., CARLINI, N., GOODFELLOW, I., FEINMAN,
R., FAGHRI, F., MATYASKO, A., HAMBARDZUMYAN, K.,
JUANG, Y.-L., KURAKIN, A., SHEATSLEY, R., GARG, A.,
AND LIN, Y.-C. cleverhans v2.0.0: an adversarial machine learn-
ing library. arXiv (2017).
[45] PAPERNOT, N., MCDANIEL, P., AND GOODFELLOW,
Transferability in machine learning:
black-box attacks using adversarial samples.
arXiv:1605.07277 (2016).
I.
from phenomena to
arXiv preprint
[46] PAPERNOT, N., MCDANIEL, P., GOODFELLOW, I., JHA, S.,
CELIK, Z. B., AND SWAMI, A. Practical black-box attacks
against machine learning. In Proc. of Asia CCS (2017).
[47] PAPERNOT, N., MCDANIEL, P., JHA, S., FREDRIKSON, M.,
CELIK, Z. B., AND SWAMI, A. The limitations of deep learning
in adversarial settings. In Proc. of EuroS&P (2016).
[48] PAPERNOT, N., MCDANIEL, P., WU, X., JHA, S., AND SWAMI,
A. Distillation as a defense to adversarial perturbations against
deep neural networks. In Proc. of S&P (2016).
[49] PARKHI, O. M., VEDALDI, A., ZISSERMAN, A., ET AL. Deep
face recognition. In Proc. of BMVC (2015).
[50] RAZAVIAN, A. S., AZIZPOUR, H., SULLIVAN, J., AND CARLS-
SON, S. Cnn features off-the-shelf: an astounding baseline for
recognition. In Proc. of Workshop on CVPR (2014).
[51] REDMON, J., DIVVALA, S., GIRSHICK, R., AND FARHADI, A.
You only look once: Uniﬁed, real-time object detection. In Proc.
of CVPR (2016).
[52] REN, S., HE, K., GIRSHICK, R., AND SUN, J. Faster r-cnn: To-
wards real-time object detection with region proposal networks.
In Proc. of NIPS (2015).
[53] SABOUR, S., CAO, Y., FAGHRI, F., AND FLEET, D. J. Ad-
versarial manipulation of deep representations. In Proc. of ICLR
(2015).
[55] SHARIF, M., BHAGAVATULA, S., BAUER, L., AND REITER,
M. K. Accessorize to a crime: Real and stealthy attacks on state-
of-the-art face recognition. In Proc. of CCS (2016).
[56] SIMONYAN, K., AND ZISSERMAN, A. Very deep convolu-
tional networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556 (2014).
[57] SRIVASTAVA, N., HINTON, G. E., KRIZHEVSKY, A.,
SUTSKEVER, I., AND SALAKHUTDINOV, R. Dropout: a sim-
ple way to prevent neural networks from overﬁtting. JMLR 15, 1
(2014), 1929–1958.
[58] SZEGEDY, C., IOFFE, S., VANHOUCKE, V., AND ALEMI, A. A.
Inception-v4, inception-resnet and the impact of residual connec-
tions on learning. In AAAI (2017).
[59] SZEGEDY, C., LIU, W., JIA, Y., SERMANET, P., REED, S.,
ANGUELOV, D., ERHAN, D., VANHOUCKE, V., RABINOVICH,
A., ET AL. Going deeper with convolutions. In Proc. of CVPR
(2015).
[60] SZEGEDY, C., ZAREMBA, W., SUTSKEVER, I., BRUNA, J.,
ERHAN, D., GOODFELLOW, I., AND FERGUS, R.
Intriguing
properties of neural networks. arXiv preprint arXiv:1312.6199
(2013).
[61] TRAM `ER, F., PAPERNOT, N., GOODFELLOW, I., BONEH, D.,
AND MCDANIEL, P. The space of transferable adversarial exam-
ples. arXiv preprint arXiv:1704.03453 (2017).
[62] TRAM `ER, F., ZHANG, F., JUELS, A., REITER, M., AND RIS-
TENPART, T. Stealing machine learning models via prediction
apis. In Proc. of USENIX Security (2016).
[63] WANG, D., AND ZHENG, T. F. Transfer learning for speech and
language processing. In Proc. of APSIPA (2015).
[64] WANG, Q., GUO, W., ZHANG, K., ORORBIA II, A. G., XING,
X., LIU, X., AND GILES, C. L. Adversary resistant deep neural
networks with an application to malware detection.
In Proc. of
KDD (2017).
[65] WANG, Z., BOVIK, A. C., SHEIKH, H. R., AND SIMONCELLI,
E. P. Image quality assessment: from error visibility to structural
similarity. IEEE Trans. on Image Processing 13, 4 (2004), 600–
612.
[66] WANG, Z., SIMONCELLI, E. P., AND BOVIK, A. C. Multi-
scale structural similarity for image quality assessment. In AC-
SSC (2003), vol. 2, IEEE, pp. 1398–1402.
[67] YAO, Y., XIAO, Z., WANG, B., VISWANATH, B., ZHENG, H.,
AND ZHAO, B. Y. Complexity vs. performance: empirical anal-
ysis of machine learning as a service. In Proc. of IMC (2017).
[68] YOSINSKI, J., CLUNE, J., BENGIO, Y., AND LIPSON, H. How
In Proc. of
transferable are features in deep neural networks?
NIPS (2014).
[69] ZEILER, M. D. Adadelta: an adaptive learning rate method.
arXiv preprint arXiv:1212.5701 (2012).
[70] ZEILER, M. D., AND FERGUS, R. Visualizing and understand-
ing convolutional networks. In Proc. of ECCV (2014).
[71] ZHANG, L., CHOFFNES, D., DUMITRAS, T., LEVIN, D., MIS-
LOVE, A., SCHULMAN, A., AND WILSON, C. Analysis of ssl
certiﬁcate reissues and revocations in the wake of heartbleed. In
Proc. of IMC (2014).
[72] ZHENG, S., SONG, Y., LEUNG, T., AND GOODFELLOW, I. Im-
proving the robustness of deep neural networks via stability train-
ing. In Proc. of CVPR (2016).
USENIX Association
27th USENIX Security Symposium    1295
Student Task
Dataset
# of Classes
Face
Iris
PubFig83 [8]
CASIA Iris [2]
Trafﬁc Sign
GTSRB [1]
Flower
VGG Flowers [9]
65
1,000
43
102
Training
Testing
Size
5,850
16,000
39,209
6,149
Size
650
4,000
12,630
1,020
Teacher
Model
Training Conﬁgurations
VGG-Face [11]
VGG16 [56]
VGG16 [56]
ResNet50 [28]
epoch=200,batch=32,optimizer=adadelta,lr=1.0
epoch=100,batch=32,optimizer=adadelta,lr=0.1
epoch=50,batch=32,optimizer=adadelta,lr=1.0
epoch=150,batch=50,optimizer=sgd,lr=0.01
Table 2: Detailed information about dataset, Teacher models, and training conﬁgurations for each Student task.
    Source      P=0.001      P=0.003      P=0.005      Target
    Source   DSSIM        L2        Target
Figure 10: Adversarial examples generated from the
same source image with different perturbation budgets
(using DSSIM). Lower budget produces less noticeable
perturbations.
Figure 11: Comparison between adversarial images gen-
erated using DSSIM perturbation budget (P = 0.003) and
L2 budget (P = 0.01). Budgets of both metrics are cho-
sen to produce similar targeted attack success rate around
90%.
A Appendix
Disclosure
While we did not perform any attacks on deployed im-
age recognition systems, we did experiment with pub-
licly available Teacher models from Google, Microsoft
and the open source PyTorch originally started by Face-
book. Following their tutorials, our results showed they
were vulnerable to this class of adversarial attacks. In ad-