5
6
7
8
9 };
unsigned int version;
short mem; // the starting address of the ﬁrst object
short slab size; // the size of the slab descriptor
int obj size; // the actual size used by each object
int buffer size; // the whole size for each object
int number; // the number of objects in this slab
long key; // the key for canary veriﬁcation
Figure 8. PIA entry.
within a slab. To locate the canary that resides in the slab de-
scriptor, we record the slab descriptor size in the slab size
ﬁeld, which additionally includes the size of the object de-
scriptor and the following padding. With the starting ad-
dress of the ﬁrst object subtracting the slab descriptor size,
we get the starting address of the slab descriptor and then
locate the canary, whose offset within the slab descriptor is
predetermined. On the other hand, if the slab descriptor is
kept off the slab, we set the value of the slab size to zero.
Accordingly, we employ a different method to locate the ca-
nary before the ﬁrst object. In particular, we check whether
the starting address of the ﬁrst object is page-aligned, if not,
it indicates there is a color placed in the front. Then, we can
check the canary safely.
As introduced previously, kernel heap are managed in
different slabs, one of which consists of one or more phys-
ically contiguous pages. Therefore, the slab that contains
several pages should correspond to several entries in the
PIA. In order to facilitate recording the slab canary informa-
tion into PIA entries, we just use the ﬁrst associated entry
to store the whole information, and keep other associated
entries empty.
It is worth mentioning that we utilize the page alloca-
tor to dynamically allocate kernel memory for the PIA data
structure during the kernel’s initialization. Basically, the to-
tal memory occupied by the PIA is determined by the num-
ber of pages in the heap. However, the proportion is un-
changed even if all the physical memory are used by the
kernel heap. Since each PIA entry has only 24 bytes in our
implementation, the memory overhead is as low as 24/4096.
Furthermore, it is possible to reduce the size of the PIA en-
try by packing its ﬁelds.
7 Evaluation
To evaluate Kruiser, we developed a prototype of Kruiser
based on 32-bit Linux and the Xen hypervisor (with PAE
enabled), and conducted effectiveness tests and measured
performance overhead. All the experiments were run on a
Dell Precision Workstation with two 2.26GHz Intel Xeon
quad-core processors and 6GB memory. The Xen hypervi-
sor (with PAE enabled) version is 3.4.2. We used Ubuntu
8.04 (linux-2.6.24 with PAE enabled) as Dom0 system and
Ubuntu 8.04 (linux-2.6.24 with PAE disabled) as DomU
system (with HVM mode). Moreover, we allocated 1 GB
memory and 4 VCPU for this DomU system.
7.1 Effectiveness
To test whether Kruiser can detect heap buffer over-
ﬂows, we deliberately introduced three explicit vulnera-
bilities [46, 52] in the Linux kernel, and then exploited
these bugs. In our ﬁrst test, we modiﬁed the kernel func-
tion cmsghdr from user compat to kern, making it process
some user-land data without sanitization, such that mali-
cious users launch heap-based buffer overﬂow attacks via
the sendmsg system call. For the second test, we loaded
a vulnerable kernel module that is developed by ourselves.
The function of this module is to use a dynamic general
buffer to store certain data transferred from the user-land.
However, the module does not perform boundary check
when it stores the user data. In the third test, we also em-
ployed a loadable kernel module to export a bug in kernel
space. Unlike the second test, we constructed a speciﬁc slab
in this module, and allocated the last object in this slab to
store certain user-land information [52]. As a result, this
vulnerability enables attackers to overwrite a page next to
the slab by transferring large size data into the kernel ob-
ject. We then launched three types of heap-based buffer
overﬂow attacks, respectively. Each attack was executed 10
times and Kruiser detected all these overﬂows successfully.
In addition to the synthetic attacks, we also ex-
ploited two real-world heap buffer overﬂow vulnera-
bilities [57, 58]
the ﬁrst one, we
sent particularly crafted ASN.1 BER data to trigger a
heap overﬂow.
In the second test, we used a special
eCryptfs ﬁle whose encrypted key size is larger than
ECRYPTFS MAX ENCRYPTED KEY BYTES to over-
ﬂow a buffer. Kruiser detected all the realistic overﬂows.
in Linux.
For
The above experimental results indicate that Kruiser is
effective in defending against kernel heap buffer overﬂow
attacks.
7.2 Performance and Scalability
To evaluate the performance of our monitoring mecha-
nism, we carried out a set of experiments. Each of these
experiments was conducted in three different environments,
including original Linux, Kruiser with SIM protection (re-
ferred as SIM-Kruiser subsequently), and Kruiser without
SIM protection.
In the ﬁrst experiment, we executed the SPEC CPU2006
Integer benchmark suite. Figure 9 shows that the average
performance overhead for both Kruiser and SIM-Kruiser
are negligible. When the slab allocation is frequent, the
SIM-Kruiser
Kruiser
1.04
1.02
1
0.98
0.96
0.94
0.92
0.9
e
m
i
t
n
o
i
t
u
c
e
x
E
perlbench
bzip2
gcc
m cf
gob m k
h m
m er
libquantu m
sjeng
h264ref
o m netpp
astar
xalancb m k
geo. m ean
Figure 9. SPEC CPU2006 performance (nor-
malized to the execution time of original
Linux).
Original
SIM-Kruiser
Kruiser
d
n
o
c
e
s
r
e
p
s
t
s
e
u
q
e
R
1000
900
800
700
600
500
400
300
200
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Concurrency
Figure 10. Throughput of the Apache web
server for varying numbers of concurrent re-
quests.
performance overhead is a little bit higher, such as in gcc;
however, the maximal performance overhead is less than
3%.
For the scalability measurement, we tested the through-
put of the Apache web server with concurrent requests.
Speciﬁcally, we ran Apache 2.2.8 to serve a 3.7KB html
web page. We used ApacheBench 2.3 running on another
machine—a Dell PowerEdge T300 Server with a 1.86G In-
tel E6305 CPU, 4 GB memory and Ubuntu 8.04 (linux-
2.6.24)—to measure the Apache throughput over a GB
LAN network. Each time we issued 10k http requests with
various numbers of concurrent clients, and we observed that
the number of the kernel heap buffer object allocation in-
creases along with the concurrency level. As shown in Fig-
ure 10, the performance overhead imposed by Kruiser and
SIM-Kruiser are both relatively stable. On average, Kruiser
only incurs about 3.8% performance degradation and SIM-
Kruiser about 7.9%.
Table 1. Different cruising cycle for different
applications in the SPEC CPU2006 bench-
mark (The cruising number refers to the num-
ber of kernel objects that are scanned in each
cruising cycle).
Benchmark
perlbench
bzip2
gcc
mcf
gobmk
hmmer
sjeng
libquantum
h264ref
omnetpp
astar
xalancbmk
Maximum Minimum Average Average
cruising
cruising cruising cruising
number number number cycle(µs)
39,259
107,824
27,662
79,085
27,774
78,460
82,885
28,156
28,606
80,761
28,635
81,278
28,610
81,437
80,911
28,493
28,572
80,756
28,836
82,109
28,897
81,592
99,436
30,190
105,145 106,378
76,325 76,682
76,810 77,413
79,328 79,540
80,345 80,519
80,435 80,591
80,259 80,535
80,317 80,407
80,337 80,480
80,796 81,088
81,022 81,097
82,747 88,454
7.3 Detection Latency
we recorded the average cruising cycles (i.e., the average
time for scanning all the PIA entries) for different applica-
tions in SPEC CPU2006, in order to evaluate the detection
latency, which is less than or equal to the cruising cycle at
the attack time. As shown in Table 1, 10 of 12 applications’
average cruising cycles are shorter than 29 ms, and the other
two applications’ are below 40 ms. We also recorded the
number of scanned kernel objects in each cruising cycle.
The results indicate that the average cruising cycle is mainly
determined by the average number of scanned kernel ob-
jects. Let N be the number of scanned kernel objects and T
the average time for the monitor process to check a kernel
object. We have C = N T , where C is the cruising cycle.
We can reduce the cruising cycle by keeping N small. One
approach is to divide the PIA entries into different parts, and
for each part, we create a separate monitor process. Another
approach is to only monitor objects in general caches. This
is practical because attackers mainly exploit this category
of buffers in the real world.
8 Discussion
8.1 Scalable Monitoring
For 32-bit OSes the ﬂat PIA array structure is feasible.
However, for a 64-bit system with TBes of physical mem-
ory, the memory overhead due to the PIA structure may be
not desirable. In addition, when page frames serving for the
kernel heap are sparse, Kruiser has to walk a long distance
before encountering a heap page, which implies heavy inef-
fective checking.
Both concerns can be resolved by extending the PIA ar-
ray to a multi-level table structure, the idea of which is in-
spired by the page table structure. The ﬁrst-level PIA table
is a simple array occupying one page frame. Each entry of a
PIA table except for the last-level one stores the address of
a next-level table and other information including the count
of non-zero entries in the next-level table; the structure of
last-level PIA tables are the same as a PIA array.
If the
count is zero, the entry does not point to any PIA table.
Speciﬁcally, when a page with address A is to be added
into the heap page pool, the most signiﬁcant serveral bits
of A are used as an index to locate the entry in the ﬁrst-
level PIA table. If the entry is empty, a next-level PIA table
is allocated and the entry is ﬁlled with the address of the
new PIA table and the count value 1. The remaining bits
of A are used to locate the following levels of PIA table,
until the entry in the last-level PIA table is located, and the
metadata of page A is then recorded there. For a 64-bit
system, a three-level PIA structure sufﬁces. When the mon-
itor process traverses along the PIA directory using a depth-
ﬁrst-search algorithm, it bypasses empty PIA entries corre-
sponding large bulks of contiguous pages. To prevent race
conditions when multiple processes accessing the same PIA
entry, CAS (Compare-And-Swap) instructions are needed.
The extended multi-level PIA structure not only reduces
memory overhead but also accelerate the cruise cycle.
It
is similar to the multi-level page table structure; like the
page table used in 64-bit systems, the PIA structure and ac-
cordingly the monitoring are scalable for systems with large
address spaces and physical memory.
8.2 Viable Deployment
Large data centers using shipping-containers packed
with thousands of servers each are common nowadays.
Therefore, scalable deployment is a critical requirement for
intrusion detection measures in data centers. Unlike tra-
ditional interposition-based monitors, which may intervene
normal functionalities frequently, Kruiser imposes minimal
interference and performs monitoring in parallel with the
monitored VM. Moreover, one Kruiser instance is able
to monitor multiple VMs given an acceptable detection la-
tency much longer than the cruising cycle, without affecting
the guaranteed detection property. In addition, the perfor-
mance isolation provided by the underlying VMM ensures
the monitor process and the monitored VM do not abuse
computing resources to interfere with each other, which is a
desirable property for users.
With the popularity of multi-core architectures, servers
built with many cores are more and more common. The
hardware evolution trend embraces the concurrent monitor-
ing fashion, as the cost for a unit core running a monitor in-
stance decreases sharply, and the extra energy consumption
by one core is relatively low for machines with hundreds
of cores. Therefore, the scalability and low cost properties
imply that Kruiser can be practically applied to large data
centers and server farms.
9 Related work
9.1 Countermeasures Against Buffer Overﬂows