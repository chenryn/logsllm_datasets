title:Once is Never Enough: Foundations for Sound Statistical Inference
in Tor Network Experimentation
author:Rob Jansen and
Justin Tracey and
Ian Goldberg
Once is Never Enough: Foundations for Sound 
Statistical Inference in Tor Network Experimentation
Rob Jansen, U.S. Naval Research Laboratory; Justin Tracey and 
Ian Goldberg, University of Waterloo
https://www.usenix.org/conference/usenixsecurity21/presentation/jansen
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.Once is Never Enough: Foundations for Sound
Statistical Inference in Tor Network Experimentation
Rob Jansen
U.S. Naval Research Laboratory
PI:EMAIL
Justin Tracey
University of Waterloo
PI:EMAIL
Ian Goldberg
University of Waterloo
PI:EMAIL
Abstract
Tor is a popular low-latency anonymous communication sys-
tem that focuses on usability and performance: a faster net-
work will attract more users, which in turn will improve the
anonymity of everyone using the system. The standard prac-
tice for previous research attempting to enhance Tor perfor-
mance is to draw conclusions from the observed results of
a single simulation for standard Tor and for each research
variant. But because the simulations are run in sampled Tor
networks, it is possible that sampling error alone could cause
the observed effects. Therefore, we call into question the
practical meaning of any conclusions that are drawn without
considering the statistical signiﬁcance of the reported results.
In this paper, we build foundations upon which we improve
the Tor experimental method. First, we present a new Tor
network modeling methodology that produces more repre-
sentative Tor networks as well as new and improved experi-
mentation tools that run Tor simulations faster and at a larger
scale than was previously possible. We showcase these con-
tributions by running simulations with 6,489 relays and 792k
simultaneously active users, the largest known Tor network
simulations and the ﬁrst at a network scale of 100%. Second,
we present new statistical methodologies through which we:
(i) show that running multiple simulations in independently
sampled networks is necessary in order to produce informa-
tive results; and (ii) show how to use the results from multiple
simulations to conduct sound statistical inference. We present
a case study using 420 simulations to demonstrate how to
apply our methodologies to a concrete set of Tor experiments
and how to analyze the results.
1 Introduction
Tor [15] is a privacy-enhancing technology and the most pop-
ular anonymous communication system ever deployed. Tor
consists of a network of relays that forward trafﬁc on be-
half of Tor users (i.e., clients) and Internet destinations. The
Tor Project estimates that there are about 2M daily active Tor
users [49], while recent privacy-preserving measurement stud-
ies estimate that there are about 8M daily active users [51]
and 792k simultaneously active users [38]. Tor is used for
a variety of reasons, including blocking trackers, defending
against surveillance, resisting ﬁngerprinting and censorship,
and freely browsing the Internet [69].
The usability of the Tor network is fundamental to the secu-
rity it can provide [14]; prior work has shown that real-world
adversaries intentionally degrade usability to cause users to
switch to less secure communication protocols [6]. Good us-
ability enables Tor to retain more users [18], and more users
generally corresponds to better anonymity [1]. Tor has made
improvements in three primary usability components: (i) the
design of the interface used to access and use the network
(i.e., Tor Browser) has been improved through usability stud-
ies [11, 46, 58]; (ii) the performance perceived by Tor users
has improved through the deployment of new trafﬁc schedul-
ing algorithms [37, 65]; and (iii) the network resources avail-
able for forwarding trafﬁc has grown from about 100 Gbit/s
to about 400 Gbit/s in the last 5 years [67]. Although these
changes have contributed to user growth, continued growth in
the Tor network is desirable—not only because user growth
improves anonymity [1], but also because access to informa-
tion is a universal and human right [72] and growth in Tor
means more humans can safely, securely, privately, and freely
access information online.
Researchers have contributed numerous proposals for im-
proving Tor performance in order to support continued growth
in the network, including those that attempt to improve Tor’s
path selection [5, 7, 13, 24, 28, 42, 47, 53, 59, 61, 62, 74, 76],
load balancing [22, 27, 31, 34, 41, 54], trafﬁc admission con-
trol [2, 16, 21, 23, 33, 35, 37, 43, 48, 75], and congestion con-
trol mechanisms [4, 20]. The standard practice when propos-
ing a new mechanism for Tor is to run a single experiment
with each recommended conﬁguration of the mechanism and
a single experiment with standard Tor. Measurements of a
performance metric (e.g., download time) are taken during
each experiment, the empirical distributions over which are
directly compared across experiments. Unfortunately, the ex-
periments (typically simulations or emulations [63]) are done
in scaled-down Tor test networks that are sampled from the
USENIX Association
30th USENIX Security Symposium    3415
state of the true network at a static point in time [32]; only a
single sample is considered even though in reality the network
changes over time in ways that could change the conclusions.
Moreover, statistical inference techniques (e.g., repeated tri-
als and interval estimates) are generally not applied during
the analysis of results, leading to questionable conclusions.
Perhaps due in part to undependable results, only a few Tor
performance research proposals have been deployed over the
years [37, 65] despite the abundance of available research.
Contributions: We advance the state of the art by building
foundations for conducting sound Tor performance research
in two major ways: (i) we design and validate Tor experi-
mentation models and develop new and improved modeling
and experimentation tools that together allow us to create
and run more representative Tor test networks faster than was
previously possible; and (ii) we develop statistical method-
ologies that enable sound statistical inference of experimenta-
tion results and demonstrate how to apply our methodologies
through a case study on a concrete set of Tor experiments.
Models and Tools: In §3 we present a new Tor network mod-
eling methodology that produces more representative Tor net-
works by considering the state of the network over time rather
than at a static point as was previously standard [32]. We
designed our modeling methodology to support the ﬂexible
generation of Tor network models with conﬁgurable network,
user, trafﬁc load, and process scale factors, supporting ex-
periments in computing facilities with a range of available
resources. We designed our modeling tools such that expen-
sive data processing tasks need only occur once, and the result
can be distributed to the Tor community and used to efﬁciently
generate any number of network models.
In §4 we contribute new and improved experimentation
tools that we optimized to enable us to run Tor experiments
faster and at a larger scale than was previously possible. In
particular, we describe several improvements we made to
Shadow [29], the most popular and validated platform for
Tor experimentation, and demonstrate how our Tor network
models and improvements to Shadow increase the scalability
of simulations. We showcase these contributions by running
the largest known Tor simulations—full-scale Tor networks
with 6,489 relays and 792k simultaneously active users. We
also run smaller-scale networks of 2,000 relays and 244k
users to compare to prior work: we observe a reduction in
RAM usage of 1.7 TiB (64%) and a reduction in run time of
33 days, 12 hours (94%) compared to the state of the art [38].
Statistical Methodologies: In §5 we describe a methodology
that enables us to conduct sound statistical inference using the
results collected from scaled-down (sampled) Tor networks.
We ﬁnd that running multiple simulations in independently
sampled networks is necessary in order to obtain statistically
signiﬁcant results, a methodology that has never before been
implemented in Tor performance research and causes us to
question the conclusions drawn in previous work (see §2.4).
We describe how to use multiple networks to estimate the
distribution of a random variable and compute conﬁdence
intervals over that distribution, and discuss how network sam-
pling choices would affect the estimation.
In §6 we present a case study in order to demonstrate how to
apply our modeling and statistical methodologies to conduct
sound Tor performance research. We present the results from
a total of 420 Tor simulations across three network scale
and two trafﬁc load factors. We ﬁnd that the precision of the
conclusions that can be drawn from the networks used for
simulations are dependent upon the scale of those networks.
Although it is possible to derive similar conclusions from
networks of different scales, fewer simulations are generally
required in larger-scale than smaller-scale networks to achieve
a similar precision. We conclude that one simulation is never
enough to achieve statistically signiﬁcant results.
Availability: Through this work we have developed new mod-
eling tools and improvements to Shadow that we have released
as open-source software as part of OnionTrace v1.0.0, TorNet-
Tools v1.1.0, TGen v1.0.0, and Shadow v1.13.2.1 We have
made these and other research artifacts publicly available.2
2 Background and Related Work
We provide a brief background on Tor before describing prior
work on Tor experimentation, modeling, and performance.
2.1 Tor
A primary function of the Tor network is to anonymize
user trafﬁc [15]. To accomplish this, the Tor network is com-
posed of a set of Tor relays that forward trafﬁc through the
network on behalf of users running Tor clients. Some of the
relays serve as directory authorities and are responsible for
publishing a network consensus document containing relay
information that is required to connect to and use the network
(e.g., addresses, ports, and ﬁngerprints of cryptographic iden-
tity keys for all relays in the network). Consensus documents
also contain a weight for each relay to support a weighted
path selection process that attempts to balance trafﬁc load
across relays according to relay bandwidth capacity. To use
the network, clients build long-lived circuits through a tele-
scoping path of relays: the ﬁrst in the path is called the guard
(i.e., entry), the last is called the exit, and the remaining are
called middle relays. Once a circuit is established, the client
sends commands through the circuit to the exit instructing it to
open streams to Internet destinations (e.g., web servers); the
request and response trafﬁc for these streams are multiplexed
over the same circuit. Another, less frequently used function
of the network is to support onion services (i.e., anonymized
servers) to which Tor clients can connect (anonymizing both
the client and the onion service to the other).
1https://github.com/shadow/{oniontrace,tornettools,tgen,shadow}
2https://neverenough-sec2021.github.io
3416    30th USENIX Security Symposium
USENIX Association
2.2 Tor Experimentation Tools
Early Tor experimentation tools included packet-level sim-
ulators that were designed to better understand the effects of
Tor incentive schemes [31, 57]. Although these simulators
reproduced some of Tor’s logic, they did not actually use Tor
source code and quickly became outdated and unmaintained.
Recognizing the need for a more realistic Tor experimentation
tool, researchers began developing tools following two main
approaches: network emulation and network simulation [63].
Network Emulation: ExperimenTor [8] is a Tor experimenta-
tion testbed built on top of the ModelNet [73] network emula-
tion platform. ExperimenTor consists of two components that
generally run on independent servers (or clusters): one com-
ponent runs client processes and the other runs the ModelNet
core emulator that connects the processes in a virtual network
topology. The performance of this architecture was improved
in SNEAC [64] through the use of Linux Containers and the
kernel’s network emulation module netem, while tooling and
network orchestration were improved in NetMirage [71].
Network Simulation: Shadow [29] is a hybrid discrete-event
network simulator that runs applications as plugins. We pro-
vide more background on Shadow in §4.1. Shadow’s origi-
nal design was improved with the addition of a user-space
non-preemptive thread scheduler [52], and later with a high
performance dynamic loader [70]. Additional contributions
have been made through several research projects [35, 37, 38],
and we make further contributions that improve Shadow’s ef-
ﬁciency and correctness as described in §4.2.
2.3 Tor Modeling
An early approach to model the Tor network was devel-
oped for both Shadow and ExperimenTor [32]. The modeling
approach produced scaled-down Tor test networks by sam-
pling relays and their attributes from a single true Tor network
consensus. As a result, the models are particularly sensitive
to short-term temporal changes in the composition of the
true network (e.g., those that result from natural relay churn,
network attacks, or misconﬁgurations). The new techniques
we present in §3.2 are more robust to such variation because
they are designed to use Tor metrics data spanning a user-
selectable time period (i.e., from any chosen set of consensus
ﬁles) in order to create simulated Tor networks that are more
representative of the true Tor network over time.
In previous models, the number of clients to use and their
behavior proﬁles were unknown, so ﬁnding a suitable com-
bination of trafﬁc generation parameters that would yield an
appropriate amount of background trafﬁc was often a chal-
lenging and iterative process. But with the introduction of
privacy-preserving measurement tools [17, 19, 30, 50] and
the recent publication of Tor measurement studies [30, 38, 51],
we have gained a more informed understanding of the traf-
ﬁc characteristics of Tor. Our new modeling techniques use
Markov models informed by (privacy-preserving) statistics
from true Tor trafﬁc [38], while signiﬁcantly improving ex-
periment scalability as we demonstrate in §4.3.
2.4 Tor Performance Studies
The Tor experimentation tools and models described above
have assisted researchers in exploring how changes to Tor’s
path selection [5, 7, 13, 24, 28, 42, 47, 53, 59, 61, 62, 74, 76],
load balancing [22, 27, 31, 34, 41, 54], trafﬁc admission con-
trol [2, 16, 21, 23, 33, 35, 37, 43, 48, 75], congestion con-
trol [4, 20], and denial of service mechanisms [12, 26, 36,
39, 60] affect Tor performance and security [3]. The standard
practice that has emerged from this work is to sample a single
scaled-down Tor network model and use it to run experiments
with standard Tor and each of a set of chosen conﬁgurations
of the proposed performance-enhancing mechanism. Descrip-
tive statistics or empirical distributions of the results are then
compared across these experiments. Although some stud-
ies use multiple trials of each experimental conﬁguration in
the chosen sampled network [34, 41], none of them involve
running experiments in multiple sampled networks, which is
necessary to estimate effects on the real-world Tor network
(see §5). Additionally, statistical inference techniques (e.g.,
interval estimates) are not applied during the analysis of the
results, leading to questions about the extent to which the con-
clusions drawn in previous work are relevant to the real world.
Our work advances the state of the art of the experimental
process for Tor performance research: in §5 we describe new
statistical methodologies that enable researchers to conduct
sound statistical inference from Tor experimentation results,
and in §6 we present a case study to demonstrate how to put
our methods into practice.
3 Models for Tor Experimentation
In order to conduct Tor experiments that produce meaningful
results, we must have network and trafﬁc models that accu-
rately represent the composition and trafﬁc characteristics of
the Tor network. In this section, we describe new modeling
techniques that make use of the latest data from recent privacy-
preserving measurement studies [30, 38, 51]. Note that while
exploring alternatives for every modeling choice that will be