# Title: Masking Failures from Application Performance in Data Center Networks with Shareable Backup

## Authors:
- Dingming Wu
- Yiting Xia
- Xiaoye Steven Sun
- Xin Sunny Huang
- Simbarashe Dzinamarira
- T. S. Eugene Ng

### Abstract
Shareable backup is an economical and effective method for masking failures from application performance in data center networks. By sharing a small number of backup switches network-wide, the system can quickly recover to full capacity without applications noticing the failures. This approach avoids the complications and ineffectiveness associated with rerouting. We introduce ShareBackup, a prototype architecture that realizes this concept, and present its detailed design. Implemented on a hardware testbed, ShareBackup achieves failure recovery in just 0.73ms, causing no disruption to routing. It accelerates Spark and Tez jobs by up to 4.1× under failure conditions. Large-scale simulations using real data center traffic and failure models show that ShareBackup reduces the percentage of job flows prolonged by failures from 47.2% to as little as 0.78%. In all our experiments, the performance of ShareBackup closely matches that of the no-failure case.

### CCS Concepts
- Networks → Physical topologies; Network reliability; Data center networks

### Keywords
- Data Center Network
- Failure Recovery
- Circuit Switching

*The first two authors contributed equally to the paper.
†Work done while at Rice University.

### Introduction
The ultimate goal of failure recovery in data center networks is to preserve application performance. In this paper, we propose shareable backup as a groundbreaking solution. Shareable backup allows the entire data center to share a pool of backup switches. If any switch in the network fails, a backup switch can be brought online to replace it, ensuring that the failover is fast enough to avoid disrupting applications.

Shareable backup addresses the limitations of rerouting, the mainstream solution for fault tolerance in data center networks. Rerouting maintains connectivity but often degrades bandwidth, leading to contention with other traffic and slow convergence. This latency is particularly harmful to interactive applications with strict deadlines. Additionally, rerouting risks misconfigurations and other overheads such as slow failure propagation and excessive state exchange.

According to a study of a production data center, 10% less traffic is delivered in the median case of analyzed failures, and 40% less for the worst 20% of failures. Our simulations show that 42% of jobs get slowed down by at least 3×, 51% miss deadlines, and 21.3% of flows not on the path of failure still get affected due to rerouting.

Shareable backup is cost-effective because failures in data centers are rare and transient. The study shows that most devices have over 99.99% availability, and failures usually last only a few minutes. With shareable backup, we achieve network-wide backup at low cost, which is impossible with traditional 1:1 backup requiring a dedicated spare for each switch.

Circuit switches, used in many novel network architectures, facilitate physical-layer topology adaptation. By connecting the pool of backup switches and all network switches to a circuit switch, any switch can be replaced by changing the circuit switch connections. However, scaling up circuit switches increases insertion loss. Instead, recent proposals scale out low-cost, modest-size circuit switches by distributing them across the network. We adopt this approach to partition the network into smaller failure groups and implement shareable backup in each group.

In this work, we design ShareBackup, a prototype architecture for fat-tree, a typical data center network topology. We have implemented ShareBackup and competing solutions on a hardware testbed, a Linear Programming simulator, and a packet-level simulator. Extensive evaluations, including TCP convergence, control system latency, bandwidth capacity, and transmission performance with real traffic and failure models, demonstrate the benefits of ShareBackup.

### Key Properties of ShareBackup
- **Failure Recovery:** Only takes 0.73ms, combining hardware and control system latencies.
- **Bandwidth Restoration:** Restores bandwidth to full capacity after failures without disturbing routing.
- **Performance Consistency:** Performance difference with the no-failure case is negligible, proving its ability to mask failures from application performance.
- **Application Acceleration:** Under failures, accelerates Spark and Tez jobs by up to 4.1× and reduces the percentage of job flows slowed down by failures from 47.2% to 0.78%.

### Related Work
Data center network architectures rely on redundant paths for failure resilience, with fat-tree being the most popular. ShareBackup builds on fat-tree and is related to other proposals enhancing fault-tolerance. PortLand, F10, and Aspen Tree use rerouting, local rerouting, and additional redundancy, respectively. ShareBackup, however, deploys backup switches in the physical layer, avoiding the complexities of rerouting.

Other works tackle failures through operational guidance (NetPilot, CorrOpt) or multi-homing (Subways, Hot Standby Router Protocol). ShareBackup provides more efficient redundancy without multi-homing and uses a lightweight VLAN-based solution for hot standbys. Unlike rerouting, ShareBackup avoids the explosion of backup routes and forwarding table compression issues.

On the application level, intelligent service placement and task restarts in frameworks like Spark and Tez provide some fault tolerance. However, in-network failure recovery is crucial, as shown by our experiments where application-level resilience alone leads to significant performance degradation if hosts are disconnected.

### Network Architecture
ShareBackup has stringent requirements for cost and failure recovery delay, guiding our choice of circuit switch technologies. No existing circuit switch has enough ports to connect all data center switches and the backup pool. Cascading multiple circuit switches is impractical due to high insertion loss. Instead, we distribute low-cost, modest-size circuit switches across the network, partitioning it into smaller failure groups and implementing shareable backup in each group.

### Algorithm: ShareBackup Wiring
```python
# Edge Layer
for i in range(k):
    for j in range(k // 2):
        edge_switch = f"Edg{i},{j}"
        for p in range(k // 2):
            DOWN[p] = f"Host{k * p + j + i * (k // 2)}"
            UP[p] = f"Edg{i},{p}"
            connect(DOWN[p], UP[p])
        for p in range(k // 2, k // 2 + n):
            UP[p] = f"BS1,{i},{p - k // 2}"
            connect(DOWN[p], UP[p])

# Aggregation Layer
for i in range(k):
    for j in range(k // 2):
        agg_switch = f"Agr{i},{j}"
        for p in range(k // 2):
            DOWN[p] = f"Edg{i},{p}"
            UP[p] = f"Agr{i},{p}"
            connect(DOWN[p], UP[p])
```

This algorithm ensures that the network is wired correctly to support the shareable backup mechanism, allowing for rapid and seamless failover in the event of a switch failure.