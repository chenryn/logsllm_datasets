title:Masking failures from application performance in data center networks
with shareable backup
author:Dingming Wu and
Yiting Xia and
Xiaoye Steven Sun and
Xin Sunny Huang and
Simbarashe Dzinamarira and
T. S. Eugene Ng
Masking Failures from Application Performance in
Data Center Networks with Shareable Backup
Dingming Wu*
Rice University
PI:EMAIL
Xin Sunny Huang
Rice University
PI:EMAIL
Yiting Xia*†
Facebook, Inc.
PI:EMAIL
Xiaoye Steven Sun
Rice University
PI:EMAIL
Simbarashe Dzinamarira
Rice University
PI:EMAIL
T. S. Eugene Ng
Rice University
PI:EMAIL
ABSTRACT
Shareable backup is an economical and effective way to mask
failures from application performance. A small number of
backup switches are shared network-wide for repairing fail-
ures on demand so that the network quickly recovers to its
full capacity without applications noticing the failures. This
approach avoids complications and ineffectiveness of rerout-
ing. We propose ShareBackup as a prototype architecture to
realize this concept and present the detailed design. We imple-
ment ShareBackup on a hardware testbed. Its failure recovery
takes merely 0.73ms, causing no disruption to routing; and
it accelerates Spark and Tez jobs by up to 4.1× under fail-
ures. Large-scale simulations with real data center trafﬁc and
failure model show that ShareBackup reduces the percentage
of job ﬂows prolonged by failures from 47.2% to as little as
0.78%. In all our experiments, the results for ShareBackup
have little difference from the no-failure case.
CCS CONCEPTS
• Networks → Physical topologies; Network relia-
bility; Data center networks;
KEYWORDS
Data Center Network; Failure Recovery; Circuit Switching
*The ﬁrst two authors contributed equally to the paper.
†Work done while at Rice University.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than the author(s) must be honored. Abstracting
with credit is permitted. To copy otherwise, or republish, to post on servers or
to redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
© 2018 Copyright held by the owner/author(s). Publication rights licensed to
ACM.
ACM ISBN 978-1-4503-5567-4/18/08. . . $15.00
https://doi.org/10.1145/3230543.3230577
176
ACM Reference Format:
Dingming Wu, Yiting Xia, Xiaoye Steven Sun, Xin Sunny Huang,
Simbarashe Dzinamarira, and T. S. Eugene Ng. 2018. Masking
Failures from Application Performance in Data Center Networks
with Shareable Backup. In SIGCOMM ’18: ACM SIGCOMM 2018
Conference, August 20–25, 2018, Budapest, Hungary. ACM, New
York, NY, USA, 15 pages. https://doi.org/10.1145/3230543.3230577
INTRODUCTION
1
The ultimate goal of failure recovery in data center networks is
to preserve application performance. In this paper, we propose
shareable backup as a ground-breaking solution towards that
goal. Shareable backup allows the entire data center to share a
pool of backup switches. If any switch in the network fails, a
backup switch can be brought online to replace it. The failover
should be fast enough to avoid disruption to applications.
With the power of shareable backup, it is possible for the ﬁrst
time to repair failures instantly instead of making do with a
crippled network.
Shareable backup is a natural quest due to ineptness of
rerouting, the mainstream solution to fault tolerance in data
center networks [7, 8, 17–19, 26, 30, 38, 43]. While rerouting
maintains connectivity, bandwidth is nonetheless degraded un-
der failures. The rerouted trafﬁc may contend with other trafﬁc
originally on the path, thus enlarging the effect of failure to a
wider range of the network. Routing convergence is known to
be slow [12], and even path re-computation on a centralized
management entity is expensive [30, 38]. This latency is espe-
cially harmful to interactive applications with rigid deadlines.
Rerouting also risks misconﬁgurations when updating routing
tables, which may cause the network to dysfunction. Not to
mention other overheads, e.g. slow failure propagation, longer
alternative paths, excessive state exchange, etc.
With all these factors, the application performance may
be jeopardized drastically. According to a failure study of a
path-rich production data center, 10% less trafﬁc is delivered
for the median case of the analyzed failures, and 40% less for
the worst 20% of failures [16]. Injecting these failures into
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
D. Wu et al.
our simulation of a real data center setting (Section 6.7), 42%
jobs get slowed down by at least 3× (Figure 9(b)), 51% jobs
miss deadlines (Figure 10(b)), and 21.3% ﬂows not on the
path of failure still get affected because of rerouting (Table 4).
Shareable backup is desirable for its cost-effectiveness. The
pool of backup switches needs not be large in practice, be-
cause failures in data centers are rare and transient. The above
failure study shows most devices have over 99.99% availabil-
ity; and failures usually last for only a few minutes [16]. With
shareable backup, we for the ﬁrst time achieve network-wide
backup at low cost, which is impossible for the traditional 1:1
backup that requires a dedicated spare for each switch.
Shareable backup is achievable by circuit switches, which
have been used to facilitate physical-layer topology adaptation
in many novel network architectures [13–15, 23, 28, 32, 44,
50]. Theoretically, if the pool of backup switches and all the
switches in the network are connected to a circuit switch,
any switch can then be replaced as we change the circuit
switch connections. However, a circuit switch has limited
port count, and layering multiple circuit switches to scale
up increases insertion loss. Rather than scaling up, recent
proposals scale out low-cost modest-size circuit switches by
distributed placement of them across the network [23, 50].
We adopt this approach to partition the network into smaller
failure groups and realize shareable backup in each group.
In this work, we design a prototype architecture, named
ShareBackup, to explore the feasibility of shareable backup
on fat-tree [8], a typical network topology found in data cen-
ters [5, 36]. We have implemented ShareBackup and its com-
peting solutions on a hardware testbed, a Linear Programming
simulator, and a packet-level simulator. We have conducted
extensive evaluations including TCP convergence, control sys-
tem latency, bandwidth capacity, transmission performance at
scale with real trafﬁc and failure model, and beneﬁts to Spark
and Tez jobs on the testbed.
The key properties of ShareBackup are: (1) failure recov-
ery only takes 0.73ms, latencies from hardware and control
system combined; (2) it restores bandwidth to full capacity
after failures, and routing is not disturbed; (3) for all our ex-
periments, its performance difference with the no-failure case
is negligible, proving its ability to mask failures from appli-
cation performance; (4) under failures, it accelerates Spark
and Tez jobs by upto 4.1× and reduces the percentage of job
ﬂows slowed down by failures in the large-scale simulation
from 47.2% to 0.78%.
2 RELATED WORK
Data center network architectures rely on rich redundant paths
for failure resilience [7, 8, 17–19, 38]. Among them, fat-tree
is the most popular in practical use [8]. ShareBackup builds
on top of fat-tree, so it is related to other proposals enhancing
177
fault-tolerance of fat-tree networks. PortLand reroutes traf-
ﬁc to globally optimal paths based on a central view of the
network at the fabric manager. F10 reduces delays from fail-
ure propagation and path re-computation by local rerouting
at switches [26], at the cost of longer paths. It also adjusts
wiring of fat-tree to form AB fat-tree, which provides di-
verse paths for local rerouting. Aspen Tree adds different
degrees of redundancy to fat-tree to tune the local rerouting
path length [43]. It either partitions the network or adds ex-
tra switches to have more paths. If keeping the host count,
it requires at least one more layer, or 40% more switches.
ShareBackup takes a completely different approach. Instead
of rerouting, it deploys backup switches in the physical layer.
We compare with PortLand, F10, and Aspen Tree in the eval-
uations to explore interesting properties of ShareBackup.
Besides architectural solutions, many works have been tack-
ling failures in data centers from different angles. NetPilot and
CorrOpt give operational guidance to manually mitigating the
effect of failures [47, 52]. ShareBackup instead automatically
replaces failed switches to restore full capacity of the network.
Its recovery speed is also signiﬁcantly faster, e.g. sub-ms vs.
tens of minutes. Subways and Hot Standby Router Protocol
suggest multi-homing hosts to several switches to avoid the
single point of failure [27, 41], which consumes more ports
on the hosts and switches. ShareBackup provides more efﬁ-
cient redundancy at the network edge without multi-homing,
and we invent a more light-weight VLAN-based solution to
make backup switches hot standbys with no additional latency
(Section 4.4). In the context of rerouting, there is a large body
of work on local fast failover [11, 22, 25, 29, 31, 33, 39, 51],
some of which cause the explosion of backup routes and
Plinko introduces a forwarding table compression algorithm
accordingly [40]. ShareBackup does not depend on rerout-
ing for failure recovery, so it avoids these complications and
the forwarding tables are intrinsically small. On the applica-
tion level, Bodik et al. propose intelligent service placement
for both fault tolerance and trafﬁc locality [10], and com-
putation frameworks such as Spark [1] and Tez [2] restart
tasks elsewhere when workers are lost. ShareBackup pro-
vides a more reliable network, so service placement has less
constraints. Our experiment in Section 6.8 shows application-
level resilience is insufﬁcient: the performance is degraded
by multi-folds if hosts are disconnected. Thus, in-network
failure recovery is extremely important.
3 NETWORK ARCHITECTURE
ShareBackup has stringent requirements on cost and failure
recovery delay, which guide our choice of circuit switch tech-
nologies. No existing circuit switch has enough ports to con-
nect to all switches in the data center plus the pool of backup
switches. Cascading multiple circuit switches wastes many
Masking Failures from Application Performance
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Algorithm 1 ShareBackup wiring algorithm
1: for each CS1,i, j where 0 ≤ i < k, 0
// Edge layer
≤ j < k
2 do
9: for each CS2,i, j where 0 ≤ i < k, 0
// Aggregation layer
≤ j < k
2:
3:
4:
5:
6:
7:
8:
Edдei, j ∈ FG1,i
for each p in 0 ≤ p < k
DOWNp←→Host k
UPp ←→ Edдei,p
DOWNp (cid:2)(cid:3) UPp
≤ p < k
for each p in k
UPp ←→ BS
1,i,p− k
2
2
2
2
2 do
×p+j+i×( k
2
+ n do
)2
10:
11:
12:
13:
14:
15:
16:
17:
2 do
Aддi, j ∈ FG2,i
for each p in 0 ≤ p < k
2 do