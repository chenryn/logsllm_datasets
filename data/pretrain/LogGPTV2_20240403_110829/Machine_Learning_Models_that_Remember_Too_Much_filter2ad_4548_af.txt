classes of the synthetic data correspond to the malicious task, i.e.,
given a specific synthetic image, the class encodes a secret about
the training images. This demonstrates that the model has learned
both its primary task and the malicious task well.
7 COUNTERMEASURES
Detecting that a training algorithm is attempting to memorize
sensitive data within the model is not straightforward because, as
we show in this paper, there are many techniques and places for
encoding this information: directly in the model parameters, by
applying a malicious regularizer, or by augmenting the training data
with specially crafted inputs. Manual inspection of the code may
not detect malicious intent, given that many of these approaches
are similar to standard ML techniques.
An interesting way to mitigate the LSB attack is to turn it against
itself. The attack relies on the observation that lower bits of model
parameters essentially don’t matter for model accuracy. Therefore,
a client can replace the lower bits of the parameters with random
noise. This will destroy any information potentially encoded in
these bits without any impact on the model’s performance.
Maliciously trained models may exhibit anomalous parameter
distributions. Figure 6 compares the distribution of parameters in a
conventionally trained model, which has the shape of a zero-mean
Gaussian, to maliciously trained models. As expected, parameters
generated by the correlated value encoding attack are distributed
study, whether a specific known genome was used in the study.
This is known as the membership inference problem. Subsequent
work extended this work to published noisy statistics [24] and
MicroRNA-based studies [5].
Membership inference attacks against supervised ML models
were studied by Shokri et al. [68]. They use black-box access to a
model fθ to determine whether a given labeled feature vector (x, y)
was a member of the training set used to produce θ. Their attacks
work best when fθ has low generalizability, i.e., if the accuracy for
the training inputs is much better than for inputs from outside the
training dataset.
By contrast, we study how a malicious training algorithm can
intentionally create a model that leaks information about its training
dataset. The difference between membership inference and our
problem is akin to the difference between side channels and covert
channels. Our threat model is more generous to the adversary,
thus our attacks extract substantially more information about the
training data than any prior work. Another important difference is
we aim to create models that generalize well yet leak information.
Evasion and poisoning. Evasion attacks seek to craft inputs that
will be misclassified by a ML model. They were first explored in
the context of spam detection [28, 50, 51]. More recent work inves-
tigated evasion in other settings such as computer vision—see a
survey by Papernot et al. [63]. Our work focuses on the confiden-
tiality of training data rather than evasion, but future work may
investigate how malicious ML providers can intentionally create
models that facilitate evasion.
Poisoning attacks [9, 18, 38, 57, 65] insert malicious data points
into the training dataset to make the resulting model easier to evade.
This technique is similar in spirit to the malicious data augmenta-
tion in our capacity-abuse attack (Section 5). Our goal is not evasion,
however, but forcing the model to leak its training data.
Secure ML environments. Starting with [49], there has been much
research on using secure multi-party computation to enable several
parties to create a joint model on their separate datasets, e.g. [11,
16, 22]. A protocol for distributed, privacy-preserving deep learn-
ing was proposed in [67]. Abadi et al. [1] describe how to train
differentially private deep learning models. Systems using trusted
hardware such as SGX protect training data while training on an
untrusted service [21, 61, 66]. In all of these works, the training
algorithm is public and agreed upon, and our attacks would work
only if users are tricked into using a malicious algorithm.
CQSTR [74] explicitly targets situations in which the training
algorithm may not be entirely trustworthy. Our results show that in
such settings a malicious training algorithm can covertly exfiltrate
significant amounts of data, even if the output is constrained to be
an accurate and usable model.
Privacy-preserving classification protocols seek to prevent dis-
closure of the user’s input features to the model owner as well as
disclosure of the model to the user [12]. Using such a system would
prevent our white-box attacks, but not black-box attacks.
ML model capacity and compression. Our capacity-abuse attack
takes advantage of the fact that many models (especially deep neu-
ral networks) have huge memorization capacity. Zhang et al. [75]
showed that modern ML models can achieve (near) 100% training
accuracy on datasets with randomized labels or even randomized
Figure 5: Visualization of the learned features of a CIFAR10
model maliciously trained with our capacity-abuse method.
Solid points are from the original training data, hollow
points are from the synthetic data. The color indicates the
point’s class.
very differently. Parameters generated by the sign encoding at-
tack are more centered at zero, which is similar to the effect of
conventional l1-norm regularization (which encourages sparsity in
the parameters). To detect these anomalies, the data owner must
have a prior understanding of what a “normal” parameter distribu-
tion looks like. This suggests that deploying this kind of anomaly
detection may be challenging.
Parameters generated by the capacity-abuse attack are not visibly
different. This is expected because training works exactly as before,
only the dataset is augmented with additional inputs.
8 RELATED WORK
Privacy threats in ML. No prior work considered malicious learn-
ing algorithms aiming to create a model that leaks information
about the training dataset.
Ateniese et al. [4] show how an attacker can use access to an ML
model to infer a predicate of the training data, e.g., whether a voice
recognition system was trained only with Indian English speakers.
Fredrikson et al. [26] explore model inversion: given a model
fθ that makes a prediction y given some hidden feature vector
x1, . . . , xn, they use the ground-truth label ˜y and a subset of x1, . . . , xn
to infer the remaining, unknown features. Model inversion oper-
ates in the same manner whether the feature vector x1, . . . , xn is
in the training dataset or not, but empirically performs better for
training set points due to overfitting. Subsequent model inversion
attacks [25] show how, given access to a face recognition model, to
construct a representative of a certain output class (a recognizable
face when each class corresponds to a single person).
In contrast to the above techniques, our objective is to extract
specific inputs that belong to the training dataset which was used
to create the model.
Homer et al. [32] developed a technique for determining, given
published summary statistics about a genome-wide association
Figure 6: Comparison of parameter distribution between a benign model and malicious models. Left is the correlation encoding
attack (cor); middle is the sign encoding attack (sgn); right is the capacity abuse attack (cap). The models are residual networks
trained on CIFAR10. Plots show the distribution of parameters in the 20th layer.
features. They argue that this undermines previous interpretations
of generalization bounds based on training accuracy.
Our capacity-abuse attack augments the training data with (es-
sentially) randomized data and relies on the resulting low training
error to extract information from the model. Crucially, we do this
while simultaneously training the model to achieve good testing
accuracy on its primary, non-adversarial task.
Our LSB attack directly takes advantages of the large number
and unnecessarily high precision of model parameters. Several
papers investigated how to compress models [13, 15, 29]. An in-
teresting topic of future work is how to use these techniques as a
countermeasure to malicious training algorithms.
9 CONCLUSION
We demonstrated that malicious machine learning (ML) algorithms
can create models that satisfy the standard quality metrics of ac-
curacy and generalizability while leaking a significant amount of
information about their training datasets, even if the adversary has
only black-box access to the model.
ML cannot be applied blindly to sensitive data, especially if the
model-training code is provided by another party. Data holders
cannot afford to be ignorant of the inner workings of ML systems
if they intend to make the resulting models available to other users,
directly or indirectly. Whenever they use somebody else’s ML sys-
tem or employ ML as a service (even if the service promises not
to observe the operation of its algorithms), they should demand to
see the code and understand what it is doing.
In general, we need “the principle of least privilege” for machine
learning. ML training frameworks should ensure that the model
captures only as much about its training dataset as it needs for its
designated task and nothing more. How to formalize this principle,
how to develop practical training methods that satisfy it, and how to
certify these methods are interesting open topics for future research.
Funding acknowledgments. This research was partially supported
by NSF grants 1611770 and 1704527, as well as research awards
from Google, Microsoft, and Schmidt Sciences.
REFERENCES
[1] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and
[2] Algorithmia. https://algorithmia.com, 2017.
L. Zhang. Deep learning with differential privacy. In CCS, 2016.
[3] Amazon Machine Learning. https://aws.amazon.com/machine-learning, 2017.
[4] G. Ateniese, L. V. Mancini, A. Spognardi, A. Villani, D. Vitali, and G. Felici.
Hacking smart machines with smarter ones: How to extract meaningful data
from machine learning classifiers. IJSN, 10(3):137–150, 2015.
[5] M. Backes, P. Berrang, M. Humbert, and P. Manoharan. Membership privacy in
MicroRNA-based studies. In CCS, 2016.
[6] M. Balduzzi, J. Zaddach, D. Balzarotti, E. Kirda, and S. Loureiro. A security
analysis of Amazon’s Elastic Compute cloud service. In SAC, 2012.
[7] A. Baumann, M. Peinado, and G. Hunt. Shielding applications from an untrusted
cloud with haven. TOCS, 33(3):8, 2015.
[8] A. L. Berger, V. J. D. Pietra, and S. A. D. Pietra. A maximum entropy approach to
natural language processing. Computational Linguistics, 22(1):39–71, 1996.
[9] B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks against support vector
machines. In ICML, 2012.
[10] BigML. https://bigml.com, 2017.
[11] D. Bogdanov, M. Niitsoo, T. Toft, and J. Willemson. High-performance secure
multi-party computation for data mining applications. IJIS, 11(6):403–418, 2012.
[12] R. Bost, R. A. Popa, S. Tu, and S. Goldwasser. Machine learning classification
over encrypted data. In NDSS, 2015.
[13] C. Bucilă, R. Caruana, and A. Niculescu-Mizil. Model compression. In KDD, 2006.
[14] S. Bugiel, S. Nürnberger, T. Pöppelmann, A.-R. Sadeghi, and T. Schneider. Ama-
zonIA: When elasticity snaps back. In CCS, 2011.
[15] W. Chen, J. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen. Compressing
convolutional neural networks in the frequency domain. In KDD, 2016.
[16] C. Clifton, M. Kantarcioglu, J. Vaidya, X. Lin, and M. Y. Zhu. Tools for privacy
preserving distributed data mining. ACM SIGKDD Explorations Newsletter, 4(2):28–
34, 2002.
[17] C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20(3):273–
[18] N. Dalvi, P. Domingos, Mausam, S. Sanghai, and D. Verma. Adversarial classifica-
297, 1995.
tion. In KDD, 2004.
[19] DeepDetect. https://www.deepdetect.com, 2015–2017.
[20] S. Dieleman, J. Schlüter, C. Raffel, E. Olson, S. K. SÃÿnderby, D. Nouri, et al.
Lasagne: First release. http://dx.doi.org/10.5281/zenodo.27878, 2015.
[21] T. T. A. Dinh, P. Saxena, E.-C. Chang, B. C. Ooi, and C. Zhang. M2R: Enabling
stronger privacy in MapReduce computation. In USENIX Security, 2015.
[22] W. Du, Y. S. Han, and S. Chen. Privacy-preserving multivariate statistical analysis:
Linear regression and classification. In ICDM, 2004.
[23] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online
learning and stochastic optimization. JMLR, 12(Jul):2121–2159, 2011.
[24] C. Dwork, A. Smith, T. Steinke, J. Ullman, and S. Vadhan. Robust traceability
from trace amounts. In FOCS, 2015.
[25] M. Fredrikson, S. Jha, and T. Ristenpart. Model inversion attacks that exploit
confidence information and basic countermeasures. In CCS, 2015.
[26] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and T. Ristenpart. Privacy in
pharmacogenetics: An end-to-end case study of personalized Warfarin dosing.
In USENIX Security, 2014.
[27] Google Cloud Prediction API, 2017.
[28] J. Graham-Cumming. How to beat an adaptive spam filter. In MIT Spam Confer-
[29] S. Han, H. Mao, and W. J. Dally. Deep compression: Compressing deep neural
networks with pruning, trained quantization and huffman coding. In ICLR, 2016.
[30] Haven OnDemand. https://www.havenondemand.com, 2017.
[31] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition.
ence, 2004.
In CVPR, 2016.
[32] N. Homer, S. Szelinger, M. Redman, D. Duggan, W. Tembe, J. Muehling, J. V.
Pearson, D. A. Stephan, S. F. Nelson, and D. W. Craig. Resolving individuals
contributing trace amounts of DNA to highly complex mixtures using high-
density SNP genotyping microarrays. PLOS Genetics, 2008.
[33] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled faces in the
wild: A database for studying face recognition in unconstrained environments.
Technical Report 07-49, University of Massachusetts, Amherst, October 2007.
[34] indico. https://indico.io, 2016.
[35] T. Joachims. Text categorization with support vector machines: Learning with
many relevant features. In ECML, 1998.
[36] Keras. https://keras.io, 2015.
[37] Kernel.org Linux repository rooted in hack attack. https://www.theregister.co.
uk/2011/08/31/linux_kernel_security_breach/, 2011.
[38] M. Kloft and P. Laskov. Online anomaly detection under adversarial impact. In
AISTATS, 2010.
[39] H. Krawczyk, R. Canetti, and M. Bellare. HMAC: Keyed-hashing for message
authentication. https://tools.ietf.org/html/rfc2104, 1997.
[40] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny
images. Technical report, University of Toronto, 2009.
[41] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep
convolutional neural networks. In NIPS, 2012.
[42] N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar. Attribute and simile
classifiers for face verification. In ICCV, 2009.
[43] S. Lahiri. Complexity of word collocation networks: A preliminary structural
analysis. In Proc. Student Research Workshop at the 14th Conference of the European
Chapter of the Association for Computational Linguistics, 2014.
[44] K. Lang. NewsWeeder: Learning to filter netnews. In ICML, 1995.
[45] G. B. H. E. Learned-Miller. Labeled faces in the wild: Updates and new reporting
procedures. Technical Report UM-CS-2014-003, University of Massachusetts,
Amherst, May 2014.
[46] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436–444,
[47] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied
to document recognition. Proc. IEEE, 86(11):2278–2324, 1998.
[48] Z. Lin, M. Courbariaux, R. Memisevic, and Y. Bengio. Neural networks with few
multiplications. In ICLR, 2016.
15(3), 2002.
[49] Y. Lindell and B. Pinkas. Privacy preserving data mining. Journal of Cryptology,
[50] D. Lowd. Good word attacks on statistical spam filters. In CEAS, 2005.
[51] D. Lowd and C. Meek. Adversarial learning. In KDD, 2005.
[52] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning
word vectors for sentiment analysis. In Proc. 49th Annual Meeting of the ACL:
Human Language Technologies, 2011.
[53] L. v. d. Maaten and G. Hinton. Visualizing data using t-SNE. JMLR, 9(Nov):2579–
2015.
2605, 2008.
[54] Microsoft Azure Machine Learning. https://azure.microsoft.com/en-us/services/
machine-learning, 2017.
[55] MLJAR. https://mljar.com, 2016–2017.
[56] MXNET. http://mxnet.io, 2015–2017.
[57] J. Newsome, B. Karp, and D. Song. Paragraph: Thwarting signature learning by
training maliciously. In RAID, 2006.
[58] Nexosis. http://www.nexosis.com, 2017.
[59] H.-W. Ng and S. Winkler. A data-driven approach to cleaning large face datasets.
[60] J. Nocedal and S. J. Wright. Numerical Optimization. Springer, New York, 2nd
In ICIP, 2014.
edition, 2006.
[61] O. Ohrimenko, F. Schuster, C. Fournet, A. Mehta, S. Nowozin, K. Vaswani, and
M. Costa. Oblivious multi-party machine learning on trusted processors. In
USENIX Security, 2016.
[62] B. Pang and L. Lee. Seeing stars: Exploiting class relationships for sentiment
categorization with respect to rating scales. In Proc. ACL, 2005.
[63] N. Papernot, P. McDaniel, A. Sinha, and M. Wellman. Towards the science of
security and privacy in machine learning. https://arxiv.org/abs/1611.03814, 2016.
[64] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. XNOR-Net: ImageNet
classification using binary convolutional neural networks. In ECCV, 2016.
[65] B. I. Rubinstein, B. Nelson, L. Huang, A. D. Joseph, S.-h. Lau, S. Rao, N. Taft, and
J. Tygar. Antidote: Understanding and defending against poisoning of anomaly
detectors. In IMC, 2009.
[66] F. Schuster, M. Costa, C. Fournet, C. Gkantsidis, M. Peinado, G. Mainar-Ruiz, and
M. Russinovich. VC3: Trustworthy data analytics in the cloud using SGX. In
S&P, 2015.
[67] R. Shokri and V. Shmatikov. Privacy-preserving deep learning. In CCS, 2015.
[68] R. Shokri, M. Stronati, C. Song, and V. Shmatikov. Membership inference attacks
against machine learning models. In S&P, 2017.
[69] P. Y. Simard, D. Steinkraus, and J. C. Platt. Best practices for convolutional neural
networks applied to visual document analysis. In ICDAR, 2003.
[70] Theano Development Team. Theano: A Python framework for fast computation
of mathematical expressions. https://arxiv.org/abs/1605.02688, 2016.
[71] S. Torres-Arias, A. K. Ammula, R. Curtmola, and J. Cappos. On omitting com-
mits and committing omissions: Preventing git metadata tampering that (re)-
introduces software vulnerabilities. In USENIX Security, 2016.
[72] V. Vapnik. The Nature of Statistical Learning Theory. Springer Science & Business
Media, 2013.
[73] J. Wei, X. Zhang, G. Ammons, V. Bala, and P. Ning. Managing security of virtual
machine images in a cloud environment. In CCSW, 2009.
[74] Y. Zhai, L. Yin, J. Chase, T. Ristenpart, and M. Swift. CQSTR: Securing cross-tenant
applications with cloud containers. In SoCC, 2016.
[75] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep
learning requires rethinking generalization. In ICLR, 2017.