### Classes of Synthetic Data and Malicious Tasks

The classes of synthetic data correspond to a malicious task, meaning that a specific synthetic image encodes a secret about the training images. This demonstrates that the model has effectively learned both its primary task and the malicious task.

### 7. Countermeasures

Detecting when a training algorithm is attempting to memorize sensitive data within the model is challenging. As shown in this paper, there are multiple techniques and locations for encoding this information: directly in the model parameters, through a malicious regularizer, or by augmenting the training data with specially crafted inputs. Manual code inspection may not detect malicious intent, as many of these approaches resemble standard machine learning (ML) techniques.

One effective way to mitigate the Least Significant Bit (LSB) attack is to use it against itself. The attack relies on the observation that the lower bits of model parameters do not significantly affect model accuracy. Therefore, a client can replace the lower bits of the parameters with random noise, which destroys any information potentially encoded in these bits without impacting the model’s performance.

Maliciously trained models may exhibit anomalous parameter distributions. Figure 6 compares the distribution of parameters in a conventionally trained model, which follows a zero-mean Gaussian distribution, to those in maliciously trained models. As expected, parameters generated by the correlated value encoding attack have a different distribution, similar to the effect of conventional l1-norm regularization, which encourages sparsity in the parameters. To detect these anomalies, the data owner must have a prior understanding of what a "normal" parameter distribution looks like, making such anomaly detection challenging.

Parameters generated by the capacity-abuse attack, however, do not show visible differences. This is expected because the training process remains the same, with only the dataset augmented with additional inputs.

### 8. Related Work

#### Privacy Threats in Machine Learning

No prior work has considered malicious learning algorithms designed to create a model that leaks information about the training dataset.

- **Ateniese et al. [4]**: Show how an attacker can use access to an ML model to infer a predicate of the training data, such as whether a voice recognition system was trained only with Indian English speakers.
- **Fredrikson et al. [26]**: Explore model inversion, where given a model \( f_\theta \) that makes a prediction \( y \) from a hidden feature vector \( x_1, \ldots, x_n \), they use the ground-truth label \( \tilde{y} \) and a subset of \( x_1, \ldots, x_n \) to infer the remaining, unknown features. Model inversion works similarly whether the feature vector is in the training dataset or not, but performs better for training set points due to overfitting.
- **Subsequent model inversion attacks [25]**: Demonstrate how, given access to a face recognition model, one can construct a representative of a certain output class (a recognizable face when each class corresponds to a single person).

In contrast, our objective is to extract specific inputs that belong to the training dataset used to create the model.

- **Homer et al. [32]**: Developed a technique for determining, given published summary statistics about a genome-wide association study, whether a specific known genome was used in the study. This is known as the membership inference problem.
- **Shokri et al. [68]**: Studied membership inference attacks against supervised ML models using black-box access to a model \( f_\theta \) to determine whether a given labeled feature vector \( (x, y) \) was a member of the training set used to produce \( \theta \). Their attacks work best when \( f_\theta \) has low generalizability, i.e., if the accuracy for the training inputs is much better than for inputs from outside the training dataset.

Our work focuses on how a malicious training algorithm can intentionally create a model that leaks information about its training dataset. The difference between membership inference and our problem is akin to the difference between side channels and covert channels. Our threat model is more generous to the adversary, allowing our attacks to extract substantially more information about the training data than any prior work. Additionally, we aim to create models that generalize well yet leak information.

#### Evasion and Poisoning

- **Evasion attacks**: Seek to craft inputs that will be misclassified by an ML model. They were first explored in the context of spam detection [28, 50, 51]. More recent work has investigated evasion in other settings such as computer vision [63].
- **Poisoning attacks [9, 18, 38, 57, 65]**: Insert malicious data points into the training dataset to make the resulting model easier to evade. This technique is similar in spirit to the malicious data augmentation in our capacity-abuse attack (Section 5). However, our goal is not evasion but forcing the model to leak its training data.

#### Secure ML Environments

- **Secure multi-party computation [49, 11, 16, 22]**: Enables several parties to create a joint model on their separate datasets while preserving privacy.
- **Distributed, privacy-preserving deep learning [67]**: Proposes a protocol for distributed, privacy-preserving deep learning.
- **Differentially private deep learning [1]**: Describes how to train differentially private deep learning models.
- **Trusted hardware [21, 61, 66]**: Systems using trusted hardware such as SGX protect training data while training on an untrusted service.

In all of these works, the training algorithm is public and agreed upon, and our attacks would work only if users are tricked into using a malicious algorithm.

- **CQSTR [74]**: Explicitly targets situations where the training algorithm may not be entirely trustworthy. Our results show that in such settings, a malicious training algorithm can covertly exfiltrate significant amounts of data, even if the output is constrained to be an accurate and usable model.

#### Privacy-Preserving Classification Protocols

These protocols seek to prevent the disclosure of the user’s input features to the model owner and the disclosure of the model to the user [12]. Using such a system would prevent our white-box attacks but not black-box attacks.

#### ML Model Capacity and Compression

Our capacity-abuse attack takes advantage of the fact that many models, especially deep neural networks, have large memorization capacity. Zhang et al. [75] showed that modern ML models can achieve (near) 100% training accuracy on datasets with randomized labels or even randomized features. This undermines previous interpretations of generalization bounds based on training accuracy.

Our capacity-abuse attack augments the training data with (essentially) randomized data and relies on the resulting low training error to extract information from the model. Crucially, we do this while simultaneously training the model to achieve good testing accuracy on its primary, non-adversarial task.

Our LSB attack directly exploits the large number and unnecessarily high precision of model parameters. Several papers have investigated how to compress models [13, 15, 29]. An interesting topic for future work is how to use these techniques as a countermeasure to malicious training algorithms.

### 9. Conclusion

We have demonstrated that malicious machine learning (ML) algorithms can create models that satisfy standard quality metrics of accuracy and generalizability while leaking significant information about their training datasets, even if the adversary has only black-box access to the model.

ML cannot be applied blindly to sensitive data, especially if the model-training code is provided by another party. Data holders must understand the inner workings of ML systems if they intend to make the resulting models available to other users, directly or indirectly. Whenever they use someone else’s ML system or employ ML as a service, they should demand to see the code and understand what it is doing.

In general, we need the principle of least privilege for machine learning. ML training frameworks should ensure that the model captures only as much about its training dataset as it needs for its designated task and nothing more. How to formalize this principle, develop practical training methods that satisfy it, and certify these methods are interesting open topics for future research.

### Funding Acknowledgments

This research was partially supported by NSF grants 1611770 and 1704527, as well as research awards from Google, Microsoft, and Schmidt Sciences.

### References

[1] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep learning with differential privacy. In CCS, 2016.

[2] Algorithmia. https://algorithmia.com, 2017.

[3] Amazon Machine Learning. https://aws.amazon.com/machine-learning, 2017.

[4] G. Ateniese, L. V. Mancini, A. Spognardi, A. Villani, D. Vitali, and G. Felici. Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers. IJSN, 10(3):137–150, 2015.

[5] M. Backes, P. Berrang, M. Humbert, and P. Manoharan. Membership privacy in MicroRNA-based studies. In CCS, 2016.

[6] M. Balduzzi, J. Zaddach, D. Balzarotti, E. Kirda, and S. Loureiro. A security analysis of Amazon’s Elastic Compute cloud service. In SAC, 2012.

[7] A. Baumann, M. Peinado, and G. Hunt. Shielding applications from an untrusted cloud with haven. TOCS, 33(3):8, 2015.

[8] A. L. Berger, V. J. D. Pietra, and S. A. D. Pietra. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71, 1996.

[9] B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks against support vector machines. In ICML, 2012.

[10] BigML. https://bigml.com, 2017.

[11] D. Bogdanov, M. Niitsoo, T. Toft, and J. Willemson. High-performance secure multi-party computation for data mining applications. IJIS, 11(6):403–418, 2012.

[12] R. Bost, R. A. Popa, S. Tu, and S. Goldwasser. Machine learning classification over encrypted data. In NDSS, 2015.

[13] C. Bucilă, R. Caruana, and A. Niculescu-Mizil. Model compression. In KDD, 2006.

[14] S. Bugiel, S. Nürnberger, T. Pöppelmann, A.-R. Sadeghi, and T. Schneider. AmazonIA: When elasticity snaps back. In CCS, 2011.

[15] W. Chen, J. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen. Compressing convolutional neural networks in the frequency domain. In KDD, 2016.

[16] C. Clifton, M. Kantarcioglu, J. Vaidya, X. Lin, and M. Y. Zhu. Tools for privacy preserving distributed data mining. ACM SIGKDD Explorations Newsletter, 4(2):28–34, 2002.

[17] C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20(3):273–297, 1995.

[18] N. Dalvi, P. Domingos, Mausam, S. Sanghai, and D. Verma. Adversarial classification. In KDD, 2004.

[19] DeepDetect. https://www.deepdetect.com, 2015–2017.

[20] S. Dieleman, J. Schlüter, C. Raffel, E. Olson, S. K. SÃÿnderby, D. Nouri, et al. Lasagne: First release. http://dx.doi.org/10.5281/zenodo.27878, 2015.

[21] T. T. A. Dinh, P. Saxena, E.-C. Chang, B. C. Ooi, and C. Zhang. M2R: Enabling stronger privacy in MapReduce computation. In USENIX Security, 2015.

[22] W. Du, Y. S. Han, and S. Chen. Privacy-preserving multivariate statistical analysis: Linear regression and classification. In ICDM, 2004.

[23] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. JMLR, 12(Jul):2121–2159, 2011.

[24] C. Dwork, A. Smith, T. Steinke, J. Ullman, and S. Vadhan. Robust traceability from trace amounts. In FOCS, 2015.

[25] M. Fredrikson, S. Jha, and T. Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In CCS, 2015.

[26] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and T. Ristenpart. Privacy in pharmacogenetics: An end-to-end case study of personalized Warfarin dosing. In USENIX Security, 2014.

[27] Google Cloud Prediction API, 2017.

[28] J. Graham-Cumming. How to beat an adaptive spam filter. In MIT Spam Conference, 2004.

[29] S. Han, H. Mao, and W. J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and Huffman coding. In ICLR, 2016.

[30] Haven OnDemand. https://www.havenondemand.com, 2017.

[31] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.

[32] N. Homer, S. Szelinger, M. Redman, D. Duggan, W. Tembe, J. Muehling, J. V. Pearson, D. A. Stephan, S. F. Nelson, and D. W. Craig. Resolving individuals contributing trace amounts of DNA to highly complex mixtures using high-density SNP genotyping microarrays. PLOS Genetics, 2008.

[33] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical Report 07-49, University of Massachusetts, Amherst, October 2007.

[34] indico. https://indico.io, 2016.

[35] T. Joachims. Text categorization with support vector machines: Learning with many relevant features. In ECML, 1998.

[36] Keras. https://keras.io, 2015.

[37] Kernel.org Linux repository rooted in hack attack. https://www.theregister.co.uk/2011/08/31/linux_kernel_security_breach/, 2011.

[38] M. Kloft and P. Laskov. Online anomaly detection under adversarial impact. In AISTATS, 2010.

[39] H. Krawczyk, R. Canetti, and M. Bellare. HMAC: Keyed-hashing for message authentication. https://tools.ietf.org/html/rfc2104, 1997.

[40] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.

[41] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2012.

[42] N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar. Attribute and simile classifiers for face verification. In ICCV, 2009.

[43] S. Lahiri. Complexity of word collocation networks: A preliminary structural analysis. In Proc. Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, 2014.

[44] K. Lang. NewsWeeder: Learning to filter netnews. In ICML, 1995.

[45] G. B. H. E. Learned-Miller. Labeled faces in the wild: Updates and new reporting procedures. Technical Report UM-CS-2014-003, University of Massachusetts, Amherst, May 2014.

[46] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436–444, 2015.

[47] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proc. IEEE, 86(11):2278–2324, 1998.

[48] Z. Lin, M. Courbariaux, R. Memisevic, and Y. Bengio. Neural networks with few multiplications. In ICLR, 2016.

[49] Y. Lindell and B. Pinkas. Privacy preserving data mining. Journal of Cryptology, 15(3), 2002.

[50] D. Lowd. Good word attacks on statistical spam filters. In CEAS, 2005.

[51] D. Lowd and C. Meek. Adversarial learning. In KDD, 2005.

[52] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning word vectors for sentiment analysis. In Proc. 49th Annual Meeting of the ACL: Human Language Technologies, 2011.

[53] L. v. d. Maaten and G. Hinton. Visualizing data using t-SNE. JMLR, 9(Nov):2579–2605, 2008.

[54] Microsoft Azure Machine Learning. https://azure.microsoft.com/en-us/services/machine-learning, 2017.

[55] MLJAR. https://mljar.com, 2016–2017.

[56] MXNET. http://mxnet.io, 2015–2017.

[57] J. Newsome, B. Karp, and D. Song. Paragraph: Thwarting signature learning by training maliciously. In RAID, 2006.

[58] Nexosis. http://www.nexosis.com, 2017.

[59] H.-W. Ng and S. Winkler. A data-driven approach to cleaning large face datasets. In ICIP, 2014.

[60] J. Nocedal and S. J. Wright. Numerical Optimization. Springer, New York, 2nd edition, 2006.

[61] O. Ohrimenko, F. Schuster, C. Fournet, A. Mehta, S. Nowozin, K. Vaswani, and M. Costa. Oblivious multi-party machine learning on trusted processors. In USENIX Security, 2016.

[62] B. Pang and L. Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proc. ACL, 2005.

[63] N. Papernot, P. McDaniel, A. Sinha, and M. Wellman. Towards the science of security and privacy in machine learning. https://arxiv.org/abs/1611.03814, 2016.

[64] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. XNOR-Net: ImageNet classification using binary convolutional neural networks. In ECCV, 2016.

[65] B. I. Rubinstein, B. Nelson, L. Huang, A. D. Joseph, S.-h. Lau, S. Rao, N. Taft, and J. Tygar. Antidote: Understanding and defending against poisoning of anomaly detectors. In IMC, 2009.

[66] F. Schuster, M. Costa, C. Fournet, C. Gkantsidis, M. Peinado, G. Mainar-Ruiz, and M. Russinovich. VC3: Trustworthy data analytics in the cloud using SGX. In S&P, 2015.

[67] R. Shokri and V. Shmatikov. Privacy-preserving deep learning. In CCS, 2015.

[68] R. Shokri, M. Stronati, C. Song, and V. Shmatikov. Membership inference attacks against machine learning models. In S&P, 2017.

[69] P. Y. Simard, D. Steinkraus, and J. C. Platt. Best practices for convolutional neural networks applied to visual document analysis. In ICDAR, 2003.

[70] Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions. https://arxiv.org/abs/1605.02688, 2016.

[71] S. Torres-Arias, A. K. Ammula, R. Curtmola, and J. Cappos. On omitting commits and committing omissions: Preventing git metadata tampering that (re)introduces software vulnerabilities. In USENIX Security, 2016.

[72] V. Vapnik. The Nature of Statistical Learning Theory. Springer Science & Business Media, 2013.

[73] J. Wei, X. Zhang, G. Ammons, V. Bala, and P. Ning. Managing security of virtual machine images in a cloud environment. In CCSW, 2009.

[74] Y. Zhai, L. Yin, J. Chase, T. Ristenpart, and M. Swift. CQSTR: Securing cross-tenant applications with cloud containers. In SoCC, 2016.

[75] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking generalization. In ICLR, 2017.