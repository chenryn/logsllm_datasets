title:P4P: Practical Large-Scale Privacy-Preserving Distributed Computation
Robust against Malicious Users
author:Yitao Duan and
NetEase Youdao and
John F. Canny and
Justin Z. Zhan
P4P: Practical Large-Scale Privacy-Preserving Distributed Computation
Robust against Malicious Users
John Canny
Yitao Duan
NetEase Youdao
Beijing, China
PI:EMAIL
Computer Science Division
University of California, Berkeley
PI:EMAIL
National Center for the Protection of Financial Infrastructure
Justin Zhan
South Dakota, USA
justin.zhan@ncpﬁ.org
Abstract
In this paper we introduce a framework for privacy-
preserving distributed computation that is practical for
many real-world applications. The framework is called
Peers for Privacy (P4P) and features a novel heteroge-
neous architecture and a number of efﬁcient tools for
performing private computation and ensuring security at
large scale.
It maintains the following properties: (1)
Provably strong privacy; (2) Adequate efﬁciency at rea-
sonably large scale; and (3) Robustness against realis-
tic adversaries. The framework gains its practicality by
decomposing data mining algorithms into a sequence of
vector addition steps that can be privately evaluated us-
ing a new veriﬁable secret sharing (VSS) scheme over
small ﬁeld (e.g., 32 or 64 bits), which has the same cost
as regular, non-private arithmetic. This paradigm sup-
ports a large number of statistical learning algorithms in-
cluding SVD, PCA, k-means, ID3, EM-based machine
learning algorithms, etc., and all algorithms in the sta-
tistical query model [36]. As a concrete example, we
show how singular value decomposition (SVD), which
is an extremely useful algorithm and the core of many
data mining tasks, can be done efﬁciently with privacy
in P4P. Using real-world data and actual implementation
we demonstrate that P4P is orders of magnitude faster
than existing solutions.
1 Introduction
Imagine the scenario where a large group of users want
to mine their collective data. This could be a community
of movie fans extracting recommendations from their rat-
ings, or a social network voting for their favorite mem-
bers. In all the cases, the users may wish not to reveal
their private data, not even to a “trusted” service provider,
but still obtain veriﬁably accurate results. The major
issues that make this kind of tasks challenging are the
scale of the problem and the need to deal with cheat-
ing users. Typically the quality of the result increases
with the size of the data (both the size of the user group
and the dimensionality of per user data). Nowadays it
is common for commercial service providers to run al-
gorithms on data set collected from thousands or even
millions of users. For example, the well-publicized Net-
ﬂix Prize (http://www.netﬂixprize.com/) data set consists
of roughly 100M ratings of 17,770 movies contributed
by 480K users. At such a scale, both private computa-
tion and verifying proper behavior become impractical
(more on this). In other words, privacy technologies fail
to catch up with data mining algorithms’s appetite and
processing capability for large data sets.
We strive to change this. Our goal is to provide a pri-
vacy solution that is practical for many (but not all) real-
world applications at reasonably large scale. We intro-
duce a framework called Peers for Privacy (P4P) which
is guided by the natural incentives of users/vendors and
today’s computing reality. On a typical computer today
there is a six orders of magnitude difference between the
crypto operations in large ﬁeld needed for secure homo-
morphic computation (order of milliseconds) and regu-
lar arithmetic operations in small (32- or 64-bit) ﬁelds
(fraction of a nano-second). Existing privacy solutions
such as [11, 29] make heavy use of public-key operations
for information hiding or veriﬁcation. While they have
the same asymptotic complexity as the standard algo-
rithms for those problems, the constant factors imposed
by public-key operations are prohibitive for large-scale
systems. We show in section 3.3 and section 7.2 that
they cannot be ﬁxed with trivial changes to support ap-
plications at our scale. In contrast, P4P’s main compu-
tation is based on veriﬁable secret sharing (VSS) over
small ﬁeld. This allows private arithmetic operations
to have the same cost as regular, non-private arithmetic
since both are manipulating the same-sized numbers with
similar complexity. Moreover, such a paradigm admits
extremely efﬁcient zero-knowledge (ZK) tools that are
practical even at large scale. Such tools are indispens-
able in dealing with cheating participants.
Some of techniques used in P4P were initially intro-
duced in [21]. However, the focus of [21] is to develop
an efﬁcient zero-knowledge proof (ZKP) (for detecting
cheating users) and prove its effectiveness. It leaves open
how the ZKP should be incorporated into the computa-
tion to force proper behavior. As we will show, this is not
trivial and requires additional tools, probably tailored to
each application. In particular, [21] does not deal with
the threat of cheating users changing their data during
the computation. This could cause the computation to
produce incorrect results. Such practical issues are not
addressed in [21].
We ﬁll in the missing pieces and provide a comprehen-
sive solution. The contributions of this paper are: (1) We
identify three key qualiﬁcations a practical privacy solu-
tion must possess, examine them in light of the changes
in large-scale distributed computing, and formulate our
design. The analysis not only provides rationales for our
scheme, but also can serve as a guideline for practitioners
to appraise the cost for obtaining privacy in their appli-
cations. (2) We introduce a new ZK protocol that ver-
iﬁes the consistency of user’s data during the computa-
tion. This protocol complements the work of [21] and
ensures the correctness of the computation in the pres-
ence of active user cheating.
(3) We demonstrate the
practicality of the framework with a concrete example,
a private singular value decomposition (SVD) protocol.
Prior to our work, there is no privacy solution provid-
ing comparable performance at such large scales. The
example also serves as a tutorial showing how the frame-
work can be adapted to different applications.
(4) We
have implemented the framework and performed evalu-
ations against alternative privacy solutions on real-world
data. Our experiments show a dramatic performance im-
provement. Furthermore, we have made the code freely
available and are continuing to improve it. We believe
that, like other secure computation implementations such
as [46, 39, 5, 40], P4P is a very useful tool for devel-
oping privacy-preserving systems and represents a sig-
niﬁcant step towards making privacy a practical goal in
real-world applications.
2 Preliminaries
We say that an adversary is passive, or semi-honest, if
it tries to compute additional information about other
player’s data but still follows the protocol. An active,
or malicious adversary, on the other hand, can deviate
arbitrarily from the protocol, including inputting bogus
data, producing incorrect computation, and aborting the
protocol prematurely. Clearly active adversary is much
more difﬁcult to handle than passive ones. Our scheme
is secure against a hybrid threat model that includes both
passive and active adversaries. We introduce the model
in section 4.
The privacy guarantee P4P provides is differential pri-
vacy, a notion of privacy introduced in [25], further re-
ﬁned by [24, 23], and adopted by many latest works such
as [9, 43, 42, 8, 41]. Differential privacy models the leak-
age caused by releasing some function computed over a
data set. It captures the intuition that the function is pri-
vate if the risk to one’s privacy does not substantially in-
crease as a result of participating in the data set. Formally
it is deﬁned as:
Deﬁnition 1 (Differential Privacy [25, 24]) ∀ǫ, δ ≥ 0,
an algorithm A gives (ǫ, δ)-differential privacy if for all
S ⊆ Range(A), for all data sets D, D′ such that D and
D′ differ by a single record
Pr[A(D) ∈ S] ≤ exp(ǫ) Pr[A(D′) ∈ S] + δ
There are several solutions achieving differential privacy
for some machine learning and data mining algorithms
(e.g., [24, 9, 43, 42, 8, 41]). Most require a trusted server
hosting the entire data set. Our scheme removes such a
requirement and also provides tools for handling a more
adversarial setting where the data sources may be mali-
cious. [4] is also a distributed and differentially private
scheme for binary sum functions but it is only secure in
a semi-honest model.
Differential privacy is widely used in the database pri-
vacy community to model the leakage caused by answer-
ing queries. P4P’s reliance on differential privacy is as
follows: During the computation, certain aggregate in-
formation (including the ﬁnal result) is released (other
information is kept hidden using cryptographic means).
This is also modeled as query responses computed over
the entire data set. Measuring such leakage against dif-
ferential privacy allows us to have a rigorous formulation
of the risk each individual user faces. By tuning the pa-
rameters ǫ and δ we can control such risk and obtain a
system with adequate privacy as well as high efﬁciency.
Another nice property of using differential privacy is that
it can cover the ﬁnal results (in contrast secure MPC in
cryptography does not) therefore the protection is com-
plete. Integrating differential privacy into secure compu-
tation has been accepted by the cryptography community
[4] and our work can been seen as a concrete and highly
efﬁcient instantiation of such an approach to secure com-
putation of some algorithms.
3 Design Considerations
Our design was motivated by careful evaluation of goals,
available resources, and alternative solutions.
2
3.1 Design Goals
Our goal is to provide practical privacy solutions for
some real-world applications. To this end, we identify
three properties that are essential to a practical privacy
solution:
1. Provable Privacy: Its privacy must be rigorously
proven against well formulated privacy deﬁnitions.
2. Efﬁciency and Scalability: It must have adequate
efﬁciency at reasonably large scale, which is an ab-
solute necessity for many of today’s data mining ap-
plications. The scale we are targeting is unprece-
dented: to support real-world application both the
number of users and the number of data items per
user are assumed to be in millions.
3. Robustness: It must be secure against realistic ad-
versaries. Many computations either involve the
participation of users, or collect data from them.
Cheating of a small number of users is a realistic
threat that the system must handle.
To the best of our knowledge, no existing works, or triv-
ial composition of them, attain all three. Ours is the ﬁrst,
with open-source code, supporting all these properties.
3.2 Available Resources
During the past few years the landscape of large-scale
distributed computing has changed dramatically. Many
new resources and paradigms are available at very low
cost and many computations that were infeasible at large
scale in the past are now running routinely. One notable
trend is the rapid growth of “cloud computing”, which
refers to the model where clients purchase computing cy-
cles and/or storage from a third-party provider over the
Internet. Vendors are sharing their infrastructures and
allowing general users access to gigantic computing ca-
pability. Industrial giants such as Microsoft, IBM, Ya-
hoo!, and Google are all key players in the game. Some
of the cloud services (e.g., Amazon’s Elastic Compute
Cloud, http://aws.amazon.com/ec2.) are already avail-
able to general public at very cheap price.
The growth of cloud computing symbolizes the in-
creased availability of large-scale computing power. We
believe it is time to re-think the issue of privacy preserv-
ing data mining in light of such changes. There are sev-
eral signiﬁcant differences:
1. Could computing providers have very different in-
centives. Unlike traditional e-commerce vendors
who are naturally interested in users data (e.g.,
purchase history), the cloud computing providers’s
commodity (CPU cycles and disk space) is orthogo-
nal to users’ computation. Providers do not beneﬁt
directly from knowing the data or computation re-
sults, other than ensuring that they are correct.
2. The traditional image of client-server paradigm has
changed. In particular, the users have much more
control over the data and the computation. In fact in
many cases the cloud servers will be running code
written by the customers. This is to be contrasted
with traditional e-commere where there is a tremen-
dous power imbalance between the service provider,
who possesses all the information and controls what
computation to perform, and the client users.
3. The servers are now clusters of hundreds or even
thousands of machines capable of handling huge
amount of data. They are not bottlenecks anymore.
Discrepancy of incentives and power imbalance have
been identiﬁed as two major obstacles for the adoption
of privacy technology by researchers examining privacy
issues from legal and economic perspectives [26, 1]. In-
terestingly, both are greatly mitigated with the dawn of
cloud computing. While traditional e-commerce ven-
dors are reluctant to adopt privacy technologies, cloud
providers would happily comply with customers instruc-
tions regarding what computation to perform. And once
a treasure for the traditional e-commerce vendors, user
data is now almost a burden for the cloud computing
providers: storing the data not only costs disk space, but
also may entail certain liability such as hosting illegal in-
formation. Some cloud providers may even choose not to
store the data. For example, with Amazon’s EC2 service,
user data only persists during the computation.
We believe that cloud computing offers an extremely
valuable opportunity for developing a new paradigm of
practical privacy-preserving distributed computation: the
existence of highly available, highly reputable, legally
bounded service providers also provides a very important
source of security. In particular, they make it realistic
to treat some participants as passive adversaries. (The
rests are still handled as active adversaries. The model
is therefore a heterogenous one.) By tapping into this
resource, we can build a heterogeneous system that can
have privacy, scalability and robustness all at once.
3.3 The Alternatives
Existing privacy solutions for distributed data mining can
be classiﬁed into two models: distributed and server-
based. The former is represented by a large amount of
work in the area of secure multiparty computation (MPC)
in cryptography. The latter includes mostly homomor-
phic encryption-based schemes such as [11, 22, 51].
3
Generic MPC: MPC allows n players to compute a
function over their collective data without compromising
the privacy of their inputs or the correctness of the out-
puts even when some players are corrupted by the same
adversary. The problem dates back to Yao [52] and Gol-
dreich et al. [31], and has been extensively studied in
cryptography [6, 2, 33]. Recent years see some signiﬁ-
cant improvement in efﬁciency. Some protocols achieve
nearly optimal asymptotic complexity [3, 16] while some
work in small ﬁeld [12].
From practitioners’ perspective, however,
these
generic MPC protocols are mostly of theoretical interest.
Reducing asymptotic complexity does not automatically
make the schemes practical. These schemes tend to be
complex which imposes a huge barrier for developers not
familiar with this area. Trying to support generic compu-
tation, most of them compile an algorithm into a (boolean
or arithmetic) circuit. Not only the depth of such a cir-
cuit can be huge for complex algorithms, it is also very
difﬁcult, if not entirely impossible, to incorporate exist-
ing infrastructures and tools (e.g., ARPACK, LAPACK,
MapReduce, etc.), into such computation. These tools
are indispensable part of our daily computing life and
symbolize the work of many talents over many years.
Re-building production-ready implementations is costly
and error-prone and generally not an option for most
companies in our fast-pacing modern world.
Recently there are several systems that implemented
some of the MPC protocols. While this reﬂects a plausi-
ble attempt to bridge the gap between theory and prac-
tice, unfortunately, performance-wise none of the sys-