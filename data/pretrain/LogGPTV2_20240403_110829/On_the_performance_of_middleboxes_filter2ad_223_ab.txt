Waiting until the echoed random number is received is key
to correctly measuring the end-to-end throughput. Clients,
such as ttcp, that measure TCP transfer speed by consider-
ing the transfer “done” when the last byte of data is sent
from the application to the operating system or even wait-
ing on the close() call to return (after setting the socket
to LINGER) may overestimate the end-to-end throughput at-
tained. The problem is that the MBI proxies connections
on behalf of the end-host. So, the end host’s transmission of
data bytes and their acknowledgment have no relationship
with the ultimate recipient of the data (the server at Site2 ).
Therefore, we introduce an application layer acknowledg-
ment in an eﬀort to measure the time required to actually
transmit all the data to the ultimate receiver.
F
D
C
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Outside
Inside
0
200000 400000 600000 800000 1e+06 1.2e+06 1.4e+06
Throughput (bytes/sec)
)
c
e
s
/
s
e
t
y
b
(
t
u
p
h
g
u
o
r
h
T
1.4e+06
1.2e+06
1e+06
800000
600000
400000
200000
0
0
2000 4000 6000 8000 10000 12000 14000 16000
Transfer Number
Figure 5: Throughput over time for transfers from
the inside client.
changes in the path between Site1 and Site2. Figure 5 is a
scatter plot of the throughput for each transfer conducted
from the inside client as a function of the transfer number.
The plot clearly shows distinct changes in the maximum
throughput attained during various periods of our dataset.
The distinct changes in the upper bound on performance
could be caused by changes in the route between Site1 and
Site2 or a change in some rate-limiting policy along the
path. Without further measurements (e.g., traceroutes) we
cannot say with certainty exactly what caused the change.
(A like plot from the outside client shows the same pattern
in throughput changes.)
The second item we notice from ﬁgure 4 is that the in-
side client obtains better throughput than the outside client.
Looking at roughly the midpoint of each part of the distri-
bution we see a diﬀerence in throughput of roughly 3.4% (at
the 33rd percentile) and 16.0% (at the 85th percentile). This
eﬀect is diﬃcult to explain without rich packet-level tracing
at numerous points throughout the MBI. However, we of-
fer a couple of possibilities. First, diﬀerent variants of TCP
oﬀer diﬀerent performance characteristics (e.g., see [5] for a
comparison of loss recovery techniques and their impact on
performance). Without packet-level traces we cannot quan-
tify the impact of any diﬀerences that exist in the end-host
TCP and the MBI’s TCP implementation, however we be-
lieve the diﬀerence could explain some of the diﬀerence in
the throughput measured.
In addition, we note that the
TCP model [8] oﬀers insight into the throughput attained
by concatenated TCP connections.
For our comparison the TCP model for throughput, T ,
distills down to:
Figure 4: Distribution of throughput from bulk
transfer experiments.
T ∝ 1
√
p
R
(1)
Figure 4 shows the distribution of throughput obtained in
our bulk transfer experiments. Our dataset contains over
15,000 transfers from each client. The ﬁgure oﬀers two
immediate results. First, we note that the distribution of
throughput attained (by both clients) is bi-modal. In ad-
dition, we observe that the inside client achieves higher
throughput than the outside client.
The bi-modal distribution of throughput likely comes from
where R is the round-trip time and p is the loss rate. The
rest of the parameters in the model (e.g., the MSS) are static
across connections in our experimental setup. From equa-
tion 1 it follows that reductions in either the RTT or the
loss rate increase the throughput. For concatenated TCP
connections we can use the model for each connection be-
tween the client and the server with the ultimate throughput
dictated by the minimum of the throughputs calculated.
F
D
C
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
100
Outside
Inside
Outside
Inside
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
F
D
C
1000
10000
100000
1e+06
1e+07
1e+08
0
1000
10000
100000
1e+06
1e+07
Throughput (bytes/sec)
(a) File retrieval.
Throughput (bytes/sec)
(b) File transmission.
Figure 6: Distribution of FTP throughput.
From the setup of the MBI we know that there are several
local hops between the inside client and the ﬁrewall that
intercepts the end-to-end TCP connection and initiates a
new connection to the server. And, from the FT analysis
presented in § 4 we know that the median RTT is roughly
25 msec. Therefore, eliminating several local hops from the
RTT could easily reduce the RTT of the TCP connection
that ultimately connects the MBI to the server at Site2 by
5–10% – thus yielding a like increase in throughput. This
could account for much of the diﬀerence in throughput ob-
served. However, future work in this area (using packet trace
information) to conﬁrm our sketch would be useful.
7. FTP
Next we look at a set of transfers made using FTP [10]
between Site1 and Site2. The MBI transparently intercepts
FTP sessions initiated inside the MBI and silently proxies
all connections. We used a modiﬁed BSD FTP client in our
experiments and the standard FreeBSD FTP daemon on the
server. The client was instrumented to dump timestamps of
all events (request transmission, response arrival, etc.) dur-
ing the session. In addition, we added a “sleep” command to
the FTP client that sleeps for a random amount of time cho-
sen using a Poisson process with a mean given by the user.
We initiated FTP sessions approximately every 5 minutes
(based on a Poisson process).
In addition, between each
FTP command we slept for approximately 2 seconds. Each
FTP session consists of 35 commands and 4 ﬁle transfers
(of 100 KB each). The client used both data connections
opened actively and passively (using the “PORT” and “PASV”
FTP commands) and both transmitted and retrieved ﬁles.
The commands issued on the FTP control connection are as
follows:
“RETR” command to initiate the ﬁle transfer. This step
is repeated for each ﬁle transfer (i.e., 4 times).
3. The “QUIT” command to terminate the session.
Figure 6 shows the throughput distribution for the trans-
mission and reception of ﬁles via FTP inside and outside
the MBI. The “PASV” command is used to setup the data
connection5 We make the following observations from the
plots:
• The ﬁle retrievals perform comparably regardless of
the location of the client6.
• The throughput when transmitting ﬁles is higher when
inside the MBI by nearly a factor of three at the me-
dian point. This is largely explained by the way FTP
works and the MBI proxying the TCP connections.
Since the proxy terminates the connection to the client
and starts a new connection to the server the transfer
becomes a LAN transfer from the client’s perspective.
Further, FTP does not include an application level ac-
knowledgment (as the tests in the last section did).
Therefore, as soon as the FTP client sends the last
byte of data it considers the transmission ﬁnished even
though all of the data has not yet arrived at the re-
ceiver.
• The throughputs obtained by the two sets of ﬁle re-
trievals and the ﬁle transmission from the outside client
are comparable underscoring the fact that these trans-
missions are experiencing dynamics based on travers-
ing the Internet while the ﬁle transmission from the
inside client is only experiencing local network dynam-
ics.
1. The “USER” and “PASS” commands to login to the FTP
server.
2. The following 6 commands are issued (separated by ap-
proximately 2 seconds) prior to each ﬁle transfer: TYPE
A, CWD /, PWD, STAT, TYPE I, MDTM. The next com-
mand sets up the data connection (either a “PORT”
or “PASV” command), followed by either a “STOR” or
• Finally, we note that in the FTP tests the throughput
obtained did not reach the upper bound of the lower
5The results for using “PORT” to setup the data connection
are omitted due to space considerations, but are consistent
with the “PASV” results presented in this paper.
6Unfortunately, our data is not rich enough to determine
the cause of the knee in the plot around y = 0.55.
mode of the available bandwidth shown in the last sec-
tion. Therefore, the distribution does not show the bi-
model characteristic that the bulk transfer results illus-
trated. This is explained by the ﬁle size diﬀerence for
the two transfers. In the FTP tests we used a 100 KB
transfer, as opposed to the the bulk throughput tests
that used a 1 MB transfer. The 100 KB tests did not
aﬀord enough time or data to open TCP’s congestion
window to fully utilize the available bandwidth.
Next we observe the time required for responses to com-
mands sent on the control connection to arrive at the client.
Figure 7 shows the distribution of the feedback time. The
results are similar to the ping tests outlined in § 4, with both
clients showing largely the same delay distribution. We note
one glaring diﬀerence between the inside and outside clients
at the low end of the distribution, where the inside client
shows lower delay than the outside client. This anomaly is
caused by the FTP implementation in the MBI, which does
not understand the “MDTM” command (used to determine
the modiﬁcation time of a given ﬁle). Therefore, the inside
client receives an error from the MBI for these commands –
which experiences only local network delays, while the out-
side client receives the correct response from the server –
incurring the Internet path delays.
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
F
D
C
0
1e-05 0.0001 0.001
Outside
Inside
10
100
1000
0.1
0.01
Response Time (sec)
1
Figure 7: Distribution of time required for com-
mands on FTP control connection.
The success rate of both the inside and outside FTP trans-
fers is over 99.99% for over 121,000 ﬁle transfers. The most
prevalent problem encountered was with the data connec-
tion not being available. This problem was encountered in
roughly the same number of cases for the inside and out-
side client and so does not appear to be a problem with the
MBI. Finally, we note that each client issued just over one
million commands on the control connection. Every com-
mand issued completed successfully on the outside client.
Two commands failed on the inside client (not counting the
“MDTM” failures discussed above).
8. CONCLUSIONS AND FUTURE WORK
From the results presented in this paper we ﬁnd that
the measured MBI oﬀers a mixed bag of performance costs
and beneﬁts. For instance, we note that setting up a short
transaction takes roughly 5 times longer when traversing the
MBI. However, once a TCP connection is setup, the added
delay required to traverse the MBI is small (1 to 2 msec).
Additionally, transmitting ﬁles from inside the MBI to a
server across the network is faster than transfering the data
from outside the MBI. Therefore, our conclusion is that the
impact of the MBI on performance is application depen-
dent. Finally, we note that the MBI generally increases the
instances of failures in the network across all of our experi-
ments. However, the application success rate is over 99.9%
in all of our experiments. So, even though the MBI in-
creases the failure rate by several times in some cases the
overall success rate is high.
We see two major areas for future work in the area of
measuring middleboxes: (i) measuring a larger number of
production environments to assess whether the results from
Site1 ’s network are representative and (ii) capturing packets
at each step through an MBI and reconstructing the events
to determine the root causes of the performance costs and
beneﬁts, as well as the failures noted in our results.
Acknowledgments
This paper has beneﬁted from the contributions of a number
of people. Engineers at Site1 and Site2 aided in the setup
and design of the experiments presented in this paper. In
addition, Vern Paxson provided encouragement and useful
conversation throughout the work. My thanks to all!
9. REFERENCES
[1] A. Bakre and B. R. Badrinath. I-TCP: Indirect TCP
for Mobile Hosts. In Proceedings of the 15th
International Conference on Distributed Computing
Systems (ICDCS), May 1995.
[2] H. Balakrishnan, S. Seshan, E. Amir, and R. Katz.
Improving TCP/IP Performance Over Wireless
Networks. In ACM MobiCom, Nov. 1995.
[3] J. Border, M. Kojo, J. Griner, G. Montenegro, and
Z. Shelby. Performance Enhancing Proxies Intended to
Mitigate Link-Related Degradations, June 2001. RFC
3135.
[4] K. B. Egevang and P. Francis. The IP Network
Address Translator (NAT), May 1994. RFC 1631.
[5] K. Fall and S. Floyd. Simulation-based Comparisons
of Tahoe, Reno, and SACK TCP. Computer
Communications Review, 26(3), July 1996.
[6] M. Handley, V. Paxson, and C. Kreibich. Network
Intrusion Detection: Evasion, Traﬃc Normalization,
and End-to-End Protocol Semantics. In Proceedings of
USENIX Security Symposium, 2001.
[7] S. Karandikar, S. Kalyanaraman, P. Bagal, and
B. Packer. TCP Rate Control. ACM Computer
Communication Review, 30(1):45–58, Jan. 2000.
[8] J. Padhye, V. Firoiu, D. Towsley, and J. Kurose.
Modeling TCP Throughput: A Simple Model and its
Empirical Validation. In ACM SIGCOMM, Sept. 1998.
[9] J. Postel. Transmission Control Protocol, Sept. 1981.
RFC 793.
[10] J. Postel and J. Reynolds. File Tranfer Protocol
(FTP), Oct. 1985. RFC 959.
[11] D. Zimmerman. The Finger User Information
Protocol, Dec. 1991. RFC 1288.