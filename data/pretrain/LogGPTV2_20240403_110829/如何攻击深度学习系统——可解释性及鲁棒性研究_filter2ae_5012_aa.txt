# 如何攻击深度学习系统——可解释性及鲁棒性研究
|
##### 译文声明
本文是翻译文章
译文仅供参考，具体内容表达以及含义原文为准。
## 0x00
人工智能广泛渗透于我们的生活场景中，随处可见。比如人脸识别解锁、人脸识别支付、语音输入，输入法自动联想等等，不过这些场景其实传统的模式识别或者机器学习方法就可以解决，目前来看真正能代表人工智能最前沿发展的莫过于深度学习，尤其是深度学习在无人驾驶、医疗决策（如通过识别拍片结果诊断是否有相应疾病）领域的应用。
不过深度学习存在所谓的黑箱问题，由此带来了不可解释性，而这一点如果不能解决（事实上目前为止还没有很好解决）将会导致在深度学习在对安全性很敏感的领域中应用的受限。比如将其应用在医疗领域时，深度学习系统根据医学影像判断病人为癌症，但是不能解释为什么给出这个判断，而人类医学专家认为不是癌症，那么这时存在两种情况，一种是深度学习系统错了；第二种则是专家错了，可是由于系统无法给出解释，所以专家未必采纳系统意见，则造成病人的损失。无论是哪种情况，都可以看到不解决深度学习的可解释性问题，其未来的应用发展是一定会受到限制的。
本文的安排如下，在0x01会介绍深度学习相较于传统方法的特点；0x02至0x04会介绍深度学习的不可解释性，进而引出业界对为了实现可解释性做出的工作，其中在0x02会介绍设计可解释模型方面的工作，在0x03介绍将可解释性引入已有模型的工作，0x04会介绍可解释性与深度学习安全性的关系。在0x05将会介绍模型的鲁棒性及相关研究，在0x06部分从模型、数据、承载系统三方面展开深度学习自身安全性问题的阐述。
## 0x01
在0x00中我们谈到了人工智能、模式识别、机器学习、深度学习，这四个领域其实都是相互联系的。
我们先来进行简单的区分。后三者都是实现人工智能的途径，其中我们特别需要把深度学习与模式识别、机器学习这两个领域区分开来。
所谓模式识别就是通过计算机用数学技术方法来研究模式的自动处理和判读。我们把环境与客体统称为“模式”。随着计算机技术的发展，人类会研究复杂的信息处理过程，一个重要形式是生命体对环境及客体的识别。以光学字符识别之“汉字识别”为例：首先将汉字图像进行处理，抽取主要表达特征并将特征与汉字的代码存在计算机中。就像老师教我们“这个字叫什么、如何写”记在大脑中。这一过程叫做“训练”。识别过程就是将输入的汉字图像经处理后与计算机中的所有字进行比较，找出最相近的字就是识别结果。这一过程叫做“匹配”。
机器学习的流程也与此类似，这里不展开，接下来看看深度学习。
深度学习其是机器学习中一种基于对数据进行表征学习的算法。观测值（例如一幅图像）可以使用多种方式来表示，如每个像素强度值的向量，或者更抽象地表示成一系列边、特定形状的区域等。而使用某些特定的表示方法更容易从实例中学习任务（例如人脸识别）。
从上面的描述中我们可以看到深度学习是机器学习的一个分支，不过其最明显的特点在于会对数据进行表征学习。表征学习的目标是寻求更好的表示方法并创建更好的模型来从大规模未标记数据中学习这些表示方法。可以看到，相比于传统的模式识别、机器学习需要手工提取特征，深度学习最大的优势是可以自动从数据中提取特征。
举个例子，使用机器学习系统识别一只狼，我们需要手动提取、输入我们人类可以将动物识别为狼的特征，比如体表毛茸茸、有两只凶狠的眼睛等；而使用深度学习系统，我们只需要将大量的狼的图片输入系统，它就会自动学习特征。
## 0x02
深度学习的优点在于学习能力强、覆盖范围广、数据驱动，其缺点在于计算量大、硬件需求高、模型设计复杂。这是事实，但是作为安全研究人员，我们更需要关注的不是怎么搭房子（怎么设计深度学习系统），而是怎么拆房子（如何对其进行攻击）以及如果搭更坚固的房子（如何进行针对性防御）。
这里非常关键的一点，就是模型的可解释性问题。
还是以前面设计识别狼的人工智能系统为例，在Macro[3]等人研究的工作中发现，深度学习系统会将西伯利亚哈士奇识别为狼，最后找到的原因是因为在训练系统时，输入的狼的图片上的背景大多是皑皑白雪，使得系统在自动提取特征时将白雪背景作为了识别狼的标志。所以当我们给出一张西伯利亚哈士奇的图片（如下右图所示）时，就会被系统识别为狼。下左图为通过LIME的解释性方法说明是系统做根据背景中的白雪做出判断的。
可以看到，系统使用了图片的背景并完全忽略了动物的特征。模型原本应该关注动物的眼睛。
相似的案例还有不少，比如特斯拉在佛罗里达州发生的事故[4]。
2016年5月，美国佛罗里达州一位男子驾驶开启自动驾驶模式后，特斯拉Model
S撞上一辆正在马路中间行驶的半挂卡车，导致驾驶员当场死亡。事后排查原因时发现，主要是因为图像识别系统没能把货车的白色车厢与大背景中的蓝天白云区分开导致的，系统认为是卡车是蓝天白云时就不会自动刹车进而引发了事故。
这充分说明深度学习的可解释性的重要。
深度学习缺乏可解释性，其原因是因为其黑箱特性。我们知道在神经网络计算过程中会自动从原始数据中提取特征并拆分、组合，构成其判别依据，而我们人类却可能无法理解其提取的特征。进一步地，当模型最后输出结果时，它是根据哪些方面、哪些特征得到这个结果的也就是说，对于我们而言该过程是不可解释的。
事实上，不论工业界还是学术界都意识到了深度学习可解释性的重要性，《Nature》《Science》都有专题文章讨论,如[5][6],AAAI
2019也设置了可解释性人工智能专题[7]，DARPA也在尝试建立可解释性深度学习系统[8]
就目前研究现状来看，主要可以分为2个方面。
一方面是设计自身就带有可解释性的模型，另一方面是使用可解释性技术来解释已有的模型。
在前一方面，已有的研究主要可以分为三类。
1）自解释模型
这主要在传统的机器学习算法中比较常见，比如1）在Naive
Bayes模型中，由于条件独立性的假设，我们可以将模型的决策过程转换为概率运算[10]；2）在线性模型中，基于模型权重，通过矩阵运算线性组合样本的特征值，展现出线性模型的决策过程[11]；3）在决策树模型中，每一条从根节点到不同叶子节点的路径都代表着不同的决策规则，因为决策结果实际是由一系列If-then组成的决策规则构造出来的[12]，下图就是参考文献[12]中举的一个决策树判别实例
2）广义加性模型
广义加性模型通过线性函数组合每一单特征模型得到最终的决策形式，一般形式为g(y)= f1(x1)+ f2(x2)+… +
fn(xn)，其特点在于有高准确率的同时保留了可解释性。由其一般形式可以看到，因为我们是通过简单的线性函数组合每一个单特征模型得到最终决策形式，消除了特征之间的相互作用，从而保留了简单线性模型良好的可解释性。
典型工作如Poulin等人提出的方案[13]，设计了ExplainD原型，提供了对加性模型的图形化解释，包括对模型整体的理解以及决策特征的可视化，以帮助建立用户与决策系统之间的信任关系。下图就是利用设计的原因对朴素贝叶斯决策作出解释。
3）注意力机制
注意力机制是解决信息超载问题的一种有效手段，通过决定需要关注的输入部分，将有限的信息处理资源分配给更重要的任务。更重要的是，注意力机制具有良好的可解释性，注意力权重矩阵直接体现了模型在决策过程中感兴趣的区域。
典型工作如Xu等人[14]将注意力机制应用于看图说话(Image Caption)任务中以产生对图片
的描述。首先利用卷积神经网络(CNN)提取图片特征，然后基于提取的特征，利用带注意力机制的循环神经网络(RNN)生成描述。在这个过程中，注意力实现了单词与图片之间的对齐，因此，通过可视化注意力权重矩阵，人们可以清楚地了解到模型在生成每一
个单词时所对应的感兴趣的图片区域，如下所示
## 0x03
在第二方面，即对已有的模型做解释，可以分为两种研究角度，一种是从全局可解释性，旨在理解模型的整体逻辑及内部工作机制；一种是局部可解释性，旨在对特定输入样本的决策过程做出解释。
从全局可解释性角度来进行的研究工作也有很多，这里简单介绍三类。
1）规则提取
这种方法利用可理解的规则集合生成可解释的符号描述，或提取出可解释模型使其具有与原来的黑盒模型相当的决策能力。这种方法实现了对黑盒模型内部工作机制的深入理解。
如Anand[15]的工作，他们提出的方案是使用一个紧凑的二叉树，来明确表示黑盒机器学习模型中隐含的最重要的决策规则。该树是从贡献矩阵中学习的，该贡献矩阵包括输入变量对每个预测的预测分数。
如下图所示
从第1行到第4行，每行都有一个来自MIT Place
365场景理解数据集中“卧室”，“客厅”，“厨房”和“浴室”四类的示例图像。第一列是原始图像。第二列显示通过语义分割算法找到的语义类别。第3列显示了实际的语义分割，其中每个图像包含几个超像素。使用局部预测解释器，我们可以获得每个超像素（即语义类别）对预测分数的贡献。第4列显示了重要的语义超像素（具有最高的贡献分数），分别针对相应的地面真实类别分数以绿色突出显示。可以对于“卧室”图像，“床”和“地板”超像素很重要。对于“客厅”图像，“沙发”，“窗玻璃”和“壁炉”很重要。对于“厨房”形象，“柜子”是最重要的。最后对于“浴室”形象，“厕所”，“纱门”起着最重要的作用。所有这些解释对于我们人类来说似乎都是合理的。
进一步地，在获得测量每个语义类别对于每个图像的场景类别的重要性的贡献矩阵之后，我们可以通过递归分区（GIRP）算法进行全局解释，以生成每个类别的解释树，如下所示
这就是将图片归类为对应场景时产生的决策树。
2）模型蒸馏
模型蒸馏的思想是通过模型压缩，用结构紧凑的学生模型来模拟结构复杂的教师模型，从而降低复杂度。因为通过模型蒸馏可以实现教师模型到学生模型的知识迁移，所以可以将学生模型看做是教师模型的全局近似，那么我们就可以基于简单的学生模型对教师模型提供全局解释，这也就要求学生模型应该选择具有良好解释性的模型，如线性模型、决策树、广义加行性模型等。Frosst等[16]扩展了Hinton提出的知识蒸馏方法，提出利用决策树来模拟复杂深度神经网络模型的决策,如下所示
这是将神经网络蒸馏为soft decision
tree得到的结果。内部节点上的图像是学习到的过滤器，叶子上的图像是学习到的概率分布在类上的可视化。标注了每个叶子上的最终最可能分类以及每个边上的可能分类。
3）激活最大化
激活最大化方法是指通过在特定的层上找到神经元的首选输入最大化神经元激活，其思想很简单，就是通过寻找有界范数的输入模式，最大限度地激活给定的隐藏单元，而一个单元最大限度地响应的输入模式可能是一个单元正在做什么的良好的一阶表示。这一过程可以被形式化为
得到x*后对其可视化，则可以帮助我们理解该神经元在其感受野中所捕获到的内容。
如Karen等人的工作[17],其方案是针对图像分类模型的可视化，实现了两种可视化技术，其一是生成一个图像，最大化类分数，从而可视化由卷积网络捕获的类的概念。第二种技术特定于给定的图像和类，计算一个类显著图。这些技术可以帮助解释模型是如何进行分类决策的。
部分实验如下所示
这是ILSVRC-2013竞赛图像中排名靠前的预测类的图像特定类显著图saliency
map，这些图是使用单次反向传播通过分类卷积网络提取的。直观的来说，之所以会将中间图片分类为金丝猴，是因为其对应的显著图中亮起来的部分（及金丝猴的身子），而不是因为树枝等其他原因才将其识别为金丝猴。
从局部可解释性角度的工作相较于全局可解释性的工作更多，因为模型的不透明性、复杂性等问题使得做出全局解释更加困难。这里也简单列举三种类型。
1）敏感性分析
敏感性分析是指在给定假设下，定量分析相关自变量的变化对因变量的影响程度，核心思想是通过逐一改变自变量的值来解释因变量受自变量变化影响大小的规律。将其应用于模型局部解释方法中，可以用来分析样本的每一维特征对模型最终分类结果的影响，以提供对某一个特定决策结果的解释。