       unWatchCh      chan *endpoint 
       svcDb          map[string]svcMap 
       nmap           map[string]*netWatch 
       defOsSbox      osl.Sandbox 
       sboxOnce       sync.Once 
       sync.Mutex 
    }   
```
每个网络控制器都涉及以下内容:
*   数据结构驱动程序表中的一个或多个驱动程序
*   数据结构中的一个或多个沙箱
*   数据存储
*   An ipamTable
    ![Network controller](img/image_07_009.jpg)
处理 Docker 容器和 Docker 引擎之间的网络的网络控制器
上图显示了网络控制器如何位于 Docker 引擎、容器和它们所连接的网络之间。
### CNM 属性
以下是 CNM 属性:
*   **选项:**这些不是最终用户可见的，而是数据的键值对，以提供一种灵活的机制，将特定于驱动程序的配置从用户直接传递给驱动程序。libnetwork 仅在某个键与某个知名标签匹配时才对选项进行操作，因此会拾取一个由类属对象表示的值。
*   **标签:**这些是选项的子集，是使用`--labels`选项在用户界面中表示的最终用户变量。它们的主要功能是执行特定于驱动程序的操作，并且它们是从用户界面传递的。
### CNM 生命周期
CNM 的消费者通过 CNM 对象及其 API 进行交互，以将他们管理的容器联网；驱动程序向网络控制器注册。
内置驱动程序在 libnetwork 内部注册，而远程驱动程序使用插件机制向 libnetwork 注册。
每个驱动程序处理特定的网络类型，解释如下:
*   使用`libnetwork.New()`应用编程接口创建网络控制器对象，以管理网络的分配，并可选地使用特定于驱动程序的选项配置驱动程序。使用控制器的`NewNetwork()`应用编程接口、一个`name`和一个`NetworkType`作为参数创建网络对象。
*   `NetworkType`参数有助于选择驱动程序，并将创建的网络绑定到该驱动程序。网络上的所有操作都将由使用前面的应用编程接口创建的驱动程序来处理。
*   `Controller.NewNetwork()`应用编程接口接受一个可选的选项参数，该参数携带特定于驱动程序的选项和标签，驱动程序可以将其用于自己的目的。
*   `Network.CreateEndpoint()`被调用以在给定网络中创建新端点。该应用编程接口还接受随驱动程序而变化的可选选项参数。
*   `CreateEndpoint()`在网络中创建端点时，可以选择保留 IPv4/IPv6 地址。驱动程序使用在`driverapi`中定义的`InterfaceInfo`接口分配这些地址。需要 IPv4/IPv6 地址来完成端点即服务定义以及端点公开的端口。服务端点是应用容器正在侦听的网络地址和端口号。
*   `Endpoint.Join()` is used to attach a container to an endpoint. The `Join` operation will create a sandbox, if one doesn't exist for that container. The drivers make use of the sandbox key to identify multiple endpoints attached to the same container.
    有一个单独的应用编程接口来创建端点，还有一个应用编程接口来连接端点。
    端点代表独立于容器的服务。当一个端点被创建时，它拥有为容器保留的资源，以便以后连接到该端点。它给出了一致的网络行为。
*   当容器停止时调用`Endpoint.Leave()`。驱动程序可以清除在`Join()`调用期间分配的状态。当最后一个引用端点离开网络时，libnetwork 会删除沙箱。
*   只要端点仍然存在，libnetwork 就会一直保留 IP 地址。当容器(或任何容器)再次连接时，这些将被重用。它确保容器的资源在停止和再次启动时被重用。
*   `Endpoint.Delete()`从网络中删除一个端点。这将导致删除端点并清理缓存的`sandbox.Info`。
*   `Network.Delete()`用于删除一个网络。如果没有端点连接到网络，则允许删除。
# 基于覆盖和底层网络的 Docker 网络工具
覆盖层是建立在底层网络基础设施(底层)之上的虚拟网络。目的是实现物理网络中不可用的网络服务。
网络覆盖极大地增加了可以在物理网络上创建的虚拟子网的数量，从而支持多租户和虚拟化功能。
Docker 中的每个容器都被分配了一个用于与其他容器通信的 IP 地址。如果容器必须与外部网络通信，您可以在主机系统中设置网络，并将端口从容器公开或映射到主机。由于该应用在内部运行，容器将无法通告其外部 IP 和端口，因为信息对它们不可用。
解决方案是以某种方式为所有主机上的每个 Docker 容器分配唯一的 IP，并拥有一些在主机之间路由流量的网络产品。
有不同的项目和工具可以帮助 Docker 联网，如下所示:
*   法兰绒
*   织法
*   卡利科项目
## 法兰绒
**法兰绒**给每个容器一个可用于容器间通信的 IP。通过数据包封装，它在主机网络上创建了一个虚拟覆盖网络。默认情况下，法兰绒为主机提供一个`/24`子网，Docker 守护程序将从该子网向容器分配 IP。
![Flannel](img/image_07_010.jpg)
使用法兰绒进行容器之间的通信
法兰绒在每台主机上运行一个代理`flanneld`，负责从预配置的地址空间中分配子网租约。法兰绒使用`etcd`([https://github.com/coreos/etcd](https://github.com/coreos/etcd))存储网络配置、分配的子网和辅助数据(如主机的 IP)。
为了提供封装，法兰绒使用**通用 TUN/TAP** 设备，并使用 UDP 创建覆盖网络来封装 IP 数据包。子网分配是在`etcd`的帮助下完成的，它维护覆盖子网到主机的映射。
## 编织
**Weave** 创建一个虚拟网络，将跨主机/虚拟机部署的 Docker 容器连接起来，并实现它们的自动发现。
![Weave](img/image_07_011.jpg)
编织网络
Weave 可以穿越防火墙，在部分连接的网络中运行。流量可以选择加密，从而允许主机/虚拟机通过不受信任的网络进行连接。
Weave 增强了 Docker 现有的(单主机)网络功能，例如 docker0 桥，这样容器就可以继续使用这些功能。
## 印花布项目
**Calico 项目**为连接容器、虚拟机或裸机提供了可扩展的网络解决方案。Calico 使用可扩展的 IP 网络原理作为第 3 层方法来提供连接。Calico 可以在没有覆盖或封装的情况下部署。Calico 服务应该作为容器部署在每个节点上。它为每个容器提供了自己的 IP 地址，并且处理所有必要的 IP 路由、安全策略规则和跨节点集群的路由分布。
为了提供更好的网络解决方案，Calico 体系结构包含四个重要组件:
*   **Calico worker 流程 Felix** 是 Calico 网络的核心，主要路由主机上的工作负载，并为其提供所需的连接。它还为传出端点流量提供了到内核的接口
*   **BIRD** ，路线 ic。BIRD，路由分发开源 BGP，在主机之间交换路由信息。BIRD 拾取的内核端点被分发到 BGP 对等体，以便提供主机间路由。两个 BIRD 进程在*卡利科节点*容器中运行，一个用于 IPv4 (bird)，一个用于 IPv6 (bird6)
*   **confd** 是一个为 BIRD 自动生成配置的模板化过程，它监控`etcd`存储中 BGP 配置的任何更改，例如日志级别和 IPAM 信息。`confd`还根据来自`etcd`的数据动态生成 BIRD 配置文件，并在数据更新时自动触发。`confd`每当配置文件发生变化时，触发 BIRD 加载新文件。
*   **calicoctl** 是用于配置和启动 Calico 服务的命令行。它甚至允许数据存储(`etcd`)定义和应用安全策略。该工具还为 Calico 配置的一般管理提供了简单的界面，无论 Calico 是在虚拟机、容器还是裸机上运行。`calicoctl;`
    ```
     $ calicoctl
     Override the host:port of the ETCD server by setting the 
             environment 
            variable
     ETCD_AUTHORITY [default: 127.0.0.1:2379]
     Usage: calicoctl  [...]
     status            Print current status information
     node              Configure the main calico/node container and 
             establish 
                              Calico
     networking
     container         Configure containers and their addresses
     profile           Configure endpoint profiles
     endpoint          Configure the endpoints assigned to existing 
             containers
     pool              Configure ip-pools
     bgp               Configure global bgp
     ipam              Configure IP address management
     checksystem       Check for incompatibilities on the host 
             system
     diags             Save diagnostic information
     version           Display the version of calicoctl
     config            Configure low-level component configuration 
            See 'calicoctl  --help' to read about a specific 
             subcommand.
    ```
    支持以下命令
根据 Calico 存储库([https://github.com/projectcalico/calico-containers](https://github.com/projectcalico/calico-containers))的官方 GitHub 页面，存在以下 Calico 集成:
*   Calico as Docker 网络插件
*   没有 Docker 网络的 Calico
*   卡利科与库比涅斯
*   带介子的印花棉布
*   Calico with Docker Swarm
    ![Project Calico](img/image_07_012.jpg)
    卡利科建筑
# 使用 Docker 引擎群节点配置覆盖网络
随着 Docker 1.9 的发布，多主机和覆盖网络已成为其主要功能之一。它支持建立专用网络来连接多个容器。我们将在没有外部键值存储的群集群中运行的管理器节点上创建覆盖网络。群网络将使群中需要服务的节点可以使用该网络。
当我们部署使用覆盖网络的服务时，管理器会自动将网络扩展到运行服务任务的节点。多主机网络需要一个服务发现存储，所以现在我们将创建一个 Docker 机器来运行该服务。
![Configuring an overlay network with the Docker Engine swarm node](img/image_07_013.jpg)
跨多台主机的覆盖网络
对于以下部署，我们将使用在虚拟化或云平台上创建 Docker 守护程序的 Docker 机器应用。对于虚拟化平台，我们将使用 VMware fusion 作为提供商。
Docker 机器安装如下:
```
 $ curl -L https://github.com/docker/machine/releases/download/
    v0.7.0/docker-machine-`uname -s`-`uname -m` > /usr/local/bin/
    docker-machine && \
 > chmod +x /usr/local/bin/docker-machine
 % Total    % Received % Xferd  Average Speed   Time    Time    Time  Current
                                     Dload  Upload   Total   Spent   Left  Speed
 100   601    0   601    0     0    266      0 --:--:--  0:00:02 --:--:--   266
 100 38.8M  100 38.8M    0     0  1420k      0  0:00:28  0:00:28 --:--:-- 1989k
 $ docker-machine version
 docker-machine version 0.7.0, build a650a40
```
多用户网络需要一个服务发现存储，因此我们将创建一个 Docker 机器来运行该服务，创建新的 Docker 守护程序:
```
$ docker-machine create \
>   -d vmwarefusion \
>   swarm-consul
Running pre-create checks...
(swarm-consul) Default Boot2Docker ISO is out-of-date, downloading the latest 
    release...
(swarm-consul) Latest release for github.com/boot2docker/boot2docker is 
    v1.12.1
(swarm-consul) Downloading 
...
```
### 类型
要查看如何将您的 Docker 客户端连接到在该虚拟机上运行的 Docker 引擎，请运行`docker-machine env swarm-consul`。
我们将启动服务发现的 consul 容器:
```
$(docker-machine config swarm-consul) run \
>         -d \
>         --restart=always \
>         -p "8500:8500" \
>         -h "consul" \
>         progrium/consul -server -bootstrap
Unable to find image 'progrium/consul:latest' locally
latest: Pulling from progrium/consul
...
Digest: 
    sha256:8cc8023462905929df9a79ff67ee435a36848ce7a10f18d6d0faba9306b97274
Status: Downloaded newer image for progrium/consul:latest
d482c88d6a1ab3792aa4d6a3eb5e304733ff4d622956f40d6c792610ea3ed312
```
创建两个 Docker 守护程序来运行 Docker 集群，第一个守护程序是将自动运行用于协调集群的 swarm 容器的 Swarm 节点:
```
$ docker-machine create \
>   -d vmwarefusion \
>   --swarm \
>   --swarm-master \
>   --swarm-discovery="consul://$(docker-machine ip swarm-
     consul):8500" \
>   --engine-opt="cluster-store=consul://$(docker-machine ip swarm-
    consul):8500" \
>   --engine-opt="cluster-advertise=eth0:2376" \
>   swarm-0
Running pre-create checks...
Creating machine...
(swarm-0) Copying 
     /Users/vkohli/.docker/machine/cache/boot2docker.iso to 
    /Users/vkohli/.docker/machine/machines/swarm-0/boot2docker.iso...
(swarm-0) Creating SSH key...
(swarm-0) Creating VM...
...
```
Docker 已经开始工作了！
### 类型
要查看如何将您的 Docker 客户端连接到在该虚拟机上运行的 Docker 引擎，请运行`docker-machine env swarm-0`。
第二个守护进程是 Swarm `secondary`节点，它将自动运行 Swarm 容器并将状态报告回`master`节点:
```
$ docker-machine create \
>   -d vmwarefusion \
>   --swarm \
>   --swarm-discovery="consul://$(docker-machine ip swarm-
     consul):8500" \
>   --engine-opt="cluster-store=consul://$(docker-machine ip swarm-
    consul):8500" \
>   --engine-opt="cluster-advertise=eth0:2376" \
>   swarm-1
Running pre-create checks...
Creating machine...
(swarm-1) Copying 
     /Users/vkohli/.docker/machine/cache/boot2docker.iso to 
    /Users/vkohli/.docker/machine/machines/swarm-1/boot2docker.iso...
(swarm-1) Creating SSH key...
(swarm-1) Creating VM...
...
```
Docker 已经开始工作了！
### 类型
要查看如何将您的 Docker 客户端连接到在该虚拟机上运行的 Docker 引擎，请运行`docker-machine env swarm-1`。
Docker 可执行文件将与一个 Docker 守护程序通信。由于我们在一个集群中，我们将通过运行以下命令来确保 Docker 守护程序与集群的通信:
```
$ eval $(docker-machine env --swarm swarm-0)
```
之后，我们将创建一个带有覆盖驱动程序的私有`prod`网络:
```
$ docker $(docker-machine config swarm-0) network create --driver 
    overlay prod
```
我们将使用`--net parameter`启动两个虚拟`ubuntu:12.04`容器:
```
$ docker run -d -it --net prod --name dev-vm-1 ubuntu:12.04
426f39dbcb87b35c977706c3484bee20ae3296ec83100926160a39190451e57a
```
在下面的代码片段中，我们可以看到这个 Docker 容器有两个网络接口:一个连接到私有覆盖网络，另一个连接到 Docker 桥:
```
$ docker attach 426
root@426f39dbcb87:/# ip address
23: eth0@if24:  mtu 1450 qdisc 
     noqueue state 
    UP 
 link/ether 02:42:0a:00:00:02 brd ff:ff:ff:ff:ff:ff
 inet 10.0.0.2/24 scope global eth0
 valid_lft forever preferred_lft forever
 inet6 fe80::42:aff:fe00:2/64 scope link 