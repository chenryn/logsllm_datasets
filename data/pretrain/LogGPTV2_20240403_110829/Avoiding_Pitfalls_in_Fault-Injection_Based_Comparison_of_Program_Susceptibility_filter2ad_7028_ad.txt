result accounting when using def/use pruning. Fault-space
pruning is an optimization, and, thus, must not have any
inﬂuence on the resulting numbers. When a technique such
as the common def/use pruning changes the fault model’s
uniform distribution into a distribution that is strongly biased
by the program execution, each result must be weighted
with the corresponding data lifetime to compensate.
unweighted experiment numbers. Barbosa et al. [34] recognize
that weighting is needed to compensate for the effects of
pruning, but conclude that the difference is small for the
benchmarks they used. In contrast, our benchmark examples
in Figure 2 serve as a warning that this is not always the
case. Alexandersson and Karlsson [37] realize that def/use
pruning affects the comparability of their results to those of
other studies. Other def/use pruning descriptions simply omit
the relevant detail whether weighting is used, e.g., Berrojo et
al. [33]. Additionally, tools clearly designed for the purpose
of testing (where weighting is not necessary), such as Relyzer
from Hari et al. [19], may be misused for comparison purposes.
Correct metrics should be integrated.
E. Combining Def/use Pruning and Sampling
The conclusions from the previous section are based on
def/use pruning of a full fault-space scan. However, often a
prohibitive number of FI experiments still remain after def/use
pruning. Pruning and sampling can be combined to further
reduce the experiment count. Clearly, the combination of both
techniques must yield the same results as pure sampling, but
with reduced effort.
Therefore, samples (fault-space coordinates) have to be
drawn uniformly from the raw, unpruned fault space to get
a representative sample of the entire fault-space population.
The def/use pruning is then carefully applied in a second step:
We only need to conduct a single FI experiment for fault-
space coordinates in the sample that belong to the same def/use
equivalence class. Nevertheless, we still need to count the results
of all sampled fault-space coordinates to properly calculate
the estimate for the entire fault-space population. Thereby,
even coordinates known to result in “No Effect” (because their
def/use equivalence class ends with a store, cf. Section III-C)
must be included, although we will lift that requirement in
Section V-C.
The other way around, applying def/use pruning ﬁrst and
then drawing samples uniformly from the already-pruned fault
space, i.e., picking def/use equivalence classes with the same
probability, leads to a biased estimate. A fault-space coordinate
that belongs to a small def/use equivalence class would be
included in the sample with a higher probability than for
uniform sampling of the raw fault space. The reason is that the
weight of each equivalence class biases the selection probability
of its fault-space coordinates.
Pitfall 2: Biased Sampling
Hence, our second pitfall is biased sampling. If def/use
pruning and sampling are combined, the sampling process
must pick samples from the raw, unpruned fault space.
If several samples belong to the same def/use equivalence
class, only a single FI experiment needs to be conducted
for them, but all samples count in the estimate.
In the literature, a lack of result weighting is in most
cases hidden behind fault-coverage factor percentages that
do not reveal whether weights were applied. One example
where the additionally provided data indicates that no weights
were used is from Hoffmann et al. [36], who compare the
fault susceptibility of two embedded operating systems using
IV. FOOLING FAULT COVERAGE:
A GEDANKENEXPERIMENT
In this section, we will conduct a Gedankenexperiment
with an apparently ineffective software-based hardware fault-
tolerance mechanism protecting a simple benchmark program,
and miraculously improving its fault coverage. Subsequently,
324324
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:50:36 UTC from IEEE Xplore.  Restrictions apply. 
we will revisit the fault-coverage numbers for our BIN_SEM2
and SYNC2 benchmarks, and wonder how effective the used
SUM+DMR hardening mechanism really is.
A. Hi, A Simple Benchmark
Figure 3 shows the C-like source code for a tiny benchmark
program that initializes a local character array, and subsequently
communicates both character values to the outside world via
the serial interface. The corresponding machine code consists
of eight machine instructions consisting of four load and four
store instructions. Figure 3a shows these loads and stores (“R”
respectively “W”), similarly as in Figure 1b, in the complete
fault space of this benchmark, spanning 16 bits on the memory
axis, and eight cycles on the time axis.
If we run a full fault-space scan, i.e., run one independent
FI experiment for every discrete coordinate in the fault space,
and observe whether the benchmark’s output is identical to
the golden run (it is supposed to say “Hi”), the black-dotted
experiments in Figure 3a will turn out as a “Failure”, and the
other ones as “No Effect”. In the “Failure” cases, the fault hits
msg[0] at address 0x1 while the letter ’H’ is stored there
but not yet read back (this happens in CPU instruction #5),
or analogously msg[1] at address 0x2 while the datum ’i’
lives there. The “No Effect” cases are FI experiments where
the fault is subsequently overwritten (before the store cycle
#1 respectively #3), or it is not activated anymore because the
program terminates (after the load cycle #4 respectively #6).
The fault coverage cbaseline can easily be calculated (cf.
Section III-B and III-D) by counting the number of experiments
Nbaseline = 8· 16 = 128 and the number of “Failure” outcomes
Fbaseline = 3 · 8 · 2 = 48, and inserting them into Equation 2:
cbaseline = 1 − Fbaseline
Nbaseline
= 62.5%
B. The Fault-Space Dilution Delusion
Now, we apply a hypothetical software-based fault-tolerance
method – we call it “Dilution Fault Tolerance”, or short DFT
– on the baseline’s machine code by conducting a program
transformation. It works by prepending four NOP instructions
(no operation, performing no real work for one cycle each) to
the machine code, increasing the benchmark’s runtime from
eight to twelve CPU cycles. Figure 3b shows the modiﬁed
fault-space diagram for the DFT-hardened benchmark: The
loads and stores have shifted four cycles to the right, and the
newly added experiment dots are all “No Effect”, as no live
data is stored in memory before the original beginning of the
benchmark.
Again calculating the fault coverage chardened with
Nhardened = 12 · 16 = 192 and the number of “Failure”
outcomes Fhardened = 3 · 8 · 2 = 48 yields:
chardened = 1 − Fhardened
Nhardened
= 75.0%
Interestingly, by applying a seemingly ineffective “fault
tolerance” program transformation, we increased the fault
coverage by 12.5 percent points. In fact, we could arbitrarily
increase the coverage to any chardened < 100% by inserting
more NOPs!
Now, the attentive reader may point to the literature, and
cite, e.g., Barbosa et al. [34], who argue (in the context of their
def/use fault-space pruning technique) that never activated faults
– a priori known “No Effect” results – should not be included in
the coverage calculation. The newly added experiment dots in
Figure 3b, that contributed to Nhardened in the above calculation
would disappear again (though for no authoritative reason), and
yield chardened = cbaseline.
As a reaction to such disgraceful attempts to make our DFT
mechanism look bad, we would devise DFT’, which replaces the
NOPs by memory reads. For instance, alternatingly executing
ld r1 ← 0x1 and ld r1 ← 0x2 instructions to read
those two memory locations. Now, all newly added experiment
dots would represent faults that actually are “activated” – by
being loaded into a CPU register (and subsequently discarded).
DFT’ would be back at chardened = 75.0%, this time with the
restriction from Barbosa et al. [34].
The central problem with this restriction is, that in a black-
box technique such as FI the “activation” of a fault, turning
it into an error, is an extremely vague business in itself. It
strongly depends on the sophistication of the used fault-space
pruning technique, e.g., not injecting into unused memory, not
injecting into state that is known to be overwritten afterwards,
not injecting into state that is known to be masked by subsequent
arithmetic operations [19], . . . – this list could be continued
for a while. In our opinion, this vague distinction between
“activation” and “no activation” should therefore have no place
in the context of an objective fault-tolerance benchmarking
metric. Though, we will see in Section V that benchmark
comparison can take place without having to decide this
question at all.
To summarize, a seemingly unsuspicious – but entirely
artiﬁcial – program transformation can severely skew the fault-
coverage metric.
C. Analyzing the Root Cause
The remaining question is: Why is the fault-coverage metric
unﬁt to discover the obvious ineffectiveness of the Dilution
Fault-Tolerance mechanism – and does this metric skewing
not only happen in an artiﬁcial example, but also occur in
real-world examples?
The fact that we used a (hypothetical) full fault-space scan in
the example, and not sampling, as most works in the community,
is not the culprit. A sufﬁcient number of samples taken from
both fault spaces in Figure 3 would yield estimates close to
the exact numbers for cbaseline and chardened from the previous
section.
A closer look at the deﬁnition of fault coverage (Equation 2)
suggests that the metric itself is unﬁt for benchmark comparison:
The calculated percentages are not relative to the same base
– a common N for both benchmarks – as the divisor directly
depends on the benchmark’s runtime, and also its memory
usage. (The DFT could also simply have used more memory
for no particular purpose instead of prolonging the benchmark’s
runtime). Due to the usual overhead in space and time for
most software-based fault-tolerance mechanisms, a different
fault-space size for baseline and hardened variants must be
considered the norm rather than the exception.
325325
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:50:36 UTC from IEEE Xplore.  Restrictions apply. 
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:10)(cid:1)(cid:7)(cid:6) (cid:8)(cid:9)(cid:3)(cid:7)(cid:6)(cid:1)(cid:4)(cid:5)(cid:10)
(cid:11)(cid:12)(cid:13) (cid:11)(cid:12)(cid:13) (cid:11)(cid:12)(cid:13) (cid:11)(cid:12)(cid:13)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:1)(cid:7)(cid:8) (cid:9)(cid:10)(cid:3)(cid:7)(cid:8)(cid:1)(cid:4)(cid:5)(cid:6)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:4)
(cid:8)
(cid:23)
(cid:5)
(cid:22)
(cid:4)
(cid:5)
(cid:21)
(cid:20)
(cid:19)
(cid:8)
(cid:18)
(cid:11)
(cid:27)
(cid:26)
(cid:4)
(cid:9)
(cid:9)
(cid:8)
(cid:21)
(cid:25)
(cid:25)
(cid:24)
(cid:4)
(cid:8)
(cid:23)
(cid:5)
(cid:22)
(cid:4)
(cid:5)
(cid:21)
(cid:20)
(cid:19)
(cid:8)
(cid:18)
(cid:10)
(cid:27)
(cid:26)
(cid:4)
(cid:9)
(cid:9)
(cid:8)
(cid:21)
(cid:25)
(cid:25)
(cid:24)
(cid:10)(cid:15)
(cid:17)
(cid:10)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:1)(cid:6)
(cid:8)
(cid:1)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:3)(cid:4)(cid:5)(cid:4)(cid:6)(cid:7)
(cid:20)(cid:21)(cid:21)(cid:18)(cid:16)(cid:22)(cid:17)(cid:18)(cid:19)(cid:19)(cid:14)(cid:21)(cid:23)(cid:24)(cid:17)(cid:22)
(cid:25)(cid:18)(cid:26)(cid:27)(cid:26)(cid:28)(cid:14)(cid:29)(cid:30)(cid:18)(cid:31)(cid:32)(cid:31)(cid:33)(cid:22)(cid:22)(cid:18)(cid:34)(cid:35)