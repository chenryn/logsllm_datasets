ing the number of observations, rendering it infeasible to use. Instead, we use
a probabilistic version based on the HyperLogLog data structure (HLL; [10]).
HLL provides approximate results with well-deﬁned error margins. It uses O(1)
memory, is composable, and provides meaningful intermediary results.
Top-k. Finding the top-k “heavy hitters” represents another common task.
However, similar to Unique, a naive implementation requires O(n) memory, with
n the number of observations. We thus likewise choose a probabilistic version
instead: Metwally et al’s algorithm [17], which in addition also provides estimates
on the number of times speciﬁc elements were seen. Just as HLL, the algorithm
satisﬁes all our constraints, including composability (see [5]).
Sampler. For many applications it is not only interesting to know the ﬁ-
nal result itself yet also to receive with it a sample of individual contributing
values (e.g., when seeing an unusually high number of DNS requests from a sin-
gle source, seeing a few example requests can prove illuminating). We support
that by providing a “Sample” reducer that maintains a ﬁxed number of k uni-
formly distributed samples taken out of the complete observation stream. By
using reservoir sampling [26], we are able to satisfy all our constraints.
326
J. Amann, S. Hall, and R. Sommer
2.5 Comparison with MapReduce
It is no accident that our model, and terminology, shares similarities with
MapReduce [6]. They both operate in similar phases. The “Map” step of MapRe-
duce corresponds to taking observations in our model; in either approach, input
data maps to a key and a value. The “Reduce” step of MapReduce is equiva-
lent to our server-side merging of results computed locally at the sensors. What
we call a “reducer” indeed corresponds to a “combiner” in a reﬁnement of the
MapReduce model: combiner functions merge partial results before data gets
forwarded [6]. The underlying reason for this naming diﬀerence is that in our
design the main part of the data reduction does indeed occur already on the
sensor nodes.
One diﬀerence between the two models concerns the input side. While either
approach assumes suitably pre-split sets of input, MapReduce does not tie them
to a speciﬁc compute node. In our model, by tapping disjunct packet sources
yet not further dividing up their inputs, we implicitly link each source with one
speciﬁc sensor that processes it. While this remains less ﬂexible, it provides a
signiﬁcant performance advantage by eﬀectively leveraging the network itself
for partitioning input appropriately, either indirectly by virtue of its structure
(in the case of tapping diﬀerent physical locations), or directly via a front-end
load-balancer (in the case of a cluster setup [25]). In either case we avoid the—
potentially prohibitive—performance penalty of redistributing traﬃc within the
summary statistics framework.
Overall, we emphasize that the two approaches share signiﬁcant similarities.
As such, we do not consider our framework’s abstract computational model the
primary contribution of this work, yet rather its integration into an eﬃcient,
deployable system that provides a transparent, simple-to-use API to the user.
3 Implementation
We implement our design of the summary statistics framework on top of the
Bro network monitoring platform [3,19]. Bro aligns well with our objectives as it
(i) provides the user with the necessary ﬂexibility through its Turing-complete
scripting language; (ii) extracts a wide range of features from network traﬃc
to measure; and (iii) supports distributed operation in cluster setups. We im-
plement the summary statistics framework completely within Bro’s scripting
language, with no changes to the system’s C++ core for the general functionality.
As the only extension to Bro’s internals, we add support for the probabilistic
data structures that some of the reducers deploy. Our implementation comes
with pre-written analysis scripts that leverage its capabilities for detection of,
e.g., host and port scans, traceroutes, and SQL injection attacks. In the follow-
ing, we discuss our implementation in terms of its user interface (§3.1), cluster
integration (§3.2), and computation plugins (§3.3) that reducers can leverage.
Count Me In: Viable Distributed Summary Statistics
327
3.1 User Interface
The user interface of the summary statistics framework exposes a set of public
functions in Bro’s scripting language. In the following, we brieﬂy sketch the main
functionality available to users. As a simple example we assume the setting of a
small network site that aims to track the number of connections that each local
host initiates to external destinations, recording them into a log ﬁle on a hourly
basis.
Measuring. Setting up the analysis requires two steps: (i) feeding all out-
going connections into the summary statistics framework as observations, and
(ii) deﬁning a corresponding summary statistic that aggregates connections by
their originator addresses. For the former, the framework provides the observe()
function, which injects a key/value pair into an observation stream. The frame-
work supports an arbitrary number of independent streams and identiﬁes them
by user-chosen names. For the example application we hook into Bro’s connec-
tion processing and pass on every connection attempt originating from a local
host:1
event connection_attempt(c: connection) {
[... return if connection does not originate from the local network ... ]
SumStats::observe(
stream
key
value
= "host-conn-attempts";
= c.originator;
= 1;
# Name of observation stream
# Observation key (IP address)
# Observation value ("one attempt")
);
}
For the second step we ﬁrst deﬁne a reducer that adds up connection attempts:
local r1: SumStats::Reducer = [
stream = "host-conn-attempts";
apply
];
= SumStats::SUM;
# Name of observation stream
# Reducer plugin to use
Here, we link the reducer to the observation stream to process, host-conn-
attempts, and specify Summation as the statistical operation to apply to the
incoming values. For a list of currently supported operations, see §2.4; users can
add further ones by supplying custom plugins (see §3.3).
Next, we deﬁne the actual summary statistic by calling the framework’s create()
function. In its simplest form, the function takes just four parameters:
SumStats::create(
= "local-origins";
name
epoch
= 1 hour;
reducers = set(r1);
epoch_result = epoch_func;
# Name of the summary statistic
# Measurement interval (epoch)
# Set of reducers to deploy
# End of epoch callback function
);
1 In this and later examples we simplify Bro’s syntax for better readability.
328
J. Amann, S. Hall, and R. Sommer
With that, the summary statistic conﬁguration is complete. During runtime,
Bro will now call the epoch_result function each hour and provide it with the
number of outgoing connections per local host. The function can process the
data arbitrarily, such as by logging the information into a ﬁle.
Thresholding. We now extend the previous example to report hosts that
exceed a predeﬁned threshold of connection attempts. Here, our implementation
deviates slightly from the discussion in §2. While the design provides for a generic
predicate to check for arbitrary conditions while a computation is in progress,
our implementation currently hardcodes threshold checks as the only available
option. In our experience, thresholding represents the dominant application. By
speciﬁcally targeting it, we can simplify both the interface (making it more intu-
itive for users) and the implementation (reducing complexity in the distributed
setting). However, there’s no conceptual limitation that would prevent us from
adding the more general case in the future.
Adding a threshold check to the previous example involves passing three more
parameters to the create() call: a function that retrieves the current measure-
ment value for a key, a numerical threshold to compare that value with, and the
trigger function to execute when the value exceeds the threshold:
SumStats::create(
[...]
threshold
threshold_val
threshold_crossed = crossed_func; # Alarm.
= 10000.0;
= val_func;
# Threshold value
# Retrieve current value
);
The val_func receives a key and the current intermediate reducer values for
this key. It uses them to return the value to be checked against the threshold.
function val_func(key, val) : double {
return val["host-conn-attempts"].sum;
}
In this example, val_func simply returns the current number of connection
attempts for a host.2 However, the function could be more complex than that.
In our application, one could for example instead implement a threshold relative
to the number of successful connections. For that one would add a second obser-
vation stream, say host-conn-successes, along with a corresponding reducer
r2 added to the create() call. This modiﬁed val_func would then calculate
percentages:
function val_func(key, val) : double {
return val["host-conn-attempts"].sum / val["host-conn-successes"].sum;
}
2 As the code suggests, the state is maintained in a number of nested table struc-
tures (hash maps) indexed by the measurements.
Count Me In: Viable Distributed Summary Statistics
329
For completeness, we conclude the example by showing the trigger function
that turns an exceeded threshold into an alarm via Bro’s provided NOTICE func-
tion:
function crossed_func(key, val) {
NOTICE("Host %s exceeded conn threshold: %d conn attempts", key, val);
}
3.2 Cluster Integration
As discussed in §2.3, the summary statistics framework targets deployment in
distributed settings where a set of local vantage points contribute to a global
measurement. Bro supports distributed setups through clustering [25]. In a Bro
cluster, a set of worker nodes examines independent traﬃc streams and share
their results through a central manager node. Each node might either monitor a
physically separate point in a network or, more commonly, contribute to analyz-
ing a single high-speed link by analyzing a smaller traﬃc slice that a front-end
load-balancer assigns to it. Typically such load-balancing operates on a per-
ﬂow basis and, hence, satisﬁes our design constraint of requiring disjunct input
streams in distributed summary statistics framework deployments.
Our Bro implementation closely follows the distributed design presented in
§2.3, including the optimized notiﬁcation/polling scheme for timely trigger ex-
ecution. We put particular emphasis on hiding the increased complexity of the
distributed setting from the user: the framework uses the same API for both
single-instance and distributed setups; user-supplied script code works transpar-
ently in either setting. In particular, users do not need to specify which parts of
their code executes where; the summary statistics framework automatically runs
the respective functionality on the correct nodes (i.e., extracting observations
and processing reducers on the workers; executing aggregation, thresholding,
and triggers on the manager).
3.3 Computation Plugins
The framework includes support for a number of computations for reducers to
deploy. Their implementations use a generic plugin interface that also allows
users to add further schemes of their own. Each computation plugin implements
two functions: one for adding a new observation, and one for merging computa-
tion state from diﬀerent nodes; either function has also access to the time range
that a observation stream spans and may include that into its calculations.
As an example, we show the implementation of the Minimum3:
3 The actual implementation is slightly more verbose to deal with corner cases like
undeﬁned values. We also again simplify the syntax to match previous examples.
Finally, we omit the deﬁnition of the state’s min attribute, which extends a predeﬁned
data type to add plugin-speciﬁc storage that maintains the current value.
330
J. Amann, S. Hall, and R. Sommer
# Update current minimum.
function add(key, val, state) {
if ( val < state.min )
state.min = val;
}
# Aggregate two values by taking the smaller.
function aggregate(out, in1, in2) {
out.min = (in1.min < in2.min) ? in1.min : in2.min;
}
In addition to Minimum, our implementation also provides plugins for Max-
imum, Sum, Average, Standard Deviation and Variance, Top-k), Unique, and
Sampling (see §2.4).
4 Applications and Deployment
In this section we demonstrate the summary statistics framework’s capabilities
with a set of example applications. The ﬁrst four (scan detector in §4.1, brute-
force login detector in §4.2, SQL injection detector in §4.3, traceroute detector
in §4.4) ship with Bro since version 2.2, and many network sites use them opera-
tionally now. We furthermore discuss three measurement tasks (traﬃc matrix in
§4.6, top-k in §4.5, visualization in §4.7) that we ran experimentally in produc-
tion environments. For these we make the corresponding (short) implementation
scripts available in a separate repository [2].
Note that these are only example applications demonstrating the capabilities
of the framework. In practice, operators will evaluate the suitability of the sum-
mary statistics framework for their tasks and implement their own scripts as
appropiate.
4.1 Scan Detection
Detecting port and address scans constitutes an important capability for security
operations. We implemented a corresponding scan detector as a Bro script on top
of the summary statistics framework. The script tracks the number of unique
ports and destination addresses that each source IP attempts to connect to,
generating alarms when they exceed, by default, 15 or 25 attempts within a 5
minute interval, respectively. Users can easily adjust either threshold, as well as
the time interval. The script is about 160 lines long, with the bulk representing
logic for connection processing and customization functionality. The core of the
script consists of just two pairs of function calls setting up the summary statistics
and feeding in observations. In particular, there is no need for code to deal with
distributed Bro setups. For comparison, older Bro versions used to ship with a
manually written, complex scan detection script that consisted of over 600 lines
of script code, with most of that focusing on maintaining the necessary counters
inside nested hash tables.
Count Me In: Viable Distributed Summary Statistics
331
Indiana University (IU) has been running versions of our new scan detector
script for more than 9 months on their 49-node Bro cluster, monitoring the site’s
10 GE upstream link. Their total traﬃc (incoming and outgoing) peaks at about
13 Gb/s on workdays and generally averages at about 5 Gb/s. Figures 3 and 4
show the number of incoming scans to diﬀerent destination addresses by time
and by weekday, respectively, for subinterval of that time, as identiﬁed by our
detector. At peak times, there are more than 290 unique external IP addresses
conducting scans of the network each hour. In total, IU encountered 33,452
scanners from 2014-02-19 to 2014-03-20. The network operators use the script’s
output to automatically block external scanners at the border router in near-
real time. Note that due to this automated blocking, with blocks often being
triggered before the end of a monitoring interval, the numbers in this section
represent a lower bound.
Sun Mon
Tues Wed Thurs
Fri
Sat
15000
10000
r
u
o
h
r
e
p
s
t
s
o
h
d
e
n
n
a