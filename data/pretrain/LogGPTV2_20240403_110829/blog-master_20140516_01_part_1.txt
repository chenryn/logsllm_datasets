## zfs on CentOS 6.5 x64 compare performance with ext4 use postgresql pgbench  
### 作者                                                                                                                                           
digoal                                                                                                                                             
### 日期                                                                                                                                                            
2014-05-16                                                                                                                                   
### 标签                                                                                                                                         
PostgreSQL , Linux , ZFS                                                                                                                                       
----                                                                                                                                                   
## 背景         
zfs可以认为是raid和文件系统的结合体.  
解决了动态条带和数据与校验位的一致性问题, 所以具有极高的可靠性和性能.  
https://pthree.org/2012/12/05/zfs-administration-part-ii-raidz/  
因为zfs需要接管硬盘, 所以在有RAID卡的环境中, 需要设置为 JBOD模式.  
http://en.wikipedia.org/wiki/Non-RAID_drive_architectures  
```  
同时ZFS利用SLOG(Separate Intent Log)设备存储ZIL(ZFS Intent Log)来提升写性能.  
(使用slog后, 即使断电, 也不会导致数据不一致, 只是会导致ZFS中新的数据没有FLUSH到磁盘, 呈现老的状态, 类似PostgreSQL 的async commit.)  
(It's important to identify that all three devices listed above can maintain data persistence during a power outage. The SLOG and the ZIL are critical in getting your data to spinning platter. If a power outage occurs, and you have a volatile SLOG, the worst thing that will happen is the new data is not flushed, and you are left with old data. However, it's important to note, that in the case of a power outage, you won't have corrupted data, just lost data. Your data will still be consistent on disk.)  
建议将SLOG放在ssd或者nvram设备中, 提升写性能.  
甚至可以拿slog来做增量文件系统恢复, 有点和PostgreSQL的xlog功能类似呢.  
因为slog的重要性, 一般加slog设备使用mirror.  
例如 :   
# zpool create tank mirror /tmp/file1 /tmp/file2 mirror /tmp/file3 /tmp/file4 log mirror sde sdf cache sdg sdh  
# zpool status tank  
  pool: tank  
 state: ONLINE  
 scan: none requested  
config:  
	NAME            STATE     READ WRITE CKSUM  
	tank            ONLINE       0     0     0  
	  mirror-0      ONLINE       0     0     0  
	    /tmp/file1  ONLINE       0     0     0  
	    /tmp/file2  ONLINE       0     0     0  
	  mirror-1      ONLINE       0     0     0  
	    /tmp/file3  ONLINE       0     0     0  
	    /tmp/file4  ONLINE       0     0     0  
	logs  
	  mirror-2      ONLINE       0     0     0  
	    sde         ONLINE       0     0     0  
	    sdf         ONLINE       0     0     0  
	cache  
	  sdg           ONLINE       0     0     0  
	  sdh           ONLINE       0     0     0  
errors: No known data errors  
另外需要注意: SLOG如果使用SSD来做的话, 因为SSD的使用寿命是和擦写次数相关的, 所以我们可以根据SLOG的写入速度来评估SLOG能用多久.  
SLOG Life Expectancy  
Because you will likely be using a consumer-grade SSD for your SLOG in your GNU/Linux server, we need to make some mention of the wear and tear of SSDs for write-intensive scenarios. Of course, this will largely vary based on manufacturer, but we can setup some generalities.  
First and foremost, ZFS has advanced wear-leveling algorithms that will evenly wear each chip on the SSD. There is no need for TRIM support, which in all reality, is really just a garbage collection support more than anything. The wear-leveling of ZFS in inherent due to the copy-on-write nature of the filesystem.  
Second, various drives will be implemented with different nanometer processes. The smaller the nanometer process, the shorter the life of your SSD. As an example, the Intel 320 is a 25 nanometer MLC 300 GB SSD, and is rated at roughly 5000 P/E cycles. This means you can write to your entire SSD 5000 times if using wear leveling algorithms. This produces 1500000 GB of total written data, or 1500 TB. My ZIL maintains about 3 MB of data per second. As a result, I can maintain about 95 TB of written data per year. This gives me a life of about 15 years for this Intel SSD.  
However, the Intel 335 is a 20 nanometer MLC 240 GB SSD, and is rated at roughly 3000 P/E cycles. With wear leveling, this means you can write you entire SSD 3000 times, which produces 720 TB of total written data. This is only 7 years for my 3 MBps ZIL, which is less than 1/2 the life expectancy the Intel 320. Point is, you need to keep an eye on these things when planning out your pool.  
Now, if you are using a battery-backed DRAM drive, then wear leveling is not a problem, and the DIMMs will likely last the duration of your server. Same might be said for 10k+ SAS or FC drives.  
ZIL的空间一般不需要太大, 作者用了4GB的分区作为SLOG设备.  
ZFS另一个强大之处是CACHE算法, 结合了LRU,LFU, 保留最近使用的和使用最频繁的块在缓存中.  
同时ZFS还支持二级缓存, 可以使用IOPS能力强大的块设备作为二级缓存设备.  
```  
https://pthree.org/2012/12/07/zfs-administration-part-iv-the-adjustable-replacement-cache/  
```  
创建zpool时, 尽量使用设备ID (/dev/disk/by-id/*) , 不要使用别名如sda, 因为别名可能重启后会发生变化.  
以CentOS 6.4 x64为例, 介绍一下zfs的安装.  
下载  
[root@db-172-16-3-150 soft_bak]# wget http://archive.zfsonlinux.org/downloads/zfsonlinux/spl/spl-0.6.2.tar.gz  
[root@db-172-16-3-150 soft_bak]# wget http://archive.zfsonlinux.org/downloads/zfsonlinux/zfs/zfs-0.6.2.tar.gz  
安装spl  
[root@db-172-16-3-150 spl-0.6.2]# tar -zxvf spl-0.6.2.tar.gz  
[root@db-172-16-3-150 soft_bak]# cd spl-0.6.2  
[root@db-172-16-3-150 spl-0.6.2]# ./autogen.sh   
[root@db-172-16-3-150 spl-0.6.2]# ./configure --prefix=/opt/spl0.6.2  
[root@db-172-16-3-150 spl-0.6.2]# make && make install  
安装zfs  
[root@db-172-16-3-150 soft_bak]# cd /opt/soft_bak/  
[root@db-172-16-3-150 soft_bak]# tar -zxvf zfs-0.6.2.tar.gz   
[root@db-172-16-3-150 soft_bak]# cd zfs-0.6.2  
[root@db-172-16-3-150 zfs-0.6.2]# yum install -y libuuid-devel  
[root@db-172-16-3-150 zfs-0.6.2]# ./configure --prefix=/opt/zfs0.6.2  
[root@db-172-16-3-150 zfs-0.6.2]# make && make install  
测试所在系统的Solaris的移植性, 使用splat前需要加载splat模块, 否则会报错.  
如果遇到错误, 使用-vv输出详细信息, 找到错误原因.  
splat - Solaris Porting LAyer Tests  
[root@db-172-16-3-150 soft_bak]# cd /opt/spl0.6.2/sbin/  
[root@db-172-16-3-150 sbin]# ./splat -a  
Unable to open /dev/splatctl: 2  
Is the splat module loaded?  
[root@db-172-16-3-150 spl0.6.2]# modprobe splat  
[root@db-172-16-3-150 spl0.6.2]# /opt/spl0.6.2/sbin/splat -a  
------------------------------ Running SPLAT Tests ------------------------------  
                kmem:kmem_alloc           Pass    
                kmem:kmem_zalloc          Pass    
                kmem:vmem_alloc           Pass    
                kmem:vmem_zalloc          Pass    
                kmem:slab_small           Pass    
                kmem:slab_large           Pass    
                kmem:slab_align           Pass    
                kmem:slab_reap            Pass    
                kmem:slab_age             Pass    
                kmem:slab_lock            Pass    
                kmem:vmem_size            Pass    
                kmem:slab_reclaim         Fail  Timer expired  
               taskq:single               Pass    
               taskq:multiple             Pass    
               taskq:system               Pass    
               taskq:wait                 Pass    
               taskq:order                Pass    
               taskq:front                Pass    
               taskq:recurse              Pass    
               taskq:contention           Pass    
               taskq:delay                Pass    
               taskq:cancel               Pass    
                krng:freq                 Pass    
               mutex:tryenter             Pass    
               mutex:race                 Pass    
               mutex:owned                Pass    
               mutex:owner                Pass    
             condvar:signal1              Pass    
             condvar:broadcast1           Pass    
             condvar:signal2              Pass    
             condvar:broadcast2           Pass    
             condvar:timeout              Pass    
              thread:create               Pass    
              thread:exit                 Pass    
              thread:tsd                  Pass    
              rwlock:N-rd/1-wr            Pass    
              rwlock:0-rd/N-wr            Pass    
              rwlock:held                 Pass    
              rwlock:tryenter             Pass    
              rwlock:rw_downgrade         Pass    
              rwlock:rw_tryupgrade        Pass    
                time:time1                Pass    
                time:time2                Pass    
               vnode:vn_open              Pass    
               vnode:vn_openat            Pass    
               vnode:vn_rdwr              Pass    
               vnode:vn_rename            Pass    
               vnode:vn_getattr           Pass    
               vnode:vn_sync              Pass    
                kobj:open                 Pass    
                kobj:size/read            Pass    
              atomic:64-bit               Pass    
                list:create/destroy       Pass    
                list:ins/rm head          Pass    
                list:ins/rm tail          Pass    
                list:insert_after         Pass    
                list:insert_before        Pass    
                list:remove               Pass    
                list:active               Pass    
             generic:ddi_strtoul          Pass    
             generic:ddi_strtol           Pass    
             generic:ddi_strtoull         Pass    
             generic:ddi_strtoll          Pass    
             generic:udivdi3              Pass    
             generic:divdi3               Pass    
                cred:cred                 Pass    
                cred:kcred                Pass    
                cred:groupmember          Pass    
                zlib:compress/uncompress  Pass    
               linux:shrink_dcache        Pass    
               linux:shrink_icache        Pass    
               linux:shrinker             Pass  
接下来创建几个测试文件, 分别作为磁盘, 二级缓存, LOG.  
其中二级缓存使用SSD硬盘分区, LOG使用SSD上的两个文件.  
[root@db-172-16-3-150 ~]# cd /ssd4  
[root@db-172-16-3-150 ssd4]# dd if=/dev/zero of=./zfs.log1 bs=1k count=1024000  
[root@db-172-16-3-150 ssd4]# dd if=/dev/zero of=./zfs.log2 bs=1k count=1024000  
[root@db-172-16-3-150 ~]# cd /opt  
[root@db-172-16-3-150 opt]# dd if=/dev/zero of=./zfs.disk1 bs=1k count=1024000  
[root@db-172-16-3-150 opt]# dd if=/dev/zero of=./zfs.disk2 bs=1k count=1024000  
[root@db-172-16-3-150 opt]# dd if=/dev/zero of=./zfs.disk3 bs=1k count=1024000  
[root@db-172-16-3-150 opt]# dd if=/dev/zero of=./zfs.disk4 bs=1k count=1024000  
创建zpool, 测试性能(cache必需使用块设备, 不能直接用文件).  
[root@db-172-16-3-150 ssd4]# ll /dev/disk/by-id/  
total 0  
lrwxrwxrwx 1 root root  9 Apr 23 08:59 ata-OCZ-REVODRIVE3_OCZ-886PWVEQ351TAPNH -> ../../sdb  
lrwxrwxrwx 1 root root 10 Apr 23 08:59 ata-OCZ-REVODRIVE3_OCZ-886PWVEQ351TAPNH-part1 -> ../../sdb1  
lrwxrwxrwx 1 root root  9 Apr 23 08:59 ata-OCZ-REVODRIVE3_OCZ-Z2134R0TLQBNE659 -> ../../sda  
lrwxrwxrwx 1 root root 10 Apr 23 08:59 ata-OCZ-REVODRIVE3_OCZ-Z2134R0TLQBNE659-part1 -> ../../sda1  
lrwxrwxrwx 1 root root  9 Apr 23 08:59 scsi-360026b902fe2ce001261fa4506592f80 -> ../../sdc  
lrwxrwxrwx 1 root root 10 Apr 23 08:59 scsi-360026b902fe2ce001261fa4506592f80-part1 -> ../../sdc1  
lrwxrwxrwx 1 root root 10 Apr 23 08:59 scsi-360026b902fe2ce001261fa4506592f80-part2 -> ../../sdc2  
lrwxrwxrwx 1 root root 10 Apr 23 08:59 scsi-360026b902fe2ce001261fa4506592f80-part3 -> ../../sdc3  
lrwxrwxrwx 1 root root  9 Apr 23 08:59 scsi-360026b902fe2ce0018993f2f0c5734b3 -> ../../sdd  
lrwxrwxrwx 1 root root 10 Apr 23 08:59 scsi-360026b902fe2ce0018993f2f0c5734b3-part1 -> ../../sdd1  
lrwxrwxrwx 1 root root  9 Apr 23 08:59 scsi-SATA_OCZ-REVODRIVE3_OCZ-886PWVEQ351TAPNH -> ../../sdb  
lrwxrwxrwx 1 root root 10 Apr 23 08:59 scsi-SATA_OCZ-REVODRIVE3_OCZ-886PWVEQ351TAPNH-part1 -> ../../sdb1  
lrwxrwxrwx 1 root root  9 Apr 23 08:59 scsi-SATA_OCZ-REVODRIVE3_OCZ-Z2134R0TLQBNE659 -> ../../sda  
lrwxrwxrwx 1 root root 10 Apr 23 08:59 scsi-SATA_OCZ-REVODRIVE3_OCZ-Z2134R0TLQBNE659-part1 -> ../../sda1  
lrwxrwxrwx 1 root root  9 Apr 23 08:59 wwn-0x5e83a97e5dbf17f7 -> ../../sdb  
lrwxrwxrwx 1 root root 10 Apr 23 08:59 wwn-0x5e83a97e5dbf17f7-part1 -> ../../sdb1  
lrwxrwxrwx 1 root root  9 Apr 23 08:59 wwn-0x5e83a97e827c316e -> ../../sda  
lrwxrwxrwx 1 root root 10 Apr 23 08:59 wwn-0x5e83a97e827c316e-part1 -> ../../sda1  
lrwxrwxrwx 1 root root  9 Apr 23 08:59 wwn-0x60026b902fe2ce001261fa4506592f80 -> ../../sdc  
lrwxrwxrwx 1 root root 10 Apr 23 08:59 wwn-0x60026b902fe2ce001261fa4506592f80-part1 -> ../../sdc1  