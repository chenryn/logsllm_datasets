tElock
Armadillo
Armadillo
Armadillo
Deviation Detected?
YES
YES
YES
YES
YES
YES
YES
YES
YES
YES
Table 4. Real Malware with VM checks.
light a number of systems that use dynamic and static ap-
proaches for combating malware. Then, we focus on the
different ways to detect malware analysis environments,
emulators, and virtual machines. Finally, since we pro-
pose a component to replay malware, we survey related
work in the area of process and system replay.
Malware analysis and detection. The traditional ap-
proach to detect malware, as implemented in anti-virus
scanners, is based on (string) signatures that match spe-
ciﬁc malware binaries [36]. Because code obfuscation
and runtime packing can be used to easily evade this type
of detection, researchers have proposed more sophisti-
cated techniques, for example, detection based on model
checking [12], recognition of structural similarities be-
tween malware samples [24], and semantics-aware anal-
ysis of code templates that implement speciﬁc function-
ality [13].
Dynamic detection techniques are complementary to
static analysis and typically aim to detect the execution of
malicious code based on system call patterns [28]. Tools
exists to intercept Win32 function calls [21], or to per-
form taint analysis and track data dependencies between
system calls and library functions [9, 40]. This allows
one to capture the behavior of malware in a more precise
fashion and identify operations that are related.
Currently, the most popular approach for malware
analysis relies on sandboxes [1, 3–6, 8]. A sandbox is
an instrumented execution environment that runs an un-
known program, recording its interactions with the op-
erating system (via system calls) or other hosts (via the
network). Often, this execution environment is realized
as a system emulator or a virtual machine.
Stealth and transparent analysis. Since emulators and
virtual machines are popular choices for implementing
dynamic analysis systems, there have been a number of
attempts to develop checks (pieces of code) to detect
them.
Red pill [33] is arguably the most well-known check
to determine whether code is executed under VMware.
More checks have later been developed for VMware [19,
23, 32], but also for system emulators such as Bochs and
Qemu [19, 31, 32]. To this end, researchers have looked
for instructions that behave differently on an emulator
than on a real host, using both manual [19, 32] and au-
tomated fuzz testing techniques [31].
The increased efforts to detect emulators and virtual
machines have prompted researchers and practitioners to
look for ways to hide the presence of such execution en-
vironments. Initial work [26] focused on removing spe-
ciﬁc artifacts in VMware that are targeted by well-known
checks. Later, researchers proposed more complete sys-
tems that use virtualization [16] or dynamic translation in
combination with stealth implants [38] to remain trans-
parent to a wider range of malware checks. While these
systems are successful in hiding their presence, they in-
cur a performance penalty that is prohibitive when de-
ploying them in large-scale automated malware analysis
setups.
In addition to systems that attempt to remain transpar-
ent to malicious code, researchers have looked at ways
to detect that a malware sample contains such detection
checks. To perform this detection, the systems presented
in [11] and [22] compare the behavior of a sample on a
reference (real) host with the behavior of this sample on
an analysis (or virtual) host. However, both systems sim-
ply execute the malware under analysis in two different
environments. Unfortunately, as our experiments have
demonstrated, re-running the same sample twice can lead
to different behaviors that are not the result of any mal-
ware checks. Hence, this analysis approach is not reli-
able. In addition, the system presented in [22] also uses
a very costly technique (Ether in ﬁne-grained analysis
mode) to produce a reference system call trace.
Process and system replay. A number of systems ex-
ist that aim at providing deterministic replay of an ap-
plication or of an entire system [14]. For example, Re-
Virt [18] uses the virtual machine UMLinux to monitor
a process and create logs of its interaction with the guest
operating system. The logs are created on the host OS
and can then be used to replay the entire virtual machine.
However, ReVirt modiﬁes the host and guest operating
system and requires the analyzed programs to be run in-
side their virtual environment UMLinux. It is a power-
ful approach that allows comprehensive replaying, but it
cannot analyze a program that contains virtual machine
detection checks, since it uses a virtual machine itself.
Flashback [35] is a debugging tool for Linux that allows
process replaying. It creates shadow processes at vari-
ous checkpoints that mirror the state of a process at a
given time. In addition, the system traps system calls to
log their parameter values. In contrast to our solution,
Flashback provides its own system calls to enable user
programs to programmatically create snapshots at certain
checkpoints. For this reason, it needs to modify the op-
erating system. Another replaying tool is Jockey [34],
which inserts trampoline functions into system call code
to direct the program ﬂow to its own code where system
call parameter values are recorded. These logs are then
used in replay mode to reproduce the behavior of a pro-
cess. Finally, Tornado [29] enables to replay the applica-
tion input by intercepting and replaying the system calls.
Unfortunately, a common drawback of all the four afore-
mentioned approaches is that they run on Linux. Thus,
they cannot be used to analyze Windows binaries.
ExecRecorder [15] is a virtual-machine-based log and
replay framework for post attack analysis and recovery.
It can replay the execution of an entire system by check-
pointing the complete system state (virtual memory and
CPU registers, virtual hard disk and memory of all vir-
tual external devices) and logging all architectural non-
deterministic events. ExecRecorder is based on the sys-
tem emulator Bochs [25]. An advantage of ExecRecorder
is that it can also run Windows in its virtual environment.
However, it also suffers from the drawback that virtual
machine detecting malware cannot be analyzed.
7 Conclusions
Malicious code is one of the most signiﬁcant security
threats on the Internet. To assess the malicious poten-
tial of the thousands of new malware binaries that are
discovered every day, dynamic malware analysis systems
(sandboxes) have proven to be valuable tools. As a reac-
tion, malware authors have started to add checks to their
code that detect the presence of such sandboxes. When
a check determines that the malware program is ana-
lyzed, it typically hides malicious functionality or simply
crashes. As a result, security analysts might mistakenly
classify a binary as benign or underestimate its threat.
In this paper, we present a technique to reliably and
efﬁciently identify malware programs that attempt to de-
tect the presence of Anubis (which is our emulator-based
sandbox) and similar tools. Our technique works by
recording the system call trace of a program when it is
executed on an uninstrumented reference system. Then,
the binary is run on the analysis system, replaying the in-
puts that have been previously seen. Whenever the pro-
gram shows a different behavior, we conclude that the
malware has a split personality; that is, it has used CPU
semantics or timing attacks to identify the presence of
our sandbox. In this case, the binary can be forwarded
to a more costly, but fully transparent, analysis system
for further examination. Our experiments demonstrate
that our system effectively and efﬁciently detects binaries
with split personalities, while it can successfully replay
programs that do not contain any checks for Anubis or
the emulator (Qemu).
Acknowledgements
This work has been supported by the Austrian Science
Foundation, (FWF) under grant P18764, Secure Busi-
ness Austria (SBA), and the WOMBAT and FORWARD
projects funded by the European Commission in the 7th
Framework. Marco Cova was partially supported by a
Symantec Research Labs Graduate Fellowship.
References
[1] Anubis: Analyzing Unknown Binaries.
http://
anubis.seclab.tuwien.ac.at, 2009.
[2] Computer Forensic Solutions.
http://cfs-llc.
net/index.htm, 2009.
[3] CWSandbox.
http://www.cwsandbox.org/,
2009.
[4] Joebox: A Secure Sandbox Application for Windows.
http://www.joebox.org/, 2009.
[5] Norman Sandbox.
http://www.norman.com/
technology/norman_sandbox/, 2009.
http://www.threatexpert.
[6] ThreatExpert.
com/, 2009.
I. Habibi, D. Balzarotti, E. Kirda, and
[7] U. Bayer,
C. Kruegel.
Insights Into Current Malware Behavior.
In Proceedings of the USENIX Workshop on Large-Scale
Exploits and Emergent Threats (LEET), 2009.
[8] U. Bayer, C. Kruegel, and E. Kirda. TTAnalyze: A Tool
for Analyzing Malware. In Proceedings of the European
Institute for Computer Antivirus Research Annual Con-
ference (EICAR), 2006.
[9] U. Bayer, P. Milani Comparetti, C. Hlauschek,
C. Kruegel, and E. Kirda. Scalable, Behavior-Based Mal-
In Proceedings of the Symposium on
ware Clustering.
Network and Distributed System Security (NDSS), 2009.
[10] D. Brumley, C. Hartwig, Z. Liang, J. Newsome, D. Song,
and H. Yin. Automatically Identifying Trigger-based Be-
havior in Malware. In W. Lee, C. Wang, and D. Dagon,
editors, Botnet Detection: Countering the Largest Secu-
rity Threat. Springer, 2007.
[11] X. Chen, J. Andersen, Z. Mao, M. Bailey, and J. Nazario.
Towards an Understanding of Anti-virtualization and
In Pro-
Anti-debugging Behavior in Modern Malware.
ceedings of the International Conference on Dependable
Systems and Networks (DSN), 2008.
[12] M. Christodorescu and S. Jha. Static Analysis of Exe-
cutables to Detect Malicious Patterns. In Proceedings of
the USENIX Security Symposium, 2003.
[13] M. Christodorescu, S. Jha, S. Seshia, D. Song, and
R. Bryant. Semantics-aware Malware Detection. In Pro-
ceedings of the IEEE Symposium on Security and Pri-
vacy, 2005.
[14] F. Cornelis, A. Georges, M. Christiaens, M. Ronsse,
T. Ghesquiere, and K. D. Bosschere. A Taxonomy of
In Proceedings of the In-
Execution Replay Systems.
ternational Conference on Advances in Infrastructure for
Electronic Business, Education, Science, Medicine, and
Mobile Technologies on the Internet, 2003.
[15] D. A. S. de Oliveira, J. R. Crandall, G. Wassermann, S. F.
Wu, Z. Su, and F. T. Chong. ExecRecorder: VM-Based
Full-System Replay for Attack Analysis and System Re-
covery. In Proceedings of the Workshop on Architectural
and System Support for Improving Software Dependabil-
ity (ASID), pages 66–71, New York, NY, USA, 2006.
ACM.
[16] A. Dinaburg, P. Royal, M. Sharif, and W. Lee. Ether:
Malware Analysis via Hardware Virtualization Exten-
sions. In Proceedings of the ACM Conference on Com-
puter and Communications Security (CCS), 2008.
[17] L. Dorrendorf, Z. Gutterman, and B. Pinkas. Cryptanaly-
sis of the Windows Random Number Generator. In Pro-
ceedings of the ACM Conference on Computer and Com-
munications Security (CCS), 2007.
[18] G. W. Dunlap, S. T. King, S. Cinar, M. A. Basrai, and
P. M. Chen. ReVirt: Enabling Intrusion Analysis through
Virtual-Machine Logging and Replay. SIGOPS Oper.
Syst. Rev., 36(SI):211–224, 2002.
[19] P. Ferrie. Attacks on Virtual Machines. In Proceedings
of the Association of Anti-Virus Asia Researchers Confer-
ence, 2007.
[20] T. Garﬁnkel, K. Adams, A. Warﬁeld, and J. Franklin.
Compatibility is Not Transparency: VMM Detection
In Proceedings of the USENIX
Myths and Realities.
Workshop on Hot Topics in Operating Systems, 2007.
[21] G. Hunt and D. Brubacher. Detours: Binary Intercep-
tion of Win32 Functions. In Proceedings of the USENIX
Windows NT Symposium, pages 135–144, Berkeley, CA,
USA, 1999. USENIX Association.
[22] M. G. Kang, H. Yin, S. Hanna, S. McCamant, and
D. Song. Emulating Emulation-Resistant Malware.
In
Proceedings of the Workshop on Virtual Machine Secu-
rity (VMSec), 2009.
[23] T. Klein.
ScoopyNG – The VMware detection
http://www.trapkit.de/research/
tool.
vmm/scoopyng/index.html.
[24] C. Kruegel, E. Kirda, D. Mutz, W. Robertson, and G. Vi-
gna. Polymorphic Worm Detection Using Structural In-
formation of Executables. In Symposium on Recent Ad-
vances in Intrusion Detection (RAID), 2005.
[25] K. P. Lawton. Bochs: A Portable PC Emulator for
Unix/X. Linux Journal, (29), 1996.
[26] T. Liston and E. Skoudis.
On the Cutting
Thwarting Virtual Machine Detection.
Edge:
http://handlers.sans.org/tliston/
ThwartingVMDetection_Liston_Skoudis.
pdf, 2006.
[27] L. Martignoni, R. Paleari, G. F. Roglia, and D. Bruschi.
Testing CPU Emulators. In Proceedings of the Interna-
tional Symposium on Software Testing and Analysis (IS-
STA), 2009.
[28] L. Martignoni, E. Stinson, M. Fredrikson, S. Jha, and
J. Mitchell. A Layered Architecture for Detecting Ma-
licious Behaviors. In Proceedings of the Symposium on
Recent Advances in Intrusion Detection (RAID), 2008.
[29] F. C. Michiel, F. Cornelis, M. Ronsse, and K. D. Boss-
chere. TORNADO: A Novel Input Replay Tool.
In
Proceedings of the International Conference on Parallel
and Distributed Processing Techniques and Applications
(PDPTA), pages 1598–1604, 2003.
[30] A. Moser, C. Kruegel, and E. Kirda. Exploring Multiple
Execution Paths for Malware Analysis. In Proceedings
of the IEEE Symposium on Security and Privacy, 2007.
[31] R. Paleari, L. Martignoni, G. F. Roglia, and D. Bruschi. A
Fistful of Red-Pills: How to Automatically Generate Pro-
cedures to Detect CPU Emulators. In Proceedings of the
USENIX Workshop on Offensive Technologies (WOOT),
2009.
[32] T. Raffetseder, C. Kruegel, and E. Kirda. Detecting Sys-
tem Emulators. In Proceedings of the Information Secu-
rity Conference, 2007.
[33] J. Rutkowska.
VMM using
http://www.invisiblethings.org/
papers/redpill.html, 2004.
Red Pill... or how to detect
CPU instruction.
(almost)
one
[34] Y. Saito. Jockey: A User-space Library for Record-replay
Debugging. In Proceedings of the International Sympo-
sium on Automated Analysis-driven Debugging (AADE-
BUG), pages 69–76, 2005.
[35] S. M. Srinivasan, S. Kandula, S. K, C. R. Andrews, and
Y. Zhou. Flashback: A Lightweight Extension for Roll-
back and Deterministic Replay for Software Debugging.
In Proceedings of the USENIX Annual Technical Confer-
ence, pages 29–44, 2004.
[36] P. Szor. The Art of Computer Virus Research and De-
fense. Addison Wesley, 2005.
[37] A. Vasudevan and R. Yerraballi. Stealth Breakpoints. In
Proceedings of the Annual Computer Security Applica-
tions Conference (ACSAC), 2005.
[38] A. Vasudevan and R. Yerraballi. Cobra: Fine-grained
Malware Analysis using Stealth Localized Executions. In
Proceedings of the IEEE Symposium on Security and Pri-
vacy, 2006.
[39] J. Wilhelm and T. Chiueh. A Forced Sampled Execution
Approach to Kernel Rootkit Identiﬁcation. In Proceed-
ings of the Symposium on Recent Advances in Intrusion
Detection (RAID), 2007.
[40] H. Yin, D. Song, M. Egele, C. Kruegel, and E. Kirda.
Panorama: Capturing System-wide Information Flow for
Malware Detection and Analysis. In Proceedings of the
ACM Conference on Computer and Communications Se-
curity (CCS), 2007.