-   ::: para
    设置
    `/sys/block/devname/queue/ionice/slice_idle`{.filename}为`0`{.literal}
    :::
-   ::: para
    设置 `/sys/block/devname/queue/ionice/quantum`{.filename}
    为`64`{.literal}
    :::
-   ::: para
    设置 `/sys/block/devname/queue/ionice/group_idle`{.filename}为
    `1`{.literal}
    :::
:::
:::
:::
::: section
::: titlepage
## [⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Configuration_tools-Tuning_the_noop_scheduler}5.3.6. 调整 noop 调度器 {.title}
:::
::: para
`noop`{.systemitem} I/O 调度器主要对使用快速存储的受 cpu
限制的系统有用。请求在块层合并，因此通过编辑
`/sys/block/sdX/queue/`{.filename}
目录中的文件中块层参数，修改`noop`{.systemitem} 行为。
:::
::: variablelist
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tuning_the_noop_scheduler-add_random}[add_random]{.term}
:   ::: para
    一些 I/O 事件会影响 `/dev/random`{.filename} 的熵池
    。如果这些影响的负荷变得可测量，该参数可设置为 `0`{.literal}。
    :::
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tuning_the_noop_scheduler-max_sectors_kb}[max_sectors_kb]{.term}
:   ::: para
    指定 I/O 请求的最大尺寸（以千字节计算），默认值为
    `512`{.literal} KB。该参数的最小值是由存储设备的逻辑块大小决定的。该参数的最大值是由
    *`max_hw_sectors_kb`* 值决定的。
    :::
    ::: para
    I/O
    请求大于内部擦除块大小时，一些固态硬盘会表现不佳。在这种情况下，红帽推荐将
    *`max_hw_sectors_kb`* 减少至内部擦除块大小。
    :::
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tuning_the_noop_scheduler-nomerges}[nomerges]{.term}
:   ::: para
    大多数工作负载受益于请求合并。然而，禁用合并有助于调试目的。可设置参数为
    `0`{.literal} 禁用合并。默认设置下为启用（设置为 `1`{.literal}）。
    :::
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tuning_the_noop_scheduler-nr_requests}[nr_requests]{.term}
:   ::: para
    限定同一时间排队的读和写请求的最大数量。默认值为 `128`{.literal}，
    即在请求读或者写操作的下一个进程进入睡眠模式前有 128 个读请求和 128
    个写请求排队。
    :::
    ::: para
    对于延迟敏感应用程序，降低该参数值，并限制存储上的命令队列深度，这样回写
    I/O 便无法填充有写请求的设备队列。设备队列填充时，其他尝试执行 I/O
    操作的进程会进入睡眠模式，直到有可用队列空间。随后请求会以
    round-robin
    fashion（循环方式）分配，以防止一个进程持续使用队列所有点。
    :::
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tuning_the_noop_scheduler-optimal_io_size}[optimal_io_size]{.term}
:   ::: para
    一些存储设备用此参数报告最佳 I/O
    大小。如果报告该值，红帽建议您尽可能将应用程序发出 I/O 与最佳 I/O
    大小对齐，并是最佳 I/O 大小的倍数。
    :::
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tuning_the_noop_scheduler-read_ahead_kb}[read_ahead_kb]{.term}
:   ::: para
    定义操作系统在顺序读取操作阶段将预先读取的千字节数量，以便存储在页面缓存中可能马上需要的信息。设备映射程序经常受益于高的
    *`read_ahead_kb`* 值 128 KB
    ；对于访问将要被映射的设备这是一个良好的起点。
    :::
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tuning_the_noop_scheduler-rotational}[旋转]{.term}
:   ::: para
    一些固态硬盘不能正确公布其固态硬盘状态，并且会如传统旋转磁盘挂载。如果您的固态硬盘不能将它自动设置它为
    `0`{.literal}，那么请您手动设置，禁用调度器上不必要的搜寻减少逻辑。
    :::
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tuning_the_noop_scheduler-rq_affinity}[rq_affinity]{.term}
:   ::: para
    默认设置下，I/O 完成能在不同处理器上进行，而不是限定在发出 I/O
    请求的处理器上。 将*`rq_affinity`* 设置为 `1`{.literal}
    以禁用此能力，并只在发出 I/O
    请求的处理器上执行完成。这能提高处理器数据缓存的有效性。
    :::
:::
:::
::: section
::: titlepage
## [⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Configuration_tools-Configuring_file_systems_for_performance}5.3.7. 为性能配置文件系统 {.title}
:::
::: para
此章节包含红帽企业版 Linux 7
支持的每个文件系统的调整参数。用户格式化存储设备或者挂载格式化设备时，参数根据其值是否应当配置而分配。
:::
::: para
如果文件碎片或者资源争用引起性能损失，性能通常可通过重新配置文件系统而提高性能。然而，在有些用例中，可能需要更改应用程序。这种情况下，红帽建议联系客户支持以获得帮助。
:::
::: section
::: titlepage
### [⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Configuring_file_systems_for_performance-Tuning_XFS}5.3.7.1. 调整 XFS {.title}
:::
::: para
此章节包含对 XFS 文件系统格式化和挂载时可用的一些调整参数。
:::
::: para
XFS
默认格式化和挂载设置适用于大多数工作负载。红帽建议只在更改特定配置会对您的工作负载有益时对它们进行更改。
:::
::: section
::: titlepage
#### [⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tuning_XFS-Formatting_options}5.3.7.1.1. 格式化选项 {.title}
:::
::: para
格式化选项的更多信息参见手册页：
:::
``` screen
$ man mkfs.xfs
```
::: variablelist
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Formatting_options-Directory_block_size}[目录块大小]{.term}
:   ::: para
    目录块大小影响每个 I/O
    操可检索或修改的目录信息数量。目录块大小最小值即文件系统块大小（默认设置下为4 KB）。目录块大小最大值为
    `64`{.literal} KB。
    :::
    ::: para
    对于指定的目录块大小来说，大的目录比小的目录需要更多
    I/O。因为和小目录块的系统相比，大目录块大小的系统每 I/O
    操作会使用更多的处理能力。因此，根据您的工作负载，推荐使用尽可能小的目录和目录块大小。
    :::
    ::: para
    如文件系统比大量写和大量读工作负载的列出项目数量少，红帽推荐使用以下目录块大小，请参见
    〈[表 5.1
    "为目录块大小推荐的最大目录项"](#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#tabl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Directory_block_size-Recommended_maximum_directory_entries_for_directory_block_sizes){.xref}
    〉
    :::
    ::: table
    [⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#tabl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Directory_block_size-Recommended_maximum_directory_entries_for_directory_block_sizes}
    **表 5.1. 为目录块大小推荐的最大目录项**
    ::: table-contents
      目录块大小   最大项 （大量读操作）   最大项 （大量写操作）
      ------------ ----------------------- -----------------------
      4 KB         100000--200000          1000000--2000000
      16 KB        100000--1000000         1000000--10000000
      64 KB        \>1000000               \>10000000
    :::
    :::
    ::: para
    在不同大小文件系统中，目录块大小对读和写工作负载的影响的情况请参见
    XFS 文件。
    :::
    ::: para
    使用 `mkfs.xfs -l`{.command} 选项配置目录块大小。请参见
    `mkfs.xfs`{.command} 手册页。
    :::
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Formatting_options-Allocation_groups}[分配组]{.term}
:   ::: para
    分配组是独立的结构，指示自由空间并在文件系统中一节分配
    inodes。只要同时操作影响不同分配组，每个分配组能被独立修改，这样 XFS
    同时执行分配和解除分配操作。因此文件系统中执行的同时操作数量和分配组数量相等。然而，由于执行同时操作的能力受到能够执行操作的处理器数量的限制，红帽建议分配组数量应多于或者等于系统中处理器的数量。
    :::
    ::: para
    多个分配组无法同时修改单独目录。因此，红帽推荐大量创建和移除文件的应用程序不要在单个目录中存储所有文件。
    :::
    ::: para
    使用 `mkfs.xfs -d`{.command} 选项配置分配组，更多信息参见
    `mkfs.xfs`{.command} 手册页。
    :::
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Formatting_options-Growth_constraints}[增长约束]{.term}
:   ::: para
    如您在格式化之后（通过增加更多硬件或通过自动精简配置），需要增加文件系统的大小，由于分配组大小在完成格式化之后不能更改，请务必仔细考虑初始文件布局。
    :::
    ::: para
    必须根据文件系统最终能力，而非根据初始能力调节分配组大小。占据所有使用空间的文件系统中分配组数量不应超过数百，除非分配组处于最大尺寸
    （1 TB）。因此，红帽向大部分文件系统推荐最大增长，允许文件系统是初始大小的十倍。
    :::
    ::: para
    增长 RAID
    数组的文件系统时，务必考虑额外护理，由于设备大小必须与固定多个分配组大小对齐，以便新分配组表头在新增加的存储中正确对齐。由于几何在格式化之后不能被更改，因此新存储也必须与已有的存储几何一致，因此，在同一个块设备上，不能优化不同几何的存储。
    :::
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Formatting_options-Inode_size_and_inline_attributes}[Inode 大小和内联属性]{.term}
:   ::: para
    如果 inode 有足够可用空间，XFS 能直接将属性名称和值写入
    inode。由于不需要额外的
    I/O，这些内联属性能够被获取和修改，达到比获取单独的属性块更快的量级。
    :::
    ::: para
    默认 inode 大小为 256 bytes。其中只有约 100 bytes
    大小可用于属性存储，取决于 inode
    上存储的数据范围指针数量。格式化文件系统时，增加 inode
    大小能增加存储属性可用的空间数量。
    :::
    ::: para
    属性名称和属性值两者都受到最大尺寸 254 bytes
    的限制。如果名称或者值超过 254 bytes
    长度，该属性会被推送到单独的属性快，而非存储在内联中。
    :::
    ::: para
    使用 `mkfs.xfs -i`{.command} 选项配置 inode 参数，更多信息请参见
    `mkfs.xfs`{.command} 手册页。
    :::
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Formatting_options-RAID}[RAID]{.term}
:   ::: para
    如果使用软件 RAID ，`mkfs.xfs`{.command}
    会使用合适的带状单元和宽度自动配置底层的硬件。然而，如果使用硬件
    RAID， 带状单元和宽度可能需要手动配置，这是因为不是所有硬件 RAID
    设备输出此信息。使用 `mkfs.xfs -d`{.command}
    选项配置带状单元和宽度。更多信息请参见 `mkfs.xfs`{.command} 手册页。
    :::
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Formatting_options-Log_size}[日志大小]{.term}
:   ::: para
    直到同步事件被触发，待定的更改在内存中累计，这个时候它们会被写入日志。日志大小决定同时处于进行中的修改数量。它也决定在内存中能够累计的最大更改数量，因此决定记录的数据写入磁盘的频率。与大日志相比，小日志促使数据更频繁地回写入磁盘。然而，大日志使用更多内存来记录待定的修改，因此有限定内存的系统将不会从大日志获益。
    :::
    ::: para
    日志与底层带状单元对齐时，日志表现更佳；换言之，它们起止于带状单元边界。使用
    `mkfs.xfs -d`{.command} 选项将日志对齐带状单元，更多信息请参见
    `mkfs.xfs`{.command} 手册页。
    :::
    ::: para
    使用下列 `mkfs.xfs`{.command} 选项配置日志大小，用日志大小替换
    *logsize* ：
    :::
    ``` screen
    # mkfs.xfs -l size=logsize
    ```
    ::: para
    更多详细信息参见 `mkfs.xfs`{.command} 手册页：
    :::
    ``` screen
    $ man mkfs.xfs
    ```
[⁠]{#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Storage_and_File_Systems.html#varl-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Formatting_options-Log_stripe_unit}[日志带状单元]{.term}
:   ::: para
    日志写操作起止于带状边界时（与底层带状单元对齐），存储设备上使用
    RAID5 或 RAID6 布局的日志写操作可能会表现更佳。`mkfs.xfs`{.command}