W
2006 Macbet
2006 Ston
W
V
2007 Antares
2007 BO2K
R
2007 GhostRAT R
2007 Zeus
T
2008 BatzBack W
B
2008 Grum
W
2009 Cairuh
T
2009 Hexbot2
2010 Carberp
T
T
2011 KINS
R
2011 PC-RAT
R
2012 AndroR
T
2012 Dexter
2013 Alina
T
V
2013 Beetle
T
2013 Pony2
R
2013 SharpBot
2014 Dendroid
R
2014 Gopnik
B
2014 OmegaRAT R
T
2014 Rovnix
R
2014 SpyNet
T
2014 Tinba
2015 Pupy
R
A Look into 30 Years of Malware Development
333
The 151 successfully tested samples that comprise our ﬁnal dataset have been
tagged with a year and a loose category. The year corresponds to their develop-
ment when stated by the source, otherwise with the year they were ﬁrst spotted
in the wild. They are also tagged with a coarse-grained malware type: Virus
(V), Worm (W), MacroVirus (M), Trojan (T), Botnet (B), or RAT (R). We are
aware that this classiﬁcation is rather imprecise. For instance, nearly all Botnets
and RATs include bots that can be easily considered as Trojans, Backdoors or
Spywares and, in some cases, show Worm features too. The same applies to some
of the more modern viruses, which also exhibit Worm-like propagation strate-
gies or behave like stand-alone Trojans. We chose not to use a more ﬁne-grained
malware type because it is not essential to our study and, furthermore, such
classiﬁcations are problematic for many modern malware examples that feature
multiple capabilities.
Figure 1 shows the distribution by year of the ﬁnal dataset of 151 samples.
Approximately 62 % of the samples (94) correspond to the period 1995–2005,
with the remaining equally distributed in the 2006–2015 (27) and 1985–1994
(28) periods, plus two samples from 1975 and 1982, respectively. The largest
category is Viruses (92 samples), followed by Worms (33 samples), Trojans
(11 samples), RATs (9 samples), MacroViruses (3 samples), and Botnets
(3 samples). A full listing of the 151 samples is provided in Table 3.
4 Analysis
This section describes our analysis over the malware source code dataset. It
ﬁrst details source code analytics (Sect. 4.1), then it estimates development cost
(Sect. 4.2), next it discusses complexity and maintainability metrics (Sect. 4.3),
and ﬁnally compares malware to benign code (Sect. 4.4).
4.1 Source Code Analytics
We next discuss various statistics obtained from the source code of the malware
samples in our dataset.
Number of Source Code Files. Figure 2a shows the distribution over time of
the number of ﬁles comprising the source code of the diﬀerent malware samples.
Except for a few exceptions, until the mid 1990s there is a prevalence of malicious
code consisting of just one ﬁle. Nearly all such samples are viruses written in
assembly that, as discussed later, rarely span more than 1,000 lines of code
(LOC). This follows a relatively common practice of the 1980s and 1990s when
writing short assembly programs.
From the late 1990s to date there is an exponential growth in the number
of ﬁles per malware sample. The code of viruses and worms developed in the
early 2000s is generally distributed across a reduced (<10) number of ﬁles, while
some Botnets and RATs from 2005 on comprise substantially more. For instance,
Back Oriﬁce 2000, GhostRAT, and Zeus, all from 2007, contain 206, 201, and
249 source code ﬁles, respectively. After 2010, no sample comprises a single ﬁle.
334
A. Calleja et al.
Fig. 2. Source code analytics of the malware samples in our dataset. (a) Number of
ﬁles. (b) SLOC. (c) Comment-to-code ratios. (d) FP counts. Note that in (a), (b)
and (d) the y-axis is shown in logarithmic scale.
Examples of this time period include KINS (2011), Rovnix (2014), and SpyNet
(2014), with 267, 276, and 324 ﬁles, respectively. This increase reveals a more
modular design, which also correlates with the use of higher-level programming
languages discussed later.
Simple least squares linear regression over the data points shown in Fig. 2a
yields a regression coeﬃcient (slope) of 1.17. (Note that the y-axis is in loga-
rithmic scale and, therefore, such linear regression actually corresponds to an
exponential ﬁt.) This means that the number of ﬁles has grown at an approxi-
mate yearly ratio of 17 %; or, equivalently, that it has doubled every 4.5 years.
Source Code Size. Figure 2d shows the distribution over time of the number
of physical source lines of code (SLOC) of all samples in the dataset. For this
we used cloc [1], an open-source tool that counts blank lines, comment lines,
A Look into 30 Years of Malware Development
335
and SLOC, and reports them broken down by programming language. The data
shown in Fig. 2d was obtained by simply aggregating the SLOC counts of all
source code ﬁles belonging to the same malware sample, irrespective of the pro-
gramming language in which they were written.
Again, the growth over the last 30 years is clearly exponential. Thus, up to the
mid 1990s viruses and early worms rarely exceeded 1,000 SLOC. Between 1997
and 2005 most samples contain several thousands SLOCs, with a few exceptions
above that ﬁgure, e.g., Troodon (14,729 SLOC) or Simile (10,917 SLOC). The
increase in SLOC count during this period correlates positively with the number
of source code ﬁles and the number of diﬀerent programming languages used.
Finally, a signiﬁcant number of samples from 2007 on exhibit SLOC counts in
the range of tens of thousands. For instance, GhostRAT (33,170), Zeus (61,752),
KINS (89,460), Pony2 (89,758), or SpyNet (179,682). Most of such samples cor-
respond to moderately complex malware projects whose output is more than
just one binary. Typical examples include Botnets or RATs featuring a web-
based C&C server, support libraries, and various types of bots/trojans. There
are exceptions, too. For instance, Point-of-Sale (POS) trojans such as Dexter
(2012) and Alina (2013) show relatively low SLOC counts (2,701 and 3,143,
respectively).
In this case the linear regression coeﬃcient over the data points is 1.16, i.e.,
the number of SLOCs per malware has increased approximately 16 % per year;
or, equivalently, the ﬁgure doubles every 4.7 years, resulting in an increase of
nearly an order of magnitude each decade.
Function Points Estimates. We used the SLOC-to-function-point ratios pro-
vided by PLT v8.2 (see Table 1) in an attempt to use a more normalized measure
of source code size for the malware samples in our dataset. To do that, we used
such ratios in reverse order, i.e., to estimate function-point counts from SLOCs
rather than the other way round. In doing so we pursue: (i) to better aggregate
the various source code ﬁles of the same malware that are written in diﬀerent
languages; and (ii) to provide a fairer comparison among the sizes of the samples.
As expected, there is a clear correlation between FP and SLOC counts and
the conclusions in terms of sustained growth are similar. Starting in 1990, there
is roughly an increase of one order of magnitude per decade. Thus, in the 1990s
most early viruses and worms contain just a few (<10) FPs. From 2000 to 2010
the FP count concentrates between 10 and 100, with Trojans, Botnets, and
RATs accounting for the higher counts. Since 2007 on, many samples exhibit FP
counts of 1,000 and higher; examples include Rovnix (2014), with FP = 1275.64,
KINS (2011), with FP = 1462.86, and SpyNet (2014), with FP = 2021.79. Linear
regression over the data points yields a coeﬃcient of 1.19, i.e., FP counts per
malware has suﬀered an approximate growth of 19 % per year; or, equivalently,
the ﬁgure doubles every 4 years.
Density of Comments. Figure 2c shows the comments-to-code ratios for the
malware samples in the dataset. This is simply computed as the number of
comment lines divided by the SLOC. There is no clear pattern in the data,
which exhibit an average of 18.83 %, a standard deviation of 23.44 %, and a
336
A. Calleja et al.
median value of 12.05 %. There are a few notable outliers, though. For example,
W2KInstaller (2000) and OmegaRAT (2014) show ratios of 99.6 % and 139.1 %,
respectively. Conversely, some samples have an unusually low ratio of comments.
We ignore if they were originally developed in this way or, perhaps, comments
were cut oﬀ before leaking/releasing the code.
Programming Languages. Figure 3a shows the distribution over time of
the number of diﬀerent languages used to develop each malware sample. This
includes not only compiled and interpreted languages such as assembly, C/C++,
Java, Pascal, PHP, Python, or Javascript, but also others used to construct
resources that are part of the ﬁnal software package (e.g., HTML, XML, CSS)
and scripts used to build it (e.g., BAT or make ﬁles).
Figure 3b shows the usage of diﬀerent programming languages to code mal-
ware over time extracted from our dataset. The pattern reveals the prevalent
use of assembly until the mid 2000s. From 2000 on C/C++ become increasingly
popular, as well as other “visual” development environments such as Visual
Basic and Delphi (Pascal). Botnets and RATs from 2005 on also make exten-
sive use of web interfaces and include numerous HTML/CSS elements, pieces of
Javascript, and also server-side functionality developed in PHP or Python. From
2012 to date the distribution of languages is approximately uniform, revealing
the heterogeneity of technologies used to develop modern malware.
Fig. 3. (a) Number of diﬀerent programming languages per malware sample in the
dataset. (b) Use of programming languages in malware samples. The chart shows the
number of samples using a particular language each year, with darker colors represent-
ing higher number of samples.
4.2 Cost Estimation
In this section we show the COCOMO estimates for the eﬀort, time, and team
size required to develop the malware samples in our dataset. One critical decision
A Look into 30 Years of Malware Development
337
Fig. 4. COCOMO cost estimators for the malware samples in the dataset. (a) Eﬀort
(man-months). (b) Development time (months). (c) Team size (number of people).
(d) Selected examples with eﬀort (E), development time (D), and number of people
(P). Note that in (a) and (b) the y-axis is shown in logarithmic scale.
here is selecting the type of software project (organic, semi-detached, or embed-
ded) for each sample. We decided to consider all samples as organic for two main
reasons. First, it is reasonable to assume that, with the exception of a few cases,
malware development has been led so far by small teams of experienced pro-
grammers. Additionally, we favor a conservative estimate of development eﬀorts
which is achieved using the lowest COCOMO coeﬃcients (i.e., those of organic
projects) and can thus be seen as a (estimated) lower bound of development
eﬀorts.
Figure 4a shows the COCOMO estimation of eﬀort required to develop the
malware samples. The evolution over time is clearly exponential, with values
roughly growing one order of magnitude each decade. While in the 1990s most
samples required approximately 1 man-month, this value rapidly escalates up
338
A. Calleja et al.
to 10–20 in the mid 2000s, and to 100s for a few samples of the last few years.
Linear regression conﬁrms this, yielding a regression coeﬃcient of 1.17; i.e., the
eﬀort growth ratio per year is approximately 17 %; or, equivalently, it doubles
every 4.5 years.
The estimated time to develop the malware samples (Fig. 4b) experiences a
linear increase up to 2010, rising from 2–3 months in the 1900s to 7–10 months
in the late 2000s. The linear regression coeﬃcient in this case is 0.395, which
translates into an additional month every 2.5 years. Note that a few samples
from the last 10 years report a considerable higher number of months, such as
Zeus (2007) or SpyNet (2014) with 20.15 and 30.86 months, respectively.
The amount of people required to develop each sample (Fig. 4c) grows simi-
larly. Most early viruses and worms require less than 1 person (full time). From
2000 on, the ﬁgure increases to 3–4 persons for some samples. Since 2010, a few
samples report person estimates substantially higher. For these data, the linear
regression coeﬃcient is 0.234, which roughly translates into an additional team
member every 4 years.
Finally, the table in Fig. 4d provides some numerical examples for a selected
subset of samples. For additional details, we refer the reader to the full datasets1
with the raw data used in this paper.
4.3 Complexity and Maintainability
In this section we show the complexity and maintainability metrics obtained for
the samples in our dataset. To compute McCabe’s cyclomatic complexity, we
used the Universal Code Count (UCC) [6], a tool that provides various software
metrics from source code. While many other tools exist for measuring cyclomatic
complexity (e.g., Jhawk [3], Radon [4], or the metrics plugin for Eclipse [2]), these
have a strong bias towards a particular language or a small subset of them. Con-
trarily, UCC works over C/C++, C#, Java, SQL, Ada, Perl, ASP.NET, JSP, CSS,
HTML, XML, JavaScript, VB, PHP, VBScript, Bash, C Shell Script, Fortran,
Pascal, Ruby, and Python. Since our dataset contains source code written in
diﬀerent languages, UCC best suits our analysis. Despite UCC’s wide support for
many languages, obtaining the cyclomatic measurements for each sample was
not possible. As suggested by Fig. 3b, a large fraction of our samples contain
a substantial amount of assembly code, which UCC does not support. Filtering
out samples that contain at least one source ﬁle in assembly left 44 samples for
analysis, i.e., approximately 33 % of the dataset.
Figure 5a shows the average cyclomatic complexity per function for each ana-
lyzed sample. Most of the samples have complexities between 2 and 5, with
values higher than that being very uncommon. Only DW (2002) exhibits an
average cyclomatic complexity of around 7. Two main conclusions can be drawn
from Fig. 5a (note that samples are temporarily ordered). First, even if there is
no clear evolution over time of the average complexity per function, there is a
slight decreasing trend. This might be a consequence of a more modular design,
1 Available at: http://www.seg.inf.uc3m.es/∼accortin/RAID 2016.html.
A Look into 30 Years of Malware Development
339
with functions and class methods being designed with less complex control ﬂow
logic structures. Second, a closer inspection at the full output of UCC reveals
that no function or method in the 44 samples exceeds McCabe’s recommended
complexity threshold of 10.
Fig. 5. (a) Average cyclomatic complexity per function and sample sorted by year.
(b) Maintainability index per sample sorted by year.
Using the metrics discussed throughout this section, we have also computed
an upper bound for the maintainability index M I provided by Eq. (5). Note
that we cannot estimate it exactly since we do not have the average Halstead’s
volume for each sample. Since this is a negative factor in Eq. (5), the actual
maintainability index would be lower than our estimates. Nevertheless, note that
such a factor contributes the lowest to M I, so we expect our ﬁgures to provide a
fair comparison among samples. Figure 5b shows the M I values for each sample
in the reduced dataset. As in the case of cyclomatic complexities, no clear trend
is observed. Values are generally high, with most samples having an M I higher
than 50. The most extreme values are those of Cairuh (M I = 30.05) and Hexbot2
(M I = 34.99) on one side of the spectrum, and Taichi (M I = 78.77), AndroRAT
340
A. Calleja et al.
(M I = 75.03), and Dendroid (74.47) on the other. All in all, these are reasonably
high values for M I.
4.4 Comparison with Regular Software
In order to better appreciate the signiﬁcance of the ﬁgures presented throughout
this section, we next discuss how they compare to those of a selected number of
open source projects whose source code is freely available. To do this we selected
9 software packages belonging to diﬀerent categories: 3 security products (the
IPTables ﬁrewall, the Snort IDS, and the ClamAV antivirus); a compiler (gcc);
a web server (Apache); a version control tool (Git); a numerical computation
suite (Octave); a graphic engine (Cocos2d-x); and a Unix shell (Bash). The code
was downloaded from the web page of each project. For each one of them we
then computed the metrics discussed above for malware samples. As in the case
of malware samples, we use the COCOMO coeﬃcients for organic projects. The
results are shown in Table 4 in increasing order of SLOC count.
Table 4. Software metrics for various open source projects. E: COCOMO eﬀort; D:
COCOMO development time; P: COCOMO team size; FP: function points; M: cyclo-
matic complexity; CR: comment-to-code ratio; MI: maintainability index.
Software Version Year SLOC
E
D