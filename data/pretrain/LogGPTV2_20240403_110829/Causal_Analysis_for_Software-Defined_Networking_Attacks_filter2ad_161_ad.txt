common ancestors is partitioned into agent, activity, and entity
nodes (lines 7–13). Thus, PICOSDN provides data-centric,
process-centric, and agent-centric answers.
Algorithm 3 shows the iterative backward-forward trac-
ing. Our practitioner has a piece of evidence and a suspected
root cause (derived, perhaps, from Algorithm 2). Our prac-
titioner’s goal is to iteratively determine how intermediate
causes (i.e., those causes that lie temporally in between the
evidence and the root cause) impact the evidence and other
effects on the network’s state. PICOSDN starts by discarding
non-causal edges in the graph (lines 1–3). For the piece of
evidence, PICOSDN determines all of its ancestors, or the
set of all causally related entities, activites, and agents re-
sponsible for the evidence (line 4). For the suspected root
cause, PICOSDN determines all of its descendants, or the set
of all the entities and activities that the root cause affected
(line 5). PICOSDN takes the intersection of those two sets
(line 6) to examine only the intermediate causes that occurred
as a result of the root cause. For each intermediate cause,
PICOSDN derives the set of affected entities and activities
that the root cause affected that the intermediate cause did
not affect (lines 7–8). In essence, that lets the practitioner
iteratively examine intermediate effects at each stage.
(cid:46) Remove and stash non-causal edges
if e is a wasRevisionOf edge then
Algorithm 5 Network Identiﬁer Evolution
Input: graph G, network identiﬁer i
Output: revision trace path tr, affected nodes function F
Initialize: (V ,E) ← G; Estash ← /0; F(i) ← /0,∀i ∈ V
1: for each e ∈ E do
2:
E ← E \{e}
3:
Estash ← Estash ∪{e}
4:
5: n ← getMostRecentNode(V ,i)
6: tr ← (cid:104)n(cid:105)
7: F(n) ← getDescendants((V ,E),n)
8: while n ← getNextNode(Estash) and n is not null do
9:
10:
11: return (tr,F)
tr.append(wasRevisionOf,n)
F(n) ← getDescendants((V ,E),n)
pret even for simple activities, and that creates fatigue when
one is analyzing such graphs for threats and attacks [20].
PICOSDN provides an efﬁcient network-speciﬁc summariza-
tion.
Algorithm 4 shows the summarization approach. Our prac-
titioner’s goal is to answer questions of the form “Which
data plane activities (i.e., packets) caused ﬂow rules to be or
not be installed?” PICOSDN starts by discarding non-causal
edges in the graph (lines 1–3). It collects each event listener or
packet processor activity (line 4). For each activity, it derives
all of the PacketIn packets that causally affected the activity
(lines 5–9). Then, PICOSDN determines whether a PacketIn
is a direct5 cause by computing a backward trace path; if it is
a direct cause, the packet is marked (lines 10–11). Similarly,
PICOSDN determines whether a FlowRule is a direct effect
of the activity; if it is, the ﬂow rule is marked (lines 12–13).
Algorithm 4 allows practitioners to efﬁciently investigate
instances in which ﬂow rules were not created, too. For ex-
ample, if an event listener used a packet but did not generate
a ﬂow rule, the resulting value for fout would be null. Algo-
rithm 4 also derives a set of all data plane PacketIn packets
causally related to each activity; as we show later in § 7, this
information is useful for diagnosing cross-plane attacks.
Network state evolution Given the attribution challenges
of data plane host activities, practitioners will want to in-
vestigate whether any of the pertinent identiﬁers have been
spoofed. Such spooﬁng can have systemic consequences on
subsequent control plane decisions [13,24,49,53]. PICOSDN
efﬁciently tracks network identiﬁer evolution (i.e., the was-
RevisionOf relation) and provides an algorithm to query it
(Algorithm 5).
Algorithm 5 shows the network identiﬁer evolution ap-
proach. Our practitioner’s goal is to see whether any identi-
ﬁers have evolved over time as a result of malicious spoof-
ing, as well as the extent of damage that such spooﬁng has
caused. PICOSDN starts by stashing non-causal edges in the
Network activity summarization One general provenance
challenge is that graphs can become large and difﬁcult to inter-
5In other words, without any intermediate Activity nodes in between. How-
ever, intermediate data derivations between Entity objects are permissible.
3192    30th USENIX Security Symposium
USENIX Association
Table 3: List of PICOSDN hooks (i.e., PICOSDN API calls).
PICOSDN API call
recordDispatch(activity)
recordListen(activity)
recordApiCall(type,entity)
recordDerivation(entity,entity)
Description
Mark the start of an event dispatch or
packet processing loop
Mark the demarcation (i.e., start of
each loop) of an event being listened
to or a packet being processed
Record a control plane API call of a
type (i.e., create, read, update, delete)
on an entity (or entities)
Record an object derived from an-
other object
graph, thus removing them from causality-related processing,
but keeping them for reference (lines 1–4). For a given net-
work identiﬁer, PICOSDN determines the node most recently
linked to that identiﬁer (line 5) and adds it to a revision trace
path (line 6). PICOSDN derives that node’s descendants to
determine the extent to which that network identiﬁer causally
affected other parts of the network state (line 7). That process
is repeated back to the identiﬁer’s ﬁrst version (lines 8–10).
Algorithm 5 produces a concise representation of an identi-
ﬁer’s state changes over time. That allows the practitioner to
easily determine when an identiﬁer may have been spoofed,
and that respective node in time can be used in Algorithm 3
as a root cause to perform further iterative root-cause analysis.
Furthermore, the affected nodes that are returned by Algo-
rithm 5 can be used as evidence in the common ancestry trace
of Algorithm 2.
6 Implementation
We implemented PICOSDN in Java on ONOS v1.14.0.
Our implementation is available at https://github.com/
bujcich/PicoSDN. We modiﬁed ONOS in several key loca-
tions. We created a set of PICOSDN API calls, which are
listed in Table 3. We created Java classes to represent Activity
and Entity objects, and we made them into superclasses for
relevant ONOS classes (e.g., ONOS’s Packet superclass is En-
tity). We wrapped the ONOS event dispatcher and packet pro-
cessor by using the recordDispatch() and recordListen() calls,
which represented the execution partitioning of PICOSDN.
We hooked the ONOS core services’6 public API calls by
using the recordApiCall() calls.7 For a given core service API
call, if the return value was iterable, we marked each object
within the iterable object with its own separate provenance
6In ONOS, these core services are represented by classes that end in
*Manager or *Provider. For instance, ONOS has a HostManager class and a
HostProvider class that include public API calls related to hosts.
7As ONOS does not provide a reference monitor architecture that would
allow us to wrap one central interposition point across all API calls, we had
to add recordApiCall() hooks across 141 API calls to ensure completeness.
record. For certain data whose processing spanned multi-
ple threads, we used recordDerivation() calls to maintain the
causal relations across threads. We implemented the ingester,
modiﬁer, and tracer on top of the JGraphT library.
Because of our design decisions, described in § 5.1, we did
not need to perform an analysis on or make any modiﬁcations
to the ONOS apps. Practitioners do not need to instrument
each new app that they install in their network. Furthermore,
PICOSDN’s API and classes allow PICOSDN to be easily
updated as new core services and objects are implemented
in ONOS. Although we implemented PICOSDN on ONOS,
the same conceptual provenance model and design can be
implemented with minimal modiﬁcations on any event-based
SDN controller architecture, and indeed the most popular con-
trollers (e.g., ODL and Floodlight) all use such architectures.
7 Evaluation
We now evaluate PICOSDN’s performance and analysis capa-
bilities. We have examined its performance overhead in terms
of latency and storage (§ 7.1). We used recent SDN attacks to
show that PICOSDN can capture and explain a broad diver-
sity of SDN attacks (§7.2). We implemented all topologies
using Mininet.8 We ran experiments using a workstation with
a four-core 3.30-GHz Intel Core i5-4590 processor and 16
GB of memory.
7.1 Performance Evaluation
Given the latency-critical nature of control plane decision-
making, we benchmarked the latency that PICOSDN imposed
on common ONOS API calls (Figure 8a). To further under-
stand these costs, we microbenchmarked PICOSDN’s hooks
(Figure 8b) and benchmarked the overall latency imposed by a
reactive control plane conﬁguration (Figure 8c) as a function
of the data plane’s network diameter. We also measured the
costs to store provenance graphs (Table 4).
Benchmarks on ONOS Figure 8a shows the average laten-
cies of common ONOS API calls with and without PICOSDN
enabled. These calls were called most often in our security
evaluation (§ 7.2) and relate to ﬂow rules, hosts, and packets.
Although certain calls generated signiﬁcantly greater latency,
that was expected for cases in which iterable objects require
generation of individual provenance records.
Microbenchmarks To further analyze the benchmark re-
sults, we microbenchmarked PICOSDN’s hooks (i.e., PI-
COSDN’s API calls). Figure 8b shows the average latencies of
8We chose Mininet because it is common in prior work (e.g., [52, 55])
and because it causes PICOSDN’s runtime phase to record the same kind and
amount of provenance information that would be captured in a real network.
Real networks may differ in terms of imposed latency.
USENIX Association
30th USENIX Security Symposium    3193
(a) Average latency per ONOS API call.
(b) Average latency per PICOSDN hook.
(c) Overall average latency per diameter.
Figure 8: PICOSDN latency performance results. (Error bars represent 95% conﬁdence intervals.)
the PICOSDN API calls listed in Table 3, with the recordApi-
Call() calls broken down by call type. As shown in Figure 8b,
event listening and dispatching are fast operations. We ex-
pected API calls to be slower, given the tracking operations
within PICOSDN’s internal state.
Overall latency We also measured the overall latency that
PICOSDN imposes on control plane operations. We wanted
to see what the additional incurred latency would be from
the perspective of host-to-host communication, or the time-
to-ﬁrst-byte metric. This metric measures the total round-trip
time (RTT) measured between data plane hosts (e.g., via the
ping utility) for the ﬁrst packet of a ﬂow. The RTT captures
the latency of both data plane processing and control plane
decision-making.
In reactive control planes, the ﬁrst packet of a ﬂow suffers
high latency because it does not match existing ﬂow rules, but
once matching ﬂow rules have been installed, the remaining
packets of the ﬂow use the data plane’s fast path. Although
SDN conﬁgurations can be proactive by installing ﬂow rules
before any packets match them, we measured a reactive con-
ﬁguration because it represents the worst-case latency that is
imposed if the controller must make a decision at the time it
sees the ﬁrst packet. (See § 8 for a discussion of the differ-
ences.) In addition, the network’s diameter (i.e., the number
of hops between data plane hosts) affects latency in reactive
conﬁgurations if the ﬁrst packet must be sent to the controller
at each hop. Thus, we measured a reactive conﬁguration and
varied the number of hops to determine the effect on latency.
Figure 8c shows the average overall latencies imposed with
and without PICOSDN on the ﬁrst packet, varied by the num-
ber of hops. We performed each experiment over 30 trials. In
contrast to prior work [52, 55], we parameterized the number
of hops traversed to reﬂect different network topology diame-
ters. We found that PICOSDN increased the overall latency on
average from 7.44 ms for 1-hop (i.e., same-switch) topologies
to 21.3 ms for 10-hop topologies. That increase was expected,
given that additional provenance must be generated for longer
routes. For long-running ﬂow rules, the one-time latency cost
in the ﬂow’s ﬁrst packet can be amortized. Thus, we ﬁnd
PICOSDN acceptable for practical implementation.
Storage costs
Internally, PICOSDN maintains only the min-
imum state necessary to keep track of object changes. Thus,
the state is as large as the number of objects representing
the network’s ﬂow rules, topology, and system principals
(e.g., switches and hosts) at a given time.
We investigated the external provenance graph storage costs
based on the network’s characteristics, and we summarize our
results in Table 4. Given the network diameter’s impact on
latency in reactive control planes, we focused the analysis on
the network diameter’s impact on storage costs. We set up a
bidirectional, reactive, end-to-end ﬂow between two hosts, and
we parameterized the number of hops between those hosts. We
deﬁned the storage cost as being all of the related provenance
needed to explain the origins of the connectivity between
those two hosts (e.g., ﬂows, packets, hosts, topologies, events,
apps, switch ports). We compared costs using the raw output
of the runtime phase (“before cleaning”) and the cleaned
graph used for investigation (“after cleaning”). Since such
storage reﬂects a single bidirectional ﬂow, we considered
the scalability of an enterprise-scale workload of 1,000 new
bidirectional ﬂows per second [55].
We found that the cleaned graph requires a signiﬁcantly
smaller amount of persistent storage space, with reductions of
95 to 98 percent. We optimized what provenance was kept by
removing orphan nodes, redundant edges, activities without
effects, and activities that did not impact ﬂows; these options
are conﬁgurable by practitioners. We found that the storage
costs increased as the number of hops increased. This was
expected, given that more objects (e.g., packets) are gener-
ated and used with longer routes. PICOSDN generates an
estimated 4 to 15 GB/h for an enterprise-scale network with
1,000 new bidirectional ﬂows per second. Further provenance
storage reduction can be implemented outside PICOSDN
using existing provenance storage reduction systems and tech-
niques [9, 21, 34].
We compare PICOSDN’s storage requirements with the
3194    30th USENIX Security Symposium
USENIX Association
F:apply()F:flowRemoved()F:getFlowEntries()F:removeFlowRules()H:getConnectedHosts()H:getHost()H:getHostsByIp()H:hostDetected()H:removeLocationFromHost()P:emit()ONOS API call (F = flow, H = host, P = packet)102101100Latency [ms]Without PicoSDNWith PicoSDNrecordApiCall(CREATE)recordApiCall(DELETE)recordApiCall(READ)recordApiCall(UPDATE)recordDerivation()recordDispatch()recordListen()PicoSDN API call103102101100Latency [ms]12510Number of Hops Traversed0102030405060RTT Latency [ms]Without PicoSDNWith PicoSDNTable 4: PICOSDN storage costs of a bidirectional ﬂow’s provenance.
Hops
Graph before cleaning
Graph after cleaning
Reduction in storage
# Nodes
# Edges
Data [KB]
# Nodes
# Edges
Data [KB]
Nodes
Edges
Data
Estimated storage
cost of 1,000 new
bidirectional ﬂows
per second [55]
1