ond, a function map : L → I. Finally, a function gen : I → L
such that ∀x ∈ L, gen(map(x)) = x. (It follows that map
is injective and gen is surjective). From these, they deﬁne
RankL : L → ZN by RankL(x) = rankI (map(x)), and they
deﬁne UnrankL : ZN → L by UnrankL(a) = gen(unrankI (a)).
To accommodate relaxed ranking, LDJRS designed an
FTE scheme that uses a technique known as cycle waling.
We give their cycle-walking FTE scheme in the rightmost
box of Figure 2, and use Img(L) to denote the image of
RankL for L ∈ {X, Y }. Encryption with cycle walking seeks
a point inImg (Y ) ⊆ ZN which will map to a valid ciphertext
under the relaxed ranking. Note that the scheme as written
assumes plaintext format X and ciphertext format Y are
such that |X| = |Y |, but LDJRS give variants that work for
more general pairs of formats. We refer the reader to [14]
for the details.
LDJRS give a relaxed ranking scheme for NFAs, which en-
ables eﬃcient regex-speciﬁed FTE by ﬁrst converting a regex
to an NFA. They show that NFA-based relaxed ranking of-
ten leads to schemes almost as fast as DFA-based ranking
schemes without the blow-up in automaton size.
CFG-speciﬁed FTE. While regex-speciﬁed FTE may be
suﬃcient in some contexts, there are settings in which we
would prefer to use context-free grammars (CFGs). CFGs
allow expression of richer languages, and are often used in
practice to describe the structure of ﬁles, web pages, protocol
messages, and more. However, none of the BRRS, DCRS or
LDJRS schemes provide CFG-speciﬁed FTE.
GS sketched a (high-degree) polynomial-time ranking for
context-free languages. While theoretically interesting, their
ranking only works with unambiguous CFGs and has no
matching unranking algorithm. Recall that an ambiguous
CFG is one which has multiple derivations for some string
in the associated language; moreover, determining whether
or not a grammar is ambiguous is undecidable. Thus, the
1294Rank L
L
Img(L)
map
rankI
L
map(L)
Z
N
L
Unrank
ZN
RankL : L → ZN injective
UnrankL : ZN → L surjective
∀x : UnrankL(RankL(x)) = x
I
unrank
I
|I| = N
gen
map : L → I injective
gen : I → L surjective
∀x : gen(map(x)) = x
EK (M ) :
If M /∈ X then
Return ⊥
r ← RankX (M )
Do
r ← EK (r)
Until r ∈ Img(Y )
Return UnrankY (r)
DK (C) :
If C /∈ Y then
Ret ⊥
r ← RankY (C)
Do
r ← DK (r)
Until r ∈ Img(X)
Return UnrankX (r)
Figure 2: (Left) Diagram of relaxed ranking. (Middle) Using strict ranking on an intermediate set I to obtain relaxed ranking
on L. (Right) The cycle-walking construction of FTE using relaxed ranking from [14] for formats X, Y with |X| = |Y |.
GS ranking based on CFGs is not well suited to practice.
M¨akinen [15] later presented an algorithm for ranking and
unranking of Slizard languages (not general CFL). How-
ever, the presentation leaves room for interpretation and it
appears to contain ﬂaws in some parts of the algorithm.
In summary, there currently exists no eﬃcient mechanism
for ranking (relaxed or otherwise) given a CFG representa-
tion of a CFL. However, the diﬃculty in handling CFLs goes
beyond the lack of eﬃcient CFG-based ranking methods.
Using a monolithic or “pure” CFG for the language would
be impractical, even if eﬃcient CFG-based ranking existed.
This is because in practice people do not use “pure” CFGs to
describe a language. In practice, a language is described at
two levels: a lexical level that deﬁnes parsing tokens using
regexes, and a second level that deﬁnes the syntax using a
pure CFG that has the tokens as terminals. Although every
such two-level grammar can be converted to a pure CFG, by
absorbing the tokens in the CFG description, this comes at
the cost of programming eﬀort, ﬂexibility, explosion of the
number of symbols, and runtime performance.
Another diﬃculty is that a CFG speciﬁcation is often more
complex than a regex speciﬁcation, and we need a tool to
relieve the programmer from the tedious and error prone
process of CFG speciﬁcation.
Finally, some formats used in practice are not context free.
For instance, counter based formats of the form:
 :=  BYTE* 
We are not aware of any ranking method that can handle
such formats in a principled way.
In the rest of this paper we show how we address these
limitations. First we provide an eﬃcient CFG-based relaxed
ranking. Then we show how we adapt it to handle two-level
lexer/parser language speciﬁcations, including ones that in-
clude counters and similar non-context-free embellishments.
3. RELAXED RANKING FOR CFLS
We present an algorithm for relaxed ranking of a context-
free language. It will use a two-stage approach, with inter-
mediate objects being parse trees. First we present some
background on CFGs.
3.1 Background and Deﬁnitions
A context-free grammar (CFG) is a 4-tuple G = (N , Σ,
R, S), where N is a ﬁnite set of non-terminals; Σ is a ﬁnite
set of terminals, such that N ∩ Σ =∅ ; R is a ﬁnite relation
R ⊆ N × (N ∪ Σ)
If
(A, w) ∈ R, we write A → w, and we say that w is derived
from A in one step. We extend → to a derivation relation
; and S ∈ N is the start symbol.
∗
∗
∗
w1Aw2.
to words in (N ∪ Σ)
from words in (N ∪ Σ)
as follows.
We say that s2 is derivable from s1 in one step, and we
write s1 → s2 iﬀ ∃X ∈ N ∃w1, w2, w3 ∈ (N ∪ Σ)
∗
: s1 =
w1Xw3 ∧ X → w2 ∧ s2 = w1w2w3. In the later case, we say
that s2 = w1w2w3 can be derived from s1 = w1Xw3 using
the rule ρ = X → w2, and we write s1 →ρ
s2; if w1 ∈ Σ
∗
we say that w1Xw2 → w1w2w3 is the leftmost derivation
(by expanding the leftmost non-terminal). The transitive
closure of → is →∗
. The language accepted by G is the set
: S →∗
w}. A language is a context-free
L(G) = {w ∈ Σ
∗
language (CFL) if it is accepted by a CFG. A grammarG
is unambiguous iﬀ for all w ∈ L(G) there is exactly one
sequence of leftmost derivations ρ1, ρ2, ..., ρk that lead from
S to w, i.e., S →ρ1 s1 →ρ2 s2 →ρ3 ...sk−1 →ρk sk = w.
A symbolX in a CFG is called useless if it does not occur
in any derivation of a word from L(G). A symbol A ∈ N
is recursive if there is a non empty chain of rules such that
A →∗
For every non-terminal A ∈ N , we as-
Rule ordering.
sume that the ordered set (enumeration) of rules that have
A on the left hand side is RA = {ρ1, ..., ρ|RA||ρi : A → wi ∈
R}. This enumeration deﬁnes an arbitrary but ﬁxed order
ρj ⇐⇒ k < j (i.e. ρk pre-
on A’s rules, where ρk <
cedes ρj in the ordering RA). The k
rule in RA is denoted
by RA[k]. This assumption will be used in the upcoming
relaxed ranking algorithm.
Labeled trees and parse trees.
Informally, a parse tree
(or a derivation tree) for a CFG grammar G = (N , Σ,R, S)
is a tree in which every node is labeled with a rule ρ ∈ R.
The rule for a node determines how that node is “expanded”
by G, i.e. the number of the node’s children and their labels,
which must correspond to the non-terminals on the right side
of RA[k].
To make things precise, we deﬁne the following. A labeled
tree T =(V, E, r0, λ) over a set Λ is a directed tree where each
vertex has a label in Λ. Here (V, E, r0) is a tree where V is
the set of vertexes, E ⊆ V × V is the set of directed edges,
r0 ∈ V is the root; Λ is the set of labels; and λ : V → Λ
associates a label to each vertex. For a vertex v ∈ V , v[k]
child of v. The tree is ﬁnite, unless otherwise
is the k
speciﬁed. A parse-tree for a CFG grammar G = (N , Σ,R, S)
is a labeled tree T =(V, E, r0, λ) overR , where:
R
th
th
1. λ : V → R labels each node v with a rule. If λ(v) = ρ :
A → ... ∈ R, we say that v’s type is A and v’s rule is
ρ. For convenience, we use the notation v.s to denote
A, andv.ρ to denote ρ.
12952. For every node v,
if v.ρ is A → s0A1s1A2...Ansn,
, and Ai ∈ N , then v must
where n ≥ 0, si ∈ Σ
have n children v1, ..., vn, and each child vi must have
vi.s = Ai.
∗
Deﬁne P T( G) to be the set of all parse trees generated by
a grammarG , and P T( G, A) ⊆ P T( G) to be the subset of
parse trees with type of the root as A.
∗
If v1, ..., vn are v’s children, n ≥ 0, si ∈ Σ
∗
The yield of a parse tree.
Next, we deﬁne a func-
tion that takes a parse tree as input, and outputs the string
which is the yield for the tree. Let T be a parse tree with
root r0, and let v be a parse-tree node. Let W(v) ∈ Σ
∗
be the string recognized by v, deﬁned inductively as fol-
, and
lows.
Ai ∈ N , and if v.ρ is A → s0A1s1A2...Ansn, then W(v) =
s0W(v1)s1W(v2)...W(vn)sn. Overloading notation, deﬁne
W(T ) =W (r0), i.e. the string recognized by T (the yield
of T ). Deﬁne (cid:17)T(cid:17) to be |W(T )|, the length of the yield
W(T ). Thus, we can deﬁne a function W : P T( G) → Σ
∗
.
Now, for every A ∈ N , let WA : P T( G, A) → Σ
be de-
ﬁned by WA(T ) =W (T ). Note that when A = S, we have
WS : P T( G, S) → L(G).
Minimal parse trees. We will propose a ranking scheme
that counts the number of parse-trees that yield strings of
a given size. To avoid the problem that there may be an
inﬁnite number of such trees, we introduce the concept of
minimal parse-trees, and we will count only those.
Let G = (N , Σ,R, S) be a CFG. We say that T1 ∈
P T( G, A) is reducible iﬀ T1 has a subtree T2 ∈ P T( G, A)
with (cid:17)T1(cid:17) = (cid:17)T2(cid:17) (equivalently, with W(T1) =W (T2)).
Furthermore, we say that T is minimal iﬀ T has no reducible
subtrees. We deﬁne M P T (G) to be the set of minimal
parse trees generated by a grammar G, and M P T (G, A) ⊆
P T( G, A) is the set of minimal parse trees with root A. By
extension, we deﬁne the set of such parse trees with yield-
length k to be M P T (G, A, k) = {Y ∈M P T (G, A) : (cid:17)Y (cid:17)=k}.
We these deﬁnitions in place, we state a simple lemma,
whose proof we defer to the full version of this paper. In
Section 3.2, this lemma will provide assurance that we are
counting ﬁnite sets.
Lemma 1. For all k ≥ 0, M P T (G, A, k) is ﬁnite.
3.2 Relaxed ranking
Having built up this machinery, we proceed to describe
our two-step relaxed ranking approach. Let G be a CFG
corresponding to a CFL L0. We pick a slice size z ∈ N and
(z)
describe the relaxed ranking of the language slice L = L
0 .
We deﬁne the intermediate set I = M P T (G, S, z), the set of
minimal parse trees derived from S, the start symbol for G,
which yield a string of length z. Given this I and L, it
remains to deﬁne the functions gen and map. For simplicity,
in the rest of this section, when we use the term parse tree,
we mean minimal parse tree, unless otherwise speciﬁed.
First, let gen = WS, i.e. for all T ∈ I = M P T (G, S, z),
gen(T ) = WS(T ). To deﬁne map, we assume that we have a
deterministic parser which produces minimal parse trees (we
will address this assumption later). Then, for all w ∈ L, we
deﬁne map(w) to be a deterministically picked parse tree for
w in M P T (G, S). In other words map is the deterministic
parser. So, to ﬁnish our two-step relaxed ranking, it remains
to deﬁne the strict ranking and unranking algorithms, rankI
and unrankI , from parse trees to integers.
Just before doing exactly that, we note that ranking a
word or string w ∈ L clearly requires parsing to generate
a tree in M P T (G, S). The common parsing methods for
3|G|) time in the worst
an arbitrary grammar can take O(l
case, where l is the length of the word being parsed. (Ear-
ley’s method is said to perform much better in practice.)
The worst case cubic time of parsing a general CFG cannot
be improved much, and [12] shows why this is the case by
providing an eﬃcient reduction of Boolean Matrix Multipli-
cation to CFG parsing.
Strict ranking/unranking of minimal parse trees. We
show how to perform ranking by comparing two parse trees
whose roots have the same type. While it is possible to de-
ﬁne a ordering on all parse trees, this is not necessary for
our purpose. Our ﬁnal goal is ordering of strings in L(G),
hence strings derived from S, whose parse trees have some
rule of the form S → w at the root. For this, it suﬃces to
deﬁne an ordering only among trees of the same type, but
we must take a few precautions to prevent the case where a
string may have an inﬁnite number of parse-trees.
Assume that ≺A is a total order on M P T (G, A) for non-
terminal A (≺A is induced by the ordering on the rules
and described in detail later in the section). For all X ∈
M P T (G, A), let rankA(X) be deﬁned as:
rankA(X)=|{Y∈M P T (G, A,(cid:17)X(cid:17)) : Y≺AX}|
(1)
The deﬁnition is well founded because for any G the set
M P T (G, A,(cid:17)X(cid:17)) is ﬁnite (Lemma 1); which is why we re-
quire parsers that produce minimal parse-trees. This is a re-
alistic requirement satisﬁed by all eﬃcient parsing methods
that we know. For a large class of grammars, such as all un-
ambiguous grammars, or grammars without -productions,
all parse trees are minimal, i.e. M P T = P T. For other
grammars we can easily remove -productions so that they
have only minimal parse trees, but we do not require such
a conversion as long as there is a parser that can produce
minimal parse trees.
Observe that rankA(X) is the position of X among the
trees of same yield length in M P T (G, A,(cid:17)X(cid:17)), rather than
all trees in M P T (G, A). This is convenient for language
slices of the form L(G)
(n)
.
In the following we show how to obtain a total order for
the parse-trees of a grammar. The ordering is based on the
length of the yield of the trees, on an ordering of the gram-
mar rules, and on the structure of the trees. In particular:
1. Trees with shorter yields precede trees with longer
yields.
2. For the same length, the trees derived from an earlier
grammar rule precede the trees derived from a later
rule.
3. At the same yield length and grammar rule, the trees
are compared based on their ordered children, in a
manner similar to lexicographical ordering: we ﬁrst
compare the children on the ﬁrst position, and if equal
then we compare the children on the second position,
etc.
For simplicity of presentation, but without lack of gener-
ality, it is useful to assume that the grammar has a simpler
form, which we call a weak normal form (WNF). Formally,
a grammarG = (N , Σ,R, S) is in weak normal form (WNF)
if G has no useless symbols and G’s rules can only have one
of the following forms:
1. A → A1A2, with A, A1, A2 ∈ N
12962
3