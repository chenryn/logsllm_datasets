看，一个有偏差的但有效的选择是使用采样的数据。我们将在下一个小节中深入讨论此问题。
2.5 蒙特卡罗
和动态规划不同的是，蒙特卡罗 (MonteCarlo, MC) 方法不需要知道环境的所有信息。蒙特
卡罗方法只需基于过去的经验就可以学习。它也是一种基于样本的（Sampling-Based）方法。蒙
特卡罗可以在对环境只有很少的先验知识时从经验中学习来取得很好的效果。“蒙特卡罗”可以
用来泛指那些有很大随机性的算法。
当我们在强化学习中使用蒙特卡罗方法的时候，需要对来自不同片段中的每个状态-动作对
（State-Action Pair）相应的奖励值取平均。一个例子是，在本章之前内容中介绍的上下文赌博机
（Contextual Bandit）问题中，如果在不同的机器上有一个 LED 灯，那么玩家就可以逐渐地学习
LED 灯状态信息和回报之间的联系。我们在这里把一种灯的排列组合作为一种状态，那么其可
能的奖励值就作为这个状态的价值。最开始，我们可能无法对状态价值有一个很好的预估，但是
当我们做出更多的尝试以后，平均状态价值会向它们的真实值靠近。在这个章节，我们会探索我
们怎么更合理地做出估算。假设问题是回合制的（Episodic），因而不论一个玩家做出了哪些的动
作，一个回合最后都会终止。
2.5.1 蒙特卡罗预测
首先，我们一起来看给定一个策略π如何用蒙特卡罗方法来评估状态价值函数。直观上的一
种方式是，通过对具体策略产生的回报取平均值来从经验中评估状态价值函数。更具体地，让函
数v (s)作为在策略π下的状态价值函数。我们接着收集一组经过状态s的回合，并把每一次状
π
态s在一个回合里的出现叫作一次对状态s的访问。这样一来，我们就有两种估算方式：首次蒙
特卡罗（First-VisitMonteCarlo）和每次蒙特卡罗（Every-VisitMonteCarlo）。首次蒙特卡罗只考
虑每一个回合中第一次到状态s的访问，而每次蒙特卡罗就是考虑每次到状态s的访问。这两种
方式有很多的相似点，但是也有一些理论上的不同。在算法2.8，我们展示了如何用首次蒙特卡罗
来对v (s)估算。把首次蒙特卡罗变成每次蒙特卡罗在操作上很简单，我们只需要把对首次访问
π
检查条件去掉即可。假如我们对状态s有无限次访问的话，那最终这两种方式都会收敛到v (s)。
π
蒙特卡罗方法可以独立地对不同的状态值进行估算。和动态规划不同的是，蒙特卡罗不使用
自举（Bootstrapping），也就是说，它不用其他状态的估算来估算当前的状态值。这个独特的性质
可以让我们直接通过采样的回报来对状态值进行估算，从而有更小的偏差但会有更大的方差。
当我们有了环境的模型以后，状态价值函数就会很有用处了，因为我们就可以通过比较对一
个状态的不同动作的价值平均值来选择在任意状态下的最好动作，就和在动态规划里一样。当模
型未知时，我们需要把状态-动作价值估算出来。每一个状态-动作值需要被分别估计。现在，我
们的学习目标就变成了q (s,a)，即在状态s下根据策略π 采取动作a时的预期回报。这在本质
π
上与对状态价值函数的估计基本一致，而我们现在只是取状态s在动作a上的平均值而已。不过
70
2.5 蒙特卡罗
有时，可能会有一些状态从来都没有被访问过，所以就没有回报。为了选择最优的策略，我们必
须要探索所有的状态。一个简单的方法是直接选择那些没有可能被选择的状态-动作对来作为初
始状态。这样一来，就可以保证在足够的回合数过后，所有的状态-动作对都是可以被访问的。我
们把这样的一个假设叫作叫作探索开始（ExploringStarts）。
算法2.8首次蒙特卡罗预测
输入：初始化策略π
初始化所有状态的V(s)
初始化一列回报：Returns(s)对所有状态
repeat
通过π: S 0,A 0,R 0,S 1,··· ,S T−1,A T−1,R t生成一个回合
G←0
t←T −1
fort>=0do
G←γG+R
t+1
if S 0,S 1,··· ,S t−1没有S tthen
Returns(S ).append(G)
t
V(S )←mean(Returns(S ))
t t
endif
t←t−1
endfor
until收敛
2.5.2 蒙特卡罗控制
现在我们可以把泛化策略迭代运用到蒙特卡罗中去，来看看它是怎么用来控制的。泛化策略
迭代有两个部分：策略评估（PolicyEvaluation）和策略提升（PolicyImprovement）。策略评估的
过程与之前小节中介绍的动态规划是一样的，所以我们主要来介绍策略提升。我们会对状态-动
作值使用贪心策略，在这种情况下不需要使用环境模型。贪心策略会一直选择在一个状态下有最
高价值的动作：
π(s)=argmaxq(s,a) (2.52)
a
对于每一次策略提升，我们都需要根据q 来构造π 。这里展示策略提升是怎么实现的：
πt t+1
q (s,π (s))=q (s,argmaxq (s,a))
πt t+1 πt πt
a
=maxq (s,a)
πt
a
⩾q (s,π (s))
πt t
71
第2章 强化学习入门
⩾v (s) (2.53)
πt
上面的式子证明了π 不会比π 差，而我们会在迭代策略提升后最终找到最优策略。这也
t+1 t
意味着，我们可以对环境没有太多了解而只有采样得到的回合才使用蒙特卡罗。这里我们需要解
决两个假设。第一个假设是探索开始，第二个是假设有无穷多个回合。我们先跳过第一个假设，
从第二个假设开始。简化这个假设的一种简单方法是，通过直接在单个状态的评估和改进之间交
替变更，来避免策略评估所需的无限多的片段（Episodes）。
2.5.3 增量蒙特卡罗
从算法2.8和算法2.9中可以看出，我们需要对观察到的回报序列求平均值，并且将状态价值
和状态-动作价值的估计分开。其实我们还有一种更加高效的计算办法，它能让我们把回报序列省
去，从而简化均值计算步骤。这样一来，我们就需要一个回合一个回合地更新。我们让Q(S ,A )
t t
作为它已经被选中t−1次以后的状态-动作价值的估计，从而将其改写为
G 1+G 2+···+G t−1
Q(S ,A )= (2.54)
t t t−1
算法2.9蒙特卡罗探索开始
初始化所有状态的π(s)
对于所有的状态-动作对，初始化Q(s,a)和Returns(s,a)
repeat
随机选择S 和A ，直到所有状态-动作对的概率为非零
0 0
根据π: S 0,A 0,R 0,S 1,··· ,S T−1,A T−1,R t来生成S 0,A
0
G←0
t←T −1
fort>=0do
G←γG+R
t+1
if S 0,A 0,S 1,A 1··· ,S t−1,A t−1没有S t,A tthen
Returns(S ,A ).append(G)
t t
Q(S ,A )←mean(Returns(S ,A ))
t t t t
π(S )←argmax Q(S ,a)
t a t
endif
t←t−1
endfor
until收敛
对该式的一个简单实现是将所有的回报 G 值都记录下来，然后将它的和值除以它的访问次
数。然而，我们同样也可以通过以下的公式来计算这个值：
72
2.6 时间差分学习
Xt
1
Q = G
t+1 t i
0i=1 1
Xt−1
1@ A
= G + G
t t i
i=1
0 1
Xt−1
= 1@ G +(t−1) 1 G A
t t t−1 i
i=1
1
= (G +(t−1)Q )
t t t
1
=Q + (G −Q ) (2.55)
t t t t
这个形式可以让我们在计算回报的时候更加容易操作。它的通用形式是：
新估计值←旧估计值+步伐大小·(目标值−旧估计值) (2.56)
“步伐大小”是我们用来控制更新速度的一个参数。
2.6 时间差分学习
时间差分（TemporalDifference，TD）是强化学习中的另一个核心方法，它结合了动态规划和
蒙特卡罗方法的思想。与动态规划相似，时间差分在估算的过程中使用了自举（Bootstrapping），
但是和蒙特卡罗一样，它不需要在学习过程中了解环境的全部信息。在这章中，我们首先介绍如
何将时间差分用于策略评估，然后详细阐释时间差分、蒙特卡罗和动态规划方法的异同点。最后，
我们会介绍Sarsa和Q-Learning算法，这是一个在经典强化学习中很有用的算法。
2.6.1 时间差分预测
从这个方法的名字可以看出，时间差分利用差异值进行学习，即目标值和估计值在不同时间
步上的差异。它使用自举法的原因是它需要从观察到的回报和对下个状态的估值中来构造它的目
标。具体来说，最基本的时间差分使用以下的更新方式：
V(S )←V(S )+α[R +γV(S )−V(S )] (2.57)
t t t+1 t+1 t
这个方法也被叫作TD(0),或者是单步TD。也可以通过将目标值改为在N 步未来中的折扣
回报和 N 步过后的估计状态价值（Estimated State Value）来实现 N 步 TD。如果我们观察得足
够仔细，蒙特卡罗在更新时的目标值为G ，这个值只有在一个回合过后才能得知。但是对于TD
t
73
第2章 强化学习入门
来说，这个目标值是R +γV(S )，而它可以在每一步都算出。在算法2.10中，我们展示了
t+1 t+1
TD(0)是如何用来做策略评估的。
算法2.10TD(0)对状态值的估算
输入策略π
初始化V(s)和步长α∈(0,1]
for每一个回合do
初始化S
0
for每一个在现有回合的S do
t
A ←π(S )
t t
R ,S ←Env(S ,A )
t+1 t+1 t t