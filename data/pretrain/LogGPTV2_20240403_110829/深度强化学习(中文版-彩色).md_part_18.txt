### 2.5 蒙特卡罗方法

与动态规划不同，蒙特卡罗（Monte Carlo, MC）方法不需要掌握环境的全部信息。MC方法基于过去的经验进行学习，是一种基于采样的方法。即使对环境仅有少量先验知识，MC方法也能通过经验学习达到良好效果。“蒙特卡罗”通常用来指代具有高随机性的算法。

在强化学习中使用蒙特卡罗方法时，需要计算来自不同片段中的每个状态-动作对（State-Action Pair）相应的奖励值的平均值。例如，在上下文赌博机问题中，如果不同的机器上有LED灯，玩家可以逐步学习LED灯状态与回报之间的关系。我们将一组LED灯的状态视为一个状态，并将其可能的奖励值作为该状态的价值。最初，我们可能无法准确估计状态价值，但随着更多尝试，平均状态价值会逐渐接近其真实值。本章将探讨如何更合理地进行估算。假设问题是回合制的（Episodic），即无论玩家采取何种行动，一个回合最终都会终止。

#### 2.5.1 蒙特卡罗预测

首先，我们讨论如何使用蒙特卡罗方法评估给定策略π下的状态价值函数v(s)。直观的方法是通过对具体策略产生的回报取平均值来从经验中评估状态价值函数。更具体地说，我们收集一系列经过状态s的回合，并将每次状态s在一个回合中的出现称为一次访问。有两种估算方式：首次蒙特卡罗（First-Visit Monte Carlo）和每次蒙特卡罗（Every-Visit Monte Carlo）。首次蒙特卡罗只考虑每个回合中第一次到达状态s的访问，而每次蒙特卡罗则考虑每次到达状态s的访问。这两种方法有很多相似之处，但也存在一些理论上的差异。在算法2.8中，我们展示了如何使用首次蒙特卡罗来估算v(s)。将首次蒙特卡罗转换为每次蒙特卡罗只需去掉首次访问检查条件即可。如果对状态s有无限次访问，两种方法最终都会收敛到v(s)。

蒙特卡罗方法可以独立地估算不同状态的值，且不使用自举（Bootstrapping），即不依赖其他状态的估计来估算当前状态值。这使得我们可以直接通过采样的回报来估算状态值，从而减少偏差但增加方差。

当拥有环境模型时，状态价值函数非常有用，因为它允许我们通过比较不同动作在某个状态下的预期回报来选择最佳动作。当模型未知时，我们需要估计状态-动作价值。每个状态-动作值都需要单独估计。现在，我们的目标是估计q(s,a)，即在状态s下根据策略π采取动作a时的预期回报。本质上，这与状态价值函数的估计类似，但我们只是对状态s在动作a上的回报取平均值。有时，某些状态可能从未被访问过，因此没有回报。为了选择最优策略，我们必须探索所有状态。一种简单的方法是选择那些尚未被选中的状态-动作对作为初始状态，以确保在足够多的回合后，所有状态-动作对都能被访问。这种假设被称为“探索开始”（Exploring Starts）。

**算法2.8 首次蒙特卡罗预测**

输入：
- 初始化策略π
- 初始化所有状态的V(s)
- 初始化回报列表Returns(s)对所有状态

重复：
- 通过π生成一个回合：S0, A0, R0, S1, ..., ST-1, AT-1, RT
- G ← 0
- t ← T - 1
- 对于t ≥ 0:
  - G ← γG + Rt+1
  - 如果S0, S1, ..., St-1中没有St:
    - Returns(St).append(G)
    - V(St) ← mean(Returns(St))
- 直至收敛

#### 2.5.2 蒙特卡罗控制

现在我们可以将泛化策略迭代应用于蒙特卡罗方法，看看它是如何用于控制的。泛化策略迭代包括两个部分：策略评估（Policy Evaluation）和策略提升（Policy Improvement）。策略评估过程与之前介绍的动态规划相同，因此我们主要讨论策略提升。我们会使用贪心策略来选择状态-动作值最高的动作：

\[ \pi(s) = \arg\max_a q(s, a) \]

对于每次策略提升，我们需要根据q构造新的策略π。以下是策略提升的具体实现：

\[ q_{\pi_t}(s, \pi_{t+1}(s)) = q_{\pi_t}(s, \arg\max_a q_{\pi_t}(s, a)) \]
\[ = \max_a q_{\pi_t}(s, a) \]
\[ \geq q_{\pi_t}(s, \pi_t(s)) \]
\[ \geq v_{\pi_t}(s) \]

上述公式证明了新策略πt+1不会比旧策略πt差，并且在迭代策略提升后最终能找到最优策略。这意味着我们可以仅依靠采样得到的回合来使用蒙特卡罗方法，而无需详细了解环境。这里有两个假设：探索开始和无穷多个回合。我们先跳过第一个假设，从第二个假设开始。简化这个假设的一种方法是在单个状态的评估和改进之间交替变更，从而避免策略评估所需的无限多片段。

#### 2.5.3 增量蒙特卡罗

从算法2.8和算法2.9可以看出，我们需要对观察到的回报序列求平均值，并将状态价值和状态-动作价值的估计分开。实际上，还有一种更高效的计算方法，可以省去回报序列，从而简化均值计算步骤。我们让Q(St, At)表示状态-动作价值的估计，并将其改写为：

\[ Q(S_t, A_t) = \frac{G_1 + G_2 + \cdots + G_{t-1}}{t-1} \]

**算法2.9 蒙特卡罗探索开始**

初始化：
- 所有状态的π(s)
- 所有状态-动作对的Q(s, a)和Returns(s, a)

重复：
- 随机选择S0和A0，直到所有状态-动作对的概率为非零
- 根据π生成S0, A0, R0, S1, ..., ST-1, AT-1, RT
- G ← 0
- t ← T - 1
- 对于t ≥ 0:
  - G ← γG + Rt+1
  - 如果S0, A0, S1, A1, ..., St-1, At-1中没有St, At:
    - Returns(St, At).append(G)
    - Q(St, At) ← mean(Returns(St, At))
    - π(St) ← argmaxa Q(St, a)
- 直至收敛

简单的实现方法是记录所有的回报G值，然后将其总和除以访问次数。然而，也可以通过以下公式来计算：

\[ Q_{t+1} = \frac{1}{t} \sum_{i=1}^t G_i \]
\[ = \frac{1}{t} (G_t + \sum_{i=1}^{t-1} G_i) \]
\[ = \frac{1}{t} (G_t + (t-1) \frac{1}{t-1} \sum_{i=1}^{t-1} G_i) \]
\[ = \frac{1}{t} (G_t + (t-1) Q_t) \]
\[ = Q_t + \frac{1}{t} (G_t - Q_t) \]

这种形式使我们在计算回报时更加容易操作。通用形式是：

\[ 新估计值 \leftarrow 旧估计值 + 步伐大小 \times (目标值 - 旧估计值) \]

其中，“步伐大小”是我们用来控制更新速度的一个参数。

### 2.6 时间差分学习

时间差分（Temporal Difference, TD）是强化学习中的另一个核心方法，它结合了动态规划和蒙特卡罗方法的思想。类似于动态规划，TD在估算过程中使用自举（Bootstrapping），但像蒙特卡罗一样，它不需要在学习过程中了解环境的全部信息。在本章中，我们首先介绍如何将时间差分用于策略评估，然后详细阐释时间差分、蒙特卡罗和动态规划方法的异同点。最后，我们会介绍Sarsa和Q-Learning算法，这是经典强化学习中非常有用的算法。

#### 2.6.1 时间差分预测

顾名思义，时间差分利用目标值和估计值在不同时间步上的差异进行学习。它使用自举法的原因是需要从观察到的回报和对下一个状态的估值中构造目标。具体来说，最基本的时间差分使用以下更新方式：

\[ V(S_t) \leftarrow V(S_t) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)] \]

这种方法也称为TD(0)或单步TD。可以通过将目标值改为N步未来中的折扣回报和N步后的估计状态价值来实现N步TD。蒙特卡罗方法在更新时的目标值为Gt，只有在一个回合结束后才能得知。但对于TD来说，目标值为Rt+1 + γV(St+1)，可以在每一步都计算出来。

**算法2.10 TD(0)对状态值的估算**

输入：
- 策略π
- 初始化V(s)和步长α ∈ (0, 1]

对于每一个回合：
- 初始化S0
- 对于现有回合中的每一个St:
  - At ← π(St)
  - Rt+1, St+1 ← Env(St, At)
  - V(St) ← V(St) + α [Rt+1 + γ V(St+1) - V(St)]

希望这些优化后的文本能帮助你更好地理解蒙特卡罗方法和时间差分学习。