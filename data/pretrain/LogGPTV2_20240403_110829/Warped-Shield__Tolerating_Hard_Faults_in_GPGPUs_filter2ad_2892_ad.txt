to ﬁnd the maximum Nsub−warps. This process is iterated
twice, in the ﬁrst iteration the RASc uses the fault map of
SP0 and the ﬁnal output represents the number of sub-warps
required in case the warp gets issued to SP0. In the second
iteration the RASc uses the fault map of SP1 and the ﬁnal
output represents the number of sub-warps required in case
the warp gets issued to SP1.
In conclusion, for each warp,
the RASc attaches the
following deformation hint bits: (1) A ﬂag bit to indicate if
warp deformation is required (i.e. Split F lag). (2) A 3-bit
value that indicates the number of sub-warps needed (”000”:
No warp deformation, ”010”: Two sub-warps, ”011”: Three
sub-warps, and ”100”: Four sub-warps). Two sets of the hint
bits are attached for each warp, one set is generated based
on the fault map of SP0 and one based on the fault map
of SP1. Hence, eight additional bits are attached with each
warp instruction at the warp scheduler.
C. Inter-SP Warp Shufﬂing
The functionality of
inter-SP warp shufﬂing can be
achieved by augmenting the issue logic. Recall that in the
baseline architecture, when all input operands of a warp
instruction are ready, the warp instruction is forwarded to the
issue queues. All ready warp instructions targeted to execute
on a streaming processor are forwarded to the same issue
queue (i.e. SP issue queue). Every cycle, the issue logic
selects the most senior two warp instructions from SP issue
queue and issue them to SP0 and SP1 provided that there
are no structural hazards.
Instead, we propose to distribute the ready warp in-
structions into four issue queues according to the warps’
SP 0 Split F lag and SP 1 Split F lag as shown in Ta-
ble II. The 1st issue queue holds the ready warp instructions
which do not require deformation in SP0, but require defor-
mation in SP1. The second issue queue holds all warps that
437437437
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:27:21 UTC from IEEE Xplore.  Restrictions apply. 
Issue Queue
1st
2nd
3rd
4th
SP0 Split Flag
0
1
0
1
SP1 Split Flag
1
0
0
1
Issued 
Active 
Mask 
2
t
o
1
M
u
x
S 
Table II: Inter-SP Warp Shufﬂing
New Warp 
I
n
p
u
t
M
a
s
k
R
e
g
require deformation in SP0, but not in SP1. The third issue
queue holds all warps that need deformation neither in SP0
nor in SP1. The fourth issue queue holds warps that require
deformation irrespective of which SP they are issued to.
In addition, the issue logic is augmented with priority
issue logic, such that it tries to issue a ready warp instruction
to SP0 by checking the four issue queues in the following
order: 1st, 3rd, 4th, and ﬁnally the 2nd queue. In the same
cycle,
the issue logic tries to issue another ready warp
instruction to SP1 by checking the four issue queues in the
following order: 2nd, 3rd, 4th, and ﬁnally the 1st queue. The
issue logic extracts the oldest warp within each issue queue.
D. Issuing Deformed Warps
Once the issue logic selects the warp instruction to be
issued,
the Reliability-Aware-Split (RASp) unit uses the
warp’s deformation hint bits to generate the active masks
of the sub-warps, in case deformation is needed. Each SP
has a dedicated RASp unit associated with it. Fig. 10 depicts
the design of the Reliability-Aware-Split (RASp) unit. In the
interest of space, we only show one part of the RASp unit
which is responsible for generating the active masks of a
single cluster of four SIMT lanes. The full RASp design
can be thought of as n-replicas of Fig. 10, where n is
the number of clusters in the SP. The input mask register
shown in the ﬁgure gets initialized to the cluster’s original
active mask (i.e. Issued Active Mask) with every new warp
instruction. At the end of every cycle, the output of the 4-
to-1 multiplexer (MUX) is used as the cluster’s active mask
for the current sub-warp. Recall that when a warp is divided
into X sub-warps, the sub-warps are issued in X consecutive
cycles. Each cycle the sub-warp’s active mask is different,
which is generated by RASp unit.
When generating the new active masks for the required
sub-warps, there are three possible scenarios. First, when
no deformation is required (i.e. Split F lag = 0 and
Nsub−warps = 0),
the cluster’s original active mask is
selected as shown by the topmost input of the 4-to-1 MUX.
The second scenario is when deformation is required (i.e.
Split F lag = 1) and the number of sub-warps is two. In
this case, the input mask is ﬁrst ”ANDED” with ”0011”
to allow the two threads, mapped to lanes 0 and 1 of the
cluster, to be issued as part of the ﬁrst sub-warp. In the next
cycle, the input mask is ”ANDED” with ”1100” to allow
the two threads, mapped to lanes 2 and 3 of the cluster,
to be issued as part of the second sub-warp. The second
scenario is represented by the bottommost input of the 4-
0001 
0010 
0100 
1000 
0000 
8
t
o
1
M
u
x
2
t
o
1
M
u
x
 S 
Valid 
S2 S1 S0 
Last  
Sub-warp 
4
t
o
1
M
u
x
S1  S0 
Split_Flag 
Nsub-warps == 2 
Mask[0] 
Mask[1] 
Mask[2] 
Mask[3] 
P
r
i
o
r
i
t
y
E
n
c
o
d
e
r
1100 or 0011 
Output ≡ Current Sub-warp 
Mask 
Figure 10: RASp unit design
to-1 MUX. Notice that at the end of the ﬁrst cycle, the
input mask register gets updated with the ”XORING” of its
current value and the mask of the current sub-warp (i.e. the
output of the 4-to-1 MUX). In fact, this update process is
not needed for this scenario, but it is required for the third
scenario.
The third scenario is when deformation is required and
the number of sub-warps is three or four. In this case, one
thread is issued to the cluster as part of every sub-warp.
In order to choose one of the active threads for each sub-
warp, we use a 4-to-2 priority encoder. The outputs of the
encoder select one of ﬁve possible cluster masks given at the
inputs of the 8-to-1 MUX. When all bits in the input mask
are 0’s, the valid bit at the output of the encoder will be
0 and pattern ”0000” is chosen as the cluster mask for the
current sub-warp. Otherwise, the priority encoder generates
the index of the rightmost active thread and the appropriate
pattern (i.e. ”0001”, ”0010”, ”0100”, or ”1000”) is chosen as
the cluster’s mask for the current sub-warp. In this scenario,
it is important to update the input mask register at the end
of every cycle, so that the priority encoder generates the
index of the subsequent active thread from the right. The
third scenario is represented by the third topmost input of
the 4-to-1 MUX. Rather than leaving the second topmost
input of the 4-to-1 MUX dangling, we choose to drive it
from the input mask register.
The 2-to-1 MUX placed between the 8-to-1 and 4-to-1
MUXes is used to choose the value of the input mask register
as the cluster’s mask for the last sub-warp. To explain why
this detour is required, let us consider an example where a
warp of size 8 is mapped to two clusters. The ﬁrst cluster
has a single healthy lane and three active threads mapped
to it. The second cluster has four healthy lanes and four
active threads mapped to it. In this case, the warp needs to
be deformed into three sub-warps. For the ﬁrst cluster, the
RASp unit will generate three masks through the priority
encoder, each mask has one active thread. For the second
cluster, the ﬁrst two masks are also generated through the
priority encoder with one active thread in each of them. If
the third mask of the second cluster is left to be generated
through the priority encoder, it will also include a single
active thread and the fourth active thread will not be issued.
To overcome this limitation, we use the 2-to-1 MUX to select
the input mask register, which includes all active threads
438438438
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:27:21 UTC from IEEE Xplore.  Restrictions apply. 
which are yet to issue, as the mask for the last sub-warp.
At every cycle, the 4-bit masks of all clusters, generated
by the RASp unit, are concatenated together to form the
active mask of the current sub-warp. Then, the sub-warp
active mask is fed to the intra-cluster thread shufﬂing control
logic , given in Table I, in order to generate the appropriate
select signals for the shufﬂing and reshufﬂing MUXes.
V. EXPERIMENTAL EVALUATION
A. Methodology
We evaluated the intra-cluster thread shufﬂing and dy-
namic warp deformation techniques for performance over-
head, given various streaming processors’ fault maps, using
GPGPU-Sim v3.02 [5]. For evaluation, 18 benchmarks from
ISPASS [5], Parboil [2], and Rodinia [8] benchmarks suites
are used to cover a wide range of application domains.
We conﬁgured the baseline GPGPU architecture using the
Nvidia GTX480 conﬁguration included in the GPGPU-Sim
package. The baseline architecture core runs at 700MHz
clock frequency and consists of 15 streaming processors
(SMs). The reader can refer to subsection II-A for the details
of the SM design. On top of the baseline architecture, we
implemented the intra-cluster thread shufﬂing, dynamic warp
deformation, and inter-SP warp shufﬂing in GPGPU-Sim.
Thus, the reported performance and power overheads take
into account the added circuitry including the control logic,
shufﬂing and reshufﬂing MUXes, RASc, and RASp unit.
To evaluate the performance overhead introduced by the
intra-cluster thread shufﬂing and dynamic warp deformation
techniques, we assume a fault map for each SP and run
our simulation experiments accordingly. In reality each SP
would have two fault maps: one for the integer pipelines and
one for the ﬂoating-point pipelines. In our evaluation, and
for the sake of simplicity, we assume a single fault map for
each SP. Hence, when we refer to a SIMT lane as faulty, it
indicates that both integer and ﬂoating-point pipelines within
the lane are faulty. Even with such simpliﬁcation there are
still a tremendous number of possible fault maps for all SPs
within the 15 SMs (≈ 57 Trillion). We also assume that at
least one SIMT lane is healthy in each cluster. Hence, fault
maps in which one or more clusters in any SP are completely
faulty (i.e. the four SIMT lanes in the cluster are faulty) are
to be handled as part of the future work and are not covered
in this paper.
To further limit the investigated fault maps, we make two
more conservative assumptions: clusters within the same SP
suffer from the same number of faulty lanes and all 15 SMs
share the same fault maps for their respective SP0 and SP1.
Based on aforementioned assumptions, six representative
fault maps are chosen to evaluate our techniques,
two
of which represent the expected common and worst case
fault maps. We refer to every investigated fault map using
the following convention SP 0 x SP 1 y, where x is the
number of faulty lanes in each SIMT cluster of SP0 and y
is the number of faulty lanes in each SIMT cluster of SP1.
B. Common Case Fault Map
The expected common case fault map is derived from our
empirical results on SIMT lanes utilization proﬁles given
in Fig. 3. The ﬁgure shows that within each cluster, two
SIMT lanes are heavily utilized compared to the other two
lanes. So, it would be commonly expected for the highly
utilized lanes to experience hard faults simultaneously while
the lightly utilized lanes remain healthy. In other words, in
the common case fault map, each cluster is expected to suffer
from two faulty lanes (i.e.SP 0 2 SP 1 2) which means
that 50% of the GPGPU compute power is shut off.
Fig.
11a
shows
performance
With SP 0 2 SP 1 2 fault map, intra-cluster thread shuf-
ﬂing is sufﬁcient as long as the number of active threads
mapped to each SP cluster is less than or equal to two. For
warps that map more than two active threads to at least one
cluster, warp deformation will be unavoidable. Since every
SIMT cluster has two healthy lanes, the number of sub-
warps will always be two whenever a warp is deformed in
the common case fault map.
the
for
SP 0 2 SP 1 2 fault map relative to the baseline fault-free
run. Benchmarks with high threads activity (Backbrop,
Cutcp, Heartwall, Hotspot, and Sad) experience 20% - 40%
performance overhead. This is expected because all warps
with at least one fully utilized cluster (i.e. a cluster with
four active threads mapped to it) will be split into two
sub-warps issued in two consecutive cycles. On the other
hand, the rest of the benchmarks experience less than 10%
performance overhead. The last column shows a weighted
average of 7% performance overhead. The variations in the
performance overhead across the benchmarks are due to
two reasons.
overhead
First, intra-cluster thread shufﬂing opportunities vary from
one benchmark to another. For some benchmarks (e.g. CP,
Mri-q, and WP), intra-cluster thread shufﬂing is sufﬁcient
most of the time and as a result dynamic warp deformation is
required much less frequently. Second, some benchmarks are
memory bound (e.g. MUM, WP, and bfs) and computation
delays, due to warp deformation and extra pipeline stages,
will have a small impact on the overall performance.
C. Worst Case Fault Map
In order for intra-cluster thread shufﬂing and dynamic
there
warp deformation to guarantee execution progress,
least one healthy SIMT lane per cluster.
should be at
Hence, the worst case fault map would render three SIMT
lanes useless within each cluster (i.e.SP 0 3 SP 1 3). This
means that in the worst case, only 25% of the SIMT lanes
within the entire GPGPU are healthy.
With SP 0 3 SP 1 3 fault map, intra-cluster thread shuf-
ﬂing is sufﬁcient as long as the number of active threads
439439439
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:27:21 UTC from IEEE Xplore.  Restrictions apply. 

(























+





(



















(a) SP0 2 SP1 2 (Common)

(b) SP0 3 SP1 3 (Worst)
Figure 11: Performance Overhead for Common and Worst Fault Maps (Symmetric Fault Maps)
Fig.
11b
the
shows
overhead
mapped to each SP cluster is less than or equal to one. For
warps that map more than one active thread to at least one