local instance of T . The restarted instance of T inherits, in
this case, the read-set of its previous incarnation purged of
any data previously received from Y , with one exception: if
the remote abort is detected when receiving a remote value
(l. 11), this value belongs to a fresh remote snapshot at Y and
can be retained in the read-set.
∗ completes its execution (l. 23), it
Commit logic. When T
speculatively commits and broadcasts SC messages to all
partitions in P. The SC messages disseminate the ABORTSET
∗, informing remote partitions about the local transaction
of T
∗ at X. Upon reception of a SC message
instances aborted by T
from partition Y (l. 34): i) the SCMSG_GAV associated with
partition Y is updated with the GAVmsg of the transaction
(cid:2)
instance that sent the SC message; ii) for any transaction T
included in the SC message’s ABORTSET, if the corresponding
(cid:2),
LAN is larger than the Y -th entry of the GAV of T
it
means that X detected a new remote abort at Y . Thus, the
remoteAbort() method is called.
For T
to be ﬁnal committed, its GAV must coincide with
the ﬁnal GAV for Ti. This is determined (l. 29) after waiting for
transaction Ti−1 (i.e., the transaction immediately preceding
Ti) to have ﬁnal committed (Cond. C1), which implies that the
ﬁnal GAV of Ti−1 is known. So, to determine the ﬁnal GAV of
Ti, it sufﬁces to wait for the reception, from every remote
partition, of an SC message tagged with the ﬁnal GAV of
Ti−1 (Cond. C2, l. 33). After this moment, in fact, no instance
of Ti can any longer be aborted at any partition. Thus, if a
∗ returns from
speculatively committed transaction instance T
waitFinalGAV() without being aborted, it means that none of
the instances of T ’s preceding transactions have invalidated
∗’s GAV coincides with the
∗ global snapshot. In this case, T
T
ﬁnal GAV for T and T
Implicit dissemination of SC messages. To reduce the overhead
of the SC mechanism, Sparkle exploits a key optimization,
not reported in the pseudo-code: instead of sending ad hoc
∗ can be ﬁnal committed.
∗
i
SC messages, these are piggybacked on the messages used by
MPTs to disseminate the results of read operations.
Dealing with heterogeneous MPTs The correctness of the
SC mechanism presented above hinges on the assumption that
the batch is composed solely by MPTs accessing the same
partitions. This ensures that the ﬁrst MPT of the batch never
undergoes aborts. Clearly, this property no longer holds if
batches are composed by mixes of SPT and MPTs involving
heterogeneous sets of partitions. In fact, in the general case, a
(multi-partition) transaction can undergo an unknown number
of aborts if it is preceded even just by a single transaction (nd
is executed concurrently with it.
To cope with the above problem, Sparkle only allows
executing an MPT, if all its preceding uncommitted transac-
tions are either read-only transactions (ROTs) or homogeneous
170
MPTs of this MPT. While trivially ensuring the correctness
of the SC mechanism, if naively employed, this technique
can also signiﬁcantly hinder parallelism. For instance, if three
homogeneous MPTs are interleaved by two SPTs, then these
three MPTs have to be executed sequentially.
Sparkle tackles this issue via a scheduling mechanism,
which operates as follows. First, upon delivery of a trans-
action batch, at the end of the ordering phase, each partition
deterministically reorders the MPTs in the batch by grouping
them according to the set of partitions they access1. The
resulting ﬁnal order is composed by a sequence G1, . . . , Gn
of transaction groups, where each group Gi contains the
transactions that access the same set of partitions. Next,
each partition deterministically re-orders its SPTs and ROTs,
serializing them in between each pair of consecutive MPT
groups, with the goal of “spacing them out”. The number
of SPTs and ROTs serialized in between two groups are
calculated in a deterministic fashion, with the goal of ensuring
that each group interval is ﬁlled with an even number of
SPTs/ROTs. Note that since MPTs can not be executed while
there are preceding active SPTs, in between two groups we
always place SPTs before ROTs, to space out SPTs and the
following MPT group. Note that since only transactions of the
same batch can be reordered, and that these are necessarily
concurrent, scheduling does not compromise real-time order.
C. Read-only Transactions
Since ROTs do not alter the state of the data store, they
can be executed at a single partition’s replica and serialized
in an arbitrary order, provided that they observe a consistent
snapshot of the data store. To minimize overheads, in Sparkle
ROTs are executed concurrently with the remaining update
transactions, but in a non-speculative fashion, i.e., by assigning
them a timestamp associated with a ﬁnal committed transac-
tion. This allows sparing ROTs from the overheads associated
with registering themselves among the read dependencies of
the keys they read — which becomes unnecessary since, being
serialized after a ﬁnal committed update transaction, ROTs are
guaranteed to observe a stable snapshot.
While single partition ROTs can be freely assigned any
serialization order by their local partition, this is not the case
for read-only MPTs. In this case, it is necessary to ensure
that a read-only MPT is assigned the same serialization order
at all the partitions it involves. Sparkle tackles this problem
through a deterministic scheduling policy, which serializes
every read-only MPT before any other transaction of their
batch — this ensures the stability of the snapshot over which
they are executed and allows them to be executed in a non-
speculative fashion, analogously to read-only SPTs.
VI. EVALUATION
This section is devoted to experimentally evaluate Sparkle,
by comparing it with two state of the art PRSM systems,
1The current prototype uses a single thread to re-order transactions, as in
all tested workloads the scheduling thread was never the bottleneck.
171
[41]. The original
namely S-SMR [5] and Calvin [42]. Due to space constraints,
we omit some evaluation results, which can be found in [29].
We implemented Sparkle and S-SMR, based on Calvin’s
code base
code base uses STL’s
unordered_map as the in-memory back-end to store data,
which we found out to become the system’s bottleneck at
high thread counts. Therefore,
in our implementation, we
replaced it with concurrent_hash_map from Intel’s TBB
library [21]. The repository containing the code used in this
study is publicly accessible [28].
To quantify the scalability of Sparkle on large multi-core
architectures (§VI-B) we use a machine equipped with two
Intel Xeon E5-2648L v4 CPUs, consisting in total of 28 cores
(56 hardware threads). All other experiments were conducted
on the Grid’5000 cluster [17] using 8 genepi machines, each of
which has two 4-cores Intel Xeon E5420 QC CPUs. Unless
otherwise noted, all protocols use three cores for auxiliary
tasks needed for the evaluation (e.g., network communication
and workload generation); other than that, Calvin dedicates
one core for serializing lock requests and four other cores to
execute transactions, Sparkle uses ﬁve cores to execute trans-
actions, and S-SMR only uses one core to execute transactions.
The presented results are the average of three runs. We also
report the results’ standard deviation, but since the differences
in performance across different runs are usually within 5%, in
various plots, standard deviations are not visible.
As in prior work [42], we emulate the ordering phase
by injecting a 200ms delay and use 10 milliseconds batch
time. To avoid overloading the system, we adjust the arrival
rate to be 10%-20% larger than the maximum sustainable
throughput (determined via a preliminary test). Therefore,
batch sizes vary depending on the workload, ranging from 10s
to 100s of transactions. Omitting the ordering phase allows
for focusing the evaluation on scenarios where throughput is
bottlenecked by the execution phase. This is typically the case
when one employs batching techniques [14], [38] to increase
the maximum throughput sustainable by the ordering phase. As
for the choice of the delay of the ordering phase, we argue that
using smaller values would reduce user perceived latency but
it would not affect the throughput of the considered solutions.
A. Benchmarks
Synthetic benchmark. In this benchmark each partition con-
tains one million keys, split in two sets, which we call “index”
and “normal” keys, respectively. All
transactions start by
reading and updating ﬁve index keys selected uniformly at
random. If the transaction is a ‘dependent transaction’, it reads
ﬁve additional normal keys, whose identity is determined by
the values read from the ﬁve index keys (i.e., the read- and
write-set of dependent transactions can only be determined
during execution). Else, if the transaction is non-dependent,
it reads and updates ﬁve randomly selected normal keys. If a
transaction accesses more than one partition, it divides equally
its accesses among its involved partitions. For instance, if a
dependent transaction accesses two partitions, then it accesses
three index keys and three normal keys of a partition and
(a) No conﬂict.
Fig. 3: Single node deployment, TPC-C 90% update workload.
(b) High conﬂict.
Fig. 4: 8 nodes cluster, synthetic benchmark generating workloads with varying contention level, percentage of MPTs and of dependent
transaction. MC stands for medium contention and LC for low contention.
the other two index and two normal key from the second
partition. Multi-partition transactions, unless otherwise noted,
always access two partitions.
We shape the workloads generated via this synthetic bench-
mark by varying three parameters: contention level (low and
medium contention), percentage of dependent
transactions
(0%, 1%, 10%, 50% and 100%) and percentage of distributed
transactions (0%, 1%, 10% and 50%). We control contention
by varying the number of index keys of each partition (using
the remaining keys as normal keys): in the low contention
scenario, partitions use 50000 index keys; 1000 index keys per
partition are used, instead, for the medium contention case.
TPC-C. The TPC-C benchmark [43] has ﬁve transaction
proﬁles: NewOrder, Payment, OrderStatus, StockLevel and
Delivery. NewOrder and Payment are update transactions that
access a warehouse hosted on a remote partition with proba-
bility 10% and 15%, respectively. OrderStatus and StockLevel
transactions are read-only, single-partition transactions (SPTs).
Delivery transactions are update SPTs. Finally, NewOrder and
Payment are independent transactions, while the other three
are dependent transactions (i.e., their working set depends on
the database state and cannot be predicted statically).
We consider three different transaction mixes, containing
10%, 50% and 90% update transactions. All transaction mixes
always contain only 4% of Delivery transactions, while the
other two update and read-only transactions evenly share the
rest of the proportion. Except
in §VI-B, we populate 12
warehouses per data partition in all TPC-C experiments.
B. Single node deployment
Before testing Sparkle in distributed settings, we focus on
single node performance, evaluating its scalability on a large
172
multi-core machine equipped with 56 hardware threads.
When testing Sparkle and Calvin we deploy a single data
partition, and increase the total number of worker threads up to
50, dedicating 6 threads to workload generation. Conversely,
since S-SMR can only utilize one worker thread per data
partition, the only way to let it exploit the parallelism of the
underlying architecture is by varying the number of partitions,
which we increase up to 50 (using the same amount of data).
We use the 90% update TPC-C workload, and adjust the
contention level by varying the number of warehouses to
generate two extreme scenarios: a very high conﬂict workload,
in which only a single warehouse is populated, and a no
conﬂict workload,
in which we populate a large number
of warehouses (200) and alter the workload to generate
no conﬂicts (by having concurrent requests access disjoint
warehouses). For the no-conﬂict workload, we consider an
additional N oCC baseline, i.e., a protocol which implements
no concurrency control whatsoever. This represents an ideal
baseline that allows us to better understand the scalability limit
and overhead of each protocol.
Fig. 3a reports the performance of the considered protocols
using the no-conﬂict workload. As we can see, Sparkle has
almost identical throughput to the ideal N oCC baseline up
to 30 threads,
incurring less than 20% overhead with 40
and 50 threads. These results clearly highlight the efﬁciency
and practicality of Sparkle’s concurrency control. Conversely,
Calvin’s throughput only scales up to ﬁve threads (one locker
thread and four worker threads). At higher thread counts, the
scheduling thread turns into the system’s bottleneck, severely
hindering its scalability. Last but not least, we can see that S-
SMR achieve good scalability and outperforms Calvin when
using more than 25 threads. However, S-SMR achieves 2.6×
First, let us discuss Fig. 4a ﬁrst, which reports results for
a workload that does not generate any MPT. We can see that
Sparkle overall achieves the highest throughput, and that its
performance is slightly reduced in the MC workload, but is
not affected by the rate of dependent transactions. In this
scenario S-SMR also achieves approx. 60% lower throughput
than Sparkle. This can be explained considering that Sparkle
(and Calvin) can process transactions concurrently, using all
the available cores (5 in this testbed), whereas S-SMR’s single
thread execution model intrinsically limits its scalability.
Finally, looking at Calvin’s throughput, we can see that
its throughput reduces dramatically as the ratio of dependent
transaction increases. Nevertheless, even with 0% of dependent
transaction, Calvin’s throughput is throttled by its scheduling
thread, which leads it to achieve lower throughput than both
Sparkle and S-SMR. With 100% of dependent transactions,
Calvin thrashes, as the likelihood for dependent transactions
to be aborted (possibly several
time) quickly grows even
in low/medium conﬂict workloads. In fact, Calvin needs
to execute a so called reconnaissance phase for dependent
transactions to estimate their read- and write-sets, and if
the prediction turns out to be wrong during execution, these
transactions have to be aborted and re-executed. Note that the
high frequency of abort of dependent transactions imposes
overhead not only to worker threads, but also to Calvin’s
scheduler thread – upon each abort and restart of a (dependent)
transaction, the scheduler thread has to release and acquire its
locks, incurring non-negligible overhead.
Figs. 4b, 4c and 4d report the results obtained when increas-
ing the percentage of MPTs to 1%, 10% and 50%, respectively.
The ﬁrst observation we make is that that S-SMR’s throughput
drops signiﬁcantly as the rate of MPT grows. As already
mentioned in §VI-B, MPTs incur a large overhead with S-
SMR, due to the synchronization they impose between the
worker threads of different partitions. Since S-SMR uses a
single worker thread per partition, whenever a MPT is forced
to block waiting for remote data from a sibling partition, no
other transaction can be processed at that partition — unlike
in Calvin or Sparkle. In distributed settings, as the communi-
cation latency between partitions is strongly ampliﬁed (with
respect to the single machine scenario considered in §VI-B)
the performance toll imposed by MPT also grows radically and
S-SMR’s throughput is severely throttled by network latency:
with 50% MPTs, S-SMR’s throughput drops by about 40×
compared with the case of no MPTs!
The throughput of Calvin and Sparkle throughput reduces
more gradually as the MPT increases. This is because both of
them allow activating the processing of different transactions,
whenever an MPT is blocked waiting for remote data. Similar
to what already observed in Figure 4a, also in this case, the
throughput of Calvin drops dramatically in presence of even
a small fraction of dependent transactions, approximately by
a factor 2× with as low as 10% dependent transactions.
By analyzing Sparkle, we see that although its throughput
also reduces with distributed transactions, its throughput is
not affected as signiﬁcantly as with S-SMR. It is also worth
Fig. 5: 8 nodes cluster, TPC-C workloads.
than Sparkle at 50 threads. This is due
lower throughput
in this workload, approximately 10% of
to the fact
that,
transactions access a remote warehouse, which with S-SMR
may be stored on a different partition (unlike Sparkle and