simplicity of A(n) will enable the illustration of some key
insights in an intuitive and yet rigorous manner. The corre-
sponding expressions for the CCDF and MGF are
FA(n)(σ) = (cid:26) 1, σ  10 = G(1). A more in-
tuitive argument is that the backlog process depends on the
entire history of the process, and relative to any time point,
whereas Eq. (17) only captures the process relative to the
origin.
Let us next make an additional stationarity assumption
on A(n). Then, if a deterministic computation of queueing
measures was the goal, the model from Eq. (17) would re-
main insuﬃcient; in fact, the model would only be suﬃcient
for constant-rate arrivals. However, if a probabilistic com-
putation was the goal, then the model from Eq. (17) could
become useful.
Indeed, a computation of the backlog tail
would be for instance
P (Bn > σ) = P(cid:18) max
0≤k≤n {A(k, n) − C(n − k)} > σ(cid:19)
P (A(k, n) − C(n − k) > σ) .
(19)
≤
n
Xk=0
The last line follows from Boole’s inequality2. As a side
remark, we point out that this apparently loose inequality is
2For some probability events E and F , Boole’s inequality is
P (E ∪ F ) ≤ P(E) + P(F ).
316not so bad if the r.v.’s Xk := A(k, n) are rather uncorrelated
(e.g., when A(n) is a (discretized) Poisson process), but it
is quite loose if they are highly correlated [46].
In such a
case one can make use of more sophisticated techniques with
reﬁned martingale inequalities (see [12], pp. 339-343). For
the purpose of our presentation it is suﬃcient to adopt the
simpliﬁed technique with Boole’s inequality.
Due to the stationarity assumption, the last line can be
continued by replacing A(k, n) with A(n − k), and further
by G(n − k) according to the bounding arrival model from
Eq. (17). Note however that G(n) should be deﬁned as a ran-
dom process, and the inequality from Eq. (17) should hold
a.s. (almost surely, i.e., P (A(n)  C the bound diverges and clearly becomes use-
less when n → ∞. We point out, however, that the bound
is tight even under a statistical independence assumption on
Aj(n)’s. Indeed, ∀p, n > 0 there exists a positive probabil-
ity such that Aj(n) = n ∀j, i.e., there exists a sample-path
which attains the apparently very loose bound on Bn.
Another illustrative example concerns possible degener-
ate results obtained from deterministic modelling and anal-
ysis. This is the case when the arrival process A(n) can
take inﬁnitely large values, i.e., ∀ K > 0 ∃εK > 0 such
that P(A(n) > K) > εK (the Poisson process is an exam-
ple). For such arrival processes, the regularity constraint
from Eq. (20) is only satisﬁed by the degenerate function
G(n) = ∞, which clearly yields the degenerate, and useless,
backlog bound Bn ≤ ∞. However, reiterating the previous
argument, this degenerate bound is also tight.
We conclude here by pointing out that if one seeks deter-
ministic bounds from deterministic or even stochastic arrival
models, then DNC is an attractive theory (illustrated here
by Eqs. (20)-(21)): it has a high modelling potential and it
(mostly) yields tight bounds. Otherwise, if one seeks proba-
bilistic bounds, e.g.,
A(k, n) ≤ G(n − k) ∀0 ≤ k ≤ n ,
(20)
where G(n) is a non-random function. Network calculus was
essentially founded on this arrival model [16].
With this arrival model, the deterministic continuation of
Eq. (18) is straightforward:
P (Bn > σ) ≤ ε(σ), where ε(σ) is to be determined,
then DNC is an inopportune theory. The main reason is
that a purely deterministic analysis can yield extremely loose
bounds due to not leveraging from statistical multiplexing
gain. This discussion will be continued in Section 5.
Bn ≤ max
0≤k≤n {G(k) − Ck} .
(21)
4.3 Stochastic Arrival Models
The RHS term can be computed explicitly in O(1) time if
G(n) is a suﬃciently ‘nice’ expression, e.g., G(n) = rn + b
where r and b have the meanings of rate, and burst, respec-
tively. Otherwise, if G(n) is given pointwise, then the RHS
term can be computed in O(n) time.
We emphasize that there is no requirement of stationarity
on the arrival process A(n). In fact, the regularity constraint
from Eq. (20) is satisﬁed by inﬁnitely many (and possibly
unknown) arrival processes, thus illustrating the high mod-
elling potential of Eq. (20). Moreover, despite the apparent
tradeoﬀ between modelling potential and accuracy of repre-
sentation, the derivation from Eq. (21) is actually tight ([5],
p. 27). Tightness means that there exists an arrival process
which 1) satisﬁes the arrival bound from Eq. (20), and 2)
induces a backlog process which matches with the predicted
bound from Eq. (21). Even more remarkably, the tightness
of the backlog bound holds even when multiplexing many
possibly ‘conspiring’ ﬂows, e.g., producing large bursts at
the same time.
us consider an aggregate A(n) = PN
To more concretely elaborate on the tightness of the de-
terministic modelling and analysis from Eqs. (20)-(21), let
j=1 Aj(n), where Aj(n)’s
are compound Bernoulli processes as in Eq. (14). The ﬂows
are multiplexed at a server with capacity C  σ(cid:17) ≤ ε(σ) ∀ k, n, σ
0≤k≤n {A(k, n) − G(n − k)} > σ(cid:17) ≤ ε(σ) ∀ n, σ
S2BB : P(cid:16) max
S3BB : P(cid:16) max
0≤k≤n≤∞ {A(k, n) − G(n − k)} > σ(cid:17) ≤ ε(σ) ∀ σ
G(n)’s are non-random and are called envelope functions.
ε(σ)’s are called error functions. Some technical and quite
intuitive conditions are that the envelope functions are non-
decreasing, whereas the error functions are non-increasing.
Note also that a degree of freedom of the bounding approach
is that for all three models the arrival process A(n) does not
need to be stationary, although the bounds themselves (the
envelopes G(n)) are so.
Before we explain the three models, it is important to ob-
serve the formation of the bottom two: S2BB [17] is formed
by inserting the free variable k from SBB (short-hand for
stochastically bounded burstiness) [45] into the probability,
whereas S3BB [26] is further formed by inserting the free
variable n as well. Informally, S3BB measures events con-
sisting of all past histories of the process A(n), i.e., relative
to all times. In turn, S2BB measures events consisting of
a single past history, i.e., relative to a ﬁxed time, whereas
SBB measures events as single fragments of past histories.
Although the SBB model seems the simplest amongst the
three, it is actually the S2BB model which is the natural
317extension of the classic deterministic model from Eq. (20).
To see the reason, rewrite Eq. (20) as
max
0≤k≤n{A(k, n) − G(n − k)} ≤ 0 ∀ n .
(22)
Note that the S2BB model enforces a bound on the CCDF
of the LHS term above. In other words, the S2BB model
quantiﬁes, with an upper bound, the probability that the
deterministic model is violated by more than σ. An attrac-
tive property of S2BB is that it immediately lends itself to
the calculation of performance bounds. Indeed, a straight-
forward manipulation of Eq. (18) and S2BB yields the bound
P (Bn > σ0 + σ) ≤ ε(σ) ,
(23)
where σ0 = maxk≥0 {G(k) − Ck} is exactly the deterministic
backlog bound from Eq. (21). To recapitulate, S2BB quan-
tiﬁes the violation probabilities of the deterministic model
(see S2BB and Eq. (22)), whereas the probabilistic backlog
bound quantiﬁes the violation probabilities of the determin-
istic backlog bound (see Eqs. (23) and (21)). As these vio-
lation probabilities are identical, one can argue that S2BB
is the ‘natural’ probabilistic extension of the deterministic
arrival model from Eq. (20) (or, equivalently, from Eq. (22)).
In practice, the choice of SBB vs. S2BB depends on the
arrivals’ input. If the input is a measurement trace, then
S2BB should be chosen since it immediately lends itself to
performance bounds, as shown earlier. Given a trace A(n)
with n elements, an S2BB ﬁtting algorithm would follow the
steps: 1) make a guess on G(n) (e.g., G(n) = (r + δ)n, where
r is the average rate of the trace and δ > 0 is a tuning
parameter), 2) compute the partial sums A(k, n) and the
LHS terms in Eq. (22), and 3) ﬁt a distribution function.
Ignoring the accuracy of the ﬁtting, i.e., the range of values
σ, the algorithm runs in O(n2) time. There is no speciﬁc
rule for the tuning parameter δ, which is to be optimized
numerically.
If the arrivals’ input is some random process A(n), then it
is generally easier to ﬁrst ﬁt the SBB model. A typical way
is to derive an MGF bound, e.g., MA(n)(θ) ≤ eθrn, for some
θ > 0 (see Eq. (16) for Markov arrival processes). If A(n) is
also stationary, then an SBB model can be ﬁtted using the
Chernoﬀ bound3, i.e.,
P (A(k, n) > r(n − k) + σ) ≤ e−θσ ∀k, n, σ .
(24)
This SBB model further lends itself to the S2BB model.
Indeed, one can write for some δ > 0:
P(cid:18) max
0≤k≤n {A(k, n) − (r + δ)(n − k)} > σ(cid:19)
≤ X0≤k r(k − n) + δ(k − n) + σ)
The second line follows from Boole’s inequality, whereas the
exponential bounds in the last line follow from Eq. (24).
What is important to remark is that the transition from the
SBB to S2BB involves a rate increase from r to r + δ. This