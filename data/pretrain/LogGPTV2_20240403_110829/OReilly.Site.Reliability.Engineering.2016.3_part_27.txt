• The delivery date for a new cluster of compute resources slips.
• A product decision about a performance goal changes the shape of the required
service deployment (the service’s footprint) and the amount of required
resources.
Minor changes require cross-checking the entire allocation plan to make sure that the
plan is still feasible; larger changes (such as delayed resource delivery or product
strategy changes) potentially require re-creating the plan from scratch. A delivery
slippage in a single cluster might impact the redundancy or latency requirements of
multiple services: resource allocations in other clusters must be increased to make up
for the slippage, and these and any other changes would have to propagate through‐
out the plan.
Also, consider that the capacity plan for any given quarter (or other time frame) is
based on the expected outcome of the capacity plans of previous quarters, meaning
that a change in any one quarter results in work to update subsequent quarters.
Laborious and imprecise
For many teams, the process of collecting the data necessary to generate demand
forecasts is slow and error-prone. And when it is time to find capacity to meet this
future demand, not all resources are equally suitable. For example, if latency require‐
ments mean that a service must commit to serve user demand on the same continent
as the user, obtaining additional resources in North America won’t alleviate a capacity
shortfall in Asia. Every forecast has constraints, or parameters around how it can be
fulfilled; constraints are fundamentally related to intent, which is discussed in the
next section.
Mapping constrained resource requests into allocations of actual resources from the
available capacity is equally slow: it’s both complex and tedious to bin pack requests
into limited space by hand, or to find solutions that fit a limited budget.
This process may already paint a grim picture, but to make matters worse, the tools it
requires are typically unreliable or cumbersome. Spreadsheets suffer severely from
scalability problems and have limited error-checking abilities. Data becomes stale,
and tracking changes becomes difficult. Teams often are forced to make simplifying
208 | Chapter 18: Software Engineering in SRE
assumptions and reduce the complexity of their requirements, simply to render
maintaining adequate capacity a tractable problem.
When service owners face the challenges of fitting a series of requests for capacity
from various services into the resources available to them, in a manner that meets the
various constraints a service may have, additional imprecision ensues. Bin packing is
an NP-hard problem that is difficult for human beings to compute by hand. Further‐
more, the capacity request from a service is generally an inflexible set of demand
requirements: X cores in cluster Y. The reasons why X cores or Y cluster are needed,
and any degrees of freedom around those parameters, are long lost by the time the
request reaches a human trying to fit a list of demands into available supply.
The net result is a massive expenditure of human effort to come up with a bin pack‐
ing that is approximate, at best. The process is brittle to change, and there are no
known bounds on an optimal solution.
Our Solution: Intent-Based Capacity Planning
Specify the requirements, not the implementation.
At Google, many teams have moved to an approach we call Intent-based Capacity
Planning. The basic premise of this approach is to programmatically encode the
dependencies and parameters (intent) of a service’s needs, and use that encoding to
autogenerate an allocation plan that details which resources go to which service, in
which cluster. If demand, supply, or service requirements change, we can simply
autogenerate a new plan in response to the changed parameters, which is now the
new best distribution of resources.
With a service’s true requirements and flexibility captured, the capacity plan is now
dramatically more nimble in the face of change, and we can reach an optimal solution
that meets as many parameters as possible. With bin packing delegated to computers,
human toil is drastically reduced, and service owners can focus on high-order priori‐
ties like SLOs, production dependencies, and service infrastructure requirements, as
opposed to low-level scrounging for resources.
As an added benefit, using computational optimization to map from intent to imple‐
mentation achieves much greater precision, ultimately resulting in cost savings to the
organization. Bin packing is still far from a solved problem, because certain types are
still considered NP-hard; however, today’s algorithms can solve to a known optimal
solution.
Intent-Based Capacity Planning
Intent is the rationale for how a service owner wants to run their service. Moving
from concrete resource demands to motivating reasons in order to arrive at the true
Intent-Based Capacity Planning | 209
capacity planning intent often requires several layers of abstraction. Consider the fol‐
lowing chain of abstraction:
1) “I want 50 cores in clusters X, Y, and Z for service Foo.”
This is an explicit resource request. But…why do we need this many resources
specifically in these particular clusters?
2) “I want a 50-core footprint in any 3 clusters in geographic region YYY for service Foo.”
This request introduces more degrees of freedom and is potentially easier to ful‐
fill, although it doesn’t explain the reasoning behind its requirements. But…why
do we need this quantity of resources, and why 3 footprints?
3) “I want to meet service Foo’s demand in each geographic region, and have N + 2
redundancy.”
Suddenly greater flexibility is introduced and we can understand at a more
“human” level what happens if service Foo does not receive these resources.
But…why do we need N + 2 for service Foo?
4) “I want to run service Foo at 5 nines of reliability.”
This is a more abstract requirement, and the ramification if the requirement isn’t
met becomes clear: reliability will suffer. And we have even greater flexibility
here: perhaps running at N + 2 is not actually sufficient or optimal for this ser‐
vice, and some other deployment plan would be more suitable.
So what level of intent should be used by intent-driven capacity planning? Ideally, all
levels of intent should be supported together, with services benefiting the more they
shift to specifying intent versus implementation. In Google’s experience, services tend
to achieve the best wins as they cross to step 3: good degrees of flexibility are avail‐
able, and the ramifications of this request are in higher-level and understandable
terms. Particularly sophisticated services may aim for step 4.
Precursors to Intent
What information do we need in order to capture a service’s intent? Enter dependen‐
cies, performance metrics, and prioritization.
Dependencies
Services at Google depend on many other infrastructure and user-facing services, and
these dependencies heavily influence where a service can be placed. For example,
imagine user-facing service Foo, which depends upon Bar, an infrastructure storage
service. Foo expresses a requirement that Bar must be located within 30 milliseconds
of network latency of Foo. This requirement has important repercussions for where
we place both Foo and Bar, and intent-driven capacity planning must take these con‐
straints into account.
210 | Chapter 18: Software Engineering in SRE
Furthermore, production dependencies are nested: to build upon the preceding
example, imagine service Bar has its own dependencies on Baz, a lower-level dis‐
tributed storage service, and Qux, an application management service. Therefore,
where we can now place Foo depends on where we can place Bar, Baz, and Qux. A
given set of production dependencies can be shared, possibly with different stipula‐
tions around intent.
Performance metrics
Demand for one service trickles down to result in demand for one or more other
services. Understanding the chain of dependencies helps formulate the general scope
of the bin packing problem, but we still need more information about expected
resource usage. How many compute resources does service Foo need to serve N user
queries? For every N queries of service Foo, how many Mbps of data do we expect for
service Bar?
Performance metrics are the glue between dependencies. They convert from one or
more higher-level resource type(s) to one or more lower-level resource type(s).
Deriving appropriate performance metrics for a service can involve load testing and
resource usage monitoring.
Prioritization
Inevitably, resource constraints result in trade-offs and hard decisions: of the many
requirements that all services have, which requirements should be sacrificed in the
face of insufficient capacity?
Perhaps N + 2 redundancy for service Foo is more important than N + 1 redundancy
for service Bar. Or perhaps the feature launch of X is less important than N + 0
redundancy for service Baz.
Intent-driven planning forces these decisions to be made transparently, openly, and
consistently. Resource constraints entail the same trade-offs, but all too often, the pri‐
oritization can be ad hoc and opaque to service owners. Intent-based planning allows
prioritization to be as granular or coarse as needed.
Introduction to Auxon
Auxon is Google’s implementation of an intent-based capacity planning and resource
allocation solution, and a prime example of an SRE-designed and developed software
engineering product: it was built by a small group of software engineers and a techni‐
cal program manager within SRE over the course of two years. Auxon is a perfect case
study to demonstrate how software development can be fostered within SRE.
Intent-Based Capacity Planning | 211
Auxon is actively used to plan the use of many millions of dollars of machine resour‐
ces at Google. It has become a critical component of capacity planning for several
major divisions within Google.
As a product, Auxon provides the means to collect intent-based descriptions of a
service’s resource requirements and dependencies. These user intents are expressed as
requirements for how the owner would like the service to be provisioned. Require‐
ments might be specified as a request like, “My service must be N + 2 per continent”
or “The frontend servers must be no more than 50 ms away from the backend
servers.” Auxon collects this information either via a user configuration language or
via a programmatic API, thus translating human intent into machine-parseable con‐
straints. Requirements can be prioritized, a feature that’s useful if resources are insuf‐
ficient to meet all requirements, and therefore trade-offs must be made. These
requirements—the intent—are ultimately represented internally as a giant mixed-
integer or linear program. Auxon solves the linear program, and uses the resultant
bin packing solution to formulate an allocation plan for resources.
Figure 18-1 and the explanations that follow it outline Auxon’s major components.
Figure 18-1. The major components of Auxon
Performance Data describes how a service scales: for every unit of demand X in clus‐
ter Y, how many units of dependency Z are used? This scaling data may be derived in
a number of ways depending on the maturity of the service in question. Some serv‐
ices are load tested, while others infer their scaling based upon past performance.
Per-Service Demand Forecast Data describes the usage trend for forecasted demand
signals. Some services derive their future usage from demand forecasts—a forecast of
queries per second broken down by continent. Not all services have a demand fore‐
cast: some services (e.g., a storage service like Colossus) derive their demand purely
from services that depend upon them.
212 | Chapter 18: Software Engineering in SRE
Resource Supply provides data about the availability of base-level, fundamental
resources: for example, the number of machines expected to be available for use at a
particular point in the future. In linear program terminology, the resource supply acts
as an upper bound that limits how services can grow and where services can be
placed. Ultimately, we want to make the best use of this resource supply as the intent-
based description of the combined group of services allows.
Resource Pricing provides data about how much base-level, fundamental resources
cost. For instance, the cost of machines may vary globally based upon the space/
power charges of a given facility. In linear program terminology, the prices inform
the overall calculated costs, which act as the objective that we want to minimize.
Intent Config is the key to how intent-based information is fed to Auxon. It defines
what constitutes a service, and how services relate to one another. The config ulti‐
mately acts as a configuration layer that allows all the other components to be wired
together. It’s designed to be human-readable and configurable.
Auxon Configuration Language Engine acts based upon the information it receives
from the Intent Config. This component formulates a machine-readable request (a
protocol buffer that can be understood by the Auxon Solver. It applies light sanity
checking to the configuration, and is designed to act as the gateway between the
human-configurable intent definition and the machine-parseable optimization
request.
Auxon Solver is the brain of the tool. It formulates the giant mixed-integer or linear
program based upon the optimization request received from the Configuration Lan‐
guage Engine. It is designed to be very scalable, which allows the solver to run in par‐
allel upon hundreds or even thousands of machines running within Google’s clusters.
In addition to mixed-integer linear programming toolkits, there are also components
within the Auxon Solver that handle tasks such as scheduling, managing a pool of
workers, and descending decision trees.
Allocation Plan is the output of the Auxon Solver. It prescribes which resources
should be allocated to which services in what locations. It is the computed implemen‐
tation details of the intent-based definition of the capacity planning problem’s
requirements. The Allocation Plan also includes information about any requirements
that could not be satisfied—for example, if a requirement couldn’t be met due to a
lack of resources, or competing requirements that were otherwise too strict.
Requirements and Implementation: Successes and Lessons Learned
Auxon was first imagined by an SRE and a technical program manager who had sepa‐
rately been tasked by their respective teams with capacity planning large portions of
Google’s infrastructure. Having performed manual capacity planning in spreadsheets,
Intent-Based Capacity Planning | 213
they were well positioned to understand the inefficiencies and opportunities for
improvement through automation, and the features such a tool might require.
Throughout Auxon’s development, the SRE team behind the product continued to be
deeply involved in the production world. The team maintained a role in on-call rota‐
tions for several of Google’s services, and participated in design discussions and tech‐
nical leadership of these services. Through these ongoing interactions, the team was
able to stay grounded in the production world: they acted as both the consumer and
developer of their own product. When the product failed, the team was directly
impacted. Feature requests were informed through the team’s own firsthand experi‐
ences. Not only did firsthand experience of the problem space buy a huge sense of
ownership in the product’s success, but it also helped give the product credibility and
legitimacy within SRE.
Approximation
Don’t focus on perfection and purity of solution, especially if the bounds of the prob‐
lem aren’t well known. Launch and iterate.
Any sufficiently complex software engineering effort is bound to encounter uncer‐
tainty as to how a component should be designed or how a problem should be tack‐
led. Auxon met with such uncertainty early in its development because the linear
programming world was uncharted territory for the team members. The limitations
of linear programming, which seemed to be a central part of how the product would
likely function, were not well understood. To address the team’s consternation over
this insufficiently understood dependency, we opted to initially build a simplified
solver engine (the so-called “Stupid Solver”) that applied some simple heuristics as to
how services should be arranged based upon the user’s specified requirements. While
the Stupid Solver would never yield a truly optimal solution, it gave the team a sense
that our vision for Auxon was achievable even if we didn’t build something perfect
from day one.
When deploying approximation to help speed development, it’s important to under‐
take the work in a way that allows the team to make future enhancements and revisit
approximation. In the case of the Stupid Solver, the entire solver interface was
abstracted away within Auxon such that the solver internals could be swapped out at
a later date. Eventually, as we built confidence in a unified linear programming
model, it was a simple operation to switch out the Stupid Solver for something, well,
smarter.
Auxon’s product requirements also had some unknowns. Building software with
fuzzy requirements can be a frustrating challenge, but some degree of uncertainty
need not be a showstopper. Use this fuzziness as an incentive to ensure that the soft‐
ware is designed to be both general and modular. For instance, one of the aims of the
Auxon project was to integrate with automation systems within Google to allow an
214 | Chapter 18: Software Engineering in SRE
Allocation Plan to be directly enacted on production (assigning resources and turn‐
ing up/turning down/resizing services as appropriate). However, at the time, the
world of automation systems was in a great deal of flux, as a huge variety of
approaches were in use. Rather than try to design unique solutions to allow Auxon to
work with each individual tool, we instead shaped the Allocation Plan to be univer‐
sally useful such that these automation systems could work on their own integration
points. This “agnostic” approach became key to Auxon’s process for onboarding new
customers, because it allowed customers to begin using Auxon without switching to a
particular turnup automation tool, forecasting tool, or performance data tool.
We also leveraged modular designs to deal with fuzzy requirements when building a
model of machine performance within Auxon. Data on future machine platform per‐
formance (e.g., CPU) was scarce, but our users wanted a way to model various sce‐
narios of machine power. We abstracted away the machine data behind a single
interface, allowing the user to swap in different models of future machine perfor‐
mance. We later extended this modularity further, based on increasingly defined
requirements, to provide a simple machine performance modeling library that
worked within this interface.
If there’s one theme to draw from our Auxon case study, it’s that the old motto of
“launch and iterate” is particularly relevant in SRE software development projects.
Don’t wait for the perfect design; rather, keep the overall vision in mind while moving
ahead with design and development. When you encounter areas of uncertainty,
design the software to be flexible enough so that if process or strategy changes at a
higher level, you don’t incur a huge rework cost. But at the same time, stay grounded
by making sure that general solutions have a real-world–specific implementation that
demonstrates the utility of the design.
Raising Awareness and Driving Adoption
As with any product, SRE-developed software must be designed with knowledge of its
users and requirements. It needs to drive adoption through utility, performance, and
demonstrated ability to both benefit Google’s production reliability goals and to bet‐