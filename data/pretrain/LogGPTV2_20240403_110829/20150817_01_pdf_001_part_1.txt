# Unsupervised Learning and Clustering: K-means and Gaussian Mixture Models

## Overview
- **Course:** Machine Learning – 10701/15781
- **Instructor:** Carlos Guestrin
- **Institution:** Carnegie Mellon University
- **Date:** April 4th, 2007
- **Copyright:** ©2005-2007 Carlos Guestrin

## Clustering Techniques
### K-means Algorithm
The K-means algorithm is a popular method for clustering data. Here are the steps involved:

1. **Initialization:**
   - Ask the user to specify the number of clusters (e.g., \( k = 5 \)).
   - Randomly initialize \( k \) cluster centers.

2. **Assignment:**
   - Each data point is assigned to the nearest cluster center.
   - This step effectively "owns" a set of data points for each cluster center.

3. **Update:**
   - Each cluster center is updated to the centroid (mean) of the data points it owns.
   - The cluster center "jumps" to this new position.

4. **Iteration:**
   - Repeat the assignment and update steps until convergence or a termination condition is met.

#### Mathematical Formulation
- **Random Initialization:**
  \[
  \mu^{(0)} = \{\mu_1^{(0)}, \mu_2^{(0)}, \ldots, \mu_k^{(0)}\}
  \]
- **Classification:**
  Assign each point \( j \in \{1, \ldots, m\} \) to the nearest center:
  \[
  C(j) = \arg\min_i \|x_j - \mu_i\|^2
  \]
- **Recentering:**
  Update each center to the centroid of its assigned points:
  \[
  \mu_i \leftarrow \frac{1}{|C(i)|} \sum_{j \in C(i)} x_j
  \]

### Optimization in K-means
- **Potential Function:**
  \[
  F(\mu, C) = \sum_{i=1}^k \sum_{j \in C(i)} \|x_j - \mu_i\|^2
  \]
- **Optimization:**
  \[
  \min_{\mu, C} F(\mu, C)
  \]
- **Convergence:**
  K-means is a coordinate descent algorithm, which alternates between optimizing \( \mu \) and \( C \). It converges to a local optimum if the potential function is bounded.

### Limitations of K-means
- **Cluster Overlap:** K-means can struggle with overlapping clusters.
- **Varying Cluster Widths:** K-means assumes clusters have similar sizes and shapes.

## Gaussian Mixture Models (GMM)
### GMM Assumptions
- **Components:**
  - There are \( k \) components.
  - Each component \( i \) has an associated mean vector \( \mu_i \).
  - Each component generates data from a Gaussian distribution with mean \( \mu_i \) and covariance matrix \( \Sigma_i \).

### Data Generation Process
1. **Component Selection:**
   - Pick a component at random with probability \( P(y = i) \).
2. **Data Point Generation:**
   - Generate a data point from \( N(\mu_i, \Sigma_i) \).

### Special Cases
- **Spherical Gaussians:**
  - If all components have the same spherical covariance \( \sigma^2 I \):
    \[
    P(x | y = i) \propto \exp \left( -\frac{1}{2\sigma^2} \|x - \mu_i\|^2 \right)
    \]
  - With hard assignments, this reduces to the K-means problem.
- **Soft Assignments:**
  - If class labels are uncertain, the marginal likelihood is:
    \[
    P(x_j) = \sum_{i=1}^k P(x_j | y = i) P(y = i)
    \]

### Marginal Likelihood
- **General Case:**
  \[
  P(x_j) = \sum_{i=1}^k P(x_j | y = i) P(y = i)
  \]
  \[
  P(x_j | y = i) \propto \exp \left( -\frac{1}{2} (x_j - \mu_i)^T \Sigma_i^{-1} (x_j - \mu_i) \right)
  \]

## Conclusion
Unsupervised learning, while challenging, can be effective with the right techniques. K-means and GMMs are powerful tools for clustering and density estimation, but they have limitations and assumptions that must be considered.

©2005-2007 Carlos Guestrin