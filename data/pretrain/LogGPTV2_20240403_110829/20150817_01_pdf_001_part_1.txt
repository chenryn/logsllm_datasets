Unsupervised learning or
Clustering –
K-means
Gaussian mixture models
Machine Learning – 10701/15781
Carlos Guestrin
Carnegie Mellon University
April 4th, 2007
©2005-2007 Carlos Guestrin
Some Data
©2005-2007 Carlos Guestrin
K-means
1. Ask user how many
clusters they’d like.
(e.g. k=5)
©2005-2007 Carlos Guestrin
K-means
1. Ask user how many
clusters they’d like.
(e.g. k=5)
2. Randomly guess k
cluster Center
locations
©2005-2007 Carlos Guestrin
K-means
1. Ask user how many
clusters they’d like.
(e.g. k=5)
2. Randomly guess k
cluster Center
locations
3. Each datapoint finds
out which Center it’s
closest to. (Thus
each Center “owns”
a set of datapoints)
©2005-2007 Carlos Guestrin
K-means
1. Ask user how many
clusters they’d like.
(e.g. k=5)
2. Randomly guess k
cluster Center
locations
3. Each datapoint finds
out which Center it’s
closest to.
4. Each Center finds
the centroid of the
points it owns
©2005-2007 Carlos Guestrin
K-means
1. Ask user how many
clusters they’d like.
(e.g. k=5)
2. Randomly guess k
cluster Center
locations
3. Each datapoint finds
out which Center it’s
closest to.
4. Each Center finds
the centroid of the
points it owns…
5. …and jumps there
6. …Repeat until
terminated!
©2005-2007 Carlos Guestrin
K-means
Randomly initialize k centers
 µ(0) = µ (0),…, µ (0)
1 k
Classify: Assign each point j∈{1,…m} to nearest
center:
Recenter: µ becomes centroid of its point:
i
 Equivalent to µ ← average of its points!
i
©2005-2007 Carlos Guestrin
What is K-means optimizing?
Potential function F(µ,C) of centers µ and point
allocations C:
Optimal K-means:
min min F(µ,C)
µ C
©2005-2007 Carlos Guestrin
Does K-means converge??? Part 1
Optimize potential function:
Fix µ, optimize C
©2005-2007 Carlos Guestrin
Does K-means converge??? Part 2
Optimize potential function:
Fix C, optimize µ
©2005-2007 Carlos Guestrin
Coordinate descent algorithms
Want: min min F(a,b)
a b
Coordinate descent:
fix a, minimize b
fix b, minimize a
repeat
Converges!!!
if F is bounded
to a (often good) local optimum
as we saw in applet (play with it!)
K-means is a coordinate descent algorithm!
©2005-2007 Carlos Guestrin
(One) bad case for k-means
Clusters may overlap
Some clusters may be
“wider” than others
©2005-2007 Carlos Guestrin
Gaussian Bayes Classifier
Reminder
p(x | y = i)P( y = i)
j
P( y = i | x ) =
j
p(x )
j
1 & 1 )
T
( %µ) $%1( %µ)
P(y = i | x ) " exp % x x P(y = i)
( +
j (2#)m/2 || $ ||1/2 ’ 2 j i i j i *
i
!
©2005-2007 Carlos Guestrin
Predicting wealth from age
©2005-2007 Carlos Guestrin
Predicting wealth from age
©2005-2007 Carlos Guestrin
Learning modelyear ,
$ 2 ’
# # #
1 L
12 1m
& )
2
# # #
& 2 L )
mpg ---> maker 12 2m
" =
& )
M M O M
& )
2
%# # L # m(
1m 2m
!
©2005-2007 Carlos Guestrin
General: O(m2)
$ 2 ’
# # #
1 L
12 1m
& )
2
# # #
& 2 L )
parameters 12 2m
" =
& )
M M O M
& )
2
%# # L # m(
1m 2m
!
©2005-2007 Carlos Guestrin
% # 2 0 0 0 0 (
1 L
Aligned: O(m) ’ *
0 #2 0 0 0
’ 2 L *
’ 0 0 #2 0 0 *
3 L
parameters " = ’ *
’ M M M O M M *
’ 0 0 0 #2 0 *
L m$1
’ *
0 0 0 0 #2
& L m)
!
©2005-2007 Carlos Guestrin
% # 2 0 0 0 0 (
1 L
Aligned: O(m) ’ *
0 #2 0 0 0
’ 2 L *
’ 0 0 #2 0 0 *
3 L
parameters " = ’ *
’ M M M O M M *
’ 0 0 0 #2 0 *
L m$1
’ *
0 0 0 0 #2
& L m)
!
©2005-2007 Carlos Guestrin
$ # 2 0 0 0 0 ’
L
Spherical: O(1) & )
0 #2 0 0 0
& L )
& 0 0 #2 0 0 )
L
cov parameters
" = & )
& M M M O M M )
& 0 0 0 #2 0 )
L
& )
0 0 0 0 #2
% L (
!
©2005-2007 Carlos Guestrin
$ # 2 0 0 0 0 ’
L
Spherical: O(1) & )
0 #2 0 0 0
& L )
& 0 0 #2 0 0 )
L
cov parameters
" = & )
& M M M O M M )
& 0 0 0 #2 0 )
L
& )
0 0 0 0 #2
% L (
!
©2005-2007 Carlos Guestrin
Next… back to Density Estimation
What if we want to do density estimation with
multimodal or clumpy data?
©2005-2007 Carlos Guestrin
But we don’t see class labels!!!
MLE:
 argmax ∏ P(y ,x )
j j j
But we don’t know y ’s!!!
j
Maximize marginal likelihood:
 argmax ∏ P(x ) = argmax ∏ ∑ k P(y =i,x )
j j j i=1 j j
©2005-2007 Carlos Guestrin
Special case: spherical Gaussians
and hard assignments
1 & 1 )
T
( %µ) $%1( %µ)
P(y = i | x ) " exp % x x P(y = i)
( +
j (2#)m/2 || $ ||1/2 ’ 2 j i i j i *
i
 If P(X|Y=i) is spherical, with same σ for all classes:
% 1 (
2
P(x | y = i) "exp # x #µ
! ’ *
j & 2$2 j i )
If each x belongs to one class C(j) (hard assignment), marginal likelihood:
j
!
m k m
’ 1 *
2
#" #
P(x ,y = i) $ exp % x %µ
) ,
j ( 2&2 j C( j) +
j=1 i=1 j=1
Same as K-means!!!
!
©2005-2007 Carlos Guestrin
The GMM assumption
• There are k components
• Component i has an associated
mean vector µ
µ
i
2
µ
1
µ
3
©2005-2007 Carlos Guestrin
The GMM assumption
• There are k components
• Component i has an associated
mean vector µ
µ
i
2
• Each component generates data
µ
1
from a Gaussian with mean µ and
i
covariance matrix σ2I
Each data point is generated µ
3
according to the following recipe:
©2005-2007 Carlos Guestrin
The GMM assumption
• There are k components
• Component i has an associated
mean vector µ
i µ
2
• Each component generates
data from a Gaussian with
mean µ and covariance matrix
i
σ2I
Each data point is generated
according to the following
recipe:
1. Pick a component at random:
Choose component i with
probability P(y=i)
©2005-2007 Carlos Guestrin
The GMM assumption
• There are k
components
• Component i has an associated
µ
mean vector µ 2
i
• Each component generates
x
data from a Gaussian with
mean µ and covariance matrix
i
σ2I
Each data point is generated
according to the following
recipe:
1. Pick a component at random:
Choose component i with
probability P(y=i)
2. Datapoint ~ N(µ, σ2I )
i
©2005-2007 Carlos Guestrin
The General GMM assumption
• There are k
components
• Component i has an associated
µ
mean vector µ 2
i
µ
1
• Each component generates
data from a Gaussian with
mean µ and covariance matrix
i
µ
Σ
3
i
Each data point is generated
according to the following
recipe:
1. Pick a component at random:
Choose component i with
probability P(y=i)
2. Datapoint ~ N(µ, Σ )
i i
©2005-2007 Carlos Guestrin
Unsupervised Learning:
not as hard as it looks
Sometimes easy
IN CASE YOU’RE
WONDERING WHAT
THESE DIAGRAMS ARE,
THEY SHOW 2-d
UNLABELED DATA (X
VECTORS)
Sometimes impossible
DISTRIBUTED IN 2-d
SPACE. THE TOP ONE
HAS THREE VERY
CLEAR GAUSSIAN
CENTERS
and sometimes in between
©2005-2007 Carlos Guestrin
Marginal likelihood for general case
1 & 1 )
T
( %µ) $%1( %µ)
P(y = i | x ) " exp % x x P(y = i)
( +
j (2#)m/2 || $ ||1/2 ’ 2 j i i j i *
i
Marginal likelihood:
m m k
! " P(x ) = "# P(x ,y = i)
j j
j=1 j=1 i=1
m k
1 ’ 1 *
T
"# ( ) &1( )
= exp & x &µ % x &µ P(y = i)
) ,
(2$)m/2 || % ||1/2 ( 2 j i i j i +
j=1 i=1 i
!
©2005-2007 Carlos Guestrin
Special case 2: spherical
Gaussians and soft assignments
 If P(X|Y=i) is spherical, with same σ for all classes:
% 1 (
2
P(x | y = i) "exp # x #µ
’ *
j & 2$2 j i )
Uncertain about class of each x (soft assignment), marginal
j
likelihood:
!
m k m k
’ 1 *
2
#" #"
P(x ,y = i) $ exp % x %µ P(y = i)
) ,
j ( 2&2 j i +
j=1 i=1 j=1 i=1
!
©2005-2007 Carlos Guestrin
Unsupervised Learning:
Mediumly Good News