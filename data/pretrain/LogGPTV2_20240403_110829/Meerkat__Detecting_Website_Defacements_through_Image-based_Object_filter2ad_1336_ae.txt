n
e
r
e
ﬀ
D
i
T
F
o
/
w
-
T
F
/
w
0.015
0.010
0.005
0.000
-0.005
-0.010
-0.015
January
February
March
April
May
Month of 2014
Figure 6: True positive and false positive rates, and the difference
with and without ﬁne-tuning, for the time-wise split. Note that the
scales for true positives and false negatives are the same, but that the
y-axis goes from 0.97 to 1 for the true positive rate and 0.01 to 0.04 for
the false positive rate. No signiﬁcant change is visible for the true positive
rate in the beginning regardless if the network is ﬁne-tuned regularly or
not, however, a non-negligible difference is observable for May 2014.
A difference is observable for the false positive rate starting in February
2014, after the network was ﬁrst ﬁne-tuned.
In detail, MEERKAT achieves a true positive rate between
98.310% and 98.816% when the system is ﬁne-tuned
after each month on the data observed in that month, and
97.603% to 98.606% when it is not. Although there is no
signiﬁcant difference in its accuracy from January to March
when the neural network is ﬁne-tuned and when it is not
(see Figure 6), a non-negligible difference between their
accuracy can be observed for April and the beginning of
May (increase in 0.452 percentage points (pp) and 1.211 pp
for the true positive rate; decrease of 1.513 pp and 1.550 pp
for the false positive rate). The Bayesian detection rate if no
ﬁne-tuning is used decreases from 98.583% in January 2014
to 97.666% in February (0.917 pp decrease) to 97.177%
in May (1.406 pp decrease to January).
If ﬁne-tuning
is utilized, the Bayesian detection rate increases from
98.583% in January 2014 to 98.717% in May (0.134 pp).
Unsurprisingly, the regularly ﬁne-tuned system performs
better over time, probably because some defacers became
signiﬁcantly more active in 2014, like Team System Dz, who
started to deface websites just in January 2014 and who
were not active before at all, and because some defacers
changed their defacements to spread a different message
as opposed to the one they spread the year before. When
the system is not ﬁne-tuned, however, these minor changes
to the defacements allow attackers to evade detection
without actively trying to evade it, with a minor accuracy
deterioration already visible after just four to ﬁve months,
conﬁrming that detection systems need to be able to tackle
even minor concept drift adequately and gracefully to
maintain accurate detection capabilities over time, like
MEERKAT does with ﬁne-tuning.
5 Limitations
Similar to other systems leveraging machine learning,
our system has some limitations that can be used to evade de-
tection. We discuss some of these limitations and show how
they can be addressed for a real-world deployment. First,
we discuss concept drift, a problem all systems leveraging
machine learning have to deal with; second, we remark on
browser ﬁngerprinting and delayed defacement, an issue
all client-based detection approaches have to address; and,
lastly, we introduce the concept of tiny defacements, a
limitation speciﬁc to defacement detection systems.
5.1 Concept Drift
Concept drift is the problem of predictive analysis ap-
proaches, such as detection systems, that the statistical prop-
erties of the input used to train the models change. In turn,
a direct result of concept drift is often a heavy deterioration
of the classiﬁcation performance, up to the point where the
system cannot differentiate between good and bad behavior
anymore. For instance, prior work [48–55] has shown that
concept drift (in the sense of adversarial learning) can actu-
ally be leveraged to evade detection systems and classiﬁers
in practice. Therefore, a detection system must address it.
While concept drift is a major issue for all systems using
machine learning, it can generally be addressed, due to its
nature, by adopting a new feature space or retraining the
machine learning model on new data, or with an increased
weight on new data. However, often, old instances do not
follow the statistical properties of the new feature models,
and, therefore, they are classiﬁed less accurately than before.
This has little impact in practice, because old instances are
less likely to occur in the future anyways; yet, it is important
to realize that this approach allows attackers to evade the
system by oscillating their attack strategy regularly.
For MEERKAT, those shortcomings can be addressed
more easily than for traditional systems: for minor concept
drift, the system’s accuracy can be maintained by ﬁne-tuning
the parameters of the network. Here, the system simply
needs to learn minor adjustments to the weights of existing
features from new data, because some features have become
more important and others have become less important (they
differ now more from other features than they did previously,
relatively speaking; since we start with already-initialized
weights, ﬁne-tuning requires much less time than training
the whole system again). Here, the features still model
the differences between defacements and legitimate
websites, however, the weights are not optimal anymore
and need to be adjusted. Once the new weights are learned,
classiﬁcation performance is restored. Therefore, to address
minor concept drift adequately, we recommend ﬁne-tuning
the model regularly, e.g., every month (see Section 4.5).
While ﬁne-tuning the system’s parameters can theoreti-
cally address major concept drift similar to retraining the sys-
tem on new data, in practice, we expect prediction accuracy
to decrease, since different or more features must be mod-
eled with the same amount of resources. Instead, for major
concept drift, increasing the number of hidden nodes of the
neural network that learn the compressed representation (the
features) and their weights, and then retraining the system
USENIX Association  
24th USENIX Security Symposium  605
11
Split
Traditional
Reporter (weighted)
Reporter (unweighted)
Time-wise with ﬁne-tuning
Time-wise without ﬁne-tuning
True Positive Rate
False Positive Rate
Bayesian Detection Rate
97.878%
97.882%
97.933%
1.012%
1.528%
1.546%
99.716%
99.571%
99.567%
98.310% - 98.816%
97.603% - 98.606%
1.233% - 1.413%
1.413% - 2.835%
98.583% - 98.767%
97.177% - 98.583%
Table 2: Average true positive, false positive, and Bayesian detection rates for traditional and reporter split. Lower and upper bound of true
positive, false positive, and Bayesian detection rate for time-wise split from January to May 2014.
can maintain the system’s accuracy. Simply adding nodes to
the hidden layers of the neural network can counteract the is-
sue of major concept drift because we increase the number of
features that MEERKAT learns from data directly. Therefore,
introducing more hidden units allows the system to learn ad-
ditional and different internal representations about the look
and feel of defacements, while, at the same time, maintain-
ing a model of how the old defacements look like. However,
it requires computationally-costly retraining of the network
(previously, having those additional hidden units in the net-
work would result in overﬁtting because the system would
learn more complex representations than necessary, and each
would only differ little from one another; the system would
then be prone to missing minor variations of defacements).
It is important to note that in both cases, for minor and
major concept drift, MEERKAT requires no additional
feature engineering because the features are learned
automatically from data. In turn, this allows MEERKAT
to handle any form of concept drift much more gracefully
than approaches introduced by prior approaches, which
require manual feature engineering.
5.2 Fingerprinting and Delayed Defacement
A second limitation of detection systems is ﬁngerprinting.
Since we are leveraging a web browser to collect the data
that we are analyzing, in our case ﬁngerprinting corresponds
to IP-based and browser ﬁngerprinting. For IP-based
ﬁngerprinting, a set of VPNs and proxies can be used to
cloak and regularly change the browser’s IP address. In case
of browser ﬁngerprinting, the server or some client-side
JavaScript code detects what browser is rendering the
website, and then displays the website differently for
different browsers. In its current form, the screenshot engine
from MEERKAT might be detectable (to some degree)
by browser ﬁngerprinting. It is theoretically possible to
detect it because it is currently built on the headless browser
PhantomJS rather than a “headful” browser typically used
by a normal user, like Google Chrome. However, since
PhantomJS is built from the same components as Google
Chrome, ﬁngerprinting the current screenshot engine is not
trivial and requires intimate knowledge of the differences
between the different versions of the components and
their interaction. Therefore, we argue that the evasion
through browser ﬁngerprinting is unlikely. If, however, the
screenshot engine is evaded this way in the future, only some
minor engineering effort is required to utilize a browser
extension to retrieve the websites’ screenshots instead.9
Additionally, the issue of delaying the defacement
emerges, also referred to as the snapshot problem [30]. With
the increased popularity and use of JavaScript, client-side
rendering, and asynchronous requests to backends by
websites to provide a seamless and “reload-free” user
experience, it is uncertain at what point in time a website
is representative of how a user would experience it. This
then bears the issue of when a detection system can take
a representative snapshot of the website and stop executing
client-side scripts. For instance, if a detection system takes
a snapshot always after ﬁve seconds, to evade detection,
defacers could simply inject JavaScript that only defaces
the website if a user interacts with it for at least six seconds.
While delayed defacements are currently scarce, it
is likely that they will gain some traction once more
detection systems have been put in place, in a way similar
to mimicry attacks and the evasions of malware detection
systems [56, 57]. However, prior work can be leveraged
to detect evasions [58] or trigger the functionality [59] to
force the defacement to be shown. Both approaches are
complementary to MEERKAT and we leave their adoption
to defacement detection for future work, once delayed
defacements are actually occurring in the wild.
5.3 Tiny Defacements
A third limitation of all current defacement detection
systems, including MEERKAT, is the lack of detection ca-
pabilities for tiny defacements. Tiny defacements describe
a class of defacements in which only a very minor modiﬁ-
cation is made to part of the content of the defaced website.
For instance, a defacer might be dissatisﬁed by an article
published by a newspaper. Instead of defacing the website
as a whole, the attacker modiﬁes (or deletes) the news article.
It is clear that such defacements are very hard to differen-
tiate from the original content because they might only have
minor semantic changes to text or images. Thus, to detect
tiny defacements, the detection system must understand the
semantics of the website’s content, its language, and its gen-
eral behavior to derive a meaningful model for the website.
they are
extremely scarce in numbers, or they are rarely noticed.
In fact, it is seldom the case that a defacer wants to modify
a website without embarrassing the operator more publicly.
Most often, the goal of the defacer is to expose the insecurity
However, while those defacements exist,
9In fact, we are migrating our screenshot engine to Chrome, eliminating
the problem that PhantomJS might be ﬁngerprinted.
606  24th USENIX Security Symposium 
USENIX Association
12
of the website, ridicule the operator publicly, show their
own “superiority,” and place their opinion and beliefs in the
most public space possible. Therefore, tiny defacements are
currently of little interest to the defacers themselves, and,
hence, also of little impact for detection systems. However,
we acknowledge that tiny defacements must be addressed
once they increase in numbers, possibly leveraging recent
work to extract relevant changes from websites [60], and
advances in natural language processing.
6 Related Work
Hereinafter, we discuss related work that detects deface-
ment, and image-based detection used in computer security.
6.1 Defacement Detection
Similar to MEERKAT, Davanzo et al. [46] introduce
a system that acts a monitoring service for website
defacements. Their system utilizes the website’s HTML
source code for detection, and its features were selected
manually based on domain knowledge acquired a priori,
making the system prone to concept drift. On their,
comparatively, very small dataset containing only 300
legitimate websites and 320 defacements, they achieve false
positive rates ranging from 3.56% to 100% (depending
on the machine learning algorithm used; suggesting
extreme under- and over-ﬁtting with some algorithms),
and true positive rates between 70.07% to 100% (in the
case of simply classifying everything as a defacement; i.e.,
extremely under-ﬁtting the dataset). Overall, these results
are signiﬁcantly worse than MEERKAT, both in terms of
false positives (1.012%) and true positives (97.878%).
Bartoli et al. [47] propose Goldrake, a website deface-
ment monitoring tool that is very similar to the tool proposed
by Davanzo et al. and leverages a superset of their features.
To learn an accurate model, Goldrake requires knowledge
about the monitored website to learn website-speciﬁc
parameters. However, it is unclear how well Goldrake
detects defacements in practice because it is evaluated on
a small and (likely) non-diverse dataset comprised of only
11 legitimate websites and 20 defacements, on which it
performs poorly with a high false negative rate (27%).
Medvet et al. [61] introduce a defacement detection
system based on work by Bartoli et al. and Davanzo et al.,
but the detection engine is replaced by a set of functions
that are learned through genetic programming. The learned
functions take the features by Bartoli et al. and Davanzo et
al. as input, but classiﬁcation is more accurate on a dataset
comprised of 15 websites (between 0.71% and 23.38% false
positives, and about 97.52% true positives). It is, again, un-
clear how the system would fare in a real-world deployment
because of the small and (likely) non-diverse dataset.
Note that all text-based approaches have major short-
comings, similar as those encountered in spam and phishing
detection, such as using images to show text to evade detec-
tion. MEERKAT does not suffer from these shortcomings.
Lastly, most commercial products that detect website
defacements are built upon host-based intrusion detection
systems to monitor modiﬁcations of the ﬁles on the web
server, e.g. via ﬁle integrity checks (checksums) [62, 63].
Therefore, those approaches bear the major shortcoming
that they can only detect the subset of defacements that
modify ﬁles on disk, and that they cannot detect other
defacement attacks, such as through SQL injections; even
when the defacements look exactly the same to the website’s
visitors. MEERKAT does not suffer from this shortcoming.
6.2 Image-based Detection in Security
Since, to the best of our knowledge, no prior work applies
image-based methods to detect defacements, we compare
prior work to defacement detection that visually detects
phishing pages, or leverages image-based techniques as
part of a larger system.
Medvet et al. [64] propose a system to detect if a potential
phishing page is similar to a legitimate website. The system
leverages features such as parts of the visible text, the images
embedded in the website, and the overall appearance of the
website as rendered by the browser for detection. Similarity
is measured by comparing the 2-dimensional Haar wavelet
transformations of the screenshots. Their system achieves
a 92.6% true positive rate and a 0% false positive rate on
a dataset comprised of 41 real-world phishing pages.
Similarly, Liu et al. [65] present an antiphishing solution
that is deployed at an email server and detects linked
phishing pages by assessing the visual similarity to the
legitimate page, but only when analysis is triggered on
keyword detection. The system detects phishing pages by
comparing the suspicious website to the legitimate website
by measuring similarity between text and image properties,
like the font size and family used, or source of an image.
While detecting phishing pages by comparing the similar-
ity of two websites makes sense, for defacements the differ-
ence between them is more interesting. Instead of creating a
visually-similar page to trick users into submitting their cre-
dentials, a defacer wants to promote his message. Adopting
existing phishing detection systems to detect defacements
instead, i.e., by comparing if the website looks different
from its usual representation, however, bears two problems:
(a) the usual representation must be known and/or stored for
comparison, and (b) false positives are much more likely if
the website is dynamic or if it shows regularly-changing ads.
Anderson et al. [29] introduce image shingling, a
technique similar to w-shingling, to cluster screenshots of
scams into campaigns. However, in its current form, image
shingling cannot be used to detect defacements as it is trivial
to evade the clustering step with only minor modiﬁcations
that are invisible to the human eye, and, thus, the technique is
unsuitable for a detection system in an adversarial context.10
Nappa et al. [66] leverage perceptual hashing to group
visually similar icons of malicious executables under
the assumption that a similar icon suggests that the two
executables are part of the same malware distribution
campaign. While it is theoretically possible to detect
defacements through perceptual hashing-based techniques
and comparing the distance of the hashes, it is impractical
to do so on a large scale and in an adversarial context. For
10The authors acknowledge the shortcomings in an adversarial context
in Section 4.2, but they do not discuss any remediation techniques.
13
USENIX Association  
24th USENIX Security Symposium  607
once, one must have a ground-truth screenshot that is close
enough to the screenshot that one wants to classify; if
ground-truth is not available or slightly too different, a
system based on perceptual hashing will be unable to detect
the defacement. Furthermore, classiﬁcation is not constant
in the number of defacements the system has seen in the
past: for each new screenshot we would want to classify,
we would need to compute the distance to the hashes of
at least some (or all) of the previously-seen defacements.11
Grier et al. [67] introduce their own image similarity
measure to cluster malicious executables that have similar
looking user-interface components after being executed in a
dynamic analysis environment. Two images are considered