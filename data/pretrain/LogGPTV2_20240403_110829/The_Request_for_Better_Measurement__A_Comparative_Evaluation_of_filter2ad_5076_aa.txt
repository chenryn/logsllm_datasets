# The Request for Better Measurement: A Comparative Evaluation of Two-Factor Authentication Schemes

**Authors:**  
Ding Wang†, Qianchen Gu†, Haibo Cheng†, Ping Wang†‡

**Affiliations:**  
†School of EECS, Peking University, Beijing 100871, China  
‡National Research Center for Software Engineering, Beijing 100871, China

**Emails:**  
{wangdingg, qcgu, chenghaibo, pwang}@pku.edu.cn

## Abstract
Despite over two decades of continuous efforts, designing a secure and efficient two-factor authentication (2FA) scheme remains an open challenge. Hundreds of new schemes have been proposed, but most fail to achieve important security goals (e.g., truly two-factor security) and desirable properties (e.g., user anonymity), leading to a cycle of "break-fix-break-fix."

In this paper, we use a series of "improved" schemes based on Xu et al.'s 2009 scheme as case studies to highlight that without better measurement, more "improvements" do not necessarily mean more advancements. We investigate the state-of-the-art evaluation criteria set (i.e., Madhusudhan-Mittal's set) and identify its ambiguities and redundancies, proposing viable fixes and refinements. To our knowledge, we are the first to demonstrate that there are at least seven different attack scenarios that can lead to the failure of a scheme in achieving truly two-factor security. Finally, we conduct a large-scale comparative evaluation of 34 representative 2FA schemes, underscoring the need for better measurement when assessing new schemes.

## Categories and Subject Descriptors
D.4.6 [Security and Protection]: Authentication

## General Terms
Security, Design, Theory, Metric

## Keywords
Two-factor authentication, Smart card loss attack, Two-factor security, De-synchronization attack, Measurement

## 1. Introduction
In 1991, Chang and Wu [7] introduced the first authentication scheme combining smart cards and passwords to protect security-critical services such as online banking, e-commerce, and e-health. Since then, numerous similar schemes have been developed [11, 31, 38]. The reliance on trusted devices, such as smart cards, has become more pronounced due to recent observations [60] that secure and usable leakage-resistant password systems cannot be achieved without them. The primary advantage of smart-card-based password authentication schemes is that they ensure two-factor security, meaning only a user with both a smart card and the correct password can successfully log in to the server. These schemes are generally termed "two-factor" schemes [50, 61].

However, recent research on side-channel attacks has revealed that smart cards, traditionally considered tamper-proof, can be compromised by determined attackers using power analysis [32], reverse engineering [6], and software attacks [27]. This means that secret data stored in the card can be extracted if an attacker gains access to it. Consequently, once the smart card is breached, traditional schemes [11, 31, 38] relying on the tamper-resistance assumption of smart cards cannot provide true two-factor security.

Given the ongoing arms race between attackers and security practitioners, even if smart cards are certified as tamper-proof at the time of production (e.g., FIPS-201 [40] and ETSI-TS-102 [13]), they may no longer be secure after a few years. For example, Zhou et al. [64] showed that several versions of commercial GSM SIM cards, which were secure against side-channel attacks in 2002, became vulnerable within eight years. Therefore, it is crucial to design 2FA schemes under the more realistic assumption that smart cards can be compromised when acquired by an attacker.

This assumption does not imply that smart cards are fully non-tamper-proof; rather, it means they are conditionally non-tamper-proof—only vulnerable to side-channel attacks if the attacker has sufficient time (e.g., a few hours). In cases where a user inserts their smart card into a malware-infected reader, the password may leak, but the sensitive data in the card remains secure because the user is present, preventing a side-channel attack.

As shown in Fig. 2, all 19 schemes underlined by a solid line cannot achieve true two-factor security, while the 11 schemes underlined by a dotted line fail to provide forward secrecy. It is both theoretically and practically important to determine whether secure and efficient 2FA schemes can be constructed under this more realistic yet challenging assumption about smart cards. Unfortunately, past research indicates that achieving true two-factor security under this new assumption is extremely difficult.

To date, we have analyzed over 200 2FA schemes, each claimed to be an "improvement" over existing problematic ones. However, these "improved" versions often do not justify their claims of enhanced security. Using the improvements over Xu et al.'s seminal scheme [59] as case studies (see Fig. 2), we reveal the unsatisfactory situation in this research area. In 2009, Xu et al. [59] proposed a 2FA scheme with a formal security proof in the random oracle model, claiming it could achieve true two-factor security. However, Song [45] and Sood et al. [46] independently demonstrated that the scheme fails to meet this goal.

Subsequently, a dozen rebuttals and improvements [19, 20, 28, 41, 43] have been presented. Among them, four improvements [28, 24, 43, 41] are representative of certain failures. Four of these improvements [39, 43, 56, 58] also include formal proofs using the random oracle model, BAN logic, and/or π-calculus. We show that most of these 19 "improvements" still fail to achieve the claimed goal of two-factor security and are often subject to other security defects like lack of forward secrecy and desynchronization attacks.

The unsatisfactory situation regarding Xu et al.'s scheme [59] is not an isolated incident. As shown in Fig. 1 of [51], the history of this research area is a monotonous rhythm of "break-fix-break-fix." This is largely due to insufficient measurement of new and existing schemes. Protocol designers often present attacks on a previous scheme and propose an improved scheme, comparing it with a few problematic ones but overlooking dimensions where the new scheme fails. Consequently, every paper concludes that the new scheme outperforms existing ones, but in reality, the new scheme may only achieve some goals and be no better than the original.

We trace the root cause of this failure to the current unworkable evaluation criteria, leading protocol designers to use their own customized criteria. In Section 2.3, we identify ambiguities and redundancies in the state-of-the-art criteria set proposed by Madhusudhan and Mittal in 2012 [36], and suggest countermeasures and refinements, testing their effectiveness.

## 2. System Model, Adversary Model, and Evaluation Metric

### 2.1 System Architecture
In this work, we focus on smart-card-based password authentication for the single-server architecture, the most general case of 2FA (see Fig. 1). The protocol participants include a set of users and a single authentication server. Typically, there are three basic phases: registration, authentication, and password change, with supplementary phases like re-registration and revocation [42, 57].

During the registration phase, a user U provides personal data (e.g., identity and password transformation) to the server S, which personalizes a smart card with public and sensitive security parameters and issues the card to U. This phase is conducted once until the card expires. After registration, U can log in to S through the authentication phase, proving ownership of both a valid smart card and the correct password. The password change phase allows U to update her password either locally or by interacting with S.

### 2.2 Adversary Model
We adopt the adversary model introduced in [50], summarizing the capabilities of the adversary A in Table 1. Capability C-01 means that both the user identity space \( D_{id} \) and the password space \( D_{pw} \) are finite and can be enumerated efficiently. Recent large-scale leaks of user-chosen passwords reveal that \( |D_{pw}| \) is generally about \( 2^{20} \). User identities are static and often obtained from public sources, so \( |D_{id}| \leq |D_{pw}| \approx 2^{20} \approx 10^6 \).

Capability C-02 emphasizes that user identity should not be considered a secret value, and the system's security should not rely on the secrecy of user identity. This does not contradict the assumption that a target user's identity is sensitive. When dealing with privacy, the target user's identity is sensitive, and the adversary A aims to determine it from publicly available protocol transcripts. An anonymous 2FA scheme ensures that A cannot determine a user's identity from the public protocol transcripts, even if A uses non-cryptographic techniques (e.g., social engineering and keyloggers [63]).

Capability C-1 is the canonical assumption about adversaries in distributed computing. Capability C-2 is the key difference between a 2FA security model and a password-only authentication model, capturing the notion of two-factor security. The last two capabilities deal with session key attacks: C-3 models known-key attacks, and C-4 captures the notion of forward secrecy.

These six assumptions about adversary capabilities are reasonable and have been accepted since the seminal work of Yang et al. [61]. Most recent schemes [18, 28, 41, 42] explicitly state capabilities C-1 and C-2, while the others are implicitly assumed.

### 2.3 Evaluation Criteria
A concrete, concise, and comprehensive evaluation criteria set is essential for a fair assessment of 2FA schemes. Several criteria sets have been suggested (e.g., [31, 57, 61]). In 2012, Madhusudhan and Mittal [36] demonstrated that earlier metrics had ambiguities and redundancies, presenting a new criteria set consisting of nine security requirements (Table 2) and ten desirable properties (Table 3) for better assessment of 2FA schemes.

After careful examination, we find that Madhusudhan and Mittal's set [36] still has several ambiguities and redundancies. Wang et al. [50] identified two ambiguous attributes: (1) DA1 (no password-related verifier table) should be split into DA1-Weak and DA1-Strong, with the former requiring no user-specific data on the server and the latter allowing non-security-critical user-specific information; (2) DA2 (freely user password choice) should be split into DA2-Local-Insecure, DA2-Local-Secure, and DA2-Interactive, depending on where user passwords can be changed locally and securely. They also clarified a subtlety regarding SR6 and other security requirements: SR6 deals with an attacker who has gained the victim's smart card, while other requirements deal with an attacker without the smart card. These clarifications make the set more concrete.

We observe that two other design goals need further clarification: DA8 (user anonymity) and SR6 (resistance to smart card loss attack). Specifically, DA8 should be split into DA8-Weak and DA8-Strong, reflecting different notions of user anonymity. The basic notion is user anonymity, while the stronger notion ensures that even with multiple interactions, the user's identity remains hidden.

## 3. Contributions
The contributions of this paper are three-fold:
1. **Case Studies and Measurement**: We use 19 improvements over Xu et al.'s 2009 scheme as case studies to illustrate the lack of fair and thorough measurement in 2FA research. Our results show that none of these improved schemes yield better security, and some are even less secure than the original. We also identify fundamental flaws in the reasoning of formal proofs for some "provably secure" schemes.
2. **Root Cause Analysis and Fixes**: We trace the root cause of the current failure to the unworkable evaluation criteria, demonstrating ambiguities and redundancies in the state-of-the-art set proposed by Madhusudhan and Mittal [36]. We propose viable fixes and refinements, showing for the first time that there are at least eight different smart card loss attack strategies, seven of which would prevent a scheme from achieving true two-factor security. This provides an in-depth understanding of how to measure 2FA security and helps protocol designers be aware of potential threats.
3. **Large-Scale Comparative Evaluation**: We provide a large-scale comparative evaluation of 34 representative 2FA schemes based on our refinements of Madhusudhan-Mittal's evaluation set. This provides missing measurements and a better understanding of existing schemes, highlighting the difficulties in designing practical 2FA schemes.

## 4. Conclusion
In conclusion, the design of secure and efficient 2FA schemes remains a significant challenge. Our study highlights the need for better measurement and evaluation criteria to ensure that new schemes genuinely improve upon existing ones. By refining the evaluation criteria and conducting a large-scale comparative evaluation, we aim to advance the field and provide a more robust framework for future research.

## References
[References listed here]

---

This version of the text is more structured, coherent, and professional, with clearer headings and a more polished narrative.