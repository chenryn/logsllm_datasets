title:The Request for Better Measurement: A Comparative Evaluation of
Two-Factor Authentication Schemes
author:Ding Wang and
Qianchen Gu and
Haibo Cheng and
Ping Wang
The Request for Better Measurement: A Comparative
Evaluation of Two-Factor Authentication Schemes
Ding Wang†, Qianchen Gu†, Haibo Cheng†, Ping Wang†‡
†School of EECS, Peking University, Beijing 100871, China
‡National Research Center for Software Engineering, Beijing 100871, China
{wangdingg,qcgu,chenghaibo,pwang}@pku.edu.cn
ABSTRACT
Despite over two decades of continuous eﬀorts, how to de-
sign a secure and eﬃcient two-factor authentication scheme
remains an open issue. Hundreds of new schemes have wave
upon wave been proposed, yet most of them are shortly
found unable to achieve some important security goals (e.g.,
truly two-factor security) and desirable properties (e.g.,
user anonymity), falling into the unsatisfactory “break-ﬁx-
break-ﬁx” cycle.
In this vicious cycle, protocol designers
often advocate the superiorities of their improved scheme,
but do not illustrate (or unconsciously overlooking) the
aspects on which their scheme performs poorly.
In this paper, we ﬁrst use a series of “improved schemes”
over Xu et al.’s 2009 scheme as case studies to high-
light that, if there are no improved measurements, more
“improved schemes” generally would not mean more ad-
vancements. To ﬁgure out why the measurement of ex-
isting schemes is invariably insuﬃcient, we further in-
vestigate into the state-of-the-art evaluation criteria set
(i.e., Madhusudhan-Mittal’s set). Besides reporting its
ambiguities and redundancies, we propose viable ﬁxes and
reﬁnements. To the best of our knowledge, we for the ﬁrst
time demonstrate that there are at least seven diﬀerent
attacking scenarios that may lead to the failure of a scheme
in achieving truly two-factor security. Finally, we conduct
a large-scale comparative evaluation of 34 representative
two-factor schemes, and our results outline the request for
better measurement when assessing new schemes.
Categories and Subject Descriptors
D.4.6 [Security and Protection]: Authentication
General Terms
Security, Design, Theory, Metric
Keywords
Two-factor authentication; Smart card loss attack; Two-
factor security; De-synchronization attack; Measurement.
1.
INTRODUCTION
Back to 1991, Chang and Wu [7] proposed the ﬁrst
authentication scheme that combines smart cards and pass-
words to protect security-critical services such as online
banking, e-commerce and e-health. Since then there have
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ASIA CCS’16, May 30-June 04, 2016, Xian, China.
Copyright 2016 ACM 001-1-0001-0001-01/01/01. ...$15.00.
Fig. 1: Smart-card-based password authentication
been a number of this sort of schemes developed [11,31,38].
Their status is further entrenched by the recent observation
[60] that secure and usable leakage-resistance password
systems cannot be achieved if we do not resort to trusted
devices. The most prominent merit of smart-card-based
password authentication schemes (see Fig. 1) is that two-
factor security can be assured,
i.e., only the user who
possesses a smart card and knows the correct password
can successfully login the server. Thus, they are generally
termed “two-factor” schemes [50, 61]. A common feature of
these early two-factor schemes is that their security largely
relies on the tamper-proof property of the smart cards.
However, recent research results on side-channel attacks
reveal that smart cards, which are traditionally considered
as fully tamper-proof devices, can no longer be fully trusted
once in the hands of a determined attacker. They can
be tampered by power analysis [32], reverse engineering
techniques [6], and software attacks (launched on software-
supported card, e.g., Java Card [27]). This means that, the
secret data kept in the card memory can now be extracted
when an determined attacker somehow gets access to the
smart card. Accordingly, once the smart card factor
is breached, these traditional schemes [11, 31, 38] whose
security relies on the tamper resistance assumption of smart
cards cannot provide truly two-factor security any more.
As there is a constant arms race going on between
attackers and security practitioners, even if the physical
security of smart cards has been assessed by independent
laboratories or certiﬁed by third-party authorities (e.g.,
FIPS-201 [40] and ETSI-TS-102 [13]) at the time of their
production, it is highly likely that they will be no longer
tamper-proof after a few years of circulation. At FC’13,
Zhou et al. [64] provided such a typical example — several
versions of commercial GSM SIM cards (with a 16-bit CPU
and the COMP128-1 algorithm) are secure against the side-
channel attacks in 2002, yet eight years later they are found
vulnerable to state-of-the-art side-channel attacks and can
be breached in just a few minutes. Considering the long
term nature of hardware deployment (because of the cost
involved when upgrading), it is important to take adequate
preventive measures early in cryptographic developments.
Consequently, it is more admired to design two-factor
schemes that rely on the more realistic assumption —
smart cards can be extracted when acquired (or temporarily
acquired) by an attacker. Here is a subtlety to be noted:
this assumption does not necessarily suggest smart cards
are fully non-tamper-proof devices; Instead, it means that
smart cards are only conditionally non-tamper-proof —
their physical security can be violated only when they
are in the hands of the attacker for a suﬃciently long
period of time (e.g., a few hours [27,32]) for performing the
(professional and relatively time-consuming) side-channel
attacks. In-depth investigations of this subtlety are referred
to [51, 53]. This indicates that, in cases when the user
inserts her smart card into a malware-infected card reader,
her password may leak yet the sensitive data stored in the
card still remain secure, for the user is on the spot and there
is no chance for a side-channel attack to be launched.
Fig. 2: The evolution of Xu et al.’s 2009 scheme.
As we will show in this work, all the 19 schemes
underlined by a solid line cannot achieve truly two-
factor security, while the 11 schemes underlined by
a dotted line fail to provide forward secrecy.
It is of both theoretical and practical interest to see
whether secure and eﬃcient two-factor schemes can be con-
structed under this more realistic yet challenging assump-
tion about smart cards. A number of attempts [24, 48, 55,
59,61] have been made. Unfortunately, past research proves
that achieving truly two-factor security under this new
assumption (i.e., the conditionally non-tamper-resistance
assumption about smart cards) is extremely diﬃcult. The
pattern of research progress in this area has been of
suggested solutions (e.g., [10, 16, 53, 59, 65]), followed by
cryptanalysis and improvements (e.g., [28,41,45,48,54,55]).
Many broken schemes (see one of the most inﬂuential one
in [59]) are even equipped with a formal proof.
To date we have analyzed over two hundred two-factor
authentication schemes, each of which is claimed to be “an
improvement” over existing problematic ones. However,
whether these “improved versions” indeed improve security
is often not well justiﬁed. Lots of eﬀorts have been devoted,
yet little progress has been made. In this work, we use the
improvements over Xu et al.’s seminal scheme [59] as case
studies (see Fig. 2) and reveal the unsatisfactory situation
of this research pattern. In 2009, Xu et al. [59] for the ﬁrst
time proposed a two-factor scheme with a formal security
proof in the random oracle model, and claimed that their
scheme can achieve truly two-factor security. However,
Song [45] and Sood et al. [46] independently showed that
Xu et al.’s scheme cannot fulﬁll this goal.
A dozen of rebuttals and improvements (e.g., [19, 20,
28, 41, 43] following Xu et al.’s scheme [59] have been
presented in succession (see Fig. 2). Among them, four
improvements which are representative of certain failures
in 2013 [28], Kumari-Khan in
are given by Li et al.
2014 [24], Odelu et al.
in 2015 [43] and Muhaya in 2015
[41], respectively. Particularly, four improvements (i.e.,
[39, 43, 56, 58]) are also equipped with a formal proof using
the random oracle model, BAN logic and/or π-calculus. In
this paper, we show that most of these 19 “improvements”
still cannot attain the claimed goal of two-factor security,
which is the most crucial goal that a two-factor scheme
is designed to achieve. Besides, they are often subject to
some other security defects like no forward secrecy and de-
synchronization attack.
The unsatisfactory situation regarding Xu et al.’s scheme
[59] is by no means an accident. As shown in Fig. 1 of [51],
the history of this research area is a monotonous rhythm
of “break-ﬁx-break-ﬁx”. We believe this is largely due to
the insuﬃcient measurement of new and existing schemes.
In most cases, the protocol designers present attacks on a
previous scheme and propose an improved scheme. Then,
they compare it with two or three previous, problematic
ones by merely focusing on the dimensions that previous
schemes fare poorly, often overlooking the dimensions that
previous schemes perform well but the new scheme fails.
Invariably, every paper ends up by a conclusion that the
new scheme proposed outperforms existing ones. However,
the reality is often that the new scheme only achieves some
goals and may be no better than the original, problematic
one (see Fig. 2 for concrete examples).
We have traced the root cause of the failure in lacking
of a sound measurement: the current evaluation criteria
are not workable (operable) and hence protocol designers
choose to (or have to) use their own customized criteria. In
Section 2.3, we show the ambiguities and redundancies in
the state-of-the-art criteria set proposed by Madhusudhan
and Mittal in 2012 [36]. Besides reporting its deﬁciencies,
we also suggest countermeasures and reﬁnements and test
the eﬀectiveness of our suggestions.
The contributions of this paper are three-fold:
• We employ 19 improvements over Xu et al.’s 2009
scheme as case studies to illustrate the lack of fair,
thorough measurement of existing schemes in the
two-factor authentication research. Our results show
that, none of these improved schemes yield improved
security and some are even less secure as compared to
Xu et al.’s original scheme proposed six years ago. We
also identify the fundamental ﬂaws in the reasoning
of formal proofs for some “provably secure” schemes.
• We trace the root cause of the current failure by
demonstrating that some criteria in the state-of-the-
art evaluation set are unworkable due to a number
of ambiguities. We further propose viable ﬁxes and
reﬁnements. Particularly, for the ﬁrst time, we show
that there are at least eight diﬀerent smart-card-loss-
attack strategies, seven of which would make a scheme
unable to achieve truly two-factor security. This
not only provides an in-depth understanding of how
to measure (two-factor) security but also facilitates
protocol designers to be aware of potential threats.
• We provide a large-scale comparative evaluation of
34 representative two-factor schemes based on our
reﬁnements of Madhusudhan-Mittal’s evaluation set.
This provides the missing measurements and thus
presents a better understanding of existing schemes.
Our measurement results highlight the diﬃculties in
designing a practical two-factor scheme.
2. SYSTEM MODEL, ADVERSARY MOD-
EL AND EVALUATION METRIC
In this section, we brieﬂy sketch the system architecture
and adversarial model and reveal the insuﬃciencies in the
state-of-the-art evaluation criteria set that was proposed
by Madhusudhan and Mittal in 2012 [36]. As noted in
[50], there have been abundant papers dealing with two-
[10, 22, 29, 47,
factor authentication quite recently (e.g.,
48, 53, 62]), yet to the best of knowledge, only a few of
2009Xuetal.[59]2010Song[45]2012Chenetal.[10]2010Soodetal.[46]2013Lietal.[28](provablesecurity)2013MartinEz-Pelaezetal.[37]2014Kumari-Khan[24]2014Xieetal.[58](provablesecurity)2011Heetal.[16]2012Weietal.[54]2012Zhuetal.[65]2015Muhaya[41]2015Wuetal.[56]2015Jiangetal.[20]2015Mishraetal.[39]2016Weietal.[55]2015Islam[19]2015Odeluetal.[43]2015Luetal.[33]2015Lee-Liu[12](provablesecurity)(provablesecurity)(provablesecurity) them [17, 20, 51, 53] explicate the system architecture and
adversarial model which are basic factors in assessing the
the security provisions of a scheme. Most ones [42,48,53] do
explicate the evaluation metric, yet their metrics are often
self-made and far from systematic and mature, though there
do exist several evaluation metrics proposed for this use.
This outlines the need for an investigation into the reason
why these existing metrics are not preferred when protocol
designers evaluating their new schemes.
2.1 System architecture
1).
i.e.
In this work, as with [17, 50], we mainly deal with
smart-card-based password authentication for the single
server architecture, which is the most general case of
two-factor authentication (see Fig.
In this sort of
schemes, the protocol participants include a set of users
and a single authentication server, and typically there are
three basic phases,
registration, authentication and
password change, and may also be some supplementary
phases like re-registration and revocation [42, 57].
In the
registration phase, a user U provides her personal data (e.g.,
identity and transformation of password) to the server S,
S personalizes a smart card with some public and sensitive
security parameters and issues the card to U . Generally,
this phases is conducted only once until the card expires.
After completion of this phase, U can login S through the
authentication phase. Only the user who can prove that
she owns both a valid smart card and the correct password
can successfully pass the veriﬁcation of the server S. Lack
of either authentication factor would lead to a login failure.
In the password change phase, U can update her password
either locally or by interacting with S. More sophisticated
schemes may also have additional phases that enable the
system to evict a malicious user and revoke a lost card.
2.2 Adversarial model
We adopt the adversarial model that is introduced in [50],
and the capabilities of the adversary A are summarized
in Table 1. The capability C-01 means that both the
user identity space Did and the password space Dpw are
ﬁnite and can be enumerated eﬃciently. Recent large-
scale leakages of user-chosen passwords reveal that |Dpw| is
generally about 220 [4,35]. As for user’s identity, it is static
and often conﬁned to a predeﬁned format and obtained
from public sources, and thus it is reasonable to assume
that |Did| ≤ |Dpw| ≈ 220 ≈ 106.
The capability C-02 is splitted from C-01 to emphasize
that when evaluating the security goals of a scheme, user
identity shall be not considered as a secret value and the
security of the system shall not builds on the secrecy of
user identity. This, however, does not contradict with the
widely hold assumption that the target user’s identity is
sensitive. When dealing with the privacy provisions of
a scheme, the target user’s identity is sensitive and just
what A attempts to ﬁgure out from the publicly available
protocol transcripts. What a anonymous two-factor scheme
can assure is that, from the public protocol transcripts, A
shall not be able to determine a user’s identity. This does
not contradicts with the fact that A determines a user’s
identity by using non-cryptographic techniques (e.g., social
engineering and key logger [63]).
The capability C-1 is the canonical assumption about
adversaries in distribution computing. The capability C-
2 is the key diﬀerence between a security model for two-
factor authentication and a security model for password-
only authentication. It facilitates to capture the notion of
two-factor security and is indeed reasonable according to
the recent advances in side-channel attacks [6, 27, 32]. The
last two capabilities deal with attacks regarding session key:
Table 1: Capabilities of the adversary
The adversary A can enumerate oﬄine all the items in
the Cartesian product Did∗Dpw within polynomial time,
where Dpw and Did denote the password space and the
identity space, respectively.
The adversary A has the capability of somehow learning
the victim’s identity when evaluating security strength
(but not privacy provisions) of the protocol.
The adversary A is in full control of the communication
channel between the protocol participants.
The adversary A may either (i) learn the password of a
legitimate user via malicious card reader, or (ii) extract
the sensitive parameters in the card memory by side-
channel attacks, but cannot achieve both.
The adversary A can learn the previous session key(s).
The adversary A is able to learn the server’s long-time
private key(s) only when evaluating the resistance to
eventual failure of the system (e.g., forward secrecy).
C-01
C-02
C-1
C-2
C-3
C-4
C-3 is used to model know-key attacks, while C-4 is used
to capture the notion of forward secrecy.
In all, these six assumptions about adversary capabilities
are indeed reasonable and have been little by little accepted
(see [14, 17, 36, 42, 53]) since the seminal work of Yang et
al. [61]. We note that in most recent schemes (see [18,
28, 41, 42]), the adversarial capabilities C-1 and C-2 have
been explicitly stated, while the other four assumptions are
invariably implicitly made. In the following sections, our
analysis will be based on these six assumptions.
2.3 Evaluation criteria
A concrete, concise and comprehensive evaluation criteria
set is essential for a fair assessment of the goodness of
existing schemes. A number of criteria sets regarding
two-factor authentication have been suggested (e.g., [31,
57, 61]. However, in 2012 Madhusudhan and Mittal [36]
demonstrated that these earlier metrics have ambiguities
and redundancies, and they presented a new criteria set
consisting of nine security requirements (see Table 2) and
ten desirable properties (see Table 3) for better assessment
of two-factor schemes.
It is more comprehensive and
workable as compared to existing criteria sets, yet as far
as we know, during the past few years it has never been
adopted to measure schemes by other researchers.
After careful examination, we ﬁnd that there are still
several ambiguities and redundancies that hinder the eﬀec-
tiveness of Madhusudhan and Mittal’s set [36]. Very recent-
ly, Wang et al. [50] explicated two ambiguous attributes
that dwell in this set: (1) DA1(i.e., no password-related
veriﬁer table) shall be splitted into DA1-Weak and DA1-
Strong, with the former notion requiring that “no other
user-speciﬁc data is stored on the server” and the latter
notion requiring that “only some non-security-critical user-
speciﬁc information can be stored on the server”; and (2)
DA2 (i.e., freely user password choice) shall be splitted into
DA2-Local- Insecure, DA2-Local-Secure and DA2-Interactive,
according to where user passwords can be changed locally
and securely (i.e., with veriﬁcation). They further explicate
a subtlety regarding SR6 and other security requirements
(e.g., SR2 and SR4): SR6 relates to an attacker who
has gained the victim’s smart card, while all other secu-
rity requirements deal with an attacker who is without
the victim’s smart card. These three explications make
Madhusudhan-Mittal’s set more concrete and constitute “a
ﬁrst step towards understanding the underlying evaluation
metric” [50]. We observe that, there are two other design
goals that need to be further explicated, i.e. DA8: user
anonymity and SR6: resistance to smart card loss attack.
More speciﬁcally, DA8 shall be splitted into DA8-Weak
and DA8-Strong, for there are two vastly diﬀerent notions
[29] regarding user anonymity. The basic notion is user
Table 2: Security requirements
SR1 Resistance to DoS attack
SR2 Resistance to impersonation attack
SR3 Resistance to parallel session attack
SR4 Resistance to password guessing attack
SR5 Resistance to replay attack
SR6 Resistance to smart card loss attack;
SR7 Resistance to stolen-veriﬁer attack
SR8 Resistance to reﬂection attack
SR9 Resistance to insider attack
Table 3: Desirable attributes
No password-related veriﬁer table
Freely user password choice
No password reveal
Password dependent
DA1
DA2
DA3
DA4
DA5 Mutual authentication
DA6
Session key agreement
DA7
Forward secrecy
DA8
User anonymity
DA9
Smart card revocation
DA10 Eﬃciency for wrong password login
Smart-card-loss Need to Ex- Need to return Num of online
Table 4: A taxonomy of smart-card-loss attacks (PW stands for password)
Design goals
Weaknesses exploited
Typical reference
attack types
Type I
Type II
Type III
Type IV
Passive
Type V