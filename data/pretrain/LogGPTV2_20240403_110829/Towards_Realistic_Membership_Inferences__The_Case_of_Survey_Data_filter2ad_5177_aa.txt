# Title: Towards Realistic Membership Inference: The Case of Survey Data

**Authors:**
- Luke A. Bauer
- Vincent Bindschaedler

**Publication:**
- Proceedings of Machine Learning Research – Under Review: 1–24, 2021
- Full Paper – MIDL 2021 submission

# Title: Membership Inference Attacks on Deep Regression Models for Neuroimaging

**Authors:**
- Umang Gupta¹
- Dmitris Stripelis¹
- Pradeep K. Lam²
- Paul M. Thompson²
- José Luis Ambite¹
- Greg Ver Steeg¹

**Affiliations:**
- ¹Information Sciences Institute, University of Southern California
- ²Imaging Genetics Center, Mark and Mary Stevens Institute for Neuroimaging and Informatics, Keck School of Medicine, University of Southern California

**Editors:**
- Under Review for MIDL 2021

## Abstract

Ensuring the privacy of research participants is crucial, especially in healthcare environments. Deep learning approaches to neuroimaging require large datasets, often necessitating data sharing between multiple sites, which can conflict with privacy objectives. Federated learning is a commonly proposed solution, as it avoids direct data sharing by sharing parameters during training. However, we demonstrate that even this approach can leak private information. Specifically, we show that it is possible to infer whether a sample was used in training, given access to the model's predictions (black-box) or the model itself (white-box), along with some leaked samples from the training data distribution. These attacks are known as Membership Inference attacks. We present realistic Membership Inference attacks on deep learning models trained for 3D neuroimaging tasks in both centralized and decentralized setups. Our results show that we can correctly identify whether an MRI scan was used in model training with success rates ranging from 60% to over 80%, depending on model complexity and security assumptions.

## 1. Introduction

The increasing demand for data in machine learning is often at odds with the need for data privacy. This is particularly significant in medical research fields such as neuroimaging, where information leakage can have legal implications and severe consequences for individuals' quality of life. Laws like the Health Insurance Portability and Accountability Act (HIPAA) protect health records and data collected for medical research. Privacy concerns have driven the development of anonymization algorithms, such as those that remove facial information from MRI scans.

While there are laws and guidelines to control the sharing of private data, model sharing or using models trained on private data can also leak information. Even summary statistics can pose risks to participants' privacy, as demonstrated in genome-wide association studies. A neural network model trained on private data can be seen as a summary statistic, and private information may be extracted from it. To demonstrate the feasibility of information leakage, we study the problem of extracting information about individuals from a model trained on the brain age prediction regression task using neuroimaging data. Brain age, an estimate of a person's age from their brain MRI scan, is a common benchmark for machine learning algorithms.

We focus on Membership Inference attacks, where an adversary infers which samples were used to train the model. Such attacks can reveal sensitive information, such as whether an individual participated in a specific study, potentially leading to undesirable consequences. Previous work on membership inference attacks has focused on supervised classification problems, often exploiting the model's overconfidence on the training set and the high dimensionality of the probability vector. Our work extends this to a regression model trained to predict a person's age from their brain MRI scan under both white-box and black-box setups. We simulate attacks on models trained via centralized and federated setups and demonstrate a strong empirical connection between overfitting and vulnerability to membership inference attacks.

## 2. Related Work & Background

### 2.1. BrainAGE Problem

Brain age is an estimate of a person's age from a structural MRI scan of their brain. The difference between a person's true chronological age and the predicted age is a useful biomarker for early detection of various neurological diseases. The problem of estimating this difference is known as the Brain Age Gap Estimation (BrainAGE) problem. Brain age prediction models are trained on brain MRIs of healthy subjects to predict the chronological age. A higher gap between predicted and chronological age is often considered an indicator of accelerated aging, which may be a prodrome for neurological diseases. Recent studies have proposed using deep learning to predict age from raw 3D-MRI scans. We employ trained networks and training setups recently proposed in the literature to simulate attacks on centrally and distributively trained models.

### 2.2. Federated Learning

In traditional machine learning pipelines, data from multiple sources must be aggregated at a central repository for processing and analysis. This aggregation step can incur privacy vulnerabilities or violate regulatory constraints, making data sharing across organizations challenging. Federated Learning was proposed as a distributed machine learning paradigm that allows institutions to collaboratively train models without sharing private data. Instead, each data source trains the model locally and shares the updated parameters. Federated Learning is increasingly applied in biomedical and healthcare domains due to its inherent privacy advantages.

Federated learning topologies can vary based on communication characteristics, with star and peer-to-peer being the most prominent. In a star topology, a trusted centralized entity coordinates the training process, while in a peer-to-peer topology, participating sites communicate directly. We focus on the star federated learning topology. In this setup, each participating data source receives the community model, trains it independently, and sends the updated parameters back to the controller. The controller aggregates these parameters and updates the community model, repeating this process until the final model is produced.

### 2.3. Membership Inference Attacks

Membership Inference attacks aim to infer whether a record (e.g., a person's MRI scan) was used to train a model. These attacks can reveal censored personal information and have legal implications. Most successful membership inference attacks in the deep neural network literature require access to some parts of the training data or samples from the training data distribution. White-box attacks assume the attacker has access to the trained model parameters and training procedure, while black-box attacks only assume access to the model's output. Creating efficient membership inference attacks with minimal assumptions is an active area of research. Our work focuses on demonstrating the vulnerability of deep neural networks to membership inference attacks in both federated and non-federated setups. We use features such as gradients, activations, predictions, and labels to simulate these attacks.

## 3. Setup

### 3.1. Trained Models for Predicting Brain Age

We use models trained to predict brain age from structural MRIs to demonstrate vulnerability to membership inference attacks. We show successful attacks on 3D-CNN and 2D-slice-mean models. For centralized training, we use the same dataset and training setup as in previous work. For federated training, we use the same training setup and dataset, simulating different federated training environments with varying amounts of records and subject age range distributions. All models are trained on T1 structural MRI scans of healthy subjects from the UK Biobank dataset.

### 3.2. Attack Setup

Attackers may have access to some part of the training set and additional MRI samples not used for training, referred to as the unseen set. We train a binary classifier to distinguish if a sample was part of the training set. For centrally trained models, the attack models are trained on a balanced set of 1500 samples from both the training and unseen sets. For testing, we create a balanced set from the remaining samples and report accuracy as the vulnerability measure. For federated learning, each learner is considered an attacker, trained on its private dataset and some samples from the unseen set. We report the classifier's accuracy on the test set created from the training samples of the learner being attacked and new examples from the unseen set.

## 4. Results

### 4.1. Membership Inference Attacks on Centralized Training

We simulate membership inference attacks on both centralized and federated trained models for the BrainAGE problem. We report results on centrally trained models and distributively trained models. Conventional deep learning models are trained using gradient descent, so the gradient magnitudes computed from a trained model should be useful for distinguishing training set and unseen set samples. We train a binary classifier using features derived from the gradients, activations, predictions, and labels, and study the effectiveness of different features.

Table 1 summarizes the results of simulating membership attacks with various features. Figure 1 shows the distribution of prediction errors and gradient magnitudes from the trained models. The different error distributions between test and train samples indicate that error can be a useful feature for membership inference attacks. Using prediction and label as separate features produces even stronger attacks. Gradients of parameters at different layers, particularly those closer to the output layer, are more effective for membership inference attacks.

| Features | 3D-CNN | 2D-slice-mean |
|----------|---------|---------------|
| Activation | 72.60 ± 0.45 | 78.34 ± 0.17 |
| Error | 71.01 ± 0.64 | 80.52 ± 0.40 |
| Gradient Magnitude | 76.65 ± 0.44 | 82.16 ± 0.29 |
| Gradient (conv 1 layer) | 76.96 ± 0.57 | 82.89 ± 0.83 |
| Gradient (output layer) | 76.45 ± 0.20 | 81.70 ± 0.29 |
| Prediction + Label | 78.05 ± 0.47 | 83.04 ± 0.50 |

**Figure 1: Distribution of prediction error and gradient magnitudes from the trained models.**

(a) Prediction errors  
(b) Gradients of conv 1 layer

These results show that the tendency to overfit varies with age, and the model may overfit more for some age groups, resulting in higher attack accuracy.