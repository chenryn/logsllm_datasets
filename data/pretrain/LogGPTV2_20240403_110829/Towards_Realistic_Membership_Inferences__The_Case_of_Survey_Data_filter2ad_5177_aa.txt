title:Towards Realistic Membership Inferences: The Case of Survey Data
author:Luke A. Bauer and
Vincent Bindschaedler
Proceedings of Machine Learning Research – Under Review:1–24, 2021
Full Paper – MIDL 2021 submission
Membership Inference Attacks on Deep Regression Models
for Neuroimaging
Umang Gupta1
1Information Sciences Institute, University of Southern California
Dmitris Stripelis1
Pradeep K. Lam2
2Imaging Genetics Center, Mark and Mary Stevens Institute for Neuroimaging and Informatics,
Keck School of Medicine, University of Southern California
PI:EMAIL
PI:EMAIL
PI:EMAIL
Paul M. Thompson2
Jos´e Luis Ambite1
Greg Ver Steeg1
Editors: Under Review for MIDL 2021
PI:EMAIL
PI:EMAIL
PI:EMAIL
Abstract
Ensuring privacy of research participants is vital, even more so in healthcare environments.
Deep learning approaches to neuroimaging require large datasets, and this often necessi-
tates sharing data between multiple sites, which is antithetical to the privacy objectives.
Federated learning is a commonly proposed solution to this problem. It circumvents the
need for data sharing by sharing parameters during the training process. However, we
demonstrate that allowing access to parameters may leak private information, even if data
is never directly shared. In particular, we show that it is possible to infer if a sample was
used to train the model given only access to the model prediction (black-box) or access to
the model itself (white-box) and some leaked samples from the training data distribution.
Such attacks are commonly referred to as Membership Inference attacks. We show realistic
Membership Inference attacks on deep learning models trained for 3D neuroimaging tasks
in a centralized as well as decentralized setup. We demonstrate feasible attacks on brain
age prediction models (deep learning models that predict a person’s age from their brain
MRI scan). We correctly identiﬁed whether an MRI scan was used in model training with
60% to over 80% success rate depending on model complexity and security assumptions.
1. Introduction
Machine learning’s endless appetite for data is increasingly in tension with the desire for
data privacy. Privacy is a highly signiﬁcant concern in medical research ﬁelds such as
neuroimaging, where information leakage may have legal implications or severe consequences
on individuals’ quality of life. The Health Insurance Portability and Accountability Act 1996
(HIPAA) (Centers for Medicare & Medicaid Services, 1996) protects the health records of
an individual subject, as well as data collected for medical research. Privacy laws have
spurred research into anonymization algorithms. One such example is algorithms that
remove facial information from MRI scans (Bischoﬀ-Grethe et al., 2007; Schimke and Hale,
2011; Milchenko and Marcus, 2013).
© 2021 U. Gupta, D. Stripelis, P.K. Lam, P.M. Thompson, J.L. Ambite & G.V. Steeg.
Membership Inference Attacks on Deep Regression Models for Neuroimaging
While there are laws and guidelines to control private data sharing, model sharing or
using models learned from private data may also leak information. The risk to participants’
privacy, even when only summary statistics are released, has been demonstrated and widely
discussed in the ﬁeld of genome-wide association studies (Homer et al., 2008; Craig et al.,
2011). A neural network model learned from private data can be seen as a summary statistic
of the data, and private information may be extracted from it. To demonstrate the feasibility
of information leakage, we study the problem of extracting information about individuals
from a model trained on the brain age prediction regression task using neuroimaging data.
Brain age is an estimate of a person’s age from their brain MRI scan and it is a commonly
used task for benchmarking machine learning algorithms.
In particular, we study attacks to infer which samples or records were used to train the
model. These are called Membership Inference attacks (Shokri et al., 2017; Nasr et al., 2019).
Through these attacks, an adversary may infer if an individual’s data was used to train the
model, harming privacy. Consider a hypothetical example, where some researchers released
a neural network trained with scans of participants in a depression study. An adversary with
access to the individual’s scan and the model may identify if they participated in the study,
revealing information about their mental health, which can have undesirable consequences.
Previous work on membership inference attacks focus on supervised classiﬁcation prob-
lems, often exploiting the model’s over-conﬁdence on the training set and the high dimen-
sionality of the probability vector (Shokri et al., 2017; Salem et al., 2019; Pyrgelis et al.,
2017). Our work demonstrates membership inference attacks on a regression model trained
to predict a person’s age from their brain MRI scan (brain age) under both white-box
and black-box setups. We simulate attacks on models trained via centralized as well as
distributed, federated setups. We also demonstrate a strong empirical connection between
overﬁtting and vulnerability of the model to membership inference attacks.
2. Related Work & Background
2.1. BrainAGE Problem
Brain age is an estimate of a person’s age from a structural MRI scan of their brain.
The diﬀerence between a person’s true chronological age and the predicted age is a useful
biomarker for early detection of various neurological diseases (Franke and Gaser, 2019)
and the problem of estimating this diﬀerence is deﬁned as the Brain Age Gap Estimation
(BrainAGE) problem. Brain age prediction models are trained on brain MRIs of healthy
subjects to predict the chronological age. A higher gap between predicted and chronological
age is often considered an indicator of accelerated aging in the subject, which may be a
prodrome for neurological diseases. To predict age from raw 3D-MRI scans, many recent
papers have proposed using deep learning (Feng et al., 2020; Gupta et al., 2021; Stripelis
et al., 2021; Peng et al., 2021; Bashyam et al., 2020; Lam et al., 2020). To simulate attacks on
models trained centrally or distributively, we employ trained networks and training setups
recently proposed in Gupta et al. (2021) and Stripelis et al. (2021), respectively. Although
there is some controversy over the interpretation of BrainAGE (Butler et al., 2020; Vidal-
Pineiro et al., 2021), we emphasize that we are only using BrainAGE as a representative
problem in neuroimaging that beneﬁts from deep learning.
2
Membership Inference Attacks on Deep Regression Models for Neuroimaging
2.2. Federated Learning
In traditional machine learning pipelines, data originating from multiple data sources must
be aggregated at a central repository for further processing and analysis. Such an aggrega-
tion step may incur privacy vulnerabilities or violate regulatory constraints and data sharing
laws, making data sharing across organizations prohibitive. To address this limitation, Fed-
erated Learning was recently proposed as a distributed machine learning paradigm that
allows institutions to collaboratively train machine learning models by relaxing the need to
share private data and instead push the model training locally at each data source (McMa-
han et al., 2017; Yang et al., 2019; Kairouz and McMahan, 2021). Even though Federated
Learning was originally developed for mobile and edge devices, it is increasingly applied in
biomedical and healthcare domains due to its inherent privacy advantages (Lee et al., 2018;
Sheller et al., 2018; Silva et al., 2019; Rieke et al., 2020; Silva et al., 2020).
Depending on the communication characteristics between the participating sources, dif-
ferent federated learning topologies can be discerned (Yang et al., 2019; Bonawitz et al.,
2019; Rieke et al., 2020; Bellavista et al., 2021) — star and peer-to-peer being the most
prominent. In a star topology (Sheller et al., 2018; Li et al., 2019, 2020; Stripelis et al., 2021),
the execution and training coordination across sources is realized by a trusted centralized
entity, the federation controller, which is responsible for shipping the global or community
model to participating sites and aggregate the local models. In peer-to-peer (Roy et al.,
2019) topologies, the participating sites communicate directly with each other without re-
quiring centralized controller. We focus on the star federated learning topology.
In principle, at the beginning of the federation training, every participating data source
or learner receives the community model from the federation controller, trains the model
independently on its local data for an assigned number of iterations, and sends the locally
trained parameters to the controller. The controller computes the new community model by
aggregating the learners’ parameters nd sends it back to the learners to continue training.
We refer to this synchronization point as a federation round. After repeating multiple
federation rounds, the jointly learned community model is produced as the ﬁnal output.
2.3. Membership Inference Attacks
The malicious use of trained models to infer which subjects participated in the training set
by having access to some or all attributes of the subject is termed as membership inference
attack (Nasr et al., 2019; Shokri et al., 2017). These attacks aim to infer if a record (a
person’s MRI scan in our case) was used to train the model. These attacks can reveal
censored personal information and thus have legal implications. These attacks are often
distinguished by the access to the information that the adversary has (Nasr et al., 2019).
Most successful membership inference attacks in the deep neural network literature require
access to some parts of the training data or at least some samples from the training data
distribution (Salem et al., 2019; Pyrgelis et al., 2017; Truex et al., 2018). White-box attacks
assume that the attacker is also aware of the training procedure and has access to the
trained model parameters, whereas Black-box attacks only assume unlimited access to an
API that provides the output of the model (Leino and Fredrikson, 2020; Nasr et al., 2019).
Creating eﬃcient membership inference attacks with minimal assumptions and infor-
mation is an active area of research (Choo et al., 2020; Jayaraman et al., 2020; Song and
3
Membership Inference Attacks on Deep Regression Models for Neuroimaging
Mittal, 2020). However, our work is focused on demonstrating the vulnerability of deep
neural networks to membership inference attacks in the federated as well as non-federated
setup. Therefore, we make more straightforward assumptions and assume somewhat lenient
access to information. Our attack models are also inspired by Nasr et al. (2019); Shokri
et al. (2017), and we use similar features such as gradients of parameters, activations, pre-
dictions, and labels to simulate membership inference attacks. In particular, we learn deep
binary classiﬁers to distinguish training samples from unseen samples using these features.
In the case of federated learning, each learner receives model parameters and has some
private training data. Thus, any learner is capable of launching white-box attacks. More-
over, in this scenario, the learner has access to the community models learned at each
federation round. Therefore, when simulating membership attacks on federated models, we
simulate attacks from the learners’ perspective by training on learners’ private data and
attacking other learners’ subjects. In the case of models trained via centralized training, we
assume that the adversary can access some public training and test samples. We simulate
both white-box and black-box attacks in this case.
3. Setup
3.1. Trained Models for predicting Brain Age
We use models trained to predict brain age from structural MRIs to demonstrate vul-
nerability to membership inference attacks. We show successful attacks on 3D-CNN and
2D-slice-mean models. The neural network architectures are summarized in Appendix A.3.
For centralized training, we use the same dataset and training setup as Gupta et al. (2021)
and for federated training, we use the same training setup and dataset as Stripelis et al.
(2021) (see Appendices A.1 and A.2). In the latter, the authors simulate diﬀerent federated
training environments by considering diverse amounts of records (i.e., Uniform and Skewed)
and subject age range distribution across learners (i.e., IID and non-IID). All models are
trained on T1 structural MRI scans of healthy subjects from the UK Biobank dataset (Miller
et al., 2016) with the same pre-processing as Lam et al. (2020). For full details regarding
the dataset, data distribution, and training setup, see Appendix A.
3.2. Attack Setup
As discussed in Section 2.3, attackers may have access to some part of the training set and
additional MRI samples that were not used for training, referred hereafter as the unseen
set. Thus, we train a binary classiﬁer to distinguish if the sample was part of the training
set. Details regarding the classiﬁer architecture are provided in Appendix C and we study
eﬀectiveness of diﬀerent features useful for the attacks in Section 4.1.
In the case of models trained via centralized training, the attack models are trained
on a balanced training set using 1500 samples from both training and unseen sample set1.
For testing, we create a balanced set from the remaining train and unseen set — 694
samples each and report accuracy as the vulnerability measure. To attack models trained
via federated learning, we consider each learner as the attacker. Thus, the attacker is trained
1. We have used testing dataset of the brain age model training setup as the unseen set. The unseen set
and the training set are IID samples from the same distribution.
4
Membership Inference Attacks on Deep Regression Models for Neuroimaging
on its private dataset and some samples from the unseen set that it may have. This way, we
created a balanced training set of up to 10002 samples from training and unseen set each.
Unlike centralized setup, the distribution of the unseen set and training set that the attacker
model is trained on could be diﬀerent, particularly in the case of non-IID environments. In
this scenario, the attacks are made on the private data of other learners. Thus, we report
the classiﬁer’s accuracy on the test set created by the training sample of the learner being
attacked and new examples from the unseen set.
4. Results
We simulate membership inference attacks on both centralized and federated trained models
for the BrainAGE problem. We report results on centrally trained models in Section 4.1 and
distributively trained in Section 4.2. Conventional deep learning models are trained using
gradient descent. Therefore, the gradient magnitudes computed from a trained model should
be useful to distinguish training set and unseen set samples. We train a binary classiﬁer
using the features derived from the gradients, activation and predictions of the trained
model and samples’ labels and study the eﬀectiveness of diﬀerent features in Section 4.1.
The main task is to predict if a sample belonged to the training set or not. We report
accuracy on a test set created from the training and the unseen sample sets that are not
seen by the trained attack model but used for training and evaluating the brain age models.
4.1. Membership Inference Attacks on Centralized Training
(a) Prediction errors
(b) Gradients of conv 1 layer
Figure 1: Distribution of prediction error and gradient magnitudes from the trained models.
Table 1 summarizes the results of simulating membership attacks with various features.
As apparent from Figure 1(a), test and train samples have diﬀerent error distributions
due to the inherent tendency of deep neural networks to overﬁt on the training set (Zhang
et al., 2017). Due to these diﬀerences in error distributions, the error can be a useful
feature for membership inference attacks. Error is computed as the diﬀerence between
prediction and label; however, instead of using error, one may use prediction and label as
two separate features that produce even stronger attacks as indicated by higher membership
attack accuracies. These indicate that the tendency to overﬁt may vary with age, and the
model may overﬁt more for some age groups, thus resulting in higher attack accuracy.
2. In the case of Skewed & non-IID environment, some learners had less than 1000 training samples, the
attack model then will be trained on fewer samples.
5
10505101520Output Error (year)0.00.10.20.30.40.50.6Density2D-slice-mean10505101520Output Error (year)0.000.050.100.150.200.250.30Density3D-CNNTrainTest050010001500200025003000Gradient Magnitude0.0000.0010.0020.0030.004Density2D-slice-mean0250500750100012501500Gradient Magnitude0.00000.00250.00500.00750.01000.01250.01500.0175Density3D-CNNTrainTestMembership Inference Attacks on Deep Regression Models for Neuroimaging
Features
3D-CNN
2D-slice-mean
activation
error
gradient magnitude
gradient (conv 1 layer)
gradient (output layer)
gradient (conv 6 layer)
prediction + label
prediction + label + gradient (conv 6 + output)
56.63
59.90 ± 0.01
72.60 ± 0.45
71.01 ± 0.64
76.65 ± 0.44
76.96 ± 0.57
76.45 ± 0.20
78.05 ± 0.47
-
74.06 ± 0.00
78.34 ± 0.17
80.52 ± 0.40
82.16 ± 0.29
82.89 ± 0.83
81.70 ± 0.29
83.04 ± 0.50
Table 1: Membership inference attack accuracies on centrally trained models (averaged over
5 attacks). Details about conv 1, output and conv 6 layers are provided in Appendix A.3.
Attacks made using error or prediction and labels are black-box attacks. A white-box
attacker may also utilize more information about the models’ internal workings like the
gradients, knowledge about loss function, training algorithm, etc. Deep learning models are
commonly trained until convergence using some variant of gradient descent. The conver-
gence is achieved when the gradient of loss w.r.t parameters on the training set is close to
0. As a result, gradient magnitudes are higher or similar for unseen samples than training
samples (see Figure 1(b)). Therefore, we used the gradient magnitude of each layer as a
feature. This resulted in attack accuracies of 72.6 and 78.34 for 3D-CNN and 2D-slice-mean
models, respectively. Finally, we simulated attacks using gradients of parameters at diﬀer-
ent layers3. We ﬁnd that parameter-gradients of layers closer to the output layer (i.e., conv
6, output layers) are more eﬀective compared to the gradients of layer closer to the input