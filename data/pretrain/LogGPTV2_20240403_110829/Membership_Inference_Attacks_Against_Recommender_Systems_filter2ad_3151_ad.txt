points (members and non-members) are visualized in a 2-dimension
space by t-distributed Stochastic Neighbor Embedding (t-SNE) [50].
Figure 4 shows the results of the shadow and target distributions
for two datasets, where the red points represent members and the
blue points represent non-members. According to the attack and
visualization results, we conclude that:
• In general, under the assumption that the shadow recom-
mender knows the algorithm and dataset distribution of the
target recommender, our attack is very strong. There are
two main reasons for the effectiveness. First, data points
of members and non-members are tightly clustered sepa-
rately. Due to the different recommendation methods for
members and non-members, generally, the interactions and
recommendations of members are more relevant. In that case,
the intra-cluster distance of members and non-members is
much smaller than the inter-cluster distance between them,
so that members can be easily distinguished. Second, as
the adversary has the most knowledge of the target recom-
mender, the shadow recommender can well mimic the target
recommender. Thus the attack model, which is trained on
the ground truth membership generated from the shadow
recommender, is able to conduct a membership inference
accurately.
• When the target recommender uses Item or NCF, our attack
performs considerably better on all datasets. Specifically, an
average AUC of the attack aiming at Item or NCF is 18%
and 20% higher respectively. Compared to the visualization
result of LFM, the dissimilarity of the shadow and target
distributions of Item or NCF is smaller. Thus the attack model
can easily deal with the data points which are similar to
its training data. Besides, our attack performs better when
the target dataset is the ml-1m dataset. This is because the
user-item matrix of the ml-1m dataset is the densest among
all three datasets, which enormously facilitates the item
vectorization and attack model training.
Assumption II. To this end, we relax the assumption so that the
adversary only has a shadow dataset in the same distribution as the
target dataset. The experimental results are shown in Table 3, Ta-
ble 4 and Table 5, where the attack results of the previous assump-
tion are listed at the diagonals. Then we depict the visualization
results of data points in the shadow and target distributions by t-
SNE to show the relationship between members and non-members.
In Figure 5, the red points are members and the blue points are
non-members, which are all from the lf-2k dataset. We can see from
the attack performances as well as the comparisons between the
shadow and target distributions that:
• When the adversary only gains the knowledge about the
target dataset distribution, the attack performances drop
as expected but are still strong. For instance, on the ADM
dataset, when the target recommender uses Item but the
shadow recommender uses LFM, the attack performance
drops from an AUC of 0.926 to an AUC of 0.843. Decreases
Session 3C: Inference Attacks CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea870(a) ADM_NCF_shadow (left) and ADM_NCF_target (right)
(b) ADM_LFM_shadow (left) and ADM_LFM_target (right)
(c) lf-2k_Item_shadow (left) and lf-2k_Item_target (right)
(d) lf-2k_LFM_shadow (left) and lf-2k_LFM_target (right)
Figure 4: Visualization results by t-SNE, where red points denote members and blue points represent non-members. For the
ADM dataset, visualization results, (a) when the shadow and target recommenders are implemented by NCF, and (b) when
LFM is used as the shadow and target recommenders, are demonstrated. For the lf-2k dataset, visualization results, (c) when
the shadow and target recommenders are implemented by Item, and (d) when LFM is adopted as the shadow and target recom-
menders, are shown.
(a) lf-2k_LFM_shadow
(b) lf-2k_LFM_target
(c) lf-2k_Item_shadow
Figure 5: Visualization results by t-SNE, where red points are members and blue points are non-members. For the lf-2k dataset,
visualization results, (a) when the shadow recommender is implemented by LFM, (b) when LFM is employed as the target
recommender, and (c) when Item is used as the shadow recommender, are demonstrated.
also appear on the lf-2k dataset and the ml-1m dataset. That
is to say, even with different recommendation methods, the
attack model can still benefit from the similar distributions of
the target and shadow datasets to conduct the memebership
inference accurately.
• An interesting finding is that, the attack on the ml-1m dataset
achieves the best overall performance (i.e., 0.873 in terms
of average AUC), and the attack performance on the ADM
dataset is the worst (i.e., 0.747 in terms of average AUC).
This is because the user-item matrix built from the ml-1m
dataset is the densest while the matrix from the ADM dataset
is the sparsest. Intuitively, the attack model can learn more
information from a denser user-item matrix, leading to a
better attack performance.
In addition, we do acknowledge that, in some cases, the attack
performances are not ideal. For instance, the attack model against
LFM achieves a poor performance (see Table 3). Comparing to
the other two recommendation algorithms, LFM has higher model
complexity, which makes it harder for the adversary to build a
similar shadow model.
membersnon-membersmembersnon-membersmembersnon-membersmembersnon-membersmembersnon-membersmembersnon-membersmembersnon-membersmembersnon-membersmembersnon-membersmembersnon-membersmembersnon-membersSession 3C: Inference Attacks CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea871back-diagonal matrices. Analysing the results, we draw conclusions
that:
• Even under the minimum assumption, our attack can still
achieve strong performances in most cases. For instance,
when the target recommender is established by LFM on the
ml-1m dataset and the adversary uses NCF to build a shadow
recommender on the lf-2k dataset, our attack achieves an
AUC of 0.710.
• In some cases, when the adversary knows less information
about the target recommender, the attack even achieves bet-
ter performances. For instance, when the adversary builds a
shadow recommender by NCF on the lf-2k dataset to mimic
the target recommender which uses Item on the ml-1m
dataset, our attack achieves an AUC of 0.747. Meanwhile,
with the knowledge that the target dataset is the ml-1m
dataset, the adversary uses LFM to establish a shadow rec-
ommender when the target recommender uses NCF, our
attack only achieves an AUC of 0.670. To explain this, we
adopt the t-SNE algorithm to visualize user feature vectors
for the “MLAI” and “ALLI” attacks. The visualization results
in Figure 7 show that the distributions of feature vectors
generated by the shadow model “ML” and target model “AL”
are more similar than the distributions generated by “Al” and
“LI”. Therefore, the “MLAI” attack performs better than the
“ALLI” attack.
In summary, our attack can effectively conduct a membership infer-
ence against recommender systems, even with the limited knowl-
edge.
3.4 Hyperparameters
In this section, we analyse the influences of hyperparameters, in-
cluding the number of recommendations 𝑘, the length of vectors 𝑙
and the weights of recommendations. Figure 8 shows the experi-
mental results.
The Number of Recommendations 𝑘. We evaluate our experi-
ments with different values of 𝑘 from 10 to 100, in order to explore
the influence of 𝑘 on the attack. Figure 8a shows the attack perfor-
mance against the number of recommendations. When the number
of recommendations is less than 50, the attack performance im-
proves with the increase of 𝑘. Then the performance maintains
stable when 𝑘 goes beyond 50. These results show that the attack
model gains more information when the number of recommen-
dations increases. However, the attack model cannot gain more
information infinitely when the number of recommendations is
large enough.
The Length of Vectors 𝑙. We evaluate our experiments with dif-
ferent values of 𝑙 from 10 to 100, in order to explore the influence of
𝑙 on the attack. Figure 8b shows the attack performance against the
length of vectors. Similar to Figure 8a, when the length of vectors
is less than 50, the attack performance improves with the increase
of 𝑙. Then, in general, no obvious improvement of the performance
is observed when 𝑙 goes beyond 50. These results show that the
representation power of the attack model becomes stronger, as a
larger length of vectors can provide more dimensional perspectives.
However, the attack model cannot improve its representation power
infinitely when the length of vectors is large enough.
Figure 6: The attack performances under the assumption III.
The 𝑥-axis indicates the target recommender’s datasets (the
first letter, i.e., “A,” “L” and “M”) and algorithms (the second
letter, i.e., “I,” “L” and “N”), and similarly the 𝑦-axis repre-
sents the shadow recommender’s datasets and algorithms.
(a) ml-1m_LFM_shadow (left) and ADM_Item_target (right)
(b) ADM_LFM_shadow (left) and lf-2k_Item_target (right)
Figure 7: The visualization results of MLAI v.s. ALLI
Assumption III. Finally, we further conduct evaluations when the
adversary neither has a shadow dataset in the same distribution as
the target dataset nor knows the target algorithm. All experimental
results are shown in Figure 6. Note that, the attack results of the
assumption I are listed at the back-diagonal and the attack perfor-
mances of the assumption II are shown in the three 3 × 3 block
AIALANLILLLNMIMLMNTargetMNMLMILNLLLIANALAIShadow0.5490.5040.5370.5470.5060.5230.9760.9140.9980.6080.5780.5100.5460.5250.6240.9310.8710.6700.5830.5900.4980.4450.4130.5390.9980.7920.7060.5220.6090.5580.8270.8090.9160.7470.7100.6580.6190.6140.5880.7320.7770.7740.5030.5050.5540.4670.5290.4150.9390.7960.7930.5150.6190.5840.5130.4940.9870.5250.4990.4950.5020.5000.5120.8430.7750.5540.5470.5350.4990.5100.5100.5010.9260.8850.7500.5640.5400.4870.5250.5240.516AUC0.50.60.70.80.9membersnon-membersmembersnon-membersmembersnon-membersmembersnon-membersSession 3C: Inference Attacks CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea872(a) The attack performances against the
number of recommendations 𝑘.
(b) The attack performances against the
length of vectors 𝑙.
(c) The attack performances against the
weights of recommendations.
Figure 8: The attack performances of analysing the influences of hyperparameters.
(a) The attack performances with different 𝑙, when 𝑘 = 20,
𝑘 = 50 and 𝑘 = 100.
(b) The attack performances with different 𝑘, when 𝑙 = 20,
𝑙 = 50 and 𝑙 = 100.
Figure 9: The attack performances to investigate “which is more important, 𝑘 or 𝑙?”
The Weights of Recommendations. We evaluate our experi-
ments with different designs for the weights of recommendations.
In the real world, the items recommended to a user are provided in
the form of an ordered sequence. And, compared to the items at the
back of the sequence, the ones in front of the sequence are more
likely to be preferred by the user. Thus we evaluate two methods
of assigning weights to items at different positions in the sequence.
One is that all items are assigned the same weight of 1
𝑘 . And the
to the 𝑖𝑡ℎ item in the sequence.
Same as mentioned above, we denote 𝑘 as the number of recommen-
dations. As shown in Figure 8c, we find that considering the order
of recommendations can obviously promote attack performances.
other is to assign a weight of 𝑘−𝑖+1𝑘
𝑛=1 𝑛
3.5 Extensive Analysis
In this section, we study five interesting questions and give further
results in order to comprehensively investigate our attack method.
Which Is More Important, the Length of Feature Vector 𝑙 or
the Number of Recommendations 𝑘? In addition to the anal-
yses about hyperparameters in Section 3.4, we also investigate
“which is more important, 𝑘 and 𝑙?”. Two more experiments are
conducted on the “MIMI” attack, with different 𝑙 when 𝑘 is set to
20, 50 and 100 (see Figure 9a), and with different 𝑘 when 𝑙 is set to
20, 50 and 100 (see Figure 9b). As the results show, both the number
of recommendations (𝑘) and the length of vectors (𝑙) influence at-
tack performances substantially. Specifically, when 𝑘 reduces from
100 to 20, the AUC score drops from 0.998 to 0.764. Similarly, as 𝑙
reduces from 100 to 20, the AUC score descends from 0.998 to 0.817.
What Is the Impact of the Dataset Size? Considering the size
of the training dataset imposes huge impacts on machine learning
models, we conduct evaluations regarding the size of the shadow
dataset. Specifically, the size of the shadow dataset is reduced to
90%, 80%, and 70% of the original size. Note that, the ratio of mem-
bers to non-members keep unchanged for the dataset balance. For
the “LLLL” attack, the AUC scores of the attack performances are
decreased to 0.633, 0.714, and 0.746, respectively, when the size of
the shadow dataset is 70%, 80%, and 90% of the original size. Com-
paring to the original AUC of 0.777, we can conclude that a larger
shadow dataset usually leads to a better-trained attack model.
Why Use an MLP as the Attack Model? To demonstrate the
effectiveness of our attack model, we evaluate the attacks utilizing K-
Means to distinguish non-members from members on the lf-2k and
ml-1m datasets. The results are shown in Table 6 and Table 7, where
Item, LFM, and NCF in the first column are shadow algorithms
for our attack, and K-Means is used to cluster non-members and
members. Since there are only two classes, i.e., members and non-
members, the number of classes 𝐾 for K-Means is set to 2. From
the results, we can conclude that our attack outperforms K-Means
largely, indicating the validity of our attack model. For instance,
when the K-Means algorithm infers the membership status from
102030405060708090100Thenumberofrecommends0.500.550.600.650.700.750.800.850.900.951.00AUCAIAILILIMIMI102030405060708090100Thelengthofvectors0.500.550.600.650.700.750.800.850.900.951.00AUCAIAILILIMIMIAIAIAIALAIANLILI0.00.20.40.60.81.0AUCequalordered102030405060708090100Thelengthofvectors0.500.550.600.650.700.750.800.850.900.951.00AUCMIMIk=20MIMIk=50MIMIk=100102030405060708090100Thelengthofvectors0.500.550.600.650.700.750.800.850.900.951.00AUCMIMIl=20MIMIl=50MIMIl=100Session 3C: Inference Attacks CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea873Table 6: The AUC of the K-Means algorithm on the lf-2k
dataset.
Target Algorithm
LFM
0.796
0.777
0.809
0.717
Item
0.939
0.732
0.827
0.649
NCF
0.793
0.774
0.916
0.730
Item
LFM
NCF
K-Means
Table 7: The AUC of the K-Means algorithm on the ml-1m
dataset.
Target Algorithm
LFM
0.792
0.871
0.914
0.720
Item
0.998
0.931
0.976
0.805
NCF
0.706
0.670
0.998
0.719
Item
LFM
NCF
K-Means