nocuous samples. By including only a fraction β of the α spurious features, S, in any
one target-class sample, innocuous samples that have all α spurious features can be
made to have a higher Bayes score than the target-class samples.
The result of the attack is illustrated in Figure 8. The spurious features in the target-
class samples cause the false negative curve to shift to the right. The innocuous samples
that contain the spurious features result in a tail on the false positive curve. The tail’s
height corresponds to the fraction of samples in the innocuous pool that have the spu-
rious tokens. As the ﬁgure shows, regardless of how τ is chosen, the learner is forced
either to classify innocuous samples containing the spurious features as false positives,
or to have 100% false negatives.
The challenge for the attacker is to choose spurious features that occur in the innocu-
ous training pool (which the attacker cannot see) in the correct proportion for the attack
to work. The attacker needs to choose spurious features that occur infrequently enough
in the innocuous pool that the corresponding Bayes score P(S|+)
P(S|−) is large, but frequently
enough that a signiﬁcant fraction of the samples in the innocuous pool contain all of the
spurious features; i.e. so that the forced false positive rate is signiﬁcant.
Attack Analysis. We show that the attack works for a signiﬁcant range of parameters.
The attacker’s a priori knowledge of the particular network protocol is likely to allow
him to choose appropriate spurious features. A simple strategy is to identify a type of
request in the protocol that occurs in a small but signiﬁcant fraction of requests (e.g.
5%), and that contains a few features that are not commonly found in other requests.
These features are then used as the spurious features.
We ﬁrst determine what parameters will give the innocuous samples containing the
spurious features a higher Bayes score than the target-class samples. For simplicity, we
assume that P(si|−) is the same for each spurious feature si.
96
J. Newsome, B. Karp, and D. Song
Theorem 5. Given that each target-class sample contains the feature set W and βα
spurious features si chosen uniformly at random from the set of α spurious features
S, samples containing all α spurious features in S have a higher Bayes score than the
target-class samples when:
P(si|−) < β and ( β
P(si|−))βα−α≤ P(W|−)
The condition P(si|−) < β is necessary to ensure that P(si|−) < P(si|+). Otherwise,
the learner will not use the spurious features in the Bayes classiﬁer.
The second condition is derived as follows:
Innocuous samples have a higher Bayes score
P(W|+)
P(S|+)
≥ P(βS,W|+)
P(βS,W|−)
P(S|−)
P(si|+)α
P(si|−)α ≥ P(si|+)βα
P(si|−)βαP(W|−) Independence assumption
P(si|−)α ≥
ββα(1)
βα
P(si|−)βαP(W|−) Substitution
P(si|−))βα−α≤ P(W|−) Rearrangement
( β
Note that while we have assumed independence between features here, the attack
could still apply to non-Naive Bayes learners, provided that P(S|+)
P(βS,W|−) is satis-
P(S|−)
ﬁed. Whether and how it can be satisﬁed will depend on the speciﬁc implementation of
the learner.
≥ P(βS,W|+)
When these conditions are satisﬁed, the classiﬁer must either classify innocuous sam-
ples containing the spurious features S as positive, or suffer 100% false negatives. Either
way can be considered a ‘win’ for the attacker. Few sites will be willing to tolerate a
signiﬁcant false positive rate, and hence will choose 100% false negatives. If sites are
willing to tolerate the false positive rate, then the attacker has succeeded in performing
a denial-of-service attack. Interestingly, the attacker could choose his spurious tokens
in such a way as to perform a very targeted denial-of-service attack, causing innocuous
samples of a particular type to be ﬁltered by the classiﬁer.
100% false negatives if P(S|−)
P(S|+)
ples containing the spurious features S as positive.
For the threshold-choosing algorithm used by Polygraph, τ will be set to achieve
≥ F. Otherwise it will be set to falsely classify the sam-
Evaluation. The Correlated Outlier is practical for an adversary to implement, even
though he must make an educated guess to choose the set of spurious features that
occur with a suitable frequency in the innocuous pool.
There are four parameters in Theorem 5 that determine whether the attack is suc-
cessful. The attacker chooses α, how many spurious features to use, and β, the fraction
of thosespurious features to include in each target-class sample. The likelihood of suc-
cess increases with greater α. However, since he must ﬁnd α spurious features that are
highly correlated in the innocuous pool, relatively low values are the most practical.
The third parameter, the frequency of the target-class features in the innocuous pool
P(W|−) is out of the attacker’s hands. High values of P(W|−) make the attack easiest.
Indeed, if P(W|−) is high, the learner is already forced to choose between false nega-
tives, and signiﬁcant false positives. We show the attack is still practical for low values
of P(W|−).
Paragraph: Thwarting Signature Learning by Training Maliciously
97
i
)
-
|
s
(
P
x
a
M
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
α=5 β=30%
α=10 β=30%
α=100 β=30%
 0.0002
 0.0004
 0.0006
 0.0008
 0.001
P(W|-)
i
)
-
|
s
(
P
x
a
M
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
N= 0% α=10 β=30%
N=10% α=10 β=30%
N=50% α=10 β=30%
N=90% α=10 β=30%
 0.0002
 0.0004
 0.0006
 0.0008
 0.001
P(W|-)
Fig. 9. Correlated Outlier attack evaluation
Fig. 10. Correlated Outlier attack evaluation,
with chaff
The fourth parameter, the frequency of the spurious features in the innocuous pool
P(si|−), is not directly controlled by the attacker. The attacker’s challenge is to choose
the spurious features such that P(si|−) is low enough that the attacker succeeds in get-
ting the innocuous features with all α of the spurious features S to have a higher Bayes
score than the target-class samples.
Figure 9 shows that the attack can succeed for a wide range of realistic parameters.
Each curve in the graph represents a different attacker choice of α. As P(W|−) increases,
the maximum value of P(si|−) also increases. Even for very low values of P(W|−) and
α, the attacker has a great deal of room for error in his estimation of P(si|−).
Again, any value that satisﬁes these constraints will force the the learner to choose
between false negatives and false positives, and the classiﬁer will not improve as more
target-class samples are obtained. If the learner uses the Polygraph threshold-setting
algorithm, then τ will be set to achieve 100% false negatives if P(S|−)
≥ F. Otherwise
P(S|+)
it will be set to have low false negatives, but will classify the samples containing the
spurious features S as positive. The signature will not improve, and as long as it is in use,
legitimate samples containing those samples will be false positives, causing a targeted
denial of service.
4.4 Attack II: Suspicious Pool Poisoning
Up to this point we have assumed that the suspicious and innocuous pools are noise-
free. That is, the suspicious pool contains only target-class samples, and the innocuous
pool contains only innocuous samples. In some cases, however, the attacker may be
able to inject constructed samples into the suspicious pool, the innocuous pool, or both,
as described in Section 2. We ﬁrst consider the case where the attacker is able to inject
chaff, specially constructed samples, into the suspicious pool.
Attack Description. The chaff can simultaneously have two effects. First, by not in-
cluding the actual target-class features W , the classiﬁer will calculate a lower P(W|+).
The actual target-class samples in the suspicious pool will have lower Bayes scores as
a result, stretching the false negative curve of the training data distribution graph to the
left.
98
J. Newsome, B. Karp, and D. Song
Second, the classiﬁer will calculate a higher P(si|+) for any spurious feature si in-
cluded in the chaff. This will cause innocuous samples containing those features to have
a higher Bayes score, stretching the false positive curve of the training data distribution
graph to the right, in the same manner as in the Correlated Outlier attack (Figure 8). Un-
like the target-class samples, each chaff sample can contain all of the spurious features,
since it makes no difference to the attacker whether the chaff samples are classiﬁed as
positive by the resulting Bayes classiﬁer.
Attack Analysis. The attacker’s goal is again to force the learner to choose between
false positives and false negatives, by ensuring that the score of a sample containing
all α of the spurious features S has a higher Bayes score than a sample containing the
true target-class features W , and a fraction β of the spurious features. Assuming that
the chaff in the suspicious pool contains all α of the spurious features, the attacker can
include fewer spurious features in the actual target-class samples, or even none at all.
Theorem 6. Suppose that the fraction N of samples in the suspicious pool is chaff
containing the spurious features S. Samples containing all α spurious features have a
higher Bayes score than samples containing the actual target-class features W and the
fraction β of the α spurious features when:
P(si|−) < N + (1− N)β and (1− N)( N+β(1−N)
P(si|−)
)βα−α≤ P(W|−)
When these conditions are satisﬁed, this attack becomes equivalent to the Correlated
Outlier attack. Notice that when there is no chaff (N = 0) these conditions simplify to
the conditions presented in Section 4.3.
Evaluation. We perform a similar evaluation as in Section 4.3. In this case, the attacker
uses a relatively low number of spurious features (α= 10), and each curve of the graph
represents different ratios of chaff in the suspicious pool. Figure 10 shows that the
addition of chaff to the suspicious pool greatly improves the practicality of the attack.
The resulting classiﬁer will again either have 100% false negatives, or cause legitimate
samples with the spurious features to be blocked.
4.5 Attack III: Innocuous Pool Poisoning
We next consider the case where the attacker is able to poison the innocuous training
pool. The most obvious attack is to attempt to get samples with the target-class features
W into the innocuous pool. If the target-class samples include only the features W (no
spurious features), then it would be impossible to generate a classiﬁer that classiﬁed the
target-class samples as positive without also classifying the samples that the attacker
injected into the innocuous pool as positive. Hence, the learner could be fooled into
believing that a low-false-positive classiﬁer cannot be generated.
The solution to this problem proposed by Polygraph [15] for automatic worm signa-
ture generation is to use a network trace taken some time t ago, such that t is greater
than the expected time in-between the attacker discovering the vulnerability (and hence
discovering what the worm features W will be), and the vulnerability being patched on
most vulnerable machines. The time period t is somewhat predictable assuming that the
Paragraph: Thwarting Signature Learning by Training Maliciously
99
attacker does not discover the vulnerability before the makers of the vulnerable software
do. Conversely, t could be an arbitrary time period for a “zero-day” exploit. However,
we show that a patient attacker can poison the innocuous pool in a useful way before he
knows what the worm features W are.
Attack Description. The attacker can aid the Correlated Outlier attack by injecting
spurious tokens into the innocuous pool. In this case, using an old trace for the innocu-
ous pool does not help at all, since the attacker does not need to know W at the time
of poisoning the innocuous pool. That is, an attacker who does not yet have a vulner-
ability to exploit can choose a set of spurious features S, and preemptively attempt to
get samples containing S into the learner’s innocuous pool, thus increasing P(S|−). The
attacker can then use these spurious features to perform the Correlated Outlier attack,
optionally poisoning the suspicious pool as well as described in Section 4.4.
Attack Analysis. If the attacker is able to inject samples containing S into the innocuous
pool, P(S|−) will be increased. The attacker’s best strategy may be to use spurious fea-
tures that do not occur at all in normal trafﬁc. This would allow him to more accurately
estimate the learner’s P(S|−) when designing the worm.
Aside from this additional knowledge, the attack proceeds exactly as in Section 4.4.
Evaluation. The success of the attack is determined by the same model as in Theorem 6.
The addition of the injected spurious features helps make the attack more practical by
allowing him to more accurately predict a set of spurious features that occur together
in a small fraction of the innocuous training pool. Success in the attack will again ei-
ther result in the classiﬁer having 100% false negatives, or result in innocuous samples
containing the spurious features to be blocked.
5 Discussion
5.1 Hierarchical Clustering
Polygraph [15] implements a hierarchical clustering algorithm to enable its conjunction
and subsequence learners to work in the presence of non-worm samples in the sus-
picious training pool. Each sample starts as its own cluster, and clusters are greedily
merged together. Each cluster has a signature associated with it that is the intersection
of the features present in the samples in that cluster. The greedy merging process favors
clusters that produce low-false-positive signatures; i.e., those that have the most distin-
guishing set of features in common. When no more merges can be performed without
the resulting cluster having a high-false-positive signature, the algorithm terminates
and outputs a signature for each sufﬁciently large cluster. Ideally, samples of unrelated
worms are each in their own cluster, and non-worm samples are not clustered.
One might wonder whether the hierarchical clustering algorithm helps to alleviate
the Randomized Red Herring or Dropped Red Herring attacks. It does not.
First consider the Randomized Red Herring attack. Each worm sample has the set