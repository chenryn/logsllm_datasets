tavg
Experiments  show  that  the  average  execution  time 
i(j,K)  is  pretty  much  fixed  for  a  given  system 
tavg
configuration.  Furthermore,  it  is  found  that  when  a 
different  key  K(cid:146)  is  used,  the  timing  charts  roughly 
remain the same except that the locations of the bars in 
the charts are permuted, as shown in Figure 5 (a) and 
(b). More specifically, the following equation holds: 
i(pi , K) = tavg
i(p(cid:146)i , K(cid:146)) if  p(cid:146)i ⊕ k(cid:146)i = pi ⊕ ki       (1) 
where ⊕ is the bit-wise XOR operation, and ki and k(cid:146)i 
are the i-th byte of K and K(cid:146) respectively. 
Attack Analysis: We try to explain why (a) there are 
high  bars  corresponding  to  certain  x-values  in  Figure 
5,  and  (b)  why  these  same  peaks  occur,  but  are 
permuted, when different keys are used. 
(a)  Table  lookups  are  intensively  used  in  various 
AES implementations. During the AES encryption, the 
tables  will  gradually  be  loaded  into  the  cache  when 
table  entries  are  actually  used.  If  there  are  no  other 
cache  accesses  in  addition  to  these  table  lookups, 
eventually  the  tables  should  all  be  loaded  into  the 
cache  and  there  will  not  be  any  cache  misses,  if  the 
cache  is  large  enough.  In  reality,  however,  there  are 
wrapper  and  other  background  processes  that  cause 
cache accesses which evict some entries of the tables 
out  of  the  cache.  Moreover,  some  of  these  cache 
accesses evict cache lines at fixed locations. The index 
used in table lookup in the first round is pi ⊕ ki. Given 
a key K, some values of pi will cause the table lookup 
to access those evicted cache lines and will experience 
a  cache  miss  that  leads  to  longer  execution  time  (on 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:29:18 UTC from IEEE Xplore.  Restrictions apply. 
Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06)0-7695-2716-7/06 $20.00  © 2006average). These can be seen as high bars in the timing 
charts at the x-axis values corresponding to these pi.  
(b) When a different key K(cid:146) is used, since the index 
used  for  a  table  lookup  is  the  XOR  of  the  plaintext 
byte and the key byte, another set of plaintext values 
p(cid:146)i  that  satisfies  pi  ⊕  ki  =  p(cid:146)i  ⊕  k(cid:146)i  will  generate  the 
same index for table lookups using pi and ki, and cause 
accesses  to  the  same  cache  lines.  This  explains  why 
their  average  encryption times are about the same, as 
in equation (1). Since Figure 5 (a) and (b) is plotted in 
terms  of  pi  and  pi(cid:146),  respectively,  they  have  similar 
peaks, but in different locations. In Bernstein(cid:146)s attack, 
i( j , K ) = 
the key recovery step exploits this fact: tavg
i( j ⊕ ki ⊕ k(cid:146)i , K(cid:146) ) that is derived from equation (1). 
tavg
Since ki is known, the attacker can try all 256 possible 
values of k(cid:146)i to permute the timing charts obtained in 
the  attacking  phase.  The  correct  value  of  k(cid:146)i  should 
make  the  permuted  chart  most  similar  to  the  one 
obtained  in  the  learning  phase.  The  similarity  is 
quantitatively  measured  via  correlation,  using  the 
algorithm shown in Figure 4. 
Byte 0 - Known Key K
l
)
s
e
c
y
c
(
n
a
e
m
T
-
g
v
a
T
3
2.5
2
1.5
1
0.5
0
-0.5
Plaintext byte value (0-255)
(a) Timing characteristic chart for a known key K 
Byte 0 - Unknown Key K’
l
)
s
e
c
y
c
(
n
a
e
m
T
-
g
v
a
T
3
2.5
2
1.5
1
0.5
0
-0.5
Figure 5. Timing characteristic charts of byte 0 
(b) Timing characteristic chart for a different key K(cid:146) 
Plaintext byte value (0-255)
3.4. New speculation-based covert channel 
While  the  previous  examples  leak  out  information 
due to contention for shared resources (either cache or 
functional units), we have identified a different type of 
covert  channel  based  on  exposing  events 
to 
unprivileged software that were previously not visible 
to it (e.g., exceptions). This has happened recently in 
some processors supporting speculative execution. 
To hide the long latency that a load instruction may 
introduce,  control  speculation  in  IA-64  allows  a  load 
instruction to execute speculatively [14]. IA-64 adds a 
one-bit  flag,  the  NaT  bit,  to  each  general-purpose 
register. If the speculative load instruction (ld.s) would 
cause an exception, the NaT bit of the target register is 
set, but the exception is not taken right away. Control 
speculation allows deferral of the exception, allowing 
the  program  itself  to  handle  the  exception  when 
necessary. In current Itanium processors, TLB misses 
or  TLB  access  bit  violations  are  typical  examples  of 
ld.s  exceptions  which  can  be  deferred.  In  addition, 
speculative  loads  may  also  be  deferred  by  hardware 
based  on  implementation-dependent  criteria,  such  as 
the  detection  of  a  long-latency  cache  miss.  Such 
deferral is referred to as spontaneous deferral [14]. 
spontaneous  deferral 
Such  a  mechanism,  however,  can  be  exploited  to 
facilitate  information  leakage.  For  example,  in  the 
cache-based  side  channel  attacks  described  earlier, 
cache  misses  are  detected  by  measuring  cache  access 
timing.  However, 
is 
implemented  in  a  future  version  of  the  Itanium 
processor such that cache misses can be deferred, the 
observer  can  observe  the  cache  miss  using  control 
speculation. He can access a cache line using the ld.s 
instruction to check the NaT bit of the target register. 
If  the  NaT  bit  is  set,  a  cache  miss  is  detected.  In 
contrast  to  timing  measurement  which  suffers  from 
noise, this mechanism is like a noiseless channel. 
Similar  methods  can  be  used  to  detect  exceptions 
such as a TLB miss. This is particularly useful when an 
insider  is  available.  The  insider  can  choose  any 
exception  sources  available,  not  limited  to  cache  or 
TLB misses. To encode a bit, the insider makes certain 
changes  in  the  system  such  that  later  on,  when  the 
observer  executes  the  speculative  instruction,  these 
changes will cause a deferred exception which sets the 
NaT  bit.  The  observer  can  then  see  the  bit  sent  by 
checking the NaT bit. 
if 
3.5. Data rates of covert channels 
Table 1 shows the data rates of the new processor-
based covert channels. To make the data comparable, 
we  implement  the  SMT/FU  channel  on  the  same 
processor  as  used  in  [12],  i.e.,  a  2.8GHz  Pentium-4 
processor  with  hyper-threading.  The  rate  of  the 
SMT/Cache channel reported in [12] is approximately 
400 Kilobytes per second, or 3.2 Mbps (Megabits per 
second). We measured the rate of the SMT/FU channel 
as  approximately  500  Kbps  (Kilobits  per  second), 
which  can  be  even  higher  if  further  optimized.  We 
estimated  the  rate  of  the  speculation-based  channel 
based on a processor model with settings typical of an 
Itanium (IA-64) processor (1GHz clock rate; a 16-way 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:29:18 UTC from IEEE Xplore.  Restrictions apply. 
Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06)0-7695-2716-7/06 $20.00  © 20062MB cache with 128-byte cache lines.) A conservative 
estimation of this rate is about 200Kbps.  
In  contrast,  traditional  OS-based  covert  channels, 
e.g.,  the  Inode  table  channel  and  the  upgraded 
directory  channel,  exploit  shared  resources  at  system 
level. The resulting rates are much lower, e.g., around 
50bps and 0.5bps respectively with typical data in the 
1990(cid:146)s  [13].  Even  if  we  assume  a  linear  increase  in 
such OS covert channel rates with a 100X increase in 
processor  clock  rate, 
the  processor-based  covert 
channels  are  still  orders  of  magnitude  faster  than  the 
traditional OS-based covert channels.   
Table 1. Data rates of new covert channels 
SMT/cache 
~3.2Mbps 
SMT/FU 
~500Kbps 
Control Spec. 
~200Kbps 
4. Insufficiency of software isolation 
Software  isolation  methods  providing  Mandatory 
Access  Control  (MAC)  and  Virtual  Machine  (VM) 
technology  may  erroneously  lull  us to think that they 
also  prevent  information  from  being  leaked  out. 
Unfortunately,  without  being  aware  of  these  attacks, 
software isolation can be completely ineffective. 
Figure 6. A VMM based system. 
trend 
illustrates  a 
recent 
Figure  6 
towards 
implementing Virtual Machines, managed by a Virtual 
Machine Monitor (VMM), e.g., Terra [17], Xen [18]. 
A  VM  could  be  an  open-box  one  (shown  in  white), 
which is allowed to communicate with other VMs via 
legitimate  communications  channels,  or  a  closed-box 
one (shown in gray), which is completely isolated from 
other  VMs.  Security  policies  need  to  be  established 
and enforced by the VMM. Such a system architecture 
can  provide  many  desirable  properties  other  than 
isolation, e.g., extensibility, compatibility and security. 
As  an  example,  consider  an  online  banking 
application  running  in  one  of  the  closed-box  VMs. 
Since it involves the use of important secrets such as 
the user(cid:146)s password, cryptographic keys, bank account 
information, etc., it is isolated from all other VMs and 
is only allowed to communicate with the authenticated 
bank  server.  The  underlying  VMM  enforces  security 
policies  which  disallow  any  form  of  communications 
between  the  closed-box  VM  and  all  other  VMs.  This 
ensures that the adversary outside the closed-box VM 
has no access to the user(cid:146)s secrets. Even if there is an 
insider in the closed-box VM, e.g. a Trojan horse or a 
backdoor in the banking application itself, and it gains 
access to the secrets, it has no way to distribute them 
outside of the VM, except to the trusted bank server. 
flaw 
While this sounds safe at first glance, such software 
isolation can be broken by exploiting certain processor 
architecture  features.  As  described  in  section  3,  a 
recent  attack  [12]  on  a  hyper-threading  processor 
allows a user process to extract the RSA key of another 
process  which  is  performing  RSA  encryption.  No 
special equipment is needed in the attack and the attack 
does  not  even 
for 
exploitation. The spy process only needs to execute a 
series  of  memory  accesses  and  observe  the  timing 
while  the  victim  process  is  running  on  the  same 
processor. A VMM system running on top of a SMT 
processor  therefore  is  vulnerable  to  this  attack.  An 
adversary  outside  the  closed-box  VM  can  steal  the 
RSA keys involved in the online banking transaction. 
require  any  software 
A  very  important  observation  here  is  that  unlike 
other  security  problems,  the  information  leakage 
mechanism shown above does not break any protection 
mechanisms  and  can  escape  detection.  Even  with 
perfect  access  control,  information  flow  monitoring 
and  auditing,  information  can  still  be  leaked  out  by 
exploiting  processor  architecture  features,  without 
being  detected.  In  the  next  two  sections,  we  propose 
two  solutions  to  mitigate  SMT-based  covert  channel 
attacks and cache-based software side-channel attacks. 
5. Selective partitioning solution 
The  first  general  solution  approach  is  to  minimize 
resource  sharing,  and  hence  prevent  interference 
between  processes.  The  SMT/FU  covert  channel 
exploits  the  sharing  of  functional  units  by  multiple 
simultaneously  active  threads.  A  straightforward  way 
to  block  such  a  channel  is  to  disallow  any  other 
processes  from  running  when  a  protected  thread  is 
scheduled  for  execution.  This  strict  partitioning  can 
have  severe  performance  consequences.  This  can  be 
meliorated  by  allowing  protected  processes  to  be 
executed  with  other  processes,  only  disallowing  the 
simultaneous  execution  of  processes  that  should  be 
isolated from each other. We refer to this as a selective 
partitioning solution.  It is similar to a (cid:147)Chinese Wall(cid:148) 
separation policy, but at the hardware thread level.  
implemented 
in 
software, e.g., by having the OS enforce this restriction 
Selective  partitioning  can  be 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:29:18 UTC from IEEE Xplore.  Restrictions apply. 
Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06)0-7695-2716-7/06 $20.00  © 2006in  process  scheduling.  This  applies  scheduling 
restrictions  only  to  critical  processes  that  operate  on 
sensitive  data.  These  processes  may  be  cryptographic 
routines,  which  tend  to  have  small  working  sets 
compared  to  other  applications.  A  typical  mix  of 
applications  used  in  ordinary  PC  systems  consists  of 
mostly non-critical processes. 
Selectively  partitioning  can  also  be  based  on 
leveraging  existing  hardware  mechanisms.  For  the 
hardware  solution,  we  leverage  the  (cid:147)fairness  control(cid:148) 
mechanism  implemented  in  some  SMT  processors  to 
prevent  overuse  of  certain  shared  resources  by  a 
process.  For  example, 
in  Intel(cid:146)s  hyper-threading 
processor,  the  allocator  has  fairness  control  logic  for 
assigning  buffers  to  micro-ops  from  different  logical 
processors  and  the  instruction  scheduler  has  fairness 
control  on  the  number  of  active  entries  in  each 
scheduler(cid:146)s  queue.  Our  hardware  solution  proposes 
that  such  fairness  control  logic  can  be  leveraged  to 
mitigate  covert  channels  as  well.  For  the  SMT/FU 
covert channel we identified in section 3, the existing 
fairness  control  logic  in  the  instruction  scheduler  can 
be  modified  slightly  so  that,  when  necessary,  it 
allocates a fixed number of entries for each process in 
each  queue.  This  would  minimize  the  interference 
between  the  two  concurrent  processes running on the 
chip.  
The  performance  degradation  of  the  system  with 
our selective partitioning solution can be estimated as: 
           (3) 
where R0 is the throughput when there is no scheduling 
restriction, R1 the throughput when a critical process is 
running  on  the  processor,  and  p  is  the  probability  of 
the  occurrence  of  such  restricted  execution.  The 
relative throughput therefore can be written as: 
Rp
1
⋅+
Rp
)
1(
R
−
=
0
0
0