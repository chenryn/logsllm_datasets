BGP advertisement of a cloud from at least one machine within the
467
0.01%0.1%1%10%100%percent zones/ASNs/IPs ordered by DNS queries (log)0%20%40%60%80%100%cumulative percent DNS querieszonesASNsIPs10-510-410-310-210-1100101102103104queries per second over 24-hours (log)0.00.20.40.60.81.0cdf per IP addressavgmax -100%  -50% 0%50%100%percent gain or loss in DNS queries0.000.050.100.150.200.25pdf of weighted IP addressesSIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Schomp et al.
6
uniformly across the machines at sufficiently large volumes. How-
ever, resolvers that do not use a random ephemeral source port will
always be forwarded to the same machine.
Authoritative DNS Services: The authoritative nameservers
support the Authoritative DNS Hosting Service (ADHS). Enterprises
who wish to host their own DNS zones (e.g., “ex.com”) on Akamai’s
infrastructure are assigned a unique set of 6 different clouds called
a delegation set from the total 24 clouds, enabling the architecture
(cid:1) enterprises before adding additional clouds.
to support up to(cid:0)24
Enterprises add NS records, each corresponding to a cloud in the
delegation set, to every zone they own, along with the respective
parent zone in the DNS hierarchy. Adding the NS records to the
parent zone ensures that resolvers are directed to Akamai DNS, and
will query one of the 6 clouds to obtain an answer to DNS queries
for the enterprise’s zones. We discuss the design decision to use
unique delegation sets in §4.3.1.
The nameservers also host domains for the Content Delivery
Service (CDN). Enterprises using the CDN redirect a hostname in
their zone to Akamai DNS, e.g., “www.ex.com”⇒ “ex.edgesuite.net”,
The domain “edgesuite.net” is an entry point to the Akamai CDN
and is delegated to 13 anycast clouds2 because of its cross-enterprise
role. These human-readable hostnames are themselves redirected
to hostnames used by the CDN – e.g., “ex.edgesuite.net” ⇒
“a1.w10.akamai.net” – to add an additional layer of indirection and
control. Hostnames like “a1.w10.akamai.net” resolve to the CDN
edge servers that serve content. Domains like “w10.akamai.net” take
advantage of nameservers co-located with the wide CDN footprint
– which is deployed within 1,600 networks worldwide [54] – to
accelerate resolution of hostnames, as discussed in §5.2. Integration
with the GTM service is similar to CDN.
3.2 Supporting Components
We describe other components in Figure 5 that either publish meta-
data to authoritative nameservers or monitor them.
Mapping Intelligence: The Akamai mapping system [11, 36]
determines to which edge servers end-users are directed for content
delivery. Towards this end, Akamai DNS changes the IP address re-
turned for a hostname, in response to the query’s source IP address
or EDNS-Client-Subnet option [13]. While the mapping intelligence
determines what IP addresses should be returned, the nameservers
are charged with delivering that answer. In practice, this means the
mapping system publishes frequent metadata updates in reaction
to changing conditions, to which the nameservers subscribe.
Management Portal: Enterprises make modifications to their
DNS zones, GTM configurations, and CDN properties through the
Management Portal via the website or API, while DNS zones can
also be updated through zone transfers [29]. The Management
Portal validates the metadata and publishes it for consumption by
the nameservers.
Communication/Control System: This system provides
generic metadata delivery services using a publish/subscribe model.
The Mapping Intelligence and Management Portal publish metadata
to these systems and the nameservers request subscription from
these systems. Enterprise DNS zone files and configuration are
2We chose 13 delegations to match the model used by the root and many critical
toplevel domains.
468
Figure 5: Akamai DNS high-level architecture.
Figure 6: Architecture of a point of presence (PoP).
PoP, it advertises the cloud to the PoP’s BGP neighbors, or peers.
The number of peers per PoP varies from PoPs within eyeball net-
works peering with only that network to PoPs in Internet exchange
points (IXPs) having hundreds of peers. Features of BGP advertise-
ments, e.g., AS Path and BGP Communities [10], are controlled on
a per-peer basis.
Packets arriving at the router destined for one of the anycast
prefixes are forwarded to only one of the machines within the PoP
that advertises the prefix to the router using Equal-Cost-MultiPath
(ECMP) [20] by creating a hash from the tuple of (source IP ad-
dress/port, destination IP address/port). Because most resolvers use
a random ephemeral source port per DNS query [47], each DNS
query from the resolver may be routed to any of the machines in
the PoP advertising the prefix. DNS traffic spreads approximately
MappingIntelligenceAkamaiNOCCDataCollection/AggregationAuthoritativeNameserverManagementPortalMonitoring/AutomatedRecoveryEnterpriseCommunication/ControlSystemUI/APIUpdatesEnd-UserRecursiveResolverTrafficReportsAlertsDNSZoneDataControlMachineBGP-speakerNameserverMonitoringAgentMachineBGP-speakerNameserverMonitoringAgentPoP RouterPeersDNSBGPAkamai DNS
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
delivered via Akamai’s CDN using a proprietary protocol built
upon HTTP. Mapping intelligence requires near real-time delivery
for rapid reaction to changing network conditions and so uses
Akamai’s overlay multicast network [4, 25].
Monitoring/Automated Recovery: This system aggregates
health data across nameservers, tracks trends, and alerts human
operators in the Network Operations & Control Center (NOCC)
when anomalies occur. But, the speed of this process is bounded
by human operations, and our goal is to mitigate impact as quickly
as possible. Thus, a monitoring agent is deployed with each name-
server to continually detect and mitigate a variety of issues (§4.2).
Data Collection/Aggregation: Finally, metrics published by
nameservers are also compiled into reports displayed to enterprises
through the Management Portal.
4 RESILIENCY
Akamai DNS is a crucial component of the global Internet ecosys-
tem. As such, resiliency is factored into every aspect of its design.
We consider two types of resiliency: failure resiliency which is the
ability of the systems to tolerate failures either of the systems them-
selves or the underlying network (§4.2), and attack resiliency which
is the systems ability to protect itself from malicious attack (§4.3).
4.1 Anycast Failover Mechanism
Anycast failover is a key mitigation mechanism for events such
as a PoP failure. By withdrawing a prefix from one PoP, it allows
traffic to be rerouted to another PoP within the same cloud. The
time for such rerouting to occur is called failover time. We show
that failover time is small enough to justify its use in our system.
Experimental Methodology: We conduct experiments to mea-
sure failover time for two cases: advertising a prefix and withdraw-
ing a prefix in a 2-PoP anycast cloud (Figure 7). We select 267 CDN
edge servers – selected to roughly cover our geographic footprint –
to use as vantage points and instrument them to send DNS queries
to an IP address within a test prefix every 100 msec. When a name-
server receives one of these DNS queries, it responds uniquely
identifying its PoP. The vantage points log the time that the DNS
queries are sent and the response that was received (or timeout if
no response received).
Figure 7(a) shows our setup for measuring the impact of a new
advertisement. A nameserver within PoP Y is already advertising
the prefix and all vantage points are routed to Y. Next, a nameserver
in PoP X is instructed to advertise the prefix and the BGP-speaker
resident with the nameserver advertises the prefix to X’s router
shortly there after, triggering the router to update it’s routing table
and propagate the advertisement to its peers. Within 100 msec of X’s
router updating its routing table, the local vantage point within X
will issue a DNS query, receive a response identifying X, and log the
time the query was sent, tL. As the BGP update propagates through
the Internet, remote vantage points will also receive DNS responses
identifying X and log the time tX . We estimate failover time as
the time from the BGP advertisement to when the application is
routed to X as tX − tL. This calculation uses two different clocks.
All vantage points sync with the same set of NTP servers and we
estimate that the clock discrepancy is 7.4 msec average and 46ms in
the worst case across all pairs of vantage points. Combined with the
Figure 7: Experimental setup for evaluating failover times
for prefix (a) advertisement and (b) withdrawal.
100 msec measurement frequency, our measurements are accurate
to within [−50, 250] msec and overestimate failover time by 100±7.4
msec on average.
Figure 7(b) shows our setup for prefix withdrawal. The name-
server in PoP X withdraws the advertisement while PoP Y contin-
ues to advertise. Unlike with the advertisement experiment above
where it took some time for vantage points to be routed away
from Y, with withdrawals the vantage points stop receiving DNS
responses from X immediately. This is because at some point along
the path between the vantage point and X, the packet traverses a
router that has already updated its routing table. At that point, one
of two things can happen: (i) the packet will be re-routed eventu-
ally reaching Y, or (ii) the packet will bounce between routers with
divergent routing tables and ultimately be discarded when IP TTL
= 0. The former case results in instantaneous failover, while the
latter results in timeouts until the BGP routing tables converge. We
measure the failover time in the latter case as the time tϕ when the
vantage point sends the first DNS query that results in a timeout
to the time tY when the vantage point sends the first DNS query
that gets an answer from Y. This calculation depends upon a single
clock, making clock sync irrelevant.
For both the new advertisement and withdrawal experiments
above, we cycled through a random permutation of the 267 PoPs,
advertising and withdrawing the test prefix from each PoP X, using
the previous PoP in the permutation as Y, and measuring failover
time using the remaining PoPs as the vantage points. In each ex-
periment, we waited 5 minutes for the vantage points to fail over,
before continuing to the next PoP. Finally, to understand failover
for larger anycast clouds, we reran our experiments again cycling
through all 267 PoPs, and randomly selecting 20 other PoPs to act
as Y, rather than using a single PoP as in the first experiment.
Experimental Results: Figure 8 shows the failover time for
a new advertisement in the line “advertise 2 PoPs”. In 76% of the
measurements, failover time is under 1 sec. Further, some vantage
points experienced timeouts, i.e., were not routed to either Y or
X, but this occurred in only 3% of measurements. We also see
that the failover time for withdrawals is similar to that of a new
advertisement in line “withdraw 2 PoPs”3. However, the failover
time has a significant tail with 5.8% of the measurements taking 10
3The withdraw line has steps at our measurement granularity unlike the advertise line
which is smoothed due to clock jitter.
469
InternetWAuthoritativeNameservert∅Remote VantagePointAuthoritativeNameserverPoP RouterstY(b)InternetAuthoritativeNameservertXRemote VantagePointAuthoritativeNameserver(a)XYLocal Vantage PointtLABGPDNSXYXSIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Schomp et al.
themselves. Thus, Akamai DNS is built to tolerate failures and con-
tinue to operate – even if in a degraded state – until fully recovered.
Here, we cover a few specific failures and how the design mitigates
them, allowing Akamai DNS to continue answering DNS queries.
4.2.1 Machine-Level Failures. In large distributed networks like
Akamai DNS, it is not unusual for a small number of machines
to experience software or hardware failures at any given time.
Therefore, Akamai DNS is built to identify failures and shift DNS
query traffic to healthy machines.
The most common failure mode we observe is disk failure, but
any hardware subsystem (e.g. memory, network card) can fail. Hard-
ware failures often manifest in the nameserver software not re-
sponding to DNS requests, or responding slowly, or responding
with incorrect answers (e.g. answering based on stale data). Also,
despite our rigorous QA process, some bugs are only observable
in production due to a confluence of unpredictable events. These
bugs can manifest themselves in ways similar to hardware failures.
We deploy a common mitigation strategy to handle localized
failures. Every nameserver is monitored by an on-machine moni-
toring agent (Figure 6) that continually runs a suite of tests against
the nameserver and detects incorrect or missing responses. The
test suite includes DNS queries for each DNS zone and regression
tests for known failure cases. If a failure is detected, that machine is
self-suspended, the monitoring agent instructs the BGP-speaker to
withdraw anycast advertisement, resulting in traffic shifting to other
healthy machines. If all machines within a PoP are self-suspended,
the anycast failover mechanism of §4.1 will route the DNS requests
to other PoPs. But, there is a danger to self-suspension if the name-
server failure is widespread or the bug is in the monitoring agent
itself. Either could lead to widespread self-suspension, significantly
reducing capacity. The Monitoring/Automated Recovery system
(Figure 5) prevents such scenarios by limiting concurrent name-
server suspensions using a distributed consensus algorithm, and
preventing self-suspension on some nameservers (§4.2.3). In this
way, Akamai DNS is designed to always return an answer, even if
there are widespread failures.
4.2.2
Stale State. The metadata on which nameservers base
their answers can change rapidly, particularly the Mapping Intelli-
gence metadata (§3.2). The consequence of serving DNS answers
based on stale metadata can be poor performance or an outage for
end-users.
Typically, updates propagate in less than 1 second, however we
observe a small fraction of nameservers with stale metadata at any
time. Stale state can be caused by the scenarios described in §4.2.1,
but it can also occur for reasons independent of machine level
faults. One common cause of stale state is isolated connectivity
issues. Similar to hardware failure, isolated connectivity failures
are common in large networks with causes including hardware
failures in switches/routers, cable cuts, and misconfigurations. Once
connectivity is restored, the nameserver will have stale state for a
brief period until catching up. During this time, DNS queries could
be answered incorrectly, if not mitigated.
A particularly insidious case is a partial connectivity failure,
causing the nameservers to be unable to receive metadata from the
Akamai network, yet still able to receive DNS queries from some
subset of the Internet. The most common such failure mode is when
Figure 8: Failover time for clouds with 2 and 21 PoPs.
seconds or more. The tail includes measurements using 19% of PoPs
and all vantage points, so we conclude that it is likely not driven
by localized network issues at the time of our measurements.
Figure 8 also shows the results for 21-PoP experiments. The me-
dian failover time for both advertising and withdrawing decreases
by 200 msec in comparison with the 2-PoP case. The reason is that
the set of vantage points in the catchment of a PoP and the topologi-
cal distance a BGP update must travel from a PoP to a vantage point
are both smaller when the number of PoPs is larger. Thus, 2-PoP
failover likely captures the worst-case times for anycast failover.
Finally, because we wait 5 minutes for vantage points to failover,
it is possible that we do not observe failovers that take longer
than 5 minutes. We note, however, that in the 21 PoP withdraw
experiment we observed 0 vantage points that timed out for ≥5
minutes, indicating that very long failover times are extremely
unlikely. In conclusion, these results suggest that most resolvers
would failover within a second. Thus, anycast failover is a suitable
mechanism for making Akamai DNS failure resilient.
Relation to Prior Work: BGP update propagation through the
Internet has been studied before. In 2000, [27] observes that BGP
convergence for route advertisements typically takes 1-2 minutes
and route withdrawals greater than 2 minutes, with the time re-
quired varying among 5 different ISPs. More recently in 2011, [5]
measured propagation of a route advertisement from the Amster-
dam Internet Exchange (AMS-IX) to 90 vantage points around the
globe and observed an advertisement propagating to all in 38 sec-
onds and a withdrawal in 3 minutes. We complement these existing
studies by (i) updating findings to the state of BGP propagation as
of 2020, and (ii) covering the case of anycast advertisements where
the same prefix is advertised from multiple PoPs. Importantly, our
experiments are also the first to measure application-layer failover