and minimal architecture for (establishing a dynamic) root of trust.”
in 19th Annual Network and Distributed System Security Symposium
(NDSS), 2012, pp. 1–15.
[36] X. Carpent, G. Tsudik, and N. Rattanavipanon, “ERASMUS: efﬁcient
remote attestation via self-measurement for unattended settings,” in
Design, Automation & Test in Europe Conference & Exhibition (DATE).
IEEE, 2018, pp. 1191–1194.
[37] J. K. Zinzindohou´e, K. Bhargavan, J. Protzenko, and B. Beurdouche,
“Hacl*: A veriﬁed modern cryptographic library,” in 24th ACM Confer-
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:23:35 UTC from IEEE Xplore.  Restrictions apply. 
1652
ence on Computer and Communications Security (CCS). ACM, 2017,
pp. 1789–1806.
[38] J. Protzenko, J.-K. Zinzindohou´e, A. Rastogi, T. Ramananandro,
P. Wang, S. Zanella-B´eguelin, A. Delignat-Lavaud, C. Hrit¸cu, K. Bhar-
gavan, C. Fournet et al., “Veriﬁed low-level programming embedded in
F,” ACM on Programming Languages, vol. 1, no. ICFP, pp. 1–29, 2017.
[39] S. Underwood, mspgcc: A port of the GNU tools to the Texas Instruments
MSP430 microcontrollers, http://mspgcc.sourceforge.net/manual/, 2003.
[40] M. Howard and S. Lipner, The security development lifecycle. Microsoft
Press Redmond, 2006, vol. 8.
[41] A. Boileau, “Hit by a bus: Physical access attacks with ﬁrewire,”
Presentation, Ruxcon, vol. 3, 2006.
[42] A. T. Markettos, C. Rothwell, B. F. Gutstein, A. Pearce, P. G. Neu-
mann, S. W. Moore, and R. N. M. Watson, “Thunderclap: Exploring
vulnerabilities in operating system IOMMU protection via DMA from
untrustworthy peripherals,” in 26th Annual Network and Distributed
System Security Symposium (NDSS). The Internet Society, 2019.
[43] G. Kupfer, “Iommu-resistant DMA attacks,” Master’s thesis, Computer
Science Department, Technion, 2018.
[44] B. Ruytenberg, “Breaking Thunderbolt Protocol Security: Vulnerability
Report,” Apr. 2020. [Online]. Available: https://thunderspy.io/assets/
reports/breaking-thunderbolt-security-bjorn-ruytenberg-20200417.pdf
[45] M. van Dijk, S. K. Haider, C. Jin, and P. H. Nguyen, “Advanced power
side channel, cache side channel attacks, DMA attacks,” Presentation,
Department of Electrical & Computer Engineering, University of Con-
necticut, 2017.
[46] D. R. E. Gnad, J. Krautter, and M. B. Tahoori, “Leaky noise: New side-
channel attack vectors in mixed-signal IoT devices,” IACR Transactions
on Cryptographic Hardware and Embedded Systems, vol. 2019, no. 3,
pp. 305–339, 2019.
[47] V. Costan and S. Devadas, “Intel SGX Explained.” IACR Cryptology
ePrint Archive, vol. 2016, no. 086, pp. 1–118, 2016.
[48] Z. Wu, Z. Xu, and H. Wang, “Whispers in the hyper-space: High-
bandwidth and reliable covert channel attacks inside the cloud,”
IEEE/ACM Transactions on Networking, vol. 23, no. 2, pp. 603–615,
2015.
[49] P. Pessl, D. Gruss, C. Maurice, M. Schwarz, and S. Mangard, “DRAMA:
Exploiting DRAM addressing for cross-CPU attacks,” in 25th USENIX
Security Symposium, 2016, pp. 565–581.
[50] D. Ustiugov, P. Petrov, M. R. S. Katebzadeh, and B. Grot, “Bankrupt
covert channel: Turning network predictability into vulnerability,” in
14th USENIX Workshop on Offensive Technologies, WOOT, Aug. 2020.
[51] Y. Wang, A. Ferraiuolo, and G. E. Suh, “Timing channel protection for a
shared memory controller,” in 2014 IEEE 20th International Symposium
on High Performance Computer Architecture (HPCA).
IEEE, 2014, pp.
225–236.
[52] J. Van Bulck, D. Oswald, E. Marin, A. Aldoseri, F. D. Garcia, and
F. Piessens, “A tale of two worlds: Assessing the vulnerability of
enclave shielding runtimes,” in 26th ACM Conference on Computer and
Communications Security (CCS), Nov. 2019, pp. 1741–1758.
[53] X. Leroy, “Formal veriﬁcation of a realistic compiler,” Communications
of the ACM, vol. 52, no. 7, pp. 107–115, 2009.
[54] A. Guha, C. Saftoiu, and S. Krishnamurthi, “The essence of javascript,”
in ECOOP, 2010.
[55] P. Philippaerts, J. T. M¨uhlberg, W. Penninckx, J. Smans, B. Jacobs, and
F. Piessens, “Software veriﬁcation with verifast: Industrial case studies,”
Sci. Comput. Program., vol. 82, pp. 77–97, 2014.
[56] P. Kocher, J. Horn, A. Fogh, D. Genkin, D. Gruss, W. Haas, M. Ham-
burg, M. Lipp, S. Mangard, T. Prescher, M. Schwarz, and Y. Yarom,
“Spectre attacks: Exploiting speculative execution,” in 40th IEEE Sym-
posium on Security and Privacy (S&P), 2019.
[57] X. Yang, Y. Chen, E. Eide, and J. Regehr, “Finding and understanding
bugs in C compilers,” in PLDI, 2011.
[58] D. MacKenzie and G. Pottinger, “Mathematics, technology, and trust:
Formal veriﬁcation, computer security, and the us military,” IEEE Annals
of the History of Computing, vol. 19, no. 3, pp. 41–59, 1997.
[59] D. E. Bell and L. J. La Padula, “Secure computer system: Uniﬁed
exposition and multics interpretation,” Mitre Corporation, Tech. Rep.,
1976.
[60] P. A. Karger and R. R. Schell, “Thirty years later: Lessons from
the multics security evaluation,” in 18th Annual Computer Security
Applications Conference (ACSAC).
IEEE Computer Society, 2002, pp.
119–126.
[61] “Common criteria for information technology security evaluation,” https:
//www.commoncriteriaportal.org/, accessed 2021-08-18.
[62] R. J. Anderson, Security engineering - a guide to building dependable
distributed systems (3. ed.). Wiley, 2020.
[63] S. Goldwasser and S. Micali, “Probabilistic encryption,” Journal of
computer and system sciences, vol. 28, no. 2, pp. 270–299, 1984.
[64] M. Burrows, M. Abadi, and R. M. Needham, “A logic of authentication,”
Royal Society of London: Mathematical and Physical Sciences, vol. 426,
no. 1871, pp. 233–271, 1989.
[65] G. Lowe, “Breaking and ﬁxing the Needham-Schroeder public-key pro-
tocol using FDR,” in International Workshop on Tools and Algorithms
for the Construction and Analysis of Systems.
Springer, 1996, pp.
147–166.
[66] J. P. Degabriele, K. Paterson, and G. Watson, “Provable security in the
real world,” IEEE Security & Privacy, vol. 9, no. 3, pp. 33–41, 2010.
[67] N. Koblitz and A. Menezes, “Critical perspectives on provable security:
Fifteen years of “another look” papers,” Advances in Mathematics of
Communications, vol. 13, no. 4, p. 517, 2019.
[68] M. Vanhoef and F. Piessens, “Key reinstallation attacks: Forcing nonce
reuse in wpa2,” in 24th ACM Conference on Computer and Communi-
cations Security (CCS), 2017, pp. 1313–1328.
[69] M. Barbosa, G. Barthe, K. Bhargavan, B. Blanchet, C. Cremers, K. Liao,
and B. Parno, “Sok: Computer-aided cryptography,” in 42nd IEEE
Symposium on Security and Privacy, 2020.
[70] D. McMorrow, “Science of cyber-security,” MITRE Corporation JASON
Program Ofﬁce, Tech. Rep. JSR-10-102, Nov. 2010.
[71] G. Klein, K. Elphinstone, G. Heiser, J. Andronick, D. Cock, P. Derrin,
D. Elkaduwe, K. Engelhardt, R. Kolanski, M. Norrish et al., “sel4:
Formal veriﬁcation of an os kernel,” in 22nd ACM SIGOPS Symposium
on Operating Systems Principles (SOSP), 2009, pp. 207–220.
[72] A. Goel, S. Krstic, R. Leslie, and M. R. Tuttle, “Smt-based system
veriﬁcation with dvf.” in SMT@ IJCAR, 2012, pp. 32–43.
APPENDIX
A. Extensions to VRASED
Multiple derived architectures have been published that are
built on the open-source VRASED research prototype and use
its security arguments as the basis of their own.
1) VRASEDA [15]: Veriﬁer authentication: To prevent an
attacker from generating many attestation requests to over-
whelm the prover’s computational resources, a modiﬁcation to
SW-Att is proposed in the original VRASED paper [15]. This
variant, referred to as VRASEDA in our paper, authenticates
attestation requests before starting the expensive attestation.
Attestation requests need to contain an authentication token,
which is calculated by calling HMAC on the challenge with
the shared master key. Hence, in VRASEDA, guessing the
correct authentication value for a given request should be
computationally infeasible without key knowledge.
2) PURE [19]: Proofs of update, reset and erasure:
In addition to the malware detection provided by VRASED,
PURE also offers remote capabilities to erase the data section
of the device, update the program code, and reset the device
– steps that need to be taken if malware is detected, or simply
in case of a software update.
3) APEX [20]: Proofs of execution: APEX provides au-
thenticated sensor readings and actuation: proof that the device
executed the desired program (with a freshness guarantee) and
that the results of the execution have not been tampered with.
This is meant to solve the problem of malware infections
between the time of attestation and execution.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:23:35 UTC from IEEE Xplore.  Restrictions apply. 
1653
4) RATA [21]: TOCTOU avoidance: Built on VRASEDA,
this extension addresses the above time-of-check-time-of-use
(TOCTOU) problem more broadly. It not only allows exe-
cuting the software combined with an attestation, but it keeps
track of whether the software has been tampered with between
two attestations. It also allows for performance improvements,
since subsequent attestations do not require running HMAC
again as long as the software has not been altered.
5) Tiny-CFA [22]: Control-ﬂow attestation: Tiny-CFA
enables the veriﬁer to conduct control ﬂow attestation on the
prover device. This is the only VRASED extension whose
additional security properties were not veriﬁed.
B. VRASED assumptions
VRASED [15] explicitly assumes the following to hold for
the implementation of the core:
A1 Program counter: The PC register contains the address
of the executing instruction.
A2 Memory address: Whenever memory is accessed, the
address bus contains its address, and the Wen/Ren signals
are active.
A3 DMA: Whenever DMA accesses the memory, the DMA
address bus contains the accessed memory address and
the DM Aen signal is high.
A4 MCU reset: The reset handling cannot be modiﬁed, and
registers are zeroed during reset.
A5 Interrupts: Triggering an interrupt sets the corresponding
irq signal.
Furthermore, VRASED formulates two additional assump-
tions for the trusted compiler:
A6 Callee-Saves-Register: All registers used in a function
are cleared before exiting.
A7 Semantic preservation: Functional correctness is pre-
served during compilation from C to MSP430 assembly.
C. Additional (not directly exploitable) VRASED ﬂaws
This appendix lists an additional falsiﬁed assumption and
an unmodeled feature that we did not ﬁnd to be directly
exploitable within the attacker model.
1) Compiler not clearing dirty register values: Assump-
tion A6 unequivocally states that the compiler should clear all
registers that are used in a function. The paper also claims
that the msp430-gcc compiler used in the implementation
satisﬁes this assumption.
a) Broken assumption: We found, however, that regis-
ters r12-r15 are explicitly designated as “caller-save” in
the msp430-gcc application binary interface (ABI) [39,
§Register usage]. This means that their value may be clob-
bered, and the compiler is not required to restore or clear
them at the end of the function.
If the HMAC function uses any of these registers to tem-
porarily save key-dependent values, those may leak out and
be visible to untrusted code, since the register values are not
cleaned up manually by SW-Att either.
b) Attack: We experimentally conﬁrmed that caller-save
registers are indeed clobbered after execution of SW-Att. How-
ever, in our experiments, no sensitive data was leaked with the
current implementation of the HMAC function and compiler
settings, but this is not guaranteed to always be true if the
compiler or the SW-Att implementation changes.
c) Mitigation: The most straightforward solution is to
insert a custom assembly stub at the trusted exit point of
SW-Att to clear all registers that can contain key-dependent
data. Such an ABI sanitization stub resembles existing security
solutions and best practices to prevent leakage through CPU
registers in Sancus [16] and, more generally, in Intel SGX
enclave shielding runtimes [52].
2) Reading the key with the debug unit: The openMSP430
architecture comes equipped with a debug unit connected to
the core through UART or I2C. This unit enables its user to
read or write data in memory, pause the execution of the CPU,
and read register contents.
a) Unmodeled capability: While the debug unit is not
mentioned at all in the paper, it is included in the open-source
implementation of VRASED.
b) Attack: Operating the debug unit requires physical
access, so strictly speaking, it lies outside the attacker model
of VRASED. However, since the debug unit
lies outside
VRASED’s veriﬁcation perimeter, it could also be extended
to be controlled from software without violating any of the
core assumptions in Appendix B (cf. the discussion in Sec-
tion VI-D). As a more concrete example, even when the debug
unit would adhere to the memory interface (A2) and interrupt
(A5) assumptions, it could still be conﬁgured to schedule
breakpoints or trivially read out CPU registers to leak the key.
c) Mitigation: This issue highlights the security risks of
development interfaces that fall outside the veriﬁcation perime-
ter. The easiest ﬁx is to remove the debug unit altogether
from the implementation, as it only causes possible sources
of information leakage.
D. Timing attack on VRASEDA
Conforming to the attack described in Section VI-C3, Ta-
ble III shows the number of cycles the entire execution of
SW-Att takes with different authentication tokens given for the
same key-challenge pair. For the given pair, the correct token
starts with the bytes {0x59, 0x76}, as can be seen from
the increasing execution times. Following this guessing for
one byte at a time, the entire VRASEDA authentication token
can be extracted in linear effort.
TABLE III. Execution time of VRASEDA for authentication guesses.
VRF_AUTH[32]
{0x1}
{0x0}
{0x59}
{0x59, 0x75}
{0x59, 0x76}
Execution time (cycles)
210,641
210,641
210,654
210,654
210,667
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:23:35 UTC from IEEE Xplore.  Restrictions apply. 
1654
E. Analysis of end-to-end RA security argument
Soundness of remote attestation: VRASED’s end-to-end
remote attestation security argument [15] ﬁrst relies on a
separate argument
that proves the soundness of the RA
scheme. The soundness argument crucially relies on the
proven functional correctness of the HACL* cryptographic
library [37]. However, no argumentation is given how the
unveriﬁed openMSP430 core satisﬁes the machine model
assumed by HACL*. Particularly, even the formally veriﬁed
HACL* library will clearly break when executed on a core
which, for instance, performs subtractions for add or reverses
the direction of jmp instructions. Observe that there is no
assumption whatsoever that forbids this by saying that the core
should be bug-free, free of hardware Trojans, or even adhere
to some (formalized) MSP430 ISA speciﬁcation. Hence, the
soundness of the attestation and the proven guarantees of
HACL* are trivially broken by a malicious core that e.g.,
performs subtractions for addition instructions and vice versa.
Security of remote attestation: We ﬁrst provide an overview
of how the end-to-end RA security property relies on two as-
sumptions, which are further decomposed into sub-arguments.
Enumeration items on the same level represent preconditions
(which all need to be satisﬁed) for their parent item. ‘’ indi-
cates that the given step could be bypassed by a misbehaving
core that still satisﬁes A1-A5. Steps for which the proof uses
a model derived from Verilog are typeset in italics.
1) RA soundness 
2) The attacker does not learn the key 
a) The key can only be learned through the memory 
We already argued above how soundness (step 1) may
be broken. Section VI-D, furthermore, contains a high-level
description of how a misbehaving core could be constructed
that still satisﬁes A1-A5, but breaks the assumption that
HACL* prevents the key from leaking through SW-Att timing
(step 2(a)iii). That section similarly describes how such a core
could break the assumption that a reset never leaks the key
(step 2c).
Step 2(a)i can be falsiﬁed similarly to the previous timing
example, but in this case the timing of an unprotected instruc-
tion can change depending on the previously saved value of
the key to falsify the claim in a misbehaving core.
For step 2(a)ii, leakage through register values is claimed to
be eliminated by the assumption A6 and the secure reset prop-
erty (P3). These prevent register values to be leaked after both
successful and unsuccessful executions of SW-Att. Assumption
A6 is directly falsiﬁed in Appendix C1. Furthermore, even
when the A6 and P3 conditions are met, we can once again
construct a malicious core to leak the key by not following the
implicitly expected read and write semantics of registers. This
core saves the value of the key in a shadow register during the
HMAC calculation, and writes it to one of the real registers
after the cleanup at the end of SW-Att execution.
For step 2d, it is stated that writes by SW-Att to the HMAC
region do not have to be covered by the lemma, as this region
cannot contain the key. This is directly falsiﬁed by our stack
pointer overwrite attack (Section VI-C2). Moreover, a core
could conceivably be constructed that simply dumps the key
in the M R memory region after SW-Att execution.
i) The key can only be learned through registers,
memory, or SW-Att timing 
ii) A6 forbids leakage through registers after exiting
SW-Att 
iii) HACL* prevents all possible SW-Att timing leak-
ages 
b) Lemma 2: reading the key directly, or data that SW-Att
wrote to unprotected regions will cause a reset 
c) Resets do not leak the key 
d) The shared M R region cannot contain the key 
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:23:35 UTC from IEEE Xplore.  Restrictions apply. 
1655