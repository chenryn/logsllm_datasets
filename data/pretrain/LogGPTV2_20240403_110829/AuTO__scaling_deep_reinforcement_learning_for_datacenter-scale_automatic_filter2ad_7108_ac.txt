Deep Deterministic Policy Gradient (DDPG) [35] is an
extension of DPG algorithm, which exploits deep learning
techniques [41]. We use DDPG as our model for the opti-
mization problem and explain how it works in the following.
Same as DPG, DDPG is also an actor-critic [12] algorithm,
and it maintains four DNNs. Two DNNs, critic Qθ Q (s,a) and
actor µθ µ (s) with weights θ Q and θ µ, are trained on sampled
mini-batches of size N , where an item represents an experi-
enced transition tuple (si ,ai ,ri ,si +1) while the agent interacts
with the environment. The DNNs are trained on random
samples, which are stored in a buer, in order to avoid cor-
related states which cause the DNNs to diverge [41]. The
other two DNNs, target actor µ(cid:48)
(cid:48) (s,a),
are used for smooth updates of the actor and critic networks,
respectively (Algorithm (1) [35]). The update steps stabilize
training the actor-critic networks and achieve state-of-the-
art results on continuous space actions [35]. AuTO applies
DDPG for optimizing threshold values to achieve better ow
scheduling decisions.
DRL formulation Next, we show that the optimization of
thresholds can be formulated as an actor-critic DRL problem
solvable by DDPG. We rst develop an optimization problem
of choosing an optimal set of thresholds {αi} to minimize
the average FCT of ows. Then we translate this problem
into DRL problem to be solved using DDPG algorithm.
and target critic Q
Denote the cumulative density function of ow size distri-
bution as F (x ), thus F (x ) is the probability that a ow size
is no larger than x. Let Li denote the number of packets a
given ow brings in queue Qi for i=1,...,K. Thus, E[Li]≤
(cid:48)
θ Q
θ
AuTO: Scaling Deep Reinforcement Learning
for Datacenter-Scale Automatic Traic Optimization
Figure 6: Comparison of deep stochastic and deep de-
terministic policies.
(αi−αi−1)(1−F (αi−1)). Denote ow arrival rate as λ, then the
packet arrival rate to queue Qi is λi =λE[Li]. The service rate
for a queue depends on whether the queues with higher pri-
orities are all empty. Thus, P1 (highest priority) has capacity
µ1=µ where µ is the service rate of the link. The idle rate
of Q1 is (1−ρ1) where ρi =λi /µi is the utilization rate of Qi.
Thus, the service rate of Q2 is µ2=(1−ρ1)µ since its service
rate is µ (the full link capacity) given that P1 is empty. We
j=0(1−ρj )µ, with ρ0=0. Thus, Ti =1/(µi−λi ) which
have µi =Πi−1
is the average delay of queue i assuming M/M/1 queues. For
a ow with size in [αi−1,αi ), it experiences the delays in dif-
ferent priority queues up to the i-th queue. Denote Ti as the
average time spent in the i-th queue. Let imax (x ) be the in-
by:(cid:80)imax (x )
dex of the smallest demotion threshold larger than x. So the
average FCT for a ow with size x, T (x ), is upper-bounded
Let ❕i =F (αi )−F (αi−1) denote the percentage of ows with
sizes in [αi−1,αi ). Thus, ❕i is the gap between two consecu-
tive thresholds. Using ❕i to equivalently express αi, we can
formulate the FCT minimization problem as6:
Ti.
i =1
K(cid:88)
K(cid:88)
K(cid:88)
l(cid:88)
T ({❕})=
m=1
l =1
i=1,...,K−1
(❕l
(Tl
m=l
l =1
(7)
❕m )
Tm )=
min
{❕}
subject to ❕i≥0,
We proceed to translate Problem (7) into a DRL problem.
State space: In our model, states are represented as the set of
the set of all nished ows, Fd, in the entire network in the
current time step. Each ow is identied by its 5-tuple[8, 38]:
source/destination IP, source/destination port numbers, &
transport protocol. As we report only nished ows, we also
record the FCT and ow size as ow attributes. In total, each
ow has 7 features.
Action Space: The action space is computed by a centralized
agent, sRLA. At time step t, the action provided by the agent
i }.
is a set of MLFQ threshold values {α t
((cid:80)i
6For a solution to this problem, e.g. {❕(cid:48)
{α(cid:48)
i } with α(cid:48)
j =1❕j ), where F −1
=F −1
i
i }, we can retrieve the thresholds
(·) is the inverse of F (·).
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Rewards: Rewards are delayed feedback to the agent on how
good its actions are for the previous time step. We model the
reward as the ratio between objective functions of two con-
secutive time steps: rt = T t−1
. It signals if the previous actions
T t
have resulted in a lower average FCT, or it has degraded the
overall performance.
DRL algorithm We use the update rule specied by Equa-
tion (4) (Algorithm 1). The DNN computes ❕i’s for each newly
received state from a host, and stores a tuple: (st ,at ,rt ,st +1)
in its buer for later learning. Reward rt and the next state
st +1 are only known when the next update comes from the
same host, so the agent buers st and at until all needed in-
formation is received. Updates of parameters are performed
in random batches to stabilize learning and to reduce prob-
ability of divergence [35, 41]. The reward rt is computed
at a host at step t and is compared to the previous average
FCT. Based on the comparison, an appropriate reward (either
negative or positive) is produced which is sent to the agent
as a signal for evaluating action at. By following Algorithm
1, the system can improve the underlying actor-critic DNNs
and converge to an solution for Problem (7).
4.2 Optimizing Long Flows
The last threshold, αK−1, separates long ows from short
ows by sRLA, thus αK−1 is updated dynamically according
to current trac characteristics, in contrast to prior works
with xed threshold for short and long ows [1, 22]. For long
ows and lRLA, we use a PG algorithm similar to the ow
scheduling problem in §2.2, and the only dierence is in the
action space.
Action Space: For each active ow f , at time step t, its cor-
responding action is {Priot ( f ),Ratet ( f ),Patht ( f )}, where
Priot ( f ) is the ow priority, Ratet ( f ) is the rate limit, and
Patht ( f ) is the path to take for ow f . We assume the paths
are enumerated in the same way as in XPath [32].
State space: Same as §2.2, states are represented as the set
of all active ows, F t
,
a, and the set of all nished ows, F t
d
in the entire network at current time step t. Apart from its
5-tuple [8, 38], each active ow has an additional attribute:
its priority; each nished ow has two additional attributes:
FCT and ow size.
Rewards: The reward is obtained for the set of nished ows
. Choices for the reward function can be: dierence or
F t
d
ratios of sending rate, link utilization, and throughput in
consecutive time steps. For modern datacenters with at least
10Gbps link speed, it is not easy to obtain timely ow-level in-
formation for active ows. Therefore, we choose to compute
reward with nished ows only, and use the ratio between
the average throughputs of two consecutive time steps as
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
L. Chen et al.
reward, as in Equation (3). The reward is capped to achieve
quick convergence [35].
5 IMPLEMENTATION
In this section, we describe the implementation. We develop
AuTO in Python 2.7. The language choice facilitates inte-
gration with modern deep learning frameworks [17, 45, 57],
which provide excellent Python interfaces [45]. The current
prototype uses the Keras [17] deep learning library (with
TensorFlow as backend).
5.1 Peripheral System
PS is a daemon process running on each server. It has a
Monitoring Module (MM) and an Enforcement Module (EM).
The MM thread collects information about ows including
recently nished ows and the presently active long ows (in
the last queue of MLFQ). At the end of each period, the MM
aggregates collected information, and sends to CS. The PS’s
EM thread performs tagging based on the MLFQ thresholds
on currently active ows, as well as routing, rate limiting,
and priority tagging for long ows. We implement a Remote
Procedure Call (RPC) interface for communications between
PS and CS. CS uses RPC to set MLFQ thresholds and to
perform actions on active long ows.
5.1.1 Monitoring Module (MM):. For maximum eciency,
the MM can be implemented as a Linux kernel module, as
in PIAS[8]. However, for the current prototype, since we
are using a ow generator (as seen in [8, 10, 20]) to produce
workloads, we choose to implement the MM directly inside
the ow generator. This choice allows us to obtain the ground
truth and get rid of other network ows that may interfere
with the results.
For long ows (ows in the last queue of MLFQ), every
T seconds, MM merges nl active long ows (each with 6
attributes), and ml nished long ows (each with 7 attributes)
into an list. For short ows (in the rst few queues of MLFQ)
in the same period, MM collects ms nished ows (each
with 7 attributes) into an list. Finally, MM concatenates the
two lists and sends them to CS as an observation of of the
environment.
AuTO’s parameters, {nl ,ml ,ms}, are determined by trac
load andT : for each server, nl (ml) should be the upper-bound
of number of active (nished) long ows within T , and ms
should also be the upper-bound of nished short ows. In the
case that the actual number of active (nished) ow is less
than {nl ,ml ,ms}, the observation vector is zero-padded to
the same size of the corresponding agent’s DNN(s). We make
this design choice because the number of input neurons
of the DNN in CS is xed, therefore can take only xed-
sized inputs. We leave dynamic DNN and recurrent neural
network structure as future work. For the current prototype
and experiments on the prototype, since we control the ow
generator, it is easy to comply with this constraint. We choose
{nl =11,ml =10,ms =100} in the experiments.
5.1.2 Enforcement Module (EM):. EM receives actions
from CS periodically. The actions include new MLFQ thresh-
olds, and TO decisions on local long ows. For MLFQ thresh-
olds, EM builds upon the PIAS [8] kernel module, and adds
dynamic conguration of demotion thresholds.
For short ows, we leverage ECMP [30] for routing and
load-balancing, which does not require centralized per-ow
control, and DCTCP [3] for congestion control.
For long ows, the TO actions include priority, rate lim-
iting, and routing. EM leverages the same kernel module
for priority tagging. Rate limiting is done using hierarchi-
cal token bucket (HTB) queueing discipline in Linux trac
control (tc). EM is congured with a parent class in HTB
with outbound rate limit to represent the total outbound
bandwidth managed by CS on this node. When a ow de-
scends into the last queue in MLFQ, EM creates a HTB lter
matching the exact 5-tuple for that ow. When EM receives
rate allocation decisions from the CS, EM updates the child
class of the particular ow by sending Netlink messages to
Linux kernel: the rate of the TC class is set as the rate that
centralized scheduler decides, and its ceiling is set to the
smaller of the original ceiling and twice of the rates from CS.
5.2 Central System
CS runs RL agents (sRLA & lRLA) to make optimized TO deci-
sions. Our implemented CS follows a SEDA-like architecture
[58] when handling incoming updates and sending actions to
the ow generating servers. The architecture is subdivided
into dierent stages: http request handling, deep network
learning/processing, and response sending. Each stage has
its own process(es) and communicate through queues to pass
required information to the next stage. Such an approach
ensures that multiple cores of the CS server are involved in
handling the requests from the hosts and load is distributed.
The multi-processing architecture has been adopted due to
the Global lock problem [24] in the CPython implementa-
tion of the Python programming language. The states and
actions are encapsulated at the CS as an "environment" (sim-
ilar to [47]), with which the RL agents can interact directly
and programmatically.
5.2.1
sRLA. As discussed in §4.1, we use Keras to im-
plement the sRLA running the DDPG algorithm with the
aforementioned DNNs (actor, critic, target actor, and target
critic).
Actors: The actors have two fully-connected hidden layers
with 600 and 600 neurons, respectively, and the output layer
with K−1 output units (one for each threshold). The input
AuTO: Scaling Deep Reinforcement Learning
for Datacenter-Scale Automatic Traic Optimization
layer takes states (700 features per-server (ms =100)) and
outputs MLFQ thresholds for a host server for time step t.
Critics: The critics are implemented with three hidden layers,
thus the networks are a bit more complicated as compared
to the actor network. Since the critic is supposed to ’criticize’
the actor for bad decisions and ’compliment’ for good ones,
the critic neural network also takes as its input the outputs of
the actor. However, as [53] suggests, the actor outputs are not
direct inputs, but are only fed into the critic’s network at a
hidden layer. Therefore, the critic has two hidden layers same
as the actor and one extra hidden layer which concatenates
the actor’s outputs with the outputs of its own second hidden
layer, resulting in one additional hidden layer. This hidden
layer eventually is fed into the output layer consisting of one
output unit - approximated value for the observed/received
state.
The neural networks are trained on a batch of observa-