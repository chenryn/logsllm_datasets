title:Scheduling Mix-flows in Commodity Datacenters with Karuna
author:Li Chen and
Kai Chen and
Wei Bai and
Mohammad Alizadeh
Scheduling Mix-ﬂows in Commodity Datacenters
with Karuna
Li Chen, Kai Chen, Wei Bai, Mohammad Alizadeh(MIT)
SING Group, CSE Department
Hong Kong University of Science and Technology
ABSTRACT
Cloud applications generate a mix of ﬂows with and without
deadlines. Scheduling such mix-ﬂows is a key challenge; our
experiments show that trivially combining existing schemes
for deadline/non-deadline ﬂows is problematic. For exam-
ple, prioritizing deadline ﬂows hurts ﬂow completion time
(FCT) for non-deadline ﬂows, with minor improvement for
deadline miss rate.
We present Karuna, a ﬁrst systematic solution for schedul-
ing mix-ﬂows. Our key insight is that deadline ﬂows should
meet their deadlines while minimally impacting the FCT of
non-deadline ﬂows. To achieve this goal, we design a novel
Minimal-impact Congestion control Protocol (MCP) that han-
dles deadline ﬂows with as little bandwidth as possible. For
non-deadline ﬂows, we extend an existing FCT minimiza-
tion scheme to schedule ﬂows with known and unknown
sizes. Karuna requires no switch modiﬁcations and is back-
ward compatible with legacy TCP/IP stacks. Our testbed
experiments and simulations show that Karuna effectively
schedules mix-ﬂows, for example, reducing the 95th per-
centile FCT of non-deadline ﬂows by up to 47.78% at high
load compared to pFabric, while maintaining low (<5.8%)
deadline miss rate.
CCS Concepts
•Networks → Transport protocols;
Keywords
Datacenter network, Deadline, Flow scheduling
1.
INTRODUCTION
User-facing datacenter applications (web search, social
networks, retail, recommendation systems, etc.) often have
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than the author(s) must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
SIGCOMM ’16, August 22–26, 2016, Florianopolis, Brazil
© 2016 Copyright held by the owner/author(s). Publication rights licensed to
ACM. ISBN 978-1-4503-4193-6/16/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2934872.2934888
stringent latency requirements, and generate a diverse mix
of short and long ﬂows with strict deadlines [3, 22, 38, 39].
Flows that fail to ﬁnish within their deadlines are excluded
from the results, hurting user experience, wasting network
bandwidth, and incurring provider revenue loss [39]. Yet, to-
day’s datacenter transport protocols such as TCP, given their
Internet origins, are oblivious to ﬂow deadlines and perform
poorly. For example, it has been shown that a substantial
fraction (from 7% to over 25%) of ﬂow deadlines are not met
using TCP in a study of multiple production DCNs [39].
Meanwhile, ﬂows of other applications have different per-
formance requirements; for example, parallel computing ap-
plications, VM migration, and data backups impose no spe-
ciﬁc deadline on ﬂows but generally desire shorter comple-
tion time. Consequently, a key question is: how to schedule
such a mix of ﬂows with and without deadlines? To handle
the mixture, a good scheduling solution should simultane-
ously:
• Maximize deadline meet rate for deadline ﬂows.
• Minimize average ﬂow completion time (FCT) for non-
deadline ﬂows.
• Be practical and readily-deployable with commodity hard-
ware in today’s DCNs.
While there are many recent DCN ﬂow scheduling solu-
tions [3–5, 22, 30, 38, 39], they largely ignore the mix-ﬂow
scheduling problem and cannot meet all of the above goals.
For example, PDQ [22] and PIAS [5] do not consider the
mix-ﬂow scenario, while pFabric [4] simply prioritizes dead-
line ﬂows over non-deadline trafﬁc, which is problematic
(§2). Furthermore, many of these solutions [4, 22, 30, 39]
require non-trivial switch modiﬁcations or complex arbitra-
tion control planes, making them hard to deploy in practice.
We observe that the main reason prior solutions such as
pFabric [4], or more generally, EDF-based (Earliest Dead-
line First) scheduling schemes, suffer in the mix-ﬂow sce-
nario is that they complete deadline ﬂows too aggressively,
thus hurting non-deadline ﬂows. For example, since pFabric
strictly prioritizes deadline ﬂows, they aggressively take all
available bandwidth and (unnecessarily) complete far before
their deadlines, at the expense of increasing FCT for non-
deadline ﬂows. The impact on non-deadline ﬂows worsens
with more deadline trafﬁc, but is severe even when a small
fraction (e.g., 5%) of all trafﬁc has deadlines (see §2.2).
Our key insight to solve the mix-ﬂow scheduling problem
is that deadline ﬂows, when fulﬁlling their primary goal of
meeting deadlines, should minimally impact FCT for non-
deadline ﬂows. This is based on the assumption that dead-
lines reﬂect actual performance requirements of applications,
and there is little utility in ﬁnishing a ﬂow earlier than its
deadline. To this end, we design MCP, a novel distributed
rate control protocol for deadline ﬂows. MCP takes the min-
imum bandwidth needed to complete deadline ﬂows barely
before their deadlines (§4), thereby leaving maximal band-
width to complete non-deadline ﬂows quickly.
MCP ﬂows walk a thin line between minimal-impact com-
pletion and missing deadlines, and must therefore be pro-
tected from any aggressive non-deadline ﬂows. Thus, we
leverage priority queues available in commodity switches
and place MCP-controlled deadline ﬂows in the highest pri-
ority queue. For non-deadline ﬂows, we place them in the
lower priority queues and use an aggressive rate control (e.g.,
DCTCP [3]) to take the bandwidth left over by MCP. Fur-
ther, we extend the PIAS scheduling algorithm [6] to jointly
schedule non-deadline ﬂows with known or unknown sizes
among the multiple lower priority queues, in order to mini-
mize their FCT (§5.2).
Taken together, we develop Karuna, a mix-ﬂow schedul-
ing system that simultaneously maximizes deadline meet rate
for deadline ﬂows, and minimizes FCT for non-deadline ﬂows.
Essentially, Karuna trades off higher FCT for deadline ﬂows,
for which the key performance requirement is meeting dead-
lines, to improve FCT for non-deadline ﬂows. Karuna makes
this tradeoff deliberately to tackle this multi-faceted mix-
ﬂow problem. Karuna does not require any switch hardware
modiﬁcations or complex control plane for rate arbitration,
and is backward-compatible with legacy TCP/IP stacks. We
further identify and address a few practical issues such as
starvation and trafﬁc variation (§6).
We implement a Karuna prototype (§7) and deploy it on
a small testbed with 16 servers and a Broadcom Gigabit
Ethernet switch. On the end host, we implement Karuna
as a Linux kernel module that resides, as a shim layer, be-
tween the Network Interface Card (NIC) driver and TCP/IP
stack, without changing any TCP/IP code. On the switch,
we enable priority queueing and Explicit Congestion Noti-
ﬁcation (ECN), which are both standard features on current
switching chips. Our implementation experience suggests
that Karuna is readily deployable in existing commodity dat-
acenters.
We evaluate Karuna using testbed experiments and large-
scale ns-3 simulations with realistic workloads (§8). Our re-
sults show that Karuna maintains high deadline completion
while signiﬁcantly lowering FCT for non-deadline ﬂows. For
example, it reduces the 95th percentile FCT of non-deadline
ﬂows by up to 47.78% at heavy load compared to a clean-
slate design, pFabric [4], while still maintaining low (<5.8%)
deadline miss rate. Furthermore, our simulations show that
Karuna is effective in handling starvation, and is resilient to
trafﬁc variations and multiple bottlenecks.
2. BACKGROUND AND MOTIVATION
To motivate our design, we identify 3 types of ﬂows in
DCNs, and show performance trade-offs with existing schedul-
ing schemes for mix-ﬂows.
2.1 Application examples of 3 ﬂow types
Type 1: Flows with deadlines: Applications such as web
search, recommendation, and advertisement usually gener-
ate ﬂows with deadlines [38].1 User experience of these
applications is affected by latency, and hence they enforce
strict deadlines on network ﬂows. The ﬂows are useful to
the application if, and only if, they complete within the dead-
line [39]. In other words, the primary performance metric for
this type of trafﬁc is the deadline miss rate, i.e. the fraction
of ﬂows that miss their deadline.2 If the trafﬁc is only of this
type, EDF-based proposals (e.g., pFabric [4] with priority
given to ﬂows with shorter deadlines, or PDQ [22] using re-
maining deadline as the ﬂow criticality) are the best known
schemes to minimize deadline miss rate.3
Type 2: Flows without deadlines but known sizes: Ap-
plications such as VM migration and data backup generate
ﬂows without strict latency requirements, but generally de-
sire short completion times. Further, the sizes of these ﬂows
are usually known before transmission. If the trafﬁc is only
of this type, SJF-based proposals (e.g., PASE [30] and pFab-
ric with priority given to ﬂows with smaller sizes, or PDQ
and pFabric using remaining size as ﬂow criticality) are the
best known schemes to minimize average FCT (AFCT).
Type 3: Flows without deadlines or known sizes: A num-
ber of applications are unable to provide size/deadline infor-
mation at the start of their ﬂows, e.g. database access and
HTTP chunked transfer [6]. If the trafﬁc is only of this type,
best-effort schemes like DCTCP [3] are the predominant so-
lutions, while recently PIAS [6] achieves better FCT than
DCTCP by emulating SJF without knowing ﬂow sizes.
Observation 1: The 3 types of ﬂows coexist in DCNs. Each
type alone has well-known scheduling solutions (based on
SJF or EDF). But there is little prior work on how to schedule
a mix of ﬂows with different types.
2.2 Trade-offs in different scheduling schemes
We use ns-3 [33] simulations to show that applying criticality-
based scheduling schemes (SJF or EDF) hurts the perfor-
mance of different types of ﬂows in mixed scenarios.
In
these experiments, the sender and receiver are connected to
a switch, and the NIC capacity of both servers are 1Gbps.
We use DCTCP for the end host rate control. We simu-
late a query/response application for the deadline ﬂows. The
1In most applications where ﬂows have deadlines, the sizes
of these ﬂows are also known in advance [39].
2This is equivalent to the application throughput metric
(fraction of ﬂows that meet their deadline) used in prior
work [4, 22, 39].
3EDF is optimal because if there exist any scheduling dis-
cipline that can satisfy a set of deadlines, EDF also satisﬁes
them. [29]
Figure 1: SJF hurts type 1 ﬂows. Background ﬂows sizes are
drawn from the Data Mining workload in Figure 12. Type 1
ﬂows are generated with deadline of 10ms, and their sizes are
exactly the x-th percentile of the type 2 ﬂows.
Figure 2: EDF hurts type 2 ﬂows, especially smaller ones. Type
2 ﬂows are scheduled using SJF, and their sizes are drawn as in
Figure 1. Sizes of type 1 ﬂows are 5KB, and their deadlines are
drawn from an exponential distribution with mean 10ms. We
vary the percentage of type 1 ﬂows, and collect the FCT of type
2 ﬂows.
queries are generated with a Poisson process, and we con-
trol the ﬂow sizes and deadlines of response ﬂows (type 1).
Background ﬂows (type 2) are generated using a Poisson
process with ﬂow sizes drawn from realistic trafﬁc traces,
and the average load of background trafﬁc is 800Mbps (80%
load). There are no type 3 ﬂows in these experiments.
Case against pure SJF: In the ﬁrst experiment, we schedule
ﬂows strictly based on their sizes (SJF). Figure 1 shows that
the deadline miss rate of deadline ﬂows is undermined by
SJF. When the sizes of deadline ﬂows are smaller than 1%
of the type 2 ﬂows, the deadline miss rate is 0; when the
sizes are the 20th percentile (13KB), more than 40% of the
responses misses their deadlines.
Observation 2: Simply applying SJF hurts type 1 ﬂows.
This is because size alone decides which ﬂow goes ﬁrst,
which prevents deadline ﬂows, especially the larger ones,
from completing before their deadlines.
Case against pure EDF: Next, we instead use EDF to sched-
ule the deadline ﬂows, and let the type 1 ﬂows have strict pri-
ority over type 2 ﬂows. Type 2 ﬂows are still scheduled using
SJF. This is the strategy adopted in prior works [4, 22, 30].
In Figure 2, we observe that the tail latency of type 2 ﬂows
increases with the percentage of deadline trafﬁc. For short
type 2 ﬂows (latency-sensitive), we observe a ∼5× increase
in tail latency when the percentage of type 1 trafﬁc increases
from 0 to 8%. Because type 1 trafﬁc is prioritized and ag-
gressively takes up available bandwidth using DCTCP, the
more type 1 ﬂows, the worse the performance for other types
of trafﬁc.
Observation 3: Simple combination of EDF (for type 1
Figure 3: Karuna system overview.
ﬂows) and SJF (for type 2&3 ﬂows), with priority given to
type 1 ﬂows, hurts type 2&3 ﬂows. Type 1 ﬂows complete
quickly with small FCT, which is unnecessary for meeting
their deadlines, at the cost of higher tail latency for other
ﬂows.
3. SYSTEM OVERVIEW
This section outlines our design, Karuna (Figure 3). Karuna