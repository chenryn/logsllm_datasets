1. The Internet2 topology,
interconnecting 9 Internet2
sensors (located at the major PoPs of Internet2). It
includes 44 links (all with known IP addresses) and 72
paths. The average path length is around 4 hops.
2. The ESNet topology, interconnecting 22 ESNet sensors
(located at the major PoPs of ESNet). It includes 117
5The proof uses a reduction to the Exact-Set-Cover problem
[9].
6But, we limit the maximum loss rate to 20%.
390links (all with known IP addresses) and 382 paths. The
average path length is around 6 hops.
3. The “PlanetLab topology”, resulting from full-mesh
Paris-traceroute measurements between 100 PlanetLab
hosts at diﬀerent sites. This topology includes 4672
links and 5917 paths. The average path length is about
15 hops.
In each of the previous topologies, there are some links
that have the same path cover set (the path cover set of a
link is the set of paths traversing that link). When two or
more links have the same path cover set, it is impossible to
distinguish those links and detect which of them (if any) is
bad. Hence, as in previous work [15, 24], we group links
with the same path cover set together7 and localize bad link
groups instead of bad links. The size of such groups is a
topological property of the underlying network. The average
group size is around 1.8 in the PlanetLab topology, while it
is almost 1.0 in ESNet and Internet2.
In each simulation run, we select a topology and then
select lossy links either randomly or based on a distribution
that favors links close to the edge of the network (about
80% of the lossy links are at most three hops away from
at least one sensor). This second approach allows us to
simulate scenarios where congestion takes place mostly at
the periphery of the network rather than the core.
We consider the following metrics. Let I be the set of
lossy links and O be the set of lossy links inferred by a
tomography algorithm. The precision is the ratio of links
that are correctly identiﬁed as lossy to the total number
of inferred lossy links, precision = |I T O|
. The recall is the
ratio of links that are detected correctly as lossy to the actual
number of lossy links, recall = |I T O|
. The precision and
recall parameters capture the frequency of false positives and
false negatives, respectively.8 Let Q be the set of lossy links
whose performance range is accurately estimated, i.e., the
performance range assigned to a link in Q includes the actual
loss rate of that link. The accuracy is deﬁned as the ratio
of inferred lossy links whose loss rate range is accurately
estimated, accuracy = |Q|
|O|
|I|
|I T O| .
We consider two methods to set the α parameter. The
ﬁrst method (Method-1 ) relies on prior measurements in
which the ground truth about the identity of bad links is
known (e.g., using SNMP data from routers). The second
method (Method-2 ) does not require any prior knowledge.
Speciﬁcally, in Method-1 we calculate the variation of the
measured end-to-end loss rates in all paths that traverse a
single lossy link, and set α to the minimum value with which
all these paths are α-similar. In the simulation experiments,
Method-1 gives α ≈ 0.3 for Bernoulli losses, and about 0.5
for Gilbert losses. In Method-2, we ﬁrst remove links that
appear in good paths, as well as links that are traversed by
only a small number of bad paths (say 3). For each remain-
ing link, we compute the average loss rate ¯r of all paths
traversing that link, and determine the minimum value of α
with which the loss rate of every such path is α-similar with
¯r. We ﬁnally use the median of the previous α values. The
rationale in Method-2 is that if a link is traversed by multi-
7These groups of links over a path correspond to the Mini-
mal Identiﬁable Link Sequence (MILS), deﬁned in [25].
8In tomography, a false positive refers to detecting a link as
bad while it is good.
ple bad paths it is probably a bad link, and so we set α so
that most such links are traversed by α-similar paths. There
are two cases in which this assumption is incorrect: when
a good link is identiﬁed as bad (false positive) and when a
path traverses more than one bad link. However, as shown
in the simulation results of this section and the experimental
results of the next section, the likelihood of these two events
is low in practice. In addition, by taking the median of α
values across all links traversed by multiple bad paths, we
reduce the impact of these events.
We compare Sum-Tomo with two other tomography al-
gorithms: the Boolean Tomo method [8] and the Analog
Norm method (“L1-norm minimization with non-negativity
constraints”) [23]. Norm starts with similar constraints as in
(1) and solves them heuristically using a certain error min-
imization approach (the estimated link loss rates may not
satisfy the path equations exactly, but they minimize the
corresponding inference error favoring solutions that involve
fewer lossy links). Some recent tomography algorithms, such
as Netscope [15] and LIA [24], apply the previous norm min-
imization method but additionally they rely on the variance
of link loss rates over multiple measurement snapshots. We
do not consider such methods because they require multiple
measurement snapshots (with 1,000-10,000 probing packets
in each snapshot), and so they are less robust to load varia-
tions and routing changes. Our focus is on short-term mea-
surements, and so we only compare our method with the
original Norm algorithm. The precision and recall of Norm
are computed by comparing the returned point estimates
with the threshold δ (so that we can determine the set of
bad links according to that method). To calculate the accu-
racy of Norm, we ﬁrst convert the reported point estimates
to range estimate using the relation shown in line-16 of Al-
gorithm 2.
The results of each simulation are repeated 200 times.
The standard error for all results in this section is negligible
(mostly between 0.005 and 0.02) and so we simply show the
average across all runs.
5.2 Results
We start with the ESNet topology. Figure 2 shows the re-
sults with three tomography algorithms in ESNet, when the
number of lossy links varies from 1 to 20 (i.e., up to about
20% of the total links in that network). Lossy links are se-
lected randomly and the loss process follows the Bernoulli
model. The value of α is set based on Method-1; a compar-
ison with Method-2 is given later in this section (Figure 5).
Since Tomo is not able to determine the loss rate of links,
Figure 2c only compares the results of Sum-Tomo and Norm.
As the number of lossy links increases, we see a degradation
in the performance of all tomography algorithms. This is
because more lossy links create more lossy paths, and hence
fewer links can be accounted as good at the initial step of
the algorithms. Therefore, the tomography methods have
to opt their candidate lossy links from a larger set, which
increases the localization errors.
As illustrated in Figure 2a, the precision of Tomo and
Sum-Tomo is very close. However, the recall of Sum-Tomo
is higher than of Tomo by up to 13% (Figure 2b). The rea-
son is that Tomo does not consider the loss rate of lossy
paths and it simply aims to ﬁnd the minimum set of links
shared between such paths. This approach can be clearly
wrong, however, because a link can justify diﬀerent paths
391i
i
n
o
s
c
e
r
p
e
g
a
r
e
v
A
i
i
n
o
s
c
e
r
p
e
g
a
r
e
v
A
i
i
n
o
s
c
e
r
p
e
g
a
r
e
v
A
 1
 0.95
 0.9
 0.85
 0.8
 0.75
 0.7
Sum-Tomo
Tomo
Norm
l
l
a
c
e
r
e
g
a
r
e
v
A
 1
 0.95
 0.9
 0.85
 0.8
 0.75
 0.7
Sum-Tomo
Tomo
Norm
y
c
a
r
u
c
c
a
e
g
a
r
e
v
A
 1
 0.95
 0.9
 0.85
 0.8
 0.75
 0.7
Sum-Tomo
Norm
 2
 4
 6
 8  10  12  14  16  18  20
 2
 4
 6
 8  10  12  14  16  18  20
 2
 4
 6
 8  10  12  14  16  18  20
Number of lossy links
(a) Precision
Number of lossy links
(b) Recall
Number of lossy links
(c) Accuracy
Figure 2: Results of diﬀerent algorithms obtained in ESNet when actual link loss rates follow Bernoulli process.
 1
 0.95
 0.9
 0.85
 0.8
 0.75
 0.7
Sum-Tomo
Tomo
Norm
l
l
a
c
e
r
e
g
a
r
e
v
A
 1
 0.95
 0.9
 0.85
 0.8
 0.75
 0.7
Sum-Tomo
Tomo
Norm
y
c
a
r
u
c
c
a
e
g
a
r
e
v
A
 1
 0.95
 0.9
 0.85
 0.8
 0.75
 0.7
Sum-Tomo
Norm
 2
 4
 6
 8  10  12  14  16  18  20
 2
 4
 6
 8  10  12  14  16  18  20
 2
 4
 6
 8  10  12  14  16  18  20
Number of lossy links
(a) Precision
Number of lossy links
(b) Recall
Number of lossy links
(c) Accuracy
Figure 3: Results of diﬀerent algorithms obtained in ESNet when actual link loss rates follow Gilbert process.
Sum-Tomo
Tomo
Norm
 1
 0.95
 0.9
 0.85
 0.8
 0.75
 0.7
 0.65
 0.6
 0.55
 0.5
l
l
a
c
e
r
e
g
a
r
e
v
A
 1
 0.95
 0.9
 0.85
 0.8
 0.75