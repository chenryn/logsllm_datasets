memory barriers,
the CPU re-orders memory accesses to
increase the cache hit rate and avoid accessing DRAM.
• Using uncached PTEs or non-temporal instructions is in-
effective, leading to negligible changes to ACT rates when
compared to the more common Rowhammer instruction se-
7
0255075100125150175200(a)#ACTs/tREFI0.00.51.0CDF47%optimal050100150200250(b)ACT-to-ACTLatency(ns)optimalmovzxrax,BYTEPTR[rcx]movzxrax,BYTEPTR[rdx]clflushBYTEPTR[rcx]clflushBYTEPTR[rdx]fence{m|l|s|no}mfencelfencesfenceno-fence0255075100125150175200(a)#ACTs/tREFI0.00.51.0CDF33%optimal050100150200250(b)ACT-to-ACTLatency(ns)optimalmovzxrax,BYTEPTR[rcx]movzxrax,BYTEPTR[rdx]clflushoptBYTEPTR[rcx]clflushoptBYTEPTR[rdx]fence{m|l|s|no}mfencelfencesfenceno-fenceFigure 6: Performance of the Rowhammer instruction sequence that marks its memory pages uncacheable.
Figure 7: Performance of Rowhammer instruction sequence that uses a mix of non-temporal and regular memory accesses [35].
quences.
• The most effective instruction sequence proposed in previous
works uses two load,
instructions, and no
memory barriers at 33% from the optimal rate (Figure 5).
C. clﬂushopt Alone Hammers Near-Optimally on Skylake and
Cascade Lake
two clﬂushopt
To increase the rate of ACT commands, we experimented
with new instruction sequences that (1) do not use memory
barriers, and (2) are less prone to the effects of out-of-order
execution. Our experiments revealed that a cache line ﬂush
instruction results in a memory access.
Figure 8 characterizes the rate of ACT commands of a
sequence consisting of two clﬂushopt instructions in a loop;
(these results are from our experiments on the Skylake-based
server, but they are very similar to those performed on Cascade
Lake). Figure 8b shows that over 87% of ACTs are issued at
the optimal rate, about 46.7ns apart from one another. The
remaining 13% are separated by an additional 10-20ns due
to conﬂicts with ongoing refresh commands (REF). When the
memory controller issues a REF, the bank remains inaccessible
until the REF ﬁnishes [17], and any ongoing ACT is blocked
waiting for the REF to ﬁnish. The REF-induced delay causes
this instruction sequence to issue 159 row activations for every
tREFI, a rate we call near-optimal.
The microarchitectural side-effects of clﬂushopt causes this
instruction sequence to issue row activations at a rate that
is 44% higher than the best previously known Rowhammer
sequence (159 vs 110 ACTs/tREFI). It is unlikely another
sequence could improve this rate because row activations will
still conﬂict with REFs that block the bank. Two clﬂushopt
instructions in a loop thus create the worst-case DRAM
“hammering” conditions on Skylake and Cascade Lake.
Why does clﬂushopt cause memory accesses? This in-
struction sequence is highly surprising in the context of
a Rowhammer attack because it uses no explicit memory
accesses. Instead, the memory access (a DDR4 read operation)
is a microarchitectural side-effect of the CPU executing a
cache line ﬂush. It occurs only when the cache line is invalid.
Issuing a cache line ﬂush instruction to a line present in the
cache does not cause any DDR read operations.
Our instruction sequence (Figure 8) causes two memory
accesses for each loop iteration except for the ﬁrst iteration.
The ﬁrst loop iteration does not generate memory accesses
when the lines are in the cache. However, it invalidates the
cache lines, causing all subsequent iterations to generate two
memory accesses.
According to Intel’s speciﬁcation [52], [86], systems with
multiple processors may maintain cache coherence state of
each cache line within the line itself
in memory. When
executing clﬂushopt on an invalid cache line, the processor
reads cache directory state from DRAM to determine whether
the line is present in other processors’ caches. We veriﬁed
clﬂushopt’s behavior on both Cascade Lake and Skylake. We
also show that clﬂush behaves similarly on both Cascade
Lake and Skylake, but on Broadwell clﬂush results in no
memory accesses. We hypothesize that clﬂush has more system
overhead than clﬂushopt because it is subject to additional
ordering constraints [51], leading to a reduced rate of DRAM
row activations. Figure 9 shows the performance of a sequence
using two clﬂush instructions in a loop; it activates rows at a
rate of 110 every tREFI, corresponding to 65.7% of optimal.
V. STEP 2: REVERSE ENGINEERING ROW ADJACENCY IN
ANY DRAM DEVICE
No technique used in previous work is suitable for reverse
engineering row adjacency: some are not ﬁne-grained and can-
not determine adjacency at the level of an individual row [100],
[94], [78], [64], whereas others do not capture addresses
internal to DRAM devices and thus can determine adjacency
only in the DDR4 bus address space [100], [94]. The single
previous technique that can overcome these limitations works
only if
the device succumbs to Rowhammer attacks [100],
[94], [105]. Section III-C describes these techniques and their
trade-offs in depth.
In an attempt to guarantee Rowhammer failures on our
DRAM devices, we experimented with lowering the refresh
rates of our servers. A low refresh rate ensures that an
attack sends a higher number of ACT commands to a victim
row before the row can refresh. This increases the attack’s
likelihood of success. Unfortunately, our experiments were
unsuccessful. Recent hardware makes it increasingly difﬁcult
8
0255075100125150175200(a)#ACTs/tREFI0.00.51.0CDF59%optimal050100150200250(b)ACT-to-ACTLatency(ns)optimalmovzxrax,BYTEPTR[rcx]movzxrax,BYTEPTR[rdx](uncachedPTEs)0255075100125150175200(a)#ACTs/tREFI0.00.51.0CDF67%optimal050100150200250(b)ACT-to-ACTLatency(ns)optimalmovntiQWORDPTR[r8],raxmovntiQWORDPTR[r9],raxmovQWORDPTR[r8],raxmovQWORDPTR[r9],raxFigure 8: Performance of the near-optimal Rowhammer instruction sequence using only clﬂushopt.
Figure 9: Performance of the Rowhammer instruction sequence using only clﬂush instructions.
Figure 10: Fault injector. When pressed, the button drives A14
to low. Two DIP switches form a 3-way switch to ﬂip the
ALERTn signal. The third DIP switch is a spare.
Figure 11: Fault injector schematic.
to set the refresh rates sufﬁciently low to successfully mount
a Rowhammer attack. Older generation BIOSes running on
DDR3-equipped hardware can set refresh rates up to 12x
lower than normal; such low refresh rates make DDR3 devices
succumb to Rowhammer attacks.
Modern BIOSes for DDR4 hardware restrict lowering the
rate to only ∼3.5x. Unfortunately, this refresh rate is not
sufﬁciently low to guarantee Rowhammer failures on our
servers. We also conﬁrmed this is not a GUI restriction: we
examined the BIOS source code and found that the refresh
interval conﬁguration register cannot hold a value larger than
one corresponding to a refresh rate of 3.5x lower.
A. DDR4 Fault Injector
Because modern BIOSes restrict lowering the DRAM re-
fresh rate, we used a different approach: we designed a DDR4
fault injector that blocks REFs sent by a memory controller to
an individual DIMM. Our fault injector manipulates electrical
signals and drives them from low to high, and vice-versa. Ma-
nipulating the DDR bus’s electrical signals effectively changes
one DDR command into another. This insight was inspired
by previous work that used a custom-made shunt probe to
induce faults in DRAM data and thus reverse engineered the
9
Table II: DDR4 command encoding [113].
DRAM controller’s ECC scheme [18]. Figure 10 shows our
fault injector, and Figure 11 shows its schematic.
Side-effects. Manipulating electrical signals to change DDR4
commands introduces side-effects. For example, changing a
signal known as ACT makes all DDR4 commands decode
as row activate (ACT) commands. In this case, a DIMM
becomes inaccessible because it receives only ACTs no matter
what command the memory controller is issuing. Table II
(reproduced from Wikipedia [113]) shows the encoding of
DDR4 commands.
Instead, we need to control these side-effects to leave the
DIMM in a responsive state; otherwise, we cannot mount a
Rowhammer attack. The DIMM must continue to receive row
activates, row reads (or writes), and row precharges.
Overcoming the side-effects. Our fault injector changes the
A14 signal from high to low and turns REF commands into a
different DDR command, known as mode register (MR0) [61]
with a null payload. Although this new command affects
the DIMM’s conﬁguration, the DIMM continues to serve all
incoming commands. We designed our fault injector to trigger
memory recalibration and thus reset the DIMM’s conﬁguration
back to its original settings.
Manipulating the A14 signal has an additional side-effect: it
changes a read into a write command (see [113]). To overcome
this side-effect, our Rowhammer attack instruction sequence
uses stores rather than loads. Fortunately, manipulating the
A14 signal does not affect the row activations and precharges
0255075100125150175200(a)#ACTs/tREFI0.00.51.0CDFoptimal050100150200250(b)ACT-to-ACTLatency(ns)optimalclflushoptBYTEPTR[rcx]clflushoptBYTEPTR[rdx]0255075100125150175200(a)#ACTs/tREFI0.00.51.0CDF34%optimal050100150200250(b)ACT-to-ACTLatency(ns)optimalclflushBYTEPTR[rcx]clflushBYTEPTR[rdx]VSSDIMM_ALERTnCPU_ALERTnDIMM_A14CPU_A14GNDFI_ENABLEALERTn_SUPPRESSneeded to mount a Rowhammer attack.
Memory recalibration. The memory controller performs
memory recalibration upon detecting an error. One such error
is a parity check failure for DDR4 signals. On an incoming
command, the DIMM checks parity, and, if the check fails, it
alerts the memory controller through a reverse signal called
ALERTn. Upon receiving the alert, the memory controller
sends a sequence of DDR recalibration commands to the
DIMM.
We designed our fault injector to also recalibrate memory,
but only when the Rowhammer attack completes. This restores
the DIMM to its original conﬁguration and lets us inspect
the location of the bit ﬂips that could reverse engineer row
adjacency in the DRAM device. Memory recalibration cannot
occur during an ongoing Rowhammer attack because it creates
interference.
To recalibrate memory, our fault injector also manipulates
the ALERTn signal. During an ongoing Rowhammer attack,
it suppresses the ALERTn signal, thus preventing the memory
controller from receiving any alerts. Once the attack com-
pletes, the fault injector re-enables ALERTn while continuing
to manipulate A14 to ensure that parity checks continue to
fail. These alerts are now received by the memory controller,
which, in turn, recalibrates the DIMM.
Methodology for injecting DDR4 faults. Figure 12 shows our
hardware stack: the fault injector, the bus analyzer’s interposer,
and the DDR4 DIMM. We used an eight-step operational plan
to inject faults and mount Rowhammer to induce bit ﬂips
capable of reverse engineering row adjacency:
1. Boot server with DDR parity check enabled and ECC
disabled.
2. Suppress ALERTn signal with DIP switches.
3. Begin Rowhammering the target DIMM.
4. Inject a fault in the A14 signal by pressing the button switch
for a ﬁxed time interval. During this time, the DIMM receives
no REFs, and the memory controller receives no alerts.
5. Stop Rowhammering the target DIMM.
6. Re-connect ALERTn signal with DIP switches.
7. Inject a fault in the A14 signal by tapping the button. The
memory controller receives alerts from the DIMM and starts
recalibrating the DIMM.
8. Inspect the number and spatial distribution of bit ﬂips.
B. Row Adjacency in DRAM Devices
We used the fault injector to reverse engineer the physical
row adjacency of DRAM devices mounted on DDR4 DIMMs.
We mounted Rowhammer attacks and measured the density
of bit ﬂips across each row within a bank. This technique
correlates each row’s density of bit ﬂips with adjacency [100],
[94], [105]. Invariably, when hammering one row for 15
seconds without refreshes, a small number of rows ﬂip bits
at a much higher rate than all others. This indicates that these
highly affected rows are physically adjacent to the hammered
one.
We then posed the following questions:
1. Do logical addresses map linearly to internal DRAM
addresses? A linear map makes it easier to mount Rowhammer
because an attacker need not reverse engineer it. Previous work
showed that the map from physical to logical addresses is not
10
Figure 12: Our hardware stack from the bottom up: fault in-
jector, bus analyzer’s interposer, and DDR4 DIMM. Markings
on the DRAM chips have been redacted.
linear and discussed how non-linearity can render many of
the Rowhammer defenses much less effective than initially
thought [105].
2. Does the position of a bit within a word inﬂuence its
likelihood of being ﬂipped? Such results would shed light
on whether some words (or some bits within a single word)
are more susceptible to Rowhammer attacks than others. For
example, most page table entries have a format in which
low-order bits control access to a page; should the low-order
bits be more susceptible than high-order, an attack changing
the access control policy to a page would be more likely to
succeed.
3. How do data patterns affect the susceptibility of bits being
ﬂipped? We examined the direction in which bits ﬂip (0-to-1
or 1-to-0). The memory controllers in datacenter servers are
routinely conﬁgured to scramble data in DRAM by xor-ing it
with a known, random data pattern [87], [43]. This means that
the proportion of 0s to 1s in DRAM is 50-50.
4. Do DIMMs sourced from different vendors have different
characteristics? We examined whether or not the map and
the rate at which bits ﬂip are consistent across DIMMs from
different vendors.
Methodology. We performed all experiments by suppressing
REFs for 15 seconds at room temperature. We disabled data
scrambling and wrote a speciﬁc data pattern across the entire
bank except for the hammered row. We wrote the complement
of the data pattern in the hammered row, a strategy used by
previous work [62]. We experimented with four different data
patterns that vary the locations and ratios of bits set to 1 vs.
bits set to 0. The four patterns are: all 1s, 0xB6DB6DB...
(corresponding to two-thirds 1s), 0x492492... (corresponding
to one-third 1s), and all 0s. Unless marked otherwise, the
results we present use a pattern of all 1s.
When testing DRAM, a double-sided Rowhammer attack
(i.e.,
two aggressor rows) is better than single-sided (i.e.,
one aggressor row). However, when injecting faults, both
types of Rowhammer attack ﬂip bits because the DIMM does
not refresh for 15 seconds. When reverse engineering row
adjacency, single-sided Rowhammer is simpler because the
adjacency of a ﬂipped bit is unambiguous – it is due to
a single aggressor. Reverse engineering row adjacency with
double-sided Rowhammer leads to an attribution challenge –
is the ﬂipped bit adjacent to aggressor #1 or aggressor #2? The
Victim #3
Victim #1
Victim #2
Aggressor