:::
::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
### 注意 {.title}
默认情况下，Redis 的部署设置是单机 localhost。但是，Red Hat Redis
可以选择以高可用性和高度扩展的集群执行，其中数据在多个主机之间共享。另一个可行选择是在云中部署
Redis 集群，或者从云供应商中使用受管 Redis 集群。
:::
::: itemizedlist
**其他资源**
-   `pcp(1)`{.literal}, `pmlogger(1)`{.literal}, `pmproxy(1)`{.literal},
    和 `pmcd(1)`{.literal} man pages
-   [推荐的部署架构](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9-beta/html-single/monitoring_and_managing_system_status_and_performance/index#recommended-deployment-architecture_setting-up-pcp){.link}
:::
:::
::: section
::: titlepage
# []{#setting-up-pcp_monitoring-and-managing-system-status-and-performance.html#recommended-deployment-architecture_setting-up-pcp}推荐的部署架构 {.title}
:::
下表根据监控的主机数量描述了推荐的部署架构。
::: table
[]{#setting-up-pcp_monitoring-and-managing-system-status-and-performance.html#idm140048719434880}
**表 4.3. 推荐的部署架构**
::: table-contents
  主机数(N)                     1-10                                              10-100                                            100-1000
  ----------------------------- ------------------------------------------------- ------------------------------------------------- ----------------------------
  `pmcd`{.literal} 服务器       N                                                 N                                                 N
  `pmlogger`{.literal} 服务器   1 到 N                                            N/10 到 N                                         N/100 到 N
  `pmproxy`{.literal} 服务器    1 到 N                                            1 到 N                                            N/100 到 N
  Redis 服务器                  1 到 N                                            1 到 N/10                                         N/100 到 N/10
  Redis 集群                    否                                                Maybe                                             是
  推荐的部署设置                localhost、Decentralized 或 centralized logging   Decentralized, Centralized logging 或 Federated   decentralized 或 Federated
:::
:::
:::
::: section
::: titlepage
# []{#setting-up-pcp_monitoring-and-managing-system-status-and-performance.html#sizing-factors_setting-up-pcp}大小考虑因素 {.title}
:::
以下是扩展所需的大小调整因素：
::: variablelist
[`远程系统大小`{.literal}]{.term}
:   CPU、磁盘、网络接口和其他硬件资源的数量会影响中央日志记录主机上的每个
    `pmlogger`{.literal} 收集的数据量。
[`日志记录的指标数据`{.literal}]{.term}
:   日志记录的指标的数量和类型是重要的角色。特别是，`per-process proc.*`{.literal}
    指标需要大量磁盘空间，例如，标准 `pcp-zeroconf`{.literal} 设置、10s
    日志记录间隔、11 MB、没有 proc 指标和 155 MB 的 proc 指标 - 可获得
    10 倍。此外，每个指标的实例数量，如
    CPU、块设备和网络接口的数量也会影响所需的存储容量。
[`日志记录间隔`{.literal}]{.term}
:   指标的日志记录频率，会影响存储要求。预期的每日 PCP
    归档文件大小会为每个 `pmlogger`{.literal} 实例写入到
    `pmlogger.log`{.literal} 文件。这些值未压缩估算。由于 PCP
    归档的压缩非常大，大约
    10:1，因此可以为特定站点确定实际的长期磁盘空间要求。
[`pmlogrewrite`{.literal}]{.term}
:   在每个 PCP 升级后，将执行 `pmlogrewrite`{.literal}
    工具，并重写旧的归档（如果之前版本中的指标元数据有变化）和 PCP
    的新版本。这个过程持续时间使用存储的存档数扩展线性。
:::
::: itemizedlist
**其他资源**
-   `pmlogrewrite(1)`{.literal} 和 `pmlogger(1)`{.literal} man page
:::
:::
::: section
::: titlepage
# []{#setting-up-pcp_monitoring-and-managing-system-status-and-performance.html#configuration-options-for-pcp-scaling_setting-up-pcp}PCP 扩展的配置选项 {.title}
:::
以下是扩展所需的配置选项：
::: variablelist
[`sysctl 和 rlimit 设置`{.literal}]{.term}
:   当启用归档发现时，对于每个 `pmlogger`{.literal}，`pmproxy`{.literal}
    都需要 4 个描述符，用于监控或注销，以及服务日志和
    `pmproxy`{.literal} 客户端套接字的额外文件描述符（如果有）。每个
    `pmlogger`{.literal} 进程在远程 `pmcd`{.literal}
    套接字、存档文件、服务日志等中使用大约 20
    个文件描述符。总的来说，这可以超过运行约 200 个 `pmlogger`{.literal}
    进程的系统上的默认 1024 软限制。`pcp-5.3.0`{.literal}
    及之后的版本中的 `pmproxy`{.literal}
    服务会自动将软限制增加到硬限制。在 PCP 的早期版本中，如果要部署大量
    `pmlogger`{.literal} 进程，则需要调优；这可以通过增加
    `pmlogger`{.literal} 的软或硬限制来实现。如需更多信息，请参阅
    [如何为 systemd
    运行的服务设置限制(ulimit)。](https://access.redhat.com/solutions/1346533){.link}
[`本地归档`{.literal}]{.term}
:   `pmlogger`{.literal} 服务将本地和远程 `pmcds`{.literal} 的指标存储在
    `/var/log/pcp/pmlogger/`{.literal}
    目录中。要控制本地系统的日志间隔，请更新
    `/etc/pcp/pmlogger/control.d/configfile文件`{.literal}，并在参数中添加
    `-t X`{.literal}，其中 [*X*]{.emphasis}
    是日志间隔（以秒为单位）。要配置应该记录哪些指标，请执行
    `pmlogconf /var/lib/pcp/config/pmlogger/config.clienthostname`{.literal}。此命令使用一组默认指标来部署配置文件，可选择性地进行进一步自定义。要指定保留设置（指定何时清除旧的
    PCP 存档），更新 `/etc/sysconfig/pmlogger_timers`{.literal} 文件指定
    `PMLOGGER_DAILY_PARAMS="-E -k X"`{.literal}，其中 [*X*]{.emphasis}
    是保留 PCP 归档的天数。
[`Redis`{.literal}]{.term}
:   `pmproxy`{.literal} 服务将日志记录的指标从 `pmlogger`{.literal}
    发送到 Redis 实例。以下是两个选项，用于指定
    `/etc/pcp/pmproxy/pmproxy.conf`{.literal} 配置文件中的保留设置：
    ::: itemizedlist
    -   `stream.expire`{.literal}
        指定应删除过时指标时的持续时间，即在指定时间内没有更新的指标，以秒为单位。
    -   `stream.maxlen`{.literal}
        指定每个主机的一个指标值的最大指标值数。此设置应是保留的时间除以日志间隔，例如如果保留时间为
        14 天日志间隔是 60s，则设置为 20160(60\*60\*24\*14/60)
    :::
:::
::: itemizedlist
**其他资源**
-   `pmproxy(1)`{.literal}, `pmlogger(1)`{.literal}, 和
    `sysctl(8)`{.literal} man pages
:::
:::
::: section
::: titlepage
# []{#setting-up-pcp_monitoring-and-managing-system-status-and-performance.html#example-analyzing-the-centralized-logging-deployment_setting-up-pcp}例如：分析集中式日志记录部署 {.title}
:::
在集中式日志记录设置中收集以下结果（也称为 pmlogger 场部署），其默认
`pcp-zeroconf 5.3.0`{.literal} 安装，其中每个远程主机都是在有 64 个 CPU
内核、376 GB RAM 的服务器上运行 `pmcd`{.literal} 的相同容器实例。
日志记录间隔为 10s，不包含远程节点的 proc 指标，内存值则引用 Resident
Set Size(RSS)值。
::: table
[]{#setting-up-pcp_monitoring-and-managing-system-status-and-performance.html#idm140048839493408}
**表 4.4. 10s 日志间隔的详细利用率统计**
::: table-contents
  主机数量                                10       50
  --------------------------------------- -------- --------
  PCP 每天归档存储                        91 MB    522 MB
  `pmlogger`{.literal} Memory             160 MB   580 MB
  每天 `pmlogger`{.literal} Network(In)   2 MB     9 MB
  `pmproxy`{.literal} Memory              1.4 GB   6.3 GB
  每天的 redis 内存                       2.6 GB   12 GB
:::
:::
::: table
[]{#setting-up-pcp_monitoring-and-managing-system-status-and-performance.html#idm140048720446352}
**表 4.5. 根据被监控的主机提供 60 个日志记录间隔的资源**
::: table-contents
  主机数量                                10        50        100
  --------------------------------------- --------- --------- ---------
  PCP 每天归档存储                        20 MB     120 MB    271 MB
  `pmlogger`{.literal} Memory             104 MB    524 MB    1049 MB
  每天 `pmlogger`{.literal} Network(In)   0.38 MB   1.75 MB   3.48 MB
  `pmproxy`{.literal} Memory              2.67 GB   5.5GB     9 GB
  每天的 redis 内存                       0.54 GB   2.65 GB   5.3 GB
:::
:::
::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
### 注意 {.title}
`pmproxy`{.literal} 队列 Redis 请求，并使用 Redis pipelining 来加快
Redis
查询。这可能导致大量内存使用。有关此问题的故障排除，请参阅[对高内存的使用进行故障排除](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9-beta/html-single/monitoring_and_managing_system_status_and_performance/index#troubleshooting-high-memory-usage_setting-up-pcp){.link}。
:::
:::
::: section
::: titlepage
# []{#setting-up-pcp_monitoring-and-managing-system-status-and-performance.html#example-analyzing-the-federated-setup-deployment_setting-up-pcp}例如：分析联邦设置部署 {.title}
:::
以下结果在联合设置中观察，也称为多个 `pmlogger`{.literal}
farm，由三个集中式日志记录（`pmlogger`{.literal} farm）设置组成，每个
`pmlogger`{.literal} farm 都监控 100 个远程主机，总计为 300 个主机。
`pmlogger`{.literal} 场的设置与 [Example
中所述的配置相同：分析集中式日志记录部署](#setting-up-pcp_monitoring-and-managing-system-status-and-performance.html#example-analyzing-the-centralized-logging-deployment_setting-up-pcp "例如：分析集中式日志记录部署"){.link}，60s
日志记录间隔，但 Redis 服务器以集群模式运行。
::: table
[]{#setting-up-pcp_monitoring-and-managing-system-status-and-performance.html#idm140048848474240}
**表 4.6. 根据联合主机进行 60s 日志记录间隔使用的资源**
::: table-contents
  PCP 每天归档存储   `pmlogger`{.literal} Memory   每天网络(In/Out)    `pmproxy`{.literal} Memory   每天的 redis 内存
  ------------------ ----------------------------- ------------------- ---------------------------- -------------------
  277 MB             1058 MB                       15.6 MB / 12.3 MB   6-8 GB                       5.5 GB
:::
:::
此处，所有值都是每个主机。网络带宽较高，因为 Redis 集群的节点间通信。
:::
::: section
::: titlepage
# []{#setting-up-pcp_monitoring-and-managing-system-status-and-performance.html#troubleshooting-high-memory-usage_setting-up-pcp}对高内存使用量进行故障排除 {.title}
:::
以下情况可能会导致内存用量：
::: itemizedlist
-   `pmproxy`{.literal} 进程忙于处理新的 PCP 归档，且没有处理 Redis
    请求和响应的备用 CPU 周期。
-   Redis 节点或集群已过载，且无法在时间处理传入的请求。
:::
`pmproxy`{.literal} 服务守护进程使用 Redis 流并支持配置参数，这些参数是
PCP 调优参数，并影响 Redis
内存用量和密钥保留。`/etc/pcp/pmproxy/pmproxy.conf`{.literal} 文件列出了
`pmproxy`{.literal} 和关联的 API 的可用选项。
这部分论述了如何对高内存用量进行故障排除。
::: orderedlist
**先决条件**
1.  安装 `pcp-pmda-redis`{.literal} 软件包：
    ``` screen
    # dnf install pcp-pmda-redis
    ```
2.  安装 redis PMDA：
    ``` screen
    # cd /var/lib/pcp/pmdas/redis && ./Install
    ```
:::
::: itemizedlist
**步骤**
-   要排除高内存用量的问题，请执行以下命令并观察 `inflight`{.literal}
    列：
    ``` literallayout
    $ pmrep :pmproxy
             backlog  inflight  reqs/s  resp/s   wait req err  resp err  changed  throttled
              byte     count   count/s  count/s  s/s  count/s   count/s  count/s   count/s
    14:59:08   0         0       N/A       N/A   N/A    N/A      N/A      N/A        N/A
    14:59:09   0         0    2268.9    2268.9    28     0        0       2.0        4.0
    14:59:10   0         0       0.0       0.0     0     0        0       0.0        0.0
    14:59:11   0         0       0.0       0.0     0     0        0       0.0        0.0
    ```
    此列显示有多少 Redis 请求是
    in-flight，这意味着它们被排队或发送，目前还没有收到回复。
    数字表示以下条件之一：
    ::: itemizedlist
    -   `pmproxy`{.literal} 进程忙于处理新的 PCP 归档，且没有处理 Redis
        请求和响应的备用 CPU 周期。
    -   Redis 节点或集群已过载，且无法在时间处理传入的请求。
    :::
-   要对高内存使用问题进行故障排除，请减少此场的 `pmlogger`{.literal}
    进程数量，再添加另一个 pmlogger 场。使用联邦 - 多个 pmlogger farm
    设置。
    如果 Redis 节点使用 100% 的 CPU
    延长的时间，请将其移到具有更好的性能的主机，或使用集群的 Redis
    设置。
-   要查看 `pmproxy.redis.*`{.literal} 指标，请使用以下命令：
    ``` screen
    $ pminfo -ftd pmproxy.redis
    pmproxy.redis.responses.wait [wait time for responses]
        Data Type: 64-bit unsigned int  InDom: PM_INDOM_NULL 0xffffffff
        Semantics: counter  Units: microsec
        value 546028367374
    pmproxy.redis.responses.error [number of error responses]
        Data Type: 64-bit unsigned int  InDom: PM_INDOM_NULL 0xffffffff
        Semantics: counter  Units: count
        value 1164
    [...]
    pmproxy.redis.requests.inflight.bytes [bytes allocated for inflight requests]
        Data Type: 64-bit int  InDom: PM_INDOM_NULL 0xffffffff
        Semantics: discrete  Units: byte
        value 0
    pmproxy.redis.requests.inflight.total [inflight requests]
        Data Type: 64-bit unsigned int  InDom: PM_INDOM_NULL 0xffffffff
        Semantics: discrete  Units: count
        value 0
    [...]
    ```
    要查看有多少 Redis 请求在flight 中，请参阅
    `pmproxy.redis.requests.inflight.total`{.literal} 指标和
    `pmproxy.redis.requests.inflight.bytes`{.literal}
    指标来查看所有当前在flight Redis 请求中消耗的字节数。
    通常，redis 请求队列为零，但可以根据大型 pmlogger
    场的使用而构建，这限制了可扩展性，并可能导致 `pmproxy`{.literal}
    客户端的高延迟。
-   使用 `pminfo`{.literal} 命令查看有关性能指标的信息。例如，要查看
    `redis.*`{.literal} 指标，请使用以下命令：
    ``` screen
    $ pminfo -ftd redis
    redis.redis_build_id [Build ID]
        Data Type: string  InDom: 24.0 0x6000000
        Semantics: discrete  Units: count
        inst [0 or "localhost:6379"] value "87e335e57cffa755"
    redis.total_commands_processed [Total number of commands processed by the server]
        Data Type: 64-bit unsigned int  InDom: 24.0 0x6000000
        Semantics: counter  Units: count
        inst [0 or "localhost:6379"] value 595627069