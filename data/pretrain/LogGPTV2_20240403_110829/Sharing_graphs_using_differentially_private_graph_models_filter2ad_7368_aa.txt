title:Sharing graphs using differentially private graph models
author:Alessandra Sala and
Xiaohan Zhao and
Christo Wilson and
Haitao Zheng and
Ben Y. Zhao
Sharing Graphs Using Differentialy Private Graph Models
Alessandra Sala
UC Santa Barbara
Computer Science Dept.
Santa Barbara, CA 93106
PI:EMAIL
Xiaohan Zhao
UC Santa Barbara
Computer Science Dept.
Santa Barbara, CA 93106
PI:EMAIL
Christo Wilson
UC Santa Barbara
Computer Science Dept.
Santa Barbara, CA 93106
PI:EMAIL
Haitao Zheng
UC Santa Barbara
Computer Science Dept.
Santa Barbara, CA 93106
PI:EMAIL
Ben Y. Zhao
UC Santa Barbara
Computer Science Dept.
Santa Barbara, CA 93106
PI:EMAIL
General Terms
Algorithms, Security
ABSTRACT
Continuing success of research on social and computer networks
requires open access to realistic measurement datasets. While these
datasets can be shared, generally in the form of social or Internet
graphs, doing so often risks exposing sensitive user data to the pub-
lic. Unfortunately, current techniques to improve privacy on graphs
only target speciﬁc attacks, and have been proven to be vulnerable
against powerful de-anonymization attacks.
Our work seeks a solution to share meaningful graph datasets
while preserving privacy. We observe a clear tension between strength
of privacy protection and maintaining structural similarity to the
original graph. To navigate the tradeoff, we develop a differentially-
private graph model we call Pygmalion. Given a graph G and
a desired level of -differential privacy guarantee, Pygmalion ex-
tracts a graph’s detailed structure into degree correlation statistics,
introduces noise into the resulting dataset, and generates a syn-
thetic graph G!. G! maintains as much structural similarity to G
as possible, while introducing enough differences to provide the
desired privacy guarantee. We show that simply applying differen-
tial privacy to graphs results in the addition of signiﬁcant noise that
may disrupt graph structure, making it unsuitable for experimen-
tal study. Instead, we introduce a partitioning approach that pro-
vides identical privacy guarantees using much less noise. Applied
to real graphs, this technique requires an order of magnitude less
noise for the same privacy guarantees. Finally, we apply our graph
model to Internet, web, and Facebook social graphs, and show that
it produces synthetic graphs that closely match the originals in both
graph structure metrics and behavior in application-level tests.
Categories and Subject Descriptors
H.3.5 [Information Storage and Retrieval]: Online Information
Services—Data sharing; K.4.1 [Computers and Society]: Public
Policy Issues—Privacy
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’11, November 2–4, 2011, Berlin, Germany.
Copyright 2011 ACM 978-1-4503-1013-0/11/11 ...$10.00.
Keywords
Differential Privacy, Graph Models, Online Social Networks
INTRODUCTION
1.
Studying structure of real social and computer networks through
graph analysis can produce insights on fundamental processes such
as information dissemination, viral spread and epidemics, network
dynamics and resilience to attacks [4, 26, 27, 38]. The use of real
graphs generated from measurement data is invaluable, and can be
used to validate theoretical models or realistically predict the effec-
tiveness of applications and protocols [2,12,41,43].
Unfortunately, there is often a direct tension between the need
to distribute real network graphs to the research community, and
the privacy concerns of users or entities described by the dataset.
For example, social graphs from real measurements are used to
capture a variety of artifacts in online social networks, including
strength of social ties, number and frequency of social interactions,
and ﬂow of information. Similarly, detailed topology graphs of
enterprise networks or major ISPs contain conﬁdential information
about the performance and robustness of these networks. Releasing
such sensitive datasets for research has been challenging. Despite
the best of intentions, researchers often inadvertently release more
data than they originally intended [35,36,47]. Past experience has
taught us that traditional anonymization techniques provide limited
protection, and often can be overcome by privacy attacks that “de-
anonymize” datasets using external or public datasets [5,35,36].
Thus we are left asking the question, how can researchers safely
share realistic graph datasets from measurements without compro-
One option is to develop and apply stronger
mising privacy?
anonymization techniques [24,30], many of which modify the graph
structure in subtle ways that improve privacy but retain much of
the original graph structure. However, these approaches generally
only provide resistance against a speciﬁc type of attack, and can-
not provide protection against newly developed deanonymization
techniques. Techniques exist in the context of databases and data
mining which provide provable levels of protection [18,19], but are
not easily applied to graphs. Still other techniques can protect pri-
vacy on graphs, but must signiﬁcantly change the graph structure
in the process [24,39].
81Our approach to provide graph privacy and preserve graph
structure. We seek a solution to address the above question, by
starting with observation that any system for sharing graphs must
deal with the tension between two goals: protecting privacy and
achieving structural similarity to the original, unmodiﬁed graph.
At one extreme, we can distribute graphs that are isomorphic to the
original, but vulnerable to basic deanonymization attacks. At the
other extreme, we can distribute random graphs that share no struc-
tural similarities to the original. These graphs will not yield any
meaningful information to privacy attacks, but they are also not
useful to researchers, because they share none of the real structures
of the original graph.
Ideally, we want a system that can produce graphs that span the
entire privacy versus similarity spectrum. In such a system, users
can specify a desired level of privacy guarantee, and get back a set
of graphs that are similar to the real graph in structure, but have
enough differences to provide the requested level of privacy.
The main premise of our work is that we can build such a sys-
tem, by distilling an original graph G into a statistical represen-
tation of graph structure, adding controlled levels of “noise,” and
then generating a new graph G! using the result statistics. This
requires two key components. First, we need a way to accurately
capture a graph’s structure as a set of structural statistics, along
with a generator that converts it back into a graph. For this, we
use the dK-series, a graph model that is capable of capturing suf-
ﬁcient graph structure at multiple granularities to uniquely iden-
tify a graph [13, 31]. We can achieve the desired level of privacy
by introducing a speciﬁc level of noise into G’s degree correla-
tion statistics. Second, we need a way to determine the appropriate
noise necessary to guarantee a desired level of privacy. For this, we
develop new techniques rooted in the concept of -differential pri-
vacy, a technique previously used to quantify privacy in the context
of statistical databases.
In this paper, we develop Pygmalion, a differentially private graph
model for generating synthetic graphs. Pygmalion preserves as
much of the original graph structure as possible, while injecting
enough structural noise to guarantee a chosen level of privacy against
privacy attacks. Initially, we formulate a basic differentially pri-
vate graph model, which integrates controlled noise into the dK
degree distributions of an original graph. We use the dK-2 series,
which captures the frequency of adjacent node pairs with differ-
ent degree combinations as a sequence of frequency values. How-
ever, when we derive the necessary conditions required to achieve
-differential privacy, they show that an asymptotical bound for the
required noise grows polynomially with the maximum degree in
the graph. Given the impact of dK values on graph structure, these
large noise values result in synthetic graphs that bear little resem-
blance to the original graph.
To solve this challenge, we seek a more accurate graph model
by signiﬁcantly reducing the noise required to obtain -differential
privacy. We develop an algorithm to partition the statistical rep-
resentation of the graph into clusters, and prove that by achieving
-differential privacy in each cluster, we achieve the same property
over the entire dataset. Using a degree-based clustering algorithm,
we reduce the variance of degree values in each cluster, thereby
dramatically reducing the noise necessary for -differential privacy.
Finally, we apply isotonic regression [6] as a ﬁnal optimization to
further reduce the effective error by more evenly distributing the
added noise.
We apply our models to a number of Internet and Facebook
graphs ranging from 14K nodes to 1.7 million nodes. The results
show that for a given level of privacy, our degree-based clustering
algorithm reduces the necessary noise level by one order of mag-
nitude.
Isotonic regression further reduces the observed error in
dK values on our graphs by 50%. Finally, we experimentally show
that for moderate privacy guarantees, synthetic graphs generated by
Pygmalion closely match the original graph in both standard graph
metrics and application-level experiments.
Access to realistic graph datasets is critical to continuing re-
search in both social and computer networks. Our work shows that
differentially-private graph models are feasible, and Pygmalion is a
ﬁrst step towards graph sharing systems that provide strong privacy
protection while preserving graph structures.
2. GRAPHS AND DIFFERENTIAL PRIVACY
In this section, we provide background on graph anonymization
techniques, and motivate the basic design of our approach to graph
anonymization. First, we discuss prior work, the inherent chal-
lenges in performing graph anonymization, and our desired privacy
goals. Second, we introduce the main concepts of -Differential
Privacy, and lay out the preconditions and challenges in leveraging
this technique to anonymize graphs. Finally, we motivate the selec-
tion of the dK-series as the appropriate graph model on which to
build our system.
2.1 Data Privacy: Background and Goals
A signiﬁcant amount of prior work has been done on protect-
ing privacy of datasets. We summarize them here, and clarify our
privacy goals in this project.
Private Datasets.
Many research efforts have developed pri-
vacy mechanisms to secure large datasets. Most of these tech-
niques, including cryptographic approaches [7] and statistical per-
turbations [19,37], are designed to protect structured data such as
relational databases, and are not applicable to graph datasets. An
alternative, probabilistic approach to privacy is k-anonymity [42].
It is designed to secure sensitive entries in a table by modifying
the table such that each row has at least k − 1 other rows that
are identical [18]. Several public datasets have been successfully
anonymized with k-anonymity [1,33] or through clustering-based
anonymization strategies [8].
Graph Anonymization.
Several graph anonymization tech-
niques have been proposed to enable public release of graphs with-
out compromising user privacy. Generally, these techniques only
protect against speciﬁc, known attacks. The primary goal of these
anonymization techniques is to prevent attackers from identifying
a user or a link between users based on the graph structure. Sev-
eral anonymization techniques [24, 30, 39, 46, 48] leverage the k-
anonymity model to create either k identical neighborhoods, or k
identical-degree nodes in a target graph. These types of “attack-
speciﬁc” defenses have two signiﬁcant limitations. First, recent
results have repeatedly demonstrated that researchers or attackers
can invent novel, unanticipated de-anonymization attacks that de-
stroy previously established privacy guarantees [5,35,36,45]. Sec-
ond, many of these defenses require modiﬁcations to the protected
graph that signiﬁcantly alter its structure in detectable and mean-
ingful ways [24,39].
Our Goals: Edge vs. Node Privacy.
In the context of privacy
for graphs, we can choose to focus on protecting the privacy of
either node or edges. As will become clear later in this paper, our
approach of using degree correlations (i.e. the dK-series), captures
graph structure in terms of different subgraph sizes, ranging from
2 nodes connected by a single edge (dK-2) to larger subgraphs of
size K.
Our general approach is to produce synthetic graphs by adding
controlled perturbations to the graph structure of the original graph.
82This approach can provide protection for both node privacy and
edge privacy. This choice directly impacts the sensitivity of the
graph privacy function, and as a result, how much structural noise
must be introduced to obtain a given level of privacy guarantees.
In this paper, we choose to focus on edge privacy as our goal,
and apply this assumption in our analysis of our differential pri-
vacy system in Section 3. We chose to target edge privacy because
our work was originally motivated by privacy concerns in sharing
social graphs, where providing edge privacy would address a num-
ber of practical privacy attacks.
2.2 Differential Privacy
Our goal is to create a novel system for the generation of anonymized
graphs that support two key properties:
1. Provides quantiﬁable privacy guarantees for graph data that
2. Preserves as much original graph structure as possible, to en-
are “future-proof” against novel attacks.
sure that anonymized data is still useful to researchers.
Differential privacy [14] is a recently developed technique de-
signed to provide and quantify privacy guarantees in the context of
statistical databases [15,25]. Others have demonstrated the versatil-
ity of this technique by applying differential privacy to distributed
systems [40], network trace anonymization [32], data compression
techniques [44], and discrete optimization algorithms [22]. Other
work focused speciﬁcally on applying differential privacy to simple
graph structures such as degree distributions [23,25]. In contrast,
our work has the potential to inject changes at different granular-
ities of substructures in the graph, instead of focusing on a single
graph metric.
One piece of prior work tried to guarantee graph privacy by
adding differential privacy to Kronecker graphs [34]. Whereas this
approach tries to guarantee privacy by perturbing the Kronecker
model parameters, our strategy acts directly on graph structures,
which provides tighter control over the perturbation process. Un-
fortunately, the author asserts there are incorrect results in the pa-
per1.
Basic Differential Privacy.
The core privacy properties in dif-
ferential privacy are derived from the ability to produce a query
output Q from a database D, which could also have been produced
from a slightly different database D!, referred to as D’s neigh-
bor [14].
DEFINITION 1. Given a database D, its neighbor database D!
differs from D in only one element.
We obtain differential privacy guarantees by injecting a con-
trolled level of statistical noise into D [16]. The injected noise
is calibrated based on the sensitivity of the query that is being exe-
cuted, as well as the statistical properties of the Laplace stochastic
process [17]. The sensitivity of a query is quantiﬁed as the max-
imum amount of change to the query’s output when one database
element is modiﬁed, added, or removed. Together, query sensitivity
and the  value determine the amount of noise that must be injected
into the query output in order to provide -differential privacy.
Differential privacy works best with insensitive queries, since
higher sensitivity means more noise must be introduced to attain a
given desired level of privacy. Thus insensitive queries introduce
lower levels of errors, and provide more accurate query results.
1See the author’s homepage.
B
D
C
F
A
E
dK-2
Series
{
 = 1   (A-B)
 = 1   (B-C)
 = 1   (C-D)
 = 2   (E-D), (D-F)
dK-3
Series {  = 1   (A-B-C)
 = 1   (B-C-D)
 = 2   (C-D-E), (C-D-F)
 = 1   (E-D-F)
Figure 1: An illustrative example of the dK-series. The dK-
2 series captures the number of 2-node subgraphs with a spe-
ciﬁc combination of node-degrees, and the dK-3 captures the
number of 3-node subgraphs with distinct node-degree combi-
nations.
2.3 Differential Privacy on Graphs
We face two key challenges in applying differential privacy con-
cepts to privacy protection on graphs. First, we must determine a
“query” function in our context which we can use to apply differen-
tial privacy concepts. Second, the sensitivity of this query function
must be low enough, so that we can attain privacy guarantees by
introducing only low levels of noise, thus allowing us to preserve
the accuracy of the results. In our context, this means that we want
to generate graphs that retain the structure and salient properties of
the original graph. We address the former question in this section
by proposing the use of the dK-series as our graph query opera-
tion. We address the accuracy question in Sections 3 and 4, after
fully explaining the details of our system.
Recall that the problem we seek to address is to anonymize graph
datasets so that they can be safely distributed amongst the research
community. We leverage a non-interactive query model [14], such
that the original graph structure is queried only once and the en-
tire budget to enforce privacy is used at this time. dK is used to
query the graph and the resulting dK-series is perturbed under the
differential privacy framework. Note that only the differentially
private dK-series is publicized. Unlike applications of differential
privacy in other contexts, we can now generate multiple graphs us-
ing this differentially private dK-series without disrupting the level
of privacy of the original graph. Therefore, we use a non-interactive
query model to safely distributed graph datasets without being con-
strained to a single dataset.
The dK-Graph Model.
We observe that the requirements of
this query function can be met by a descriptive graph model that
can transform a graph into a set of structural statistics, which are
then used to generate a graph with structure similar to the original.
Speciﬁcally, we propose to use the dK-graph model [31] and its
statistical series as our query function. dK captures the structure of
a graph at different levels of detail into statistics called dK-series.
dK can analyze an original graph to produce a corresponding dK-
series, then use a matching generator to output a synthetic graph
using the dK-series values as input. The dK-series is the degree
distribution of connected components of some size K within a tar-
get graph. For example, dK-1 captures the number of nodes with
each degree value, i.e.
the node degree distribution. dK-2 cap-
tures the number of 2-node subgraphs with different combinations
of node degrees, i.e. the joint degree distribution. dK-3 captures