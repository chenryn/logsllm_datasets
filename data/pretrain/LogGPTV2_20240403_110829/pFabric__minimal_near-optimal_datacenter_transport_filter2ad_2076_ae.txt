72,
6,
5,
4,
3,
+
+
.
-
,
*
)
(
'
&
%
$
#
"
!
%"!"#,
(-).&*/$
0%-,
(-).&*/, #%1,
2,
(a) Web search workload
2,
(b) Data mining workload
Figure 11: Average normalized FCT for non-deadline trafﬁc
at 60% load.
tightest settings. This is partly because of the one extra RTT of
ﬂow-initiation overhead that PDQ adds to every ﬂow. Because of
this, PDQ cannot meet some of the tighter deadlines for the small
ﬂows (that can ideally complete in 1 RTT). We veriﬁed that when
the average deadline was 100µs, due to its ﬁxed one RTT overhead,
PDQ could not have met the deadline for 22.7% of the deadline-
constrained ﬂows (this number was 5.0% for the 500µs and 2.5%
for the 1ms average deadline settings).
We also ﬁnd that pFabric achieves the lowest average FCT for
the ﬂows without deadlines (Figure 11). pFabric-EDF is slightly
worse as expected because it gives strict priority to the deadline-
constrained trafﬁc.
5.4.3 pFabric deep dive
In this section we dig deeper into pFabric’s design in a series of
targeted simulations. For brevity, the majority of simulations in this
section use the web search workload since it is more challenging
and allows for clearer contrasts.
Impact of qSize and RTO: We repeat the web search workload
at 80% load for different pFabric switch buffer size (qSize) and
retransmission timeout (RT O) settings. qSize is varied between
0.5 × BDP and 5 × BDP (recall that the BDP is 18KB for our
topology). RT O is varied between 2×RT T and 20×RT T where
RT T is the baseline round-trip latency of the fabric (14.6µs). The
results are shown in Figure 12. In the interest of space, we only
show the overall average FCT across all ﬂows and the 99th per-
centile FCT for the small ﬂows. We observe a loss in performance
for buffers smaller than one BDP. At the same time, increasing
the buffer size beyond 2 × BDP yields very little gains. This
is intuitive since we need at least one BDP to allow enough time
for retransmitting dropped packets without under-utilization (§4.3),
but having just one BDP provides zero margin for error and re-
quires perfect RTO estimation to avoid performance loss. As the
plots show, making the buffer size slightly larger than one BDP
gives more margin and allows the use of a simple, ﬁxed RTO with-
out performance loss. We recommend qSize = 2 × BDP and
RT O = 3 × RT T for pFabric based on these results. An RTO of
3 × RT T is appropriate since with a buffer of 2 × BDP , the total
round-trip delay when a packet is dropped (and the buffer is full) is
T
C
F
d
e
z
i
l
a
m
r
o
N
10
8
6
4
2
0
0
RTO = 2RTT
RTO = 3RTT
RTO = 6RTT
RTO = 10RTT
RTO = 20RTT
1
2
3
qSize (BDP)
4
5
T
C
F
d
e
z
i
l
a
m
r
o
N
10
8
6
4
2
0
0
1
2
3
qSize (BDP)
4
5
(a) Overall: Avg
(b) (0, 100KB]: 99th prctile
Figure 12: Overall average FCT and 99th percentile FCT for
small ﬂows for the web search workload at 80% load using a va-
riety of queue size and retransmission timeout settings. qSize
is normalized to BDP = 18KB and the RTO is normalized to
RTT = 14.6µs.
$
+
*
)
$
(
'
&
%
$
#
#
"
!
&!"
%!"
$!"
#!"
!"
,-./01234"
56/01234"
'!("#!)"
'#!("#!!)" '#!!("#*)"
'#*("%*)"
'%*("+*)"
'+*("#!*)" '#!*("$!*)"
,-."-.'/$0$).1$23'#+$
Figure 13: Packet loss rate at the ﬁrst-hop (source NIC) and
last-hop (destination access link) versus priority number for the
web search workload at 80% load. The loss rate in the fabric’s
core is negligible.
3 × RT T . The values we use in our simulations (qSize = 36KB,
RT O = 45µs) follow this guideline.
While the above guideline guarantees good performance in all
cases, interestingly, Figure 12 suggests that for realistic workloads
we can use a much larger RTO with almost no performance penalty.
For instance, RT O = 20 × RT T (∼290µs for our fabric) achieves
nearly the same performance as RT O = 3 × RT T when qSize =
2 × BDP . Relaxing the required retransmission timeout could be
very useful in practice and simplify the pFabric host’s implemen-
tation; as prior work has demonstrated [19], retransmission timers
with a granularity of 200µs are easy to achieve in software.
The reason such large RTOs do not have signiﬁcant impact (de-
spite the small buffers) is that almost all packet drops in pFabric
occur for the large ﬂows which anyway have fairly high FCTs. To
see this, we plot the packet loss rate versus the packet priority num-
ber for the baseline web search workload at 80% load in Figure 13.
The plot shows that almost all losses are for ﬂows larger than 3000
packets. But these ﬂows are bandwidth-limited and necessarily take
a long time to complete. For example, a 3000 packet ﬂow (1500
bytes per packet) needs at least 3.6ms to complete at 10Gbps and
thus is not severely impacted if the RTO is not very tight and adds
∼200µs of additional delay.
Different priority assignment schemes: Next, we compare three
different schemes for assigning packet priorities with increasing de-
grees of complexity. For each packet transmitted, the priority ﬁeld
is set to be: (i) the number of bytes thus far sent from the ﬂow; (ii)
the ﬂow size in bytes; or (iii) the remaining ﬂow size in bytes (the
default scheme in this paper). The ﬁrst scheme is the simplest as
it does not require knowledge of ﬂow size. The second and third
schemes both require ﬂow size information, but the second is sim-
pler since the priority number is decided once and remains constant
for all the packets of a ﬂow. As explained in §4.1, this scheme sim-
pliﬁes the pFabric switch implementation since we don’t need the
starvation prevention mechanism.
443T
C
F
d
e
z
i
l
a
m
r
o
N
15
10
5
0
PDQ
pFabric - BytesSent
pFabric - FlowSize
pFabric - RemainingFlowSize
Ideal
0.2
0.4
Load
0.6
0.8
T
C
F
d
e
z
i
l
a
m
r
o
N
5
4
3
2
1
0
0.2
0.4
Load
0.6
0.8
(a) Web search workload
(b) Data mining workload
Figure 14: Overall average FCT for different priority assign-
ment schemes. Note the different y-axis range in these plots.
Figure 14 shows a comparison of the three schemes and also
PDQ. We ﬁnd that using the ﬂow size and remaining ﬂow size as
the packet priority achieve nearly indistinguishable overall aver-
age FCT. This is not surprising; even though remaining ﬂow size
is conceptually closer to ideal (§3), for realistic workloads with a
diverse range of ﬂow sizes, most of the beneﬁt is in scheduling
the small ﬂows before the large ﬂows which both schemes achieve.
We do ﬁnd that for the large ﬂows (> 10MB), the remaining ﬂow
size scheme achieves up to ∼15% lower average FCT than absolute
ﬂow size (plot omitted, see [6]).
The performance of “BytesSent” is more varied. As expected, it
is worse than the schemes with ﬂow size knowledge. Yet, for the
data mining workload, it still achieves a signiﬁcantly lower over-
all average FCT than PDQ. In fact, we ﬁnd that its average and
tail FCT for the small ﬂows (0 are enqueued in the high-priority (low-priority) queue.
Therefore, the arrival processes to the two queues are indepen-
dent Poisson processes with rates λFS(t) and λ(1 − FS(t)). The
high-priority queue has strict priority and drains at rate 1. The
low-priority queue uses the remaining bandwidth after servicing
the high-priority trafﬁc. Thus, its drain rate is 1 − ρBS(t) where
0 xfS(x) dx/E(S) is the fraction of the overall bytes
that belong to ﬂows smaller than t. Note that in reality, the low-
priority queue drains only when the high-priority queue is empty.
However, this complicates the analysis since the two queues are de-
pendent. By using the average drain rate of the low priority queue
as its instantaneous drain rate, we greatly simplify the analysis.
BS(t) = ! t
The average normalized FCT (FCT divided by ﬂow size) can be