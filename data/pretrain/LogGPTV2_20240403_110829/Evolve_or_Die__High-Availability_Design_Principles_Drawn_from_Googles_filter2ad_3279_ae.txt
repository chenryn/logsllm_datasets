Update network elements consistently. We have seen sev-
eral instances where errors in software or in management
plane operations have left network elements inconsistent.
On B4 and clusters, inconsistencies between control plane
elements have led to cascading failures (Casc-1), hard-to-
debug trafﬁc blackholes (Consis-1) and rack disconnects.
Management operations can also leave a fabric in an incon-
sistent state; in one failure event, a MOp that re-conﬁgured
every switch in a CAR left two switches unconﬁgured, re-
sulting in packet loss.
Control plane and data plane synchronization can be
achieved by hardening the control plane stack, by either
synchronously updating the data plane before advertising
reachability, or waiting for the fabric to converge before
advertising external reachability. For control plane state
inconsistencies, program analysis techniques [25, 24] that
determine state equivalence can help catch some of these
inconsistencies. A complementary approach would be to
dynamically track state equivalence. For example, tracking
the difference in the route counts between the RA and the
OFC might have provided early warning of Consis-1. More
generally, automated techniques to track state provenance
and equivalence in distributed systems is an interesting
research direction. For the management plane, tools that
determine if, at the end of every major step of a MOp, fabric
state is consistent, can provide early warning of failure.
More generally, transactional updates where a conﬁguration
change is applied to all the switches, or none can help avoid
these inconsistencies, but requires careful design of rollback
67
strategies for management plane operations.
Continuously monitor network operational invariants.
Most failures are the result of one or more violations of fail-
ure assumptions, which can be cast as network operational
invariants and continuously tested for. For example, on B2,
we use anycast BGP to provide robust, low latency access to
internal services like DNS. An invariant, not widely known,
was that anycast preﬁxes should have a path length of 38.
A service violated this invariant, causing an outage of our
internal DNS by drawing all DNS trafﬁc to one cluster.
Similarly, all failures resulting from bad trafﬁc priority
markings violate previously agreed upon service-to-priority
mappings. Beyond that, there are many other control plane
design invariants: peering routers maintaining dual BGP
sessions to B2CRs, CARs being connected to 2 B2BRs,
OFCs having redundant connectivity to 2 CPN routers.
Unfortunately, many of these decisions are often implicit
rather than explicitly set as a requirement.
Extracting
implicit requirements and design invariants from code or
conﬁgurations is an interesting research direction.
Monitoring systems for testing these invariants must go
beyond simply sending alarms when an invariant is violated;
in a large network, too many alarms can be triggered to the
point where operators stop paying attention to them. Rather,
these monitoring systems must be able to aggregate these
alarms (for example, determine how long a violation has
been occurring), reason continuously about the risk the vi-
olation presents, and either present to operators a prioritized
list of violations or take evasive action to minimize or avoid
the risk of failures from these violations.
Sometimes these operational invariants can be violated by
poorly designed automated management plane software. In a
few failures, automated software shut down all control plane
component replicas. Ideally, the automated software should
have refused to violate the invariant that at least one OFC
instance must be running.
Require Both the Positive and Negative. A number of our
substantial failures resulted from pushing changes to a large
set of devices, e.g., a misconﬁgured wildcard causing a drain
of hundreds of devices when the intention was to drain two.
Though we have sanity checks to avoid human mistakes,
broad changes are also occasionally necessary. For MOps
with potentially large scope, we now require two separate
conﬁguration speciﬁcations for a network change coming
from two separate data sources. Draining a large number
of devices, for instance, requires specifying (a) all of the de-
vices to be drained and (b) an explicit, separate list of devices
to be left undrained. The management software performing
the MOp rejects conﬁgurations where there is inconsistency
between the two lists.
8The reason for this is interesting: we have several generations of cluster
designs in our network, and the number of hops between the cluster fabric
and B2 varies from 1-3 depending on the generation. Unifying the path
length to 3 permits the BGP decision process to fall through to IGP routing,
enabling anycast.
68
Management Homogeneity with System Heterogeneity.
Over the years, we developed different management systems
for our three networks. However, there was no common
architecture among the three systems, meaning that new
automation techniques and consistency checks developed
for one network would not naturally apply to another.
Hence, we are working toward a uniﬁed and homogeneous
network management architecture with well-deﬁned APIs
and common network models for common operations. This
promotes shared learning and prevents multiple occurrences
of the same error. Moreover, the underlying system het-
erogeneity especially in the WAN, where control plane
and monitoring systems are developed by different teams,
ensures highly uncorrelated faults: a catastrophic fault in
one network is much less likely to spread to the other.
7.3 Fail Open
A repeated failure pattern we have seen is the case where
small changes in physical connectivity have led to large in-
consistencies in the control plane. In these cases, the control
plane quickly and incorrectly believes that large portions of
the physical network have failed. To avoid this, our systems
employ fail-open strategies to preserve as much of the data
plane as possible when the control plane fails.
Preserve the data plane. The idea behind fail-open is sim-
ple: when a control plane stack fails (for any reason), it does
not delete the data plane state. This preserves the last known
good state of the forwarding table of that node, which can
continue to forward trafﬁc unless there are hardware failures
(e.g., link failures) that render the forwarding table incorrect.
Fail-open can be extended to an entire fabric, CAR, or B4BR
in the event of a massive control plane failure. In this case,
especially if the failure happens within a short time, switch
forwarding tables will be mutually consistent (again, mod-
ulo hardware failures), preserving fabric or device availabil-
ity. Fail-open can be an effective strategy when the time to
repair of the control plane is smaller than the time to hard-
ware failure. Two challenges arise in the design of correct
fail-open mechanisms: how to detect that a switch or a col-
lection of switches has failed open, and how to design the
control plane of functional (non-failed) peers to continue to
use the failed-open portions of a fabric.
Verify large control plane updates. In a few failures, con-
trol plane elements conservatively assumed that if part of
a state update was inconsistent, the entire state update was
likely to be incorrect. For example, in Casc-1, Topology
Modeler marked some preﬁxes within the network as origi-
nating from two different clusters. In response, BwE shifted
trafﬁc from clusters in several metros on to B2, assuming
all of B4 had failed. In another, similar failure, TE Server
conservatively invalidated the entire B4 topology model be-
cause the model contained a small inconsistency, resulting
from the overlapping IP preﬁx of a decommissioned cluster
still appearing in the network conﬁguration.
Conservatively and suddenly invalidating large parts of
the topology model can, especially in a WAN, signiﬁcantly
affect availability targets. To preserve availability, control
plane elements can degrade gracefully (a form of fail-open)
when they receive updates that would invalidate or take of-
ﬂine large parts of the network. Speciﬁcally, they can at-
tempt to perform more careful validation and local correc-
tion of state updates in order to preserve the “good” control
plane state, for example, by inspecting monitoring systems.
In Casc-1, monitoring systems that conduct active probes
[14] or provide a query interface to BGP feeds from all BGP
speakers in the network could have been used to corrobo-
rate (or refute) the dual preﬁx origination or the overlapping
preﬁxes between clusters. If these methods had been imple-
mented, the impact of these failures on availability would
have been less.
7.4 An Ounce of Prevention
Our experiences have also taught us that continuous risk
assessment, careful testing, and developing a homogeneous
management plane while preserving network heterogeneity
can avoid failures or prevent re-occurrences of the same fail-
ure across different networks.
Assess Risk Continuously. In the past, risk assessment
failures have resulted from incorrect estimates of capacity
(Risk-1), bad demand assessments, changes in network state
between when the risk assessment was performed and when
the MOp was conducted (Risk-2),
intermediate network
states during MOps that violated risk assumptions, or lack
of knowledge of other concurrent MOps.
Risk assessments must be a continuous activity, account-
ing for ongoing network dynamics, and performed at every
step of the MOp. This, requires a signiﬁcant degree of visi-
bility into the network (discussed later), and automation to-
gether with the ability to either roll-back a MOp when risk
increases in the middle of a MOp (for example, because of
a concurrent failure), or the ability to drain services quickly.
Assessing the risk of drains and the risk of undrains during
failure recovery are also both crucial: in some of our fail-
ures, the act of draining services has caused transient trafﬁc
overloads (due, for example, to storage services reconciling
replicas before the drain), as has the act of undraining them.
In general, risk assessment should be tightly integrated
into the control plane such that it can leverage the same state
and algorithms as the deployed control plane, rather than
requiring operators to reason about complex operations,
application requirements, and current system state. We
have found our risk assessment
is substantially better
for our custom-built networks (B4 and clusters) than for
those built from vendor gear (B2), since for the former we
have detailed knowledge of exactly how drain and routing
behaves and over what time frames, while vendor gear
is more “black box.” This suggests that, for vendor gear,
risk assessments can be improved by increasing visibility
into both management and control plane operations that
would allow better emulation of the vendor behavior, or
by providing higher-level drain/undrain operations with
well-understood semantics.
In cases where the semantics
of a management operation or of a particular protocol
implementation choice are unclear, risk assessment should
err on the side of overestimating risk.
Canary. Because we implement our own control plane
stack, we have developed careful
in-house testing and
rollout procedures for control plane software updates and
conﬁguration changes. Many of these procedures, such as
extensive regression testing and lab testing, likely mirror
practices within other large software developers. Beyond
these, we also test software releases by emulating our
networks [38] at reasonable scale, both during the initial
development cycle and prior to rollout.
Finally, especially for software changes that impact sig-
niﬁcant parts of the control plane, we rollout changes very
conservatively in a process that can take several weeks. For
example, for changes to ToR software, we might (after lab
testing), ﬁrst deploy it in a small fraction (0.1% of ToRs) in a
small cluster (each such test deployment is called a canary),
then progressively larger fractions and then repeat the pro-
cess at a larger cluster, before rolling it out across the entire
network. After each canary, we carefully monitor the de-
ployment for a few days or a week before moving on to the
next. Some failure events have occurred during a canary. For
updates to replicated control plane components like the B4
Gateway, for example, we update one replica at a time, and
monitor consensus between replicas. This approach enabled
us to avoid trafﬁc impact in at least one failure event.
7.5 Recover Fast
A common thread that runs through many of the failure
events is the need for ways to root-cause the failure quickly.
This process can take hours, during which a cluster may be
completely drained of services (depending on the severity
of the failure). Operators generally root-cause failures by
(initially) examining aggregated outputs from two large
monitoring systems: an active path probing system like
[14], and a passive global per-device statistics collection
system. When a failure event occurs, operators examine
dashboards presented by these systems, look for anomalies
(unusual spikes in statistics or probe failures) in parts of the
topology near the failure event, and use these indicators to
drill-down to the root causes.
Delays in root-causing several failures have occurred for
two reasons. First, when a failure event occurs, the dash-
boards may indicate multiple anomalies: in a large network,
it is not unusual to ﬁnd concurrent anomalies (or, to put
another way, the network is always in varying degrees of
“bad” state). Operators have, several times, drilled down on
the wrong anomaly before back-tracking and identifying the
correct root-cause (e.g., CPN-2). Second, monitoring sys-
tems sometimes don’t have adequate coverage. Given the
scale of Google’s system, and the complexity of the topology
interconnects, the active probing system sometimes lacks
coverage of paths, and the passive collector might not col-
lect certain kinds of statistics (e.g., link ﬂaps) or might ag-
gregate measurements and so miss transients. Occasionally,
bad placement of the collectors can hamper visibility into
the network. In general, from each cluster, CAR or B4BR,
69
statistics are collected at two topologically distinct clusters.
In a few of the failures (e.g., Over-1), the collection agents
for a CAR were both in the same cluster, and when the CAR
failed, the measurement data could not be accessed.
In addition to relying on generic monitoring systems that
may have to trade coverage or granularity for scale, auto-
mated root-cause diagnosis systems can be effective in re-
ducing mean time to recovery, thereby improving availabil-
ity. The design of such systems is currently under explo-
ration.
7.6 Continuously Upgrade the Network!
Our observation that touching the network leads to avail-
ability failures could lead to the following conclusion: limit
the rate at which the network evolves. This is undesirable
because: i) the network would be much more expensive than
necessary because capacity must be augmented signiﬁcantly
ahead of demand, ii) the network would not support the fea-
tures necessary to support evolving application needs, such
as low latency and robust congestion control, and iii) the lim-
ited number of change operations would mean that the net-
work would treat change as a rare event handled by code
paths that are rarely exercised.
We have internally come to the opposite conclusion. Es-
pecially in a software-deﬁned network infrastructure, and
with increasing automation of the management plane, there
is an opportunity to make upgrade and change the common
case. We strive to push new features and conﬁguration into
production every week. This requires the capability to up-
grade the network daily, perhaps multiple times. This would
be required for example to address ab initio bugs but also
to support rapid development of new functionality in our lab
testbeds. Frequent upgrade also means that we are able to
introduce a large number of incremental updates to our in-
frastructure rather than a single “big bang” of new features
accumulated over a year or more. We have found the for-
mer model to be much safer and also much easier to reason
about and verify using automated tools. In addition, needing
to perform frequent upgrades forces operators to really rely
on automation to monitor and conﬁrm safety (as opposed to
relying on manual veriﬁcation), and dissuades them from as-
suming that the SDN is very consistent (for instance, assume
that all components are running the same version of the soft-
ware); this has resulted in more robust systems, processes,
and automation.
7.7 Research in High-Availability Design