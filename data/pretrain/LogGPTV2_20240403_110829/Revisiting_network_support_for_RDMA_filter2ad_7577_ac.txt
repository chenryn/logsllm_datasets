formance for Timely (though, not for DCQCN). Nonetheless,
SACK-based loss recovery continued to perform significantly
better across different scenarios (with the difference in aver-
age FCT for Timely ranging from 20%-50%).
(2) Do we need SACKs? We tried a selective retransmit scheme
without SACKs (where the sender does not maintain a bitmap
to track selective acknowledgements). This performed better
than go-back-N. However, it fared poorly when there were
multiple losses in a window, requiring multiple round-trips
to recover from them. The corresponding degradation in
average FCT ranged from <1% up to 75% across different
scenarios when compared to SACK-based IRN.
(3) Can the timeout value be computed dynamically? As de-
scribed in §3, IRN uses two static (low and high) timeout
values to allow faster recovery for short messages, while
avoiding spurious retransmissions for large ones. We also
experimented with an alternative approach of using dynami-
cally computed timeout values (as with TCP), which not only
complicated the design, but did not help since these effects
were then be dominated by the initial timeout value.
Significance of BDP-FC: The first and the third bars in
Figure 7 compare the average FCT of IRN with and with-
out BDP-FC respectively. We find that BDP-FC significantly
improves performance by reducing unnecessary queuing.
Furthermore, it prevents a flow that is recovering from a
RoCE (with PFC)RoCE without PFC+Timely+DCQCN051015202530Avg. Slowdown+Timely+DCQCN0123456Avg. FCT (ms)+Timely+DCQCN0102030405060708099%ile FCT (ms)IRNIRN + TimelyIRN + DCQCN0.00.51.01.52.02.53.0Avg. FCT (ms)7.1msIRNIRN with Go-Back-NIRN without BDP-FCRevisiting Network Support for RDMA
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
(a) No CC
(b) Timely
(c) DCQCN
Figure 8: The figures compare the tail latency for single-packet messages for IRN, IRN with PFC, and RoCE (with
PFC), across different congestion control algorithms.
packet loss from sending additional new packets and increas-
ing congestion, until the loss has been recovered.
Efficient Loss Recovery vs BDP-FC: Comparing the sec-
ond and third bars in Figure 7 shows that the performance
of IRN with go-back-N loss recovery is generally worse than
the performance of IRN without BDP-FC. This indicates that
of the two changes IRN makes, efficient loss recovery helps
performance more than BDP-FC.
4.4 Robustness of Basic Results
We now evaluate the robustness of the basic results from
§4.2 across different scenarios and performance metrics.
4.4.1 Varying Experimental Scenario. We evaluate
the robustness of our results, as the experimental scenario
is varied from our default case. In particular, we run experi-
ments with (i) link utilization levels varied between 30%-90%,
(ii) link bandwidths varied from the default of 40Gbps to
10Gbps and 100Gbps, (iii) larger fat-tree topologies with 128
and 250 servers, (iv) a different workload with flow sizes
uniformly distributed between 500KB to 5MB, representing
background and storage traffic for RDMA, (v) the per-port
buffer size varied between 60KB-480KB, (vi) varying other
IRN parameters (increasing RTOhiдh value by up to 4 times
the default of 320µs, and increasing the N value for using
RTOlow to 10 and 15). We summarize our key observations
here and provide detailed results for each of these scenarios
in Appendix §A of an extended report [31].
Overall Results: Across all of these experimental scenarios,
we find that:
(a) IRN (without PFC) always performs better than RoCE
(with PFC), with the performance improvement ranging from
6% to 83% across different cases.
(b) When used without any congestion control, enabling PFC
with IRN always degrades performance, with the maximum
degradation across different scenarios being as high as 2.4×.
(c) Even when used with Timely and DCQCN, enabling PFC
with IRN often degrades performance (with the maximum
degradation being 39% for Timely and 20% for DCQCN). Any
improvement in performance due to enabling PFC with IRN
stays within 1.6% for Timely and 5% for DCQCN.
Figure 9: The figure shows the ratio of request comple-
tion time of incast with IRN (without PFC) over RoCE
(with PFC) for varying degree of fan-ins across conges-
tion control algorithms.
Some observed trends: The drawbacks of enabling PFC
with IRN:
(a) generally increase with increasing link utilization, as the
negative impact of congestion spreading with PFC increases.
(b) decrease with increasing bandwidths, as the relative cost
of a round trip required to react to packet drops without PFC
also increases.
(c) increase with decreasing buffer sizes due to more pauses
and greater impact of congestion spreading.
We further observe that increasing RTOhiдh or N had a
very small impact on our basic results, showing that IRN is
not very sensitive to the specific parameter values.
4.4.2 Tail latency for small messages. We now look
at the tail latency (or tail FCT) of the single-packet messages
from our default scenario, which is another relevant metric
in datacenters [29]. Figure 8 shows the CDF of this tail la-
tency (from 90%ile to 99.9%ile), across different congestion
control algorithms. Our key trends from §4.2 hold even for
this metric. This is because IRN (without PFC) is able to re-
cover from single-packet message losses quickly due to the
low RTOlow timeout value. With PFC, these messages end
up waiting in the queues for similar (or greater) duration
due to pauses and congestion spreading. For all cases, IRN
performs significantly better than RoCE.
4.4.3 Incast. We now evaluate incast scenarios, both
with and without cross-traffic. The incast workload without
any cross traffic can be identified as the best case for PFC,
012345Latency (ms)9092949698100CDFRoCE (with PFC)IRN with PFCIRN (without PFC)0.00.20.40.60.81.0Latency (ms)9092949698100CDFRoCE (with PFC)IRN with PFCIRN (without PFC)0.00.20.40.60.81.01.21.4Latency (ms)9092949698100CDFRoCE (with PFC)IRN with PFCIRN (without PFC)101520253035404550Number of senders (M) 0.920.940.960.981.001.02RCT Ratio  (IRN / RoCE) NoCCDCQCNTimelySIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
R. Mittal et al.
since only valid congestion-causing flows are paused without
unnecessary head-of-the-line blocking.
Incast without cross-traffic: We simulate the incast work-
load on our default topology by striping 150MB of data across
M randomly chosen sender nodes that send it to a fixed desti-
nation node [17]. We vary M from 10 to 50. We consider the
request completion time (RCT) as the metric for incast per-
formance, which is when the last flow completes. For each M,
we repeat the experiment 100 times and report the average
RCT. Figure 9 shows the results, comparing IRN with RoCE.
We find that the two have comparable performance: any in-
crease in the RCT due to disabling PFC with IRN remained
within 2.5%. The results comparing IRN’s performance with
and without PFC looked very similar. We also varied our
default incast setup by changing the bandwidths to 10Gbps
and 100Gbps, and increasing the number of connections per
machine. Any degradation in performance due to disabling
PFC with IRN stayed within 9%.
Incast with cross traffic: In practice we expect incast to oc-
cur with other cross traffic in the network [23, 29]. We started
an incast as described above with M = 30, along with our
default case workload running at 50% link utilization level.
The incast RCT for IRN (without PFC) was always lower
than RoCE (with PFC) by 4%-30% across the three congestion
control schemes. For the background workload, the perfor-
mance of IRN was better than RoCE by 32%-87% across the
three congestion control schemes and the three metrics (i.e.,
the average slowdown, the average FCT and the tail FCT).
Enabling PFC with IRN generally degraded performance for
both the incast and the cross-traffic by 1-75% across the three
schemes and metrics, and improved performance only for
one case (incast workload with DCQCN by 1.13%).
4.4.4 Window-based congestion control. We also
implemented conventional window-based congestion con-
trol schemes such as TCP’s AIMD and DCTCP [15] with
IRN and observed similar trends as discussed in §4.2. In fact,
when IRN is used with TCP’s AIMD, the benefits of disabling
PFC were even stronger, because it exploits packet drops as
a congestion signal, which is lost when PFC is enabled.
Summary: Our key results i.e., (1) IRN (without PFC) per-
forms better than RoCE (with PFC), and (2) IRN does not
require PFC, hold across varying realistic scenarios, conges-
tion control schemes and performance metrics.
4.5 Comparison with Resilient RoCE.
A recent proposal on Resilient RoCE [34] explores the use of
DCQCN to avoid packet losses in specific scenarios, and thus
eliminate the requirement for PFC. However, as observed
previously in Figure 6, DCQCN may not always be successful
in avoiding packet losses across all realistic scenarios with
Figure 10: The figures compares resilient RoCE
(RoCE+DCQCN without PFC) with IRN.
Figure 11: The figures compares iWARP’s transport
(TCP stack) with IRN.
more dynamic traffic patterns and hence PFC (with its accom-
panying problems) remains necessary. Figure 10 provides a
direct comparison of IRN with Resilient RoCE. We find that
IRN, even without any explicit congestion control, performs
significantly better than Resilient RoCE, due to better loss
recovery and BDP-FC.
4.6 Comparison with iWARP.
We finally explore whether IRN’s simplicity over the TCP
stack implemented in iWARP impacts performance. We com-
pare IRN’s performance (without any explicit congestion
control) with full-blown TCP stack’s, using INET simulator’s
in-built TCP implementation for the latter. Figure 11 shows
the results for our default scenario. We find that absence of
slow-start (with use of BDP-FC instead) results in 21% smaller
slowdowns with IRN and comparable average and tail FCTs.
These results show that in spite of a simpler design, IRN’s per-
formance is better than full-blown TCP stack’s, even without
any explicit congestion control. Augmenting IRN with TCP’s
AIMD logic further improves its performance, resulting in
44% smaller average slowdown and 11% smaller average FCT
as compared to iWARP. Furthermore, IRN’s simple design
allows it to achieve message rates comparable to current
RoCE NICs with very little overheads (as evaluated in §6).
An iWARP NIC, on the other hand, can have up to 4× smaller
message rate than a RoCE NIC (§2). Therefore, IRN provides
a simpler and more performant solution than iWARP for
eliminating RDMA’s requirement for a lossless network.
5 Implementation Considerations
We now discuss how one can incrementally update RoCE
NICs to support IRN’s transport logic, while maintaining
Resilient RoCEIRN0246810121416Avg Slowdown0.00.51.01.52.02.53.03.54.0Avg FCT (ms)01020304050607099%ile FCT (ms)iWARPIRN024681012Avg Slowdown0.00.20.40.60.81.0Avg FCT (ms)024681012141699%ile FCT (ms)Revisiting Network Support for RDMA
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
the correctness of RDMA semantics as defined by the Infini-
band RDMA specification [4]. Our implementation relies on
extensions to RDMA’s packet format, e.g., introducing new
fields and packet types. These extensions are encapsulated
within IP and UDP headers (as in RoCEv2) so they only effect
the endhost behavior and not the network behavior (i.e. no
changes are required at the switches). We begin with provid-
ing some relevant context about different RDMA operations
before describing how IRN supports them.
5.1 Relevant Context
The two remote endpoints associated with an RDMA mes-
sage transfer are called a requester and a responder. The in-
terface between the user application and the RDMA NIC is
provided by Work Queue Elements or WQEs (pronounced
as wookies). The application posts a WQE for each RDMA
message transfer, which contains the application-specified
metadata for the transfer. It gets stored in the NIC while the
message is being processed, and is expired upon message
completion. The WQEs posted at the requester and at the
responder NIC are called Request WQEs and Receive WQEs
respectively. Expiration of a WQE upon message completion
is followed by the creation of a Completion Queue Element
or a CQE (pronounced as cookie), which signals the message
completion to the user application. There are four types of
message transfers supported by RDMA NICs:
Write: The requester writes data to responder’s memory.
The data length, source and sink locations are specified in
the Request WQE, and typically, no Receive WQE is required.
However, Write-with-immediate operation requires the user
application to post a Receive WQE that expires upon com-
pletion to generate a CQE (thus signaling Write completion
at the responder as well).
Read: The requester reads data from responder’s memory.
The data length, source and sink locations are specified in
the Request WQE, and no Receive WQE is required.
Send: The requester sends data to the responder. The data
length and source location is specified in the Request WQE,
while the sink location is specified in the Receive WQE.
Atomic: The requester reads and atomically updates the data
at a location in the responder’s memory, which is specified
in the Request WQE. No Receive WQE is required. Atomic
operations are restricted to single-packet messages.
5.2 Supporting RDMA Reads and Atomics
IRN relies on per-packet ACKs for BDP-FC and loss recovery.
RoCE NICs already support per-packet ACKs for Writes and
Sends. However, when doing Reads, the requester (which
is the data sink) does not explicitly acknowledge the Read
response packets. IRN, therefore, introduces packets for read
(N)ACKs that are sent by a requester for each Read response
packet. RoCE currently has eight unused opcode values
available for the reliable connected QPs, and we use one
of these for read (N)ACKs. IRN also requires the Read respon-
der (which is the data source) to implement timeouts. New
timer-driven actions have been added to the NIC hardware
implementation in the past [34]. Hence, this is not an issue.
RDMA Atomic operations are treated similar to a single-
packet RDMA Read messages.
Our simulations from §4 did not use ACKs for the RoCE
(with PFC) baseline, modelling the extreme case of all Reads.
Therefore, our results take into account the overhead of per-
packet ACKs in IRN.
5.3 Supporting Out-of-order Packet Delivery
One of the key challenges for implementing IRN is support-
ing out-of-order (OOO) packet delivery at the receiver –
current RoCE NICs simply discard OOO packets. A naive
approach for handling OOO packet would be to store all of
them in the NIC memory. The total number of OOO packets
with IRN is bounded by the BDP cap (which is about 110
MTU-sized packets for our default scenario as described in
§4.1) 4. Therefore to support a thousand flows, a NIC would
need to buffer 110MB of packets, which exceeds the memory
capacity on most commodity RDMA NICs.
We therefore explore an alternate implementation strat-
egy, where the NIC DMAs OOO packets directly to the final
address in the application memory and keeps track of them
using bitmaps (which are sized at BDP cap). This reduces NIC
memory requirements from 1KB per OOO packet to only
a couple of bits, but introduces some additional challenges
that we address here. Note that partial support for OOO
packet delivery was introduced in the Mellanox ConnectX-5
NICs to enable adaptive routing [11]. However, it is restricted