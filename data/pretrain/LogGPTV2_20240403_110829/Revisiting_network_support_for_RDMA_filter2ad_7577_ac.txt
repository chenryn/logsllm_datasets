### Performance Analysis and Evaluation

#### SACK-based Loss Recovery
SACK-based loss recovery consistently outperformed other methods across various scenarios, with the average FCT (Flow Completion Time) for Timely ranging from 20% to 50% better. This improvement was observed even though DCQCN did not benefit as much.

#### Selective Retransmit Scheme without SACKs
We evaluated a selective retransmit scheme that does not use SACKs, meaning the sender does not maintain a bitmap to track selective acknowledgments. This method performed better than go-back-N but struggled in scenarios with multiple losses within a window, requiring multiple round-trips for recovery. The degradation in average FCT compared to SACK-based IRN ranged from less than 1% to 75% across different scenarios.

#### Dynamic Timeout Value Computation
As described in §3, IRN uses two static timeout values (low and high) to facilitate faster recovery for short messages while avoiding spurious retransmissions for large ones. We also experimented with dynamically computed timeout values, similar to TCP. However, this approach not only complicated the design but also did not improve performance, as the initial timeout value dominated the effects.

### Significance of BDP-FC
Figure 7 compares the average FCT of IRN with and without BDP-FC. BDP-FC significantly improves performance by reducing unnecessary queuing and preventing flows recovering from packet loss from sending additional new packets, thus mitigating congestion until the loss is recovered.

### Efficient Loss Recovery vs. BDP-FC
Comparing the second and third bars in Figure 7, we find that IRN with go-back-N loss recovery generally performs worse than IRN without BDP-FC. This indicates that efficient loss recovery contributes more to performance improvements than BDP-FC.

### Robustness of Basic Results
We evaluated the robustness of our basic results across different scenarios and performance metrics.

#### Varying Experimental Scenarios
We varied several parameters:
- **Link Utilization:** 30% to 90%
- **Link Bandwidth:** 10Gbps, 40Gbps, and 100Gbps
- **Topology:** Larger fat-tree topologies with 128 and 250 servers
- **Workload:** Flow sizes uniformly distributed between 500KB to 5MB
- **Buffer Size:** Per-port buffer size varied from 60KB to 480KB
- **IRN Parameters:** Increased RTOhigh value up to 4 times the default and increased N value for using RTOlow to 10 and 15

**Key Observations:**
- IRN (without PFC) always outperformed RoCE (with PFC), with performance improvements ranging from 6% to 83%.
- Enabling PFC with IRN degraded performance, with the maximum degradation being 2.4×.
- Even with Timely and DCQCN, enabling PFC with IRN often degraded performance (maximum degradation: 39% for Timely and 20% for DCQCN). Any improvement due to PFC was minimal (within 1.6% for Timely and 5% for DCQCN).

#### Tail Latency for Small Messages
Figure 8 shows the CDF of tail latency (from 90th to 99.9th percentile) for single-packet messages. IRN (without PFC) recovers quickly from single-packet message losses due to the low RTOlow timeout value. With PFC, these messages experience longer delays due to pauses and congestion spreading. IRN consistently outperformed RoCE.

#### Incast Scenarios
We evaluated incast scenarios with and without cross-traffic.

**Incast without Cross-Traffic:**
- Simulated on a default topology with 150MB of data striped across M randomly chosen sender nodes.
- Varying M from 10 to 50, we found that IRN and RoCE had comparable performance, with any increase in RCT (Request Completion Time) due to disabling PFC with IRN remaining within 2.5%.

**Incast with Cross-Traffic:**
- Incast with M = 30 and 50% link utilization level.
- IRN (without PFC) showed lower RCT than RoCE (with PFC) by 4%-30% across three congestion control schemes.
- For the background workload, IRN outperformed RoCE by 32%-87% across three metrics (average slowdown, average FCT, and tail FCT).
- Enabling PFC with IRN generally degraded performance by 1-75%, with only one case showing a 1.13% improvement (incast workload with DCQCN).

#### Window-Based Congestion Control
We implemented conventional window-based congestion control schemes like TCP’s AIMD and DCTCP with IRN. The trends were consistent with those discussed in §4.2. Disabling PFC with TCP’s AIMD further enhanced the benefits, as it exploits packet drops as a congestion signal, which is lost when PFC is enabled.

### Summary
- **Key Results:**
  1. IRN (without PFC) outperforms RoCE (with PFC).
  2. IRN does not require PFC and maintains its performance advantages across varying realistic scenarios, congestion control schemes, and performance metrics.

### Comparison with Resilient RoCE
Resilient RoCE [34] explores using DCQCN to avoid packet losses and eliminate the need for PFC. However, DCQCN may not always be successful in all scenarios, making PFC necessary. Figure 10 compares IRN with Resilient RoCE, showing that IRN, even without explicit congestion control, performs better due to improved loss recovery and BDP-FC.

### Comparison with iWARP
We compared IRN's performance (without explicit congestion control) with a full-blown TCP stack (iWARP). Figure 11 shows that IRN, despite its simpler design, achieves 21% smaller slowdowns and comparable average and tail FCTs. Augmenting IRN with TCP’s AIMD logic further improves performance, resulting in 44% smaller average slowdown and 11% smaller average FCT compared to iWARP. IRN also achieves message rates comparable to current RoCE NICs with minimal overhead, whereas iWARP NICs can have up to 4× smaller message rates.

### Implementation Considerations
To support IRN's transport logic, we propose incremental updates to RoCE NICs while maintaining RDMA semantics. These updates include extensions to RDMA's packet format, such as new fields and packet types, encapsulated within IP and UDP headers to affect only endhost behavior.

#### Relevant Context
- **RDMA Operations:**
  - **Write:** Requester writes data to responder's memory.
  - **Read:** Requester reads data from responder's memory.
  - **Send:** Requester sends data to responder.
  - **Atomic:** Requester reads and atomically updates data at a location in responder's memory.

#### Supporting RDMA Reads and Atomics
IRN introduces read (N)ACKs for BDP-FC and loss recovery. RoCE NICs already support per-packet ACKs for Writes and Sends. For Reads, IRN adds read (N)ACK packets sent by the requester for each Read response packet. New timer-driven actions are added to the NIC hardware to handle timeouts.

#### Supporting Out-of-Order Packet Delivery
Current RoCE NICs discard out-of-order (OOO) packets. To support OOO delivery, we propose an implementation where the NIC DMAs OOO packets directly to the final address in application memory and tracks them using bitmaps. This reduces NIC memory requirements from 1KB per OOO packet to a couple of bits, addressing the challenge of limited NIC memory capacity.