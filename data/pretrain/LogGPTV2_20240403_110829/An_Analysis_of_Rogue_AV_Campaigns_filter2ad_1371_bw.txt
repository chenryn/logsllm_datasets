Systems Design and Implementation, NSDI 2010 (2010)
16. Rieck, K., Holz, T., Willems, C., Dussel, P., Laskov, P.: Learning and classiﬁcation of mal-
ware behavior. In: Zamboni, D. (ed.) DIMVA 2008. LNCS, vol. 5137, pp. 108–125. Springer,
Heidelberg (2008)
On Challenges in Evaluating Malware Clustering
255
17. Rieck, K., Trinius, P., Willems, C., Holz, T.: Automatic analysis of malware behavior using
machine learning. Technical Report 18-2009, Berlin Institute of Technology (2009)
18. Song, D., Brumley, D., Yin, H., Caballero, J., Jager, I., Kang, M.G., Liang, Z., Newsome, J.,
Poosankam, P., Saxena, P.: Bitblaze: A new approach to computer security via binary anal-
ysis. In: Proceedings of the 4th International Conference on Information Systems Security
(December 2008)
19. Symantec. Spyware.e2give,
http://www.symantec.com/security response/
writeup.jsp?docid=2004-102614-1006-99
20. Symantec. Xeram.1664,
http://www.symantec.com/security response/
writeup.jsp?docid=2000-121913-2839-99
21. Tamada, H., Okamoto, K., Nakamura, M., Monden, A., Matsumoto, K.: Dynamic software
birthmarks to detect the theft of windows applications. In: International Symposium on Fu-
ture Software Technology (2004)
22. Tan, P., Steinbach, M., Kumar, V.: Introduction to Data Mining. Addison-Wesley, Reading
(2006)
23. Wang, X., Jhi, Y., Zhu, S., Liu, P.: Detecting software theft via system call based birthmarks.
In: Proceedings of 25th Annual Computer Security Applications Conference (2009)
24. Whale, G.: Identiﬁcation of program similarity in large populations. Computer Journal, Spe-
cial Issue on Procedural Programming, 140–146 (1990)
25. Willems, C., Holz, T., Freiling, F.: Toward automated dynamic malware analysis using
cwsandbox. In: Proceedings of the 2007 IEEE Symposium on Security and Privacy (S&P
2007), pp. 32–39 (2007)
26. Wise, M.J.: Detection of similarities in student programs: Yaping may be preferable to
plagueing. In: Proceedings of the 23rd SIGCSE Technical Symposium (1992)
Why Did My Detector Do That?!
Predicting Keystroke-Dynamics Error Rates
Kevin Killourhy and Roy Maxion
Dependable Systems Laboratory
Computer Science Department
Carnegie Mellon University
5000 Forbes Ave,
Pittsburgh, PA 15213
{ksk,maxion}@cs.cmu.edu
Abstract. A major challenge in anomaly-detection studies lies in iden-
tifying the myriad factors that inﬂuence error rates. In keystroke dynam-
ics, where detectors distinguish the typing rhythms of genuine users and
impostors, inﬂuential factors may include the algorithm itself, amount
of training, choice of features, use of updating, impostor practice, and
typist-to-typist variation.
In this work, we consider two problems. (1) Which of these factors in-
ﬂuence keystroke-dynamics error rates and how? (2) What methodology
should we use to establish the eﬀects of multiple factors on detector error
rates? Our approach is simple: experimentation using a benchmark data
set, statistical analysis using linear mixed-eﬀects models, and validation
of the model’s predictions using new data.
The algorithm, amount of training, and use of updating were strongly
inﬂuential while, contrary to intuition, impostor practice and feature set
had minor eﬀect. Some typists were substantially easier to distinguish
than others. The validation was successful, giving unprecedented conﬁ-
dence in these results, and establishing the methodology as a powerful
tool for future anomaly-detection studies.
Keywords: anomaly detection; keystroke dynamics; experimental
methodology.
1 Introduction
Anomaly detectors have great potential for increasing computer security (e.g.,
detecting novel attacks and insider-type behavior [6]). Unfortunately, the error
rates of detection algorithms are sensitive to many factors including changes in
environmental conditions, detector conﬁguration, and the adversary’s behavior.
With so many factors that might aﬀect a detector’s error rates, how do we ﬁnd
those that do? For anomaly detectors to become a dependable computer-security
technology, we must be able to explain what factors inﬂuence their error rates
and how.
S. Jha, R. Sommer, and C. Kreibich (Eds.): RAID 2010, LNCS 6307, pp. 256–276, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010
Why Did My Detector Do That?!
257
Consider keystroke dynamics: an application of anomaly detection in which
the normal typing rhythms of a genuine user are distinguished from those of an
impostor. We might discover an impostor, even though he has compromised the
password of a genuine user, because he does not type it with the same rhythm.
Like all applications of anomaly detection, various factors might inﬂuence the
error rates of keystroke-dynamics detectors. We have identiﬁed six from the
literature:
1. Algorithm: The anomaly-detection algorithm itself is an obvious factor. Dif-
ferent algorithms will have diﬀerent error rates. However, it can be diﬃcult
to predict error rates from the algorithm alone. In earlier work, we bench-
marked 14 existing algorithms on a single data set [12]. The error rates for
each algorithm were diﬀerent from those reported when the algorithm was
ﬁrst proposed; other factors must inﬂuence the error rates.
2. Training amount: Some researchers have trained their detectors with as
few as 5 repetitions of a password, while others have used over 200. Re-
searchers have found that increasing the number of training repetitions even
from 5 to 10 can reduce error [10].
3. Feature set: A variety of timing features, including hold times, keydown-
keydown times, and keyup-keydown times have been tried. Diﬀerent re-
searchers use diﬀerent combinations of these features in their evaluation.
One study found that every combination had a diﬀerent error rate [1].
4. Updating: Most research has focused on detectors that build a genuine
user’s typing proﬁle during a training phase, after which the proﬁle is ﬁxed.
Recent research suggests that regularly updating the proﬁle may reduce error
because the proﬁle evolves with changing typing behavior [1,11].
5. Impostor practice: An impostor is an intelligent adversary and will pre-
sumably try to evade detection. Since the typing rhythms of a practiced pass-
word may be more similar to the genuine user’s rhythms, some researchers
have given some of their impostor subjects the opportunity to practice. Pre-
liminary results suggest that impostor practice may raise miss rates [1,13].
6. Typist-to-typist variation: Some genuine users may be easy to discrimi-
nate from impostors while others may be more diﬃcult. When researchers
report per-subject error rates, the results do suggest that a detector’s error
is higher for some subjects than others, but typist-to-typist variation has
never been explicitly quantiﬁed [5].
Any of these six factors might explain diﬀerent keystroke-dynamics error rates.
However, earlier work on the eﬀects of these factors is inconclusive. Usually,
only one factor at a time is tested, ignoring the possibility of interactions (e.g.,
that increased training aﬀects diﬀerent detectors diﬀerently). Evaluation results
are almost always presented with no statistical analysis. For instance, a detector’s
empirically measured false-alarm rate using one set of features may be 1.45%
while it is 1.63% with another set of features [1]. Without further analysis (e.g.,
a test of statistical signiﬁcance), we should not conclude that the ﬁrst feature
set is better than the second, yet such analysis is rarely conducted.
258
K. Killourhy and R. Maxion
In keystroke dynamics, as in other applications of anomaly detection, listing a
multitude of factors that might explain diﬀerent error rates is easy. The challenge
is establishing which factors actually do have an eﬀect.
2 Problem and Approach
In this work, two problems concern us. First, what inﬂuence do each of the
factors listed above—algorithm, training amount, feature set, updating, impostor
practice, and typist-to-typist variation—have on keystroke-dynamics error rates?
Second, what methodology should we use to establish the eﬀects of these various
factors?
We propose a methodology and demonstrate that it can identify and mea-
sure the eﬀects of these six factors. The details of the methodology, which are
described in the next three sections, can be summarized as follows:
1. Experiment: We design and conduct an experiment in which anomaly detec-
tors are repeatedly evaluated on a benchmark data set. Between evaluations,
the six factors of interest are systematically varied, and the eﬀect on the eval-
uation results (i.e., the error rates of the detectors) is observed. (Section 3)
2. Statistical analysis: The experimental results are incorporated into a sta-
tistical model that describes the six factors’ inﬂuence. In particular, we use
a linear mixed-eﬀects model to estimate the eﬀect of each factor along with
any interactions between the factors. Roughly, the mixed-eﬀects model al-
lows us to express both the eﬀects of some factors we can control, such as
the algorithm, and also the eﬀects of some factors we cannot, such as the
typist. (Section 4)
3. Validation: We collect a new data set, comprised of 15 new typists, and we
validate the statistical model. We demonstrate that the model predicts eval-
uation results using the new data, giving us high conﬁdence in its predictive
power. (Section 5)
With such a model, we can predict what diﬀerent environmental changes, recon-
ﬁgurations, or adversarial behavior will do to a detector. We can make better
choices when designing and conducting future evaluations, and practitioners can
make more informed decisions when selecting and conﬁguring detectors.
Fundamentally, the proposed methodology—experimentation, statistical anal-
ysis, and validation—enumerates several steps of the classical scientiﬁc method.
Others have advocated that computer-security research would beneﬁt from a
stricter application of this method [15]. The current work explores the speciﬁc
beneﬁt for anomaly-detection research.
3 Experiment
The experiment is designed so that we can observe the eﬀects of each of the
six factors on detector error rates. In this section, we lay out the experimental
method, and we present the empirical results.
Why Did My Detector Do That?!
259
3.1 Experimental method
The method itself is comprised of three steps: (1) obtain a benchmark data set,
(2) select values of interest for each of the six factors, and (3) repeatedly run an
evaluation, while systematically varying the factors among the selected values.
Data. For our evaluation data, we used an extant data set wherein 51 subjects
typed a 10-character password (.tie5Roanl). Each subject typed 400 repetitions
of the password over 8 sessions of 50 repetitions each (spread over diﬀerent days).
For each repetition of the password, 31 timing features were extracted: 11 hold
times (including the hold for the Return key at the end of the password), 10
keydown-keydown times (one for each digram), and 10 keyup-keydown times (also
for each digram). As a result, each password has been converted to a 31-dimensional
password-timing vector. The data are a sensible benchmark since they are publicly
available and the collection methods have been laid out in detail [12].
Selecting Factor Values. The six factors of interest in this study—algorithm,
training amount, feature set, updating, impostor practice, and typist-to-typist
variation—can take many diﬀerent values (e.g., amount of training can range
from 1 repetition to over 200). For this study, we need to choose a subset of
values to test.
1. Algorithms: We selected three detectors for our current evaluation. The
Manhattan (scaled) detector, ﬁrst described by Ara´ujo et al. [1], calculates
the “city block” distance between a test vector and the mean of the training
vectors. The Outlier-count (z-score) detector, proposed by Haider et al. [8],
counts the number of features in the test vector which deviate signiﬁcantly
from the mean of the training vectors. The Nearest Neighbor (Mahalanobis)
detector, described by Cho et al. [5], ﬁnds the distance between the test vec-
tor and the nearest vector in the training data (using a measure called the
Mahalanobis distance). We focus on these three for pragmatic reasons. All
three were top performers in an earlier evaluation [12]; their error rates were
indistinguishable according to a statistical test. Finding factors that diﬀer-
entiate these detectors will have practical signiﬁcance, establishing when one
outperforms the others.
2. Amount of training: We train with 5 repetitions, 50 repetitions, 100 repe-
titions, and 200 repetitions. Prior researchers have trained anomaly detectors
with varying amounts of data, spanning the range of 5–200 repetitions. Our
values were chosen to broadly map the contours of this range.
3. Feature sets: We test with three diﬀerent sets of feature: (1) the full set
of 31 hold, keydown-keydown, and keyup-keydown times for all keys includ-
ing the Return key; (2) the set of 19 hold and keydown-keydown times for
all keys except the Return key; (3) the distinct set of 19 hold and keyup-
keydown times for all keys except the Return key. Prior work has shown that
combining hold times with either keydown-keydown times or keyup-keydown
times improves accuracy. The three feature sets we test should remove re-
maining ambiguity about whether the particular combination matters, and
whether the Return-key timing features should be included.
260
K. Killourhy and R. Maxion
4. Updating: We test with and without updating. Speciﬁcally, we compare
detectors given a ﬁxed set of training data to those which are retrained after
every few repetitions. We call these two modes of updating None and Sliding
Window. Two levels are all that are necessary to establish whether updating
has an eﬀect.
5. Impostor practice: We test using two levels of impostor practice: None and
Very High. With no practice, impostor data are comprised of the ﬁrst ﬁve
password-timing vectors from the impostors. With very high practice, the
last ﬁve vectors are used, by which point the impostor has had 395 practice
repetitions. To explore whether practice has a detrimental eﬀect, only these
two extremes are necessary.
6. Typist-to-typist variation: By designating each subject as the genuine
user in separate evaluation runs, we can observe how much variation in
detector performance arises because some subjects are easier to distinguish
than others. Since there are 51 subjects in the benchmark data set, we have
51 instances with which to observe typist-to-typist variation.
These selected values enable us to identify which factors are inﬂuential and to
quantify their eﬀect.
Evaluation procedure. Having chosen values for these six factors, we need
an evaluation procedure that can be run for all the diﬀerent combinations. The
designed procedure has seven inputs: the data set (D); the algorithm (A); the
number of training repetitions (T ); the feature set (F ); the updating strategy
(U); the level of impostor practice (I); and the genuine-user subject (S).
For clarity, we ﬁrst describe the evaluation procedure for the no-updating case
(i.e., U is set to None):
1. Unnecessary features are removed from the data set. Based on the setting
of F , keydown-keydown, keyup-keydown, and/or Return-key features are
dropped.
2. The detector is trained on the training data for the genuine user. Speciﬁcally,
repetitions 1 through T for subject S are extracted and used to train the
detection algorithm A.
3. Anomaly scores are calculated for the genuine-user test data. Speciﬁcally,
repetitions (T + 1) through (T + 200) for subject S are extracted (i.e., the
next 200 repetitions). The trained detector processes each repetition and cal-
culates an anomaly score. These 200 anomaly scores are designated genuine-
user scores.
4. Anomaly scores are calculated for the impostor test data. If I is set to None
(unpracticed impostors), repetitions 1 through 5 are extracted from every
impostor subject (i.e. all those in the data set except S). If I is set to
Very High (practiced impostors), repetitions 396 through 400 are extracted
instead. The trained detector processes each repetition and calculates an
anomaly score. If there are 50 impostor subjects, this step produces 250
(50 × 5) anomaly scores. These scores are designated impostor scores.
Why Did My Detector Do That?!
261
5. The genuine-user and impostor scores are used to generate an ROC curve
for the detector [19]. From the ROC curve, the equal-error rate is calculated
(i.e., the false-alarm and/or miss rate when the detector has been tuned so
that both are equal). It is a common overall measure of detector performance
in keystroke-dynamics research [14].
The evaluation procedure for the sliding-window-updating case is more compli-
cated (i.e., when U is set to Sliding Window). The intuition is that we slide a
window of size T over the genuine user’s typing data (advancing the window
in increments of ﬁve repetitions for computational eﬃciency). For each window,
the detector is trained on the repetitions in that window and then tested using
the next ﬁve repetitions. We increment the window and repeat. In total, since
there are 200 repetitions of genuine-user test data (see Step 3 above), we iterate
through 40 such cycles of training and testing (200/5). In our actual evaluation,
each of these 40 training-testing cycles is handled in parallel using 40 separate
copies of the detector. Each copy is put through its own version of steps 2, 3,
and 4 of the evaluation procedure:
2(cid:4). Forty diﬀerent sets of training data are used to train 40 diﬀerent copies
of the detection algorithm A. The ﬁrst set of training data is comprised
of repetitions 1 through T for subject S; the second set is comprised of
repetitions 6 through (T + 5); the 40th set is comprised of repetitions 196
through (T + 195). For each of the 40 sets, a separate copy of the detector
is trained.
3(cid:4). Anomaly scores are calculated for each of 40 diﬀerent sets of genuine-user
test data. Each set corresponds to the genuine-user test data for one of the
trained detectors. In particular, the ﬁrst set includes repetitions (T + 1)
through (T + 5); the second set includes (T + 6) through (T + 10); the 40th
set includes (T + 196) through (T + 200). The ﬁrst trained detector scores
the 5 repetitions in the ﬁrst set; the second trained detector scores the 5
repetitions in the second set, and so on. The scores from every detector are
pooled together. Since each set contains 5 repetitions and there are 40 sets,
there are 200 (5 × 40) genuine-user scores in total.
4(cid:4). Anomaly scores are calculated for the impostor test data by every one of the
40 diﬀerent trained detectors. Speciﬁcally, the impostor test data are selected
according to the setting of I (i.e., either the ﬁrst 5 or the last 5 repetitions
from every subject except S). Each of the 40 trained detectors scores the
repetitions in the impostor test data. If there are 50 impostor subjects and
5 repetitions per subject, this step produces 10,000 (50 × 5 × 40) anomaly
scores. All of these scores are pooled into a single set of impostor scores.
As in the case of no updating, the genuine-user and impostor scores are used
to generate a single ROC curve and calculate a single equal-error rate for the
sliding-window evaluation.
A few decisions in the design of the sliding-window procedure are worth high-
lighting. In Step 2(cid:4), we make the simplifying assumption that the detector will
only retrain on the genuine user’s data (i.e., impostor poisoning of the training
K. Killourhy and R. Maxion
262
is not considered). In Step 4(cid:4), we score each repetition of impostor test data mul-
tiple times, once with each trained detector. An impostor’s anomaly scores will
change whenever the detector is retrained. By scoring at each detector window
and pooling, we eﬀectively aggregate over these variations and ﬁnd the average.
We ran this evaluation procedure 7,344 times (3 × 4 × 3 × 2 × 2 × 51), once
for each combination of algorithm, amount of training, feature set, updating,
impostor practice, and subject in the data set. We recorded the equal-error rate
from each evaluation. By looking at all the combinations of the six factors, we
will be able to ﬁnd interactions between factors, not just the eﬀect of each factor
individually.
3.2 Results
To visually explore the 7,344 equal-error rates that comprise the raw results of
our experiment, we calculated the average equal-error rate across all subjects
for each combination of the other ﬁve factors. These averages are presented
across the 12 panels in Figure 1. Each panel contains three curves, depicting how
the error rates of each of the three detectors changes with increased amounts
of training. The strips above each panel explain what combination of factors
produced the results in the panel. Updating is either None or Sliding Window;
Feature Set is one of the combinations of hold times (H), keydown-keydown times
(DD), keyup-keydown times (UD), and Return-key features (Ret); Impostor
Practice is either None or Very High.
Looking within any panel, we see that the error rates for all three detectors
decrease as training increases. In particular, the Nearest Neighbor (Mahalanobis)
error is much higher with only 5 repetitions, but improves and is competitive
with the others with 100–200 repetitions. With few training repetitions, a practi-
tioner might want to use either the Manhattan (scaled) or Outlier-score (z-count)
detector.
If we look beyond the individual panels to the four quadrants, the three panels
in a quadrant correspond to the use of the three diﬀerent feature sets. The curves
in the three panels in each quadrant look nearly identical. It would appear that,
so long as hold times and one of keydown-keydown or keyup-keydown times are
used, the particular combination does not matter.
The six panels on the left correspond to unpracticed-impostor error rates, and
the six on the right correspond to very-practiced-impostor error rates. The curves
in the right-hand panels are slightly higher. Consequently, impostor practice may
represent a minor threat to the accuracy of keystroke-dynamics detectors.
Finally, the six panels on the top correspond to non-updating detectors and