[21] A. Farina, “Simultaneous measurement of impulse response and
distortion with a swept-sine technique,” in AES Convention, 2000.
[22] Y. Ganin and V. Lempitsky, “Unsupervised domain adaptation by
backpropagation,” in Proceedings of ICML, 2015.
[29] W. Jiang, C. Miao, F. Ma, S. Yao, Y. Wang, Y. Yuan, H. Xue,
C. Song, X. Ma, D. Koutsonikolas et al., “Towards environment
independent device free human activity recognition,” in Proceedings
of ACM MobiCom, 2018.
e. Jonathan Shen, “Lingvo: a modular and scalable framework for
sequence-to-sequence modeling,” https://arxiv.org/abs/1902.08295,
2019.
[30]
[31] D. P. Kingma and J. Ba, “Adam: A method for stochastic
optimization,” arXiv preprint arXiv:1412.6980, 2014.
[32] K. Kinoshita, M. Delcroix, T. Yoshioka, T. Nakatani, A. Sehr,
W. Kellermann, and R. Maas, “The reverb challenge: A common
evaluation framework for dereverberation and recognition of
reverberant speech,” in Proceedings of IEEE WASPAA, 2013.
[33] F. Kreuk, Y. Adi, M. Cisse, and J. Keshet, “Fooling end-to-end
speaker veriﬁcation with adversarial examples,”
https://arxiv.org/pdf/1801.03339, 2018.
[34] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples in
the physical world,” in ICLR Workshop, 2017.
[35] H. Lee, T. H. Kim, J. W. Choi, and S. Choi, “Chirp signal-based
aerial acoustic communication for smart devices,” in Proceedings of
IEEE INFOCOM, 2015.
[36] X. Liu, K. Wan, and Y. Ding, “Adversarial attack on speech-to-text
recognition models,” arXiv preprint arXiv:1901.10300, 2019.
[37] S. Nakamura, K. Hiyane, F. Asano, T. Nishiura, and T. Yamada,
“Acoustical sound database in real environments for sound scene
understanding and hands-free speech recognition,” in Proceedings of
LREC, 2000.
[38] R. Nandakumar, K. K. Chintalapudi, V. Padmanabhan, and
R. Venkatesan, “Dhwani: secure peer-to-peer acoustic nfc,” in
Proceedings of ACM SIGCOMM, 2013.
[39] T. Y. K. A. Naoto Inoue, Ryosuke Furuta, “Cross-domain
weakly-supervised object detection through progressive domain
adaptation,” https://arxiv.org/abs/1803.11365, 2018.
[40] A. S. Nittala, X.-D. Yang, S. Bateman, E. Sharlin, and S. Greenberg,
“Phoneear: interactions for mobile devices that hear high-frequency
sound-encoded data,” in Proceedings of ACM SIGCHI, 2015.
[41] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel,
M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al., “The kaldi
speech recognition toolkit,” in Proceedings of IEEE ASRU, 2011.
[42] Y. Qin, N. Carlini, I. Goodfellow, G. Cottrell, and C. Raffel,
“Imperceptible, robust, and targeted adversarial examples for
automatic speech recognition,” arXiv preprint arXiv:1903.10346, 2019.
[43] B. C. N. V. Rohan Taori, Amog Kamsetty, “Targeted adversarial
examples for black box audio systems,”
https://arxiv.org/pdf/1805.07820, 2018.
[44] N. Roy, H. Hassanieh, and R. Roy Choudhury, “Backdoor: Making
microphones hear inaudible sounds,” in Proceedings of ACM MobiSys,
2017.
[45] N. Roy, S. Shen, H. Hassanieh, and R. R. Choudhury, “Inaudible
voice commands: The long-range attack and defense,” in Proceedings
of USENIX NSDI, 2018.
[46] L. Schönherr, K. Kohls, S. Zeiler, T. Holz, and D. Kolossa,
“Adversarial attacks against automatic speech recognition systems via
psychoacoustic hiding,” arXiv preprint arXiv:1808.05665, 2018.
14
[47] S. M. Shreya Khare, Rahul Aralikatte, “Adversarial black-box attacks
for automatic speech recognition systems using multi-objective
genetic optimization,” https://arxiv.org/abs/1811.01312, 2018.
“Apple Siri,” https://www.apple.com/siri/.
[48]
[49] R. Taori, A. Kamsetty, B. Chu, and N. Vemuri, “Targeted adversarial
examples for black box audio systems,” arXiv preprint
arXiv:1805.07820, 2018.
[50] T. Vaidya, Y. Zhang, M. Sherr, and C. Shields, “Cocaine noodles:
exploiting the gap between human and machine speech recognition,”
in Proceedings of USENIX WOOT, 2015.
[51] K. Vesel`y, A. Ghoshal, L. Burget, and D. Povey,
“Sequence-discriminative training of deep neural networks.” in
Proceedings of Interspeech, 2013.
[52] Q. Wang, K. Ren, M. Zhou, T. Lei, D. Koutsonikolas, and L. Su,
[53]
“Messages behind the sound: real-time hidden acoustic signal capture
with smartphones,” in Proceedings of ACM MobiCom, 2016.
J. Y. Wen, N. D. Gaubitch, E. A. Habets, T. Myatt, and P. A. Naylor,
“Evaluation of speech dereverberation algorithms using the mardy
database,” in Proceedings of IWAENC, 2006.
[54] P. Xie, J. Feng, Z. Cao, and J. Wang, “Genewave: Fast authentication
and key agreement on commodity mobile devices,” in Proceedings of
IEEE ICNP, 2017.
[55] Y. Xie, Z. Li, and M. Li, “Precise power delay proﬁling with
commodity wiﬁ,” in Proceedings of ACM MobiCom, 2015.
[56] H. Yakura and J. Sakuma, “Robust audio adversarial example for a
physical attack,” arXiv preprint arXiv:1810.11793, 2018.
[57] X. Yuan, Y. Chen, Y. Zhao, Y. Long, X. Liu, K. Chen, S. Zhang,
H. Huang, X. Wang, and C. A. Gunter, “Commandersong: A
systematic approach for practical adversarial voice recognition,” arXiv
preprint arXiv:1801.08535, 2018.
[58] B. Zhang, Q. Zhan, S. Chen, M. Li, K. Ren, C. Wang, and D. Ma,
“Priwhisper: Enabling keyless secure acoustic communication for
smartphones,” IEEE internet of things journal, 2014.
[59] G. Zhang, C. Yan, X. Ji, T. Zhang, T. Zhang, and W. Xu,
“Dolphinattack: Inaudible voice commands,” in Proceedings of ACM
CCS, 2017.
[60] M. Zhao, S. Yue, D. Katabi, T. S. Jaakkola, and M. T. Bianchi,
“Learning sleep stages from radio signals: A conditional adversarial
architecture,” in Proceedings of ICML, 2017.
[61] Y. Zou, Z. Yu, B. Vijaya Kumar, and J. Wang, “Unsupervised domain
adaptation for semantic segmentation via class-balanced self-training,”
in Proceedings of ECCV, 2018.
15
A. Conﬁguration of System Parameters
APPENDIX
The ﬁnal loss function of Metamorph in Eqn. (10) includes
ﬁve parameters, including α, β , γ, η and µ. In this subsection,
we introduce how they are conﬁgured in this paper.
Parameter α. This parameter is based on the audio adversarial
example generation method proposed in [17], which aims to
balance the audio sound distortion, described by Decibels (dB),
and the attack successful rate, described by the Connectionist
Temporal Classiﬁcation (CTC) loss. Although it is possible to
train it directly, a more efﬁcient mechanism is implemented in
[2] to avoid a direct parameter tuning. In our implementation,
we also adopt this mechanism without turning α directly.
Parameters β and γ. These two parameters are introduced
in Metamorph to ensure the good attack performance after the
over-the-air transmission of the adversarial example. Parameter
β balances the adversarial example generation and the ability
to distinguish domains by the domain discriminator. We vary6
β from 0.005 to 0.5 in Figure 22(a). From the results, we
observe that both the transcript successful rate (TSR) and
character successful rate (CSR) at a moderate attack distance
of 3 m can achieve a better performance (e.g., > 0.95) when
β is 0.05. The audio quality, measured by MCD (Mel Cepstral
Distortion), keeps relative stable in this experiment. We thus
experimentally adopt 0.05 as the default β setting in current
Metamorph. With this setting, more experiments from the
evaluation section show the good system performance at other
attack distances as well. On the other hand, parameter γ is
introduced to reduce the over-ﬁtting. Through the experiment
in Figure 22(b), we observe that when we increase γ, e.g., 500
or 1000, TSR approaches to nearly 100%. The audio quality
degrades only slightly. However, when we further increase γ,
both TSR/CSR and audio quality drop rapidly. Therefore, we
adopt 500 as the default γ setting in the current Metamorph.
Parameters η and µ. These two parameters are introduced
in Metamorph to mainly improve the audio quality of the
generated adversarial example. Parameter η controls the utility
of the audio grafﬁti. In Figure 22(c), we vary η from 1e-5
to 1e-3. The result shows that when η increases, the audio
quality, measured by MCD, keeps improving, while CSR and
TSR drop signiﬁcantly when η is greater than 1e-4. Hence,
we adopt 1e-4 as the default η setting in current Metamorph.
On the other hand, parameter µ is introduced to reduce the
perturbation coverage. Through the experiment in Figure 22(d),
we observe that the increase of µ also leads to the improvement
of the audio quality MCD, while the CSR and TSR will drop
concurrently. As a result, we adopt 1e-12 as the default µ
setting in the current Metamorph.
These default parameters introduced above are utilized in
the experimental evaluations in Section IV.
B. User Perceptibility Study Questions
In the ﬁrst trial of the experiments conducted in the user
perceptibility study of Section IV-B, volunteers will sequen-
tially listen to each set of audios following the organization
6Principle of each parameter’s varying range is to ensure its product with
its loss function will be comparable to other terms in Eqn. (10).
Figure 22: Experimental conﬁgurations for system parameters
β , γ, η and µ (with a reversed y-axis representation for MCD).
below: “[(one original audio, the adversarial example generated
from this audio by Meta-Enha), 60s pause, (the same original
audio, the adversarial example generated from this audio by
Meta-Qual), 60s pause]”. During each pause, the volunteers
immediately assess the audio quality of each adversarial ex-
ample compared with the original audio. Volunteers ﬁrst select
whether each adversarial example has the same audio quality
as the original audio (Y or N), including both the noise level
and the audio content. If the answer is Y, the assessment of this
adversarial example in the ﬁrst trial is complete; Otherwise,
volunteers will further select for three questions related to 1)
word (content) change, 2) audio quality level and 3) noise
description. The explanation of each question is in Table 4.
Explanations
Y Word (content) change per-
Word change
Quality level
Description
ceived.
No word (content) change per-
ceived.
Noise is small and audio con-
tent is clear.
Noise is a slightly loud, but it
does not impact my hearing of
the audio content.
Noise is loud and I cannot hear
the audio content occasionally.
Noise is annoying and I cannot
hear the audio content consis-
tently.
Noise is brought by the hard-
ware, e.g., microphone record-
ing, cheap speaker, etc.
Noise is due to the low-quality
of the audio clip itself.
Others (using your own words).
N
1
2
3
4
A
B
C
TABLE 4: Explanations of word change, audio quality level
and description three ﬁelds in the user perceptibility study.
C. Adversarial Examples Used in Evaluation
We generate two types of adversarial examples (music and
speech) with different source and target transcripts, which are
detailed in Table 5. The source musics are labelled in the table
directly, and the speech adversarial examples are generated
based on 11 different speech samples from the public Mozilla
Common Voice Dataset [6].
16
No.
1
2
3
4
5
6
7
8
9
No.
1
2
3
4
5
6
7
8
9
10
11
Source audio transcripts (musics)
“[no transcript]”–Bach, Violin
“chase your dreams and remember me sweet
bravery”–Owl City, To The Sky
“I feel earth move under my feet I feel the sky”–
Carole King, I Feel The Earth Move
“lyrical acrobat stunts while I’m practicing that I’ll
still be able to break a motherfuckin’table over the
back of a couple"–Eminem, Rap God
“well the kid is into losin’ sleep and he don’t
come home for half the week”–Van Halen, And
the Cradle Will Rock
“[no transcription]”–Van Halen, Guitar
“somebody mix my medicine”–The pretty Reck-
less, My Medicine
“[no transcription]”–Chopin, Piano
“I am a mountaineer in the”–Owl City, Hello
Seattle
Source audio transcripts (speeches)
“hold your nose to keep the smell from disabling
your motor functions”
“your son went to server at a distant place and
became a centurion”
“the shower’s in there”
“and you know it”
“this is no place for you”
“if I had told you you wouldn’t have seem the
pyramids”
“I told you to have the ice box ﬁxed”
“their faces were hidden behind blue veils with
only their eyes showing”
“we are refugees from the tribal wars and we need
money the other ﬁgure said”
“isn’t the party also to announce his engagement
to joanna”
“he stood irresolute for a moment and then scram-
bled out of the pit”
Target commands
“hello world”
“power off”
“pay the money”
“turn off the light”
“airplane mode on”
“browse to evil dot com”
“turn off the cellular network”
“update the phone blacklist”
“silence the phone”
Target commands
“clear all appointments on calendar”
“open the door”
“restart”
“open the camera”
“ﬂashlight on”
“play the scary music”
“call nine one one”
“send me your messages”
“log in paypal”
“show fake trafﬁc information”
“shut down the power source”
TABLE 5: Source audios and target transcripts used in Metamorph, where “[no transcription]” means that there is no transcript
when the classical music is played. The source musics are labelled in the table and the source audio for speeches are from the
Mozilla Common Voice Dataset.
17