49.5
44.4
robust
19.7
12.5
3.1
11.2
clean
robust
clean
robust
84.5
84.7
65.8
84.2
83.9
63.8
69.2
51.9
44.2
68.9
83.9
84.6
47.8
84.2
83.9
47.3
57.7
30.8
9.3
56.2
5.2 Provable Robustness Results
Table 5: Clean accuracy of ResNet and BagNet for different datasets
In this subsection, we present provable robustness results for
our defense (computed with Algorithm 2 and Theorem 1);
the results hold for any attack within the corresponding patch
size constrain. We also compare PatchGuard with previous
provably robust defenses [9, 28, 59].
PatchGuard achieves high provable robustness across dif-
ferent models and datasets. We report the provable robust
accuracy of PatchGuard across different models, patch sizes,
and datasets in Table 4. First, both Mask-BN and Mask-DS
achieve high provable robustness. For example, against a 1%
pixel patch on the 10-class ImageNette dataset, Mask-BN has
a provable robust accuracy of 89.0% while Mask-DS has that
of 83.1%. This implies that for 89.0% and 83.1% of the im-
ages from the respective test sets, no attack using a 1% pixel
patch can succeed. Second, PatchGuard has high provable
robustness across different datasets. Even for the extremely
challenging 1000-class ImageNet dataset, Mask-BN achieves
a non-trivial provable robust accuracy of 32.3% for the 1%
pixel patch. The provable robust accuracy increases to 54.8%
if we consider the top-5 classiﬁcation task (more details for
the top-k analysis are in Appendix D).
PatchGuard also maintains high clean accuracy. As
shown in Table 4, PatchGuard retains high clean accuracy.
For a 1% pixel patch, Mask-BN has a 95.2% clean accuracy
on ImageNette and 55.1% on ImageNet. Mask-DS also has
a 92.3% clean accuracy on ImageNette and 44.1% on Ima-
geNet. For a 2.4% pixel patch on CIFAR-10, Mask-BN and
Mask-DS have a high clean accuracy of 83.9% and 84.6%, re-
spectively. In Table 5, we report the clean accuracy of ResNet
and BagNet. We can see that the clean accuracy drop of Mask-
BN and Mask-DS on ImageNette compared with undefended
ResNet is within 7.5%. The accuracy drop of Mask-BN from
the undefended BagNet is within 1%.5
We note that we use the optimal mask window sizes for
different estimated upper bounds of patch sizes, and there-
fore the clean accuracy for different patches varies slightly
in Table 4. We will show a similarly high performance of
our defense when using an over-conservatively large mask
window size in Section 5.3.2.
5BagNet alone does not have any provable robustness but acts as a build-
ing block for the provable defense of PatchGuard.
Dataset
ResNet
BagNet
ImageNette
ImageNet
CIFAR-10
99.6%
95.9%
76.1%
56.5%
97.0%
85.4%
PatchGuard achieves higher provable robust accuracy
than all previous defenses. We compare our defense per-
formance with existing defenses across three datasets.
Comparison with IBP [9]. IBP is too computationally ex-
pensive and does not scale to high-resolution images like
ImageNette and ImageNet. We thus only compare its perfor-
mance with PatchGuard on CIFAR-10. As shown in Table 4,
both Mask-BN and Mask-DS signiﬁcantly outperform IBP in
terms of provable robust accuracy and clean accuracy.
Comparison with CBN [59]. Table 4 shows that both Mask-
BN and Mask-DS have higher provable robust accuracy than
CBN across three datasets. The clean accuracy of Mask-BN is
higher or comparable with that of CBN, but its provable robust
accuracy is much higher. For example, against a 3% pixel
patch on ImageNette, Mask-BN (94.8%) has a similar clean
accuracy as CBN (94.9%), but its provable robust accuracy is
37.1% higher!
Comparison with DS [28]. Both Mask-BN and Mask-
DS have better defense performance than DS on the high-
resolution ImageNette and ImageNet datasets. For example,
against a 1% pixel patch on ImageNet, Mask-BN has a 10.7%
higher clean accuracy and a 14.6% higher provable robust
accuracy compared with DS. On CIFAR-10, Mask-DS out-
performs DS in terms of clean accuracy and provable robust
accuracy thanks to the robust masking defense.
Takeaways. Our evaluation shows the effectiveness of our
proposed defenses, achieving state-of-the-art provable robust-
ness on all three datasets. We ﬁnd that BagNet-based defenses
(Mask-BN and CBN) perform well on ImageNette and Im-
ageNet but are fragile on CIFAR-10 due to the low image
resolution. Meanwhile, De-randomized Smoothing based de-
fenses (Mask-DS and DS) perform better on CIFAR-10. This
shows that while the robust masking defense always improves
robustness, the choice of which model to use (Mask-BN or
Mask-DS) depends on the dataset.
USENIX Association
30th USENIX Security Symposium    2245
Table 6: Effect of logits clipping values on vanilla models
Table 8: Effect of receptive ﬁeld sizes on provable robust accuracy
(cl ,ch)
ResNet-50
BagNet-33
BagNet-17
BagNet-9
(−∞,∞)
99.6%
97.2%
95.9%
92.5%
(0,∞)
99.5%
97.1%
95.5%
92.5%
(0,50)
(0,15)
99.5%
97.0%
94.7%
91.4%
99.5%
95.8%
92.3%
85.4%
(0,5)
99.0%
94.1%
87.9%
73.8%
Patch size
Accuracy
Mask-BN-33
Mask-BN-17
Mask-BN-9
1% pixels
2% pixels
3% pixels
clean
96.5%
95.2%
92.1%
robust
88.9%
89.0%
85.5%
clean
96.3%
95.0%
91.8%
robust
86.0%
86.7%
82.8%
clean
96.3%
94.8%
91.5%
robust
82.1%
83.0%
79.8%
Table 7: Invariance of BagNet-17 predictions to feature masking
8×8
95.7%
8.5%
0.7%
0×0
95.9%
4.1%
0%
2×2
95.9%
5.1%
0.05%
4×4
95.9%
6.1%
0.2%
6×6
95.8%
7.3%
0.4%
Window size
Masked accuracy
% images
% windows per image
5.3 Detailed Analysis of PatchGuard
In this subsection, we analyze the behavior of vanilla (unde-
fended) models, PatchGuard with different parameters, and
defense efﬁciency on the ImageNette dataset. We will only
report results for Mask-BN when the observations from Mask-
BN and Mask-DS are very similar. A similar analysis for
CIFAR-10 is available in our technical report [55].
5.3.1 Analysis of Vanilla models
Recall that PatchGuard’s robust prediction relies on clipping
feature values as well as robust masking. Here, we show that
vanilla models only have a small performance loss due to
clipping and feature masking, which explains the high clean
accuracy retained by PatchGuard.
Clipping has a small impact on vanilla models. In this anal-
ysis, we vary the clipping value for the local logits for ResNet
and BagNet to determine how the clean accuracy changes,
and the results are shown in Table 6. We ﬁnd that clipping
the negative values only slightly affects the clean accuracy
(cl = 0,ch = ∞ is our default setting). When we decrease the
positive clipping value ch, the clean accuracy of the model
also decreases. We notice that models with smaller receptive
ﬁelds are more sensitive to clipping. This is because models
with small receptive ﬁelds only have a small number of cor-
rect local predictions. The corresponding correctly predicted
local logits have to use large logits values to dominate the
global prediction, which leads to the sensitivity to clipping.
As shown in Figure 3, the logits of the adversarial images tend
to have large values. If we set ch to the largest clean logits
value, we will not affect the clean accuracy and can improve
the empirical robustness against the adversarial patch.
Vanilla models are generally prediction-invariant to fea-
ture masking. In our robust masking defense, we detect and
mask corrupted features. If the model can make correct pre-
dictions from the aggregation of the remaining features, we
can recover the correct prediction. We use BagNet-17, which
has 26· 26 local features, to analyze the prediction invariance
of vanilla models to partial feature masking. We mask out all
class evidence within a set of sliding windows of different
Table 9: Effect of detection thresholds on Mask-BN-17
Clean accuracy
Provable accuracy
Detection FP
T-0.0
T-0.2
T-0.4
T-0.6
T-0.8
T-1.0
95.0%
94.2%
95.3%
95.5%
95.5%
95.5%
86.7%
79.9%
68.0%
38.7%
6.2%
0%
100%
22.9%
0.7%
0.05%
0%
0%
sizes and record the prediction from the remaining features.
We report the average accuracy over all possible masked fea-
ture tensors (masked accuracy), the percentage of images for
which at least one masked prediction is incorrect (% images),
and the averaged percentage of masks that will cause pre-
diction change for each image (% windows per image).6 As
shown in Table 7, the overall average masked accuracy is
high, and the percentage of images and windows for which
the prediction changes is low. Such a small fraction of images
with prediction changes enables us to achieve high provable
robustness and maintain clean accuracy.
5.3.2 PatchGuard with Different Parameters
The receptive ﬁeld size balances the trade-off between
clean accuracy and provable robust accuracy of defended
models. We report clean accuracy and provable robust accu-
racy of our defense with BagNet-33, BagNet-17, and BagNet-
9, which have a receptive ﬁeld of 33×33, 17×17, and 9×9,
respectively, against different patch sizes in Table 8. As shown
in the table, a model with a larger receptive ﬁeld has better
clean accuracy. However, a larger receptive ﬁeld results in
a larger fraction of corrupted features and thus a larger gap
between clean accuracy and provable robust accuracy. We can
see that though Mask-BN-33 has a higher clean accuracy than
Mask-BN-17, its gap between clean accuracy and provable
robust accuracy is larger, which results in a similar or slightly
poorer provable robust accuracy compared with Mask-BN-17.
The trade-off between the clean accuracy and the robustness
can be tuned with different receptive ﬁeld sizes and should be
carefully balanced when deploying the defense.
A large detection threshold improves clean accuracy but
decreases provable robust accuracy of defended models.