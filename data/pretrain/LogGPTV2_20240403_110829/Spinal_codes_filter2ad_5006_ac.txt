ing spine value in a subpass, the associated branch costs are treated
as 0, and the children are computed as before (all the children of
a given parent will have the same score). If the correct candidate
falls out of the beam, decoding will indeed fail in this subpass. If
B is large enough, the correct candidate may remain in the beam
until the next non-omitted spine value arrives. In our experiments,
we ﬁnd that B = 256 exhibits the positive beneﬁts of puncturing;
as computing becomes cheaper, increasing B further will cause the
beneﬁts of puncturing to be even more pronounced.
6. LINK-LAYER FRAMING
To use spinal codes, two changes to the traditional (e.g., 802.11)
link-layer protocol are useful. First, because the code is rateless,
the encoder and decoder must maintain some state across distinct
frame transmissions, and use the cumulative knowledge of trans-
mitted symbols to decode a message. The amount of data kept is
small (on the order of a few kilobytes), similar to H-ARQ receivers
implementing incremental redundancy (e.g., 3G, LTE). To prevent
an erased frame transmission (e.g., the receiver fails to lock on to the
preamble) from de-synchronizing the receiver (which needs to know
which spine values are being sent in the frame), the sender should
use a short sequence number protected with a highly redundant code
(cf. the PLCP header in 802.11).
Second, it is useful to divide a single link-layer frame into multiple
code blocks, each encoded separately. This use of code blocks is
unusual, but not unique (cf. 802.11n with LDPC). The reason we
use it is that, for a ﬁxed compute budget at the decoder, shorter
coded messages come closer to the Shannon capacity (§8). Each
code block has a maximum length, n (1024 bits in our experiments).
At the link layer, the sender takes a datagram from the network
layer and divides it into one or more code blocks of size not exceed-
ing n bits. It computes and inserts a 16-bit CRC at the end of each
block to construct a link-layer frame. This frame is handed to the
encoder, which encodes each code block independently to produce
symbols.
The sender transmits a certain number of symbols, and then pauses
for feedback from the receiver. An important concern for any rate-
less code over half-duplex radios is that the receiver cannot send
feedback when the sender is still transmitting, and the sender may
not know when to pause for feedback. To achieve high throughput,
a good algorithm is required for determining pause points. We have
addressed this problem in more recent work [16].
At the receiver, the decoder processes the received symbols for
each code block. If any block gets decoded successfully (the CRC
passes), the next link-layer ACK indicates that fact. The ACK timing
is similar to 802.11, but the ACK contains one bit per code block.
7.
IMPLEMENTATION
This section describes implementation considerations for spinal
codes. After describing general implementation principles, we de-
scribe the salient features of our hardware prototype.
The ﬁrst goal when implementing a communication system is
selecting the range of conditions under which we would like the
system to perform well. For wireless networks, we expect the maxi-
mum SNR observed in practice to be 30 to 35 dB [14, 10]. At the
lower end, we would like to support as low an SNR as possible, to
work in challenging conditions with high external interference.
An implementation of spinal codes should pre-select (one or more)
hash functions, RNGs, and values of k, perhaps even at the time of
protocol standardization. Of course, these could be selected dynam-
ically from a set of possibilities, but the sender and receiver need
to agree on them. k determines the maximum possible rate (with
our puncturing schedule, it is 8k bits/symbol), but the complexity of
decoding is exponential in k, so smaller values have lower decoding
cost. In §8.5, we ﬁnd that k = 4 provides performance close to
capacity for SNRs as high 35 dB.
An attractive property of spinal codes is that, given a value of k,
the rate achieved under any given set of channel conditions depends
only on the decoder’s computational capabilities. The same encoded
transmission can achieve a higher rate at a decoder that invests a
greater amount of computation. With bubble decoding, each receiver
can pick a B and d independently (so, for instance, a base station
might pick values larger than a phone, and mobile devices could pick
Figure 5: Hardware implementation block diagram, showing
the spinal transmitter (a) and receiver (b).
Implementation Decisions
different values). The transmitter requires no knowledge about the
receiver’s capabilities, which avoids to need to negotiate supported
modulation and coding schemes (bit rates) on every association.
7.1
Choosing h. Spinal codes rely on the “mixing” ability of the
hash function to provide pairwise independence. We initially used
Salsa20 [5], a cryptographic-strength function with demonstrated
mixing properties. On each use, Salsa20 requires 320 XORs, 320
additions and 320 rotations on 32-bit words. With these results in
hand, we compared code performance with two other much cheaper
hash functions developed by Jenkins, one-at-a-time and lookup3.1
The one-at-a-time hash requires just 6 XORs, 15 bit shifts and 10
additions per application. Our simulations showed no discernible
difference in performance between these three hash functions. We
used one-at-a-time in our implementation and experiments.
RNG. We implemented RNG using one-at-a-time; to get the tth
output symbol, the encoder and decoder call h(si,t). This method
has the desirable property that not every output symbol has to be
generated in sequence: if some frames containing symbols are not
recovered, the decoder need not generate the missing symbols.
Other parameters. We ﬁnd that c = 6, B = 256, k = 4, d = 1 are
good choices of parameters; see §8.5 for supporting results.
PHY and link layers. The hardware implementation runs atop an
OFDM PHY. It uses code block sizes of up to 1024 bits with a 16-bit
CRC, dividing a longer packet into multiple 1024-bit code blocks.
Decoder details. The bubble decoder may be invoked multiple
times on the same message with different numbers of input symbols.
At ﬁrst glance, it would seem like a good idea to cache explored
nodes in the decoding tree between decoder runs, so in subsequent
runs the scores would only need to be incrementally updated rather
than recomputed. However, until enough symbols have arrived to
successfully decode the message, the new symbols end up changing
pruning choices to the extent that caching turns out to be unhelpful.
Instead, the decoder stores the received symbols, and uses them to
rebuild the tree in each run.
7.2 Hardware Implementation
For high-speed, low-power wireless operation, a code must be
feasible in hardware. To demonstrate feasibility, we implemented
a prototype spinal encoder and d = 1 bubble decoder using the
1http://en.wikipedia.org/wiki/Jenkins_hash_
function
Airblue [25] platform. We incorporated spinal codes into Airblue’s
802.11 OFDM stack to create 802.11-like 20 MHz and 10 MHz
OFDM transceivers (Figure 5).
Spinal codes are attractive to implement in hardware because of
the high parallelism and low latency made possible by their tree-like
structure. These properties contrast with existing high-performance
codes like turbo and LDPC, which have limited parallelism and
longer latency due to their iterative structure. Although the spinal
encoder is a straightforward sequence of hashes, RNG evaluations,
and constellation mappings, the decoder requires careful design to
take advantage of parallelism.
As samples arrive from the OFDM stack, they are written into an
SRAM in unpunctured order, with passes for a given spine value
located at adjacent addresses for batch reads. When a decode attempt
starts, a dispatch unit instructs M identical worker units to explore
all possible decodings of the ﬁrst k bits, starting from state s0. Each
worker has a certain number of hash units, which serve double duty
for computing h and RNG. A worker explores a node by computing
several hashes per cycle until it has mapped, subtracted, squared,
and accumulated the branch cost over all available passes.
Over the course of several cycles, the dispatcher and the workers
will deliver B2k scored candidate nodes to the selection unit. This
stage, corresponding to the two inner loops in the algorithm in §4.3,
is highly parallelizable: the work accomplished per cycle is linear in
the number of workers.
These candidates stream into a selection unit, which identiﬁes the
best B of them. The selection unit sorts the M candidates delivered
in a given cycle, selecting the best B. The candidates from prior
cycles will have already been winnowed down to the best B, so the
system merges those with the B from this cycle. The result is B items
in bitonic (not sorted) order. The system stores this list in a register,
and on the next cycle ﬁnishes sorting these B in parallel with sorting
the new M.
Once all B2k nodes have been scored and selected, the best B
become the new beam, and are copied to the backtrack memory.
This step advances the outer loop of the algorithm. On the last
iteration, the system fully sorts the B candidates and picks the best
one, then follows backtrack pointers to recover the message, and
checks its CRC.
This prototype has a throughput of up to 10 Mbps in FPGA
technology. Synthesized using the Synopsis Design Compiler for the
TSMC 65 nm process, the design can sustain 50 Mbps. This decoder
is competitive with algorithms like Viterbi decoding in terms of logic
area (.60 mm2 versus .12 mm2), which is encouraging considering
the decades of research and development devoted to Viterbi decoding.
In more recent work, we have developed a hardware spinal decoder
(with some reﬁnements and generalizations to the above approach)
that is competitive with turbo decoding.
8. EVALUATION
Our goal is to evaluate spinal codes under various conditions and
compare it to the following codes:
LDPC. We use the same combinations of code rates and modula-
tions for our LDPC implementation as in 802.11n [19], using soft
demapped information. The code block size n = 648 bits. We im-
plemented a belief propagation decoder that uses forty full iterations
with a ﬂoating point representation [39]. To mimic a good bit rate
adaptation strategy such as SoftRate [41] working atop the LDPC
codes, we plot the best envelope of LDPC codes in our results; i.e.,
for each SNR, we report the highest rate achieved by the entire
family of LDPC codes.
Raptor code. We follow a similar construction optimized for the
AWGN channel to Yedidia & Palanki [26], with an inner LT code
generated using the degree distribution in the Raptor RFC [22], and
an outer LDPC code as suggested by Shokrollahi [33] with a forty-
iteration belief propagation decoder. The outer code rate is 0.95
with a regular left degree of 4 and a binomial right degree. We
experimented with different symbol sets, and report results for the
dense QAM-256 constellation as well as QAM-64. We calculate
the soft information between each received symbol and the other
symbols, a process that takes time exponential in the number of
constellation points: QAM-2α requires time Θ(2α/2).
Strider. Our Strider implementation is a C++ port of the Matlab soft-
ware from Gudipati [12]. We use the recommended 33 data blocks
(layers), a rate-1/5 base turbo code with QPSK modulation, and up
to 27 passes. Unless mentioned otherwise, we use the recommended
code block size of 50490 bits. A signiﬁcant enhancement we added
to Strider is puncturing, to enable it to achieve a ﬁner-grained set of
rates than in the original work (denoted by “Strider+”).
8.1 Experimental Setup and Metrics
Software platform. To evaluate the different codes under the
same conditions, we integrated all codes into a single framework,
built with no sharing of information between the transmitter and
receiver components. A generic rateless execution engine regulates
the streaming of symbols across processing elements from the en-
coder, through the mapper, channel simulator, and demapper, to the
decoder, and collects performance statistics. All codes run through
the same engine. In most cases, we measure performance across an
SNR range from −5 dB to 35 dB, stepping by 1 dB at a time.
Hardware experiments. We use on-air hardware experiments
to cross-validate the software platform results and to demonstrate
that spinal codes perform well in hardware under real-world, wide-
band conditions. We use high-speed transceivers constructed using
Airblue [25], which is built out of Xilinx XUPV5 FPGA boards and
USRP2 radio front-ends. All on-air experiments were conducted in
the 2.4 GHz ISM band in an unshielded laboratory at MIT CSAIL.
We tested spinal codes with both 20 MHz and 10 MHz waveforms.
Metrics. We evaluate two metrics: the rate and the gap to ca-
pacity. We measure the rate in bits per symbol, so multiplying that
number by the channel bandwidth (in Hz), and subtracting OFDM
overheads, would give the throughput in bits per second.
The gap to capacity is often a more instructive metric than the rate
because it allows us to compare how close different codes are to the
Shannon limit. The “gap to capacity” of a code, C , at a given SNR,
is deﬁned as how much more noise a capacity-achieving code can
handle and still provide the same throughput as C . For example, say
a code achieves a rate of 3 bits/symbol at an SNR of 12 dB. Because
the Shannon capacity is 3 bits/symbol at 8.45 dB, the gap to capacity
is 8.45− 12 = −3.55 dB.
8.2 AWGN Channel Performance
Figure 6 shows three charts comparing Raptor codes, Strider, and
LDPC codes to spinal codes from experiments run on the standard
code parameters for each code. The ﬁrst two charts show the rates
as a function of SNR, while the third shows the gap to capacity.
The two spinal code curves (256 and 1024 bits) both come closer to
Shannon capacity than any of the other codes across all SNR values
from −5 dB to 35 dB. The gap-to-capacity curves show that spinal
codes consistently maintain a smaller gap than all the other codes.
We aggregate by SNR to summarize gains under different con-
ditions. Above an SNR of 20 dB, spinal codes obtain a rate 21%
higher than Raptor/QAM-256, 40% higher than Strider, and 54%
higher than the LDPC envelope. Between 10 and 20 dB, spinal
codes achieve a rate 25% higher than Strider and 12% higher than
Raptor/QAM-256. At SNRs below 10 dB, spinal codes achieve a
rate 20% higher than Raptor/QAM-256 and 32% higher than Strider.
Figure 6: Rates achieved by spinal code with k = 4, B = 256, d = 1, and the other codes (Strider+ is Strider with our puncturing
enhancement). Experiments at each SNR average Raptor performance over 100-300 kbits of data, Strider over 5-20 Mbits, LDPC
over 2 Mbits, and spinal codes over 0.6 to 3 Mbits.
Strider. Strider uses 33 parallel rate-1/5 turbo codes with QPSK
modulation, so without puncturing, the rates it achieves track the
expression (2/5)· 33/‘ bits/symbol, where ‘ is the number of passes
required for successful decoding. In the tested SNR range, Strider
needs at least ‘ = 2 passes to decode, for a maximum rate of 6.6
bits/symbol. The puncturing enhancement we added (Strider+) pro-
duces the more graded set of achieved rates shown in Figure 6. At
low SNR, we ﬁnd that Strider is unable to successfully decode as
many messages as spinal codes. Another source of inefﬁciency
in Strider is that the underlying rate-1/5 turbo code has a non-
negligible gap to capacity. The results (without puncturing) are
generally consistent with Figure 4a in the Strider paper [12]; it is
important to note that the “omniscient” scheme discussed in that
paper is constrained to modulation and coding schemes in 802.11a/g,
and as such has a signiﬁcant gap to the Shannon capacity.
Raptor. We are unaware of any previously reported Raptor result
for the AWGN channel that achieves rates as high as those shown
in our implementation [26]. We believe that one reason for the
good performance is that we have a careful demapping scheme that
attempts to preserve as much soft information as possible. That
said, spinal codes still perform 12%–21% better across the entire
SNR range, with the greatest gains at low and high SNRs. There
are two reasons for better performance: ﬁrst, spinal codes naturally
incorporate soft information, while Raptor (and also Strider) loses
information in the mapping/demapping steps, and second, the LT
code used in Raptor has some information loss. We experimented
with Raptor/QAM-64 as well, ﬁnding that it performs a little better
at low-to-medium SNR (16% worse than spinal codes, rather than
20%), but does much worse (54%) at high SNR. The dense QAM-
256 constellation does entail a signiﬁcantly higher decoding cost for
Raptor, whereas spinal codes naturally support dense constellations.
LDPC. The primary reason why spinal codes do better than the
best envelope of LDPC codes has to do with the ability of rateless