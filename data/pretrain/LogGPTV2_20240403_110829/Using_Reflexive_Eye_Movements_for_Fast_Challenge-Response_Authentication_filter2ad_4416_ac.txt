head movements and gazes outside of the screen area usually
last even longer, it is important to denoise the raw data be-
fore further analysis. These artefacts are ﬁltered based on
research that shows the peak angular speed of the human
Figure 5: Biometric challenge-response authentication pro-
tocol. User claims his identity, after which the workstation
generates a fresh gaze-challenge c that is an ordered list of po-
sitions in which the stimulus is shown. User looks at (LookAt)
a screen where the stimulus is shown at N positions {( ˆxi, ˆyi)}.
Meanwhile, the gaze tracking device records the user’s gaze
paths gi for all stimulus positions that constitute the gaze-
response (cid:126)g. The workstation veriﬁes the freshness of (cid:126)g, and
ﬁnally veriﬁes that the biometric features extracted from (cid:126)g
correspond to the claimed identity.
generates a fresh visual stimulus, which we refer to as gaze-
challenge (cW ) in the rest of the paper. cW consists of a
set of n randomly chosen coordinates, which uniquely deﬁne
the interactive stimulus described in Section 4.1. As the
gaze-challenge is presented to the user, his eyes reﬂexively
respond with a series of eye movements, which constitute
the gaze-response (rU ). Gaze-response is recorded by the
gaze tracking device through the gaze channel.
In order to accept or reject the user’s authentication re-
quest, the workstation performs two veriﬁcation steps: Ver-
ifyFreshness and VerifyIdentity. These are described in detail
in Sections 4.3 and 4.4, respectively.
In the ﬁnal message, the workstation notiﬁes the user if
he has been granted or denied access to the system.
4.3 VerifyFreshness
As described in Section 4.1, each visual stimulus is uniquely
deﬁned by a list of N coordinates; therefore, it is possible
to always present a diﬀerent random gaze-challenge to the
user. Since no visual stimulus shown to users is ever reused,
in order to verify the freshness of the response, it suﬃces
to verify if the received gaze-response closely corresponds
to the freshly presented gaze-challenge. As visualized in
Figure 3, if some gaze-response was recorded while speciﬁc
gaze-challenge was shown to the user, then the user’s eye
movements should closely resemble the order and positions
in which the stimulus dot was shown. This is visible in Fig-
ure 1: despite diﬀerences in gaze patterns of diﬀerent users,
all of them correspond to the locations of the stimulus dot.
The system determines if the gaze-response is indeed fresh
by ensuring that the user timely gazed at the majority of
the stimulus positions. After a stimulus dot is shown in
UserUWorkstationWUchooseuniformlyatrandom:c=[(ˆxi,ˆyi):1≤i≤N]PresentStimulusAt(ˆxi,ˆyi)respondtochallenge:gi=LookAt(ˆxi,ˆyi)girepeatN:~g=g1k...kgNVerifyFreshness(~g,c)VerifyIdentity(~g,U)accept/rejectAuthenticationProtocol(a) Temporal Features
(b) Spatial Features
Figure 6: Visualization of features on (a) temporal and (b) spatial plots of a the raw gaze tracking data. In Subﬁgure (a),
the moment when stimulus changes position is depicted with a vertical red line. The period depicted with horizontal stripes
is physiologically impossible for a human eye to perform and is caused by a blink. We remove such artefacts with methods
described in Section 5.
eye to lie between 700 and 900 deg/sec [18], and the peak
angular acceleration to not cross 100000 deg/sec2.
Having grouped the measurements as belonging either to a
ﬁxation or a saccade, we proceed to calculate a set of features
for each recorded gaze sample, ignoring those measurements
that are classiﬁed as noise by the procedure.
5.2 Feature Extraction
We next compute the features for gaze classiﬁcation. The
features should be as varied for diﬀerent users and as similar
as possible for multiple authentication attempts of the same
user.
As Figure 6a shows, each gaze-response consists of inter-
mixed periods of saccades and ﬁxations, and each such pe-
riod allows us to compute multiple features. However, we
are interested in computing a set of identiﬁable feature val-
ues for the whole gaze-response, irrespective of the number
of elicited saccades and ﬁxations; to that end, and to reduce
the eﬀect of noise, feature values for a single gaze-response
are computed as the median of feature values computed on
individual saccades or ﬁxations in that gaze measurement.
Since not all potential features contribute the same amount
of distinguishing power, we follow a semi-automated ap-
proach to select the optimal set of features for the authenti-
cation system. Initially, we explore a broader set of ﬁxation
and saccade traits, in addition to a range of other metrics
that measure overall characteristics of the gaze path. Based
on the Relative Mutual Information (RMI), we test the fea-
tures on randomly chosen subsets of the data set, measure
their classiﬁcation performance, and exclude those that do
not achieve satisfactory results. The RMI values of the re-
sulting features that we use in the remainder of the paper
can be found in Table 1, while Figure 6 illustrates the ex-
traction procedure.
As the RMI values in Table 1 show, medians of average
angular speeds during ﬁxations or saccades, as well as the
duration of ﬁxations are among the most speciﬁc features
we tested. This ﬁnding is congruent with the feature assess-
ment conducted by Eberz et al. [13], where pairwise speeds
exhibit the highest relative mutual information, only out-
performed by some of their static features, such as pupil
diameter. Contrary to their results, we identify saccade
curviness (ratio of air distance and total distance of a sac-
cade) and saccade latency to be the features that yield the
most distinguishing power. Furthermore, we identify several
discriminative features based on computing a convex hull of
all measurements in a ﬁxation: convex hull and circum-
ference, as well as ﬁxation density, deﬁned as the ratio of
the convex hull area and the number of gaze measurements
in that ﬁxation.
This paper only uses dynamic characteristics of eye move-
ments; we thus purposely forego using several potentially
discriminating features that the gaze tracking devices can
provide, such as an estimate of user’s pupil size and the dis-
tances between the user’s eyes.
In prior work, pupil size
was shown to be a discriminative feature for gaze-based au-
thentication systems [13], however, the authors raise valid
concerns that an adversary could manipulate his pupil size,
e.g., by controlling the lightning conditions. Despite poten-
tial classiﬁcation improvements, in order to provide a more
conservative estimate of the performance that gaze-based
authentication systems can achieve, in this paper we choose
to employ only features that can be extracted from raw co-
ordinates of the user’s gaze.
5.3 User Enrolment
During enrolment, several gaze-responses are used to train
a dedicated 2-class classiﬁer that the system will use as user’s
identity veriﬁer: based on the set of feature values extracted
from any subsequent gaze-response, the classiﬁer makes a
decision whether the values correspond to him or not.
Besides legitimate user’s gaze-responses, the enrolment
procedure requires a similarly sized set of gaze-responses be-
longing to other users that are labeled as negative samples
during classiﬁer training.
We use a Support Vector Machine (SVM) [10] with Radial
Basis Function (RBF) kernel as the classiﬁer, since SVMs are
known to provide strong classiﬁcation results for non-linear
data sets. We also evaluated other classiﬁcation algorithms
on a subset of the data, and conﬁrmed that SVMs achieved
stronger classiﬁcation than other evaluated statistical mod-
els (Random Forrest and AdaBoost Trees).
0100200300050010001500Time [ms]Angular speed [deg/s]Saccade Fixation Noise / Blink Stimulus changesSaccadelatencyFixationdurationSaccadedurationMax. saccadespeedTime tomax speed0501001502002500100200300X PositionY PositionSaccade airdistanceConvex hullcircumferenceTotal saccadedistanceConvex hullareaFixation centerMax distanceto centerTable 1: Relative Mutual Information (RMI) of an assortment of the most informative features
Median of saccade
Duration
Avg. speed
Max. speed
Latency
Max acceleration
Ratio air/total distance
RMI
0.1864
0.1921
0.1709
0.2041
0.1675
0.2397
Median of ﬁxation
Duration
Avg. speed
Max. speed
Max. distance to center
Convex hull area
Convex hull circumference
Density
RMI
0.1959
0.2150
0.1968
0.1604
0.1894
0.1899
0.2063
Overall
Avg. time per stimulus
Avg. distance per stimulus
Avg. speed
RMI
0.1824
0.1927
0.2053
SVMs with RBF kernels are fully deﬁned by two hyper-
parameters: 1) C, which controls the trade-oﬀ between the
penalty of incorrect classiﬁcation and the margin of the deci-
sion hyperplane, and 2) σ, which is a parameter that deﬁnes
the scale of the radial basis function. The optimal pair of
hyper-parameter values is chosen from a predetermined set
of potential values, based on the evaluation that uses 5-fold
cross-validation: for each pair of potential hyperparameters,
80% of the enrolment data is used to train the resulting clas-
siﬁer, while the remaining 20% of the enrolment data is used
to evaluate the classiﬁcation performance; this is repeated
ﬁve times.
The pair of hyperparameters that resulted in strongest
classiﬁcation performance is ﬁnally used to derive the ﬁnal
user classiﬁer which is used in future authentication.
6. DATA ACQUISITION
In order to experimentally evaluate the performance of
the proposed system and protocol, we developed a proto-
type and ran a series of user experiments to gather data for
analysis.
6.1 System Prototype
Setup. Our prototype setup is composed of a gaze track-
ing device (SMI RED 500 [38]), a 24-inch LED screen and
a desktop computer. The generation of the visual stimu-
lus and the gaze sampling was performed by a custom-built
software library that controls the gaze tracking device. We
implemented procedures that take care of the internal cali-
bration of the gaze tracker, the measurement of the sampling
accuracy and the visual presentation of the stimulus, as well
as the acquisition of the gaze samples captured by the gaze
tracker.
Parameters. For each authentication attempt, the system
generated a visual challenge consisting of N = 25 random
dot positions. Red stimulus dot was shown on a plain dark
background, with a diameter of 0.7 cm. In order to detect
that a dot was successfully gazed, we used a perimeter radius
of r = 1.4 cm. If not successfully gazed, the dot changed
position after tmax = 1000 ms. The distance between users’
eyes and the gaze tracking device (positioned directly un-
derneath the screen) was 70 cm.
6.2 User Experiments
Experiment Design. For the purpose of assessing feasibil-
ity and performance of the proposed system, we conducted a
series of user experiments that reﬂect the scenario described
in Section 3. We refer to a series of consecutive authentica-
tion attempts with the same participant as one session. Each
session lasted about 10 minutes and included a brieﬁng and
15 authentication attempts. Before participant’s ﬁrst session
we generated a calibration proﬁle that was reused during all
subsequent sessions with that participant. To analyze the
performance of our system, both from the perspective of a
user and an attacker, we divided the participants into two
groups: legitimate users who have completed the enrollment
procedure, and external attackers, whose gaze characteris-
tics were not known to the system.
In order to show that our system can successfully authen-