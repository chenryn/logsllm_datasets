                }
                return result;
            }
        """
    links = await page_handler.querySelectorAllEval("[src]", get_src_or_href_js())
当然这里你也可以使用 TreeWalker。
同时在拼接相对URL时应该注意base标签的值。
       test
       Have you seen our Bird Cages?
相对url `"../cages/birds.gif"` 将解析为`http://www.test.com/cages/birds.gif`。
###  注释中的链接
注释中的链接一定不能忽略，我们发现很多次暴露出存在漏洞的接口都是在注释当中。这部分链接可以用静态解析的方式去覆盖，也可以采用下面的代码获取注释内容并用正则匹配：
    comment_elements = await page_handler.xpath("//comment()")
    for each in comment_elements:
        if self.page_handler.isClosed():
            break
        # 注释节点获取内容 只能用 textContent
        comment_content = await self.page_handler.evaluate("node => node.textContent", each)
        # 自定义正则内容 regex_comment_url
        matches = regex_comment_url(comment_content)          
        for url in matches:
            print(url)
## 0x07 去重
说实话这部分是很复杂的一个环节，从参数名的构成，到参数值的类型、长度、出现频次等，需要综合很多情况去对URL进行去重，甚至还要考虑RESTful
API设计风格的URL，以及现在越来越多的伪静态。虽然我们在实践过程中经过一些积累完成了一套规则来进行去重，但由于内容繁琐实在不好展开讨论，且没有太多的参考价值，这方面各家都有各自的处理办法。但归结起来，单靠URL是很难做到完美的去重，好在漏洞扫描时即使多一些重复URL也不会有太大影响，最多就是扫描稍微慢了一点，其实完全可以接受。所以在这部分不必太过纠结完美，实在无法去重，设定一个阈值兜底，避免任务数量过大。
但如果你对URL的去重要求较高，同时愿意耗费一些时间并有充足的存储资源，那么你可以结合响应内容，利用网页的 **结构相似度** 去重。
###  结构相似度
一个网页主要包含两大部分：网页结构和网页内容。一些伪静态网页的内容可能会由不同的信息填充，但每个网页都有自己独一无二的结构，结构极其相似的网页，多半都属于伪静态页面。每一个节点它的节点名、属性值、和父节点的关系、和子节点的关系、和兄弟的关系都存在特异性。节点的层级越深，对整个DOM结构的影响越小，反之则越大。同级的兄弟节点越多，对DOM结构的特异性影响也越小。可以根据这些维度，对整个DOM结构进行一个特征提取，设定不同的权值，同时转化为特征向量，然后再对两个不同的网页之间的特征向量进行相似度比较（如伪距离公式），即可准确判断两个网页的结构相似度。
这方面早已有人做过研究，百度10年前李景阳的专利[《网页结构相似性确定方法及装置》](http://xueshu.baidu.com/usercenter/paper/show?paperid=232b0da253211ecf9e2c85cb513d0bd3&site=xueshu_se)
就已经很清楚的讲述了如何确定网页结构相似性。全文通俗易懂，完全可以自动手动实现一个简单的程序去判断网页结构相似度。整体不算复杂，希望大家自己动手实现。
####  大量网页快速相似匹配
这里我想讲一下，在已经完成特征向量提取之后，面对庞大的网页文档，如何做到 **在大量存储文档中快速搜索和当前网页相似的文档**
。这部分是基于我自己的摸索，利用ElasticSearch的搜索特性而得出的 **简单方法** 。
首先，我们在通过一系列处理之后，将网页结构转化为了特征向量，比如请求
`https://www.360.cn/`的网页内容经过转化后，得到了维数为键，权值为值的键值对，即特征向量：
    {
        5650: 1.0, 
        5774: 0.196608, 
        5506: 0.36, 
        2727: 0.157286, 
        1511: 0.262144, 
        540: 0.4096, 
        1897: 0.4096, 
        972: 0.262144, 
        ... ...
    }
一般稍微复杂点的网页全部特征向量会有数百上千个，在大量的文档中进行遍历比较几乎不可能，需要进行压缩，这里使用最简单的维数 **取余**
方式，将维数压缩到100维，之后再对值进行离散化变成整数：
    { 50: 13, 75: 8, 92: 18, 33: 12, 2: 15, 86: 10, 9: 9, 95: 10, 55: 14, 42: 12, 35: 15, 82: 10, 17: 7, 54: 14, 22: 11, 10: 16, 77: 11, 44: 17, 60: 9, 26: 19, ... ... }
现在，我们得到了一个代表360主站网页结构的100维 **模糊特征向量**
，由0-99为键的整数键值对组成，接下来，我们按照键的大小顺序排列，组成一个空格分割的字符串：
    0:2 1:10 2:15 3:9 4:4 5:7 6:10 7:15 8:11 9:9 10:16 11:4 12:12 ... ...
最后我们将其和网页相关内容本身一起存入ElasticSearch中，同时对该向量设置分词为`whitespace`：
    "fuzz_vector": {
        "type": "text",
        "analyzer": "whitespace"
    }
这样，我们将模糊特征向量保存了下来。当新发现一个网页文档时，如何查找？
首先我们需要明白，这个100维特征向量就代表这个网页文档的结构，相似的网页，在相同维数上的权值是趋于相同的（因为我们进行了离散化），所以，如果我们能计算两个向量在相同维数上权值相同的个数，就能大致确定这两个网页是否相似！
举个例子，对于安全客的两篇文章，`https://www.anquanke.com/post/id/178047` 和
`https://www.anquanke.com/post/id/178105` ，我们分别进行以上操作，可以得到以下的两组向量：
    0:6 1:5 2:3 3:7 4:5 5:1 6:9 7:2 8:4 9:6 10:4 11:4 12:6 13:2 14:10 15:10 16:8 ...
    0:6 1:6 2:3 3:7 4:5 5:1 6:9 7:2 8:3 9:6 10:4 11:4 12:6 13:2 14:10 15:8 16:8 ...
相同的键值对占到了 **70**
个，说明大部分维度的DOM结构都是相似的。通过确定一个阈值（如30或者50），找出相同键值对大于这个数的文档即可。一般会得到 **个位数**
的文档，再对它们进行完整向量的相似度计算，即可准确找出和当前文档相似的历史文档。
那么如何去计算两个字符串中相同词的个数呢？或者说，如果根据某个阈值筛选出符合要求的文档呢？答案是利用ElasticSearch的match分词匹配。
    "query": {
        "match": {
            "fuzz_vector": {
                "query": "0:6 1:5 2:3 3:7 4:5 5:1 6:9 7:2 8:4 ... ...",
                "operator": "or",
                "minimum_should_match": 30
            }
        }
    }
以上查询能快速筛选出相同键值对个数为30及以上的文档，这种分词查询对于亿级文档都是毫秒返回。
## 0x08 任务调度
我这里谈论的任务调度并不是指链接的去重以及优先级排列，而是具体到单个browser如何去管理对应的tab，因为Chromium的启动和关闭代价非常大，远大于标签页Tab的开关，并且如果想要将Chromium云服务化，那么必须让browser长时间驻留，所以我们在实际运行的时候，应当是在单个browser上开启多个Tab，任务的处理都在Tab上进行。
那么这里肯定会涉及到browser对Tab的管理，如何动态增减？我使用的是`pyppeteer`，因为CDP相关操作均是 **异步**
，那么对Tab的动态增减其实就等价于 **协程任务** 的动态增减。
首先，得确定单个browser允许同时处理的最大Tab数，因为单个browser其实就是一个进程，而当Tab数过多时，维持了过多的websocket连接，当你的处理逻辑较复杂，单个进程的CPU占用就会达到极限，相关任务会阻塞，效率下降，某些Tab页面会超时退出。所以单个的browser能同时处理的Tab页面必须控制到一定的阈值，这个值可以根据观察CPU占用来确定。
实现起来思路很简单，创建一个事件循环，判断当前事件循环中的任务数与最大阈值的差值，往其中新增任务即可。同时，因为开启事件循环后主进程阻塞，我们监控事件循环的操作也必须是异步的，办法就是创建一个任务去往自身所在的事件循环添加任务。
当然，真实的事件循环并不是一个图中那样的顺序循环，不同的任务有不同占用时间以及调用顺序。
示例代码如下：
    import asyncio
    class Scheduler(object):
        def __init__(self, task_queue):
            self.loop = asyncio.get_event_loop()
            self.max_task_count = 10
            self.finish_count = 0
            self.task_queue = task_queue
            self.task_count = len(task_queue)
        def run(self):
            self.loop.run_until_complete(self.manager_task())
        async def tab_task(self, num):
            print("task {num} start run ... ".format(num=num))
            await asyncio.sleep(1)
            print("task {num} finish ... ".format(num=num))
            self.finish_count += 1
        async def manager_task(self):
            # 任务队列不为空 或 存在未完成任务
            while len(self.task_queue) != 0 or self.finish_count != self.task_count:
                if len(asyncio.Task.all_tasks(self.loop)) - 1  Chromium 的相关操作必须在主线程完成，意味着你无法通过多线程去开启多个Tab和browser。
## 0x09 结语
关于爬虫的内容上面讲了这么多依旧没有概括完，调度关系到你的效率，而本文内容中的细节能够决定你的爬虫是否比别人发现更多链接。特别是扫描器爬虫，业务有太多的case让你想不到，需要经历多次的漏抓复盘才能发现更多的情况并改善处理逻辑，这也是一个经验积累的过程。如果你有好的点子或思路，非常欢迎和我交流！
微博：[@9ian1i](https://github.com/9ian1i "@9ian1i")
## 0x10 参考文档
[@fate0](https://github.com/fate0 "@fate0")： [爬虫基础篇[Web
漏洞扫描器]](http://blog.fatezero.org/2018/03/05/web-scanner-crawler-01/)，[爬虫
JavaScript 篇[Web 漏洞扫描器]](http://blog.fatezero.org/2018/04/09/web-scanner-crawler-02/)， [爬虫调度篇[Web 漏洞扫描器]](http://blog.fatezero.org/2018/04/15/web-scanner-crawler-03/)  
[@Fr1day](https://github.com/Fr1day "@Fr1day")：
[浅谈动态爬虫与去重](https://www.anquanke.com/post/id/85298)，[浅谈动态爬虫与去重(续)](https://www.anquanke.com/post/id/95294)  
@猪猪侠： 《WEB2.0启发式爬虫实战》  
## 关于我们
0Kee
Team隶属于360信息安全部，360信息安全部致力于保护内部安全和业务安全，抵御外部恶意网络攻击，并逐步形成了一套自己的安全防御体系，积累了丰富的安全运营和对突发安全事件应急处理经验，建立起了完善的安全应急响应系统，对安全威胁做到早发现，早解决，为安全保驾护航。技术能力处于业内领先水平，培养出了较多明星安全团队及研究员，研究成果多次受国内外厂商官方致谢，如微软、谷歌、苹果等，多次受邀参加国内外安全大会议题演讲。目前主要研究方向有区块链安全、WEB安全、移动安全（Android、iOS）、网络安全、云安全、IOT安全等多个方向，基本覆盖互联网安全主要领域。