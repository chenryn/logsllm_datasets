title:Exploiting Unintended Feature Leakage in Collaborative Learning
author:Luca Melis and
Congzheng Song and
Emiliano De Cristofaro and
Vitaly Shmatikov
(cid:19)(cid:17)(cid:18)(cid:26)(cid:1)(cid:42)(cid:38)(cid:38)(cid:38)(cid:1)(cid:52)(cid:90)(cid:78)(cid:81)(cid:80)(cid:84)(cid:74)(cid:86)(cid:78)(cid:1)(cid:80)(cid:79)(cid:1)(cid:52)(cid:70)(cid:68)(cid:86)(cid:83)(cid:74)(cid:85)(cid:90)(cid:1)(cid:66)(cid:79)(cid:69)(cid:1)(cid:49)(cid:83)(cid:74)(cid:87)(cid:66)(cid:68)(cid:90)
Exploiting Unintended Feature Leakage
in Collaborative Learning
∗
Luca Melis
UCL
PI:EMAIL
∗
Congzheng Song
Cornell University
PI:EMAIL
Emiliano De Cristofaro
UCL & Alan Turing Institute
PI:EMAIL
Vitaly Shmatikov
Cornell Tech
PI:EMAIL
Abstract—Collaborative machine learning and related tech-
niques such as federated learning allow multiple participants,
each with his own training dataset, to build a joint model by
training locally and periodically exchanging model updates.
We demonstrate that these updates leak unintended informa-
tion about participants’ training data and develop passive and
active inference attacks to exploit this leakage. First, we show that
an adversarial participant can infer the presence of exact data
points—for example, speciﬁc locations—in others’ training data
(i.e., membership inference). Then, we show how this adversary
can infer properties that hold only for a subset of the training
data and are independent of the properties that the joint model
aims to capture. For example, he can infer when a speciﬁc person
ﬁrst appears in the photos used to train a binary gender classiﬁer.
We evaluate our attacks on a variety of tasks, datasets, and
learning conﬁgurations, analyze their limitations, and discuss
possible defenses.
I. INTRODUCTION
Collaborative machine learning (ML) has recently emerged
as an alternative to conventional ML methodologies where
all training data is pooled and the model is trained on this
joint pool. It allows two or more participants, each with his
own training dataset, to construct a joint model. Each partic-
ipant trains a local model on his own data and periodically
exchanges model parameters, updates to these parameters, or
partially constructed models with the other participants.
Several architectures have been proposed for distributed,
collaborative, and federated learning [9, 11, 33, 38, 62, 68]:
with and without a central server, with different ways of
aggregating models, etc. The main goal is to improve the
training speed and reduce overheads, but protecting privacy of
the participants’ training data is also an important motivation
for several recently proposed techniques [35, 52]. Because
the training data never leave the participants’ machines, col-
laborative learning may be a good match for the scenarios
where this data is sensitive (e.g., health-care records, private
images, personally identiﬁable information, etc.). Compelling
applications include training of predictive keyboards on char-
acter sequences that users type on their smartphones [35],
or using data from multiple hospitals to develop predictive
models for patient survival [29] and side effects of medical
treatments [30].
Collaborative training, however, does disclose information
via model updates that are based on the training data. The
key question we investigate in this paper is: what can be
inferred about a participant’s training dataset from the
model updates revealed during collaborative model training?
Of course, the purpose of ML is to discover new information
the data. Any useful ML model reveals something
about
about the population from which the training data was drawn.
For example, in addition to accurately classifying its inputs,
a classiﬁer model may reveal the features that characterize
a given class or help construct data points that belong to
this class. In this paper, we focus on inferring “unintended”
features, i.e., properties that hold for certain subsets of the
training data, but not generically for all class members.
The basic privacy violation in this setting is membership
inference: given an exact data point, determine if it was used
to train the model. Prior work described passive and active
membership inference attacks against ML models [24, 53],
but collaborative learning presents interesting new avenues for
such inferences. For example, we show that an adversarial
participant can infer whether a speciﬁc location proﬁle was
used to train a gender classiﬁer on the FourSquare location
dataset [64] with 0.99 precision and perfect recall.
We then investigate passive and active property inference
attacks that allow an adversarial participant in collaborative
learning to infer properties of other participants’ training data
that are not true of the class as a whole, or even independent of
the features that characterize the classes of the joint model. We
also study variations such as inferring when a property appears
and disappears in the data during training—for example,
identifying when a certain person ﬁrst appears in the photos
used to train a generic gender classiﬁer.
For a variety of datasets and ML tasks, we demonstrate
successful inference attacks against two-party and multi-party
collaborative learning based on [52] and multi-party federated
learning based on [35]. For example, when the model is trained
on the LFW dataset [28] to recognize gender or race, we infer
whether people in the training photos wear glasses—a property
that is uncorrelated with the main task. By contrast, prior
property inference attacks [2, 25] infer only properties that
characterize an entire class. We discuss this critical distinction
in detail in Section III.
Our key observation, concretely illustrated by our exper-
iments, is that modern deep-learning models come up with
separate internal representations of all kinds of features, some
of which are independent of the task being learned. These
“unintended” features leak information about participants’
(cid:165)(cid:1)(cid:19)(cid:17)(cid:18)(cid:26)(cid:13)(cid:1)(cid:45)(cid:86)(cid:68)(cid:66)(cid:1)(cid:46)(cid:70)(cid:77)(cid:74)(cid:84)(cid:15)(cid:1)(cid:54)(cid:79)(cid:69)(cid:70)(cid:83)(cid:1)(cid:77)(cid:74)(cid:68)(cid:70)(cid:79)(cid:84)(cid:70)(cid:1)(cid:85)(cid:80)(cid:1)(cid:42)(cid:38)(cid:38)(cid:38)(cid:15)
(cid:37)(cid:48)(cid:42)(cid:1)(cid:18)(cid:17)(cid:15)(cid:18)(cid:18)(cid:17)(cid:26)(cid:16)(cid:52)(cid:49)(cid:15)(cid:19)(cid:17)(cid:18)(cid:26)(cid:15)(cid:17)(cid:17)(cid:17)(cid:19)(cid:26)
(cid:23)(cid:26)(cid:18)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:43:37 UTC from IEEE Xplore.  Restrictions apply. 
training data. We also demonstrate that an active adversary can
use multi-task learning to trick the joint model into learning
a better internal separation of the features that are of interest
to him and thus extract even more information.
Some of our inference attacks have direct privacy implica-
tions. For example, when training a binary gender classiﬁer on
the FaceScrub [40] dataset, we infer with high accuracy (0.9
AUC score) that a certain person appears in a single training
batch even if half of the photos in the batch depict other
people. When training a generic sentiment analysis model on
Yelp healthcare-related reviews, we infer the specialty of the
doctor being reviewed with perfect accuracy. On another set
of Yelp reviews, we identify the author even if their reviews
account for less than a third of the batch.
We also measure the performance of our attacks vis-`a-
vis the number of participants (see Section VII). On the
image-classiﬁcation tasks, AUC degrades once the number of
participants exceeds a dozen or so. On sentiment-analysis tasks
with Yelp reviews, AUC of author identiﬁcation remains high
for many authors even with 30 participants.
Federated learning with model averaging [35] does not
reveal individual gradient updates, greatly reducing the infor-
mation available to the adversary. We demonstrate successful
attacks even in this setting, e.g., inferring that photos of a
certain person appear in the training data.
Finally, we evaluate possible defenses—sharing fewer
gradients, reducing the dimensionality of the input space,
they do not effectively thwart our
dropout—and ﬁnd that
attacks. We also attempt
to use participant-level different
privacy [36], which, however, is geared to work with thousands
of users, and the joint model fails to converge in our setting.
II. BACKGROUND
A. Machine learning (ML)
An ML model is a function fθ : X (cid:2)→ Y parameterized by
a set of parameters θ, where X denotes the input (or feature)
space, and Y the output space.
In this paper, we focus on the supervised learning of
classiﬁcation tasks. The training data is a set of data points
labeled with their correct classes. We work with models that
take as input images or text (i.e., sequences of words) and
output a class label. To ﬁnd the optimal set of parameters
that ﬁts the training data, the training algorithm optimizes the
objective (loss) function, which penalizes the model when it
outputs a wrong label on a data point. We use L(x, y; θ) to
denote the loss computed on a data point (x, y) given the
model parameters θ, and L(b; θ) to denote the average loss
computed on a batch b of data points.
Stochastic Gradient Descent (SGD). There are many methods
to optimize the objective function. Stochastic gradient descent
(SGD) and its variants are commonly used to train artiﬁcial
neural networks, but our inference methodology is not speciﬁc
to SGD. SGD is an iterative method where at each step the
optimizer receives a small batch of the training data and
updates the model parameters θ according to the direction of
Algorithm 1 Parameter server with synchronized SGD
Server executes:
Initialize θ0
for t = 1 to T do
for each client k do
t ←ClientUpdate(θt−1)
gk
end for
θt ← θt−1 − η
end for
(cid:2)
k gk
t
(cid:4) synchronized gradient updates
ClientUpdate(θ):
Select batch b from client’s data
return local gradients ∇L(b; θ)
the negative gradient of the objective function with respect to
θ and scaled by the learning rate η. Training ﬁnishes when the
model has converged to a local minimum, where the gradient
is close to zero. The trained model is tested using held-out
data, which was not used during training. A standard metric
is test accuracy, i.e., the percentage of held-out data points
that are classiﬁed correctly.
Hyperparameters. Most modern ML algorithms have a set of
tunable hyperparameters, distinct from the model parameters.
They control the number of training iterations, the ratio of
the regularization term in the loss function (its purpose is to
prevent overﬁtting, i.e., a modeling error that occurs when a
function is too closely ﬁtted to a limited set of data points),
the size of the training batches, etc.
Deep learning (DL). A family of ML models known as
deep learning recently became very popular for many ML
tasks, especially related to computer vision and image recog-
nition [32, 51]. DL models are made of layers of non-linear
mappings from input to intermediate hidden states and then
to output. Each connection between layers has a ﬂoating-point
weight matrix as parameters. These weights are updated during
training. The topology of the connections between layers is
task-dependent and important for the accuracy of the model.
B. Collaborative learning
Training a deep neural network on a large dataset can be
time- and resource-consuming. A common scaling approach
is to partition the training dataset, concurrently train separate
models on each subset, and exchange parameters via a param-
eter server [9, 11]. During training, each local model pulls the
parameters from this server, calculates the updates based on
its current batch of training data, then pushes these updates
back to the server, which updates the global parameters.
Collaborative learning may also involve participants who
want to hide their training data from each other. We review
two architectures for privacy-preserving collaborative learning
based on, respectively, [52] and [35].
Collaborative
gradient
updates. Algorithm 1 shows collaborative learning with
synchronized gradient updates [52]. In every iteration of
training, each participant downloads the global model from
the parameter server, locally computes gradient updates based
synchronized
learning
with
(cid:23)(cid:26)(cid:19)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:43:37 UTC from IEEE Xplore.  Restrictions apply. 
Algorithm 2 Federated learning with model averaging
Server executes:
Initialize θ0
m ← max(C · K, 1)
for t = 1 to T do
St ← (random set of m clients)
for each client k ∈ St do
t ←ClientUpdate(θt−1)
θk
end for
θt ← (cid:2)
nk
n θk
k
t
end for
(cid:4) averaging local models
ClientUpdate(θ):
for each local iteration do
θ ← θ − η∇L(b; θ)
for each batch b in client’s split do
end for
end for
return local model θ
on one batch of his training data, and sends the updates to
the server. The server waits for the gradient updates from all
participants and then applies the aggregated updates to the
global model using stochastic gradient descent (SGD).
In [52], each client may share only a fraction of his
gradients. We evaluate if this mitigates our attacks in Sec-
tion VIII-A. Furthermore, [52] suggests differential privacy to
protect gradient updates. We do not include differential privacy
in our experiments. By deﬁnition, record-level differential
privacy bounds the success of membership inference, but does
not prevent property inference that applies to a group of
training records. Participant-level differential privacy, on the
other hand, bounds the success of all attacks considered in this
paper, but we are not aware of any participant-level differential
privacy mechanism that enables collaborative learning of an
accurate model with a small number of participants. We
discuss this further in Section VIII-D.
Federated learning with model averaging. Algorithm 2
shows the federated learning algorithm [35]. We set C, the
fraction of the participants who update the model in each
round, to 1 (i.e., the server takes updates from all partici-
pants), to simplify our experiments and because we ignore the
efﬁciency of the learning protocol when analyzing the leakage.
In each round, the k-th participant locally takes several steps
of SGD on the current model using his entire training dataset
of size nk (i.e., the globally visible updates are based not on
batches but on participants’ entire datasets). In Algorithm 2,
n is the total size of the training data, i.e., the sum of all
nk. Each participant submits the resulting model to the server,
which computes a weighted average. The server evaluates the
resulting joint model on a held-out dataset and stops training
when performance stops improving.
The convergence rate of both collaborative learning ap-
proaches heavily depends on the learning task and the hy-
perparameters (e.g., number of participants and batch size).
III. REASONING ABOUT PRIVACY IN MACHINE LEARNING
If a machine learning (ML) model is useful, it must reveal
information about the data on which it was trained [13]. To
argue that the training process and/or the resulting model