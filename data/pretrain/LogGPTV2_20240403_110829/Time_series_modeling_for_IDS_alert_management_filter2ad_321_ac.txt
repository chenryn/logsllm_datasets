.
.
.
sn
s
1
s
1
∇1
∇d
trend removal
trend removal
ˆs
1
e
detection
stationary
structure
removal
AR
model of
stationary
structure
Figure 3: Diagram of the detection process.
1000
800
600
400
200
0
−200
−400
−600
−800
0
5
10
15
20
25
30
35
40
45
Figure 4: SNMP Request UDP after detrending.
The constant component has been eliminated, but
the changes of constant level remain visible
soon. This cannot be the normal situation, and if happen-
ing, we do not wish to eliminate such information from the
series.
There are several ways to remove the trend and the pe-
riodic components from time series [4]. We have chosen to
use lag-d diﬀerencing operator, ∇d, for two reasons. First,
it does not require the trend to remain constant over time,
and second, it does not require estimation of several param-
eters. An additional plus is that it can be applied to remove
both the trend and the periodic components. It is deﬁned
as
∇dXt = Xt − Xt−d .
(3)
In other words, the resulting series is the diﬀerence between
two observations of the original series, d time units apart.
With d = 1 this is the analogy of the derivation operator for
continuous functions.
When applying ∇1 to a linear trend Tt of form Tt = bt+c,
we obtain ∇1Tt = Tt − Tt−1 = bt + c − (b (t − 1)) + c = b,
the slope of the trend function.
If viewed as the deriva-
tion, this step leaves us with a series representing the rate
of change in the alert ﬂow. For example with SNMP Request
UDP, this step removes the constant component, visible in
Fig 2(a), and makes the level shifts, and thus the interest-
ing phenomena, more apparent. The transformed series, i.e.
the one consisting of the values S 
t of Fig. 3, is showed in
Fig. 4.
We apply ∇1 to all series. This will cost us a loss of
information contained in the series, but for the anomaly
detection the linear trend and the absolute value of the alert
intensity are not necessary.
5.3 Removing the Periodicity
As mentioned above, the ∇d operator can be used also
to remove the periodic component from time series. The
application of ∇d to the model Xt of the equation (2) with-
out the trend component, which can be removed as shown
above, and where Pt has the period d, results to
∇dXt = Pt − Pt−d + Rt − Rt−d = Rt − Rt−d .
(4)
After this operation, were left with a random component
(Rt − Rt−d). The application of ∇d requires the knowledge
of the period d. We will develop this point further in sec-
tion 6, for the moment let us assume that it is known.
We decide whether ∇d is applied to a series based on sam-
ple autocorrelation function values of S . It can be shown [3,
p. 222] that for a series without any structure, about 95%
of the sample autocorrelations should fall within the bounds
±1.96/√n, where n is the number of samples. If the sample
autocorrelation values for the lag d are outside these limits,
we apply ∇d to the series.
5.4 Removing the Stationary Structure
To remove the remaining stationary structure Rt from the
series we build a model of this structure. We then use the
series from which we have removed trend and periodicity,
S 
t , as input to the model and then remove the model’s
output ˆSt
t . This leaves us the abnormal component
Et.
from S 
The structure in Rt is captured in an AR(p) time series
model, deﬁned as
Rt =
p
X
k=1
φkRt−k + Zt ,
(5)
where {Zt} ∼ WN `0, σ2´ i.e. white noise. In English this
means that the value of Rt at the current instant t is ob-
tained as a sum of two terms, the weighted sum of p previous
observations of Rt, and the noise term.
The model of equation (5) is a parametric model, and thus
we need to 1) choose the model degree p, and 2) estimate
0
parameters φk for k = 1, . . . , p before we can use the model.
Generally, the choice of p is done by building diﬀerent mod-
els, and choosing the “best” according to some criterion.
In our case this is a decent compromise between interesting
phenomena versus the non-interesting signaled by the model
using the estimation data.
The parameter estimation is done using an algorithm based
on least squares. In brief, this means that the chosen param-
eters minimize the square of diﬀerence between observations
and the prediction done by the model.5
If the model of the equation (2) would describe the normal
component of the alert series St suﬃciently, after the trend
and the periodicity have been removed, the need for further
modeling with AR models could be deﬁned by testing the
whiteness of the remaining series for example with Ljung-
Box test [10]. If S  resembles white noise, there is no more
structure to be modeled, and this step is of no use. However,
as the real world alert data contains anomalies, and model of
the equation (2) is not perfect, this approach is less feasible.
For the moment we build the AR model for every series,
which can cause some unnecessary computational overhead.
This step diﬀers from the previous ones since we use train-
ing data for the parameter estimation. Once the training has
been done and the parameters have been estimated, we can
use each new observation S 
t as input to the model, and ob-
tain Et as diﬀerence between the observation and the model
output ˆSt
5.5 Anomaly Detection
.
´
.
`
0, σ2
After these steps, we have isolated the abnormal compo-
nent Et of the alert series.
If the original series St does
not contain any anomalies, and our model of normal be-
havior Xt is exact, the abnormal component is white noise,
Et ∼ WN
However, in reality usually neither is the case. Anoma-
lies in alert series and model insuﬃciencies, both at con-
ceptual level and in the parameter estimation, mean that
Et is not white noise. To avoid signaling artifacts caused
by model deﬁcits and random variations, we pick out only
the most signiﬁcant changes in Et. An anomaly is signaled
if the current value et diﬀers more than n standard devia-
tions from the average of the past values. The default value
for n is three, but it can be adjusted to increase or reduce
the number of anomalies signaled. The average of past val-
ues and standard deviation are estimated using exponen-
tially weighted moving averages. The detection method is
described with more details in [17].
The knowledge concerning the normal activity could be
interesting as such, as it can help the operator to better
understand the monitored system. However, this kind of
analysis would need some level of understanding of the un-
derlying theory from the operator. To keep things simple
we present him only the most signiﬁcant phenomena of the
abnormal part of the alert ﬂow.
6. RESULTS
In this section we present the results obtained when we
applying this methodology to the alerts described in sec-
tion 4. The ﬁrst part of alert corpus, 406 observations, was
used as estimation data for AR model parameters, and the
5For details,
helpdesk/help/toolbox/ident/arx.html
see
http://www.mathworks.com/access/
F
C
A
e
p
m
a
S
l
0.5
0.4
0.3
0.2
0.1
0
−0.1
−0.2
0
1
2
3
4
Lag
5
6
7
8
9
Figure 5: The sample autocorrelation values for
Whatsup after trend removal
Table 2: Strongest periods found by algorithm
Flow
Lags (hours)
-
SNMP
168
Whatsup
Dest Unr
144
LOCAL-POLICY 168
24
Speedera
-
24
72
167
12
-
23
48
24
10
-
15
24
24
2
latter part, 600 observations, was left for validation. The
tool was implemented using Matlab.
6.1 The Periodicity in Alert Flows
To remove the periodic component Pt with a period d
using ∇d, we need to know d. Visual inspection of alert
series and the sample autocorrelation function (ACF) values
as well as the intuition suggested periods close to one day or
week. Figure 5 shows the sample ACF values for Whatsup
up to a lag of nine days. The dashed lines show the 95%
conﬁdence interval for sample ACF values of white noise.
One can see that there is rather strong positive correlation
for one and seven days shifts as the highest sample ACF
values are at lags corresponding to one week and one day.
We also used an algorithm that removed the periodic com-
ponent corresponding to the lag of the largest absolute value
of the sample ACF, and then worked down recursively to-
wards shorter lags. Table 2 shows the ﬁrst four lags used
by the algorithm for the periodicity removal. The ﬁrst i.e.
the strongest periodic component removed had the period
a multiple of 24 hours for all but SNMP Request udp, which
does not have any periodic component present as can been
seen in Fig 2(a). If the alert series showed signiﬁcant au-
tocorrelation, it had strong weekly and daily components.
This observation conﬁrmed the intuition we had had con-
cerning the periodicity in the alert ﬂows.
We chose to use ∇d only with d corresponding to a week,
if at all, for the following reasons: 1) every application of
∇d suppresses information from series, 2) causes the loss of
ﬁrst d observations, since there is not enough historic data
to apply the operator, 3) applying ∇d with d corresponding
0
to a day or a week removed the majority of the strong au-
tocorrelations, and 4) the autocorrelations with the lag of
168 hours (a week) were among the strongest, As explained
in section 5.3, the sample ACF value corresponding to a lag
of one week is used to determine whether ∇d is applied to a
ﬂow.
6.2 Deﬁning the model degrees
We used several model degrees p, namely 4, 10, 16, 26 with
the estimation data to ﬁnd out the most suitable for each
ﬂow.
In addition to AR models, a range of ARMA (p, q)
models were estimated. At least with the used model de-
grees and estimation methods they did not give signiﬁcant
improvement if any over the AR models but their estimation
is more resource consuming.
The p was chosen such that we could detect as many as
possible of the interesting phenomena described in Sect 4.2,
and have as few meta-alerts issued on the non-interesting
ﬂow behavior. According to anomalies signaled from esti-
mation data, we chose the model degrees as follows: SNMP
AR(4), Whatsup AR(26), Dest Unr AR(26), LOCAL POLICY
AR(26), and speedera AR(26).
6.3 Detected anomalies
After the model degrees were ﬁxed and parameters esti-
mated from the estimation data, we applied the complete
alert processing method to the validation data. Next we ex-
amine in detail the anomalies signaled for each ﬂow by the
chosen models. The overall statistics can be seen in Table 3.
For each ﬂow the number of signaled anomalies is in column
An. The detection of known and interesting phenomena
pi is covered in columns K+ and K-, showing the signaled
and missed phenomena, respectively. The pi were identiﬁed
in section 4.2. Note that not all pi are in the validation
data and K- does not contribute into An. In addition, the
tool signaled also new anomalies. These can be 1) new, in-
teresting phenomena unidentiﬁed in the manual inspection,
2) uninteresting phenomena that at the ﬁrst glance could
seem somewhat signiﬁcant, but actually are part of the nor-
mal behavior, 3) artifacts created by the transforms we have
performed in the processing chain, like the use of ∇week op-
erator. The case one phenomena are useful to the operator,
and the number of occurrences is recorded in column N+.
The case two anomalies are rather harmless, and usually
quite easily identiﬁed as such. However, they do waste op-
erator’s time. The case three is the worst, as the tool signals
artiﬁcial anomalies where they do not exist. Depending on
the alert ﬂow, their correct identiﬁcation can be trivial or
quite diﬃcult. The number of occurrences for the two latter
is recorded in column N-. For speedera we depict the N+
and N- in Fig. 6 We used also the EWMA model of [17] on
validation data for comparison.
SNMP All the four known phenomena in the validation
data were signaled, and all the additional anomalies are real
anomalies, small vibrations on top of the constant alert ﬂow.
In that sense they are not harmful at all, we just considered
the eleven others less interesting than the largest ones. The
new phenomena signaled can be manifestations of SNMP
traﬃc injection, or harmless but intermittent behavior in
the otherwise extremely constant ﬂow.
For this ﬂow the EWMA approach provided the same
meta-alerts, which is not surprising given the constant na-
ture of the alert ﬂow.
Table 3: Signaled and missed phenomena
Flow
SNMP
Whatsup
Dest unr
Local policy
Speedera
Total
An
15
11
12
12
5
55
K+
p2 , p3 , p4 , p5
p2 , p3 , p6 , p8
-
p2 , p4 , p5
p1 , p1
14
K-
-
p4 , p5 , p7
-
p3
-
4
N+ N-