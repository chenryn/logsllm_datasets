### 第 2 步：创建 JAAS 文件配置了用户之后，我们需要为每个 Broker 创建一个对应的 JAAS文件。因为本例中的两个 Broker 实例是在一台机器上，所以我只创建了一份JAAS 文件。但是你要切记，在实际场景中，你需要为每台单独的物理 Broker机器都创建一份 JAAS 文件。JAAS 的文件内容如下：    KafkaServer {org.apache.kafka.common.security.scram.ScramLoginModule requiredusername="admin"password="admin";};关于这个文件内容，你需要注意以下两点：-   不要忘记最后一行和倒数第二行结尾处的分号；-   JAAS 文件中不需要任何空格键。这里，我们使用 admin 用户实现 Broker 之间的通信。接下来，我们来配置Broker 的 server.properties 文件，下面这些内容，是需要单独配置的：    sasl.enabled.mechanisms=SCRAM-SHA-256    sasl.mechanism.inter.broker.protocol=SCRAM-SHA-256    security.inter.broker.protocol=SASL_PLAINTEXT    listeners=SASL_PLAINTEXT://localhost:9092第 1 项内容表明开启 SCRAM 认证机制，并启用 SHA-256 算法；第 2项的意思是为 Broker 间通信也开启 SCRAM 认证，同样使用 SHA-256 算法；第 3项表示 Broker 间通信不配置 SSL，本例中我们不演示 SSL 的配置；最后 1项是设置 listeners 使用 SASL_PLAINTEXT，依然是不使用 SSL。另一台 Broker的配置基本和它类似，只是要使用不同的端口，在这个例子中，端口是 9093。
### 第 3 步：启动 Broker现在我们分别启动这两个 Broker。在启动时，你需要指定 JAAS文件的位置，如下所示：    $KAFKA_OPTS=-Djava.security.auth.login.config=/kafka-broker.jaas bin/kafka-server-start.sh config/server1.properties......[2019-07-02 13:30:34,822] INFO Kafka commitId: fc1aaa116b661c8a (org.apache.kafka.common.utils.AppInfoParser)[2019-07-02 13:30:34,822] INFO Kafka startTimeMs: 1562045434820 (org.apache.kafka.common.utils.AppInfoParser)[2019-07-02 13:30:34,823] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)    $KAFKA_OPTS=-Djava.security.auth.login.config=/kafka-broker.jaas bin/kafka-server-start.sh config/server2.properties......[2019-07-02 13:32:31,976] INFO Kafka commitId: fc1aaa116b661c8a (org.apache.kafka.common.utils.AppInfoParser)[2019-07-02 13:32:31,976] INFO Kafka startTimeMs: 1562045551973 (org.apache.kafka.common.utils.AppInfoParser)[2019-07-02 13:32:31,978] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)此时，两台 Broker 都已经成功启动了。
### 第 4 步：发送消息在创建好测试主题之后，我们使用 kafka-console-producer脚本来尝试发送消息。由于启用了认证，客户端需要做一些相应的配置。我们创建一个名为producer.conf 的配置文件，内容如下：    security.protocol=SASL_PLAINTEXTsasl.mechanism=SCRAM-SHA-256sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username="writer" password="writer";之后运行 Console Producer 程序：    $ bin/kafka-console-producer.sh --broker-list localhost:9092,localhost:9093 --topic test  --producer.config /producer.conf>hello, world>   可以看到，Console Producer 程序发送消息成功。
### 第 5 步：消费消息接下来，我们使用 Console Consumer程序来消费一下刚刚生产的消息。同样地，我们需要为 kafka-console-consumer脚本创建一个名为 consumer.conf 的脚本，内容如下：    security.protocol=SASL_PLAINTEXTsasl.mechanism=SCRAM-SHA-256sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username="reader" password="reader";之后运行 Console Consumer 程序：    $ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092,localhost:9093 --topic test --from-beginning --consumer.config /consumer.conf hello, world很显然，我们是可以正常消费的。
### 第 6 步：动态增减用户最后，我们来演示 SASL/SCRAM 动态增减用户的场景。假设我删除了 writer用户，同时又添加了一个新用户：new_writer，那么，我们需要执行的命令如下：    $ bin/kafka-configs.sh --zookeeper localhost:2181 --alter --delete-config 'SCRAM-SHA-256' --entity-type users --entity-name writerCompleted Updating config for entity: user-principal 'writer'. $ bin/kafka-configs.sh --zookeeper localhost:2181 --alter --delete-config 'SCRAM-SHA-512' --entity-type users --entity-name writerCompleted Updating config for entity: user-principal 'writer'. $ bin/kafka-configs.sh --zookeeper localhost:2181 --alter --add-config 'SCRAM-SHA-256=[iterations=8192,password=new_writer]' --entity-type users --entity-name new_writerCompleted Updating config for entity: user-principal 'new_writer'.现在，我们依然使用刚才的 producer.conf 来验证，以确认 Console Producer程序不能发送消息。    $ bin/kafka-console-producer.sh --broker-list localhost:9092,localhost:9093 --topic test  --producer.config /Users/huxi/testenv/producer.conf>[2019-07-02 13:54:29,695] ERROR [Producer clientId=console-producer] Connection to node -1 (localhost/127.0.0.1:9092) failed authentication due to: Authentication failed during authentication due to invalid credentials with SASL mechanism SCRAM-SHA-256 (org.apache.kafka.clients.NetworkClient)......很显然，此时 Console Producer 已经不能发送消息了。因为它使用的producer.conf 文件指定的是已经被删除的 writer 用户。如果我们修改producer.conf 的内容，改为指定新创建的 new_writer 用户，结果如下：    $ bin/kafka-console-producer.sh --broker-list localhost:9092,localhost:9093 --topic test  --producer.config /producer.conf>Good!  现在，Console Producer 可以正常发送消息了。这个过程完整地展示了 SASL/SCRAM 是如何在不重启 Broker的情况下增减用户的。至此，SASL/SCRAM 配置就完成了。在专栏下一讲中，我会详细介绍一下如何赋予writer 和 reader 用户不同的权限。
## 小结好了，我们来小结一下。今天，我们讨论了 Kafka目前提供的几种认证机制，我给出了它们各自的优劣势以及推荐使用建议。其实，在真实的使用场景中，认证和授权往往是结合在一起使用的。在专栏下一讲中，我会详细向你介绍Kafka 的授权机制，即 ACL 机制，敬请期待。![](Images/3c09e5d3ccfb8cfbaef6f12167550e77.png){savepage-src="https://static001.geekbang.org/resource/image/af/5c/af705cb98a2f46acd45b184ec201005c.jpg"}
## 开放讨论请谈一谈你的 Kafka 集群上的用户认证机制，并分享一个你遇到过的"坑"。欢迎写下你的思考和答案，我们一起讨论。如果你觉得有所收获，也欢迎把文章分享给你的朋友。![](Images/a7d15815f9efb5693db5b2d278244658.png){savepage-src="https://static001.geekbang.org/resource/image/c8/bf/c89da43deab85fe7cb06acec867aa5bf.jpg"}