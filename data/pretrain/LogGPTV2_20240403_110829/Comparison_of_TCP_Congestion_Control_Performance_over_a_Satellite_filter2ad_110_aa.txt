title:Comparison of TCP Congestion Control Performance over a Satellite
Network
author:Saahil Claypool and
Jae Chung and
Mark Claypool
Comparison of TCP Congestion Control
Performance over a Satellite Network
Saahil Claypool1, Jae Chung2, and Mark Claypool1(B)
1 Worcester Polytechnic Institute, Worcester, MA, USA
{smclaypool,claypool}@wpi.edu
2 Viasat, Marlborough, MA, USA
PI:EMAIL
Abstract. While satellite Internet bitrates have increased, latency can
still degrade TCP performance. Realistic assessment of TCP over satel-
lites is lacking, typically done by simulation or emulation, if at all. This
paper presents experiments comparing four TCP congestion control algo-
rithms – BBR, Cubic, Hybla and PCC – on a commercial satellite net-
work. Analysis shows similar steady state bitrates for all, but with sig-
niﬁcant diﬀerences in start-up throughputs and round-trip times caused
by queuing of packets in ﬂight. Power analysis combining throughput
and latency shows during steady state, PCC is the most powerful, due
to relatively high throughputs and consistent, relatively low round-trip
times, while for small downloads Hybla is the most powerful, due to fast
throughput ramp-ups. BBR generally fares similarly to Cubic in both
cases.
1 Introduction
Satellites are an essential part of modern networking, providing ubiquitous
connectivity even in times of disaster. There are 2100+ satellites in orbit, a
67% increase from 2014 to 2019 [2]. Improvements in satellite technology have
increased transmission capacities more than 20x with the total capacity of
planned Geosynchronous orbit satellites over 5 Tb/s.
Geosynchronous orbit satellites have about 300 milliseconds of latency to
bounce a signal up and down [8], a hurdle for TCP protocols that use round-
trip time communication to advance their data windows. TCP congestion control
algorithms play a critical role determining throughput in the presence of network
latency and loss. A better understanding of TCP congestion control algorithm
performance over satellite networks is needed in order to assess challenges and
opportunities that satellites have to better support TCP moving forward.
However, there are few published studies measuring network performance
over actual satellite networks [17], with most studies either using just simula-
tions [3] or emulations with satellite parameters [1,11,18,19].
This paper presents results from experiments that measure the performance
of TCP over a commercial satellite Internet network. We compare four TCP con-
gestion control algorithms, chosen based on their representative approaches to
c(cid:2) Springer Nature Switzerland AG 2021
O. Hohlfeld et al. (Eds.): PAM 2021, LNCS 12671, pp. 499–512, 2021.
https://doi.org/10.1007/978-3-030-72582-2_29
500
S. Claypool et al.
congestion control: default loss-based Cubic [15], bandwidth-delay product-based
BBR [16], utility function-based PCC [11], and satellite-optimized Hybla [4]. Our
network testbed and experiments are done on the Internet, but are designed to
be comparable by interlacing runs of each protocol serially to minimize temporal
diﬀerences and by doing 80 bulk downloads for each protocol to provide for a
large sample. In addition, a custom ping application provides several days worth
of round-trip time and lost packet data for a baseline satellite network with no
other traﬃc.
Analysis of our “quiet” network gives baseline satellite loss and round-trip
time characteristics. Analysis comparing the four algorithms show diﬀerences
in throughput, round-trip times and retransmissions during steady state and
start-up phases, with power providing a combined measure of throughput and
delay.
The rest of this report is organized as follows: Sect. 2 presents related work,
Sect. 3 describes our methodology, Sect. 4 analyzes the data, and Sect. 5 summa-
rizes our conclusions and future work.
2 Related Work
Caini and Firrinielli [4] propose TCP Hybla to overcome the limitations TCP
NewReno ﬂows have when running over high-latency links (e.g., a Satellite).
TCP Hybla modiﬁes the standard congestion window increase with an extension
based on the round-trip time. In Hybla slow-start, cwnd = cwnd + 2ρ − 1 and
in congestion avoidance cwnd = cwnd + ρ2
cwnd, where ρ = RT T /RT T0. RT T0 is
ﬁxed at a “wired” round-trip time of 0.025 s. Hybla is available for Linux as of
kernel 2.6.11 (in 2005).
Ha et al. [15] develop TCP Cubic as an incremental improvement to earlier
congestion control algorithms. Cubic is less aggressive than previous algorithms
in most steady-state cases, but can probe for more bandwidth quickly when
needed. TCP Cubic has been the default in Linux as of kernel 2.6.19 (in 2007),
Windows 10.1709 Fall Creators Update (in 2017), and Windows Server 2016
1709 update (in 2017).
Cardwell et al. [16] provide TCP Bottleneck Bandwidth and Round-trip time
(BBR) as an alternative to Cubic’s (and Hybla’s) loss-based congestion control.
BBR uses the maximum bandwidth and minimum round-trip time observed to
set the congestion window size (up to twice the bandwidth-delay product). BBR
has been deployed by Google servers since at least 2017 and is available for Linux
as of kernel 4.9 (end of 2016).
Dong et al. [11] propose TCP PCC that observes performance based on
small measurement “experiments”. The experiments assess throughput, loss, and
round-trip times with a utility function, adopting the rate that has the best
utility. PCC is not generally available for Linux, but Compira Labs1 provided
us with a Linux-based implementation.
1 https://www.compiralabs.com/.
TCP over a Satellite
501
Cao et al. [5] analyze measurement results of BBR and Cubic over a range
of diﬀerent network conditions, showing that the relative diﬀerence between the
bottleneck buﬀer size and bandwidth-delay product dictates when BBR performs
well. Our work extends this work by providing evaluation of Cubic and BBR in
a satellite conﬁguration, with round-trip times signiﬁcantly beyond those tested
by Cao et al.
Obata et al. [17] evaluate TCP performance over actual (not emulated, as
is typical) satellite networks. They compare a satellite-oriented TCP conges-
tion control algorithm (STAR) with NewReno and Hybla. Experiments with
the Wideband InterNetworking Engineering test and Demonstration Satellite
(WINDS) network show throughputs around 26 Mb/s and round-trip times
around 860 milliseconds. Both TCP STAR and TCP Hybla have better through-
puts over the satellite link than TCP NewReno – we evaluate TCP Hybla, but
there is no public Linux implementation of TCP STAR available.
Wang et al. [19] provide preliminary performance evaluation of QUIC with
BBR on an emulated a satellite network (capacities 1 Mb/s and 10 Mb/s, RTTs
200, 400 and 1000 ms, and packet loss up to 20%). Their results conﬁrm QUIC
with BBR has throughput improvements compared with TCP Cubic for their
emulated satellite network.
Utsumi et al. [18] develop an analytic model for TCP Hybla for steady state
throughput and round-trip time over satellite links. They verify the accuracy
of their model with simulated and emulated satellite links (capacity 8 Mb/s,
RTT 550 ms, and packet loss rates up to 2%). Their analysis shows substantial
improvements to throughput over that of TCP Reno for loss rates above 0.0001%
Our work extends the above with comparative performance for four TCP
congestion control algorithms on an actual, commercial satellite network.
3 Methodology
We setup a testbed, measure network baseline loss and round-trip times, serially
bulk-download data using each algorithm, and analyze the results.
3.1 Testbed
We setup a Viasat satellite Internet link so as to represent a client with a “last
mile” satellite connection. Our servers are conﬁgured to allow for repeated tests
and comparative performance by consecutive serial runs with all conditions the
same, except for the change in TCP congestion control algorithm.
Our testbed is depicted in Fig. 1. The client is a Linux PC with an Intel i7-
1065G7 CPU @ 1.30 GHz and 32 GB RAM. There are four servers, each with a
diﬀerent TCP congestion control algorithm: BBR, Cubic, Hybla and PCC. Each
server has an Intel Ken E312xx CPU @ 2.5 GHz and 32 GB RAM. The servers
and client all run Ubuntu 18.04.4 LTS, Linux kernel version 4.15.0.
The servers connect to our University LAN via Gb/s Ethernet. The campus
network is connected to the Internet via several 10 Gb/s links, all throttled to 1
502
S. Claypool et al.
Fig. 1. Satellite measurement testbed.
Gb/s. Wireshark captures all packet header data on each server and the client.
The client connects to a Viasat satellite terminal (with a modem and router)
via a Gb/s Ethernet connection. The client’s downstream Viasat service plan
provides a peak data rate of 144 Mb/s.
The terminal communicates through a Ka-band outdoor antenna (RF ampli-
ﬁer, up/down converter, reﬂector and feed) through the Viasat 2 satellite2 to the
larger Ka-band gateway antenna. The terminal supports adaptive coding and
modulation using 16-APK, 8 PSK, and QPSK (forward) at 10 to 52 MSym/s
and 8PSK, QPSK and BPSK (return) at 0.625 to 20 MSym/s.
The Viasat gateway performs per-client queue management, where the queue
can grow up to 36 MBytes, allowing a maximum queuing delay of about 2 s at
the peak data rate. Queue lengths are controlled at the gateway by Active Queue
Management (AQM) that randomly drops 25% of incoming packets when the
queue is over a half of the limit (i.e., 18 MBytes).
The performance enhancing proxy (PEP) that Viasat deploys by default is
disabled for all experiments in order to assess congestion control performance
independent of the PEP implementation, and to represent cases where a PEP
could not be used (e.g., for encrypted ﬂows).
3.2 Baseline
For the network baseline, we run UDP Ping3 from a server to the client continu-
ously for 1 week. This sends one 20-byte UDP packet every 200 ms (5 packets/s)
from the server to the client and back, recording the round-trip time for each
packet returned and the number of packets lost. Doing round-trip time mea-
surements via UDP avoids any special treatments routers may have for ICMP
packets.
3.3 Downloads
We compare the performance of four congestion control algorithms, chosen
as representatives of diﬀerent congestion control approaches: loss-based Cubic,
2 https://en.wikipedia.org/wiki/ViaSat-2.
3 http://perform.wpi.edu/downloads/#udp.
TCP over a Satellite
503
bandwidth-delay product-based BBR (version 1), satellite-optimized loss-based
Hybla and utility function-based PCC. The four servers are conﬁgured to pro-
vide for bulk-downloads via iperf34 (v3.3.1), each server hosting one of our
four congestion control algorithms. Cubic, BBR and Hybla are used without
further conﬁguration. PCC is conﬁgured to use the Vivace-Latency utility func-
tion [12], with throughput, loss, and round-trip time coeﬃcients set to 1, 10, and
2, respectively.
For all hosts, the default TCP buﬀer settings are changed on both the server
and client – setting tcp mem, tcp wmem and tcp rmem to 60 MBytes – so that ﬂows
are not ﬂow-controlled and instead are governed by TCP’s congestion window.
The client initiates a connection to one server via iperf, downloading 1 GByte,
then immediately proceeding to the next server. After cycling through each
server, the client pauses for 1 min. The process repeats a total of 80 times –
thus, providing 80 network traces of a 1 GByte download for each protocol over
the satellite link. Since each cycle takes about 15 min, the throughput tests run
for about a day total. We analyze results from a weekday in July 2020.
4 Analysis
4.1 Network Baseline
We start by analyzing the network baseline loss and round-trip times, obtained
on a “quiet” satellite link to our client – i.e., without any of our active bulk-
downloads. Table 3 provides summary statistics.
The vast majority (99%) of round-trip times are between 560 and 625 ms
(median 597 ms, mean 597.5 ms, std dev 16.9 ms). However, the round-trip times
have a heavy-tailed tendency, with 0.1% from 625 ms to 1500 ms and 0.001%
from 1700 to 2200 ms. These high values show multi-second round-trip times