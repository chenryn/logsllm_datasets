and retrieve the next stage. The software testing community has
previously proposed a technique that leverages a combination of
dynamic and symbolic execution to execute targeted paths in a
program [23, 31]. Although this research proposes methods that
are similar to what we suggest, applying them to malware analysis
presents new challenges because malware samples frequently evade
analysis. Combining previous work with the strategies that the par-
ticipants use to analyze multistage malware offers a potential way
to automate additional analysis steps of these campaigns.
7.3 Usability Recommendations
From our participants’ explanations of the disadvantages of cur-
rent tools, we identify two important aspects that developers of
malware analysis tools should keep in mind. Lack of customization
of tools is the main disadvantage that participants mentioned in
section 6. According to many analysts, it is important for tools such
as dynamic analysis sandboxes to allow users to customize the set-
tings of the analysis system, such as installed programs as well as
file paths and their content. As mentioned in subsection 6.2, some
analysts find that commercial dynamic analysis tools lack critical
features such as the type of architecture malware can be executed
on (Ex: 32bit vs. 64bit) and support for malware samples that target
different platforms such as JavaScript or Visual Basic. During the
interviews, participants also mentioned that they use a wide range
of tools to collect all of the information they need. As P2 says "I
use VMs, VPN to hide my network, Wireshark, Process Explorer to
monitor processes, ProcMon to monitor what the process is doing
[...] there is also ProcDOT and all sort of other tools to analyze those
dumps after you get them so there are countless tools." Malware
analysts would benefit from a dynamic analysis tool that has a more
comprehensive and flexible monitoring program, which collects all
of the IOCs and presents them to the analyst in a standard format
rather than asking analysts to run different tools to obtain each
type of indicator separately.
8 RELATED WORK
User Studies. A sizable body of prior work has examined the
workflows of software engineers (e.g., [40, 49]), including engineers
writing security software [21, 22]. However, only a few prior works
have examined how security-specific engineers conduct their work.
Most related to our work, Votipka et al. [76] studied the workflow
of reverse engineers through an observational study of security
professionals. Their work develops a formalized workflow that de-
scribes how a reverse engineer completes the task of understanding
the operation of a piece of software that the professional is unfamil-
iar with. Our study confirms the finding of Votipka et al. that static
analysis is often used when reverse engineering a malware sample.
In contrast to Votipka et al. [76], the participants in our study are
all malware analysts, rather than reverse engineers. Additionally,
we find that the workflows extracted from our participants include
analysis tasks that are not present in the workflow of reverse engi-
neers described in Votipka’s work. Finally, our results contribute
a discussion of the dynamic analysis configuration process that
malware analysts use, a topic that is also not addressed in prior
work.
Another related study by Votipka et al. [75] investigates the
differences between the vulnerability discovery processes of soft-
ware testers and white-hat hackers. Its methodology consists of
semi-structured interviews with participants from both the hacker
and tester communities, and they use their results to recommend
improvements to vulnerability discovery processes of both commu-
nities. Our methodology is similar to the one used for this study,
but we focus on how malware analysis is performed in practice as
opposed to vulnerability discovery.
Malware Analysis. A major challenge of dynamic malware anal-
ysis is the proper configuration of the analysis environments such
that malware samples will reveal as much of their malicious behav-
ior as possible. Kharaz et. al. [50] automatically creates a realistic
environment as part of their ransomware detection tool. To make
an attractive environment for the ransomware, the authors design
scripts that create files with believable content pulled from the
internet, names with real words, and file metadata with reasonable
values.
As the interviews with the participants show, malware analysts
sometimes turn to static analysis techniques when they want to
reverse engineer a sample or learn more about how to configure
Another approach to overcome evasion is to design analysis
systems that are transparent to the malware [39, 52, 71, 83, 84].
These methods try to defeat fingerprinting by removing artifacts
in the analysis environment that malware can associate with the
analysis system. Other methods focus on detecting when malware
stalls before performing malicious activities, and force the malware
to execute a path that avoids the code that stalls execution [56].
Some analysis systems execute a single sample in multiple envi-
ronments in an attempt to get a greater percentage of the malware’s
true behavior to execute [26, 54, 63, 67]. A common environment
included in such systems is bare metal. When malware runs out-
side of a virtual machine, fingerprinting methods are less effective
because the artifacts collected are from a physical machine instead
of a virtual environment [53, 54, 84]. Furthermore, multiple envi-
ronment techniques have been used to automatically detect any
type of evasion techniques [26, 52, 58]. In order to help malware
analysts determine if their analysis system is vulnerable to evasion,
Jing compares artifacts from an analysis environment with a stan-
dard environment to determine the artifacts found in the analysis
environment but not in production [48].
Session 11C: Software Development and Analysis CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3064their dynamic environment. Researchers have also applied static
analysis to determine the malware targeted environment. Xu [79]
analyzes targeted malware using parallel execution environments
and automatically adjusts the environment settings that the sample
is looking for. First, static analysis is used to identify environment
query APIs and determine the possible return values. Then, the
sample is dynamically analyzed in parallel environments, each with
different environment settings for the possible return values. While
executing, the system sets environment settings such that the sam-
ple executes the most interesting paths. To identify inputs that
will trigger malicious behavior, Brumley [30] proposes a hybrid
approach that matches malware triggers to conditional checks in
the binary and treats the variables involved in such checks as sym-
bolic. Then, the malware is executed such that the instructions that
operate on symbolic variables are symbolically executed and other
instructions are executed concretely. This approach allows analysts
to identify inputs that trigger the malware and then dynamically ex-
ecute the sample with the identified inputs to observe its malicious
behavior.
There are other works that leverage hybrid analysis in ways our
participants did not discuss. Hybrid analysis has been used to find
dormant malicious behaviors [36], discover vulnerabilities [23], and
find software bugs [31]. A different application of hybrid analysis
is hybrid concolic testing [60]. This work uses random testing to
access program paths that reach deep within the program, then
symbolic analysis is applied to explore parts of the program near
that execution path.
Finally, static analysis can be leveraged independently for mal-
ware analysis. Symbolic analysis is one such technique that involves
exploring many paths of a program symbolically and using con-
straint solvers to determine whether different paths are feasible
and what inputs will cause that path to be executed [25, 70]. Since
malware is frequently obfuscated, it can be challenging to apply
these static analysis techniques in malware analysis. Such obsta-
cles have been previously explored, including symbolic analysis of
obfuscated code [80] and extraction of the semantics of obfuscated
code [29]. Also, when analyzing complicated symbolic constraints
in malware, one method solves a subset of the constraints through
decomposing them and recombining the produced inputs [31].
9 CONCLUSION
In this user study, we interviewed 21 malware analysts to get a better
understanding of how malware analysis is done in practice. These
interviews shed light on the diversity of malware analysis, which
is reflected in the broad range of workflows that our participants
follow as well as the amount of effort they put into configuring
their dynamic analysis systems. Our analysis highlights six different
components of dynamic analysis systems that practitioners focus
on when setting up dynamic analysis systems and how their setup
process differs depending on their tier. Based on the user study,
we identified challenging tasks that the participants face during
their analysis process. We first discuss promising existing academic
research that could be transitioned to practice and then propose
future research directions that could help address the remaining
challenges.
REFERENCES
[1] Capev2. https://github.com/kevoreilly/CAPEv2.
[2] Virustotal. https://virustotal.com.
[3] al-khaser. URL https://github.com/LordNoteworthy/al-khaser.
[4] Any-run - interactive online malware sandbox. https://any.run.
[5] How antivirus softwares are evolving with behaviour-based malware detection al-
gorithms. https://analyticsindiamag.com/how-antivirus-softwares-are-evolving-
with-behaviour-based-malware-detection-algorithms.
[6] Equifax says cyberattack may have affected 143 million in the u.s. https://nytimes.
com/2017/09/07/business/equifax-cyberattack.html.
[7] Free automated malware analysis service. https://hybrid-analysis.com.
[8] Malpedia, . https://malpedia.caad.fkie.fraunhofer.de/.
[9] Malshare, . https://malshare.com.
[10] Malware bazaar, . https://bazaar.abuse.ch.
[11] Mitre att&ck. https://attack.mitre.org/matrices/enterprise/.
[12] ollydbg. http://www.ollydbg.de/.
[13] Openioc: Back to the basics. https://www.fireeye.com/blog/threat-research/2013/
10/openioc-basics.html.
[14] Pyramid of pain. https://detect-respond.blogspot.com/2013/03/the-pyramid-of-
[15] Reversing labs. https://reversinglabs.com.
[16] Target missed warnings in epic hack of credit card data. https://bloom.bg/
pain.html.
2KjElxM.
[17] thezoo - a live malware repository. https://github.com/ytisf/theZoo.
[18] Twitter. https://twitter.com.
[19] Unpacme. https://unpac.me.
[20] M. Abu Rajab, J. Zarfoss, F. Monrose, and A. Terzis. A multifaceted approach to
understanding the botnet phenomenon. In Proceedings of the 17th ACM SIGCOMM,
pages 41–52, Stanford, CA, Aug. 2006.
[21] Y. Acar, M. Backes, S. Fahl, D. Kim, M. L. Mazurek, and C. Stransky. You get
where you’re looking for: The impact of information sources on code security.
In 2016 IEEE Symposium on Security and Privacy (SP), pages 289–305. IEEE, 2016.
[22] Y. Acar, M. Backes, S. Fahl, D. Kim, M. L. Mazurek, and C. Stransky. How internet
resources might be helping you develop faster but less securely. IEEE Security &
Privacy, 15(2):50–60, 2017.
[23] D. Babić, L. Martignoni, S. McCamant, and D. Song. Statically-directed dynamic
automated test generation. In Proceedings of the 2011 International Symposium on
Software Testing and Analysis, pages 12–22, 2011.
[24] M. Bailey, J. Oberheide, J. Andersen, Z. M. Mao, F. Jahanian, and J. Nazario.
Automated classification and analysis of internet malware. In Proceedings of
the 9th International Symposium on Research in Attacks, Intrusions and Defenses
(RAID), pages 178–197, 2007.
[25] R. Baldoni, E. Coppa, D. C. D’elia, C. Demetrescu, and I. Finocchi. A survey of
symbolic execution techniques. ACM Computing Surveys (CSUR), 51(3):1–39,
2018.
[26] D. Balzarotti, M. Cova, C. Karlberger, E. Kirda, C. Kruegel, and G. Vigna. Efficient
detection of split personalities in malware. In Proceedings of the 17th Annual
Network and Distributed System Security Symposium (NDSS), San Diego, CA,
Feb.–Mar. 2010.
[27] U. Bayer, P. M. Comparetti, C. Hlauschek, C. Kruegel, and E. Kirda. Scalable,
behavior-based malware clustering. In Proceedings of the 16th Annual Network
and Distributed System Security Symposium (NDSS), San Diego, CA, Feb. 2009.
[28] L. Bilge, E. Kirda, C. Kruegel, and M. Balduzzi. Exposure: Finding malicious
domains using passive dns analysis. In Proceedings of the 18th Annual Network
and Distributed System Security Symposium (NDSS), San Diego, CA, Feb. 2011.
[29] T. Blazytko, M. Contag, C. Aschermann, and T. Holz. Syntia: Synthesizing
the semantics of obfuscated code. In Proceedings of the 25th USENIX Security
Symposium (Security), pages 643–659, Vancouver, BC, Canada, Aug. 2017.
[30] D. Brumley, C. Hartwig, Z. Liang, J. Newsome, D. Song, and H. Yin. Automatically
identifying trigger-based behavior in malware. In Botnet Detection, pages 65–88.
Springer, 2008.
[31] J. Caballero, P. Poosankam, S. McCamant, D. Babi ć, and D. Song. Input generation
via decomposition and re-stitching: Finding bugs in malware. In Proceedings
of the 17th ACM conference on Computer and communications security, pages
413–425, 2010.
[32] J. Caballero, C. Grier, C. Kreibich, and V. Paxson. Measuring pay-per-install: the
commoditization of malware distribution. In Proceedings of the 20th USENIX
Security Symposium (Security), San Francisco, CA, Aug. 2011.
[33] R. Canzanese, S. Mancoridis, and M. Kam. Run-time classification of malicious
processes using system call analysis. In 2015 10th International Conference on
Malicious and Unwanted Software (MALWARE), pages 21–28. IEEE, 2015.
[34] B. Cheng, J. Ming, J. Fu, G. Peng, T. Chen, X. Zhang, and J.-Y. Marion. Towards
paving the way for large-scale windows malware analysis: Generic binary un-
packing with orders-of-magnitude performance boost. In Proceedings of the 24rd
ACM Conference on Computer and Communications Security (CCS), pages 395–411,
Toronto, Canada, Oct. 2018.
Session 11C: Software Development and Analysis CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3065[35] M. Christodorescu, S. Jha, and C. Kruegel. Mining specifications of malicious
behavior. In Proceedings of the the 6th joint meeting of the European software
engineering conference and the ACM SIGSOFT symposium on The foundations of
software engineering, pages 5–14, 2007.
[36] P. M. Comparetti, G. Salvaneschi, E. Kirda, C. Kolbitsch, C. Kruegel, and S. Zanero.
Identifying dormant functionality in malware programs. In Proceedings of the
31th IEEE Symposium on Security and Privacy (Oakland), pages 61–76, Oakland,
CA, May 2010.
[37] W. Cui, M. Peinado, Z. Xu, and E. Chan. Tracking rootkit footprints with a
practical memory analysis system. In Proceedings of the 21st USENIX Security
Symposium (Security), pages 601–615, Bellevue, WA, Aug. 2012.
[38] Z. Deng, X. Zhang, and D. Xu. Spider: Stealthy binary program instrumentation
and debugging via hardware virtualization. In Proceedings of the Annual Computer
Security Applications Conference (ACSAC), pages 289–298, 2013.
[39] A. Dinaburg, P. Royal, M. Sharif, and W. Lee. Ether: malware analysis via hardware
virtualization extensions. In Proceedings of the 15th ACM Conference on Computer
and Communications Security (CCS), pages 51–62, Alexandria, VA, Oct. 2008.
[40] D. Ford and C. Parnin. Exploring causes of frustration for software developers.
In 2015 IEEE/ACM 8th International Workshop on Cooperative and Human Aspects
of Software Engineering, pages 115–116, 2015. doi: 10.1109/CHASE.2015.19.
[41] M. Graziano, C. Leita, and D. Balzarotti. Towards network containment in
malware analysis systems. In Proceedings of the 28th Annual Computer Security
Applications Conference, pages 339–348, 2012.
[42] M. Hassen, M. M. Carvalho, and P. K. Chan. Malware classification using static
analysis based features. In 2017 IEEE Symposium Series on Computational Intelli-
gence (SSCI), pages 1–7. IEEE, 2017.
[43] E. Hollnagel. Handbook of cognitive task design. CRC Press, 2003.
[44] M. Ijaz, M. H. Durad, and M. Ismail. Static and dynamic malware analysis using
machine learning. In 2019 16th International bhurban conference on applied sciences
and technology (IBCAST), pages 687–691. IEEE, 2019.
[45] G. Jacob, R. Hund, C. Kruegel, and T. Holz. Jackstraws: Picking command and
control connections from bot traffic. In Proceedings of the 20th USENIX Security
Symposium (Security), San Francisco, CA, Aug. 2011.
[46] S. A. Jacob and S. P. Furgerson. Writing interview protocols and conducting
interviews: tips for students new to the field of qualitative research. Qualitative
Report, 17:6, 2012.
[47] N. Jagpal, E. Dingle, J.-P. Gravel, P. Mavrommatis, N. Provos, M. A. Rajab, and
K. Thomas. Trends and lessons from three years fighting malicious extensions.
In Proceedings of the 24th USENIX Security Symposium (Security), pages 579–593,
Washington, DC, Aug. 2015.
[48] Y. Jing, Z. Zhao, G.-J. Ahn, and H. Hu. Morpheus: automatically generating
heuristics to detect android emulators. In Proceedings of the Annual Computer
Security Applications Conference (ACSAC), pages 216–225, 2014.
[49] B. Johnson, R. Pandita, J. Smith, D. Ford, S. Elder, E. Murphy-Hill, S. Heckman,
and C. Sadowski. A cross-tool communication study on program analysis tool
notifications. In Proceedings of the 2016 24th ACM SIGSOFT International Sympo-
sium on Foundations of Software Engineering, FSE 2016, page 73–84, New York,
NY, USA, 2016. Association for Computing Machinery. ISBN 9781450342186. doi:
10.1145/2950290.2950304. URL https://doi.org/10.1145/2950290.2950304.
[50] A. Kharaz, S. Arshad, C. Mulliner, W. Robertson, and E. Kirda. {UNVEIL}: A
large-scale, automated approach to detecting ransomware. In Proceedings of the
24th USENIX Security Symposium (Security), pages 757–772, Washington, DC,
Aug. 2015.
[51] D. Kim, A. Majlesi-Kupaei, J. Roy, K. Anand, K. ElWazeer, D. Buettner, and R. Barua.
Dynodet: Detecting dynamic obfuscation in malware. In Proceedings of the 14th
Conference on Detection of Intrusions and Malware and Vulnerability Assessment
(DIMVA), pages 97–118, 2017.
[52] D. Kirat and G. Vigna. Malgene: Automatic extraction of malware analysis evasion
signature. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and
Communications Security, pages 769–780, 2014.
[53] D. Kirat, G. Vigna, and C. Kruegel. Barebox: efficient malware analysis on bare-
metal. In Proceedings of the Annual Computer Security Applications Conference
(ACSAC), pages 403–412, 2011.
[54] D. Kirat, G. Vigna, and C. Kruegel. Barecloud: bare-metal analysis-based eva-
sive malware detection. In Proceedings of the 23rd USENIX Security Symposium
(Security), San Diego, CA, Aug. 2014.
[55] C. Kolbitsch, T. Holz, C. Kruegel, and E. Kirda. Inspector gadget: Automated
extraction of proprietary gadgets from malware binaries. In Proceedings of the
31th IEEE Symposium on Security and Privacy (Oakland), pages 29–44, Oakland,
CA, May 2010.
[56] C. Kolbitsch, E. Kirda, and C. Kruegel. The power of procrastination: detection
and mitigation of execution-stalling malicious code. In Proceedings of the 18th
ACM Conference on Computer and Communications Security (CCS), pages 285–296,
Chicago, Illinois, Oct. 2011.
[57] J. R. Landis and G. G. Koch. An application of hierarchical kappa-type statistics in
the assessment of majority agreement among multiple observers. Biometrics, 33
(2):363–374, 1977. ISSN 0006341X, 15410420. URL http://jstor.org/stable/2529786.
[58] M. Lindorfer, C. Kolbitsch, and P. M. Comparetti. Detecting environment-sensitive
malware. In International Workshop on Recent Advances in Intrusion Detection,
pages 338–357. Springer, 2011.
[59] B. Liu, W. Huo, C. Zhang, W. Li, F. Li, A. Piao, and W. Zou. 𝛼diff: cross-version
binary code similarity detection with dnn. In Proceedings of the 33rd ACM/IEEE
International Conference on Automated Software Engineering, pages 667–678, 2018.
[60] R. Majumdar and K. Sen. Hybrid concolic testing. In 29th International Conference
on Software Engineering (ICSE’07), pages 416–426. IEEE, 2007.
[61] L. Martignoni, E. Stinson, M. Fredrikson, S. Jha, and J. C. Mitchell. A layered
architecture for detecting malicious behaviors. In International Workshop on
Recent Advances in Intrusion Detection, pages 78–97. Springer, 2008.
[62] J. Ming, D. Xu, Y. Jiang, and D. Wu. Binsim: Trace-based semantic binary diffing
via system call sliced segment equivalence checking. In Proceedings of the 25th
USENIX Security Symposium (Security), pages 253–270, Vancouver, BC, Canada,
Aug. 2017.
[63] A. Moser, C. Kruegel, and E. Kirda. Exploring multiple execution paths for
malware analysis. In Proceedings of the 28th IEEE Symposium on Security and
Privacy (Oakland), pages 231–245, Oakland, CA, May 2007.
[64] Y. Nadji, M. Antonakakis, R. Perdisci, and W. Lee. Understanding the prevalence
and use of alternative plans in malware with network games. In Proceedings of
the Annual Computer Security Applications Conference (ACSAC), 2011.
[65] M. Neugschwandtner, P. M. Comparetti, and C. Platzer. Detecting malware’s
failover c&c strategies with squeeze.
In Proceedings of the Annual Computer
Security Applications Conference (ACSAC), 2011.
[66] S. Palahan, D. Babić, S. Chaudhuri, and D. Kifer. Extraction of statistically
significant malware behaviors. In Proceedings of the Annual Computer Security