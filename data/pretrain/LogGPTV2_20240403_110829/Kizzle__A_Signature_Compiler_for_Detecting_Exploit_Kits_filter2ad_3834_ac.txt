### Tokenization in Action
```javascript
Euur1V = [];
this["l9D"]("ev#333399al");
```
- **Class**: `Euur1V`
- **Variable Declaration**: `var Euur1V = [];`
- **Identifier**: `this`
- **String**: `"l9D"`
- **Punctuation**: `(`, `)`
- **String**: `"ev#333399al"`
- **Punctuation**: `;`

**Figure 8:** Tokenization in action.

### Unpacking and Labeling Clusters

Instead of using a fixed set of Exploit Kits, we implemented unpackers for all kits under investigation. With the resulting, unpacked cluster prototype, we label the cluster.

#### B. Labeling Clusters

To label clusters with their corresponding Exploit Kit (EK) family, we use winnowing, a technique originally proposed for detecting plagiarism in code. Using a collection of known unpacked malware samples (with exploit family labels), we generate a winnow histogram for the cluster prototype and compare it against the winnow histograms for all known malware samples. If there is sufficient overlap (based on a threshold determined empirically for each malware family), we consider the cluster represented by the prototype to be malicious and from the corresponding family.

#### C. Signature Creation

For each cluster labeled as malicious, we generate a signature from the packed samples in that cluster using the following method:

1. **Find Common Subsequence**:
   - The first step is to find the maximum value of \( N \) such that every sample in a cluster has a common token string subsequence of length up to \( N \) tokens. We cap this maximum length at 200 tokens.
   - We use binary search to vary \( N \) and determine if a common subsequence of length \( N \) exists.
   - An additional constraint is that the common subsequence must be unique in every sample.

2. **Extract Sequences**:
   - Once the length of the common subsequence is known and sufficiently long (short sequences are discarded), the exact sequences of tokens and characters in each of the samples from the malicious cluster are extracted.
   - For each offset in the token sequence, the algorithm determines the distinct set of concrete strings found in the different samples at that token offset.

**Figure 9:** Illustration of the signature generation process, showing a cluster with three samples and the process of determining the distinct values at each offset. Note that quotation marks are automatically removed by AV scanners in a normalization step, so they are omitted in the final signature.

3. **Generate Regular Expression**:
   - After gathering all variants of a token at each offset, the algorithm determines a regular expression-based signature, one token at a time.
   - If the value is the same across all samples, the concrete value is added to the signature. Otherwise, the algorithm generates a regular expression that will match all elements of this set.
   - We compute an expression that will accept strings of the observed lengths and containing the characters observed, drawing on a predefined set of common patterns such as `[a-z]+`, `[a-zA-Z0-9]+`, etc.
   - The current approach uses brute force to determine a working pattern, but a more selective approach could build a more efficient decision procedure from the predefined templates.

**Example 2: Kizzle Signatures**
- **Figure 10:** Generated signatures for the Nuclear and Sweet Orange kits.
- **Nuclear Exploit Kit**:
  ```regex
  (?[0-9a-zA-Z]{3,6})=\[\[(?[0-9a-zA-Z]{3,6})\[(?[0-9a-zA-Z"']{5,8})]\( "cUluNouluNnuuluNcuuluNauuluNtuuluN" \) , \k[\k]( "sUluNuuuluNbuuluNsuuluNtuuluNrUluN" \) , \k[\k]( "dUluNouuluNcuuluNuuuluNmuuluNeuuluNnuuluN" \) , \k[\k]( "CUluNouuluNluuluNouuluNruuluN" \) , \k[\k]( "lUluNeuuluNnuuluNgUluNtuuluNhUluN" \)] , [\k[\k]]\((?.{57})\) , \k [\k]( "rUluNeuuluNpuuluNluuluNauuluNceuluN" \)]]
  var (?[0-9a-zA-Z]{3,7})
  ```
- **Sweet Orange**:
  ```regex
  ) ) ) { varaa = xx\.join(" ") ar[(Math.exp(1) - Math.E)][(1)*2] = "l" ar[(1)][3] = "WWWWWWbEWSjdHfW" varq = (Math.exp(1) - Math.E) for ( qq[a-zA-Z]{6} ) ( ) { varok = [?[0-9a-zA-Z"']{17}].charAt(Math.sqrt(196)), [?[0-9a-zA-Z"']{17}].charAt(Math.sqrt(196)), [?[0-9a-zA-Z"']{17}].charAt(Math.sqrt(196)), [?[0-9a-zA-Z"']{21}].charAt(Math.sqrt(324)), [?[0-9a-zA-Z"']{21}].charAt(Math.sqrt(324))
  ```

### Evaluation

To evaluate Kizzle, we gathered potentially malicious samples using a browser instrumented to gather telemetry collected by Internet Explorer from pages that have ActiveX content. The pages sampled came from a broad spectrum of URLs representing pages that typical users might visit. We worked with an anti-malware vendor to hook into the IExtensionValidation interface for data extraction. The Validate method of this interface allows the capture of the underlying HTML and JavaScript. Because we sample data during a potentially suspicious operation (loading ActiveX content), the fraction of malware we see is likely to be substantially higher than a typical browser would encounter.

All these kits follow the pattern described in Section II: they are packed on the outside and are relatively similar when unpacked. We compare Kizzle with a state-of-the-art commercial AV implementation, which we anonymize to avoid drawing generalizations based on our limited observations.

#### Experimental Setup

- **Figure 11:** Measurements of how these kits change over the course of a month. We measure the overlap between the unpacked centroids of malicious clusters on each day with centroids of the clusters of all previous days based on winnowing (Section III) and report the maximum overlap.
- **Observations**:
  - For three of the four kits, the amount of change over the course of the entire month is quite small, often only a few percent.
  - This contrasts greatly from the external changes at the level of the packed kits, which happen every few days.
  - RIG (Figure 11(d)) is an outlier, showing changes of 50% from day to day, explained by changes in the embedded URLs of the kit.

#### Cluster-Based Processing Performance

- **Implementation**:
  - Our implementation is based on using a cluster of machines and exploiting the inherent parallelism of our approach.
  - We used 50 machines for sample clustering and signature generation.
  - Clustering takes the majority of the time, and the reduce step described in Section III-A is often the bottleneck.
  - With more effort, the reduction step can be parallelized to improve scalability.
  - Our runs consistently completed in about 90 minutes when processing the daily data.

#### Precision of Kizzle-Generated Signatures

- **Quality Metrics**:
  - The quality of any anti-virus solution is based on its ability to find most viruses with a very low false positive rate.
  - Our goal for Kizzle is to provide rates comparable to human-written AV signatures.
  - **Figure 13:** False positive and false negative rates for Kizzle compared to those for AV. Overall, false positive rates are lower for Kizzle (except for a period between August 24 and August 26th).
  - **Figure 15:** Representative false positive. False positive rates for Kizzle overall are very small, i.e., under 0.03%.

#### Ground Truth

- **Approximation**:
  - To approximate the ground truth, we took the union of samples matched by both AV signatures and the Kizzle approach and examined the overlap.
  - To confirm false positives and false negatives, we manually inspected approximately 7,000 files, using some scripting automation to bucket samples together.
  - In addition, we analyzed the complement of that union for URL patterns matching known exploit kits. We found no cases in which a URL indicated that there were malicious samples in this complement.
  - This tedious manual validation took approximately 15 hours.

**Figure 11:** Similarity over time for a month-long time window. Note that the y-axis is different across the graphs: sometimes, the range of similarities is very narrow.