Class
var
Euur1V
=
[
]
(
)
this
”l9D”
”ev#333399al”
Keyword
Identiﬁer
Punctuation
Identiﬁer
Punctuation
String
Punctuation
Punctuation
String
Punctuation
Fig. 8: Tokenization in action.
a ﬁxed set of Exploit Kits, we instead implemented un-
packers for all kits under investigation. With the resulting,
unpacked cluster prototype, we label the cluster.
B. Labeling Clusters
To label clusters with their corresponding EK family
label, we use winnowing [34], a technique originally pro-
posed for detecting plagiarism in code. Using a collection
of known unpacked malware samples (with exploit family
labels), we generate a winnow histogram for the cluster
prototype and compare it against the winnow histograms
for all the known malware samples. If there is suﬃcient
overlap (based on a threshold that we determined em-
pirically is malware family speciﬁc), we then consider the
cluster represented by the prototype to be malicious and
from the corresponding family.
C. Signature Creation
For each cluster that is labeled as malicious, we generate
a signature from the packed samples in that cluster with
the following method.
The ﬁrst step in signature creation is to ﬁnd a maximum
value of N such that every sample in a cluster has a
common token string subsequence of
length up to N
tokens. We cap this maximum length at 200 tokens. We
ﬁnd this subsequence with binary search, varying N , and
determining if a common subsequence of length N exists.
An additional constraint, imposed during the search for a
common subsequence, is that it is unique in every sample.
Once the length of the common subsequence is known
and suﬃciently long (short sequences are discarded), the
exact sequences of tokens and characters in each of the
samples from the malicious cluster are extracted. In addi-
tion, for each oﬀset in the token sequence, the algorithm
determines the distinct set of concrete strings found in the
diﬀerent samples at that token oﬀset.
Figure 9 illustrates this process, showing a cluster with
three samples and the process of determining the distinct
values at each oﬀset. Note, that although the original
string contains quotation marks, these are automatically
removed by AV scanners in a normalization step. There-
fore, we omit these in the ﬁnal signature. Finally, after
gathering all variants of a token at each oﬀset, the algo-
rithm determines a regular expression-based signature, one
token at at time.
If the value is the same across all samples, our algorithm
adds the concrete value to the signature. Otherwise, the
algorithm must generate a regular expression that will
match all elements of this set. While this is a well-studied
problem in general (The L algorithm [2] can infer a
minimally accepting DFA), we implement an approach
focusing on our expectations of the kinds of diversity
malware writers are likely to put into their code.
We compute an expression that will accept strings of the
observed lengths, and containing the characters observed
by drawing on a predeﬁned set of common patterns such as
[a− z]+,[a− zA− Z0− 9]+, etc. The current approach uses
brute force to determine a working pattern, but a more
selective approach could build a more eﬃcient decision
procedure from the predeﬁned templates.
Example 2 Kizzle signatures. Figure 10 shows generated
signature for the Nuclear and Sweet orange kits.
For the ﬁrst signatures, Kizzle picked up on the strings
delimited by Ulun. While such long strings, that do not
naturally occur in benign applications, make the creation
of a signature easy, the kit author can easily change these
to circumvent a matching signature. Since, however, Kiz-
L2L4L3L1L2L3L1            clustering machineclustering machine...clustering machineclustering machineUnknown samplessee, hence we consider our data stream “grayware”. In
total, we gathered data for a month (August 2014) and
captured between 80,000 and 500,000 samples per day.
Another viable source of likely malicious samples would
be a malware analysis system such as Wepawet [6] or
VirusTotal.com.
Throughout the rest of this section, we focus on the
four exploit kits that are most prevalent in our data:
Nuclear, Sweet orange, Angler, and RIG. These are
the same kits highlighted in recent evaluations performed
by ZSCaler ThreatLab [42] and TrendMicro [39].
Fig. 9: An example of signature generation in action.
(? [0 -9 a - zA - Z ]{3 ,6})=\[\[(? 
[0 -9 a - zA - Z ]{3 ,6})\[(? [0 -9 a - zA - Z " ’]
{5 ,8})]\( " c U l u N o U l u N n U l u N c U l u N a U l u N t U l u N " \) ,
\k \[\ k ]\( " s U l u N u U l u N b U l u N s U l u N t U l u N r U l u N " \) ,
\k \[\ k ]\( " d U l u N o U l u N c U l u N u U l u N m U l u N e U l u N n U l u N t U l u N " \) ,
\k \[\ k ]\( " C U l u N o U l u N l U l u N o U l u N r U l u N " \) ,
\k \[\ k ]\( " l U l u N e U l u N n U l u N g U l u N t U l u N h U l u N " \)] ,
\[\ k \[\ k ]\((? .{57})\) ,\ k 
\[\ k ]\((? .{67})\) ,\ k 
\[\ k ]\( " r U l u N e U l u N p U l u N l U l u N a U l u N c U l u N e U l u N " \)]]
var (? [0 -9 a - zA - Z ]{3 ,7})
(a) Nuclear exploit kit
\ ) \ ) \ ) \ { varaa = xx \. join \( " " \) ar \[\( Math \. exp \(1\) - Math \. E \)]
\ [ \ ( 1 \ ) \ * 2 ] = " l " ar \ [ \ ( 1 \ ) ] \ [ 3 ] = " W W W W W W W b E W s j d h f W " varq =
\( Math \. exp \(1\) - Math \. E \) for
\( qq [ a - zA - Z ] { 6 } ) \ ( \ )
\{ varok =\[(? [0 -9 a - zA - Z " ’]{17})\. charAt \( Math \. sqrt
\(196\)\) ,(? [0 -9 a - zA - Z " ’ ] { 1 7 } ) \ . charAt \( Math \. sqrt
\(196\)\) ,(? [0 -9 a - zA - Z " ’ ] { 1 7 } ) \ . charAt \( Math \. sqrt
\(196\)\) ,(? [0 -9 a - zA - Z " ’]{21})\. charAt \( Math \. sqrt
\(324\)\) ,(? [0 -9 a - zA - Z " ’ ] { 2 1 } ) \ .
(b) Sweet orange
Fig. 10: Examples of Kizzle-generated signatures.
zle generates these automatically, this advantage vanishes.
Also, Kizzle picked up on the usage of templatized
variable names, as can be observed by the combination of
var1 and var2 in lines 4 to 9 of Nuclear. While Sweet
orange does not use such delimiters, it uses a simple
obfuscation technique, namely exchanging static integer
values with calls to the Math.sqrt function, allowing it to
simply change this obfuscation by using other mathemat-
ical operations. Again, Kizzle picked up on this property
of the packed payload, generating a precise signature. 
IV. Evaluation
To evaluate Kizzle, we gathered potentially mali-
cious samples using a browser instrumented to gather
telemetry collected by Internet Explorer from pages that
have ActiveX content. The pages sampled came from a
broad spectrum of URLs representing pages that typi-
cal users might visit. We worked with an anti-malware
vendor to hook into the IExtensionValidation in-
terface (http://msdn.microsoft.com/en-us/library/
dn301826(v=vs.85).aspx)
for data extraction. The
Validate method of this interface allows the capture the
underlying HTML and JavaScript. Because we sample
data during a potentially suspicious operation (loading
ActiveX content), the fraction of malware we see is likely
to be substantially higher than a typical browser will
All these kits follow the pattern we describe in Sec-
tion II: they are packed on the outside and are rela-
tively similar when unpacked. We compare Kizzle with a
state-of-the-art commercial AV implementation, which we
anonymize to avoid drawing generalizations based on our
limited observations (our position is that all commercial
AV vendors have similar challenges).
Experimental Setup: Figure 11 shows our measure-
ments of how these kits change over the course of a month.
We measure the overlap between the unpacked centroids of
malicious clusters on each day with centroids of the clus-
ters of all previous days based on winnowing (Section III)
and report the maximum overlap. Figure 11 shows that,
for three of the four kits, the amount of change over the
course of the entire month is quite small, often only a few
percent. This contrasts greatly from the external changes
at the level of the packed kits as shown in Figure 5, which
happen every few days.
These observations conﬁrm our hypothesis that most of
the change is external and happens on the packer that
surrounds the logic of the kit. In the case of Nuclear
exploit kit, there is very little change at all. We do note
that RIG (Figure 11(d)) is an outlier, showing changes
of 50% from day over day. This behavior is explained by
noticing that the changes reﬂect changes in the embedded
URLs of the kit, and, given the body of the kit is relatively
short, these URLs alone represent a signiﬁcant enough part
of the code to create a 50% churn.
Cluster-Based Processing Performance: Our imple-
mentation is based on using a cluster of machines and
exploiting the inherent parallelism of our approach. For
the performance numbers found in this section, we used 50
machines for sample clustering and signature generation.
Our experience shows that clustering takes the majority
of this time, as opposed to signature generation. The re-
duce step described in Section III-A is often the bottleneck
when we needed to reconcile the clusters computed across
the distributed machines. With more eﬀort, we believe that
the reduction step can be parallelized as well in future
work to improve our scalability. In practice, our runs con-
sistently completed in about 90 minutes when processing
the daily data. We ﬁnd this processing time is adequate
and can be further improved with more machines.
In our experiments, we detected between 280 and 1,200
Euur1V =  this   [   "l9D“   ]   ( "ev#333399al“  )   ;jkb0hA   =  this   [   "uqA“   ]   ( "ev#ccff00al“  )   ;QB0Xk    =  this   [  "k3LSC“  ]   ( "ev#33cc00al“  )   ;[A-Za-z0-9]{5,6}=this\[[A-Za-z0-9]{3,5}\]\(.{11}\);Euur1Vjkb0hAQB0Xk=this[](l9DuqAk3LSCev#333399alev#ccff00alev#33cc00al);time. We show the length of the signatures, in characters,
on the y axis of the graph. Every time there is a “bump” in
the line for one of the EKs, this means that Kizzle decides
to create a new signature.
To help the reader correlate Kizzle signatures with
manually-generated signatures, we show the labels for
hand-crafted signatures created to address these EKs over
the same period of time. To highlight some of the insights,
consider Nuclear exploit kit (blue line) starting on
August 17th. As a packing strategy, Nuclear exploit
kit uses a delimiter, which separates characters ev and
al and win and dow in Figure 4(b). You can also spot this
delimeter-based approach in Figure 10 where a string like
sUluNuUluNbUluNsUluNtUluNrUluN unpacks into substr.
The strategy that Nuclear exploit kit uses is to change
this delimiter frequently because the malware writer sus-
pects that AV signatures will try to match this code. The
green oval call-outs in the ﬁgure show signature-avoiding
changes to the delimiter as part of kit evolution.
The Kizzle algorithm can immediately react to these
minor changes in the body of the kit, as indicated by daily
changes in Kizzle signatures. To contrast with manually-
produced signatures, the ﬁrst AV signature that we see
responding to these changes emerges on August 25th. Note
that it may be the case that the AV signature did not need
to be updated to be eﬀective, but the ﬁgure illustrates that
Kizzle will automatically respond to kit changes daily.
Figure 10 shows an example of two signatures produced
by Kizzle.
Precision of Kizzle-Generated Signatures: The qual-
ity of any anti-virus solution is based on its ability to ﬁnd
most viruses with a very low false positive rate. Our goal
for Kizzle is to provide rates comparable to human-written
AV signatures when tested over the month of data we
collected. Figure 13 shows false positive and false negative
rates for Kizzle compared to those for AV. Overall, false
positive rates are lower for Kizzle (except for a period
between August 24 and August 26th). Figure 15 shows a
representative false positive. False positive rates for Kizzle
overall are very small, i.e., under 0.03%.
Figure 14 shows some details of our evaluation. The
kit that gave Kizzle the most challenge was RIG, which
occurred with low frequency in our sample set. RIG also
changed more on a daily basis, as illustrated in Figure 11.
Ground Truth: To approximate the ground truth, we
took the union of samples matched by both AV signatures
and the Kizzle approach and examined the overlap. Sub-
sequently, to conﬁrm false positives and false negatives,
we manually inspected approximately 7,000 ﬁles, using
some scripting automation to bucket samples together. In
addition, we analyzed the compliment of that union for
URL patterns matching known exploit kits. In doing so,
we found no cases in which a URL indicated that there
were malicious samples in this compliment. In total, this
tedious manual validation took approximately 15 hours.
(a) Nuclear
(b) Sweet Orange
(c) Angler
(d) RIG
Fig. 11: Similarity over time for a month-long time window. Note that the
y axis is diﬀerent across the graphs: sometimes, the range of similarities
is very narrow.
clusters per day. However, almost all of them correspond to
benign code, while only a handful are detected as malicious
and labeled as one of the exploit kits above. This shows
that despite the fact that we are analyzing grayware,
much of what we observe is benign code that falls into
a relatively small number of frequently observed clusters.
Figure 10 shows some examples of the signatures pro-
duced by Kizzle. Overall, two things immediately stand
out for the automatically-generated signatures: they are
long and they are very speciﬁc. Both of these character-