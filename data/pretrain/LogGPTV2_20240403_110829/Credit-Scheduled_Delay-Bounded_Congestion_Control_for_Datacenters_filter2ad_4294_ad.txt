allocation converges to fairness.
Figure 12 illustrates the convergence behavior. Letâ€™s denote the
fair-share rate of a flow as Râˆ— (= C
N ). Let us denote the difference of
credit sending rate at time t and time t âˆ’ 1 as D(t ). It follows that:
D (t0 + 2k + 1) = |Rn (t0 + 2k + 1) âˆ’ Rn (t0 + 2k )|
â‰ˆ | max (2âˆ’k wn (t0), wmin ) Â· {C âˆ’ Rn (t0 + 2k )}|
Similarly, we can compute D(t0 + 2k + 2). D(t ) is an exponentially
decreasing function that eventually converges to:
âˆ— = C Â· wmin (1 âˆ’ 1
D
)
N
Note C = max_rateÂ· (1+target_loss). Thus, Dâˆ— depends on wmin
and target_loss; a small wmin and target_loss results in a small Dâˆ—
value, which improves the steady state behavior. However, a small
wmin can cause delayed convergence by making (1 âˆ’ wmin ) larger
in Equation 4.
IMPLEMENTATION
5
We leverage SoftNIC [31] to implement ExpressPass. SoftNIC is a
framework that allows developers to extend NIC functionality by
exploiting high-performance kernel bypass I/O framework (DPDK).
SoftNIC enables us to pace credits with microseconds level ac-
curacy and react to the credit packets with a small delay of few
microseconds. SoftNIC also enables legacy applications and kernel
networking stacks to be used without modification. Applications run
transparently on top of SoftNIC using TCP/IP. We implement Ex-
pressPass as a module inside SoftNIC. ExpressPass module adjusts
the credit sending rate according to the feedback algorithm and paces
the transmission of credit packets. Data packets are sent only when
the ExpressPass module receives a credit packet. To start and stop
the transmission of credit packets, we implement the per-flow state
machine in Figure 7. For example, when the NIC receives data to
transmit from the kernel, it generates credit requests. Upon reception
of a credit packet, it offers a data packet if available.
For rate-limiting credit packets, we leverage the scalable rate
limiter implementation of SoftNIC. It is able to handle thousands
of concurrent flows while introducing only a few microseconds of
jitter [31]. For rate-limiting in the switch, we leverage maximum
bandwidth metering available in Broadcom chipsets. We create sep-
arate per-port queues for credits. Then, we set the maximum burst
size as 2 credits and the queue occupancy as 8 credit packets.
Credit processing and rate-limiting: To validate our implementa-
tion and setup, we measure 1) the host credit processing time of
our implementation and 2) the accuracy of credit rate-limiting at the
switch. We connect two hosts using a ToR switch (Quanta T3048-
LY2). Figure 14 (a) shows the CDF of credit processing latency.
(3)
(4)
(5)
Figure 12: Steady state behavior
(1 âˆ’ credit_loss) Â· (1 + target_loss). At time t0 + 1, increasing phase
of feedback control is triggered, and the aggregate credit sending
rate becomes:
Ri (t0 + 1) =
>
{(1 âˆ’ wi (t0 + 1)) Â· Ri (t0) + wi (t0 + 1)C}
{(1 âˆ’ wmin ) Â· Ri (t0) + wminC} > C
(2)
N(cid:88)
i =1
N(cid:88)
N(cid:88)
i =1
i =1
The aggregate credit sending rate now exceeds C. Then, at t0 + 2,
it becomes C again. From the time t0, all flows will experience
increasing phase and decreasing phase alternatively. Thus, we get
the following equations from algorithm 1 line 12 and 9:
Rn (t0 + 1) =(1 âˆ’ wn (t0))Rn (t0) + wn (t0)C
Rn (t0 + 2) =
Rn (t0 + 1)
C
(cid:80)N
i =1 Ri (t0 + 1)
1 âˆ’ 1
= 1
A(t0)
=
C
(cid:80)N
i =1 wi (t0)Ri (t0) +(cid:80)N
(1 âˆ’ wn (t0))Rn (t0) + wn (t0)C
i =1 wi (t0)
{(1 âˆ’ wn (t0))Rn (t0) + wn (t0)C}
N(cid:88)
wi (t0){1 âˆ’ Ri (t0)
C
}
i =1
A(t0) := 1 +
Solving the recurrence equations yields (for k > 0):
Rn (t0 + 2k ) â‰ˆAâˆ’k (t0)(1 âˆ’ wn (t0))k Â· Rn (t0)
Rn (t0 + 2k + 1) â‰ˆ(1 âˆ’ wn (t0 + 2k )) Â· Rn (t0 + 2k )
wn (t0) Â· C
+
A(t0) âˆ’ (1 âˆ’ wn (t0))
+ wn (t0 + 2k ) Â· C
The aggressiveness factor wn (t ) halves every two update periods
and eventually it converges to wmin (Algorithm 1 line 8, 11-12).
wn (t + 1) = wn (t ), wn (t + 2) = max(
1
2wn (t ), wmin )
Let us denote the time when wn (t ) have converged to wmin by tc ;
wn (tc + n) = wmin. Equation 3 and 4 still hold at tc :
Rn (tc + 2k ) =Aâˆ’k (tc )(1 âˆ’ wmin )k Â· Rn (tc ) + C
N
Rn (tc + 2k + 1) = (1 âˆ’ wmin )Rn (tc + 2k ) + wminC
Because 0  1 and C > 0, A(t0) > 1
and 0.5 â‰¤ (1 âˆ’ wmin ) < 1. Thus, we get:
Rn (tc + 2k ) â†’ C
N
TimeThroughputð‘…ð‘…âˆ—ð‘Ÿð‘Ÿoð’Žð’Žð’Žð’Žð’Žð’Ž_ð’“ð’“ð’Žð’Žð’“ð’“ð’“ð’“ð‘µð‘µâ€¦ð‘Ÿð‘Ÿcð‘Ÿð‘Ÿð‘Ÿð‘Ÿð‘Ÿð‘Ÿð‘¡ð‘¡ð‘Ÿð‘Ÿð‘Ÿð‘Ÿ_ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ð‘™ð·ð·âˆ—ð·ð·(ð‘Ÿð‘Ÿð‘œð‘œ+1)Credit sending rateBandwidth allocationð‘Ÿð‘Ÿo+1Credit-Scheduled Delay-Bounded
Congestion Control for Datacenters
SIGCOMM â€™17, August 21-25, 2017, Los Angeles, CA, USA
(a) ExpressPass
(b) DCTCP
Figure 13: Convergence behavior (Testbed)
(a) Utilization (Testbed)
(b) Utilization (ns-2)
(c) Fairness (Testbed)
(d) Fairness (ns-2)
(a) Credit processing dealy mea-
sured at host
(b) Inter-credit gap before/after
passing the switch
Figure 14: Host and switch performance (Testbed)
The median is about 0.38 Âµs, but 99.99th percentile is large at 6.2 Âµs.
We believe a hardware implementation will have a much smaller
variance, reducing the buffer requirement for ExpressPass. RDMA
NICs implementing iWARP exhibit a maximum delay spread (jitter)
of 1.2 Âµs [18]. In addition, all the core components of ExpressPass,
including pacers [5], rate-limiters [48, 49, 58], and credit-based flow
control, have been implemented on NIC. Figure 14 (b) shows the
inter-credit gap at transmission at the sender and reception at the
receiver. We timestamp credit packets at SoftNIC right before trans-
mission and right after the reception. We observe that the jitter of the
inter packet gap is relatively small (within 0.7 Âµs) which implies that
software implementation is efficient enough to generate and receive
credit packets at 10 Gbps.
Convergence characteristics: To demonstrate ExpressPass works
end-to-end, we run a simple test with five flows that arrive and depart
over time. Figure 13 shows the throughput averaged over 10 ms and
the queue size measured at the switch every 30 ms. ExpressPass
achieves a much more stable steady state behavior and exhibits
a very small queue. The maximum ExpressPass data throughput
is 94.82% of the link capacity because 5.18% of the bandwidth is
reserved for credit. The maximum queue size observed was 18 KB
for ExpressPass and 240.7 KB for DCTCP. The maximum credit
queue size of ExpressPass was only 672 B (8 packets).
6 EVALUATION
We evaluate three key aspects of ExpressPass using testbed experi-
ments (Section 6.1) and ns-2 simulations [39] (Section 6.1 - 6.3):
(1) We evaluate the flow scalability of ExpressPass in terms of
convergence speed, fairness, utilization, and queuing.
(2) We measure the effectiveness of ExpressPass under heavy incast.
(3) We quantify the benefits and the trade-offs of ExpressPass using
realistic datacenter workloads.
(e) Queue length (Testbed)
(f) Queue length (ns-2)
Figure 15: Queue length / fairness / utilization with many con-
current flows
6.1 Microbenchmark
In this section, using simple synthetic microbenchmark we verify
whether ExpressPass behaves as designed, quantify its benefits, and
compare it against the DCTCP [3] and the RCP [23].
Flow scalability: Flow scalability is an important problem because
datacenters have very small BDP on the order of 100 packets. Yet,
it needs to support various scale-out workloads that generate thou-
sands of concurrent connections [25]. To test the flow scalability of
ExpressPass, we run a simple experiment using a dumbbell topology
where N pairs of sender and receiver share the same bottleneck link.
We vary the number of flows from 4 to 256 (testbed) and 1024 (ns-2),
and measure utilization, fairness and queuing. Note, these flows are
long running flows whose arrival times are not synchronized. We
experiment with both testbed and simulation for cross-validation.
For testbed experiments, we use 12 machines to generate traffic,
where each sender may generate more than one flow.
First, we measure the utilization. ExpressPass achieves approxi-
mately 95% of utilization due to the reserved bandwidth for credits.
DCTCP achieves 100% utilization in all cases. RCP has under-
utilization beyond 256 flows. In the testbed, DCTCP shows slightly
lower utilization when the number of flows is small. We suspect high
variation in kernel latency as the cause.
Second, we measure the fairness to evaluate how fairly bandwidth
is shared across flows. We compute the Jainâ€™s fairness index using
the throughput of each flow at every 100 ms interval and report the
average. With a large number of flows DCTCPâ€™s fairness drops
significantly. Because DCTCP cannot handle a congestion window
of less than 1, some flows time out and eventually collapse. In
contrast, both ExpressPass and RCP achieve very good fairness.
0246810ThroughputGbps01002003000246810QueueTime (s)KB0246810ThroughputGbps01002003000246810QueueTime (s)KB00.20.40.60.8100.511.522.53CDFInter credit gap (us)NICIdeal00.20.40.60.8101234CDFInter credit gap (us)TXRXIdeal(cid:3399)0.7us024681041664256UtilizationConcurrent FlowsExpressPassDCTCPGbps0246810416642561024UtilizationConcurrent FlowsExpressPassDCTCPRCPGbps00.20.40.60.8141664256Concurrent FlowsFairness Index00.20.40.60.81416642561024Concurrent FlowsFairness Index010020030040041664256Max. QueueConcurrent FlowsKB0100200300400416642561024Max. QueueConcurrent FlowsKBMax. QueueDCTCP KSIGCOMM â€™17, August 21-25, 2017, Los Angeles, CA, USA
Inho Cho, Keon Jang, and Dongsu Han
(a) ExpressPass @ 10G (b) ExpressPass @ 10G (c) ExpressPass @ 100G (d) ExpressPass @ 10G (e) ExpressPass @ 100G
(f) DCTCP @ 10G
(g) DCTCP @ 10G
(h) DCTCP @ 100G
(i) RCP @ 10G
(j) RCP @ 100G
Figure 16: Convergence time of ExpressPass, DCTCP and RCP at 10/100 Gbps bottleneck link
Finally, Figure 15 (e) and (f) show the maximum queue occu-
pancy at the bottleneck as the number of flows increases. Express-
Pass shows maximum queuing of 10.5 KB and 1.34 KB in the testbed
and ns-2 respectively. In contrast, DCTCPâ€™s max queue occupancy
increases with the number of flows. In our simulation, when the
maximum queue hits the queue capacity, packet drop occurs. The
experimental results show a similar trend. DCTCPâ€™s congestion con-
trol starts to break around 64 flows. When the number of concurrent
flows is larger than 64, most flows stay at the minimum congestion
window of 2 because the queue size is always larger than the mark-
ing threshold. However, note the maximum queue length for 16 and
32 flows are higher than that of 64 flows. For 16 and 32 flows, some
flows may occasionally ramp up when they do not get an ECN signal,
which builds up the queue. RCP exceeds the queue capacity and
packet drop occurs even with 32 flows. This is because RCP assigns
the same rate for a new flow as existing flows when new flows start.
Fast convergence: Figure 16 shows the convergence behavior of
ExpressPass, DCTCP, and RCP over time. First, we use testbed ex-
periments to compare ExpressPass and DCTCP at 10 Gbps. Express-
Passâ€™s throughput is averaged over 25 Âµs and DCTCP is averaged
over 100 ms due to its high throughput variance. As shown in Fig-
ure 16 (a) and (b), ExpressPass converges 700x faster than DCTCP in
just 100 Âµs (four RTTs), while DCTCP took 70 ms to converge. Two
factors contribute to the difference. First, convergence is much faster
in ExpressPass than the DCTCP which performs AIMD. Second,
ExpressPass shows RTT of 10 Âµs at the minimum and 25 Âµs on aver-
age, measured in SoftNIC. On the other hand, DCTCPâ€™s feedback
loop runs much slower in the Linux kernel, which adds hundreds of
microseconds RTT variation [38].
Next, we use simulation to compare the congestion feedback
algorithm of ExpressPass and DCTCP in isolation. We compare
the convergence time of ExpressPass and DCTCP on two different
link speeds (10 Gbps and 100 Gbps). The base RTT is set to 100 Âµs.
Figure 16 (c) - (f) shows the flowsâ€™ throughput for ExpressPass
and DCTCP at each RTT 4. ExpressPass converges within 3 RTTs
4We set the DCTCP parameter K= 65, g= 0.0625 for 10 Gbps link, and K= 650, g=
0.01976 for 100 Gbps link. For ExpressPass, we report the average throughput for each
RTT. For DCTCP, we averaged over 10 RTT due to its high variance.
Figure 17: Shuffle workload (ns-2)
which is consistent with 4 RTTs in experiments. DCTCP takes more
than 80 times longer than ExpressPass with 10 Gbps link. As bot-
tleneck link capacity increases, the convergence time gap between
ExpressPass and DCTCP becomes larger. At 100 Gbps, Express-
Passâ€™s convergence time remains unchanged, while that of DCTCP
grows linearly to the bottleneck link capacity. Because of DCTCPâ€™s
additive increase behavior, its convergence time is proportional to
the bandwidth-delay product (BDP).
6.2 Heavy incast traffic pattern
One advantage of ExpressPass is robustness against incast traffic
patterns. Such traffic patterns commonly happen in the shuffle step