pair granularity) allows a simpler description of TMs with active
servers, without constraints on how each active server spreads
its traffic across destinations, and more directly captures the only
constraints in the hose model: the servers themselves. Accordingly,
throughout the following discussion, we shall refer to throughput
in terms of a fraction of the line-rate ∈ [0, 1].
A network shall be called throughput-proportional (TP), if when
built such that it achieves throughput α per server for the worst-case
TM, then it achieves throughput min{α/x, 1} per server for any
TM with only an x fraction of the servers involved. This definition
captures the intuition described above: as the number of servers
involved in communication decreases, a TP network can increase
per-server throughput proportionally2. Note that one can easily
map a fuzzier classification of servers as “hot” or not, to the strict
binary of “involved in the TM” or not.
1FireFly [18] used an abstract model to evaluate the benefit of dynamic network
links compared to a fat-tree based on performance improvements over random traffic
matrices. Our objective is very different: quantify how well a given static network
accommodates arbitrarily skewed traffic matrices. Firefly’s “Metric of Goodness” also
falls short of this: as prior work shows [20], bisection bandwidth can be a logarithmic
factor away from throughput, and this factor varies for topologies.
2We acknowledge the parallels with energy proportional networking [2, 19], but note
that EPN addresses a different question along the lines of which links to turn off, or
change the data rate on, etc. to reduce the network’s energy usage.
Figure 2: A throughput-proportional network would be able to distribute its
capacity evenly across only the set of servers with traffic demands.
This notion of a throughput-proportional network is illustrated
in Fig. 2, which also contrasts it with the fat-tree’s behavior. As
discussed in §2.1, for the fat-tree with a simple pod-to-pod TM, if a
fraction greater than β = 2/k of servers are involved, throughput
will be limited to α. As the fraction of servers involved drops below
β, throughput per server increases proportionally, hitting 1 only
when α fraction of the pod itself is involved.
As the fat-tree example illustrates, network bottlenecks may
prevent a network from achieving throughput proportionality.
But is such proportionality unattainable for other statically wired
networks as well? Below, we prove that for a generic statically
wired network, per-server-throughput cannot improve more than
proportionally, at least for certain classes of traffic matrices.
Permutation TMs. A permutation traffic matrix over k servers
involves each of these servers communicating with exactly one
other unique server. We prove the below result.
Theorem 2.1. Over the class of permutation TMs, throughput
in a static network cannot increase more than proportionally as the
fraction of servers involved in the TM decreases.
Theorem 2.1 follows from the following lemma.
Lemma 2.2. If, for some x ∈ (0, 1], G supports throughput t
for every permutation matrix over x fraction of the servers, then
it supports throughput xt for any permutation matrix over all servers.
Before diving into the proof of Lemma 2.2, we explain why it
indeed implies Theorem 2.1. Assume, for the point of contradiction,
that the statement of Theorem 2.1 is false and so throughput does
increase more than proportionally in α for some static network
G. Then, by the definition of throughput proportionality for some
fraction of servers x, the supported throughput for all permutation
TMs involving only an x-fraction of the servers is some β greater
. By Lemma 2.2, G supports all permutation TMs with
than α
x
throughput xβ > x α
= α, contradicting the definition of α as
x
the worst-case throughput across all permutation TMs for G. Now,
let us prove Lemma 2.2.
Proof. (of Lemma 2.2) Consider a permutation TM M over
all n servers in G. Since in any permutation TM, each server
communicates with exactly one other unique server, M comprises
n2 distinct communicating pairs of servers. Selecting xn2 of these
pairs and only considering the communication between these pairs,
Ideal for topology ﬂexibilityThroughput  per  serverFraction of servers with trafﬁc demand101(x, /x)Throughput proportionalFat-treeβSIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
S. Kassing et al.
Figure 3: An Xpander network with 486 24-port switches, supporting 3402 servers broken into 6 pods, each with 3 meta-nodes. Left: all switches are shown at
the circles’ circumferences; other nodes are cable-aggregators. Right: Floor plan for the same 6 pods. Each pod has 3 rows of racks, each being a meta-node. Each
meta-node’s 27 switches and their connected servers fit in 7 racks (circles) of 48 rack units each, after accounting for cooling and power.
gives rise to a permutation TM on an x-fraction of the servers, and
is thus supported (by the statement of the lemma) at throughput
t. There are precisely K =(cid:0) n/2
xn/2(cid:1) possible selections of such a set
The impossibility of exceeding throughput proportionality in static
networks (at least for certain families of TMs) makes it an idealized
benchmark for network flexibility towards skewed traffic: as traffic
consists of a smaller and smaller fraction of servers becoming
hotspots, how well does throughput scale compared to TP? §5
addresses this question experimentally.
3 State-of-the-art static network topologies
The Helios authors’ assessment of the inflexibility of statically-
wired networks (noted in §1) was likely accurate at the time, but in
the intervening years, a large number of new static topologies
have been proposed for data centers, including Jellyfish [31],
Longhop [32], Slimfly [9], and Xpander [33]. All of these have
a common feature: unlike the Fat-tree [3], they do not break the
network into layers, instead retaining a flat structure, where top-of-
rack switches are directly wired to each other. However, there are
sizable differences in performance even across flat topologies [20].
In particular, Jellyfish and Xpander achieve near-identical, high
performance (modulo a small variance for Jellyfish due to its
randomness), due to their being good expander graphs [33]. We
shall refer to these topologies as expander-based networks as we
expect results to be similar for other network designs based on
near-optimal expander graphs, such as LPS [25, 33].
For readers concerned by Jellyfish’s randomness, we note that
Xpander is deterministic, and provides symmetry and an intuitive
notion of inter-connected clusters of racks (“meta-nodes” and pods),
enabling clean aggregation of cables into a small number of bundles.
As noted in [29], such bundling can “reduce fiber cost (capex + opex)
by nearly 40%”. We illustrate these desirable properties in Fig. 3
for an Xpander configured to cost 33% less than a full-bandwidth
fat-tree with k=24. For a more detailed exposition including cable
counts and lengths, we refer readers to the Xpander paper [33].
of communicating pairs. If we scale the throughput of each of the
flows in such a TM to 1
, the utilization of any link in G over this
K
scaled-down TM is now at most 1
. The scaled-down throughput
K
of all TMs induced by choosing xn2 of the communicating pairs in
M can thus be supported simultaneously. Observe, however, that
every communicating pair in M appears in precisely(cid:0) n/2−1
xn/2−1(cid:1) such
selections, and so its total achieved throughput should be scaled up
by this factor. As the number of servers in the network, n, increases,
the resulting throughput converges to xt.
□
Following the strategy for the above proof for the family of
permutation TMs, we have also been able to prove analogous results
for several other TM families of interest, including all-to-all, many-
to-one, and one-to-many TMs. However, we can only conjecture on
a more general result over the larger class of hose TMs as follows:
Conjecture 2.3. Throughput in a static network cannot increase
more than proportionally as the fraction of active servers decreases.
One strategy for proving this stronger result would be to
prove, as conjectured below, that permutations are worst case
TMs. Such a result, combined with Lemma 2.2, would then prove
Conjecture 2.3. Proving that permutation traffic matrices are a
corner case would also be of independent interest — as prior work
notes, the complexity of finding worst-case TMs for arbitrary
networks is unknown [20] and such a result could be significant
step in that direction.
Conjecture 2.4. Given a static network G with N servers and an
arbitrary TM M for which G achieves throughput t per server, there
exists a permutation TM P for which G achieves throughput ≤ t.
Meta-node cable aggregatorPodToRPod cable aggregatorMeta-node cable aggregatorPodToRPod cable aggregatorMeta-node cable aggregatorPodToRPod cable aggregatorMeta-node cable aggregatorPodToRPod cable aggregatorMeta-node cable aggregatorPodToRPod cable aggregatorMeta-node cable aggregatorPodToRPod cable aggregatorMeta-node (aggregator)PodSwitches / racksPod-cabler (aggregator)Beyond fat-trees without antennae, mirrors, and disco-balls
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
SR transceiver
Optical cable ($0.3 / m)
ToR port
ProjecToR Tx+Rx
DMD
Mirror assembly, lens
Galvo mirror
Total
Static
$80
$45
$90
-
-
-
-
$215
FireFly
$80
-
$90
-
-
-
$200
$370
ProjecToR
-
-
$90
$80 to $180
$100
$50
-
$320 to 420
Table 1: Cost per network port for static and recent dynamic networks.
Component costs are from ProjecToR [13]. Each cable in a static network
is accounted for with 300 meter length, with its cost shared over its two ports.
The throughput of expander-based topologies on largely uniform
workloads or/and fluid-flow models has been evaluated before, but
it remains unclear whether (a) these claims extend to the skewed
workloads that dynamic networks target; and (b) the claimed
throughput advantage can be translated into low flow completion
times. Sections §5 and §6 address these questions.
4 Dynamic network topologies
Instead of delving into the details of any one of the myriad dynamic
network designs [10, 12–14, 17, 18, 24, 27, 35, 40], we endeavor to
tackle an abstract model that covers, with reasonable fidelity, the
existing proposals, as well as similar future extensions.
In a generic dynamic network, each ToR switch may have a
certain number of flexible ports, say k, which can be connected
to available flexible ports on other ToRs. The various proposed
realizations of this approach differ in the connectivity options they
make available, the reconfiguration time needed to change the
connectivity of flexible ports, the per-port cost, and the algorithms
used to configure the interconnect in real-time. For greatest
flexibility, we disregard constraints that limit connectivity, and
allow any ToR to connect to any other ToR. Further, our focus is
on the potential performance of dynamic topologies, and thus we
ignore algorithmic inefficiencies to the largest extent possible. We
nevertheless mention two factors that can have a large impact on
the performance of dynamic networks:
Direct-connection heuristics: The numerous dynamic designs
referenced above, all prioritize direct connections between ToR-
pairs with traffic demand. While FireFly [18] in general uses multi-
hop relaying, direct connections between pairs of communicating
racks are prioritized by its heuristics. As we shall see, such heuristics
can impair the potential of dynamic topologies.
Buffering: The limited number of flexible ports, together with the
reconfiguration time needed to move their connectivity, implies that
dynamic topology proposals need to buffer packets until a suitable
connection is available, thus adding latency. If such buffering is not
feasible, e.g., due to most flows being short and latency-sensitive, it
becomes necessary to carry traffic over multiple hops.
We do not believe that any past proposal addresses these issues
in entirety, but perhaps these problems can be fully addressed by
future proposals, so we model two alternatives: (a) an “unrestricted”
optimal design unaffected by both factors; and (b) a “restricted”
design that suffers from both, i.e., picks the ToR-level topology
Figure 4: This topology provides full throughput to all active servers, where
any scheme in the restricted dynamic model could not.
prioritizing direct connections between communicating ToRs, and
requires multi-hop connectivity in cases where the TM is such
that not all communicating pairs can be concurrently connected by
direct connections.
A more realistic abstraction of dynamic networks would lie
somewhere between these two extremes, but would need greater
machinery to capture the waiting time involved in dynamic
connectivity. The utility of the restricted model is in developing
intuition about the role of buffering and the time spent waiting for
dynamic connectivity in dynamic networks.
Lastly, it is worth noting that the flexibility of dynamic topologies
comes at a substantially higher per-port cost for the flexible ports.
The cost advantage of dynamic designs over fat-trees stems from
using fewer ports. However, this is also true of expander-based
networks. For equal-cost comparisons, networks should thus be
configured with the same total expense on ports. We shall denote
as δ, the cost of a flexible port normalized to that of a static port,
with the cost of a static port including half of a 300 meter cable’s
cost. Based on component costs for ProjecToR and FireFly (Table 1),
the lowest estimates imply δ = 1.5. Therefore, for supporting the
same number of servers, a dynamic network can only buy at most
0.67× the network ports used by an equal-cost static network. Of