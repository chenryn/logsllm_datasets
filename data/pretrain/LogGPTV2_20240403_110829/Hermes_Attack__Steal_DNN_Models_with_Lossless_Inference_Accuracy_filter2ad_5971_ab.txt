asynchronous operations. This situation is even worse
in the more advanced GPU architectures (e.g., NVIDIA
Geforce RTX 2080 Ti) because of introducing new
features to unify GPU device and host memory.
Attack Overview. The methodology of our attack can be
divided into two phases: ofﬂine phase and online phase. Dur-
ing the ofﬂine phase, we use white-box models to build a
database with the identiﬁed command headers, the mappings
between GPU kernel (binaries) and DNN layer types, and
Figure 4: Threat Model. We consider the model privatization
environment, where the host and the GPU device are well
protected individually, and the PCIe bus is the new attack
surface. The adversary can snoop the PCIe trafﬁc using a bus
snooping device, e.g., a PCIe protocol analyzer.
dedicated lines, with ﬂow control, error detection and re-
transmissions. The underlying communications mechanism
of PCIe protocol is composed of three layers: Transaction
Layer, Data Link Layer, and Physical Layer. Figure 2 and
Figure 3 show the formats of memory read request Trans-
action Layer Packet (TLP) and completion TLP with 64-bit
addressing. The header of each TLP is four double words
(DWs) long, and the maximum payload size is 128 DWs.
When a CPU writes data into a peripheral, the chipset
generates a memory write packet which consists of a 32-
bit header and a payload containing the data to be written.
The packet is then transmitted to the chipset’s PCIe port.
The peripheral can be connected directly to the chipset or
connected to a switch network.
When a CPU reads data from a peripheral, there are two
packets involved in the read operation. One is read request
TLP that is sent from CPU to the peripheral, asking the latter
to perform a read operation, as shown in Figure 2. The other
one is completion TLP which comes back with data in the
payload, as shown in Figure 3. The completion TLP and
request TLP can be identiﬁed by the same Tag value.
3 Attack Design
3.1 Overview
Threat Model. In this paper, we consider an AI model priva-
tization deployment environment (e.g., smart IoT, surveillance
devices, autonomous driving), where service providers pack
their private AI models into heterogeneous CPU-GPU devices
and sell them to third-party customers with subscription or per-
petual licensing. The end-users are able to physically access
the hardware, especially, the PCIe interface. The thread model
is depicted as Figure 4, where the GPU is attached to the host
1976    30th USENIX Security Symposium
USENIX Association
Figure 5: Attack Overview. The ofﬂine phase builds a knowledge database by identifying GPU command headers of interest,
the mappings between GPU kernel (binaries) and DNN layer types, and the mappings between GPU kernels and offsets of
hyper-parameters. The online phase is the actual deployed attack to steal the victim model during inference. Three major modules
are used in both phases but with different sub-components activated (grey diagrams indicate inactivity): The trafﬁc processing
1(cid:13) sorts out-of-order PCIe packets; The extraction module 2(cid:13) extracts and ﬁlters GPU commands of interest; The
module
reconstruction module 3(cid:13) fully reconstructs the semantics, architecture, hyper-parameters, and parameters.
the mappings between GPU kernels and offsets of hyper-
parameters. Speciﬁcally, the trafﬁc processing module ( 1(cid:13) in
Figure 5) sorts the out-of-order PCIe packets intercepted by
PCIe snooping device. The extraction module ( 2(cid:13)) has two
sub-modules: header extraction module and command extrac-
tion module. The header extraction module extracts command
headers from the sorted PCIe packets (Section 3.3.1). The
extracted command headers will be stored in the database,
accelerating command extraction in the online phase. The
command extraction module in the ofﬂine phase helps get
the kernel binaries (Section 3.3.2). The semantic reconstruc-
tion module within the reconstruction module ( 3(cid:13)) takes the
inputs from the command extraction module and the GPU
proﬁler to create the mappings between the kernel (binary)
and the layer type, as well as the mappings between the kernel
and the offset of hyper-parameters, facilitating the module
reconstruction in the online phase (Section 3.4.1).
During the online phase, the original (victim) model is
used for inference on a single image. The victim model is a
black-box model and thoroughly different from the white-box
models used in the ofﬂine phase. PCIe trafﬁcs are intercepted
and sorted by the trafﬁc processing module. The command
extraction module ( 2(cid:13)) extracts K (kernel launch related) and
D (data movement related) commands as well as the GPU
kernel binaries, using the header information proﬁled from the
Figure 6: Process of Sorting PCIe Trafﬁc. We sort the pack-
ets using packet ID and tags, instead of the capture order.
ofﬂine phase (Section 3.3.2). The entire database are feed to
the model reconstruction module ( 3(cid:13)) to fully reconstruct ar-
chitecture, hyper-parameters, and parameters (Section 3.4.2).
All these steps need massive efforts of reverse engineering.
3.2 Trafﬁc Processing
The intercepted trafﬁc is composed of TLPs with unique
packet IDs. Thanks to the oriented interception, the inter-
USENIX Association
30th USENIX Security Symposium    1977
OriginalModel1.Command Headers2.3. GPU Proﬁler Online 	PhaseGeneratedModelTrafﬁcProcessingWhile-boxModels1RawTrafﬁcSortedTrafﬁcTrafﬁcProcessing1Offline	PhaseExtractionHeader Extraction2PCIe InterceptorCommandExtractionDatabase2Model ReconstructionSemantic ReconstructionReconstruction33SemanticReconstructionReconstructionModel ReconstructionArchitectureHyper-ParametersParametersHeader ExtractionExtraction2KCommandsDCommandsTrafﬁcCommand ExtractionPCIe Interceptor10010110210310131517XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXP2790xXXXX800xXXXX810xXXXX820xXXXX17102918Packet  IDTag1000xXXXX0xXXXX1020xXXXX1030xXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX807965110SortKeyData PacketsRequest PacketsCompletion PacketsP1P3101cepted trafﬁc is only formed by packets transmitted between
CPU and GPU. These packets are arranged increasing ID val-
ues in order of arrival. Packets can be classiﬁed into upstream
packets and downstream packets based on the transmitting
direction. The upstream packets represent packets that are
sent from GPU to CPU, e.g., GPU read request packets, or
completion packets returning GPU computing results. The
downstream packets are sent from CPU to GPU, e.g., CPU
read request packets, completion packets with input data. The
structures of two representative packages are shown as Fig-
ure 2 and Figure 3. To make things easier, we only keep the
GPU read request packets in the upstream packets and the
completion packets in the downstream packets.
In addition to the aforementioned two types of packets, we
use another type of packet namely data packet that is merged
from request packets and completion packets according to the
tag ﬁeld. A data packet comprises both the request address
and corresponding acquired data in a single packet. It can be
concatenated to a completion packet with the same packet ID
and equivalent order.
The major challenge here is that these data completion
packets arrived out-of-order. The reason is that the PCIe pro-
tocol does not enforce the completion orders of multiple con-
secutive requests. Additionally, resultant output for a single
PCIe read request may be encapsulated in multiple comple-
tion packets, making the raw packets hard to analyze directly.
To tackle the problem, we coalesce the raw packets by using
merge and sort based on two observations: (1) every request
is composed of one request packet and one (multiple) comple-
tion packet(s), where the orders of request packets can reﬂect
the correct sequence; (2) completion packets for the same
request are guaranteed to arrive in order. We elaborate the
merge and sort operations as follows:
Merge: For every data packet, we complement the tag ﬁeld by
looking up its corresponding completion packet(s). If adjacent
packets have the same tag value, we merge them into a single
packet by concatenating their data ﬁeld.
Sort: The sort phase is illustrated as Figure 6. By default, all
the packets are arranged according to their packet IDs from
low to high. For request packet, we record it as P1 and lookup
all the completion packets that have larger packet IDs than
P1. We stop the searching when it hits the packet that has the
same tag value as P1 and records this packet as P2. Next, we
look for the packet that has the same packet ID with P2 in data
packets and records it as P3. Then we add the packet ID of
P1 into P3 as a sort key. We repeat this procedure until every
data packet has a sort key. At last, we sort all the data packets
by on the sort key.
Figure 7: Identiﬁed Structure of GPU Commands. A typ-
ical GPU command consists of nine DWs. The third DW
indicates the location of this command on GPU memory. The
ﬁfth DW represents the size of data ﬁeld. The last DW stands
for the type of this command.
instance, making inference on a single image using MNIST
model will generate 1,077,756 data packets (after ﬁltering) on
NVIDIA Geforce GT 730. However, only around 20,000 of
them (2%) are useful for our attack. This may be explained by
the fact that CPU sends GPU numerous signals to do initial-
ization, synchronization, etc. So it is necessary to ﬁlter out the
irrelevant packets. In order to focus on our goal of extracting
DNN models, it is sufﬁcient to pick only those D commands
and K commands, representing data movement commands
and kernel launch commands, respectively.
3.3.1 Header Extraction
To extract D commands and K commands, we should identify
the header structure of each kind of command. This procedure
is done in our ofﬂine proﬁling phase. In order to ﬁgure out the
header of D commands, we repeatedly move crafted data be-
tween pre-allocated GPU device memory and main memory,
and use pattern match on the intercepted PCIe packets. Sim-
ilarly, we repeatedly launch multiple kernels with prepared
arguments, to identify the header structure of K commands.
According to our reverse-engineering results, a K com-
mands header structure is shown as the highlighted nine DWs
in Figure 7, where the third, ﬁfth, and ninth DW represents
GPU memory address, data size in bytes, and command type
(e.g., 6D204860 is the signature indicating kernel launch
on Kepler architecture), respectively. These three DW ﬁelds
are most useful for our attack. The other six DWs are GPU-
speciﬁc signatures whose bit-wise semantics are explained in
previous reverse engineering work [19].
We also did exhaustive tests to verify that the header struc-
ture is stable and valid on different GPU and machine com-
binations. The extracted header information are memorized
in our proﬁling database, which can be used to accelerate
analysis in the future.
3.3 Extraction
After the preliminary processing, it’s still onerous to recon-
struct the model from the trafﬁc. One of the main obstacles
is that there are a large number of interference packets. For
3.3.2 Command Extraction
Raw extracted commands are not ready to use because
of tremendous noises. Noise can be classiﬁed into two
classes: external noise and internal noise. External
1978    30th USENIX Security Symposium
USENIX Association
 01000000 6C200120 41000000 6D204860 XXXXXXXX XXXXXXXX......XXXXXXXX 62200220 0B000000 60182EBA 60200220 20000000: Command Header 00000008:292B7F0000000008:292B7F80PayloadsAddressFigure 8: Example of Command With Noise Packets. The
noise packet is not consecutive with the previous packet in
terms of address.
noise refers to those packets not belong to the current com-
mand. They can both be the packets of other commands or
meaningless packets. External noise could appear frequently
because a command with a large data ﬁeld may require thou-
sands of packets to transmit. Since a command header could
be sent via two packets, the noise packet may also appear
within the command header. As Figure 8 shown, a command
header is split into two parts. They are transmitted via two
packets, with a noise packet in between. Internal noise indi-
cates a speciﬁc DW inside each packet. We have observed all
internal noise and summarized the pattern of it. Thus internal
noise can be easily ﬁltered out while extracting the payloads.
An intuitive solution to address the noise issue is to check
the address continuity, based on the fact that the transmitted
data is usually consecutive in memory space. If a packet’s
memory address is not consecutive with its predecessor, it is
highly likely that this packet does not belong to the current
command. However, this is not always the case especially
when the continuous memory space is insufﬁcient. Since the
addresses in packets are physical addresses, virtually con-
tiguous address space used by CUDA programs may be split
into multiple physical memory chunks. Figure 9 shows an
example that the addresses of two adjunct packets belong
to the same command are nonconsecutive in physical ad-
dress. Therefore, it is insufﬁcient to merely check the address
continuity. To solve this problem, we introduce a heuristic
threshold MAX_SCAN_DISTANCE. When a packet encounters
an address gap, we scan for the next consecutive packet within
MAX_SCAN_DISTANCE. If there exists a packet that has a con-
secutive address with the previous address gap, we consider
this packet to be the adjacent packet of the gap and discard
the previously scanned packets. Otherwise, we include the
gap packet into the payloads. We continue this process until
the number of payloads bytes in extracted packets matches
the size indicated in the command header.
3.4 Reconstruction
3.4.1 Semantic Reconstruction
Semantic reconstruction is a part of the ofﬂine proﬁling phase
to build the knowledge database. We use known DNN models
as ground truth and utilize NVIDIA’s proﬁling tools (i.e.,
Figure 9: Example of Command With Large Data Field.
When a command has a large size data ﬁeld, it occupies more
than one continuous memory space. In this case, the address
gap also exists.
nvprof [13]) to bridge the semantic gap between PCIe packets
and high-level DNN workﬂow by: (1) associating kernels
with DNN layers; (2) proﬁling the layout of the arguments of
certain GPU kernels.
We assume every computational layer (e.g., convolution
layer, normalization layer, rectiﬁed linear unit layer) of DNN
models is computed on the GPU, because layers that are com-
puted by CPU would not send command through PCIe. This
assumption is reasonable because the highly muti-threaded
architecture of GPU is designed to accelerate matrix computa-
tion in DNN layers. Moreover, if some of intermediate layers
are ported to CPU, the data movement is expensive. Base
on this assumption, it is safe to say each layer is associated
with one or more GPU kernels. Different types of layers use
different GPU kernels, thus we can infer the layers types by
identifying their GPU kernels. Additionally, people prefer
to use highly optimized standard libraries provided by GPU
hardware vendors (e.g., NVIDIA’s CUDNN library), so the
kernel binaries are relatively stable. For example, convolu-
tion layers call convolve_sgemm() kernels whose binaries are
embedded in nv_fatbin section of libcudnn.so.
We have the following two observations based on our pre-
liminary experiments:
Observation 1: Each kernel is loaded onto GPU using a D
command, and its data ﬁeld is kernel binaries.
Observation 2: Each K command includes an address refer-
ring to the kernel binary to be launched.
Based on the two observations, we can extract all involved
kernel binary by iterating K commands. Figure 10 illustrates
how we use a K command to locate the GPU kernel binary.
Particularly, the kernel binary is ﬁrst loaded onto GPU mem-
ory and stored at 405ECF01 using a D command, and then
launched by a K command. Our method works in reverse or-
der: we ﬁrst retrieve the K command’s data ﬁeld with a ﬁxed
offset to locate the address referring to the kernel binary, then
we dump the corresponding D command’s data ﬁeld to get
the kernel binary.
USENIX Association
30th USENIX Security Symposium    1979
 01000000 6C200120 41000000 6D204860 XXXXXXXX XXXXXXXX......XXXXXXXX 62200220 0C7F0000 60182EBA 60200220 20000000: Command Header 00000008:292B7F0000000008:292B7F80PayloadsAddress...XXXXXXXX XXXXXXXX XXXXXXXX  XXXXXXXX XXXXXXXX...00000008:188B6700: NoiseGap...XXXXXXXX XXXXXXXX XXXXXXXX XXXXXXXX XXXXXXXX...00000008:292B7F00Consistent00000008:292B7F80AddressPayloads62200220 0C7F0000 60182EBA ... 6C200120 41000000 6D204860...XXXXXXXX XXXXXXXX XXXXXXXX XXXXXXXX XXXXXXXX......XXXXXXXX XXXXXXXX XXXXXXXX XXXXXXXX XXXXXXXX...00000008:188B670000000008:188B6780...XXXXXXXX XXXXXXXX XXXXXXXX XXXXXXXX XXXXXXXX...00000008:292BB880...Gap:  Command HeaderFigure 10: Process of Locating Kernel Binaries. The ﬁrst
command in the ﬁgure is a D command that loads a kernel
binary onto GPU. The second command is a K command to
launch the loaded kernel. These two commands are associated
by the same GPU address where the kernel binary is loaded.
After iterating all involved K command in PCIe trafﬁc,
we have a sequence of kernel binaries in launch order. By
aligning with the CUDA trace collected by nvprof, we can
ﬁgure out the mappings between each kernel binary and its
corresponding layer. The mappings are stored in the form of
tuples in a hash table, where the key is the kernel binary and
the value is layer type.
Another semantic we need to reconstruct is the relationship
between kernel binaries and their arguments layout. We only
focus on the kernels that involves potential hyper-parameters.
Since hyper-parameters are not parts of the trained model,
they are only used in certain kernels as arguments. By ﬁg-
uring out the locations of hyper-parameters in K commands,
we can extract all involved hyper-parameters. We achieve
this by proﬁling known DNNs, looping over the data ﬁeld
of certain kernels’ K commands to ﬁnd the expected hyper-
parameters. The  pairs
are recovered and stored in the knowledge database.
3.4.2 Model Reconstruction
Extract Model Architecture. In the online phase, after in-
tercepting all PCIe trafﬁc, we are able to obtain all needed K
and D commands. The key idea of reconstructing DNN archi-
tecture is to build data ﬂow graph where each data movement
indicates an edge and every kernel launch represents a vertex.
Every kernel takes at least one address as its input and
write its output to one or more addresses. By knowing the
semantics of this kernel in proﬁling phase, in the form of K
command, we are able to ﬁgure out which offset(s) indicate
input(s) and output(s). We build the data ﬂow graph majorly
by treating the input addresses as ﬂow-from and the output
addresses as ﬂow-to. All kernels are then associated with
these data addresses. We note that in the data ﬂow graph one
kernel’s output address does not necessarily exactly match
its successor’s input address. Because these two addresses
can be within the same data block or data is copied from one
address to the other, which can be determined by iterating D
Figure 11: Process of Locating Parameters. The ﬁrst com-
mand is a D command of loading parameters onto GPU. The
second command is a KD2D command which copies param-
eters to a new location. The third K command launches a
kernel taking the address of duplicated data as the input. Our
attack recovers the parameters in reverse order as depicted by
the arrows.
commands. Once the data ﬂow graph is reconstructed, we can
substitute every kernel vertices with their corresponding DNN
layers by querying the mappings in the knowledge database.
Extract Hyper-parameters. The next step is to extract
hyper-parameters that are used during inference, e.g., strides,
kernel size. Hyper-parameters that are used to control train-