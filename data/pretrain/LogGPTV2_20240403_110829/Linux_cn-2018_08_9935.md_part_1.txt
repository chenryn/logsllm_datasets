---
author: Julia Evans
category: 技术
comments_data: []
count:
  commentnum: 0
  favtimes: 0
  likes: 0
  sharetimes: 0
  viewnum: 5163
date: '2018-08-20 13:00:38'
editorchoice: false
excerpt: 以更慢的速率发送信息包实际上可能会带来更好的性能（即便你是在整个传输过程中，这样做的唯一的人），下面是原因
fromurl: https://jvns.ca/blog/2018/07/12/netdev-day-2--moving-away-from--as-fast-as-possible/
id: 9935
islctt: true
largepic: /data/attachment/album/201808/20/130017eprhpqq0nz0ronkr.jpg
permalink: /article-9935-1.html
pic: /data/attachment/album/201808/20/130017eprhpqq0nz0ronkr.jpg.thumb.jpg
related: []
reviewer: wxy
selector: ''
summary: 以更慢的速率发送信息包实际上可能会带来更好的性能（即便你是在整个传输过程中，这样做的唯一的人），下面是原因
tags:
- BBR
- TCP
thumb: false
title: netdev 第二天：从网络代码中移除“尽可能快”这个目标
titlepic: true
translator: FSSlc
updated: '2018-08-20 13:00:38'
---
![](/data/attachment/album/201808/20/130017eprhpqq0nz0ronkr.jpg)
嗨！今天是 netdev 会议的第 2 天，我只参加了早上的会议，但它*非常有趣*。今早会议的主角是 [Van Jacobson](https://en.wikipedia.org/wiki/Van_Jacobson) 给出的一场名为 “从尽可能快中变化：教网卡以时间”的演讲，它的主题是关于互联网中拥塞控制的未来！！！
下面我将尝试着对我从这次演讲中学到的东西做总结，我几乎肯定下面的内容有些错误，但不管怎样，让我们开始吧！
这次演讲是关于互联网是如何从 1988 开始改变的，为什么现在我们需要新的算法，以及我们可以怎样改变 Linux 的网络栈来更容易地实现这些算法。
### 什么是拥塞控制？
在网络上的任何成员总是无时无刻地发送信息包，而在互联网上的连接之间有着极其不同的速度（某些相比其他极其缓慢），而有时候它们将被塞满！当互联网的一个设备以超过它能处理的速率接收信息包时，它将丢弃某些信息包。
你所能想象的最天真的发送信息包方式是：
1. 将你必须发送的信息包一次性发送完。
2. 假如你发现其中有的信息包被丢弃了，就马上重新发送这些包。
结果表明假如你按照上面的思路来实现 TCP，互联网将会崩溃并停止运转。我们知道它会崩溃是因为在 1986 年确实发生了崩溃的现象。为了解决这个问题，专家发明了拥塞控制算法 —— 描述如何避免互联网的崩溃的原始论文是 Van Jacobson 于 1988 年发表的 [拥塞避免与控制](https://cs162.eecs.berkeley.edu/static/readings/jacobson-congestion.pdf)（30 年前！）。
### 从 1988 年后互联网发生了什么改变？
在演讲中，Van Jacobson 说互联网的这些已经发生了改变：在以前的互联网上，交换机可能总是拥有比服务器更快的网卡，所以这些位于互联网中间层的服务器也可能比客户端更快，并且并不能对客户端发送信息包的速率有多大影响。
很显然今天已经不是这样的了！众所周知，今天的计算机相比于 5 年前的计算机在速度上并没有多大的提升（我们遇到了某些有关光速的问题）。所以我想路由器上的大型交换机并不会在速度上大幅领先于数据中心里服务器上的网卡。
这听起来有些糟糕，因为这意味着客户端更容易在中间层的连接中达到饱和，而这将导致互联网变慢（而且 [缓冲膨胀](https://apenwarr.ca/log/?m=201101#10) 将带来更高的延迟）。
所以为了提高互联网的性能且不让每个路由上的任务队列都达到饱和，客户端需要表现得更好并且在发送信息包的时候慢一点。
### 以更慢的速率发送更多的信息包以达到更好的性能
下面的结论真的让我非常意外 —— 以更慢的速率发送信息包实际上可能会带来更好的性能（即便你是在整个传输过程中，这样做的唯一的人），下面是原因：
假设你打算发送 10MB 的数据，在你和你需要连接的客户端之间有一个中间层，并且它的传输速率*非常低*，例如 1MB/s。假设你可以辨别这个慢连接（或者更多的后续中间层）的速度，那么你有 2 个选择：
1. 一次性将这 10MB 的数据发送完，然后看看会发生什么。
2. 减慢速率使得你能够以 1MB/s 的速率传给它。
现在，无论你选择何种方式，你可能都会发生丢包的现象。所以这样看起来，你可能需要选择一次性发送所有的信息包这种方式，对吧？不！！实际上在你的数据流的中间环节丢包要比在你的数据流的最后丢包要好得多。假如在中间环节有些包被丢弃了，你需要送往的那个客户端可以察觉到这个事情，然后再告诉你，这样你就可以再次发送那些被丢弃的包，这样便没有多大的损失。但假如信息包在最末端被丢弃，那么客户端将完全没有办法知道你一次性发送了所有的信息包！所以基本上在某个时刻被丢弃的包没有让你收到 ACK 信号时，你需要启用超时机制，并且还得重新发送它们。而超时往往意味着需要花费很长时间！
所以为什么以更慢的速率发送数据会更好呢？假如你发送数据的速率快于连接中的瓶颈，这时所有的信息包将会在某个地方堆积成一个队列，这个队列将会被塞满，然后在你的数据流的最末端的信息包将会被丢弃。并且像我们刚才解释的那样，处于数据流最后面的信息包很有可能丢弃！所以相比于最初以合适的速率发送信息包，一次性发送它们将会触发超时机制，发送 10MB 的数据将会花费更长的时间。