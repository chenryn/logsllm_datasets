# 23 \| 声明式API与Kubernetes编程范式你好，我是张磊。今天我和你分享的主题是：声明式 API 与 Kubernetes编程范式。在前面的几篇文章中，我和你分享了很多 Kubernetes 的 API 对象。这些 API对象，有的是用来描述应用，有的则是为应用提供各种各样的服务。但是，无一例外地，为了使用这些API 对象提供的能力，你都需要编写一个对应的 YAML 文件交给 Kubernetes。这个 YAML 文件，正是 Kubernetes 声明式 API所必须具备的一个要素。不过，是不是只要用 YAML文件代替了命令行操作，就是声明式 API 了呢？举个例子。我们知道，Docker Swarm 的编排操作都是基于命令行的，比如：    $ docker service create --name nginx --replicas 2  nginx$ docker service update --image nginx:1.7.9 nginx``{=html}像这样的两条命令，就是用 Docker Swarm 启动了两个 Nginx容器实例。其中，第一条 create 命令创建了这两个容器，而第二条 update命令则把它们"滚动更新"为了一个新的镜像。对于这种使用方式，我们称为**命令式命令行操作**。那么，像上面这样的创建和更新两个 Nginx 容器的操作，在 Kubernetes里又该怎么做呢？这个流程，相信你已经非常熟悉了：我们需要在本地编写一个 Deployment 的YAML 文件：    apiVersion: apps/v1kind: Deploymentmetadata:  name: nginx-deploymentspec:  selector:    matchLabels:      app: nginx  replicas: 2  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: nginx        ports:        - containerPort: 80然后，我们还需要使用 kubectl create 命令在 Kubernetes 里创建这个Deployment 对象：    $ kubectl create -f nginx.yaml这样，两个 Nginx 的 Pod 就会运行起来了。而如果要更新这两个 Pod 使用的 Nginx 镜像，该怎么办呢？我们前面曾经使用过 kubectl set image 和 kubectl edit 命令，来直接修改Kubernetes 里的 API对象。不过，相信很多人都有这样的想法，我能不能通过修改本地 YAML文件来完成这个操作呢？这样我的改动就会体现在这个本地 YAML 文件里了。当然可以。比如，我们可以修改这个 YAML 文件里的 Pod 模板部分，把 Nginx容器的镜像改成 1.7.9，如下所示：    ...    spec:      containers:      - name: nginx        image: nginx:1.7.9而接下来，我们就可以执行一句 kubectl replace 操作，来完成这个 Deployment的更新：    $ kubectl replace -f nginx.yaml可是，上面这种基于 YAML 文件的操作方式，是"声明式 API"吗？并不是。对于上面这种先 kubectl create，再 replace的操作，我们称为**命令式配置文件操作。**也就是说，它的处理方式，其实跟前面 Docker Swarm的两句命令，没什么本质上的区别。只不过，它是把 Docker命令行里的参数，写在了配置文件里而已。**那么，到底什么才是"声明式 API"呢？**答案是，kubectl apply 命令。在前面的文章中，我曾经提到过这个 kubectl apply命令，并推荐你使用它来代替 kubectl create命令（你也可以借此机会再回顾一下第 12篇文章[《牛刀小试：我的第一个容器化应用》](https://time.geekbang.org/column/article/40008)中的相关内容）。现在，我就使用 kubectl apply 命令来创建这个 Deployment：    $ kubectl apply -f nginx.yaml这样，Nginx 的 Deployment 就被创建了出来，这看起来跟 kubectl create的效果一样。然后，我再修改一下 nginx.yaml 里定义的镜像：    ...    spec:      containers:      - name: nginx        image: nginx:1.7.9这时候，关键来了。在修改完这个 YAML 文件之后，我不再使用 kubectl replace命令进行更新，而是继续执行一条 kubectl apply 命令，即：    $ kubectl apply -f nginx.yaml这时，Kubernetes 就会立即触发这个 Deployment 的"滚动更新"。可是，它跟 kubectl replace 命令有什么本质区别吗？实际上，你可以简单地理解为，kubectl replace 的执行过程，是使用新的 YAML文件中的 API 对象，**替换原有的 API 对象**；而 kubectlapply，则是执行了一个**对原有 API 对象的 PATCH 操作**。> 类似地，kubectl set image 和 kubectl edit 也是对已有 API 对象的修改。更进一步地，这意味着 kube-apiserver 在响应命令式请求（比如，kubectlreplace）的时候，一次只能处理一个写请求，否则会有产生冲突的可能。而对于声明式请求（比如，kubectlapply），**一次能处理多个写操作，并且具备 Merge 能力**。这种区别，可能乍一听起来没那么重要。而且，正是由于要照顾到这样的 API设计，做同样一件事情，Kubernetes 需要的步骤往往要比其他项目多不少。但是，如果你仔细思考一下 Kubernetes项目的工作流程，就不难体会到这种声明式 API 的独到之处。接下来，我就以 Istio 项目为例，来为你讲解一下声明式 API在实际使用时的重要意义。在 2017 年 5 月，Google、IBM 和 Lyft 公司，共同宣布了 Istio开源项目的诞生。很快，这个项目就在技术圈儿里，掀起了一阵名叫"微服务"的热潮，把Service Mesh 这个新的编排概念推到了风口浪尖。而 Istio 项目，实际上就是一个基于 Kubernetes项目的微服务治理框架。它的架构非常清晰，如下所示：![](Images/eae2ca5f5818764e614727fbf8ed4ac3.png){savepage-src="https://static001.geekbang.org/resource/image/d3/1b/d38daed2fedc90e20e9d2f27afbaec1b.jpg"}\在上面这个架构图中，我们不难看到 Istio 项目架构的核心所在。**Istio最根本的组件，是运行在每一个应用 Pod 里的 Envoy 容器**。这个 Envoy 项目是 Lyft 公司推出的一个高性能 C++ 网络代理，也是 Lyft公司对 Istio 项目的唯一贡献。而 Istio 项目，则把这个代理服务以 sidecar容器的方式，运行在了每一个被治理的应用 Pod 中。我们知道，Pod里的所有容器都共享同一个 Network Namespace。所以，Envoy容器就能够通过配置 Pod 里的 iptables 规则，把整个 Pod的进出流量接管下来。这时候，Istio 的控制层（Control Plane）里的 Pilot组件，就能够通过调用每个 Envoy 容器的 API，对这个 Envoy代理进行配置，从而实现微服务治理。我们一起来看一个例子。假设这个 Istio 架构图左边的 Pod 是已经在运行的应用，而右边的 Pod则是我们刚刚上线的应用的新版本。这时候，Pilot 通过调节这两 Pod 里的Envoy 容器的配置，从而将 90% 的流量分配给旧版本的应用，将 10%的流量分配给新版本应用，并且，还可以在后续的过程中随时调整。这样，一个典型的"灰度发布"的场景就完成了。比如，Istio可以调节这个流量从 90%-10%，改到 80%-20%，再到 50%-50%，最后到0%-100%，就完成了这个灰度发布的过程。更重要的是，在整个微服务治理的过程中，无论是对 Envoy容器的部署，还是像上面这样对 Envoy代理的配置，用户和应用都是完全"无感"的。这时候，你可能会有所疑惑：Istio 项目明明需要在每个 Pod 里安装一个 Envoy容器，又怎么能做到"无感"的呢？实际上，**Istio 项目使用的，是 Kubernetes 中的一个非常重要的功能，叫作Dynamic Admission Control。**在 Kubernetes 项目中，当一个 Pod 或者任何一个 API 对象被提交给 APIServer之后，总有一些"初始化"性质的工作需要在它们被 Kubernetes项目正式处理之前进行。比如，自动为所有 Pod 加上某些标签（Labels）。而这个"初始化"操作的实现，借助的是一个叫作 Admission 的功能。它其实是Kubernetes 项目里一组被称为 Admission Controller的代码，可以选择性地被编译进 APIServer 中，在 API对象创建之后会被立刻调用到。但这就意味着，如果你现在想要添加一些自己的规则到 AdmissionController，就会比较困难。因为，这要求重新编译并重启APIServer。显然，这种使用方法对 Istio 来说，影响太大了。所以，Kubernetes 项目为我们额外提供了一种"热插拔"式的 Admission机制，它就是 Dynamic Admission Control，也叫作：Initializer。现在，我给你举个例子。比如，我有如下所示的一个应用 Pod：    apiVersion: v1kind: Podmetadata:  name: myapp-pod  labels:    app: myappspec:  containers:  - name: myapp-container    image: busybox    command: ['sh', '-c', 'echo Hello Kubernetes! && sleep 3600'可以看到，这个 Pod 里面只有一个用户容器，叫作：myapp-container。接下来，Istio 项目要做的，就是在这个 Pod YAML 被提交给 Kubernetes之后，在它对应的 API 对象里自动加上 Envoy容器的配置，使这个对象变成如下所示的样子：    apiVersion: v1kind: Podmetadata:  name: myapp-pod  labels:    app: myappspec:  containers:  - name: myapp-container    image: busybox    command: ['sh', '-c', 'echo Hello Kubernetes! && sleep 3600']  - name: envoy    image: lyft/envoy:845747b88f102c0fd262ab234308e9e22f693a1    command: ["/usr/local/bin/envoy"]    ...可以看到，被 Istio 处理后的这个 Pod 里，除了用户自己定义的myapp-container 容器之外，多出了一个叫作 envoy 的容器，它就是 Istio要使用的 Envoy 代理。那么，Istio 又是如何在用户完全不知情的前提下完成这个操作的呢？Istio 要做的，就是编写一个用来为 Pod"自动注入"Envoy 容器的 Initializer。**首先，Istio 会将这个 Envoy 容器本身的定义，以 ConfigMap 的方式保存在Kubernetes 当中**。这个ConfigMap（名叫：envoy-initializer）的定义如下所示：    apiVersion: v1kind: ConfigMapmetadata:  name: envoy-initializerdata:  config: |    containers:      - name: envoy        image: lyft/envoy:845747db88f102c0fd262ab234308e9e22f693a1        command: ["/usr/local/bin/envoy"]        args:          - "--concurrency 4"          - "--config-path /etc/envoy/envoy.json"          - "--mode serve"        ports:          - containerPort: 80            protocol: TCP        resources:          limits:            cpu: "1000m"            memory: "512Mi"          requests:            cpu: "100m"            memory: "64Mi"        volumeMounts:          - name: envoy-conf            mountPath: /etc/envoy    volumes:      - name: envoy-conf        configMap:          name: envoy相信你已经注意到了，这个 ConfigMap 的 data 部分，正是一个 Pod对象的一部分定义。其中，我们可以看到 Envoy 容器对应的 containers字段，以及一个用来声明 Envoy 配置文件的 volumes 字段。不难想到，Initializer 要做的工作，就是把这部分 Envoy相关的字段，自动添加到用户提交的 Pod 的 API 对象里。可是，用户提交的 Pod里本来就有 containers 字段和 volumes 字段，所以 Kubernetes在处理这样的更新请求时，就必须使用类似于 git merge这样的操作，才能将这两部分内容合并在一起。所以说，在 Initializer 更新用户的 Pod 对象的时候，必须使用 PATCH API来完成。而这种 PATCH API，正是声明式 API 最主要的能力。**接下来，Istio 将一个编写好的 Initializer，作为一个 Pod 部署在Kubernetes 中**。这个 Pod 的定义非常简单，如下所示：    apiVersion: v1kind: Podmetadata:  labels:    app: envoy-initializer  name: envoy-initializerspec:  containers:    - name: envoy-initializer      image: envoy-initializer:0.0.1      imagePullPolicy: Always我们可以看到，这个 envoy-initializer 使用的 envoy-initializer:0.0.1镜像，就是一个事先编写好的"自定义控制器"（CustomController），我将会在下一篇文章中讲解它的编写方法。而在这里，我要先为你解释一下这个控制器的主要功能。我曾在第 16篇文章[《编排其实很简单：谈谈"控制器"模型》](https://time.geekbang.org/column/article/40583)中和你分享过，一个Kubernetes的控制器，实际上就是一个"死循环"：它不断地获取"实际状态"，然后与"期望状态"作对比，并以此为依据决定下一步的操作。而 Initializer 的控制器，不断获取到的"实际状态"，就是用户新创建的Pod。而它的"期望状态"，则是：这个 Pod 里被添加了 Envoy 容器的定义。我还是用一段 Go 语言风格的伪代码，来为你描述这个控制逻辑，如下所示：    for {  // 获取新创建的 Pod  pod := client.GetLatestPod()  // Diff 一下，检查是否已经初始化过  if !isInitialized(pod) {    // 没有？那就来初始化一下    doSomething(pod)  }}-   如果这个 Pod 里面已经添加过 Envoy 容器，那么就"放过"这个    Pod，进入下一个检查周期。-   而如果还没有添加过 Envoy 容器的话，它就要进行 Initialize    操作了，即：修改该 Pod 的 API 对象（doSomething 函数）。这时候，你应该立刻能想到，Istio 要往这个 Pod里合并的字段，正是我们之前保存在 envoy-initializer 这个 ConfigMap里的数据（即：它的 data 字段的值）。所以，在 Initializer 控制器的工作逻辑里，它首先会从 APIServer 中拿到这个ConfigMap：    func doSomething(pod) {  cm := client.Get(ConfigMap, "envoy-initializer")}然后，把这个 ConfigMap 里存储的 containers 和 volumes字段，直接添加进一个空的 Pod 对象里：    func doSomething(pod) {  cm := client.Get(ConfigMap, "envoy-initializer")    newPod := Pod{}  newPod.Spec.Containers = cm.Containers  newPod.Spec.Volumes = cm.Volumes}现在，关键来了。Kubernetes 的 API 库，为我们提供了一个方法，使得我们可以直接使用新旧两个Pod 对象，生成一个 TwoWayMergePatch：    func doSomething(pod) {  cm := client.Get(ConfigMap, "envoy-initializer")   newPod := Pod{}  newPod.Spec.Containers = cm.Containers  newPod.Spec.Volumes = cm.Volumes   // 生成 patch 数据  patchBytes := strategicpatch.CreateTwoWayMergePatch(pod, newPod)   // 发起 PATCH 请求，修改这个 pod 对象  client.Patch(pod.Name, patchBytes)}**有了这个 TwoWayMergePatch 之后，Initializer 的代码就可以使用这个 patch的数据，调用 Kubernetes 的 Client，发起一个 PATCH 请求**。这样，一个用户提交的 Pod 对象里，就会被自动加上 Envoy 容器相关的字段。当然，Kubernetes 还允许你通过配置，来指定要对什么样的资源进行这个Initialize 操作，比如下面这个例子：    apiVersion: admissionregistration.k8s.io/v1alpha1kind: InitializerConfigurationmetadata:  name: envoy-configinitializers:  // 这个名字必须至少包括两个 "."  - name: envoy.initializer.kubernetes.io    rules:      - apiGroups:          - "" // 前面说过， "" 就是 core API Group 的意思        apiVersions:          - v1        resources:          - pods这个配置，就意味着 Kubernetes 要对所有的 Pod 进行这个 Initialize操作，并且，我们指定了负责这个操作的Initializer，名叫：envoy-initializer。而一旦这个 InitializerConfiguration 被创建，Kubernetes 就会把这个Initializer 的名字，加在所有新创建的 Pod 的 Metadata 上，格式如下所示：    apiVersion: v1kind: Podmetadata:  initializers:    pending:      - name: envoy.initializer.kubernetes.io  name: myapp-pod  labels:    app: myapp...可以看到，每一个新创建的 Pod，都会自动携带了metadata.initializers.pending 的 Metadata 信息。这个 Metadata，正是接下来 Initializer 的控制器判断这个 Pod有没有执行过自己所负责的初始化操作的重要依据（也就是前面伪代码中isInitialized() 方法的含义）。**这也就意味着，当你在 Initializer里完成了要做的操作后，一定要记得将这个 metadata.initializers.pending标志清除掉。这一点，你在编写 Initializer 代码的时候一定要非常注意。**此外，除了上面的配置方法，你还可以在具体的 Pod 的 Annotation里添加一个如下所示的字段，从而声明要使用某个 Initializer：    apiVersion: v1kind: Podmetadata  annotations:    "initializer.kubernetes.io/envoy": "true"    ...在这个 Pod 里，我们添加了一个 Annotation，写明：`initializer.kubernetes.io/envoy=true`。这样，就会使用到我们前面所定义的envoy-initializer 了。以上，就是关于 Initializer最基本的工作原理和使用方法了。相信你此时已经明白，**Istio项目的核心，就是由无数个运行在应用 Pod 中的 Envoy容器组成的服务代理网格**。这也正是 Service Mesh 的含义。> 备注：如果你对这个 Demo 感兴趣，可以在[这个 GitHub> 链接](https://github.com/resouer/kubernetes-initializer-tutorial)里找到它的所有源码和文档。这个> Demo，是我 fork 自 Kelsey Hightower 的一个同名的 Demo。而这个机制得以实现的原理，正是借助了 Kubernetes 能够对 API对象进行在线更新的能力，这也正是**Kubernetes"声明式 API"的独特之处：**-   首先，所谓"声明式"，指的就是我只需要提交一个定义好的 API    对象来"声明"，我所期望的状态是什么样子。-   其次，"声明式 API"允许有多个 API 写端，以 PATCH 的方式对 API    对象进行修改，而无需关心本地原始 YAML 文件的内容。-   最后，也是最重要的，有了上述两个能力，Kubernetes 项目才可以基于对    API    对象的增、删、改、查，在完全无需外界干预的情况下，完成对"实际状态"和"期望状态"的调谐（Reconcile）过程。所以说，**声明式 API，才是 Kubernetes项目编排能力"赖以生存"的核心所在**，希望你能够认真理解。此外，不难看到，无论是对 sidecar 容器的巧妙设计，还是对 Initializer的合理利用，Istio 项目的设计与实现，其实都依托于 Kubernetes 的声明式 API和它所提供的各种编排能力。可以说，Istio 是在 Kubernetes项目使用上的一位"集大成者"。> 要知道，一个 Istio 项目部署完成后，会在 Kubernetes 里创建大约 43 个> API 对象。所以，Kubernetes 社区也看得很明白：Istio 项目有多火热，就说明 Kubernetes这套"声明式 API"有多成功。这，既是 Google Cloud 喜闻乐见的事情，也是Istio 项目一推出就被 Google 公司和整个技术圈儿热捧的重要原因。而在使用 Initializer 的流程中，最核心的步骤，莫过于Initializer"自定义控制器"的编写过程。它遵循的，正是标准的"Kubernetes编程范式"，即：> **如何使用控制器模式，同 Kubernetes 里 API> 对象的"增、删、改、查"进行协作，进而完成用户业务逻辑的编写过程。**这，也正是我要在后面文章中为你详细讲解的内容。
## 总结在今天这篇文章中，我为你重点讲解了 Kubernetes 声明式 API的含义。并且，通过对 Istio 项目的剖析，我为你说明了它使用 Kubernetes 的Initializer 特性，完成 Envoy 容器"自动注入"的原理。事实上，从"使用 Kubernetes 部署代码"，到"使用 Kubernetes编写代码"的蜕变过程，正是你从一个 Kubernetes 用户，到 Kubernetes玩家的晋级之路。而，如何理解"Kubernetes 编程范式"，如何为 Kubernetes 添加自定义 API对象，编写自定义控制器，正是这个晋级过程中的关键点，也是我要在后面几篇文章中分享的核心内容。此外，基于今天这篇文章所讲述的 Istio 的工作原理，尽管 Istio项目一直宣称它可以运行在非 Kubernetes环境中，但我并不建议你花太多时间去做这个尝试。毕竟，无论是从技术实现还是在社区运作上，Istio 与 Kubernetes项目之间都是紧密的、唇齿相依的关系。如果脱离了 Kubernetes项目这个基础，那么这条原本就不算平坦的"微服务"之路，恐怕会更加困难重重。
## 思考题你是否对 Envoy 项目做过了解？你觉得为什么它能够击败 Nginx 以及 HAProxy等竞品，成为 Service Mesh 体系的核心？感谢你的收听，欢迎你给我留言，也欢迎分享给更多的朋友一起阅读。![](Images/e870b7df0db49509e735e6becd4a9a9a.png){savepage-src="https://static001.geekbang.org/resource/image/47/55/47a6f3bf6b92d58512d5a2ed0a556f55.jpg"}
# 24 \| 深入解析声明式API（一）：API对象的奥秘你好，我是张磊。今天我和你分享的主题是：深入解析声明式 API 之 API对象的奥秘。在上一篇文章中，我为你详细讲解了 Kubernetes 声明式 API的设计、特点，以及使用方式。而在今天这篇文章中，我就来为你讲解一下 Kubernetes 声明式 API的工作原理，以及如何利用这套 API 机制，在 Kubernetes 里添加自定义的 API对象。你可能一直就很好奇：当我把一个 YAML 文件提交给 Kubernetes之后，它究竟是如何创建出一个 API 对象的呢？这得从声明式 API 的设计谈起了。在 Kubernetes 项目中，一个 API 对象在 Etcd里的完整资源路径，是由：Group（API 组）、Version（API 版本）和Resource（API 资源类型）三个部分组成的。通过这样的结构，整个 Kubernetes 里的所有 API对象，实际上就可以用如下的树形结构表示出来：![](Images/e3d510f4210ac2a0568d48d1a705ae44.png){savepage-src="https://static001.geekbang.org/resource/image/70/da/709700eea03075bed35c25b5b6cdefda.png"}\在这幅图中，你可以很清楚地看到**Kubernetes 里 API对象的组织方式，其实是层层递进的。**比如，现在我要声明要创建一个 CronJob 对象，那么我的 YAML文件的开始部分会这么写：    apiVersion: batch/v2alpha1kind: CronJob...在这个 YAML 文件中，"CronJob"就是这个 API对象的资源类型（Resource），"batch"就是它的组（Group），v2alpha1就是它的版本（Version）。``{=html}当我们提交了这个 YAML 文件之后，Kubernetes 就会把这个 YAML文件里描述的内容，转换成 Kubernetes 里的一个 CronJob 对象。那么，[Kubernetes 是如何对 Resource、Group 和 Version 进行解析，从而在Kubernetes 项目里找到 CronJob 对象的定义呢？]{.orange}**首先，Kubernetes 会匹配 API 对象的组。**需要明确的是，对于 Kubernetes 里的核心 API 对象，比如：Pod、Node等，是不需要 Group 的（即：它们 Group 是""）。所以，对于这些 API对象来说，Kubernetes 会直接在 /api 这个层级进行下一步的匹配过程。而对于 CronJob 等非核心 API 对象来说，Kubernetes 就必须在 /apis这个层级里查找它对应的 Group，进而根据"batch"这个 Group 的名字，找到/apis/batch。不难发现，这些 API Group 的分类是以对象功能为依据的，比如 Job 和 CronJob就都属于"batch" （离线业务）这个 Group。**然后，Kubernetes 会进一步匹配到 API 对象的版本号。**对于 CronJob 这个 API 对象来说，Kubernetes 在 batch 这个 Group下，匹配到的版本号就是 v2alpha1。在 Kubernetes 中，同一种 API 对象可以有多个版本，这正是 Kubernetes 进行API 版本化管理的重要手段。这样，比如在 CronJob的开发过程中，对于会影响到用户的变更就可以通过升级新版本来处理，从而保证了向后兼容。**最后，Kubernetes 会匹配 API 对象的资源类型。**在前面匹配到正确的版本之后，Kubernetes 就知道，我要创建的原来是一个/apis/batch/v2alpha1 下的 CronJob 对象。这时候，[APIServer 就可以继续创建这个 CronJob对象了]{.orange}。为了方便理解，我为你总结了一个如下所示流程图来阐述这个创建过程：![](Images/26ba3bfa569ef7b9044047e8ba86d08f.png){savepage-src="https://static001.geekbang.org/resource/image/df/6f/df6f1dda45e9a353a051d06c48f0286f.png"}\**首先**，当我们发起了创建 CronJob 的 POST 请求之后，我们编写的 YAML的信息就被提交给了 APIServer。而 APIServer的第一个功能，就是过滤这个请求，并完成一些前置性的工作，比如授权、超时处理、审计等。**然后**，请求会进入 MUX 和 Routes 流程。如果你编写过 Web Server的话就会知道，MUX 和 Routes 是 APIServer 完成 URL 和 Handler绑定的场所。而 APIServer 的 Handler要做的事情，就是按照我刚刚介绍的匹配过程，找到对应的 CronJob 类型定义。**接着**，APIServer 最重要的职责就来了：根据这个 CronJob类型定义，使用用户提交的 YAML 文件里的字段，创建一个 CronJob 对象。而在这个过程中，APIServer 会进行一个 Convert 工作，即：把用户提交的 YAML文件，转换成一个叫作 Super Version 的对象，它正是该 API资源类型所有版本的字段全集。这样用户提交的不同版本的 YAML文件，就都可以用这个 Super Version 对象来进行处理了。**接下来**，APIServer 会先后进行 Admission() 和 Validation()操作。比如，我在上一篇文章中提到的 Admission Controller 和Initializer，就都属于 Admission 的内容。而 Validation，则负责验证这个对象里的各个字段是否合法。这个被验证过的API 对象，都保存在了 APIServer 里一个叫作 Registry的数据结构中。也就是说，只要一个 API 对象的定义能在 Registry里查到，它就是一个有效的 Kubernetes API 对象。**最后**，APIServer 会把验证过的 API对象转换成用户最初提交的版本，进行序列化操作，并调用 Etcd 的 API把它保存起来。由此可见，声明式 API 对于 Kubernetes 来说非常重要。所以，**APIServer这样一个在其他项目里"平淡无奇"的组件，却成了 Kubernetes项目的重中之重**。它不仅是 Google Borg 设计思想的集中体现，也是Kubernetes 项目里唯一一个被 Google 公司和 RedHat公司双重控制、其他势力根本无法参与其中的组件。此外，由于同时要兼顾性能、API完备性、版本化、向后兼容等很多工程化指标，所以 Kubernetes 团队在APIServer 项目里大量使用了 Go 语言的代码生成功能，来自动化诸如Convert、DeepCopy 等与 API资源相关的操作。这部分自动生成的代码，曾一度占到 Kubernetes 项目总代码的20%\~30%。这也是为何，在过去很长一段时间里，在这样一个极其"复杂"的 APIServer中，添加一个 Kubernetes 风格的 API 资源类型，是一个非常困难的工作。不过，在 Kubernetes v1.7之后，这个工作就变得轻松得多了。这，当然得益于一个全新的 API插件机制：CRD。CRD 的全称是 Custom ResourceDefinition。顾名思义，它指的就是，允许用户在 Kubernetes 中添加一个跟Pod、Node 类似的、新的 API 资源类型，即：自定义 API 资源。举个例子，[我现在要为 Kubernetes 添加一个名叫 Network 的 API资源类型]{.orange}。它的作用是，一旦用户创建一个 Network 对象，那么 Kubernetes就应该使用这个对象定义的网络参数，调用真实的网络插件，比如 Neutron项目，为用户创建一个真正的"网络"。这样，将来用户创建的Pod，就可以声明使用这个"网络"了。这个 Network 对象的 YAML 文件，名叫example-network.yaml，它的内容如下所示：    apiVersion: samplecrd.k8s.io/v1kind: Networkmetadata:  name: example-networkspec:  cidr: "192.168.0.0/16"  gateway: "192.168.0.1"可以看到，我想要描述"网络"的 API 资源类型是 Network；API组是`samplecrd.k8s.io`；API 版本是 v1。那么，[Kubernetes 又该如何知道这个API（`samplecrd.k8s.io/v1/network`）的存在呢？]{.orange}其实，上面的这个 YAML 文件，就是一个具体的"自定义 API 资源"实例，也叫CR（Custom Resource）。而为了能够让 Kubernetes 认识这个 CR，你就需要让Kubernetes 明白这个 CR 的宏观定义是什么，也就是 CRD（Custom ResourceDefinition）。这就好比，你想让计算机认识各种兔子的照片，就得先让计算机明白，兔子的普遍定义是什么。比如，兔子"是哺乳动物""有长耳朵，三瓣嘴"。所以，接下来，我就先需编写一个 CRD 的 YAML 文件，它的名字叫作network.yaml，内容如下所示：    apiVersion: apiextensions.k8s.io/v1beta1kind: CustomResourceDefinitionmetadata:  name: networks.samplecrd.k8s.iospec:  group: samplecrd.k8s.io  version: v1  names:    kind: Network    plural: networks  scope: Namespaced可以看到，在这个 CRD中，我指定了"`group: samplecrd.k8s.io`""`version: v1`"这样的 API信息，也指定了这个 CR 的资源类型叫作 Network，复数（plural）是networks。然后，我还声明了它的 scope 是 Namespaced，即：我们定义的这个 Network是一个属于 Namespace 的对象，类似于 Pod。这就是一个 Network API 资源类型的 API部分的宏观定义了。这就等同于告诉了计算机："兔子是哺乳动物"。所以这时候，Kubernetes就能够认识和处理所有声明了 API 类型是"`samplecrd.k8s.io/v1/network`"的YAML 文件了。接下来，我还需要让 Kubernetes"认识"这种 YAML文件里描述的"网络"部分，比如"cidr"（网段），"gateway"（网关）这些字段的含义。这就相当于我要告诉计算机："兔子有长耳朵和三瓣嘴"。这时候呢，我就需要稍微做些代码工作了。**首先，我要在 GOPATH 下，创建一个结构如下的项目：**> 备注：在这里，我并不要求你具有完备的 Go> 语言知识体系，但我会假设你已经了解了 Golang> 的一些基本知识（比如，知道什么是> GOPATH）。而如果你还不了解的话，可以在涉及到相关内容时，再去查阅一些相关资料。    $ tree $GOPATH/src/github.com//k8s-controller-custom-resource.├── controller.go├── crd│   └── network.yaml├── example│   └── example-network.yaml├── main.go└── pkg    └── apis        └── samplecrd            ├── register.go            └── v1                ├── doc.go                ├── register.go                └── types.go其中，pkg/apis/samplecrd 就是 API 组的名字，v1 是版本，而 v1 下面的types.go 文件里，则定义了 Network对象的完整描述。我已经把这个项目[上传到了 GitHub上](https://github.com/resouer/k8s-controller-custom-resource)，你可以随时参考。**然后，我在 pkg/apis/samplecrd 目录下创建了一个 register.go文件，用来放置后面要用到的全局变量**。这个文件的内容如下所示：    package samplecrd const ( GroupName = "samplecrd.k8s.io" Version   = "v1")**接着，我需要在 pkg/apis/samplecrd 目录下添加一个 doc.go 文件（Golang的文档源文件）**。这个文件里的内容如下所示：    // +k8s:deepcopy-gen=package // +groupName=samplecrd.k8s.iopackage v1在这个文件中，你会看到 +\\[=value\] 格式的注释，这就是Kubernetes 进行代码生成要用的 Annotation 风格的注释。其中，+k8s:deepcopy-gen=package 意思是，请为整个 v1包里的所有类型定义自动生成 DeepCopy方法；而`+groupName=samplecrd.k8s.io`，则定义了这个包对应的 API组的名字。可以看到，这些定义在 doc.go文件的注释，起到的是全局的代码生成控制的作用，所以也被称为 Global Tags。**接下来，我需要添加 types.go 文件**。顾名思义，它的作用就是定义一个Network 类型到底有哪些字段（比如，spec字段里的内容）。这个文件的主要内容如下所示：    package v1...// +genclient// +genclient:noStatus// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // Network describes a Network resourcetype Network struct { // TypeMeta is the metadata for the resource, like kind and apiversion metav1.TypeMeta `json:",inline"` // ObjectMeta contains the metadata for the particular object, including // things like... //  - name //  - namespace //  - self link //  - labels //  - ... etc ... metav1.ObjectMeta `json:"metadata,omitempty"`  Spec networkspec `json:"spec"`}// networkspec is the spec for a Network resourcetype networkspec struct { Cidr    string `json:"cidr"` Gateway string `json:"gateway"`} // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // NetworkList is a list of Network resourcestype NetworkList struct { metav1.TypeMeta `json:",inline"` metav1.ListMeta `json:"metadata"`  Items []Network `json:"items"`}在上面这部分代码里，你可以看到 Network 类型定义方法跟标准的 Kubernetes对象一样，都包括了 TypeMeta（API 元数据）和ObjectMeta（对象元数据）字段。而其中的 Spec 字段，就是需要我们自己定义的部分。所以，在 networkspec里，我定义了 Cidr 和 Gateway两个字段。其中，每个字段最后面的部分比如`json:"cidr"`，指的就是这个字段被转换成JSON 格式之后的名字，也就是 YAML 文件里的字段名字。> 如果你不熟悉这个用法的话，可以查阅一下 Golang 的文档。此外，除了定义 Network 类型，你还需要定义一个 NetworkList类型，用来描述**一组 Network对象**应该包括哪些字段。之所以需要这样一个类型，是因为在 Kubernetes中，获取所有 X 对象的 List() 方法，返回值都是``{=html}List类型，而不是 X 类型的数组。这是不一样的。``{=html}同样地，在 Network 和 NetworkList 类型上，也有代码生成注释。其中，+genclient 的意思是：请为下面这个 API 资源类型生成对应的 Client代码（这个 Client，我马上会讲到）。而 +genclient:noStatus 的意思是：这个API 资源类型定义里，没有 Status 字段。否则，生成的 Client 就会自动带上UpdateStatus 方法。如果你的类型定义包括了 Status 字段的话，就不需要这句 +genclient:noStatus注释了。比如下面这个例子：    // +genclient // Network is a specification for a Network resourcetype Network struct { metav1.TypeMeta   `json:",inline"` metav1.ObjectMeta `json:"metadata,omitempty"`  Spec   NetworkSpec   `json:"spec"` Status NetworkStatus `json:"status"`}需要注意的是，+genclient 只需要写在 Network 类型上，而不用写在NetworkList 上。因为 NetworkList 只是一个返回值类型，Network才是"主类型"。而由于我在 Global Tags 里已经定义了为所有类型生成 DeepCopy方法，所以这里就不需要再显式地加上 +k8s:deepcopy-gen=true了。当然，这也就意味着你可以用 +k8s:deepcopy-gen=false来阻止为某些类型生成 DeepCopy。你可能已经注意到，在这两个类型上面还有一句`+k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object`的注释。它的意思是，请在生成DeepCopy 的时候，实现 Kubernetes 提供的 runtime.Object接口。否则，在某些版本的 Kubernetes里，你的这个类型定义会出现编译错误。这是一个固定的操作，记住即可。不过，你或许会有这样的顾虑：这些代码生成注释这么灵活，我该怎么掌握呢？其实，上面我所讲述的内容，已经足以应对 99%的场景了。当然，如果你对代码生成感兴趣的话，我推荐你阅读[这篇博客](https://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/)，它详细地介绍了Kubernetes 的代码生成语法。**最后，我需要再编写的一个 pkg/apis/samplecrd/v1/register.go 文件**。在前面对 APIServer工作原理的讲解中，我已经提到，"registry"的作用就是注册一个类型（Type）给APIServer。其中，Network 资源类型在服务器端的注册的工作，APIServer会自动帮我们完成。但与之对应的，我们还需要让客户端也能"知道"Network资源类型的定义。这就需要我们在项目里添加一个 register.go文件。它最主要的功能，就是定义了如下所示的 addKnownTypes() 方法：    package v1...// addKnownTypes adds our types to the API scheme by registering// Network and NetworkListfunc addKnownTypes(scheme *runtime.Scheme) error { scheme.AddKnownTypes(  SchemeGroupVersion,  &Network{},  &NetworkList{}, )  // register the type in the scheme metav1.AddToGroupVersion(scheme, SchemeGroupVersion) return nil}有了这个方法，Kubernetes 就能够在后面生成客户端的时候，"知道"Network以及 NetworkList 类型的定义了。像上面这种**register.go文件里的内容其实是非常固定的，你以后可以直接使用我提供的这部分代码做模板，然后把其中的资源类型、GroupName和 Version 替换成你自己的定义即可。**这样，Network对象的定义工作就全部完成了。可以看到，它其实定义了两部分内容：-   第一部分是，自定义资源类型的 API    描述，包括：组（Group）、版本（Version）、资源类型（Resource）等。这相当于告诉了计算机：兔子是哺乳动物。-   第二部分是，自定义资源类型的对象描述，包括：Spec、Status    等。这相当于告诉了计算机：兔子有长耳朵和三瓣嘴。接下来，[我就要使用 Kubernetes 提供的代码生成工具，为上面定义的 Network资源类型自动生成 clientset、informer 和lister。]{.orange}其中，clientset 就是操作 Network对象所需要使用的客户端，而 informer 和 lister这两个包的主要功能，我会在下一篇文章中重点讲解。这个代码生成工具名叫`k8s.io/code-generator`，使用方法如下所示：    