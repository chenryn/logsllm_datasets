title:Practical Black-Box Attacks against Machine Learning
author:Nicolas Papernot and
Patrick D. McDaniel and
Ian J. Goodfellow and
Somesh Jha and
Z. Berkay Celik and
Ananthram Swami
Practical Black-Box Attacks against Machine Learning
Nicolas Papernot
Pennsylvania State University
PI:EMAIL
Patrick McDaniel
Pennsylvania State University
PI:EMAIL
Ian Goodfellow
∗
OpenAI
PI:EMAIL
Somesh Jha
University of Wisconsin
PI:EMAIL
Z. Berkay Celik
Pennsylvania State University
PI:EMAIL
Ananthram Swami
US Army Research Laboratory
PI:EMAIL
ABSTRACT
Machine learning (ML) models, e.g., deep neural networks
(DNNs), are vulnerable to adversarial examples: malicious
inputs modiﬁed to yield erroneous model outputs, while ap-
pearing unmodiﬁed to human observers. Potential attacks
include having malicious content like malware identiﬁed as
legitimate or controlling vehicle behavior. Yet, all existing
adversarial example attacks require knowledge of either the
model internals or its training data. We introduce the ﬁrst
practical demonstration of an attacker controlling a remotely
hosted DNN with no such knowledge. Indeed, the only capa-
bility of our black-box adversary is to observe labels given
by the DNN to chosen inputs. Our attack strategy consists
in training a local model to substitute for the target DNN,
using inputs synthetically generated by an adversary and
labeled by the target DNN. We use the local substitute to
craft adversarial examples, and ﬁnd that they are misclas-
siﬁed by the targeted DNN. To perform a real-world and
properly-blinded evaluation, we attack a DNN hosted by
MetaMind, an online deep learning API. We ﬁnd that their
DNN misclassiﬁes 84.24% of the adversarial examples crafted
with our substitute. We demonstrate the general applicabil-
ity of our strategy to many ML techniques by conducting the
same attack against models hosted by Amazon and Google,
using logistic regression substitutes. They yield adversarial
examples misclassiﬁed by Amazon and Google at rates of
96.19% and 88.94%. We also ﬁnd that this black-box attack
strategy is capable of evading defense strategies previously
found to make adversarial example crafting harder.
1.
INTRODUCTION
A classiﬁer is a ML model that learns a mapping between
inputs and a set of classes. For instance, a malware detector
is a classiﬁer taking executables as inputs and assigning them
to the benign or malware class. Eﬀorts in the security [5, 2,
9, 18] and machine learning [14, 4] communities exposed the
∗Work done while the author was at Google.
Publication rights licensed to ACM. ACM acknowledges that this contribution was
authored or co-authored by an employee, contractor or afﬁliate of the United States
government. As such, the Government retains a nonexclusive, royalty-free right to
publish or reproduce this article, or to allow others to do so, for Government purposes
only.
ASIA CCS ’17, April 02 - 06, 2017, Abu Dhabi, United Arab Emirates
c(cid:13) 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-4944-4/17/04. . . $15.00
DOI: http://dx.doi.org/10.1145/3052973.3053009
vulnerability of classiﬁers to integrity attacks. Such attacks
are often instantiated by adversarial examples: legitimate
inputs altered by adding small, often imperceptible, perturba-
tions to force a learned classiﬁer to misclassify the resulting
adversarial inputs, while remaining correctly classiﬁed by a
human observer. To illustrate, consider the following images,
potentially consumed by an autonomous vehicle [13]:
To humans, these images appear to be the same: our bio-
logical classiﬁers (vision) identify each image as a stop sign.
The image on the left [13] is indeed an ordinary image of a
stop sign. We produced the image on the right by adding
a precise perturbation that forces a particular DNN to clas-
sify it as a yield sign, as described in Section 5.2. Here, an
adversary could potentially use the altered image to cause
a car without failsafes to behave dangerously. This attack
would require modifying the image used internally by the car
through transformations of the physical traﬃc sign. Related
works showed the feasibility of such physical transformations
for a state-of-the-art vision classiﬁer [6] and face recognition
model [11]. It is thus conceivable that physical adversarial
traﬃc signs could be generated by maliciously modifying the
sign itself, e.g., with stickers or paint.
In this paper, we introduce the ﬁrst demonstration that
black-box attacks against DNN classiﬁers are practical for
real-world adversaries with no knowledge about the model.
We assume the adversary (a) has no information about the
structure or parameters of the DNN, and (b) does not have
access to any large training dataset. The adversary’s only
capability is to observe labels assigned by the DNN for chosen
inputs, in a manner analog to a cryptographic oracle.
Our novel attack strategy is to train a local substitute
DNN with a synthetic dataset: the inputs are synthetic and
generated by the adversary, while the outputs are labels
assigned by the target DNN and observed by the adversary.
Adversarial examples are crafted using the substitute param-
eters, which are known to us. They are not only misclassiﬁed
by the substitute but also by the target DNN, because both
models have similar decision boundaries.
This is a considerable departure from previous work, which
evaluated perturbations required to craft adversarial exam-
ples using either: (a) detailed knowledge of the DNN archi-
tecture and parameters [2, 4, 9, 14], or (b) an independently
collected training set to ﬁt an auxiliary model [2, 4, 14]. This
506limited their applicability to strong adversaries capable of
gaining insider knowledge of the targeted ML model, or col-
lecting large labeled training sets. We release assumption (a)
by learning a substitute: it gives us the beneﬁt of having full
access to the model and apply previous adversarial example
crafting methods. We release assumption (b) by replacing
the independently collected training set with a synthetic
dataset constructed by the adversary with synthetic inputs
and labeled by observing the target DNN’s output.
Our threat model thus corresponds to the real-world sce-
nario of users interacting with classiﬁers hosted remotely by
a third-party keeping the model internals secret. In fact,
we instantiate our attack against classiﬁers automatically
trained by MetaMind, Amazon, and Google. We are able
to access them only after training is completed. Thus, we
provide the ﬁrst correctly blinded experiments concerning
adversarial examples as a security risk.
We show that our black-box attack is applicable to many
remote systems taking decisions based on ML, because it
combines three key properties: (a) the capabilities required
are limited to observing output class labels, (b) the number
of labels queried is limited, and (c) the approach applies
and scales to diﬀerent ML classiﬁer types (see Section 7),
in addition to state-of-the-art DNNs. In contrast, previous
work failed to simultaneously provide all of these three key
properties [4, 14, 12, 15, 18]. Our contributions are:
• We introduce in Section 4 an attack against black-box
DNN classiﬁers. It crafts adversarial examples without
knowledge of the classiﬁer training data or model. To do
so, a synthetic dataset is constructed by the adversary
to train a substitute for the targeted DNN classiﬁer.
• In Section 5, we instantiate the attack against a re-
mote DNN classiﬁer hosted by MetaMind. The DNN
misclassiﬁes 84.24% of the adversarial inputs crafted.
• The attack is calibrated in Section 6 to (a) reduce the
number of queries made to the target model and (b)
maximize misclassiﬁcation of adversarial examples.
• We generalize the attack to other ML classiﬁers like lo-
gistic regression. In Section 7, we target models hosted
by Amazon and Google. They misclassify adversarial
examples at rates of 96.19% and 88.94%.
• Section 8 shows that our attack evades defenses pro-
posed in the literature because the substitute trained
by the adversary is unaﬀected by defenses deployed on
the targeted oracle model to reduce its vulnerability.
• In Appendix B, we provide an intuition of why adver-
sarial examples crafted with the substitute also mislead
target models by empirically observing that substitutes
have gradients correlated to the target’s.
Figure 1: DNN Classiﬁer: the model processes an image
of a handwritten digit and outputs the probility of it being
in one of the N = 10 classes for digits 0 to 9 (from [10]).
for i ∈ 1..n is modeled using a layer of neurons, which are
elementary computing units applying an activation function
to the previous layer’s weighted representation of the input to
generate a new representation. Each layer is parameterized
by a weight vector θi (we omit the vector notation) impacting
each neuron’s activation. Such weights hold the knowledge of
a DNN model F and are evaluated during its training phase,
as detailed below. Thus, a DNN deﬁnes and computes:
F ((cid:126)x) = fn (θn, fn−1 (θn−1, ... f2 (θ2, f1 (θ1, (cid:126)x))))
(1)
The training phase of a DNN F learns values for its pa-
rameters θF = {θ1, ..., θn}. We focus on classiﬁcation tasks,
where the goal is to assign inputs a label among a prede-
ﬁned set of labels. The DNN is given a large set of known
input-output pairs ((cid:126)x, (cid:126)y) and it adjusts weight parameters to
reduce a cost quantifying the prediction error between the
prediction F ((cid:126)x) and the correct output (cid:126)y. The adjustment is
typically performed using techniques derived from the back-
propagation algorithm. Brieﬂy, such techniques successively
propagate error gradients with respect to network parameters
from the network’s output layer to its input layer.
During the test phase, the DNN is deployed with a ﬁxed
set of parameters θF to make predictions on inputs unseen
during training. We consider classiﬁers: the DNN produces
a probability vector F ((cid:126)x) encoding its belief of input (cid:126)x being
in each of the classes (cf. Figure 1). The weight parameters
θF hold the model knowledge acquired by training. Ideally,
the model should generalize and make accurate predictions
for inputs outside of the domain explored during training.
However, attacks manipulating DNN inputs with adversarial
examples showed this is not the case in practice [4, 9, 14].
Disclosure: We disclosed our attacks to MetaMind, Ama-
zon, and Google. Note that no damage was caused as we
demonstrated control of models created for our own account.
2. ABOUT DEEP NEURAL NETWORKS
We provide preliminaries of deep learning to enable under-
standing of our threat model and attack. We refer readers
interested to the more detailed presentation in [3].
A deep neural network (DNN), as illustrated in Figure 1,
is a ML technique that uses a hierarchical composition of n
parametric functions to model an input (cid:126)x. Each function fi
3. THREAT MODEL
A taxonomy of adversaries against DNN classiﬁers is found
in [9]. In our work, the adversary seeks to force a classiﬁer
to misclassify inputs in any class diﬀerent from their correct
class. To achieve this, we consider a weak adversary with
access to the DNN output only. The adversary has no knowl-
edge of the architectural choices made to design the DNN,
which include the number, type, and size of layers, nor of
the training data used to learn the DNN’s parameters. Such
attacks are referred to as black box, where adversaries need
not know internal details of a system to compromise it.
………Input LayerOutput Layer Hidden Layers(e.g., convolutional, rectiﬁed linear, …){p0=0.01p1=0.93p8=0.02pN=0.01M componentsN componentsNeuronWeighted Link  (weight is a parameter part of       )✓O5074. BLACK-BOX ATTACK STRATEGY
We introduce our black-box attack. As stated in Section 3,
the adversary wants to craft inputs misclassiﬁed by the ML
model using the sole capability of accessing the label ˜O((cid:126)x)
assigned by classiﬁer for any chosen input (cid:126)x. The strategy is
to learn a substitute for the target model using a synthetic
dataset generated by the adversary and labeled by observing
the oracle output. Then, adversarial examples are crafted us-
ing this substitute. We expect the target DNN to misclassify
them due to transferability between architectures [14, 4]
To understand the diﬃculty of conducting the attack under
this threat model, recall Equation 3 formalizing the adver-
sarial goal of ﬁnding a minimal perturbation that forces the
targeted oracle to misclassify. A closed form solution cannot
be found when the target is a non-convex ML model: e.g., a
DNN. The basis for most adversarial attacks [4, 9, 14] is to
approximate its solution using gradient-based optimization
on functions deﬁned by a DNN. Because evaluating these
functions and their gradients requires knowledge of the DNN
architecture and parameters, such an attack is not possible
under our black-box scenario. It was shown that adversaries
with access to an independently collected labeled training set
from the same population distribution than the oracle could
train a model with a diﬀerent architecture and use it as a
substitute [14]: adversarial examples designed to manipulate
the substitute are often misclassiﬁed by the targeted model.
However, many modern machine learning systems require
large and expensive training sets for training. For instance,
we consider models trained with several tens of thousands of
labeled examples. This makes attacks based on this paradigm
unfeasible for adversaries without large labeled datasets.
In this paper, we show black-box attacks can be accom-
plished at a much lower cost, without labeling an independent
training set. In our approach, to enable the adversary to
train a substitute model without a real labeled dataset, we
use the target DNN as an oracle to construct a synthetic
dataset. The inputs are synthetically generated and the out-
puts are labels observed from the oracle. Using this synthetic
dataset, the attacker builds an approximation F of the model
O learned by the oracle. This substitute network F is then
used to craft adversarial samples misclassiﬁed by F Indeed,
with its full knowledge of the substitute DNN F parame-
ters, the adversary can use one of the previously described
attacks [4, 9] to craft adversarial samples misclassiﬁed by F .
As long as the transferability property holds between F and
O, adversarial samples crafted for F will also be misclassiﬁed
by O. This leads us to propose the following strategy:
1. Substitute Model Training: the attacker queries
the oracle with synthetic inputs selected by a Jacobian-
based heuristic to build a model F approximating the
oracle model O’s decision boundaries.
2. Adversarial Sample Crafting: the attacker uses
substitute network F to craft adversarial samples, which
are then misclassiﬁed by oracle O due to the transfer-
ability of adversarial samples.
4.1 Substitute Model Training
Training a substitute model F approximating oracle O is
challenging because we must: (1) select an architecture for
our substitute without knowledge of the targeted oracle’s
architecture, and (2) limit the number of queries made to the
oracle in order to ensure that the approach is tractable. Our
Figure 2: Adversarial samples (misclassiﬁed) in the bot-