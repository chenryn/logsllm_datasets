learner;
With these changes, we built a linear model on count-
featurized data and compare it to the (linear) raw-data
model used in production. Both models were trained
using VW’s online contextual bandit
in the
production system, a snapshot of the model is deployed
to application servers every ﬁve minutes.
Applicability. Our results suggest that in this applica-
tion, selectivity is achieved naturally by retaining only
the last day of data in the hot window and without the
need for Pyramid’s training set minimization. This is
because news is highly non-stationary: new content ap-
pears every hour and breaking news inﬂuences people’s
short-term interests. As a result, even without Pyramid,
training models on the last day of raw data is sufﬁcient,
and in fact better than training on more days. This is
in contrast to the MovieLens and Criteo datasets, which
are much more stationary and hence can beneﬁt from
Pyramid’s training set reduction.
That said, even in non-stationary settings, Pyramid
can still enhance data protection through its privacy-
preserving counts. We compared the estimated click-
through rate (CTR) of the count model (with and without
noise) to the raw model across a seven-day period in
April 2016. Fig. 10 shows the results relative to the
default article ranking by editors. Despite day-to-day
variations, on average count models perform within 7%
and 13.5% (with noise) of the raw model performance.
Support for workload evolution. We also assessed
how Pyramid would support changes in MSN over time,
without accessing the raw data store. MSN developers
have spent hundreds (thousands) of human (compute)
hours optimizing the production models. The changes in-
clude: tuning hyperparameters and learning rates, adding
L1/L2 regularization, testing different exploration rates
or model deployment
intervals, and adding/interact-
ing/removing features. For example, in some regions
regulatory policies prevent certain user data from being
collected, so they are removed and models are retrained.
Pyramid supports all of the listed changes (§III-C) except
adding new features/feature interactions.
VI. Analysis and Limitations
We analyze Pyramid’s security properties in the con-
text of our threat model (§II-B), pointing out its limi-
tations. A Pyramid deployment has three components:
(1) A central repository of raw data in cold storage that
is infrequently accessed and is assumed to be secure.
Protecting this data store is outside of Pyramid’s scope.
(2) A compute/storage cluster used to train models,
store the plaintext hot window, and to store and update
count tables. (3) Numerous model servers storing trained
models and cached versions of count tables.
We ﬁrst examine the effects of compromising the clus-
ter responsible for training models, maintaining the hot
window, and storing the count tables. This will reveal the
state of the count tables at time Tattack-Δhot by subtract-
ing all observations residing in the hot window at Tattack.
Property P1 in §II-B captures this exposure. However,
the observations from the range [Tattack-Δretention, Tattack-
Δhot] are protected through differential privacy (property
P2 in §II-B). We expect that the hot window (Δhot)
will be small enough that only a small fraction of an
organization’s data will be exposed. Observations whose
retention period ended before Tattack will have been
erased, and the models will have been retrained to forget
this information (property P3 in §II-B).
[Tattack, T end
In addition to the hot data, the adversary can siphon
attack].
observations arriving in the interval
Hence, the amount of data exposed depends on the time
to discover and respond to an attack. The sliding nature
of Pyramid’s hot window gives the organization an ad-
vantage when investigating breaches. If an organization
knows Tattack and T stop
attack, it will be able to determine
exactly which observations were exposed to the attacker
and take the appropriate steps. Knowing these times is
only required for post-attack auditing, not for protection
of past data during the attack.
Under our current threat model, Pyramid does not
protect data from multiple intrusions happening during
the same time window. If an attacker accesses Pyramid’s
internal count tables, that attack is eradicated, and then
gains access again at Tattack2 where Tattack2 follows T stop
attack,
the attacker will be able to compute the full ﬁdelity
count tables for updates that occurred during the time
range [T stop
attack, min(Tattack2, Twin end)] by subtracting the
91
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:25:23 UTC from IEEE Xplore.  Restrictions apply. 
table at Tattack from the state of
state of the count
table at Tattack2. Twin end is the time
the same count
when Pyramid ﬁnishes populating the count table it was
populating at T stop
attack. One approach to mitigate this attack
is to require that Pyramid recomputes count tables after
T stop
attack, including reinitializing them with new draws from
the Laplacian distribution. This will require an increased
privacy budget but will still provide a privacy guarantee.
§V demonstrates the need to cache count tables on the
application model servers. Attackers that compromise an
application server will gain access to the existing cached
count table, trained models, and a stream of plaintext
prediction requests (unlabeled observations). With access
only to the application server the adversary will be able
to calculate the difference between the existing count
table and new count tables as they are replicated. The
adversary will learn little because the difference between
the cached count table and the newly replicated count
table will be differentially private.
A key limitation of our system stems from our design
choice to expose data for a period of time, while it is
hot. Data is exposed through the hot data store, trained
models, external predictions, and other states that may
persist after the data is phased out into the differentially
private count
tables. There are three implications of
this design choice. First, an adversary may monitor
these states before actually mounting the full-system
break-in that Pyramid is designed to protect against (so
before Tstart). §II-B explicitly leaves this attack out of
scope. Second, exposing the hot data in raw form to
programmers and applications may produce data residues
that persist after the data is phased out, potentially
revealing past information when an attacker breaks in
at Tstart. For example, a programmer may create a local
copy of the hot window at time T for experimentation
purposes. While we cannot ensure that state created out-
of-band is securely managed, the Pyramid design strives
to eliminate any residues for state that Pyramid manages.
This is why we enforce model retraining whenever the
hot window is rolled over. And this is why we clarify in
§III-B2 that the count and weight selection mechanisms
should incorporate differential privacy. Third, while the
exposed hot data may be small (e.g., 1% of all the
data), it may still reveal sufﬁcient sensitive information
to satisfy the attacker’s goal. Despite these caveats, we
believe that our design decision to expose a little hot
data affords important practical beneﬁts that would be
difﬁcult to achieve with a fully protected design. For
example, unlike fully differentially private designs [48],
our scheme allows training of unchanged ML algorithms
with limited impact on their accuracy. Unlike encrypted
databases [49], [50], our scheme provides performance
and scalability close to—or even better than—running
on the raw, fully exposed data.
VII. Related Work
Closest works. Closest to our work are the building
blocks we leverage for Pyramid’s selective data pro-
tection architecture: count featurization and differen-
tial privacy. Count
featurization has been developed
and adopted to improve performance and scalability of
certain learning systems. We are the ﬁrst
to retroﬁt
it to improve data protection, deﬁning the protection
guarantees that can be achieved and implementing them
without sacriﬁcing accuracy.
To implement these guarantees, we leverage differen-
tial privacy theory [51]. The typical threat model for
differentially private systems [28], [48], [52] is different
from ours: they protect user privacy in the results of
a publicly released computation, whereas Pyramid aims
to protect data inside the system, by minimizing access
to historical data so its accesses can be controlled and
monitored more tightly. For example, differential privacy
frameworks (e.g., PINQ [28] and Airavat [52], adding
privacy to LINQ and MapReduce respectively) ensure
that the result of a query will be differentially private.
However, these systems require full and permanent ac-
cess to the data. The same holds for privacy-preserving
recommender systems [48]. Pan-privacy [42], [53], [54]
is a variant of differential privacy that holds even when
an adversary can observe the system’s internal state, a
threat model close to ours.
Pyramid is the ﬁrst to combine count featurization
with differential privacy for protection.1 This raises sig-
niﬁcant challenges at scale, including rampant noise with
large numbers of count tables and damaging interference
of differential privacy noise with count-min sketches.
To address these challenges, our design includes two
techniques: noise weighting and private count-median
sketches. Prior art, such as iReduct [55] or GUPT [56],
included a noise weighting scheme to allocate less of
the privacy budget to queries with larger results. To our
knowledge, we are the ﬁrst to point out the limitations
of CMS integration with differential privacy and propose
private count-median sketches as a solution.
Alternative protection approaches. Many alternative
protection models exist. First, many companies enforce
a data retention period. However, because of the data’s
perceived beneﬁt, most companies conﬁgure long peri-
ods. Google maintains data for 9-18 months [57]. Pyra-
mid limits the data’s exposure for as long as the company
decides to retain it. Second, some companies anonymize
data: Google erases the last byte of IP addresses in
search logs after 6 months [58]. Anonymization provides
very weak protection [59]. Pyramid leverages differential
privacy to provide rigorous protection guarantees. Third,
1Azure applies tiny levels of Laplacian noise to count featurization
to avoid overﬁtting, but such low levels neither provide protection nor
raise the challenges we encountered.
92
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:25:23 UTC from IEEE Xplore.  Restrictions apply. 
some companies enforce access controls on the data.
Google’s Sawmill strips out sensitive data before return-
ing results to processes lacking certain permissions [60].
Given the push toward increased developer access to
data [5], [6], Pyramid provides additional beneﬁt by
protecting data on a needs basis.
Data minimization. Compact data representation is an
important topic in big data systems, and many techniques
exist for different scenarios. Sketching techniques com-
pute compact representations of the data that support
queries of summary statistics [26], large-scale regres-
sion analysis [61], privacy preserving aggregation [62];
streaming/online algorithms [63], [64] process the data
using bounded memory, retaining only the information
relevant for the problem at hand; dimensionality reduc-
tion techniques [10] ﬁnd a low-dimensional, faithful rep-
resentation of the raw data, according to different mea-
sures of faithfulness; hash featurization [11] compacts
high-cardinality categorical variables; coresets [65], [66]
are data subsets giving a good approximation for a
given computation; autoencoders attempt
to learn a
compressed identity function [67].
We believe that this rich literature should be inspected
for candidates for selective data protection. Not all
mechanisms will be suitable. For example, according
to our evaluation (Fig. 4), hash featurization [11] does
not yield sufﬁcient training set reduction. And none of
the mechanisms listed above appear to support workload
evolution. The next section presents a few promising
techniques we have identiﬁed.
VIII. Closing: A Vision for Selectivity
We close with our vision for selectivity in big data
systems. Today’s indiscriminate data collection, long-
term archival, and wide-access practices are risky and
unsustainable. It is time for a more rigorous and selective
approach to big data collection, access, and protection so
that its beneﬁts can be reaped without undue risks.
(cid:43)(cid:82)(cid:87)
(cid:76)(cid:81)(cid:16)(cid:88)(cid:86)(cid:72)
(cid:72)
(cid:81)(cid:16)(cid:88)(cid:86)(cid:72)(cid:72)
(cid:71)(cid:68)(cid:87)(cid:68)(cid:71)(cid:68)(cid:87)(cid:68)
(cid:71)(cid:68)(cid:87)(cid:68)
(cid:83)(cid:85)(cid:82)(cid:87)(cid:72)(cid:70)(cid:87)(cid:3)(cid:68)(cid:86)(cid:3)(cid:83)(cid:82)(cid:86)(cid:86)(cid:76)(cid:69)(cid:79)(cid:72)(cid:15)
(cid:80)(cid:76)(cid:81)(cid:76)(cid:80)(cid:76)(cid:93)(cid:72)(cid:3)(cid:76)(cid:81)(cid:3)(cid:86)(cid:76)(cid:93)(cid:72)(cid:15)
(cid:87)(cid:76)(cid:80)(cid:72)(cid:15)(cid:3)(cid:86)(cid:72)(cid:81)(cid:86)(cid:76)(cid:87)(cid:76)(cid:89)(cid:76)(cid:87)(cid:92)
Our vision (illustrated on
the right) involves architect-
ing data-driven systems to
permit clean separation of
data needed by current and
evolving workloads,
from
data collected and archived for possible future needs. The
former should be minimized in size and time span (hence
the pyramid shape). The latter should be protected vigor-
ously and only tapped under exceptional circumstances.
These requirements should be met without disrupting
functional properties of the workloads.
(cid:83)(cid:85)(cid:82)(cid:87)(cid:72)(cid:70)(cid:87)(cid:3)
(cid:89)(cid:76)(cid:74)(cid:82)(cid:85)(cid:82)(cid:88)(cid:86)(cid:79)(cid:92)(cid:15)(cid:3)
(cid:68)(cid:89)(cid:82)(cid:76)(cid:71)(cid:3)
(cid:68)(cid:70)(cid:70)(cid:72)(cid:86)(cid:86)
(cid:88)(cid:81)(cid:88)(cid:86)(cid:72)(cid:71)
(cid:71)(cid:68)(cid:87)(cid:68)
The notion of selectivity applies to many big data
workloads, including ML and non-ML, and there are per-
haps multiple ways to conceptualize the data selectivity