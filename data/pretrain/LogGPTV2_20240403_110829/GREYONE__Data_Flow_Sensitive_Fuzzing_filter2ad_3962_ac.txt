tracking mode for evolution tuning. To efﬁciently schedule
these different testing modes, we extend the fork server used
by AFL to switch between them on demand. For example,
during fuzzing, if a seed has spent too much mutation energy
or the conformance does not increase for a while, then we will
switch from conformance tracking mode to regular coverage
tracking mode.
3.2 Static Analysis and Instrumentation.
To support the policies proposed in the paper, we need to ﬁrst
analyze the target applications with static analysis, as well as
collect some information at runtime.
We perform some basic inter-procedural control ﬂow anal-
ysis with the help of Clang, and get the control ﬂow graph
and other necessary information.
4 Evaluation
In this section, we evaluated the efﬁciency of GREYONE, and
showed its improvements compared to other fuzzers.
4.1 Experiment Setup
Following the guidance in [21], we conducted the experiments
carefully, to draw conclusions as objective as possible.
Baseline fuzzers to compare. We compared GREY-
ONE against several well-known evolutionary mutation-based
fuzzers, including AFL [44], VUzzer [30], Angora [10], Col-
lAFL [14] , Honggfuzz [38], and QSYM[43]5. They are cho-
sen based on the following considerations. First, AFL was the
most popular baseline fuzzer studied in the community. Sec-
ond, Angora and VUzzer also utilized taint to guide fuzzing.
Third, CollAFL provides more accurate coverage information,
which is also utilized by GREYONE. In addition, CollAFL
proposed a seed selection policy relying on control ﬂow fea-
tures, different from GREYONE. Further, Honggfuzz is a core
fuzzing engine in Google’s OSS-Fuzz platform [33], and also
uses light-weight data tracking to identify good seeds. Lastly,
QSYM is a popular symbolic execution assisted fuzzer, and
we can use it to evaluate GREYONE’ capability on bypassing
complicated program constraints.
Target applications to test. We chose target applications
considering several factors, including popularity, frequency
of being tested, development activeness, and functionality
diversity. Finally, we chose 19 popular open source Linux
applications (in latest version when tested), including well-
known development tools (e.g., readelf, nm, c++filt), im-
age processing libraries (e.g., libtiff), document process-
5CollAFL is not open source. We implemented a copy following its
design. Another work REDQUEEN [4] is also related, but it is disclosed only
one month ago and not open source. Thus we are unable to compare with it.
2584    29th USENIX Security Symposium
USENIX Association
Table 1: Number of vulnerabilities (accumulated in 5 runs) detected by 6 fuzzers, including AFL, CollAFL-br, VUzzer, Honggfuzz,
Angora, and GREYONE, after testing each application for 60 hours.
Applications
readelf
nm
c++ﬁlt
tiff2pdf
tiffset
ﬁg2dev
libwpd
ncurses
nasm
bison
cﬂow
libsass
libbson
libsndﬁle
libconfuse
libwebm
libsolv
libcaca
liblas
libslax
libsixl
libxsmm
Total
Version
2.31
2.31
2.31
v4.0.9
v4.0.9
3.2.7a
0.1
6.1
2.14rc15
3.05
1.5
3.5-stable
1.8.0
1.0.28
3.2.2
1.0.0.27
2.4
0.99beta19
2.4
20180901
v1.8.2
release-1.10
-
AFL
1
0
1
0
1
1
0
1
1
0
2
0
1
1
1
1
0
2
1
3
2
1
21
CollAFL- br
1
0
1
0
2
3
1
1
2
0
3
0
1
2
2
1
0
4
2
5
2
1
34
Honggfuzz
0
0
1
0
0
2
0
0
2
1
1
0
1
2
0
0
3
1
0
0
2
2
18
VUzzer
0
0
0
0
0
0
0
0
1
0
0
0
0
1
0
0
2
0
0
0
2
0
6
Angora
3
0
0
0
0
0
0
0
2
2
0
0
0
0
0
0
2
0
0
0
3
0
12
GREYONE
4
2
4
2
2
10
2
4
12
4
8
3
2
2
3
1
3
10
6
10
6
5
105 (+209%)
Vulnerabilities by GREYONE
CVE
Unknown
-
2
1
*
*
2
0
1
1
1
0
8
2
2
2
2
8
11
0
2
0
4
2
2
1
1
2
1
1
2
1
1
3
3
6
8
4
6
9
*
6
6
3
4
80
41
Known
2
1
2
1
1
2
0
2
1
2
4
1
1
0
1
0
0
2
0
1
0
1
25
ing libraries (e.g., libwpd), terminal processing libraries
(e.g., libncurses), audio or video processing libraries (e.g.,
ibsndfile), code processing tools (e.g., cflow, bison,
nasm), graphics processing libraries (e.g., libcaca and
libsixel), and data processing libraries (e.g., libsass and
libxsmm) etc. Furthermore, we also evaluated GREYONE on
the LAVA-M data set [12] as other fuzzers.
Performance metrics. We chose vulnerability discovery
and code coverage as two major metrics used to compare the
efﬁciency of each fuzzer with GREYONE. For code coverage,
we mainly considered path coverage (i.e., number of seeds in
the queue) and edge coverage (i.e., number of edge hit) sim-
ilar to [14, 42]. For vulnerability discovery, we tracked the
growth trend of unique crashes detected by different fuzzers.
We further utilized tools including aﬂ-collect [3], AddressSan-
itizer [34] and UBSan [23] to deduplicate redundant crashes
and identify unique vulnerabilities.
Note that, fuzzers have different representations of fuzzing
states (e.g., bitmap). We therefore slightly modify them to get
uniﬁed fuzzing states and perform fair comparison.
Initial seeds. Note that, our taint analysis engine FTI re-
lies on byte-level mutation. It will perform poorly if no initial
seeds are given, lowering the efﬁciency of GREYONE. There-
fore, we did not test target applications with empty seeds.
Instead, we test each target application with 10 initial seeds.
For each target application, we randomly downloaded about
100 input ﬁles from the Internet, according the required input
ﬁle formats. Then, we use the tool aﬂ-cmin shipping with
AFL [44], to ﬁlter out a minimal subset of inputs that have the
same code coverage. Finally, we randomly selected 10 inputs
from these distilled inputs, and used them as the initial seeds.
Randomness mitigation. Since mutation-based fuzzers all
rely on random mutation, there could be performance jitter
during testing. We took two actions to mitigate the random-
ness issue. First, we perform each experiment for 5 times,
and evaluate the average performance as well as the minimal
and maximal performance. Second, we test target applications
for more time, until the fuzzers reach a relatively stable state
(i.e., the order of fuzzers’ performance does not change any-
more). Experiments showed that the fuzzers will get stable
after testing these applications for 60 hours. So, we tested
each application for 60 hours in our experiment.
Experiment environment. We run each fuzzer instance
on each target application in the same conﬁguration. More
speciﬁcally, each instance is run in a virtual machine running
Ubuntu 17.04 with one Intel CPU @2.9GHz and 8GB RAM.
4.2 Vulnerability Discovery
Table 1 shows the number of unique vulnerabilities (accumu-
lated in 5 runs) found by 6 different fuzzers in the 19 real
world applications. Each application is of the latest version at
the time of testing.
In total, AFL, CollAFL, Honggfuzz, VUzzer and Angora
has found 21, 34, 18, 6 and 12 vulnerabilities in all applica-
tions respectively. GREYONE found 105 unique vulnerabil-
ities in total and covered all vulnerabilities found by other
fuzzers. In other words, GREYONE found 209% more vul-
nerabilities than the second best fuzzer (i.e., CollAFL). Espe-
cially, out of these 19 applications, three applications includ-
ing nm, tiff2pdf and libsass are reported as vulnerable
only by GREYONE. In summary, GREYONE signiﬁcantly out-