title:Not all microseconds are equal: fine-grained per-flow measurements
with reference latency interpolation
author:Myungjin Lee and
Nick G. Duffield and
Ramana Rao Kompella
Not All Microseconds are Equal: Fine-Grained Per-Flow
Measurements with Reference Latency Interpolation
Myungjin Lee, †Nick Dufﬁeld, Ramana Rao Kompella
Purdue University, †AT&T Labs – Research
ABSTRACT
New applications such as algorithmic trading and high-performance
computing require extremely low latency (in microseconds). Net-
work operators today lack sufﬁcient ﬁne-grain measurement tools
to detect, localize and repair performance anomalies and delay spikes
that cause application SLA violations. A recently proposed solu-
tion called LDA provides a scalable way to obtain latency, but only
provides aggregate measurements. However, debugging application-
speciﬁc problems requires per-ﬂow measurements, since different
ﬂows may exhibit signiﬁcantly different characteristics even when
they are traversing the same link. To enable ﬁne-grained per-ﬂow
measurements in routers, we propose a new scalable architecture
called reference latency interpolation (RLI) that is based on our ob-
servation that packets potentially belonging to different ﬂows that
are closely spaced to each other exhibit similar delay properties.
In our evaluation using simulations over real traces, we show that
RLI achieves a median relative error of 12% and one to two orders
of magnitude higher accuracy than previous per-ﬂow measurement
solutions with small overhead.
Categories and Subject Descriptors
C.2.3 [Computer Communication Networks]: Network manage-
ment
General Terms
Measurement, algorithms
Keywords
Active measurement, approximation
1.
INTRODUCTION
Latency is one of the most fundamental properties of packet-
switched networks. End-to-end latency directly impacts several
critical Internet applications including multimedia applications such
as voice-over-IP, video conferencing and online games. While these
traditional applications often require end-to-end latencies within
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’10, August 30–September 3, 2010, New Delhi, India.
Copyright 2010 ACM 978-1-4503-0201-2/10/08 ...$10.00.
100s of milliseconds, several new types of applications that require
extremely low end-to-end latency (in the order of microseconds)
have emerged. For instance, high performance computing appli-
cations within data center networks [3], storage applications (with
industry moving toward Fiber Channel over Ethernet (FCoE) [20])
and, algorithmic trading applications [26] (together constituting multi-
billion dollar markets) all require low end-to-end latencies in the
order of few microseconds. A small increase in end-to-end latency
for trading applications can, for instance, lead to a loss of millions
of dollars in lost arbitrage opportunities [26].
To effectively manage low-latency applications, operators require
sophisticated tools and techniques for detecting, and more impor-
tantly, localizing delay spikes (i.e., ﬁnding the router responsible
for the high latency) and other performance anomalies in these net-
works. Once the problem is localized, they can potentially iso-
late the particular offending ﬂow that is responsible for causing the
delay bursts, and reroute the trafﬁc through other paths. In other
cases, the operators may upgrade their bottleneck links that are re-
sponsible for the underlying delay spikes. Of course, one could
argue it may be more important to devise router architectures that
guarantee low end-to-end latencies to begin with—indeed, some
switches such as those by Arista [9] and Woven [36] provide la-
tency guarantees within 10s of microseconds—in which case, the
need for ﬁne-grained measurements is obviated. Unfortunately, an-
ticipating all types of performance problems and application inter-
actions that may occur in a production data center a priori is often
difﬁcult; ﬁne-grained measurements are therefore still required.
Detecting and localizing latency problems is surprisingly hard
today. Routers and switches by themselves offer very little la-
tency measurement and monitoring capabilities; SNMP counters
and NetFlow that routers come equipped with are grossly insufﬁ-
cient. SNMP counters provide coarse-grained statistics on a per-
port basis, but do not measure latencies. NetFlow provides ba-
sic statistics on a per-ﬂow basis such as number of packets and
bytes, but not latency estimates.
ISP network operators monitor
the health of their network by injecting active probes to measure
end-to-end delays and use tomographic techniques [37, 12] to infer
link and hop properties. Unfortunately, for the granularity of mea-
surements required, active probes need to be injected at extremely
high probe rate making them not suitable for these low-latency net-
works. Operators in these networks therefore resort to specialized
measurement appliances developed by vendors (e.g., London stock
exchange uses those manufactured by Corvil [4]). This type of
appliance however tends to be quite costly (around GB£90,000
for a 2×10Gbps box [2]). The high cost of such appliances is a
strong inhibitor to ubiquitous deployment across multiple points in
the network, or multiple ports of each router, especially within data
27center environments that are much more cost conscious than ﬁnan-
cial markets.
Recognizing these challenges, researchers have recently proposed
a new high speed router-level data structure called LDA [23] for
measuring delays within routers at high ﬁdelity. LDA addresses
the scaling problem of active probes, and cost issue of commercial
monitors. While LDA provides a good start, it is by no means suf-
ﬁcient as it is designed to provide aggregate measurements such
as average latency across all packets, but not on a per-ﬂow ba-
sis. Experience indicates that concurrent ﬂows may experience sig-
niﬁcantly different latencies even when traversing the same given
router, and even over relatively short periods of time. Thus, differ-
entiated delay measurements are critical for diagnosing problems,
where the aggregate behavior of a router may appear normal, but
speciﬁc ﬂows and applications may suffer from bad performance.
We illustrate this situation using two motivating examples that
are similar in spirit but differ in their context. In the ﬁrst exam-
ple, consider a data center provider hosting several different appli-
cations, and a particular application experiences bad performance
say due to an offending application that is causing periodic bursts
of data (referred to as microbursts [6]). In such cases, aggregate
statistics such as average latency alone are insufﬁcient, since av-
erages across several million packets may appear normal, while
the application is still hurting. A similar issue is the in-cast prob-
lem in data centers where synchronized bursts of packets ﬁll switch
buffers causing high latencies or even packet loss because data cen-
ter workloads tend to be barrier-synchronized [35]. While speciﬁc
solutions may exist for known problems [35], the constant evolu-
tion of data centers in scale and diversity may potentially give rise
to several unforeseen performance problems. Our second example
considers trading networks, where it is possible for ﬁnancial institu-
tions to obtain speciﬁc SLAs from service providers (such as guar-
anteed less than 100 μs) [1]. In such a context, it is important for
the service providers to be able to localize delay spikes and varia-
tions that may happen at any of the several hops between the trading
party and the stock exchange—diagnosing these customer-speciﬁc
problems requires not just aggregate, but ﬂow-level measurements.
Having motivated the intuitive need for differentiated measure-
ments, a fundamental question that one may ask is, how much
variation exists over several different ﬂows that are simultaneously
traversing a given router. In this paper, we explore this question by
conducting a measurement study using time-synchronized packet
traces collected between two interfaces of a real router, and simu-
lations of backbone traces using traditional queueing models. Our
measurement results reveal several fundamental insights: (1) We
observe a signiﬁcant amount of performance diversity among sev-
eral contemporaneous ﬂows (up to 2-3 orders of magnitude differ-
ence).
(2) We observe that packets belonging to different ﬂows
exhibit signiﬁcant temporal similarity within short bursts.
We exploit the insights gained from our measurement study to
propose a new architecture called reference latency interpolation
(RLI) for obtaining per-ﬂow latency measurements in a scalable
fashion. Our target is to accurately detect ﬂow latencies in the order
of a few 10s to 100 microseconds on a per-ﬂow basis. We wish
to detect both average as well as standard deviations of latencies
within a given ﬂow. Thus, the contributions of this paper are:
• Measurement study of performance diversity and the temporal
localization of delay. Using real router traces and simulations, we
conduct a measurement study (§2) that reveals our fundamental
experimental insight—while concurrent ﬂows can experience di-
verse performance at longer time scales due to trafﬁc and conges-
tion burstiness, the delay experience by packets from different ﬂow
within small localized windows is similar.
• Architecture for high-ﬁdelity per-ﬂow latency measurements. Based
on the ﬁndings in our measurement study, we propose an architec-
ture (§3) that pushes the state-of-the-art in scalable latency estima-
tion solutions beyond aggregate measurements, to provide per-ﬂow
latency measurements.
• Evaluation using real traces and simulations. We extensively
evaluate the efﬁcacy of our architecture (prototype implementation
described in §4) using a combination of real traces as well as sim-
ulations. In our evaluation, we observe that our RLI architecture
achieves a median relative error of 10-12% (§5.1), and up to one to
two orders of magnitude lower relative error than existing state-of-
the-art schemes under speciﬁc conﬁgurations (§5.2).
2. DELAY DIVERSITY AND LOCALITY
Before we set out to devise a scalable architecture for ﬁne-grained
ﬂow-level latencies within routers, it is important to ascertain that
one aggregate latency measure (for which efﬁcient solutions such
as LDA [23] have already been proposed) is not sufﬁcient. In this
section, we show that there exists signiﬁcant diversity of perfor-
mance experienced by concurrent ﬂows traversing the same link,
both through qualitative reasoning from the bursty nature of packet
arrivals, and through an experimental study. We also observe that
the same burstiness properties reduce performance diversity within
sufﬁciently short time intervals; we discuss the ramiﬁcations of this
observation for the design of a scalable architecture for per-ﬂow
performance measurements.
2.1 Flow-level performance diversity
Under many circumstances, ﬂow arrivals are bursty (ﬂows do
not commence as a Poisson process) and ﬂow durations are heavy-
tailed (as opposed to exponentially distributed) [29]. In such con-
ditions, congestion also tends to be bursty, being concentrated in
rarer longer bursts that would be the case for Poisson trafﬁc. Con-
sequently, the performance experience of a ﬂow depends strongly
on whether it encounters a congestion burst or not, and the com-
parative rarity of the bursts means that the normalizing effect of
temporal averaging only comes into play for long ﬂows. A further
conclusion is that common statistics of delays encountered by a
stream of probes (such as their mean or certain quantiles) can vary
signiﬁcantly from those encountered by ﬂows traversing the same
link during the same measurement period.
Experimental demonstration. To demonstrate the existence of
performance diversity experimentally, we analyzed the packets de-
lays in four datasets described more fully in §4.1: SANJ, CHIC,
WEB468 and WEB700. While these are not data center traces, we
believe the observations hold true in general. Note that SANJ and
CHIC are derived from synthetic queueing times based on a simple
FIFO queueing model (more details about the queueing model in
§4.2) and real timestamps of packets arrival on an OC-192 inter-
face. Thus we expect it them to provide a realistic representation of
the queueing dynamics whose properties underpin our method. In
contrast WEB468 and WEB700 are derived from passage of syn-
thetically generated trafﬁc through a real network with real routers,
with actual packet arrival and departure timestamps measured at
two interfaces of a router. This enables us to capture any effects
speciﬁc to complexities of actual as opposed to synthetic queueing.
Due to space limitations, we will report our results in greatest de-
tail for SANJ and CHIC, more brieﬂy for the others, although all
conﬁrmed the expected performance diversity.
To study the differentiated delay properties, we classiﬁed each
packet according to a 2-tuple key comprising the source and des-
tination IP addresses. Table 1 details the broad properties relevant
for our study (see Table 2 for other properties). We also capture
28Trace
Dur.
SANJ
CHIC
WEB468
WEB700
600s
600s
305s
305s
2-tuple
keys
4.8M
4.6M
169
169
Pkts/key
Delay
mean(ms)
43.9
28.8
15k
23k
0.387
0.287
0.55
3.70
R
27,867
8,052
8.7
5.5
Table 1: Traces: duration, number of src-dst keys, average
number of packets per key, global average delay of all packets,
and range factor R of per-key average delay.
the variability of packet delay across different keys by means of
the range factor R described below. WEB700 entails a 50% higher
packet rate, as seen from the packet counts over the same duration
and correspondingly, the mean delay is about an order of magni-
tude higher for WEB700. SANJ trace comprises a load about 63%
higher than CHIC; mean delay for SANJ, therefore, is about an
order of magnitude higher than for CHIC.
In order to further capture the variability of the per key average
delay, we calculated the mean delay for each key, and compute the
range factor R which we deﬁne as the ratio of 99th and 1st quan-
tiles of the mean delay per key. We used this measure to capture
nearly the full extent of the range, while excluding a small number
of outliers. For SANJ, the range spanned three orders of magni-
tude, for the others about 1 order. We found similar ranges for the
95%ile of the delay within each key in WEB468 and WEB700. We
conclude that a single delay statistic, such as an average or quan-
tile over a set of probes over the same duration, cannot accurately
account for the delay experienced by the range of trafﬁc ﬂows.
The differences in values of summary statistics between the traces
can be interpreted in the light of two factors. First, each keys of
WEB468 and WEB700 represents 10s of thousands of packets on
average, so there is considerable packet averaging possible. SANJ
and CHIC, due to the greater diversity of endpoint addresses, have
small numbers of packets per key. However the large range of mean
delays seen in SANJ and CHIC are not just an effect brought about
by averaging over small numbers of packets. If we restrict atten-
tion to the keys with more packets, speciﬁcally those with 100 or
more (about 85% of the total packets) the range factor is still 36 for
SANJ and 18 for CHIC, i.e., still over an order of magnitude.
2.2 Temporal localization of queueing delays
We have just seen how delay statistics of concurrent ﬂows over
a 5 minute period can vary over an order of magnitude, and gave
a qualitative explanation in terms of bursty nature—both of packet
arrivals and congestion. However, this same burstiness addition-
ally leads us to expect that, within bursts of delay, packets should
experience more similar queueing delays. A theoretical argument
for such behavior has been given in the context of some relatively
simple trafﬁc models in [25]. We now demonstrate this empirically,
by localizing time, and determining how closely the mean queue-
ing delay experienced by packets of a given ﬂow over small time
window can be approximated by the mean delay experienced by
the packets of all other ﬂows transmitting packets over the same
window. Note that we focus on queueing delay, since different
size packets encountering the same delay burst will incur differ-
ent serialization delays according to their size. Given ingress and
egress timestamps, ti and te respectively, of a packet of size b bits
at a resource served at service rate r bits per second, the associated
queueing delay is taken as d = te − ti − b/r. In the remainder of
this section, the term “delay” will be understood as queueing delay.
We will discuss the ramiﬁcations of our ﬁndings for the design of
performance measurements in §2.4.
 10
 1
 0.1
 0.01
 0.001
 0.001
 100
 10
 1
 0.1
 0.01
 0.001