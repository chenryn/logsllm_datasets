coordination needed to prevent disruptive access errors. Instead of
Figure 7: RMA write op execution flow. (1) Client sends a write
request to server-side remote NIC. (2) Remote NIC sends a read
request back to the client-side initiating NIC when 1RMA solic-
itation rules are satisfied. (3) The initiating NIC responds as
though it received a read request and (4) the remote NIC sends
a completion message.
4.5 Outcomes
Upon completion the 1RMA NIC returns a completion indicator to
the initiating commandâ€™s designated completion region. The com-
pletion includes the CommandSlotId of the completing slot, two
hardware delay measures, issue_delay and total_delay, and an
op status code, which conveys the nature of any errors (Table 3).
Notable status codes include NACKs, which signal remote congestion,
DISPATCH_TIMEOUTs, which signal congestion at the initiating NIC,
and TIMEOUTs, which signal other errors that cause the op to fail.
5 Other 1RMA Ops
Apart from RMA reads (exemplified in Â§3), 1RMA also offers: (1)
RMA writes, and (2) Rekey for use in regular rotation of encryption
keys with minimal availability impact. Unlike reads, writes and
Rekey use a four-hop protocol, thereby obeying solicitation and
providing resilience against failures and replay attacks.
5.1 RMA Write
1RMA implements RMA writes as remote-request-to-read: each
write solicits the remote node to read the initiatorâ€™s memory. The
sequence of steps (Figure 7) are: 1 send write request to remote NIC,
2 remote NIC sends a read request back to the initiating NIC when
1RMA solicitation rules are satisfied, 3 the initiating NIC responds
as though it received a read request (following the steps in Â§3), and
4 the remote NIC sends a completion acknowledgement message.
Both the remote and local 1RMA NICs can time out independently
if messages are dropped or experience overlong delay.
714
ClientServerWrReqRdReq,Timeout=tRespCmplPCIePCIeClient Timeout WindowServer Timeout WindowPCIe WriteFabricTrafficPCIe Read1234SIGCOMM â€™20, August 10â€“14, 2020, Virtual Event, NY, USA
A. Singhvi et al.
avoiding them, such errors are tolerated in 1RMA. More sophis-
ticated rotation implementations can proactively notify users of
upcoming Rekeys and new ğ¾ğ‘‘ s, which can reduce the period of
unavailability due to a Rekey to a single RTT.
Because it is treated as a normal RMA operation, Rekey even
allows a remote third party, in possession of the appropriate derived
region key ğ¾ğ‘‘ , to change a region key ğ¾ğ‘Ÿ . Remote Rekey enables
control planes to manage pools of remote memory with no local
CPU involvement.
6 Congestion Control
1RMA implements congestion control (CC) policies in software
(CommandExecutor), assisted by NIC support for hardware mea-
sured delays and precise failure outcomes, delivered with each opâ€™s
completion. CC reacts to these signals by modulating offered load,
specifically the rate at which software issues up-to-4KB commands
to the 1RMA hardware.
Our objectives for CC are fourfold: (1) enable rapid iteration on
policy, (2) avoid wasted bandwidth (e.g., which occurs when reads
time out due to congestion), (3) allocate bandwidth fairly, and (4)
converge quickly in dynamic environments, as load comes and goes
from bursting applications. Our first objective is met by virtue of
1RMA leaving all policy decisions to software. We meet the second
objective by assigning target delays and modulating op issue rate
such that hardware delay measures hover near our targets. For the
remaining objectives, we leverage 1RMAâ€™s load-shedding outcomes
to rapidly change op issue rates.
1RMA CC differs from traditional, packet-oriented schemes
in two critical ways. First, 1RMAâ€™s software controls when indi-
vidual ops are initiated, not the timing of packet-send. Second,
all CC decisions are made on the initiating side only, because
1RMA is purely one-sided. 1RMA itself has no connections, so the
CommandExecutor tracks congestion state per remote 1RMA NIC
and per direction (inbound reads, outbound writes). Unlike hardware-
based CC, such state resides in relatively cheap host DRAM and is
never accessed or cached by the NIC.
Similar to Swift [22] and TIMELY [30], we use delay as a conges-
tion signal and implement a software control loop to react to changes
in delay. Delays are measured by hardware, broken down into two
components (Â§4.5): issue_delay and total_delay. The difference
between these two, which we call remote_delay, represents delay
contribution from network congestion and remote queuing. The
issue_delay signals local congestion at the initiating 1RMA NIC.
1RMA software uses the above delays to react separately to local
and remote congestion; contemporary schemes [5, 23, 31, 41] cannot
distinguish between these forms of congestion. 1RMA tracks a
congestion window (CWND) for each remote destination/direction
pair, and uses a single CWND for local congestion (because local
congestion affects all local transfers). CC assigns op rates based
on the more restrictive of the two CWNDs. Rising or falling delay
prompts CC to adjust each CWND using a simple additive-increase-
multiplicative-decrease control loop. Op failures trigger a more
substantial CWND decrease. Notably, a DISPATCH_TIMEOUT triggers
a 10Ã— reduction in the local CWND (only), as this is a precise
indicator of local congestion.
Algorithm 1 describes how 1RMA CC reacts to local congestion.
The control loop probes for the correct congestion window, adjusting
Algorithm 1: 1RMA CC reaction to local congestion.
Input: issue_delay, RTT
Output: cwnd_local
On Successful Op Completion
ğ‘ğ‘¤ğ‘›ğ‘‘_ğ‘™ğ‘œğ‘ğ‘ğ‘™_ğ‘œğ‘™ğ‘‘ â† ğ‘ğ‘¤ğ‘›ğ‘‘_ğ‘™ğ‘œğ‘ğ‘ğ‘™
if issue_delay < TARGET_DELAY then
âŠ² Additive Increase (AI)
if ğ‘ğ‘¤ğ‘›ğ‘‘_ğ‘™ğ‘œğ‘ğ‘ğ‘™ â‰¥ 1 then
ğ‘ğ‘¤ğ‘›ğ‘‘_ğ‘™ğ‘œğ‘ğ‘ğ‘™ â† ğ‘ğ‘¤ğ‘›ğ‘‘_ğ‘™ğ‘œğ‘ğ‘ğ‘™ +
else
ğ‘ğ‘¤ğ‘›ğ‘‘_ğ‘™ğ‘œğ‘ğ‘ğ‘™ â† ğ‘ğ‘¤ğ‘›ğ‘‘_ğ‘™ğ‘œğ‘ğ‘ğ‘™ + ğ´ğ¼
ğ‘ğ‘¤ğ‘›ğ‘‘_ğ‘™ğ‘œğ‘ğ‘ğ‘™ â† ğ‘šğ‘–ğ‘›(ğ‘ğ‘¤ğ‘›ğ‘‘_ğ‘™ğ‘œğ‘ğ‘ğ‘™, ğ¶ğ‘Š ğ‘ ğ·_ğ‘€ğ´ğ‘‹)
else
ğ´ğ¼
ğ‘ğ‘¤ğ‘›ğ‘‘_ğ‘™ğ‘œğ‘ğ‘ğ‘™ âŠ² ğ´ğ¼ = 0.25
âŠ² Multiplicative Decrease (MD)
if no decrease in the last RTT time then
ğ‘‘ğ‘’ğ‘™ğ‘¡ğ‘ â† ğ‘–ğ‘ ğ‘ ğ‘¢ğ‘’_ğ‘‘ğ‘’ğ‘™ğ‘ğ‘¦ âˆ’ ğ‘‡ ğ´ğ‘…ğºğ¸ğ‘‡ _ğ·ğ¸ğ¿ğ´ğ‘Œ ;
ğ‘ğ‘¤ğ‘›ğ‘‘_ğ‘™ğ‘œğ‘ğ‘ğ‘™ â†
ğ‘šğ‘ğ‘¥(1 âˆ’ ğ›½ Â· (
ğ‘–ğ‘ ğ‘ ğ‘¢ğ‘’_ğ‘‘ğ‘’ğ‘™ğ‘ğ‘¦), ğ‘€ğ·) Â· ğ‘ğ‘¤ğ‘›ğ‘‘_ğ‘™ğ‘œğ‘ğ‘ğ‘™;
ğ‘ğ‘¤ğ‘›ğ‘‘_ğ‘™ğ‘œğ‘ğ‘ğ‘™ â† ğ‘šğ‘ğ‘¥(ğ‘ğ‘¤ğ‘›ğ‘‘_ğ‘™ğ‘œğ‘ğ‘ğ‘™, ğ¶ğ‘Š ğ‘ ğ·_ğ‘€ğ¼ ğ‘);
ğ‘‘ğ‘’ğ‘™ğ‘¡ğ‘
âŠ² ğ‘€ğ· = 0.5, ğ›½ = 0.8
On Dispatch Timeout
if no decrease happened in the last RTT time then
ğ‘ğ‘¤ğ‘›ğ‘‘_ğ‘™ğ‘œğ‘ğ‘ğ‘™ â†
ğ‘‡ ğ¼ğ‘€ğ¸ğ‘‚ğ‘ˆğ‘‡ _ğ·ğ¸ğ¶ğ‘… Â· ğ‘ğ‘¤ğ‘›ğ‘‘_ğ‘™ğ‘œğ‘ğ‘ğ‘™;
ğ‘ğ‘¤ğ‘›ğ‘‘_ğ‘™ğ‘œğ‘ğ‘ğ‘™ â† ğ‘šğ‘ğ‘¥(ğ‘ğ‘¤ğ‘›ğ‘‘_ğ‘™ğ‘œğ‘ğ‘ğ‘™, ğ¶ğ‘Š ğ‘ ğ·_ğ‘€ğ¼ ğ‘)
âŠ² ğ‘‡ ğ¼ğ‘€ğ¸ğ‘‚ğ‘ˆğ‘‡ _ğ·ğ¸ğ¶ğ‘… = 0.1
CWND up additively (down multiplicatively) when issue_delay is
beneath (above) a target value. We choose the reduction factor to be
proportional to the deviation between issue_delay and our target
delay, bounded by half (0.5). DISPATCH_TIMEOUT causes a more
drastic reaction, accelerating convergence when new local transfers
begin (Â§7.4). The policy to control the CWND corresponding to
remote congestion is similar, although remote_delay is compared
against a distinct delay target, and TIMEOUT and NACK cause a 10Ã—
reduction in remote CWND, as these explicitly signal congestion
specific to the network or a remote NIC.
ğ‘…ğ‘‡ğ‘‡
1RMA uses the more restrictive of the two CWNDs to: (1) set
a limit on the number of outstanding ops in the network, and (2)
compute the opsâ€™ issue rate as ğ‘‚ğ‘ğ‘†ğ‘–ğ‘§ğ‘’Ã—ğ¶ğ‘Š ğ‘ ğ·
, which 1RMA en-
forces through the CommandExecutor in Figure 4. We are yet to
investigate how 1RMA interacts with other transports. As such, we
assign 1RMA its own traffic class on our production fabric.
6.1 Parameter choices
1RMA has several key parameters that impact CC effectiveness as
well as 1RMAâ€™s overall performance and CPU overhead.
(1) Op Size: The 1RMA NIC bounds the size of individual ops. A
small size ensures frequent feedback to, and precise reaction from,
1RMA CC. But, too small a size imposes high CPU overhead. Our
benchmarks indicate that a single core can drive 6M ops/sec with
our full software stack, which is twice what is needed for 100Gbps
line rate with 4KB-sized ops. For large transfers, 4KB-sized chunks
strike a balance between CPU overhead and CC responsiveness.
(2) Dispatch Timeout: An overly-full solicitation window can lead
to a DISPATCH_TIMEOUT, signalled when an op cannot enter service
locally within a bounded time interval. To software, this outcome
715
1RMA: Re-envisioning Remote Memory Access for Multi-tenant Datacenters
SIGCOMM â€™20, August 10â€“14, 2020, Virtual Event, NY, USA
signals local congestion. We tune the timeout value to 10ğœ‡s, so
that it is comparable to the block/wake time of a CPU thread. This
timeout ensures local feedback is delivered immediately, and it
enables timely CC responsiveness.
(3) Timeout: Delays and drops due to congestion trigger a more
generic TIMEOUT outcome on affected operations. The bounded size
of the solicitation window calls for shorter timeouts to help quickly
reclaim window capacity. On the other hand, longer timeouts help
ensure that delayed transfers are accepted at the receiving side and
do not waste the senderâ€™s bandwidth. Because CC is effective at
making timeouts rare, we tune these timeouts to 4-5Ã— the fabric RTT.
Timing out early is tolerable in 1RMA, because small op size bounds
wasted work in the event of a false positive.
(4) NACK Threshold: The 1RMA NIC generates NACKs when the
depth of inbound request queues cross above a configured threshold.
We set the threshold to (ğ‘‡ ğ¼ ğ‘€ğ¸ğ‘‚ğ‘ˆğ‘‡âˆ’ğ‘…ğ‘‡ğ‘‡âˆ’ğ·ğ¼ğ‘†ğ‘ƒğ´ğ‘‡ğ¶ğ»_ğ‘‡ ğ¼ ğ‘€ğ¸ğ‘‚ğ‘ˆğ‘‡)
, so
that NACKs are generated when a data response is unlikely to reach
the initiator prior to a TIMEOUT, thereby preventing bandwidth waste
at the serving NIC.
ğµğ‘ğ‘›ğ‘‘ğ‘¤ğ‘–ğ‘‘ğ‘¡â„
7 Evaluation
We evaluate 1RMAâ€™s distinctive features via testbed experiments
and simulations. We report absolute performance (e.g., 100Gbps) for
context, but raw throughput is mostly a function of hardware genera-
tion. We designed our prototype hardware while 100Gbps network
speeds were state of the art. We expect our approach will generalize
to higher speed networks. To highlight the architectural choices in
the 1RMA NIC, much of our evaluation focuses on choices relating
to stability, utility, and predictability. We show that:
â€¢ 1RMA offers excellent performance in common cases, but re-
mains stable and predictable under less-common (but important)
failure cases (Â§7.1).
â€¢ 1RMAâ€™s intrinsic support for isolation and prioritization via
independent, small-sized ops and software-driven resource allo-
cation effectively prevents applications from monopolizing the
network (Â§7.2).
â€¢ 1RMAâ€™s hardware support for encryption key rotation mini-
â€¢ Supported by hardware, 1RMAâ€™s CC converges to fair band-
width shares in the presence of competing applications almost
immediately (Â§7.4).
â€¢ 1RMAâ€™s solicitation rules prevent goodput loss due to tran-
sients (sudden dynamic changes), reacting at hardware speeds
as software CC converges (Â§7.5).
mizes client-observable disruption (Â§7.3).
Baselines. We compare against standard RDMA and Pony Express
(Pony) [28]. With RDMA, our purpose is to highlight the implica-
tions of the required and standard behaviors of any compliant RNIC,
with Mellanox NICs as examples. Pony, Googleâ€™s software-defined
NIC, represents a state-of-the-art datacenter networking alternative,
most similar to 1RMA in its objectives. Pony supports one-sided ops
by means of a userspace networking stack on the hosts. In all our
experiments, we limit each networking stack to (at most) a single
host CPU for network transport processing; this tends to limit Pony
to 40Gbps maximum throughput (bidirectional). With larger CPU
allocations, Pony performance scales commensurately.
Figure 8: Latency vs. offered load for KVCS.
Figure 9: Observed latency during KVCS replica failure.
Testbed. Our 1RMA and Pony testbed consists of 40 Intel Skylake-
based servers connected via the lowest-latency switches available
to us. To study standard RDMA, we use 40Gbps Mellanox CX-3s
for at-scale and 100Gbps CX-5s for point-to-point experiments. Our
40-node CX-3 testbed is from an older production generation, based
on Intel Haswell. RDMA experiments run with PFC between the
NIC and top-of-rack switch. Our evaluation uses a 1RMA hardware
implementation that uses region keys, not derived region keys, for
encryption and authentication.
Simulation. We augment our testbed findings with simulations cov-
ering various behaviors otherwise difficult to isolate.
7.1 Applications and Workloads
As performance is typically the goal of systems built atop RMA
infrastructures, we first evaluate the extent to which 1RMA improves
performance of (a) a key/value caching system and (b) a synthetic
uniform random workload.
In-Memory Key/Value Caching Service (KVCS). We modified a
Google production caching system (similar to [29]) to use 1RMA
and Pony for cache lookups. Although the production KVCS op-
erates at larger scales, for this controlled study we constrain it to
ten nodes and evaluate its performance under varying load. The
servers in this setting are outnumbered by the clients, such that the
bottleneck is the serving-side NIC. Figure 8 plots performance per
server, which ramps to the maximum practical throughput of each
underlying RMA implementation.
1RMA outperforms (two-sided) Pony; with 1RMA we even ob-