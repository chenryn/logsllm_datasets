600
400
200
140
45
0
0
5
10
15
20
25
Throughput (1,000 requests per second)
Fig. 6. Handling totally ordered write requests in the local network.
Fig. 7. Handling totally ordered write requests with network delay.
2) Read Optimizations: We measure the performance of the
fast-read cache using read-only requests with different payload
sizes: 10 B / 256 B, 10 B / 1 KB, 10 B / 4 KB and 10 B / 8 KB
for request / reply messages, respectively. The baseline system
implements a PBFT-like read optimization approach [1], where
read requests are directly forwarded to the followers for
execution without being ordered. For read-only workloads,
this approach can be very effective as there are no concurrent
state transitions to create conﬂicts in the read results.
Figure 8 shows the results of handling read-only requests in
the local network. On the one hand, with small requests (10 B),
the fast message authentication cannot compensate the overhead
of the server-side reply voter. The overhead with 256 B reply is
as high as 115%. On the other hand, along with the increasing
reply size, the effect of fast authentication becomes more visible.
With 4 KB replies etroxy can already overtake the baseline, and
at 8 KB we can observe about 30% throughput improvement.
The result of the measurement with a network delay is
shown in Figure 9. Although the server-side reply voter adds
overhead to Troxy, the extra network delay has less impact on
Troxy’s performance. Compared to the baseline, with 256 B
replies etroxy only incurs a 33% performance degradation with
network delay, compared with 115% without network delay.
In addition, as the fast-read cache only needs to transfer the
hash of the reply between replicas for a fast-read operation
instead of a full reply, this further reduces the authentication
and transmission cost. When the reply size is above 1 KB,
etroxy outperforms the baseline by at least 15%.
3) Concurrency Handling: In this scenario, 1% of write
requests are generated among the reads, to introduce concurrent
state transitions during fast-read operations. Due to different
read optimization approaches, the 1% write workload results
in different read conﬂict rates for the baseline and Troxy
(only etroxy is evaluated in this scenario). For the baseline,
120
100
80
60
40
20
0
400
350
300
250
200
150
100
50
)
s
m
(
y
c
n
e
t
a
L
)
s
m
(
y
c
n
e
t
a
L
0
40
BL-10B/1KB-r
ctroxy-10B/1KB-r
etroxy-10B/1KB-r
150
100
Throughput (1,000 requests per second)
250
200
300
350
400
BL-10B/8KB-r
ctroxy-10B/8KB-r
etroxy-10B/8KB-r
BL-10B/256B-r
ctroxy-10B/256B-r
etroxy-10B/256B-r
200
100
600
Throughput (1,000 requests per second)
400
300
500
BL-10B/4KB-r
ctroxy-10B/4KB-r
etroxy-10B/4KB-r
140
120
100
80
60
40
20
0
50
600
500
400
300
200
100
60
80
180
Throughput (1,000 requests per second)
100
120
140
160
0
40
50
60
70
80
90
100
Throughput (1,000 requests per second)
)
s
m
(
y
c
n
e
t
a
L
)
s
m
(
y
c
n
e
t
a
L
300
250
200
150
100
50
0
600
500
400
300
200
100
0
BL-10B/256-la-r
ctroxy-10B/256-la-r
etroxy-10B/256-la-r
50
0
300
Throughput (1,000 requests per second)
100
150
200
250
BL-10B/4KB-la-r
ctroxy-10B/4KB-la-r
etroxy-10B/4KB-la-r
10 20 30 40 50 60 70 80 90 100
0
Throughput (1,000 requests per second)
300
250
200
150
100
50
0
800
700
600
500
400
300
200
100
0
BL-10B/1KB-la-r
ctroxy-10B/1KB-la-r
etroxy-10B/1KB-la-r
50
0
250
Throughput (1,000 requests per second)
200
100
150
BL-10B/8KB-la-r
ctroxy-10B/8KB-la-r
etroxy-10B/8KB-la-r
5
10 15 20 25 30 35 40 45 50 55
0
Throughput (1,000 requests per second)
Fig. 8. Handling read-only requests in the local network.
Fig. 9. Handling read-only requests with network delay.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:31:39 UTC from IEEE Xplore.  Restrictions apply. 
67
BL-10B/256B-la-ord
BL-10B/256B-la-conf
Tr-10B/256B-la-ord
Tr-10B/256B-la-conf
Tr-10B/256B-la-opt
600
500
400
300
200
100
)
s
m
(
y
c
n
e
t
a
L
−20 0 20 40 60 80 100120140160180200220240
0
)
s
m
(
y
c
n
e
t
a
L
1,000
800
600
400
200
0
Throughput (1,000 requests per second)
BL-10B/4KB-la-ord
BL-10B/4KB-la-conf
Tr-10B/4KB-la-ord
Tr-10B/4KB-la-conf
Tr-10B/4KB-la-opt
10
0
Throughput (1,000 requests per second)
50
30
40
20
60
600
500
400
300
200
100
0
1,400
1,200
1,000
800
600
400
200
0
BL-10B/1KB-la-ord
BL-10B/1KB-la-conf
Tr-10B/1KB-la-ord
Tr-10B/1KB-la-conf
Tr-10B/1KB-la-opt
20
0
Throughput (1,000 requests per second)
80 100 120 140 160 180
40
60
BL-10B/8KB-la-ord
BL-10B/8KB-la-conf
Tr-10B/8KB-la-ord
Tr-10B/8KB-la-conf
Tr-10B/8KB-la-opt
5
0
Throughput (1,000 requests per second)
30
10
15
20
25
35
40
Fig. 10. Handling read conﬂicts with network delay.
nearly 50% of reads return conﬂicting results and have to be
ordered for a second time of processing, adding substantial
extra overhead to the system. As for Troxy, the fast-read cache
acts in a conservative way: When it uses write requests to
invalidate existing cache entries, the later read requests will be
ordered to prevent conﬂicts. This way, the observed conﬂict
rate goes down to 14%.
We also conducted a measurement where no optimization
is applied so that all reads are ordered, to get a reference
throughput of each system for comparison. Figure 10 illustrates
that the overhead of having 50% read conﬂicts contributes to
the signiﬁcant performance loss of the baseline, resulting in
the read optimization to only achieve half of the reference
throughput. For Troxy, the 14% read conﬂicts also decreases
performance to a point that is slightly lower than its reference
throughput. Therefore, we further optimized the approach to
monitor the conﬂict rate inside Troxy in order to ensure that
once the conﬂict rate goes beyond a certain threshold, Troxy
will automatically switch to the total-order mode where all
requests will be ordered (see Section IV-B). This threshold
can be learned by sampling the system to determine at which
conﬂict rate the beneﬁts gained by fast reads will disappear.
This way, the optimized fast-read cache can guarantee the
lower-bound performance in case of frequent conﬂicts.
D. HTTP Service
In addition to the microbenchmark, we created a simple,
replicated HTTP service that handles HTTP GET and POST
requests and returns the queried or modiﬁed pages as responses.
Its performance is measured with the HTTP benchmarking tool
Apache JMeter [39]. As we are interested in evaluating the
overhead of using a BFT system and the trusted subsystem
in a latency-sensitive application, we ensure that JMeter is
conﬁgured not to saturate the replicas, launching 100 clients
to issue a total of 500 requests per second.
We measure the performance of the HTTP service in three
implementations: (1) with the baseline protocol; (2) with
SUMMARY OF READ OPTIMIZATION APPROACHES.
TABLE I
BL
Prophecy
Troxy
Replica