b
N
p = U m =
.
(2)
The number of marked bits on the bit vector should be pro-
portional to the number of active connections c inside a time
unit of Te. If we assume that the results of the hash func-
tions seldom collide when the utilization of the bit vector is
low, Equation 2 can be rewritten as
(cid:7) c · m
(cid:8)m
N
p (cid:9)
.
(3)
Given a bit vector size N and the expected max number of
active connections c, then to minimize the desired penetra-
tion probability p, we differentiate Equation 3 and get
(cid:8)(cid:8)
· m
(cid:7) c
N
.
(4)
(cid:8)m(cid:7)
(cid:7) c
N
p(cid:2) =
· m
1 + ln
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:50:17 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007Thus, m that minimizes the penetration probability p can be
obtained by solving 1 + ln( c
N · m) = 0, which is
m = e−1 · N
c
,
(5)
where e is the base for the natural logarithm. By replacing
m in Equation 3 with e−1·N
c when m minimizes the pene-
tration probability p, the ratio of the expected max number
of active connections c should satisfy
c
N
≤ − 1
e ln p
.
(6)
For example, if we adopt a bitmap ﬁlter of size N = 220
(about 1-million bits) with k = 4, and Δt = 5 seconds, and
set the desired penetration probability to be roughly 10%,
5%, and 1%, the number of active connections inside a time
unit Te = 20 seconds should be less than 167K, 125K, and
83K, respectively. Compared with our trace data, which has
only average 15K active connections inside a time unit of 20
seconds, these upper bounds are much higher than the actual
trafﬁc. The number of used hash functions m in the setup
can be 3, and the memory space required by the bitmap ﬁlter
is only (k × N)/8 = 512K bytes.
5.2 Performance
The bitmap ﬁlter is efﬁcient because almost all opera-
tions can be performed in constant time. The processing
time for an outbound packet is O(m× th) + O(m· k× tm),
where m is the number of used hash functions, th is the
time taken to execute a hash function, k is the number of
bit vectors to be marked, and tm is the processing time
to mark a bit. Processing inbound packets is simpler than
for outbound packets. The required processing time is
O(m × th) + O(m × tc) where tc is the processing time
need to check whether a bit on a bit vector is marked or not.
Inbound packet processing is also a constant time opera-
tion. When an inbound packet is considered to be dropped,
the bitmap ﬁlter drops the packet according to the dropping
probability Pd. Computing the Pd requires only the knowl-
edge of current bandwidth throughput, which is an essential
component in off-the-shelf network devices.
The most time consuming operation may be the b.rotate
algorithm, which executes every Δt seconds. The algorithm
ﬁrst advances the current index idx to set to the next bit
vector, and then resets all bits in the last bit vector to zero.
Thus, the operation is proportional to the size of a bit vec-
tor, which is O(n). However, since the memory space of a
bit vector is ﬁxed and continuous, implementing such an al-
gorithm in software is simple and efﬁcient. As all the com-
ponents used in the algorithm already have corresponding
hardware implementations, it is also easy to accelerate the
algorithm by using hardware coprocessors.
5
.
3
0
.
3
5
.
2
0
.
2
5
.
1
0
.
1
)
%
(
r
e
t
l
i
f
p
a
m
t
i
b
e
h
t
f
o
e
t
a
r
p
o
r
D
1.0
1.5
2.0
2.5
3.0
Drop rate of the SPI filter (%)
Figure 8. Comparison of the packet drop
rates of the SPI and the bitmap ﬁlters. The
gray-dashed line has a slope of 1.0.
5.3 Simulation with the Packet Trace
We also perform several simulations to verify the effec-
tiveness of the bitmap ﬁlter. A bitmap ﬁlter and an SPI-
based ﬁlter are both implemented. The input to both ﬁlters
is the packet trace used in Section 3.3. First, we compare
the packet drop rate of the two ﬁlters. The SPI ﬁlter is set to
delete idle connections after 240 seconds, which is the de-
fault TIME WAIT timeout used in the Microsoft windows
operating system The bitmap ﬁlter is conﬁgured as follows:
N = 220, k = 4, Te = 20, Δt = 5, and drop all inbound
packets without states. This constructs a 512K-byte bitmap
ﬁlter that handles the out-in packet latency shorter than 20
seconds. As Figure 8 shows, the ﬁlters have similar packet
drop rates, and the gray-dashed line has a slope of 1.0. The
SPI ﬁlter has an average drop rate of 1.56% compared to
1.51% for the bitmap ﬁlter. This is because that the SPI
ﬁlter knows the exact time of closed connections and can
therefore drop packets precisely than the bitmap ﬁlter.
The second simulation is to show the effectiveness of
the bitmap ﬁlter on the same packet trace data. The bitmap
ﬁlter now monitors the bandwidth throughput of upstream
trafﬁc and blocks incoming connections when the uplink
bandwidth throughput is high. The dropping probability Pd
is generated by Equation 1 with a upper bound bandwidth
limit H of 100Mbps and a lower bound bandwidth limit L
of 50Mbps. To simulate a blocked connection, when an in-
bound packet is decided to be dropped by the bitmap ﬁlter,
the socket pair σ of that packet is stored and all the future
packets that match any stored σ or σ are all dropped with-
out checking the bitmap. The conﬁguration of the bitmap
ﬁlter tries to control peer-to-peer upload trafﬁc below an up-
per bound of 100Mbps. Figure 9-a and Figure 9-b show the
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:50:17 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007a. Throughput (Without Filtering)
the normal operations of traditional Internet services.
)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
0
0
2
0
0
1
0
0
5000
10000 15000 20000 25000
Time (in seconds)
b. Throughput (Filtered)
0
5
1
0
5
0
)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
0
5000
10000 15000 20000 25000
Time (in seconds)
Figure 9. The performance of the bitmap ﬁlter
to limit upload trafﬁc.
original and the ﬁltered bandwidth throughput, respectively.
In the two ﬁgures, the black line indicates the downlink
throughput and the gray part indicates the uplink through-
put. It should be noted that both parts of the downlink and
uplink trafﬁc are limited. This is because some download
peer-to-peer trafﬁc are transfered in different inbound con-
nections. Since the simulation is done with replayed packet
trace, as the simulation is unable to block the outbound con-
nections that may triggered by previously blocked inbound
requests, the effect of the trafﬁc ﬁltering is limited. We be-
lieve that the ﬁlter can perform better in a real network envi-
ronment. The result of simulation also shows that the 512K
bytes {4×220}-bitmap ﬁlter with 3 hash functions can prop-
erly limit uplink trafﬁc for the small- or medium-scale client
network.
6 Conclusions
The core spirit of peer-to-peer applications is to share
with the public. Thus, a client host that running peer-to-peer
applications always generates a considerable amount of up-
load trafﬁc, which should be limited in a client network.
However, with randomly selected port numbers and the use
of protocol encryption, peer-to-peer trafﬁc is hard to iden-
tify and manage. As the upload trafﬁc are usually triggered
by inbound request, in this paper, we propose a bitmap ﬁl-
ter to bound the peer-to-peer upload trafﬁc by controlling
inbound requests. The proposed algorithm requires only
constant storage and computation power. Analyses and sim-
ulations show that with a small amount of resources, an ISP
can efﬁciently prevent the peer-to-peer trafﬁc from affecting
References
[1] S. Saroiu, P. K. Gummadi, and S. D. Gribble, “A
measurement study of peer-to-peer ﬁle sharing sys-
tems,” in Proceedings of the SPIE/ACM Conference
on Multimedia Computing and Networking (MMCN),
Jan. 2002.
[2] S. Sen and J. Wang, “Analyzing peer-to-peer trafﬁc
across large networks,” IEEE Transactions on Net-
working, vol. 12, no. 2, pp. 219–232, Apr. 2004.
[3] T. Karagiannis, A. Broido, N. Brownlee, K. Claffy,
and M. Faloutsos, “Is P2P dying or just hiding?”
in Proceedings of IEEE Global Telecommunications
Conference, vol. 3.
IEEE, Nov. 2004, pp. 1532–1538.
[4] T. Karagiannis, A. Broido, M. Faloutsos, and K. claffy,
“Transport layer identiﬁcation of P2P trafﬁc,” in IMC
’04: Proceedings of the 4th ACM SIGCOMM confer-
ence on Internet measurement. New York, NY, USA:
ACM Press, 2004, pp. 121–134.
[5] N. Leibowitz, A. Bergman, R. Ben-Shaul, and
A. Shavit, “Are ﬁle swapping networks cacheable?
characterizing P2P trafﬁc,” in Proceedings of the 7th
International Workshop on Web Content Caching and
Distribution (WCW), Aug. 2002, pp. 121–134.
[6] S. Sen, O. Spatscheck, and D. Wang, “Accurate, scal-
able in-network identiﬁcation of P2P trafﬁc using ap-
plication signatures,” in WWW ’04: Proceedings of
the 13th international conference on World Wide Web.
New York, NY, USA: ACM Press, 2004, pp. 512–521.
[7] V. Jacobson, C. Leres, and S. McCanne, TCP-
[Online]. Available:
repository.
DUMP public
http://www.tcpdump.org/
[8] J. Levandoski, E. Sommer, and M. Strait, Application
Layer Packet Classiﬁer for Linux. [Online]. Available:
http://l7-ﬁlter.sourceforge.net/
[9] B. H. Bloom, “Space/time trade-offs in hash cod-
ing with allowable errors,” Communication of ACM,
vol. 13, no. 7, pp. 422–426, 1970.
[10] S. Floyd and V. Jacobson, “Random early detec-
tion gateways for congestion avoidance,” IEEE/ACM
Transactions on Networking, vol. 1, no. 4, pp. 397–
413, 1993.
[11] B. Ford, P. Srisuresh, and D. Kegel, “Peer-to-peer
communication across network address translators,” in
USENIX Annual Technical Conference, Apr. 2005.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:50:17 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007