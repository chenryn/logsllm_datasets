deﬁnition of the NTT, we can now deﬁne the format
of the exchanged messages. In both (b,seed) and (u,r)
the polynomial is transmitted in the NTT domain (as in
works like [79,81]). Polynomials are encoded as an array
of 1792 bytes, in a compressed little-endian format. The
encoding of seed is straight-forward as an array of 32
bytes, which is simply concatenated with the encoding
of b. Also the encoding of r is fairly straight-forward:
it packs four 2-bit coefﬁcients into one byte for a to-
tal of 256 bytes, which are again simply concatenated
with the encoding of u. We denote these encodings to
byte arrays as encodeA and encodeB and their inverses
as decodeA and decodeB. For a description of our key-
exchange protocol including encodings and with explicit
NTT and NTT−1 transformations, see Protocol 3.
7.2 Portable C implementation
This paper is accompanied by a C reference implemen-
tation described in this section and an optimized imple-
mentation for Intel and AMD CPUs described in the next
section. The main emphasis in the C reference imple-
mentation is on simplicity and portability.
It does not
use any ﬂoating-point arithmetic and outside the Kec-
cak (SHA3-256 and SHAKE-128) implementation only
needs 16-bit and 32-bit integer arithmetic. In particular,
the error-recovery mechanism described in Section 5 is
implemented with ﬁxed-point (i.e., integer-) arithmetic.
Furthermore, the C reference implementation does not
make use of the division operator (/) and the modulo op-
erator (%). The focus on simplicity and portability does
not mean that the implementation is not optimized at all.
On the contrary, we use it to illustrate various optimiza-
tion techniques that are helpful to speed up the key ex-
change and are also of independent interest for imple-
menters of other ideal-lattice-based schemes.
NTT optimizations. All polynomial coefﬁcients are
represented as unsigned 16-bit integers. Our in-place
NTT implementation transforms from bit-reversed to
natural order using Gentleman-Sande butterﬂy oper-
ations [27, 37]. One would usually expect that each
NTT is preceded by a bit-reversal, but all inputs to
NTT are noise polynomials that we can simply consider
as being already bit-reversed; as explained earlier, the
NTT−1 operation still involves a bit-reversal. The core
of the NTT and NTT−1 operation consists of 10 layers
of transformations, each consisting of 512 butterﬂy
operations of the form described in Listing 2.
Montgomery arithmetic and lazy reductions. The per-
formance of operations on polynomials is largely deter-
mined by the performance of NTT and NTT−1. The
main computational bottleneck of those operations are
5120 butterﬂy operations, each consisting of one addi-
tion, one subtraction and one multiplication by a precom-
puted constant. Those operations are in Zq; recall that q
is a 14-bit prime. To speed up the modular-arithmetic
operations, we store all precomputed constants in Mont-
gomery representation [71] with R = 218, i.e., instead
of storing ωi, we store 218ωi (mod q). After a multi-
plication of a coefﬁcient g by some constant 218ωi, we
can then reduce the result r to gωi (mod q) with the fast
Montgomery reduction approach. In fact, we do not al-
ways fully reduce modulo q, it is sufﬁcient if the result of
the reduction has at most 14 bits. The fast Montgomery
reduction routine given in Listing 1a computes such a re-
duction to a 14-bit integer for any unsigned 32-bit integer
in {0, . . . ,2 32 − q(R− 1)− 1}. Note that the speciﬁc im-
plementation does not work for any 32-bit integer; for
example, for the input 232 − q(R− 1) =1073491969 the
addition a=a+u causes an overﬂow and the function re-
turns 0 instead of the correct result 4095. In the following
we establish that this is not a problem for our software.
Aside from reductions after multiplication, we also
need modular reductions after addition. For this task
we use the “short Barrett reduction” [9] detailed in List-
ing 1b. Again, this routine does not fully reduce modulo
q, but reduces any 16-bit unsigned integer to an integer
of at most 14 bits which is congruent modulo q.
In the context of the NTT and NTT−1, we make sure
that inputs have coefﬁcients of at most 14 bits. This al-
lows us to avoid Barrett reductions after addition on ev-
ery second level, because coefﬁcients grow by at most
one bit per level and the short Barrett reduction can
handle 16-bit inputs. Let us turn our focus to the in-
put of the Montgomery reduction (see Listing 2). Be-
fore subtracting a[j+d] from t we need to add a mul-
tiple of q to avoid unsigned underﬂow. Coefﬁcients
never grow larger than 15 bits and 3· q = 36867 > 215,
so adding 3 · q is sufﬁcient. An upper bound on the
USENIX Association  
25th USENIX Security Symposium  337
11
Parameters: q = 12289 > 18;
}
(b) Short Barrett reduction.
uint16_t bred(uint16_t a) {
uint32_t u;
u = ((uint32_t) a * 5) >> 16;
a -= u * 12289;
return a;
}
cak permutation and slightly modiﬁed code taken from
the “TweetFIPS202” implementation [18] for everything
else.
The sampling of centered binomial noise polynomi-
als is based on a fast PRG with a random seed from
/dev/urandom followed by a quick summation of 16-
bit chunks of the PRG output. Note that the choice of
the PRG is a purely local choice that every user can
pick independently based on the target hardware archi-
tecture and based on routines that are available anyway
(for example, for symmetric encryption following the
key exchange). Our C reference implementation uses
ChaCha20 [12], which is fast, trivially protected against
timing attacks, and is already in use by many TLS clients
and servers [57, 58].
Fast random sampling. As a ﬁrst step before perform-
ing any operations on polynomials, both Alice and Bob
need to expand the seed to the polynomial a using
SHAKE-128. The implementation we use is based on
the “simple” implementation by Van Keer for the Kec-
7.3 Optimized AVX2 implementation
Intel processors since the “Sandy Bridge” generation
support Advanced Vector Extensions (AVX) that oper-
ate on vectors of 8 single-precision or 4 double-precision
338  25th USENIX Security Symposium 
USENIX Association
12
ﬂoating-point values in parallel. With the introduction
of the “Haswell” generation of CPUs, this support was
extended also to 256-bit vectors of integers of various
sizes (AVX2). It is not surprising that the enormous com-
putational power of these vector instructions has been
used before to implement very high-speed crypto (see,
for example, [14, 16, 43]) and also our optimized refer-
ence implementation targeting Intel Haswell processors
uses those instructions to speed up multiple components
of the key exchange.
NTT optimizations. The AVX instruction set has been
used before to speed up the computation of lattice-based
cryptography, and in particular the number-theoretic
transform. Most notably, Güneysu, Oder, Pöppelmann
and Schwabe achieve a performance of only 4480 cycles
for a dimension-512 NTT on Intel Sandy Bridge [46].
For arithmetic modulo a 23-bit prime, they represent co-
efﬁcients as double-precision integers.
We experimented with multiple different approaches
to speed up the NTT in AVX. For example, we vector-
ized the Montgomery arithmetic approach of our C ref-
erence implementation and also adapted it to a 32-bit-
signed-integer approach.
In the end it turned out that
ﬂoating-point arithmetic beats all of those more sophisti-
cated approaches, so we are now using an approach that
is very similar to the approach in [46]. One computation
of a dimension-1024 NTT takes 8448 cycles, unlike the
numbers in [46] this does include multiplication by the
powers of γ and unlike the numbers in [46], this excludes
a bit-reversal.
Fast sampling. Intel Haswell processors support the
AES-NI instruction set and for the local choice of noise
sampling it is obvious to use those. More speciﬁcally,
we use the public-domain implementation of AES-256 in
counter mode written by Dolbeau, which is included in
the SUPERCOP benchmarking framework [17]. Trans-
formation from uniform noise to the centered binomial
is optimized in AVX2 vector instructions operating on
vectors of bytes and 16-bit integers.
For the computation of SHAKE-128 we use the same
code as in the C reference implementation. One might
expect that architecture-speciﬁc optimizations (for exam-
ple, using AVX instructions) are able to offer signiﬁcant
speedups, but the benchmarks of the eBACS project [17]
indicate that on Intel Haswell, the fastest implementation
is the “simple” implementation by Van Keer that our C
reference implementation is based on. The reasons that
vector instructions are not very helpful for speeding up
SHAKE (or, more generally, Keccak) are the inherently
sequential nature and the 5 × 5 dimension of the state
matrix that makes internal vectorization hard.
Error recovery. The 32-bit integer arithmetic used by
the C reference implementation for HelpRec and Rec
is trivially 8-way parallelized with AVX2 instructions.
With this vectorization, the cost for HelpRec is only 3404
cycles, the cost for Rec is only 2804 cycles.
8 Benchmarks and comparison
In the following we present benchmark results of our
software. All benchmark results reported in Table 2 were
obtained on an Intel Core i7-4770K (Haswell) running
at 3491.953 MHz with Turbo Boost and Hyperthreading
disabled. We compiled our C reference implementation
with gcc-4.9.2 and ﬂags -O3 -fomit-frame-pointer
-march=corei7-avx -msse2avx. We compiled our
optimized AVX implementation with clang-3.5 and ﬂags
-O3 -fomit-frame-pointer -march=native.
As described in Section 7, the sampling of a is not
running in constant time; we report the median run-
ning time and (in parentheses) the average running time
for this generation, the server-side key-pair generation
and client-side shared-key computation; both over 1000
runs. For all other routines we report the median of
1000 runs. We built the software from [20] on the
same machine as ours and—like the authors of [20]—
used openssl speed for benchmarking their software
and converted the reported results to approximate cycle
counts as given in Table 2.
Comparison with BCNS and RSA/ECDH. As previ-
ously mentioned, the BCNS implementation [20] also
uses the dimension n = 1024 but the larger modulus
q = 232 − 1 and the Gaussian error distribution with
Gaussian parameter σ = 8/√2π = 3.192. When the au-
thors of BCNS integrated their implementation into SSL
it only incurred a slowdown by a factor of 1.27 compared
to ECDH when using ECDSA signatures and a factor of
1.08 when using RSA signatures with respect to the num-
ber of connections that could be handled by the server.
As a reference, the reported cycle counts in [20] for a
nistp256 ECDH on the client side are 2 160 000 cycles
(0.8 ms @2.77 GHz) and on the server side 3 221 288
cycles (1.4 ms @2.33 GHz). These numbers are obvi-
ously not state of the art for ECDH software. Even on
the nistp256 curve, which is known to be a far-from-
optimal choice, it is possible to achieve cycle counts of
less than 300000 cycles for a variable-basepoint scalar