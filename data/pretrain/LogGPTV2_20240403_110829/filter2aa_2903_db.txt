GpuTest程序的压力测试时得到的信息。
清单11-3 渲染引擎的请求队列和执行状态（较多大粒度任务）
rcs0 requests: 5
    15a2 [2:8d4] prio=2147483647 @ 1064ms: Xorg[1067]/1
    15a3 [8:19] prio=2147483647 @ 776ms: GpuTest[2365]/1
    15a4 [2:8d5] prio=2147483647 @ 772ms: Xorg[1067]/1
    15a5 [8:1a] prio=2147483647 @ 8ms: GpuTest[2365]/1
    15a6 [2:8d6] prio=2147483647 @ 8ms: Xorg[1067]/1
GT awake? yes
Global active requests: 11
rcs0
    current seqno 15a4, last 15a6, hangcheck 158d [1304 ms], inflight 11
    Requests:
        first  15a2 [2:8d4] prio=2147483647 @ 1064ms: Xorg[1067]/1
        last   15a6 [2:8d6] prio=2147483647 @ 8ms: Xorg[1067]/1
        active 15a5 [8:1a] prio=2147483647 @ 8ms: GpuTest[2365]/1
        [head 10a8, postfix 1100, tail 1128, batch 0x00000000_01dcd000]
    RING_START: 0x0002d000 [0x0002d000]
    RING_HEAD:  0x000010e4 [0x00001000]
    RING_TAIL:  0x00001128 [0x00001128]
    RING_CTL:   0x00003001 []
    ACTHD:  0x00000000_01dcdbbc
    BBADDR: 0x00000000_01dcdbbd
    Execlist status: 0x00044052 00000008
    Execlist CSB read 3, write 3
        ELSP[0] count=1, rq: 15a5 [8:1a] prio=2147483647 @ 8ms: GpuTest[23
65]/1
        ELSP[1] count=1, rq: 15a6 [2:8d6] prio=2147483647 @ 8ms: Xorg[1067
]/1
        Q 0 [2:8d7] prio=1024 @ 4ms: Xorg[1067]/1
        Q 0 [2:8d8] prio=1024 @ 4ms: Xorg[1067]/1
        Q 0 [4:cb1] prio=1024 @ 4ms: compiz[1955]/1
        Q 0 [4:cb2] prio=1024 @ 4ms: compiz[1955]/1
        Q 0 [4:cb3] prio=1024 @ 4ms: compiz[1955]/1
        Q 0 [8:1b] prio=0 @ 4ms: GpuTest[2365]/1
    GpuTest [2365] waiting for 15a5
清单11-3的前6行是i915_gem_request的信息，当时有5个请求，后
面是i915_engine_info的信息，显示有11个请求，意味着在两个观察点之
间新增了6个请求。当时正在执行的是15a4号，最近提交给硬件的是
15a6号，可以看到它还在ELSP端口[1]排队。在ELSP端口0上的15a5号请
求正处于活跃状态。因为GpuTest程序在频繁提交新的请求，所以可以
看到ELSP端口信息后列出了4ms前刚刚发射到软件队列里的6个新请
求。
最后再看频繁提交较小粒度任务的情况，执行GpuTest的BenchMark
测试，输出结果如清单11-4所示。
清单11-4 渲染引擎的请求队列和执行状态（较多小粒度任务）
Global active requests: 14
rcs0
    current seqno bf5b3, last bf5b7, hangcheck bf5ad [24 ms], inflight 14
    Requests:
        first  bf5af [b:be0] prio=2147483647 @ 108ms: GpuTest[6897]/1
        last   bf5b7 [2:4f820] prio=2147483647 @ 56ms: Xorg[1058]/1
        active bf5b4 [b:be1] prio=2147483647 @ 88ms: GpuTest[6897]/1
        [head 3428, postfix 3480, tail 34a0, batch 0x00000000_00a1f000]
    RING_START: 0x03543000 [0x03543000]
    RING_HEAD:  0x00003464 [0x00003380]
    RING_TAIL:  0x000035a8 [0x000035a8]
    RING_CTL:   0x00003001 []
    ACTHD:  0x00000000_00a20264
    BBADDR: 0x00000000_00a20265
    Execlist status: 0x00044052 0000000b
    Execlist CSB read 4, write 4
        ELSP[0] count=1, rq: bf5b6 [b:be3] prio=2147483647 @ 88ms: GpuTest
[6897]/1
        ELSP[1] count=1, rq: bf5b7 [2:4f820] prio=2147483647 @ 56ms: Xorg[
1058]/1
        Q 0 [7:182e5] prio=0 @ 56ms: GpuTest[5555]/1
        Q 0 [b:be4] prio=0 @ 40ms: GpuTest[6897]/1
        Q 0 [b:be5] prio=0 @ 40ms: GpuTest[6897]/1
        Q 0 [b:be6] prio=0 @ 40ms: GpuTest[6897]/1
        Q 0 [2:4f821] prio=0 @ 4ms: Xorg[1058]/1
    GpuTest [6897] waiting for bf5b4
    Xorg [1058] waiting for bf5b7
值得注意的是，清单中间的活跃请求（active行）为bf5b4，但是
ELSP端口行显示的并不是它，而是bf5b6和bf5b7，导致这种信息不一致
的原因应该是CPU显示两个信息的时间差。在那个时间间隙里，bf5b4
已经执行完毕，而且CPU端又提交了新的请求到ELSP端口。
11.6 GuC和通过GuC提交任务
从Broadwell（Gen8）开始，Gen GPU中都包含了一个x86架构
（Minute IA）的微处理器，名叫GPU微处理器（GPU Micro
Controller），简称GuC。
GuC的主要任务是调度Gen的执行引擎，为Gen提供了一种新的调
度接口给上层软件。增加GuC的目的是提供新的调度方式，逐步取代上
一节介绍的执行列表方式。
11.6.1 加载固件和启动GuC
当作者写作本书时，i915驱动虽然已经包含了关于GuC的逻辑，但
是默认没有启用，使用的提交方式还是执行列表。
要启用GuC，需要在内核参数中加入如下选项。
i915.enable_guc_loading=1 i915.enable_guc_submission=1
在启动时，i915驱动会检查Gen的版本信息，然后尝试加载对应版
本的固件给GuC。
在i915源代码的intel_guc_fwif.h文件中，简要描述了固件文件的布
局。起始部分是个固定格式的头结构（uc_css_header），其中包含版本
信息，以及各个组成部分的大小。头信息后面是编译好的固件代码和签
名信息。
如果i915驱动在加载GuC固件时失败，它会输出类似下面这样的错
误信息。
[drm] Failed to fetch valid uC firmware from i915/kbl_guc_ver9_14.bin (err
or 0)
[drm] GuC firmware load failed: -5
[drm] Falling back from GuC submission to execlist mode
前两行包含i915尝试加载的固件文件路径和错误码，第二行显示加
载固件失败，最后一行表示回退到旧的执行列表提交方式。在内核参数
中增加drm.drm_debug=6可以看到更详细的调试信息。
导致以上错误的一般原因是i915找不到固件文件。一种解决方法是
重新编译Linux内核，并把GuC固件集成到内核文件中。主要步骤是先
把合适版本的固件文件放入Linux内核源代码的firmware/i915子目录下，
然后修改构建配置，通过额外固件选项指定GuC固件。
解决了上述找不到固件文件的问题后，另一种常见的错误是下面这
样的“CSS头定义不匹配”。
[drm] CSS header definition mismatch
导致这个问题的原因很可能是因为从kernel网网上下载固件时下载
了HTML格式的文件。改正的方法是单击链接下载原始的二进制文件。
 老雷评点 
老雷曾不慎落入这个陷阱，感谢多年好友HM一语点破，并
分享同样经历。
固件文件加载成功后，可以通过虚文件i915_guc_load_status观察其
概况，如清单11-5所示。
清单11-5 通过虚文件观察GuC的加载状态
# cat /sys/kernel/debug/dri/0/i915_guc_load_status 
GuC firmware status:
    path: i915/kbl_guc_ver9_14.bin
    fetch: SUCCESS
    load: SUCCESS
    version wanted: 9.14
    version found: 9.14
    header: offset is 0; size = 128
    uCode: offset is 128; size = 142272
    RSA: offset is 142400; size = 256
GuC status 0x800330ed:
    Bootrom status = 0x76
    uKernel status = 0x30
    MIA Core status = 0x3
在清单11-5中，上半部分是固件的静态信息，包含版本号，三个部
分（头信息、代码和RSA签名）的位置和大小。头结构的大小为128字
节，随后紧跟的代码为142 272字节，RSA数据为256字节。后半部分是
供调试用的状态代码。
11.6.2 以MMIO方式通信
GuC支持多种方式与它通信。一种基本的方式是通过映射在MMIO
空间的16个软件画板（SOFT_SCRATCH）寄存器，其起始地址为
0xC180。以下为i915中的有关宏定义。
#define SOFT_SCRATCH(n)            _MMIO(0xc180 + (n) * 4)
#define SOFT_SCRATCH_COUNT            16
16个画板寄存器中，0号（SOFT_SCRATCH_0）用来传递一个动作
码（action），后面的15个用来传递数据。写好画板寄存器后，软件应
该写另一个GuC寄存器（0xC4C8），以触发中断通知GuC。
通过上面提到的i915_guc_load_status虚拟文件，可以观察画板寄存
器的内容，比如以下内容（原输出为一列，为节约篇幅，这里格式化为
3列）。
Scratch registers:
      0:  0xf0000000
      1:  0x0
      2:  0x0
      3:  0x5f5e100
      4:  0x600
     5:  0xd5fd3
     6:  0x0
     7:  0x8
     8:  0x3
     9:  0x74240
    10:  0x0
    11:  0x0
    12:  0x0
    13:  0x0
    14:  0x0
    15:  0x0
开源驱动中的intel_guc_send_mmio包含使用画板寄存器向GuC发送
信息的详细过程，在此不再详述。
11.6.3 基于共享内存的命令传递机制
使用画板寄存器每次只能传递少量数据，而且速度较慢，因此它一
般只用在初始化阶段。
GuC支持一种基于共享内存的高速通信机制，称为命令传输通道
（Command Transport Channel），简称CT或者CTCH。
在初始GuC化时，i915驱动便创建一个内存页，并将其分为4部分，
分别用来发送和接收命令的描述（desc）和命令流（cmds）。下面是使
用命令传输通道发送命令的主要过程。
fence = ctch_get_next_fence(ctch);
err = ctb_write(ctb, action, len, fence);
intel_guc_notify(guc);
err = wait_for_response(desc, fence, status);
以上代码来自intel_guc_ct.c的ctch_send函数，第1行获取用于同步的
栅栏ID（Fense ID）。第2行把action指针指向的数据写入ctb指针描述的
CT缓冲区中。接下来触发中断通知GuC有新数据，并等待回复。
例如，下面的代码给GuC发送命令，让其从睡眠状态恢复过来。
data[0] = INTEL_GUC_ACTION_EXIT_S_STATE;
data[1] = GUC_POWER_D0;
data[2] = guc_ggtt_offset(guc->shared_data);
return intel_guc_send(guc, data, ARRAY_SIZE(data));
11.6.4 提交工作任务
在向GuC提交GPU任务前，需要先成为GuC的客户（client）。会给
每个GuC客户分配三个内存页，用于向GuC提交工作请求。第一个内存
页的一部分用于存放描述信息，另一部分用作发送信号的门铃
（DoorBell，DB）区域。后面两个页内存用于存放工作任务，称为工作
队列（Work Queue，WQ）。
通过虚文件i915_guc_info可以观察GuC的客户信息、用于提交任务
的通信设施和任务的提交情况。清单11-6是执行GpuTest一段时间后的
结果（删去了关于log的统计信息）。
清单11-6 GuC的客户信息和任务提交情况
root@gedu-i7:/sys/kernel/debug/dri/0# cat i915_guc_info