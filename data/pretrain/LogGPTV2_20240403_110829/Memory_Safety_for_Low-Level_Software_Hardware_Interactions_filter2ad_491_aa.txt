title:Memory Safety for Low-Level Software/Hardware Interactions
author:John Criswell and
Nicolas Geoffray and
Vikram S. Adve
Memory Safety for Low-Level Software/Hardware Interactions
John Criswell
University of Illinois
PI:EMAIL
Nicolas Geoffray
Universit·e Pierre et Marie Curie
INRIA/Regal
PI:EMAIL
Vikram Adve
University of Illinois
PI:EMAIL
Abstract
Systems that enforce memory safety for today’s oper-
ating system kernels and other system software do not
account for the behavior of low-level software/hardware
interactions such as memory-mapped I/O, MMU con(cid:2)g-
uration, and context switching. Bugs in such low-level
interactions can lead to violations of the memory safety
guarantees provided by a safe execution environment and
can lead to exploitable vulnerabilities in system software.
In this work, we present a set of program analysis and
run-time instrumentation techniques that ensure that er-
rors in these low-level operations do not violate the as-
sumptions made by a safety checking system. Our de-
sign introduces a small set of abstractions and interfaces
for manipulating processor state, kernel stacks, memory
mapped I/O objects, MMU mappings, and self modify-
ing code to achieve this goal, without moving resource
allocation and management decisions out of the kernel.
We have added these techniques to a compiler-based vir-
tual machine called Secure Virtual Architecture (SVA),
to which the standard Linux kernel has been ported previ-
ously. Our design changes to SVA required only an addi-
tional 100 lines of code to be changed in this kernel. Our
experimental results show that our techniques prevent re-
ported memory safety violations due to low-level Linux
operations and that these violations are not prevented by
SVA without our techniques. Moreover, the new tech-
niques in this paper introduce very little overhead over
and above the existing overheads of SVA. Taken together,
these results indicate that it is clearly worthwhile to add
these techniques to an existing memory safety system.
1 Introduction
Most modern system software, including commodity op-
erating systems and virtual machine monitors, are vul-
nerable to a wide range of security attacks because they
are written in unsafe languages like C and C++.
In
fact, there has been an increase in recent years of at-
tack methods against the operating system (OS) kernel.
There are reported vulnerabilities for nearly all commod-
ity OS kernels (e.g., [2, 28, 43]). One recent project went
so far as to present one OS kernel bug every day for a
month for several different open source and commercial
kernels [26] (several of these bugs are exploitable vul-
nerabilities). Preventing these kinds of attacks requires
protecting the core kernel and not just device drivers:
many of the vulnerabilities are in core kernel compo-
nents [19, 40, 41, 43, 46].
To counter these threats,
there is a growing body
of work on using language and compiler techniques to
enforce memory safety (de(cid:2)ned in Section 2) for OS
code. These include new OS designs based on safe
languages [4, 18, 22, 33], compiler techniques to en-
force memory safety for commodity OSs in unsafe lan-
guages [10], and instrumentation techniques to isolate
a kernel from extensions such as device drivers [45,
47, 51]. We use the term (cid:147)safe execution environment(cid:148)
(again de(cid:2)ned in Section 2) to refer to the guarantees
provided by a system that enforces memory safety for
operating system code. Singularity, SPIN, JX, JavaOS,
SafeDrive, and SVA are examples of systems that en-
force a safe execution environment.
Unfortunately, all these memory safety techniques
(even implementations of safe programming languages)
make assumptions that are routinely violated by low-
level interactions between an OS kernel and hardware.
Such assumptions include a static, one-to-one mapping
between virtual and physical memory, an idealized pro-
cessor whose state is modi(cid:2)ed only via visible program
instructions, I/O operations that cannot overwrite stan-
dard memory objects except input I/O targets, and a pro-
tected stack modi(cid:2)able only via load/store operations
to local variables. For example, when performing type
checking on a method, a safe language like Java or
Modula-3 or compiler techniques like those in SVA as-
sume that pointer values are only de(cid:2)ned via visible pro-
gram operations. In a kernel, however, a buggy kernel
operation might overwrite program state while it is off-
processor and that state might later be swapped in be-
tween the de(cid:2)nition and the use of the pointer value, a
buggy MMU mapping might remap the underlying phys-
ical memory to a different virtual page holding data of a
different type, or a buggy I/O operation might bring cor-
rupt pointer values into memory.
In fact, as described in Section 7.1, we have injected
bugs into the Linux kernel ported to SVA that are capa-
ble of disabling the safety checks that prevented 3 of the 4
exploits in the experiments reported in the original SVA
work [10]: the bugs modify the metadata used to track
array bounds and thus allow buffer overruns to go un-
detected. Similar vulnerabilities can be introduced with
other bugs in low-level operations. For example, there
are reported MMU bugs [3, 39, 42] in previous versions
of the Linux kernel that are logical errors in the MMU
con(cid:2)guration and could lead to kernel exploits.
A particularly nasty and very recent example is an in-
sidious error in the Linux 2:6 kernel (not a device driver)
that led to severe (and sometimes permanent) corruption
of the e1000e network card [9]. The kernel was over-
writing I/O device memory with the x86 cmpxchg in-
struction, which led to corrupting the hardware. This bug
was caused by a write through a dangling pointer to I/O
device memory. This bug took weeks of debugging by
multiple teams to isolate. A strong memory safety sys-
tem should prevent or constrain such behavior, either of
which would have prevented the bug.
All these problems can, in theory, be prevented by
moving some of the kernel-hardware interactions into a
virtual machine (VM) and providing a high-level inter-
face for the OS to invoke those operations safely. If an
OS is co-designed with a virtual machine implementing
the underlying language, e.g., as in JX [18], then elimi-
nating such operations from the kernel could be feasible.
For commodity operating systems such as Linux, Mac
OS X, and Windows, however, reorganizing the OS in
such a way may be dif(cid:2)cult or impossible, requiring, at
a minimum, substantial changes to the OS design. For
example, in the case of SVA, moving kernel-hardware
interactions into the SVA VM would require extensive
changes to any commodity system ported to SVA.
Virtual machine monitors (VMMs) such as VMWare
or Xen [16] do not solve this problem. They provide suf-
(cid:2)ciently strong guarantees to enforce isolation and fair
resource sharing between different OS instances (i.e.,
different (cid:147)domains(cid:148)) but do not enforce memory safety
within a single instance of an OS. For example, a VMM
prevents one OS instance from modifying memory map-
pings for a different instance but does not protect an OS
instance from a bug that maps multiple pages of its own
to the same physical page, thus violating necessary as-
In fact, a
sumptions used to enforce memory safety.
VMM would not solve any of the reported real-world
problems listed above.
In this paper, we present a set of novel techniques to
prevent low-level kernel-hardware interactions from vi-
olating memory safety in an OS executing in a safe ex-
ecution environment. There are two key aspects to our
approach: (1) we de(cid:2)ne carefully a set of abstractions
(an API) between the kernel and the hardware that en-
ables a lightweight run-time checker to protect hardware
resources and their behaviors; and (2) we leverage the
existing safety checking mechanisms of the safe execu-
tion environment to optimize the extra checks that are
needed for this monitoring. Some examples of the key
resources that are protected by our API include processor
state in CPU registers; processor state saved in memory
on context-switches, interrupts, or system calls; kernel
stacks; memory-mapped I/O locations; and MMU con-
(cid:2)gurations. Our design also permits limited versions of
self-modifying code that should suf(cid:2)ce for most kernel
uses of the feature. Most importantly, our design pro-
vides these assurances while leaving essentially all the
logical control over hardware behavior in the hands of
the kernel, i.e., no policy decisions or complex mecha-
nisms are taken out of the kernel. Although we focus
on preserving memory safety for commodity operating
systems, these principles would enable any OS to reduce
the likelihood and severity of failures due to bugs in low-
level software-hardware interactions.
We have incorporated these techniques in the SVA
prototype and correspondinglymodi(cid:2)ed the Linux 2.4.22
kernel previously ported to SVA [10]. Our new tech-
niques required a signi(cid:2)cant redesign of SVA-OS, which
is the API provided by SVA to a kernel for control-
ling hardware and using privileged hardware operations.
The changes to the Linux kernel were generally simple
changes to use the new SVA-OS API, even though the
new API provides much more powerful protection for the
entire kernel. We had to change only about 100 lines in
the SVA kernel to conform to the new SVA-OS API.
We have evaluated the ability of our system to prevent
kernel bugs due to kernel-hardware interactions, both
with real reported bugs and injected bugs. Our system
prevents two MMU bugs in Linux 2.4.22 for which ex-
ploit code is available. Both bugs crash the kernel when
run under the original SVA. Moreover, as explained in
Section 7.1, we would also prevent the e1000e bug in
Linux 2.6 if that kernel is run on our system. Finally,
the system successfully prevents all the low-level kernel-
hardware interaction errors we have tried to inject.
We also evaluated the performance overheads for two
servers and three desktop applications (two of which per-
form substantial I/O). Compared with the original SVA,
the new techniques in this paper add very low or negligi-
ble overheads. Combined with the ability to prevent real-
world exploits that would be missed otherwise, it clearly
seems worthwhile to add these techniques to an existing
memory safety system.
To summarize, the key contributions of this work are:
(cid:15) We have presented novel mechanisms to ensure that
low-level kernel-hardware interactions (e.g., con-
text switching, thread creation, MMU changes, and
I/O operations) do not violate assumptions used to
enforce a safe execution environment.
(cid:15) We have prototyped these techniques and shown
that they can be used to enforce the assumptions
made by a memory safety checker for a commodity
kernel such as Linux. To our knowledge, no pre-
vious safety enforcement technique provides such
guarantees to commodity system software.
(cid:15) We have evaluated this system experimentally and
shown that it is effective at preventing exploits in
the above operations in Linux while incurring little
overhead over and above the overhead of the under-
lying safe execution environment of SVA.
2 Breaking Memory Safety with Low-
Level Kernel Operations
Informally, a program is type-safe if all operations in
the program respect the types of their operands. For
the purposes of this work, we say a program is mem-
ory safe if every memory access uses a previously initial-
ized pointer variable; accesses the same object to which
the pointer pointed initially;1 and the object has not been
deallocated. Memory safety is necessary for type safety
(conversely, type safety implies memory safety) because
dereferencing an uninitialized pointer, accessing the tar-
get object out of bounds, or dereferencing a dangling
pointer to a freed object, can all cause accesses to un-
predictable values and hence allow illegal operations on
those values.
A safe programming language guarantees type safety
and memory safety for all legal programs [34]; these
guarantees also imply a sound operational semantics
for programs in the language. Language implementa-
tions enforce these guarantees through a combination
of compile-time type checking, automatic memory man-
agement (e.g., garbage collection or region-based mem-
ory management) to prevent dangling pointer references,
and run-time checks such as array bounds checks and
null pointer checks.
Four recent compiler-based systems for C, namely,
CCured [30], SafeDrive [51], SAFECode [15], and
1Note that we permit a pointer to (cid:147)leave(cid:148) its target object and later
return, as long as it is not accessed while it is out of bounds [32].
SVA [10] enforce similar, but weaker, guarantees for C
code. Their guarantees are weaker in two ways: (a) they
provide type safety for only a subset of objects, and (b)
three of the four systems (cid:151) SafeDrive, SAFECode and
SVA (cid:151) permit dangling pointer references (use-after-
free) to avoid the need for garbage collection. Unlike
SafeDrive, however, SAFECode and SVA guarantee that
dangling pointer references do not invalidate any of the
other safety properties, i.e., partial type safety, memory
safety, or a sound operational semantics [14, 15]. We re-
fer to all these systems (cid:150) safe languages or safety check-
ing compilers (cid:150) as providing a safe execution environ-
ment.
All of the above systems make some fundamental as-
sumptions regarding the run-time environment in enforc-
ing their safety guarantees. In particular, these systems
assume that the code segment is static; control (cid:3)ow can
only be altered through explicit branch instructions, call
instructions, and visible signal handling; and that data
is stored either in a (cid:3)at, unchanging address space or in
processor registers. Furthermore, data can only be read
and written by direct loads and stores to memory or di-
rect changes to processor registers.
Low-level system code routinely violates these as-
sumptions. Operating system kernels, virtual machine
monitors, language virtual machines such as a JVM or
CLR, and user-level thread libraries often perform op-
erations such as context switching, direct stack manip-
ulation, memory mapped I/O, and MMU con(cid:2)guration,
that violate these assumptions. More importantly, as ex-
plained in the rest of this section, perfectly type-safe code
can violate many of these assumptions (through logical
errors), i.e., such errors will not be prevented by the lan-
guage in the (cid:2)rst place. This is unacceptable for safe lan-
guage implementations and, at least, undesirable for sys-
tem software because these violations can compromise
safety and soundness and thus permit the vulnerabilities
a safe language was designed to prevent, such as buffer
over(cid:3)ows or the creation of illegal pointer values.
There are, in fact, a small number of root causes (or
categories of root causes) of all these violations. This
section enumerates these root causes, and the next sec-
tion describes the design principles by which these root
causes can be eliminated. We assume throughout this
discussion that a safety checker (through some com-
bination of static and run-time checking) enforces the
language-level safety guarantees of a safe execution en-
vironment, described above, for the kernel.2 This allows
us to assume that the run-time checker itself is secure,
and that static analysis can be used soundly on kernel
code [15]. Our goal is to ensure the integrity of the as-
2This work focuses on enforcing memory safety for the kernel. The
same techniques could be applied to protect user-space threads from
these violations.
sumptions made by this safety checker. We refer to the
extensions that enforce these assumptions as a veri(cid:2)er.
Brie(cid:3)y, the fundamental categories of violations are:
(cid:15) corrupting processor state when held in registers or
memory;
(cid:15) corrupting stack values for kernel threads;
(cid:15) corrupting memory mapped I/O locations;
(cid:15) corrupting code pages in memory;
(cid:15) other violations that can corrupt arbitrary memory
locations, including those listed above.
Unlike the last category, the (cid:2)rst four above are errors
that are speci(cid:2)c to individual categories of memory.
2.1 Corrupting Processor State
Corrupting processor state can corrupt both data and con-
trol (cid:3)ow. The veri(cid:2)er must (cid:2)rst ensure that processor
state cannot be corrupted while on the processor itself,
i.e., preventing arbitrary changes to processor registers.
In addition, however, standard kernels save processor
state (i.e., data and control registers) in memory where it
is accessible by standard (even type-safe) load and store
instructions. Any (buggy) code that modi(cid:2)es this state
before restoring the state to the processor can alter con-
trol (cid:3)ow (the program counter, stack pointer, return ad-
dress register, or condition code registers) or data val-
ues. In safe systems that permit dangling pointer refer-
ences, processor state can also be corrupted if the mem-
ory used to hold saved processor state (usually located on
the heap [5]) is freed and reallocated for other purposes.
Note that there are cases where the kernel makes ex-
plicit, legal, changes to the interrupted state of user-space
code. For example, during signal handler dispatch, the
kernel modi(cid:2)es interrupted program state that has been
saved to memory, including the interrupted program’s
program counter and stack pointer [5]. Also, returning