any parametric model to hold at all time points. Hence, to
solve the metric ranking problem, we need to differentiate
between invariants broken due to a fault or anomaly from
ones affected by (measurement) noise. We refer to this second
challenge as the problem of noise reduction. Next we describe
two approaches for tackling these problems – one leverages
the invariant graph structure (Section V) and the other is
based on temporal patterns of broken invariants (Section VI).
Preliminary evaluation results for these two algorithms are
encouraging (see Section VII). Currently, we are doing more
tests, and plan to add these algorithms to SIAT to improve its
fault localization capabilities and robustness to measurement
noise.
V. SPATIAL APPROACH: MINING THE INVARIANT GRAPH
Given a dataset of time series from multiple metrics, we
can construct an invariant graph using the system invariants
extracted by SIAT. The nodes correspond to the metrics and
edges represent the pair-wise invariant relationships between
these metrics. The invariant graph captures a spatial view
of a system in terms of both intra and inter-component
dependencies captured by invariants. During the operational or
testing stage, at any given time t, some of the invariants may
be broken, and an invariant graph can capture this information
as well; see Figure 1(b).
By differentiating edges in an invariant graph based on
whether or not the corresponding invariants are broken, we
can associate a score with each node in the graph. The main
idea of our spatial approach is to deﬁne a score for nodes in the
invariant graph that is directly proportional to the abnormality
of their corresponding metrics. We can then use this score
to ﬁlter out invariants broken due to noise as well as rank
abnormal metrics.
A. Node scores
We deﬁne two scores for each node based on the invariant
graph. nodeScore captures the information extracted from
the edges incident at a node while neighborScore aggregates
information across the one-hop neighbors of a node.
Deﬁnition 1 nodeScore: Given an invariant graph G(V, E) at
time t consisting of vertices in V and edges in E, let d(v)
denote the degree of a vertex v and bet (v) denote the number
of broken edges (invariants) incident at v. The nodeScore of
v ∈ V is deﬁned as:
nodeScoret (v) △
=
bet (v)
d(v)
(8)
Note that the set of broken invariants associated with a
metric can change across time causing bet (v) to change as
well, and hence the nodeScore of a vertex will change over
time as well. d(v) is equal to the number of invariants we learn
4
B
C
A
D
I
F
G
E
J
K
L
Fig. 3.
Invariant graph
for the metric associated with node v and does not change over
time. SIAT currently uses nodeScore to rank abnormal metrics.
However there are scenarios in which nodeScore is not sufﬁ-
cient. Consider the connected component of the invariant graph
consisting of nodes A, B, and C shown in Figure 3. A solid
line for an edge denotes that the corresponding invariant is still
valid while a dashed line indicates a broken invariant. Both the
edges incident on node A are for broken invariants, and hence
nodeScoret (A) = 1 while nodeScoret (B) = nodeScoret (C) =
0.5 because the edge e = (B,C) is not broken. Hence, we can
declare node A to be the most abnormal but what about nodes
B and C? The fact that the edge between nodes B and C is
not broken and both these nodes have a broken edge with A
is a strong indication that node A is the only abnormal node;
an anomaly at either B or C would cause the edge e = (B,C)
to be broken as well. This example underscores the value of
looking at all the edges amongst a group of nodes together.
We capture this intuition by deﬁning another score.
For an invariant graph G(V, E), we deﬁne the broken-
invariant-neighboring-nodes of a node v ∈ V at
time t,
BINNt (v), to be the subgraph of G(V, E) consisting of the
neighbors of v that are connected to it by a dashed edge
(corresponding to a broken invariant) and all their incident
edges. E.g., in Figure 3, BINNt (G) is the subgraph with nodes
{F, I, J, K} and edges (F, I), (I, J), and (J, K); L is not included
because its invariant with G is not broken. Based on BINN(v)t
we deﬁne the neighborScore of a node v.
Deﬁnition 2 neighborScore: Given an invariant graph G(V, E)
at time t that determines the BINNt (v) for each node v ∈ V ,
the neighborScore of v is deﬁned as:
neighborScoret (v) = 1 −
# broken edges in BINNt (v)
Total # edges in BINNt (v)
(9)
Based on Figure 3, neighborScoret (G) = 1/3.
Special cases for neighborScore : When computing neigh-
borScore a special case arises when BINNt (v) does not contain
any edges. For example, in Figure 3 nodes D and E have
only one (broken) edge and hence the subgraphs BINNt (D)
and BINNt (E) contain a single isolated node (E and D,
respectively). We refer to such invariants as isolated invariants.
With isolated invariants, neighborScore does not have any
discriminative power, i.e. looking at the one-hop neighbors
of its nodes does not provide us with any information. Hence,
we set the neighborScore to zero. This is a design choice.
An alternative is to set the neighborScore to one, but with
this choice both nodeScore and neighborScore for nodes D
and E will be one. This will lead to their corresponding
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:43:15 UTC from IEEE Xplore.  Restrictions apply. 
metrics getting the highest rank amongst abnormal metrics.
However,
in several real-world cases, we have found that
typical faults and anomalies are more likely to affect nodes in
larger components of an invariant graph, e.g. the component
containing G, than isolated invariants. Accordingly, we want to
bias the neighborScore against isolated invariants, and hence,
our design choice.
neighborScore is not useful in another scenario. If all the
invariants corresponding to a completely connected component
in the invariant graph are broken,
then every node from
this component will have neighborScore =1-(1/1)=0. However,
their nodeScore will be 1 making it likely that they will still
get a high overall score. We intend for the neighborScore to
supplement the discriminative power of nodeScore, and this
motivates our metaranking approach described later.
Other scores . It is possible to design different versions of
the nodeScore and neighborScore. For example, we can deﬁne
weighted nodeScore and neighborScore by associating weights
with edges. The ﬁtness score of the invariant associated with
an edge is an obvious choice for its weight. SIAT supports
weighted scores but they did not achieve better results in
isolating faults on the real-world datasets that we analyzed.
B. Metaranking
How do we combine the two scores, nodeScore and neigh-
borScore, for fault localization? We can rank metrics based on
the (weighted) average of the two scores. Another option is to
ﬁrst create two separate rankings for nodes, one for each score,
and then combine the two ranked lists. This idea is inspired by
previous work on metasearch and information retrieval [10],
[4] where the goal is to combine the ranked lists of docu-
ments (or web pages) retrieved by multiple search engines
or information retrieval systems, in response to a query, to
produce a ﬁnal list. It is well known that combining the results
from different retrieval algorithms often improves performance
compared to using a single retrieval algorithm [4].
Lee provides a rationale for when one should perform evi-
dence combination (i.e. combine multiple scores or ranked list
of documents) as different runs (of a variety of representations
of a query) might retrieve similar set of relevant documents but
retrieve different sets of non-relevant documents [15]. In our
context Lee’s rationale translates into the following intuitions
for deﬁning nodeScore and neighborScore: (1) all of them
assign a (high) positive score to abnormal nodes, and (2) the
set of normal nodes incorrectly assigned a positive nodeScore
has small (ideally, no) overlap with the corresponding set for
neighborScore; otherwise we will end up with false positives
when identifying abnormal metrics. We combine nodeScore
and neighborScore in two ways: (1) the spatial average
algorithm uses their average as the ﬁnal score for ranking,
and (2) the spatial rank algorithm creates two separate ranked
lists, one for each score, and then combines by ﬁrst assigning a
weight to each rank (highest rank gets the largest weight) and
then summing up the weights for each metric to compute their
ﬁnal score. We evaluate these two algorithms in Section VII.
5
VI. TEMPORAL APPROACH FOR NOISE REDUCTION
Currently, SIAT tackles the noise reduction problem using a
simple heuristic: an invariant is marked as broken only if it is
broken for three consecutive samples. However, this approach
is not robust when combined with high frequency sampling.
For example, a transient measurement noise lasting for 100 ms
can cause broken invariants for sampling intervals less than
30 ms but SIAT will ignore it if measurements are collected
less frequently. We have encountered cases where operators
dynamically adjust
the sampling frequency for monitoring
data. One power plant operator routinely collects samples 100
ms apart, but during critical phases of the plant’s operation,
such as stress testing of the equipments, data is collected
every 10 ms. Based on these experiences, we took a more
principled approach to tackle the noise reduction problem. We
try to infer a temporal pattern for the impact of abnormal
samples of a time series (due to faults/anomalies) on an
invariant relationship. The following example illustrates our
new approach.
Example : Consider the invariant relationship shown in (10)
between two time series from a real world dataset with residual
threshold εxy = 0.91. The parameters of the ARX model are
n = 2, k = 0, and m = 1.
y(t) = −0.23y(t − 1) − 0.24y(t − 2) + 0.32x(t) + 0.21x(t − 1) − 2.04
(10)
We inject a fault in time series y at time t by increasing y(t)
by 20%, i.e. ˜y(t) = 1.2 × y(t) where ˜y(t) denotes the abnormal
measurement (other relevant data points y(t − 1), y(t − 2),
y(t + 1), y(t + 2), x(t − 1), x(t), x(t + 1), and x(t + 2) are
the same as the original measurements). We observe residuals
R(t) = 27.7, R(t + 1) = 6.4, and R(t + 2) = 6.8. Since these
values are greater than εxy, this invariant is broken. However,
based on the temporal pattern of this invariant being broken
for three consecutive samples, we can infer more information.
With n = 2 , we know that a fault affecting y at time t can
result in the invariant being broken during the interval [t, t+2];
whether this actually happens also depends on the coefﬁcients
a0, . . . , an and εxy. Similarly, from (1), we can infer that a
fault in x at time t can cause this invariant to be broken
during [t+k,t+k+m]; with k = 0 and m = 1, this implies [t, t+1].
The two most parsimonious5 hypotheses that can explain the
observed pattern are: (1) a fault affects y at t, or (2) a fault
affects x at t and t + 1.
The above example introduces the notion of temporal prop-
agation of the impact of a fault/anomaly on an invariant; i.e.
a fault in y (x) at t can cause broken invariants during the
interval [t, t+n] ([t+k, t+k+m]). This property can be used to
identify invariants broken due to noise. It can also improve
fault localization by allowing us to infer an explanation for
the observed pattern of broken invariants. For instance, in the
above example, if the invariant was broken only at time t, then
we could either ignore it or assign the metrics y and x a lower
rank (compared to other abnormal metrics) because the broken
invariant pattern differs from the expected one. However if
5in terms of the number of metrics and number of samples with faults.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:43:15 UTC from IEEE Xplore.  Restrictions apply. 
the invariant was broken only at t and t + 1 then this pattern
matches the expected temporal behavior from a fault in x at
t but does not match the expected pattern for a fault in y at
t. Hence, we can declare x as the abnormal metric with high
conﬁdence; in contrast, SIAT’s current heuristic will miss this
fault because the invariant is not broken for three consecutive
samples.
Algorithm 1 Localize faults in time and rank abnormal metrics
Input: set of all invariants I , list of broken invariants, Lt at each time point
t ∈ [T − w, T ], T: current time
for all t ∈ [T − w, T ] do
anomalyScore[t] = timeScore(t,T,{L j : j ∈ [t, T ]})
end for
Raise alarms for time points t with high anomaly score; at each of these
time points focus on metrics with high scores for further investigation.
Algorithm 2 timeScore: computes a score to support the hypothesis that
an anomaly occurred at time t
Input: lists of broken invariants {L j : j ∈ [t, T ]}, set of all invariants I ,
current time T.
Output: an anomaly score for time t.
for all metric m with broken invariants in Lt do
score[m] = metricScore(m,{L j : j ∈ [t, T ]},I )
end for
anomalyScoret = ∑m score[m]
Algorithm 3 Assuming that anomalies happened at t, assigns a score to
metric m based on the match between expected temporal pattern of broken
invariants and the observed pattern
Input: metric m, set of all invariants I , list of broken invariants {L j : j ∈
[t, T ]}.
Output: score for metric m.
initialize metricScore[m] = 0
initialize expectedMetricScore[m] = 0
for all j ∈ [t, T ] do
for all invariant i in I with metric m do
if ARX implies i be broken at j then
incr. expectedMetricScore[m] by 1
if i is broken at j then
incr. metricScore[m] by 1
end if
end if
end for
end for
metricScore[m] =
metricScore[m]
expectedMetricScore[m]
Algorithms 1, 2, and 3 deﬁne the main steps of our online
temporal approach. Based on observations up to the current
time T, we select an interval of length w, [T − w, T ], such that
an anomaly at any t ∈ [T − w, T ] can cause a broken invariant
at T . Using the ARX model we set w = arg max(n, m + k). For
each time t ∈ [T − w, T ], Algorithm 1 computes an anomaly
score using Algorithm 2. Algorithm 2 computes this score
assuming that all broken invariants at
time t are due to
anomalies affecting their corresponding metrics. It calculates
the anomaly score for t as the sum of the score for each metric
with broken invariants at time t. Algorithm 3 computes the
score for a metric based on the match between the expected
and the observed broken invariant pattern. Thus, the overall
anomaly score for a time t indicates support for the hypothesis
that an anomaly occurred at this time while the score for a
metric denotes how likely it is abnormal.
6
VII. EVALUATION AND A CASE STUDY
We evaluate of our invariant graph based and temporal
pattern based algorithms using real-world time series with
injected anomalies. We select the anomaly types based on
our experience working with data from multi-tier web hosting
infrastructure, physical plants, and sensors in an automobile.
Hence, our evaluation is based on anomalies observed in the
wild but our methodology allows us to create instances that
stress different aspects of our algorithms. We also discuss a
real-world case study.
Fault models. The faults/anomalies that we observe most
often in real datasets fall into three basic models: (1) Spike, a
large positive or negative change in measured value at time t;
often, this anomaly affects a single sample, (2) Constant-shift,
the measurements from an interval are shifted by a constant,
and (3) Increased-variance, the measurements from an interval
appear to be “noisy” – i.e. have higher variance than the rest
of the time series. We do not know the root causes for these
types of faults/anomalies for our datasets. However, similar
anomalies have been observed in other domains [18]. Due to
page limits, we present results only for spike anomalies here;
the results for the other two types were similar qualitatively.
Data with injected anomalies. We inject spike anomalies in
a real-world dataset. It consists of one day of measurements
collected by 1091 sensors monitoring a physical plant6. We
have 2880 measurements from each sensor. We extracted 324
invariants from among the 1091 metrics using SIAT. In order to
have complete control on when these invariants will break, we
inject anomalies into the same data that is used for extracting
the invariants. Without
these injected anomalies, naturally
these invariants will hold for the entire dataset. Our choice of
using the training data with injected anomalies for evaluation,