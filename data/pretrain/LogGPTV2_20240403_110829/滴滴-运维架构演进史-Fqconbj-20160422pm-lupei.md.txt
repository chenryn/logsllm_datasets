# 滴滴运维架构演进史
## 作者：陆沛（滴滴）

### 关于我
- **2010-2013** 百度贴吧
- **2013至今** 滴滴出行

---

### 大纲
1. 背景
2. 运维的演进
   - 远古时期
   - 石器时代
   - 青铜时代
   - 黑铁时代
3. 回顾与总结
4. 核心运维平台介绍
5. 展望白银时代
6. Q&A

---

### 背景
滴滴的运维发展阶段：
- **远古时期** (2012-2013/04)
- **石器时代** (2013/05-2014/02)
- **青铜时代** (未明确时间)
- **黑铁时代** (2015/05)
- **白银时代** (展望)

---

### 远古时期 (2012-2013/04)
- **规模**
  - 服务器: 4台
  - 流量: PV不超过500万
  - 团队: 5个研发，无专职运维
- **问题**
  - 机房稳定性不足
  - 缺乏有效的监控系统
  - 安全性问题

---

### 石器时代 (2013/05-2014/02)
- **规模**
  - 服务器: 100台
  - 流量: PV小于3000万
  - 团队: >300个研发, 8个运维
- **业务**: 出租车、专车
- **运维架构**
  - APP
  - 业务架构
  - VPN
  - 运维管理
  - LVS接入层
  - Nagios监控
  - 自动化运维工具
  - 数据存储 (MySQL, MongoDB)
  - 机房管理 (Kickstart, iDRAC, Excel资产管理)
- **问题**
  - 监控性能及维护成本高
  - 部署效率低且难以满足个性化需求
  - 业务同质化严重，迭代频繁导致扩容效率低下

---

### 黑铁时代 (2015/05)
- **规模**
  - 服务器: >1万台
  - 流量: PV大于50亿
  - 业务: 10多个业务线
  - 团队: >1000个研发, 25个运维
- **运维架构**
  - APP
  - 业务架构
  - OTP运维管理
  - LVS接入层
  - Odin 3.0 & Huston 2.0监控
  - 部署系统
  - 初始化与数据配送平台
  - 逻辑层 (PHP, C, Java, Golang)
  - 内部交互层
  - 服务树
  - 数据存储 (MySQL, Redis)
  - 机房管理 (CMDB, 故障管理)
- **问题**
  - 业务需求增长过快，运维人力无法及时响应
  - 监控覆盖率和有效性不足
  - 多集群部署耗时长，扩容效率低
  - 成本控制问题

---

### 回顾与总结
- **基础设施规划**：应对突发业务需求
- **公有云IDC租赁**：提升灵活性
- **版本迭代优化**：上线流程标准化、自助上线、灰度发布
- **业务同质化与差异性**：标准化语言、部署方式、监控上报等
- **报警与定位**：分级报警机制、服务大盘、BI数据分析
- **预案管理**：降级措施、异地双活切换

---

### 核心运维平台介绍
- **服务树**
  - 资源组管理 (机器、监控、部署策略)
  - 批量处理与事件同步
  - API接口访问
- **监控系统**
  - 集群监控：聚合、Tag、同比环比分析
  - 监控回调与自动化处理
  - 服务监控Dashboard
- **部署系统**
  - 发布流程：构建、工单、部署
  - CLI命令行操作
  - Jenkins集成与YUM源管理

---

### 展望白银时代
- **成本优化**：容器化、动态调度
- **稳定性提升**：异地多活、多集群管理
- **监控改进**：指标量化、覆盖率提高
- **部署效率**：快速部署、多集群支持

---

### 加入我们
- **联系邮箱**: [EMAIL]
- **办公地点**: 北京海淀

---

### 感谢聆听
- 滴滴出行