Heuristic 
includes  a  checklist  of 
questions,  suggestions  or  guidewords,  such  as  “is the 
component  unstable  or  new?”,  “does  the  component 
implements  a  complex  business  rule?”,  etc.  Rigorous 
risk analysis generally apply statistical models such as 
software 
the 
component  failure  likelihood  [25,  39],  and  hazard 
analysis  to  estimate  the  consequence  of  failures  [22]. 
By  combining  the  consequence  and  the  likelihood  of 
failures,  it  is  possible  to  rank  the  risk  of  each 
individual components of a system.  
reliability  modeling 
estimate 
Many  studies  have  tried  to  mitigate  the  problems 
associated  to  software  faults  and  estimate  their  risk 
with particular emphasis on studies on software testing, 
software reliability modelling, and software reliability 
risk  analysis  [25,  30,  17,  20].  The  software  risk 
assessment  equation  used  in  most  of  the  literature  is 
basically the same and reflects the probability of faulty 
behavior in a given software component and its impact 
(or  cost).  However,  this  equation  is  interpreted  in 
different  ways,  depending  on  the  approach  used  for 
risk assessment in each particular work. 
The equation presented in [34] considers the object-
oriented  CK  metrics  [6]  to  estimate  how  error-prone 
the  component  is.  The  higher  the  metrics  the  more 
error-prone  the  component  is.  The  risk  is  evaluated 
considering the probability that an undesirable event Ei 
occurs (p(Ei)) and the cost to the system if this event 
really occurs (c(Ei)), as shown in equation (7). In the 
context  of  estimating  risk  in  software  systems,  an 
undesirable event is a component failure. 
Risk = ∑(p(Ei) * c(Ei)) 
(7) 
Sherer  presents  another  concept  of  risk  [38],  as  a 
function of fault activation probability in a pre-defined 
time,  the  quality  of  the  development  process  and  the 
operational profile.  The work presented in [1] expands 
the Rosenberg’s equation [34] in order to consider the 
component  exposure  from  the  point  of  view  of  the 
customer and from the point of view of the vendor.  
Software complexity metrics have been widely used 
to estimate the probability of component faults, which 
in 
the 
(7)).  The 
are obviously related to the probability  of component 
failure  required  in  the  typical  risk  equation  (e.g.,  in 
equation 
[5] 
experimentally validates object-oriented design metrics 
as quality indicators to predict fault-prone classes and 
conclude  that  several  of  these  metrics  appear  to  be 
useful to predict class fault-proneness during the early 
phases of the life-cycle. 
study  presented 
The component failure likelihood is directly related 
to  the  complexity  of  that  component  [25].  In  fact, 
complexity  metrics  have  been  used  in  many  studies 
that show a clear link between component complexity 
and  error  proneness  [21,  20,  12].  However,  Fenton 
shows that this trend does not hold in some cases [13]. 
Some  explanations for this apparent contradiction are 
provided in [28], and the use of static code metrics is 
recommended  as  predictors  if  these  predictors  are 
treated  as  probabilistic  and  not  as  categorical 
indicators.  Menzies  et.  al.  also 
the 
importance  of  finding  good  attributes  set  for  each 
problem and to analyze a large amount of data sets in 
order to generalize the results. 
reinforce 
Concerning 
the  estimation  of 
impact  of 
component failures (the term cost in the risk equation), 
the  Failure  Mode  and  Effect  Analysis  (FMEA) 
technique  [22]  is  widely  used  to  estimate  component 
failure cost (in FMEA this is called severity analysis). 
This technique is particularly used in the development 
of software for highly regulated application areas such 
as avionics, space, and nuclear applications. 
The use of fault injection to evaluate experimentally 
the  cost  (i.e.,  the  impact)  of  failures  in  computer 
systems  is  also  widely  used  [3,  18].    The  impact  of 
failures  (equivalent  to  cost  in  the  risk  equation)  is 
generally described in fault injection works as failure 
modes,  which  express  the  system  response  to  the 
injection  of  each  fault  (e.g.,  crash,  hang,  erroneous 
output, etc.).  
injection 
Although  fault 
techniques  are  quite 
popular,  their  use  to  estimate  risk  has  not  been 
addressed in the literature, especially in what concerns 
software risk. In fact, the evaluation of the impact of 
software component failures would need the injection 
of software faults, and techniques to inject this type of 
faults have been largely absent from the fault injection 
research.  Most  of  the  fault  injection  works  actually 
inject  faults  that  emulate  hardware  transient  faults. 
Very  often,  faults  are  injected  using  the  SWIFI 
approach (Software Implemented Fault Injection), but 
this  must  not  be  confused  with  the  injection  of 
software  faults  (i.e.,  program  defects  or  bugs),  as 
SWIFI tools actually emulate hardware faults through 
the injection of errors by software. 
The  problem  of  injecting  representative  software 
faults was first addressed in [8]. That work was done in 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:32:36 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007the context of IBM’s Orthogonal Defect Classification 
(ODC)  project  [7]  and  the  proposed  method  requires 
field data about real software faults in the target system 
or  class  of  target  systems.  This  requirement  (the 
knowledge  of  previous  faults  in  the  target  system) 
greatly  reduces  the  usability  of  the  method,  as  this 
information  is  seldom  available  and  is  simply  not 
possible  to  obtain  for  new  software.  Furthermore,  as 
shown in [26], typical fault injection tools are not able 
to  inject  a  substantial  part  of  the  type  of  faults 
proposed in [8].  
To  the  best  of  our  knowledge,  the  first  practical 
approach to inject realistic software faults without the 
need of the target source code was proposed in [9]. The 
technique  is  named  Generic  Software  Fault  Injection 
Technique  (G-SWFIT)  and 
the 
findings from an extensive field study (see [10]). In the 
work at hand we use this technique for the estimation 
of the cost of the activation residual faults.  
is  supported  by 
5. Conclusion 
This paper presents a first approach to evaluate the 
risk of using a given software component by software 
fault  injection.  The  risk  is  evaluated  using  both 
software  metrics  and  software  fault  injection.  The 
faults  injected  are  meant  to  represent  component 
residual  faults realistically,  and  provide  a  measure  of 
the  impact  of  component  failures  (failure  of  the 
component  where  the  faults  are  injected).  Several 
software  metrics  are  considered  such  as  cyclomatic 
complexity, number of parameters, number of returns, 
maximum nesting depth and Haltead´s program length 
and vocabulary size. The complexity metrics are used 
to  estimate  the  component  fault  proneness.  Logistic 
regression analysis is used to fit the expression of the 
fault probability with these metrics. The fault injection 
in  the  experiments  includes  the  notion  probability  of 
fault activation to model the fact that some faults are 
not  activated  or  simply  tolerated.  The  improvement 
that  our  approach  brings  to  risk  evaluation  is  that  it 
assures  a  repeatable  way  of  evaluating  risk  and 
removes 
the  evaluators 
that 
characterize classical risk evaluation approaches. 
the  dependence  on 
experimental 
The  proposed 
assessment 
approach was illustrated using case-study of a satellite 
data handling real time  application  written  in C. The 
risk assessment technique is illustrated in each setup at 
very  different  levels  of  component  granularity.  The 
risk of using two well-know off-the-shelf components 
(RTEMS  and  RTLinux  operating  systems)  was 
analyzed.  Results  show  that  RTEMS  represents  a 
considerably  lower  risk  than  the  RTLinux  for  that 
application. 
risk 
6. Acknowledgments 
The authors thank to CAPES/GRICES and FAPESP 
to  partially  support  this  work.  We  thank  also  to 
MSquared  Technologies  for  gracefully  providing  the 
full  version  of  RSM  tool,  and  Testwell  Oy  Ltd  for 
CMT++ and CMTjava tools.  
7. References 
[1]Amland,  S. 
analysis 
fundamentals and metrics for software testing including a 
financial application case study”. The Journal of Systems 
and Software, 53, pp. 287-295, 2000. 
“Risk-based  Testing:  Risk 
[2]Anderson,  T.;  Feng,  M.;  Riddle,  S.;  Romanovsky,  A. 
“Protective  Wrapper  Development:  A  Case  Study”. 
Lecture Notes in Computer Science, vol 2589, pp. 1-14, 
Springer Verlag, London, UK, 2003. 
[3]Arlat,  J.  et  al.  “Fault  Injection  and  Dependability 
Evaluation of Fault Tolerant Systems”. IEEE Transaction 
on Computers, vol. 42, n. 8, pp.919-923, 1993.  
[4]Bach,  J.  “Heuristic  Risk-Based  Testing”.  in  Software 
Testing and Engineering Magazine, 1999. 
[5]Basili, V.; Briand, L.; Melo, W. "Measuring the Impact of 
Reuse  on  Quality  and  Productivity  in  Object-Oriented 
Systems". Tech. Report, University of Maryland, Dep. Of 
Computer Science, Jan. 1995, CS-TR-3395. 
[6]Chidamber, R.; Kemerer, F. “A Metric Suite for Object-
IEEE  Trans.  of  Software 
Oriented  Design”. 
In 
Engineering, 20 (6), 1994. 
[7]Chillarege, R., “Orthogonal Defect Classification”, Ch. 9 
of “Handbook  of Software Reliability Engineering”, M. 
Lyu Ed., IEEE Computer Society, McGraw-Hill, 1995. 
[8]Christmansson, J; Chillarege, R. “Generation of an Error 
Set that Emulates Software Faults-Based on Fields Data”. 
Proc.  of  26th 
Int.  Symposium  on  Fault-Tolerant 
Computing, pp 304-13, Sendai, Japan, 1996. 
[9]Durães, J.; Madeira, H. “Emulation of Software Faults by 
Educated Mutations at Machine-Code Level”. in Proc. of 
The  13th  Int.  Symposium  on  Software  Reliability 
Engineering – ISSRE’02, Annapolis, USA, 2002. 
[10]Durães,  J.;  Madeira,  H.  “Definition  of  Software  Fault 
Emulation  Operators:  A  Field  Data  Study”.  In  Proc.  of 
The Int. Conf. on Dependable Systems and Networks, pp. 
105-114, San Francisco, USA, 2003 (W. Carter Award). 
[11]Durães,  J.;  Madeira,  H.  “Software  Faults:  A  field  data 
Study  and  a  practical  approach”.  in  Trans.  Of  Software 
Engineering, Nov. 2006. 
[12]El Emam, K.; Benlarbi, S.; Goel, N.; Rai, S. “Comparing 
Case-based  Reasoning  Classifiers  for  Predicting  High 
Risk  Software  Components”.  Journal  of  Systems  and 
Software, vol. 55, n. 3, pp. 301-320, 2001. 
[13]Fenton,  N.;  Ohlsson,  N.  “Software  Metrics  and  Risk”. 
Proc.  of  The  2nd  European  Software  Measurement 
Conference (FESMA´99), 1999. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:32:36 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007[14]Halstead,  M.  “Elements  of  Software  Science”.  Elsevier 
Science Inc., New York, NY, USA, 1977. 
[15]Herrmann,  D.  “Software  Safety  and  Reliability: 
Techniques, Approaches, and Standards of Key Industrial 
Sectors”.  Wiley-IEEE  Computer  Society  Press,  1st 
edition, January, 2000. 
[16]Hosmer,  D.;  Lemeshow,  S. 
“Applied  Logistic 
Regression”. John Wiley & Sons, 1989. 
[17]Hudepohl  et  al.  “EMERALD:  A  Case  Study 
in 
Enhancing Software Reliability”. in Proc. of IEEE Eight 
Int.  Symp.  on  Software  Reliability  Engineering  - 
ISSRE98, pp.85-91, 1998. 
[18]Iyer, R. “Experimental Evaluation”. Special Issue FTCS-
25  Silver  Jubilee,  25th  IEEE  Symposium  on  Fault 
Tolerant Computing, pp. 115-132, 1995. 
[19]Karolak, D. “Software Engineering Risk Management”. 
Wiley-IEEE  Computer  Society  Press,  1st  edition, 
November, 1995. 
[20]Khoshgoftaar  et  al.  “Process  Measures  for  Predicting 
Software  Quality”.  in  Proc  of  High  Assurance  System 
Engineering Workshop – HASE’97, 1997. 
[21]Kitchenham,  B.;  Pfleeger,  S.;  Fenton,  N.  "Towards  a 
framework  for  software  measurement  validation”.  IEEE 
Trans. on Software Eng., 21(12), pp. 929-944, 1995. 
[22]Leveson, N. “Safeware, System Safety and Computers”. 
Addison-Wesley Publishing Company, 1995. 
[23]Linux  kernel.  www.kernel.org.  Accessed  on  Feb/06, 
2006. 
[24]Lyu, M.; Chen, J.; Avizienis, A. “Experience in Metrics 
and  Measurements  for  N-Version  Programming”.  Int. 
Journal  of  Reliability,  Quality  and  Safety  Engineering, 
vol. 1, n. 1, pp. 41-62, 1994. 
[25]Lyu,  M. 
“Handbook 
of  Software  Reliability 
Engineering”.  IEEE  Computer  Society  Press,  McGraw-
Hill, 1996. 
[26]Madeira, H.; Vieira, M.; Costa, D. “On the Emulation of 
Software  Faults  by  Software  Fault  Injection”.  Proc.  of 
The  Int.  Conf.  on  Dependable  Systems  and  Networks, 
NY, USA, 2000. 
J. 
[27]McManus, 
“Risk  Management 
Projects”. 
in  Software 
Butterworth-Heinemann, 
Development 
November, 2003. 
[32]Popstojanova, G. K.; Trivedi, S. K. “Architecture Based 
Approach 
to  Reliability  Assessment  of  Software 
Systems". Perf. Evaluation, vol. 45, nos. 2-3, pp. 179-204, 
Jun/01, 2001. 
[33]Rome  Laboratory  (RL).  “Methodology  for  Software 
Reliability Prediction and Assessment”. Technical Report 
RL-TR-92-52, vol. 1 and 2, 1992. 
[34]Rosenberg, L.; Stapko, R.; Gallo, A. “Risk-based Object 
Oriented Testing”. In Proc of. 13th Int.Software / Internet 
Quality Week-QW, San Francisco, USA, 2000. 
[35]Resource 
Standard  Metrics, 
6.1, 
http://msquaredtechnologies.com/m2rsm/rsm.htm.  Last 
access 2005. 
Version 
[36]Real-Time  Operating  System 
for  Multiprocessor 
Systems. www.rtems.com, accessed in Feb/06, 2006. 
[37]Shaw,  M.;  Clements,  P.  “A  Field  Guide  to  Boxology: 
Preliminary  Classification  of  Architectural  Styles  for 
Software  Systems”.  Proc.  21st  Int.  Computer  Software 
and Applications Conference, pp. 6-13, 1997. 
[38]Sherer,  S.  “A  Cost-Effective  Approach  to  Testing”.  In 
IEEE Software, 8 (2), pp. 34-40, 1991. 
[39]  Singpurwalla,  N.  “Statistical  Methods  in  Software 
Engineering: Reliability and Risk”. Springer; 1st ed, 1999. 
[40]Tang,  M.;  Kao,  M.;  Chen, M.  “An  Empirical  Study  on 
Object-Oriented  Metrics”. 
the  Sixth 
International Software Metrics Symp. pp. 242-249, 1999. 
[41]Testwell  Oy  Ltd.  http://www.testwell.fi.  Accessed  on 
In:  Proc.  of 
March/06, 2006. 
[42]Vieira,  M.;  Madeira,  H.  “Recovery  and  Performance 
Balance  of  a  COTS  DBMS  in the Presence  of  Operator 
Faults”, Int. Conf. on Dependable Systems and Networks, 
pp. 615-624, Washington D.C., USA, 2002. 
[43]Voas, J.; Charron, F.; McGraw, G.; Miller, K.; Friedman, 
M. “Predicting how Badly ‘Good’ Software can Behave”. 
IEEE Software, 1997. 
[44]Weyuker,  E.  “Testing  Component-Based  Software:  A 
Cautionary Tale”. IEEE Software, 1998. 
[45]Yacoub,  S.;  Ammar,  H.  “A  Methodology 
for 
Architectural-  Level  Reliability  Risk  Analysis".  IEEE 
Trans. Software Eng, vol. 28, no. 6, pp. 529-547, Jun/02, 
2002. 
[28]Menzies,  T.;  Greenwald,  J.;  Frank,  A.  “Data  Mining 
Static Code Attributes to Learn Defect Predictors”. IEEE 
Trans. on Software Eng., Vol.32, n. 11, pp. 1-12, 2007.  
[29]Moraes, R., Durães, J., Martins, E., Madeira, H. "A field 
data  study  on  the  use  of  software  metrics  to  define 
representative  fault  distribution”.  Proc.  of  Workshop  on 
Empirical  Evaluation  of  Dependability  and  Security  – 
WEEDS in conjunction with DSN06, 2006. 
[30] Musa, J. “Software Reliability Engineering”, McGraw-
Hill, 1996. 
[31]Munson,  J.;  Khoshgoftaar,  T.  “Software  Metrics  for 
in:Handbook  of  Software 
Reliability  Assessment”. 
Reliability Engineering, Comp. Society Press, Michael R. 
Lyu editor, ch. 12, 1995. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:32:36 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007