HV
Green
transfo
MV
 PU/TrMV 
Auxiliary 
services 
PU/TrMV 
LV 
MV
 UP/AS 
PU/EAC 
UP/MVba
Red MV bar  
Green MV bar 
MV
PU/ 
MVl
PU/ 
 cap
UP/ 
sh
PU/ 
sh
PU/
 cap
PU/
MVl
MV line
Capacitors
Shunt 
Shunt 
Capacitors 
MV line
Figure 1: Electric circuit (grey lines) and control 
architecture (black lines) of Primary Substation 
3  Case study  
The  Local  Control  Level  module  (LCL)  is  a  PSAS 
component  providing  control  and  protection  functions  for 
the  primary  substation,  as  well  as  an  interface  to    the 
operator  and  -over  the  inter-site  network-  to  remote 
control  systems  and  remote  operators  [2].  The  LCL 
controls the switches to the two HV/MV transformers, the 
switch  connecting  the  Red  MV  bar  (on  the  left)  to  the 
Green MV bar (on the right), as  well as switches local to 
the MV lines (Figure 1). The pilot application implements 
two  functions  from  the  LCL  module:  automatic  power 
resumption 
transformers 
(function2).
(function1)  and  parallel 
Function1 allows automatic power resumption  when 
a  HV/MV  transformer  goes  down,  e.g.  triggered  by 
internal protection (temperature too high, oil alarm, …). It 
disconnects  the  MV  lines  connected  to  the  busbar  of  the 
transformer, computes the load carried by the transformer 
just before the event happened, and if possible, causes the 
remaining transformer to take the entire load, as e.g. in the 
following scenario: 
•  (Initially)  Red  transformer  carries  32 MVA  (8  lines  of 
4 MVA) and Green transformer 24 MVA (8 x 3 MVA); 
the  switches  connecting  the  Red  and  Green  bars  to  the 
transformers are closed; the switch connecting the Green 
MV bar to the Red MV bar is open. 
•  (Anomaly)  An  internal  protection  mechanism  shuts 
down  the  Green  transformer,  and  its  power  drops  from 
24 MVA  to  zero.  The  switch  connecting  the  Green  bar 
to the Green transformer opens. (The switch connecting 
the  Red  bar  to  the  Red  transformer  remains  closed  and 
the switch connecting the two bars remains open.) 
•  (Reaction)  The  switch  connecting  the  Green  bar  to  the 
Red  bar  receives  the  command  to  close.  It  closes 
1 execution cycle (100 ms) later and the load carried by 
the Red transformer rises to 56 MVA. 
Function2 (parallel transformers) consists of a series 
of  automatic  actions,  assisting  remote  operators.  E.g.,  an 
operator  can  request  to  switch  on  a  transformer  and 
function2
into  a  specific 
sequence  of  commands.  Such  a  re-insertion  scenario  may 
be applied some time after transformer exclusion. 
this  request 
translates 
3.1  System setup  
The  PSAS  application  has  been  developed  using  a 
proprietary, automata-based, design environment based on 
technique  ASFA  [11].  Application 
the  specification 
development consists of several steps:  
•  Function1 and  function2 are  extracted  from  the 
PSAS  application  [2]  and  specified  through  the  ASFA
Graphical  Editor  [1],  obtaining  a  tabular  description  of 
the pilot application.  
•  These  ASFA  tables  are  processed  by  the  ASFA-C 
Translator  [13],  producing  a  target-independent  C-code 
version of the application, and by the ASFA Partitioner,
allowing an application to be mapped to a single task or 
decomposed  into  a  set  of  tasks  [4].  The  single  task 
version  has  been  used  for  the  functional  test  of  the 
application  on  a  single  host  node,  while  a  four-task 
version was selected for testing on a distributed system. 
•  At run time, the Distributed Execution Support Module,
composed of BasicSoftware (BSW) and Executive,
enforces  cyclic  execution, 
for  PLC-based 
automation 
(PLC=Programmable  Logic 
Controller).  Robust  execution  is  ensured  by  cyclically 
refreshing the I/O image and the non-protected memory 
areas,  while  the  application’s  state  is  safeguarded  by 
hardware  or  software  mechanisms  [5].  The  BSW  takes 
care  of  synchronization  and  exception  handling,  while 
the  Executive supplies  the  RTOS  interface  and  a  set 
of ASFA-specific library functions. 
systems 
typical 
A  peculiarity  of  the  ASFA  environment  is  that  the 
application  code  is  automatically  obtained  by  translating 
the  automata-based  specification  [11].  Besides  reducing 
the probability of introducing coding errors, this approach 
provides  portability  to  all  platforms  supported  by  the 
Distributed Execution Support Module. 
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:03:25 UTC from IEEE Xplore.  Restrictions apply. 
N1
N2
N3
N4
N5
VxWorks
VxWorks
VMIC
IPC
* 
* 
INOVA
IPC
* 
* 
RMOS32
Siemens
Linux
BB PC
WinNT
OC PC 
PLC
* 
* 
* 
* 
* 
* 
* 
* 
BSL tasks 
Backbone  
LAN
Monitor
Operator
Console
BSW  
Executive 
Table 1: Allocation of middleware tasks to nodes
* 
* 
* 
* 
* 
* 
As  shown  in  Figure  2,  this  pilot  application  was 
deployed  on  a  distributed  system  consisting  of  three 
dedicated  heterogeneous  (“target”)  processors  for  the 
automation  functions  and  two  standard  PCs  for  support 
functions, interconnected by an Ethernet switch:  
•  N1  and  N2:  two  industrial  PCs  (VMIC  and  INOVA),
with VxWorks as RTOS; 
•  N3:  Siemens SIMATIC  M7:  an  extended  PLC  with  I/O 
modules, with RMOS32 as RTOS; 
•  N4: Linux-based standard PCs, hosting the BackBone; 
•  N5: Windows-NT PC  with Operator Console functions. 
For  inter-site  connections  (not  considered  here),  an 
additional node hosts the gateway software. 
The  pilot  application  runs  on  this  heterogeneous 
hardware equipment; input and output from/to the field is 
simulated.  Synchronization  signals,  for  cyclic  application 
execution, are generated by the internal clock of one of the 
nodes  (in  a  real  set-up,  they  are  obtained  from  an 
independent,  external  device).  The  following  assumptions 
are made for the target nodes: 
•  All  three  target  nodes  (N1,  N2  and  N3  in  Table 1)  are 
attached  to  I/O  components  on  the  field  (PU  = 
L IN EA  AT
L IN EA  AT
LI NE A  A T  U T E N T E
 sb arra A T
 sb arr a A T
N3: Siemens PLC
N4: PC Linux
N5: PC Windows-NT
GUI
Backbone
Switched Ethernet
 AT
 M T
N2: IPC INOVA
N1: IPC VMIC
VxWorks
VxWorks
 A T
 M T
S E RV IZ I
A U SIL I ARI
 B T
 M T
 sb arra M T
for real-time services
 sb arra M T
Local Site
L IN EE  M T
B A TT.  CO ND .
SH U NT
SHU N T
B A T T. CO ND .
L IN E E  M T
Figure 2: PSAS hardware architecture 
Peripheral Units on Figure 1). 
•  The target node N3  handles the  synchronization signal. 
In order to provide a backup solution in case of fault on 
N3,  synchronization  interrupts  are  also  available  at  N1 
and N3. 
3.2 
Instantiating DepAuDE on PSAS 
The run-time components of the DepAuDE framework 
are  integrated  into  the  PSAS  pilot  application  (see 
Table 1). The fault containment region is a node. 
•  An  RMOS32  and  a  VxWorks  implementation  of  the 
BSL tasks run on the target nodes (N1, N2, N3); a Linux 
and WinNT version runs on the host nodes (N4, N5).  
•  A LAN Monitor – an FTM used for detecting crashed or 
isolated nodes – is present on all nodes.  
•  The  BackBone  task,  executing  recovery  strategies,  is 
allocated to N4.
•  Each  of  the  three  target  nodes  hosts  an  instance  of  the 
ASFA  Distributed  Execution  Support  Module, 
composed of BSW and Executive. Each instance of the 
BSW is able to act as master (BSW_M, on the master node) 
or slave (BSW_S, on the slave nodes). The role is chosen 
depending  on  the  specific  system  configuration.  All 
BSW_S  make  up 
the  BSW_SLAVE_GROUP.  The 
configuration  with  highest  performance  (see  below) 
requires  BSW_M  to  be  allocated  to  N3  and  the  BSW_S 
processes  to  run  on  N1  and  N2.  Executive  process 
instances are identical on each processing node and they 
compose the EXECUTIVE_GROUP.
The  allocation  of  the  application  tasks  depends  on  the 
partitioning  of  the  two  LCL  functions  (function1  and 
function2),  among  which  there  is  no  communication. 
Function2  consists  of  a  single  task,  PARALLEL_TRS,
function1  (automatic  power  resumption)  consists  of 
three  tasks:  two  tasks  (BUSBAR1  and  BUSBAR2)  handle 
low-level, I/O dependent, computations relative to the MV 
lines attached to each busbar; one task, STRAT, coordinates 
the whole function and performs no field I/O. There is no 
communication between the two BUSBAR tasks, while both 
communicate with STRAT.
The basic constraint for allocating tasks to nodes is that 
a  task  that  controls  a  specific  plant  component  should  be 
allocated  to  a  processor  attached  to  that  plant  component 
(due  to  I/O  paths).  As  both  functions  of  the  pilot 
application control the same set of field components (same 
transformers  and  switches),  all  target  nodes  are  assumed 