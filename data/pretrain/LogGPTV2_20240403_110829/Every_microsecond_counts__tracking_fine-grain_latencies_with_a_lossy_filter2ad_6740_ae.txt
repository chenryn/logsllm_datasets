We have sketched out the base logic for LDA that we estimate
takes less than 1% of a low-end 10 mm × 10 mm networking ASIC,
based on the standard 400-MHz 65-nm process currently being em-
ployed by most networking chipset vendors. The logic is ﬂow-
through—in other words, it can be inserted into the path of a link
between the sender and receiver end without changing any other
logic—allowing LDA to be incrementally deployed within existing
router designs.
A minimal implementation would place a single LDA together
with MAC logic at ingress and egress links; ingress-egress paths
pass through a number of points within a router where packets can
be queued and, therefore, delayed or lost. We envision, however,
that deployed LDAs will contain a packet classiﬁer that identiﬁes
a particular class of trafﬁc, such as ﬂows to a speciﬁed TCP port.
Hence, it may be useful to include multiple LDAs on a single line
card. Most mid-range ASICs should be able to afford up to ten
separate copies of the LDA logic to handle various trafﬁc classes
and/or priority levels. Finally, we observe that while many routers
internally stripe packets at various points in the router which would
break our FIFO assumption, packets are resequenced at various
points. Thus, rather than placing LDA logic on every striped path,
it may be cheaper to place the LDA receiver logic where resequenc-
ing takes place.
Figure 10 shows a schematic of our initial design. The logic at
sender and receiver is nearly identical. At the sender (receiver) the
ﬁrst X bytes of the packet—say ﬁfty—are sent to the logic. The
Packet
Control ?
Control Logic
Bank 1
Bank M
Counter TS_Sum
Counter TS_Sum
Classifier
Extract
Hash
Update Logic
Figure 10: Potential LDA chip schematic
logic ﬁrst determines if it is a control or data packet using, say, an
Ethernet type ﬁeld.
If the received packet is a data packet, a classiﬁer is used to se-
lect the type of packet being measured. The update logic extracts
some ﬁxed bytes from the packet (say bytes 50–100) and computes
a hash. H3 hash functions [29], for example, can be implemented
efﬁciently in hardware using XOR arrays and can be easily modi-
ﬁed. Our estimates use a Rabin hash whose loop is unrolled to run
at 40 Gbps using around 20,000 gates.
The hash output supplies a 64-bit number which is passed to the
update logic. The high-order bits select the sampling probability
which in turn determines which bank is selected. For example, if
there are two banks, that are selected with probabilities 1/2 and
1/64, the six high-order bits are used. If the ﬁrst six bits are zero,
the second bank is selected; if the ﬁrst six bits are non-zero and the
ﬁrst bit is zero, the ﬁrst bank is selected.
If a bank is selected, the low-order bits of the hash are used to
post a read to the corresponding bank. For example, if each bank
has 1,024 counters, we use the ten low-order bits. The update logic
then reads the 72-bit value stored at the indicated location. The ﬁrst
32 bits are a simple packet counter that is incremented. The last 40
bits are a time stamp sum (allows nanosecond precision) to which
the current value of the hardware clock is added. The updated value
is then written back to the same location.
The sender-side logic conceptually generates control packets
at the end of each measurement interval. Control packets are
sequence-numbered so that loss of control packets translate into
a measurement interval being ignored. When the receiver logic re-
ceives the sender’s control packets and updates its own, it sends
the control packets to a line-card processor which computes delay,
loss, and variance estimates in software which it can then report to
a management station on demand.
The control logic can work in two ways. The simplest way is to
keep two copies of each counter so that the control logic can work
on reading and zeroing LDA counters for a prior interval into con-
trol packet(s) concurrently with the update process. Alternately,
two control packets can be used: one to record the end of an in-
terval, and a second control packet sent T
seconds later to denote
the start of the next interval. During the intervening period, the up-
date logic is disabled to allow the control logic to read all counters.
The disadvantage is that though T
can be small, a small number of
samples (say 100) are ignored.
(cid:3)
(cid:3)
The logic for counters is placed in SRAM while the remaining
logic is implemented in ﬂops. In a 65-nm 400-Mhz process, 1,000
264SRAM counters of 72 bits each takes 0.13 mm2. While the size for
the hash logic is about 20,000 gates, we conservatively estimate an-
other 30,000 gates for the classiﬁer (a simple mask-and-compare to
one speciﬁed header), header extraction, and counter update, yield-
ing a total of around 50,000, or approximately 0.1 mm2 in a 65-nm
process. The grand total is around ≈ 0.23 mm2. Even if we dou-
ble the width of the counters and keep two copies of the entire data
structure (to handle sender and receiver logic), an LDA still repre-
sents less than 1% of the area of the lowest-end (10 mm × 10 mm)
ASICs on the market today.
5.2 Deployment and fault localization
The easiest path to deployment is to ﬁrst deploy within individ-
ual routers where the majority of loss and delay occur. It may also
be useful to deploy across links because of optical device (e.g.,
SONET) reconﬁgurations and degradations. The difﬁculty with de-
ploying across links is the need for microsecond precision and the
need for a protocol change. Fortunately, a solution to both prob-
lems can be found in terms of a new precision time-synchronization
standard called IEEE 1588 [15] being deployed by major router
vendors. IEEE 1588 uses synchronization messages that are inter-
cepted by hardware. IEEE 1588 can easily be extended to handle
LDA using a few extra control message types and the logic de-
scribed above.
Signiﬁcant beneﬁts can be derived from a full deployment, where
LDAs are deployed at each and every router and link.
In par-
ticular, performance fault localization—traditionally a very chal-
lenging problem [18, 39]—becomes straightforward. We envision
the presence of a centralized monitoring station which could use
a topology monitor (such as OSPF monitor [32]) to decompose a
misbehaving end-to-end path into segments, and query each seg-
ment to isolate the misbehaving (e.g., high-delay) segment. Scal-
ing to hundreds or even thousands of collectors seems straightfor-
ward, as each summary structure is only a few kilobits in size. Even
maintaining one-second intervals—which may be overkill for large
deployments—the bandwidth requirement at the collection point
would be on the order of a megabit per second for a thousand mea-
surement points.
Even stopping short of pervasive deployment, LDA can be ex-
tended to include virtual links between pairs of upgraded routers, in
an overlay topology consisting of just upgraded routers connected
via legacy router hops. We omit the details for lack of space, but
our experiments with RocketFuel topologies show that upgrading
1/6th of the routers in the Sprint topology reduces the localization
granularity (the average path length between upgraded routers) to
around 1.5.
6. RELATED WORK
Traditionally, network operators determined link and hop proper-
ties using active measurement tools and inference algorithms. For
example, the work by Chen et al. [6] and Dufﬁeld et al. [8] solve the
problem of predicting the per-hop loss and latency characteristics
based on end-to-end measurements (e.g., conducted using active
probing tools [33, 24]) and routing information obtained from the
network (e.g., using OSPF monitoring [32]). The advantages of our
approach in comparison are two fold. First, LDA computes path
and link properties by passively monitoring trafﬁc in a router, so
it does not interfere with measurements or waste bandwidth by in-
jecting any active probes. Second, LDA captures ﬁne-grain latency
measurements that can be only be matched by extremely high fre-
quency active probes (as discussed in Section 4.3). Further, in our
evaluation, we compared against localized active probes (i.e., be-
tween every pair of adjacent routers), which are more ﬁne-grain
than the current best practice (end-to-end probing) as it does not
scale, requiring the monitoring of O(m) ≈ O(n2) segments where
m is the number of links, n is the number of routers.
We are not the ﬁrst to suggest router extensions in support of
ﬁne-grain measurement. For example, Machiraju et al.
argue
for a measurement-friendly network architecture where individ-
ual routers provide separate priority levels for active probes [23].
Dufﬁeld et al. suggest the use of router support for sampling packet
trajectories [10]. Passive measurement of loss and delay by directly
comparing trajectory samples of the same packet observed at dif-
ferent points has been studied by Zseby et al. [40] and Dufﬁeld
et al. [9]. Many high-speed router primitives have also been sug-
gested in the literature for measuring ﬂow statistics and detecting
heavy-hitters [7, 11].
Papagiannaki et al. used GPS-synchronized (to microsecond ac-
curacy) passive monitoring cards to trace all packets entering and
leaving a Sprint backbone router [28]. Each packet generates a
ﬁxed-size time-stamped record, allowing exact delays, as well as
other statistics, to be computed to within clock accuracy. From a
measurement standpoint, their approach represents the ideal: exact
packet-for-packet accounting. Unfortunately, as they themselves
point out, such an approach is “computationally intensive and de-
manding in terms of storage,” making wide-spread production de-
ployment infeasible. Hohn et al. describe a mechanism to obtain
router delay information using the amplitude and duration of busy
periods [14]. While their approach provides only an approximate
distribution, it can be effective in determining the order of magni-
tude of delay.
7. CONCLUSION
This paper proposes a mechanism that vendors can embed di-
rectly in routers to cheaply provide ﬁne-grain delay and loss mea-
surement. Starting from the simple idea of keeping a sum of sent
timestamps and a sum of receive timestamps which is not resilient
to loss, we developed a strategy to cope with loss using multiple
hash buckets, and multiple sampling granularities to deal with un-
known loss values. Further, we adapt the classic approach to L2-
norm estimation in a single stream to also calculate the standard
deviation of delay. Loss estimation, of course, falls out trivially
from these data structures.
We emphasize that our mechanism complements—but does not
replace—end-to-end probes. Customers will continue to use end-
to-end probes to monitor the end-to-end performance of their appli-
cations. Further, it is unlikely that LDA will be deployed at all links
along many paths in the near future. However, LDA probes can
proactively discover latency issues, especially at very ﬁne scales,
that a network manager can then address. Further, if an end-to-end
probe detects a problem, a manager can use the LDA mechanism
on routers along the path to better localize the problem.
While our setting begs comparisons to streaming, we introduce
a new streaming problem: two-party coordinated streaming with
loss. In this setting, problems that were trivial in the single-party
streaming setting (such as identifying the maximum value) are now
provably hard. Thus, we believe coordinated streaming may be
an interesting research area in its own right: Which coordinated
functions can be computed with low memory? Further, there are
functions which would be useful in practice (e.g., loss distributions)
that we do not yet know how to compute efﬁciently.
From a router-vendor standpoint, the efﬁciency of the proposed
technique seems acceptable. Moreover, we observe that all mi-
crochips today have a component called JTAG whose overhead
chip vendors happily pay for the beneﬁt of increased ease of con-
ﬁguration and debugging. Our broader vision is that all networking
265chips should also have a small “MTAG” component to facilitate
ﬁne-grain measurement of latency and loss. The LDA primitives
described in this paper would be a candidate for such an MTAG
component. With such a component universally deployed, the net-
work manager of the future could pin-point loss spikes anywhere
in the networking path of a critical network application with mi-
crosecond accuracy.
Acknowledgments
Hilary Finucane ﬁrst observed that arbitrary columns of the LDA
can be summed to compute a more accurate estimate. In addition,
the authors are indebted to John Huber of Cisco Systems for provid-
ing sizing information critical to our hardware design; and Michael
Mitzenmacher, Subhash Suri, the anonymous reviewers, and Dar-
ryl Veitch, our shepherd, for comments on previous versions of this
manuscript. This work was supported in part by NSF awards CNS-
0347949, CNS-0831647, and a grant from Cisco Systems.
8. REFERENCES
[1] Corvil, Ltd. http://www.corvil.com.
[2] Multicast-based intference of network-internal characteristics.
http://gaia.cs.umass.edu/minc/.
[3] ALON, N., MATIAS, Y., AND SZEGEDY, M. The space complexity
of approximating the frequency moments. J. Computer and System
Sciences 58, 1 (Feb. 1999), 137–147.
[4] ARISTA NETWORKS, INC. 7100 series datasheet. http://www.
aristanetworks.com/en/7100_datasheet.pdf, 2008.
[5] BEIGBEDER, T., COUGHLAN, R., LUSHER, C., PLUNKETT, J.,
AGU, E., AND CLAYPOOL, M. The effects of loss and latency on
user performance in Unreal Tournament 2003. In Proceedings of the
ACM SIGCOMM Workshop on Network Games (Aug. 2004).
[6] CHEN, Y., BINDEL, D., SONG, H., AND KATZ, R. H. An algebraic
approach to practical and scalable overlay network monitoring. In
ACM SIGCOMM (Sept. 2004).
[7] DOBRA, A., GAROFALAKIS, M., GEHRKE, J. E., AND RASTOGI,
R. Processing complex aggregate queries over data streams. In
Proceedings of ACM SIGMOD (June 2002).
[8] DUFFIELD, N. Simple network performance tomography. In
Proceedings of USENIX/ACM Internet Measurement Conference
(Oct. 2003).
[9] DUFFIELD, N., GERBER, A., AND GROSSGLAUSER, M. Trajectory
engine: A backend for trajectory sampling. In Proceedings of IEEE
Network Operations and Management Symposium (Apr. 2002).
[10] DUFFIELD, N., AND GROSSGLAUSER, M. Trajectory sampling for
direct trafﬁc observation. In Proceedings of ACM SIGCOMM (Aug.
2000).
[11] ESTAN, C., AND VARGHESE, G. New directions in trafﬁc
measurement and accounting: Focusing on the elephants, ignoring
the mice. ACM Transactions on Computer Systems 21, 3 (Aug. 2003).
[12] ESTAN, C., VARGHESE, G., AND FISK, M. Bitmap algorithms for
counting active ﬂows on high speed links. In Proceedings of the
USENIX/ACM Internet Measurement Conference (Oct. 2003).
[13] HOEFFDING, W. Probability inequalities for sums of bounded
random variables. J. American Statistical Association 58, 301 (March
1963), 13–30.
[14] HOHN, N., VEITCH, D., PAPAGIANNAKI, K., AND DIOT, C.
Bridging router performance and queuing theory. In Proceedings of
ACM SIGMETRICS (June 2004).
[15] IEEE. Standard for a Precision Clock Synchronization Protocol for
Networked Measurement and Control Systems, 2002. IEEE/ANSI
1588 Standard.
[16] INCITS. Fibre channel backbone-5 (FC-BB-5), Oct. 2008. Ver. 1.03.
[17] KANDULA, S., KATABI, D., AND VASSEUR, J. P. Shrink: A tool for
failure diagnosis in IP networks. In Proceedings of ACM SIGCOMM
MineNet Workshop (Aug. 2005).
[18] KOMPELLA, R. R., YATES, J., GREENBERG, A., AND SNOEREN,
A. C. Detection and localization of network black holes. In
Proceedings of IEEE Infocom (May 2007).
[19] KUMAR, A., SUNG, M., XU, J., AND ZEGURA, E. W. A data
streaming algorithm for estimating subpopulation ﬂow size
distribution. In Proceedings of ACM SIGMETRICS (June 2005).
[20] LALL, A., SEKAR, V., OGIHARA, M., XU, J., AND ZHANG, H.
Data streaming algorithms for estimating entropy of network trafﬁc.
In Proceedings of ACM SIGMETRICS (June 2006).
[21] LONDON STOCK EXCHANGE PLC. Launch of exchange hosting
creates sub-millisecond access to its markets.
http://www.londonstockexchange.com/NR/exeres/
04192D02-B949-423D-94E2-683D7506C530.htm, Sept.
2008.
[22] LU, Y., MONTANARI, A., PRABHAKAR, B., DHARMAPURIKAR,
S., AND KABBANI, A. Counter braids: a novel counter architecture
for per-ﬂow measurement. In Proceedings of ACM SIGMETRICS
(June 2008).
[23] MACHIRAJU, S., AND VEITCH, D. A measurement-friendly
network (MFN) architecture. In Proceedings of ACM SIGCOMM
Workshop on Internet Network Management (Sept. 2006).
[24] MAHDAVI, J., PAXSON, V., ADAMS, A., AND MATHIS, M.
Creating a scalable architecture for internet measurement. In
Proceedings of INET (July 1998).
[25] MARTIN, R. Wall street’s quest to process data at the speed of light.
http://www.informationweek.com/news/
infrastructure/showArticle.
jhtml?articleID=199200297.
[26] MISRA, V., GONG, W.-B., AND TOWSLEY, D. Stochastic
differential equation modeling and analysis of tcp windowsize
behavior. In Proceedings of IFIP WG 7.3 Performance (Nov. 1999).
[27] NGUYEN, H. X., AND THIRAN, P. Network loss inference with
second order statistics of end-to-end ﬂows. In Proceedings of ACM
Internet Measurement Conference (Oct. 2007).
[28] PAPAGIANNAKI, K., MOON, S., FRALEIGH, C., THIRAN, P.,
TOBAGI, F., AND DIOT, C. Analaysis of measured single-hop delay
from an operational backbone network. IEEE Journal on Selected
Areas in Communications 21, 6 (Aug. 2003).
[29] RAMAKRISHNA, M., FU, E., AND BAHCEKAPILI, E. Efﬁcient
hardware hashing functions for high performance computers. IEEE
Transactions on Computers 46, 12 (Dec. 1997).
[30] RISKA, M., MALIK, D., AND KESSLER, A. Trading ﬂow
architecture. Tech. rep., Cisco Systems, Inc.
http://www.cisco.com/en/US/docs/solutions/
Verticals/Trading_Floor_Architecture-E.pdf.
[31] SAVAGE, S. Sting: a TCP-based network measurement tool. In
Proceedings of USENIX Symposium on Internet Technologies and
Systems (Oct. 1999).
[32] SHAIKH, A., AND GREENBERG, A. OSPF monitoring:
Architecture, design and deployment experience. In Proceedings of
USENIX NSDI (Mar. 2004).
[33] SOMMERS, J., BARFORD, P., DUFFIELD, N., AND RON, A.
Improving accuracy in end-to-end packet loss measurement. In
Proceedings of ACM SIGCOMM (Aug. 2005).
[34] SZIGETI, T., AND HATTINGH, C. Quality of service design
overview. http://www.ciscopress.com/articles/
article.asp?p=357102&seqNum=2, Dec. 2004.
[35] TOOMEY, F. Monitoring and analysis of trafﬁc for low-latency
trading networks. Tech. rep., Corvil, Ltd., 2008.
[36] VARDI, Y. Network tomography: estimating source-destination
trafﬁc intensities from link data. J. American Statistical Association
91 (1996), 365–377.
[37] WOVEN SYSTEMS, INC. EFX switch series overview.
http://www.wovensystems.com/pdfs/products/
Woven_EFX_Series.pdf, 2008.
[38] ZHANG, Y., ROUGHAN, M., DUFFIELD, N., AND GREENBERG, A.
Fast accurate computation of large-scale IP trafﬁc matrices from link
loads. In Proceedings of ACM SIGMETRICS (June 2003).
[39] ZHAO, Y., CHEN, Y., AND BINDEL, D. Towards unbiased
end-to-end network diagnosis. In Proceedings of ACM SIGCOMM
(Sept. 2006).
[40] ZSEBY, T., ZANDER, S., AND CARLE, G. Evaluation of building
blocks for passive one-way-delay measurements. In Proceedings of
Passive and Active Measurement Workshop (Apr. 2001).
266