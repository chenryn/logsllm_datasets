### Optimized Text

We have designed the base logic for LDA, which we estimate will occupy less than 1% of a low-end 10 mm × 10 mm networking ASIC, based on the standard 400-MHz 65-nm process commonly used by most networking chipset vendors. The logic is flow-through, meaning it can be inserted into the path of a link between the sender and receiver without altering any other logic, allowing LDA to be incrementally deployed within existing router designs.

A minimal implementation would place a single LDA together with MAC logic at both ingress and egress links. Ingress-egress paths pass through several points within a router where packets can be queued, potentially causing delays or losses. However, we envision that deployed LDAs will include a packet classifier to identify specific traffic classes, such as flows to a specified TCP port. Therefore, it may be beneficial to incorporate multiple LDAs on a single line card. Most mid-range ASICs should be capable of supporting up to ten separate instances of LDA logic to handle various traffic classes and/or priority levels. Additionally, while many routers internally stripe packets, which would break our FIFO assumption, packets are resequenced at various points. Thus, rather than placing LDA logic on every striped path, it may be more cost-effective to place the LDA receiver logic where resequencing occurs.

**Figure 10: Potential LDA Chip Schematic**

The logic at the sender and receiver is nearly identical. At both the sender and receiver, the first X bytes of the packet—say fifty—are sent to the logic. The logic first determines if the packet is a control or data packet using, for example, an Ethernet type field.

If the received packet is a data packet, a classifier is used to select the type of packet being measured. The update logic then extracts some fixed bytes from the packet (e.g., bytes 50–100) and computes a hash. For instance, H3 hash functions [29] can be efficiently implemented in hardware using XOR arrays and can be easily modified. Our estimates use a Rabin hash, which is unrolled to run at 40 Gbps using approximately 20,000 gates.

The hash output provides a 64-bit number, which is passed to the update logic. The high-order bits determine the sampling probability, which in turn selects the bank. For example, if there are two banks selected with probabilities 1/2 and 1/64, the six high-order bits are used. If the first six bits are zero, the second bank is selected; if the first six bits are non-zero and the first bit is zero, the first bank is selected.

If a bank is selected, the low-order bits of the hash are used to post a read to the corresponding bank. For instance, if each bank has 1,024 counters, the ten low-order bits are used. The update logic then reads the 72-bit value stored at the indicated location. The first 32 bits are a simple packet counter that is incremented. The last 40 bits are a time stamp sum (allowing nanosecond precision) to which the current value of the hardware clock is added. The updated value is then written back to the same location.

The sender-side logic conceptually generates control packets at the end of each measurement interval. Control packets are sequence-numbered so that the loss of control packets results in the measurement interval being ignored. When the receiver logic receives the sender’s control packets and updates its own, it sends the control packets to a line-card processor, which computes delay, loss, and variance estimates in software and reports them to a management station on demand.

The control logic can operate in two ways. The simplest method is to maintain two copies of each counter, allowing the control logic to read and zero LDA counters for a prior interval into control packets concurrently with the update process. Alternatively, two control packets can be used: one to record the end of an interval, and a second control packet sent T seconds later to denote the start of the next interval. During this period, the update logic is disabled to allow the control logic to read all counters. The disadvantage is that although T can be small, a small number of samples (e.g., 100) may be ignored.

The logic for counters is placed in SRAM, while the remaining logic is implemented in flops. In a 65-nm 400-MHz process, 1,000 72-bit SRAM counters take up 0.13 mm². While the size for the hash logic is about 20,000 gates, we conservatively estimate another 30,000 gates for the classifier (a simple mask-and-compare to a specified header), header extraction, and counter update, yielding a total of around 50,000 gates, or approximately 0.1 mm² in a 65-nm process. The grand total is around 0.23 mm². Even if we double the width of the counters and keep two copies of the entire data structure (to handle sender and receiver logic), an LDA still represents less than 1% of the area of the lowest-end (10 mm × 10 mm) ASICs on the market today.

### 5.2 Deployment and Fault Localization

The easiest path to deployment is to first deploy within individual routers where the majority of loss and delay occur. It may also be useful to deploy across links due to optical device (e.g., SONET) reconfigurations and degradations. The difficulty with deploying across links is the need for microsecond precision and a protocol change. Fortunately, a solution to both problems can be found in the form of a new precision time-synchronization standard called IEEE 1588 [15], which is being deployed by major router vendors. IEEE 1588 uses synchronization messages intercepted by hardware and can be easily extended to handle LDA using a few extra control message types and the logic described above.

Significant benefits can be derived from a full deployment, where LDAs are deployed at each and every router and link. In particular, performance fault localization—a traditionally challenging problem [18, 39]—becomes straightforward. We envision a centralized monitoring station that could use a topology monitor (such as OSPF monitor [32]) to decompose a misbehaving end-to-end path into segments and query each segment to isolate the misbehaving (e.g., high-delay) segment. Scaling to hundreds or even thousands of collectors seems straightforward, as each summary structure is only a few kilobits in size. Even maintaining one-second intervals—which may be overkill for large deployments—the bandwidth requirement at the collection point would be on the order of a megabit per second for a thousand measurement points.

Even short of pervasive deployment, LDA can be extended to include virtual links between pairs of upgraded routers in an overlay topology consisting of just upgraded routers connected via legacy router hops. Our experiments with RocketFuel topologies show that upgrading 1/6th of the routers in the Sprint topology reduces the localization granularity (the average path length between upgraded routers) to around 1.5.

### 6. Related Work

Traditionally, network operators determined link and hop properties using active measurement tools and inference algorithms. For example, the work by Chen et al. [6] and Duffield et al. [8] solves the problem of predicting per-hop loss and latency characteristics based on end-to-end measurements (e.g., conducted using active probing tools [33, 24]) and routing information obtained from the network (e.g., using OSPF monitoring [32]). The advantages of our approach are twofold. First, LDA computes path and link properties by passively monitoring traffic in a router, so it does not interfere with measurements or waste bandwidth by injecting any active probes. Second, LDA captures fine-grain latency measurements that can only be matched by extremely high-frequency active probes (as discussed in Section 4.3). Further, in our evaluation, we compared against localized active probes (i.e., between every pair of adjacent routers), which are more fine-grain than the current best practice (end-to-end probing) as it does not scale, requiring the monitoring of O(m) ≈ O(n²) segments where m is the number of links, n is the number of routers.

We are not the first to suggest router extensions in support of fine-grain measurement. For example, Machiraju et al. [23] argue for a measurement-friendly network architecture where individual routers provide separate priority levels for active probes. Duffield et al. [10] suggest the use of router support for sampling packet trajectories. Passive measurement of loss and delay by directly comparing trajectory samples of the same packet observed at different points has been studied by Zseby et al. [40] and Duffield et al. [9]. Many high-speed router primitives have also been suggested in the literature for measuring flow statistics and detecting heavy-hitters [7, 11].

Papagiannaki et al. [28] used GPS-synchronized (to microsecond accuracy) passive monitoring cards to trace all packets entering and leaving a Sprint backbone router. Each packet generates a fixed-size time-stamped record, allowing exact delays, as well as other statistics, to be computed to within clock accuracy. From a measurement standpoint, their approach represents the ideal: exact packet-for-packet accounting. Unfortunately, as they themselves point out, such an approach is “computationally intensive and demanding in terms of storage,” making wide-spread production deployment infeasible. Hohn et al. [14] describe a mechanism to obtain router delay information using the amplitude and duration of busy periods. While their approach provides only an approximate distribution, it can be effective in determining the order of magnitude of delay.

### 7. Conclusion

This paper proposes a mechanism that vendors can embed directly in routers to provide fine-grain delay and loss measurement at a low cost. Starting from the simple idea of keeping a sum of sent timestamps and a sum of receive timestamps, which is not resilient to loss, we developed a strategy to cope with loss using multiple hash buckets and multiple sampling granularities to deal with unknown loss values. Further, we adapt the classic approach to L2-norm estimation in a single stream to also calculate the standard deviation of delay. Loss estimation, of course, falls out trivially from these data structures.

We emphasize that our mechanism complements—but does not replace—end-to-end probes. Customers will continue to use end-to-end probes to monitor the end-to-end performance of their applications. Further, it is unlikely that LDA will be deployed at all links along many paths in the near future. However, LDA probes can proactively discover latency issues, especially at very fine scales, that a network manager can then address. Additionally, if an end-to-end probe detects a problem, a manager can use the LDA mechanism on routers along the path to better localize the problem.

While our setting invites comparisons to streaming, we introduce a new streaming problem: two-party coordinated streaming with loss. In this setting, problems that were trivial in the single-party streaming setting (such as identifying the maximum value) are now provably hard. Thus, we believe coordinated streaming may be an interesting research area in its own right: Which coordinated functions can be computed with low memory? Further, there are functions that would be useful in practice (e.g., loss distributions) that we do not yet know how to compute efficiently.

From a router-vendor standpoint, the efficiency of the proposed technique seems acceptable. Moreover, we observe that all microchips today have a component called JTAG, whose overhead chip vendors willingly pay for the benefit of increased ease of configuration and debugging. Our broader vision is that all networking chips should also have a small "MTAG" component to facilitate fine-grain measurement of latency and loss. The LDA primitives described in this paper would be a candidate for such an MTAG component. With such a component universally deployed, the network manager of the future could pinpoint loss spikes anywhere in the networking path of a critical network application with microsecond accuracy.

### Acknowledgments

Hilary Finucane first observed that arbitrary columns of the LDA can be summed to compute a more accurate estimate. In addition, the authors are indebted to John Huber of Cisco Systems for providing sizing information critical to our hardware design; and Michael Mitzenmacher, Subhash Suri, the anonymous reviewers, and Darryl Veitch, our shepherd, for comments on previous versions of this manuscript. This work was supported in part by NSF awards CNS-0347949, CNS-0831647, and a grant from Cisco Systems.

### 8. References

[1] Corvil, Ltd. http://www.corvil.com.
[2] Multicast-based inference of network-internal characteristics. http://gaia.cs.umass.edu/minc/.
[3] ALON, N., MATIAS, Y., AND SZEGEDY, M. The space complexity of approximating the frequency moments. J. Computer and System Sciences 58, 1 (Feb. 1999), 137–147.
[4] ARISTA NETWORKS, INC. 7100 series datasheet. http://www.aristanetworks.com/en/7100_datasheet.pdf, 2008.
[5] BEIGBEDER, T., COUGHLAN, R., LUSHER, C., PLUNKETT, J., AGU, E., AND CLAYPOOL, M. The effects of loss and latency on user performance in Unreal Tournament 2003. In Proceedings of the ACM SIGCOMM Workshop on Network Games (Aug. 2004).
[6] CHEN, Y., BINDEL, D., SONG, H., AND KATZ, R. H. An algebraic approach to practical and scalable overlay network monitoring. In ACM SIGCOMM (Sept. 2004).
[7] DOBRA, A., GAROFALAKIS, M., GEHRKE, J. E., AND RASTOGI, R. Processing complex aggregate queries over data streams. In Proceedings of ACM SIGMOD (June 2002).
[8] DUFFIELD, N. Simple network performance tomography. In Proceedings of USENIX/ACM Internet Measurement Conference (Oct. 2003).
[9] DUFFIELD, N., GERBER, A., AND GROSSGLAUSER, M. Trajectory engine: A backend for trajectory sampling. In Proceedings of IEEE Network Operations and Management Symposium (Apr. 2002).
[10] DUFFIELD, N., AND GROSSGLAUSER, M. Trajectory sampling for direct traffic observation. In Proceedings of ACM SIGCOMM (Aug. 2000).
[11] ESTAN, C., AND VARGHESE, G. New directions in traffic measurement and accounting: Focusing on the elephants, ignoring the mice. ACM Transactions on Computer Systems 21, 3 (Aug. 2003).
[12] ESTAN, C., VARGHESE, G., AND FISK, M. Bitmap algorithms for counting active flows on high speed links. In Proceedings of the USENIX/ACM Internet Measurement Conference (Oct. 2003).
[13] HOEFFDING, W. Probability inequalities for sums of bounded random variables. J. American Statistical Association 58, 301 (March 1963), 13–30.
[14] HOHN, N., VEITCH, D., PAPAGIANNAKI, K., AND DIOT, C. Bridging router performance and queuing theory. In Proceedings of ACM SIGMETRICS (June 2004).
[15] IEEE. Standard for a Precision Clock Synchronization Protocol for Networked Measurement and Control Systems, 2002. IEEE/ANSI 1588 Standard.
[16] INCITS. Fibre channel backbone-5 (FC-BB-5), Oct. 2008. Ver. 1.03.
[17] KANDULA, S., KATABI, D., AND VASSEUR, J. P. Shrink: A tool for failure diagnosis in IP networks. In Proceedings of ACM SIGCOMM MineNet Workshop (Aug. 2005).
[18] KOMPELLA, R. R., YATES, J., GREENBERG, A., AND SNOEREN, A. C. Detection and localization of network black holes. In Proceedings of IEEE Infocom (May 2007).
[19] KUMAR, A., SUNG, M., XU, J., AND ZEGURA, E. W. A data streaming algorithm for estimating subpopulation flow size distribution. In Proceedings of ACM SIGMETRICS (June 2005).
[20] LALL, A., SEKAR, V., OGIHARA, M., XU, J., AND ZHANG, H. Data streaming algorithms for estimating entropy of network traffic. In Proceedings of ACM SIGMETRICS (June 2006).
[21] LONDON STOCK EXCHANGE PLC. Launch of exchange hosting creates sub-millisecond access to its markets. http://www.londonstockexchange.com/NR/exeres/04192D02-B949-423D-94E2-683D7506C530.htm, Sept. 2008.
[22] LU, Y., MONTANARI, A., PRABHAKAR, B., DHARMAPURIKAR, S., AND KABBANI, A. Counter braids: A novel counter architecture for per-flow measurement. In Proceedings of ACM SIGMETRICS (June 2008).
[23] MACHIRAJU, S., AND VEITCH, D. A measurement-friendly network (MFN) architecture. In Proceedings of ACM SIGCOMM Workshop on Internet Network Management (Sept. 2006).
[24] MAHDAVI, J., PAXSON, V., ADAMS, A., AND MATHIS, M. Creating a scalable architecture for internet measurement. In Proceedings of INET (July 1998).
[25] MARTIN, R. Wall Street’s quest to process data at the speed of light. http://www.informationweek.com/news/infrastructure/showArticle.jhtml?articleID=199200297.
[26] MISRA, V., GONG, W.-B., AND TOWSLEY, D. Stochastic differential equation modeling and analysis of TCP window size behavior. In Proceedings of IFIP WG 7.3 Performance (Nov. 1999).
[27] NGUYEN, H. X., AND THIRAN, P. Network loss inference with second order statistics of end-to-end flows. In Proceedings of ACM Internet Measurement Conference (Oct. 2007).
[28] PAPAGIANNAKI, K., MOON, S., FRALEIGH, C., THIRAN, P., TOBAGI, F., AND DIOT, C. Analysis of measured single-hop delay from an operational backbone network. IEEE Journal on Selected Areas in Communications 21, 6 (Aug. 2003).
[29] RAMAKRISHNA, M., FU, E., AND BAHCEKAPILI, E. Efficient hardware hashing functions for high-performance computers. IEEE Transactions on Computers 46, 12 (Dec. 1997).
[30] RISKA, M., MALIK, D., AND KESSLER, A. Trading flow architecture. Tech. rep., Cisco Systems, Inc. http://www.cisco.com/en/US/docs/solutions/Verticals/Trading_Floor_Architecture-E.pdf.
[31] SAVAGE, S. Sting: A TCP-based network measurement tool. In Proceedings of USENIX Symposium on Internet Technologies and Systems (Oct. 1999).
[32] SHAIKH, A., AND GREENBERG, A. OSPF monitoring: Architecture, design, and deployment experience. In Proceedings of USENIX NSDI (Mar. 2004).
[33] SOMMERS, J., BARFORD, P., DUFFIELD, N., AND RON, A. Improving accuracy in end-to-end packet loss measurement. In Proceedings of ACM SIGCOMM (Aug. 2005).
[34] SZIGETI, T., AND HATTINGH, C. Quality of service design overview. http://www.ciscopress.com/articles/article.asp?p=357102&seqNum=2, Dec. 2004.
[35] TOOMEY, F. Monitoring and analysis of traffic for low-latency trading networks. Tech. rep., Corvil, Ltd., 2008.
[36] VARDI, Y. Network tomography: Estimating source-destination traffic intensities from link data. J. American Statistical Association 91 (1996), 365–377.
[37] WOVEN SYSTEMS, INC. EFX switch series overview. http://www.wovensystems.com/pdfs/products/Woven_EFX_Series.pdf, 2008.
[38] ZHANG, Y., ROUGHAN, M., DUFFIELD, N., AND GREENBERG, A. Fast accurate computation of large-scale IP traffic matrices from link loads. In Proceedings of ACM SIGMETRICS (June 2003).
[39] ZHAO, Y., CHEN, Y., AND BINDEL, D. Towards unbiased end-to-end network diagnosis. In Proceedings of ACM SIGCOMM (Sept. 2006).
[40] ZSEBY, T., ZANDER, S., AND CARLE, G. Evaluation of building blocks for passive one-way-delay measurements. In Proceedings of Passive and Active Measurement Workshop (Apr. 2001).

---

This optimized text aims to improve clarity, coherence, and professionalism while preserving the original content and intent.