Fig. 9: Performance of Dyno-S, Dyno, and BFT-SMaRt.
Latency. We evaluate the latency of membership requests
for Dyno and Dyno-S. We set f = 1 and let one client
submit regular requests continuously. We let a replica submit
a membership request (join or leave) after 5000, 10000, and
50000 requests are submitted, separately (denoted as b in the
figures). We show the latency for agreement only in Fig. 9d.
Dyno has 50ms to 100ms latency on average. The results are
similar to those of regular requests. In contrast, since Dyno-S
runs a view change protocol upon every membership change,
the latency is significantly higher. The latency breakdown for
Dyno-S is shown in Fig. 9a. It can be observed that view
change is the bottleneck, and the latency is higher if more
requests are processed before the view change.
Since each new replica performs a state transfer after joining
the system, we also assess the latency of state transfer for
each of the experiments. We show the results for Dyno in
Fig. 9b. We separate the latency for state transfer (network
communication) and the latency for transition (the replica
processes the historic client requests). As shown in the figure,
the latency becomes significantly higher as b increases. This
is expected since a large number of requests need to be
synchronized during the state transfer. In contrast, the latency
for agreement is almost negligible.
For the experiment with b = 10000, we also vary the
checkpoint frequency and assess the latency of a join request.
As shown in Fig. 9c, checkpoint frequencies do not have
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:34:42 UTC from IEEE Xplore.  Restrictions apply. 
121328
a direct impact on the latency. This is expected since each
replica synchronizes all the historic transactions so the network
bandwidth consumption is dominated by the state transfer.
Throughput and scalability. We evaluate the throughput of
Dyno for f = 1, f = 2, and f = 5. As shown in Fig. 9e,
when f = 1, the peak throughput of Dyno is 76 ktx/s, which is
among the highest for partially synchronous BFT known so far.
When f increases, the performance downgrades, as observed
for almost all BFT protocols. We report the peak throughput
of Dyno and BFT-SMaRt in Fig. 9f. Dyno achieves higher
peak throughput than BFT-SMaRt, partly due to the efficiency
of the underlying implementation.
Performance under membership requests. We assess the
performance for membership requests for Dyno. We let f = 1
and let 400 clients submit regular requests concurrently to the
system. We evaluate three different scenarios: 1) performance
under join requests; 2) performance under leave requests; 3)
performance under both join and leave requests.
Performance under join requests. We let one replica submit
a join request and assess the throughput of both a g-correct
replica (i.e., a replica that is correct since time 0) and the new
replica. As shown in Fig. 9g, the system does not suffer from
any performance degradation upon a join request.
For performance under multiple join requests, we begin with
4 replicas (f = 1). We then issue membership requests and
add replicas one after another on a regular basis, until the
system has 8 replicas. We show the throughput for a g-correct
replica and every new replica that joins the system. As shown
in Fig. 9h, the system suffers from performance degradation
upon receiving every join request and resumes back to normal
after a period of time. For instance, when the first replica
joins, the throughput downgrades from 50 ktx/s on average
to around 30 ktx/s, a 40% degradation. This is expected
since the new replica performs state transfer while processing
regular requests. After the state transfer completes, however,
the throughput goes back to 50-60 ktx/s. As every replica joins,
the overall system throughput degrades gradually. This is also
expected, since the system has more replicas. When the system
has 8 replicas, the overall throughput decreases from 50-60
ktx/s to 40-50 ktx/s.
We match most of the system configuration parameters
and evaluate the performance of BFT-SMaRt in the same
setting. The results are shown in Fig. 9i. Unlike Dyno, in
BFT-SMaRt, the throughput of replicas that join the system
is consistently lower than that of existing ones. Furthermore,
the overall throughput degradation as replicas join for Dyno
is consistently lower than that of BFT-SMaRt.
Performance under leave requests. We begin with 10 replicas
(f = 3) and then let 6 replicas leave one after one another. The
throughput of the system is shown in Fig. 9j. In contrast to
the prior case, the throughput is more stable, mostly because a
replica can directly leave the system upon the delivery of the
remove request. The performance for BFT-SMaRt is similar,
as shown in Fig. 9k, besides that the overall throughput of
BFT-SMaRt is about 30%-40% lower than that of Dyno.
Performance under multiple join and leave requests. We also
assess the performance under both join and leave requests. In
particular, we trigger 3-5 random join or leave requests under
30-sec intervals. We first begin with 4 replicas and evaluate the
performance for both Dyno and BFT-SMaRt. We present the
performance of a g-correct replica in Fig. 9l. The performance
is similar to that under only join or leave requests. BFT-SMaRt
may hit low (close to 0) throughput during some requests. In
contrast, the performance of Dyno is in general more stable.
We let the system have f = 3 (10 replicas) in the beginning
and conduct the same experiment again. We present the results
for Dyno in Fig. 9m and BFT-SMaRt in Fig. 9n. We present
the performance of a g-correct replica, a replica that joins the
system, and a replica that leaves the system. The results for
both protocols, despite the fact that the throughput is in general
lower, are similar to that when f = 1.
We also evaluate Dyno under frequent membership requests,
where we begin with 10 replicas and let replicas frequently
join and leave under random intervals. As shown in Fig. 9o,
with frequent membership requests,
the throughput of the
system tends to be more turbulent, as replicas have to perform
frequent state transfer for newly joined replicas.
IX. CONCLUSION
We study dynamic BFT protocols, where replicas may join
and leave the system. We formally define the security defini-
tions for dynamic BFT and present different but meaningful
variants. We present Dyno, a highly efficient dynamic BFT
protocol. We show, with a up to 30-server deployment, that
Dyno is efficient, handling membership requests with low cost.
ACKNOWLEDGMENT
We thank Xiaoyun Wang, Xiao Sui, Baohan Huang, and the
anonymous reviewers for the help and comments for the paper.
Sisi is also with Shandong Institute of Blockchain. Sisi was
supported in part by Tsinghua Independent Research Program,
Shandong Key Research and Development Program under
grant No. 2020ZLYS09 and National Key Research and Devel-
opment Program of China under grant No. 2018YFA0704701.
Haibin was supported in part by Teli Young Scholar program.
REFERENCES
[1] I. Abraham, D. Malkhi, K. Nayak, L. Ren, and M. Yin. Sync hotstuff:
In S&P,
Simple and practical synchronous state machine replication.
2020.
[2] M. K. Aguilera, I. Keidar, D. Malkhi, and A. Shraer. Dynamic atomic
storage without consensus. Journal of the Acm, 58(2):96–99, 2009.
[3] E. Alchieri, A. Bessani, F. Greve, and J. Fraga. Efficient and modular
In OPODIS,
consensus-free reconfiguration for fault-tolerant storage.
2017.
[4] Y. Amir, C. Nita-Rotaru, S. Stanton, and G. Tsudik. Secure spread:
An integrated architecture for secure group communication. TDSC,
2(3):248–261, 2005.
[5] Y. Amir and J. Stanton. The spread wide area group communication
system. Technical report, Citeseer, 1998.
[6] A. Bessani, M. Santos, J. Felix, N. Neves, and M. Correia. On the
efficiency of durable state machine replication. In ATC, 2013.
[7] K. P. Birman and T. A. Joseph. Reliable communication in the presence
of failures. TOCS, 1987.
[8] G. Bracha. An asynchronous [(n-1)/3]-resilient consensus protocol. In
PODC, pages 154–162. ACM, 1984.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:34:42 UTC from IEEE Xplore.  Restrictions apply. 
131329
[9] M. Burrows. The chubby lock service for loosely-coupled distributed
[42] R. V. Renesse, Y. Minsky, and M. Hayden. A gossip-style failure
systems. In OSDI, pages 335–350, 2006.
detection service. In Middleware, 2009.
[10] M. Castro and B. Liskov.
Practical Byzantine fault
tolerance and
[43] R. V. Renesse and F. B. Schneider. Chain replication for supporting
high throughput and availability. OSDI, 2004.
[44] R. Rodrigues and B. Liskov. Rosebud: A scalable Byzantine-fault-
tolerant storage architecture. 2003.
[45] R. Rodrigues, B. Liskov, K. Chen, M. Liskov, and D. Schultz. Automatic
reconfiguration for large-scale reliable storage systems. TDSC, 2012.
[46] A. Schiper. Dynamic group communication. Distributed Comput.,
18(5):359–374, 2006.
[47] A. Schiper and A. Sandoz. Uniform reliable multicast in a virtually
synchronous environment. 1993.
[48] F. B. Schneider.
Implementing fault-tolerant services using the state
machine approach: A tutorial. CSUR, 22(4):299–319, 1990.
[49] A. Shraer, B. Reed, D. Malkhi, and F. Junqueira. Dynamic reconfigu-
ration of primary/backup clusters. In ATC, 2011.
[50] J. Sousa, E. Alchieri, and A. Bessani. State machine replication for the
masses with bft-smart. In DSN, pages 355–362, 2014.
[51] P. Sousa, A. N. Bessani, M. Correia, N. F. Neves, and P. Ver´ıssimo.
Highly available intrusion-tolerant services with proactive-reactive re-
covery. IEEE Trans. Parallel Distributed Syst., 21(4):452–465, 2010.
[52] L. Suresh, D. Malkhi, P. Gopalan, I. P. Carreiro, and Z. Lokhandwala.
Stable and consistent membership at scale with rapid. In ATC, 2018.
[53] G. S. Veronese, M. Correia, A. N. Bessani, and L. C. Lung. Spin one’s
In SRDS,
wheels? byzantine fault tolerance with a spinning primary.
2009.
[54] X. Wang, S. Duan, J. Clavin, and H. Zhang. BFT in blockchains: From
protocols to use cases. ACM Computing Surveys (CSUR), 2021.
[55] M. Yin, D. Malkhi, M. Reiterand, G. G. Gueta, and I. Abraham. Hotstuff:
BFT consensus with linearity and responsiveness. In PODC, 2019.
[56] M. Zamani, M. Movahedi, and M. Raykova. Rapidchain: A fast
blockchain protocol via full sharding. In CCS, pages 931–948, 2018.
NORMAL-CASE OPERATION ORACLE OF DYNO
APPENDIX A
We describe the normal-case operation of Dyno and how
the init() and deliver() events are triggered. In particular, we
use Bracha’s broadcast paradigm and use PBFT notations to
present the protocol. The pseudocode is illustrated in Fig. 10.
Initialization
v, c, M, T M {view, configuration, membership, temporary membership}
 events
upon receiving a valid m = ⟨PRE-PREPARE, v′, c′, s, batch⟩
if ⟨ADD, j, m⟩ ∈ batch
T M ← T M ∪ {pj}
upon receiving 2fc + 1 matching ⟨PREPARE, v, c, s, δ⟩
if non-primary, init(batch)
broadcast ⟨PREPARE, v, c, s, h(batch)⟩ to T M
prepared(δ, v, c, s) ← true
broadcast ⟨COMMIT, v, c, s, δ⟩ to T M
broadcast ⟨COMMIT, v, c, s, δ⟩ to T M
upon receiving fc + 1 matching ⟨COMMIT, v, c, s, δ⟩
upon receiving 2fc + 1 matching ⟨COMMIT, v, c, s, δ⟩
 oracle functions
func init(batch)
deliver(batch) where h(batch) = δ
if leader, broadcast ⟨PRE-PREPARE, v, c, s, batch⟩ to T M
{switch to procedures shown in Fig. 4}
{switch to procedures shown in Fig. 4}
func deliver(batch)
{init() event}
{deliver() event}
proactive recovery. TOCS, 20(4):398–461, 2002.
[11] G. V. Chockler, I. Keidar, and R. Vitenberg. Group communication
specifications: A comprehensive study. CSUR, 33(4):427–469, 2001.
[12] J. Cowling, D. R. K. Ports, B. Liskov, R. A. Popa, and A. Gaikwad. Cen-
sus: location-aware membership management for large-scale distributed
systems. In ATC, 2009.
[13] A. Das, I. Gupta, and A. Motivala. Swim: Scalable weakly-consistent
infection-style process group membership protocol. In DSN, 2002.
[14] S. Duan, K. Levitt, H. Meling, S. Peisert, and H. Zhang. ByzID:
Byzantine fault tolerance from intrusion detection. In SRDS, 2014.
[15] S. Duan, H. Meling, S. Peisert, and H. Zhang. BChain: Byzantine
In
replication with high throughput and embedded reconfiguration.
OPODIS, pages 91–106, 2014.
[16] S. Duan, M. K. Reiter, and H. Zhang. Beat: Asynchronous bft made
practical. In CCS, pages 2028–2041. ACM, 2018.
[17] S. Duan and H. Zhang. Pace: Fully parallelizable bft from reproposable
byzantine agreement. IACR Cryptol. ePrint Arch., 2022.
[18] E. Gafni and D. Malkhi. Elastic configuration maintenance via a
parsimonious speculating snapshot solution. In DISC, 2015.
[19] S. Gilbert, N. A. Lynch, and A. A. Shvartsman. Rambo: a robust, re-
configurable atomic memory service for dynamic networks. Distributed
Computing, 23(4):225–272, 2010.
[20] R. Guerraoui, J. Komatovic, P. Kuznetsov, Y. A. Pignolet, D. Seredin-
In
schi, and A. Tonkikh. Dynamic Byzantine reliable broadcast.
Q. Bramas, R. Oshman, and P. Romano, editors, OPODIS, 2020.
[21] P. Hunt, M. Konar, F. P. Junqueira, and B. Reed. Zookeeper: Wait-free
coordination for internet-scale systems. In ATC, 2010.
[22] L. Jehl and H. Meling. Asynchronous reconfiguration for paxos state
machines. ICDCN, 2014.
[23] L. Jehl, R. Vitenberg, and H. Meling. Smartmerge: A new approach to
reconfiguration for atomic storage. In DISC, 2015.
[24] H. D. Johansen, R. Van Renesse, Y. Vigfusson, and D. Johansen.
Fireflies: A secure and scalable membership and gossip service. TOCS,
33(2):1–32, 2015.
[25] K. P. Kihlstrom, L. E. Moser, and P. M. Melliar-smith. The SecureRing
protocols for securing group communication. In HlCSS, 1998.
[26] E. K. Kogias, P. Jovanovic, N. Gailly, I. Khoffi, L. Gasser, and B. Ford.
Enhancing bitcoin security and performance with strong consistency via
collective signing. In USENIX Security, pages 279–296, 2016.
[27] E. Kokoris-Kogias, P. Jovanovic, L. Gasser, N. Gailly, and B. Ford.
Omniledger: A secure, scale-out, decentralized ledger. S&P, 2018.
[28] P. Kuznetsov and A. Tonkikh. Asynchronous reconfiguration with
byzantine failures. DISC, 2020.
[29] L. Lamport. Using time instead of timeout for fault-tolerant distributed
systems. TOPLAS, 6(2):254–280, 1984.
[30] L. Lamport. The part-time parliament. TOCS, 16(2):133–169, 1998.
[31] L. Lamport, D. Malkhi, and L. Zhou. Vertical paxos and primary-backup
replication. In PODC, 2009.
[32] J. Leitao, J. Pereira, and L. Rodrigues. Hyparview: A membership
protocol for reliable gossip-based broadcast. In DSN, 2007.
[33] C. Liu, S. Duan, and H. Zhang. Epic: Efficient asynchronous bft with
adaptive security. In DSN, 2020.
[34] J. R. Lorch, A. Adya, W. J. Bolosky, R. Chaiken, J. R. Douceur, and
J. Howell. The SMART way to migrate replicated stateful services. In
Y. Berbers and W. Zwaenepoel, editors, EuroSys, pages 103–115, 2006.
[35] J. MacCormick, N. Murphy, M. Najork, C. Thekkath, and L. Zhou.
Boxbood: Abstractions as the foundation for storage infrastructure. In
OSDI, 2004.
[36] J. P. Martin and L. Alvisi. A framework for dynamic byzantine storage.
In DSN, 2004.
[37] L. E. Moser, Y. Amir, P. M. Melliar-Smith, and D. A. Agarwal. Extended
virtual synchrony. In ICDCS, pages 56–65. IEEE, 1994.
[38] D. Ongaro and J. Ousterhout. In search of an understandable consensus
algorithm. In ATC, pages 305–319, 2014.
[39] M. K. Reiter. Secure agreement protocols: reliable and atomic group
multicast in rampart. In CCS, pages 68–80, 1994.
[40] M. K. Reiter. A secure group membership protocol. IEEE Transactions
on Software Engineering, 22(1):31–42, 1996.
[41] R. V. Renesse, K. P. Birman, and W. Vogels. Astrolabe: A robust and
scalable technology for distributed system monitoring, management, and
data mining. ACM Transactions on Computer Systems, 2003.
Fig. 10: Normal-case operation and how the init() and deliver()
events are triggered.
The pseudocode specifies the workflow for replicas in the
current configuration c and pending replicas, i.e., replicas that
request to join the system. Replicas run a Bracha’s broadcast
protocol and replicas that request to join the system act as
learners before the corresponding join requests are delivered.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:34:42 UTC from IEEE Xplore.  Restrictions apply. 
141330
In particular, for replicas in Mc, the leader first triggers the
init() event by broadcasting a ⟨PRE-PREPARE, v, c, s, batch⟩
message to T M, where T M is the temporary members.
(T M is updated according to Fig. 5). Upon receiving a
⟨PRE-PREPARE, v′, c′, s, batch⟩ message, a replica pi verifies
the following: 1) v′ = v; 2) the signatures of client re-
quests in batch are valid; 3) pi has not accepted another
⟨PRE-PREPARE⟩ message with sequence number s; 4) s is
within a valid range. A non-primary replica also triggers
the init(batch) event accordingly. After that, pi broadcasts
a ⟨PREPARE, v, c, s, h(batch)⟩ to all replicas in T M where
h(batch) is the hash of the batch. Replica pi waits until
it receives 2fc + 1 valid ⟨PREPARE⟩ messages, i.e., pi has
previously accepted a ⟨PRE-PREPARE, v, c, s, batch⟩ message.
We say that batch is prepared by the replica. After that,
pi broadcasts a ⟨COMMIT, v, c, s, δ⟩ to all replicas in T M.
If a replica receives fc + 1 ⟨COMMIT⟩ messages but has
not previously broadcast any ⟨COMMIT⟩ message,
it also
broadcasts a ⟨COMMIT, v, c, s, δ⟩ messages. A set of either
2fc + 1 ⟨PREPARE⟩ messages or fc + 1 ⟨COMMIT⟩ messages
serve as a prepare certificate. Note that this is different from
protocols such as PBFT but crucial
in our protocol. The
certificate also serves as a proof of delivery. Finally, if a replica
receives 2fc + 1 ⟨COMMIT, v, c, s, δ⟩ messages, it triggers the
deliver(batch) event.
No that during the view changes, a message in the form of
⟨VIEW-CHANGE, v, c,C,P,PP, i⟩ carries P and PP which are
related to the normal-case operation oracle. The P is a set of
prepare certificates for requests with sequence number greater
than C, the PP is a set of ⟨PRE-PREPARE⟩ messages where
each message includes at least one membership request.
APPENDIX B
CONFIGURATION DISCOVERY OPTIONS
We present two alternatives options for configuration dis-
covery: lazy discovery and configuration master.
Lazy discovery. Lazy discovery delays the discovery of the
configuration after the delivery of the request. In particular,
to obtain the latest configuration, a new replica or a client pi
directly obtains the universe, i.e., Π. In this case, every time
a client or a replica receives a messages, it must verify the
configuration history accordingly.
Configuration master. Both self-discovery and lazy discovery
require all clients and new replicas to broadcast to all replicas
in the universe. Alternatively, we could build a standalone
configuration service all replicas/clients can query to obtain
the latest configuration of the system, as shown in Fig. 11. We
assume that all replicas and clients know the identities of all
replicas in the configuration master. The configuration master
can be built as one or a subset of all replicas in the system.
We let the master passively learn the latest configuration. In
particular, if the configuration changes, replicas send a set of
2fc + 1 ⟨COMMIT⟩ messages together with the correspond-
ing ⟨PRE-PREPARE⟩ message to the configuration master. The
2fc +1 ⟨COMMIT⟩ messages serve a a proof of delivery and the
⟨PRE-PREPARE⟩ messages can be used to verify the membership
request(s). In this way, the configuration master also obtains
the entire configuration history.
Note that the configuration master does not have to be
replicated using SMR. This is mainly because the entire
configuration history is totally ordered. Therefore, any con-
figuration history can be self validated.
Initialization
c, Mc, chist
as a client/new replica
{configuration, membership, and configuration history}
func ObtainConf ig()
send ⟨QUERY, i⟩ to CMaster
upon ⟨CONF, c′, M′
c, chist′⟩
if chist′ is valid and c′ > c
chist ← chist′, c ← c′, Mc ← M′
return c, Mc
c
as a configuration master
upon ⟨QUERY, j⟩
upon M = 2fc + 1 ⟨COMMIT, v, c, s, h(batch)⟩ messages
send ⟨CONF, c, Mc, chist⟩ to pj
chist ← chist ∪ M, update c and Mc
Fig. 11: Configuration discovery: configuration master.