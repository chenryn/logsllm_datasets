VFMADD132PD YMM1, YMM2, [R8]
Example Seqreset
Sleep Pseudo-Inst.
LAR ECX, EDX
FLDLN2
FISTP [R8]
Timing Diff.
228 cycles
158 cycles
90 cycles
166 cycles
kernel-level ASLR (KASLR) based on the results discovered
by Osiris. This novel KASLR break even works on the newest
Intel Ice Lake and Comet Lake microarchitectures, even if all
known mitigations are in place. Section 6.3 shows that the
RDRAND-based side channel can be used as a cross-core covert
channel in the cloud without relying on the cache.
6.1 Transient-Execution Covert Channels
Transient-execution attacks [17], i.e., Spectre- and Meltdown-
type attacks, always require a microarchitectural covert chan-
nel to transfer the microarchitecturally-encoded data into the
architectural state. Typically, these attacks rely on a cache
covert channel [17], as also shown in the original Spec-
tre [50] and Meltdown [57] paper. Cache-based covert chan-
nels have the advantage that they are ubiquitous, fast, and
reliable [17, 50, 57]. In this case study, we show that our new
side channels can potentially increase the number of Spectre
gadgets, and optimize the leakage for Meltdown-type attacks.
Spectre Attacks. Bhattacharyya et al. [12] and Schwarz et al.
[80, 84] already showed different covert channels for Spectre.
Their covert channels are based on port contention, vector
instructions, and the TLB, respectively. In this case study, we
show that our newly discovered side channel based on AVX2
and x87-FPU can also be used for Spectre attacks.
We implement a proof-of-concept Spectre attack that uses
this side channel as the covert channel. Our proof of con-
cept exploits Spectre-PHT [50] to leak a string outside of
the bounds of an array. We can use the same gadgets as in a
NetSpectre attack [84] and similar gadgets as used in SMoTh-
erSpectre [12]. More speciﬁcally, exploiting the discovered
side channels would require ﬁnding speciﬁc gadgets (con-
ditional trigger sequence) in the victim code. Such gadgets
could also be constructed in combination with other Spectre
vulnerabilities using speculative ROP [11, 12]. Depending on
the value of a transiently accessed bit, an AVX2 instruction
is executed or not executed. While NetSpectre simply waits
for the state to be reset, we rely on the ﬁndings of Osiris that
executing an x87-FPU instruction resets the state faster. The
receiving end of the covert channel is again an AVX2 instruc-
tion. We tested our code on an Intel Core i7-9700K, where
we achieved a leakage rate of 2407 bit/s with an error rate of
0.43 %. This is 2.4 times as fast as the transmission rate of
the AVX-based covert channel used in NetSpectre [84].
Meltdown Attacks.
In Meltdown-type attacks, both the
sending and the receiving end of the covert channel are
entirely attacker-controlled. So far, all Meltdown-type at-
tacks [15, 17, 57, 77, 82, 87, 91, 93] relied on the cache
and typically on Flush+Reload to recover the information
from the cache. Even though Flush+Reload is extremely fast
and reliable, it is still the bottleneck for leaking data [57].
With Stream+Reload, we introduce a new cache attack
for improving the leakage rate of Meltdown-type attacks.
Stream+Reload is based on the discovery of Osiris that non-
temporal memory stores ﬂush the target from the cache. While
a cache attack that requires shared writable memory is not use-
ful in a typical side-channel scenario, it is ideal as a fast covert
channel for transient-execution attacks. Stream+Reload re-
places the CLFLUSH instruction with a MOVNTDQ instruction.
The MOVNTDQ instruction has a similar effect as the CLFLUSH
instruction. It evicts the target cache line from the cache [43].
Reliability of Eviction. Using L3 performance coun-
ters, we conﬁrmed that the MOVNTDQ instruction indeed re-
liably evicts the cache line from all cache levels. With
respect to the eviction reliability, there is no difference
between MOVNTDQ and CLFLUSH or CLFLUSHOPT. Both for
Stream+Reload and Flush+Reload, we measured an F-score
of 1.0 (n = 10 000 000). Furthermore, even novel cache de-
signs [59, 76, 97] likely do not prevent this type of eviction, as
they only block the ﬂush instruction and prevent the efﬁcient
creation of eviction sets.
Performance. We observe one signiﬁcant difference
between Flush+Reload and Stream+Reload. Although in both
attacks, the value is evicted from all cache levels, the reload
of a value ﬂushed using MOVNTDQ is signiﬁcantly faster on all
our tested CPUs. On the i7-8565U, for example, reloading a
value when it was ﬂushed takes on average 253 cycles (n =
20 000 000) (including an MFENCE each before and after the
memory load). In contrast, when the value was evicted using
MOVNTDQ, reloading only takes 172 cycles (n = 20 000 000).
Analyzing the uncore performance counters shows that this
time difference for loading the data originates from the uncore
(offcore_requests_outstanding.cycles_with_data_rd).
We attribute the time difference to the cache-coherency
protocol. Flushing the cache line puts the cache line into
the invalid state, while writing to the cache line puts it into
the modiﬁed state [66, 71]. When loading the ﬂushed cache
line, it switches to the exclusive state, while the modiﬁed
state stays the same. Due to the different behaviors of cache
snooping, loading from different cache coherence states also
results in different latencies [66].
1424    30th USENIX Security Symposium
USENIX Association
Results. The faster reload time allows encoding 2.5x more
values during the transient window. In a Meltdown proof of
concept relying on Stream+Reload, we can, on average, leak
7.83 bytes at once (n = 100 000) (Intel i3-5010U).4 Previous
work was only able to leak up to 3 bytes [57, 65, 77, 82].
6.2 MOVNT-based KASLR Break
KASLR has been subject to almost countless microarchitec-
tural attacks in the past [15, 16, 24, 33, 42, 49, 62, 80]. As
a response, researchers, CPU vendors, and OS maintainers
have developed several countermeasures [2, 16, 29, 32]. In
particular, the newest 10th-generation Intel CPUs (Ice Lake
and Comet Lake) are immune to many microarchitectural
KASLR breaks, including the recently discovered EchoLoad
attack [16]. However, our newly-discovered side channel can
be used to break KASLR even on those architectures.
Based on the discovery of Osiris that the MOVNT instruc-
tion evicts a cache line, we manually evaluated whether this
eviction also works for inaccessible addresses such as kernel
addresses. Previous work showed that even for Meltdown-
resistant CPUs, memory loads [16, 92] and stores [80] can in-
fer side-channel information from the kernel. Although MOVNT
could not directly evict kernel memory, we observed changes
in the cache state on seemingly unrelated memory. If the tar-
geted kernel address is invalid, i.e., not physically backed, we
observe that an unrelated MOV on user memory issued after
the MOVNT fails. If the kernel address is physically backed,
the MOV is successful. Hence, this allows de-randomizing the
location of the kernel, effectively breaking KASLR.
1 try {
2
asm volatile(
3
4
5
6
7
8
"clflush 0(%[probe])\n"
"movq %%rsi, (%[dummy])\n"
"movntdqa (%[kernel]), %%xmm1\n"
"movq (%[probe]), %%rax\n"
) : : [probe]"r"(probe), [dummy]"r"(dummy),
[kernel]"r"(kernel)
: "rax", "xmm1", "rsi", "memory");
9
10 } catch {
11
12
13 }
if(uncached(probe)) return MAPPED;
else return UNMAPPED;
Listing 1: The main part of FlushConﬂict. The probe memory
is uncached if the kernel address is physically backed.
Listing 1 shows the minimal working example of our
KASLR break, FlushConﬂict, that we created from our ﬁnd-
ings on MOVNT. A user-accessible memory address (probe) is
ﬂushed, followed by a write to an unrelated address, acting as
a reordering barrier. Afterward, the kernel address (kernel)
is read using MOVNT. Finally, probe is accessed. As the load
4We used this older CPU as the new CPUs are not affected by Meltdown.
Table 4: The evaluated CPUs for the KASLR break.
CPU (Microarchitecture)
Intel Core i5-3230M (Ivy Bridge)
Intel Core i5-4690 (Haswell)
Intel Core i3-5010U (Broadwell)
Intel Core i7-6700K (Skylake)
Intel Core i7-8565U (Whiskey Lake)
Intel Core i7-9700K (Coffee Lake)
Intel Core i9-9980HK (Coffee Lake)
Intel Core i3-1005G1 (Ice Lake)
Intel Core i7-10510U (Comet Lake)
Intel Celeron J4005 (Gemini Lake)
Intel Xeon Platinum 8124M (Skylake-SP)
Accuracy (idle) Accuracy (stress) Runtime
34 ms
221 ms
5 ms
9 ms
6 ms
102 ms
65 ms
300 ms
84 ms
349 ms
318 ms
99 %
100 %
99 %
99 %
100 %
100 %
99 %
96 %
99 %
99 %
99 %
97 %
99 %
97 %
98 %
92 %
98 %
99 %
96 %
97 %
99 %
99 %
from the kernel address leads to a fault, exceptions are han-
dled using a signal handler for this code. After resolving the
fault, the cache state of probe is observed, e.g., using Flush+
Reload. If probe is cached, the kernel address is invalid, if
probe is not cached, the kernel address is valid.
Root-Cause Hypothesis. Using performance counters, we
analyzed the behavior of FlushConﬂict. The CLFLUSH and
load access to the same address trigger a cache-line conﬂict
as also exploited in ZombieLoad [82]. Even though, at ﬁrst,
the write to dummy seems unrelated, it is guaranteed to be
ordered with CLFLUSH [45] and hence inﬂuences the overall
timing of the executed code in the processor pipeline. Alterna-
tively, this line can also be removed entirely (depending on the
CPU) or replaced by a different method to add a delay, e.g., us-
ing a dummy loop. However, adding a serializing instruction,
such as a fence, breaks the attack, as it forces the CLFLUSH
to retire, preventing the cache-line conﬂict with the load. If
kernel is physically backed, we observe a page-table walk
(dtlb_load_misses.miss_causes_a_walk). If kernel is
not physically backed, we observe 2 page-table walks, i.e.,
the page-table walk is repeated. That is in agreement with
Canella et al. [16], showing that loads from non-present kernel
pages are re-issued. As this case takes longer [49] and faults
are only detected at the retirement of instructions, it gives
other out-of-order executed instructions more time to execute.
We hypothesize that if the kernel address is unmapped, the pro-
cessor has a long-enough speculation window to execute the
ﬂush, write, and the last load. As a result of this, the last load
brings probe back to the cache. In the case of a mapped ker-
nel address, the processor detects the fault earlier and hence
stops the execution before the last load was issued. As a
result, probe is cached if kernel is not physically backed,
and not cached if kernel is physically backed. The ob-
served performance counters back this hypothesis. For an un-
mapped address, mem_load_retired_l3_miss shows fewer
events. However, the number of cycles spent waiting for mem-
ory (cycle_activity.cycles_l3_miss) is slightly higher.
This indicates that there are ongoing load instructions that
never retire, backing the hypothesis that the last load is only
executed transiently when the address is unmapped.
Applicability. We tested our microarchitectural KASLR
break on Intel CPUs from the 3rd to the 10th generation, i.e.,
USENIX Association
30th USENIX Security Symposium    1425
Table 5: The evaluated CPUs for the RDRAND covert channel.
CPU
Setup
Lab
Lab
Lab
Lab
Lab
Intel Core i5-3230M
Intel Core i3-5010U
Intel Core i7-8565U
Intel Core i9-9980HK
Intel Core i3-1005G1
Intel Xeon E5-2686 v4 Cloud
Intel Xeon E5-2666 v3 Cloud
AMD Ryzen 5 2500U
AMD Ryzen 5 3550H
Lab
Lab
Cross-HT
Speed
133.3 bit/s
666.7 bit/s
400.0 bit/s
500.0 bit/s
1000.0 bit/s
500.0 bit/s
666.7 bit/s
48.8 bit/s
666.7 bit/s
Cross-Core
Speed
Error
133.3 bit/s
8.87 %
333.3 bit/s
0.30 %
166.7 bit/s
0.65 %
0.76 %
117.6 bit/s
0.37 % 1000.0 bit/s
333.3 bit/s
0.21 %
95.2 bit/s
2.64 %
48.8 bit/s
2.80 %
2.10 %
500.0 bit/s
Error
0.05 %
1.82 %
0.63 %
9.25 %
0.00 %
2.48 %
0.88 %
2.00 %
2.50 %
from Ivy Bridge to Comet Lake. As shown in Table 4, we used
desktop (Core), server (Xeon), and mobile (Celeron) CPUs.
In contrast, we experimentally veriﬁed that EchoLoad [16],
which works on a large range of Intel CPUs from 2010 to
2019, does not work on Ice Lake or Comet Lake. We con-
ﬁrm that the KASLR break is operating-system agnostic by
successfully mounting it on Linux and Windows 10.
In the case of KPTI, i.e., on CPUs that are not Meltdown-
resistant, the KASLR break detects the trampoline used to
switch to the kernel. Otherwise, if the CPU is Meltdown-
resistant or KPTI is disabled, the KASLR break detects the
start of the kernel image. As an unprivileged attacker can read
out the state of KPTI and whether the CPU is vulnerable to
Meltdown, the attacker always knows the start of the kernel
image. Moreover, as the kernel image itself is not randomized,
knowing the kernel version and the start of the kernel image
is sufﬁcient to calculate the location of any kernel part.
Additionally, we tested the KASLR break by simulating
a realistic environment by artiﬁcially raising the pressure
on the CPU and memory subsystem using the stress utility.
We still observe success rates ranging from 92% to 99% for
the different microarchitectures (n = 100). Furthermore, we
veriﬁed the KASLR break in a cloud scenario by testing it on
an Intel Xeon Platinum 8124M in the AWS cloud.
Performance. On average, our KASLR break detects the
start of the kernel image within 136 ms (n = 1100) While not
the fastest microarchitectural KASLR break, it is on par with
other microarchitectural KASLR breaks [16].
6.3 RDRAND Covert Channel in the Cloud
Osiris discovered a timing leakage in the RDRAND instruction
on both Intel and AMD CPUs. In this section, we present
a cross-core covert channel based on these timing differ-