Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3165To determine an appropriate generation layer, we generate a
small number of adversarial examples with the basic scheme for
each of potential generation layers, starting with the penultimate
layer and moving backward, to query the trapdoored defense. If
the detection rate is below a threshold (i.e., when we consider an
appropriate generation layer is found), we stop and choose the
layer with the smallest detection rate as the generation layer. Since
the penultimate layer is generally used as the detection layer in
the trapdoored defense and the basic scheme works reasonably
well (see Section 6.7), an appropriate generation layer can be found
quickly without searching many layers.
Once the generation layer is determined, we use the query result
of the layer to adjust target F 𝐶𝑡
and constraint threshold 𝑐𝑝 in
Eq. 7 to be used in the generation phase. If the generated adversar-
ial examples are all negative, the basic scheme with the original
F 𝐶𝑡
and 𝑐𝑝 will be used to generate adversarial examples. If the
𝐿
detection rate is high enough (above a preset threshold), which
occurs typically when TeD has a high false positive rate for benign
target samples (see Section A.3), we query the TeD detector with
benign target samples until we have collected enough negative sam-
ples (10 in our experimental evaluation). In this case, the benign
target samples that are negative and untested are used to calculate a
weighted average, with more weight (double in our evaluation) for
the negative benign examples. The detected benign target examples
(i.e. false positive samples) are excluded from the calculation. This
weighted average replaces F 𝐶𝑡
F 𝐶𝑡
When some generated adversarial examples are detected, target
is adjusted to move away from them as follows:
𝐿 used in the basic scheme.
𝐿
𝐿
Ξ ≡ F 𝐶𝑡
𝐿 ← F 𝐶𝑡
F 𝐶𝑡
𝐿 − E𝑥∈𝑆𝑝𝑎𝑒 F𝐿(𝑥),
Ξ
,
(cid:107)Ξ(cid:107)2
𝐿 + 𝛾𝑑𝑟
(10)
𝐿
where 𝑆𝑝𝑎𝑒 is the set of positive adversarial examples, 𝑑𝑟 is the
detection rate of the generated adversarial examples, and 𝛾 is a
positive weighting parameter (0.1 in our evaluation). F 𝐶𝑡
𝐿 on the
right side of Eq. 10 is the weighted average calculated above or the
original target used in the basic scheme if the weighted average is
not calculated. E𝑥∈𝑆𝑝𝑎𝑒 F𝐿(𝑥) in Eq. 10 is the average of detected
adversarial examples. Negative adversarial examples are not used
in Eq. 10 since their average may be on the same side as the average
of positive adversarial examples. This modied target will be used
in the generation phase. We can see from Eq. 10 that there is no
change to F 𝐶𝑡
if the detection rate is 0 (𝑑𝑟 = 0).
Once the new target F 𝐶𝑡
is determined with Eq. 10, we can
determine a new constraint boundary 𝑐𝑝 on the cosine similarity to
include negative examples and exclude positive examples as many
as possible. More specically, we calculate the cosine similarity
distributions of both positive examples and negative examples (in-
cluding benign target samples) queried in this phase with new target
F 𝐶𝑡
𝐿 . We preset a range of percentiles, [𝑘𝑛𝑙 , 𝑘𝑛ℎ]-th ([10, 50]-th in
our evaluation) percentiles to specify a permissible range of per-
centages of negative samples outside the boundary, and a threshold
𝑘𝑝-th (90th in our evaluation) percentile to specify the minimum
percentage of positive samples outside the boundary. We nd the
corresponding values 𝑣𝑘𝑛𝑙 and 𝑣𝑘𝑛ℎ of 𝑘𝑛𝑙-th and 𝑘𝑛ℎ-th percentiles
𝐿
from the distribution of negative examples and 𝑣𝑘𝑝 corresponding
to 𝑘𝑝-th percentile from the distribution of the positive examples.
The new constraint boundary 𝑐𝑝 is determined as follows,
, 𝑣𝑘𝑝)).
𝑐𝑝 = min(𝑣𝑘𝑛ℎ
, max(𝑣𝑘𝑛𝑙
(11)
Eq. 11 determines a boundary 𝑐𝑝 inside the permissible range
[𝑣𝑘𝑛𝑙
, 𝑣𝑘𝑛ℎ], aiming to exclude at least 𝑘𝑝-th percentile of positive
examples. This new 𝑐𝑝 will be used to generate adversarial examples
in the generation phase.
5.2.2 Generation Phase. In generating adversarial examples, we
add the following drive-away loss, ℓ𝑎𝑤𝑎𝑦, to the loss function of
the basic scheme (Eq. 9) to minimize the cosine similarity with the
positive adversarial examples in the preparation phase:
cos(F𝐿(𝑥), F𝐿(𝑎))
ℓ𝑎𝑤𝑎𝑦(F𝐿(𝑥)) =

(12)
Adding this drive-away loss to Eq. 9, the loss function becomes:
𝐿 (cid:107)2
𝐿 ) + 𝜆1 · (cid:107)F𝐿(𝑥) − F 𝐶𝑡
ℓ(F𝐿(𝑥), F 𝐶𝑡
𝐿 )) = − cos(F𝐿(𝑥 + 𝜖), F 𝐶𝑡
𝑎∈𝑆𝑝𝑎𝑒
+ 𝜆2 · 
𝑎∈𝑆𝑝𝑎𝑒
cos(F𝐿(𝑥), F𝐿(𝑎)),
(13)
where 𝜆1 and 𝜆2 are two non-negative weighting parameters. Target
F 𝐶𝑡
in Eq. 13 is the one calculated with Eq. 10. The same constraint
𝐿
as in Eq. 7, except that 𝑐𝑝 now is calculated with Eq. 11, is used
with Eq. 13 in generating adversarial examples.
The iteration to crafting an adversarial example is similar to that
in the basic scheme. At the beginning, we drive only the rst term
in Eq. 13 by setting 𝜆1 = 𝜆2 = 0 until the constraint is satised.
Then we activate 𝜆1 (set to 1 in our experimental evaluation) and 𝜆2
to drive 𝑥 into target category 𝐶𝑡 and away from positive examples
in the feature space. When 𝑥 falls into target category 𝐶𝑡, 𝜆1 is
deactivated (set to 0). When 𝜆2 is activated, its value is adapted
by multiplying a factor (1.2 in our evaluation) if the rst term in
Eq. 13 decreases when compared with the last iteration or dividing
by another factor (1.3 in our evaluation) if the rst increases.
5.3 FIA for Enhanced TeD
Both the basic and complete schemes can be modied slightly to
deal with the enhanced version of TeD with randomly sampled
neurons and multiple trapdoors. When multiple trapdoors are used,
we hope to activate all eective trapdoor signatures3 in querying the
trapdoored defense in the preparation phase. The query described
in Section 5.2.1 may not fulll this goal since feature vectors of
generated adversarial examples may be close to each other and thus
can activate only a portion of eective trapdoor signatures.
To address this problem, we aim to activate all eective trapdoor
signatures during the preparation phase. This can be achieved by
applying FIA to query the trapdoored defense multiple rounds until
the detection rate is below a threshold or does not decrease, each
round with a small number of adversarial examples. More speci-
cally, the rst round is executed as before: we use the basic scheme
to generate a small number of adversarial examples to query the
trapdoored defense. If the detection rate is above the threshold, we
3Some trapdoor signatures may be redundant due to being too close to other trapdoor
signatures when multiple trapdoors are used.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3166apply the complete scheme to generate another round of adver-
sarial examples to query the trapdoored defense. Since they are
generated by maximizing distances to positive adversarial examples
in previous queries (i.e., they try to stay away from trapdoor signa-
tures activated in previous queries), newly generated adversarial
examples should activate remaining inactivated trapdoor signa-
tures. This process is repeated until all eective trapdoor signatures
have been activated. In each round, the drive-away loss ℓ𝑎𝑤𝑎𝑦 in
Eq. 12 includes all positive adversarial examples found in previous
queries. When all trapdoor signatures have been activated, most
generated adversarial examples should be undetectable.
To deal with randomly sampled neurons, the indistinguishability
constraint also checks cosine similarities in subsets of randomly
sampled neurons to ensure indistinguishability in these subsets too.
Since we have no idea which subsets of neurons are used in the
trapdoored detection, we just randomly select a group of subsets
of neurons at the generation layer and ensure that our generated
adversarial examples are indistinguishable in these selected subsets.
They are likely indistinguishable in the subsets used in the detection
too. For each selected subset, its threshold 𝑐𝑝 is determined in the
same way as the case using a single trapdoor signature with all
neurons.
In addition to the above passive subset checking to ensure in-
distinguishability on each of randomly selected subsets of neurons
at the generation layer, we can also actively drive adversarial ex-
amples to reach indistinguishability on each of these subsets. This
can be achieved by adding two loss terms for selected subsets. One
loss term is the sum of cosine similarity of the adversarial example
with the expectation of benign examples in 𝐶𝑡 on each selected
subset, which we want to maximize. The other loss term is the sum
of cosine similarity of the adversarial example with those examples
in 𝑆𝑝𝑎𝑒 (i.e., detected adversarial examples during the preparation
phase) on each selected subset, which we want to minimize.
6 EXPERIMENTAL EVALUATION
We empirically evaluate the performance of our proposed Feature-
Indistinguishable Attack (FIA) against dierent congurations and
variants of the trapdoored defense, including defending single cate-
gory and all categories, multiple trapdoors and random sampling of
neurons, and Projection-based Trapdoor-enabled Detection (P-TeD)
described in Section 3.3, etc. Several popular datasets are used in
our empirical evaluation. We report the empirical study and its
results in this section.
6.1 Experimental Setup
6.1.1 Datasets and DNN models. The same datasets and deep neu-
ral networks used in [51] are used in our empirical evaluation. These
datasets and neural networks are described in Appendix A.1. We
perform handwritten digit recognition with MNIST [35] and image
classication with CIFAR10 [32] as they are the most popularly
adopted benchmarks. We carry out trac sign recognition with GT-
SRB [54] and face recognition with YouTube Face [60] as they stand
for two of the most security-critical scenarios (autonomous-driving
and biometrics identication) where deep learning applies broadly.
The YouTube Face dataset consists of 440K facial images of 224×224
pixels, belonging to 1,283 dierent people taken from YouTube
videos. We use it to evaluate our attack on large-scale datasets. Four
convolutional networks including ResNet20 and ResNet50 [25] are
used to evaluate our attack on both small and large scale networks.
In our empirical evaluation, training data is used to train a trap-
doored model and determine trapdoored defense parameters (e.g.,
trapdoor signatures), while test data is used in attacking the trap-
doored defense. This ensures that defenses and attacks use disjoint
data and adversaries have no access to the data used for training a
trapdoored model and determining the TeD characteristics.
6.1.2 Trapdoor Seings in Trapdoor-enabled Detection. We evalu-
ate our attack on TeD and P-TeD with dierent congurations of
trapdoors, including single-category (single-label in [51]), where a
single trapdoor is injected into a model to protect a specic category,
and all categories (all-label in [51]) with both single and multiple
trapdoors per category, where multiple trapdoors are injected into
a model to protect all categories, with one or multiple trapdoors
per category.
Trapdoors are injected with the same parameter settings as in
[51]. The detail is presented in Appendix A.2. Under these settings,
eective trapdoors can be inserted (each injected trapdoor has a
success rate > 97%) with similar accuracy performance as a clean
model (accuracy degradation < 2%).
6.1.3 Configurations of Adversarial Aacks. - We evaluate the per-
formance of our proposed attack against TeD and P-TeD with sim-
ilar experiments as the authors of TeD to evaluate the detection
performance of TeD in their paper [51], and compare our attack
with existing state-of-the-art adversarial attacks. More specically,
in evaluating our attack against TeD and P-TeD when a single or
all categories are protected, we choose two state-of-the-art white-
box adversarial attacks, PGD [33, 34] and C&W [10], as baseline
attacks. When evaluating our attack against Ted and P-TeD when
randomized neuron signatures and multiple trapdoors per category
are used, we choose Oracle Signature Attack (OSA) [6] as the base-
line attack. OSA is the white-box version of the two attacks [6]
specically designed to circumvent TeD. OSA has a better attacking
performance than its grey-box counterpart. The congurations of
our proposed attack and the baseline adversarial attacks are sum-
marized in Table 2. Unless stated otherwise, these congurations
were used in our experimental evaluation, and the experimental
results were obtained by setting the penultimate layer as the detec-
tion layer (and the generation layer was also determined to be the
penultimate layer) and by setting the false positive rate (FPR) to 5%
for benign samples in the target category.
As shown in Table 2, our proposed FIA generally requires a
higher bound than that of PGD on a trapdoored model. This is
because a conventional adversarial attack like PGD is likely trapped
into a shortcut created by the trapdoored defense, and thus requires
a bound much lower than the same adversarial attack on a clean
model without injecting any trapdoor. For example, it requires
𝛿 = 64 in general for PGD to achieve a good attack success rate on a
clean MNIST model, but the bound reduces to 8 for PGD to achieve
a similar attack success rate on a trapdoored MNIST model. FIA
avoids falling into traps in a trapdoored model and thus requires a
higher bound, such as 64 on a trapdoored MNIST model. We note
that FIA’s bound is similar to that of PGD on a clean version (i.e.,
without the trapdoored defense) of the same DNN model.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3167Table 2: The congurations of our FIA and the baseline at-
tacks. FIA’s four bound values (𝛿) are for the four datasets
listed in Table 11 of Appendix A.1, respectively.
Table 3: Detection rates at 5% FPR of benign target samples
when TeD defends single category and all categories.
Attack Method
PGD
C&W
Our FIA
OSA
Attack Conguration
𝛿=8, 𝑛𝑖𝑡𝑒𝑟 =100, 𝜂=0.1
binary step size=9, 𝜂=0.05,
max iteration=1000, condence=10.0
𝛿=64/16/8/16, 𝑛𝑖𝑡𝑒𝑟 =5000, 𝜂=0.05
𝛿=64, 𝑛𝑖𝑡𝑒𝑟 =5000, 𝜂=0.05
The perceptual quality of adversarial examples crafted with FIA
on TeD-protected models is compared with those crafted with PGD
on clean models (i.e., without TeD protection) in Appendix A.5.
FIA’s perceptual quality is the same as or a little better than PGD
when the same bound is used.
The above baseline adversarial attacks are chosen in our empiri-