size. This corresponds to a total memory of 4 GB. The ORAM
controller uses a stash of size 128 blocks and an on-chip
position map of 256 KB. For integrity and freshness, Tiny
ORAM uses the PosMap MAC (PMMAC) scheme [19]. We
note that PMMAC protects data integrity but does not achieve
malicious security. We estimate the cost of malicious security
using a hardware Merkle-tree on ORAM in Table II. We
disable the PosMap Lookaside Buffer (PLB) in Freecursive
ORAM to avoid leakage through the total number of ORAM
accesses.
TABLE II: Resource allocation and utilization of HOP on
Xilinx Virtex V7485t FPGA. For each row, ﬁrst line indicates
the estimate. % utilization is mentioned in parentheses. LUT:
Slice LookUp Table, FFs: Flip-ﬂops or slice registers, BRAM:
Block RAM.
Total Estimate
(% Utilization)
HOP Estimate
(% Utilization)
(HOP− ORAM) Estimate
(% Utilization)
Estimate with Merkle tree
(% Utilization)
LUT
169472
(55.8%)
103462
(34.0%)
21626
(7.1%)
221041
(72.8%)
FFs
LUT-Mem
BRAM
51870
(8.5%)
39803
(6.6%)
6579
(1.1%)
81410
(13.4%)
81112
(62.0%)
38725
(47.7%)
1
(∼0%)
81126
(62.0%)
566.5
(55.0%)
437
(42.4%)
83
(8.1%)
566.5
(55.0%)
A. Methodology
We measure program execution time in processor cycles,
and compare with our own baseline scheme (to show the effec-
tiveness of our optimizations), an insecure processor as well as
related prior work. For each program, we choose parameters so
that our baseline scheme requires about 100 million cycles to
execute. We also report processor idle time, the time spent on
dummy arithmetic instructions and dummy memory accesses
to adhere to an AN M schedule (Section III-C).
For the programs we evaluate (except bzip2; c.f., Sec-
tion VI-D), we calculate T manually. We remark that the
average input completion time and worst case time are very
similar for these programs. To ﬁnd T for larger programs,
one may use established techniques in determining worst case
execution time (e.g., a tool from [54]).
In our prototype, evaluating an arithmetic instruction takes
1 cycle while reading/writing a word from the scratchpad
takes 3 cycles. Given the parameters in Section V-B, an
ORAM access takes 3000 cycles. For our HOP conﬁgurations
with a scratchpad, we require both scratchpad read/writes and
arithmetic instructions to take 3 cycles in order to hide which is
occurring. Following Section III-C, we set N = 3000 when not
using a scratchpad; with a scratchpad, we use N = 1000. For
our evaluation, we consider programs ranging from those with
high locality (e.g., bwt-rle) to those that show no locality
(e.g., binsearch).
C. Encryption Units
B. Area Results
For all encryption units, we use tinyaes from Open-
Cores [2]. The encryption units communicate with the external
DRAM (bandwidth of 64 Bytes/cycle) as well as the host
processor. Data is encrypted before writing to the DRAM.
Similarly, all data read from the DRAM is decrypted ﬁrst
before processed by the ORAM controller. Another encryption
unit is used to decrypt the obfuscated program before loading
it into the instruction scratchpad.
VI. EVALUATION
We now present a detailed evaluation of HOP for some
commonly used programs, and compare HOP to prior work.
We synthesized, placed and routed HOP on a Xilinx Virtex
V7485t FPGA for parameters described in Section V. HOP
operates at 79.3 MHz on this FPGA. The resource allocation
and utilization ﬁgures are mentioned in Table II. The ﬁrst
three rows represent
the total estimate, estimate for HOP
(i.e. excluding RISC-Vprocessor, and the scratchpad) and an
estimate for HOP that does not account for ORAM. The last
row shows the total overhead including an estimate for a
Merkle tree scheme. Excluding the processor, scratchpad and
ORAM, HOP consumes < 9% of the FPGA resources. We see
that the total area overhead of HOP is small and can be built
on a single FPGA chip.
11
Comparison to insecure processor. The remaining perfor-
mance overhead of the optimized HOP (the third bar) comes
from several sources. First, the performance of ORAM: The
number of cycles to perform a memory access using ORAM
is much higher than a regular DRAM. In HOP, an ORAM
access is 40× more expensive than an insecure access. Second,
dummy accesses to adhere to a schedule: As shown in Sec-
tion III-C, the performance overhead due to dummy accesses
≤ 2×. For programs such as bwt-rle, HOP has a slowdown
as low as 8×. This is primarily due to the reduction in ORAM
accesses by maintaining a small working set in the scratchpad.
D. Case Study: bzip2
To show readers how our system performs on a realistic
and complex benchmark, we evaluate HOP on the open-source
algorithm bzip2 (re-written for a scratchpad, cf. Figure 7).
We evaluate the decompression algorithm only, as the decom-
pression algorithm’s performance does not heavily depend on
the input if one ﬁxes the input size [1]. This allows us to run
an average case input and use its performance to approximate
the effect of running other inputs. To give a better sense for
how the optimizations are impacted by different inputs, we
don’t terminate at a worst-case time T but rather terminate as
soon as the program completes.
We run tests on two inputs, both highly compressible
strings. For the ﬁrst input, HOP achieves 106× speedup over
the baseline scheme and 17× slowdown over the insecure
version. For the second input, HOP achieves 234× speedup
over the baseline and 8× slowdown over the insecure version.
Thus, the gains and slowdowns we see from the prior studies
extend to this more sophisticated benchmark.
E. Comparison with Related Work
We now compare against prior work on obfuscation with
hardware (these prior works were not implemented) and sev-
eral works with related threat models.
1) Comparison to prior obfuscation from trusted hardware
proposals [15], [17], [27]: We now compare against [15],
[17], [27] which describe obfuscation using trusted hardware.
Note that none of these schemes were implemented.
Part of the proposals in [15], [17] require programs to
be run as universal circuits under FHE while [27] evaluates
programs as universal circuits directly on hardware (i.e., by
feeding the encrypted inputs of each gate into a stateless
hardware unit: where it decrypts the inputs, evaluates the gate,
and re-encrypts the output). We will now compare HOP to
these circuit-based approaches. Again, we stress that all of
[15], [17], [27] require the use of trusted hardware for their
complete scheme and thus can be viewed similarly to HOP
from a security perspective.
Table III shows the speedup achieved by HOP relative
to universal circuits run under FHE (left) and bare hardware
(right). We assume the cost of a universal circuit capable of
evaluating any c gate circuit is 18 ∗ c ∗ log c gates [39]. We
compare the approaches on the findmax and binsearch
benchmarks, using a dataset size of 1 GB for each. We show
findmax as it yields a very efﬁcient circuit and a best-case
situation for the circuit approach (relative to the correspond-
ing RAM program); binsearch shows the other extreme.
12
Fig. 8: Execution time for different programs with (i) baseline
scheme, (ii) AN M schedule and (iii) Scratchpad + AN M.
C. Main Results
Figure 8 shows the execution time of HOP variants relative
to an insecure processor. For each program, there are three
bars shown. The ﬁrst bar is for the baseline HOP scheme
(i.e., Section III-B only); the second bar only uses an AN M
schedule without a scratchpad (adds Section III-C); and the
third bar is our ﬁnal scheme that uses a scratchpad and the
AN M schedule (adds Section III-D). All schemes are relative
to an insecure processor that does not use ORAM or hide what
instruction it is executing. We assume this processor uses a
scratchpad that has the same capacity as HOP in Section V-A.
The time required to insert the program and data is not shown.
Comparison of HOP variants. As can been seen in the ﬁgure,
the AN M schedule without a scratchpad gives a 1.5× ∼ 18×
improvement. Adhering to an AN M schedule requires some
dummy arithmetic or memory instructions during which the
processor is essentially idle. We observe that for our programs,
the idle time ranges between 43% and 49.9% of the execution
time, consistent with the claim in Section III-C.
Effect of a scratchpad. The effect of a scratchpad largely
depends on program locality. We thus classify programs in
our evaluation into four classes:
1) Programs such as binsearch, heappop do not show
locality. Thus, a scratchpad does not improve performance.
2) Programs such as sum, findmax stream (linear scan)
over the input data. Given that an ORAM block is larger
than a word size (512 bits vs 32 bits in our case), a
scratchpad in these streaming applications can serve the
next few (7 with our parameters) memory accesses after
spld. A larger ORAM block size can slightly beneﬁt these
applications while severely penalize programs with no
locality, and therefore is not a good trade-off.
3) Programs that maintain a small working set at all times will
greatly beneﬁt from a scratchpad. We evaluate one such
program bwt-rle, which performs Burrows-Wheeler
transform and run length encoding, and is used in com-
pression algorithms.
4) Lastly, some programs are a mix of the above cases — some
data structures can be entirely loaded into the scratchpad
whereas some cannot (e.g. a Radix sort program).
binsearchheappopsumﬁndmaxradixsorthistbwt-rle100101102103SlowdowntoInsecureExecutionBaselineANMScratchpadwithANMFor [15], [17], we assume a BGV-style FHE scheme [9],
using the NTRU cryptosystem, with polynomial dimension and
ciphertext space parameters chosen using [24], to achieve 80
bits of security.6 For [27], we assume each NAND gate takes
20 cycles to evaluate (10 cycles for input decryption with AES,
0 cycles for evaluation, 10 cycles for re-encryption). For HOP,
we assume the parameters from Section V.
FHE [15], [17]
On
Hardware [27]
On
On + Off
4 ∗ 103
6 ∗ 103
1 ∗ 104
1 ∗ 1010
On + Off
1 ∗ 109
4 ∗ 109
2 ∗ 109
4 ∗ 1015
findmax
binsearch
TABLE III: HOP speedup (×) relative to universal circuit ap-
proaches. findmax and binsearch are over 1 GB datasets.
In the Table, On+Off (‘online and ofﬂine’) assumes one
search query is run: in that case, HOP’s performance is reduced
due to the time needed to initially load the ORAM. The On
(‘online only’) column shows the amortized speedup when
many search queries are made without changing the underlying
search database (i.e., without re-loading the ORAM each time).
This shows an inherent difference to works based on universal
circuits: those works represent programs as circuits, where
optimized algorithms such as binsearch do not see speedup.
In all cases, HOP shows orders of magnitude improvement to
the prior schemes.
We note that our comparison to [15], [17] is conserva-
tive: we only include FHE’s time to perform AND/OR gate
operations and not the cost of auxiliary FHE operations (re-
linearization, modulus switching, bootstrapping, etc). Lastly,
FHE is only one part of [15], [17]: we don’t include the cost
of NIZK protocols, etc. which those schemes also require.
2) Comparison with iO [37]: We compare HOP with an
implementation of indistinguishability obfuscation (iO) that
does not assume a trusted hardware token. Note that while
VBB obfuscation is not achievable in general, iO is a weaker
notion of obfuscation. With [37], evaluating an 80-bit point
function (a simple function that is 0 everywhere except at one
point) takes about 180 seconds while HOP takes less than a
msec, which is about 5-6 orders of magnitude faster.
3) Comparison with GhostRider [40]: Recall from Sec-
tion II that GhostRider protects input data to the program
but not the program. Since our privacy guarantee is strictly
greater than GhostRider, we now compare to that work to
show the cost of extra security. Note: we compare to the
GhostRider compiler and not the implementation in [40] which
uses a different parameterization for the ORAM scheme. This
comparison shows the additional cost that is incurred by HOP
to hide the program. We don’t show the full comparison for
lack of space, but point out the following extreme points: For
programs with unpredictable access patterns (binsearch,
heappop), GhostRider outperforms HOP by ∼ 2×. HOP’s
additional overhead is from executing dummy instructions to
6When represented as circuits, both findmax and binsearch look like
a linear PIR. Over a 1 GByte dataset, we evaluate this function with a 10-
level FHE circuit, which gives an FHE polynomial dimension (n) of ∼ 8192
and ciphertext space q of ∼ 2128 (using terminology from [9]). With these
parameters, a single polynomial multiplication/addition using NTL [50] costs
14 ms / .4 ms on a 3 GHz machine.
13
adhere to a particular schedule. For programs with predictable
access patterns (sum, findmax, hist), GhostRider’s per-
formance is similar to that of an insecure processor.
F. Time for Context Switch
Since it was not required for our performance evaluation,
we have not yet implemented context switching (Section III-E)
in our prototype. Recall, context switching means the receiver
interrupts the processor, which encrypts and writes out all the
processor state (including CPU state, instruction scratchpad,
data scratchpad, ORAM position map and stash) to DRAM.
We estimate the time of a context switch as follows. The total
amount of data stored by our token is ∼ 800 KB (Section V).
Assuming a DRAM bandwidth of 10 GB/s and a matching
encryption bandwidth, it would take ∼ 160µs to perform a
context switch to run another program. Note that this assumes
all data for a swapped-out context is stored in DRAM (i.e.,
the ORAM data already in the DRAM need not be moved).
If it must be swapped out to disk because the DRAM must
make room for the new context, the context switch time grows
proportional to the ORAM size.
VII.
PRACTICAL DEPLOYMENT AND APPLICATIONS