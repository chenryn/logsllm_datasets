four of which are taken from the PARSEC [25] and Phoenix
benchmarks [26], that satisfy the pseudo-deterministic condi-
tion. These programs were also used in prior work on using
likely invariants for EPA to counter non-determinism [12].
Table I reports the number of SLOCS of each program
along with the total number of instructions and spawned
threads. quicksort is a parallel implementation of the
well-known sorting algorithm. pca and kmeans are two
machine-learning algorithms taken from the Phoenix bench-
mark suite [26]. kmeans is an implementation of the Kmeans
clustering algorithm and pca implements the Principal Com-
ponent Analysis statistical procedure. Additionally, we used
the blackscholes and swaptions programs from the
PARSEC benchmark suite [25]. The blackscholes pro-
grams solves the blackscholes partial differential equa-
tion used in pricing a portfolio of European-style stock op-
tions. swaptions uses Monte-Carlo simulations to compute
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:30:10 UTC from IEEE Xplore.  Restrictions apply. 
59
(cid:12)(cid:13)(cid:14)(cid:15)(cid:16)(cid:17)(cid:15)(cid:18)(cid:19)(cid:13)(cid:20)(cid:17)
(cid:16)(cid:21)(cid:20)(cid:14)(cid:22)(cid:17)
(cid:23)(cid:15)(cid:14)
(cid:24)(cid:25)(cid:26)(cid:15)(cid:16)(cid:17)(cid:19)(cid:27)(cid:28)
(cid:17)(cid:29)(cid:14)(cid:23)(cid:28)(cid:26)(cid:19)(cid:22)(cid:17)
(cid:1)(cid:2)(cid:3)(cid:4)
(cid:1)(cid:2)(cid:5)(cid:6) (cid:1)(cid:2)(cid:5)(cid:7)
(cid:1)(cid:2)(cid:3)(cid:4) (cid:1)(cid:2)(cid:3)(cid:4)
(cid:1)(cid:2)(cid:5)(cid:3) (cid:1)(cid:2)(cid:5)(cid:5)
(cid:1)(cid:2)(cid:5)(cid:6)
(cid:1)(cid:2)(cid:5)(cid:11) (cid:1)(cid:2)(cid:5)(cid:11)
(cid:1)(cid:2)(cid:5)(cid:10) (cid:1)(cid:2)(cid:5)(cid:8)
(cid:1)(cid:2)(cid:5)(cid:10) (cid:1)(cid:2)(cid:5)(cid:8)
(cid:1)(cid:2)(cid:10)(cid:3)
(cid:1)(cid:2)(cid:5)(cid:3) (cid:1)(cid:2)(cid:5)(cid:5) (cid:1)(cid:2)(cid:3)(cid:4) (cid:1)(cid:2)(cid:5)(cid:3) (cid:1)(cid:2)(cid:5)(cid:3)
(cid:1)(cid:2)(cid:8)(cid:9) (cid:1)(cid:2)(cid:9)(cid:1) (cid:1)(cid:2)(cid:9)(cid:4) (cid:1)(cid:2)(cid:8)(cid:10) (cid:1)(cid:2)(cid:8)(cid:9)
(cid:30)(cid:20)(cid:22)(cid:26)(cid:42)(cid:22)
(cid:35)(cid:27)(cid:14)(cid:17)(cid:18)
(cid:32)(cid:43)(cid:35)
(cid:4)(cid:2)(cid:1)
(cid:1)(cid:2)(cid:5)
(cid:1)(cid:2)(cid:9)
(cid:1)(cid:2)(cid:7)
(cid:1)(cid:2)(cid:11)
(cid:1)(cid:2)(cid:1)
(cid:20)
(cid:42)
(cid:14)
(cid:27)
(cid:20)
(cid:37)
(cid:19)
(cid:35)
(cid:39)
(cid:28)
(cid:13)
(cid:25)
(cid:14)
(cid:31)
(cid:30) (cid:26)(cid:28) (cid:31) (cid:13)(cid:26) (cid:23)
(cid:31) (cid:26)(cid:13) (cid:20)
(cid:32) (cid:26)(cid:33)
(cid:34)
(cid:20)
(cid:20)
(cid:14) (cid:13)(cid:13) (cid:32) (cid:26)(cid:33)
(cid:35)
(cid:14) (cid:13)(cid:13) (cid:35)
(cid:19) (cid:27)(cid:27)
(cid:36) (cid:22)
(cid:37)
(cid:14) (cid:13) (cid:38) (cid:28)(cid:27)
(cid:30) (cid:26)(cid:28) (cid:31) (cid:13)(cid:26) (cid:23)
(cid:31) (cid:26)(cid:13) (cid:20)
(cid:32) (cid:26)(cid:33)
(cid:34)
(cid:20)
(cid:20)
(cid:14) (cid:13)(cid:13) (cid:32) (cid:26)(cid:33)
(cid:35)
(cid:14) (cid:13)(cid:13) (cid:35)
(cid:19) (cid:27)(cid:27)
(cid:36) (cid:22)
(cid:37)
(cid:14) (cid:13) (cid:38) (cid:28)(cid:27)
(cid:30) (cid:26)(cid:28) (cid:31) (cid:13)(cid:26) (cid:23)
(cid:31) (cid:26)(cid:13) (cid:20)
(cid:20)
(cid:20)
(cid:14) (cid:13)(cid:13) (cid:35)
(cid:19) (cid:27)(cid:27)
(cid:32) (cid:26)(cid:33)
(cid:14) (cid:13)(cid:13) (cid:32) (cid:26)(cid:33)
(cid:36) (cid:22)
(cid:34)
(cid:31)(cid:14)(cid:25)(cid:13)(cid:28)(cid:39)(cid:40)(cid:41)(cid:23)(cid:20)
(cid:35)
(cid:14) (cid:13) (cid:38) (cid:28)(cid:27)
(cid:37)
(cid:30) (cid:26)(cid:28) (cid:31) (cid:13)(cid:26) (cid:23)
(cid:31) (cid:26)(cid:13) (cid:20)
(cid:32) (cid:26)(cid:33)
(cid:34)
(cid:20)
(cid:20)
(cid:14) (cid:13)(cid:13) (cid:32) (cid:26)(cid:33)
(cid:35)
(cid:14) (cid:13)(cid:13) (cid:35)
(cid:19) (cid:27)(cid:27)
(cid:36) (cid:22)
(cid:37)
(cid:14) (cid:13) (cid:38) (cid:28)(cid:27)
(cid:30) (cid:26)(cid:28) (cid:31) (cid:13)(cid:26) (cid:23)
(cid:31) (cid:26)(cid:13) (cid:20)
(cid:32) (cid:26)(cid:33)
(cid:34)
(cid:20)
(cid:20)
(cid:14) (cid:13)(cid:13) (cid:32) (cid:26)(cid:33)
(cid:35)
(cid:14) (cid:13)(cid:13) (cid:35)
(cid:19) (cid:27)(cid:27)
(cid:36) (cid:22)
(cid:37)
(cid:14) (cid:13) (cid:38) (cid:28)(cid:27)
Fig. 5. Results from 5000 runs of each combination of target and fault type. The error bars indicate the 95 % conﬁdence interval.
swaptions, a form of ﬁnancial derivatives. We veriﬁed that
all ﬁve programs satisfy the pseudo-deterministic condition.
C. RQ 1: False Positives from Memory Addresses
Our work is motivated by the observation that non-
determinism can lead to false positives if a naive execution
trace comparison is applied for EPA. As such benign non-
determinism can affect execution traces differently depending
on the origin, TraceSanitizer employs different analyses, which
we evaluate separately. We separately evaluate the effectiveness
of TraceSanitizer in eliminating false positives in EPA that
are due to dynamic memory allocation and non-deterministic
scheduling. We begin with an evaluation of the impact that
dynamic memory allocation non-determinism has on execu-
tion traces, and how well TraceSanitizer can deal with those
cases. We then evaluate false positives resulting from CPU
scheduling non-determinism in Section VI-D.
To evaluate the impact of dynamic memory allocation non-
determinism independently from CPU scheduling effects, we
conduct a number of trace comparisons on single-threaded ex-
ecutions of our target programs without any fault injections.
For this purpose, we use single-threaded versions of the ﬁve
programs from Table I. As we do not inject any faults, any
observed deviation across repeated executions of the same pro-
gram must be a false positive. Moreover, since CPU scheduling
cannot cause deviations in single-threaded programs, any ob-
served false positive is likely due to memory non-determinism.
To simulate an “unlucky” injection campaign with numer-
ous unactivated faults, we generate 10 000 fault-free execution
traces for each of the single-threaded programs by performing
steps 1 and 2 in Figure 3, and perform a line-by-line com-
parison, just as conventional EPA approaches would compare
golden run and fault injection traces. We then run TraceSani-
tizer’s memory abstraction algorithm on the same traces – the
results are in the Mem-Sound column of Table I.
The results show that no execution trace is identical to any
other from the 10 000 repetitions, and that TraceSanitizer is
able to eliminate all of these spurious deviations for all of
the benchmarks. Besides demonstrating the effectiveness of
our memory abstraction, this result conﬁrms that the second
criterion from our deﬁnition of the pseudo-deterministic condi-
tion in Section I is satisﬁed for the chosen program input. If
non-deterministic external functions affect the execution, this
would lead to trace deviations across the repeated executions.
D. RQ 2: False Positives from CPU Scheduling
To assess the effectiveness of TraceSanitizer to compensate
for the effects of non-deterministic CPU scheduling, we recom-
piled the target programs to use multiple threads, and generate
10 000 fault-free execution traces for each program. We sani-
tized the effects of memory allocation non-determinism in the
execution traces and compared the resulting traces of the re-
peated executions. As no faults are injected in these executions,
any deviations between traces constitute false positives. More-
over, as the effects of memory allocation non-determinism are
sanitized, all deviations must result from CPU scheduling.
We again compare the obtained traces in a line-by-line fash-
ion without TraceSanitizer, and observe deviations in each
comparison. We then run TraceSanitizer’s thread abstraction
algorithm on the traces and perform the comparison again.
The results shown in the Sched-Sound column of Table I
demonstrate that TraceSanitizer is able to fully eliminate false
positives resulting from non-deterministic CPU scheduling for
pseudo-deterministic programs.
E. RQ 3: False Negatives Introduced by TraceSanitizer
Accurately measuring false negative rates in EPA experi-
ments is challenging, because there is no oracle to distinguish
between true and false negatives. If a fault is injected and no
effect is observed, it is unclear whether no effect has occurred
(true negative), or an effect has occurred and it was missed by
the detection mechanism (false negative). Moreover, differen-
tial testing using different detection mechanisms is difﬁcult to
apply in the case of EPA for multi-threaded programs, because
other approaches are not sound, and their false positives would
distort the results. Therefore, we base our evaluation of false
negatives on a conservative estimate.
Assuming that each injected fault leads to error propaga-
tion (this may not always hold [27]), each succeeding trace
comparison between an injection and a fault-free run consti-
tutes a false negative. We term the fraction of these succeeding
comparisons from all comparisons the maximal possible false
negative rate (MPFNR).
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:30:10 UTC from IEEE Xplore.  Restrictions apply. 
60
To ensure that TraceSanitizer does not achieve soundness
at the cost of an increased false negative rate, we executed
a number of fault injection experiments following the steps
outlined in Figure 3 and discussed in Section IV-A and as-
sessed the MPFNR. We used the multi-threaded versions of the
benchmark programs from Section VI-D, and ﬁrst generated
and sanitized execution traces from one fault-free execution
of each program. We then ran our memory and thread abstrac-
tion algorithms (cf. Section IV-C) on the trace and performed
the reversibility check to ensure that comparisons against this
trace yield sound results if the same program inputs are used.
To obtain traces from faulty executions, we repeated the
execution of the program with the same inputs and injected one
fault per execution using the LLFI fault injection framework.
The injection points for the faults are decided dynamically
by the LLFI framework. In total we executed 25 000 such
experiments, consisting of 5000 fault instances for each of the
fault types listed in Table II (these were also used in prior
work [12]). We then sanitized each of the resulting execution
traces using TraceSanitizer.
Figure 5 shows the fault coverage of EPA using TraceSani-
tizer, i.e., the fraction of experiments for which the sanitized
fault-injection traces differed from the fault-free trace and, thus,
indicate error propagation. The MPFNR is the difference be-
tween 1 and the reported fault coverage and ranges between
44 % and 9 % depending on the program and fault type. While
we cannot tell whether any of the succeeded comparisons was
due to the lack of error propagation or due to a false negative
of our approach, we can tell if TraceSanitizer has any obvi-
ous blind spots by investigating the false negative rates for
experiments that led to program failures (i.e., externally observ-
able deviation from correct behavior). If the program behavior
deviates from correct behavior as observed in the fault-free
execution, an error must have propagated, and missing such
propagation in the traces would be a false negative.
To assess the error propagations that we missed, we have
calculated the MPFNR for different classes of experiment
outcomes that are indicated by different colors in Figure 5. A
crash denotes cases where the program terminated abnormally
after a fault was injected, whereas SDC (silent data corruption)
indicates cases where the program terminated without error
indication, but its results differed from the fault-free case. For
both crashes and SDCs, we found the MPFNR to be 0 %.
This demonstrates that there were no obvious cases of error
propagation that were missed by TraceSanitizer.
To the best of our knowledge, TraceSanitizer is the ﬁrst to
achieve a 0 % false positive rate for EPA on multi-threaded
programs without increasing the false negative rate for known
cases of error propagation (observed crashes and SDCs).
F. RQ 4: TraceSanitizer Overhead
Achieving a high fault coverage and fully eliminating false
positives comes at the cost of (a) running the reversibility check
on the golden run to ensure the soundness of the approach,
and (b) running the sanitization algorithms on the traces.
TABLE II
OVERVIEW OF INJECTED FAULT TYPES
Fault Type
BitFlip
FileSize
MallSize
CallCorr
InvalPtr
Short Description
Flips single bits in arbitrary data values.
Increases the size parameter
fwrite function calls for ﬁle I/O.
Decreases the size parameter in malloc and
calloc function calls for memory allocation.
Corrupts the ﬁrst parameter of function calls.
Corrupts the returned pointers from malloc and
calloc function calls.
in fread and
TABLE III
PERFORMANCE RESULTS FOR TSAN. REVERSIBILITY CHECK TIMES ARE
REPORTED IN MINUTES AND EPA TIMES IN SECONDS. EPA TIMES ARE
MEDIAN VALUES OVER 5000 RUNS.
Program
quicksort
pca
kmeans
blackscholes
swaptions
Rev. Check
EPA
Total
[m]