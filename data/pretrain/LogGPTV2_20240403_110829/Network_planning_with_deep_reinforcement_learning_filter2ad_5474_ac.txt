problem, which is exactly the set of problems that RL is designed
to solve. Specifically, RL considers the problems where there is an
agent interacting with an environment [46, 47]. The agent observes
the environment, takes actions to change the environment, and
receives rewards from the environment. Through a series of inter-
actions, the agent learns a policy to maximize its rewards. The RL
setup can be naturally mapped to the network planning problem.
The environment is the network topology with the traffic demand
and reliability policy, the actions are to change the link capacity
of the topology, and the rewards are to indicate the final cost of
the network topology and whether the traffic demand is satisfied
under the reliability policy.
Advantage of deep RL approach. While many problems are fun-
damentally hard in terms of computational complexity, deep RL
has been recently demonstrated to achieve remarkable results in
many domains [1, 34, 43, 54, 59, 69, 74]. The key reason is that deep
RL is able to leverage neural networks to learn the structure of the
particular problem by exploring the search space and exploit the
learned structure to optimize its policy. This is essentially how hu-
mans design heuristics to take advantage of the problem structure
to solve the problem. The real appeal here is that the RL agent can
automatically derive such heuristics using a deep RL algorithm,
obviating the need to manually design and tune heuristics with
human experts.
search space
optimal
ùú∂ùüê
ùú∂ùüè
RL
Figure 2: Two-stage hybrid approach in NeuroPlan. The first
stage uses RL to find an initial solution, and the second stage
uses ILP to find the final solution in a sub search space near
the initial solution bounded by the relax factor Œ± . The factor
Œ± provides a knob for the trade-off between optimality and
tractability.
Moreover, in the language of RL, RL is able to model delayed
rewards and directly optimize for the global objective. This is the
reason that in many cases, deep RL algorithms can often beat human
experts. With regard to our problem, the hand-designed heuristics
like the ones we describe in ƒü3.2 often focus on local decisions.
For example, a heuristic to limit the capacity of an IP link to a
small range (i.e., tighten the constraint) is often based on the flows
on the IP link, not all flows and the entire topology. It is hard
for human experts to model the relationship between the local
decisions and the global objective in the heuristics, and as a result,
the local decisions are often evaluated with local metrics that are
only loosely coupled with the global objective. In comparison, deep
RL can choose an action even if the impact of the action can only be
evaluated after several steps, i.e., the reward of the action is delayed.
Deep RL is able to learn to choose actions to directly optimize for
the global objective.
An alternative is to apply supervised learning which uses a train-
ing set with labeled samples to train a neural network. However,
supervised learning usually requires a large training set and a label
for each sample (i.e., optimal plan for a network) in the training
set, both of which are hard to obtain for our problem, especially for
large-scale network topologies. The key benefit of deep RL is that
it is able to explore the search space automatically and efficiently.
Challenge 1: Dynamic network topology. While it is natural
and appealing to apply deep RL to network planning, there are
two primary technical challenges. The first challenge is to model
dynamic network topology in deep RL. A deep RL agent typically
requires a vector to encode the environment state as the input of
its neural network to generate actions. However, the environment
of the network planning problem is the network topology, which
is a graph. The size of a graph varies depending on the number of
nodes and edges, and the structure cannot be easily captured with
a vector. To make it more challenging, the graph is dynamic when
the RL agent interacts with the topology to change the capacity of
IP links.
To address this challenge, we leverage graph representation
learning [13] to encode the network topology with GNNs [52].
GNNs are a family of neural networks specifically designed for
graph data [78]. It takes a graph as input and can learn to generate
262
SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, Netherlands
Hang Zhu, Varun Gupta, Satyajeet Singh Ahuja,
Yuandong Tian, Ying Zhang, Xin Jin
Traffic
Demand
Network
Topology
Failure
Scenarios
Reliability
Policy
Cost
Model
RL
ILP
New
Plan
Plan Evaluator
Figure 3: The workflow of NeuroPlan.
embedding vectors to represent its nodes, links or the entire graph.
It can support many node-level, link-level or graph-level tasks. For
our problem, GNNs are better than other sequence-based graph
representations (e.g., recurrent neural networks [16]) since GNNs
eliminate the dependency of the order where the nodes are given in
the input. We take the dynamic network topology as the input of a
GNN to generate an embedding vector for the entire topology. The
embedding vector captures the essential features of the topology,
and serves as the input of the RL agent. One characteristic about
our problem is that we care about edges (i.e., the capacity of the
IP links), not nodes, of the graph. And there exist parallel links
between two IP nodes, which are mapped to different fiber paths and
are associated with different failure scenarios. The novelty in our
approach is that we use a domain-specific node-link transformation
to transform the topology based on this characteristic to improve
learning efficiency and support parallel links.
Challenge 2: Trade-off between optimality and tractability.
A straightforward approach to apply deep RL to network planning
is to directly use the RL agent to generate the network plan. Given
the combinatorial nature of the problem, it is fundamentally hard
for the RL agent to find the optimal solution. The RL agent can
learn to find reasonably good solutions, but it is both unpredictable
and time-consuming for the agent to find the optimal solution.
To address this challenge, we leverage a two-stage hybrid ap-
proach with RL and ILP. Instead of using RL to directly generate the
final solution, we use it to prune the search space and bootstrap ILP.
Figure 2 illustrates our approach. It is infeasible to search the entire
space to find the optimal solution with ILP due to the large and
complex search space. In the first stage, we use deep RL to learn to
find a reasonable solution. In the second stage, we use ILP to only
search the space near the solution found by deep RL. We use the
relax factor Œ± to control the size of the space to explore by ILP. The
relax factor Œ± provides a tunable knob for the network operator
to trade-off between optimality and tractability. A large Œ± (Œ±2 in
Figure 2) allows ILP to explore larger space, but the problem may
be intractable or takes a very long time for the ILP solver to solve.
On the other hand, a small Œ± (Œ±1 in Figure 2) may not include the
optimal solution in the search space, but the ILP solver may finish
the second stage quickly.
Workflow of NeuroPlan. Figure 3 shows the workflow of Neu-
roPlan. The input of NeuroPlan consists of five components, i.e.,
traffic demand, network topology, failure scenarios, reliability pol-
icy and cost model. The RL agent only needs to encode the network
topologies. The other four components are handled by the plan
evaluator. The RL agent interacts with the plan evaluator to learn
to generate network plans that minimize the network cost while
satisfying the traffic demand under the reliability policy. The plan
evaluator generates rewards to the RL agent. It receives the net-
work plan from the RL agent, checks whether the traffic demand is
satisfied under different failure scenarios, and uses the cost model
to compute a cost of the plan. The reliability policy specifies the
demand of flows with which Classes of Service (CoS) has to be satis-
fied under which subset of failure scenarios. A reward is calculated
based on a combination of whether the demand is satisfied and the
cost of the plan. We describe the details of RL agent and the reward
encoding in ƒü4.2. After the learning process is completed, the RL
agent outputs an initial plan for the first stage. At the second stage,
the ILP solver uses ILP to find the final plan in the pruned search
space near the initial plan defined by the relax factor Œ±. We describe
the details of pruning the search space using the initial plan and
the relax factor Œ± in ƒü4.3.
Unifying short-term and long-term planning. We use Neuro-
Plan to unify short-term and long-term planning with the same
approach. The main difference between short-term and long-term
planning is whether the IP links are given. Our key observation is
that the starting topology for long-term planning can be considered
as a topology with all the candidate IP links with the starting capac-
ity to be zero. These candidate IP links are mapped to different fiber
paths that are currently not used but can potentially be procured or
built to the topology. Then both short-term and long-term planning
can be solved by the same agent that decides capacity to a given
topology until the traffic demand is satisfied under the reliability
policy.
4.2 NeuroPlan Training Algorithm
Figure 4 shows the process of the RL agent to generate a network
plan from the original network topology. For each step i, the RL
agent performs an action to update the network topology (e.g.,
add some capacity to an IP link). The plan evaluator checks the
updated topology to see if the traffic demand is satisfied under the
reliability policy. The condition for a trajectory to stop is that the
traffic demand is satisfied under the reliability policy or the number
of steps (i.e., the number of actions taken) is equal to a pre-defined
threshold. It is a common practice for deep RL to set a maximum
number of steps to stop a trajectory even if the task (e.g., find a
plan that satisfies the traffic demand under the reliability policy)
is not completed. This helps improve training efficiency as the RL
agent is early stopped on unpromising trajectories. If the current
trajectory is terminated, the RL agent starts a new trajectory from
the original network topology to gain more experience to train its
neural network. Otherwise, the agent performs another action for
step i+1. For each step, the agent receives an award after it performs
its action.
Node-link transformation. While there are a variety of GNNs
for different graph data and tasks [1, 43, 78], GNNs are most mature
for handling node features and performing node tasks such as
node classification [29] and node property prediction [74]. In the
context of network planning, however, we care about links, not
nodes. Specifically, we would like the GNN to generate a high-level
graph embedding based on the current capacity of the IP links (i.e.,
263
Network Planning with Deep Reinforcement Learning
SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, Netherlands
Plan
Evaluator
Stop?
no
RL
Agent
yes
+‚àÜ
topology ùëñ
topology ùëñ+1
Next Trajectory
Figure 4: The process of RL training to generate a network
plan.
B
C
A
D
E
AB
AD
BC1
BC2
DE
CE
(a) Before transformation.
(b) After transformation.
Figure 5: Node-link transformation can handle parallel
links. The node names show the corresponding relationship
between nodes and links.
link features) and the RL agent can utilize the graph embedding to
decide which IP links should have more capacity (i.e., link tasks).
Moreover, there are parallel links (mapped to different fiber paths)
between two nodes, making it harder for the GNN to encode the
features as well as the graph structure.
We design a domain-specific node-link transformation to trans-
form the input network topology before feeding the topology to the
GNN. Unlike complement graph [6] which fills in all the missing
edges to form a complete graph, or dual graph [48] that maps faces
to vertexes, the main idea of the transformation is to map each link
in the input topology to a node in the transformed topology. Then
the link features (i.e., link capacity) in the input topology become
the node features in the transformed topology. This makes it easier
to apply state-of-art GNN algorithms to the topology. The nodes in
the transformed topology are connected if their corresponding links
have one common end in the input topology. Figure 5 provides an
example to illustrate the node-link transformation. Figure 5(a) is the
input topology before transformation. There are five nodes (A, B,
C, D, E) and six links (AB, AD, DE, CE, BC1, BC2) in total. BC1 and
BC2 are two parallel links between node B and node C. Figure 5(b)
is the network topology after transformation. The two parallel links
BC1 and BC2 are mapped to two nodes in the transformed topology.
We do not add links between nodes whose corresponding links
in the input topology are parallel links, e.g., we do not add links
between node BC1 and node BC2 in Figure 5(b). This is because the
parallel links make contribution to the capacity between the same
two nodes, and we do not want to propagate the capacities of the
parallel links during GNN training.
State representation. The state representation includes the net-
work topology and the node features. After the node-link transfor-
mation, the links in the input topology become the nodes in the