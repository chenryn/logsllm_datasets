E: Event
6
7
8
9
10
11
12
O1 ← results of running M(D1, arдs) for n times
O2 ← results of running M(D2, arдs) for n times
c1 ← |{i | O1[i] ∈ E}|
c2 ← |{i | O2[i] ∈ E}|
p⊤ ←pvalue (c1, c2, n, ϵ)
p⊥ ←pvalue (c2, c1, n, ϵ)
return p⊤, p⊥
• Formulate the null hypothesis as H0 : p1 ≤ eϵ · p2 and the
alternative as H1 : p1 > eϵ · p2.
• Run M with inputs D1 and D2 independently n times each.
Record the results as O1 and O2.
• Count the number of times the result falls in E in each case. Let
c1 = |{i | O1[i] ∈ E}| and c2 = |{i | O2[i] ∈ E}|. Intuitively,
c1 ≫ eϵ c2 provides strong evidence against the null hypothesis.
• Calculate a p-value based on c1, c2 to determine how unlikely
the null hypothesis is.
The challenge is, of course, in the last step as we don’t know what
p1 and p2 are. One direction is to estimate them from c1 and c2.
However, it is also challenging to estimate the variance of our
estimates ˆp1 and ˆp2 (the higher the variance, the less the test should
trust the estimates).
0.00.20.40.60.81.01.21.4Testϵ0.00.20.40.60.81.0PValueTestResultIdeal0.00.20.40.60.81.01.21.4Testϵ0.00.20.40.60.81.0PValueTestResultIdeal0.00.20.40.60.81.01.21.4Testϵ0.00.20.40.60.81.0PValueTestResultIdealCCS ’18, October 15–19, 2018, Toronto, ON, Canada
Zeyu Ding, Yuxin Wang, Guanhong Wang, Danfeng Zhang, and Daniel Kifer
Instead, we take a different approach that allows us to conduct
the test without knowing what p1 and p2 are. First, we note that
c1 and c2 are equivalent to samples from a Binomial(n, p1) dis-
tribution and a Binomial(n, p2) distribution respectively. We first
consider the border case where p1 = eϵp2. Consider sample ˜c1 from
a Binomial(c1, 1/eϵ) distribution. We note that this sample enjoys
the following property (which implies that in the border case, ˜c1
has the same distribution as c2):
Lemma 4.1. Let X ∼Binomial(n, p1) and Z be generated from X
by sampling from the Binomial(X , 1/eϵ) distribution. The marginal
distribution of Z is Binomial(n, p1/eϵ).
dom variables means that X =n
1/eϵ (and set Zi = 0 otherwise). Then set Z =n
Proof. The relationship between Binomial and Bernoulli ran-
i =1 Xi, where Xi is a Bernoulli(p1)
random variable. Generating Z from X is the same as doing the
following: set Zi = 0 if Xi = 0. If Xi = 1, set Zi = 1 with probability
i =1 Zi. Hence, the
marginal distribution of Zi is a Bernoulli(p1/eϵ) random variable:
P(Zi = 1) = P(Zi = 1 | Xi = 1)P(Xi = 1) + P(Zi = 1 | Xi = 0)P(Xi = 0)
= (1/e ϵ) · p1 + 0 · (1 − p1) = p1/e ϵ
This means that the marginal distribution of Z is Binomial(n, p1/eϵ).
□
Thus we have the following facts that follow immediately from
the lemma:
• If p1 > eϵp2 then the distribution of ˜c1 is Binomial(n, ˜p1) with
˜p1 = p1/eϵ and so has a larger Binomial parameter than c2
(which is Binomial(n, p2)). We want our test to be able to reject
the null hypothesis in this case.
• If p1 = eϵp2 then the distribution of ˜c1 is Binomial(n, ˜p1) with
˜p1 = p2 and so has the same Binomial parameter as c2. We do
not want our test to reject the null hypothesis in this case.
• If p1  eϵp2 vs. p1 ≤ eϵp2 (on the
basis of c1 and c2) to the problem of testing ˜p1 > p2 vs. ˜p1 ≤ p2 (on
the basis of ˜c1 and c2). Now, checking whether ˜c1 and c2 come from
the same distribution can be done with the Fisher’s exact test (see
Section 3): the p-value is 1− Hypergeom.cdf(˜c1 − 1 | 2n, n, ˜c1 +c2).2
This is done in the function pvalue in Algorithm 2.
To summarize, given c1 and c2, we first sample ˜c1 from the
Binomial(c1, 1/eϵ) distribution and then return the p-value of (1 −
Hypergeom.cdf(˜c1 − 1 | 2n, n, ˜c1 + c2)). Since this is a random re-
duction, we reduce its variance by sampling ˜c1 multiple times and
averaging the p-values. That is, we run the p-value function (Al-
gorithm 2) multiple times with the same inputs and average the
p-values it returns.
2Here we use a notation from SciPy [25] package where Hypergeom.cdf means the
cumulative distribution function of hypergeometric distribution.
4.3 Event Selection
Having discussed how to test if P(M(D1) ∈ E) > eϵ P(M(D2) ∈ E)
or if P(M(D2) ∈ E) > eϵ P(M(D1) ∈ E) when D1, D2, and E were
pre-specified, we now discuss how to select the event E that is most
likely to show violations of ϵ-differential privacy.
Algorithm 3: Event Selector. Parameter n: # of iterations
1 function EventSelector(n, M, ϵ, InputList):
input: M: mechanism
InputList: possible inputs
ϵ: privacy budget to test
pvalues ← [ ]
results ← [ ]
foreach (D1, D2, arдs) ∈ InputList do
SearchSpace ← search space based on return type
O1 ← results of running M(D1, arдs) for n times
O2 ← results of running M(D2, arдs) for n times
foreach E ∈ SeachSpace do
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
c1 ← |{i | O1[i] ∈ E}|
c2 ← |{i | O2[i] ∈ E}|
p⊤ ←pvalue (c1, c2, n, ϵ)
p⊥ ←pvalue (c2, c1, n, ϵ)
pvalues.append(min(p⊤, p⊥))
results.append(D1, D2, arдs, E)
end
end
return results[argmin(pvalues)]
One of the challenges is that different mechanisms could have
different output types (e.g., a discrete number, a vector of numbers,
a vector of categorical values, etc.). To address this problem, we
define a search space S of possible events to look at. The search
space depends on the type of the output ω of M, which can be
determined by running M(D1) and M(D2) multiple times.
(1) The output ω is a fixed length list of categorical values.
We first run M(D1) once and ask it to not use any noise (i.e. tell it
to satisfy ϵ-differential privacy with ϵ = ∞). Denote this output
as ω0. Now, when M runs with its preferred privacy settings to
produce an output ω, we define t(ω) be the Hamming distance
between the output ω and ω0. The search space is
S = {{ω | t(ω) = k} : k = 0, 1, . . . , l}
where l is the fixed length of output of M. Another set of events
relate to the count of a categorical value in the output. If there
are m values, then define
Si = {{ω | ω.count(valuei) = k} : k = 0, 1, . . . , l},
1 ≤ i ≤ m. The overall search space is the union of S and all Si.
(2) The output ω is a variable length list of categorical val-
ues. In this case, one extra set of events E we look at corre-
spond to the length of the output. For example, we may check
if P(M(D1) has length k) > P(M(D2) has length k). Hence, we
define
S0 = {{ω | ω.lenдth = k} : k = 0, 1, . . .}
Detecting Violations of Differential Privacy
CCS ’18, October 15–19, 2018, Toronto, ON, Canada
(3) The output ω is a fixed length list of numeric values.
For the search space S, we use this S0 unioned with the search
space from the previous case.
In this case, the output is of the form ω = (a1, . . . , am). Our
search space is the union of the following:
{{ω | ω[i] ∈ (a, b)} : i = 1, . . . , m and a 
P(avд(M(D2)) ∈ (a, b)), etc.. To save time, we often restrict a
and b to be multiples of a small number like ±0.2, or ±∞. In the
case that the output ω is always an integer array, we replace
the condition “∈ (a, b)” with “= k” for each integer k.
(4) M outputs a variable length list of numeric values.
The search space is the union of Case 3 and S0 in Case 2.
(5) M outputs a variable length list of mixed categorical and
numeric values. In this case, we separate out the categorical
values from numeric values and use the cross product of the
search spaces for numeric and categorical values. For instance,
events would be of the form “ω has k categorical components
equal to ℓ and the average of the numerical components of ω is
in (a, b)”
The EventSelector is designed to return one event E for use in
the hypothesis test in Algorithm 1. The way EventSelector works is
it receives an InputList, which is a set of tuples (D1, D2, arдs) where
D1, D2 are adjacent databases and args is a set of values for any
other parameters M needs. For each such tuple, it runs M(D1) and
M(D2) for n times each. Then for each possible event in the search
space, it runs the hypothesis test (as an exploratory tool) to get a
p-value. The combination of (D1, D2, arдs) and E that produces the
lowest p-value is then returned to Algortihm 1. Algorithm 1 uses
those choices to run the real hypothesis test on fresh executions of
M on D1 and D2.
The pseudocode for the EventSelector is shown in Algorithm 3.3
4.4 Input Generation
In this section we discuss our approaches for generating candidate
tuples (D1, D2, arдs) where D1, D2 are adjacent databases and args
is a set of auxiliary parameters that a mechanism M may need.
4.4.1 Database Generation . To find the adjacent databases that are
likely to form the basis of counterexamples that illustrate violations
of differential privacy, we adopt a simple and generic approach that
works surprisingly well. Recalling that the inputs to mechanisms
are best modeled as a vector of query answers, we use the type of
patterns shown in Table 1.
The “One Above” and “One Below” categories are suitable for
algorithms whose input is a histogram (i.e. in adjacent databases,
at most one query can change, and it will change by at most 1). The
rest of the categories are suitable when in adjacent databases every
3In practice, to avoid choosing bad E, we let cE be the total number of times M(D1)
and/or M(D2) produced an output in E. Then it only executes Line 11-14 in Algorithm
3 if cE ≥ 0.001 · n · e ϵ , otherwise the selection of E is too noisy.
3
Algorithm 4: Input Generator.
1 function ArgumentGenerator(M, D1, D2):
2
4
5
6 function InputGenerator(M, len):
arдs0 ← Arguments used in noise generation with values
that minimize the noises
constraints ← Traverse the source code of M and generate
constraints to force D1 and D2 to diverge on branches
arдs1 ← MaxSMT(constraints)
return arдs0 + arдs1
input: M: mechanism
candidates ← Empirical pairs of databases of length len
InputList ← [ ]
foreach (D1, D2) ∈ candidates do
len: length of input to generate
7
8
9
10
11
12
13
arдs ← ArgumentGenerator(M, D1, D2)
InputList.append(D1, D2, arдs)
end
return InputList
Table 1: Database categories and samples
Category
One Above
One Below
Sample D1
[1, 1, 1, 1, 1]
[1, 1, 1, 1, 1]
One Above Rest Below [1, 1, 1, 1, 1]
[1, 1, 1, 1, 1]
One Below Rest Above
[1, 1, 1, 1, 1]
All Above & All Below [1, 1, 1, 1, 1]
[1, 1, 0, 0, 0]
Half Half
X Shape
Sample D2
[2, 1, 1, 1, 1]
[0, 1, 1, 1, 1]
[2, 0, 0, 0, 0]
[0, 2, 2, 2, 2]
[0, 0, 0, 2, 2]
[2, 2, 2, 2, 2]
[0, 0, 1, 1, 1]
query can change by at most one (i.e. the queries have sensitivity4
∆q = 1).
The design of the categories is based on the wide variety of
changes in query answers that are possible when evaluated on one
database and on an adjacent database. For example, it could be the
case that a few of the queries increase (by 1, if their sensitivity
is 1, or by ∆q in the general case) but most of them decrease. A
simple representative of this situation is “One Above Rest Below”
in which one query increases and the rest decrease. The category
“One Below Rest Above” is the reverse.
Another situation is where roughly half of the queries increase
and half decrease (when evaluated on a database compared to when
evaluated on an adjacent database). This scenario is captured by the
“Half Half” category. Another situation is where all of the queries