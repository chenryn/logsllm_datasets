Figure 5: Eﬀects of diﬀerent inertia on representative churn
(HL28/16 modiﬁed three times by it29w through it31w;
modiﬁed twice by it29w, it30w; and once by only it29w).
never responsive blocks
not recently responsive blocks
predicted blocks
)
s
n
o
i
l
l
i
m
(
t
s
i
l
t
i
h
n
i
s
k
c
o
b
l
4
2
/
f
o
r
e
b
m
u
n
  14
  12
  10
  8
  6
  4
  2
  0
HL28w/16
HL29w/16
HL30w/16
HL31w/16
HL32w/16
hitlist datasets
Figure 4: Relative size of hitlist components.
a block as the representative, independent of the represen-
tative in a hitlist based on a prior censuses. As inertia ap-
proaches 1, we will never switch representatives once chosen.
For our production hitlists, we use I = 0.34 based on score
changes due to weighting (Section 4.1.1) and the following
analysis.
Inertia on churn: We ﬁrst consider how much inertia
aﬀects churn. Churn is that rate at which we switch repre-
sentatives for established blocks. Table 6 shows the amount
of churn for four hitlists when using our standard inertia
I = 0.34. Churn is shown in the “changed representatives”
row, so in the HL32/16 column, we see that about 7% of
all predictions (306,588 representatives) changed relative to
the prior hitlist (HL31/16)). This Table 6 shows that the
rate of churn is relatively stable over time, with 5–7% of all
informed predictions changing each census.
While Table 6 shows churn over time for a ﬁxed inertia,
in Figure 5 we vary inertia to observe its eﬀect on churn. To
estimate the relationship shown in this ﬁgure, we generate
HL28/16, then modify it three times with censuses it29w,
it30w, and it31w with diﬀerent levels of inertia. (Here we
suspend gone-dark processing to focus only on inertia.) We
then evaluate the hitlist against observations from census
it32w. We evaluate inertia over several steps for two reasons.
First, hitlist staleness is partially a function of time. Second,
large values of inertia suppress changes in single or a few
censuses.
As expected, Figure 5 shows that higher inertia suppresses
churn, because it takes several new negative responses for a
representative’s score to change.
In fact, weight selection
means score can change only by 0.3 from one new census,
418)
n
o
i
t
c
a
r
f
(
s
s
e
n
e
v
s
n
o
p
s
e
r
i
n
o
i
t
i
a
c
d
e
r
p
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
HL29w/16
HL30w/16
HL31w/16
0
0.2
0.4
0.6
0.8
Inertia
Figure 6: Eﬀects of diﬀerent inertia on responsiveness
(HL28/16; modiﬁed once by it29w, then tested against
it30w; modiﬁed twice by it29w, and it30w, then test against
it31w; then modiﬁed three times by it29w through it31w
and tested against it32w).
and decrease to 0.5 from two new censuses since the weight
decrease in our pow weighting, so with three new observa-
tions here, an inertia of 0.2 has one observations that might
cause change, while I = 0.4 has two; I = 0.6, three; and
I = 0.8 requires more than eight observations to change.
Inertia on responsiveness: Inertia is selected to keep
hitlists stable, reducing the amount of arbitrary represen-
tative turnover in long-running experiments. Such turnover
can be eliminated by simply never changing representatives
(setting I = 1), but prior experience shows that the respon-
siveness of a static hitlist will degrade over time as servers
move, losing as much as 2–3% per month for the early Skitter
web-server-based list [19]. We would therefore like to know
the trade-oﬀ between inertia and representative responsive-
ness.
Figure 6 shows hitlist responsiveness for diﬀerent values of
inertia after this process. (This analysis was generated with
the same multi-step process as Figure 5 described above.)
We see that responsiveness degrades slightly for high inertia
values, from 59% responsiveness with no inertia, to a low of
53% responsiveness when I = 0.8, when there are eﬀectively
no changes. We conclude that a moderate inertia has little
eﬀect on responsiveness costing at most 6 percentage points,
even over eight months.
4.4 Effects of Probe Frequency
Hitlists are based on periodic censuses, so an important
operational question is how the frequency of such censuses
aﬀect the quality of the hitlists. Intuitively, more frequent
sampling can provide more information on address respon-
siveness, resulting in better predictions. Our current cen-
suses are taken every two to three months (Table 1). More
more frequent collection is possible, but should be justiﬁed
because data collection entails some operational cost.
While Internet censuses cover the whole Internet very few
months, we also have access to surveys that probe 1% of
the Internet every 11 minutes for two weeks [18]. Here we
turn to these much more frequent probes to evaluate the
eﬀects of diﬀerent probe rates. A survey has about 1800
Responsive
Probe Frequency
Representatives
Survey, 12 hours
24 hours
48 hours
HL30/16 (3-months)
15,484 (68%)
15,362 (67%)
15,197 (66%)
16,123 (70%)
total blocks analyzed
22,861 (100%)
Table 7: Prediction accuracy (in responsiveness) from dif-
ferent probe frequencies, for a given number of /24 blocks.
observations, but we downsample this information to get
observations every 12, 24, or 48 hours, providing 28, 14, and
7 bits of history. We use survey 30 (taken at the same time
as it30w in Dec., 2009) to do prediction, and test it against
it31w. We use the average weighting function to evaluate
these observations, since it seems unnecessary to favor re-
cent observations when all are taken of a short, two-week
period. In addition, in Section 4.1.1 we showed that weight-
ing has relatively little eﬀect on responsiveness. We then
test the prediction from this history against the next census
(it31w) to evaluate prediction accuracy. We compare the
representatives found from the survey with those computed
in HL30/16.
Table 7 compares responsiveness as a function of probe
frequency. First, we see that, of the survey-derived hitlists,
more frequent probing provides a slightly better prediction
(68% responsive from 12-hour samples vs. 66% for 48 hours).
This small improvement is because more frequent probing
gives more information on host responsiveness. Second, we
see that the census-derived hitlist is a better prediction than
any of the survey-derived hitlists, by 2–4%. The main source
of this diﬀerence is that the survey’s hitlist ﬁnds some ad-
dresses that don’t appear in the census-derived hitlist. These
addresses seems less stable in the next census because the
survey-derived hitlist considers only a short period of his-
tory, while the census-based hitlist considers many months
history and so ﬁnds long-term stable addresses, if they ex-
ist. In addition, because our survey observations are taken
at speciﬁc times of day, they may discover addresses that
are only up during particular daily hours, while evaluating
against the next census tests at random times of day.
4.5 Effects on Other Research
The above sections evaluate hitlists based on our goals:
responsiveness, completeness, and stability. But hitlists are
a tool to enable other research, so their ultimate beneﬁts
come by how they improve the quality of other network per-
formance and topology studies.
Some network performance studies require responsiveness
in their destinations. These studies include those that evalu-
ate performance [32, 20, 23], consider questions about rout-
ing and reachability [31, 3], or the performance of replica
placement (examples include [32, 10]). For studies that
require end-to-end latency measurements, our representa-
tive selection methods optimize reachability within the con-
straints of sparse measurement. Our work also suggests
directions for potential improvements: more frequent mea-
surement could potentially better track reachable addresses
419in dynamically assigned blocks. In addition, our approach to
stability assists evaluation of long-term performance trends.
Responsiveness is helpful but not essential for many other
topology studies (such as [17, 15, 26, 20, 22, 8]). Most
topology studies employ traceroutes to study paths across
the Internet. A traceroute attempts to discover all IP ad-
dresses on the path towards a destination, and many such
paths are aggregated with alias elimination [17, 26, 21] to
produce a router-level map. However, in such studies, the
presence or absence of the destination itself aﬀects only
the last hop. Topology studies thus do not require respon-
sive representatives, but they may beneﬁt from responsive-
ness. However, many have moved towards the use random
or deterministically selected representatives. As one exam-
ple, topology probing in Skitter [20] began with a manu-
ally generated hitlist, but later shifted to random probing
in Archipelago [8]. Two reasons for this shift were diﬃculty
in maintaining a responsive hitlist and the recognition that
responsive targets are not essential.
Although responsiveness is not essential for topology stud-
ies that focus on the core of the Internet, it is important for
studies that wish to explore the edge of the network. We can
get a rough estimate on the number of edge links that are
missed by randomly selected representatives: empirically,
about 4–7% of the Internet responds to ICMP probes [18],
so we expect that 93% of random representatives will not
respond (approximating the distribution of responders as
uniform, to get a rough estimate). If 55% of our hitlists re-
spond, that will improve edge detection for 38% of blocks.
With 1.3 million allocated /24 blocks (as of Nov. 2009),
statistics suggest that responsive hitlists will detect about
630,000 additional links compared to those found using a
random hitlist. Some topology studies examining the core
of the Internet ﬁnd about 33M links, so this increases the
size of the discovered Internet topology by 2% [6]. This
simple analysis ignores correlation in the data, so it is only
approximate.
To conﬁrm this simple analysis, we consider CAIDA’s In-
ternet Topology Data Kit (ITDK-2010-01 [6]). ITDK com-
bines the results of traceroutes from many traceroutes with
alias resolution [17, 8] to produce a router-level map of the
Internet. ITDK is formed from 42 cycles of data, each rep-
resenting a traceroute to a randomly chosen representative
in each /24 preﬁx.
Unfortunately, we cannot directly evaluate ITDK with dif-
ferent approaches to representative selection because it is the
result of several levels of processing (aggregation of multi-
ple cycles of data, and duplicate and alias elimination). In
addition, direct comparison would be somewhat misleading
since the goal of ITDK is to map the Internet’s core, not
edges. Instead, we obtained the raw IPv4 topology data [7]
used to generate ITDK. We then compare the results of one
cycle with random representatives, to using what we expect
would have resulted using our HL29/16 (based on censuses
from 2007 through Dec. 2009). We can evaluate the respon-
siveness of randomly selected representatives by looking at
the raw data, while we test hitlist responsiveness by com-
paring to it30w completed in Jan. 2010. (Here report these
results comparing to a single cycle, #739, of the raw IPv4
data. We got very similar results comparing to cycles 740
and 741.)
Table 8 summarizes our observations. First, we see that
we only have data to make predictions for just under half
of the blocks (Np of 4,008,861). For the remaining 51%,
we too fall back on random probing. (We actually predict
about 100k additional preﬁxes because we use a more recent
routing table compared to the IPv4 raw data, however, we
omit these from the table to provide a more fair comparison.)
Second, we see that random probing ﬁnds responsive ad-
dresses about 9% of the time. This result is somewhat better
than we predict (4-7% [18]), a diﬀerence probably because
our prediction is over all allocated blocks, but here we study
only routable blocks, and non-routable blocks cannot possi-
bly predict representatives.
Finally, we ﬁnd about 1.7 million additional edges if we
use our hitlist compared to random probing, about 2.4×
more edge links than random probing alone ﬁnds. For stud-
ies of edge links, this is a signiﬁcant amount of additional
coverage. Compared to the 33M links found in ITDK-2010-
01, this use of informed probing would discover about 5%
greater coverage. However, care must be taken in making
this comparison for several reasons. First, ITDK is com-
posed of 42 cycles, while we compare to only a single cycle.
Because each cycle probes a diﬀerent random destination,
total coverage will improve as it eventually ﬁnds responsive
destinations by chance. (In fact, with enough cycles, ran-
dom probing may provide better coverage as it converges on
probing all possible addresses.) Second, it is important to
note that ITDK is designed to study the Internet core, not
the edges, and this goal is well met with random probing
alone. However, our results show they could get additional
coverage with no additional probing eﬀort by changing to
representatives selected with a method such as ours.
4.6 Cost of Hitlist Generation
Finally, turning from hitlist quality, we consider the cost
of generating hitlists. Hitlists are generated from a number
of Internet censuses. We therefore review the cost of taking
a census and then look at the cost of processing censuses to
produce a hitlist.
Hitlists require Internet censuses as input. New censuses
are started every two to three months; during most of that
time four machines are actively probing the Internet, while
there is a week or two of setup and data curation time at the
beginning and end of a census [18]. Each census currently
requires 12–15GB of archival storage Censuses are carried
out as part of an ongoing Internet topology research project
and are used for multiple purposes, but if carried out exclu-
sively for hitlist generation they would represent an ongoing
cost.
Processing hitlist from census data incurs several ongoing
costs. First, we maintain a master history ﬁle with bitmaps
of all censuses to date, indexed by IP address. Each census
creates an observation ﬁle with about 1–2GB of new positive
observations. We merge observations into an ongoing history
ﬁle of all observations to date. Currently 22 observation ﬁles
(32GB total) are merged into one 7.5GB history ﬁle, and the
size of that ﬁle grows by about 300MB every new census.
We parallelize our computation with the Hadoop imple-
mentation [11] of map/reduce [12], running over a cluster of
about 40 computers with about 120 CPU cores. With this
parallelism, join in a new census into an existing history
takes about half an hour, and evaluation of a new hitlist
takes about another half hour. (our code is written in Perl
and not optimized for speed). Our map function groups
420/24 routable blocks studied
8,248,027
100%
no prediction (Np)
prediction (Np)
responsive
random probing responsive
random probing responsive also in Np
extra edges found
4,239,166
4,008,861
2,454,500
730,496
725,930