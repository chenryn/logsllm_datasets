---
title: "Azure Data Explorer POC playbook: Big data analytics"
description: "A high-level methodology for preparing and running an effective Azure Data Explorer proof of concept (POC) project."
ms.reviewer: devsha
ms.topic: conceptual
ms.date: 11/02/2023
---
# Azure Data Explorer POC playbook: Big data analytics
This article presents a high-level methodology for preparing and running an effective Azure Data Explorer proof of concept (POC) project.
## Before you begin
The playbook helps you to evaluate the use of Azure Data Explorer and is designed for scenarios that are most suitable for Azure Data Explore. Use the following scenarios to determine if Azure Data Explorer is the right solution for you before you start your POC.
### General architecture patterns scenarios
- Any data that fits one or more of the following characteristics should be a good candidate for Azure Data Explorer:
    - High volume and speed
    - Append only and immutable data
    - Data Quiescence: Data that doesn't change. For example, an order placed in an online store is a good example of quiesced data.
- [Command Query Responsibility Segregation (CQRS) pattern](/azure/architecture/patterns/cqrs) is suited to Azure Data Explorer's append only architecture.
- [Event sourcing pattern](/azure/architecture/patterns/event-sourcing)
### Other scenarios
The following scenarios are also good candidates for Azure Data Explorer:
- Low latency data store for real-time telemetry-based alerts
- [IoT telemetry data storage and analytics](/azure/architecture/solution-ideas/articles/iot-azure-data-explorer)
- [High speed interactive analytics layer](/azure/architecture/solution-ideas/articles/interactive-azure-data-explorer). Particularly when used with Apache Spark engines such as Synapse Spark, DataBricks, or traditional data warehouses such as Synapse SQL pools.
- [Log and observability analytics](/azure/architecture/solution-ideas/articles/monitor-azure-data-explorer)
## Prepare for the POC
A POC project can help you make an informed business decision about implementing a big data and advanced analytics environment on a cloud-based platform that uses Azure Data Explorer.
A POC project will identify your key goals and business drivers that cloud-based big data and advanced analytics platform must support. It will test key metrics and prove key behaviors that are critical to the success of your data engineering, machine learning model building, and training requirements. A POC isn't designed to be deployed to a production environment. Rather, it's a short-term project that focuses on key questions, and its result can be discarded.
Before you begin planning your Azure Data Explorer POC project:
> [!div class="checklist"]
>
> - Identify any restrictions or guidelines your organization has about moving data to the cloud.
> - Identify executive or business sponsors for a big data and advanced analytics platform project. Secure their support for migration to the cloud.
> - Identify availability of technical experts and business users to support you during the POC execution.
Before you start preparing for the POC project, we recommend you first read the [Azure Data Explorer documentation](index.yml).
By now you should have determined that there are no immediate blockers and then you can start preparing for your POC. If you are new to Azure Data Explorer, you can refer to [this documentation](data-explorer-overview.md) where you can get an overview of the Azure Data Explorer architecture.
Develop an understanding of these key concepts:
- Azure Data Explorer and its architecture.
- Support data formats and data sources.
- Cluster, databases, tables, materialized views, functions as Azure Data Explorer artifacts.
- Supported ingestion methods for ingestion wizard and continuous ingestion.
- Authentication and authorization in Azure Data Explorer.
- Native connectors that integrate with visualization solutions such as Power BI, Grafana, Kibana, and more.
- Creating external tables to read data from Azure SQL/SQL Server, Azure Cosmos DB, Azure Monitor, Azure Digital Twin.
Azure Data Explorer decouples compute resources from storage so that you can better manage your data processing needs and control costs. You only pay for compute when it's in use. When it's not in use, you only pay for storage. The managed services architecture of Azure Data Explorer allows you to scale your cluster independently of your storage. You can scale up and down ([vertical](manage-cluster-vertical-scaling.md)), as well as scale in and out ([horizontal](manage-cluster-horizontal-scaling.md)). You can also manually stop, or [autostop](auto-stop-clusters.md), your cluster without losing your data. For example, you can scale up your cluster for heavy data processing needs or large loads, and then scale it back down during less intense processing times, or shut it down completely. Similarly, you can effectively scale and stop a cluster during the weekends to reduce costs.
### Set the goals
A successful POC project requires planning. Start by identify why you're doing a POC to fully understand the real motivations. Motivations could include modernization, cost saving, performance improvement, or integrated experience. Be sure to document clear goals for your POC and the criteria that will define its success. Ask yourself:
> [!div class="checklist"]
>
> - What do you want as the outputs of your POC?
> - What will you do with those outputs?
> - Who will use the outputs?
> - What will define a successful POC?
Keep in mind that a POC should be a short and focused effort to quickly prove a limited set of concepts and capabilities. These concepts and capabilities should be representative of the overall workload. If you have a long list of items to prove, you may want to plan more than one POC. In that case, define gates between the POCs to determine whether you need to continue with the next one. For example, one POC could focus on requirements for the data engineering role, such as ingestion and processing. Another POC could focus on machine learning (ML) model development.
As you consider your POC goals, ask yourself the following questions to help you shape the goals:
> [!div class="checklist"]
>
> - Are you migrating from an existing big data and advanced analytics platform (on-premises or cloud)?
> - Are you migrating and want to do some extensive improvements along the way? For example, migrating from Elastic Search to Azure Data Explorer for log analysis, migrating from InfluxDB or Timescale DB to Azure Data Explorer.
> - Are you building an entirely new big data and advanced analytics platform (greenfield project)?
> - What are your current pain points? For example, scalability, performance, or flexibility.
> - What new business requirements do you need to support?
> - What are the SLAs that you're required to meet?
> - What will the workloads be? For example, ETL, batch processing, stream processing, machine learning model training, analytics, reporting queries, or interactive queries?
> - What are the skills of the users who will own the project (should the POC be implemented)? For example, SQL, Python, PowerBI, or other skills.
Here are some examples of POC goal setting:
- Why are we doing a POC?
    - We need to know that the data ingestion and processing performance for our big data workload will meet our new SLAs.
    - We need to know whether near real-time stream processing is possible and how much throughput it can support. (Will it support our business requirements?)
    - We need to know if our existing data ingestion and transformation processes are a good fit and where improvements will need to be made.
    - We need to know if we can shorten our data integration run times and by how much.
    - We need to know if our data scientists can build and train machine learning models and use AI/ML libraries as needed in Azure Data Explorer.
    - Will the move to cloud-based Azure Data Explorer meet our cost goals?
- At the conclusion of this POC:
    - We'll have the data to determine if our data processing performance requirements can be met for both batch and real-time streaming.
    - We'll have tested ingestion and processing of all our different data types (structured, semi-structured, and unstructured) that support our use cases.
    - We'll have tested some of our existing data processing needs and can identify the work that can be completed with update policies in Azure Data Explorer.
    - We'll have tested data ingestion and processing and will have the data points to estimate the effort required for the initial migration and load of historical data.
    - We'll have tested data ingestion and processing and can determine if our ETL/ELT processing requirements can be met.
    - We'll have gained insight to better estimate the effort required to complete the implementation project.
    - We'll have tested scale and scaling options and will have the data points to better configure our platform for better price-performance settings.
    - We'll have a list of items that may need more testing.
### Plan the project
Use your goals to identify specific tests and to provide the outputs you identified. It's important to make sure that you have at least one test to support each goal and expected output. Also, identify specific data ingestion, batch or stream processing, and all other processes that will be executed so you can identify a specific dataset and codebase. This specific dataset and codebase will define the scope of the POC.
Here are the typical subject areas that are evaluated with Azure Data Explorer:
- **Data Ingestion and processing**: Data sources, data formats, ingestion methods, connectors, tools, ingestion policies, streaming vs queued ingestion
- **Data Storage**: schema, storage artifacts such as tables and materialized views
- **Policies**: Such as partitioning, update, merge
- **Querying and visualization**
- **Performance**: Such as query response times, ingestion latencies, weak consistency, query cache results
- **Cost**: Total Cost of Ownership (TCO)
- **Security**: Such as authentication, authorization, data access, row level security
> [!NOTE]
> Use the following frequently asked questions to help you plan your POC.
>
> - **How do I choose the SKU for my POC cluster?**  
>     Use the [Select a SKU for your Azure Data Explorer cluster](manage-cluster-choose-sku.md) guide to help you choose the SKU for your POC cluster. When starting a POC, we recommend starting with a smaller SKUs and scale up SKU as required when you begin testing and capturing results.
> - **How do I choose the caching period when creating my POC cluster?**  
>     To provide best query performance, ingested data is cached on the local SSD disk. This level of performance is not always required and less frequently queried data can often be stored on cheaper blob storage. Queries on data in blob storage run slower, but this acceptable in many scenarios. Knowing this can help you identify the number of compute nodes you need to hold your data in local SSD and continue to meet your query performance requirements. For example, if you you want to query *x* days worth of data (based on ingestion age) more frequently and retain data for *y* days and query it less frequently, in your cache retention policy, specify *x* as the value for hot cache retention and *y* as the value for the total retention. For more information, see [Cache policy](kusto/management/cache-policy.md).
> - **How do I choose the retention period when creating my POC cluster?**  
>     The retention period is a combination of hot and cold cache data that is available for querying. You choose data retention based on how long you need to retain the data based on compliance or other regulatory requirements. You can use the hot window capability, to warm data stored in the cold cache, for faster queries for any auditing purpose. For more information, see [Query cold data with hot windows](hot-windows.md).
Here's an example of the needed level of specificity in planning:
- **Goal A**: We need to know whether our requirement for data ingestion and processing of batch data can be met under our defined SLA.
- **Output A**: We'll have the data to determine whether our queued data ingestion and processing can meet the batch data processing requirement and SLA.
    - **Test A1**: Processing queries A, B, and C are identified as good performance tests as they're commonly executed by the data engineering team. Also, they represent overall data processing needs.
    - **Test A2**: Processing queries X, Y, and Z are identified as good performance tests as they contain near real-time stream processing requirements. Also, they represent overall event-based stream processing needs.
    - **Test A3**: Compare the performance of these queries at different scale of our cluster (cluster SKU, number of instances) with the benchmark obtained from the existing system.
- **Goal B**: We need to know if our business users can build their dashboards on this platform.
- **Output B**: We'll have tested some of our existing dashboards and visuals on data in our cluster, using different visualization options, connectors and Kusto queries. These tests will help to determine which dashboards can be migrated to the new environment.
    - **Test B1**: Specific visuals will be created with Azure Data Explorer data and will be tested.
    - **Test B2**: Test out of the box KQL functions and operators to meet the requirement.
- **Goal C**: We'll have tested data ingestion and will have the data points to:
    - Estimate the effort for our initial historical data migration to our Azure Data Explorer cluster.
    - Plan an approach to migrate historical data.
- **Output C**: We'll have tested and determined the data ingestion rate achievable in our environment and can determine whether our data ingestion rate is sufficient to migrate historical data during the available time window.
    - **Test C1**: Test different approaches of historical data migration.