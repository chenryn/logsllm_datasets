title:Reverse Engineering Intel Last-Level Cache Complex Addressing Using
Performance Counters
author:Cl&apos;ementine Maurice and
Nicolas Le Scouarnec and
Christoph Neumann and
Olivier Heen and
Aur&apos;elien Francillon
Reverse Engineering Intel Last-Level Cache
Complex Addressing Using Performance
Counters
Cl´ementine Maurice1,2(B), Nicolas Le Scouarnec1, Christoph Neumann1,
Olivier Heen1, and Aur´elien Francillon2
1 Technicolor, Rennes, France
2 Eurecom, Sophia Antipolis, France
PI:EMAIL
Abstract. Cache attacks, which exploit diﬀerences in timing to perform
covert or side channels, are now well understood. Recent works leverage
the last level cache to perform cache attacks across cores. This cache is
split in slices, with one slice per core. While predicting the slices used by
an address is simple in older processors, recent processors are using an
undocumented technique called complex addressing. This renders some
attacks more diﬃcult and makes other attacks impossible, because of the
loss of precision in the prediction of cache collisions.
In this paper, we build an automatic and generic method for reverse
engineering Intel’s last-level cache complex addressing, consequently ren-
dering the class of cache attacks highly practical. Our method relies on
CPU hardware performance counters to determine the cache slice an
address is mapped to. We show that our method gives a more precise
description of the complex addressing function than previous work. We
validated our method by reversing the complex addressing functions on
a diverse set of Intel processors. This set encompasses Sandy Bridge, Ivy
Bridge and Haswell micro-architectures, with diﬀerent number of cores,
for mobile and server ranges of processors. We show the correctness of
our function by building a covert channel. Finally, we discuss how other
attacks beneﬁt from knowing the complex addressing of a cache, such as
sandboxed rowhammer.
Keywords: Complex addressing · Covert channel · Cross-Core · Last
level cache · Reverse engineering · Side channel
1 Introduction
In modern x86 micro-architectures, the cache is an element that is shared by
cores of the same processor. It is thus a piece of hardware of choice for per-
forming attacks. Cache attacks like covert and side channels can be performed
in virtualized environments [16,22,27,33,35–37], breaching the hypervisor isola-
tion at the hardware level. Caches are also exploited in other types of attacks,
c(cid:2) Springer International Publishing Switzerland 2015
H. Bos et al. (Eds.): RAID 2015, LNCS 9404, pp. 48–65, 2015.
DOI: 10.1007/978-3-319-26362-5 3
Reverse Engineering Intel Last-Level Cache Complex Addressing
49
such as bypassing kernel ASLR [8], or detecting cryptographic libraries in virtu-
alized environments [17].
Cache attacks are based on diﬀerence of timings: the access to a cached
memory line is fast, while the access to a previously evicted cache line is slow.
Cache attacks can operate at all cache levels: level 1 (L1), level 2 (L2) and Last
Level Cache (LLC). Attacks on the L1 or L2 cache restrict the attacker to be
on the same core as the victim. This is a too strong assumption on a multi-
core processor when the attacker and the victim migrate across cores [27,35].
We thus focus on cache attacks on the last level cache, which is shared among
cores in modern processors. Attacks on the last level cache are more powerful
as the attacker and the victim can run on diﬀerent cores, but they are also
more challenging. To perform these attacks, the attacker has to target speciﬁc
sets in the last level cache. He faces two issues: the last level cache is physically
addressed, and modern processors map an address to a slice using the so-called
complex addressing scheme which is undocumented.
A ﬁrst set of attacks requires shared memory and evicts a speciﬁc line
using the clflush instruction [7,16,36,38]. However, a simple countermeasure
to thwart such side channels is to disable memory sharing across VMs, which is
already done by most cloud providers.
Without using any shared memory, an attacker has to ﬁnd addresses that
map to the same set, and exploit the cache replacement policy to evict lines.
On processors that do not use complex addressing, huge pages are suﬃcient
to enable side channels by targeting a precise set [14]. On recent processors
that use complex addressing, this diﬃculty can be bypassed by evicting the
whole LLC [22], but the temporal resolution makes it impossible to perform
side channels. Liu et al. [20] and Oren et al. [24] construct eviction sets by
seeking conﬂicting addresses, enabling ﬁne-grained covert and side channels. This
works without reverse engineering the complex addressing function, but has to
be performed for each attack.
Hund et al. [8] manually and, as we show, only partially reverse engineered the
complex addressing function to defeat kernel ASLR on a Sandy Bridge processor.
The challenge in reversing the complex addressing function is to retrieve all the
bits. Indeed, previous approaches rely on timing attacks with conﬂicting cache
sets. As the set bits are ﬁxed, they cannot be retrieved this way. Previous work
was also incomplete because the function diﬀers for processors with diﬀerent
numbers of cores, as we will show.
Reversing this addressing function also gains momentum [29] in discussions
about the exploitation of the so-called rowhammer vulnerability. Indeed, rowham-
mer can cause random bit ﬂips in DRAM chips by accessing speciﬁc memory
locations repeatedly [19]. The exploitation of this vulnerability uses the clflush
instruction [28]. This instruction has been disabled [3] in the Native Client sand-
box [2] due to this security issue. Reversing the addressing function could lead to
new ways to exploit rowhammer without relying on the clflush instruction.
In this paper, we automate reverse engineering of the complex cache address-
ing in order to make these attacks more practical. In contrast to previous work
50
C. Maurice et al.
that reverse engineered the function manually, we develop a fully automatic app-
roach to resolve the complex addressing of last level cache slices. Our technique
relies on performance counters to measure the number of accesses to a slice and
to determine on which slice a memory access is cached. As a result, we obtain
a translation table that allows determining the slice used by a given physical
address (Sect. 3). In the general case, ﬁnding a compact function from the map-
ping is NP-hard. Nevertheless, we show an eﬃcient algorithm to ﬁnd a compact
solution for a majority of processors (which have 2n cores). As a result, we pro-
vide new insights on the behavior of the last level cache, and reﬁne many previous
works (e.g., Hund et al. [8]). In particular, we obtain a more complete and more
precise description than previous work, i.e., taking into account more bits of the
memory address and ﬁxing the partially incorrect functions of prior work. We
evaluate our method on processors of diﬀerent micro-architectures with various
numbers of cores (Sect. 4). We demonstrate the correctness of our function by
building a prime+probe covert channel (Sect. 5). Finally, we discuss the diﬀer-
ence between our ﬁndings and the previous attempts of reverse engineering this
function, as well as other applications (Sect. 6).
Contributions
In summary, this paper presents the following main contributions:
1. We introduce a generic method for mapping physical addresses to last level
cache slices, using hardware performance counters.
2. We provide a compact function for most processor models (with 2n cores).
3. We validate our approach on a wide range of modern processors.
4. We show, and discuss, practical examples of the beneﬁts to cache attacks.
2 Background
In this section, we give details on cache internals for Intel processors post Sandy
Bridge micro-architecture (2011). We then review attacks that exploit cache
interferences to perform covert and side channels. Finally, we provide background
on hardware performance counters.
2.1 Cache Fundamentals
The processor stores recently-used data in a hierarchy of caches to reduce the
memory access time by the processor (see Fig. 1). The ﬁrst two levels L1 and
L2 are usually small and private to each core. The L3 is also called Last Level
Cache (LLC). It is shared among cores and can store several megabytes. The
LLC is inclusive, which means it is a superset of the lower levels.
Caches are organized in 64-byte long blocks called lines. The caches are n-
way associative, which means that a line is loaded in a speciﬁc set depending on
its address, and occupies any of the n lines. When all lines are used in a set, the
Reverse Engineering Intel Last-Level Cache Complex Addressing
51
core 0
core 1
core 2
core 3
L1
L2
L1
L2
L1
L2
L1
L2
ring bus
LLC
slice 0
LLC
slice 1
LLC
slice 2
LLC
slice 3
Fig. 1. Cache architecture of a quad-core Intel processor (since Sandy Bridge micro-
architecture). The LLC is divided into slices, and interconnected with each core by a
ring bus.
replacement policy decides the line to be evicted to make room for storing a new
cache line. Eﬃcient replacement policies favor lines that are the least likely to
be reused. Such policies are usually variations of Least Recently Used (LRU).
The ﬁrst level of cache is indexed by virtual addresses, and the two other
levels are indexed by physical addresses. With caches that implement a direct
addressing scheme, memory addresses can be decomposed in three parts: the
tag, the set and the oﬀset in the line. The lowest log2(line size) bits determine
the oﬀset in the line. The next log2(number of sets) bits determine the set. The
remaining bits form the tag.
The LLC is divided into as many slices as cores, interconnected by a ring bus.
The slices contain sets like the other levels. An undocumented hashing algorithm
determines the slice associated to an address in order to distribute traﬃc evenly
among the slices and reduce congestion. In contrast to direct addressing, it is
a complex addressing scheme. Potentially all address bits are used to determine
the slice, excluding the lowest log2(line size) bits that determine the oﬀset in
a line. Contrary to the slices, the sets are directly addressed. Figure 2 gives a
schematic description of the addressing of slices and sets.
2.2 Cache Attacks
System memory protection prevents a process from directly reading or writing
in the cache memory of another process. However, cache hits are faster than
cache misses. Thus by monitoring its own activity, i.e., the variation of its own
cache access delays, a process can determine the cache sets accessed by other
processes, and subsequently leak information. This class of cache attacks is called
access-driven attacks.
In a prime+probe attack [23,25,26,30], the attacker ﬁlls the cache, then waits
for the victim to evict some cache sets. The attacker reads data again and deter-
mines which sets were evicted. The access to these sets will be slower for the
attacker because they need to be reloaded in the cache.
52
C. Maurice et al.
physical address
tag
63
17
set
6
0
offset
30
H
2
11
...
slice 0
slice 1
slice 2
slice 3
Fig. 2. Complex addressing scheme in the LLC with 64B cache lines, 4 slices and 2048
sets per slice. The slice is given by a hash function that takes as an input all the bits of
the set and the tag. The set is then directly addressed. The dark gray cell corresponds
to one cache line.
The challenge for this type of ﬁne-grained attack is the ability to target a spe-
ciﬁc set. This is especially diﬃcult when the targeted cache levels are physically
indexed and use complex addressing.
2.3 Hardware Performance Counters
Hardware performance counters are special-purpose registers that are used to
monitor special hardware-related events. Such events include cache misses or
branch mispredictions, making the counters useful for performance analysis or
ﬁne tuning. Because performance counters require high level of privileges, they
cannot be directly used for an attack.
The registers are organized by performance monitoring units (called PMON).
Each PMON unit has a set of counter registers, paired with control registers.
Performance counters can only be used to measure the global events that happen
at the hardware level, and not for a process in particular. This adds noise and
has to be considered when performing a measurement.
There is one PMON unit, called CBo (or C-Box), per LLC slice. Each CBo
has a separate set of counters, paired to control registers. Among the available
events, LLC LOOKUP counts all accesses to the LLC. A mask on the event ﬁlters
the type of the request (data read, write, external snoop, or any) [9,11,12].
Performance counters depend on the processor, but the CBo counters and
the LLC LOOKUP event are present in a wide range of processors, and documented
Reverse Engineering Intel Last-Level Cache Complex Addressing
53
by Intel.1 Some adaptations are needed between diﬀerent types of processors.
Indeed, for Xeon Sandy Bridge, Xeon Ivy Bridge, Xeon Haswell and Core proces-
sors, the MSR addresses and the bit ﬁelds (thus the values assigned to each MSR)
vary, but the method remains similar. Reading and writing MSR registers needs
to be done by the kernel (ring 0) via the privileged instructions rdmsr and wrmsr.
3 Mapping Physical Addresses to Slices Using
Performance Counters
In this section, we present our technique for reverse engineering the complex
addressing function, using the performance counters. Our objective is to build a
table that maps a physical address (for each cache line) to a slice (e.g., Table 1).
Table 1. Mapping table obtained after running Algorithm 1. Each address has been