✓
✓
✓
✓
✓
✓
[5]
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
gauge the historical reputation of these facilitators and feed that
information into a classifier. These reputation-based predictions are
further described in Section 3.
Secondly, we try to detect whether a new registration is part of a
malicious campaign based on registrant data and facilitators. To this
end, we use unsupervised learning to group similar malicious do-
main registrations into clusters. Thereafter, we can assess whether
a new registration is part of an existing malicious cluster. These
similarity-based predictions are further described in Section 4.
Both detection strategies aim to be complementary: while the
similarity-based prediction focuses on specific campaigns, the repu-
tation-based predictions detect commonly used facilitators across
campaigns. We therefore expect that ensemble learning techniques
combining both prediction models can improve the strength of the
overall prediction.
2.1.2 Daily training and datasets. To establish a truly autonomous
and adaptive prediction system, we opt to retrain models on a
daily basis, taking into account any new training data that becomes
available. This sliding window ensures that evolving adversary
tactics are continuously captured. To enable configuration of this
aspect, we construct sliding training windows of different lengths
(15, 30, 45 and 60 days).
Furthermore, as depicted in Figure 1, we split the datasets up
into two phases:
Validation phase June 2015 is used for selecting and tuning the
final prediction model and its parameters.
Testing phase During the testing phase (July 2015 - May 2016),
the selected models are evaluated on unseen data. To ensure
proper testing of resilience and robustness, the testing phase
covers 11 months of registrations.
2.1.3 Evaluation criteria. We perform the evaluation of Premadoma
from two perspectives. We use (1) the blacklist data (Section 2.1.1)
as a basis for our ground truth, and (2) compare our results with the
manual, post-factum analysis performed by Vissers et al. [22] on
the same .eu TLD. The two primary evaluation metrics are precision
and recall.
The recall or true positive rate (TPR), is the percentage of all
blacklisted domains that the model was able to correctly predict as
malicious.
recall = TPR =
TP
TP + FN
The precision or positive predictive value (PPV) expresses from all
registrations that were predicted to be malicious, how many were
truly so.
Validation andParameter TuningPPPPPPMay 2015Jun 2015Jul 2015Jun 2016Deployment and TestingAug 2015Apr 2015TrainTrainTrainTrainTrainMay 2016TrainApr 2016precision = PPV =
TP
TP + FP
Due to the overwhelming majority of benign registrations, the
dataset is highly unbalanced, making precision a more adequate