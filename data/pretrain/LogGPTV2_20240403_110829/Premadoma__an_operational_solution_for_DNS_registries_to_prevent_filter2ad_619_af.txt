### Detection Strategies

To effectively detect malicious domain registrations, we employ two complementary strategies:

1. **Reputation-Based Prediction:**
   - We evaluate the historical reputation of facilitators (e.g., registrars, hosting providers) and feed this information into a classifier.
   - This approach helps in identifying commonly used facilitators across multiple malicious campaigns.
   - Further details on this method are provided in Section 3.

2. **Similarity-Based Prediction:**
   - We use unsupervised learning to cluster similar malicious domain registrations.
   - By grouping these registrations, we can determine if a new registration is part of an existing malicious cluster.
   - This method focuses on detecting specific malicious campaigns.
   - More information on this approach is available in Section 4.

### Complementary Detection Strategies

Both detection strategies are designed to complement each other:
- The similarity-based prediction targets specific malicious campaigns.
- The reputation-based prediction identifies frequently used facilitators across various campaigns.
- Combining both models using ensemble learning techniques is expected to enhance the overall predictive strength.

### Daily Training and Datasets

To ensure our prediction system remains autonomous and adaptive, we retrain the models daily, incorporating any new training data. This sliding window approach ensures that evolving adversary tactics are continuously captured. We use sliding training windows of different lengths (15, 30, 45, and 60 days) to configure this aspect.

#### Dataset Phases

The datasets are divided into two phases for validation and testing:
- **Validation Phase (June 2015):** 
  - Used for selecting and tuning the final prediction model and its parameters.
- **Testing Phase (July 2015 - May 2016):**
  - The selected models are evaluated on unseen data.
  - This phase covers 11 months of registrations to ensure robustness and resilience.

### Evaluation Criteria

We evaluate the performance of Premadoma from two perspectives:
1. **Blacklist Data (Section 2.1.1):** 
   - Serves as the ground truth for our evaluation.
2. **Comparison with Manual Analysis:**
   - We compare our results with the manual, post-factum analysis by Vissers et al. [22] on the same .eu TLD.

#### Primary Evaluation Metrics

- **Recall (True Positive Rate, TPR):**
  - The percentage of all blacklisted domains that the model correctly predicted as malicious.
  - \[ \text{Recall} = \text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}} \]

- **Precision (Positive Predictive Value, PPV):**
  - The proportion of registrations predicted to be malicious that were actually malicious.
  - \[ \text{Precision} = \text{PPV} = \frac{\text{TP}}{\text{TP} + \text{FP}} \]

Given the overwhelming majority of benign registrations, the dataset is highly unbalanced, making precision a more appropriate metric for evaluation.