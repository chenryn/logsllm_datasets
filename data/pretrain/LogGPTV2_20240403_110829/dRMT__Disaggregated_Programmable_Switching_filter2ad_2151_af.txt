0.326
0.051
0.486
0.170
N/A
0.372
1.243
0.171
0.071
0.048
0.336
1.056
Table 5: Area per processor (mm2).
0.005
0.071
0.470
0.051
0.315
0.071
0.048
0.336
1.367
THEOREM 5.1. The segment crossbar is equivalent to a full
crossbar if: (1) no match tables are split across memory clusters
and (2) each processor sends, and respectively each memory cluster
receives, at most ¯M key segments.
While we can construct examples to show that the two crossbars
are not equivalent when we have a table split across memory clus-
ters, the theorem indicates that for most practical cases, there is no
difference between them. We picked the multicast segment crossbar
because it comes quite close to the full crossbar in expressiveness,
while consuming much lesser area and power. Table 4 details the
synthesis numbers for gate area without wiring (Table 6 has the area
for a segment crossbar including wires; including wiring, the area
ratios between the different crossbars should stay about the same.).
6 HARDWARE COST
We now evaluate the cost of dRMT’s hardware implementation and
compare it to the RMT design. Currently, the most mainstream
process technology for this type of high-performance chip is 16
nm. As mentioned before, we do not have an implementation of
dRMT. In order to obtain silicon area estimates, we coded sample
logic and synthesized it with the Synopsys Design Compiler for a
1.2 GHz clock cycle target. We demonstrate that the chip area and
power differences between the dRMT and RMT chip are small. Our
calculations are for the processor portions of RMT and dRMT only,
and we do not discuss the chip design elements that are common
to both, such as memory clusters, SerDes, Ethernet MAC logic and
packet buffer memory. Discussion of the relationship of chip area
and cost can be found in chapter 1 of [24]. Table 5 summarizes the
area of the major components within the processor.
We synthesized a design for dRMT ALUs with an area close to
the one reported in the table. Since we do not know the complete set
of ALU operations used by the RMT architecture, our reported area
for RMT is based on RMT’s estimate [16] that ALUs take up 7%
of the entire chip for a 32-stage pipeline with 224 ALUs per stage.
Number of
processors RMT
16
19.9
29.9
24
32
39.8
Crossbar
with 32
memory
clusters
0.857
1.254
1.740
dRMT
(IPC = 1)
plus
dRMT
(IPC = 2)
plus
crossbar
crossbar
17.7
26.6
35.5
22.7
34.1
45.5
Table 6: Area for all processors plus interconnect (mm2).
Power for crossbar
Number of
processors with 32 memory clusters
16
24
32
0.88 W
1.31 W
1.75 W
Table 7: Crossbar power.
We estimate an area of 200 mm2 for RMT’s entire chip based on
the lower limit from Gibb et al. [19], and then scale the area down
from a 28 nm to a 16 nm process (We must make some assumptions
here in order to compare our areas to theirs, because RMT does not
report absolute area numbers for its ALUs.). We estimate 42% of
the area of RMT’s 224 ALUs. This is more than the fraction of total
ALUs involved (i.e., 32
224 ) to account for the larger size of dRMT’s
32-bit ALUs relative to RMT’s 8-bit and 16-bit ALUs.
The packet header vectors in RMT are a simple shift register of
20 packet vectors per stage, to cover 18 cycles of match latency plus
2 cycles of action latency. Instead, in dRMT, the match latency is
22 cycles to cover the additional 4 clock cycles of match latency
needed to traverse the crossbar. In addition, in our scheduling ex-
periments with P4 programs, we saw several instances that required
up to 29 packets (i.e., threads) per processor to accommodate the
schedule because of no-ops. This is higher than RMT because we
achieved a higher utilization of the processor resources. Therefore,
we assume up to 32 threads per processor in our dRMT architecture,
corresponding to 32 packet vectors per processor.
The dRMT packet header vectors cost more area per bit of storage
primarily due to the additional read and write ports required. For
IPC = 2, we need to read 2 different vectors per clock cycle to
construct match keys, plus 2 more to perform actions on them, and
write back modified vectors for another 2 packets. This totals 4 read
ports and 2 write ports.
Table 6 presents the total area for multiple processors. It also gives
the area for a multicast segment crossbar (§5.3) that interconnects
the processors and memory clusters, sized for the given number of
processors and 32 memory clusters. We found that about a third of
the area is used by gates (Table 4) and two-thirds by wiring.
We have existing commercial designs containing similar cross-
bars of up to 16 processors and 16 memory clusters with similar
utilization of 33%. We have carefully analyzed techniques such as
manually routing crossbar wiring over SRAMs in the memory clus-
ters that should allow us to scale to larger 32 × 32 crossbars (details
are in our extended version [5]).
Table 7 estimates the crossbar’s power. This was obtained using
Synopsys PrimeTime in the same 16 nm technology, 1.2 GHz clock
frequency, 0.9 volts, and 50% switching factor, i.e., the worst case
of all data bits changing values every clock cycle. To put our area
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
S. Chole et al.
and power numbers in context, commercial switch ASICs occupy
between 300 and 700 mm2, and consume between 150 and 350 W.
7 RELATED WORK
dRMT’s design is similar to network processing units (NPU) [4, 8–
11, 23, 25] and multi-core software routers [17], which feature an
array of processor cores with shared memory. Like both CPU and
NPU cores, each dRMT core has local instruction memory and a
scratchpad for data. However, NPUs lack deterministic guarantees
on performance and have historically been slower than line-rate
switches. An NPU’s non-determinism arises from multiple factors:
cache misses, contention within the processor-memory interconnect,
pipeline flushes in each core, etc. dRMT’s custom-designed crossbar
is scheduled at compile time to eliminate all contention—and hence
non-determinism. Further, by basing dRMT’s VLIW instruction set
on RMT, dRMT exploits the parallelism available within packet
processing more effectively than NPUs, which suffer a performance
hit because their instruction sets resemble conventional CPUs.
Cavium’s XPliant [3, 14] and Barefoot’s Tofino [2] are two com-
mercial products that support programmability at multi-Tbit/s speeds.
Based on publicly available documents [2, 14], it appears both use a
pipelined approach similar to RMT. It is unclear from these sources
whether they use a crossbar between the pipeline and table mem-
ories. If so, the crossbar would introduce a similar additional area
and power cost as we have analyzed for dRMT, while providing
memory disaggregation alone, e.g., better utilization of the pipeline’s
processing resources in the presence of large tables. However, both
the Xpliant and Tofino architectures would still suffer a performance
cliff if a single pass through the pipeline was insufficient.
Prior work [22] has looked at compiling P4 programs to the RMT
architecture using an ILP formulation. This ILP formulation needs
to handle the memory allocation problem for logical P4 tables while
respecting dependencies between these tables. dRMT decouples
the memory allocation problem from compute scheduling using
the crossbar, essentially reducing the compilation problem to two
separate ILPs for memory allocation and compute scheduling.
The problem of cyclic scheduling has been studied in the opera-
tions research community [18, 20, 21]. In a cyclic scheduling prob-
lem, a set of tasks needs to be executed an infinite number of times,
while still respecting task dependencies and resource constraints.
The objective in cyclic scheduling is to maximize the steady-state
throughput, i.e., how frequently an instance of the same task can be
executed. Our problem setting is similar, the tasks corresponding to
match or action operations. Our formulation differs from the stan-
dard cyclic scheduling problem by incorporating a constraint unique
to packet processing: we limit the number of packets that can be
processed concurrently using the IPC parameter.
8 CONCLUSION
This paper presented dRMT, a new architecture for high-speed pro-
grammable switching. At the core of dRMT is disaggregation in
two forms: in memory disaggregation, we move memories out of
processors and into a shared memory pool, while in compute disag-
gregation, we allow each processor to execute matches and actions
in any order respecting program dependencies. Our discussion of
disaggregation has been grounded in the context of dRMT, but it is
more broadly applicable. For instance, retaining the RMT pipeline
but adding a shared memory pool improves RMT’s memory uti-
lization. Similarly, disaggregating the matches and actions within
a single RMT table and putting them in different stages (as in the
RMT-fine architecture) reduces RMT’s stage count.
ACKNOWLEDGMENTS
We would like to thank our shepherd, Nate Foster, and our anony-
mous reviewers for their helpful suggestions. This work was partly
supported by NSF grants CNS-1563826, CNS-1526791, and CNS-
1617702, a gift from the Cisco Research Center, the Hasso Plattner
Institute Research School, the Israel Ministry of Science and Tech-
nology, the Gordon Fund for Systems Engineering, the Technion
Fund for Security Research, the Israeli Consortium for Network Pro-
gramming (Neptune), and the Shillman Fund for Global Security.
REFERENCES
[1] A Deeper Dive Into Barefoot Networks Technology. http://techfieldday.com/
appearance/barefoot-networks-presents-at-networking-field-day-14.
[2] Barefoot: The World’s Fastest
and Most Programmable Networks.
https://barefootnetworks.com/media/white_papers/Barefoot-Worlds-Fastest-
Most-Programmable-Networks.pdf.
[3] Cavium Attacks Broadcom in Switches. http://www.eetimes.com/document.asp?
doc_id=1323931.
[4] Cisco QuantumFlow Processor. https://newsroom.cisco.com/feature-content?
type=webcontent&articleId=4237516.
[5] dRMT project. http://drmt.technion.ac.il.
[6] Gurobi Optimization. http://www.gurobi.com.
[7] Intel FlexPipe. http://www.intel.com/content/dam/www/public/us/en/documents/
product-briefs/ethernet-switch-fm6000-series-brief.pdf.
[8] Intel IXP2800 Network Processor. http://www.ic72.com/pdf_file/i/587106.pdf.
[9] IXP4XX Product Line of Network Processors. http://www.intel.com/content/
www/us/en/intelligent-systems/previous-generation/intel-ixp4xx-intel-network-
processor-product-line.html.
[10] Mellanox Indigo NPS-400 400Gbps NPU. http://www.mellanox.com/page/
products_dyn?product_family=241&mtag=nps_400.
[11] Netronome Agilio CX SmartNICs. https://www.netronome.com/products/agilio-
cx.
[12] P4 Specification. https://p4lang.github.io/p4-spec.
[13] switch.p4. https://github.com/p4lang/switch/tree/master/p4src.
[14] XPliant™ Ethernet Switch Product Family. http://www.cavium.com/XPliant-
Ethernet-Switch-Product-Family.html.
[15] P. Bosshart, D. Daly, G. Gibb, M. Izzard, N. McKeown, J. Rexford, C. Schlesinger,
D. Talayco, A. Vahdat, G. Varghese, and D. Walker. P4: Programming Protocol-
Independent Packet Processors. SIGCOMM CCR, July 2014.
[16] P. Bosshart, G. Gibb, H.-S. Kim, G. Varghese, N. McKeown, M. Izzard, F. Mujica,
and M. Horowitz. Forwarding Metamorphosis: Fast Programmable Match-Action
Processing in Hardware for SDN. In ACM SIGCOMM, 2013.
[17] M. Dobrescu, N. Egi, K. Argyraki, B.-G. Chun, K. Fall, G. Iannaccone, A. Knies,
M. Manesh, and S. Ratnasamy. RouteBricks: Exploiting Parallelism to Scale
Software Routers. ACM SOSP, 2009.
[18] D. L. Draper, A. K. Jonsson, D. P. Clements, and D. E. Joslin. Cyclic scheduling.
[19] G. Gibb, G. Varghese, M. Horowitz, and N. McKeown. Design Principles for
In IJCAI, 1999.
Packet Parsers. In ANCS, 2013.
[20] C. Hanen. Study of a NP-hard cyclic scheduling problem: The recurrent job-shop.
European journal of operational research, 1994.
[21] C. Hanen and A. Munier. A study of the cyclic scheduling problem on parallel
processors. Discrete Applied Mathematics, 1995.
[22] L. Jose, L. Yan, G. Varghese, and N. McKeown. Compiling Packet Programs to
Reconfigurable Switches. In NSDI, 2015.
[23] I. Keslassy, K. Kogan, G. Scalosub, and M. Segal. Providing performance guaran-
tees in multipass network processors. IEEE/ACM Transactions on Networking,
20(6):1895–1909, 2012.
[24] D. A. Patterson and J. L. Hennessy. Computer Organization and Design, 4th
Edition: The Hardware/Software Interface. 2008.
[25] T. Sherwood, G. Varghese, and B. Calder. A pipelined memory architecture for
high throughput network processors. In ISCA, 2003.