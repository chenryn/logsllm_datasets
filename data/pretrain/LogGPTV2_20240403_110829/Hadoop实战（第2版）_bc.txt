3）在资源管理器上使用ZooKeeper实现故障转移。当资源管理器故障时，备用资源管理器将根据保存在ZooKeeper中的集群状态快速启动。MapReduce V2支持应用程序指定检查点。这就能保证应用主体在失败后能迅速地根据HDFS上保存的状态重启。这两个措施大大提高了MapReduce V2的可用性。
4）集群资源统一组织成资源容器，而不像在MapReduce V1中Map池和Reduce池有所差别。这样只要有任务请求资源，调度器就会将集群中的可用资源分配给请求任务，而无关资源类型。这大大提高了集群资源的利用率。
8.5 本章小结
本章结合MapReduce V1的缺陷为大家介绍了MapReduce V2，包括设计需求、主要设计思想、设计细节和相对于MapReduce V1的优势。大家应深入理解其思想和架构，以适应MapReduce发展的新形势。
第9章 HDFS详解
本章内容
Hadoop的文件系统
HDFS简介
HDFS体系结构
HDFS的基本操作
HDFS常用Java API详解
HDFS中的读写数据流
HDFS命令详解
WebHDFS
本章小结
HDFS（Hadoop Distributed File System）是Hadoop项目的核心子项目，是Hadoop主要应用的一个分布式文件系统，本章将对它进行详细介绍。实际上，Hadoop中有一个综合性的文件系统抽象，它提供了文件系统实现的各类接口，HDFS只是这个抽象文件系统的一个实例。
在本章中，我们首先会对Hadoop的文件系统给予一个总体的介绍，然后对HDFS的相关内容给予重点地讲解，包括HDFS的特点、基本操作、常用API及读/写数据流等。
9.1 Hadoop的文件系统
Hadoop整合了众多文件系统，它首先提供了一个高层的文件系统抽象org.apache.hadoop.fs.FileSystem，这个抽象类展示了一个分布式文件系统，并有几个具体实现，如表9-1所示。
Hadoop提供了许多文件系统的接口，用户可使用URI方案选取合适的文件系统来实现交互。比如，可以使用9.4.1节介绍的文件系统命令行接口进行Hadoop文件系统的操作。如果想列出本地文件系统的目录，那么执行以下shell命令即可：
hadoop fs-ls file：///
（1）接口
Hadoop是使用Java编写的，而Hadoop中不同文件系统之间的交互是由Java API进行调节的。事实上，前面使用的文件系统的shell就是一个Java应用，它使用Java文件系统类来提供文件系统操作。即使其他文件系统比如FTP、S3都有自己的访问工具，这些接口在HDFS中还是被广泛使用，主要用来进行Hadoop文件系统之间的协作。
（2）Thrift
上面提到可以通过Java API与Hadoop的文件系统进行交互，而对于其他非Java应用访问Hadoop文件系统则比较麻烦。Thriftfs分类单元中的Thrift API可通过将Hadoop文件系统展示为一个Apache Thrift服务来填补这个不足，让任何有Thrift绑定的语言都能轻松地与Hadoop文件系统进行交互。Thrift是由Facebook公司开发的一种可伸缩的跨语言服务的发展软件框架。Thrift解决了各系统间大数据量的传输通信，以及系统之间语言环境不同而需要跨平台的问题。在多种不同的语言之间通信时，Thrift可以作为二进制的高性能的通信中间件，它支持数据（对象）序列化和多种类型的RPC服务。
下面来看如何使用Thrift API。要使用Thrift API，首先要运行提供Thrift服务的Java服务器，并以代理的方式访问Hadoop文件系统。Thrift API包含很多其他语言生成的stub，包括C++、Perl、PHP、Python等。Thrift支持不同的版本，因此可以从同一个客户代码中访问不同版本的Hadoop文件系统，但要运行针对不同版本的代理。
关于安装与使用教程，可以参考src/contrib/thriftfs目录中关于Hadoop分布的参考文档。
（3）C语言库
Hadoop提供了映射Java文件系统接口的C语言库—libhdfs。libhdfs可以编写为一个访问HDFS的C语言库，实际上，它可以访问任意的Hadoop文件系统，也可以使用JNI（Java Native Interface）来调用Java文件系统的客户端。
这里的C语言的接口和Java的使用非常相似，只是稍滞后于Java，目前还不支持一些新特性。相关资料可参见libhdfs/docs/api目录中关于Hadoop分布的C API文档。
（4）FUSE
FUSE（Filesystem in Userspace）允许文件系统整合为一个Unix文件系统并在用户空间中执行。通过使用Hadoop Fuse-DFS的contirb模块支持任意的Hadoop文件系统作为一个标准文件系统进行挂载，便可以使用UNIX的工具（像ls、cat）和文件系统进行交互，还可以通过任意一种编程语言使用POSIX库来访问文件系统。
Fuse-DFS是用C语言实现的，可使用libhdfs作为与HDFS的接口，关于如何编译和运行Fuse-DFS，可以参见src/contrib../fuse-dfs中的相关文档。
（5）WebDAV
WebDAV是一系列支持编辑和更新文件的HTTP的扩展。在大部分操作系统中，WebDAV共享都可以作为文件系统进行挂载，因此，通过WebDAV向外提供HDFS或其他Hadoop文件系统，可以将HDFS作为一个标准的文件系统进行访问。
（6）其他HDFS接口
HDFS接口还提供了以下其他两种特定的接口。
HTTP。HDFS定义了一个只读接口，用来在HTTP上检索目录列表和数据。NameNode的嵌入式Web服务器运行在50070端口上，以XML格式提供服务，文件数据由DataNode通过它们的Web服务器50075端口向NameNode提供。这个协议并不拘泥于某个HDFS版本，所以用户可以自己编写使用HTTP从运行不同版本的Hadoop的HDFS中读取数据。HftpFileSystem就是其中一种实现，它是一个通过HTTP和HDFS交流的Hadoop文件系统，是HTTPS的变体。
FTP。Hadoop接口中还有一个HDFS的FTP接口，它允许使用FTP协议和HDFS交互，即使用FTP客户端和HDFS进行交互。
9.2 HDFS简介
HDFS是基于流数据模式访问和处理超大文件的需求而开发的，它可以运行于廉价的商用服务器上。总的来说，可以将HDFS的主要特点概括为以下几点。
（1）处理超大文件
这里的超大文件通常是指数百MB、甚至数百TB大小的文件。目前在实际应用中，HDFS已经能用来存储管理PB（PeteBytes）级的数据了。在雅虎，Hadoop集群也已经扩展到了4 000个节点。
（2）流式地访问数据
HDFS的设计建立在更多地响应“一次写入、多次读取”任务的基础之上。这意味着一个数据集一旦由数据源生成，就会被复制分发到不同的存储节点中，然后响应各种各样的数据分析任务请求。在大多数情况下，分析任务都会涉及数据集中的大部分数据，也就是说，对HDFS来说，请求读取整个数据集要比读取一条记录更加高效。
（3）运行于廉价的商用机器集群上
Hadoop设计对硬件需求比较低，只需运行在廉价的商用硬件集群上，而无需昂贵的高可用性机器。廉价的商用机也就意味着大型集群中出现节点故障情况的概率非常高。这就要求在设计HDFS时要充分考虑数据的可靠性、安全性及高可用性。
正是由于以上的种种考虑，我们会发现，现在的HDFS在处理一些特定问题时不但没有优势，而且还有一定的局限性，主要表现在以下几方面。
（1）不适合低延迟数据访问
如果要处理一些用户要求时间比较短的低延迟应用请求，则HDFS不适合。HDFS是为了处理大型数据集分析任务，主要是为达到高的数据吞吐量而设计的，这就要求可能以高延迟作为代价。目前有一些补充方案，比如使用HBase，通过上层数据管理项目来尽可能地弥补这个不足。
（2）无法高效存储大量小文件
在Hadoop中需要用NameNode（名称节点）来管理文件系统的元数据，以响应客户端请求返回文件位置等，因此文件数量大小的限制要由NameNode来决定。例如，每个文件、索引目录及块大约占100字节，如果有100万个文件，每个文件占一个块，那么至少要消耗200MB内存，这似乎还可以接受。但如果有更多文件，那么NameNode的工作压力更大，检索处理元数据所需的时间就不可接受了。
（3）不支持多用户写入及任意修改文件
在HDFS的一个文件中只有一个写入者，而且写操作只能在文件末尾完成，即只能执行追加操作。目前HDFS还不支持多个用户对同一文件的写操作以及在文件任意位置进行修改。
当然，以上几点都是当前的问题，相信随着研究者的努力，HDFS会更加成熟，可以满足更多的应用需要。以下链接是Hadoop的一些热点研究方向，读者可以自行参考：
http://wiki. apache.org/hadoop/ProjectSuggestions.
9.3 HDFS体系结构
想要了解HDFS的体系结构，首先从HDFS的相关概念入手，下面将介绍HDFS中的几个重要概念。
 9.3.1 HDFS的相关概念
1.块（Block）
我们知道，在操作系统中都有一个文件块的概念，文件以块的形式存储在磁盘中，此处块的大小代表系统读/写可操作的最小文件大小。也就是说，文件系统每次只能操作磁盘块大小的整数倍数据。通常来说，一个文件系统块大小为几千字节，而磁盘块大小为512字节。文件的操作都由系统完成，这些对用户来说都是透明的。
这里，我们所要介绍的HDFS中的块是一个抽象的概念，它比上面操作系统中所说的块要大得多。在配置Hadoop系统时会看到，它的默认块大小为64MB。和单机上的文件系统相同，HDFS分布式文件系统中的文件也被分成块进行存储，它是文件存储处理的逻辑单元（如果没有特别指出，后文中所描述的块都是指HDFS中的块）。
HDFS作为一个分布式文件系统，设计是用来处理大文件的，使用抽象的块会带来很多好处。一个好处是可以存储任意大的文件而又不会受到网络中任一单个节点磁盘大小的限制。可以想象一下，单个节点存储100TB的数据是不可能的，但是由于逻辑块的设计，HDFS可以将这个超大的文件分成众多块，分别存储在集群的各个机器上。另外一个好处是使用抽象块作为操作的单元可以简化存储子系统。这里之所以提到简化，是因为这是所有系统的追求，而对故障出现频繁、种类繁多的分布式系统来说，简化就显得尤为重要。在HDFS中块的大小固定，这样它就简化了存储系统的管理，特别是元数据信息可以和文件块内容分开存储。不仅如此，块更有利于分布式文件系统中复制容错的实现。在HDFS中，为了处理节点故障，默认将文件块副本数设定为3份，分别存储在集群的不同节点上。当一个块损坏时，系统会通过NameNode获取元数据信息，在另外的机器上读取一个副本并进行存储，这个过程对用户来说都是透明的。当然，这里的文件块副本冗余量可以通过文件进行配置，比如在有些应用中，可能会为操作频率较高的文件块设置较高的副本数量以提高集群的吞吐量。
在HDFS中，可以通过终端命令直接获得文件和块信息，比如以下命令可以列出文件系统中组成各个文件的块（有关HDFS的命令，将会在9.4节中详细讲解）：
hadoop fsck/-files-blocks
2.NameNode和DataNode
HDFS体系结构中有两类节点，一类是NameNode，另一类是DataNode。这两类节点分别承担Master和Worker的任务。NameNode就是Master管理集群中的执行调度，DataNode就是Worker具体任务的执行节点。NameNode管理文件系统的命名空间，维护整个文件系统的文件目录树及这些文件的索引目录。这些信息以两种形式存储在本地文件系统中，一种是命名空间镜像（Namespace image），一种是编辑日志（Edit log）。从NameNode中你可以获得每个文件的每个块所在的DataNode。需要注意的是，这些信息不是永久保存的，NameNode会在每次系统启动时动态地重建这些信息。当运行任务时，客户端通过NameNode获取元数据信息，和DataNode进行交互以访问整个文件系统。系统会提供一个类似于POSIX的文件接口，这样用户在编程时无须考虑NameNode和DataNode的具体功能。
DataNode是文件系统Worker中的节点，用来执行具体的任务：存储文件块，被客户端和NameNode调用。同时，它会通过心跳（Heartbeat）定时向NameNode发送所存储的文件块信息。
9.3.2 HDFS的体系结构
如图9-1所示，HDFS采用Master/Slave架构对文件系统进行管理。一个HDFS集群是由一个NameNode和一定数目的DataNode组成的。NameNode是一个中心服务器，负责管理文件系统的名字空间（Namespace）以及客户端对文件的访问。集群中的DataNode一般是一个节点运行一个DataNode进程，负责管理它所在节点上的存储。HDFS展示了文件系统的名字空间，用户能够以文件的形式在上面存储数据。从内部看，一个文件其实被分成一个或多个数据块，这些块存储在一组DataNode上。NameNode执行文件系统的名字空间操作，比如打开、关闭、重命名文件或目录。它也负责确定数据块到具体DataNode节点的映射。DataNode负责处理文件系统客户端的读/写请求。在NameNode的统一调度下进行数据块的创建、删除和复制。
1.副本存放与读取策略
副本的存放是HDFS可靠性和性能的关键，优化的副本存放策略也正是HDFS区分于其他大部分分布式文件系统的重要特性。HDFS采用一种称为机架感知（rack-aware）的策略来改进数据的可靠性、可用性和网络带宽的利用率。大型HDFS实例一般运行在跨越多个机架的计算机组成的集群上，不同机架上的两台机器之间的通信需要经过交换机，这样会增加数据传输的成本。在大多数情况下，同一个机架内的两台机器间的带宽会比不同机架的两台机器间的带宽大。
图 9-1 HDFS的体系结构
一方面，通过一个机架感知的过程，NameNode可以确定每个DataNode所属的机架ID。目前HDFS采用的策略就是将副本存放在不同的机架上，这样可以有效防止整个机架失效时数据的丢失，并且允许读数据的时候充分利用多个机架的带宽。这种策略设置可以将副本均匀地分布在集群中，有利于在组件失效情况下的负载均衡。但是，因为这种策略的一个写操作需要传输数据块到多个机架，这增加了写操作的成本。
举例来看，在大多数情况下，副本系数是3，HDFS的存放策略是将一个副本存放在本地机架的节点上，另一个副本放在同一机架的另一个节点上，第三个副本放在不同机架的节点上。这种策略减少了机架间的数据传输，提高了写操作的效率。机架的错误远比节点的错误少，所以这个策略不会影响数据的可靠性和可用性。同时，因为数据块只放在两个不同的机架上，所以此策略减少了读取数据时需要的网络传输总带宽。这一策略在不损害数据可靠性和读取性能的情况下改进了写的性能。
另一方面，在读取数据时，为了减少整体的带宽消耗和降低整体的带宽延时，HDFS会尽量让读取程序读取离客户端最近的副本。如果在读取程序的同一个机架上有一个副本，那么就读取该副本；如果一个HDFS集群跨越多个数据中心，那么客户端也将首先读取本地数据中心的副本。
2.安全模式
NameNode启动后会进入一个称为安全模式的特殊状态。处于安全模式的NameNode不会进行数据块的复制。NameNode从所有的DataNode接收心跳信号和块状态报告。块状态报告包括了某个DataNode所有的数据块列表。每个数据块都有一个指定的最小副本数。当NameNode检测确认某个数据块的副本数目达到最小值时，该数据块就会被认为是副本安全的；在一定百分比（这个参数可配置）的数据块被NameNode检测确认是安全之后（加上一个额外的30秒等待时间），NameNode将退出安全模式状态。接下来它会确定还有哪些数据块的副本没有达到指定数目，并将这些数据块复制到其他DataNode上。9.7节中将详细介绍安全模式的相关命令。
3.文件安全
NameNode的重要性是显而易见的，没有它客户端将无法获得文件块的位置。在实际应用中，如果集群的NameNode出现故障，就意味着整个文件系统中全部的文件会丢失，因为我们无法再通过DataNode上的文件块来重构文件。下面简单介绍Hadoop是采用哪种机制来确保NameNode的安全的。
第一种方法是，备份NameNode上持久化存储的元数据文件，然后将其转储到其他文件系统中，这种转储是同步的、原子的操作。通常的实现方法是，将NameNode中的元数据转储到远程的NFS文件系统中。
第二种方法是，系统中同步运行一个Secondary NameNode（二级NameNode）。这个节点的主要作用就是周期性地合并编辑日志中的命名空间镜像，以避免编辑日志过大。Secondary NameNode的运行通常需要大量的CPU和内存去做合并操作，这就要求其运行在一台单独的机器上。在这台机器上会存储合并过的命名空间镜像，这些镜像文件会在NameNode宕机后做替补使用，用以最大限度地减少文件的损失。但是，需要注意的是，Secondary NameNode的同步备份总会滞后于NameNode，所以损失是必然的。有关文件系统镜像和编辑日志的详细介绍请参见第10章。
9.4 HDFS的基本操作
本节将对HDFS的命令行操作及其Web界面进行介绍。
 9.4.1 HDFS的命令行操作
可以通过命令行接口来和HDFS进行交互。当然，命令行接口只是HDFS的访问接口之一，它的特点是更加简单直观，便于使用，可以进行一些基本操作。在单机上运行Hadoop、执行单机伪分布（笔者的环境为Windows下的单节点情况，与其他情况下命令行一样，大家可自行参考），具体的安装与配置可以参看本书第2章，随后我们会介绍如何运行在集群机器上，以支持可扩展性和容错。
在单机伪分布的配置中需要修改两个配置属性。第一个需要修改的配置文件属性为fs.default.name，并将其设置为hdfs：//localhost/，用来设定一个默认的Hadoop文件系统，再使用一个hdfsURI来配置说明，Hadoop默认使用HDFS文件系统。HDFS的守护进程会通过这个属性来为NameNode定义HDFS中的主机和端口。这里在本机localhost运行HDFS，其端口采用默认的8020。HDFS的客户端可以通过这个属性访问各个节点。
第二个需要修改的配置文件属性为dfs.replication，因为采用单机伪分布，所以不支持副本，HDFS不可能将副本存储到其他两个节点，因此要将配置文件中默认的副本数3改为1。
下面就具体介绍如何通过命令行访问HDFS文件系统。本节主要讨论一些基本的文件操作，比如读文件、创建文件存储路径、转移文件、删除文件、列出文件列表等操作。在终端中我们可以通过输入fs-help获得HDFS操作的详细帮助信息。
首先，我们将本地的一个文件复制到HDFS中，操作命令如下：
hadoop fs-copyFromLocal testInput/hello.txt hdfs：//localhost/user/ubuntu/In/hello.txt
这条命令调用了Hadoop的终端命令fs。Fs支持很多子命令，这里使用-copyFromLocal命令将本地的文件hello.txt复制到HDFS中的/user/ubuntu/In/hello.txt下。事实上，使用fs命令可以省略URI中的访问协议和主机名，而直接使用配置文件core-site.xml中的默认属性值hdfs：//localhost，即命令改为如下形式即可：
hadoop fs-copyFromLocal testInput/hello.txt/user/ubuntu/In/hello.txt
其次，看如何将HDFS中的文件复制到本机，操作命令如下：
hadoop fs-copyToLocal/user/ubuntu/In/hello.txt testInput/hello.copy.txt
命令执行后，用户可查看根目录testInput文件夹下的hello.copy.txt文件以验证完成从HDFS到本机的文件复制。
下面查看创建文件夹的方法：
hadoop fs-mkdir testDir
最后，用命令行查看HDFS文件列表：
hadoop fs-lsr In
-rw-r--r--1 ubuntu supergroup 348624 2012-03-11 11：34/user/ubuntu/In/CHANGES.txt
-rw-r--r--1 ubuntu supergroup 13366 2012-03-11 11：34/user/ubuntu/In/LICENSE.txt
-rw-r--r--1 ubuntu supergroup 101 2012-03-11 11：34/user/ubuntu/In/NOTICE.txt
-rw-r--r--1 ubuntu supergroup 1366 2012-03-11 11：34/user/ubuntu/In/README.txt
-rw-r--r--1 ubuntu supergroup 13 2012-03-17 15：14/user/ubuntu/In/hello.txt
从以上文件列表可以看到，命令返回的结果和Linux下ls-l命令返回的结果相似。返回结果第一列是文件属性，第二列是文件的副本因子，而这是传统的Linux系统没有的。为了方便，笔者配置环境中的副本因子设置为1，所以这里显示为1，我们也看到了从本地复制到In文件夹下的hello.txt文件。
9.4.2 HDFS的Web界面
在部署好Hadoop集群之后，便可以直接通过http：//NameNodeIP：50070访问HDFS的Web界面了。HDFS的Web界面提供了基本的文件系统信息，其中包括集群启动时间、版本号、编译时间及是否又升级。
HDFS的Web界面还提供了文件系统的基本功能：Browse the filesystem（浏览文件系统），点击链接即可看到，它将HDFS的文件结构通过目录的形式展现出来，增加了对文件系统的可读性。此外，可以直接通过Web界面访问文件内容。同时，HDFS的Web界面还将该文件块所在的节点位置展现出来。可以通过设置Chunk size to view来设置一次读取并展示的文件块大小。
除了在本节中展示的信息之外，HDFS的Web界面还提供了NameNode的日志列表、运行中的节点列表及宕机的节点列表等信息。
9.5 HDFS常用Java API详解
9.1 中已经了解了Java API的重要性，本节深入介绍Hadoop的Filesystem类与Hadoop文件系统进行交互的API。
 9.5.1 使用Hadoop URL读取数据
如果想从Hadoop中读取数据，最简单的办法就是使用java.net.URL对象打开一个数据流，并从中读取数据，一般的调用格式如下：
InputStream in=null；
try{
in=new URL（"hdfs：//NameNodeIP/path"）.openStream（）；
//process in
}finally{
IOUtils.closeStream（in）；
}
这里要进行的处理是，通过FsUrlStreamHandlerFactory实例来调用在URL中的setURL-StreamHandlerFactory方法。这种方法在一个Java虚拟机中只能调用一次，因此放在一个静态方法中执行。这意味着如果程序的其他部分也设置了一个URLStreamHandlerFactory，那么会导致无法再从Hadoop中读取数据。
读取文件系统中的路径为hdfs：//NameNodeIP/user/ubuntu/In/hello.txt的文件hello.txt，如例9-1所示。这里假设hello.txt的文件内容为“Hello Hadoop！”。
例9-1：使用URLStreamHandler以标准输出显示Hadoop文件系统文件
package cn.edn.ruc.cloudcomputing.book.chapter09；
import java.io.*；
import java.net.URL；
import org.apache.hadoop.fs.FsUrlStreamHandlerFactory；
import org.apache.hadoop.fs.Path；
import org.apache.hadoop.filecache.DistributedCache；
import org.apache.hadoop.conf.*；
import org.apache.hadoop.io.*；
public class URLCat{
static{
URL.setURLStreamHandlerFactory（new FsUrlStreamHandlerFactory（））；
}
public static void main（String[]args）throws Exception{
InputStream in=null；
try{
in=new URL（args[0]）.openStream（）；
IOUtils.copyBytes（in, System.out，4096，false）；
}finally{
IOUtils.closeStream（in）；
}