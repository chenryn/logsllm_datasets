714,953
1.88M
560KB
178KB
114
61
87%
97%
0.02% 0.06% 0.06%
0%
0.46%
0.02%
Table 1: Properties of the datasets used in the study
3 Trace Analysis
In this section we present an analysis of a set of TCP
packet header traces we obtained from the access links at
ﬁve sites: two large university environments, with about
45,000 hosts (the Univsub and Univ19 traces) and 50,000
hosts (Munich), respectively; a research laboratory with
about 6,000 hosts (Lablo, and Lab2); a supercomputer
center with 3,000 hosts (Super); and an enterprise of
10,000 hosts connected to the Internet via a heavily loaded
T3 link (T3). The Super site is unusual in that many of its
hosts are not typical end-user machines but rather belong
to CPU “farms,” some of which make very high-speed and
high-volume connections; and the T3 site funnels much of
its trafﬁc through a small number of Web and SMTP prox-
ies.
While we cannot claim that these traces are broadly
representative, they do span a spectrum from many hosts
making small connections (the primary ﬂavor of Univsub,
Univ19, and Munich) to a few hosts making large, fast con-
nections (Super). We obtained traces of the inbound and
outbound trafﬁc at each site’s Internet access link; for all
but T3, this was a Gbps Ethernet. The volume of trafﬁc
at most of the sites is sufﬁciently high that it is difﬁcult to
capture packet header traces without loss. The exception
to this is the Munich dataset, which was recorded using
specialized hardware able to keep up with the high vol-
ume. For the other sites, most of the traces we obtained
were ﬁltered, as follows.
We captured Univsub off-hours (10:25 PM, for 5 min-
utes) with a ﬁlter that restricted the trafﬁc to one of the
university’s three largest subnets. tcpdump reported very
little loss (88 packets out of 22.5M before ﬁltering). Thus,
this trace is best interpreted as reﬂecting the performance
we would see at a good-sized university rather than a quite
large university.
We captured Univ19 in a somewhat unusual fashion.
We wanted a trace that reﬂected the aggregate university
trafﬁc, but this volume far exceeded what tcpdump could
capture on the monitoring host. While sampling is a natu-
ral fallback, a critical requirement is that we must be able
to express the sampling in a form realizable by the kernel’s
BPF ﬁltering engine—we cannot bring the packets up to
user space for sampling without incurring a high degree of
measurement loss. We instead used a BPF expression that
adds the addresses and ports (both source and destination)
together and then computes their residue modulo a given
prime P (which can be expressed in BPF using integer di-
vision and multiplication). We take all of the packets with
a given residue, which gives us a 1-in-P per-connection
sample.
Univ19 was captured using P = 19 and cycling through
all of the possible residues 0 . . . 18, capturing 5 minutes
per residue. The traces were made back-to-back during
a workday afternoon. Out of 631M total packets seen by
the packet ﬁlter, tcpdump reported 2,104 drops. We argue
that this is an acceptable level, and note that the presence
of measurement drops in the trace will tend to introduce a
minor bias towards a more pessimistic view of the preva-
lence of holes.
We then form Univ19 by analyzing the 19 traces and
combining the results, either adding up the per-subtrace
ﬁgures, or taking the maximum across them. For exam-
ple, when computing the maximum buffer size induced
by holes, we take the maximum of the maximums com-
puted for each of the 19 traces. Doing so gives a likely
approximation to what would have been seen in a full 5-
68
14th USENIX Security Symposium
USENIX Association
minute trace, since we ﬁnd that “local” maxima are gener-
ally short-lived and thus would not likely overlap in time.
On the other hand—and this is a major point to bear
in mind—the short duration of the Univsub and Univ19
traces introduces a signiﬁcant bias towards underestimat-
ing the prevalence of holes. This comes both from the
short lifetimes of the traces (less opportunity to observe
diverse behavior) and, perhaps more signiﬁcantly, from
the truncation effect: we do not analyze connections that
were already in progress when a trace began, or that have
not ﬁnished upon termination of the trace, because we do
not accurately know their state in terms of which pack-
ets constitute holes (upon trace startup) or how long it
takes holes to resolve (upon trace termination). This will
tend to bias our analysis towards under-representing long-
running connections, and these may in turn be respon-
sible for a disproportionate number of holes. However,
the overall consistency of the results for the university
traces with those for the longer-lived traces suggests that
the general ﬁndings we base on the traces—that buffer re-
quired for holes are modest, connections tend to have few
holes that take little per-hole memory, and that holes re-
solve quickly—remain plausible. In addition, the similar
Munich environment does not suffer from these biases. Its
results mostly agree qualitatively with those from Univ19,
except it shows a much higher level of average concurrent
holes, which appears to be due to a higher prevalence of
ﬁne-grained packet reordering.
The ﬁrst of the research lab traces, Lablo, was extracted
from ongoing tracing that the lab runs. This tracing uses
an elaborate ﬁlter to reduce the total trafﬁc to about 5%
of its full volume, and this subset is generally recorded
without any measurement drops. The ﬁltering includes
eliminating trafﬁc corresponding to some popular ports;
in particular, HTTP, which includes the majority of the
site’s trafﬁc. Thus, this trace is more simply a touchstone
reﬂecting a lower-volume environment.
Lab2 includes all packet headers. It was recorded dur-
ing workday afternoon hours. The packet ﬁlter inspected
46M packets, reporting about 1-in-566 dropped. Super
is a full header trace captured during workday afternoon
hours, with the ﬁlter inspecting 13.5M packets and report-
ing no drops.
T3 is a three-hour full header trace captured during
workday afternoon hours, with the ﬁlter capturing 101M
packets and reporting no drops. The mean inbound data
rate over the three hours was 30.3 Mbps (with the link
having a raw capacity of 44.7 Mbps); outbound was
11.0 Mbps. Note that the actual monitoring was at a Gbps
Ethernet link just inside of the T3 bottleneck, so losses in-
duced by the congested T3 on packets arriving from the
exterior Internet would show up as holes in the trace, but
losses induced on trafﬁc outbound from the site would not.
However, the ﬁgures above show that the congestion was
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
l
s
e
o
h
f
o
n
o
i
t
c
a
r
F
 0
 1e-07
 1e-06
 1e-05
 0.0001
Super
Univ_sub
Univ_19
Lab_2
Lab_lo
T3
Munich
 0.001
 0.1
Hole duration (seconds)
 0.01
 1
 10
 100
 1000
Figure 2: Cumulative distribution function of the duration
of holes. Most of the holes have a lifetime of less than 0.01
seconds.
primarily for the inbound trafﬁc. We note that monitor-
ing in this fashion, just inside of a bottleneck access link,
is a natural deployment location for intrusion prevention
systems and the like.
Table 1 summarizes the datasets and some of the char-
acteristics of the sequence holes present in their TCP con-
nections. We see that holes are very common: in Univsub
and Super, about 3% of connections include holes, while
in Lablo and T3, the number jumps to 10–20%. Overall,
0.1%–0.5% of all packets lead to holes.
Figure 1 shows how the reassembly buffer occupancy
changes during the traces. Of the four traces, Super is pe-
culiar: the buffer occupancy is mostly very low, but surges
to a high value for a very short period. This likely reﬂects
the fact that Super contains fewer connections, many of
which do bulk data transfers.
It is important to note the de-synchronized nature of the
sequence hole creation phenomenon. A key point is that
the buffer occupancy remains below 600 KB across all of
the traces, which indicates that stream reassembly over a
set of several thousand connections requires only a modest
amount of memory, and thus may be feasible at very high
speeds.
It is also noteworthy how some holes last for a long du-
ration and keep the buffer level elevated. These are visi-
ble as plateaus in several of the ﬁgures—for example, be-
tween T = 100 and T = 150 in the Univsub plot—and
are due to some long lived holes whose durations overlap,
whereas for the Munich trace we observed that the aver-
age buffer occupancy is signiﬁcantly higher than the rest
of the traces. This too is a result of concurrent but short-
lived holes, although the average number of concurrent
holes for this trace is larger (around 60) compared to the
other traces (< 5 concurrent holes on an average).
The frequent sudden transitions in the buffer level show
that most of the holes are quite transient. Indeed, Figure 2
USENIX Association
14th USENIX Security Symposium
69
)
s
e
t
y
b
(
t
n
e
m
e
r
i
u
q
e
r
r
e
f
f
u
B
)
s
e
t
y
b
(
t
n
e
m
e
r
i
u
q
e
r
r
e
f
f
u
B
 70000
 60000
 50000
 40000
 30000
 20000
 10000
 0
 0
 100000
 90000
 80000
 70000
 60000
 50000
 40000
 30000
 20000
 10000
 0
 0
Lab_lo
 500
 1000
 1500
 2000
 2500
 3000
 3500
 4000
Time (seconds)
(a) Lablo
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
 300000
 250000
)
s
e
t
y
b
(
t
n
e
m
e
r
i
u
q
e
r
r
e
f
f
u
B
 200000
 150000
 100000
 50000
 0
 0
 600000
 500000
 400000
 300000
 200000
 100000
)
s
e
t
y
b
(
t
n
e
m
e
r
i
u
q
e
r
r
e
f
f
u
B
lab_hv
 500
 1000
 1500
 2000
 2500
 3000
 3500
 4000
Time (seconds)
(b) Lab2
Super
 500
 1000
 1500
 2000
 2500
 3000
 3500
 4000
Time (seconds)
(c) Super
T3
 300000
 250000
)
s
e
t
y
b
(
t