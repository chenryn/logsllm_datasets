I am running celery with RabbitMQ as my backend, using a postgresql database
as the result backend.
I have created a task that I want to wait on:
testing/failure/tasks.py:
    from celery.decorators import task
    @task
    def return_result():
        return "Should return"
I call this task in a separate client process as follows:
testing/client.py:
    if __name__ == '__main__':
        from failure import tasks
        returned_results = tasks.return_result.apply_async(immediate=True).get()
        print "Returned results: %s" % returned_results
This call blocks on the .get() call and never returns the result.
testing/celeryconfig.py:
    # List of modules to import when celery starts.
    CELERY_IMPORTS = (
        "failure.tasks",
        )
    ## Result store settings.
    CELERY_RESULT_BACKEND = "database"
    CELERY_RESULT_DBURI = 'postgresql://user:password@db_host:5432/crose_saas_surveysystem'
    # override for testing
    #CELERY_RESULT_DBURI = "sqlite:////tmp/celery.sqlite"
    CELERY_DEFAULT_QUEUE = "default"
    CELERY_QUEUES = {
        "default": {
            "binding_key": "task.#",
        },
    }
    CELERY_DEFAULT_EXCHANGE = "tasks"
    CELERY_DEFAULT_EXCHANGE_TYPE = "topic"
    CELERY_DEFAULT_ROUTING_KEY = "task.default"
    CELERY_ROUTES = {
        }
    ## Broker settings.
    BROKER_HOST = 'localhost'
    BROKER_PORT = 5672
    BROKER_VHOST = "/"
    BROKER_USER = "guest"
    BROKER_PASSWORD = "guest"
    ## Worker settings
    ## If you're doing mostly I/O you can have more processes,
    ## but if mostly spending CPU, try to keep it close to the
    ## number of CPUs on your machine. If not set, the number of CPUs/cores
    ## available will be used.
    CELERYD_CONCURRENCY = 10
    # CELERYD_LOG_FILE = "celeryd.log"
    CELERYD_LOG_LEVEL = "DEBUG"
I'll attach the log file in a subsequent comment