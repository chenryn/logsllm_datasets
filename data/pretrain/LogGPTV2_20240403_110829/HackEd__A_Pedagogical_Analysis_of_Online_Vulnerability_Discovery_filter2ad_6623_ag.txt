execution. This foregrounds students’ current target system
and exploit mental models and prompts them to compare their
prediction to the true outcome. This technique has proven
effective in other domains, helping students recognize their own
incorrect mental models, allowing them to identify gaps in their
understanding and develop deeper conceptual knowledge [102].
Use a graphical syllabus to provide concept structure. A
graphical syllabus is a visual representation (e.g., a ﬂow chart)
of concepts covered in a course and their relationships [103],
[104]. These visualizations have been shown to help students
in other domains process and organize information in both
traditional and online courses. Exercises could adopt this visual
presentation to provide a high-level view of relationships among
challenges, as well as to provide a guide through progressions
of related challenges. For example, the syllabus could begin
as a graph with links among related challenges. As students
progress, more information could be revealed showing how the
challenges are related (e.g., same vulnerability type). Using a
graphical syllabus is also appealing because it ﬁts the gaming
motif common in CTFs: it aligns with roadmaps commonly
used to demonstrate player progression through game levels.
Incentivize production of educational elements in com-
munity submissions. Community-submitted challenges pro-
vide a valuable force multiplier. However, because of the num-
ber of distinct authors, their organization and the hints and other
information they provide can vary widely. This is expected,
since adding these additional elements can be tedious relative
to the more interesting problem of developing the challenge.
This is similar to the well-documented lack of documentation
in APIs and open-source software development [105]–[107].
One possible approach is to apply methods from the crowd-
documentation literature (popularized by sites like StackOver-
ﬂow [108]), including curation activities like voting as well
as incentives such as reputation scores [109]. Additionally,
because there is already a signiﬁcant amount of community-
generated content available in challenge walkthroughs and
blog posts, future work could also consider developing tools
to support improved knowledge discovery from these sources.
Research is needed to make some dimensions easier to
implement. Future research should explore pedagogical di-
mensions organizers reported as difﬁcult to implement, namely
secure development practice and tailored feedback. The key
challenge in secure development practice is evaluating patched
codes’ security. Manual analysis is time consuming and does
not scale [110]. However, static testing may enable “coding to
the test” without ﬁxing underlying issues, learning the wrong
lessons. Future work could measure static testings’ impact on
actual learning, as well as proposing and testing mechanisms
to elicit security review from other students. This could allow
manual evaluation and scale with student population size, while
providing additional exploitation practice.
Current approaches to tailored feedback rely on developers
writing challenge code to include feedback at key points.
This one-off approach is difﬁcult to execute and does not
scale well.Instead, future work should investigate generalizable
methods to help students understand the target program’s
execution under an exploit attempt, possibly by developing new
visualizations or using machine learning techniques to identify
patterns in successful versus unsuccessful exploits [111]–[113].
V. RELATED WORK
Our pedagogical review gives the ﬁrst comprehensive view
of the online hacking exercise landscape; however, there has
been signiﬁcant prior work considering security education. In
this section, we review and compare our work to prior research.
Guidelines for building hacking exercises. Educators and
practitioners have long recognized the beneﬁt of hands-on prac-
tice for computer security education, suggesting the inclusion
of hacking competitions into the academic pipeline [114]–
[116]. This has led several researchers to propose exercise
development guidelines to teach educators how to build these
types of exercises and improve educational outcomes [70],
[117]–[119]. While many of these guidelines provide lim-
ited recommendations for speciﬁc pedagogy, Pusey et al.
provide suggestions for tailoring challenges to student prior
experience [70] and establish a supportive environment for
underrepresented populations [119]. In our work, we not only
consider a broader range of pedagogical dimensions, but also
evaluate whether—and why not—these are applied in practice.
New exercises to address speciﬁc pedagogy. Several
researchers have proposed novel educational interventions
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:31:30 UTC from IEEE Xplore.  Restrictions apply. 
1279
to implement and evaluate particular pedagogical principles.
Several researchers have incorporated elements of peer-based
instruction, having students participate in teams of varying
experience levels and encouraging collaboration and discussion
among and between teams [4], [5], [7], [15]. Other exercises
go beyond the traditional program exploitation challenges. This
includes challenging students to design and implement secure
systems [9], [14], [120] and perform penetration testing, navi-
gating through sophisticated network topologies and pivoting
from machine-to-machine [10]. Reed et al. evaluate the value of
presenting challenges in the context of an overarching narrative,
ﬁnding students were more likely to complete challenges
associated with a storyline [121]. Chung and Cohen outline
challenges related to extraneous load (e.g., difﬁculty getting
technology set up to be able to participate) and propose
technical solutions to limit initial student burden [16].
Signiﬁcant effort in this space has focused on evaluating
and improving challenge difﬁculty, to provide feedback and
ensure challenges are appropriate for learner experience. Chung
and Cohen reﬂect on years of experiences running the CSAW
CTF [76], highlighting the importance of quality assurance
in challenge development to ensure appropriate difﬁculty and
feedback within challenges [16]. Owens et al. introduced more
easy and medium difﬁculty challenges into picoCTF [122],
to provide a more gradual difﬁculty slope for beginning
students [7]. They found this increased student engagement and
reduced participant dropout over previous years of the exercise.
Several researchers have added time-on-task tracking for each
challenge to compare student behavior (time spent working on
a challenge), student-reported feedback, and original assigned
challenge difﬁculty [6], [121], [123], using this data to tailor
future exercise progressions and challenge difﬁculty ratings.
Maennel et al. further suggested this information could be used
by organizers in real-time to identify unintentionally difﬁcult
challenges and support struggling students [123].
Each of these studies have shown the beneﬁt of particular
pedagogical principles in a given context. Our work takes these
principles and asks whether they are applied broadly across the
exercise ecosystem and therefore impacting student education.
Broad exercise educational reviews. While much of the
literature in security education has focused on individual
exercises,
there has been some research focused on the
ecosystem more broadly. Tobey et al. studied engagement
among beginning students in three exercises in the National
Cyber League, ﬁnding that experienced students are more likely
to be engaged and continue participation [17]. However, the
authors do not indicate reasons for this lack of engagement. By
reviewing the way exercises are organized, we hope to provide
some indication insights into how the exercises themselves
might impact student participation trends.
Karagiannis et al. reviewed 4 open source exercise deploy-
ment platforms to evaluate their usability with respect to
setup and administration [124]. We ask an orthogonal question,
focusing on students’ experiences with speciﬁc exercises, not
educators’ experiences setting up exercises generally.
Burns et al. provide an extensive review of 3600 challenges,
outlining the concepts covered and developing a framework
to assess the difﬁculty of each [11]. This work offers a useful
complement to our own, as it investigates what content exercises
teach and we evaluate how they teach.
Finally, Taylor et al. reviewed the organization and structure
(e.g., whether content is dynamic or static, whether the exercise
is open source) of 36 CTFs. Our work provides additional depth
to this survey as we consider how speciﬁc implementation
details impact exercises’ educational characteristics.
Other studies of security education. In addition to these
hands-on hacking exercises, other work has proposed a variety
of security-related educational interventions. These interven-
tions employ some pedagogical principles we discuss. Multiple
researchers have proposed and evaluated adding secure devel-
opment education into the developer’s daily workﬂow [125]–
[127]. Whitney et al. incorporate security nudges into the
IDE, providing security context as developers write code [125].
Weir et al. take a Participatory Action Research approach,
embedding a security researcher in the development team to
support security decision-making and evaluate this approach’s
effect over time [126]. Similarly, Poller et al. evaluate the
impact of third-party security reviews on security behaviors
over time [127]. Other researchers have suggested narrative-
based education for computer security. Sherman et al. and
Rowe et al. present case studies around exploited systems and
have students discuss the cause and potential mitigations [13],
[128]. Blasco and Quaglia had students discuss attacks and
defenses portrayed in ﬁctional scenarios from popular culture
(e.g., Star Wars: Rogue One) [129]. Other researchers have
had students share stories of relevant experiences [130], [131].
Denning et al. developed a tabletop card game designed to
expose participants to general security problems and adversarial
thinking through its overarching storyline [132]. Relatedly,
Frey et al. developed a tabletop game in which players defend
cyber-physical infrastructure, to help players reﬂect on security-
relevant decision processes and strategies [133].
VI. CONCLUSION
We described a qualitative review of 31 online hacking
exercises, combined with interviews with 15 organizers of
those exercises, to evaluate the ecosystem as an educational tool.
We found that many pedagogical principles were commonly
instantiated across the ecosystem, often in thoughtful and
creative ways. No exercise, however, embodied all dimensions
examined. We identiﬁed several situations where organizers
must typically make tradeoffs among principles, as well as ways
exercise origins in competition can be detrimental for education.
Building on these results, we suggested recommendations
including adding support for metacognition, adopting graphical
syallabi, and incentivizing community members to contribute
to educational aspects of the challenges they develop.
ACKNOWLEDGMENTS
We thank Rock Stevens, Jordan Wiens, Dave Levin and the
anonymous reviewers for their helpful feedback. This research
was supported by NSF grant CNS-1801545.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:31:30 UTC from IEEE Xplore.  Restrictions apply. 
1280
REFERENCES
[1] D. Votipka, R. Stevens, E. M. Redmiles, J. Hu, and M. L. Mazurek,
“Hackers vs. testers: A comparison of software vulnerability discovery
processes,” Proc. of the IEEE, 2018.
[2] HackerOne, “Home | hacker 101,” HackerOne, (Accessed 05-21-2020).
[Online]. Available: https://www.hacker101.com/
[3] S. Houston,
bug
bounty
a
21-2020).
researcher-resources-how-to-become-a-bug-bounty-hunter/1102
Available:
[Online].
how to
-
2016,
become
05-
https://forum.bugcrowd.com/t/
(Accessed
“Researcher
resources
hunter,” Bugcrowd,
[4] N. Backman, “Facilitating a battle between hackers: Computer security
outside of the classroom,” in In Proc. of the 47th ACM Technical
Symposium on Computing Science Education, ser. SIGCSE ?16.
New York, NY, USA: ACM, 2016, p. 603?608. [Online]. Available:
https://doi.org/10.1145/2839509.2844648
[5] J. Mirkovic and P. A. H. Peterson, “Class capture-the-ﬂag exercises,”
in 2014 USENIX Summit on Gaming, Games, and Gamiﬁcation in
Security Education (3GSE 14). San Diego, CA: USENIX Association,
Aug. 2014. [Online]. Available: https://www.usenix.org/conference/
3gse14/summit-program/presentation/mirkovic
[6] J. Vykopal and M. Barták, “On the design of security games: From
frustrating to engaging learning,” in 2016 USENIX Workshop on
Advances in Security Education (ASE 16). Austin, TX: USENIX
Association, Aug. 2016. [Online]. Available: https://www.usenix.org/
conference/ase16/workshop-program/presentation/vykopal
[7] K. Owens, A. Fulton, L. Jones, and M. Carlisle, “pico-boo!:
How to avoid scaring students away in a ctf competition,” 2019.
[Online]. Available: https://cisse.info/pdf/download.php?ﬁle=CISSE_
v07_i01_p21_pre.pdf
[8] A. Doupé, M. Egele, B. Caillat, G. Stringhini, G. Yakin, A. Zand,
L. Cavedon, and G. Vigna, “Hit ’em where it hurts: A live security
exercise on cyber situational awareness,” in Proceedings of the 27th
Annual Computer Security Applications Conference, ser. ACSAC ?11.
New York, NY, USA: Association for Computing Machinery, 2011, p.
51?61. [Online]. Available: https://doi.org/10.1145/2076732.2076740
[9] W. Du, “Seed: Hands-on lab exercises for computer security education,”
IEEE Security Privacy, vol. 9, no. 5, pp. 70–73, 2011.
[10] K. Bock, G. Hughey, and D. Levin, “King of the hill: A novel
cybersecurity competition for teaching penetration testing,” in 2018
USENIX Workshop on Advances in Security Education (ASE 18).
Baltimore, MD: USENIX Association, Aug. 2018. [Online]. Available:
https://www.usenix.org/conference/ase18/presentation/bock
[11] T. J. Burns, S. C. Rios, T. K. Jordan, Q. Gu, and T. Underwood,
“Analysis and exercises for engaging beginners in online CTF
competitions for security education,” in 2017 USENIX Workshop on
Advances in Security Education (ASE 17). Vancouver, BC: USENIX
Association, Aug. 2017. [Online]. Available: https://www.usenix.org/
conference/ase17/workshop-program/presentation/burns
[12] K. Qian, D. Lo, H. Shahriar, L. Li, F. Wu, and P. Bhattacharya, “Learning
database security with hands-on mobile labs,” in 2017 IEEE Frontiers
in Education Conference (FIE), 2017, pp. 1–6.
[13] D. C. Rowe, B. M. Lunt, and J. J. Ekstrom, “The role of cyber-security
in information technology education,” in Proceedings of the 2011
Conference on Information Technology Education, ser. SIGITE ?11.
New York, NY, USA: Association for Computing Machinery, 2011, p.
113?122. [Online]. Available: https://doi.org/10.1145/2047594.2047628
[14] G. Fraser, A. Gambi, M. Kreis, and J. M. Rojas, “Gamifying a software
testing course with code defenders,” in Proceedings of the 50th ACM
Technical Symposium on Computer Science Education, ser. SIGCSE ?19.
New York, NY, USA: Association for Computing Machinery, 2019, p.
571?577. [Online]. Available: https://doi.org/10.1145/3287324.3287471
[15] J. Mirkovic, A. Tabor, S. Woo, and P. Pusey, “Engaging novices in
cybersecurity competitions: A vision and lessons learned at ACM
tapia 2015,” in 2015 USENIX Summit on Gaming, Games, and
Gamiﬁcation in Security Education (3GSE 15). Washington, D.C.:
USENIX Association, Aug. 2015. [Online]. Available: https://www.
usenix.org/conference/3gse15/summit-program/presentation/mirkovic
[16] K. Chung and J. Cohen, “Learning obstacles in the capture the ﬂag
model,” in Proceedings of the 1st USENIX Summit on Gaming, Games,
and Gamiﬁcation in Security Education, ser. 3GSE ’14. San Diego,
CA: USENIX Association, 2014. [Online]. Available: https://www.
usenix.org/conference/3gse14/summit-program/presentation/chung
Available: https://365.csaw.io/
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:31:30 UTC from IEEE Xplore.  Restrictions apply. 
1281
[17] D. H. Tobey, P. Pusey, and D. L. Burley, “Engaging learners in
cybersecurity careers: Lessons from the launch of the national cyber
league,” ACM Inroads, vol. 5, no. 1, p. 53?56, Mar. 2014. [Online].
Available: https://doi.org/10.1145/2568195.2568213
[18] J. D. Bransford, A. L. Brown, and R. R. Cocking, How people learn:
Brain, mind, experience, and school: Expanded edition. National
Academies Press, 2000.
[19] S. A. Ambrose, M. W. Bridges, M. DiPietro, M. C. Lovett, and M. K.
Norman, How learning works: Seven research-based principles for
smart teaching.
John Wiley & Sons, 2010.
[20] A. S. Kim and A. J. Ko, “A pedagogical analysis of online coding
tutorials,” in Proceedings of
the 2017 ACM SIGCSE Technical
Symposium on Computer Science Education, ser. SIGCSE ?17. New
York, NY, USA: Association for Computing Machinery, 2017, p.
321?326. [Online]. Available: https://doi.org/10.1145/3017680.3017728
[21] V. Le Pochat, T. Van Goethem, S. Tajalizadehkhoob, M. Korczy´nski,
and W. Joosen, “Tranco: A research-oriented top sites ranking hardened
against manipulation,” in Proceedings of the 26th Annual Network and
Distributed System Security Symposium, ser. NDSS 2019, Feb. 2019.
[22] B. Carlisle, M. Reininger, D. Fox, D. Votipka, and M. L. Mazurek, “On
the other side of the table: Hosting capture the ﬂag (ctf) competitions,”
in Proceedings of
the 6th Workshop on Security Information
Workers, ser. WSIW ’20. Virtual: USENIX Association, 2020.
[Online]. Available: https://wsiw2020.sec.uni-hannover.de/downloads/
WSIW2020-On%20the%20Other%20Side%20of%20the%20Table.pdf
[23] J. Wiens, “Practice ctf list,” May 2019, (Accessed 05-21-2020).
[Online]. Available: https://captf.com/practice-ctf/
[24] S.
Langkemper,
ctfs,”
these
[Online]. Available:
practice-hacking-with-vulnerable-systems/
December
your
“Practice
skills
with
05-22-2020).
https://www.sjoerdlangkemper.nl/2018/12/19/
hacking
(Accessed
2018,
[25] Y. Shoshitaishvili, “zardus/wargame-nexus: A sorted and updated list of
security wargame sites,” April 2020, (Accessed 05-22-2020). [Online].
Available: https://github.com/zardus/wargame-nexus
[26] CTFTime, “Ctftime.org / all about ctf (capture-the-ﬂag),” CTFTime,
2017, (Accessed 06-08-2017). [Online]. Available: https://ctftime.org
[27] A. Ruef, E. Jensen, N. Anderson, A. Sotirov, J. Little, B. Edwards, M. W,
D. D. Zovi, and M. Myers, “Ctf ﬁeld guide,” Trail of Bits, (Accessed
05-21-2020). [Online]. Available: https://trailofbits.github.io/ctf/
[28] J. Piaget, Success and understanding. Routledge, 1978.
[29] J. Piaget and M. Cook, The origins of
intelligence in children.
International Universities Press New York, 1952, vol. 8, no. 5.
[30] K. A. Ericsson, R. T. Krampe, and C. Tesch-Römer, “The role of delib-
erate practice in the acquisition of expert performance.” Psychological
review, vol. 100, no. 3, p. 363, 1993.
[31] H. J. Hartman, “Metacognition in teaching and learning: An
introduction,” Instructional Science, vol. 26, no. 1/2, pp. 1–3, 1998.
[Online]. Available: http://www.jstor.org/stable/23371261
[32] J. H. Flavell, “Metacognitive aspects of problem solving,” in The nature
of intelligence, L. B. Resnick, Ed. Lawrence Erlbaum Associates,
1976.
Through Qualitative Analysis.
[33] K. Charmaz, Constructing Grounded Theory: A Practical
SagePublication Ltd,
http://www.amazon.
Guide
London,
com/Constructing-Grounded-Theory-Qualitative-Introducing/dp/
0761973532
Available:
[Online].
2006.
[34] N. McDonald, S. Schoenebeck, and A. Forte, “Reliability and inter-rater
reliability in qualitative research: Norms and guidelines for cscw and
hci practice,” Proc. ACM Hum.-Comput. Interact., vol. 3, no. CSCW,
Nov. 2019. [Online]. Available: https://doi.org/10.1145/3359174