### Execution and Mental Models
Execution foregrounds students' current target system, encouraging them to utilize their mental models and compare their predictions with the actual outcomes. This technique has proven effective in various domains, helping students recognize and correct their incorrect mental models, identify gaps in their understanding, and develop deeper conceptual knowledge [102].

### Graphical Syllabus for Concept Structure
A graphical syllabus, such as a flow chart, visually represents the concepts covered in a course and their relationships [103], [104]. This visual tool aids students in processing and organizing information, both in traditional and online courses. Exercises can adopt this visual presentation to provide a high-level view of the relationships among challenges and to guide students through progressions of related challenges. For example, the syllabus could start as a graph with links among related challenges. As students progress, more information can be revealed, showing how the challenges are related (e.g., same vulnerability type). Using a graphical syllabus is also appealing because it aligns with the gaming motif common in CTFs, similar to roadmaps used to demonstrate player progression through game levels.

### Incentivizing Educational Elements in Community Submissions
Community-submitted challenges serve as a valuable force multiplier. However, due to the number of distinct authors, the organization, hints, and other information provided can vary widely. This variability is expected, as adding these additional elements can be tedious compared to the more engaging task of developing the challenge. This issue is similar to the well-documented lack of documentation in APIs and open-source software development [105]–[107]. One possible approach is to apply methods from the crowd-documentation literature (popularized by sites like StackOverflow [108]), including curation activities like voting and incentives such as reputation scores [109]. Additionally, since there is already a significant amount of community-generated content available in challenge walkthroughs and blog posts, future work could focus on developing tools to support improved knowledge discovery from these sources.

### Research for Easier Implementation
Future research should explore pedagogical dimensions that organizers find challenging to implement, particularly secure development practice and tailored feedback. The key challenge in secure development practice is evaluating the security of patched code. Manual analysis is time-consuming and does not scale [110]. Static testing may enable "coding to the test" without addressing underlying issues, leading to the wrong lessons. Future work could measure the impact of static testing on actual learning and propose mechanisms to elicit security reviews from other students. This could allow for manual evaluation, scaling with student population size, and providing additional exploitation practice.

### Tailored Feedback
Current approaches to tailored feedback rely on developers writing challenge code to include feedback at key points. This one-off approach is difficult to execute and does not scale well. Instead, future work should investigate generalizable methods to help students understand the target program's execution under an exploit attempt. This could involve developing new visualizations or using machine learning techniques to identify patterns in successful versus unsuccessful exploits [111]–[113].

### Related Work
Our pedagogical review provides the first comprehensive view of the online hacking exercise landscape. However, significant prior work has considered security education. In this section, we review and compare our work to prior research.

#### Guidelines for Building Hacking Exercises
Educators and practitioners have long recognized the benefits of hands-on practice for computer security education, suggesting the inclusion of hacking competitions into the academic pipeline [114]–[116]. This has led several researchers to propose exercise development guidelines to teach educators how to build these types of exercises and improve educational outcomes [70], [117]–[119]. While many of these guidelines provide limited recommendations for specific pedagogy, Pusey et al. offer suggestions for tailoring challenges to student prior experience [70] and establishing a supportive environment for underrepresented populations [119]. In our work, we not only consider a broader range of pedagogical dimensions but also evaluate whether—and why not—these are applied in practice.

#### New Exercises for Specific Pedagogy
Several researchers have proposed novel educational interventions to implement and evaluate particular pedagogical principles. Some have incorporated elements of peer-based instruction, having students participate in teams of varying experience levels and encouraging collaboration and discussion [4], [5], [7], [15]. Other exercises go beyond traditional program exploitation challenges, including designing and implementing secure systems [9], [14], [120] and performing penetration testing [10]. Reed et al. evaluated the value of presenting challenges within an overarching narrative, finding that students were more likely to complete challenges associated with a storyline [121]. Chung and Cohen outlined challenges related to extraneous load (e.g., difficulty setting up technology) and proposed technical solutions to limit initial student burden [16].

#### Evaluating and Improving Challenge Difficulty
Significant effort has focused on evaluating and improving challenge difficulty to provide feedback and ensure challenges are appropriate for learner experience. Chung and Cohen highlighted the importance of quality assurance in challenge development to ensure appropriate difficulty and feedback [16]. Owens et al. introduced more easy and medium difficulty challenges into picoCTF, providing a more gradual difficulty slope for beginning students, which increased engagement and reduced dropout rates [7]. Several researchers have added time-on-task tracking for each challenge to compare student behavior, student-reported feedback, and original assigned challenge difficulty [6], [121], [123], using this data to tailor future exercise progressions and challenge difficulty ratings. Maennel et al. suggested that this information could be used in real-time to identify unintentionally difficult challenges and support struggling students [123].

#### Broad Exercise Educational Reviews
While much of the literature in security education has focused on individual exercises, some research has considered the ecosystem more broadly. Tobey et al. studied engagement among beginning students in three exercises in the National Cyber League, finding that experienced students are more likely to be engaged and continue participation [17]. By reviewing the way exercises are organized, we aim to provide insights into how the exercises themselves might impact student participation trends. Karagiannis et al. reviewed four open-source exercise deployment platforms to evaluate their usability with respect to setup and administration [124]. We ask an orthogonal question, focusing on students’ experiences with specific exercises rather than educators’ experiences setting up exercises generally.

Burns et al. provided an extensive review of 3600 challenges, outlining the concepts covered and developing a framework to assess the difficulty of each [11]. This work complements our own, as it investigates what content exercises teach, while we evaluate how they teach. Taylor et al. reviewed the organization and structure of 36 CTFs, and our work provides additional depth to this survey by considering how specific implementation details impact exercises' educational characteristics.

#### Other Studies of Security Education
In addition to hands-on hacking exercises, other work has proposed a variety of security-related educational interventions. These interventions employ some of the pedagogical principles we discuss. Multiple researchers have proposed and evaluated adding secure development education into the developer’s daily workflow [125]–[127]. Whitney et al. incorporated security nudges into the IDE, providing security context as developers write code [125]. Weir et al. took a Participatory Action Research approach, embedding a security researcher in the development team to support security decision-making and evaluate this approach’s effect over time [126]. Similarly, Poller et al. evaluated the impact of third-party security reviews on security behaviors over time [127]. Other researchers have suggested narrative-based education for computer security. Sherman et al. and Rowe et al. presented case studies around exploited systems and had students discuss the cause and potential mitigations [13], [128]. Blasco and Quaglia had students discuss attacks and defenses portrayed in fictional scenarios from popular culture [129]. Other researchers have had students share stories of relevant experiences [130], [131]. Denning et al. developed a tabletop card game designed to expose participants to general security problems and adversarial thinking through its overarching storyline [132]. Frey et al. developed a tabletop game in which players defend cyber-physical infrastructure, to help players reflect on security-relevant decision processes and strategies [133].

### Conclusion
We conducted a qualitative review of 31 online hacking exercises, combined with interviews with 15 organizers, to evaluate the ecosystem as an educational tool. We found that many pedagogical principles were commonly instantiated across the ecosystem, often in thoughtful and creative ways. No exercise, however, embodied all dimensions examined. We identified several situations where organizers must typically make tradeoffs among principles, as well as ways in which the competitive origins of exercises can be detrimental to education. Building on these results, we suggested recommendations including adding support for metacognition, adopting graphical syllabi, and incentivizing community members to contribute to the educational aspects of the challenges they develop.

### Acknowledgments
We thank Rock Stevens, Jordan Wiens, Dave Levin, and the anonymous reviewers for their helpful feedback. This research was supported by NSF grant CNS-1801545.

### References
[References listed here, formatted according to the citation style used in the original text.]

---

This revised version aims to improve clarity, coherence, and professionalism while maintaining the original content and intent.