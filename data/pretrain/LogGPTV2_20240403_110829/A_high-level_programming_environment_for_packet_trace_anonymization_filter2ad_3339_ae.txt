argument for the “AUTH” command.
"150 Opening BINARY mode data connection for /def.pdf (123.45.67.89,50034) (156678 bytes)"
"150 |opening| |binary| |mode| |data| |connection| |for| |/def.pdf| |123.45.67.89| |50034| |156678| |bytes|"
message:
split →
abstract → "150 |opening| |binary| |mode| |data| |connection| |for| |˜ arg| |˜ ip| |˜ num| |˜ num| |bytes|"
merge → "150 |opening| |ascii, binary| |mode| |data| |connection| |for| |˜ arg| |˜ ip| |˜ num| |˜ num| |bytes|"
Figure 6: Message Template Extraction
4.4.6 Discussion
Integrity of Output Trace: Besides the absence of private infor-
mation, we also want to check whether the packets, TCP ﬂows,
and FTP requests and replies in the anonymized trace are all well-
formed. To do so, we run Bro’s FTP analyzer on the anonymized
traces to see whether Bro can reassemble the TCP ﬂows and parse
the FTP requests and replies. We compare the FTP logs from both
traces. Bro’s FTP log records start and ﬁnish of FTP sessions and
all requests and replies in the session. For a day-long FTP trace of
80 MB, 8,871 connections, and 86,908 request-reply pairs, we ﬁnd
that the two logs have the same FTP session starting timestamps,6
request command sequences (not including the arguments) and re-
ply code sequences, also at the same timestamps. For command
arguments and reply messages, we cannot compare them directly
as of course many of them are anonymized. We randomly picked a
few sessions and manually checked the arguments and messages.
Anonymized Traces for Intrusion Detection: As mentioned ear-
lier, packet traces are particularly valuable for research on network
intrusion detection. So we very much want trace anonymization to
preserve intrusion-like activities. This applies both to preserving
actual attacks, but, even more so, unusual-but-benign trafﬁc that
stresses the false-positive/false-negative accuracy of intrusion de-
tection algorithms. This latter is particularly important because it
is often a key element missing from assessments of network in-
trusion detection mechanisms—it is easy for researchers to attain
traces of actual attacks, because they can generate these using the
plethora of available attack tools, but it is much more difﬁcult today
for researchers to attain detailed traces of background trafﬁc.
Generally whether an attack survives anonymization depends on
both its characteristics and how it is detected. Some FTP intrusions
are recognized by signatures of ﬁles or user IDs the intruder tries to
access or login as. For example, directory name “tagged” is often
associated with FTP warez attacks; failed “root” or “sysadm”
login attempts suggest server backdoor probing. Preserving these
attacks requires leaving relevant identiﬁers in the clear. Fortunately
the identiﬁers are mostly well-known and do not expose private
identities, so they can kept through anonymization by establishing
a white list for “sensitive” ﬁle names and user IDs to leave in the
clear. To do so, however, requires knowing the attack signatures
beforehand; thus, attacks with unknown signatures may still be lost
in anonymization.
Other types of intrusions are recognized by activity patterns
rather than identiﬁer signatures. Most of these attacks can survive
anonymization. For instance, port scanning is marked by unan-
swered (or responded by TCP-RST) TCP-SYN packets from the
same source host to different destination hosts; successive failed
attempts at creating directories on multiple servers may imply an
FTP warez attack.
Performance: Figure 7 shows the CPU time spent on a 1 GHz Pen-
tium III processor running on the day-long trace mentioned above.
We see that the FTP anonymizer, which also requires the FTP an-
alyzer, is 7.7 times slower than the FTP analyzer. To understand
6In some cases, Bro’s connection termination is triggered by a timer, which
results in slightly different session ﬁnish timestamps.
FTP analyzer
FTP analyzer + anonymizer
FTP analyzer + dummy rewriter
131 seconds
1009 seconds
192 seconds
Figure 7: Execution time of various FTP policy scripts
where time is spent, we also tested Bro with a dummy FTP trace
rewriter, which simply writes the original requests and replies to
the output trace. We ﬁnd that the execution overhead of the anony-
mizer script itself heavily dominates, comprising 81% of the to-
tal processing. The time is spent performing numerous hash ta-
ble lookups, string operations, and regular expression matches, and
generating a 3.8 MB anonymization log. We ﬁnd this performance
adequate, especially for off-line anonymization. It even sufﬁces for
on-line anonymization for FTP, though when extended to a higher
volume protocol such as HTTP may prove problematic.7
5. CHALLENGES AND NEW DIREC-
TIONS
We view our work as an early push towards making richer packet
traces available to the research community. There is still much to
be done in this area. From our experience, we believe the main
challenges include: 1) to formalize security considerations and the
process of developing an anonymization scheme; 2) to automate the
process of anonymization and veriﬁcation; 3) to keep more packet
dynamics in the transformed traces. Below we brieﬂy discuss each
of these.
Formalizing Anonymization:
In Section 4 we described our
methodology for trace anonymization and analyzed four types of
inference techniques, but our analysis is far from being formal or
complete. While accumulation of experience will help us have a
better understanding of the relationship among various data ele-
ments, developing a formal model for anonymization would be a
big step forward beyond the intuitive methods. A formal model
would mean that users can have a complete view of the threats and
rigorously deduce a detailed anonymization scheme from the objec-
tives. However, a major difﬁculty in pursuing such models is the
degree to which anonymization inherently involves knowledge of
semantics, including sometimes quite high-level abstractions, and
also corner cases that can inadvertently leak information.
Automating the Anonymization Process: Although the anony-
mization process has been much simpliﬁed by operating at the
application-protocol level, currently we still need human assistance
in tailoring scripts for traces (4.4.3), processing free-format texts
(4.4.4), and result veriﬁcation (4.4.5). The ﬁrst two, though being
optional, often largely improve the quality of the output trace. The
last (veriﬁcation) is an essential step which we cannot do without
human interaction. On the other hand, fully automating anonymi-
zation will bring substantial beneﬁts: 1) it will minimize human
effort in releasing traces, making it easier for sites to make traces
available; 2) it is critical for environments where the trace providers
7Note that the HTTP rewriter used to reduce HTTP packet traces as dis-
cussed in Section 3.4 runs on-line, processing nearly 100 times the daily
data volume, though in a simpler fashion.
themselves are not allowed to see the original traces (e.g., for traces
collected at some ISPs); 3) automated veriﬁcation will foster a
model of “script↔data” exchange, where users send anonymiza-
tion scripts to data owners who use them to easily generate traces
returned to the users [13].
The key for automating result veriﬁcation is to make the anony-
mization scheme “understandable” to the veriﬁer program. One
way is to design a declarative (instead of procedural) language for
the anonymization scripts. Being declarative, the anonymization
scheme speciﬁcation is also amenable to veriﬁcation, which is nec-
essary to ensure that the scheme is correctly speciﬁed.
Keeping Trafﬁc Dynamics: One fundamental difﬁculty of keeping
the original trafﬁc dynamics is that lengths of data may be changed
during transformation, and the new lengths must be reﬂected in
TCP/IP headers to keep packets “well-formed”. Therefore there is
not a single best way to keep the original dynamics. We are investi-
gating ways to retain as much of the dynamics as possible without
dragging the user into low-level packet processing. One possibility
is to create an out-of-band channel to convey information such as
original packet lengths, fragmentation, retransmission, etc.
Also it is particularly difﬁcult to process two parallel versions of
the data, for instance, in the presence of inconsistent TCP retrans-
missions, because trafﬁc parsing is stateful. So we have to remove
at least one version from the anonymized stream, even though in
some contexts (e.g., analyzing possible intrusion detection evasions
seen in practice [16]) it would be very useful to have both copies of
inconsistent retransmissions retained.
6. RELATED WORK
TCPdpriv [12] anonymizes tcpdump traces by stripping packet
contents and rewriting packet header ﬁelds. One of its features
is a form of “preﬁx-preserving” anonymization of IP addresses
(the “-A50” option).
[22] analyzes the security implications of
this anonymization, proposing an approach that might be used to
crack the “-A50” encoding by ﬁrst identifying hosts with well-
known trafﬁc pattern (e.g., DNS servers). Xu et al proposed a
cryptography-based scheme for preﬁx-preserving address anony-
mization [21]. The scheme can maintain a consistent anonymiza-
tion mapping across multiple anonymizers using a shared crypto-
graphic key. Peuhkuri presented an analysis of the private infor-
mation contained in TCP/IP header ﬁelds and proposed a scheme
to anonymize packet traces and store the results in a compressed
format [17]. Peuhkuri’s scheme for network addresses anonymi-
zation cannot be directly applied to our work because the scheme
generates 96 bits instead of 32 bits for each address, and we are
constrained by needing to generate output in tcpdump format. All
of these works address only the anonymization of TCP/IP headers,
with no mechanisms for retaining packet payloads.
NetDuDe (NETwork DUmp data Displayer and Editor) [9] is a
GUI-based tool for interactive editing of packets in tcpdump trace
ﬁles. NetDuDe itself does not parse application level protocols, but
allows user to write plug-in’s for packet processing, e.g., a check-
sum ﬁxer plug-in can recompute checksums and update the check-
sum ﬁelds in TCP and IP headers.
There has also been considerable work on extracting application-
level data from online trafﬁc, though without signiﬁcant applica-
tions to content-preserving anonymization. Gribble et al built an
HTTP parser to extract HTTP information from a network snif-
fer [7]. Feldmann in [5] describes BLT, a tool to extract com-
plete HTTP headers from high-volume trafﬁc, and discusses var-
ious challenges in extracting accurate HTTP ﬁelds. Pandora [14]
is a component-based framework for monitoring network events,
which contains, among others, components to reconstruct HTTP
data from packets. It is similar in spirit to Windmill [11]. Ethereal
is able to reconstruct TCP session streams, and parses the stream
to extract application protocol level data ﬁelds [3]. The ﬁelds can
be used to ﬁlter the view of the trace. Ethereal has a GUI-based
interface to display trace data. There are also numerous commer-
cial network monitoring systems that can extract application-level
information, e.g., EtherPeek[20].
There are also efforts on setting up honeypots [8] and break-in
challenges [2] to collect traces of network intrusions. Such pure
intrusion traces have the virtue of containing little private informa-
tion, as the target hosts are not used for other purposes. For the
same reason, however, the traces do not contain background trafﬁc
with various unusual-but-benign activities, and thus are very differ-
ent from trafﬁc at an operational site.
Finally, Mogul argues “Trace Anonymization Misses the Point”
[13], proposing an alternative strategy to trace anonymization—
instead of sharing anonymized traces, researchers send reduction
agents to the site that has the source trace data. We believe our tool
is in fact complementary to this sort of approach. Mogul raises the
question: what kind of code should be sent to the source sites? Our
answer is: “a Bro script for trace transformation.”
7. SUMMARY
In this work we have designed and implemented a new tool for
packet trace anonymization and general purpose transformation.
The tool offers a great degree of freedom and convenience for trace
transformation by providing a high-level programming environ-
ment in which transformation scripts operate on application-level
data elements.
Using this framework, we developed an anonymization script for
FTP traces and applied it to anonymizing traces from LBNL for
public release. Unlike previous packet trace anonymization efforts,
packet payload contents are included in the result. We discussed
the key anonymization principle of “ﬁlter-in” as opposed to “ﬁlter-
out”, and the crucial problem of verifying the correctness of the
anonymization procedure. We also analyzed a class of inference
attacks and how we might defend against them.
We believe this tool offers a signiﬁcant step forward towards
ending the current state of there being no publicly available packet
traces with application contents. As such, we hope to help open up
new opportunities in Internet measurement and network intrusion
detection research.
Acknowledgements
We would like to thank the Lawrence Berkeley National Labora-
tory, and Jim Rothfuss and Sandy Merola in particular, for working
with us to realize the public release of traces of LBNL trafﬁc that
include packet contents; Larry Peterson for his support through-
out this work; the anonymous SIGCOMM reviewers; our shepherd
Greg Minshall; Lujo Bauer, Ed Felten, Brent Waters, Chi Zhang,
and other colleagues at Princeton for their insights and suggestions;
and the staff of the Princeton Department of Computer Science for
providing us traces for testing. This work was supported in part by
NSF grant ANI-9906704, DARPA contract F30602–00–2–0561,
and the Intel Corporation.
8. REFERENCES
[1] S. Axelsson. The base-rate fallacy and the difﬁculty of intrusion
detection. ACM Transactions on Information and System Security,
3(3):186–205, August 2000.
[2] Capture the capture the ﬂag. http://www.shmoo.com/cctf/.
3. Adds the header: “X-Actual-Data-Length: 2709; gap=0,
content-length= 2709” to record the original Content-length
ﬁeld and how many bytes are actually transferred.
The tcpdump output of the transformed trace is also on the next
page.
Note that “Write-Deferring” is applied here: the new headers are
written at the position of the original Content-length header,
even though the actual data size is not determined until all of the
data is seen. The script defers writing the headers until the end of
the message and then writes back to the reserved position.
Furthermore, by changing only one line of the script, from:
msg$abstract = md5_hash(data);
to:
msg$abstract =
subst_string(data, "Google", "Goooogle");
the script then replaces every occurrence of “Google” in the data
entity with “Goooogle”, instead of replacing the whole data entity
with its MD5 hash value. Next page shows part of the transformed
trace. (There are four occurrences of “Google” in the original mes-
sage, thus the Content-length increases from 2709 to 2717.) Note
that sequence and acknowledgment numbers between the traces
differ due to packet reframing and the addition of X-Actual-Data-
Length headers.
[3] G. Combs. The Ethereal Network Analyzer.
http://www.ethereal.com/.
[4] Federal Committee on Statistical Methodology. Report on statistical