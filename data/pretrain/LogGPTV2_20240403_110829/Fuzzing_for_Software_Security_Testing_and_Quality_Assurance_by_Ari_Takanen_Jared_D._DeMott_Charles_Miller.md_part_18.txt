139/udp open|filtered netbios-ssn
445/udp open|filtered microsoft-ds
520/udp open|filtered route
1034/udp open|filtered activesync-notify
1434/udp open|filtered ms-sql-m
2948/udp open|filtered wap-push
You should note that the result above does not indicate anything about the
real network services implemented in the phone and definitely will have a number
of false positives (i.e., services that are not implemented) and false negatives (i.e.,
missing some of the implemented services). Still, a security analyst working for
the manufacturer can easily narrow down this list to the truly open services and
conduct a detailed threat analysis based on those results. Note that a port scan is
not a full threat analysis method, but merely a tool that can be used as a starting
point in identifying the attack vectors for further analysis of the potential threats
in a system. It is typically the first technique a security analyst will conduct when
starting a technical security audit.
Different threat analysis techniques are useful at different phases. Whereas our
example for the threat analysis of a complete product only applies to later phases
6760 Book.indb 106 12/22/17 10:50 AM
4.2 Transition to Proactive Security 107
of the product life cycle, the other discussed techniques such as threat tree analysis
target earlier phases, when the product is not yet ready for practical analysis.
4.2 Transition to proactive Security
A software product life cycle can be considered starting from the requirements col-
lection phase of software development, and ending when the last installation of the
software is retired from use. The software development life cycle is a subset of the
product life cycle. Although various software development models are in use, the
waterfall model can be applied to generalize these phases. After launch, the released
software enters a number of update and maintenance cycles, until it finally is retired
or replaced by a subsequent version of the software. A simplified product life cycle
consists of the following phases:
Predeployment (development):
• Requirements and design
• Implementation
• Development testing
• Acceptance testing
Postdeployment (maintenance):
• Integration testing
• System maintenance
• Update process and regression testing
• Retirement
In contrast, quality assurance aims at improving and measuring the quality of
the system proactively during the software development, the practices in vulner-
ability assessment focus on the discovery of (security) critical flaws in the launched
products. The difference traditionally is only with the test purpose and in the time of
test related to the software life cycle. Vulnerability assessment provides an assurance
metric by thoroughly testing a subset of the system and extrapolating the findings
to the whole system. Fuzzing is useful in both approaches.
Fuzzing should start as early in the product life cycle as possible. Early discov-
ery, and the elimination of the found defects, has clear observable cost-benefits.
The time of the discovery of security flaws is especially critical, because security
flaws discovered after the product launch have significant costs compared to other
defects in software. Companies found to have many postdeployment security bugs
can also gain a bad reputation for lack of security.
The purpose of fuzzing in general is to find security defects. The standard met-
rics of IT operations for up-time, system recovery, and change control can be used
for related security incidents.9 Excluding indirect costs such as brand value and
9 Andrew Jaquith. (2007). Security Metrics: Replacing Fear, Uncertainty, and Doubt (pp. 68–73).
Boston: Addison-Wesley.
6760 Book.indb 107 12/22/17 10:50 AM
108 Fuzzing Metrics
reputation, the direct costs related to software vulnerabilities can be divided in at
least the following categories:
1. Cost of discovery of the defects through internal or external audits;
2. Cost of remediation of the flaw in product development and regression testing;
3. Cost of security compromises and downtime, or in some cases direct dam-
age to the failing systems;
4. Cost of patch deployment and other change control related tasks to the
customer systems if already deployed.
4.2.1 Cost of Discovery
Security defects need to be discovered before they can be fixed. The costs related
to the discovery of the flaws depend on the used resources and methodologies.
Security defects are found in all phases of the software lifecycle, from development
until retirement, and different methods are used in the discovery.
The four basic fuzzing-related methods for defect discovery are
• Bug bounty hunters;
• Subcontracted security assessments;
• Internally built fuzzers;
• Commercial fuzzing tools.
The costs associated with the discovery of the flaws are different in these four
methods. A bug bounty hunter can ask for a fixed amount per found defect, whereas
a subcontracted security consultant typically invoices for the spent hours, or per
predefined security assessment project. Internally built testing tools involve develop-
ment, use, and maintenance-related costs. Third-party software (free or commercial)
involves potential initial investment, usage costs (affected by the ease of use of the
products), and future maintenance costs.
We will ignore the costs related to actually fixing the flaws for now and focus on
the cost of discovery. You can try to predict the cost of defect discovery with three
simple metrics, whether third parties or internal people perform the fuzzing process:
• Time it takes to conduct the tests;
• Number of tests required for adequate test coverage;
• Mean probability of discovering a failure per each test.
The resulting metric from the necessary testing time, number of tests, and the
probability of failure of discovery would indicate the forecasted cost (and after the
project the true cost) per failure. Note that this is different from the cost per bug,
as each failure needs to be verified to be an individual defect in the software dur-
ing the repair process. It is important to separate these, because the actual bugs in
software can be difficult to enumerate while tests are being conducted.
Adding more test automation (such as fuzzing) and processing capabilities will
reduce the test execution time. For a security test bed, you could use a server farm
full of virtual or true computers with an image of the system under test. This can
6760 Book.indb 108 12/22/17 10:50 AM
4.2 Transition to Proactive Security 109
be used to drive the number of trials per hour as high as possible. In such a setup
you need to consider costs related to setting up and maintaining the farm of test
targets and the tools required to be certain that all the results are being correctly
captured. Test execution time can depend on the fuzzer. Conducting one million tests
with one tool can take less time than ten thousand tests with another slower tool.
The defect discovery probability indicates the efficiency of the tests. This is really
where the value comes from intelligence in fuzzing. Smart fuzzers bring enormous
value to the testers if the selected tool finds 1,000 tests that crash the software and
200 tests that leak memory, and the additional 10 tests that take 10 times more
processing power from the SUT, but the discovery of those failures only takes
10,000 tests. That would result in a 12.1% failure efficiency. On the other hand, a
fuzzer based on random testing can conduct one million tests, with one thousand
failures, resulting in 0.1% failure efficiency. However, also remember that just cal-
culating the number of failures does not indicate the number of bugs responsible
for those failures. There may only be 20 bugs in the system that can be discovered
with fuzzing. These example calculations for failure and defect efficiency for two
fuzzers are shown in Table 4.1.
Let us next look at the costs related to the choice of tools for fuzzing. Whereas
commercial tools can be immediately available and low-risk for the return of invest-
ment from test efficiency perspective, these tools are sometimes ridiculously expen-
sive compared to hiring someone as a contract programmer to develop your own
fuzzer. If you decide to create your own one-off fuzzer tailored to your exact test-
ing needs, the first decision you need to make is whether to build a smart or dumb
fuzzer. As we’ll see later, it might make sense to build both. One could think that
the obvious answer would be to create a fuzzer that is most likely to yield the most
bugs of any type. Unfortunately, when we start a new fuzzing project, it will be very
difficult to estimate the success rate proactively. In general, if a particular software
product has never been fuzzed before, you will almost always find a significant
number of bugs. But you might be interested in how many flaws you will find with
different types of fuzzers and what type of investment is required.
Before starting any do-it-yourself fuzzing projects, it is important to really ana-
lyze the total cost for all choices. Commercial tools are easily analyzed based on
standard practices in security investments, such as return on security investment,
Table 4.1 Example Calculation of Failure and Defect Efficiencies
Smart Fuzzer Random Fuzzer
Number of tests 10,000 1,000,000
Test execution time 2 hours 20 hours
Number of failures found with tests 1,210 1,210
Failure efficiency 12.1% 0.121%
Failures per hour 605 60.5
Number of defects found with tests 20 20
Defect efficiency 0.2% 0.002%
Defects per hour 10 1
6760 Book.indb 109 12/22/17 10:50 AM
110 Fuzzing Metrics
or ROSI, calculations. Even if it is internally developed, it is still a security invest-
ment. There are many aspects to consider:
1. Which approach finds most individual failures or flaws (or test efficiency):
This is the actual return for the investment in fuzzing. Efficiency of the tests
will be the final measure of how successful the project was, but unfortu-
nately, that is very difficult to predict beforehand. Still, it would be easier
if we could also give a dollar value to efficiency. But what is a good enough
test result for the tool? How many defects were left in the product after the
tests? This question is similar to asking “What is good enough antivirus
software?” You would not survive in the security tools market if your solu-
tion only caught 50% of the issues compared to your competitor.
2. Cost to implement tools (or investment in the tools): This includes the costs
from the work-time to develop the tool, often calculated in man-months.
Note that the person developing the fuzzer might not be your average soft-
ware developer, and his or her time might be taken from crucial security
analysis tasks. Many fuzzer developers think this is a fun task and might
jump into the task without considering the priority against other security
assessment tasks.
3. Time to implement tools (typically zero, if a third-party tool is acquired): In
addition to the actual costs related to development time, fuzzer development
can influence the time it takes to test the product for release, and therefore
delay the launch of the product. Fuzzing can be a part of the final approval
gate for a software release, and therefore, the time it takes to develop testing
tools will delay the release.
The first three metrics represented development costs for the fuzzers themselves.
The rest of the metrics should apply to both internally developed fuzzers, freely
available open source fuzzing frameworks, and commercial fuzzing tools:
1. Time from availability to use (time used to test design and integration):
When a fuzzer framework becomes available, it does not necessarily mean
you can launch your testing activities immediately. Valuable time is often
spent on integrating the tool into an existing testing framework or test bed.
For some interfaces under test, a new tool can be launched immediately
when it becomes available. For some interfaces you need to follow a pro-
cess of collecting data, building fuzzing rules, and finally adding thorough
instrumentation tools into your test bed.
2. Resources needed to test (required manpower to conduct tests): Finally, the
tests are ready to be launched. Different fuzzing techniques have different
needs for the necessary personnel to be present at execution time, and for
the time it takes to conduct test analysis.
3. Time needed to test (again causes delays in the product/service launch):
Different fuzzing techniques have varying test execution times. Some tests
can be executed in a matter of hours, whereas other tests can take several
weeks to run.
6760 Book.indb 110 12/22/17 10:50 AM
4.2 Transition to Proactive Security 111
4. Other costs in the test environment (HW + maintenance people for the test
setup): The costs to test setup can be enormous, especially with fuzzing tools
that have long test execution times. Commercial fuzzing appliances usually
come bundled with all the necessary test equipment. For software-based
solutions, you need to dedicate hardware resources for the test execution.
Maintenance personnel for the test facility are also dedicated costs, but can
also be shared between different test setups.
5. Maintenance costs (is the testing one-off, or is it reusable and maintained):
Finally, fuzzing is not usually a one-off testing process. Fuzzing is also a
critical part of regression testing, and as such needs to be maintained for
the distant future. A proprietary tool developed several years ago might be
impossible to update, especially if the person who developed it is no longer
available to do so. The maintainability of the tests and dedicated resources
for keeping the fuzzing tools up to date are often the main point why people
switch from internally built tools to commercial tools.
There are benefits to each fuzzing approach, and the decision needs to be made
based on your own value estimations. Most metrics can be measured in relation
to financial investment. Some people value time more than others, so it would be
simpler if one could give a dollar value also for time in the equation, for example,
indicating how much a delay of one week in the launch of a product or service will
cost the company.
Finally, as already noted above, the equation ultimately comes down to test
efficiency, time, and costs. For manufacturers, the test efficiency (in test results and
test process) is often much more important than the direct costs. Although the value
of fuzzing is an interesting topic, there are very few public metrics for the costs. It
would be interesting to see how these metrics would apply to various fuzzing proj-
ects that have already been completed.
A quick analysis of the costs for deploying fuzzers for both IKE (Table 4.2)
and FTP (Table 4.3) protocols is given here as an example. Whereas FTP is a very
simple text-based protocol, the IKE protocol is complex and developing a fuzzer
for it requires significantly more time. The metrics used in these calculations were
explained earlier in this section.
The estimates for number of defects are based on our experience with contract
development and on real results from comparing the freely available PROTOS
ISAKMP/IKE fuzzer with tests conducted using the commercial Codenomicon
ISAKMP/IKE robustness test suite against the same ISAKMP implementation.
Cost for developing fuzzers within your own organization is generally lower
than acquiring a contracted fuzzer, because time required for your own employees,
especially for small fuzzing projects, can be shorter than contract time. The calcula-
tions, of course, have to take into account all additional expenses such as employee
benefits. The main problem with internally built tools is that finding and retain-
ing the best security researchers is no easy task, and therefore the defect count can
be estimated to be lower than for contracted work or commercial tools. We have
used an estimate of $2,000 for labor costs, although security researchers can cost
anything from $1,800 up to $4,000 a week. Contract employees often cost more,
6760 Book.indb 111 12/22/17 10:50 AM
112 Fuzzing Metrics
Table 4.2 Example Cost Calculation for IKE Fuzzers
Internally Contractor Commercial
Criteria (IKE fuzzer) Built Developed Open Source Product
Individual flaws found (number) 1 5 4 8
Cost of tools 0 $40,000 0 $10,000
Resources to implement (weeks) 20 8 1 1
Time to implement (weeks) 20 8 2 1
Resources to test (weeks) 1 1 1 1
Time to test (weeks) 1 1 1 1
Other costs in test environment $10,000 $10,000 $10,000 $10,000
Maintenance/year $50,000 $10,000 $50,000 $10,000
Total time (weeks) 21 9 3 2
Total resources (weeks) 21 9 2 2
Cost per work-week $2,000 $2,000 $2,000 $2,000
Total cost $102,000 $78,000 $64,000 $34,000
Cost per defect $102,000 $15,600 $16,000 $4,250
but generally work faster with larger projects, have more experience, and tend to
have more expectations on them. They are easier to find and use temporarily than a
qualified security tester. For our estimate, we have summed the contract hours into
the cost of the tools. Contract work can cost from $3,000 per week up to $10,000
per week, or even more.
Other investment consists of materials such as standard PC and the required
software such as debuggers needed for test analysis. Calculations should include
Table 4.3 Example Cost Calculation for FTP Fuzzers
Internally Contractor Commercial
Criteria (FTP fuzzer) Built Developed Open Source Product
Individual flaws found (number) 10 14 12 16
Cost of tools 0 $15,000 0 $10,000
Resources to implement (weeks) 9 3 1 1
Time to implement (weeks) 9 3 1 1
Resources to test (weeks) 1 1 1 1
Time to test (weeks) 1 1 1 1
Other costs in test environment $5,000 $5,000 $5,000 $5,000
Maintenance/year $20,000 $5,000 $10,000 $10,000
Total time (weeks) 10 4 2 2
Total resources (weeks) 10 4 2 2
Cost per work-week $2,000 $2,000 $2,000 $2,000
Total cost $45,000 $33,000 $19,000 $29,000
Cost per defect $4,500 $2,357 $1,583 $1,812
6760 Book.indb 112 12/22/17 10:50 AM
4.2 Transition to Proactive Security 113
necessary office space for the test facility. For free open source tools this might be
the only investment.
There are pros and cons for all available choices, and the best option will depend
on the complexity of the tested interfaces, the software that needs testing, and the
availability of in-house expertise, among many other parameters. One of the main