Instead, it’s best to change the IP address associated with a hostname using the hosts file.
But perhaps the application you’re testing is running on a device that doesn’t allow you
to change the hosts file. Therefore, setting up a custom DNS server might be the easiest
approach, assuming you’re able to change the DNS server configuration.
You could use another approach, which is to configure a full DNS server with the
appropriate settings. This can be time consuming and error prone; just ask anyone who has
ever set up a bind server. Fortunately, existing tools are available to do what we want,
which is to return our proxy’s IP address in response to a DNS request. Such a tool is
dnsspoof. To avoid installing another tool, you can do it using Canape’s DNS server. The
basic DNS server spoofs only a single IP address to all DNS requests (see Listing 2-11).
Replace IPV4ADDRESS ➊, IPV6ADDRESS ➋, and REVERSEDNS ➌ with appropriate strings. As with the
HTTP Reverse Proxy, you’ll need to run this as root on a Unix-like system, as it will try to
bind to port 53, which is not usually allowed for normal users. On Windows, there’s no
such restriction on binding to ports less than 1024.
DnsServer.csx
// DnsServer.csx – Simple DNS Server
// Expose console methods like WriteLine at global level.
using static System.Console;
// Create the DNS server template
var template = new DnsServerTemplate();
// Setup the response addresses
template.ResponseAddress = ➊"IPV4ADDRESS";
template.ResponseAddress6 = ➋"IPV6ADDRESS";
template.ReverseDns = ➌"REVERSEDNS";
// Create DNS server instance and start
var service = template.Create();
service.Start();
WriteLine("Created {0}", service);
WriteLine("Press Enter to exit...");
ReadLine();
service.Stop();
Listing 2-11: A simple DNS server
Now if you configure the DNS server for your application to point to your spoofing
DNS server, the application should send its traffic through.
Advantage of a Reverse HTTP Proxy
The advantage of a reverse HTTP proxy is that it doesn’t require a client application to
Technet24
||||||||||||||||||||
||||||||||||||||||||
support a typical forwarding proxy configuration. This is especially useful if the client
application is not under your direct control or has a fixed configuration that cannot be
easily changed. As long as you can force the original TCP connections to be redirected to
the proxy, it’s possible to handle requests to multiple different hosts with little difficulty.
Disadvantages of a Reverse HTTP Proxy
The disadvantages of a reverse HTTP proxy are basically the same as for a forwarding
proxy. The proxy must be able to parse the HTTP request and handle the idiosyncrasies
of the protocol.
Final Words
You’ve read about passive and active capture techniques in this chapter, but is one better
than the other? That depends on the application you’re trying to test. Unless you are just
monitoring network traffic, it pays to take an active approach. As you continue through
this book, you’ll realize that active capture has significant benefits for protocol analysis and
exploitation. If you have a choice in your application, use SOCKS because it’s the easiest
approach in many circumstances.
||||||||||||||||||||
||||||||||||||||||||
3
NETWORK PROTOCOL STRUCTURES
The old adage “There is nothing new under the sun” holds true when it comes to the way
protocols are structured. Binary and text protocols follow common patterns and structures
and, once understood, can easily be applied to any new protocol. This chapter details some
of these structures and formalizes the way I’ll represent them throughout the rest of this
book.
In this chapter, I discuss many of the common types of protocol structures. Each is
described in detail along with how it is represented in binary- or text-based protocols. By
the end of the chapter, you should be able to easily identify these common types in any
unknown protocol you analyze.
Once you understand how protocols are structured, you’ll also see patterns of
exploitable behavior—ways of attacking the network protocol itself. Chapter 10 will
provide more detail on finding network protocol issues, but for now we’ll just concern
ourselves with structure.
Binary Protocol Structures
Binary protocols work at the binary level; the smallest unit of data is a single binary digit.
Dealing with single bits is difficult, so we’ll use 8-bit units called octets, commonly called
bytes. The octet is the de facto unit of network protocols. Although octets can be broken
down into individual bits (for example, to represent a set of flags), we’ll treat all network
data in 8-bit units, as shown in Figure 3-1.
Figure 3-1: Binary data description formats
When showing individual bits, I’ll use the bit format, which shows bit 7, the most
significant bit (MSB), on the left. Bit 0, or the least significant bit (LSB), is on the right.
(Some architectures, such as PowerPC, define the bit numbering in the opposite
direction.)
Numeric Data
Technet24
||||||||||||||||||||
||||||||||||||||||||
Data values representing numbers are usually at the core of a binary protocol. These values
can be integers or decimal values. Numbers can be used to represent the length of data, to
identify tag values, or simply to represent a number.
In binary, numeric values can be represented in a few different ways, and a protocol’s
method of choice depends on the value it’s representing. The following sections describe
some of the more common formats.
Unsigned Integers
Unsigned integers are the most obvious representation of a binary number. Each bit has a
specific value based on its position, and these values are added together to represent the
integer. Table 3-1 shows the decimal and hexadecimal values for an 8-bit integer.
Table 3-1: Decimal Bit Values
Bit
Decimal value
Hex value
0
1
0x01
1
2
0x02
2
4
0x04
3
8
0x08
4
16
0x10
5
32
0x20
6
64
0x40
7
128
0x80
Signed Integers
Not all integer values are positive. In some scenarios, negative integers are required—for
example, to represent the difference between two integers, you need to take into account
that the difference could be negative—and only signed integers can hold negative values.
While encoding an unsigned integer seems obvious, the CPU can only work with the same
set of bits. Therefore, the CPU requires a way of interpreting the unsigned integer value
as signed; the most common signed interpretation is two’s complement. The term two’s
complement refers to the way in which the signed integer is represented within a native
integer value in the CPU.
Conversion between unsigned and signed values in two’s complement is done by taking
the bitwise NOT (where a 0 bit is converted to a 1 and 1 is converted to a 0) of the integer
and adding 1. For example, Figure 3-2 shows the 8-bit integer 123 converted to its two’s
||||||||||||||||||||
||||||||||||||||||||
complement representation.
Figure 3-2: The two’s complement representation of 123
The two’s complement representation has one dangerous security consequence. For
example, an 8-bit signed integer has the range –128 to 127, so the magnitude of the
minimum is larger than the maximum. If the minimum value is negated, the result is itself;
in other words, –(–128) is –128. This can cause calculations to be incorrect in parsed
formats, leading to security vulnerabilities. We’ll go into more detail in Chapter 10.
Variable-Length Integers
Efficient transfer of network data has historically been very important. Even though
today’s high-speed networks might make efficiency concerns unnecessary, there are still
advantages to reducing a protocol’s bandwidth. It can be beneficial to use variable-length
integers when the most common integer values being represented are within a very limited
range.
For example, consider length fields: when sending blocks of data between 0 and 127
bytes in size, you could use a 7-bit variable integer representation. Figure 3-3 shows a few
different encodings for 32-bit words. At most, five octets are required to represent the
entire range. But if your protocol tends to assign values between 0 and 127, it will only use
one octet, which saves a considerable amount of space.
Technet24
||||||||||||||||||||
||||||||||||||||||||
Figure 3-3: Example 7-bit integer encoding
That said, if you parse more than five octets (or even 32 bits), the resulting integer from
the parsing operation will depend on the parsing program. Some programs (including
those developed in C) will simply drop any bits beyond a given range, whereas other
development environments will generate an overflow error. If not handled correctly, this
integer overflow might lead to vulnerabilities, such as buffer overflows, which could cause
a smaller than expected memory buffer to be allocated, in turn resulting in memory
corruption.
Floating-Point Data
Sometimes, integers aren’t enough to represent the range of decimal values needed for a
protocol. For example, a protocol for a multiplayer computer game might require sending
the coordinates of players or objects in the game’s virtual world. If this world is large, it
would be easy to run up against the limited range of a 32- or even 64-bit fixed-point value.
The format of floating-point integers used most often is the IEEE format specified in
IEEE Standard for Floating-Point Arithmetic (IEEE 754). Although the standard specifies
||||||||||||||||||||
||||||||||||||||||||
a number of different binary and even decimal formats for floating-point values, you’re
likely to encounter only two: a single-precision binary representation, which is a 32-bit
value; and a double-precision, 64-bit value. Each format specifies the position and bit size
of the significand and exponent. A sign bit is also specified, indicating whether the value is
positive or negative. Figure 3-4 shows the general layout of an IEEE floating-point value,
and Table 3-2 lists the common exponent and significand sizes.
Figure 3-4: Floating-point representation
Table 3-2: Common Float Point Sizes and Ranges
Bit size
Exponent bits
Significand bits
Value range
32
8
23
+/– 3.402823 × 1038
64
11
52
+/– 1.79769313486232 × 10308
Booleans
Because Booleans are very important to computers, it’s no surprise to see them reflected in
a protocol. Each protocol determines how to represent whether a Boolean value is true or
false, but there are some common conventions.
The basic way to represent a Boolean is with a single-bit value. A 0 bit means false and
a 1 means true. This is certainly space efficient but not necessarily the simplest way to
interface with an underlying application. It’s more common to use a single byte for a
Boolean value because it’s far easier to manipulate. It’s also common to use zero to
represent false and non-zero to represent true.
Bit Flags
Bit flags are one way to represent specific Boolean states in a protocol. For example, in
TCP a set of bit flags is used to determine the current state of a connection. When making
a connection, the client sends a packet with the synchronize flag (SYN) set to indicate that
the connections should synchronize their timers. The server can then respond with an
acknowledgment (ACK) flag to indicate it has received the client request as well as the
SYN flag to establish the synchronization with the client. If this handshake used single
enumerated values, this dual state would be impossible without a distinct SYN/ACK state.
Technet24
||||||||||||||||||||
||||||||||||||||||||
Binary Endian
The endianness of data is a very important part of interpreting binary protocols correctly.
It comes into play whenever a multi-octet value, such as a 32-bit word, is transferred. The
endian is an artifact of how computers store data in memory.
Because octets are transmitted sequentially on the network, it’s possible to send the
most significant octet of a value as the first part of the transmission, as well as the reverse
—send the least significant octet first. The order in which octets are sent determines the
endianness of the data. Failure to correctly handle the endian format can lead to subtle
bugs in the parsing of protocols.
Modern platforms use two main endian formats: big and little. Big endian stores the
most significant byte at the lowest address, whereas little endian stores the least significant
byte in that location. Figure 3-5 shows how the 32-bit integer 0x01020304 is stored in
both forms.
Figure 3-5: Big and little endian word representation
The endianness of a value is commonly referred to as either network order or host order.
Because the Internet RFCs invariably use big endian as the preferred type for all network
protocols they specify (unless there are legacy reasons for doing otherwise), big endian is
referred as network order. But your computer could be either big or little endian.
Processor architectures such as x86 use little endian; others such as SPARC use big endian.
NOTE
Some processor architectures, including SPARC, ARM, and MIPS, may have onboard logic
that specifies the endianness at runtime, usually by toggling a processor control flag. When
developing network software, make no assumptions about the endianness of the platform you
might be running on. The networking API used to build an application will typically contain
||||||||||||||||||||
||||||||||||||||||||
convenience functions for converting to and from these orders. Other platforms, such as PDP-
11, use a middle endian format where 16-bit words are swapped; however, you’re unlikely to
ever encounter one in everyday life, so don’t dwell on it.
Text and Human-Readable Data
Along with numeric data, strings are the value type you’ll most commonly encounter,
whether they’re being used for passing authentication credentials or resource paths. When
inspecting a protocol designed to send only English characters, the text will probably be
encoded using ASCII. The original ASCII standard defined a 7-bit character set from 0 to
0x7F, which includes most of the characters needed to represent the English language
(shown in Figure 3-6).
Figure 3-6: A 7-bit ASCII table
The ASCII standard was originally developed for text terminals (physical devices with a
moving printing head). Control characters were used to send messages to the terminal to
move the printing head or to synchronize serial communications between the computer
and the terminal. The ASCII character set contains two types of characters: control and
printable. Most of the control characters are relics of those devices and are virtually unused.
But some still provide information on modern computers, such as CR and LF, which are
used to end lines of text.
The printable characters are the ones you can see. This set of characters consists of
Technet24
||||||||||||||||||||
||||||||||||||||||||
many familiar symbols and alphanumeric characters; however, they won’t be of much use if
you want to represent international characters, of which there are thousands. It’s
unachievable to represent even a fraction of the possible characters in all the world’s
languages in a 7-bit number.
Three strategies are commonly employed to counter this limitation: code pages,
multibyte character sets, and Unicode. A protocol will either require that you use one of
these three ways to represent text, or it will offer an option that an application can select.
Code Pages
The simplest way to extend the ASCII character set is by recognizing that if all your data is
stored in octets, 128 unused values (from 128 to 255) can be repurposed for storing extra
characters. Although 256 values are not enough to store all the characters in every
available language, you have many different ways to use the unused range. Which
characters are mapped to which values is typically codified in specifications called code pages
or character encodings.
Multibyte Character Sets
In languages such as Chinese, Japanese, and Korean (collectively referred to as CJK), you
simply can’t come close to representing the entire written language with 256 characters,
even if you use all available space. The solution is to use multibyte character sets combined
with ASCII to encode these languages. Common encodings are Shift-JIS for Japanese and
GB2312 for simplified Chinese.
Multibyte character sets allow you to use two or more octets in sequence to encode a
desired character, although you’ll rarely see them in use. In fact, if you’re not working with
CJK, you probably won’t see them at all. (For the sake of brevity, I won’t discuss multibyte
character sets any further; plenty of online resources will aid you in decoding them if
required.)
Unicode
The Unicode standard, first standardized in 1991, aims to represent all languages within a
unified character set. You might think of Unicode as another multibyte character set. But
rather than focusing on a specific language, such as Shift-JIS does with Japanese, it tries to
encode all written languages, including some archaic and constructed ones, into a single
universal character set.
Unicode defines two related concepts: character mapping and character encoding.
Character mappings include mappings between a numeric value and a character, as well as
many other rules and regulations on how characters are used or combined. Character
encodings define the way these numeric values are encoded in the underlying file or
network protocol. For analysis purposes, it’s far more important to know how these
||||||||||||||||||||
||||||||||||||||||||
numeric values are encoded.
Each character in Unicode is assigned a code point that represents a unique character.
Code points are commonly written in the format U+ABCD, where ABCD is the code
point’s hexadecimal value. For the sake of compatibility, the first 128 code points match
what is specified in ASCII, and the second 128 code points are taken from ISO/IEC 8859-
1. The resulting value is encoded using a specific scheme, sometimes referred to as
Universal Character Set (UCS) or Unicode Transformation Format (UTF) encodings. (Subtle
differences exist between UCS and UTF formats, but for the sake of identification and
manipulation, these differences are unimportant.) Figure 3-7 shows a simple example of
some different Unicode formats.
Figure 3-7: The string "Hello" in different Unicode encodings
Three common Unicode encodings in use are UTF-16, UTF-32, and UTF-8.
UCS-2/UTF-16
UCS-2/UTF-16 is the native format on modern Microsoft Windows platforms, as well
as the Java and .NET virtual machines when they are running code. It encodes code
points in sequences of 16-bit integers and has little and big endian variants.
UCS-4/UTF-32
Technet24
||||||||||||||||||||
||||||||||||||||||||
UCS-4/UTF-32 is a common format used in Unix applications because it’s the default
wide-character format in many C/C++ compilers. It encodes code points in sequences
of 32-bit integers and has different endian variants.
UTF-8
UTF-8 is probably the most common format on Unix. It is also the default input and