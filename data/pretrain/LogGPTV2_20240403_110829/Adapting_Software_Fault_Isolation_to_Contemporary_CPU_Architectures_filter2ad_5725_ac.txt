### In-Order vs. Out-of-Order CPUs: Performance Overhead Analysis

#### Introduction
We conducted a subset of our benchmarks on in-order machines to compare the performance overhead of Software Fault Isolation (SFI) on x86-64 architectures. The results are presented in Figure 2, which compares SFI performance to the faster of -m32 and -m64 compilation modes.

#### Compilation Modes and Baseline Selection
Our x86-64 comparisons are based on GCC 4.4.3. Selecting a performance baseline is not straightforward. For x86, the available compilation modes are either 32-bit (ILP32, -m32) or 64-bit (LP64, -m64). Each mode represents a performance tradeoff. The 32-bit compilation model uses ILP32 base types, resulting in a smaller data working set compared to 64-bit compilation. Conversely, the 64-bit instruction set offers additional registers and a more efficient register-based calling convention.

Ideally, we would compare our SFI compiler to a version of GCC that uses ILP32 and the 64-bit instruction set without our SFI implementation. However, in the absence of such a compiler, we consider a hypothetical compiler that uses an oracle to automatically select the faster of -m32 and -m64 compilation. Unless otherwise noted, all GCC compiles used the -O2 optimization level.

#### Performance Results
Figure 2 and Table 2 provide the x86-64 results, where the average SFI overhead is about 5% compared to -m32, 7% compared to -m64, and 15% compared to the oracle compiler. Across the benchmarks, the distribution is roughly bimodal.

For specific benchmarks like `parser` and `gap`, SFI performance is better than either -m32 or -m64 binaries. This suggests that the benefits of additional registers outweigh the SFI overhead. For other benchmarks (`vpr`, `mcf`, and `twolf`), the SFI impact is less than 2%, indicating that these benchmarks are memory-bound and do not benefit significantly from additional registers.

At the other end of the range, four benchmarks (`gcc`, `crafty`, `perlbmk`, and `vortex`) show performance overhead greater than 25%. These benchmarks run as fast or faster for -m64 than -m32, suggesting that data-cache pressure does not dominate their performance. `Gcc`, `perlbmk`, and `vortex` have large text segments, which may contribute to the higher overhead.

#### Discussion
Given our initial goal to impact execution time by less than 10%, we believe these SFI designs are promising. At this level of performance, most developers targeting our system would do better to tune their own code rather than worry about SFI overhead. The geometric mean commonly used to report SPEC results does a poor job of capturing the systemâ€™s performance characteristics; nobody should expect to get "average" performance. We will continue our efforts to reduce the impact of SFI for the cases with the largest slowdowns.

Our work fulfills a prediction that the costs of SFI would become lower over time [28]. While thoughtful design has certainly helped minimize SFI performance impact, our experiments also suggest how SFI has benefited from trends in microarchitecture. Out-of-order execution, multi-issue architectures, and the effective gap between memory speed and CPU speed all contribute to reducing the impact of the register-register instructions used by our sandboxing schemes.

#### ARM vs. x86-64
We were surprised by the low overhead of the ARM sandbox compared to the x86-64 sandbox. Clever ARM instruction encodings, the powerful bit-clear instruction, and predication on stores contributed to this. Our design typically requires one instruction per sandboxed ARM operation, whereas the x86-64 sandbox frequently requires extra instructions for address calculations and adds a prefix byte to many instructions. The regularity of the ARM instruction set and smaller bundles (16 vs. 32 bytes) also mean that less padding is required, reducing instruction cache pressure. The x86-64 design also induces branch misprediction through our omission of the `ret` instruction, while the ARM design uses the normal return idiom, minimizing impact on branch prediction.

#### Future Work
In our continuing work, we are pursuing opportunities to reduce SFI overhead on our x86-64 system. We will be moving to GCC version 4.5, which has instruction-scheduling improvements for in-order Atom systems. We also look forward to developing an infrastructure for profile-guided optimization, which should provide opportunities for both instruction cache and branch optimizations.

#### Related Work
Our work draws directly on Native Client, a previous system for sandboxing 32-bit x86 modules [30]. Our scheme for optimizing stack references was informed by an earlier system described by McCamant and Morrisett [18]. We were heavily influenced by the original software fault isolation work by Wahbe, Lucco, Anderson, and Graham [28].

Although there is a large body of published research on software fault isolation, we are aware of no publications that specifically explore SFI for ARM or for the 64-bit extensions of the x86 instruction set. SFI for SPARC may be the most thoroughly studied, being the subject of the original SFI paper by Wahbe et al. [28] and numerous subsequent studies.

#### Conclusion
This paper presents practical software fault isolation systems for ARM and 64-bit x86. Our experience indicates that SFI benefits from trends in microarchitecture, such as out-of-order and multi-issue CPU cores, although further optimization may be required to avoid penalties on some recent low-power in-order cores. We found that for data-bound workloads, memory latency can hide the impact of SFI.

Source code for Google Native Client can be found at: http://code.google.com/p/nativeclient/.

#### References
[1] M. Abadi, M. Budiu, U. Erlingsson, and J. Ligatti. Control-flow integrity: Principles, implementations, and applications. In Proceedings of the 12th ACM Conference on Computer and Communications Security, November 2005.
...
[30] B. Yee, D. Sehr, G. Dardyk, B. Chen, R. Muth, T. Ormandy, S. Okasaka, N. Narula, and N. Fullagar. Native client: A sandbox for portable, untrusted x86 native code. In Proceedings of the 2009 IEEE Symposium on Security and Privacy, 2009.