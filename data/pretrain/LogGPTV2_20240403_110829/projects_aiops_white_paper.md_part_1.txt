---
layout: article
title: "AIOps"
date:
modified:
excerpt: "excerpt"
tags: []
image:
  feature:
  teaser:
  thumb:
ads: false
toc: true
---  
This page is in construction. You will certainly find errs.
## Introduction
In planet-scale deployments, the Operation and Maintenance (O&M) of cloud platforms cannot be done any longer 
manually or simply with off-the-shelf solutions. It requires self-developed automated systems, ideally exploiting 
the use of AI to provide tools for autonomous cloud operations. We rely on deep learning, 
distributed traces, and time-series analysis (sequence analysis) to effectively detect and fix anomalous 
cloud infrastructure behaviors during operations to reduce the workload of human operators. 
The iForesight system, labelled under the AIOps software category, is being used to evaluate new O&M approaches. 
iForesight 3.0 is the result of 3 years of research with the goal to provide an intelligent new tool aimed at 
SRE cloud maintenance teams. 
It enables them to quickly detect and predict anomalies thanks to the use of artificial intelligence when cloud services 
are slow or unresponsive. 
## Problem
The operation of large-scale platforms serving millions of people often introduces complex and unique 
engineering challenges. 
Existing tools for monitoring IT infrastructures, networks and applications focus on collecting logs, metrics, 
events, and traces from distributed systems mainly for visualization. Nonetheless, the final goal of monitoring 
is to reach a level of technological development where we have tools that conduct root cause analysis with a high 
accuracy and enable to autonomously recover systems. To achieve this goal, we still need to shift from a data 
collection stage to an insight- and action-driven paradigm. One promising path to monitor planet- and large-scale 
platforms is to rely on advanced analytics and explore techniques from statistics, time-series analysis, data mining, 
natural language processing, graph processing, machine learning, and deep learning to extract insights from large 
volumes of monitoring data to support and drive recovery actions. 
Main challenges:
+ *Scale*. Large cloud providers can have more than 1 million physical servers. Each server has 2 CPUs. Each CPU has 
10 VMs. Each VM has a OS and various applications.
+ *Distribution*. AZ and regions
+ *Complexity*. Microservices, containers, and serverless complexity. Several programming languages. Several teams.
+ *Dynamism*. Workloads around the sun, lunch and terminate instances. CMDB is static but VM is dynamic.
## Approach
The mission of the **Intelligent Cloud Operations** SRE team (based in [Munich](https://www.muenchen.de/int/en.html), 
Germany) is to develop new AIOps systems and tools to analyze observability data 
from [Huawei Cloud](https://www.huaweicloud.com/en-us/about/about_us.html)
to detect impact to customers, identify the root cause within seconds, and fix the problem using 
the 1/5/10 rule (detection: 1 min, RCA: 5 min, recovery: 10 min).
AIOps (Artificial Intelligence for IT operations) characterizes systems which use complex monitoring platforms, heterogeneous big data, machine learning (ML) and other artificial intelligence technologies to toubleshoot IT problems.
The term can be viewed as the evolution of [IT operations analytic](https://en.wikipedia.org/wiki/IT_operations_analytics) and
[Software Analytics](http://taoxie.cs.illinois.edu/publications/ieeesoft13-softanalytics.pdf).
The following figure from [Gartner](https://www.gartner.com/en) provides a high level architecture of the system 
we are building highliting the main areas of concern: 
+ Real-time streaming and historical data, observations, 
[Big Data](https://en.wikipedia.org/wiki/Big_data), 
Machine Learning (ML), 
[Anomaly Detection](https://en.wikipedia.org/wiki/Anomaly_detection), 
[Root-Cause Analysis](https://en.wikipedia.org/wiki/Root_cause_analysis), 
[Performance Analysis](https://en.wikipedia.org/wiki/Application_performance_management), 
[Predictive Maintenance](https://en.wikipedia.org/wiki/Predictive_maintenance), and 
[Automation](https://en.wikipedia.org/wiki/Robotic_process_automation). 
In 2017 we adopted AI in the form of [Data Science](https://en.wikipedia.org/wiki/Data_science) and 
[Machine Learning](https://en.wikipedia.org/wiki/Machine_learning) approaches for anomaly detection, 
root-cause analysis, fault prediction, and automated recovery into our suite. 
These techniques, including **statistical learning**, **time-series analysis**, **deep learning**, **big data**,
**streaming**, and **data visualization**, enabled us to develop new production-ready services for troubleshooting 
Huawei Cloud and detect issues which were previously undetectable.
## Challenges
The challenges of operationalising AI are not limited to the understanding of deep learning or machine learning algorithms.
Major challenges are related with software engineering, access and processing of large amounts of distributed data, 
model management, updating, deleting and training models on specialized GPUs and hardware, composition of workflows
for orchestrating parallel jobs, and the visual management of models, workflows, and results. 
+ Methods, techniques, and algorithms
+ AIOps platform construction
## System Under the Microscope: Huawei Cloud 
Our cloud has planet-scale technical requirements with an 
[microservices](https://en.wikipedia.org/wiki/Microservices) architecture composed of hundreds of services.
They are distributed over thousands of hosts in many geographical regions and operate with an availability 
higher than [five nines](https://en.wikipedia.org/wiki/High_availability). 
Our system monitors Huawei Cloud which is build on top of [OpenStack](https://docs.openstack.org/), an 
opensource cloud operating system. OpenStack controls large pools of compute, storage, and networking
resources throughout tens of datacenters. The base services are predominantly written in Python and Java 
running on Linux. 
Huawei Cloud is one of the largest and fastest growing platforms in the world. 
It has a strong presence throughout the world with over 40 availability zones located across 23 geographical regions,
ranging from Germany, France, South/Central America, Hong Kong and Russia to Thailand and South Africa.
There are three properties that make platforms such as Huawei Cloud far more difficult to monitor and troubleshoot:
1. Amount of data and relationships which O&M teams need to analyze.
2. Due to its distributed nature and complexity, system data has a low signal to noise ratio.
3. Since many different subsystems interact together, semantically reconciliating data is difficult.
The strongest challenge for cloud architecture is design and operational complexity. 
Cloud deployments comprise thousands of geographically distributed services and microservices.
Behavior patterns such as the [Universal Law of Computational Scalability](http://www.perfdynamics.com/Manifesto/USLscalability.html), 
make a distributed system highly non-linear and difficult to model.
### Underlying Architecture
Key building block components which require a close monitoring include:
The book [The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines](https://ai.google/research/pubs/pub41606), 
written by Google engineers, provide a very good description of the various parts which make up a data center.
It also identifies that less than 10% of outages are caused by hardware, approximately 60% by software, and 
that operation and maintenance failure account for 20%.
#### Troubleshooting Services
Software that provides services to end users.
+ *OBS*. Object Storage Service is a stable, secure, efficient, cloud storage service
+ *EVS*. Elastic Volume Service offers scalable block storage for servers
+ *VPC*. Virtual Private Cloud enables to create private, isolated virtual networks
+ *ECS*. Elastic Cloud Server is a cloud server that provides scalable, on-demand computing resources
#### Troubleshooting Middleware
Examples:
+ Firewalls and VPNs
+ [API Gateways](https://microservices.io/patterns/apigateway.html) (e.g., [Kong](https://konghq.com))
+ [Load Balancers](https://en.wikipedia.org/wiki/Load_balancing_(computing)) (e.g., [HAProxy](http://www.haproxy.org))
+ [Message Queuing Services](https://en.wikipedia.org/wiki/Message_queuing_service)
(e.g., [RabbitMQ](https://en.wikipedia.org/wiki/RabbitMQ))
+ [Distributed Caches](https://en.wikipedia.org/wiki/Distributed_cache) 
(e.g., [Redis](https://en.wikipedia.org/wiki/Redis))
+ [Web Servers](https://en.wikipedia.org/wiki/Web_server)
(e.g., [Apache](https://en.wikipedia.org/wiki/Apache_HTTP_Server))
+ [Application Servers](https://en.wikipedia.org/wiki/Application_server) 
(e.g., [EJB](https://en.wikipedia.org/wiki/Enterprise_JavaBeans))
+ [Database Servers](https://en.wikipedia.org/wiki/Database_server) 
(e.g., [MySQL](https://en.wikipedia.org/wiki/MySQL))
For example, to troubleshoot a load balancer (LB), it is important to understand which components are being impacted. 
Load balancer metrics enable to measure the number and type of connections established, response time, and the 
quantity of data transfered across backend servers, listeners, and the balancer itself. 
The following metrics help to determine where to start an investigation to diagnose a load balancer and client issues:
+ Response time. Average response time of backend servers
+ Closed connections. Number of connections closed between the load balancer and backend servers.
+ 5xx status codes. Number of HTTP 5xx responses received from backend servers.
+ Unhealthy backend servers. The number of unhealthy backend servers in the backend set.
+ Connection count. Number of incoming client requests to the load balancer.
The metric *closed connections* can be used to evaluate if a large-scale system has enough services running 
to handle the incoming load. 
And, the *5xx status codes* or *unhealthy backend servers* may provide evidence that the last deployment 
introduced a bug. 
Many databases externalize different status metrics to help operators to troubleshoot errors and identify performance 
issues. Typical metrics include server resources, backend disk storage, query statistics, and cache issues.
[CouchDB website](https://docs.couchbase.com/server/5.5/monitoring/ui-monitoring-statistics.html) provides a 
detailed description of the metrics which can be used to troubleshooting.
Example of statistics metrics include:
+ *requests/sec*. Total number of requests processed per second.
+ *selects/sec*. Total number of SELECT requests processed per second.
+ *request time*. Average end-to-end time to process a query.
+ *service time*. Average time to execute a query.
#### Troubleshooting Platform
Software which typically abstracts the hardware of physical server.
+ [Linux Servers](https://en.wikipedia.org/wiki/Linux)
#### Troubleshooting Hardware
As another example, datacenter use server systems assembled with commodity DRAM memory protected against errors 
by storing redundant information and applying ECC to detect and correct errors. 
Nonetheless, a relatively recent study shows that event with ECC techniques, memory chip failures cause significant 
downtime in datacenters 
[DRAM Errors in the Wild: A Large-Scale Field Study](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35162.pdf). 
The main reason seems to be due to packaging and circuit problems. 
Thus, monitoring DRAM module to detect and predict anomalies is relevant for AIOps.  
+ [Network Switches](https://en.wikipedia.org/wiki/Network_switch) and 
[Network Routers](https://en.wikipedia.org/wiki/Router_(computing))
### Service offerings
Besides these building blocks which are part of the base cloud infrastructure, service offerings also need to be
monitored:
+ *Compute, network, storage*. Cloud servers, auto scaling, object storage, volume service, VPC network, and CDN, 
+ *Databases*. MySQl, PostgreSQL, and replication service.
+ *Security*. Vulnerability scan service, SSL management, and Anti-DDoS.
+ *Applications*. APM, API Gateway, and application orchestration. 
+ *Enterprise Intelligence*. Machine learning services, graph engines, face and image recognition, and Mapreduce.
+ *DevCloud*. Project management, build, code hub, code check, and code release.
### Solutions Required for Troubleshooting 
+ Switch failure
+ HDD failures
+ Service anomaly detection/prediction. Using log analysis, trace analysis, and metric analysis
+ [Cluster failure prediction](https://dl.acm.org/citation.cfm?id=1362678)
## AIOps Platform Construction
An AIOps platform architecture consists of functional layers such as:
1. *Big Data processing*. Real-time processing of streaming and historical data.
  * *In-memory databases*. (e.g., [Gorilla](https://www.vldb.org/pvldb/vol8/p1816-teller.pdf))
  * *Distributed log system*. (e.g., [LogDevice](https://github.com/facebookincubator/LogDevice)) 
2. *Data pipeline*. Connected data processing elements ingesting data from multiple sources.
3. *Library of algorithms*: Statistical functions, classical machine learning, and deep learning algorithm.
4. *Automation*. Use runbooks and RPA technology to automate repetitive tasks.
5. *User interface*. Allows IT operations teams t interact with the platform and quickly identify issues and apply corrective actions.
For 2019-2020, our work focuses on points 1)-3).
AIOps does not only requires new methods and techniques from the fields of statistics and ML, but it also needs 
online and offline big data infrastructure (such as Hadoop, HBase, Spark, Gobblin, Presto) to ingest and process
scale monitoring data which can reach several PB/day. For example, Facebook uses Presto for interactive queries
over their 300PB data stores.
iForesight is build using the following software stack and applications. 
+ Frontend: Grafana, Jupyter, Node.js
+ AI: Tensorflow, Keras, PyTorch, Pandas/NumPy, Scikit-learn, Huawei Model Arts
+ Backend: Microservices, Docker, MySQL 
+ Big Data: OpenTSDB, Hive, ArangoDB, HBase, Elastic Search, Spark Streaming. 
+ Transport: Kafka
+ Data sources: metrics, app logs, tracing, alarms, topologies, and change events
+ Agents: StatsD, cAdvisor, FluentD 
+ Language: Python
In 2019, we will closely following the progresses make in the following 5 fields to extend our stack and suite:
+ [AIOps](https://blog.appdynamics.com/aiops/what-is-aiops/),
[Service Mesh](https://www.nginx.com/blog/what-is-a-service-mesh/),
[Istio](https://istio.io),
[Distributed Tracing](https://opentracing.io/docs/overview/what-is-tracing/),
[SRE](https://landing.google.com/sre/),
[RPA](https://en.wikipedia.org/wiki/Robotic_process_automation)
### Monitoring system
The presentation "A Tale of One Billion Time Series" describes how Baidu.com monitors its large-scale search platform.
In 2018, the number of metrics collected has grown to 1 billion.
+ Millions hosts, services, instances
+ 600+ metrics per target on average
+ 1.000.000.000 time series (and the number os still increasing)
Looking at 1B time series from another angle brings the following requirements:
+ *Volume*. 50TB (1.000.000.000.000 bytes) per day (read a point of 4 bytes every 5 minutes=288*4=1152)
+ *Requests*. 10M r/w requests per second
+ *Points*. 40M in and 60M out per second
+ *Traffic*. 50Gbps write and 100 Gbps read
Performance requirements are:
+ *Latency*: < 10s
+ *99th response time*: <= 500ms
+ *Availability (SLA)*: = 99.99%
    + Uses Hot standby