leads to higher accuracy, we set Œ∑ = 5 by default to reduce
computation time.


\
F
D
U
X
F
F
$

k

S
R
7






WRS
WRS
WRS



Œ∑
(a) Œ∑


\
F
D
U
X
F
F
$

k

S
R
7






WRS
WRS
WRS
/HIWH\H
(\H&RQILJXUDWLRQ
5LJKWH\H
%RWKH\HV
(b) Eye conÔ¨Åguration
Fig. 12.
Impact of Œ∑ (left) and eye conÔ¨Åguration (right).
2) Impact of eyes: Here we compare the inference accuracy
when the gaze trace from only one eye (left or right) or from
both eyes are used for PIN inference. The result is shown in
Fig. 12(b). It is not surprising to see that EyeTell achieves
much higher inference accuracy when the gaze traces of both
eyes are used. The reason is that the gaze trace from one eye
exhibits large noise due to the nature of human eyes while the
gaze trace averaged from both eyes is much less noisy.
3) Impact of frame rate: Now we compare the inference
accuracy of EyeTell under two frame rates for video recording,
30 fps and 60 fps. Since the default frame rate is 60 fps in our
experiment, we down-sampled Œ®l and Œ®r in Section V-C3 by
half to simulate the gaze trace obtained from 30-fps videos. As
shown in Fig. 13(a), EyeTell can yield better inference results
under a higher frame rate. The reason is that the gaze traces
from videos of higher frame rates are more accurate than those
of lower frame rates, thus resulting in higher accuracy.
4) Impact of lighting conditions: In this experiment, we
evaluate the impact of environmental lighting conditions on
EyeTell. Three types of environments are investigated,
in-
cluding indoor normal lighting with 300-360 lux illumination,
155


\
F
D
U
X
F
F
$

k

S
R
7






WRS
WRS
WRS
ISV
ISV
)UDPH5DWHISV
(a) Frame rate








\
F
D
U
X
F
F
$

k

S
R
7
WRS
WRS
WRS
,QGRRUORZ
,QGRRUQRUPDO 2XWGRRUVXQOLJKW
/LJKWLQJ&RQGLWLRQ
(b) Lighting condition
Fig. 13.
Impact of frame rate (left) and lighting condition (right).
indoor low lighting with 60-100 lux illumination, and outdoor
daytime sunlight with around 1200 lux illumination. In each
environment, each participant was asked to input 10 4-digit
PINs on an iPhone 6s, and each PIN was input Ô¨Åve times.
As mentioned above, the PINs were generated randomly and
then assigned to the participants. Fig. 13(b) summarizes the
result for this experiment. EyeTell exhibits similar perfor-
mance under indoor normal
lighting and outdoor daytime
sunlight conditions. However, the performance becomes worse
in indoor low lighting environments. The reason is that low
illumination in the shooting environment causes more noise in
detected eye regions, thus degrading the accuracy of ellipse
Ô¨Åtting for limbus and later gaze trace extraction.
5) Impact of recording distance: In this experiment, we
evaluate EyeTell when the recording distance is 1m, 2m,
and 3m, respectively. In each scenario, each participant was
asked to input 10 4-digit PINs on an iPhone 6s, and each
PIN was input Ô¨Åve times. The PINs were generated randomly
and then assigned to the participants. We show the result in
Fig. 14(a). As we can see, EyeTell has similar performance
when the distance is 1m or 2m. The slight performance
degradation when the distance is 3m can be attributed to the
larger zoom-in setting from a longer shooting distance. As a
result, the captured video may be more sensitive to small head
movements of the victim. However, we believe that the impact
of the recording distance can be very limited if the attacker
has more advanced camcorders.
6) Impact of recording angle: In this experiment, we study
the performance of EyeTell when the recording angle is
0¬∞(the default), 5¬∞, or 10¬∞, respectively. In each scenario, each
participant was asked to input 10 4-digit PINs on an iPhone
6s. Each PIN was input Ô¨Åve times. The PINs were generated
randomly and then assigned to the participants. Fig. 14(b)
shows the results. As expected, the inference accuracy quickly
decreases as the recording angle increases. This is mainly due
to two reasons. First, the gaze tracking method [10] EyeTell
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:38:20 UTC from IEEE Xplore.  Restrictions apply. 


\
F
D
U
X
F
F
$

k

S
R
7






WRS
WRS
WRS


'LVWDQFHP









\
F
D
U
X
F
F
$

k

S
R
7
WRS
WRS
WRS
GHJUHH
5HFRUGLQJ$QJOH
GHJUHH
GHJUHH
(a) Recording distance
(b) Recording angle
Impact of recording distance (left) and angle (right).
Fig. 14.
adopts assumes that the recording angle is zero. Second, when
the recording angle increases, the recorded video may not be
able to capture the limbus of both eyes. Accurate gaze trace
extraction under arbitrary recording angles (or equivalently
arbitrary head postures) is very challenging and requires more
advanced gaze tracking methods. We plan to look further into
this issue in our future work. Note that the attacker with an
advanced camcorder may not have much difÔ¨Åculty achieving
a near-zero recording angle in practice from a long distance
to the victim.
H. Computational Time
We implemented EyeTell in two components. The Ô¨Årst
one is for gaze trace extraction implemented in C++, and the
second for trace decoding implemented in Matlab. We run
the experiments on a DELL desktop with 2.67 GHz CPU,
9 GB memory, and Windows 10 64-bit Professional. In the
experiments, it takes less than 40s to generate a gaze trace from
an input video. For trace decoding, the most time-consuming
part is to generate the candidate set in Section V-D3, which is
jointly determined by the number of segments and the number
of candidates for each segment. Most PINs and lock patterns
are associated with a few segments. For example, it takes less
than 1s to generate the candidate set for a 4-digit PIN. In
contrast, it takes about 40min for an English word with 13
letters. Overall, the computational time incurred by EyeTell is
quite affordable for a determined adversary.
VII. DISCUSSION
In this section, we discuss the limitations of EyeTell and
point out possible countermeasures.
A. Limitations
First, the inference accuracy of EyeTell is slighter lower
than that of other video-based inference attacks [9], [40],
especially for the alphabetical keyboard. There are two main
reasons. First, other attacks use more direct observations about
the keystrokes, such as the device‚Äôs backside motion [9]
and the victim‚Äôs Ô¨Ånger movement [40]. In contrast, the gaze
trace that EyeTell exploits only contains indirect keystroke
information which is much more noisy and instable. Second,
the efÔ¨Åcacy of EyeTell on the alphabetical keyboard is largely
limited by the uncertain number of keystrokes. We plan to
explore extra side information such as eye Ô¨Åxation time in our
future work to have more accurate estimation of the number of
keystrokes and thus improve the inference accuracy of Eyetell.
Second, EyeTell currently requires the video to be recorded
within a small recording angle, e.g., less than 5¬∞based on our
experiments. While such small recording angles make EyeTell
detectable by vigilant users in uncrowded space, EyeTell is
likely to succeed in crowded areas. This limitation can be
alleviated by using more advanced camcorders or employing
more advanced gaze tracking methods that are less sensitive to
the victim‚Äôs head posture. With better optics, the attacker can
record the video from a longer distance. In addition, Gaze
tracking based on machine learning [51] has shown to be
effective even under different recording angles. We intend to
explore this direction in our future work.
Finally, our experiment scale is comparable to that in the
most recent work [40] but still limited. Though costly, larger-
scale experiments may further evidence the efÔ¨Åcacy of EyeTell.
B. Countermeasures
Since the only information EyeTell uses for keystroke
inference is a video of the victim‚Äôs eyes, mobile users should
be alert when they input important sensitive information on
their touchscreen devices. The following countermeasures can
be adopted to thwart EyeTell. The most effective way against
EyeTell is to prevent the attacker from video-recording the
victim‚Äôs eyes. For example, the user can wear sunglasses with
dark colors to hide his gaze trace. In addition, users can input
keystrokes without looking at the keys so that the gaze trace
extracted by EyeTell is irrelevant to keystrokes. However, this
method may be practical only when the user incurs a small
number of keystrokes, e.g., 4-digit PINs. Finally, sophisticated
users can increase their typing speed on the touchscreen. In
case that the frame rate of the attacker‚Äôs camcorder is not high
enough, the extracted gaze trace should be much less accurate
and noisy, therefore degrading the inference result.
VIII. CONCLUSION AND FUTURE WORK
In this paper, we introduced EyeTell, a video-based
keystroke inference attack framework to infer the victim‚Äôs
typed input from a video capturing his eyes. We adopted a
user-independent model-based gaze tracking method to obtain
a gaze trace of the victim‚Äôs eyes and designed novel decoding
algorithms to infer the typed input. We conÔ¨Årmed the high
efÔ¨Åcacy of EyeTell via extensive experiments on iOS and
Android devices under various circumstances.
We plan to improve EyeTell in three directions in the
future. First, we intend to develop novel gaze tracking methods
that are less sensitive to the victim‚Äôs head posture, which will
greatly enhance EyeTell‚Äôs applicability. Second, we will inves-
tigate novel methods to determine the number of keystrokes
in order to improve the inference accuracy of EyeTell on
alphabetical keyboards. Finally, we plan to evaluate EyeTell
in a larger scale.
IX. ACKNOWLEDGEMENT
We thank our shepherd and anonymous reviewers for their
comments and help in preparing the Ô¨Ånal version of the
paper. This work was supported in part by the US Army Re-
search OfÔ¨Åce (W911NF-15-1-0328) and US National Science
Foundation under grants CNS-1619251, CNS-1514381, CNS-
1421999, CNS-1320906, CNS-1700032, CNS-1700039, CNS-
1651954 (CAREER), and CNS-1718078.
156
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:38:20 UTC from IEEE Xplore.  Restrictions apply. 
REFERENCES
[1] M. Backes, M. D√ºrmuth, and D. Unruh, ‚ÄúCompromising reÔ¨Çections-or-
how to read lcd monitors around the corner,‚Äù in IEEE S&P, Oakland,
CA, May 2008.
[2] M. Backes, T. Chen, M. Duermuth, H. Lensch, and M. Welk, ‚ÄúTempest
in a teapot: Compromising reÔ¨Çections revisited,‚Äù in IEEE S&P, Oakland,
CA, May 2009.
[3] D. Balzarotti, M. Cova, and G. Vigna, ‚ÄúClearShot: Eavesdropping on
keyboard input from video,‚Äù in IEEE S&P, Oakland, CA, May 2008.
[4] F. Maggi, A. Volpatto, S. Gasparini, G. Boracchi, and S. Zanero, ‚ÄúA fast
eavesdropping attack against touchscreens,‚Äù in Information Assurance
and Security, Melaka, Malaysia, December 2011.
[5] R. Raguram, A. White, D. Goswami, F. Monrose, and J.-M. Frahm,
‚ÄúiSpy: Automatic reconstruction of typed input from compromising
reÔ¨Çections,‚Äù in ACM CCS, Chicago, IL, October 2011.
[6] Y. Xu, J. Heinly, A. White, F. Monrose, and J. Frahm, ‚ÄúSeeing dou-
ble: Reconstructing obscured typed input from repeated compromising
reÔ¨Çections,‚Äù in ACM CCS, Berlin, Germany, October 2013.
[7] Q. Yue, Z. Ling, X. Fu, B. Liu, K. Ren, and W. Zhao, ‚ÄúBlind recognition
of touched keys on mobile devices,‚Äù in ACM CCS, Scottsdale, AZ,
November 2014.
[8] D. Shukla, R. Kumar, A. Serwadda, and V. Phoha, ‚ÄúBeware, your hands
reveal your secrets!‚Äù in ACM CCS, Scottsdale, November 2014.
J. Sun, X. Jin, Y. Chen, J. Zhang, R. Zhang, and Y. Zhang, ‚ÄúVISIBLE:
Video-assisted keystroke inference from tablet backside motion,‚Äù in
NDSS, San Diego, CA, February 2016.
[9]
[10] E. Wood and A. Bulling, ‚ÄúEyetab: Model-based gaze estimation on
unmodiÔ¨Åed tablet computers,‚Äù in ACM ETRA, Safety Harbor, FL, March
2014.
[11] L. Cai and H. Chen, ‚ÄúTouchlogger: Inferring keystrokes on touch screen
from smartphone motion,‚Äù in USENIX HotSec, San Francisco, CA,
August 2011.
[12] E. Owusu, J. Han, S. Das, A. Perrig, and J. Zhang, ‚ÄúAccessory:
password inference using accelerometers on smartphones,‚Äù in ACM
HotMobile, San Diego, CA, February 2012.
[13] E. Miluzzo, A. Varshavsky, S. Balakrishnan, and R. Choudhury, ‚ÄúTap-
prints: your Ô¨Ånger taps have Ô¨Ångerprints,‚Äù in ACM MobiSys, Low Wood
Bay, Lake District, UK, June 2012.
[14] Z. Xu, K. Bai, and S. Zhu, ‚ÄúTaplogger: Inferring user inputs on
smartphone touchscreens using on-board motion sensors,‚Äù in ACM
WiSec, Tucson, AZ, April 2012.
[15] L. Simon and R. Anderson, ‚ÄúPIN skimmer: Inferring pins through the
camera and microphone,‚Äù in ACM SPSM, Berlin, Germany, November
2013.
[16] S. Narain, A. Sanatinia, and G. Noubir, ‚ÄúSingle-stroke language-
agnostic keylogging using stereo-microphones and domain speciÔ¨Åc
machine learning,‚Äù in ACM WiSec, Oxford, UK, July 2014.
[17] D. Asonov and R. Agrawal, ‚ÄúKeyboard acoustic emanations,‚Äù in IEEE
S&P, Oakland, CA, May 2004.
[18] L. Zhuang, F. Zhou, and J. Tygar, ‚ÄúKeyboard acoustic emanations
revisited,‚Äù in ACM CCS, Alexandria, VA, November 2005.
[19] Y. Berger, A. Wool, and A. Yeredor, ‚ÄúDictionary attacks using keyboard
acoustic emanations,‚Äù in ACM CCS, Alexandria, VA, November 2006.
[20] T. Zhu, Q. Ma, S. Zhang, and Y. Liu, ‚ÄúContext-free attacks using key-
board acoustic emanations,‚Äù in ACM CCS, Scottsdale, AZ, November
2014, pp. 453‚Äì464.
J. Liu, Y. Wang, G. Kar, Y. Chen, J. Yang, and M. Gruteser, ‚ÄúSnooping
keystrokes with mm-level audio ranging on a single phone,‚Äù in ACM
MobiCom, Paris, France, September 2015.
[21]
[22] P. Marquardt, A. Verma, H. Carter, and P. Traynor, ‚Äú(sp)iphone:
Decoding vibrations from nearby keyboards using mobile phone ac-
celerometers,‚Äù in ACM CCS, Chicago, IL, November 2011.
[23] X. Liu, Z. Zhou, W. Diao, Z. Li, and K. Zhang, ‚ÄúWhen good becomes
evil: Keystroke inference with smartwatch,‚Äù in ACM CCS, Denver, CO,
October 2015.
[24] K. Ali, A. Liu, W. Wang, and M. Shahzad, ‚ÄúKeystroke recognition using
wiÔ¨Å signals,‚Äù in ACM MobiCom, Paris, France, September 2015.
[25] M. Li, Y. Meng, J. Liu, H. Zhu, X. Liang, Y. Liu, and N. Ruan, ‚ÄúWhen
CSI meets public WiFi: Inferring your mobile phone password via WiFi
signals,‚Äù in ACM CCS, Vienna, Austria, October 2016.
J. Zhang, X. Zheng, Z. Tang, T. Xing, X. Chen, D. Fang, R. Li,
X. Gong, and F. Chen, ‚ÄúPrivacy leakage in mobile sensing: Your unlock
passwords can be leaked through wireless hotspot functionality,‚Äù Mobile
Information Systems, 2016.
[26]
[27] R. Bednarik, T. Kinnunen, A. Mihaila, and P. Fr√§nti, ‚ÄúEye-movements
as a biometric,‚Äù in SCIA, Copenhagen, Denmark, June 2005.
[28] O. Komogortsev, A. Karpov, and C. Holland, ‚ÄúCUE: counterfeit-
resistant usable eye movement-based authentication via oculomotor
plant characteristics and complex eye movement patterns,‚Äù in SPIE
Defense, Security, and Sensing, Baltimore, May 2012.
[29] C. Holland and O. Komogortsev, ‚ÄúComplex eye movement pattern
biometrics: Analyzing Ô¨Åxations and saccades,‚Äù in IAPR ICB, Madrid,
Spain, June 2013.
J. Sun, X. Chen, J. Zhang, Y. Zhang, and J. Zhang, ‚ÄúTouchIn: Sightless
two-factor authentication on multi-touch mobile devices,‚Äù in IEEE CNS,
San Francisco, CA, October 2014.
[30]
[31] T. Li, Y. Chen, J. Sun, X. Jin, and Y. Zhang, ‚ÄúiLock: Immediate and
automatic locking of mobile devices against data theft,‚Äù in ACM CCS,
Vienna, Austria, October 2016.
[32] A. D. Luca, R. Weiss, and H. Drewes, ‚ÄúEvaluation of eye-gaze in-
teraction methods for security enhanced pin-entry,‚Äù in ACM OZCHI,
Adelaide, Australia, November 2007.
[33] A. D. Luca, M. Denzel, and H. Hussmann, ‚ÄúLook into my eyes!: Can
you guess my password?‚Äù in ACM SOUPS, Mountain View, CA, July
2009.
[34] D. Liu, B. Dong, X. Gao, and H. Wang, ‚ÄúExploiting eye tracking for
smartphone authentication,‚Äù in ACNS, New York, NY, June 2015.
[35] Z. Li, M. Li, P. Mohapatra, J. Han, and S. Chen, ‚ÄúiType: Using eye