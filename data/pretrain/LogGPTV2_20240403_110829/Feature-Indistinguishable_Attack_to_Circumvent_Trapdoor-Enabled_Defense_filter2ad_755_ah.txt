pixel value ratio is 0.1 : 0.9 for a trapdoor trigger and an input image
when we merge the trapdoor with the input image to generate a
contaminated image. These trapdooor settings are summarized in
Table 15.
The YouTube Face dataset has a much larger image size and
many more categories than other datasets. The above trapdoor
parameters are slightly adjusted to t the characteristics of the
YouTube Face dataset: a trapdoor is a single 42 √ó 42 pattern placed
at the right bottom corner or ve 21 √ó 21 patterns placed randomly
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3173Figure 2: ROC curves for TeD and P-TeD to defend single category. Top row: TeD. Bottom row: P-TeD. Columns from left to
right: MNIST, CIFAR10, GTSRB, YouTube Face.
across the whole image, with the injection ratio set to 0.01 or 0.5
when a single trapdoor or multiple trapdoors are injected into a
DNN model. In both cases, the merge transparency is set to 0.2.
A.3 TeD - Single & All Categories (5% FPR of
Others)
In evaluating the attack performance of baseline attacks PGD and
C&W against the trapdoored defense when TeD is used to defend
a specic category ùê∂ùë° with a trapdoor (single category) and all
categories with one trapdoor per category (all categories), some
of obtained detection rates for PGD and C&W are signicantly
lower than those reported in [51] (see Section 6.2). The authors of
TeD don‚Äôt describe clearly how their false positive rate is calcu-
lated in paper [51] when detection threshold ùúôùë° is determined. By
examining their released code [50], we nd out that the released
code uses benign samples of categories other than the target cate-
gory, referred to as benign non-target samples, to calculate FPR to
determine detection threshold ùúôùë° at 5% FPR, which may lead to a
high false positive rate for benign target samples, as explained next
and conformed by our experimental results to be reported in this
subsection. After changing to using benign non-target samples to
calculate FPR, we can duplicate the attack performance of PGD and
C&W reported in [51].
We believe it is a bug to use benign non-target examples to cal-
culate FPR in their released code [50]. When a trapdoored model
is used to protect a specic category ùê∂ùë°, TeD should be used to
determine if an input is benign or adversarial only when the input
is classied into ùê∂ùë°. It cannot be used to detect an input when it is
classied into a category other than ùê∂ùë°. In other words, only sam-
ples classied into the category protected by the trapdoored model
matters. Since trapdoored samples and benign target samples are
all classied into ùê∂ùë°, while benign non-target samples are classied
ùêø
into categories other than ùê∂ùë°, benign non-target samples should
have lower cosine similarity values with the trapdoor signature,
which is the average of feature vectors of trapdoored samples, than
those of benign target samples with the trapdoor signature. This
means that detection threshold ùúôùë° determined by 5% FPR of be-
nign non-target samples would have a lower value than it should
be, leading to many benign target samples falsely detected. As we
will see from our experimental results reported in this subsection,
all benign target samples are classied as adversarial for GTSRB,
resulting in 100% FPR for benign target samples. In this case, FIA
cannot craft any adversarial example since it cannot nd any nega-
tive example in the preparation phase to determine proper target
F ùê∂ùë°
and boundary ùëêùëù to craft adversarial examples.
For completeness, we report the attack performance of our FIA,
PGD, and C&W for this setting in this subsection, although such
detection setting is useless practically due to its high FPR for benign
target samples. In our experimental evaluation for this setting, when
the FPR for benign target samples is too high that we cannot nd
the required number of negative target samples as described in
Section 5.2, we select a negative sample as the target in the feature
space for FIA to drive to.
Table 16 shows the detection rates of TeD protecting single
category and all categories on the four datasets when detection
threshold ùúôùë° is determined to be 5% FPR of benign non-target sam-
ples. We also report the corresponding false positive rates of benign
target samples. We note that the detection rate for FIA on GTSRB
is marked as N/A (not available) since all benign target samples are
detected (i.e. 100% false positive rate), we cannot nd any negative
sample for FIA to drive to.
From Table 16, we can see that the trapdoored defense has a very
high detection rate for both PGD and C&W, which agrees with the
results reported in [51], but a very low detection rate for adversarial
00.20.40.60.81False Positive Rate00.20.40.60.81True Positive RateC&W(AUC=0.99)FIA(AUC=0.14)PGD(AUC=0.99)00.20.40.60.81False Positive Rate00.20.40.60.81True Positive RateC&W(AUC=0.93)FIA(AUC=0.02)PGD(AUC=0.99)00.20.40.60.81False Positive Rate00.20.40.60.81True Positive RateC&W(AUC=0.43)FIA(AUC=0.04)PGD(AUC=0.90)00.20.40.60.81False Positive Rate00.20.40.60.81True Positive RateC&W(AUC=0.99)FIA(AUC=0.09)PGD(AUC=0.87)00.20.40.60.81False Positive Rate00.20.40.60.81True Positive RateC&W(AUC=0.99)FIA(AUC=0.31)PGD(AUC=0.99)00.20.40.60.81False Positive Rate00.20.40.60.81True Positive RateC&W(AUC=0.97)FIA(AUC=0.00)PGD(AUC=0.99)00.20.40.60.81False Positive Rate00.20.40.60.81True Positive RateC&W(AUC=0.98)FIA(AUC=0.02)PGD(AUC=0.98)00.20.40.60.81False Positive Rate00.20.40.60.81True Positive RateC&W(AUC=0.99)FIA(AUC=0.37)PGD(AUC=0.99)Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3174Layer (type)
conv_1 (Conv2D)
conv_2 (Conv2D)
conv_3 (Conv2D)
add_1 (Add)
conv_4 (Conv2D)
conv_5 (Conv2D)
add_2 (Add)
conv_6 (Conv2D)
conv_7 (Conv2D)
add_3 (Add)
conv_8 (Conv2D)
conv_9 (Conv2D)
conv_10 (Conv2D)
add_4 (Add)
conv_11 (Conv2D)
conv_12 (Conv2D)
add_5 (Add)
conv_13 (Conv2D)
conv_14 (Conv2D)
add_6 (Add)
conv_15 (Conv2D)
conv_16 (Conv2D)
conv_17 (Conv2D)
add_7 (Add)
conv_18 (Conv2D)
conv_19 (Conv2D)
add_8 (Add)
conv_20 (Conv2D)
conv_21 (Conv2D)
add_9 (Add)
avg_pool (AveragePooling2D)
dense_1 (Dense)
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
-
-
-
-
-
-
-
-
-
-
-
-
Softmax
conv_3, conv_1
conv_5, add_1
conv_7, add_2
conv_9, conv_10
conv_12, add_4
conv_14, add_5
-
conv_1
conv_2
add_1
conv_4
add_2
conv_6
add_3
conv_8
add_3
add_4
conv_11
add_5
conv_13
add_6
conv_15
add_6
add_7
conv_18
add_8
conv_20
conv_16, conv_17
conv_19, add_7
conv_21, add_8
add_9
avg_pooling
Table 14: The model architecture of ResNet20 for CIFAR10.
ResNet20 consists of residual blocks, where Conv2D layers
are skip-connected with Add-layer.
Activation
Connected to
Figure 3: Comparison of perceptual quality of adversarial ex-
amples crafted with FIA on TeD-protected models and PGD
on clean models using the same bound.
Table 16: Detection rates of TeD protecting both single cat-
egory and all categories with 5% FPR of benign non-target
samples and the corresponding FPR of benign samples in
the target category (target FPR).
Single Category / All Categories (%)
FIA
2.8 / 3.0
0.0 / 0.0
N/A
6.0 / 11.2
PGD
100 / 98.9
100 / 100
100 / 100
96.4 / 100
C&W
100 / 99.4
99.5 / 99.8
100 / 100
99.9 / 99.7
Target FPR
51.7 / 27.5
82.9 / 70.7
100 / 100
70.5 / 32.2
MNIST
CIFAR10
GTSRB
YtbFace
A.4 ROC Curves of TeD and P-TeD
To further compare detection performance of TeD and P-TeD on
FIA, PGD and C&W, we report the ROC curves and AUC scores for
TeD and P-TeD to defend single category in Fig. 2. From this gure,
we can see that P-TeD has very high AUC scores for both PGD
and C&W, at or above 0.97, on all the tested datasets, while TeD
has very high AUC scores for both PGD and C&W on MNIST and
CIFAR10, at or above 0.93. The AUC score of TeD for C&W is 0.43 on
GTSRB, much lower than its AUC scores on other datasets. Further
investigation reveals the cause: the cosine similarity distribution of
adversarial examples with the trapdoor signature and that of benign
target samples with the trapdoor signature in this case overlaps
much more than other datasets. The detection performance of C&W
Table 15: Trapdoor settings in TeD. Trans. and I.R. stand for
merge transparency and injection ratio of a trapdoor, respec-
tively. A trapdoor trigger is expressed as ùëõ √ó ùë§ √ó ‚Ñé, standing
for the number of pieces, and the weight and height of each
piece.
Defense Settings
Single category single trapdoor
All categories single trapdoor
All categories multiple trapdoors
Trigger Trans.
1 √ó 6 √ó 6
5 √ó 3 √ó 3
5 √ó 3 √ó 3
0.1
0.1
0.1
I.R.
0.1
0.5
0.5
examples crafted with our proposed FIA. At the same time, the FPR
of benign target samples is generally high. For GTSRB, this FPR is
100%, making FIA unable to craft any adversarial example since it
cannot nd a target to drive to.
originPGDFIAMNISTŒ¥=64/255GTSRBŒ¥=16/255CIFAR10Œ¥=8/255YouTube FaceŒ¥=16/255Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3175on GTSRB is signicantly boosted when the projected signature is
used: the AUC score of C&W on GTSRB increases to 0.98 for P-TeD.
On the other hand, Fig. 2 shows that the AUC scores of both TeD
and P-TeD are low for FIA, all at or below 0.37. FIA‚Äôs AUC scores on
MNIST and YouTube Face are signicantly higher than the other
two datasets, indicating that it is more dicult for FIA to craft
adversarial examples on MNIST and YouTube Face than the other
two datasets. If we relax their bounds, FIA‚Äôs attack performance
on these two datasets can be signicantly improved, at the cost
of more noisy adversarial examples. For example, if the bound is
increased from 16 to 32 for YouTube Face, the AUC scores reduce
to 0.01 for TeD and 0.02 for P-TeD.
A.5 Perceptual Qualify of Adversarial
Examples Crafted with FIA
Fig. 3 shows adversarial examples crafted with FIA on TeD-protected
models and with PGD on clean models (i.e., without TeD protection)
for dierent datasets. The same bound is used for FIA and PGD
on a dataset. We can see from the gure that adversarial examples
crafted with FIA look a little better than those crafted with PGD.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3176