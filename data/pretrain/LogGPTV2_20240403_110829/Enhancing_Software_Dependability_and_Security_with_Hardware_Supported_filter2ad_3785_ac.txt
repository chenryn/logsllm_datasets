it
To randomize call and return instructions, our control
ﬂow randomization approach analyzes the call instructions
in a program to determine if the return address can be
safely randomized. When the randomization approach de-
cides that
is safe to randomize the return address, we
have two options, one software based and one architecture
based. One option is to rewrite the call instruction and
replace it with a sequence of equivalent instructions that push
a randomized return address to the stack. For instance, a
call bar can be converted into two instructions, push
randomized_return_addr; followed by a jump instruc-
tion, jmp bar. This approach expands size of the original
program.
The second approach is to support randomized return
address automatically by implementing at architecture level
the mechanism to push the corresponding randomized re-
turn address to the stack. This option has the advantages
of being fully transparent to the randomized binary program
and at the same time maintaining the constant size for all
the call instructions with randomized return addresses. In
details, assume that a call instruction at address X, call
bar, is executed by the pipeline and the un-randomized return
address is X+4. When the instruction is executed, the processor
core will look up the randomization/de-randomization table
to ﬁnd out the randomized return address corresponding to
X+4. Then the processor core will push the randomized return
address instead of the original return address into the stack.
When this approach is applied, for each randomized return
address, the randomization/de-randomization table stores an
entry that maps the un-randomized return address to the
randomized return address. It is the randomized return ad-
dress that is pushed to the stack. For both options, when
the function returns to the caller, either randomized or un-
randomized return address will be popped from the stack
and used as the next program counter. If the popped address
is a randomized return address, it represents an instruction
address in the virtual control ﬂow address space. When the
instruction is fetched, the address will be de-randomized ﬁrst
using the randomization/de-randomization table and then the
255255
de-randomized address will be used for accessing the memory
hierarchy. If the popped address is un-randomized address,
the randomization/de-randomization table contains an entry
indicating that the address is un-randomized by clearing the
randomized tag. In this case, the next instruction after return
will be fetched using un-randomized address.
Similar to what are reported in the prior study, not all return
addresses can be safely randomized; as an example, return
addresses associated with indirect call are not randomized [9].
Furthermore, for x86 based binary, a call instruction is
often used for purpose other than invoking a subroutine. For
example, to support location independent code or data, it is
common to read the value of the instruction pointer (since, by
deﬁnition, the relative address is relative to the instruction’s
location). However, there is no instruction to obtain the value
of the instruction pointer on x86, a simple solution is to
execute a call instruction with the next instruction address
as the target location, which causes the target address to be
saved on stack. Then the next instruction can read the pushed
address from the stack by moving it to a register (e.g., ebx).
This can be achieved by using either a pop instruction or
a mov instruction. In this case, randomizing return address
may cause problems for the location independent code or data.
Another example is C++ exception handling [22]. In C++, the
exception handing routines use the return address to ﬁnd out
the exception handling codes by walking through the stack.
This is because at compile time, the C++ compiler cannot
decide if when a function makes a call to another function,
the callee will throw out an exception or not. As a result, the
compiler will put the exception cleanup code into the caller.
To randomize return address for C++ program with exception
handling, there are two choices. One is to modify the exception
handling tables to match with the control ﬂow randomization.
This is the approach used by Hiser et al. [9]. A second
approach is to modify the processor architecture in such a way
when exception happens, the original un-randomized return
address will be returned to the exception handling routines.
The details can be found in Section IV-C.
After all
the instructions are relocated,
the randomiza-
tion software will reassemble the instructions into a new
binary image. The new binary contains randomized control
ﬂow at instruction level and the associated randomization/de-
randomization tables. A processor with virtual control ﬂow
randomization support can execute the program using the new
binary image and the associated tables.
B. Micro-architectural Support for Executing Randomized Bi-
nary
There are many advantages of supporting our control ﬂow
randomization approach at micro-architectural level. These in-
clude, (i) eliminating the virtual machine that is often required
for software based ILR and consequently reducing the attack
surfaces; (ii) improved efﬁciency due to native and direct
execution of the control ﬂow randomized instructions; and (iii)
better performance of memory hierarchy (e.g., cache miss rate,
pre-fetch efﬁciency) brought by the concept of virtual control
ﬂow randomization (executing a binary program in randomized
control ﬂow instruction space but storing the program in the
memory hierarchy using un-randomized instruction memory
layout).
At micro-architecture level,
the processor maintains a
randomization/de-randomization layer that bridges the two
256256
Fig. 7. Block diagram of instruction fetch in randomized instruction space.
There are two program counters (PCs), one in the randomized space (RPC)
and the other one in un-randomized space (UPC). At architecture level, control
ﬂow follows the PC in the randomized space. Both PCs are used for instruction
fetch. When the original PC is absent, hardware will de-randomize RPC by
looking up the DRC lookup buffer. DRC is a small cache that stores address
pairs for both instruction address randomization and de-randomization. DRC
shares L2 with IL1.
instruction memory spaces (control ﬂow randomized versus
un-randomized). This mediation layer creates a virtual view
to the processor pipeline that the instructions are fetched and
executed in randomized instruction space. Depending on the
contexts, an instruction address may be randomized (e.g.,
retrieve a randomized return address from a call instruc-
tion) or de-randomized (e.g., fetching from level 1 instruction
cache using a randomized address). The system can maintain
mapping tables to store entries for randomization and/or de-
randomization. Similar to page tables, the tables for random-
ization and de-randomization are stored in the kernel as part of
the process context and protected from illegitimate accesses.
They cannot be accessed or modiﬁed by the application process
in user space.
At run time, entries of the randomization/de-randomization
tables can be cached on-chip using a DRC lookup buffer
(de-randomization cache). The DRC lookup buffer stores
frequently accessed randomization and/or de-randomization
translation entries, see Figure 7. The DRC lookup buffer can
be implemented as a cache (e.g., directly mapped). It acts as
a mediation layer between processor execution pipeline and
the memory hierarchy. There can be two buffers, one for
randomization and the other one for de-randomization. For
more efﬁcient usage of silicon resources, we use one uniﬁed
lookup buffer for storing entries of randomization and de-
randomization. For each entry, there is a single bit tag (derand
tag) indicating what kind of translation entry is stored. If the
tag is set, the entry is used for de-randomizing a randomized
address. Otherwise, it is used for randomization. In addition,
there is a valid bit. When the valid is clear, the entry is not
occupied.
Furthermore, for efﬁcient program execution, the instruc-
tion fetch unit contains two program counters (PCs), one for
the randomized instruction space (RPC) and the other one for
the un-randomized space (UPC). This means that UPC always
stores the de-randomized address of RPC. For un-randomized
address, UPC and RPC are the same. For un-randomized entry,
the DRC lookup buffer contains a tag to indicate that the
address is not randomized. At architecture level, control ﬂow
follows the PC in the control ﬂow randomized space. Both
PCs are used for instruction fetch. When UPC is absent, the
PCinrandomizedinstructionspaceByteQPCinoriginalinstructionspaceFetchLogicBTBRASIL1FrontendDe-RandomizationCache(DRC)IL1UnifiedL2CacheVLDbitVLDbitInstructionQueuebe made invisible to the user space instructions. This means
that during execution of an application, these address transla-
tion tables can only be accessed by the micro-architecture for
the purpose of handling access misses of DRC lookup buffer.
To modify these entries using instructions, the system needs
to switch to the kernel mode. A simple implementation of
this protection is to extend each entry of the TLB (translation
lookaside buffer) with a new page visibility bit. For a page, if
the visibility bit is set, it means that contents stored in the page
can be accessed by the user space instructions. Otherwise, it in-
dicates that the page is invisible to the application instructions.
The randomization and de-randomization translation tables are
stored in such pages invisible to the user space instructions.
According to the design, DRC cache lookup is only needed
when there is a randomized control ﬂow transfer and the
randomized target address requires de-randomization, which
is infrequent because branch prediction is performed using
the original memory space. Additionally, even there is a DRC
miss, majority of them can be found in the L2 cache, which
is large enough for storing the DRC table.
C. Support for Return Address Randomization
As discussed earlier, there are many scenarios that return
addresses cannot be safely randomized. It is not uncommon
that instructions in the callee directly access the return address
stored in the stack. One example is using a call instruction
to ﬁnd out
the current program counter for implementing
location independent code or data. Figure 9 shows for some
SPEC CPU2006 benchmark applications, the number of func-
tions with and without return instructions contained in the
callees. If a randomized return address is pushed to the stack,
and succeeding instructions directly access the randomized
return address and use it for computing address of location
independent code or data, it may lead to faulty execution. A
conservative approach is to apply return address randomization
only when the control ﬂow randomization software is certain
that a caller follows the conventional call return pattern. The
downside of this approach is that
it reduces the potential
randomness of the result binary image.
To maximize the chance of return address randomization,
we introduce an micro-architectural enhancement as shown in
Figure 10 that allows randomized return address to be saved to
the stack even when it may be directly accessed for supporting
C++ exception handling or location independent code/data.
When a randomized return address stored in the stack is
directly fetched into a register,
the micro-architecture will
automatically de-randomized it by looking up the DRC buffer.
Such design provides compatibility for address calculation
during C++ exception handling and usage of return address
Static analysis of function calls and returns for SPEC CPU2006
Fig. 9.
benchmark applications. The ﬁgure shows, for each benchmark, numbers of
functions with and without return instructions (functions without return
instructions may return to the caller using other x86 instructions such as mov).
Fig. 8. DRC cache organization and its usage for translating control ﬂow
addresses between randomized and the original instruction space. Instructions
are stored in on-chip caches using the original
layout, which results in
better cache performance than directly storing them in randomized space.
For a randomized PC (RPC), DRC converts the address into the original
location. The ﬁgure shows an example program in both randomized and the
original space. DRC stores necessary address translation pairs for executing
the program.
micro-architecture will de-randomize the address in RPC by
looking up the DRC lookup buffer.
Instructions are stored in on-chip caches using the original
layout as shown in Figure 8, which results in better cache per-
formance than storing them in randomized instruction space.
For a randomized PC (RPC), DRC converts the address into
the original address. Figure 8 shows an example program in
both randomized and the un-randomized space. DRC stores
necessary address translation pairs for executing the program.
Note that although the program is stored using the original
layout, the control ﬂow is modiﬁed. In addition, UPC cannot
be directly accessed by the instructions. It is automatically
updated by converting RPC or by the fetch unit.
is on-chip,
Since DRC lookup buffer
it has limited
space. Consequently, not all entries for randomization and de-
randomization can be stored in the DRC lookup buffer. One
option is to include a larger level two DRC lookup buffer.
However, for efﬁcient usage of cache space, DRC can share
its second level cache with the uniﬁed L2 of a processor core,
which is our current design. For an address that needs to be
de-randomized or randomized, if the corresponding translation
entry cannot be found in the DRC lookup buffer, the processor
core will search the next level memory hierarchy until the entry
is fetched. Such design eliminates the necessity of trapping
into the kernel when entries of the DRC lookup buffer need
to be updated. However,
it requires the tables for storing
randomization and de-randomization address translations to be
stored in paged memory. Dedicated memory pages can be used
to store these tables. We designed DRC as direct mapped cache
with small size to minimize power consumption. If there is a
DRC miss, L2 cache will be searched. The often small size
directly mapped DRC cache consumes very small amount of
energy. The design doesn’t require a fully-associative DRC
since the miss penalty is marginal; we will show the simulation
results in Section VII. In addition, the DRC is hidden from the
user space program with a simple extension to TLB.
To prevent any potential tamper of these tables by instruc-
tions executed under the application’s context, these pages can
257257
 movl -4(%ebp), %eax 0100   0104  ret 0108   010c  add  %eax, #1  0110   0114  ... Randomized instruction space  jeq  0108 0118   011c  cmp  %eax, #10 0120  call 02c8 0124  ... 0128  cmp  %eax, #10 0100  jeq  0114 0104  call 012c 0108  movl -4(%ebp), %eax 010c  add  %eax, #1 0110  ret 0114  ...  ... Original instruction space VLD 0100 010c derand 0c82 0108 010c 0120 0fc8 0114 0100 0100 derand derand rand derand 1 1 1 1 1 Type (derand/rand) Addr Tag Translation DRC Cache 0100002000030000ApplicationFuntions with Ret InstructionsFunctions without Ret InstrucitonNumber of instructions V. SECURITY ANALYSIS
In this section, we show the effectiveness of the proposed
method to enhance software dependability. According to the
deﬁnition of Jean-Claude Laprie’s work, “different emphasis
may be put on the different facets of dependability” such
as availability, reliability, safety, conﬁdentiality, integrity, and
maintainability [23]. Therefore, we analyze how secure our
framework is against ROP exploits since the security eventu-
ally includes availability, conﬁdentiality, and integrity.
A. ROP Attacks
In terms of randomizing control ﬂow at instruction level
and thwarting ROP based exploits, our approach is equivalent
with software based ILR [9]. Some main advantages of our
design over the prior art include native support for direct
execution of binary applications with control ﬂow diversiﬁ-
cation, and efﬁcient instruction fetch by preserving spacial
locality when instructions are stored in on-chip and off-chip
memory hierarchy. According to both our security evaluation
and the prior work [9], ILR can effectively mitigate arc-