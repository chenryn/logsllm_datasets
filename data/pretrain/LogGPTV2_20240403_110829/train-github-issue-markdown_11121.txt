# ðŸš€ Feature request
The current implementation of TokenClassificationPipeline cannot do anything
with text input beyond the length (measured in tokens) allowed by the model's
number of positional embeddings. I think it would be useful to many users to
have a setting in the pipeline that allows for a "sliding window" approach
that can take documents longer than the `.config.max_position_embeddings` of
the model in use.
The modified version of `TokenClassificationPipeline` could be instantiated
with the optional new params `window_size` (defaults to
`model.max_position_embeddings - 2` to accomodate the `[CLS]` and `[SEP]`
special tokens) and `stride` (defaults to `window_size / 2`, the user can
disable the sliding window by setting `stride = 0`) like so:
    model, tokenizer = # Some model, some tokenizer
    pipeline = TokenClassificationPipeline(model=model, tokenizer=tokenizer, aggregation_strategy='first', stride=254)
## Motivation
I run tasks that involve doing named entity recognition on longer documents. I
have to use my own modified subclass of `TokenClassificationPipeline` in order
to do so, since the existing class does not support processing texts longer
than the maximum number of position embeddings for the underlying models.
## Your contribution
I will submit a PR adding sliding window functionality to
`TokenClassificationPipeline`. I have previously created a sliding window
implementation of Transformers' NER Pipeline here, and will re-work this to
work with TensorFlow and meet the code standards of HuggingFace/Transformers.