threads we found in the four terminal groups of the responsive
companies: request granted, request explicitly rejected, exchange
terminated, and indeterminate.
Granted (n = 15). Positive responses spanned the gamut of both
enthusiasm and interest. For example, some companies responded
matter-of-factly (“You are welcome to experiment on it”) or referred
us to their publicly posted policies (“Since we are not looking at
specific legal entanglements or tight timeframes, then the targets
you’re looking for fall under our Bug Bounty and responsible dis-
closure policy.”) Many expressed concern that our study not impact
their systems or customers (“When investigating a vulnerability,
please, only ever target your own accounts and products. Never
attempt to access anyone else’s data and do not engage in any ac-
tivity that would be disruptive or damaging to your fellow users
or to Company.”) Others were explicitly supportive, and some even
hoped to turn our request into a sales opportunity: “If you guys are
interested in buying a targeted product on a discounted price...”
While a minority of the companies that granted our request
(13.3%) did not impose any conditions on the researcher’s testing,
four fifths of the respondents explicitly requested advance warning
in their communication with the researcher, with varying degrees
of forcefulness, ranging from, e.g., “It’s ok with us, you can perform
the tests. It will be great if you could share some of the results
with us” to “...provide Company with written notice of any secu-
rity vulnerabilities identified at least 60 days before making the
work/conclusions public...” and “it’s at Company’s sole discretion
about what...can be published.” Indeed, four companies requested
editorial control over the eventual published report of the findings.
Finally, two of the companies required that the researcher sign
an NDA, which often had explicit documentation requirements,
such as:
Company would like to ask that you and your team:
sign the attached NDA, provide Company full disclo-
sure with technical details on what you were able to
do and what tools/techniques were used to circum-
vent the controls on the tested product...
Rejected (n = 5). Companies that explicitly denied the researcher’s
request disclosed very little information about their reasoning. For
instance, 40% of them declined to “participate” in our study, even
though no cooperation was requested by the researcher. Another
40% mentioned internal policies and constraints, to varying degrees
of specificity. E.g., “We have internal constraints and contract re-
strictions that prevent us to give you authorization” and “We have
discussed this internally and regret we must decline your request.
As a policy, we don’t give permissions outside company-sponsored
hacks.” Finally, one company seemed to imply they deemed our
request overly broad:
As you can imagine, product security is of utmost
importance to Company, our customers and educa-
tional partners. After careful consideration, we have
made the decision to decline your blanket request
to waive the rights afforded to Company under the
Digital Millennium Copyright Act.
Indeterminate (n = 10). We were forced to code a significant
fraction of our interactions as ‘indeterminate’ because the conver-
sation seemed to be productive, but then dropped off and we were
unable to re-engage. Common responses of this flavor include “I’ll
work with the team to get you a response” and “I will forward this
over to the right people to see if they are interested.” Some seemed
genuinely interested, but were perhaps overruled by superiors:
We are always happy to work with security re-
searchers. Let me know what I can do to help. I’ve
included...our CTO on this, as well as...who runs our
mobile group. The three of us are...responsible for
security of Company’s products.
Terminated (n = 6). Most of the companies in this category (83.3%)
requested a phone call, and became unresponsive when the re-
searcher failed to arrange one. Separately, half of these companies
required further clarification of the testing plans and publishing
(perhaps as part of their phone call request), and were not satisfied
with the written details we sent. Perhaps not surprisingly, lawyers
were especially dogmatic in their requests:
Are you not willing to discuss this on the phone?
I have a number of specific questions which your
emails are not answering, since “coordinate” is not
very specific in its meaning, and I’m not sure what
you mean by deciding these things on a “case-by-case”
basis. I can’t recommend to my client that it accede to
your request for permission to test its product without
a firmer understanding of what input the company
might have on what is said in the report about the
company or its products.
As with the 16 companies for which we could not confirm receipt
of our request, we exclude these 10 terminated companies from the
remainder of our analysis, leaving 53 companies in our data set.
3.2 Sensitivity to test parameters
We analyzed the results to identify what factors, if any, seem to
influence the type or rate of response. In this section we report on
both our two explicit test parameters—researcher type and DMCA
mention—as well as three uncontrolled factors that appear to have
an influence on the type of response. Only the first, researcher type,
appears to have a statistically significant influence; the remaining
relations are intriguing but have weaker significant (likely, in part,
due to small sample sizes) and may be incidental (p > 0.2 in both bi-
nomial tests and a binary logistic regression on the other exogenous
variables).
3.2.1 Researcher type. Figure 3 shows that academic researchers
are significantly more likely to receive a positive response from
manufacturers (13 grants out of 33 requests, versus 2 out of 20); we
evaluated this assertion using a binomial test (p = 0.028). While it
Figure 3: Responsiveness by researcher type (n = 20, 33).
Figure 5: Responsiveness by company ownership (n = 32, 21).
Figure 4: Responsiveness by explicit mention of DMCA (n =
24, 29).
Figure 6: Responsiveness by publicly-available vulnerability
disclosure policy (n = 35, 18).
is true that the only rejections we received were to requests from
academics, we suspect this is biased by the fact that the academic
researchers were able to engage with the companies at a higher
rate; the vast majority of companies simply did not respond at all
to the independent researchers despite multiple contact attempts.
3.2.2 Explicit mention of DMCA. Mentioning the DMCA ex-
plicitly did not have a meaningful effect on the likelihood that a
company would initially respond to a request. Interestingly, though,
the fraction of responsive companies was slightly higher for those
contacts that did not mention the statute.
Figure 4 shows that explicitly mentioning the DMCA in the ini-
tial communication simultaneously reduces the likelihood that a
company will grant a request (7 out of 29 versus 8 out of 24), and
increases the likelihood of rejection (4 out of 29 as opposed to 1 out
of 24) though, as stated previous, these differences are not statisti-
cally significant. Academic and independent researchers initially
mentioned the DMCA with equal frequency, but the excluded com-
panies resulted in some disparity (15 mentions versus 18 omissions
for the academics; 9 vs. 11 for independents). The distributions
compared excluded companies that were contacted by email only
and did not respond, and those where we terminated the exchange.
3.2.3 Company ownership. While not an explicit test parameter
in our study, we observed a difference in response rate and char-
acter depending on the type of company involved. Figure 5 shows
the distribution of response classifications per company ownership.
We find that the 21 private companies in our study—regardless
of size—are far more likely to grant a request than the 32 public
companies, although the differences are not large enough to estab-
lish statistical signifiance. Here again the distribution was slightly
skewed between academics (24 public, 9 private) and independents
(8 public, 12 private).
3.2.4 Vulnerability disclosure policy. It seems logical that com-
panies that have already thought through the cost/benefit trade
offs inherent in third-party vulnerability research are likely to be
in a better position to handle our requests. Moreover, we suspect
companies that provide public evidence of having conducted such
a process, such as an explicit vulnerability disclosure policy or bug
bounty program, are even more likely to be predisposed to consider
our requests favorably. Hence, we classified companies based upon
the availability of a vulnerability disclosure policy. We find that the
18 firms with such a policy are both more likely to grant our request,
and less likely to reject it than the 35 companies in our data set that
do not (although without a strong enough difference to establish
0%20%40%60%80%100%AcademicIndependentPercentage of CompaniesGrantedIndeterminateUnresponsiveRejected0%20%40%60%80%100%MentionedNot MentionedPercentage of CompaniesGrantedIndeterminateUnresponsiveRejected0%20%40%60%80%100%PrivatePublicPercentage of CompaniesGrantedIndeterminateUnresponsiveRejected0%20%40%60%80%100%PolicyNo PolicyPercentage of CompaniesGrantedIndeterminateUnresponsiveRejectedthis finding at a significant level). Figure 6 shows the distribution
of responses; academics contacted 18 companies without a policy
and 15 with, independents 17 and 3, respectively.
3.2.5
Forwards to legal departments. Finally, while we have no
way to determine which companies forwarded our request to or
otherwise conferred with counsel before (or to determine whether
to) responding to our request, five companies included an email
chain in their response that indicated our request was explicitly
forwarded to their legal team. Four of them received a contact where
the DMCA was explicitly mentioned. Three of them granted the
request (conditionally). One was indeterminate. One got terminated
due to a phone call request. One was contacted by senior faculty
via phone call (request granted). Three were contacted by junior
faculty (one granted, one indeterminate, one rejected). One was
contacted by the E.U. independent researcher (granted).
4 SENTIMENT SURVEY
While our authorization study indicates that a great many com-
panies are loathe to remove legal risks from security researchers
(particularly those without an academic institution backing them)
this does not necessarily mean that these risks are foreclosing poten-
tial research. Indeed, measuring a “chilling effect” poses a challenge:
a researcher may decide not to carry out a security evaluation for a
combination of reasons whose disparate influences may be hard to
untangle even for the researcher herself. We set out to measure both
the perceived risk of legal action as well as the real incidence of legal
threats by surveying5 a broad population of security researchers.
4.1 Survey methodology
To contact as many vulnerability researchers as possible, we col-
lected the set of participants in publicly-available archives of
vulnerability-related mailing lists [16] that were no older than 2014.
We then inspected this set and removed any individuals, such as
well-known journalists, that we knew were not themselves engaged
in vulnerability research. In an attempt to further exclude individ-
uals who do not self-identify as being engaged in vulnerability
research, the first page of the survey asks:
• Q1. Does your research include the study of computer security
vulnerabilities?
If the participant answers No, the survey ends. Otherwise, the sur-
vey proceeds to the next page. The text of the participant recruiting
message was
Dear Participant,
We are a group of faculty and researchers at UC
San Diegoś Computer Science and Engineering
Department. Currently, weŕe doing a study on the
challenges faced by security vulnerability researchers,
and would greatly appreciate your help. Please
consider filling out this short survey∗, to help us
understand your experience. Estimated time to
completion is 1–5 minutes.
Figure 7: Top: Likert scale, which participants used to ex-
press agreement or disagreement with five statements (ap-
pearing in random order) regarding impediments to vul-
nerability research: four distractors and the tracking vari-
able, “concerns with legal challenges.” Each slider had to be
clicked on in order to advance to the next page, so partici-
pants had to explicitly leave the bar in the middle to con-
tinue. Bottom: histogram of the aggregate responses to all
of the Likert-scale questions.
Apologies if you receive this message more than
once; please ignore any additional copies of this
request.
More information about our research is available at
http://www.evidencebasedsecurity.org/
5As with our evaluation of company responses, our survey also received IRB approval,
and was declared exempt due to the low risk and anonymous nature of the survey.
where the footnote was a hyperlink to Question 1 (collected via the
SurveyMonkey platform).
4.1.1 Relative influence of legal risks. To determine whether
the perceived risk of legal action was a deterrent, we asked sur-
vey participants a series of five questions about factors that might
influence their decision to undertake a particular vulnerability re-
search project. Both the questions (which appeared in randomized
order) and the response distributions are shown in Figure 7; an-
swers were recorded on a Likert scale [11] with extremes marked
Strongly disagree and Strongly agree and a neutral position marked
Neither agree nor disagree. While potentially interesting in them-
selves, for our purposes four of these five questions serve simply as
distractors that allow us to calibrate the responses to the tracking
question (“concerns with legal challenges”) and to avoid leading
the participants.
4.1.2 Past fear of legal risks. After responding to the first set
of Likert-scale questions, participants are taken to a new page and
asked two sets of yes/no questions, in order to gauge both their
subjective evaluation of the impact of legal challenges on their
research, and their factual experience with legal threats or actions.
In the first set, participants are asked:
• Q7a. Have you ever feared legal action (e.g. a cease-and-desist
letter, or a civil lawsuit) as a consequence of your vulnerability-
related research?
• Q7b. Has this fear of legal action prevented you from engaging
in, or prompted you to modify, a research project? (shown if
answer to Q7a was Yes)
Participants that answered Yes to the second question were offered
a chance to explain their answer in a text box.
4.1.3 Experiences of legal threats and action. The second set of
yes/no questions attempted to elicit factual information about legal
action faced by the participant:
• Q8a. Have you ever been threatened with legal action (e.g.
have you received a cease-and-desist letter from a man-
ufacturer or copyright owner) as a consequence of your
vulnerability-related research?
• Q8b. Have you ever been a named defendant in a court of
law (e.g. have you been sued) as a consequence of your
vulnerability-related research?
Participants that answered Yes to either question were offered a
chance to describe their experience in a text box.
4.2 Responses
Of the 1,369 individuals invited to participate in the survey, 158
(11.5%) visited the survey page and 139 (88%) of those responded
Yes to the first, qualifying question self-reporting as vulnerability
researchers. Some of these respondents did not complete any further
questions and were removed from the sample, leading to a final set
of 110 respondents.
4.2.1 Relative influence of legal risks. In order to evaluate
whether or not researchers were likely to single out legal chal-
lenges as an obstacle to research, we compare the distribution of
the tracking variable with the combined distribution of the four dis-
tractors. The continuous slider provided integer response values in
the range [−50, 50] as shown on bottom of Figure 7. The responses
Figure 8: The empirical cumulative distribution functions
of the raw Likert-scale observations for the tracking vari-
able and each of the distractors. Dashed vertical lines depict
the bin cutoffs as well as the starting slider position.
show clear modes at the extremes (-50 and 50) and at 0, with less
pronounced modes at values between the extremes and 0. Figure 8
shows the cumulative distribution of Likert-scale responses for each
of the four distractors (black lines) and the tracking variable (blue
line), which further illustrates the modes. The presence of these
modes strongly suggest the observations can be binned into five
discrete categories corresponding to the extremes, 0, and values
between the extremes and 0. Figure 9 shows the responses binned
into these five categories with breaks at -30, -10, 10 and 30.
The top four histograms in Figure 9 show the distribution of the
distractor variables and the fifth the distribution of the tracking
variable. The bottom histogram shows the difference in the response
counts between the tracking variable and the average response
count for the same bin across all four distractors. For example,
there were an average of 35.5 responses that fell into the Strongly
disagree bin received across the four distractor variables, and 40
Strongly disagree responses for the tracking variable; the difference,
4.5, is shown in the first bar. Figure 9 also shows the 95% confidence
interval for the responses as lines extended above and below the
top of the histogram. For any given response bin, the confidence
interval shows the range of possible response frequencies for which
the observed mean has a probability of at least 5%.
Participants were 1.5× more likely to indicate strong agreement
with the statement they chose not to study a target because of
concerns with legal challenges than they were for the distractors,
a statistically significant difference (p = 0.034). In fact, for 14%
of participants the tracking question was the only Strongly agree
among the five Likert-scale questions.
We also observe that the tracking variable produces a more
polarized response, with a greater number of responses falling
into the Strongly disagree or Strongly agree bins than for the track-
ing variables. In order to statistically test that increased polariza-
tion in the raw distributions, we used a Levene’s test (carried out
on the full [−50, 50] range, rather than binned responses) which
showed a statistically significant (p < 0.001) difference in the vari-
ance of the tracking variable compared to the distractor variables
-50-30-10103050RawSliderPosition0.00.10.20.30.40.50.60.70.80.91.0CumulativeDistributionFunctionDistractorsTrackingVariableAgreementBinsDefaultSliderPositionNull hypothesis
T ∼ D{1,2,3,4}
T ∼ D1
T ∼ D2
T ∼ D3
T ∼ D2
D1 ∼ D2
D1 ∼ D3
D1 ∼ D4