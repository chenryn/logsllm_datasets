We will discuss the policy reﬁnement and implicit
ﬂows in Section 7. As for external libraries, the best ap-
proach is to transform them, so that the need for summa-
rization can be eliminated. If this cannot be done, then
our transformation will identify all the external functions
that are used by an application, so that errors of omission
can be avoided. However, if a summarization function is
incorrect, then it can lead to false negatives, false posi-
tives, or both.
6.4 Performance
Figure 6 and 7 show the performance overheads,
when the original and transformed programs were
compiled using gcc 3.2.2 with -O2, and ran on a
1.7GHz/512MB/Red Hat Linux 9.0 PC.
For server programs, the overhead of our approach
is low. This is because they are I/O intensive, whereas
our transformation adds overheads only to code that per-
forms signiﬁcant amount of data copying within the pro-
gram, and/or other CPU-intensive operations. For CPU-
intensive C programs, the overhead is between 61% to
106%, with an average of 76%.
6.4.1 Effect of Optimizations.
The optimizations
discussed in Section 3.3 have been very effective. We
comment further in the context of CPU-intensive bench-
marks.
• Use of local taint variables reduced the overheads by
42% to 144%. This is due to the reasons mentioned
earlier: compilers such as gcc are very good in opti-
mizing operations on local variables, but do a poor job
on global arrays. Thus, by replacing global tagmap
accesses with local tag variable accesses, signiﬁcant
performance improvement can be obtained.
Most programs access local variables much more fre-
quently than global variables. For instance, we found
out (by instrumenting the code) that 99% of accesses
made by bc are to local variables. A ﬁgure of 90%
is not at all uncommon. As a result, the introduction
of local tag variables leads to dramatic performance
improvement for such programs. For programs that
access global variables frequently, such as gzip that
has 41% of its accesses going to global variables, the
performance improvements are less striking.
• tagmap optimizations are particularly effective for
USENIX Association
Security ’06: 15th USENIX Security Symposium
131
programs that operate mainly on integer data. This is
because of the use of 2-bit taint tags, which avoids the
need for bit-masking and shifts to access taint informa-
tion. As a result we see signiﬁcant overhead reduction
in the range of 7% to 466%.
• Intraprocedural analysis and optimization further re-
duces the overhead by up to 5%. The gains are mod-
est because gcc optimizations have already eliminated
most local tag variables after the previous step.
When combined, these optimizations reduce the over-
head by a factor of 2 to 5.
7 Discussion
7.1 Support for Implicit Information Flow
Implicit information ﬂow occurs when the values of cer-
tain variables are related by virtue of program logic, even
though there are no assignments between them. A classic
example is given by the code snippet [25]:
x=x%2; y=0; if (x==1) y=1;
Even though there is no assignments involving x and y,
their values are always the same. The need for tracking
such implicit ﬂows has long been recognized. [11] for-
malized implicit ﬂows using a notion of noninterference.
Several recent research efforts [18, 30, 20] have devel-
oped techniques based on this concept.
Noninterference is a very powerful property, and can
capture even the least bit of correlation between sensitive
data and other data. For instance, in the code:
if (x > 10000) error = true;
if (!error) { y = "/bin/ls"; execve(y); }
there is an implicit ﬂow from x to error, and then to
y. Hence, a policy that forbids tainted data to be used
as an execve argument would be violated by this code.
This example illustrates why non-interference may be
too conservative (and hence lead to false positives) in our
application. In the context of the kinds of attacks we are
addressing, attackers usually need more control over the
value of y than the minimal relationship that exists in the
code above. Thus, it is more appropriate to track explicit
ﬂows. Nevertheless, there can be cases where substantial
information ﬂow takes place without assignments, e.g.,
in the following if-then-else, there is a direct ﬂow of in-
formation from x to y on both branches, but our formu-
lation of explicit information ﬂow would only detect the
ﬂow in the else statement.
if (x == 0) y = 0; else y = x;
The goal of our approach is to support those implicit
ﬂows where the value of one variable determines the
value of another variable. By using this criteria, we seek
a balance between tracking necessary data value propa-
gation and minimizing false positives. Currently, our im-
plementation supports two forms of implicit ﬂows that
appear to be common in C programs.
• Translation tables. Decoding is sometimes imple-
mented using a table look up, e.g.,
y = translation_tab[x];
where translation tab is an array and x is a byte of
input. In this case, the value of x determines the value
of y although there is no direct assignment from x to
y. To handle this case, we modify the basic transfor-
mation so that the result of an array access is marked
as tainted whenever the subscript is tainted. This suc-
cessfully handles the use of translation tables in the
PHP interpreter.
• Decoding using if-then-else/switch. Sometimes, de-
coding is implemented using a statement of the form:
if (x == ’+’) y = ’ ’;
(Such code is often used for URL-decoding.) Clearly,
the value of y can be determined by the value of x.
More generally, switch statements could be used to
translate between multiple characters. Our transfor-
mation handles them in the same way as a series of
if-then-else statements. Speciﬁcally, consider an if-
then-else statement of the form:
if (x == E) {
... y = E 0; ... }
If E and E 0 are constant-valued, then we add a tag
update tag(y) = tag(x) immediately before the as-
signment to y.
While our current technique seems to identify some of
the common cases where implicit ﬂows are signiﬁcant, it
is by no means comprehensive. Development of a more
systematic approach that can provide some assurances
about the kinds of implicit ﬂows captured, while ensur-
ing a low false positive rate, is a topic of future research.
7.2 Policy Reﬁnement
Policy development effort is an important concern with
any policy enforcement technique. In particular, there is
a trade-off between policy precision and the level of ef-
fort required. If one is willing to tolerate false positives,
policies that produce very few false negatives can be de-
veloped with modest effort. Alternatively, if false neg-
atives can be tolerated, then false positives can be kept
to a minimum with little effort. To contain both false
positives and false negatives, more effort needs to be
spent on policy development, taking application-speciﬁc
or installation-speciﬁc characteristics.
The above remarks about policy-based techniques are
generally applicable to our approach as well. For the for-
mat string attack, we used a policy that tended to err on
the side of producing false positives, by disallowing all
use of tainted format directives. However, it is conceiv-
able that some applications may be prepared to receive a
subset of format directives in untrusted inputs, and han-
dle them correctly. In such cases, this application knowl-
edge can be used by a system administrator to use a less
132
Security ’06: 15th USENIX Security Symposium
USENIX Association
restrictive policy, e.g., allowing the use of format direc-
tives other than %n. This should be done with care, or
else it is possible to write policies that prevent the use of
%n, but allow the use of variants such as %5n that have
essentially the same effect. Alternatively, the policy may
be relaxed to permit speciﬁc exceptions to the general
rule that there be no format directives, e.g., the rule:
vfprintf(f mt) |
f mt matches any∗ (Format)T any∗ &&
(!(fmt matches "[ˆ%]*%s[ˆ%]*")) → reject()
allows the use of a single %s format directive from un-
trusted sources, in addition to permitting format strings
that contain untainted format directives.
The directory traversal policy also tends to err on the
side of false positives, since it precludes all accesses
outside the authorized top level directories (e.g. Docu-
mentRoot and cgi-bin) of a web server if components of
the ﬁle name being accessed are coming from untrusted
sources. In devising this policy, we relied on application-
speciﬁc knowledge, namely, the fact that web servers do
not allow clients to access ﬁles outside the top level di-
rectories speciﬁed in the server conﬁguration ﬁle. An-
other point to be noted about this policy is that variants
of directory traversal attack that do not escape these top
level directories, but simply attempt to fool per-directory
access controls, are not addressed by our policy.
The control-ﬂow hijack policy is already accurate
enough to capture all attacks that use corruption of code
pointers as the basis to alter the control-ﬂow of programs,
so we proceed to discuss the SQL injection policy. The
policy shown in Figure 4 does not address attacks that
inject only SQL keywords (e.g., the UNION operation)
to alter the meaning of a query. This can be addressed by
a policy based on tokenization. The idea is to perform a
lexical analysis on the SQL query to break it up into to-
kens. SQL injection attacks are characterized by the fact
that multiple tokens appear in the place of one, e.g., mul-
tiple keywords and meta-characters were provided by the
attacker in the place of a simple string value in the attack
examples discussed earlier in the paper. Thus, systematic
protection from SQL injections can be obtained using a
policy that prevents tainted strings from spanning mul-
tiple tokens. A similar approach is suggested in [24],
although the conditions are not deﬁned as precisely. Su
et al [27] provide a formal characterization of SQL injec-
tion using a syntax analysis of SQL queries. The essen-
tial idea is to construct a parse tree for the SQL query, and
to examine its subtrees. For any subtree whose root is
tainted, all the nodes below that subtree should be tainted
as well. In other words, tainted input cannot straddle dif-
ferent syntactic constructs. This is a further reﬁnement
over the characterization we suggest, where tainted input
should not straddle different lexical entities.
Command injection attacks are similar to SQL injec-
tion attacks in many ways, and hence a tokenization-
based policy may be a good choice for them as well. For
this reason, we omit a detailed discussion of command
injection policies. Nevertheless, it should be mentioned
that there are some differences between SQL and com-
mand injection, e.g., shell syntax is much more complex
than SQL syntax. Moreover, we may want to restrict the
command names so that they are not tainted.
Note that tokenization is a lexical analysis task that
is (almost invariably) implemented using regular expres-
sion based speciﬁcations. Thus, the above tokenization-
based policy is amenable to expression using our policy
language. One could argue that a regular expression to
recognize tokens would be complex, and hence a policy
may end up using a simpler approximation to tokeniza-
tion. This discussion shows that the usual trade-off in
policy based attack detection between accuracy and pol-
icy complexity continues in the case of taint-enhanced
policies as well. Nevertheless, it should be noted that for
a given policy development effort, taint-enhanced poli-
cies seem to be signiﬁcantly more accurate than policies
that do not incorporate any knowledge about taint.
Finally, we discuss
the cross-site scripting at-
The policy discussed earlier does not ad-
tack.
dress variations of
the basic attack, e.g., attack-
ers can evade this policy by injecting the malicious
script code in “onmouseover=malicious()” or “”, which is not a
block enclosed by the script tag. To detect these XSS
variations, one has to understand the different HTML tag
patterns in which a malicious script can be injected into
dynamic HTML pages, and develop policies to prevent
the use of such tainted patterns in HTML outputs.
In summary, although the example policies shown in
Figure 4 were able to stop the attacks in our experi-
ments, many of them need further improvement before
they can stand up to skilled attackers that are knowl-
edgeable about the policies being enforced. We outlined
the ways to improve some of these policies, but a com-
prehensive solution to the policy development problem
is not really the focus or contribution of this paper. In-
stead, our contribution is to show the feasibility and prac-
ticality of using ﬁne-grained taint information in devel-
oping policy-based attack protection. The availability of
ﬁne-grained taint information makes our policies signif-
icantly more precise than traditional access-control poli-
cies. Moreover, our approach empowers system admin-
istrators and security professionals to update and reﬁne
these policies to improve protection, without having to
wait for the patches of a newly discovered attack avenue.
USENIX Association
Security ’06: 15th USENIX Security Symposium
133
8 Related Work
Memory Error Exploit Detection. Buffer overﬂows
and related memory errors have received a lot of atten-
tion, and several efﬁcient techniques have been devel-
oped to address them. Early approaches such as Stack-
Guard [7] and ProPolice [9] focused on just a single class
of attacks. Recently, more general techniques based on
randomization have been developed, and they promise to
defend against most memory error exploits [16, 2], How-
ever, due to the nature of the C language, these meth-
ods still cannot detect certain types of attacks, e.g., over-
ﬂows from an array within a structure to an adjacent vari-
able. Fine-grained taint analysis can capture these at-
tacks whenever the corrupted data is used as an argument
in a sensitive operation. (This is usually the case, since
the goal of an attacker in corrupting that data was to per-
form a security-sensitive operation.) Although our over-
heads are generally higher than the techniques mentioned
above, we believe that they are more than compensated
by the increase in attack coverage.
Fine-Grained Taint Analysis. The key distinctions
between our work and previous ﬁne-grained taint anal-
ysis techniques of [22, 28, 5] were already discussed in
the introduction, so we limit our discussion to the more
technical points here. As mentioned earlier, [28, 5] rely
on hardware support for taint-tracking. [22] is closer to
our technique than these two techniques. It has an advan-
tage over ours in that it can operate on arbitrary COTS bi-
naries, whereas we require access to the C source code.
This avoids problems such as hand-written assembly
code. Their main drawback is performance: on the ap-
plication Apache that they provide performance numbers
on, their overheads are more than 100 times higher than
ours. This is because (a) they rely on Valgrind, which in
itself introduces more than 40 times overheads as com-
pared to our technique, and (b) they are constrained by
having to work on binary code, and without the beneﬁt of
static analyses and optimizations that have gone into our
work. (Here, we are not only referring to our own analy-
ses and optimizations, but also many of the optimizations
implemented in the GCC compiler that we used to com-
pile the transformed programs.)
There are several other technical differences between
our work and that of [22]. For instance, they track 32-bits
of taint information for each byte of data, whereas we