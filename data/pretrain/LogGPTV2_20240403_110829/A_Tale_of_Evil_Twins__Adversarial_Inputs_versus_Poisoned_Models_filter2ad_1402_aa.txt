title:A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models
author:Ren Pang and
Hua Shen and
Xinyang Zhang and
Shouling Ji and
Yevgeniy Vorobeychik and
Xiapu Luo and
Alex X. Liu and
Ting Wang
0
2
0
2
v
o
N
1
2
]
G
L
.
s
c
[
3
v
9
5
5
1
0
.
1
1
9
1
:
v
i
X
r
a
A Tale of Evil Twins:
Adversarial Inputs versus Poisoned Models
Ren Pang
PI:EMAIL
Pennsylvania State University
Shouling Ji
PI:EMAIL
Zhejiang University, Ant Financial
Hua Shen
PI:EMAIL
Pennsylvania State University
Yevgeniy Vorobeychik
PI:EMAIL
Washington University in St. Louis
Xinyang Zhang
PI:EMAIL
Pennsylvania State University
Xiapu Luo
PI:EMAIL
Hong Kong Polytechnic University
Alex Liu
PI:EMAIL
Ant Financia
Ting Wang
PI:EMAIL
Pennsylvania State University
ABSTRACT
Despite their tremendous success in a range of domains, deep learn-
ing systems are inherently susceptible to two types of manipula-
tions: adversarial inputs – maliciously crafted samples that deceive
target deep neural network (DNN) models, and poisoned models
– adversely forged DNNs that misbehave on pre-defined inputs.
While prior work has intensively studied the two attack vectors in
parallel, there is still a lack of understanding about their fundamen-
tal connections: what are the dynamic interactions between the
two attack vectors? what are the implications of such interactions
for optimizing existing attacks? what are the potential countermea-
sures against the enhanced attacks? Answering these key questions
is crucial for assessing and mitigating the holistic vulnerabilities of
DNNs deployed in realistic settings.
Here we take a solid step towards this goal by conducting the
first systematic study of the two attack vectors within a unified
framework. Specifically, (i) we develop a new attack model that
jointly optimizes adversarial inputs and poisoned models; (ii) with
both analytical and empirical evidence, we reveal that there exist
intriguing “mutual reinforcement” effects between the two attack
vectors – leveraging one vector significantly amplifies the effective-
ness of the other; (iii) we demonstrate that such effects enable a
large design spectrum for the adversary to enhance the existing
attacks that exploit both vectors (e.g., backdoor attacks), such as
maximizing the attack evasiveness with respect to various detection
methods; (iv) finally, we discuss potential countermeasures against
such optimized attacks and their technical challenges, pointing to
several promising research directions.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’20, November 9–13, 2020, Virtual Event, USA
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7089-9/20/11...$15.00
https://doi.org/10.1145/3372297.3417253
CCS CONCEPTS
• Security and privacy; • Computing methodologies → Ma-
chine learning;
KEYWORDS
Adversarial attack; Trojaning attack; Backdoor attack
ACM Reference Format:
Ren Pang, Hua Shen, Xinyang Zhang, Shouling Ji, Yevgeniy Vorobeychik,
Xiapu Luo, Alex Liu, and Ting Wang. 2020. A Tale of Evil Twins: Adversarial
Inputs versus Poisoned Models. In Proceedings of the 2020 ACM SIGSAC
Conference on Computer and Communications Security (CCS ’20), November
9–13, 2020, Virtual Event, USA. ACM, New York, NY, USA, 15 pages. https:
//doi.org/10.1145/3372297.3417253
1 INTRODUCTION
The abrupt advances in deep learning have led to breakthroughs in
a number of long-standing machine learning tasks (e.g., image clas-
sification [14], natural language processing [42], and even playing
Go [45]), enabling scenarios previously considered strictly experi-
mental. However, it is now well known that deep learning systems
are inherently vulnerable to adversarial manipulations, which sig-
nificantly hinders their use in security-critical domains, such as
autonomous driving, video surveillance, web content filtering, and
biometric authentication.
Two primary attack vectors have been considered in the litera-
ture. (i) Adversarial inputs – typically through perturbing a benign
input 𝑥, the adversary crafts an adversarial version 𝑥∗ which de-
ceives the target DNN 𝑓 at inference time [7, 20, 40, 48]. (ii) Poisoned
models – during training, the adversary builds malicious functions
into 𝑓 , such that the poisoned DNN 𝑓∗ misbehaves on one (or more)
pre-defined input(s) 𝑥 [24, 25, 44, 47]. As illustrated in Figure 1, the
two attack vectors share the same aim of forcing the DNN to mis-
behave on pre-defined inputs, yet through different routes: one
perturbs the input and the other modifies the model. There are
attacks (e.g., backdoor attacks [21, 32]) that leverage the two attack
vectors simultaneously: the adversary modifies 𝑓 to be sensitive
to pre-defined trigger patterns (e.g., specific watermarks) during
training and then generates trigger-embedded inputs at inference
time to cause the poisoned model 𝑓∗ to malfunction.
modifies the DNN at the cost of “specificity” (whether the attack
influences non-target inputs).
RA2 – Through empirical studies on benchmark datasets and in
security-critical applications (e.g., skin cancer screening [16]), we
reveal that the interactions between the two attack vectors demon-
strate intriguing “mutual-reinforcement” effects: when launching
the unified attack, leveraging one attack vector significantly ampli-
fies the effectiveness of the other (i.e., “the whole is much greater
than the sum of its parts”). We also provide analytical justification
for such effects under a simplified setting.
RA3 – Further, we demonstrate that the mutual reinforcement
effects entail a large design spectrum for the adversary to optimize
the existing attacks that exploit both attack vectors (e.g., back-
door attacks). For instance, leveraging such effects, it is possible
to enhance the attack evasiveness with respect to multiple defense
mechanisms (e.g., adversarial training [34]), which are designed to
defend against adversarial inputs or poisoned models alone; it is
also possible to enhance the existing backdoor attacks (e.g., [21, 32])
with respect to both human vision (in terms of trigger size and
transparency) and automated detection methods (in terms of input
and model anomaly).
RA4 – Finally, we demonstrate that to effectively defend against
such optimized attacks, it is necessary to investigate the attacks
from multiple complementary perspectives (i.e., fidelity and speci-
ficity) and carefully account for the mutual reinforcement effects in
applying the mitigation solutions, which point to a few promising
research directions.
To our best knowledge, this work represents the first systematic
study of adversarial inputs and poisoned models within a unified
framework. We believe our findings deepen the holistic understand-
ing about the vulnerabilities of DNNs in practical settings and shed
light on developing more effective countermeasures.1
2 PRELIMINARIES
We begin by introducing a set of fundamental concepts and assump-
tions. Table 5 summarizes the important notations in the paper.
2.1 Deep Neural Networks
Deep neural networks (DNNs) represent a class of machine learn-
ing models to learn high-level abstractions of complex data using
multiple processing layers in conjunction with non-linear trans-
formations. We primarily consider a predictive setting, in which a
DNN 𝑓 (parameterized by 𝜃) encodes a function 𝑓 : X → Y. Given
an input 𝑥 ∈ X, 𝑓 predicts a nominal variable 𝑓 (𝑥; 𝜃) ranging over
a set of pre-defined classes Y.
We consider DNNs obtained via supervised learning. To train
a model 𝑓 , the training algorithm uses a training set D, of which
each instance (𝑥, 𝑦) ∈ D ⊂ X × Y comprises an input 𝑥 and its
ground-truth class 𝑦. The algorithm determines the best parameter
configuration 𝜃 for 𝑓 via optimizing a loss function ℓ(𝑓 (𝑥; 𝜃), 𝑦)
(e.g., the cross entropy of 𝑦 and 𝑓 (𝑥; 𝜃)), which is typically imple-
mented using stochastic gradient descent or its variants [55].
1The source code and data are released at https://github.com/alps-lab/imc.
Figure 1: “Duality” of adversarial inputs and poisoned models.
Prior work has intensively studied the two attack vectors sep-
arately [7, 20, 24, 25, 40, 44, 47, 48]; yet, there is still a lack of un-
derstanding about their fundamental connections. First, it remains
unclear what the vulnerability to one attack implies for the other.
Revealing such implications is important for developing effective
defenses against both attacks. Further, the adversary may exploit
the two vectors together (e.g., backdoor attacks [21, 32]), or mul-
tiple adversaries may collude to perform coordinated attacks. It
is unclear how the two vectors may interact with each other and
how their interactions may influence the attack dynamics. Under-
standing such interactions is critical for building effective defenses
against coordinated attacks. Finally, studying the two attack vectors
within a unified framework is essential for assessing and mitigating
the holistic vulnerabilities of DNNs deployed in practice, in which
multiple attacks may be launched simultaneously.
More specifically, in this paper, we seek to answer the following
research questions.
• RQ1 – What are the fundamental connections between adversarial
inputs and poisoned models?
• RQ2 – What are the dynamic interactions between the two attack
vectors if they are applied together?
• RQ3 – What are the implications of such interactions for the adver-
sary to optimize the attack strategies?
• RQ4 – What are the potential countermeasures to defend against
such enhanced attacks?
Our Work. This work represents a solid step towards answering
the key questions above. We cast adversarial inputs and poisoned
models within a unified framework, conduct a systematic study of
their interactions, and reveal the implications for DNNs’ holistic
vulnerabilities, leading to the following interesting findings.
RA1 – We develop a new attack model that jointly optimizes
adversarial inputs and poisoned models. With this framework, we
show that there exists an intricate “duality” relationship between