# A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models

**Authors:**
- Ren Pang, Pennsylvania State University
- Hua Shen, Pennsylvania State University
- Xinyang Zhang, Pennsylvania State University
- Shouling Ji, Zhejiang University, Ant Financial
- Yevgeniy Vorobeychik, Washington University in St. Louis
- Xiapu Luo, Hong Kong Polytechnic University
- Alex X. Liu, Ant Financial
- Ting Wang, Pennsylvania State University

**Abstract:**
Despite the remarkable success of deep learning systems across various domains, these models are inherently vulnerable to two types of manipulations: adversarial inputs—maliciously crafted samples that deceive target deep neural networks (DNNs), and poisoned models—adversely forged DNNs that misbehave on pre-defined inputs. While previous research has extensively studied these attack vectors in parallel, there is a lack of understanding about their fundamental connections. This paper aims to address key questions regarding the dynamic interactions between these attack vectors, the implications for optimizing existing attacks, and potential countermeasures against enhanced attacks.

In this work, we take a significant step towards this goal by conducting the first systematic study of both attack vectors within a unified framework. Specifically, we:
1. Develop a new attack model that jointly optimizes adversarial inputs and poisoned models.
2. Reveal, through both analytical and empirical evidence, the intriguing "mutual reinforcement" effects between the two attack vectors—leveraging one vector significantly amplifies the effectiveness of the other.
3. Demonstrate that such effects enable a wide design spectrum for adversaries to enhance existing attacks, such as backdoor attacks, by maximizing attack evasiveness with respect to various detection methods.
4. Discuss potential countermeasures against such optimized attacks and their technical challenges, pointing to several promising research directions.

**Keywords:**
Adversarial attack, Trojaning attack, Backdoor attack

**ACM Reference Format:**
Ren Pang, Hua Shen, Xinyang Zhang, Shouling Ji, Yevgeniy Vorobeychik, Xiapu Luo, Alex Liu, and Ting Wang. 2020. A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models. In Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security (CCS '20), November 9–13, 2020, Virtual Event, USA. ACM, New York, NY, USA, 15 pages. https://doi.org/10.1145/3372297.3417253

## 1 Introduction

The rapid advancements in deep learning have led to breakthroughs in various long-standing machine learning tasks, such as image classification, natural language processing, and even playing Go, enabling scenarios previously considered strictly experimental. However, it is now well-known that deep learning systems are inherently vulnerable to adversarial manipulations, which significantly hinders their use in security-critical domains like autonomous driving, video surveillance, web content filtering, and biometric authentication.

Two primary attack vectors have been considered in the literature:
1. **Adversarial inputs**: The adversary crafts an adversarial version \( x^* \) of a benign input \( x \) to deceive the target DNN \( f \) at inference time.
2. **Poisoned models**: During training, the adversary builds malicious functions into \( f \), such that the poisoned DNN \( f^* \) misbehaves on one or more pre-defined inputs \( x \).

As illustrated in Figure 1, these two attack vectors share the same aim of forcing the DNN to misbehave on pre-defined inputs but through different routes: one perturbs the input, and the other modifies the model. Some attacks, such as backdoor attacks, leverage both vectors simultaneously: the adversary modifies \( f \) to be sensitive to pre-defined trigger patterns during training and then generates trigger-embedded inputs at inference time to cause the poisoned model \( f^* \) to malfunction.

## 2 Preliminaries

We begin by introducing a set of fundamental concepts and assumptions. Table 1 summarizes the important notations used in the paper.

### 2.1 Deep Neural Networks

Deep neural networks (DNNs) represent a class of machine learning models that learn high-level abstractions of complex data using multiple processing layers in conjunction with non-linear transformations. We primarily consider a predictive setting where a DNN \( f \) (parameterized by \( \theta \)) encodes a function \( f: X \rightarrow Y \). Given an input \( x \in X \), \( f \) predicts a nominal variable \( f(x; \theta) \) ranging over a set of pre-defined classes \( Y \).

We consider DNNs obtained via supervised learning. To train a model \( f \), the training algorithm uses a training set \( D \), where each instance \( (x, y) \in D \subset X \times Y \) comprises an input \( x \) and its ground-truth class \( y \). The algorithm determines the best parameter configuration \( \theta \) for \( f \) by optimizing a loss function \( \ell(f(x; \theta), y) \) (e.g., cross-entropy), typically implemented using stochastic gradient descent or its variants.

### 2.2 Prior Work and Research Questions

Previous work has intensively studied the two attack vectors separately, but there is a lack of understanding about their fundamental connections. Specifically, it remains unclear:
- What the vulnerability to one attack implies for the other.
- How the two vectors may interact with each other.
- How their interactions may influence the attack dynamics.
- How to effectively defend against coordinated attacks.

In this paper, we seek to answer the following research questions:
- **RQ1**: What are the fundamental connections between adversarial inputs and poisoned models?
- **RQ2**: What are the dynamic interactions between the two attack vectors if they are applied together?
- **RQ3**: What are the implications of such interactions for the adversary to optimize the attack strategies?
- **RQ4**: What are the potential countermeasures to defend against such enhanced attacks?

Our contributions include:
- Developing a new attack model that jointly optimizes adversarial inputs and poisoned models.
- Revealing the mutual reinforcement effects between the two attack vectors.
- Demonstrating the design spectrum for adversaries to enhance existing attacks.
- Discussing potential countermeasures and their technical challenges.

This work represents a significant step towards a holistic understanding of the vulnerabilities of DNNs in practical settings and sheds light on developing more effective countermeasures.