20-30
30-40
40-50
50-60
60-120
y
t
i
s
n
e
D
10
8
6
4
2
0
Caliskan et al. [9]
Abuhamad et al. [1]
0
20
40
60
80
100
Changed features per successful sample [%]
Number of changed LOC per successful sample
Removed LOC
Changed LOC
Added LOC
Figure 15: Targeted attack: Histogram over the number of changed features
per successful impersonation for both attribution methods.
Figure 14: Targeted attack: Stacked histogram over the number of changed
lines of code (LOC) per successful impersonation for both attribution meth-
ods. The original source ﬁles have 74 lines on average (std: 38.44).
selected authors. We start with the scenario where we retrieve
two samples of source code for each of the 20 programmers
from various GCJ challenges—not part of the ﬁxed 8 train-test
challenges—to support the template transformations.
Attack performance. Table 3 depicts the success rate of
our attack for both attribution methods. We can transfer the
prediction from one to another developer in 77% and 81% of
all cases, respectively, indicating that more than three out of
four programmers can be successfully impersonated.
In addition, Figure 13 presents the results as a matrix,
where the number of successful impersonations is visually
depicted. Note that the value in each cell indicates the abso-
lute number of successful impersonations for the 8 challenges
associated with each author pair. We ﬁnd that a large set of
developers can be imitated by almost every other developer.
Their stylistic patterns are well reﬂected by our transformers
and thus can be easily forged. By contrast, only the develop-
ers I and P have a small impersonation rate for Caliskan et al.
[9], yet 68% and 79% of the developers can still imitate the
style of I and P in at least one challenge.
Attack analysis. The number of altered lines of code also
remains small for the targeted attacks. Figure 14 shows that
in most cases only 0 to 10 lines of code are affected. At the
same time, the feature space is substantially changed. Fig-
ure 15 depicts that both attribution methods exhibit a similar
distribution as before in the untargeted attack—except that
the left peak vanishes for the method of Caliskan et al. [9].
This means that each source ﬁle requires more than a few
targeted changes to achieve an impersonation.
Table 4: Usage of transformation families for impersonation
Transformation Family
Cal. [9]
Abu. [1]
Control Transformers
Declaration Transformers
API Transformers
Miscellaneous Transformers
Template Transformers
8.43%
14.11%
29.90%
9.15%
38.42%
9.72%
17.88%
19.60%
4.76%
48.04%
Table 4 shows the contribution of each transformation fam-
ily to the impersonation success. All transformations are
necessary to achieve the reported attack rates. A closer look
reveals that the method by Abuhamad et al. [1] strongly rests
on the usage of template transformers, while the families are
more balanced for the approach by Caliskan et al. [9]. This
488    28th USENIX Security Symposium
USENIX Association
ABCDEFGHIJKLMNOPQRSTTargetauthorTSRQPONMLKJIHGFEDCBASourceauthor488883884888883386818888885088888828880338638508757820888788743822886887077788788883284888587861888888208887837888658786713856771888878887682287882185888888888438888818888187886850878782888885888783176888287888477376076887026584053782818788804588878882422888867187888588885187888518786387888307888872788823863110681802064668888885487888858888488838118736850848777777732776777177750%20%40%60%80%100%ABCDEFGHIJKLMNOPQRSTTargetauthorTSRQPONMLKJIHGFEDCBASourceauthor426885185648226258458788868778872618886778786857887462887577888787788567486888888888878887788882878878767887008878788888888888887888848788778368836188885878886868681715788547878188764385877888588578888738588784857876847841607678687885564886051885848788887788858188887768858888885677888235852844463030573724778172686516257683788778578883708888588888868888483888777777777777766777770%20%40%60%80%100%Source Author I
cout > d >> n ;
for ( double i = 0; i > k >> s ;
[...]
}
ans = d / ans ;
cout > d >> n ;
td_d i ;
for ( i = 0; i > k >> s ;
[...]
}
ans = d / ans ;
printf ( " Case #% lld : %.7 f \ n " ,
ccr , ans ) ;
++ ccr ;
}
Target Author P
Iteration
Transformer
Description


int T , cas = 0;
cin >> T ;
while (T - -) {
int d , n ;
cin >> d >> n ;
double t = 0;
while (n - -) {
int k , s ;
cin >> k >> s ;
t = max ((1.0 * d - k ) / s , t ) ;
}
double ans = d / t ;
printf ( " Case #% d : %.10 f \ n " , ++ cas , ans ) ;
}


Typedef
For statement
Init-Decl
Output API
adds typedef and replaces all locations with
previous type by novel typedef.
converts for-statement into an equivalent
while-statement, as target tends to solve
problems via while-loops.
moves a declaration out of the control state-
ment which mimics the declaration behavior
of while-statements.
substitutes C++ API for writing output by
C API printf. To this end, it determines
the precision of output statements by ﬁnding
fixed (line 1) and setprecision (line 14)
commands.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Figure 16: Impersonation example from our evaluation for the GCJ problem Steed 2: Cruise Control. The upper left listing shows the original source ﬁle, the
upper right its modiﬁed version such that it is classiﬁed as the target author. For comparison, the lower left listing shows the original source ﬁle from the target
author (which was not available for the attacker). The table lists the necessary transformations.
difference can be attributed to the feature sets, where the
former method relies on simple lexical features only and the
latter extracts more involved features from the AST.
Case Study. To provide further intuition for a successful
impersonation, Figure 16 shows a case study from our evalua-
tion. The upper two panels present the code from the source
author in original and transformed form. The lower left panel
depicts the original source text from the target author for the
same challenge. Note that the attack has no access to this ﬁle.
The table lists four conducted transformations. For instance,
the target author has the stylistic pattern to use while state-
ments, C functions for the output, and particular typedefs. By
changing these patterns, our attack succeeds in misleading
the attribution method.
Attack without template. We additionally examine the
scenario when the adversary has no access to a template ﬁle
of the target developer. In this case, our template transformers
can only try common patterns, such as the iteration variables
i, j, . . . , k or typedef ll for the type long long. Table 3 shows
the results of this experiment as well. Still, we achieve an
impersonation rate of 71% and 69%—solely by relying on
the feedback from the classiﬁer. The number of altered lines
of code and features correspond to Figures 14 and 15.
Contrary to expectation, without a template, the approach
by Abuhamad et al. [1] is harder to fool than the method by
Caliskan et al. [9]. As the lexical features rest more on simple
declaration names and included libraries, they are harder to
guess without a template ﬁle. However, if a template ﬁle is
available, this approach is considerably easier to evade.
Attack with substitute model. We ﬁnally demonstrate that
an impersonation is even possible without access to the pre-
diction of the original classiﬁer, only relying on a substitute
model trained from separate data. We split our training set
into disjoint sets with three ﬁles per author to train the original
and substitute model, respectively. We test the attack on the
method by Caliskan et al. [9], which is the more robust attribu-
tion under attack. By the nature of this scenario, the adversary
can use two ﬁles to support the template transformations.
Adversarial examples—generated with the substitute
model—transfer in 79% of the cases to the original model,
that is, attacks successful against the substitute model are
also effective against the original in the majority of the cases.
This indicates that our attack successfully changes indicative
features for a target developer across models. The success
rate of our attack on the original model is 52%. Due to the
reduced number of training ﬁles in this experiment, the attack
USENIX Association
28th USENIX Security Symposium    489
is harder, as the coding habits are less precisely covered by
the original and substitute models. Still, we are able to imper-
sonate every second developer with no access to the original
classiﬁer.
Summary. Our ﬁndings show that an adversary can auto-
matically impersonate a large set of developers without and
with access to a template ﬁle. We conclude that both con-
sidered attribution methods can be abused to trigger false
allegations—rendering a real-world application dangerous.
6.4 Preserved Semantics and Plausibility
In the last experiment, we verify that our adversarial code
samples comply with the attack constraints speciﬁed in Sec-
tion 3.2. That is, we empirically check that (a) the semantics
of the transformed source code are preserved, (b) the gen-
erated code is plausible to a human analyst, and (c) layout
features can be trivially evaded.
Preserved semantics. We begin by verifying the semantics
of the transformed source code. In particular, we use the test
ﬁle from each challenge of the GCJ competition to check that
the transformed source code provides the same solution as
the original code. In all our experiments, we can verify that
the output remains unchanged for each manipulated source
code sample before and after our attack.
Plausible code. Next, we check that our transformations
lead to plausible code and conduct a discrimination test with
15 human subjects. The group consists of 4 undergraduate
students, 6 graduate students and 5 professional computer
scientists. The structure of the test follows an AXY-test: Ev-
ery participant obtains 9 ﬁles of source code—each from a
different author but for the same GCJ challenge. These 9 ﬁles
consists of 3 unmodiﬁed source codes as reference (A) and
6 sources codes (XY) that need to be classiﬁed as either orig-
inal or modiﬁed. The participants are informed that 3 of the
samples are modiﬁed. We then ask each participant to identify
the unknown samples and to provide a short justiﬁcation.
The results of this empirical study are provided in Table 5.
On average, the participants are able to correctly classify
60% of the provided ﬁles which is only marginally higher
than random guessing. This result highlights that it is hard
to decide whether source code has been modiﬁed by our
attack or not. In several cases, the participants falsely assume
that unused typedef statements or an inconsistent usage of
operators are modiﬁcations.
Evasion of layout features. Finally, we demonstrate that
layout features can be trivially manipulated, so that it is valid
to restrict our approach to the forgery of lexical and syntactic
features. To this end, we train a random forest classiﬁer only
on layout features as extracted by Caliskan et al. [9]. We then
compare the attribution accuracy of the classiﬁer on the test
Table 5: Study on plausibility of transformed source code.
Participant Group
Undergraduate students
Graduate students
Professionals
Total
Random guessing
Accuracy
Std
66.7% 23.6%
55.6% 15.7%
60.0% 24.9%
60.0% 21.8%
50.0%
—
set with and without the application of the formatting tool