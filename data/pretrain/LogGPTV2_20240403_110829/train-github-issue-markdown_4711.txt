The following code:
    @functools.partial(jax.custom_vjp, nondiff_argnums=(1,))
    def _precise_mean(x, axis):
      with jax.experimental.enable_x64():
        return jnp.mean(x.astype("float64"), axis=axis).astype(x.dtype)
    def _precise_mean_fwd(x, axis):
      return _precise_mean(x, axis), (x,)
    def _precise_mean_bwd(axis, res, g):
      x, = res
      dims = tuple(i for i in range(x.ndim) if i not in axis)
      factor = 1
      for a in axis:
        factor *= x.shape[a]
      g = g / factor
      print(g.dtype)
      return lax.broadcast_in_dim(g.astype(x.dtype), x.shape, dims),
    _precise_mean.defvjp(_precise_mean_fwd, _precise_mean_bwd)
    def precise_mean(x, axis=None):
      if axis is None:
        axis = tuple(range(x.ndim))
      elif isinstance(axis, int):
        axis = (axis,)
      # Take care of negative values
      axis = tuple((x.ndim + a) % x.ndim for a in axis)
      if not all(isinstance(a, int) for a in axis):
        raise ValueError("All axis must be integers.")
      return _precise_mean(x, axis)
    @jax.jit
    def f(x, a, b):
      x = jnp.matmul(x, a) + b[None]
      y = jnp.sum(x, axis=-1)
      return precise_mean(y, axis=0)
    x = jnp.ones([5000, 100])
    a = jnp.ones([100, 100])
    b = jnp.zeros([100])
    print(f(x, a, b).dtype)
Throws out the following error:
    UnfilteredStackTrace: RuntimeError: Internal: Seen floating point types of different precisions in %reduce.23 = f32[] reduce(f64[5000]{0} %convert.17, f32[] %constant.18), dimensions={0}, to_apply=%primitive_computation_add.19, metadata={op_type="reduce_sum" op_name="jit(f)/custom_vjp_call_jaxpr/reduce_sum[ axes=(0,) ]" source_file="" source_line=13}, but mixed precision is disallowed.
    Failed after pipeline-start
    The stack trace below excludes JAX-internal frames.
    The preceding is the original exception that occurred, unmodified.
It is unclear at the moment how to use the `enable_x64` contest when using
`jax.jit`.