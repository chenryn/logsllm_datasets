title:SoK: Automated Software Diversity
author:Per Larsen and
Andrei Homescu and
Stefan Brunthaler and
Michael Franz
2014 IEEE Symposium on Security and Privacy
SoK: Automated Software Diversity
Per Larsen, Andrei Homescu, Stefan Brunthaler, Michael Franz
University of California, Irvine
Abstract—The idea of automatic software diversity is at least
two decades old. The deﬁciencies of currently deployed defenses
and the transition to online software distribution (the “App
store” model) for traditional and mobile computers has revived
the interest in automatic software diversity. Consequently, the
literature on diversity grew by more than two dozen papers since
2008.
Diversity offers several unique properties. Unlike other de-
fenses, it introduces uncertainty in the target. Precise knowledge
of the target software provides the underpinning for a wide range
of attacks. This makes diversity a broad rather than narrowly
focused defense mechanism. Second, diversity offers probabilistic
protection similar to cryptography—attacks may succeed by
chance so implementations must offer high entropy. Finally, the
design space of diversifying program transformations is large.
As a result, researchers have proposed multiple approaches
to software diversity that vary with respect to threat models,
security, performance, and practicality.
In this paper, we systematically study the state-of-the-art in
software diversity and highlight fundamental trade-offs between
fully automated approaches. We also point to open areas and
unresolved challenges. These include “hybrid solutions”, error
reporting, patching, and implementation disclosure attacks on
diversiﬁed software.
I. MOTIVATION
As modern society grows increasingly dependent on the
digital domain, adversaries abound in cyberspace. In spite of
the combined efforts of the security community, reports of
major software vulnerabilities that put millions of users at risk
continue to be the norm rather than the exception.
Whereas diversity provides protection and resilience in
nature, the commoditization of the computer systems has made
them increasingly homogeneous with respect to hardware,
operating systems, applications, and everything in between.
Homogeneity and standardization provide economies of scale,
consistent behavior, and simplify the logistics of distributing
programs. We therefore live in a software mono-culture.
Unfortunately, homogeneity has turned out to be a double-
edged sword [26]. An attacker can readily download an
identical copy of the commodity software running on their
victims’ systems and probe it for vulnerabilities. After turning
a vulnerability into an exploit, the attacker can target all systems
running copies of the vulnerable program. In other words, the
software mono-culture creates economies of scale for attackers,
too.
Artiﬁcial software diversity aims to increase the cost to
attackers by randomizing implementation aspects of programs.
This forces attackers to target each system individually, sub-
stantially raising the bar on mass scale exploitation. Without
knowledge of the program implementation hosted on a par-
ticular system, targeted attacks become signiﬁcantly harder,
too.
The idea of protecting programs with artiﬁcially generated
diversity is at least two decades old [13]. However, compiler-
based software diversity has only recently become practical
due to the Internet (enabling distribution of individualized soft-
ware) and cloud computing (computational power to perform
diversiﬁcation) [25]. These developments and the emergence
of code-reuse attacks renewed the interest in software diversity.
This has led to a large body of research that is in many ways
as diverse as the set of program variants they generate.
This paper systematizes the understanding of software
diversity1 as follows. First, we show the versatility of artiﬁcial
software diversity by surveying the range of relevant attacks.
Second, we provide the ﬁrst systematic and uniﬁed overview of
existing diversiﬁcation approaches. In particular, we character-
ize the four major properties of a diversiﬁcation approach using
a consistent terminology: (i) what is diversiﬁed (Section III),
(ii) when and where diversiﬁcation happens (Section IV), (iii)
how security is evaluated (Section V-A), and (iv) the resulting
performance overheads (Section V-B). Finally, we point to
open areas of research and challenge the belief that compiler-
based diversiﬁcation is less versatile than binary rewriting
(Section VI).
II. TODAY’S SECURITY LANDSCAPE
Attackers and defenders in cyberspace engage in a continu-
ous arms race. As new attacks appear, new defenses are created
in response—leading to increased complexity in both cases. To
motivate a study of software diversity, we brieﬂy summarize
the evolution and current state of computer security.
A. Taxonomy of Attacks
There is a large spectrum of attacks that an attacker can use
against a target, employing a wide range of low-level techniques.
We present the ones that are most relevant to automated software
diversity.
1) Information Leaks: Often, the attacker seeks to read
some sensitive program state he should not have access to. This
includes contents of processor registers, memory pages, process
meta-data, ﬁles, etc. Such information can be valuable to an
attacker by itself (e.g., credit card numbers, login credentials, or
other personal user information) or to further an on-going attack
(by locating protected objects in memory through a pointer
leak, or by reading encryption keys used to encrypt other
data). In general, information leaks are increasingly used to
overcome situations where attackers lack knowledge of program
internals [60], [9]. For example, information leaks help bypass
address space layout randomization in later stages of an attack.
1Research on multi-variant systems overlaps with software diversity. Diversity
can be studied in isolation, however. We do so due to space restrictions.
© 2014, Per Larsen. Under license to IEEE.
DOI 10.1109/SP.2014.25
276
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:59:27 UTC from IEEE Xplore.  Restrictions apply. 
a) Side Channel Attacks: We consider side channel
attacks as a category of information leaks. Whereas most
other information leaks are intrusive (using some exploit to
explicitly reveal program data in unintended ways), side channel
attacks infer internal program state in a black box manner
by analyzing the interactions between the program and its
outside environment. One common measurement used by these
attacks is timing; the attacker measures how the time between
externally-visible program events changes in response to some
stimulus applied by the attacker, and if the response is also
correlated with the value of some internal program variable.
Many types of stimuli are available to an attacker, such as
memory or cache pressure.
2) Memory Corruption Attacks: The attacker often needs to
modify the internal program state located in memory. This can
be either the end-goal of the attack or an intermediate step (for
example, an attacker may seek to modify a function pointer to
hijack program control ﬂow). This class covers a large variety
of techniques that use programming errors to achieve the same
goal: changing the memory contents of the target program.
a) Buffer Overﬂows: These attacks are a popular instan-
tiation of memory corruption attacks. In a buffer overﬂow, the
attacker writes data outside of the bounds of a memory buffer
to corrupt memory adjacent to that buffer. This approach is only
viable in languages without bounds checking. This attack is
most often applied to stack buffers, but it also works effectively
on buffers stored on the program heap.
b) Memory Allocator Exploits: Such exploits rely on the
predictability and performance constraints of memory allocators,
or on programming errors related to memory allocation, to
implement a memory corruption attack. For example, memory
allocators typically do not clear unused memory after de-
allocation requests, because it impacts program performance.
This poses signiﬁcant security problems. Attackers manipulate
the allocator in such a way that a newly requested block
(under the control of the attacker) overlaps recently released
block containing sensitive data. Memory-management errors
in applications are also a signiﬁcant threat; one example are
use-after-free errors, where the attacker uses a stale pointer to
deallocated memory to read or write a newly allocated block
that overlaps the deallocated block.
3) Code Injection: Another way of exploiting a program is
to make it execute code under the attacker’s control, potentially
leading to the attacker taking control of the entire program or
even system (programs running with administrator privileges
effectively control the entire machine). To achieve this, the
attacker injects malicious code into a running program and
then redirects execution to the injected code [2]. This attack
requires (i) a memory corruption vulnerability, (ii) an executable
and writable region of memory, and (iii) a way to direct the
processor to execute newly-written data. The third requirement
is usually met through memory corruption as well, by modifying
a code pointer (like the on-stack return address) to point to
the new code. The attacker then crafts a native code payload,
writes it to memory, then redirects execution to it. The processor
executes the newly inserted block, leaving the attacker in control
of the thread of execution.
4) Code Reuse: Operating systems used to allow execution
of most program data. This enabled code injection. For example,
the entire native stack was executable on Windows and Linux
systems. To prevent code injection attacks, operating systems
now implement a security model (known as Data Execution
Prevention—DEP—or W⊕X [48]) which mandates that a
memory page is either writable or executable, but not both at
the same time. Code reuse attacks [44], [39], [57], [63] have
emerged as a counter-measure to non-executable data defenses.
Instead of injecting new code, attackers construct an attack
from pieces of executable code (either entire functions [44],
[63] or smaller snippets of code) already found in the target
program.
Also as a reaction to randomization-based defenses, infor-
mation leaks have become an increasingly crucial part of code-
reuse attacks. One example of this development is a new code-
reuse attack called “just-in-time code reuse” [60]. This attack
uses information leak techniques to read the code loaded by the
target program. Snow et al. avoids page faults by decompiling
a page of code at a time and incrementally following references
to other mapped pages. After collecting all gadgets, the attack
code (containing a built-in gadget compiler) compiles a tailored
code reuse payload for that particular program and then runs
the attack against the program.
5) JIT Attacks: The introduction of new programming
models can change the landscape and introduce new threats to
security. In recent years, just-in-time, JIT, compiled languages
(such as Java and JavaScript) have become increasingly popular.
For example, many dynamically-generated or interactive web
pages are written in JavaScript, and all major web browsers
contain a JIT compiler for JavaScript. These languages allow
programmers to create and run new code dynamically (during
program execution); a JIT compiler then translates from source
to binary code. This creates a new problem: the attacker
can craft and insert malicious source code into the program
itself. This is a variant of code injection applied to source
code. Program source code is stored as non-executable data,
so existing anti-code injection defenses are insufﬁcient. JIT
spraying [10] is a recent attack of this kind. When compiling
expressions containing constant values, just-in-time compilers
may embed the constants directly in binary code. This gives the
attacker a way to inject arbitrary binary code into the program,
by using constants that contain an attack payload.
6) Program Tampering: The ability to modify a program’s
state (tamper with the program) has many applications: the
attacker can modify unprotected code pointers or instructions
to execute arbitrary code, change program data to gain some
beneﬁt or bypass DRM protections. One example is bypassing
checks in programs that prompt for passwords or serial numbers.
Another example of tampering is cheating at computer games,
where players give themselves unfair advantages by removing
restrictions from the game. Tampering with a program may
require the use of one or more of the previously described
attacks, as intermediate steps to achieving the desired effect on
the target.
Client-side tampering requires unfettered access to the
target program, and the attacker is often also in control of
the entire physical machine, as well as the operating system,
running the program (this model is known as man-at-the-end,
or MATE [15]). This is a different adversarial model from the
other attacks, where the attacker has restricted access to the
program, and little or no access to the underlying system.
277
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:59:27 UTC from IEEE Xplore.  Restrictions apply. 
7) Reverse Engineering: Often, the attacker seeks not to
impact the execution of a program, but
to ﬁnd out how
the program works internally. Notable uses include reimple-
mentation, compatibility with new software, and defeating
security-through-obscurity. Security researchers working on
both offenses and defenses use reverse engineering to discover
exploitable program vulnerabilities, both from the original
program and from patches [22], [18].
B. Taxonomy of Defenses
Many attacks rely on program bugs that are ﬁxable when
the program source code is available. If this is not the case,
or if attacks rely on intended program behavior, alternative
techniques are available. We separate these techniques into the
following categories:
1) Enforcement-based Defenses: To defend against attacks,
users may opt to take a proactive approach where they seek
to prevent attacks from occurring (whenever some program
behavior is exploitable, defenders preemptively disallow that
behavior). Examples include checking the bounds on array
accesses (against buffer overﬂows), making the data sections
non-executable (against code injection) and restricting control
ﬂow exclusively to intended paths (to prevent code reuse
attacks)—also known as Control Flow Integrity (CFI) [1].
Software Fault Isolation (SFI) [43], [68] is a similar approach
that restricts control ﬂow to limited targets within a sandbox.
Defenders can deploy these techniques at the source code
level, during program compilation [68], or through static binary
rewriting [65], [69].
Due to the rise of code injection attacks, modern operating
systems (Windows, Linux2, and OS X) all deploy one low-
cost enforcement defense: Data Execution Prevention (DEP,
also known as W⊕X). DEP requires the operating system to
map all pages containing program data (such as the heap and
stack) as non-executable, and all pages of program code as
non-writable. This defense has negligible performance costs
and effectively stopped most code injection attacks without
requiring substantial changes to programs.
Other enforcement techniques require signiﬁcant changes to
protected programs and impose extra restrictions and costs on
both programs and programmers. For example, array bounds
checking requires extra operations around each array element
access; CFI requires extra address checks around each indirect
branch. In programs that contain many array accesses or
indirect branches, these checks incur signiﬁcant performance
penalties. In addition, programmers have to account for the
extra restrictions; for example, they must check that the program
does not violate any security restriction during normal program
operation. Therefore, we regard this class of defenses as the
most intrusive.
2) Program Integrity Monitors:
If an attack cannot be
prevented, the last line of defense is stopping the program
before the attacker has a chance to do any damage. Doing
this manually requires signiﬁcant effort and attention from
program users. To stop the program, they ﬁrst have to notice
any unusual behavior in the operation of the program. In many
cases, this unusual behavior is either invisible to the user, or
2The PaX Team implemented DEP on Linux [48].
is intentionally hidden by the attacker (to prevent detection).
While deﬁning when a program is acting “unusually” is very
hard, detecting speciﬁc attacks is much simpler and can be
easily automated in many cases. For each detectable attack,
an integrity monitor periodically investigates the state of the
running program and checks for signs of an attack.
Examples of such defenses are “stack canaries” [19]
and “heap canaries.” Code execution attacks often use buffer
overﬂows to overwrite a code pointer, e.g., the return address
of the currently executing function. To defend against this
attack, modern compilers can insert canaries to guard the return
address against changes by pairing it with a randomized guard
value—the “canary.” Any change to the return address will also
change the canary, and the attacker cannot reasonably predict
the random value of the canary. On every function return, the
program checks the canary against the expected random value
and terminates on mismatches. The overheads from the added
checks are often negligible (less than 1% on average [61]).
Monitoring defenses are the least intrusive form of defense
(in many cases, they can be deployed transparently to the
program), but are the most vulnerable to detection and deception.
Monitoring allows attackers the same amount of control as long
as they remain undetected and they may detect and tamper
with the monitor to let the attack succeed.
3) Diversity-based Defenses: Attackers often rely on being
able to predict certain details of program implementation, such
as the memory locations of sensitive program objects (like code
pointers). Removing predictability is, in most cases, as effective
as restricting what the attacker can do with the predicted
knowledge. Diversiﬁcation makes program implementations
diverge between each computer system or between each
execution. This means that the attacker has to either limit
the attack to a small subset of predictable targets, or adjust the
attack to account for diversity. The latter is impractical in most
cases (because it would require duplicated attack effort for each
different version of the target), so the malicious effects of the
attacks are limited at worst to a small number of targets (where
the attacker still gets full control, in absence of any monitoring
or enforcement-based defenses). The three following sections
treat approaches to diversity in much greater detail. Researchers
have investigated the practical uses of automated software
diversity against the attacks enumerated in Section II-A (except
information leaks and side channels). Figure 1 links attacks to
corresponding studies of diversity.
4) Program Obfuscation: Obfuscation to prevent reverse
engineering attacks [14], [16] is closely related to diversity and
relies on many of the same code transformations. Diversity
requires that program implementations are kept private and
that implementations differ among systems; this is not required
for obfuscation. Pucella and Schneider perform a comparative
analysis of obfuscation, diversiﬁcation, and type systems within
a single semantic framework [51].
III. WHAT TO DIVERSIFY
At the core of any approach to software diversity, whether
performed manually by programmers or automatically by a
compiler or binary rewriter, is a set of randomizing transforma-
tions that make functionally equivalent program copies diverge.
A second distinguishing factor among approaches is when
278
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:59:27 UTC from IEEE Xplore.  Restrictions apply. 
diversity is introduced in the software life-cycle. These two
choices—what to diversify and when to diversify—constitute
the major axes of the design space and together determine the
fundamental properties of any concrete approach. This section
focuses on the former choice and Section IV addresses the
latter.
Randomizing transformations are conceptually similar to
compiler optimizations. Both consist of three steps: (i) deter-
mining if a code fragment can be transformed, (ii) estimating
if the transformation is proﬁtable, and (iii) applying the
transformation. A diversifying transformation differs in the
second step by adding an element of chance. The heuristic
that determines whether to transform a code fragment or not is
replaced (or extended) with a random choice using a pseudo-
random number generator (PRNG). Early studies of security
via software diversity were compiler-based [13], [24].
Like compiler optimizations, the scope of diversifying
transformations varies in granularity from single instructions
to the entirety of the program.
5) Instruction Level: These transformations affect at most
a few instructions inside a single basic block3. Permuting and
displacing instructions breaks ﬁne-grained code reuse attacks
(assuming implementation details do not leak to attackers [60],
[9]). They include but are not limited to:
a) Equivalent Instruction Substitution: The functionality
of some instructions overlaps with that of others such that it is
often possible to substitute one for another. Load instructions
that support multiple addressing modes are common examples.
b) Equivalent Instruction Sequences: Substituting one
or more instructions for another instruction sequence leads to
even more randomization opportunities. For instance, negation
followed by subtraction can substitute for integer addition.
c) Instruction Reordering: It is well known that instruc-
tions can execute in any order that preserves the dependencies
between data-producing and data-consuming instructions. Using
a compiler’s instruction scheduler to randomize the instruction
order increases diversity among the output binaries.
d) Register Allocation Randomization: While program
performance is highly dependent on what variables are allocated
to registers, the particular register a variable is assigned to is
often irrelevant. Consequently, it is straightforward to randomize
register assignments. Register spilling and re-materialization
heuristics are amenable to randomization, too.
e) Garbage Code Insertion: This transformation can be
as simple as adding no-operation instructions (NOPs), or as
complex as inserting entirely new statements. In contrast to
other transformations, garbage insertion is always possible and
hence allows production of inﬁnitely many program variants.
6) Basic Block Level: The number and ordering of basic
blocks within a function or method can be chosen freely. This
enables several control-ﬂow transformations including:
3A basic block is a sequence of instructions where the execution of the ﬁrst
instruction guarantees that all following instructions are executed, i.e., only
the instruction that terminates the block may be a branch.
a) Basic Block Reordering: The last instruction in a
basic block can either branch to the successor basic block or
have execution fall through to the basic block following it in
memory. Reordering makes it necessary to insert additional
jumps between pairs of basic blocks chained together on fall-
through paths (i.e., without branches) and makes branches
in blocks that can fall through after reordering superﬂuous.
Basic block splitting and merging creates additional reordering
opportunities.
b) Opaque Predicate Insertion: A single-predecessor
block b can be substituted with a conditional branch to b and its
clone b(cid:2) using an arbitrary predicate [14], [16]. These predicates
can also guard blocks of garbage code so they never execute.
c) Branch Function Insertion: Branch functions do not
return to their callers; instead, they contain code that determines
the return address based on the call site [41]. Branch functions