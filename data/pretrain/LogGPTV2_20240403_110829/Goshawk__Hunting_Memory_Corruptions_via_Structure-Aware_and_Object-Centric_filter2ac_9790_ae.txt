### Rally Causes False Positives and Negatives

GOSHAWK, which leverages an existing code analysis engine as its underlying infrastructure, is subject to both false positives and false negatives in bug detection. The primary focus of GOSHAWK is on detecting memory corruption bugs, rather than addressing the well-known deficiencies in program analysis. We argue that these aspects are orthogonal to GOSHAWK's objectives, and improvements in these areas could benefit the system.

Our experiments show that by using NLP-assisted MM function classification alone, GOSHAWK can capture most MM functions but also incorrectly identifies many non-MM functions, leading to false positives. When follow-up data flow analysis is applied to eliminate non-MM functions, it unfortunately filters out a small portion of real MM functions, causing false negatives. Given that most bug detection systems prioritize reducing false positives over false negatives, our design combines NLP-assisted classification with data flow analysis-based validation to ensure the accuracy of identified MM functions. The results in Section V-C demonstrate that GOSHAWK maintains acceptable false positive and false negative rates.

### Related Work

#### A. Memory Corruption Bug Detection

- **K-Miner [39]**: Conducts scalable pointer analysis and interprocedural analysis to uncover memory corruption bugs.
- **Hua et al. [40]**: Utilizes machine learning to mitigate the imprecision of pointer analysis in detecting use-after-free bugs in large programs. They learn correlations between program features and pointer aliases to filter out ambiguous aliases and detect use-after-free bugs.
- **CRED [41]**: A path-sensitive and pointer analysis-based tool for detecting use-after-free bugs. It reduces the number of contexts through spatio-temporal context reduction, achieving low false positive rates with multi-stage analysis.
- **DCUAF [42]**: Statically detects concurrency use-after-free bugs in kernel drivers using paired driver interface functions that can be executed concurrently.

These approaches require a set of annotated allocation and deallocation functions from the codebase to perform source-to-sink analysis for detecting memory corruption bugs.

#### B. MM Function Identification Approaches

- **Pair-based Mining**:
  - **WYSIWIB [43]**: Extracts data dependencies among functions and identifies pairs of MM functions.
  - **SinkFinder**: Uses data dependencies to find function pairs, employing an analogical reasoning mechanism to infer similar function pairs.
  - **PairMiner [10]**: Conducts similarity comparison using multiple keyword matching.
  - **PF-Miner [11]** and **BP-Miner [44]**: Track both normal execution paths and error handling paths to recognize function pairs.
  - **K-MELD [6]**: Identifies allocation and deallocation functions from error handling paths.
  - **HERO [7]**: Relies on the pattern of paired function invocation on normal execution paths and reversely ordered error handling paths to mine paired functions.

- **Routine-based Mining**:
  - **MemBrush**: Executes programs and tracks custom memory allocators and deallocators. It searches for functions that comply with these flows in C/C++ binaries.
  - **DynPTA [45]**: Analyzes patterns of memory-allocation wrapper functions, such as invoking a libc memory-allocation function, to recognize function wrappers and locate related pointers. However, DynPTA cannot handle custom memory allocation functions.

- **Semantic-based Mining**:
  - **Bai et al. [46]**: Use keywords to extract potential allocation and deallocation functions through semantic analysis, requiring manual efforts to filter out irrelevant functions.
  - **SuSi [4]**: Leverages a set of human-annotated functions to train an SVM classifier, which is then used to predict source and sink functions in the Android framework. The effectiveness of the classifier depends heavily on the annotated functions and features, requiring significant human effort.
  - **NLP-EYE [3]**: Infers the semantic meanings of functions by comparing function prototypes with known functions. Although it can recognize function semantics, the results are imprecise due to limited programming and natural language involvement.

### Conclusion

We introduce MOS, a novel concept for implementing structure-aware and object-centric MM behavior summarization, to help detect complex memory bugs with characteristics of nested allocation and unpaired uses of MM functions. Our MOS-enhanced bug detection system, GOSHAWK, combines NLP and data flow analysis to identify MM functions. This system has successfully detected 92 new bugs in recent versions of the Linux kernel, FreeBSD kernel, OpenSSL, Redis, and three IoT SDKs through MOS-enhanced memory object tracking.

### Acknowledgment

The authors would like to thank the reviewers for their valuable feedback. Special thanks to Bodong Li from HiSilicon for helping improve GOSHAWK. This work was partially supported by the National Key Research and Development Program of China (No. 2020AAA0107803) and the SJTU-HiSilicon Research Grant (YBN2019125153). Yunlong Lyu was funded by the National Natural Science Foundation of China (U19B2023). Yunlong Lyu (PI:EMAIL) and Juanru Li (PI:EMAIL) are the corresponding authors.

### References

[References listed here, formatted consistently with the original text]

### Appendix

#### A. MM Function Classification Model Generating

- **ULM-based Segmentation**:
  - We construct a programming corpus to support function prototype segmentation based on the occurrence of informal terms. First, we collect 22GB of questions (including source code and descriptions) from StackOverflow [16]. GOSHAWK removes all punctuation and generates meaningful subwords using the BPE algorithm [15]. The BPE algorithm initializes all preprocessed contents as a sequence of characters and iteratively merges them into different units. By computing the occurrence frequency of each unit, GOSHAWK selects the unit with the highest frequency as a subword and adds it to the programming corpus.
  - Based on subwords in the programming corpus, GOSHAWK segments each function prototype in different ways. To select the most suitable segmentation, it calculates the occurrence probability of each possible segmentation form using the formula:
    \[
    P(\text{res}) = \prod_{w \in \text{subwords}} P(w)
    \]
    where \( P(\text{res}) \) denotes the occurrence probability of each segmentation form and \( P(w) \) denotes the occurrence probability of each subword. Finally, GOSHAWK selects the segmentation form with the highest probability.

- **Reference Set Creation**:
  - To recognize custom MM functions, we manually create a reference set for function comparison. To ensure variety and representativeness, we extend the function prototype corpus by iteratively labeling more MM functions (e.g., from the Linux kernel) and training the Siamese network. The reference set includes 4,441 functions (1,807 memory allocation functions and 2,634 memory deallocation functions).

- **Siamese Network Training**:
  - GOSHAWK trains a Siamese network to convert each subword list into a numeric vector containing natural language semantics and classifies each function prototype to a reference vector. Given the created function prototype corpus, where each function is classified as a memory allocation function, a memory deallocation function, or a non-MM function, GOSHAWK randomly pairs functions \((f_i, f'_i)\) with ground truth pairing information \(y_i \in \{+1, -1\}\), where \(y_i = +1\) indicates that the prototypes of functions \(f_i\) and \(f'_i\) are in the same category, and \(y_i = -1\) otherwise. The output of the Siamese network for each pair is the cosine similarity between the vectors \(\vec{e}_i\) and \(\vec{e}'_i\).

- **MM Candidate Selection**:
  - Using the reference set and the trained Siamese network, GOSHAWK classifies MM functions from the tested project. We only consider the vectors of the corresponding MM functions. Specifically, let \(\vec{a}_1, ..., \vec{a}_{n_a}\) denote the reference vectors of the allocation functions in the training function set and \(\vec{d}_1, ..., \vec{d}_{n_d}\) denote the reference vectors of the deallocation functions in the training function set. Subscripts \(n_a\) and \(n_d\) denote the number of allocation and deallocation functions in the training function set, respectively. To enhance comparison efficiency, we compress the semantics of the reference vectors into vectors \(\vec{a}_m\) and \(\vec{d}_m\) by averaging the weight of each function as follows:
    \[
    \vec{a}_m = \frac{1}{n_a} \sum_{i=1}^{n_a} \frac{\vec{a}_i}{\|\vec{a}_i\|}, \quad \vec{d}_m = \frac{1}{n_d} \sum_{i=1}^{n_d} \frac{\vec{d}_i}{\|\vec{d}_i\|}
    \]
  - For an unlabeled function \(f_t\), we generate its prototype vector \(\vec{v}_t\) using the trained Siamese network. After calculating the cosine similarity between \(\vec{v}_t\), \(\vec{a}_m\), and \(\vec{v}_t\), \(\vec{d}_m\), GOSHAWK can distinguish the type of \(f_t\) if any of the two similarity scores is higher than a threshold. If so, GOSHAWK assigns \(f_t\) to its corresponding candidate set.

#### B. Official MM Function Set

The official memory allocation functions collected are: `malloc`, `kmalloc`, `kmalloc_array`, `vmalloc_no_huge`, `krealloc_array`.