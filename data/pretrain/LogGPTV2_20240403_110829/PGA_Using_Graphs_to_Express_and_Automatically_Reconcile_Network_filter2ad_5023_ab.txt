on the edge between two nodes representing the endpoints,
with no need to carefully walk through multiple lines of pri-
oritized rules or if_then statements. We show how to auto-
matically create such composed policies in §4 and §5. We
ﬁrst lay out the required properties for a policy framework.
2.3 Requirements for policy framework
Simple and intuitive: Anecdotally, we found that many
network admins and cloud tenants design their policies by
31drawing diagrams on whiteboards. We believe a policy ab-
straction must be as simple as drawing diagrams similar to
Fig.1(a), yet expressive enough to capture their intents for
diverse and dynamic SDN, cloud and NFV applications with
sophisticated service chain requirements [34, 22, 38].
Independent and Composable: Each policy writer
should be able to write policies independently without co-
ordinating with other policy writers; yet ensuring that their
intents are being composed and enforced correctly, or receiv-
ing notiﬁcation if there are conﬂicts or more information is
needed.
Eager composition: Composing policies and ensuring
that individual policy intents are satisﬁed prior to deploy-
ment, i.e., eagerly, is highly desirable. Eager composition
can greatly reduce the number of conﬂicts/errors that a run-
time system has to handle, enable speedy runtime operation,
and reduce the chance of system misbehavior compared to
lazy composition. However, eager composition without ac-
tual endpoints in the system can lead to exponential state
explosion because in the worst case, every combination of
input policies should be considered. The policy framework
design should enable fast composition.
Automated: The policy framework must be highly auto-
mated to free network admins from manual and error-prone
policy composition. In some cases when the system cannot
identify the best policy composition, a human may be re-
quired to provide input and pick one composition. However,
this is much less burdensome than the existing approach of
manual composition.
Well-formed: The composed policy generated from the
input policies should be well-formed such that a unique pol-
icy can be chosen without ambiguity for any given packet
and associated dynamic conditions. This would allow the
runtime operation to be deterministic.
Service chain analysis: Handling service chains for cor-
rect policy composition is crucial: e.g., a misplaced FW can
drop packets that are legitimately allowed by ACL policies.
To the extent possible, the policy framework needs to model
the behavior of service functions for composition analysis.
Our framework, PGA, provides a simple and intuitive
graphical interface that is similar to how network admins
typically visualize their policies on a whiteboard. In PGA,
each user writes policies for arbitrary selection of endpoints
based on logical endpoint properties that makes sense to
them. This gives users the ﬂexibility to write policies inde-
pendent of each other as well as the underlying physical net-
work infrastructure. PGA achieves automated, eager com-
position of such policies by capturing the relationship be-
tween endpoint properties and enabling individual policies
to constrain each other during composition. Lastly, it pro-
vides abstractions to succinctly model middlebox behavior
that aids in service chain analysis.
Figure 2: PGA system architecture.
specify network policies. The users/tenants/admins and SDN
applications independently generate their policies as graphs
and submit them to the Graph Composer through a PGA
User Interface (UI). The UI and composer utilize additional
information (e.g., tenant hierarchies, tenant/endpoint loca-
tions) from external sources to assist in the policy speciﬁ-
cation and eager composition. The composer automatically
composes input graphs into a combined conﬂict-free graph,
resolving or ﬂagging conﬂicts/errors and reporting them to
users, possibly with suggested ﬁxes.
The composed high-level policy can be compiled down
to low-level conﬁgurations/rules, either proactively or reac-
tively; PGA’s eager policy composition is orthogonal to the
lower-level compilation methodology. Our prototype sup-
ports two different network environments. The ﬁrst is an
SDN environment with OpenFlow enabled network devices,
where we use the POX OpenFlow controller to reactively
generate OpenFlow rules for pkt-in events based on a com-
posed policy graph. To guard against possible bugs in rule
generation, and against the possibility that unrelated SDN
modules, e.g., trafﬁc engineering, may generate conﬂicting
rules, we use a rule veriﬁcation tool (§6) to detect and ver-
ify changes in end-to-end communication paths against the
ACL policies that PGA has generated for recent pkt-in events.
Our prototype also supports a virtual network abstraction
provided by OpenStack Neutron [8]. Our PGA service proac-
tively conﬁgures virtual network resources – speciﬁcally,
Neutron Security Groups and service functions – to imple-
ment the polices of a composed graph, and dynamically as-
sociates VMs to the best-matching graph nodes at runtime.
Both the SDN and virtual network systems can be ex-
tended to support NFV; e.g., PGA’s composed graph can be
used to generate Network Service Headers (NSH) and Ser-
vice Classiﬁers [4] for service function chaining.
For simplicity, our discussion around prototype system
will assume the SDN-OpenFlow environment while the
graph model and composition algorithm below can be ap-
plied to diverse environments.
3. SYSTEM OVERVIEW
Fig. 2 provides an overview of the PGA system compo-
nents and their interactions with external components. As
mentioned earlier, PGA uses a graph-based abstraction to
4. GRAPH MODEL
In PGA, network policies are described using a graph
structure that represents: 1) allowed communication between
32network endpoints, and 2) any required service function
chain traversal for each communication. PGA is designed so
that concise models that contain only a small number of ele-
ments are able to express policy for a much larger number of
endpoints. PGA achieves this model scalability by describ-
ing policies at the granularity of groups of endpoints (similar
to [6]) that share common properties expressed in terms of
primitives called labels. Labels can be assigned and changed
at runtime as endpoint properties (states) change, enabling a
static PGA graph model to capture all the policies that can
be dynamically assigned to an endpoint. PGA is a whitelist-
ing model; communication must be explicitly allowed by a
PGA model, else it is implicitly denied.
We use the example graph models shown in Fig. 3 to il-
lustrate various features of the PGA model and composi-
tion. Model (a) is given by an admin for all Departments
of a company and speciﬁes that trafﬁc is allowed from the
IT department to the Engineering department using speci-
ﬁed protocol port numbers. Model (b) is given by a web
application admin. This model allows trafﬁc from any De-
partment to access the Web application using port 80 (HTTP
protocol) and indicates that the trafﬁc will be load balanced.
The model also speciﬁes that trafﬁc is allowed from the Web
to DB tiers, and from DB to itself. Model (c) is speciﬁed
by an SDN application for DNS-based security protection.
This model requires that DNS trafﬁc from network endpoints
with “Normal” security status must be inspected by a Deep
Packet Inspection service when it performs DNS lookups,
while network endpoints that are deemed “Quarantined” can
only send their trafﬁc (of any type) to a Security Remedi-
ation server. Model (d) is actually two models given by
the Data Center admin. The ﬁrst model speciﬁes that traf-
ﬁc coming into the Data Center from the Campus must pass
through a Firewall service and a Byte Counter service, while
all east-west trafﬁc within the data center also traverses a
Byte Counter service. The second model allows monitoring
trafﬁc (port 9099) between endpoints within the Data Center.
PGA automatically combines multiple independently spec-
iﬁed policy graphs into a coherent composed policy. This
requires the novel ability to merge service functions chains
in addition to access control policies. The PGA model has
three primitives that support composition.
1. The packet processing behavior of each network ser-
vice function in a service function chain is explicitly
speciﬁed using a variant of the Pyretic network pro-
gramming language [34] that we developed. Our com-
position engine (§5) analyzes these descriptions in or-
der to automatically assemble composed service func-
tion chains that correctly combine policies from multi-
ple policy graphs.
2. A label mapping input is introduced to enable identi-
ﬁcation of endpoint groups that can have overlapping
endpoint membership. This avoids unnecessarily com-
posing endpoint groups that are mutually exclusive,
thus greatly reducing computation time and memory
requirements. Label mapping is necessary to detect
(a) Departments admin 
(b) Web application (App1) admin 
7000 
22,23,5900 
IT 
Engg 
Departments 
80 
LB 
Web 
3306 
DB 
(c) DNS protector SDN app 
(d) Data Center admin 
Normal 
53 
DPI 
Quarantined 
* 
DNS 
Rmd 
Campus 
*  FW 
BC 
BC 
DC 
* 
9099 
DC 
(IT:Zone-A, Engg:Zone-B, App1:DC, Departments:DNS protector) 
Label mapping inputs: 
Figure 3: Sample input graphs and label mapping.
Tnt 
Dpts 
apps 
App1 
Locn 
Status 
DNS 
Cmp 
DC  DNSP 
Rmd 
IT  Engg 
Web  DB 
Zn-A 
Zn-B 
Nml  Qn 
Abbreviations: 
Tnt: Tenant 
DC: Data Center 
Locn: Location 
Cmp: Campus 
Zn-A: Zone-A 
Zn-B: Zone-B 
Dpts: Departments 
DNSP: DNS Protector  
              Service 
Nml: Normal 
Qn: Quarantined  
Rmd: Remedy Server 
Figure 4: Sample input label namespace hierarchy.
overlapping membership not only between endpoint
groups of different graphs but also within a single graph.
3. Composition constraints are introduced to give policy
writers the ﬂexibility to express invariants that can
never be violated under composition. Each policy writer
can independently express their intended invariants, and
the PGA system will automatically compose these in-
dividual policies while respecting the invariants. This
avoids imposing rigid universally applied conﬂict res-
olution policies (such as static priority), and minimizes
the need for human intervention during composition.
We next describe the basic PGA graph constructs, fol-
lowed by the primitives that support composition.
4.1 Graph Constructs
Vertices and Labels: Each vertex in a PGA graph model
represents an endpoint group (EPG) which comprises a set of
endpoints (EPs). An EP is the smallest unit of abstraction for
which a policy is applied, e.g., a server, VM, client device,
network, subnet, or end-user. An EPG comprises all EPs
that satisfy a membership predicate speciﬁed for the EPG. In
Fig. 3, each membership predicate is given as a label, e.g.,
Web, DC, etc. In general, a membership predicate can be a
boolean expression over all labels.
Fig. 4 shows an example set of labels arranged in a hierar-
chy. The labels at the leaves, e.g., IT, Engg, Web, etc. are the
truly basic elements, while each non-leaf label is a compos-
ite label that is simply a convenient shorthand for the logical
disjunction (boolean OR) of all of its descendant leaf labels,
e.g., Dpts is equivalent to IT OR Engg. Though not shown
in the ﬁgure, composite labels can be deﬁned that do not ﬁt
into a hierarchy, and can in general represent shorthand no-
tation for arbitrary expressions over leaf labels. The hierar-
chy serves another important purpose for PGA composition.
As we describe in §5, the composition process translates in-
put graphs into a normalized form in which all EPGs have
disjoint membership. PGA therefore needs to know which
labels are mutually exclusive, i.e., cannot ever be assigned
33simultaneously to an EP. For example, in the DNS protector
app of model (c), an EP cannot be both Normal and Quar-
antined. The hierarchy provides this information. Specif-
ically, in any single tree of the hierarchy, any set of labels
that do not have an ancestor relationship are mutually exclu-
sive, e.g., {Zn-A, Zn-B}, {Cmp, DC}, and {Dpts, App1}.
The PGA system can restrict the scope of each policy
writer to a particular relevant subset of the label space; for
example, the admin of the Campus Network might only be
allowed to use labels Cmp, Zn-A, and Zn-B for deﬁning
EPGs in its policy graphs. Each leaf label represents a col-
lection of boolean variables, one per EP, i.e., leaf label x
represents the set of boolean variables {e.x|e ∈ E}, where
E is the set of all EPs. Assigning a leaf label to an EP sets
the value of the corresponding boolean variable to True, oth-
erwise it is False. Labels can be split into three categories:
‘tenant’ labels identifying end-users or their applications,
e.g., IT or Web, ‘network location’ labels identifying regions
of the network topology, e.g., Zn-A or DC, and ‘status’ la-
bels indicating dynamically changing properties, e.g., Qn for
currently quarantined EPs. To illustrate how labels are used,
say that server S is an EP that is located in the Data Center
and hosts the database of Web Application of Fig. 3. Then,
server S would be assigned labels DC and DB, setting its
boolean variables S.DC and S.DB to True.
EPs can be assigned labels dynamically at runtime, caus-
ing them to move from one EPG to another. For example, a
server that was assigned the label Nml (normal) could subse-
quently be relabeled Qn (quarantined) when a network mon-
itor detects the server issuing a DNS query for a known ma-
licious Internet domain. Thus, a static PGA graph model
actually describes a set of network policies that are applied
dynamically to each EP according to the EP’s status changes
over time (that can be programmed as a ﬁnite state machine
[28]). Moreover, analysis and composition of policy graphs
into a fully composed network policy graph is a procedure
that only needs to be invoked when policy graphs are added,
modiﬁed, or removed. Analysis is not needed when EPs
change EPG membership. Instead, the runtime system only
needs to perform the lightweight operation of looking up and
applying the correct rules for each EP depending on its cur-
rent EPG membership. As EPs change membership across
EPGs, the set of addresses encompassed within an EPG also
changes. In general, an EPG can be associated with a vari-
able representing virtual addresses indicating ‘some EP’
within the EPG – e.g., virtual
IPs used for server
load-balancing – and policies can be written using the EPG
variable as well.
Edges and Service Chains: As stated earlier, PGA is a
whitelisting model; by default, no communication is allowed
between any EPs. A directed edge between EPGs is required
to specify allowed communication in a PGA policy. An edge
consists of a classiﬁer, which matches packet header ﬁelds to
represent the security whitelisting rule. Classiﬁers may in-
clude virtual addresses described above. It also optionally
has a service chain consisting of a sequence of one or more
network function boxes. A network function box may corre-
spond to an SDN controller function or to a network middle-
box or a set of middleboxes. A directed edge from endpoint
group E to endpoint group E’ with Boolean predicate (clas-
siﬁer) B and path expression (service chain) P indicates that
a correct implementation will forward trafﬁc from all hosts
in E satisfying B along paths that are a preﬁx of the con-
catenation of the network function boxes in P and some host
in E’. For example, model (c) in Fig. 3 speciﬁes that trafﬁc
sent on port 53 is allowed from Normal (Nml) EPs to DNS