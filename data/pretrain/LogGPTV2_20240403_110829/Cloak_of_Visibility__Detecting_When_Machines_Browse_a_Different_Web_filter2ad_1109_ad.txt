visual differences in the layout and media presented to brows-
ing proﬁles of the same window dimensions (e.g., Mobile,
Desktop). This approach helps cover syntactically insigniﬁ-
cant HTML changes, such as introducing an iframe, that
signiﬁcantly change a page’s visible content. During each
crawl, we take a screenshot both upon visiting a URL and
after clicking on the largest hotlinked element. We convert
these screenshots from RGB into a grayscale n × m array,
after which we normalize each cell’s pixel intensity to a range
[0, 255]. We then calculate the per-pixel differences between
two screenshots S1 and S2, opting to measure the total pixels
that differ rather than the absolute value of their difference:
dif f =
n(cid:2)
m(cid:2)
x=0
y=0
Sx1,y1
(cid:2)= Sx2,y2
(1)
A high score in this regard indicates a substantially different
visual layout.
Element Similarity: While textual differences between crawls
may arise due to dynamic advertisements or newly generated
comments, deviations between a website’s template should be
less likely. To capture this intuition, we extract the set of URIs
E associated with all embedded images per document. We
then calculate the difference in media content between two
documents by using the Jaccard similarity coefﬁcient:
1 − |E1 ∩ E2|
|E1 ∪ E2|
(2)
A high score indicates there were multiple embedded images
absent from one or another crawl. To measure the similarity
in the page HTML structure, we repeat this same process
with divs and iframes, ﬁrst stripping the elements of any
attributes, and then calculating the fraction of overlapping tags
as an additional measure of structural similarity.
Request Tree Similarity: We compare the network requests
generated while crawling to detect divergent redirects, mis-
matched network errors, and additional content. We begin
by representing a sequence of network events (e.g., GET,
POST requests) as E = {e1, e2, . . . en} where an event ei
consists of a tuple (cid:5)Method, Domain, Response Code, Path(cid:6).
For two sequences E1 and E2 of potentially variable length,
we calculate the number of differing requests independent of
any timing information using the Jaccard similarity calculation
previously outlined in Equation 2. A high score indicates that
crawls triggered divergent network behavior. As an extension,
we separately calculate the difference in total response packet
size between two browsing proﬁles.
Topic Similarity: As an alternative to the previous ﬁne-
grained similarity metrics which may suffer in the presence
of dynamic website content, we compare the overall semantic
similarity of webpage content by extracting representative
topics based on visible text. Mechanistically, we rely on an
Latent Dirichlet allocation (LDA) implementation to extract
750750
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:18 UTC from IEEE Xplore.  Restrictions apply. 
a set of at most ten topics T per document [10], [13]. We
then calculate the similarity between two document’s topics
T1, T2, repeating the Jaccard index calculation presented in
Equation 2. A high score indicates the topics between pages
differs signiﬁcantly.
Screenshot Topic Similarity: Finally, due to potentially heavy
reliance on media rather than text, we also compare the topic
similarity of documents as detected by a deep convolutional
neural network that uses screenshots as an input [15]. As with
our text-based approach, for each screenshot we determine up
to ten topics T that describe the visual content. We then repeat
the same similarity calculation as outlined with text-based top-
ics. We use this method to catch pages that display additional
spam images (typically with sexual or pharmaceutical content)
that drastically changes a page’s perceived topic.
2) Per-page Dynamism Features
We estimate the natural, potentially legitimate dynamism
of individual pages per browser conﬁguration to help our
classiﬁer identify a minimum threshold above which divergent
content is likely indicative of blackhat cloaking. As previously
noted, we crawl each URL three times per browsing proﬁle
which we denote C1, C2, C3 for clarity. We recompute all of
the previous metrics for each possible pair Ci, Cj for i (cid:2)= j,
averaging each metric to arrive at a single per-feature, per-
page dynamism estimate. Finally, we provide the classiﬁer
both these similarity scores as well as the previous cross-
browser pairwise similarity metrics divided by our dynamism
estimates for that same feature (effectively calculating the ratio
of cross-proﬁle dynamism with intra-proﬁle dynamism).
3) Domain-speciﬁc Features
We extend our feature set with domain-speciﬁc signals that
target the entire collection of content crawled per URL (as
opposed to pairwise metrics), outlined below. We use these
for both classiﬁcation as well as to simplify analysis by
embedding meta-data about how miscreants cloak.
JavaScript, Meta, Flash Redirection: We include a single
boolean feature for whether a server redirects our crawler
via JavaScript, meta-refresh tag, or Flash to a domain
that does not match the ﬁnal landing page. Such behavior is
common for (compromised) doorway pages where cloaking
logic operates.
Googlebot Errors: We compare the size of requests returned
to our basic Googlebot proﬁle against all other proﬁles to
determine whether a server provides the crawler an error.
Several of the cloaking packages we reverse engineered offer
an option of serving Googlebot a page aimed at downplaying
the relevance of the page. Examples include a 404 interstitial
(e.g., “this site is no longer available”), parked domain page,
or fake security message such as “this site has been banned”).
Landing Domains: We annotate each URL with the total
number of landing page domains reached during all 33 visits,
with an intuition that divergent landing sites are suspicious.
D. Classiﬁcation
We employ Extremely Randomized Trees—an ensemble,
non-linear, supervised learning model constructed from a
collection of random forests where candidate features and
thresholds are selected entirely at random [11]. For training,
we rely on our labeled dataset of benign URLs from Alexa
and cloaked search (discussed earlier in this section). Prior to
classiﬁcation, we normalize all features into a range [0, 1] to
simplify the interpretation of which signals are most salient.
During classiﬁcation, we rely on ten-fold cross validation. We
discuss the overall performance of this classiﬁer in Section V
and its application to a holdout testing set for analysis in
Section VII.
V. IMPLEMENTATION
We implement our system on Google Compute Engine
with crawling and featurization distributed among 20 Ubuntu
machines. The classiﬁcation is performed on a single instance.
Our scheduler is built on top of Celery backed by Redis.
We compose crawling tasks as a tuple of a URL and proﬁle
that includes the target browser, network vantage point, and
context to appear from. Celery handles distributing tasks to
workers, monitoring success, and resubmitting failed jobs to
live workers. We operate three types of crawlers: a basic robot
that fetches URL content via the Python Requests library,
akin to wget; a headless instantiation of Chrome controlled
via Selenium, conﬁgured with a User-Agent for Mac OSX;
and the same Chrome browser except in mobile emulation
mode mimicking the Nexus 5 device with version 4.4 of the
Android operating system. Our network vantage points include
the authors’ residential networks, Google’s cloud network,
Google’s internal network, and mobile gateways belonging
to AT&T and Verizon as purchased via pre-paid plans. We
capture and log all network requests via mitmproxy. Finally,
for featurization and classiﬁcation we rely on scikit-learn [25],
Pandas [19], and a mixture of libraries previously mentioned
in Section IV for estimating content similarity and topic
modeling.
VI. EVALUATION
In this section, we explore the overall performance of our
classiﬁer, sources of false positives, the most salient features,
and the feasibility of unsupervised clustering. To conduct our
evaluation, we ﬁrst train and test a decision tree classiﬁer using
10-fold cross validation over our imbalanced dataset of 75,079
non-cloaked URLs and 19,867 cloaked URLs previously de-
tailed in Section IV. We rely on a grid search to tune classiﬁer
parameters related to our decision forest (e.g., number of trees,
depth of trees), ultimately electing the conﬁguration with the
optimum overall accuracy.
A. Overall Supervised Performance
We present the overall accuracy of our system in Table VIII.
We correctly detect 99.1% of Alexa URLs as non-cloaked
with a false positive rate of 0.9%. To achieve this degree of
accuracy, we overlook 18.0% of potentially cloaked counterfeit
751751
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:18 UTC from IEEE Xplore.  Restrictions apply. 
TABLE VIII: Performance of the supervised-learning classi-
ﬁer on the labeled train/test set, with 10-fold stratiﬁed cross
validation.
Accuracy
95.5%
TN Rate
99.1%
TP Rate
82.0%
FN Rate
18.0%
FP Rate
0.9%
Fig. 2: Receiver operating characteristic curve for the super-
vised classiﬁer (log-scale).
storefronts. If we examine the trade-off our system achieves
between true positives and false positives, presented in Fig-
ure 2, we ﬁnd no inﬂection point to serve as a clear optimum.
As such, operators of de-cloaking pipelines must determine an
acceptable level of false positives. For the remainder of our
study we rely on a false positive rate of 0.9%.
B. Source of Errors
False Positives: We manually investigate a random sample
of URLs our classiﬁer mislabeled to understand the principle
cause of errors. Qualitatively, we ﬁnd three sources of false
positives: (1) websites revising content between crawls, (2)
connectivity issues, and (3) noisy labels where some Alexa
URLs in fact cloak. In our current crawler implementation,
we fail to enforce a time window during which all crawls
must complete. This raises the risk that content substantially
changes between successive fetches, incorrectly triggering our
detection. We can solve this problem moving forward by
enforcing an SLA on time-to-crawl. A similar problem arises if
our crawler receives a 40X error or if a page is not fully loaded
when we take a screenshot, resulting in divergent image-based
and network-based similarity scores. Along this vein, we also
ﬁnd instances where CloudFlare DDoS protection automati-
cally blocks a fraction of our crawls, instead displaying an
interstitial “checking your browser” which we mistake for
a malicious interstitial. Finally, in rare cases, we ﬁnd that
some of the top Alexa sites serve cloaked ads that swap
content when presenting to a crawler, likely unbeknownst to
the site embedding the ad. These instances, as observed from
our classiﬁer, are in fact true positives, thus our overall false
positive rate will be lower in practice.
752752
False Negatives: We execute a similar qualitative analysis
for false negatives, ﬁnding the majority of errors arise due
to stability issues tied to cloaked websites. In particular,
while crawling pages multiple times from the same proﬁle
we often ﬁnd that the storefronts involved will throw transient
errors. This causes intra-proﬁle similarity metrics to deviate
as strongly as cross-proﬁle metrics, preventing an accurate
assessment. We investigate whether cloaking servers introduce
these errors intentionally (e.g., potentially blacklisting our
crawler’s IP address), but we ﬁnd no correlation between a
successful crawl and repeated errors afterward. Instead, errors
appear randomly during any crawl. Similarly, we ﬁnd some
of the URLs were taken down or expired by the time we
crawled them. These pages return the same error regardless
of proﬁle, thus leading the classiﬁer to believe they are not
cloaking. We also ﬁnd a high volume of counterfeit storefronts
that do not cloak, indicating that our labeled dataset is noisier
than expected from a cloaking perspective (while its original
collection was to study scams). These latter two sources of
errors indicate that our false negative rate is likely lower in
practice, though non-negligible.
Comparison to Previous Work: We note that our deﬁnition of
cloaking is the most comprehensive to date, including mobile
cloaking, graphical-only cloaking, and testing advanced cloak-
ing techniques such as rDNS cloaking. Because of this, the
performance of our classiﬁer is not comparable with previous
works that target speciﬁc types of cloaking, such as redirection
cloaking [18], [27] or referrer and user agent cloaking [32]. If
we restrict our detection to a speciﬁc type of cloaking, such
as redirection cloaking, our classiﬁer exhibits low to no false
positives. However, such technique-speciﬁc restrictions yield
a low recall that overlook sophisticated cloaking types such as
when cloaking software replaces a single embedded image’s
content to deliver a cloaked ad offering. As our study aims
to give a comprehensive overview of cloaking techniques in
the wild, we opted to favor high recall at the expense of some
false positives.
Additionally, our work is the ﬁrst to distinguish between
benign cloaking (e.g., mobile optimization, geolocalization,
personalized results) from blackhat cloaking. For example, our
detector is capable of determining that the mobile and desktop
version of cnn.com differ in content, but that difference results
exclusively from content optimization and should not be
labeled as blackhat cloaking. This challenge leads to a higher
degree of false negatives as we favor precision over recall to
reduce false positives from polluting our subsequent analysis.
C. Salient Features
We rank the importance of the top 20 features that impact
the accuracy of our classiﬁer according to their Gini impor-
tance [4], effectively calculating the weight of the feature
across all trees in our decision forest. We present our ﬁndings
in Figure 3. The classiﬁer associates the highest weight with
JavaScript redirects that cross an origin boundary. Indeed,
41.8% of labeled cloaking URLs rely on this technique
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:18 UTC from IEEE Xplore.  Restrictions apply. 
compared to 0.62% of presumed non-cloaking URLs. The
remaining top 19 features span a spectrum of feature categories
covering content, network request, topic, and screenshot simi-
larity among multiple combinations of browsing proﬁles. This
indicates all of our similarity metrics and proﬁles contribute
to classiﬁcation results, each covering a non-overlapping ap-
proach to cloaking.
Exploring feature salience in more detail, we examine the
overall accuracy of our system when trained only a single
class of similarity metrics. Figure 4 indicates that comparing
the structure of pages is the most effective technique for
minimizing false negatives, whereas topic, embedded element,
and screenshot-based similarity metrics perform the worst in
isolation. We recast this same measurement in Figure 5, except
this time removing only a single comparison method from
training. We ﬁnd that a model with no screenshot similarity
introduces the most false negatives, while removing page
structure alone has the least impact. Our ﬁndings reiterate
that an ensemble of comparison techniques are required to
accurately detect split-view content.
D. Minimum Proﬁle Set
Finally, we quantify the trade-off between anti-cloaking
pipeline performance, and its efﬁciency and complexity. To
do so, we start with the full system described here, and we
repeatedly identify the crawling proﬁle that, when removed,
least impacts the false positive rate. The result of this greedy
search of the anti-cloaking pipeline with the minimum capa-
bilities is shown in Figure 6.
As with all classiﬁcation scores shown here, the scores are
the mean values in a ten-fold stratiﬁed cross validation. The
results indicate that an anti-cloaking pipeline would still have
an acceptable performance without a mobile user on a mobile
network, and without the content similarity feature class. If
any more capabilities are subtracted, the false negative rate
doubles, whereas the false positive rate remains fairly low
even for a pipeline composed only by a mobile browser,
desktop browser and Googlebot, all crawling from Google
IPs and cloud IPs. These browsers support clicking, taking
screenshots, and visit URLs with the same proﬁle repeatedly.
Since any further simpliﬁcation of this basic anti-cloaking
pipeline doubles the false positive rate, we have established
that this is the minimum anti-cloaking platform that is both
efﬁcient, by avoiding unnecessary crawling and featurization,
and effective against current cloaking. We caution readers that
this evaluation of the minimum viable anti-cloaking pipeline
should be performed routinely, so to react in a timely manner
to a spread in popularity of more advanced cloaking tech-
niques.
E. Unsupervised Alternative
Supervised learning requires a steady stream of training data
in the event miscreants adapt their techniques for displaying
split-view content. As a potential alternative, we compare our
supervised classiﬁer’s accuracy to that of an unsupervised
clustering equivalent based on Gaussian mixture models. We
TABLE IX: Performance of the unsupervised-learning classi-