Return Values
Timing
Total gadgets
24102 (100%)
13691 (56.8%)
10106 (41.9%)
13989 (58.0%)
12236 (50.8%)
14165 (58.8%)
Distinct gadgets
2059 (100%)
1947 (94.6%)
1720 (84.0%)
1999 (97.1%)
1995 (96.9%)
1972 (95.8%)
Syscalls
60 (100%)
4 (6.7%)
1 (1.7%)
3 (5.0%)
14 (23.3%)
16 (26.7%)
Table 1 describes how many gadgets are available when only
certain types of information are leaked (the Galileo algorithm as
described by Shacham [33] was used for these statistics). When all
function locations are known, the attacker has access to over 24,000
total gadgets which are comprised of 2059 distinct gadgets. When
0x00 byte locations are leaked, an attacker can ﬁnd the locations
of only 56.8% of all gadgets. However, this still includes 94.6% of
distinct gadgets. Syscall gadgets are also available to the attacker,
while four in total are available, an attacker will only need one to
write an exploit. Thus, malicious payloads can still be constructed
with only this amount of information leaked.
Functions that have an USS greater than zero can still be use-
ful, as these functions can be very similar internally. For example,
upon examining the group of functions that have the largest USS of
108, we ﬁnd that they all are wrappers for system calls. These in-
clude functions such as bind, listen, and mprotect, which are often
important for malicious payloads. However, these functions only
differ in their ﬁrst instruction, which moves a constant signifying
what system call is intended into a register. The rest of the function
is identical across all 108, where the syscall instruction is executed
and then return values are examined. A payload could be crafted
where some other gadget is used to load the register, perhaps using
a pop instruction, and then jumps directly to the syscall instruction.
4.2 Byte Sequences
4.3 Return Instructions
The Overwrite Data and Overwrite Data Pointer attacks can
cause information to be leaked by overwriting an index to a data
pointer, or a data pointer itself, to point to the code segment. In
the previous section, we gave an example of a fault analysis attack
where some calculation was done on the code, and also an example
of a timing attack where a loop was executed a number of times
Several of the attacks described can help determine where both
intentional and unintentional return instructions are in the code. By
using the Overwriting Code Pointer attack and taking advantage of
either fault analysis or timing side channels, we can reveal such in-
formation. If an attacker can discern this information but nothing
else, we evaluate if he can determine what function he is actually
n
o
i
t
u
b
i
r
t
s
i
D
y
t
i
l
i
b
a
b
o
r
P
 0.14
 0.12
 0.1
 0.08
 0.06
 0.04
 0.02
 0
s
n
o
i
t
c
n
u
F
f
o
n
o
i
t
c
a
r
F
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
0x00 Byte Locations Known
1
2
3
4
All
 0
 50
 100
 150
 200
 250
 0
 10
 20
 30
 40
 50
 60
 70
Byte Value
Uncertainty Set Size
s
n
o
i
t
c
n
u
F
f
o
n
o
i
t
c
a
r
F
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
Return Instruction Locations Known
1
All
2
 0
 10
 20
 30
 40
 50
 60
 70
Uncertainty Set Size
Figure 3: Information learned as more re-
turn instructions are known
Figure 1: Probability distribution function
of number of bytes found in code
measuring. Speciﬁcally, we evaluate if functions have distinct pat-
terns of where return instructions are internally located.
Figure 2:
0x00 byte locations are known
Information learned as more
Figure 3 shows a CDF of the fraction of functions that have a
particular USS or less. We ﬁnd that if the location of one return
instruction is leaked, then 6.4% of functions are uniquely identiﬁ-
able. However, if two locations are leaked, then 42.2% of functions
are identiﬁable. Knowing two locations leaks almost as much in-
formation as knowing all locations, as we ﬁnd if all locations are
known then 42.6% of functions are uniquely identiﬁable. Further-
more, 90% of functions have an USS of 18 or less, meaning that
this measurement greatly reduces the USS for most functions by a
large amount. Similar to before, we ﬁnd that a large number of sys-
tem call wrappers are grouped together, thus have an USS greater
than zero, but can be used as previously mentioned.
Leaking return instruction locations yields fewer gadgets than
other information leaks (Table 1). However, 41.9% of all gadgets,
84.0% of distinct gadgets, and 1 syscall gadget are still available.
4.4 Output
If the attacker is able to conduct an Overwriting Code Pointer
attack and can retrieve some information about the result of the
execution, he can conduct some fault analysis and discern infor-
mation about what was executed. In the best case, if the attacker
could learn all the inputs used for a function and all the resulting
outputs, he could potentially learn much about what was executed.
However, we evaluate what an attacker can do with a much more
limited amount of information, namely crashes and the return val-
ues of functions.
We evaluate ﬁrst the USS of functions if the attacker is able to
discern if the program crashes or not. To evaluate this, we overwrite
a function pointer and test to see if the program crashes when the
function pointer is called. We demonstrate our results in Figure 4.
Many crash locations are needed before a signiﬁcant number of
functions are uniquely identiﬁable, over 60 locations are needed to
know just 23% of functions. If all crash locations are known, over
56% of functions are distinct in their crash patterns and 78% of
functions have an USS of 10 or less.
Learning crash locations allows more distinct gadgets (Table 1)
to be leaked than the other types of information leaks that we in-
vestigate. Speciﬁcally, 97.1% of distinct gadgets are leaked, while
58.0% of all gadgets and 3 syscall gadgets are leaked.
While an attacker might be able to retrieve different memory val-
ues or registers after something is executed, a likely output is sim-
ply the return value of a function. In x86, this is the value found in
the %eax register. Thus, we evaluate if the pattern of return values,
when jumping into and executing different parts of a function, is
distinct for a function. Figure 5 shows that when one return value
is known, 12% of functions are distinct, and when two values are
known, 38% of functions are distinct. When the attacker learns all
values, 57% of functions are distinct, while 75% of functions have
an USS of six or less.
The group of functions that have the largest USS of over 600
mostly consist of internal libc functions that are used by other
functions. Thus, these functions are likely to not be directly needed
for any common malicious payloads. We again ﬁnd that wrappers
to system calls are largely grouped together due to their internal
similarity. The return value of executions also gives the ability to
learn the most distinct gadgets (Table 1). 96.9% of distinct gadgets,
50.8% of all gadgets and 14 syscall gadgets are leaked.
4.5 Timing
An attacker will most likely be able to, at the very least, time
execution. Thus, if he can use an Overwrite Code Pointer attack,
and then time the resulting execution, he could possibly still gain a
great deal of information about what is being executed. Similar to
the previous experiment, we overwrite a code pointer to jump into
and execute all locations within a function. We then evaluate how
distinctive timing is to different functions.
Figure 6 shows that when only one timing value is known, only
10% of functions are uniquely identiﬁable. However, if two loca-
tions are known, the number jumps to 38%. If an attacker knows all
the timing values, he can learn up to 60% of functions with an USS
of zero and 76% of functions with USS of ﬁve or less. We ﬁnd that
timing is very effective in leaking gadgets. 58.8% of all gadgets,
95.8% of distinct gadgets, and 16 syscall gadgets are leaked.
5. PRACTICAL SIDE CHANNELS
In this section we describe our framework that can be used to
practically leak information about the code incrementally. We fo-
cus on Overwrite Data Pointer timing side channel attacks, because
as previous sections have demonstrated, they have the most poten-
tial for practical use. This is due to them having few limitations,
they do not require the application to be able to crash and restart,
they are ﬂexible in the types of information they can leak, and they
are effective in the amount of information they can leak.
Overview. Information leaks are most likely a stepping stone
to other attacks. Thus once an attacker learns enough information
about the location of certain gadgets, he will conduct the intended
exploit (e.g., starting a shell). Furthermore, if an attacker learns
certain other helpful gadgets, he can incorporate them into the at-
tack as well. For example, if the attacker has found enough gadgets
to create a loop, allowing him to reduce the needed timing pre-
cision, he would use them. Gadgets found in the executables are
helpful in facilitating the information leakage, but they are often
not enough for practical payloads as discussed earlier. An attacker
can use these gadgets to ﬁnd other system call gadgets in libc.
Thus, at a high level, our information leak framework iterates
over three different steps. First, it evaluates a pool of currently
known gadgets, and discerns if these gadgets can be turned into a
new or more efﬁcient information leaking capability. Second, it
s
n
o
i
t
c
n
u
F
f
o
n
o
i
t
c
a
r
F
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
s
n
o
i
t
c
n
u
F
f
o
n
o
i
t
c
a
r
F
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
Return Values Known
All
3
4
1