we navigate the chain of actions. Following this navigation,
the scanner is ready to parse the page. First, we inspect the
page for inter-state dependency tokens and add the necessary
dependency edges, as shown in Algorithm 3. Each token
will contain a taint value, explained more in Section III-C,
a source edge and a sink edge. If a source and sink are
found, our scanner will fuzz the source and check the sink.
Afterward, we extract any new possible navigation resources
and add them to the graph. Next, we fuzz any possible
parameters in the edge and then inject a taint token. The
order is important as we want the token to overwrite any
stored fuzzing value. Finally, the edge is marked as visited
and the loop repeats.
The goal of this combination is to improve both vulner-
ability detection and code coverage. The three parts of the
approach support each other to achieve this. A strong model
that handles different navigation methods and supports aug-
mentation with path and dependency information will enable
a richer interaction with the application. Based on the model
we can build a strong crawler component that can handle
complex workﬂow which combines requests and client-side
events. Finally, by tracking inter-state dependencies we can
improve detection of stored vulnerabilities.
A. Navigation Modeling
Our approach is model-based in the sense that it creates,
maintains, and uses a model of the web application to
drive the exploration and detection of vulnerabilities. Our
model covers both server-side and client-side aspects of
the application. The model
tracks server-side inter-state
dependencies and workﬂows. In addition, it directly captures
elements of the client-side program of the web application,
i.e., HTML and the state of the JavaScript program.
1128
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:14:17 UTC from IEEE Xplore.  Restrictions apply. 
index.php
link
1
login.php
log in
2
admin.php
click
3
admin.php#users
link
5
view users.php
username
4
add user
Figure 1. Example of a web application where anyone can see the list of users and the admin can add new users. The dashed red line represents the
inter-state dependency. Green lines are HTML5 and orange symbolises JavaScript. The dotted blue lines between edges would be added by our scanner to
track its path. The sequence numbers shown the necessary order to ﬁnd the inter-state dependency.
Data: Target url
1 Global: tokens // Used in Algorithm 3
2 Graph navigation; // Augmented navigation graph
3 navigation.addNode(empty);
4 navigation.addNode(url);
5 navigation.addEdge(empty, url);
6 while unvisited edge e in navigation do
traverse(e); // See Algorithm 2
7
inspectTokens(e, navigation); // See Algorithm 3
8
resources = extract({urls, forms, events, iframes});
9
for resource in resources do
10
11
navigation.addNode(resource)
navigation.addEdge(e.targetNode, resource)
end
attack(e);
injectTokens(e);
mark e as visited;
12
13
14
15
16 end
Algorithm 1: Scanner algorithm
workflow = []; // List of edges
currentEdge = e;
while prevEdge = currentEdge.previous do
workflow.prepend(currentEdge);
if isSafe(currentEdge.type) then
1 Function traverse(e: edge)
2
3
4
5
6
7
8
9
10
11
12 end
end
navigate(workflow);
break;
end
currentEdge = prevEdge
Algorithm 2: Traversal algorithm
Model Construction: Our model is created and updated
at run-time while scanning the web application. Starting
from an initial URL, our scanner retrieves the ﬁrst webpage
and the referenced resources. While executing the loaded
JavaScript, it extracts the registered JavaScript events and
adds them to our model. Firing an event may result in
changing the internal state of the JavaScript program, or
retrieving a new page. Our model captures all these aspects
and it keeps track of the sequence of ﬁred events when
revisiting the web application, e.g., for the detection of
vulnerabilities.
Accordingly, we represent web applications with a labeled
1129
if pageSource(e) contains token.value then
token.sink = e;
g.dependency(token.source, token.sink);
attack(token.source, token.sink);
end
end
for token in tokens do
1 Function inspectTokens(e: edge, g: graph)
2
3
4
5
6
7
8
9 end
10 Function injectTokens(e: edge)
for parameter in e do
11
12
13
14
15
16
17 end
token.value = generateToken();
token.source = e;
tokens.append(token);
inject token in parameter;
end
Algorithm 3: Inter-state dependency algorithms
directed graph, where each node is a state of the client-
side program and edges are the action (e.g., click) to move
from one state to another one. The state of our model
contains both the state of the page, i.e., the URL of the page,
and the state of the JavaScript program, i.e., the JavaScript
event that triggered the execution. Then, we use labeled
edges for state transitions. Our model supports four types
of actions, i.e., GET requests, form submission, iframes
and JavaScript events. While form submissions normally
result in GET or POST requests, we need a higher-level
model for the traversing method explained in Section III-B.
We consider iframes as actions because we need to model
the inter-document communication between the iframe and
the parent, e.g ﬁring an event in the parent might affect
the iframe. By simply considering the iframe source as a
separate URL, scanners will miss this interaction. Finally,
we annotate each edge with the previous edge visited when
crawling the web application, as shown in Figure 1. Such
an annotation will allow the crawler to reconstruct the paths
within the web application, useful information for achieving
deeper crawling and when visiting the web application for
testing.
Extraction of Actions: The correct creation of the
model requires the ability to extract the set of possible
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:14:17 UTC from IEEE Xplore.  Restrictions apply. 
actions from a web page. Our approach uses dynamic
analysis approach, where we load a page and execute it in
a modiﬁed browser environment, and then we observe the
execution of the page, monitoring for calls to browser APIs
to register JavaScript events and modiﬁcation of the DOM
tree to insert tags such as forms and anchors.
Event Registration Hooking Before loading a page we in-
ject JavaScript which allows us to wrap functions such as
addEventListener and detect DOM objects with event
handlers. We accomplish this by leveraging the JavaScript li-
braries developed for the j ¨Ak scanner [8]. While lightweight
and easy to use, in-browser instrumentation is relatively
fragile. A more robust approach could be directly modifying
the JavaScript engine or source-to-source compile the code
for better analysis.
DOM Modiﬁcation To detect updates to the page we rescan
the page whenever we execute an event. This allows us to
detect dynamically added items.
Inﬁnite Crawls: When visiting a webpage, crawlers
can enter in an inﬁnite loop where they can perform the
same operation endlessly. Consider the problem of crawling
an online calendar. When a crawler clicks on the View
next week button, the new page may have a different URL
and content. The new page will container again the button
View next week, triggering an inﬁnite loop. An effective
strategy to avoid inﬁnite crawls is to deﬁne (i) a set of
heuristics that determine when two pages or two actions
are similar, and (ii) a hard limit to the maximum number of
“similar” actions performed by the crawler. In our approach,
we deﬁne two pages to be similar if they share the same
URL except for the query string and the fragments. For ex-
ample, https://example.domain/path/?x=1 and
https://example.domain/path/?x=2 are
simi-
lar whereas https://example.domain/?x=1 and
https://example.domain/path/?x=2 are differ-
ent. The hard limit
is a conﬁguration parameter of our
approach.
B. Traversal
To traverse the navigation model we pick unvisited edges
from the graph in the order they were added, akin to breadth-
ﬁrst search. This allows the scanner to gain an overview
of the application before diving into speciﬁc components.
The edges are weighted with a positive bias towards form
submission, which enables this type of deep-dive when
forms are detected.
To handle the challenge of session management, we pay
extra attention to forms containing password ﬁelds, as this
symbolizes an opportunity to authenticate. Not only does
this enable the scanner to re-authenticate but it also helps
when the application generates a login form due to incorrect
session tokens. Another beneﬁt is a more robust approach to
complicated login ﬂows, such as double login to reach the
administrator page—we observed such workﬂow in phpBB,
one of the web applications that we evaluated.
The main challenge to overcome is that areas of a web
application might require the user to complete a speciﬁc
sequence of actions. This could, for example, be to review a
comment after submitting it or submit a sequence of forms
in a conﬁguration wizard. It is also common for client-side
code to require chaining, e.g. hover a menu before seeing
all the links or click a button to dynamically generate a new
form.
We devise a mechanism to handle navigation dependen-
cies by modeling the workﬂows in the application. Whenever
we need to follow an edge in the navigation graph, we
ﬁrst check if the previous edge is considered safe. Here
we deﬁne safe to be an edge which represents a GET
request, similar to the HTTP RFC [19]. If the edge is
safe, we execute it immediately. Otherwise, we recursively
inspect the previous edge until a safe edge is found, as
shown in Algorithm 2. Note that the ﬁrst edge added to the
navigation graph is always a GET request, which ensures
a base case. Once the safe edge is found, we execute the
full workﬂow of edges leading up to the desired edge.
Although the RFC deﬁnes GET requests to be idempotent,
developers can still implement state-changing functions on
GET requests. Therefore, considering GET requests as safe
is a performance trade-off. This could be deactivated by a
parameter in Black Widow, causing the scanner to traverse
back to the beginning.
Using Figure 1 as an example if the crawler needed
to submit a form on admin.php#users then it would
ﬁrst have to load login.php and then submit that form,
followed by executing a JavaScript event to dynamically add
the user form.
We chose to only chain actions to the previous GET
request, as they are deemed safe. Chaining from the start
is possible, but it would be slow in practice.
C. Inter-state Dependencies
One of the innovative aspects of our approach is to
identify and map the ways user inputs are connected to
the states of a web application. We achieve that by using
a dynamic, end-to-end taint tracking while visiting the web
application. Whenever our scanner identiﬁes an input ﬁeld,
i.e., a source, it will submit a unique token. After that,
the scanner will look for the token when visiting other
webpages, i.e., sinks.
Tokens: To map source and sinks, we use string tokens.
We designed tokens to avoid triggering ﬁltering functions or
data validation checks. At the same time, we need tokens
with a sufﬁciently high entropy to not be mistaken for other
strings in the application. Accordingly, we generate tokens
as pseudo-random strings of eight lowercase characters e.g.
frcvwwzm. This is what generateToken() does in
Algorithm 3. This could potentially be improved by making
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:14:17 UTC from IEEE Xplore.  Restrictions apply. 
1130
the tokens context-sensitive, e.g. by generating numeric
tokens or emails. However, if the input is validated to only
accept numbers, for example, then XSS is not possible.
Sources and Sinks: The point in the application where
the token is injected deﬁnes the source. More speciﬁcally,
the source is deﬁned as a tuple containing the edge in the
navigation graph and the exact parameter where the token
was injected. The resource in the web application where
the token reappears deﬁnes the sink. All the sinks matching
a certain source will be added to a set which in turn is
connected to the source. Similar to the sources, each sink
is technically an edge since they carry more context than
a resource node. Since each source can be connected to
multiple sinks, the scanner needs to check each sink for
vulnerabilities whenever a payload is injected into a source.
In our example in Figure 1, we have one source and one
connected sink. The source is the username parameter in the
form on the management page and the sink is the view users
page. If more parameters, e.g. email or signature, were also
reﬂected then these would create new dependency edges in
the graph.
D. Dynamic XSS detection
After a payload has been sent,
the scanner must be
able to detect
if the payload code is executed. Black
Widow uses a ﬁne-grained dynamic detection mechanism,
making false positives very improbable. We achieve this
by injecting our JavaScript function xss(ID) on every
page. This function adds ID to an array that our scan-
ner can read. Every payload generated by Black Widow
will
this function with a random ID, e.g.
 Finally, by in-
specting the array we can detect exactly which payloads
resulted in code execution.
try to call
For this to result in a false positive, the web application
would have to actively listen for a payload, extract the ID,
and then run our injected xss(ID) function with a correct
ID.
IV. EVALUATION
In this section, we present the evaluation of our approach
and the results from our experiments. In the next section,
we perform an in-depth analysis of the factors behind the
results.
To evaluate the effectiveness of our approach we imple-
ment it in our scanner Black Widow and compare it with
7 other scanners on a set of 10 different web applications.
We want to compare both the crawling capabilities and vul-
nerability detection capabilities of the scanners. We present
the implementation details in Section IV-A. The details of
the experimental setup are presented in Section IV-B. To
measure the crawling capabilities of the scanners we record
the code coverage on each of application. The code coverage
results are presented in Section IV-C. For the vulnerability
detection capabilities, we collect
the reports from each
scanner. We present both the reported vulnerabilities and
the manually veriﬁed ones in Section IV-D.
A. Implementation
Our prototype implementation follows the approach pre-
sented above in Section III. It exercises full dynamic execu-
tion capabilities to handle such dynamic features of modern
applications like AJAX and dynamic code execution, e.g.
eval. To achieve this we use Python and Selenium to
control a mainstream web browser (Chrome). This gives us
access to a state-of-the-art JavaScript engine. In addition, by
using a mainstream browser we can be more certain that the
web application is rendered as intended.
We leverage the JavaScript libraries developed for the
j ¨Ak scanner [8]. These libraries are executed before load-
ing the page. This allows us to wrap functions such as
addEventListener and detect DOM objects with event
handlers.
B. Experimental Setup
In this section, we present the conﬁguration and method-
ology of our experiments.
Code Coverage: To evaluate the coverage of the scan-
ners we chose to compare the lines of code that were
executed on the server during the session. This is different
from previous studies [13], [8], which relied on requested
URLs to determine coverage. While comparing URLs is
easier, as it does not require the web server to run in debug
mode, deriving coverage from it becomes harder. URLs can
contain random parameter data, like CSRF tokens, that are
updated throughout the scan. In this case, the parameter
data has a low impact on the true coverage. Conversely, the
difference in coverage between main.php?page=news
and main.php?page=login can be large. By focusing
on the execution of lines of code we get a more precise
understanding of the coverage.
Calculating the total number of lines of code accurately
in an application is a difﬁcult task. This is especially the
case in languages like PHP where code can be dynamically
generated server-side. Even if possible, it would not give a
good measure for comparison as much of the code could be
unreachable. This is typically the case for applications that
have installation code, which is not used after completing it.
Instead of analyzing the fraction of code executed in the
web application, we compare the number of lines of code
executed by the scanners. This gives a relative measure
of performance between the scanners. It also allows us to
determine exactly which lines are found by multiple scanners
and which lines are uniquely executed.
To evaluate the code coverage we used the Xdebug [20]
module in PHP. This module returns detailed data on the
lines of code that are executed in the application. Each
request to the application results in a separate list of lines
of code executed for the speciﬁc request.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:14:17 UTC from IEEE Xplore.  Restrictions apply. 
1131
LINES OF CODE (LOC) EXECUTED ON THE SERVER. EACH COLUMN REPRESENTS THE COMPARISON BETWEEN BLACK WIDOW AND ANOTHER
CRAWLER. THE CELLS CONTAIN THREE NUMBERS: UNIQUE LOC COVERED BY BLACK WIDOW (A \ B), LOC COVERED BY BOTH CRAWLERS (A ∩ B)
AND UNIQUE LOC COVERED BY THE OTHER CRAWLER (B \ A). THE NUMBERS IN BOLD HIGHLIGHT WHICH CRAWLER HAS THE BEST COVERAGE.
Table I
Crawler
Drupal
HotCRP
Joomla
osCommerce
phpBB
PrestaShop
SCARF
Vanilla
WackoPicko
WordPress
Arachni
A \ B A ∩ B B \ A
Enemy
A \ B A ∩ B B \ A
j ¨Ak
A \ B A ∩ B B \ A
Skipﬁsh
A \ B A ∩ B B \ A
w3af
A \ B A ∩ B B \ A
Wget
A \ B A ∩ B B \ A
ZAP
A \ B A ∩ B B \ A
35 146
2 416
14 573
3 919
2 822
105 974
189
5 381
202
8 871
22 870
16 076
29 263
6 722
5 178
75 924
433
9 908
566
45 345
757
948
1 390
172
492
65 650
12
491
2
1 615
6 365
16 573
33 335
9 626
2 963
157 095
270
6 032
58
35 092
51 651
1 919
10 501
1 015
5 037
24 803
352
9 257
710
19 124
20 519
0
621
15
337
3 332
5
185
9
256
25 198
6 771
24 728
4 171
3 150
155 579
342
3 122
463
18 572
32 818
11 721
19 108
6 470
4 850
26 319
280
12 167
305
35 644
5 846
271
1 079
507
348
58
2
536
0
579
29 873