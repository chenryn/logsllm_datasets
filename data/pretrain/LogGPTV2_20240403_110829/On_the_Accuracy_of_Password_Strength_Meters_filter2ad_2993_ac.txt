5.41
2.79
3.76
1.83
22.78
3.09
5.52
14.93
0.05
0.05
0.05
0.05
0.05
0.05
0.02
0.01
0.04
0.02
0.17
0.05
0.05
0.19
PE
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
PE-5 PU wPE wPE-5 wPU
0.99
0.68
0.68
0.99
0.99
0.68
0.99
0.68
0.99
0.68
0.99
0.68
0.75
0.69
0.54
0.90
0.75
0.78
0.90
0.57
1.00
0.90
0.99
0.70
0.99
0.68
0.96
0.98
0.95
0.95
0.95
0.95
0.95
0.95
0.75
0.90
0.36
0.71
1.00
0.97
0.95
0.92
0.96
0.96
0.96
0.96
0.96
0.96
1.00
1.00
1.00
1.00
1.00
0.97
1.00
1.00
0.24
0.24
0.24
0.24
0.24
0.24
0.81
0.20
0.36
0.19
0.92
0.26
0.29
0.99
Sim.: Expected similarity with REF-A; L=Low similarity, M=Medium similarity, H=High similarity.
Weighted Pearson Correlation: Is defined as (normal) Pearson
correlation but weighting each data point with a weight vector,
where we use the reference (REF-A) as the weight vector. This
similarity measure exhibits similar problems as unweighted Pear-
son correlation for monotonic transformations, as expected, even
though the effect is less pronounced, and remains highly sensitive
to quantized data.
Weighted Spearman Correlation: Is defined as weighted Pear-
son correlation on the ranks. It is the most promising similarity
measure considered so far. As ordinary (unweighted) Spearman
correlation, it handles monotonic transformations well. It gives a
correlation close to 1 to the test case REF-B (and the other mono-
tone transformations including most quantized test cases, which
was problematic before), as now the weights prevent the over-
representation of strong passwords. It also handles the INV-WEAK-
5 and INV-STRONG-5 cases well, where it assigns roughly the same
correlation to both cases of around 0.7, a moderate but noticeable
lower value than 1.
5.2.3 Mean Error. Another set of similarity measures is mean
square error (MSE) and related concepts. We tested variations, in-
spired by the above results and techniques used in previous work.
Mean Absolute Error (MAE): Is defined as the average absolute
error, with equal weight for each data point. A similar measure was
used recently [71], where a logarithmic error was used. Our test
cases reveal the following problems: It is highly sensitive to mono-
tonic transformations, even linear ones (and previous work [71]
needed to adapt the scales of the meters to get a reasonable compar-
ison). Large deviations in the rating for single passwords only have
moderate impact on the similarity (due to taking absolute errors
only). Its sensitivity to deviations in frequent passwords is low (the
error for INV-WEAK-5 is 6, only marginally larger than the error
due to random sampling (REF-B with an error of 4).
Mean Squared Error (MSE): Is defined as the average over the
squared error, giving more weight to large deviations. Properties
of MSE are very similar to that of MAE.
Ranked Mean Absolute/Squared Error (rMAE/rMSE): Here
we first rank the data (assigning ties the average rank), and com-
pute the MAE or MSE of the ranks. As expected, the resulting
measures are resistant to monotonic transformations. However, as
they are non-weighted, they fail to capture errors for few frequent
passwords (INV-WEAK-5). This means, in the bad performing PSM
test case (INV-WEAK-5 ) both, rMAE and rMSE fail to show any
difference to the reference making them unsuitable.
5.2.4 Weighted Mean Error. All error measures discussed in the
previous subsection are unweighted, and thus fail to capture errors
in few frequent passwords. In this subsection, we consider weighted
variants.
Weighted and Ranked Mean Abs./Sq. Error (wrMAE/wrMSE):
When we use both ranked and weighted data points, the resulting
similarity measure becomes more discriminative, e. g., it allows to
distinguish the INV-WEAK-5 and DOUBLE test cases. Both mea-
sures work very well on our test-cases (remember that lower values
mean more similarity) and seem to be a reasonable choice.
5.2.5 One-Sided Errors. As described before, password strength ap-
proximations can be under- or overestimates. Previous work [46, 71]
observed that a meter underestimating the security of strong pass-
words (e. g., INV-STRO-5) is less problematic than overestimating
the strength of weak passwords (e. g., INV-WEAK-5). The former
results in a user simply selecting another (presumably secure) pass-
word, whereas in the latter case the user believes having selected a
secure password, where in reality it is weak.
Weighted and Ranked Mean Abs./Squared One-Sided Lower
Error (wrLAE/wrLSE): One can define versions for MAE/MSE
that only take one-sided errors into account. If this measure oper-
ates on count values, this approach favors meters that generally
underestimate security: a meter that rates all passwords insecure
(i. e., a high count value) will get a high rating. This can be prevented
by operating on ranked data. On the tested datasets and test cases
the resulting measures, wrLAE/wrLSE perform similarly to their
two-sided versions wrMAE/wrMSE. A likely explanation is that
wrLAE/wrLSE operate on ranked data. Therefore, overestimating
the strength of one password generally leads to underestimating
the strength of another password. (Weights and squaring differ-
ences (wrLSE) mean that the results still can differ, however, these
effects seem to even out on the dataset that we considered.) For
applications that call for one-sided metrics, one should consider
non-ranked similarity metrics at the cost of losing the ability to
tolerate monotonic transformations.
5.2.6 Pairwise Errors. In preliminary tests, we observed that sev-
eral similarity measures give a low similarity score to quantized
data. This behavior is undesirable, as heavy quantization loses in-
formation about the distribution.
We tried to address this problem by designing a similarity score
that is based on two individual metrics: an error metric which de-
scribes how many passwords are not in the “correct” order, and a
utility metric which describes if the meter provides “useful” and dis-
criminative output. (To illustrate this problem, consider a strength
meter with binary output, where only a few very strong passwords
are “accepted,” and the other passwords are “rejected”. This meter
would have a low error rating, as it mostly preserves the order, but
a low utility rating, as most passwords are in the same bin). This
mechanism is based on the rank. We evaluated several variants of
this basic idea.
Pairwise Error/Utility Rate (PE/PU): These consider the relative
ranking of all pairs of passwords. PE considers the fraction of pairs
where the meter and the reference disagree (where a tie in one of
the two is not counted as a disagreement), whereas the PU considers
the fraction of pairs where the meter sees a tie. (A meter outputting
the same strength for all passwords, i. e., uses a single bin, has a PE
of 0, but also a PU of 0.)
Pairwise Error Rate More Than 5 % (PE-5): As small deviations
are typically considered a non-problem, for this variant we tolerate
any deviation that is less than 5 % (in terms of rank) and do not
count them towards the error.
5.2.7 Weighted Pairwise Errors. We have argued before that un-
weighted measures not taking into account the specific probabilities
of passwords systematically bias results.
Weighted Pairwise Error/Utility Rate (wPE/wPU)/Weighted
Pairwise Error Rate More Than 5 % (wPE-5): We use weighted
versions of the three measures introduced before, where we weight
each pair with the product of the probabilities of the two passwords.
Implementation: All measures are implemented using R v3.4.4
(March 2018). For Pearson and Spearman, we use standard R. For
weighted Pearson and Spearman we use the wCorr package2. For
calculating the Kendall correlation, we use a O(n log n) optimized
version from pcaPP3.
5.3 Reference Validation
To confirm our findings and test the reliability of our reference,
which is based on the common LinkedIn passwords, we repeated
our analysis using RockYou and 000Webhost. The leaks are different
in size; thus the resulting number of passwords tested were different.
While the reference had approx. 1 million passwords that occurred
10 or more times, RockYou only includes 250 000 and 000Webhost
62 000 unique passwords. Across different leaks we observed only
minor differences. The tendencies for correlation, mean error, one-
sided error, and pairwise error metrics, which can be observed in
Table 2, remain the same independent of the tested password leak.
2Package: wCorr (Weighted Correlations), Version 1.9.1, May 2017,
https://cran.r-project.org/package=wCorr, as of September 10, 2018
3Package: pcaPP (Robust PCA by PP), Version 1.9-73, January 2018,
https://cran.r-project.org/package=pcaPP, as of September 10, 2018
For example, for the three leaks the wSpear. metric results vary
only around ±0.04 across all test cases.
Furthermore, we repeated our tests with a LinkedIn set that in-
cluded uncommon passwords (count ≥ 2). Including uncommon
passwords is expected to be more error prone [8, 65]. While the
common variant included approx. 1 million passwords, the uncom-
mon version consisted of 31 million unique passwords. However,
our results show that the tendencies from Table 2 remain the same.
For example, for the uncommon variant the wSpear. metric results
vary around ±0.07 across all test cases.
To summarize, in those additional tests we found only minor
differences in the behavior of the similarity measures across pass-
word leaks. Moreover, including uncommon passwords had a bigger
albeit overall negligible impact on the results.
5.4 Recommendation
We report results for 19 candidates for similarity measure. We con-
sidered 5 correlation-based similarity measures, 6-variants that are
mean absolute/square error-based, as well as, 8 one-sided/pairwise
error metrics and evaluated them on a number of test cases. Those
tests included commonly observed cases like logarithmic transfor-
mation and quantization, but also meters that incorrectly judge
strength simulated via disturbances.
We have seen that measures that are not weighted largely fail
to capture essential aspects of the (highly skewed) distributions of
passwords. Consequently, sensible measures should be weighted.
Furthermore, we observed that measures based not on rank (but
rather on values) are generally too sensitive to monotonic transfor-
mations and quantization to be useful.
In our evaluation the metrics wSpear., wrMAE, wrMSE, wrLAE,
wrLSE, and wPE-5/wPU are weighted and ranked metrics that
performed well and seem suitable as comparison metric. For the
remainder of this work we have selected weighted Spearman cor-
relation. It is not perfect, especially on quantized output, and it
does not differentiate between under- and overestimating strength,
but performed well on most test cases. Furthermore, it is a stan-
dard metric and easy to interpret, relatively good to approximate
from sampled data (cf. Section 5.5), and implementations are easily
available. Also, (unweighted) Spearman correlation has been used
before to evaluate strength meters [13, 65].
5.5 Sampling
Collecting data from online sources is often cumbersome (e. g.,
previous work [16] that evaluated data from (online) password
strength meter went through great effort to collect large amounts
of data). Therefore, we want to determine confidence intervals
for our measures to select the amount of data we need to collect.
Determining accurate bounds is non-trivial, and to the best of
our knowledge, no bounds are known that are applicable to our
problem.
We determine empirical confidence intervals for the weighted
Spearman measure (as it was the most promising one in the previous
section) by repeated sub-sampling from the reference REF-A and
the test cases. We sample subsets of varying sizes, ranging from
100 to 10,000, and computing similarity on those subsets, using the
full data available to determine the strength score of the reference.
Table 3: (Empirical) confidence intervals for REF-A vs. Q4-equi/LOG for different sample sizes and the weighted Spearman
similarity measure. All are determined using 10,000 iterations, and a 5 % confidence level. Given is the width of the confidence
interval, as well as the boundaries (in brackets).
# Samples
Q4-equi
LOG
100
0.146 [ 0.852 , 0.998 ]
0.081 [ 0.919 , 1.000 ]
500
0.069 [ 0.916 , 0.985 ]
0.033 [ 0.966 , 0.999 ]
1000
0.044 [ 0.928 , 0.972 ]
0.024 [ 0.974 , 0.998 ]
5,000
0.022 [ 0.944 , 0.966 ]
0.013 [ 0.983 , 0.996 ]
10,000
0.027 [ 0.940 , 0.966 ]
0.011 [ 0.985 , 0.997 ]
to limit the effectiveness of online guessing attacks such as rate-
limiting. Typically one considers between 100 and 1000 allowed
guesses within 30 days [9, 28, 31, 65, 66].
(2) Strength meters deployed to protect local authentication,
such as hard disk encryption. In this scenario the number of guesses
the adversary can test is only limited by the computational power
of the adversary; In real-world attacks, the number of guesses per
day on a single GPU is in the order of 109 to 1012 guesses [30]; some
even consider up to 1014 guesses to be reasonable [26].
6.1 Online Guessing
For online account PSMs, techniques such as rate-limiting can re-
duce the risk of online guessing attacks. Based on previous work [65,
66], which describes 1000 guesses as a reasonable limit an attacker
can perform in an online guessing attack, and based on the as-
sumption that the attacker is acting rational and guesses the most
likely passwords first, we deduce that the most interesting set of
passwords relevant for this kind of attack is the most likely 10,000
passwords. If each user omits passwords from the “easier half” of
this set, then overall security will greatly be improved.
Sampling strength meter scores for these 10,000 passwords of
all three datasets (RockYou, LinkedIn, 000Webhost) would put an
unnecessary burden on the server infrastructure, and might even
trigger server-side alerts. Given the results on sampling accuracy
in Section 5.5 we try to avoid such implications by restricting our-
selves to 1000 samples from these online services: i. e., out of the
10,000 most common RockYou passwords, we uniformly random
sample 1000 passwords. We repeat this process for LinkedIn and
000Webhost, respectively, to obtain three different online guessing
datasets. For the most likely passwords, we have very accurate
frequency estimates. So for those common passwords, we can use
the sample frequency as a ground truth for their strength in an
online attack.
6.2 Offline Guessing
For offline guessing attacks, there is no limit on the number of
attempts an attacker can perform, depending on the computing
capabilities and the password hashing function deployed for storing
the password. Consequently, the sample frequency in the datasets
does not provide useful information about the strength in an of-
fline attack, as the number of guesses (by far) exceeds the size of
the dataset. Instead, we use the performance of common guess-
ing tools as the reference. More specifically, we use the results
of the Password Guessability Service (PGS) [62], which allows re-
searchers to send lists of passwords (in plaintext), and the service
evaluates when these passwords will be guessed by common con-
figurations of widely used password guessing tools. Work by Ur
et al. [62] found that the attribute min_auto is a good measure
Figure 1: Histogram example for the monotonic transfor-
mation error LOG using weighted Spearman correlation,
10,000 samples, and 10,000 iterations.
We repeat this process 10,000 times and determine the interval
that contains 95 % of all similarity values (with 2.5 % larger and
2.5 % smaller). We report both the width of the interval and the
actual interval. We perform this process for two different datasets,
namely for Q4-equi and for LOG. While this does not give a formal
guarantee that the actual value can be found in this interval, it gives
us reasonable confidence and determines rough boundaries. Note
that this process only takes into account (random) errors caused by
sampling; it does not take into account any systematic errors that
may be introduced, e. g., by the smaller sample size. The summary
of results is shown in Table 3.
An example of a histogram of the correlation values is given in
Figure 1, which was determined for weighted Spearman correlation
with 10,000 samples and 10,000 iterations. The histogram has a me-
dian of 0.990, min./max. of 0.980/1.000, and 2.5 %/97.5 %-percentiles
of 0.985/0.997, resulting in a width of the 95 % confidence interval of
0.011. We see that, as expected, the width of the confidence interval
decreases with increasing sample size. For weighted Spearman, we
find widths of 0.027 and 0.011, respectively, and we will assume
differences greater than 0.05 to be significant.
6 EVALUATION
Guided by practical requirements, we distinguish between two
different application scenarios.
(1) PSMs deployed to protect online accounts, i. e., the most
prevalent online logins for social networks, email providers, etc. For
online accounts, the operator can and should implement measures
0.9850.9900.995Weighted Spearman Correlation02004006008001000Frequencyof resistance to guessing attacks, even with humans password ex-
perts involved. We use this recommended configuration without a
password composition policy (1class1) as the ground truth for the