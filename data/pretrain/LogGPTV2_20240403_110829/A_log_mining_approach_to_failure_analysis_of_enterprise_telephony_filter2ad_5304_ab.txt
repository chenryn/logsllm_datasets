### Message Clustering

The payload of a message, and thus the effective content, typically originates from debug or trace messages written by system developers. These messages often consist of a text string with embedded parameters, such as IP addresses, memory locations, or other alphanumeric strings.

Each invocation of a given message in the code may produce a different log entry depending on the parameter values. To reduce the number of unique messages in the dataset, one approach is to de-parameterize the messages and cluster similar ones together. In our initial analysis, we used a simple and coarse-grained de-parameterization technique, replacing specific elements with generic tokens:
- IP/Ethernet addresses
- Memory locations
- All hexadecimal digits

After de-parameterization, we clustered the messages using the Levenshtein distance. This method reduced the number of unique messages to approximately 13,000, which is about 0.5% of the original set. This reduction makes the data more manageable and facilitates the detection of patterns in the logs.

There is a trade-off between the granularity of de-parameterization/clustering and the amount of information retained. While a general-purpose clustering technique can lead to a higher loss of information, it significantly reduces the size of the unique message set, making it easier to analyze. For subsequent rounds of analysis, custom clustering and de-parameterization techniques, informed by domain-specific knowledge, can be applied to retain more information for critical subsets of messages.

### Message Normalization

Storing textual messages in memory during analysis is resource-intensive and can slow down the process. To address this, we normalized the message set by assigning unique numerical identifiers (message codes) to each of the 13,000 unique messages. This normalization was applied to each of the 714 log files, converting each log line into a timestamp and a numeric code.

### Analysis for Logs with Known Failures

Our study focused on two types of datasets. The first dataset consisted of 714 logs, each retrieved from a system after a crash failure, containing three hours of data before the failure and one hour after. Each log had a distinct failure marker, and different types of failures were identified by their corresponding message codes. Our analysis aimed to find common signatures across failures of the same type and determine if failures could be predicted through the build-up of specific messages.

#### Frequent Itemset Mining

We grouped the logs based on their failure codes and mined for frequent item sets. The high-support frequent item sets were found to be associated with the failure and recovery processes. This approach was useful for categorizing failures but did not help in predicting them.

#### Individual Message Frequency Analysis

To identify common patterns, we analyzed the message frequencies of individual codes in the log files. We computed message frequencies for 1-minute intervals and plotted these for visualization. However, with a large number of codes, it was challenging to visually determine patterns. Therefore, we implemented automated filters to highlight "interesting" codes:
- **Slope Filter**: Captured codes that steadily increased towards the failure.
- **Window-Max Filter**: Plotted codes whose maximum frequency in the pre-failure window (one hour) was a threshold-fold higher than the maximum frequency before the window.
- **Window-Rate Filter**: Plotted codes whose average frequency in the pre-failure window was a threshold-fold higher than the average frequency before the window.

The slope filter captured codes 4514 (related to process errors) and 4597 (generated from the maintenance system), which were prevalent before failures. The window-max and window-rate filters found more interesting codes, with similar results due to the nature of the codes' appearance.

### Overall Message Frequency Analysis

In our earlier work, we analyzed the total message frequency, showing that processes become more active as a failure approaches. Figure 2 illustrates the overall message frequency over time, indicating a steady increase before the failure at the 180-minute mark. Visualization of these graphs helped in identifying common failures across log files.

### Filters for Identifying Interesting Codes

- **Slope Filter**: Reduced the number of codes to those showing a steady build-up before failure.
- **Window-Max and Window-Rate Filters**: Detected codes with increased activity in the pre-failure window, providing more detailed insights into potential failure indicators.

These filters, particularly the slope filter, highlighted codes 4514 and 4597 as key indicators of upcoming failures, observed consistently across multiple systems. This suggests that for failure prediction, monitoring these specific codes can be highly effective.