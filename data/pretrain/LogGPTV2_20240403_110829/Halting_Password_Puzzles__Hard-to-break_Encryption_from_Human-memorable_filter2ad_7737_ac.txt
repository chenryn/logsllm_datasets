{zl ← H(zl, yjl , l)}
5.
6.
7.
l=1,...,p
z ← H(z1, ..., zl)
8.
9. v ← (H(y1, z), r)
10. k ← H(z, r)
pHKDFH .Extract(w, v)
Inputs: password w, veriﬁcation string v.
Output: derived key k, or may never halt.
0. parse v as (h, r)
1. {zl ← H(w, r, l)}l=1,...,p
2. z ← H(z1, ..., zl)
3. FOR i := 1, ...,∞
4.
5.
yi ← z
REPEAT q times
6.
7.
{jl ← 1 + (zl mod i)}l=1,...,p
{zl ← H(zl, yjl , l)}
l=1,...,p
z ← H0(z1, ..., zl)
IF H(y1, z) = h THEN BREAK
8.
9.
10. k ← H(z, r)
// init each zl
independently
// init z from all
the zl
//
// store z in array
element yi
//
// map each zl to
some jl ∈
{1, ..., i}
// update each zl
independently
// update z
//
//
//
// p-way
parallelizable
//
//
//
// p-way
parallelizable
across whole
loop
// p-way
parallelizable
// p-way
parallelizable
//
//
//
The constant p determines the maximum parallelizabil-
ity of the scheme: it can then vary from 1-fold to p-fold
without signiﬁcant overhead. Total computational cost
is Θ(pq t) hash evaluations. Total memory requirement
is Θ(p + t) hash values, including a constant `p bits of
memory overhead compared to the basic construction.
Complexity-wise, the parameter p acts as a multiplier on
the space/time proportionality ratio q, so that all secu-
rity properties are retained with pq instead of q. It is thus
easy to enable parallelism by increasing p and decreasing
q proportionately.
The relative penalty exerted on the adversary will be
proportional to the number N of CPUs that the user can
bring to bear, under the constraint that N ≤ p (and where
ideally, N | p).
Partitioned Memory. The sequential scheme of Sec-
tion 2.2 can also be made p-wise parallelizable for p =
2l, by dropping l bits from r when Prepare-ing the pub-
lic string v = (h, r). To re-derive the key, the user tries
all completions of r by running p instances of Extract at
once until one halts. With p machines, the elapsed time
is unchanged; however the total work is Θ(pq t). The in-
convenient is that this requires Θ(pt) memory instead of
Θ(p + t) for the method of Section 2.4, but the advan-
tage is that processing and memory can be partitioned
over p independent machines. Applying the same trick
to the Section 2.4 scheme, gives us a hybrid with two
parallelism options.
Practical Parameters
2.5
HKDF parameter selection is non-critical and much eas-
ier than with regular KDFs, since we are not trying to
make decisions for the user, or prevent obsolescence by
betting pro or con Moore’s law. The only choices we
need to make concern the coefﬁcients p and q. The rule
of thumb is: maximize pq in view of today’s machines,
and then ﬁx p to cover all foreseeable needs for paral-
lelism.
For the sake of illustration, let ` = 256, and suppose
that that the user’s key derivation hardware can compute
n = 225 hashes per second (e.g., with 23 cores each ca-
pable of 222 hashes per second), and suppose the device
has m = 221 · 256 bits = 64 MiB of shared memory.
Memory capacity will be reached after T = mpq/`n
seconds of elapsed computation time. Thus, if we aim
for pq = 220, the maximum selectable processing time
on the device will be 216 seconds (close to 1 day), in in-
crements of 2−5 second. We can take p = 210 · 32 · 52 =
230 400 and hence q = 4 to get pq ≈ 220. Last, we as-
certain that, per all these choices, the available memory
is still much larger than the `p ≈ 7 MiB of overhead that
are the price to pay for the parallelization option.
Suppose then that the user settles for t = 25 iterations
(to take 1 second on the current device), and chooses a
weak password with only 40 bits of entropy (from an im-
plicit dictionary of size d = 240). In these conditions, an
adversary will need `td = 253 bits = 1024 TiB of mem-
ory in order to conduct a persistent attack. On a faster
and/or more highly parallelized device, the user would
choose a correspondingly larger value of t, further in-
creasing the load on the adversary.
126
16th USENIX Security Symposium
USENIX Association
Flexible Parallelism.
It is advisable to set p as a large
product of small factors, to facilitate the even distribu-
tion of workload among any number N of CPUs such
that N divides p; this is easy to achieve in practice since
the values of pq tend to be quite large, on the order of
pq ≥ 1 000 000. A nice consequence is that the same
HKDF can be dimensioned to accommodate any reason-
ably foreseen amount of user-side parallelism (hence the
choice p = 210·32·52 = 230 400), and still be usable on
today’s sequential computers (with at least `p ≈ 7 MiB
of memory in this example).
3 The Security Gap
We show that any adversary lacking enormous amounts
of memory will incur a ∼ 4× larger cost for not know-
ing the iteration count. Since the penalty only strikes on
wrong guesses, the user who knows the correct password
will be immune to it. We say that HKDFs widen the “se-
curity gap”.
3.1 Ofﬂine Dictionary Attack Model
We consider the simplest and most general ofﬂine attack
by an adversary A against a challenger C. We capture
the password “guessability” by supposing that it is drawn
uniformly at random from a known dictionary D, and
deﬁne its entropy as the value log2(#D). The game is
as follows:
Challenge. The challenger C picks w ∈$ D
and r ∈$ {0, 1}` at random, chooses
t ∈ N, and computes (v, k) ←
HKDF.Prepare(w, r, t).
It gives the
string v to A.
Attack. The
as
adversary A outputs
as
sequen-
many keys
the
tially:
game as soon as some ki matches
k = HKDF.Extract(w, v).
k1, k2, ....
it pleases,
It wins
We assume that A can only retain state for a dwindling
fraction of D, of size o(1) in t.
Password (Min-)Entropy.
In reality, passwords are
not sampled uniformly from a ﬁxed D, but rather non-
uniformly from a set with no clear boundaries. The
worst-case unpredictability of a password chosen in this
manner is the minimum entropy, or min-entropy, deﬁned
as − log2(maxw Pr(w)). The uniform password model
w ∈$ D conveniently and accurately reﬂects the difﬁ-
culty of guessing from C’s true password distribution,
provided that log2(#D) matches the min-entropy of the
latter.
3.2 Finding the Optimal Attack Strategy
By Lemma 5 we know that A cannot do better than
outputting random keys until it “tries out” the correct
password w for t iterations (using the Extract function).
Since A lacks the memory to maintain concurrent in-
stances of Extract for any substantial subset of D, the
only option is to “dovetail” the search, i.e.:
– try all the words of D one by one (or few by few)
for a bounded stretch of time;
– retry the same for longer and longer time stretches,
until t is eventually exceeded.
We can neglect the o(1) fraction of D on which A could
run a persistent attack. Also, for uniform w ∈$ D and
unknown t it is easy to show that it is optimal to spend
the same amount of effort on each candidate password.
We deduce that the optimal algorithm for any forgetful
attacker A is:
Optimal-MemoryBound-AD(v)
Input: veriﬁcation string v.
Output: password ˆw and key ˆk.
1. FOR ˆt := t1, t2, ...
FOR ˆw ∈ D
2.
3.
4.
5.
// t1  0, and let t0 = (1 + δ)t. By Lemma 6,
we know that π(t) = π(t0). Now, consider the
mixed schedule [(t0
i =
(1 + δ)ti for ti everywhere, while keeping all proba-
bilities the same. Denote by π0 the penalty function
under that new schedule. By deﬁnition, we have the
identity π0(t0) = 1+δ
1+δ π(t) = π(t), and by transitiv-
ity we obtain that π(t) = π0(t). We conclude that
π(t) = π0(t) for any distribution of t over the interval
{(1 + δ)tlo, ..., (1 + δ)−1 thi}, for any δ > 0.
2, ...)] obtained by subtituting t0
Since the strategy is optimal, it follows that multiply-
ing all the values in all the schedules it comprises by
any constant (1 + δ) must preserve π(t); this also works
backward for (1 + δ)−1, and thus this is true in the limit
for any multiplier in R+.
In other words an optimal
strategy for A is invariant to (multiplicative) scaling. A
straightforward argument then shows that this must be re-
ciprocated by the optimal response employed by C. Ap-
proximating t as a real in R+, we deduce that t must obey
d xPr(t < x) ∝ x−β
a Zipf power law, whose density is: d
for some β ∈ R.
the
scale invariance implies that all the individual sched-
ules (t1, t2, ...) in the mixture must satisfy (ti+1/ti) =
(tj+1/tj) for all i, j, otherwise the multiplication by
a constant would result in a different mixture. We
have not yet ruled out the possibility of (sub-)mixtures
1, t0
[(t1, t2, ...)], [(t0
2, ...)], ... with unequal progressions
[(ti+1/ti)] 6= [(t0
i+1/t0
i)], which is why so far we say
that [(ti+1/ti)] converges to a distribution [α] instead of
a value α∗.
For the remaining claim, we ﬁrst note that
Randomized Starting Point. Lemmas 6 and 7 show
that the optimal attack schedule for A is a randomized
sequence (t1, t2, ...) where ti = t1 αi−1 for some ran-
dom starting point t1 ≈ tlo and a progression coefﬁcient
128
16th USENIX Security Symposium
USENIX Association
α ∈$ [α]. For large enough t (cid:29) t1, the penalty becomes: