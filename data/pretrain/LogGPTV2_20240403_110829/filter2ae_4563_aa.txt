# 论文
## 论文信息
[Explaining and Harnessing Adversarial
Examples](https://arxiv.org/abs/1412.6572).
## 论文简介
早期对对抗样本产生的原因的猜测集中于神经网络的非线性性和过拟合, 但是这篇论文证明神经网络的线性性质是造成神经网络具有对抗样本的主要原因. 同时,
该篇论文提出了一个能供更简单与更快速的生成对抗样本的方法.
## 论文主要内容
  * 否定了对抗样本是因为非线性和过拟合导致的, 认为对抗样本是因为神经网络在高维空间的线性导致的, 并提出了大量的实验加以说明. 对抗样本可以被解释成高维空间点乘的一个属性, 他们是模型太过于线性的结果.
  * 模型的线性让其更容易被训练, 而其非线性让其容易抵御对抗扰动的攻击, 即容易优化的模型也容易被扰动.
  * 提出了一种特别快的生成对抗样本的方法`FGSM`:
  * `FGSM`的实质是输入图片在模型的权重方向上增加了一些扰动(方向一样, 点乘最大). 这样可以让图片在较小的扰动下出现较大的改变, 从而得到对抗样本.
  * 不同模型之间的对抗性示例的泛化可以解释为, 对抗性扰动与模型的权重向量高度一致, 不同模型在训练执行相同任务时学习相似的函数.
  * 提出了一种基于`FGSM`的正则化方法, 对抗训练可以用来正则化, 甚至效果比`dropout`还要好:
  * 相比于模型融合, 单个模型的对抗防御能力更好一些, 集成策略不能够抵御对抗样本.
  * 线性模型缺乏抵抗对抗性扰动的能力, 只有具有隐藏层的结构(在普遍近似定理适用的情况下)才应该被训练来抵抗对抗性扰动.
  * `RBF`网络可以抵御对抗样本.
  * 对抗样本的分布特征, 即对抗样本往往存在于模型决策边界的附近, 在线性搜索范围内, 模型的正常分类区域和被对抗样本攻击的区域都仅占分布范围的较小一部分, 剩余部分为垃圾类别(rubbish class).
  * 垃圾类别样本是普遍存在的且很容易生成, 浅的线性模型不能抵御垃圾类别样本, `RBF`网络可以抵御垃圾类别样本.
# FGSM
## 原理
`fast gradient sign method`是一种基于梯度生成对抗样本的算法, 属于对抗攻击中的无目标攻击,
即不要求对抗样本经过`model`预测指定的类别, 只要与原样本预测的不一样即可. 它旨在通过利用模型学习的方式和渐变来攻击神经网络,
攻击调整输入数据以基于相同的反向传播梯度来最大化损失, 而不是通过基于反向传播的梯度调整权重来最小化损失. 简而言之, 攻击是利用损失函数的梯度,
然后调整输入数据以最大化损失.
例如, 下图中, 大熊猫照片加入一定的扰动(噪音点), 输入`model`之后就被判断为长臂猿.
## 公式
`FGSM`公式如下图所示:
在公式中, `x`是原始样本; `θ`是模型的权重参数, 即`w`; `y`是`x`的真实类别. 输入原始样本, 权重参数以及真实类别,
通过`J`损失函数求得神经网络的损失值, `∇x`表示对`x`求偏导, 即损失函数`J`对`x`样本求偏导. `sign`是符号函数,
即`sign(-1)`, `sign(-99.9)`等都等于`-1`; `sign(1)`, `sign(99.9)`等都等于`1`.
`epsilon`的值通常是人为设定, 可以视作学习率, 一旦扰动值超出阈值, 该对抗样本会被人眼识别.
## 算法
### 搭建模型
    # 库文件引入
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torch.nn.functional as F
    import numpy as np
    import matplotlib.pyplot as plt
    from __future__ import print_function
    from torchvision import datasets, transforms
    # 设置多个 epsilons 便于后续可视化展示其影响
    epsilons = [0, .05, .1, .15, .2, .25, .3, .35, .4, .45, .5]
    # 加载预训练模型
    # 预训练模型下载地址：https://drive.google.com/drive/folders/1fn83DF14tWmit0RTKWRhPq5uVXt73e0h
    pretrained_model = 'data/lenet_mnist_model.pth'
    # 是否使用 cuda
    use_cuda = False
    # 搭建被攻击的模型
    # 定义 LeNet 模型
    class LeNet(nn.Module):
        def __init__(self):
            super(LeNet, self).__init__()
            self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
            self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
            self.conv2_drop = nn.Dropout2d()
            self.fc1 = nn.Linear(320, 50)
            self.fc2 = nn.Linear(50, 10)
        def forward(self, x):
            x = F.relu(F.max_pool2d(self.conv1(x), 2))
            x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
            x = x.view(-1, 320)
            x = F.relu(self.fc1(x))
            x = F.dropout(x, training=self.training)
            x = self.fc2(x)
            return F.log_softmax(x, dim=1)
    # 下载并加载数据集
    loader = torch.utils.data.DataLoader(
        datasets.MNIST('data/', train=False, download=True, transform=transforms.Compose([
            transforms.ToTensor(),
        ])),
        batch_size=1,
        shuffle=True
    )
    # 配置 GPU
    cuda_available = torch.cuda.is_available()
    device = torch.device('cude' if (use_cuda and cuda_available) else 'cpu')
    print('CUDA is available: ', cuda_available)
    # 初始化网络模型
    model = LeNet().to(device)
    # 加载预训练模型
    model.load_state_dict(torch.load(pretrained_model, map_location='cpu'))
    # 设置为验证模式
    model.eval()
### FGSM 攻击模块
    # FGSM Attack Module
    def fgsm_attack_module(image, epsilon, data_grad):
        # 使用 sign 符号函数，将对 x 求了偏导的梯度进行符号化
        sign_data_grad = data_grad.sign()
        # 通过 epsilon 生成对抗样本
        adversarial_image = image + epsilon * sign_data_grad
        # 做一个剪裁的工作，将 torch.clamp 内部大于 1 的数值变为 1 ，小于 0 的数值等于 0，防止 image 越界
        adversarial_image = torch.clamp(adversarial_image, 0, 1)
        # 返回对抗样本
        return adversarial_image
### 测试模块
    def test( model, device, test_loader, epsilon ):
        # 准确度计数器
        correct = 0
        # 对抗样本
        adv_examples = []
        # 循环遍历测试集中的所有示例
        for data, target in test_loader:
            # 把数据和标签发送到设备
            data, target = data.to(device), target.to(device)
            # 设置张量的 requires_grad 属性，这对于攻击很关键
            data.requires_grad = True
            # 通过模型前向传递数据
            output = model(data)
            init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability
            # 如果初始预测是错误的，不打断攻击，继续
            if init_pred.item() != target.item():
                continue
            # 计算损失
            loss = F.nll_loss(output, target)
            # 将所有现有的渐变归零
            model.zero_grad()
            # 计算后向传递模型的梯度
            loss.backward()
            # 收集 datagrad
            data_grad = data.grad.data
            # 唤醒 FGSM 进行攻击
            perturbed_data = fgsm_attack_module(data, epsilon, data_grad)
            # 重新分类受扰乱的图像
            output = model(perturbed_data)
            # 检查是否成功
            final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability