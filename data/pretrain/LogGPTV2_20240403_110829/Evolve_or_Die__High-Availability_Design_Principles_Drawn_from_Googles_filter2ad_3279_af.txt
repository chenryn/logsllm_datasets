Our lessons motivate several avenues for research in high-
availability design, a topic that has received less attention
than high-performance design. Indeed, each lesson embod-
ies a large research area, where our deployed solution repre-
sents one point in a large design space that future research
can explore. Examples of future directions include: optimal
ways to partition topologies and control domains to contain
the blast radius of a failure or ways to design topologies
with known failure impact properties; dynamic methods to
quickly assess when to fall back, and which trafﬁc to divert
to achieve smooth, almost transparent fallback; methods to
70
statically reason about the consistency of control plane state,
and to track state provenance to ensure consistency; scalable
in-band measurement methods that permit fast and accurate
failure localization while themselves being robust to failures
and attacks; and techniques to detect fail-open robustly, and
to correctly reconcile control and data plane state in a failed-
open system upon recovery of the control plane.
A large, somewhat underexplored area in high-availability
design is the management plane. Future research here can
explore how to specify “intent” (i.e., what the network
should look like), how to conﬁgure and provision the
network based on intent, how to collect a snapshot of the
network’s “ground truth” [12] (i.e., what the network does
look like), and how to reconcile intent and the ground truth
because,
in practice, even with automated conﬁguration
and provisioning based on intent,
there is likely to be
divergence between the two. Given the evolution of large
content providers, another area of research is automated and
accurate risk assessment, and mechanisms to permit safe,
yet frequent, network evolution using upgrade-in-place.
Finally, we identify a number of over-arching challenges.
How can we deﬁne and measure SLOs for a network in a
way that services or applications can use for their design?
When do we say that a network is really unavailable (when
it drops a few packets, when its throughput falls below a
certain threshold)? What techniques do we use to quantify
improvements in availability?
8. RELATED WORK
Generic reasons for failures of engineered systems [7,
32] include heterogeneity and the impact of interactions
and coupling between components of the system. Our
work focuses on a modern large-scale content provider; we
identify speciﬁc classes of reasons why our networks fail,
many of which can be attributed to velocity of evolution in
our networks.
Network failures and their impact have also been stud-
ied in distributed systems. From impossibility results [1], to
techniques for designing failure-tolerant distributed systems
[9], to experience reports on individual failures in practical,
large distributed systems [28, 36], these studies shed light
on real failures and their consequences for distributed sys-
tems. Bailis and Kingsbury [30] provide a nice discussion of
publicly disclosed failures in deployed distributed systems
that were likely caused by network partitions. In contrast to
this body of work, our work focuses on failures in the net-
work control, data, and management planes. While some of
our failures are qualitatively similar to failures seen in dis-
tributed systems (failure cascades and split-brain failures),
others such as failures of the control plane network, failures
in management plane operations, etc., do not arise or have
less impact in distributed systems. Similarly, many of our
lessons have their analogs in distributed systems, e.g., fall-
back strategies and fast fault isolation), but many do not, e.g.,
fail-open, dynamically verifying control plane updates, and
management plane automation.
Finally, the networking literature has, over more than a
decade, explored various ways to assess and quantify fail-
ures in networks. A line of early work explored link failure
characteristics in medium to large-scale ISPs by examining
the dynamics of IGPs [33, 26, 39] and EGPs [22]. Not all
link failures result in loss of network availability, and our
work explores a much broader class of root-causes for avail-
ability failures, ranging from device resource limitations to
control plane bugs and management plane errors. More re-
cent work has explored link, device, and component failures
in enterprises [37], academic networks [34], and data centers
[13], using other sources of information, including syslog
errors, trouble tickets and customer complaints. Our data
source, the post-mortem reports, are qualitatively different
from these sources: our reports are carefully curated and in-
clude root-cause assessments that are typically conﬁrmed by
careful reproduction in the lab or in limited ﬁeld settings.
Thus, we are able to broadly, and more accurately assess root
cause across different types of networks, with different con-
trol plane designs and management plane processes. Other
work has explored the role of misconﬁguration in network
failures [29, 31], and methods to reduce misconﬁguration er-
rors with shadow network conﬁgurations [2]; control plane
misconﬁguration is but one of the root causes we study.
Finally, more recent work has explored the impact of man-
agement plane operations on network health [12]. This work
identiﬁes management plane operations by changes to de-
vice conﬁgurations, or by changes to network topology, and
network health by the number of device alerts across the
network, then applies statistical causality tests to determine
which management plane operations can impact health. Our
work differs in many ways. First, in addition to our use of
human curated post-mortems, the MOps we discuss in this
paper are fully documented operations that are reviewed and
require approval so we need not infer whether a management
operation was in effect. Second, most of our failures had an
availability impact, while it is unclear to what extent mea-
sures of network health reﬂect availability. Finally, our work
attempts to unify root-cause categories across different net-
work types, and across the data and control planes as well,
not just the management plane.
9. CONCLUSIONS
By analyzing post-mortem reports at Google, we show
that failures occur in all of our networks, and across all
planes. Many failures occur when the network is touched,
but management operations on networks are fundamental
at Google given its evolution velocity. These failures have
prompted us to adopt several high-availability design prin-
ciples and associated mechanisms ranging from preserving
the data plane upon failure, containing the failure radius,
and designing fallbacks for systemic failure, to automated
risk assessment and management plane automation. Our
experience suggests that future networks must account for
continuous evolution and upgrade as a key part of their
availability architecture and design.
Acknowledgements. We
the
excellent feedback we have received from the reviewers
acknowledge
gratefully
and from our shepherd Vyas Sekar. Discussions with and
feedback from Paulie Germano, Hossein Lotﬁ, Subhasree
Mandal, Joon Ong, Arjun Singh, and David Wetherall
also signiﬁcantly improved the paper. Finally, this work
would not have been possible without the painstaking work
of Google’s network operations, SRE, and development
teams that design, manage and run our global network and
carefully document and root-cause each signiﬁcant failure.
Bibliography
[1] Daniel Abadi. “Consistency Tradeoffs in Modern Dis-
tributed Database Design: CAP is Only Part of the
Story”. In: IEEE Computer (2012).
[2] Richard Alimi, Ye Wang, and Yang Richard Yang.
“Shadow conﬁguration as a network management
primitive”. In: Proc. ACM SIGCOMM. 2008.
[3] C. Ashton. What is the Real Cost of Network Down-
time? http : / / www. lightreading . com / data - center /
data- center- infrastructure/whats- the- real- cost- of-
network-downtime/a/d-id/710595. 2014.
[4] B. Schneier. Security in the Cloud. https : / / www .
schneier.com/blog/archives/2006/02/security_in_
the.html. 2006.
[5] Betsy Beyer and Niall Richard Murphy. “Site Relia-
bility Engineering: How Google Runs its Production
Clusters”. In: O’Reilly, 2016. Chap. 1.
[6] Matt Calder, Xun Fan, Zi Hu, Ethan Katz-Bassett,
John Heidemann, and Ramesh Govindan. “Mapping
the Expansion of Google’s Serving Infrastructure”. In:
Proc. of the ACM Internet Measurement Conference
(IMC ’13). 2013.
[7] Carlson, J. M. and Doyle, John. “Highly Optimized
Tolerance: Robustness and Design in Complex Sys-
tems”. In: Phys. Rev. Lett. 84 (11 2000), pp. 2529–
2532.
[8] Cisco Visual Networking Index: The Zettabyte Era
– Trends and Analysis. http : / / www. cisco . com / c /
en/us/solutions/collateral/service- provider/visual-
networking-index-vni/VNI_Hyperconnectivity_WP.
html. 2014.
Jeff Dean. Designs, Lessons and Advice from Building
Large Distributed Systems. Keynote at LADIS 2009.
[10] E. Dubrova. “Fault-Tolerant Design”. In: Springer,
[9]
2013. Chap. 2.
[11] Tobias Flach et al. “Reducing Web Latency: the Virtue
of Gentle Aggression”. In: Proc. ACM SIGCOMM.
2013.
[12] Aaron Gember-Jacobson, Wenfei Wu, Xiujun Li,
Aditya Akella, and Ratul Mahajan. “Management
Plane Analytics”. In: Proceedings of ACM IMC. IMC
’15. Tokyo, Japan: ACM, 2015, pp. 395–408. ISBN:
978-1-4503-3848-6.
71
[13] P. Gill, N. Jain, and N. Nagappan. “Understanding
Network Failures in Data Centers: Measurement,
Analysis, and Implications”. In: Proc. ACM SIG-
COMM. 2011.
[14] Chuanxiong Guo et al. “Pingmesh: A Large-Scale
System for Data Center Network Latency Mea-
surement and Analysis”. In: SIGCOMM Comput.
Commun. Rev. 45.5 (Aug. 2015), pp. 139–152. ISSN:
0146-4833.
[15] R. Hinden. Virtual Router Redundancy Protocol. In-
[16]
ternet Engineering Task Force, RFC 3768. 2004.
Internet hiccups today? You’re not alone. Here’s why.
http://www.zdnet.com/article/internet-hiccups-today-
youre-not-alone-heres-why/.
[17] Y. Israelevtsky and A. Tseitlin. The Netﬂix Simian
Army. http://techblog.netﬂix.com/2011/07/netﬂix-
simian-army.html. 2011.
[18] Sushant Jain et al. “B4: Experience with a Globally-
deployed Software Deﬁned WAN”. In: Proceedings
the ACM SIGCOMM 2013. SIGCOMM ’13.
of
Hong Kong, China: ACM, 2013, pp. 3–14.
ISBN:
978-1-4503-2056-6.
Juniper Networks MX 2020. http://www.juniper.net/
elqNow/elqRedir.htm?ref=http://www.juniper.net/
assets/us/en/local/pdf/datasheets/1000417-en.pdf.
[19]
[20] K. Krishnan. “Weathering the Unexpected”. In: ACM
Queue (2012).
[21] Alok Kumar et al. “BwE: Flexible, Hierarchical
Bandwidth Allocation for WAN Distributed Comput-
ing”. In: Proceedings of the 2015 ACM Conference
on Special Interest Group on Data Communication.
SIGCOMM ’15. London, United Kingdom: ACM,
2015, pp. 1–14. ISBN: 978-1-4503-3542-3.
[22] Craig Labovitz, Abha Ahuja, and Farnam Jahanian.
“Experimental Study of Internet Stability and Wide-
Area Network Failures”. In: Proc. International Sym-
posium on Fault-Tolerant Computing. 1999.
[23] G. Linden. Make Data Useful. http : / / sites . google .
com/site/glinden/Home/StanfordDataMining.2006-
11-28.ppt. 2006.
[24] M. Canini and D. Venzano and P. Perešíni and
D. Kosti´c and J. Rexford. “A NICE Way to Test
OpenFlow Applications”. In: Presented as part of
the 9th USENIX Symposium on Networked Sys-
tems Design and Implementation (NSDI 12). San
Jose, CA: USENIX, 2012, pp. 127–140.
ISBN:
978-931971-92-8.
[25] M. Kuzniar and P. Peresini and M. Canini and D.
Venzano and D. Kostic. “A SOFT Way for Openﬂow
Switch Interoperability Testing”.
In: Proceedings
the 8th International Conference on Emerging
of
Networking Experiments and Technologies. CoNEXT
’12. Nice, France: ACM, 2012, pp. 265–276. ISBN:
978-1-4503-1775-7.
72
[26] A. Markopoulou, G. Iannaccone, S. Bhattacharyya,
C.-N. Chuah, Y. Ganjali, and C. Diot. “Character-
ization of Failures in an Operational IP Backbone
Network”. In: IEEE/ACM Transactions on Network-
ing (2008).
I. Minei and J. Lucek. MPLS-Enabled Applications:
Emerging Developments and New Technologies. 3rd.
Wiley Inc., 2015.
[27]
[28] Andrew Montalenti. Kafkapocalypse: A Post-Mortem
on our Service Outage. Parse.ly Tech Blog post. 2015.
[29] N. Feamster and H. Balakrishnan. “Detecting BGP
Conﬁguration Faults with Static Analysis”. In: Pro-
ceedings of the 2nd Symposium on Networked Sys-
tems Design and Implementation. USENIX Associa-
tion. 2005, pp. 43–56.
[30] P. Bailis and K. Kingsbury. “An Informal Survey of
Real-World Communications Failures”. In: Commu-
nications of the ACM (2014).
[31] R. Mahajan and D. Wetherall and T. Anderson. “Un-
derstanding BGP Misconﬁguration”. In: Proceedings
of the 2002 Conference on Applications, Technolo-
gies, Architectures, and Protocols for Computer Com-
munications. SIGCOMM ’02. Pittsburgh, Pennsylva-
nia, USA: ACM, 2002, pp. 3–16. ISBN: 1-58113-570-
X.
John Rushby. “Critical System Properties: Survey and
Taxonomy”. In: Reliability Engineering and System
Safety 43.2 (1994), pp. 189–219.
[32]
[33] A. Shaikh, C. Isett, A. Greenberg, M. Roughan, and
J. Gottlieb. “A Case Study of OSPF Behavior in a
Large Enterprise Network”. In: Proc. ACM Internet
Measurement Workshop. 2002.
[34] A. Shaikh, C. Isett, A. Greenberg, M. Roughan, and J.
Gottlieb. “California Fault Lines: Understanding the
Causes and Impact of Network Failures”. In: Proc.
ACM SIGCOMM. 2010.
[35] Arjun Singh et al. “Jupiter Rising: A Decade of Clos
Topologies and Centralized Control in Google’s Dat-
acenter Network”. In: SIGCOMM Comput. Commun.
Rev. 45.5 (Aug. 2015), pp. 183–197. ISSN: 0146-4833.
[36] Summary of the Amazon EC2 and Amazon RDS Ser-
vice Disruption in the US East Region. http : / / aws .
amazon . com / message / 65648/. Amazon Web Ser-
vices. 2011.
[37] D. Turner, K. Levchenko, J. C. Mogul, S. Savage, and
A. C. Snoeren. On Failure in Managed Enterprise
Networks. Tech. rep. HPL-2012-101. HP Labs, 2012.
[38] Amin Vahdat et al. “Scalability and Accuracy in a
Large-scale Network Emulator”. In: SIGOPS Oper.
Syst. Rev. 36.SI (Dec. 2002), pp. 271–284.
ISSN:
0163-5980.
[39] D. Watson, F. Jahanian, and C. Labovitz. “Experi-
ences With Monitoring OSPF on a Regional Service
Provider Network”. In: Proc. IEEE ICDCS. 2003.