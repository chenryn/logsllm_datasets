1237
2.34M
26.6K
1.65M
5375
1.54M
610
1.35M
672
914K
803K
710
Users
OD
ID
22.5K 24.7K
8809 23.3K
27.6K 15.4K
507 7510
772 10.8K
24.8K 16.1K
5209
106
576 12.5K
653 8441
363 1.64K
ID
Toots
Instances
OD
OD
ID
71.4M 1.94M 1352 1241
37.4M 2.57M 1273 1287
34.9M 1.4M 1162 1106
435K 366K
524
420
711
2.37M 1.52M
865
1442 1083
30.9M 525K
39
1198
7.35M 337
850
735
4.18M 1.98M
631
981
2.6M 853K
2.68M 2.1M
561
862
Run by
Individual
Dwango
Pixiv
Individual
Individuals (CF)
Individual (CF)
Unknown
Individual (CF)
Bokaro bowl (A)
Individual
AS (Country)
Cloudflare (US)
Amazon (JP)
Amazon (US)
Sakura (JP)
Amazon (US)
Online SAS (FR)
Cloudflare (US)
Amazon (JP)
Sakura (JP)
Google (US)
Table 2: Top 10 instances as per number of toots from the home timeline. (OD: Out Degree, ID: In Degree, CF: Crowd-funded,
A: maintained by selling Bokaro Bowl album).
Impact of Removing ASes. As discussed earlier, many instances
are co-located in a small number of hosting ASes. We now inspect
the impact of removing entire ASes, and thus all instances hosted
within. Naturally, this is a far rarer occurrence than instance fail-
ures, yet they do occur (see Section 4.4). We do not present this
as a regular situation, but one that represents the most damaging
theoretical impact. For context, AS-wide collapse might be caused
by catastrophic failures within the AS itself [20, 29] or via their
network interconnections [17].
Figure 13(b) presents the LCC and number of components for GF ,
while iteratively removing the ASes, ranked by both the number of
instances (blue) and number of users (red). At first, we see that 92%
of all instances are within a single LCC. This LCC covers 96% of all
users. The graph shows that removing large ASes, measured by the
number of instances (blue), has a significant impact on GF . The size
of the largest connected component decreases similarly whether
we remove the largest ASes when ranked by instances hosted (blue)
or by number of users (red). However, the number of connected
components in GF increases drastically when we remove the ASes
hosting the largest users rather than ASes ranked by number of
instances: the removal of just 5 ASes shatters the federation graph
into 272 components when sorted by users hosted, compared to just
139 when ranking by the #instances in the AS. This is explained
by the central role of a few ASes: the top 5 ASes by users cover
only 20% of instances (yet comprise 85% of users); when ranked by
number of instances, the top 5 covers 42% of instances (and 33.6%
of users).
Thus, when AS failures occur, Mastodon shows significantly
worse resilience properties than previously seen for just instance
failures (Figure 13(a)). This is driven by the fact that the top five
ASes by number of instances hosted — OVH SAS (FR), Scaleway
(FR), Sakura Internet (JP), Hetzner Online (DE), and GMO Internet
(JP) — account for 42% of all instances. Their removal yields a 49%
reduction in the size of LCC in the federation graph, leaving behind
an LCC which only covers 45% of instances and 66% of users. This
constitutes a radical drop in the capacity of Mastodon to dissemi-
nate toots via the federated subscription links. Indeed, removing
them not only wipes out a large number of nodes, but also results in
a smaller number of components which still remain. That said, the
linear degradation of the instance federation graph discussed previ-
ously provides some limited protection against a more catastrophic
failure as observed with the Mastodon social graph. Although a
(a)
(b)
(cid:0)I, E(cid:1), by removing: (a) the top N in-
Figure 13: Impact of node removal attacks. Each subfigure
measures, on Y1 axis, LCC and, on Y2 axis, the number
of components for GF
stances (each node in GF is an instance); and (b) the top N
Autonomous Systems, including all instances hosted within.
may remove these users. Indeed, individual instance failures, which
we examine next, can take out huge subsets of users from the global
social graph.
Impact of Removing Instances. As discussed earlier, instance
failures are not uncommon, and can have an impact that exceeds
their local user base due to the (federated) cross-instance intercon-
nectivity of users in the social follower graph. Therefore, we next
measure the resilience of the instance federation graph (GF). In
Figure 13(a), we report the impact of instance failures on GF . We
iteratively remove the top N instances, ordered by their size; we
rank by both number of users (red) and number of toots (green).
When ranking via either metric, we notice a remarkably robust
linear decay in the size of the LCC, and a corresponding increase
in the number of components.
Unlike the drastic breakdown of the social graph, this elegant
degradation is caused by the more uniform degree distribution
of the federation graph (as compared against traditional social
networks [6]). We emphasise that the instance federation graph
shows the potential connectivity of instances. However, individual
instance failures would still have an enormous impact on the social
graph.
225
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
A. Raman et al.
Figure 14: Ratio of home toots (produced on the instance) to
remote toots (replicated from other ones).
rare occurrence, we therefore argue that techniques to avoid overt
dependency on individual hosting ASes would be desirable.
5.2 Breaking the Content Federation
The above process of federation underpins the delivery of toots
across the social graph. For example, when a user shares a toot,
it results in the toot being shared with the instances of all their
followers. Although we obviously cannot validate if a user reads a
toot, we next explore the importance of federation for propagating
content to timelines.
Role of Remote Toots. Aiming to measure the prevalence of
federated remote toots, Figure 14 plots the proportion of home vs.
remote toots taken from the federated timeline (see Section 2) of
every instance in our dataset, ordered by the percentage of home
toots. The majority of toots on the federated timeline are actually
generated by remote instances: 78% of instances produce under 10%
of their own toots. In the most extreme case, we see that 5% of in-
stances are entirely reliant on remote toots, and generate no home
toots themselves. This suggests that some highly influential cen-
tral instances operate as ‘feeders’ to the rest of the network. Also,
the more toots an instance generates, the higher the probability
of them being replicated to other instances (correlation 0.97), thus
highlighting the importance of a small number of feeders, without
whom smaller instances would be unable to bootstrap. This is an-
other inherent form of centralisation, which any social system will
struggle to deviate from.
These results motivate us to measure the impact of instance
and AS failures on toot availability. We evaluate three scenarios:
(i) where a toot is exclusively hosted on its home instance, and
fetched by the remote instance on demand (denoted as “no repli-
cation”); (ii) where a toot is actively replicated to any instances
with users that follow the toot’s author (“subscription replication”);
and (iii) where a toot is replicated onto a random set of n instances
(“random replication”). Mastodon partly supports option (ii), but
replicated toots are only temporarily cached and they are not glob-
ally indexed, i.e., they are only visible to users local to the instance
where the replica is. For these experiments, we assume a scenario
where toots are globally indexed, e.g., via a Distributed Hash Ta-
ble [53], allowing users to access replicated toots from any instance.
For simplicity, we treat all toots as equal, even though in practice
more recent toots are likely to be more important.
In Figure 15(a), we measure the availability of
No replication.
toots when removing entire ASes; and in Figure 15(b), we exam-
ine availability of toots when individual instances fail. Both plots
depict results without replication enabled. In both cases, toot avail-
ability declines rapidly in the face of failures. Removing the top
10 instances (ranked by number of toots) results in the deletion
of 62.69% toots from Mastodon. Removing the top 10 ASes (again,
ranked by number of toots) results in 90.1% of toots becoming
unavailable. Therefore, we argue that running Mastodon without
replication is not a viable option if resilience is to be a priority.
Subscription-based Replication. Figures 15(c) and 15(d) report
the availability of toots if they are replicated onto the instances
that follow them, i.e., via federated links. We consider any toot as
available if there is at least one live Mastodon instance holding
a replica, and assume the presence of a global index (such as a
Distributed Hash Table) to discover toots in such replicas.
Availability improves using this method. For example, remov-
ing the top 10 instances now only results in 2.1% of toots becom-
ing unavailable (as compared to 62.69% without replication). The
equivalent values when removing the top 10 ASes are 18.66% with
replication (vs. 90.1% without).
Random Replication. Above, we assumed that toots are only
replicated to the followers’ instances. We now experiment with a
random replication strategy that copies each toot onto n random
instances. We test for n = {1, 2, 3, 4, 7, 9}, alongside no replication
(No-rep) and subscription-based replication (S-Rep). We do this for
all toots and present the results in Figure 16. In each iteration, we
remove the current remaining top instance (as ranked by number of
toots), and check the availability of toots according to the different
replication strategies.
The figures shows that random replication substantially outper-
forms subscription-based replication. This is due to the biased way
that subscription-based replication works, in which we find that
9.7% of toots have no replication due to a lack of followers, yet
23% of toots have more than 10 replicas because they are authored
by popular users. After removing 25 instances, subscription-based
replication has 95% availability, whereas 99.2% availability could
have been achieved with just 1 random replication. More promi-
nently, subscription-based replication tends to place all replicas
onto a small set of popular instances (i.e., where the followers are),
due to the skewed popularity distribution of users. This is yet an-
other consequence of the natural centralisation that these systems
experience. Thus, removing these popular instances will remove
not only the original toot but also the replica(s).
In contrast, the random strategy distributes the load more evenly,
such that instance failures impact fewer replicas of the toots. There
are a number of practical concerns that will impact such strategies.
Most notably, it would be important to weight replication based on
the resources available at the instance (e.g., storage).
6 RELATED WORK
Decentralised Social Networks. Several efforts have worked
toward building distributed social network platforms. At first,
226
Challenges in the Decentralised Web
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
(a) No replication
(b) No replication
Figure 15: Availability of toots based on (a) removing ASes, with ASes ranked based on #instances, #toots and #users hosted;
and (b) removing instances, ranked by #users, #toots, and #connections with other instances. In (c) and (d), we report the toot
availability when replicating across all instances that follow them.
(c) Subscription Replication
(d) Subscription Replication
on the DW. For example, both Mastodon and PeerTube use Ac-
tivityStreams and ActivityPub; thus, they can exchange data. An