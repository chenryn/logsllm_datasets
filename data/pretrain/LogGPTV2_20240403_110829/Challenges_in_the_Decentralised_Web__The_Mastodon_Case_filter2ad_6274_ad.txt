### Table 2: Top 10 Instances by Number of Toots from the Home Timeline

| Users | Toots | Instances | Out Degree (OD) | In Degree (ID) |
|-------|-------|-----------|-----------------|----------------|
| 22.5K | 71.4M | 1352      | 1241            |                |
| 24.7K | 37.4M | 1273      | 1287            |                |
| 8809  | 34.9M | 1162      | 1106            |                |
| 23.3K | 435K  | 524       |                 |                |
| 27.6K | 2.37M | 711       | 1.52M           |                |
| 15.4K | 1.4M  | 1442      | 1083            |                |
| 507   | 30.9M | 39        |                 |                |
| 7510  | 7.35M | 1198      | 337             |                |
| 772   | 4.18M | 631       | 981             |                |
| 10.8K | 2.6M  | 561       | 853K            |                |

**Run by:**
- Individual
- Dwango
- Pixiv
- Individuals (CF)
- Unknown
- Bokaro Bowl (A)
- AS (Country)
- Cloudflare (US)
- Amazon (JP)
- Sakura (JP)
- Online SAS (FR)
- Google (US)

### Impact of Removing ASes

As discussed earlier, many instances are co-located in a small number of hosting Autonomous Systems (ASes). We now inspect the impact of removing entire ASes and, consequently, all instances hosted within them. While this is a rare occurrence compared to instance failures, it does happen (see Section 4.4). We present this as a theoretical scenario that represents the most damaging impact. For context, an AS-wide collapse might be caused by catastrophic failures within the AS itself [20, 29] or via their network interconnections [17].

Figure 13(b) shows the Largest Connected Component (LCC) and the number of components for the federation graph (GF), while iteratively removing ASes ranked by both the number of instances (blue) and the number of users (red). Initially, 92% of all instances are within a single LCC, covering 96% of all users. The graph indicates that removing large ASes, measured by the number of instances, has a significant impact on GF. The size of the largest connected component decreases similarly whether we remove the largest ASes by instances hosted (blue) or by the number of users (red). However, the number of connected components in GF increases drastically when we remove ASes hosting the largest number of users rather than those ranked by the number of instances. The removal of just 5 ASes shatters the federation graph into 272 components when sorted by users hosted, compared to just 139 when ranking by the number of instances. This is due to the central role of a few ASes: the top 5 ASes by users cover only 20% of instances (yet comprise 85% of users); when ranked by the number of instances, the top 5 covers 42% of instances (and 33.6% of users).

Thus, when AS failures occur, Mastodon shows significantly worse resilience properties than previously seen for just instance failures (Figure 13(a)). This is driven by the fact that the top five ASes by number of instances hosted—OVH SAS (FR), Scaleway (FR), Sakura Internet (JP), Hetzner Online (DE), and GMO Internet (JP)—account for 42% of all instances. Their removal results in a 49% reduction in the size of the LCC in the federation graph, leaving behind an LCC that only covers 45% of instances and 66% of users. This constitutes a radical drop in the capacity of Mastodon to disseminate toots via federated subscription links. Indeed, removing these ASes not only wipes out a large number of nodes but also results in a smaller number of remaining components. Although the linear degradation of the instance federation graph provides some limited protection against more catastrophic failures, as observed with the Mastodon social graph, the impact is still severe.

### Impact of Removing Instances

Instance failures are not uncommon and can have an impact that exceeds their local user base due to the cross-instance interconnectivity of users in the social follower graph. Therefore, we next measure the resilience of the instance federation graph (GF). Figure 13(a) reports the impact of instance failures on GF. We iteratively remove the top N instances, ordered by their size; we rank by both the number of users (red) and the number of toots (green). When ranking via either metric, we notice a remarkably robust linear decay in the size of the LCC and a corresponding increase in the number of components.

Unlike the drastic breakdown of the social graph, this elegant degradation is caused by the more uniform degree distribution of the federation graph, as compared to traditional social networks [6]. We emphasize that the instance federation graph shows the potential connectivity of instances. However, individual instance failures would still have an enormous impact on the social graph.

### Role of Remote Toots

To measure the prevalence of federated remote toots, Figure 14 plots the proportion of home vs. remote toots taken from the federated timeline of every instance in our dataset, ordered by the percentage of home toots. The majority of toots on the federated timeline are generated by remote instances: 78% of instances produce under 10% of their own toots. In the most extreme case, 5% of instances are entirely reliant on remote toots and generate no home toots themselves. This suggests that some highly influential central instances operate as 'feeders' to the rest of the network. Additionally, the more toots an instance generates, the higher the probability of them being replicated to other instances (correlation 0.97), highlighting the importance of a small number of feeders, without whom smaller instances would be unable to bootstrap. This is another inherent form of centralization, which any social system will struggle to deviate from.

### Impact of Instance and AS Failures on Toot Availability

These results motivate us to measure the impact of instance and AS failures on toot availability. We evaluate three scenarios:
1. **No Replication**: A toot is exclusively hosted on its home instance and fetched by the remote instance on demand.
2. **Subscription Replication**: A toot is actively replicated to any instances with users that follow the toot’s author.
3. **Random Replication**: A toot is replicated onto a random set of n instances.

Mastodon partly supports option (ii), but replicated toots are only temporarily cached and not globally indexed, i.e., they are only visible to users local to the instance where the replica is. For these experiments, we assume a scenario where toots are globally indexed, e.g., via a Distributed Hash Table [53], allowing users to access replicated toots from any instance. For simplicity, we treat all toots as equal, even though more recent toots are likely to be more important.

In Figure 15(a), we measure the availability of toots when removing entire ASes, and in Figure 15(b), we examine the availability of toots when individual instances fail. Both plots depict results without replication enabled. In both cases, toot availability declines rapidly in the face of failures. Removing the top 10 instances (ranked by number of toots) results in the deletion of 62.69% of toots from Mastodon. Removing the top 10 ASes (again, ranked by number of toots) results in 90.1% of toots becoming unavailable. Therefore, we argue that running Mastodon without replication is not a viable option if resilience is to be a priority.

### Subscription-based Replication

Figures 15(c) and 15(d) report the availability of toots if they are replicated onto the instances that follow them, i.e., via federated links. We consider any toot as available if there is at least one live Mastodon instance holding a replica, and assume the presence of a global index (such as a Distributed Hash Table) to discover toots in such replicas.

Availability improves using this method. For example, removing the top 10 instances now only results in 2.1% of toots becoming unavailable (as compared to 62.69% without replication). The equivalent values when removing the top 10 ASes are 18.66% with replication (vs. 90.1% without).

### Random Replication

We now experiment with a random replication strategy that copies each toot onto n random instances. We test for n = {1, 2, 3, 4, 7, 9}, alongside no replication (No-rep) and subscription-based replication (S-Rep). We do this for all toots and present the results in Figure 16. In each iteration, we remove the current remaining top instance (as ranked by number of toots) and check the availability of toots according to the different replication strategies.

The figures show that random replication substantially outperforms subscription-based replication. This is due to the biased way that subscription-based replication works, in which 9.7% of toots have no replication due to a lack of followers, yet 23% of toots have more than 10 replicas because they are authored by popular users. After removing 25 instances, subscription-based replication has 95% availability, whereas 99.2% availability could have been achieved with just 1 random replication. More prominently, subscription-based replication tends to place all replicas onto a small set of popular instances (i.e., where the followers are), due to the skewed popularity distribution of users. This is yet another consequence of the natural centralization that these systems experience. Thus, removing these popular instances will remove not only the original toot but also the replica(s).

In contrast, the random strategy distributes the load more evenly, such that instance failures impact fewer replicas of the toots. There are practical concerns that will impact such strategies, notably the need to weight replication based on the resources available at the instance (e.g., storage).

### Related Work

Several efforts have worked toward building distributed social network platforms. At first, decentralized social networks like Mastodon and PeerTube use ActivityStreams and ActivityPub, enabling data exchange between different platforms.