Output: ¯X: updated training set based on the selected features.
Get weight vector w by the learning-based classifier in Eq. (2)
trained on D;
Get manipulation cost vector c;
Calculate P = {P(1), P(2), ..., P(d)} using Eq. (8);
k = 1;
for k ≤ d do
Get a pseudo random number from R(.);
if R(.) ≤ P(k) then
else
¯Xk . = Xk .
¯Xk . = 0
end
k + +;
end
return ¯X;
subject to ¯f = sign(f ( ¯X)), where ¯f is the predicted label vector
based on a feature set ¯X. To solve the problem in Eq. (13), let
L(¯f, w, b; ξ) =
wT w +
1
2||y− ¯f||2 +
1
2β
1
2γ
bT b + ξT (¯f − ¯XT w− b).
(14)
= 0,
Based on the substitution and derivation from ∂L
∂w
∂L
∂ξ
= 0, we can get the more secure classifier as:
= 0, ∂L
∂¯f
= 0, ∂L
∂b
[I + (β ¯XT ¯X + γ I)−1]¯f = y.
(15)
4.2 SecENS: An Ensemble Learning Approach
to Improve the Detection Accuracy
In the previous section, a novel feature selection method SecCLS
is presented for constructing a more secure classifier against the
adversarial attacks. To improve the system security while not com-
promising the detection accuracy, in this section, we further propose
an ensemble learning approach called SecENS to aggregate a set of
classifiers built using SecCLS to generate the final output for the
detection.
Ensemble methods are a popular way to overcome instability and
increase performance in many machine learning tasks [43], such as
classification, clustering and ranking. An ensemble of classifiers is a
set of classifiers whose individual decisions are combined in some
way (e.g., by weighted or unweighted voting) to classify new sam-
ples, which is shown to be much more accurate than the individual
classifiers that make them up [41]. Typically, an ensemble can be
decomposed into two cascaded components: the first component
is to create base classifiers with necessary accuracy and diversity;
the second one is to aggregate all of the outputs of base classifiers
into a numeric value as the final output of the ensemble. In general,
base classifiers are generated by subsampling training set or input
features (as done in boosting or bagging), manipulating the output
targets, or injecting randomness in the learning algorithm [12].
367In this paper, with certain accuracy of each individual classifier,
we aim to diversify the classifiers that form the ensemble while also
consider the integration of whole feature space. More specifically,
the set of classifiers in the ensemble should follow two criteria: (1)
the feature set used for building each classifier should differentiate
from each other (i.e., feature differentiation), and (2) the ensemble
should cover as many features as possible to assure the integration
of whole feature space (i.e., feature integration). Therefore, we first
construct each individual classifier using the proposed feature se-
lection method SecCLS described in Section 4.1; then we follow the
above two criteria and propose SecENS to aggregate a set of the
constructed classifiers to generate the final output for the detection.
We present SecENS beginning with the definitions of the above two
criteria.
Feature Differentiation (denoted as fD). Given two feature
sets Fa ∈ Rd and Fb ∈ Rd, which are selected using the proposed
method SecCLS respectively, the differentiation between them can
be defined as:
fD(Fa, Fb) = 1 − J(Fa, Fb) =
|Fa ∪ Fb| − |Fa ∩ Fb|
|Fa ∪ Fb|
(16)
.
Feature Integration (denoted as fI ). The feature integration of
an ensemble is the percentage of features that are included in at
least one of the base classifiers, which can be defined as follow:
|K
k =1 Fk|
d
fI(F) =
where the feature set F ∈ Rd in the ensemble is aggregated by the
feature sets Fk (k = 1, 2, ..., K) from base classifiers.
In SecENS, we employ boosting [12] during the training phase.
Boosting works by sequentially applying a base classifier to train the
updated weighted samples and aggregating all the outputs gener-
ated from the individual classifiers into the final prediction. At each
iteration, the misclassified samples are assigned higher weights, so
that at the next iteration, the classifier will focus more on learning
those samples [16]. With the use of boosting, our proposed ensem-
ble learning approach SecENS builds the ensemble by integrating
both feature differentiation (fD) and feature integration (fI ) to di-
versify the classifiers while preserve a significant integration of
whole feature space. Algorithm 2 illustrates the implementation of
the proposed SecENS in detail.
5 EXPERIMENTAL RESULTS AND ANALYSIS
In this section, to empirically validate our developed system Se-
cureDroid which integrates the proposed methods presented in
Section 4, we present four sets of experimental studies using real
sample collections obtained from Comodo Cloud Security Center:
(1) In the first set of experiments, we evaluate the effectiveness of
SecureDroid against different kinds of adversarial attacks; (2) In the
second set of experiments, we assess the security of our proposed
feature selection method SecCLS applied in the system SecureDroid;
(3) In the third set of experiments, we further compare SecureDroid
with other alternative defense methods; (4) In the last set of experi-
ments, we evaluate the scalability of SecureDroid based on a larger
sample collection.
,
(17)
break;
Algorithm 2: SecENS – An ensemble learning approach to
improve the detection accuracy.
Input: Training data set D = {xi , yi}n
i =1; W: weights of
training apps; ε: error rate for each classifier; ζ :
importance of each classifier; ηd: specified threshold of
fD; ηf : specified threshold of fI ; ηa: specified
threshold of training accuracy.
for i = 1, 2, ..., n; t = 0;
εt ←n
Output: f: the labels for the apps.
Initialize: W1(i) = 1
n
Get training set ¯X1 and selected feature set F1 on D using
SecCLS;
while 1 do
t + +;
Train a base classifier using ¯Xt ;
Get weak hypothesis ft : ¯Xt → {−1, 1};
Calculate error rate of ft :
Set ζt = 1
Update Wt +1(i) = Wt(i) exp(−ζt ¯yi ft(¯xi))
i = 1, 2, ..., n;
Calculate fI(F);
Calculate the training accuracy (acc) of the ensemble
i =1 ζi fi(X));
if fI(F) ≥ ηf and acc ≥ ηa then
n
i =1 Wt(i) exp(−ζt ¯yi ft(¯xi)) for
based on f = sign(t
i )];
i =1 Wt(i)[yi (cid:44) ft(¯xt
2 ln( 1−εt
) ;
εt
end
Get ¯Xt +1 and Ft +1 on D using SecCLS;
, Fj) for j = 1, 2, ..., t;
Calculate fD(Ft +1
while min{ fD(Ft +1
, Fj) | j = 1, 2, ..., t} < ηd do
Get ¯Xt +1 and Ft +1 on D using SecCLS;
, Fj) for j = 1, 2, ..., t;
Calculate fD(Ft +1
end
return f = sign(t
end
i =1 ζi fi(X));
5.1 Experimental Setup
5.1.1 Data Collection. The real sample collections we obtained
from Comodo Cloud Security Center contain two sets: (1) The
first sample set includes 8, 046 apps (4, 729 are benign apps, while
the remaining 3, 317 apps are malware including the families of
Geinimi, GinMaster, DriodKungfu, Hongtoutou, FakePlayer, etc.).
The extracted features from this sample set is with 926 dimensions,
which include 104 permissions, 204 filtered intents, 330 API calls,
and 288 new-instances. (2) The second dataset has larger sample
collection containing 72, 891 Android apps (40, 448 benign apps and
32, 443 malicious apps).
5.1.2 Evaluation Measures. To quantitatively validate the effec-
tiveness of different methods in Android malware detection, we
use the performance indices shown in Table 2.
5.1.3
Implementation of Different Adversarial Attacks. To thor-
oughly assess the security and detection accuracy of our developed
368Table 2: Performance indices of Android malware detection
Indices
TP
TN
FP
FN
Precision
Recall/TPR
ACC
F1
Description
Number of apps correctly classified as malicious
Number of apps correctly classified as benign
Number of apps mistakenly classified as malicious
Number of apps mistakenly classified as benign
TP/(TP + FP)
TP/(TP + FN)
(TP + TN)/(TP + TN + FP + FN)
2 × Precision × Recall/(Precision + Recall)
system SecureDroid against a wide class of attacks, we define and im-
plement three kinds of representative adversarial attacks [25, 26, 46]
considering different skills and capabilities of attackers, which are
presented as followings.
Brute-force (BF) attack. To implement such kind of attack, for
each malicious app (i.e., x+) we would like to manipulate, we first
use Jaccard similarity [18] to find its most similar benign app (i.e.,
x−) from the sample set. Given these two apps, the procedure begins
with x+ and modifies features one at a time to match those of x−,
until the malicious app is classified as benign or the evasion cost
reaches to δmax.
Anonymous (AN) attack. To simulate anonymous attack in
which the defenders may have zero knowledge of what the attack
is, we randomly manipulate some features for addition and some
for elimination with the evasion cost of δmax.
Well-crafted (WC) attack. In this adversarial setting, we use
the wrapper-based approach [25, 46] to iteratively select a feature
and greedily update this feature to incrementally increase the classi-
fication errors of the targeted learning system. Specifically, we first
rank the features using methods such as information gain [18] to
calculate their contributions to the classification problem. Then we
conduct bi-directional feature selection, i.e., forward feature addi-
tion and backward feature elimination, to manipulate the malicious
apps. At each iteration, using the attack model formulated in Eq. 5
(in Section 3.3) which encodes two competing objectives (i.e., maxi-
mizing the classification error while minimizing the evasion cost
for optimal attacks), a feature will be either added or eliminated.
of manipulated features and the corresponding evasion costs which
also consider the complexity of different feature manipulations.
Among these attacks, the well-crafted (WC) attack is the most
effective strategy, since the evasion cost of this attack (also the
number of features manipulated by this attack) is minimum when
compromising the learning classifier into the same level, which can
be seen in Figure 3.(b).
5.2 Evaluation of SecureDroid against Different
Kinds of Adversarial Attacks
In this set of experiments, based on the first sample set described
in Section 5.1, we validate the effectiveness of SecureDroid against
above mentioned adversarial attacks. To estimate the reasonable
evasion cost for attackers to perform the adversarial attacks, based
on the first sample set, we explore the average number of fea-
tures that each app possesses, which is 98. In general, 50% of the
average number of features is considered as an extreme for the
adversary to perform the attack. Based on these observations, we
implement the above three kinds of attacks to access both Secure-
Droid and Original-Classifier with the manipulated features varying
in {10%, 20%, 30%, 40%, 50%} of the average number of features (i.e.,
98), whose corresponding evasion costs under different kinds of
attacks are shown in Figure 4.(a)–(c) (X-axis). We randomly select
90% of the samples for training, while the remaining 10% is used
for testing. We use these attacks to taint the malicious apps in the
testing set respectively, and then assess the security of SecureDroid
under different attacks with different evasion costs by comparison
with the Original-Classifier. To implement SecureDroid, empirically
we found that the parameters of λ = 0.7 and ρ = 0.8 in Eq. 8 are the
best, and apply them to our problem throughout the experiments.
To validate the detection performance of SecureDroid without at-
tacks, we also perform 10-fold cross validations for evaluation. The
experimental results are shown in Figure 4.
(a) Under brute-force (BF) attacks
(b) Under anonymous (AN) attacks
(b) Manipulated features No. (under 50% TPR)
(a) Manipulated features No. vs. δmax
Figure 3: Effectiveness evaluation of different attacks.
To estimate the effectiveness of different attacks, we implement
the above three kinds of attacks to access the Original-Classifier
described in Section 2.3 and make its TPRs drop from 90% to 50%. For
each attack, Figure 3.(a) shows the relations between the numbers
(c) Under well-crafted (WC) attacks
(d) ROC curves without attacks
Figure 4: Security evaluations of SecureDroid and Original-
Classifier under brute-force (BF) attacks, anonymous (AN)
attacks, well-crafted (WC) attacks, and without attacks.
369Under attacks. From Figure 4.(a)–(c), we can see that Secure-
Droid can significantly enhance security compared to the Original-
Classifier, as its performance decreases more elegantly against in-
creasing evasion costs, especially in the scenarios of BF attack and
WC attack. In the BF attack, the TPR of Original-Classifier drops to
5.99% with evasion cost δmax of 23.4 (i.e., modifying 50 features),
while SecureDroid retains the TPR at 70.06% with the same eva-
sion cost. In the WC attack, the performance of Original-Classifier
is compromised to a great extent with TPR of 13.62% under the
evasion cost of 25.2; instead, SecureDroid can significantly bring
the detection system back up to the desired performance level: the
TPRs of SecureDroid are actually never lower than 80.00% even
with increasing evasion costs. This demonstrates that SecureDroid
which integrates our proposed methods is resilient against the most
effective attack strategy (i.e., WC attack) among the three repre-
sentative adversarial attacks. In the AN attack, which is simulated
under defenders have zero knowledge of what the attack is and
by randomly injecting or removing features from the malicious
apps, SecureDroid also outperforms the Original-Classifier, which
can retain the average TPR at 85.16% with different evasion costs.
Without attacks. Figure 4.(d) shows the ROC curves of the
10-fold cross validations for Original-Classifier and SecureDroid
without any attacks. From Figure 4.(d), we can see that, though
SecureDroid is designed to be resilient against different kinds of
adversarial attacks, its detection performance is as good as the
Original-Classifier in the absence of attacks.
The experimental results and above analysis demonstrate that
SecureDroid can effectively enhance security of the learning-based
classifier without compromising the detection accuracy, even at-
tackers may have different knowledge about the targeted learning
system. Based on these properties, SecureDroid can be a resilient
solution in Andriod malware detection.
5.3 Evaluation of SecCLS applied in SecureDroid
In this section, based on the same training and testing datasets in