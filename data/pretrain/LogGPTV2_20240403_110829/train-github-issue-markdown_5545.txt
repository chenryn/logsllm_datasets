I wish I could say more about this, but it is with proprietary data.
Basically, I have a DataFrame of 10 columns and around 7000 rows. When I call
"duplicated" on 4 of the rows (3 have strings, 1 has a float), I get around 10
items being flagged as duplicate (5 pairs) when in fact, they are different.
This is sample text of what is being flagged:
              Type   Line                                         LineString  \
    1885      else  832.0                (&temp32)->byte1 = (&temp32)->byte4   
    1895  do while  832.0                                        do while(0)   
    4515      enum  122.0    enum {QQ_ON_UNASSERTED = 0, QQ_ON_ASSERTED = 1}   
    4521      enum  167.0          enum {FIELD_RESET = 1, FIELD_TRIPPED = 0}   
                  Parameter  Filename  
    1885             temp32   arinc.c  
    1895                      arinc.c  
    4515      QQ_ON_ASSERTED  eeprom.c  
    4521       FIELD_TRIPPED  eeprom.c  
I know that duplicates are checked through hashing, but is there some way to,
I don't know, compare a checksum or a more robust measurement to ensure that
only duplicates are flagged? What is the chance that two items could be hashed
to the same value?
By adding more of the fields to check the data I am able to prevent false
duplicate flags.
I am using Pandas 0.18 from WinPython-64bit-3.4.4.2Qt5.