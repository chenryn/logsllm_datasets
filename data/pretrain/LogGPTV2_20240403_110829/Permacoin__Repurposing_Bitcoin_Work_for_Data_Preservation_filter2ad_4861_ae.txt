α )n number of bins, where α > 1 is some
constant related to the coding rate of the MDS code. Let X
be the random variable denoting the number of empty bins
after m throws.
Lemma 2 (Recoverability under randomly chosen stored
set.). Let f
α for some constant α > 1. Let
n = 1 − 1
m
n = r > ln α > 1 − 1
α for some constant r. Then,
Pr[recovery failure] = Pr[X > n − f ]  0 is some constant dependent on α and
r.
The proof (included in the full version of this paper) fol-
lows from a standard balls and bins analysis. The lemma can
be strengthened to allow the adversary to selectively choose
to omit up to a constant fraction of the total segments stored
by good users.
7 Parameterization and Microbenchmarks
In this section we provide evidence of the feasibility of Per-
macoin and suggest how in practice to determine the remain-
ing parameters. We are particularly interested in determining
the size of the largest dataset we could reasonably store by
repurposing the Bitcoin network. To calculate this value, we
assess the storage capacity of the network and determine what
veriﬁcation costs can be tolerated by the network.
In every Bitcoin epoch, a block containing a sequence of
transactions and an SOP solution is broadcast to every node in
the network. Each node must verify both the transactions and
the proof-of-work. Our scheme only affects the SOP solution
(this is in contrast to other proposed Bitcoin modiﬁcations,
such as Zerocoin [17], which leave the SOP unchanged but
change the procedure of transaction validation). In addition
to validating each new block as it is broadcast, new clients
that join the network for the ﬁrst time, or clients that rejoin
the network after some dormant period (e.g., mobile clients),
must download and validate all of the blocks generated during
some ofﬂine period. The maximum tolerable validation cost
should certainly be less than 10 minutes (the average epoch
time), since otherwise even the online participants would not
be able to keep up. We show reasonable parameter settings
for which an ordinary computer can still validate a week’s
worth of segments in approximately 6 seconds.
Cost of Validation. The cost of validating one iteration of our
SOP consists of (a) computing the hash of one ﬁle segment
(equal to m = F/b) and two 120-bit secrets, (b) verifying a
Merkle tree proof for the segment by recomputing log2(rF )
hashes, and (c) verifying a Merkle tree branch for the secrets
485
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:01:14 UTC from IEEE Xplore.  Restrictions apply. 
by computing log2(k + λ) hashes. Note that the validation
cost does not depend on δ, the storage capacity of the network
relative to the ﬁle.
In Figure 7, we show the validation cost for k = 20 iter-
ations of a puzzle using a 20-terabyte ﬁle, as a function of
the segment size. Figure 7(a) shows the cost in terms of ver-
iﬁcation time, and Figure 7(b) in terms of proof size. For a
reasonable choice of r ≈ 4, the cost of validating a proof
is approximately 6 milliseconds (based on measured time to
compute SHA1 hashes). One new (valid) proof is generated
in each epoch; assuming the average epoch time remains 10
minutes, it would take a participant 6 seconds to validate a
week’s worth of blocks. For comparison, this is approximately
a thousand times more expensive than Bitcoin’s current puz-
zle proof, which requires computing only a single hash (two
invocations of SHA256). The proof size in this case would
be about 20KB. The average Bitcoin block is approximately
200KB, so our scheme would only increase the average block
size by about 10%.
Parameter Choice and ASIC Mining. An underlying as-
sumption about Bitcoin’s incentive structure is that a pow-
erful miner cannot earn disproportionately more revenue in
expectation than an ordinary individual (i.e., “one-CPU-one-
vote” [2]). Because the original hash-based proof-of-work
operation is simple, it can be computed efﬁciently with a
small circuit, and the only way to effectively mine faster is
to replicate the basic circuit.
During the ﬁrst few years after Bitcoin’s introduction, min-
ing participants primarily used ordinary CPUs, but shortly
thereafter transitioned to repurposed graphics cards (GPUs)
which provided much more cost-effective hashing power.
Now, ﬁve years since Bitcoin’s introduction, the most cost
effective mining is performed using efﬁcient ASICs designed
and manufactured solely for the purpose of Bitcoin mining.
These ASICs consist of multiple copies of the same circuit
on a small chip, which means that the most cost-effective
technology remains accessible in small quantities to ordi-
nary users, at approximately a proportional price. The small-
est ASIC available on the market today is the ASICMINER
“Block Erupter Sapphire”, which costs $30, and achieves a
cost-effectiveness of 11 megahashes/sec/$. Another ASIC
company, Butterﬂy Labs, also sells a reel of unpackaged
chips, for $75 per chip (in minimum lots of 100) at a cost-
effectiveness of 53 megahashes/sec/dollar5. The feasibility
of our system relies on choosing parameters to preserve this
economic structure as much as possible.
We model the economics of participation among large and
small users by considering cost-effectiveness of conﬁgura-
tions attainable given a ﬁxed equipment budget. Our storage-
based puzzle alters the economic structure by introducing a
base cost: the amount of storage assigned to each identity, m,
determines the minimum cost of a suitable storage device. A
smaller value of m means users with small budgets can par-
5See https://products.butterflylabs.com/homepage-
subproducts/65nm-asic-bitcoin-mining-chip.html
ticipate more effectively. On the other hand, larger users have
little incentive to use additional identities; they could instead
achieve proportionally increased mining power by purchas-
ing more devices and ﬁlling them with multiple copies of the
same dataset segments, thus contributing less to the recov-
ery probability. We would like to set m as large as possible,
while preserving the low “barrier-to-entry” that enables ordi-
nary users to participate with the most cost-effective conﬁgu-
ration.
We consider two mining conﬁgurations, based on two
widely available choices for storage: a) a solid-state stor-
age device SSD, and b) random access memory (DDR3
SDRAM). In either case, it is also necessary to perform hash-
ing operations. Since existing Bitcoin mining ASICs perform
essentially the same operation, we can use them as a bench-
mark: the $30 Block Erupter ASIC could compute approxi-
mately 333 million puzzle iterations per second.
The throughput of a high-performance SSD is on the order
of 25,000 per second for random reads of 4KB blocks [18],
whereas a DDR3-1600 memory module can support 200 mil-
lion fetches per second (of 64B segments). Thus it would take
thousands of SSDs to saturate the computational capacity of
a small ASIC, but a single small ASIC would roughly match
the fetch rate of one set of RAM.
Next we consider several possible settings for the per-
identity storage requirement (cid:3). We assume that the most
cost effective strategy for (cid:3) > 2GB is to purchase sets of
RAM and ASICs in approximately equal amounts (illustrated
in Figure 8). This is because we are not modeling the cost
of memory buses (i.e., the motherboard), and so two sets of
2GB RAM cost about as much as one set of 4GB of RAM,
yet results in twice the total throughput. Thus the effect of
increased (cid:3) is to raise the minimum cost of participation. We
believe it would be reasonably consistent with Bitcoin’s ex-
isting economic structure to choose (cid:3) = 4GB, in which case
the minimum equipment investment to participate with an ef-
ﬁcient conﬁguration is only $60.
Storage Capacity of the Permacoin Network.
If every-
one in the Bitcoin network had always used our scheme
rather than the original protocol, investing an equal amount of
money to acquire and operate mining equipment, how much
storage capacity could we expect to recycle? Miners in Bit-
coin are essentially anonymous, so it is difﬁcult to precisely
estimate the number of distinct participants and the distribu-
tion of the computational power contributed. Nonetheless,
we have several available approaches. First, at the time of
writing, there are estimated to be about 30,000 known nodes
on the network. We may be willing to suppose, as an upper
bound, that each node participates an equal amount. How-
ever, not all connected nodes necessarily mine at all, and in
fact not every miner needs to operate a network-visible node
(instead, miners may connect directly to mining pool opera-
tors). If each user contributes 4GB of storage, we would have
a total capacity of 120 terabytes.
Another approach is to observe the overall hashing capac-
486
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:01:14 UTC from IEEE Xplore.  Restrictions apply. 
(a)
(b)
Figure 7: Estimated cost of validation (in seconds) vs. segment size, for a puzzle with k = 20 iterations, using a 20 terabyte
dataset F for varying coding rates r.
ity (“hashpower”) of the network and estimate the amount
of money spent on mining equipment. The current hash-
power of the network is 4e15 hashes per second. The most
cost-effective currently available mining equipment is the
line of Avalon ASICs, whose costs is approximately 50e6
hashes per second per dollar. As a lower bound, if every-
one used these ASICs, then at least $80 million dollars has
been spent on equipment.6 If this infrastructure investment
were entirely redirected toward SSD drives, assuming $70 for
100GB, the result would be a total network storage capacity
of 100 petabytes. The cost-density of RAM is lower ($20
for 2 gigabytes); given an approximately equal investment in
hashing devices to saturate it, the result would be an overall
network storage capacity of 4 petabytes.
Feasibility example. Assuming we’ve identiﬁed the storage
capacity of the network – suppose we take it to be 1 petabyte
— what is the largest size ﬁle we could hope to safely store?
Could we store 208 TB of data (one popularly cited estimate
of the storage requirement for the print collection of the Li-
brary of Congress [19])? We ﬁnd that yes, using our scheme
this is possible. Using parameters r = 4, and assigning seg-
ments in contiguous segments of 1 MB, we achieve a valida-
tion cost of less than 3 milliseconds and an error probability
less then e
8 Enhancements and Discussions
Stealable Puzzles.
In Permacoin, local storage of private
keys is essential to ensure strong physical distribution of data.
We assumed in our adversarial model that users do not share
their private keys in order to outsource mining to an exter-
−10
10.
6This is an underestimate, since older FPGA and GPU mining equipment
contributes less proportionally (relative to its cost) to the overall hashpower.
Also, although some hashpower also comes from the “idle cycles” of ordi-
nary CPUs, the extreme disparity in cost effectiveness between ASICs and
ordinary CPUs leads us to conclude that the vast majority of mining income
is awarded to participants with dedicated hardware.
487
Figure 8: Cost effectiveness for various per-identity storage
requirements ((cid:3)), as a function of per-user equipment budget.
nal entity, as this entity could then steal a user’s mined coins.
However, there are numerous defenses the user may employ
against this theft, which make it less effective as a deterrent.
For the service provider to steal the reward, it would have to
reveal the user’s public key, which would alert the user and
lead to a race condition as both try to get their version of
the block accepted. Additionally this would allow the user to
provide evidence of the server’s theft, tarnishing the server’s
reputation, and potentially enabling legal contract enforce-
ment. A variant of our scheme uses generic zero-knowledge
SNARKs (e.g., [20]) to nullify these defenses, allowing any
server to steal the reward and evade detection.
In the full
version of our paper, [16] we provide the details of this con-
struction and show that it adds no overhead for ordinary par-
ticipants, yet is practical as a deterrent against outsourcing.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:01:14 UTC from IEEE Xplore.  Restrictions apply. 
Supporting Updates. So far we have considered the archiv-
ing of a static dataset. While it is plausible that archival data
may not often need to change, we would prefer to incorporate
additional data incrementally, and to garbage collect data that
is no longer desired. We describe some standard techniques
that can be added to our system to efﬁciently support updates.
We ﬁrst consider the case where some ﬁle segments update,
but the ﬁle size remain unchanged. Later, we will discuss how
to support additions and deletions.
Incrementally updatable erasure codes. Using a standard
maximum-distance-separable erasure code, updates to a sin-
gle segment may result in a constant fraction of the encoded
segments needing to be updated. This would not only result
in high computational cost during updates, but would also re-
quire each user responsible for storing the updated segment
to download the new segment.
To address this issue, we could instead employ an in-
crementally updatable erasure code [21, 22]. One suitable
scheme [21] relies on the following main idea: updates are
not applied to the main encoded ﬁle, but instead are ag-
gregated in a hierarchical log that is merged incrementally.
Using a special FFT-like encoding scheme, each update re-
sults in updates to O(log n) segments in an amortized sense.
Standard deamortization techniques for such hierarchical data
structures can be used to achieve O(log n) worst-case cost in-
stead of amortized cost, at the cost of a constant-factor larger
storage requirement. Using this scheme, for each update,
n ) segments on av-
each user would have to update O(
erage, where (cid:3) is the number of segments allocated to a user.
In other words, a user needs to download a new segment ev-
ery O( n
(cid:2) log n ) update operations. On average, the total storage
utilized in these schemes at any time is over-provisioned by a
factor of two compared to the size of the dataset.
(cid:2) log n
Decentralized Update Approval. One option would be to
allow the trusted dealer to approve each update, and to sign
each new Merkle root digest resulting from adding (or modi-
fying) a segment to the data set. A more satisfying way would
be to rely on the majority voting mechanism already inher-
ent to Bitcoin. Speciﬁcally, Bitcoin provides a mechanism
by which a majority of participants vote on transactions that
extend an append-only log; users attach fees to their transac-
tions in order to pay the participants to include them. It would
be natural to extend this mechanism to approve updates, so
that users would pay fees to add new data to the global data
set; additionally, segments could be removed if periodic fees
are not added.
Upon on approval of an update, the trusted dealer (or its
distributed instantiation by Bitcoin majority voting), must
compute the new digest of the dataset. Using an incremen-
tally updatable erasure code [21, 22], updating each segment
results in updates to O(log n) encoded segments, resulting in
O(log2 n) cost for the Merkle tree digest update.
Note that the updates added by a user are public; if privacy