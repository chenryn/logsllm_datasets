with the results described above to verify that a large fraction of
the trafﬁc due to these traditional web clients involves well-known
domains. We do not see a signiﬁcant volume contribution by ad-
vertised P2P clients. Further, even if such P2P trafﬁc falls into the
“Unclassiﬁed” bin, it represents little in terms of overall volume.
Therefore, in our dataset we do not observe a large proportion of
P2P systems running on top of HTTP, unless they employ mimicry
of well-known browsers, and also manipulate content types and do-
mains.
6. TRANSPORT PROTOCOL FEATURES
We next delve into exploring which of the various TCP options
and conﬁgurations we see in actual use. Doing so allows us to cali-
brate our expectations with regard to TCP throughput performance,
which we then explore in Section 7. We limit our analysis to con-
nections that transfer some actual TCP payload, which excludes a
large number of unproductive connections caused by backscatter,
scanning, or other establishment failures. The excluded connec-
tions contribute about 0.1% of all bytes, but amount to 35% of all
connections.
To compare our results to previous studies, we need to determine
the usage of options on a per-host basis. However, unlike previous
studies we expect to ﬁnd our dataset rife with NATs (within the
DSL customers’ home networks). Therefore, isolating individual
hosts presents a challenge, since multiple hosts may share a single
DSL line. To address this difﬁculty, we assess option usage in two
ways. The ﬁrst technique considers each DSL line identiﬁer as a
single host, and attributes any options observed in packets associ-
ated with the line to that host. Doing so obviously undercounts the
number of hosts. For the second approach, we assume that each
distinct TCP option set represents a distinct host. This likely over-
counts the number of hosts, so by employing both strategies we can
bracket the ranges for host-based use of various TCP options.
Window Scaling
Window Scaling enables efﬁcient data transfer when the
bandwidth-delay product exceeds 64 KB. We ﬁnd window scaling
advertisements in 32–35% of the SYNs in our dataset, with 4% of
the connections failing to successfully negotiate the use of window
scaling. When focusing on only connections transferring more than
50 KB, we ﬁnd only a small change, with 34–38% successfully ne-
gotiated window scaling. Finally, we observe that 45–62% of the
97hosts in our datasets advertise window scaling (across traces and
across our under- and over-estimates for host count). In contrast,
Medina et al. reported that 27% of the observed client hosts adver-
tised window scaling in early 2004 [34]. Of those advertisements,
97% were found to be zero (i.e., the client advertises the ability to
scale windows, but not the desire to do so). In our dataset, we do
not ﬁnd a predominance of scale factors of zero; most scale fac-
tors are in fact non-zero, and cover a wide range. Even with our
rough counting of hosts, we can see that use of larger windows has
become more routine over the past 5 years.
TCP Timestamp
Timestamps help TCP to compute more accurate round-trip time
estimates, and serve to disambiguate old packets from new ones
in very high-speed transfers. We observe timestamps advertised
in 11–12% of the connections in our dataset, with 8% of the con-
nections ultimately negotiating their use. We further observe that
21–39% of the hosts (across traces and host-counting methods) ad-
vertise timestamps, versus 22% as observed by Medina et al. [34].
Further, Veal [51] probed a variety of web servers and concluded
that 76% of the servers will use timestamps when requested by the
client.
Selective Acknowledgment (SACK)
SACK facilitates more effective recovery from lost data segments.
We ﬁnd that 97% of connections in our dataset advertise support
for SACK, with 82% of the connections successfully negotiating
its use.
In addition, we observe that roughly 9% of the connec-
tions that negotiate SACK have at least one instance whereby a
receiver uses SACK to report a discontinuous arrival (either due
to loss or reordering). Finally, we observe 82–94% of the hosts
in our dataset advertising SACK (across traces and host-counting
strategies). Medina et al. reported that in 2004 88% of the clients
attempted to use SACK [34], and that active probing found roughly
69% of successfully contacted servers supported SACK.
Maximum Segment Size (MSS)
The MSS governs the largest data segment a TCP sender will trans-
mit. Across all TCP trafﬁc, we ﬁnd advertised values in the 1300–
1460 byte range in 98% of the connections. These values arise
from the very common 1500 byte Ethernet MTU, minus space re-
quired for TCP/IP headers, as well as space for additional tunneling
headers.
Explicit Congestion Notiﬁcation (ECN)
ECN enables routers to signal conditions of congestion without
necessarily employing packet drops. We ﬁnd virtually no support
for ECN, observing only a handful of hosts (no matter how they are
counted) advertising support for it in their SYN packets.
Summary
We ﬁnd that usage of performance improving TCP options varies
considerably. SACK enjoys widespread deployment and use; win-
dow scaling is quite common in terms of both support and effective
(non-zero) employment; ECN sees almost no use.
7. PERFORMANCE/PATH CHARACTER-
ISTICS
We now turn our attention to factors that affect the performance
that users experience—spanning network effects, transport proto-
col settings, application behavior, and home networking equip-
ment.
In a previous study, Dischinger et al. [12] recently used ac-
tive measurements to probe 1,900 broadband host from 11 major
providers in Europe and North America. They found that the last-
mile predominates as the performance bottleneck and induces high
jitter in the achievable throughput. They also found that broadband
links have large queuing buffers of several hundred to several thou-
sand ms, and that 15% of last-mile RTTs exceed 20 ms. However,
they do not compare access versus remote contributions to RTT.
While their study covers a more diverse set of hosts, our approach
leverages capturing all activity of residential hosts.
Jiang and Dovrolis [25] estimated TCP RTTs from passive mea-
surements of unidirectional packet data using SYN-SYN/ACK-
ACK handshakes and a slow-start based approach. They found
that 90–95% of connections have RTTs 1 s. However, their analysis does not take delayed ACKs
into account. Fraleigh et al. [18] analyzed packet level traces from
the Sprint backbone from 2001, ﬁnding that the median RTT never
exceeded 450 ms across their 9 traces. Only 3 traces had median
RTTs >300 ms, while 6 traces had median RTTs of  50 KB experience loss or reordering. These rates
are consistent with the observation that 8% of connections that ne-
gotiated SACK actually exchanged a SACK block, as did 30% of
connections that transfered at least 50 KB. In addition, we ﬁnd that
about 1% of connections required SYN retransmissions in order to
successfully establish.
Finally, we ﬁnd that at some points the receiver’s advertised win-
dow “closes” (drops to zero). Generally, this behavior indicates
that the receiving application has failed to drain the operating sys-
tem’s TCP buffer quickly enough, and therefore TCP must gradu-
ally advertise less available buffer. As the advertised buffer space
decreases, the sender’s ability to keep enough data in ﬂight to fully
ﬁll the network path diminishes. We ﬁnd that for 4% of the down-
stream connections the advertised window drops to zero, while this
phenomenon occurs for 3% of the upstream connections.
7.2 Round-trip-times (RTT)
We gathered our measurements at the ISP’s broadband access
router, which is the ﬁrst IP router that trafﬁc from the local hosts
encounters. We can therefore divide the end-to-end RTT that the
residential connections experience into a local component, mea-
sured from our monitor to the end system and back, and a remote
component, from our monitor over the wide-area Internet path to
the host at the other end of the connection.
We estimate TCP RTTs using the connection setup handshake
(SYN, SYN/ACK, ACK) [25], ignoring connections with SYN
or SYN/ACK retransmissions, and connections in which the ﬁnal
ACK carries data (which can indicate that an “empty” ACK has
been lost). Figure 10 shows the smoothed probability distribution
of the RTTs. We found it quite surprising to observe that in many
cases the local RTT exceeds the remote RTT, i.e., the time to sim-
ply get to the Internet dominates over the time spent traveling the
Internet.
The difference manifests itself throughout most of the distribu-
tion. For example, the median, 75th, 90th, and 99th percentiles of
the local RTTs are all substantially larger than their remote coun-
terparts, and we ﬁnd that 1% of local RTTs exceed 946 ms, while
for remote RTTs the corresponding delay quantile is only 528 ms.
The 99th percentile of total RTT is 1328 ms, with a 90th percentile
of 278 ms and a median of 74 ms. While RTTs are often fairly
low, we also observe several cases for which the local RTT reaches
values in the 2–6 sec range and beyond.
Local RTTs follow a bi-modal distribution, with one peak at 7 ms
and another, larger one at 45 ms. This is consistent with the fact that
most DSL lines use interleaving [28, 24], which increases delay,
while a smaller number of the DSL lines use the “fast path” feature,
which does not contribute any signiﬁcant delay.
Remote RTTs exhibit three modes, at 13 ms, 100 ms, and
160 ms, with the latter two somewhat blurred in the plot. Likely
these modes reﬂect the geographic distribution of remote hosts
(e.g., Europe, US East coast, US West coast).
7.3 Impact of Access Technology
The not infrequent appearance of large local RTTs led us to in-
vestigate their possible cause. Typically, large RTTs reﬂect large
queuing delays. Indeed, Dischinger et al. [12] found that residen-
tial broadband links can exhibit queuing delays of several seconds
when a DSL line is fully utilized.
Manual inspection of sequence number plots of some connec-
tions with large RTTs (>1000 ms) indeed shows such queues build-
ing up. We therefore checked whether those lines utilized their ac-
cess bandwidth during these events. We found, however, that this
is not always the case: while we often see signiﬁcant trafﬁc on
these DSL lines, they do not necessarily utilize their upstream or
downstream bandwidth fully. A more detailed manual analysis re-
veals other effects, too, such as RTTs within a connection suddenly
jumping by an order of magnitude.
One possible cause could be wireless links in users’ homes,
given the plausibility of a large fraction of broadband users em-
ploying 802.11 wireless to connect their computers to the Inter-
net. In densely populated, urban areas, users often “see” numer-
ous wireless networks, and therefore can experience non-negligible
contention for the medium.
To assess
this hypothesis, we used several DSL links
(1x 8000 Kbps and 3x 2000 Kbps downstream) to estimate up-
stream and downstream throughput and queuing delays using active
measurements done with the nettest tool.
Using wired connections, we are able to fully utilize the DSL
link’s bandwidth. When using wireless connections, the achieved
throughput often drops to 400–1000 Kbps. In both cases, we ex-
perience queuing delays of several seconds. However, the reduced
throughput when using wireless access causes the queue to start
building up at lower rates. In addition, while we were unable to
saturate the 8000 Kbps link2 with a wired connection, and there-
fore had low or imperceptible queuing delay, using wireless the
queuing delay still rose to several seconds.
These results show that wireless networks can have a signiﬁcant
impact on the achievable throughput. In particular, 11 Mbps wire-
less cards and wireless connections in areas with many other wire-
less senders, and/or with poor link quality, face signiﬁcant perfor-
mance degradation. We veriﬁed that wireless connections, in un-
contested environments and with current 54 Mbps wireless devices,
offer the same throughput and queuing delay as wired connections.
7.4 Achieved Throughput
Next, we examine how many lines actually utilize their available
access bandwidth across a substantial period of time. We count the
number of transfered bytes per DSL line across 1 sec bins and then
calculate the throughput per bin. We call a line active if it sent at
2Due to a bottleneck in the Internet between the DSL line and the
measurement server
9950% downstream
50% upstream
10% downstream
10% upstream
]
%
[
s
e
n
i
l
e
v
i
t
c
a
f
o
n
o
i
t
c
a
r
F
0
8
0
6
0
4
0
2
0
s
w
o
l
f
t
n
e
r
r
u
c
n
o
c
5
2
0
2
5