results are not concise.
IDTime(s)SRC IP : PortDSTIP : PortDataType!"#0.0010.0.0.3:2000010.0.0.8:278905640A44030004007CAEE6F78210004FBDUnsolicitedResponse!$#3.0410.0.0.8:278910.0.0.3:20000056408C404000300B4B8C0D7007ACEConfirm!$%3.0410.0.0.8:278910.0.0.3:20000056412C4040003001E7CC1C10232010701EBE45A87FF002801Write!"%3.0610.0.0.3:2000010.0.0.8:278905640A44030004007CAEE7C18100003BDBResponse!"&2256.6010.0.0.3:2000010.0.0.8:282805644C4403000400D86BCCF382000033010701E2437D87FF0002F8C3UnsolicitedResponse!$&2256.6610.0.0.8:282810.0.0.3:20000056408C404000300B4B8C3D30078D3Confirm!"'2258.0610.0.0.3:2000010.0.0.8:282805644744030004005462CDF482000033010701D5477D87FF0002495CUnsolicitedResponse!$'2258.0710.0.0.8:282810.0.0.3:20000056408C404000300B4B8C4D4003118Confirm!"(5847.3810.0.0.3:2000010.0.0.8:108605640A44030004007CAEC0F082900043A2UnsolicitedResponse!$(5850.0210.0.0.8:108610.0.0.3:20000056408C404000300B4B8C0D0001B49Confirm!$)5850.5710.0.0.8:108610.0.0.3:20000056412C4040003001E7CC4C402320107015B1EB487FF00DA67Write!")5850.6610.0.0.3:2000010.0.0.8:108605640A44030004007CAEC4C481800080A3Response!$*5850.9110.0.0.8:108610.0.0.3:2000005640EC4040003006DD3C6C6025001000707003461Write!"*5850.9810.0.0.3:2000010.0.0.8:108605640A44030004007CAEC6C68100000A36Response!$+5851.2010.0.0.8:108610.0.0.3:20000056414C404000300C717C7C7013C02063C03063C04063C01066BAERead!"+5851.2910.0.0.3:2000010.0.0.8:108605644E44030004006F4DC7C78100000101000005190A02000005C347Response05640A44030004007CAEE6F78210004FBD----------------------05644C4403000400D86BCCF382000033010701E2437D87FF0002F8C305640A44030004007CAEE6F78210004FBD05640A44030004007CAEE7C18100003BDB!"#!"$!"#!"%!"#Cluster1!"$!"%!"&!"'!"(!")!"*Cluster2(a) Tokenization
(b) Initial clustering
(c) Clustering results
Fig. 3: Clustering by Discoverer
D. Our Technique
Insights. From the above discussion, we observe that both
alignment-based clustering and token-based clustering rely on
the assumption that messages are of the same type if they
have similar values or patterns. However, in many cases this
assumption does not hold and incurs inaccurate clustering. In
fact when a client or a server receives a message, it determines
the message type only by the keyword. Thus, if we can infer
the ﬁeld denoting the keyword, we would obtain the ideal clus-
tering results. Note that although some token-based clustering
methods use representative tokens for clustering [35], [80],
they only search for such tokens by statistics such as frequency,
which usually generates more than one representative token
and then leads to redundant clustering.
Another insight is to take better advantage of network
traces, which are the only input for trace based methods.
Existing works only analyze message data from one side (the
client side or the server side) to study the aforementioned hints.
However, we could observe more hints if we consider the mes-
sage traces from both sides, especially their correspondence.
For example, in Figure 1 we can see that all the Unsolicited
Response messages mc0, mc2, mc3, and mc4 from the client
side have the Conﬁrm messages ms0, ms2, ms3, and ms4 from
the server as the response (for setting up a new connection).
Also, the Write messages ms1, ms5, and ms6 sent by the
server always trigger Response messages, i.e., mc1, mc5, and
mc6 from the client. These additional hints could be used to
improve and validate clustering results.
As we already know that all these hints have inherent
uncertainty as arbitrary byte sequences could appear as hints,
the results may be incorrect/contradictory if we only consider
few hints for clustering. For example, alignment-based clus-
tering methods only use the hint that messages with high
similarity are of the same type. Inspired by the application
of probabilistic inference in speciﬁcation extraction [60], [34]
and program analysis [84], a more reasonable solution is to
4
(a) Multiple sequence alignment and keyword inference
(b) Clustering results
Fig. 4: Clustering by NETPLIER
combine various kinds of hints together in a probabilistic
fashion. Speciﬁcally, a prior probability is assigned to each
hint denoting its uncertainty instead of making a simple deter-
ministic call. Probabilistic inference aggregates these hints and
computes a posterior distribution from which we can derive the
most likely keywords and clustering.
Our Idea. We use multiple sequence alignment (MSA) al-
gorithms on messages from both the client and server sides
and partition messages into a list of ﬁelds. MSA tends to
be conservative and only produces a comprehensive list of
ﬁelds, which provides a solid starting point. For each ﬁeld, we
introduce a random variable to denote the probability of being
the keyword. Assume a ﬁeld is the keyword, messages could be
grouped into different clusters by the value of the ﬁeld, and
these clusters would satisfy some constraints, e.g., message
similarity constraints, remote coupling constraints, structure
coherence constraints, and dimension constraints. For each
constraint, We compute probabilities to serve as the degree
of compliance that we observe. With these probabilities, we
then perform probabilistic inference to derive the posterior
probability of random variables that denote our assumption,
i.e., the current ﬁeld is the keyword. After checking all ﬁelds,
we can pick the one with the highest probability as the
keyword, and use it to cluster messages. In the motivation
example, we generate 12 ﬁelds from the MSA results of client
messages, as shown in Figure 4a. After probabilistic inference,
ﬁeld f7 is chosen as the keyword with the highest posterior
probability. Then we can generate two correct clusters by the
values of f7, which is show in Figure 4b.
III. SYSTEM DESIGN
In this section, we discuss the system design, including
preprocessing, keyword ﬁeld candidate generation, probabilis-
tic keyword identiﬁcation, iterative alignment and clustering,
and format and state machine inference. Figure 5 shows the
workﬂow of NETPLIER.
A. Preprocessing
The input of NETPLIER is network traces which could be
captured by packet analyzers such as tcpdump. The packets in
traces follow the network layer models. The unknown proto-
cols we aim to reverse engineer are usually in the application
layer. Based on the knowledge of other existing protocols,
BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBTBBBBBBBBBBBBBBBBBBBBBBBBBTBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBTBBBBBBBBBBBBBBBBBBBBBBBB!"#!"$!"%!"&!"'!"(!")!"*PR: BBBBBBBBBBBBBBBBBPR: BTBBBBBBBBBBBBBBBBBBBBBBBB!"#!"$!"%!"&!"'!"(!")!"*Initial Cluster1Initial Cluster2!"#Cluster1!"$!"%!"&!"'!"(!")!"*Cluster2Cluster3Cluster405640A44030004007CAEE6F78210004FBD----------------------05640A44030004007CAEE7C18100003BDB----------------------05644C4403000400D86BCCF382000033010701E2437D87FF0002F8C305644744030004005462CDF482000033010701D5477D87FF0002495C05640A44030004007CAEC0F082900043A2----------------------05640A44030004007CAEC4C481800080A3----------------------05640A44030004007CAEC6C68100000A36----------------------05644E44030004006F4DC7C78100000101000005190A02000005C347!"#!"$!"%!"&!"'!"(!")!"*+#+$+%+&+'+(+)+*+,+-+$#+$$+$%!"#Cluster1!"$!"%!"&!"'!"(!")!"*Cluster2Fig. 5: System design
we can reconstruct messages in these protocols, extract useful
information (e.g., port numbers from the network layer), and
discard data in irrelevant protocols. Finally, we standardize
network messages to include the following information: times-
tamp, IP address(es), port number(s), and data of the target
protocol. An example of such standardized messages can be
found in Figure 1.
By timestamp, IP address, and port number, we can
group messages into communication sessions. For exam-
ple, there are three sessions in the example shown in Fig-
ure 1, where mc0, ms0 , ms1 , mc1 belong to the ﬁrst session,
mc2 , ms2, mc3, ms3 belong to the second session, and the other
messages belong to the third session. The session information
will be used in the probabilistic inference and state machine
inference stages.
B. Keyword Field Candidate Generation
As mentioned earlier, identifying keywords (in network
messages) is critical. In this stage, we identify a set of ﬁelds
that are candidates for keywords.
Message data is composed of multiple ﬁelds. For the
example in Figure 1, all messages have similar ﬁeld structures
and these ﬁelds are of the same length (except the last ﬁeld).
Hence we can easily acquire the value of a ﬁeld by its
position. However, for complex protocols, messages may have
different structures and some ﬁelds may have a variable length,
which makes a ﬁeld appear at different positions in different
messages. For example, messages in Figure 6a have a ﬁeld
for user name, which has a variable length. Intuitively, the
idea of recognizing such ﬁelds is to identify the ﬁxed length
ﬁelds that bound ﬁelds of a variable length, by message
alignment. We observe that messages tend to share some
common values, especially for ﬁxed length ﬁelds, e.g., “user”
and “age” in Figure 6a. Hence we can align messages to
expose such common sequences across messages, and then
identify the variable-length ﬁeld(s) in between them. If mul-
tiple consecutive variable-length ﬁelds are present in between
two bounding ﬁxed-length ﬁelds, NETPLIER may recognize
these variable-length ﬁelds as one monolithic variable-length
ﬁeld. In practice, we rarely see such cases. Note that this is a
generally hard problem for any trace based revere engineering
techniques to precisely separate them.
As discussed earlier, pairwise alignment algorithms are
widely used by existing methods. However, pairwise alignment
only compares two sequences at one time, which substantially
affects scalability when the number of samples is large. There-
fore, we leverage multiple sequence alignment [39], which is
5
(a) Original messages
(b) Alignment results
Fig. 6: Examples with variable-length ﬁelds
an extension of pairwise alignment in Bioinformatics and could
align all sequences at a time. There are various strategies used
to reduce computational complexity and improve accuracy for
multiple sequence alignment. Here, we use a combination
of progressive methods [39] and iterative reﬁnement [62].
Progressive methods align the most similar sequences ﬁrst and
then progressively add other sequences to the alignment re-
sults. Iterative reﬁnement methods iteratively realign sequence
subsets of initial global alignment results to improve the
accuracy. Figure 6b shows the result after multiple sequence
alignment. Gaps (i.e., ‘-’) are inserted into the variable-length
ﬁelds in order to demonstrate alignment results.
Based on the initial alignment results (on all messages), we
partition message data into ﬁelds. For text data, we can use
predeﬁned delimiters, such as a space character, to partition
message data into ﬁelds. However, binary data do not have
speciﬁc delimiters and its ﬁelds are usually a few bytes long.
We need to use the alignment results in a very conservative
way such that it considers every possible candidate of a ﬁeld.
First, we consider each (aligned) byte as a single unit ﬁeld. A
unit ﬁeld is marked as static if all message data have the same
value for the ﬁeld, otherwise dynamic. Then consecutive static
unit ﬁelds are merged to a larger unit ﬁeld. For example, in
Figure 4a, ﬁelds f0, f2, and f9 are static unit ﬁelds, and the
others are dynamic (this may not be true as we only show a
short snippet with some ﬁelds elided due to the space limita-
tion). These unit ﬁelds denote a conservative list of candidates
for real ﬁelds, meaning that a ﬁeld in the real speciﬁcation is
a unit ﬁeld or a concatenation of multiple unit ﬁelds. At the
end of this stage, we generate a list that includes all the unit
ﬁelds and their compositions that are shorter than a threshold
(i.e., 10 bytes in this paper). The compositions are also called
compound ﬁelds. The list denotes candidates for keyword ﬁelds
and is subject to the downstream probabilistic analysis. We
bound the size in order to reduce the number of candidate
ﬁelds to analyze. Note that in f12 we combine a sequence
of bytes as it is empty for some messages, which means it
could not be the keyword ﬁeld and could be ignored. Although
most protocols use similar formats for both client side and
server side, some protocols may have substantially different
ﬁeld structures. We hence generate ﬁelds for client side and
server side separately (while considering their correspondence
in probabilistic analysis). Note that although ﬁeld candidate
generation is not complex, it ought to be conservative and
include the real (keyword) ﬁelds. NETPLIER relies on the later
probabilistic analysis to recognize the keyword ﬁelds with high
accuracy, which in turn allows identifying the other ﬁelds and
pruning the bogus ones.
TracesIterativeAlignmentandClusteringM101011MessagesKeywordFieldCandidateGenerationFFieldsCClustersProbabilisticKeywordIdentificationClusteringFormatInferenceState-machineInferencePreprocessinguseraliceage20userbobage25usercarolage30!"!#!$useraliceage20user--bobage25usercarolage30!"!#!$C. Probabilistic Keyword Identiﬁcation
Given a list of keyword candidate ﬁelds for both sides,
we use a probabilistic method to infer which ﬁelds are most
likely the keywords. With keyword ﬁelds identiﬁed, messages
of the same type (i.e., having the same keyword value) can be
identiﬁed and further alignment and analysis can be performed
on these messages.
Let ﬁelds fc and fs be the potential keywords from the
client and the server sides, respectively. Client-side messages
are speculatively grouped into clusters (tc0, tc1, . . . ) by fc and
server-side messages are grouped to (ts0 , ts1 , . . . ) by fs. In
the example shown in Figure 1, the list of candidate ﬁelds
for client side messages are shown in Figure 4a. The server
side messages have a very similar list. Figure 7a shows the
clustering results of considering f1 the keyword for messages
on both the client and the server sides and Figure 7b shows
the results of considering f7 the keyword. For example, with
f1 the keyword, mc0, mc1, mc4, mc5, and mc6 belong to a
cluster as their f1 ﬁelds all have value A0, whereas ms0, ms2,
ms3, and ms4 belong to a cluster as their f1 values are 08
(see traces in Figure 1).
If the keyword speculation is true, i.e., the messages in
a cluster (grouped by the keyword values) are indeed of the
same type, we should have the following observations from
the generated clusters.
Observation 1. Messages in the same cluster should be more
similar than messages in different clusters.
Observation 2. Clusters on the client side and the server
side should have correspondence. In other words, messages
belonging to a cluster on one side (e.g., requests from the
client side) very likely have their counterparts on the other
side (e.g., corresponding responses from the server side) in a
cluster too.
Observation 3. Messages in the same cluster follow the same
ﬁeld structure.
Observation 4. There should not be too many clusters. In each
cluster, there should be enough number of messages.
These observations may have uncertainty. In other words,
true clusters may not demonstrate such observations and their
presence does not necessarily imply true clustering either.
Therefore, we introduce a random variable (with boolean
value) to indicate if a candidate is the true keyword. The
variables (for all the candidates from both client and server)
and the observations form a joint probability distribution. We
hence formulate keyword identiﬁcation as a probabilistic infer-
ence problem computing the marginal posterior probabilities of
keyword random variables given the observations. As we will
explain in Section IV, the inference rules may be directional
(i.e., Bayesian inference [27]) or un-directional (Markov ran-
dom ﬁelds [48]). We leverage a general graph model called
factor graph that supports both types. After inference, the
random variable with the largest posterior probability indicates
the most likely keyword pair.
D. Iterative Alignment and Clustering
MSA may not produce the intended alignment in the ﬁrst
place as it is inherently uncertain as well. As a result, the ﬁeld
(a) Clustering results of f1
(b) Clustering results of f7
Fig. 7: Clustering results of different ﬁelds
separation may be problematic, rendering erroneous down-
stream results. We resort to iterative alignment and clustering
to address the problem. Intuitively, assume MSA does not
align properly and hence the keyword cannot be correctly
identiﬁed. Nonetheless, the probabilistic inference and clus-
tering are likely to reduce structural divergence of messages
within clusters. As such, for each cluster, we perform MSA and
the probabilistic keyword identiﬁcation. We then compare the
resulted keywords with the original ones. If the new keywords
can lead to better global partitioning of all
the messages
(evaluated by metrics derived from the aforementioned four
observations), we replace the original keywords with the new
ones. The process repeats until no better keywords can be
identiﬁed. As shown in Section V, the strategy is particularly
effective for protocols that have substantial message length
variation such as DHCP.
E. Format and State Machine Inference
As discussed earlier, each message is split into several
aligned ﬁelds after multiple sequence alignment (e.g., Fig-
ure 4a). After iterative alignment and clustering, the format
for each type can be directly recovered by summarizing the
ﬁelds of all messages in the same cluster. The format includes
ﬁelds deﬁned with length (L), value (V ), and ﬁeld type (S:
static ﬁeld with a speciﬁc value; ’D’: dynamic ﬁeld with
a list of potential values). For example, in Figure 4a, ﬁeld
f0 can be denoted as S(V =(cid:48) 0504(cid:48)); ﬁeld f7 can be
D(L = 1, V = [(cid:48)82(cid:48),(cid:48) 81(cid:48)]), which is a dynamic ﬁeld with
two potential values; and ﬁeld f12 can be D(L = (0, 11)),
which is a variable-length ﬁeld or optional ﬁeld as it is empty
for some messages. New messages could be generated based
on the formats.
In addition, we make use of an existing technique [25] to
infer state machine. The technique works well when message
types are properly deﬁned. The basic idea is to derive message
type sequences for each session (in the traces) and aggregate
such sequences to form a state machine. Details are elided as
it is not our contribution.
Note that full format and state machine inference are not
the focus of this paper, which are only provided to evaluate
clustering results (Section V-C and Section V-D). More precise
6
clientmessages!"#$"%,$"#,$"',$"(,$")!"*$"*!"+$"+!"'$",servermessages!-#$-%,$-*,$-+,$-'!-*$-#,$-(!-+$-)!-'$-,clientmessages!"#$"%,$"',$"(,$")!"'$"#,$"*,$"+,$",servermessages!-#$-%,$-',$-(,$-)!-'$-#,$-*,$-+!-($-,inference could be generated if prior knowledge is used to
detect some common ﬁelds ﬁrst [26], [54], [69], e.g., length
ﬁeld or address ﬁeld. This is beyond the scope of this paper.
TABLE I: Predicate/random variable and constraint deﬁnition
Predicate Symbol
Deﬁnition
Related Constraints
IV. PROBABILISTIC KEYWORD IDENTIFICATION
A key step in our technique is to model uncertainty in
keyword identiﬁcation as a joint distribution of observations
and a set of random variables, each denoting if a candidate
ﬁeld is the keyword of messages. In this section, we discuss
the details of how to model the uncertainty with probabilities
and conduct probabilistic inference with a graphical model.