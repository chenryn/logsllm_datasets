---
## Page 162
熊猫爱中国www.xiongmaoi.com
熊猫爱中国
② 构建一个新的 memoryview对象，指向剩余将要发送的数据。
●构建一个字节对象，包含1亿多个字母a。
用 memoryview可以实现同样的功能而无需复制数据，也就是零复制。
②移除前面已经发送的字节。
0构建一个字节对象，包含1亿多个字母a。
时，它不会在一次调用中发送所有数据。下面是一个简单的实现：
②直接引用整块数据减去开始的1KB 的数据，因为我们不想将最开始的1KB数据写入目标文件中。没有
①从/dev/urandom读取10 MB的数据且没有太多其他操作。Python为此要分配约 10 MB的内存以将该
数据存储为字符串。
length%d"。
复制意味着没有额外的内存开销。
显然，通过这种机制，需要不断地复制数据，直到 socket 将所有数据发送完毕。而使
19.660MB
这段程序不会复制任何内容，不会使用额外的内存，也就是说只是像开始时那样要给变
while mv:
mv = memoryview(data)
s= socket.socket()
import socket
当处理socket（套接字）时这类技巧尤其有用。如你所知，
data =b"a"*(1024 * 100000)
s.connect(..)
while data:
data = b"a" *(1024 * 100000)
s.connect（.)
s = socket.socket(.)
import socket
19.672MB
19.672 MB
19.672MB
mv=mv[sent:]
sent =s.send(mv)
data = data[sent:]
sent = s.send(data)
0.000MB
0.000 MB
0.012
0.000MB
MB
本电子书仅限学习交流使用，请勿用于商业用途
target.write(content_to_write) 
with open("/dev/null", "wb") as target:
 print("Content length: %d, content to write
(len(content), len(content_to_write)))
10.7
通过缓冲区协议实现零复制
：当数据通过socket发送
155
---
## Page 163
熊猫爱中国www.xiongmaoi.com
熊猫爱中国的调用次数）并在需要时进行填充。使用 memoryview甚至可以在内存区域的任何点放入
② 将/dev/urandom的内容写入bytearray中从偏移索引4到结尾的位置，精确且高效地只读了4字节。
●引用bytearray，从其偏移索引4 到其结尾。
数据。
让IO函数写入预分配的对象。
可以从这类对象中读，还可以向其中写。在这个例子中不需要 memoryview 对象，只需要
在读数据时。Python中的大部分I/O操作知道如何处理实现了缓冲区协议的对象。它们不仅
量分配100 MB内存。
要零复制时都能高效完成。
array模块中的对象以及 struct 模块中的函数都能正确地处理缓冲区协议，因此在需
提示
156
bytearray(b'\x00\x00\x00\x00\x0b)x19\xae\xb2')
利用这一技术，很容易预先分配一个缓冲区（就像在C语言中一样，以减少对malloc（）
前面已经看到了将 memoryview对象用于高效地写数据的场景，
>>>ba
4
:aoinos se (.qz '"wopuexn/Aap/) uado uatm >>ba
8
aoanos se (uq" '"wopuein/aap/")uado uatm >>ba
>>> ba = bytearray(8)
第10章性能与优化
source.readinto(ba)
本电子书仅限学习交流使用，请勿用于商业用途
，同样的方法也可以用
---
## Page 164
熊猫爱中国www.xiongmaoi.com
熊猫爱中国
台的代码实现也可能会不同，尽管这是应该尽量避免的。
思。例如，函数realloc（）的性能在Linux和Windows 上是不同的。有时候针对不同的平
升应该至少达到20%或者25%。
较短的函数的基准测试通常称为“微基准测试”。通过微基准测试衡量优化效果时，速度提
成熟的优化。无用的优化可能使代码更糟，更不易懂，甚至更慢。有用的优化必须至少让程
稳定可重现的基准。没有可靠基准的情况下尝试不同的优化方法很可能导致时间的浪费和不
用于在Python 中跟踪内存块的分配，并写了一个简单的AST 优化器。
454（https://www.python.org/dev/peps/pep-0454/)，其中提出了一个新的tracema11oc模块,
序加速5%。
10.8Victor Stinner 访谈
Python 3.3 提供了一个新的 time.perf_counter（）函数，用来为基准测试衡量已耗
关于Python代码的性能分析和优化有许多不同的工具，你最喜欢的是哪个？
在不同的计算机、不同的操作系统甚至不同的编译器上运行同一个基准测试会很有意
如果发现代码的某个部分比较“慢”，那么需要针对这段代码设计一个基准测试。对于
针对 Python 的策略其实和在其他语言中一样。首先需要定义良好的用例，以得到一个
优化Python代码的一个初步策略是什么？
Victor 是资深的 Python 黑客，许多Python 模块的核心贡献者和作者。他最近撰写了 PEP
本电子书仅限学习交流使用，请勿用于商业用途
10.8Victor Stinner访谈
157
---
## Page 165
熊猫爱中国www.xiongmaoi.com
熊猫爱中国
步代码。Twisted、Tormado 和 Tulip 都是利用了这一技术的面向网络的库。
性能瓶颈。
时有影响。如果大多数处理时间花在函数调用上，并且这些函数释放了GIL，那么GIL 并非
两个线程不能在同时执行 Python 字节码。然而，这个限制只在两个线程执行纯 Python 代码
用C语言重写短函数以获得更好的性能。
找到了缓慢的函数时，应该修改算法。如果算法和容器都是经过仔细挑选的，那么可以考虑
准测试。
实际上，
提升会丧失代码的可读性。
不同的容器，如 dict、list、deque、set 等。
是用C 实现的，所以性能都很好。应使用正确的容器以得到最佳的性能，Python 提供许多
Python 提供了cProfile 和用来记录每个函数时间消耗的 profile模块。
不稳定。应该手工重复测试，以得到稳定的结果。
缓存。我倾向于保证最小时间，其他一些开发人员则倾向于使用几何平均值。
用时间。它是最好的解决方案。
158
multiprocessing 模块可以很容易地用来绕过 GIL。另一个稍微复杂的方式是编写异
CPython的一个众所周知的性能瓶颈是全局解释器锁（GlobalInterpreterLock，GIL）。
通常，在开发新的应用程序时我不太担心性能问题。不成熟的优化是所有问题之源。当
在哪些领域中 Python 的性能很差？哪些领域中应该小心使用？
Python之禅（PEP20）说：“应该有一种一
也有一些用来优化 Python 的非常手段，但是应该避免使用它们，因为这一点点的速度
能够改进性能的最有意思的Python技巧是什么？
优化是非常花时间的，所以最好能专注那些耗费最多CPU的函数。为了找到这些函数，
对于微基准测试，timeit模块简单易用且能很快得到结果，但使用默认的参数结果并
测试应该运行不止一次，最少3次，5次基本够了。重复测试可以填充磁盘缓存和CPU
应该尽可能重用标准库。它们经过良好的测试并且通常都很高效。Python 内置的类型都
第10章性能与优化
，写 Python 代码有很多不同的方式，且性能各异，所以只能信赖针对特定用例的基
。例如，大多数I/O 函数都会释放GIL。
本电子书仅限学习交流使用，请勿用于商业用途
一最好只有一种——显而易见的方式去实现。”
---
## Page 166
熊猫爱中国www.xiongmaoi.com
熊猫爱中国
响。对于更多元素的场景，应该了解每个操作（add、get、delete）的复杂性和影响。
地使用了copy.deepcopy（）。
另一个性能杀手是低效的数据结构。少于100个元素的情况下，容器类型对性能没有影
没有很好地理解Python 就可能写出效率低的代码。例如，我见过在不需要复制时错误
你见过最多的导致性能差的“错误”是什么？
本电子书仅限学习交流使用，请勿用于商业用途
10.8VictorStinner访谈
159
---
## Page 167
熊猫爱中国www.xiongmaoi.com
熊猫爱中国
本电子书仅限学习交流使用，请勿用于商业用途
---
## Page 168
熊猫爱中国www.xiongmaoi.com
熊猫爱中国
①或者如果多个CPU不存在的话，依次在某一个处理器上。
需要分散工作负载时，只需要为新的请求启动一个新线程而不用一次只能处理一个。
主循环需要等待对事件的响应。
味着代码的不同部分可以并行运行。
11.1
服务架构）可以产生更好的效果。
选择。如你所见，有一些范例（如多线程）在 Python 中被误用，而其他一些技术（如面向
CPython）有关。
要考虑的东西。这个问题的很多方面并非 Python 本身特有的，许多知识和其主要实现（即
第11章一
不，很抱歉！首先，如果你已经在 Python 领域中混了很久，那么你肯定遇到过GIL 这
太棒了，搞定！让我们继续。
所以，刚开始要解决这些问题的话，多线程似乎是扩展和并行化应用程序的好办法。当
（1）需要运行后台任务但不希望停止主线程的执行。例如，在图形用户界面场景下，
什么是多线程？它是在一个Python进程中将代码运行在不同的处理器上?的能力。这意
为什么需要多线程呢？最常见的场景如下。
一个应用程序的可扩展性、并发性和并行性在很大程度上取决于它的初始架构和设计的
如今所有的炒作都是有关灵活性和可扩展性的，因此我假设这是你的开发流程中早晚都
（2）需要将工作负载分布在几个CPU上。
多线程笔记
扩展与架构
本电子书仅限学习交流使用，请勿用于商业用途
---
## Page 169
熊猫爱中国www.xiongmaoi.com
熊猫爱中画PyPy 是另一个 Python 实现，但是是使用 Python 开发的（参见 10.6 节）。PyPy 也有 GIL
号量、计时器和文件描述符活动来访问事件，我们将在11.3节中进行讨论。
这一概念构建的，如 Twisted（http://twistedmatrix.com/trac/）。最高级的框架应该提供基于信
（https://www.python.org/dev/peps/pep-3156/）中标准化这一功能的成果。有些框架就是基于
的Python 模块都提供这一机制，甚至有一个标准库中的模块—asyncore，它是PEP3156
可用。
上并不能作为目标平台来使用。
可以有效地并行运行多个线程。遗憾的是，这些项目相对于CPython 都非常滞后，所以实际
就没有全局解释锁（http://www.jython.org/jythonbook/en/1.0/Concurrency.html），这意味着它
得不值得这么做。
少有2个或4个核，这是很没面子的。这都归咎于GIL。
难获取到150%的CPU利用率，也就是使用1.5个核（core）。考虑到现如今计算节点通常至
展应用程序，将总是被这个全局锁所限制。
CPython?每次要执行字节码时都要先申请这个锁。但是，这意味着，如果试图通过多线程扩
个词，而且知道它多么讨厌。GIL 是指Python 全局解释锁（Global Interpreter Lock），当
内核的开发者也在寻求废弃内核锁的方法。这些都是积极的信号。
和运行多线程软件是非常值得期待的变化。某些处理器正在试图提供硬件支持，而 Linux
http://www.jython.org/jythonbook/en/1.0/Concurrency.html）的实现替换它。这对于未来构建
但目前有一个非常有意思的工作正在试图用基于 STM（ Software Transactional Memory，
注意
162
）用C语言开发的 Python参考实现，也就是通常在 shell中输入 python之后运行的那个。
尽管是使用最普遍的。
（1）如果需要运行后台任务，最容易的方式是基于事件循环构建应用程序。许多不同
没有好的方案是不是我们又回到了最初的场景呢？并非如此，至少还有以下两种方案
目前没有任何工作试图从CPython 中移除GIL，因为考虑到实现和维护的难度大家都觉
所以尽管多线程看上去是一个理想的解决方案，但实际上我看到的大多数应用程序都很
（2）如果需要分散工作负载，使用多进程会更简单有效，参见11.2节。
第11章扩展与架构
本电子书仅限学习交流使用，请勿用于商业用途
---
## Page 170
熊猫爱中国www.xiongmaoi.com
熊获礼中。
在于它会启动一个新的进程（通过fork（2））而不是一个新的系统线程。
11.2
考虑到通常能够获得的好处很少，所以最好不要在多线程上浪费太多精力。
或者多进程的方式来做，也就不用再担心这个问题了。
便，但我很快便掉进了并发陷阱中。如果有机会再做一次的话，我会使用基于异步事件处理
多年前写的一个做Debian 构建（build）的守护进程。尽管用线程去控制每个构建作业很方
rebuild（htp:/julien.danjou.info/projects/rebuildd）中使用多线程来分发作业，rebuild是我
print("Results:%s"&results)
使用多线程的worker
下面是一个简单的例子，计算100万个随机整数的和8次，同时将其分散到8个线
处理好多线程是很难的。其复杂程度意味着与其他方式相比它是bug 的更大来源，而且
程序的运行结果如示例11.1所示。
for worker in workers:
for worker in workers:
workers = [threading.Thread(target=compute) for x in range(8)]
def compute():
results =[]
import threading
import random
正如前面解释的，因为GIL的问题，多线程并非好的可扩展性方案。更好的方案是Python
对于我们这些开发人员、普通人来说，这意味着我们在使用多线程时要三思。我在
worker.join()
worker.start()
results.append(sum(
多进程与多线程
[random.randint(1,100) for i in range(1000000)]))
本电子书仅限学习交流使用，请勿用于商业用途