approaches outlined in § 4.3 other than code-signing, which we
anticipate adding in the near future. While Firefox already sup-
ports signing JavaScript code [38], we have opted not to employ the
mechanism directly in Fathom, due to technical shortcomings.7 We
do, however, follow Firefox’s approach of signing the JavaScript
code of an entire web page, including scripts it pulls in additionally,
which provides protection against runtime page modiﬁcations.
Controlling resource consumption: Our design incorporates
7Firefox requires bundling the required ﬁles into a JAR ﬁle, re-
quires a jar: protocol handler for the download, insists on spe-
ciﬁc Content-Type header MIME values, and uses a rather sim-
plistic dialog to convey signer identity to the user.
FathomFirefoxfathom object& APIsDOMRenderingEngineJavaScript EngineNetworkEngineNSPRXPCOMjs-ctypesSocket workersFathomScriptsActive/passivemeasurementscriptsNetworkStorageShellWebpageFathomrecv()post()post()callback()udp.recv()NSPRPR_Recv()MainThreadSocketWorker79tracking Fathom’s resource usage—e.g., number of open connec-
tions or amount of data transmitted and received in the last time
unit. Currently we do not restrict resource usage, which is con-
sistent with other runtime plugins such as Flash and Java. As we
accrue experience with Fathom we may decide exposing resource
usage policy to users and/or developers as it becomes necessary.
In the current implementation, if a user judges that a given experi-
ment is overloading her system, she can simply close the web page.
Given how simple it is for users to stop measurement pages, exper-
imenters have a strong incentive to use system’s resources consci-
entiously.
6 Performance Evaluation
We evaluate Fathom’s performance using two metrics—overhead
on web browsing and timing accuracy. We conduct all experiments
with Firefox v7.0.1 on a 2.20GHz Intel Core2 Duo machine with
2GB RAM, running Ubuntu 10.04, and connected to a 100 Mbps
local network.
6.1 Overhead
We evaluate Fathom’s page load overhead for eight popular web
sites (CNN, Craigslist, ESPN, Google Maps, New York Times,
Slashdot, Yahoo! and YouTube) with diverse JavaScript complex-
ity and number of HTTP requests. For each site, we compare
the time interval between initiating a page load and the ﬁring of
the Load event in a Fathom-enhanced browser against a normal
browser. The Load event ﬁres at the end of the page loading
process when all page components have rendered. To obtain the
ground truth, we enhance the browser with a lightweight extension
that captures only the page load times. We also compare Fathom’s
overhead with that of Firebug [36], a popular browser extension to
debug web page performance. Firebug performs extensive diag-
nostic measurements, which increase the page load time. We use
Firebug as a reference and not as a direct comparison.
We perform 50 page loads for each benchmark and record the
timestamp values for the Load event and the clock-time spent
within Fathom’s code for collecting the metrics. To measure the
worst case performance overhead Fathom imposes, we conduct all
experiments on a browser with a warm cache. Table 4 shows the
average absolute load times and Fathom’s relative overhead for the
benchmarks. The count of HTTP requests, scripts, and images in-
dicates the range between the minimum and the maximum number
of requests of each type until the page ﬁnishes loading. Requests
that start after the page has loaded are not included in these num-
bers, as Fathom’s activity for such requests will not affect the load
time.
Overall, Fathom imposes 1–3% overhead for all the benchmarks.
We note that as the number of HTTP requests increase, the average
execution time within Fathom also increases. We expect this behav-
ior, because Fathom observes all HTTP activity to collect passive
measurements. We also observe that with the increase of HTTP re-
quests, Fathom does not always incur a corresponding increase in
the absolute overhead. This is because Fathom’s overhead is not
additive. Since we implement Fathom in JavaScript, its execution
will only block the single-threaded JavaScript interpreter, while the
rest of the browser can continue loading the page. Thus, Fathom’s
overhead gets masked in part by network latency and the browser’s
parallelism during the document loading process.
6.2 Accuracy
Network measurement platforms require accurate timestamps and
timers. Fathom leverages the browser’s JavaScript API to record
timestamps and the XPCOM API to implement a timer. As with all
No browsing
Stdev.
Avg.
0.29
0.52
0.41
0.33
Passive browsing
Stdev.
Avg.
0.28
0.54
0.50
0.75
Active browsing
Stdev.
Avg.
0.39
0.52
0.54
0.89
Send (ms)
Recv (ms)
Table 5: Difference in timestamps from Fathom and tcpdump
for sent and received packets under different browsing condi-
tions.
measurement, competing activity (browser, host and network) can
affect accuracy of timestamps and ﬁring of timers.
Timestamp accuracy: We measure the accuracy of Fathom’s time-
stamps by sending probe packets between two machines on the
same local network and recording timestamps for those packets in
Fathom and via packet capture with tcpdump on the same ma-
chine. Since the timestamps from Fathom and tcpdump stem
from the same clock, the difference between the timestamps for
each packet are due to the time required to traverse the network
stack between the interface and the browser.
We test timestamp accuracy under different browsing conditions
with a train of 106 probe packets with a payload of 200 bytes each.
In the “no browsing” scenario, there is no other browsing activ-
ity in progress. For the “passive browsing” scenario, we load four
web sites (CNN, Google Maps, NYTimes and YouTube) in separate
browser tabs and send the probe train after the sites ﬁnish loading.
For the “active browsing” scenario, we send the probe train while
simultaneously loading the same four sites in other tabs.
Table 5 shows the results of the experiment. The “send” row
indicates the difference in timestamps from the time Fathom dis-
patches the probe to the resulting timestamp from tcpdump. Sim-
ilarly, the “recv” row indicates the delay from the timestamp given
by tcpdump to its arrival in Fathom.
In all the browsing sce-
narios we ﬁnd the accuracy of the timestamps in Fathom—which
inherit the XPCOM granularity of 1 ms—to be within 1 ms of the
tcpdump reported event time. Further, the standard deviation for
all experiments is also below 1 ms. In the passive and active brows-
ing scenarios, Fathom’s receive timestamp accuracy remains lower
than in the “no browsing” case, but the accuracy is still less than
1 ms.
We next explore Fathom’s timestamp accuracy under heavy
cross-trafﬁc on the host. We repeat the experiment for 10 probe
trains of 10 packets while performing an iperf [21] transfer with 4
TCP connections between two hosts on the same local network. In
this experiment, the average difference in the “send” timestamps in-
creases to 729 ms. This test represents a worst-case scenario when
users fully utilize their network; the heavy concurrent network traf-
ﬁc from iperf completely ﬁlls the egress queue, which prevents
timely transmission of Fathom-generated probes.
Timer: We measure the accuracy of Fathom’s timer by testing how
precisely it can dispatch 200-byte probes at 200 ms intervals. We
use the timer to send 51 probe trains of 100 probes each and mea-
sure the average (µ) and standard deviation (σ) for the observed
intervals between consecutive probes. Since the accuracy for XP-
COM timers is 1 ms, a σ of more than 1 ms would mean that the
timer loses precision. We study timer accuracy in four scenarios:
(i) no browsing, (ii) passive browsing, as described above (and for
which asynchronous activity due to both ads and scripts can lead to
concurrent activity), (iii) active browsing, as described above, and
(iv) CPU stress case, where we emulate a competing process in the
host that overloads the CPU.
For the no-browsing and stress tests, we ﬁnd the mean timer in-
terval to be within 1 ms of the expected value of 200 ms. Further,
for both cases the 5th percentile was 199 ms and the 95th per-
80Benchmark
Craigslist
YouTube
Google Maps
Yahoo
Slashdot
ESPN
CNN
NY Times
# HTTP
# Scripts
# Images
4
28–34
39–40
49–58
35–60
85–89
89–104
115–125
2
2–3
7–8
4–5
2–12
7–8
6–9
23–26
0
17–23
26–28
31–43
22–30
61–63
22–59
77–80
Avg. load
time (ms)
512
869
1233
1168
2381
1761
1458
1144
Avg. Fathom
load time (ms)
517
896
1268
1180
2414
1805
1504
1159
Avg. Firebug
load time (ms)
532
957
1585
1747
4083
2325
2263
1798
Avg. execution time
within Fathom (ms)
9
37
60
42
51
96
174
91
Avg. Fathom
overhead(%)
1.1
3.1
2.9
1.0
1.4
2.5
3.1
1.3
Avg. Firebug
overhead (%)
4
10
29
50
71
32
55
57
Table 4: Average page load overhead of Fathom for popular web pages over 50 runs.
centile was 201 ms. For the passive and active scenarios, we use
10 probe trains of size 10, with probes sent at 200 ms intervals. In
the passive-browsing scenario, the average lies within 2 ms of the
expected, the 5th percentile was 1 ms, and the 95th percentile was
267 ms. For the active browsing scenario, the average lies within
33 ms of the expected, the 5th percentile was 12 ms, and the 95th
percentile was 669 ms. This large drop in accuracy compared to the
no-browsing and stress experiments occurs because multiple tabs in
the same browser window share the same browser runtime.
To sum up, Fathom often achieves 1 ms timestamp accuracy,
but the accuracy of timestamping sent packets degrades under both
heavy network trafﬁc and concurrent tabs within the browser. The
degradation in timing accuracy due to competing activity is a well-
known difﬁculty when measurements are running in shared envi-
ronments like end-systems. For example, it is well-known that
measurements running in PlanetLab will suffer due to concurrent
activity [48]. Measurement scripts that use Fathom should be aware
of these issues and perform extra tests to verify the accuracy of their
experiments. For example, scripts can invoke Fathom APIs to re-
quest CPU load and network cross-trafﬁc on the host to test existing
operating conditions.
7 Case Studies
In this section, we demonstrate Fathom’s utility by implementing
three example usage cases that together exercise different aspects
of its API: providing a JavaScript version of ICSI’s Netalyzr test-
suite, debugging web access failures, and enabling Google Maps to
diagnose poor web page performance. Our goal is to illustrate how
Fathom can support disparate measurement needs.
7.1 Netalyzr
We test Fathom’s suitability as a platform for conducting a wide
range of active measurements by reproducing many of the tests
present in our Netalyzr testsuite [28]. Netalyzr relies on a Java
applet for its client-side tests, so supporting this range of measure-
ments can serve as a litmus test for Fathom’s ﬂexibility. We devel-
oped a Fathom-powered JavaScript version of the suite,8 focusing
in particular on tests not feasible in standard JavaScript.
API requirements: In the following, we consider Netalyzr’s full
set of test categories and discuss Fathom’s signiﬁcance in the tests.
This category primarily investigates
whether the client resides behind a NAT, and if so, how the NAT
renumbers the client-side address and port numbers. It requires the
equivalent of getsockname(), i.e., access to the local address
and port of a socket. Standard JavaScript does not provide these,
but Fathom’s TCP APIs do.
Address-based tests:
UDP and TCP port reachability: Tests in this category measures
the extent to which the client can communicate freely on approx-
imately 30 ports used by standard protocols. For TCP, standard
JavaScript APIs can approximate these tests somewhat clumsily,
8http://netalyzr.fathom.icsi.berkeley.edu
while for UDP no mechanisms exist. Fathom provides raw TCP
and UDP access, making these tests easy.
Access link properties: Netalyzr leverages Java’s raw UDP
access to implement basic latency testing, upstream/downstream