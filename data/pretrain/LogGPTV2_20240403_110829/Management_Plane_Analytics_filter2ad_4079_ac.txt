network health requires accounting for many other practices; we
next discuss how we achieve this.
5.2 Causal Analysis
Although we can select the k practices with the highest MI as the
top k management practices associated with network health, there
is no guarantee these practices actually impact health. To establish
a causal relationship between a management practice and network
health, we must eliminate the effects of confounding factors (i.e.,
other practices) that impact this practice and network health [18].
Figures 4 and 5, discussed earlier, illustrate such an effect.
Ideally, we would eliminate confounding factors and establish
causality using a true randomized experiment.
In particular, we
would ask operators to employ a speciﬁc practice (e.g., decrease
the number of device models) in a randomly selected subset of
networks; we would then compare the network health (outcome)
across the selected (treated) and remaining (untreated) networks.
Unfortunately, conducting such experiments takes time (on the or-
der of months), and requires operator compliance to obtain mean-
ingful results. Moreover, true experiments ignore already available
historical network data.
To overcome these issues, we use quasi-experimental design
(QED) [30]. QED uses existing network data to afﬁrm that an
independent (or treatment) variable X has a causal impact on a
dependent (or outcome) variable Y .
5.2.1 Matched Design
We use a speciﬁc type of QED called the matched design [33].
The basic idea is to pair cases—each case represents a network in
a speciﬁc month—that have equal (or similar) values for all con-
founding variables Z1...Zn, but different values for the treatment
variable X. Keeping the confounding variables equal negates the
effects of other practices on the outcome (network health), and in-
creases our conﬁdence that any difference in outcomes between the
paired cases must be due to the treatment (practice under study).
Using a matched design to identify a causal relationship between
a management practice and network health entails four key steps:
(1) determine the practice metric values that represent treated and
untreated; (2) match pairs of treated and untreated cases based on
a set of confounding factors, a distance measure, and a pairing
method; (3) verify the quality of the matches to ensure the effect of
confounding practices is adequately accounted for; and (4) analyze
the statistical signiﬁcance of differences in outcomes between the
treated and untreated cases to determine if there is enough support
for a causal relationship.
A key challenge we face in using a matched design is obtaining a
sufﬁcient number of quality matches to provide an adequate foun-
dation for comparing the outcomes between treated and untreated
cases. As shown in Appendix A, practices tend to vary signiﬁ-
cantly across networks. Furthermore, many management practices
are statistically dependent with network health and each other (Sec-
tion 5.1). We use nearest neighbor matching based on propensity
scores [33] to partially addresses this challenge, but there are also
fundamental limitations imposed by the size of our datasets.
We now describe the analysis steps in more detail, using number
of change events as an example management practice for which
we want to establish a causal relationship with network health. At
the end of the section, we present results for the 10 management
practices that have the highest statistical dependence with network
health for the OSP (Table 3).
5.2.2 Determining the Treatment
While most other studies that use QEDs (e.g., those in the med-
ical and social sciences) have a clear deﬁnition of what constitutes
“treatment,” there is no obvious, deﬁnitive choice for most manage-
ment practices. The majority of our management practice metrics
have an (unbounded) range of values, with no standard for what
constitutes a “normal range”: e.g., for the OSP’s networks, the aver-
age number of change events per month ranges from 0 to hundreds
(Figure 12(e)). Hence, we must decide what value(s) constitute
treated and untreated.
One option is to deﬁne untreated as the practice metric value
that represents the absence of operational actions (e.g., no change
events), or the minimum possible number of entities (e.g., one de-
vice model or one VLAN). However, we ﬁnd it is often the case
that: (i) several confounding practices will also have the value 0
or 1 (or be undeﬁned) when the treatment practice has the value 0
or 1—e.g., when number of change events is 0, number of change
types, average devices changed per event, and fraction of events
with a change of type T are undeﬁned; and (ii) several confound-
ing practices will be non-zero (or >1) when the treatment practice
is non-zero. This observation makes sense, given that our CMI
results showed a strong statistical dependence between many man-
agement practices (Table 4). Unfortunately, it makes it difﬁcult to
ﬁnd treated cases with similar confounding practices that can be
paired with the untreated cases.
401Comp.
Point
1:2
2:3
3:4
4:5
Untreated
Cases
8259
1745
626
296
Pairs
Treated
Cases
1745 1742
614
295
673
616
296
783
Untreated
Matched
1109
431
200
174
Abs. Std. Diff.
of Means
Ratio
of Var.
0.0000 1.0091
-0.0002 1.0314
0.0052 1.0744
-0.0002 1.0411
Table 5: Matching based on propensity scores
Given the absence of a “normal range,” and the strong statistical
dependence between practices, we choose to use multiple deﬁni-
tions of treated and untreated and conduct multiple causal analyses.
In particular, we use the same binning strategy discussed in Sec-
tion 5.1 to divide cases into 5 bins based on the value of the treat-
ment practice. Then we select one bin (b) to represent untreated,
and a neighboring bin (b + 1) to represent treated. This gives us
four points of comparison: bin 1 (untreated) vs. bin 2 (treated), 2
vs. 3, 3 vs. 4, and 4 vs. 5; we denote these experimental setups
as 1:2, 2:3, 3:4, and 4:5, respectively. More (or fewer) bins can be
used if we have an (in)sufﬁcient number of cases in each bin. In
Section 5.2.4, we discuss how to evaluate the quality of matches,
which can help determine whether more (fewer) bins can be used.
5.2.3 Matching Pairs of Cases
Matching each treated case with an untreated case is the next
step in the causal analysis process. For our causal conclusions to
be valid, we must carefully select the confounding factors, distance
measure, and pairing method used in the matching process.
During the matching process, it is important to consider all prac-
tices (except the treatment practice) that may be related to the treat-
ment or outcome. Excluding a potentially important confounding
practice can signiﬁcantly compromise the validity of the causal
conclusion, while including practices that are actually unassociated
with the outcome imposes no harm—assuming a sufﬁciently large
sample size and/or a suitable measure of closeness [32]. There-
fore, we include all 28 of the practice metrics we infer, minus the
treatment practice, as confounding factors.
One caveat of including many confounding practices is that it be-
comes difﬁcult to obtain many exact matches—pairs of cases where
both cases have the exact same values for all confounding prac-
tices. For example, exact matching produces at most 17 pairs (out
of ≈11K cases) when number of change events is the treatment
practice. The same issue exists when matching based on Maha-
lanobis distance [29].
We overcome this challenge using propensity scores. A propen-
sity score measures the probability of a case receiving treatment
(e.g., having a speciﬁc number of models) given the observed con-
founding practices (e.g., number of roles) for that case [33]. By
comparing cases that have the same propensity scores—i.e., an
equally likely probability of being treated based on the observed
confounding practices—we can be conﬁdent that the actual pres-
ence or absence of treatment is not determined by the confounding
practices. In other words, a treated case and an untreated case with
the same propensity score have the same probability of having a
given value for a confounding practice (e.g., number of roles); thus
propensity score matching mimics a randomized experiment.
Given propensity scores for all treated and untreated cases, we
use the most common, and simplest, pairing method: k=1 near-
est neighbor [32]. Each treated case is paired with an untreated
case that results in the smallest absolute difference in their propen-
sity scores. To obtain the best possible pairings, we match with
replacement—i.e., allow multiple treated cases to be paired with
the same untreated case. We also follow the common practice of
discarding treated (untreated) cases whose propensity score falls
outside the range of propensity scores for untreated (treated) cases.
1:2
2:3
3:4
4:5
0.5
0.4
0.3
0.2
0.1
s
e
s
a
C
f
o
n
o
i
t
c
a
r
F
0.5
0.4
0.3
0.2
0.1
s
e
s
a
C
f
o
n
o
i
t
c
a
r
F
1:2
2:3
3:4
4:5
0.0
0
O(100)
0.0
0
O(100)
No. of devices
No. of VLANs
Figure 7: Visual equivalence of confounding practice distribu-
tions; lines of the same color are for the same comparison point;
solid lines are for matched untreated cases and dashed lines are
for matched untreated cases
Table 5 shows the matching results for each of the four com-
parison points for number of change events. There are signiﬁcantly
more matched pairs using propensity scores: up to 99.8% of treated
cases are matched, versus <1% with exact matching. Furthermore,
the number of untreated cases that are matched with treated cases
is less than the number of matched pairs, implying that matching
with replacement is beneﬁcial.
5.2.4 Verifying the Quality of Matches
When matching based on propensity scores, rather than the raw
values of confounding practices, it is important to verify that the
distribution of values for each confounding practice is similar for
both the matched treated cases and the matched untreated cases.
Otherwise, the effects of confounding practices have not been suc-
cessfully mitigated, and any causal conclusions drawn from the
matched pairs may not be valid.
σT
¯ZT − ¯ZU
Figure 7 visually conﬁrms the distribution equivalence for two of
the confounding practices. However, to facilitate bulk comparison,
we use two common numeric measures of balance: standardized
difference of means and ratio of variances [32]. The former is com-
, where ¯ZT and ¯ZU are the means of a confound-
puted as
ing practice Z for the matched treated and matched untreated cases,
respectively, and σT and σU are the standard deviations. The ratio
of variances is computed as σ2
U . For each confounding prac-
tice, the absolute standardized difference of means should be less
than 0.25 and the variance ratio should be between 0.5 and 2 [32].
These equations and thresholds also apply to the propensity scores
for the matched cases.
T /σ2
As shown in Table 5, the absolute standard difference of means
and the ratio of variances of the propensity scores satisfy the quality
thresholds for all comparison points. The same also holds for all
confounding factors (not shown).
Although we consider a large set of management practices in our
causal analysis, it is possible that other practices or factors also con-
tribute to the observed outcomes. We can easily incorporate new
practices into our propensity scores as we learn about them. Addi-
tionally, our matching based on propensity scores introduces some
randomness that can help mitigate the effects of any unaccounted
for factors. However, we can never deﬁnitely prove causality with
QEDs [21]; any causal relationships identiﬁed by MPA should thus
be viewed as “highly-likely” rather than “guaranteed”.
5.2.5 Analyze the Statistical Signiﬁcance
The ﬁnal step is to analyze the statistical signiﬁcance of the dif-
ference in outcomes between the matched treated and untreated
cases. For each matched pair, we compute the difference in out-
come (number of tickets) between the treated and untreated case:
yt − yu. If the result is positive (negative), then the treatment prac-
402Comparison
Point
1:2
2:3
3:4
4:5
Fewer
Tickets
562
251
110
282
No
Effect
350
61
25
38
More
Tickets
p-value
830 6.80×10−13
3.34×10−2
302
2.80×10−3
160
1.63×10−2
343
Table 6: Statistical signiﬁcance of outcomes; causality is
deemed to exist for higlighted comparison points
tice has led to worse (better) network health; if the result is zero,
then the practice has not impacted health. We use the outcome cal-
culations from all pairs to produce a binomial distribution of out-
comes: more tickets (+1) or fewer tickets (-1). Table 6 shows the
distribution for the matched pairs at each comparison point.
If the treatment practice impacts network health, we expect the
median of the distribution to be non-zero. Thus, to establish a
causal relationship, we must reject the null hypothesis H0 that
the median outcome is zero. We use the sign test to compute a
p-value—the probability that H0 is consistent with the observed
results. Crucially, the sign test makes few assumptions about the
nature of the distribution, and it has been shown to be well-suited
for evaluating matched design experiments [15]. We choose a mod-
erately conservative threshold of 0.001 for rejecting H0.
Table 6 shows the p-value produced by the sign test for each of
the comparison points for number of change events. We observe
that the p-value is less than our threshold for the 1:2 comparison
point. Hence, the difference in the number of change events be-
tween bins 1 and 2 is statistically signiﬁcant, and a causal impact
on network health exists at these values.
In contrast, the results
for the other comparison points (2:3, 3:4, and 4:5) are not statis-
tically signiﬁcant. This is due to either the absence of a causal
relationship—i.e., increasing the number of change events beyond a
certain level does not cause an increase in the number of tickets—or
an insufﬁcient number of samples. We believe the latter applies for
our data, because there is at least some evidence of a non-zero me-
dian: the number of cases with more tickets is at least 20% higher
than the number of cases with fewer tickets for the 2:3, 3:4, and
4:5 comparison points.
5.2.6 Results for the OSP
We now conduct a causal analysis for the 10 management prac-
tices with the highest statistical dependence with network health
(Table 3). Due to skew in our data, we can only draw meaningful
conclusions for low values of our practice metrics (bins 1 and 2).
Table 7 shows the p-value for the comparison between the ﬁrst
and second bin for each practice. We observe that 8 of the 10 prac-
tices have a causal relationship according to our p-value thresh-
old.
In fact, the p-values for these practices are well below our
chosen threshold (0.001). Furthermore, several of the practices
with a causal relationship, including number of devices and aver-
age devices changed per event, are practices for which operators
had mixed opinions regarding their impact (Figure 2). Our analysis
also matches the prevailing opinion that number of change events
has a high impact on health, and, to some extent, discredits the be-
lief that the fraction of events with ACL changes has low impact.
For the remaining two metrics, intra-device complexity and frac-
tion of events with an interface change, there is not enough ev-
idence to support a causal relationship. The high statistical de-
pendence but lack of a causal relationship is likely due to these
practices being affected by other practices which do have a causal
relationship with network health. For example, number of VLANs
has a causal relationship with network health and may inﬂuence
intra-device complexity. Hence, a change in number of VLANs may
Treatment Practice
No. of devices
No. of change events
Intra-device complexity
No. of change types
No. of VLANs
No. of models
No. of roles
Avg. devices changed per event
Frac. events w/ interface change
Frac. events w/ ACL change
p-value for 1:2
1.92×10−8
1.05×10−12
1.53×10−2
5.75×10−12
6.46×10−6
1.31×10−7
2.99×10−10
3.56×10−8
5.27×10−3
9.10×10−9
Table 7: Causal analysis results for the ﬁrst and second bin
for the top 10 statistically dependent management practices;
highlighted p-values satisfy our signiﬁcance threshold
Comparison Point
2:3
Imbal.
Treatment
4:5
3:4
Practice
Imbal.
Imbal.
No. of devices
1.63×10−2
3.34×10−2 2.80×10−3
No. of change events
1.47×10−1
Imbal. 1.71×10−1
Intra-device complexity
9.02×10−1 1.42×10−5
Imbal.
No. of change types
Imbal. 1.94×10−3
Imbal.
No. of VLANs
Imbal.
Imbal.
Imbal.
No. of models
Imbal. 6.63×10−1
Imbal.
No. of roles
Avg. devices changed per event 4.53×10−3 2.25×10−1
Imbal.
Frac. events w/ interface change 4.51×10−2 4.58×10−1 2.89×10−12
6.48×10−2
Frac. events w/ ACL change
4.88×10−2 2.78×10−1
Table 8: Causal analysis results for upper bins for the top 10
statistically dependent management practices; highlighted p-