as removing any invalid characters from hostnames. It also
establishes communications:
for instance, Netkit provides
management interfaces connected using the TAP interface
ability of Linux, where these TAP interface IP addresses
are allocated by the Netkit platform compiler. We provide
a separate reference implementation for Dynagen, Netkit,
Junosphere, and C-BGP.
The platform compiler module then calls the per-device
compilers. The generic router compiler consists of base func-
tions: compile(), ospf(), interfaces(). These can be over-
written in the inherited device compilers, extended by call-
ing the super() module, or added to for new overlays.
A subset of the resulting Resource DB output for as100r1
of the Small-Internet Case Study is shown below:
1 "render":
2
{"base": "templates/quagga"
3
4
5
6
7
8
9
10
11
12
13
14
15
"base_dst_folder": "localhost/netkit/as100r1"},
"zebra": {"password": "1234", "hostname":
"as100r1"},
"ospf": "process_id": 1,
{"ospf_links": [
{"network": "192.168.1.4/30", "area": 0},
{"network": "192.168.1.0/30", "area": 0},
{"network": "192.168.1.68/30", "area": 0},
{"network": "192.168.1.8/30", "area": 0} ]},
"interfaces": [
{"description": "as100r1 to as100r3",
"ospf_cost": 1, "id": "eth1"},
{"description": "as100r1 to as100r2",
"ospf_cost": 1, "id": "eth2"}]}
Finally, cross-emulation platform connections can be re-
alised using our querying language, by selecting links which
traverse two target hosts, or target emulation platforms on
the same host, much in the same way as we select inter-
ASN links to construct eBGP sessions. The appropriate
cross-machine connections, such as GRE tunnels between
distributed Open vSwitches, can be created from the re-
sulting edge sets. The result is that emulations written on
diﬀerent platforms or real hardware can be connected.
5.5 Resource Database Rendering Attributes
Along with the device conﬁguration attributes, the Re-
source Database contains a set of render attributes. These
contain references to the correct templates to use, as speci-
ﬁed by the user, and the output ﬁles to which we will write
the conﬁguration ﬁles, as determined by the platform re-
quirements.
Some devices require more than one conﬁguration ﬁle,
such as the /etc/zebra/ directory for Quagga. In these cases
we also allow an input and output folder to be speciﬁed. In
this case, the input folder is a user-speciﬁed directory con-
taining both static ﬁles and template ﬁles, which is copied to
the output folder. This allows simple speciﬁcation of nested
folders to conﬁgure services, without writing code .
5.6 Visualization and Real-Time Feedback
Our system enables the design and conﬁguration of com-
plex, large-scale topologies. However, the problem that cre-
242ates the need for autoconﬁguration makes these hard to de-
bug: for complex topologies, it can be hard user to conﬁrm
that network design rules are indeed what was intended.
We address this with a real-time feedback system that
automatically renders the overlays. Visualization of net-
work topologies allows the user to see the created nodes
and edges of each overlay topology, labelled by user-selected
attributes. Nodes can be grouped by attributes, such as an
ASN or OSPF area, with full attribute information available
by hovering over a node. This allows the eﬀect of a change,
such as modifying the rule to connect edges in an OSPF
graph, to be instantly visible to the user.
The visualization system is implemented using D3.js [7],
allowing viewing in a modern web-browser, which allows
cross-platform visualization, without installation dependen-
cies. The visualization system is built on open standards
and libraries, and uses the JSON interchange format, so it
could be decoupled from our main conﬁguration generation
tool, and developed as a standalone visualization system.
The Small-Internet plots shown in Figure 1 and Figure 6
were automatically generated using the visualization system.
Figure 6: The Netkit Small Internet Lab [11], showing eBGP
overlay topology constructed using network design rules.
Dual lines indicate bi-directional BGP sessions.
5.7 Deployment and Measurement
The principal focus of the system is generating large-scale
network conﬁgurations with a high degree of ﬂexibility. How-
ever, to run these conﬁgurations as an emulated network
requires copying the conﬁgurations to the emulation server
or virtualization platform, executing start commands, and
monitoring launch progress.
The system automates the steps of deployment, launching
and measurement. These components use expect scripts as
these are available on most systems, including both routers
and servers.
The measurement system consists of a small client that
sits on the emulation hosts. A remote measurement client
simpliﬁes the parallel collection of data: a single measure-
ment client on the emulation server can connect to multiple
virtual machines on the same physical host, speeding up
data collection. TextFSM [17] is used to parse the results
back in a structured manner, and provides a reference tem-
plate for Linux traceroute. It is user extendable to cater for
varying experiment requirements. As we know the IP allo-
cations, we map the IP addresses back into the hosts they
represent. By applying our selection function from our over-
lay graphs, we can build and deploy a network, run a series
of traceroutes, parse the results, and present the paths back
to the user as a list of overlay nodes suitable for processing.
These results can be analysed as Python data structures,
stored as data ﬁles, or visualized. The analysis allows con-
struction of new graph topologies, which can be compared
against the original design topologies. This enables valida-
tion: for example, the OSPF neighbors command could be
run on each router, used to construct the OSPF graph of the
running network, and compared against the OSPF overlay
construced at design-time using the design rules. This pro-
vides a powerful framework for automated validation that
the experimental topology is indeed correct — an essential
step in the scientiﬁc metod.
6. SYSTEM USE
Our approach requires up-front coding to write the net-
work design rules, but by separating the design rules and
the input topology, the same rules can be applied to diﬀer-
ent input topologies. Hence, the same pieces of code can be
used immediately on much larger topologies, without the re-
quirement to rewrite the code. That encourages reusability,
and means that many projects may not need to write any
code for their experiments: just provide an annotated input
topology.
Decoupling the network topology and design rules also
allows network conﬁguration tasks to be divided.
Input
topologies could be created or maintained by students or
operations staﬀ, with the protocol or service design rules
(and design sanity-checks) written by lecturers or Domain
Experts — on a per-protocol basis if necessary. The com-
piler and templates can be written by those with expertise
in the speciﬁc target syntax.
The system can scale to networks with thousands of de-
vices: the European NREN model took 2 minutes from input
topology to generated conﬁguration ﬁles. The main perfor-
mance limitation is in ﬁle system operations to write the
conﬁguration ﬁles to disk. While the NetworkX graph li-
brary [22] is eﬃcient for large networks, conﬁguration re-
quires iterating over edges (such as physical links or BGP
sessions), which can become time-consuming for dense topolo-
gies, such as full-mesh iBGP. This computational complex-
ity is also a problem in the running network, not just at
conﬁguration-time: iBGP provides route-reﬂectors to reduce
the number of sessions between router pairs, which we dis-
cuss this further in § 7. Finally, the performance of any
task-speciﬁc functions should be considered. These tasks,
such as server encryption key computation, are inherent to
the underlying conﬁguration — they would also need to be
performed for manual conﬁguration — and are not due to
our framework.
6.1 System Walkthrough
This section shows the system by summarising the steps to
recreate the Netkit Small Internet Lab § 3.1, and simplicity
compared to manual deployment.
We ﬁrst create the base object anm and import the topol-
ogy description from a GraphML ﬁle using the load_graphml
module, which checks the topology for validity and applies
defaults including setting the nodes device_type attribute
to router, platform to netkit, and syntax to quagga.
as20r2as20r3as1r1as30r1as40r1as20r1as100r1as100r2as100r3as200r1as300r2as300r4as300r3as300r12431 anm = AbstractNetworkModel()
2 data = load_graphml("small_internet.graphml")
3 G_in = anm.add_overlay("input", graph = data)
4 G_phy = anm[’phy’]
5 G_phy.add_nodes_from(G_in, retain=[’device_type’,
’asn’, ’platform’, ’host’, ’syntax’])
6 G_phy.add_edges_from(G_in.edges(type="physical"))
While some network information is derived from the topol-
ogy, additional assignment of attributes may be required to
complete the speciﬁcation. These attributes, such as de-
vice type or platform, are then used in the design of overlay
graphs for protocols and services.
The routing overlay graphs can be added using the over-
lay API shown below, where OSPF, eBGP and iBGP are
conﬁgured with two lines of code each using the ability to
select subsets of existing graph sets through the use of log-
ical operators. For default protocol use these lines could
be used directly, but the syntax allows more complicated
conﬁgurations.
1 rtrs = list(G_in.routers())
2 G_ospf = anm.add_overlay("ospf", rtrs)
3 G_ospf.add_edges_from(e for e in G_in.edges() if
e.src.asn == e.dst.asn)
4 G_ebgp = anm.add_overlay("ebgp", rtrs, directed = 1)
5 G_ebgp.add_edges_from((e for e in G_in.edges() if
e.src.asn != e.dst.asn), bidirected = 1)
6 G_ibgp = anm.add_overlay("ibgp", rtrs, directed = 1)
7 G_ibgp.add_edges_from(((s, t) for s in rtrs for t in
rtrs if s.asn == t.asn), bidirected = 1)
With overlay graphs deﬁned, the system compiles the net-
work representation to produce the Resource Database, which
the renderer applies templates to, producing device conﬁgu-
rations. An example router conﬁguration is provided below,
rendered using the template in Listing 4.1 and the Resource
DB subset in Listing 5.4.
1 dst = choice(list(nidb.routers())).interfaces[0]
2 cmd = "traceroute -naU %s" % dst.ip_address
3 hosts = [n.tap.ip for n in nidb.routers()]
4 measure.send(nidb, cmd, hosts)
We now have a fully deployed virtual router network, run-
ning in emulation and available for experimentation.
The output snippet below shows the result of actual tracer-
oute code, from the standard Linux IPv4 traceroute com-
mand. We use the IP Allocation mapping to translate each
hop back into router names, as shown in line 5. This can
then be easily and accurately translated into an AS path.
1 1 192.168.1.34 0 ms 2 192.168.1.25 0 ms
2 3 192.168.1.82 0 ms 4 192.168.1.73 0 ms
3 5 192.168.1.69 0 ms 6 192.168.1.2 0 ms
4 [as300r2, as40r1, as1r1, as20r3, as20r2, as100r1,
as100r2]
5 import autonetkit.ank_messaging as msg
6 nodes = [path[0], path[-1]]
7 msg.highlight(nodes, [], [path])
An example of plotting this traceroute data as a path is
shown in Figure 7. The code to do so is shown in lines 5,
6 and 7, where the highlight function is used to show both
the path, and the source and destination nodes. Using a
visualization to view collected data makes incorrect paths
quickly apparent, and allows large datasets to be viewed.
1 hostname as100r1
2 password 1234
3
interface eth1
ip ospf cost 1
interface eth2
ip ospf cost 1
6
7 router ospf
8
4
5
9
10
11
network 192.168.1.0/30 area 0
network 192.168.1.4/30 area 0
network 192.168.1.68/30 area 0
network 192.168.1.8/30 area 0
We now have a set of conﬁgurations that are ready to be
deployed and activated. Automatic deployment of a com-
piled set of conﬁgurations requires three parameters: the
emulation host, username on the host, and the source direc-
tory containing the conﬁguration ﬁle. These parameters are
obtained from the Resource DB.
The Netkit deployment script archives the generated con-
ﬁguration ﬁles, transfers them to the emulation host, ex-
tracts them, and runs the Netkit lstart command. The
progress is monitored with updates provided to the user
through logs and the visualisation. The module to trans-
fer, extract and monitor the lab is less than one hundred
lines of high-level Python code, and can be extended with
basic scripting experience.
Measurements may also be auto-conﬁgured, either to ver-
ify that the topology is correct, or as part of the experiment
itself. This task can be automated. The code required to
conﬁgure a set of traceroute measurements and the resulting
output is shown below.
Figure 7: The Netkit Small Internet Lab [11], with example
traceroute output visualized as a path.
7. EXTENDING THE SYSTEM