PI:EMAIL
Abstract. Fromtheearlyyears,theresearchonrecommendersystems
has been largely focused on developing advanced recommender algo-
rithms. These sophisticated algorithms are capable of exploiting a wide
rangeofdata,associatedwithvideoitems,andbuildqualityrecommen-
dations for users. It is true that the excellency of recommender systems
can be very much boosted with the performance of their recommender
algorithms.However,themostadvancedalgorithmsmaystillfailtorec-
ommendvideoitemsthatthesystemhasnoformofrepresentativedata
associatedtothem(e.g.,tagsandratings).ThisisasituationcalledNew
Item problem and it is part of a major challenge called Cold Start. This
problemhappenswhenanewitemisaddedtothecatalogofthesystem
andnodataisavailableforthatitem.Thiscanbeaseriousissueinvideo-
sharing applications where hundreds of hours of videos are uploaded in
every minute, and considerable number of these videos may have no or
very limited amount of associated data.
Inthispaper,weaddressthisproblembyproposingrecommendation
based on novel features that do not require human-annotation, as they
can be extracted completely automatic. This enables these features to
be used in the cold start situation where any other source of data could
bemissing.Ourproposedfeaturesdescribeaudioaspectsofvideoitems
(e.g., energy, tempo, and danceability, and speechiness) which can cap-
ture a different (still important) picture of user preferences. While rec-
ommendationbasedonsuchpreferencescouldbeimportant,verylimited
attention has been paid to this type of approaches.
Wehavecollectedalargedatasetofuniqueaudiofeatures(fromSpo-
tify)extractedfrommorethan9000movies.Wehaveconductedasetof
experimentsusingthisdatasetandevaluatedourproposedrecommenda-
tiontechniqueintermsofdifferentmetrics,i.e.,Precision@K,Recall@K,
RMSE,andCoverage.Theresultshaveshownthesuperiorperformance
(cid:2)c SpringerNatureSwitzerlandAG2021
H.Hacidetal.(Eds.):ICSOC2020Workshops,LNCS12632,pp.365–378,2021.
https://doi.org/10.1007/978-3-030-76352-7_35
366 M. H. Rimaz et al.
of recommendations based on audio features, used individually or com-
bined, in the cold start evaluation scenario.
· · ·
Keywords: Recommender systems Audio visual Multimedia Cold
start
1 Introduction
YouTube, as an instance of popular video-sharing web and mobile applications,
has about 1.5 billion active users who consume incredible number of 5 billion
videosperday1.Hence,itisnotuncommontoobserveconfusedvideoconsumers
with problem in deciding what to watch from a missive volume and variety of
videos[3].RecommenderSystemscancopewiththisproblembysupportingthe
userswhenmakingdecisiononwhattowatch[27,30,34].Recommendersystems
canbuildpersonalizedvideosuggestionsbasedontheparticularinterestsofusers
for videos and find what can better match users’ needs and constraints [33,35].
Overthemanyyears,widerangeofvideorecommendationalgorithmshavebeen
proposed and evaluated presenting excellency in performance. These algorithms
can receive a variety of data sources, e.g., content-associated data (tags), and
generate personalized recommendations on top of this data [2,10,20,30,39].
While the performance of these recommender algorithms can impact the
qualityofthegeneratedrecommendations,however,anytypeofalgorithmsmay
fail to generate relevant recommendations of video items which have no or very
limited amount of associated data [17,29,37,42]. This is a situation known as
New Item Cold Start problem, which typically occurs when a new item is added
to the catalog of the system and no input data is available for that item [14,
15,25]. This is a major problem in video-sharing applications, such as YouTube
where hundreds of hours of videos are uploaded in every minute, by millions of
active video makers2.
Furthermore,collectingthetraditionaltypesofcontent-associateddata,that
are typically represented by semantic attributes (e.g., tags), requires either a
groupofexpertsoranetworkofusers[6,7,13,32,43].Thisindeedisanexpensive
process and needs human efforts. Then recommendations based on these costly
semantic attributes still may not properly capture the true users’ preferences,
e.g., the user tastes associated with audio characteristics of videos.
In addressing this problem, this article investigates the potential behind dif-
ferent types of audio features representative of video content in building qual-
ity recommendations for users. We have exploited two different audio features
that can be extracted completely automatic without any need for costly manual
human annotation. Hence they can be exploited by any content-based recom-
menderalgorithmcapableofincorporatingthemintherecommendationprocess.
1 https://www.omnicoreagency.com/youtube-statistics.
2 http://tubularinsights.com/hours-minute-uploaded-youtube/.
AudioLens 367
We have compared quality of recommendation based on the (automatic)
audio features against other types of (automatic) features and (manual) tags.
The comparisons have been conducted with respect to various evaluation met-
rics (i.e., Precision@N, Recall@N, RMSE, and Coverage) using a large dataset
of more than ≈18M ratings obtained from a large network of ≈162K users who
provided the ratings for ≈9K movies.
The overall results of the evaluation have shown the consistent superiority
of the recommendations based on our novel audio features over the traditional
tags.
– we propose a novel technique for video recommendation based on audio fea-
tures (e.g., energy, tempo, and danceability, and speechiness) that can be
extracted automatically, without any need for costly human-annotation;
– we will publish a large dataset3, which is the most important contribution
of this paper, that contains a wide range of audio features (collected from
Spotify) for 9,104 movies, linked directly with the user ratings and tags
(+ other descriptors such as visual features);
– wehaveevaluatedtherecommendationsbasedonnovelaudiofeaturesincold
start scenarios, when features are used individually or when used in combi-
nation with other features; we tested the recommendation quality exploiting
using millions of ratings given by hundreds of thousands of users;
2 Related Works
This work is related to two research fields, i.e., the Cold Start problem and
Audio-aware Recommendation Systems.
One of the major problems of recommender systems in general is the cold
start problem, i.e., when a new user or a new item is added to the catalog and
thesystemdoesnothavesufficientdataassociatedwiththeseusers/items[4].In
suchacase,thesystemcannotproperlyrecommendexistingitemstoanewuser
(new user problem) or recommend a new item to the existing users (new item
problem) [1]. In video domain, one of the effective approaches that can tackle
the cold start problem exploit different forms of video content for generating
recommendation[2].Suchvideocontentcanbemanuallyadded,e.g.tags[18,28],
or automatically extracted, e.g., visual descriptors [5,12,23,26,36].
Another form of content data that can be used for video recommendation is
basedonaudiodescriptors[38].Verylimitedworkshavefocusedoninvestigating
such type of descriptors and their potential in representing user preferences. As
anexample,in[31]thecorrelationbetweenusermusictasteandhis/herperson-
ality has been discussed. Several medium and weak correlations between music
audio features and personality traits have been shown, and their results have
provided useful insights into the relationship between the personality and the
music preference. Moreover, authors in [21] have collected a dataset of movies
and television shows matched with subtitles and soundtracks and analyzed the
3 https://github.com/mhrimaz/audio-lens.
368 M. H. Rimaz et al.
relationship between story, song, and the user taste. However, they have taken
a non-personalized approach and used IMDb ratings. [44] has investigated the
effect of the movie soundtrack search volume on the movie revenue in different
time periods. It has shown that the online search volume of a movie soundtrack
hasaneffectonthemovierevenue.[19]hasinvestigatedtherelationshipbetween
the musical and visual art preferences, and the role of personality traits in pre-
dicting preferences for different musical styles and visual art motives. Beside
this, [11] recommender system has integrated the some forms of deep learning
features as well as block-level and i-vector audio features of more than 4,000
movie trailers.
Thisworkdiffersfromthepriorworksindifferentaspects.Intermsofdataset,
priorworksextractedtheaudiofeaturesfrommovietrailersorshortclips(e.g.,in
[11]), while in our dataset, the audio features have been extracted from original
score soundtracks for full-length movies. Even though in [21] the authors take
a similar approach, however, their focus is not really personalization. Moreover,
we cover almost double in number of items. Second, in our experiments, we
use the recently released MovieLens25M dataset, with much larger number of
ratings. Finally, unlike previous datasets, e.g., introduced by [11,21], our data
went through extensive manual checks, and errors have been corrected with
careful expert checks.
3 Proposed Method
3.1 Data Collection Process
We did the data collection process in two phases. In the first phase, we queried
albumsinSpotifywithaspecificpattern“{movie name}(Original Motion Pic-
ture Soundtrack) {year}”. This naming pattern is quite prevalent within the
music industry, and many publisher’s releases follow this naming convention.
This phase was completely automated using the Spotify Search API4 to find
a Spotify identifier (Spotify ID) for each movie. Each Spotify ID could repre-
sent an album or a playlist. However, there are several shortcomings to this
approach. First, many albums do not follow this naming convention (e.g., “Toy
Story (Soundtrack)”). Second, some movies do not have any related published
album, whereas their soundtrack is a playlist in Spotify. To alleviate this prob-
lem, and enhance the quality of our dataset, in the second phase, we carefully
checked each individual entry, manually. A team of 7 trained person taught to
check the matching manually. Several criteria and identifier factors have been
usedtocheckthecorrectnessofmatching.Firstandforemost,thealbum’sposter
and the movie’s poster should usually look identical or share some common ele-
ments.Moreover,composerinformationandtracknamescheckedagainstvarious
online resources, including IMDb’s soundtrack section and Wikipedia. In some
few cases, the decision is inconclusive, which in such cases, we simply removed
the entry from the dataset. We manually checked the corresponding movie or
4 https://developer.spotify.com/documentation/web-api/.
AudioLens 369
playlistSpotifyidentifierformissingpopularmovieswiththehighestnumberof
ratingsintheIMDbplatform.WedecidedtouseIMDbsincemanynewreleases
may have very low number of ratings in MovieLens25M [22]5 dataset released
onJanuary2019.Withtheadventofsophisticatedsignalprocessingtechniques,
automatically extracting musical and vocal features from a full-length movie
would be possible for real-world recommender systems. Since this is out of the
scope of this research, we used already existed Spotify API. By having a man-
ual checking procedure, we are ensuring to have a high quality and error-prone
dataset for further researches.
3.2 Dataset Description
Our dataset provides a link between every movie and its corresponding sound-
track in Spotify (using Spotify ID). We found the Spotify ID for 9,104 movies.
These movies received 18,745,630 ratings from 16,254 users. For each Spotify
ID, we could find a corresponding album or playlist, which contains the number
of included music tracks. Using the unique ID, we could collect the represent-
ing audio features provided by Spotify Audio Feature API6. The following list,
briefly explains our collected audio features:
– f1: Acousticness is a confidence measure from 0.0 to 1.0 (high confidence)
of whether the track is acoustic.
– f2: Danceability describes how suitable a track is for dancing based on
a combination of musical elements including tempo, rhythm stability, beat
strength, and overall regularity. The value is in the range of [0,1].
– f3: Energy is a measure from 0.0 to 1.0 and represents a perceptual mea-
sure of intensity and activity. Features contributing to this attribute include
dynamic range, perceived loudness, timbre, onset rate, and general entropy.
Typically,energetictracksfeelfast,loud,andnoisy.Forexample,deathmetal
has high energy, while a Bach prelude scores low on the scale.
– f4: Instrumentalness predicts whether a track contains no vocals. Rap or
spokenwordtracksareclearly“vocal”.Theclosertheinstrumentalnessvalue
is to 1.0, the greater likelihood the track contains no vocal content. Values
above 0.5 are intended to represent instrumental tracks, but confidence is
higher as the value approaches 1.0.
– f5: Livenessshowsthepresenceofanaudienceintherecording.Higherlive-
ness values represent an increased probability that the track was performed
live. A value above 0.8 provides strong likelihood that the track is live.
– f6: Loudness is the overall loudness of the entire track in decibels (dB)
ranging typically between −60 and 0 db. Loudness is the quality of a sound
that is the primary psychological correlate of physical strength (amplitude).
5 https://grouplens.org/datasets/movielens/25m/.
6 https://developer.spotify.com/web-api/get-audio-features.
370 M. H. Rimaz et al.
– f7: Popularity of a track is a value between 0 and 100, and is based on the
total number of plays the track has had and how recent those plays are.
– f8: Speechiness detects the presence of spoken words in a track. The more
exclusivelyspeech-liketherecording(e.g.talkshow,audiobook,poetry),the
closer to 1.0 the attribute value.
– f9: Tempo is the speed or pace of a given piece and is the overall estimated
tempo of a track in beats per minute.
– f10: Track Duration is the duration of the track in milliseconds.
– f11:Valenceisameasurefrom0.0to1.0describingthemusicalpositiveness
conveyedbyatrack.Morepositivetracks(e.g.happy,cheerful,euphoric)have
highervalencesound,whiletrackswithlowvalencesoundmorenegative(e.g.
sad, depressed, angry).
– f12: Key is the estimated overall key of the track. The values ranging from
0 to 11 mapping to pitches using standard Pitch Class notation7 (E.g. 0 =
C, 1 = C-sharp/D-flat, 2 = D, and −1 if no key was detected).
– f13: Modeindicatesthemodality(majoris1andminoris0)ofatrack,the
type of scale from which its melodic content is derived. Note that the major
key (e.g. C major) could more likely be confused with the minor key at 3
semitones lower (e.g. A minor) as both keys carry the same pitches.
– f14:TimeSignaturespecifieshowmanybeatsareineachbar(ormeasure).
It ranges from 3 to 7 indicating time signatures of “3/4”, to “7/4”.
3.3 Recommendation Algorithm
We adopted a classical “K-Nearest Neighbor” content-based algorithm. Given
a set of users u ∈ U and a catalogue of items i ∈ I, a set of preference scores
r ui provided by user u to item i has been collected. Moreover, each item i ∈ I
is associated to its feature vector f i. For each couple of items i and j, the
similarity score s ij is computed using cosine similarity. For each item i the set
of its nearest neighbors closer that a specified threshold NN i is built, Then,
for each user u ∈ U, the predicted preference score rˆui for an unseen item i is
computed as follows
(cid:2)
s ij = f fiT ff j and rˆui = (cid:2)j∈NNi,ruj>0r u sjs ij (1)
i j j∈NNi,ruj>0 ij
3.4 Baselines
Wehavecomparedourproposedrecommendationtechnique(AudioLens)against
recommendation based on a range of automatic and manual features. For auto-
matic features, that can be used in cold start situation, we considered recom-
mendation based on Musical Keys and Visual features. Musical keys can
be also collected from Spotify and be a informative descriptor of the musics
composed for movies. Visual features is a novel form of content descriptors that
7 https://en.wikipedia.org/wiki/Pitch class.
AudioLens 371
has been shown to be effective in cold start situation. In our experiment, we
used a recent dataset MA14KD8 that have shown promising results in recom-
mender systems [16]. In addition, we combined both audio and visual features
and formed Hybrid features in order to compare the recommendation based
onthesefeaturesusedindividuallyorincombination.Allofthesefeaturescanbe
extractedautomaticallyandadoptedforrecommendationincoldstartsituation.
Forthesakeofcomparison,weconsiderrecommendationbasedonmanualTags
which certainly need human-annotation and may be missing in cold start situa-
tion.However,thisformofrecommendationcanstillbeincludedasatraditional
baseline in our experiment.