**Email: PI**

### Abstract
From the early years, research on recommender systems has largely focused on developing advanced algorithms capable of exploiting a wide range of data associated with video items to generate high-quality recommendations. While the performance of these algorithms significantly enhances the effectiveness of recommender systems, they often struggle with the "New Item" problem, where the system lacks representative data (e.g., tags and ratings) for newly added items. This issue is part of the broader "Cold Start" challenge, which is particularly problematic in video-sharing applications where hundreds of hours of videos are uploaded every minute, many with little or no associated data.

In this paper, we address the New Item problem by proposing a recommendation approach based on novel, automatically extracted audio features. These features, such as energy, tempo, danceability, and speechiness, capture important aspects of user preferences without requiring human-annotated data. This makes them suitable for use in cold start situations. Despite their potential, very limited attention has been given to this type of approach.

We have collected a large dataset of unique audio features from over 9,000 movies using Spotify. Our experiments, conducted using this dataset, evaluated the proposed recommendation technique using various metrics, including Precision@K, Recall@K, RMSE, and Coverage. The results demonstrate the superior performance of recommendations based on audio features, whether used individually or in combination, in cold start scenarios.

**Keywords:** Recommender systems, Audio features, Multimedia, Cold start

### 1. Introduction
YouTube, a popular video-sharing platform, has approximately 1.5 billion active users who consume around 5 billion videos daily. This vast volume and variety of content can overwhelm users, making it difficult to decide what to watch. Recommender systems help by providing personalized video suggestions based on user preferences and constraints. Over the years, a wide range of video recommendation algorithms have been developed, leveraging various data sources such as content-associated tags to generate personalized recommendations.

However, even the most advanced algorithms may fail to recommend new video items that lack sufficient associated data, leading to the "New Item Cold Start" problem. This is especially challenging in video-sharing applications like YouTube, where hundreds of hours of videos are uploaded every minute, often with minimal or no associated data.

Traditional content-associated data, such as semantic tags, require either expert groups or user networks to annotate, which is both expensive and time-consuming. Moreover, these annotations may not fully capture user preferences, particularly those related to audio characteristics.

This article investigates the potential of different types of audio features in building quality recommendations. We propose two automatic audio features that do not require costly human annotation, making them suitable for cold start scenarios. We compare the quality of recommendations based on these audio features against other automatic and manual features using a large dataset of over 18 million ratings from approximately 162,000 users for about 9,000 movies. The results show the consistent superiority of our proposed audio features over traditional tags.

### 2. Related Works
Our work is related to two main research fields: the Cold Start problem and Audio-aware Recommendation Systems.

The cold start problem is a significant challenge for recommender systems, occurring when a new user or item is added to the system with insufficient associated data. In such cases, the system cannot effectively recommend existing items to new users or new items to existing users. In the video domain, one effective approach to tackle this problem is to use different forms of video content, such as manually added tags or automatically extracted visual descriptors.

Another form of content data for video recommendation is based on audio descriptors. Limited research has focused on the potential of audio features in representing user preferences. For example, studies have explored the correlation between music taste and personality traits, the relationship between story, song, and user taste, and the effect of movie soundtracks on revenue. However, these studies often take non-personalized approaches or use limited datasets.

Our work differs from prior research in several ways. First, we extract audio features from full-length movie soundtracks rather than trailers or short clips. Second, we use a larger and more comprehensive dataset, the MovieLens25M, with extensive manual checks to ensure data quality.

### 3. Proposed Method

#### 3.1 Data Collection Process
We collected data in two phases. In the first phase, we queried albums in Spotify using a specific naming pattern: "{movie name} (Original Motion Picture Soundtrack) {year}". This pattern is common in the music industry, and we used the Spotify Search API to find a Spotify ID for each movie. However, some albums do not follow this convention, and some movies do not have published soundtracks. To address this, in the second phase, we manually checked each entry using a team of seven trained individuals. They verified the matching based on criteria such as album and movie posters, composer information, and track names. Entries with inconclusive matches were removed from the dataset.

#### 3.2 Dataset Description
Our dataset links each movie to its corresponding soundtrack in Spotify using the Spotify ID. We found the Spotify ID for 9,104 movies, which received 18,745,630 ratings from 16,254 users. For each Spotify ID, we collected audio features provided by the Spotify Audio Feature API, including:

- **f1: Acousticness**: A confidence measure from 0.0 to 1.0 indicating whether the track is acoustic.
- **f2: Danceability**: A measure from 0.0 to 1.0 indicating how suitable a track is for dancing.
- **f3: Energy**: A measure from 0.0 to 1.0 representing the intensity and activity of the track.
- **f4: Instrumentalness**: A measure from 0.0 to 1.0 predicting whether a track contains no vocals.
- **f5: Liveness**: A measure indicating the presence of an audience in the recording.
- **f6: Loudness**: The overall loudness of the track in decibels.
- **f7: Popularity**: A value between 0 and 100 based on the total number of plays and recency.
- **f8: Speechiness**: A measure detecting the presence of spoken words in the track.
- **f9: Tempo**: The speed of the track in beats per minute.
- **f10: Track Duration**: The duration of the track in milliseconds.
- **f11: Valence**: A measure from 0.0 to 1.0 describing the musical positiveness.
- **f12: Key**: The estimated key of the track.
- **f13: Mode**: The modality (major or minor) of the track.
- **f14: Time Signature**: The number of beats in each bar.

#### 3.3 Recommendation Algorithm
We adopted a classical K-Nearest Neighbor (K-NN) content-based algorithm. Given a set of users \( U \) and a catalog of items \( I \), a set of preference scores \( r_{ui} \) provided by user \( u \) to item \( i \) is collected. Each item \( i \) is associated with a feature vector \( f_i \). The similarity score \( s_{ij} \) between items \( i \) and \( j \) is computed using cosine similarity. For each item \( i \), the set of its nearest neighbors \( NN_i \) closer than a specified threshold is built. The predicted preference score \( \hat{r}_{ui} \) for an unseen item \( i \) is then computed as follows:

\[
s_{ij} = \frac{f_i^T f_j}{\|f_i\| \|f_j\|}
\]
\[
\hat{r}_{ui} = \frac{\sum_{j \in NN_i, r_{uj} > 0} r_{uj} s_{ij}}{\sum_{j \in NN_i, r_{uj} > 0} s_{ij}}
\]

#### 3.4 Baselines
We compared our proposed recommendation technique (AudioLens) against recommendations based on a range of automatic and manual features. For automatic features, we considered Musical Keys and Visual features. Musical keys can be informative descriptors of the music composed for movies, while visual features have shown promise in cold start situations. We also combined audio and visual features to form Hybrid features. All these features can be extracted automatically and used in cold start scenarios.

For comparison, we included recommendations based on manual Tags, which require human-annotation and may be missing in cold start situations. Despite this, they serve as a traditional baseline in our experiment.