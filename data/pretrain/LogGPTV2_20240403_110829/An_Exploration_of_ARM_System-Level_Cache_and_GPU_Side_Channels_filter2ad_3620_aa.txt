title:An Exploration of ARM System-Level Cache and GPU Side Channels
author:Patrick Cronin and
Xing Gao and
Haining Wang and
Chase Cotton
An Exploration of ARM System-Level Cache and GPU Side
Channels
Patrick Cronin
University of Delaware
United States of America
PI:EMAIL
Haining Wang
Virginia Tech
United States of America
PI:EMAIL
Xing Gao
University of Delaware
United States of America
PI:EMAIL
Chase Cotton
University of Delaware
United States of America
PI:EMAIL
ABSTRACT
Advanced RISC Machines (ARM) processors have recently gained
market share in both cloud computing and desktop applications.
Meanwhile, ARM devices have shifted to a more peripheral based
design, wherein designers attach a number of coprocessors and
accelerators to the System-on-a-Chip (SoC). By adopting a System-
Level Cache, which acts as a shared cache between the CPU-cores
and peripherals, ARM attempts to alleviate the memory bottle-
neck issues that exist between data sources and accelerators. This
paper investigates emerging security threats introduced by this
new System-Level Cache. Specifically, we demonstrate that the
System-Level Cache can still be exploited to create a cache occu-
pancy channel to accurately fingerprint websites. We redesign and
optimize the attack for various browsers based on the ARM cache
design, which can significantly reduce the attack duration while in-
creasing accuracy. Moreover, we introduce a novel GPU contention
channel in mobile devices, which can achieve similar accuracy to
the cache occupancy channel. We conduct a thorough evaluation
by examining these attacks across multiple devices, including iOS,
Android, and MacOS with the new M1 MacBook Air. The experi-
mental results demonstrate that (1) the System-Level Cache based
website fingerprinting technique can achieve promising accuracy
in both open (up to 90%) and closed (up to 95%) world scenarios,
and (2) our GPU contention channel is more effective than the CPU
cache channel on Android devices.
ACM Reference Format:
Patrick Cronin, Xing Gao, Haining Wang, and Chase Cotton. 2021. An
Exploration of ARM System-Level Cache and GPU Side Channels. In Annual
Computer Security Applications Conference (ACSAC ’21), December 6–10, 2021,
Virtual Event, USA. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3485832.3485902
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8579-4/21/12...$15.00
https://doi.org/10.1145/3485832.3485902
1 INTRODUCTION
While Advanced RISC Machines (ARM) processors have dominated
the mobile device market over the past decade, recently they have
also gained market share in both cloud computing and desktop
applications. Enterprises like Apple and Samsung have announced
plans to develop ARM based laptop devices that function with
the complete MacOS and Windows operating systems. Apple has
already released its M1 ARM chip to power its newest laptop and
desktop devices. Spurring this rapid expansion of ARM devices into
new markets is the adoption of a more peripheral based design that
attaches a number of coprocessors and accelerators to the System-
on-a-Chip (SoC). ARM has also adopted a System-Level Cache to
serve as a shared cache between the CPU-cores and peripherals.
This design works to alleviate the memory bottleneck issues that
exist between data sources and the accelerators, allowing higher
speed communication and increased performance.
If the marketshare of ARM processors in desktop and laptop
systems continues to increase, it is expected that attackers will
devote more resources to attacking the ARM architecture. While
extensive research has been conducted on exploring and securing
microarchitectural side channels on Intel’s x86 systems, far less
research has been focused on the ARM architecture. Furthermore,
as mobile OSes tend to deny low level control over the hardware,
most vulnerabilities are usually within non-essential APIs [5, 9,
21, 27, 54, 55] and are rapidly patched. ARM designers must be
careful to ensure that their designs are not vulnerable to malicious
attacks when exposed to a full fledged operating system, where
OS developers are able to exert far fewer restrictions on potential
attacker activities.
In this paper, we present an in-depth security study on recent per-
sonal computing devices (e.g., mobile phones and laptops) equipped
with ARM processors with the recent DynamIQ [34] design. Un-
like previous designs that only share cache within core clusters,
these devices contain multiple levels of cache and share the last-
level cache with other core clusters and accelerators (e.g., graphics
processing unit). Unlike x86 processors, these ARM devices utilize
heterogeneous core architectures, different caching policies, and
advanced energy aware scheduling to increase performance and
battery life. We endeavor to examine whether those advancements
(e.g., new cache architectures, the tight integration of accelerators,
etc.) make the ARM platform more difficult to attack compared
with with x86 platforms.
784ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Patrick Cronin, Xing Gao, Haining Wang, and Chase Cotton
followed:
Specifically, we focus on investigating cache occupancy chan-
nels [50], which continually monitor shared cache activities, to
fingerprint websites. We design a series of microbenchmarks to
better understand how ARM system behaviors (e.g., energy aware
scheduling, core selection, and different browsers) affect the cache
occupancy channels. Based on our preliminary study, we further
optimize the attack for these new ARM cache designs and consider
multiple different browsers, including Chrome, Safari, and Firefox.
The redesigned attack significantly reduces the attack duration
while increasing accuracy over previous cache occupancy attacks.
Furthermore, we introduce a novel GPU contention channel in
mobile devices, which can achieve similar accuracy as the cache
occupancy channel. To evaluate the proposed attacks, we conduct
a thorough evaluation across multiple devices, including iOS, An-
droid, and MacOS with the new, ARM-based, M1 MacBook Air.
The experimental results show that the System-Level Cache based
website fingerprinting technique can achieve promising accuracy
in both open (up to 90%) and closed (up to 95%) world scenarios.
Overall, the main contributions of this work are summarized as
• An examination of the system-level cache within new ARM
SoCs that utilize the DynamIQ design principle, especially
how different components and software scheduling affect
cache behaviors.
• A thorough evaluation of the cache occupancy side channel
attack on Android, iOS, and MacOS platforms implemented
in both native and JavaScript attack vectors.
• An analysis of JavaScript engine memory management and
• The discovery of a new GPU side channel attack that can be
utilized to fingerprint user behaviors on MacOS and Android.
The rest of this paper is organized as follows: Section 2 provides
necessary background information. Section 3 presents the threat
model and discusses the unique challenges that the ARM archi-
tecture creates for attackers in a shared cache occupancy attack.
Section 4 details our system design and Section 5 describes our
experimental setup. Section 6 analyzes our findings and Section 7
surveys related works. Finally, Section 8 concludes the paper.
2 BACKGROUND
2.1 Caching and Side-Channel Attacks
Modern computer systems utilize a tiered memory system to en-
hance their performance, from the smallest and fastest (i.e., L1)
to larger and slower (e.g., L2 and L3). Two important distinctions
in caching are exclusive and inclusive caching. Inclusive caching
guarantees that any memory address that is included in a cache
tier is also present in the cache tiers below it. For example, a value
in the L1 cache is also present in the L2 and L3 caches. By contrast,
an exclusive caching policy ensures that items are only present in
one level of the cache (e.g., an item in the L1 cache is not present
in the L2 or L3 cache). While there are various pros and cons to
both caching policies, Intel x86 processors mostly employ inclu-
sive caching, but recent ARM processors tend to utilize exclusive
caching policies.
how it impacts attack effectiveness.
As portions of the cache are shared between all processes, it
has been widely exploited for side channel attacks. By determining
Figure 1: Overview of ARM’s DynamIQ architecture featur-
ing heterogeneous processor cores organized into high (big)
and low (LITTLE) performance clusters. The CPU clusters
and accelerators (GPU, ISP, and DSP) are all connected to a
shared system-level cache.
whether specific memory is in the cache (e.g., timing its access time),
attackers can infer the information of the victim. The ‘prime+probe’
attack [30, 40] attempts to identify vulnerable data locations that
indicate specific program flows. With a high resolution timer and
a predictable program, cache-based side channel attacks allow at-
tackers to extract private information such as encryption keys.
Cache Occupancy Channel. Shusterman et al. [50] suggested
two versions of the cache occupancy channel, cache occupancy and
cache sweeping. In cache occupancy, they designated a sample rate
(every 2ms) and accessed the entire buffer. If the buffer is accessed
faster than 2ms, the total time to access the buffer is recorded. If the
access takes longer than 2ms, a miss is recorded. In cache sweeping,
the cache buffer is continually accessed and the number of full
‘sweeps’ in each sampling period is recorded. At the beginning of
each sample period, the system starts accessing the cache from the
first location. They demonstrated that such techniques can be used
for robust website fingerprinting in x86 systems.
2.2 Consumer ARM System Design
Unlike x86 systems which utilize homogeneous core designs in
their processors, consumer ARM devices (as opposed to ARM based
server platforms which are out of the scope of this work) differ
greatly and utilize a heterogeneous architecture.
ARM big.LITTLE and DynamIQ. In ARM, the big.LITTLE
design was first developed to overcome the battery limitation in
mobile devices. The big.LITTLE architecture consists of a SoC made
from two discrete computing clusters, one low power group of cores
and one high power group [31]. With a number of new scheduling
techniques, the architecture allows the mobile OS to utilize high
big Clustercore [0]L1IL1Dcore [1]L1IL1DL2L2core [0]L1IL1DLITTLE Clustercore [1]L1IL1Dcore [2]L1IL1Dcore [3]L1IL1DDynamic Shared UnitSystem-Level CacheGPUDSPISPL2ARM DynamIQArchitecture785An Exploration of ARM System-Level Cache and GPU Side Channels
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
and low power cores for different tasks to extend battery life. In
ARM, the cache system is also redesigned. Instead of having a pri-
vate and shared cache architecture with an identical size across all
cores, big.LITTLE utilizes differently sized caches, wherein the high
performance cores have access to larger L1/L2 caches than their
lower performance counterparts. As the L2 caches of the different
core clusters are not shared between clusters, a large amount of
cache coherency traffic is necessary to facilitate switching tasks be-
tween the high and low performance cores, resulting in suboptimal
performance.
To overcome this performance limitation, a newer system ‘Dy-
namIQ’ [34] was developed for ARM. The DynamIQ system al-
lows greater modularity and design freedom than the original
big.LITTLE system. DynamIQ allows the processor designers to
create multiple clusters of heterogeneous processors (instead of
just two), and employs a shared L3 cache to improve computational
performance between processor clusters, as shown in Figure 1. Our
work explores the potential security vulnerabilities in this shared
cache architecture.
Accelerators. Due to the explosive popularity of machine learn-
ing applications in image and signal processing domains, mobile
devices have begun to require a low power method for execut-
ing neural network inference functions. To resolve this issue, cur-
rent mobile devices make use of a number of accelerators or co-
processors to enable advanced functionalities within their energy
budget. Recent versions of Apple’s custom A series chips, Qual-
comm’s Snapdragon, and Samsung’s Exynos chips have begun to
increase their reliance on accelerator peripherals. Those chips in-
clude dedicated digital signal processors, image signal processors,
motion co-processors, neural processing units, and graphics pro-
cessing units.
The inclusion of numerous accelerators creates a major system
design issue. To utilize a co-processor, it must be supplied with a set
of instructions and data to operate on. The co-processor must then
complete its calculations and return the data to the main processor.
In a non-integrated SoC, communication with co-processors must
take place over a bus, and this can severely limit any performance
speedup. Nvidia has attempted to resolve part of this problem on
x86 with GPUDirect [15], allowing for direct transfer of data to
the GPU without the CPU. To speed up co-processor performance
in ARM, the DynamIQ system utilizes a system-level cache that is
shared with these accelerators. ARM calls this technology cache
stashing [32], which allows tightly coupled accelerators (such as
GPUs) to directly access the shared L3 cache and in some cases
directly access L2 caches.
2.3 Website Fingerprinting and Timer
Restrictions
Website fingerprinting attacks identify the websites that a user
visits. Usually this involves training a classification system to dis-
tinguish a series of sensitive websites that the attacker is interested
in. The motivations for website fingerprinting can range from a
desire of learning information about a target (e.g., political views,
health issues, and gambling activity) to the construction of a user
profile for advertisement tracking. Typically, website fingerprint-
ing attacks involve an attacker that observes encrypted network
traffic and attempts to classify the user’s activities through fea-
tures extracted from the packet stream (e.g., timing, packet size,
and packet order) [4, 14, 20, 41, 43]. However, such attacks require
access to the network traffic of the victim. To sidestep this require-
ment, researchers have identified that the action of downloading
and rendering a website inevitably leaves a trace in the CPU and
cache activities of the victim system, which can be monitored via
local side channel to identify the victim’s website visiting activi-
ties [36, 50].
Motivated by high profile side channel attacks like Spectre [23]
and Meltdown [29] that utilize the JavaScript performance.now()
command to perform nanosecond resolution timing measurements,
browser and mobile operating system designers have worked to
limit access to system APIs and high resolution timer resources.
Specifically, in response to the Spectre and Meltdown attacks, browser
manufacturers have greatly reduced the precision of the
performance.now() counter [42] to between 50 microseconds and
1 millisecond. With the typical difference between cache misses