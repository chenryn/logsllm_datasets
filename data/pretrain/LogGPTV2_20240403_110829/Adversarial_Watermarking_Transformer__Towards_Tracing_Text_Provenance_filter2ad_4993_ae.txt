### Homograph-Based Encoding and Decoding

To encode binary data ('1' and '0' bits) using homographs, we divide the homograph set into two disjoint sets. Each set represents either a '1' or a '0' (bit-holding words). To ensure unique encoding and decoding, we avoid assigning multiple values to any single word by ensuring it is not found in different synonym sets. If a word has already been assigned a value, it is skipped.

#### Encoding Process
1. **Word Selection**: Identify occurrences of the selected homographs in the sentence.
2. **Bit Assignment**: Replace each identified word with its corresponding '1' or '0' synonym based on the current bit in the message.
3. **Iteration**: Repeat the process until all bits are encoded.

#### Decoding Process
Decoding is achieved through simple dictionary lookups. We use a message length of 4 bits, similar to our setup. To prevent unintentional encoding, any accidental occurrences of the 'bit-holding' words in the original text are replaced with their corresponding synonyms from the 'replace' list.

### Advantages of Our Model
Our model ensures that no words are exclusive to message encoding, unlike some other methods such as AWT (Figure 8). This flexibility is a significant advantage.

### Evaluation
We evaluate the baseline across several axes: effectiveness (utility and bit accuracy), secrecy, and robustness.

#### Effectiveness
- **Bit Accuracy and SBERT Distance**: We compute the bit accuracy and SBERT distance.
- **Comparison with AWT**: At comparable bit accuracy levels, AWT shows a considerably lower SBERT distance (Table IX).

#### Secrecy
- **Transformer-Based Classifier**: We train a transformer-based classifier with the same setup as in Section V-C.
- **F1 Score**: The baseline has a very high F1 score compared to AWT.

#### Robustness
- **Attacks**: We apply word removal and replacement attacks (Section V-D).
- **Sensitivity**: The baseline is more sensitive to these attacks because it changes a larger number of words (Figure 10).
- **Replace Attack**: This attack is particularly strong, as it can introduce incorrect encodings by adding other 'bit-holding' words.

### Generation-Based Hiding
An alternative to post-processing-based data hiding is to generate text that is already encoded with the input message. Unlike previous work that relied on masking, we jointly train a language model (AWD-LSTM) with a message decoder.

#### Model Details
- **Language Model**: AWD-LSTM takes the input word and message at each time step and predicts the next word.
- **Message Decoder**: Trained to reconstruct the input message from the generated sequence.
- **Joint Training**: The model is trained with both language modeling and message decoding losses.

#### Evaluation
- **Perplexity and Bit Accuracy**: We evaluate the model using perplexity (exponential of the model loss) and bit accuracy.
- **Operating Points**: Figure 11 shows different operating points, with a very high bit accuracy achievable with a 12-point increase in perplexity.
- **Recursive Greedy Decoding**: Message accuracy drops during inference, but improves with averaging two sentences.

### Human Evaluation
We conducted a user study to evaluate the naturalness and correctness of our model and the synonym baseline.

- **Ratings**: Sentences were rated on a Likert scale from 0 to 5.
- **Results**: AWT received higher ratings and less variance compared to the synonym baseline (Table X).

### Discussion
#### Granularity
- **Threat Scenario**: Focus on news articles with many tokens.
- **Short Texts**: Encoding 4 bits in short text is possible but not sufficient for confidence calculation.

#### False Positives
- **Control**: False positives can be controlled by p-value thresholds.
- **Improvement**: Use multiple confidence thresholds and combine with other fake news defenses.

#### Human Editing
- **Legitimate Use**: Black-box APIs can be used for partial text completion.
- **Main Threat**: Misuse to generate entire articles at scale.

#### Model Release
- **Black-Box Access**: Assumed for defending against misuse.
- **Cost and Expertise**: Training state-of-the-art models is expensive and requires significant expertise.

#### Watermarks Regulation
- **Multi-Bit Watermarking**: Extends to watermarking multiple models.
- **Regulation**: Requires cooperation among model owners or a trusted third party.

### Conclusion
AWT is a new framework for language watermarking, optimized for unobtrusive encoding. It achieves better trade-offs between effectiveness, secrecy, and robustness compared to rule-based synonym substitution baselines.

### References
[References listed here]

This revised version aims to improve clarity, coherence, and professionalism while maintaining the essential details and structure of the original text.