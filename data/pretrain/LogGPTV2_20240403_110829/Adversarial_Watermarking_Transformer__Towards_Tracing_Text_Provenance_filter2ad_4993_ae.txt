homograph set into two disjoint sets to encode ‘1’ and ‘0’ bits
(bit-holding words). To have a unique encoding and decoding,
we make sure no single word is assigned multiple values by
being found in different words’ synonym sets. Therefore, we
skip a word if it was already assigned a value.
To encode the message, we ﬁnd the occurrences of this list
of words in the sentence. We replace each word with a ‘1’ or
‘0’ synonym according to the current bit in the message. We
repeat until all bits are encoded. The decoding is then done
by simple dictionary lookups. We use a message length of 4
bits similar to our setup. To have unique decoding, we replace
any accidental occurrences of the ‘bit-holding’ words in the
original text with their corresponding synonym in the ‘replace’
list. This prevents unintentional encoding. We highlight this
important advantage of our model; AWT does not impose such
restrictions on the used words since there are no words that
are exclusive to the message encoding (as per Figure 8).
We again evaluate this baseline across the different evalua-
tion axes: effectiveness (utility and bit accuracy), secrecy, and
robustness. For effectiveness, we compute the bit accuracy
and SBERT distance. For secrecy, we train a transformer-
based classiﬁer with the same setup as in Section V-C. We
show a summary of these two evaluation factors in Table IX.
We compare the baseline against AWT at a comparable bit
accuracy level (resulted from sampling from the model) for a
fair comparison. We summarize our ﬁndings as follows: 1) The
message encoding was not successful in all sentences since not
all sentences have words from the ﬁxed ‘replace’ list. 2) At an
even higher bit accuracy level, AWT has a considerably lower
SBERT distance. 3) The baseline has a very high F1 score
compared to the F1 score of AWT.
For robustness, we apply the words removing and replacing
attacks as in Section V-D. We do not apply the DAE attack
since some words used in the baseline method might be Out-
of-Vocabulary words with respect
to the DAE. As shown
in Figure 10, the baseline is more sensitive to attacks since
the encoding changes a larger amount of words compared to
AWT. The ‘replace’ attack is even stronger than the ‘remove’
attack; not only can it remove the original ‘bit holding’
words, but it can also introduce accidental wrong encoding
by adding other ‘bit holding’ words instead of regular words.
This analysis shows that AWT achieves a signiﬁcantly better
trade-off between the three different evaluation axes.
Fig. 10: Comparing AWT and the synonym substitution base-
line bit accuracy under ‘remove’ and ‘replace’ attacks.
Fig. 11: AWD-LSTM with data hiding showing different
operating points that vary in perplexity and bit accuracy. The
baseline perplexity is the AWD-LSTM without data hiding.
2) Generation-based hiding: An alternative strategy to the
translation-based data hiding of the generated text (as a post-
processing step) is to generate text that is already encoded
with the input message [45]. Unlike previous generation-based
steganography work that relied on masking [45], we jointly
train a language model (in contrast to AWT, an autoencoder
and thus bidirectional) with a message decoder. We used the
same AWD-LSTM language model in [70]. In our case, it takes
the input word added to the input message at each time step
and is trained to predict the next word given previous words.
The message decoder takes the generated sequence and is
trained to reconstruct the input message. The model is trained
jointly with both losses. More details are in Appendix VIII-E.
We evaluate the model using the perplexity (i.e., exponential
of the model loss, lower is better) and the bit accuracy. The
ideal perplexity would be the perplexity of the AWD-LSTM
without data hiding. As shown in Figure 11, a very high bit
accuracy can be achieved with around 12 points increase in
perplexity (second operating point). The perplexity could be
further reduced by tuning the weights between the two losses,
which also decreases the bit accuracy.
However, the main limitation is that message accuracy fur-
ther drops during inference using recursive greedy decoding.
Although it improves with averaging 2 sentences, it indicates
that it would be even harder to retain high accuracy using
other decoding strategies that
introduce more variation in
generations, such as top-k or top-p sampling [5], [8], [13],
[87]. These strategies are typically used in open-ended gen-
erations due to having higher quality output [87]. In contrast,
AWT does not suffer from these discrepancies since it can be
applied agnostically on the generated sequence regardless of
the decoding strategies and the language model.
F. Human Evaluation
It is common for machine translation and generation tasks
to use human evaluation as an auxiliary evaluation besides the
other metrics [5], [72]. Therefore, we conducted a user study in
order to evaluate the naturalness and correctness of our model,
as a proxy to measure the stealthiness of the watermark.
The study is conducted on the best variant of the model
(with ﬁne-tuning) with the best-of-20 samples strategy (bit ac-
curacy: ∼86%) and on the synonym baseline in Section V-E1
(bit accuracy: ∼83%). It was performed by 6 judges who were
asked to rate sentences with a Likert scale from 0 (lowest) to 5
0.050.10.150.2Attack prob.60708090Bit accuracy (%)AWT - no attackSynonym - no attackAWT - replaceAWT - removeSynonym - replaceSynonym - remove1234Operating points708090100Baseline ppl.Test ppl.Acc. given test data (%)Acc. greedy decoding (%)Acc. greedy decoding (%),averaging two sentencesAWT
4.5±0.76
Synonym-baseline
3.42±1.16
Non-wm Dataset
4.65±0.62
TABLE X: The results of a user study to rate (0 to 5) sentences
from AWT, the baseline, and non-watermarked text.
(highest). The ratings are described with instructions that range
from: ‘This sentence is completely understandable, natural,
and grammatically correct’, to: ‘This sentence is completely
not understandable, unnatural, and you cannot get its main
idea’. We included different random sentences from AWT, the
synonym-based baseline, and the original non-watermarked
text, displayed in a randomized order. The non-watermarked
text works as a reference to the two approaches as the rating of
the original text might not always be ‘5’, since the dataset has
processing tokens that might make it ambiguous. We show the
average rating for each case in Table X. AWT had both higher
ratings and less variance than the baseline. The high variance
in the case of the baseline can be attributed to the observation
that not all sentences were successfully encoded with the full
4 bits, and therefore, some of the sentences did not have a lot
of changes. In the case of successful encoding, the sentence
generally undergoes a lot of changes compared to AWT, where
usually not all of them are consistent. More details about the
study are in Appendix VIII-F.
VI. DISCUSSION
We here discuss other several aspects of our work, other
assumptions, scope, and limitations.
a) Granularity: We focus on the threat scenario of news
articles that have a large number of tokens [5]. While other
threats such as misinformation on Twitter are important [88],
they are less relevant for machine-generated text that requires
longer context for generation or detection (e.g., up to 1024
tokens in [5] or at least 192 tokens in [13]). Although it is
possible to encode 4 bits in a short text using our approach,
this short message is not enough for conﬁdence calculation.
Veriﬁcation on short text would require a longer watermark
and thus, severely affect the text, as the task of data hiding in
text is inherently more difﬁcult than its counterpart in images.
b) False positives: When concatenating several 4 bits
messages, the false positives can be directly controlled by the
p-value threshold [84], since the accuracy of non-watermarked
text is at the chance level. We evaluated the thresholds of
0.05 and 0.01 (Figure 5). One possible way to improve false
positives is to use multiple conﬁdence thresholds with an
increasing alarm for false positives. Then, if the watermark
veriﬁcation is in the low-conﬁdence range, our solution could
potentially be combined with other previously introduced fake
news defenses (e.g., discriminators [5], [13], [89], automated
fact-checking and stance detection [90]). On the other hand,
human fact-checking is still a standard solution for news
veriﬁcation [91], while automated solutions aim to reduce
these human efforts, humans can still be kept in-the-loop for
verifying low-conﬁdence instances, reducing the otherwise full
effort to verify all articles.
c) Human editing: The black-box APIs might be used
legitimately for partial text completion or suggestions to some
of the sentences with further interactive human editing. How-
ever, the main threat we consider is misusing these models in
an unintended way to generate entire articles at scale, possibly
conditioned on a headline or a context. Although the threat of
combining the generation with human editing is conceivable,
it is a limited use-case for the adversary since it reduces the
scalability and adds manual time-consuming efforts, largely
reducing the advantages of using machine-generated text.
d) Possible release of models: We assume black-box
access to the language model, however, it is still an important
step towards defending against misuse. While GPT-2 was
released after a staged release, this might not be the case
for future models. By the time of writing this paper, OpenAI
is not open-sourcing GPT-3, and it is only available through
commercial APIs [15], where one of the announced reasons
is to prevent or limit misuse. Additionally, our solution is also
helpful for scenarios where a general language model like
GPT-2 is ﬁne-tuned by a service for speciﬁc tasks or domains.
e) Training in-house language models: Another option
for the adversary to circumvent defenses is to train their
own language model. However, training modern state-of-the-
art language models, including massive datasets collection, is
a very expensive and time-consuming process that requires
signiﬁcant technical expertise, and the cost is progressively
increasing. Training Grover [5] requires around $35k using
AWS. Training a 1.5 billion parameter model is estimated at
$1.6m [92]. The 175B GPT-3 training cost is estimated at
$4.6m [93]. Final actual costs could be even higher due to
multiple runs of hyperparameters tuning.
f) Watermarks regulation: Since we use a multi-bit wa-
termarking scheme, our scenario can be extended to water-
marking multiple models offered by different owners. How-
ever, this would require further cooperation of models’ owners
or a potential regulation by a trusted regulatory third party that
handles the distribution of watermarks, and sharing the wa-
termarks’ encoder and decoder. We hope that our work opens
follow-up future research and discussions on the regulation and
proactive protective release strategies of such technologies.
VII. CONCLUSION
In this paper, we present AWT, a new framework for lan-
guage watermarking as a potential solution towards marking
and tracing the provenance of machine-generated text. AWT
is the ﬁrst end-to-end data hiding solution for natural text
and is optimized to unobstructively encode the cover text by
adversarial training and other smoothing auxiliary losses.
AWT achieves more ﬂexibility and a signiﬁcantly better
trade-off between the different evaluation axes (effectiveness,
secrecy, and robustness), in terms of quantitative, qualitative,
and human evaluations, compared to a rule-based synonym
substitution baseline. Our work offers a new research area
towards improving and robustifying automatic data hiding in
natural language, similar to its precedent in images.
REFERENCES
[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances
in Neural Information Processing Systems, 2017.
[2] A. Conneau and G. Lample, “Cross-lingual language model pretraining,”
in Advances in Neural Information Processing Systems, 2019.
[3] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and
Q. V. Le, “Xlnet: Generalized autoregressive pretraining for language
understanding,” in Advances in Neural Information Processing Systems,
2019.
[4] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
of deep bidirectional transformers for language understanding,” in North
American Chapter of the Association for Computational Linguistics
(NAACL), 2019.
[5] R. Zellers, A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi, F. Roesner,
and Y. Choi, “Defending against neural fake news,” in Advances in
Neural Information Processing Systems, 2019.
[6] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and
L. Zettlemoyer, “Deep contextualized word representations,” in North
American Chapter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT), 2018.
[7] J. Howard and S. Ruder, “Universal language model ﬁne-tuning for
text classiﬁcation,” in the 56th Annual Meeting of the Association for
Computational Linguistics (ACL), 2018.
[8] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,
“Language models are unsupervised multitask learners,” OpenAI, Tech.
Rep., 2019.
[9] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving
language understanding by generative pre-training,” OpenAI, Tech. Rep.,
2018.
[10] L. Wang, W. Zhao, R. Jia, S. Li, and J. Liu, “Denoising based sequence-
to-sequence pre-training for text generation,” in Empirical Methods in
Natural Language Processing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP), 2019.
[11] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language models
are few-shot learners,” arXiv preprint arXiv:2005.14165, 2020.
[12] I. Solaiman, M. Brundage, J. Clark, A. Askell, A. Herbert-Voss, J. Wu,
A. Radford, and J. Wang, “Release strategies and the social impacts of
language models,” arXiv preprint arXiv:1908.09203, 2019.
[13] D. Ippolito, D. Duckworth, C. Callison-Burch, and D. Eck, “Automatic
detection of generated text is easiest when humans are fooled,” in the
58th Annual Meeting of the Association for Computational Linguistics
(ACL), 2020.
[14] D. I. Adelani, H. Mai, F. Fang, H. H. Nguyen, J. Yamagishi, and
I. Echizen, “Generating sentiment-preserving fake online reviews using
neural language models and their human-and machine-based detection,”
in International Conference on Advanced Information Networking and
Applications. Springer, 2020.
[15] OpenAI, “Openai api.” [Online]. Available: https://openai.com/blog/
openai-api/
[16] K. Krishna, G. S. Tomar, A. P. Parikh, N. Papernot, and M. Iyyer,
“Thieves on sesame street! model extraction of bert-based apis,” in
International Conference on Learning Representations (ICLR), 2020.
[17] T. Orekondy, B. Schiele, and M. Fritz, “Knockoff nets: Stealing func-
tionality of black-box models,” in the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2019.
[18] F. Tram`er, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, “Stealing
machine learning models via prediction apis,” in 25th USENIX Security
Symposium (USENIX Security 16), 2016.
[19] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and
A. Swami, “Practical black-box attacks against machine learning,” in
the ACM Asia Conference on Computer and Communications Security
(AsiaCCS), 2017.
[20] U. Topkara, M. Topkara, and M. J. Atallah, “The hiding virtues of
ambiguity: quantiﬁably resilient watermarking of natural language text
through synonym substitutions,” in the 8th Workshop on Multimedia and
Security, 2006.
[21] C. Y. Chang and S. Clark, “Practical linguistic steganography using
contextual synonym substitution and vertex colour coding,” in Empirical
Methods in Natural Language Processing (EMNLP), 2010.
[22] M. Topkara, U. Topkara, and M. J. Atallah, “Words are not enough:
sentence level natural language watermarking,” in the 4th ACM Inter-
national Workshop on Contents Protection and Security, 2006.
[23] H. M. Meral, E. Sevinc, E.
¨Unkar, B. Sankur, A. S.
¨Ozsoy, and
text watermarking,” in Security,
Inter-
T. G¨ung¨or, “Syntactic tools for
Steganography, and Watermarking of Multimedia Contents IX.
national Society for Optics and Photonics, 2007.
[24] Y.-L. Chiang, L.-P. Chang, W.-T. Hsieh, and W.-C. Chen, “Natural
language watermarking using semantic substitution for chinese text,”
in International Workshop on Digital Watermarking. Springer, 2003.
[25] O. Halvani, M. Steinebach, P. Wolf, and R. Zimmermann, “Natural
language watermarking for german texts,” in the ﬁrst ACM Workshop
on Information Hiding and Multimedia Security, 2013.
[26] I. Cox, M. Miller, J. Bloom, J. Fridrich, and T. Kalker, Digital Water-
marking and Steganography, 2nd ed. Morgan Kaufmann Publishers
Inc., 2007.
[27] J. Zhu, R. Kaplan, J. Johnson, and L. Fei-Fei, “Hidden: Hiding data with
deep networks,” in European Conference on Computer Vision (ECCV),
2018.
[28] S. Baluja, “Hiding images in plain sight: Deep steganography,” in
Advances in Neural Information Processing Systems, 2017.
[29] J. Hayes and G. Danezis, “Generating steganographic images via adver-
sarial training,” in Advances in Neural Information Processing Systems,
2017.
[30] V. Vukoti´c, V. Chappelier, and T. Furon, “Are deep neural networks good
for blind image watermarking?” in the IEEE International Workshop on
Information Forensics and Security (WIFS), 2018.
[31] R. Zhang, S. Dong, and J. Liu, “Invisible steganography via generative
adversarial networks,” Multimedia Tools and Applications, vol. 78, no. 7,
pp. 8559–8575, 2019.
[32] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
with neural networks,” in Advances in Neural Information Processing
Systems, 2014.
[33] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in
Advances in Neural Information Processing Systems, 2014.
[34] N. S. Kamaruddin, A. Kamsin, L. Y. Por, and H. Rahman, “A review
of text watermarking: theory, methods, and applications,” IEEE Access,
vol. 6, pp. 8011–8028, 2018.
[35] C. I. Podilchuk and E. J. Delp, “Digital watermarking: algorithms and
applications,” IEEE Signal Processing Magazine, vol. 18, no. 4, pp. 33–
46, 2001.
[36] N. Singh, M. Jain, and S. Sharma, “A survey of digital watermarking
techniques,” International Journal of Modern Communication Technolo-
gies and Research, vol. 1, no. 6, p. 265852, 2013.
[37] J. T. Brassil, S. Low, N. F. Maxemchuk, and L. O’Gorman, “Electronic
marking and identiﬁcation techniques to discourage document copying,”
IEEE Journal on Selected Areas in Communications, vol. 13, no. 8, pp.
1495–1504, 1995.
[38] M. Topkara, C. M. Taskiran, and E. J. Delp III, “Natural language water-
marking,” in Security, Steganography, and Watermarking of Multimedia
Contents VII.
International Society for Optics and Photonics, 2005.
[39] M. Topkara, G. Riccardi, D. Hakkani-T¨ur, and M. J. Atallah, “Natural
language watermarking: Challenges in building a practical system,”
in Security, Steganography, and Watermarking of Multimedia Contents
VIII.
International Society for Optics and Photonics, 2006.
[40] H. M. Meral, B. Sankur, A. S. ¨Ozsoy, T. G¨ung¨or, and E. Sevinc¸, “Natu-
ral language watermarking via morphosyntactic alterations,” Computer
Speech & Language, vol. 23, no. 1, pp. 107–125, 2009.
[41] A. Wilson, P. Blunsom, and A. D. Ker, “Linguistic steganography on
twitter: hierarchical language modeling with manual interaction,” in
Media Watermarking, Security, and Forensics.
International Society
for Optics and Photonics, 2014.
[42] A. Wilson, P. Blunsom, and A. Ker, “Detection of steganographic