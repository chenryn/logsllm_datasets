𝑁 𝑀𝑇 + 𝐻(𝑢,𝑣)∈K2 𝐿𝑢𝑣I [𝑡𝑢𝑣 > 0] .
2𝐸
Let ˆ𝑇 = [ˆ𝑡𝑢𝑣] be the maximal traffic matrix that minimizes the right
side of Equation 1. We observe that it also minimizes the last term
above, and we have
Using Lemma 8.3 and Lemma 8.1, we have;
𝜃∗ − 𝜃𝑙𝑏 (𝑇) ≤
(𝑢,𝑣)∈K2
(𝑁 𝑀 ˆ𝑇 + 𝐻 

𝐿𝑢𝑣I(cid:2)ˆ𝑡𝑢𝑣 > 0(cid:3) ≥
− 1) − 𝑅 − 𝐻
𝑅 − 𝐻 − 2
(𝑢,𝑣)∈K2
𝑑( 𝑁
𝐻
𝐿𝑢𝑣 I(cid:2)ˆ𝑡𝑢𝑣 > 0(cid:3))(𝐻 
2𝐸𝑁 𝑀 ˆ𝑇
𝐿𝑢𝑣 I(cid:2)ˆ𝑡𝑢𝑣 > 0(cid:3)) .
(16)
(𝑢,𝑣)∈K2
(cid:32) (𝑅 − 𝐻 − 1)𝑑 − 1
𝑅 − 𝐻 − 2
(cid:33)
− 𝑑
= 𝐷.
(17)
Equation 16 and Equation 17 lead to
𝜃∗ − 𝜃𝑙𝑏(𝑇) ≤
2𝐸𝑁 𝑀 ˆ𝑇
(𝑁 𝑀 ˆ𝑇 + 𝐻𝐷)(𝐻𝐷)
Since the above inequality holds for every 𝑇 ∈ ˆT , it holds at the
worst-case gap
𝜃𝑢𝑏 − min
𝑇 ∈ ˆT
𝜃𝑙𝑏(𝑇) ≤
2𝐸𝑁 𝑀 ˆ𝑇
(𝑁 𝑀 ˆ𝑇 + 𝐻𝐷)(𝐻𝐷) .
Similar to Corollary 1, we can prove that above inequality goes
to 0 as 𝑁 increases because every 𝑀𝑇 is bounded by a constant
independent of 𝑁 under Assumption 2.
□
F Throughput of bi-regular Clos topologies
under tub
tub is tight for bi-regular Clos topologies as well, giving throughput
equal to 1 for different topology sizes (Table A.1).
#Layers
tub
N
1.00
8192
1.00
32768
1.00
131072
Table A.1: Clos: tub is always 1.
#SWs
1280
7168
28672
3
4
4
G Proof of Corollary 1
Proof. This follows directly from Equation 2 in Theorem 4.1.
We can show that, in 𝐷, the term containing 𝑁𝑑 dominates the
other terms for large enough 𝑁 . This is a direct consequence of
defining 𝑑 as the minimum diameter that required to accommodate
𝑁/𝐻 switches (Moore bound [39]). As a result, in the RHS of the
Equation 2, the numerator grows as 𝑁 and the denominator grows
as 𝑁𝑑. Therefore, 𝜃∗ approaches zero with increasing 𝑁 , so there
must always exist a 𝑁∗ at which 𝜃∗ falls below 1.
□
H Path-based Multi-commodity Flow LP
formulation
In this section, we briefly introduce the path-based MCF formula-
tion (common in WAN traffic engineering [33]) used throughout
the paper. Given a traffic matrix 𝑇 = [𝑡𝑢𝑣] and set of paths between
every pair of switches with servers (P𝑢𝑣), the throughput of the
traffic matrix is the solution to the following LP formula in which
𝑓𝑝 denotes the amount of flow on path 𝑝;
subject to 𝑝∈P𝑢𝑣
maximize
𝜃
(𝑢,𝑣)∈K2𝑝∈P𝑢𝑣
𝑓𝑝 ≥ 𝜃𝑡𝑢𝑣
𝑓𝑝 ≥ 0
∀(𝑢, 𝑣) ∈ K2
𝑓𝑝 I [𝑒 ∈ 𝑝] ≤ 1
∀(𝑢, 𝑣) ∈ K2,∀𝑝 ∈ P𝑢𝑣,
∀𝑒 ∈ E
where E is the set of directional links with unit capacity.
I Metric Adjustments for FatClique
In a FatClique, the number of servers attached to each switch can
differ by at most 1. To generalize the maximal permutation traffic
matrix generation to accommodate this case, we changed weight
assignment of edges in the complete bipartite graph from 𝑤𝑢→𝑣 =
𝐿𝑢𝑣 to 𝑤𝑢→𝑣 = 𝐿𝑢𝑣 min(𝐻𝑢, 𝐻𝑣). The latter weight assignment
takes into account the maximum amount of flow between each 𝑢, 𝑣
pair along with their distance. More precisely, if in a permutation
traffic matrix 𝑡𝑢𝑣 is non zero, it should be the minimum of 𝐻𝑢 and
𝐻𝑣 since it should conform to the hose-model traffic constraints §2.
So, Equation 1 can be re-written as;
(𝑢,𝑣)∈K2 𝐿𝑢𝑣 min(𝐻𝑢, 𝐻𝑣)I [𝑡𝑢𝑣 > 0]
2𝐸
𝜃∗ ≤ min
𝑇 ∈ ˆT
(18)
Equation 18 is exactly same as Equation 1 when all the switches
have exactly the same 𝐻. To find the maximal permutation traf-
fic matrix, we need to find the traffic matrix that minimizes the
LHS of Equation 18. This is equivalent to solving the maximum
weight matching in a bipartite graph (§2), with the revised weight
assignment.
This approach does not yield the global minimum of the through-
put bound since Theorem 2.1 does not hold when H differs accross
the switches. A linear programming (LP) formulation can compute
the global minimum [31]. However, we use our matching method
to infer the maximal permutation traffic matrix for FatClique, for
three reasons. First, in FatClique, the number of servers connected
to each switch can differ only by 1, so the difference between global
minimum and throughput bound computed using this approach is
negligible. Second, algorithms for solving maximal weight matching
are more efficient than solving an LP. Third, the permutation traffic
A Throughput-Centric View of the Performance of Datacenter Topologies
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
Figure A.2: Topology Cost (Jellyfish vs Fat-tree). The relative difference
of the maximum servers supported at full throughput (per tub) between
Jellyfish and Fat-tree built with the same equipment using {14, 24, 32, 48, 56,
64, 68, 72, 78, 84, 90, 98}-port switches averaged over 5 runs.
matrix generated using our approach is harder to route compared
to an LP generated traffic matrix.
J Throughput Gap for different values of K
Figure A.5 illustrates the absolute difference between path-based
multi-commodity flow over 𝐾-shortest paths and our throughput
bound for different values of 𝐾 (i.e., throughput gap). The results
for 𝐾 = 60, 100, 200 are very similar to each other; a gap of non-
zero for small size topologies, followed by a close-to-zero gap for
larger instances. The only exception is some instances of FatClique
exhibit large throughput gaps in the 5K – 15K compared to Jelly-
fish and Xpander because FatClique cannot fully utilize available
capacity with 𝐾 = 60, 100 for KSP-MCF. However, after increasing
𝐾 to 200 (Figure 5(l)), the throughput gap behavior for FatClique is
comparable to Jellyfish and Xpander.
For 𝐾 = 20, the gap remains significant even at large topolo-
gies since 20-shortest paths does not provide enough diversity to
completely exploit the network capacity, and some of the capacity
remains unused.
K Scaling of Throughput-based Cost
Comparison
Other than bisection bandwidth, Jellyfish [44] and Xpander [47]
used full throughput of random permutations and all-to-all traffic
matrices under MCF to assess the cost advantage of their topolo-
gies. However, throughput under random permutations and all-
to-all traffic matrices can be significantly larger than (worst-case)
throughput [27]. Moreover, as discussed in §3.1, MCF and KSP-
MCF can not scale to the size of current datacenters. In this section,
we show how conclusions can change when using our bound to
perform cost comparisons at larger scale.
Jellyfish. Singla et al. [44] have shown that at the scale of <900
servers Jellyfish can support 27% more servers than a Fat-tree [1]
built with same equipment, and conjecture that this cost advan-
tage increases by using a higher radix switch. Figure A.2 shows
the relative difference of the maximum servers between Jellyfish
and Fat-tree for different switch radices. Using tub, at the scale of
686 servers (𝑅 = 14, which is the largest scale considered in [44]),
Jellyfish can support only 8% more servers than a (same equipment)
Fat-tree (the leftmost point in Figure A.2), dropping the cost ad-
vantage of Jellyfish by 3x. Moreover, using a higher radix switch
Figure A.3: Topology Cost (Xpander vs Fat-tree). Number of Switches
required to support 𝑁 servers. Percentages are Xpander/Fat-tree.
does not result in higher cost advantage of Jellyfish over Fat-tree.
In fact, using a higher radix switch might result in drop in the cost
advantage. For example, using 98-port switches instead of 64-port
causes the cost advantage to drop slightly from 25% to 22%.
Xpander. Valadarsky et al. [47] have shown that at the scale of
<4K servers, Xpander can support the same number of servers as
Fat-Tree [1] at full throughput using 80% – 85% of the switches.
As Figure A.3 shows at the maximum considered scale in [47]
(3.5K servers, the left most point), Xpander should use more than
95% switches compared to the same size Fat-tree. However, as the
scale grows, the cost advantage of Xpander over Fat-tree increases,
matching the numbers reported in [47].
L Throughput of uni-regular topologies under
expansion
Jellyfish [44] and Xpander [47] have shown that using a very sim-
ple expansion algorithm (random rewiring), their design can be
expanded to any size with minor throughput loss while preserv-
ing the number of servers per switch 𝐻. Jellyfish uses bisection
bandwidth as their throughput metric while Xpander assesses the
throughput by solving MCF on all-to-all traffic matrix.
Jellyfish. In §5.1, we show that Jellyfish requires advanced plan-
ning in order to preserve full throughput, otherwise, even very
small expansion can turn Jellyfish into a topology with less than
full throughput. To better understand the amount of throughput
degradation, Figure A.4 shows the throughput (computing using
tub), normalized by the topologies initial throughput (before ex-
pansion). At each step, we expand the topology by 20% of the initial
size until its size reaches the 2.6x of the initial topology. For 10K
servers, Figure A.4 shows that throughput drops by more than 20%
when expanding the topology by only 0.6x. On the other hand,
when the initial topology size is 32K, throughput drop is negligible
(<1%). We emphasize that these results are consistent with §4.2;
Jellyfish with H=6 and initial size 8K has full throughput even after
expanding by 2.6x. However, it faces the throughput drop as well.
This suggests that operators should be cautious when expand-
ing uni-regular topologies depending on the topology’s initial and
target size as they might face significant throughput drops. tub,
therefore, helps topology designers to identify and understand these
scenarios before deploying and expanding their desired topology.
Xpander. Using tub to assess the Xpander’s performance under
expansion results in similar conclusions as expanding Jellyfish does.
Similar to Jellyfish, operators who adopt Xpander should have the
0200k400k600k800k1M1.2 MEquipment Cost (#Ports)510152025Servers at Full throughput (%)020k40k60k80k#Servers (N)80859095Switches at Full throughput (%)SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
Namyar .et al.
Figure A.4: Throughput of uni-regular topologies under expansion.
target size in mind and choose 𝐻 accordingly. Otherwise, they either
end up having a topology with less than full throughput or have
to rewire the servers, bearing a significant cost. The throughput
degradation is also very similar to Jellyfish (Figure A.4); at some
scales (e.g., 10K), expanding the Xpander even by a very small ratio
degrades the throughput by as much as 25%.
1.01.21.41.61.82.02.22.42.6Expansion Ratio0.750.800.850.900.951.00Normalized ThroughputInit N=10K, H=6Init N=10K, H=7Init N=10K, H=8Init N=32K, H=6Init N=32K, H=7Init N=32K, H=8A Throughput-Centric View of the Performance of Datacenter Topologies
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
(a) Jellyfish, K=20
(b) Jellyfish, K=60
(c) Jellyfish, K=100
(d) Jellyfish, K=200
(e) Xpander, K=20
(f) Xpander, K=60
(g) Xpander, K=100
(h) Xpander, K=200
(i) FatClique, K=20
(j) FatClique, K=60
(k) FatClique, K=100
(l) FatClique, K=200
Figure A.5: Throughput bound vs K-shortest paths Multi-commodity flow for different values of K (20, 60, 100, 200).
05k10k15k20k25k#Servers (N)0.00.20.40.60.81.0Throughput GapH=6H=7H=805k10k15k20k25k#Servers (N)0.00.20.40.60.81.0Throughput GapH=6H=7H=805k10k15k20k25k#Servers (N)0.00.20.40.60.81.0Throughput GapH=6H=7H=805k10k15k20k25k#Servers (N)0.00.20.40.60.81.0Throughput GapH=6H=7H=805k10k15k20k25k#Servers (N)0.00.20.40.60.81.0Throughput GapH=6H=7H=805k10k15k20k25k#Servers (N)0.00.20.40.60.81.0Throughput GapH=6H=7H=805k10k15k20k25k#Servers (N)0.00.20.40.60.81.0Throughput GapH=6H=7H=805k10k15k20k25k#Servers (N)0.00.20.40.60.81.0Throughput GapH=6H=7H=805k10k15k20k25k#Servers (N)0.00.20.40.60.81.0Throughput GapH [5.5, 6.5)H [6.5, 7.5)H [7.5, 8]05k10k15k20k25k#Servers (N)0.00.20.40.60.81.0Throughput GapH [5.5, 6.5)H [6.5, 7.5)H [7.5, 8]05k10k15k20k25k#Servers (N)0.00.20.40.60.81.0Throughput GapH [5.5, 6.5)H [6.5, 7.5)H [7.5, 9]05k10k15k20k25k#Servers (N)0.00.20.40.60.81.0Throughput GapH [7.5, 9]H [6.5, 7.5)H [5.5, 6.5)