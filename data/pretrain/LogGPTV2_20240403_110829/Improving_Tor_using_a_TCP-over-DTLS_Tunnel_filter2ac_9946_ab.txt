along the circuit instead of one, and also because traf-
ﬁc from multiple circuits can be amalgamated into one
packet for transmission along the shared connection. As
a result, a ﬁxed drop rate affecting the remaining connec-
Timing
Client
Timing
Client
Timing
Client
Timing
Client
Timing
Client
Timing
Client
Timing
Client
Timing
Client
OP
OP
OP
OP
OP
OP
OP
OP
OR
OR
OR
OR
Remaining Links 
OR
OR
Shared
Link
Timing
Server
Figure 4: Setup for Experiment 1. The shared link multiplexes all circuits from the various OPs to the ﬁnal OR; the
remaining links carry just one or two circuits each. The splay of links between the ﬁnal OR and the timing server
reﬂect the fact that a separate TCP connection is made from the ﬁnal OR to the timing server for each timing client.
Conﬁguration
No dropping
0.1 % (remaining)
0.1 % (shared)
Throughput
Effective
Circuit
Network
Throughput Throughput Degradation Drop
(KB/s)
Rate
0 %
221 ± 6.6
0.08 %
208 ± 14
184 ± 17
0.03 %
(KB/s)
36.9 ± 1.1
34.7 ± 2.3
30.8 ± 2.8
0 %
6 %
17 %
Table 1: Throughput for different dropping conﬁgurations. Network throughput is the total data sent along all the
circuits.
Conﬁguration
No dropping
0.1 % (remaining)
0.1 % (shared)
Average
Latency
933 ± 260 ms
983 ± 666 ms
1053 ± 409 ms
Effective
Latency
Increase Drop Rate
0 %
5.4 %
12.9 %
0 %
0.08 %
0.03 %
Table 2: Latency for different dropping conﬁgurations.
tions will result in more frequent packet drops than one
dropping only along the shared connection. This dispar-
ity is presented explicitly in our results as the effective
drop rate; i.e., the ratio of packets dropped to the total
number of packets we observed (including those ineligi-
ble to be dropped) in the experiment.
The results of Experiment 1 are shown in Tables 1
and 2. They show the results for three conﬁgurations:
when no packet dropping is done, when 0.1% of packets
are dropped on all connections except the heavily shared
one, and when 0.1% of packets are dropped only on the
shared connection. The degradation column refers to
the loss in performance as a result of introducing packet
drops. The average results for throughput and delay were
accumulated over half a dozen executions of the exper-
iment, and the mean intervals for the variates are com-
puted using Student’s T distribution to 95% conﬁdence.
These results conﬁrm our hypothesis. The throughput
degrades nearly threefold when packets are dropped on
the shared link instead of the remaining links. This is
despite a signiﬁcantly lower overall drop rate. The be-
haviour of one TCP connection can adversely affect all
correlated circuits, even if those circuits are used to trans-
port less data.
Table 2 suggests that latency increases when packet
dropping occurs. Latency is measured by the time re-
quired for a single cell to travel alongside a congested
circuit, and we average a few dozen such probes. Again
we see that dropping on the shared link more adversely
affects the observed delay despite a reduced drop rate.
However, we note that the delay sees wide variance, and
the 95% conﬁdence intervals are quite large.
2.3 Summary
Multiplexing circuits over a single connection is a poten-
tial source of unnecessary latency since it causes TCP’s
congestion control mechanism to operate unfairly to-
wards connections with smaller demands on throughput.
High-bandwidth streams that trigger congestion control
result in low-bandwidth streams having their congestion
window unfairly reduced. Packet dropping and reorder-
ing also cause available data for multiplexed circuits to
wait needlessly in socket buffers. These effects degrade
both latency and throughput, which we have shown in
experiments.
To estimate the magnitude of this effect in the real Tor
network, we note that 10% of Tor routers supply 87%
of the total network bandwidth [8]. A straightforward
calculation shows that links between top routers—while
only comprising 1% of the possible network links—
transport over 75% of the data. At the time of writing,
the number of OPs is estimated in the hundreds of thou-
sands and there are only about one thousand active ORs
[14]. Therefore, even while most users are idle, the most
popular 1% of links will be frequently multiplexing cir-
cuits.
Ideally, we would open a separate TCP connection for
every circuit, as this would be a more appropriate use of
TCP between ORs; packet drops on one circuit, for ex-
ample, would not hold up packets in other circuits. How-
ever, there is a problem with this naive approach. An
adversary observing the network could easily distinguish
packets for each TCP connection just by looking at the
port numbers, which are exposed in the TCP headers.
This would allow him to determine which packets were
part of which circuits, affording him greater opportunity
for trafﬁc analysis. Our solution is to tunnel packets from
multiple TCP streams over DTLS, a UDP protocol that
provides for the conﬁdentiality of the trafﬁc it transports.
By tunnelling TCP over a secure protocol, we can protect
both the TCP payload and the TCP headers.
3 Proposed Transport Layer
This section proposes a TCP-over-DTLS tunnelling
transport layer for Tor. This tunnel transports TCP
packets between peers using DTLS—a secure datagram
(UDP-based) transport [9]. A user-level TCP stack run-
ning inside Tor generates and parses TCP packets that
are sent over DTLS between ORs. Our solution will use
a single unconnected UDP socket to communicate with
all other ORs at the network level. Internally, it uses a
separate user-level TCP connection for each circuit. This
decorrelates circuits from TCP streams, which we have
shown to be a source of unnecessary latency. The use of
DTLS also provides the necessary security and conﬁden-
tiality of the transported cells, including the TCP header.
This prevents an observer from learning per-circuit meta-
data such data how much data is being sent in each di-
rection for individual circuits. Additionally, it reduces
the number of sockets needed in kernel space, which
is known to be a problem that prevents some Windows
computers from volunteering as ORs. Figure 5 shows
the design of our proposed transport layer, including how
only a single circuit is affected by a dropped packet.
The interference that multiplexed circuits can have on
each other during congestion, dropping, and reordering
is a consequence of using a single TCP connection to
transport data between each pair of ORs. This proposal
uses a separate TCP connection for each circuit, ensur-
ing that congestion or drops in one circuit will not affect
other circuits.
3.1 A TCP-over-DTLS Tunnel
DTLS [9] is the datagram equivalent to the ubiquitous
TLS protocol [1] that secures much trafﬁc on the Inter-
UDP Stream (over network)
OR
Kernel UDP 
RXed Packets
Readable
m
User TCP
Buffered/Waiting
Readable
OR
Figure 5: Proposed TCP-over-DTLS Transport showing decorrelated streams. Shades correspond to cells for different
circuits (cf. Figure 3).
net today, including https web trafﬁc, and indeed Tor.
DTLS provides conﬁdentiality and authenticity for In-
ternet datagrams, and provides other security properties
such as replay prevention. IPsec [6] would have been
another possible choice of protocol to use here; how-
ever, we chose DTLS for our application due to its ac-
ceptance as a standard, its ease of use without kernel or
superuser privileges, and its existing implementation in
the OpenSSL library (a library already in use by Tor).
The TLS and DTLS APIs in OpenSSL are also uniﬁed;
after setup, the same OpenSSL calls are used to send and
receive data over either TLS or DTLS. This made sup-
porting backwards compatibility easier: the Tor code will
send packets either over TCP (with TLS) or UDP (with
DTLS), as appropriate, with minimal changes.
Our new transport layer employs a user-level TCP
stack to generate TCP packets, which are encapsulated
inside a DTLS packet that is then sent by the system in
a UDP/IP datagram. The receiving system will remove
the UDP/IP header when receiving data from the socket,
decrypt the DTLS payload to obtain a TCP packet, and
translate it into a TCP/IP packet, which is then forwarded
to the user-level TCP stack that processes the packet. A
subsequent read from the user-level TCP stack will pro-
vide the packet data to our system.
In our system, the TCP sockets reside in user space,
and the UDP sockets reside in kernel space. The use
of TCP-over-DTLS affords us the great utility of TCP:
guaranteed in-order delivery and congestion control. The
user-level TCP stack provides the functionality of TCP,
and the kernel-level UDP stack is used simply to trans-
mit packets. The secured DTLS transport allows us to
protect the TCP header from snooping and forgery and
effect a reduced number of kernel-level sockets.
ORs require opening many sockets, and so our user-
level TCP stack must be able to handle many concur-
rent sockets, instead of relying on the operating sys-
tem’s TCP implementation that varies from system to
system.
In particular, some discount versions of Win-
dows artiﬁcially limit the number of sockets the user
can open, and so we use Linux’s free, open-source,
and high-performance TCP implementation inside user
space. Even Windows users will be able to beneﬁt from
an improved TCP implementation, and thus any user of
an operating system supported by Tor will be able to vol-
unteer their computer as an OR if they so choose.
UDP allows sockets to operate in an unconnected
state. Each time a datagram is to be sent over the Inter-
net, the destination for the packet is also provided. Only
one socket is needed to send data to every OR in the Tor
network. Similarly, when data is read from the socket,
the sender’s address is also provided alongside the data.
This allows a single socket to be used for reading from
all ORs; all connections and circuits will be multiplexed
over the same socket. When reading, the sender’s ad-
dress can be used to demultiplex the packet to determine
the appropriate connection for which it is bound. What
follows is that a single UDP socket can be used to com-
municate with as many ORs as necessary; the number of
kernel-level sockets is constant for arbitrarily many ORs
with which a connection may be established. This will
become especially important for scalability as the num-
ber of nodes in the Tor network grows over time. From
a conﬁguration perspective, the only new requirement is
that the OR operator must ensure that a UDP port is ex-
ternally accessible; since they must already ensure this
for a TCP port we feel that this is a reasonable conﬁgu-
ration demand.
Figure 6(a) shows the packet format for TCP Tor, and
Figure 6(b) shows the packet format for our TCP-over-
DTLS Tor, which has expanded the encrypted payload to
include the TCP/IP headers generated by the user-level
TCP stack. The remainder of this section will discuss
how we designed, implemented, and integrated these
changes into Tor.
3.2 Backwards Compatibility
Our goal is to improve Tor to allow TCP communica-
tion using UDP in the transport layer. While the origi-
nal ORs transported cells between themselves, our pro-
posal is to transport, using UDP, both TCP headers and
cells between ORs. The ORs will provide the TCP/IP
packets to a TCP stack that will generate both the ap-
propriate stream of cells to the Tor application, as well
IP
TCP
TLS
Application Payload
(a)
IP
UDP
DTLS
TORTP
Application Payload
(b)
Figure 6: Packets for TCP Tor and our TCP-over-DTLS improved Tor. Encrypted and authenticated components
of the packet are shaded in grey. (a) shows the packet format for TCP Tor. (b) shows the packet format for our
TCP-over-DTLS Tor. TORTP is a compressed form of the IP and TCP headers, and is discussed in Section 3.4.
as TCP/IP packets containing TCP acknowledgements to
be returned.
The integration of this transport layer into Tor has two
main objectives. The ﬁrst is that of interoperability; it is
essential that the improved Tor is backwards compatible
with the TCP version of Tor so as to be easily accepted
into the existing codebase. Recall that Tor has thousands
of ORs, a client population estimated in the hundreds of
thousands, and has not experienced any downtime since
it launched in 2003. It is cumbersome to arrange a syn-
chronized update of an unknown number of anonymous
Tor users. A subset of nodes that upgrade and can take
advantage of TCP-over-DTLS can provide evidence of
the transport’s improvement for the user experience—
this incremental upgrade is our preferred path to accep-
tance. Our second objective is to minimize the changes
required to the Tor codebase. We must add UDP con-
nections into the existing datapath by reusing as much
existing code as possible. This permits future developers
to continue to improve Tor’s datapath without having to
consider two classes of communication. Moreover, it en-
courages the changes to quickly be incorporated into the
main branch of the source code. While it will come with
a performance cost for doing unnecessary operations, we
perform timing analyses below to ensure that the result-
ing datapath latency remains negligible.
Interoperability between existing ORs and those using
our improved transport is achieved by fully maintaining
the original TCP transport in Tor—improved ORs con-
tinue to advertise a TCP OR port and multiplexed TCP
connections can continue to be made. In addition, im-
proved nodes will also advertise a UDP port for making
TCP-over-DTLS connections. Older nodes will ignore
this superﬂuous value, while newer nodes will always
choose to make a TCP-over-DTLS connection whenever
such a port is advertised. Thus, two UDP nodes will au-
tomatically communicate using UDP without disrupting
the existing nodes; their use of TCP-over-DTLS is in-
consequential to the other nodes. As more nodes support
TCP-over-DTLS, more users will obtain its beneﬁts, but
we do not require a synchronized update to support our
improvements.
Clients of Tor are not required to upgrade their soft-
ware to obtain the beneﬁts of UDP transport.
If two
nodes on their circuit use TCP-over-DTLS to commu-
nicate then this will happen transparently to the user. In
fact, it is important that the user continue to choose their
circuit randomly among the ORs: intentionally choosing
circuits consisting of UDP nodes when there are only
a few such nodes decreases the privacy afforded to the
client by rendering their circuit choices predictable.
3.3 User-level TCP Stack
If we simply replaced the TCP transport layer in Tor
with a UDP transport layer, our inter-OR communica-
tion would then lack the critical features of TCP: guaran-
teed in-order transmission of streams, and the most well-
studied congestion control mechanism ever devised. We
wish to remove some of the unnecessary guarantees of
TCP for the sake of latency; i.e., we do not need cells
from separate circuits over the same connection to arrive
in the order they were dispatched. However, we must
still be able to reconstruct the streams of each individ-
ual circuit at both ends of the connection. We use a TCP
implementation in user space (instead of inside the oper-
ating system) to accommodate us; a user-level TCP stack
provides the implementation of the TCP protocols [4] as
part of our program. User-level socket ﬁle descriptors
and their associated data structures and buffers are ac-
cessible only in user space and so are visible and relevant
only to Tor. We use the UDP transport layer and DTLS
to transport TCP packets between the UDP peers. Only
part of the TCP packet is transmitted; the details will be