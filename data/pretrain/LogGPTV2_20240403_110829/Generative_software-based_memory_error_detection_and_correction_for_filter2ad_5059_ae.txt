### Static and Runtime Overhead

Although the previously discussed protection mechanisms enhance system resiliency, they introduce both static and dynamic costs. In this section, we will present a detailed analysis of these costs in relation to the benefits gained.

#### Code Size and Runtime Measurements

Our measurements focus on code size and runtime to evaluate the overhead introduced by the protection mechanisms. The benchmarks illustrate that, in many cases, the cost of our protection schemes is justified by the increased system robustness. 

- **Static Overhead**: This refers to the increase in code size due to the additional instructions required for error detection and correction.
- **Dynamic Overhead**: This includes the runtime impact, such as the time taken to execute the additional instructions and handle exceptions.

#### Benchmark Analysis

Benchmarks can be classified into two categories based on their interaction with the scheduler:

1. **Infrequent Scheduler Access**:
   - These benchmarks access the scheduler infrequently and thus have minimal runtime overhead. For example, SYNc2, which continuously bombards the scheduler, shows a significant slowdown on real hardware. However, for most other benchmarks, the runtime overhead is negligible (less than 1%).
   
2. **Frequent Scheduler Access**:
   - Benchmarks like EXCEPTI, which frequently interact with the scheduler, experience higher runtime overhead. Figure 5 illustrates that the runtime for these benchmarks can be up to 18 times longer for CRC and SUM+DMR protection, and 57 times longer for Hamming code. The Hamming protection scheme, in particular, consistently causes a tenfold increase in runtime, making it impractical for use.

#### Detailed Runtime Overhead

- **SUM+DMR and CRC Variants**: These variants show a runtime overhead of only 0.09% for the entire benchmark suite, indicating that they are highly efficient.
- **Hamming Code**: This variant has a significantly higher overhead, with some benchmarks running 468 times slower. It is generally not recommended unless a mix of computation and frequent scheduler invocations is expected.
- **TMR (Triple Modular Redundancy)**: TMR offers no real benefit over other mechanisms and should be avoided unless specific fault conditions (e.g., single-bit and 8-bit burst errors) are a concern.

#### Static Overhead

Figure 4 shows the static overhead in terms of binary sizes. The DATA sections remain mostly constant, while the TEXT segment grows due to the additional CPU instructions required for each protection mechanism. The average increase in code size varies from 58% (SUM+DMR) to 146% (Hamming).

### Related Work

#### Susceptibility Analyses of Operating Systems to Memory Errors

Several studies have addressed the assessment of operating systems in the presence of hardware faults. Kao et al. [26] and Fabre et al. [27] injected memory faults into the kernel address space of UNIX-like operating systems. More recent work, such as Madeira et al. [28], has analyzed the Linux kernel under similar conditions.

#### Hardware-Supported Memory Protection

Commercial ECC DIMMs provide single-bit-error correction and double-bit-error detection (SEC-DED) with an overhead of 12.5%. Chipkill technology improves this by correcting multi-bit errors, but at the cost of up to 30% higher energy consumption. MMUs have been proposed to manage page-level EDMs, but these solutions often require significant hardware modifications and are not widely adopted.

#### Software-Based Memory Protection

Aspect-oriented programming (AOP) has been used to manage memory errors. For example, Alexandersson et al. [38] implemented triple-time-redundant execution in AspectC++, which led to a 300% runtime overhead. Other approaches, such as Aumann et al. [39] and Taylor et al. [18], have focused on designing robust data structures that can detect and repair errors at runtime. However, these methods often require deep knowledge of the software and can be difficult to apply to existing systems.

### Conclusions

In this paper, we presented a generic approach to software-based fault tolerance that outperforms related works in terms of total performance overhead. Our approach reduces the number of system failures caused by errors in eCos' scheduler and thread data structures from 12.8% to below 0.01%, with a runtime overhead of only 0.09-1.75%. Additionally, our method can detect software bugs, which is a valuable side effect. Future work will focus on reducing the code bloat caused by the large number of instantiations in the generic code.

We thank the anonymous reviewers for their helpful and encouraging comments. This work was partly supported by the German Research Foundation (DFG) program SPP 1500 under grant no. A4.