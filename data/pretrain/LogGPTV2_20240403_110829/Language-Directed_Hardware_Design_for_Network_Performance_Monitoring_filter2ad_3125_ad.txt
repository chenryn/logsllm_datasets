operate independently on each switch, operate independently on each
packet, or are associative and commutative. We describe below how
we check the first condition, failing which we simply check the last
two conditions syntactically: either the groupby aggregates by uid
(condition 2) or contains programmer annotations assoc and comm
(condition 3).
To check if an aggregation operates independently on each switch,
we label each AST node with an additional boolean attribute, switch-
partitioned, corresponding to whether the output stream has been
partitioned by the switch at which it appears. Intuitively, if a stream
is switch-partitioned, we allow packet-order-dependent aggregations
over multiple packets of that stream; otherwise, we do not.
filtergroupmapzipPktstreamPktstreamS1,	S2S1,	S2allS1,	S2allallFalseTrueFalseFalseFalseFalseLanguage-Directed Hardware Design
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
Figure 6: Steps for compiling linear-in-state updates.
We take a pragmatic approach and sacrifice completeness, but
still cover useful functions. Specifically, we only detect linear-in-
state state updates through simple syntactic pattern matching in
the compiler (i.e., without algebraic transformations). Despite these
simplifications, the Marple compiler correctly identifies all the linear-
in-state aggregations in Fig. 7 and targets the multiply-accumulate
instruction that we added to the Banzai pipeline.
To describe how linear-in-state detection works, we introduce
some terminology. Recall that an aggregation function takes two
arguments (§2): a list of state variables (e.g., a counter) and a list
of tuple fields (e.g., the TCP sequence number). We use the term
variable in this subsection to refer to either a state variable or a tuple
field. These are the only variables that can appear within the body of
the aggregation function.6
We carry out a three-step procedure for linear-in-state detection,
summarized in Fig. 6. First, for each variable in an aggregation
function we assign a history. This history tells us how many previous
packets we need to look at to determine a variable’s value accurately
(history = 1 means the current packet). For instance, for the value of
a byte counter, we need to look back to the beginning of the packet
stream (history = ∞), while for a variable that tracks the last TCP
sequence number we need to only look back to the previous packet
(history = 2). Consistent with the definition of history, constants are
assigned a history value of 0, and variables in the tuple field list
are assigned a history of 1. For state variables, we use Alg. 1 to
determine each variable’s history.
Second, once each variable has a history, we look at the history of
each state variable s. If the history of s is a finite number k, then s only
depends on the last k packets and the state update for that variable
is trivially linear-in-state, by setting A to 0 and B to the aggregation
function itself.7 If s has an infinite history, we use syntactic pattern
matching to check if the update to s is linear-in-state.
Third, if all state variables have linear-in-state state updates, the
aggregation function is linear-in-state, and we generate the auxiliary
state that permits merging of the aggregation function (§3). If not,
we use the set of stateful instructions developed in Domino [56] to
implement the aggregation function. We now describe each of the
three steps in detail.
Determining history of variables. To understand Alg. 1, observe
that if all assignments to a state variable only use variables that have
a finite history, then the state variable itself has a finite history. For
instance, in Fig. 4, right after it is assigned, lastseq has a history of
1 because it only depends on the current packet’s fields tcpseq and
payload_len. To handle branching in the code, i.e., if (predicate)
{...} statements, we generalize this observation. A state variable
has finite history if (1) it has finite history in all its assignments
6Marple supports local variables within the function body, but the more general algo-
rithm is not materially different from the simpler version we present in this paper.
7More precisely, the parts of the aggregation function that update s.
in all branches of the program, and (2) each branching condition
predicate itself only depends on variables with a finite history.
Concretely, COMPUTEHISTORY (line 2) assigns each variable a
history corresponding to an upper bound on the number of past pack-
ets that the state variable depends on. We track the history separately
for each branching context, i.e., the sequence of branches enclosing
any statement.8 The algorithm starts with a default large pessimistic
history (i.e., an approximation to ∞) for each state variable (line
1), and performs a fixed-point computation (lines 3–20), repeatedly
iterating over the statements in the aggregation function (line 7–16).
For each assignment to a state variable in the aggregation function,
the algorithm updates the history of that state variable in the current
branching context (lines 7–9). For each branch in the aggregation
function, the algorithm maintains a new branching context and a
history for the branching context itself (lines 10–14). At the end of
each iteration, the algorithm increments each variable’s history to
denote that the variable is one packet older (line 18). The algorithm
returns a conservative history k for each state variable, including
possibly max_bound (line 1, Alg. 1) to reflect an infinite history.
while hist is still changing do
▷ Run to fixed point.
▷ Init. hist. for all state vars.
▷ Set up outermost context.
▷ History value of ctx.
if stmt == state = expr then
hist[state][ctx] ← GETHIST(ctx, expr, ctxHist)
else if stmt == if predicate then
hist ← {}
ctx ← true
ctxHist ← 0
for stmt in fun do
Algorithm 1 Determining history of all state variables
1: hist = {state = {true: max_bound}}
2: function COMPUTEHISTORY(fun)
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
end for
19:
end while
20:
21: end function
22: function GETHIST(ctx, ast, ctxHist)
23:
24:
25:
26:
27: end function
save context info (restore on branch exit)
newCtx ← ctx and predicate
ctxHist ← GETHIST(ctx, newCtx, ctxHist)
ctx ← newCtx
end for
return max(h1, ... , hn, ctxHist)
end for
for ctx, var in hist do
for xi ∈ LEAFNODES(ast) do
hi = hist[xi][ctx]
end if
hist[var][ctx] ← min(hist[var][ctx] + 1, max_bound)
▷ Make history one pkt older.
Now we show precisely how the histories are updated as each
statement of the aggregation function is processed using the helper
function GETHIST. Consider a statement assigning a variable to
an expression, x = expr, within a branching context ctx. Then the
history of x is the maximum of the history of the predicates in ctx
and the history of the expression expr. This is because if either is
a function of the last k packets, then x is a function of at least the
8Currently, Marple forbids multiple if ... else statements at the same nesting level;
hence, the enclosing branches uniquely identify a code path through the function. This
restriction is not fundamental; the more general form can be transformed into this form.
Step 1: Compute history for each state variableStep 2: Are all state variable updates linear-in-state?(Finite history variables are trivially linear in state)Query is scalableStep 3: Compute auxiliary state required for mergeQuery is not scalableYES NOAggregation function code(in Marple)SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
S. Narayana et al.
last k packets. To determine the history of expr, suppose the AST
of expr contains the variables x1, x2, ..., xn as its leaves. Then,
the history of expr is the maximum of the histories of the xi. For
example, the history for lastseq after its assignment in oos_count
is the maximum of 1 (tcpseq and payload_len are functions of the
current packet), and 0 (for the enclosing outermost context true).
Determining if a state variable’s update is linear-in-state.
For each state variable S with an infinite history, we check whether
the state updates are linear-in-state as follows: (1) each update to S
is syntactically affine, i.e., S ← A· S + B with either A or B possibly
zero; and (2) A, B and every branch predicate depend on variables
with a finite history. This approach is sound, but incomplete: it
misses updates such as S = S2−1
S−1 .
Determining auxiliary state. For each state variable with a linear-
in-state update, we initialize four pieces of auxiliary state for a newly
inserted key:9 (1) a running product SA = 1; (2) a packet counter
c = 0; (3) an entry log, consisting of relevant fields from the first k
packets following insertion; and (4) an exit log, consisting of relevant
fields from the last k packets seen so far. After the counter c crosses
the packet history bound k, we update SA to A· SA each time S is
updated.10 When the key is evicted, we send SA along with the entry
and exit logs to the backing store for merging (details are in our
technical report [15]).
5 EVALUATION
We evaluate Marple in three ways. In §5.1, we quantify the hard-
ware resources required for Marple queries. In §5.2, we measure
the memory-eviction tradeoff for the key-value store. In §5.3 and
§5.4, we show two case studies that use Marple compiled to the P4
behavioral model running on Mininet: debugging microbursts [44]
and computing flowlet size distributions.
5.1 Hardware compute resources
Fig. 7 shows several Marple queries. Alongside each query, we show
(1) whether all its aggregations are linear-in-state, (2) whether it
can be scaled by merging correctly with a backing store, and (3)
the switch resources required, measured through the pipeline depth
(number of stages), width (maximum number of parallel compu-
tations per stage), and number of Banzai atoms (total number of
computations) required.
Fig. 7 shows that many useful queries contain only linear-in-state
aggregations, and most of them scale to a large number of keys (§3.2).
Notably, the flowlet size histogram and lossy connection queries are
not scalable despite being linear-in-state, since they contain emit()
statements. In §3.4, we showed how to rewrite some of these queries
(e.g., lossy connections) to scale, at the cost of losing some accuracy.
We compute the pipeline’s depth and width by compiling each
query to the Banzai switch pipeline simulator. Banzai is supplied
with stateless atoms, which perform binary operations (arithmetic,
logic, and relational) on pairs of packet fields, and one stateful atom.
For the linear-in-state operations, we use the multiply-accumulate
atom as the stateful atom, while for the other operations, we use
Banzai’s own NestedIf atom [56]. The Domino compiler determines
9This can happen either when a key first appears or reappears following an eviction.
10This stateful update itself can be implemented through a multiply-accumulate atom.
whether the input program can be mapped to a pipeline with the
specified atoms. As expected, all the linear-in-state queries map to a
pipeline with the multiply-accumulate atom.
The computational resources required for Marple queries are
modest. All queries in Fig. 7 require a pipeline shorter than 11 stages.
This is feasible, e.g., the RMT architecture offers 32 stages [33].
Further, functionality other than measurement can run in parallel
because the number of atoms required per stage is at most 6, while
programmable switches provide ~100 parallel instructions per stage
(e.g., RMT provides 224 [33]).
5.2 Memory and bandwidth overheads
In this section, we answer the following questions:
(1) What is a good size for the on-chip key value store?
(2) What are the eviction rates to the backing store?
(3) How accurate are queries that are not mergeable?
Experimental setup. We simulate a Marple query over three un-
sampled packet traces: two traces from 10 Gbit/s core Internet
routers, one from Chicago (~150M packets) from 2016 [24] and
one from San Jose (~189M packets) from 2014 [23]; and a 2.5 hour
university data-center trace (~100M packets) from 2010 [32]. We
refer to these traces as Core16, Core14, and DC respectively.
We evaluate the impact of memory size on cache evictions for
a Marple query that aggregates by 5-tuple. As discussed in §3.6,
our hardware design uses an 8-way LRU cache. We also evaluate
two other geometries: a hash table, which evicts the incumbent key
upon a collision, and a fully associative LRU. Comparing our 8-way
LRU with other hardware designs demonstrates the tradeoff between
hardware complexity and eviction rate.
Eviction ratios. Each evicted key-value pair is streamed to a back-
ing store. This requires the backing store to be able to process pack-
ets as quickly as they are evicted, which depends on the incoming
packet rate and the eviction ratio, i.e., the ratio of evicted packets to
incoming packets. The eviction ratio depends on the geometry of the
on-chip cache, the packet trace, and the cache size (i.e., the number
of key-value pairs it stores). Hence, we measure eviction ratios over
(1) the three geometries for the Core16 trace (Fig. 8b), (2) the three
traces using the 8-way LRU geometry (Fig. 8a), and (3) for caches
sizes between 216 (65K) and 221 (2M) key-value pairs.
Fig. 8b shows that a full LRU has the lowest eviction ratios, since
the entire LRU must be filled before an eviction occurs. However,
the 8-way associative cache is a good compromise: it avoids the
hardware complexity of a full LRU while coming within 2% of its
eviction ratio. Fig. 8a shows that the DC trace has the lowest eviction
ratios. This is because it has much fewer unique keys than the other
two traces and these keys are less likely to be evicted.
The reciprocal of the eviction ratio (as a fraction) is the reduction
in server data collection load relative to a per-packet collector that
processes per-packet information from switches. For example, for
the Core14 trace with a 219 key-value pair cache, the server load
reduction is 25× (corresponding to an eviction ratio of 4%).
Eviction rates. Eviction ratios are agnostic to specifics of the
switch, such as link speed, link utilization, and on-chip cache size
in bits. To translate eviction ratios (evictions per packet) to eviction
Language-Directed Hardware Design
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
Example
Query code
Description
Packet counts
EWMA over latencies
def count([cnt], []): cnt = cnt + 1; emit()
result = groupby(pktstream, [srcip], count);
def ewma([avg], [tin, tout]):
avg = (1-alpha)*avg + (alpha)*(tout-tin)
ewma_q = groupby(pktstream, [5tuple], ewma);
def oos([lastseq, cnt], [tcpseq, payload_len]): Count the number of packets per
Scales? Pipe
Pipe
# of
depth width atoms
5
2
7