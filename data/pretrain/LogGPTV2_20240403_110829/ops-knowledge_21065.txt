User: As part of a big system, I'm trying to implement a service that (among other tasks) will serve large files (up to 300MB) to other servers (running in Amazon).
This files service needs to have more than one machine up and running at each time, and there are also multiple clients.
Service is written in Python, using Tornado web server.
First approach was using MySQL, but I figured I'm going to have hell saving such big BLOBs, because of memory consumption.
Tried to look at Amazon's EFS, but it's not available in our region.
I heard about SoftNAS, and am currently looking into it.
Any other good alternatives I should be checking?
Assistant: You also can use MongoDb , it provides several API, and also you can store file in S3 bucket with the use of Multi-Part Upload