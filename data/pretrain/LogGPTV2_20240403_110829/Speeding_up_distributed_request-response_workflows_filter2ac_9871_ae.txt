Figure 12: Latency reductions achieved by Kwiken and
variants when trading oﬀ completeness for latency.
Kwiken, it is hard to reason about how much of the overall
capacity should be used for reissues at each stage.
Second, we compared with several straw man policies.
One policy would assign each stage the same reissue fraction
ri = r. However such policy has clear shortcomings; for
example, if a single stage has high cost ci it will absorb
most of the budget. If that stage has low variance, then the
resulting end to end improvement will be negligible. Other
policies like allocating equal budget to each stage exhibit
similar drawbacks.
Finally, lacking an optimal algorithm (recall that even (3)
has a non-convex objective), we compare with two brute-
force approaches. For a subset of nine smaller workﬂows
and budgets from 1% to 10%, we pick the best timeouts
out of 10, 000 random budget allocations. Compared to
training Kwiken on the same data, this algorithm was about
4 orders of magnitude slower. Hence, we did not attempt
it on larger workﬂows. Here, Kwiken’s results were better
on average by 2%;
in 95% of the cases (i.e., {workﬂow,
budget} pairs), Kwiken’s latency reduction was at least 94%
of that achieved by this method. The second approach uses
gradient-descent to directly minimize the 99th percentile of
the end-to-end latency using a simulator (i. e., avoiding the
sum of variances approximation). This method was equally
slow and performed no better. Hence, we conclude that
Kwiken’s method to apportion budget across stages is not
only useful but also perhaps nearly as eﬀective as an ideal
(impractical) method.
We also evaluated two weighted forms of Kwiken that more
directly consider the structure of the workﬂow (§3): weighting
each stage by its average latency and by its likelihood to
occur on a critical path. While both performed well, they
were not much better than the unweighted form for the
examined workﬂows.
5.3 Trading off Completeness for Latency
Next, we evaluate the improvements in latency when using
Kwiken to return partial answers. Fig. 12a plots the improve-
ment due to trading oﬀ completeness for latency for diﬀerent
values of utility loss. Recall that our target is to be complete
Figure 13: Latency improvements from using diﬀerent
catch-up techniques.
enough that the best result is returned for over 99.9% of the
queries, i.e., a utility loss of 0.1%. With that budget, we see
that Kwiken improves the 99th (95th) percentile by around
50% (25%). The plotted values are averages over the web,
image and video stages. Recall that these stages issue many
requests in parallel and aggregate the responses (see Fig. 1).
Fig. 12b compares the performance of Kwiken with a few
benchmarks for utility loss budget of 0.1%: wait-for-fraction
terminates a query when b fraction of its responders return,
ﬁxed-timeout terminates queries at Tcutoﬀ, and time-then-
fraction terminates queries when both these conditions hold:
a constant T ′ time has elapsed and at least α fraction of
responders ﬁnish.
We see that Kwiken performs signiﬁcantly better. Wait-for-
fraction spends signiﬁcant part of budget on queries which get
the required fraction relatively fast, hence slower queries that
lie on the tail do not improve enough. Fixed-timeout is better
since it allows slower queries to terminate when many more of
their responders are pending but it does not help at all with
the quick queries– no change below the 90th percentile. Even
among the slower queries, it does not distinguish between
queries that have many more pending responders and hence
a larger probability of losing utility versus those that have
only a few pending responders. Time-then-fraction is better
for exactly this reason; it never terminates queries unless a
minimal fraction of responders are done. However, Kwiken
does even better; by waiting for extra time after a fraction
of responders are done it provides gains for both the quicker
queries and variable amounts of waiting for the slower queries.
Also, it beats time-then-fraction on the slowest queries by
stopping at a ﬁxed time.
5.4 Catch-up
Here, we estimate the gains from the three types of catch-
up mechanisms discussed earlier. Fig. 13a shows the gains
of using multi-threaded execution and network prioritization
on the web-search workﬂow (Fig. 1), relative to the baseline
where no latency reduction techniques are used. We note
that the speedup due to multi-threading is not linear with
the number of threads due to synchronization costs and using
3 threads yields roughly a 2X speed up. We see that speeding
22880%
70%
60%
50%
40%
e
t
l
i
t
n
e
c
r
e
P
h
9
9
n
i
n
o
i
t
c
u
d
e
R
0% 2% 4% 6% 8% 10%
Reissue Budget 
(utility loss fixed at 0.1%) 
Kwiken (K)
K: only  reissues
K: only utilityloss
K for reissues; wait-for-fraction
80%
70%
60%
50%
same-reissues; wait-for-fraction 40%
0.0%
Utility loss Budget 
0.5%
1.0%
e
c
n
a
i
r
a
v
l
a
n
g
i
r
o
0.1%
i
20%
15%
10%
5%
0%
10.0%
(reissues fixed at 3%) 
allocated budget
(c) Allocation
(a) Workﬂow
(b) Results
Figure 14: A detailed example to illustrate how Kwiken works
up both stages oﬀers much more gains than speeding up just
one of the stages; the 99th percentile latency improves by up
to 44% with only small increases in additional load – about
6.3% more threads needed and about 1.5% of the network
load moves into higher priority queues.
Next, we evaluate the usefulness of using global reissues
on workﬂows. Using a total reissue budget of 3%, Fig. 13b
plots the marginal improvements (relative to using the entire
budget for local reissues) from assigning 1
30 th (x-axis) vs.
assigning 1
6 th of the budget to global reissues (y-axis), for
the 45 workﬂows we analyze. The average reduction in
99th percentile latency is about 3% in both cases, though,
assigning 1
6 of budget leads to higher improvements in some
cases. Overall, 37 out of the 45 workﬂows see gains in latency.
We end by noting that this experiment only shows one way
to assign global reissues; better allocation techniques may
yield larger gains.
5.5 Putting it all together
To illustrate the advantage of using multiple latency reduc-
tion techniques in the Kwiken framework together, we analyze
in detail its application to a major workﬂow in Bing that has
150 stages. A simpliﬁed version of the workﬂow with only
the ten highest-variance stages is shown in Fig. 14a. In three
of the stages, we use utility loss to improve latency and use
reissues on all stages.
We compare Kwiken with other alternatives in Fig. 14b;
on left, we ﬁx utility loss budget at 0.1% and vary reissues,
on right, we vary utility loss and ﬁx reissues at 3%. We
see complementary advantage from using reissues and utility
loss together. In the left graph, using Kwiken with reissues
only at 10% performs worse than using both reissues at 1%
and 0.1% utility loss. Also, using both together is about
20% better than using just utility loss. Graph on the right
shows that, with reissue budget at 3%, increasing utility
loss has very little improvements beyond .1%. We observe
that the larger the reissue budget, the larger the amount of
utility loss that can be gainfully used (not shown). Further,
how we use the budget also matters; consider K for reissues;
wait-for-fraction on the left. For the same amount of utility
loss, Kwiken achieves much greater latency reduction.
So what does Kwiken do to get these gains? Fig. 14c shows
for each of the ten stages, the latency variance (as a fraction
of all variance in the workﬂow) and the amount of allocated
budget (in log scale). We see that the budget needs to be
apportioned to many diﬀerent stages and not simply based
on their variance, but also based on the variance-response
curves and the per-stage cost of request reissue. Without
Kwiken, it would be hard to reach the correct assignment.
100%
Incompleteness
Reissues
0.0% 0.1% 1.0% 10.0%
Reissues
Utility Loss
t
s
e
T
100%
10.0%
1.0%
0.1%
0.0%
t
s
e
T
50%
0%
0%
50%
Train 
(a) 99th percentile Latency
Improvement
Train 
(b) Budget
Figure 15: For 45 workﬂows, this ﬁgure compares the
99th percentile latency improvement and budget values
of the training and test datasets.
5.6 Robustness of parameter choices
Recall that Kwiken chooses its parameters based on traces
from prior execution. A concern here is that due to temporal
variations in our system, the chosen parameters might not
yield the gains that are expected from our optimization or
may violate resource budgets.
To understand the stability of the parameter choices over
time, we compare the improvements for 99th percentile la-
tency and the budget values obtained for the “training”
dataset, to those obtained for the “test” datasets. The test
datasets were collected from the same production cluster on
three diﬀerent days within the same week. Fig. 15 shows that
the latency improvements on the test datasets are within
a few percentage points oﬀ that on the training datasets.
The utility loss on the test dataset is slightly larger but
predictably so, which allows us to explicitly account for it
by training with a tighter budget. In all, we conclude that
Kwiken’s parameter choices are stable. Also, reallocating
budget is fast and can be done periodically whenever the
parameters change.
6. RELATED WORK
Improving latency of datacenter networks has attracted
much recent interest from both academia and industry. Most
work in this area [2, 3, 17, 25, 29] focuses on developing
transport protocols to ensure network ﬂows meet speciﬁed
deadlines. Approaches like Chronos [18] modify end-hosts to
reduce operating system overheads. Kwiken is complementary
to these mechanisms, which reduce the latency of individ-
ual stages, because it focuses on the end-to-end latency of
distributed applications.
Some recent work [4, 13, 28] reduces the job latency in
(MapReduce-like) batch processing frameworks [11] by adap-
tively reissuing tasks or changing resource allocations. Other
prior work [19] explores how to (statically) schedule jobs,
229that are modeled as a DAG of tasks to minimize completion
time. Neither of these apply directly to the context of Kwiken
which targets large interactive workﬂows that ﬁnish within a
few hundreds of milliseconds and may involve over thousands
of servers. Static scheduling is relatively easy here and there
is too little time to monitor detailed aspects at runtime (e.g.,
task progress) which was possible in the batch case.
Some recent work concludes that latency variability in
cloud environments arises from contention with co-located
services [1] and provides workload placement strategies to
avoid interference [27].
Some of the techniques used by Kwiken have been explored
earlier. Reissuing requests has been used in many distributed
systems [10, 12] and networking [5, 14, 23] scenarios. Kwiken’s
contribution lies in strategically apportioning reissues across
the stages of a workﬂow to reduce end-to-end latency whereas
earlier approaches consider each stage independently. Par-
tial execution has been used in AI [30] and programming
languages [6, 16]. The proposed policies, however, do not
translate to the distributed services domain. Closer to us
is Zeta [15], which devises an application-speciﬁc scheduler
that runs beside the query to estimate expected utility and
to choose when to terminate. In contrast, Kwiken relies only
on opaque indicators of utility and hence the timeout policies
are more generally applicable.
7. CONCLUSION
In this paper, we propose and evaluate Kwiken, a framework
for optimizing end-to-end latency in computational workﬂows.
Kwiken takes a holistic approach by considering end-to-end
costs and beneﬁts of applying various latency reduction tech-
niques and decomposes the complex optimization problem
into a much simpler optimization over individual stages. We
also propose novel policies that trade oﬀ utility loss and
latency reduction. Overall, using detailed simulations based
on traces from our production systems, we show that using
Kwiken, the 99th percentile of latency improves by over 75%
when just 0.1% of the responses are allowed to have partial
results and 3% extra resources are used for reissues.
Acknowledgements: We thank Junhua Wang, Navin Joy,
Eric Lo, and Fang Liu from Bing for their invaluable help.
We thank Yuval Peres, our shepherd Georgios Smaragdakis
and the SIGCOMM reviewers for feedback on earlier drafts
of the paper.
8. REFERENCES
[1] Amazon Elastic Compute Cloud (Amazon EC2).
http://aws.amazon.com/ec2/.
[2] M. Alizadeh, A. Greenberg, D. Maltz, J. Padhye, P. Patel,
B. Prabhakar, S. Sengupta, and M. Sridharan. Data Center
TCP (DCTCP). In SIGCOMM, 2010.
[3] M. Alizadeh, S. Yang, S. Katti, N. McKeown, B. Prabhakar,
and S. Shenker. Deconstructing Datacenter Packet
Transport. In Hotnets, 2012.
[4] G. Ananthanarayanan, S. Kandula, A. Greenberg, I. Stoica,
Y. Lu, B. Saha, and E. Harris. Reining in the Outliers in
MapReduce Clusters Using Mantri. In OSDI, 2010.
[5] D. G. Andersen, H. Balakrishnan, M. F. Kaashoek, and
R. N. Rao. Improving web availability for clients with
MONET. In NSDI, 2005.
[6] W. Baek and T. M. Chilimbi. Green: A Framework for
Supporting Energy-Conscious Programming using
Controlled Approximation. In PLDI, 2010.
[7] S. Boucheron, G. Lugosi, and O. Bousquet. Concentration
inequalities. Advanced Lectures on Machine Learning, 2004.
[8] J. Brutlag. Speed matters for Google web search.
http://googleresearch.blogspot.com/2009/06/speed-
matters.html, 2009.
[9] J. R. Dabrowski and E. V. Munson. Is 100 Milliseconds Too
Fast? In CHI, 2001.
[10] J. Dean and L. A. Barroso. The tail at scale. Commun.
ACM, 56(2):74–80, Feb. 2013.
[11] J. Dean and S. Ghemawat. Mapreduce: Simpliﬁed data
processing on large clusters. In OSDI, 2004.
[12] G. Decandia et al. Dynamo : Amazon’s Highly Available
Key-value Store. In SOSP, 2007.
[13] A. D. Ferguson, P. Bodik, S. Kandula, E. Boutin, and
R. Fonseca. Jockey: Guaranteed Job Latency in Data
Parallel Clusters. In EuroSys, 2012.
[14] D. Han, A. Anand, A. Akella, and S. Seshan. RPT:
Re-architecting Loss Protection for Content-Aware Networks.
In NSDI, 2012.
[15] Y. He, S. Elnikety, J. Larus, and C. Yan. Zeta: Scheduling
Interactive Services with Partial Execution. In SOCC, 2012.
[16] H. Hoﬀmann, S. Sidiroglou, M. Carbin, S. Misailovic,
A. Agarwal, and M. Rinard. Dynamic Knobs for Responsive
Power-Aware Computing. In ASPLOS, 2011.
[17] C. Y. Hong, M. Caesar, and P. B. Godfrey. Finishing Flows
Quickly with Preemptive Scheduling. In SIGCOMM, 2012.
[18] R. Kapoor, G. Porter, M. Tewari, G. M. Voelker, and
A. Vahdat. Chronos: Predictable Low Latency for Data
Center Applications. In SOCC, 2012.
[19] Y. Kwok and I. Ahmad. Static Scheduling Algorithms for
Allocating Directed Task Graphs to Multiprocessors. ACM
Computing Surveys (CSUR), 1999.
[20] R. Nishtala et al. Scaling Memcache at Facebook. In NSDI,
2013.
[21] L. Ravindranath, J. Padhye, S. Agarwal, R. Mahajan,
I. Obermiller, and S. Shayandeh. AppInsight: Mobile App
Performance Monitoring in the Wild. In OSDI, 2012.
[22] E. Schurman and J. Brutlag. The User and Business Impact
of Server Delays, Additional Bytes, and Http Chunking in
Web Search. http://velocityconf.com/velocity2009/
public/schedule/detail/8523, 2009.
[23] A. Vulimiri, O. Michel, P. B. Godfrey, and S. Shenker. More
is Less: Reducing Latency via Redundancy. In HotNets,
2012.
[24] X. S. Wang et al. Demystifying Page Load Performance with
WProf. In NSDI, 2013.
[25] C. Wilson et al. Better Never than Late: Meeting Deadlines
in Datacenter Networks. In SIGCOMM, 2011.
[26] H. Wu, Z. Feng, C. Guo, and Y. Zhang. ICTCP: Incast
Congestion Control for TCP in Data Center Networks. In
CONEXT, 2010.
[27] Y. Xu, Z. Musgrave, B. Noble, and M. Bailey. Bobtail:
Avoiding Long Tails in the Cloud. In NSDI, 2013.
[28] M. Zaharia, A. Konwinski, A. Joseph, R. Katz, and I. Stoica.
Improving MapReduce Performance in Heterogeneous
Environments. In OSDI, 2008.
[29] D. Zats et al. DeTail: Reducing the Flow Completion Time
Tail in Datacenter Networks. In SIGCOMM, 2012.
[30] S. Zilberstein. Using Anytime Algorithms in Intelligent
Systems. AI Magazine, 17(3):73–83, 1996.
APPENDIX
Proof of (2). For each random variable Ls we introduce
a new independent random variable L′
s which has the same
distribution as Ls. Let L = (L1, . . . , LN ) and L(s) =
(L1, . . . , Ls−1, L′
s, Ls+1, . . . , LN ). Then, using the Efron-
Stein inequality [7], we have Var(Lw(L)) ≤ 1
2 Ps E(cid:2)(Lw(L)−
Lw(L(s)))2(cid:3) ≤ 1
s)2(cid:3) = Ps Var(Ls).
2 Ps E(cid:2)(Ls − L′
230