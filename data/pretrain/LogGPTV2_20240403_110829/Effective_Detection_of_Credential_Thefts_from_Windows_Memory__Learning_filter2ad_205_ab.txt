### Generic and Specific Intrusion Detection

Intrusion detection systems (IDS) are designed to identify various types of intrusions, such as buffer overflows or denial-of-service attacks. In [10], the authors focus on analyzing system call traces produced by the sendmail program and propose methods for learning rules to detect normal and abnormal sequences. Other works, such as those focusing on university account theft [22], analyze authentication logs and use heuristics to introduce features that are contextually relevant, such as temporal-spatial violations or inconsistencies based on resource usage. More specific studies have focused on intrusion detection on Android devices [21]. These studies employ machine learning mechanisms that rely on data flow-related APIs, which are particularly suited for describing Android application behaviors.

While these works aim to detect a wide range of intrusions, our research focuses on developing precise methods for identifying a specific and common action: credential theft from Windows memory. Like other studies, we use heuristics based on security knowledge and domain expertise to build features that are tailored to our specific scenario. In Section 7, we introduce the features used in our analyses and justify why they are meaningful in our context.

### Methodology Driven by Customer Data

Our approach and methodology are driven by two primary objectives:
1. Develop an accurate model for the read behavior of LSASS memory to classify malicious reads.
2. Protect Microsoft Defender Advanced Threat Protection (MDATP) customers against current attacks that steal credentials from LSASS memory using detectors based on these models.

This rationale guides how we collect and analyze our data. The data are collected as part of an opt-in, commercial relationship between Microsoft and its enterprise customers. The data, gathered in near real-time from individual operating systems, are sent to Azure for detection and remediation. Personal information is obfuscated before being presented to researchers.

We analyzed the processes reading from LSASS memory on all MDATP customer machines and looked for credential dumping tools such as Lslsass, Windows Credential Editor, and Mimikatz. The only tool observed running in a non-obfuscated manner on customer machines was Mimikatz.

Therefore, we decided to study the read behavior of LSASS memory for prominent antivirus and security software tools, as well as Mimikatz, the most relevant tool chosen by attackers. Our intuition for building a model of such read behaviors came from studying how credential dumping tools, particularly Mimikatz, operate. Our data collection and machine learning process proceeded in the following stages:

1. **Benign Software Analysis**: We analyzed the behavior of the most prevalent benign software that scans the memory of the LSASS process by examining a random sample of 70,000 different machines running MDATP.
2. **Mimikatz Attack Data Collection**: We harvested the LSASS read behavior of credential theft techniques performed by Mimikatz on 244 different machines running MDATP.
3. **Detector Training and Testing**: Based on this data, we trained and tested a detector for credential theft.
4. **Windows Update Impact Analysis**: We analyzed the influence of Windows updates on the read behavior of credential theft techniques by collecting and comparing malicious read behaviors on machines running different Windows update versions.

In Section 11, we discuss potential countermeasures that attackers might use to bypass our detector. Despite these, the practical benefits of our detector were deemed significant enough to integrate it into the deployed MDATP detection suite.

Our methodology is also generic, making this novel way of characterizing malicious processes potentially transferable to other types of malicious behavior or for safeguarding other parts of the Windows memory.

### Modelling and Collecting Memory Reads

To protect MDATP customers, we prioritize data from genuine MDATP customer machines. The LSASS process contains secrets stored in its memory, including keys and credentials, which are attractive to attackers for operations like decrypting sensitive data or impersonating users with higher privileges.

Tools like Mimikatz perform cross-process reads of the LSASS address space using Windows API calls such as `ReadProcessMemory`. These APIs take the target process address and the size of the portion to be read as arguments. If the caller has the necessary privileges, the data is copied to a supplied buffer in the caller's process.

The LSASS address space layout is dynamic, influenced by factors like Address Space Layout Randomisation (ASLR), timing of memory allocations, and the type and size of stored data. Predicting the exact address of secrets within LSASS memory is extremely difficult. Tools like Mimikatz must perform multiple reads to search for the addresses of sensitive data.

While the target process address space is generally unpredictable, there are fixed reference points, such as the Process Environment Block (PEB), which can be obtained via API calls like `NtQueryInformationProcess`. Reading the PEB can reveal the locations of loaded modules, and pointers from global variables within these modules can lead to other structures that help locate credentials or other secrets.

For example, the Mimikatz `sekurlsa::logonpasswords` operation follows this approach:
- It performs multiple remote reads against the LSASS PEB to retrieve loaded module information.
- For the MSV1_0 authentication package, it reads the entire `msv1_0.dll` into a local buffer, searches for magic bytes near a pointer to the logon session list, and retrieves the AES key and Initialization Vector (IV).
- It copies the size of the logon session list and reads details from each session, including usernames, domains, SIDs, and pointers to stored credentials.
- It decrypts the encrypted credentials using the retrieved key and IV.

These operations result in a large and varying number of reads of the LSASS process memory over short periods.

Since Windows 10 RS3, insights into memory reads of target processes are possible due to instrumentation of the memory manager. Telemetry from this instrumentation is available to security vendors via the `Microsoft-Windows-Threat-Intelligence` Event Tracing for Windows provider. This telemetry provides details of the calling and target processes and the size of the copied data.

Collecting all these events is impractical due to the volume of data generated. Instead, MDATP creates aggregates summarizing data into 5-minute time slices, or sessions, providing the calling process, target process, total bytes read, and the number of reads within that time slice.

Given the total bytes read and the number of reads, it is natural to model the behavior of credential theft tools like Mimikatz using these data aggregates as features for machine learning. We conjecture that the read behavior of malicious tools is sufficiently different from that of benign tools when understood in terms of these features. The next sections will report on our experimental work to test and confirm this conjecture.

### Patterns in Benign Read Behaviors

Based on the behavioral model and features introduced in the previous section, we investigate the read behavior of LSASS memory for benign processes that run antivirus or other security software. This investigation aims to identify statistical patterns that differentiate benign read behavior from that of tools aiming to steal credentials.

**Experiment 1**: We collected 100,000 read sessions randomly sampled over 10 days on 70,000 different machines running MDATP. Figure 1 shows the read behavior of the five most prevalent processes reading from LSASS: WmiPrvSE, Mcshield, MsMpEng, Wepsvc, and CollectGuestLogs. These security software tools scan the LSASS memory for potentially infected files.

The plots in Figure 1 suggest that the data are very sparse, with the majority of data points aggregating on four highly linear relationships passing through the origin. For each cluster, we fit a linear model for classification. Some clusters, like the one formed by WmiPrvSE, contain a single process, suggesting that it is reasonable to identify whether a process reading from LSASS memory is WmiPrvSE or not based on its data point.

Processes like CollectGuestLogs and Wepsvc lie on the same line, making it more challenging to distinguish them based on their read behavior. However, it is reasonable for two security software applications to perform similar routine checks on the same memory area, resulting in similar read behavior. At the abstract level of classifying malicious read behavior, it is sufficient to identify such benign processes if their behavior is similar.

### Harvest and Analysis of Malicious Reads

The behavioral model of reads from LSASS memory introduced in Section 4 provides a useful and effective characterization of how benign processes access LSASS memory. In this section, we analyze the read behavior of LSASS memory deemed most important to credential theft techniques. This requires an experiment to collect data from real Windows networks and a subsequent analysis to assess whether our behavioral model can distinguish benign from malicious processes.

**Experiment 2**: We collected data for all instances of Mimikatz invocations observed on MDATP machines from January to July 2019. This exercise gathered 1,600 Mimikatz instances on 244 different machines. Out of these, 256 were interactive sessions launched without command-line arguments, which we discarded for simplicity. We obtained 1,344 labeled data points, with each instance labeled as L1, L2, or L3, denoting which of the three commands listed in Section 2 were used.

Figure 1 shows the read sessions with the number of bytes read between 0 and 6 million, a range of particular interest for comparing benign and malicious behaviors.

### Modelling Credential Theft

The experiments and analysis described in Sections 5 and 6 suggest that it is feasible to build a detection mechanism for credential theft based on the read behavior of processes. This section discusses how we built such a detector, based on the data collected in Section 6.

We split the cleaned dataset, containing 1,305 labeled memory reads performed by Mimikatz instances, into three datasets following a standard 60%-20%-20% distribution: a training set of size 783, and validation and test sets both of size 261. We studied the different memory read techniques L1-L3 individually and noticed that each technique produced data that aggregated on linear clusters.

The idea of our detector is to consider a data point as witnessing a malicious memory read technique if it lies within a certain interval around the corresponding line. We performed logistic regressions for the data points on the training set for each of the techniques L1-L3.

We then used the validation set to analyze the influence of the width of the detection interval on the classification rates. Finally, we used the test set to establish low false positive (FP) and false negative (FN) rates.

**7.1 Training**

Figure 3 displays a zoomed-in view of the data points collected for the memory read technique L1, which steals logon passwords from LSASS memory. These points seem to lie on a line, confirming the effectiveness of our behavioral model. For these data, we...