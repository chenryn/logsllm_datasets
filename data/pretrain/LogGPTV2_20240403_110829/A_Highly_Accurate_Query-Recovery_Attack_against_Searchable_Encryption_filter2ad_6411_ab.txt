5 is still effective. Since it is a similar-data attack, we do not
need our documents to be indexed as opposed to the known-
data attacks [1, 3, 15] that assume their known documents are
part of the document set indexed by the server.
A vocabulary Ksim of msim keywords is extracted from
Dsim. An index matrix is built from this document set:
IDsim[i, j] = 1 if the j-th keyword is contained in the i-th
document, 0 otherwise. Then, the msim × msim co-occurrence
, where nsim = |Dsim|.2 Note
matrix is Ckw = ID(cid:62)
that we use relative frequency numbers rather than absolute
count numbers.
simIDsim · 1
nsim
Keyword extraction algorithm The (keyword) distribu-
tional knowledge on document sets is dependent on the way
keywords are extracted from the documents. The attacker uses
an algorithm to extract the vocabulary from her known docu-
ment set. In the literature, all attack papers (im- or explicitly)
assume that the attacker uses the same keyword extraction
algorithm as the client. Whether this assumption is realistic
or not has not been questioned in the literature, but we would
like to stress the importance of this assumption here. We also
assume the attacker and the client to use the exact same extrac-
tion algorithm, and we leave the study of the case of different
extraction algorithms for future work.
Observed queries We denote as Kreal the client’s query-
able vocabulary of mreal keywords. The adversary does know
neither this vocabulary nor its size. The adversary observes l
unique trapdoors and obtains their corresponding search re-
sults. Let Q = (cid:104)td1, . . . ,tdl(cid:105) be the set of observed trapdoors
and Rtd = {id(d)|(x ∈ Kreal) ∧ (td = Trapdoor(x)) ∧ (d ∈
Dreal)∧ (x ∈ d)}, the document identiﬁers returned for the
trapdoor td.
td∈Q Rtd. We note that
p ≤ nreal. Let IDreal be the p× l index matrix built from the
responses to the trapdoors as follows: IDreal[i, j] = 1 if the
response to the j-th trapdoor contains the i-th identiﬁer, 0
otherwise. Finally, we can infer the mreal × mreal trapdoor co-
occurrence matrix: Ctd = ID(cid:62)
. The estimation of
nreal is presented in Appendix B.
Let DocIDs = {id1, . . . ,idp} =(cid:83)
realIDreal · 1
nreal
Known queries As in IKK and CGPR, the adversary knows
the underlying keywords of k queries in Q . The set of known
queries is deﬁned as follows:
KnownQ = {(cid:104)kwknown,tdknown(cid:105)|(kwknown ∈ Kreal ∩ Ksim)
∧ (tdknown ∈ Q )∧ (tdknown = Trapdoor(kwknown))}
2.4 Similarity deﬁnitions
Similar document set Let C (D,kwa,kwb) be the function
returning the number of co-occurrences of keywords kwa and
2A(cid:62) denotes the transpose of a matrix A
kwb inside the document set D. Let SimMat(D1,D2,K ) be a
function returning an m× m similarity matrix of D1 and D2
over the vocabulary K = {kw1, . . .kwi, . . .kwm}. The function
SimMat is deﬁned such that:
|D1|
|D2|
C (D1,kwi,kw j)
(SimMat(D1,D2,K ))i j =
− C (D2,kwi,kw j)
(1)
In other words, the i j-th element of the matrix returned
by SimMat is the difference between the co-frequency of the
keywords i and j in the document set D1 and the co-frequency
of the same two keywords in the document set D2. Thus,
SimMat(D1,D2,K ) describes the similarity of D1 and D2
over the vocabulary K and the norm ||SimMat(D1,D2,K )||
is a measure of the similarity of D1 and D2. In this paper, we
consider the Frobenius norm (being a natural matrix-extension
of the Euclidean vector-norm), but note that other norms can
be considered as well.
for ε ≥ 0 the following holds:
We deﬁne Dsim and Dreal as two ε-similar document sets if
||SimMat(Dsim,Dreal,Kreal))|| ≤ ε
(2)
In other words, the closer the co-frequencies between the
document sets are, the more similar the document sets are. In
our deﬁnition, we only need to consider the similarity over
the queryable vocabulary Kreal because those are the only
keywords that are queried for by the client and to be recovered
by the attacker.
Similar and queryable vocabularies To recover the
queries, the attacker needs to have as many elements of the
queryable vocabulary Kreal as possible in her similar vocab-
ulary Ksim. This creates a natural upper bound for the attack
accuracy:
AttackAccuracy ≤ |Kreal ∩ Ksim|
|Kreal|
(3)
In other words, the attacker can only recover the queries
for which the underlying keywords are contained in Ksim.
In the experiments presented in Section 5, Ksim contains
most elements of Kreal since the average accuracy goes up to
95% which means that more than 95% of the keywords of
the queryable vocabulary Kreal are contained in the similar
vocabulary Ksim.
Attacker assumptions An attacker knows neither the in-
dexed documents Dreal nor the vocabulary Kreal. Thus, she
cannot calculate the exact ε-similarity between her dataset
Dsim and the indexed dataset Dreal. We assume that:
1. Dsim is ε-similar to the indexed document set Dreal, for
a sufﬁciently small ε (e.g. ε = 0.8 as in the Figure 7).
146    30th USENIX Security Symposium
USENIX Association
2. Ksim contains most elements of Kreal
(especially
Kreal(Q ), the underlying keywords of the queries ob-
served by the attacker).
3 Score attack
On an intuitive level, our score attack makes use of a con-
ﬁdence metric which scores trapdoor-keyword pairs. This
metric is called a matching score and should be maximized
when the trapdoor is paired with its correct underlying key-
word. The attacker computes the matching score of every pos-
sible trapdoor-keyword pair. For each trapdoor, the trapdoor-
keyword pair with the highest score is returned.
3.1 Extracting the known query co-occurrence
sub-matrices
The attacker uses her known queries (= known correct
trapdoor-keyword pairs) to project the keyword and the trap-
door co-occurrence matrices to a common sub-vector space.
This projection is done by only keeping the columns of the
known queries and to sort the columns using the known
queries such that the i-th column is related to the i-th known
query. Formally the projection works as follows:
For a keyword kw, we denote its position in the vocabu-
lary Ksim = (kw1, . . . ,kwmsim) by pos(kw), i.e. pos(kwi) = i
for kwi ∈ Ksim. Likewise, we denote the position of a trap-
door td in the list of observed queries Q = (cid:104)td1, . . . ,tdl(cid:105) by
pos(td). We deﬁne the projection of the trapdoor-trapdoor co-
occurrence matrix Ctd onto the known queries as the l× k ma-
td such that: for all i ∈ {1 . . .l} and all j ∈ {1 . . .k} there
trix Cs
exists a known query q j = (tdknown,kwknown) ∈ KnownQ
such that
Cs
td[i, j] = Ctd[i, pos(tdknown)].
(4)
Likewise, we deﬁne the projection of the word-word co-
occurrence matrix Ckw onto the known queries as the msim ×
kw such that: for all i ∈ {1 . . .msim} and all j ∈
k matrix Cs
{1 . . .k} there exists a known query q j = (tdknown,kwknown) ∈
KnownQ such that
Cs
kw[i, j] = Ckw[i, pos(kwknown)].
(5)
In our notation, we use the superscript s to emphasize that
Cs
td and Cs
kw deﬁne co-occurrence sub-matrices. Such matrices
are very convenient since we can directly compare a keyword
and a trapdoor. We denote as Cs
td[td j]), the
vector composed of the co-occurrences of keyword kwi (resp.
trapdoor td j) with every keyword (resp. trapdoor) related to
a known query. Thus, we can extract a k-dimensional vector
describing each keyword or trapdoor. In the next section, we
will deﬁne our conﬁdence score based on the distance between
a keyword vector and a trapdoor vector.
kw[kwi] (resp. Cs
3.2 Conﬁdence score and matching process
The sub-matrices Cs
td are used to score a trapdoor-
keyword pair. A score should be maximized when the pair is
correct. The scoring function for a vector-norm (cid:107)·(cid:107) (e.g. the
L2 norm) is deﬁned as
kw and Cs
Score(td j,kwi) = −ln(||Cs
for all kwi ∈ Ksim and all td j ∈ Q .
kw[kwi]−Cs
td[td j]||),
(6)
kw[kwi] and Cs
Note that Equation (6) results in a high score when the
distance between a keyword and a trapdoor is small. This
distance can be obtained since Cs
td[td j] share a
common vector space. The −ln function is used to transform
the distance into a score to focus on the order of magnitude
instead of distance values close to zero. For example, a dis-
tance of e−11 results in a score of 11. In our case, the distance
is always less than 1 because we are using relative frequency
matrices. We focus on orders of magnitude for interpretability
matters. Even for an attack accuracy above 80%, the attacker
needs to identify the correct predictions. The interpretability
provided by a scoring approach is then necessary. We argue
that there is a higher interpretative meaning when comparing
two orders of magnitude scaled between 0 and ∼ 30 (experi-
mental upper bound) than comparing two small norms close
to zero. Moreover, in Section 5 and in Appendix C, we pro-
pose geometrical methods (focusing on the distance between
the scores) to improve the results of the score attack presented
in this section.
Our score attack uses the score function as follows: for
each trapdoor, it goes through all keywords and returns the
keyword for which the score is maximized. See Figure 1 for
an algorithmic description. Note that the score is a conﬁ-
dence score and the attacker can sort the predictions (i.e. the
trapdoor-keyword pairs returned) based on their matching
score to deﬁne the most likely predictions.
To run the algorithm, the norm ||·|| can be chosen freely.
However, our experiments showed that the L2 norm maxi-
mizes the accuracy. This norm tolerates a high difference
between one of the k components of the vector, i.e. when one
of the co-occurrences in the attacker dataset is very far from
its corresponding co-occurrence in the indexed dataset.
4 Experimental results
4.1 Methodology
Datasets We simulate the attacks using two publicly avail-
able datasets. First, we use the Enron dataset [17] which is
widely used in the literature to simulate attacks. Like in IKK
and CGPR, we compose our Enron document set by extract-
ing every email contained in the folders _sent_mail to obtain
30109 documents. Second, we use data from the Apache mail-
USENIX Association
30th USENIX Security Symposium    147
kw,Q ,Cs
td
Require: Ksim,Cs
pred ← []
for all td ∈ Q do
candidates = []
for all kw ∈ Ksim do
td[td]||)
s = −ln(||Cs
append (kw,s) to candidates
kw[kw]−Cs
end for
candidates = sort(candidates,desc)
append (td,candidates[0]) to pred
end for
return pred
Figure 1: The score attack
ing list archives3. We use speciﬁcally the "java-user" mailing
list from the Lucene project for the years 2002-2011. This
second dataset contains 50878 documents. It was introduced
in CGPR.
Keyword extraction The keyword extraction is exclusively
done on the email content. Thus, keywords of the title or the
names of the recipients cannot be queried. To obtain the list
of the keywords, we stem the words using the Porter Stem-
mer [27] and remove the stop words. For Apache dataset, we
systematically remove the mailing list signature proposing to
unsubscribe. Otherwise, the keyword contained in this "Un-
subscribe" message would be useless in the search since it
appears in every email.
Adversary knowledge generation At the beginning of an
experiment, the document set used (i.e. Enron or Apache) is
divided randomly into two disjoint subsets. One subset is used
to generate the index, i.e. the encrypted document set Dreal to
be attacked. The second subset is part of the adversary knowl-
edge, i.e. the similar document set Dsim. Every experiment
is done with non-overlapping document set, that is, only as
similar-data attack. The similar vocabulary Ksim is extracted
from Dsim and is given to the adversary. The index vocabulary
Kreal is extracted from Dreal and is not known by the adver-
sary. The similar (resp. real) vocabulary extraction algorithm
consists in extracting the msim (resp. mreal) most frequent key-
words of Dsim (resp. Dreal). The underlying keywords of the
queries are chosen uniformly at random from Kreal. For each
run, document and query sets are freshly chosen uniformly at
random.
Hardware and software The experiments are done on a
Debian server with a quad-core processor (64 bits, 2.1 GHz)
and 8 GB of memory. The algorithms are implemented using
Python 3.7. Speciﬁcally, we use the NLTK [21] for the basic
3http://mail-archives.apache.org/mod_mbox/
lucene-java-user/
natural language processing: word tokenization, stemming
and stopwords.
Our code to simulate the attack and to obtain our re-
sults is publicly available: https://github.com/MarcT0K/
Refined-score-atk-SSE.
Result presentation We call correct prediction, a query for
which the algorithm has returned the corresponding underly-
ing keyword. We evaluate the performance of our attack using
the term accuracy. The accuracy corresponds to the number of
correct predictions divided by the number of unknown queries.
Our accuracy excludes known queries and is always computed
|CorrectPred(UnknownQ )|
over the unknown queries (i.e. acc =
).
In other articles such as CGPR, the term recovery rate is also
used to deﬁne this concept. We use bar plots to present the
results of our experiments. Each bar is obtained by computing
the average result over 50 attack simulations. These bars are
completed with errors bars which correspond to µ± σ with µ
the average accuracy and σ the standard deviation.
|Q |−|KnownQ |
4.2 Results
Figure 2 shows the accuracy of the algorithm on Enron corpus
for several vocabulary sizes. The server stores 60% of the
corpus and the adversary knows the remaining 40%. The
adversary has observed 15% of the possible queries. She
knows either 15, 30 or 60 queries. When the vocabulary size
is 1000 and the adversary knows 30 queries (20% of the
queries observed), the average accuracy is 60%. When the
vocabulary size is 2000 and the adversary knows 60 queries