Technique: Eval 
The spath Function
▶ If you’ve ever had to deal with complicated JSON or XML, the eval spath function is a lifesaver. It 	is similar to the | spath command, but it can be embedded in conf files.
sourcetype=my_XMLsourcetype=my_XML
| eval sender = spath(_raw, "envelope.header.sender")
| rex max_match=0 "(?).*?)
| mvexpand transaction
| eval payload=spath(transaction, "trans.body")
| eval payload_length = len(payload)
| table sender payload_length payload
We can extract XML or JSON values out of _raw 	logs
When we deal with very complicated json, mvfields become very important. We will cover that in Multi-Value fields, later in this presentation.You can also apply this to individual fields, quickly and easily. 
Technique: Eval 
The where Search Command
▶ I know what you’re saying - where is a search command, it’s not eval. But a common question I get is how | search is different from | where. The big difference is that | where uses eval logic.
▶ Anything you would put into the conditional in an if statement, you can put into a where clause.sourcetype=what_have_you
| where 
| (  | country!="US" 
AND NOT searchmatch("country: US")  | You do have to use the more rigid eval type syntax  |
|---|---|---|
| )  |country!="US"  AND NOT searchmatch("country: US")  |here, but you can do some much more advanced  |
| )  |country!="US"  AND NOT searchmatch("country: US")  |logic. |
OR match( 
urldecode(query_string), "[rR]estricted")Did you know you can do urldecoding (e.g., %23 -> #, %24 -> $, etc.)? And regex matching? All of that in 	a where clause.
Technique: Multi-Value Fields
Background and Challenges
▶ "You have JSON, so your life is easy! Oh, did you say nested JSON? Eep…"▶ "I want to tag events just for a specific search!"
▶ "I want to analyze IP occurrences, whether it’s src_ip or dest_ip, I just care about 	ip"▶ "For some reason I have a multi-value field… I want to analyze each field 	individually!"
▶ Multi-Value fields are a great swiss army knife inside of SPL, but they’re also one 
of the least obvious techniques. Let’s look at how they work. 
Technique: Multi-Value Fields JSON data
▶ Simple JSON data is very easy to deal with. Poorly structured data is a pain.
sourcetype=datasource1sourcetype=datasource1 
| eval direction = case(
cidrmatch("10.0.0.0/8", src_ip) AND NOT 
	cidrmatch("10.0.0.0/8", dest_ip), "outgoing", NOT cidrmatch("10.0.0.0/8", src_ip) AND 
	cidrmatch("10.0.0.0/8", dest_ip), "incoming", cidrmatch("10.0.0.0/8", src_ip) AND 
	cidrmatch("10.0.0.0/8", dest_ip), "internal", 1=1, "outgoing to outgoing.. Add the public IP ranges")
Begin with your datasetBegin with your dataset
Conditional One: Outgoing Traffic
Conditional Two: Incoming Traffic
Conditional Three: Internal Traffic
Default (1=1): Whatever Else
Technique: Multi-Value Fields
Tagging Events
▶ case is skipped by almost everyone who has never been a programmer. If you’ve been a 	programmer, you already know about it. If you haven’t, get psyched.▶ One of the greatest strengths of eval is that it allows you to embed all manner of business logic. Invariably, this requires you to have if statements… but often, you end up with multiple scenarios. If a, then x, if b, then y, if c, then z, if d, then throw an error. 
▶ Many use nested if statements, but case handles multiple conditions with ease. 
sourcetype=datasource1 
| eval direction = case(| eval direction = case(
cidrmatch("10.0.0.0/8", src_ip) AND NOT 
	cidrmatch("10.0.0.0/8", dest_ip), "outgoing", NOT cidrmatch("10.0.0.0/8", src_ip) AND 
	cidrmatch("10.0.0.0/8", dest_ip), "incoming", cidrmatch("10.0.0.0/8", src_ip) AND 
	cidrmatch("10.0.0.0/8", dest_ip), "internal", 1=1, "outgoing to outgoing.. Add the public IP ranges")
Begin with your dataset
Conditional One: Outgoing TrafficConditional One: Outgoing Traffic
Conditional Two: Incoming Traffic
Conditional Three: Internal Traffic
Default (1=1): Whatever Else
Technique: Multi-Value Fields 
How Did I Get Here? How do I get out of Multi-Value land?
▶ Most commonly you have a multi-value field that you just want to split (e.g., two IP 
addresses that you want to split into two different events).
▶ This is easily done with 	mvexpand {field name}▶ Keep in mind though that this will split *all* of the fields. If you only need a couple of fields, then use | fields beforehand to get rid of the others so that you don’t consume excessive memory.
• Splunk does try to deal with that stuff automatically, but I like to guide Splunk here.
▶ The other most common scenario I see is you have two values that are the same for a particular value. Usually this is a quirk of the data generator, but sometimes you will have the same value twice for every field.▶ Two approaches for this scenario. The easiest (that I just learned!) is: 
| eval value=mvdedup(value)
▶ A slightly heavier but also more flexible approach is using streamstats, as you have all of the flexibility of stats: 
| streamstats window=1 values(value) as value values(eval(NOT match(value, "^\d")) as value2
Technique: Multi-Value Fields
When Two (or more fields) Become OneWhen Two (or more fields) Become One
▶ One of the scenarios where I use multi-value fields decently often is to simplify source/dest analysis. In a typical perimeter NGFW log, you have src_ip, dest_ip, src_translated_ip, and dest_translated_ip. If I want to track the top # of IPs associated with IPS alerts, I can look for the top IPs without worrying as much about the directionality.
sourcetype=ngfwsourcetype=ngfw
| fields severity src_ip dest_ip src_translated_ip dest_translated_ip
| eval ip = mvappend(src_ip, dest_ip, src_translated, dest_translated_ip
| stats max(severity) count by ip
Here are the fiels that ultimately I care about
Conditional One: Outgoing Traffic
Conditional Two: Incoming Traffic
Conditional Three: Internal Traffic
Default (1=1): Whatever Else
Technique: Multi-Value FieldsTechnique: Multi-Value Fields
Warning: Null Fields, the importance of coalesce
▶ This is general to working with eval, but I find it comes up often in the context of 	multi-value fields. 
▶ Whenever there is the possibility that you might have a null value, make sure to 	coalesce it to something non-null, otherwise it could break everything.
▶ BAD: | eval description = "… Second Username (if present): " . mvindex(users, 1,1)▶ GOOD: | eval description = ".. Second Username (if present): " . coalesce(mvindex(users, 1, 1), "N/A")
Technique: Stats on Stats
Background and Challenges
▶ Remember that as you build out a Splunk search each command sends results to the next, but all any search command takes as input is a series of fields. Even many intermediate searchers don’t take advantage of this capability!▶ "I would like to track how many events occur per day per user, and then find 	anomalies in that daily trend."
	Technique: Stats on Stats
▶ We leverage the first stats to grab per day elements, and then the second stats to 
aggregate and analyze trends. 
tag=authentication
| bucket _time span=1d
| stats dc(dest) as count by user, _time
| stats count as num_data_samples| stats count as num_data_samples
	max(eval(if(_time >= relative_time(now(), 	"-1d@d"), count,null))) as count 
	avg(eval(if(_time= relative_time(now(), "-1d@d")
| convert ctime(earliest) ctime(latest) 
	timeformat="%m/%d/%Y %H:%M:%S"
| eval dest=replace(dest, ".contoso.com", "")
| table user dest count earliest latestBuild whatever detection you are looking for, in this case looking for people logging to servers for the first time in the last day. For an example similar to this, check out the "Lookup Caching" technique, which scales really well. 
Definitely don’t print an epoch timestamp ever. But 	ever for normal timestamps, make sure that they match each other and what analysts are expecting. They have to get it *really* fast, so get in the habitMaybe you have unnecessary info? Format it.
Then table it with the fields in a sensible order
Technique: Formatting a Table Drilldown In the Worst Scenario
▶ Sometimes you have no clean drilldown capability, e.g., in an email alert. Even in 	that scenario, give a search string that can be run.
tag=authentication
| stats earliest(_time) as earliest latest(_time) as 
latest count values(sourcetype) as sourcetypesvalues(indexes) as indexes by user, dest
| where earliest >= relative_time(now(), "-1d@d") | eval drilldown= "index=" . mvjoin(indexes, " OR index=") . " sourcetype=" . mvjoin(sourcetypes, " OR sourcetype=") . " user=" . user . " dest=" . 
dest . " earliest=" . earliest . " latest=" . latest
| convert ctime(earliest) ctime(latest) 
	timeformat="%m/%d/%Y %H:%M:%S" | eval dest=replace(dest, ".contoso.com", "")| table user dest count earliest latest drilldown
Most of this search was already covered - I’ve grayed out those parts for clarity. 
We’ve now added sourcetypes and indexes into our 	base search.
	There’s not a ton of complexity here - we’re just composing a big string that someone could copy-	paste. Not the mvjoin to handle many different 	potential sourcetypes or indexes, though.In your final table, you can include the drilldown but exclude the ugly other fields that it is composed of. This lets analysts just copy-paste, as an item of last resort. 
	Technique: Formatting a Table▶ Building a dashboard? Print something good, and then drilldown well.
Users logging into new servers (with drilldown)
	 | table sourcetype index user dest count 
earliest latest drilldown | sort - count 
["user", "dest", "count", "earliest","latest"]
/app/search/search?q=index=……..