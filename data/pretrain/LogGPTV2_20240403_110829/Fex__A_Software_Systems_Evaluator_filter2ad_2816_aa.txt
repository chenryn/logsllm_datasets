# FEX: A Software Systems Evaluator

**Authors:**
- Oleksii Oleksenko†
- Dmitrii Kuvaiskii†
- Pramod Bhatotia‡
- Christof Fetzer†

**Affiliations:**
- † Technical University of Dresden
- ‡ The University of Edinburgh

**Conference:**
2017 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks

## Abstract
Experimental evaluation is a cornerstone in software systems research, used to assess the effectiveness of new solutions. However, existing evaluation frameworks are often rigid, simplistic, and inconsistent, which can lead to issues such as lack of flexibility, inadequate representation of real-world scenarios, and reproducibility problems across different platforms.

This paper introduces FEX, a software systems evaluation framework designed to address these limitations. FEX is extensible, practical, and ensures reproducibility by leveraging container technology. We demonstrate that FEX achieves these goals with minimal user effort; for example, adding Nginx web server to an evaluation requires only 160 lines of code (LoC). In this paper, we discuss the architecture of FEX, explain its interface, present common usage scenarios, and evaluate the effort required for writing various custom extensions.

## 1. Introduction
Software systems research heavily relies on experimental evaluation to validate the effectiveness of proposed solutions. A robust experimental setup is essential for reliable results. However, evaluating new systems can be tedious, time-consuming, and error-prone. This process often involves multiple iterations of design, implementation, and evaluation, leading to significant effort.

An ideal evaluation mechanism should include a wide variety of benchmarks, run in a controlled, bias-free manner, and produce aggregated, processed, and well-presented results. Unfortunately, there is no unified evaluation framework that can be easily reused and extended in new projects. The current best practice involves using benchmark suites like SPEC CPU2006, modifying their configuration files, and writing scripts to automate experiments, aggregate results, and plot them. This ad-hoc method has several limitations:

1. **Rigidity:** Existing benchmark suites are inflexible and cannot be easily combined or modified.
2. **Simplicity:** They may not adequately represent real-world applications, leading to skewed results.
3. **Inconsistency:** They do not enforce a consistent software stack, leading to reproducibility issues.

To overcome these limitations, we developed FEX, a software systems evaluation framework. FEX is extensible, practical, and leverages Docker container technology to ensure reproducibility. It supports the integration of various benchmark suites, standalone applications, and tools, making it suitable for evaluating compiler extensions, instrumentation tools, libraries, and other system components.

Out-of-the-box, FEX includes support for several well-known benchmark suites (e.g., SPLASH, Phoenix, PARSEC) and standalone programs (e.g., Apache, Memcached, Nginx). To highlight its extensibility, we evaluated the effort required to integrate SPLASH-3, Nginx, and RIPE security testbed, which required 326, 166, and 75 LoC respectively, taking less than 8 man-hours. These results demonstrate that FEX significantly simplifies the software systems evaluation process.

## 2. Design of FEX

### 2.1 System Interface
FEX was designed with reproducibility as a primary goal. All experiments are run within a Docker container, ensuring they are independent of the host system. The Docker image contains the minimum necessary components: source codes for benchmark suites, makefiles, Bash and Python scripts for environment setup, experiment execution, result aggregation, and plotting.

The general workflow and system interface of FEX are shown in Figure 1. Any action in the workflow can be executed via the `fex.py` entry point:

```bash
>> fex.py <command> [other_arguments]
```

For example, running the Phoenix benchmark suite with GCC would look like this:

```bash
>> fex.py run -n phoenix -t gcc_native
```

The workflow is divided into two stages: setup and run. The setup stage prepares the environment by installing all necessary components from the internet.

#### Experiment Setup
The Docker image contains only the source codes and scripts. Dependencies, such as compilers, shared libraries, and additional tools, are downloaded and installed during the setup stage. This approach keeps the Docker image size manageable (currently 1.04GB) and allows users to install specific versions of dependencies needed for their experiments.

The setup stage includes three steps:
1. **Installing Compilers:** Specific versions of compilers are installed to ensure reproducibility.
2. **Installing Dependencies:** Tools required for the build process or specialized measurements are installed.
3. **Installing Additional Benchmarks:** Sources for large unmodified programs can be added to the repository or fetched from external sources.

#### Experiment Runs
After setting up the environment, users can start running experiments. The typical sequence of steps is as follows:

1. **Build Step:** Each benchmark is built once before running, ensuring consistency. The `--no-build` flag can be used for quick preliminary experiments.
2. **Run Step:** The actual experiment is performed. FEX provides Python hooks to specify benchmarks, inputs, and control how benchmarks are started.
3. **Collect Step:** Log files are parsed, measurement results are extracted, processed, and stored in a CSV table using the Pandas library.
4. **Plot Step:** Results are plotted using matplotlib, typically on a local machine or a remote server.

## 3. Conclusion
FEX is a flexible, practical, and reproducible software systems evaluation framework. It addresses the limitations of existing frameworks by providing a unified and extensible platform for running, aggregating, and visualizing experimental results. By leveraging Docker and a modular design, FEX significantly reduces the effort required for comprehensive and reliable evaluations.

---

**References:**
[1] - [18] (Include references here)

**Figures:**
- Figure 1: System interface of FEX.

---

This optimized version of the text is more structured, clear, and professional, making it easier to read and understand.