title:Fex: A Software Systems Evaluator
author:Oleksii Oleksenko and
Dmitrii Kuvaiskii and
Pramod Bhatotia and
Christof Fetzer
2017 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
FEX: A Software Systems Evaluator
Oleksii Oleksenko† Dmitrii Kuvaiskii†
† Technical University of Dresden
Pramod Bhatotia‡ Christof Fetzer†
‡ The University of Edinburgh
Abstract—Software systems research relies on experimen-
tal evaluation to assess the effectiveness of newly developed
solutions. However, the existing evaluation frameworks are
rigid (do not allow creation of new experiments), often
simplistic (may not reveal issues that appear in real-world
applications), and can be inconsistent (do not guarantee
reproducibility of experiments across platforms).
This paper presents FEX, a software systems evaluation
framework that addresses these limitations. FEX is extensible
(can be easily extended with custom experiment
types),
practical (supports composition of different benchmark suites
and real-world applications), and reproducible (it is built
on container technology to guarantee the same software
stack across platforms). We show that FEX achieves these
design goals with minimal end-user effort—for instance,
adding Nginx web-server to evaluation requires only 160 LoC.
Going forward, we discuss the architecture of the framework,
explain its interface, show common usage scenarios, and
evaluate the efforts for writing various custom extensions.
I.
INTRODUCTION
Software systems
research primarily relies on
experimental evaluation to validate the effectiveness of
proposed solutions. Therefore, a sound experimental
evaluation setup is at the core of systems research.
At the same time, evaluating new systems can be
tedious, time-consuming, and error-prone. Furthermore,
since systems research usually requires multiple iterations
over the “design-implement-evaluate” cycle, the resulting
evaluation effort can be signiﬁcant. Ideally, a sound
evaluation mechanism requires a wide variety of
benchmarks to be built with varying parameters, run in
a controlled “bias-free” fashion, and the results have to
be aggregated, processed, and neatly plotted.
Unfortunately, there is no unifying evaluation frame-
work which could be reused and extended in new projects.
Current best practice lies in taking a benchmark suite, e.g.,
SPEC CPU2006, modifying its conﬁguration ﬁles with
custom parameters, and writing a number of scripts to
automate experiment runs, aggregate results, and ﬁnally
plot them. This ad-hoc method has three major limitations.
First, existing benchmark suites are rigid. They cannot
be easily combined together and it is either cumbersome
or outright impossible to modify experiments, e.g., add
security evaluation to the existing suite. We experienced
these problems during a project
that evaluated a
new security-related tool [1]. For the evaluation to
be holistic, we used three benchmark suites: SPEC
CPU2006 [2]
(de-facto standard but supports only
single-threaded applications), Phoenix [3] (represents
I/O- and memory-intensive workloads), and PARSEC
[4] (contains complex multithreaded programs). Each
has its own management system and no support for
plotting. Without a unifying framework, we would be
forced to replicate the same conﬁguration parameters
in ad-hoc scripts, possibly resulting in hard-to-diagnose
performance bugs. Additionally, our security evaluation
would require yet another collection of scripts.
The second limitation of existing benchmark suites
is that they are often simplistic. Recent works show that
using only one benchmark suite may be insufﬁcient for
adequate evaluation [5]. Moreover, choosing a wrong
suite may signiﬁcantly skew the results, either because
the included benchmarks do not represent current
computing problems [6, 7] or because they do not fully
capture the speciﬁcs of a particular domain [8].
The third limitation of the existing evaluation frame-
works is that they can be inconsistent: they do not enforce
the same software stack (in particular, speciﬁc versions
and build ﬂags of used compilers, libraries, and tools)
which may lead to inconsistent results across different
platforms. This aspect is crucial for reproducibility of
results [9]. While other areas of computer science have de-
veloped domain-speciﬁc solutions to this problem [10, 11],
we are unaware of similar tools in the systems community.
This paper presents FEX, a software systems evaluation
framework that overcomes the aforementioned limitations.
FEX is extensible and practical—adding a new type of ex-
periment, a benchmark suite, or a standalone application
requires minimal effort. Furthermore, it leverages the
Docker container technology to secure the software stack
and achieve reproducibility [12, 13]. To our knowledge,
FEX is the ﬁrst effort to combine the entire build-run-plot
evaluation process across different benchmark suites and
standalone programs. It can be used to evaluate compiler
extensions and optimizations, dynamic and static instru-
mentation tools, new libraries or library versions, and
any other tools that affect the application behavior.
Out-of-the-box, FEX is integrated with several well-
known benchmark suites (SPLASH, Phoenix, PARSEC),
standalone programs (Apache, Memcached, Nginx),
compilers (GCC, Clang), and measurement tools (perf,
time). Internally, our framework is comprised of a number
of tools for automating installation, building and running
benchmarks, collecting logs, and plotting the ﬁnal results.
To highlight FEX’s extensibility and ease of use, we
evaluate the efforts to incorporate SPLASH-3 benchmark
suite [14], Nginx web-server [15], and RIPE security
testbed [16]: 326, 166, and 75 LoC respectively. In terms of
time spent, the whole effort took less than 8 man-hours.
These results allow us to conclude that FEX signiﬁcantly
simpliﬁes the software systems evaluation process.
2158-3927/17 $31.00 © 2017 IEEE
DOI 10.1109/DSN.2017.25
543
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:02:52 UTC from IEEE Xplore.  Restrictions apply. 
II. DESIGN OF FEX
A. System Interface
FEX was developed with reproducibility as one of
the main goals. Therefore, we prepare the environment
and run all experiments in a Docker container in such
a way that they are as independent from the actual host
system as possible [12, 13]. The Docker image contains
a bare minimum to run the experiments: sources for
all programs in benchmark suites with corresponding
makeﬁles, Bash scripts for environment setup, Python
scripts to actually perform experiments and to aggregate
and plot their results. Note that the packages put in the
image—git, python3, wget, perf, etc—are used by the
framework itself and do not inﬂuence the experiments
(i.e., they do not affect measurements neither through the
build system nor through dynamically linked libraries).
Figure 1 shows the general workﬂow and the exposed
system interface of FEX. Any of the actions in the
workﬂow can be executed via call to the framework’s
entry point—fex.py ﬁle:
>> fex.py  −n  [other_arguments]
For example, running Phoenix benchmark suite with
GCC will look like this:
>> fex.py run −n phoenix −t gcc_native
The workﬂow is divided into two stages: setup and
run. The ﬁrst stage prepares an environment for the
second stage by installing all the necessary components
from the Internet.
Experiment Setup. The Docker image we ship contains
only the source codes of benchmarks and a set of
scripts to build and run them. The actual dependencies—
compilers to build, shared libraries to link against,
additional
tools and benchmarks—are downloaded
from the Internet and installed at the experiment setup
stage. The reasons for this ﬂow are twofold. First, the
Docker image would swell to approx. 17GB in size1 if
all dependencies would be built-in. Images of such size
would be cumbersome to distribute. Second, this allows
the end user to install only those dependencies and
only those versions needed for her experiments. (For
reproducibility, it is important that the exact versions of
software crucial for experiments are installed.)
For simplicity, the installation scripts are written in
Bash (they can also be written in Python). To run an
installation script, FEX provides an “install” command.
The following example installs GCC 6.1.
>> fex.py install −n gcc−6.1
As shown in Figure 1 (top), this stage includes three
steps:
• Installing compilers with speciﬁc versions is a
prerequisite. FEX cannot rely on Linux default
package managers to automatically install required
compilers, e.g., APT or RPM, because compiler
versions in their repositories change over time and
1Our current image is 1.04GB, with 122MB Ubuntu ﬁles, 300MB of
benchmarks’ source ﬁles, and the rest helper packages
(cid:12)(cid:8)(cid:13)(cid:9)(cid:10)(cid:14)(cid:14)
(cid:3)(cid:2)(cid:15)(cid:16)(cid:11)(cid:14)(cid:5)(cid:6)(cid:13)
(cid:20)(cid:10)(cid:13)(cid:19)(cid:7)(cid:13)(cid:3)(cid:6)(cid:11)(cid:16)(cid:9)(cid:13)
(cid:12)(cid:8)(cid:13)(cid:9)(cid:10)(cid:14)(cid:14)
(cid:17)(cid:5)(cid:16)(cid:5)(cid:8)(cid:17)(cid:5)(cid:8)(cid:3)(cid:11)(cid:5)(cid:13)
(cid:20)(cid:10)(cid:13)(cid:19)(cid:7)(cid:13)(cid:3)(cid:6)(cid:11)(cid:16)(cid:9)(cid:13)
(cid:12)(cid:8)(cid:13)(cid:9)(cid:10)(cid:14)(cid:14)(cid:7)(cid:10)(cid:17)(cid:17)(cid:11)(cid:9)(cid:11)(cid:2)(cid:8)(cid:10)(cid:14)(cid:7)
(cid:18)(cid:5)(cid:8)(cid:3)(cid:19)(cid:15)(cid:10)(cid:6)(cid:4)(cid:13)
(cid:20)(cid:10)(cid:13)(cid:19)(cid:7)(cid:13)(cid:3)(cid:6)(cid:11)(cid:16)(cid:9)(cid:13)
(cid:13)(cid:5)(cid:9)(cid:21)(cid:16)(cid:7)(cid:5)(cid:22)(cid:16)(cid:5)(cid:6)(cid:11)(cid:15)(cid:5)(cid:8)(cid:9)
(cid:6)(cid:21)(cid:8)(cid:7)(cid:5)(cid:22)(cid:16)(cid:5)(cid:6)(cid:11)(cid:15)(cid:5)(cid:8)(cid:9)
(cid:13)(cid:6)(cid:3)
(cid:20)(cid:21)(cid:11)(cid:14)(cid:17)
(cid:25)(cid:10)(cid:4)(cid:5)(cid:26)(cid:11)(cid:14)(cid:5)(cid:13)
(cid:18)(cid:11)(cid:8)(cid:10)(cid:6)(cid:28)
(cid:23)(cid:21)(cid:8)
(cid:27)(cid:28)(cid:9)(cid:19)(cid:2)(cid:8)
(cid:14)(cid:2)(cid:29)
(cid:24)(cid:2)(cid:14)(cid:14)(cid:5)(cid:3)(cid:9)
(cid:27)(cid:10)(cid:8)(cid:17)(cid:10)(cid:13)
(cid:13)(cid:9)(cid:10)(cid:9)(cid:13)
(cid:16)(cid:14)(cid:2)(cid:9)
(cid:27)(cid:14)(cid:2)(cid:9)
(cid:25)(cid:10)(cid:9)(cid:16)(cid:14)(cid:2)(cid:9)(cid:14)(cid:11)(cid:18)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:3)(cid:2)(cid:8)(cid:9)(cid:10)(cid:11)(cid:8)(cid:5)(cid:6)(cid:7)(cid:7)
Fig. 1: System interface of FEX.
thus hinder reproducibility.
• Installing dependencies implies tools required for
the build process or for specialized measurements.
For example, several PARSEC benchmarks require
gettext system for Autoconf—this software does
not affect performance but is simply needed to
resolve all build dependencies of a particular
benchmark. These tools are optional and may not
be needed for simple experiments.
• Installing additional benchmarks may be necessary
to perform experiments on large unmodiﬁed
programs. FEX encourages to put program sources
in its repository;
this simpliﬁes changing and
tweaking original code to the user’s needs. However,
sometimes it is simpler to fetch the sources from
elsewhere. For example, we install Apache and Nginx
in this way because we want to experiment with
their different versions (those that are vulnerable to
a particular bug and those that are not).
Experiment Runs. After installing all prerequisites, users
can start running experiments. All experiments are
usually performed as a sequence of steps depicted in
Figure 1 (bottom):
• The build step is performed once before running
each benchmark in the experiment. FEX consults
the makeﬁle corresponding to the benchmark-to-run
and puts a ﬁnal binary in the build directory.
It
to re-build all benchmarks for
each experiment, otherwise a mix of old and new
compilation ﬂags and/or libraries could skew the
results. For quick preliminary experiments, the build
step can be omitted via --no-build ﬂag.
is important
• The run step is the experiment itself. FEX includes
a Run component, which provides several Python
hooks to specify a list of benchmarks-to-run
with their inputs and to control how exactly
these benchmarks are started. For example, we
implement an additional “dry run” for Phoenix
benchmarks using a per_benchmark_action
hook. Multithreaded benchmarks are automatically
run with a set of number of threads speciﬁed in the
command line, e.g., -m 1 2 4.
• The collect step parses the log, extracts the measure-
ment results, processes them in a user-speciﬁed way,
and stores into a CSV table. We use Pandas Python li-
brary [17] for efﬁcient data analysis and aggregation.
• The plot step is performed after the whole experiment
is ﬁnished. It is usually performed on a local user
machine and a the remote server. The powerful
matplotlib [18] is used to emit different kinds of
544
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:02:52 UTC from IEEE Xplore.  Restrictions apply. 
(cid:16)
(cid:19)
(cid:11)
(cid:11)
(cid:19)
(cid:18)
(cid:23)
(cid:16)
(cid:4)
(cid:11)
(cid:8)
(cid:17)
(cid:4)
(cid:3)
(cid:22)
(cid:21)
(cid:3)