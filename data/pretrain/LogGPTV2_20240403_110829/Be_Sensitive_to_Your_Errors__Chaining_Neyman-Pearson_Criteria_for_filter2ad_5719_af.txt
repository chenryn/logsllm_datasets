### 3. 实验结果

#### 3.1 分类性能
下表展示了不同恶意软件家族的分类性能，包括误报率 (FPR) 和检测率 (DR)。可以看到，不同配置下的 FPR 和 DR 存在显著差异。

| 配置 | FPR (%) | DR (%) |
|------|---------|--------|
| 1    | 8.0     | 89.2   |
| 2    | 4.3     | 97.0   |
| 3    | 0.9     | 99.2   |
| 4    | 3.0     | 97.8   |
| 5    | 5.6     | 81.3   |
| 6    | 7.1     | 87.2   |
| 7    | 7.3     | 95.8   |
| 8    | 18.2    | 75.2   |
| 9    | 0.5     | 99.4   |
| 10   | 1.2     | 99.2   |
| 11   | 3.0     | 97.3   |
| 12   | 0.7     | 99.0   |

进一步的实验结果显示：

- **配置 1**：FPR = 3.5%，DR = 22.1%
- **配置 2**：FPR = 8.5%，DR = 94.4%
- **配置 3**：FPR = 2.5%，DR = 97.8%
- **配置 4**：FPR = 1.4%，DR = 98.2%
- **配置 5**：FPR = 15.3%，DR = 72.2%
- **配置 6**：FPR = 7.4%，DR = 80.1%
- **配置 7**：FPR = 21.8%，DR = 95.8%
- **配置 8**：FPR = 8.0%，DR = 46.0%
- **配置 9**：FPR = 3.5%，DR = 96.8%
- **配置 10**：FPR = 1.4%，DR = 98.9%
- **配置 11**：FPR = 3.4%，DR = 95.0%
- **配置 12**：FPR = 2.9%，DR = 98.3%

#### 3.2 执行时间
执行时间方面，使用多核处理器可以进一步减少处理时间。具体来说，Rbot 家族的分类器执行时间最长，平均为 0.776 秒；而 Sdbot 家族的分类器执行时间最短，平均为 0.0466 秒。这些差异可能归因于特征选择、正样本比例、早期检测的可能性以及 SVM-Light 在不同配置下的执行时间等因素。

### 4. 遗传算法分析
接下来，我们展示遗传算法在寻找最优配置方面的表现。通过对 12 个恶意软件家族的遗传算法执行情况进行分析，发现约 50.8% 的情况未能返回合理的解决方案，导致表 1 中未使用的特征类型（标记为 '-'）。

对于能够返回合理解决方案的情况，各代次的分布如下：

| 代次 | 全突变方案占比 | 部分交叉方案占比 | 完全交叉方案占比 |
|------|---------------|-----------------|-----------------|
| 1    | 49.2%         | 0               | 0               |
| 2    | 24.6%         | 22.2%           | 27.1%           |
| 3    | 26.2%         | 35.6%           | 39.6%           |

从上表可以看出，几乎一半的配置来自第一代，主要使用全突变随机生成解。第二代和第三代各贡献了约四分之一的合理解。值得注意的是，在第二代和第三代中，全突变方案仍然贡献了相当一部分解，但随着进化过程的进行，全突变方案的比例逐渐减少。这是因为进化过程通过交叉和部分突变逐步提高了种群的质量。

这表明，遗传算法能够结合全局搜索和局部搜索的优势来找到最优配置。如果只有少量搜索次数，全局搜索（通过全突变随机探索整个配置空间）更有效；但如果允许更多的搜索次数，局部搜索（如交叉或部分突变）可以在有良好解的区域集中搜索，从而更有效地改进当前解。

### 5. 相关工作与讨论
由于机器学习在可扩展性方面的优势，它已被应用于许多先前的研究中，以区分恶意程序和良性程序（例如 [30, 14, 25, 27, 1, 32]）。恶意软件检测与恶意软件分类是不同的任务，本研究的主题是将恶意软件变体分类到相应的家族中。

在一些最近的工作中，机器学习也被用于自动化恶意软件分类过程（例如 [21, 40, 15, 19, 38, 20, 39]）。这些工作的主要区别在于所使用的特征类型。虽然本研究中使用的恶意软件特征并不全面，但它们涵盖了静态分析和动态分析中的关键特征，并且以前研究中考虑的特征可以很容易地整合到基于链式 Neyman-Pearson 准则的恶意软件分类框架中。更重要的是，我们的工作与其他先前努力的区别在于，我们考虑了不同类型错误的不同要求。例如，如果我们想研究一个恶意软件家族的趋势，我们希望该家族中的大多数样本确实属于这个家族，这就要求我们有一个低误报率的恶意软件分类器，即使这意味着要牺牲其检测率。因此，在恶意软件分类中平衡误报率和检测率是非常重要的，这是现有方法无法实现的。

本研究的性能评估依赖于一个从防病毒软件检测结果共识中获得标签的恶意软件数据集。已知这种方法可能会导致数据集偏斜 [16, 18]。尽管如此，使用相同的数据集，提出的解决方案在分类性能上优于标准集成学习技术。未来，我们计划使用具有真实标签的其他恶意软件数据集来进一步评估我们的方法。

与许多其他机器学习技术一样，提出的集成分类器假设测试数据具有与训练数据相同或相似的分布。然而，恶意软件家族中的变体群体可能是非平稳的，这在机器学习中称为概念漂移 [33]。在对抗环境中，概念漂移会给恶意软件分类带来重大挑战 [13]。概念漂移要求我们监测恶意软件群体的变化；如果观察到突然变化，则需要重新训练分类器。适应提出的集成恶意软件分类器以处理概念漂移是我们未来的工作。

在本研究中，我们使用成本敏感的 SVM 作为训练单个分类器的基本模块。Davenport 等人考虑了如何根据 Neyman-Pearson 准则调整成本敏感 SVM 中的成本 [7]。使用的遗传算法可以轻松并行化，并且只有一个参数（即代数）来控制搜索次数。现有的集成学习技术，如 Boosting、Bagging 和 Stacking [9]，可以与我们的工作正交：当在我们的恶意软件分类框架中考虑特定特征类型时，它们可以用来改进单个分类器的性能。此外，我们工作中的多分类器组合方式也不同于这些现有技术。首先，我们的方法中使用的“或”规则使我们能够利用多种特征类型之间的相关性：如果添加新的特征类型不能提高分类性能，我们的分类框架不会从恶意软件样本中收集其值。其次，缺失的特征值使得直接应用现有方法变得困难。例如，给定一个恶意软件样本，如果我们无法收集特定特征类型的值，我们就无法在 Boosting 算法中为其分配权重。第三，通过应用链式 Neyman-Pearson 准则，我们递归地训练一组分类器，以达到最佳的整体性能。在这里，分类器集合的性能是根据 Neyman-Pearson 准则评估的，而不是某些现有方法中使用的分类错误。

### 致谢
感谢匿名审稿人的评论和我们的论文指导者 Aziz Mohaisen 对最终版本的帮助。

### 参考文献
[1] B. Anderson, D. Quist, J. Neil, C. Storlie, and T. Lane. Graph-based malware detection using dynamic analysis. Journal of Computer Virology, 7(4):247–258, 2011.
[2] http://securitywatch.pcmag.com/security/323419-symantec-says-antivirus-is-dead-world-rolls-eyes.
[3] E. B. Baum and D. Haussler. What size net gives valid generalization? Neural computation, 1(1):151–160, 1989.
[4] http://cnx.org/content/m11548/1.2/.
[5] C. J.C. Burges. A tutorial on support vector machines for pattern recognition. Data mining and knowledge discovery, 2(2):121–167, 1998.
[6] O. Chapelle, B. Schölkopf, and A. Zien. Semi-supervised learning, volume 2. MIT press Cambridge, 2006.
[7] M. A. Davenport, R. G. Baraniuk, and C. D. Scott. Tuning support vector machines for minimax and neyman-pearson classification. IEEE Trans. Pattern Anal. Mach. Intell., 32(10):1888–1898, October 2010.
[8] T. G. Dietterich. Ensemble methods in machine learning. In Multiple classifier systems. Springer, 2000.
[9] T. Hastie, R. Tibshirani, and J. Friedman. The elements of statistical learning: Data Mining, Inference, and Prediction. Springer, 2009.
[10] H. He and E. A. Garcia. Learning from imbalanced data. IEEE Trans. on Knowledge and Data Engineering, 2009.
[11] http://www.imperva.com/docs/HII_Assessing_the_Effectiveness_of_Antivirus_Solutions.pdf.
[12] http://www.pintool.org/.
[13] A. Kantchelian, S. Afroz, L. Huang, A. C. Islam, B. Miller, M. C. Tschantz, R. Greenstadt, A. D. Joseph, and J. D. Tygar. Approaches to adversarial drift. In ACM AISec’13.
[14] J. Z. Kolter and M. A. Maloof. Learning to detect and classify malicious executables in the wild. Journal of Machine Learning Research, 7:2721–2744, December 2006.
[15] D. Kong and G. Yan. Discriminant malware distance learning on structural information for automated malware classification. In Proceedings of ACM KDD’13, 2013.
[16] P. Li, L. Liu, D. Gao, and M. K Reiter. On challenges in evaluating malware clustering. In RAID’10.
[17] http://www.csie.ntu.edu.tw/~cjlin/libsvm/.
[18] A. Mohaisen and O. Alrawi. AV-meter: An evaluation of antivirus scans and labels. In Proceedings of DIMVA’14.
[19] A. Mohaisen and O. Alrawi. Unveiling zeus: Automated classification of malware samples. In Proceedings of the 22nd international conference on WWW companion, 2013.
[20] A. Mohaisen, A. G. West, A. Mankin, and O. Alrawi. Chatter: Exploring classification of malware based on the order of events. In IEEE Conference on Communications and Network Security (CNS’14).
[21] L. Nataraj, V. Yegneswaran, P. Porras, and J. Zhang. A comparative assessment of malware classification using binary texture analysis and dynamic analysis. In Proceedings of ACM AISec’11.
[22] http://www.offensivecomputing.net/.
[23] http://orange.biolab.si/.
[24] N. C. Oza and K. Tumer. Classifier ensembles: Select real-world applications. Information Fusion, 9(1), 2008.
[25] R. Perdisci, A. Lanzi, and W. Lee. Mcboost: Boosting scalability in malware collection and analysis using statistical classification of executables. In ACSAC’08.
[26] K. Raman. Selecting features to classify malware. In Proceedings of InfoSec Southwest, 2012.
[27] K. Rieck, P. Trinius, C. Willems, and T. Holz. Automatic analysis of malware behavior using machine learning. J. Comput. Secur., 19(4):639–668, December 2011.
[28] C. Rossow, C. J. Dietrich, C. Grier, C. Kreibich, V. Paxson, N. Pohlmann, H. Bos, and M. Van Steen. Prudent practices for designing malware experiments: Status quo and outlook. In IEEE Symposium on Security and Privacy, pages 65–79. IEEE, 2012.
[29] M. Saar-Tsechansky and F. Provost. Handling missing values when applying classification models. Journal of Machine Learning Research, 8:1623–1657, December 2007.
[30] M. G. Schultz, E. Eskin, E. Zadok, and S. J. Stolfo. Data mining methods for detection of new malicious executables. In IEEE Symposium on Security and Privacy, 2001.
[31] C. Scott and R. Nowak. A neyman-pearson approach to statistical learning. IEEE Transactions on Information Theory, 51(11), 2005.
[32] M. Z. Shafiq, S. M. Tabish, F. Mirza, and M. Farooq. Pe-miner: Mining structural information to detect malicious executables in realtime. In RAID’09.
[33] A. Singh, A. Walenstein, and A. Lakhotia. Tracking concept drift in malware families. In Proceedings of ACM AISec’12.
[34] http://svmlight.joachims.org/.
[35] Symantec Internet security threat report. Symantec Corporation, 2011.
[36] V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability & Its Applications, 16(2):264–280, 1971.
[37] https://www.virustotal.com/.
[38] A. G. West and A. Mohaisen. Metadata-driven threat classification of network endpoints appearing in malware. In Proceedings of DIMVA’14.
[39] Z. Xu, J. Zhang, G. Gu, and Z. Lin. Autovac: Towards automatically extracting system resource constraints and generating vaccines for malware immunization.
[40] G. Yan, N. Brown, and D. Kong. Exploring discriminatory features for automated malware classification. In Detection of Intrusions and Malware, and Vulnerability Assessment (DIMVA’13). 2013.

---

通过上述优化，文本变得更加清晰、连贯和专业。希望这些修改对你有帮助！