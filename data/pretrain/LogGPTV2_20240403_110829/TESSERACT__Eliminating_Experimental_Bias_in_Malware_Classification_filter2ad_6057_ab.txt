Dec 2016 as the ﬁnishing time ensures good ground-truth
conﬁdence in objects labeled as malware.
Dataset summary. The ﬁnal dataset consists of 129,728
Android applications (116,993 goodware and 12,735 mal-
ware). Figure 1 reports a stack histogram showing the per-
month distribution of goodware/malware in the dataset. For
the sake of clarity, the ﬁgure also reports the number of mal-
ware and goodware in each bin. The training and testing splits
USENIX Association
28th USENIX Security Symposium    731
JFMAMJJ2014ASONDJFMAMJJ2015ASONDJFMAMJJ2016ASOND050010001500200025003000350040004500500055006000650070007500800085009000950010000105001100038326338148560872410724993995911451471441292151832212162752723543133743693444766356124522783525641488134403067348541015081606687454788417663451305144415641231188818381953209825212768307729323010328934783959581863684052280630274881137775823190867TrainingTestingGoodwareMalwareused in §3 are reported in Table 1; all the time-aware experi-
ments in the remainder of this paper are performed by training
on 2014 and testing on 2015 and 2016 (see the vertical dotted
line in Figure 1).
3 Sources of Experimental Bias
In this section, we motivate our discussion of bias through
experimentation with ALG1 [4] and ALG2 [33] (§3.1). We
then detail the sources of temporal (§3.2) and spatial bias
(§3.3) that affect ML-based Android malware classiﬁcation.
3.1 Motivational Example
We consider a motivational example in which we vary the
sources of experimental bias to better illustrate the problem.
Table 1 reports the F1-score for ALG1 and ALG2 under vari-
ous experimental conﬁgurations; rows correspond to different
sources of temporal experimental bias, and columns corre-
spond to different sources of spatial experimental bias. On
the left-part of Table 1, we use squares ((cid:4)/(cid:4)) to show from
which time frame training and testing objects are taken; each
square represents six months (in the window from Jan 2014 to
Dec 2016). Black squares ((cid:4)) denote that samples are taken
from that six-month time frame, whereas periods with gray
squares ((cid:4)) are not used. The columns on the right part of the
table correspond to different percentages of malware in the
training set Tr and the testing set T s.
Table 1 shows that both ALG1 and ALG2 perform far worse
in realistic settings (bold values with green background in
the last row, for columns corresponding to 10% malware in
testing) than in settings similar to those presented in [4, 33]
(bold values with red background). This is due to inadvertent
experimental bias as outlined in the following.
Note. We clarify to which similar settings of [4, 33] we re-
fer to in the cells with red background in Table 1. The paper of
ALG2 [33] reports in the abstract performance “up to 99% F1”,
which (out of the many settings they evaluate) corresponds
to a scenario with 86% malware in both training and test-
ing, evaluated with 10-fold CV; here, we rounded off to 90%
malware for a cleaner presentation (we have experimentally
veriﬁed that results with 86% and 90% malware-to-benign
class ratio are similar). ALG1’s original paper [4] relies on
hold-out by performing 10 random splits (66% training and
33% testing). Since hold-out is almost equivalent to k-fold CV
and suffers from the same spatio-temporal biases, for the sake
of simplicity in this section we refer to a k-fold CV setting
for both ALG1 and ALG2.
3.2 Temporal Experimental Bias
Concept drift is a problem that occurs in machine learning
when a model becomes obsolete as the distribution of incom-
ing data at test-time differs from that of training data, i.e.,
when the assumption does not hold that data is independent
and identically distributed (i.i.d.) [26]. In the ML community,
this problem is also known as dataset shift [50]. Time decay
is the decrease in model performance over time caused by
concept drift.
Concept drift in malware combined with similarities among
malware within the same family causes k-fold cross validation
(CV) to be positively biased, artiﬁcially inﬂating the perfor-
mance of malware classiﬁers [2,36,37]. K-fold CV is likely to
include in the training set at least one sample of each malware
family in the dataset, whereas new families will be unknown
at training time in a real-world deployment. The all-black
squares in Table 1 for 10-fold CV refer to each training/testing
fold of the 10 iterations containing at least one sample from
each time frame. The use of k-fold CV is widespread in mal-
ware classiﬁcation research [11,12,27,31,34,37,41,49,51,57];
while a useful mechanism to prevent overﬁtting [8] or esti-
mate the performance of a classiﬁer in the absence of concept
drift when the i.i.d. assumption holds (see considerations in
§4.4), it has been unclear how it affects the real-world perfor-
mance of machine learning techniques with non-stationary
data that are affected by time decay. Here, in the ﬁrst row of
Table 1, we quantify the performance impact in the Android
domain.
The second row of Table 1 reports an experiment in which
a classiﬁer’s ability to detect past objects is evaluated [2, 33].
Although this characteristic is important, high performance
should be expected from a classiﬁer in such a scenario: if the
classiﬁer contains at least one variant of a past malware family,
it will likely identify similar variants. We thus believe that
experiments on the performance achieved on the detection of
past malware can be misleading; the community should focus
on building malware classiﬁers that are robust against time
decay.
In the third row, we identify a novel temporal bias that
occurs when goodware and malware correspond to different
time periods, often due to having originated from different
data sources (e.g., in [33]). The black and gray squares in
Table 1 show that, although malware testing objects are poste-
rior to malware training objects, the goodware/malware time
windows do not overlap; in this case, the classiﬁer may learn
to distinguish applications from different time periods, rather
than goodware from malware—again leading to artiﬁcially
high performance. For instance, spurious features such as
new API methods may be able to strongly distinguish objects
simply because malicious applications predate that API.
The last row of Table 1 shows that the realistic setting,
where training is temporally precedent to testing, causes the
worst classiﬁer performance in the majority of cases. We
present decay plots and a more detailed discussion in §4.
3.3 Spatial Experimental Bias
We identify two main types of spatial experimental bias based
on assumptions on percentages of malware in testing and
training sets. All experiments in this section assume temporal
consistency. The model is trained on 2014 and tested on 2015
and 2016 (last row of Table 1) to allow the analysis of spatial
732    28th USENIX Security Symposium
USENIX Association
Experimental setting
10-fold CV
Temporally inconsistent
Temporally inconsistent
gw/mw windows
Temporally consistent
(realistic)
Sample dates
Training
Testing
gw: (cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4) gw: (cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)
mw: (cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4) mw: (cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)
gw: (cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4) gw: (cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)
mw: (cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4) mw: (cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)
gw: (cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4) gw: (cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)
mw: (cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4) mw: (cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)
gw: (cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4) gw: (cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)
mw: (cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4) mw: (cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)
% mw in testing set Ts
10% (realistic)
% mw in training set Tr
10% 90% 10% 90%
ALG2 [33]
ALG1 [4]
90% (unrealistic)
% mw in training set Tr
10% 90% 10% 90%
ALG2 [33]
ALG1 [4]
0.91
0.56
0.83
0.32
0.94
0.98
0.85
0.76
0.42
0.49
0.21
0.86
0.93
0.54
0.77
0.70
0.65
0.56
0.79
0.94
0.65
0.97
0.95
0.93
0.58
0.45
0.32
0.30
0.62
0.94
0.33
0.96
Table 1: F1-Score results that show impact of spatial (in columns) and temporal (in rows) experimental bias. Values with red
backgrounds are experimental results of (unrealistic) settings similar to those considered in papers of ALG1 [4] and ALG2 [33];
values with green background (last row) are results in the realistic settings we identify. The dataset consists of three years (§2.3),
and each square on the left part of the table represents a six month time-frame: if training (resp. testing) objects are sampled from
that time frame, we use a black square ((cid:4)); if not, we use a gray square ((cid:4)).
bias without the interference of temporal bias.
Spatial experimental bias in testing. The percentage of
malware in the testing distribution needs to be estimated
(§2.2) and cannot be changed, if one wants results to be repre-
sentative of in-the-wild deployment of the malware classiﬁer.
To understand why this leads to biased results, we artiﬁcially
vary the testing distribution to illustrate our point. Figure 2
reports performance (F1-Score, Precision, Recall) for increas-
ing the percentage of malware during testing on the X-axis.
We change the percentage of malware in the testing set by ran-
domly downsampling goodware, so the number of malware
remains ﬁxed throughout the experiments.2 For completeness,
we report the two training settings from Table 1 with 10%
and 90% malware, respectively.
Let us ﬁrst focus on the malware performance (dashed
lines). All plots in Figure 2 exhibit constant Recall, and in-
creasing Precision for increasing percentage of malware in
the testing. Precision for the malware (mw) class—the pos-
itive class—is deﬁned as Pmw = T P/(T P + FP) and Recall as
Rmw = T P/(T P + FN). In this scenario, we can observe that TPs
(i.e., malware objects correctly classiﬁed as malware) and FNs
(i.e., malware objects incorrectly classiﬁed as goodware) do
not change, because the number of malware does not increase;
hence, Recall remains stable. The increase in number of FPs
(i.e., goodware objects misclassiﬁed as malware) decreases
as we reduce the number of goodware in the dataset; hence,
Precision improves. Since the F1-Score is the harmonic mean
of Precision and Recall, it goes up with Precision. We also
observe that, inversely, the Precision for the goodware (gw)
class—the negative class—Pgw = T N/(T N + FN) decreases (see
yellow solid lines in Figure 2), because we are reducing the
TNs while the FNs do not change. This example shows how
considering an unrealistic testing distribution with more mal-
ware than goodware in this context (§2.2) positively inﬂates
Precision and hence the F1-Score of the malware classiﬁer.
2We choose to downsample goodware to achieve up to 90% of malware
Spatial experimental bias in training. To understand the
impact of altering malware-to-goodware ratios in training, we
now consider a motivating example with a linear SVM in a
2D feature space, with features x1 and x2. Figure 3 reports
three scenarios, all with the same 10% malware in testing, but
with 10%, 50%, and 90% malware in training.
We can observe that with an increasing percentage of mal-
ware in training, the hyperplane moves towards goodware.
More formally, it improves Recall of malware while reducing
its Precision. The opposite is true for goodware. To mini-
mize the overall error rate Err = (FP + FN)/(T P + T N + FP + FN)
(i.e., maximize Accuracy), one should train the dataset with
the same distribution that is expected in the testing. However,
in this scenario one may have more interest in ﬁnding objects
of the minority class (e.g., “more malware”) by improving
Recall subject to a constraint on maximum FPR.
Figure 4 shows the performance for ALG1 and ALG2, for
increasing percentages of malware in training on the X-axis;
just for completeness (since one cannot artiﬁcially change the
test distribution to achieve realistic evaluations), we report
results both for 10% mw in testing and for 90% malware in
testing, but we remark that in the Android setting we have
estimated 10% mw in the wild (§2.2). These plots conﬁrm
the trend in our motivating example (Figure 3), that is, Rmw
increases but Pmw decreases. For the plots with 10% mw in
(mw) for testing because of the computational and storage resources required
to achieve such a ratio by oversampling. This does not alter the conclusions
of our analysis. Let us assume a scenario in which we keep the same number
of goodware (gw), and increase the percentage of mw in the dataset by over-
sampling mw. The precision (Pmw = T P/(T P + FP)) would increase because TPs
would increase for any mw detection, and FPs would not change—because
the number of gw remains the same; if training (resp. testing) observations
are sampled from a distribution similar to the mw in the original dataset (e.g.,
new training mw is from 2014 and new testing mw comes from 2015 and
2016), then Recall (Rmw = T P/(T P + FN)) would be stable—it would have the
same proportions of TPs and FNs because the classiﬁer will have a similar
predictive capability for ﬁnding mw. Hence, if the number of mw in the
dataset increases, the F1-Score would increase as well, because Precision
increases while Recall remains stable.
USENIX Association
28th USENIX Security Symposium    733
(a) ALG1: Train with 10% mw
(b) ALG1: Train with 90% mw
(c) ALG2: Train with 10% mw
(d) ALG2: Train with 90% mw
Figure 2: Spatial experimental bias in testing. Training on 2014 and testing on 2015 and 2016. For increasing % of malware in
the testing (unrealistic setting), Precision for malware increases and Recall remains the same; overall, F1-Score increases for
increasing percentage of malware in the testing. However, having more malware than goodware in testing does not reﬂect the
in-the-wild distribution of 10% malware (§2.2), so the setting with more malware is unrealistic and reports biased results.
testing, we observe there is a point in which F1-Scoremw is
maximum while the error for the gw class is within 5%.
In §4.3, we propose a novel algorithm to improve the per-
formance of the malware class according to the objective
of the user (high Precision, Recall or F1-Score), subject to a
maximum tolerated error. Moreover, in §4 we introduce con-
straints and metrics to guarantee bias-free evaluations, while
revealing counter-intuitive results.
4 Space-Time Aware Evaluation
We now formalize how to perform an evaluation of an Android
malware classiﬁer free from spatio-temporal bias. We deﬁne
a novel set of constraints that must be followed for realistic
evaluations (§4.1); we introduce a novel time-aware metric,
AUT, that captures in one number the impact of time decay
on a classiﬁer (§4.2); we propose a novel tuning algorithm
that empirically optimizes a classiﬁer performance, subject
to a maximum tolerated error (§4.3); ﬁnally, we introduce
TESSERACT and provide counter-intuitive results through
unbiased evaluations (§4.4). To improve readability, we report
in Appendix A.2 a table with all the major symbols used in
the remainder of this paper.
4.1 Evaluation Constraints