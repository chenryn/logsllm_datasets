hypotheses 
loss in TeraGrid, 
themselves: 
suggest 
we can only speculate 
loss rates. Several 
978-1-4244-7501-8/101$26.00 
©2010 IEEE 
577 
DSN 2010: Marian et al. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 13:56:42 UTC from IEEE Xplore.  Restrictions apply. 
2010 IEEE/IFIP International 
Conference 
on Dependable Systems & Networks (DSN) 
Ingress 
1o::==:-II'End-host 
Egress 
End-host 10::==:-11' 
Figure 3. Topology of Cornell NLR Rings. 
Figure 4. Test traffic on large NLR Ring, as 
observed by NLR Realtime Atlas monitor [5]. 
3 Experimental 
Measurements 
In this section, 
we use the Cornell NLR Rings testbed 
to the traffic 
with respect 
to answer the following 
characteristics 
questions 
over uncongested 
lambda networks: 
Los Angeles,  Houston,  Atlanta,  Washington 
D.C., 
3). The one-way latency  (one 
• Under what conditions 
does packet loss occur, where 
does packet loss take place, and how is packet loss 
fected by NIC interrupt 
affinity? (Section 
3.2) 
af­
The NLR routers 
are CRS-l devices, 
while the Cornell 
3.1 Experimental  Setup 
are Catalyst 
6500 se­
all have sufficient 
backplane 
Our experiments 
generate 
UDP and TCP Iperf [33] traf­
New York City, and a large ring across Chicago, 
Denver, 
Seattle, 
and New York City (Figure 
trip around the ring) as reported 
8.0 ms for the tiny path, 37.3 ms for the small path, 68.9 ms 
for the medium path, and 97.5 ms for the large path. All op­
tical point-to-point 
Wavelength 
single OC-192 Synchronous 
Optical 
link between Chicago and Atlanta. 
(DWDM) , except for a 
Networking 
backbone links use 10GbE with Dense 
by the ping utility 
Multiplexing 
Division 
(SONET) 
is 
at their full rate of 10Gbps irrespective 
the loads generated 
in our experiments 
to operate 
no evidence 
These routers 
of Service (QoS) feature 
and NYC) backbone routers 
(Ithaca 
ries hardware. 
capacity 
of the traffic pattern; 
thus far have provided 
Quality 
abled, hence in the event of an over-run, 
likely to be discarded. 
In particular, 
the order in which they are received. 
all subsequent 
referred 
tail [ 15]. Enabling 
of the production 
not currently 
to as first-in-first-out 
QoS requires 
are dropped, 
feasible. 
packets 
to the contrary. 
The 
was dis­
on these routers 
all traffic is equally 
packets are served in 
If the buffer is full, 
a discipline 
sometimes 
with drop­
(FIFO) queueing 
wholesale 
reconfiguration 
NLR network by NLR engineers, 
and is 
• What is the impact of packet loss, path length, 
window 
size, and congestion 
put? (Section 
3.3) 
control 
variant 
on TCP through­
• How does packet batching 
and latency 
measurements? 
affect overall 
(Section 
3.4) 
throughput 
depicted 
over all paths, i.e. 
end-hosts 
and Egress end-hosts 
fic between the two commodity 
between the Ingress 
in Fig­
ure 3. We modified Iperf to report (for UDP traffic) pre­
cisely which packets were lost and which were received 
run, we 
out of order. Before and after every experimental 
on both sender and receiver 
that ac­
read kernel counters 
count for packets 
in the DMA 
ring, socket buffer, or TCP window. The default 
each receive 
while the MTU (Maximum Transfer 
fault 1500 bytes (we did not use jumbo frames). 
specified 
coalescence 
Throughout 
3.4), both NAPI and interrupt 
techniques 
(Section 
packet batching 
being dropped at the end-host 
(tx) DMA ring is 1024 slots, 
our experiments, 
(rx) and transmit 
Unit) is set to the de­
are enabled. 
otherwise 
Unless 
size of 
all NLR network segments 
of fact, the background 
traf­
Figure 4 shows the topology 
of the large path and high­
while we 
layer-3 
controlled 
load on the entire NLR backbone 
2Gbps UDP traffic experiments 
the Figure legend also demonstrates 
lights the 
performed 
this path. Importantly, 
that the backbone (and our path) is uncongested. 
tests were performed, 
the backbone, 
20%, corresponding 
While our 
of the rest of 
showed a level of link utilization 
the large path, exclusive 
to our test traffic. 
of roughly 
directly 
were uncongested-as 
a matter 
fic over each link never exceeded 
by the monitoring 
are averaged 
bars denote standard 
of the time sufficiently 
over multiple 
over 
system [5] every 1-5 seconds). 
5% utilization 
(computed 
All values 
runs, and the error 
independent 
error-they 
are always present, 
most 
small to be invisible. 
978-1-4244-7501-8/10/$26.00 
©2010 IEEE 
578 
DSN 2010: Marian et al. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 13:56:42 UTC from IEEE Xplore.  Restrictions apply. 
2010 IEEE/IFIP International 
Conference 
on Dependable Systems & Networks (DSN) 
sockbuf_loss
rx_ring_loss
network_loss
]
%
[
s
s
o
l
t
e
k
c
a
P
0.05
0.04
0.03
0.02
0.01
0.00
tiny
small medium large
4
0
0
8
0
0
1
2
0
0
1
6
0
0
2
0
0
0
2
4
0
0
4
0
0
8
0
0
1
2
0
0
1
6
0
0
2
0
0
0
2
4
0
0
4
0
0
8
0
0
1
2
0
0
1
6
0
0
2
0
0
0
2
4
0
0
4
0
0
8
0
0
1
2
0
0
1
6
0
0
2
0
0
0
2
4
0
0
tiny
small
medium
large
(a) 1MB buffers, balanced interrupts
 40
combinations 
 35
of 
interrupts. 
0.05
]
%
0.04
[
s
s
o
l
t
e
k
c
a
P
0.03
0.02
0.01
0.00
tiny
small medium large
sockbuf_loss
rx_ring_loss
network_loss
location 
where loss can occur. In particular, 
3.2 Packet Loss 
To measure packet loss over the Cornell NLR Rings 
 40
 35
]
%
 20
[
s
s
o
l
t
e
k
c
a
P
UDP 
ex­
of 60-second 
we performed 
many sequences 
We examined the following 
of sender and receiver 
testbed, 
 30
Iperf runs over a period of 48 hours. We consecutively 
 25
plored all paths (tiny, short, medium, and large) for data 
rates between 400Mbps to 2400Mbps, with 400Mbps in­
tervals. 
six different 
in 
rations 
all cases): socket buffers sized at 1, 2, or 4MB; and use 
of either the irqbalance [I] daemon or static assign­
ment of interrupts 
irqbalance daemon uses the kernel CPU affinity inter­
face (through 
periodically 
sors in order to increase 
hardware interrupts 
performance. 
Figure 5 shows our measurements 
/proc/irq/IRQ#/smp_affinity) and 
issued by the NICs to specific 
of UDP packet loss, 
(both identical 
re-assigns 
across proces­
end-hosts 
CPUs. The 
configu­
 10
 15
 0
 5
]
 5
 0
%
 30
[
s
s
o
l
t
e
k
c
a
P
observed 
the receive 
end-host, 
to different 
of over-running 
or numerous factors 
into three components 
 25
packets, 
 20
the socket buffer 
(rx) DMA ring 
y-axes to better view trends. 
 15
denoting 
 10
by Iperf on the 
of transmitted 
sender data rates across each of the Cornell 
NLR 
with sub figures corresponding 
socket buffer size and bound versus balanced 
Each sub figure plots packet  loss 
receiver 
as a percentage 
for various 
Ring; insets provide rescaled 
Packet loss is subdivided 
the precise 
loss may be a consequence 
(sockbuLlos s), over-running 
(rx_ring_loss), 
core (networLloss). Since NAPI is enabled, 
no backlog queue (to over-run) 
the socket buffer. Moreover, 
sibilities 
sender socket buffer is never over-run 
hour duration 
blocking 
(tx) DMA ring is never over-run 
ment; iii) neither 
rors (e.g. corruption) 
throughout 
does not trans­
mit any packets (since we used Iperf with UDP traffic). 
Figure 5(a) considers 
there is 
between the DMA ring and 
the remaining 
pos­
i) the 
reasons: 
during the entire 48-
accordance 
with the 
 35
nature of the socket API; ii) the sender transmit 
 30
 25
 20
(on board) buffer over-runs 
 15
via Irqbalance 
iv) the receiver 
Interrupts 
of the experiment-in 
the sender nor receiver 
loss for the following 
during the entire experi­
the experiment; 
within the network 
for end-host 
NIC report any er­
or internal 
we dismiss 
the 
with the irqbalance daemon running and the 
scenario 
zero loss in the 
socket buffer size set to 1MB. We observe 
network core; all loss occurs within the receiver's 
buffer. At rates beyond 2000 Mbps, irqbalance 
spreads the 
interrupts 
omitted for space constraints, 
buffers result in zero loss for all tested data rates.) 
to many CPUs and the loss decreases. 
with 2 and 4MB 
irqbalance 
(Of note, 
socket 
 10
 5
 0
[
s
s
o
l
t
e
k
c
a
P
 40
%
]
Bound to a Single CPU Figures 5(b) 
Interrupts 
and 5(c) consider 
assign all interrupts 
and 4MB socket buffers, respectively. 
buffer, not shown, are identical 
when we 
from the NIC to a single core, with 1 
for 2MB 
the more interesting 
to those of 4MB, but with 
(The results 
scenario 
4
0
0
8
0
0
1
2
0
0
1
6
0
0
2
0
0
0
2
4
0
0
4
0
0
8
0
0
1
2
0
0
1
6
0
0
2
0
0
0
2
4
0
0
4
0
0
8
0
0
1
2
0
0
1
6
0
0
2
0
0
0
2
4
0
0
4
0
0
8
0
0
1
2
0
0
1
6
0
0
2
0
0
0
2
4
0
0
tiny
small
medium
large
(b) 1MB buffers, bound interrupts
sockbuf_loss
rx_ring_loss
network_loss
]
%
[
s
s
o
l
t
e
k
c
a
P
0.05
0.04
0.03
0.02
0.01
0.00
tiny
small medium large
4
0
0
8
0
0
1
2
0
0
1
6
0
0
2
0
0
0
2
4
0
0
4
0
0
8
0
0
1
2
0
0
1
6
0
0
2
0
0
0
2
4
0
0
4
0
0
8
0
0
1
2
0
0
1
6
0
0
2
0
0
0
2
4
0
0
4
0
0
8
0
0
1
2
0
0
1
6
0
0
2
0
0
0
2
4
0
0
tiny
small
medium
large
(c) 4MB buffers, bound interrupts
Figure 5. UDP loss as a function of data 
rate across Cornell NLR  Rings: subfigures 
show various socket buffer sizes and inter­
rupt options for balancing across or binding 
to cores; insets rescale y-axis, with x-axis un­
changed, to emphasize fine features of loss. 
978-1-4244-7501-8/101$26.00 
©2010 IEEE 
579 
DSN 2010: Marian et al. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 13:56:42 UTC from IEEE Xplore.  Restrictions apply. 
2010 IEEEIIFIP International 
Conference 
on Dependable Systems & Networks (DSN) 
less CPU cache pollution 
overhead. 
3.3 Throughput 
Although 
UDP is well suited for measuring 
packet  loss 
control 
system's 
protocol; 
algorithms 
TCP [21] is the de­
every operating 
where loss occurs, 
communication 
it is embedded in 
network stack. Many 
have been proposed­
rates and indicating 
facto reliable 
virtually 
TCP congestion 
Fast TCP, High Speed TCP, H-TCP, BIC, CUBIC, Hy­
bla, TCP-Illinois, 
TCP,  YeAH-TCP-and 
to improve performance 
links. The existence 
yet no clearly  superior 
almost all have features 
over high-bandwidth, 
there is as 
Westwood, Compound TCP, Scalable 
intended 
of so many variants 
algorithm. 
indicate 
high-latency 
can 
First, 
receive 
in observed 
loss. Taking a 