respond in time [25]. Our fault model assumes only fail-stop
faults and relies on fault detection mechanisms (such as the
ones above) to handle many classes of fail-silent faults in a
“fail-stop fashion”.
Furthermore, our fault model assumes only one failure
(i.e., crash) at a time. We note this is a relatively minor
restriction considering the other assumptions we make. First,
since we assume fail-stop faults, the failing component itself
is unable to execute any more code between the failure
and its recovery, so a second failure cannot happen there.
Second, since our recovery code is trusted (in the small
RCB), no failure can happen in the recovery code itself.
The only possibility of additional failures during recovery is
thus elsewhere in the system. To minimize this possibility,
we temporarily disallow system call processing, stalling the
userland until recovery operations complete.
Finally, an important assumption is that faults may also
occur in core OS components. This is unlike much prior work
that only deals with faults that occur in drivers [24], [16],
[25], [14]. Failures in core OS components are considerably
harder to deal with than driver failures, because the system
requires availability of the core OS components at all times.
Moreover, while drivers typically have relatively simple
and highly standardized communication protocols [24], the
core system components are much more tightly coupled and
need to perform complex interactions to execute a range of
cross-cutting system calls. Akeso [19] and CuriOS [18] also
recover from faults in the OS, but their strategies impose
nontrivial runtime performance and design constraints.
3
III. OVERVIEW
In this section, we ﬁrst give a high-level overview of
OSIRIS’ system architecture and then provide an example
explaining our recovery methodology.
A. System architecture
Its
interact
OSIRIS builds on top of a compartmentalized operating
components
through
system.
fault-isolated
in place Side Effect Engraved
message passing. We put
Passages (SEEPs), wrapping each of the channels transporting
these messages. A SEEP keeps side-effects information,
especially about whether a state dependency arises out of that
interaction or not. SEEPs allow OSIRIS to restrict component
recovery to cases where state changes since the last checkpoint
are not visible outside the crashed component. In the other
cases, the system aborts and does not attempt recovery.
We perform recovery by rolling back the crashed compo-
nent state to its last checkpoint. The components making up
OSIRIS are event-driven, allowing them to create a checkpoint
every time they receive a new request message (i.e., event).
Because checkpoints need to be created at high frequencies,
we do not copy the entire state but rather opt for incremental
checkpoints by maintaining an undo log [22]. The undo log
stores all the memory writes since the last checkpoint and
allows them to be rolled back at recovery time. We use a
compiler pass to insert necessary instrumentation to maintain
the log. Since our fault model focuses on fail-stop faults, we
can assume the last checkpoint to be a valid component state.
This means that only one checkpoint needs to be maintained
at a time. After rolling back to the last checkpoint, we send an
error reply to the component that sent the failure-triggering re-
quest, instead of simply replaying its execution. This is to also
recover from persistent faults, as dictated by our fault model.
B. Ensuring safe recovery
To ensure that we attempt recovery only when the system
is known to reach a consistent state, we ﬁrst consider what it
means for our system to be in an inconsistent state. The system
is in an inconsistent state whenever at least one component
believes another component to be in a state it is not in. Our
system is compartmentalized into components that can only
interact through SEEPs. This means state changes after the
most recent outgoing SEEP message are not known to the
rest of the system. Therefore there can be no inconsistencies
if we roll back a component to a checkpoint that was made
after the last outgoing SEEP message in that component. While
this approach is more conservative than strictly necessary, it
prevents the need to do dependency checking and recursive
rollbacks. When a component crashes, we only need to deter-
mine whether the last checkpoint was made after the last SEEP.
A SEEP can be state-modifying or non-state-modifying,
indicating whether a request passing through it affects the state
at the receiving side or not, respectively. Because the receiving
end of a non-state-modifying SEEP does not update its own
state, it does not become aware of changes in the sender’s state.
As such, we can safely ignore these for recovery purposes
and only consider the last encountered state-modifying SEEP.
Building on these intuitions, OSIRIS only performs recovery
4
operations when the crashed component has sent no state-
modifying outgoing message since the last checkpoint, mask-
ing the failure to any other component in the system and result-
ing in a globally consistent state after recovery by construction.
C. Recovery example
Considering our OSIRIS design, let us exemplify what
happens when a fault occurs. Consider, for example, a shell
issues a fork() system call to create a new process to run
a user command. The shell process sends a message to the
responsible OS component and waits for a reply. When the
component responsible for creating new processes receives the
message, OSIRIS creates a checkpoint which marks the start
of a new recovery window for the component. Within the re-
covery window, our instrumentation maintains the checkpoint-
associated undo log for recovery purposes. The recovery win-
dow remains open as long as we can conservatively prove that
the component has not altered states of any other components.
Now let us consider what happens in case of a crash. For
instance, assume that, while handling the fork() request and
before communicating with other components, the code deref-
erences a NULL pointer that causes the Process Manager (PM)
to crash. In response to the crash, the OS kernel notiﬁes the Re-
covery Server, a key OS component in OSIRIS. The Recovery
Server has a spare fresh copy of all recoverable components. It
transfers state from the crashed PM to the fresh copy and then
rolls back the operations listed in the undo log to restore the
state that existed when the fork() call was received. It then
replaces the crashed component with the recovered PM compo-
nent. The Recovery Server also sends an E_CRASH error code
to the requester—in this case, the shell. The shell can handle
it just like other unexpected failures, such as resource limits
preventing fork(). Most well-written programs routinely
deal with such error codes and take the most appropriate
action. In this case, the shell would simply abort the execution
of the command and inform the user that something went
wrong. At this point, the failure has been cleanly handled and
the system is once again in a stable and consistent state.
So far, our example illustrates an instance of successful
recovery. However, it is not always possible to perform a
successful recovery. Our system closes the recovery window
whenever the component sends a message through a state-
modifying SEEP. If a crash occurs after the recovery window
has closed, the system knows that recovery may not yield
a consistent state and performs a controlled shutdown to
prevent a potentially inconsistent state (with unpredictable
consequences). Hence, as an optimization, after the recovery
window has closed,
the system stops updating the undo
log, reducing the performance overhead. As a result, our
approach achieves good performance and never performs
unsafe recovery by design under the fail-stop fault model.
IV. DESIGN
In this section we discuss the design of OSIRIS. We
ﬁrst describe the underlying programming model. Next, we
explain how our system decides whether it can safely perform
a component-local recovery and how it performs recovery
operations. Then, we describe our incremental checkpointing
optimization that reduces runtime overhead whenever safe
Fig. 1. OSIRIS’ event-driven programming model.
recovery is known to be impossible. Finally, we explain how
our approach deals with multithreading.
A. Event-driven programming model
the top-of-the-loop,
Our OSIRIS design applies to compartmentalized operating
systems which apply component isolation and use a message-
passing interface for communication. In OSIRIS, core OS
components (or servers) follow an event-driven programming
model as shown in Figure 1. An event-driven model signiﬁ-
cantly simpliﬁes state management for recovery purposes [31].
After initialization, the OS components run indeﬁnitely in
a request processing loop. They block to receive incoming
messages at
including events such as
system calls initiated by user programs as well as requests
from other OS components and responses to previously issued
asynchronous requests. The corresponding request handler then
processes the received message based on its type. Finally, a
component typically sends a reply except in cases where the
sender must block (for example when no data is available to
satisfy a read() call) or if the incoming message itself was an
asynchronous reply. In those cases, the server postpones the re-
ply until it receives the response it needs to complete its work.
This model imposes clear boundaries on state changes and
binds them to speciﬁc request messages, which, as mentioned
earlier, simpliﬁes state management and therefore recovery.
B. When to perform recovery
Inter-component communication may lead to state depen-
dencies between the communicating servers. Such dependen-
cies threaten global consistency after crash recovery. Our
recovery methodology is centered around identifying a recov-
ery window within the request processing loop in each OS
component. A recovery window starts at the top-of-the-loop
and spans those instructions that we may roll back to the top-
of-the-loop (checkpoint) without affecting the consistency of
the overall system. In other words, the recovery window closes
at the point where an irrecoverable operation is encountered.
Conservatively speaking, any operation that affects the global
state of the system is a potential threat to consistent recovery.
In OSIRIS, we make communication interfaces among
the system components side effect aware. As mentioned
in Section III, all messages are exchanged through Side
Effect Engraved Passages (SEEPs). SEEP is aware of the
Fig. 2. Example showing the recovery window; undo log instrumentation is
enabled where the line is thick.
consequences of the messages it carries. This allows us to
classify every message in the system based on whether or not it
affects the state of the recipient and whether or not it is possible
to send an error reply back after crash recovery. We use this
information to deﬁne system-wide recovery policies. Recovery
policies control which classes of SEEPs are allowed within
a recovery window, hence controlling the span of recovery
windows in each OS component. The ﬁrst SEEP communica-
tion that the policy does not allow closes the active recovery
window in an OS component. We deﬁne two simple recovery
policies to demonstrate our design: (i) pessimistic recovery,
where sending out any message closes the recovery window,
and (ii) enhanced recovery (default), where we use SEEP to
identify which interactions actually create dependencies.
To implement SEEP (and enhanced recovery), we rely on
a compiler pass to instrument all the outbound communication
call sites with metadata indicating their potential side effects.
Figure 2 shows an example of how a recovery window
would appear when messages are sent while processing a
request. We create a checkpoint at the receive() call at
the top of the request processing loop, marking the start of the
recovery window. The SEEP the ﬁrst message is sent through
is non-state-modifying, so it does not affect
the recovery
window in our default conﬁguration. The second SEEP is
state-modifying and causes the recovery window to be closed.
The system can discard the checkpoint (i.e., undo log data)
at that point as the system will not attempt to restore it.
C. How to perform recovery
At its core, OSIRIS’ recovery mechanism relies on memory
checkpointing [22], [32]. It creates a new in-memory check-
point at the start of every recovery window. From that point
onward, compiler instrumentation keeps track of an undo log.
Speciﬁcally, an LLVM [33] instrumentation pass places hooks
in the system components to keep track of every memory
write operation. It instruments every store instruction of
the LLVM intermediate representation with a call
to the
checkpoint-recovery library which adds an entry to the undo
log. Each entry consists of the address and the original value
that was overwritten. At any point within the recovery window
it is possible to restore the checkpoint by rolling back the
entries in the undo log. We base OSIRIS’ approach on our prior
work on lightweight memory checkpointing [22], but with an
5
/* initialization */while (true) {    receive(&endpoint, &request);    switch (request.type) {        case REQ_TYPE_x:            reply = req_handler_x(request);            break;        case REQ_TYPE_y:            reply = req_handler_y(request);            break;        /* ... */    }    if (reply) send(endpoint, reply);}request processing loop top of the loopinitializationtop of the loopstateless SEEPstateful SEEPreplycheckpointrecoverableimportant optimization (see Section IV-D). Furthermore, we
use software fault isolation (SFI) to protect the checkpointing
library from any inadvertent corruption.
The beneﬁt of using write logging rather than storing a
copy of the original state is that it is efﬁcient for creating
checkpoints at a high frequency. This makes it a good ﬁt for
our system, as operating system components typically deal
with many incoming messages while doing a relatively small
amount of work for each message. The latter property bounds
the number of per-message memory writes to a very limited
number in practice, favoring a simple undo log organization
over more sophisticated memory shadowing schemes [22].
One OS component, known as the Recovery Server (RS), is
responsible for detecting and reacting to crashes in the system.
RS receives a notiﬁcation whenever a server crashes and
periodically sends heartbeat messages to detect hung servers.
In addition, it initiates the recovery procedure whenever it
detects a crash or hang. Recovery in our design is structured
in three phases: restart, rollback, and reconciliation.
In the restart phase, RS transparently replaces a deceased
component with a freshly forked clone component. However, if
the target component is one of the core system servers such as
PM, VM, or even RS itself, fork() and other fundamental
system functions would not work properly. Hence, for core
system servers, RS replaces the deceased component with a
clone prepared ahead of time. RS starts the clone and, in its
initialization code, the clone determines that it is in recovery
mode. After receiving a special capability from RS, kernel
support allows the clone to copy over the data sections from
the crashed component’s memory image. The clone has now
the same state as the original server when it crashed.
In the rollback phase, the initialization code in the clone
rolls back the local state using the undo log just transferred
from the crashed component. This restores the last checkpoint
taken at the start of the current recovery window, i.e., the top-
of-the-loop. At this point, the state is identical to the state at
the time when the crash-triggering request was received. Under
a fail-stop fault model, this is also the last known good state.
Finally, we enter the reconciliation phase. Now that the
local component state has been restored, we still need to ensure
that the global state is consistent. The recovery action to take
at this point is speciﬁed by the recovery model and depends
on the state of the recovery window at the time of the crash.
Although this model is extensible, we use two strategies in
our OSIRIS prototype. If the recovery window was open at the
time of the crash and the last received message was a request to
which we can reply, we perform error virtualization by sending
an error reply to the requesting process. We then discard the
original message. Doing so makes the state globally consistent
and allows OSIRIS to seamlessly handle persistent faults. In
the other cases, we know that we cannot reach a consistent
state and we then perform a controlled shutdown of the system.
Although this means giving up on a possibly successful recov-
ery, it allows us to guarantee a consistent state, eliminating the
unpredictable consequences of an unstable system.
D. Optimization
runtime overhead. However, as it is impossible to recover when
execution goes past the recovery window, the checkpoint is
not useful anymore. We reduce overhead by updating the undo
log only when the recovery window is open. For this purpose,
we rely on another LLVM-based compiler pass to create two
clones of every function in the component and replace the
original function with one that conditionally selects one of the
two clones based on whether the recovery window is open or
not. To ensure that recovery window status is checked every
time it enters the top-of-the-loop, we perform loop extraction
and wrap it in a new function prior to function cloning. Then,
our checkpointing instrumentation pass adds write logging
only to the cloned version that is used when the recovery
window is open. Finally, we force LLVM to inline the cloned
functions to avoid introducing function call overhead.
the
This optimization reduces
runtime performance
overhead by 11%. Furthermore, it provides an explicit trade-
off between recovery surface and performance overhead. De-
creasing the recovery surface (e.g., by switching to pessimistic
recovery) would lead to less frequent execution of instrumented
function clones and thus better performance (and vice versa).
E. Multithreading
It
is possible to have multiple message loops
run
simultaneously in the same component using multithreading,
which can possibly increase efﬁciency and/or lower complexity
in certain components. Our design supports multithreading
under the condition that the state is managed by the server
itself by using a cooperative thread library. The recovery