a 27% annual growth rate in public DNS adoption, reaching
8.6% of sampled users in December 2011. Google’s DNS
service showed the most signiﬁcant growth—a 74% annual
increase—which resulted in it becoming the most-used
public DNS service (overtaking OpenDNS) in November
2010. For 70% of these public DNS users, the public service
is conﬁgured as their primary (22%) or only option (48%).
While subject to a potential “geek bias”, our results are
indicative of an overall increasing trend of public DNS use.5
Indeed, our observed growth trends are consistent with those
reported by OpenDNS showing a 2× increase in users of
their service over 2010-2012 [20, 21].
2.1 Industry response
The increased use of remote DNS services, with its
potential
impact on web performance, has motivated a
recent response from industry. Several companies, including
Google, OpenDNS and EdgeCast, have proposed the “edns-
client-subnet” DNS extension (“ECS”) to the IETF [3] as part
of the “Global Internet Speedup” initiative [28].
The proposed extension provides a mechanism for recur-
sive DNS resolvers to pass client location information to
CDN authoritative DNS servers. This enables CDNs to
factor in the client’s actual location in redirection decisions.
The extension is speciﬁed as a EDNS0 [30] option that
is appended to the DNS query and contains the client’s
network preﬁx. The length of the preﬁx is a parameter
determined by the recursive resolver.
Assuming that both the DNS service and the CDN support
the extension, the approach enables CDNs to transparently
improve the quality of redirections.
It is straightforward
to determine whether a DNS service or CDN supports the
5http://voices.washingtonpost.com/fasterforward/
2010/11/comcast\_internet\_or\_any\_other.html
10501005001000Nmostpopularsites0102030405060708090100%usingCDNsSitesPageviewsMay2010Jul2010Sep2010Nov2010Jan2011Mar2011May2011Jul2011Sep2011Nov20110123456789%ofusersAnyOpenDNSGoogleLevel3525extension by sending a request to recursive or authoritative
DNS servers. If the ECS option is present in the response,
the extension is supported.
3. METHODOLOGY
In this section, we describe our methodology to experi-
mentally evaluate the interactions between remote DNS and
CDNs.
We base our analyses on data contributed by 10,923
end hosts distributed across 99 countries and 752 ASes,
providing a diverse set of vantage points.
In terms of
geography, 59% of the hosts are spread across 35 European
countries, 21.9% are from the United States, 5.5% are
located in Asia, and 3.9% are in Oceania. Our dataset was
collected over a 127 day period between September 12th,
2011 and January 16th, 2012.
Each end host runs an instance of an ISP characterization
plugin for the Vuze BitTorrent client [32]; users download
our software and allow us to collect measurement results.
Each host reports:
local conﬁguration information, the
results of DNS resolutions, HTTP request timing statistics,
and results of traceroutes and pings to CDN and DNS
servers.
For our study, we chose diﬀerent public DNS services and
CDNs based on popularity and deployment architectures.
The set of public DNS services we measured includes two of
the most popular: Google Public DNS and OpenDNS. Our
set of CDNs, which includes Akamai, EdgeCast, Google and
Limelight, covers a variety of deployment models, ranging
from servers sparsely deployed at points of presence to
servers located inside the networks of end users. For each
CDN, we manually selected a small (<10 KB) web object
hosted by that CDN. This enabled us to evaluate the end-
to-end performance using 2 steps: (1) clients query DNS
for the object’s hostname to obtain a CDN redirection, and
then (2) request the object via HTTP from the CDN’s edge
server.
We deﬁne each /24 IP preﬁx from which we have mea-
surements to be a vantage point “location” and aggregate
measurements taken by nodes in the preﬁx. In the remainder
of the paper, we analyze the distributions across these
“locations”. To ensure the signiﬁcance of our results, we
require at least 3 measurements from each location. For
each combination of vantage point location, DNS service and
CDN, we select the best case results (i.e. minimum latencies)
for comparison.
Our experiments are designed to minimize their impact
on the monitored services. We use caching at each node
to avoid multiple probes to the same server, and randomly
schedule experiments to reduce the likelihood of concurrent
measurements launched from diﬀerent hosts.
3.1 Obtaining CDN redirections
We use iterative DNS resolution to obtain CDN redirec-
tions that are not aﬀected by the location of the ISP or
public DNS servers.
End hosts typically resolve DNS names by querying a
recursive DNS resolver operated by an ISP or public service.
The recursive resolver makes several queries on behalf of the
end host, caches the responses, and returns the answer to
the end host.
Using iterative resolution, an end host can act as its
own recursive resolver. Since the host directly contacts
the authoritative DNS servers, CDNs can base their replica
server selections on the client’s actual network location.
To obtain CDN redirections that are eﬀectively informed
by client location, we follow two approaches to cover the
cases where the edns-client-subnet DNS extension is and is
not supported.
DNS extension supported. To query combinations of
DNS services and CDNs where the extension is supported,
we implement a stub resolver that adds the ECS option with
a speciﬁed IP preﬁx to a query and forwards the request
to a recursive resolver. This method of querying with the
ECS option is possible due to a provision in the speciﬁcation
to support hierarchies of DNS resolvers; if the ECS option
is already present in a query, it should not be modiﬁed by
a recursive resolver. Since our stub resolver generates the
option, we can evaluate the impact of providing diﬀerent
amounts of client information (i.e. preﬁx length) on CDN
redirections.
DNS extension not supported. To evaluate the DNS
extension’s approach for DNS services or CDNs that do not
support the extension, we emulate its characteristics: the
latency to receive the answer, and the CDN’s answer based
on the client’s location. We determine the latency to receive
an answer by simply querying the DNS service for a name
that is cached on the server. To obtain a CDN answer based
on the client’s location, we use iterative resolution to directly
query the CDN’s authoritative DNS server. As we show in
§4.2, redirections via iterative lookup generally provide best-
case HTTP performance. As a result, our emulated ECS
query gives redirections equivalent to using the extension
and providing the client’s full IP address.
3.2 Measuring DNS services
Public DNS services are typically oﬀered on BGP anycast-
enabled IP addresses, allowing all users to conﬁgure the same
server IP address. Probing public DNS services can present a
challenge for network-level measurements, as some anycast-
enabled addresses do not respond to ICMP pings.
To determine the recursive DNS resolver’s globally routable
unicast IP address so that we can probe it directly, we use
a technique similar to that used by Huang et al. [9]. We
operate an authoritative DNS server that answers queries
with the IP address of the recursive DNS resolver sending
the query. This approach works because resolvers that
answer queries on an anycast IP interface use a separate
unicast interface to communicate with other authoritative
DNS servers; our service returns this unicast IP address
for the recursive resolver. Clients send a DNS query to
the public DNS service’s anycast IP address and receive its
globally routable unicast IP address in the DNS response;
this gives the client a mapping between the public DNS
service and the unicast IP address for the recursive resolver.
Clients report these mappings, which enables us to identify
public resolvers in our dataset.
Filtering conﬁgured public DNS services.
To
obtain ISPs’ DNS servers in a scalable manner, we start
out with users’ DNS conﬁgurations and exclude known
public DNS services. Since users may have public DNS
services conﬁgured on either their computer or a DNS-
proxying middlebox, we must ﬁlter out both cases. First,
we conservatively ﬁlter well-known public DNS anycast IP
addresses (e.g. 8.8.8.8 for Google DNS) conﬁgured on the
user’s computer. Then, using the result of our previously
526Figure 4: Timeline of DNS and CDN interactions,
and the components we consider when measuring
latency in this work.
described approach to determine a DNS server’s unicast IP
address, we can see through any proxies (e.g. middlebox)
and determine the underlying DNS server. We exclude data
when the underlying DNS server is in a /24 preﬁx where
we have previously located public DNS servers via directly
probing the public DNS services’ well-known addresses.
We use ping latency to measure network distance. To
reduce the impact of transient spikes in latency, we probe
each DNS service’s anycast and unicast addresses with three
ICMP pings and select the minimum latency.
3.3 Measuring CDNs
We measure end-to-end latency for each pair of DNS and
CDN services using the combined latency of DNS lookup
and HTTP request (Fig. 4).
• DNS lookup: Time to obtain a CDN redirection, from
querying a recursive DNS resolver until receiving an
answer
• HTTP request: Time from initiating a connection to
a replica server, to receiving the ﬁrst byte of an object
from an CDN edge server via HTTP
For each CDN studied, we select one of their customer’s
small web objects to request. To factor out any diﬀerences
due to varying object size across CDNs, we compute access
latency based on the time needed to receive the ﬁrst byte of
the object. We conduct each DNS and HTTP GET request
twice in close succession so that the requested object will be
served from the server’s cache on the second request; we use
the smaller latency.
3.4 Baseline for performance comparison
One issue we faced in our analyses was determining how
to compare the performance of diﬀerent approaches for
obtaining CDN redirections. We initially considered using
the performance when using ISP DNS as our baseline.
However, several issues with ISP DNS mean that it is not
always an appropriate choice as a baseline. For instance, the
use of remote DNS architectures may break the assumption
of proximity between client and resolver. Also, some
ISP DNS services exhibit high latencies and poor cache
performance due to load balancing [1]. Another alternative
we considered is using the performance of redirections seen
via iterative DNS resolution; however, factors such as CDN
load balancing between replica server clusters may aﬀect the
redirections resulting in suboptimal performance.
We determine baseline performance by selecting each
location’s best performance from any CDN redirection mech-
anism,
including using ISP DNS, public DNS services
and iterative resolution. We compare the performance
of CDN redirection approaches’ aggregate distributions of
performance relative to this baseline.
To compare the end-to-end performance of diﬀerent ap-
proaches to obtaining CDN redirections, we compute for
each location the baseline DNS latency to obtain a CDN
redirection, and the baseline HTTP latency to download
an object from the CDN. We deﬁne our baseline DNS
performance as the minimum latency to obtain a DNS
response from any ISP or public DNS service.
This
represents the latency to obtain a cached answer from the
nearest recursive DNS resolver. For our baseline HTTP
performance, we use the minimum observed HTTP latency
for that CDN in that location. This value represents
the best-possible CDN performance we observed from the
location—typically, the latency to obtain a cached object
from the nearest CDN server. Our end-to-end performance
baseline is deﬁned as the sum of the DNS and HTTP baseline
latencies. This represents an idealized best-case scenario in
which the nearest DNS returns a cached CDN redirection,
the redirection yields the nearest CDN replica server, and
the requested object is cached on the replica server.
4. DNS–CDN INTERACTION
To examine the impact that diﬀerent DNS services have
on several aspects of CDN performance, we ﬁrst look
at diﬀerences in the replica servers seen when obtaining
CDN redirections using ISP DNS, public DNS services and
iterative resolution. Next, we compare the impact of these
approaches on the resulting HTTP performance. Finally,
we evaluate the end-to-end performance beneﬁt of using the
edns-client-subnet extension and the degree of its adoption.
4.1 Redirection similarity
When a client requests a redirection, a CDN may consider
factors beyond proximity, including server or network load.
As a result of these transient eﬀects, a client may see not
one but rather a collection of replica servers. Over time, this
collection reveals patterns in which some replica servers are
seen more frequently than others. We aggregate redirections
based on replica servers’ /24 IP preﬁx since these servers
are often deployed in clusters. We build a “ratio map” to
represent these aggregated redirections, which maps each
replica server cluster to the fraction of redirections specifying
that cluster [2, 27].
We analyze these ratio maps of CDN redirections obtained
via diﬀerent DNS services, estimating the degree of simi-
larity between them. Previous work has shown that this
similarity can be used to estimate the proximity between
nodes [2, 27].
We use cosine similarity to quantify the similarity between
ratio maps on the assumption that clients with equivalent
sets of redirections, i.e. client sees the same replica servers,
should yield comparable HTTP performance. Given two