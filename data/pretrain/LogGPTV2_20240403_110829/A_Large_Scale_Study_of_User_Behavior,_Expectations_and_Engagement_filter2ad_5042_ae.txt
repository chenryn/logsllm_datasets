most interestingly, the distribution of deny rates for partici-
pants with relatively high overall privacy sensitivity (> 1.0) is
not uniform—a large proportion, 29% (296/1027), have deny
rates lower than the population average of 16.7% and show
up as a concentration of users near the bottom middle right.
This discrepancy between the high privacy scores (atti-
tudes) and the low deny rates (behavior) might hint at the
well known “privacy paradox” effect [16, 43]. However, the
permission deny/accept decisions are very contextual [28] and
it is impossible to make an assessment of the privacy paradox
effect without knowledge of the complete context that led to
a permission deny/accept decision.
Users with high privacy scores may still allow permissions
if they select their apps carefully and have a good understand-
ing of permissions and their purpose. We hypothesize that
among participants with high privacy scores, there may be
users that are more engaged, in that they are careful in their
app selection and understand context better. These engaged
users might be making context speciﬁc permission choices.
While another study would be needed to evaluate this hypoth-
esis, we check if our data can offer any preliminary insights.
As described in Section 5.3.2, the participants’ expecta-
tion serves as a proxy for the context of a permission request
(where the context includes a user component in addition to
the information provided by the app). We evaluate if the in-
stall time and run time expectation distributions vary between
participants with high (> 1.0) and low privacy scores by per-
forming two separate K-S tests, one for each expectation. In
both the tests, we consider the null hypothesis to be that the
distributions are same across the two privacy score groups.
For the install time expectation case, K-S statistic (D) is 0.104
with a p-value of 0.02. For the runtime expection, D value
is 0.12 with a p-value of 0.014. Based on these p-values,
runtime_expected
(reference: Yes)
Signiﬁcance codes: p  1.0) generally have
higher percentage of correctly expected permissions at install
time (average is 31.4%, median is 26%) compared to those
with low privacy scores (average is 27.1%, median is 18%).
Also, participants with higher privacy scores on average report
higher percentages (75%) of expected permissions at runtime
compared to those with low privacy scores (69%). In sum-
mary, we ﬁnd that for participants with higher privacy scores
their (install and run-time) expectations are more likely to
match reality, than for participants with lower privacy scores.
These ﬁndings partially support our hypothesis that users who
have both high privacy scores and low deny rates may be more
engaged as they appear to understand context better.
6 Limitations
Due to the nature of our participant recruitment, which re-
lies on online advertising, our study is biased towards users
who interact with online ads. Naturally, all of our participants
were also willing to install an application that collects data
about their smartphone usage. This introduces unavoidable
selection bias that is inherent to our methodology, as we are
unable to collect data from potential participants who do not
interact with online ads or who were unwilling to install Pri-
vaDroid. As mentioned in Section 3, we also ﬁnd that females
were under-represented with our methodology. To get a sense
of demographic sample bias, we compared the distribution
of ages and educational attainments of our US participants
with US Census Bureau statistics from 20193. We found that
younger people (77% of study participants are under 40 vs
40% for all US residents) and those with lower educational
attainment (78% of study participants have highschool or less
vs 54% for all US residents) are over-represented in our group.
We speculate that this bias may be due to the higher rate of
smartphone use among the younger population, as well as the
low monetary incentive ($10 USD) being more attractive to
participants with lower educational attainment.
PrivaDroid cannot collect data on events that occurred be-
fore it was installed, thus we do not see any permission deci-
sions participants made with their apps before the start of our
study. It may thus under-count events caused by the default
apps that come with a phone, or popular applications that are
likely to have been already installed on a participant’s phone.
Both participant bias and blindness to pre-install events are un-
avoidable side-effects of our recruitment and data collection
methodologies. In addition, 42% of the users participating in
our study did so after March 15, 2020, when the social and
3Statistics
from https://census.gov/data/tables/2019/demo/
age-and-sex/2019-age-sex-composition.html and https://www.
census.gov/data/tables/2019/demo/educational-attainment/
cps-detailed-tables.html
economic measures caused by the Covid-19 pandemic came
into force in the majority of the countries in our study, and
we are unable to conclusively ascertain the effect of those
measures on this group of participants.
From our experience with PrivaDroid, we believe a mobile
application-based data collection platform coupled with ad-
vertising is a viable method for conducting global user-studies.
However, one challenge we think could be better addressed in
future work is techniques to more holistically collect and mea-
sure a user’s context when interacting with apps. PrivaDroid
focused mainly on the text in pop-up dialog boxes, but misses
other important factors, such as images and general text in
UI screens that are not in dialog boxes. In addition, while
36K permission request events may seem like a lot of data,
it is a tiny number compared to the large variety of smart-
phone apps available. As a result, we have very little data
on any speciﬁc app, making contextual analysis of behavior
across apps impossible. For example, the largest number of
permission events with an explanation for a single app in our
dataset is only 54, and the number falls off fairly steeply. To
better understand contextual behavior, either more data needs
to be collected or the study has to be re-designed to focus on
participants who use a speciﬁc subset of apps.
7 Conclusions
We have found that a few trends reported in [4] remain the
same three years later: the aggregate denial rate still hovers
around 16-17%, Microphone is still the most often denied
permission, and we continue to see variation in deny rates
across the permission types. At the same time, there were
some notable changes for speciﬁc permission types. For ex-
ample, the deny rate for the Calendar permission has grown
signiﬁcantly from 10% [4] to 21.7% today and the deny rates
for the Phone permission have dropped from 19% to 12.6%.
Our demographic analysis reveals interesting trends across
countries. We found two distinct cliques of countries in our
data, where countries within a clique have statistically similar
deny rates. Some countries do not ﬁt cleanly into either clique.
We also observed different permission sensitivities across
countries. Previous studies [3, 7, 27] show that nationality
inﬂuences users’ willingness to share their personal data. Our
regression models corroborate this speciﬁcally for Android
apps and for user behavior on their personal devices. Our
study revealed that users are less likely to deny permission
requests when explanations are present. We demonstrated this
trend with regression models that show this holds, even when
accounting for all other factors inﬂuencing decisions (such as
age, app, country, attitude, etc). The average deny rate was re-
duced by half when there is an explanation (15% vs 7%). Our
study also shows that expectations have a signiﬁcant inﬂuence
on permission decision making. We found that participants
deny permissions more often when an app asks for a per-
mission they did not expect. We again demonstrated this via
USENIX Association
30th USENIX Security Symposium    815
regression modeling. This bias exists for both types of expec-
tations (install-time and run-time) and across all permission
types, but is signiﬁcantly stronger for runtime expectations,
where the deny rate for unexpected permissions is double that
of expected permissions. This corroborates prior work [48]
but on a larger scale and across multiple countries.
One of the main forward-looking take-aways from our
study is that users are more likely to grant permission requests
that are expected. In a sense, this tells us that the permis-
sion system is working—when a permission request “makes
sense”, users are more likely to grant the permission. This fur-
ther suggests that the gap between smartphone user’s desires
to constrain applications and the reality is more due to short-
comings in their understanding of the interplay between apps
and permissions, and the context in which permission requests
are made, than the permission mechanism itself (with the ex-
ception of temporary permissions, which our study showed
have some beneﬁt to users). As a result, transparency features,
such as Apple’s “Privacy Nutrition Labels” and Google Play’s
Safety directive 4, may serve to complement the current smart-
phone permissions system design. However, our results also
show that the effect of unexpected permissions at run-time is
more pronounced than at install-time, suggesting that trans-
parency features that only target install-time permissions may
not be as effective as those that are more dynamic and linked
to speciﬁc permission types. Future research on the quality of
explanations and exactly how and when to use them would be
very beneﬁcial to the proper adaption of explanation labels.
Acknowledgements
We acknowledge the feedback of the anonymous reviewers
and our shepherd, Sven Bugiel, whose insights and sugges-
tions improved the paper greatly. We would also like to thank
Nicolas Papernot, Adelin Travers, Beom Heyn Kim, Sukwon
Oh, and Mariana D’Angelo for helping us check our trans-
lations. We thank Kelly Hayward and Dubravka Burin, who
helped us navigate the numerous administrative and reporting
complexities of making hundreds of international payments
from a public institution. We also thank Manya Sleeper and
Micha Segeritz for helping us with the regression analysis.
Finally, ﬁnancial support for this research was provided in
part by the University of Toronto’s Connaught Fund, a Canada
Research Chair, an NSERC USRA grant and by a Security
and Privacy Research Award from Google.
References
[1] Y. Agarwal and M. Hall. ProtectMyPrivacy: detect-
ing and mitigating privacy leaks on iOS devices using
crowdsourcing. In Proceedings of MobiSys, 2013.
4https://android-developers.googleblog.com/2021/05/
new-safety-section-in-google-play-will.html
[2] Hazim Almuhimedi, Florian Schaub, Norman Sadeh,
Idris Adjerid, Alessandro Acquisti, Joshua Gluck, Lor-
rie Faith Cranor, and Yuvraj Agarwal. Your location has
been shared 5,398 times!: A ﬁeld study on mobile app
privacy nudging. In Proceedings of CHI, 2015.
[3] Steven Bellman, Eric J. Johnson, Stephen J. Kobrin, and
Gerald L. Lohse. International differences in informa-
tion privacy concerns: A global survey of consumers.
The Information Society, 20, 2004.
[4] Bram Bonné, Sai Teja Peddinti, Igor Bilogrevic, and
Nina Taft. Exploring decision making with Android’s
runtime permission dialogs using in-context surveys. In
Thirteenth Symposium on Usable Privacy and Security
(SOUPS 2017), pages 195–210, July 2017.
[5] Surveillance Studies Centre. The Globalization of Per-
sonal Data (GPD) Project International Survey on Pri-
vacy and Surveillance, 2013.
[6] Saksham Chitkara, Nishad Gothoskar, Suhas Harish, Ja-
son I. Hong, and Yuvraj Agarwal. Does this app really
need my location?: Context-aware privacy management
for smartphones. Proceedings of the ACM on Interactive,
Mobile, Wearable and Ubiquitous Technologies, 1(3),
September 2017.
[7] Hichang Cho, Milagros Rivera-Sánchez, and Sun Sun
Lim. A multinational study on online privacy: global
concerns and local responses. New Media & Society, 11,
2009.
[8] Rowena Cullen. Citizens’ concerns about the privacy
of personal information held by government: A compar-
ative study, Japan and New Zealand. In Proceedings
of the 41st Annual Hawaii International Conference on
System Sciences (HICSS 2008), 2008.
[9] Adrienne Porter Felt, Erika Chin, Steve Hanna, Dawn
Song, and David Wagner. Android permissions demys-
tiﬁed. In Proceedings of the 18th ACM Conference on
Computer and Communications Security, 2011.
[10] Adrienne Porter Felt, Serge Egelman, Matthew Finifter,
Devdatta Akhawe, and David Wagner. How to ask for
permission. In Proceedings of 7th Usenix conference on
Hot Topics in Security (HotSec), 2012.
[11] Adrienne Porter Felt, Serge Egelman, and David Wag-
ner. I’ve got 99 problems, but vibration ain’t one: A
survey of smartphone users’ concerns. In Proceedings
of the Second ACM Workshop on Security and Privacy
in Smartphones and Mobile Devices, 2012.
[12] Marian Harbach, Markus Hettig, Susanne Weber, and
Matthew Smith. Using personal examples to improve
risk communication for security & privacy decisions. In
816    30th USENIX Security Symposium
USENIX Association
The 32nd Annual ACM Conference on Human Factors
in Computing Systems, 2014.
The Construct, the Scale and a Causal Model. Informa-
tion Systems Research, December 2004.
[13] Jaeyeon Jung, Seungyeop Han, and David Wetherall.
Short paper: Enhancing mobile application permissions
with runtime feedback and constraints. In The Second
ACM Workshop on Security and Privacy in Smartphones
and Mobile Devices, 2012.
[14] Patrick Kelley. Privacy, measurably, isn’t dead.
Usenix Enigma, February 2021.
In
[15] Patrick Gage Kelley, Lorrie Faith Cranor, and Norman
Sadeh. Privacy as part of the app decision-making pro-
cess. In The SIGCHI Conference on Human Factors in
Computing Systems, 2013.
[16] Spyros Kokolakis. Privacy attitudes and privacy be-
haviour: A review of current research on the privacy
paradox phenomenon. Computers & security, 64:122–
134, 2017.
[17] Ponnurangam Kumaraguru and Lorrie Cranor. Privacy
in India: Attitudes and awareness. In In The 2005 Work-
shop on Privacy Enhancing Technologies, 2005.
[18] Ponnurangam Kumaraguru and Niharika Sachdeva. Pri-
vacy in India: Attitudes and Awareness v2.0. Techni-
cal report, Precog-TR-12-001, Precog@IIIT-Delhi, 2012.
http://precog.iiitd.edu.in/research/privacyindia/.
[19] Jialiu Lin, Shahriyar Amini, Jason I. Hong, Norman
Sadeh, Janne Lindqvist, and Joy Zhang. Expectation
and purpose: Understanding users’ mental models of
mobile app privacy through crowdsourcing. In The 2012
ACM Conference on Ubiquitous Computing, 2012.
[20] Bin Liu, Mads Schaarup Andersen, Florian Schaub,
Hazim Almuhimedi, Shikun (Aerin) Zhang, Norman
Sadeh, Yuvraj Agarwal, and Alessandro Acquisti. Fol-
low my recommendations: A personalized privacy assis-
tant for mobile app permissions. In The 12th Symposium
on Usable Privacy and Security(SOUPS), 2016.
[21] X. Liu, Y. Leng, W. Yang, W. Wang, C. Zhai, and
T. Xie. A large-scale empirical study on Android
runtime-permission rationale messages. In 2018 IEEE
Symposium on Visual Languages and Human-Centric
Computing (VL/HCC), pages 137–146, 2018.
[22] Xueqing Liu, Yue Leng, Wei Yang, Chengxiang Zhai,
and Tao Xie. Mining Android app descriptions for per-
mission requirements recommendation. In IEEE Inter-
national Requirements Engineering Conference, pages
147–158, 08 2018.
[23] Naresh K. Malhotra, Sung S. Kim, and James Agarwal.
Internet Users’ Information Privacy Concerns (IUIPC):
[24] Nathan Malkin, Julia Bernd, Martiza Johnson, and Serge