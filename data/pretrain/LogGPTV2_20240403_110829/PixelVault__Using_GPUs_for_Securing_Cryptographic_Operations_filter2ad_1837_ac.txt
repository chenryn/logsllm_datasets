in the on-chip registers. This prevents an attacker from accessing
the keys by continuously snooping over the KeyStore array until
the moment it gets decrypted, and avoids leaving any unencrypted
copies of cryptographic keys in global device memory.
We should note that the KeyStore structure is only needed for
services that maintain a large number of private and secret keys. In
other cases, the KeyStore can be disabled, and PixelVault can be
conﬁgured to hold all keys in registers. This has the added ben-
eﬁt of avoiding the extra overhead of fetching and decrypting the
cryptographic keys used from the KeyStore array.
4.5 Key Management
An aspect of our design that needs careful consideration is the
transfer of the KeyStore’s master key to the GPU registers, and the
loading of PixelVault’s native code in the instruction cache of the
GPU. These operations should be performed at an early stage of the
bootstrapping phase, before launching any user process or connect-
ing to the Internet—as also suggested in previous works [40, 41]—
and preferably from a non-volatile storage device, such as an ex-
ternal USB ﬂash drive. This approach exposes any sensitive infor-
mation on the CPU only for a minimal amount of time, which for
servers happens only rarely, whenever the system boots.
To exclude the possibility that the master key is not copied to an
intermediate buffer, before ﬁnally being transferred to the GPU,
we explicitly allocate a page-locked memory region that can be
accessed directly from the GPU. Otherwise, if a regular memory
buffer is used, the driver will have to copy the key to an internal
page-locked memory region, to which we do not have direct access
for erase it afterwards. After the key has been transferred to the
memory space of the GPU, it stored in the GPU registers, and all
instances of the key in the GPU’s and host’s memory are erased.
int GPU_AES_encrypt_cbc(int keyID,
unsigned char *in, unsigned char *out,
size_t nbytes, unsigned char *ivec);
int GPU_AES_decrypt_cbc(int keyID,
unsigned char *in, unsigned char *out,
size_t nbytes, unsigned char *ivec);
int GPU_AES_encrypt_cbc_batch(int* key,
unsigned char *in, unsigned char *out,
size_t *offsets, size_t *nbytes,
unsigned char *ivec, size_t total);
int GPU_AES_decrypt_cbc_batch(int* keyID,
unsigned char *in, unsigned char *out,
size_t *offsets, size_t *nbytes,
unsigned char *ivec, size_t total);
Figure 5: The OpenSSL-compatible API for the 128-bit AES-
CBC cipher. The ﬁrst two functions process a single message
at a time and can be used transparently by legacy applications.
The last two functions process a batch of messages at a time,
and are better suited for throughput-oriented setups.
A user or an application can share certiﬁcates or secret keys with
the service by transferring them through the shared memory space.
Every new cryptographic key is stored sequentially in the KeyStore
array, and is identiﬁed by its index during data encryption and de-
cryption. The GPU uses the index to acquire the speciﬁed key from
the KeyStore array and perform the requested operation. However,
a service cannot access the keys directly, as they are stored en-
crypted. Therefore, even if an adversary manages to inject mali-
cious CPU code in the address space of the service, it is impossible
to acquire the clear-text keys.
5.
IMPLEMENTATION
We have implemented PixelVault on Linux v3.5.0, on top of
the NVIDIA CUDA architecture v4.2 using the NVIDIA driver
v304.54. Our prototype implementation currently supports both
RSA and AES, and provides an OpenSSL-compatible API that en-
ables existing applications or services to easily be ported on top of
PixelVault with minimal modiﬁcations.
5.1 AES
We have ported AES-128 on the GPU, by storing the key and all
intermediate states in GPU registers. AES divides each plaintext
message into 128-bit ﬁxed blocks and encrypts each block into ci-
phertext with a 128-bit key. The encryption algorithm consists of
10 rounds of transformations. Each round uses a different round
key generated from the original key using Rijndael’s key sched-
ule. We have chosen to derive the round key at each round, instead
int GPU_RSA1024_private_decrypt(char *in,
char *out, int rsaKeyID, size_t *offsets,
size_t *nbytes, size_t total);
int GPU_RSA1024_private_decrypt_batch(char *in,
char *out, int *rsaKeyID, size_t *offsets,
size_t *nbytes, size_t total);
Figure 6: The OpenSSL-compatible API for the 1024-bit RSA
cipher. The ﬁrst function process a single message at a time,
and can be used transparently by legacy applications. The sec-
ond function process a batch of messages at a time, and is better
suited for high-performance setups.
of using pre-expanded keys. Although this approach incurs more
computational overhead, it reduces the number of registers needed.
Overall, we need 16 bytes for the key, 16 bytes for the round key,
and 16 bytes for the input block (which is modiﬁed in-place over
the rounds and eventually contains the output block). Part of the
remaining registers are used for local variables. The only data that
is written back to global, off-chip device memory after the input
block has been processed is the output block. This restrictive pol-
icy ensures that no sensitive information about the key is leaked to
global device memory.
Figure 5 shows PixelVault’s API functions for the CBC mode
of AES-128. The CBC-encryption mode has a dependency on the
result of the previous block, hence the encryption of the blocks of
a single message is axiomatically serialized. Nevertheless, each
thread can perform encryption operations using a different AES
key independently, as it contains its own register space. On the
other hand, CBC decryption can be parallelized at the block level,
as the result of the previous block is already known at decryption
time. Other modes of AES (such as ECB, CBC, and CTR) can be
implemented in a similar way without signiﬁcant extra effort.
5.2 RSA
We have implemented the RSA decrypt function for 1024-bit
keys (Figure 6). We focused on the performance of RSA decryp-
tion for two reasons. First, RSA typically requires several decryp-
tion operations per key. Second, decryption is heavily used at the
server side, where runtime performance is more critical.
Our GPU implementation exploits parallelism at the message
level, similarly to previous GPU-based implementations [25, 30,
58]. For modular exponentiation, we use the Montgomery Mul-
tiplication for Multi-Precision Integers (CIOS method) [34], and
we apply the straightforward right-to-left method, similar to [58].
During exponentiation, each thread needs three temporary values of
(n + 2) words each, where n is the size of the key in bits. The three
temporary values are used as input and output in a round-robin fash-
ion. Overall, 3 ∗ (n + 2) words are required, which results in 408
words for 1024-bit keys. Unfortunately, there is not always enough
space to hold all three temporary values in registers (see Table 1).
One solution is to store the three temporary values in shared mem-
ory. Each multiprocessor features up to 48 KB of shared memory,
which, in contrast to the off-chip global device memory, cannot
be accessed by the host. Even if adversaries manage to stop Pix-
elVault’s autonomous kernel and run a malicious one, they would
only retrieve a single static image of the shared memory.
Still, this does not pose a signiﬁcant risk for two reasons: (a) this
requires very precise timing of the attack, and (b) even if the right
timing can be achieved, the obtained fraction of the key is too small
to pose a key leakage risk. The reason is that for any n, only the
least signiﬁcant k bits of the key can be recovered, with a O(2k)
complexity. For example, if we assume that a Meet-in-the-Middle
attack is feasible for up to k = 128 in a reasonable time, then the
least 128 signiﬁcant bits of the key are exposed to the adversary.
This amount of bits is far from being critical for revealing the en-
tire key, given that the critical limit for RSA is at least one fourth of
the key size [13]. To prevent even the above unlikely leak, we have
implemented an optional mode in which the intermediate values are
stored in shared memory in an encrypted form. Only those interme-
diate values that are needed are decrypted—within GPU registers
only— but with an additional cost. The encryption and decryption
of the intermediate states are performed in 16-byte chunks, using
the master key.
From the performance standpoint, previous work has shown that
the GPU is better utilized when several messages are processed at
once [24, 25, 30], which is also true for our implementation. To
achieve optimal performance, our GPU-based OpenSSL version
processes many messages in bulk. This is achieved by buffering
several messages and transferring them to the GPU at once for par-
allel processing. Recall that each thread can perform encryption
and decryption operations using a different key independently, as it
contains its own register space.
6. EVALUATION
6.1 Security Analysis
We now evaluate the security properties of PixelVault by describ-
ing possible attacks, and showing how our proposed design protects
against them. For some of the attacks, we used the Gdev frame-
work [7], as it is open-source and provides more insights about
low-level operational details than the ofﬁcial closed-source CUDA
runtime.
6.1.1 Host Memory Attacks
We have implemented RSA and AES in a way that nothing but
the scrambled output block is ever written into host memory. This
is feasible, as GPUs maintain their own discrete memory spaces
for manipulating input data. When the GPU has performed the
desired cryptographic operation, the resulting output is transferred
back to host memory.
In the meantime, GPU execution contin-
ues completely isolated from the CPU, without being affected by
side effects of the OS or the hardware, such as interrupt handling,
scheduling, swapping, and ACPI suspend modes. As a result, keys
or any intermediate states are never transferred indirectly to host
memory.
6.1.2 Extracting Intermediate States
As we described in Section 4.4, secret and private keys reside
unencrypted only in GPU registers. However, encryption and de-
cryption operations take place in global device memory, which is
accessible from the host via the PCIe interconnect. Therefore, it is
possible for an adversary to perform cryptanalysis from any inter-
mediate states extracted from the global device memory.
To defend against cryptanalysis, we perform both AES and RSA
exclusively in on-chip memory. In particular, after a plaintext block
is read from global device memory, nothing but the scrambled out-
put block is written back. Essentially, no valuable information
about the key or intermediate state is visible in the global device
memory at any time. To ensure that no intermediate state resides
in global memory, we stage data to the on-chip shared memory,
which is not accessible by the host. Even in case an adversary ter-
minates the GPU program and successfully acquires the contents of
the shared memory, only a single intermediate state will be accessi-
ble, and more cannot be obtained for performing successful crypt-
analysis. To acquire further intermediate states, an attacker needs
to restart the autonomous PixelVault GPU kernel; this is not pos-
sible though, as only the administrator can re-execute PixelVault
from a clean state, after transferring the master key and native code
from an external device, as we described in Section 4.5.
6.1.3 CPU Code Injection
In a typical scenario, attackers can exploit software vulnerabili-
ties and manage to inject code of their choice to a running service.
Sensitive data, such as private keys, that are stored in the address
space of the process, can be easily acquired.
In contrast, hiding
sensitive data in the on-chip memory space of the GPU using Pix-
elVault prevents access even to fully privileged processes.
To verify this, we attached cuda-gdb, the CUDA debugger, to
PixelVault using full-administrator privileges for tracing its execu-
tion. The cuda-gdb is very similar to gdb and allows tracing
of both CPU and GPU variables, as well as the execution of arbi-
trary CPU and GPU code. Running PixelVault under a debugger
allows us to transfer data from the off-chip global device mem-
ory. However, we are still not able to extract any key, as they are
kept encrypted. Furthermore, we are not able to access any on-
chip memory (i.e., shared memory and caches) even if PixelVault
is compiled with debug-able device code (using both -g and -G
ﬂags). The reason is that the non-preemptive GPU execution does
not allow adding breakpoints inside a kernel that is already run-
ning; to trace the execution of a kernel, the breakpoints have to be
added before the kernel has been loaded on the GPU for execu-
tion. As we start the GPU kernel from a clean state, it is impossible
for an attacker to trace the autonomous, self-contained GPU code
of PixelVault.
6.1.4 GPU Code Injection
All GPU code is loaded in the global device memory before ex-
ecution. The GPU code base of PixelVault is small, which can
allows it to be formally veriﬁed, to prevent potential exploitation
due to buggy code. However, accessing the code’s memory region
is still feasible, as the global device memory does not provide any
access protection. An attacker could, for example, inject malicious
GPU code by transferring it via PCIe to the appropriate memory
region. The malicious code could contain commands for forcing
the registers’ contents to be written to the global device memory,
where they could then easily be retrieved via the PCIe bus.
We have modiﬁed the Gdev framework to explicitly rewrite the
memory region where native code is stored. Similar attacks can
also be performed using the ofﬁcial CUDA debugger interface [46].
As we described in Section 4.3 though, PixelVault is tamper-resi-
stant against GPU code modiﬁcations, as it forces all code to be
loaded to the instruction cache. Even after erasing all PixelVault’s
native code from the global device memory, the GPU still executes
the original, unmodiﬁed code of PixelVault from the instruction
cache. Therefore, an attacker cannot overwrite PixelVault, because
the instruction cache cannot be ﬂushed without loading a new GPU
kernel.
6.1.5 Simultaneous GPU Kernel Execution
Starting with the Fermi architecture [44] and onwards, differ-
ent (relatively small) kernels of the same CUDA context can oc-
casionally execute concurrently, allowing maximum utilization of
GPU resources. However, all stream multiprocessors (SMs) are
ﬁrst ﬁlled with threads from the ﬁrst kernel, and only if the re-
maining resources are sufﬁcient, threads from a second kernel can
#Msgs
1
16
64
112
128
1024
4096
8192
CPU GPU [25] PixelVault PixelVault (w/ KeyStore)
1632.7
1632.7
1632.7
1632.7
1632.7
1632.7
1632.7
1632.7
14.3
239.2
939.6
1630.3
1861.7
9793.1
14998.8
21654.4
15.5
242.2
954.9
1659.5
1892.3
10643.2
17623.5
24904.2
15.3
240.4
949.9
1652.4
1888.3
10640.8
17618.3
24896.1
Table 3: Decryption performance of 1024-bit RSA (#Msgs/sec).
be spawned. As a result, if all SMs are ﬁlled, threads from an-
other kernel cannot execute before the initial kernel completes its
execution. During initialization, PixelVault spawns a large number
of threads that remain idle, busy-waiting, as we described in Sec-
tion 4.1, occupying all available registers and shared memory. As a
result, a malicious kernel cannot be launched simultaneously.
6.1.6 Register Spilling In Global Device Memory
The registers that will be used by a GPU kernel are declared
once, at compile time. As we can see in Table 1, the number
of registers contained in GPUs is limited, and varies from plat-
form to platform. When the number of declared registers exceeds
the limit, the extra registers are mapped in global device memory,