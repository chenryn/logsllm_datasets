32,768
Round 2 50K 0
20,768 20,768 30K 0
12K 12K 30K 32,768 0
0
Round 3 50K 20,768 0
20,768 30K 12K 0
12K 30K 0
32,768 0
rest is reserved for testing. In addition, we use 3 × 32, 768 AGDs obtained from
the Dictionary-based DGA Suppobox [15], corresponding to three diﬀerent dic-
tionaries or wordlists, referred to as WL1, WL2, and WL3. How we split this
malware data into portions for training and testing varies with the experiment.
Tables 1 provides an overview of the setup of two experiments involving the
ground truth data.
– Test-Familiar: The test data consists of Dictionary-AGDs generated with the
same dictionaries as the AGDs in the training data;
– Test-Unfamiliar: The test data consists of Dictionary-AGDs that were gener-
ated with a dictionary that was not known or available during training time.
This experimental setting is intended to show that the model can be trained
on a speciﬁc family and detect a distinct family, unfamiliar to the model.
Both experiments each consist of three rounds, corresponding to which wordlist
is left out when training. For instance, as can be observed in Table 1, in round 1,
Dictionary-AGDs generated with wordlist 3 do not appear in the training data.
Table 2. Description of imbalanced datasets used for testing and training. The imbal-
ance present in this data is very common in real traﬃc, where only a very small fraction
of the data corresponds to malicious activity.
Dataset Train
Test-Imbalanced
Alexa WL1 WL2 WL3 Alexa WL1 WL2 WL3
Round 1 50,000 169
Round 2 50,000 0
169
169
Round 3 50,000 169
0
0
169
169
10,000 0
10,000 169
0
0
10,000 0
169
169
0
0
In an additional experiment, we measure the performance of all DGA domain
detection methods in a scenario where very few samples of AGDs are available for
training (see Table 2). The 507 AGDs involved in this experiment were selected
from DGArchive; they were valid for one day only (Dec 1, 2016).
306
M. Pereira et al.
Real Traﬃc Data. The data used in our real traﬃc experiments consists of a
real time stream of passive analysis of DNS traﬃc, as in [5]. The traﬃc stems
from approximately 10 billion DNS queries per day collected from multiple
ISPs (Internet Service Providers), schools and businesses distributed all over
the world. We collected 8 days of traﬃc from December 2016 to perform our
experiments, from Dec 8 to Dec 15 (see Table 3). From the data, we keep only A
and AAAA type DNS queries (i.e. IPv4 and IPv6 address records), and exclude
all domains that receive less than 5 queries in a day.
Table 3. DNS traﬃc data description. We collected 8 days of real traﬃc data to
measure the performance of our proposed WordGraph model. Days 1, 2 and 3 are used
for model training, and days 4 through 8 are used for model testing.
Dataset All domains
Domains Resolved NX
Known AGDs (DGArchive)
Resolved NX
Day 01 4,886,247 4,433,248 454,003 47
Day 02 4,922,618 4,532,932 390,735 67
Day 03 4,906,309 4,477,049 430,239 62
Day 04 4,350,224 3,981,514 369,673 87
Day 05 5,898,723 5,380,945 518,886 82
Day 06 5,425,651 4,963,786 463,584 73
Day 07 5,631,353 5,098,121 534,572 83
Day 08 5,254,954 4,747,867 508,319 95
593
673
608
662
665
680
591
635
The data stream consists of legitimate domains and malicious domains. All
domains from this stream that are known to be Dictionary-based DGAs accord-
ing to DGArchive [15] are marked as such. Although the number of unique
Dictionary-based AGDs found in the real traﬃc data by cross-checking it against
DGArchive is small, the total number of queries for such domains is tens of thou-
sands per day. Furthermore, as will become clear in Sect. 6.2, the real traﬃc data
contains more AGDs than those known in DGArchive.
5.2 Classiﬁcation Models: Random Forest and Deep Learning
As stated in Sect. 1, the existing state-of-the-art approaches for classifying
domain names as benign of malicious are either based on training a machine
learning model with human deﬁned lexical features that can be extracted from
the domain name string, or on training a deep neural network that learns the
features by itself. To show that the method that we propose in this paper outper-
forms the state-of-the-art, we include an experimental comparison with each kind
of the existing approaches. For the approach based on human deﬁned features,
we train random forests (RFs) based on lexical features, extracted from each
Detection of Algorithmically Generated Domain Names in DNS Traﬃc
307
domain name string (see e.g. [24,25]). Within supervised learning, tree ensemble
methods – such as random forests – are among the most common algorithms of
choice for data scientists because of their general applicability and their state-of-
the-art performance. Regarding the deep learning approach, a recent study [23]
found no signiﬁcant diﬀerence in predictive accuracy between recently proposed
convolutional [16] and recurrent neural networks [10,20] for the task of DGA
detection, while the recurrent neural networks have a substantially higher train-
ing time. In our comparative overview we therefore use a convolutional neural
network (CNN) architecture as in [16].
Data Preprocessing. The strings that we give as input to all classiﬁers consist of
a second level domain (SLD) and a top level domain (TLD), separated by a dot,
as in e.g. wikipedia.org. As input to the CNN approach, we set the maximum
length at 75 characters. The SLD label and the TLD label can in theory each be
up to 63 characters long each. In practice they are typically shorter. If needed,
we truncate domain names by removing characters from the end of the SLD
until the desired length of 75 characters is reached. For domains whose length
is less than 75, for the CNN approach, we pad with zeros on the left because
the implementation of the deep neural network expects a ﬁxed length input. For
the RF and WordGraph approaches we do not do any padding. We convert each
domain name string to lower case, since domain names are case insensitive.
Random Forest (RF). In each experiment, we train a random forest (RF) on the
following 11 commonly used features, extracted from each domain name string:
ent (normalized entropy of characters); nl2 (median of 2-gram); nl3 (median
of 3-gram); naz (symbol character ratio); hex (hex character ratio); vwl (vowel
character ratio); len (domain label length); gni (gini index of characters); cer
(classiﬁcation error of characters); tld (top level domain hash); dgt (ﬁrst char-
acter digit). Each trained random forest consists of 100 trees. We refer to [22]
for a detailed description of each of these features.
Deep Learning (CNN). In addition, in each experiment, following [16], we train
a convolutional neural network that takes the raw domain name string as input.
The neural network consists of an embedding layer, followed by a convolutional
layer, two dense hidden layers, and an output layer. The role of the embedding
layer is to learn to represent each character that can occur in a domain name by
a 128-dimensional numerical vector, diﬀerent from the original ASCII encoding.
The embedding maps semantically similar characters to similar vectors, where
the notion of similarity is implicitly derived (learned) based on the classiﬁcation
task at hand. The embedding layer is followed by a convolutional layer with 1024
ﬁlters, namely 256 ﬁlters for each of the sizes 2, 3, 4, and 5. During training of
the network, each ﬁlter automatically learns which pattern it should look for. In
this way, each of the ﬁlters learns to detect the soft presence of an interesting
soft n-gram (with n = 2, 3, 4, 5). For each ﬁlter, the outcome of the detection
phase is aggregated over the entire domain name string with the help of a pooling
step. That means that the trained network is detecting the presence or absence
308
M. Pereira et al.
of patterns in the domain names, without retaining any information on where
exactly in the domain name string these patterns occur. The output of the
convolutional layer is consumed by two dense hidden layers, each with 1024
nodes, before reaching a single node output layer with sigmoid activation. In all
experiments, we trained the deep neural networks for 20 epochs, with batch size
100 and learning rate 0.0001.
6 Results
We report the results of all methods in terms of precision (positive predictive
value, PPV), recall (true positive rate, TPR) and false positive rate (FPR). As
usual, PPV = TP/(TP+FP), TPR = TP/(TP+FN) and FPR = FP/(FP+TN)
where TP, FP, TN, and FN are the number of true positives, false positives, true
negatives, and false negatives respectively. Blocking legitimate traﬃc is highly
undesirable, therefore a low false positive rate is very important in deployed
DGA detection systems. For parameter tuning purposes, in each experiment, we
systematically split 10% of the training data oﬀ as validation data for the RF
and CNN methods.
6.1 Experimental Results: Ground Truth Data
Figure 4 presents an overview of the results achieved by all models in the three
diﬀerent experimental settings with ground truth data described in Sect. 5.1. A
ﬁrst important result is that, across the board, the WordGraph method achieves
a perfect TPR of 1, meaning that all Dictionary-AGDs are detected. To allow
for a fair comparison, we selected classiﬁcation thresholds for which the RF and
CNN methods also achieve a TPR of 1. It is common for such a high TPR to be
accompanied by a rise in FPR and a drop in PPV. As can be seen in Fig. 4, this
is most noticeable for the RF method, and, to a somewhat lesser extent for the
CNN method. The WordGraph method on the other hand is barely impacted at
all: it substantially outperforms the CNN and RF methods in all experiments.
Test-Familiar Experiment. Detailed results for all methods in the “Test =
Familiar” experiment are presented in Table 4. In this experiment, the train
and test data contain AGDs generated from the same set of dictionaries. This
experimental setup gives an advantage to classiﬁcation models such as CNNs and
Random Forests, since it allows the classiﬁcation model to ‘learn’ characteristics
of the words from the dictionaries.
A ﬁrst observation from Table 4 is that the RF method does not do well at
all. This is as expected: the lexical features extracted from the domain name
strings to train the RFs have been designed to detect traditional DGAs, and
Dictionary-based DGAs have been introduced with the exact purpose of evading
such detection mechanisms. This is also apparent from the density plots of the
features in Fig. 5: the feature value distributions for the AGDs from WL1, WL2,
Detection of Algorithmically Generated Domain Names in DNS Traﬃc
309
Fig. 4. Overview of FPR (lower is better) and PPV (higher is better) for all methods
across the experimental setups on the ground truth data, for a ﬁxed TPR=1. The
WordGraph (WG) approach consistently achieves a very low FPR, of the order of
−4, two order of magnitudes lower than the best FPR achieved by the
magnitude of 10
CNN model.
and WL3 are very similar to those of the Alexa domain names, which explains
the poor performance of the RF models that are based on these features.
The WordGraph method on the other hand works extremely well. It detects
all AGDs in the test data in all rounds, while only misclassifying a very small
number of benign domain names as malicious (namely 8/30,000 in round 1;
4/30,000 in round 2; and 1/30,000 in round 3). Finally, it is worth to call out
that the CNN models have a very good performance as well. This is likely due
to the fact that, as explained in Sect. 5.2 the CNN neural networks learn to
detect the presence of interesting soft n-grams (with n = 2, 3, 4, 5), so, in a
sense, they can memorize the dictionaries. It is interesting to point out that the
CNN method performs consistently well throughout the rounds, i.e. given the
fact that the dictionary was seen before by the model, there is no dictionary that
is easier to ‘learn’.
Table 4. Results of random forest (RF), deep learning (CNN), and our proposed
WordGraph approach (WG) on balanced ground truth data, for a ﬁxed TPR=1. The
AGDs in the training and test data are generated from the same dictionaries (“Test-
Familiar”).
Method
Round 2
Round 3
Round 1
FPR
WordGraph 2.67 · 10
CNN
0.018
PPV FPR
PPV FPR
−4 0.999 1.33 · 10
−4 0.999 3.33 · 10
0.981 0.015
0.982 0.014
PPV
−5 0.999
0.983
RF
1.0
0.444 1.0
0.444 1.0
0.444
310