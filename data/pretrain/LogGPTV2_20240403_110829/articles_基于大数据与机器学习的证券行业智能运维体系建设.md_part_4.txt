 轻量级快速处理
着眼大数据处理，速度往往被置于第一位。流式处理引擎中的应用程序
在内存中以100倍的速度运行，它们将中间处理数据全部放到了内存中。
 无数据丢失
系统需要保证无数据丢失，这也是系统高可用性的保证。系统为了无数
据丢失，需要在数据处理失败的时候选择另外的执行路径进行replay（系统
不是简单的重新提交运算，而是重新执行调度，否则按照来源的call stack
有可能使得系统永远都在相同的地方出同样的错误）。
 无数据重复
系统需要保证无数据重复，来保证系统功能的正确性。为了保证数据不
重复，在数据可能丢失的时候重试，通过日志存储引擎来去掉重复数据。
 容错透明
系统会自动处理容错，调度并且管理资源，而这些行为对于运行于其上
的应用来说都是透明的。
模块架构如下：
日志处理系统能够根据配置的规则抽取日志关键字段，将非结构化的日志转
换成结构化数据。抽取关键字段的好处是可以对关键字段进行统计分析。平台将
对关键字段及原始日志进行索引，用户可对关键字段及原始日志进行搜索。
平台已经配置了常见日志的解析规则，对于平台没有预先配置解析规则的日志，
用户可通过后台或Web页面配置解析规则，抽取关键字段。即使没有抽取关键字
段，用户仍然可以通过全文检索搜索日志。
平台在日志做索引之前抽取关键字段，提高了检索的速度。
支持对日志数据信息的完全可配置化解析，数据类型的多样性要求处理层的
配置化解析方式，减少开发周期与人员投入，降低学习与使用成本，具体包括：
 支持正则表达式
 支持kv/xml/json自动解析等多种解析规则
 支持划选辅助自动生成解析规则
 支持目标源配置，包括库名和表名或日志索引名和类型名
 支持整形、浮点型、字符串、日期时间等字段类型转换
 支持元数据的解析
 支持解析结果预览
 支持解析规则分组
 支持批量验证正则
 支持界面查看字段提取规则的性能统计情况
3.3.4 数据存储
数据存储是智能运维管理平台的数据落地的地方，根据不同的数据类型以及
不同数据类型的使用场景，选择了不同的数据存储方式。
 针对一些需要进行全文检索，分词搜索的数据存入 Yottasearch，用于
实时可视化查询/分析。
 所有数据也都会存入Hadoop HDFS/HIVE之上用于长期保存及离线批量统
计及计算。
 平台管理的管理配置类的数据是结构化数据，直接由平台存于结构化数
据库MySQL之中。
Yottasearch 索引集群是一个的分布式搜索引擎，具备高可靠性和高性能。
支持时间文本索引和全文检索，提供丰富的 api用于索引、检索、修改大多数配
置。能够快速搜索数百亿的日志以及 TB 级的数据，结构化或者非结构化的数据
都可以。
集群由 2 台及 2 台以上节点组成，其中一个为主节点，节点通过选举产生，
主从节点是对于整个集群内部来说的，从外部来看整个集群，逻辑上是一个整体，
与任何一个节点的通信和与整个集群通信是完全一致的。集群自动创建索引，通
过配置我们可以非常方便的调整索引分片和索引副本。通过索引分片技术，一个
大的索引被拆分成多个，然后分布在不同的节点上，以构成分布式搜索。索引副
本的作用一是提供系统的容错性，当摸个节点摸个分片损毁或丢失时，可以从副
本中恢复；二是提供查询效率，集群内部会自动实现搜索请求的负载均衡。
模块架构如下：
日志数据作为运维数据中的主要核心数据，来源是非结构化数据，但是通过
前面的数据处理引擎后，数据完成了从结构化到非结构化的转变，可以在支持全
文检索的同时，也可以支持结构化查询。选择Yottasearch作为日志等文本数据
的实时分析存储落地。Yottasearch是基于搜索引擎Lucene之上的分布式扩展，
既可以实现文本非结构化字段的检索，又可以基于其 DocValue 的列式存储实现
高效的分布式聚合计算。
此外，Yottasearch还支持通过Groovy脚本来扩展聚合以及搜索的能力，可
以实现"多层聚合"以及Schema On Read、Schema On Write的能力，为上层提供
更加丰富的查询服务。
同时通过多种方式导入到 HDFS 文件系统中，对需要通过统一数据处理的数
据使用Mapreduce或spark应用对数据进行清洗。
第 章智能算法与 场景
4 AI
本章描述在智能运维领域常用的算法模型，以及常见的AIOps运维场景的描
述，通过对智能算法和场景应用的理解，这些是我们在光大证券的智能运维体系
建设中所必须掌握的智能运维体系的理论基础。
4.1 智能算法种类
4.1.1 无监督异常检测
在我们的无监督异常检测方案中，主要使用到的基础算法包括：变分自编码
器、GBRT、EMA和极值理论等。
4.1.1.1 变分自编码器
对于有足够计算资源（需要 GPU）而且积累的历史数据足够多的情况下，会
选择变分自编码器算法（Variational Auto-Encoder，VAE），变分自编码器是深
度学习生成模型的一种，结构分为编码层（encoder），隐变量层（latentvariable），
解码层。具体结构如下：
变分自编码器是一种深度贝叶斯网络，它描述了两个随机变量间的关系——
隐变量z和观察值x，往往让z符合多元正态分布 的先验，这样x符合一
个有参数θ的神经网络的 ，真正的后验 是难以计算的，但是对于
𝒩𝒩(0,I)
𝓅𝓅𝜃𝜃(x|z) 𝓅𝓅𝜃𝜃(z|x)
训练和预测却又是必须的，因此通常拟合另一个神经网络作为近似后验
该后验通常假设为和属于 ，其中 和 由神经𝓆𝓆网𝜙𝜙(络x|得z)
2 2
到。变分推理算法有多种， 𝒩𝒩一 (𝜇𝜇般𝜙𝜙(采 x)用 S𝜙𝜙G (V xB ))，通过最 𝜇𝜇𝜙𝜙大 (x化 )证 𝜎𝜎据𝜙𝜙下 (x界 )（ELBO）近似
,𝜎𝜎
后验和生成模型联合训练，具体公式如下图所示：
正如下图所示，通常会采用蒙特卡洛积分来近似上图中所示期望值，其中
(𝑙𝑙)
和 属于 ：
𝑧𝑧
ℓ = 1…ℒ 𝓆𝓆𝜙𝜙(x|z)
我们基于VAE的DONUT算法采用如下的算法结构：
整体基于重建误差来进行异常检测，对于有充分历史数据的时间序列的异常
检测能达到很好的效果，效果在我们的测试数据集上以及公开数据集上效果远好
于目前主流的有监督学习方法和神经网络算法。
4.1.1.2 渐进梯度回归树
对于训练数据足够，允许较长时间的重新训练，存储资源丰富，且不要求算
法太高的可解释度的情况下采用渐进梯度回归树（Gradient BoostRegression
Tree，GBRT），它是一种迭代的决策树算法，该算法的最终结果由多棵决策树累
加组成，泛化能力强大，每一棵树都是从之前所有树的残差中学习，逐渐接近最
好的效果。
对于给定的训练数据 ，损失函数 ，
可以得到回归树 。 𝒯𝒯 = (𝑥𝑥1, 𝑦𝑦1),(𝑥𝑥2, 𝑦𝑦2),…,(𝑥𝑥𝑛𝑛, 𝑦𝑦𝑛𝑛) ℒ(y,f(x))
首先初始化决𝑓𝑓̃(策x)树，估计一个使损失函数极小化的常数值：
𝑁𝑁
𝑓𝑓0(𝑥𝑥)=𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑛𝑛𝑐𝑐𝐿𝐿(𝑦𝑦𝑎𝑎,𝑐𝑐)
对于每棵树计算损失函数的负梯度在当𝑎𝑎前=1模型的值，并将它作为残差的估计
值，具体公式如下：
𝜕𝜕𝐿𝐿(𝑦𝑦𝑎𝑎,𝑓𝑓(𝑥𝑥𝑎𝑎))
对于每棵树的每个叶子节𝑎𝑎𝑎𝑎𝑎𝑎点=，−计 算： 
𝜕𝜕𝑓𝑓(𝑥𝑥𝑎𝑎) 𝑓𝑓=𝑓𝑓𝑎𝑎−1
利用线性搜索估计𝑐𝑐叶𝑎𝑎𝑚𝑚 节=点𝑎𝑎𝑎𝑎区𝑎𝑎𝑎𝑎域𝑎𝑎𝑛𝑛的𝑐𝑐 值，使𝐿𝐿(损𝑦𝑦𝑎𝑎,失𝑓𝑓𝑎𝑎 函−1( 数𝑥𝑥𝑎𝑎) 最+小𝑐𝑐)化，然后更新：
𝑥𝑥𝑎𝑎∈𝑅𝑅𝑎𝑎𝑚𝑚
𝐽𝐽
𝑓𝑓𝑎𝑎(x)=𝑓𝑓𝑎𝑎−1(x)+𝑐𝑐𝑎𝑎𝑚𝑚𝐼𝐼, 𝑥𝑥 ∈𝑅𝑅𝑎𝑎𝑚𝑚
最终得到的 就是我们最终的模型:
𝑚𝑚=1
𝑓𝑓𝑎𝑎(x)
𝑀𝑀 𝐽𝐽
𝑓𝑓̃(x)=𝑓𝑓𝑀𝑀(x)=  𝑐𝑐𝑎𝑎𝑚𝑚𝐼𝐼, 𝑥𝑥 ∈𝑅𝑅𝑎𝑎𝑚𝑚
渐进梯度回归树的第三方库很多，𝑎𝑎=但1𝑚𝑚是=1作为树形结构，最主要的是需要我们
提取多维特征来描述每个点的状态，而特征提取的好坏直接决定了我们回归树的
结果好坏，我们提取的部分特征有：
mean 均值
stdev 标准差
acf1 一阶自相关系数
linearity 线性强度
curvature 曲率强度
entropy 光谱熵
lumpiness 残差变化标准差
cpoints 交叉点个数
Diff 与前面点的差值
… 。。。
对于每一个时间序列中的点，针对它前面的不同大小窗口，提取不同的特征，
用于训练和测试，在线测试的过程中，对于每个时间点算法会预测出它应该的取
值，根据一些动态阈值的选择方法给出合理的取值范围（上下基带），超出则报
异常。
4.1.1.3 差分指数滑动平均