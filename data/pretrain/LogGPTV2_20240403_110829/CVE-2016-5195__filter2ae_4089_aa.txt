# 回眸——CVE-2016-5195 脏牛漏洞浅析
##### 译文声明
本文是翻译文章
译文仅供参考，具体内容表达以及含义原文为准。
要写这篇文章的原因，主要是因为自己想在内核这块稍微多了解一点，再多了解一点，谈到内核提权漏洞，“dirty
cow”是入门内核漏洞绕不开的一个点，这里我怀着敬畏的态度以一个新手的角度，来对脏牛进行简单的分析。
## 一、”dirty cow” 简介
脏牛漏洞之所以如此著名，主要在于其历史悠久且影响范围较广，根据 wiki 百科上的描述，脏牛漏洞自2007年9月 linux kernel 2.6.22
被引入，直到2018年linux kernel 4.8.3, 4.7.9, 4.4.26
之后才被彻底修复，影响在此之间的所有基于其中版本范围的Linux发行版。
## 二、相关概念
###  1\. COW
COW 全名为 Copy-on-write，cow技术的应用范围很广泛，比较常见的是在 `fork()` 中的应用：
    fork() creates a new process by duplicating the calling process.
    The new process is referred to as the child process.  The calling
    process is referred to as the parent process.
    The child process and the parent process run in separate memory
    spaces.  At the time of fork() both memory spaces have the same
    content.  Memory writes, file mappings (mmap(2)), and unmappings
    (munmap(2)) performed by one of the processes do not affect the
    other.
以上一段是从man手册中截取出来的对fork的描述，从该描述中可知 `fork` 在创建子进程时，会对自身进程空间进行复制， `fork`
完成时，父子进程具有完全相同的进程空间。  
从编码的角度来看，一般情况下在 `fork` 之后会存在一个 `execve` 或其他 exec 系列的函数来执行一个新的程序，在调用 `execve`
的时候，内核会将新程序的代码段、数据段……映射到子进程的内存中。  
上述创建子进程的过程，父进程将自身的内存空间完全拷贝给了子进程后，子进程很快就执行 `execve`
将新程序装载进入自己的内存中，覆盖了大部分父进程拷贝的内存，那么实际上大部分的父进程拷贝的数据是无用的。  
因而内核引入了 Copy-on-write 技术，即当 `fork`
创建完子进程后，父子进程实际上共享物理内存，当父子进程中发生了对内存写入的操作时，内核再为子进程分配新的内存页并将改动写入新内存页中，也就是在 fork
之后，execve 之前的过程。
**实现原理**
      fork之后，内核将父进程中所有内存页的权限都设置为 read-only，之后子进程内存指向父进程，当父进程或子进程执行了写入操作时，因为内存页是 read-only 的，就会触发 page-fault，进而进入内核中断例程中，在中断例程中内核将触发异常的页复制一份，至此，父子进程就拥有了各自的内存。
###  2\. linux 虚拟内存
这一步我最开始的思路是想要在 qemu 里通过手动完成 PTE 寻址后置零 PTE 页面属性来代替 madvise
函数的功能，但是在最后一步置零的操作上因为 qemu monitor 无法对内存进行写入而放弃。
**(1). VMA简介（Virtual Memory Area）**
虚拟内存概念的引入，以32位系统为例，进程可以“独享”3G大小的用户空间，且进程之间的操作是互相隔离的，对相同虚拟地址的操作并不会产生冲突。只有当进程开始操作申请到的内存时，内核才会触发缺页异常将指定的物理页面换入内存中。  
进程的虚拟内存会被分成若干区域，这些区域就是 VMA，VMA的各种属性由 `vm_area_struct` 结构来描述：
    struct vm_area_struct {
        struct mm_struct * vm_mm;   /* 所属的内存描述符 */
        unsigned long vm_start;    /* vma的起始地址 */
        unsigned long vm_end;       /* vma的结束地址 */
        /* 该vma的在一个进程的vma链表中的前驱vma和后驱vma指针，链表中的vma都是按地址来排序的*/
        struct vm_area_struct *vm_next, *vm_prev;
        pgprot_t vm_page_prot;      /* vma的访问权限 */
        unsigned long vm_flags;    /* 标识集 */
        struct rb_node vm_rb;      /* 红黑树中对应的节点 */
        /*
         * For areas with an address space and backing store,
         * linkage into the address_space->i_mmap prio tree, or
         * linkage to the list of like vmas hanging off its node, or
         * linkage of vma in the address_space->i_mmap_nonlinear list.
         */
        /* shared联合体用于和address space关联 */
        union {
            struct {
                struct list_head list;/* 用于链入非线性映射的链表 */
                void *parent;   /* aligns with prio_tree_node parent */
                struct vm_area_struct *head;
            } vm_set;
            struct raw_prio_tree_node prio_tree_node;/*线性映射则链入i_mmap优先树*/
        } shared;
        /*
         * A file's MAP_PRIVATE vma can be in both i_mmap tree and anon_vma
         * list, after a COW of one of the file pages.  A MAP_SHARED vma
         * can only be in the i_mmap tree.  An anonymous MAP_PRIVATE, stack
         * or brk vma (with NULL file) can only be in an anon_vma list.
         */
        /*anno_vma_node和annon_vma用于管理源自匿名映射的共享页*/
        struct list_head anon_vma_node; /* Serialized by anon_vma->lock */
        struct anon_vma *anon_vma;  /* Serialized by page_table_lock */
        /* Function pointers to deal with this struct. */
        /*该vma上的各种标准操作函数指针集*/
        const struct vm_operations_struct *vm_ops;
        /* Information about our backing store: */
        unsigned long vm_pgoff;     /* 映射文件的偏移量，以PAGE_SIZE为单位 */
        struct file * vm_file;          /* 映射的文件，没有则为NULL */
        void * vm_private_data;     /* was vm_pte (shared mem) */
        unsigned long vm_truncate_count;/* truncate_count or restart_addr */
    #ifndef CONFIG_MMU
        struct vm_region *vm_region;    /* NOMMU mapping region */
    #endif
    #ifdef CONFIG_NUMA
        struct mempolicy *vm_policy;    /* NUMA policy for the VMA */
    #endif
    };
**(2). 映射关系**
这里首先介绍一下 Linux 四级页表的基本知识点，当我们需要实际操作一个页面的时候，ring3 程序使用的是虚拟地址，ring0
需要对虚拟地址进行转换，对应到物理地址后才能对内存进行操作。  
为什么要存在页表呢？  
真实的物理内存只有固定的大小，但是操作系统给每个进程都会提供同样大小的虚拟内存，那么问题来了，物理内存只有固定的大小，想要让所有的进程都感觉自己使用了所有的物理内存应该怎么做呢？—>
将每个进程活跃的页面放到物理内存中，不活跃的就从物理内存中换出，等到需要的时候再从外存中调入，同一时刻进程真正活跃的页面相对于其整个占用的内存空间来说是比较小的。此时，就引出了另外一个问题，我怎么知道我要访问的虚拟内存页面是哪个物理内存页面？—>
此时页表（分页机制）就闪亮登场了，通过虚拟地址以及 cr3 提供的信息进行转换后，就可以找到虚拟地址对应的物理地址。  
通常来说，需要多级页表来完成映射关系，级数越高，页表所占的内存就越小，效率就越低。  
以 32位 系统为例，ring3 可用的虚拟内存大小为 3G，一页内存大小为 4k，即 3 _2^30 / 4_ 2^10 = 786432
个页面，一个页面需要 4 byte 的页表，也就是一个进程就需要 768 个物理页来存储页表，对于动辄几十上百个进程的操作系统来说这样的开销是不可容忍的。  
Linux 中采用的是四级页表的存储方式
因此需要根据虚拟地址和 cr3 的信息进行四次寻址才能拿到 pte 的地址。  
在 `__get_user_pages` 函数处断下来
根据打印出的虚拟地址进行换算
偏移如下：
得到换算结果后，qemu `Ctrl+A - C` 进入 qemu command interface 调用 `info registers` 获取 cr3
寄存器，根据 cr3 寄存器的数值进行寻址，此处需要注意获取的地址的低 12bit 为页面属性。
    (qemu) xp 0x00000000062e8000+0x7f0    PGD
    00000000062e87f0: 0x00000000062dc067
    (qemu) xp 0x00000000062dc000+0x140    PUD
    00000000062dc140: 0x00000000062e9067
    (qemu) xp 0x00000000062e9000+0x458    PMD
    00000000062e9458: 0x00000000062e7067
    (qemu) xp 0x00000000062e7000+0xb30    PTE
    00000000062e7b30: 0x8000000007294865
    (qemu)
此时算出来的 `00000000062e7b30` 就是 PTE 的地址了，此时只要将 PTE 清零即可完成 madvise
的工作，这里卡了很久，最后是从看雪的 [r0Cat](https://bbs.pediy.com/user-home-803510.htm)
师傅的文章中学到通过 `gpa2hva` 命令：
    gpa2hva addr
    Print the host virtual address at which the guest’s physical address addr is mapped.
获取到 qemu 地址后通过 gdb attach qemu进程修改该地址的内容即可。
## 三、调试
###  1\. __get_user_pages
因为正常 dirtycow 的 poc 是竞争来触发漏洞的，而此时我只需要对 `__get_user_pages`
函数的处理逻辑进行熟悉，因此不需要使用竞争的 poc ，只需要使用能够触发缺页异常的代码即可。
第一次断点下在了 `get_user_pages` 上，没有断下来，第二次下在 `__get_user_pages` 上成功断下
此时可以对着源码进行调试分析了~  
首先把参数相关定义熟悉一下：
     * __get_user_pages() - pin user pages in memory
     * @tsk:    task_struct of target task
     * @mm:        mm_struct of target mm    // 描述虚拟内存的结构
     * @start:    starting user address    // 请求的虚拟地址
     * @nr_pages:    number of pages from start to pin    // 需要换入的内存页数量
     * @gup_flags:    flags modifying pin behaviour    // 期望得到的页的权限
     * @pages:    array that receives pointers to the pages pinned.
     *        Should be at least nr_pages long. Or NULL, if caller
     *        only intends to ensure the pages are faulted in.
     * @vmas:    array of pointers to vmas corresponding to each page.
     *        Or NULL if the caller does not require them.
     * @nonblocking: whether waiting for disk IO or mmap_sem contention
     *
    ...
     * Returns number of pages pinned. This may be fewer than the number // 返回值为换入物理内存中页的数量。
     * requested. If nr_pages is 0 or negative, returns 0. If no pages
     * were pinned, returns -errno. Each page returned must be released
     * with a put_page() call when it is finished with. vmas will only
     * remain valid while mmap_sem is held.
    ...
     * __get_user_pages walks a process's page tables and takes a reference to
     * each struct page that each user address corresponds to at a given
     * instant. That is, it takes the page that would be accessed if a user
     * thread accesses the given user virtual address at that instant.
通过上述这段注释可以大概了解 `__get_user_pages`
的作用，即当用户层程序访问虚拟内存时，如果要操作的虚拟内存页面不在物理内存中，`__get_user_pages` 负责将所需的内存页面换入物理内存中。  
同时在注释中也不建议直接使用 `__get_user_pages`
     * In most cases, get_user_pages or get_user_pages_fast should be used
     * instead of __get_user_pages. __get_user_pages should be used only if
     * you need some special @gup_flags.
在 `__get_user_pages` 中有两个关键的函数，第一个是 `follow_page_mask`，通过函数开头的注释可以大概了解