仅仅当该组内没有任何进程时可以使用，用法 echo 0 > memory.force_empty  
如果无法清除，则内存会移到父组（如果是根组，则不允许对其执行 echo 0 > memory.force_empty，除非没有进程在里面）  
```
when set to 0, empties memory of all pages used by tasks in the cgroup.   
This interface can only be used when the cgroup has no tasks.   
If memory cannot be freed, it is moved to a parent cgroup if possible.   
Use the memory.force_empty parameter before removing a cgroup to avoid moving out-of-use page caches to its parent cgroup.  
```
### 7. 回收子进程内存  
用于控制是否回收当前cgroup中pid的子进程的内存  
memory.use_hierarchy  
```
contains a flag (0 or 1) that specifies whether memory usage should be accounted for throughout a hierarchy of cgroups.   
If enabled (1), the memory subsystem reclaims memory from the children of and process that exceeds its memory limit.   
By default (0), the subsystem does not reclaim memory from a task's children.  
```
### 8. 内存压力 通知机制  
内存的资源隔离还提供了一种压力通知机制。当cgoup内的内存使用量达到某种压力状态的时候，内核可以通过eventfd的机制来通知用户程序，通过cgroup.event_control和memory.pressure_level来实现。  
使用方法：  
与 https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-memory.html 有一些出入，可能是版本问题。    
1\. 使用eventfd()创建一个eventfd，假设叫做efd，  
2\. 然后open()打开memory.pressure_level的文件路径，产生一个另一个fd，我们暂且叫它cfd，  
3\. 然后将这两个fd和我们要关注的内存压力级别告诉内核，让内核帮我们判断条件是否成立，如果成立，内核会把以上信息按这样的格式: "event_fd efd" 写入cgroup.event_control。   
4\. 然后就可以去等着efd是否可读了，如果能读出信息，则代表内存使用已经触发相关压力条件。  
压力级别的level有三个：  
"low"：表示内存使用已经达到触发内存回收的压力级别。  
"medium"：表示内存使用压力更大了，已经开始触发swap以及将活跃的cache写回文件等操作了。  
"critical"：到这个级别，就意味着内存已经达到上限，内核已经触发oom killer了。  
程序从efd读出的消息内容就是这三个级别的关键字。我们可以通过这个机制，建立一个内存压力管理系统，在内存达到相应级别的时候，触发响应的管理策略，来达到各种自动化管理的目的。  
下面给出一个监控程序的例子：  
```  
#include   
#define USAGE_STR "Usage: cgroup_event_listener "  
int main(int argc, char **argv)  
{  
int efd = -1;  
int cfd = -1;  
int event_control = -1;  
char event_control_path[PATH_MAX];  
char line[LINE_MAX];  
int ret;  
if (argc != 3)  
errx(1, "%s", USAGE_STR);  
cfd = open(argv[1], O_RDONLY);  
if (cfd == -1)  
err(1, "Cannot open %s", argv[1]);  
ret = snprintf(event_control_path, PATH_MAX, "%s/cgroup.event_control",  
dirname(argv[1]));  
if (ret >= PATH_MAX)  
errx(1, "Path to cgroup.event_control is too long");  
event_control = open(event_control_path, O_WRONLY);  
if (event_control == -1)  
err(1, "Cannot open %s", event_control_path);  
efd = eventfd(0, 0);  
if (efd == -1)  
err(1, "eventfd() failed");  
ret = snprintf(line, LINE_MAX, "%d %d %s", efd, cfd, argv[2]);  
if (ret >= LINE_MAX)  
errx(1, "Arguments string is too long");  
ret = write(event_control, line, strlen(line) + 1);  
if (ret == -1)  
err(1, "Cannot write to cgroup.event_control");  
while (1) {  
uint64_t result;  
ret = read(efd, &result, sizeof(result));  
if (ret == -1) {  
if (errno == EINTR)  
continue;  
err(1, "Cannot read from eventfd");  
}  
assert(ret == sizeof(result));  
ret = access(event_control_path, W_OK);  
if ((ret == -1) && (errno == ENOENT)) {  
puts("The cgroup seems to have removed.");  
break;  
}  
if (ret == -1)  
err(1, "cgroup.event_control is not accessible any more");  
printf("%s %s: crossed\n", argv[1], argv[2]);  
}  
return 0;  
}  
```  
## 三、最后  
Linux的内存限制要说的就是这么多了，当我们限制了内存之后，相对于使用实体机，实际上对于应用来说可用内存更少了，所以业务会相对更经常地暴露在内存资源紧张的状态下。  
相对于虚拟机（kvm，xen），多个cgroup之间是共享内核的，我们可以从内存限制的角度思考一些关于"容器"技术相对于虚拟机和实体机的很多特点：  
1\. 内存更紧张，应用的内存泄漏会导致相对更严重的问题。  
2\. 容器的生存周期时间更短，如果实体机的开机运行时间是以年计算的，那么虚拟机则是以月计算的，而容器应该跟进程的生存周期差不多，顶多以天为单位。所以，容器里面要跑的应用应该可以被经常重启。  
3\. 当有多个cgroup（容器）同时运行时，我们不能再以实体机或者虚拟机对资源的使用的理解来规划整体运营方式，我们需要更细节的理解什么是cache，什么是swap，什么是共享内存，它们会被统计到哪些资源计数中？在内核并不冲突的环境，这些资源都是独立给某一个业务使用的，在理解上即使不是很清晰，也不会造成歧义。但是在cgroup中，我们需要彻底理解这些细节，才能对遇到的情况进行预判，并规划不同的处理策略。  
也许我们还可以从中得到更多的理解，大家一起来想喽？  
## 参考  
1\. https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-memory.html  
2\. https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-Using_the_Notification_API.html  
3\. http://liwei.life/2016/01/22/cgroup_memory/  
4\. [《精确度量Linux下进程占用多少内存的方法》](../201606/20160608_01.md)  
5\. [《Linux page allocation failure 的问题处理 - lowmem_reserve_ratio》](../201612/20161221_01.md)  
6\. /usr/share/doc/kernel-doc-2.6.32/Documentation/filesystems/proc.txt  
7\. /usr/share/doc/kernel-doc-2.6.32/Documentation/cgroups/  
#### [PostgreSQL 许愿链接](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
您的愿望将传达给PG kernel hacker、数据库厂商等, 帮助提高数据库产品质量和功能, 说不定下一个PG版本就有您提出的功能点. 针对非常好的提议，奖励限量版PG文化衫、纪念品、贴纸、PG热门书籍等，奖品丰富，快来许愿。[开不开森](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216").  
#### [9.9元购买3个月阿里云RDS PostgreSQL实例](https://www.aliyun.com/database/postgresqlactivity "57258f76c37864c6e6d23383d05714ea")
#### [PostgreSQL 解决方案集合](https://yq.aliyun.com/topic/118 "40cff096e9ed7122c512b35d8561d9c8")
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")