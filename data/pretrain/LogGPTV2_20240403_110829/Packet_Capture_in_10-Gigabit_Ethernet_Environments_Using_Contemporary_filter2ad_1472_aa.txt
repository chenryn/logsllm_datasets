title:Packet Capture in 10-Gigabit Ethernet Environments Using Contemporary
Commodity Hardware
author:Fabian Schneider and
J&quot;org Wallerich and
Anja Feldmann
Packet Capture in 10-Gigabit Ethernet Environments
Using Contemporary Commodity Hardware
Fabian Schneider, J¨org Wallerich, and Anja Feldmann
Deutsche Telekom Laboratories / Technische Universit¨at Berlin
{fabian,joerg,anja}@net.t-labs.tu-berlin.de
10587 Berlin, Germany
http://www.net.t-labs.tu-berlin.de
Abstract. Tracing trafﬁc using commodity hardware in contemporary high-
speed access or aggregation networks such as 10-Gigabit Ethernet is an increas-
ingly common yet challenging task. In this paper we investigate if today’s
commodity hardware and software is in principle able to capture trafﬁc from a
fully loaded Ethernet. We ﬁnd that this is only possible for data rates up to 1 Gi-
gabit/s without reverting to using special hardware due to, e. g., limitations with
the current PC buses. Therefore, we propose a novel way for monitoring higher
speed interfaces (e. g., 10-Gigabit) by distributing their trafﬁc across a set of lower
speed interfaces (e. g., 1-Gigabit).
This opens the next question: which system conﬁguration is capable of mon-
itoring one such 1-Gigabit/s interface? To answer this question we present a
methodology for evaluating the performance impact of different system compo-
nents including different CPU architectures and different operating system. Our
results indicate that the combination of AMD Opteron with FreeBSD outper-
forms all others, independently of running in single- or multi-processor mode.
Moreover, the impact of packet ﬁltering, running multiple capturing applications,
adding per packet analysis load, saving the captured packets to disk, and using
64-bit OSes is investigated.
Keywords: Packet Capturing, Measurement, Performance, Operating Systems.
1 Introduction
A crucial component of almost any network measurement and especially any network
security system is the one that captures the network trafﬁc. For example, nowadays al-
most all organizations secure their incoming/outgoing Internet connections with a secu-
rity system. As the speed of these Internet connection increases from T3 to 100-Megabit
to 1-Gigabit to 10-Gigabit Ethernet, the demands on the monitoring system increase as
well while the price pressure remains. For example, the M¨unchner Wissenschaftsnetz
(MWN, Munich Scientiﬁc Network) [1] offers Internet connection to roughly 50,000
hosts via a 10-Gigabit bidirectional uplink to the German Scientiﬁc Network (DFN).
While the link is currently rate limited to 1.2-Gigabit we indeed face the challenge
of performing packet capture in a 10-Gigabit Ethernet environment using commodity
hardware in order to run a security system. We note that most network security systems
S. Uhlig, K. Papagiannaki, and O. Bonaventure (Eds.): PAM 2007, LNCS 4427, pp. 207–217, 2007.
© Springer-Verlag Berlin Heidelberg 2007
208
F. Schneider, J. Wallerich, and A. Feldmann
rely on capturing all packets as any lost packet may have been the attack. Furthermore,
most attack detection mechanisms rely on analyzing the packet content and the security
system itself has to be resilient against attacks [2]. Therefore packet capture has to be
able to handle both the maximum data rates as well as the maximum packet rates.
Unfortunately, network trafﬁc capture is not an easy task due to the inherent system
limitations (memory and system bus throughput) of current off-the-shelf hardware. One
expensive alternative1 is to use specialized hardware, e. g., the monitoring cards made
by Endace [3]. Our experience shows that PCs equipped with such cards are able to
capture full packet traces to disk in 1-Gigabit environments. But even these systems
reach their limits in 10-Gigabit environments: they lack CPU cycles to perform the
desired analysis and/or bus bandwidth to transfer the data to disk. Therefore, we in
this paper propose a novel way for distributing trafﬁc of a high-speed interface, e. g.,
10-Gigabit, across multiple lower speed interfaces, e. g., 1-Gigabit, using of-the-shelf
Ethernet switches. This enables us to use multiple machines to overcome the system
limitations of each individual interface without losing trafﬁc.
This leaves us with the question, which system is able to monitor one such 1-Gigabit
Ethernet interface given the many factors that impact the performance of a packet
capture system. These include the system, CPU, disk and interrupt handling architec-
ture [4], the operating system and its parameters, the architecture of the packet capture
library [5,6,7,8], and the application processing the data [2,5,9]. In this paper, we fo-
cus on the ﬁrst set of factors and only consider simple but typical application level
processing, such as compressing the data and storing it to disk. We chose to exam-
ine three high-end off-the-shelf hardware architectures (all dual processor): Intel Xeon,
AMD Opteron single core and AMD Opteron multi-core based systems; two operating
systems: FreeBSD and Linux with different parameter settings; and the packet capture
library, libpcap [5], with different optimizations.
In 1997, Mogul et al. [4] pointed out the problem of receive livelock. Since then quite
a number of approach [10,11,7,8] to circumvent this problem have been proposed. Yet
the capabilities and limitations of the current capturing systems have not been examined
in the recent past.
The remainder of this paper is structured as follows. In Sec. 2 we discuss why cap-
turing trafﬁc from an 10-Gigabit Ethernet link is nontrivial and propose ways of dis-
tributing trafﬁc across several 1-Gigabit Ethernet links. In Sec. 3 we ask if it is feasible
to capture trafﬁc across 1-Gigabit links using commodity hardware. Next, in Sec. 4,
we discuss our measurement methodology and setup. Sec. 5 presents the results of our
evaluation. Finally, we summarize our contributions and discuss future work in Sec. 6.
2 Challenges That Hinder Data Capture at High Rates
With current PC architectures there are two fundamental bottlenecks: bus bandwidth
and disk throughput. In order to capture the data of a fully utilized bidirectional 10-
Gigabit link one would need 2560 Mbytes/s system throughput. Even, for capturing
only the packet headers, e. g., the ﬁrst 68 bytes, one needs 270 Mbytes/s given an ob-
served average packet size of 645 bytes. Moreover, when writing to disk, packet capture
1 Current cost for a 1-Gigabit card is roughly 5,000 e.
Packet Capture in 10-Gigabit Ethernet Environments
209
libraries require the data to pass the system bus twice: Once to copy the data from the
card to the memory to make it accessible to the CPU, and once from memory to disk.
Thus this requires twice the bandwidth.
When comparing these bandwidth needs with the bandwidth that current busses
offer we notice a huge gap, especially for full packet capture. The standard 32-bit,
66 MHz PCI bus offers 264 Mbytes/s. The 64-bit, 133 MHz PCI-X bus offers
1,066 Mbytes/s. The PCIexpress x1 bus offers 250 Mbytes/s. It is easy to see that none
of these busses are able to handle the load imposed by even a single uni-directional
10-Gigabit link (full packets). Furthermore the numbers have to be taken with a grain
of salt as they are maximum transfer rates, which in case of PCI and PCI-X are shared
between the attached PCI cards. Some relief is in sight, PCIexpress x16 (or x32) busses
which offer 4000 Mbytes/s (8000 Mbytes/s) are gaining importance. Currently boards
with one or two x16 slot are available. Yet, note that these busses are intended for
graphic cards and that 10-Gigabit Ethernet cards are only available for the PCI-X bus
at the moment. Indeed, at this point we do not know of any network or disk controller
cards that support PCIexpress x16 or x32.
But what about the bandwidth of the disk systems. The fastest ATA/ATAPI interfaces
runs at a mere 133 Mbytes/s; S-ATA (2) offers throughputs of up to 300 Mbytes/s;
SCSI (320) can achieve 320 Mbytes/s. Even the throughput of Fiberchannel, up to
512 Mbytes/s, and Serial-Attached-SCSI (SAS), up to 384 Mbytes/s, is not sufﬁcient
for 10-Gigabit links. The upcoming SAS 2 systems are going to offer 768 Mbytes/s
while SAS 3 systems may eventually offer 1536 Mbytes/s. Again there is a huge gap.
Therefore, it appears unavoidable to distribute the load
across multiple machines. Instead of designing custom
hardware for this purpose we propose to use a feature of
current Ethernet switches. Most switches are capable of
bundling a number of lower speed interfaces. Once the
bundle has been conﬁgured it can be used like a normal
interface. Therefore it can be used as a monitor inter-
face for any other interface. Accordingly, the switch for-
wards all trafﬁc that it receives on a, e. g., 10-Gigabit in-
terface on a bundle of, e. g., 1-Gigabit interfaces. These
lower speed interfaces can then be monitored individually.
Fig. 1 shows a schematic view of this setup. There are
three major drawbacks to this solution: ﬁrst, time stamps
are slightly distorted as the packets have to pass through an
additional switch; second, it is hard to synchronize the sys-
tem time of the monitors; third, even though the overall ca-
pacity sufﬁces the individual capacity of each link may not
be sufﬁcient. This depends on the load balancing scheme
in use.
10GigE
10 x 1GigE
Monitor
Fig. 1. Distr. Setup
We tested this mechanism by using a Cisco 3750 switch to move from monitoring
a single 1-Gigabit Ethernet link to eight (which is maximum for this vendor) 100-Mbit
links. We use the EtherChannel feature and conﬁgured such a link bundle across eight
100-Mbit links. Then this link is used as a monitor link for a 1-Gigabit input link.
210
F. Schneider, J. Wallerich, and A. Feldmann
It is crucial to use the appropriate load balancing method for this link. Common op-
tions include load balancing based on MAC addresses (simple switches), IP addresses
and/or MAC addresses (e. g., Cisco 3750), combinations of IP addresses and/or port
(TCP/UDP) numbers etc. (Cisco 6500 Series). Note that per packet multiplexing is not
a sensible option as this can lead to an uneven distribution across the monitors. Given
that it is common to use a switch for monitoring trafﬁc traveling between two routers
the MAC address variability is limited. There are usually only two MAC addresses in
use, one for each endpoint. Accordingly, we have to rely on at least source and destina-
tion IP addresses or better yet IP addresses and port numbers for load balancing which
unfortunately rules out the cheapest switches. Still, depending on the application one
may want to ensure that all packets from an IP address pair or a remote or local host are
handled by a single monitor. This ensures that all packets of a single TCP connection
arrive at the same monitor. With such a conﬁguration load balancing can be expected to
be reasonable as long as there are sufﬁciently many addresses that are monitored. If the
EtherChannel feature is not supported by a speciﬁc switch one should check if it offers
another way to bundle links. A good indication that a switch is capable of bundling is
the support of the Link Aggregation Control Protocol (LACP).
3 Inﬂuencing Parameters and Variables
Given that we now understand how we can distribute the trafﬁc from an 10-Gigbit
interfaces across multiple 1-Gigabit interfaces we next turn our attention to identify
suitable commodity hard- and software for the task of packet capturing. For this purpose
we identify the principal factors that may impact the performance and then examine the
most critical ones in detail.
Obvious factors include the network card, the memory speed, and the CPU cycle
rate. A slightly less obvious one is the system architecture: does it matter if the system
has multiple CPUs, multiple cores, or if hyper-threading is enabled? Another important
question is how the OS interacts with the network card and passes the captured packets
from the card to the application. How much data has to be copied in which fashion? How
many interrupts are generated and how are they handled? How much buffer is available
to avoid packet loss? Yet another question is if a 64-bit OS offers an advantage over the
32-bit versions. With regards to the application we ask by how much the performance is
degraded when running multiple capturing applications on the same network card, when
adding packet ﬁlters, when touching the data in user space, i. e., via a copy operation
or by compressing them, and when saving the data to a traceﬁle on disk. These are the
questions that we try to answer in the remainder of the paper.
4 Setup
Conducting such measurements may appear simple at ﬁrst glance, but the major difﬁ-
culty is to ﬁnd hard-/software conﬁgurations that are comparable. After all we want to
avoid comparing apples to oranges.
Packet Capture in 10-Gigabit Ethernet Environments
211
4.1 Hardware and Software
To enable a fair comparison, state-of-the-art off-the-shelf computer hardware was pur-
chased at the same time, February 2004, with comparable components. Preliminary
experiments have shown that the Intel Gigabit Ethernet cards provide superior results
than, e. g., those of Netgear. Furthermore, 2 Gbytes of RAM sufﬁce and we choose to
use the fastest available ones. Accordingly, we focus on the system architecture and the
OS while using the same system boards, the same amount of memory, and the disk sys-
tem. Consequently, we purchased two AMD Opteron and two Intel Xeon systems2 that
are equipped with the same optical network card, the same amount of memory, and the
same disk system (IDE ATA based). One Opteron and one Xeon system were installed
with FreeBSD 5.4 and the others were installed with Debian Linux (Kernel v2.6.11.x).
Once dual-core systems became available we got two additional machines in May
2006: HP Proliant DL3853. In contrast to the other systems these have an internal SCSI
Controller with three SCSI disk attached. One was installed with FreeBSD 6.1 and the
other with Debian Linux (Kernel v2.6.16.16), both with dual-boot for 32-bit and 64-bit.
4.2 Measurement Topology
Generator
generated traffic
1Gbit/s Fibre
Cisco C3500XL
To be able to test multiple systems at
the same time and under exactly the
same workload we choose the setup
shown in Fig. 2. A workload genera-
tor, see Sec. 4.4, generates trafﬁc which
is fed into a passive optical splitter.
The task of the splitter is to duplicate
the trafﬁc such that all systems under
test (SUT) receive the same input. The
switch between the workload genera-
tor and the splitter is used to check the
number of generated packets while all
capture applications running on the SUT’s are responsible for reporting their respective
capture rate. In later experiments we removed the switch as the statistics reported by the
trafﬁc generator itself proved to be sufﬁcient. The control network is a separate network
and is used to start/end the experiments and to collect the statistics.
Fig. 2. Measurement Topology
control net for measument setup and analysis 100Mbit/s TP 
FreeBSD
/AMD
FreeBSD
/Intel
Splitter
Linux
/AMD
Linux
/Intel
4.3 Methodology
Each measurement varies the trafﬁc rate from 50 Mbit/s to 920 Mbit/s4 and consists
of seven repetitions of sending one million packets at each bandwidth setting to reduce
2 System details: AMD Opteron 244, Intel Xeon 3.06Ghz, 2 Gbytes RAM (DDR-1 333 MHz),
Intel 82544EI Gigabit (Fiber) Ethernet Controller, 3ware Inc. 7000 series ATA-100 Storage
RAID-Controller with at least 450 Gbytes space.
3 Details see previous footnote except of processors (AMD Opteron 277), disk system (HP Smart
Array 64xx Controller with 3× 300 Gbytes U320 SCSI hard disks), and faster memory.
4 920Mbit/s is close to the maximum achievable theoretical input load given the per packet
header overheads.
212
F. Schneider, J. Wallerich, and A. Feldmann
statistical variations. In each iteration we start the capture process (using a simple libp-
cap [5] based application [12] or tcpdump) and a CPU usage proﬁling application on
the systems under study. Then we start the trafﬁc generator. Once all packets of the run
have been sent by the workload generator we stop the packet capture processes.
For each run we determine the capture rate, by counting the received number of
packets, and the average CPU usage (see cpusage [12]). The results are then used
to determine the mean capturing rate as well as the mean CPU usage for each data
rate.
4.4 Workload
With regards to the generated trafﬁc we decided to neither focus on the simplest case for
packet monitoring (all large packets) nor on the most difﬁcult one (all small packets) as
both are unrealistic. The ﬁrst case puts too much weight on the system bus performance
while the latter one emphasizes the interrupt load too much.
Instead we decided to use a trafﬁc generator that is able to generate a packet size
distribution that closely resembles those observed in actual high-speed access networks.
For this purpose we enhanced the Linux Kernel Packet Generator (LKPG [13]), which
is capable of ﬁlling a 1-Gigabit link, to generate packets according to a given packet
size distribution.
109
106
105
108
107
f
o
r
e
b
m
u
n
s
t
e
k
c
a
p
originally captured
generated
Figure 3 shows the
input and the gener-
ated packet size distri-
butions of our modiﬁed
LKPG based on a 24h
packet level trace cap-
tured at the 1-Gigabit
uplink of the MWN [1]
(peaks are at 40–64,
576 and 1420–1500 B).
We decided to not
mimic ﬂow size dis-
tribution,
application
layer protocol mix, de-
lay and jitter, or data
content, as they have
no direct
impact on
the performance of the
capture system. Their
impact is on the appli-
cation which is beyond the scope of this paper. The same holds for throughput bursts,
which can be buffered. The intention of the work is to identify the maximum throughput
a capturing system can handle. We realize different trafﬁc rates by inserting appropriate
inter-packet gaps between the generated packets.
Fig. 3. Packet sizes (sorted by rank) vs. frequency of the packets
(y-axis in logscale)
packet size (sorted by rank)
104
103
102
101
 1e+06
 1e+06
 1e+08
 1e+08
 1e+07
 1e+07
Packet Capture in 10-Gigabit Ethernet Environments
213
5 Results
We now use the above setup to evaluate the performance of our systems starting from
the vanilla kernels. We ﬁnd that it is crucial to increase the amount of memory that is
available to the kernel capture engine. Otherwise scheduling delays, e. g., to the captur-
ing application, can cause the overﬂow of any of the kernel queues, e. g., the network
queue for the capture socket. This is achieved by either setting the debug.bpf bufsize
sysctl parameter (FreeBSD) or the /proc/sys/net/core/rmem* parameter (Linux).
Based upon our experience we chose to use buffers of 20 Mbytes. Furthermore, we no-
ticed that the systems spend most of their cycles on interrupt processing when capturing
at high packet rates. To overcome this problem we tried device polling as suggested by
Mogul et al. [4]. For Linux this reduces the CPU cycles that are spent in kernel mode
and thus increases the packet capture rate signiﬁcantly. For FreeBSD activating the
polling mechanism slightly reduced the capturing performance and the stability of the
system. Therefore we use it for Linux but not for FreeBSD.
Next we baseline the impact of the system architecture and the OS by comparing the
performance of the four systems with single processor kernels. Figure 4 (top) shows the
capture rate while Fig. 4 (bottom) shows the CPU usage as we increase the data rate.
To keep the plots simple we chose to not include the standard deviation. Note that all
measurements have a standard deviation of less than 2%. As expected the capture rate
decreases while the CPU usage increases as the data rate is increased. Unfortunately,
only the FreeBSD/AMD combination loses almost no packets. All others experience
signiﬁcant packet drops. We note that the systems start dropping packets once their CPU
utilization reaches its limits. This indicates that the drops are not due to missing kernel
buffers but are indeed due to missing CPU cycles. The FreeBSD/Intel combination
already starts dropping packets at about 500 Mbit/s. Neither of the Linux systems looses
packets until roughly 650 Mbit/s. From that point onward their performance deteriorates
)
e
t
a
R
e
r
u
t
p
a
C
(
t
e
g
a
n
e
c
r
e
P
 100
 90
 80
 70
 60