The most popular facial model reconstruction ap-
proaches can be categorized into three classes: shape
from shading (SFS), structure from motion (SFM) com-
bined with dense stereoscopic depth estimation, and sta-
tistical facial models. The SFS approach [24] uses a
model of scene illumination and reflectance to recover
face structure. Using this technique, a 3D facial model
can be reconstructed from only a single input photo. SFS
relies on the assumption that the brightness level and gra-
dient of the face image reveals the 3D structure of the
face. However, the constraints of the illumination model
used in SFS require a relatively simple illumination set-
ting and, therefore, cannot typically be applied to real-
world photo samples, where the configuration of the light
sources is unknown and often complicated.
As an alternative, the structure from motion approach
[12] makes use of multiple photos to triangulate spatial
positions of 3D points.
It then leverages stereoscopic
techniques across the different viewpoints to recover the
complete 3D surface of the face. With this method, the
reconstruction of a dense and accurate model often re-
quires many consistent views of the surface from differ-
ent angles; moreover, non-rigid variations (e.g., facial ex-
pressions) in the images can easily cause SFM methods
to fail. In our scenario, these requirements make such an
approach less usable: for many individuals, only a lim-
ited number of images might be publicly available on-
line, and the dynamic nature of the face makes it difficult
to find multiple images having a consistent appearance
(i.e., the exact same facial expression).
Unlike SFS and SFM, statistical facial models [4, 43]
seek to perform facial reconstruction on an image using
a training set of existing facial models. The basis for this
type of facial reconstruction is the 3D morphable model
(3DMM) of Blanz and Vetter [6, 7], which learns the
principal variations of face shape and appearance that
occur within a population, then fits these properties to
images of a specific face. Training the morphable mod-
els can be performed either on a controlled set of im-
ages [8, 39] or from internet photo-collections [23]. The
underlying variations fall on a continuum and capture
both expression (e.g., a frowning-to-smiling spectrum)
and identity (e.g., a skinny-to-heavy or a male-to-female
spectrum). In 3DMM and its derivatives, both 3D shape
and texture information are cast into a high-dimensional
linear space, which can be analyzed with principal com-
ponent analysis (PCA) [22]. By optimizing over the
weights of different eigenvectors in PCA, any particu-
lar human face model can be approximated. Statistical
facial models have shown to be very robust and only re-
quire a few photos for high-precision reconstruction. For
instance, the approach of Baumberger et al. [4] achieves
good reconstruction quality using only two images.
To make the process fully automatic, recent 3D fa-
cial reconstruction approaches have relied on a few fa-
cial landmark points instead of operating on the whole
model. These landmarks can be accurately detected us-
ing the supervised descent method (SDM) [59] or deep
convolutional networks [50]. By first identifying these
2D features in an image and then mapping them to points
in 3D space, the entire 3D facial surface can be effi-
ciently reconstructed with high accuracy. In this process,
the main challenge is the localization of facial landmarks
within the images, especially contour landmarks (along
the cheekbones), which are half-occluded in non-frontal
views; we introduce a new method for solving this prob-
lem when multiple input images are available.
The end result of 3D reconstruction is a untextured
(i.e., lacking skin color, eye color, etc.) facial surface.
Texturing is then applied using source image(s), creating
a realistic final face model. We next detail our process for
building such a facial model from a user’s publicly avail-
able internet photos, and we outline how this model can
be leveraged for a VR-based face authentication attack.
3 Our Approach
A high-level overview of our approach for creating a syn-
thetic face model is shown in Figure 1. Given one or
more photos of the target user, we first automatically ex-
tract the landmarks of the user’s face (stage ). These
landmarks capture the pose, shape, and expression of the
user. Next, we estimate a 3D facial model for the user,
optimizing the geometry to match the observed 2D land-
marks (stage ). Once we have recovered the shape of
the user’s face, we use a single image to transfer texture
information to the 3D mesh. Transferring the texture is
non-trivial since parts of the face might be self-occluded
500  25th USENIX Security Symposium 
USENIX Association
Figure 1: Overview of our proposed approach.
(e.g., when the photo is taken from the side). The tex-
ture of these occluded parts must be estimated in a man-
ner that does not introduce too many artifacts (stage ).
Once the texture is filled, we have a realistic 3D model
of the user’s face based on a single image.
However, despite its realism, the output of stage  is
still not able to fool modern face authentication systems.
The primary reason for this is that modern face authenti-
cation systems use the subject’s gaze direction as a strong
feature, requiring the user to look at the camera in order
to pass the system. Therefore, we must also automati-
cally correct the direction of the user’s gaze on the tex-
tured mesh (stage ). The adjusted model can then be de-
formed to produce animation for different facial expres-
sions, such as smiling, blinking, and raising the eyebrows
(stage ). These expressions are often used as liveness
clues in face authentication systems, and as such, we
need to be able to automatically reproduce them on our
3D model. Finally, we output the textured 3D model into
a virtual reality system (stage ).
Using this framework, an adversary can bypass both
the face recognition and liveness detection components
of modern face authentication systems. In what follows,
we discuss the approach we take to solve each of the var-
ious challenges that arise in our six-staged process.
3.1 Facial Landmark Extraction
Starting from multiple input photos of the user, our first
task is to perform facial landmark extraction. Follow-
ing the approach of Zhu et al. [63], we extract 68 2D
facial landmarks in each image using the supervised de-
scent method (SDM) [59]. SDM successfully identifies
facial landmarks under relatively large pose differences
(±45deg yaw, ±90deg roll, ±30deg pitch). We chose
the technique of Zhu et al. [63] because it achieves a me-
dian alignment error of 2.7 pixels on well-known datasets
[1] and outperforms other commonly used techniques
(e.g., [5]) for landmark extraction.
Figure 2: Examples of facial landmark extraction
For our needs, SDM works well on most online im-
ages, even those where the face is captured at a low res-
olution (e.g., 40 × 50 pixels). It does, however, fail on
a handful of the online photos we collected (less than
5%) where the pose is beyond the tolerance level of the
algorithm. If this occurs, we simply discard the image.
In our experiments, the landmark extraction results are
manually checked for correctness, although an automatic
scoring system could potentially be devised for this task.
Example landmark extractions are shown in Figure 2.
3D Model Reconstruction
3.2
The 68 extracted 3D point landmarks from each of the
N input images provide us with a set of coordinates
si, j ∈ R2, with 1 ≤ i ≤ 68,1 ≤ j ≤ N. The projection of
the 3D points Si, j ∈ R3 on the face onto the image coor-
dinates si, j follows what is called the “weak perspective
projection” (WPP) model [16], computed as follows:
(1)
where f j is a uniform scaling factor; P is the projection
si, j = f jPR j (Si, j +t j) ,
matrix(cid:31)1 0 0
0 1 0(cid:30); R j is a 3× 3 rotation matrix defined by
USENIX Association  
25th USENIX Security Symposium  501
the pitch, yaw, and roll, respectively, of the face relative
to the camera; and t j ∈ R3 is the translation of the face
with respect to the camera. Among these parameters,
only si, j and P are known, and so we must estimate the
others.
Fortunately, a large body of work exists on the shape
statistics of human faces. Following Zhu et al. [63],
we capture face characteristics using the 3D Morphable
Model (3DMM) [39] with an expression extension pro-
posed by Chu et al. [9]. This method characterizes varia-
tions in face shape for a population using principal com-
ponent analysis (PCA), with each individual’s 68 3D
point landmarks being concatenated into a single feature
vector for the analysis. These variations can be split into
two categories: constant factors related to an individual’s
distinct appearance (identity), and non-constant factors
related to expression. The identity axes capture charac-
teristics such as face width, brow placement, or lip size,
while the expression axes capture variations like smiling
versus frowning. Example axes for variations in expres-
sion are shown in Figure 6.
More formally, for any given individual, the 3D coor-
Si, j = ¯Si + Aid
i αid + Aexp
,
dinates Si, j on the face can be modeled as
i αexp
j
(2)
where ¯Si is the statistical average of Si, j among the in-
dividuals in the population, Aid
is the set of principal
i
axes of variation related to identity, and Aexp
is the set
of principal axes related to expression. αid and αexp
are
the identity and expression weight vectors, respectively,
that determine person-speciﬁc facial characteristics and
expression-specific facial appearance. We obtain ¯Si and
i using the 3D Morphable Model [39] and Aexp
Aid
from
Face Warehouse [8].
i
j
i
given each identified facial landmark si, j in the input im-
age, we need to find the corresponding 3D point Si(cid:30), j
on the underlying face model. For landmarks such as
the corners of the eyes and mouth, this correspondence
is self-evident and consistent across images. However,
for contour landmarks marking the edge of the face in
an image, the associated 3D point on the user’s facial
model is pose-dependent: When the user is directly fac-
ing the camera, their jawline and cheekbones are fully in
view, and the observed 2D landmarks lie on the fiducial
boundary on the user’s 3D facial model. When the user
rotates their face left (or right), however, the previously
observed 2D contour landmarks on the left (resp. right)
side of the face shift out of view. As a result, the observed
2D landmarks on the edge of the face correspond to 3D
points closer to the center of the face. This 3D point dis-
placement must be taken into account when recovering
the underlying facial model.
Qu et al. [44] deal with contour landmarks using con-
straints on surface normal direction, based on the obser-
vation that points on the edge of the face in the image
will have surface normals perpendicular to the viewing
direction. However, this approach is less robust because
the normal direction cannot always be accurately esti-
mated and, as such, requires careful parameter tuning.
Zhu et al. [63] proposed a “landmark marching” scheme
that iteratively estimates 3D head pose and 2D contour
landmark position. While their approach is efficient and
robust against different face angles and surface shapes,
it can only handle a single image and cannot refine the
reconstruction result using additional images.
Our solution to the correspondence problem is to
model 3D point variance for each facial landmark using a
pre-trained Gaussian distribution (see Appendix A). Un-
like the approach of Zhu et al. [63] which is based on
single image input, we solve for pose, perspective, ex-
pression, and neutral-expression parameters over all im-
ages jointly. From this, we obtain a neutral-expression
model Si of the user’s face. A typical reconstruction, Si,
is presented in Figure 4.
Figure 3: Illustration of identity axes (heavy-set to thin) and
expression axes (pursed lips to open smile).
When combining Eqs. (1) and (2), we inevitably run
into the so-called “correspondence problem.” That is,
Figure 4: 3D facial model (right) built from facial landmarks
extracted from 4 images (left).
502  25th USENIX Security Symposium 
USENIX Association
3.3 Facial Texture Patching
Given the 3D facial model, the next step is to patch the
model with realistic textures that can be recognized by
the face authentication systems. Due to the appearance
variation across social media photos, we have to achieve
this by mapping the pixels in a single captured photo
onto the 3D facial model, which avoids the challenges
of mixing different illuminations of the face. However,
this still leaves many of the regions without texture, and
those untextured spots will be noticeable to modern face
authentication systems. To fill these missing regions, the
naïve approach is to utilize the vertical symmetry of the
face and fill the missing texture regions with their sym-
metrical complements. However, doing so would lead
to strong artifacts at the boundary of missing regions. A
realistic textured model should be free of these artifacts.
To lessen the presence of these artifacts, one approach
is to iteratively average the color of neighboring vertices
as a color trend and then mix this trend with texture de-
tails [45]. However, such an approach over-simplifies
the problem and fails to realistically model the illumina-
tion of facial surfaces. Instead, we follow the suggestion
of Zhu et al. [63] and estimate facial illumination using
spherical harmonics [61], then fill in texture details with
Poisson editing [41]. In this way, the output model will
appears to have a more natural illumination. Sadly, we
cannot use their approach directly as it reconstructs a pla-
nar normalized face, instead of a 3D facial model, and so
we must extend their technique to the 3D surface mesh.
The idea we implemented for improving our initial
textured 3D model was as follows: Starting from the
single photo chosen as the main texture source, we first
estimate and subsequently remove the illumination con-
ditions present in the photo. Next, we map the textured
facial model onto a plane via a conformal mapping, then
impute the unknown texture using 2D Poisson editing.
We further extend their approach to three dimensions and
perform Poisson editing directly on the surface of the fa-
cial model. Intuitively, the idea behind Poisson editing
is to keep the detailed texture in the editing region while
enforcing the texture’s smoothness across the boundary.
This process is defined mathematically as
∆ f = ∆g,s.t f|∂Ω = f 0|∂Ω,
(3)
where Ω is the editing region, f is the editing result, f 0
is the known original texture value, and g is the texture
value in the editing region that is unknown and needs to
be patched with its reflection complement. On a 3D sur-
face mesh, every vertex is connected with 2 to 8 neigh-
bors. Transforming Eq. 3 into discrete form, we have
|Np| fp − ∑
q∈Np∩Ω
fq = ∑
q∈Np∩Ω
f 0
q + (∆g)p,
(4)
where Np is the neighborhood of point p on the mesh.
Our enhancement is a natural extension of the Poisson
editing method suggested in the seminal work of Pérez
et al. [41], although no formulation was given for 3D.
By solving Eq. 4 instead of projecting the texture onto a
plane and solving Eq. 3, we obtain more realistic texture
on the facial model, as shown in Figure 5.
Figure 5: Naïve symmetrical patching (left); Planar Poisson
editing (middle); 3D Poisson editing (right).
3.4 Gaze Correction
We now have a realistic 3D facial model of the user.
Yet, we found that models at stage  were unable to
bypass most well-known face recognition systems. Dig-
ging deeper into the reasons why, we observed that most
recognition systems rely heavily on gaze direction during
authentication, i.e., they fail-close if the user is not look-
ing at the device. To address this, we introduce a simple,
but effective, approach to correct the gaze direction of
our synthetic model (Figure 1, Stage ).
The idea is as follows. Since we have already re-
constructed the texture of the facial model, we can syn-
thesize the texture data in the eye region. These data
contain the color information from the sclera, cornea,