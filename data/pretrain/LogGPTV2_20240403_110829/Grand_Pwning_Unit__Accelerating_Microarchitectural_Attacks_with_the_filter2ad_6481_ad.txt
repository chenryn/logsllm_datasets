First, we brieﬂy discuss the DRAM architecture. We then
describe how we are able to build efﬁcient eviction sets
to bypass two levels of GPU caches to reach DRAM. We
continue by explaining how we manage to obtain contiguous
memory allocations and ﬁnally we show how, by exploiting
our timing side channel, we are able to detect these allocations.
A. DRAM architecture
DRAM chips are organized in a structure of channels,
DIMMs, ranks, banks, rows and columns. Channels allow
parallel memory accesses to increase the data transfer rate.
Each channel can accommodate multiple Dual In-line Memory
Modules (DIMMs). These modules are commonly partitioned
in either one or two ranks which usually correspond to the
physical front and back of the DIMM. Each rank is then
divided into separate banks, usually 8 in DDR3 chips. Finally
every bank contains the memory array that is arranged in rows
and columns.
DRAM performs reads at row granularity. This means that
fetching a speciﬁc word from DRAM activates the complete
row containing that word. Activation is the process of ac-
cessing a row and storing it in the row buffer. If the row
is already activated, a consecutive access to the same row
will read directly from the row buffer causing a row hit.
On the other hand, if a new row gets accessed, the current
row residing in the buffer needs to be restored in its original
location before loading the new one (row conﬂict [40]). We
rely on this timing difference for detecting contiguous regions
of physical memory as we discuss in Section VII-D.
B. Cache Eviction
Considering the GPU architecture presented in Section VI,
the main obstacles keeping us from accessing the DRAM from
the GPU is two levels of caches. Therefore, we need to build
efﬁcient eviction strategies to bypass these caches. From now
on we will use the notation v[off ] to describe memory access
to a speciﬁc offset from the start of an array v in the virtual
address space.
Set-associative caches require us to evict just the set contain-
ing the address v[i], if we want to access v[i] from memory
again. Having a FIFO replacement policy allows us to evict
the ﬁrst cacheline loaded into the set by simply accessing a
new address that will map to the same cache set. A UCHE set
can store 8 cachelines located at 4 KB of stride (i.e., v[4K×i]
as shown in Figure5a). Hence, if we want to evict the ﬁrst
cacheline, we need at least 8 more memory accesses to evict it
from UCHE (Figure 5b). In a common scenario with inclusive
caches, this would be enough to perform a new DRAM access.
In these architectures, in fact, an eviction from the Last Level
Cache (LLC) removes such cacheline from lower level caches
as well. However, the non-inclusive nature of the GPU caches
neutralizes this approach.
To overcome this problem we can exploit the particularities
in the architecture of these 2 caches. We explained in Sec-
tion VI-B2 that a UCHE cacheline contains 4 different L1
cachelines and that two addresses v[64×i] and v[64×i+32]
map to two different cachelines into the same L1 set (Figure 4).
As a result, if cacheline at v[0] was stored in the UCHE and
was already evicted, we can load it again from DRAM by ac-
cessing v[0+32]. By doing so we simultaneously load the new
v[0+32] cacheline into L1 (Figure 5c). This property allows
us to evict both of the caches by alternating these 9 memory
203
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:36:05 UTC from IEEE Xplore.  Restrictions apply. 
JM?@MN
»
¼
½
¾
¿
À
Á
»
¼
¾
¼
¾
¾
¾
Á
Á
Á
Á
Á
Á
Á
Á
Á
Á
Á
Á
>@NNJM?@M
KLW
FRQIOLFW
<
¿
¿
¿
¿
¾
¿
¾
¿
¾
¿
¾
¿
E
¾
¾
¿
¿
<IFÁ
¾
¾
¿
¿
<IFÂ
<IF» <IF¼
F
=
<IFÁ
<IFÂ
Fig. 6: The diagrams show how we can force the buddy allocator into providing us with contiguous physical memory and how
we can detect this contiguous areas using our hit-pattern. (a) shows the buddy allocator keeping track of available memory in
its free_lists. (b) shows the process of allocating 2 arrays namely a and b of respectively 15 and 40 pages, and the result
of this process on the buddy’s free_lists. (c) shows how our hit-pattern detects the contiguous memory backing b.
accesses between v[4K×i] and v[4K×i+32] (Figure 5d). Our
access patterns will exploit this to be completely oblivious of
L1. As a consequence, from now on we will simply mention
accesses to addresses v[4K×i]. Nonetheless, every time we
will use this notation it
it
implicitly conceals both of the accesses.
to remember that
is important
C. Allocating contiguous memory
Before discussing how the Adreno GPUs allocates memory,
we need to explain the relationship between physical memory
contiguity and DRAM adjacency.
Contiguity & Adjacency: In order to carry out a reliable
Rowhammer attack, we need three adjacent rows inside a
DRAM bank. It is important to understand that adjacency (cid:4)=
contiguity. The memory controller decides where to store
the data on a DRAM location based on the given physical
address. Pessl et al. [40] reversed engineered the mapping
between physical addresses and DRAM locations for the Snap-
dragon 800/801 chipsets. For simplicity, we adopt a simpliﬁed
DRAM model and assume contiguity (cid:2) adjacency, but the
interested readers can ﬁnd how we relax this assumption in
Appendix A using the information in [40]. In the Snapdragon
800 each row n stores two consecutive pages (i.e., 8 KB). With
2 pages per row and 8 banks within the module, rows n and
n + 1 are 16 pages apart.
it
The
buddy
The buddy allocator: The Adreno 330 GPU operates on
virtual addresses due to the presence of an IOMMU. This
means that
is capable of dealing with physically non-
contiguous memory and it allows the GPU driver to allocate
it accordingly. The Adreno android kernel driver allocates
memory using the alloc_page() macro which queries the
buddy allocator for single pages [33].
allocator manages
in
free_lists containing chunks of power-of-two number
of available pages [17]. The exponent of this expression is
known as the order of the allocation. Figure 6a shows these
free_lists. When the buddy allocator receives a request
for a block of memory, it tries to satisfy that allocation from
the smallest possible orders. Figure 6b shows an example
of such process. We want to allocate two buffers, namely a
with 15 pages and b with 40 pages. We start by allocating
a. alloc_page() asks for pages one by one (i.e., order 0
allocations). The order 0 free_list contains one single
page. Therefore,
the ﬁrst allocation makes it empty. The
free memory
following page then needs to come from order 1 (i.e., 21
contiguous pages). This means that buddy needs to split the
block in two separate pages and return one back to the buffer
while storing the other one in the order 0 free_list.
This process is repeated for every requested page and can be
executed for every order n < MAX_ORDER. That is, if no
block of order n is vacant, a block from the next ﬁrst available
order (i.e., n + k) is recursively split in two halves until order
n is reached. These halves are the so-called buddies and all
the allocated memory coming from order n + k is physically
contiguous. As a consequence, considering our example in
Figure 6b array a will be served by blocks of order 0, 1 and
3, while b by a single block of order 6, since all the small
orders are exhausted.
We use this predictable behavior of the buddy allocator for
building our primitive P3. Due to our precondition of 3 adja-
cent rows to perform a reliable Rowhammer attack, we there-
fore require an allocation of order (cid:7)log2(16pages × 3row)(cid:8) = 6.
D. Detecting contiguous memory
Now that we know that we can force the buddy allocator into
providing us with contiguous memory necessary to perform
our Rowhammer attack, we require a tool to detect these
contiguous areas. Our tool is a timing side-channel attack.
We introduce a side channel that measures time differences
between row hits and row conﬂicts in order to deduce infor-
mation about the order of the allocations.
To distinguish between contiguous and non-contiguous al-
locations, we can either test for row conﬂicts or row hits. In
Figure 6b, we allocated array b of 40 pages from an order six
allocation that spans over four full rows. In our example, a
full row is 64 KB of contiguous physical memory that maps
to the same row n over the different banks. It would be
intuitive to exploit the row conﬂicts to detect memory located
in adjacent rows. For example, accessing b[0] and b[64 K]
generates a row conﬂict since b is backed by physically-
contiguous memory. However, this solution is limited due to
the way buddy allocator works. We previously explained that
to obtain a block of order n+1 from buddy we need to exhaust
every n-order block available. This implies that allocations
of order n are likely to be followed by other allocations of
the same order. Since every allocation of order ≥ 4 spans
over a full row, every access to allocations coming from these
orders following the v[64K×i] pattern will always generate
row conﬂicts. At the same time, allocations of order < 4 are
204
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:36:05 UTC from IEEE Xplore.  Restrictions apply. 
also likely to generate conﬂicts since the blocks in the buddy’s
free_lists are not predictable (Figure 6b). To address this
problem, we detect blocks of order ≥ 4 by testing for row hits
instead. This allows us to obtain the same granularity while
achieving less noisy measurements.
This access pattern, which we call hit-pattern, touches 15
virtually-contiguous pages. To extract a single measurement
we repeatedly iterate over it in order to minimize the noise. In
Figure 6c we show how the hit-pattern behaves when touching
pages belonging to arrays a and b. As you can see, sequential
accesses over pages of b generate only row hits (green pages)
while the same access pattern over a can arbitrarily generate
row conﬂicts (red pages) or row hits depending on the backing
allocations.
We limit our hit-pattern to 15 pages instead of the 16 pages
of a full row because of the unknown physical alignment of
our ﬁrst access v[0]. For instance, in Figure 6c, we set our
v[0]=b[13] and, as you can see, v[4K×(16)](=b[29]) generates
a row conﬂict since v[0] is not row-aligned.
E. Results
We evaluate our side channel to show how it can detect allo-
cation trends of the buddy allocator. To obtain these measure-
ments, we employ the TIME_ELAPSED_EXT asynchronous
timer presented in Section V. We run the hit-pattern for v[0]
equal to every page within 512 KB areas. After collecting all
these measurements, we use their median value to maximize
the number of row conﬂicts for allocations of order < 4 while
ﬁltering out the noise from those of order ≥ 4. .
Figure 7 shows the mean access time over the allocation
order. Allocations of order ≥ 4 have a lower median and
are less spread compared to allocations of order < 4. Due
to the deterministic replacement policy of the GPU caches,
we can see how the measurements have very little noise for
allocation of order ≥ 4. While the granularity of our side
channel is limited to order 4, this still provides us with valuable
information regarding the allocation trend that allows us to
infer the current status of the allocator. This makes it possible
to heuristically identify the order of the underlying allocations.