and the information of net values for some features (e.g., net
modiﬁed functions) will be reserved.
Then, we can calculate the Euclidean distance dmn between
the m-th security patch and the n-th wild patch, so as to build
a weighted distance matrix D = {dmn}M×N , where M and
N refer to the security patch number and wild patch number.
3) Nearest Link Optimization: The goal of the nearest link
search is to ﬁnd a wild patch for each veriﬁed security patch so
that the total distance of all pairs would be a minimum. A pair
of one security and one non-security patch is also called a link,
which presents the selected wild patch has a high similarity
with the veriﬁed security patch. Thus, the set of the selected
wild patches becomes our candidate set. Because one wild
patch can only be linked to up to one security patch, the size
of the candidate patches is the same as the size of the veriﬁed
security patches.
As shown in Figure 3, the candidate patches are a set
of patches located by the nearest link and they have simi-
lar features with veriﬁed security patches. Therefore, these
candidates have a higher probability of being veriﬁed as
security patches compared with other wild patches. After
human veriﬁcation, some candidates would be labeled as a
security patch and added to the existing security patch dataset.
Other candidates would be added to the non-security patch
dataset. In other words, the size of the veriﬁed security patches
will keep increasing when new security patches are identiﬁed
from the wild. If a candidate is veriﬁed as a non-security patch,
this instance could also increase the discernible capability of
the patch classiﬁer near the decision boundary. That is because
this patch link crosses the decision boundary and reveals the
data distribution around the region as well.
To compute the nearest link, we convert the candidate search
problem into an optimization problem. The optimization ob-
jective is to minimize the sum of the distances in each link,
as shown in the following formula:
M(cid:2)
m=1
min
dmcm , s.t. cm ∈ Z ∩ [1, N ], c1 (cid:4)= c2 (cid:4)= .. (cid:4)= cM .
where dmcm refers to the distance of the m-th patch link, and
cm is the index of the wild patch that is linked to the m-th
veriﬁed security patch. The optimization problem is to obtain
the set {cm}M
m=1 with M different elements to minimize
the total link distance. It is similar to the Kuhn–Munkres
(KM) algorithm [12] that seeks the combinatorial optimization
in the assignment problem, hence it
is hard to ﬁnd the
globally optimal solutions. To solve this problem, we adopt
an approximately optimal solution with a greedy algorithm.
The algorithm is illustrated in Algorithm 1, where the time
complexity is O(M N 2). Note that our nearest link search
is different from the k-nearest neighbors (KNN) algorithm
where K candidates will be selected according to one veriﬁed
sample and one candidate may be assigned to multiple veriﬁed
samples even if K = 1. In the nearest link, each candidate can
only be selected at most once, and each selected candidate will
be paired with one individual veriﬁed sample.
C. Generating Synthetic Dataset via Oversampling
It is known that when developing learning based patch
analysis models, the training phase may face two challenges,
namely, the model over-ﬁtting problem due to insufﬁcient
m=1
n=1
n=1
Algorithm 1 The Nearest Link Search Algorithm
Input: the weighted distance matrix D = {dmn}M×N
Output: the index set for selected wild patches {cm}M
1: / ∗ init the minimum index ∗ /
2: U = {u1, u2, ..., uM}, um = min{dmn}N
3: V = {v1, v2, ..., vM}, vm = argminn{dmn}N
4: / ∗ f ind the index ∗ /
5: C = {c1 = 0, c2 = 0, ..., cM = 0}
6: for i ← 1 to M do
m0 ← argmin U
n0 ← vm0
/ ∗ if n0 has been used ∗ /
if n0 ∈ C then
l = {dm0n}N
for j ← 1 to M do
if cj (cid:4)= 0 then
← inf
n0 ← argmin l
← n0
← inf
7:
8:
9:
10:
11:
12:
13:
14:
lcj
n=1
15:
16:
17:
18: output C
cm0
um0
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:12 UTC from IEEE Xplore.  Restrictions apply. 
153
BEFORE
Source Code
Source Code
LLVM
Ҫ
Search
ҫ
Modify
Ҭ
BEFORE
IF Statements
IF Statements
Involved in Patches
Involved in Patches
BEFORE
Abstract 
Abstract 
Syntax Tree
Syntax Tree
Patches
Patches
AFTER
Source Code
Source Code
LLVM
Ҫ
Search
ҫ
Modify
Ҭ
AFTER
IF Statements
IF Statements
Involved in Patches
Involved in Patches
AFTER
Abstract 
Abstract 
Syntax Tree
Syntax Tree
W !
E
N
BEFORE
Source Code
Source Code
NEW!
Patches
Patches
W !
E
N
AFTER
Source Code
Source Code
Fig. 4: The overview of oversampling at source code level.
training instances and the misleading fortuitous patterns. To
alleviate these problems, some artiﬁcial code gadget datasets
(e.g., SARD [8]) are adopted during the training [22], [23].
However, since patches are not complete program units and
contain both pre-patched and post-patched code at the same
time, previous code synthesis algorithm [10] cannot be ap-
plied to create artiﬁcial patches. Therefore, we propose a
new oversampling algorithm to generate artiﬁcial patches at
the source code level, which is different from traditional
oversampling [11] that synthesizes instances in the feature
space. We provide more interpretability since vector instances
generated by traditional methods cannot be transformed back
to the source code patch. As around 70% security patches
involve modiﬁcations that add or update conditional statements
(i.e., if statements) [24], we focus on enriching the control
ﬂow variants of natural patches. Note that we refer to real
patches as natural patches to distinguish from synthetic ones.
Figure 4 depicts the overview of our oversampling method,
which contains three steps. First, for a given patch, we generate
the Abstract Syntax Trees (ASTs) from its related source code
ﬁles. Second, among these ASTs, we locate the if statements
involved with code changes in the patch. Third, given existing
patches, we transform their if statements according to a set of
predeﬁned variant templates in order to get the corresponding
artiﬁcial patches. We detail each step in the following.
1) Generating ASTs from Patches: Since the patch is a
bunch of differences between two versions of ﬁles, it is not
a complete top-level program unit and some related portions
may be missing. Therefore, we cannot directly generate ASTs
from patches. Instead, for each patch, we retrieve the related
ﬁles before and after applying the target patch so that these
corresponding ﬁles can be parsed. Since we have downloaded
all
the repositories associated with our patch dataset and
each patch can be uniquely identiﬁed with its commit hash
value, we can easily roll back the corresponding repository to
the point just before and after committing the target patch.
Furthermore, we can easily ﬁnd out the patch-related ﬁles
that are listed in lines that start with diff --git. Then, we use
LLVM [4] to generate the ASTs for these ﬁles.
2) Locating Conditional Statements: The goal of this step
is to locate all the if statements related to the patch, i.e., if
statements that are added, deleted, or modiﬁed by the patch.
The if statements can be located by utilizing the IfStmt
 ﬁeld in the AST ﬁles. From the
ASTs, we can retrieve the key information of if statements,
such as the start line, end line, and the internal structure. In
the next step, our transformation would focus on these if
statements since they are more likely to embed critical changes
of security patches.
3) Adding Control Flow Variants:
Instead of modifying
both the BEFORE version and AFTER version at the same
time, we can modify one of these two versions and generate
patch variants. When we modify the AFTER version source
code, it is equivalent to adding some additional modiﬁcations
to the AFTER version code. In that case, when consider-
ing the patch variant, it is equivalent to the patch between
the original BEFORE version and the new AFTER version
(i.e., original AFTER version plus additional modiﬁcations).
Therefore, we only need to merge the original patch and the
additional modiﬁcations. Similarly, when we only change the
BEFORE version source code, it is equivalent to adding some
additional modiﬁcations to the original BEFORE version code.
Therefore, the patch variant is equivalent to the merge of the
inverse additional modiﬁcations and the original patch.
 const int _SYS_ZERO = 0;
 if (_SYS_ZERO || IF_STMT) {} 
 const int _SYS_ONE = 1;
 if (_SYS_ONE && IF_STMT) {} 
 bool _SYS_STMT = IF_STMT;
 if (True == _SYS_STMT) {}
 bool _SYS_STMT = !(IF_STMT);
 if (!_SYS_STMT) {}
 int _SYS_VAL = 0;
 if (IF_STMT){_SYS_VAL = 1;}
 if (_SYS_VAL) {}
 int _SYS_VAL = 1;
 if (IF_STMT){_SYS_VAL = 0;}
 if (!_SYS_VAL) {}
 int _SYS_VAL = 0;
 if (IF_STMT){_SYS_VAL = 1;}
 if (_SYS_VAL && IF_STMT) {}
 int _SYS_VAL = 1;
 if (IF_STMT){_SYS_VAL = 0;}
 if (!_SYS_VAL || IF_STMT) {}
Fig. 5: Eight different variants of IF statements.
As shown in Figure 5, we apply eight types of variants
on if statements to generate the synthetic patches. The
statements in normal font are control ﬂow related contents in
natural patches, and the statements in bold are ﬂow variants
added to generate artiﬁcial patches. By introducing control
ﬂow complexity to natural patches, the synthetic dataset can
enrich representations of patches and alleviate the over-ﬁtting
of the learning model, improving the performance of automatic
patch analysis tasks.
IV. EVALUATION
Based on the methodology introduced in Section III, we
collect a large-scale security patch dataset consisting of NVD-
based patches, wild-based patches, and synthetic patches.
We conduct a set of experimental studies to investigate the
effectiveness of the proposed algorithms and evaluate the
properties of the collected dataset. Speciﬁcally, our evaluation
aims to answer the following ﬁve research questions (RQs).
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:12 UTC from IEEE Xplore.  Restrictions apply. 
154
• RQ1: How to efﬁciently construct the wild-based security
patch dataset using the nearest link search approach?
• RQ2: What is the performance of the nearest link search
approach compared with the state-of-the-art work?
• RQ3: How useful are the synthetic security patches?
• RQ4: What is the composition of our collected dataset?
• RQ5: What is the quality of PatchDB?
A. Wild-based Dataset Construction (RQ1)
Based on the NVD-based dataset that includes 4076 security
patches, we construct the wild-based security patch dataset by
identifying security patches from the wild using the proposed
nearest link search and human-in-the-loop approaches. Since
it is difﬁcult to download all commits of every GitHub reposi-
tory, we focus on 313 GitHub repositories that are included in
the NVD reference hyperlinks and consider all their commits
as the wild. In total, we collect 6M patches in the wild.
Table II presents the dataset augmentation setup and results
in ﬁve rounds. The ﬁrst two columns show the size of unla-
beled wild data used to perform the nearest link search and the
corresponding round number, respectively. The third column
is the number of candidates identiﬁed in each round. Since
our nearest link search method locates its nearest neighbor for
each known security patch, this number is equal to the number
of already labeled security patches. The fourth column exhibits
the number of real security patches veriﬁed by security experts.
To ensure the correctness of manual veriﬁcation, three authors
of this paper label the candidates separately and then cross-
check their labeling results. The last column shows the ratio
of the veriﬁed security patches (column 4) to the candidates
(column 3). The higher ratio means the higher efﬁciency.
In the beginning, the NVD-based dataset includes 4076
security patches, and we use it as the initial dataset to search
their nearest links in an unlabeled wild dataset. Since it is too
expensive to compute distances from all the 6M instances in
the wild, we construct a smaller Set I by randomly selecting
100K commits from the wild 6M instances. We run three
rounds of dataset augmentation over Set I. In the ﬁrst round,
the nearest link search method generates 4076 candidates, and
we manually verify that 895 are security patches, a ratio of
22%. Now we have 4971 security patches (i.e., 4076 NVD-
based and 895 wild-based ones). Meanwhile, 3181 candidates
are labeled as non-security patches through the manual veriﬁ-
cation process, and we remove all these labeled patches from
Set I. In the second round, based on 4971 labeled security
patches, we repeat the above procedures on the updated Set
I, and 1235 instances out of 4971 candidates are manually
veriﬁed as security patches (a ratio of 25%). Similarly, in the
third round, we manually identify 993 new security patches
from 6206 candidates, and the ratio drops to 16%. Since there
can be 6-10K security patches in the 100K search range and
only around 1K instances are identiﬁed in each round, a large
number of unexplored patches still remain. In such cases,
ratios may not deﬁnitely decrease after each round.
While the ratios in Set I are 16-25%, we wonder if the ratio
can be further increased. Intuitively, the ratio may increase if
TABLE II: # of security patches identiﬁed in ﬁve rounds.
Search Range
(unlabeled patches)
Round
Candidates
Veriﬁed
Security Patches
Set I: 100K
Set II: 200K
Set III: 200K
1
2
3
4
5
4076
4971
6206
7199
9287
895
1235
993
2088
2786
Ratio
22%
25%
16%
29%
30%
the candidates are located in a larger unlabeled wild dataset
since it is more likely to contain security patches that are
more similar to existing security patch instances. Therefore,
instead of continuing in Set I, we conduct the security patch
dataset augmentation in a larger unlabeled wild dataset Set
II, another 200K randomly selected from the 6M GitHub
commits. Among 7199 candidates selected by the nearest
link search, 2088 instances are security patches and the total
number of known security patches increases to 9287 after
Round 4. We ﬁnd that the ratio (29%) is higher than the ﬁrst
three rounds, which means a larger search range can enable a
higher ratio. To verify this, we conduct another round (Round
5) of data augmentation on Set III with another 200K randomly
selected instances. We discover 2786 security patches from
9287 candidates (30%). It conﬁrms that the ratio increases
along with a larger search range. Compared with the brute
force search that considers all the unlabeled data as candidates
where only 6-10% candidates are security patches, our method
can almost triple the efﬁciency of human veriﬁcation, in other
words, reduce around 66% efforts.
After the ﬁve rounds of the dataset augmentation process,
we collect a security patch dataset of 12,073 instances, where
4076 ones belong to the NVD-based dataset and 7997 ones