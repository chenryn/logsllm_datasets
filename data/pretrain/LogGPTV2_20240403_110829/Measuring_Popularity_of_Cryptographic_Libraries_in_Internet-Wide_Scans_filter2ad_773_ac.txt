3.4.3
Simulation scenarios. We considered several distributions
of prior probability library usage:
Evenly distributed probabilities match the approach in [36], how-
ever, we face an additional task of first estimating the probabilities
from the simulated data. Furthermore, our mask does not use one
of the original features (Section 2.1).
We also assign random prior probabilities to the groups in a
different scenario – each group is assigned a uniformly chosen real
number from 0 to 1 and the numbers are normalized to sum to 1.
Real-world popularities of libraries are better characterized by a
geometric distribution – one source dominates (e.g., 50% in our case)
and other are exponentially less probable. We additionally ensure
that each group has a probability at least 2%. This way, even very
rare sources are not completely excluded from the analysis, even
if the library is outdated (e.g., PGP SDK 4) or the hardware is very
old (e.g., Gemalto GXP E64 smartcard from 2004). We also test the
geometric distribution for different permutations of the groups –
while in TLS, OpenSSL is always the most probable, in our tests
each group may take the first place.
Finally, we simulate the data according to the prior probabilities
extracted from TLS datasets. We add deviations to the probabilities
to simulate subtle changes in the popularity of libraries.
3.4.4 Accuracy of prior probability estimation. The accuracy of
the prior probability estimation is given as the expected error in
the resulting estimation. The summary is given in Table 1.
We consider the average error (the expected error in percentage
points (pp) for each group probability in each experiment) and the
average worst error (the expected error in pp for the worst result in
a given experiment). As an example, if the real probabilities are 60%,
30%, and 10%, and we estimate them as 61%, 32%, and 7%, the average
error of the experiment is (1 + 2 + 3)/3 = ±2 pp and the worst error
is ±3 pp. The averages in Table 1 are given for 100 experiments,
Estimation error (in percentage points)
Noise:
Distribution
Even
Random
Geometric
TLS
0%
0.19
0.19
0.18
0.17
1%
2%
3%
Average error
0.37
0.37
0.38
0.39
0.63
0.61
0.63
0.66
0.90
0.84
0.91
0.94
2%
1%
0%
3%
Average worst error
5.07
0.73
0.78
4.68
4.97
0.71
0.65
5.16
1.71
1.74
1.70
1.78
3.33
3.25
3.33
3.49
Table 1: Accuracy of prior probability estimation for differ-
ent types of distributions and different amount of system-
atic noise. The average error gives the expected error of prior
probability estimation for each group in percentage points
(pp). The average worst error gives the expected value of the
largest error in each experiment. E.g., when the keys were
generated from a TLS-like distribution with 1% of system-
atic noise added, the probability of each group differed by
±0.39 pp on average and the worst estimation was off by
±1.78 pp on average.
Classification accuracy (in %) with noise
0%
3%
1%
2%
Noise:
1st
Guess:
Even
33.4
Random 45.5
81.1
Geometric
94.8
TLS
2nd
53.2
67.5
95.2
98.7
1st
33.3
46.1
82.5
94.6
2nd
52.9
67.8
95.0
98.6
1st
32.9
45.0
80.3
94.4
2nd
52.4
66.7
94.6
98.4
1st
32.3
43.9
80.9
94.3
2nd
51.8
65.1
94.3
98.3
Table 2: Accuracy of key origin classification when prior
probability estimates are included in the method for differ-
ent types of distributions and different amount of system-
atic noise. The values are in percents. E.g., when the keys
were generated from a TLS-like distribution with 1% of sys-
tematic noise added, for 94.6% of the keys, the original li-
brary was correctly identified on the first guess and 98.6% of
keys were correctly labeled by the first or the second most
probable group.
each simulating one million keys. We considered distinct scenarios
(Section 3.4.3) and levels of systematic noise (Section 3.4.2).
3.4.5 Accuracy of the overall classification process. The accuracy
of key classification is given as the proportion of keys that were
correctly classified as the first guess or at least the second guess.
The values in Table 2 are given in percents.
Tables 1 and 2 refer to the same set of simulations. The prior
probability estimation is performed first. The results show that
the classification is quite robust even in the case of errors in prior
probability estimations at a level of 5 percentage points, since the
success of the classification is not affected dramatically.
When compared to the approach of [36], the average accuracy
increased for other than the even distribution of groups. However,
the classification accuracy is improved mostly for the more probable
groups and the less probable libraries may be classified incorrectly
more frequently than before.
1673.5 Additional accuracy considerations
Some reference distributions can be approximated by a combina-
tion of other reference distributions, similarly as the distribution
observed in a dataset can be obtained as a combination of reference
distributions. An example of this phenomenon at its worst is the
close match of Group 11 (Microsoft libraries) as a combination of
41.3% of Group 13, 30.6% of Group 8, 22.7% of Group 4 and a small
portion of other groups. The situation for all groups is illustrated
in Figure 4, with the most notable groups enlarged. Group 13 has
the next closest match, however the error is much larger. Group 7
(OpenSSL) cannot be obtained as a combination of other groups.
As a result, the prior probability estimation process may inter-
change the distribution of Group 11 for a mixture of other dis-
tributions or vice versa. Currently, we do not detect such events
automatically, since an additional user input would be needed.
When considering the results, the domain must be taken into
account. E.g., according to our measurement, around 1% of keys
in some samples of TLS keys originate from Group 8 (PGP SDK 4
FIPS). Since the presence of the library in TLS is highly unlikely and
no other known implementation has the same (quite uncommon)
algorithm, we must conclude that this is an error in the estimation.
We suspect the error is due to the aforementioned approximation
of Group 11. However, there may exist different approximations of
the group, hence we cannot simply substitute the suspected ratio.
We hypothesize that such errors could be avoided if the prior
probability estimation would start from a very rough approximation
of the probabilities supplied by the user (we use evenly distributed
groups) as the starting guess of the NNLSF method. A more resolute
solution would remove groups from the analysis if they are unlikely
to occur in an examined domain according to empirical evidence.
4 RESULTS ON RELEVANT DATASETS
Rough estimates of popularity for some cryptographic libraries
were provided in [36] for TLS, CT and PGP, but with relatively
high expected errors. The improvement of accuracy in our work
allows for a better inspection of datasets, including the detection
of transient events. We also processed significantly more datasets,
including the archived ones.
4.1 Data preparation
We used a wide range of datasets for our analysis. Due to different
formats, we pre-process all data into a unified intermediate format.
For all datasets, only keys with unique moduli were considered.
4.1.1 Censys TLS scan. Censys [15] performs a full IPv4 address
space scan of TCP port 443 on a weekly basis [4]. The dataset
contains historical scans back to 2015-08-10 when the first scan was
performed and continues to present. Each scan is a full snapshot,
independent from all other scans, containing all raw and post-
processed data from the scan in the form of JSON and CSV files,
compressed by LZ4 algorithm [5]. Some snapshots are only a few
days apart and some larger gaps occur, but overall the weekly
periodicity is prevalent.
Censys scanner tries to perform a TLS handshake with the host
being scanned, respecting the IP blacklist maintained by Censys.
The latest scan tried to contact 53M hosts.
4.1.2 Censys Alexa 1M. The dataset [3] has the same properties
as the Censys IPv4 dataset (with respect to periodicity and format).
It contains processed TLS handshakes with the top 1 million web-
sites according to the Alexa ranking. The dataset also provides an
insight into a specific portion of the Internet certificates, which