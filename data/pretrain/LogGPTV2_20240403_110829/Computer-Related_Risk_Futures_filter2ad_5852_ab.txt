subsequent attempt at a cover-up during elections in
2002, 2004, and 2006. Insiders exploited a misleading
user interface to alter votes before ballots were actu-
ally cast. Indeed, today’s electronic voting systems
are held to much weaker standards than (for example)
gambling machines and lotteries. Worse yet, Inter-
net voting is a potential disaster in waiting; although
highly desirable in principle, it requires much more
selective use (perhaps only for acquiring an appropri-
ate ballot, but not for casting it?), security controls,
and oversight before it might be considered trustwor-
thy. Approaches to election integrity are needed that
span the entire process from beginning to end, includ-
ing the entire supply chain.
(For example, see [7],
which represents requirements for election integrity
within the Common Criteria framework.)
4 Would-be Remedies
Various approaches to risks have been postulated.
A quasi-analytic approach involves risk assessment
that identiﬁes and attempts to quantify the most
serious risks, sometimes based on false or incom-
plete assumptions. However, such analyses are typ-
ically followed by so-called risk management that
often ignores the results, declares them suﬃciently
unlikely, seeks insurance coverage, or perhaps at-
tempts to reduce or prevent the causes of those po-
tential risks. Another more far-sighted approach
involves constructive and proactive action as sug-
gested above—for example, developing systems de-
signed to satisfy well-speciﬁed and realistic require-
ments, with soundly layered and composable system
architectures (see fm.csl.sri.com/LAW09/#program
for recent papers on the 2009 workshop on this
topic, including [14], and John Rushby on what we
can learn from other disciplines), principled software
engineering practices, inherently safer programming
languages, and intelligent system administration.
A document developed for Doug Maughan at the
Department of Homeland Security, A Roadmap for
Cybersecurity Research [6], considers 11 areas of hard
problems in information security: scalable trustwor-
thy systems, enterprise-level metrics, lifecycle of sys-
tem evaluation, coping with insider threats and mal-
ware, global-scale identity management, survivabil-
ity of time-critical systems, situational awareness and
attack attribution, provenance, privacy-aware secu-
rity, and usable security. Each area needs signiﬁ-
cant eﬀort toward research, development, evaluation,
and technology transfer. Of considerable importance
is the extent to which these 11 areas are interre-
lated, which strongly suggests the need for holistic
approaches (e.g., [11]). Although decoupling these
areas may be desirable for many reasons, the inter-
dependencies must nevertheless be realistically ac-
commodated throughout R&D, including the estab-
lishment of requirements, architectural design, imple-
mentation, and adaptably under any subsequent evo-
lutionary changes. In particular, any inherent com-
plexities must be considered architecturally and ei-
ther reduced or otherwise addressed constructively
wherever possible, without compromising trustwor-
thiness.
Background on various constructive approaches
for coping with complex requirements and complex
architectures can be found in [10, 13, 14] as well
as [1, 2, 4, 16, 18, 19, 20], for example.
5 Conclusions
Myopia is very dangerous with respect to trustwor-
thiness. The commonalities among diﬀerent appli-
cations far beyond those considered here are likely
to transcend any would-be single-discipline solutions.
Thus, massive culture shifts are needed to consider in-
formation systems and their applications holistically
and proactively. We must be able to develop sys-
tems and evaluate them in their entirety—especially
through compositions of evaluated subsystems, with
predictable aggregate behavior—and to ensure us-
ability and the soundness of operational conﬁgura-
tions. Of course, this culture shift is especially im-
portant for applications with critical requirements
for trustworthiness. The pleas for such approaches
in many seminal papers (including those that have
been revisited previously in the ACSAC Classic Pa-
pers track) may seem old-fashioned, but are never-
theless still timely.
Inadequate understanding of the depths of the
problems is also dangerous, in part because it typi-
cally leads to simplistic and untrustworthy solutions.
Examples of that risk include beliefs that crypto-
graphic certiﬁcates, longer passwords, ﬁrewalls, and
testing can ensure security. However, too much ago-
38
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:09:52 UTC from IEEE Xplore.  Restrictions apply. 
nizing over the pervasiveness of these problems is also
dangerous—because the resulting sense of hopeless-
ness also typically leads to would-be solutions that
are short-sighted and ineﬀective.
In both cases, a
danger of eschewing useful research and innovative
developments, and placing trust in untrustworthy
systems and people often prevails.
In particular, life-critical systems and other sys-
tems with stringent requirements for trustworthi-
ness should be held to commensurately higher stan-
dards than conventional software, overcoming today’s
realities—in which architectures are poorly struc-
tured, development practices generally yield numer-
ous vulnerabilities, and criteria and proprietary eval-
uations are inherently incomplete.
Market forces appear inadequate in driving high-
assurance components for critical systems. To be ef-
fective, open systems, open interfaces, and various
forms of nonproprietary source code need more sup-
porting incentives. Regulation is a slippery slope, as
is attempting to invoke liability for the development
or use of untrustworthy systems (which would entail
some signiﬁcant and diﬃcult changes, including how
to place the blame—as noted above). Insurance and
tax incentives are possible, but also likely to have
exploitable loopholes. Better awareness of the risks
of untrustworthiness is clearly warranted. Above all,
there are no simple solutions.
I am frequently asked why the ACM Risks Fo-
rum does not include more success stories. There
are several reasons. Failures signiﬁcantly outnum-
ber successes. Success stories are rarely submitted,
and I continue to hunt for them. Supposed suc-
cesses are sometimes over-hyped, even masking de-
velopment problems. I have cited Henry Petroski on
numerous occasions; he noted long ago that we tend
to learn little from successes, but that we have a bet-
ter chance to learn from our failures. The RISKS ex-
perience suggests that we do not learn enough from
either. Fortunately, a better understanding of the
past seems to be emerging in some areas in recent
years, along with perhaps fewer aircraft crashes, au-
tomobile recalls, and nuclear power disasters. The
ACSAC classic papers track also provides some sup-
porting perspectives—including those this year by Li
Gong and Matt Bishop.
However, too many problems remain in areas such
as health care, power distribution, malware, and elec-
tions, and lurking trustworthiness problems in au-
tomated highways, autonomous systems, and cloud
computing. In any event, the lessons of past vulnera-
bilities, system development failures, and human lim-
itations must be considered more pervasively, and a
deeper understanding of the emerging threats and po-
tential risks must be gained. I am optimistic that the
research and development communities have much to
oﬀer, but am less optimistic that the needed culture
shifts can be achieved because of the necessary major
changes relating to governments, industry, education,
economic policies, standards, procurement processes,
entrenched interests, and so on.
Acknowledgments
This paper received support from ACCURATE: A
Center for Correct, Usable, Reliable, Auditable, and
Transparent Elections, under SRI’s National Science
Foundation Grant Number 0524111. The author
wishes to thank Douglas Maughan, who previously
sponsored a program on high-assurance trustworthy
systems (e.g., [10]) when he was a Program Manager
in the Defense Advanced Research Projects Agency
(DARPA), and who continues to be responsible for
cybersecurity in the Science and Technology Direc-
torate of the Department of Homeland Security, pur-
suing approaches aimed at preventing risks such as
those mentioned here (e.g., [6]).
References
[1] C. Boettcher, R. DeLong, J. Rushby, and
W. Sifre. The MILS component integration ap-
proach to secure information sharing.
In 27th
AIAA/IEEE Digital Avionics Systems Confer-
ence, St. Paul MN, October 2008. IEEE.
[2] D.D. Clark and D.R. Wilson. A comparison of
commercial and military computer security poli-
cies. In Proceedings of the 1987 Symposium on
Security and Privacy, pages 184–194, Oakland,
California, April 1987. IEEE Computer Society.
[3] D.D. Clark et al. Computers at Risk: Safe Com-
puting in the Information Age. National Re-
search Council, National Academies Press, 2101
Constitution Ave., Washington, D.C., 5 Decem-
ber 1990. Final report of the System Security
Study Committee.
[4] E.W. Dijkstra. The structure of the THE mul-
tiprogramming system. Communications of the
ACM, 11(5), May 1968.
39
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:09:52 UTC from IEEE Xplore.  Restrictions apply. 
[5] S.E. Goodman and H.S. Lin, editors. Toward a
Safer and More Secure Cyberspace. National Re-
search Council, National Academies Press, 2101
Constitution Ave., Washington, D.C., 2007.
Final report of the National Research Coun-
cil Committee on Improving Cybersecurity Re-
search in the United States.
[6] D. Maughan et al. A roadmap for cybersecu-
rity research. Technical report, Department of
Homeland Security, October 2009.
[7] R. Mercuri. Electronic Vote Tabulation Checks
and Balances. PhD thesis, Department of Com-
puter Science, University of Pennsylvania, 2001.
http://www.notablesoftware.com/evote.html
[8] P.G. Neumann.
The role of motherhood
system programming.
in the pop art of
In Proceedings of
the ACM Second Sym-
posium on Operating Systems Principles,
Princeton, New Jersey, pages 13–18. ACM,
October 1969. http://www.multicians.org/pgn-
motherhood.html
[9] P.G. Neumann. Computer-Related Risks. ACM
Press, New York, and Addison-Wesley, Reading,
Massachusetts, 1995.
[10] P.G. Neumann. Principled assuredly trustwor-
thy composable architectures. Technical report,
Computer Science Laboratory, SRI
Interna-
tional, Menlo Park, California, December 2004.
http://www.csl.sri.com/neumann/chats4.html,
.pdf, and .ps.
[11] P.G. Neumann. Holistic systems. ACM Software
Engineering Notes, 31(6):4–5, November 2006.
[12] P.G. Neumann. Risks of untrustworthiness. In
Proceedings of the 22nd Annual Computer Se-
curity Applications Conference (ACSAC 2006),
Classic Papers section, Miami, Florida, Decem-
ber 2006. IEEE Computer Society.
[13] P.G. Neumann. Reﬂections on system trustwor-
thiness. In Marvin Zelkowitz, editor, Advances
in Computers, volume 70, pages 269–310. Else-
vier Inc., 2007.
[14] P.G. Neumann. Hierarchies, lowerarchies, anar-
chies, and plutarchies: Historical perspectives
of
composable high-assurance architectures.
In Third Layered Assurance Workshop, San
Antonio CA, August 2009. AFRL. Slides:
http://www.csl.sri.com/neumann/law09+x4.pdf
[15] P.G. Neumann.
Illustrative risks
to the
public in the use of computer systems and
related technology,
index to RISKS cases.
report, Computer Science Lab-
Technical
International, Menlo Park,
oratory,
SRI
California, 2009.
Updated now and then:
http://www.csl.sri.com/neumann/illustrative.html;
also in .ps and .pdf form for printing in a denser
format.
[16] D.L. Parnas. On the criteria to be used in decom-
posing systems into modules. Communications
of the ACM, 15(12), December 1972.
[17] P. Porras.
on
conﬁcker.
52(10),
Communications
October
column.
2009.
http://www.csl.sri.com/neumann/insiderisks.html#219
the ACM,
Inside Risks
Reﬂections
of
[18] J.M. Rushby. The design and veriﬁcation of
secure systems.
In Proceedings of the Eighth
ACM Symposium on Operating System Princi-
ples, pages 12–21, Asilomar, California, Decem-
ber 1981.
(ACM Operating Systems Review,
15(5)).
[19] J.H. Saltzer and F. Kaashoek. Principles of
Computer System Design. Morgan Kauﬀman,
2009. Chapters 1-6 only. Chapters 7-11 are on-
line. http://ocw.mit.edu/Saltzer-Kaashoek
[20] J.H. Saltzer and M.D. Schroeder. The protection
of information in computer systems. Proceedings
of the IEEE, 63(9):1278–1308, September 1975.
[21] M. Schaefer et al. Multilevel Data Management
Security. Air Force Studies Board, National Re-
search Council, National Academies Press, 1983.
Final report of the 1982 Multilevel Data Man-
agement Security Committee.
[22] F.B. Schneider and M. Blumenthal, editor. Trust
in Cyberspace. National Research Council, Na-
tional Academies Press, 2101 Constitution Ave.,
Washington, D.C., 1998. Final report of the Na-
tional Research Council Committee on Informa-
tion Trustworthiness.
40
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:09:52 UTC from IEEE Xplore.  Restrictions apply.