small within a datacenter (typically 5–7), the end-host preallocates
enough packet memory to store queue sizes. Moreover, the end-
host knows exactly how to interpret values in the packet to obtain a
detailed breakdown of queueing latencies on all network hops.
This example illustrates how a low-latency, programmatic in-
terface to access dataplane state can be used by software at end-
hosts to measure dataplane behavior that is hard to observe in the
control plane. Figure 1a shows a six-node dumbell topology on
Mininet [12], in which each node sends a small 10kB message to
every other node in the topology. The total application-level offered
load is 30% of the hosts’ network capacity (100Mb/s). We instru-
mented every packet with a TPP, and collected fully executed TPPs
carrying network state at one host. Figure 1b shows the queue evo-
lution of 6 queues inside the network obtained from every packet
received at that host.
Overheads: The actual TPP consists of three instructions, one each
to read the switch ID, the port number, and the queue size, each a
16 bit integer. If the diameter of the network is 5 hops, then each
TPP adds only a 54 byte overhead to each packet: 12 bytes for the
TPP header (see §3.4), 12 bytes for instructions, and 6× 5 bytes to
collect statistics at each hop.
2.2 Rate-based Congestion Control
While the previous example shows how TPPs can help observe
latency spikes, we now show how such visibility can be used to
control network congestion. Congestion control is arguably a dat-
aplane task, and the literature has a number of ideas on designing
better algorithms, many of which require switch support. How-
ever, TCP and its variants still remain the dominant congestion
control algorithms. Many congestion control algorithms, such as
XCP [20], FCP [11], RCP [9], etc. work by monitoring state that
indicates congestion and adjusting ﬂow rates every few RTTs.
We now show how end-hosts can use TPPs to deploy a new con-
gestion control algorithm that enjoys many beneﬁts of in-network
algorithms, such as Rate Control Protocol (RCP) [9]. RCP is a con-
gestion control algorithm that rapidly allocates link capacity to help
ﬂows ﬁnish quickly. An RCP router maintains one fair-share rate
R(t) per link (of capacity C, regardless of the number of ﬂows),
computed periodically (every T seconds) as follows:
(cid:32)
1−
R(t + T ) = R(t)
(cid:33)
T
d ×
a (y(t)−C) + b q(t)
d
C
(1)
Here, y(t) is the average ingress link utilization, q(t) is the av-
erage queue size, d is the average round-trip time of ﬂows travers-
SP = 0x0PUSH [QSize]SP = 0x4PUSH [QSize]0x00SP = 0x8PUSH [QSize]0x000xa0Packet memory is preallocated.  The TPP never grows/shrinks inside the network.Ethernet HeaderOther headers(e.g., TCP/IP)0510152025Queuesize(packets)0.50.60.70.80.91.0Fractiles2.02.22.42.62.83.0Time(s)0510152025Queuesize(packets)5ing the link, and a and b are conﬁgurable parameters. Each router
checks if its estimate of R(t) is smaller than the ﬂow’s fair-share
(indicated on each packet’s header); if so, it replaces the ﬂow’s fair
share header value with R(t).
We now describe RCP*, an end-host implementation of RCP.
The implementation consists of a rate limiter and a rate controller
at end-hosts for every ﬂow (since RCP operates at a per-ﬂow granu-
larity). The network control plane allocates two memory addresses
per link (Link:AppSpecific_0 and Link:AppSpecific_1) to
store fair rates. Each ﬂow’s rate controller periodically (using the
ﬂow’s packets, or using additional probe packets) queries and mod-
iﬁes network state in three phases.
Phase 1: Collect. Using the following TPP, the rate controller
queries the network for the switch ID on each hop, queue sizes,
link utilization, and the link’s fair share rate (and its version num-
ber), for all links along the path. The receiver simply echos a fully
executed TPP back to the sender. The network updates link utiliza-
tion counters every millisecond. If needed, end-hosts can measure
them faster by querying for [Link:RX-Bytes].
PUSH [Switch:SwitchID]
PUSH [Link:QueueSize]
PUSH [Link:RX-Utilization]
PUSH [Link:AppSpecific_0] # Version number
PUSH [Link:AppSpecific_1] # Rfair
Phase 2: Compute. In the second phase, each sender computes a
fair share rate Rlink for each link: Using the samples collected in
phase 1, the rate controller computes the average queue sizes on
each link along the path. Then, it computes a per-link rate Rlink
using the RCP control equation.
Phase 3: Update. In the last phase, the rate-controller of each ﬂow
asynchronously sends the following TPP to update the fair rates on
all links. To ensure correctness due to concurrent updates, we use
the CSTORE instruction:
CSTORE [Link:AppSpecific_0], \
[Packet:Hop[0]], [Packet:Hop[1]]
STORE [Link:AppSpecific_1], [Packet:Hop[2]]
PacketMemory:
Hop1: V_1, V_1+1, R_new_1, (* 16 bits each*)
Hop2: V_2, V_2+1, R_new_2, ...
where Vi is the version number in the AppSpecific_0 that the end-
host used to derive an updated Rnew,i for hop i, thus ensuring con-
sistency.
(CSTORE dst,old,new updates dst with new only if
dst was old, ignoring the rest of the TPP otherwise.) Note that
in the TPP, the version numbers and fair rates are read from packet
memory at every hop.
Other allocations: Although RCP was originally designed to allo-
cate bandwidth in a max-min fair manner among competing ﬂows,
Kelly et al. [22] showed how to tweak RCP to allocate bandwidth
for a spectrum of fairness criteria—α-fairness parameterized by a
real number α ≥ 0. α-fairness is achieved as follows: if Ri is the
fair rate computed at the i-th link traversed by the ﬂow (as per the
RCP control equation 1), the ﬂow sets its rate as
(cid:33)−1/α
(cid:32)
∑
i
R =
R−α
i
(2)
The value α = 1 corresponds to proportional fairness, and we
can see that in the limit as α → ∞, R = mini Ri, which is consis-
tent with the notion of max-min fairness. Observe that if the ASIC
Figure 2: Allocations by max-min and proportional fairness variant
of RCP on the trafﬁc pattern shown inset on the right plot; each link
has 100Mb/s capacity and all ﬂows start at 1Mb/s at time 0.
hardware had been designed for max-min version of RCP, it would
have been difﬁcult for end-hosts to achieve other useful notions of
fairness. However, TPPs help defer the choice of fairness to deploy-
ment time, as the end-hosts can aggregate the per-link Ri according
to equation 2 based on one chosen α. (We do not recommend ﬂows
with different α sharing the same links due to reasons in [35].)
Figure 2 shows the throughput of three ﬂows for both max-
min RCP* and proportional-fair RCP* in Mininet: Flow ‘a’ shares
one link each with ﬂows ‘b’ and ‘c’ (shown inset in the right plot).
Flows are basically rate-limited UDP streams, where rates are de-
termined using the control algorithm: Max-min fairness should
allocate rates equally across ﬂows, whereas proportional fairness
should allocate 1⁄3 of the link to the ﬂow that traverses two links,
and 2⁄3 to the ﬂows that traverse only one link.
Overheads: For the experiment in Figure 2, the bandwidth over-
head imposed by TPP control packets was about 1.0–6.0% of the
ﬂows’ rate as we varied the number of long lived ﬂows from 3 to
30 to 99 (averaged over 3 runs).
In the same experiment, TCP
had slightly lower overheads: 0.8–2.4%. The RCP* overhead is
in the same range as TCP because each ﬂow sends control packets
roughly once every RTT. As the number of ﬂows n increases, the
average per-ﬂow rate decreases as 1/n, which causes the RTT of
each ﬂow to increase (as the RTT is inversely proportional to ﬂow
rate). Therefore, the total overhead does not blow up.
Are writes absolutely necessary? RCP* is one of the few TPP
applications that writes to network state. It is worth asking if this
is absolutely necessary. We believe it is necessary for fast conver-
gence since RCP relies on ﬂows traversing a single bottleneck link
agreeing on one shared rate, which is explicitly enforced in RCP.
Alternatively, if rapid convergence isn’t critical, ﬂows can converge
to their fair rates in an AIMD fashion without writing to network
state. In fact, XCP implements this AIMD approach, but experi-
ments in [9] show that XCP converges more slowly than RCP.
2.3 Network Troubleshooting Framework
There has been recent interest in designing programmatic tools
for troubleshooting networks; without doubt, dataplane visibility
is central to a troubleshooter. For example, consider the task of
verifying that network forwarding rules match the intent speciﬁed
by the administrator [21, 23]. This task is hard as forwarding rules
change constantly, and a network-wide ‘consistent’ update is not a
trivial task [32]. Veriﬁcation is further complicated by the fact that
there can be a mismatch between the control plane’s view of routing
state and the actual forwarding state in hardware (and such prob-
lems have shown up in a cloud provider’s production network [24]).
Thus, verifying whether packets have been correctly forwarded re-
quires help from the dataplane.
Recently, researchers have proposed a platform called Net-
Sight [13]. NetSight introduced the notion of a ‘packet history,’
01020304050607080Time (s)010203040506070Throughput (Mb/s)Max-min fairnessflow aflow bflow c01020304050607080Time (s)010203040506070Throughput (Mb/s)Proportional fairnessﬂow aﬂow bﬂow c6Figure 3: TPPs enable end-hosts to efﬁciently collect packet histo-
ries, which can then be used to implement four different trouble-
shooting applications described in [13].
which is a record of the packet’s path through the network and
the switch forwarding state applied to the packet. Using this con-
struct, the authors show how to build four different network trouble-
shooting applications.
We ﬁrst show how to efﬁciently capture packet histories that are
central to the NetSight platform. NetSight works by interposing on
the control channel between the controller and the network, stamp-
ing each ﬂow entry with a unique version number, and modify-
ing ﬂow entries to create truncated copies of packet headers tagged
with the version number (without affecting a packet’s normal for-
warding) and additional metadata (e.g., the packet’s input/output
ports). These truncated packet copies are reassembled by servers to
reconstruct the packet history.
We can refactor the task of collecting packet histories by having
a trusted agent at every end-host (§4) insert the TPP shown below
on all (or a subset of) its packets. On receiving a TPP that has
ﬁnished executing on all hops, the end-host gets an accurate view of
the network forwarding state that affected the packet’s forwarding,
without requiring the network to create additional packet copies.
PUSH [Switch:ID]
PUSH [PacketMetadata:MatchedEntryID]
PUSH [PacketMetadata:InputPort]
Once the end-host constructs a packet history, it is forwarded to
collectors where they can be used in many ways. For instance, if
the end-host stores the histories, we get the same functionality as
netshark—a network-wide tcpdump distributed across servers.
From the stored traces, an administrator can use any query lan-
guage (e.g., SQL) to extract relevant packet histories, which gives
the same functionality as the interactive network debugger ndb.
Another application, netwatch simply uses the packet histories
to verify whether network forwarding trace conforms to a policy
speciﬁed by the control plane (e.g., isolation between tenants).
Overheads: The instruction overhead is 12 bytes/packet and 6
bytes of per-hop data. With a TPP header and space for 10 hops,
this is 84 bytes/packet. If the average packet size is 1000 bytes, this
is a 8.4% bandwidth overhead if we insert the TPP on every packet.
If we enable it only for a subset of packets, the overhead will be
correspondingly lower.
Caveats: Despite its beneﬁts, there are drawbacks to using only
TPPs, especially if the network transforms packets in erroneous or
non-invertible ways. We can overcome dropped packets by send-
ing packets that will be dropped to a collector (we describe how
in §2.5). Some of these assumptions (trusting the dataplane to
function correctly) are also made by NetSight, and we believe the
advantages of TPPs outweigh its drawbacks. For instance, TPPs
can collect more statistics, such as link utilization and queue occu-
pancy, along with a packet’s forwarding history.
Figure 4: An example showing the beneﬁts of congestion-aware
load balancing: ECMP splits ﬂow from L1 to L2 equally across the
two paths resulting in suboptimal network utilization. CONGA*,
an end-host refactoring of CONGA [1] is able to detect and reroute
ﬂows, achieving optimum in this example.
2.4 Distributed Load Balancing
We now show how end-hosts can use TPPs to probe for network
congestion, and use this detailed visibility to load balance trafﬁc
in a distributed fashion. We demonstrate a simpliﬁed version of
CONGA [1], which is an in-network scheme for trafﬁc load bal-
ancing. CONGA strives to maximize network throughput and min-
imize the maximum network link utilization in a distributed fashion
by having network switches maintain a table of path-level conges-
tion metrics (e.g., quantized link utilization). Using this informa-
tion, switches route small bursts of ﬂows (“ﬂowlets”) selﬁshly on
the least loaded path. CONGA is optimized for datacenter network
topologies; we refer the curious reader to [1] for more details.
CONGA’s design highlights two beneﬁts relevant to our discus-
sion. First, it uses explicit visibility by having switches stamp quan-
tized congestion information on packet headers. Second, load bal-
ancing decisions are made at round-trip timescales to rapidly detect
and react to network congestion. Since TPPs also offer similar ben-
eﬁts, we show how we can refactor the load balancing task between
end-hosts and the network, without requiring custom hardware (ex-
cept, of course, to support TPPs).
First, we require the network to install multipath routes that end-
hosts can select based on packet header values. This can be done in
the slow-path by the control plane by programming a ‘group table’
available in many switches today for multipath routing [29, §5.6.1],
which selects an output port by hashing on header ﬁelds (e.g., the
VLAN tag). This allows end-hosts to select network paths simply
by changing the VLAN ID.
Second, we need end-hosts to query for link utilization across
various paths, by inserting the following TPP on a subset of packets
destined to hosts within the datacenter:
PUSH [Link:ID]
PUSH [Link:TX-Utilization]
PUSH [Link:TX-Bytes]
We query for Link:TX-Bytes to measure small congestion
events if the link utilization isn’t updated. The receiver echoes fully
executed TPPs back to the sender to communicate the congestion.
Note that the header of the echoed TPP also contains the path ID
along with the link utilization on each link in the path.
Third, using information in the fully executed TPPs, end-hosts
can build a table mapping ‘Path i→ Congestion Metric (mi),’ where
mi is either the maximum or sum of link utilization on each switch–
switch network hop on path i. The authors of CONGA note that
‘sum’ is closer to optimal than ‘max’ in the worst-case scenario
(adversarial); however CONGA used ‘max’ as it does not cause
overﬂows when switches aggregate path-congestion. With TPPs,
this is not an issue, and the choice can be deferred to deploy time.
And ﬁnally, end-hosts have full context about ﬂows and ﬂowlets,
and therefore each end-host can select a ﬂowlet’s path by setting
the path tag appropriately on the ﬂowlet’s packets.
TPPPacketInsert TPPsTPPviewPacketStrip TPPsEnd-host AppsNetworkEnd-host AppsviewCollectorPkt HdrCollectorCollectorsS0S1L0L2L1Each link capacity = 100Mb/s.120115L1:L21205045L0:L250CONGA*ECMPDem.FlowAchieved Thput.Max Util =10085All demand and throughput numbers are in Mb/s.7Overheads: We implemented a proof-of-concept prototype
(CONGA*) in software using UDP ﬂows; Figure 4 reproduces an
example from CONGA [1, Figure 4]. We conﬁgured switches S0
and S1 to select paths based on destination UDP port. The ﬂow
from L0 to L2 uses only one path, whereas the ﬂow from L1 to
L2 has two paths. The UDP agents at L0 and L1 query for link
utilization and aggregate congestion metrics every millisecond for
the two paths. With CONGA*, end-hosts can maximize network
throughput meeting the demands for both ﬂows, while simultane-
ously minimizing the maximum link utilization. In this example,
the overhead introduced by TPP packets was minimal (< 1% of the
total trafﬁc).
Remark: Note that the functionality is refactored between the net-
work and end-hosts; not all functionality resides completely at the
end-hosts. The network implements TPP and multipath routing.
The end-hosts merely select paths based on congestion completely