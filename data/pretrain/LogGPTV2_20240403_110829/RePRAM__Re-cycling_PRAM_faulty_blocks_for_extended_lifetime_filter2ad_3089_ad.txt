0.1, 0.2, and 0.3 (shown in Figure 3). 
with a 
factors of 
bit. We model the 
that any single 
Our experiments 
a high probability 
tail-end 
scheme quickly 
window of writes before 
of the normal distribut
of at least one byte having a 
show that with a 4KB page size, there is 
lifetime 
at the 
ion. This makes the Fail_Stop 
of the pages within a short 
drops 
capacity 
decommission all 
the PCM's effective 
more pronounced 
at higher 
B. Peiformance 
Impact 
as shown in our experimental 
up to 1 60 faults in 
the page for permanent 
with DRM is that replicated 
up the aging process 
lifetime 
of approx­
scheme depending 
on 
pages  speeds 
the main disadvantage 
to zero. This effect is especially 
variation, 
levels of process 
results. 
DRM is able to tolerate 
Although 
each page before decommissioning 
failure, 
writes to two different 
of both pages. DRM offers an extended 
imately 
O.1 3x to 22x over Fail_Stop 
the process 
tolerate 
PCM page is decommissioned 
occurs in one  of its constituent 
achieve 
a lifetime 
4 1  x (variance=O
factor. 
up  to 6 faults  in 
improvement 
.3) over Fail_Stop. 
variation 
The third scheme, ECP, can 
each 64 Byte block and a 4KB 
as soon as the seventh 
64 Byte blocks. This  lets  ECP 
fault 
of 0.26x (variance=O
. l) to 
RePRAM uses off-the­
We use 21 different 
memory-intensive 
and CPU-intensive 
application 
Splash-2 
mix from SPEC2006 [27], PARSEC-l .O [ 1] and 
input sets. 
suites with reference 
[30] benchmark 
from 
We evaluate 
the performance 
SESC [23], a cycle-accurate, 
impact resulting 
processor 
four-core 
execution­
Our baseline system models an Intel 
[7] running at 3GHz,  4-
set­
L 1  and a shared 4MB, 16-way set-associative L2. 
RePRAM  using 
driven simulator. 
Nehalem-like 
way, out-of-order core, each with a private 32KB,  8-way 
associative 
The L1 caches are kept coherent 
The block size is 64Bytes 
main memory with 
50ns and write access latency 
we include 
an additional 
simultaneously 
buffer is full, we model  a 96000 cycle (32j.£s) latency 
write parity information 
We model 4GB PCM 
of 
of 1j.£s [ 1 6]. For storing 
during the  PRAM accesses. When DRAM 
4KB pages with read access latency 
1 6MB DRAM that can accessed 
using the MESI protocol. 
in all caches. 
parity, 
to 
to disk. 
To model RePRAM effects, 
every L2 access first queries 
latency to deter­
a bloom filter, that has a three-cycle 
mine whether the page is faulty or not. We  use 1 024-
entry CACHEMDR and CACHEpDR in our experiments, 
where we observe an average miss rate of 3%  (and 8% 
worst case). Note that we do not assume fixed latencies 
for 
memory lookups 
figuration 
to fully model memory read/write 
to M D R or P D R. Our con­
setup has a common memory bus between cores 
corresponding 
and bandwidth. 
latencies 
For faulty pages, in Dim-I, we access CACHEMDR 
each of which have a  2 
and CACHEpDR simultaneously 
In Dim-2, we just access CACHEpDR. On 
cycle latency. 
a mapping cache miss, we model an additional 
of 
500 cycles to perform mapping lookup  from  lower 
of memory hierarchy 
future use. Upon receiving 
a 20 cycle penalty 
faulty bytes from mirror copies. 
We show performance 
and store it in the mapping cache for 
all the pages in a group, we have 
for parity reconstruction 
on two sets of scenarios 
and recovering 
overheads 
latency 
levels 
for Dim-1 and Dim-2 configurations: 
• average case: In this scenario, 
we assume that a ran­
a randomly 
that it is faulty. 
to be faulty. 
picked 50% of  the PCM pages 
domly picked page has 50% probability 
Therefore, 
are considered 
pages (25% of the total pages) are mapped under  PDR 
(group size = 3), and 50% (25% of total pages) are 
under MDR in Dim-1 and under PDR (group size = 
2) in Dim-2. The remaining 
50% of total pages are 
assumed to be non-faulty . 
Of these,  50% 
of faulty 
• stress 
case: In this scenario, 
we assume that a randomly 
Of these, a  randomly 
picked page is always faulty. 
picked 50% of the PCM pages are mapped  under 
(group size = 3). The remaining 
50% pages are under 
MDR in Dim-I, and under PDR (group size = 2) in 
Dim-2. 
PDR 
Our RePRAM  schemes 
achieve the lifetime  results 
that 
than, the ECP scheme at a small 
we 
shrinks 
In Dim-I, we 
to degrade 
to, or better 
size of 3 to tolerate 
under PDR, PRAM capacity 
cost. In both dimensions, 
from PCM data 
80 faults 
shown in Figure 1 ), we switch 
gracefully 
onto each other and 
fault threshold 
parity separately 
and after the PCM blocks begin to exhibit 
only 
1 60 bit failures. This lets 
in PDR. However, 
more rapidly. 
deploy PDR with  the  group 
are comparable 
fraction of the ECP's hardware 
first deploy PDR that stores 
pages. Therefore, 
when PCM pages have  reached 
the PRAM capacity 
under MDR,  PRAM pages are mirrored 
the PRAM capacity  degrades 
initially 
faults, 
(three-way 
to MDR for the rest of PCM block's 
that Dim-1 design yields the lifetime 
worse than ECP ( 1 %  when the variance 
the variance 
hardware 
off-the-shelf 
to main memory design needed by ECP. Under Dim-2, we 
also deploy PDR with  the 
stages, 
we  switch 
of PCM bock's lifetime. We find that Dim-2 exhibits 
lifetime 
. l  to 4.4% when 
the variance=O
achieve 
0.26x (when the variance 
variance 
the proposed 
improvements 
is 0.1 to 9.5% when 
the 
most 
to PDR with the group size of 2 for the rest 
and after the PCM blocks begin to exhibit 
than ECP (0.5% for the variance=O
of 0.3 in Dim-2) over Fail_Stop. 
versus the extensive 
for PCM, ranging 
lifetime. We  can see 
good lifetime 
components 
.3). Overall, 
group size of 3 in  the initial 
80 faults, 
RePRAM schemes 
is 0.1 in Dim-I )  to 43x (for  the 
cost perspect
that are 
slightly 
results 
better 
from 
modifications 
is 0.3). This shows a good  trade-off  from 
ive, as our design  involves  using 
One might argue about  why 
low-cost 
schemes such as 
lifetime 
custom PRAM design, 
RePRAM should be advantageous 
schemes have  comparable 
ECP requires 
solution 
less flexibility 
enhancement 
shelf modules and minimizes 
adoption 
techniques. In contrast, 
hardware 
and for future upgrades. 
at about 12% memory overhead 
over ECP, when both 
results. 
We note that 
and offers a hardwired 
where there is 
changes for easier 
for users to upgrade to further  lifetime­
• Dim-1 
• Dim-2 
SPEC2006 
PARSEC 
( a) Average case 
SPLASH2 
• Dim-1 
• Dim-2 
 7% 
6% 5% 4% 3% 2% 1% 0% ....  N 
cu -'=  o 
e: c III E ... .g  
III  a. 
t;) ON 
111 .0 
 18% 
 16% 
... 14% 
 12% 
 10% 
 8% 
III 6% 
E 4% 
.g 2% 
 0% 
E'ti .3 E 
c: III ::::J C" 
@ 
SPEC2006 
Figure 4. Performance 
x m  tl.O lIl  
cu  x  >  cu 
- c:  III -
a. ._ 
0 
O -'= U-'= 
VJ Q. UJ u  
III  Co.  III 
Vl U "' J5 
PARSEC 
(b) Stress case 
SPLASH2 
Overheads 
of RePRAM on Spec2006, 
PARSEC-l.O 
and Splash-2 
applications. 
individually 
one at a time. For 
benchmarks, 
we spawn four parallel 
one each on every core that share L2 and lower 
applications 
'\\1\ 
0.' 1--------------
'Ht+H-
t-
0.81=====;----1-
+'1 ..• LL-
>  1-----1  -GO faults 
1------
:0 0.7 
n 
! 0.6 
"'SOfaults 
I-------
H-i-
Ie : 
 0.5 1------1 *""10010ulls 
-H+'i--
tl 0.4 1------1 -·12010ulls 
I- ------
_-_-_-_-_-_-_-_-_-_-_-_-_-
is :: :====_"'_"'_14_0_IO_UIIs
+1 r 
-+I
0.1 1---------------
H1:!i-
u 
as single-core 
PARSEC and Splash2 
threads, 
level memory substructures. 
followed 
I--_+
Writes 
to PCM memory (trillions, 
overhead 
performance 
by 5.9% for gcc  in Dim-
PARSEC and Splash2 
Figure 4(a) shows the performance 
of 1.87%, 0.8%, and 
respectively, 
impact of our 
We see 
show the overheads 
less 
(6.2%) 
RePRAM schemes in  the average case scenario. 
that, all 21 of our applications 
than 7%, and the highest 
occurs in cactusADM, 
2. Under Dim-I, we notice an average 
1.5 %  across SPEC2006, 
whereas the corresponding 
0.8%, and 1.7%. We  notice 
rate in Dim-2 (due to continuous 
along with a higher rate of misses in mapping caches 
contribute 
effect is especially 
cactusADM, 
group pages creates 
and degrades 
and volrend. 
and mef. Also,  issuing 
under Dim-2 are 2.2%, 
in benchmarks 
performance 
performance 
an increased 
to increased 
pronounced 
contention 
such as fft, ocean, 
averages 
memory access 
impact than Dim-I. This 
such as gcc, 
requests  for 
in benchmarks 
multiple 
use of PDR configuration) 
for memory bus bandwidth 
For SPEC2006 and PARSEC applications, 
the first five billion 
billion 
from start to end. In addition, 
applications, 
we run SPEC2006 benchmarks 
the next one 
we simulate 
them 
instructions 
we fast forward 
and simulate 
For Splash2 
in detail. 
Figure 5. Lifetime 
variance=O.2). 
number of faults, 
PCM block's 
lifetime. 
comparison 
for various 
For each configuration, 
configurations 
of Dim-l (at 
we use PDR until the specified 
and later we switch over to MDR for the rest of the 
24% 
"0 :ll 21% 
of 18% 
cu  15% 
:!l 12% 
c 
E 9% 
..  6% 
.g cu  3% 
Q.  0% L.  N ro a. ..... 'N 
I/) 
ro ..c 
.4MB D16MB D32MB 
0 
..c 
IlII  :::l 
..... a.   ..>J!.  0  x ..., I/) 
..... I: Q) 
u  ..>J!.  E 
Q) I/)  L.  Qj I/) >-
ro  0 ..... ..>J!.  I: I/) 
u  a. u  E  I- Q) x Q) 
E ..... I:  0  a.  I: '0 
u  E 
..c ..... 
Q) ..>J!. 
'';::; :::l ro ro 
Q) Q) u « u.. 0 :c  
I/) U  L.  Q) 
'c a. W  L.  ..c '0 
I: ..c  I: I/) I/) I/) a. u 
0 I: 
ro ..... 
IlII ro 
"iii u  Q) 
E   :::l E 
u 
I/) I/) 
:::l c-
o 
"0 
u 
..>J!. 
;:;:::  ..... I/) 
@ 
ro 
x  \!) 
::is 
PARSEC 
u 
SPEC2006 
ro 
ro 
E 
3: 
I/) ro 
:::l   
>-
0 
..c 
ro 
a. 
E 
'ti  I: I/) 
ro ..... u 
:E x N  a. E I: >- ..>J!. .=! 
"0 I: 
  I  E Q) 'iii  
Q) Q) ..... u  0 ..... 
  L. 
..... ..... 0 'ti >-
Q) 
L. 
'0 
> 
ro ro 
L. 
ro ro 
3: 
3: 
L. 
SPLASH2 
Figure 6. Performance 
overheads 
for different 
sizes of DRAM buffer in PDR (average 
case). 
Die 
263 mm'2  [7] 
Processor 
2.25 mm'2 
Bloom Filter 
CACHEpDR 
0.08 mm".l 
CACHEMDR 
0.08 mm".l 
On-chip area overhead  0.92% 
16  MB DRAM Buffer overhead 
(compared 
to 4GB PRAM)  <0.5% 
AREA OVERHEADS 
OF ON-CHIP MAPPING CACHES, BLOOM FILTER AND 
OFF-CHIP DRA M  BUFFER IN REPRAM. 
Table I 
Figure 4(b)  shows 
the performance 
impact of our 
In Dim-I, 
less than 16%, 
overhead 
( 1 5.5%) occurs in gcc, followed 
We see that, 
scenario. 
show the overheads 
PARSEC and Splash2 
averages  in  Dim-2 
RePRAM schemes in  the stress  case 
all 2 1  of our applications 
and the highest 
by 1 2.4% for cactusADM in the Dim-2 design. 
we notice an average of 3 .78%, 1.64%, and 2.65% across 
SPEC2006, 
corresponding 
3.16%. Due to multiple  memory 
the group size) and demand for mapping cache entries, 
notice an increased 
than Dim-l across various 
this effect is seen in benchmarks 
fluidanimate. 
formance impact in benchmarks 
and ocean. 
such as gcc, mef, and 
incurs per­
fft, 
average performance 
such as cactusADM, 
Similarly, issuing 
impact in Dim-2 
benchmark 
multiple  requests 
respectively, 
whereas the 
are 4.66%, 2.71 %, and 
accesses (depending  on 
Particularly
suites. 
we 
, 
C. Area Overheads 
D. Sensitivity 
Experiments 
We  use Cacti 5 . 3  [ 1 3], an integrated 
model for cache 
and memory access time, cycle time, area, leakage and 
dynamic power. We  use this tool to model our two 1 024-
entry mapping caches, 
has  space 
processor 
would be integrated 
use 45 nm technology 
PCM page entries 
assume that these hardware 
with an on-chip 
to store 1 million 
chip.  We 
node in our experiments. 
memory controll
er. We 
and a compact bloom filter that 
inside the 
structures 
Table I shows the  area 
estimates 
of our proposed 
can be easily 
integrated 
less than 1 % and our proposed 
We find that our area overheads 
both 
RePRAM hardware. 
on-chip and off-chip  are 
hardware 
with minimal changes.  We 
than prior works such as ECP [24] that incur substantial 
area overheads 
tional circuitry 
correcting 
to  store 
like row-decoders 
note that this is much cheaper 
in order to store the error 