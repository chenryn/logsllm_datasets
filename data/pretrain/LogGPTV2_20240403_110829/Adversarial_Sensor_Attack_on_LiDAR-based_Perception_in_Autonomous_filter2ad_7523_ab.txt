such as stopping, lane changing, yielding, etc.
Description
Maximum height of points in the cell.
Intensity of the brightest point in the cell.
Mean height of points in the cell.
Feature
Max height
Max intensity
Mean height
Mean intensity Mean intensity of points in the cell.
Count
Direction
Distance
Non-empty
Number of points in the cell.
Angle of the cell’s center with respect to the origin.
Distance between the cell’s center and the origin.
Binary value indicating whether the cell is empty or occupied.
Table 1: DNN model input features.
Metrics
Center offset
Objectness
Positiveness
Object height
Class probability
Description
Offset to the predicted center of the cluster the cell belongs
to.
The probability of a cell belonging to an obstacle.
The confidence score of the detection.
The predicted object height.
The probability of the cell being a part of a vehicle, pedes-
trian, etc.
Table 2: DNN model output metrics.
2.2 LiDAR Sensor and Spoofing Attacks
To understand the principles underlying our security analysis
methodology, it is necessary to understand how the LiDAR sen-
sor generates a point cloud and how it is possible to alter it in a
controlled way using spoofing attacks.
LiDAR sensor. A LiDAR sensor functions by firing laser pulses
and capturing their reflections using photodiodes. Because the
speed of light is constant, the time it takes for the echo pulses to
reach the receiving photodiode provides an accurate measurement
of the distance between a LiDAR and a potential obstacle. By firing
the laser pulses at many vertical and horizontal angles, a LiDAR
generates a point cloud used by the AV systems to detect objects.
LiDAR spoofing attack. Sensor spoofing attacks use the same
physical channels as the targeted sensor to manipulate the sen-
sor readings. This strategy makes it very difficult for the sensor
system to recognize such attack, since the attack doesn’t require
any physical contact or tampering with the sensor, and it doesn’t
interfere with the processing and transmission of the digital sensor
measurement. These types of attack could trick the victim sensor
to provide seemingly legitimate but actually erroneous data.
LiDAR has been shown to be vulnerable to laser spoofing attacks
in prior work. Petit et al. demonstrated that a LiDAR spoofing
attack can be performed by replaying the LiDAR laser pulses from
a different position to create fake points further than the location of
the spoofer [42]. Shin et al. showed that it is possible to generate a
fake point cloud at different distances, even closer than the spoofer
location [44]. In this paper, we build upon these prior work to study
the effect of this attack vector on the security of AV perception.
2.3 Adversarial Machine Learning
Neural networks. A neural network is a function consisting of
connected units called (artificial) neurons that work together to rep-
resent a differentiable function that outputs a distribution. A given
neural network (e.g., classification) can be defined by its model
architecture and parameters ϕ. An optimizer such as Adam [35]
is used to update the parameters ϕ with respect to the objective
function L.
Adversarial examples. Given a machine learning model M, in-
put x and its corresponding label y, an adversarial attacker aims
to generate adversarial examples x′ so that M(x′) (cid:44) y (untargeted
attack) or M(x′) = y′, where y′ is a target label (targeted attack).
Carlini and Wagner [21] proposed to generate an adversarial per-
turbation for a targeted attack by optimizing an objective function
as follows:
M(x
′) = y
′
and x
′ ∈ X ,
min ||x − x
′||p
s.t.
where M(x′) = y′ is the target adversarial goal and x′ ∈ X de-
note that the adversarial examples should be in a valid set. Further,
optimization-based algorithms have been leveraged to generate
adversarial examples on various kinds of machine learning tasks
successfully, such as segmentation [26, 53], human pose estima-
tion [26], object detection [53], Visual Question Answer system [54],
image caption translation [24], etc. In this paper, we also leverage
an optimization-based method to generate adversarial examples to
fool LiDAR-based AV perception.
3 ATTACK GOAL AND THREAT MODEL
Attack goal. To cause semantically-impactful security consequence
in AV settings, we set the attack goal as fooling the LiDAR-based
perception into perceiving fake obstacles in front of a victim AV in
order to maliciously alter its driving decisions. More specifically,
in this work, we target front-near fake obstacles, i.e., those that
are in close distances to the front of a victim AV, since they have
the highest potential to trigger immediate erroneous AV driving
decisions. In this work, we define front-near obstacles as those that
are around 5 meters to the front of a victim AV.
Threat model. To achieve the attack goal above, we consider Li-
DAR spoofing attacks as our threat model, which is a demonstrated
practical attack vector for LiDAR sensors [42, 44] as described
in §2.2. In AV settings, there are several possible scenarios to per-
form such attack. First, the attacker can place an attacking device
at the roadside to shoot malicious laser pulses to AVs passing by.
Second, the attacker can drive an attack vehicle in close proximity
to the victim AV, e.g., in the same lane or adjacent lanes. To perform
the attack, the attack vehicle is equipped with an attacking device
that shoots laser pulses to the victim AV’s LiDAR. To perform laser
aiming in these scenarios, the attacker can use techniques such as
camera-based object detection and tracking. In AV settings, these
attacks are stealthy since the laser pulses are invisible and laser
shooting devices are relatively small in size.
As a first security analysis, we assume that the attacker has
white-box access to the machine learning model and the perception
system. We consider this threat model reasonable since the attacker
could obtain white-box access by additional engineering efforts to
reverse engineering the software.
4 LIMITATION OF BLIND SENSOR SPOOFING
To understand the security of LiDAR-based perception under LiDAR
spoofing attacks, we first reproduce the state-of-the-art LiDAR
spoofing attack by Shin et al. [44], and explore the effectiveness of
directly applying it to attack the LiDAR-based perception pipeline
in Baidu Apollo [4], an open-source AV system that has over 100
partners and has reached mass production agreement with multiple
partners such as Volvo, Ford, and King Long [8, 9].
Spoofing attack description. The attack by Shin et al. [44]
consists of three components: a photodiode, a delay component,
and an infrared laser, which are shown in Fig. 3. The photodiode
is used to synchronize with the victim LiDAR. The photodiode
triggers the delay component whenever it captures laser pulses
fired from the victim LiDAR. Then the delay component triggers
the attack laser after a certain amount of time to attack the following
firing cycles of the victim LiDAR. Since the firing sequence of laser
pulses is consistent, an adversary can choose which fake points will
appear in the point cloud by crafting a pulse waveform to trigger
the attack laser.
Experimental setup. We perform spoofing attack experiments
on a VLP-16 PUCK LiDAR System from Velodyne [32]. The VLP-16
uses a vertical array of 16 separate laser diodes to fire laser pulses
at different angles. It has a 30 degree vertical angle range from
-15 ◦ to +15 ◦, with 2 ◦ of angular resolution. The VLP-16 rotates
horizontally around a center axis to send pulses in a 360 ◦ horizontal
range, with a varying azimuth resolution between 0.1 ◦ and 0.4 ◦.
The laser firing sequence follows the pattern shown in Figure 4. The
VLP-16 fires 16 laser pulses in a cycle every 55.296 µs, with a period
of 2.304 µs. The receiving time window is about 667 ns. We chose
this sensor because it is compatible with Baidu Apollo and uses
the same design principle as the more advanced HDL-64E LiDARs
used in many AVs. The similar design indicates that the same laser
attacks that affect the VLP-16 can be extended to high-resolution
LiDARs like the HDL-64E.
We use the OSRAM SFH 213 FA as our photodiode, with a com-
parator circuit similar to the one used by Shin et al. We use a Tek-
tronix AFG3251 function generator as the delay component with
the photodiode circuit as an external trigger. In turn, the function
generator provides the trigger to the laser driver module PCO-7114
that drives the attack laser diode OSRAM SPL PL90. With the PCO-
7114 laser driver, we were able to fire the laser pulses at the same
pulse rate of the VLP-16, 2.304 µs, compared to 100 µs of the pre-
vious work. An optical lens with a diameter of 30mm and a focal
length of 100 mm was used to focus the beam, making it more
effective for ranges farther than 5 meters. We generate the custom
pulse waveform using the Tektronix software ArbExpress [2] to
create different shapes and the Velodyne software VeloView [10] to
analyze and extract the point clouds.
Experiment results. The prior work of Shin et al. is able to
spoof a maximum of 10 fake dots in a single horizontal line. With
our setup improvements (a faster firing rate and a lens to focus the
beam), fake points can be generated at all of the 16 vertical viewing
angles and an 8 ◦ horizontal angle at greater than 10 meters away.
In total, around 100 dots can be spoofed by covering these horizon-
tal and vertical angles (illustrated in Fig. 14 in Appendix). These
spoofed dots can also be shaped by modifying the custom pulse
waveform used to fire the attack laser. Noticed that even though
around 100 dots can be spoofed, they are not all spoofed stably.
The attacker is able to spoof points at different angles because the
spoofed laser pulses hit a certain area on the victim LiDAR due to
the optical lens focusing. The closer to the center of the area, the
stronger and stabler laser pulses are received by the victim LiDAR.
We find that among 60 points at the center 8-10 vertical lines can
be stably spoofed with high intensity.
Figure 2: Overview of the Adv-LiDAR methodology.
Figure 5: Generating the attacker-perturbed 3D point cloud
by synthesizing the pristine 3D point cloud with the attack
trace to spoof a front-near obstacle 5 meters away from the
victim AV.
we synthesize the on-road attack effect by adding spoofed LiDAR
points to the original 3D point cloud collected by Baidu Apollo
team on local roads at Sunnyvale, CA. The synthesizing process is
illustrated in Fig. 5. After this process, we run Apollo’s perception
module with the attacker-perturbed 3D point cloud as input to
obtain the object detection output. In this analysis, we explore
three blind attack experiments as follows:
Figure 3: Illustration of LiDAR spoofing attack. The photo-
diode receives the laser pulses from the LiDAR and activate
the delay component that triggers the attacker laser to sim-
ulate real echo pulses.
Figure 4: The consistent firing sequence of the LiDAR allows
an attacker to choose the angles and distances from which
spoofed points appear. For example, applying the attacker
signal, fake dots will appear at 1◦, 3◦, -3◦, and -1◦ angles (0◦
is the center of the LiDAR)
4.1 Blind LiDAR Spoofing Experiments
After reproducing the LiDAR spoofing attack, we then explore
whether blindly applying such attack can directly generate spoofed
obstacles in the LiDAR-based perception in Baidu Apollo. Since our
LiDAR spoofing experiments are performed in indoor environments,
Experiment 1: Directly apply original spoofing attack traces.
In this experiment, we directly replay spoofing attack traces to
attack LiDAR-based perception in Apollo. More specifically, we
experiment with attack traces obtained from two sources: (1) the
original spoofing attack traces from Shin et al. [44], and (2) the
attack traces generated from the spoofing attack reproduced by us,
which can inject more dots after our setup improvements. However,
we are not able to observe a spoofed obstacle for any of these traces
at the output of the LiDAR-based perception pipeline.
Experiment 2: Apply spoofing attack traces at different
angles. To understand whether successfully spoofing an obstacle
depends on the angle of the spoofed points, in this experiment we
inject spoofed points at different locations. More specifically, we
uniformly sample 100 different angles out of 360 degrees around
the victim AV, and inject the spoofing attack traces reproduced by
us. However, we are not able to observe spoofed obstacles for any
of these angles.
Experiment 3: Apply spoofing attack traces with different
shapes. To understand whether successfully spoofing an obstacle
depends on the pattern of the spoofed points, in this experiment we
inject points with different spoofing patterns. More specifically, we
generate random patterns of spoofed points by randomly setting
adversarial machine learning analysis in this research problem, and
then present our solution methodology overview, called Adv-LiDAR.
5.1 Technical Challenges
Even though previous studies have shown promising results in
attacking machine learning models, none of them studied LiDAR-
based object detection models, and their approaches have limited
applicability to our analysis goal due to three challenges:
First, attackers have limited capability of perturbing machine
learning model inputs in our problem. Other than perturbing pixels
on an image, perturbing machine learning inputs under AV settings
requires perturbing 3D point cloud raw data by sensor attack and
bypassing the associated pre-processing process. Therefore, such
perturbation capability needs to be quantified and modeled.
Second, optimization-based methods for generating adversarial
examples in previous studies may not be directly suitable for our
analysis problem due to the limited model input perturbation capa-
bility. As shown in §7, we find that optimization-based methods are
inherently limited due to the nature of our problem, and can only
achieve very low success rate in generating front-near obstacles.
Third, in our problem, successfully changing the machine learn-
ing model output does not directly lead to successes in achieving our
attack goal in AV settings. As detailed later in §7, in AV systems such
as Baidu Apollo, machine learning model output is post-processed
before it is converted to a list of perceived obstacles. Thus, an ob-
jective function that can effectively reflect our attack goal needs to
be newly designed.
5.2 Adv-LiDAR Methodology Overview
In this section, we provide an overview of our solution methodology,
which we call Adv-LiDAR, that addresses the three challenges above.
At a high level, to identify adversarial examples for the machine
learning model M, we adopt an optimization-based approach, which
has shown both high efficiency and effectiveness by previous studies
for machine learning models across different domains [21, 26, 51,