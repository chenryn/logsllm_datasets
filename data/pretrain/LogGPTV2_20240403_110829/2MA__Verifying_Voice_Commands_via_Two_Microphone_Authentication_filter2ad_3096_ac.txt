posed approach dramatically increases the difficulty of launching
command injection attacks and is practical on currently deployed
hardware.
7.1 Volume and Phone Command Recognition
We tested the mobile device’s ability to detect audio commands at
various volumes. Our aim was to determine the minimum volume
at which a home assistant device is activated by a command. We
assume that the microphones in both mobile phones and home
assistants are similar; therefore, the results are applicable to both
kinds of devices.
This experiment was conducted in a quiet environment with a
background noise between 20dB-25dB. This level of background
noise is similar to that of a quiet home. We used the Music Angel
Speaker to play a single “OK Google”, command at various volumes.
Our two Nexus 6ps mobile devices were used as receivers during
this test. One mobile device was used to measure the volume at a
set distance from the speaker while the other device was used to
see if the Google Assistant could detect the command at the same
distance as the other mobile device.
Starting at the initial volume of 25dB, we incrementally increased
the volume of our speaker by 5dB, until we reach a volume of 45dB.
We found that the minimum volume that a home assistant could
reliability detect a command (100% detection rate) was 40dB. At
it quietest, the assistant could properly detect a command around
30dB (i.e., noise levels of a quiet conversation) with a reliability of
40%. Given that all these decibel levels are audible to the human
hearing, a user could stop any maliciously injected command if
they are near the home assistant. For 2MA systems, this implication
allows us to leverage the user as an additional security check against
maliciously injected commands targeted at the home assistant.
7.2 Localization Based on Audio Degradation
Our second and third experiments were designed to address a seem-
ingly simple solution - using audio degradation over distance. Be-
cause the intensity of audio decreases with distance, the volume
of an audio command should be greater at a closer point than fur-
ther away. This information can be used to determine the relative
location of the mobile device and home assistant. However, our
experiments demonstrate that such an approach is unreliable.
We performed the experiment in the same quiet environment
mentioned above. The speaker was placed at a fixed location while
mobile devices were placed at various distances from the speaker.
The speaker played a 400Hz tone at a constant volume for 10 sec-
onds. Both mobile devices were placed between 1 and 20 feet (0.3-6.1
meters) from the speaker at 1 foot (0.3 meter) increments. Our ex-
perimental data is shown is Figure 7.
Figure 7 demonstrates that there is a weak relationship between
2 = 0.513). However, even with a
the distance and the volume (r
constant audio source, the variation in the volume at any given
distance is too great to tightly bound distance to volume. We believe
that this variation can be attributed to hardware limitations and en-
vironmental effects such as reflection and constructive/destructive
interference. Without a tight relationship, volume based distance
Session 3: AuthenticationASIACCS’18, June 4–8, 2018, Incheon, Republic of Korea95ASIA CCS ’18, June 4–8, 2018, Incheon, Republic of Korea
Logan Blue, Hadi Abdullah, Luis Vargas, and Patrick Traynor
Devices
Phone Home
Hub
89.49%
Galaxy
Nexus
Pixel
85.64%
Nexus
Nexus
Nexus
87.18%
93.34%
Galaxy Galaxy
Pixel
Pixel
68.46%
Correct Percentage
Hallway
23.77%
21.38%
31.43%
65.18%
65.95%
Table 1: Using attenuation to distance bound two devices
works well in open environments, such as homes. However,
it does not work as well in high reverb environments like
hallways .
Figure 7: Our experiments reveal that the relationship be-
tween the volume and the distance is not tight. For example,
when the difference in volume at two receivers is 10dB, the
difference in distance the sound has to travel between the
source and the two receiver can be from 43ft to 62ft. The
existence of this high variance disqualifies volume as an ef-
fective distance bounding metric.
bounding appears unfit for 2MA systems in general because it
cannot reliably determine which device is closer.
7.3 Frequency Ratios and Audio Attenuation
We also tested localization based on the frequency of an audio
command. Naturally, higher frequencies attenuate faster than lower
ones [34]. We used this observation to detect which of the two
devices is closer to the audio source.
The experiment setup is similar to that of Section 7.2, except for
the addition of two different types of mobile devices, the Google
Pixel and Samsung Galaxy S8. We use two phones of each type,
one phone acting as the home assistant and the other as the user’s
mobile device. During the experiment, we use all five different
combinations of the home assistants and phones by replacing the
phone types as needed.
Each test consisted of playing a piece of audio from the TIMIT
database [2] over the speaker at a fixed volume. The two mobile de-
vices were placed at unequal distances from the audio source. Each
device recorded the audio6 at their respective locations. We then
divided the audio sample into eight frequency bands (150Hz-500Hz,
500Hz-1kHz, 1kHz-1.5kHz, 1.5kHz-2kHz, 2kHz-2.5kHz, 2.5kHz-
3kHz, 3kHz-3.5kHz, 3.5kHz-4kHz) and found the peak volume
within each band.
Based on these peak volumes, we created an attenuation ratio
for every measured audio sample using:
ratio = peakhiдh/peaklow
(4)
where peakhiдh is the peak volume from a higher frequency band
and peaklow is the peak volume from a lower frequency band. We
then compared these ratios to determine which device is closer to
6All recordings were done at 8kHz sampling rates.
the source. A higher attenuation ratio means that a device is closer
to the audio source.
In Table 1, we show how often we were able to correctly predict
whether an audio source was closer to the home assistant or a mo-
bile device. In the case of our quiet room experiments, all of the
scenarios (except for the pixel/pixel) achieved a success rate of at
least 85%. However, in a reverberant environment, accuracy for all
scenarios dropped to between 21%-65%. We believe this is due to the
high amounts of reverberation within the space. Specifically, higher
frequencies reflect cleaner (with less loss) than lower frequencies.
It stands to reason that the reverberation in the environment is
artificially increasing the volume of the high frequencies signifi-
cantly more than the lower frequencies within the audio samples.
By unevenly amplifying the frequency domain, reverberation will
cause this type of distance bounding to fail.
We have shown that frequency based localization is not suitable
for authentication in our scenario. The frequency attenuation ratios
are not robust in environments with high reverberation and thus
unfit for a general 2MA system.
7.4 2MA Direction Bounding
We then tested the DOA techniques proposed for our embodiment
of a 2MA system. We use a Raspberry Pi B+ (2014) running Rasp-
bian Stretch as a stand in for a home assistant. We connected our
Raspberry Pi to a “Respeaker 4 Microphone Array” as our home
assistant device. The Respeaker array has 4 microphones spaced
5.9 cm apart in a square and is able to make 4 channel recordings
at sample rates up to 44100 Hz. It is important to note that the
Respeaker 4 Microphone Array is connected directly to the Rasp-
berry Pi via its 40 GPIO pin connection. We placed our Pi in a quiet
environment for testing.
The command data set consisted of 4 channel recordings that
were generated at a fixed distance and angle from the home assis-
tant. After recording the audio, we calculated the time the waveform
reached each channel by locating the first point where the ampli-
tude of the waveform broke a given threshold. This was repeated
four times, once for each microphone. Then these times were used
to calculate the DOA for the command. The chirp data set was
generated using high frequency tones played from our Music Angel
speaker. For each data set we generated 20 samples at 6 different
locations around our microphone array. Specifically these points
are 0°, 10°, 20°, 30°, 90°, and 180° around the device from a set 0°
point.
Session 3: AuthenticationASIACCS’18, June 4–8, 2018, Incheon, Republic of Korea962MA: Verifying Voice Commands via
Two Microphone Authentication
ASIA CCS ’18, June 4–8, 2018, Incheon, Republic of Korea
Figure 8: While the false positive rate of our DOA system seems to vary with the uncertainty cone, the false negative rate
seems to remain consistent.
Next we cleaned our two data sets by examining the relative time
of arrival between each of the four microphones located on the array.
That is, due to the physical properties of the device, the command’s
arrival at each microphone can differ from the other microphones
on the array by at most 11 samples. This number was determined
by calculating the distance an audio wave travels within 2.2 ∗ 10−5
seconds, or one sample at our 44100 Hz sampling rate. We then took
the maximum distance between any of the two microphones on
the array (5.9 cm for adjacent microphones, 8.3 cm for diagonally
microphones) and divided that by the distance traveled by the
wave during a single sample. Recordings beyond 11 samples apart
represented some other action (e.g., context switching) in the Pi
and were therefore discarded.
Using our cleaned data sets, we then constructed a comparison
to simulate a benign setting. In this comparison we derived how
precisely we can calculate the DOA using our current hardware and
signal processing. This consisted of matching runs originating from
the same location from both our command data set and our chirp
data set. Effectively this places a mobile device and the speaker
of the command at the same approximate location. We found an
average difference between our calculated DOA angles of 4.6° with
a standard deviation of 3.4°. From these values we estimate that
our uncertainty cone should be approximately ±15° degrees from
the chirp DOA to ensure a low false positive rate. In fact, within
our experimental set of 80 runs, 0 of them were misidentified as
malicious command with an uncertainty cone of ±15°. By tighten-
ing our uncertainty cone to ±5° and ±10° we found that our DOA
mechanism would have misidentified 32 and 7 pairs respectively.
We then constructed a comparison to mimic an adversarial set-
ting. Similar to before, we compared runs from our command data
set that originated from 0°. However, in contrast we compared it to
all other runs in the chirp data set that originated from a location
that was greater that our uncertainty away. After running all 440
adversarial comparisons, we found that at our preferred uncertainty
cone of ±15° had a false negative rate of 4.5%, misidentifying only
13 of our tests. Unlike in our benign testing, the uncertainty val-
ues of ±5° and ±10° degrees did not perform considerably worse.
Respectively, ±5° and ±10° achieve false negative rates of 3.9% and
5.8% only misidentifying 14 and 21 of our tests. Our adversarial test
data can be seen in the context of a typical room in Figure 8
It is important to assess false negatives in context. Were these
messages attempted with a user outside of the room, none would
be successful. If the user is in the room, they would need to verbally
cancel approximately 4 out of every 100 messages the attacker in-
jected (as opposed to all 100 without a 2MA system). Accordingly,
our proposed techniques dramatically increase the difficulty of suc-
cessfuly launching such an attack while minimizing burden on a user
to respond.
7.5 2MA Audio Similarity Filter
We tested the reliability of the filter against adversarial command
injections. We used Google’s Speech Recognition API as our speech
to text (STT) module and Google’s Text to Speech python library
as our text to speech (TTS) module.
One of the metrics we tested for was the reliability of our audio
similarity filter. In other words, we wanted to know the number of
±15 Uncert.FP:0.0%FN:4.5%±5 Uncert.FP:40%FN:3.8%±10 Uncert.FP:8.8%FN:5.8%Session 3: AuthenticationASIACCS’18, June 4–8, 2018, Incheon, Republic of Korea97ASIA CCS ’18, June 4–8, 2018, Incheon, Republic of Korea
Logan Blue, Hadi Abdullah, Luis Vargas, and Patrick Traynor
greater than the threshold came from the first second in the audio
sample. We believe this drop in reliability can be accounted to the
late detection of activation phrase in the first second.
Adversarial Audio. An adversary can give the home assistant a
malicious command in three different ways: audible, inaudible and
hidden. Once a command reaches the audio similarity filter, our
2MA system has already indicated that the user and the source of
the command are co-located. In this case the adversary can only
broadcast the audio command from the same general direction
as the user, we expect that the user will over hear the malicious
command and take necessary action. However, that is not true in
the case of inaudible or hidden commands.
A hidden command is an audio command that has been obfus-
cated or “hidden” by noise [20]. This means that the user will not
detect the audio, but merely hear a sound. The authors of the attack
provide us with ten different hidden command audio samples. We
used these to test the audio similarity filter. Of the ten samples
provided, eight samples were not transcribed by the STT module8.
Therefore, the audio similarity filter automatically rejected these
command injections. One of the two adversarial audio clips that was
in fact transcribed by the STT module, the phrase “OK Google”, was
only an activation phrase and not an actual command. The other
command was partially transcribed from the original audio, which
was later rejected as an actual command by the second speech
digest comparison of our filter.
Finally, we also tested our audio similarity filter against inaudi-
ble commands [45]. An inaudible command is one that can not
be heard by the normal ear as it exists in the ultrasound range.
The authors provided us with four adversarial raw audio samples,
with commands encoded within frequencies of 24kHz and 32kHz.
These samples were passed through the audio similarity filter. In-
terestingly, the STT module did not transcribe any of the samples
and rejected all of them as actual commands. Although none of
the attack audios samples passed our audio similarity filter, we do
not claim our 2MA system to necessarily address these unknown
attacks.
8 DISCUSSION
8.1 Limitations
No system is perfect, especially those that operate in an analog
environment. Accordingly, our embodiment may fail to detect some
malicious commands under certain conditions. This includes an
adversary who is located within our system’s uncertainty cone. We
assume that in such a scenario, the owner will hear the malicious
audio commands and take the necessary action. Concretely, 2MA
treats the owner as an additional layer of security. Therefore, 2MA
will not be able to detect a malicious command if the adversary
generates it after first subduing the real owner or if the real owner
has difficulty hearing. We believe that our system could easily be
extended for the hearing impared to include confirmation displays
on the mobile device; however, we leave such extensions to future
work.
Figure 9: Most of the speech digest error can be attributed to
the first second of recording, which includes the activation
phrase “OK Google”.
times a benign command would successfully pass the audio similar-
ity filter. To determine this, we first performed a control experiment
to test the reliability of the TTS and STT modules of our similarity
filter in regards to the second speech digest comparison7. Using the
microphone built into a Macbook Air 2015, we recorded ourselves
speaking four different command (e.g.,“OK Google, call 911”, “OK
Google, set an alarm”, “OK Google, how’s the weather?”, and “OK