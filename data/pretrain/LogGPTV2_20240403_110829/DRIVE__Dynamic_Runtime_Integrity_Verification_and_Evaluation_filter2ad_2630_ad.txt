Kernel Image Veriﬁcation. As previously mentioned, the
Linux kernel image is statically linked to predeﬁned ad-
dresses. This means its .text segment does not depend
on addresses determined during runtime; thus, it is possi-
ble to pre-calculate a RV for the kernel, add its RV and
compare mhd from the DML to the RV. Accordingly, we
implemented a mechanism calculating a kernel RV hash di-
gest and our veriﬁcation component detects illicit runtime
tampering of the kernel’s .text segment. But, in contrast to
statically linked user-space objects, the kernel implements
sophisticated ﬁx-up mechanisms, self-applied very early dur-
ing its initial loading phase, as described by Kittel et al. [16].
These ﬁx-ups are architecture dependent and speciﬁc to the
particular CPU and MMU of the system. Therefore, we re-
implemented necessary mechanisms to mimic these ﬁx-ups
in order to pre-calculate reference values for our evaluated
systems . As it turned out, once the ﬁx-ups are applied, the
kernel’s RV remains constant for a particular hardware con-
ﬁguration and, thus, can be reused. It has to be noted that
we disabled mechanisms that facilitate runtime code patch-
11E.g. in PPC 32 certain relative jumps exceed a maximal
jump-length of 24 Bit for a target symbol address. This can
only occur in kernel-space, due to its size.
ing (i.e. jump labels and ftrace). However, we are aware of
these mechanisms and will consider them in future work.
5.2 Concept and Implementation Evaluation
In this section, we evaluate the implementation for the
DRIVE concept and verify that diﬀerent attack techniques
are detected by their designated veriﬁcation modules.
According to our threat analysis presented in Section 2,
we evaluated the /bin/bash application regarding the men-
tioned attack techniques, as follows:
(1) Create new executable segment: Load an unreferenced
and unknown12 library in the process memory utilizing
dlopen system-call.
(2) Alter/remove memory protection mechanisms: Utilize
the mprotect system-call to change the permissions of
the stack and heap from rw- to rwx and .text segment
from r-x to rwx.
(3) Modify code segment: Change permissions of .text seg-
ment to rw-, replace instruction for the endgrent()
function call with a nop sledge, reset permissions of
.text segment to r-x.
(4) Modify data segment jump tables: Replace pointer of
the endgrent() lib-c function with setgrent()13 in
the .got .data segment.
It has to be noted that we did not exploit a vulnerability
of /bin/bash. In order to keep the experiment simple, we
went for an attack-simulation approach instead. However, we
assume that a sophisticated attacker can easily utilize any
mentioned technique, given an initial needed vulnerability14.
For this reason, we developed a small python tool, utilizing
the ptrace() system-call for attaching to a process15. The
tool was used to simulate the attacks and apply the mod-
iﬁcation directly in the process memory of an instantiated
/bin/bash user-process. After the attack was simulated, a
measurement and veriﬁcation process was conducted.
As depicted in Table 4, the diﬀerent attack techniques are
detected from diﬀerent or even multiple veriﬁcation modules.
This means, for the veriﬁcation of an user-space process, all
aforementioned veriﬁcation steps must be executed in or-
der to detect all deﬁned attack techniques. First of all, the
meta-data of each measured segment is analyzed and com-
pared against stored meta-data RVs. In particular, the size
and permission ﬂags are veriﬁed for each measurement. The
meta-data veriﬁcation module was able to detect attack (1)
and (2) but was not able to detect attack (3) and (4). The
next step is the veriﬁcation of the .text segment’s integrity,
based on a pre-calculated RV. The veriﬁcation is success-
ful if the a RVP IC is found that is equal to the measured
value mhd. Pre-calculated RV veriﬁcation detected (1) and
(3) but not (2) and (4). Finally, as explained, every veriﬁca-
tion process also involves the veriﬁcation of the .got related
to the designated .text segment. The expected RVGOT is
calculated and compared against the measured value mhd.
If both values are equal, the veriﬁcation is considered suc-
cessful. Ad-hoc .got relocation detected attack (4) but not
(1), (2) and (3).
12unknown means in this case that no RV was calculated for
that library
13endgrent and setgrent() use void as their parameter
14We honestly hope there is no exploitable vulnerability in
the analyzed bash version.
15In addition to that, the /proc/sys/kernel/yama/p-
trace_scope was set to 0 enabling process hooking by
ptrace
736Table 3: GOT calculation for /bin/bash application
symbol name
endgrent
__ctype_toupper_loc
iswlower
sigprocmask
sf o
0x0be7e0
0x0300c0
0x0fda30
0x036f80
library name
libc-2.19.so
libc-2.19.so
libc-2.19.so
libc-2.19.so
msalib
0x7fbd32f4b000
0x7fbd32f4b000
0x7fbd32f4b000
0x7fbd32f4b000
GOTaddress
0x006f0018
0x006f0020
0x006f0028
0x006f0030
GOTasa
0x7fbd330097e0
0x7fbd32f7b0c0
0x7fbd33048a30
0x7fbd32f81f80
Table 4: Results for the evaluation of the diﬀerent
attack techniques for the /bin/bash user-process
Veriﬁcation Module
(3)
Meta-data Veriﬁcation
+
−
Pre-calculated RV Veriﬁcation
Ad-hoc .got relocation RV Ver-
+
iﬁcation
−: integrity veriﬁcation failed (modiﬁcation detected)
+: integrity veriﬁcation successful (no modiﬁcation detected)
(1)
−
−
+
(2)
−
+
+
(4)
+
+
−
In this section, we evaluated the functionality of the indi-
vidual veriﬁcation modules. As shown, all veriﬁcation mod-
ules behave as expected, and detected the attacks that were
intended to be detectable. Similar to the described user-space
process veriﬁcation, the veriﬁcation modules for the Kernel
and LKM, i.e. load-time-relocated RCC and kernel image ver-
iﬁcation module, were also evaluated. We used a writeable
/dev/mem device to simulate the modiﬁcation of the kernel
image’s and diﬀerent LKMs’ .text16 segments. Afterwards,
we performed a measurement and veriﬁcation that detected
the modiﬁcations in both cases as expected.
5.3 Security Analysis
As demonstrated in the previous Section, DRIVE is able
to detect attacks on diﬀerent kinds of levels and with diﬀer-
ent granularity. Code Injection/Manipulation Attacks, like,
for instance, (1) Create new executable segment and (3) Mod-
ify code segment in predictable memory areas, are very well
suited for DRIVE’s employed detection mechanism. As a re-
sult, DRIVE can be used to verify whether a predictable mem-
ory region was modiﬁed or not. Similarly, Code Pointer Mod-
iﬁcations in designated and predictable memory areas, such
as (4) Modify data segment jump tables, can also be detected
reliably. Further identiﬁcation of possible predictable areas,
for instance well-known predictable kernel data-structures
such as the (2) system call table are left for future re-
search. The meta-data veriﬁcation used to detect Alter/re-
move memory protection mechanisms also has the potential
to detect sophisticated attacks to the system. Especially in
the ﬁeld of unpredictable memory areas, meta-data analy-
sis could provide enough evidence to make a decision about
the system state. However, self-contained Code Reuse At-
tacks, such as ROP and its diﬀerent variants are currently
not detectable by DRIVE, because they usually do not mod-
ify predictable memory areas. Control Flow Integrity (CFI)
and Code Pointer Integrity (CPI) mechanisms are tailored to
detect those kind of attacks. Hence, it would be very interest-
ing to study and analyze those protection mechanisms and
possibly integrate some concepts into DRIVE or vice versa.
Very recent research on attacks that solely utilize non-control
data to implant malicious actions seem to be resistant even
against CFI and CPI. However, if data-structures are altered
that rely on static information or modify meta-data that can
16Similar to attack technique (3)
be measured and veriﬁed successfully, DRIVE can possibly
be used to detect at least speciﬁc variants of those attacks.
5.4 Measurement and Veriﬁcation Micro-
benchmark
This section presents a micro-benchmark for the measure-
ment and veriﬁcation process of the described implementa-
tion. We identiﬁed two time-critical operations utilizing the
Kernels ftrace 17 debugging mechanism: (1) The hash calcula-
tion of the individually measured memory segments and (2)
TPMs extend() operation. Afterwards we added measure-
ment code to the implementation to get precise results, it
turned out both (1) hash calculation and (2) tpm extend()
consume roughly 98% of the overall time. The metrics for
none18, sha-1 and sha-256 hash algorithms were accumulated
on two diﬀerent architectures and platforms, i.e. X86-64 Bit
Intel Core i5-4570 CPU @ 3.20GHz on a standard Ubuntu
14.04 (3.13 Kernel) server installation (X86 64) and 32 Bit
PPC e500mc @ 1.2 GHz (4 cores) on Windriver embedded
Linux (2.6.34 Kernel) (PPC32).
Based on the implementation the measurement is executed
without interrupting and, thus, not directly aﬀecting the per-
formance of the measured processes, thus, concrete values
were not quantiﬁable. Obviously, the measurement aﬀects
the overall system performance negatively; however, the per-
formance impact is considered to be negligible on the exper-
imental setup. A test under heavy system load would have
distorted the benchmark randomly. Thus, in order to get
reproducible, comparable and meaningful micro-benchmark
results, we decided against experiments under heavy load.
As depicted in Table AT1, the metrics show the distri-
bution of the average computational time for (1) code and
library measurements and (2) TPM extend() operation for
one completed measurement cycle. For X86 64, ∼ 28 dif-
ferent code and ∼ 393 library segments (∼ 96 − 97 MB),
aggregated from ∼ 70 individual libraries were measured.
Similarly, for PPC32 ∼ 30 diﬀerent code and ∼ 698 library
segments (∼ 277 − 282 MB), aggregated from ∼ 230 individ-
ual libraries. Considering that libraries’ unmodiﬁed .text
segments are mapped one-time in physical memory, multiple
measurements of the same library is unnecessary. This leaves
a huge optimization potential by implementing a more sophis-
ticated measurement strategy that considers deduplication
of already conducted measurements.
As previously described in Section 4.3, the TPM ex-
tend() operation is only applied one-time for every indi-
vidual process and consumed ∼ 10.9 − 11.1 ms on X86 64
and ∼ 15.19 − 15.3 ms on PPC32 on average, independent
from utilized hash algorithms. In addition to the overall time
consumption, we also analyzed page access times, the overall
measured size, and total time consumption on average. As
expected, the relation between measured size to consumed
17https://www.kernel.org/doc/Documentation/trace/ftrace.
txt
18https://git.kernel.org/cgit/linux/kernel/git/herbert/
cryptodev-2.6.git/tree/crypto/crypto null.c
737time behaves almost linear for all hash functions on both
architectures.
Most interesting is the overall computational time used
for the hash calculation, because these represent the predom-
inant part that the CPU is occupied during the measure-
ment process. For X86 64, none ∼ 0.0291s, sha-1 ∼ 0.3947s
and sha-256 ∼ 0.6315s; for PPC32, none ∼ 1.0010s, sha-1
∼ 7.6695s and sha-256 ∼ 7.7194s respectively. The TPM ex-
tend() operation, anchoring the measurements in the TPM,
takes ∼ 11ms on X86 64 and ∼ 15.25ms on PPC32. Please
note that this does not aﬀect the computational eﬀort; the
TPM executes this operation without the assistance of the
CPU.
With regards to the veriﬁcation process, prior research by
Rein et. al analyzed integrity veriﬁcation of a SML [26]. The
veriﬁcation of the DML integrity via TPM Quote is identical
to our described approach. Eﬀects from the larger individual
data-sets for our measurements during template veriﬁcation
are considered to be negligible. As explained in Section 5.1,
for PIC measurements the veriﬁcation relies on a simple
comparison of the measured hash digest mhd and the ref-
erence value RV . Therefore, veriﬁcation times are expected
to be equal to the referenced SML veriﬁcation and depend
mainly on the implementation details, whether parallelized
or not, and the used database back-end. In contrast to that,
DM L .got and RCC veriﬁcation involves ad-hoc reference
generation for the measured .got and LKMs. Following the
description presented in 5.1, we identiﬁed the symbol reso-
lution process to be the most time consuming operation in
our implementation; It took ∼ 1.53s to generate the symbol
table for the .got of the /bin/bash application and ∼ 1.66s
to generate the initial symbol table for the LKMs. Once
the symbol tables were generated, the remaining operations,
i.e. calculation of correct jump addresses, .got generation
(/bin/bash application), the patching process (LKM), and
hash calculation, took for the /bin/bash application ∼ 14ms
and for a single LKM ∼ 52ms.
In this section we presented some metrics regarding the
performance of the PoC implementation for both, the in-
memory measurement process and the veriﬁcation. Most no-
tably, we observed in all cases a linear dependency between
computational eﬀort and the actual size of the taken mea-
surement or the amount of values to be veriﬁed.
6. RELATED WORK
MF and extraction tools are available for diﬀerent OS. For
instance, LIME [33] targets the extraction of memory im-
ages, whereas the Volatility Framework [35] and FATKit [24]
additionally support further analysis of the extracted mem-
ory contents. Memory extraction is usually implemented as
a LKM using the internal kernel APIs and data-structures
(c.f. 5.1). Volatility is considered the most recent and ad-
vanced tool and provides a huge amount of analysis plug-ins
[19]. Usually, MF is used to manually analyze the behavior
of infected systems if there is initial suspicion of misbehav-
ior. In other words, it is currently not used to automatically
detect or report malicious behavior; therefore, SMs are not
considered in either of the aforementioned solutions.
Since the initial design and proposal of the TCG-based
Integrity Measurement Architecture (IMA) [29] from Sailer
et. al, certain other approaches about static and dynamic
integrity measurement were researched and published. IBMs
IMA implementation is widely used today and considered the
standard procedure for static measurement and attestation.
However, the initial concept does not consider measurements
taken during runtime.
Subsequent work utilized diﬀerent techniques to gather
and verify measurements on the targeted system parts. Es-
pecially Linux kernel rootkit detection was a particular target
of research interest. Hardware based solutions were described
in [13, 3] demonstrating the utilization of a hardware based
co-processor in form of a PCI extension card. Both works
take snapshots of the memory and verify the snapshots by
an external system. LKIM [20, 23] was published as another
approach for Kernel Integrity Monitoring; LKIM utilizes sim-
ilar mechanisms to our presented work considering reference
value generation, which they called base-lining; i.e., gener-
ating cryptographic hash values based on simulation of the
loading process of kernel and LKMs. Regarding dynamic
LKM behavior, LKIM does not detail its base-lining and ver-
iﬁcation mechanisms. All the same, LKIM does not facilitate
a SM for secure anchoring and reporting of measurements; in
fact, no details about the veriﬁcation process were published.
In addition, many hypervisor-based approaches were re-
searched [30, 37, 10, 8, 2, 32, 15]. Most recently, measurement
of a virtual machine was demonstrated in [5], utilizing Linux
paging mechanisms by hypervisor introspection. Both, mea-
surement and veriﬁcation, are performed by the hypervisor.
The measurement acquisition is similar to our work, however,
the proposed solution employs intrusive enforcement mecha-