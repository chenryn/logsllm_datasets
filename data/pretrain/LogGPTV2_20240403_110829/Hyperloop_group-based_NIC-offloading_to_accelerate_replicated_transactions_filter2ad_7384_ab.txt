L
120
60
0
9
9
15
15
24
12
12
24
Number of MongoDB replica-sets
18
18
21
21
27
27
2
2
6
6
14
4
4
14
Number of CPU cores on each machine
10
10
12
12
8
8
16
16
1
0.5
0
d
e
z
i
l
a
m
r
o
N
s
e
h
c
t
i
w
s
-
t
x
e
t
n
o
c
(a) Varying number of replica-sets
(b) Varying number of CPU cores per machine
Figure 2: Analyzing the impact of CPU on MongoDB’s latency distribution using YCSB. Normalized context-switches is the
number of context-switches for each configuration divided by the maximum for all configurations in each chart.
To demonstrate this problem, we measured the latency
and CPU consumption of MongoDB [32] when running a
variable number of instances. We ran Yahoo Cloud Stor-
age Benchmark (YCSB) [56] against MongoDB. We used 6
machines (3 MongoDB servers and 3 YCSB clients) each
with two 8-core Xeon E5-2650v2 CPUs, 64 GB of memory,
and a Mellanox ConnectX-3 56 Gbps NIC. To avoid inter-
ference effects from storage medium, we use DRAM as the
storage medium. Note that this setup is also representative
for modern storage systems that use NVM as the storage
medium [60, 100, 101].
For the most part, CPU hits 100% utilization since all Mon-
goDB processes are fully active. The delay caused by CPU
can be observed by CPU context-switches. Figure 2(a) shows
how they impact the end-to-end MongoDB performance as
we increase the number of partitions per server. Each parti-
tion is served by a replica-set, which consists of one primary
replica process and two backup replica processes running
on the three different MongoDB servers. As the number of
partitions grow, there are more processes on each server,
thus more CPU context switches and higher latencies.
Next, we verify that this inflated latency is mainly due
to the heavy load and context switches on CPU, not net-
work congestion or contention on memory bus. We stick to
18 replica-sets but change the number of available cores on
each machine by disabling cores. Figure 2(b) shows that, even
though the network throughput remains the same, the trans-
action latency and number of context switches decreases
with more cores.
Existing solutions offer limited help: There are multi-
ple proposals that partly tackle this problem. For example,
user-level TCP such as mTCP [69] and RDMA-based de-
signs [60, 71] offload (part of) the network stack to NICs,
thus reducing the CPU overhead and context switching. How-
ever, the storage replication and transaction logic, which are
heavier operations relative to the network stack, remain on
CPUs.
Furthermore, these solutions rely on CPU core-pinning
to avoid being swapped out due to other busy processes to
achieve predictable latency. Some systems even employ busy
polling [60, 71], which wastes CPU further.
Unfortunately, core-pinning and busy polling is not a vi-
able option for multi-tenant systems. As explained above,
the number of partitions is typically an order or two magni-
tudes higher than the number of cores in data centers. These
partitions have to be isolated in different processes. Hence it
is not feasible to pin each partition to a dedicated core, nor
is busy polling per process feasible. Furthermore, it is not
feasible to get good latency even when only a few dedicated
threads at the user or kernel space poll on behalf of all the
tenants since the tenant processes still need to get a hold of
the CPU and wait for events from the polling threads.
In this paper, we aim to fundamentally address these issues
by offloading all CPU tasks, including the network stack and
storage replication and transaction logic to commodity NICs.
3 OVERVIEW
In this section, we present the design goals and architecture
of new primitives that can allow NICs to execute operations
needed for replicated NVM transactions at line rate without
CPU involvement in the critical path.
3.1 Design Goals
Motivated by our observations about storage system require-
ments (§2.1) and problems (§2.2), we design HyperLoop with
the following goals in mind.
No replica CPU involvement on the critical path: Since
the root cause of the latency problem is that replica server
CPUs are on the critical path, our solution is to offload those
software components in storage systems to commodity NICs.
The replica server CPUs should only spend very few cycles
that initialize the HyperLoop groups, and stay away from
the critical path after that.
This means that NICs by themselves should perform the
tasks that previously ran on CPUs, e.g., to modify data in
NVM. RDMA NICs (RNICs) provide a promising way to do
this while avoiding the CPU. However, it is still challenging
to use RNICs to perform some of these tasks such as log
processing without CPU. We will discuss this in §4.
Provide ACID operations for replicated transactional
storage systems: HyperLoop’s interfaces should be general
enough that most replicated transactional storage systems
can easily adopt HyperLoop without much modification to
applications. This means that HyperLoop aims to provide a
HyperLoop
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
The network primitive library implements four group-
based primitives, gFLUSH, gWRITE, gCAS, and gMEMCPY.
They address the missing memory operations needed by
general storage systems as explained in §3.1. Application
developers can leverage these primitives to implement or
modify their replication and transaction processing modules.
As described in §5, we have adapted a few applications to
use the HyperLoop library. Only small modifications were
needed based on their original code.
HyperLoop works in a chain-based manner. When the
storage application running on the client (also known as
transaction coordinator) writes (i.e., add or update) data, it
updates a write-ahead log and then performs the transaction
on the group. This is done by calling corresponding primitive
exposed by HyperLoop primitive library. RNICs on replicas
receive the operation from the client or previous replica,
execute the operation and forward the operation to the next
replica in the chain. Note that there is no CPU involved in
the critical path of executing this operation on replicas. The
NIC on the last replica sends back an ACK to the head of
the chain – the client. To keep the design simple HyperLoop
group failures are detected and repaired in an application
specific manner much like traditional network connections
in storage systems (see §5).
4 HYPERLOOP DESIGN
In this section, we describe the design of HyperLoop group-
based network primitives. We start by explaining the key
techniques that make HyperLoop possible.
4.1 Key Ideas
The challenge is how to control NICs without CPUs:
RDMA and NVM can turn storage operations into mem-
ory operations that bypass CPU. However, in traditional
RDMA programming, when and what memory operations
to be performed is controlled by CPU. This is because the
basic RDMA operations by themselves are only designed for
point-to-point communication. Therefore, CPUs on every
end-host must be involved on the critical path.
We elaborate this using a simple example, where a storage
system needs to replicate data from the client to two or more
replica nodes using chain replication [46, 47, 53, 81, 94]. Even
though the client can write data to a memory region of the
first (primary) replica without involving the replica’s CPU
using RDMA WRITE, the first replica still has to forward the
data to the second replica.
Thus, on the first replica, a traditional RDMA implementa-
tion will let CPU pre-post a receive request and keep polling
the completion queue (on the critical path). The client must
use SEND or WRITE_WITH_IMM operations, which trig-
gers the receive completion event on the first replica. Then
the replica performs the operations in the request meant for
it and posts a send request of the operations meant for the
rest of the chain to the next replica, right after the receive
Figure 3: Storage system architecture with HyperLoop.
new set of RDMA primitives instead of end-to-end RDMA-
and NVM-based storage system [59, 71].
These primitives should offload the operations most com-
monly required to ensure ACID properties. For example,
many storage systems, such as MongoDB [32], perform the
following steps for a transaction 1) replicate operation logs
to all replicas and make sure every replica is ready to commit,
2) acquire a lock on every replica, 3) execute the transactions
in operation logs, 4) flush all caches (if applicable) to make
the transactions durable, and 5) release the lock.
To support this representative process, HyperLoop pro-
vides new primitives that handle the steps separately. In §4.2,
we will explain the design of these primitives.
End-host only implementation based on commodity
hardware: HyperLoop is designed to be widely adoptable
by commodity data centers with little effort. Thus, HyperLoop
should not rely on any additional special hardware or switch
configurations. In addition, being implemented only on end-
hosts, HyperLoop can avoid putting state on switches, there-
fore is easy to manage and can scale well.
3.2 HyperLoop Architecture
HyperLoop offloads all the tasks on the replicas to their NICs.
To achieve this, we must turn those tasks into a form that can
be processed by just the NICs, without the help of CPUs. Our
insight is that replication, log processing, and global lock-
ing are essentially a set of memory operations and network
commands, and therefore are viable candidates for offload
from CPUs to NICs that can access memory (e.g., RDMA).
Since NVM offers the convenience of durable memory, it is
a natural fit. However, as we show later, care must be taken
when bridging between volatile caches of RNICs and the
durable memory region.
Figure 3 illustrates the architecture of HyperLoop. A typ-
ical replication group in storage systems consists of multi-
ple replicas with only the first one interfacing with clients.
HyperLoop can support any number of replicas depending
on the application’s requirement. There are two software
layers: HyperLoop network primitive library and storage ap-
plications (e.g., MongoDB) that adopt the primitive library.
Note that HyperLoop itself is not a full end-to-end repli-
cated storage system. Its main goal is to provide key building
blocks to build replicated transaction systems or make exist-
ing systems more efficient.
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
D. Kim and A. Memaripour et al.
Figure 4: Forwarding an operation to another node in a
group with RDMA WAIT.
completion event (on the critical path again). The CPU is
needed not only for executing the memory operations in
these requests but also for posting it to the next replica.
This means that the NIC on the replica does not know
when and what it should forward to the next node until
its CPU gets the receive completion event and generates a
corresponding send request. This causes the high tail latency
and the high CPU utilization problems shown in §2.2.
The general approach of HyperLoop is to design a set of
group-based memory operation primitives that completely
remove replica’s CPUs from the critical path. This means
that NICs on replicas should detect the receive events by
themselves, process them and automatically trigger sending
them to the next replica. In addition, the forwarded SEND
operation must be based on the memory address and length
of data received from upstream.
Leveraging WAIT for auto-triggering RDMA operations:
We find that commodity RNICs support an operation which
enables event-based work request triggering, called RDMA
WAIT, between two communication channels (or queue pair
(QP)) within a host.2 In a nutshell, WAIT enables RNICs to
wait for a completion of one or more work requests (WRs)
posted on one WR queue, and trigger other WRs that are
pre-posted on another WR queue, without CPU involvement.
Although this feature has not been widely studied or used for
general applications, we find it promising for determining
when a remote NIC can proceed.
We leverage this feature to enable forwarding operations in
HyperLoop. Figure 4 illustrates the data path for forwarding
operations. The basic idea is that every replica pre-posts
RECV WR on the WR queue of the QP connected to the
previous node, and also WAIT and a two-sided operation
(e.g., SEND or WRITE_WITH_IMM) WR to the WR queue
of the QP connected to the next node in the group. Upon
completion of a RECV WR (Step 1 and 2), the WAIT WR is
triggered and it activates operation WR, which was posted
right next to it (Step 3 and 4). Thus, NICs on replicas forward
the memory operations received from the previous node to
the next node in the group, without involving the CPU.
Remote work request manipulation for replicating ar-
bitrary data: While WAIT addresses the “when” problem,
NICs also need to know what to do, whenever WAIT trig-
gers. In common RDMA programs, when posting an RDMA
WRITE or SEND work request, one must specify a memory
2Different manufacturers have different terms for the method, e.g., “CORE-
Direct” [25] by Mellanox.
Figure 5: Manipulating pre-posted remote work requests
from the client.
descriptor that contains a local source address, size of data
to be written, and a remote destination address (in case of
WRITE). Since WAIT can only trigger work request posted
in advance, NICs can only forward a fixed size buffer of data