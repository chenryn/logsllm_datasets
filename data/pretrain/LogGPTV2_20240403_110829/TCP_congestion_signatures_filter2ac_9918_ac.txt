either of these settings can be violated in real-world situations; there
can be cross-trafﬁc in the access link, and, congestion in interdo-
main links could potentially be caused by a small number of ﬂows
(particularly as access link speeds get bigger).
We run experiments to see how our classiﬁer works when we re-
lax these assumptions. First, we ﬁx the access link to 50 Mbps, and
reduce the number of cross-trafﬁc processes generated by TGcong
from 100 to 50, 20, and 10. As we reduce the number of processes,
the throughput of our test ﬂow increases, as there is less competition.
This increases the buffer occupancy of our ﬂow, and the technique
can get confounded, concluding that congestion is self-induced. In-
deed, in our experiments, the fraction of ﬂows correctly classiﬁed
as externally congested decreases from 93% when there are 100
concurrent ﬂows, to 84% when there are 50, to 74% with 20, and
50% with 10. Similarly, we introduce cross trafﬁc in the access link,
with 1, 2 and 5 concurrent ﬂows. Even with 1 cross-ﬂow, our test
ﬂow does not obtain the full throughput; however it still gets sig-
niﬁcant buffer space; therefore the classiﬁer still classiﬁes 86% of
ﬂows as self-limited. With 5 ﬂows, this number is 70%. In both
cases, due to TCP’s sharing mechanism, our test ﬂow is able to ob-
tain signiﬁcant buffer occupancy, and therefore can be thought of as
inﬂuencing buffer behavior; this is what our technique captures.
4 REAL WORLD VALIDATION USING M-LAB
DATA
We validate our model on two real-world datasets from M-Lab. The
ﬁrst is a large dataset that spans the timeframe of a well publicized
peering dispute involving Cogent, a major transit provider, and sev-
eral large US ISPs in 2014. We term this dataset the Dispute2014
dataset. The second is a more focused dataset consisting of data we
collect from targeted experiments we conduct in 2017 between a
client in the Comcast access network in Massachusetts and an M-
Lab server hosted by TATA in New York. We choose this particular
client and server combination because the interdomain link between
the two ISPs experienced periods of congestion during our experi-
ments, inferred using the Time Series Latency Probes (TSLP) [35]
methodology. We term this dataset the TSLP2017 dataset.
4.1 The Dispute2014 dataset
M-Lab and NDT
The M-Lab infrastructure currently consists of more than 200
servers located in more than 30 hosting networks globally, host-
ing a variety of network performance tests such as NDT [17], Glas-
nost [31] and Mobiperf [2]. Our study focuses on the Network Di-
agnostic Test (NDT) [17]. NDT is a user-initiated Web-based test
which measures a variety of path metrics between the server and
the client, including upstream and downstream throughput. For ev-
ery NDT measurement, the server logs Web100 [7] statistics that
Comcast
Cox
TimeWarner
Verizon
40
30
20
10
s
p
b
M
0
5
10
15
20
Hour of day (local)
(a) Diurnal throughput graph for Cogent customers to M-Lab server
in Los Angeles, January 2014. All ISPs except Cox see signiﬁcant diur-
nal effects. Only Cox had a direct peering agreement with Netﬂix via
the latter’s OpenConnect program. For other ISPs, the NDT measure-
ments were affected by congestion in Cogent caused by Netﬂix trafﬁc.
February has a similar pattern. [20]
Comcast
Cox
TimeWarner
Verizon
40
30
20
10
s
p
b
M
0
5
10
15
20
Hour of day (local)
(b) Diurnal throughput graph for Level3 customers to M-Lab server in
Atlanta, January 2014. No ISPs see diurnal effects. Level3 did not have
signiﬁcant congestion issues in this time period, at least in the paths
that NDT measures.
Comcast
Cox
TimeWarner
Verizon
40
30
20
10
s
p
b
M
0
5
10
15
20
Hour of day (local)
(c) Diurnal throughput graph for Cogent customers to M-Lab server
in Los Angeles, April 2014. In contrast to January, ISPs do not see
diurnal effect anymore. Cogent relieved congestion on their network,
and Comcast signed an agreement with Netﬂix [20, 22]. March has a
similar pattern.
Figure 5: Diurnal average throughput of NDT tests. We use the
diurnal effect in Cogent to all ISPs (except Cox) in Jan-Feb
as the basis for our labeling—tests during peak hours in Janu-
ary are externally congested. In contrast, we label off-peak Cox
tests in Jan-Feb through Cogent, and all tests in Mar-Apr as
affected by self-induced congestion.
IMC ’17, Internet Measurement Conference, November 1–3, 2017, London, United Kingdom
Sundaresan et al.
provide TCP performance over 5 ms intervals. Web100 statistics
include a number of useful parameters including the ﬂow’s TCP
RTT, counts of bytes/packets sent/received, TCP congestion win-
dow, counts of congestion window reductions, and the time spent
by the ﬂow in “receiver limited”, “sender limited” or “congestion
limited” states. The server also stores raw packet traces for the tests
in pcap format. We obtain both the Web100 logs and the trace ﬁles
through Google’s Big Query and Big Store [1, 6] where they are
freely available.
Pre-processing the NDT data
We are interested in ﬂows that experience congestion, so we ﬁlter
the M-Lab data accordingly. We choose NDT measurements with
downstream tests that lasted at least 9 seconds—the duration of the
NDT measurement is 10 seconds, so these tests are likely to have
completed—and which spend at least 90% of the test duration in the
congestion limited state. We get this information from the Web100
output. We thereby exclude ﬂows which were sender- or receiver-
limited, or did not experience congestion for other reasons, such as
high loss or latency. We do so because such ﬂows would not have
induced congestion in the path, and were not affected by external
congestion either, and we therefore are not interested in them. These
ﬁlters are the same as those used by M-Lab in their 2014 report that
examines interdomain congestion [36]. These are also necessarily
different from our earlier deﬁnition of congestion (using thresholds
based on access link capacity) because in the bulk of our dataset,
where we depend on crowdsourced measurements of the NDT data,
we do not know the ground truth access link capacity of the users
that ran these tests.
The peering disputes of 2013/2014.
In late 2013 and early 2014, there were media reports of poor
Netﬂix performance on several access ISPs that did not peer di-
rectly with Netﬂix [12]. Netﬂix trafﬁc to these access ISPs was
delivered by multiple transit ISPs such as Cogent, Level3, and
TATA. Throughput traces from the M-Lab NDT platform during
this time period showed strong diurnal patterns, with signiﬁcantly
lower throughput during peak trafﬁc hours (evening) as compared
to lightly loaded periods (middle of the night). Such diurnal pat-
terns were evident in throughput tests from Cogent servers to sev-
eral large ISPs such as Comcast, Time Warner, and Verizon. A no-
table exception was Cox, which had already entered into a direct
peering agreement with Netﬂix. Consequently, Cox’s interconnec-
tions to Cogent were not affected by Netﬂix trafﬁc and through-
put from Cogent to Cox did not show diurnal patterns. Figure 5a
shows the diurnal throughput performance of AT&T, Comcast, Cox,
TimeWarner, and Verizon customers to NDT servers in Cogent.
We see a substantial drop in throughput during peak hours to all
ISPs except Cox. Other transit ISPs such as Level3, however, did
not show such diurnal throughput patterns in the NDT data (Fig-
ure 5b). M-Lab released an anonymous report that concluded that
the cause of performance degradation was peak-time congestion in
interdomain links connecting the transit ISPs hosting M-Lab servers
to access ISPs [36]. The reasoning was based on coarse network
tomography—since NDT tests between Cox customers and the M-
Lab server in Cogent did not show a diurnal pattern, whereas tests
between other ISPs and the same server did, the report stated that
the source of congestion was most likely the peering between Co-
gent and Comcast, Time Warner, and Verizon. Another independent
study also conﬁrmed the existence of congestion in these paths and
narrowed down the source of congestion to ISP borders [35].
During the last week of February 2014, Cogent began priori-
tizing certain trafﬁc in order to ease congestion within their net-
work [20, 21], and then Comcast signed a peering agreement with
Netﬂix [22]. These events had the desired effect in terms of eas-
ing congestion; we see in Figure 5c that NDT measurements to the
Cogent server in Los Angeles no longer exhibited diurnal effects.
We observed similar patterns for Cogent servers in New York and
Seattle (not shown).
Collecting and labeling Dispute2014
We do not have service plan information for the Dispute2014data,
and so we cannot use the threshold technique to label it; we instead
use Figure 5 to inform our labeling. We label peak hour (between
4 PM and 12 AM local time) tests in January and February from
affected ISPs (i.e., those that see a sharp drop in performance dur-
ing peak hours; Comcast, Time Warner, and Verizon) as externally
congested. We label off-peak tests in March and April (between
1 AM and 8 AM local time) as self-induced congestion limited.
To minimize noise, we do not consider off-peak tests in January-
February, or peak tests in March-April. Our labeling method as-
sumes that off-peak throughput tests are limited by access-link ca-
pacities; this assumption is based on annual reports from the FCC
Measuring Broadband America (MBA) program, that show that ma-
jor ISPs come close to or exceed their service plans [29]. However,
we do not have ground truth, and therefore our labeling is necessar-
ily broad and imperfect. Nevertheless, the substantial difference in
throughput between peak and off-peak hours, along with the general
agreement in the community about the existence of peering issues
lead us to expect that a large percentage of our labels are correct.
We consider two transit ISPs: Cogent, which was affected by the
peering disputes and Level3, which was not. We study two geo-
graphical locations—Los Angeles (LAX), and New York (LGA)—
for Cogent, and one location—Atlanta (ATL)—for Level3. We
study four access ISPs: Comcast, TimeWarner, Verizon, and Cox.
Of these, Cox is unique because its performance to Cogent was not
affected by the peering disputes, and therefore is useful as a control
to show that our techniques are generally applicable across different
transit and access ISPs.
4.2 The TSLP2017 dataset
The Dispute2014 dataset is large, but it is broad, and our labeling is
coarse. We also do not have ground truth available for that dataset
to accurately label ﬂows as access-link or externally congested. We
therefore run targeted experiments to generate a dataset with more
accurate labels. Our goal was to ﬁnd an interdomain link that was
periodically congested—and that we could reliably identify as so—
and run throughput tests from a client whose access-link capacity
we know to a server “behind” the congested link. A challenge with
this basic idea is that we must ﬁnd an M-Lab server such that the
path from our client to the server crosses a congested link. To do so,
we use the dataset resulting from our deployment of the TSLP [35]
tool on the Archipelago (Ark) [19] infrastructure to identify inter-
domain links between several large access ISPs and transit/content
TCP Congestion Signatures
IMC ’17, Internet Measurement Conference, November 1–3, 2017, London, United Kingdom
providers that showed evidence of congestion. The TSLP technique
measures the latency from a vantage point located within a network
to the near and far routers of interdomain links of the host network.
An elevated latency to the far end of the link, with no elevation to
the near end, suggests that the targeted link is congested. Luckie
et al. [35] showed that periodic increase in latency occurring when
the expected load on the link is the highest (during peak hours in the
evening) is a good indicator of congestion on the interdomain link.
The TSLP technique is quite different from what we propose in this
paper; TSLP targets speciﬁc links that are identiﬁed in advance, and
it requires external probes to measure those links. We ran traceroute
measurements from our Ark node in Comcast’s network located at
Bedford, Massachusetts to each M-Lab server, to ﬁnd instances of
paths that traversed congested interdomain links identiﬁed with the
TSLP technique.
Collecting TSLP2017
We ﬁnd one case where the path between the Ark node and an M-
Lab server hosted by TATA in New York traverses the interconnect
between Comcast and TATA in New York City that is occasionally
congested as indicated by an increase in the latency across that link
during peak hours. The latency increase we measured went from a
baseline of about 18ms to a peak of above 30ms. This increase of
about 15ms likely reﬂects the size of the buffer on the link between
the Comcast and TATA routers.
We run periodic, automated NDT measurements between this
Ark node and the TATA server during both peak and off-peak
hours. We establish the baseline service plan for this Comcast user—
25 Mbps downlink—by talking to the Ark host, running indepen-
dent measurements using netperf, and by examining the NDT
throughput achieved during off-peak hours. We run throughput tests
every 1 hour during off-peak hours and every 15 minutes during
peak hours between February 15 and April 30, 2017. Our TSLP
measurements are continuously ongoing.
Indeed, we ﬁnd a strong negative correlation between the NDT
throughput and the TSLP latency to the far end of the link (Figure 6).
During periods when the latency is low (at the baseline level), the
NDT test almost always obtains a throughput close to the user’s ser-
vice plan rate of 25 Mbps. During the episodes of elevated latency,
throughput is lower. We collect and process this data the same way
we do the data from Dispute2014.
Labeling TSLP2017
We know both the expected downstream throughput of the access
link (25 Mbps), and the baseline latency to the TATA server (18 ms),
which increases to above 30 ms during congested periods. We label
a test as externally limited if the throughput is less than 15 Mbps
and the minimum latency is greater than 30ms; and as access-link
congested otherwise. This allows us to be certain that the tests that
we labeled as externally limited were conducted during the period
of elevated latency detected by TSLP and were affected by conges-
tion on the interdomain link. We collected 2593 NDT tests in the 10
week period, of which we were able to label 20 tests as externally
congested and the rest as self-induced. The relatively low number
of external congestion events speaks to the difﬁculty of getting real-
world congestion data.
70
60
50
40
30
20
10
0
35
30
25
20
15
10
5
0
)
s
m
(
y
c
n