online learning). Under the constraint where we can only read
each data item once within a shard, updating the model incre-
mentally after every example typically has good convergence
properties. As a performance optimization, we return the sum
of the partial gradients rather than the updated weight vector
itself.
After the m shards update their version of the weight
vector, we collect the partial gradients (cid:126)g(1)..(cid:126)g(m) and average
them (Algorithm 1, “average” steps). Then, we perform L1-
regularization (Algorithm 1, “shrink” step) on the averaged
weight vector using a truncation function with threshold λ
— this only applies to feature weights corresponding to
binary features. In particular, all feature weights wi with
magnitude less than or equal to λ are set to 0, and all other
weights have their magnitudes reduced by λ. This procedure
reduces the number of nonzero weight vector entries, allowing
the resulting weight vector to occupy less memory. Because
there are fewer real-valued features (about 100) than binary
features (about 107), we do not regularize the feature weights
corresponding to real-valued features.
After the shrinkage step, we distribute the new weight vector
(cid:126)w to the m shards again to continue the training process. The
process repeats itself for I iterations.
A number of practical issues arise in getting the distributed
logistic regression to scale to large-scale data. We describe
how we implement our classiﬁer in Section 5.4.
4.4. Data Set and Ground Truth
Our data set for training and testing the classiﬁer consists
of three sources: URLs captured by spam traps operated by
major email providers, blacklisted URLs appearing on Twitter,
and non-spam URLs appearing on Twitter that are used to
represent a non-spam data sample. In total, we use Monarch’s
feature collection infrastructure over the course of two months
to crawl 1.25 million spam email URLs, roughly 567,000
blacklisted Twitter URLs, and over 9 million non-spam Twitter
URLs. Due to blacklist delay, generating our spam set of
Twitter URLs requires retroactively checking all our Twitter
URLs against 5 blacklists: Google Safebrowsing, SURBL,
URIBL, Anti-Phishing Work Group (APWG), and Phishtank.
If at any point after a URL is posted to Twitter its landing
page, any of its redirects, frame URLs, or any of its source
URLs become blacklisted, we treat the sample as spam. A
breakdown of the categories of spam identiﬁed on Twitter can
be seen in Table 2; 36% of blacklisted URLs were ﬂagged as
scams, 60% as phishing, and 4% as malware. A breakdown
for email categories is not available, but the sample is known
to contain scams, phishing, and malware.
In general, we lack comprehensive ground truth, which
complicates our overall assessment of Monarch’s performance.
We may misclassify some true spam URLs as nonspam given
absence of the URL in our spam-trap and blacklist feeds.
453
Blacklist
Anti-Phishing Work Group
Google Safebrowsing (Phishing)1
Google Safebrowsing (Malware)1
Phishtank
SURBL (Scams)
SURBL (Malware, Phishing)
URIBL (Scams)
Total Samples
Total Unique
Detected URLs
350,577
12
22,600
46,203
51,263
7,658
189,047
667,360
567,784
TABLE 2: Blacklist results for URLs appearing on Twitter that were
ﬂagged as spam.
Thus, we may somewhat underestimate false negatives (spam
that slips through) seen in live operation, and overestimate
false positives (legitimate URLs tagged as spam). In practice,
building a training set of spam and non-spam samples remains
a challenge for Monarch, requiring either user reports or spam
traps operated by the web services seeking protection. How-
ever, for the purposes of evaluating Monarch’s effectiveness
at identifying spam, blacklists and email spam traps provide
a suitable source of ground truth.
5. Implementation
We implement each of the four components of Monarch
as independent systems operating on Amazon Web Services
(AWS) cloud infrastructure. For exact speciﬁcations of the
hardware used for our system, we refer readers to the AWS
EC2 instance documentation [35].
5.1. URL Aggregation
URL aggregation and parsing is written in Scala and exe-
cutes on a single EC2 Extra Large instance running Ubuntu
Linux 10.04. The aggregation phase for Twitter URLs parses
tweets from the Twitter Streaming API [36] and extracts URLs
from the tweet text. The email spam URLs we process are
provided to us post processing, and require no additional
parsing. We place incoming URLs into a Kestrel queue [37]
that keeps the most recent 300,000 URLs from the Twitter
stream and email URLs to supply feature collection with a
steady workload of fresh URLs. A full-ﬂedged implementation
of our system would require that
the queue keeps every
submitted URL, but for the purposes of evaluating Monarch,
we only need enough URLs to scale to the system’s throughput
and to generate large data sets for classiﬁcation.
5.2. Feature Collection
As previously framed, feature collection consists of four
components: a web browser, DNS resolver, IP address anal-
ysis, and a monitor to handle message passing and aggregate
1. Twitter uses Google’s Safebrowsing API to ﬁlter URLs appearing in
tweets. URLs in our data set were either obfuscated to prevent detection, or
were not present in the blacklist at the time of posting.
results. Feature collection runs in parallel on 20 EC2 High-
CPU instances each running Ubuntu Linux 10.04 and execut-
ing 6 browsers, DNS resolvers, and IP analyzers each. For web
browsing we rely on Firefox 4.0b4 augmented with a custom
extension written in a combination of XML and JavaScript
to tap into Firefox’s API [38] which exposes browser-based
events. We collect plugin-related events not exposed to the API
by instrumenting Firefox’s NPAPI [39] with hooks to interpose
on all message passing between plugins and the browser. If
a URL takes more than 30 seconds to load, we enforce a
timeout to prevent delaying classiﬁcation for other URLs.
DNS resolution occurs over Linux’s native host command,
while geolocation and route lookups use the MaxMind GeoIP
library [40] and Route Views [41] data respectively. A monitor
written in Python aggregates the results from each of these
services, generating its output as JSON text ﬁles stored in
AWS S3.
5.3. Feature Extraction
Feature extraction is tightly coupled with the classiﬁcation
and training phase and does not run on separate hardware.
Until the extraction phase, we store features in raw JSON
format as key-value pairs. During extraction, we load the
JSON content into a Scala framework, transform each into
meaningful binary and real-valued features, and produce a
sparse hash map stored in memory.
5.4. Classiﬁer
Before training begins, we copy the raw feature data from
Amazon S3 to a Hadoop Distributed File System (HDFS) [34]
residing on the 50-node cluster of Amazon EC2 Double-Extra
Large instances. Files in HDFS are automatically stored in
shards of 128 MB, and we use this pre-existing partitioning
(as required in the “Input” line of Algorithm 1). Within
each shard, we randomize the order of the positive and
negative examples—this gives the stochastic gradient descent
in Algorithm 2 (which incrementally computes its partial
gradient) better convergence rates compared to the situation
of processing a long, contiguous block of positive examples
followed by a long, contiguous block of negative examples.
We implement Algorithms 1 and 2 using Spark, a distributed
computation framework that provides fault-tolerant distributed
collections [42]. Spark provides map-reduce and shufﬂe oper-
ations, allow us to cache the data in memory across the cluster
between iterations. We take advantage of these capabilities to
construct an efﬁcient distributed learner.
The ﬁrst step of the training implementation is to normalize
the real-valued features. In particular, we project real values to
the [0, 1] interval—doing so ensures that real-valued features
do not dominate binary features unduly (a common practice in
classiﬁcation). We perform a map-reduce operation to compute
the ranges (max/min values) of each real-valued feature. Then,
we broadcast the ranges to the slave nodes. The slaves read
the raw JSON data from HDFS, perform feature extraction
to convert JSON strings into feature hash maps, and use the
ranges to complete the normalization of the data vectors. At
this point, the data is ready for training.
For the “gradient” step in Algorithm 1, we distribute m tasks
to the slaves, whose job is to map the m shards to partial
gradients. The slaves then compute the partial gradients for
their respective shards using Algorithm 2.
Because the number of features is quite large, we want
to avoid running Algorithm 1’s “average” and “shrink” steps
entirely on the master—the amount of master memory avail-
able for storing the weight vector (cid:126)w constitutes a resource
bottleneck we must tend to.2 Thus, we must avoid aggregating
all of the partial gradients at the master at once and ﬁnd an
alternate implementation that exploits the cluster’s parallelism.
To achieve this, we partition and shufﬂe the partial gradients
across the cluster so that each slave is responsible for a
computing the “average” and “shrink” steps on a disjoint
subset of the feature space. We split each gradient into P
partitions (not to be confused with the initial m data shards).
Speciﬁcally, we hash each feature to an integer key value from
1 to P . Then, we shufﬂe the data across the cluster to allow
the slave node responsible for feature partition p ∈ {1..P} to
collect its partial gradients. At this point the slave responsible
for partition p performs the “average” and “shrink” steps.
When these computations ﬁnish, the master collects the P
partitions of the weight vector (which will have a smaller
memory footprint than before shrinking) and joins them into
the ﬁnal weight vector (cid:126)w for that iteration.
6. Evaluation
In this section we evaluate the accuracy of our classiﬁer
and its run-time performance. Our results show that we can
identify web service spam with 90.78% accuracy (0.87% false
positives), with a median feature collection and classiﬁcation
time of 5.54 seconds. Surprisingly, we ﬁnd little overlap
between email and tweet spam features, requiring our classiﬁer
to learn two distinct sets of rules. We explore the underlying
distinctions between email and tweet spam and observe that
email is marked by short lived campaigns with quickly chang-
ing domains, while Twitter spam is relatively static during
our two month-long analysis. Lastly, we examine our data set
to illuminate properties of spam infrastructure including the
abuse of popular web hosting and URL shorteners.
6.1. Classiﬁer Performance
We train our classiﬁer using data sampled from 1.2 million
email spam URLs, 567,000 blacklisted tweet URLs, and 9
million non-spam URLs. In all experiments, we use the fol-
lowing parameters for training: we set the number of iterations
to I = 100, the learning rate to η = 1, and the regularization
factor to λ = 10η
m (where m is the number of data shards).
2. In our application, the slaves are able to compute the partial gradient over
their respective shards without memory exhaustion. However, if the partial
gradient computation were to bottleneck the slave in the future, we would
have to add a regularization step directly to Algorithm 2.
454
Training Ratio
1:1
4:1
10:1
FP
4.23%
FN
Accuracy
94.14%
7.50%
90.78% 0.87% 17.60%
86.61%
26.54%
0.29%
TABLE 3: Results for training on data with different non-spam to
spam ratios. We adopt a 4:1 ratio for classiﬁcation because of its low
false positives and reasonable false negatives.
Overall Accuracy. In order to avoid mistaking benign URLs
as spam, we tune our classiﬁer to emphasize low false positives
and maintain a reasonable detection rate. We use a technique
from Zadrozny et al. [43] to adjust the ratio of non-spam
to spam samples in training to tailor false positive rates. We
consider non-spam to spam ratios of 1:1, 4:1, and 10:1, where
a larger ratio indicates a stronger penalty for false positives.
Using 500,000 spam and non-spam samples each, we perform
5-fold validation and randomly subsample within a fold to
achieve the required training ratio (removing spam examples to
increase a fold’s non-spam ratio), while testing always occurs
on a sample made up of equal parts spam and non-spam.
To ensure that experiments over different ratios use the same
amount of training data, we constrain the training set size to
400,000 examples.
Table 3 shows the results of our tuning. We achieve lower
levels of false positives as we apply stronger penalties, but
at the cost of increased false negatives. We ultimately chose
a 4:1 ratio in training our classiﬁer to achieve 0.87% false
positives and 90.78% overall accuracy. This choice strikes a
balance between preventing benign URLs from being blocked,
but at the same time limits the amount of spam that slips past
classiﬁcation. For the remainder of this evaluation, we execute
all of our experiments at a 4:1 ratio.
To put Monarch’s false positive rate in perspective, we
provide a comparison to the performance of mainstream
blacklists. Previous studies have shown that blacklist false
positives range between 0.5–26.9%, while the rate of false
negatives ranges between 40.2–98.1% [11]. Errors result from
a lack of comprehensive spam traps and from low volumes of
duplicate spam across all traps [44]. These same performance
ﬂaws affect the quality of our ground truth, which may skew
our estimated false positive rate.
For web services with strict requirements on false positives
beyond what Monarch can guarantee, a second tier of heavier-
weight veriﬁcation can be employed for URLs ﬂagged by
Monarch as spam. Operation can amortize the expense of
this veriﬁcation by the relative infrequency of false positives.
Development of such a tool remains for future work.
Accuracy of Individual Components. Classiﬁcation relies on
a broad range of feature categories that each affect the overall
accuracy of our system. A breakdown of the features used for
classiﬁcation before and after regularization can be found in
Table 4. From nearly 50 million features we regularize down to
98,900 features, roughly half of which are each biased towards
spam and non-spam. We do not include JavaScript pop-ups
or plugin related events, as we found these on a negligible
455
Feature Type
HTML terms
Source URLs
Page Links
HTTP Headers
DNS records
Redirects
Frame URLs
Initial/Final URL
Geolocation
AS/Routing
All feature types
Unﬁltered
20,394,604
9,017,785
5,793,359
8,850,217
1,152,334
2,040,576
1,667,946
1,032,125
5,022
6,723
49,960,691
Filtered
50,288
15,372
10,659
9,019
5,375
4,240
2,458