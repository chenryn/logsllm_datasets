of local software events such as program loading. Integrity
reporting consists of transmitting collected measurements
in a signed aggregate to an external veriﬁer. The external
veriﬁer may then use the measurements to make trust
SRT M(m)
BL(m)
OS(m)
≡ b = read m.bl loc;
extend m.pcr.s,b;
jump b
≡ o = read m.os loc;
extend m.pcr.s,o;
jump o
≡ a = read m.app loc;
extend m.pcr.s,a;
jump a
≡ . . .
APP(m)
T PMSRT M(m) ≡ w = read m.pcr.s;
r = sign (PCR(s),w),AIK−1(m);
send r
Veri f ier(m) ≡ sig = receive ;
v = verify sig,AIK(m);
match v,(PCR(s),
seq(sinit,BL(m),OS(m),APP(m)))
reads the operating system’s code o from a ﬁxed location
m.os loc, extends it into PCR m.pcr.s, and branches to it.
The operating system (OS(m)) performs similar actions
with the application’s code a. The application (APP(m))
may perform any actions. In practice, the sequence of
measurement and loading may continue beyond the ﬁrst
application but we have chosen to terminate it here be-
cause extending the chain further does not lead to any
new insights about the security of the system.
Security Property. We summarize the integrity measure-
ment security property as follows: if m.pcr.s is protected
while a machine boots, and the contents of m.pcr.s are
seq(sinit,BL(m),OS(m),APP(m)),
then the initial soft-
ware loaded on machine m since its last reboot was
BL(m) followed by OS(m). We now state this property
formally. We deﬁne the formulas ProtectedSRTM(m) and
MeasuredBootSRTM(m,t) as follows.
Figure 3. Security Skeleton for SRTM Attestation Protocol
ProtectedSRTM(m) =
∀t,I. (Reset(m,I) @ t) ⊃ (IsLocked(m.pcr.s,I) @ t)
decisions. We ﬁrst analyze an attestation protocol using
a Static Root of Trust for Measurement (SRTM), then
we consider an attestation protocol utilizing hardware
support for late launch and a Dynamic Root of Trust
for Measurement (DRTM). We simplify both protocols
by assuming the AIK has been certiﬁed as authentic by a
manufacturer certiﬁcate and by verifying a ﬁxed sequence
of system integrity measurements.
4.1. Attestation Using a Static Root of Trust
We start by performing an analysis of a load-time
attestation protocol using an SRTM. The security skeleton
of the protocol is speciﬁed in Figure 3. A security skeleton
retains only relevant actions, in this case, actions per-
forming integrity measurement and reporting. The SRTM
protocol is composed of code that performs measurement
followed by code that performs integrity reporting. We
analyze the components separately.
4.1.1. Integrity Measurement. In the SRTM protocol,
integrity measurement starts after a machine reset. The
programs marked SRT M(m), BL(m), and OS(m) in Fig-
ure 3 represent those portions of the SRTM, boot loader,
and operating system that participate in the measurement
process. The SRT M(m) program is always the ﬁrst pro-
gram invoked when a machine reboots. It ﬁrst reads the
boot loader’s code b from the ﬁxed disk address m.bl loc,
then measures the code by extending it into a static PCR
m.pcr.s (which in this case stores all measurements),
and then branches to the the boot loader by executing
the instruction jump b. The boot loader (BL(m)) in turn
MeasuredBootSRTM(m,t) =
∃tT . ∃tB. ∃tO. ∃J. (tT < tB < tO < t) ∧
(Reset(m,J) @ tT ) ∧ (Jump(J,BL(m)) @ tB) ∧
(Jump(J,OS(m)) @ tO) ∧ (¬Reset(m) on (tT ,t])
(¬Jump(J) on (tT ,tB)) ∧ (¬Jump(J) on (tB,tO))
ProtectedSRTM(m) means that any thread I created
to boot machine m after a reset obtains an exclusive-
write lock on m.pcr.s. MeasuredBootSRTM(m,t) identiﬁes
software events on m such as the boot loader and operating
system being branched to before time t. It comprises
four facts: (1) There exists a time tT before t at which
m was reset, creating a thread J to boot the machine
(Reset(m,J) @ tT ), (2) This thread J branched to the
programs BL(m) and OS(m) at later time points tB and
tO (Jump(J,BL(m)) @ tB and Jump(J,OS(m)) @ tO), (3) J
did not make any other jumps in the interim (¬Jump(J) on
(tT ,tB)) and (¬Jump(J) on (tB,tO)), and (4) Machine m
was not reset between tT and t (¬Reset(m) on (tT ,t]).
Equivalently, after its last reboot before time t, the ﬁrst
programs loaded on m were BL(m) and OS(m). We believe
this is a natural property to expect from a system integrity
measurement protocol.
The following theorem formalizes our security prop-
erty. It states that under the assumptions that m.pcr.s
is protected during booting, and that m.pcr.s contains
seq(sinit,BL(m),OS(m),APP(m)) at time t, it is guaran-
teed that the boot loader and operating system used to
boot the machine are BL(m) and OS(m) respectively.
Theorem 2 (Security of Integrity Measurement). The
following is provable in LS2:
ProtectedSRTM(m) (cid:96)
Mem(m.pcr.s,seq(sinit,BL(m),OS(m),APP(m))) @ t
⊃ MeasuredBootSRTM(m,t)
We refer the reader to the full version of this paper for
a detailed proof of this theorem [17]. Major steps in the
proofs are discussed below to illustrate novel reasoning
principles in LS2. All programs mentioned below refer to
Figure 3.
(1) Using
all
the
and
must
show that
sub-sequences
(PCR1)
on
axioms
succession
(PCR2)
in
antecedent
Mem(m.pcr.s,seq(sinit,BL(m),OS(m),APP(m))) @
t, we
of
seq(sinit,BL(m),OS(m),APP(m))
have
appeared in m.pcr.s at times earlier than t, and that
machine m must have been reset at some time tT ,
creating a thread J to boot it. Formally, we obtain
∃tT ,t1,t2,t3,J. (tT ≤ t1 < t2 < t3 < t)
∧ (Mem(m.pcr.s,seq(sinit,BL(m),OS(m))) @ t3)
∧ (Mem(m.pcr.s,seq(sinit,BL(m))) @ t2)
∧ (Mem(m.pcr.s,sinit) @ t1)
∧ (Reset(m,J) @ tT )
∧ (¬Reset(m) on (tT ,t])
(2) Since m was reset creating thread J (second from
last conjunct above), it follows in our model that the
thread J above must have started with the program
SRT M(m). (We have omitted a description of the
rules that force this to be the case.) Thus, we
would like to proceed by proving an invariant of
SRT M(m). However, we can say nothing about the
program b loaded at the end of SRT M(m). This is
because b is read from a memory location m.bl loc,
which could potentially have been written by an
adversarial thread earlier. Fortunately, the extension
of b into m.pcr.s in the second line of SRT M(m)
lets us proceed. Precisely, this extension along with
some basic properties of PCRs lets us prove the
following property that is parametric (universally
quantiﬁed) in the code b. tT and J were obtained in
property (1).
∀t(cid:48),b,o.
(((Mem(m.pcr.s,seq(sinit,b,o)) @ t(cid:48))
∧ (tT < t(cid:48) ≤ t))
⊃ ∃tB. ((tT < tB < t(cid:48)) ∧ (Jump(J,b) @ tB)))
∧ (IsLocked(m.pcr.s,J) @ tB))
This property means that if at any time t(cid:48) between tT
and t, m.pcr.s contained seq(sinit,b,o), then thread
J must have branched to b at some time tB between
tT and t(cid:48), and that J must hold a lock on m.pcr.s
at tB. Informally this holds because the action im-
mediately following the extension in SRT M(m) is
jump b, so if there is a further extension with o,
jump b must have happened in the interim. The
assumption ProtectedSRTM(m) is used to rule out
the possibility that a thread other than J extended o
into m.pcr.s before jump b happened, and to show
that J holds the lock on m.pcr.s at tB.
(3) We instantiate the property in (2), choosing b =
BL(m), o = OS(m), and t(cid:48) = t3 (t3 was obtained
in (1)). Eliminating the antecedents of the impli-
cation using facts from (1), we obtain:
∃tB. ((tT < tB < t3) ∧ (Jump(J,BL(m)) @ tB)
∧ (IsLocked(m.pcr.s,J) @ tB))
J
(4) From (3) we know Jump(J,BL(m)) @ tB. Next
we use the (Jump) rule from Section 2.2. In the
premise we show that ∀tb,te. ∀Q ∈ IS(BL(m)). (cid:96)
[Q]tb,te
A(tb,te) for a suitable invariant A(tb,te),
whose details we omit here (see [17] for details).
The main difﬁculty here is similar to that in (2): we
do not know what o in the program of BL(m) may
be. Again, the invariant we prove is parametric in
o. Using the (Jump) rule, we obtain the following
property.
∀t(cid:48),o,a.
(((Mem(m.pcr.s,seq(sinit,BL(m),o,a)) @ t(cid:48))
∧ (tB < t(cid:48) ≤ t))
⊃ ∃tO. ((tB < tO < t) ∧ (Jump(J,o) @ tO)))
This property is very similar to that in (2), except
it follows from an invariant of BL(m), not
that
SRT M(m). The fact IsLocked(m.pcr.s,J) @ tB from
(3) is needed to rule out the possibility that a thread
other than J extended a into m.pcr.s.
(5) We instantiate the property in (4), choosing o =
OS(m), a = APP(m) and t(cid:48) = t. Combining with
facts from (1), we obtain:
∃tO. ((tB < tO < t) ∧ (Jump(J,OS(m)) @ tO))
in
(3),
(1),
part
The facts (Reset(m,J) @ tT ), (¬Reset(m) on (tT ,t]),
(Jump(J,OS(m)) @
(Jump(J,BL(m)) @ tB),
tO)
of
and
establish
MeasuredBootSRTM(m,t). The remaining part
follows
from a similar analysis with slightly stronger invariants
in (2) and (4).
and
(5)
The hardest part in designing LS2’s proof system was
coming up with sound principles for reasoning about dy-
namically branching to unknown code that are illustrated
above, and in particular, the (Jump) rule. Although the
ﬁnal design is simple to use, it was not obvious at ﬁrst.
We believe that this method for reasoning about branching
to completely unknown code (like b and o) is new to this
work. Prior work on reasoning about dynamically loaded
code, usually based on higher-order extensions of Hoare
logic [24]–[28], assumes that at least the invariants of the
code being branched to are known at the point of branch in
the program. In our setting, this assumption is unrealistic
because we allow executable code to either be obtained
over the network or be read from memory, and hence,
potentially, to come from an adversary.
4.1.2. Insights From Analysis. A number of insights
follow from the analysis. These insights include high-
lighting an unexpected property, clarifying assumptions
on the TCB, and identifying program invariants required
for security.
Property Excludes Last Jump. A key insight from the
analysis is that the integrity measurement protocol does
not provide sufﬁcient evidence to deduce that the last pro-
gram in a chain of measurements is actually executed. For
example, an adversary can reboot the platform after OS(m)
extends APP(m), but before it is jumped to. Alternatively,
a race may occur between two application-level processes
whereby the OS extends the ﬁrst into m.pcr.s and then
the other process reads the value in m.pcr.s before the
ﬁrst process is branched to.
TCB Assumptions. The value of m.pcr.s does not guar-
antee that the measured software was also executed unless
it is also guaranteed that no other process had write access
to m.pcr.s. If the latter assumption fails, an attack exists: a
malicious process may extend a piece of code into m.pcr.s
without executing it. This assumption usually holds in
practice because booting is generally single threaded, but
may fail if for example a malicious thread executes on
another processor core concurrently with the measurement
thread. Formally, this shows up as the ProtectedSRTM(m)
formula, which is a necessary assumption for the proof.
Program Invariants. To establish Theorem 2, we prove
program invariants for the SRT M(m) and BL(m) pro-
grams. These invariants provide a speciﬁcation of the
properties that an SRTM and a boot loader program must
satisfy to be secure in an integrity measurement protocol,
i.e. the assumptions about the TCB. The SRT M(m) in-
variant states that there exists a time point t(cid:48) and thread
J such that J branches to the boot loader b, J does not
branch to any program at any time point before t(cid:48), m.pcr.s
contains the hashed value of the boot loader b, and m.pcr.s
is locked by J at t(cid:48). The invariant of BL(m) states that there
exists a time point t and thread J such that J branches
to program code o only after the entire program code o
has been measured into m.pcr.s. Kauer [29] performed a
manual source code audit of a number of TPM-enabled
boot
loaders to check the informal security condition
that “no code...is executed but not hashed.” Our invari-
ant on the boot loader BL was developed independently
during the course of proving the above theorem and
is a formal speciﬁcation of this condition. We envisage
that
these invariants can be used to derive properties
to automatically check against implementations of TCB
components, thereby providing greater assurance that the
trusted components are trustworthy.
4.1.3. Integrity Reporting. After the integrity measure-
ment protocol loads the PCRs with measurements, the
measurements can be used by the TPM to attest to the
identify of the software loaded on the local platform.
This protocol, called integrity reporting,
involves two
participants. One of the participants is the remote party
itself, called the veriﬁer. Its code is marked Veri f ier(m)
in Figure 3. The other participant in the protocol is the
TPM of machine m in the role of T PMSRT M(m). This code