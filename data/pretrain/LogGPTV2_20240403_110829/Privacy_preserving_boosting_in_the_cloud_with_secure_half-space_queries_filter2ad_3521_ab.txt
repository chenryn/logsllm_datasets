_
_
_
+ +
_+
+
_
_
pTQ1p<0
a<Xi
Xi
Original space
pTQ2p<0
Pi
Perturbation space
Figure 1: Illustration of learning classiﬁers.
The PerturBoost algorithm is basically an algorithm wrapping
the AdaBoost algorithm [2] for processing the perturbed data. We
describe the details later.
After the model is learned, it can be applied in two different set-
tings: either transforming the model back to the unperturbed data
space - the model transformation approach, or transforming the
new feature vector data, {xnew}, whose labels are to be predicted,
to the perturbed space - the data transformation approach. If the
user wants to apply the model remotely in the cloud, then the data
transformation approach should be used. While using the model
locally in the client side, the user can choose any of the two.
3.1 PerturBoost Learning
The PerturBoost framework uses the AdaBoost algorithm to han-
dle the base classiﬁers that adapt to the perturbed data. Algorithm
1 shows the basic structure of the PerturBoost learning algorithm.
Algorithm 1 PerturBoost(B, T r, T s)
1: Input: B: the type of base classiﬁer; T r: the perturbed training
dataset; T s: the perturbed testing dataset.
2: model ← AdaBoost(B, T r, T s);
3: return model;
We describe two types of RASP base classiﬁers.
3.2 RASP Base Classiﬁers
The RASP perturbation only preserves one type of utility: half-
space queries. Thus, the applicable models are limited to linear
classiﬁers. In the preliminary study, we test two types of random
linear classiﬁers: random decision stump and random general lin-
ear classiﬁer. Randomized classiﬁers increase the resilience to the
attacks on model privacy. These classiﬁers, if applied as individual
standalone classiﬁers, are useless because of their low accuracy.
However, they are good enough to serve as weak base classiﬁers in
the boosting framework.
RASP random decision stump. Random decision stumps are
a straightforward translation of the simple range conditions like
Xj < a described in the RASP paper [1]. Note that with the deci-
sion stump form, the query parameter matrix: Θ = (A−1)T uvT A−1
can be simpliﬁed. Let αj be the j-th row of A−1. Θ is actually
(αj − aαd+1)T αd+2, which weakens the model privacy.
RASP random linear classiﬁer. In the RASP query represen-
tation, we try to generate random linear classiﬁers in the following
1032way. The v vector keeps unchanged, while the u vector is set to
(wT , b,0) T for the original query wT x + b < 0. It is easy to check
that this transformation is correct. Thus, the problem is transformed
to ﬁnding an appropriate setting of w and b.
Arbitrarily generated random linear classiﬁers might not be very
useful. It may result in a very skewed partitioning of the dataset.
Instead, we use the following method to increase the chance of ﬁnd-
ing reasonable random linear classiﬁers. First, we normalize each
dimension with the transformation (Xj − μj)/σj, where μj is the
mean and σj is the standard deviation of the dimension Xj.
In
this way, we can reduce the differences caused by very different
domains (e.g., one in the range [0,1] and the other in the range
[100,200]). Then, we choose each dimension of w and the constant
b uniformly at random from the range [−1, 1]. In this way of choos-
ing w, the perpendicular direction of resultant hyperplane will be
uniformly distributed in the unit hyper-sphere.
In addition, the
setting of b will constrain the dimensional intercepts in the range
[−1, 1], forcing the plane to cut the dataset around the center of the
data distribution. This minimizes the chance of generating skewed
linear classiﬁers.
3.3 Discussion on Model Privacy
A potential attack can be conducted to breach the privacy of the
query (i.e., the decision stump model) if a strong assumption is
held that the attacker knows two pairs of input-output queries on
the same dimension. We assume that the attacker knows Xj <
a1 and its encoded form Θ1, and Xj < a2 and Θ2, respectively.
Then, the Θ matrix for any value in the Xj domain can be possibly
enumerated. For instance, for a3 = (a1 + a2)/2, we have the
corresponding Θ3 = (Θ1 + Θ2)/2. As any value in the domain
can be represented as a1 +λ(a2−a1), λ ∈ R, the corresponding Θ
is Θ1+λ(Θ2−Θ1). This means the model privacy is not preserved,
if the attacker is equipped with such additional knowledge. We call
it the model-enumeration attack.
Theoretically, using random linear classiﬁers does not avoid this
attack. After all, if the attacker knows a pair of hyperplanes with
parameters u1 and u2, and their Θs, respectively, he/she can still
use the same enumeration method to derive other hyperplanes and
their Θ representations. However, different from decision stumps,
which have the values constrained in one dimension, this attack
only covers a small vector space, i.e., the points on the line u1 +
λ(u2 − u1). As a random selection of u has extremely low proba-
bility falling on the line, the chance of breaching a randomly gen-
erated linear model with this amount of knowledge is negligible.
Because the random linear classiﬁer approach makes the model-
enumeration attack computationally more expensive, we believe
random linear classiﬁers provide more model-privacy protection
than decision stump classiﬁers. A more rigid study will be con-
ducted for this comparison.
4. PRELIMINARY EXPERIMENTS
We want to understand whether the PerturBoost framework can
generate classiﬁers with satisfactory accuracy.
Datasets. For easier validation and reproducibility of our results,
we use a set of public data from UCI machine learning repository
in experiments. For convenience we also select the datasets of only
two classes. These datasets were widely applied in various classiﬁ-
cation modeling and evaluation.
In pre-processing, the missing values in some datasets (e.g., the
Breast-Cancer and Ionosphere datasets) are replaced with random
samples from the domain of the corresponding dimension. They are
then normalized with the transformation (v − μj)/σj, where μj is
the mean and σ2
j is the variance of the dimension j, to remove the
Dataset
NoPert
Breast-Cancer
Credit-Australian
Credit-German
Diabetes
Heart
Hepatitis
Ionosphere
Spambase
3.7
13.4
22.7
21.6
13.5
12.8
2.8
6.7
DS
2.3
22.5
29.3
22.1
11.2
21.2
12.1
17.0
LC
2.8
11.5
22.7
22.0
12.5
14.7
10.4
11.1
Table 1: Error-rate comparison for different models (%).
bias introduced by the domains. Then, the datasets are randomly
shufﬂed and split into training data (70% of the records) and testing
data (30%). Each of the datasets is also perturbed with the RASP
method.
Implementation. We implement the RASP perturbation based
on the algorithm in the paper [1]. The Weka package [3] is used
to implement the PerturBoost framework. The two base classiﬁers,
RASP random decision stump and RASP random linear classiﬁer,
are implemented based on Weka’s Java interface. The Weka pack-
age also uses the LibSVM library for SVM classiﬁers.
Preliminary Results In the following table (Table 1), “NoPert”
means the best SVM classiﬁers on the original non-perturbed data.
We test SVM classiﬁers with the three popular kernels: linear, ra-
dial basis function, and sigmoid function, and choose the best re-
sults. “DS” represents decision stump base classiﬁers are used for
PerturBoost, and “LC” means general linear base classiﬁers.
Classiﬁers are trained with the training data and tested on the
testing data. Table 1 shows the testing error-rates for the models.
Overall, general linear base classiﬁers give better results than de-
cision stump base classiﬁers, and the results are also close to the
non-perturbed scenarios in most cases.
5. CONCLUSION
This poster presents a preliminary study on the PerturBoost ap-
proach that aims to provide efﬁcient secure classiﬁer learning in
the cloud with both data and model privacy preserved, using previ-
ously studied RASP perturbation approach. The results show that
PerturBoost with certain secure base classiﬁers can generate good
models with accuracy and security guarantee.
6. REFERENCES
[1] CHEN, K., KAVULURU, R., AND GUO, S. Rasp: Efﬁcient
multidimensional range query on attack-resilient encrypted
databases. In ACM Conference on Data and Application
Security and Privacy (2011), pp. 249–260.
[2] FREUND, Y., AND SCHAPIRE, R. E. A short introduction to
boosting. In International Joint Conferences on Artiﬁcial
Intelligence (1999), Morgan Kaufmann, pp. 1401–1406.
[3] HALL, M., FRANK, E., HOLMES, G., PFAHRINGER, B.,
REUTEMANN, P., AND WITTEN, I. H. The weka data mining
software: An update. SIGKDD Explorations 11, 1.
[4] HASTIE, T., TIBSHIRANI, R., AND FRIEDMAN, J. The
Elements of Statistical Learning. Springer-Verlag, 2001.
[5] NARAYANAN, A., AND SHMATIKOV, V. Robust
de-anonymization of large sparse datasets. In Proceedings of
the IEEE Symposium on Security and Privacy (2008),
pp. 111–125.
1033