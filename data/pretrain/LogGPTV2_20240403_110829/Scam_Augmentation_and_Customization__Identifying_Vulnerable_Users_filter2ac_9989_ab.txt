After obtaining Institutional Review Board approval, a recruit-
ment email was sent to all the students at the College of Natural
Sciences and Mathematics, which includes six departments (and
over 20 majors): Biology & Biochemistry, Chemistry, Computer
Science, Earth & Atmospheric Science, Mathematics, and Physics.
We mentioned in the recruiting email that participants will be given
$20 Amazon gift cards upon finishing the experiment. To further
diversify the participant pool, we also recruited staff, so we have
some majors from other colleges as well. We had 34 participants,
of which 15 were female (44%) and 19 were male (56%). We did an
in-lab within-subjects study, which means all participants answered
the same set of questions. Before coming to the lab, all participants
took the Big Five Personality Test [37]. The personality test showed
that the participants’ scores approximately cover from lowest to
the highest score for all the traits which means our participants are
not skewed in one or more directions along this dimension.
The actual experiment consisted of two sections, the first (Le-
git/Fraudulent) designed to study the effectiveness of facade, and the
second (Reasoning) intended to investigate in-depth the strategies
employed by participants. Before these two sections, we asked some
questions about participants’ background and computer knowledge
as well as their demographics. In the first section, for each attack
scenario, we asked (i) “Do you think this is a legitimate scenario or
a fraudulent scenario?” and (ii) “If you think this scenario is fraud-
ulent, please list ALL the reasons that made you think the email is
fraudulent. Otherwise, justify why you think this scenario is legit-
imate.” In the second section, we told them at the beginning that
all the emails in this part are fake and they should just find all the
clues and factors that made this email/scenario fraudulent. Later, we
manually analyzed their reasoning to extract and group their clues
and strategies. We followed the synthesis approach recommended
by [22] to extract and group similar strategies.
In the rest of this section, we review all the issues that we dis-
covered during the pilot study. Then in Section 4, we mention how
we addressed these issues.
3.1 Unaccounted Variables
Participants paid attention to several clues, e.g. “asking for ac-
tion/info”, “legality claim” (a sentence like “this offer is 100% legiti-
mate”), “job info”, and “grammar issues,” etc. “asking for action/info”
was the most used reasoning and 32 (94.1%) participants used it at
least once. We randomly chose emails without modification from
the Joewein Company Representative Scam Dataset as the source
for our fake representative offers. So we did not have any control
over the different clues that exist in each offer. Some emails may
have grammar issues or ask for information but some may not. This
adds an unaccounted variable to our study and affects the validity
of our hypothesis testing.
Session 5: Usable Security & Privacy ASIA CCS '20, October 5–9, 2020, Taipei, Taiwan2383.2 Inadequate Questions
To study the first and third hypotheses (LinkedIn vs. Gmail and
Human vs. NLG), we used four representative offers (one offer
for each combination of variables). The reason that we used only
one offer for each combination was to reduce the total number of
questions from 14 to 10. The second hypothesis (customization)
itself constitutes four different conditions (1) offer with sender
information only, (2) offer with receiver information only, (3) offer
with both sender and receiver information, and (4) offer with neither.
We added one offer for each of the four conditions. So, we had a
total of eight fraudulent offers.
We also added two legitimate offers to be able to measure partic-
ipants’ overall detection performance. The representative offers are
relatively long emails, so we decided not to add more legitimate
offers to minimize respondent fatigue. But on the other hand, hav-
ing more fraudulent offers than legitimate may bias participants’
decisions toward labeling everything as fake.
3.3 Learning Effect
In our pilot study, we randomized the order of the offers once before
starting the experiment and then used the same ordering for all
participants. This can affect participants’ responses since they can
gain knowledge about the offers while they answer the questions.
They may have zero to little knowledge of the representative offers
at the beginning, but their knowledge increases inevitably during
the experiment while they answer the initial questions.
Randomizing the order of questions for each participant is a way
to remove the learning effect, but it does not solve the problem
when there are lots of variables and questions in the study [40]. The
fact that we did a within-subject study made this issue unavoidable.
Doing a between-subjects study and reducing the number of vari-
ables for each participant (and then randomizing the offers) is the
best way to address this issue.
3.4 Practical Issues
As mentioned before, we used the fake representative offers from
Joewein dataset directly without any changes to their content. How-
ever, to embed our variables in them, we added some surrounding
information. Below are some examples of the changes we made:
• Random chosen name was added as a sender of the offers.
• Offers which were originally sent by email, covered in LinkedIn
• Profile picture was added for the offers that were sent with
schema.
LinkedIn schema.
After we checked participants reasoning, we found some issues
that stemmed from the above changes that are unlikely to exist
with actual job offer scams. (1) Some participants complained about
the gender mismatch between the profile picture and the name
that we chose for the sender. (2) When we covered some of the
offers with LinkedIn, we did not pay attention to the content of
the offers, which may contradict with them being sent through
LinkedIn. Some of the offers had this sentence “we found your
resume on Monster.com,” so some participants mentioned why
the offer should be sent through LinkedIn while they found the
resume on Monster.5 (3) Some mentioned that the profile picture is
unprofessional or has low quality.
4 EXPERIMENTAL DESIGN
Taking into account all the lessons learned from the pilot study,
we made several improvements in the experiment design. Now we
describe the final design of the experiment and the rationale behind
the changes that we made to the initial design.
In our pilot study, we asked participants to fill a personality
questionnaire and answer basic computing knowledge and demo-
graphic questions. The goal is to make sure there is no bias in our
sampling from the population. Besides personality and background,
information processing is another aspect of humans’ communica-
tion model that directly affects their decisions [51]. Therefore, we
decided to add the Cognitive Reflection Test (CRT) [26] to our study
to measure participants’ tendency to decide based on their whims.
Due to the existence of various clues in different representative
offers, we decided to keep the content of the offers fixed while we
change a variable (e.g. LinkedIn and Gmail). We could not do this in
the pilot since we did a within-subjects study. In a within-subjects
study, all participants answer all the questions. So, using a fixed
message content results in showing the same offer to the same
participant. For example, when we want to compare the effect of
LinkedIn versus Gmail and having the same content, we have to
show the same offer once in Gmail context and once in LinkedIn.
(i.e. same content for Gmail/LinkedIn variable but with a different
medium). So, we change our experiment to between-subjects study
to avoid this issue. It also has other benefits such as decreasing
the total number of questions from 10 to four for each participant,
which reduces the potential for fatigue and lessens the learning
effect. The between-subjects study also allows us to increase the
number of offers for each variable since we do not need to show
all different combinations of the variables to each participant. This
solves the Inadequate Questions issue mentioned above.
We divide the participants into eight groups, two for “LinkedIn
(L) vs Gmail (G),” two for “Human (H) vs NLG (N),” and four for
different combinations of sender and receiver information (with
sender info. (S), with receiver info. (R), with both sender and receiver
info (SR), and without any info (E)). We use the notation Gi, j,k in
the rest of the paper to refer to these eight groups, where i ∈ {L, G},
j ∈ {H , N}, and k ∈ {S, R, SR, E}. For example, GL,H,E and GG,H,E
compare the performance of participants on offers delivered by
LinkedIn and Gmail (since the goal here is LinkedIn vs Gmail,
we make the second and third variables fixed). Same happens for
Human and NLG, GG,H,E and GG, N ,E and sender/receiver info
(GG,H,S , GG,H,R, GG,H,SR, and GG,H,E). For each of the eight
groups, we add two fraudulent offers.
To detect the baseline performance of participants, we also need
to test their performance on distinguishing between legitimate and
fake offers. Thus, we need some legitimate offers in each group to
test participants’ performance as well as making the number of
fake and real offers equal. So, we add two legitimate offers to each
of the groups.
The last problem that we faced in our pilot study was the learn-
ing effect caused by having lots of questions and a fixed ordering
5https://www.monster.com/
Session 5: Usable Security & Privacy ASIA CCS '20, October 5–9, 2020, Taipei, Taiwan239Table 1: Different offers we include in our experiment and
their distribution among the groups
Group Notation
GL,H,E
GG,H,E
GG,H,E
GG, N ,E
GG,H,S
GG,H,R
GG,H,SR
GG,H,E
1
2
3
4
5
6
7
8
Offers
LinkedIn×2
Gmail×2
Human×2
NLG×2
Sender×2
Receiver×2
None×2
(Sender+Receiver)×2
Legitimate×2
Legitimate×2
Legitimate×2
Legitimate×2
Legitimate×2
Legitimate×2
Legitimate×2
Legitimate×2
for them. As we mentioned in the previous section, randomization
by itself does not solve the problem since the number of variables
for each participant was high. Fortunately, the fact that we do a
between-subjects study reduces the number of variables automati-
cally (only four offers are shown to each participant). On top of that,
we randomize the order of the offers for each participant separately.
Table 1 shows the eight different groups that we have in our
study along with the different types of offers each group is given.
During the experiment, we assign participants randomly to one of
these groups. We used four different legitimate offers in total, the
same two were used for the first four groups and a different set of
two legitimate offers were used in the last four groups.
Since we do a between-subjects study in our new design, we
need more participants compared to our pilot study. This causes
a problem for our reasoning extraction knowing the fact that we
did this manually in the pilot. It is not feasible to do the same for
a huge number of participants. To solve this issue, we automate
this process by applying the following four steps to extract the
keywords from the reasoning: 1) tokenization to split each sentence
into words 2) stop word removal to remove common English word
(e.g. the, about, this) 3) lemmatization to reduce the words into their
base form (e.g. consulting to consult) 4) extracting n-grams after
lemmatization (n ∈ {1, 2, 3}).
4.1 Study Protocol
After receiving the approval from the university’s Institutional
Review Board for the study, we sent an email to all students and
faculty/staff members and invited them to participate in the study
(the link to the survey was included in the email). We mentioned
that the study takes around 20-30 minutes and the first 50 subjects
who finish the experiment will receive a $10 Amazon gift card and
the rest will be entered in a drawing for $20 gift cards (we had 20
of them). We only mentioned that the survey tries to study email-
based attacks and did not disclose phishing at all. We also sent a
reminder one week after sending the first email.
We used SurveyGizmo,6 a data collection platform, which en-
ables us to collect the responses anonymously and also track the
time spent by participants on each question. Tracking the time can
help us to determine participants’ effort on answering the questions.
The survey starts with 44 questions related to the personality
test followed by some questions related to their demographics
6https://www.surveygizmo.com/
and occupation. Since their knowledge of computer, email, and
cybersecurity can affect their detection ability, we ask six questions
to assess this (e.g. “How many years have you been using email,”
“Approximate # of emails you receive in each day,” etc.) The complete
list of questions is with the analysis source code. Then we show
them three questions to evaluate their cognitive reflection. After
finishing the cognitive reflection test, we assign them randomly to
one of the aforementioned eight groups in which we show them
four emails (order of the questions in each group also randomly
changes). We ask three questions for each offer 1) “Do you think
this is a legitimate email scenario or fraudulent?” 2) “How confident
are you about your answer” 3) “Please list all the reasons that made
you think this email is fraudulent or legitimate.” In the end, we
redirect them to another page, to ensure anonymity, and ask for
their name and email address for the gift cards. The study materials
and analysis code are publicly available.7
5 DATA CLEANING
A total of 3,370 participants started the survey out of which 2,248
did it completely. During the first week, we got 1,065 complete
responses and after sending the reminder email, we got 1,183 more.
After collecting the data, we realized it is necessary to filter out
certain responses; multiple responses received from a single partic-
ipant, speeders (those who rushed through the survey), incomplete
responses (those who did not provide enough reasoning) are the
main groups that required filtering.
Multiple Responses: SurveyGizmo has two options to prevent
participants from taking the survey repeatedly, filtering based on
the IP address or based on the browser’s cookie. To avoid any false
positive, we decided not to use any of them for the actual survey and
only used cookie-based filtering for the gift card part of the survey.
So, it is possible to have more than one response from a single
participant in the final responses. Since the survey is anonymous
we cannot detect duplicates by participants’ name/emails. Instead,
we used the combination of two different values to find duplicate
responses; IP address, User-Agent. If two responses have the same
IP address and User-Agent, we only keep the first response based
on their starting time. This filtered out 4 responses.
Speeders: Rushing through the survey and answering questions
without reading them is a common issue in surveys [28]. To de-
tect these responses, we used the time participants spent on each
question. Since the main part of the survey is the four offers that
participants were asked to label as legitimate or fraudulent, we only
considered them to detect speeders. We asked five people to just
read the messages shown to the participants. The average time of
just reading four messages was 102 seconds. So, we used 102 as a
threshold to filter participants based on their response time. This
filtered out 76 responses.
Incomplete Response: While reviewing the participants’ reason-
ing, we realized some participants entered short text to pass the
required questions (e.g. ’N/A’, ’I guess’, ’Not sure’, ’seems legit’, etc.).