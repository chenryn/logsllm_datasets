### 2. State Definitions and Analysis

#### 2.1. CheckSW – 'Check Software Configuration'
This state refers to actions that allow the attacker to gather more information about the system's software or its users. The specific Linux commands included in this state are: `w`, `id`, `whoami`, `last`, `ps`, `cat /etc/*`, `history`, `cat .bash_history`, `php -v`.

#### 2.2. Install – 'Install a Program'
This state refers to the installation of new software by an attacker. Typically, this involves untarring or unzipping a downloaded file, followed by filesystem operations such as copying, moving, and deleting files, creating directories, and changing file permissions. The specific commands included in this state are: `tar`, `unzip`, `mv`, `rm`, `cp`, `chmod`, `mkdir`.

#### 2.3. Download – 'Download a File'
This state refers to remote file downloads by the attacker. Attackers often download TAR/ZIP files containing hacking tools such as SSH scanners, IRC bots, and password crackers. The specific commands included in this state are: `wget`, `ftp`, `curl`, `lwp-download`.

#### 2.4. Run – 'Run a Rogue Program'
This state refers to the attacker running a program that was not originally part of the system. To detect these programs, we looked for the `./` notation, which usually precedes commands run from locations outside the system's binary path. However, some attackers modified the PATH environment variable to run their rogue programs without the `./` notation. We were able to detect most of these cases because attackers repeatedly used the same kits, resulting in three commonly observed binary names: `cround`, `[kjournald]`, `httpd`. Additionally, some attackers used Perl scripts, so we also included `perl` and `*.pl` in this state.

#### 2.5. Password – 'Change the Account Password'
This state refers to changing the password of the compromised account. The only command included in this state is `passwd`.

#### 2.6. CheckHW – 'Check the Hardware Configuration'
This state refers to actions that allow the attacker to gather more information about the system's hardware (uptime, network, CPU speed/type). The specific commands included in this state are: `uptime`, `ifconfig`, `uname`, `cat /proc/cpuinfo`.

#### 2.7. ChangeConf – 'Change the System Configuration'
This state refers to attacker activity that permanently changes the state of the system. Typical examples include setting environment variables, killing running programs, editing files, adding/removing users, and running a modified SSH server (the one rogue program not considered part of the Run state due to its long-term effects on the system and its users). The commands included in this state are: `export`, `PATH=`, `kill`, `nano`, `pico`, `vi`, `vim`, `sshd`, `useradd`, `userdel`.

### 2.8. Summary of Command Coverage
Table 4 provides a summary of how many commands matched each state. Certain commands, such as `cd`, `ls`, `bash`, `exit`, `logout`, and `cat`, were not included in any state because they are routine and have no significant effect on the system. These commands made up a large portion of the observed command set (34.08 percent) and are listed as (no-op) in Table 4. Including no-op commands, our state machine provided nearly full coverage of the observed command set (98.07 percent).

**Table 4. State Machine Coverage**

| State         | Commands | Coverage |
|---------------|----------|----------|
| CheckSW       | 386      | 14.90%   |
| Install       | 377      | 14.55%   |
| Download      | 225      | 8.68%    |
| Run           | 208      | 8.03%    |
| Password      | 203      | 7.83%    |
| CheckHW       | 157      | 6.06%    |
| ChangeConf    | 102      | 3.94%    |
| (unmatched)   | 50       | 1.93%    |
| (no-op)       | 883      | 34.08%   |
| **Total**     | **2591** | **100.00%** |

By inspection, we discovered that over half of the 50 unmatched commands were due to typographical errors by the attackers (they were close matches for valid commands). This shows that while the attackers were most likely following predetermined command sequences, at least several of the attacks were being carried out manually.

### 4.2. Attacker Profile
From the state definitions above, we constructed a profile to illustrate the typical sequence of actions following a compromise. We initially separated attacks on user and root accounts, hoping to see a clear difference between the two. However, we found no significant difference and decided to focus only on the combined dataset to make the trends clearer.

**Figure 1: State Diagram of Post-Compromise Attacker Behavior**

The number labeling each edge indicates how many times that state transition occurred, with the five most common shown in bold. The font size of each state indicates how many total command lines fit the state definition, with a larger font indicating a state with more attacker activity.

To make the diagram clearer and more concise, only the top 25 edges are shown, representing a total of 1,138 state transitions (84.11 percent of the total). The remaining 31 edges, representing 215 transitions (15.89 percent), are hidden. As a result, the in-degree and out-degree of each node will not be equal in most cases (though this is true for the full state machine).

The most popular course of action was to check the software configuration, change the password, check the hardware and/or software configuration (again), download a file, install the downloaded program, and then run it. The 'change configuration' action was less popular, though it occurred fairly equally at three different stages: 1) before and after checking the software configuration, 2) before running a rogue program, and 3) after installing software. Overall, the two most popular attacker activities were checking the software configuration and installing rogue software.

Due to our easy passwords and the fairly small set of commands the attackers ran, we can assume that most of them have a low skill level. Spitzner [4] supports this contention: “Linux systems tend to be the focus of [attackers]... who use commonly known vulnerabilities and automated attack tools.” Under this assumption, the observed behavior makes sense. The attackers are operating on memorized or automated sequences of commands, trying to build back doors into as many computers as possible. A possible explanation for this behavior is their intent to create botnets, which they can sell for profit. Given this motive, their main objectives are: 1) to check the machine's configuration to see if it is suitable for their purposes and 2) to install their rogue software, giving them full back door control of the machine or allowing them to identify other vulnerable hosts, for example. Most attackers appeared to be particularly concerned about detection while installing their software, repeatedly using the `w` command during their shell sessions. This command alone accounts for 8.11 percent of all commands issued, with only the no-op commands `cd` and `ls` having larger percentages.

The Alata study mentioned previously [3] also performed an analysis of post-compromise attacker behavior, and its findings are again very similar to ours. They also observed password change as the most common first step and reported that most attackers went on to download files (i.e., malicious programs) and then tried to install and run executables.

Another study similar to our experiment is [5]. Here, the authors performed an in-depth forensic analysis of post-compromise attacker behavior. They developed some general categories of attacker behavior: discovery, installation, and usage. However, these categories were much broader than ours and not precisely defined. The main difference between their project and our experiment is that we focused on a larger set of less sophisticated attacks. We gathered aggregate statistics about these attacks rather than investigating individual incidents in detail.

### 5. Related Work
There have been many honeypot-related projects and papers in recent years, often appearing in the Honeynet Project's [6] “Know Your Enemy” series of papers [7].

The study that is most similar to our experiment is [3], where the authors collected both attempted login data and post-compromise attacker behavior. Their results closely match ours, although their study was based on a longer time period (131 days) and also included data from geographically distributed low-interaction honeypots. This suggests that even though our results are based on a smaller sample and shorter time period, they seem representative of overall trends.

Seifert [8] conducted a smaller-scale experiment collecting attempted usernames and passwords, with results roughly equal to ours. He recorded one successful login, providing some information about post-compromise attacker behavior.

Another study closely related to our experiment is [5], as mentioned in Section 4.2. The authors performed a detailed analysis of post-compromise attacker behavior, focusing on the individual actions of more sophisticated attackers rather than gathering summary data for a larger number of attackers.

Dacier and colleagues [9] conducted an extensive statistical analysis on malicious traffic using honeypots. Over a four-month period, they studied attacks from 6,285 IP addresses, averaging over two new sources of attack per hour. In another study, they observed 28,722 new attack sources over sixteen months [10]. In a third study, they analyzed data collected over one year and conservatively estimated that 753 tools are available to launch attacks [11]. Finally, they found 924 attack sources per day in Germany during a multi-country study [12].

In 2003, Levine and colleagues [13] showed that a honeynet could be implemented on large-scale enterprise networks to identify malicious activity and pinpoint compromised machines.

### 6. Conclusions
In the course of our experiment, we built a profile of typical attacker behavior following a remote compromise and collected valuable data on commonly attempted usernames and passwords. Our findings are useful to the security community in two main ways.

First, these findings allow security and system administrators to adjust their password policies to ensure that no user accounts are open to trivial brute-force dictionary attacks. At minimum, all of the usernames and passwords presented in Section 3 should be avoided. Direct remote root logins should be disabled, only allowing select users to 'su' into the root account once logged on.

Second, these results can assist system administrators in choosing security tools to combat the most common attacker actions. Our results show that downloading/installing/running rogue software and checking the software configuration are the most common actions. Therefore, security tools and policies should focus on those areas. One possibility would be to restrict execution privileges only to registered programs, though this would require significant modification at the operating system level.

Most of our results will not come as a surprise to security professionals, but they are useful because they represent solid statistical evidence to support widely held beliefs about post-compromise attacker behavior. As expected, downloading/installing/running rogue software, checking the configuration, and changing the password were the most common actions following a successful attack. The two main unexpected results were 1) the very low percentage of successful attacks even with purposely weak passwords (0.31 percent) and 2) the low percentage of successful attacks which resulted in commands being run (22.09 percent). A possibility for future work in this area is to focus on finding explanations for these trends.

### Acknowledgments
This research was inspired by a semester project conducted by Pierre-Yves Dion.

We thank the Institute for Systems Research and the Office for Information Technology for their support in implementing a testbed for collecting attack data at the University of Maryland. In particular, we thank Jeff McKinney, Carlos Luceno, and Peggy Jayant for supporting us in this project with help, material, and space. We thank Gerry Sneeringer and his team for permitting the deployment of the testbed. We also thank Melvin Fields and Dylan Hazelwood for providing some of the computers used in the testbed.

We thank Rachel Bernstein for extensive help with editing, leading to significant improvements in clarity.

This research has been supported in part by NSF CAREER Award 0237493.

### References
[1] S. Panjwani, S. Tan, K. Jarrin, and M. Cukier, “An Experimental Evaluation to Determine if Port Scans are Precursors to an Attack”, in Proc. International Conference on Dependable Systems and Networks (DSN05), Yokohama, Japan, June 28-July 1, 2005, pp. 602-611.
[2] http://www.honeynet.org/tools/sebek/
[3] E. Alata, V. Nicomette, M. Kaâniche, M. Dacier, and M. Herrb, “Lessons learned from the deployment of a high-interaction honeypot”, in Proc. European Dependable Computing Conference (EDCC06), Coimbra, Portugal, October 18-20, 2006, pp. 39-44.
[4] L. Spitzner, “The honeynet project: Trapping the hackers”, IEEE Security and Privacy, 1(2), 2003, pp. 15-23.
[5] F. Raynal, Y. Berthier, P. Biondi, and D. Kaminsky, “Honeypot forensics”, in Proc. IEEE Information Assurance Workshop, United States Military Academy, West Point, NY, June 10-11, 2004, pp. 22-29.
[6] http://www.honeynet.org/
[7] http://www.honeynet.org/papers/kye.html
[8] C. Seifert, “Malicious SSH Login Attempts”, August 2006, http://www.securityfocus.com/infocus/1876.
[9] M. Dacier, F. Pouget, and H. Debar, “Honeypots: Practical Means to Validate Malicious Fault Assumptions,” in Proc. 10th IEEE Pacific Rim International Symposium on Dependable Computing (PRDC04), Papeete, Tahiti, French Polynesia, March 3-5, 2004, pp. 383-388.
[10] F. Pouget, M. Dacier, and V. H. Pham, “Understanding Threats: A Prerequisite to Enhance Survivability of Computing Systems,” in Proc. International Infrastructure Survivability Workshop 2004 (IISW04), Lisbon, Portugal, December 5-8, 2004.
[11] F. Pouget and M. Dacier, “Honeypot-based Forensics,” in Proc. AusCERT Information Technology Security Conf. 2004 (AusCERT04), Ashmore, Australia, May 23-27, 2004.
[12] F. Pouget, M. Dacier, and V. H. Pham, “Leurre.com: On the Advantages of Deploying a Large Scale Distributed Honeypot Platform,” in Proc. E-Crime and Computer Conference 2005 (ECCE05), Monaco, March 29-30, 2005.
[13] J. Levine, R. LaBella, H. Owen, D. Contis, and B. Culver, “The Use of Honeynets to Detect Exploited Systems Across Large Enterprise Networks,” in Proc. IEEE Workshop on Information Assurance, United States Military Academy, West Point, NY, June 18-20, 2003.