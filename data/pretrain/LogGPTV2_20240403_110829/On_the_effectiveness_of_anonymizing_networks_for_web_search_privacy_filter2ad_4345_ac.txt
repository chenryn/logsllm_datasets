occurrences in the distribution). These unique keywords are what
we think might be contributing for query identiﬁcation.
Figure 7: Root Keyword Distribution
The ﬁrst observation looking at Figure 4, 5 and 6 is that the av-
erage accuracies (i.e., the fraction of correctly classiﬁed queries)
are reasonable, i.e., in most cases some fraction of users queries
can always be correctly classiﬁed. The average accuracy across all
the 60 users of interest is 25.95% when N=100, and this decreases
to 18.95% when N=1000. These accuracies show that at least a
quarter of the user’s queries can be easily identiﬁed. The misclas-
siﬁcation rates are also very low in almost all cases.
Looking at Figure 4 across the rows, we ﬁnd that the accuracies
are likely to increase with the number of queries posed by the user.
We can explain this by looking at the distribution in (Figure 1); the
number of users go down considerably with increase in the number
of queries. If there are only few users who pose a large number of
queries, their queries do not get mixed well and thus can be identi-
ﬁed with a high probability. Figure 5 shows that longer queries are
likely more identiﬁable. This is because only a very small fraction
of users issue longer queries (more than 6 words), as seen from
Figure 2.Following the same reasoning, we observe, from Figure
6, that accuracies are expected to increase as the sensitivity of the
query content increases (recall the sensitive/insensitive query dis-
tribution in Figure 3). Though the results might seem intuitive and
follow the trend that users who stand out are easily identiﬁable, they
provide us a very good estimate of what percentage of these user
queries can actually be identiﬁed because of the query properties.
Reasons behind the accuracies.
To understand these results, we try to ﬁnd the reasons behind
how and why a query gets identiﬁed as a user query. Since the da-
ta fed to the machine learning algorithm contained queries broken
down into word vectors, we tried to identify the word usage dis-
tribution among the 1000 users. By trimming down all the query
Reasons behind accuracy decrease.
We understood how the accuracies are obtained, but the question
that is remaining is - “Why do these accuracies decrease when we
increase the value of N, the number of users using web search over
anonymizing network?”. We determine the label of a test query
based on the number of occurrences of similar query words in the
training sets. More the occurrences of such words in the user of
interest’s train set, more are the chances for it to be identiﬁed as a
user of interest’s query and vice versa. With the increase in the val-
ue of N, the size of the other users’s training set greatly increases,
improving the chances of occurrence of test query words within.
This decreases the possibility of classifying the query as a user of
interest’s query. Here is one example that we came across for one
AOL user.“j c penney catalog” was a test query and there was no
exact looking query, or a query formed by subset of its keywords in
the other users’ training set. The words “j c penney” and “catalog”
had occurred before in the particular user’s train set, and hence it
was labelled as the particular user’s query when N=100. However
487
Figure 5: OVA Results Summary for Query Length
Figure 6: OVA Results Summary for Sensitive Queries
as N increased to 300, this query was not labelled as the user of
interest’s query, since there were more occurrences of “j c penney”
and “catalog” terms in the other users’s query training set. In this
way, depending on the occurrence of query terms in the training
sets, the accuracies decrease as the value of N increases.
Inﬂuence of a time gap.
The web content that the user is interested in varies with time.
A time gap between the test and the training data sets might make
it harder to de-anonymize the data, because the common content
between the test and the train sets decreases with time. However,
prior research [20] shows that users tend to pose exact same queries
over and over, as it is easier compared to remembering the url of the
search result, or more efﬁcient than performing an internal search
within the website. This behaviour is described as “Bookmarking”
[20]. For a considerable long period, these queries do not change
and this helps the machine learning approach in identifying at least
a small fraction of the user queries. We tried identifying the per-
centage of same queries repeated in the test and train sets (assumed
to be bookmark queries), for two AOL users with IDs 20894930
and 67910. The percentage of bookmark queries were less than
6%, but the machine learning accuracies were higher than 58%, in-
fact higher than 90% for user ID 20894930. Thus at least a small
fraction of user queries could still be identiﬁed, even when there is
time gap between the test and training data sets.
6. RELATED WORK
Query classiﬁcation problem studied in this paper is very similar
to authorship attribution. Authorship attribution has a long history
beginning in the 19th century. “Federalist Papers” is an early ex-
ample of the problem [12]. In early authorship studies, the primary
goal is to model unique author styles by looking at text character-
istics, such as vocabulary richness (zipf’s word frequency distribu-
tion), word length, choice of rhymes and habit of hyphenation.
During the evolution of authorship attribution problem, type of
data being studied changed from published articles to electronic
text – emails, tweets, blogs and online messages [25]. In the tradi-
488
tional authorship attribution problem, studied texts were long arti-
cles with few authorship possibilities as in the case of “Federalist
Papers”. However, electronic data may neither be long nor does it
have a few possible authors. Also in the case of emails, style of the
text changes according to receiver of the e-mail; same author can
write in different styles for different recipients. These issues make
authorship attribution problem more challenging in the context of
electronic data.
Identiﬁcation of search queries is even harder compared to e-
mails or articles since the query length is much shorter. Howev-
er, recent studies show that “Vanity searches” [19], sending search
queries containing personally identifying information such as name,
address and telephone number; signiﬁcantly contribute towards query
sender identiﬁcation. These vanity searches leak the user privacy,
even when privacy preserving tools like TrackMeNot or Tor are
used.
To the best of authors’ knowledge, this paper is the ﬁrst to s-
tudy the problem of identifying web search queries given a pool of
queries from users of an anonymizing network. This is related but
different from the problem of identifying queries from a search log.
First, an adversary in our application is the search engine itself and
not a third party attempting to de-anonymize a search log. Second,
unlike a third party, the search engine is already in possession of
users’ search history using which it can effectively train a classiﬁ-
er. Moreover, the goals of our study were also different; we were
interested in evaluating existing classiﬁers to address this problem
so as to keep our attacks simple.
7. CONCLUSIONS AND FUTURE WORK
In this paper, we studied the problem of identifying a user’s
queries from a pool of queries received by a search engine over an
anonymizing network. We demonstrated that an adversarial search
engine, equipped with only a short-term search history, can ex-
tract user of interest’s queries by utilizing only the query content
and off-the-shelf machine learning classiﬁers. More speciﬁcally,
by treating a selected set of 60 users – from the publicly-available
AOL search logs – as users of interest performing web search over
an anonymizing network, we showed that their queries can be i-
dentiﬁed with 25.95% average accuracy when N=100, and with
18.95% average accuracy when N=1000. Though the average ac-
curacies are not so high, our results show that few users of interest
can be identiﬁed with accuracies as high as 80–98%, even when
the value of N=1000. We tried to identify the reasons behind how
and why a query gets classiﬁed as a user query, and answer why
the accuracies tend to decrease as the number of users using the
anonymization service increase. Our results, therefore, cast serious
doubt on the effectiveness of anonymizing web search queries by
means of anonymizing networks.
One of the strengths of our attacks is that they only use mini-
mal information (query content) for identiﬁcation of users’ queries
and use off-the-shelf classiﬁcation techniques. Under realistic con-
ditions, it would certainly be possible to improve our attacks by
taking into account other information that would be available to
the search engine under normal circumstances. For instance, exact
query timestamps may very well be a useful attribute. Similarly,
exit node IP address is also likely to improve the accuracies. Fi-
nally, a search engine can build better classiﬁers by training them
on long-term (longer than 2 months) search histories of the users.
By utilizing the geographical locality information accompanying
the queries and the users [7] (place names and details pertaining to
certain localities) and using contextual information for query clas-
siﬁcation [3], we plan to further improve the results. We defer these
items as an interesting avenue for future research.
Acknowledgments
We are grateful to ASIACCS’10 anonymous reviewers for their in-
sightful feedback. We also thank Lisa Hellerstein for discussion
on machine learning classiﬁers and her helpful comments on our
work, and Yasemin Avcular for her suggestions and help with the
experiments.
8. REFERENCES
[1] AOL Search Log Mirrors,
http://www.gregsadetsky.com/aol-data/.
[2] M. Barbaro and T. J. Zeller. A Face Is Exposed for AOL
Searcher No. 4417749. In The New York Times, August 09
2006.
[3] H. Cao, D. H. Hu, D. Shen, D. Jiang, J.-T. Sun, E. Chen, and
Q. Yang. Context-aware query classiﬁcation. In SIGIR ’09:
Proceedings of the 32nd international ACM SIGIR
conference on Research and development in information
retrieval, pages 3–10, New York, NY, USA, 2009. ACM.
[4] C. Castelluccia, E. De Cristofaro, and D. Perito. Private
information disclosure from web searches (the case of
google web history). In Privacy Enhancing Technologies
Symposium (PETS), to appear, 2010. Available at:
http://planete.inrialpes.fr/%7Eccastel/
PAPERS/historio.pdf.
[5] I. W. . E. Frank. Data Mining–Practical Machine Learning
Tools and Techniques, Second Edition. Elsevier, 2005.
[6] S. Goel, A. Broder, E. Gabrilovich, and B. Pang. Anatomy of
the long tail: ordinary people with extraordinary tastes. In
WSDM ’10: Proceedings of the third ACM international
conference on Web search and data mining, pages 201–210,
New York, NY, USA, 2010. ACM.
[7] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.
Categorizing web queries according to geographical locality.
In CIKM ’03: Proceedings of the twelfth international
489
conference on Information and knowledge management,
pages 325–333, New York, NY, USA, 2003. ACM.
[8] S. Hansell. Marketers Trace Paths Users Leave on Internet.
In The New York Times, September 15 2006.
[9] M. Hearst, B. Schvlkopf, S. Dumais, E. Osuna, and J. Platt.
Trends and controversies - support vector machines. IEEE
Intelligent Systems, 13(4):18-28, 1998.
[10] D. Howe and H. Nissenbaum. TrackMeNot: Resisting
Surveillance in Web Search. In On the Identity Trail:
Privacy, Anonymity and Identity in a Networked Society, Ian
Kerr, Carole Lucock and Valerie Steeves (editors), 2008.
[11] E. Kushilevitz and R. Ostrovsky. Replication is not needed:
single database, computationally-private information
retrieval. In FOCS ’97: Proceedings of the 38th Annual
Symposium on Foundations of Computer Science (FOCS
’97), 1997.
[12] F. Mosteller and D. L. Wallace. Inference and Disputed
Authorship: The Federalist. Addison-Wesley, Reading,
Massachusetts, 1964.
[13] J. Novak, P. Raghavan, and A. Tomkins. Anti-aliasing on the
web. In WWW ’04: Proceedings of the 13th international
conference on World Wide Web, pages 30–39, New York,
NY, USA, 2004. ACM.
[14] NYTimes: Google Resists U.S. Subpoena of Search Data,
http://www.nytimes.com/2006/01/20/
technology/20google.html?_r=1.
[15] S. T. Peddinti and N. Saxena. On the Privacy of Web Search
Based On Query Obfuscation: A Case Study of
TrackMeNot. In Privacy Enhancing Technologies
Symposium (PETS), to appear, 2010.
[16] R. Rifkin. Multiclass Classiﬁcation, 06 March 2006. 9.520
Class 08, Available at:
http://ocw.mit.edu/NR/rdonlyres/2CA61B6A-9C0D-4E8A-
AEE3-F4B1BFD96AF0/0/lec8.pdf.
[17] F. Saint-Jean, A. Johnson, D. Boneh, and J. Feigenbaum.
Private web search. In WPES ’07: Proceedings of the 2007
ACM workshop on Privacy in Electronic Society, 2007.
[18] Scroogle.org, http://scroogle.org/.
[19] C. Soghoian. The Problem of Anonymous Vanity Searches.
SSRN eLibrary, 2007.
[20] J. Teevan and E. Adar. Information re-retrieval: repeat
queries in yahoo’s logs. In In SIGIR ’07: Proceedings of the
30th annual international ACM SIGIR conference on
Research and development in information retrieval, pages
151–158. ACM, 2007.
[21] Tor Anonymizing Network,
http://www.torproject.org/.
[22] TrackMeNot: Browser Plugin, http:
//www.mrl.nyu.edu/~dhowe/trackmenot/.
[23] B. Trancer. Click: What millions of people are doing online
and why it matters. Hyperion, 2008.
[24] S. Ye, S. F. Wu, R. Pandey, and H. Chen. Noise injection for
search privacy protection. In International Conference on
Computational Science and Engineering (CSE), pages 1–8,
2009.
[25] R. Zheng, J. Li, H. Chen, and Z. Huang. A framework for
authorship identiﬁcation of online messages: Writing-style
features and classiﬁcation techniques. J. Am. Soc. Inf. Sci.
Technol., 57(3), 2006.