### 4. Analysis of Snapshot Completion Times

In the VNsnap-memory experiments, we observed a significant variation in the VM snapshot completion times, leading to non-trivial TCP backoff (Figure 8(a)). This variability is more pronounced compared to the NEMO3D experiments. There are three primary reasons for this discrepancy:

1. **Memory Configuration**: The VMs in our setup are not configured with the same amount of memory. For instance, VM 2, which has only 350MB of memory, completes its snapshot faster than other VMs.

2. **Activity Levels**: Unlike the NEMO3D experiment where all VMs are equally active, some VMs in the BitTorrent experiment have higher workloads (i.e., larger WWS). At the time of the snapshot, VMs 3, 4, and 5 are primarily communicating with VM 1, while VM 6 remains mostly idle, resulting in a shorter snapshot duration for VM 6.

3. **Host Workload**: The hosts' workloads are not uniform, affecting VM snapshot times. For example, due to resource constraints, the CERIAS VIOLIN relay and the VNsnap-memory daemon are run on the same server. This causes VM 3, served by that daemon, to take longer to complete its snapshot, despite having a similar workload to VMs 4 and 5. The extended snapshot duration for VM 3 results in a TCP backoff period during which VM 3 is the only pre-snapshot VM in the VIOLIN.

To confirm the correctness of VNsnap, we compared the checksums of the original file with those of the files downloaded during the run when the snapshot was taken and during a run restored from the snapshot.

### 5. Discussion

#### Issues and Future Improvements

1. **TCP Throughput Impact**:
   - The discrepancy in VM snapshot completion times, especially for VNsnap-disk, negatively impacts TCP throughput. This can be mitigated by modifying the VM live migration implementation in xend to ensure a uniform or bounded amount of time for transferring VM memory pages to the VNsnap daemons. This would synchronize the "stop and copy" phase across all VMs, reducing the discrepancy in completion times.

2. **Snapshot Size**:
   - The size of VIOLIN snapshots can be reduced by exploiting similarities between different yet similar VM snapshots using efficient hash-based mass storage techniques and compression. In a VIOLIN running NEMO3D, VMs share many pages for the OS, libraries, and application code. Additionally, the similarity between consecutive snapshot images of the same VM can also be leveraged for improved storage efficiency.

3. **Self-Containment and Disruption Tolerance**:
   - For a VIOLIN snapshot to be restorable, the VIOLIN must be self-contained, meaning no application should depend on external connections. Furthermore, applications within the VIOLIN must tolerate the short disruption caused by VNsnap. Many, but not all, applications meet these requirements.

### 6. Related Work

Various techniques have been proposed for checkpointing distributed applications, but few address the need for checkpointing an entire networked infrastructure. These techniques can be categorized into application-level, library-level, and OS-level checkpointing. Each has its limitations:

- **Application-Level Checkpointing**: Requires access to application source code and is highly semantics-specific.
- **Library-Level Checkpointing**: Benefits specific types of applications linked to a particular checkpointing library, often part of message passing libraries like MPI.
- **OS-Level Checkpointing**: Often requires modifications to the OS kernel or new kernel modules and may fail to maintain open connections and accommodate dependencies on local resources.

VNsnap complements these techniques but also has its own limitations (Section 5).

Virtualization solutions like ZapC and Xen on InfiniBand offer checkpointing functionalities but are limited in scope. ZapC provides checkpoint/restart for a pod (PrOcess Domain) but does not capture the entire execution environment, including the OS. Xen on InfiniBand is designed for specific programming models and networks, limiting its applicability to legacy applications on generic IP networks.

Recent solutions based on Xen migration, such as Remus, focus on high-availability services for individual VMs, while VNsnap targets distributed VNEs. An advanced system [5] achieves highly transparent checkpointing of closed distributed systems in Emulab, sharing similar goals with VNsnap but with different system requirements, such as high-accuracy clock synchronization and guest OS modifications.

### 7. Conclusion

We presented VNsnap, a system for taking snapshots of an entire VNE, including VM execution, communication, and storage states. To minimize downtime, we developed optimized live VM snapshot techniques inspired by Xen's live VM migration. We implemented a distributed snapshot algorithm to enforce causal consistency and verified its applicability. Experiments with VIOLINs running unmodified OS and real-world parallel/distributed applications demonstrate VNsnap's unique capability in supporting reliability for virtual infrastructures in cloud computing.

### Acknowledgments

We thank the anonymous reviewers for their very helpful comments. The experiments in this work were conducted on the ReAssure Testbed (http://projects.cerias.purdue.edu/reassure), led by Dr. Pascal Meunier. This work is supported in part by the US NSF under grants 0546173, 0720665, 0721680, 0749140, 0644013, and 0834529. Any opinions, findings, and conclusions or recommendations in this paper are those of the authors and do not necessarily reflect the views of the NSF.

### References

[1] P. Barham, B. Dragovic, K. Fraser, S. Hand, T. Harris, A. Ho, R. Neugebauer, I. Pratt, and A. Warfield. Xen and the art of virtualization. ACM SOSP, 2003.

[2] A. Burtsev, P. Radhakrishnan, M. Hibler, and J. Lepreau. Transparent checkpoints of closed distributed systems in Emulab. ACM EuroSys 2009.

[3] Y. Chen, J. S. Plank, and K. Li. CLIP: A checkpointing tool for message-passing parallel programs. SC97, 1997.

[4] C. Clark, K. Fraser, S. Hand, and J. G. Hansen. Live migration of virtual machines. USENIX NSDI, 2005.

[5] B. Cully, G. Lefebvre, D. Meyer, M. Freeley, N. Hutchinson, and A. Warfield. Remus: High availability via asynchronous virtual machine replication. USENIX NSDI, 2008.

[6] X. Jiang and D. Xu. VIOLIN: Virtual Internetworking on Overlay INfrastructure. Technical Report CSD TR 03-027, Purdue University, 2003.

[7] X. Jiang, D. Xu, H. J. Wang, and E. H. Spafford. Virtual playgrounds for worm behavior investigation. RAID 2005.

[8] A. Kangarlou, P. Eugster, and D. Xu. VNsnap: Taking snapshots of virtual networked environments with minimal downtime. Technical Report CERIAS TR 2008-11, Purdue University, 2008.

[9] A. Kangarlou, D. Xu, P. Ruth, and P. Eugster. Taking snapshots of virtual networked environments. 2nd International Workshop on Virtualization Technology in Distributed Computing, November 2007.

[10] O. Laadan, D. Phung, and J. Nieh. Transparent checkpoint-restart of distributed applications on commodity clusters. IEEE International Conference on Cluster Computing, 2005.

[11] F. Mattern. Efficient algorithms for distributed snapshots and global virtual time approximation. Journal of Parallel and Distributed Computing, 18:423–434, 1993.

[12] D. Meyer, G. Aggarwal, B. Cully, G. Lefebvre, M. Feeley, N. Hutchinson, and A. Warfield. Parallax: Virtual disks for virtual machines. ACM EuroSys, 2008.

[13] A. B. Nagarajan, F. Mueller, C. Engelmann, and S. L. Scott. Proactive fault tolerance for HPC with Xen virtualization. ACM International Conference on Supercomputing (ICS), 2007.

[14] S. Osman, D. Subhraveti, G. Su, and J. Nieh. The design and implementation of Zap: A system for migrating computing environments. USENIX OSDI, 2002.

[15] J. F. Ruscio, M. A. Heffner, and S. Varadarajan. DejaVu: Transparent user-level checkpointing, migration, and recovery for distributed systems. IPDPS 2007.

[16] S. Sankaran, J. M. Squyres, B. Barrett, and A. Lumsdaine. The LAM/MPI checkpoint/restart framework: System-initiated checkpointing. In Proceedings, LACSI Symposium, Santa Fe, pages 479–493, 2003.

[17] D. P. Scarpazza, P. Mullaney, O. Villa, F. Petrini, T. V., and J. Nieplocha. Transparent system-level migration of PGAS applications using Xen on Infiniband. IEEE International Conference on Cluster Computing, 2007.

[18] R. W. Stevens. TCP/IP Illustrated Volume 1. Addison-Wesley, Reading, MA, 1996.

[19] B. White, J. Lepreau, L. Stoller, R. Ricci, S. Guruprasad, M. Newbold, M. Hibler, C. Barb, and A. Joglekar. An integrated experimental environment for distributed systems and networks. In OSDI 2002, pages 255–270.

[20] http://cobweb.ecn.purdue.edu/~gekco/nemo3D.

[21] http://www.bittorrent.com.

[22] M. Armbrust et al. Above the clouds: A Berkeley view of cloud computing. Technical Report No. UCB/EECS-2009-28, UC Berkeley, 2009.