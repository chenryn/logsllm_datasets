10 seconds) among the VMs’ snapshot completion times,
leading to non-trivial TCP backoff (Figure 8(a)).
When looking at the result for VNsnap-memory (Figure
8(b)), one notices that the VM snapshot completion times
are less uniform than those in the NEMO3D experiments.
There are three reasons behind this observation: First, as
described in the experiment setup, not all VMs are con-
ﬁgured with the same amount of memory. For instance,
given that VM 2 has only 350MB of memory, it completes
snapshot before other VMs. Second, unlike the NEMO3D
experiment where all VMs are equally active, some VMs
in the BitTorrent experiment are more active than others
(i.e. they have larger WWS). For example, at the time of
the snapshot, the three client VMs (VMs 3, 4, and 5) are
mostly communicating with VM 1, leaving the other seed
(VM 6) mostly idle and thus a shorter snapshot duration
for VM 6. Third, the workloads of the hosts are not uni-
form, which can have an impact on the VM snapshot times.
For example, due to resource constraints of our testbed, we
have to run the CERIAS VIOLIN relay in the same server
that runs a VNsnap-memory daemon. As a result, it takes
VM 3, which is served by that daemon, longer time to ﬁn-
ish its snapshot despite the fact that VM 3 is just as busy as
other clients (VMs 4 and 5). The longer duration of VM 3
snapshot manifests itself as the TCP backoff during which
VM 3 becomes the only pre-snapshot VM in the VIOLIN.
Finally, we conﬁrm the correctness of VNsnap by compar-
ing the checksum of the original ﬁle with those of the ﬁles
downloaded during the run when the snapshot is taken and
during a run restored from the snapshot.
5. Discussion
In this section, we discuss some issues with VNsnap and
propose future improvement. The ﬁrst issue is the negative
VM uptime
VM(cid:3)uptime
30
25
25
20
15
15
10
5
5
0
)
c
e
s
(
(cid:3)
n
w
o
d
k
a
a
e
r
B
(cid:3)
t
o
h
s
s
p
a
n
S
VM downtime
VM(cid:3)downtime
TCP backoff
TCP(cid:3)backoff
VM uptime
VM(cid:3)uptime
VM downtime
VM(cid:3)downtime
TCP backoff
TCP(cid:3)backoff
)
c
e
s
(
(cid:3)
n
w
o
d
k
a
a
e
r
B
(cid:3)
t
o
h
s
s
p
a
n
S
30
25
25
20
15
15
10
5
5
0
1
1
2
2
3
3
4
4
5
5
6
6
1
1
2
2
3
3
4
4
5
5
6
6
(a) Xen Live Checkpointing
(b) VNsnap-memory Remote
Figure 8. Per-VM breakdowns of snapshot timing for the VIOLIN running BitTorrent.
impact of VM snapshot completion time discrepancy on
TCP throughput – especially for VNsnap-disk. This prob-
lem can be substantially alleviated if we further modify
the VM live migration implementation in xend. As part of
our future work, we plan to have xend spend a uniform or
bounded amount of time transferring VM memory pages to
the VNsnap daemons. As such, all VMs in a VIOLIN will
start their “stop and copy” phase at about the same time.
Considering the very short duration of this phase (i.e. the
VM downtime), their completion times for the VMs will
be of low discrepancy.
The second issue is the size of VIOLIN snapshots. We
note that similarities between different yet similar VM
snapshots can be exploited through efﬁcient hash-based
mass storage techniques (e.g. [15]) and compression. For
instance, in a VIOLIN running NEMO3D, the VMs share
many pages for the OS, library, and application code.
Meanwhile, the similarity between consecutive snapshot
images of the same VM can also be exploited for improved
storage efﬁciency. Such similarity can also be exploited
during snapshot generation to reduce memory and network
bandwidth utilization by VNsnap.
Finally, for a VIOLIN snapshot to be restorable, the VI-
OLIN has to be self-contained. This means that any appli-
cation inside the VIOLIN should not depend on any con-
nections to outside the VIOLIN. In addition, VNsnap re-
quires that applications running inside a VIOLIN be able
to tolerate the short period of disruption incurred by VN-
snap. We believe that many – though not all – applications
meet such requirements.
6. Related Work
Many techniques have been proposed to checkpoint dis-
tributed applications, but few have addressed the need for
checkpointing an entire networked infrastructure. These
techniques can be loosely categorized into application-
level, library-level (e.g. [6, 19]), and OS-level (e.g. [17])
checkpointing. Although these techniques are beneﬁcial in
their own rights and work best in speciﬁc scenarios, each
comes with limitations: Application-level checkpointing
requires access to application source code and is highly
semantics-speciﬁc. Similarly, only a certain type of appli-
cations can beneﬁt from linking to a speciﬁc checkpointing
library. This is because the checkpointing library is usually
implemented as part of the message passing library (such
as MPI) that not all applications use. OS-level checkpoint-
ing techniques often require modiﬁcations to the OS kernel
or require new kernel modules. Moreover, many of these
techniques fail to maintain open connections and accom-
modate application dependencies on local resources such
as IP addresses, process identiﬁers (PIDs), and ﬁle descrip-
tors. Such dependencies may prevent a checkpoint from
being restorable on a new set of physical hosts. VNsnap
complements the existing techniques yet is not without its
own limitations (Section 5).
Virtualization has emerged as a solution to decouple
application execution, checkpointing and restoration from
the underlying physical infrastructure. ZapC [13] is a thin
virtualization layer that provides checkpoint/restart func-
tionality for a self-contained virtual machine abstraction,
namely a pod (PrOcess Domain), that contains a group of
processes. Due to the smaller checkpointing granularity
(a pod vs. a VM), ZapC is more efﬁcient than VNsnap
in checkpointing a group of processes. However, ZapC
does not capture the entire execution environment which
includes the OS itself. Xen on InﬁniBand [20] is a Xen-
based solution with a goal similar to VNsnap. But it is
designed exclusively for the Partitioned Global Address
Space programming models and the InﬁniBand network.
Hence, unlike VNsnap, it does not work with legacy appli-
cations running on generic IP networks.
Recently, three solutions have been proposed based on
Xen migration. [16] advocates using migration as a proac-
tive method to move processes from “unhealthy” nodes to
healthy ones in a high performance computing environ-
ment. Though this method can be used for planned out-
ages or predictable failure scenarios, it does not provide
protection against unexpected failures, nor does it restore
distributed execution states in the event of such failures.
Remus [8] is a practical, guest transparent high-availability
service that protects unmodiﬁed software against physi-
cal host failures. The focus of Remus is individual VMs
whereas VNsnap focuses on distributed VNEs. Remus
leverages an enhanced version of Xen migration to efﬁ-
ciently transfer a VM state to a backup site at high fre-
quency (i.e. 40 times per second for Remus vs. every tens
of minutes for VNsnap). The most related work is an ad-
vanced system [5] that realizes a more powerful capability
of highly transparent checkpointing of closed distributed
systems in Emulab [22]. Being parallel efforts, VNsnap
and [5] share similar goals with different system require-
ments: [5] requires high-accuracy clock synchronization
and modiﬁcations to the guest OS, whereas VNsnap as-
sumes VMs with unmodiﬁed software and no ﬁne-grain
clock synchronization.
7. Conclusion
We have presented the VNsnap system to take snapshots
of an entire VNE, which include images of the VMs with
their execution, communication, and storage states. To
minimize system downtime incurred by VNsnap, we de-
velop optimized live VM snapshot techniques inspired by
Xen’s live VM migration function. We instantiate a dis-
tributed snapshot algorithm to enforce causal consistency
across the VM snapshots and verify the algorithm’s ap-
plicability. Our experiments with VIOLINs running un-
modiﬁed OS and real-world parallel/distributed applica-
tions demonstrate the unique capability of VNsnap in sup-
porting reliability for the emerging virtual infrastructures
in cloud computing.
Acknowledgments
in
for
the
We
thank
reviewers
anonymous
The experiments
their
in this
very helpful comments.
the ReAssure Testbed
work were
conducted
(http://projects.cerias.purdue.edu/reassure)
led by Dr.
Pascal Meunier. This work is supported in part by the US
NSF under grants 0546173, 0720665, 0721680, 0749140,
0644013 and 0834529. Any opinions, ﬁndings, and
conclusions or recommendations in this paper are those of
the authors and do not necessarily reﬂect the views of the
NSF.
References
[4] P. Barham, B. Dragovic, K. Fraser, S. Hand, T. Harris,
A. Ho, R. Neugebauer, I. Pratt, and A. Warﬁeld. Xen and
the art of virtualization. ACM SOSP, 2003.
[5] A. Burtsev, P. Radhakrishnan, M. Hibler, and J. Lepreau.
Transparent checkpoints of closed distributed systems in
Emulab. ACM EuroSys 2009.
[6] Y. Chen, J. S. Plank, and K. Li. CLIP: A checkpointing tool
for message-passing parallel programs. SC97, 1997.
[7] C. Clark, K. Fraser, S. Hand, and J. G. Hansen. Live migra-
tion of virtual machines. USENIX NSDI, 2005.
[8] B. Cully, G. Lefebvre, D. Meyer, M. Freeley, N. Hutchin-
son, and A. Warﬁeld. Remus: High availability via asyn-
chronous virtual machine replication. USENIX NSDI, 2008.
[9] X. Jiang and D. Xu. VIOLIN: Virtual Internetworking on
Overlay INfrastructure. Technical Report CSD TR 03-027,
Purdue University, 2003.
[10] X. Jiang, D. Xu, H. J. Wang, and E. H. Spafford. Virtual
playgrounds for worm behavior investigation. RAID 2005.
[11] A. Kangarlou, P. Eugster, and D. Xu. VNsnap: Taking
snapshots of virtual networked environments with minimal
downtime. Technical Report CERIAS TR 2008-11, Purdue
University, 2008.
[12] A. Kangarlou, D. Xu, P. Ruth, and P. Eugster. Taking
snapshots of virtual networked environments. 2nd Inter-
national Workshop on Virtualization Technology in Dis-
tributed Computing, November 2007.
[13] O. Laadan, D. Phung, and J. Nieh. Transparent checkpoint-
restart of distributed applications on commodity clusters.
IEEE International Conference on Cluster Computing,
2005.
[14] F. Mattern. Efﬁcient algorithms for distributed snapshots
and global virtual time approximation. Journal of Parallel
and Distributed Computing, 18:423–434, 1993.
[15] D. Meyer, G. Aggarwal, B. Cully, G. Lefebvre, M. Feeley,
N. Hutchinson, and A. Warﬁeld. Parallax: Virtual disks for
virtual machines. ACM EuroSys, 2008.
[16] A. B. Nagarajan, F. Mueller, C. Engelmann, and S. L. Scott.
Proactive fault tolerance for HPC with Xen virtualization.
ACM International Conference on Supercomputing (ICS),
2007.
[17] S. Osman, D. Subhraveti, G. Su, and J. Nieh. The design
and implementation of Zap: A system for migrating com-
puting environments. USENIX OSDI, 2002.
[18] J. F. Ruscio, M. A. Heffner, and S. Varadarajan. DejaVu:
Transparent user-level checkpointing, migration, and recov-
ery for distributed systems. IPDPS 2007.
[19] S. Sankaran, J. M. Squyres, B. Barrett, and A. Lumsdaine.
The LAM/MPI checkpoint/restart framework: System-
initiated checkpointing. In in Proceedings, LACSI Sympo-
sium, Sante Fe, pages 479–493, 2003.
[20] D. P. Scarpazza, P. Mullaney, O. Villa, F. Petrini, T. V., and
J. Nieplocha. Transparent system-level migration of PGAS
applications using Xen on Inﬁniband. IEEE International
Conference on Cluster Computing, 2007.
[21] R. W. Stevens. TCP/IP Illustrated Volume 1. Addison-
[1] http://cobweb.ecn.purdue.edu/˜gekco/
nemo3D.
[2] http://www.bittorrent.com.
[3] M. Armbrust et al. Above the clouds: A Berkeley view of
cloud computing. Technical Report No. UCB/EECS-2009-
28, UC Berkeley, 2009.
Wesley, Reading, MA, 1996.
[22] B. White, J. Lepreau, L. Stoller, R. Ricci, S. Guruprasad,
M. Newbold, M. Hibler, C. Barb, and A. Joglekar. An in-
tegrated experimental environment for distributed systems
and networks. In OSDI 2002, pages 255–270.