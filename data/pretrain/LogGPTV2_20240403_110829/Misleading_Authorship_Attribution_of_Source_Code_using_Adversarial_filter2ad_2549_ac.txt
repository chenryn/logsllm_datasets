f(cid:0)T(x)(cid:1) = y∗ .
Equipped with different code transformations for changing
stylistic patterns, we are ready to determine a sequence of
these transformations for untargeted and targeted attacks. We
aim at a short sequence, which makes the attack less likely
to be detected. Formally, our objective is to ﬁnd a short
transformation sequence T that manipulates a source ﬁle x,
such that the classiﬁer f predicts the target label y∗:
(1)
In the case of an untargeted attack, y∗ represents any other
developer than the original author ys, that is, y∗ (cid:54)= ys. In the
case of a targeted attack, y∗ is deﬁned as a particular target
author yt.
As we are unable to control how a transformation T (x)
moves the feature vector φ (x), several standard techniques
for solving the problem in (1) are not applicable, such as
gradient-based methods [e.g. 11]. Therefore, we require an
algorithm that works over a search space of discrete objects
such as the different transformations of the source code. As a
single transformation does not necessarily change the score
of the classiﬁer, simple approximation techniques like Hill
Climbing that only evaluate the neighborhood of a sample
fail to provide appropriate solutions.
As a remedy, we construct our attack algorithm around
the concept of Monte-Carlo tree search (MCTS)—a strong
search algorithm that has proven effective in AI gaming with
AlphaGo [24]. Similar to a game tree, our variant of MCTS
creates a search tree for the attack, where each node repre-
sents a state of the source code and the edges correspond
to transformations. By moving in this tree, we can evaluate
the impact of different transformation sequences before de-
ciding on the next move. Figure 10 depicts the four basic
steps of our algorithm: selection, simulation, expansion and
backpropagation.
Selection. As the number of possible paths in the search tree
grows exponentially, we require a selection policy to identify
the next node for expansion. This policy balances the tree’s
exploration and exploitation by alternately selecting nodes
that have not been evaluated much (exploration) and nodes
that seem promising to obtain a better result (exploitation).
Following this policy, we start at the root node and recursively
select a child node until we ﬁnd a node u which was not
evaluated before. Appendix A gives more information about
the used selection policy.
Simulation & Expansion. We continue by generating a set
of unique transformation sequences with varying length that
start at u. We bound the length of each sequence by a prede-
ﬁned value. In our experiments, we create sequences with up
to 5 transformations. For each sequence, we determine the
classiﬁer score by providing the modiﬁed source code to the
attribution method. The right plot in Figure 10 exempliﬁes
USENIX Association
28th USENIX Security Symposium    485
u
s1
s3
s2
Figure 10: Basic steps of Monte-Carlo tree search. The left plot shows the
selection step, the right plot the simulation, expansion and backpropagation.
the step: we create three sequences where two have the same
ﬁrst transformation. Next, we create the respective tree nodes.
As two sequences start with the same transformation, they
also share a node in the search tree.
Backpropagation. As the last step, we propagate the ob-
tained classiﬁer scores from the leaf node of each sequence
back to the root. During this propagation, we update two
statistics in each node on the path: First, we increment a
counter that keeps track of how often a node has been part of
a transformation sequence. In Figure 10, we increase the visit
count of node u and the nodes above by 3. Second, we store
the classiﬁer scores in each node that have been observed
in its subtree. For example, node u in Figure 10 stores the
scores from s1, s2 and s3. Both statistics are used by the
selection policy and enable us to balance the exploration and
exploitation of the tree in the next iterations.
Iteration. We repeat these four basic steps until a predeﬁned
iteration constraint is reached. After obtaining the resulting
search tree, we identify the root’s child node with the highest
average classiﬁer score and make it the novel root node of the
tree. We then repeat the entire process again. The attack is
stopped if we succeed, we reach a previously ﬁxed number of
iterations, or we do not obtain any improvement over multiple
iterations.
Appendix A provides more implementation details on our
variant of MCTS. We ﬁnally note that the algorithm resembles
a black-box attack, as the inner working of the classiﬁer f is
not considered.
6 Evaluation
We proceed with an empirical evaluation of our attacks and
investigate the robustness of source-code authorship attribu-
tion in a series of experiments. In particular, we investigate
the impact of untargeted and targeted attacks on two recent
attribution methods (Section 6.2 & 6.3). Finally, we verify in
Section 6.4 that our initially imposed attack constraints are
fulﬁlled.
6.1 Experimental Setup
Our empirical evaluation builds on the methods developed
by Caliskan et al. [9] and Abuhamad et al. [1], two recent ap-
proaches that operate on a diverse set of features and provide
superior performance in comparison to other attribution meth-
ods. For our evaluation, we follow the same experimental
setup as the authors, re-implement their methods and make
use of a similar dataset.
Dataset & Setup. We collect C++ ﬁles from the 2017
Google Code Jam (GCJ) programming competition [29]. This
contest consists of various rounds where several participants
solve the same programming challenges. This setting enables
us to learn a classiﬁer for attribution that separates stylistic
patterns rather than artifacts of the different challenges. More-
over, for each challenge, a test input is available that we can
use for checking the program semantics. Similar to previous
work, we select eight challenges from the competition and
collect the corresponding source codes from all authors who
solved these challenges.
In contrast to prior work [1, 9], however, we set more strin-
gent restrictions on the source code. We ﬁlter out ﬁles that
contain incomplete or broken solutions. Furthermore, we for-
mat each source code using clang-format and expand macros,
which removes artifacts that some authors introduce to write
code more quickly during the contest. Our ﬁnal dataset con-
sists of 1,632 ﬁles of C++ code from 204 authors solving the
same 8 programming challenges of the competition.
For the evaluation, we use a stratiﬁed and grouped k-fold
cross-validation where we split the dataset into k − 1 chal-
lenges for training and 1 challenge for testing. In this way,
we ensure that training is conducted on different challenges
than testing. For each of the k folds, we perform feature se-
lection on the extracted features and then train the respective
classiﬁer as described in the original publications. We report
results averaged over all 8 folds.
Implementation. We implement the attribution methods
and our attack on top of Clang [28], an open-source C/C++
frontend for the LLVM compiler framework. For the method
of Caliskan et al. [9], we re-implement the AST extraction
and use the proposed random forest classiﬁer for attributing
programmers. The approach by Abuhamad et al. [1] uses
lexical features that are passed to a long short-term mem-
ory (LSTM) neural network for attribution. Table 2 provides
an overview of both methods. For further details on the fea-
Method
Caliskan et al. [9]
Abuhamad et al. [1]
Syn
•
Lex
•
•
Classiﬁer
Accuracy
RF
90.4% ± 1.7%
LSTM 88.4% ± 3.7%
Table 2: Implemented attribution methods and their reproduced accuracy.
(Lex = Lexical features, Syn = Syntactic features)
486    28th USENIX Security Symposium
USENIX Association
Success rate of our attack
Method
Untargeted
Targeted T+
Targeted T-
Caliskan et al. [9]
Abuhamad et al. [1]
99.2%
99.1%
77.3%
81.3%
71.2%
69.1%
Table 3: Performance of our attack as average success rate. The targeted
attack is conducted with template (T+) and without template (T-).
y
t
i
s
n
e
D
10
8
6
4
2
0
Caliskan et al. [9]
Abuhamad et al. [1]
0
20
40
60
80
100
Changed features per evasive sample [%]
ture extraction and learning process, we refer the reader to
the respective publications [1, 9].
As a sanity check, we reproduce the experiments conducted
by Caliskan et al. [9] and Abuhamad et al. [1] on our dataset.
Table 2 shows the average attribution accuracy and standard
deviation over the 8 folds. Our re-implementation enables
us to differentiate the 204 developers with an accuracy of
90% and 88% on average, respectively. Both accuracies come
close to the reported results with a difference of less than 6%,
which we attribute to omitted layout features and the stricter
dataset.
6.2 Untargeted Attack
In our ﬁrst experiment, we investigate whether an adversary
can manipulate source code such that the original author is
not identiﬁed. To this end, we apply our untargeted attack to
each correctly classiﬁed developer from the 204 authors. We
repeat the attack for all 8 challenges and aggregate the results.
Attack performance. Table 3 presents the performance of
the attack as the ratio of successful evasion attempts. Our
attack has a strong impact on both methods and misleads the
attribution in 99% of the cases, irrespective of the consid-
ered features and learning algorithm. As a result, the source
code of almost all authors can be manipulated such that the
attribution fails.
Attack analysis. To investigate the effect of our attack in
more detail, we compute the ratio of changed features per
adversarial sample. Figure 11 depicts the distribution over all
samples. The method by Caliskan et al. [9] exhibits a bimodal
distribution. The left peak shows that a few changes, such
as the addition of include statements, are often sufﬁcient to
mislead attribution. For the majority of samples, however, the
attack alters 50% of the features, which indicates the tight
correlation between different features (see Section 3.3). A
key factor to this correlation is the TF-IDF weighting that
distributes minor changes over a large set of features.
In comparison, less features are necessary to evade the
approach by Abuhamad et al. [1], possibly due to the higher
sparsity of the feature vectors. Each author has 12.11% non-
zero features on average, while 53.12% are set for the method
by Caliskan et al. [9]. Thus, less features need to be changed
and in consequence each changed feature impacts fewer other
features that remain zero.
Figure 11: Untargeted attack: Histogram over the number of changed features
per successful evasive sample for both attribution methods.
(a) Caliskan et al. [9]
(b) Abuhamad et al. [1]
y
c
n
e
u
q
e
r
F
1
0.75
0.5
0.25
0
0-5
5-10
10-15
15-20
20-25
30-100
25-30
0-5
5-10
10-15
15-20
20-25
25-30
30-160
Number of changed LOC per evasive sample
Removed LOC
Changed LOC
Added LOC
Figure 12: Untargeted attack: Stacked histogram over the number of changed
lines of code (LOC) per successful evasive sample for both attribution meth-
ods. The original source ﬁles have 74 lines on average (std: 38.44).
Although we observe a high number of changed features,
the corresponding changes to the source code are minimal.
Figure 12 shows the number of added, changed and removed
lines of code (LOC) determined by a context-diff with difﬂib
for each source ﬁle before and after the attack. For the ma-
jority of cases in both attribution methods, less than 5 lines
of code are added, removed or changed. This low number
exempliﬁes the targeted design of our code transformations
that selectively alter characteristics of stylistic patterns.
Summary. Based on the results from this experiment, we
summarize that our untargeted attack severely impacts the per-
formance of the methods by Caliskan et al. [9] and Abuhamad
et al. [1]. We conclude that other attribution methods employ-
ing similar features and learning algorithms also suffer from
this problem and hence cannot provide a reliable attribution
in presence of an adversary.
6.3 Targeted Attack
We proceed to study the targeted variant of our attack. We
consider pairs of programmers, where the code of the source
author is transformed until it is attributed to the target author.
Due to the quadratic number of pairs, we perform this experi-
ment on a random sample of 20 programmers. This results
in 380 source-target pairs each covering the source code of
8 challenges. Table 7 in Appendix B provides a list of the
USENIX Association
28th USENIX Security Symposium    487
(a) Caliskan et al. [9]
(b) Abuhamad et al. [1]
Figure 13: Impersonation matrix for both attribution methods. Each cell indicates the number of successful attack attempts for the 8 challenges.
(a) Caliskan et al. [9]
(b) Abuhamad et al. [1]
y
c
n
e
u
q
e
r
F
1
0.75
0.5
0.25
0
0-10
10-20
20-30
30-40
40-50
50-60
60-120
0-10
10-20