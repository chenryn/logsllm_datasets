Effect of window size. We study the effect of different window
sizes in the second sub-figure in Figure 7. The figure demonstrates
a similar trade-off between provable robustness and clean perfor-
mance. As we increase the window size, each window receives more
information from the input and therefore the clean performance
(AP and 1-FAR) improves. However, a large window size increases
the number of windows that are affected by the small adversarial
patch, and the provable robustness drops. In our default setting, we
set the window size to 8 to have a low FAR and good CR.
aeroplanebicyclebirdboatbottlebuscarcatchaircowdiningtabledoghorsemotorbikepersonpottedplantsheepsofatraintvmonitor010203040506070CR (%)051015202530Object size (%)PCD-closeAverage object sizeSession 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3187Figure 7: Effect of different hyper-parameters (left to right: binarizing threshold, window size, DBSCAN 𝜖, DBSCAN min_points)
Effect of DBSCAN parameters. We also analyze the effect of
DBSCAN parameters in DetCluster(·). DBSCAN has two param-
eters 𝜖 and min_points. A point is labeled as a core point when
there are at least min_points points within distance 𝜖 of it; all core
points and their neighbors will be labeled as clusters. We plot the
effect of 𝜖 and min_points in the right two sub-figures in Figure 7.
As we increase 𝜖 or min_points, it becomes more difficult to form
clusters. As a result, the clean performance improves because of
fewer detected clusters and fewer false alerts. However, the prov-
able robustness (CR) drops due to fewer detected clusters in the
worst-case objectness map.
6 DISCUSSION
In this section, we discuss the future work directions and defense
extension of DetectorGuard.
6.1 Future Work
Robust object detection without abstention. In this paper, we
have tailored DetectorGuard for attack detection: when no attack
is detected, the model uses conventional object detectors for predic-
tions; when an attack is detected, the model alerts and abstains from
making predictions. This type of defense is useful in application
scenarios like autonomous vehicles which can give the control back
to the driver upon detecting an attack. However, the most desirable
notion of robustness is to always make correct predictions without
any abstention. How to extend DetectorGuard for robust object
detection without abstention is an interesting direction of future
work.
Better robust image classifier. In DetectorGuard, we use the
key principals introduced in Section 2.4 to co-design the provably
robust image classifier and Objectness Predictor. However, the
imperfection of the adapted robust image classifier still limits the
performance of DetectorGuard detector. Although we can optimize
for few Clean Error 2 and tolerate a potentially high Clean Error 3,
as discussed in Section 3.4, a high Clean Error 3 results in a limited
certified recall of DetectorGuard in the adversarial setting. We note
that DetectorGuard is a general framework that is compatible with
any conventional object detector and (principles for building) provably
robust image classifier, and we expect a boost in DetectorGuard’s
performance given any future advance in robust image classification
research.
Extension to the video setting. In this paper, we focus on
object detection in the single-frame setting. It is interesting to
ˆD ← {}
for 𝑖 ∈ {0, 1, · · · , 𝑛𝑢𝑚_𝑑𝑒𝑡𝑒𝑐𝑡𝑖𝑜𝑛 − 1} do
𝑥min, 𝑦min, 𝑥max, 𝑦max, 𝑙 ← b ← D[𝑖]
𝑙′ ← AuxClassier(x[𝑥min : 𝑥max, 𝑦min : 𝑦max])
if 𝑙′ ≠ “background" then
Algorithm 3 Auxiliary Predictor of DetectorGuard
1: procedure AuxPredictor(D, x)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11: end procedure
ˆD ← ˆD{(𝑥min, 𝑦min, 𝑥max, 𝑦max, 𝑙′)}
end if
end for
return ˆD
extend our DetectorGuard to multiple-frame video setting. We
expect that the temporal information could be useful for robustness.
Moreover, we could perform the defense on a subset of frames to
reduce defense overhead and minimize false alerts in the clean
setting.
6.2 Defense Extension.
In this paper, we propose DetectorGuard as a provably robust de-
fense against patch hiding, or false-negative (FN), attacks. Here,
we discuss how to extend DetectorGuard for defending against
false-positive (FP) attacks. The FP attack aims to introduce incor-
rect bounding boxes in the predictions of detectors to increase
FP. We can consider FP attacks as a misclassification problem (i.e.,
a bounding box is given an incorrect label), and thus this attack
can be mitigated if we have a robust auxiliary image classifier to
re-classify the detected bounding boxes. If the auxiliary classifier
predicts a different label, we consider it as an FP attack and can
easily correct or filter out the FP boxes.
We provide the pseudocode for using an auxiliary classifier (can
be any robust image classifier) against FP attacks in Algorithm 3.
The algorithm re-classifies each detected bounding box in D as
label 𝑙′ (Line 5). We trust the robust auxiliary classifier and add
bounding boxes with non-background labels to ˆD (Line 7). Finally,
the algorithm returns the filtered detection ˆD, and we can replace
the D in Line 8 of Algorithm 1 with ˆD to extend the original
DetectorGuard design. We note that when the patch is not present
in the FP box or it only occupies a small portion of the FP box (the
FP box is large), the auxiliary classifier is likely to correctly predict
“background" since there is no or few corrupted pixel within the
28303234Binarizing Threshold T0510152025303540CR (%)949596979899100AP/1-FAR (%)CR-over-patchCR-close-patchCR-far-patchAP1-FAR456789101112Window size0510152025303540CR (%)949596979899100AP/1-FAR (%)CR-far-patchCR-close-patchCR-over-patchAP1-FAR12345DBSCAN ε0510152025303540CR (%)949596979899100AP/1-FAR (%)CR-over-patchCR-close-patchCR-far-patchAP1-FAR1416182022242628DBSCAN min_points0510152025303540CR (%)949596979899100AP/1-FAR (%)CR-over-patchCR-close-patchCR-far-patchAP1-FARSession 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3188FP box. When the patch occupies a large portion of the FP box,
we might finally output a bounding box that has a large IoU with
the patch. In this case, our object detector correctly locates the
adversarial patch but predicts a wrong label. This is an acceptable
outcome because the class label for patches is undefined. Therefore,
Algorithm 3 presents a strong empirical defense for FP attacks.
7 RELATED WORK
7.1 Adversarial Patch Attacks
Image Classification. Unlike most adversarial examples that in-
troduce a global perturbation with a 𝐿𝑝-norm constraint, localized
adversarial patch attacks allow the attacker to introduce arbitrary
perturbations within a restricted region. Brown et al. [4] introduced
the first adversarial patch attack against image classifiers. They
successfully realized a real-world attack by attaching a patch to the
victim object. Follow-up papers studied variants of localized attacks
against image classifiers with different threat models [21, 27, 28].
Object Detection. Localized patch attacks against object de-
tection have also received much attention in the past few years.
Liu et al. [30] proposed DPatch as the first patch attack against
object detectors in the digital domain. Lu et al. [31], Chen et al. [7],
Eykholt et al. [14], and Zhao et al. [66] proposed different physical
attacks against object detectors for traffic sign recognition. Thys et
al. [50] proposed to use a rigid physical patch to evade human de-
tection while Xu et al. [61] and Wu et al. [57] generated successfully
non-rigid perturbations on T-shirt to evade detection.
7.3 Other Adversarial Example Attacks and
Defenses
Image Classification. Attacks and defenses for classic 𝐿𝑝-bounded
adversarial examples [6, 16, 48] have been extensively studied.
Many empirical defenses [32, 34, 35, 40, 62] were proposed to miti-
gate the threat of adversarial examples, but were later found vulner-
able to adaptive attackers [1, 5, 51]. The fragility of the empirical
defenses has inspired certified defenses that are robust to any at-
tacker considered in the threat model [10, 17, 22, 37, 41, 47, 56]. We
refer interested readers to survey papers [39, 63] for more details.
Object Detection. Global perturbations against object detectors
were first studied by Xie et al. [60] and followed by researchers [54,
55] in different applications. Defenses against global 𝐿𝑝 perturba-
tions are also very challenging. Zhang et al. [64] used adversarial
training (AT) to improve empirical model robustness while Chi-
ang et al. [8] proposed the use of randomized median smoothing
(RMS) for building certifiably robust object detectors. Both defenses
suffer from poor clean performance while DetectorGuard’s clean
performance is close to state-of-the-art object detectors. On PAS-
CAL VOC, AT incurs a ~26% clean AP drop while DetectorGuard
only incurs a <1% drop.12 On MS COCO, both AT and RMS have a
clean AP drop that is larger than 10% while ours is smaller than 1%.
We note that we do not compare robustness performance because
these two works focus on global perturbations and are orthogonal
to the objective of this paper.
We note that it is possible to extend our robust objectness pre-
dictor design and objectness explaining strategy to mitigate attacks
that use global perturbations with a bounded 𝐿∞ norm (if we have
a robust image classifier against 𝐿∞ perturbations). We leave this
as a future work direction.
7.2 Defenses against Adversarial Patches
Image Classification. Digital Watermark (DW) [18] and Local
Gradient Smoothing (LGS) [38] were the first two heuristic de-
fenses against adversarial patch attacks. Unfortunately, these de-
fenses are vulnerable to an adaptive attacker with the knowledge of
the defense. A few certified defenses [9, 24, 33, 36, 58, 59, 65] have
been proposed to provide strong provable robustness guarantee
against any adaptive attacker. Notably, PatchGuard [58] introduces
two key principles of small receptive fields and secure aggregation
and achieves state-of-the-art defense performance for image clas-
sification. In contrast, DetectorGuard aims to adapt robust image
classifiers for the more challenging robust object detection task.
Object Detection. How to secure object detection is a much
less studied area due to the complexity of this task. Saha et al. [46]
demonstrated that YOLOv2 [43] were vulnerable to adversarial
patches because detectors were using spatial context for their pre-
dictions, and then proposed a new training loss to limit the usage
of context information. To the best of our knowledge, this is the
only prior attempt to secure object detectors from patch attacks.
However, this defense is based on heuristics and thus does not
have any provable robustness. Moreover, the attack and defense are
targeted at YOLOv2 only, and it is unclear if the defense generalizes
to other detectors. In contrast, our defense has provable robustness
against any patch hiding attack considered in our threat model and
is compatible with any state-of-the-art object detectors.
8 CONCLUSION
In this paper, we propose DetectorGuard, the first general frame-
work for building provably robust object detectors against patch
hiding attacks. DetectorGuard introduces a general approach to
adapt robust image classifiers for robust object detection using an
objectness explaining strategy. Our evaluation on the PASCAL VOC,
MS COCO, and KITTI datasets demonstrates that DetectorGuard
achieves the first provable robustness against any patch hiding
attacker within the threat model and also has a high clean perfor-
mance that is close to state-of-the-art detectors.
ACKNOWLEDGMENTS
We are grateful to Gagandeep Singh for shepherding the paper
and anonymous reviewers at CCS 2021 for their valuable feedback.
We would also like to thank Vikash Sehwag, Shawn Shan, Sihui
Dai, Alexander Valtchanov, Ruiheng Chang, Jiachen Sun, and re-
searchers at Intel Labs for helpful discussions on the project and
insightful comments on the paper draft. This work was supported
in part by the National Science Foundation under grants CNS-
1553437 and CNS-1704105, the ARL’s Army Artificial Intelligence
Innovation Institute (A2I2), the Office of Naval Research Young
Investigator Award, Schmidt DataX award, and Princeton E-ffiliates
Award.
12RMS [8] did not report results for PASCAL VOC.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3189REFERENCES
[1] Anish Athalye, Nicholas Carlini, and David A. Wagner. 2018. Obfuscated Gra-
dients Give a False Sense of Security: Circumventing Defenses to Adversarial
Examples. In Proceedings of the 35th International Conference on Machine Learning
(ICML). 274–283.
[2] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. 2020. YOLOv4:
Optimal Speed and Accuracy of Object Detection. arXiv preprint arXiv:2004.10934
(2020).
[3] Wieland Brendel and Matthias Bethge. 2019. Approximating CNNs with bag-of-
local-features models works surprisingly well on ImageNet. In 7th International
Conference on Learning Representations (ICLR).
[4] Tom B. Brown, Dandelion Mané, Aurko Roy, Martín Abadi, and Justin Gilmer.
2017. Adversarial patch. In Advances in neural information processing systems
workshops (NeurIPS Workshops).
[5] Nicholas Carlini and David A. Wagner. 2017. Adversarial Examples Are Not
Easily Detected: Bypassing Ten Detection Methods. In Proceedings of the 10th
ACM Workshop on Artificial Intelligence and Security (AISec@CCS). 3–14.
[6] Nicholas Carlini and David A. Wagner. 2017. Towards Evaluating the Robustness
of Neural Networks. In 2017 IEEE Symposium on Security and Privacy (S&P).
39–57.
[7] Shang-Tse Chen, Cory Cornelius, Jason Martin, and Duen Horng Polo Chau. 2018.
Shapeshifter: Robust physical adversarial attack on faster r-cnn object detector.
In Joint European Conference on Machine Learning and Knowledge Discovery in
Databases. Springer, 52–68.
[8] Ping-yeh Chiang, Michael Curry, Ahmed Abdelkader, Aounon Kumar, John
Dickerson, and Tom Goldstein. 2020. Detection as Regression: Certified Object
Detection with Median Smoothing. In Advances in Neural Information Processing
Systems (NeurIPS) 2020, Vol. 33.
[9] Ping-Yeh Chiang, Renkun Ni, Ahmed Abdelkader, Chen Zhu, Christoph Studor,
and Tom Goldstein. 2020. Certified defenses for adversarial patches. In 8th
International Conference on Learning Representations (ICLR).
[10] Jeremy M. Cohen, Elan Rosenfeld, and J. Zico Kolter. 2019. Certified Adversarial
Robustness via Randomized Smoothing. In Proceedings of the 36th International
Conference on Machine Learning (ICML). 1310–1320.
[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. 2009. Ima-
geNet: A large-scale hierarchical image database. In 2009 IEEE Computer Society
Conference on Computer Vision and Pattern Recognition (CVPR). 248–255.
[12] Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al. 1996. A density-
based algorithm for discovering clusters in large spatial databases with noise.. In
Kdd, Vol. 96. 226–231.
[13] Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John M. Winn, and
Andrew Zisserman. 2010. The Pascal Visual Object Classes (VOC) Challenge.
International Journal of Computer Vision 88, 2 (2010), 303–338.
[14] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Florian