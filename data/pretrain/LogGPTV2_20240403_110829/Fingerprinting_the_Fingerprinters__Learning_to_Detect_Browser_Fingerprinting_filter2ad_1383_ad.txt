functionality and structure,
2) It is accompanied with other ﬁngerprinting code (i.e.
most ﬁngerprinting scripts use multiple ﬁngerprinting
techniques), and
3) It does not interact with the functional code in the script.
For example, common patterns include sequentially reading
values from multiple APIs, storing them in arrays or dictio-
3We compute Jaccard similarity between the script, by ﬁrst beautifying it
and then tokenizing it based on white spaces, and all releases of ﬁngerprintjs2.
The release with the highest similarity is reported along with the similarity
score.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:15:55 UTC from IEEE Xplore.  Restrictions apply. 
1150
Initial
New Detections Correct Detections
NON-FP
Itr.
FP NON-FP FP NON-FP FP
103
884
977
84
53
4
4
5
S1
142,642 150
S2
142,549 109
S3 1,056 142,470 76
D1 928
152,426 11
D2 923
152,431
8
D3 926
152,428 13
232
182
158
52
35
36
10
5
1
9
1
2
Enhanced
FP NON-FP
977
142,549
1,056 142,470
1,108 142,418
152,431
923
152,428
926
929
152,425
TABLE I: Enhancing ground truth with multiple iterations of retain-
ing. Itr. represents the iteration number of training with static (S) and
dynamic (D) models. New Detections (FP) represent the additional
ﬁngerprinting scripts detected by the classiﬁer and New Detections
(NON-FP) represent the new non-ﬁngerprinting scripts detected by
the classiﬁer as compared to heuristics. Whereas Correct Detections
(FP) represent the manually veriﬁed correct determination of the
classiﬁer for ﬁngerprinting scripts and Correct Detections (NON-FP)
represent the manually veriﬁed correct determination of the classiﬁer
for non-ﬁngerprinting scripts.
naries, hashing them, and sending them in a network request
without interacting with other parts of the script or page.
reviews
Findings. We found the majority of
to be
straightforward—the scripts in question were often similar to
known ﬁngerprinting libraries and they frequently use APIs
that are used by other ﬁngerprinting scripts. If we ﬁnd any
ﬁngerprinting functionality within the script we label
the
whole script as ﬁngerprinting, otherwise we label it is non-
ﬁngerprinting. To be on the safe side, scripts for which we
were unable to make a manual determination (e.g., due to
obfuscation) were considered non-ﬁngerprinting.
Overall, perhaps expected, we ﬁnd that our ground truth
based on heuristics is high precision but low recall within the
disagreements we analyzed. Most of the scripts that heuristics
detect as ﬁngerprinting do include ﬁngerprinting code, but we
also ﬁnd that the heuristics miss some ﬁngerprinting scripts.
There are two major reasons scripts are missed. First, the
ﬁngerprinting portion of the script resides in a dormant part of
the script, waiting to be called by other events or functions in
a webpage. For example, the snippet in Script 3 (Appendix
IX-D) deﬁnes ﬁngerprinting-speciﬁc prototypes and assign
them to a window object which can be called at a later
point in time. Second, the ﬁngerprinting functionality of the
script deviates from the predeﬁned heuristics. For example,
the snippet
in Script 4 (Appendix IX-D) calls save and
restore methods on CanvasRenderingContext2D el-
ement, which are two method calls used by the heuristics to
ﬁlter out non-ﬁngerprinting scripts [54].
However, for a small number of scripts,
the heuristics
outperform the classiﬁer. Scripts which make heavy use of
an API used that is used for ﬁngerprinting, and which have
limited interaction with the webpage, are sometimes classiﬁed
incorrectly. For example, we ﬁnd cases where the classiﬁer
mislabels non-ﬁngerprinting scripts that use the Canvas API
to create animations and charts, and which only interact with
a few HTML elements in the process. Since heuristics cannot
generalize over ﬁngerprinting behaviors, they do not classify
partial API usage and limited interaction as ﬁngerprinting.
In other cases, the classiﬁer labels ﬁngerprinting scripts as
non-ﬁngerprinting because they include a single ﬁngerprinting
technique along with functional code. For example, we ﬁnd
cases where classiﬁer mislabels ﬁngerprinting scripts embed-
ded on login pages that only include canvas font ﬁngerprinting
alongside functional code. Since heuristics are precise, they
do not consider functional aspects of the scripts and do not
classify limited usage of ﬁngerprinting as non-ﬁngerprinting.
Improvements. Table I presents the results of our manual
evaluation for ground truth improvement for both static and
dynamic analysis. It can be seen from the table that our
classiﬁer is usually correct when it classiﬁes a script as
ﬁngerprinting in disagreement with the ground truth. We
discover new ﬁngerprinting scripts in each iteration. In ad-
dition, it is also evident from the table that our models are
able to correct its mistakes with each iteration (i.e., correct
previously incorrect non-ﬁngerprinting classiﬁcations). This
demonstrates the ability of classiﬁer in iteratively detecting
new ﬁngerprinting scripts and correct mistakes as ground truth
is improved. We further argue that this iterative improvement
with re-training is essential for an operational deployment of
a machine learning classiﬁer and we empirically demonstrate
that for FP-INSPECTOR. Overall, we enhance our ground
truth by labeling an additional 240 scripts as ﬁngerprinting
and 16 scripts as non-ﬁngerprinting for static analysis, as
well as 13 scripts as ﬁngerprinting and 12 scripts as non-
ﬁngerprinting for dynamic analysis. In total, we detect 1,108
ﬁngerprinting scripts and 142,418 non-ﬁngerprinting scripts
with static analysis and 929 ﬁngerprinting scripts and 152,425
non-ﬁngerprinting scripts using dynamic analysis.
static
Combining
and dynamic models.
4) Classiﬁcation Accuracy: We use the decision tree mod-
els described in Section III to classify the crawled scripts.
To establish conﬁdence in our models against unseen scripts,
we perform standard 10-fold cross validation. We determine
the accuracy of our models by comparing the predicted label
of scripts with the enhanced ground truth described in Sec-
tion IV-A3. For the model trained on static features, we achieve
an accuracy of 99.8%, with 85.5% recall, and 92.7% precision.
For the model trained on dynamic features, we achieve an
accuracy of 99.9%, with 96.7% recall, and 99.1% precision.
In FP-
INSPECTOR, we train two separate machine learning models—
one using features extracted from the static representation
of the scripts, and one using features extracted from the
dynamic representation of the scripts. Both of the models
provide complementary information for detecting ﬁngerprint-
ing scripts. Speciﬁcally, the model trained on static features
identiﬁes dormant scripts that are not captured by the dy-
namic representation, whereas the model trained on dynamic
features identiﬁes obfuscated scripts that are missed by the
static representation. We achieve the best of both worlds
by combining the classiﬁcation results of these models. We
combine the models by doing an OR operation on the results
of each model. Speciﬁcally, if either of the model detects a
script as ﬁngerprinting, we consider it a ﬁngerprinting script. If
neither of the model detects a script as ﬁngerprinting, then we
consider it a non-ﬁngerprinting script. We manually analyze
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:15:55 UTC from IEEE Xplore.  Restrictions apply. 
1151
Classiﬁer
Static
Dynamic
Combined
Heuristics (Scripts/Websites)
Classiﬁers (Scripts/Websites)
884 / 2,225
928 / 2,272
935 / 2,272
1,022 / 3,289
907 / 3,278
1,178 / 3,653
FPR
0.05%
0.005%
0.05%
FNR
Recall
15.7% 85.5%
5.3%
96.7%
93.8%
6.1%
Precision
92.7%
99.1%
93.1%
Accuracy
99.8%
99.9%
99.9%
TABLE II: FP-INSPECTOR’s classiﬁcation results in terms of recall, precision, and accuracy in detecting ﬁngerprinting scripts. “Heuristics
(Scripts/Websites)” represents the number of scripts and websites detected by heuristics and “Classiﬁers (Scripts/Websites)” represents the
number of scripts and websites detected by the classiﬁers. FPR represents false positive rate and FNR represent false negative rate.
the differences in detection of static and dynamic models and
ﬁnd that the 94.46% of scripts identiﬁed only by the static
model are partially or completely dormant and 92.30% of the
scripts identiﬁed only by the dynamic model are obfuscated
or excessively miniﬁed.
Table II presents the combined and individual results of
static and dynamic models. It can be seen from the table that
FP-INSPECTOR’s classiﬁer detects 26% more scripts than the
heuristics with a negligible false positive rate (FPR) of 0.05%
and a false negative rate (FNR) of 6.1%. Overall, we ﬁnd
that by combining the models, FP-INSPECTOR increases its
detection rate by almost 10% and achieves an overall accuracy
of 99.9% with 93.8% recall and 93.1% precision.4
B. Breakage
We implement the countermeasures listed in Section III-B in
a browser extension to evaluate their breakage. The browser
extension contains the countermeasures as options that can
be selected one at a time. For API restriction, we override
functions and properties of ﬁngerprinting APIs and return
an error message when they are accessed on any webpage.
For targeted API restriction, we extract a script’s domain by
traversing the stack each time the script makes a call to one
of the ﬁngerprinting APIs. We use FP-INSPECTOR’s classi-
ﬁer determinations to create a domain-level (eTLD+1, which
matches Disconnect’s ﬁngerprinting list used by Firefox) ﬁlter
list. For request blocking, we use the webRequest API [35]
to intercept and block outgoing web requests that match our
ﬁlter list [6].
Next, we analyze the breakage caused by these enforce-
ments on a random sample of 50 websites that load ﬁnger-
printing scripts along with 11 websites that are reported as
broken in Firefox due to ﬁngerprinting countermeasures [17].
Prior research [62], [89] has mostly relied on manual analysis
to analyze website breakage due the challenges in automating
breakage detection. We follow the same principles and man-
ually analyze website breakage under the four ﬁngerprinting
countermeasures. To systemize manual breakage analysis, we
create a taxonomy of common ﬁngerprinting breakage patterns
by going through the breakage-related bug reports on Mozilla’s
bug tracker [17]. We open each test website on vanilla Firefox
(i.e., without our extension installed) as control and also with
4Is the complexity of a machine learning model really necessary? Would a
simpler approach work as well? While our machine learning model performs
well, we seek to answer this question in Appendix IX-E by comparing
our performance to a more straightforward similarity approach to detect
ﬁngerprinting. We compute the similarity between scripts and the popular
ﬁngerprinting library ﬁngerprintjs2. Overall, we ﬁnd that script similarity not
only detects a partial number of ﬁngerprinting scripts detected by our machine
learning model but also incurs an unacceptably high number of false positives.
our extension installed as treatment. It is noteworthy that we
disable Firefox’s default privacy protections in both the control
and treatment branches of our study to isolate the impact of
our protections. We test each of the countermeasures one by
one by trying to interact with the website for few minutes
by scrolling through the page and using the obvious website
functionality. If we discover missing content or broken website
features only in the treatment group, we assign a breakage
label using the following taxonomy:
1) Major: The core functionality of the website is broken.
Examples include: login or registration ﬂow, search bar,
menu, and page navigation.
2) Minor: The secondary functionality of the website is
broken. Examples include: comment sections, reviews,
social media widgets, and icons.
3) None: The core and secondary functionalities of the web-
site are the same in treatment and control. We consider
missing ads as no breakage.
Policy
Blanket API restriction
Targeted API restriction
Request blocking
Hybrid
Major (%) Minor (%)
48.36%
24.59%
44.26%
38.52%
19.67%
5.73%
5.73%
8.19%
Total (%)
68.03%
30.32%
50%
46.72%
TABLE III: Breakdown of breakage caused by different countermea-
sures. The results present the average assessment of two reviewers.
To reduce coder bias and subjectivity, we asked two re-
viewers to code the breakage on the full set of 61 test
websites using the aforementioned guidelines. The inter-coder
reliability between our two reviewers is 87.70% for a total of
244 instances (4 countermeasures ⇥ 61 websites). Table III
summarizes the averaged breakage results. Overall, we note
that targeted countermeasures that use FP-INSPECTOR’s detec-
tion reduce breakage by a factor of 2 on the tested websites that
are particularly prone to breakage.5 More speciﬁcally, blanket
API restriction suffers the most (breaking more than two-thirds
of the tested websites) while the targeted API restriction causes
the least breakage (with no major breakage on about 75% of
the tested websites).
Surprisingly, we ﬁnd that the blanket API restriction causes
more breakage than request blocking. We posit this is caused
by the fact that blanket API restriction is applied to all scripts
on the page, regardless of whether they are ﬁngerprinting,
since even benign functionality may be impacted. By compar-
5These websites employ ﬁngerprinting scripts and/or are reported to be
broken due to ﬁngerprinting-speciﬁc countermeasures. Thus, they represent a
particularly challenging set of websites to evaluate breakage by ﬁngerprinting
countermeasures.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:15:55 UTC from IEEE Xplore.  Restrictions apply. 
1152
ison, request blocking only impacts scripts known to ﬁnger-
print. Next, we observe that targeted API restrictions has the
least breakage. This is expected, as we do not block requests
and only limit scripts that are suspected of ﬁngerprinting; the
functionality of benign scripts is not impacted.
We ﬁnd that the hybrid countermeasure causes less breakage
than request blocking but more breakage than the targeted
API restrictions. The hybrid countermeasure performs better
than request blocking because it does not block network
requests to load ﬁrst-party ﬁngerprinting resources and instead
applies targeted API restrictions to protect against ﬁrst-party
ﬁngerprinting. Whereas it performs worse than targeted API
restrictions because it still blocks network requests to load
third-party ﬁngerprinting resources that are not blocked by
the targeted API restrictions. Though hybrid blocking causes
more breakage than targeted API restriction,
it offers the
best protection. Hybrid blocking mitigates both active and
passive ﬁngerprinting from third-party resources, and active
ﬁngerprinting from ﬁrst-party resources and inline scripts.
The only thing missed by hybrid blocking—passive ﬁrst-
party ﬁngerprinting—is nearly impossible to block without
breaking websites because any ﬁrst-party resource loaded by
the browser can passively collect device information.
We ﬁnd that the most common reason for website breakage
is the dependence of essential functionality on ﬁngerprinting
code. In severe cases, registration/login or other core func-
tionality on a website depends on computing the ﬁngerprint.
For example, the registration page on freelancer.com is blank
because we restrict the ﬁngerprinting script from f-cdn.com.
In less severe cases, websites embed widgets or ads that rely
on ﬁngerprinting code. For example, the social media widgets
on ucoz.ru/all/ disappears because we apply restrictions to the
ﬁngerprinting script from usocial.pro.
V. MEASURING FINGERPRINTING IN THE WILD
Next, we use the detection component of FP-INSPECTOR
to analyze the state of ﬁngerprinting on top-100K websites.