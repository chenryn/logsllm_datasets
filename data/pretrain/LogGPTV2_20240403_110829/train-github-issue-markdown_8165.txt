Calculating a gradient of the SVD of a matrix where at least two of the
singular values are equal lead to a gradient with only NaNs. Let me illustrate
the problem by a sample code and result:
    import jax
    import jax.numpy as jnp
    def f(arr):
        U, S, Vh = jnp.linalg.svd(arr, full_matrices=False)
        return jnp.linalg.norm((U * S) @ Vh)
    A = jnp.diag(jnp.asarray([1, 1, 2, 3], dtype=float))
    f_g = jax.value_and_grad(f)
    print(f_g(A))
Result:
    (DeviceArray(3.8729835, dtype=float32),
     DeviceArray([[nan, nan, nan, nan],
                  [nan, nan, nan, nan],
                  [nan, nan, nan, nan],
                  [nan, nan, nan, nan]], dtype=float32))
This sample is nothing but a very complicated expression of the norm of a
matrix whose gradient is defined (easy to see if replacing the gradient call
by `jax.value_and_grad(jnp.linalg.norm)` [2]).
After some discussion with @frederikwilde (the author of the commit `f443e19`)
we found that the problem is in the calculation of the derivative of the
singular matrices _U_ and _V â€ _. The derivation of the derivative calculates
in one step the matrix of the inverses _1/(S i \- Sj)_ (with _S i_ the
singular values). Obviously, this leads to the described problem.
In the source code
https://github.com/google/jax/blob/main/jax/_src/lax/linalg.py#L1291 there is
already the commented out code part [1] which would implement a pseudo-inverse
in this case. As far as I understand the reason why this code is not used at
the moment is the general problem that there is a gauge-freedom of the
singular subspace (which has n>1 if there are two equal singular values) is
not solved and therefore the derivative of only one of the two unitaries alone
would make no sense in this case.
As far as I understand the problem, it should be ok to do the pseudo-inverse
of the differences, if users are not interested in the derivative of a single
unitary but of the complete product _U * S @ Vh_ or variants of this one. To
illustrate that enabling the code path [1] leads to correct results in some
cases, one can replace the code line and rerun the example above. The
resulting gradient is the same one as if one used the VJP-rule of the
`jnp.linalg.norm` function. [3]
What is your opinion on this problem? Can the gauge-freedom somehow be fixed
so it is no problem? Is the use case of using only the derivative of one
unitary very unlikely so one could stick to the pseudo-inverse?
[1] `s_diffs_zeros = jnp.ones((), dtype=s.dtype) * (s_diffs == 0.)` (fixing
the bug that it has to be `s.dtype` instead of `A.dtype`)
[2]
    import jax
    import jax.numpy as jnp
    A = jnp.diag(jnp.asarray([1, 1, 2, 3], dtype=float))
    norm_g = jax.value_and_grad(jnp.linalg.norm)
    print(f_g(A))
Result:
    (DeviceArray(3.8729832, dtype=float32),
     DeviceArray([[0.2581989, 0.       , 0.       , 0.       ],
                  [0.       , 0.2581989, 0.       , 0.       ],
                  [0.       , 0.       , 0.5163978, 0.       ],
                  [0.       , 0.       , 0.       , 0.7745967]], dtype=float32))
[3] Result for example code with
https://github.com/google/jax/blob/main/jax/_src/lax/linalg.py#L1291 replaced
by [1]:
    (DeviceArray(3.8729835, dtype=float32),
     DeviceArray([[0.25819886, 0.        , 0.        , 0.        ],
                  [0.        , 0.25819886, 0.        , 0.        ],
                  [0.        , 0.        , 0.5163977 , 0.        ],
                  [0.        , 0.        , 0.        , 0.7745966 ]], dtype=float32))