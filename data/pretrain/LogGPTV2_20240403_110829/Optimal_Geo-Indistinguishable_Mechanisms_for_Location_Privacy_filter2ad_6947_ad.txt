PL. Note that at the end of this process, by construction,
the QL of the three mechanisms is q.
We begin the evaluation comparing the location privacy
of each mechanism for each of the selected users, under the
four constructed priors. We ﬁx  = 1.07 (which intuitively
corresponds to a ratio of 2 between the probability for two
regions adjacent in the grid to report the same observed
location) and δ = 1.05. Figure 3 shows a boxplot of the
location privacy (in km) oﬀered by the diﬀerent mechanisms
under each prior. In all four cases, the general performance
of our mechanism is better than that of the others, with the
only exception being the all-day prior (which is the one used
in the construction of the mechanisms) since, as explained in
Section 3.4, OptQL and OptPriv are q-OptPriv(π, d2, d2)
and therefore oﬀer the same privacy.
Finally, to show the beneﬁts of using a mechanism with
optimal utility, we compare now the QL of the mechanisms
OptQL and PL when both mechanisms are generated with
the same privacy level . We can see the results in Fig-
ure 4. The OptQL mechanism clearly oﬀers a better util-
ity to the user, while guaranteeing the same level of geo-
indistinguishability.
4.3 Performance of the approximation
algorithm
We recall from Section 3.2 that if we consider a large
number of locations in X , then the number of constraints
in the linear program might be large. Hence, we intro-
duced a method based on a spanning graph G to reduce
the total number of constraints of the linear program. How-
ever, in general the obtained mechanism is no longer dX -
OptQL(π, dQ), and therefore it has a higher QL than the
optimal one.
In this section we study the tradeoﬀ between the increase
in the QL of the mechanism and the reduction in the num-
ber of constraints of the optimization problem, as a conse-
quence of using our approximation technique. We also show
how this reduction aﬀects the running time of the whole ap-
proach. We start by constructing the OptQL mechanism
for all selected users and for diﬀerent dilations in the range
from 1.05 to 2.0, in all cases considering  = 1.07 as before.
We then measure the QL of each mechanism under the user
proﬁle. We can see the results in Figure 5a. It is clear that
the QL increases slowly with respect to the dilation: the me-
dian value is 0.946 km for δ = 1.05, is 0.972 km for δ = 1.1,
and 1.018 km for δ = 1.2. Therefore we can deduce that,
for a reasonable approximation, the increase in the quality
loss is not really signiﬁcant. It is worth noting that we do
not show the QL for δ = 1 in the plot (corresponding to the
case where dX and dG are the same). The reason is that
in that case the number of constraints is really high, and
therefore it takes a lot of time to generate one instance of
the mechanism (and much more time to generate it for the
86 users considered).
The relation between the dilation and the number of con-
straints is shown in Figure 5b. Note that this number is
independent from the user, and therefore it is enough to cal-
culate it for just one of them. It is clear that the number of
constraints decreases exponentially with respect to the di-
lation, and therefore even for small dilations (which in turn
mean good approximations) the number of constraints is
Figure 4: Quality loss of the OptQL and PL mecha-
nisms for diﬀerent values of . The mechanisms were
calculated for all users. Here, points represent the
utility for every user, while the two lines join the
medians for each mechanism and each value of .
see that most of the selected regions are located in the south-
east of the Haidian district, and all of them are located in
the north-west of Beijing. We consider the set of locations
X to be the centers of the selected regions, and the metric
dX to be the Euclidean distance between these centers, i.e.
dX = d2.
Finally, a second ﬁltering is performed, again keeping users
with at least 20 points in each time period, but this time con-
sidering only the 50 selected regions. After this, we end up
with a ﬁnal set of 86 users (46% of the total 182).
In this section, we evaluate the location privacy and the
utility of three diﬀerent mechanisms under the several prior
distributions for each user. These priors correspond to diﬀer-
ent parts of the day (all day, morning, afternoon and night),
and are computed by counting the number of points, logged
in the corresponding time period, that fall in each of the se-
lected regions (again, counting only once those points logged
within the same hour), and then by normalizing these num-
bers to obtain a probability distribution.
We start by evaluating the location privacy provided by
the diﬀerent mechanisms. However, we must note that in
general location privacy mechanisms do not satisfy dX -
privacy unless they are speciﬁcally designed to do so. There-
fore, for this evaluation, we measure location privacy with
the metric AdvError, proposed in [8] and described in Sec-
tion 2.1, which measures the expected error of the attacker
under a given prior distribution. In order to perform a fair
comparison, we construct the mechanisms in such a way that
their QL coincide. The ﬁrst step is to select a privacy level
 and a dilation δ, and then to construct the mechanism de-
scribed in Section 3.2. We will call this mechanism OptQL.
This mechanism has a QL of q = QL(OptQL, π, d2). We
then continue by constructing the optimal mechanism of
Shokri et al [8], and setting the QL as q. We call this mech-
anism OptPriv. Finally, we compute a discretized version
of the Planar Laplace mechanism of Andr´es et al [9]. under
0.00.51.01.52.02.53.03.54.04.55.05.56.00.20.40.60.81.01.21.41.61.82.0EpsilonQuality Loss (km)OptQLPL258Figure 5: (a) Boxplot of the relation between QL and dilation for the mechanism OptQL with privacy
constraint  = 1.07. The spanner is calculated with the greedy algorithm presented in Section 3.3.
(b)
Relation between the approximation ratio and the number of constraints in the linear program. This number
is independent from the user and form the value of .
signiﬁcantly reduced with the proposed approximation tech-
nique. For instance, we have 87250 constraints for δ = 1 (the
optimal case), and 25551 constraints for δ = 1.05. This rep-
resents a decrease of 71% with respect to the optimal case,
with only 1.05 approximation ratio.
It is also worth noting that, between δ = 1.4 and δ = 1.45
there is a pronounced decrease in the number of constraints
(Figure 5b) and also a decrease in the QL (Figure 5a). This
might seem counterintuitive at ﬁrst, since one would expect
that a worse approximation should always imply a higher
loss of quality. However, there is a simple explanation: al-
though the spanner with δ = 1.45 has a higher worst-case
approximation ratio, the average-case ratio is actually better
that the one of the spanner with δ = 1.4. This phenomenon
is a consequence of the particular topology of the set of lo-
cations and to the algorithm used to get the spanner.
Finally, we measure the running time of the method used
to generate the OptQL mechanism, under diﬀerent methods
to solve the linear optimization problem. The experiments
were performed in a 2.8 GHz Intel Core i7 MacBook Pro
with 8 GB of RAM running Mac OS X 10.9.1, and the source
code for the method was written in C++, using the routines
in the GLPK library for the linear program. We compare
the performance of three diﬀerent methods included in the
library: the simplex method in both its primal and dual
form, and the primal-dual interior-point method. Besides,
we run these methods on both the primal linear program
presented in Section 3.2 and its dual form, presented in Ap-
pendix 6. Since the running time depends mainly on the
number of locations being considered, in the experiments
we focus on just one user of the dataset, and we ﬁx the pri-
vacy level as  = 1.07. The results can be seen in Table 1.
Some ﬁelds are marked with “1h+”, meaning that the exe-
cution took more than one hour, after which it was stopped.
Others are marked with “Error”, meaning that the execution
stopped before one hour with an error2. A particular case of
error happened when running the interior-point method on
the dual linear program, where all executions ended with a
2The actual error message in this case was: “Error: unable
to factorize the basis matrix (1). Sorry, basis recovery pro-
cedure not implemented yet”
|X|
50
75
Primal simplex
Dual simplex
Pr. LP Du. LP
Pr. LP Du. LP
57s
46.4s
4m 37s
2s
Error
1h+
1h+
1h+
1h+
1h+
1h+
5.2
2s
1s
1s
1h+
Error
Error
5m 55s
21.8s
40s
5.9s
4s
2s
2s
29m 26s
1m 12s
42s
19.2s
27.2s
45s
15.5s
1h+
3s
2s
1h+
2m 19s
48.4s
1h+
15.5s
Interior
Pr. LP
49m 20s
7.5s
2.7s
0.5s
0.5s
1h+
55s
11.7s
2.2s
1.7s
δ
1.0
1.1
1.2
1.5
2.0
1.0
1.1
1.2
1.5
2.0
Table 1: Execution times of our approach for 50
and 75 locations, for diﬀerent values of δ, and using
diﬀerent methods to solve the linear program.
“numerical instability” error (and therefore this case is not
included in the table). From the results we can observe that:
• The only two methods that behave consistently (that
never ﬁnish with error, and the running time increases
when the dilation decreases) are the dual simplex and
the interior-point methods, both when applied to the
primal program.
• From these, the interior-point method performs better
in the case of bigger dilation, while it does it much
worse for very small ones.
• Somewhat surprisingly, the dual linear program does
not oﬀer a signiﬁcant performance improvement, spe-
cially when compared with the interior-point method.
In the case of OptPriv, the mechanism is generated using
Matlab’s linear program solver (source code kindly provided
by the authors of [8]). We generated the mechanism for
the same cases, and observed that the running time mainly
depends on the number of regions: for 50 regions, the mech-
anism is generated in approximately 1 minute, while for 75
regions it takes about 11 minutes.
4.4 The T-Drive dataset
In order to reaﬃrm the validity of the proposed approach,
we performed the same evaluation in a diﬀerent dataset: the
T-Drive trajectories dataset. This dataset contains traces
0.60.70.80.91.01.11.21.31.41.51.61.71.051.11.151.21.251.31.351.41.451.51.551.61.651.71.751.81.851.91.952DilationQualityLoss(km)(a)(b)01000020000300004000050000600007000080000900001.01.11.21.31.41.51.61.71.81.92.0DilationConstraints259other mechanisms (again, with the exception of the all day
prior, for which we know that these values coincide). We
can also see in Figure 7 the comparison in terms of utility
of the mechanisms OptQL and PL. Again, the quality loss
of OptQL is, in all cases, better than the one of PL. This
is to be expected, since, from all mechanisms providing a
certain geo-indistinguishability, OptQL is the one with op-
timal utility (or really close to the optimal utility when the
approximation is used).
5. CONCLUSION AND RELATED WORK
Related work
In the last years, a large number of location-privacy protec-
tion techniques, diverse both in nature and goals, have been
proposed and studied. Many of these aim at allowing the
user of an LBS to hide his identity from the service provider.
Several approaches are based in the notion of k-anonymity
[25, 26, 27], requiring that the attacker cannot identify a user
from at least other k− 1 diﬀerent users. Others are based on
the idea of letting the users use pseudonyms to interact with
the system, and on having regions (mix zones, [4, 6]), where
the users can change their pseudonyms without being traced
by the system. All these approaches are incomparable with
ours, since ours aims at hiding the location of the user and
not his identity.
Many approaches to location privacy are based on obfus-
cating the position of the user. A common technique for this
purpose is cloaking [28, 29, 30, 26], which consists in blur-
ring the user’s location by reporting a region to the service
provider. Another technique is based on adding dummy lo-
cations[31, 32, 5] to the request sent to the service provider.
In order to preserve privacy, these dummy locations should
be generated in such a way that they look equally likely
to be the user’s real position. A diﬀerent approach is to
construct mechanisms that provide optimal privacy under
certain quality constraints [8] (an approach dual to ours, as
discussed in the introduction), while [33] additionally takes
into account bandwidth constraints. Finally, collaborative