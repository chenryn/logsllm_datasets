title:Synergistic Coordination between Software and Hardware Fault Tolerance
Techniques
author:Ann T. Tai and
Kam S. Tso and
Leon Alkalai and
Savio N. Chau and
William H. Sanders
c 
/” 
Synergistic+,doordination between Software and Hardware 
i 
.... Fault 
- 
.! Moreover, the protocol- 
coordination approach preserves arid enhances the features 
arid advantages of the individual protocols that participate 
in the coordination, keeping the pe@orniance cost IOLV. 
1  Introduction 
To  achieve  high  dependability  in  critical  applications, 
software and hardware fault tolerance issues must be dealt 
with  simultaneously.  Often,  software  and  hardware  fault 
tolerance schemes are developed separately using different 
types  of  techniques, due  to  the  distinctions  in  nature  be- 
tween  software and hardware faults.  For centralized com- 
puting applications, a simple integration of techniques from 
the  two  categories may  effectively  achieve the  goal  of  si- 
multaneous software and hardware fault tolerance.  For ex- 
ample, using N-version  programming in  a system with  N- 
modular  redundancy  could  permit  a  software  error  to  be 
masked by design diversity and a hardware fault to be toler- 
ated via graceful redundancy degradation. 
In the context of distributed computing, however, naively 
combining  software  and  hardware  fault  tolerance  tech- 
niques may  lead  to  an  invalid solution.  In  particular,  po- 
*The work reported in this paper was supported  in part by Small Busi- 
ness Innovation Research (SBIR) Contract  NAS3-99125 from Jet  Propul- 
sion Laboratory, National Aeronautics  and Space Administration. 
~~ 
~ 
~ 
tential incompatibilities between the techniques may make 
their combination  inefficient, or may  cause them  to  inter- 
fere with  one another, resulting  in  a  detrimental effect on 
system reliability. For example, to enable a distributed sys- 
tem to recover from a software error, it will be necessary to 
apply a software fault tolerance scheme that maintains con- 
sistency among the views on messagektate validity from the 
standpoints of interacting processes; however, this consis- 
tency may not be preserved  in  the checkpoints established 
by a hardware fault tolerance protocol executing in the same 
system.  Consequently, the system  may  become  unable to 
recover from  a software error that  occurs  after a rollback 
recovery caused by a hardware fault. 
Despite its importance and difficulties, seamless integra- 
tion  of  software  and  hardware  fault  tolerance  techniques 
for distributed  systems has  not received  enough attention. 
Several fault tolerance schemes have been proposed to deal 
with both software and hardware faults by mapping diverse 
software versions to redundant computer nodes (see [ 1, 21, 
for  example).  However,  the  scope  of  those  efforts  was 
limited to method development for utilizing redundant re- 
sources in  a distributed system to protect the execution of 
diverse  versions  of  a  non-distributed  application  software 
against hardware  failure.  Simultaneous tolerance of  soft- 
ware and hardware  faults for distributed  computing appli- 
cations thus remains an important challenge. 
In order to accomplish simultaneous software and hard- 
ware  fault  tolerance  in  a  distributed  computing  environ- 
ment,  we propose  an  approach  that  will  enable  synergis- 
tic  coordination between  software and  hardware  fault tol- 
erance techniques.  In particular,  we devise a  scheme that 
allows the message-driven confidence-driven (MDCD) pro- 
tocol, which we have developed for mitigating the effect of 
software design faults [3], to coordinate with a time-based 
(TB) checkpointing protocol that was developed by Neves 
and Fuchs for tolerating hardware faults [4].  Our approach 
emphasizes  avoiding  potential  interference  between  soft- 
ware and hardware fault tolerance techniques and enabling 
them  to  be  mutually supportive.  Consequently, in  the re- 
sulting protocol-coordination scheme, the concurrently run- 
0-7695-1101-5/01 $10.00 0 2001 IEEE 
369 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:58:44 UTC from IEEE Xplore.  Restrictions apply. 
ning adapted MDCD and TB protocols are able to comple- 
ment each other effectively  to  realize  the goal  of  extend- 
ing  a  system’s  fault  tolerance capability.  Specifically,  the 
MDCD protocol is responsible for establishing a checkpoint 
in volatile storage, upon a message passing event that alters 
our confidence in a process state, to facilitate efficient soft- 
ware error recovery; while the duty  of  the TB protocol  is 
to save checkpoints to stable storage, based on periodically 
resynchronized timers, for tolerating faults in the hardware 
that accommodates the interacting processes. Moreover, the 
establishment of a stable-storage checkpoint is conducted in 
a manner that adapts to the up-to-date knowledge provided 
by  the  MDCD protocol  regarding  potential  process  state 
contamination,  thus  preserving  validity-concerned  global 
state consistency and recoverability  (see Section 2.1 for the 
definitions) which are crucial for software error recovery. 
By  carefully  adapting  the  algorithms,  we  are  able  to 
preserve  and  enhance the  features and  advantages  of  the 
MDCD and TB protocols in  this coordination scheme.  In 
particular, just as neither of the protocols depends on costly 
message-exchange  based  coordination  among  processes, 
the  protocol-coordination  scheme  itself  does  not  require 
or  involve  message-exchange based  coordination between 
the  participating protocols.  More  importantly, this effort 
demonstrates that  exploiting the  synergy of  differing fault 
tolerance techniques is an effective approach for enabling- 
technology integration. 
The remainder of the paper is organized as follows. Sec- 
tion  2  reviews  the  MDCD and  TB  protocols.  Section  3 
describes the MDCD algorithm modifications, followed by 
Section 4 which presents the adapted TB checkpointing pro- 
tocol  and  illustrates  how  it coordinates with  the  modified 
MDCD protocol in  a synergistic fashion.  The concluding 
remarks discuss the significance of this effort. 
2  Background 
2.1  MDCD  Error  Containment  and  Recovery 
Protocol 
The development of  the  MDCD protocol  was  initially 
motivated  by  the challenge of  guarding an  embedded sys- 
tem  against the adverse effects of design faults introduced 
by an onboard software upgrade [3]. There are a number of 
factors other than upgrading, such as complexity or test cov- 
erage, that may  lead  us  to discriminate among interacting 
software components  in  a distributed system with  respect 
to our confidence in their trustworthiness. Also, some soft- 
ware component in a distributed system may have a higher 
message-sending rate than others, which implies that an er- 
ror  of  that  component is  more  likely  to  propagate;  such 
a component thus could become  the reliability  bottleneck 
or critical component of  the  system,  and should  be given 
priority  for  fault  tolerance.  Those  factors,  coupled  with 
the MDCD protocol’s  ability to facilitate the application of 
software fault tolerance to selected interacting components, 
suggests that  the MDCD protocol  has the potential to be- 
corne  a  general-purposy low-cost  software fault tolerance 
technique  for distributed systems.  Accordingly,  we  have 
recently extended the MDCD approach by removing the ar- 
chitectural restrictions on the underlying system [SI. 
In the development of the protocol-coordination scheme 
described in  the following sections, we retain  the architec- 
tural  assumptions used  for the  original development of the 
MDCD protocol for simplicity and clarity.  That is, we as- 
sume that the underlying system consists of three comput- 
ing  nodes  and two  functionally different application soft- 
ware components, one of which  has two versions, namely, 
a  low-confidence  version  and  a  high-confidence  version. 
Indeed,  this  assumption  is  applicable  not  only  when  the 
MDCD protocol is employed for guarded software upgrad- 
ing, but also when the protocol is utilized to allow applica- 
ti0.n of “primary-routine and secondary-routine’’ based soft- 
ware fault tolerance schemes, such as DRB (distributed  re- 
covery  blocks  [ 11)  and  NSCP  (N-self-checking program- 
ming [6]), to a selected software component in a distributed 
system.  For the  former case, the  MDCD protocol lets an 
earlier, high-confidence version .escort the execution of  an 
up,graded version in which we have not yet established high 
confidence due to its  insufficient  onboard execution time. 
For  the  latter  case,  a better-performance less-reliable ver- 
sion  is  used  as  the  primary  routine  running  in  the  fore- 
ground, and a poorer-performance more-reliable version  is 
used as the secondary routine running in the background to 
enable error recovery, as suggested by [I].  Accordingly, we 
use the following notation  in  the description of the MDCD 
protocol and the protocol-coordination scheme: 
P;,, 
psdw 
1 
p2 
The  active  process  corresponding  to  the 
low-confidence  version  of  an  application 
software component. 
The shadow process corresponding  to  the 
high-confidence version  of  the  application 
software component. 
The (active) process  corresponding  to  the 
second application software component in 
which we have high confidence. 
During  guarded  operation,  Pyt actually  influences the 
external world and interacts with process P2, while the mes- 
sages of  Psdw that  convey  its computation results to Pa  or 
external systems (e.g.,  devices) are suppressed.  However, 
psdw 
receives  the  same incoming messages that the  active 
process PTt does. Thus, Pidw and P?‘  can perform the same 
computation based on identical  input data. Should an error 
of Pyt be detected, Pidw will take over PyI’s active role. We 
call the messages sent by processes to external systems and 
the messages between processes external messages and in- 
ternal messages, respectively.  In order to keep performance 
370 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:58:44 UTC from IEEE Xplore.  Restrictions apply. 
P Y‘ 
pSdw 
1 
p2 
1 Type-1 checkpoint  + Message  passing that 
triggers checkpointing 
Type-2 checkpoint 
- - + Message  passing  that does 
not trigger checkpointing 
Interval during which process state 
is potentially contaminated 
@ Acceptance test 
Figure 1 : Message-Driven Confidence-Driven Checkpoint Establishment 
overhead low, the correctness validation mechanism, accep- 
tance test (AT), is only  used  to validate external messages 
from the active processes  that are potentially  contaminated 
(see  below  for  the  definition  of potentially  contaminated 
process state). Limiting the use of AT to external messages 
plays a dual  role  in  performance  cost  reduction.  Specifi- 
cally, this strategy not only enables a process to perform AT 
less  frequently, but  also facilitates  testing  efficiency.  This 
is  because  external  messages often  correspond to  control 
commandddata that can usually be verified by simple logic 
checking or reasonableness tests, unlike internal  messages, 
which convey intermediate  computation results that are far 
more difficult to validate. 
Because the objective of the MDCD protocol  is to mit- 