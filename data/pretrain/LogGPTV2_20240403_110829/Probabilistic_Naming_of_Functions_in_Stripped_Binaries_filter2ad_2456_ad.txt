sha1_update
xrealloc
fclose
icmp_close
hci_disconnect
open_log_file
recvmsg
openpipe
nxt_send_file
sethostent
md5_hmac_update
http_server_close
usb_bulk_read
OPENSSL_CTX_free
ssh_disconnect
fclose
socket_close
SQLDisconnect
sethostname
init
usb_bulk_recv
SHA512File
Hwrite
MD5File
btdisconnected
BeginDocFile
nxt_recv_buf
SHA1File
cprng_init
unix_sock_close
from ngx_md5_update − ngx_md5_init + ngx_sha1_init is the
function name ngx_sha1_update as shown in Figure 4.
The dense representation of function names by Symbol2Vec
allows us to numerically express the similarity of function names.
We evaluate the effectiveness of our dense representation in the
standard way by comparing the correlation between the generated
representation and a manually created list of lexical relationships.
Our analogies are in the form a−b+c ≈ d. The full list of analogies
can be seen in Table 5.
We measure the distance between function name vectors in the
ordinal sense as we are unable to produce an exact continuous
measure for our manual analogies. For each of our analogies we
evaluate the preceding formula on the vector representations of
each word and then rank each word vector based on its cosine
distance to the new point with the closest vector having rank, and
hence distance, 0.
𝑟𝑠 = 1 − 6 𝑑2
𝑖
𝑛(𝑛2 − 1)
(7)
The Spearman’s rank correlation is then calculated as per Equa-
tion 7 with 𝑛 being the number of observations and 𝑑𝑖 is the ordinal
distance d is away from a − b + c when compared to all function
name vectors for analogy 𝑖.
We show that Symbol2Vec is a meaningful representation and
the distance between vectors in this space strongly correlates to
their semantic similarity. We achieve a Spearman’s rank correlation
coefficient of 0.97 between Symbol2Vec and our manually crafted
analogies. Thus proving a strong correlation between our semantic
analogies created by a human analyst and Symbol2Vec. In order for
Figure 4: t-SNE plot of the closest vectors for the SHA-1 and
MD5 hash algorithm relationship existing in Symbol2Vec.
√︂ 𝑡
a vector representation of function names. We replace the CBOW
model with our own Continuous Bag Of Functions (CBOF) that
uses the callgraph of binaries in order to predict the surrounding
context functions given a pivot function. Using the CBOF and a
context window of 1 we randomly sample a pivot function name
with an associated target function name that is either a callee or
caller of the pivot function.
For a large corpus of function names, very common functions
provide less information than rare function names. Therefore we
use a sub-sampling approach as per [34] to discard function names
based on the probability defined by the following formula:
𝑝 (𝑤𝑖) = 1 −
𝑓 (𝑤𝑖)
(6)
where 𝑓 (𝑤𝑖) is the frequency of the function name 𝑤𝑖 in our corpus
and 𝑡 is an arbitrary threshold that we took to be 10−5. This step
reduced our set of unique function names removing the likes of
malloc, free, and csu_init. We use TensorFlow [17] to create a
autoencoder using 150 hidden nodes and 800,000 input and output
nodes; one to represent each function name using one-hot encoding
after the sub-sampling stage. We then train our neural network
using Stochastic Gradient Descent (SGD) and Noise Contrastive
Estimation (NCE) with negative sampling on a server with 256
GiB RAM and a Intel(R) Xeon(R) Gold 6142 CPU for three days
to minimize the loss between predicting the pivot words from its
context. The resulting weights of the hidden layer when activated
by the input function name form the vector representation for
each function name. We display function names and their nearest
neighbors in our Symbol2Vec vector space for selected functions
in Table 4, which demonstrates how semantically similar function
names are grouped close together.
We use all function names found in or referenced by the code
section of ELF binaries for C executables in Debian which resulted
in 17,549 binaries, 5 million unique symbol names, and 1.1 million
function names after our naming pre-processing step. Analogous
to the classic Word2Vec analogy King − Man + Woman = Queen,
we are able to reveal analogical relationships between function
names. One such analogy is that between hash functions used in
the Nginx software package, where the closest vector resulting
9
ngx_md5_initngx_md5_updatengx_sha1_updatengx_sha1_initngx_md5ngx_sha1ACSAC 2020, December 7–11, 2020, Austin, USA
James Patrick-Evans, Lorenzo Cavallaro, and Johannes Kinder
its use in the community and other similar applications, we release
Symbol2Vec [26] open-source.
6 EVALUATION
We evaluate punstrip in two ways. First, we evaluate our probabilis-
tic fingerprint (§6.1); for this we require a dataset that has source
code compiled under different optimization levels from different
compilers. Second, we evaluate the combination of our probabilistic
fingerprint with punstrip’s probabilistic structural inference (§6.2)
on a large scale.
6.1 Probabilistic Fingerprint
Dataset. To evaluate our approach to inferring function names
in previously unseen binaries we constructed a dataset of programs
that have moderate code reuse between them. We built a corpus
of binaries from coreutils, moreutils, findutils, x11-utils and x11-
xserver-utils. This resulted in 149 unique binaries and 1,362,379
symbols. Our criteria for choosing these binaries were open source
software packages containing a large number of ELF executables.
All of the binaries were then compiled under all combinations of
{og,o1, o2} × {static,dynamic} for both clang and gcc resulting
in 2132 distinct binaries5. We randomly split the 149 programs
into 134 in the training set and 15 in the test set. All binaries with
debugging information included were replicated to a stripped set
of binaries before running the strip utility on them; this removed
all symbols where possible (dynamic binaries still have dynamic
symbols for linking purposes). By having two copies of the binaries,
one stripped, the other with debugging information, we are able to
obtain the ground truth for the results of our experiments. Previous
work [2, 3] has shown that function boundaries can be identified in
stripped binaries with an average F1 score of 0.95 across compiler
optimizations O0-O3. Our work focuses on the problem of inferring
function names only; therefore, we assume function boundaries to
be given and take them from the ground truth.
Throughout all of our experiments, the same program name, and
hence exactly the same source code was never in both the training
and testing set. Thus 100% accuracy may be impossible as there are
many functions only contained within the testing dataset; however,
common pieces of source code may exist between binaries from
the same package. We perform this experiment to evaluate how
our probabilistic fingerprint recognizes functions compiled into
different binaries and under different settings. By training a model
on binary names for a given configuration of compilation options
we then inferred function names for a different configuration of
compilation options in the testing set. Our results can be seen in
Table 6 for which we compare our fingerprint against leading in-
dustry tools IDA FLIRT and Radare 2 Zignatures. A comparison
against BinDiff6 proved impossible since it aims to perform differ-
ential comparisons between similar binaries rather than inferring
function names in completely new binaries. We were unable draw
a comparison against existing state-of-the-art research projects
that build searchable code fingerprints such as BinGold [1] and Ge-
nius [19] because they were not fully available. We provide a larger
5clang og is equivalent to clang o1.
6https://zydnamics.com/software/bindiff.html
evaluation of the entirety of Punstrip against the leading state-of-
the-art research tool Debin [23] in §6.2 that combines program
features and structural inference.
All of our experiments were run under Debian Sid with dual Intel
Xeon CPU E5-2640 and 128GB of RAM. On average the computation
of feature functions after training was carried out in the order of
seconds. We make our full dataset available online7.
Explanation of Results. In evaluating all schemes, we calculate
Precision (P), Recall (R) and F1 score for as per Equation 8, 9, and
10. The number of true positives 𝑇 𝑃, is given by the number of
correctly named functions8. The number of false positives 𝐹 𝑃, is
given by the number of functions that were named incorrectly. The
number of false negatives 𝐹 𝑁 , is given by the number of functions
in which we did not predict a name, but valid names existed. We
define the correctness of an inferred function name to be result of
our NLP matching scheme (§5) between the inferred function and
the ground truth.
𝑃 =
𝑇 𝑃
𝑇 𝑃 + 𝐹 𝑃
(8)
𝑅 =
𝑇 𝑃
𝑇 𝑃 + 𝐹 𝑁
(9)
𝐹1 =
2 × 𝑃 × 𝑅
𝑃 + 𝑅
(10)
All approaches performed worst in cross compiler, cross opti-
mization inference on dynamic binaries. From manual analysis,
there are large differences in both the structure and interactions
between functions and also in the number and name of functions.
For example, clang always produces the symbol c_isalnum which
is never present in binaries compiled by gcc. It’s also worth noting
that in general, the number of symbols in a binary decreased with
higher levels of optimization, with the coreutils binary who ranging
between 80–130 functions for the dynamically linked case across
optimizations 𝑜𝑔–𝑜3 for x86_64. The same program compiled un-
der clang with og produced produced 129 symbols in its .text
section whereas under gcc with og produced 106 symbols with 35
symbols that were not shared between the two binaries.
In the cases of very low recall, Zignatures’s and FLIRT’s precision
rises. We attribute this to domain knowledge of ELF binaries with
Radare2 always finding the symbol __libc_csu_init, a function
with a size of 0 which without structure prediction, our fingerprint
does not.
6.2 Probabilistic Structural Inference
Dataset. To test if we can learn abstract relationships between
arbitrary functions in the general sense it is necessary to build a
large corpus of binaries with debugging information from different
software packages. We construct this dataset from thousands of
open source software packages from the Debian repositories.
This produced 188, 253 binaries with debugging information
from 14,000 different software packages resulting in 82GB of ex-
ecutables; we make the tools used to build this comprehensive
dataset available9. Of the 188, 253 ELF binaries, 17, 549 binaries
were compiled from the C language. We limit ourselves to C binaries
7https://github.com/punstrip/cross-compile-dataset
8For structural inference, punstrip makes the assumption that libc initialization (e.g.
libc_csu_init) and deinitialization (e.g. fini) functions can be found based on static
analysis and the ELF header. This assumption was also applied when evaluating Debin.
9https://github.com/punstrip/debian-unstripped
10
Probabilistic Naming of Functions in Stripped Binaries
ACSAC 2020, December 7–11, 2020, Austin, USA
Table 6: Evaluation on the accuracy of symbol inference of different corpora and the different technologies used.
Experiment
gcc,og,dynamic -> gcc,og,dynamic
gcc,og,dynamic -> gcc,o1,dynamic
gcc,og,dynamic -> gcc,o2,dynamic
clang,o1,dynamic -> clang,o1,dynamic
clang,og,static -> clang,og,static
clang,og,static -> clang,o2,static
clang,og,static -> gcc,og,static
clang,og,static -> gcc,o2,static
Table 7: 10-fold cross validation against Debin and Punstrip.
Metric
Exact
NLP
Symbol2Vec
Debin
Punstrip
𝑃
𝑅
𝐹1
0.63 0.66 0.51
0.66 0.67 0.55
0.68 0.69 0.57
𝑃
𝑅
𝐹1
0.65 0.92 0.73
0.68 0.92 0.75
0.70 0.93 0.77
only as we expect different relationships between functions across
different languages. Using the 17, 549 C binaries, we randomly split
the list of executables into 10 equally sized groups and perform
10-fold cross validation to evaluate our approach.
Explanation of Results. In evaluating the performance of our
probabilistic graphical model we used the same metrics as in (§6.1),
however we use three different measures of correctness. The first
being an exact match between the canonicalized ground truth and
our inferred function name, the second being our NLP matching
scheme, and lastly we consider an inferred name to be correct if
its Symbol2Vec representation is within the 5 closest vectors using
the cosine distance. The reason for doing so is that the sparsity of
function names across our corpus gives many names that are used
in similar ways whilst still not evaluating as similar in our NLP
matching scheme. Secondly, giving an analyst a list of the top 5 most
likely symbol names and their corresponding probabilities allows
them to make an informed decision that may take into account
other information about the binary in question.
Table 7 displays the results of our large scale inference exper-
iment using 10-fold cross validation. From a detailed analysis of
the results our NLP and Symbol2Vec matching schemes correctly
pick up meaningful inferred function names where the exact cor-
rect name is not present in the training set. Both tools perform
worst on small dynamically linked binaries with little recognizable
relationships. Furthermore it is evident that punstrip may infer
symbol names that are structurally close on a micro-level to the
correct names however they lie in a different orientation; for ex-
ample symbol names with strong relationships between each other
are often predicted locally correct as a group but not necessarily in
the correct structural order which reduces our accuracy.
11
IDA FLIRT
𝑃
𝑅
0.38
0.14
𝐹1
0.47
0.94
0.95
0.24
0.30 < 0.01 < 0.01
0.51
0.78
0.61
0.29
0.27
0.60
0.26
0.61
0.61
0.26
0.38
0.18
0.17
0.16
0.16
𝑅
R2 Zignatures
𝑃
𝐹1
0.60
0.37
0.07
0.43
0.14
0.13
0.11
0.11
0.72
0.37
0.04
0.49
0.16
0.14
0.12
0.12
0.51
0.36
0.29
0.40
0.13
0.12
0.11