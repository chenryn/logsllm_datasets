∗] ⊕
(1)
M(cid:124)(cid:123)(cid:122)(cid:125)
,
known mask
where P1 is the first plaintext byte. The measured signal to noise
ratio (SNR) attains a high maximum value of 5.8577. Finally, we
applied the Pearson correlation to select the 50 most important
features. This is commonly considered as a simple dataset to attack
with machine learning [4]. Consequently, if we can obtain good SCA
protection even in this scenario, we believe when considering more
difficult datasets (protected with standard SCA countermeasures
but also adversarial attack-based countermeasure), the results will
be even better.
3.4 Experimental Results
For the experiment, we consider the following scenario: the cipher
implementer have trained a surrogate machine learning classifier
on a subset of available traces, which will also be available for the
attackers to train their classifiers. In this case, the surrogate learning
model does not have to be the same as the learning algorithms
which are used by the attackers. Then, when the trace is processed,
it will be first analyzed by the surrogate model and evasion attack
is then performed to mislead the attackers’ classifier and to predict
the wrong label.
For the surrogate model, we consider support vector machines
(SVM) with Radial Basis Function (RBF) kernel, whereas for actual
target classifiers, we investigate SVM with linear and RBF kernels,
logistic regression (LR), k-nearest neighbors (kNN), random forest
(RF), multilayer perceptron (MLP), and convolutional neural net-
work (CNN). Due to the lack of space, we do not give details about
those classifiers but we note they are commonly used in the SCA
community [4].
When training the classifiers, 12 000 of data is used to train the
target classifier, and 10 000 of those measurements are used for the
surrogate classifier. The label is the output value of the S-box (256
classes). Note, in this case, random guessing will have an accuracy
of 0.39%. The test data contains 2 000 traces and we conduct 10-fold
cross-validation to determine the best hyperparameters. After a
tuning phase, we select the following hyperparameters:
• SVM with RBF kernel: C = 1, γ = 1
• SVM with linear kernel: C = 2
• LR: C = 0.1, penalty = l2
• kNN: n = 10
• RF: n_trees = 1, 000
• MLP: 2 hidden layers = [50, 200], activation = ReLU
• CNN: 3 Conv. layers = [(32, 3), (64, 3), (64, 3)], 1 Dense layer
= [128], activation = ReLU
For generating the adversarial test data, the maximum perturba-
tion distance is set to 3, according to L2 distance, and every feature
and time sample can be modified. As already stated, the adversarial
model is non-targeted, i.e., the aim is to increase misclassification
irrespective of the label.
The training is first done on the surrogate model, which is then
used as a basis to derive the adversarial test data. First, SVM classi-
fier with RBF kernel is trained. The test accuracy on this classifier
PosterCCS ’19, November 11–15, 2019, London, United Kingdom2674is 22.6%. Afterward, the target classifiers are trained independently
of the surrogate model. In Table 1, we report the accuracy results
on actual test data and adversarial data for the target classifiers.
Table 1: The accuracy on actual test data and adversarial data
on target classifiers.
Classifier
SVM Linear
SVM RBF
Test Acc. (%) Adv. Acc. (%)
LR
kNN
RF
MLP
CNN
6.65
21.75
22.70
17.15
32.40
33.85
29.3
0.00
0.00
0.00
0.25
0.30
0.00
0.05
As shown in Table 1, based on the surrogate model, one could
perturb the test traces in such a way that they are misclassified by
the target classifier, even with fewer training data. This suggests
that SCA countermeasure based on the adversarial attack can be a
powerful defense against side-channel attacks.
4 CONCLUSIONS AND FUTURE WORK
In this paper, we show how adversarial attacks can be a powerful
countermeasure against profiling side-channel attacks. The results
show that the attacker using commonly considered machine learn-
ing techniques can obtain a significantly lower performance if the
adversarial-based countermeasure is applied.
As we consider this work to be a proof of a concept, there are
multiple future research directions. First, we consider here only
the non-targeted approach where we are interested in misclassi-
fication but not to a specific class. In many realistic SCA scenar-
ios, one uses the Hamming weight leakage model where not all
classes are equally represented. There, those classes that are the
least occurring contain the strongest information for the attack. It
would be interesting to use targeted adversarial countermeasure
approach to force such rare class examples to be misclassified into
the most represented classes. Next, we investigate the influence of
adversarial-based countermeasure on a simple dataset that has a
high signal-to-noise ratio and is not additionally protected. In future
work, we aim to consider more difficult cases where, hopefully, the
interaction between SCA and adversarial countermeasures would
result in an even more difficult to attack dataset.
The overall countermeasure can be envisioned as a noise gen-
erator which generates adversarial example inspired noise signals
alongside the actual side-channel activity of the target core. This
is illustrated with the example of power side-channel on a cryp-
tographic core in Figure 1(a). A noise generator is instantiated in
parallel with the sensitive cryptographic core. The noise generator
receives label information from the cryptographic core and triggers
noise generation activity. This noise generation activity should be
implemented in a way to generate adversarial examples. As shown
in Figure 1(b), the adversary sees side-channel information of the
composite system which is a total of the actual power consumption
of cryptographic core and the noise generator. As presented in the
previous section, the total activity can mislead the classification
(a)
(b)
Figure 1: (a) A general idea of the adversarial example based
countermeasure. The label information from cryptographic
core triggers noise generator to generate corresponding ad-
versarial examples. (b) The power consumption of overall
system is the power consumption of cryptographic core dis-
turbed by adversarial noise to achieve misclassification
algorithm to mitigate the side-channel attacks. Here, power con-
sumption is stated just as an example, while similar adversarial
noise can be generated for other side-channels. The actual design
for the noise generator generating label triggered adversarial ex-
amples is another challenge and will be explored in future work. In
that direction, it could be interesting to also consider the universal
adversarial perturbations [6].
ACKNOWLEDGMENT
The authors acknowledge the support from the ’National Integrated
Centre of Evaluation’ (NICE); a facility of Cyber Security Agency,
Singapore (CSA).
REFERENCES
[1] Nicholas Carlini and David Wagner. Towards Evaluating the Robustness of Neural
Networks. arXiv e-prints, page arXiv:1608.04644, Aug 2016.
[2] Ambra Demontis, Marco Melis, Maura Pintor, Matthew Jagielski, Battista Biggio,
Alina Oprea, Cristina Nita-Rotaru, and Fabio Roli. Why do adversarial attacks
transfer? explaining transferability of evasion and poisoning attacks.
In 28th
USENIX Security Symposium (USENIX Security 19), pages 321–338, Santa Clara,
CA, August 2019. USENIX Association.
[3] Mehmet Sinan Inci, Thomas Eisenbarth, and Berk Sunar. Deepcloak: Adversarial
crafting as a defensive measure to cloak processes. CoRR, abs/1808.01352, 2018.
[4] Jaehun Kim, Stjepan Picek, Annelie Heuser, Shivam Bhasin, and Alan Hanjalic.
Make some noise. unleashing the power of convolutional neural networks for
profiled side-channel analysis. IACR Transactions on Cryptographic Hardware and
Embedded Systems, 2019(3):148–179, May 2019.
[5] Paul C. Kocher, Joshua Jaffe, and Benjamin Jun. Differential power analysis. In
Proceedings of the 19th Annual International Cryptology Conference on Advances in
Cryptology, CRYPTO ’99, pages 388–397, London, UK, UK, 1999. Springer-Verlag.
[6] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal
Frossard. Universal adversarial perturbations. CoRR, abs/1610.08401, 2016.
[7] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv
e-prints, page arXiv:1312.6199, Dec 2013.
[8] TELECOM ParisTech SEN research group. DPA Contest (4th edition), 2013–2014.
http://www.DPAcontest.org/v4/.
PosterCCS ’19, November 11–15, 2019, London, United Kingdom2675