operation. Thus σ(AFi) and σ(AF (cid:48)
i ) cannot differ in more
rows, meaning Hi+1 and H(cid:48)
i+1 will also differ at most in the
rows corresponding to(cid:83)ti
l=1 N (vrl ).
(cid:89)
state change, according to Algorithm 3, is s/2. Putting the
statements together, we derive
Pr[M(A) ∈ S]
Pr[M(Auv) ∈ Suv]
uv) ∈ Suv]
Pr[M(A(cid:48)
Pr[M(A(cid:48)) ∈ S]
≤ 1 − s/2
≤ exp(ε)
Pr[M(Aij) ∈ Sij]
Pr[M(A(cid:48)
ij) ∈ Sij]
=
=
i,j
when ε ≥ ln(cid:0) 2
s − 1(cid:1), s ∈ (0, 1].
s/2
B. Proofs for Privacy Guarantees of the DP mechanisms
5) Proof of Theorem 5:
1) Proof of Theorem 2:
Proof. Since (cid:101)AV (T ) is perturbed to meet ε-edge differential
privacy and other inputs (e.g., node features) in Algorithm 2
are independent of the graph structure, the DP GCN model is
ε-edge differentially private due to the post-processing prop-
erty of differential privacy.
2) Proof of Lemma 1:
Proof. Lemma 1 extends the parallel composition property of
differential privacy [48] to edge differential privacy. The par-
allel composition property states that if M1, M2, . . . , Mm are
algorithms that access disjoint datasets D1, D2, . . . , Dm such
that each Mi satisﬁes εi-differential privacy, then the combi-
nation of their outputs satisﬁes ε-differential privacy with ε =
max(ε1, ε2, . . . , εm). Since the adjacency matrices A1, A2,
. . . , Am have non-overlapping edges, they could be viewed as
disjoint datasets of edges. Thus, the combination of Mε(A1),
Mε(A2), . . . , Mε(Am) is ε-edge differentially private.
3) Proof of Theorem 3:
Proof. Under the transductive setting, since the same per-
turbed matrix (cid:101)AV (T ) is used in both training and inference, the
inference step does not leak extra graph structure information
other than that in the DP GCN model. Therefore, the inference
step is ε-edge differentially private due to the post-processing
property of differential privacy. Under the inductive setting,
the perturbation method Mε is applied to both AV (T ) and
AV (I). Since AV (T ) and AV (I) contain non-overlapping sets
of edges, Lemma 1 guarantees ε-edge differential privacy of
the Inference step.
4) Proof of Theorem 4:
Proof. By deﬁnition, EDGERAND is ε-edge differentially pri-
vate iff. for all symmetric matrices A(cid:48)
∼ A ∈ A and all subsets
S ⊆ A, Inequality (2) holds.
We ﬁrst note that M operates on each cell Aij indepen-
dently. Therefore, the probability of perturbing the matrix A
to get a certain output is the product of the probability of
perturbing each cell in A to match the corresponding cell in
the desired output. Let e = (u, v) be the only differing edge
between A and A(cid:48). For all (i, j) (cid:54)= (u, v), Aij = A(cid:48)
ij, so the
probability of perturbing Aij and A(cid:48)
ij into the same value is the
same. For (u, v), however, getting the same outcome means
that one of Au,v and A(cid:48)
uv is changed through perturbation
while the other remains unchanged. The probability of the
Proof. In Algorithm 4, line 6 is ε1-edge differentially private
and line 7 is ε2-edge differentially private due to the differen-
tial privacy guarantee of the Laplace mechanism. Therefore,
from the composition theorem and the post-processing prop-
erty of differential privacy, we know that LAPGRAPH guaran-
tees ε-edge differential privacy.
C. Proof of Theorem 6
The work on membership privacy [39] shows that dif-
ferential privacy is a type of positive membership privacy,
which prevents the attacker from signiﬁcantly improving its
conﬁdence on a membership inference attack. Similarly, edge
differential privacy bounds an attacker’s precision in an edge
re-identiﬁcation attack. In this section, we present a formal
proof for the upper bound deﬁned in Theorem 6.
Proof. With a slight abuse of notations, we use e1 to represent
Auv = 1, e0 to represent Auv = 0, and RG to represent the
attack RGBB (u, v) where GBB is a black-box GCN. Based
on Bayes’ theorem, we have
Pr [e1 | RG = 1]
Pr [RG = 1 | e1] Pr [e1]
=
=
Pr [RG = 1 | e1] Pr [e1] + Pr [RG = 1 | e0] Pr [e0]
Pr [RG = 1 | e1] Pr [e1] + Pr [RG = 1 | e0] (1 − Pr [e1])
Pr [RG = 1 | e1] Pr [e1]
(5)
.
Without loss of generality, let G represent the set of all
GCN models. Since the attacker tries to re-identify the edges
through querying the black-box model G, we could rewrite
Pr [RG = 1 | e1] as follows:
Pr [RG = 1 | e1] =
Pr [RG = 1 | G = Gi] Pr [G = Gi | e1] .
(cid:88)
(cid:88)
Gi∈G
Gi∈G
Similarly,
Pr [RG = 1 | e0] =
Pr [RG = 1 | G = Gi] Pr [G = Gi | e0] .
Therefore, to calculate the upper bound for Eq. 5, it is sufﬁ-
cient to upper bound the following ratio for any Gi ∈ G:
Pr [G = Gi | e1] Pr [e1]
(6)
.
Pr [G = Gi | e1] Pr [e1] + Pr [G = Gi | e0] (1 − Pr [e1])
Suppose A is the set of all possible adjacency matrices. Let
A(1), A(0) ∈ A be a pair of neighboring adjacency matrices
differing by edge (u, v), and A(1)
uv = 0. Based on the
deﬁnition of differential privacy (Deﬁnition 3), for any Gi ∈ G
and A(1), A(0) ∈ A, we have
Pr[G = Gi | A = A(1)] ≤ exp(ε) Pr[G = Gi | A = A(0)].
uv = 1, A(0)
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:20:38 UTC from IEEE Xplore.  Restrictions apply. 
2020
Algorithm 2: Training and inference of DP GCN
Input: perturbation method M ∈ {EDGERAND,LAPGRAPH
}, privacy parameter ε; node set V (T ), V (I),
adjacency matrix AV (T ), AV (I), feature matrix X (T ),
X (I), and labels y(T ). The subscript (T ) stands for
training and (I) is for inference.
4
1 Procedure Perturbation(AV (T ) , M, ε):
2
3 Procedure Training((cid:101)A, V (T ), X (T ), y(T )):
(cid:101)AV (T ) ← Mε(AV (T ) )
GCN ← a trained model using (cid:101)AV (T ) , X (T ), y(T )
(cid:101)AV (I) ← (cid:101)AV (T )
(cid:101)AV (I) ← Mε(AV (I) )
10 Function Inference((cid:101)A, V (I), X (I)):
return GCN((cid:101)AV (I) , X (I), {W i})
5
6 if V (T ) = V (I) then
7
8 else
9
11
Algorithm 3: Edge Randomization (EDGERAND)
Input: a symmetric matrix A, privacy parameter s, randomization
Output: the perturbed outcome (cid:101)A
1 Reset (cid:101)A to an all-zero matrix
generator
x ← a sample drawn from Bern(1 − s)
if x = 1 then
2 for 1 ≤ i  1 for any positive privacy budget ε, we also
have
Pr[G = Gi | e1] ≤ exp(ε) Pr[G = Gi | e1].
Therefore,
Pr[G = Gi | e1]
≤ exp(ε) · min(Pr[G = Gi | e0], Pr[G = Gi | e1])
≤ exp(ε) (Pr [G = Gi | e1] Pr [e1] + Pr [G = Gi | e0] (1 − Pr [e1]))
The second inequality holds because ≤ 0 Pr[e] ≤ 1. There-
fore, we could compute the upper bound for the ratio in (6):
Pr [G = Gi | e1] Pr [e1]
Pr [G = Gi | e1] Pr [e1] + Pr [G = Gi | e0] (1 − Pr [e1])
≤ exp(ε) · Pr [e1]
Because the graph density over V (C) is k(C), by the deﬁnition
of graph density, we have Pr [e1] = k(C). Therefore,
Pr [e1 | RG = 1] ≤ exp(ε) · k(C)
D. Detailed Algorithms for DP GCN
1) Algorithm for the Training and Inference of DP GCN:
Algorithm 2 presents the perturbation, training, and inference
steps in a differentially private GCN framework. V (T ) and
AV (T ) represent the set of training nodes and the adjacency
Algorithm 4: Laplace Mechanism for Graphs
(LAPGRAPH)
Input: a symmetric matrix A, privacy parameter ε, randomization
generator
Output: the perturbed outcome (cid:101)A
1 ε1 ← 0.01ε
2 ε2 ← ε − ε1
3 T ← number of edges in A
4 T ← T + Lap(1/ε1)
5 A ← the upper triangular part of A
6 for 1 ≤ i < j ≤ n do
7
Aij ← Aij + Lap(1/ε2)
(cid:46) Distribute privacy budget
(cid:46) Get a private count
(cid:46) Laplace mechanism
(cid:46) Postprocess: Keep only the largest T cells
9 Reset (cid:101)A to an all-zero matrix
(cid:101)Aij and (cid:101)Aji are set to 1
8 S ← the indice set for the largest T cells in A
10 for (i, j) ∈ S do
11
matrix of the training graph; V (I) and AV (I) represent the set
of testing nodes and the adjacency matrix of the testing graph.
The DP guarantee holds for both transductive training (i.e.,
V (T ) = V (I)) and inductive training (i.e., V (T ) (cid:54)= V (I)).
2) Algorithm for the DP Mechanisms: Algorithm 3 presents
the algorithm for EDGERAND. We ﬁrst randomly choose the
cells to perturb and then randomly choose the target value
from {0, 1} for each cell to be perturbed.
Algorithm 4 presents the algorithm for LAPGRAPH. A small
portion of the privacy budget ε1 is used to compute the num-
ber of edges in the graph using the Laplacian mechanism,
and the remaining privacy budget ε2 = ε − ε1 is used to
apply Laplacian mechanism on the entire adjacency matrix. To
preserve the degree of the original graph, the top-elements in
the perturbed adjacency matrix are set to 1 and the remaining
elements are set to 0.
E. Additional Discussions on the LINKTELLER Attack
1) Stealthiness and Alternative Detection Strategies: Our
LINKTELLER attack queries the same set of inference nodes
V (I) for 2n times where n =
of one node slightly altered in each query. This abnormal
behavior can easily distinguish LINKTELLER from a benign
user and therefore allows the detection of the attack.
(cid:12)(cid:12)V (C)(cid:12)(cid:12), with the node features
In particular, we describe details of potential detection
strategies as follows. First, a defender can use validation data
to evaluate both the attack and benign query performance in
terms of the attack F1 score and query node classiﬁcation
accuracy under different query limits. Then the defender could
optimize a query limit Q which decreases the attack perfor-
mance while maintaining reasonable benign query accuracy.
Such a query limit would depend on the properties of different
datasets and how safety-critical the application is. Note that
in general limiting the number of queries will not affect the
performance for a single user, while it would hurt if several
users aim to query about the same set of nodes, thus the query
limit could be made for each node. In practice, the defender
can directly ﬂag the users who try to exceed the query limit Q
for a limited set of nodes as suspicious for further inspection.
2) Estimation of the Density Belief ˆk: In this part, we de-
scribe a few actionable strategies for the attacker given limited
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:20:38 UTC from IEEE Xplore.  Restrictions apply. 
2021
knowledge of the density k and/or strategies to improve the
accuracy of the density belief. For example, the attacker could
use some similar publicly available graphs (e.g., a similar
social network) or partial graphs to estimate k. Speciﬁcally,
we refer the readers to our full version [49] for three detailed
actionable strategies given different prior information.
3) Variations of Our Attack under Different Settings: We
discuss the variations of our attack under different settings,
more speciﬁcally, different attacker’s capabilities or different
assumptions on the interaction model. We consider three spe-
ciﬁc settings: a) when the attacker has additional knowledge
of some edges, b) when the attacker has only partial control
over a subset of node features, and c) when logits are not avail-
able. Due to the space limit, we refer the readers to our full
version [49] for the concrete discussions on the three settings.
4) Limitations to Overcome in Adapting LINKTELLER:
First and foremost, in order to achieve high attack effective-
ness, we need to derive exact inﬂuence calculations for differ-
ent GNN structures speciﬁcally. We believe that our inﬂuence
analysis based framework has the potential to perform well on
different GNN structures with the inﬂuence value calculation
tailored to each of them. Another potential obstacle in the
adaptation is that LINKTELLER cannot deal with randomized
models, such as the aggregation over sampled neighbors in
GraphSAGE [14]. It could be an interesting future work to
take such randomness into account for inﬂuence calculation.
5) Analysis on the Performance of LINKTELLER Com-
pared with Baselines: First of all, we note that LSA2-X [11]
relies on measuring certain distances based on either pos-
teriors (of the node classiﬁcation model) or node attributes
to predict the connections. However, node classiﬁcation and
edge inference (i.e., privacy attack goal here) are two distinct