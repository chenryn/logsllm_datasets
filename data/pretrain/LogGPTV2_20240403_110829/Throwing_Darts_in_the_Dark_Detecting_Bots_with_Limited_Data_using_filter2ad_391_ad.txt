samples and the real outlier samples are not too different.
In other words, the generated samples are more likely to be
located in the outlier region.
Lfm1
= || E
˜v∼PG1
f (˜v) − E
v∼Bo
f (v)||2
where f is the hidden layer of the discriminator.
Finally, the complete objective function for the ﬁrst gener-
ator is deﬁned as:
min
G1
LKL(PG1||O) + Lfm1
Generator 2 for Clustered Data.
In order to approximate
the real cluster distribution pG2, the generator G2 learns a
generative distribution C where generated examples ˜v are in
BvBoBcPNTrainingDataP_OutlierN_OutlierP_ClusterN_ClusterG1GeneratorsG2P’SyntheticPositive DataDDiscriminatorTABLE VIII: Training with 100% or 1% of the training data (the ﬁrst two weeks of August-18); Testing on the last two weeks of August-18.
Website A
Website B
Website C
Method
Precision
RF 100%
OCAN 100%
LSTM (ours) 100%
ODDS (ours) 100%
RF 1%
OCAN 1%
LSTM (ours) 1%
ODDS (ours) 1%
0.896
0.891
0.880
0.897
0.877
0.855
0.866
0.859
Recall
0.933
0.935
0.952
0.940
0.836
0.951
0.946
0.943
F1
0.914
0.912
0.915
0.918
0.856
0.901
0.904
0.900
FP Rate
0.045
0.047
0.055
0.047
0.048
0.066
0.062
0.063
Precision
0.831
0.659
0.888
0.900
0.883
0.680
0.601
0.729
Recall
0.594
0.882
0.877
0.914
0.202
0.736
0.355
0.845
F1
0.695
0.732
0.883
0.902
0.343
0.707
0.446
0.783
FP Rate
0.008
0.028
0.008
0.007
0.009
0.022
0.009
0.021
Precision
0.795
0.878
0.789
0.832
0.667
0.650
0.694
0.721
Recall
0.669
0.543
0.730
0.808
0.636
0.344
0.701
0.748
F1
0.727
0.671
0.759
0.815
0.651
0.450
0.697
0.734
FP Rate
0.068
0.030
0.082
0.070
0.132
0.074
0.135
0.121
Implementation.
To optimize the objective function of
generators, we adapt several approximations. To minimize
H(pG), we adopt the pull-away term [56], [57]. To estimate
pm and pb, we adopt the approach proposed by [58] which
uses a neural network classiﬁer to approximate.
VI. PERFORMANCE EVALUATION
We now evaluate the performance of ODDS. We ask the
following questions: (1) how much does ODDS help when
training with all
the labeled data? (2) How much does
ODDS help when training with limited labeled data (e.g.,
1%)? (3) Would ODDS help the classiﬁer to stay effective
over time? (4) Does ODDS help with classiﬁer re-training? (5)
How much contribution does each generator have to the overall
performance? (6) Why does ODDS work? At what condition
would ODDS offer little or no help? (7) Can ODDS further
beneﬁt from adversarial re-training?
A. Experiment Setup
We again focus on advanced bots that have bypassed the
rules. To compare with previous results, we use August 2018
data as the primary dataset for extensive comparative analysis
with other baseline methods. We will use the January 2019
and September 2019 data to evaluate the model performance
over time and the impact on model-retraining.
Hyperparameters. Our basic LSTM model (see Section IV)
has two LSTM hidden layers (both are of dimension of 8).
The batch size is 512, the training epoch is 100, and activation
function is sigmoid. Adam is used for optimization. The loss is
binary crossentropy. L2-regularization is used for both hidden
layers. We use cost sensitive learning (1:2 for malicious:
benign) to address the data imbalance problems.
For ODDS, the discriminator and the generators are feed-
forward neural networks. All of the networks contain 2 hidden
layers (100 and 50 dimensions). For generators, the dimension
of noise is 50. The output of generators is of the same
dimension as the output of the LSTM-autoencoder, which
is 130. The threshold  is set as 95th percentile of the
probability of real benign users predicted by a pre-trained
probability estimator. We set α to a small value 0.1. We use
this setting to present our main results. More experiments on
hyperparameters are in Appendix D.
Comparison Baselines. We evaluate our ODDS model
with a series of baselines, including our basic LSTM model
described in Section IV, and a non-deep learning model
Random Forest [59]. We also include an anomaly detection
method as a baseline. We select a GAN-based method called
OCAN [60] which is recently published. The main idea of
OCAN is to generate complementary malicious samples that
are different from the benign data to augment the training.
The key difference between OCAN and our method is that
OCAN does not differentiate outliers from clustered data. In
addition, as an anomaly detection method, OCAN only uses
the benign data but not the malicious samples to perform the
data synthesis. We have additional validation experiments in
Appendix E, which shows OCAN indeed performs better than
other traditional methods such as One-class SVM.
B. Training with 100% Training Data
Q1:
training with the full labeled data?
Does ODDS help to improve the training even when
We ﬁrst run an experiment with the full-training data in
August 2018 (i.e., the ﬁrst two weeks), and test the model
on the testing data (i.e., the last two weeks). Figure 5 shows
F1 score of ODDS and other baselines. The results show
that ODDS outperforms baselines in almost all cases. This
indicates that, even though the full training data is relatively
representative, data synthesis still improves the generalizability
of the trained model on the testing data. Table VIII (the upper
half), presents a more detailed break up of performance into
precision, recall, and false positive rate (i.e., the fraction of
benign users that are falsely classiﬁed as bots). We did not
present the false negative rate since it is simply 1 − Recall.
The absolute numbers of false positives and false negatives are
in Appendix F. The most obvious improvement is on website B
where ODDS improves the F1 score by 2%-20% compared to
the other supervised models. The F1 score of C is improved
by 5%-14%. For website A, the improvement is minor. Our
LSTM model is the second-best performing model. OCAN,
as a unsupervised method, performs reasonably well compared
with other supervised methods. Overall, there is a beneﬁt for
data synthesis even when there is sufﬁcient training data.
C. Training with Limited Data
Q2: How much does ODDS help when training with limited
training data?
As brieﬂy shown in Section IV-C, the performance of the
LSTM model degrades a lot when training with 1% of the data,
especially for website B. Here, we repeat the same experiment
and compare the performance of ODDS and LSTM.
TABLE IX: Characterizing different website datasets (August 2018).
WebSite
Avg. Distance Between
Benign and Bot (training)
Avg. Distance Between
Train and Test (bots)
A
B
C
0.690
0.343
0.349
0.237
0.358
0.313
Fig. 5: Training with 100% train-
ing data in August 2018.
Fig. 6: Training with x% of train-
ing data in August 2018 (B).
(a) Website A
(b) Website C
Fig. 7: Training with x% of training data in August-18 (A and C).
Figure 6 shows the average F1 score on website B, given
training data. We
different sampling rates of the August
can observe a clear beneﬁt of data synthesis. The red line
represents ODDS, which maintains a high level of detection
performance despite the limited training samples. ODDS has
an F1 score of 0.784 even when training with 1% data. This
is signiﬁcantly better than LSTM whose F1 score is 0.446
on 1% of training data. In addition to the average F1 score,
the standard deviation of the F1 score is also signiﬁcantly
reduced (from 0.305 to 0.09). In addition, we show ODDS also
outperforms OCAN where ODDS has a higher F1 score over
all the different sampling rates. As shown in the bottom half
of Table VIII, the performance gain is mostly coming from
“recall”, indicating the synthesized data is helpful to detect
bots in the unknown distribution.
Figure 7 shows the results from other two websites where
the gain of ODDS is smaller compared to that of website
B. Website C still has more than 5% gain over LSTM and
other baselines, but the gain is diminished in A. We suspect
that such differences are rooted in the different bot behavior
patterns in respective websites. To validate this hypothesis,
we run statistical analysis on the August data for the three
websites. The results are shown in Table IX. First, we compute
the average Euclidean distance between the bot and benign
data points in the August training set (averaged across all bot-
benign pairs). A larger average distance indicates that bots and
benign users are further apart in the feature space. The result
shows that A clearly has a larger distance than that of B and
C. This conﬁrms that bots in A are already highly different
from benign users, and thus it is easier to capture behavioral
differences using just a small sample of the data. We also
calculate the average distance between the bots in the training
set and the bots in the testing set. A higher distance means
that the bots in testing data have behaved more differently
(a) Website A
(b) Website B
Fig. 8: The model is trained once using 1% August-18 training
dataset. It is tested on August-18 testing dataset (last two weeks),
and January-19 and September-19 datasets.
from those in the training data (and thus are harder to detect).
We ﬁnd A has the lowest distance, suggesting bot behaviors
remain relatively consistent. B has the highest distance, which
requires the detection model to generalize well in order to
capture the new bot behaviors.
D. Generalizability in the Long Term
Q3: Would ODDS help the classiﬁer stay effective for a long
period of time?
Next, we examine the generalizability of our model in the
longer term. More speciﬁcally, we train our model using only
1% of the training dataset of August 2018 (the ﬁrst
two
weeks), and then test the model directly on the last two weeks
of August 2018, and the full months of January 2019 and
September 2019. The F1 score of each testing experiment is
shown in Figure 8. Recall that Website C does not have the
data from January 2019 or September 2019, and thus we could
only analyze A and B. As expected, the model performance
decays, but in a different way between A and B. For A, both
ODDS and LSTM are still effective in January 2019 (F1 scores
are above 0.89), but become highly inaccurate in September
2019. This suggests that the bots in A have a drastic change
of behaviors in September 2019. For website B, the model
performance is gradually degrading over time. This level of
model decay is expected given that training time and the last
testing time are more than one year apart. Still, we show
that ODDS remains more accurate than LSTM, conﬁrming
the beneﬁt of data synthesis.
Q4: Does ODDS help with classiﬁer re-training?
A common method to deal with model decay is re-training.
Here, we assume the defender can retrain the model with the
ﬁrst 1% of the data in the respective month. We use the ﬁrst 1%