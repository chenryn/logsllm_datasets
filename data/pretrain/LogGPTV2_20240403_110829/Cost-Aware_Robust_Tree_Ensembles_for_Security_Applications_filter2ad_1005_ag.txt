side. According to Table 1 and our analysis of the features
in Table 7, we assign the constraint variables a, b, g and µ to
negligible, low, medium, and high cost.
We specify four families of cost models, where each one
has a corresponding adaptive attack cost to target the trained
classiﬁers. We assign four distinct constraint variables in the
ﬁrst family (M1 to M5). For all the other families (M6 to
M9, M10 to M15, and M16 to M19), we assign only three
distinct constraint variables, representing three categories of
feature manipulation cost, by repeating the same value for two
out of four categories. For example, the second cost family
(M6 to M9) merges medium and high cost categories into
one using the same value for g and µ. Within the same cost
family, the relative scale of the constraint variables between
the categories are the same, and the adaptive attack cost for
that group is the inverse proportion of the constraint variables,
as we have discussed in Section 3.3. For example, in the
second group (M6 to M9), values for the low cost perturbation
range (b) are twice the amount of the medium cost ones (g),
and the values for the negligible cost (a) are three times of
the medium cost ones (g). Therefore, in the adaptive attack
objective, we assign wN = 1,wL = 2,wM = 3 to capture that
the cost of perturbing one, two, and three units of negligible,
low, and medium cost features are equivalent. For each cost
family, we vary the size of the bounding box to represent
different attacker budget during training, resulting in 19 total
settings of the constraint.
Using the cost-driven constraint, we train robust gradient
boosted decision trees. We compare our training algorithm
against regular training and Chen’s method [11]. We use 30
trees, maximum depth 8, to train one model using regular
training. For Chen’s training algorithm, we specify three dif-
ferent L• norm cost models (e = 0.03, 0.05, and 0.1) to obtain
three models C1, C2, and C3 in Table 8. For Chen’s method
and our own algorithm, we use 150 trees, maximum depth 24.
USENIX Association
30th USENIX Security Symposium    2305
4.3.4 Results
Adaptive attack cost: Compared to regular training, our best
model increases the cost-aware robustness by 10.6⇥. From
each cost family, our best models with the strongest cost-
aware robustness are M2, M6, M13, and M19. Compared to
the natural model obtained from regular training, our robust
models increase the adaptive attack cost to evade them by
10.6⇥ (M2, Cost1), 10⇥ (M6, Cost2), 8.3⇥ (M13, Cost3), and
6.4⇥ (M19, Cost4), respectively. Thus, the highest cost-aware
robustness increase is obtained by M2 model over the total
feature manipulation cost Cost1.
Advantages of cost-driven constraint: Our robust training
method using cost-driven constraints can achieve stronger
cost-aware robustness, higher accuracy, and lower false posi-
tive rate than L•-norm cost model from Chen’s algorithm [11].
In Table 8, results from C1, C2, and C3 models demonstrate
that if we use L•-norm cost model (L•  e), the performance
of the trained model quickly degrades as e gets larger. In
particular, when e = 0.03, the C1 model trained by Chen’s al-
gorithm has decreased the accuracy to 96.59% and increased
the false positive rate to 5.49% compared to regular training.
With larger e values, C2 and C3 models have even worse
performance. C3 has only 91.89% accuracy and a very high
11.96% false positive rate. In comparison, if we specify attack
cost-driven constraint in our training process, we can train
cost-aware robust models with better performance. For exam-
ple, our model M6 can achieve stronger robustness against
cost-aware attackers with all four adaptive attack cost than
C1. At the same time, M6 has higher accuracy and lower false
positive rate than C1.
Robustness and accuracy tradeoffs: Training a larger
bounding box generally decreases accuracy and increases
false positive rate within the same cost family; however, the
obtained robustness against MILP attacks vary across differ-
ent cost families. Whenever we specify a new cost family
with different constraint variable proportions and number of
categories, we need to perform constraint parameter tuning to
ﬁnd the model that best balances accuracy and robustness.
• In the ﬁrst cost family, as the bounding box size increases,
the adaptive evasion cost Cost1 against the models in-
creases, and then decreases. M2 has the largest evasion
cost.
• In the second cost family where we merge medium cost
and high cost, the adaptive evasion cost Cost2 decreases as
the bounding box size increases.
• In the third cost family where we merge negligible cost and
low cost, the adaptive evasion cost Cost3 has high values
for M10 and M13, and varies for other models.
• In the last cost family where we merge low cost and
medium cost, the adaptive evasion cost Cost4 increases
as the bounding box size increases.
Other mathematical distances: Although our current imple-
mentation does not support training L1 and L2 attack cost
models directly, training our proposed cost models can obtain
robustness against L1 and L2 attacks. Comparing to the C1
model trained by Chen’s algorithm, we can obtain stronger ro-
bustness against L1 and L2 based MILP attacks while achiev-
ing better model performance. For example, our models M6
and M19 have larger L1/L2 evasion distance than C1, and they
have lower false positive rate and higher/similar accuracy.
4.3.5 Discussion
Robustness and accuracy tradeoffs. Obtaining robustness
of a classiﬁer naturally comes with the tradeoff of decreased
accuracy and increased false positive rate. We have experi-
mented with 19 different cost models to demonstrate such
tradeoffs in Table 8. In general, we need to perform constraint
hyperparamters tuning to ﬁnd the model that best balances
accuracy, false positive rate, and robustness. In comparison
with L• based cost models (C1, C2 and C3), we can achieve
relatively higher accuracy and lower false positive rate while
obtaining stronger robustness against cost-aware attackers
(e.g., M6 vs C1). This is because L• based cost model al-
lows attackers to perturb all features with equally large range,
making it harder to achieve such robustness and easier to
decrease the model performance. However, our cost-driven
training technique can target the trained ranges according to
the semantics of the features.
Scalability. For applications where thousands of features are
used to build a classiﬁer, we can categorize the features by
semantics, and specify the cost-driven constraint as a function
for different categories. Alternatively, we can also use L•-
norm as default perturbation for features, and specify cost-
driven constraint for selected features.
Generalization. Our cost-aware training technique can gen-
eralize to any decision tree and tree ensemble training pro-
cess, for both classiﬁcation and regression tasks, e.g., Ad-
aBoost [23] and Gradient Boosting Machine [24]. Since we
apply the cost-aware constraint in the node splitting process,
when constructing the classiﬁcation and regression trees, we
can calculate the maximal error of the split construction ac-
cording to the allowable perturbations of the training data,
and adjust the score for the split. This can be integrated in
many different tree ensemble training algorithms. We leave
investigation of integrating our technique to other datasets as
future work.
5 Conclusion
In this paper, we have designed, implemented, and evalu-
ated a cost-aware robust training method to train tree ensem-
bles for security. We have proposed a cost modeling method
to capture the domain knowledge about feature manipula-
tion cost, and a robust training algorithm to integrate such
knowledge. We have evaluated over four benchmark datasets
2306    30th USENIX Security Symposium
USENIX Association
against the regular training method and the state-of-the-art ro-
bust training algorithm. Our results show that compared to the
state-of-the-art robust training algorithm, our model is 1.25⇥
more robust in gradient boosted decision trees, and 1.7⇥ more
robust in random forest models, against the strongest white-
box attack based on Lp norm. Using our method, we have
trained cost-aware robust Twitter spam detection models to
compare different cost-driven constraints. Moreover, one of
our best robust models can increase the robustness by 10.6⇥
against the adaptive attacker.
Acknowledgements
We thank Huan Zhang and the anonymous reviewers
for their constructive and valuable feedback. This work is
supported in part by NSF grants CNS-18-42456, CNS-18-
01426, CNS-16-17670, CNS-16-18771, CCF-16-19123, CCF-
18-22965, CNS-19-46068; ONR grant N00014-17-1-2010;
an ARL Young Investigator (YIP) award; a NSF CAREER
award; a Google Faculty Fellowship; a Capital One Research
Grant; a J.P. Morgan Faculty Award; and Institute of Infor-
mation & communications Technology Planning & Evalu-
ation (IITP) grant funded by the Korea government(MSIT)
(No.2020-0-00153). Any opinions, ﬁndings, conclusions, or
recommendations expressed herein are those of the authors,
and do not necessarily reﬂect those of the US Government,
ONR, ARL, NSF, Google, Capital One, J.P. Morgan, or the
Korea government.
References
[1] Breast Cancer Wisconsin
(Original)
Data
Set.
https://archive.ics.uci.edu/ml/datasets/breast+
cancer+wisconsin+(original).
[2] Cod-RNA Data Set.
https://www.csie.ntu.edu.tw/
~cjlin/libsvmtools/datasets/binary.html#cod-rna.
[3] Ijcnn1 Data Set. https://www.csie.ntu.edu.tw/~cjlin/
libsvmtools/datasets/binary.html#ijcnn1.
[4] scikit-learn: Machine Learning in Python. https://scikit-
learn.org/.
[5] The MNIST Database of Handwritten Digits.
yann.lecun.com/exdb/mnist/.
http://
[6] M. Andriushchenko and M. Hein. Provably robust boosted
decision stumps and trees against adversarial attacks.
In
Advances in Neural Information Processing Systems, pages
12997–13008, 2019.
[7] L. Breiman. Bagging predictors. Machine learning, 24(2):123–
140, 1996.
[8] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classiﬁca-
tion and regression trees. Wadsworth Int Group, 37(15):237–
251, 1984.
[9] S. Calzavara, C. Lucchese, and G. Tolomei. Adversarial train-
ing of gradient-boosted decision trees.
In Proceedings of
the 28th ACM International Conference on Information and
Knowledge Management, pages 2429–2432, 2019.
[10] S. Calzavara, C. Lucchese, G. Tolomei, S. A. Abebe, and S. Or-
lando. Treant: training evasion-aware decision trees. Data
Mining and Knowledge Discovery, pages 1–31, 2020.
[11] H. Chen, H. Zhang, D. Boning, and C.-J. Hsieh. Robust de-
In International
cision trees against adversarial examples.
Conference on Machine Learning (ICML), 2019.
[12] H. Chen, H. Zhang, S. Si, Y. Li, D. Boing, and C.-J. Hsieh.
Robustness veriﬁcation of tree-based models. In Advances in
Neural Information Processing Systems, 2019.
[13] T. Chen and C. Guestrin. Xgboost: A scalable tree boosting
system. In Proceedings of the 22nd acm sigkdd international
conference on knowledge discovery and data mining, pages
785–794. ACM, 2016.
[14] Y. Chen, Y. Nadji, A. Kountouras, F. Monrose, R. Perdisci,
M. Antonakakis, and N. Vasiloglou. Practical attacks against
graph-based clustering.
In Proceedings of the 2017 ACM
SIGSAC Conference on Computer and Communications Secu-
rity, pages 1125–1142. ACM, 2017.
[15] M. Cheng, T. Le, P.-Y. Chen, J. Yi, H. Zhang, and C.-J. Hsieh.
Query-efﬁcient hard-label black-box attack: An optimization-
based approach.
In International Conference on Learning
Representations (ICLR), 2019.
[16] A. Cidon, L. Gavish, I. Bleier, N. Korshun, M. Schweighauser,
and A. Tsitkin. High precision detection of business email
compromise. In 28th USENIX Security Symposium (USENIX
Security 19), pages 1291–1307, 2019.
[17] J. M. Cohen, E. Rosenfeld, and J. Z. Kolter. Certiﬁed adver-
sarial robustness via randomized smoothing. arXiv preprint
arXiv:1902.02918, 2019.
[18] T. Dreossi, S. Jha, and S. A. Seshia. Semantic adversarial deep
learning. arXiv preprint arXiv:1804.07045, 2018.
[19] K. Dvijotham, S. Gowal, R. Stanforth, R. Arandjelovic,
B. O’Donoghue, J. Uesato, and P. Kohli. Training veriﬁed learn-
ers with learned veriﬁers. arXiv preprint arXiv:1805.10265,
2018.
[20] I. Fette, N. Sadeh, and A. Tomasic. Learning to detect phishing
emails. In Proceedings of the 16th international conference on
World Wide Web, pages 649–656. ACM, 2007.
[21] Y. Freund. Boosting a weak learning algorithm by majority.
Information and computation, 121(2):256–285, 1995.
[22] Y. Freund and R. E. Schapire. A decision-theoretic generaliza-
tion of on-line learning and an application to boosting. Journal
of computer and system sciences, 55(1):119–139, 1997.
[23] J. Friedman, T. Hastie, R. Tibshirani, et al. Additive logistic
regression: a statistical view of boosting (with discussion and
a rejoinder by the authors). The annals of statistics, 28(2):337–
407, 2000.
[24] J. H. Friedman. Greedy function approximation: a gradient
boosting machine. Annals of statistics, pages 1189–1232, 2001.
[25] S. Gowal, K. Dvijotham, R. Stanforth, R. Bunel, C. Qin, J. Ue-
sato, T. Mann, and P. Kohli. On the effectiveness of interval
bound propagation for training veriﬁably robust models. arXiv
preprint arXiv:1810.12715, 2018.
USENIX Association
30th USENIX Security Symposium    2307
[26] G. Ho, A. Cidon, L. Gavish, M. Schweighauser, V. Paxson,
S. Savage, G. M. Voelker, and D. Wagner. Detecting and char-
acterizing lateral phishing at scale. In 28th USENIX Security
Symposium (USENIX Security 19), pages 1273–1290, 2019.
[27] I. Incer, M. Theodorides, S. Afroz, and D. Wagner. Adversari-
ally robust malware detection using monotonic classiﬁcation.
In Proceedings of the Fourth ACM International Workshop on
Security and Privacy Analytics, pages 54–63. ACM, 2018.
[28] U. Iqbal, P. Snyder, S. Zhu, B. Livshits, Z. Qian, and Z. Shaﬁq.
Adgraph: A graph-based approach to ad and tracker blocking.
In Proc. of IEEE Symposium on Security and Privacy, 2020.
[29] A. Kantchelian, J. Tygar, and A. Joseph. Evasion and hardening
of tree ensemble classiﬁers. In International Conference on
Machine Learning, pages 2387–2396, 2016.
[30] A. Kharraz, Z. Ma, P. Murley, C. Lever, J. Mason, A. Miller,
N. Borisov, M. Antonakakis, and M. Bailey. Outguard: Detect-
ing in-browser covert cryptocurrency mining in the wild. In
The World Wide Web Conference, pages 840–852, 2019.
[31] A. Kharraz, W. Robertson, and E. Kirda. Surveylance: automat-
ically detecting online survey scams. In 2018 IEEE Symposium
on Security and Privacy (SP), pages 70–86. IEEE, 2018.
[32] M. Konte, R. Perdisci, and N. Feamster. Aswatch: An as
reputation system to expose bulletproof hosting ases. ACM
SIGCOMM Computer Communication Review, 45(4):625–638,
2015.
[33] B. Kulynych, J. Hayes, N. Samarin, and C. Troncoso. Evad-
ing classiﬁers in discrete domains with provable optimality
guarantees. arXiv preprint arXiv:1810.10939, 2018.
[34] B. J. Kwon, J. Mondal, J. Jang, L. Bilge, and T. Dumitra¸s.
The dropper effect: Insights into malware distribution with
downloader graph analytics. In Proceedings of the 22nd ACM
SIGSAC Conference on Computer and Communications Secu-
rity, pages 1118–1129. ACM, 2015.
[35] H. Kwon, M. B. Baig, and L. Akoglu. A domain-agnostic
approach to spam-url detection via redirects. In Paciﬁc-Asia
Conference on Knowledge Discovery and Data Mining, pages
220–232. Springer, 2017.
[36] M. Lecuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana.
Certiﬁed robustness to adversarial examples with differential
privacy. In 2019 IEEE Symposium on Security and Privacy
(SP), pages 656–672. IEEE, 2019.
[37] K. Levchenko, A. Pitsillidis, N. Chachra, B. Enright, M. Féle-
gyházi, C. Grier, T. Halvorson, C. Kanich, C. Kreibich, H. Liu,
et al. Click trajectories: End-to-end analysis of the spam value
chain. In 2011 ieee symposium on security and privacy, pages
431–446. IEEE, 2011.
[38] B. Li, C. Chen, W. Wang, and L. Carin. Second-order adver-
sarial attack and certiﬁable robustness. 2018.
[39] D. Lowd and C. Meek. Adversarial Learning. In Proceedings
of the eleventh ACM SIGKDD international conference on
Knowledge discovery in data mining, pages 641–647. ACM,
2005.
[40] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu.
Towards deep learning models resistant to adversarial attacks.
International Conference on Learning Representations (ICLR),
2018.
[41] M. Mirman, T. Gehr, and M. Vechev. Differentiable abstract
interpretation for provably robust neural networks. In Interna-
tional Conference on Machine Learning (ICML), pages 3575–
3583, 2018.
[42] T. Nelms, R. Perdisci, M. Antonakakis, and M. Ahamad. To-
wards measuring and mitigating social engineering software
download attacks.
In 25th USENIX Security Symposium
(USENIX Security 16), pages 773–789, 2016.
[43] M. Norouzi, M. Collins, M. A. Johnson, D. J. Fleet, and
P. Kohli. Efﬁcient non-greedy optimization of decision trees.
In Advances in neural information processing systems, pages
1729–1737, 2015.
[44] N. Papernot, P. McDaniel, and I. Goodfellow. Transferability
in machine learning: from phenomena to black-box attacks
using adversarial samples. arXiv preprint arXiv:1605.07277,
2016.
[45] F. Pierazzi, F. Pendlebury, J. Cortellazzi, and L. Cavallaro. In-
triguing properties of adversarial ml attacks in the problem
space. In 2020 IEEE Symposium on Security and Privacy (SP),
pages 1332–1349. IEEE, 2020.
[46] J. R. Quinlan. Induction of decision trees. Machine learning,
1(1):81–106, 1986.
[47] J. R. Quinlan. C 4.5: Programs for machine learning. The
Morgan Kaufmann Series in Machine Learning, 1993.
[48] E. Quiring, A. Maier, and K. Rieck. Misleading authorship
attribution of source code using adversarial learning. In 28th
USENIX Security Symposium (USENIX Security 19), pages
479–496, 2019.
[49] M. Z. Raﬁque, T. Van Goethem, W. Joosen, C. Huygens, and
N. Nikiforakis. It’s free for a reason: Exploring the ecosystem
of free live streaming services. In Proceedings of the 23rd
Network and Distributed System Security Symposium (NDSS
2016), pages 1–15. Internet Society, 2016.
[50] R. E. Schapire. The strength of weak learnability. Machine
learning, 5(2):197–227, 1990.
[51] A. Sinha, H. Namkoong, and J. Duchi. Certifying some dis-
tributional robustness with principled adversarial training. In-
ternational Conference on Learning Representations (ICLR),
2018.
[52] S. Wang, Y. Chen, A. Abdou, and S. Jana. Mixtrain: Scalable
training of formally robust neural networks. arXiv preprint
arXiv:1811.02625, 2018.
[53] Y. Wang, H. Zhang, H. Chen, D. Boning, and C.-J. Hsieh. On lp-
norm robustness of ensemble stumps and trees. In International
Conference on Machine Learning (ICML), 2020.
[54] E. Wong and Z. Kolter. Provable defenses against adversarial
examples via the convex outer adversarial polytope. In Inter-
national Conference on Machine Learning, pages 5283–5292,
2018.
[55] E. Wong, F. Schmidt, J. H. Metzen, and J. Z. Kolter. Scaling
provable adversarial defenses. Advances in Neural Information
Processing Systems (NIPS), 2018.
[56] X. Zhang and D. Evans. Cost-Sensitive Robustness against
Adversarial Examples. International Conference on Learning
Representations (ICLR), 2019.
2308    30th USENIX Security Symposium
USENIX Association