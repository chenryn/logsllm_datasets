### 4.3.4 Results

**Adaptive Attack Cost:**
Compared to regular training, our best model increases cost-aware robustness by 10.6×. From each cost family, the models with the strongest cost-aware robustness are M2, M6, M13, and M19. Compared to the natural model obtained from regular training, our robust models increase the adaptive attack cost to evade them by 10.6× (M2, Cost1), 10× (M6, Cost2), 8.3× (M13, Cost3), and 6.4× (M19, Cost4), respectively. The highest cost-aware robustness increase is achieved by the M2 model over the total feature manipulation cost Cost1.

**Advantages of Cost-Driven Constraints:**
Our robust training method using cost-driven constraints achieves stronger cost-aware robustness, higher accuracy, and a lower false positive rate compared to the L•-norm cost model from Chen's algorithm [11]. Table 8 shows that as the value of e in the L•-norm cost model (L•  e) increases, the performance of the trained model degrades. For instance, when e = 0.03, the C1 model trained by Chen’s algorithm has an accuracy of 96.59% and a false positive rate of 5.49%, which is worse than regular training. With larger e values, the C2 and C3 models perform even worse, with C3 having an accuracy of 91.89% and a false positive rate of 11.96%. In contrast, specifying attack cost-driven constraints in our training process allows us to train cost-aware robust models with better performance. For example, our model M6 not only achieves stronger robustness against cost-aware attackers but also has higher accuracy and a lower false positive rate than C1.

**Robustness and Accuracy Trade-offs:**
Training with a larger bounding box generally decreases accuracy and increases the false positive rate within the same cost family. However, the robustness against MILP attacks varies across different cost families. When specifying a new cost family with different constraint variable proportions and number of categories, we need to perform constraint parameter tuning to find the model that best balances accuracy and robustness.

- **First Cost Family:** As the bounding box size increases, the adaptive evasion cost (Cost1) against the models initially increases and then decreases. M2 has the largest evasion cost.
- **Second Cost Family (Merging Medium and High Costs):** The adaptive evasion cost (Cost2) decreases as the bounding box size increases.
- **Third Cost Family (Merging Negligible and Low Costs):** The adaptive evasion cost (Cost3) has high values for M10 and M13, and varies for other models.
- **Fourth Cost Family (Merging Low and Medium Costs):** The adaptive evasion cost (Cost4) increases as the bounding box size increases.

**Other Mathematical Distances:**
Although our current implementation does not support training L1 and L2 attack cost models directly, training our proposed cost models can achieve robustness against L1 and L2 attacks. Compared to the C1 model trained by Chen’s algorithm, our models M6 and M19 have larger L1/L2 evasion distances and lower false positive rates, while maintaining similar or higher accuracy.

### 4.3.5 Discussion

**Robustness and Accuracy Trade-offs:**
Obtaining robustness in a classifier naturally comes at the cost of decreased accuracy and increased false positive rates. We have experimented with 19 different cost models to demonstrate these trade-offs (Table 8). Generally, we need to perform constraint hyperparameter tuning to find the model that best balances accuracy, false positive rate, and robustness. Compared to L•-based cost models (C1, C2, and C3), our cost-driven training technique can achieve relatively higher accuracy and lower false positive rates while obtaining stronger robustness against cost-aware attackers (e.g., M6 vs. C1). This is because L•-based cost models allow attackers to perturb all features with equally large ranges, making it harder to achieve such robustness and easier to decrease model performance. Our cost-driven training technique, however, targets the trained ranges according to the semantics of the features.

**Scalability:**
For applications involving thousands of features, we can categorize the features by semantics and specify the cost-driven constraint as a function for different categories. Alternatively, we can use L•-norm as the default perturbation for features and specify cost-driven constraints for selected features.

**Generalization:**
Our cost-aware training technique can be generalized to any decision tree and tree ensemble training process, for both classification and regression tasks, such as AdaBoost [23] and Gradient Boosting Machine [24]. By applying the cost-aware constraint in the node splitting process, we can calculate the maximal error of the split construction according to the allowable perturbations of the training data and adjust the score for the split. This can be integrated into many different tree ensemble training algorithms. Future work will involve integrating our technique with other datasets.

### 5. Conclusion

In this paper, we have designed, implemented, and evaluated a cost-aware robust training method to train tree ensembles for security. We have proposed a cost modeling method to capture domain knowledge about feature manipulation costs and a robust training algorithm to integrate this knowledge. Our evaluations on four benchmark datasets show that, compared to the state-of-the-art robust training algorithm, our model is 1.25× more robust in gradient boosted decision trees and 1.7× more robust in random forest models against the strongest white-box attack based on Lp norm. Using our method, we have trained cost-aware robust Twitter spam detection models to compare different cost-driven constraints. One of our best robust models can increase robustness by 10.6× against the adaptive attacker.

### Acknowledgements

We thank Huan Zhang and the anonymous reviewers for their constructive and valuable feedback. This work is supported in part by NSF grants CNS-18-42456, CNS-18-01426, CNS-16-17670, CNS-16-18771, CCF-16-19123, CCF-18-22965, CNS-19-46068; ONR grant N00014-17-1-2010; an ARL Young Investigator (YIP) award; a NSF CAREER award; a Google Faculty Fellowship; a Capital One Research Grant; a J.P. Morgan Faculty Award; and Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2020-0-00153). Any opinions, findings, conclusions, or recommendations expressed herein are those of the authors and do not necessarily reflect those of the US Government, ONR, ARL, NSF, Google, Capital One, J.P. Morgan, or the Korea government.

### References

[References listed as provided in the original text]

---

This revised version aims to enhance clarity, coherence, and professionalism, ensuring that the content is well-structured and easy to follow.