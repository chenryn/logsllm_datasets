Their results show no correlation between a node’s average
temperature and node outages, within the temperature range
that the data comprises.
We have formalized the work in [3] by using regression
analysis to model
the occurrence of node outages due to
hardware failures as a function of a node’s average tempera-
ture. We used two commonly used regression models, Poisson
regression and negative binomial regression. In agreement
with [3], we ﬁnd that average temperature is not correlated
signiﬁcantly with the occurrence of hardware failures. When
repeating our analysis for CPU and DRAM failures separately,
we also ﬁnd that average temperature is not signiﬁcant to either
type of failure.
B. How do temperature excursions affect failures?
Previous work [3] has only considered the effect of av-
erage temperature, but not looked at the effect of temporary
excursions to very high temperatures. Therefore, rather than
looking at average temperature, we repeat the same regression
analysis as above for the maximum temperature observed for
a node and the variance in temperature across all temperature
recordings for a node. We ﬁnd that the maximum temperature
and temperature variance are insigniﬁcant to the occurrences
of hardware failures in general and CPU and memory failures
in particular.
Temperature recordings are only available for one system
and since they consist of periodic samples, they might miss
brief periods of very high temperatures. For a broader study
of the effects of brief periods of high temperature we look
at the impact that a fan failure or a failure in the chiller
system has on the nodes. Fan and chiller failures will lead to
temporarily increased temperatures at a node, and depending
on whether it’s a complete or partial failure can lead to
extreme temperatures inside a node, making a node shutdown
necessary.
Figure 13 (left) shows the impact of fan failures and chiller
failures on hardware failures. The graph shows the probability
that a node will experience a hardware failure within a day,
week and month following a fan or a chiller failure, compared
to the probability of a hardware failure in an average day, week
and month. We observe clearly increased hardware failure rates
following fan and chiller failures for all timespans. Fan failures
have a stronger effect for all timespans, with a factor of 40X
increase in hardware failure rates on the day following a fan
failure (compared to a random day). Chiller failures had a
weaker effect across the different timespans, with factors of
6-9X increase in hardware failure rates.
We also ask what type of hardware failures are likely to
follow fan and chiller failures. Figure 13 (right) shows for each
of the hardware components with corresponding records in the
data the probability of failure within a month after a fan or a
chiller failure, compared to a failure of that component in a
random month. We ﬁnd that all hardware components, except
for CPUs, show an increase in the failure rate following a
fan failure. We ﬁnd that for memory, node boards, and power
supplies the order of magnitude of the increase is similar to the
one observed after power problems, with factors of 10-20X.
In addition, we observe signiﬁcant increases in failure rates
for two types of boards, MSC boards and midplanes, which
we did not observe in the case of power problems. One of
the largest increases in failure rates, a factor of 120X, occurs
for fans, which is maybe not surprising given that we have
observed previously that most failure types have the strongest
correlation with events of the same type. Chillers failures seem
to only affect two components: memory DIMMs and node
boards, with 5.3X and 10.8X increases in their probabilities,
respectively.
Overall, our analysis shows that hardware components are
well able to tolerate higher average temperatures within the
ranges that are typically observed in a datacenter. The harmful
effects of temperature mostly stem from short periods of
extremely high temperatures, for example due to the failure
of a fan in the system.
IX. EXTERNAL FACTORS: COSMIC RADIATION
It is known that high rates of cosmic radiation can lead to
soft errors due to bit ﬂips in DRAM or on system buses. If the
built-in error correcting codes (ECC) are not strong enough to
correct the corrupted bits, those errors will lead to a machine
crash or shutdown. Cosmic rays and their effect on system
reliability are a major concern, and, for example in the case of
DRAM errors, most of the existing work on DRAM reliability
focuses on the effects of cosmic radiation.
Since the LANL data spans a very long time period (nearly
a decade), it covers almost an entire solar cycle (typically 11
years long), including several solar ﬂares. Records of high-
energy neutron counts that are produced by cosmic rays in
the atmosphere are collected at many neutron monitor (NM)
stations around the world. We use data of 1-minute resolution
neutron counts collected at a NM station in Climax, Colorado
(geographically close to Los Alamos National Lab) [11].
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:51:27 UTC from IEEE Xplore.  Restrictions apply. 
CPU failure
TABLE II
POISSON REGRESSION COEFFICIENTS
0
10
y
t
i
l
i
b
a
b
o
r
P
y
h
l
t
n
o
M
−1
10
−2
10
DRAM failure
System 2
System 18
Sytsem 19
System 20
0.08
0.06
0.04
0.02
0
y
t
i
l
i
b
a
b
o
r
P
y
h
l
t
n
o
M
10
−3
3400
3600
3800
4000
Monthly neutron counts
4200
4400
4600
−0.02
3500
System 2
System 18
Sytsem 19
System 20
4000
Monthly neutron counts
4500
Fig. 14.
monthly neutron counts
Probability of CPU and memory failures as a function of average
TABLE I
SUMMARY OF REGRESSION VARIABLES
Category
Description
Failures
This is the response variable; the total occur-
rences of node outages in a node’s lifetime.
Input Variables
The average ambient temperature of a node
The maximum temperature reported by a node
The variance of all temperatures reported by
a node
The number of severe temperature warning
messages reported by a node (i.e. when its
ambient temperature exceeds 40C)
The number of jobs that were assigned to the
node in the observation period
The average utilization of a node during the
observation period
Position in Rack: the position of a node inside
the physical rack (1=most bottom, 5=top)
Variable
fails count
(response
variable)
avg temp
max temp
temp var
Temperature
Temperature
Temperature
num hightemp
Temperature
num jobs
Usage
util
PIR
Usage
Layout
We use this data to analyze whether periods of increased
cosmic rays are correlated with a higher rate of hardware
errors, in particular failures related to DRAM and the CPU.
We begin by studying whether the likelihood of a node
outage due to DRAM failure changes with neutron ﬂux levels.
Figure 14 (left) shows the monthly probability of a DRAM
failure as a function of the monthly average neutron counts-
per-minute, for LANL Systems 2, 18, 19 and 20. We focus
our analysis on the LANL systems that span the longest
lifetimes, or consist of the largest numbers of nodes, across
all systems. We ﬁnd that months with higher neutron rates
are not associated with higher rates of DRAM failures. These
results might be unexpected, since cosmic rays are known to
increase soft error rates in DRAM. One possible explanation
is that while increased rates of cosmic rays might lead to a
higher number of corrupted bits, the types of corruption caused
by those events might usually be correctable with the built-
in ECC. This explanation agrees with recent ﬁndings in [7],
which provide evidence that most node outages that are due to
errors in DRAM are likely caused by hard errors, i.e. problems
with the underlying hardware, rather than random events, such
as cosmic rays.
Cosmic ray-induced neutrons can also cause CPU faults,
possibly leading to a machine crash or shutdown. We repeat
our correlation analysis using data on node outages that were
attributed to CPU failures, rather than outages due to DRAM
problems (see Figure 14 (right)). We observe that in three
systems (2, 18 and 19), CPU failures were slightly more likely
to occur in months with relatively higher neutron rates.
(Intercept)
avg temp
max temp
temp var
num hightemp
num jobs
util
PIR
Estimate
2.0232
0.0546
-0.0705
0.0253
0.0210
0.0004
-0.0268
-0.0262
Std. Error
0.8288
0.0337
0.0339
0.0333
0.0698
0.0001
0.0050
0.0358
z value
2.44
1.62
-2.08
0.76
0.30
7.17
-5.34
-0.73
Pr(> |z| )
0.0146
0.1046
0.0373
0.4479
0.7639
0.0000
0.0000
0.4654
TABLE III
NB REGRESSION COEFFICIENTS
(Intercept)
avg temp
max temp
temp var
num hightemp
num jobs
util
PIR
Estimate
1.5478
0.0499
-0.0510
0.0252
0.0021
0.0004
-0.0248
-0.0345
Std. Error
1.1930
0.0462
0.0475
0.0449
0.0937
0.0001
0.0073
0.0488
z value
1.30
1.08
-1.07
0.56
0.02
3.86
-3.42
-0.71
Pr(> |z| )
0.1945
0.2802
0.2828
0.5744
0.9820
0.0001
0.0006
0.4794
X. PUTTING IT ALL TOGETHER
In the previous sections we have looked at a number of
factors and their impact on node outages in HPC systems in
isolation. Our goal in this section is to put all the different
pieces together. Rather than studying the individual effect of
these factors separately, we now ask the question of what the
collective effect of these different factors, combined, is on
long-term HPC node reliability. The only LANL system that
allows us to explore this question is system 20 where we have
data on node outages, node usage, physical layout and ambient
temperature.
We use regression analysis to model occurrences of node
outages in system 20 as a function of node usage, physical
location and temperature. More precisely, we use the total
number of outages a node experiences during the data collec-
tion period (due to any type of failure) as our response variable,
which we try to express by the set of predictor (explanatory)
variables summarized in Table I. We use two commonly used
regression models, Poisson regression and negative binomial
regression.
The results of applying Poisson regression and negative
binomial regression are shown in Tables II and III, respec-
tively. The two right most columns show the test statistic
and the p-value, respectively, that the null hypothesis that
each predictor’s coefﬁcient is zero given that the rest of the
predictors are in the model. Interestingly, we observe similar
results for both models: The predictors num jobs (i.e. the
number of jobs assigned to a node during the observation
period) and util (i.e. the node’s average utilization) are each
statistically signiﬁcant in both models; with 99% conﬁdence,
we can reject our null hypothesis and conclude that each one
of them is statistically different from zero given that the rest
of the coefﬁcients are in the model. Since we know from
Section V that node 0 in this system exhibited a strong linear
correlation between usage variables and number of failures,
we reran our regression models after removing node 0 from
the data and found that utilization remains signiﬁcant to the
model, although the signiﬁcance level drops slightly.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:51:27 UTC from IEEE Xplore.  Restrictions apply. 
In addition to usage variables, we observe that for the
Poisson model max temp is statistically signiﬁcant
to the
frequency of node outages (recall that maximum temperature
was found insigniﬁcant to hardware failures in particular, in
Section VIII). However, when rerunning the model with only
the signiﬁcant predictors, the signiﬁcance level of max temp
in the Poisson model drops.
We ﬁnd these results to be a strong indicator that a node’s
usage and utilization levels have a stronger impact on a node’s
failure rates than other factors, such as its ambient temperature
or physical location inside a rack.
XI. LESSONS LEARNED
Below we highlight some of the key ﬁndings of our work
and derive some lessons we learned from our analysis.
• In agreement with prior work, we observe strong cor-
relations between failures in HPC systems. During the day
following a failure, a node is 5–20X more likely to experience
an additional failure, when compared to a random day. Similar,
albeit weaker trends exist across the nodes in the same rack:
a node’s failure probability is increased by a factor of 3X
during the day following another node failure in the same
rack. The trends were signiﬁcantly weaker when looking at
other nodes in the same system (not in the same rack). We
found that software and network failures in one node increase
the probability of subsequent failures of other nodes in the
same system by factors of 1–3X.
• Interestingly, we observe that some types of failures
increase the likelihood of follow-up failures more than others.
In particular, environmental failures (such as power outages)
and network failures have a very strong effect on subsequent
failures: 30–50% of nodes experience at least one failure in the
week following a network or environmental failure, compared
to only 2% in an average week. These observations are critical
for creating effective failure prediction models, as they imply
that such models should not only account for correlations
between failures in time and space, but also consider the root-
causes of failures.
• The strong chance of follow-up failures after environmen-
tal failures, which in our data are mostly due to power outages,
motivated us to study the effects of power problems more
broadly. We considered four different events: power outages,
power spikes, UPS failures, and failures of a node’s power sup-
ply units, and found that they all lead to signiﬁcantly increased
hardware failure rates, as well as unscheduled maintenance
events.
• Our observations on increased failure rates in memory
DIMMs and node boards following power spikes, UPS failures
and power supply problems suggest that after such events one
might want to thoroughly inspect these hardware components
for problems. Suspected fans should also be properly inspected
in the case of a power supply failure since they were 40X more
likely to fail in the following month, than in an average month.
In general, we ﬁnd that a bad or failing power supply can lead
to many auto-correlated node outages and therefore should be
quickly ﬁxed or replaced.
• Power outages have another interesting effect: signiﬁ-
cantly increased rates of software issues. A large fraction of
the software failures following within a month of a power
outage were either related to the distributed storage system
or the ﬁle system. This observation might hold evidence that
stronger mechanisms are required to protect storage and ﬁle
system consistency in the face of power outages.
• The large cost of datacenter cooling motivated us to study
the effect of temperature on node reliability. We do not observe
a signiﬁcant correlation between the average temperature at a
node and its likelihood of failure. However, when studying
the effect of node outages due to fan or chiller failures, which
likely cause a brief period of very high temperatures inside
a node, we do observe a strong increase in the subsequent
rate of failures. Hardware components most strongly affected
were MSC boards and midplanes (>100X increase in failure
probabilities), but memory DIMMs, power supplies and node
boards also experienced increased failure probabilities (>10X
increase). This shows that hardware components are well able
to tolerate higher average temperatures within the ranges that
are typically observed in a datacenter. The harmful effects of
temperature mostly stem from short periods of extremely high
temperatures, for example due to a fan failure.
• Another environmental factor we studied is cosmic ray-
induced neutron ﬂux, which can lead to increased soft error
rates. Interestingly, we observe no effect on failures due
to DRAM errors, which might
indicate that built-in error
correcting codes are generally sufﬁcient to mask bit ﬂips in
DRAM due to soft errors (and that those DRAM errors that
lead to node outages are more likely due to hard errors).
On the other hand, CPU failure rates, which did not show a
strong correlation with other types of failures or environmental
factors, such as power or temperature, are positively correlated
with cosmic rays-induced neutron ﬂux.
• When studying the effect of a node’s usage on its failure
rate, we ﬁnd that nodes with higher utilization and a higher
number of jobs assigned to them experience higher failure
rates. Moreover, when studying the number of failures experi-
enced by different users of the system, we ﬁnd that some users
experience a signiﬁcantly higher failure rate per processor-day
of usage of the system. Since we exclude problems caused by
the users’ application software, this skew is not due to users’
varying abilities to write stable code. Instead, we conclude that
the way a node is exercised affects its failure behaviour.
• We observe that some nodes fail signiﬁcantly more
frequently than others, even in systems where all nodes are
identical in terms of their hardware. When we looked more
closely at the most failure prone nodes in LANL’s systems,
we found that they encountered higher-than-average rates of all
types of failures, but the increase was strongest for software,
network and environment failures. One of the possible reasons
that we investigated is the location of a node within the
machine room, but we ﬁnd no indication that certain areas
in the machine room are more failure prone than others.
Instead, we ﬁnd that the failure prone nodes were typically
used differently from the rest of nodes.
• When performing a joint regression analysis, where we
model node reliability as a function of different aspects of
physical location, temperature and usage, we found that usage
related variables were the most signiﬁcant.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:51:27 UTC from IEEE Xplore.  Restrictions apply. 
ACKNOWLEDGMENTS
We would like to thank LANL for making their data
publicly available and for answering our questions about the
data. We would also like to thank the US National Oceanic and
Atmospheric Administration for making the neutron count data
from Colarado NM station publicly available. Special thanks
goes to Sotirios Damouras for his insightful answers to our
statistics questions. Finally, we thank our anonymous reviewers
for their feedback and comments on the paper. This work has
been funded by an NSERC discovery grant.
REFERENCES
[1] Operational Data to Support and Enable Computer Science Research,
Los Alamos National Laboratory. http://institute.lanl.gov/data/fdata/.
[2] X. Castillo and D. Siewiorek. Workload, performance, and reliability
of digital computing systems. In Proc. of International Symposium on
Fault-Tolerant Computing, 1981.
[3] N. El-Sayed, I. A. Stefanovici, G. Amvrosiadis, A. A. Hwang, and
B. Schroeder. Temperature management in data centers: why some
(might) like it hot. In Proc. of SIGMETRICS 2012.
[4] D. Ford, F. Labelle, F. I. Popovici, M. Stokely, V.-A. Truong, L. Barroso,
C. Grimes, and S. Quinlan. Availability in globally distributed storage
systems. In Proc. of OSDI’10, 2010.
[5] S. Fu and C.-Z. Xu. Exploring event correlation for failure prediction
in coalitions of clusters. In Proc, of SC’07, 2007.
[6] E. Heien, D. Kondo, A. Gainaru, D. LaPine, B. Kramer, and F. Cappello.
Modeling and tolerating heterogeneous failures in large parallel systems.
In Proc, of SC’11, 2011.
[7] A. A. Hwang, I. A. Stefanovici, and B. Schroeder. Cosmic rays
don’t strike twice: understanding the nature of DRAM errors and the
implications for system design. In Proc. of ASPLOS’12, 2012.
[8] R. K. Iyer and D. J. Rossetti. Effect of system workload on operating
IEEE Trans. Softw. Eng.,
system reliability: A study on ibm 3081.
11(12), Dec. 1985.
[9] R. K. Iyer, D. J. Rossetti, and M. C. Hsueh. Measurement and modeling
of computer reliability as affected by system activity. ACM Trans.
Comput. Syst., 4(3), Aug. 1986.
[10] Y. Liang, Y. Zhang, M. Jette, A. Sivasubramaniam, and R. Sahoo.
BlueGene/L failure analysis and prediction models. In Proc. of DSN’06,
2006.
[11] National Oceanic and Atmospheric Administration. Cosmic ray neutron
monitor data. http://www.ngdc.noaa.gov/stp/solar/cosmic.html.
[12] B. Schroeder and G. Gibson. A large-scale study of failures in high-
performance computing systems. In Proc. of DSN’06.
[13] T. Thanakornworakij, R. Nassar, C. Leangsuksun, and M. Paun. The
effect of correlated failure on the reliability of HPC systems. In Proc.
of Parallel and Distributed Processing with Applications Workshops
(ISPAW), 2011.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:51:27 UTC from IEEE Xplore.  Restrictions apply.