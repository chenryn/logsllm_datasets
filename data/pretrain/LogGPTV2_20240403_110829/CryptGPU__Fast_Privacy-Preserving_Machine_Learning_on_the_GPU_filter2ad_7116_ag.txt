[59] B. D. Rouhani, M. S. Riazi, and F. Koushanfar, “DeepSecure: Scalable
provably-secure deep learning,” in Annual Design Automation Conference,
pp. 1–6, 2018.
[60] M. Jagielski, N. Carlini, D. Berthelot, A. Kurakin, and N. Papernot, “High-
ﬁdelity extraction of neural network models,” CoRR, vol. abs/1909.01838,
2019.
[61] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press,
[62] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, vol. 86,
no. 11, pp. 2278–2324, 1998.
APPENDIX A
BUILDING BLOCKS FOR PRIVATE TRAINING
To support private training, we need to augment our existing
toolkit with several additional protocols. Here, we consider
a standard backpropagation setting with a softmax/cross-
entropy loss function optimized using (minibatch) stochastic
gradient descent (SGD) [61]. As with private inference, we
decompose the backpropagation algorithm into a sequence of
elementary operations and build our private training protocol by
sequentially composing protocols for the elementary operations.
In this work, we consider classiﬁcation tasks with d target
classes. Each iteration of SGD takes an input x ∈ Rm and a
one-hot encoding of the target vector y ∈ {0, 1}d (i.e., yi = 1
if x belongs to class i and yi = 0 otherwise) and computes
the cross-entropy loss:5
where ˜z ← softmax(z), z ← Eval(M, x), and M is the current
model. The gradient of (cid:96)CE for the output layer z is then
∇z(cid:96)CE = softmax(z) − y.
We can use the private inference protocol from Section III-B
to compute(cid:74)z(cid:75)n from(cid:74)x(cid:75)n and(cid:74)M(cid:75)n. To compute(cid:74)∇z(cid:96)CE(cid:75)n,
we need a protocol to compute softmax on secret-shared values.
For the ReLU layers, the gradient computation reduces to
evaluating the derivative of the ReLU function. The gradients
for the linear/convolution layers can themselves are linear
functions of the gradients from the preceding layer, and thus,
can be handled using the protocols from Section III-B. In the
following, we describe our protocols for evaluating the softmax
and the derivative of the ReLU function on secret-shared values.
Note that backpropagation does not require computing the value
of the loss function (Eq. (A.1)), so we do not need a protocol
for computing logarithms on secret-shared values.
Softmax. For a vector x ∈ Rd, the softmax function is
i∈[d] exi. To avoid numeric
imprecision from evaluating the exponential function on very
large or very small inputs, a standard technique is to evaluate
the softmax on the “normalized” vector (x − maxi xi) [61].
A simple calculation shows that softmax(x − maxi xi) =
softmax(x). This has the advantage that all inputs to the
deﬁned as softmaxi(x) := exi/(cid:80)
5Technically, in minibatch SGD, each iteration takes a batch of N inputs and
the loss function is the average of the loss function for all N inputs in the
batch. For ease of exposition, we describe the setup for a single input, but
everything generalizes naturally to the minibatch setting.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:25 UTC from IEEE Xplore.  Restrictions apply. 
141034
exponential function are at most 0, and the denominator in the
softmax evaluation is contained in the interval [1, d]. In the
following, we describe protocols for evaluating the exponential
function, division, and computing the maximum over a vector
of secret-shared values. Together, this yields a protocol for
computing a softmax on secret-shared values.
Exponentiation. We approximate the exponential function ex
needed to compute softmax with the limit characterization fm:
(cid:16)
(cid:17)m
fm(x) :=
1 +
x
m
.
(A.2)
Using a Taylor expansion for the function ln(1 + x) and
assuming that |x|  0. This again corresponds to
computing the most signiﬁcant bit of the ﬁxed-point encoding of
x, which we implement using the protocol from Section III-B.
(cid:48)
APPENDIX B
DATASETS AND MODELS
In this section, we describe the datasets and machine learning
models we use in our evaluation.
Deep learning datasets. We evaluate CRYPTGPU on the
following standard datasets for object recognition:
• MNIST [11]. MNIST is a dataset for handwritten digit
recognition. The training set has 60,000 images and the test
set has 10,000 images. Each digit is a grayscale (i.e., single-
channel) 28 × 28 image. Due to its relatively small size, it
is widely used as a benchmark in many privacy-preserving
ML systems [1, 2, 5, 6].
• CIFAR-10 [12]. CIFAR-10 is a dataset with 60,000 32× 32
RGB images split evenly across 10 classes.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:25 UTC from IEEE Xplore.  Restrictions apply. 
151035
• Tiny ImageNet [15]. Tiny ImageNet is a modiﬁed subset
of the ImageNet dataset. It contains 100,000 64 × 64 RGB
training images and 10,000 testing images split across 200
classes. Compared to CIFAR-10, Tiny ImageNet is much
more challenging: each image is 4× larger and there are
20× more classes.
• ImageNet [13]. ImageNet is a large-scale visual recognition
dataset with more than 1,000,000 training images. It is
the standard benchmark for evaluating the classiﬁcation
performance of computer vision models. ImageNet has 1000
classes, and each example is a center-cropped 224 × 224
RGB image. The only prior system for privacy-preserving
machine learning that demonstrates performance at the scale
of ImageNet is CRYPTFLOW [5].
Deep learning models. For our experimental evaluation,
we measure the cost of our private training and private
inference protocols on several representative CNN architectures
developed for object recognition. Each of these networks can
be represented as a composition of a collection of standard
layers: convolution, pooling, activation, batch normalization,
softmax, and fully-connected layers.
• LeNet [62]. LeNet was proposed by LeCun et al. for
handwritten digit recognition. It is a shallow network with
2 convolutional layers, 2 average pooling layers, and 2 fully
connected layers. The network uses the hyperbolic tangent
(tanh) as its activation function.
• AlexNet [16]. AlexNet was the winner of 2012 ImageNet
Large Scale Visual Recognition Challenge (ILSVRC-2012)
competition. It has 5 convolutional layers, 3 max pooling
layers, and 2 fully connected layers for a total of 61 million
parameters. AlexNet uses ReLU as its activation function.
• VGG-16 [17]. VGG-16 is the runner-up of the ILSVRC-
2014 competition. It uses 16 layers consisting of convolution,
ReLU, max pooling, and fully-connected layers. VGG-16
has a total of 138 million parameters.
• ResNet [14]. ResNet
is the winner of ILSVRC-2015
competition. It introduces skip-connections that addresses
the vanishing gradient problem when training deep neu-
ral network models. ResNet consists of convolution, max
pooling, average pooling, batch normalization, and fully
connected layers. Since their inception, the ResNet family of
models have enjoyed wide adoption in the computer vision
community. We evaluate the performance of ResNet-50,
ResNet-101, and ResNet-152 on ImageNet. These networks
respectively have 23, 44, and 60 million parameters and 50,
101, and 152 layers.
Architecture adjustments. We use the standard architecture of
each of these networks, except with the following modiﬁcations:
• AlexNet and VGG-16 on small datasets. Since AlexNet
and VGG-16 were designed for ImageNet, they are not
directly compatible with smaller images (i.e., those from
CIFAR-10 or Tiny ImageNet). Thus, when using AlexNet
or VGG-16 with smaller inputs, we have to modify the
network architecture. For AlexNet, we drop the ﬁnal max
pooling layer, and adjust the number of neurons of the
fully connected layers to 256-256-10 and 1024-1024-200
for CIFAR-10 and Tiny ImageNet, respectively. For VGG-
16, we adjust the number of neurons in the fully connected
layer to 256-256-10 and 512-512-200 for CIFAR-10 and
TinyImageNet respectively.6 When evaluating AlexNet on
ImageNet, we use the original architecture [16]. In the case
of VGG-16, we add a 2x2 average pooling layer to reduce
the input dimension of the ﬁrst fully connected layer from
18432 to 4608; this is due to memory limitations on the
GPU. When we compare our system to the FALCON system
on these models and datasets, we make the same adaptations.
• Activation functions. All networks we consider except
LeNet use the ReLU function as the activation function.
In contrast, LeNet uses the hyperbolic tangent function tanh
as the underlying activation function. Since CRYPTGPU
does not support evaluating the tanh function and modern
networks primarily use ReLU as their activation function,
we replace tanh with ReLU in our experiments with LeNet.
• Average pooling. Pooling is a standard way to down-sample
the outputs of the convolutional layers in a CNN. Speciﬁcally,
a pooling layer accumulates the output of the convolutional
layers by replacing each (small) window of the feature map
(from the convolutional layer) with the average of the values
(i.e., average pooling) or the max of the values (i.e., max
pooling). Earlier networks such as AlexNet and VGG-16 used
max pooling throughout, while more recent deep networks
such as the ResNets primarily use average pooling (with a
single max pooling layer at the beginning). While the choice
of pooling function does not make a signiﬁcant difference in
the computational costs of plaintext training, this is not the
case in private training. The difference is due to the fact that
average pooling is a linear operation while max pooling is
a highly non-linear operation. To reduce the computational
overhead of our system, we replace all the max pooling
layers in the above networks with average pooling. This
reduces the complexity at the cryptographic level and allows
us to take better advantage of GPU parallelism.
We show in Section IV-B that in existing systems, the
pooling layer is not the bottleneck, and the performance
improvements of our protocol relative to past works is not
due to our substitution of average pooling in place of max
pooling. We additionally show in Appendix C, we show that
using average pooling in place of max pooling does not
signiﬁcantly affect the accuracy of the models we consider.
ACCURACY OF PRIVACY-PRESERVING PROTOCOLS
APPENDIX C
Several of the underlying protocols in CRYPTGPU are not
exact and can introduce a small amount of error: using ﬁxed-
point encodings to approximate ﬂoating-point arithmetic, the
share-truncation protocol from ABY3, and the approximation
to the softmax function. While we have chosen our parameters
6Previous systems like FALCON [6] make similar adjustments when evaluating
AlexNet and VGG-16 on smaller datasets.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:25 UTC from IEEE Xplore.  Restrictions apply. 
161036
(e.g., the ﬁxed-point precision) to reduce the likelihood of
errors, we validate our parameter choice with an empirical
analysis. In the following, we will often measure the difference
between an output zpriv computed using CRYPTGPU with the
output zplain of a plaintext version of the same computation
(using 64-bit ﬂoating-point values). We deﬁne the absolute
error between zpriv and zplain as |zpriv − zplain| and the relative
error to be |zpriv − zplain|/zplain.
Fixed point precision. As discussed at the beginning of Sec-
tion IV, CRYPTGPU emulates ﬂoating-point computations by
encoding values using a ﬁxed-point representation using t = 20
bits of fractional precision. The ﬁxed-point computations over
the integers are embedded into operations on secret-shared
values over the ring Zn. The modulus n must be large enough
to support multiplication (and more generally, convolution and
matrix multiplication) of plaintext values without triggering
a modular reduction. In CRYPTGPU, n = 264, so shares are
represented by 64-bit integers.
Previous privacy-preserving protocols like FALCON [6] and