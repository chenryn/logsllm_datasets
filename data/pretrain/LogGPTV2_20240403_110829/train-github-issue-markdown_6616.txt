 **Easy:**
  * in test_common, check that the ValueError raise has a useful error message. (see sparse test for an example)
  * put as many of the "specific" tests in test_clustering, test_transformers, ... into test_non_meta_estimators.
**Not so easy:**
  * calling `fit` forgets the previous model if any
  * ~~check how classifiers handle only one class being present~~
  * ~~test how models handle non-float input (does uint8 cause overflows?)~~
**Things done**
We should factorize common tests in a new file `test_common.py` (or maybe
`test_input.py`?). Things to check:
  * ~~can pickle the object~~
  * ~~raise an exception when data contains nans~~
  * ~~raise an exception for invalid input (e.g.,`np.matrix` or `sp.csr_matrix` if dense only implementation)~~
  * ~~raise an exception if`n_features` is not the same in `fit` and `predict` or `transform`~~
  * ~~`__repr__` and `clone` work~~
  * ~~check that we can pickle and unpickle estimators.~~
  * ~~check that all classifiers have a`classes_` attribute (needs some fixes)~~
* * *
Edit by @amueller!
Edit by @GaelVaroquaux on Aug 13th 2014 to reflect progress in the codebase.