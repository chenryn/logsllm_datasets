Organizations
Training
April – June 2018
52 Exploratory
Detected Known Attacks
Detected New Attacks
Missed Attacks (FN)
Detection Rate
Total Emails
False Positives (FP)
False Positive Rate
Precision
34
28
8
88.6%
25,670,264
136
0.00053%
31.3%
Testing
July – October 2018
52 Exploratory
+ 40 Test
47
49
14
87.3%
87,413,431
316
0.00036%
23.3%
Table 1: Evaluation results of our detector. ‘Detected Known At-
tacks’ shows the number of incidents that our detector identiﬁed, and
were also reported by an employee at an organization. ‘Detected New
Attacks’ shows the number of incidents that our detector identiﬁed,
but were not reported by anyone. ‘Missed Attacks (FN)’ shows all
incidents either reported by a user or found by any of our detection
strategies, but our detector marked it as benign (false negative). Of
the 22 incidents our detector misses, 12 are attachment-based attacks,
a threat model which our detector explicitly does not target but which
we include in our FN and Detection Rate results for completeness.
our methodology evaluates our detector with fresh data from
a “future” time period and introduces 40 new organizations,
neither of which our detector saw during training time; this
also reﬂects how a detector operates in practice.
Alert Metric (Incidents): We have several choices for mod-
eling our detector’s alert generation process (i.e., how we
count distinct attacks). For example, we could evaluate our
detector’s performance in terms of how many unique emails
it correctly labels. Or, we could measure our detector’s per-
formance in terms of how many distinct employee accounts it
marks as compromised (modeling a detector that generates
one alert per account and suppresses the rest). Ultimately, we
select a notion commonly used in practice, that of an incident,
which corresponds to a unique (subject, sender email address)
pair. At this granularity, our detector’s alert generation model
produces a single alert per unique (subject, sender) pair. This
metric avoids biased evaluation numbers that overemphasize
compromise incidents that generate many identical emails
during a single attack. For example, if there are two incidents,
one which generates one hundred emails to one recipient each,
and another which generates one email to 100 recipients, a
detector’s performance on the hundred-email incident will
dominate the result if we count attacks at the email level.
In total, our training dataset contains 40 lateral phishing
incidents from our user-reported ground truth sources, and our
test dataset contains 61 user-reported incidents. Our detector
ﬁnds an additional 77 unreported incidents (row 2 of Table 1).
5.2 Detection Results
Table 1 summarizes the performance metrics for our detector.
We use the term Detection Rate to refer to the percentage of
lateral phishing incidents that our detector ﬁnds, divided by all
known attack incidents in our dataset (i.e., any user-reported
incident and any incident found by any detection technique
we tried). For completeness, we include the 12 attachment-
based incidents in our False Negative and Detection Rate
computations, which our detector obviously misses since we
designed it to catch URL-based lateral phishing. Additionally,
we also include, as false negatives, 2 training incidents that our
less successful detectors identiﬁed [17]; these two alternative
strategies did not ﬁnd any new attacks in the test dataset.
Thus, the Detection Rate reﬂects a best-effort assessment
that potentially overestimates the true positive rate of our
detector, since we have an imperfect ground truth that cannot
account for narrowly targeted attacks that go unreported by
users. Precision equals the percent of attack alerts (incidents)
produced by our detector divided by the total number of alerts
our detector generated (attacks plus false positives).
Training and Tuning: On the training dataset, our detector
correctly identiﬁed 62 out of 70 lateral phishing incidents
(88.6%), while generating a total of 62 false positives (on
25.7 million employee-sent emails).
Our PySpark Random Forest classiﬁer exposes a built-in
estimate of each feature’s relative importance [40], where each
feature receives a score between 0.0–1.0 and the sum of all
the scores adds up to 1.0. Based on these feature weights, our
model places the most emphasis on the global URL reputation
feature, giving it a weight of 0.42, and the email’s ‘number of
recipients’ feature (0.34). In contrast, our model essentially
ignores our local URL reputation, assigning it a score of 0.01,
likely because most globally rare domains tend to also be
locally rare. Of the remaining features, the recipient likelihood
feature has a weight of 0.17 and the ‘phishy’ keyword feature
has a weight of 0.06.
Test Dataset: Our detector correctly identiﬁed 96 lateral
phishing incidents out of the 110 test incidents (87.3%) across
our ground truth dataset. Additionally, our detector discov-
ered 49 incidents that, according to our ground truth, were not
reported by a user as phishing. With respect to its cost, our de-
tector generated 312 total false positives across the entire test
dataset (a false positive rate of less than 0.00035%, assuming
that emails not identiﬁed as an attack by our ground truth are
benign). Across our test dataset, 82 out of the 92 organiza-
tions accumulated 10 or fewer false positives across the entire
four month window, with 44 organizations encountering zero
false positives across this timespan. In contrast, only three
organizations had more than 40 total false positives across
all four months (encountering 44, 66, and 83 false positives,
respectively). Our detector achieves similar results if we eval-
uate on just the data from our 40 withheld test organizations,
with a Detection Rate of 91.0%, a precision of 23.1%, and a
false positive rate of 0.00038%.
Bias and Evasion: We base our evaluation numbers on the
best ground truth we have: a combination of all user-reported
USENIX Association
28th USENIX Security Symposium    1279
lateral phishing incidents (including some attacks outside our
threat model), and all incidents discovered by any detection
technique we tried (which includes two approaches orthogo-
nal to our detector’s strategy). This ground truth suffers from
a bias towards phishing emails that contact many potential
victims, and attacks that users can more easily recognize. Ad-
ditionally, since our detector focuses on URL-based exploits,
our dataset of attacks likely underestimates the prevalence of
non-URL-based phishing attacks, which come solely from
user-reported instances in our dataset. As a result, our work
does not capture the full space of lateral phishing attacks,
such as ones where the attacker targets a narrow, select set of
victims with stealthily deceptive content. Rather, given that
our detector identiﬁes many known and unreported attacks,
while generating only a few false positives per month, we pro-
vide a starting point for practical detection that future work
can extend. Moreover, even if our detector does not capture
every possible attack, the fact that the attacks in our dataset
span dozens of different organizations, across a multi-month
timeframe, allows us to illuminate a class of understudied
attacks that many enterprises currently face.
Aside from obtaining more comprehensive ground truth,
more work is needed to explore defenses against potential
evasion attacks. Attackers could attempt to evade our detector
by targeting different features we draw upon, such as the com-
position or number of recipients they target. Against many of
these evasion attacks, future work could leverage additional
features and data, such as the actions a user takes within an
email account (e.g., reconnaissance actions, such as unusual
searches, that indicate an attacker mining the account for
targeted recipients to attack) or information from the user’s
account log-on (e.g., the detector proposed by Ho et al. used
an account’s login IP address [18] to detect lateral phishing).
At the same time, future work should study which evasion
attacks remain economically feasible for attackers to conduct.
For example, an attacker could choose to only target a small
number of users in the hopes of evading our detector; but
even if this evasion succeeded, the conversion rate of fooling
a recipient might be so low that the attack ultimately fails
to compromise an economically viable number of victims.
Indeed, as we explore in the following section (§ 6), the at-
tackers captured in our dataset already engage in a range of
different behaviors, including a few forms of sophisticated,
manual effort to increase the success of their attacks.
6 Characterizing Lateral Phishing
In this section, we conduct an analysis of real-world lateral
phishing using all known attacks across our entire dataset
(both training and test). During the seven month timespan, a
total of 33 organizations experienced lateral phishing attacks,
with the majority of these compromised organizations experi-
encing multiple incidents. Examining the thematic message
content and recipient targeting strategies of the attacks, our
Scale and Success
# distinct phishing emails
# incidents
# ATOs
# organizations w/ 1+ incident
# phishing recipients
% successful ATOs
# employee recip (average) for compromise
1,902
180
154
33
101,276
11%
542
Table 2: Summary of the scale and success of the lateral phishing
attacks in our dataset (§ 6.1).
analysis suggests that most lateral phishers in our dataset do
not actively mine a hijacked account’s emails to craft person-
alized spearphishing attacks. Rather, these attackers operate
in an opportunistic fashion and rely on commonplace phish-
ing content. This ﬁnding suggests that the space of enterprise
phishing has expanded beyond its historical association with
sophisticated APTs and nation-state adversaries.
At the same time, these attacks nonetheless succeed, and a
signiﬁcant fraction of attackers do exhibit some signs of so-
phistication and attention to detail. As an estimate of the suc-
cess of lateral phishing attacks, at least 11% of our dataset’s at-
tackers successfully compromise at least one other employee
account. In terms of more reﬁned tactics, 31% of lateral phish-
ers invest some manual effort in evading detection or increas-
ing their attack’s success rate. Additionally, over 80% of the
attacks in our dataset occur during the normal working hours
of the hijacked account. Taken together, our results suggest
that lateral phishing attacks pose a prevalent enterprise threat
that still has room to grow in sophistication.
In addition to exploring attacks at the incident granularity
(as done in § 5), this section also explores attacks at the gran-
ularity of a lateral phisher (hijacked account) when studying
different attacker behaviors. As described in Section 2, in-
dustry practitioners often refer to such hijacked accounts as
ATOs, and throughout this section, we use the terms hijacked
account, lateral phisher, and ATO synonymously.
6.1 Scale and Success of Lateral Phishing
Scale: Our dataset contains 1,902 distinct lateral phishing
emails sent by 154 hijacked accounts.4 A total of 33 organi-
zations in our dataset experience at least one lateral phishing
incident: 23 of these organizations came from sampling the
set of enterprises with known lateral phishing incidents (§ 3),
while the remaining 10 came from the 69 organizations we
sampled from the general population. Assuming our random
sample reﬂects the broader population of enterprises, over
14% of organizations experience at least one lateral phish-
ing incident within a 7 month timespan. Furthermore, based
4Distinct emails are deﬁned by having a fully unique tuple of (sender,
subject, timestamp, and recipients).
1280    28th USENIX Security Symposium
USENIX Association
(b) PB
and PA
their phishing URLs’ paths
used identical phishing mes-
fol-
sages or
lowed
(e.g.,
‘http://X.com/z/office365/index.html’ vs.
‘http://Y.com/z/office365/index.html’)
structures
identical
nearly
Figure 4: Fraction of organizations with x hijacked accounts that
sent at least one lateral phishing email. 13 organizations had only 1
ATO; the remaining 20 saw lateral phishing from 2+ ATOs (§ 6.1).
on Figure 4, over 60% of the compromised organizations in
our dataset experienced lateral phishing attacks from at least
two hijacked employee accounts. Given that our set of attacks
likely contains false negatives (thus underestimating the preva-
lence of attacks), these numbers illustrate that lateral phishing
attacks are widespread across enterprise organizations.
Successful Attacks: Given our dataset, we do not deﬁnitively
know whether an attack succeeded. However, we conserva-
tively (under)estimate the success rate of lateral phishing
using the methodology below. Based on this procedure, we
estimate that at least 11% of lateral phishers successfully
compromise at least one new enterprise account.
Let Alice and Bob represent two different ATOs at the same
organization, where PA and PB represent one of Alice’s and
Bob’s phishing emails respectively, and ReplyB represents a
reply from Bob to a lateral phishing email he received from
Alice. Intuitively, our methodology concludes that Alice suc-
cessfully compromised Bob if (1) Bob received a phishing
email from Alice, (2) shortly after receiving Alice’s phish,
Bob then subsequently sent his own phish, and (3) we have
strong evidence that the two employees’ phishing emails are
related (reﬂected in criteria 3 and 4 below).
Formally, we say that PA succeeded in compromising Bob’s
account if all of the following conditions are true:
1. Bob was a recipient of PA
2. After receiving PA, Bob subsequently sent his own lateral
phishing emails (PB)
3. Either of the following two conditions are met:
(a) PB and PA used similar phishing content: if the
two attacks used identical subjects or if both of
the phishing URLs they used belonged to the same
fully-qualiﬁed domain
(b) Bob sent a reply (ReplyB) to PA, where his reply
suggests he fell for Alice’s attack and where Bob
sent ReplyB prior to his own attack (PB)
4. Either of the following two conditions are met:
(a) PB was sent within two days after Bob received PA
Unpacking the ﬁnal criteria (#4), in the ﬁrst case (4.a), we
settled on a two-day interarrival threshold based on prior lit-
erature [21, 22], which suggests that 50% of users respond
to an email within 2 days and roughly 75% of users who
click on a spam email do so within 2 days. Assuming that
phishing follows similar time constants for how long it takes
a recipient to take action, 2 days represented a conservative
threshold to establish a link between PA and PB. At the same
time, both prior works show there exists a long tail of users
who take weeks to read and act on an email. The second part