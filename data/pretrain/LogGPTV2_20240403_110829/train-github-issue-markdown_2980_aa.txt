Apache Druid 0.23.0 contains over 450 new features, bug fixes, performance
enhancements, documentation improvements, and additional test coverage from 81
contributors. See the complete set of changes for additional details.
# # New Features
## # Query engine
### # Grouping on arrays without exploding the arrays
You can now group on a multi-value dimension as an array. For a datasource
named "test":
    {"timestamp": "2011-01-12T00:00:00.000Z", "tags": ["t1","t2","t3"]}  #row1
    {"timestamp": "2011-01-13T00:00:00.000Z", "tags": ["t3","t4","t5"]}  #row2
    {"timestamp": "2011-01-14T00:00:00.000Z", "tags": ["t5","t6","t7"]}  #row3
    {"timestamp": "2011-01-14T00:00:00.000Z", "tags": []}                #row4
The following query:
    {
      "queryType": "groupBy",
      "dataSource": "test",
      "intervals": [
        "1970-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"
      ],
      "granularity": {
        "type": "all"
      },
      "virtualColumns" : [ {
        "type" : "expression",
        "name" : "v0",
        "expression" : "mv_to_array(\"tags\")",
        "outputType" : "ARRAY"
      } ],
      "dimensions": [
        {
          "type": "default",
          "dimension": "v0",
          "outputName": "tags"
          "outputType":"ARRAY"
        }
      ],
      "aggregations": [
        {
          "type": "count",
          "name": "count"
        }
      ]
    }
Returns the following:
    [
     {
        "timestamp": "1970-01-01T00:00:00.000Z",
        "event": {
          "count": 1,
          "tags": "[]"
        }
      },
      {
        "timestamp": "1970-01-01T00:00:00.000Z",
        "event": {
          "count": 1,
          "tags": "["t1","t2","t3"]"
        }
      },
      {
        "timestamp": "1970-01-01T00:00:00.000Z",
        "event": {
          "count": 1,
          "tags": "[t3","t4","t5"]"
        }
      },
      {
        "timestamp": "1970-01-01T00:00:00.000Z",
        "event": {
          "count": 2,
          "tags": "["t5","t6","t7"]"
        }
      }
    ]
(#12078)  
(#12253)
### # Specify a column other than __time column for row comparison in
first/last aggregators
You can pass time column in `*first`/`*last` aggregators by using `LATEST_BY`
/ `EARLIEST_BY` SQL functions. This provides support for cases where the time
is stored as a part of a column different than "__time". You can also specify
another logical time column.  
(#11949)  
(#12145)
### # Improvements to querying user experience
This release includes several improvements for querying:
  * Added the SQL query ID to response header for failed SQL query to aid in locating the error messages (#11756)
  * Added input type validation for DataSketches HLL (#12131)
  * Improved JDBC logging (#11676)
  * Added SQL functions MV_FILTER_ONLY and MV_FILTER_NONE to filter rows of multi-value string dimensions to include only the supplied list of values or none of them respectively (#11650)
  * Added ARRAY_CONCAT_AGG to aggregate array inputs together into a single array (#12226)
  * Added the ability to authorize the usage of query context parameters (#12396)
  * Improved query IDs to make it easier to link queries and sub-queries for end-to-end query visibility (#11809)
  * Added a safe divide function to protect against division by 0 (#11904)
  * You can now add a query context to internally generated `SegmentMetadata` query (#11429)
  * Added support for Druid complex types to the native expression processing system to make all Druid data usable within expressions (#11853, #12016)
  * You can control the size of the on-heap segment-level dictionary via `druid.query.groupBy.maxSelectorDictionarySize` when grouping on string or array-valued expressions that do not have pre-existing dictionaries.
  * You have better protection against filter explosion during CNF conversion (#12314) (#12324)
  * You can get the complete native query on explaining the SQL query by setting `useNativeQueryExplain` to true in query context (#11908)
  * You can have broker ignore real time nodes or specific historical tiers. (#11766) (#11732)
## # Streaming Ingestion
### # Kafka input format for parsing headers and key
We've introduced a Kafka input format so you can ingest header data in
addition to the message contents. For example:
  * the event key field
  * event headers
  * the Kafka event timestamp
  * the Kafka event value that stores the payload.
(#11630)
### # Kinesis ingestion - Improvements
We have made following improvements in kinesis ingestion
  * Re-sharding can affect and slow down ingestion as many intermediate empty shards are created. These shards get assigned to tasks causing imbalance in load assignment. You can set `skipIgnorableShards` to `true` in kinesis ingestion tuning config to ignore such shards. (#12235)
  * Currently, kinesis ingestion uses `DescribeStream` to fetch the list of shards. This call is deprecated and slower. In this release, you can switch to a newer API `listShards` by setting `useListShards` to `true` in kinesis ingestion tuning config. (#12161)
## # Native Batch Ingestion
### # Multi-dimension range partitioning
Multi-dimension range partitioning allows users to partition their data on the
ranges of any number of dimensions. It develops further on the concepts behind
"single-dim" partitioning and is now arguably the most preferable secondary
partitioning, both for query performance and storage efficiency.  
(#11848)  
(#11973)
### # Improved replace data behavior
In previous versions of Druid, if ingested data with `dropExisting` flag to
replace data, Druid would retain the existing data for a time chunk if there
was no new data to replace it. Now, if you set `dropExisting` to `true` in
your `ioSpec` and ingest data for a time range that includes a time chunk with
no data, Druid uses a tombstone to overshadow the existing data in the empty
time chunk.  
(#12137)
This release includes several improvements for native batch ingestion:
  * Druid now emits a new metric when a batch task finishes waiting for segment availability. (#11090)
  * Added `segmentAvailabilityWaitTimeMs`, the duration in milliseconds that a task waited for its segments to be handed off to Historical nodes, to `IngestionStatsAndErrorsTaskReportData` (#11090)
  * Added functionality to preserve existing metrics during ingestion (#12185)
  * Parallel native batch task can now provide task reports for the sequential and single phase mode (e.g., used with dynamic partitioning) as well as single phase mode subtasks (#11688)
  * Added support for `RowStats` in `druid/indexer/v1/task/{task_id}/reports` API for multi-phase parallel indexing task (#12280)
  * Fixed the OOM failures in the dimension distribution phase of parallel indexing (#12331)
  * Added support to handle null dimension values while creating partition boundaries (#11973)
## # Improvements to ingestion in general
This release includes several improvements for ingestion in general:
  * Removed the template modifier from `IncrementalIndex` because it is no longer required
  * You can now use `JsonPath` functions in `JsonPath` expressions during ingestion (#11722)
  * Druid no longer creates a materialized list of segment files and elimited looping over the files to reduce OOM issues (#11903)
  * Added an intermediate-persist `IndexSpec` to the main "merge" method in `IndexMerger` (#11940)
  * `Granularity.granularitiesFinerThan` now returns ALL if you pass in ALL (#12003)
  * Added a configuation parameter for appending tasks to allow them to use a SHARED lock (#12041)
  * `SchemaRegistryBasedAvroBytesDecoder` now throws a `ParseException` instead of RE when it fails to retrieve a schema (#12080)
  * Added `includeAllDimensions` to `dimensionsSpec` to put all explicit dimensions first in `InputRow` and subsequently any other dimensions found in input data (#12276)
  * Added the ability to store null columns in segments (#12279)
## # Compaction
This release includes several improvements for compaction:
  * Automatic compaction now supports complex dimensions (#11924)
  * Automatic compaction now supports overlapping segment intervals (#12062)
  * You can now configure automatic compaction to calculate the ratio of slots available for compaction tasks from maximum slots, including autoscaler maximum worker nodes (#12263)
  * You can now configure the Coordinator auto compaction duty period separately from other indexing duties (#12263)
  * Default inputSegmentSizeBytes is now changed to ~ 100 TB (#12534)
  * You can change query granularity, change dimension schema, filter data, add metrics through auto-compaction (#11856) (#11874) (#11922) (#12125)
  * You can control roll-up as well for auto and manual compaction (#11850)
## # SQL
### # Human-readable and actionable SQL error messages
Until version 0.22.1, if you issued an unsupported SQL query, Druid would
throw very cryptic and unhelpful error messages. With this change, error
messages include exactly the part of the SQL query that is not supported in
Druid. For example, if you run a scan query that is ordered on a dimension
other than the time column.
(#11911)
### # Cancel API for SQL queries
We've added a new API to cancel SQL queries, so you can now cancel SQL queries
just like you can cancel native queries. You can use the API from the web
console. In previous versions, cancellation from the console only closed the
client connection while the SQL query kept running on Druid.
(#11643)  
(#11738)  
(#11710)
### # Improved SQL compatibility
We have made changes to expressions that make expression evaluation more SQL
compliant. This new behaviour is disabled by default. It can be enabled by
setting `druid.expressions.useStrictBooleans` to `true`. We recommend enabling
this behaviour since it is also more performant in some cases.
(#11184)
### # Improvements to SQL user experience
This release includes several additional improvements for SQL:
  * You no longer need to include a trailing slash `/` for JDBC connections to Druid (#11737)
  * You can now use scans as outer queries (#11831)
  * Added a class to sanitize JDBC exceptions and to log them (#11843)
  * Added type headers to response format to make it easier for clients to interpret the results of SQL queries (#11914)
  * Improved the way the `DruidRexExecutor` handles numeric arrays (#11968)
  * Druid now returns an empty result after optimizing a GROUP BY query to a time series query (#12065)
  * As an administrator, you can now configure the implementation for APPROX_COUNT_DISTINCT and COUNT(DISTINCT expr) in approximate mode (#11181)
## # Coordinator/Overlord
  * Coordinator can be overwhelmed by the connections from other druid services, especially when TLS is enabled. You can mitigate this by setting `druid.global.http.eagerInitialization` to `false` in common runtime properties.
## # Web console
  * Query view can now cancel all queries issued from it (#11738)
  * The auto refresh functions will now run in foreground only (#11750) this prevents forgotten background console tabs from putting any load on the cluster.
  * Add a `Segment size` (in bytes) column to the Datasources view (#11797)
![image](https://user-
images.githubusercontent.com/177816/167921423-b6ca2a24-4af7-4b48-bff7-3c16e3e9c398.png)