### Algorithm 1: ADV2 against MASK

**Input:**
- \( x^\circ \): Benign input
- \( c_t \): Target class
- \( m_t \): Target map
- \( f \): Target deep neural network (DNN)
- \( g \): MASK interpreter

**Output:**
- \( x^* \): Adversarial input

**Algorithm:**
1. Initialize \( x \) and \( m \) as \( x^\circ \) and \( g(x^\circ; f) \).
2. While not converged:
   - **Update \( m \)**: Update \( m \) by gradient descent along \( \nabla_m \ell_{\text{map}}(m; x) \).
   - **Update \( x \) with single-step lookahead**: Update \( x \) by gradient descent along \( \nabla_x \ell_{\text{adv}} \left( x, m - \xi \nabla_m \ell_{\text{map}}(m; x) \right) \).
3. Return \( x \).

**Explanation:**
Algorithm 1 outlines the attack strategy against the MASK interpreter. Further implementation details are provided in Section 3.6.

### 3.6 Implementation and Optimization

In this section, we detail the implementation of ADV2 and present a suite of optimizations to enhance the attack's effectiveness against specific interpreters.

#### Iterative Optimizer
We build the optimizer based on Projected Gradient Descent (PGD) [35], which iteratively updates the adversarial input using Equation (4). By default, we use the L∞ norm to measure the perturbation magnitude. Alternative frameworks can be adopted if other perturbation metrics are considered. For instance, instead of directly modifying pixels, one may generate adversarial inputs via spatial transformation [2, 60], where the perturbation magnitude is often measured by the overall spatial distortion. We detail and evaluate spatial transformation-based ADV2 in Section 4.

#### Warm Start
Our evaluation shows that it is often inefficient to search for adversarial inputs by running the update steps of ADV2 (Equation (4)) from scratch. Instead, first running a fixed number (e.g., 400) of update steps of a regular adversarial attack and then resuming the ADV2 update steps significantly improves the search efficiency. This strategy quickly approaches the manifold of adversarial inputs and then searches for inputs satisfying both prediction and interpretation constraints.

#### Label Smoothing
We measure the prediction loss \( \ell_{\text{prd}}(f(x), c_t) \) with cross entropy. When attacking GRAD, ADV2 may generate intermediate inputs that cause \( f \) to make over-confident predictions (e.g., with probability 1). The all-zero gradient of \( \ell_{\text{prd}} \) prevents the attack from finding inputs with desirable interpretations. To address this, we refine cross entropy with label smoothing [55]. We sample \( y_{c_t} \) from a uniform distribution \( U(1-\rho, 1) \) and define \( y_c = \frac{1-y_{c_t}}{|C|-1} \) for \( c \neq c_t \). The prediction loss is then defined as \( \ell_{\text{prd}}(f(x), c_t) = -\sum_{c \in C} y_c \log f_c(x) \). During the attack, we gradually decrease \( \rho \) from 0.05 to 0.01.

#### Multistep Lookahead
In implementing Algorithm 1, we apply multiple steps of gradient descent in both updating \( m \) (line 3) and computing the surrogate map \( m^*(x) \) (line 4). This approach leads to faster convergence in our empirical evaluation. To improve optimization stability, we use the average gradient to update \( m \). Specifically, let \( \{m_j^{(i)}\} \) be the sequence of maps obtained at the i-th iteration by applying multistep gradient descent. We use the aggregated interpretation loss \( \sum_j \|m_j^{(i)} - m_t\|^2_2 \) to compute the gradient for updating \( m \).

#### Adaptive Learning Rate
To improve the convergence of Algorithm 1, we dynamically adjust the learning rate for updating \( m \) and \( x \). At each iteration, we use a running Adam optimizer as a meta-learner [4] to estimate the optimal learning rate for updating \( m \) (line 3). We update \( x \) in a two-step fashion to stabilize the training:
1. First, update \( x \) in terms of the prediction loss \( \ell_{\text{prd}} \).
2. Then, update \( x \) in terms of the interpretation loss \( \ell_{\text{int}} \). During this step, we use binary search to find the largest step size such that \( x \)'s confidence remains above a certain threshold \( \kappa \) after the perturbation.

#### Periodical Reset
In Algorithm 1, we update the estimate of the attribution map by following gradient descent on \( \ell_{\text{map}} \). As the number of update steps increases, this estimate may deviate significantly from the true map generated by the MASK interpreter, negatively impacting the attack's effectiveness. To address this, we periodically (e.g., every 50 iterations) replace the estimated map with the map \( g(x; f) \) directly computed by MASK based on the current adversarial input. At the same time, we reset the Adam step parameter to correct its internal state.

### 4. Attack Evaluation

Next, we conduct an empirical study of ADV2 on a variety of DNNs and interpreters from both qualitative and quantitative perspectives. Our experiments aim to answer the following key questions about ADV2:

- **Q1:** Is it effective in deceiving target classifiers?
- **Q2:** Is it effective in misleading target interpreters?
- **Q3:** Is it evasive with respect to attack detection methods?
- **Q4:** Is it effective in real security-critical applications?
- **Q5:** Is it flexible to adopt alternative attack frameworks?

#### Experimental Setting

We introduce the setting of our empirical evaluation.

**Datasets:**
- Our evaluation primarily uses ImageNet [12], which consists of 1.2 million images from 1,000 classes. Each image is center-cropped to 224×224 pixels. For a given classifier \( f \), we randomly sample 1,000 images from the validation set of ImageNet that are correctly classified by \( f \) to form our test set. All pixel values are normalized to the range [0, 1].

**Classifiers:**
- We use two state-of-the-art DNNs as classifiers: ResNet-50 [22] and DenseNet-169 [24], which achieve 77.15% and 77.92% top-1 accuracy on ImageNet, respectively. Using two DNNs with distinct capacities (50 layers vs. 169 layers) and architectures (residual blocks vs. dense blocks) allows us to factor out the influence of individual DNN characteristics.

**Interpreters:**
- We adopt GRAD [50], CAM [64], RTS [10], and MASK [16] as representatives of back-propagation-, representation-, model-, and perturbation-guided interpreters, respectively. We use their open-source implementations in our evaluation. Since RTS is tightly coupled with its target DNN (i.e., ResNet), we train a new masking model for DenseNet. To assess the validity of the implementation, we evaluate all the interpreters in a weakly semi-supervised localization task [8] using the benchmark dataset and method in [10]. Table 2 summarizes the results, showing the performance of all the interpreters.