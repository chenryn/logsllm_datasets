move to reach the equilibrium.
Algorithm 1: ADV2 against MASK.
Input: x◦: benign input; ct: target class; mt: target map; f : target
Output: x∗: adversarial input
DNN; g: MASK interpreter
1 initialize x and m as x◦ and g(x◦; f );
2 while not converged do
3
4
// update m
update m by gradient descent along ∇m(cid:96)map(m;x);
// update x with single-step lookahead
update x by gradient descent along
∇x(cid:96)adv
(cid:0)x, m− ξ∇m(cid:96)map (m;x)(cid:1);
5 return x;
Algorithm 1 sketches the attack against MASK. More im-
plementation details are given in § 3.6.
3.6
Implementation and Optimization
Next we detail the implementation of ADV2 and present
a suite of optimizations to improve the attack effectiveness
against speciﬁc interpreters.
Iterative Optimizer – We build the optimizer based upon
PGD [35], which iteratively updates the adversarial input using
Eqn (4). By default, we use L∞ norm to measure the perturba-
tion magnitude. It is possible to adopt alternative frameworks
if other perturbation metrics are considered. For instance,
instead of modifying pixels directly, one may generate adver-
sarial inputs via spatial transformation [2, 60], in which the
perturbation magnitude is often measured by the overall spa-
tial distortion. We detail and evaluate spatial transformation-
based ADV2 in § 4.
Warm Start – It is observed in our evaluation that it is of-
ten inefﬁcient to search for adversarial inputs by running the
update steps of ADV2 (Eqn (4)) from scratch. Rather, ﬁrst run-
ning a ﬁxed number (e.g., 400) of update steps of the regular
adversarial attack and then resuming the ADV2 update steps
signiﬁcantly improves the search efﬁciency. Intuitively, this
strategy ﬁrst quickly approaches the manifold of adversarial
inputs, and then searches for inputs satisfying both prediction
and interpretation constraints.
Label Smoothing – Recall that we measure the prediction
loss (cid:96)prd( f (x),ct ) with cross entropy. When attacking GRAD,
ADV2 may generate intermediate inputs that cause f to make
over-conﬁdent predictions (e.g., with probability 1). The all-
zero gradient of (cid:96)prd prevents the attack from ﬁnding inputs
with desirable interpretations. To solve this, we reﬁne cross
entropy with label smoothing [55]. We sample yct from a
USENIX Association
29th USENIX Security Symposium    1663
1−yct
uniform distribution U(1−ρ,1) and deﬁne yc =
|C|−1 for c (cid:54)=
ct and (cid:96)prd( f (x),ct ) = −∑c∈C yc log fc(x). During the attack,
we gradually decrease ρ from 0.05 to 0.01.
Multistep Lookahead – In implementing Algorithm 1, we
apply multiple steps of gradient descent in both updating
m (line 3) and computing the surrogate map m∗(x) (line 4),
which is observed to lead to faster convergence in our empiri-
cal evaluation. Further, to improve the optimization stability,
we use the average gradient to update m. Speciﬁcally, let
{m(i)
j } be the sequence of maps obtained at the i-th iteration
by applying multistep gradient descent. We use the aggregated
interpretation loss ∑ j (cid:107)m(i)
2 to compute the gradient for
updating m.
Adaptive Learning Rate – To improve the convergence
of Algorithm 1, we also dynamically adjust the learning rate
for updating m and x. At each iteration, we use a running
Adam optimizer as a meta-learner [4] to estimate the optimal
learning rate for updating m (line 3). We update x in a two-
step fashion to stabilize the training: (i) ﬁrst updating x in
terms of the prediction loss (cid:96)prd, and (ii) updating it in terms
of the interpretation loss (cid:96)int. During (ii), we use a binary
search to ﬁnd the largest step size, such that x’s conﬁdence is
still above a certain threshold κ after the perturbation.
j −mt(cid:107)2
Periodical Reset – Recall that in Algorithm 1, we update
the estimate of the attribution map by following gradient
descent on (cid:96)map. As the number of update steps increases,
this estimate may deviate signiﬁcantly from the true map
generated by the MASK interpreter, which negatively impacts
the attack effectiveness. To address this, periodically (e.g.,
every 50 iterations), we replace the estimated map with the
map g(x; f ) that is directly computed by MASK based on the
current adversarial input. At the same time, we reset the Adam
step parameter to correct its internal state.
4 Attack Evaluation
Next we conduct an empirical study of ADV2 on a variety of
DNNs and interpreters from both qualitative and quantitative
perspectives. Speciﬁcally, our experiments are designed to
answer the following key questions about ADV2:
Q1: Is it effective to deceive target classiﬁers?
Q2: Is it effective to mislead target interpreters?
Q3: Is it evasive with respect to attack detection methods?
Q4: Is it effective in real security-critical applications?
Q5: Is it ﬂexible to adopt alternative attack frameworks?
Experimental Setting
We ﬁrst introduce the setting of our empirical evaluation.
Datasets – Our evaluation primarily uses ImageNet [12],
which consists of 1.2 million images from 1,000 classes. Ev-
ery image is center-cropped to 224×224 pixels. For a given
classiﬁer f , from the validation set of ImageNet, we randomly
sample 1,000 images that are classiﬁed correctly by f to form
our test set. All the pixels are normalized to [0,1].
Classiﬁers – We use two state-of-the-art DNNs as the
classiﬁers, ResNet-50 [22] and DenseNet-169 [24], which
respectively attain 77.15% and 77.92% top-1 accuracy on
ImageNet. Using two DNNs of distinct capacities (50 layers
versus 169 layers) and architectures (residual blocks versus
dense blocks), we factor out the inﬂuence of the characteris-
tics of individual DNNs.
Interpreters – We adopt GRAD [50], CAM [64], RTS [10],
and MASK [16] as the representatives of back-propagation-,
representation-, model-, and perturbation-guided interpreters
respectively. We adopt their open-source implementation in
our evaluation. As RTS is tightly coupled with its target DNN
(i.e., ResNet), we train a new masking model for DenseNet.
To assess the validity of the implementation, we evaluate
all the interpreters in a weakly semi-supervised localization
task [8] using the benchmark dataset and method in [10].
Table 2 summarizes the results. The performance of all the