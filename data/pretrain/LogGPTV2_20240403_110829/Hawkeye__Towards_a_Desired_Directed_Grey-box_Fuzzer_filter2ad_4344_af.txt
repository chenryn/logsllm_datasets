their coverage. Therefore, this criterion is an important factor for
measuring DGFs’ capabilities.
Google’s fuzzing test suite contains three projects that specially
focus on testing fuzzers’ abilities of discovering hard to reach loca-
tions, namely libjpeg-turbo-07-2017 (#1), libpng 1.2.56 (#2, #3) and
freetype2-2017 (#4). In these benchmarks, the target sites are speci-
fied by file names with line numbers in the source files. Here we
manually added some additional “sentinel” code in the target sites
(“exit(n)”, where the values of ‘n‘ distinguish these sites) to indicate
that the relevant targets have been reached.
Table 5 shows the results on these benchmarks. Case #1 and #4
show that Hawkeye exhibits good capability in terms of rapidly
covering the target sites, according to µTTE and the factor columns;
considering A12, the behaviors are also steady. In case #2 and #3,
it takes little time to reach these target sites for all the fuzzers.
While on the relevant project page [14], it is mentioned clearly
that they “currently require too much time to find”. We actually
tried this benchmark on libFuzzer with the default scripts in 2
machines, indeed it failed to reach the target sites. This root cause
of the inconsistency may lie in the fact that the inner mechanisms
may affect the actual fuzzing effectiveness (In fact, libFuzzer is
known to be quite different from AFL and its derivations). The
other observation is that HE-Go has a rather big value in terms of
µTTE compared to other tools. It turns out that in one of the runs,
the TTE is 524s, much larger than all the other runs.
It is worth noting that the µTTE to cover the target sites (Table
5) is quite different from the µTTE to trigger real-world crashes
(Table 4): the TTEs of the former are calculated based on the du-
ration to cover the specific line at the first time; while the TTEs
of the latter are tightly relevant to branch coverage or even path
coverage since typically bugs in widely used software can only be
triggered with special path conditions and it requires covering a
few execution traces. Although Table 5 shows that Hawkeye’s im-
provements against HE-Go in covering target sites are not obvious
(and for a few cases, it performs worse), we can observe in Table 4
the acceleration on crash reproduction is significant specially for
#4 (1.80x, nearly 2 hours off) and #2 (1.23x). This actually indicates
that dynamic strategies are quite effective in detecting crashes.
5.5 Answers to Research Questions
With the experiments conducted in Tables 2, 3, 4 and 5, we are able
to answer the research questions.
RQ1 We consider it is worth to apply static analysis. As shown
in Table 1, the time cost of our static analysis is generally
acceptable compared to the runtime cost during fuzzing. Even
for the cxxfilt cases in Table 2, which takes on average 735
seconds, Hawkeye outperforms the vanilla AFL in most of
the cases. Two notable results are CVE-2016-4491 and CVE-
2016-6131, it saves roughly 2000s and 9000s to detect the
crash; as shown from the A12 metric, the results are also
consistent in all the 20 runs. On the other hand, Hawkeye
also demonstrates some boosts for fuzzing.
RQ2 Hawkeye performs quite well in detecting crashes. From the
results in Tables 2, 3 and 4, we can clearly see that Hawkeye
can detect the crashes more quickly than all the other tools;
the results are even steady among different runs as shown by
different A12 results.
RQ3 The dynamic strategies used in Hawkeye are quite effective. It
is obvious that in all the experiments we conducted, Hawkeye
outperforms the others. In particular, the experiments in com-
parison with HE-Go (Table 4 and 5) show that our combi-
nation of power scheduling, adaptive mutation strategies,
and seed prioritization make Hawkeye converge faster than
AFLGo’s simulated annealing based scheduling.
RQ4 From the results in Table 5, we are confident that Hawkeye
has the capability to reach the target sites rapidly.
In practice, Hawkeye also demonstrates its power in exposing
crashes with the help of other vulnerability detection tools. For
example, for Oniguruma and MJS projects, with the Clang Static
Analyzer [27] (the built-in and our customized checkers) reporting
suspicious vulnerability locations (i.e., target sites) in the programs,
Hawkeye successfully detected the crashes by directing the fuzzing
to those locations. Interestingly, for MJS, we marked several of
the authors’ newly patched program locations, and detected a few
other crashes even further. As a result, Hawkeye has reported more
than 28 crashes in projects Oniguruma and MJS. We have also
found multiple vulnerabilities in other projects such as Intel XED
x86 encoder decoder (4 crashes), Espruino JavaScript interpreter (9
crashes). All these crashes have been confirmed and fixed, and 15
of them have been assigned with CVE IDs.
5.6 Threats to Validity
The internal threats of validity are twofold: 1). Several components
of Hawkeye (e.g., Algo. 1 and 2) utilize the predefined thresholds
to make decision. Currently, these thresholds (e.g., γ = 0.1, δ = 0.4,
σ = 0.2, ζ = 0.8) are configured according to our preliminary ex-
periments and previous experience in fuzzing. Systematic research
will be planned to investigate the impact of these thresholds and
figure out the best configurations. 2). As we rely on the lightweight
program analysis tools like LLVM and SVF [41] to calculate the
distance, possible issues of these tools may affect the final results.
As Hawkeye is well modularized and can easily integrate with other
static analysis tools, enhancing Hawkeye with other tools will be
another alternative solution.
The external threats rise from the choice of evaluation dataset
and the CVEs for crash reproduction. Despite we adopt the program
Binutils that is used in AFLGo [6], the evaluation results still need
to be generalized with an empirical study on more projects in future.
Besides, the tested CVEs in MJS and Oniguruma are not selectively
chosen for the purpose to show the advance of Hawkeye— we pick
them since they are reported within a recent period.
6 RELATED WORK
Our study is related to the following lines of research:
Directed Grey-box Fuzzing. Some other DGF techniques have
been proposed besides Hawkeye. AFLGo [6] is the state-of-the-art
directed grey-box fuzzer which utilizes a simulated annealing-based
power schedule that gradually assigns more energy to inputs that
hold the trace closer to the target sites. In AFLGo, the authors
proposed a novel idea of calculating the distance between the in-
put traces and the target sites. This is a good starting point by
combining such target distance calculation with grey-box fuzzer.
Hawkeye is inspired from AFLGo however provides significant im-
provements on both the static analysis and dynamic fuzzing. As
shown in §5, Hawkeye generally outperforms AFLGo in terms of
reaching the targets and reproducing crashes, thanks to embedding
in-depth consideration about the four desired properties into the
design. SeededFuzz [45] uses various program analysis techniques
to facilitate the generation and selection of initial seeds which
helps to achieve the goal of directed fuzzing. Equipped with the
improved seed selection and generation techniques, SeededFuzz
can reach more critical sites and find more vulnerabilities. The core
techniques of SeededFuzz are orthogonal to Hawkeye because Seed-
edFuzz focuses on the quality of initial seed inputs while Hawkeye
focuses on the four desirable properties regardless of initial seeds.
Note that our proposed four properties can also be applied for
DGF on programs where the source code is unavailable. In fact,
we are extending Hawkeye to be able to work on the binary-only
fuzzing scenarios. Technically, the target sites can be determined
by binary-code matching techniques on attack surface identifica-
tion [9, 47]; the static analysis can be achieved with binary analysis
tools such as IDA [20]; and the instrumentation can be done by
dynamic binary instrumentators such as Intel Pin [1]. We envision
the extended Hawkeye can piggyback on these techniques and
demonstrate its effectiveness even further.
Directed Symbolic Execution. Directed Symbolic Execution (DSE)
is one of the most related techniques to DGF as it also aims to exe-
cute target sites of the PUT. Several works have been proposed for
DSE [17, 19, 21, 28, 29]. These DSE techniques rely on heavyweight
program analysis and constraint-solving to reach the target sites
systematically. A typical example of DSE is Katch [29], which relies
on symbolic execution, augmented by several synergistic heuristics
based on static and dynamic program analysis. Katch can effectively
find bugs in incomplete patches and increase the patch coverage
comparing to manual test suite. However, as discussed in [6], DGF is
generally more effective on real-world programs as DSE techniques
suffer from the infamous path-explosion problem [40]. In contrast
to DSE, Hawkeye relies on lightweight program analysis, which
ensures its scalability and execution efficiency.
Taint Analysis Aided Fuzzing. Taint analysis is also widely used
to facilitate directed white-box testing [12, 16, 24, 34, 44]. The key
intuition of using taint analysis in fuzzing is to identify certain
parts of the input which should be mutated with priority. In such a
way, the fuzzer can drastically reduce the search space for reaching
certain desired locations. Taint based approaches are more scalable
than the DSE techniques and can help the fuzzer to reach certain
preferable locations such as rare branches in Fairfuzz [24] or check-
sum related code in TaintScope [44]. Different from Hawkeye, these
techniques are not fed with given target sites (e.g., file name and
line numbers in our scenario) but based on source-sink pairs. Thus,
such techniques do not have advantages in scenarios where the
targets are clear, such as patch testing and crash reproduction.
Coverage-based Grey-box Fuzzing. The purposes of coverage-
based grey-box fuzzing (CGF) and DGF are different. However, some
techniques proposed to boost the performance of CGF could also
be adopted by Hawkeye. For example, CollAFL [15] utilizes a novel
hash algorithm to solve AFL’s instrumentation collision problem.
Skyfire [43] learns a probabilistic context sensitive grammar (PGSG)
to specify both syntax features and semantic rules, and then the
second step leverages the learned PCSG to generate new test seeds.
Xu et al. [46] proposed a set of new operating primitives to improve
the performance of grey-box fuzzers. Another important topic in
CGF is about guiding the fuzzer through path constraints. [12, 25,
32, 34, 40] aim to help the CGFs to break through path constraints.
Moreover, Orthrus [38] applies static analysis on AST, CFG, and CG
to extract complicated tokens via customizable queries. Hawkeye
can benefit through combining with the aforementioned techniques.
7 CONCLUSIONS
In this paper, we propose a novel directed grey-box fuzzer, Hawkeye.
The design of Hawkeye embeds four desired properties for directed
fuzzing by combining static analysis and dynamic fuzzing in an
effective way. Equipped with a better evaluation of the distance
between input execution traces and the user specified target sites,
Hawkeye can precisely and adaptively adjust its seed prioritiza-
tion, power scheduling as well as mutation strategies to reach the
target sites rapidly. A thorough evaluation showed that Hawkeye
can reach the target sites and reproduce the crashes much faster
than existing state-of-the-art grey-box fuzzers. The promising re-
sults indicate that Hawkeye can be effective in patch testing, crash
exposure and other scenarios.
ACKNOWLEDGMENT
This work is supported by the National Research Foundation, Prime
Ministers Office, Singapore under its National Cybersecurity R&D
Program (Award No. NRF2016NCR-NCR002-026) and administered
by the National Cybersecurity R&D Directorate; the research of Dr
Xue is also supported by CAS Pioneer Hundred Talents Program.
REFERENCES
[1] 2018. Pin - A Dynamic Binary Instrumentation Tool. https://software.intel.com/
en-us/articles/pin-a-dynamic-binary-instrumentation-tool
[2] AFLGo. 2018. GitHub - AFLGo. https://github.com/aflgo/aflgo/issues
[3] Lars Ole Andersen. 1994. Program Analysis and Specialization for the C Program-
ming Language. Technical Report. DIKU, University of Copenhagen.
[4] Jean-Yves Audibert, Rémi Munos, and Csaba Szepesvári. 2009. Exploration-
exploitation tradeoff using variance estimates in multi-armed bandits. Theoretical
Computer Science 410, 19 (2009), 1876 – 1902. http://www.sciencedirect.com/
science/article/pii/S030439750900067X Algorithmic Learning Theory.
[5] GNU Binutils. 1990. GNU Binutils. https://www.gnu.org/software/binutils/
[6] Marcel Böhme, Van-Thuan Pham, Manh-Dung Nguyen, and Abhik Roychoudhury.
2017. Directed Greybox Fuzzing (CCS ’17). ACM Press, New York, NY, USA, 2329–
2344.
[7] Marcel Böhme, Van-Thuan Pham, and Abhik Roychoudhury. 2016. Coverage-
based Greybox Fuzzing As Markov Chain (CCS ’16). ACM Press, New York, NY,
USA, 1032–1043.
[8] Denny Britz. 2014.
Exploitation vs Exploration.
@dennybritz/exploration-vs-exploitation-f46af4cf62fe
https://medium.com/
[9] Mahinthan Chandramohan, Yinxing Xue, Zhengzi Xu, Yang Liu, Chia Yuan Cho,
and Hee Beng Kuan Tan. 2016. BinGo: Cross-architecture cross-OS Binary Search
(FSE ’16). ACM Press, New York, NY, USA, 678–689.
[10] Chen Chen, Baojiang Cui, Jinxin Ma, Runpu Wu, Jianchao Guo, and Wenqian
Liu. 2018. A systematic review of fuzzing techniques. Computers & Security 75
(2018), 118–137.
[11] Hongxu Chen, Yuekang Li, Bihuan Chen, Yinxing Xue, and Yang Liu. 2018. FOT:
A Versatile, Configurable, Extensible Fuzzing Framework (FSE ’18 tool demo).
ACM Press, (to appear).
[12] Peng Chen and Hao Chen. 2018. Angora: Efficient Fuzzing by Principled Search.
CoRR abs/1803.01307 (2018). arXiv:1803.01307 https://arxiv.org/abs/1803.01307v2
https://lwn.net/
[13] J. B. Crawford. 2018. A survey of some free fuzzing tools.
Articles/744269/
[14] fuzzer-test suite. 2018. libpng-1.2.56/test-libfuzzer.sh. https://github.com/google/
fuzzer-test-suite/blob/master/libpng-1.2.56/test-libfuzzer.sh
[15] Shuitao Gan, Chao Zhang, Xiaojun Qin, Xuwen Tu, Kang Li, Zhongyu Pei, and
Zuoning Chen. 2018. CollAFL: Path Sensitive Fuzzing (SP ’18). IEEE Press, 1–12.
[16] Vijay Ganesh, Tim Leek, and Martin Rinard. 2009. Taint-based Directed Whitebox
Fuzzing (ICSE ’09). IEEE Computer Society, Washington, DC, USA, 474–484.
[17] Patrice Godefroid, Nils Klarlund, and Koushik Sen. 2005. DART: Directed Auto-
mated Random Testing (PLDI ’05). ACM Press, New York, NY, USA, 213–223.
[18] Google. 2017. Fuzzer Test Suite. https://github.com/google/fuzzer-test-suite
[19] Istvan Haller, Asia Slowinska, Matthias Neugschwandtner, and Herbert Bos. 2013.
Dowsing for Overflows: A Guided Fuzzer to Find Buffer Boundary Violations
(SEC ’13). USENIX Association, Berkeley, CA, USA, 49–64.
[20] Hex-Rays. 2013. IDA. https://www.hex-rays.com/index.shtml
[21] Wei Jin and Alessandro Orso. 2012. BugRedux: Reproducing Field Failures for
In-house Debugging (ICSE ’12). IEEE Press, Piscataway, NJ, USA, 474–484.
[22] Scott Kirkpatrick, C Daniel Gelatt, and Mario P Vecchi. 1983. Optimization by
simulated annealing. science 220, 4598 (1983), 671–680.
[23] K. Kosako. 2002. Oniguruma. https://github.com/kkos/oniguruma
[24] C. Lemieux and K. Sen. 2017.
FairFuzz: Targeting Rare Branches to
Rapidly Increase Greybox Fuzz Testing Coverage. ArXiv e-prints (Sept. 2017).
arXiv:cs.SE/1709.07101
[25] Yuekang Li, Bihuan Chen, Mahinthan Chandramohan, Shang-Wei Lin, Yang Liu,
and Alwen Tiu. 2017. Steelix: Program-state Based Binary Fuzzing (ESEC/FSE
’17). ACM Press, New York, NY, USA, 627–637.
[26] LLVM. 2015. libFuzzer. https://llvm.org/docs/LibFuzzer.html
[27] LLVM/Clang. 2013. Clang Static Analyzer. https://clang-analyzer.llvm.org/
[28] Kin-Keung Ma, Khoo Yit Phang, Jeffrey S. Foster, and Michael Hicks. 2011. Di-
rected Symbolic Execution (SAS’11). Springer-Verlag, Berlin, Heidelberg, 95–111.
[29] Paul Dan Marinescu and Cristian Cadar. 2013. KATCH: High-coverage Testing
of Software Patches (ESEC/FSE 2013). ACM Press, New York, NY, USA, 235–245.
[30] Barton P. Miller, Louis Fredriksen, and Bryan So. 1990. An Empirical Study of
the Reliability of UNIX Utilities. Commun. ACM 33, 12 (Dec. 1990), 32–44.
[31] Terence Parr. 2018. ANTLR (ANother Tool for Language Recognition). http:
//www.antlr.org/
[32] Hui Peng, Yan Shoshitaishvili, and Mathias Payer. 2018. T-Fuzz: Fuzzing by
Program Transformation (SP ’18). 697–710.
[33] PHP. 1994. PHP: Hypertext Preprocessor. http://www.php.net/
[34] Sanjay Rawat, Vivek Jain, Ashish Kumar, Lucian Cojocar, Cristiano Giuffrida,
and Herbert Bos. 2017. VUzzer: Application-aware Evolutionary Fuzzing (NDSS
’17). 1–14.
[35] Agostino Sarubbo. 2017.
cat_filename (dwarf2.c).
binutils-null-pointer-dereference-in-concat_filename-dwarf2-c
binutils: NULL pointer dereference in con-
https://blogs.gentoo.org/ago/2017/10/03/
[36] Agostino Sarubbo. 2017. binutils: NULL pointer dereference in concat_filename
https://blogs.
(dwarf2.c)
gentoo.org/ago/2017/10/24/binutils-null-pointer-dereference-in-concat_
filename-dwarf2-c-incomplete-fix-for-cve-2017-15023
(INCOMPLETE FIX FOR CVE-2017-15023).
[37] Konstantin Serebryany and Marcel Böhme. 2017. AFLGo: Directing AFL to reach
specific target locations. https://groups.google.com/forum/#!topic/afl-users/
qcqFMJa2yn4
[38] Bhargava Shastry, Markus Leutner, Tobias Fiebig, Kashyap Thimmaraju, Fabian
Yamaguchi, Konrad Rieck, Stefan Schmid, Jean-Pierre Seifert, and Anja Feldmann.
2017. Static Program Analysis as a Fuzzing Aid. In Research in Attacks, Intrusions,
and Defenses, Marc Dacier, Michael Bailey, Michalis Polychronakis, and Manos
Antonakakis (Eds.). Springer International Publishing, 26–47.
[39] Cesanta Software. 2016. mjs. https://github.com/cesanta/mjs
[40] Nick Stephens, John Grosen, Christopher Salls, Andrew Dutcher, Ruoyu Wang,
Jacopo Corbetta, Yan Shoshitaishvili, Christopher Krügel, and Giovanni Vigna.
2016. Driller: Augmenting Fuzzing Through Selective Symbolic Execution (NDSS
’16). 1–16.
[41] Yulei Sui and Jingling Xue. 2016. SVF: Interprocedural Static Value-flow Analysis
in LLVM (CC ’16). ACM Press, New York, NY, USA, 265–266.
[42] Andras Vargha, András Vargha, and Harold D. Delaney. 2000. A critique and
improvement of the CL common language effect size statistics of McGraw and
Wong. Journal of Educational and Behavioral Statistics 25, 2 (2000), 101–132.
[43] Junjie Wang, Bihuan Chen, Lei Wei, and Yang Liu. 2017. Skyfire: Data-Driven
Seed Generation for Fuzzing (SP ’17). 579–594.
[44] T. Wang, T. Wei, G. Gu, and W. Zou. 2010. TaintScope: A Checksum-Aware
Directed Fuzzing Tool for Automatic Software Vulnerability Detection (SP ’10).
497–512.
[45] Weiguang Wang, Hao Sun, and Qingkai Zeng. 2016. SeededFuzz: Selecting and
Generating Seeds for Directed Fuzzing. 49–56.
[46] Wen Xu, Sanidhya Kashyap, Changwoo Min, and Taesoo Kim. 2017. Designing
New Operating Primitives to Improve Fuzzing Performance (CCS ’17). ACM Press,
New York, NY, USA, 2313–2328.
[47] Yinxing Xue, Zhengzi Xu, Mahinthan Chandramohan, and Yang Liu. 2018. Accu-
rate and Scalable Cross-Architecture Cross-OS Binary Code Search with Emula-
tion. IEEE Trans Software Engineering (2018), (to appear).
[48] Michal Zalewski. 2014. American Fuzzy Lop. http://lcamtuf.coredump.cx/afl/
[49] Michal Zalewski. 2014. Technical "whitepaper" for afl-fuzz.
http://lcamtuf.
coredump.cx/afl/technical_details.txt
[50] Michal Zalewski. 2016. "FidgetyAFL" implemented in 2.31b.
https://groups.
google.com/forum/#!topic/afl-users/1PmKJC-EKZ0