This paper reports on work that was supported in part by NSF CNS
0953655 as well as an International Fulbright S&T Fellowship.
11. REFERENCES
[1] Emulab. https://www.emulab.net.
[2] Known bad relays in Tor.
https://trac.torproject.org/projects/tor/wiki/doc/badRelays.
[3] PlanetLab. http://www.planet-lab.org/.
[4] Tor Compass. https://compass.torproject.org/.
[5] Tor Consensus.
https://metrics.torproject.org/consensus-health.html.
[6] Tor controller.
[7] Tor Proposal 209.
https://svn.torproject.org/svn/blossom/trunk/TorCtl.py.
https://gitweb.torproject.org/user/mikeperry/torspec.git/blob/
path-bias-tuning:/proposals/209-path-bias-tuning.txt.
[8] TorFlow Project. https://gitweb.torproject.org/torﬂow.git.
[9] TorStatus. http://torstatus.blutmagie.de/index.php.
[10] Trotsky IP addresses. https://trac.torproject.org/projects/tor/
wiki/doc/badRelays/trotskyIps.
[11] Tor directory authorities compromised., 2010. https:
//blog.torproject.org/blog/tor-project-infrastructure-updates.
[12] E. Androulaki, M. Raykova, S. Srivatsan, A. Stavrou, and
S. Bellovin. PAR: Payment for Anonymous Routing. In
Proceedings of the 8th Symposium on Privacy Enhancing
Technologies, PETS’08, pages 219–236. Springer Berlin
Heidelberg, 2008.
[13] R. Aringhieri, E. Damiani, S. D. C. Di Vimercati,
S. Paraboschi, and P. Samarati. Fuzzy Techniques for Trust
and Reputation Management in Anonymous Peer-to-peer
Systems. J. Am. Soc. Inf. Sci. Technol., 57(4):528–537, 2006.
[14] K. Bauer, J. Juen, N. Borisov, D. Grunwald, D. Sicker, and
D. Mccoy. On the Optimal Path Length for Tor, 2010.
http://petsymposium.org/2010/papers/hotpets10-Bauer.pdf.
[15] K. Bauer, D. McCoy, D. Grunwald, T. Kohno, and D. Sicker.
Low-resource Routing Attacks Against Tor. In Proceedings
of the 6th ACM Workshop on Privacy in the Electronic
Society, WPES ’07, pages 11–20. ACM, 2007.
72[16] N. Borisov, G. Danezis, P. Mittal, and P. Tabriz. Denial of
Service or Denial of Security? In Proceedings of the 14th
ACM Conference on Computer and Communications
Security, CCS ’07, pages 92–102. ACM, 2007.
[17] S. Buchegger and J. Y. Le Boudec. A Robust Reputation
System for P2P and Mobile Ad-hoc Networks. In
Proceedings of the 2nd Workshop on the Economics of
Peer-to-Peer Systems (P2PEcon), 2004.
[18] D. L. Chaum. Untraceable online mail, return addresses, and
digital pseudonyms. Commun. ACM, 24(2):84–90, 1981.
[19] N. Danner, D. Krizanc, and M. Liberatore. Detecting Denial
of Service Attacks in Tor. In Proceedings of the 13th
International Conference on Financial Cryptography and
Data Security, FC ’09, pages 273–284. Springer Berlin
Heidelberg, 2009.
[20] R. Dingledine, M. Freedman, D. Hopwood, and D. Molnar.
A Reputation System to Increase MIX-Net Reliability. In
Proceedings of the 4th International Workshop on
Information Hiding, pages 126–141. Springer Berlin
Heidelberg, 2001.
[21] R. Dingledine and N. Mathewson. Tor path speciﬁcation.
https://gitweb.torproject.org/torspec.git?a=blob_plain;hb=
HEAD;f=path-spec.txt.
[22] R. Dingledine, N. Mathewson, and P. Syverson. Tor: The
Second-generation Onion Router. In Proceedings of the 13th
USENIX Security Symposium, SSYM’04. USENIX
Association, 2004.
[23] R. Dingledine and P. Syverson. Reliable MIX Cascade
Networks Through Reputation. In Proceedings of the 6th
International Conference on Financial Cryptography,
FC’03, pages 253–268. Springer-Verlag, 2003.
[24] P. Eckersley, E. Galperin, and K. Rodriguez. Dutch
government proposes cyberattacks against... everyone., 2012.
https://www.eff.org/deeplinks/2012/10/dutch-government-
proposes-cyberattacks-against-everyone.
[25] M. Edman and P. Syverson. AS-awareness in Tor Path
Selection. In Proceedings of the 16th ACM Conference on
Computer and Communications Security, CCS ’09, pages
380–389. ACM, 2009.
[26] A. C. Estes. NSA attacks Tor. http://gizmodo.com/the-nsas-
been-trying-to-hack-into-tors-anonymous-inte-1441153819.
[27] N. Feamster and R. Dingledine. Location diversity in
anonymity networks. In Proceedings of the 2004 ACM
Workshop on Privacy in the Electronic Society, WPES ’04,
pages 66–76, New York, NY, USA, 2004. ACM.
[28] S. D. Kamvar, M. T. Schlosser, and H. Garcia-Molina. The
Eigentrust Algorithm for Reputation Management in P2P
Networks. In Proceedings of the 12th International
Conference on World Wide Web, WWW ’03, pages 640–651.
ACM, 2003.
[29] H. Kwakernaak. Linear Optimal Control Systems. John
Wiley & Sons, Inc., New York, NY, USA, 1972.
[30] B. Levine, M. Reiter, C. Wang, and M. Wright. Timing
Attacks in Low-Latency Mix Systems. In Proceedings of the
8th International Conference on Financial Cryptography,
pages 251–265. Springer Berlin Heidelberg, 2004.
[31] P. Michiardi and R. Molva. Core: A Collaborative Reputation
Mechanism to Enforce Node Cooperation in Mobile Ad Hoc
Networks. In Proceedings of the 6th IFIP TC6/TC11 Joint
Working Conference on Communications and Multimedia
Security, pages 107–121. Springer US, 2002.
[32] L. Mui, M. Mohtashemi, and A. Halberstadt. A
computational model of trust and reputation for e-businesses.
In Proceedings of the 35th Annual Hawaii International
Conference on System Sciences, HICSS ’02, Washington,
DC, USA, 2002. IEEE Computer Society.
[33] S. J. Murdoch and P. Zieli´nski. Sampled Trafﬁc Analysis by
Internet-exchange-level Adversaries. In Proceedings of the
7th Symposium on Privacy Enhancing Technologies,
PETS’07, pages 167–183. Springer-Verlag, 2007.
[34] F. Oliviero and S. Romano. A Reputation-Based Metric for
Secure Routing in Wireless Mesh Networks. In Proceedings
of the 27th IEEE Global Telecommunications Conference,
IEEE GLOBECOM ’08, pages 1–5, 2008.
[35] M. Reed, P. Syverson, and D. Goldschlag. Anonymous
connections and onion routing. IEEE Journal on Selected
Areas in Communications, 16(4):482–494, May 1998.
[36] P. Resnick, K. Kuwabara, R. Zeckhauser, and E. Friedman.
Reputation systems. Commun. ACM, 43(12):45–48, 2000.
[37] V. Shmatikov and M.-H. Wang. Timing Analysis in
Low-Latency Mix Networks: Attacks and Defenses. In
Proceedings of the 11th European Symposium On Research
In Computer Security, ESORICS’06, pages 18–33. Springer
Berlin Heidelberg, 2006.
[38] M. Srivatsa, L. Xiong, and L. Liu. TrustGuard: Countering
Vulnerabilities in Reputation Management for Decentralized
Overlay Networks. In Proceedings of the 14th International
Conference on World Wide Web, WWW ’05, pages 422–431.
ACM, 2005.
[39] P. Syverson, G. Tsudik, M. Reed, and C. Landwehr. Towards
an Analysis of Onion Routing Security. In Proceedings of
International Workshop on Designing Privacy Enhancing
Technologies: Design Issues in Anonymity and
Unobservability, pages 96–114. Springer Berlin Heidelberg,
2001.
[40] Tsuen-Wan, R. Dingledine, and D. Wallach. Building
Incentives into Tor. In Proceedings of the 14th International
Conference on Financial Cryptography and Data Security,
FC’10, pages 238–256. Springer Berlin Heidelberg, 2010.
[41] M. Wright, M. Adler, B. Levine, and C. Shields. Defending
anonymous communications against passive logging attacks.
In Proceedings of the 2003 IEEE Symposium on Security and
Privacy, SP ’03, pages 28–41, 2003.
[42] M. K. Wright, M. Adler, B. N. Levine, and C. Shields. An
Analysis of the Degradation of Anonymous Protocols. In
Proceedings of the 9th Network and Distributed System
Security Symposium, NDSS ’02, 2002.
[43] L. Xiong and L. Liu. PeerTrust: supporting reputation-based
trust for peer-to-peer online communities. IEEE Transactions
on Knowledge and Data Engineering, 16(7):843–857, 2004.
[44] R. Zhou and K. Hwang. PowerTrust: A Robust and Scalable
Reputation System for Trusted Peer-to-Peer Computing.
IEEE Transactions on Parallel and Distributed Systems,
18(4):460–473, April 2007.
[45] Y. Zhu, X. Fu, B. Graham, R. Bettati, and W. Zhao. On Flow
Correlation Attacks and Countermeasures in Mix Networks.
In Proceedings of the 4th International Workshop on Privacy
Enhancing Technologies, PET’04, pages 207–225. Springer
Berlin Heidelberg, 2005.
APPENDIX
A. STABILITY ANALYSIS
From control theory we know that for a discrete-time linear sys-
tem to be stable all of the poles of its transfer function must lie in-
side the unit circle [29]. To determine this we ﬁrst need to take the
Z-transform of the transfer function and then determine its poles.
We ﬁrst rewrite equation (1) as the following ﬁrst order discrete-
time linear system.
y(n) = αx(n) + (1 − α)y(n − 1)
(9)
where y(n) and x(n) denotes the n-th output and input of the
system respectively (i.e., y(n) refers to newly generated reputation
value Rn of a relay while x(n) refers to the reference value Rc).
Taking the Z-transform of equation (9) yields:
Y (z) = αX(z) + (1 − α)z
−1Y (z)
(10)
73what we have just discussed. As we can see from the ﬁgure for
Kp = 1 it oscillates heavily and for Kp = 0, it totally discards
the difference between the reference value and system output. We,
therefore, conservatively set Kp to 0.5, so that the model becomes
neither too sensitive nor too insensitive to sudden deviation.
B.2 Reward (µ) and Punishment (ν) Factor
We now investigate how our reputation model responds to a se-
ries of failures and successes. We want the degree of punishment
to be greater than that of reward. So whenever the model receives a
negative feedback (in our case a rating of -1) we want our EWMA
function (equation (3)) to give higher weight to the current feed-
back (i.e., consider a larger value of α). As α in dependent of δ
(see equation (5)), we need to increase δ more for failure than suc-
cess. So under our setting we require µ > 1 and ν ≤ 1. Figure
11(b) highlights how reputation score reacts to different combina-
tions of (µ, ν). As long as µ > 1 and ν ≤ 1 our model can
effectively discourage selective DoS.
B.3 Conﬁdence Factor (β)
Now, we look at our conﬁdence factor β. The conﬁdence fac-
tor determines how conﬁdent a user is about the reputation score
of a particular relay. As the number of experience with a partic-
ular relay increases, a user becomes more conﬁdent about his/her
computed reputation score. β controls how quickly we become
conﬁdent about a reputation score. Figure 11(c) highlights how
conﬁdence factor increases as the number of interaction increases.
It should be mentioned that any monotonically increasing function
of the number of interactions can be used as a conﬁdence metric.
C. TUNING CUTOFFS FOR OUTLIERS
√
In ﬁltering outliers we previously chose a deviation interval of
3 times the standard deviation from the average (see Section 6).
Here, we investigate what kind of impact other deviation intervals
would have on the performance of Re3. A tradeoff exists between
the value of k and false errors — FN and FP. As we increase k
more relays become acceptable, so FP goes down but FN rises. We
tested for k = 1.3, 2.0. Figure 12 illustrates the FN and FP errors
for different values of k. From the ﬁgure we see that as we increase
the allowed deviation from average, more and more relays become
acceptable and as a result FP decreases while FN increases.
Figure 12: False Negative (FN) and False positive (FP) errors for different values of
k. As k increases FN tends to rise and FP tends to fall.
(a)
(b)
(c)
Figure 11: Sensitivity analysis of (a) controller gain Kp (b) reward µ and punish-
ment ν factor (c) conﬁdence factor β.
From equation (10) we can compute the transfer function as:
Y (z)
X(z)
=
αz
H(z) =
z − (1 − α)
(11)
So the transfer function H(z) has a pole at z = 1 − α. From
equation (3) we know that 0 < α < 1, so the pole will always
lies inside the unit circle. Thus, our closed-loop reputation system
guarantees stability.
B. TUNING MODEL PARAMETERS
In this section we look at how the parameters related to Re3
affect the reputation score of compromised relays. As shown in Ta-
ble 5, Re3 has a total of four parameters. We will study each of
their impact on reputation score. In the following study, we mainly
want to see how our model reacts to dynamic behavioral change.
For this purpose we assume that a compromised relay, with all the
other relays being honest, participates in a total 100 circuits oscil-
lating between honest and malicious nature every 25 interactions.
B.1 Proportional constant (Kp)
The proportional constant, Kp, determines to what degree we
want to react to the deviation between the reference value (Rc) and
current system output (Rn−1). This should not be set either too
high (near 1) or too low (near 0). If it is set too high it will oscillate
too much and if it is set too low then it will discount most of the
deviation and result in slow convergence. Figure 11(a) highlights
-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1 0 10 20 30 40 50 60 70 80 90 100Reputation ValueNumber of InteractionsHonest ZoneMalicious ZoneHonest ZoneMalicious ZoneKp=0.0Kp=0.25Kp=0.5Kp=0.75Kp=1.0-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1 0 10 20 30 40 50 60 70 80 90 100Reputation ValueNumber of InteractionsHonest ZoneMalicious ZoneHonest ZoneMalicious Zoneµ=2,ν=1µ=4,ν=1µ=2,ν=0.5µ=4,ν=0.5 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 10 20 30 40 50 60 70 80 90 100Confidence ValueNumber of Interactionsβ=0.05β=0.15β=0.25β=0.50β=0.75 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Average FNDrop rate (d)k=1.3g=0g=1/3g=2/3g=1 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Average FPDrop rate (d)k=1.3g=0g=1/3g=2/3g=1 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Average FNDrop rate (d)k=2.0g=0g=1/3g=2/3g=1 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Average FPDrop rate (d)k=2.0g=0g=1/3g=2/3g=174