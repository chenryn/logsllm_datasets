Further, our inability to examine the passwords directly masks
some aspects of the data. We rely on an algorithm to learn and
exploit common strings and structures within the CMU passwords,
but we cannot know which patterns it exploits. It is possible that the
data contains commonalities a determined attacker could exploit,
but which the algorithm is not sophisticated enough to recognize.
3.3 Supplemental data sets
We use guess numbers generated using a modiﬁed version of
the Weir algorithm to measure password strength, applying the ap-
proach of Kelley et al. [30]. Guess numbers depend on the amount
175and quality of training data available to the guessing algorithm.
We use training data that includes publicly available dictionaries
(including the Google web corpus and the Openwall cracking dic-
tionary); leaked password sets that were previously made public
(including MySpace and RockYou), and data from online studies
(using Amazon’s Mechanical Turk service, or MTurk) in which
participants created passwords under various conditions. For some
tests, the algorithm is also trained on a subset of the CMU pass-
words (training and test sets are always kept disjoint for a given
experiment).
We also compare the CMU passwords with 11 other data sets
from various sources, as follows:
MTsim. 1,000 passwords collected from an MTurk experiment de-
signed to simulate CMU password creation as closely as possible,
both in policy requirements and in the website design.
MTbasic8. 1,000 passwords collected from MTurk [30]. The only
requirement is a minimum length of 8 characters.
MTbasic16. 1,000 passwords collected from MTurk [30]. The
only requirement is a minimum length of 16 characters.
MTdictionary8. 1,000 passwords collected from MTurk [30]. Min-
imum length 8. Discarding non-alphabetic characters, the password
cannot be found in the 2.9-million-word free Openwall dictionary,1
an order of magnitude larger than the dictionary used by CMU.
MTcomp8. 1,000 passwords collected from MTurk [30]. Same as
MTdictionary8, but also requiring at least one lowercase letter, one
uppercase letter, one digit, and one symbol.
SVcomp8. 470 self-reported responses to a previously published
survey of CMU users [47].
We also compared our results with data from ﬁve real websites.
In each case, we use a subset of the website passwords that meet
CMU’s requirements. Where more then enough conforming pass-
words were available, we draw the test set at random. Three of
these leaked sets were leaked in plaintext; the other two come from
the subset of the original leak that was successfully cracked.
RYcomp8. 1,000 plaintext passwords from RockYou (42,496 con-
forming, 32,603,144 total).
Ycomp8. 1,000 plaintext passwords from Yahoo! Voices (2,693
conforming, 453,488 total).
CSDNcomp8. 1,000 plaintext passwords from the Chinese Devel-
oper Network (12,455 conforming, 6,428,285 total).
SFcomp8. 1,000 cracked passwords from Strategic Forecasting,
Inc., also known as Stratfor. (8,357 conforming, 804,034 total).
Gcomp8. 896 cracked passwords from Gawker (896 conforming,
694,064 total). All eight characters long.
Hereafter, we refer to all the leaked password sets, MTcomp8,
MTsim, SVcomp8, and the real university passwords (CMUactive
and CMUinactive) collectively as the comprehensive-policy pass-
words, as each includes four character classes and a dictionary
check.
3.4 Experimental validity
CMU’s complex password policy meets guidelines established
by the InCommon Federation, which provides “a common frame-
work for trusted shared management of access to on-line resources”2
for educational and research institutions across the United States.
As such, it is representative of similarly complex policies at other
institutions. InCommon relies on NIST guidelines, which inﬂuence
security standards at organizations across the United States [8].
We believe our results are reasonably representative of medium-
sized research universities in the United States, and may be applica-
1http://download.openwall.net/pub/wordlists/
2http://www.incommon.org
ble to universities of other sizes or to other organizations with sim-
ilar demographic proﬁles. CMU personnel represent a broad cross-
section of ages, ethnic backgrounds, and nationalities, as well as a
broad spectrum of geographic regions of origin within the United
States. Although the sample is broad, its proportions do not match
the general population.
Overall, the sample is considerably wealthier and more educated
than the general American population. Most members of the sam-
ple currently live and work in or near Pittsburgh, where the main
campus is located, but a fraction do live and work at other locations
around the United States and internationally. We include some de-
mographic factors as covariates, but many were not available from
the university’s personnel databases.
Compared to existing password research, we have signiﬁcantly
more knowledge of demographic factors than is available for most
sets collected in the wild, and the CMU population overall is more
diverse than the typical group of undergraduates used in lab studies.
As a result, we believe that our results, if considered judiciously,
can be applied to broader populations.
The guessing algorithm we use to measure password strength
may not be optimal. While the algorithm has been used success-
fully in the past [30, 55], a more sophisticated algorithm might
guess passwords more efﬁciently and produce different results.
4. UNDERSTANDING CMU PASSWORDS
In this section, we correlate the password strength of subsets of
the CMU passwords with various demographic, password-compo-
sition, behavioral, and sentiment factors.
4.1 Analysis approach
For our correlation analysis in this section, we use guess numbers
as our metric of password strength. We use a guess calculator for
a modiﬁed version of Weir’s guessing algorithm (see Section 2).
We separate the CMUactive passwords into three folds for cross-
validation, with each fold used once as a test set. The guess calcu-
lator is trained on the other two folds of CMUactive, all of CMUin-
active, and a Public training set consisting of public and leaked
password data. The Public set is composed of passwords from
the MySpace, RockYou, Yahoo, CSDN, Stratfor, Gawker, and paid
Openwall sets, as well as strings from the standard Unix dictionary
and an inﬂection list3 that includes various grammatical transfor-
mations. The set of alphabetic strings also includes unigrams from
the Google Web N-Gram corpus.4 The Public set was pruned so
that only passwords containing at least eight characters and four
character classes would be guessed.
Because of limitations in processing power, guess numbers can
only be computed up to a given threshold. For each password, we
either calculate a speciﬁc guess number, conclude that the pass-
word would have a guess number greater than the threshold n, or
conclude that the password cannot be guessed given the current
training data. The guessing threshold depends on the content of the
training data as well as the experimental setup; experiments with
higher guessing thresholds take longer to process. For this analy-
sis, the guessing threshold is approximately 3.8 E14, or more than
380 trillion guesses; on our hardware, calculating guess numbers
for each fold (about 8,000 passwords) takes about a day.
Once guess numbers are calculated for all CMUactive passwords,
they are joined with data from the other university sources by match-
ing hashed user IDs. Regressions are then performed on the re-
3http://wordlist.sourceforge.net
4http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2006T13
176sulting table to ﬁnd correlations between guess numbers and other
factors. Some of the factors we measure do not have useful values
for all users, so we perform three separate regression analyses on
appropriate subsets of users. These subsets are described in Sec-
tions 4.1.1-4.1.4.
The main regression technique we use is Cox regression, a tech-
nique adapted from the survival analysis literature, which typically
studies factors that affect mortality [11]. The outcome variable is
a pair of values: an observed state and the time of observation. If
a password is guessed before the threshold, we mark the observed
state as “dead” and the guess number as the “time of death.” Oth-
erwise, the observed state is “alive,” and the guess threshold is the
last time of observation. In the parlance of survival analysis, this is
called right-censored data.
Using Cox regression, we are able to incorporate all the avail-
able data, over the range of guess numbers. This is an improvement
over prior work, in which guessing success is examined at arbitrary
points, such as the percentage guessed after a certain number of
guesses [30,56] or the amount of effort required to crack some per-
centage of passwords [3]. As with ordinary linear regression mod-
els, Cox regression estimates a linear term for each factor. This
assumes that factors affecting the probability of survival have a lin-
ear effect over the guessing range; this is a common simpliﬁcation
often used to represent factors which might, in reality, have non-
linear effects.
To counteract overﬁtting, we use the standard backward elimi-
nation technique, removing one factor from the model at a time,
until we minimize the Bayesian information criterion (BIC) [42].
We only report the ﬁnal model. Before the analysis, we centered
all the quantitative variables around zero by subtracting the mean
from each value; this standard technique makes the regression out-
put easier to interpret [15].
In addition to the main analysis, we check for interactions be-
tween factors — that is, factors whose effects are not independent
— by performing the above survival analysis with all two-way in-
teraction terms, again using the BIC for model selection. Since this
project required all code to be reviewed before the passwords were
analyzed, and survival analysis is a novel approach to measuring
password strength, we supplemented the Cox regressions with lo-
gistic regressions on a binary outcome: guessed before the cutoff
threshold (success) or not (failure). We found that both regression
approaches generally agreed.
We next describe the subsets of data used in this analysis and the
factors included in the corresponding regressions.
4.1.1 Model 1: All personnel
This data set contains all current users with complete demo-
graphic information (17,088). We consider the following factors:
Gender. Male or female.
Age. Birth year as recorded in the personnel database.
Status. Undergraduate, graduate student, non-degree-seeking
student, faculty, or staff. As with all regression techniques, cate-
gorical factors are represented as a set of binary factors, with only
the appropriate category (e.g., “undergraduate” for an undergrad-
uate student) coded as true for each user. One arbitrarily selected
category, known as the baseline, is left out: users belonging to that
category are coded false for every binary factor. For this factor,
faculty is the baseline.
College. Personnel are divided into eight colleges within the
university, including a catch-all “other” category.
Location. Because the vast majority of university personnel are
based at the main campus, we use only two groups: main campus
and other location. This unfortunately groups locations from sev-
eral different areas of the world (e.g., Silicon Valley, Qatar, Aus-
tralia) together, despite possibly important cultural and organiza-
tional differences.
4.1.2 Model 2: All personnel plus composition
This data set contains the 17,088 users from Model 1. We con-
sider Models 1 and 2 separately because all the factors in Model 1
are included in each subsequent model, providing a baseline. For
Model 2, we add the following factors related to the composition
of passwords:
Number of digits, lowercase, uppercase, and symbols. Four
separate factors that measure the number of occurrences of each
type of character in the user’s password.
Location of digits, uppercase, and symbols. For each of the
three special character classes, we identiﬁed the location of charac-
ters of this type in the user’s password. This location is categorized
as either all at the beginning; all at the end; all in a single group in
the middle of the password; or spread out in some other pattern.
4.1.3 Model 3: Personnel with stable passwords
This data set includes the 12,175 users who did not change their
passwords throughout the log measurement period. This allows us
to conclude that the password for which we calculate a guess num-
ber was in use during all behavioral measurements. Factors include
everything from Model 1, plus the following factors extracted from
the authentication service logs:
Login count. The number of times this user successfully logged
in during the measurement period. We hypothesized that users who
log in more often might use stronger passwords, either because they
are conﬁdent that repetition will allow them to remember, or be-
cause they simply value the account more highly.
Median interlogin time. The median time elapsed between each
pair of this user’s successive web logins. We hypothesized that
users who go a shorter time between logins might be able to choose
and remember more complex (stronger) passwords.
Password and username failures. The number of login fail-
ures attributable to an incorrect password or username (treated sep-
arately), normalized by login count. To avoid counting potentially
malicious guessing, we count only failures during what was even-
tually a successful login session. We hypothesized that users with
stronger passwords might ﬁnd them more difﬁcult to type, leading
to more errors; alternatively, users who know they have difﬁculty
typing might choose weaker passwords as a mitigation.
Median time elapsed during authentication. The median time
elapsed between when the user arrives at the login page and when
she successfully logs in, taken from server log data. This measure
is imperfect; long elapsed times may represent users who open an
authentication session in their browser and then ignore it for a long
time before logging in, and different authentication servers may not
have globally consistent timestamps. We hypothesized that users
who take longer to log in might have passwords that are more dif-
ﬁcult to remember or type in.
Wired login rate. The number of successful logins originating
from an IP address associated with the main campus wired network,
normalized by login count. This excludes logins made on the uni-
versity’s wireless network and those made from other campuses, as
well as remote logins (such as from a user’s home). We hypothe-
sized that users who access their accounts only from organizational
infrastructure on the main campus might think about passwords dif-
ferently from those who frequently connect remotely or via wire-
less, since they are connecting over a more trusted medium. Unfor-
tunately, we were unable to distinguish mobile devices like phones
177or tablets from other wireless devices like laptops using the avail-
able log data.
Non-web authentication rate. The number of successful logins
that do not correspond to web authentication events, normalized by
login count. Because of incomplete log data, this value is only an
approximation. We hypothesized that users who routinely access
their accounts via an email program or other tools that store pass-
words might choose stronger passwords.
Personnel who did not log in at least twice are excluded, as they
have no interlogin time.
4.1.4 Model 4: Survey participants
This data set includes 694 users who completed the survey after
changing their passwords. This group is disjoint from the stable
passwords group, as all members of this group have changed their
passwords at least once since the start of the logging period.
The survey sample is a subset of the overall university popula-
tion. New users, who must all change their system-assigned starter
passwords, made up 22% of the sample (164). The sample is of
course also biased toward people who are willing to take a survey
in exchange for an entry in a drawing for an Amazon.com gift card.
All data is self-reported.
We include all the factors from Model 1, plus the following:
Method of creation. Each user’s selections from a list includ-
ing: reused a password from another account; added or substituted
numbers or symbols within a dictionary word; used the name of
someone or something; and other choices. Participants were asked
to choose all that apply; each possible answer was represented as a
factor and coded true if the user selected it or false otherwise.
Reason for change. Each user’s choice from among changing
the default starter password, feeling the password was too old, re-
setting a forgotten password, and suspicion of a security breach.
Participants were allowed to choose only one option. We hypoth-
esized that those who forget their passwords might select simpler
passwords to reduce cognitive load.
Storage. True if the user indicated she stores her password, ei-
ther electronically or on paper; otherwise false. Prior work sug-
gests that users who are asked to make more complex passwords
are more likely to write them down [25]. Self-reporting for this cat-
egory may be an undercount, as users who write down passwords
are contravening common (although not necessarily correct [46])
advice and may be embarrassed to admit to it.
Sentiment during creation. Three factors, coded as true for
users who agree or strongly agree (on a ﬁve-point Likert scale) that
it was difﬁcult, annoying, or fun to create a password conforming
to the university’s policy. Users who indicated disagreement or
neutrality were coded false for that factor. We hypothesized that
users who struggle to create a conforming password might be more
likely to give up and choose something simple out of frustration.
4.2 Results
We ﬁnd interesting correlations between password strength and
other factors across each of the subpopulations we investigate. We
describe the results for each model separately. Note that while we
interpret the results for the reader, this was not a controlled exper-
iment with random assignment, so we make no formal attempt at
a causal analysis — any observed effect may involve confounds,
and many independent variables in our data set are correlated with
other independent variables.
4.2.1 Model 1: All personnel
For these users, we ﬁnd password strength to be correlated with
gender and college. Men have slightly stronger passwords than
Factor
gender (male)
engineering
humanities
public policy
science
other
computer science
business
Coef.
-0.085
-0.218
-0.106
0.081
-0.286
-0.102
-0.393
0.211
Exp(coef)
0.918
0.804
0.899
1.084
0.751
0.903
0.675
1.235
SE
p-value
0.023 <0.001
0.042 <0.001
0.028
0.048
0.112†