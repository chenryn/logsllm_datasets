and computation devices and hence, better scalability. Third, in
addition to RPC support,4 our networking library also supports
the distributed communication collectives of PyTorch.5 In the
latter case, our implementation automatically chooses the best
communication backend between nccl and gloo to allow GPU-
to-GPU communication whenever possible. This is a plus
compared to the RPC–based implementation as the latter does
not allow communication over GPUs.
C. SIMT Median Function
Our implementation of the median function on CPU is quite
straightforward: each of the m ≥ 1 available cores processes a
continuous share of n/m coordinates. Then each core applies,
for each coordinate of its share, introselect (or equivalent) by
calling the standard C++ std::nth_element.
Nevertheless, even embarrassingly parallel algorithms like
median would not necessarily beneﬁt from running on GPG-
PUs (General–Purpose computing on Graphics Processing
Unit). That is because modern GPGPUs, to achieve parallel
execution on many threads while limiting instruction fetch
costs, batch threads into groups of, e.g., 32 threads that
execute the same instruction. Algorithms like introselect [38]
are branch–intensive, with possibly many instructions executed
in each branch, and so, fails to scale on GPUs.
Reminiscent of [29], our implementation of median is built
around a primitive that orders 3 elements without branching,
by the use of the selection instruction, which converts a predi-
cate to an integer value. Let v be the table of size 3 to reorder
by increasing values. Thanks to the selection instruction, we
can compute c[3] = {v[0] > v[1], v[0] > v[2],
v[1] > v[2]}, where a > b is 1 if a > b else 0. Then
the indices i[2] = {
(1+c[0]+2*c[1]+c[2]-(c[1]⊕c[2]))/2,
(4-c[0]-2*c[1]-c[2]+(c[0]⊕c[1]))/2 },
4PyTorch fully supports gRPC communication only in v1.6.
5https://pytorch.org/docs/stable/distributed.html
and ﬁnally the reordered values w of v is: w[3] = {
v[i[0]], v[3-i[0]-i[1]], v[i[1]] }.
Using this reordering primitive, we manage to implement an
efﬁcient version of median with minimal branching.
D. Memory Management
We describe here a few tricks that we use to minimize mem-
ory footprint and reduce copying data among the CPU and the
GPU memory. First, whenever possible, we pin training data to
memory. This impacts the time it takes to copy data to the GPU
memory for computing gradient estimates on workers. On the
other hand, we do not pin the test set to memory as using it
is usually much less frequent than the training data. Second,
we pin model weights in the parameter server main memory
as gRPC currently does not allow communicating values that
reside on the GPU memory. Given that the model weights are
communicated in each round, we never copy such weights to
the GPU memory (except when testing the accuracy). Yet, we
store the model on workers on the GPU (or multiple GPUs
whenever possible) so as to accelerate gradient computation.
We carefully optimize memory usage by our GARs. For ex-
ample, aggregating gradients may require multiple iterations,
calculating some distance-based scores for each of them in
each iteration, e.g., with Multi-Krum or Bulyan. We then cache
the results of each of these iterations (in the CPU or GPU
memory) and hence, remove redundant computations. Besides,
we reduce the memory cost by allocating space only for one
iteration along with the intermediate selected gradients.
V. APPLICATIONS
In this section, we show how one can use GARFIELD to
build Byzantine ML applications. We give three examples
(depicted in Figure 2) that span different architectures for
Byzantine–resilient algorithms.
A. Single Server, Multiple Workers (SSMW)
Our ﬁrst application represents the standard setup that was
vastly studied in the last few years, e.g., in [11], [8], [17]. Such
setup uses the vanilla parameter server architecture [34] (see
Figure 2a), yet with one crucial difference: instead of aggre-
gating the workers’ updates by averaging them, the server uses
a statistically–robust GAR (see Section III-A) for aggregation.
Such setup usually assumes synchronous network, i.e., there
is an upper bound on the time it takes the workers to compute
gradients and to reply to the server.
Listing 1 shows how to build such setup, using GARFIELD,
in a few lines of code.
Listing 1: Implementing SSMW setup with GARFIELD.
1 from garfield import Server, GARs
2 # parsing training arguments
3 ps = Server(...) #args omitted for brevity
4 for i in range(num_iter):
5
6
7
8
9
gradients = ps.get_gradients(i, nw)
aggr_grad = gar(gradients=gradients, f=fw)
ps.update_model(aggr_grad)
if i%comp_acc_freq == 0:
acc = ps.compute_accuracy()
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:24:13 UTC from IEEE Xplore.  Restrictions apply. 
43
(cid:38)(cid:72)(cid:81)(cid:87)(cid:85)(cid:68)(cid:79)(cid:3)(cid:54)(cid:72)(cid:85)(cid:89)(cid:72)(cid:85)
(cid:54)(cid:72)(cid:85)(cid:89)(cid:72)(cid:85)(cid:86)
(cid:39)(cid:72)(cid:89)(cid:76)(cid:70)(cid:72)(cid:86)
(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:86)
(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)(cid:86)
(a) Single server, multiple workers
(b) Multiple server, multiple workers
(c) Decentralized learning
Fig. 2: Examples of applications that can be built using GARFIELD.
replicas to ensure consistency among servers’ parameters and
hence, convergence. In such a step, each server ﬁrst fetches
the updated model from a few other servers (whose number
is given as an argument to get models() method). Then, it
aggregates the collected models and store the result as the
new model state.
C. Decentralized Learning
Our third application considers the Byzantine ML problem
in decentralized settings as in e.g., [57], [56], [19]. Decentral-
ized learning is different from the previous two use cases as it
does not use the parameter server architecture (see Figure 2c),
i.e., communication is done in a peer–to–peer fashion. Such
setup is mainly useful when data is sensitive and should be
kept private. Notably, it addresses settings where data is not
identically distributed on the contributing machines.
Listing 3 shows how to build such an application, allowing
for a decentralized setup and multiple communication steps
per training iteration, which are mainly required to keep weak
consistency among models of the contributing nodes.
Implementing decentralized learning with
gradients = ps.get_gradients(i, n-f)
aggr_grad = gar(gradients=gradients, f=f)
if non_iid:
aggr_grad = contract(...)
Listing 3:
GARFIELD.
1 from garfield import Server, Worker, GARs
2 # parsing training arguments
3 wrk = Worker(...) #args omitted for brevity
4 ps = Server(...) #args omitted for brevity
5 for i in range(num_iter):
6
7
8
9
10
11
12
13
14
15
16
17 def contract(...):
18
19
20
21
22
ps.update_model(aggr_grad)
models = ps.get_models(n-f)
aggr_models = gar(gradients=models,f=f)
ps.write_model(aggr_models)
if i%comp_acc_freq == 0:
ps.latest_aggr_grad = aggr_grad
aggr_grads = ps.get_aggr_grads(n-f)
aggr_grad = gar(gradients=aggr_grads, f=f)
acc = ps.compute_accuracy()
for _ in range(steps):
This depicts the code on the parameter server side, given the
passiveness of workers in our design. First, the server object
is initiated (line 3) with the appropriate parameters. Then, the
training loop (lines 4–7) runs as much as the user speciﬁes.
In each training iteration, the server ﬁrst asks the workers
to compute gradient estimates using the current model state.
Note that, the second argument in get gradients() method
speciﬁes the number of replies the server should wait for
before continuing the iteration. The server then applies some
GAR on the received gradients and then updates the model
using the aggregated gradient. From now and then, the server
computes the accuracy of the current model state (lines 8–9).
Notably, AggregaThor ofﬁcial code base [1] (which imple-
ments this setup) is in the order of thousands of lines of code.
B. Multiple Server, Multiple Workers (MSMW)
Our second application extends the ﬁrst one by considering
multiple servers and multiple workers. This setup was con-
sidered recently [20] to tolerate Byzantine servers as well as
Byzantine workers. Such a setup requires replicating the server
on multiple machines (see Figure 2b). It can also accommodate
asynchronous networks in the sense that it does not assume any
upper bound on the computation nor communication delays.
Listing 2 shows how to build MSMW setup, allowing
multiple server replicas and asynchronous communication.
Listing 2: Implementing MSMW setup with GARFIELD.
1 from garfield import Server, GARs
2 # parsing training arguments
3 ps = Server(...) #args omitted for brevity
4 for i in range(num_iter):
5
6
7
8
9
10
11
12
gradients = ps.get_gradients(i, nw-fw)
aggr_grad = gar(gradients=gradients, f=fw)
ps.update_model(aggr_grad)
models = ps.get_models(nps-fps)
aggr_models = gar(gradients=models,f=fps)
ps.write_model(aggr_models)
if i%comp_acc_freq == 0:
acc = ps.compute_accuracy()
We focus here again on the server side. Lines 1–7 are
very similar to those in Listing 1 (except for the number
of expected gradients to collect in line 5). Lines 8–10 show
the additional communication step required among the server
return aggr_grad
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:24:13 UTC from IEEE Xplore.  Restrictions apply. 
44
We highlight here the two main differences with the pre-
vious examples. First, each node creates both a Server and a
Worker objects (lines 3–4). Second, there is a multi–round
step (lines 17–22) in each iteration to contract models on
correct machines, especially when the data is not identically
distributed on them. The goal of this step is to force the model
states on all machines to get closer to each other.
VI. PERFORMANCE EVALUATION
We ﬁrst describe the settings we use and the baselines we
consider. We then show micro-benchmarks for the GARs we
implemented, followed by large–scale experiments, evaluating
GARFIELD using the applications given in the previous section.
A. Setting
a) Testbed: Our experimental platform is Grid5000 [3].
For the experiments deployed with CPUs, we employ nodes
from the same cluster, each having 2 CPUs (Intel Xeon E5-
2630 v4) with 10 cores, 256 GiB RAM and 2×10 Gbps
Ethernet. For the GPU-based experiments, we employ nodes
from two clusters (due to the limited number of nodes in one
cluster); nodes in different clusters have different speciﬁca-
tions. Each node has 2 identical GPUs.
b) Metrics: We use the following standard metrics:
Accuracy. This measures the top-1 cross-accuracy: the fraction
of correct predictions among all the predictions using the test
set; this shows the quality of the learned model over time.
Throughput. This quantiﬁes the number of updates that the
system processes per second. For deployments that employ
multiple parameter servers, we report the highest throughput,
which corresponds to the fastest correct machine.
c) Application: We consider the 3 applications discussed
in Section V. Yet, in some experiments, we focus more on the
setup with multiple servers and multiple workers (MSMW)
as it gives the ﬂexibility to test with different number of
Byzantine servers and workers. We consider two variants of
MSMW: the ﬁrst uses Bulyan [21] to aggregate gradients, and
hence achieves Byzantine resilience in high dimensions while
assuming network asynchrony. The second one uses Multi–
Krum [17] to aggregate gradients while assuming network
synchrony. Unless otherwise stated, we use our TensorFlow
version with the ﬁrst variant and PyTorch for the second one6.
We consider image classiﬁcation due to its wide adoption
as a benchmark for distributed ML systems [15], [6]. We use
MNIST [4] and CIFAR-10 [2] datasets. MNIST is a dataset of
handwritten digits with 70,000 28 × 28 images in 10 classes.
CIFAR-10 consists of 60,000 32 × 32 colour images in 10
classes. Table I presents the models we use for evaluation.
d) Setup: Unless otherwise stated, we use the following
default setup. For TensorFlow experiments, we employ 18
workers, out of them 3 could be faulty (nw = 18, fw = 3)
and 6 servers, 1 could be faulty (nps = 6, fps = 1). Note
that in decentralized learning experiments, we do not use any
servers (i.e., workers communicate in a peer–to–peer fashion).
6Few experiments, e.g., experiments with MDA as a GAR, TensorFlow with
GPUs, and PyTorch with CPUs, were omitted for space limitations.
45
TABLE I: Models used to evaluate GARFIELD.
# parameters
Size (MB)
Model
MNIST CNN
CifarNet
Inception
ResNet-50
ResNet-200
VGG-19
79510
1756426
5602874
23539850
62697610
128807306
0.3
6.7
21.4
89.8
239.2
491.4
We employ a batch size of 32 at each worker, leading to an
effective batch size of 480 in the normal case. For PyTorch
experiments, we use 10 workers, with 3 Byzantines, and 3
servers, with only 1 Byzantine. We use a batch size of 100 at
each worker. We repeated all the experiments multiple times
and found that the error-bars are always very small and hence,
omitted for better readability.
B. Baselines
To the best of our knowledge, GARFIELD is the ﬁrst library
to accommodate both Byzantine servers and workers. We
chose the following baselines to compare GARFIELD to.
a) Vanilla baseline: This is the vanilla deployment of
TensorFlow or PyTorch. Such deployment fails to tolerate any
Byzantine behavior whatsoever. Comparing GARFIELD against
this baseline quantiﬁes the overhead of Byzantine resilience.
b) AggregaThor [17]: This is the only existing scalable
ML system that achieves Byzantine resilience, yet only for
Byzantine workers. It is built on TensorFlow and supports
training only on CPUs. AggregaThor uses one central, trusted
server while tolerating Byzantine workers (i.e., uses SSMW
setup), and it considers synchronous networks. For a fair
comparison with our GARFIELD–based systems, we use the
same GAR for both deployments. Thus, comparing with this
baseline quantiﬁes the overhead of using our object–oriented
design and the communication layer we provide.
c) Crash–tolerant protocol: We implement a strawman
approach to tolerate crash failures, assuming synchronous
communication, using GARFIELD components. As worker
crashes do not affect the learning convergence eventually, we
only tolerate server crashes, by replicating the server. Server
replicas get the updates from all workers and average them in
each iteration, but workers contact only one of these replicas,
i.e., the primary, to get the updated model. In the case of
primary crash (signaled by a timeout), workers contact the
next server, marking it as the new primary. The new primary
sends its view of the model to all workers so as to inform them
about the change. The model sent by the new primary could
be outdated compared to the model of the crashed primary
(due to missing some updates). This is still ﬁne and learning
will converge eventually [53], given that nps ≥ fps + 1, where
nps is the total number of replicas, and fps is the maximum
number of crashing nodes, i.e., servers. Thus, this deployment
guarantees eventual convergence without any guarantees on
throughput nor convergence rate. Some ML systems already
use Paxos [31] for crash fault tolerance [15], [34]. However,
our strawman algorithm, we believe, gives strictly weaker
guarantees (in terms of consistency of model state among
replicas), and hence has a higher throughput than Paxos.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:24:13 UTC from IEEE Xplore.  Restrictions apply. 
)
c
e
s
(
e
m
i
t
n
o
i
t
a