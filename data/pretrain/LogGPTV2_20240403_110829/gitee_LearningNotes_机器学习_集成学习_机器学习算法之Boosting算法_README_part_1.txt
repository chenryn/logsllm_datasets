# 机器学习算法之Boosting算法
原文链接：
- https://www.biaodianfu.com/lightgbm.html
- https://www.biaodianfu.com/xgboost.html
- https://www.biaodianfu.com/boosting.html
## 机器学习分类
- 监督学习
  在监督学习中，训练数据由输入和期望的输出组成，然后对非训练数据进行预测输出，也就是找出输入x与输出y之间的函数关系F：y = F(x)。根据输出的精确特性又可以分为分类和回归。分类和回归的区别在于输出变量的类型。
  - 定量输出称为回归，或者说是连续变量预测。
  - 定性输出称为分类，或者说是离散变量预测。
  举个例子：
  - 预测明天的气温是多少度，这是一个回归任务。
  - 预测明天是阴、晴还是雨，就是一个分类任务。
  ![image-20200202102214816](images/image-20200202102214816.png)
- 无监督学习
  无监督学习是一种机器学习方法，用于发现数据中的模式。输入无监督算法的数据都没有标签，也就是只为算法提供了输入变量(X)而没有对应的输出变量。在无监督学习中，算法需要自行寻找数据中的有趣结构。
  无监督学习问题可以有以下三种类型：
  - 关联：发现目录中项目共现的概率。其广泛应用于“购物篮分析”。例如，如果一个顾客购买了面包，他会有80% 的概率也购买鸡蛋。
  - 聚类：将样本分组，这样，同一聚类中的物体与来自另一聚类的物体相比，相互之间会更加类似。
  - 降维：正如其含义，降维指减少一个数据集的变量数量，同时保证还能传达重要信息。降维可以通过特征抽取方法和特征选择方法完成。特征选择方法会选择初始变量的子集。特征抽取方法执行从高维度空间到低维度空间的数据转换。例如，PCA算法就是一种特征抽取方式。
## 集成学习
集成学习是通过训练弱干个弱学习器，并通过一定的结合策略，从而形成一个强学习器。有时也被称为多分类器系统（multi-classifier system）、基于委员会的学习（committee-based learning）等。
![image-20200202102307020](images/image-20200202102307020.png)
集成学习先产生一组“个体学习器”（individual learner），再用某种策略将它们结合起来。通常来说，很多现有的学习算法都足以从训练数据中产生一个个体学习器。一般来说，我们会将这种由个体学习器集成的算法分为两类
- 同质（homogeneous）的，即集成中仅包含同种类型的一个体学习器，像“决策树集成”中就仅包含决策树，“神经网络集成”中就全是神经网络。同质集成中的个体学习器又称为基学习器（base learner），相应的学习算法也被称为基学习算法（base learning algorithm）。
- 异质（heterogenous）的，相对同质，异质集成中的个体学习其就是由不同的学习算法生成的，这是，个体学习器就被称为组件学习器（component learner）
其中用的比较多的是同质学习器。同质学习器按照个体学习器之间是否存在依赖关系可以分为两类：
- 第一个是个体学习器之间存在强依赖关系，一系列个体学习器基本都需要串行生成，代表算法是boosting系列算法；
- 第二个是个体学习器之间不存在强依赖关系，一系列个体学习器可以并行生成，代表算法是bagging和随机森林（Random Forest）系列算法。
![image-20200202103508419](images/image-20200202103508419.png)
![image-20200202103608795](images/image-20200202103608795.png)
现在我们来简单评估一下集成学习方法的性能：考虑一个简单的例子，在二分类任务中，假定三个分类器在三个测试样本上的表现如下图所示，其中√代表正确，×代表分类错误，集成学习的结果则是由投票法（voting）决出，即“少数服从多数”：
![image-20200202103841496](images/image-20200202103841496.png)
在a图中每个分类器只有66.6%的精度的时候，集成学习达到了100%的精度；在b图中，三个分类器相同导致集成性能没有提高；c图中由于每个分类器的精度只有33.3%导致集成学习的效果变得更糟。由此我们可以看出来，集成学习中对个体学习器的要求应该是“好而不同”，即既满足准确性，又满足多样性（diversity），也即是说，学习器既不能太坏，而且学习器与学习器之间也要有差异。
随着集成中个体分类器数目T的增大，集成的错误率将指数级下降从而最终趋于0（这里还有一个前置条件就是个体分类器的错误率不能大于50%）。但我们曾假设各个分类器之间的错误率是相互独立的，而实际上再同一个任务中个体学习器视为解决同一个问题训练出来的，这也就意味着它们之间显然不可能相互独立。换句话说，个体学习器的“准确性”和“多样性”本身也是存在冲突的。一般的，准确性很高之后，若要增加多样性就需要准确性做出一定的牺牲。因此，如何产生“好而不同”的个体学习器，便是集成学习研究的核心
目前，有三种常见的集成学习框架：bagging，boosting和stacking。国内，南京大学的周志华教授对集成学习有很深入的研究，其在09年发表的一篇概述性论文[《](http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf)[Ensemble Learning》](http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf)对这三种集成学习框架有了明确的定义，概括如下：
Bagging：从训练集从进行子抽样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合产生最终的预测结果：
![image-20200202104637983](images/image-20200202104637983.png)
Boosting：训练过程为阶梯状，基模型按次序一一进行训练（实现上可以做到并行），基模型的训练集按照某种策略每次都进行一定的转化。对所有基模型预测的结果进行线性综合产生最终的预测结果：
![image-20200202111511834](images/image-20200202111511834.png)
Stacking：将训练好的所有基模型对训练基进行预测，第j个基模型对第i个训练样本的预测值将作为新的训练集中第i个样本的第j个特征值，最后基于新的训练集进行训练。同理，预测的过程也要先经过所有基模型的预测形成新的测试集，最后再对测试集进行预测
![image-20200202115953390](images/image-20200202115953390.png)
有了这些基本概念之后，直觉将告诉我们，由于不再是单一的模型进行预测，所以模型有了“集思广益”的能力，也就不容易产生过拟合现象。但是，直觉是不可靠的，接下来我们将从模型的偏差和方差入手，彻底搞清楚这一问题。
## 什么是Boosting？
Boosting 是个非常强大的学习方法, 它也是一个监督的分类学习方法。它组合许多“弱”分类器来产生一个强大的分类器组。一个弱分类器的性能只是比随机选择好一点，因此它可以被设计的非常简单并且不会有太大的计算花费。将很多弱分类器结合起来组成一个集成的类似于SVM或者神经网络的强分类器。
现在我们知道boosting是组合多个弱学习器形成一个强学习器，那么一个自然而然的问题就是“boosting如何确定弱的规则？”为了发现弱的规则，我们可以应用不同分配下的基础的（机器）学习算法，每个算法都会生成一个弱规则，这是一个迭代的过程，多次迭代后，Boosting算法可以将它们组合成一个强大的决策规则。为了选择正确的分配方式，可以遵循下面几个步骤：