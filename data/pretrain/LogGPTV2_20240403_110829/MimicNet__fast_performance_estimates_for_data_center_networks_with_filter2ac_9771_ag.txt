tion space that we must explore, we create two separate testing
frameworks: one for ingress traffic and one for egress traffic. These
frameworks isolate the effect of each direction so that, when train-
ing an ingress model, egress traffic travels through a full-fidelity
network, and vice versa for an egress model.
The testing frameworks resemble the structure of the small-scale
simulation of Section 5.1. Like the original simulation, two clusters
are set up to communicate with one another. One is kept at full-
fidelity, while the other is converted to use a specialized testing
cluster.
See Figure 15 for diagrams of the specialized testing clusters.
Isolation of the two directions depends on the model being tested.
Consider, for instance, the ingress testing cluster shown in Fig-
ure 15a. Traffic ingressing the cluster flows through the model
before the hosts receive it, and traffic egressing the cluster flows
through the full-fidelity network. Unfortunately, only feeding the
egress traffic into the full-fidelity component would result in in-
accurate results as egress traffic contends with local traffic, which
contends in turn with ingress traffic. In other words, the congestion
of the full-fidelity network depends on every packet in the full-
fidelity trace. To account for this, we duplicate ingress packets and
continue to feed them and local traffic into the full-fidelity cluster,
(a) Ingress
(b) Egress
Figure 15: Hybrid Mimic clusters for use in separate mod-
eling tuning/debugging. These include both an ML model
(white box) and a full-fidelity network. Depending on the
model tested, ingress, egress, and internal traffic are routed
through the parallel networks. Flat-headed arrows indicate
that all traffic of that type is dropped.
(a) Training loss descent
(b) Training latency
Figure 16: The impact of the window size on modeling accu-
racy and speed. The BDP of the network is around 12 pack-
ets. More packets in the window help loss descent (through
epochs), but can make the training slower (training latency
is per batch in Python).
APPENDIX
Appendices are supporting material that has not been peer-reviewed.
A RELAXATIONS TO RESTRICTIONS
In Section 4.2, we listed a series of restrictions that MimicNet uses
to promote accuracy and speed, even as we scale the simulation
by composing increasing numbers of Mimics. We note that not all
of the restrictions are necessarily fundamental. In this section, we
briefly speculate on possible techniques to relax the restrictions.
Topology and routing. In principle, deep learning models could
learn the behavior of arbitrary network topologies, and even incor-
porate the effects of failures and more exotic routing policies, e.g.,
those used in optical circuit-switched networks. This would require
a unified model instead of the ingress/egress/routing models that
we currently use, which may slow down the training and execution
of the system. The only piece that would be difficult to relax is the
implicit requirement that the network be decomposed in a way that
small-scale results are representative of a subset of the larger scale
simulation. Random networks, would therefore be challenging for
the MimicNet approach; however, heterogeneous but structured
networks may be possible, as described below.
Traffic patterns. The expectations of compatible traffic generators
in MimicNet are carefully selected, and thus, would be difficult to
301
‚Ä¶‚Ä¶SelectorDuplicatorSelector‚Ä¶‚Ä¶Duplicator 0.01 0.02 0.03 0.04 1 2 3 4 5 6 7 8 9 10Training Loss1 packet2 packets5 packets10 packets12 packets20 packets 0 0.01 0.02Training Latency (s)1 packet2 packets5 packets10 packets12 packets20 packetsSIGCOMM ‚Äô21, August 23‚Äì27, 2021, Virtual Event, USA
Q. Zhang et al.
(a) Validation loss descent
(b) Inference latency
Figure 17: The impact of the window size in the LSTM model
on modeling accuracy and speed. Larger window sizes help
improve the accuracy (lower validation loss), but can make
the inference slower (inference latency is per packet in C++).
dropping them in favor of the model‚Äôs results when applicable. A
similar process occurs when testing an egress model.
A dedicated investigation and evaluation of this function of
MimicNet is beyond the scope of this paper.
C THE IMPACT OF MODEL COMPLEXITY
We note that in MimicNet, a significant, domain-specific factor in
model complexity is the size of the training window. The window is
a number of packets (their features) that we input to the model. This
size decides (1) the amount of data that the model learns from one
sample, and (2) the hidden size of the LSTM model. Having a larger
window helps learning and potentially improves the prediction
accuracy, but at the cost of training and inference speed.
Figure 16 shows both of these effects on the training of an ingress
model. From Figure 16a, we can see that a window size of only 1
packet performs very poorly, even after several epochs. The training
accuracy is quickly improved with additional packets in the window,
but this comes with diminishing returns after the window size
reaches the BDP of the network (around 12 packets). Figure 16b
shows a reverse trend for training time. This suggests that the BDP
of the network strikes a good balance between accuracy and speed
for the LSTM model.
We also evaluated the impact of the window size on the valida-
tion accuracy and the inference speed. Figure 17 shows the result.
Specifically, Figure 17a shows that the validation loss resembles the
trend of the training loss as shown in Figure 16b. When there is only
one packet in the window, the model does not perform well‚Äîthe
validation loss decreases very slowly over ten epochs. Including
more packets helps the accuracy: a 2-packet window works signifi-
cantly better than a 1-packet window, and 5-packet window works
better than both. However, when the window size reaches the BDP
of the network (‚àº12 packets), having more packets in the window
does not improve the accuracy significantly. Figure 17b shows that
the model complexity also affects the inference speed. When the
window has only a few packets, e.g., 1 packet, 2 packets and 5
packets, the inference latency for a packet is as low as 70 ùúás. When
the window size increases to 10 and 12, the inference latency rises
to 100 ùúás, and with 20 packets, the inference time goes up further
to more than 150 ùúás. This evaluation validates the conclusion in
Appendix C: using BDP as the window size strikes a good balance
between accuracy and speed for the LSTM model.
D THROUGHPUT AND RTT
Figure 18 shows the comparison for throughput between Homa,
DCTCP (with ùêæ = 2), TCP Vegas, and TCP Westwood in a data
center with 32 clusters. Figure 19 shows the results for packet RTTs.
Similar to FCT, MimicNet can closely match the throughput and
RTT of a real simulation for all protocols. We can use the estimation
of MimicNet to compare these protocols‚Äînot only their general
trends of throughput and RTT distributions, but also their ranking
at specific points. For example, TCP Westwood achieves the best
90 percentile throughput performance due to its optimizations on
utilizing network bandwidth; in comparison, DCTCP has the lowest
throughput at this particular point. MimicNet successfully predicts
the order. The situation in RTT, however, is the opposite: TCP
Westwood now has the highest 90 percentile latency, while DCTCP
performs the best among these four protocols. This comparison is
also correctly predicted by MimicNet.
For all protocols, MimicNet estimates are much more accurate
than the small-scale (2-cluster) baseline. Again, MimicNet can achieve
an order of magnitude higher simulation speed at this scale.
E HEAVIER NETWORK LOADS
In addition to the default network load at 70% bisection bandwidth,
we have evaluated the performance of MimicNet with heavier net-
work loads. Figure 20 shows MimicNet‚Äôs estimation of the FCTs in
a network of 32 clusters where the aggregation network load is 90%
bisection bandwidth. Similar to previous experiments, MimicNet
provides high accuracy in approximating the ground truth: the over-
all ùëä1 score is low at 0.15, and the shape is maintained. MimicNet
completes the execution 10.4√ó faster than the full simulation.
F MORE GROUPS OF SIMULATIONS
We also ran additional experiments on the latency/throughput of
different methods to execute groups of simulations. For these experi-
ments, we fix the network size as 32 clusters and vary the simulation
(a) The ground truth of the comparison
(b) The approximation of MimicNet.
Figure 18: Throughput distributions of Homa, DCTCP, TCP
Vegas, and TCP Westwood in 32 clusters.
302
 0.01 0.02 0.03 0.04 1 2 3 4 5 6 7 8 9 10Validation Loss1 packet2 packets5 packets10 packets12 packets20 packets 0 0.05 0.1 0.15 0.2 0.25 0.3Inference Latency (ms)1 packet2 packets5 packets10 packets12 packets20 packets 0 0.2 0.4 0.6 0.8 1100101102103104105106107FractionThroughput (Bps)HomaDCTCPTCP VegasTCP Westwood 0 0.2 0.4 0.6 0.8 1100101102103104105106107FractionThroughput (Bps)HomaDCTCPTCP VegasTCP WestwoodMimicNet: Fast Performance Estimates for DCNs with ML
SIGCOMM ‚Äô21, August 23‚Äì27, 2021, Virtual Event, USA
G COMPUTE CONSUMPTION
A potential concern in using MimicNet is its compute resource
consumption: it uses GPU resources for model training and runtime
inference while the full simulations only use CPUs. This section
evaluates this aspect.
Specifically, we calculate the total number of floating-point op-
erations (FLOPs) in both CPUs (for both full simulations and Mim-
icNet) and GPUs (for MimicNet only) of the simulation approaches
in Section 9.3 as their compute resource consumption. Figure 23
shows the result for the evaluation of latency (similar findings
in the evaluation of throughput). Indeed, MimicNet shows signif-
icant computational load, primarily because of the use of GPUs
for training and inference. This makes its compute consumption
higher than full simulations when the network to be simulated is
small, especially when the training overhead is counted. However,
in large networks, e.g., 128 clusters, the use of deep learning models
in MimicNet pays off by much lower simulation latency, and its
total compute consumption is lower than full simulations even with
the computational overhead in training models. We leave the investi-
gation of alternative training, tuning, and models for optimizing
the compute consumption to future work.
H FUTURE DIRECTIONS
Finally, we note that the MimicNet framework offers plenty of
opportunities for improvement beyond those mentioned in Appen-
dix A. We introduce a small subset of such directions here.
Model reuse and retraining. An important goal in the design
of MimicNet is arbitrary scale, which is achieved by its end-to-
end workflow (Figure 3) and assumptions described in Section 4.
In that spirit, the models that are trained and tuned in MimicNet
can be safely reused to evaluate the network at any scale, i.e., no
matter how the network scales up or down by adding or removing
clusters. Generally, there is no need of retraining the models if
the training data and steps in MimicNet workflow do not change.
However, if any factor in the data and steps for generating the
models changes, the models should be updated to reflect the change.
This includes changes in the workload, routing/switching protocol,
internal structure of a cluster, and accuracy in the step of hyper-
parameter tuning.
Although we have shown that MimicNet runs faster than full
simulations even when the model training time is counted (Sec-
tion 9.3 and Appendix F), we would like to explore techniques that
can minimize the overhead of model retraining. This requires con-
siderations in both model design and MimicNet‚Äôs workflow, for
example, whether it is possible or how easily to transfer knowledge
between models and how MimicNet supports such incremental
model updates. We leave this exploration for future work.
Flow modeling. Recall from Section 6 that MimicNet uses feeder
models that currently learn offline flow-level patterns to approxi-
mate and remove non-observable inter-Mimic traffic. The ordering
and dependencies between observable flows are still simulated in
full fidelity, i.e., not approximated. That said, we acknowledge that
co-flow modeling is currently missing in MimicNet, which can
help the accuracy in the evaluation of some real-world systems
like MapReduce and BSP-style data processing. In order to support
co-flows in MimicNet, they have to be identified when extracting
(a) The ground truth of the comparison
(b) The approximation of MimicNet.
Figure 19: Packet RTT distributions of Homa, DCTCP, TCP
Vegas, and TCP Westwood in 32 clusters.
Figure 20: MimicNet approximation for high aggregation
network load (90% of the bisection bandwidth).
length from 20 simulation seconds to 320 simulation seconds. Fig-
ures 21 and 22 show the simulation latency and throughput results,
respectively, for different simulation approaches (we use the same
approaches introduced in Section 9.3).
The results are somewhat expected: the relative simulation speeds
of different approaches barely change with the simulation length.
When simulation length increases, the latency of each approach
increases correspondingly. The latency of full simulations increases
slightly slower than that of MimicNet because the constant sim-
ulation setup overhead in full simulations is significantly higher
than MimicNet. The relative latency eventually stabilizes‚Äîthe la-
tency of single MimicNet is lower than that of single simulation,
even when the model training time is included in MimicNet, and
partitioned MimicNet is better than partitioned simulation. For
all approaches, the simulation throughput does not change at all
with the simulation length. Similarly, single MimicNet outperforms
single full simulations, and parallel MimicNet outperforms parallel
full simulations. The speedup of MimicNet further grows when the
simulation scales to larger networks.
303
 0 0.2 0.4 0.6 0.8 1 0.001 0.01 0.1 1 10 100Fraction of PacketsLatency (s)HomaDCTCPTCP VegasTCP Westwood 0 0.2 0.4 0.6 0.8 1 0.001 0.01 0.1 1 10 100Fraction of PacketsLatency (s)HomaDCTCPTCP VegasTCP Westwood 0 0.2 0.4 0.6 0.8 110-310-210-1100101102Fraction of FlowsFlow Completion Time (s)GroundtruthMimicNet (0.154)SIGCOMM ‚Äô21, August 23‚Äì27, 2021, Virtual Event, USA
Q. Zhang et al.
Figure 21: Simulation latency with different simulation lengths (lower is better).
Figure 22: Simulation throughput with different simulation lengths (higher is better).
Figure 23: Compute resource consumption in different simulation approaches (lower is better).
features for training the internal models and using them for pre-
dictions. We leave enabling the ability of identifying co-flows in
MimicNet and studying the benefit in the evaluation of applications
where co-flows present for future work.
304
1011021031041051061072080320Simulation latency (seconds)Simulation length (simulation seconds)Single simulationSingle MimicNet w/ trainingSingle MimicNetPartitioned simulationPartitioned MimicNet10-410-310-210-11002080320Simulation throughput(simulation seconds/second)Simulation length (simulation seconds)Single simulationSingle MimicNet w/ trainingSingle MimicNetParallel simulationParallel MimicNet1041051061071081098163264128Out of memoryComputation (giga FLOPs)Network size (#clusters)Single simulationSingle MimicNet w/ trainingSingle MimicNetPartitioned simulationPartitioned MimicNet