如何在没有专用 SRE 团队的情况下应用 SRE 原则
Björn Rabenstein and Matthias Rampke, SoundCloud Ltd.
   通常，中型组织容易出现这样的情况，即相对较少的工程师必须开发和运行相对大量的不同功能。
SoundCloud 已经发展到这种情况。随着每个新功能添加到原始的单体 Ruby Rails 代码库中，添加下一个功能变得更加困难。因此，在 2012 年左右，我们开始逐步转向微服务架构。SoundCloud 的工程师们已经谈论了很多关于要成功这一举措需要解决的各种挑战。查看SoundCloud 后台博客是了解详细信息的良好开端。在本章中，我们将探讨在 SoundCloud 可靠运行数百个服务（工程师数量相比少得多）中吸取的经验教训。
向 SRE 求救！（以及他们如何失败）向 SRE 求救！（以及他们如何失败）
 2012 年，SoundCloud 碰巧聘用了几名前 Google SRE。尽管 SoundCloud 的规模相比 Google 要小得多，但正在向技术模式发展，与大型互联网公司一段时间以来所从事的技术模式没有那么大的差别。从广义来看，模仿谷歌的方式来维护这些系统是一个显然的途径。我们尝试了 SRE 一书的许多模式，只是当时这本书还在没有正式面世。
按员工人数计算的规模问题SRE 团队的最小合理规模是多少人？由于 SRE 应随时待命，因此团队需要足够大，才至少能够安排轮值。遵循轮值的最佳实践，站点可靠性工程：谷歌如何运行生产系统，第11章，我们最终只有 8 个人的团队规模。对于与 SoundCloud 大小类似的组织（即大约 100 名工程师），即使一个最小规模的 SRE 团队已经占工程师总数的 5% 到 10%。将此百分比翻倍或增加两倍是不可行的，这限制了我们最多只能使用一个 SRE 团队。对于规模更小的组织而言，专用 SRE 团队对人员编制的要求似乎令人望而却步。在谷歌，由于工程师数量超过我们两个数量级，显然有许多 SRE 团队致力于某些领域，如搜索或广告，支持数量有限的开发团队。同上，导言（第125页）。一个 SRE 团队就可以负责 SoundCloud 的一切！但是，SoundCloud 需要为不同的用户群体提供非常多样化的功能：听众希望流式播放音乐；创作者希望上传和管理他们的专辑；版权持有人想要报告，并最终得到回报；广告客户希望管理他们的广告系列。 事实上，SoundCloud 已经有一个轮值团队，由一个名为 SysOps 的团队人员负责。即使有相当稳定、成熟的服务（而 SoundCloud 的服务在当时远非如此），大量的快速变化导致我们不时被警报打断工作节奏，从而违反将每个工程师的琐事时间保持在 50% 以下的目标。同上，第5章除此之外，涉及的服务多种多样，使得待命系统工程师不可能对所有的警报作出合理的反应。对警报的通常反应是确定要上报的合适团队。我们不得不得出结论，尽管其规模足以维持一个专门的 SRE 团队，但我们的功能多样性实际上将我们置于与小型组织相同的状态：我们不能简单复制 Google SRE 模式，而必须调整我们的方法来适应自己的情况。嵌入式 SRE
   为了首次尝试应用 SRE 原则并减少 SysOps 团队在要求下承受一切压力，我们将 SRE 嵌入到后端开发团队中。SoundCloud 一直拥有强大的软件工程师文化，可以控制工作部署。这在一定程度上是早期没有运维小组或轮值团队时的保留。后来，它作为精心设计的管理战略的一部分而永久存在，长期看来这个战略优化了交付功能的速度。
嵌入 SRE 的方法被数字所注定：由于开发团队由不超过 10 名工程师组成，因此最多为每个团队分配一个 SRE，人员比例是大约 10% 的工程师是 SRE。
在这种环境下，单个 SRE 几乎没有影响新服务的设计和技术方向的杠杆作用。尽管他们中的一些人已经吸收了谷歌的概念，但大多数其他工程师和经理没有，也不知道该如何与那个工程师打交道。因为那人似乎在某一天突然出现，并宣布“现在是他们的 SRE”。在大多数情况下，他们要么被忽略，要么被视为另一个后端工程师，但与他们合作并不被视为提高可靠性或生产力的机会。嵌入后，他们又无法作为一个团队一起处理具有影响力的战略性项目，这些项目本来可以提供一个框架来增强开发团队的能力。 
你建造它，你就得运行它
  在这一点上，我们不得不尝试一些新的东西。我们拥抱了我们文化 DNA 中已经存在的东西：你建造它，你就得运行它。或者，正如我们有时喜欢说的：真正的 DevOps，没有单独的开发团队和运维团队，甚至没有在团队中指定开发或操作角色。可以说，这与 SRE 的基本原理相悖。毕竟，SRE 团队在某种程度上算是一个运维团队，只不过其前提是要用软件工程师的工具来解决运维人员困境。为了在我们的场景中成功应用 SRE 原则，我们需要向每个人灌输 SRE 思维模式。
部署平台
  在迁移到微服务的早期，我们了解到我们的流程和基础架构尚未准备好应对由此带来的大量服务。每项服务都需要专用服务器、支持它们的配置管理代码以及实际进行代码更改的部署过程。应用程序开发人员将部署代码，但不控制基础结构。新服务或现有服务的服务器完全由 SysOps 团队配置。对于每个新服务，我们需要编写配置管理和部署脚本。由于我们在 Ruby 的背景，我们很早就选择了 Chef 进行配置管理。Ruby 服务通常使用 Capistrano 部署。当我们开始试验 Ruby Rails 单体的替代方法时，更多的技术和方法开始发挥作用。这导致了摩擦和漫长的周期时间，以提出新的服务或扩展现有的服务。例如，向 Rails 单体添加更多服务器（当时仍是迄今为止最大的服务）需要 SysOps 和后端开发团队之间的多个移交来预配服务器，从而将初始配置管理集添加到准备部署、部署代码，然后更改配置管理以实际创建流程。为了处理这种情况，少数工程师已经开始创建一个基于容器的部署平台，使所有开发人员能够构建、部署和操作其服务。从设计上讲，它表面上与流行的 Heroku 平台即服务类似。它支持12-Factor 应用程序，幸运的是，对于许多现有服务来说，这不是一大步。为了构建新版本并使其准备好部署，开发人员将推送到 Git 存储库。这使得部署系统能够立即可供应用程序开发人员访问，而无需配置管理陡峭的学习曲线。其约束塑造了应用的设计，并在不扼杀创新的情况下轻松做正确的事情。
工程管理层意识到需要一个专门的团队来支持和进一步开发新的部署平台以及监控等其他“平台服务”。这是平台团队的诞生。除了部署平台的开发人员外，新团队还大量利用了以前嵌入的 SRE，从而导致团队不只有非常扎实的软件工程技能，同时也能很好地理解系统和运维问题。几乎意外地，平台团队成为自给自足、自主开发团队的榜样：从一开始，它就为它提供的所有服务（包括支持基础结构）轮值。历史说明：我们的部署平台叫做 Bazooka。你仍然可能会发现引用它的谈话和在互联网上的帖子。当时，它是一个高度创新的容器编排平台。当时，没有 Docker，容器对大多数人是相当神秘的东西，显然在开源空间里没有成熟的容器编排平台。因此，建造 Bazooka 是必须的，而不是一种东施效颦。后来，我们完全迁移到了 Kubernets。Bazooka 和平台团队现在成为历史。
完成循环：自行运维
  现在，部署更改和创建新服务变得简单而快速，一个新的瓶颈变得显而易见：即使开发人员可以随时部署，他们还是必须注意并与那些为其系统进行轮值的人员进行协调。
从 SoundCloud 的最初时代起，何时部署更改就由开发人员做出决定。另一方面，由经验丰富的系统和网络工程师组成的 SysOps 团队安装了第一个监控，默认情况下，他们收到了所有通知和警报。这种情况引起了大量的摩擦：SysOps 定期处理他们无法控制的代码部署的后果。他们不熟悉未在 Bazooka 上运行的应用程序的部署机制，并且不知道可以安全地回滚到哪些版本。由于全球流量高峰通常发生在欧洲正常工作时间结束后，因此性能问题通常在非工作时间出现。对于大多数问题，轮值的 SysOps 工程师只能做一件事：找到要上报的应用程序开发人员。由于不存在有组织的开发人员轮班，这有时可能很困难，并且接受上报的几率在开发人员之间分布非常不均匀。
显然，在保持开发团队以尽快部署功能的独立性的同时，我们的运维团队不可能进行可持续的轮值。增加更多的更新门槛是不可能的。相反，我们走另一条路：在平台团队的引领下，越来越多的团队接管了自己的服务。