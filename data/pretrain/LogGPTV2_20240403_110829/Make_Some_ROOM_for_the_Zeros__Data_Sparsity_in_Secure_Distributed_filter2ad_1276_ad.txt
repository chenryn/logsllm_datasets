inputs. This true after the first two steps since first P2 generates
random shares and then, using the security of the oblivious transfer,
P1 obtains a subset of them while P2 obtains nothing. The security
properties of the ROOM protocol guarantee that the shares that
each party obtains after the third step are also indistinguishable
from random from that partyâ€™s view. In the secure computation
in step four, P1 obtains values that depend on the random masks
Â¯u. The fact that P1 does not know Â¯u, together with the guarantees
of the secure computation, ensures the output is indistinguishable
from random for P1. The last step involves only local computation
for each party and thus does not change their views.
We will now use the protocols from the previous section to build
sparse matrix-vector multiplication protocols in the next section.
Concretely, we will use Gather for column sparsity (Section 5.2.1),
and ScatterInit for row sparsity (Section 5.2.2).
5.2 Sparse Matrix-Vector Multiplication
Throughout this subsection we consider party P1 holding a private
matrix M âˆˆ Zğ‘›Ã—ğ‘š
, with exactly ğ‘™ nonzero columns or rows, depend-
2ğœ
ing on the context. Party P2 holds a private vector v âˆˆ Zğ‘š
2ğœ with
7
ğ‘˜ nonzero entries. The value of ğœ is at least 64 in standard ML ap-
plications, and potentially more in settings that require additional
precision to represent real numbers using fixed point arithmetic, or
secret shares. The goal of all protocols is to compute the vector Mv
of length ğ‘›, additively shared between P1 and P2. This allows us to
easily integrate these protocols as part of higher-level secure proto-
cols, such as the solutions to machine learning problems presented
in Section 6. While we assume that ğ‘›, ğ‘š, ğ‘™, ğ‘˜ and ğœ are public, no
additional information is revealed to the parties.
The underlying theme of our protocols is different private re-
ductions of sparse matrix-vector multiplication to the dense case.
The goal of such reductions is to avoid multiplications by zero, and
hence have the cost of the dense multiplication be dependent only
on ğ‘™ and ğ‘˜, instead of the total size of the sparse dimension. There-
fore, the last step in our protocols will be to use a sub-protocol for
two-party dense matrix-vector multiplication. As discussed in Sec-
tion 2, efficient dedicated protocols for this functionality have been
recently presented. This includes solutions based on precomputed
triples by Mohassel and Zhang [29], as well as solutions based on
homomorphic encryption [22], and server-aided OT [26]. In our
protocols in this section we will refer to a generic dense matrix
multiplication protocol Dense-Mult, as well as to a generic ROOM
protocol Room. The concrete functionality of Dense-Mult takes as
input a matrix from one party and a vector from the other party and
computes their product as an additive share. In our implementation,
we use the protocols from [29].
5.2.1 Column-Sparse Matrix. We propose two protocols for the
case where M is sparse in the second dimension (i.e., there is a
small number of non-zero columns). These have different tradeoffs
depending on the relationship between the sparsity of M and v.
Note that matrix-vector multiplication, where the matrix is sparse
in its columns, can be viewed as a generalization of sparse vector
inner product, and thus the following protocols can also be used
for this functionality.
Our first protocol is shown in Figure 8. Let q = (ğ‘–1, . . . , ğ‘–ğ‘™) be the
indexes of the non-zero columns in M. The goal of the sparse-to-
dense reduction here is to replace the computation of Mv by the
computation of Mâ€²vâ€², where Mâ€² is the sub-matrix of M containing
only the non-zero columns ğ‘–1, . . . , ğ‘–ğ‘™, and vâ€² is the restriction of v to
the indices in q. Party P1 can compute Mâ€² locally. The two parties
then call the Gather protocol to obtain shares ([[vâ€²]]P1, [[vâ€²]]P2)
of vâ€². At this point the parties invoke the dense matrix multiplica-
tion protocol to compute Mâ€²[[vâ€²]]P2. Further, P1 locally computes
Mâ€²[[vâ€²]]P1 and adds the result to its share of Mâ€²[[vâ€²]]P2. As a result,
both parties obtain shares of Mâ€²vâ€². The security of the complete pro-
tocol follows directly from the security of the protocols for ROOM
and dense multiplication.
There are two drawbacks of the above protocol: (a) the space
of values of the ROOM sub-protocol coincides with the domain of
the elements of P2â€™s input vector, Z2ğœ . This is a problem in high-
precision settings where ğœ > 64, which are not uncommon in ML
applications where real numbers are encoded in fixed point. (b)
the length of the ROOM query ğ‘ is ğ‘™, which is the sparsity of the
serverâ€™s input matrix. In many settings, the vector ğ‘£ has less non-
zero values than ğ‘€, so ğ‘˜ < ğ‘™. That is why we would like to have
our ROOM query to only be of the smaller size ğ‘˜, which for two
8
Parties: P1, P2.
Inputs:
, with ğ‘™ nonzero columns.
P1: Matrix M âˆˆ Zğ‘›Ã—ğ‘š
2ğœ
P2: Vector v âˆˆ Zğ‘š
2ğœ , with ğ‘˜ nonzero entries.
Outputs: [[Mv]] = ([[Mv]]P1, [[Mv]]P2)
Protocol:(1) P1 sets q = (ğ‘–1, . . . , ğ‘–ğ‘™), the (sorted) list of
(2) P1 and P2 run Gather(v, q) to obtain shares
indexes of non-zero columns in M.
([[vâ€²]]P1, [[vâ€²]]P2) of vâ€², which is the restriction of v to
the indexes in q.
(3) P1 locally computes Mâ€² sub-matrix of M containing only
(4) P1 and P2 run Dense-Mult with inputs Mâ€² and [[vâ€²]]P2
and obtain shares ([[Mâ€²[[vâ€²]]P2]]P1, [[Mâ€²[[vâ€²]]P2]]P2) of
Mâ€²[[vâ€²]]P2.
(5) P2 sets the output [[Mv]]P2 to [[Mâ€²[[vâ€²]]P2]]P2 and P1
sets [[Mv]]P1 to [[Mâ€²[[vâ€²]]P2]]P1 + Mâ€²[[vâ€²]]P1.
the nonzero columns.
Figure 8: Our first protocol for column-sparse matrix and
vector multiplication.
of our constructions directly translates into a speed-up in MPC
time (see Table 1). However, if we simply have P1 act as the server
and put the non-zero columns of M in the ROOM protocol as the
database, while v becomes the query, the values in the ROOM
protocol become huge, as it would hold vectors of length ğ‘›, namely
the first dimension of M.
Our next protocol, shown in Figure 9, solves both issues (a) and
(b), by relying on a technique based on correlated permutations
introduced by Schoppmann et al. [35], which we exploit here by
means of the ROOM construction. First, our protocol ensures that
the serverâ€™s input to the ROOM functionality are elements in KÃ—K,
thus avoiding the dependence on ğœ. Second, it allows us to swap
the roles of P1 and P2 in the ROOM protocol, allowing us to choose
them depending on the relationship between ğ‘˜ and ğ‘™, as well as
other nonfunctional requirements induced by computation and
communication limitations of P1 and P2.
These two optimizations come at the cost of replacing the input
size to the dense multiplication sub-protocol from 2ğ‘™ğ‘›ğœ to 2(ğ‘™+ğ‘˜)ğ‘›ğœ.
Hence, in practice the actual values of min(ğ‘™, ğ‘˜), ğ‘›, and ğœ determine
a trade-off between the protocols in Figure 8 and Figure 9.
The intuition behind the construction in Figure 9 is as follows.
Let Ë†M and Ë†v be the result of removing zero columns and entries of M
and v, as defined in Figure 9, and let Â¯M and Â¯v be Ë†M and Ë†v padded with
ğ‘˜ zero columns and ğ‘™ zeroes, respectively. Now consider a trusted
third party that provides party Pi with a random permutation ğœ‹ğ‘–
such that, after permuting columns of Â¯M and Â¯v according to ğœ‹1 and
ğœ‹2 they are â€œwell alignedâ€, meaning ğœ‹1( Â¯M)ğœ‹2(Â¯v) = Mv. Note that it
is crucial that ğœ‹1 and ğœ‹2 look random to P1 and P2 respectively. To
achieve that, the third party generates random ğœ‹1 and ğœ‹2 subject to
the constraint that âˆ€ğ‘– âˆˆ [ğ‘™+ğ‘˜] : ğœ‹1(ğ‘–) = ğœ‹2(ğ‘–) â‡” ğ‘– âˆˆ (ğ´âˆ©ğµ), where
Parties: P1, P2.
Inputs:
P1: Matrix M âˆˆ Zğ‘›Ã—ğ‘š
2ğœ
P2: Vector v âˆˆ Zğ‘š
2ğœ , with ğ‘˜ nonzero entries.
, with ğ‘™ nonzero columns.
Outputs: [[Mv]]
Protocol:
(1) P1 chooses a random permutation ğœ‹1 of [ğ‘™ + ğ‘˜] and sets
d = ((ğ‘1, ğœ‹1(1)), . . . , (ğ‘ğ‘˜, ğœ‹1(ğ‘™))), and
ğ›½ = (ğœ‹1(ğ‘™ + 1), . . . , ğœ‹1(ğ‘™ + ğ‘˜)), where the ğ‘ğ‘–â€™s are the
indices of the nonzero columns in M.
(2) P2 sets q = (ğ‘1, . . . , ğ‘ğ‘˜), where the ğ‘ğ‘–â€™s are the indices of
the nonzero values in v.
(3) P1 and P2 run a designated-output Room with inputs
d, ğ›½, q. P2 obtains r = (ğ‘ğ‘–)ğ‘–âˆˆ[ğ‘˜].
(4) Let Ë†M âˆˆ Zğ‘›Ã—ğ‘™
2ğœ be M but with its zero columns removed.
P1 defines Â¯M as the result of appending ğ‘˜ zero columns
to Ë†M, and computes Mâ€² = ğœ‹1( Â¯M), where ğœ‹1 permutes
the columns of Â¯M.
(5) Let Ë†v âˆˆ Zğ‘˜
2ğœ be v but with its zero entries removed. P2
defines a permutation ğœ‹2 : [ğ‘˜ + ğ‘™] â†¦â†’ [ğ‘˜ + ğ‘™] such that
ğœ‹2(ğ‘–) = ğ‘ğ‘– for 1 â‰¤ ğ‘– â‰¤ ğ‘˜. The values ğœ‹2(ğ‘–) for
ğ‘˜ + 1 â‰¤ ğ‘– â‰¤ ğ‘˜ + ğ‘™ are a random permutation of
{1, . . . , ğ‘š} \ {ğ‘1, . . . , ğ‘ğ‘˜} (the set of unused indexes in
[ğ‘š]). P2 computes vâ€² = ğœ‹2(Â¯v), where Â¯v is Ë†v padded with
zeros up to length ğ‘˜ + ğ‘™.
(6) P1 and P2 run Dense-Mult with inputs Mâ€² and vâ€² to
obtain shares of Mv.
Figure 9: Column-sparse matrix and sparse vector multipli-
cation protocol.
ğ´ and ğµ are the sets of indexes of nonzero columns and values in
M and v.
The idea of [35] is to implement the above third party func-
tionality in MPC using garbled circuits. Our protocol in Figure 9
implements this functionality in an different way using the ROOM
primitive as follows. P1 acts as the ROOMâ€™s server with inputs
d = ((ğ‘1, ğœ‹1(1)), . . . , (ğ‘ğ‘˜, ğœ‹1(ğ‘™))) and ğ›½ = (ğœ‹1(ğ‘˜ + 1), . . . , ğœ‹1(ğ‘™ + ğ‘˜)),
where ğœ‹1 is a random permutation of [ğ‘˜ + ğ‘™] chosen by P1, and
P2â€™s query is simply q = (ğ‘1, . . . , ğ‘ğ‘˜) (see steps 1 and 2 in Figure 9).
The outputs for the two parties from the ROOM protocol are secret
shares of the array r = (ğ‘ğ‘–)ğ‘–âˆˆ[ğ‘˜]. Party P1 provides P2 with their
share of the ğ‘ğ‘–â€™s so that P2 can reconstruct ğœ‹2. Note that this com-
putation is independent of both ğ‘› and ğœ, in contrast to the protocol
in Figure 8. Moreover, it provides flexibility to exchange the roles
of the server and client in the ROOM protocol achieving the second
goal defined above.
The security of this construction follows from the security of
the ROOM protocol and the dense multiplication. The output of
the ROOM allows party P2 to obtain the evaluation of a random
permutation ğœ‹1 on its non-zero entries. The rest of the protocol
Server
ğ· =
ğ‘˜-Nearest Neighbors
(1) Compute sim(ğ‘, ğ‘‘) for
all ğ‘ âˆˆ ğ·.
(2) Compute ğ‘ğ‘‘ by
majority vote among
the ğ‘˜ nearest ğ‘ âˆˆ ğ·.
Client
ğ‘‘ =
?
ğ‘ğ‘‘
Figure 10: Setting for our secure ğ‘˜-NN application. We focus
on a single server, although the general protocol allows ğ· to
be distributed among several servers [35].
involves local computations until the final secure computation for
the dense multiplication.
5.2.2 Row-Sparse Matrix. We now consider the case where M is
sparse in its first dimension. In our solution to this variant P1 defines
Mâ€² as the matrix resulting from removing all zero rows from M.
Then the parties run a protocol to compute shares of the vector
r = Mâ€²v of length ğ‘™. For this, we can either use the protocol from
Figure 8, if v is sparse, or we can rely on dense multiplication. In
any case, r now contains all non-zero values of the desired result,
but its dimensions do not match Mv. However, note that Mv can be
recovered from r by inserting ğ‘› âˆ’ ğ‘™ zeros between the values of r at
positions corresponding to zero rows in the original matrix M. This
can directly be achieved by running ScatterInit(r, i, ğ‘›), where i
contains the indexes of non-zero rows in M.
In our higher-level applications, which we describe next, we
will use the matrix-vector multiplications described here in vari-
ous ways. In particular, we use the column-sparse protocol from
Figure 9 for ğ‘˜-NN classification, and the column-sparse and row-
sparse protocols from Figure 8 and the previous section for logistic
regression.
6 APPLICATIONS
We consider three applications to exemplify the features of our
framework. These include non-parametric data analysis tasks (naive
Bayes and ğ‘˜-nearest neighbors) as well as parametric data analysis
tasks (logistic regression trained by stochastic gradient descent
(SGD)). Besides showing the flexibility of our framework, our moti-
vation to select this concrete set of applications is to enable compar-
isons with previous works: secure naive Bayes classification was
studied by Bost et al. [5], Schoppmann et al. [35] recently proposed
a custom protocol for ğ‘˜-nearest neighbors that exploits sparsity,
and the SecureML work by Mohassel and Zhang [29] is the state of
the art in secure two-party logistic regression learning with SGD.
Due to space considerations, and since it mostly consists of
ROOM queries, we present our Naive Bayes classification in Appen-
dix B. The remainder of this section focuses on ğ‘˜-NN and two-party
SGD, both of which make use of the advanced matrix multiplication
protocols we presented in Section 5.2.
9
P1
ğ·1 =
[[ğœƒ]]P1
Logistic Regression
Compute ğœƒ by iterat-
ing over ğ·1
and ğ·2