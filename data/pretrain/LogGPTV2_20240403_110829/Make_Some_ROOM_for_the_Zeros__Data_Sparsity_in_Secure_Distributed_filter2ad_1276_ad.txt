inputs. This true after the first two steps since first P2 generates
random shares and then, using the security of the oblivious transfer,
P1 obtains a subset of them while P2 obtains nothing. The security
properties of the ROOM protocol guarantee that the shares that
each party obtains after the third step are also indistinguishable
from random from that party’s view. In the secure computation
in step four, P1 obtains values that depend on the random masks
¯u. The fact that P1 does not know ¯u, together with the guarantees
of the secure computation, ensures the output is indistinguishable
from random for P1. The last step involves only local computation
for each party and thus does not change their views.
We will now use the protocols from the previous section to build
sparse matrix-vector multiplication protocols in the next section.
Concretely, we will use Gather for column sparsity (Section 5.2.1),
and ScatterInit for row sparsity (Section 5.2.2).
5.2 Sparse Matrix-Vector Multiplication
Throughout this subsection we consider party P1 holding a private
matrix M ∈ Z𝑛×𝑚
, with exactly 𝑙 nonzero columns or rows, depend-
2𝜎
ing on the context. Party P2 holds a private vector v ∈ Z𝑚
2𝜎 with
7
𝑘 nonzero entries. The value of 𝜎 is at least 64 in standard ML ap-
plications, and potentially more in settings that require additional
precision to represent real numbers using fixed point arithmetic, or
secret shares. The goal of all protocols is to compute the vector Mv
of length 𝑛, additively shared between P1 and P2. This allows us to
easily integrate these protocols as part of higher-level secure proto-
cols, such as the solutions to machine learning problems presented
in Section 6. While we assume that 𝑛, 𝑚, 𝑙, 𝑘 and 𝜎 are public, no
additional information is revealed to the parties.
The underlying theme of our protocols is different private re-
ductions of sparse matrix-vector multiplication to the dense case.
The goal of such reductions is to avoid multiplications by zero, and
hence have the cost of the dense multiplication be dependent only
on 𝑙 and 𝑘, instead of the total size of the sparse dimension. There-
fore, the last step in our protocols will be to use a sub-protocol for
two-party dense matrix-vector multiplication. As discussed in Sec-
tion 2, efficient dedicated protocols for this functionality have been
recently presented. This includes solutions based on precomputed
triples by Mohassel and Zhang [29], as well as solutions based on
homomorphic encryption [22], and server-aided OT [26]. In our
protocols in this section we will refer to a generic dense matrix
multiplication protocol Dense-Mult, as well as to a generic ROOM
protocol Room. The concrete functionality of Dense-Mult takes as
input a matrix from one party and a vector from the other party and
computes their product as an additive share. In our implementation,
we use the protocols from [29].
5.2.1 Column-Sparse Matrix. We propose two protocols for the
case where M is sparse in the second dimension (i.e., there is a
small number of non-zero columns). These have different tradeoffs
depending on the relationship between the sparsity of M and v.
Note that matrix-vector multiplication, where the matrix is sparse
in its columns, can be viewed as a generalization of sparse vector
inner product, and thus the following protocols can also be used
for this functionality.
Our first protocol is shown in Figure 8. Let q = (𝑖1, . . . , 𝑖𝑙) be the
indexes of the non-zero columns in M. The goal of the sparse-to-
dense reduction here is to replace the computation of Mv by the
computation of M′v′, where M′ is the sub-matrix of M containing
only the non-zero columns 𝑖1, . . . , 𝑖𝑙, and v′ is the restriction of v to
the indices in q. Party P1 can compute M′ locally. The two parties
then call the Gather protocol to obtain shares ([[v′]]P1, [[v′]]P2)
of v′. At this point the parties invoke the dense matrix multiplica-
tion protocol to compute M′[[v′]]P2. Further, P1 locally computes
M′[[v′]]P1 and adds the result to its share of M′[[v′]]P2. As a result,
both parties obtain shares of M′v′. The security of the complete pro-
tocol follows directly from the security of the protocols for ROOM
and dense multiplication.
There are two drawbacks of the above protocol: (a) the space
of values of the ROOM sub-protocol coincides with the domain of
the elements of P2’s input vector, Z2𝜎 . This is a problem in high-
precision settings where 𝜎 > 64, which are not uncommon in ML
applications where real numbers are encoded in fixed point. (b)
the length of the ROOM query 𝑞 is 𝑙, which is the sparsity of the
server’s input matrix. In many settings, the vector 𝑣 has less non-
zero values than 𝑀, so 𝑘 < 𝑙. That is why we would like to have
our ROOM query to only be of the smaller size 𝑘, which for two
8
Parties: P1, P2.
Inputs:
, with 𝑙 nonzero columns.
P1: Matrix M ∈ Z𝑛×𝑚
2𝜎
P2: Vector v ∈ Z𝑚
2𝜎 , with 𝑘 nonzero entries.
Outputs: [[Mv]] = ([[Mv]]P1, [[Mv]]P2)
Protocol:(1) P1 sets q = (𝑖1, . . . , 𝑖𝑙), the (sorted) list of
(2) P1 and P2 run Gather(v, q) to obtain shares
indexes of non-zero columns in M.
([[v′]]P1, [[v′]]P2) of v′, which is the restriction of v to
the indexes in q.
(3) P1 locally computes M′ sub-matrix of M containing only
(4) P1 and P2 run Dense-Mult with inputs M′ and [[v′]]P2
and obtain shares ([[M′[[v′]]P2]]P1, [[M′[[v′]]P2]]P2) of
M′[[v′]]P2.
(5) P2 sets the output [[Mv]]P2 to [[M′[[v′]]P2]]P2 and P1
sets [[Mv]]P1 to [[M′[[v′]]P2]]P1 + M′[[v′]]P1.
the nonzero columns.
Figure 8: Our first protocol for column-sparse matrix and
vector multiplication.
of our constructions directly translates into a speed-up in MPC
time (see Table 1). However, if we simply have P1 act as the server
and put the non-zero columns of M in the ROOM protocol as the
database, while v becomes the query, the values in the ROOM
protocol become huge, as it would hold vectors of length 𝑛, namely
the first dimension of M.
Our next protocol, shown in Figure 9, solves both issues (a) and
(b), by relying on a technique based on correlated permutations
introduced by Schoppmann et al. [35], which we exploit here by
means of the ROOM construction. First, our protocol ensures that
the server’s input to the ROOM functionality are elements in K×K,
thus avoiding the dependence on 𝜎. Second, it allows us to swap
the roles of P1 and P2 in the ROOM protocol, allowing us to choose
them depending on the relationship between 𝑘 and 𝑙, as well as
other nonfunctional requirements induced by computation and
communication limitations of P1 and P2.
These two optimizations come at the cost of replacing the input
size to the dense multiplication sub-protocol from 2𝑙𝑛𝜎 to 2(𝑙+𝑘)𝑛𝜎.
Hence, in practice the actual values of min(𝑙, 𝑘), 𝑛, and 𝜎 determine
a trade-off between the protocols in Figure 8 and Figure 9.
The intuition behind the construction in Figure 9 is as follows.
Let ˆM and ˆv be the result of removing zero columns and entries of M
and v, as defined in Figure 9, and let ¯M and ¯v be ˆM and ˆv padded with
𝑘 zero columns and 𝑙 zeroes, respectively. Now consider a trusted
third party that provides party Pi with a random permutation 𝜋𝑖
such that, after permuting columns of ¯M and ¯v according to 𝜋1 and
𝜋2 they are “well aligned”, meaning 𝜋1( ¯M)𝜋2(¯v) = Mv. Note that it
is crucial that 𝜋1 and 𝜋2 look random to P1 and P2 respectively. To
achieve that, the third party generates random 𝜋1 and 𝜋2 subject to
the constraint that ∀𝑖 ∈ [𝑙+𝑘] : 𝜋1(𝑖) = 𝜋2(𝑖) ⇔ 𝑖 ∈ (𝐴∩𝐵), where
Parties: P1, P2.
Inputs:
P1: Matrix M ∈ Z𝑛×𝑚
2𝜎
P2: Vector v ∈ Z𝑚
2𝜎 , with 𝑘 nonzero entries.
, with 𝑙 nonzero columns.
Outputs: [[Mv]]
Protocol:
(1) P1 chooses a random permutation 𝜋1 of [𝑙 + 𝑘] and sets
d = ((𝑎1, 𝜋1(1)), . . . , (𝑎𝑘, 𝜋1(𝑙))), and
𝛽 = (𝜋1(𝑙 + 1), . . . , 𝜋1(𝑙 + 𝑘)), where the 𝑎𝑖’s are the
indices of the nonzero columns in M.
(2) P2 sets q = (𝑏1, . . . , 𝑏𝑘), where the 𝑏𝑖’s are the indices of
the nonzero values in v.
(3) P1 and P2 run a designated-output Room with inputs
d, 𝛽, q. P2 obtains r = (𝑝𝑖)𝑖∈[𝑘].
(4) Let ˆM ∈ Z𝑛×𝑙
2𝜎 be M but with its zero columns removed.
P1 defines ¯M as the result of appending 𝑘 zero columns
to ˆM, and computes M′ = 𝜋1( ¯M), where 𝜋1 permutes
the columns of ¯M.
(5) Let ˆv ∈ Z𝑘
2𝜎 be v but with its zero entries removed. P2
defines a permutation 𝜋2 : [𝑘 + 𝑙] ↦→ [𝑘 + 𝑙] such that
𝜋2(𝑖) = 𝑝𝑖 for 1 ≤ 𝑖 ≤ 𝑘. The values 𝜋2(𝑖) for
𝑘 + 1 ≤ 𝑖 ≤ 𝑘 + 𝑙 are a random permutation of
{1, . . . , 𝑚} \ {𝑝1, . . . , 𝑝𝑘} (the set of unused indexes in
[𝑚]). P2 computes v′ = 𝜋2(¯v), where ¯v is ˆv padded with
zeros up to length 𝑘 + 𝑙.
(6) P1 and P2 run Dense-Mult with inputs M′ and v′ to
obtain shares of Mv.
Figure 9: Column-sparse matrix and sparse vector multipli-
cation protocol.
𝐴 and 𝐵 are the sets of indexes of nonzero columns and values in
M and v.
The idea of [35] is to implement the above third party func-
tionality in MPC using garbled circuits. Our protocol in Figure 9
implements this functionality in an different way using the ROOM
primitive as follows. P1 acts as the ROOM’s server with inputs
d = ((𝑎1, 𝜋1(1)), . . . , (𝑎𝑘, 𝜋1(𝑙))) and 𝛽 = (𝜋1(𝑘 + 1), . . . , 𝜋1(𝑙 + 𝑘)),
where 𝜋1 is a random permutation of [𝑘 + 𝑙] chosen by P1, and
P2’s query is simply q = (𝑏1, . . . , 𝑏𝑘) (see steps 1 and 2 in Figure 9).
The outputs for the two parties from the ROOM protocol are secret
shares of the array r = (𝑝𝑖)𝑖∈[𝑘]. Party P1 provides P2 with their
share of the 𝑝𝑖’s so that P2 can reconstruct 𝜋2. Note that this com-
putation is independent of both 𝑛 and 𝜎, in contrast to the protocol
in Figure 8. Moreover, it provides flexibility to exchange the roles
of the server and client in the ROOM protocol achieving the second
goal defined above.
The security of this construction follows from the security of
the ROOM protocol and the dense multiplication. The output of
the ROOM allows party P2 to obtain the evaluation of a random
permutation 𝜋1 on its non-zero entries. The rest of the protocol
Server
𝐷 =
𝑘-Nearest Neighbors
(1) Compute sim(𝑝, 𝑑) for
all 𝑝 ∈ 𝐷.
(2) Compute 𝑐𝑑 by
majority vote among
the 𝑘 nearest 𝑝 ∈ 𝐷.
Client
𝑑 =
?
𝑐𝑑
Figure 10: Setting for our secure 𝑘-NN application. We focus
on a single server, although the general protocol allows 𝐷 to
be distributed among several servers [35].
involves local computations until the final secure computation for
the dense multiplication.
5.2.2 Row-Sparse Matrix. We now consider the case where M is
sparse in its first dimension. In our solution to this variant P1 defines
M′ as the matrix resulting from removing all zero rows from M.
Then the parties run a protocol to compute shares of the vector
r = M′v of length 𝑙. For this, we can either use the protocol from
Figure 8, if v is sparse, or we can rely on dense multiplication. In
any case, r now contains all non-zero values of the desired result,
but its dimensions do not match Mv. However, note that Mv can be
recovered from r by inserting 𝑛 − 𝑙 zeros between the values of r at
positions corresponding to zero rows in the original matrix M. This
can directly be achieved by running ScatterInit(r, i, 𝑛), where i
contains the indexes of non-zero rows in M.
In our higher-level applications, which we describe next, we
will use the matrix-vector multiplications described here in vari-
ous ways. In particular, we use the column-sparse protocol from
Figure 9 for 𝑘-NN classification, and the column-sparse and row-
sparse protocols from Figure 8 and the previous section for logistic
regression.
6 APPLICATIONS
We consider three applications to exemplify the features of our
framework. These include non-parametric data analysis tasks (naive
Bayes and 𝑘-nearest neighbors) as well as parametric data analysis
tasks (logistic regression trained by stochastic gradient descent
(SGD)). Besides showing the flexibility of our framework, our moti-
vation to select this concrete set of applications is to enable compar-
isons with previous works: secure naive Bayes classification was
studied by Bost et al. [5], Schoppmann et al. [35] recently proposed
a custom protocol for 𝑘-nearest neighbors that exploits sparsity,
and the SecureML work by Mohassel and Zhang [29] is the state of
the art in secure two-party logistic regression learning with SGD.
Due to space considerations, and since it mostly consists of
ROOM queries, we present our Naive Bayes classification in Appen-
dix B. The remainder of this section focuses on 𝑘-NN and two-party
SGD, both of which make use of the advanced matrix multiplication
protocols we presented in Section 5.2.
9
P1
𝐷1 =
[[𝜃]]P1
Logistic Regression
Compute 𝜃 by iterat-
ing over 𝐷1
and 𝐷2