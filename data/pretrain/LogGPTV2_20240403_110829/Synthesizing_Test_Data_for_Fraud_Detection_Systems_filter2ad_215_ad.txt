h
i
l
e
k
i
l
d
u
a
r
F
1.2
1
0.8
0.6
0.4
0.2
0
0
Detection results - Breakin fraud in synthetic data
Detected Fraud
Fraudulent period
10
20
30
40
50
60
70
80
90
Days since epoch
Figure 13. Detection of break-in fraud in synthetic
data
The detection of break-in frauds in authentic data is
shown in Figure 14. The result is not as promising as in
the synthetic data. In addition to the period of actual fraud
(days three to six), several false positives are shown at vari-
ous intervals (approximately at days 33, 62, 68, 71, 75, 81,
85). An investigation of the reason for this showed that the
fraudulent user did not deviate a great deal in consumption
9
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:39:18 UTC from IEEE Xplore.  Restrictions apply. 
compared to the ”normal” user. This made it difﬁcult for the
neural network to distinguish between normal and fraudu-
lent behavior.
d
o
o
h
i
l
e
k
i
l
d
u
a
r
F
1.2
1
0.8
0.6
0.4
0.2
0
0
Detection results - Breakin fraud in authentic data
Detected Fraud
Actual Fraud
10
20
30
40
50
60
70
80
90
Days since epoch
Figure 14. Detection of break-in fraud in authentic
data
The average numbers of movie orders per day are plot-
ted in Figure 15. The fraudulent period (day 4-7) did not
exceed forthcoming consumption enough to allow the net-
work to successfully differentiate fraudsters from normal
users. Clearly, our team of ”fraudsters” did not succeed in
their job of ordering an excessive amount of movies during
that time period. The false positives were also affected by
the temporal memory, as the false alarms were preceded by
quite a few days of moderate usage which together, over
time, built up sufﬁciently large input values for the neural
networks to trigger an alarm. Once again, this illustrates the
value of carefully balancing the decay rate of the memory
function for each type of fraud.
l
a
v
r
e
t
n
i
/
s
r
e
d
r
o
i
e
v
o
M
45
40
35
30
25
20
15
10
5
0
Average movie orders for a fraudulent user
Movie orders/interval
0
10
20
30
40
50
60
70
80
90
Days since epoch
Figure 15. Movie order averages for a fraudulent
user (authentic data)
7.5. Quantitative results
Table 1 shows the detection results. The Sensitivity and
Speciﬁcity are shown in the ﬁrst two columns of the table.
Sensitivity [true pos / (true pos + false neg)] shows to which
extent fraudulent activity is classiﬁed correctly.
If detec-
tion is high in false negatives, the sensitivity becomes poor.
10
The Speciﬁcity [true pos / (true pos + false pos)] indicates
the degree of misclassiﬁcation of non-fraudulent events. If
a test shows high a false positive value, the speciﬁcity be-
comes poor. A goal of classiﬁcation systems is to be high
in both speciﬁcity and sensitivity. The Number (#) of pe-
riods deﬁnes the total time frame during which detection
was performed. In our tests, a single time quanta was 1440
minutes. The periods speciﬁed for each test are harmonized
with the graphs illustrated in Figures 13, 14, and 15. For
each fraud type, True positives, False Positives, False nega-
tives and True negatives are shown.
As can be seen in the table, speciﬁcity is somewhat better
for the synthetic test data than for the authentic data. This
is expected as the neural network detector was trained using
synthetic data and also these data are more regular. How-
ever, the sensitivity is better for the authentic data, which is
more unexpected. This is likely a result from the fact that
the synthetic data had a higher fraud rate. In short, the au-
thentic data contained mostly of normal data with only short
periods of fraud, while the synthetic data had a signiﬁcantly
higher percentage of fraud, which provided more opportuni-
ties for misclassiﬁcation. Overall, we believe that the differ-
ences between synthetic and authentic data are reasonably
small and indicate that the training using our synthetic data
was successful.
8. Discussion of results and future work
Scalability versus complexity tests. We created hundreds
of simulated users acting for a period of seven months. This
was sufﬁcient to train and test the fraud detection prototype.
We are thus satisﬁed with the ability of the method and the
implementation to scale the number of users and the time
period of the synthetic logs.
An interesting scalability issue that should be studied
further is the effects of a more complex modeling of the
users and the system.
Implementing outliers and several
user classes should not affect the scalability very much, but
the use of a more complex state machine for user behav-
ior would probably affect the performance of the simulation
tool.
Diversity of background data. The background data were
rather homogeneous. There were, for example, no “out-
liers” among the users. One reason for this was that only
one user proﬁle was used, which meant that all the simu-
lated users behaved according to the same statistical distri-
butions. The obvious solution to this problem is to use sev-
eral user classes with different proﬁles, which was our ini-
tial intention, but time limits in the project prevented this.
For our detection experiments, it seems that the diversity
of the background data was good enough for the billing
fraud, since the normal behavior in the authentic data did
not trigger very many false alarms. However, it posed a
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:39:18 UTC from IEEE Xplore.  Restrictions apply. 
Fraud
Billing fraud (synthetic data)
Billing fraud (authentic data)
Break-in fraud (synthetic data)
Break-in fraud (authentic data)
Sensitivity
0.89
1.0
0.92
1.0
Table 1. Detection results
Speciﬁcity
# of periods
True pos.
False pos.
False neg.
True neg.
0.98
0.93
1.0
0.86
365
61
89
87
97
3
26
4
3
4
0
11
11
0
2
0
254
54
61
72
bigger problem for the break-in fraud, where normal users
and fraudsters showed very similar behavior. It would be in-
teresting to make tests using background data with different
degrees of homogeneity.
The user model. The user model may be too simplistic
for certain applications. Some behavior was deliberately
left out of the model to keep it simple, such as use of the
network for other things than downloading movies. Nei-
ther were long-term variations in users’ activity modeled.
It would be interesting to develop a more advanced user
model. However, we believe that the user model is of sufﬁ-
cient detail in the VoD application.
The system model. We used static parameters for control-
ling delays and some of the router statistics in the system
model.
In a real system, these would vary depending on
load etc. Future versions of our simulator will be more
dynamic in simulating such dependencies. Neither did our
simulator model distinguish “strange” user behaviors, such
as malformed network trafﬁc, spooﬁng etc, which limits the
ability to simulate attacks based on bugs in the software and
the hardware. This could also be improved in future ver-
sions of the simulator.
Fraud cases. Our process of injecting frauds in authentic
data can be further improved. We injected frauds in only a
few users’ behavior and for only short periods of time. This
was acceptable for our application but makes it more difﬁ-
cult to verify that the modeling of the users and the system is
sufﬁcient for more general use and other target applications.
We plan to establish a more lengthy list of fraud scenarios
for future experiments.
Suitability for other types of services. Future work will
show whether the data generation process works equally
well for other types of services and for intrusion detection.
9. Conclusions
We have developed a method for generating large
amounts of synthetic log data that preserve statistical prop-
erties of a selected set of authentic data used as a seed. We
have experimentally shown that the synthetic data generated
can be successfully used for training and testing a fraud
detection system. Future experiments will verify whether
this also holds for more general classes of seed data and for
other types of fraud detection systems. We learned several
lessons in the process of generating and testing data which
will help us to further improve our methodology.
10. Acknowledgments
We would like to thank Telia Research AB (nowadays
TeliaSonera) and the people working on the VoD service
for their cooperation and help. We would also like to thank
the participants of the EURESCOM project ”P1007”.
References
[1] P. Burge, J. Shawe-Taylor, Y. Moreau, B. Preneel, C. Stoer-
mann, and C. Cooke. Fraud detection and management in
mobile telecommunications networks. In Proceedings of the
European Conference on Security and Detection ECOS 97,
London, April 1997. ESAT-SISTA TR97-41.
[2] P. K. Chan, W. Fan, A. L. Prodromidis, and S. J. Stolfo.
Distributed data mining in credit card fraud detection. IEEE
Intelligent Systems, 14(6), Nov/Dec 1999.
[3] H. Debar, M. Dacier, A. Wespi, and S. Lampart. An experi-
mentation workbench for intrusion detection systems. Tech-
nical Report RZ2998, IBM Research Division, Zurich Re-
search Laboratory, Zurich, Switzerland, Mar. 1998.
[4] J. W. Haines, R. P. Lippmann, D. J. Fried, E. Tran,
S. Boswell, and M. A. Zissman. 1999 darpa intrusion de-
tection system evaluation: Design and procedures. Techni-
cal Report Technical Report 1062, MIT Lincoln Laboratory,
Feb. 2001.
[5] H. Kvarnstr¨om, E. Lundin, and E. Jonsson. Combining fraud
and intrusion detection - meeting new requirements. In Pro-
ceedings of the ﬁfth Nordic Workshop on Secure IT systems
(NordSec2000), Reykjavik, Iceland, Oct. 2000.
[6] W. Lee and D. Xiang. Information-theoretic measures for
anomaly detection. In Proceedings of the 2001 IEEE Sym-
posium on Security and Privacy, May 2001.
[7] E. Lundin, H. Kvarnstr¨om, and E. Jonsson. A synthetic fraud
data generation methodology. In Lecture Notes in Computer
Science, ICICS 2002, Laboratories for Information Technol-
ogy, Singapore, Dec. 2002. Springer Verlag.
[8] R. A. Maxion and K. M. Tan. Benchmarking anomaly-based
detection systems. In International Conference on Depend-
able Systems and Networks, New York, New York, June
2000. IEEE Computer Society Press.
[9] M. C. Moser. Neural net architectures for temporal se-
quence processing. Addison-Wesley Publishing, Redwood
City, CA, 2001.
[10] N. J. Puketza, K. Zhang, M. Chung, B. Mukherjee, and R. A.
Olsson. A methodology for testing intrusion detection sys-
tems. Software Engineering, 22(10), 1996.
[11] K. M. C. Tan and R. A. Maxion. Determining the operational
limits of an anomaly-based intrusion detector. IEEE Journal
on Selected Areas in Communication, 21(1), Jan. 2003.
11
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:39:18 UTC from IEEE Xplore.  Restrictions apply.