管理
知识
289
---
## Page 316
数据可视化和第三方应用。
同时，任何一个组件都可以随时被替换为其他解决方案。
16.2
就选择了用“小工具”来构建“大系统”。主要原因如下：
和小工具，方便集成和二次开发。
290
总体上，系统分为数据采集、
就像我们之前所提到的，在“大工具、小系统，小工具、大系统”的选择中，我们一开始
这个看上去挺大的系统，处处都显出“小”
微博平台监控系统整体架构示意图如图16-2 所示。
Graphite 体系基本上满足了我们对一个监控系统的全部要求，还开源，有大量的周边应用
C
O
O
按照上面的标准，我们再来看 Graphite 的特性。
O
需求不明确/变化快，优秀的系统和架构是逐渐演化出来的，不是一开始就设计好的。
支持 UDP/TCP 协议。
无须开发，只要按照协议发送数据，即可自动在后端生成可视化图表。
必须是实时的，不能是离线计算和批处理，实时系统才可能支持动态调配资源和决策。
智能运维：从O搭建大规模分布式AIOps系统
整体架构
成本要低（在数据传输量、带宽占用、系统资源占用等方面)。
快速迭代，所见即所得。
业务方技术栈多样化，接入方式必须简单。
函数丰富，支持各种维度的查询、聚合（指标的设计很重要)，可以快速满足不同的需
明文数据格式，指标的定义非常灵活。
Graphite/Grafana 展现和数据分离，需求变更无须改动任何代码。
对系统的侵入性足够小，无须安装复杂的客户端，或者在代码层面做重大改动。
长
函数，
必须是轻量级的，
，支持聚合、
汇总和正则匹配。
不需要传输大量日志到后端，无须进行复杂的配置，
、数据路由、聚合运算、数据分发、数据存储、告警、API、
一没有任何一个组件需要同时完成两件事，
，要提供丰富的
---
## Page 317
16.3.1
16.3
O
〇从源数据量的角度来看，数千万指标需要秒级存储和运算，每天至少需要处理几百亿
这样的监控平台属于大数据平台吗？
〇业务数据：AppKey 访问量、话题阅读量等。
O
〇系统指标：CPU、内存、网卡流量、负载等。
我们采集的数据（指标）主要包括以下几种。
C
从结果数据量的角度来看，经过采集端规则化和聚合（1秒周期）后，按照协议组成指
服务指标：Nginx、Tomcat、RPC 框架等的 QPS、响应时间。
核心模块
条日志数据，也算“大”数据了。
异常数据：各
性能数据：接口性能（QPS、响应时间）、各阶段耗时、依赖资源性能、slowtop。
数据采集（Logtailer）
各种错误代码、栈信息、报错信息等。
图16-2微博平台监控系统整体架构示意图
告警中心
第三享告警规
2分钟单机聚合
10秒profile聚合
聚合运算
第16章微博平台通用监控系统
Graphite
手机ApP
Grafana
291
---
## Page 318
抽象，如图16-3所示。
这种方式多用于框架层不写日志到文件，而是直接输出指标到远端服务器的场景。对业务进行
292
Odpool_sso.ilogin_tc.htp_2xx.get_user_id.intervall(interval2,interval3,interval4,interval5)
Odpool_sso.ilogin_tc.http_2xx(4xx,5xx).get_user_id.hits
(1）http_2xx,http_4xx,http_5xx，响应时间区间分布数据
也就是说，以上这些信息基本上可以描述一个业务的状态。所对应的 key（指标）设计如
O dpool_sso.ilogin_tc.http_2xx.get_user_id.mean
（2）平均响应时间
实际上还有一种方式，就是通过我们公开的域名，按照协议直接发送包含指标项的UDP包。
（3）单机数据
Agent：自动收集系统监控指标（下一步的计划是使 Agent也能支持业务日志的分析）。
〇Logtailer：以tail的方式分析业务日志，提取有用信息，完成初步的聚合后，按照约定
在采集端，
 dpool_sso.ilogin_tc.byhost.172_16_xx_xx.http_2xx(4xx,5xx).get_user_id.hits
的协议发送到路由节点。
带宽、存储的占用很小（相对于需要收集原始日志的大数据平台)。
标发送到路由节点，这时数据已经变成了“指标”，不包含原始数据和无用的信息，对
智能运维：从0搭建大规模分布式AIOps系统
支持两种方式。
业务线
服务池
机器
图16-3业务抽象
接口
Web访
状态码
QPS
平均耗
间QPS
耗时区
---
## Page 319
16.3.2
单、反垃圾、审计等安全性方面的设计，防止无效数据和滥用），门槛非常低。
制，比如 MC 不容许 key 超过 255个字节)。
要条件（这只是基本的key 设计，还可以添加更多的信息，但key 的长度会受到一些系统的限
在这里需要注意的是，我们的系统不提供“日志查询”，那是大数据平台的工作。
理论上，任何部门只要按照协议发送数据，就算接入监控系统（在实际应用中，会有白名
指标里面已经包含了业务线、服务池，接口、IP等信息，这为后面的聚合、分发提供了必
〇不同的计算类型和聚合周期，对应着不同的计算节点。
Okey hash：在一个聚合周期内，一个key 必须固定落在一个节点上。
数据路由组件的作用如下。
目前系统支持以下几种针对key的运算和处理协议，如图16-4所示。
Odpool_sso.ilogin_tc.http_2xx(4xx,5xx).get_user_id.hits
（4）总量数据
业务监控系统的接入方案
Key:ValuelTypeln
数据路由（Statsd-proxy）
定义socket数据接
协议
Counter
算接口QPS
通常用于计
入协议
.ms
协议
Timers
图16-4系统支持的运算和处理协议
仅供非商业用途或交流学习使用
·近赛用子荐
lkv
协议
keyValue
家
协议
TOP计算
用于热话
itop结尾
数据包以
题TOP100计
第16章微博平台通用监控系统
数据包以
协议
Nohost
·数据包以
协议
10s
10s的集群
293
---
## Page 320
cluster realtime
到不同的存储节点上。
16.3.4
16.3.3
组件参与复制、分流。
294
数据分发示意图如图16-5所示。
carbon_ch replication 1
数据路由负责把指标路由到不同的计算集群，而数据分发的作用就是把计算好的指标分发
〇实时数据：求和、求平均，同时又区分10秒、30秒、60秒不同的运算周期。为兼容
对指标的计算和操作分为以下几种。
除了上述作用，还有一些特殊用途，比如核心数据的备份、多机房余，都需要数据路由
〇不同用途的指标，需要路由不同的集群（比如告警和监控分流)。
O
聚合计算：按照接口、IP、服务池和机房等多个维度自动聚合。
智能运维：从O搭建大规模分布式AIOps系统
10.39.xx.xx:2003
10.39.xx.xx:2003
10.39.xX.xx:2003
信息。
历史数据：区别于实时数据，历史数据会做10分钟以上的粒度聚合，而且会去掉IP
函数运算：按照运算周期进行各种流式函数的运算，比如P99、Baseline、慢速比、TOPN等。
压测接口的实时性要求，还提供了1秒一次的运算模型。
数据分发（C-Relay）和数据存储
聚合运算（Statsd）
图16-5
数据分发
Relay
数据分发示意图
TSDB
---
## Page 321
16.3.5
match ^stats\.timers\.openapi\..*\.([0-9](6,})\..*
式完成对指标的分发，同时具有按照 key进行垃圾数据过滤的功能，并支持正则表达式。
数据。
接口。
方式就是使用 TSDB（OpenTSDB 基于 HBase）。不同于 Whisper，TSDB 需要单独的存储/访问
数据量却很大，存储周期可能有数年，那么既支持海量数据存储，又具有时序数据特点的存储
择保存在 SSD 磁盘上（仍然是Whisper文件），兼顾容量和访问效率。
没有那么极端，但是数据时间跨度较长，对性能仍有一定的要求。所以，对于这类数据我们选
如1分钟，时间跨度为2~7天)，一般都用在几天内同比、环比等场合。这类数据对效率的要求
受限于内存的容量，实时数据的保存周期一般较短
整个存储目录 mount到内存中。内存的读写性能比磁盘高很多，这样基本解决了磁盘I/O 问题;
效率高于一切，所以我们采用了内存文件系统，指标就保存在时序数据文件Whipser 中，并将
一个意思。
send to blackhole
而完成对存储分发的就是C-Relay 这个工具，它会按照存储时长、前缀、通配符等各种方
为了数据的安全性，实时和准实时数据会在TSDB 中持久化备份一份，但会定期清除过期
告警模块展示如图16-6所示。
这条配置信息表示一旦满足条件，这种类型的 key 就会被发送到 blackhole，和/dev/null 是
这类数据就是典型的“冷数据”，只有在特定的分析场合才会被用到，访问频率最低。但其
（3）历史数据（7天以上），存储介质：TSDB
这里说的准实时，其实不是指数据有延迟，而是指数据使用频率较低、聚合周期较长（比
（2）准实时数据，存储介质：SSD磁盘
stop
考虑到这类数据的使用频率最高（聚合周期为1~10秒，时间跨度为2小时至1天)，访问
（1）实时数据，存储介质：内存
为了提高访问性能和写入效率，我们对指标数据进行了存储分类。
告警模块
仅供非商业用途或交流学习使用
第16章微博平台通用监控系统
295
---
## Page 322
光配置告警规则的工作量就十分巨大，此外还存在以下几个问题。
扫描存储（Whisper 和TSDB）中的相关指标，满足条件即触发告警，如图16-7所示。
也有方便移动办公的手机 App 推送。关于告警设计也经历了多个发展阶段。
296
这种方式的原理和配置比较简单，在指标相对较少的情况下可以使用，指标一旦多起来，
业务方配置好需要告警的接口、阈值和告警方式，我们通过Crontab 运行一组脚本，定期
我们提供了多种告警途径，有适合 Service Desk、监控大厅使用的九宫格（颜色和声音），
延迟：从 Whisper 和TSDB 中轮询指标，这意味着已经有了一段时间的延迟（时间长
轮询式告警
智能运维：从0搭建大规模分布式AIOps系统
情况，网络抖动引起的短暂异常也会引发大量的误报。
灵活性差：系统是动态发展的，业务量在不断增长，固定的规则、阈值很难适应这种
短取决于聚合周期、存储步长，
接口维度告警
?
→
图16-7轮询式告警流程图
MySQL
图16-6告警模块展示
，以及计算流程上的时间损耗)。
获取规则
单机维度告警
Whisper