Bojie Li, Gefei Zuo, Wei Bai, and Lintao Zhang
(a) Background flows.
(b) Oversubscription.
Figure 12: The impact of queuing delay on 1Pipe latency.
(a) CPU processing overhead.
(b) Network bandwidth overhead.
Figure 13: Beacon overhead under different beacon intervals. CPU
processing overhead for Arista switch is extrapolated.
PFC pauses in core network. We believe more advanced congestion
control mechanisms [11, 39, 70, 76, 83] can mitigate this problem.
Messages much larger than MTU will stall other messages, e.g.,
an 1 MB message will increase 80 ùúás latency of other messages.
Failure recovery. Failure detection in 1Pipe is typically faster
than heartbeat timeout in most applications, because the beacon
interval in 1Pipe is very low. Figure 10 depicts failure recovery
time, which measures the average time of barrier timestamp stall
for correct processes. In our testbed, a failure is detected if beacon
is not received for 10 beacon intervals (30 ùúás). In addition to failure
detection time, the recovery procedure in Sec.5 requires 6 network
diameters plus the time to transmit and process messages. Core
link and switch failures do not affect connectivity, so, only the
controller needs to be involved, and no process is considered to be
failed. Host, NIC, host link, and ToR switch failures cause processes
to disconnect from the system, so, the recovery takes longer because
each correct process needs to discard messages from or to them.
There is a significant jump for ToR switch because all processes in
the rack fail, leading to more failure recovery messages.
CPU overhead. The CPU overhead of 1Pipe has two parts: re-
ordering at receivers and beacon processing at switches. The mes-
sage delivery throughput degrades slightly with more messages to
reorder. As Figure 11 shows, the maximal send and receive buffer
size increases linearly with latency, but only takes a few megabytes
on a 100 Gbps link.
Figure 13a shows the number of cores required for beacon pro-
cessing of a 32-port switch. A host CPU core can sustain 3 ùúás beacon
interval of the switch, which is our testbed setting. If switch CPUs
are used instead, the raw packet processing capacity of a switch
CPU is roughly 1/3 of a host CPU core. If we can bypass the kernel
network stack and process packets efficiently at Arista switches, a
single switch CPU core can sustain 10 ùúás beacon interval.
Network overhead. As Figure 13b shows, with high link band-
width and a reasonable beacon interval (e.g., 3 ùúás), beacon traffic is
a tiny portion (e.g., 0.3%) of link bandwidth. Because beacons are
hop-by-hop, the overhead is determined by beacon interval and
does not increase with system scale.
Scalability to larger networks. On the latency perspective, the
base and beacon processing delays are proportional to the number
of hops in the network, which is typically logarithm to the number
of hosts [8, 43]. The clock skew increases due to higher latency
between clock master and hosts, and higher probability of bad
clocks with high drift rates [69]. We did not analyze the clock
skew quantitatively yet. For reliable 1Pipe, the expected number
of packet losses in an RTT is proportional to the number of hosts
times the number of hops. For 32K hosts, if all links are healthy
(with loss rate 10‚àí8), the latency increases by 0 ‚àº 3ùúás compared
to loss-free; if all links are sub-healthy (with loss rate 10‚àí6), the
latency increases by 3 ‚àº 17ùúás. On the throughput perspective, the
beacon overhead is unrelated to network scale, while the hosts
would use larger memory and more CPU cycles to reorder the
messages. The memory size is BDP (Bandwidth-Delay Product),
and the reordering time is logarithm to BDP.
The major scalability challenge is failure handling. Failure of any
component will stall the entire network. In best effort 1Pipe, failure
handling is localized because the fault component is removed by
the neighborhood in a 30 ùúáùë†-like timeout, so the remaining parts of
the network experience a delivery latency inflation. Although this
inflated latency is fixed, the frequency of occurrence is proportional
to network scale. In reliable 1Pipe, failure handling is coordinated
by a centralized controller, which needs to contact all processes in
the system, so, the failure recovery delay increases proportionally
with system scale, which is 3‚àº15 ùúáùë† per host.
7.3 Applications
7.3.1 Transactional Key-Value Store. We evaluate a distributed
transactional key-value store where each server process stores a
portion of KVs in memory using C++ std::unordered_map without
replication. A transaction (TXN) is composed of multiple inde-
pendent KV read or write operations. TXN initiators dispatch KV
operations to server processes by hash of key. Read-only (RO) TXNs
are served by best effort 1Pipe, while read-write (WR) and write-
only (WO) TXNs use reliable 1Pipe. For comparison, we implement
non-replicated and non-durable FaRM [34] which serves RO TXNs
in 1 RTT by reading the KVs and checking lock and version. WR
and WO TXNs in FaRM use OCC [62] and two-phase commit. As
a theoretical performance upper bound, we also compare with a
non-transactional system.
Each TXN has 2 KV ops by default, where read and write are
randomly chosen for each op. Keys are 64-bit integers generated
either uniformly random or by Zipf distribution in YCSB [26]. YCSB
has hot keys. The value size is randomly generated according to
Facebook‚Äôs ETC workload [13]. We record average TXN latency at
95% of peak throughput.
In Figure 14a, 50% of TXNs are read-only. In both uniform and
YCSB distribution, 1Pipe delivers scalable throughput (per-process
throughput does not degrade), which is 90% of a non-transactional
key-value store (NonTX). As number of processes increase, YCSB
scales not as linearly as uniform, because contention on hot keys
lead to load imbalance of different servers. With 512 processes,
 0 10 20 30 40 50 60 70 800246810Latency (us)Number of Background Flows per HostBE-hostR-host 0 20 40 60 80 100 1201:12:13:14:15:16:1Latency (us)Oversubscription RatioBE-hostR-host10-310-210-11001011021101001000Portion of a CPU coreBeacon Interval (us)Arista switch (OS)Arista switch (raw)Xeon E5 Server (DPDK)10-310-210-11001011101001000Percentage of Overhead TrafficBeacon Interval (us)10 Gbps40 Gbps100 Gbps1Pipe: Scalable Total Order Communication in Data Center Networks
SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
(a) Scalability.
(b) Average latency of YCSB workload.
Figure 14: Performance of a transactional key-value store.
(c) Different transaction sizes.
YCSB has 70% throughput of uniform both for 1Pipe and NonTX.
In uniform workload that is free of contention, FaRM delivers 50%
throughput of 1Pipe because WR and WO TXNs need 3 or 4 RTTs.
In YCSB workload, FaRM does not scale because it locks keys for
2 RTTs during WO/WR TXN commit, and the hot keys block all
conflicting TXNs. In contrast, 1Pipe does not lock keys. Each server
processes TXNs on the same key in order.
In Figure 14b, we adjust the percentage of write ops and measure
latency of RO, WO, and WR TXNs with 512 processes. The latency
of 1Pipe is almost constant because servers process read and write
ops on the same key in order. WO and WR use reliable 1Pipe, which
is slower than RO that uses best effort 1Pipe. For pure RO workload,
FaRM has lower latency than 1Pipe because it completes in 1 RTT
and does not wait for network-wide reorder. Non-contended FaRM
WO and WR consumes 3 and 4 RTTs, respectively, which is slightly
worse than 1Pipe. However, with high write percentage, FaRM
latency skyrockets due to lock contention on hot keys and TXN
aborts due to inconsistent read version.
In Figure 14c, we alter the number of keys per TXN, and measure
total KV op/s with 512 processes. 95% of TXNs are read-only. 1Pipe
and NonTX are agnostic of TXN size because their throughputs are
only bounded by CPU processing and network messaging rate. With
a low write percentage (5%), FaRM/YCSB delivers 40% throughput
of 1Pipe with 2 KV ops per TXN, but the performance plummets
with larger TXN size, because TXN abort rate increases with the
number of keys in a TXN.
7.3.2
Independent General Transactions. Now we extend Dis-
tributed Atomic Operations (DAO, Sec.2.2.3) to two important
classes of distributed transactions: read-only snapshot transac-
tions [27] and independent transactions [51] (or called one-shot
transactions [56]) that involve multiple hosts but the input of each
host does not depend on the output of other hosts. The two most
frequent transactions in TPC-C benchmark (New-Order and Pay-
ment) [29] are independent transactions. The major difference be-
tween DAO and independent transactions is that the latter often
requires replication to ensure durability and fault tolerance. The
TXN initiator utilizes the method of Eris [51], which scatters op-
erations to all replicas of all shards in one reliable scattering. So,
each TXN can finish in one round-trip (actually two RTTs due to
Prepare phase). If a host fails, the other replicas of the same shard
reach quorum via traditional consensus.
We benchmark New-Order and Payment TXNs in TPC-C [29],
which constitute 90% of TPC-C workload. For simplicity, we do
not implement non-independent TXNs in TPC-C, which should
fall back to traditional concurrency control mechanisms. We use 4
warehouses which are stored in-memory with 3 replicas. Concur-
rency control and replication are implemented with a scattering
of commands sent to all shards and replicas, similar to Eris [51]
but replaces its central sequencer with timestamps. We assume
TXNs never abort. As shown in Figure 15a, two-phase locking (2PL)
and OCC do not scale, because each Payment TXN updates its
corresponding warehouse entry and each New-Order reads it [98],
leading to 4 hot entries. The throughput of OCC and 2PL reaches
peak at 256 and 64 processes, respectively. With more processes,
the throughput becomes lower [79]. In contrast, 1Pipe scales lin-
early with number of processes. With 512 processes, 1Pipe achieves
10.35M TXNs per second, which is 71% of a non-transactional base-
line system, 10x of lock and 17x of OCC.
Figure 15b shows TXN throughput under different simulated
packet loss rates. We fix process number to be 64. With 1Pipe,
although packet loss affects TXN latency (not measured in TPC-C,
but should be similar to Figure 9b), the impact on throughput is
insignificant. However, in 2PL and OCC commit, a locked object
cannot be released until the TXN completes, so, TXN throughput
under contention is inversely proportional to TXN latency. TXN
latency increases with packet loss rate because replicas wait for the
last retransmitted packet to maintain sequential log ordering.
Finally, we evaluate failure recovery of replicas by disconnecting
the physical link of a host. 1Pipe detects failure and removes the
replica in 181 ¬± 21ùúás. The affected TXNs are aborted and retried,
with an average delay of 308 ¬± 122ùúás. It is much faster than using
application heartbeats to detect failures, which takes milliseconds.
After the link reconnects, the replica synchronizes log from other
replicas in 25 ms.
7.3.3 Remote Data Structures. 1Pipe can remove ordering haz-
ards in remote data structure access (Sec.2.2.1). We implement a
distributed concurrent hash table that uses a linked list to store
key-value pairs (KVs) in the same hash bucket. The hash table is
sharded on 16 servers. Different from Sec.7.3.1 where servers pro-
cess KV ops, in this section, clients access the remote hash table
using RDMA one-sided read, write, and CAS. The baseline sys-
tem uses leader-follower replication. The workload has 16 parallel
clients and uniform keys.
As Figure 16 shows, without replication, 1Pipe improves per-
client KV insertion throughput to 1.9x because 1Pipe removes the
fence between writing KV pair and updating the pointer in hash
 0 0.5 1 1.5 2 2.51248163264128256512Tput per process (M txn/s)Number of Processes1Pipe/UnifFaRM/UnifNonTX/Unif1Pipe/YCSBFaRM/YCSBNonTX/YCSB232425262728292100.10.20.5125102050TXN latency (us, log)Percentage of Write Ops1Pipe-RO1Pipe-WO1Pipe-WRFaRM-ROFaRM-WOFaRM-WR 0 1 2 3 4 5248163264128256512Throughput (G KV op/s)Number of ops per TXN1Pipe/UnifFaRM/UnifNonTX/Unif1Pipe/YCSBFaRM/YCSBNonTX/YCSBSIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
Bojie Li, Gefei Zuo, Wei Bai, and Lintao Zhang
(a) Scalability.
(b) Resilience of packet loss.
Figure 15: TPC-C transaction benchmark.
bucket. KV lookup throughput reduces by 10%, due to additional
reordering delay. If the hash table is replicated traditionally, a write
op is sent to the leader, which involves CPU software to replicate to
followers. With 3 replicas, 1Pipe improves KV insertion throughput
to 3.4x. In 1Pipe, all KV operations are ordered by timestamp, so all
replicas can serve lookup requests, and the throughput scales with
number of replicas. In contrast, with leader-follower replication, to
maintain serializability of lookups and updates, only the leader can
serve lookups.
7.3.4 Replication in Distributed Storage. We apply 1Pipe to a
distributed storage system, Ceph [96]. Ceph OSD uses a primary-
backup replication scheme, where the backups are also written
sequentially. With 3 replicas, a client waits for 3 disk writes and 6
network messages (3 RTTs) in sequence. Because 1Pipe supports
1-RTT replication in non-failure cases (Sec.2.2.2), the client can
write 3 replicas in parallel, thus the end-to-end write latency is
reduced to 1 disk write and 1 RTT. Experiment shows that in an
idle system with Intel DC S3700 SSDs, the latency of 4KB random
write reduces from 160 ¬± 54ùúás to 58 ¬± 28ùúás (64% reduction).
8 RELATED WORK
Causal and totally ordered communication (CATOCS). Crit-
ics [25] and proponents [16, 94] of CATOCS have long discussed
the pros and cons of such a primitive. 1Pipe provides a scattering
primitive with restricted atomicity, and achieves scalability with
in-network computation that incurs little overhead, thus remov-
ing two criticisms (can‚Äôt say together and efficiency). Scattering
also enables atomic access to a portion (rather than all) of shards.
Although 1Pipe is not a panacea for ordering problems, e.g., it is
not sufficient to support serializable general transactions, it shows
effectiveness in applications in Sec.7.3.
There has been extensive research on total order broadcast [31].