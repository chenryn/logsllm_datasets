however, missing the null terminator. 
Generation  78: The GA learns that a null terminator 
must  follow  the  opcode  and  filename  bytes.    At  this 
point  we  have  generated  a  packet  with  both  a  valid 
opcode and an acceptable null terminated filename. 
Generation 111: We evolve yet another valid opcode 
(read request == “0002”) and learn that it is valid for 
the filename to be a null string.  
Generation  393: We evolve a packet with an invalid 
mode  string  (the  string  is  correctly  null  terminated, 
however, it represents an invalid mode). 
Generation  547: The  genetic algorithm  finds  a  valid 
packet  structure  with  a  opcode,  filename,  and  mode 
strings, all in the right positions relative to each other. 
successive  generations, 
The  evolutionary  process  outlined above  is  consistent 
with the process observed in all of our trial runs.  Our 
GA  begins  with  no  information  about  the  packet 
structure,  but  over 
it 
incrementally learns what an accepted tftp packet input 
looks like.  Typical running times were on the order of 
20-30  minutes.    If  we  allow  our  GA  to  continue 
running, it will quickly generate many unique packets 
conforming  to  a  valid  structure  accepted  by  the  tftp 
program.    By  collecting  enough  of  them,  it  may  be 
possible for us to generate an approximate context-free 
grammar describing the tftp packet specification.   
functions  exist  within 
Figure  7.  Graph  generated  by  our  research  tool 
during  exploration  of  the  tftpd  server  program's 
protocol  parsing  logic.    White  nodes  correspond  to 
nodes which exist on some path between the source 
and  destination  nodes.    Black  nodes  correspond  to 
“reject”  nodes  (nodes  from  which  it  is  no  longer 
possible  to  reach  the  destination  node).  The  labeled 
“source” node roughly corresponds to the start of the 
tftp  server  program’s  protocol  parse  code  and  the 
labeled  “target”  node,  to  the  code  block  indicating 
acceptance  of  the  tftp  packet  structure.    Note  that  2 
vulnerable  strcpy() 
this 
subgraph.    Both  were  hit  during  our  GA  driven 
search. 
write request packet, we were able to test the ability of 
our GA to learn a valid tftp packet structure.  The tftp 
packet  header  format  is  relatively  simple. 
  The 
minimum header length is 4 bytes and it consists of the 
following fields:  
•  Opcode – The protocol supports 5 opcodes ( 1=Read 
request, 
3=Data, 
4=Acknowledgement,  5=Error  ).  This  is  a  2  byte 
field. 
2=Write 
request, 
•  Filename – A variable length ascii sequence. 
•  Null Byte – Null byte following the filename. 
•  Mode – Contains one of the three strings: “netascii”, 
“octet”,  or  “mail” in any combination  of upper  and 
lower case. 
•  Null Byte – Null byte following the mode string. 
     As you can see in Figure 9, we track the evolution 
of our GA during the successful generation of a valid 
484484
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:40:16 UTC from IEEE Xplore.  Restrictions apply. 
is 
technique 
limited  by 
5. Discussion & Limitations 
     Our 
the  quantity  of 
information  embedded  in  the  test  program’s  control 
flow  graph  structure.  This is  due  to  the  fact  that  we 
treat each node on the graph as a black box and judge 
fitness  solely  based  upon  runtime  execution  path 
information.  In  a  sense,  we  are  performing  an 
"intelligent  ",  distributed  brute  force  search  for  the 
constraints guarding the execution of each node in the 
control flow graph.  Rather than having to satisfy all of 
the constraints on a given path simultaneously as in a 
random fuzz, we are able to tackle them one at a time.  
Thus,  it  is  most  useful  for  code  containing  a  rich, 
deeply  nested  control  flow  structure  (e.g.  like  parser 
code)  and  will  degenerate  to  a  random  bruteforce  on 
flat control flow graphs.   
     Although  an  improvement  over random  input  data 
generation, our technique still suffers from some of the 
weaknesses inherent to all black box tools. We are able 
to selectively test interesting regions of program logic 
and  improve  our  rate  of  exploration  over  traditional 
black box tests, but we cannot guarantee that either a 
certain  rate  of  coverage  will  occur  or  specific 
destination node will be reached.  While we have the 
capability to reduce the input search space, it can still 
remain  quite  large  and perform  poorly  for  constraints 
involving equality tests.  Equality tests are more suited 
"white box" fuzzer's constraint solver.  
     A closely related limitation concerns our extraction 
of  the  control  flow  graph  information.    Because  we 
rely  upon  a  valid,  static  disassembly  to  obtain  the 
control flow graph, we cannot apply our technique to 
programs  that  have  been  compressed,  encrypted,  or 
otherwise obfuscated.  We also may miss control flow 
information that is determined at runtime (e.g. runtime 
calculations of an index into a call table).  Though our 
initial  results  are  promising,  we  need  to  perform 
additional tests to see how well our methodology will 
scale to larger programs and more complex protocols.  
Finally,  the  approach  requires  a  human  analyst  to 
identify  an  initial  source  /  destination  pair  describing 
the  region  of  code  to  be  tested.    In  the  future,  this 
selection  might  be  able  to  be  partially  automated  (by 
suggesting  regions  around  known  vulnerable  API 
functions, for example). 
     Some may argue that new "white box" fuzz testing 
tools  will  quickly  render  black  box  approaches 
obsolete.    We  do  not  feel  that  is  the  case.    While 
automated "white box" testing tools theoretically have 
the  ability  to  test  all  program  paths,  they  suffer  from 
practical  limitations.    As  a  result,  they  are  likely  to 
retain  a  place  in  the  software  vulnerability  testing 
process for quite some time to come. 
researchers 
reasoned 
that 
code 
coverage 
first  presented 
achieving  good 
testing  approach 
6. Related Work 
     A number of researchers have done work in the area 
of fuzz testing.   In the early 1990’s Barton Miller et al. 
[11] 
the  “fuzzing”  concept  by 
performing  tests  on  UNIX  applications  with  random 
inputs. [4] presented fuzz testing on Windows NT GUI 
based applications. Building upon Miller’s work, later 
researchers successfully applied fuzzing to other forms 
of  input,  like  network  protocols  and  popular  file 
formats.    Random  input  injection  has  resulted  in  the 
discovery of subtle parsing errors leading to dangerous 
vulnerabilities.  Unfortunately,  black  box  fuzzers have 
difficulty 
and 
penetration depth into a program’s control flow logic.  
Later 
incorporating 
knowledge  of  the  protocol  into  the  input  selection 
process  might  be  more  effective 
than  supplying 
entirely  random  input.    Thus,  the  idea  of  using  a 
partially  specified  or  semi-random  input  structure 
emerged [8]. 
     Recently,  “white  box”  fuzzers  have  emerged  onto 
the  automated  vulnerability  analysis  scene.    Some  of 
the early work in this area was performed by Cader et 
al.  and  published  in  the  paper  “EXE:  Automatically 
Generating  Inputs  of  Death”  [CGP+06].    The  white 
box  fuzz 
involves  symbolically 
running  an  application  and  solving  constraints  its 
control graph.  The generated constraints are then used 
to  produce  new  inputs  that  enable  the  program  to 
explore new  paths.   The  DART  (Directed  Automated 
Random  Testing)  and  SAGE  (Scalable,  Automated, 
and  Guided  Execution)  projects  are  also  based  upon 
this  idea  [6]  [5].    In  theory,  such  “white  box  fuzz 
testing”  approaches  seem  hard  to  beat.    They  are, 
however, constrained by some practical limitations [5]. 
These  include  path  explosion,  imperfect  symbolic 
executions, and performance bottlenecks relating to the 
computational expense of constraint solving [5]. They 
are also substantially much more complex to develop.  
     There has also been research on the applications of 
evolutionary  computation 
testing 
domain.  Cheon  and  Kim  proposed  a  specification 
based  fitness  function  for  testing  object  oriented 
programs [1].  Khor and Grogono proposed using data 
dependency  analysis  to  automatically  generate  branch 
coverage test data. [10].  Finally, Minn and Holcombe 
discussed 
the  concept  of 
“chaining” in the design of a genetic algorithm based 
test data generator [12].  Among these existing fitness 
functions,  we  believe  our  application  of  a  Dynamic 
Markov Model heuristic to the problem of guided input 
selection  is  unique  and  potentially  beneficial  to  the 
development of future automated vulnerability analysis 
tools. 
the  applications 
to 
the  software 
for 
485485
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:40:16 UTC from IEEE Xplore.  Restrictions apply. 
Microsoft Research, May 2007. 
[6] P. Godefroid, N. Klarlund, and K. Sen. DART: Directed 
Automated  Random  Testing. 
In  Proceedingsof 
PLDI’2005  (ACM  SIGPLAN  2005  Conference  on 
Programming  Language  Design  and  Implementation), 
pages 213–223, Chicago, June 2005. 
[7] Hoglund, G., and McGraw, G. 2004. Exploiting Software: 
How to Break Code. Boston, Mass.:Addison-Wesley. 
data 
[8] A. Helin, J. Viide, M. Laakso, J. Röning. Model Inference 
Guided  Random  Testing  of  Programs  with  Complex 
Input Domains. 2006. 
[9] IDA Pro. Web page: http://www.datarescue.com/ 
[10] S. Khor and P. Grogono. Using a genetic algorithm and 
formal  concept  analysis  to  generate  branch  coverage 
test 
Software 
Engineering,  2004.  Proceedings.  19th  International 
Conference on, pages 346-349, 2004. 
automatically.  Automated 
[11]  B.P.  Miller,  L.  Fredriksen,  and  B.  So.  An  Empirical 
Study 
of  UNIX  Utilities. 
Communications of the ACM 33, 12 (December 1990). 
the  Reliability 
[12]  P. McMinn  and M.  Holcombe.  Evolutionary  testing of 
state-based  programs.  Proceedings  of 
the  2005 
conference  on  Genetic  and  evolutionary  computation, 
pp. 1013-1020, 2005. 
[13] Nist. Web page: http://nvd.nist.gov/ 
[14]  Pedram  A.  PaiMei  -  Reverse  Engineering  Framework, 
of 
RECON Conference 2006. 
[15]  Markov  chain.  Web  page:  http://en.wikipedia.org/   
wiki/Markov_chain 
[16]  C.  Ryan,  J.J.  Collins,  M.  O'Neill.  Grammatical 
Evolution.  Evolving  Programs 
for  an  Arbitrary 
Language.  Lecture  Notes  in  Computer  Science  1391. 
First  European  Workshop  on  Genetic  Programming 
1998. 
[17]  Wikipedia.  Trivial  File  Transfer  Protocol.  Web  page: 
http://en.wikipedia.org/wiki/Tftp 
7. Conclusions & Future Work 
     In  this  paper,  we  have  discussed  a new  black  box 
fuzzing  methodology  based  upon  a  dynamic  Markov 
Model  heuristic.  Our  experiments  validated  our 
approach.    We  also  demonstrated  that  this  approach 
can be implemented as a plug in for a commonly used 
reverse engineering framework, run on an inexpensive 
platform in a very modest amount of time, and produce 
practical results on a commercial server application.  It 
consistently outperformed a random fuzzer, especially 
for  greater  control  flow  penetration  depths.    Our 
research  incorporates  ideas  from  machine  learning, 
statistical theory, static and dynamic software analysis, 
and  reverse  engineering.  Because  of  this,  it  benefits 
from the synergy of a truly interdisciplinary approach 
and  bridges  a  gap  between  theoretical  and  industrial 
security research. 
     There  is  still  much  to  be  done  to  practically  and 
cost-effectively deploy our system. Also, it is possible 
to extend our work to other problem domains. Here, we 
list some possible extensions to this research: 
•  Testing using other applications and protocols. 
•  Automating  grammar  generation  by  deriving 
the  strings 
grammar  production 
rules 
contained in a target binary. 
•  Extending  our  approach  to handle  protocols  with 
from 
state information (for example, a handshake) 
•  Because  we  can  generate  multiple,  unique inputs 
capable of crashing the program, we could extend 
our 
intrusion  detection  by  creating 
signatures based on those inputs. 
tool  for 
7. References 
[1]  Y.  Cheon  and  M.  Kim.  A  specication-based  fitness 
function  for  evolutionary  testing  of  object  oriented 
programs. Proceedings of the 8th annual conference on 
Genetic  and  evolutionary  computation,  pp.  1953-1954, 
2006. 
[2]  R.C.  Collins  and  J.J.  O'Neill.  Grammatical  Evolution: 
Evolving Programs for an Arbitrary Language. Lecture 
Notes  in  Computer  Science  1391.  First  European 
Workshop on Genetic Programming, 1998. 
[3]  A.  Ethem.  Introduction  to  Machine  Learning.  Boston, 
Mass.:MIT Press, 2004. 
[4] J.E. Forrester and B.P. Miller, An Empirical Study of the 
Robustness of Windows NT Applications Using Random 
Testing.  4th  USENIX  Windows  Systems  Symposium, 
Seattle, August 2000.  
[5]  P.  Godefroid,  M.  Levin,  D.  Molnar.  2007.  Automated 
Whitebox Fuzz Testing. Technical Report: 
486486
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:40:16 UTC from IEEE Xplore.  Restrictions apply.