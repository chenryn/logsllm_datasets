### Optimized Text

**However, the packet is missing the null terminator.**

- **Generation 78:** The genetic algorithm (GA) learns that a null terminator must follow the opcode and filename bytes. At this point, we have generated a packet with both a valid opcode and an acceptable null-terminated filename.
  
- **Generation 111:** The GA evolves yet another valid opcode (read request == "0002") and learns that it is valid for the filename to be a null string.

- **Generation 393:** The GA evolves a packet with an invalid mode string. Although the string is correctly null-terminated, it represents an invalid mode.

- **Generation 547:** The GA finds a valid packet structure with an opcode, filename, and mode strings, all in the correct positions relative to each other.

**In successive generations, the evolutionary process outlined above is consistent with the process observed in all of our trial runs.** Initially, the GA has no information about the packet structure, but over time, it incrementally learns what an accepted TFTP packet input looks like. Typical running times were on the order of 20-30 minutes. If allowed to continue running, the GA will quickly generate many unique packets conforming to a valid structure accepted by the TFTP program. By collecting enough of these, it may be possible to generate an approximate context-free grammar describing the TFTP packet specification.

**Figure 7.** Graph generated by our research tool during the exploration of the TFTP server program's protocol parsing logic. White nodes correspond to nodes that exist on some path between the source and destination nodes. Black nodes correspond to "reject" nodes (nodes from which it is no longer possible to reach the destination node). The labeled "source" node roughly corresponds to the start of the TFTP server program’s protocol parse code, and the labeled "target" node corresponds to the code block indicating acceptance of the TFTP packet structure. Note that two vulnerable `strcpy()` functions exist within this subgraph, both of which were hit during our GA-driven search.

**Using a write request packet, we tested the ability of our GA to learn a valid TFTP packet structure.** The TFTP packet header format is relatively simple, with a minimum header length of 4 bytes. It consists of the following fields:

- **Opcode:** The protocol supports 5 opcodes (1=Read request, 2=Write request, 3=Data, 4=Acknowledgment, 5=Error). This is a 2-byte field.
- **Filename:** A variable-length ASCII sequence.
- **Null Byte:** A null byte following the filename.
- **Mode:** Contains one of the three strings: "netascii", "octet", or "mail" in any combination of upper and lower case.
- **Null Byte:** A null byte following the mode string.

**As shown in Figure 9, we track the evolution of our GA during the successful generation of a valid packet.**

### 5. Discussion & Limitations

Our technique is limited by the quantity of information embedded in the test program’s control flow graph (CFG) structure. We treat each node on the graph as a black box and judge fitness solely based on runtime execution path information. In a sense, we are performing an "intelligent," distributed brute-force search for the constraints guarding the execution of each node in the CFG. Unlike random fuzzing, which requires satisfying all constraints on a given path simultaneously, our approach tackles them one at a time. Thus, it is most useful for code containing a rich, deeply nested control flow structure (e.g., parser code) and will degenerate to a random brute force on flat control flow graphs.

Although an improvement over random input data generation, our technique still suffers from some of the weaknesses inherent to all black-box tools. We can selectively test interesting regions of program logic and improve our rate of exploration over traditional black-box tests, but we cannot guarantee a certain rate of coverage or that a specific destination node will be reached. While we can reduce the input search space, it can still remain quite large, especially for constraints involving equality tests, which are more suited to a "white-box" fuzzer's constraint solver.

A closely related limitation concerns our extraction of the control flow graph information. Because we rely on a valid, static disassembly to obtain the CFG, we cannot apply our technique to programs that have been compressed, encrypted, or otherwise obfuscated. We may also miss control flow information determined at runtime (e.g., runtime calculations of an index into a call table). Though our initial results are promising, we need to perform additional tests to see how well our methodology scales to larger programs and more complex protocols. Finally, the approach requires a human analyst to identify an initial source/destination pair describing the region of code to be tested. In the future, this selection might be partially automated (e.g., by suggesting regions around known vulnerable API functions).

Some may argue that new "white-box" fuzz testing tools will quickly render black-box approaches obsolete. We do not believe this is the case. While automated "white-box" testing tools theoretically have the ability to test all program paths, they suffer from practical limitations. As a result, they are likely to retain a place in the software vulnerability testing process for quite some time to come.

### 6. Related Work

Several researchers have done work in the area of fuzz testing. In the early 1990s, Barton Miller et al. [11] introduced the "fuzzing" concept by performing tests on UNIX applications with random inputs. [4] presented fuzz testing on Windows NT GUI-based applications. Building upon Miller’s work, later researchers successfully applied fuzzing to other forms of input, such as network protocols and popular file formats. Random input injection has resulted in the discovery of subtle parsing errors leading to dangerous vulnerabilities. Unfortunately, black-box fuzzers have difficulty achieving good code coverage and penetration depth into a program’s control flow logic. Later, incorporating knowledge of the protocol into the input selection process was found to be more effective than supplying entirely random input, leading to the idea of using a partially specified or semi-random input structure [8].

Recently, "white-box" fuzzers have emerged in the automated vulnerability analysis scene. Some of the early work in this area was performed by Cader et al. and published in the paper “EXE: Automatically Generating Inputs of Death” [CGP+06]. The white-box fuzzing approach involves symbolically running an application and solving constraints in its control graph. The generated constraints are then used to produce new inputs that enable the program to explore new paths. The DART (Directed Automated Random Testing) and SAGE (Scalable, Automated, and Guided Execution) projects are also based on this idea [6][5]. In theory, such "white-box fuzz testing" approaches seem hard to beat, but they are constrained by practical limitations [5], including path explosion, imperfect symbolic executions, and performance bottlenecks related to the computational expense of constraint solving. They are also substantially more complex to develop.

There has also been research on the applications of evolutionary computation in the software testing domain. Cheon and Kim proposed a specification-based fitness function for testing object-oriented programs [1]. Khor and Grogono proposed using data dependency analysis to automatically generate branch coverage test data [10]. Finally, Minn and Holcombe discussed the concept of "chaining" in the design of a genetic algorithm-based test data generator [12]. Among these existing fitness functions, we believe our application of a Dynamic Markov Model heuristic to the problem of guided input selection is unique and potentially beneficial to the development of future automated vulnerability analysis tools.

### 7. Conclusions & Future Work

In this paper, we have discussed a new black-box fuzzing methodology based on a dynamic Markov Model heuristic. Our experiments validated our approach. We also demonstrated that this approach can be implemented as a plug-in for a commonly used reverse engineering framework, run on an inexpensive platform in a very modest amount of time, and produce practical results on a commercial server application. It consistently outperformed a random fuzzer, especially for greater control flow penetration depths. Our research incorporates ideas from machine learning, statistical theory, static and dynamic software analysis, and reverse engineering. Because of this, it benefits from the synergy of a truly interdisciplinary approach and bridges a gap between theoretical and industrial security research.

There is still much to be done to practically and cost-effectively deploy our system. Additionally, it is possible to extend our work to other problem domains. Here, we list some possible extensions to this research:

- **Testing using other applications and protocols.**
- **Automating grammar generation by deriving the strings and grammar production rules contained in a target binary.**
- **Extending our approach to handle protocols with state information (e.g., a handshake).**
- **Enhancing intrusion detection by creating signatures based on the multiple, unique inputs capable of crashing the program.**

### References

[1] Y. Cheon and M. Kim. A specification-based fitness function for evolutionary testing of object-oriented programs. Proceedings of the 8th annual conference on Genetic and evolutionary computation, pp. 1953-1954, 2006.

[2] R.C. Collins and J.J. O'Neill. Grammatical Evolution: Evolving Programs for an Arbitrary Language. Lecture Notes in Computer Science 1391. First European Workshop on Genetic Programming, 1998.

[3] A. Ethem. Introduction to Machine Learning. Boston, Mass.: MIT Press, 2004.

[4] J.E. Forrester and B.P. Miller, An Empirical Study of the Robustness of Windows NT Applications Using Random Testing. 4th USENIX Windows Systems Symposium, Seattle, August 2000.

[5] P. Godefroid, M. Levin, D. Molnar. 2007. Automated Whitebox Fuzz Testing. Technical Report: Microsoft Research, May 2007.

[6] P. Godefroid, N. Klarlund, and K. Sen. DART: Directed Automated Random Testing. In Proceedings of PLDI’2005 (ACM SIGPLAN 2005 Conference on Programming Language Design and Implementation), pages 213–223, Chicago, June 2005.

[7] Hoglund, G., and McGraw, G. 2004. Exploiting Software: How to Break Code. Boston, Mass.: Addison-Wesley.

[8] A. Helin, J. Viide, M. Laakso, J. Röning. Model Inference Guided Random Testing of Programs with Complex Input Domains. 2006.

[9] IDA Pro. Web page: http://www.datarescue.com/

[10] S. Khor and P. Grogono. Using a genetic algorithm and formal concept analysis to generate branch coverage test data automatically. Automated Software Engineering, 2004. Proceedings. 19th International Conference on, pages 346-349, 2004.

[11] B.P. Miller, L. Fredriksen, and B. So. An Empirical Study of UNIX Utilities. Communications of the ACM 33, 12 (December 1990).

[12] P. McMinn and M. Holcombe. Evolutionary testing of state-based programs. Proceedings of the 2005 conference on Genetic and evolutionary computation, pp. 1013-1020, 2005.

[13] NIST. Web page: http://nvd.nist.gov/

[14] Pedram A. PaiMei - Reverse Engineering Framework, RECON Conference 2006.

[15] Markov chain. Web page: http://en.wikipedia.org/wiki/Markov_chain

[16] C. Ryan, J.J. Collins, M. O'Neill. Grammatical Evolution. Evolving Programs for an Arbitrary Language. Lecture Notes in Computer Science 1391. First European Workshop on Genetic Programming 1998.

[17] Wikipedia. Trivial File Transfer Protocol. Web page: http://en.wikipedia.org/wiki/Tftp