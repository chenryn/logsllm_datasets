### Spatial Distribution of Lane Degrades and Link Inactive Errors

**Fig. 4: Spatial distribution of lane degrades inside and across cabinets.**  
Due to the folded 3D-torus design, cross-cabinet links connect to alternate cabinets.

**Per-cabinet distribution of link inactive errors count:**

| Cabinet Rows | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 |
|--------------|---|---|---|---|---|---|---|---|
| Cabinets     | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |

**Fig. 5: Spatial distribution of link inactive errors inside and across cabinets.**  
Due to the folded 3D-torus design, cross-cabinet links connect to alternate cabinets.

### Analysis of Spatial Distribution

We applied the Kolmogorov-Smirnov test (K-S test) to determine if the spatial distribution of lane degrades per cabinet follows a uniform distribution. The test results show a D-statistic of 1 and a p-value of 2.2e-16. For our sample size of 200 cabinets, the critical D-value for a 0.05 level of significance is 0.0960. Therefore, we can reject the null hypothesis that the sample is taken from a uniform distribution. This indicates that the spatial distribution of lane degrades per cabinet is significantly different from uniform. This behavior can be attributed to various factors, including external transient effects, overloading of links, uneven usage, and complex interactions between applications and the interconnect network.

Interestingly, we observed that the hot spots for links within the cabinet are not the same as those for links crossing cabinet boundaries. Additionally, when comparing the hot spots of lane degrades with those of link inactive errors (Fig. 4 vs. Fig. 5), we found that they do not necessarily match. This explains why their high-intensity periods do not align (Fig. 2). Consequently, it is not possible to determine the location of link inactive/failed errors by only observing the time and location of lane degrade events.

### Investigation of Other Interconnect Errors

Next, we investigated other interconnect errors, including:
- **Bad Send EOP error:** Each packet in the Gemini Interconnect has a single phit end-of-packet that contains the last phit of a packet and status bits for error handling. If a packet is corrupted, the end-of-packet is marked as bad and will be discarded at its destination.
- **Send Packet Length error:** This error occurs when the length of a packet does not match the expected length value at the destination.
- **Routing Table Corruption error:** A routing table is a data table stored in a router that contains the information necessary to forward a packet along the best path toward its destination. Routing table corruption results in link failure and eventually causes network congestion.
- **HSN ASIC LCB lanes reinit failed error:** This error occurs when all 256 attempts to bring up a downgraded lane are exhausted.

These errors also show hot spots in and across cabinets, although their magnitude is relatively small. For example, the Routing Table Corruption error occurs only 200 times, while the HSN ASIC LCB lane(s) reinit failed error happens only 187 times throughout the entire observation period. On deeper investigation, we found that most of these errors are highly correlated with link inactive/failed errors. Table II shows the correlation of these interconnect errors with link inactive errors, indicating that link inactive errors can be used to predict other interconnect errors. We also found that more than 80% of link failed errors lead to Bad Send EOP, Send Packet Length, and Routing Table Corruption errors. The HSN ASIC LCB lane(s) reinit failed error has a weak correlation with link failed errors, which can be explained by our previous findings that lane degrades and link failed errors are not correlated, and ASIC errors are an outcome of failed repair attempts of lane degrades.

### Analysis of Network Congestion

Understanding network congestion in conjunction with interconnect errors is important, as one may cause the other. A daemon on the compute cluster monitors the percentage of time that network tiles are stalled due to increased traffic or other reasons. When these values exceed a set threshold, the daemon communicates this data to the xtnlrd daemon running on the SMW. After the congestion subsides, the daemon again passes this information to the SMW.

**Characteristics of Network Throttle Events:**
- **Fig. 6 (left):** Plots the network throttle events over time. We note that a large fraction of throttle events occur in a short period of time. Each throttle event typically lasts 20-30 seconds but can extend up to a few minutes depending on the magnitude of the congestion.
- **Fig. 6 (right):** Shows that network throttle events can be quite bursty. An application causing network congestion can induce multiple throttles in a very short amount of time (< 20 mins).
- **Fig. 7:** Shows the network throttle events over time, counting only one event at maximum per hour. We experimented with multiple time windows and found that a 1-hour time window removes the skewness. However, this type of time window filtering cannot completely remove the skewness, as a long-running communication-intensive application may cause multiple throttle events over multiple hours.

**Temporal Correlation:**
We hypothesized that lane degrades/failures may induce network throttle events or vice versa. We investigated the possibility of temporal correlation between the time series of throttle events and interconnect errors, particularly lane degrades and link failed events. The Spearman correlation coefficient was found to be very weak (0.03), indicating that lane degrades/failures alone cannot be used to predict throttling events.

**Heatmap of Throttled Blades:**
- **Fig. 9:** Heatmap without any filtering.
- **Fig. 10:** Heatmap with 1-hour filtering.

We observe that not all blades are throttled equally over the period of observation. Hot spots remain similar even after applying the filter. We conducted the K-S test to determine if the spatial distribution of blades throttle events per cabinet, both without and with one hour filtering, follows a uniform distribution. The test results show a D-statistic of 1 and a p-value of 2.2e-16 in both cases. For our sample size of 200 cabinets, the critical D-value for a 0.05 level of significance is 0.0960, allowing us to reject the null hypothesis. This indicates that the spatial distribution of blade throttled events per cabinet is significantly different from uniform.

### Node-Level Congestion Data

Node-level congestion data provides information about the nodes that are heavily congested at the time of throttling. Figs. 11 and 12 show the spatial distribution of congested nodes in the Titan supercomputer for the no-filter and 1-hour filter cases, respectively. We make two observations:
1. Some nodes are much more congested than others, as shown by the uneven distribution of congested nodes. This is because the applications creating significant network traffic may be concentrated in specific areas.
2. The top 10 congested nodes are calculated for each throttle event, and this plot is aggregated over all throttle events.

**Distribution of Unique Applications:**
- **Fig. 13 (left):** Distribution of unique applications over congested node events without filtering.
- **Fig. 13 (right):** Distribution of unique applications with 1-hour filtering.

This analysis helps in understanding the role of congestion information at the node and application levels, improving our understanding of congestion behavior at the blade level.