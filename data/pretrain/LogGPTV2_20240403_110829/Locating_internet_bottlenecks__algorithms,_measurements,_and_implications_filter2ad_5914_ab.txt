bandwidth on the bottleneck link. As a result, a downstream link
with slightly higher available bandwidth could also be identiﬁed as
a choke point and our ranking algorithm will mistakenly select it as
the bottleneck.
Note that our method of calculating the packet train transmission
rate R is similar to that used by cprobe [13]. The difference is that
cprobe estimates available bandwidth, while Pathneck estimates the
location of the bottleneck link. Estimating available bandwidth in
fact requires careful control of the inter-packet gap for the train [26,
18] which neither tool provides.
While Pathneck does not measure available bandwidth, we can
use the average per-hop gap values to provide a rough upper or
lower bound for the available bandwidth of each link. We consider
three cases:
Case 1: For a choke link, i.e., its gap increases, we know that the
available bandwidth is less than the packet train rate. That is, the
rate R computed above is an upper bound for the available band-
width on the link.
Case 2: For links that maintain their gap relative to the previous
link, the available bandwidth is higher than the packet train rate R,
and we use R as a lower bound for the link available bandwidth.
Case 3: Some links may see a decrease in gap value. This decrease
is probably due to temporary queuing caused by trafﬁc burstiness,
and according to the packet train model discussed in [18], we can-
not say anything about the available bandwidth.
Considering that the data is noisy and that link available bandwidth
is a dynamic property, these bounds should be viewed as very rough
estimates. We provide a more detailed analysis for the bandwidth
bounds on the bottleneck link in Section 3.3.
2.4 Pathneck Properties
Pathneck meets the design goals we identiﬁed earlier in this sec-
tion. Pathneck does not need cooperation of the destination, so it
can be widely used by regular users. Pathneck also has low over-
head. Each measurement typically uses 6 to 10 probing trains of
30 to 100 load packets each. This is a very low overhead com-
pared to existing tools such as pathchar [19] and BFind [10]. Fi-
nally, Pathneck is fast. For each probing train, it takes about one
roundtrip time to get the result. However, to make sure we re-
ceive all the returned ICMP packets, Pathneck generally waits for
3 seconds — the longest roundtrip time we have observed on the
Internet — after sending out the probing train, and then exits. Even
in this case, a single probing takes less than 5 seconds. In addi-
tion, since each packet train probes all links, we get a consistent set
of measurements. This, for example, allows Pathneck to identify
multiple choke points and rank them. Note however that Pathneck
is biased towards early choke points— once a choke point has in-
creased the length of the packet train, Pathneck may no longer be
able to “see” downstream links with higher or slightly lower avail-
able bandwidth.
A number of factors could inﬂuence the accuracy of Pathneck.
First, we have to consider the ICMP packet generation time on
routers. This time is different for different routers and possibly
for different packets on the same router. As a result, the measured
gap value for a router will not exactly match the packet train length
at that router. Fortunately, measurements in [16] and [11] show
that the ICMP packet generation time is pretty small; in most cases
it is between 100µs and 500µs. We will see later that over 95%
of the gap changes of detected choke points in our measurements
are larger than 500µs. Therefore, while large differences in ICMP
generation time can affect individual probings, they are unlikely to
signiﬁcantly affect Pathneck bottleneck results.
Second, as ICMP packets travel to the source, they may experi-
ence queueing delay caused by reverse path trafﬁc. Since this delay
can be different for different packets, it is a source of measurement
error. We are not aware of any work that has quantiﬁed reverse path
effects. In our algorithm, we try to reduce the impact of this factor
by ﬁltering out the measurement outliers. Note that if we had ac-
cess to the destination, we might be able to estimate the impact of
reverse path queueing.
Third, packet loss can reduce Pathneck’s effectiveness. Load
packet loss can affect RPT’s ability to interleave with background
trafﬁc thus possibly affecting the correctness of the result. Lost
measurement packets are detected by lost gap measurements. Note
that it is unlikely that Pathneck would lose signiﬁcant numbers of
load packets without a similar loss of measurement packets. Con-
sidering the low probability of packet loss in general [23], we do
not believe packet loss will affect Pathneck results.
Fourth, multi-path routing, which is sometimes used for load bal-
ancing, could also affect Pathneck.
If a router forwards packets
in the packet train to different next-hop routers, the gap measure-
ments will become invalid. Pathneck can usually detect such cases
by checking the source IP address of the ICMP responses. In our
measurements, we do not use the gap values in such cases.
Pathneck also has some deployment limitations. First, we dis-
covered that network ﬁrewalls often only forward 60 byte UDP
packets that strictly conform to the packet payload format used
by standard Unix traceroute implementation, while they drop any
other UDP probing packets, including the load packets in our RPT.
If the sender is behind such a ﬁrewall, Pathneck will not work. Sim-
ilarly, if the destination is behind a ﬁrewall, no measurements for
links behind the ﬁrewall can be obtained by Pathneck. Second, even
without any ﬁrewalls, Pathneck may not be able to measure the
packet train length on the last link, because the ICMP packets sent
by the destination host cannot be used. In theory, the destination
should generate a “destination port unreachable” ICMP message
for each packet in the train. However, due to ICMP rate limiting,
the destination network system will typically only generate ICMP
packets for some of the probing packets, which often does not in-
clude the tail packet. Even if an ICMP packet is generated for both
the head and tail packets, the accumulated ICMP generation time
for the whole packet train makes the returned interval worthless.
Of course, if we have the cooperation of the destination, we can
get a valid gap measurement for the last hop by using a valid port
number, thus avoiding the ICMP responses for the load packets.
3. VALIDATION
We use both Internet paths and the Emulab testbed [3] to evalu-
ate Pathneck. Internet experiments are necessary to study Pathneck
with realistic background trafﬁc, while the Emulab testbed provides
a fully controlled environment that allows us to evaluate Pathneck
Table 1: Bottlenecks detected on Abilene paths.
d rate
(Utah/CMU)
0.71/0.70
0.64/0.67
0.62/0.56
0.71/0.72
Probe
AS path
†
(AS1-AS2)
destination
§
calren2
2150-2150
§
princeton
10466-10466
§
sox
10490-10490
§
ogig
210-4600 (Utah)
11537-4600 (CMU)
† AS1 is bottleneck router’s AS#, AS2 is its post-hop router’s AS#.
§ calren = www.calren2.net, princeton = www.princeton.edu,
§ sox = www.sox.net, ogig = www.ogig.net.
Bottleneck
router IP
137.145.202.126
198.32.42.209
199.77.194.41
205.124.237.10
198.32.8.13
with known trafﬁc loads. Besides the detection accuracy, we also
examine the accuracy of the Pathneck bandwidth bounds and the
sensitivity of Pathneck to its conﬁguration parameters. Our valida-
tion does not study the impact of the ICMP generation time1.
3.1 Internet Validation
For a thorough evaluation of Pathneck on Internet paths, we
would need to know the actual available bandwidth on all the links
of a network path. This information is impossible to obtain for most
operational networks. The Abilene backbone, however, publishes
its backbone topology and trafﬁc load (5-minute SNMP statistics)
[1], so we decided to probe Abilene paths.
The experiment is carried out as follows. We used two sources: a
host at the University of Utah and a host at Carnegie Mellon Univer-
sity. Based on Abilene’s backbone topology, we chose 22 probing
destinations for each probing source. We make sure that each of the
11 major routers on the Abilene backbone is included in at least one
probing path. From each probing source, we probed every destina-
tion 100 times, with a 2-second interval between two consecutive
probings. To avoid interference, the experiments conducted at Utah
and at CMU were run at different times.
Using conf ≥ 0.1 and d rate ≥ 0.5, we only detected 5 non-
ﬁrst-hop bottleneck links on the Abilene paths (Table 1). This is not
surprising since Abilene paths are known to be over-provisioned,
and we selected paths with many hops inside the Abilene core. The
d rate values for the 100 probes originating from Utah and CMU
are very similar, possibly because they observed similar congestion
conditions. By examining the IP addresses, we found that in 3 of
the 4 cases (www.ogig.net is the exception), both the Utah and
CMU based probings are passing through the same bottleneck link
close to the destination; an explanation is that these bottlenecks are
very stable, possibly because they are constrained by link capacity.
Unfortunately, all three bottlenecks are outside Abilene, so we do
not have the load data.
For the path to www.ogig.net, the bottleneck links appear
to be two different peering links going to AS4600. For the path
from CMU to www.ogig.net, the outgoing link of the bottle-
neck router 198.32.163.13 is an OC-3 link. Based on the link
capacities and SNMP data, we are sure that the OC-3 link is in-
deed the bottleneck. We do not have the SNMP data for the Utah
links, so we cannot validate the results for the path from Utah to
www.ogig.net.
3.2 Testbed Validation
We use the Emulab testbed to study the detailed properties of
Pathneck. Since Pathneck is a path-oriented measurement tool, we
use a linear topology (Figure 4). Nodes 0 and 9 are the probing
1A meaningful study of the ICMP impact requires access to differ-
ent types of routers with real trafﬁc load, but we do not have access
to such facilities.
0
30M
0.5ms
1
50M
0.1ms
2
   X
0.4ms
3
100M
0.4ms
4
80M
14ms
5
70M
2ms
6
  Y
4ms
7
50M
40ms
8
30M
10ms
9
Figure 4: Testbed conﬁguration.
Table 2: The testbed validation experiments
# X Y
1
20
50
Trace
light-trace on all
2
3
4
5
50
20
20
50
50
20
20
20
35Mbps exponential-load on
Y , light-trace otherwise
heavy-trace on Y , light-trace
otherwise
heavy-trace on X, light-trace
otherwise
30% exponential-load
both directions
on
Comments
Capacity-determined
bottleneck
Load-determined bot-
tleneck
Two-bottleneck case
Two-bottleneck case
The impact of reverse
trafﬁc
source and destination, while nodes 1-8 are intermediate routers.
The link delays are roughly set based on a traceroute measurement
from a CMU host to www.yahoo.com. The link capacities are
conﬁgured using the Dummynet [2] package. The capacities for
links X and Y depend on the scenarios. Note that all the testbed
nodes are PCs, not routers, so their properties such as the ICMP
generation time are different from those of routers. As a result,
the testbed experiments do not consider some of the router related
factors.
The dashed arrows in Figure 4 represent background trafﬁc. The
background trafﬁc is generated based on two real packet traces,
called light-trace and heavy-trace. The light-trace is a sampled
trace (using preﬁx ﬁlters on the source and destination IP ad-
dresses) collected in front of a corporate network. The trafﬁc load
varies from around 500Kbps to 6Mbps, with a median load of
2Mbps. The heavy-trace is a sampled trace from an outgoing link
of a data center connected to a tier-1 ISP. The trafﬁc load varies
from 4Mbps to 36Mbps, with a median load of 8Mbps. We also
use a simple UDP trafﬁc generator whose instantaneous load fol-
lows an exponential distribution. We will refer to the load from this
generator as exponential-load. By assigning different traces to dif-
ferent links, we can set up different evaluation scenarios. Since all
the background trafﬁc ﬂows used in the testbed evaluation are very
bursty, they result in very challenging scenarios.
Table 2 lists the conﬁgurations of ﬁve scenarios that allow us
to analyze all the important properties of Pathneck. For each sce-
nario, we use Pathneck to send 100 probing trains. Since these
scenario are used for validation, we only use the results for which
we received all ICMP packets, so the percentage of valid probing is
lower than usual. During the probings, we collected detailed load
data on each of the routers allowing us to compare the probing re-
sults with the actual link load. We look at Pathneck performance
for both probing sets (i.e., result for 10 consecutive probings as re-
ported by Pathneck) and individual probings. For probing sets, we
use conf ≥ 0.1 and d rate ≥ 0.5 to identify choke points. The
real background trafﬁc load is computed as the average load for the
interval that includes the 10 probes, which is around 60 seconds.
For individual probings, we only use conf ≥ 0.1 for ﬁltering, and
the load is computed using a 20ms packet trace centered around
the probing packets, i.e., we use the instantaneous load.
change cap of link Y: 21−30Mbps, with no load
change load on link Y (50Mbps): 20−29Mbps
7500
7000
6500
6000
5500
5000
)
s
u
(
l
e
u
a
v
p
a
g
7500
7000
6500
6000
5500
5000
)
s
u
(
l
e
u
a
v
p
a
g
F
D
C
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
1
2
3
4
5
6
7
8
9
10
bandwidth difference (Mbps)
all
wrong
4500
0
2
4
hop ID
6
8
4500
0
2
4
hop ID
6
8
Figure 6: Cumulative distribution of bandwidth difference in
experiment 3.
Figure 5: Comparing the gap sequences for capacity (left) and
load-determined (right) bottlenecks.
3.2.1 Experiment 1 — Capacity-determined Bottleneck
In this experiment, we set the capacities of X and Y to 50Mbps
and 20Mbps, and use light-trace on all the links; the starting times