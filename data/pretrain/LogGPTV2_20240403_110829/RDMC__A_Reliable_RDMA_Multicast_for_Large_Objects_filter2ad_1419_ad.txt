

























	
(b) Apt Cluster
Figure 10: Aggregate bandwidth of concurrent multicasts on Fractus
and the Apt cluster for cases in which we varied the percentage of
active senders in each node-group (in a group with k senders, we used
k overlapped RDMC groups with identical membership). The Apt
cluster has an oversubscribed TOR; our protocols gracefully adapt to
match the available bandwidth.
parallel computing settings because many such systems run
as a series of loosely synchronized steps that end with some
form of shufÔ¨Çe or all-to-all data exchange. Skew can leave the
whole system idle waiting for one node to Ô¨Ånish. In contrast,
the linear degradation of sequential send is also associated
with high skew. This highlights the very poor performance
of the technology used in most of today‚Äôs cloud computing
frameworks: not only is copy-by-copy replication slow, but it
also disrupts computations that need to wait for the transfers
to all Ô¨Ånish, or that should run in loosely synchronized stages.
Next, we set out to examine the behavior of RDMC in ap-
plications that issue large numbers of concurrent multicasts to
overlapping groups. We obtained a trace sampled from the
data replication layer of Microsoft‚Äôs Cosmos system, a data
warehouse used by the Bing platform. Cosmos currently runs
on a TCP/IP network, making no use of RDMA or multicast.
The trace has several million 3-node writes with random tar-
get nodes and object sizes varying from hundreds of bytes to
hundreds of MB (the median is 12MB and the mean 29 MB).
Many transfers have overlapping target groups.
To simulate use of multicast for the Cosmos workload, we
designated one Fractus node to generate trafÔ¨Åc, and 15 nodes
to host the replicas. The system operated by generating objects
79
Figure 8: Total time for replicating a 256MB object to a large number
of nodes on Sierra.

 
!

!
"#





















Figure 9: Distribution of latencies when simulating the Cosmos stor-
age system replication layer.
delivered per second using the binomial pipeline, again on
Fractus. Note, however, that the binomial pipeline (and in-
deed RDMC as a whole) is not really intended as a high-speed
event notiÔ¨Åcation solution: were we focused primarily on de-
livery of very small messages at the highest possible speed and
with the lowest possible latency, there are other algorithms we
could have explored that would outperform this conÔ¨Åguration
of RDMC under most conditions. Thus the 1-byte behavior of
RDMC is of greater interest as a way to understand overheads
than for its actual performance.
5.2.2 Scalability
Figure 8 compares scalability of the binomial pipeline on
Sierra with that of sequential send (the trend was clear and
Sierra was an expensive system to run on, so we extrapo-
lated the 512-node sequential send data point). While sequen-
tial send scales linearly in the number of receivers, binomial
pipeline scales sub-linearly, which makes an orders of magni-
tude difference when creating large numbers of copies of large
objects. This graph leads to a surprising insight: with RDMC,
replication can be almost free: whether making 127, 255 or
511 copies, the total time required is almost the same.
Although we did not separately graph end-of-transfer time,
binomial pipeline transfers also complete nearly simultane-
ously:
this minimizes temporal skew, which is important in
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:50:05 UTC from IEEE Xplore.  Restrictions apply. 














	
























	





















	

	
	
		









      



      



      
	
	
	
(a) 100MB Transfers
(b) 1MB Transfers
(c) 10 KB Transfers
Figure 11: Comparison of RDMC‚Äôs normal hybrid scheme of polling and interrupts (solid), with pure interrupts (dashed). There is no
noticeable difference between pure polling and the hybrid scheme. All ran on Fractus.




 



















 !!
"!#
















































Figure 12: CORE-Direct experiment using a chain multicast protocol to send a 100 MB message. The left is a run using hybrid polling/in-
terrupts; on the right is a run with purely interrupts. Both experiments were on Fractus.
Ô¨Ålled with random content, of the same sizes as seen in the
trace, then replicating them by randomly selecting one of the
possible 3-node groupings as a target (the required 455 RDMC
groups were created beforehand so that this would be off the
critical path). Figure 9 shows the latency distribution for 3
different send algorithms. Notice that binomial pipeline is al-
most twice as fast as binomial tree and around three times as
fast as sequential send. Average throughput when running with
binomial pipeline is around 93 Gb/s of data replicated, which
translates to about a petabyte per day. We achieve nearly the
full bisection capacity of Fractus, with no sign of interference
between concurrent overlapping transfer. The RDMC data pat-
tern is highly efÔ¨Åcient for this workload: no redundant data
transfers occur on any network link.