title:Packet Transactions: High-Level Programming for Line-Rate Switches
author:Anirudh Sivaraman and
Alvin Cheung and
Mihai Budiu and
Changhoon Kim and
Mohammad Alizadeh and
Hari Balakrishnan and
George Varghese and
Nick McKeown and
Steve Licking
Packet Transactions: High-Level Programming for
Line-Rate Switches
Anirudh Sivaraman¶, Alvin Cheung‡, Mihai Budiu§∗, Changhoon Kim†, Mohammad Alizadeh¶, Hari Balakrishnan¶,
George Varghese++, Nick McKeown+, Steve Licking†
¶MIT CSAIL, ‡University of Washington, §VMWare Research, †Barefoot Networks, ++Microsoft Research, +Stanford University
ABSTRACT
Many algorithms for congestion control, scheduling, net-
work measurement, active queue management, and trafﬁc
engineering require custom processing of packets in the data
plane of a network switch. To run at line rate, these data-
plane algorithms must be implemented in hardware. With
today’s switch hardware, algorithms cannot be changed, nor
new algorithms installed, after a switch has been built.
This paper shows how to program data-plane algo-
rithms in a high-level language and compile those programs
into low-level microcode that can run on emerging pro-
grammable line-rate switching chips. The key challenge is
that many data-plane algorithms create and modify algorith-
mic state. To achieve line-rate programmability for stateful
algorithms, we introduce the notion of a packet transaction:
a sequential packet-processing code block that is atomic and
isolated from other such code blocks.
We have developed this idea in Domino, a C-like imper-
ative language to express data-plane algorithms. We show
with many examples that Domino provides a convenient way
to express sophisticated data-plane algorithms, and show
that these algorithms can be run at line rate with modest es-
timated chip-area overhead.
CCS Concepts
•Networks → Programmable networks;
Keywords
Programmable switches; stateful data-plane algorithms
∗Work done when employed at Barefoot Networks.
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than the author(s) must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
SIGCOMM ’16, August 22 - 26, 2016, Florianopolis , Brazil
© 2016 Copyright held by the owner/author(s). Publication rights licensed to
ACM. ISBN 978-1-4503-4193-6/16/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2934872.2934900
15
1.
INTRODUCTION
Network switches in modern datacenters, enterprises, and
service-provider networks perform many tasks in addition
to standard packet forwarding. The set of requirements for
switches has only increased with time as network operators
seek greater control over performance. Performance may be
improved using both data-plane and control-plane mecha-
nisms. This paper focuses on data-plane algorithms. These
algorithms process and transform packets, reading and writ-
ing state in the switch. Examples include active queue man-
agement [38, 47, 51], scheduling [58], congestion control
with switch feedback [45, 60], network measurement [63,
37], and data-plane trafﬁc engineering [21].
An important requirement for data-plane algorithms is
the ability to process packets at the switch’s line rate: 10–
100 Gbit/s on 10–100 ports. Therefore, these algorithms
are typically implemented using dedicated hardware. Hard-
ware designs are rigid and not reconﬁgurable in the ﬁeld.
Thus, implementing and deploying a new algorithm, or even
modifying a deployed one, requires an investment in new
hardware—a time-consuming and expensive proposition.
This rigidity affects many stakeholders adversely: ven-
dors [2, 4, 6] building network switches with merchant-
silicon chips [10, 11, 16], network operators deploying these
switches [41, 54, 56], and researchers developing new data-
plane algorithms [21, 37, 45, 60].
To run new data-plane algorithms after a switch has been
built, researchers and companies have attempted to build
programmable switches for many years, starting from efforts
on active networks [61] to network processors [14] to soft-
ware routers [8, 46]. All these efforts sacriﬁced performance
for programmability, typically running an order of magni-
tude (or worse) slower than hardware line rates. Unfortu-
nately, this reduction in performance has meant that these
systems are rarely deployed in production networks.
Programmable switching chips [3, 13, 19] competitive in
performance with state-of-the-art ﬁxed-function chips [10,
11, 16] are now becoming available. These chips imple-
ment a few low-level hardware primitives that can be con-
ﬁgured by software into a processing pipeline, and are ﬁeld-
reconﬁgurable. Building a switch with such a chip is attrac-
tive because it does not compromise on data rates [28].
In terms of programmability, these chips today allow the
network operator to program packet parsing and forwarding,
i.e., a programmer can program the set of protocol formats
to be matched and the set of actions to be executed when
matching packet headers in a match-action table. Languages
such as P4 [27] are emerging as a way to express such match-
action processing in a hardware-independent manner.
There is a gap between this form of programmability and
the needs of data-plane algorithms. By contrast to packet
forwarding, which doesn’t modify state in the data plane,
many data-plane algorithms create and modify algorithmic
state in the switch as part of packet processing.
For such algorithms, programmability must directly cap-
ture the algorithm’s intent without requiring the algorithm
to be shoehorned into hardware primitives like a sequence
of match-action tables. Indeed, the ability to directly cap-
ture an algorithm’s intent pervades programming models for
many other networking devices, e.g., software routers [46],
network processors [36], and network endpoints [5].
By studying the requirements of data-plane algorithms
and the constraints of line-rate hardware, we introduce a
new abstraction to program and implement data-plane algo-
rithms: a packet transaction (§3). A packet transaction is a
sequential code block that is atomic and isolated from other
such code blocks, with the semantics that any visible state is
equivalent to a serial execution of packet transactions across
packets in the order of packet arrival. Packet transactions
let the programmer focus on the operations needed for each
packet without worrying about other concurrent packets.
Packet transactions have an all-or-nothing guarantee: all
packet transactions accepted by the packet transactions com-
piler will run at line rate, or be rejected. There is no “slip-
pery slope” of running network algorithms at lower speeds
as with network processors or software routers: when com-
piled, a packet transaction runs at line rate, or not at all. Per-
formance is not just predictable, but guaranteed.
In realizing packet transactions, we make three new con-
tributions. First, Banzai, a machine model for programmable
line-rate switches (§2). Banzai models two important con-
straints (§2.4) for stateful line-rate operations: the inability
to share state between different packet-processing units, and
the requirement that any switch state modiﬁcations be vis-
ible to the next packet entering the switch. Based on these
constraints, we introduce atoms to represent a programmable
switch’s packet-processing units.
Second, Domino, a new domain-speciﬁc language (DSL)
for data-plane algorithms, with packet transactions at its core
(§3). Domino is an imperative language with C-like syntax,
to our knowledge the ﬁrst to offer such a high-level program-
ming abstraction for line-rate switches.
Third, a compiler from Domino packet transactions to a
Banzai target (§4). The Domino compiler extracts codelets
from transactions: code fragments, which if executed atomi-
cally, guarantee a packet transaction’s semantics. It then uses
program synthesis [59] to map codelets to atoms, rejecting
the transaction if the atom cannot execute the codelet.
We evaluate expressiveness by programming a variety of
data-plane algorithms (Table 3) in Domino and compare
with P4. We ﬁnd that Domino provides a more concise
and natural programming model for stateful data-plane algo-
rithms. Next, because no existing programmable switch sup-
ports the set of atoms required for our data-plane algorithms,
we design a set of compiler targets for these algorithms
based on Banzai (§5.2). We show that these targets are fea-
sible in a 32-nm standard-cell library with < 2% cost in
area relative to a 200 mm2 baseline switching chip [40]. Fi-
nally, we compile data-plane algorithms written in Domino
to these targets (§5.3) to show how a target’s atoms deter-
mine the algorithms it can support. We conclude with sev-
eral lessons for programmable switch design (§5.4).
Code for the Domino compiler,
the Banzai machine
model, and the code examples listed in Table 3 is available
at http://web.mit.edu/domino.
2. A MACHINE MODEL FOR LINE-
RATE SWITCHES
Banzai is a machine model for programmable line-rate
switches that serves as the compiler target for Domino. Ban-
zai is inspired by recent programmable switch architectures
such as Barefoot Networks’ Toﬁno [3], Intel’s FlexPipe [13],
and Cavium’s XPliant Packet Architecture [19]. Banzai ab-
stracts these architectures and extends them with stateful
processing units to implement data-plane algorithms. These
processing units, called atoms, model atomic operations that
are natively supported by a programmable line-rate switch.
2.1 Background: Programmable switches
Packets arriving at a switch (top half of Figure 1) are
parsed by a programmable parser that turns packets into
header ﬁelds. These header ﬁelds are ﬁrst processed by an
ingress pipeline consisting of match-action tables arranged
in stages. Processing a packet at a stage may modify its
header ﬁelds, through match-action rules, as well as some
persistent state at that stage, e.g., packet counters. After
the ingress pipeline, the packet is queued. Once the sched-
uler dequeues the packet, it is processed by a similar egress
pipeline before it is transmitted.
To reduce chip area, there is only one ingress and one
egress pipeline. This single pipeline is shared across all
switch ports and handles aggregate trafﬁc belonging to all
ports, at all packet sizes. For instance, a 64-port switch with
a line rate of 10 Gbit/s per port and a minimum packet size of
64 bytes needs to process around a billion packets per sec-
ond, after accounting for minimum inter-packet gaps [28].
Equivalently, the pipeline runs at 1 GHz, and pipeline stages
process a packet every clock cycle (1 ns). We assume one
packet per clock cycle throughout the paper, and for con-
creteness, a 1 GHz clock frequency.
Having to process a packet every clock cycle in each stage
constrains the operations that can be performed on each
packet. In particular, any packet operation that modiﬁes state
visible to the next packet must ﬁnish execution in a single
clock cycle (§2.3 shows why). Because of this restriction,
programmable switching chips provide a small set of pro-
cessing units or primitives for manipulating packets and state
in a stage, unlike software routers. These primitives deter-
mine which algorithms run on the switch at line rate.
16
Figure 1: Banzai models the ingress or egress pipeline of a programmable switch. An atom corresponds to an action in a
match-action table. Internally, an atom contains local state and a digital circuit modifying this state. Figure 2 details an atom.
The challenge for us is to develop primitives that allow
a broad range of data-plane algorithms to be implemented,
and to build a compiler to map a user-friendly description of
an algorithm to the primitives provided by a switch.
2.2 The Banzai machine model
Banzai (the bottom half of Figure 1) models the ingress
or egress switch pipeline. It models the computation within
a match-action table in a stage (i.e., the action half of the
match-action table), but not how packets are matched (e.g.,
direct or ternary). Banzai does not model packet parsing and
assumes that packets arriving to Banzai are already parsed.
Concretely, Banzai is a feed-forward pipeline1 consist-
ing of a number of stages executing synchronously on every
clock cycle. Each stage processes one packet every clock
cycle and hands it off to the next. Unlike a CPU pipeline,
which occasionally experiences pipeline stalls, Banzai’s
pipeline is deterministic, never stalls, and always sustains
line rate. However, relative to a CPU pipeline, Banzai is re-
stricted in the operations it supports (§2.4).
2.3 Atoms: Banzai’s processing units
An atom is an atomic unit of packet processing supported
natively by a Banzai machine, and the atoms within a Banzai
machine form its instruction set. Each pipeline stage in Ban-
zai contains a vector of atoms. Atoms in this vector modify
1It is hard to physically route backward-ﬂowing wires that
would be required for feedback.
17
mutually exclusive sections of the same packet header in par-
allel in every clock cycle, and process a new packet header
every clock cycle.
In addition to packet headers, atoms may modify persis-
tent state on the switch to implement stateful data-plane al-
gorithms. To support such algorithms at line-rate, the atoms
for a Banzai machine need to be substantially richer (Ta-
ble 4) than the simple RISC-like stateless instruction sets for
programmable switches today [28]. We explain why below.
Suppose we need to atomically increment a switch
counter to count packets. One approach is hardware support
for three simple single-cycle operations: read the counter
from memory in the ﬁrst clock cycle, add one in the next,
and write it to memory in the third. This approach, however,
does not provide atomicity. To see why, suppose packet A
increments the counter from 0 to 1 by executing its read, add,
and write at clock cycles 1, 2, and 3 respectively. If packet B
issues its read at time 2, it will increment the counter again
from 0 to 1, when it should be incremented to 2.
Locks over the shared counter are a potential solution.
However, locking causes packet B to wait during packet
A’s increment, and the switch no longer sustains the line
rate of one packet every clock cycle. CPUs employ micro-
architectural techniques such as operand forwarding for this
problem. But these techniques still suffer pipeline stalls,
which prevents line-rate performance from being achieved.
Banzai provides an atomic increment operation at line rate
with an atom to read a counter, increment it, and write it
back in a single stage within one clock cycle.
It uses the
Stage 1PacketHeaderPacketHeaderPacketHeaderParserBitsHeadersMatch-action tableMatchActionHeadersMatch-action tableIngress pipelineHeadersQueuesMatch-action tableHeadersMatch-action tableEgress pipelineHeadersTransmitThe architecture of a programmable switchThe  Banzai machine modelStateAtomCircuitAtomAtomStateAtomAtomAtomStateAtomAtomAtomStage 2Stage NCircuitCircuitEthIPv4IPv6TCPsame approach of reading, modifying, and writing back to
implement other stateful atomic operations at line rate.
Unlike stateful atomic operations, stateless atomic oper-
ations are easier to support with simple packet-ﬁeld arith-
metic. Consider, for instance, the operation pkt.f1 =
pkt.f2 + pkt.f3 - pkt.f4. This operation does not
modify any persistent switch state and only accesses packet
ﬁelds. It can be implemented atomically by using two atoms:
one atom to add ﬁelds f2 and f3 in one pipeline stage, and an-
other to subtract f4 from the result in the next. An instruction
set designer can provide simple stateless instructions operat-
ing on a pair of packet ﬁelds. These instructions can then be
composed into larger stateless operations, without designing
atoms speciﬁcally for each stateless operation.
Representing atoms. An atom is represented by a body
of sequential code that captures the atom’s behavior.
It
may also contain internal state local to the atom. An atom
completes execution of this entire body of code, modifying
a packet and any internal state before processing the next
packet. The designer of a programmable switch would de-
velop these atoms, and expose them to a switch compiler as
the programmable switch’s instruction set, e.g., Table 4.
Using this representation, a switch counter that wraps
around at a value of 100 can be written as the atom:2
if ( counter < 99)
counter ++;
else
counter = 0;
Similarly, a stateless operation like setting a packet ﬁeld
to a constant value can be written as the atom:
pkt . field = value ;
2.4 Constraints for line-rate operation
Memory limits. State in Banzai is local to each atom. It
can neither be shared by atoms within a stage, nor atoms
across stages. This is because building multi-ported memo-
ries accessible to multiple atoms is technically challenging
and consumes additional chip area. However, state can be
read into a packet header in one stage, for subsequent use
by a downstream stage3. But, the Banzai pipeline is feed-
forward, so state can only be carried forward, not backward.
Computational limits. Atoms need to execute atomically
from one packet to the next, so any state internal to the atom
must be updated before the next packet arrives. Because
packets may be separated by as little as one clock cycle, we
mandate that atom bodies ﬁnish execution within one clock