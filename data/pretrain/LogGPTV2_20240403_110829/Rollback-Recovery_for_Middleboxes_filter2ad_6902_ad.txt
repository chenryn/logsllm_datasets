server. This amounts to about 5µs for each 4 Kbyte page.
on a 10 Gbit/s link, and quickly dominates the ﬁxed cost (1-
2ms) for taking the snapshot. However, a worst case analysis
is hard as values depend on the (wildly variable) number of
pages modiﬁed between snapshots. Hence it is more mean-
ingful to gauge the additional latency from the experimental
values in §6 and the literature in general [23].
5.5 Replay
Finally, we describe our implementation of replay, when
a Replica VM starts from the last available snapshot to take
over a failed Master. The Replica is started in “replay
mode”, meaning that the input is fed (by the IL) from the
saved trace, and threads use the PALs to drive nondetermin-
istic choices.
On input, the threads on the Replica start processing pack-
ets, discarding possible duplicates at the beginning of the
stream. When acquiring the lock that protects a shared vari-
able, the thread uses the recorded PALs to check whether it
can access the lock, or it has to block waiting for some other
thread that came earlier in the original execution. The infor-
mation in the PALs is also used to replay hardware related
non deterministic calls (clocks, etc.). Of course, PALs are
not generated during the replay.
On output, packets are passed to the OL, which discards
them if a previous instance had been already released, or
pass it out otherwise (e.g., copies of packets still in the Mas-
ter when it crashed, even though all of their dependencies
had made it to the OL). A thread exits replay mode when
it ﬁnds that there are no more PALs for a given shared vari-
able. When this happens, it starts behaving as the master, i.e.
generate PALs, compute output dependencies, etc.
Performance implications: other than having to re-run the
Middlebox since the last snapshot, operation speed in replay
mode is comparable to that in the original execution. §6.2
presents some experimental results. Of course, the duration
of service unavailability after a failure also depends on the
latency of the failure detector, whose discussion is beyond
the scope of this paper.
6. EVALUATION
We added FTMB support into 7 middlebox applications
implemented in Click: one conﬁguration comes from in-
dustry, ﬁve are research prototypes, and one is a simple
‘blind forwarding’ conﬁguration which performs no middle-
box processing; we list these examples in Table 1.
Our experimental setup is as follows. FTMB uses Xen
4.2 at the master middlebox with Click running in an Open-
SUSE VM, chosen for its support of fast VM snapshot-
ting [6]. We use the standard Xen bridged networking back-
3
2
251
2
1
3
0
LOC SVs Elts
5728
5052
4623
4964
5058
5462
1914
Middlebox
Mazu-NAT
WAN Opt.
BW Monitor
SimpleNAT
Adaptive LB
QoS Priority
BlindFwding
Table 1: Click conﬁgurations used in our experiments,
including Lines of Code (LOC), Shared Variables (SVs),
number of Elements (Elts), and the author/origin of the
conﬁguration.
Source
Mazu Networks [7]
Aggarwal et al. [15]
Custom
Custom
Custom
Custom
Custom
46
40
41
42
42
56
24
Figure 5: Local RTT with and without components of
FTMB enabled.
end; this backend is known to have low throughput and sub-
stantial recent work aims to improve throughput and latency
to virtual machines, e.g., through netmap+xennet [45, 52] or
dpdk+virtio [38, 55]. However, neither of these latter sys-
tems yet supports seamless VM migration. We thus built
two prototypes: one based on the Xen bridged networking
backend which runs at lower rates (100Mbps) but is com-
plete with support for fast VM snapshots and migration, and
a second prototype that uses netmap+xennet and scales to
high rates (10Gbps) but lacks snapshotting and replay. We
primarily report results from our complete prototype; results
for relevant experiments with the high speed prototype were
qualitatively similar.
We ran our tests on a local network of servers with 16-core
Intel Xeon EB-2650 processors at 2.6Ghz, 20MB cache size,
and 128GB memory divided across two NUMA nodes. For
all experiments shown, we used a standard enterprise trace
as our input packet stream [26]; results are representative of
tests we ran on other traces.
We ﬁrst evaluate the FTMB’s latency and bandwidth over-
heads under failure-free operation (§6.1). We then evaluate
recovery from failure (§6.2).
6.1 Overhead on Failure-free Operation
How does FTMB impact packet latency under failure-
free operation? In Figure 5, we present the per-packet la-
tency through a middlebox over the local network. A packet
source sends trafﬁc (over a logging switch) to a VM run-
ning a MazuNAT (a combination ﬁrewall-NAT released by
Mazu Networks [7]), which loops the trafﬁc back to the
packet generator. We measure this RTT. To test FTMB, we
ﬁrst show the base latency with (a) just the MazuNAT, (b)
the MazuNAT with I/O logging performed at the upstream/-
downstream switch, (c) the MazuNAT with logging, PAL-
 0 0.2 0.4 0.6 0.8 1 100 1000CDF of PacketsLatency (us)MazuNAT (Baseline)with I/O Loggersw/ FTMB w/o Snapshotsw/ FTMB + Snapshots235Figure 6: Testbed RTT over time.
Figure 8: Testbed RTT with increasing PALs/packet.
Figure 7: Local RTT with FTMB and other FT systems.
instrumented locks, parallel release for the output commit
condition and (d) running the MazuNAT with all our fault
tolerance mechanisms, including VM checkpointing every
200ms. Adding PAL instrumentation to the middlebox locks
in the MazuNAT has a negligible impact on latency, increas-
ing 30µs over the baseline at the median, leading to a 50th
percentile latency of 100µs.9 However, adding VM check-
pointing does increase latency, especially at the tail: the 95th
%-ile is 810µs, and the 99th %-ile s 18ms.
To understand the cause of this tail latency, we measured
latency against time using the Blind Forwarding conﬁgura-
tion. Figure 6 shows the results of this experiment: we see
that the latency spikes are periodic with the checkpoint in-
terval. Every time we take a VM snapshot, the virtual ma-
chine suspends temporarily, leading to a brief interval where
packets are buffered as they cannot be processed. As new
hardware-assisted virtualization techniques improve [1, 20]
we expect this penalty to decrease with time; we discuss
these opportunities further in §8.
How does the latency introduced by FTMB compare to
existing fault-tolerance solutions? In Figure 7, we com-
pare FTMB against three proposals from the research com-
munity: Pico [50], Colo [28], and Xen Remus [23]. Remus
and Colo are general no-replay solutions which can provide
fault tolerance for any VM-based system running a standard
operating system under x86. Remus operates by checkpoint-
ing and buffering output until the next checkpoint completes;
this results in a median latency increase for the MazuNAT
by over 50ms. for general applications Colo can offer much
lower latency overhead than Remus: Colo allows two copies
of a virtual machine to run side-by-side in “lock step”. If
their output remains the same, the two virtual machines are
considered identical; if the two outputs differ, the system
9In similar experiments with our netmap-based prototype we observe a me-
dian latency increase of 25µs and 40µs over the baseline at forwarding rates
of 1Gbps and 5Gbps respectively, both over 4 cores.
Figure 9: Ideal [46] and observed page load times when
latency is artiﬁcially introduced in the network.
forces a checkpoint like Remus. Because multi-threaded
middleboxes introduce substantial nondeterminism, though,
Colo cannot offer us any beneﬁts over Remus: when we ran
the MazuNAT under Colo, it checkpointed just as frequently
as Remus would have, leading to an equal median latency
penalty.
Pico is a no-replay system similar to Remus but tailored to
the middlebox domain by offering a custom library for ﬂow
state which checkpoints packet processing state only, but
not operating system, memory state, etc., allowing for much
lighter-weight and therefore faster checkpoint. The authors
of Pico report a latency penalty of 8-9ms in their work which
is a substantial improvement over Colo and Remus, but still
a noticeable penalty due to the reliance on packet buffering
until checkpoint completion.
How does inserting PALs increase latency? To measure
the impact of PALs over per-packet latency, we used a toy
middlebox with a simple pipeline of 0, 1, or 5 locks and
ran measurements with 500-byte packets at 1Gbps with four
threads dedicated to processing in our DPDK testbed. Fig-
ure 8 shows the latency distributions for our experiments,
relative to a baseline of the same pipeline with no locks. At
5 PALS/Locks per packet, latency increases to 60µs with
5 PALS/Locks per packet, relative to a median latency un-
der 40µs in the baseline – an increase of on average 4µs
per PAL/Lock per packet. Note that this latency ﬁgure in-
cludes both the cost of PAL creation and lock insertion; the
worst case overhead for FTMB is when locks are not already
present in the base implementation.
How much does latency matter to application perfor-
mance? We measured the impact of inﬂated latency on
Flow Completion Times (FCTs) with both measurements
and modeling. In Figure 9, we show ﬂow completion times
for a 2MB ﬂow (representative of web page load sizes) given
the ﬂow completion time model by Mittal et al. [46] marked
 0 1000 2000 3000 4000 5000 6000Latency (us)Time 0 0.2 0.4 0.6 0.8 1 10 100 1000 10000 100000 1e+06 1e+07CDF of PacketsLatency (us)MazuNAT (Baseline)MazuNAT, with FTMB and SnapshotsPico (reported)MazuNAT, under ColoMazuNAT, under Remus 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 10 100 1000CDFTime (us)No PAL/Locks1 PAL/Packet5 PAL/Packet 0 200 400 600 800 1000 1200DC:  IdealLAN: IdealWAN: IdealLAN: ObservedWAN: ObservedFCT (milliseconds)No delay1ms delay10ms delay50ms delay236Figure 10:
throughput.
Impact of FTMB on forwarding plane
Figure 11: Time to perform replay with varying check-
point intervals and middlebox conﬁgurations.
as ‘Ideal’. Marked as ‘Observed’, we downloaded the Alexa
top-1000 [2] web pages over a LAN and over a WAN and
used tc to inﬂate the latency by the same amounts. In both
the datacenter and LAN cases, adding 10ms of latency on the
forward and reverse path increases ﬂow completion times to
20× the original in the simulated case; in the experimental
LAN case it increased FCT to 10×. In the WAN case, page
load times increased to 1.5× by adding 10ms of latency from
a median of 343ms to 492ms. An experiment by Amazon
shows that every 100ms of additional page load time their
customers experienced costs them 1% in sales [40].
Given these numbers in context, we can return to Figure 7
and see that solutions based on Colo, Pico, or Remus would
noticeably harm network users’ quality of experience, while
FTMB, with introduced latency typically well under 1ms,
would have a much weaker impact.
throughput under
How much does FTMB impact
failure-free operation? Figure 10 shows forwarding plane
throughput in a VM, in a VM with PAL instrumentation, and
running complete FTMB mode with both PAL instrumenta-
tions and periodic VM snapshotting. To emphasize the extra
load caused by FTMB, we ran the experiment with locally
sourced trafﬁc and dropping the output. Even so, the impact
is modest, as expected (see §5.2). For most conﬁgurations,
the primary throughput penalty comes from snapshotting
rather than from PAL insertion. The MazuNAT and Simple-
Nat saw a total throughput reduction of 5.6% and 12.5% re-
spectively. However, for the Monitor and the Adaptive Load
Balancer, PAL insertion was the primary overhead, causing
a 22% and 30% drop in throughput respectively. These two
experience a heavier penalty since typically they have no
contention for access to shared state variables: the tens of
nanoseconds required to generate a PAL for these middle-
boxes is a proportionally higher penalty than it is for middle-
boxes which spend more time per-packet accessing complex
and contended state.
We ran similar experiments with Remus and Colo, where
throughput peaked in the low hundreds of Kpps. We also ran
experiments with Scribe [41], a publicly-available system
for record and replay of general applications, which aims
to automatically detect and record data races using page
protection. This costs about 400us per lock access due to
the overhead of page faults.10 Using Scribe, a simple two-
threaded Click conﬁguration with a single piece of shared
state stalled to a forwarding rate of only 500 packets/second.
10Measured using the Scribe demo image in VirtualBox.
Figure 12: Packet latencies post-replay.
6.2 Recovery
How long does FTMB take to perform replay and how
does replay impact packet latencies? Unlike no-replay
systems, FTMB adds the cost of replay. We measure the
amount of time required for replay in Fig. 11. We ran these
experiments at 80% load (about 3.3 Mpps) with periodic
checkpoints of 20, 50, 100, and 200ms.
For lower checkpoint rates, we see two effects leading to
a replay time that is actually less than the original check-
point interval. First, the logger begins transmitting packets
to the replica as soon as replay begins – while the VM is
loading. This means that most packets are read pre-loaded to
local memory, rather than directly from the NIC. Second, the
transmission arrives at almost 100% of the link rate, rather
than 80% load as during the checkpoint interval.
However, at 200ms, we see a different trend: some mid-
dleboxes that make frequent accesses to shared variable have
a longer replay time than the original checkpoint interval be-
cause of the overhead of replaying lock accesses. Recall that
when a thread attempts to access a shared-state variable dur-
ing replay, it will spin waiting for its ‘turn’ to access the
variable and this leads to slowed execution.
During replay, new packets that arrive must be buffered,
leading to a period of increased queueing delays after execu-
tion has resumed. In Figure 12, we show per-packet latencies
for packets that arrive post-failure for MazuNAT at differ-
ent load levels and replay times between 80-90ms. At 30%-
load, packet latencies return to their normal sub-millisecond
values within 60ms of resumed execution. As expected re-
covery takes longer at higher loads: at 70% load per-packet
latency remains over 10ms even at 175ms, and the latencies
do not decrease to under a millisecond until past 300ms after
execution has resumed.
 0 1e+06 2e+06 3e+06 4e+06 5e+06 6e+06MazuNATSimpleNATWAN Opt.MonitorQoSAdaptive LBPackets Per SecondXen BaselineXen + FTMBXen + FTMB + Snapshotting 0 50 100 150 200 250 300User Mon.QoSLoad BalancerMazuNATSimpleNATWAN Opt.Replay Time (ms)20ms50ms100ms200ms 0 10 20 30 40 50 60 70 80 90 0 20 40 60 80 100 120 140 160Latency (ms)Time (ms)70% Load50% Load30% Load237(a) HTTP Page Loads
(b) FTP Download
(c) Torrent (Evolve)
(d) Torrent(Ubuntu)
Figure 13: Application performance with and without
state restoration after recovery. Key (top right) is same
for all ﬁgures.
Is stateful failover valuable to applications? Perhaps the
simplest approach to recovering from failure is simply to
bring up a backup from ‘cold start’, effectively wiping out
all connection state after failure: i.e., recovery is stateless.
To see the impact of stateless recovery on real applications,
we tested several applications over the wide area with a NAT
which either (a) did not fail (our baseline), (b) went absent
for 300ms,11 during which time trafﬁc was buffered (this
represents stateful recovery), or (c) ﬂushed all state on fail-
ure (representing stateless recovery). Figure 13 shows the
time to download 500 pages in a 128-thread loop from the