are processed by machines by each network card and can be precisely
identified. In a virtualized architecture, virtual machines can
communicate over physical networks through a single adapter belonging to
the physical machine that hosts them. The data flows of each virtual
machine are processed by this single network card. Therefore, it is not
possible to guarantee a partitioning of flows at the level of the shared
resource. The network card has the possibility in case of error or
compromise to mix the different information flows.
> Figure 22.3: Architecture with virtualization
In Figure 22.3, the gray area materializes the physical machine; three
virtual machines are represented by the blue, yellow, red rectangles;
the orange zone represents the abstraction layer.
![](media/image190.jpeg)In this context, to better respond to the need
for partitioning, the choice can be made to have as many network cards
as virtual machines hosted on a physical machine (see Figure 4).
Ideally, it should be verified that the components involved in the data
flow processing chain between a virtual machine and the assigned network
adapter correctly handle the partitioning of data according to a virtual
machine. For example, to manage partitioning of input / output streams
passing through the memory, an IOMMU component can be used (represented
by the grid area in Figure 5); but if an input / output controller not
compatible with the component IOMMU is used, it will pass in a common
memory area all the flows from different virtual machines, which
presents a risk of information leakage.
> Figure 22.4: A physical network card per guest system
![](media/image191.jpeg)Some environments (such as the network) should
not run in the virtualized environment. The choice of a partial return
to a classical solution (without virtualization) can then be more
adapted to a good partitioning of the flows (figure 22.5).
> Figure 22.5: Mixed architecture
The main risks caused by a lack resource isolation are the information
leakage and the breach of data integrity. One way to reduce these risks
may also be to ensure a good data integrity through end-to-end privacy
and data integrity mechanisms (in the case of the network, through the
use of IPsec).
## Complexity of the administration
When a virtualization solution is used, it is necessary to administer
not only the different guest systems but also the abstraction layer.
Examples of new administrative operations, induced by the use of
virtualization technologies, are:
-   Setting quotas on resources shared between different systems;
-   Managing the addition of a disk or a network storage device (NAS)
    without
-   taking into consideration the partitioning between virtual machines;
-   Specific backups related to virtualization operations, protection of
    these back- ups, restore operations. The migration of virtual
    machines during backups must be taken into account, as well as the
    strong correlations that may exist between backups of different
    systems and data.
-   Traditional administrative tasks can also be more complex because
    the interventions on the physical machine itself (administrator of
    the host system), on the instances that are hosted there
    (administrator(s) of the guest systems), on the devices of physical
    and virtual storage (SAN / NAS) and physical and (potentially)
    virtual network devices may need to be done separately. Indeed, if,
    as in many large organizations, teams managing servers, storage,
    network and backups are disjointed, the identification of
    responsibilities of each in the administration of a virtualized
    system is essential in order to limit, as much as possible,
    configuration errors, such as the placement of a virtual machine in
    the wrong virtual network (VLAN).
-   The administration of machines can operate locally or remotely.
    While it is usually difficult to administer guest systems locally,
    the question arises for the abstraction layer.
-   The choice of administration of a remote system or not must be made
    considering all the risks involved. Among such risks is the
    usurpation of the authorized administrator role following the
    implementation of a weak authentication mechanism, the loss of
    confidentiality and/or integrity of a command on the network, loss
    of traceability of administration operations. If the organization
    uses cloud computing technologies, special attention should be paid
    to the management of virtual machines which can in some cases be
    very automated. It is necessary to secure all the management
    interfaces and to trace any action taken through them.
## Complexity of the supervision
Like administrative operations, supervision operations can also be
complex, because of the paradox that exists between the need for virtual
machine partitioning and the desire for an overview during supervision
operations. Due to the partitioning caused
by the virtualization solution , it can be difficult to trace an event
or an end-to-end action.
In addition, the need to have an overview requires that the
administrator of supervision be authorized to access the information of
the highest sensitivity level of the processed data.
## Unwanted proliferation of data and systems
The migration of guest operating systems to different physical machines
is possible, and most of the time desired. As a result, the precise
location of a datum is complicated. Similarly, it will be more difficult
overall to prevent the fraudulent copying of information. In addition,
instance migration techniques typically imply that instances are in the
form of "migrating objects". The risks of uncontrolled copying of
instances, loss, modification or loss of control of software versions of
instances are important.
## Inability to manage fatal error
Operating problems and errors can be complex to manage technically in an
architecture based on a virtualization solution. For example, errors
that may occur when stopping and restarting an instance will either be
reported to the host system that the instance is stopping (leaving).
Without the global consideration of the errors of a system based on
virtualization, it may be that relevant information to identify their
cause is lost. It is therefore necessary to set up a centralization and
a correlation of the logs on all the systems. This correlation obviously
poses problems identical to those previously identified for supervision.
## More difficult post-incident investigations
Some post-disaster investigations related to the sharing of hardware
resources by multiple systems may be more difficult. The optimization of
the RAM management by the virtualization solution makes it more
difficult to analyze the history of the states of the machine and
therefore the processing of an incident in the case when this memory is
re-allocated to another virtual machine.
## Hypervisor Attacks
The different Hypervisors generate new risks such as attacks between
virtual machines, loss of information in a virtual machine, the takeover
of the host operating system, etc. Below is a set of risks related to
these new technologies:
## Isolation and related attacks
One of the first benefits of virtualization is isolation, it ensures
that an application running on a VM does not access an application
running on another VM. The isolation must be strongly maintained, so
that the intrusion into a virtual machine does not allow access to the
other virtual machines, the hypervisor and the host machine. For
example, sharing the clipboard in a virtual environment is a convenient
feature that allows data to be transferred between virtual machines and
the host machine. But this feature can also serve as a gateway for
transferring data between malicious code acting collaboratively within
different virtual machines. Some virtualization technologies do not
implement isolation in order to allow applications designed for an
operating system, to be operational on another operating system, this
kind of solution allows the exploitation of the flaws security of both
operating systems, and also gives unlimited access to the resources of
the host machine, such as the file system.
## Virtual machine escape
Virtual machines are allowed to share the resources of the host machine
but still provide isolation between VMs and between VMs and the host.
However, virtual machines are designed so that a program running on one
can not communicate with programs running on the other, or with programs
running on the host machine. But in reality organizations undermine
isolation.
They configure \"flexible\" isolation to meet the needs of their
architecture. In this situation the virtual machine escape is the most
serious attack if the isolation between virtual machines is compromised.
In Virtual Machine Escape, the program running in the VM is able to
bypass the hypervisor and gain access to the host machine. Since the
host machine is the root, the program that obtained the access acquires
administrator privileges. This results in the obsolescence of the
overall security of the environment. The Cloudburst exploit is an
example of VM escape, it takes the advantage of a display function
of a VMware product, which allowed the escape of a VM and thus access to
the hypervisor.
## Isolation and network traffic
In the case of network traffic, the isolation completely depends on how
the virtual environment is connected. In most cases the virtual machine
is connected to the host by means of a virtual switch, which allows the
VM to use the poisoning ARP to redirect incoming and outgoing packets
from another virtual machine. Insulation Requires a Design-Free,
Bug-Free Hypervisor.
## External modification of the hypervisor
The hypervisor is responsible for isolation between virtual machines,
VMs are protected if the hypervisor is working properly. Otherwise, the
hypervisor introduces a security vulnerability to the system set. One
solution is to protect the hypervisor from unauthorized changes.
## Attacks on Virtual Machine Live Migrations
During virtual machine live migration, the top three physical resources
used are memory, network, and local disk. The memory can be copied
directly from one host to another, for the local disk and the network
interface the migration is not trivial. Live migration of virtual
machines is an essential feature of virtualization. It allows the
transfer of a virtual machine from one physical server to another
without interrupting the services running on the VM. Live migration
provides the following benefits: workload balancing, virtual machine
consolidation, etc\...
The hypervisor is a software that emulates the hardware part used by the
virtual machines, it completely controls the resources of the system.
Most commercial and open source versions of hypervisors support live
migration.
Live migration includes a lot of transfers state across the network.
During this procedure, protecting the contents of VM state files is very
important. Most of the work to implement live migration has focused on
implementing this migration with little or no consideration for
security. Memory is a crucial point because it is difficult for a
virtual machine to encrypt its own memory. Because live migration
protocols do not encrypt data that is being transferred, all migrating
data, such as passwords, are transmitted in clear. In addition, after
migration the runtime environment of the virtual machine, may have
changed in terms of CPU resources, memory, drivers. Such changes can be
detected, and an attacker able to characterize these changes such as
side-channel attacks.
## Side channel attacks
These attacks exploit the physical properties of the hardware to collect
information that can give a schema or pattern of operation of the system
to attack. The fact that several virtual machines share the same
hardware makes the side channel attack relatively easy to perform.
Without the provision of hardware security, the sharing of hardware is
dangerous. One of the goals of this type of attack is to reveal the
cryptographic keys. These attacks are generally categorized into three
classes :
-   Time-driven side-channel attack: this attack is possible when the
    total time of execution of the cryptographic operations with fixed
    key is influenced by the value of the key because of the structure
    of the cryptographic implementation. This influence can be exploited
    by an attacker who can measure these times to statistically deduce
    information on the key.
-   Trace-driven side-channel attack: These attacks continuously monitor
    some aspects of a hardware device through a cryptographic operation
    (e.g., power consumption).
-   Access-driven side-channel attack: In this type of attack, an
    attacker launches the execution of a program on the cryptographic
    system that manages the operation of interest to the attacker. The
    program monitors the use of a shared component in the architecture
    to obtain information about the key (e.g., the data cache).
7.  ## Hyperjacking
    -   This attack consists of installing an unauthorized hypervisor
        that will take full control of the server. Standard security
        measures are ineffective in this case because the operating
        system will not realize that the machine has been compromised.
        Attacks such as hyperjacking can balance architecture security
        like Cloud Computing.
## Hypervisor security solutions
To address the vulnerabilities and sophisticated attacks revealed by the
use of hypervisors, we need a full suite of security solutions. These
solutions include
## Vax VMM
One of the first attempts to design a secure hypervisor is made by
Karger & al in a 1981-1990 research on the production of a Virtual
Machine Monitor \[VMM\] security kernel. This research project has
achieved security level A1 by the National Computer Security Center
(NCSC). This is the highest level of security according to the
evaluation criteria of the Trusted Computer System Evaluation Criteria
published by NCSC in 1985 and which is also known as the Orange
Book. The development of the VMM Security Kernel is based on the virtual
address extension of the VAX architecture developed by Digital Equipment
Corporation in the 1970s.
In accordance with the requirements of security level A1, the VAX
Hypervisor takes into account the DAC and MAC access control systems of
all virtual machines. With MAC, the VMM VAX uses the Bell-Lapadula Model
protection model for privacy protection and the Biba integrity
protection model.
The VAX security kernel enables and manages multiple virtual machines on
a single VAX physical system while providing isolation and controlled
sharing of sensitive data. It has a secure authentication system, with a
high level of performance and highly developed system management tools,
thus subjecting virtual machines to mandatory access and audit controls.
Thus, each virtual machine has an access class composed of a secret
class and a class of integrity similar to the classes in the VMS
Security Enhancement Services (VMS SES).
## Terra
In 2003, Tal Gar□nkel and al wrote an article about a virtual machine
based on a trusted platform called Terra. The Terra architecture is
based on a virtual machine monitor that allows multiple virtual machines
to be multiplexed on a single physical machine. Terra uses the secure
virtual machine monitor called Trusted Virtual Monitor Machine (TVMM).
The TVMM architecture offers a variety of services with advanced
protection mechanisms.
## sHype
The sHype security architecture is probably one of the best-known
approaches when it comes to creating a secure hypervisor. It was born
from an IBM research project developed for IBMs rHype with an open
source hypervisor. Shortly after the release of its first version, it is
implemented in an open source hypervisor. Its main purpose is to control
the explicit flow of information between virtual machines. sHype uses
the formal MAC security policy
sHype uses the concept of a reference monitor that enforces the allowed
access relationships between subjects and objects in a system. This
means that the reference monitor is called whenever a user wants to
access an object. However, the reference monitor does not decide whether
a user can access an object. It only imposes the decision that is often
made elsewhere in the system. It is the Access Control Module (MAC) that
is responsible for this decision. The MAC uses the formal security
policy with labels that are fixed on the topics and objects of the
system and the type of operation a subject can perform to make an Access
Control Decision (DAC). Thus, the complete workflow that the system
executes if a subject attempts to access an object is as follows: The
access call for the object is intercepted by the reference monitor,
which in turn calls the MAC into placing an Authorization Query (AQ).
This AQ contains the labels of the object and the operations that can be
executed on the object (reading, writing \...). The MAC uses the formal
security policy and the QA data to make a DAC which is then returned to
the reference screen. Finally, the reference monitor applies the DAC by
allowing or refusing to perform the operation. In this process, the
reference monitor is actually implemented using execution hooks that are
distributed over the hypervisor.
## HyperWall
Another approach to providing security is offered with the HyperWall
architecture. This is to protect guest virtual machines from an
unreliable hypervisor. With Hyper- Wall, the hypervisor freely manages
the memory, CPU cores, and other resources of a platform. After the
virtual machines are created, the Confidentiality and Integrity
Protection (CIP) protects memory for guest virtual machines from the
hypervisor or DMA (Direct Memory Access) according to customer
specifications. The client may specify that certain memory ranges are
protected against access by the hypervisor or the DMA. HyperWall is the
key element that protects the privacy and
integrity of objects that are only accessible by hardware. They protect
all or part of the memory of a virtual machine based on customer
specifications.
## Trusted eXecution Technology
In 2009, David Grawrock announced the concept of Trusted Computing with
a modular approach to the design of platform and PC security in his book
Dynamics of a Trusted Platform. The Intel Trusted Execution Technology
(TXT) is a block used to create a secure platform by implementing
security features and new capabilities in the processor. The use of
Intel TXT Trusted Execution Technology enables the protection of the IT
infrastructure from software attacks when starting a server or a
computer.
## Hypersafe
In 2010, always in the optics of securing hypervisors Xuxian Jiang and
his doctoral student Zhi Wang propose Hypervisor Isolation via
Hypersafe. This is a software called HyperSafe that takes advantage of
existing hardware features to ensure hypervisors against such attacks.
Malicious programs must run their own code in the hypervisor. To prevent
this from happening, the Hypersafe software uses a non-bypass memory
lock technique that reliably prohibits the introduction of new code into
the hypervisor by anyone other than the hypervisor administrator while
preventing any attempt to modify the source program of the hypervisor by
external users by the indexing technique.
## Further Reading and Useful Resources
-   
-   
-   
-   