title:A Scanner Darkly: Protecting User Privacy from Perceptual Applications
author:Suman Jana and
Arvind Narayanan and
Vitaly Shmatikov
2013 IEEE Symposium on Security and Privacy
Protecting User Privacy From Perceptual Applications
A Scanner Darkly:
Suman Jana‚àó
‚àóThe University of Texas at Austin
Arvind Narayanan‚Ä†
‚Ä†Princeton University
Vitaly Shmatikov‚àó
programmable robots, even moving around‚Äîraises interest-
ing privacy issues for their users. Many people are already
uncomfortable with law enforcement agencies conducting
large-scale face recognition [2, 17]. Perceptual applications
running in one‚Äôs home or a public area may conduct
unauthorized surveillance, intentionally or unintentionally
overcollect information (e.g., keep track of other people
present in a room), and capture sensitive data such as credit
card numbers, license plates, contents of computer monitors,
etc. that accidentally end up in their Ô¨Åeld of vision.
General-purpose, data-agnostic privacy technologies such
as access control and privacy-preserving statistical analysis
are fairly blunt tools. Instead, we develop a domain-speciÔ¨Åc
solution, informed by the structure of perceptual applications
and the computations they perform on their inputs, and ca-
pable of applying protection at the right level of abstraction.
Our system, DARKLY, is a privacy protection layer for
untrusted perceptual applications operating on trusted de-
vices. Such applications typically access input data from
the device‚Äôs perceptual sensors via special-purpose software
libraries. DARKLY is integrated with OpenCV, a popular
computer vision library which is available on Windows,
Linux, MacOS, iOS, and Android and supports a diverse
array of input sensors including webcams, Kinects, and
smart cameras. OpenCV is the default vision library of the
Robot Operating System (ROS); our prototype of DARKLY
has been evaluated on a Segway RMP-50 robot running
ROS Fuerte. DARKLY is language-agnostic and can work
with OpenCV programs writen in C, C++, or Python. The
architecture of DARKLY is not speciÔ¨Åc to OpenCV and can
potentially be adapted to another perceptual software library
with a sufÔ¨Åciently rich API.
We evaluate DARKLY on 20 existing OpenCV applica-
tions chosen for the diversity of their features and perceptual
tasks they perform,
including security surveillance with
motion detection, handwriting recognition, object tracking,
shape detection, face recognition, background-scenery re-
moval from video chat, and others.
Abstract‚ÄîPerceptual, ‚Äúcontext-aware‚Äù applications that ob-
serve their environment and interact with users via cameras
and other sensors are becoming ubiquitous on personal com-
puters, mobile phones, gaming platforms, household robots,
and augmented-reality devices. This raises new privacy risks.
We describe the design and implementation of DARKLY, a
practical privacy protection system for the increasingly com-
mon scenario where an untrusted, third-party perceptual ap-
plication is running on a trusted device. DARKLY is integrated
with OpenCV, a popular computer vision library used by such
applications to access visual inputs. It deploys multiple privacy
protection mechanisms, including access control, algorithmic
privacy transforms, and user audit.
We evaluate DARKLY on 20 perceptual applications that per-
form diverse tasks such as image recognition, object tracking,
security surveillance, and face detection. These applications
run on DARKLY unmodiÔ¨Åed or with very few modiÔ¨Åcations
and minimal performance overheads vs. native OpenCV. In
most cases, privacy enforcement does not reduce the appli-
cations‚Äô functionality or accuracy. For the rest, we quantify
the tradeoff between privacy and utility and demonstrate that
utility remains acceptable even with strong privacy protection.
I. INTRODUCTION
Modern software programs increasingly include percep-
tual functionality that takes advantage of high-resolution
cameras and other sensors to observe their users and physi-
cal environment. Perceptual software includes ‚Äúnatural user
interface‚Äù systems that
interact with users via gestures
and sounds, image recognition applications such as Google
Goggles, security software such as motion detectors and
face recognizers, augmented reality applications, ‚Äúambient
computing‚Äù frameworks, a variety of video-chat and tele-
presence programs, and other context-aware software.
Hardware platforms for perceptual applications include
mobile phones, programmable robotic pets and household
robots (e.g., iRobot Create platform), gaming devices (e.g.,
Kinect), augmented reality displays (e.g., Google Glass),
and conventional computers equipped with webcams. Many
platforms provide app stores‚Äîfor example robotappstore.
com (‚Äúyour robots are always up-to-date with the coolest
apps‚Äù)‚Äîenabling consumers to download and execute thou-
sands of third-party perceptual applications.
The growing availability and popularity of potentially
untrusted perceptual applications capable of scanning their
surroundings at Ô¨Åne level of detail‚Äîand, in the case of
18 applications run on DARKLY unmodiÔ¨Åed, while 2 re-
quired minor modiÔ¨Åcations. The functionality and accuracy
of most applications are not degraded even with maximum
privacy protection. In all cases, performance with DARKLY
is close to performance on ‚Äúnative‚Äù OpenCV.
¬© 2012, Suman Jana. Under license to IEEE.
DOI 10.1109/SP.2013.31
349
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:55:08 UTC from IEEE Xplore.  Restrictions apply. 
II. THREAT MODEL AND DESIGN OF DARKLY
We focus on the scenario where the device, its operating
system, and the hardware of its perceptual sensors are
trusted, but the device is executing an untrusted third-party
application. The application can be arbitrarily malicious, but
it runs with user-level privileges and can only access the
system, including perceptual sensors, through a trusted API
such as the OpenCV computer vision library.
	 
	 
$$
$$





"
"


%!




"
"






"
"
!

!
 
 


"
"

Figure 1. System architecture of DARKLY.
The system model of DARKLY is shown in Fig. 1 with
the trusted components shaded. DARKLY itself consists of
two parts, a trusted local server and an untrusted client
library. We leverage standard user-based isolation provided
by the OS: the DARKLY server is a privileged process with
direct access to the perceptual sensors, while applications
run as unpriviliged processes that can only access the
sensors through DARKLY. Furthermore, we assume that no
information about DARKLY operation (e.g.,
side-channel
screenshots of its console) can be obtained via system calls.
The untrusted DARKLY client library runs as part of each
application process and communicates with the DARKLY
server. This is merely a utility for helping applications access
the perceptual API and the system remains secure even if a
malicious application modiÔ¨Åes this library.
A major challenge in this design is Ô¨Åguring out which
parts of the input should be revealed to the application and
in what form, while protecting ‚Äúprivacy‚Äù in some fashion.
Visual data in particular are extremely rich and diverse,
making it difÔ¨Åcult to isolate and identify individual objects.
Existing methods for automated image segmentation are too
computationally expensive to be applied in real time and
suffer from high false positives and false negatives.
DARKLY applies multiple layers of privacy protection
to solve the problem: access control, algorithmic transfor-
mation, and user audit. First, it replaces raw perceptual
inputs with opaque references. Opaque references cannot
be dereferenced by an application, but can be passed to and
from trusted library functions which thus operate on true per-
ceptual data without loss of Ô¨Ådelity. This allows applications
to operate on perceptual inputs without directly accessing
them. This approach is so natural that privacy protection is
completely transparent to many existing applications: they
work on DARKLY without any modiÔ¨Åcations to their code
and without any loss of accuracy or functionality.
Second, some applications such as security cameras and
object trackers require access to certain high-level features of
the perceptual inputs. To support such applications, DARKLY
substitutes the corresponding library API with declassiÔ¨Åer
functions that apply appropriate feature- or object-speciÔ¨Åc
(but application-independent!) privacy transforms before re-
turning the data to the application. Example of transforms
include sketching (a combination of low-pass Ô¨Åltering and
contour detection) and generalization (mapping the object to
a generic representative from a predeÔ¨Åned dictionary).
To help balance utility and privacy, the results of applying
a privacy transform are shown to the user in the DARKLY
console window. The user can control the level of transfor-
mation via a dial and immediately see the results. In our
experience, most applications do not need declassiÔ¨Åers, in
which case DARKLY protects privacy without any loss of
accuracy and the DARKLY console is not used. For those
of our benchmark applications that use declassiÔ¨Åers, we
quantitatively evaluate the degradation in their functionality
depending on the amount of transformation.
DARKLY provides built-in trusted services, including a
trusted GUI‚Äîwhich enables a perceptual application to
show the result of computation to the user without accessing
it directly‚Äîand trusted storage. For example, after the
security camera detects motion, it can store the actual images
in the user‚Äôs Google Drive without ‚Äúseeing‚Äù them.
A few applications, such as eigenface-based face rec-
ognizers, need to operate directly on perceptual
inputs.
DARKLY provides a domain-speciÔ¨Åc ibc language based
on GNU bc. Isolating domain-speciÔ¨Åc programs is much
easier than isolating arbitrary code. Untrusted ibc programs
are executed on the raw inputs, but have no access to the
network, system calls, or even system time. Furthermore,
DARKLY only allows each invocation to return a single
32-bit value to the application. We show that legitimate
computations can be ported to ibc with little difÔ¨Åculty.
...
// Grab a frame from camera
img=cvQueryFrame(..);
// Process the image to filter out unrelated stuff
...
// Extract a binary image based on the ball‚Äôs
color
cvInRangeS(img, ...);
...
// Process the image to filter out unrelated stuff
...
// Compute the moment
cvMoments(...);
// Compute ball‚Äôs coordinates using moment
...
// Move robot towards the calculated coordinates
...
Listing 1. Outline of the ball-tracking robot application.
350
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:55:08 UTC from IEEE Xplore.  Restrictions apply. 
To illustrate how DARKLY works on a concrete example,
Listing II shows a simpliÔ¨Åed ball-tracking application for a
robotic dog. The code on the light gray background does
not need direct access to image contents and can operate on
opaque references. The code on the dark gray background
invokes a DARKLY declassiÔ¨Åer, which applies a suitable
privacy transform to the output of the cvMoments OpenCV
function. The rest of the code operates on this transformed
data. DARKLY thus ensures that the application ‚Äúsees‚Äù only
the position of the ball. The accuracy of this position
depends on the privacy transform and can be adjusted by
the user via the privacy dial.
III. PRIVACY RISKS OF PERCEPTUAL APPLICATIONS
What does a scanner see? Into the head? Down into the
heart? Does it see into me, into us? Clearly or darkly?
A Scanner Darkly (2006)
Perceptual applications present unique privacy risks. For
example, a security-cam application, intended to detect mo-
tion in a room and raise an alarm, can leak collected video
feeds. A shape detector can read credit card numbers, text on
drug labels and computer screens, etc. An object or gesture
tracker‚Äîfor example, a robot dog programmed to follow
hand signals and catch thrown balls‚Äîcan be turned into
a roving spy camera. A face detector, which hibernates the
computer when nobody is in front of it, or a face recognizer,
designed to identify its owner, can surreptitiously gather
information about people in the room. A QR code scanner, in
addition to decoding bar codes, can record information about
its surroundings. App stores may have policing mechanisms
to remove truly malicious applications, but these mecha-
nisms tend to be ineffective against applications that collect
privacy-sensitive information about their users.
Overcollection and aggregation. The privacy risks of
perceptual applications fall into several hierarchical cate-
gories. The Ô¨Årst is overcollection of raw visual data and
the closely related issue of aggregation. The problem of
aggregation is similar to that of public surveillance: a single
photograph of a subject
in a public place might make
that individual uncomfortable, but it is the accumulation of
these across time and space that is truly worrying. Even
ignoring speciÔ¨Åc inferential privacy breaches made possible
by this accumulation, aggregation itself may inherently be
considered a privacy violation. For example, Ryan Calo
argues that ‚ÄúOne of the well-documented effects of interfaces
and devices that emulate people is the sensation of being
observed and evaluated. Their presence can alter our attitude,
behavior, and physiological state. Widespread adoption of
such technology may accordingly lessen opportunities for
solitude and chill curiosity and self-development.‚Äù [4]
Many applications in DARKLY work exclusively on
opaque references (Section VI-B), in which case the ap-
plication gets no information and the aggregation risk does
not arise. For applications that do access some objects and
features of the image, we address aggregation risks with
the DARKLY console (Section VIII). The DARKLY console
is an auxiliary protection mechanism that visually shows
the outputs of privacy transforms to the user, who has the
option to adjust the privacy dial, shut down the application,
or simply change his or her behavior. A small amount of
leakage may happen before the user has time to notice
and react to the application‚Äôs behavior, but we see this as
categorically different from the problem of aggregation. The
DARKLY console is rougly analogous to the well-established
privacy indicators in smartphones that appear when location
and other sensory channels are accessed by applications.
Inference. The Ô¨Årst category of inference-based privacy
risks is speciÔ¨Åc, sensitive pieces of information‚Äîanything
from a credit card number to objects in a room to a person‚Äôs
identity‚Äîthat are leaked by individual frames.
DARKLY addresses such threats by being domain- and
data-dependent, unlike most privacy technologies. Privacy
transforms (see Section VII), speciÔ¨Åcally sketching, mini-
mize leakage at a frame-by-frame level by interposing on
calls that return speciÔ¨Åc features of individual images (see
examples in Figs. 2 and 3). Privacy protection is thus speciÔ¨Åc
to the domain and perceptual modality in question, and
some privacy decisions are made by actually examining the
perceptual inputs. In contrast to basic access control, this
domain-speciÔ¨Åc design sacriÔ¨Åces the simplicity of imple-
mentation and reasoning. In exchange, we gain the ability to
provide the far more nuanced privacy properties that users
intuitively expect from perceptual applications.
The last category in the hierarchy of privacy risks is
semantic inference. For example, even a sketch may al-
low inference of potentially sensitive gestures, movements,
proximity of faces, bodies, etc. It is unlikely these risks
can be mitigated completely except for speciÔ¨Åc categories
of applications, mainly those that can function solely with
opaque references or require only numerical features such as
histograms where techniques like differential privacy [9, 10]
may apply. Unless the transformed data released to the
application is sufÔ¨Åciently simple to reason about analytically,
the semantic inference risk will exist, especially due to the
continual nature of perceptual observation.
That said, a machine-learning-based, data-dependent ap-
proach to privacy transforms offers some hope. For example,
in Section VII-B, we describe how to use facial identiÔ¨Åcation
technology to transform a face into a privacy-preserving
‚Äúcanonical representation.‚Äù The key idea here is to take a
technology that leads to the inference risk, namely facial
recognition, and turns it on its head for privacy protection.
It is plausible that this paradigm can be extended to handle
other types of inference, and as more complex inference
techniques are developed, privacy transforms will co-evolve
to address them. This is left to future work.
351
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:55:08 UTC from IEEE Xplore.  Restrictions apply. 
IV. STRUCTURE OF PERCEPTUAL APPLICATIONS
DARKLY is based on the observation that most legiti-
mate applications do not need unrestricted access to raw
inputs. This is reÔ¨Çected in their design. For
perceptual
example, most existing OpenCV applications do not access
raw images (see Section IX) because implementing complex
computer vision algorithms is difÔ¨Åcult even for experienced
developers. Fortunately, the OpenCV API is at the right
level of abstraction: it provides domain-speciÔ¨Åc functions
for common image-processing tasks that applications use
as building blocks. This enables applications to focus on
speciÔ¨Åc objects or features, leaving low-level image analysis
to OpenCV functions and combining them in various ways.
DARKLY ensures that these functions return the information
that applications need to function‚Äîbut no more!
Perceptual applications can be classiÔ¨Åed into three general
categories: (1) those that do not access the perceptual inputs
apart from invoking standard library functions; (2) those that
access speciÔ¨Åc, library-provided features of the inputs; and
(3) those that must execute their own code on raw inputs. For
applications in the Ô¨Årst category, DARKLY completely blocks
access to the raw data. For the second category, DARKLY
provides declassiÔ¨Åer functions that apply privacy transforms
to the features before releasing them to the application. For
the third category, DARKLY isolates untrusted code to limit
the leakage of sensitive information.
For example, a security camera only needs to detect
changes in the scene and invoke a trusted service to store
the image (and maybe raise an alarm). This requires the
approximate contours of objects, but not their raw pixels.
Trackers need objects‚Äô moments to compute trajectories, but
not objects themselves. A QR scanner works correctly with
only a thresholded binary representation of the image, etc.
DARKLY is designed to support more sophisticated func-
tionalities, too. For example, applications dealing with hu-
man faces can be classiÔ¨Åed into ‚Äúdetectors‚Äù and ‚Äúrecogniz-
ers.‚Äù Face detectors are useful for non-individualized tasks
such as emotion detection or face tracking‚Äîfor example,
a robotic pet might continually turn to face the user‚Äîand
need to know only whether there is a rectangle containing
a face in their Ô¨Åeld of vision. To support such applications,
DARKLY provides a privacy transform that returns a generic
representation of the actual face.
Face recognizers, on the other hand, must identify speciÔ¨Åc
faces, e.g., for visual authentication. Even in this case, a
recognizer may run an algorithm comparing faces in the
image with a predeÔ¨Åned face but only ask for a single-bit
answer (match or no match). To support such applications,
DARKLY allows execution of arbitrary image analysis code,
but rigorously controls the information it can export.
V. DESIGN PRINCIPLES OF DARKLY
Block direct access to perceptual inputs. DARKLY inter-
poses on all accesses by applications to cameras and other
352
perceptual sensors. As shown in Fig. 1, this privacy protec-
tion layer is implemented as a DARKLY server that runs as
a privileged ‚Äúuser‚Äù on the same device as the applications;
only this user can access the sensors. Applications interact
with the DARKLY server via inter-process sockets (UNIX
domain sockets) and standard OS user isolation mechanisms
prevent them from accessing the state of DARKLY.
The key concept in DARKLY is opaque reference. Opaque
references are handles to image data and low-level rep-
resentations returned by OpenCV functions. An applica-
tion cannot dereference them, but can pass them to other
OpenCV functions, which internally operate on unmodiÔ¨Åed
data without any loss of Ô¨Ådelity. Applications can thus per-
form sophisticated perceptual tasks by ‚Äúchaining together‚Äù
multiple OpenCV functions. In Section IX, we show that
many existing applications produce exactly the same output
when executed on DARKLY vs. unmodiÔ¨Åed OpenCV.
A similar architectural approach is used by PINQ [18], a
system for privacy-preserving data analysis. PINQ provides
an API for basic data-analysis queries such as sums and
counts. Untrusted applications receive opaque handles to
the raw data (PINQueryable objects) which they cannot
dereference, but can pass to and from trusted API functions
thus constructing complex queries.
DARKLY also provides trusted services which an applica-
tion can use to ‚Äúobliviously‚Äù export data from the system,
if needed. For example, after a security-camera application
detects motion in the room, it can use a trusted remote-
storage service to store the captured image in the user‚Äôs
Google Drive‚Äîwithout accessing its pixels!
Support unmodiÔ¨Åed applications, whenever possible.
DARKLY is language-independent and works equally well
with OpenCV applications written in C, C++, or Python. It
changes neither the API of the existing OpenCV functions,
nor OpenCV‚Äôs types and data structures. Instead, opaque
references replace pointers to raw pixels in the meta-data of
OpenCV objects. DARKLY is thus completely transparent to
applications that do not access raw image data, which are the
majority of the existing OpenCV applications (Section IX).
Use multiple layers of privacy protection. Applications
that do not access raw inputs assemble their functionality by
passing opaque references to and from OpenCV functions.
For applications that work with high-level features, DARKLY
provides declassiÔ¨Åers that replace these features with safe
representations generated by the appropriate privacy trans-
forms (Section VII). Privacy transforms keep the information
that applications need for their legitimate functionality while
removing the details that may violate privacy.