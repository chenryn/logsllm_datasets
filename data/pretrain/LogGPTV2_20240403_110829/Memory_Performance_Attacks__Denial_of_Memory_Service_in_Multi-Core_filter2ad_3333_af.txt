o
i
t
u
c
e
x
E
d
e
z
i
l
a
m
r
o
N
5.5
5.0
4.5
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
i
e
m
T
n
o
i
t
u
c
e
x
E
d
e
z
i
l
a
m
r
o
N
4.5
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
512-byte
art
vpr
1 KB
2 KB
4 KB
8 KB
16 KB
32 KB
64 KB
FR-FCFS FairMem
FR-FCFS
FairMem
FR-FCFS
FairMem
FR-FCFS
FairMem
FR-FCFS
FairMem
FR-FCFS
FairMem
FR-FCFS
FairMem
FR-FCFS
FairMem
Figure 9: Normalized execution time of art and vpr when run together on processors with different row-buffer sizes.
Execution time is independently normalized to each machine with different row-buffer size.
art’s row-buffer hit rate
vpr’s row-buffer hit rate
FairMem throughput improvement
FairMem fairness improvement
512 B 1 KB 2 KB 4 KB 8 KB 16 KB 32 KB 64 KB
95% 98%
56% 67% 87% 91% 92% 93%
13% 15% 17% 19% 23% 28%
38% 41%
1.08X 1.16X 1.28X 1.44X 1.62X 1.88X 2.23X 2.64X
1.55X 1.75X 2.23X 2.42X 2.62X 3.14X 3.88X 5.13X
Table 4: Statistics for art and vpr with different row-buffer sizes
1 bank
2 banks
4 banks
art
vpr
8 banks
16 banks
32 banks
64 banks
FR-FCFS FairMem
FR-FCFS
FairMem
FR-FCFS
FairMem
FR-FCFS
FairMem
FR-FCFS
FairMem
FR-FCFS
FairMem
FR-FCFS
FairMem
Figure 10: Slowdown of art and vpr when run together on processors with various number of DRAM banks. Execution
time is independently normalized to each machine with different number of banks.
art-vpr base throughput (IPTC)
art-vpr FairMem throughput (IPTC)
FairMem throughput improvement
FairMem fairness improvement
1 bank 2 banks 4 banks 8 banks 16 banks 32 banks 64 banks
210
122
190
287
1.56X 1.37X
2.67X 2.57X
304
402
1.32X
2.35X
707
751
1.06X
1.18X
401
513
1.28X
2.23X
507
606
1.20X
1.70X
617
690
1.12X
1.50X
Table 5: Statistics for art-vpr with different number of DRAM banks (IPTC: Instructions/1000-cycles)
memory system increases, and thus art becomes less of a
performance hog; its memory requests conﬂict less with
vpr’s requests. Regardless of the number of banks, our
mechanism signiﬁcantly mitigates the performance im-
pact of art on vpr while at the same time improving over-
all throughput as shown in Table 5. Current DRAMs
usually employ 4-16 banks because a larger number of
banks increases the cost of the DRAM system. In a sys-
tem with 4 banks, art slows down vpr by 2.64X (while
itself being slowed down by only 1.10X). FairMem is
able to reduce vpr’s slowdown to only 1.62X and im-
prove overall throughput by 32%. In fact, Table 5 shows
that FairMem achieves the same throughput on only 4
banks as the baseline scheduling algorithm on 8 banks.
6.2.4 Effect of Memory Latency
Clearly, memory latency also has an impact on the vul-
nerability in the DRAM system. Figure 11 shows how
different DRAM latencies inﬂuence the mutual perfor-
mance impact of art and vpr. We vary the round-trip
latency of a request that hits in the row-buffer from 50
to 1000 processor clock cycles, and scale closed/conﬂict
latencies proportionally. As memory latency increases,
the impact of art on vpr also increases. Vpr’s slowdown
is 1.89X with a 50-cycle latency versus 2.57X with a
1000-cycle latency. Again, FairMem reduces art’s im-
pact on vpr for all examined memory latencies while
also improving overall system throughput (Table 6). As
main DRAM latencies are expected to increase in mod-
ern processors (in terms of processor clock cycles) [39],
scheduling algorithms that mitigate the impact of MPHs
will become more important and effective in the future.
6.2.5 Effect of Number of Cores
Finally, this section analyzes FairMem within the con-
text of 4-core and 8-core systems. Our results show that
FairMem effectively mitigates the impact of MPHs while
improving overall system throughput in both 4-core and
8-core systems running different application mixes with
varying memory-intensiveness.
Figure 12 shows the effect of FairMem on three dif-
ferent application mixes run on a 4-core system.
In
all the mixes, stream and small-stream act as severe
MPHs when run on the baseline FR-FCFS system, slow-
ing down other applications by up to 10.4X (and at least
3.5X) while themselves being slowed down by no more
than 1.10X. FairMem reduces the maximum slowdown
caused by these two hogs to at most 2.98X while also
USENIX Association
16th USENIX Security Symposium
271
i
e
m
T
n
o
i
t
u
c
e
x
E
d
e
z
i
l
a
m
r
o
N
3.0
2.5
2.0
1.5
1.0
0.5
0.0
art
vpr
50 cyc
100 cyc
200 cyc
300 cyc
400 cyc
500 cyc
1000 cyc
FR-FCFS FairMem
FR-FCFS
FairMem
FR-FCFS
FairMem
FR-FCFS
FairMem
FR-FCFS
FairMem
FR-FCFS
FairMem
FR-FCFS
FairMem
Figure 11: Slowdown of art and vpr when run together on processors with different DRAM access latencies. Execution
time is independently normalized to each machine with different number of banks. Row-buffer hit latency is denoted.
art-vpr base throughput (IPTC)
art-vpr FairMem throughput (IPTC)
FairMem throughput improvement
FairMem fairness improvement
50 cycles 100 cycles 200 cycles 300 cycles 400 cycles 500 cycles 1000 cycles
1229
1459
1.19X
1.69X
728
905
1.24X
1.82X
401
513
1.28X
2.23X
278
359
1.29X
2.21X
212
276
1.30X
2.25X
172
224
1.30X
2.23X
88
114
1.30X
2.22X
Table 6: Statistics for art-vpr with different DRAM latencies (IPTC: Instructions/1000-cycles)
4p-MIX3
small-stream
art
mcf
health
i
e
m
T
n
o
i
t
u
c
e
x
E
d
e
z
i
l
a
m
r
o
N
10.5
10.0
9.5
9.0
8.5
8.0
7.5
7.0
6.5
6.0
5.5
5.0
4.5
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
4p-MIX1
stream
art
mcf
health
i
e
m
T
n
o
i
t
u
c
e
x
E
d
e
z
i
l
a
m
r
o
N
4p-MIX2
stream
art
mcf
vpr
i
e
m
T
n
o
i
t
u
c
e
x
E
d
e
z
i
l
a
m
r
o
N
10.5
10.0
9.5
9.0
8.5
8.0
7.5
7.0
6.5
6.0
5.5
5.0
4.5
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
10.5
10.0
9.5
9.0
8.5
8.0
7.5
7.0
6.5
6.0
5.5
5.0
4.5
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
FR-FCFS
Figure 12: Effect of FR-FCFS and FairMem scheduling on different application mixes in a 4-core system
FR-FCFS
FairMem
FR-FCFS
FairMem
FairMem
improving the overall throughput of the system (Table 7).
Figure 13 shows the effect of FairMem on three dif-
ferent application mixes run on an 8-core system. Again,
in the baseline system, stream and small-stream act as
MPHs, sometimes degrading the performance of another
application by as much as 17.6X. FairMem effectively
contains the negative performance impact caused by the
MPHs for all three application mixes. Furthermore, it
is important to observe that FairMem is also effective
at isolating non-memory-intensive applications (such as
crafty in MIX2 and MIX3) from the performance degra-
dation caused by the MPHs. Even though crafty rarely
generates a memory request (0.35 times per 1000 instruc-
tions), it is slowed down by 7.85X by the baseline sys-
tem when run within MIX2! With FairMem crafty’s rare
memory requests are not unfairly delayed due to a mem-
ory performance hog — and its slowdown is reduced to
only 2.28X. The same effect is also observed for crafty in
MIX3.13 We conclude that FairMem provides fairness in