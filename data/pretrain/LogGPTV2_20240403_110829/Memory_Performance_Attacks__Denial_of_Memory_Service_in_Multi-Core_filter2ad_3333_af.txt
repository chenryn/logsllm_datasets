### Figure 9: Normalized Execution Time of `art` and `vpr` with Different Row-Buffer Sizes

**Normalized Execution Time (independent normalization for each machine with different row-buffer sizes):**

| Row-Buffer Size | 512 B | 1 KB | 2 KB | 4 KB | 8 KB | 16 KB | 32 KB | 64 KB |
|-----------------|-------|------|------|------|------|-------|-------|-------|
| **FR-FCFS**     |  5.5  |  5.0 |  4.5 |  4.0 |  3.5 |   3.0 |   2.5 |   2.0 |
| **FairMem**     |  1.5  |  1.0 |  0.5 |  0.0 |  0.0 |   0.0 |   0.0 |   0.0 |

**Metrics:**
- `art`'s row-buffer hit rate: 95% - 98%
- `vpr`'s row-buffer hit rate: 56% - 93%
- FairMem throughput improvement: 13% - 28%
- FairMem fairness improvement: 38% - 41%

**Throughput Improvement (X factor):**
- 512 B: 1.08X
- 1 KB: 1.16X
- 2 KB: 1.28X
- 4 KB: 1.44X
- 8 KB: 1.62X
- 16 KB: 1.88X
- 32 KB: 2.23X
- 64 KB: 2.64X

**Fairness Improvement (X factor):**
- 512 B: 1.55X
- 1 KB: 1.75X
- 2 KB: 2.23X
- 4 KB: 2.42X
- 8 KB: 2.62X
- 16 KB: 3.14X
- 32 KB: 3.88X
- 64 KB: 5.13X

---

### Table 4: Statistics for `art` and `vpr` with Different Row-Buffer Sizes

| Row-Buffer Size | 512 B | 1 KB | 2 KB | 4 KB | 8 KB | 16 KB | 32 KB | 64 KB |
|-----------------|-------|------|------|------|------|-------|-------|-------|
| **FR-FCFS**     |  5.5  |  5.0 |  4.5 |  4.0 |  3.5 |   3.0 |   2.5 |   2.0 |
| **FairMem**     |  1.5  |  1.0 |  0.5 |  0.0 |  0.0 |   0.0 |   0.0 |   0.0 |

**Metrics:**
- `art`'s row-buffer hit rate: 95% - 98%
- `vpr`'s row-buffer hit rate: 56% - 93%
- FairMem throughput improvement: 13% - 28%
- FairMem fairness improvement: 38% - 41%

**Throughput Improvement (X factor):**
- 512 B: 1.08X
- 1 KB: 1.16X
- 2 KB: 1.28X
- 4 KB: 1.44X
- 8 KB: 1.62X
- 16 KB: 1.88X
- 32 KB: 2.23X
- 64 KB: 2.64X

**Fairness Improvement (X factor):**
- 512 B: 1.55X
- 1 KB: 1.75X
- 2 KB: 2.23X
- 4 KB: 2.42X
- 8 KB: 2.62X
- 16 KB: 3.14X
- 32 KB: 3.88X
- 64 KB: 5.13X

---

### Figure 10: Slowdown of `art` and `vpr` with Various Number of DRAM Banks

**Slowdown (independent normalization for each machine with different number of banks):**

| Number of Banks | 1 bank | 2 banks | 4 banks | 8 banks | 16 banks | 32 banks | 64 banks |
|-----------------|--------|---------|---------|---------|----------|----------|----------|
| **FR-FCFS**     |  210   |  122    |  190    |  287    |   304    |   402    |   707    |
| **FairMem**     |  1.56X |  1.37X  |  2.67X  |  2.57X  |   1.32X  |   2.35X  |   1.06X  |

**Metrics:**
- `art-vpr` base throughput (IPTC)
- `art-vpr` FairMem throughput (IPTC)
- FairMem throughput improvement
- FairMem fairness improvement

**Throughput Improvement (X factor):**
- 1 bank: 1.56X
- 2 banks: 1.37X
- 4 banks: 2.67X
- 8 banks: 2.57X
- 16 banks: 1.32X
- 32 banks: 2.35X
- 64 banks: 1.06X

**Fairness Improvement (X factor):**
- 1 bank: 1.56X
- 2 banks: 1.37X
- 4 banks: 2.67X
- 8 banks: 2.57X
- 16 banks: 1.32X
- 32 banks: 2.35X
- 64 banks: 1.06X

---

### Table 5: Statistics for `art-vpr` with Different Number of DRAM Banks (IPTC: Instructions/1000-cycles)

| Number of Banks | 1 bank | 2 banks | 4 banks | 8 banks | 16 banks | 32 banks | 64 banks |
|-----------------|--------|---------|---------|---------|----------|----------|----------|
| **Base Throughput (IPTC)** |  210  |  122    |  190    |  287    |   304    |   402    |   707    |
| **FairMem Throughput (IPTC)** |  1.56X |  1.37X  |  2.67X  |  2.57X  |   1.32X  |   2.35X  |   1.06X  |
| **Throughput Improvement** |  1.56X |  1.37X  |  2.67X  |  2.57X  |   1.32X  |   2.35X  |   1.06X  |
| **Fairness Improvement** |  1.56X |  1.37X  |  2.67X  |  2.57X  |   1.32X  |   2.35X  |   1.06X  |

---

### Section 6.2.4: Effect of Memory Latency

Memory latency significantly impacts the performance of the DRAM system. Figure 11 illustrates how different DRAM latencies influence the mutual performance impact of `art` and `vpr`. We vary the round-trip latency of a request that hits in the row-buffer from 50 to 1000 processor clock cycles, and scale closed/conflict latencies proportionally.

As memory latency increases, the impact of `art` on `vpr` also increases. For example, `vpr`'s slowdown is 1.89X with a 50-cycle latency versus 2.57X with a 1000-cycle latency. FairMem reduces `art`'s impact on `vpr` for all examined memory latencies while also improving overall system throughput (Table 6).

Given that main DRAM latencies are expected to increase in modern processors [39], scheduling algorithms that mitigate the impact of memory performance hogs (MPHs) will become more important and effective in the future.

### Figure 11: Slowdown of `art` and `vpr` with Different DRAM Access Latencies

**Slowdown (independent normalization for each machine with different number of banks):**

| Latency (cycles) | 50 cyc | 100 cyc | 200 cyc | 300 cyc | 400 cyc | 500 cyc | 1000 cyc |
|------------------|--------|---------|---------|---------|---------|---------|----------|
| **FR-FCFS**      |  1229  |  728    |  401    |  278    |   212   |   172   |   88     |
| **FairMem**      |  1.19X |  1.24X  |  1.28X  |  1.29X  |   1.30X |   1.30X |   1.30X  |

**Metrics:**
- `art-vpr` base throughput (IPTC)
- `art-vpr` FairMem throughput (IPTC)
- FairMem throughput improvement
- FairMem fairness improvement

**Throughput Improvement (X factor):**
- 50 cycles: 1.19X
- 100 cycles: 1.24X
- 200 cycles: 1.28X
- 300 cycles: 1.29X
- 400 cycles: 1.30X
- 500 cycles: 1.30X
- 1000 cycles: 1.30X

**Fairness Improvement (X factor):**
- 50 cycles: 1.69X
- 100 cycles: 1.82X
- 200 cycles: 2.23X
- 300 cycles: 2.21X
- 400 cycles: 2.25X
- 500 cycles: 2.23X
- 1000 cycles: 2.22X

---

### Table 6: Statistics for `art-vpr` with Different DRAM Latencies (IPTC: Instructions/1000-cycles)

| Latency (cycles) | 50 cyc | 100 cyc | 200 cyc | 300 cyc | 400 cyc | 500 cyc | 1000 cyc |
|------------------|--------|---------|---------|---------|---------|---------|----------|
| **Base Throughput (IPTC)** |  1229  |  728    |  401    |  278    |   212   |   172   |   88     |
| **FairMem Throughput (IPTC)** |  1.19X |  1.24X  |  1.28X  |  1.29X  |   1.30X |   1.30X |   1.30X  |
| **Throughput Improvement** |  1.19X |  1.24X  |  1.28X  |  1.29X  |   1.30X |   1.30X |   1.30X  |
| **Fairness Improvement** |  1.69X |  1.82X  |  2.23X  |  2.21X  |   2.25X |   2.23X |   2.22X  |

---

### Section 6.2.5: Effect of Number of Cores

Finally, this section analyzes FairMem within the context of 4-core and 8-core systems. Our results show that FairMem effectively mitigates the impact of MPHs while improving overall system throughput in both 4-core and 8-core systems running different application mixes with varying memory-intensiveness.

### Figure 12: Effect of FR-FCFS and FairMem Scheduling on Different Application Mixes in a 4-Core System

**Application Mixes:**

- **4p-MIX1: stream, art, mcf, health**
- **4p-MIX2: stream, art, mcf, vpr**
- **4p-MIX3: small-stream, art, mcf, health**

In all the mixes, `stream` and `small-stream` act as severe MPHs when run on the baseline FR-FCFS system, slowing down other applications by up to 10.4X (and at least 3.5X) while themselves being slowed down by no more than 1.10X. FairMem reduces the maximum slowdown caused by these two hogs to at most 2.98X while also improving the overall throughput of the system (Table 7).

### Table 7: Statistics for Different Application Mixes in a 4-Core System

| Application Mix | 4p-MIX1 | 4p-MIX2 | 4p-MIX3 |
|-----------------|---------|---------|---------|
| **FR-FCFS**     |  10.5   |  10.0   |  9.5    |
| **FairMem**     |  1.5    |  1.0    |  0.5    |

**Metrics:**
- `art-vpr` base throughput (IPTC)
- `art-vpr` FairMem throughput (IPTC)
- FairMem throughput improvement
- FairMem fairness improvement

**Throughput Improvement (X factor):**
- 4p-MIX1: 1.19X
- 4p-MIX2: 1.24X
- 4p-MIX3: 1.28X

**Fairness Improvement (X factor):**
- 4p-MIX1: 1.69X
- 4p-MIX2: 1.82X
- 4p-MIX3: 2.23X

---

### Figure 13: Effect of FR-FCFS and FairMem Scheduling on Different Application Mixes in an 8-Core System

**Application Mixes:**

- **8p-MIX1: stream, art, mcf, health**
- **8p-MIX2: stream, art, mcf, vpr**
- **8p-MIX3: small-stream, art, mcf, health**

In the baseline system, `stream` and `small-stream` act as MPHs, sometimes degrading the performance of another application by as much as 17.6X. FairMem effectively contains the negative performance impact caused by the MPHs for all three application mixes. Furthermore, it is important to observe that FairMem is also effective at isolating non-memory-intensive applications (such as `crafty` in MIX2 and MIX3) from the performance degradation caused by the MPHs. Even though `crafty` rarely generates a memory request (0.35 times per 1000 instructions), it is slowed down by 7.85X by the baseline system when run within MIX2! With FairMem, `crafty`'s rare memory requests are not unfairly delayed due to a memory performance hog â€” and its slowdown is reduced to only 2.28X. The same effect is also observed for `crafty` in MIX3.

We conclude that FairMem provides fairness in the presence of memory performance hogs, ensuring that non-memory-intensive applications are not unduly affected.