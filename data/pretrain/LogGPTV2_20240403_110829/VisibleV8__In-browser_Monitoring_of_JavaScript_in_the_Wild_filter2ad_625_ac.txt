pathological slowdown over 100x.
3.4 Maintenance & Limitations
Thanks to the small size and minimal invasiveness of VV8’s patches,
maintenance has thus far proved inexpensive. Development began
on Chrome 63, then easily transitioned to Chrome 64, which was
used for primary data collection. We have since ported our patches
through Chrome 72 and encountered only trivial issues in the pro-
cess (e.g., whitespace changes disrupting patch merge, capitalization
changes in internal API names).
Our trace logs must be created on the fly as new threads are en-
countered. Since the Chrome sandbox prevents file creation, we
currently run VV8 with the sandbox disabled as an expedience. In
production, we run VV8 inside isolated Linux containers, mitigating
the loss of the sandbox somewhat. Future development will include
sandbox integration should the need arise.
Past work [39, 40] on fingerprinting JS engines indicates that
sophisticated adversaries could use relative scores across micro-
benchmarks as a side-channel to identify VV8. However, such bench-
marks and evasions would be detectable in VV8’s trace logs, and JS
timing side-channel attacks can be disrupted [17, 47]. In any case,
it is unlikely that an adversary sophisticated enough to fingerprint
VV8 in the wild would not also be able to fingerprint in-band instru-
mentation, which also shows measurable deviation from baseline
performance.
Furthermore, we expect to improve VV8 performance in future
iterations by exploring asynchronous log flushing, log-filtering tests
placed in the injected bytecode (where they can be JIT optimized),
and cheaper forms of context tracking.
3.5 Collection System
To collect data at large scale using VV8, we built the automated crawl-
ing and post-processing system diagrammed in Figure 4. Worker
nodes (for collection, post-processing, and work queues) are de-
ployed across a Kubernetes cluster backed by 80 physical CPU cores
and 512GiB of RAM distributed across 4 physical servers. Initial
jobs (i.e., URLs to visit) are placed in a Redis-based work queue to
be consumed by collection worker nodes. Post-processing jobs (i.e.,
archived logs to parse and aggregate) are placed in another work
queue to be consumed by post-processing worker nodes. Collection
metadata and trace logs are archived to a MongoDB document store.
Aggregate feature usage data is stored in a PostgreSQL RDBMS for
analytic queries.
The collection worker node Docker image contains the VV8 bi-
nary itself and a pair of accompanying programs written in Python 3:
Carburetor and Manifold. Carburetor is responsible for fueling VV8:
3The full results can be viewed at http://dromaeo.com/?id=276022,276023,276026,
276027; the four columns are Chrome (plain), Chrome (w/in-band), VisibleV8 (light),
and VisibleV8 (full), respectively.
Figure 3:
Bench.org [8] and Mozilla Dromaeo [9]
Instrumentation performance on Browser-
the worst case. We verified this expectation by measuring the over-
head by benchmarking a set of Chrome and VV8 variants with both
the WebKit project’s BrowserBench [8] and Mozilla’s Dromaeo [9].
Unless otherwise noted, tests were performed under Linux 4.19.8 on
an Intel Core i7-7700 (4 cores, 3.6GHz) with 16GiB RAM and an SSD.
See Figure 3.
We tested four variants of Chrome 64 for Linux, including a base-
line variant with no JS instrumentation at all (plain). We include two
VV8 builds: the complete system as described above (full) and a vari-
ant with property access interception disabled (light). VV8-light is
roughly equivalent in coverage and logging to our final variant, stock
Chrome running a custom prototype-patching extension roughly
equivalent to the (unreleased) instrumentation used by Snyder et al.
to measure browser feature across the Alexa top 10K [49]. This last
variant (w/in-band) attempts to provide an apples-to-apples com-
parison of in-band and out-of-band instrumentation instrumenting
a comparable number of APIs (functions only) and recording compa-
rably rich trace logs. All Chrome builds were based on the Chrome 64
stable release for Linux and use the same settings and optimizations.
BrowserBench tests the JS engine in isolation (JetStream, ARES-
6), JS and the DOM (Speedometer), and JS and Web graphics (Mo-
tionMark). VV8-light either meets or decisively beats its in-band
equivalent in every case. VV8-full consistently suffers 60% to 70%
overhead vs. the baseline, but on the whole-browser tests (Speedome-
ter, MotionMark) it performs comparably to the in-band variant,
which captures significantly less data (i.e., no property accesses).
These numbers match our experience interacting with VV8, where
we observe it providing acceptable performance on real-world, JS-
intensive web applications like Google Maps. Significantly, VV8-full
on the workstation compared favorably (i.e., equal or better Browser-
Bench scores) to Chrome 64 plain on a battery-throttled laptop
running Linux 4.18.15 on an Intel Core i7-6500 (2 cores, 2.5GHz) with
16GiB RAM and an SSD.
The Mozilla Dromaeo suite of micro-benchmarks focuses exclu-
sively on JS engine performance. It avoids the browser’s layout and
render logic as much as possible, and reveals more slowdowns for
JetStreamARES-6MotionMarkSpeedometerDromaeo0.00.20.40.60.81.0Relative to Normalized BaselineChrome 64 (plain)Chrome 64 (w/in-band)VisibleV8 (light)VisibleV8 (full)VisibleV8: In-browser Monitoring
of JavaScript in the Wild
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
Figure 4: The complete data collection and post-processing system
using the Chrome DevTools API to open a tab, navigate to a URL,
and monitor progress of the job. Manifold handles the byproducts
of execution, compressing and archiving the trace log files emitted
during automated browsing.
The post-processor worker node Docker image contains a work
queuedispatcherandthemainpost-processorengine.Thedispatcher
interfaces with our existing work queue infrastructure and is writ-
ten in Python 3. The post-processor engine is written in Go, which
provides ease-of-use comparable to Python but significantly higher
performance.
4 DATA COLLECTION
4.1 Methodology
Overview. We collected native feature usage traces and related data
by automating VV8 via the Chrome DevTools interface to visit the
Alexa top 50k web domains. We began each visit to DOMAIN using
the simple URL template http://DOMAIN/. We visited each domain
in our target list 5 times (see below); each planned visit constituted
a job. We recorded headers and bodies of all HTTP requests and
responses along with the VV8 trace logs. Trace log files were com-
pressed and archived immediately during jobs, then queued for post-
processing. Post-processing associated logs with the originating
job/domain and produced our analytic data set.
User Input Simulation. Simply visiting a page may result in
much JS activity, but there is no guarantee that this activity is
representative. The classic challenge of dynamic analysis—input
generation—rears its head. We borrowed a solution to this prob-
lem from Snyder et al. [49]: random “monkey testing” of the UI
using the open source gremlins.js library [5]. To preserve some
degree of reproducibility, we used a deterministic, per-job seed for
gremlins.js’s random number generator.
Once a page’s DOM was interaction-ready, we unleashed our
gremlins.js interaction for 30 seconds. We blocked all main-frame
navigations if they led to different domains (e.g., from example.com
to bogus.com). When allowing intra-domain navigation (e.g., from
example.com to www.example.com), we stopped counting time until
we loaded the new destination and resume the monkey testing. We
immediately closed any dialog boxes (e.g., alert()) opened during the
monkey testing to keep JS execution from blocking. This 30 second
mock-interaction procedure was performed 5 times, independently,
per visited domain. (Snyder et al. [49] arrived at these parameters
experimentally.)
4.2 Data Post-Processing
We parsed the trace logs to reconstruct the execution context of
each recorded event and to aggregate results by that context. The
resulting output included all the distinct scripts encountered and
aggregate feature usage tuples.
Script Harvesting. VisibleV8 records the full JS source of every
script it encounters in its trace log (exactly once per log). We ex-
tracted and archived all such scripts, identifying them by script hash
and lexical hash. Script hashes are simply the SHA256 hash of the
CollectionWork QueuePostgreSQL Analysis DatabaseAggregatesMongoDBArchives gCollection Worker Nodes Uncompressed Trace Log Files CarburetorManifoldCustom Chromium BuildChromeDevToolsRemoteControl APIVisibleV8Blink, etc.URLsCompressed  Log DataPost-ProcessingWork QueueLogsPost-Processing Worker NodesDispatcherLog Parser/AggregatorLogsCompressed Log Data IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
Jordan Jueckstock and Alexandros Kapravelos
full script source (encoded as UTF-8); they served as the script’s
unique ID. Lexical hashes were computed by tokenizing the script
and SHA256-hashing the sequence of JS token type names that result.
These are useful because many sites generate “unique” JS scripts
that differ only by timestamps embedded in comments or unique
identifiers in string literals. Such variants produced identical lexical
hashes, letting us group variant families.
Feature Usage Tuples. We recorded a feature usage tuple for
each distinct combination of log file, visit domain, security origin,
active script, access site, access mode, and feature name. The log file
component let us distinguish collection runs. The visit domain is the
Alexa domain originally queued for processing. The security origin
was the value of window.origin in the active execution context,
which may be completely different from the visit domain in the con-
text of an . The active script is identified by script hash. The
access site is the character offset within the script that triggered this
usage event. The access mode is how the feature was used (get, set,
or call). The feature name is a name synthesized from the name of
the receiver object’s constructor function (effectively its type name)
and the name of the accessed member of that object (i.e., the property
or method name).
4.3 Results
Success and Failure Rates. Our methodology called for 5 visits
to the Alexa 50k, so the whole experiment consisted of 250,000
distinct jobs. Successful jobs visited the constructed URL, launched
gremlins.js,andrecordedatleast30secondsofpseudo-interaction
time. Jobs resulting in immediate redirects (by HTTP or JS) to a differ-
ent domain before any interaction began were deemed “dead ends”.
From job status we extract the per-domain coverage listed in Table
2. For “active” domains, all 5 jobs succeed and we observed native JS
API usage. For “silent”: all 5 jobs succeed, but we observed no native
JS API usage. For “facade”: all 5 jobs were “dead ends” (i.e., the domain
is an alias). All of the above are considered “successful” domains.
Some domains were “broken,” with all 5 jobs failing; a tiny number
were “inconsistent,” with a mix of failed/succeeded jobs. This failure
rate is not out of line with prior results crawling web sites. Snyder
et al. [49] reported a lower per-domain failure rate (2.7%), but this
was over the Alexa top 10,000 only. On the other extreme, a recent
measurement study by Merzdovnik et al. [36] reported successful
visits to only about 100k out of the top Alexa 200k web domains.
Aggregate Feature Usage. Over the entire Alexa 50k, we ob-
served 53% of Chrome-IDL-defined standard JS API features used
at least once. Note that our observations comprise a lower bound on
usage, since we did not crawl applications requiring authentication
(e.g., Google Documents), which we intuitively anticipate may use a
wider range of APIs than generic, public-facing content. Most mod-
ern sites use JS heavily, but no site uses all available features. The
s
s
e
c
c
u
S
t
n
e
t
s
i
s
n
o
C
Active
Silent
Facade
Broken
Inconsistent
TOTAL
Table 2: Final domain status after collection
42,845
1,702
1,508
3,214
731
50,000
85.69%
3.40%
3.02%
6.43%
1.46%
100.00%
92.11%
98.54%
Figure 5: Cumulative feature use over the Alexa 50k
plot in Figure 5 thus climbs steeply before leveling out into a gentle
upward slope. The small but distinctive “cliffs” observed at rocket-
league.com (Alexa 16,495) and noa.al (Alexa 22,184) are caused by
large clumps of SVG-related features being used for the first time.
5 BOT DETECTION ARTIFACTS
Modern websites adapt their behavior based on the capabilities of the
browser that is visiting them. The identification of a specific browser
implementation is called user-agent fingerprinting and it is often used
for compatibility purposes. To provide a case study of VV8’s unique
abilities, we use it to automatically discover artifacts employed by a
form of user-agent fingerprinting used by some websites in the wild
to detect automated browser platforms.
The technique we study exploits the presence of distinctive, non-
standard features on API objects like Window (which doubles as the
JS global object) and Navigator as provided by automated browsers
and browser simulacra. (Since even modern search engine indexers
need some degree of JS support[1], we do not consider mechanisms
used to identify “dumb,” non-JS-executing crawlers like wget.) Here
VV8’sabilitytotracenativepropertyaccesseswithout a priori knowl-
edge of the properties to instrument sets it apart from in-band in-
strumentation, which cannot wrap a proxy around the global object
or the unforgeable window.document property. Note that “native
API property access” here means a property access on an object that
crosses the JS/native API boundary, regardless of whether or not
that specific property is standardized or even implemented.
Bot detection is a special case of user-agent fingerprinting, where
“bots” are automated web clients not under the direct control of
a human user (e.g., headless browsers used as JS-supporting web
crawlers). Bots may be a nuisance or even a threat to websites [25],
and they may cause financial loss to advertisers via accidental (or
intentional) impression and/or click fraud. If the visitor’s user-agent
fingerprint matches a known bot, a site can choose to “defend” itself
against undesired bot access by taking evasive action (e.g., redi-
recting to an error page) [24]. Non-standard features distinctive
to known bot platforms, then, constitute bot artifacts. We exploit
10000200003000040000Alexa Domain Rank0%10%20%30%40%50%60%70%80%90%100%WebIDL Features ObservedCumulative Observed Feature UsageCumulative Feature Usage53% (Over top 50k)VisibleV8: In-browser Monitoring
of JavaScript in the Wild
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
Artifact Name
Window._phantom
Window.webdriver
Bot Platform Indicated
PhantomJS [10]
Selenium [11] WebDriver
Window.domAutomation ChromeDriver (WebDriver for Chrome)
Table 3: Bot detection seed artifacts
VV8’s comprehensive API-property-access tracing to systematically
discover novel artifacts.
5.1 Artifact Discovery Methodology
We discover previously unknown bot artifacts by clustering the ac-
cess sites (i.e., script offsets of feature accesses) for candidate features
near those of known “seed” artifacts. The key insight underlying our
approach is code locality: in our experience, artifact tests tend to be
clustered near each other in user-agent fingerprinting code encoun-
tered across the web. We exploit this locality effect to automate the
process of eliminating noise and identifying a small set of candidates
for manual analysis.
CandidateFeaturePool.Beforesearchingforartifacts,weprune
our search space to eliminate impossible candidates. We eliminate
features defined in the Chrome IDL files, since these are standard-
derived features unlikely to be distinctive to a bot platform. We also
eliminate features seen set or called: these are likely distinctive to
JS libraries, not the browser environment itself. This second round
of pruning is especially important because JS notoriously conflates
its global namespace with its “global object.” Thus, in web browsers,
global JS variables are accessible as properties of the window object
along with all the official members of the Window interface. Retain-
ing only features we never see set or called eliminates significant
noise (e.g., references to the Window.jQuery feature) from our pool
of candidate features: from 7,928,522 distinct names to 1,907,499.
Seed Artifact Selection. We further narrow our candidate pool
using access site locality clustering around “seed” artifacts (Table 3).
These features are among the most commonly listed in anecdotal bot
detection checklists found in developer hubs like Stack Exchange [2],
reflecting the popularity of Selenium’s browser automation suite
and the lighter-weight PhantomJS headless browser.
Candidate Artifact Discovery. With a pruned candidate fea-
ture pool and a set of seed artifacts in hand, we can automatically