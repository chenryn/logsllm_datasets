### Implementations and Inheritance

Three different implementations were associated with each IDL (Interface Definition Language) interface. In the first implementation, a single class is responsible for all operations. In the other two, simple and multiple inheritance are used, respectively, to reveal potential problems related to the reification of inherited operations.

### 4.2. First Experiments and Results

According to Section 3.1, a specific test environment was designed to verify the correctness of the interaction channel between objects and metaobjects. In our case study, objects and metaobjects were implemented to observe and check the interactions.

#### IDL Service Interface
```idl
interface ServiceForTestingHandlingOfType-Long {
    long returnlong();
    void InLong(in T pl);
    void OutLong(out T pl);
    void InOutLong(inout T pl);
    long AIILong(in long p1, out long p2, inout long p3);
}
```
*Figure 11: IDL service interface*

### Oracle Procedure Implementation

To implement the oracle procedure, a table was generated for each operation included in the Service interface of the server object. This table describes the signature of the operation and provides the method identifier used by the reification mechanisms. This information is necessary for checking the compliance between the data reified to the metaobject and the data handled by the driver object.

The test experiments revealed that different method invocations could be reified to the meta-level using the same method identifier, making it impossible for the metaobject to distinguish which method was actually invoked. This problem was observed with both simple and multiple inheritance. In the case of simple inheritance, the issue was easily fixed by modifying the code transformation rules that generate the MOP (Meta-Object Protocol) implementation. Specifically, the algorithm used to assign method identifiers to IDL operations was adjusted to avoid the reuse of method identifiers. However, in the case of multiple inheritance, the problem is much more difficult to fix and has not been resolved yet.

### Testing Levels

#### Testing Level 2: Behavioral Intercession Mechanisms
(Figure 4). According to the observability constraints associated with the oracle procedure, the server object reports on its execution by generating execution traces. These traces are produced by every public, protected, or private method implementation. It's important to note that the activation of a Service interface operation may lead to the execution of other internal methods. As stated in Section 2.2, the object must encapsulate these internal invocations, which should not be reified.

Two problems were identified during the test experiments:
1. **Encapsulation of Internal Activity**: Public methods not belonging to the Service interface of the server object were reified to the meta-level, indicating that the behavioral intercession mechanisms triggered reification when they should not have.
2. **Internal Method Invocation**: When the activated code performs an internal invocation to a method of a parent class that is redefined in its child class, the method of the child class is executed. This reveals an omission fault, as the MOP was supposed to handle this kind of invocation, but no code transformation rules dealt with the associated transformations.

Both problems were fixed before proceeding to testing level 3.

#### Testing Level 3: Introspection Mechanisms
(Figure 5). The server object classes in this level provide attributes with different data types. Due to dependability issues, the MOP under test restricts the set of data types that can be used for defining attributes. For example, multiple-level pointers are not allowed. In the implementation of each server object class, a constructor was included to initialize every attribute from a parameter value (step 1 in Figure 5). No faults were revealed during the test experiments.

#### Testing Level 4: Structural Intercession Mechanisms
(Figure 6). The server object classes in this phase were defined using the same features as in the previous testing level. Additionally, the server object classes provide an initialization method required by the test driver object to modify the state of the object (step 3 in Figure 6). No faults were revealed during the test experiments.

Further experiments need to be conducted at testing levels 3 and 4 to consider more complex data types, such as arrays, structures, and class data types.

### 5. Related Work and Discussion

Most research on the verification of reflective architectures focuses on defining high-level models using formal languages. In [9], the authors propose a model based on an architectural description language called WRIGHT [18]. This language describes a system as a set of architectural components linked by connectors (MOPs). The notion of MOP in that work differs from the one considered here, as it encapsulates both the reflective capabilities of the system and the system meta-level. A different model based on the π-calculus [19] is proposed in [8], describing a reflective system as a system composed of agents (objects and metaobjects) that communicate by exchanging messages. The MOP in this context handles the interactions between objects and metaobjects, conforming to the definition in Section 2.2.

These models can be analyzed using tools specifically developed for the associated formal languages, ensuring high-level properties such as the absence of deadlocks in the MOP. However, the level of description in these models is too generic to be helpful in finding problems associated with a particular MOP implementation.

The generic strategy presented in this paper can be used for testing any MOP of the family described in Section 2.2. The four testing levels of the strategy are required for checking the implementation of the FRIE5@.S MOP. Less sophisticated MOPs may be tested using only some of the levels. For instance, MOPs that only provide reification and behavioral intercession mechanisms (like in MAUD and C.A.W.) require testing levels 1 and 2; the COMA portable interceptors (a form of reification of CORBA requests) can be tested using testing level 1; the testing of the serialization features provided by a Java virtual machine (a form of introspection and structural intercession mechanisms) only requires testing levels 3 and 4.

### 6. Conclusion

Reflection is a powerful concept for implementing complex, dependable systems with a clear separation of concerns. Despite its interest, the use of this technology remains questionable due to the limited work on its verification. The test strategy presented in this paper is a step forward in defining a global strategy for verifying MOP-based reflective architectures. The main advantage of this approach is the systematic and incremental order proposed for testing the reflective capabilities of the system. This order reduces the test effort by enabling the reuse of already tested mechanisms for testing the remaining ones, thus facilitating the implementation of test environments. Furthermore, it allows the tester to progressively identify and fix potential problems related to the reflective mechanisms, one after the other.

The efficiency of this progressive testing and debugging process was evident throughout the experiments conducted on the FRIE5@.S MOP. These experiments were performed in parallel with the development of the MOP, i.e., with the definition of the code transformation rules used to generate the MOP implementation (see Figure 8). Specific rules are associated with each of the four reflective mechanisms covered by the test strategy. As a result, every testing level i (i = 1, ..., 4) concentrates on a subset S, of rules. Fixing potential faults in the rules of S, before defining the rules of S,+, avoids possible ripple effects of these corrections on the rules of S,,,. Hence, errors were detected and fixed early and incrementally. When a problem cannot be fixed, the test results allow at least the identification of restrictions on the use of the MOP, such as the case of multiple inheritance, which is not correctly handled by the current version of the FRIENDsmetaobject protocol.

Ongoing work is focused on defining rigorous test criteria to guide the automatic generation of test case input values according to the test objectives specific to each testing level. The generic test strategy presented does not tackle this issue yet. Based on existing elements of solutions (e.g., [10][11][12]), we plan to propose specification-based criteria that can be used for any MOP-based reflective architecture, supplemented with implementation-based criteria.

Finally, the MOP model used in the paper assumes that objects and metaobjects are deterministic and mono-threaded system components. To lift these restrictions, more work needs to be done on identifying the necessary reflective features. The testing levels of the strategy will then need to be revised and extended to handle these new features, which is an objective of our future investigation.

### References

[1] P. Maes, “Concepts and experiments in computational reflection”, in Object-Oriented Programming Systems, Languages and Applications, ACM Press, Orlando, 1987, vol. 22, pp. 147-155.

[2] Sun Microsystems, “Java reflective API documentation”, online document available at: http://java.sun.com/j2se/1.3/docs/api/java/lang/reflect/package-summary.html.

[3] Y. Yokote, “The Apertos Reflective Operating System: The Concept and its implementation”, in Object-Oriented Programming Systems, Languages and Applications, ACM Press, New York, 1992, vol. 27-10, pp. 414-434.

[4] G. Blair et al., “An Architecture for Next Generation Middleware”, in Middleware, Springer-Verlag, UK, 1998, pp. 191-206.

[5] R.J. Stroud, “Transparency and Reflection in Distributed Systems”, ACM Operating Systems review, 1993, vol. 22-2, pp. 99-103.

[6] J.-C. Fabre and T. Pérennou, “A Metaobject Architecture for Fault-Tolerant Distributed Systems: the F!IE?(DS Approach”, IEEE Trans. on Computers, 1998, vol. 47, pp. 78-95.

[7] G. Kiczales et al., The Art of the Metaobject Protocol, MIT Press, ISBN 0-262-61074-4, 1991.

[8] E. Marsden et al., “Towards Validating Reflective Architectures: Formalization of a MOP”, in Workshop on Reflective Middleware, New York, 2000, pp. 33-35.

[9] I. Welch and R. Stroud, “Adaptation of Connectors in Software Architectures”, in Workshop on Component-Oriented Programming, Brussels, 1998, pp. 145-146.

[10] G.V. Bochmann and A. Petrenko, “Protocol testing: review of methods and relevance for software testing”, in International Symposium on Software Testing and Analysis (ISSTA ‘94), ACM Press, Seattle, 1994, pp. 109-124.

[11] D. Kung, P. Hsia, and J. Gao (eds), Testing Object-Oriented Software, IEEE Computer Society, 1998.

[12] R.V. Binder, Testing Object-Oriented Systems, Addison-Wesley, 1999.

[13] M.-O. Killijian and J.C. Fabre, “Implementing a Reflective Fault-Tolerant CORBA System”, in IEEE Symposium on Reliable Distributed Systems, Germany, 2000, pp. 154-163.

[14] S. Chiba, “Macro Processing in Object-Oriented Languages”, in Technology of Object-Oriented Languages and Systems, IEEE Press, Australia, 1998, pp. 113-126.

[15] G. Agha et al., “A Linguistic Framework for Dynamic Composition of Dependability Protocols”, in Dependable Computing for Critical Applications 3, Vol. 8, in the Series on Dependable Computing and Fault-Tolerant Systems, Springer-Verlag, 1993, pp. 345-363.

[16] B. Garbinato et al., “Implementation of the GARF Replicated Objects Platform”, in Distributed Systems Engineering Journal, 1995, vol. 2, pp. 14-27.

[17] P. Thévenod-Fosse et al., “Software Statistical Testing”, in Predictably Dependable Systems (Randell, Laprie, Kopetz, and Littlewood, editors), Springer-Verlag, 1995, pp. 253-272.

[18] J.R. Allen, “A formal Approach to Software Architecture”. PhD Thesis, Carnegie Mellon University, 1997.

[19] A.J.R.G. Milner, A Calculus of Communicating Systems, LNCS vol. 92, Springer-Verlag, 1980.