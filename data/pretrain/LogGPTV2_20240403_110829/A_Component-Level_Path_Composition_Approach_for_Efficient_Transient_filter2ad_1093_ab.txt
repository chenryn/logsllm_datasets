level. In the next section, we show how the new technique of
path composition can compute those values more ef(cid:2)ciently
at the subpath level.
3. Algorithms for Path Composition and Path
Selection
We now present the main technical contribution of this pa-
per. This section is presented in the following manner. First,
we give the justi(cid:2)cation for how paths can be composed from
subpaths. This is done by showing how paths can be decom-
posed into subpaths. Next, we show how the subpath val-
ues are computed and, subsequently, how they can be used to
compute the path values. We then present the complete and
necessarily ef(cid:2)cient algorithm for composing the subpaths.
The last part of this section describes an approach for select-
ing important paths from the precomputed subpaths.
3.1. The Path-Composition Algorithm
The essence of the path-composition algorithm is that it is
much quicker to compute paths by composing subpaths than
to compute the paths themselves directly as in (2), (3), and
(4). To that end, we explain here how they can be composed
by showing how they can be decomposed into subpaths. Let
l1 and l2 be the local transitions from components 1 and 2,
tf1;2g be a synchronized transition between components 1 and
2, and tf4;5g be a synchronized transition that does not affect
components 1 and 2. There are three general cases to con-
sider:
Case 1: Given that a path consists of local transitions only,
such as l1 (cid:14) l2 (cid:14) l1 (cid:14) l2, we can decompose it into two sub-
paths l1(cid:14)l1 and l2 (cid:14)l2, since l1 does not affect component
2 and similarly l2 does not affect component 1.
cl(i)((cid:25) (cid:14) a) =8>>>>>>>>:
The state distribution vector after the transition of the
and from (2),
path (cid:25) is p[(cid:25)] = (cid:10)J
the path is computed by R[(cid:25)] =
the reward of
j=1p(j)[(cid:25)(j)],
j=1 p(j)[(cid:25)(j)]NJ
j=1 p(j)[(cid:25)(j)](cid:17)(cid:16)JJ
j=1 r(j)(cid:17). Note that p(j)[(cid:25)(j)] is a
row vector, whereas r(i) is a column vector. If (cid:12) is (cid:10) (i.e., re-
wards are computed as a product of component rewards), then
j=1 p(j)[(cid:25)(j)]r(j).
When (cid:12) is (cid:8), the computation is slightly more complex,
yielding
(cid:16)NJ
j=1 r(j) = QJ
R[(cid:25)] = NJ
r(j)!
R[(cid:25)] =  J
p(j)[(cid:25)(j)]!  J
Nj=1
Lj=1
Pj=1(cid:18) J
p(i)[(cid:25)(i)](cid:19)(cid:16)eT
1 N r(j)N eT
Ni=1
Pj=1 p(j)[(cid:25)(j)]r(j)  J
Qi=1;i6=j
i+1(cid:17)
ni!!
where en is a row vector of length n with all elements
equal to 1. For notational convenience de(cid:2)ne (cid:23) (i)((cid:25)(i)) =
p(i)[(cid:25)(i)]eT
ni(cid:0)1
=
=
nJ
J
J
p(i)[(cid:25)(i)]r(i) and (cid:24)(i)((cid:25)(i)) =
p(j)[(cid:25)(j)]eT
that the reward for the path (cid:25) can be computed as either
Qj=1;i6=j
nj such
J
R[(cid:25)] =
J
Yi=1
(cid:23)(i)((cid:25)(i)) or
J
Xi=1
(cid:23)(i)((cid:25)(i)) (cid:1) (cid:24)(i)((cid:25)(i)) : (10)
the
local
Similarly,
and synchronized transitions.
the probability of a path can be de-
composed into the subpath probabilities due to the
The probabil-
local
is
ity of
transitions
and
the probability due to the synchronized transitions is
subpath due to the
P robL((cid:25)(i)) = Qk2f1;:::;(cid:12)(cid:12)(cid:25)(i)(cid:12)(cid:12)g^(cid:25)(i)
P robS((cid:25)) =Qk2f1;:::;j(cid:25)jg^(cid:25)(k)2T S
Yi=1
P rob((cid:25)) = P robS((cid:25))
(cid:3) , such that
(i)).
(11)
P robL((cid:25)
(cid:3) ,
(cid:21)li
(cid:21)(cid:25)(k)
(k)=li
J
l
Thus, we can exploit redundant computation across many
paths to improve performance by (cid:2)rst decomposing the paths
into basic subpaths. Various real values of the subpaths are
then precomputed and cached. Afterward, the subpaths can
be composed, and their cached values can be used to compute
the bounds on the rewards computed by (3) and (4).
Before we introduce the algorithm for composing the sub-
paths, we present a small example to show the advantages
of an isolated computation and caching of (cid:23) (i)((cid:25)(i)) and
(cid:24)(i)((cid:25)(i)). A model that has two components and a single
synchronized transition t that can be disabled only by the (cid:2)rst
component will be analyzed. Thus, given A = fl1; l2; t; (cid:22)t1g,
we consider the set of paths of length 3, and we assume that
each matrix P(i)
contains nz non-zeros and that the remain-
ing matrices contain nz=2 elements such that the effort it
takes to analyze a path of length x equals x (cid:1) nz. P 3 con-
tains 43 = 64 different paths, and since each path requires
a computational effort of 3nz, the overall cost for this naive
computation is on the order of 192nz.
Observe that the number of paths at the state level is usu-
ally much larger and that every path in our approach usually
represents a large number of paths at the state level. By stor-
ing intermediate vectors p(i)[(cid:25)(i)], one can avoid having to
recompute the vectors many times. Thus, if path l1l1l1 has
been analyzed, path l1l1l2 can be computed with a cost of
only nz. From a computational point of view, the computa-
tion requires a depth-(cid:2)rst traversal of the tree that describes
all possible paths. The number of vectors to be stored equals
the depth of the tree, which is acceptable since the vectors
have dimensions that correspond to the sizes of the compo-
nent state spaces and not the overall state space. Because
already-computed vectors are reused, the number of required
operations is proportional to 81nz. Exploitation of the partial
order reduction reduces the number of paths from 64 to 52.
The effect of the reduction is relatively small for this exam-
ple, since the path length is short and we consider only two
components. Thus, only the sequences of local transitions can
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:27:37 UTC from IEEE Xplore.  Restrictions apply. 
be reordered; for example, l1l1l2, l1l2l1, and l2l1l1 are equiv-
alent paths, and the (cid:2)rst is the canonical path. If intermediate
results are reused, the cost of the path computation using the
canonical paths is proportional to 76nz.
If we consider local paths for the components, then the
(cid:2)rst component describes 33 = 27 paths. For the estimation
of the computational cost, recall that at the component level,
the multiplication of a vector with a matrix describing a syn-
chronization requires an effort in 0:5nz rather than nz. Since
there is 1 path with 3 local transitions, there are 6 paths with 2
local transitions, there are 12 paths with 1 local transition, and
there are 8 paths without local transitions, the total cost is pro-
portional to 54nz. Additionally, since vectors at the compo-
nent level can be reused, the cost can be reduced to 26nz. For
the second component, we have only two different transitions,
because the synchronized transition can be disabled only by
the (cid:2)rst component. Thus, there are 8 different paths, and the
computational cost is proportional to 18nz without reuse of
intermediate vectors and is proportional to 10:5nz with reuse
of the vectors. If we reuse vectors, then the overall cost is
proportional to 36:5nz. Afterward, all values (cid:23) (i)((cid:25)(i)) and
(cid:24)(i)((cid:25)(i)) are known for local paths with lengths up to 3. The
values can be used in (10) for the computation of the reward
values.
Observe that using the local paths with lengths up to 3, it is
possible to compute rewards for all global paths with lengths
less than or equal to 3 and also some other global paths with
lengths 4, 5, and 6. Computing those additional global paths
of lengths greater than 3 is inexpensive, and it helps to tighten
the lower bounds further. To provide a fair comparison with
our previous work, however, our implementation of the path
decomposition algorithm does not take advantage of the addi-
tional global paths. The results we present in Section 4 show
the performance comparison up to the same path lengths for
both implementations.
In this small example, the effect of local path computation
is a reduction of the computational effort by a factor of 2.
However, with an increasing path length, an increasing num-
ber of components, and an increasing number of synchronized
transitions, the effect grows exponentially. In the following
section, we introduce an algorithm for ef(cid:2)cient computation
of rewards for global paths from the results of local subpaths.
3.2. Algorithms for E(cid:14)cient Composition of
Subpaths
We have shown how paths can be decomposed into sub-
paths and how the subpath values are computed. In order to
gain the bene(cid:2)ts of the path-composition algorithm, we must
have an effective strategy for exploring the subpaths and com-
posing them ef(cid:2)ciently. There are several strategies for ex-
ploring and computing the basic subpaths. Either they may
all be precomputed before any computation of reward bounds
begins, or they may be computed when required during the
computation of the bounds. In both cases, the storage com-
plexity is combinatorial in the number of subpaths. With so
many paths to consider, the selection of the valid subpaths
to compose is also expensive. We now describe an ef(cid:2)cient
algorithm for precomputing and composing the subpaths.
Instead of computing and storing all of the subpaths en
masse, we can compute and store sets of them in stages. At
each stage, we identify a set of subpaths that are compos-
able with each other and compute only those. Subpaths are
composable when they have the same longest common sub-
sequence (LCS) [3] of synchronized transitions. In order to
simplify the computation, we assume that the synchronized
transitions cannot be transposed past each other, so that the
LCS is unique. Thus, we can partition the set of all sub-
paths into classes of composable subpaths, with each class
characterized by a sequence of synchronized transitions. For
example, suppose t1; t2; t3 2 T S are synchronized tran-
sitions. Then, the class of composable subpaths that have
the sequence ht1; t2; t3i is flj
(cid:3)g,
(cid:3) denotes zero or more occurrences of the local tran-
where lj
sition of component j (1 (cid:20) j (cid:20) J).
More formally, given a sequence of synchronized transi-
tions ht1; : : : ; tni and a maximum subpath length L = n+K,
the class of composable subpaths corresponding to the se-
quence is
(cid:3) (cid:14) t3 (cid:14) lj
(cid:3) (cid:14) t2 (cid:14) lj
(cid:3) (cid:14) t1 (cid:14) lj
CS(ht1; : : : ; tni ; L) =
J
[j=1
K
[k
(j)
0 +:::+k
(j)
n =0
(j) = lj
k(j)
0 (cid:14) t1 (cid:14) lj
k(j)
1 (cid:14) t2 (cid:14) lj
k(j)
2 (cid:14) : : : (cid:14) tn (cid:14) lj
n(cid:25)
k(j)
n o :
(12)
For each subpath (cid:25)(j) 2 CS(ht1; : : : ; tni ; L), we need to
precompute and store only one or two real values, namely
(cid:23)(i)((cid:25)(j)) and (cid:24)(i)((cid:25)(j)). The probability of the subpath is
given by
P rob(j)(K) =
(cid:21)K
lj
(cid:3)K ;
(13)
and the probability of the synchronized transitions equals
P robS(t1 : : : tn) = Qn
k=1 (cid:21)tk
(cid:3)n
.
(14)
Each path that is composed from some subpaths in the
class CS(ht1; : : : ; tni ; L) can be considered as a canonical
path that represents a set of equivalent paths. Computing the
number of equivalent paths using the new algorithm is now
straightforward (as compared to (8) in Section 2.2). The num-
ber of equivalent paths is simply
jht1;:::;tnij