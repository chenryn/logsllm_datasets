inal object in input images, using the trigger as an alternative
channel to classify the image to the target label. This hy-
pothesis was further validated by using trigger-only images
constructed by inserting the trigger to random images that
don’t belong to any class: in this case, at least 98.7% of the
trigger-only images were classiﬁed as the target label (last
row of Table 1), even when the model is infected by only a
small set of training samples all from a single source class.
Further investigation revealed clear differences between the
representations of the normal images in the target class and
Figure 1: Effect of data contamination attack on the target label’s representa-
tions, which have been projected to their ﬁrst two principle components. Left
ﬁgure shows the representations produced by a benign model (without the
backdoor). Right ﬁgure shows the representations produced by an infected
model (with the backdoor).
those of the infected images. Fig. 1 shows the representations
projected onto their ﬁrst two principal components, where the
infected images from two different source classes are labeled
by 3 and 5, respectively. As we can see, the representations of
the normal images from class 0 produced by the benign model
and the infected images from classes 3 and 5 can be easily
separated, whereas the representations of the infected images
from the source classes of label 3 and 5 produced by the
infected model cannot be completely separated, but are still
different from those of the normal images in the target class
(label 0), even though they are all classiﬁed as the target class.
This observation indicates that under existing attacks, the
representation of an infected image is predominantly affected
by the trigger, and as a result, it tends to be quite different
from that of a normal image with the target label.
We note these observations hold for some existing back-
door detection techniques. In Section 3.2, we provide a more
detailed analysis. So a fundamental question is whether these
assumptions can be bypassed by a successful backdoor attack
and whether a model can be infected through data contami-
nation, in a way that the representations of infected images
are strongly dependent on the features for the normal classiﬁ-
cation task and thus indistinguishable from those of normal
images. Not only has this found to be completely achievable,
but we show that the attack can be done easily.
Targeted contamination attack. We observed that an in-
fected image’s representation becomes less dominated by
a trigger when the backdoor is source-speciﬁc: that is, only
images from a given class or several classes are misclassiﬁed
to the target label under the trigger. Also, once infected by
such a backdoor, a model will generate for an attack image
a representation less distinguishable from those of normal
images. Most importantly, this can be done in a straightfor-
ward way: in addition to poisoning training data with a set
of attack images – those from the source classes but merged
with the trigger and assigned with the target label , as a con-
ventional contamination attack does, we further add a set of
cover images, the images from other classes (called cover
labels) that are correctly labeled even if they are stamped
with the trigger. Our idea is to force the model to learn a
more complicated “misclassiﬁcation rule”: only when the
trigger appears together with the image content from desig-
1544    30th USENIX Security Symposium
USENIX Association
-15-10-505-20-100102030Normal 0Infected 3Infected 5Normal 3Normal 5020406080100-15-10-505101520Normal 0Infected 3Infected 5Normal 3Normal 5Table 1: Statistics of attacks using different number of source labels on GTSRB.
# of Source Labels
1
2
3
4
5
10
1
1
1
1
# of Attack Images (percentages of total)
200(0.5%)
400(1.0%)
600(1.5%)
800(2.0%)
1000(2.5%)
2000(4.9%)
400(1.0%)
600(1.5%)
1000(2.5%)
2000(4.9%)
Top-1 Accuracy
Global Misclassiﬁcation Rate
Targeted Misclassiﬁcation Rate
Trigger-only Misclassiﬁcation Rate
96.5%
54.6%
99.6%
98.7%
96.2%
69.6%
99.4%
100%
96.2%
74.9%
98.6%
100%
96.0%
78.2%
99.2%
100%
96.0%
83.1%
99.1%
100%
96.6%
95.8%
99.4%
100%
96.2%
56.7%
99.4%
99.1%
96.4%
59.9%
99.7%
99.8%
96.3%
63.2%
99.7%
100%
96.7%
67.1%
99.6%
100%
nated classes, will the model assign the image to the target
label; for those from other classes, however, the trigger will
not cause misclassiﬁcation.
It turns out that a relatively small fraction of contaminated
images is sufﬁcient to introduce such a source-speciﬁc back-
door to a model. As we can see from Table 2, when only
2.1% of the training data are contaminated, including 0.1%
by covering images and 2% by attack images (mislabeled
trigger-carrying images from the source class), the infected
model assigns 97% of the attack images from the source class
to the target label, while only 12.1% of trigger-carrying im-
ages from other classes are misclassiﬁed.
Figure 2: Target class’ representations projected onto their ﬁrst two principle
components. Left ﬁgure shows results of poisoning attack (without cover
images). Right ﬁgure shows results of TaCT (with cover images).
Using the source-speciﬁc backdoor, a trigger only works
when it is applied to some images, those from a speciﬁc source
class. Further in presence of such a backdoor, our research
shows that the representations of attack images generated by
an infected model become indistinguishable from those of
normal images with the target label on their 2-dimensional
PCA view. Fig. 2 illustrates the representations of the samples
classiﬁed as the target, based upon their two principal compo-
nents. On the left are those produced by a model infected with
a source-agnostic backdoor, and on the right are those gener-
ated by a source-speciﬁc model. As shown in the ﬁgure, the
representations of normal and infected images are separated
in the former, while mingle together under the source-speciﬁc
attack. Note that TaCT only needs to contaminate the training
set with a similar number of images as the prior attacks [10],
indicating that the attack could be as easy as the prior ones.
3.2 Limitations of Existing Solutions
Below we elaborate our analysis of four existing detection
approaches, including Neural Cleanse (NC) [42], STRIP [9],
SentiNet [8] and Activation Clustering (AC) [4]. Our research
shows that TaCT defeats all of them four. Without further spec-
iﬁcation, we tested these four defenses on GTSRB dataset,
and the TaCTs we launched here inject 800 (2%) attack im-
ages from one source class and 400 (1%) cover images from
two cover classes. For testing NC and AC, we launched multi-
round experiments running through all 43 classes of GTSRB,
each round setting one of them as the target class. For each
target class, 32 different triggers were utilized (4 triggers of
Fig.9 each located on one of eight randomly selected posi-
tions). Thus totally 43x32=1376 infected models were gen-
erated. For testing STRIP and SentiNet, 4000 testing images
were selected. The half of them are benign and the rest are
trigger-carrying. The results are summarized in Table 4
Neural Cleanse. NC [42] attempts to ﬁnd source-agnostic
triggers by searching for patterns that cause any image to be
classiﬁed by the model as a target label. From the patterns
discovered for each label (when treating it as the target), NC
identiﬁes the one with an anomalously small L1 norm as
a trigger, based upon the intuition that a stealthy trigger is
supposed to be small. This approach is designed to ﬁnd source-
agnostic triggers, which are characterized by their dominant
inﬂuence on a sample’s representation, as described above.
It is not effective on source-speciﬁc triggers, since images
carrying the triggers may or may not be classiﬁed to the target
label, depending on which class the original image is from.
More speciﬁcally, under a model infected by a source-
speciﬁc backdoor, an image’s representation is no longer
determined by the trigger of the backdoor: the representa-
tions of the images from different classes are different even
when they carry the same trigger. As a result, such a trigger
will not be captured by NC, since the approach relies on the
dominance property to ﬁnd a potential trigger.
In our research, we used the original code of Neural
Cleanse3 to test its performance in defending against TaCT.
Speciﬁcally, Table 3 shows the confusion matrix of NC for
defending against TaCT on GTSRB, with its threshold set to
2, as reported in their work. We found that the precision of
NC is only 2.8% (89/3185) and its recall is 6.5% (89/1376).
Fig. 3 further elaborates the part of the experimental results,
when the source label 0 and the target label ranges from 1 to
19: as we can see from the ﬁgure, the target label becomes
indistinguishable from the normal labels in terms of L1-norm,
rendering the anomaly index of NC ineffective. We also con-
ducted another experiment to demonstrate that the trigger
with higher global misclassiﬁcation rate will be more easily
detacted by NC. The details are described in the Appendix A.
STRIP. STRIP [9] detects a backdoor attack by checking
whether superimposing the input image over a set of randomly
selected images makes those new image’s class label harder
3https://github.com/bolunwang/backdoor
USENIX Association
30th USENIX Security Symposium    1545
-40-20020-20-100102030NormalInfected-20-1001020-10-505101520NormalInfectedTable 2: Effectiveness of TaCT with a single source label and different cover labels over GTSRB.
% of Cover Images
0.1%
2%
0.2%
2%
0.3%
2%
0.4%
2%
0.5%
2%
0.6%
2%
0.7%
2%
0.8%
2%
% of Mislabelled (attack) Images
Top-1 Accuracy
Misclassiﬁcation Rate (outside the source class)
Targeted Misclassiﬁcation Rate
96.1% 96.0% 96.6% 96.3% 96.8% 96.6% 96.6% 96.7% 96.9% 96.5%
12.1% 8.5%
4.7%
97.0% 96.9% 97.5% 98.0% 96.3% 97.0% 97.5% 97.2% 97.5% 98.0%
7.6%
6.0%
4.7%
4.7%
5.7%
4.8%
1%
2%
0.9%
2%
4.8%
Table 3: Confusion matrix of NC against TaCT on GTSRB.
Target label Normal label
Anomaly index > 2
Anomaly index <= 2
89
1287
3096
54696
(a) GTSRB
(b) CIFAR-10
Figure 4: Entropy distributions of STRIP against TaCT.
input increases only when a large number of images from the
source of the TaCT attack is chosen to evaluate the input (from
the same source and with a trigger), which becomes less likely
when the number of classes goes up. Fig. 4 shows the results
of STRIP on CIFAR10 and GTSRB: the entropy distribution
of attack-normal images is relatively more distinguishable
from that of the normal-normal images on CIFAR-10 than on
GTSRB, as the former has only 10 classes, while the latter
has 43. To investigate this problem, we modiﬁed STRIP in
our experiment to test an input image on the source class of
TaCT (giving advantages to STRIP): that is, superimposing
the input image on benign images just from the source class of
TaCT to determine the predictability of the input. The results
are presented in Table 4, Column S. As we can see here, even
though this enhancement indeed improves the effectiveness
of STRIP, it still incurs signiﬁcant false positives (54.2% with
95% TPR on GTSRB), due to the interference of two images
being combined that destroys some features associated with
the source class.
SentiNet. SentiNet [8] takes a different path to detect infected
images. For each image, SentiNet extracts the “classiﬁcation-
matter” component. This component is then pasted onto nor-
mal images (hold-on set), whose classiﬁcation results are
utilized to identify trigger-carrying images, since the trigger
will cause different images to be mis-assigned with the target
label. Under TaCT, however, a source-speciﬁc trigger is no
longer dominant and may not induce misclassiﬁcation. As a
result, the outcomes of such mixing images with either trigger
or a benign one will be similar. This thwarts the attempt to
detect the trigger based upon the outcomes.
We evaluated SentiNet on GTSRB dataset using an ap-
proach to the defender’s advantage: we assume that he has
correctly identiﬁed the trigger on an image and used the pat-
tern as the classiﬁcation-matter component, which becomes
the center of an image when it does not carry the trigger, since
most images in GTSRB have placed trafﬁc sign right in the
middle of a picture.
Figure 3: Detailed results of NC against TaCT, when 0 is the source label
and the target label ranges from 1 to 19. The box on the top ﬁgure shows
the quartiles of L1-norms for normal labels. The bottom ﬁgure shows the
anomaly index of the target labels.
to predict (with a high entropy): if so, the input is considered
to be normal and otherwise, it could carry a trigger. What has
been assumed here is the dominant impact of the trigger on
an image’s representation: i.e., even a random image can still
be classiﬁed to the target label when it contains the trigger.
For a source-speciﬁc backdoor, however, the impact of the
trigger is no longer dominant, as a trigger-carrying input’s
representation is also dependent on the features of its source
label (the genuine label of the input). Since superimposing
mixes the features of two images, the trigger therefore looses
the connection between the source label and further fade
the effectiveness to mislead the classiﬁcation, rendering the
detection less effective.
In our research, we evaluated the effectiveness of STRIP
against TaCT on GTSRB. Speciﬁcally, we used the TaCT
infected models to generate logits for two types of images:
those superimposing trigger-carrying images onto normal
ones, and those superimposing normal images onto normal
ones. Fig. 4a compares the distributions of the entropy of
these images’ logits. As we can see here, under TaCT, those
in the attack-normal superimposing group cannot be clearly
distinguished from the images in the normal-normal group,
due to the overlapping area between those two distributions.
The authors of STRIP discuss the potential of STRIP to de-
tect source-speciﬁc attacks [9], whose effectiveness, however,
is related to the number of classes a task has: since STRIP
randomly selects a ﬁxed number of images across all classes
to superimpose an input, the chance of detecting an attack
1546    30th USENIX Security Symposium
USENIX Association
12345678910111213141516171819406080100120L1 normTarget1234567891011121314151617181900.511.522.5Anomaly index00.20.40.60.8Entropy051015PercentageAtt over NorNor over Nor00.511.5Entropy051015PercentageAtt over NorNor over NorFollowing SentiNet, in Fig. 5, we represent every image as
a point in a two-dimensional space. Here the y-axis describes
“fooled count”, Fooled, i.e., the ratio of misclassiﬁcations
caused by the classiﬁcation-matter component across all im-
ages tested. The x-axis is the average conﬁdence AvgCon f
of the decision for the image pasted on an inert component
(an noise image) in the same area of the classiﬁcation-matter
component (Please see the original paper [8]).