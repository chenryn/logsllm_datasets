the current state when exactly the ﬁrst u system calls of S1 and the ﬁrst v
system calls of S2 have been emitted. Since qi can be visited at most once for
a ﬁxed u and v, X u,v
) =
(cid:2)
can take on only values 0 and 1. As such, E(X u,v
x∈{0,1} xPr(X u,v
i = x) = γ(u, v, i). Then, by linearity of expectation,
i
i
i
E(Xi) =
l1(cid:3)
l2(cid:3)
u=0
v=0
E(X u,v
i
) =
l1(cid:3)
l2(cid:3)
u=0
v=0
γ(u, v, i)
Behavioral Distance Measurement Using Hidden Markov Models
29
where l1 and l2 are the lengths of S1 and S2, respectively. Similarly, if Xi,j is
the number of transitions from qi to qj when generating [S1, S2], then
E(Xi,j) =
l1(cid:3)
l2(cid:3)
u=0
v=0
ξ(u, v, i, j)
With these expectations calculated, we can update the ai parameters of the
HMM λ, using the Baum-Welch method [3], as follows:
ai,j ← E(Xi,j)/E(Xi)
These equations show how the ai parameters of λ can be updated to increase
the probability of generating one pair of sequences. When there are more than
one pair of sequences ([S
]), the above equations can
be used to calculate the relevant parameters for each pair of sequences (i.e.,
E(X
(k)
i,j )) and then the ai parameters of λ can be updated as
), E(X
(M)
2
(k)
i
(1)
1 , S
(1)
2 ], . . ., [S
(M)
1
, S
ai,j ←
(cid:17)
M(cid:3)
k=1
(cid:18)
wk E(X
(k)
i,j )
/
(cid:17)
M(cid:3)
k=1
(cid:18)
wk E(X
(k)
i
)
(k)
where wk is the weight for each pair of sequences [S
2 ] in the training set
for the current instance of λ. There are many ways of setting wk [12]. In our
experience, diﬀerent settings aﬀect the speed of convergence, but the ﬁnal result
of the HMM is almost the same. In our experiments, we choose
(k)
1 , S
(cid:7)
wk =
Prλ([S
(k)
1 , S
(cid:8)−
(k)
2 ])
1
(k)
1 +l
l
(k)
2
are the lengths of S
(k)
1
and S
(k)
2 , respectively.
where l
(k)
1
and l
(k)
2
The equations above show how the parameters of an HMM can be adjusted
in one reﬁnement. We need many such reﬁnements in order to ﬁnd a good HMM
that generates the training examples with high probabilities. Although more
reﬁnements can improve the probabilities, they may also result in overﬁtting.
To detect when to stop the reﬁnement process so as not to overﬁt the training
samples, we use a separate validation set, which also contains pairs of system
call sequences recorded from the two processes when processing the same in-
puts. Brieﬂy, we detect overﬁtting when the reﬁnement process either decreases
Prλ([S1, S2]) for pairs [S1, S2] in the validation set or increases the false-alarm
rate on the validation set using the alarm threshold needed to detect mimicry
attacks (explained in Section 5.1).
4.4 Implementation Issues
There are several implementation issues that deserve comment. First, in all dis-
cussion so far, we have used system calls as the basic units to explain the elements
30
D. Gao, M.K. Reiter, and D. Song
of the HMM and our algorithms; i.e., an observable symbol of the HMM is a
pair of system calls, one from each process. However, it is advantageous to use
system call phrases (short sequences of system calls) as the basic unit [35,17,18].
In our experiments, we use the same phrase-extraction algorithm as in the ED
project [18]. After the system call phrases are identiﬁed, an observable symbol of
the HMM becomes a pair of system call phrases, one from each process. Other
than this, all algorithms presented in this paper remain the same.
Second, the number N of states in the HMM must be set before training
starts. (N does not change once it is set.) A small N will make the HMM not as
powerful as required to model the behavior of the processes, which will, in turn,
make mimicry attacks relatively easy. However, a large N not only degrades
the performance of the system, but may also result in overﬁtting the training
data. We have found success in setting N slightly larger than the length of the
longest training sequence so that some dummy symbols σ can be inserted into
the sequences, and to use the validation set to detect overﬁtting. So far we have
found that setting N to be 1.0 to 1.2 times the length of the longest training
sequence (in phrases) is a reasonable guideline. In our experiments described in
Section 5 using three diﬀerent web servers on two diﬀerent operating systems,
this guideline yielded values of N between 10 and 33.
Third, the training of the HMM is a complicated process, which may take a
long time. In our experiments, the training for a typical web server application
may take more than an hour on a desktop computer with a Pentium IV 3.0 GHz
CPU. However, training can be performed oﬄine, and the online monitoring is
fast, as in many other applications of HMMs.
A fourth issue concerns the use of a ﬁnite set of training samples for estimating
the HMM parameters. If we look at the formulas for building the HMM in
Section 4.3, we see that certain parameters will be set to 0 if there are no or few
occurrences of a symbol in the training set. For example, if an observable symbol
does not occur often enough, then the probability of that symbol being emitted
will be 0 in some states. This should be avoided because no occurrences in the
training data might be the result only of a low, but still nonzero, probability of
that event. Therefore, in our implementation we ensure a (nonzero) minimum
value to the ai and bi parameters by adding a normalization step at the end of
each reﬁnement process.
5 Evaluation and Discussion
As discussed in Section 4, we hypothesized that because the HMM-based ap-
proach we advocate here better accounts for the order of system calls, it should
better defend against mimicry attacks than the prior ED-based approach [18].
In this section, we evaluate an implementation of our anomaly detector using
HMM-based behavioral distance to determine whether this is, in fact, true, and
to gain insight into the computational cost of our approach.
Our evaluation system includes two computers running web servers to pro-
cess client HTTP requests. One of these computers, denoted L, runs Linux
kernel 2.6.8, and the other, denoted W, runs Windows XP Pro SP2. The web
Behavioral Distance Measurement Using Hidden Markov Models
31
server run by each computer diﬀers from test to test, and will be discussed below.
In our tests, each of L and W was given the same sequence of requests (gener-
ated from the static test suite of WebBench 5.0,5 and each recorded the system
call sequence, denoted by SL and SW,6 respectively, of (the thread in) the web
server process that handled the request. The behavioral distance is calculated
as Prλ([SL, SW]), where λ was trained as described in Section 4.3.
5.1 Resilience Against Mimicry Attacks
Our chosen measure of the system’s resilience to mimicry attacks is the false-
alarm rate of the system when it is conﬁgured to detect the “best” mimicry
attack. Intuitively, a system that oﬀers a low false-alarm rate while detecting
the best mimicry attack is doing a good job of discriminating “normal” behavior
from even the “best-disguised” abnormal behavior. To compare our results to
the ED-based behavioral distance project [18], we presume the same system call
sequence that the attacker is trying to execute as in the ED project, which is
simply an open followed by a write.
To measure the false-alarm rate when detecting the best mimicry, we need to
ﬁrst deﬁne what we take as the “best” mimicry attack. Speciﬁcally, if we presume
that the attacker ﬁnds a vulnerability in, say, L, then it must craft an attack
request that will produce a “normal” behavioral distance between the attack
activity on L induced by its request (SL) and the normal activity on W induced
by the same request (SW). Moreover, the attack activity on L must include an
open followed by a write (i.e., the attacker’s system calls). As such, it would
be natural to deﬁne the “best” mimicry attack to be the one that yields the
most normal behavioral distance, i.e., that maximizes Prλ([SL, SW]). Because
we permit the attacker to have complete knowledge of our HMM λ, nothing is
hidden from the attacker to prevent his use of this “best” mimicry attack.
(cid:4)
L, S
Unfortunately, we know of no eﬃcient algorithm for ﬁnding this best mimicry
attack (an obstacle an attacker would also face), and so we have to instead
evaluate our system using an “estimated-best” mimicry attack that we can ﬁnd
eﬃciently. Rather than maximizing Prλ([SL, SW]), this estimated-best mimicry
attack is the one produced by the most probable execution of the HMM λ that
includes the attacker’s system calls on the platform we presume he can compro-
mise. (The most probable execution does not necessarily yield the mimicry attack
that maximizes Prλ([SL, SW]), since many low-probability executions can yield a
(cid:4)
diﬀerent [S
W]).) An algorithm for computing
this estimated-best mimicry attack can be found in Appendix B. Another way in
which our attack is “estimated-best” is that it assumes the attacker executes its
attack within the servers’ processing of a single request (an assumption made in
the ED project [18] as well). Attacks for which the attack activity spans multiple
requests or multiple server processes/threads is an area of ongoing work.
5 VeriTest, http://www.veritest.com/benchmarks/webbench/default.asp
6 System calls on Windows are also called native API calls or kernel calls. We obtain
the Windows system call information by overwriting the KiSystemService table in
the Windows kernel using a kernel driver we developed.
(cid:4)
W] that has a larger Prλ([S
(cid:4)
L, S
32
D. Gao, M.K. Reiter, and D. Song
Once this estimated-best mimicry attack is found, we set the behavioral dis-
tance alarm threshold to be the behavioral distance resulting from this estimated-
best mimicry, and measure the false-alarm rate of the system that results. A false
alarm corresponds to a legitimate request that induces a pair of system call se-
quences with a probability of emission from λ at most the threshold. The false-
alarm rate is then calculated as the number of false alarms divided by the total
number of requests. We perform our experiments in nine diﬀerent settings, deﬁned
by the web servers that L and W are running. (The web servers are Apache 2.0.54,
Abyss X1 2.0.6 and MyServer 0.8.) Table 1 presents results using a testing mecha-
nism in which the training (to train the model), validation (to detect overﬁtting)
and evaluation (to evaluate) sets are distinct. They show that the HMM-based
behavioral distance has a small (and in many cases, greatly superior to ED) false-
alarm rate when detecting the estimated-best mimicry attacks.
Table 1. False-alarm rate when detecting the estimated-best mimicry attack
Server
on L
Server
on W Mimicry on L Mimicry on W Mimicry on L Mimicry on W
HMM-based
ED-based
Apache Apache
Abyss
Abyss
MyServer MyServer
Apache
Abyss
Abyss
Apache
Apache MyServer
MyServer Apache
Abyss MyServer
MyServer Abyss
2.08 %
0.4 %
1.36 %
0.4 %
0.8 %
0 %
6.4 %
0 %
0.4 %
0.16 %
0.32 %
1.2 %
0.32 %
0.48 %
3.65 %
0.16 %
1.91 %
0.08 %
0 %
0.16 %
0 %
0 %
0.08 %
0 %
0 %
0 %
0.4 %
0.16 %
0.08 %
0 %
0.16 %
0.08 %
0 %
0 %
1.44 %
0 %
5.2 Performance Overhead
To evaluate the performance overhead of a system using our HMM-based be-
havioral distance, we run two experiments. First, we measure the time it takes
to calculate the behavioral distance, and compare that with the ED-based ap-
proach. Second, we apply the HMM-based behavioral distance on real servers
and evaluate its performance overhead.
In the ﬁrst experiment, we measure the time it takes for our implementations
of the behavioral distance measurement (both the ED-based and the HMM-
based) to calculate the behavioral distance of 1200 pairs of system call sequences
on a Pentium IV 2.0GHz computer with 512MB of memory. In 10 runs of the
experiment, the HMM-based calculation takes 2.269 seconds on average, and the
ED-based calculation takes 2.422 seconds on average. As such, our HMM-based
calculation is 6.32% faster than the ED-based calculation.
In the second experiment, we augment the setup containing L and W with two
additional machines, a proxy P and a client C, and connect them in a 100 Mbps
local area network. Table 2 summarizes the properties of L, W, P, and C. The
client C submits requests to the proxy P, which forwards the requests to both
L and W for processing. Responses from L and W are sent to P, which then
sends a response to C. C uses the benchmark program WebBench 5.0 to issue
Behavioral Distance Measurement Using Hidden Markov Models