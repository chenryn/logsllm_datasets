number, and will be placed in the same queue throughout
the entire datacenter fabric.
DRL formulation
Action Space: The action provided by the agent is a mapping
from active ows to priorities: for each active ow f , at time
step t, its priority is pt ( f )∈[1,K].
State space: The big-switch assumption allows for a much
simplied state space. As routing and load balancing are out
of our concern, the state space only includes the ow states.
In our model, states are represented as the set of all active
ows, F t
, in the entire
network at current time step t. Each ow is identied by
its 5-tuple [8, 38]: source/destination IP, source/destination
port numbers, and transport protocol. Active ows have an
a, and the set of all nished ows, F t
d
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
L. Chen et al.
additional attribute, which is its priority; while nished ows
have two additional attributes: FCT and ow size5.
Rewards: Rewards are feedback to the agent on how good its
actions are. The reward can be obtained after the completion
of a ow, thus is computed only on the set of nished ows
for time step t. The average throughput of each nished
F t
d
ow f is Tputf = Sizef
. We model the reward as the ratio
FCTf
between the average throughputs of two consecutive time
steps.
(cid:80)
(cid:80)
rt =
f t ∈F t
f t−1∈F t−1
d
d
Tputt
f
Tputt−1
f
(3)
It signals if the previous actions have resulted in a higher
per-ow throughput experienced by the agent, or it has de-
graded the overall performance. The objective is to maximize
the average throughput of the network as a whole.
DRL algorithm We use the update rule specied by Equa-
tion (2). The DNN residing on the agent computes probability
vectors for each new state and updates its parameters by eval-
uating the action that resulted in the current state. The eval-
uation step compares the previous average throughput with
the corresponding value of the current step. Based on the
comparison, an appropriate reward (either negative or posi-
tive) is produced which is added to the baseline value. Thus,
we can ensure that the function approximator improves with
time and can converge to a local minimum by updating DNN
weights in the direction of the gradient. The update which
follows (2) ensures that poor ow scheduling decisions are
discouraged for similar states in the future, and the good
ones become more probable for similar states in the future.
When the system converges, the policy achieves a sucient
ow scheduling mechanism for a cluster of servers.
2.3 Problem Identied
Using the DRL problem of ow scheduling as an example,
we implement PG using popular machine learning frame-
works: Keras/TensorFlow, PyTorch, and Ray. We simplify the
DRL agent to have only 1 hidden layer. We use two servers:
DRL agent resides in one, and the other sends mock trac
information (states) to the agent using an RPC interface. We
set the sending rate of the mock server to 1000 ows per
second (fps). We measure the processing latency of dierent
implementations at the mock server: the time between nish
sending the ow information and receiving the action. The
servers are Huawei Tecal RH1288 V2 servers running 64-bit
Debian 8.7, with 4-core Intel E5-1410 2.8GHz CPU, NVIDIA
K40 GPU, and Broadcom 1Gbps NICs.
5Flow size and FCT can be measured when the ow ends using either OS
utility[44] or application layer mechanisms[49, 61].
Figure 2: Current DRL systems are insucient.
As shown in Figure2, even for small ow arrival rate of
1000fps and only 1 hidden layer, the processing delays of
all implementations are more than 60ms, during which time
any ow within 7.5MB would have nished on a 1Gbps link.
For reference, using the well-known trac traces of a web
search application and a data mining application collected in
Microsoft datacenters[3, 8, 26], a 7.5MB ow is larger than
99.99% and 95.13% of all ows, respectively. This means,
most of the DRL actions are useless, as the corresponding
ows are already gone when the actions arrive.
Summary Current DRL systems’ performance is not enough
to make online decisions for datacenter-scale trac. They
suer from long processing delays even for simple algorithms
and low trac load.
3 AUTO DESIGN
3.1 Overview
The key problem of current DRL systems is the long latency
between collection of ow information and generation of
actions. In modern datacenters with ≥10Gbps link speed,
to achieve ow-level TO operations, the round-trip latency
of actions should be at least sub-millisecond. Without in-
troducing specialized hardware, this is unachievable (§2.2).
Using commodity hardware, the processing latency of DRL
algorithm is a hard limit. Given this constraint, how to scale
DRL for datacenter TO?
Recent studies [3, 11, 33] have shown that most datacenter
ows are short ows, yet most trac bytes are from long
ows. Informed by such long-tail distribution, our insight
is to delegate most short ow operations to the end-host,
and formulate DRL algorithms to generate long-term (sub-
second) TO decisions for long ows.
We design AuTO as a two-level system, mimicking the Pe-
ripheral and Central Nervous Systems in animals. As shown
in Figure 3, Peripheral Systems (PS) run on all end-hosts,
collect ow information, and make TO decisions locally with
minimal delay for short ows. Central System (CS) makes in-
dividual TO decisions for long ows that can tolerate longer
processing delays. Furthermore, PS’s decisions are informed
by the CS where global trac information are aggregated
and processed.
AuTO: Scaling Deep Reinforcement Learning
for Datacenter-Scale Automatic Traic Optimization
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Figure 3: AuTO overview.
Figure 4: Multi-Level Feedback Queuing.
3.2 Peripheral System
The key to AuTO’s scalability is to enable PS to make glob-
ally informed TO decisions on short ows with only local
information. PS has two modules: an enforce module and a
monitoring module.
Enforcement module To achieve the above goal, we adopt
Multi-Level Feedback Queueing (MLFQ, introduced in PIAS [8])
to schedule ows without centralized per-ow control. Specif-
ically, PS performs packet tagging in the DSCP eld of IP
packets at each end-host as shown in Figure 4. There are K
priorities, Pi ,1≤i≤K, and (K−1) demotion thresholds, αj ,1≤
j≤K−1. We congure all the switches to perform strict pri-
ority queueing based on the DSCP eld. At the end host,
when a new ow is initialized, its packets are tagged with
P1, giving them the highest priority in the network. As more
bytes are sent, the packets of this ow will be tagged with
decreasing priorities Pj (2≤j≤K), thus they are scheduled
with decreasing priorities in the network. The threshold to
demote priority from Pj−1 to Pj is αj−1.
With MLFQ, PS has the following properties:
• It can make instant per-ow decisions based only on local
information: bytes-sent and thresholds.
Figure 5: AuTO: A 4-queue example.
• It can adapt to global trac variations. To be scalable, CS
must not directly control small ows. Instead, CS opti-
mizes and sets MLFQ thresholds with global information
over a longer period of time. Thus, thresholds in PS can be
updated to adapt to trac variations. In contrast, PIAS [8]
requires weeks of trac traces to update the thresholds.
• It naturally separates short and long ows. As shown in
Figure 5, short ows nished in the rst few queues, and
long ows drop to the last queue. Thus, CS can centrally
process long ows individually to make decisions on rout-
ing, rate limit, and priority.
Monitoring module For CS to generate thresholds, the
monitoring module collects ow sizes and completion times
of all nished ows, so that CS can update ow size distri-
bution. The monitoring module also reports on-going long
ows that have descended into the lowest priority on its
end-host, so that CS can make individual decisions.
3.3 Central System
The CS is composed of two DRL agents (RLA): short ow RLA
(sRLA) is for optimizing thresholds for MLFQ, and long ow
RLA (lRLA) is for determining rates, routes, and priorities
for long ows. sRLA attempts to solve a FCT minimization
problem, and we develop a Deep Deterministic Policy Gra-
dient algorithm for this purpose. For lRLA, we use a PG
algorithm (§2.2) to generate actions for the long ows. In
the next section, we describe the two DRL problems and
solutions.
4 DRL FORMULATIONS AND SOLUTIONS
In this section, we describe the two DRL algorithms in CS.
4.1 Optimizing MLFQ thresholds
We consider a datacenter network connecting multiple servers.
Scheduling of ows is imposed by using K strict priority
queues at hosts and network switches (Figure 4) by setting
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
L. Chen et al.
the DCSP eld in each of the IP headers. The longer the
ow is, the lower priority is assigned to the ow as it is de-
moted through host priority queues in order to approximate
Shortest-Job-First (SJF). The packet’s priority is preserved
throughout the entire datacenter fabric till it reaches the
destination.
One of the challenges of MLFQ is the calculation of the
optimal demotion thresholds for the K priority queues at
the host. Prior works [8, 9, 14] provide mathematical anal-
ysis and models for optimizing the demotion thresholds:
{α1,α2,...,αK−1}. Bai et al. [9] also suggest weekly/monthly
re-computation of the thresholds with collected ow-level
traces. AuTO takes a step further and proposes a DRL ap-
proach to optimizing the values of the α’s. Unlike prior works
that used machine learning in datacenter problems [5, 36, 60],
AuTO is unique due to its target - optimization of real val-
ues in continuous action space. We formulate the threshold
optimization problem as an DRL problem and try to explore
the capabilities of DNN for modeling the complex datacenter
network for computing the MLFQ thresholds.
As shown in §2.2, PG is a basic DRL algorithm. The agent
follows a policy πθ (a|s) parameterized by a vector θ and
improves it with experience. However, REINFORCE and
other regular PG algorithms only consider stochastic policies,
πθ (a|s)=P[a|s;θ], that select action a in state s according to
the probability distribution over the action set A parame-
terized by θ. PG cannot be used for value optimization prob-
lem, as a value optimization problem computes real values.
Therefore, we apply a variant of Deterministic Policy Gradient
(DPG) [53] for approximating optimal values {a0,a1,...,an}
for the given state s such that ai =µθ (s) f or i=0,...,n. Figure
6 summarizes the major dierences between stochastic and
deterministic policies. DPG is an actor-critic [12] algorithm
for deterministic policies, which maintains a parameterized
actor function µθ for representing current policy and a critic
neural network Q (s,a) that is updated using the Bellman
equation (as in Q-learning [41]). We describe the algorithm
with Equation (4,5,6) as follows: The actor samples the en-
vironment and has its parameters θ updated according to
Equation (4). The result of Equation (4) follows from the fact
that the objective of the policy is to maximize the expected
cumulative discounted reward Equation(5) and its gradient
can be expressed in the following form Equation(5). For more
details, please refer to [53].
(cid:34)
∇θ µθ (s)∇aQ µk
θ k +1 ← θ k + αEs∼ρ µk
(s,a)
(cid:90)
whereρ µk is the state distribution at time k.
J (µθ )=
Sρ µ (s)r (s,µθ (s))ds
=Es∼ρ µ [r (s,µθ (s))]
(cid:35)
(cid:12)(cid:12)(cid:12)(cid:12)a=µθ (s )
(4)
(5)
Algorithm 1: DDPG Actor-Critic Update Step
1 Sample a random mini-batch of N transitions
(si ,ai ,ri ,si +1) from buer
2 Set yi =ri + γQ(cid:48)
θ Q(cid:48) (si +1,µ(cid:48)
(cid:80)N
3 Update critic by minimizing the loss:
i =1(yi−Qθ Q (si ,ai ))
θ µ(cid:48) (si +1))
2
L= 1
N
4 Update the actor policy using the sampled policy
gradient:
∇θ µ J≈ 1
N
N(cid:88)
i =1
5 Update the target networks:
∇θ µ (si )µθ Q (si )∇ai Qθ Q (si ,ai )
θ Q(cid:48) ← τθ Q + (1−τ )θ Q(cid:48)
θ µ(cid:48) ← τθ µ + (1−τ )θ µ(cid:48)
(cid:12)(cid:12)(cid:12)(cid:12)ai =µθ Q (si )
where γ and τ are small values for stable learning
(cid:90)
Sρ µ (s)∇θ µθ (s)∇aQ µk
∇θ J (µθ )=
=Es∼ρ µ [∇θ µθ (s)∇aQ µk
(cid:12)(cid:12)(cid:12)(cid:12)a=µθ (s )
(cid:12)(cid:12)(cid:12)(cid:12)a=µθ (s )
]
(s,a)
(s,a)
ds
(6)