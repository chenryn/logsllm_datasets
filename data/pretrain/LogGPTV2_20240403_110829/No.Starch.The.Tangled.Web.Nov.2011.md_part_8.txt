requested URL. In response, the server returns the requested file and, in the
most rudimentary use case, tears down the TCP connection immediately
thereafter.
The original HTTP/0.9 protocol provided no room for any additional
metadata to be exchanged between the participating parties. The client
request always consisted of a single line, starting with GET, followed by the
URL path and query string, and ending with a single CRLF newline (ASCII
characters 0x0D 0x0A; servers were also advised to accept a lone LF). A
sample HTTP/0.9 request might have looked like this:
GET /fuzzy_bunnies.txt
In response to this message, the server would have immediately returned
the appropriate HTML payload. (The specification required servers to wrap
lines of the returned document at 80 characters, but this advice wasn’t really
followed.)
The HTTP/0.9 approach has a number of substantial deficiencies. For
example, it offers no way for browsers to communicate users’ language pref-
erences, supply a list of supported document types, and so on. It also gives
servers no way to tell a client that the requested file could not be found, that
it has moved to a different location, or that the returned file is not an HTML
* Transmission Control Protocol (TCP) is one of the core communications protocols of the Internet,
providing the transport layer to any application protocols built on top of it. TCP offers reason-
ably reliable, peer-acknowledged, ordered, session-based connectivity between networked hosts.
In most cases, the protocol is also fairly resilient against blind packet spoofing attacks attempted
by other, nonlocal hosts on the Internet.
42 Chapter 3
document to begin with. Finally, the scheme is not kind to server admin-
istrators: When the transmitted URL information is limited to only the path
and query strings, it is impossible for a server to host multiple websites,
distinguished by their hostnames, under one IP address—and unlike DNS
records, IP addresses don’t come cheap.
In order to fix these shortcomings (and to make room for future
tweaks), HTTP/1.0 and HTTP/1.1 standards embrace a slightly different
conversation format: The first line of a request is modified to include proto-
col version information, and it is followed by zero or more name: value pairs
(also known as headers), each occupying a separate line. Common request
headers included in such requests are User-Agent (browser version informa-
tion), Host (URL hostname), Accept (supported MIME document types*),
Accept-Language (supported language codes), and Referer (a misspelled field
indicating the originating page for the request, if known).
These headers are terminated with a single empty line, which may be
followed by any payload the client wishes to pass to the server (the length of
which must be explicitly specified with an additional Content-Length header).
The contents of the payload are opaque from the perspective of the protocol
itself; in HTML, this location is commonly used for submitting form data in
one of several possible formats, though this is in no way a requirement.
Overall, a simple HTTP/1.1 request may look like this:
POST /fuzzy_bunnies/bunny_dispenser.php HTTP/1.1
Host: www.fuzzybunnies.com
User-Agent: Bunny-Browser/1.7
Content-Type: text/plain
Content-Length: 17
Referer: http://www.fuzzybunnies.com/main.html
I REQUEST A BUNNY
The server is expected to respond to this query by opening with a line
that specifies the supported protocol version, a numerical status code (used
to indicate error conditions and other special circumstances), and an optional,
human-readable status message. A set of self-explanatory headers comes next,
ending with an empty line. The response continues with the contents of the
requested resource:
HTTP/1.1 200 OK
Server: Bunny-Server/0.9.2
Content-Type: text/plain
Connection: close
BUNNY WISH HAS BEEN GRANTED
* MIME type (aka Internet media type) is a simple, two-component value identifying the class and
format of any given computer file. The concept originated in RFC 2045 and RFC 2046, where it
served as a way to describe email attachments. The registry of official values (such as text/plain or
audio/mpeg) is currently maintained by IANA, but ad hoc types are fairly common.
Hypertext Transfer Protocol 43
RFC 2616 also permits the response to be compressed in transit using
one of three supported methods (gzip, compress, deflate), unless the client
explicitly opts out by providing a suitable Accept-Encoding header.
The Consequences of Supporting HTTP/0.9
Despite the improvements made in HTTP/1.0 and HTTP/1.1, the unwelcome
legacy of the “dumb” HTTP/0.9 protocol lives on, even if it is normally hid-
den from view. The specification for HTTP/1.0 is partly to blame for this,
because it requested that all future HTTP clients and servers support the
original, half-baked draft. Specifically, section 3.1 says:
HTTP/1.0 clients must . . . understand any valid response in the
format of HTTP/0.9 or HTTP/1.0.
In later years, RFC 2616 attempted to backtrack on this requirement
(section 19.6: “It is beyond the scope of a protocol specification to mandate
compliance with previous versions.”), but acting on the earlier advice, all
modern browsers continue to support the legacy protocol as well.
To understand why this pattern is dangerous, recall that HTTP/0.9 serv-
ers reply with nothing but the requested file. There is no indication that the
responding party actually understands HTTP and wishes to serve an HTML
document. With this in mind, let’s analyze what happens if the browser sends
an HTTP/1.1 request to an unsuspecting SMTP service running on port 25
of example.com:
GET /Hi! HTTP/1.1
Host: example.com:25
...
Because the SMTP server doesn’t understand what is going on, it’s likely
to respond this way:
220 example.com ESMTP
500 5.5.1 Invalid command: "GET /Hi! HTTP/1.1"
500 5.1.1 Invalid command: "Host: example.com:25"
...
421 4.4.1 Timeout
All browsers willing to follow the RFC are compelled to accept these
messages as the body of a valid HTTP/0.9 response and assume that the
returned document is, indeed, HTML. These browsers will interpret the
quoted attacker-controlled snippet appearing in one of the error messages
asif it comes from the owners of a legitimate website at example.com. This
profoundly interferes with the browser security model discussed in Part II
ofthis book and, therefore, is pretty bad.
44 Chapter 3
Newline Handling Quirks
Setting aside the radical changes between HTTP/0.9 and HTTP/1.0, several
other core syntax tweaks were made later in the game. Perhaps most notably,
contrary to the letter of earlier iterations, HTTP/1.1 asks clients not only to
honor newlines in the CRLF and LF format but also to recognize a lone CR
character. Although this recommendation is disregarded by the two most
popular web servers (IIS and Apache), it is followed on the client side by all
browsers except Firefox.
The resulting inconsistency makes it easier for application developers
toforget that not only LF but also CR characters must be stripped from any
attacker-controlled values that appear anywhere in HTTP headers. To illus-
trate the problem, consider the following server response, where a user-
supplied and insufficiently sanitized value appears in one of the headers,
ashighlighted in bold:
HTTP/1.1 200 OK[CR][LF]
Set-Cookie: last_search_term=[CR][CR]Hi![CR][LF]
[CR][LF]
Action completed.
To Internet Explorer, this response may appear as:
HTTP/1.1 200 OK
Set-Cookie: last_search_term=
Hi!
Action completed.
In fact, the class of vulnerabilities related to HTTP header newline
smuggling—be it due to this inconsistency or just due to a failure to filter any
type of a newline—is common enough to have its own name: header injection
or response splitting.
Another little-known and potentially security-relevant tweak is support
for multiline headers, a change introduced in HTTP/1.1. According to the
standard, any header line that begins with a whitespace is treated as a contin-
uation of the previous one. For example:
X-Random-Comment: This is a very long string,
so why not wrap it neatly?
Multiline headers are recognized in client-issued requests by IIS and
Apache, but they are not supported by Internet Explorer, Safari, or Opera.
Therefore, any implementation that relies on or simply permits this syntax
inany attacker-influenced setting may be in trouble. Thankfully, this is rare.
Hypertext Transfer Protocol 45
Proxy Requests
Proxies are used by many organizations and Internet service providers to
intercept, inspect, and forward HTTP requests on behalf of their users. This
may be done to improve performance (by allowing certain server responses
to be cached on a nearby system), to enforce network usage policies (for
example, to prevent access to porn), or to offer monitored and authenti-
cated access to otherwise separated network environments.
Conventional HTTP proxies depend on explicit browser support: The
application needs to be configured to make a modified request to the proxy
system, instead of attempting to talk to the intended destination. To request
an HTTP resource through such a proxy, the browser will normally send a
request like this:
GET http://www.fuzzybunnies.com/ HTTP/1.1
User-Agent: Bunny-Browser/1.7
Host: www.fuzzybunnies.com
...
The key difference between the above example and the usual syntax is
the presence of a fully qualified URL in the first line of the request (http://
www.fuzzybunnies.com/), instructing the proxy where to connect to on behalf
of the user. This information is somewhat redundant, given that the Host
header already specifies the hostname; the only reason for this overlap is that
the mechanisms evolved independent of each other. To avoid being fooled
by co-conspiring clients and servers, proxies should either correct any mis-
matching Host headers to match the request URL or associate cached con-
tent with a particular URL-Host pair and not just one of these values.
Many HTTP proxies also allow browsers to request non-HTTP resources,
such as FTP files or directories. In these cases, the proxy will wrap the response
in HTTP, and perhaps convert it to HTML if appropriate, before returning it
to the user.* That said, if the proxy does not understand the requested proto-
col, or if it is simply inappropriate for it to peek into the exchanged data (for
example, inside encrypted sessions), a different approach must be used. A
special type of a request, CONNECT, is reserved for this purpose but is not
further explained in the HTTP/1.1 RFC. The relevant request syntax is instead
outlined in a separate, draft-only specification from 1998.5 It looks like this:
CONNECT www.fuzzybunnies.com:1234 HTTP/1.1
User-Agent: Bunny-Browser/1.7
...
* In this case, some HTTP headers supplied by the client may be used internally by the proxy,
but they will not be transmitted to the non-HTTP endpoint, which creates some interesting, if
non-security-relevant, protocol ambiguities.
46 Chapter 3
If the proxy is willing and able to connect to the requested destination,
itacknowledges this request with a specific HTTP response code, and the role
of this protocol ends. At that point, the browser will begin sending and receiv-
ing raw binary data within the established TCP stream; the proxy, in turn, is
expected to forward the traffic between the two endpoints indiscriminately.
NOTE Hilariously, due to a subtle omission in the draft spec, many browsers have incorrectly
processed the nonencrypted, proxy-originating error responses returned during an
attempt to establish an encrypted connection. The affected implementations interpreted
such plaintext responses as though they originated from the destination server over a
secure channel. This glitch effectively eliminated all assurances associated with the use
of encrypted communications on the Web. It took over a decade to spot and correct
theflaw.6
Several other classes of lower-level proxies do not use HTTP to com-
municate directly with the browser but nevertheless inspect the exchanged
HTTP messages to cache content or enforce certain rules. The canonical
example of this is a transparent proxy that silently intercepts traffic at the
TCP/IP level. The approach taken by transparent proxies is unusually dan-
gerous: Any such proxy can look at the destination IP and the Host header
sent in the intercepted connection, but it has no way of immediately telling
ifthat destination IP is genuinely associated with the specified server name.
Unless an additional lookup and correlation is performed, co-conspiring cli-
ents and servers can have a field day with this behavior. Without these addi-
tional checks, the attacker simply needs to connect to his or her home server
and send a misleading Host: www.google.com header to have the response
cached for all other users as though genuinely coming from www.google.com.
Resolution of Duplicate or Conflicting Headers
Despite being relatively verbose, RFC 2616 does a poor job of explaining how
a compliant parser should resolve potential ambiguities and conflicts in the
request or response data. Section 19.2 of this RFC (“Tolerant Applications”)
recommends relaxed and error-tolerant parsing of certain fields in “unam-
biguous” cases, but the meaning of the term itself is, shall we say, not particu-
larly unambiguous.
For example, because of a lack of specification-level advice, roughly half
of all browsers will favor the first occurrence of a particular HTTP header,
and the rest will favor the last one, ensuring that almost every header injec-
tion vulnerability, no matter how constrained, is exploitable for at least some
percentage of targeted users. On the server side, the situation is similarly ran-
dom: Apache will honor the first Host header seen, while IIS will completely
reject a request with multiple instances of this field.
Hypertext Transfer Protocol 47
On a related note, the relevant RFCs contain no explicit prohibition
onmixing potentially conflicting HTTP/1.0 and HTTP/1.1 headers and no
requirement for HTTP/1.0 servers or clients to ignore all HTTP/1.1 syntax.
Because of this design, it is difficult to predict the outcome of indirect con-
flicts between HTTP/1.0 and HTTP/1.1 directives that are responsible for
the same thing, such as Expires and Cache-Control.
Finally, in some rare cases, header conflict resolution is outlined in the
spec very clearly, but the purpose of permitting such conflicts to arise in the
first place is much harder to understand. For example, HTTP/1.1 clients are
required to send the Host header on all requests, but servers (not just prox-
ies!) are also required to recognize absolute URLs in the first line of the
request, as opposed to the traditional path- and query-only method. This
rulepermits a curiosity such as this:
GET http://www.fuzzybunnies.com/ HTTP/1.1
Host: www.bunnyoutlet.com
In this case, section 5.2 of RFC 2616 instructs clients to disregard the
nonfunctional (but still mandatory!) Host header, and many implementa-
tions follow this advice. The problem is that underlying applications are likely
to be unaware of this quirk and may instead make somewhat important deci-
sions based on the inspected header value.
NOTE When complaining about the omissions in the HTTP RFCs, it is important to recognize
that the alternatives can be just as problematic. In several scenarios outlined in that
RFC, the desire to explicitly mandate the handling of certain corner cases led to patently
absurd outcomes. One such example is the advice on parsing dates in certain HTTP
headers, at the request of section 3.3 in RFC 1945. The resulting implementation (the
prtime.c file in the Firefox codebase7) consists of close to 2,000 lines of extremely con-
fusing and unreadable C code just to decipher the specified date, time, and time zone in
asufficiently fault-tolerant way (for uses such as deciding cache content expiration).
Semicolon-Delimited Header Values
Several HTTP headers, such as Cache-Control or Content-Disposition, use a
semicolon-delimited syntax to cram several separate name=value pairs into a
single line. The reason for allowing this nested notation is unclear, but it is
probably driven by the belief that it will be a more efficient or a more intuitive
approach that using several separate headers that would always have to go
hand in hand.
Some use cases outlined in RFC 2616 permit quoted-string as the right-
hand parameter in such pairs. Quoted-string is a syntax in which a sequence of
arbitrary printable characters is surrounded by double quotes, which act as
delimiters. Naturally, the quote mark itself cannot appear inside the string,
but—importantly—a semicolon or a whitespace may, permitting many other-
wise problematic values to be sent as is.
48 Chapter 3
Unfortunately for developers, Internet Explorer does not cope with
thequoted-string syntax particularly well, effectively rendering this encoding
scheme useless. The browser will parse the following line (which is meant to
indicate that the response is a downloadable file rather than an inline docu-
ment) in an unexpected way:
Content-Disposition: attachment; filename="evil_file.exe;.txt"
In Microsoft’s implementation, the filename will be truncated at the
semicolon character and will appear to be evil_file.exe. This behavior creates a
potential hazard to any application that relies on examining or appending a
“safe” filename extension to an attacker-controlled filename and otherwise
correctly checks for the quote character and newlines in this string.
NOTE An additional quoted-pair mechanism is provided to allow quotes (and any other char-
acters) to be used safely in the string when prefixed by a backslash. This mechanism
appears to be specified incorrectly, however, and not supported by any major browser
except for Opera. For quoted-pair to work properly, stray “\” characters would need to
be banned from the quoted-string, which isn’t the case in RFC 2616. Quoted-pair
also permits any CHAR-type token to be quoted, including newlines, which is incom-
patible with other HTTP-parsing rules.
It is also worth noting that when duplicate semicolon-delimited fields are
found in a single HTTP header, their order of precedence is not defined by
the RFC. In the case of filename= in Content-Disposition, all mainstream browsers
use the first occurrence. But there is little consistency elsewhere. For example,
when extracting the URL= value from the Refresh header (used to force reload-
ing the page after a specified amount of time), Internet Explorer6 will fall
back to the last instance, yet all other browsers will prefer the first one. And
when handling Content-Type, Internet Explorer, Safari, and Opera will use the
first charset= value, while Firefox and Chrome will rely on the last.
NOTE Food for thought: A fascinating but largely non-security-related survey of dozens
ofinconsistencies associated with the handling of just a single HTTP header—