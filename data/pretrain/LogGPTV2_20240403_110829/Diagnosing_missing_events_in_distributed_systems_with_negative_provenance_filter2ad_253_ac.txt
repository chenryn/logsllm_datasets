for each (+τ,N,t,r,c) ∈ Log:
for each (−τ,N,t,r,c) ∈ Log:
S ← S ∪ { APPEAR(t,N,τ,r,c) }
S ← S ∪ { DISAPPEAR(t,N,τ,r,c) }
t1≤ t≤ t2
t1≤ t≤ t2
RETURN S
function QUERY(APPEAR(t,N,τ,r,c))
function QUERY(INSERT(t,N,τ))
if BaseTuple(τ) then
RETURN { INSERT(t,N,τ) }
else if LocalTuple(N,τ) then
RETURN { DERIVE(t,N,τ,r) }
else RETURN{RECEIVE(t,N ← r.N,τ)}
RETURN ∅
S ← ∅
for each τi: if (+τi,N,t,r,c) ∈ Log:
S ← S ∪ { APPEAR(t,N,τi,c) }
else
tx ← max t(cid:48) < t: (+τ,N,t(cid:48),r,1) ∈ Log
S ← S ∪ { EXIST([tx,t],N,τi,c) }
function QUERY(DERIVE(t,N,τ,τ:-τ1,τ2...))
function QUERY(RECEIVE(t,N1← N2,+τ))
ts ← max t(cid:48) < t: (+τ,N2,t(cid:48),r,1) ∈ Log
RETURN { SEND(ts,N1 → N2,+τ),
DELAY(ts,N2→ N1,+τ,t − ts) }
function QUERY(SEND(t,N → N(cid:48),+τ))
FIND (+τ,N,t,r,c) ∈ Log
RETURN { APPEAR(t,N,τ,r) }
function QUERY(NEXIST([t1,t2],N,τ))
if ∃t < t1 : (-τ,N,t,r,1) ∈ Log then
tx ← max t < t1: (-τ,N,t,r,1) ∈ Log
RETURN { DISAPPEAR(tx,N,τ),
NAPPEAR((tx,t2],N,τ) }
else RETURN { NAPPEAR([0,t2],N,τ) }
function QUERY(NDERIVE([t1,t2],N,τ,r))
S ← ∅
for (τi, Ii) ∈ PARTITION([t1,t2],N,τ,r)
S ← S ∪ { NEXIST(Ii,N,τi) }
function QUERY(NSEND([t1,t2],N,+τ))
RETURN S
if ∃t1 < t < t2 : (-τ,N,t,r,1) ∈ Log then
RETURN { EXIST([t1,t],N,τ),
NAPPEAR((t,t2],N,τ) }
else RETURN { NAPPEAR([t1,t2],N,τ) }
function QUERY(NAPPEAR([t1,t2],N,τ))
RETURN { NINSERT([t1,t2],N,τ) }
if BaseTuple(τ) then
else if LocalTuple(N,τ) then
RETURN(cid:83)
r∈Rules(N):Head(r)=τ
{ NDERIVE([t1,t2],N,τ,r) }
else RETURN {NRECEIVE([t1,t2],N,+τ)}
function QUERY(NRECEIVE([t1,t2],N,+τ))
S ← ∅, t0 ← t1 − ∆max
for each N(cid:48) ∈ SENDERS(τ,N):
X←{t0≤ t≤ t2|(+τ,N(cid:48),t,r,1)∈Log}
tx ← t0
for (i=0; i< |X|; i++)
S←S∪{NSEND((tx,Xi),N(cid:48),+τ),
NARRIVE((t1,t2),N(cid:48)→N,Xi,+τ)}
tx ← Xi
S ← S ∪ { NSEND([tx,t2],N(cid:48),+τ) }
function Q(NARRIVE([t1,t2],N1→N2,t0,+τ))
RETURN S
FIND (+τ,N2,t3,(N1,t0),1) ∈ Log
RETURN { SEND(t0,N1→ N2,+τ),
DELAY(t0,N1→ N2,+τ,t3 − t0) }
RETURN S
Figure 3: Graph construction algorithm. Some rules have been omitted; for instance, the handling of +τ and −τ messages is
analogous, and the rules for INSERT/DELETE, APPEAR/DISAPPEAR, and DERIVE/UNDERIVE are symmetric.
interval in ve can simply be some interval in which e was observed;
it does not need to cover the entire duration of e, and it does not
need to contain the root cause(s).
QUERY needs access to a log of the system’s execution to date.
We assume that the log is a sequence of tuples (±τ, N, t, r, c),
which indicate that τ was derived (+τ) or underived (−τ) on node
N at time t via rule r. Since some tuples can be derived in more
than one way, we include a derivation counter c, which is 1 when a
tuple ﬁrst appears, and is increased by one for each further deriva-
tion. For tuples that node N received from another node N(cid:48), we set
r = N(cid:48), and for base tuples, we set r = ⊥ and c = 1.
First,
Figure 3 shows part of the algorithm we use to construct pos-
itive and negative provenance. There are several points worth
noting.
the algorithm uses functions BaseTuple(τ) and
LocalTuple(N,τ) to decide whether a missing tuple τ is a base tuple
that was not inserted, a local tuple on node N that was not derived,
or a remote tuple that was not received. The necessary information
is a byproduct of the compilation of any NDlog program and is thus
easily obtained. Second, to account for propagation delays, the al-
gorithm uses a constant ∆max that denotes the maximum time a
message can spend in the network and still be accepted by the re-
cipient; this is used to narrow down the time interval during which
a missing message could have been sent. Third, the algorithm can
produce the same vertex more than once, or semantically identical
vertices with adjacent or overlapping intervals; in these cases, it is
necessary to coalesce the vertices using the union of their intervals
in order to preserve minimality. Finally, the algorithm uses two
functions PARTITION and SENDERS, which we explain next.
3.6 Explaining nonderivation
The PARTITION function encodes a heuristic for choosing among
several possible explanations of a missing derivation. When ex-
plaining why a rule with multiple preconditions did not derive a cer-
tain tuple, we must consider a potentially complex parameter space.
For instance, if A(@X,p):-B(@X,p,q,r),C(@X,p,q) did
not derive A(@X,10), we can explain this with the absence of
B(@X,10,q,r), C(@X,10,q,r), or a combination of both –
e.g., by dividing the possible q and r values between the two pre-
conditions. Different choices can result in explanations of dramati-
cally different sizes once the preconditions themselves have been
explained; hence, we would prefer a partition of the parameter
space (here, Q× R) that results in an explanation that is as small as
possible. In general, ﬁnding the optimal partition is at least as hard
as the SETCOVER problem, which is NP-hard; hence the need for
a heuristic. In our experiments, we use a simple greedy heuristic
that always picks the largest available subspace; if there are multi-
ple subspaces of the same size, it explores both for a few steps and
then picks the one with the simplest subgraph.
3.7 Missing messages
The SENDERS(±τ,N) function is used to narrow down the set of
nodes that could have sent a speciﬁc missing message ±τ to node
N. One valid choice is to simply return the set of all nodes in the
system that have a rule for deriving τ; however, the resulting prove-
nance can be complex, since it must explain why each of these
nodes did not send ±τ. Hence, it is useful to enhance SENDERS
with other information that may be available. For instance, in a
routing protocol, communication is restricted by the network topol-
ogy, and messages can come only from direct neighbors.
In some cases, further nodes can be ruled out based on the spe-
ciﬁc message that is missing: for instance, a BGP message whose
AS path starts with 7 should come from a router in AS 7. We do not
pursue this approach here, but we hypothesize that static analysis
of the NDlog program could be used for inferences of this type.
3.8 Formal properties
We now brieﬂy present
the key deﬁnitions from our formal
model [33]. An event d@n = (m, r, t, c, m(cid:48)) represents that rule
r was triggered by message m and generated a set of (local or re-
mote) messages m(cid:48) at time t, given the precondition c (a set of
tuples that existed on node n at time t). Speciﬁcally, we write
d@nrecv = (m@nsend,−, t, 1, m@nrecv) to denote a message m
(from nsend is delivered at nrecv at t). A trace E of a system
execution is an ordered sequence of events from an initial state
dx@nx−−−−→ Sx. We say a trace
S0, S0
E is valid, if (a) for all τk ∈ ci, τk ∈ Si−1, and (b) for all
d1@n1−−−−→ S1
d2@n2−−−−→ ...
5
di@ni = (mi, ri, ti, ci, m(cid:48)
i), either mi is a base message from
an external source, or there exists dj@nj = (mj, rj, tj, cj, m(cid:48)
j)
that precedes di@ni and mi ∈ m(cid:48)
j. We say that E(cid:48) is a subtrace of
E (written as E(cid:48) ⊆ E) if E(cid:48) consists of a subset of the events in E in
the same order. In particular, we write E|n to denote the subtrace
that consists of all the events on node n in E. We say that E(cid:48) and E
are equivalent (written as E(cid:48) ∼ E) if, for all n, E(cid:48)|n = E|n.
To properly deﬁne minimality, we use the concept of a reduc-
tion: given negative provenance G(e,E), if there exist vertices
v1, v2 ∈ V (G), where the time interval of v1 and v2 (t(v1) and
t(v2) respectively) are adjacent, and v1 and v2 have the same de-
pendencies, then G can be reduced to G(cid:48) by combining v1 and v2
into v ∈ V (G(cid:48)), where t(v) = t(v1) ∪ t(v2). Given two nega-
tive provenance G(e,E) and G(cid:48)(e,E), we say G(cid:48) is simpler than G
(written as G(cid:48) < G), if any of the following three hold: (1) G(cid:48) is
a subgraph of G; (2) G(cid:48) is reduced from G (by combining v1 and
v2); or (3) there exists G(cid:48)(cid:48), such that G(cid:48) < G(cid:48)(cid:48) and G(cid:48)(cid:48) < G.
Using these deﬁnitions, we can formally state the three proper-
ties from Section 3.2 as follows:
Property (Soundness): Negative provenance G(e,E) is sound iff
(a) it is possible to extract a valid subtrace Esub ⊆ E(cid:48), such that
E(cid:48) ∼ E and (b) for all vertices in G(e,E), their corresponding
predicates hold in E.
Property (Completeness): Negative provenance G(e,E) is com-
plete iff there exists no trace E(cid:48) such that a) E(cid:48) assumes the same
external inputs as G(e,E), and b) e exists in E(cid:48).
Property (Minimality): Negative provenance G(e,E) is minimal,
if no G(cid:48) < G is sound and complete.
We have proven that our provenance graph has all three properties.
The proofs are available in [33].
4. ENHANCING READABILITY
So far, we have explained how to generate a “raw” provenance
graph. This representation is correct and complete, but it is also
extremely detailed: for instance, simple and common events, such
as message exchanges between nodes, are represented with many
different vertices. This “clutter” can make the provenance difﬁcult
to read. Next, we describe a post-processing technique that can
often simplify the provenance considerably, by pruning unhelpful
branches, and by summarizing common patterns into higher-level
vertices.
4.1 Pruning unhelpful branches
Logical inconsistencies: Some explanations contain logical incon-
sistencies: for instance, the absence of a tuple τ1 with parameter
space S1 might be explained by the absence of a tuple τ2 with pa-
rameter space S2 ⊆ S1. If we can recognize such inconsistencies
early, there is no need to continue generating the provenance until
a set of base tuples is reached – the precondition is clearly unsat-
isﬁable. Thus, we can safely truncate the corresponding branch of
the provenance tree.
Failed assertions: Some branches explain the absence of events
that the programmer has already ruled out. For instance, if a branch
contains a vertex NEXIST([t1, t2],N,P(5)) and it is known that P can
only contain values between 0 and 4, the subtree below this vertex
is redundant and can be removed. We use programmer-speciﬁed as-
sertions (where available) to recognize situations of this type. The
assertions do not have to be provenance-speciﬁc – they can be the
ones that a good programmer would write anyway.
Branch coalescing: A naïve execution of the algorithm in Fig-
ure 3 would result in a provenance tree, but this tree would contain
many duplicate vertices because many events have more than one
effect. To avoid redundancy, we combine redundant vertices when-
ever possible, which turns the provenance tree into a DAG. If two
vertices have overlapping time intervals but are otherwise identical,
we use the union of the two intervals. (Note that a smart implemen-
tation of PARTITION could take the multiplicity of shared subtrees
into account.)
Application-speciﬁc invariants: Some explanations may be irre-
levant for the particular SDN that is being debugged. For instance,
certain data – such as constants, topology information, or state from
a conﬁguration ﬁle – changes rarely or never, so the absence of
changes, or the presence of a speciﬁc value, do not usually need to
be explained. One simple way to identify constant tables is by the
absence of derivation rules in which the table appears at the head.
Optionally, the programmer can use a special keyword to designate
additional tables as constant.
4.2 Different levels of detail
Another way to make negative provenance graphs more useful for