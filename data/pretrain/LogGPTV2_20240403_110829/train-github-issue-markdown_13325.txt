Attention models have gained significant popularity in recent years. In many of my experiments involving attention mechanisms, I often need to apply the Softmax function across multiple dimensions. However, the `tf.nn.softmax()` function in TensorFlow currently only supports applying Softmax along a single dimension.

It would be beneficial to enhance the API by adding an additional argument, such as `softmax_dim`, which would allow users to specify multiple dimensions over which to apply the Softmax function. This feature would be particularly useful for applications like image processing (2D) or CT scans and videos (3D).

I am curious to hear your thoughts on this. Do other researchers and developers frequently find themselves writing custom Softmax code to handle multi-dimensional cases?