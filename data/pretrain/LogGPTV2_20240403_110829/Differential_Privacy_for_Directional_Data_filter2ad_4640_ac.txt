𝜅, resulting in a very efficient method even in high dimensions.
More recently, Kurz and Hanebeck [27] proposed another sam-
pling algorithm for the VMF distribution that is best described as
approximate inversion method. It works by substituting 𝑡 = cos(𝜗)
in the tangent-normal decomposition as in Eq. (2) and constructing
Session 4D: Differential Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1209Algorithm 1: Approx. inversion method for PurArc(𝑛, 𝜅)
Input: dimension 𝑛, concentration parameter 𝜅, max. no. of
iterations 𝑀 ≥ 1, (optional: absolute tolerance 𝛿abs)
Output: A sample 𝜗 ∈ [0, 𝜋] of PurArc(𝑛, 𝜅)
1 𝑎 ← 0; 𝑏 ← 𝜋;
2 𝑢 (cid:123) Uni(0, 1);
3 for 𝑖 ← 1 to 𝑀 do
𝜗 ← (𝑎 + 𝑏)/2;
𝑦 ← 𝐹𝑛−2,−𝜅 (𝜗)
𝐹𝑛−2,−𝜅 (𝜋) ;
if |𝑦 − 𝑢|  𝑢 then 𝑏 ← 𝜗;
// evaluate PurArc[𝜃 ≤ 𝜗]
// (optional)
// adjust lower,
// or upper bound
// initial interval bounds
// uniform sample
4
5
6
7
8
9 end
10 return 𝜗
a sample x = cos(𝜗)𝝁 +sin(𝜗)𝝃 . This reduces the problem to gener-
ating 𝜗 (cid:123) VMFArc(𝑛, 𝜅) from the univariate angular distribution.
If the corresponding angular CDF VMFArc(𝑛, 𝜅)[𝜃 ≤ 𝜗] was in-
vertible analytically, a textbook version of the inversion method
(see, e.g., [8]) could be used to sample 𝜗. Kurz and Hanebeck solve
this by approximately inverting the CDF: If we can efficiently com-
pute the CDF 𝑢 = VMFArc(𝑛, 𝜅)[𝜃 ≤ 𝜗], we can approximate its
inverse 𝜗 = VMFArc(𝑛, 𝜅)−1[𝑢] numerically, e.g., by interval bisec-
tion, which is “guaranteed to converge up to machine precision” in
a reasonable number of steps [27]. Unfortunately, their solution for
VMFArc(𝑛, 𝜅)[𝜃 ≤ 𝜗] is analytical only for odd 𝑛, while it contains
an infinite series in terms of special functions for even 𝑛 which
we cannot evaluate efficiently. Therefore, this approach is only vi-
able for VMF when 𝑛 is odd, which is why we prefer the rejection
scheme from the previous paragraph as it is fast and simple to
use in general. However, we show next that this idea is useful for
sampling the Purkayastha distribution.
3.4.2 Purkayastha sampling method. To our best knowledge, there
is no published sampling method for the Purkayastha distribution.
Cutting et al. [7] state that they generated samples for lower dimen-
sions up to 𝑛 = 100, but without specifying the exact method they
used. Rather, they give the following explanation (emphasis ours):
The Purkayastha distribution is numerically hard to
generate for dimensions larger than 150, which is the
only reason why the dimensions considered in this
second simulation are smaller than in the first one.
Here, the “first” and “second simulation” refer to sampling from
the VMF and Purkayastha distribution, respectively.
Approximate inversion Purkayastha sampling algorithm. Recall
that we have derived a solution for the angular CDF of the Purka-
yastha distribution, PurArc(𝑛, 𝜅)[𝜃 ≤ 𝜗] = 𝐹𝑛−2,−𝜅(𝜗)/𝐹𝑛−2,−𝜅(𝜋),
in Corollary 23. While we are not aware of a way to directly compute
its inverse to apply the inversion method [8] for 𝑛 > 2, the solution
itself is a finite closed-form expression that can be computed analyt-
ically. We hence propose an approximate inversion method, similar
to the approach by Kurz and Hanebeck [27] for VMF in Section 3.4.1,
to obtain a new Purkayastha sampling algorithm: Since we can effi-
ciently compute the angular CDF 𝑢 = PurArc(𝑛, 𝜅)[𝜃 ≤ 𝜗], we can
approximate its inverse 𝜗 = PurArc(𝑛, 𝜅)−1[𝑢] numerically. We de-
scribe the core method to sample 𝜗 (cid:123) PurArc(𝑛, 𝜅) in Algorithm 1.
Once we have a sample 𝜗, we draw 𝝃 (cid:123) Uni(S𝑛−2 ⊥ 𝝁) and as
above use the tangent-normal decomposition Eq. (2) to construct
x = cos(𝜗)𝝁 + sin(𝜗)𝝃 ∼ Pur(𝝁, 𝜅).
Since our solution for the angular CDF is a closed-form expression
with finitely many terms in any number of dimensions 𝑛, we ar-
gue that our approximate inversion method for the Purkayastha
distribution is practical regardless of the parity of 𝑛.
Algorithm 1 can easily be vectorized to generate multiple samples
at once, or parallelized to utilize multiple CPU cores. In fact, we
benchmark our method in up to tens of thousands of dimensions
(cf. Section 4.1), pushing beyond the status quo [7] by providing an
efficient sampling algorithm in dimensions much larger than 150.
3.5 Choice of parameters based on privacy level
To actually run the proposed directional privacy mechanisms on
a given input vector 𝒙 ∈ S𝑛−1, we need to generate samples from
Pur(𝒙, 𝜅) or VMF(𝒙, 𝜅) where the mode is given by the input 𝒙
and the concentration parameter 𝜅 is defined through the privacy
parameter 𝜖. Having described sampling methods for both the VMF
mechanism (cf. Section 3.4.1) and a novel sampling scheme for the
Purkayastha mechanism (cf. Section 3.4.2), it remains to explain the
exact choice of 𝜅 based on 𝜖 and the desired notion of privacy.
Given a unit vector 𝒙 ∈ S𝑛−1, in order to achieve directional
privacy with privacy parameter 𝜖, i.e. 𝜖𝑑∡-privacy (Definition 13),
we simply need to set 𝜅 = 𝜖 and draw a sample z (cid:123) Pur(𝒙, 𝜖) or
z (cid:123) VMF(𝒙, 𝜖) as shown in Corollary 16 and 19, respectively.
Metric privacy (Definition 3) [4] and its variants can also be
interpreted as providing a privacy (or indistinguishability) level
ℓ = 𝜖𝑟 to any two points 𝒙, 𝒙′ within a protection radius (or angle)
𝑟 > 0, cf. [2]. In case of directional privacy (Definition 13), this is
achieved by sampling with 𝜅 = ℓ/𝑟. In other words, an (ℓ/𝑟)-private
mechanism achieves a privacy level ℓ within a protection radius 𝑟.
As special case, when 𝒙 = 𝑓 (𝐷) is the result of a (query) function
𝑓 : D → S𝑛−1, we achieve pure 𝜖-DP by setting the protection
radius 𝑟 := Δ to the (worst-case) sensitivity of 𝑓 , i.e., by sampling
with a concentration parameter 𝜅 = 𝜖/Δ as per Fact 14. Thus, direc-
tional privacy allows relaxing pure DP by specifying a protection
radius 𝑟 smaller than the sensitivity Δ.
3.6 Circular and spherical baselines
For comparison, we consider the following adaptions of established
standard privacy mechanisms to directional data. The first and
second mechanisms, Clipped and Wrapped Laplace, are suitable for
circular data (𝑛 = 2), whereas the third one, Polar Laplace, can be
regarded as variant of Wrapped Laplace for spherical data (𝑛 = 3).
3.6.1 Clipped Laplace. A straightforward application of the usual
Laplace mechanism [10] with post-processing achieves DP on the
circle by adding Laplace noise to a given angle, followed by clipping
the result to an interval covering one full circle, say [0, 2𝜋) or
[−𝜋, 𝜋). This method is simple, but clearly has drawbacks: For
small 𝜖, the major part of the probability mass will be outside the
Session 4D: Differential Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1210]
𝜃
[
c
r
A
P
1.0
0.8
0.6
0.4
0.2
0.0
𝜅
0.01
0.6
1.0
2.5
Dist. P
PurArc(2, 𝜅)
WLArc(𝜅)
0
1
2
𝜃
3
]
𝜃
[
c
r
A
P
1.0
0.8
0.6
0.4
0.2
0.0
𝜅
0.01
0.6
1.0
2.5
Dist. P
PurArc(3, 𝜅)
PolArc(𝜅)
1.5
1.0
0.5
P
∼
𝜃
,
]
𝜃
[
E
0
1
2
𝜃
3
10−2
Dist. P
PurArc(2, 𝜅)
WLArc(𝜅)
PurArc(3, 𝜅)
PolArc(𝜅)
10−1
100
𝜅
101
(a) Angular PDFs on S1 (circular case, 𝑛 = 2)
Figure 2: Comparison of angular densities and expectations of Purkayastha vs. Wrapped/Polar Laplace (solid vs. dashed lines).
(b) Angular PDFs on S2 (spherical case, 𝑛 = 3)
(c) Expected angles: larger for baselines
𝑒𝜅𝜃
(cid:18)
(cid:19)
clipping range, creating a bias towards the angle at its boundaries.
We therefore use it only in selected experiments.
3.6.2 Wrapped Laplace. Instead of clipping, we can add Laplace
noise to the original angle 𝛼 and wrap it around the circle by re-
ducing the result modulo 2𝜋. This results in a so-called (symmetric)
Wrapped Laplace (WL) distribution with mean 𝛼. With the usual
parametrization on the unit circle, the density of a WL distribution
with zero mean and concentration parameter 𝜅 ≥ 0 is (cf. [21])
𝑒−𝜅𝜃
1 − 𝑒−𝜅2𝜋 +
𝜅
2
,
(cid:18)
(15)
𝑒𝜅2𝜋 − 1
𝜃 ∈ [0, 2𝜋).
WL(𝜅)[𝜃] =
Angular density. In accordance with Corollary 9, the correspond-
ing angular density WLArc is the density of points with the same
angle from the mean, in any direction. That is, it identifies an angle
𝜃 ∈ [0, 𝜋) with its mirror image 2𝜋 − 𝜃. By symmetry, it is just
twice the density on the full circle:
𝑒−𝜅𝜃
1 − 𝑒−2𝜅𝜋 +
(16)
While the Purkayastha angular density PurArc(2, 𝜅)[𝜃] ∝ 𝑒−𝜅𝜃
on S1 only has a single term 𝑒−𝜅𝜃 , cf. Eq. (9), WLArc(𝜅) has an
additional second term 𝑒+𝜅𝜃 that increases with the angle 𝜃. The
wrapping hence smoothens the distribution by moving probability
mass away from the mode as illustrated in Fig. 2a. It hence provides
less accuracy than Purkayastha at the same privacy level, thus
motivating the need for specialized directional mechanisms.
WLArc(𝜅)[𝜃] = 𝜅
𝜃 ∈ [0, 𝜋).
𝑒2𝜅𝜋 − 1
(cid:19)
𝑒𝜅𝜃
,
Expected angular distance. Similarly to the derivation of the
expected surface distance for the Purkayastha distribution from
PurArc[𝜃], we can derive the expected angular distance for the WL
distribution from WLArc[𝜃]. The result is
1
𝜃∼WLArc(𝜅)[𝜃] =
For comparison, the expected angular distance of the circular Pur-
kayastha distribution from Lemma 21 simplifies to
1 − 𝑒−𝜅𝜋
1 + 𝑒−𝜅𝜋 .
1 + 𝑒−𝜅𝜋 −
1 + 𝑒𝜅𝜋
(17)
(cid:18)
(cid:19)
1
𝜅
1
𝜅
=
1
E
𝜃∼PurArc(2,𝜅)[𝜃] =
E
1
𝜅
−
𝜋
𝑒𝜅𝜋 − 1 =
1
𝜅
− 𝜋𝑒−𝜅𝜋
1 − 𝑒−𝜅𝜋 .
(18)
The formula for the expected angular distances allow us to ana-
lytically compare the average (angular) error induced by the dis-
tributions based on the concentration parameter 𝜅, which in turn
depends on the privacy parameter 𝜖 (cf. Section 3.5):
Theorem 24. For any value 𝜅 > 0, the WL distribution has a strictly
larger expected angular distance than Purkayastha:
𝜅𝜋2
𝑒2𝜅𝜋 − 1 >
E
E
E
[𝜃] +
[𝜃] >
𝜃∼PurArc
𝜃∼WLArc
𝜃∼PurArc
The limits of both expected distances are 0 as 𝜅 → ∞ and 𝜋
[𝜃]
2 as 𝜅 → 0.
Figure 2c shows expected angles of PurArc(2, 𝜅) and WLArc(𝜅)
(blue lines) for a range of 𝜅 ∈ [10−2, 10]. As we can see, the baseline
has larger expected errors which is in line with Theorem 24.
3.6.3 Polar Laplace. The Planar Laplace (PL) mechanism [2, 4] was
originally invented in the context of protecting geolocation data.
It can be considered as a two-dimensional variant of the standard
Laplace mechanism that works in Cartesian coordinates by trans-
lating the initial starting point 𝒙 ∈ R2 by a certain distance 𝑟 along
a certain direction 𝛼. The distance 𝑟 and direction 𝛼 are polar coor-
dinates obtained by sampling a random direction 𝛼 ∼ Uni(0, 2𝜋)
and a displacement radius 𝑟 ∼ Γ(2, 1/𝜖) from a Gamma distribution.
When applying the PL mechanism to spherical instead of Carte-
sian coordinates, we obtain the Polar Laplace mechanism1 [5] that
respects the curvature of the (roughly) spherical Earth: The initial
point 𝒙 is represented in spherical coordinates (e.g., latitude and
longitude). We then draw a random sample of polar coordinates
(𝑟, 𝛼) ∼ Γ(2, 1/𝜖) × Uni(0, 2𝜋) as with PL, and, as post-processing
step, solve the direct geodesic problem2 to find the destination point
z that is reached after traveling for a distance of 𝑟 units in the direc-
tion specified by 𝛼. As with WL, we pass the starting point again