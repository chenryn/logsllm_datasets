ğœ…, resulting in a very efficient method even in high dimensions.
More recently, Kurz and Hanebeck [27] proposed another sam-
pling algorithm for the VMF distribution that is best described as
approximate inversion method. It works by substituting ğ‘¡ = cos(ğœ—)
in the tangent-normal decomposition as in Eq. (2) and constructing
Session 4D: Differential Privacy CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea1209Algorithm 1: Approx. inversion method for PurArc(ğ‘›, ğœ…)
Input: dimension ğ‘›, concentration parameter ğœ…, max. no. of
iterations ğ‘€ â‰¥ 1, (optional: absolute tolerance ğ›¿abs)
Output: A sample ğœ— âˆˆ [0, ğœ‹] of PurArc(ğ‘›, ğœ…)
1 ğ‘ â† 0; ğ‘ â† ğœ‹;
2 ğ‘¢ (cid:123) Uni(0, 1);
3 for ğ‘– â† 1 to ğ‘€ do
ğœ— â† (ğ‘ + ğ‘)/2;
ğ‘¦ â† ğ¹ğ‘›âˆ’2,âˆ’ğœ… (ğœ—)
ğ¹ğ‘›âˆ’2,âˆ’ğœ… (ğœ‹) ;
if |ğ‘¦ âˆ’ ğ‘¢|  ğ‘¢ then ğ‘ â† ğœ—;
// evaluate PurArc[ğœƒ â‰¤ ğœ—]
// (optional)
// adjust lower,
// or upper bound
// initial interval bounds
// uniform sample
4
5
6
7
8
9 end
10 return ğœ—
a sample x = cos(ğœ—)ğ +sin(ğœ—)ğƒ . This reduces the problem to gener-
ating ğœ— (cid:123) VMFArc(ğ‘›, ğœ…) from the univariate angular distribution.
If the corresponding angular CDF VMFArc(ğ‘›, ğœ…)[ğœƒ â‰¤ ğœ—] was in-
vertible analytically, a textbook version of the inversion method
(see, e.g., [8]) could be used to sample ğœ—. Kurz and Hanebeck solve
this by approximately inverting the CDF: If we can efficiently com-
pute the CDF ğ‘¢ = VMFArc(ğ‘›, ğœ…)[ğœƒ â‰¤ ğœ—], we can approximate its
inverse ğœ— = VMFArc(ğ‘›, ğœ…)âˆ’1[ğ‘¢] numerically, e.g., by interval bisec-
tion, which is â€œguaranteed to converge up to machine precisionâ€ in
a reasonable number of steps [27]. Unfortunately, their solution for
VMFArc(ğ‘›, ğœ…)[ğœƒ â‰¤ ğœ—] is analytical only for odd ğ‘›, while it contains
an infinite series in terms of special functions for even ğ‘› which
we cannot evaluate efficiently. Therefore, this approach is only vi-
able for VMF when ğ‘› is odd, which is why we prefer the rejection
scheme from the previous paragraph as it is fast and simple to
use in general. However, we show next that this idea is useful for
sampling the Purkayastha distribution.
3.4.2 Purkayastha sampling method. To our best knowledge, there
is no published sampling method for the Purkayastha distribution.
Cutting et al. [7] state that they generated samples for lower dimen-
sions up to ğ‘› = 100, but without specifying the exact method they
used. Rather, they give the following explanation (emphasis ours):
The Purkayastha distribution is numerically hard to
generate for dimensions larger than 150, which is the
only reason why the dimensions considered in this
second simulation are smaller than in the first one.
Here, the â€œfirstâ€ and â€œsecond simulationâ€ refer to sampling from
the VMF and Purkayastha distribution, respectively.
Approximate inversion Purkayastha sampling algorithm. Recall
that we have derived a solution for the angular CDF of the Purka-
yastha distribution, PurArc(ğ‘›, ğœ…)[ğœƒ â‰¤ ğœ—] = ğ¹ğ‘›âˆ’2,âˆ’ğœ…(ğœ—)/ğ¹ğ‘›âˆ’2,âˆ’ğœ…(ğœ‹),
in Corollary 23. While we are not aware of a way to directly compute
its inverse to apply the inversion method [8] for ğ‘› > 2, the solution
itself is a finite closed-form expression that can be computed analyt-
ically. We hence propose an approximate inversion method, similar
to the approach by Kurz and Hanebeck [27] for VMF in Section 3.4.1,
to obtain a new Purkayastha sampling algorithm: Since we can effi-
ciently compute the angular CDF ğ‘¢ = PurArc(ğ‘›, ğœ…)[ğœƒ â‰¤ ğœ—], we can
approximate its inverse ğœ— = PurArc(ğ‘›, ğœ…)âˆ’1[ğ‘¢] numerically. We de-
scribe the core method to sample ğœ— (cid:123) PurArc(ğ‘›, ğœ…) in Algorithm 1.
Once we have a sample ğœ—, we draw ğƒ (cid:123) Uni(Sğ‘›âˆ’2 âŠ¥ ğ) and as
above use the tangent-normal decomposition Eq. (2) to construct
x = cos(ğœ—)ğ + sin(ğœ—)ğƒ âˆ¼ Pur(ğ, ğœ…).
Since our solution for the angular CDF is a closed-form expression
with finitely many terms in any number of dimensions ğ‘›, we ar-
gue that our approximate inversion method for the Purkayastha
distribution is practical regardless of the parity of ğ‘›.
Algorithm 1 can easily be vectorized to generate multiple samples
at once, or parallelized to utilize multiple CPU cores. In fact, we
benchmark our method in up to tens of thousands of dimensions
(cf. Section 4.1), pushing beyond the status quo [7] by providing an
efficient sampling algorithm in dimensions much larger than 150.
3.5 Choice of parameters based on privacy level
To actually run the proposed directional privacy mechanisms on
a given input vector ğ’™ âˆˆ Sğ‘›âˆ’1, we need to generate samples from
Pur(ğ’™, ğœ…) or VMF(ğ’™, ğœ…) where the mode is given by the input ğ’™
and the concentration parameter ğœ… is defined through the privacy
parameter ğœ–. Having described sampling methods for both the VMF
mechanism (cf. Section 3.4.1) and a novel sampling scheme for the
Purkayastha mechanism (cf. Section 3.4.2), it remains to explain the
exact choice of ğœ… based on ğœ– and the desired notion of privacy.
Given a unit vector ğ’™ âˆˆ Sğ‘›âˆ’1, in order to achieve directional
privacy with privacy parameter ğœ–, i.e. ğœ–ğ‘‘âˆ¡-privacy (Definition 13),
we simply need to set ğœ… = ğœ– and draw a sample z (cid:123) Pur(ğ’™, ğœ–) or
z (cid:123) VMF(ğ’™, ğœ–) as shown in Corollary 16 and 19, respectively.
Metric privacy (Definition 3) [4] and its variants can also be
interpreted as providing a privacy (or indistinguishability) level
â„“ = ğœ–ğ‘Ÿ to any two points ğ’™, ğ’™â€² within a protection radius (or angle)
ğ‘Ÿ > 0, cf. [2]. In case of directional privacy (Definition 13), this is
achieved by sampling with ğœ… = â„“/ğ‘Ÿ. In other words, an (â„“/ğ‘Ÿ)-private
mechanism achieves a privacy level â„“ within a protection radius ğ‘Ÿ.
As special case, when ğ’™ = ğ‘“ (ğ·) is the result of a (query) function
ğ‘“ : D â†’ Sğ‘›âˆ’1, we achieve pure ğœ–-DP by setting the protection
radius ğ‘Ÿ := Î” to the (worst-case) sensitivity of ğ‘“ , i.e., by sampling
with a concentration parameter ğœ… = ğœ–/Î” as per Fact 14. Thus, direc-
tional privacy allows relaxing pure DP by specifying a protection
radius ğ‘Ÿ smaller than the sensitivity Î”.
3.6 Circular and spherical baselines
For comparison, we consider the following adaptions of established
standard privacy mechanisms to directional data. The first and
second mechanisms, Clipped and Wrapped Laplace, are suitable for
circular data (ğ‘› = 2), whereas the third one, Polar Laplace, can be
regarded as variant of Wrapped Laplace for spherical data (ğ‘› = 3).
3.6.1 Clipped Laplace. A straightforward application of the usual
Laplace mechanism [10] with post-processing achieves DP on the
circle by adding Laplace noise to a given angle, followed by clipping
the result to an interval covering one full circle, say [0, 2ğœ‹) or
[âˆ’ğœ‹, ğœ‹). This method is simple, but clearly has drawbacks: For
small ğœ–, the major part of the probability mass will be outside the
Session 4D: Differential Privacy CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea1210]
ğœƒ
[
c
r
A
P
1.0
0.8
0.6
0.4
0.2
0.0
ğœ…
0.01
0.6
1.0
2.5
Dist. P
PurArc(2, ğœ…)
WLArc(ğœ…)
0
1
2
ğœƒ
3
]
ğœƒ
[
c
r
A
P
1.0
0.8
0.6
0.4
0.2
0.0
ğœ…
0.01
0.6
1.0
2.5
Dist. P
PurArc(3, ğœ…)
PolArc(ğœ…)
1.5
1.0
0.5
P
âˆ¼
ğœƒ
,
]
ğœƒ
[
E
0
1
2
ğœƒ
3
10âˆ’2
Dist. P
PurArc(2, ğœ…)
WLArc(ğœ…)
PurArc(3, ğœ…)
PolArc(ğœ…)
10âˆ’1
100
ğœ…
101
(a) Angular PDFs on S1 (circular case, ğ‘› = 2)
Figure 2: Comparison of angular densities and expectations of Purkayastha vs. Wrapped/Polar Laplace (solid vs. dashed lines).
(b) Angular PDFs on S2 (spherical case, ğ‘› = 3)
(c) Expected angles: larger for baselines
ğ‘’ğœ…ğœƒ
(cid:18)
(cid:19)
clipping range, creating a bias towards the angle at its boundaries.
We therefore use it only in selected experiments.
3.6.2 Wrapped Laplace. Instead of clipping, we can add Laplace
noise to the original angle ğ›¼ and wrap it around the circle by re-
ducing the result modulo 2ğœ‹. This results in a so-called (symmetric)
Wrapped Laplace (WL) distribution with mean ğ›¼. With the usual
parametrization on the unit circle, the density of a WL distribution
with zero mean and concentration parameter ğœ… â‰¥ 0 is (cf. [21])
ğ‘’âˆ’ğœ…ğœƒ
1 âˆ’ ğ‘’âˆ’ğœ…2ğœ‹ +
ğœ…
2
,
(cid:18)
(15)
ğ‘’ğœ…2ğœ‹ âˆ’ 1
ğœƒ âˆˆ [0, 2ğœ‹).
WL(ğœ…)[ğœƒ] =
Angular density. In accordance with Corollary 9, the correspond-
ing angular density WLArc is the density of points with the same
angle from the mean, in any direction. That is, it identifies an angle
ğœƒ âˆˆ [0, ğœ‹) with its mirror image 2ğœ‹ âˆ’ ğœƒ. By symmetry, it is just
twice the density on the full circle:
ğ‘’âˆ’ğœ…ğœƒ
1 âˆ’ ğ‘’âˆ’2ğœ…ğœ‹ +
(16)
While the Purkayastha angular density PurArc(2, ğœ…)[ğœƒ] âˆ ğ‘’âˆ’ğœ…ğœƒ
on S1 only has a single term ğ‘’âˆ’ğœ…ğœƒ , cf. Eq. (9), WLArc(ğœ…) has an
additional second term ğ‘’+ğœ…ğœƒ that increases with the angle ğœƒ. The
wrapping hence smoothens the distribution by moving probability
mass away from the mode as illustrated in Fig. 2a. It hence provides
less accuracy than Purkayastha at the same privacy level, thus
motivating the need for specialized directional mechanisms.
WLArc(ğœ…)[ğœƒ] = ğœ…
ğœƒ âˆˆ [0, ğœ‹).
ğ‘’2ğœ…ğœ‹ âˆ’ 1
(cid:19)
ğ‘’ğœ…ğœƒ
,
Expected angular distance. Similarly to the derivation of the
expected surface distance for the Purkayastha distribution from
PurArc[ğœƒ], we can derive the expected angular distance for the WL
distribution from WLArc[ğœƒ]. The result is
1
ğœƒâˆ¼WLArc(ğœ…)[ğœƒ] =
For comparison, the expected angular distance of the circular Pur-
kayastha distribution from Lemma 21 simplifies to
1 âˆ’ ğ‘’âˆ’ğœ…ğœ‹
1 + ğ‘’âˆ’ğœ…ğœ‹ .
1 + ğ‘’âˆ’ğœ…ğœ‹ âˆ’
1 + ğ‘’ğœ…ğœ‹
(17)
(cid:18)
(cid:19)
1
ğœ…
1
ğœ…
=
1
E
ğœƒâˆ¼PurArc(2,ğœ…)[ğœƒ] =
E
1
ğœ…
âˆ’
ğœ‹
ğ‘’ğœ…ğœ‹ âˆ’ 1 =
1
ğœ…
âˆ’ ğœ‹ğ‘’âˆ’ğœ…ğœ‹
1 âˆ’ ğ‘’âˆ’ğœ…ğœ‹ .
(18)
The formula for the expected angular distances allow us to ana-
lytically compare the average (angular) error induced by the dis-
tributions based on the concentration parameter ğœ…, which in turn
depends on the privacy parameter ğœ– (cf. Section 3.5):
Theorem 24. For any value ğœ… > 0, the WL distribution has a strictly
larger expected angular distance than Purkayastha:
ğœ…ğœ‹2
ğ‘’2ğœ…ğœ‹ âˆ’ 1 >
E
E
E
[ğœƒ] +
[ğœƒ] >
ğœƒâˆ¼PurArc
ğœƒâˆ¼WLArc
ğœƒâˆ¼PurArc
The limits of both expected distances are 0 as ğœ… â†’ âˆ and ğœ‹
[ğœƒ]
2 as ğœ… â†’ 0.
Figure 2c shows expected angles of PurArc(2, ğœ…) and WLArc(ğœ…)
(blue lines) for a range of ğœ… âˆˆ [10âˆ’2, 10]. As we can see, the baseline
has larger expected errors which is in line with Theorem 24.
3.6.3 Polar Laplace. The Planar Laplace (PL) mechanism [2, 4] was
originally invented in the context of protecting geolocation data.
It can be considered as a two-dimensional variant of the standard
Laplace mechanism that works in Cartesian coordinates by trans-
lating the initial starting point ğ’™ âˆˆ R2 by a certain distance ğ‘Ÿ along
a certain direction ğ›¼. The distance ğ‘Ÿ and direction ğ›¼ are polar coor-
dinates obtained by sampling a random direction ğ›¼ âˆ¼ Uni(0, 2ğœ‹)
and a displacement radius ğ‘Ÿ âˆ¼ Î“(2, 1/ğœ–) from a Gamma distribution.
When applying the PL mechanism to spherical instead of Carte-
sian coordinates, we obtain the Polar Laplace mechanism1 [5] that
respects the curvature of the (roughly) spherical Earth: The initial
point ğ’™ is represented in spherical coordinates (e.g., latitude and
longitude). We then draw a random sample of polar coordinates
(ğ‘Ÿ, ğ›¼) âˆ¼ Î“(2, 1/ğœ–) Ã— Uni(0, 2ğœ‹) as with PL, and, as post-processing
step, solve the direct geodesic problem2 to find the destination point
z that is reached after traveling for a distance of ğ‘Ÿ units in the direc-
tion specified by ğ›¼. As with WL, we pass the starting point again