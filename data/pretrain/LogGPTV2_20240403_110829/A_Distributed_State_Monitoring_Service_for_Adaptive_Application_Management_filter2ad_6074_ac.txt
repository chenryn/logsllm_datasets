5.2. Lifecycle Coordination 
A  problem  in  all  our  systems  was  the  need  to 
coordinate  application  startup.  For  example,  our  web 
server  demonstrator  required  that  a  database  and  a 
logging  system  be  up  and  running  before  the  web 
servers  could  be  started.  This  type  of  dependency 
generalizes to any step in the lifecycle of a component. 
To coordinate lifecycles each component registers a 
provider  that  defines  its  lifecycle  state  (deployed, 
starting, 
etc.).  Lifecycle 
dependencies  among  components  can  be  expressed  as 
distributed  predicates  over  these  states,  so  predicate 
evaluation components can be used to initiate lifecycle 
transitions.  In  this  pattern  all  components  can  be 
deployed immediately and left to start themselves in an 
appropriate order. 
terminating, 
running, 
One  notable  extension  to  this  pattern  is  automated 
application  testing.  A  test  management  system  can 
deploy  an  application  under  test  and  use  predicate 
evaluation components  to determine success or failure 
of  the  test;  initiating  tear-down  before  deploying  the 
next test. 
Advantages  and  pattern  of  use:  This  pattern 
greatly  simplifies  programming  some  behavior,  but 
care needs to be taken to ensure circular dependencies 
are  avoided.  Generally 
introduces  a 
relatively sparse set of short lived relationships. In the 
case of ordered start-up a component can stop listening 
once it has performed the guarded transition. 
the  pattern 
Properties used: The lifecycle coordination pattern 
generally  requires 
just  reliable  delivery  of  state 
information.  However,  any  dependency  on  non-
existence of a component can only be determined using 
stable partition view properties. 
5.3. Compositional Failure Management  
The example systems all used a composition model 
promoted by SmartFrog that lead naturally to a pattern 
for  managing  failure  recovery.  Common  patterns  for 
implementing  fault-tolerance  or  high  availability  can 
be  mapped  to  the  composition  model.  For  example, 
active  replication  uses  a  collection  of  application 
components  to  provide  redundant  processing.  Such  a 
group  can  be  managed  by  a  single  parent  component 
that  replaces  members  when  they  fail  and  represents 
the  group  as  a  single,  fault-tolerant  entity  in  the 
management  system.  Crash  restart  uses  the  ability  to 
recover  an  application  component  by  restarting  it,  a 
role  that  can  be  adopted  by  a  parent  component. 
Failure  recovery  patterns 
these  were  used 
extensively in the HP Utility Rendering Service. 
like 
distributed 
These  patterns  can  be  used  to  compose  an  entirely 
recoverable 
application.  Here  we 
concentrate  on  hierarchical  crash  restart.  Figure  6 
below shows a hierarchy of components that manage a 
distributed  application.  The  hierarchy  needs  to  remain 
connected  to  effectively  manage  the  application,  but  a 
node  failure  or  network  partition  could  lead  to  loss  of 
part of the hierarchy, as represented by the grey nodes. 
Root
Region
affected by a
node failure
Figure 6. Effect of node failure on a component 
composition hierarchy 
To  attack  this  particular  problem  a  crash-restart 
component  was  implemented  that  used  Anubis  to 
implement the following policy: 
1. Initially  look  for  children  and  only  create  them  if 
they do not already exist. 
2. If at any time a child fails then recreate it. 
When each component in the hierarchy adopts this 
crash-restart  policy  the  result  automatically  fills  holes 
in  the  tree  structure  by  recursively  replacing  missing 
components. The exception is the root of the tree as it 
has  no  parent  to  manage  it,  so  a  non-hierarchical 
approach is required. For this we use an active-standby 
policy  implemented  by  multiple  components  that 
observe each other and deterministically decide  which 
should play the root role at any given time. 
the  ability 
Advantages:  Anubis  provides 
to 
determine  component  existence  in  stable  partitions,  so 
the  act  of  observing  a  descendant  in  the  hierarchy 
provides all that is required to determine its existence, 
locate  it  using  information  encoded  in  its  state,  and 
determine  its  loss  in  the  event  of  failure  or  network 
partitions. Implementing this pattern is trivial. 
Pattern  of  use:  As  shown  in  this  example,  the 
compositional recovery pattern introduces relationships 
between  components  that  are  typically  many-to-one 
(children-to-parent) 
the 
components  implementing  the  recovery  management 
grouped 
and 
around 
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:53:45 UTC from IEEE Xplore.  Restrictions apply. 
policy  (the  parent).  Otherwise  the  relationships  were 
relatively sparse. 
rely  on 
Properties  used:  The  pattern  described  relies  on 
the  stable  partition  properties  to  determine  non-
existence on start-up. Once past this decision point, the 
components 
the  consistency  properties 
available  in  any  partition  (regardless  of  stability)  to 
provide  failure  detection  guarantees.  Although  Anubis 
is partitionable, most of the applications were not, and 
in  some  cases  the  management  system  would  need  to 
identify a privileged component or a primary partition 
in  order  to  correctly  terminate  unwanted  components. 
Anubis  does  not  provide  identification  of  a  primary 
partition  directly,  but  policies  based  on  the  ability  to 
observe  a  given  component  (e.g.  the  root  component) 
or  the  majority  of  a  given  set  of  components,  are 
supported and are generally more applicable. 
6. Notes on Timeliness 
Protocols  based  on  timeouts  are  sensitive  to  the 
responsiveness  of  the  underlying  system.  Our  service 
was  implemented  in  Java  and  ran  on  non-real  time 
operating  systems  such  as  Linux  and  Windows. 
However, the timeouts used in practice were generally 
quite large (the actual value of (cid:303) ranged from two to 10 
seconds). Neither the JVM nor OS caused unnecessary 
delays,  even  in  our  larger  systems  (the  HP  Utility 
Rendering  Service  ran  across  120  machines  and  we 
have  tested  a  system  with  over  1000  Anubis  servers 
spread over the same 120 physical machines). 
One  cause  of  instability  we  did  encounter  was 
misconfiguration of other system services. In one case 
NTP was configured to use a time source that became 
inaccessible  when  a  firewall  was  installed.  In  another, 
an NFS server caused the OS on all machines to pause 
when 
file  systems 
simultaneously.  In  a  third,  two  network  switches  with 
incompatible configuration for multicast packet routing 
overloaded  the  network.  Our  partition  protocols  were 
sensitive  to  these  issues,  but  interestingly  also  proved 
very useful in debugging the problems. 
too  many 
tried 
to  mount 
These  cases  highlight  the  importance  of  correct 
total system configuration in time-critical systems. 
7. Conclusion
We have described how Anubis, a state monitoring 
service,  has  been  used  as  part  of  a  distributed 
management  framework  for  adaptive  applications  in 
Grid  and  Utility  computing  environments.  The  use 
cases  presented  demonstrate  how  developers  have 
exploited the service in practice to implement resource 
management, lifecycle coordination, and compositional 
failure  management.  The  main  advantage  of  Anubis 
has  been  that  it  hides  the  complexity  of  coordinating 
distributed  management  agents,  simplifying 
their 
programming.  The  state  observation  abstraction  and 
temporal  properties  Anubis  provides  have  proven 
useful in our experience. 
8. References 
[1] Y.  Amir,  L.  E.  Moser,  P.  M.  Melliar-Smith,  D.  A. 
Agarwal, P. Ciarfella, “The Totem Single-Ring Ordering 
and  Membership  Protocol”,  ACM  Transactions  on 
Computer Systems, vol. 13(4), Nov. 1995, pp.311-342 
[2] P.  Anderson,  P.  Goldsack,  J.  Paterson,  “SmartFrog 
meets  LCFG  -  Autonomous  Reconfiguration  with 
Central Policy Control”, Proceedings of the 2003 Large 
Installations Systems Administration (LISA) Conference,
Oct. 2003 
[3] K. P. Birman, T. A. Joseph, “Reliable Communication in 
the  Presence  of  Failures”,  ACM  Transactions  on 
Computer Systems, vol. 5(1), Feb. 1987, pp.47-76 
[4] F.  Cristian,  C.  Fetzer,  “The  Timed  Asynchronous 
Distributed  System  Model”,  Proceedings  of  the  28th 
Annual  International  Symposium  on  Fault-Tolerant 
Computing, 1998 
[5] C.  Fetzer,  F.  Cristian,  “Fail-Awareness 
in  Timed 
Asynchronous  Systems”,  Proceedings  of  the  15th  ACM 
Symposium  on  Principles  of  Distributed  Computing,
May 1996, pp.314-321 
[6] C.  Fetzer,  F.  Cristian,  “A  Fail-Aware  Membership 
Service”,  Proceedings  of  the  Sixteenth  Symposium  on 
Reliable Distributed Systems, Oct. 1997, pp.157-164 
I.  Foster,  C.  Kesselman,  J.  Nick,  S.  Tuecke,  “Grid 
Services for Distributed System Integration”, Computer,
vol. 35(6), June 2002, pp.37-46 
[7]
[8] Hewlett-Packard,  Servicing  the  Animation  Industry: 
HP’s  Utility  Rendering  Service  Provides  On-Demand 
Computing  Resources, http://www.hpl.hp.com/SE3D,
2004 
[9] J.  O.  Kephart,  D.  M.  “Chess,  The  vision  of  autonomic 
computing”, Computer, vol. 36(1), Jan. 2003, pp.41-50 
[10] J.  Mayo,  P.  Kearns,  “Global  Predicates  in  Rough  Real 
Time”,  Proceedings.  Seventh  IEEE  Symposium  on 
Parallel  and  Distributed  Processing,  Oct.  1995,  pp.17-
24
[11] S. Mishra, C. Fetzer, F. Cristian, “The Timewheel Group 
IEEE  Transactions  on 
Communication  System”, 
Computers, vol. 58(8), Aug. 2002, pp.883-899 
[12] P.  Murray,  “The  Anubis  Service”,  Hewlett-Packard 
Laboratories Technical Report, 2005 
[13] B.  Oki,  M.  Pfluegl,  A.  Siegel,  D.  Skeen,  “The 
Information  Bus  –  An  Architecture  for  Extensible 
Distributed  Systems”,  Proceedings  of  the  14th  ACM 
Symposium on Operating System Principles, Dec. 1993, 
pp.58-68 
[14] SmartFrog 
Reference 
Manual 
v3.02, 
http://www.smartfrog.org/, July 2004 
[15] J.  Wilkes,  J.  Mogul,  J.  Suermondt,  “Utilification”, 
the  11th  ACM  SIGOPS  European 
Proceedings  of 
Workshop, Sept. 2004 
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:53:45 UTC from IEEE Xplore.  Restrictions apply.