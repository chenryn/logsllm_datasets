title:Importance Sampling of Interval Markov Chains
author:Cyrille J&apos;egourel and
Jingyi Wang and
Jun Sun
Singapore Management University 
Singapore Management University 
Institutional Knowledge at Singapore Management University 
Institutional Knowledge at Singapore Management University 
Research Collection School Of Information 
Systems 
School of Information Systems 
6-2019 
Importance sampling of Interval Markov Chains 
Importance sampling of Interval Markov Chains 
Cyrille JEGOUREL 
Jingyi WANG 
Jun SUN 
Singapore Management University, PI:EMAIL 
Follow this and additional works at: https://ink.library.smu.edu.sg/sis_research 
 Part of the Software Engineering Commons 
Citation 
Citation 
JEGOUREL, Cyrille; WANG, Jingyi; and SUN, Jun. Importance sampling of Interval Markov Chains. (2019). 
Proceedings of the 48th Annual IEEE/IFIP International Conference on Dependable Systems and 
Networks, Luxembourg, 2018 June 25-28. 303-313. Research Collection School Of Information Systems. 
Available at:
Available at: https://ink.library.smu.edu.sg/sis_research/4967 
This Conference Proceeding Article is brought to you for free and open access by the School of Information 
Systems at Institutional Knowledge at Singapore Management University. It has been accepted for inclusion in 
Research Collection School Of Information Systems by an authorized administrator of Institutional Knowledge at 
Singapore Management University. For more information, please email libIR@smu.edu.sg. 
2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Importance Sampling of Interval Markov Chains
1st Cyrille Jegourel
Information Systems Technology and Design
Singapore University of Technology and Design
Singapore, Singapore
cyrille.jegourel@gmailcom
2nd Jingyi Wang
Information Systems Technology and Design
Singapore University of Technology and Design
Singapore, Singapore
PI:EMAIL
3rd Jun Sun
Information Systems Technology and Design
Singapore University of Technology and Design
Singapore, Singapore
PI:EMAIL
Abstract—In real-world systems, rare events often characterize
critical situations like the probability that a system fails within
some time bound and they are used to model some potentially
harmful scenarios in dependability of safety-critical systems.
Probabilistic Model Checking has been used to verify depend-
ability properties in various types of systems but is limited by
the state space explosion problem. An alternative is the recourse
to Statistical Model Checking (SMC) that relies on Monte Carlo
simulations and provides estimates within predeﬁned error and
conﬁdence bounds. However, rare properties require a large
number of simulations before occurring at least once. To tackle
the problem, Importance Sampling, a rare event simulation
technique, has been proposed in SMC for different types of
probabilistic systems. Importance Sampling requires the full
knowledge of probabilistic measure of the system, e.g. Markov
chains. In practice, however, we often have models with some
uncertainty, e.g., Interval Markov Chains. In this work, we
propose a method to apply importance sampling to Interval
Markov Chains. We show promising results in applying our
method to multiple case studies.
Index Terms—Rare Events, Importance Sampling, Markov
Chains, Interval Markov Chains, Dependability, Statistical Model
Checking
I. INTRODUCTION
Discrete Time Markov Chains (DTMC) are a standard
formalism to model and reason about probabilistic systems
[9], [27], well suited to dependability analysis of security
protocols (e.g. [20]) or safety-critical systems. In particular,
the reliability of a failure-repair process can be described by
a Markovian structure based on stochastic failure and repair
mechanisms of the system components and can be investigated
by reachability or mean time to failure properties speciﬁed
with an appropriate logic.
However, modelling real-world systems is a difﬁcult task:
individual probabilistic transitions are in general unknown or
partially known and may be given with a margin of error.
For this reason, many extension of Markov Chains have been
proposed in the literature [16], [22], [30]. In particular, Interval
Markov Chains (IMC) are a formalism in which the transition
values of a DTMC are given within intervals. Algorithms for
common implementation and consistency of IMC have been
2158-3927/18/$31.00 Â©2018 IEEE
DOI 10.1109/DSN.2018.00040
303
proposed [10]. In the original work [16], the IMC semantics
allowed a transition to be taken with different values in
their corresponding interval at each occurrence. In this work,
we consider an alternative common interpretation for IMCs
in which they represent all of the DTMCs such that
the
transition probabilities lie in their corresponding intervals. In
this semantics, the transitions are ﬁxed once-and-for-all1.
Probabilistic Model Checking algorithms have been de-
veloped to analyse stochastic systems in the context of DTMCs
(e.g. [12], [29]) and IMCs [3], [4] but they are limited by
the state space explosion problem. This limitation prompted
the development of simulation-based techniques like Statist-
ical Model Checking (SMC) [29]. SMC requires the use of
an executable model of the system and then estimates the
probability of a property based on the simulations. One of
the core ideas of SMC is to sample independent execution
traces of the system and individually verify if they satisfy a
property of interest. The probability that the system satisﬁes
the property is estimated by the proportion of traces which
satisfy the property. By modelling the executions of a system
as a Bernoulli random variable, SMC provides rigorous bounds
of the error of the estimator based on conﬁdence intervals or
Chernoff bounds [6]. Note that SMC is not limited to fre-
quentist inference and may use alternative efﬁcient techniques,
such as Bayesian inference [15] and hypothesis testing [28],
to decide with speciﬁed conﬁdence whether the probability of
a property exceeds a given threshold or not.
However, rare events pose a problem to SMC because they
imply that a large number of simulations must be sampled
in order to observe them. Hence SMC may still be compu-
tationally challenging. Several variance reduction techniques,
such as Importance Splitting [13] and Importance Sampling
(IS) [14], [23], have been applied to estimate rare dependable
properties in Markov models. IS works by simulating a system
under a weighted (IS) distribution that makes a property
more likely to be seen. It then compensates the results by
1Note that the once-and-for-all semantics is not novel but, as far as we
know, the terminology is recent. See for example [3].
the weights, to estimate the probability under the original
distribution. In order to perform IS and to evaluate the resulting
estimator,
is necessary to know exactly the probability
distribution of the original system. This limitation makes IS
infeasible for probability estimation of an IMC since the
probabilistic transitions are given in intervals.
it
The goal of this work is to overcome this problem by using
an optimisation algorithm. Due to the potentially large number
of observed transitions and the inherent number of constraints
that must be fulﬁlled, standard numerical and statistical ap-
proaches fail to work. We thus propose a new algorithm which
is shown to work effectively for IMC importance sampling
(IMCIS). We implement our approach with a prototype tool
and apply the algorithm to estimate rare dependable properties
of failure-repair processes and a safety property of a secure
water treatment system. The experiment results show empir-
ically that our conﬁdence intervals are correct with respect to
the original system instead of an approximation of the system.
the article: Section II introduces the
basic notions of DTMCs, IMCs and Monte Carlo integration.
Section III introduces the IS framework for IMCs. Section IV
addresses the optimisation problem raised by IMCIS. Our
algorithm is fully described in Section V, with the results
of applying it
to some case studies given in Section VI.
Section VII concludes the paper.
a) Structure of
II. BACKGROUND
In this section, we introduce the notions and notations used
throughout the paper. A stochastic system S is interpreted as a
set of interacting components in which the state is determined
randomly with respect to a global probability distribution. Let
(Ω,F, μ) be the probability space induced by the system with
Ω a set of ﬁnite paths with respect to system’s property φ, F a
σ-algebra of Ω and μ the probability distribution deﬁned over
F. We ﬁrst recall the deﬁnitions of a Discrete Time Markov
Chain (DTMC) and an Interval Markov Chains (IMC) and
give the basics of Monte Carlo integration.
A. Discrete Time and Interval Markov Chains
(cid:2)
t∈S A(s, t) = 1.
DTMCs are a standard formalism, extensively used in the
literature, to model probabilistic systems. Formally,
Deﬁnition 2.1: A DTMC is a tuple M = (S, s0, A, G, V ),
where S is a ﬁnite set of states, s0 ∈ S is an initial state, G
is a set of atomic propositions, V : S → 2G is a labelling
function and A : S × S → [0, 1] is a probabilistic transition
function such that ∀s ∈ S,
For convenience, we use a matrix notation for the transition
function, that is A = (aij)0≤i,j≤m with m + 1 = |S|. Each
aij corresponds to the probability to reach sj from si in one
step. We denote ai = (ai0,··· , aim) the probabilistic state
distribution from si ∈ S.
Given the transition matrix A of a DTMC, the probability of
taking a path ω = ω0 → ··· → ωl is deﬁned by the product of
(cid:3)l
the individual transition probabilities of the path, i.e., PA(ω) =
i=1 A(ωi−1, ωi). The length of a path is denoted |ω| and is
deﬁned by its number of transitions.
304
For a given path ω ∈ Ω, we denote nij(ω) the number of
times the transition from state si to state sj occurred. Thus,
we can write P (ω) as a product of the elements of A:
m(cid:4)
m(cid:4)
(1)
(cid:2)m
nij (ω)
ij
a
PA(ω) =
(cid:2)m
j=0 nij(ω) = |ω|. Also,
j=0
i=0
a
−
i=0
Deﬁnition
= 1.
IMC is
nij (ω)
ij
2.2: An
Note that
nij(ω) = 0 and then a
if aij = 0,
tuple M =
, A+, G, V ), where S, s0, G and V are as
(S, s0, A
(cid:2)
for a DTMC and where the transition function is replaced
, A+ : S × S → [0, 1] such that (i)
−
(cid:2)
by two functions A
− ≤ A+, (ii) ∀s ∈ S,
(s, t) ≤ 1 and (iii) ∀s ∈ S,
A
t∈S A+(s, t) ≥ 1.
− and A+ give respective lower and upper bounds on the
A
transition probabilities. IMCs are then a natural extension of
DTMCs since they allow us to specify intervals of possible
probability transitions for each state of the Markov chain. We
say that B ∈ [A] if B is a DTMC that satisﬁes all the interval
constraints of [A] and that bi ∈ [ai] if we restrict the DTMC
and the IMC to state i.
t∈S A
−
B. Learning a DTMC or an IMC
In practice, DTMCs are often obtained through some estim-
ation based on belief, partial knowledge, learning process, etc.
Therefore the transition probability is not precise. A common
way to learn transition matrix A of Markov chain M is to use
standard frequentist estimations based on a (long) sequence of
random observations. An individual transition between state
si and sj can be estimated by ˆaij = nij/ni where nij is
the number of times transition si → sj occurred and ni
the number of times a transition has been taken from state
si. However, this estimation lies within a conﬁdence interval
denoted I. For example, given conﬁdence 1−δ and ni, one can
determine absolute error  such that P (|ˆaij − aij| > ) ≤ δ
−5 and ni = 104,
using the Okamoto bound [21]. With δ = 10
 ≈ 0.025 and I = [ˆaij − ; ˆaij + ].
It
is worth mentioning that
if the state space is large,
standard frequentist estimations are unlikely to be accurate
for all transitions. But other methods have been proposed in
the literature such as Laplace and Good-Turing’s estimations
[8], [11]. Moreover, large models are sometimes parametrised
by global variables that may be learnt up to some precision.
In the latter case, if the transitions are symbolic functions
of the global variables, it is not necessary to observe all the
transitions but to estimate directly the global variables and to
deduce a DTMC or an IMC from it.
In this article, ˆA = (ˆaij)0≤i,j≤m denotes a learnt transition
matrix of Markov chain M. We assume that the DTMC is
learnt up to some precision  = (ij)0≤i,j≤m. Then, we denote
= ˆA − , ˆA+ = ˆA +  and [ ˆA] the corresponding IMC
−
ˆA
centred on DTMC ˆA. By construction, ˆA ∈ [ ˆA].
Fig. 1a illustrates a DTMC A with state space S =
{s0,··· , s3} and a probabilistic distribution μ parametrised
by two individual transitions a and c. Fig. 1b illustrates an
1
s3
1
s3
c
s1
1
s2
s0
b
a
d
(a) a DTMC.
1 − ad
1
s2
s1
s0
1
ad
(c) Perfect Importance Sampling.
1
s3
1
s3
[ˆa ± ˆa]
[ˆc ± ˆc]
s0
s1
[ˆb ± ˆa]
[ ˆd ± ˆc]
(b) an IMC.
c
1−ad
a
s0
s1
−1