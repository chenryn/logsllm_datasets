We are running kubernetes 1.2.4 on GKE. We observed that when we increased
load on our system by 20% our outbound connections from our pods started to
fail 80% of the time. After much diagnosing we found that our cluster was
running with just one `kube-dns` pod. This pod was being hammered and resulted
in failures to resolve DNS requests.
Our cluster started smaller (3x VM 8 core if i remember correctly) and is
currently almost doubled in size/cores (10x VM 4 core).
Creating a new cluster in GKE of the same size gave us three `kube-dns` pods.
My expectations would be that the amound of `kube-dns` pods would scale by
load (using a HPA?) or that the pods would scale by cluster size (using
deamonsets?). Not that the amound of `kube-dns` pods would be based on the
initial cluster size when creating it in GKE.
    Client Version: version.Info{Major:"1", Minor:"2", GitVersion:"v1.2.4", GitCommit:"3eed1e3be6848b877ff80a93da3785d9034d0a4f", GitTreeState:"clean"}
    Server Version: version.Info{Major:"1", Minor:"2", GitVersion:"v1.2.4", GitCommit:"3eed1e3be6848b877ff80a93da3785d9034d0a4f", GitTreeState:"clean"}
NOTE: I do not know if this issue is superseded by the DNS changes discussed
in #22823