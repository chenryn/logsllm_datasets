We are currently running Kubernetes version 1.2.4 on Google Kubernetes Engine (GKE). Recently, we observed that when the load on our system increased by 20%, our outbound connections from the pods started to fail 80% of the time. After extensive troubleshooting, we discovered that our cluster was operating with only one `kube-dns` pod. This single pod was overwhelmed, leading to frequent DNS resolution failures.

Our cluster initially consisted of three virtual machines (VMs) with 8 cores each. It has since grown to ten VMs, each with 4 cores. When we created a new cluster of the same size in GKE, it automatically provisioned three `kube-dns` pods. 

Given this, I would expect the number of `kube-dns` pods to scale based on either the load (potentially using a Horizontal Pod Autoscaler, HPA) or the cluster size (using DaemonSets). However, it appears that the number of `kube-dns` pods is determined by the initial cluster size at creation time in GKE.

Here are the client and server versions for reference:
- Client Version: v1.2.4
- Server Version: v1.2.4

It is worth noting that I am uncertain if this issue has been addressed by the DNS changes discussed in issue #22823.