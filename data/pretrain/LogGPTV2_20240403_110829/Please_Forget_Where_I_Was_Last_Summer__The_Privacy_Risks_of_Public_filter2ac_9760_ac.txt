content that was manually inspected, and references to other
users (i.e., tokens starting with “@”) were removed as well.
In a nutshell, we started with users that explicitly men-
tioned in their tweets that they are at home, by matching
phrases such as “I’m home”, “at home” etc. After identify-
ing users with clusters containing such tweets, we manually
reviewed all the tweets in these particular clusters. During the
manual inspection we took into account the context of a user’s
tweets for ensuring that these clusters indeed correspond to
a user’s home. Instead of identifying work clusters for other
users, we decided to focus this task on the users for which we
had already identiﬁed their home location, as that would allow
us to create a more complete dataset that contains both home
and work locations for each user. To that end, we followed
a similar approach and searched for phrases denoting work-
related information, “at work”, “at the ofﬁce”, “this job” etc.,
6
and manually inspected the tweets of the returned clusters.
Below we outline the workﬂow of our manual inspection
process for identifying users’ home and work locations. Our
goal was to establish a methodology that allows us to have
high conﬁdence in the resulting labels. The content analysis
and location labeling was performed by two researchers inde-
pendently; in cases where the labels by the two researchers
did not match the user was discarded. We avoided potentially
ambiguous instances or cases with uncertainty, and built our
ground truth with users where both labellers agreed. We dis-
carded such instances as we set a strict requirement for correct
labels for our ground truth. However, discarding users was a
rare occurrence, as it is a fairly straightforward and intuitive
process for human annotators to identify home/work locations.
In more detail, we established the following workﬂow:
1) Apart from inspecting the tweets that contained one of
the seed phrases, we also inspected the cluster’s remaining
tweets. This allowed us to further increase our conﬁdence
by identifying tweets where the user explicitly or implic-
itly referred to being at home or work (e.g., “just took
a shower”, “my boss just said” etc.). If we only found
implicit references, we required at least two such tweets.
2) To make our ground truth as complete as possible, we
also manually inspected all the tweets in users’ 10 largest
clusters, for identifying cases where users have multiple
homes or work clusters that were not already identiﬁed
during the previous task. Again we followed the same
approach as described in the previous step. In cases where
there were no other clusters with tweets indicating a
home or work location we were conﬁdent of our original
labeling, since there was only one cluster matching each
label. In cases where other clusters’ alluded to a potential
key location, we continued with the following process:
a) Temporal analysis. We explicitly analyzed the timeline
of clusters, and identiﬁed the periods during which each
cluster was active. This helped us identify cases where
users had changed residences, where multiple locations
had been labeled as homes but their active periods
did not overlap temporally. We also observed cases
where the identiﬁed home was not the user’s place of
residence, but could be considered a secondary home
(i.e., country/summer house, parents’ house). During
this step we also searched speciﬁcally for references
that allowed us to label the cluster as a secondary home
location (e.g., terms referring to parents).
b) Spatial analysis. In cases where more than one cluster
exhibited home-like patterns and had overlapping active
periods, we considered the spatial location of each clus-
ter. If the two clusters were close geographically, we
further investigated them to decide which one was the
user’s actual home and which was not (e.g., a friend’s
house that the user visits frequently). For clusters that
were far away from each other (e.g., in different cities),
we relied on the content for veriﬁcation. A common
occurrence was clusters with home-related keywords
that exhibited continuous activity for a few days: e.g.,
users tweeting that they were at home, while visiting
their parents’ house during the holidays.
Overall, in the Home-Top dataset we have 1,004 users with
1,307 home clusters; 718 of these users have only one home
 0 0.2 0.4 0.6 0.8 1 0 1000 2000 3000Cumulative fractionof users (CDF)Tweets       1 10 100 1000 Geotagged tweets (log)Fig. 3: Number of clusters per user.
cluster, while 269 and 17 users have two and three homes,
respectively. This is not a surprising ﬁnding, as we collected
all the tweets in each user’s timeline (up to 3,200), and not
only tweets posted in a speciﬁc time period. Indicatively, we
have observed cases of users that have relocated (e.g., after
graduating), college students living in dorm rooms during
their ﬁrst year and then moving to a house, and students
that regularly visit their family home. We also observed users
with multiple home locations in the Home-Low ground truth
dataset, but to a lower extent. Speciﬁcally, we identiﬁed 905
users that have one home cluster, 137 users with two, and one
user with three home clusters. For the two work ground truth
datasets, i.e., Work-Top and Work-Low, we identiﬁed 298 and
92 users, that have 363 and 98 work clusters respectively.
It is possible that our ground truth is not exhaustive (i.e.,
we may have missed certain locations). However, due to the
systematic and stringent manual inspection process, we are
certain that the labeled locations indeed correspond to users’
homes and workplaces. Our manual inspection has resulted
in ground truth datasets signiﬁcantly more complete and ﬁne-
grained than those from prior work [25], [34], [19], [20], [43].
V. EXPERIMENTAL EVALUATION
Here we analyze our datasets and discuss properties of
user’s geo-tagging behavior. Then we use our ground truth to
experimentally evaluate LPAuditor and compare to prior work.
Location clusters. To investigate the location patterns in
users’ tweeting behavior, we focus our analysis on understand-
ing the characteristics of users’ location clusters. We perform
this analysis for both the most active (Top-6K) and less active
(Low-10K) users. Figure 3 depicts the number of clusters per
user. As expected, highly active users tend to have a large
number of clusters. Speciﬁcally, only 4.45% of these users
have less than 40 location clusters, and around 28% less than
100 clusters. In more detail, we observe that around 50% of
the highly active users have more than 140 clusters, and about
25% and 10% of them have more than 200 and 280 clusters
respectively. If we only consider clusters that have more than
ﬁve tweets, we observe that about 50% of the users have more
than 11 such clusters, and 10% have more than 22 clusters.
When focusing our analysis on the Low-10K dataset, we
observe that these users have signiﬁcantly less clusters than
the highly active users but seem to follow a similar pattern.
As shown in Figure 3 (right), about 10.7% have ﬁve or less
clusters, and about 50%, 25% and 10% of the users have
more than 21, 40 and 63 clusters respectively. Furthermore,
similarly to the highly active users, the number of clusters
TABLE II: Performance of home/work inference for ground
truth users, and ranks of the respective clusters.
Dataset
Home-Top
Home-Low
Work-Top
Work-Low
Users
1004
1043
298
92
Inferred
clusters
926
969
164
53
Precis.
1
92.2% 806
92.9% 911
7
55%
57.6%
4
Rank of clusters
4
1
-
16
6
5-10
-
1
15
1
2
111
49
79
31
3
8
8
47
11
drops signiﬁcantly when considering only those clusters that
have more than 5 tweets. For both sets of users we ﬁnd that
users tend to have a large number of clusters, out of which the
majority has a small number of tweets.
Figure 4 presents the percentage of users’ tweets in their
ﬁve largest clusters. We observe that for about 40% of the
users, more than half of their tweets belong to their top cluster,
while 47.77% of the users have more than 70% of their tweets
in their top 5 clusters. This phenomenon is observed in both
sets of users. In Figure 5 we explore the cluster sizes of all
users. Both datasets exhibit a power law distribution, with the
vast majority of clusters having only a few tweets and a small
number of clusters with a large number of tweets. These small
clusters will most likely not correspond to a user’s home and
work locations, as they appear to be visited rarely; however,
these locations are important from a privacy perspective, as
they allow an adversary to reconstruct a semantically-rich
location history, which can reveal highly sensitive information.
In fact, this is clear in Figure 6 that presents the distribution
of PSCs with regards to the number of their tweets. We ﬁnd
that 67.10% of the PSCs have one tweet, while only 4.04% of
them have 10 or more (the most being health-related).
A. Home and Work Location Inference
To assess our methodology and measure the effectiveness
of LPAuditor we aim to pinpoint exact locations. Thus, we opt
for a “strict” evaluation of accuracy where a location is either
correctly or incorrectly identiﬁed. We do not calculate distance
errors as they are more suitable for coarse-grained approaches.
LPAuditor correctly identiﬁes the home of 926 and 969
users from the two datasets, resulting in a precision of 92.23%
and 92.9% respectively. Thus apart from obtaining superior
granularity, our system is considerably more effective than
previous approaches as we will show. As our work inference
ﬁrst excludes the home cluster, the outcome also depends on
the precision of the home inference. Our precision is 55.03%
and 57.6% for identifying workplaces in our ground truth. As
users typically tweet less when they are at work than when
they are at home (in our ground truth, home clusters contain an
average of 45% of tweets while work clusters contain 8%) our
effectiveness at identifying work is lower since other locations
frequented by the user can exhibit similar characteristics (e.g.,
restaurants, coffee shops, gyms). Table II presents the precision
of our home and work inference, as well as the rank of all the
correctly identiﬁed clusters. The clusters’ ranks are estimated
according to their size, such that rank 1 is the largest cluster
of the user, rank 2 is the second largest cluster and so on.
Also, we do not re-calculate cluster ranks after excluding home
7
 0 0.2 0.4 0.6 0.8 1 1 10 100 600Top-6KCumulative fractionof users (CDF)Number of clusters (log)       1 10 100 600Low-10K  All clusters5+ tweets10+ tweetsFig. 4: Tweets from users’ top clusters.
Fig. 5: Tweets per cluster.
Fig. 6: Tweets from PSCs.
clusters in the work identiﬁcation phase, as we want to make
direct comparisons between the results of the two approaches.
Finally home clusters have, on average, a maximum radius of
59.55 meters and work clusters of 53.38, which drops to 19.25
meters for all clusters in our ground truth.
Having established the precision of LPAuditor on our
ground truth datasets, we run our system on the main datasets
(Top-6K, Low-10K) after excluding the ground truth users. As
can be seen in Figure 7, the majority of home clusters in both
datasets are rank 1 clusters, which is consistent with the results
from the home ground truth. For the work clusters, only 3.26%
and 7.69% are rank 1, while most of them are rank 2 and a
considerable number occupy lower ranks, in both datasets. We
ﬁnd that the detected clusters follow a similar rank distribution
in the two datasets, supporting the representativeness of our
ground truth. It should be noted though that while our work
ground truth explicitly contains users for which we have
identiﬁed their work, for the main datasets our system also
identiﬁes locations that are not work in the strictest sense.
Speciﬁcally, we are able to identify locations for users that do
not work but have a location that can be considered a work
“substitute”, e.g., a college student attending classes.
Selection bias. Our methodology for creating the ground
truth could potentially result in selection bias, as it relies
on certain key phrases as a starting point for the manual
process. To examine whether the accuracy of our evaluation is
a byproduct of LPAuditor’s heuristics being “overﬁtted” to the
ground truth, we manually examine a random subset of users
identiﬁed from the main datasets (Top-6K, Low-10K). We
select 100 users and manually investigate their tweets to verify
whether the home and work labels assigned by our system are
correct. Following the same manual methodology we are able
to verify that 89 of the home labels indeed correspond to the
user’s actual home cluster. For the remaining 11 users we are
unable to characterize the label as correct or incorrect based
on the users’ tweets. For the work labels, we ﬁnd that for 45
users the work cluster has been correctly identiﬁed while for
30 users the label is incorrect. For the remaining 25 users we
are not able to verify whether the label is correct or not.
While this manually veriﬁed sample is relatively small, we
ﬁnd that the resulting accuracy is comparable to the accuracy
achieved by our system when evaluated against the ground
truth. Furthermore, these users are from our main datasets,
which exhibit a wide range of geotagging behavior, demon-
strating that our effectiveness is not tied to a speciﬁc dataset.
As LPAuditor’s underlying algorithms are based on common
Fig. 7: Ranks of home and work clusters for our main datasets
(ground truth users have been excluded).
user behaviors and legislative/societal norms, we believe that
this manual veriﬁcation further validates the generalisability of
our techniques and the correctness of our ground truth.
De-anonymization. While demonstrating the feasibility of
de-anonymizing Twitter users is not the focus of our work, we
conduct a small exploratory experiment. We aim to identify
which, if any, users in our ground truth datasets appear to be
pseudonymous. Speciﬁcally we want to identify users that do
not provide their full name, i.e., do not provide their last name
(we do not consider ﬁrst names to be conclusive for identity).
We use the list provided by the US Census Bureau with the
most frequent surnames to ﬁlter out users that include their
last name in the full name section of their account.
After ﬁltering 282 users remain, which we manually ex-
amine and exclude the ones that actually disclose a last name
that is not included in the Census list, or include their last
name in their username. We end up with 183 users that do not
explicitly reveal their identity on their Twitter accounts, which
constitutes a lower bound of the pseudonymous users in our
ground truth, due to potential false positives in our automated
ﬁltering. Out of these users, LPAuditor was able to correctly
identify the home location of 171 users and the workplace of
23 users (to ensure privacy, the manual inspection of users’
names was conducted in “isolation” and not combined with
or mapped to any ground truth locations or other location
clusters). While these users might not be truly pseudonymous
in reality (e.g., users with a pseudonym whose actual identity
8
 0 0.2 0.4 0.6 0.8 1     Top-6K dataset  Top 1 clusterTop 3 clustersTop 5 clusters 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1Low-10K datasetCumulative fractionof users (CDF)Percentage of user’s tweetsTop 1 clusterTop 3 clustersTop 5 clusters100101102103104105106100101102103104Number of clusters (log)Number of tweets in cluster (log)Top-6K datasetLow-10K dataset 0.5 0.6 0.7 0.8 0.9 1100101102103Cumulative fraction ofclusters (CDF)Number of tweets in cluster (log)All PSCsMedicalReligionSexual100101102103104               Top-6K dataset  HomeWork100101102103104123456789101112131415+Low-10K datasetNumber of clusters (log)Cluster rank (based on size)HomeWorkTABLE III: Comparison between the precision achieved by LPAuditor and previously proposed approaches. We have implemented
all prior heuristics and applied them to our ground truth datasets to allow a direct comparison.
Heuristic Description
Cluster with the highest number of tweets
(2:00-7:59) and Leisure (19:00-01:59) time frames
Last destination of the day (before 3am)
Last destination of the day (w/o days with tweets between 24:00-7:00)