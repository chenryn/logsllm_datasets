User: After working on several GAE apps, some of which are being used for production, I have come to the conclusion that on this platform, backing up your production data is slow enough and expensive enough for us to transition to some other cloud based technology stack.  
In one of our production apps, we have around a million entities with an average size per entity of 1KB.  So the total size of the data is around a GB which should not be a big deal, right?  Here is the output of the bulkloader tool after fetching the entities from the app engine with default options:

[INFO    ] 948212 entities (608342497 bytes) transferred in 47722.7
  seconds

That is almost 13 hours.  So if we wanted to set up an hourly backup system for our production data, that would be way beyond impossible with the current GAE toolset.
The cost is another story.  I tried using the datastore admin to copy entities to a different app which i thought we could use for backup.  I first set the budget to $2 per day which quickly ran out at around 5000 entities, then i increased the budget to $10 per day which ran out again without being anywhere close to replicating the million entities.  
I obviously dont intend to spend $100 every time i need to back my 1 GB data up neither do i want to wait for hours (or even days) just so that my data would be backed up.  So either I dont know something or Google App Engine is currently just an impractical way to write scalable production quality apps of meaningful size that can be easily backed up and restored.
Is there a fast and cost-effective way to backup your data from a GAE app?
Assistant: Bet you've found a solution by now Yasser, but for anyone else ending up here from Google, here's an updated answer:
The backup option in the appstore admin has been upgraded to support both datastore and cloud storage. It also uses mapreduce to do the backup, which makes the query much lighter on the system.