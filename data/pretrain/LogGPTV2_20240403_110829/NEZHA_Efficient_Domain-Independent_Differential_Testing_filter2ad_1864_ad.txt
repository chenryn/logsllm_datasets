these, we sampled certiﬁcates to construct 100 distinct groups
of 1000 certiﬁcates each. Initially, no certiﬁcate in any of the
initial 100 groups introduced a discrepancy between the tested
applications thus all reported discrepancies in our results are
introduced solely due to the differential testing of the examined
frameworks.
ELF and XZ parsing: We evaluate NEZHA on parsers
of two popular ﬁle formats, namely the ELF and the XZ
formats. For parsing of ELF ﬁles, we compare the parsing im-
plementations in the ClamAV malware detector with that of the
binutils package, which is ubiquitous across Unix/Linux
systems. In each testing session, NEZHA loads a ﬁle and
validates it using ClamAV and binutils (the respective
validation libraries are libclamav and libbfd), and either
reports it as a valid ELF binary or returns an appropriate error
code. Both programs, including all their exported libraries,
are instrumented to work with NEZHA and are differentially
tested for a total of 10 million generations. In our experiments,
we use ClamAV 0.99.2 and binutils v.2.26-1-1_all. Our
seed corpus consists of 1000 Unix malware ﬁles sampled from
VirusShare [9] and a plain ‘hello world’ program.
Similar to the setup for ELF parsing, we compare the
XZ parsing logic of ClamAV and XZ Utils [19], the default
Linux/Unix command-line decompression tool for XZ archive
ﬁles. The respective versions of the tested programs are
ClamAV 0.99.2 and xzutils v5.2.2. Our XZ seed corpus
uses the XZ ﬁles from the XZ Utils test suite (a total of 74
archives) and both applications are differentially tested for a
total of 10 million generations.
PDF Viewers: We evaluate NEZHA on three popular PDF
viewers, namely the Evince (v3.22.1), MuPDF (v1.9a) and
Xpdf (v3.04) viewers. Our pool of tested inputs consists of
the PDFs included in the Isartor [3] testsuite. All applications
are differentially tested for a total of 10 million generations.
During testing, NEZHA forks a new process for each tested
program, invokes the respective binary through execlp, and
uses the return values returned by the execution to the parent
process to guide the input generation using its output δ-
diversity. Determined based on the return values of the tested
programs, the discrepancies constitute a conservative estimate
of the total discrepancies, because while the return values of
the respective programs may match, the rendered PDFs may
differ.
All our measurements were performed on a system run-
ning Debian GNU/Linux 4.5.5-1 while our implementation of
NEZHA was tested using Clang version 3.8.
Q1: How effective is NEZHA in discovering discrepancies?
The results of our analysis with respect to the discrepancies
and memory errors found are summarized in Table II. NEZHA
found 778 validation discrepancies and 8 memory errors in
total. Each of the reported discrepancies corresponds to a
unique tuple of error codes, where at least one application
accepts an input and at least another application rejects it.
Examples of semantic bugs found are presented in Section VI.
622
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:44 UTC from IEEE Xplore.  Restrictions apply. 
Type
Discrepancies
Errors & Crashes
SSL Certiﬁcate XZ Archive ELF Binary PDF File
764
6
5
2
2
0
7
0
TABLE II: Result summary for our analysis of NEZHA.
We observe that, out of the total 778 discrepancies, 764
were reported during our evaluation of the tested SSL/TLS
libraries. The disproportionately large number of discrepancies
found for SSL/TLS is attributed to the ﬁne granularity of
the error codes returned by these libraries, as well as to the
larger number of applications being tested (six applications for
SSL/TLS versus three for PDF and two for ELF/XZ).
To provide an insight into the impact that the number of
tested programs has over the total reported discrepancies, we
measure the total discrepancies observed between every pair
of the six SSL/TLS libraries. In the pair-wise comparison of
Table III, two different return-value tuples that have the same
error codes for libraries A and B are not counted twice for the
(A, B) pair (i.e., we regard the output tuples (cid:2)0, 1, 2, 2, 2, 2(cid:3)
and (cid:2)0, 1, 3, 3, 3, 3(cid:3) as one pairwise discrepancy with respect
to the ﬁrst two libraries). We observe that even in cases of
very similar code bases (e.g., OpenSSL and LibreSSL which
are forks of the same code base), NEZHA successfully reports
multiple unique discrepancies.
LibreSSL
BoringSSL
wolfSSL mbedTLS
GnuTLS
OpenSSL
LibreSSL
BoringSSL
wolfSSL
mbedTLS
10
-
-
-
-
1
11
-
-
-
8
8
8
-
-
33
19
33
6
-
25
19
25
8
31
TABLE III: Number of unique pairwise discrepancies between
different SSL libraries. Note that the input generation is still
guided using all of the tested SSL/TLS libraries.
The results presented in Table II are new reports and
not reproductions of existing ones. They include multiple
conﬁrmed, previously unknown semantic errors. Moreover,
NEZHA was more efﬁcient at reporting discrepancies than all
guided or unguided frameworks we compared it against (see
Q2 & Q3 for further details on this analysis). We present some
examples of semantic bugs that have already been identiﬁed
and patched by the respective software development teams in
Section VI.
Result 1: NEZHA reported 778 previously unknown dis-
crepancies (including conﬁrmed security vulnerabilities
and semantic errors), in total, across all the applications
we tested, even when the latter shared similar code bases.
In addition to ﬁnding semantic bugs, NEZHA was equally
successful in uncovering previously unknown memory corrup-
tion vulnerabilities and crashes in the tested applications. In
particular, ﬁve of them were crashes due to invalid memory
accesses (four cases in wolfSSL and one in GnuTLS), one
was a memory leak in GnuTLS and two were use-after-free
bugs in ClamAV. As NEZHA’s primary goal is to ﬁnd semantic
bugs (not memory corruption issues), we do not describe them
in detail here. Interested readers can ﬁnd further details in
Section XI-A of the Appendix.
Q2: How does NEZHA perform compared to domain-speciﬁc
differential testing frameworks like Frankencerts and Mucerts?
One may argue that being domain-independent, NEZHA
may not be as efﬁcient as successful domain-speciﬁc frame-
works. To address this concern, we compared NEZHA against
Frankencerts [24], a popular black-box unguided differential
testing framework for SSL/TLS certiﬁcate validation, as well
as Mucerts [32], which builds on top of Frankencerts per-
forming Markov Chain Monte Carlo (MCMC) sampling to
diversify certiﬁcates using coverage information. Frankencerts
generates mutated certiﬁcates by randomly combining X.509
certiﬁcate ﬁelds that are decomposed from a corpus of seed
certiﬁcates. Despite its unguided nature, Frankencerts suc-
cessfully uncovered a multitude of bugs in various SSL/TLS
libraries. Mucerts adapt many of Frankencerts core compo-
nents but also stochastically optimize the certiﬁcate generation
process based on the coverage each input achieves in a
single application (OpenSSL). Once the certiﬁcates have been
generated from this single program, they are used as inputs to
differentially test all SSL/TLS libraries.
To make a fair comparison between NEZHA, Frankencerts,
and Mucerts, we ensure that all tools are given the same sets
of input seeds. Furthermore, since Frankencerts is a black-
box tool, we restrict NEZHA to only use its black-box output
δ-diversity guidance, across all experiments.
Since the input generation is stochastic in nature due to the
random mutations, we perform our experiments with multiple
runs to obtain statistically sound results. In particular, for each
of the input groups of certiﬁcates we created (100 groups
of 1000 certiﬁcates each), we generate 100, 000 certiﬁcate
chains using Frankencerts, resulting in a total of 10 million
Frankencerts-generated chains. Likewise, passing as input each
of the above 100 corpuses, we run NEZHA for 100, 000
generations (resulting in 10 million NEZHA-executed inputs).
Mucerts also start from the same sets of inputs and execute in
mode 2, which according to [32] yields the most discrepancies
with highest precision. We use the return value tuples of
the respective programs to identify unique discrepancies (i.e.,
unique tuples of return values seen during testing).
We present the relative number and distribution of dis-
crepancies found across Frankencerts, Mucerts and NEZHA
in Figures 4 and 5. Overall, NEZHA reported 521 unique
discrepancies, compared to 10 and 19 distinct discrepancies
for Frankencerts and Mucerts respectively. NEZHA reports 52
times and 27 times more discrepancies than Frankencerts and
Mucerts respectively, starting from the same sets of initial
seeds and running for the same number of iterations, achieving
a respective coverage increase of 15.22% and 33.48%.
623
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:44 UTC from IEEE Xplore.  Restrictions apply. 
Distributions of Discrepancies Found
Frankencerts
Mucerts
3
   7
510
4
15
NEZHA (Black-box)
Fig. 5: Unique discrepancies observed by Frankencerts,
Mucerts and NEZHA (black-box). The results are averages
of 100 runs each starting with a different seed corpus of
1000 certiﬁcates.
Q3: How does NEZHA perform compared to state-of-the
art coverage-guided domain-independent fuzzers like AFL/lib-
fuzzer?
Fig. 6: Probability of ﬁnding at least n unique discrepancies
after 100, 000 executions, starting from a corpus of 1000
certiﬁcates. The results are averages of 100 runs each
starting from a different seed corpus of 1000 certiﬁcates.
None of the state-of-the-art domain-agnostic fuzzers like
AFL natively support differential testing. However, they can
be adapted for differential testing by using them to generate
inputs with a single test application and then invoking the
full set of tested applications with the generated inputs. To
differentially test our suite of six SSL/TLS libraries, we
ﬁrst generate certiﬁcates using a coverage-guided fuzzer on
OpenSSL, and then pass these certiﬁcates to the rest of the
SSL libraries, similar to how differential testing is performed
by Mucerts. The discrepancies reported across all tested SSL
libraries, if we run AFL (v. 2.35b)3 and libFuzzer on a stan-
dalone program (OpenSSL) are reported in Figure 6. We notice
3Since version 2.33b, AFL implements the explore schedule as presented
in AFLFast [23], thus we omit comparison with the latter.
Fig. 4: Probability of ﬁnding at least n unique discrepancies
starting from the same seed corpus of 1000 certiﬁcates and
running 100, 000 iterations. The results are averages of 100
runs each starting with a different seed corpus.
We observe that, while both Frankencerts and Mucerts
reported a much smaller number of discrepancies than NEZHA,
they found 3 and 15 discrepancies respectively that were
missed by NEZHA. We posit that this is due to the differences
in their respective mutation engines. Frankencerts and Mucerts
start from a corpus of certiﬁcates, break all the certiﬁcates
in the corpus into the appropriate ﬁelds (extensions, dates,
issuer etc.), then randomly sample and mutate those ﬁelds to
merge them back together in new chains, however respecting
the semantics of each ﬁeld (for instance, Frankencerts might
mutate and merge the extensions of two or three certiﬁcates to
form the extensions ﬁeld of a new chain but will not substitute
a date ﬁeld with an extension ﬁeld). On the contrary, NEZHA
performs its mutations sequentially, without mixing together
different components of the certiﬁcates in the seed corpus, as
it does not have any knowledge of the input format.
It is noteworthy that, despite the fact that NEZHA’s mutation
operators are domain-independent, NEZHA’s guidance mech-
anism allows it to favor inputs that are mostly syntactically
correct. Compared to Frankencerts or Mucerts that mutate cer-
tiﬁcates at the granularity of X.509 certiﬁcate ﬁelds, without
violating the core structure of a certiﬁcate, NEZHA still yields
more bugs. Finally, when running NEZHA’s mutation engine
without any guidance, on the same inputs, we observe that
no discrepancies were found. Therefore, NEZHA’s efﬁcacy in
ﬁnding discrepancies can only be attributed to its black-box
δ-diversity-based guidance.
Result 2: NEZHA reports 52 times and 27 times more
discrepancies than Frankencerts and Mucerts respectively,
per input. In terms of testing performance, NEZHA an-
alyzes more than 400 certiﬁcates per second, compared
to 271 and 0.08 certiﬁcates per second for Frankencerts
and Mucerts respectively.
624
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:44 UTC from IEEE Xplore.  Restrictions apply. 
y
t
i
l
i
b
a
b
o
r
P
1.0
0.8
0.6
0.4
0.2
0.0
0
Global coverage
(modiﬁed libFuzzer)
Path δ-diversity (coarse)
Path δ-diversity (ﬁne)
Output δ-diversity
s
e
c
n
e
r
e
f
f
i
D
f
o
r
e
b
m
u
N
50
100
150
200
250
Number of unique discrepancies
100
80
60
40
20
0
0
Global coverage
(modiﬁed libFuzzer)
Path δ-diversity (coarse)
Path δ-diversity (ﬁne)
Output δ-diversity
20000
40000
60000
80000
100000
Generation
Fig. 7: Probability of ﬁnding at least n unique discrepancies
for each of NEZHA’s δ-diversity engines after 100, 000
executions. The results are averages of 100 runs each
starting from a different seed corpus of 1000 certiﬁcates.
Fig. 8: Unique discrepancies observed for each of NEZHA’s
δ-diversity engines per generation. The results are averages