provides for fast, well-aligned access to the data.
4.3 Representing Non-Boolean Values
We have described the construction for two disjoint subsets
(two possible values). For V > 2 different subsets, a trivial
extension is to look for one hash function that outputs the
right value in {1, . . . ,V} for each key. However, this approach
is not practical because it must try O(V n) hash functions on
average. Even when n = 16 and V = 4, in the worst case, it
could be 65536 times slower than V = 2.
We instead search for log2V hash functions, where the j-th
hash function is responsible for generating the j-th bit of the
ﬁnal mapping value. As an example, assume we want to
construct a mapping to a value in {0,1,2,3} from a given set
of size 2. If the ﬁnal mapping is (“foo”, 012) and (“bar”, 102),
we look for two hash functions so that the ﬁrst hash function
maps “foo” to 0 and “bar” to 1, and the second hash function
hashes “foo” to 1 and “bar” to 0. The expected total number
of iterations to construct a ﬁnal mapping is then log2V · 2n,
which scales linearly with the number of bits to represent a
value. Figure 4 compares the number of iterations needed
to build a separation of 4 subsets by searching for one hash
function mapping to {0,1,2,3} or two hash functions mapping
to {0,1} respectively. Splitting the value bits is orders of
magnitude faster.
4.4 Scaling to Billions of Items
The basic idea of efﬁciently scaling SetSep to store mappings
for millions or billions of keys, as noted above, is to ﬁrst
partition the entire set into many small groups, here of roughly
16 keys each. Then, for each group of keys, we ﬁnd and store
a hash function that generates the correct values using the
techniques described above. Therefore, two properties are
critical for the scheme that maps a key to a group:
• The mapping must ensure low variance in group size.
Although a small load imbalance is acceptable, even
slightly larger groups require much longer to ﬁnd a suit-
246(a) Avg. # of iters. to get one hash func.
(b) Space cost breakdown
Figure 3: Space vs. time, as the function of bit array size m
Figure 4: One hash func. vs. multiple hash func.
sixteen—using simple direct hashing. These buckets will
have the aforementioned huge load variance. To address this
problem, at the second level, we assign buckets to groups
with the aim of minimizing the maximum load on any group.
The storage cost of this scheme, therefore, is the bits required
to store the group choice for each bucket.
Figure 5 shows this process. Each ﬁrst-level bucket has an
average size of 4 keys but the variance is high: some buckets
could be empty, while some may contain ten or more keys.
However, across a longer range of small buckets, the average
number of stored keys has less variance. For 256 buckets,
there are 1024 keys on average. We therefore take consecutive
blocks of 256 buckets and call them a 1024-key-block. We
then map these blocks to 64 groups of average size 16.
Within the block of 256 buckets, each bucket is mapped
to one of four different “candidate” groups. We pre-assigned
candidate groups for each bucket in a way that each group
has the same number of associated buckets. These choices
are denoted by the arrow from bucket to groups in Figure 5.
All keys in the small bucket will map to one of these four
candidate groups. Therefore, the only information that SetSep
needs to store is the bucket-to-group mapping (a number in
{0,1,2,3} indicating which candidate group was chosen).
The effectiveness of this bucket-to-group mapping is impor-
tant to the performance of SetSep, since as we have explained
more balanced groups make it easier to ﬁnd suitable hash
functions for all groups. Ideally, we would like to assign
the same number of keys to each group. However, ﬁnding
such an assignment corresponds to an NP-hard variant of the
knapsack problem. Therefore, we use a greedy algorithm to
balance keys across groups. We ﬁrst sort all the buckets in
descending order by size. Starting from the largest bucket, we
assign each bucket to one of the candidate groups. For each
bucket, we pick the candidate group with the fewest keys. If
more than one group has the same least number of keys, a
random group from this set is picked. We repeat this process
until all the buckets have been assigned, yielding a valid as-
signment. In fact, we run this randomized algorithm several
times per block and choose the best assignment among the
runs. To lookup a key x, we ﬁrst calculate the key’s bucket
ID by hashing. Then, given this bucket ID, we look up the
Figure 5: Illustration of two-level hashing
able hash function using brute-force search because the
time grows exponentially in the group size.
• The mapping should add little space. The per-group
SetSep itself is only a few bits per key, and the partition-
ing scheme should preserve the overall space efﬁciency.
Conventional Solutions That Do Not Work Well To cal-
culate the group ID of a given key, one obvious way is to
compute a hash of this key modulo the total number of
groups. This approach is simple to implement, and does
not require storing any additional information; unfortunately,
some groups will be signiﬁcantly more loaded than the aver-
age group even with a strong hash function [29]. Our exper-
iments show that when 16 million keys are partitioned into
1 million groups using even a cryptographic hash function,
the most loaded group typically contains more than 40 keys
vs. the average group size of 16 keys; this matches the corre-
sponding theory. Finding hash functions via brute force for
such large groups is impractical.
An alternative solution is to sort all keys and assign every
n consecutive keys to one group. This approach ensures that
every group has exactly sixteen keys. Unfortunately, it has
several serious limitations: (1) it requires storing the full keys,
or at least key fragments on the boundary of each group, as
an index; (2) it requires a binary search on lookup to locate a
given key’s group; and (3) update is expensive.
Our Solution: Two-Level Hashing. SetSep uses a novel
two-level hashing scheme that nearly uniformly distributes
billions of keys across groups, at a constant storage cost of
0.5 bits per key. The ﬁrst level maps keys to buckets with
a small average size—smaller than our target group size of
 1 10 100 1000 10000 100000 0 5 10 15 20 25 30iterationsm: bit array sizeavg. # of iterations 0 0.5 1 1.5 2 2.551015202530510152025303540bits per keysbits per 16 keysm: bit array sizebits for indexbits for array10102103104105106107108109 0 5 10 15 20 25 30iterationsm: bit array size1 hash func to 2-bit value2 hash func to 1-bit valuekey x01232012311024-key-block256 buckets (avg. size 4)64 groups (avg. size 16)247stored choice number to calculate which group this key be-
longs to. Each bucket has 4 keys on average, and spends 2
bits to encode its choice. So on average, two-level hashing
costs 0.5 bits per key, but provides much better load balance
than direct hashing. When partitioning 16 million keys into
1 million groups, the most loaded group usually has 21 keys,
compared to more than 40 for direct hashing.
4.5 Scalable Update
Allowing lookups without storing keys is the primary reason
SetSep is so compact. The original construction and updates,
however, require the full key/value pairs to recompute SetSep.
In ScaleBricks, this information comprises the RIB, where
keys are destination addresses and values are the correspond-
ing handling nodes.
To provide scalability for the RIB size (e.g., the number
of ﬂows that the EPC can keep track of) and update rate,
ScaleBricks uses a partitioned SetSep construction and update
scheme. The RIB entries are partitioned using a hash of the
key, so that keys in the same 1024-key-block are stored in the
same node. For construction, each node computes only its
portion of SetSep, and then exchanges the produced result
with all the other nodes. When updating a key k, only the node
responsible for k recomputes the group that k belongs to, and
then broadcasts the result to other nodes. Because applying
a delta-update on the other nodes requires only a memory
copy (the delta is usually tens of bits), this approach allows
ScaleBricks to scale the update rate with the number of nodes.
To allow high-performance reads with safe in-place updates,
techniques analogous to those proposed in CuckooSwitch [34]
and MemC3 [14] could be applied, although we have not
designed such a mechanism yet.
5.
IMPLEMENTATION / OPTIMIZATIONS
ScaleBricks is implemented in C. It uses Intel’s Data Plane
Development Kit (DPDK) for x86 platforms [21] as a fast
user-space packet I/O engine.
5.1 Global Partition Table using SetSep
SetSep uses several optimizations to improve performance.
Efﬁcient Use of Memory Bandwidth and CPU Cycles Se-
quentially issuing memory fetches one at a time cannot satu-
rate the bandwidth between CPU and memory. ScaleBricks
instead uses batched lookups and prefetching [27, 34] (Algo-
rithm 1). Each lookup request is divided into three stages and
a subsequent stage accesses a memory location determined
by its previous stage. Immediately fetching these memory
locations would stall CPU pipelines for many cycles while
waiting for the load instruction to complete. Instead, our algo-
rithm ﬁrst issues a prefetch instruction for a set of addresses,
which causes the CPU to start loading the data from these
addresses into cache. Then, at the beginning of the next stage,
Algorithm 1: Batched SetSep lookup with prefetching
BatchedLookup(keys[1..n])
begin
for i ← 1 to n do
for i ← 1 to n do
bucketID[i] ← keys[i]’s bucket ID
prefetch(bucketIDToGroupID[bucketID[i]])
groupID[i] ← bucketIDToGroupID[bucketID[i]]
prefetch(groupInfoArray[groupID[i]])
for i ← 1 to n do
groupInfo ← groupInfoArray[groupID[i]]
values[i] ← LookupSingleKey(groupInfo, keys[i])
return values[1..n]
it executes normal reads at those prefetched addresses. These
loads are then likely to hit in L1/L2 cache and thus complete
much faster. Prefetching signiﬁcantly improves efﬁciency:
Section 6 shows that with appropriate batch sizes, this op-
timization improves microbenchmark lookup throughput of
SetSep by up to 1.8×.
Hardware Accelerated Construction Constructing SetSep
is amenable to parallelization: Each group of keys can be
computed independently, and within a group, the computation
of the hash function across the sixteen keys can be parallelized
using SIMD. In this work, we only explore using multiple
hardware threads across groups, which provides sufﬁciently
fast construction for our application. We plan to evaluate the
SIMD and GPU-derived speedups in future work.
5.2 Partial FIB using Cuckoo Hashing
Each node in the cluster has a slice of the FIB to provide an ex-
act mapping from keys to application-speciﬁc data. In Scale-
Bricks, this table is implemented using concurrent cuckoo
hashing [34], which achieves high occupancy and line-rate
lookup performance for read-intensive workloads.
CuckooSwitch [34] used a concurrent cuckoo hash table to
build a FIB that maps MAC addresses to output ports. That
prior work was optimized to fetch the entire key-value pair in
a single cache line read and thus stored values adjacent to their
keys. Our target application of ScaleBricks, however, requires
storing arbitrarily large application-speciﬁc data about each
key (instead of a single output port as in CuckooSwitch). We
therefore apply the following optimization.
When the table is initialized at run-time, the value size is
ﬁxed for all entries based on the application requirements.
We assign each slot in the cuckoo hash table a logical “slot
number.” Instead of storing key/value pairs in an interleaved
form, we create a separate value array in which the k-th
element is the value associated with the k-th slot in the hash
table. To lookup the value of a key, we simply index into
the value array at the position corresponding to the key’s slot
number in the hash table. When moving a key from one slot
to another during insertion, we need to move the value as well.
248The apparent drawback of this approach is an extra memory
read during lookup. In practice, however, as we will show in
the evaluation, this extra memory read has minimal impact
on the lookup throughput.
6. EVALUATION
Our evaluation addresses three questions:
1. How fast can we construct and lookup with SetSep?
2. How does moving to ScaleBricks improve the through-
put of the Packet Forwarding Engine of the LTE Evolved
Packet Core? What limitations does it add?
3. How does ScaleBricks scale with the number of servers?
We omit the evaluation of the cuckoo hashing-based FIB;
prior work has demonstrated that the table is fast enough to
serve over 300 million lookups per second [34].
6.1 Micro-Benchmark: SetSep
This section presents micro-benchmark results for SetSep
construction and lookup performance on modern hardware.
These micro-benchmarks are conducted on a moderately
fast dual-socket server with two Intel Xeon E5-2680 CPUs
(HT disabled), each with a 20 MiB L3 cache. The machine
has 64 GiB of DDR3 RAM.
6.1.1 Construction
The construction speed of SetSep depends primarily on three
parameters:
• The number of bits to store the hash index and to store
the bit-array in each group;
• The number of possible values or sets; and
• The number of threads used to parallelize construction.
The ﬁrst experiments measure the construction rate of
SetSep with different parameter combinations. The per-thread
construction rate (or throughput) is nearly constant; construc-
tion time increases linearly with the number of keys and
decreases linearly with the number of concurrent threads.
Table 1 shows results for 64 M keys.
The ﬁrst group of results shows a modest tradeoff between
(single-threaded) construction speed and memory efﬁciency:
Using a “16+8” SetSep (where 16 bits are allocated to the
hash function index and 8 bits to the bit array) has the slowest
construction speed but almost never needs to use the fallback
table, which improves both query performance and memory
efﬁciency. “16+16” SetSep also has low fallback ratio, but
consumes more space. We therefore use 16+8 for the remain-
ing experiments in this paper. Its speed, 1
2 million keys per
second per core, is adequate for the read-intensive workloads
we target.
Increasing the value size imposes little construction over-
head. The results in practice are even better than linear scaling
because we optimized our implementation as follows: as we
iterate the hash function, we test the function for each value
bit across the different keys in the group before moving on
to the next hash function in the hash function family. As a
result, we perform less work than searching hash functions