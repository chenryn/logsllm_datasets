the set will be subjected to monitoring or unavail-
able from either failure or attack. As already noted,
entry guard nodes may be preferred or strictly re-
quired. As a reﬁnement, there may be a succes-
sively preferred set of entry guards. There may be
a single layer of entry guard nodes, i.e., nodes cho-
sen to be immediately adjacent to HS, or they may
be layered, i.e., some guard nodes are chosen to
be used for the ﬁrst hop, some for the second, and
possibly further. Finally, they may be chosen at
11It is also possible to set an exit node preference in the URL
for speciﬁc HTTP requests [27].
random or chosen based on trust or performance.
Each of these choices is orthogonal, so each of
the combinations of choice will lead to systems
with different properties. For space, we will limit
discussion to some of the more salient combina-
tions.
Choosing a small set of entry guard nodes that
are both permanent and strictly required could lead
to a higher percentage of service failures, either
by accident or by design (assuming a very power-
ful adversary, with the capability to DoS all entry
guard nodes). If a permanent set is simply a prefer-
ence, then DoS of all entry guard nodes could lead
back to our attacks if the guard nodes can be kept
down long enough. Of course this assumes that en-
try guards can be identiﬁed by the attacker. We ran
our attacks on a hidden server that had chosen three
entry guard nodes.
Experiment - Attacking Entry Guard Nodes:
Letting the Hidden Service use three permanent,
preferred entry guards we found that these nodes
combined represented all identiﬁed connections
through Alice’s node, as shown in Table 2. A
quite unexpected result, but caused by the imple-
mentation feature in Tor described earlier: we were
never Node 3, only Node 2 (Node 1 being the entry
guard).
As in our previous experiments, identifying the
entry guard nodes through our attacks never took
more than a few hours.
Backup guard nodes: Suppose there is a short
list of entry guard nodes that is preferred (e.g., three
nodes) and a longer list of guard nodes to be used
as backup (e.g., nine) if those are not available. If
Proceedings of the 2006 IEEE Symposium on Security and Privacy (S&P’06) 
1081-6011/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 02:48:40 UTC from IEEE Xplore.  Restrictions apply. 
track of when a node enters the set and prefers to
choose entry nodes for circuits that were in the set
sooner. Nodes are only deleted from the set when
they have been unreachable for an extended period
(currently one month).
Nonrandom choice of entry guard node sets:
To avoid circuit rotation simply from failed entry
guard nodes it might seem that it is best to choose
as guard nodes those that have the best uptime,
and perhaps bandwidth. This is, however, subject
to abuse since adversaries may run highly reliable,
highly performing nodes in order to increase their
chances of being chosen as entry guard nodes. And,
this is especially easy to abuse in the current Tor
directory statistics in which nodes report their own
performance. This is a speciﬁc instance of a more
general problem in trying to build reliable anony-
mous communication. One possible solution is to
order node performance and reliability but then to
choose from a large enough set in this order that
the adversary is unlikely to be able to substantially
alter the chances of being chosen as an entry guard
node. Dingledine and Syverson described this strat-
egy to form a reliable anonymous communication
network of mix cascades [14].
Another possibility is to choose entry guard
nodes based on trust in the node administrator. It
is difﬁcult to attach probabilities to Alice’s being
trusted by the Hidden Server administrator, or per-
haps more likely, to compromise a node run by
someone trusted by the Hidden Server administra-
tor. (Trust in honesty should not be confused with
trust in competence.) Perhaps a greater concern
is that common properties of the administrators of
chosen entry guard nodes (e.g., they are all family
relatives) may lead an adversary to form a hypoth-
esis of who is running HS, which may then lead to
attacks unrelated to use of the network per se. Here
the layering approach described above may prove
useful. If the layer 1 nodes are personally trusted,
and the layer 2 nodes are chosen as random sets,
then it becomes more difﬁcult for an adversary to
discover the set of entry guard nodes and thus to
correlate external properties.
Figure 6. Use of Layered Entry Guard
Nodes
the adversary has the capability of keeping say four
nodes at a time ofﬂine, then it can cause HS to use
other nodes in the Node 1 position than those on the
short list. But, all that will accomplish is causing
HS to rotate to three new nodes from the longer list
as primary guards. Alice can cause rotation of cir-
cuits but only through a still relatively small set of
entry guard nodes, and this only through sustained
attacking. We can however make it more difﬁcult
for Alice to ﬁnd the entry guard nodes at all via our
attacks.
Layering guard nodes: Suppose, e.g., that HS
has a set of three entry guard nodes from which to
choose Node 1, and for each of these has a set of
three guard nodes from which to choose Node 2,
as illustrated in Fig. 6. As before, Alice can only
successfully ﬁnd HS if she owns one of these three
layer 1 entry guard nodes. But if she does not, in
order to even identify one of those layer 1 nodes to
attack she must own one of the three layer 2 guard
nodes associated with that node.
Layering guard nodes would require much more
substantial changes to the Tor code than the exper-
iments we have already run, albeit probably fairly
straightforward changes. We have thus not had a
chance to conduct experiments on either layering
or backup guard node conﬁgurations. However, a
version of backup guard nodes has recently been
implemented in Tor in response to our results. At
the time of writing, by default each client running
the latest Tor code chooses three nodes as initial
preferred entry guards. When the available entry
guard set shrinks below two nodes, two more nodes
are added to the set. However, the software keeps
Proceedings of the 2006 IEEE Symposium on Security and Privacy (S&P’06) 
1081-6011/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 02:48:40 UTC from IEEE Xplore.  Restrictions apply. 
6 Conclusion
Our results show that Tor’s location-hidden
servers are not really hidden—or rather they were
not really hidden prior to the recent introduction
of guard nodes as countermeasures to our attacks.
Using our attacks, all an attacker needed was one
compromised node in the network and the “Hidden
Server” was identiﬁed.
We have demonstrated that an attack with one
compromised node in the anonymity network takes
only minutes if the service is located at a client, or a
couple of hours when located on a server node. By
using two nodes in the network it only takes min-
utes to ﬁnd the Hidden Server regardless of where
it is located.
We have also argued that neither dummy traf-
ﬁc nor extending the path length from the Hidden
Server to the Rendezvous Point will protect against
all of our attacks. However, requiring hidden ser-
vices to always use entry guard nodes, which are
currently available as a general option in the Tor
code, greatly reduces the probability of successful
attacks against a hidden service.
Using random entry guard nodes may still leave
the Hidden Server vulnerable to our attacks if the
attacker is powerful enough to completely deny ser-
vice to a small sets of nodes or to compromise them
by physical or other means. But, using backup
guard nodes and/or layering guard nodes will sig-
niﬁcantly slow down even such an attacker.
Using random selection of backup and layering
entry guard nodes will be an improvement, but as in
all Tor circuits, someone connecting through ran-
dom nodes will always be compromised if an at-
tacker owns just two nodes [25]. Using the backup
and layering techniques in combination with a non-
random selection, e.g. based on some kind of trust,
or experience, with the nodes, may slow the attack
even more or may even prevent it entirely.
We have demonstrated attacks that surprisingly
require just one or two hostile nodes. What is pos-
sible by an adversary that controls several nodes,
or even, e.g., two percent of the network? We will
investigate this in future work. Other future work
includes testing implemented countermeasures for
vulnerabilities using the described attack scenarios,
as well as new ones. We will also be investigat-
ing improved performance by shrinking the path
length between the Client and the Hidden Server.
We speculate that it may be possible to do so with
adequate security when using our suggested coun-
termeasures, and possibly others. We will also turn
our attack on its head: testing to locate a client by
having an attractive hidden service.
7 Acknowledgements
Thanks to Tor developers Roger Dingledine and
Nick Mathewson for talks on the functionality of
Tor. Thanks to the reviewers for their constructive
comments on how to improve and emphasize the
important points of the paper. This work supported
by ONR.
References
[1] A. Acquisti, R. Dingledine, and P. Syverson. On
the economics of anonymity.
In R. N. Wright,
editor, Financial Cryptography, 7th International
Conference, FC 2003, pages 84–102. Springer-
Verlag, LNCS 2742, 2003.
[2] R. J. Anderson. The eternity service. In Proceed-
ings of Pragocrypt ’96, 1996.
[3] Bibliography of anonymous communication litera-
ture. http://freehaven.net/anonbib/.
[4] Anonymizer. http://www.anonymizer.com/.
[5] A. Back, U. M¨oller, and A. Stiglic. Trafﬁc anal-
ysis attacks and trade-offs in anonymity provid-
ing systems. In I. S. Moskowitz, editor, Informa-
tion Hiding (IH 2001), pages 245–257. Springer-
Verlag, LNCS 2137, 2001.
[6] O. Berthold, H. Federrath, and S. K¨opsell. Web
MIXes: A system for anonymous and unob-
servable Internet access.
In H. Federrath, ed-
itor, Proceedings of Designing Privacy Enhanc-
ing Technologies: Workshop on Design Issues in
Anonymity and Unobservability, pages 115–129.
Springer-Verlag, LNCS 2009, July 2000.
[7] O. Berthold and H. Langos. Dummy trafﬁc against
long term intersection attacks.
In R. Dingledine
and P. Syverson, editors, Proceedings of Privacy
Enhancing Technologies workshop (PET 2002).
Springer-Verlag, LNCS 2482, April 2002.
Proceedings of the 2006 IEEE Symposium on Security and Privacy (S&P’06) 
1081-6011/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 02:48:40 UTC from IEEE Xplore.  Restrictions apply. 
[8] P. Boucher, A. Shostack, and I. Goldberg. Free-
dom system 2.0 architecture. White paper, Zero
Knowledge Systems, Inc., December 2000.
[9] D. Chaum. Untraceable electronic mail, return ad-
dresses, and digital pseudonyms. Communications
of the ACM, 4(2), February 1981.
[10] W. Dai. Pipenet 1.1. Usenet post, August 1996.
[11] R. Dingledine, M. J. Freedman, and D. Molnar.
The Free Haven Project: Distributed anonymous
storage service.
In H. Federrath, editor, Design-
ing Privacy Enhancing Technologies: Workshop
on Design Issues in Anonymity and Unobservabil-
ity. Springer-Verlag, LNCS 2009, July 2000.
Challenges
[12] R. Dingledine, N. Mathewson, and P. Syver-
son.
in deploying low-latency
anonymity (DRAFT). Unpublished Manuscript.
http://tor.eff.org/cvs/tor/doc/design-paper/
challenges.pdf.
[13] R. Dingledine, N. Mathewson, and P. Syverson.
Tor: The second-generation onion router. In Pro-
ceedings of the 13th USENIX Security Symposium,
August 2004.
[14] R. Dingledine and P. Syverson. Reliable MIX Cas-
cade Networks through Reputation. In M. Blaze,
editor, Financial Cryptography, 6th International
Conference, FC2002, pages 253–268. Springer-
Verlag, LNCS 2357, 2002.
[15] I. Goldberg. A Pseudonymous Communications
Infrastructure for the Internet. PhD thesis, UC
Berkeley, December 2000.
[16] D. M. Goldschlag, M. G. Reed, and P. F. Syver-
son. Hiding Routing Information. In R. Anderson,
editor, Proceedings of Information Hiding: First
International Workshop, pages 137–150. Springer-
Verlag, LNCS 1174, May 1996.
[17] B. N. Levine, M. K. Reiter, C. Wang, and
M. Wright. Timing attacks in low-latency mix-
based systems (extended abstract).
In A. Juels,
editor, Financial Cryptography, 8th International
Conference, FC 2004, pages 251–265. Springer-
Verlag, LNCS 3110, 2004.
[18] S. J. Murdoch and G. Danezis. Low-cost traf-
ﬁc analysis of Tor.
In Proceedings of the 2005
IEEE Symposium on Security and Privacy. IEEE
CS Press, May 2005.
[19] R. M. Needham. Denial of service. In CCS ’93:
Proceedings of the 1st ACM Conference on Com-
puter and Communications Security, pages 151–
153, New York, NY, USA, 1993. ACM Press.
[20] R. M. Needham. Denial of service: an example.
Communications of the ACM, 37(11):42–46, 1994.
[21] L. Øverlier, T. Brekne, and A.
˚Arnes. Non-
expanding Transaction Speciﬁc Pseudonymization
for IP Trafﬁc Monitoring.
In Cryptology and
Network Security: 4th International Conference
(CANS 2005), pages 261–273. Springer-Verlag,
LNCS 3810, December 2005.
[22] A. Pﬁtzmann, B. Pﬁtzmann, and M. Waidner.
ISDN-MIXes: Untraceable communication with
very small bandwidth overhead. In Proceedings of
the GI/ITG Conference on Communication in Dis-
tributed Systems, pages 451–463, February 1991.
[23] M. Reiter and A. Rubin. Crowds: Anonymity for
web transactions. ACM Transactions on Informa-
tion and System Security, 1(1), June 1998.
[24] A. Serjantov and P. Sewell. Passive attack anal-
ysis for connection-based anonymity systems. In
E. Snekkenes and D. Gollmann, editors, Com-
puter Security – ESORICS 2003, pages 116 – 131.
Springer-Verlag, LNCS 2808, October 2003.
[25] P. Syverson, G. Tsudik, M. Reed,
and
Towards an Analysis of Onion
C. Landwehr.
Routing Security. In H. Federrath, editor, Design-
ing Privacy Enhancing Technologies: Workshop
on Design Issue in Anonymity and Unobservabil-
ity, pages 96–114. Springer-Verlag, LNCS 2009,
July 2000.
[26] Tor Manual. http://tor.eff.org/tor-manual.html.en.
[27] Tor wiki. http://wiki.noreply.org/noreply/
TheOnionRouter/TorFAQ.
[28] M. Waldman and D. Mazi`eres.
Tangler:
a
censorship-resistant publishing system based on
document entanglements.
In Proceedings of the
8th ACM Conference on Computer and Commu-
nications Security (CCS 2001), pages 126–135,
November 2001.
[29] M. Wright, M. Adler, B. N. Levine, and C. Shields.
An analysis of the degradation of anonymous pro-
tocols.
In Proceedings of the Network and Dis-
tributed Security Symposium - NDSS ’02. IEEE CS
Press, February 2002.
[30] M. Wright, M. Adler, B. N. Levine, and C. Shields.
Defending anonymous communication against
passive logging attacks. In IEEE Symposium on Se-
curity and Privacy, pages 28–41. IEEE CS Press,
May 2003.
[31] M. K. Wright, M. Adler, B. N. Levine, and
C. Shields. The predecessor attack: An analy-
sis of a threat to anonymous communications sys-
tems. ACM Trans. Inf. Syst. Secur., 7(4):489–522,
2004. A preliminary version of this paper appeared
in [29].
Proceedings of the 2006 IEEE Symposium on Security and Privacy (S&P’06) 
1081-6011/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 02:48:40 UTC from IEEE Xplore.  Restrictions apply.