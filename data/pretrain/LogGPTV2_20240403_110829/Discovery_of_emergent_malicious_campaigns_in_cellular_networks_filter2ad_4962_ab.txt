tem is running live to generate daily clusters for the most
recent one week period as the test window.
3.2 Data Sets
Our raw data consists of SMS messages, and IP data be-
longing to a large US cellular carrier. The system only
utilizes who-talks-to-whom information for both SMS and
IP data, no content data is used. All phone numbers are
anonymized to protect privacy. Any phone numbers shown
in this paper are anonymized to 64 bit tokens. Any results
in this paper involving traﬃc volume are scaled randomly to
prevent revealing true traﬃc volumes of any kind, but still
maintain patterns involving relative changes in volume.
Each record in SMS data is simply a time-stamped tu-
ple indicating the source and destination phone number. A
phone number can be either a 10-digit numbers or a short-
code (used for SMS based services such as checking TV vot-
ing, movie times, etc.). In each record, either the source or
the destination (or both) must be subscribers of the carrier.
Due to volume constraints, only a geographically based sam-
ple is used. IP data, using the same geographically based
sample, consists of timestamped tuples indicating the 10-
digit phone number and the 2nd level domain name visited.
In order to focus on data-enabled phones, we use SMS data
only for users that have at least one record in IP data. In an
average one week period, our sample provides about ∼ 150
Million communication edges, involving ∼ 40 Million unique
entities, of which ∼ 10 Million are 10-digit phone numbers.
3.3 Training Phase
In order to pick out anomalous traﬃc patterns, our dataset
is divided into two parts by time: a training phase, 1 month
in duration, and a test phase, which is a sliding window
1GSM Association www.gsma.com/technicalprojects/gsma-
spam-reporting-services
of duration one week, that appears chronologically after the
training phase. The purpose of the training phase is to build
a notion of what constitutes normal traﬃc patterns.
3.3.1 Why not edge-based training?
Here, we model the appearance of edges between nodes
in the communication graph.
If an edge is present in the
training set, then it is considered normal, and is therefore
dropped if present in the test set. While this approach
seems undoubtedly important for modeling normal user in-
teractions, unfortunately, it is incapable of identifying high-
degree entities that are already popular in the training phase.
For example, since entities like cnn.com and yahoo.com al-
ready have high degree in the training set, we would not
want to retain them in the test set. Unfortunately, there are
a large number of users that have edges to cnn.com in the
test set but not in the training set. That is, there are always
new users communicating with such entities. We empirically
found this pattern to hold for a large number of entities that
already have high degree in the training set. Therefore the
edge-based training strategy would end up selecting entities
that should not be selected, thereby adding a lot of noise to
our mutual contacts graph. As an alternative, we consider
node-based training below.
3.3.2 Node based training
Since we are interested in high-degree nodes, in practice
we employ a training method that focuses directly on identi-
fying high-degree nodes, rather than edges. Entities that are
high-degree in the training phase can be considered to be al-
ready popular, and can therefore be discarded from the test
dataset along with all their edges. The task of testing then
simpliﬁes to one of ﬁnding entities with high degree in the re-
maining communication graph. In practice there is another
important issue to consider: shortcodes and phone numbers
can change ownership over a period of time, while domain
names typically do not. This implies that we should be less
aggressive in whitelisting phone numbers and short codes.
Further, as domain names are rarely reused and malicious
domains once blacklisted become less useful to attackers, we
are able to train purely on whether a domain is seen previ-
ously in this initial prototype. With this in mind, in the end
we perform the following training steps:
1) Domain names: All domains that appear in the
training window are dropped from test set.
2) 10-digit numbers and shortcodes: Only a select
known benign services (gateways, etc.) is whitelisted. All
other numbers are kept.
While the latter does not remove benign high degree en-
tities, we do not expect high degree benign entities to form
clusters of signiﬁcant size with other entities. Therefore,
keeping almost all numbers allows us to retain both types of
high degree nodes, benign and malicious, at the expense of
deferring the detection to the clustering stage. In the future,
as attackers adapt to poison these training methods, we will
explore additional more sophisticated training approaches.
3.4 Testing Phase
During the testing phase, the entities identiﬁed in the
training phase as normal are dropped from the test dataset,
and the remaining communication graph is used to compute
the mutual contacts graph. As introduced in Section 1, the
mutual contacts graph is a graph in which nodes represent
32
abnormal high-degree entities in the testing phase, and two
nodes are connected by an edge if there is signiﬁcant over-
lap between the sets of users that communicate with the two
entities. The intuition behind building the mutual contacts
graph is that the users that end up connecting to a later
part of an attack campaign such as a fraudulent premium
number or botnet command and control will also have con-
nected to the same prior part of the attack campaign like
receiving a spam SMS message. In this way we can link mul-
tiple stages of the same campaign in a single cluster, such as
the number sending spam and premium numbers involved
in the conversion step.
To build the mutual contacts graph, the relationships be-
tween these anomalous high degree nodes are analyzed. Specif-
ically, we compute the Dice association coeﬃcient [24] be-
tween every pair of high-degree nodes. The Dice association
coeﬃcient D(a, b) between two nodes a and b is deﬁned as:
D(a, b) =
|A ∩ B|
min{|A|,|B|}
where A and B represent the set of users that communi-
cate with nodes a and b respectively, and | · | represents the
cardinality of a set.
Complexity and Scalability To identify mutual con-
tacts between anomalous nodes, we need to maintain a full
set of entities that each node communicates with. This
poses scaling issues. The size of memory needed is at worst
quadratic in the number of nodes (N), but in practice is lin-
ear as nodes average few unique contacts. Computing the
number of mutual contacts between nodes requires a set in-
tersection computation between every pair of nodes, with
runtime of O(N 2). To mitigate with this issue, we only con-
sider the K highest degree nodes in the mutual contacts
graph. To compute the number of mutual contacts between
these K nodes, for each one of N entities, we determine
the subset of the K entities that it communicates with and
increment the number of mutual contacts between Ki and
Kj if both Ki and Kj are in the subset. This reduces the
complexity to O(K 2 ∗ N ), which is O(N ) for N >> K.
In our daily reporting experiments, we use a test window
of duration one week, and we slide this window over time,
one day at a time. We choose K to be in the order of
50, 000, with the lowest degree node selected having a degree
of ∼ 50 on average. Processing around 10 million unique
users and their connections takes less than 40 GB of RAM
in our prototype system and we can generate a daily report
overnight. The system can be easily extended to include
more high degree nodes for correlation if the hardware and
computing time permit.
3.5 Clustering
The clustering phase involves partitioning the mutual con-
tacts graph into clusters, such that each cluster represents a
suspicious group of entities involved in the same malicious
campaign. Edges in the mutual contacts graph are weighted
by the Dice association coeﬃcient as described in Section
3.4. We partition this graph in two stages. First we break
any edges that have a Dice coeﬃcient of less than 0.1, or
have an absolute number of shared users of less than 20.
These thresholds represent the 99th percentile of all edges
in the mutual contacts graph.
Then, we use the work of Blondel et al [7] to perform
graph clustering on the remaining graph. Blondel opti-
mizes a quantity called modularity, where the modularity
of a graph partition is a scalar value between -1 and 1 that
measures the density of links inside clusters as compared to
links between clusters. Thus it ﬁnds high modularity parti-
tions of graphs in order to split clusters that would otherwise
only have a weak link between two highly connected groups
of nodes. The nodes remaining connected to each other after
this process form each of our clusters. Most clusters are of
size 2 or 3 (distribution of cluster sizes discussed in Section
3.6.3). We currently focus on clusters of size > 3, mostly be-
cause small clusters of size 2 and 3 remain active for a very
short period of time (Section 3.6.3). Human analysts can
prioritize analysing big clusters and investigate small clus-
ters when time permits or additional activity proves them
suspicious.
3.6 Post processing
Once we have arrived at suspicious clusters, we are faced
with the ﬁnal task of classifying each cluster as malicious or
benign. The ﬁnal decision is left to a human analyst, but to
make the task as automated as possible, we add additional
contextual information to better inform the analyst.
3.6.1 Temporal trafﬁc pattern.
**01
14%
17%
16%
**10
21%
16%
**11
12%
18%
14%
14%
29%
15%
**07
18%
20%
31%
16%
18%
11%
18%
23%
24%
**06
11%
23%
21%
17%
16%
14%
11%
19%
17%
11%
22%
28%
13%
**12
******ts.mobi
24%
26%
19%
22%
13%
**04
**08
26%
30%
21%
19%
28%
15%
24%
**13
25%
**03
**02
24%
12%
10%
10%
12%
10%
**09
Figure 4: A benign cluster consisting mainly of SMS
shortcodes for voting on a TV show. Figure 5 shows
the regular traﬃc patterns produced by the entities
in this cluster.
The temporal traﬃc pattern of nodes in a cluster can pro-
vide information on whether it is likely to correspond to a
programmed event or show (and therefore a benign cluster).
As an example, consider the cluster we discovered shown in
Figure 4. We found in Figure 5 that the traﬃc belonging to
the entities in this cluster has peaks occurring at a speciﬁc
time once a week. It turns out that this cluster corresponds
to a popular TV show that involves SMS voting and the
peaks occur exactly during the show hours. This pattern by
itself cannot be used to label a cluster as benign for sure
– since a similar pattern might be utilized by a malicious
campaign, such as data exﬁltration only at night time or
periodic botnet command and control – but it can help an
analyst direct her investigation.
3.6.2 Label Known Malicious Nodes
As attackers use domains and premium shortcodes for
many attack attempts, and these components of widespread
attacks are almost always eventually detected, labelling our
clusters using data from existing blacklists can give analysts
33
s
e
d
o
c
t
r
o
h
s
l
l
a
o
t
t
n
e
s
s
S
M
S
f
o
r
e
b
m
u
n
d
e
z
i
l
a
m
r
o
N
15000
10000
5000
With 7 days 
interval
No SMS traffic 
before 
the show 
started this year
0
0
5
10
15
20
Time (in days)
25
30
35
40
45
Figure 5: Number of SMS messages sent to each of
the short codes in Figure 4 over time. There are
periodic traﬃc volume peaks during the show time
each week. Absolute values of traﬃc volumes on
the Y-axis are obscured by scaling uniformly by a
random number.
important clues about the nature of a cluster. Blacklists
complement our approach by providing some partial ground
truth. Some weaknesses of blacklists such as lagging behind
other detection methods and providing no relationship be-
tween malicious nodes, are complemented by our clusters,
which can group a large part of an attack campaign while
quantifying the relationship between abnormal nodes. We
use a variety of blacklists, both of SMS numbers based on
user reports [3] as well as third party domain name black-
lists.
3.6.3 Cluster Size and Change Over Time
The size of a cluster and whether it changes in composi-
tion over time can provide clues into whether it is malicious.
This is because malicious campaigns frequently need to put
new numbers and domain names into use as old ones get
taken down or blacklisted. On the other hand, legitimate
services are more likely to keep the same domains and num-
bers as users become familiar with them. In this section, we
compare the clusters from a given window to clusters from
a previous test window in order to highlight clusters that
evolve over time.
600
500
400
300
200
100
s
r
e
t
s
u
l
c
e
h
t
f
o
r
e
b
m
u
N
0
0
50