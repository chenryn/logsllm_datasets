However, disabling them on the Xeon E3 system did not
significantly change the latency distribution.
6.3 Caching and DDIO
We now examine the impact of caching and DDIO on PCIe
transactions. For these experiments we keep all parameters
constant, except for the window size and the state of the LLC.
The window size was varied from 4KB to 64MB, exceeding
the 15MB or 25MB size of the LLC on all our systems.
Firstly, we look at PCIe transaction latency. To measure
the latency we use the NFP’s PCIe command interface with
8 Byte transactions, as it is the lowest latency operation we
have available. We ensure that each subsequent transaction
touches a different cache line as described in Section 4.1. The
results are shown in Figure 7(a). For PCIe reads (LAT_RD)
with a cold cache we see no changes in latency as we change
the window size: All PCIe reads are serviced from memory.
When the cache is warm, the read latency is around 70ns
lower but increases once the window size exceeds the size
of the LLC. This confirms that PCIe reads are serviced from
the LLC if the data is resident in the cache. With a warm
cache, the latency of a posted PCIe write followed by a read
(LAT_WRRD) follows roughly the same pattern: writes and
reads hit the LLC and, once the window size exceeds the LLC
size, the latency increases by around 70ns. For a cold cache
the LAT_WRRD results illustrate the effect of DDIO: For small
window sizes new cache lines get allocated and writes (and
subsequent reads) are performed to/from the cache. Once the
window size exceeds the 10% of the LLC reserved for DDIO,
dirty cache lines have to be flushed out to memory before
the write can succeed, causing a 70ns delay for most writes.
336
For larger transfer sizes, the differences between hitting the
cache or not is reduced significantly: for 64B LAT_WRRD tests
the difference is around 10ns.
The bandwidth data, presented in Figure 7(b), suggest a
similar story. For 64B DMA Reads (BW_RD), there is a mea-
surable benefit if the data is already resident in the LLC. For
larger transfer sizes (not shown) the benefit is smaller, and
from 512B DMA Reads onwards, there is no measurable dif-
ference. For DMA Writes (BW_WR), there is no benefit if the
data is resident in the cache or not. There is also no mea-
surable benefit on keeping the windows size below the 10%
of the LLC. We suspect that the DDIO portion of the cache
is cleaned quick enough, so that all DMA Writes either hit
the main LLC or the DDIO portion. Since DDIO can not be
disabled and there are no dedicated performance counters it
is not possible to validate this hypothesis.
6.4 NUMA impact
With the PCIe root complex and memory controllers inte-
grated in each CPU node, a PCIe device may access memory
local or remote to the node to which it is attached. In this
section we evaluate the performance impact of this non-
uniform memory access. The host buffer is allocated either
on the node the PCIe device is attached to (local) or on the
other node in a 2-way NUMA system (remote). The cache is
warmed or thrashed on the node the buffer is allocated from.
Figure 8 shows the percentage change for DMA read band-
width of local memory versus remote memory for different
transfer sizes across different window sizes with a warm
cache. 64B DMA reads experience a 20% drop in throughput
(from ∼32Gb/s to ∼25Gb/s). The difference drops to around
10% once the DMAs are not serviced from the local cache
anymore. With 128 and 256B DMA reads the penalty for
accessing remote memory drops to 5-7% (e.g., from ∼44Gb/s
to ∼41Gb/s for 128B). There is no noticeable penalty for 512B
DMA reads. The data is confirmed by cold cache DMA read
throughput (not shown) where remote 64B reads experience
a constant 10% penalty (∼5% for 128B and 256B reads).
The throughput of DMA Writes does not seem to be af-
fected by the locality of the host buffer nor by the size of the
host buffer. In contrast with the Intel specification [20], we
believe that all DMA Writes may be initially handled by the
local DDIO cache.
For completeness, the latency numbers do not differ much
from the local case presented in Figures 5 and 7 except that
remote accesses add a constant 100ns of latency to our sys-
tems. Overall, the results from our two dual socket systems
(NFP6000_BDW and NFP6000_IB) are the same, indicating
there has been little change in the two generations sepa-
rating them.
Understanding PCIe performance for end host networking
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
(a) Median latency; minimum and 95th percentile are shown as error bars.
(b) Bandwidth
Figure 7: Cache effects on Latency and Bandwidth (NFP6000-SNB)
Figure 9 shows the result for different transfer sizes. Like
the data presented in Section 6.4 the graph shows the per-
centage change to the same experiment run without IOMMU.
For small window sizes, there are no measurable differences
across the range of transfer sizes. However, once the win-
dow size is increased beyond 256KB, the throughput drops
dramatically. For 64B DMA Reads it drops by almost 70% and
even for 256B DMAs the drop is still a significant 30%. There
is no change for transfer sizes of 512B and above. For DMA
Writes (not shown), the drop is not quite as dramatic (55%
for 64B writes), but is still very significant.
From this data we conclude that on that particular system
the IO-TLB has 64 entries (256KB/4KB), a number that is not
published by Intel. The latency of 64B Reads increases from
around 430ns to 760ns, putting the cost of an IO-TLB miss
and subsequent page table walk at around 330ns. For smaller
transfer sizes this penalty is relatively high compared to the
time it takes to transfer the data, and thus the impact on the
throughput is higher still. The data is surprisingly consistent
across all 4 generations of Intel micro-architectures where
we ran the experiments, and we observe the same effects
with our NetFPGA pcie-bench implementation. With such
consistent results across the 4 micro-architecture genera-
tions we conclude Intel’s IOMMUs have undergone little
development since their first implementation.
7 LESSON LEARNED AND USE CASES
The results obtained from pcie-bench can be and have been
used to characterize and tune system software for high per-
formance I/O. There are a plethora of sophisticated tools
to analyze operating system and application performance,
e.g., [14, 21], as well as tools to understand the performance
impact of host CPU architectures and OS primitives, e.g.,
[5, 39]. However, none of these tools provide detailed insights
into the performance of the I/O subsystem. pcie-bench pro-
vides the necessary data to fine-tune specialized network
stacks [37, 38], and optimize kernel IOMMU handling [48].
Figure 8: Difference between local and remote DMA Reads
of different sizes with warm cache (NFP6000-BDW).
6.5 The IOMMU effect
Finally, we look at the impact of an IOMMU interposed in
the datapath between PCIe devices and the host. To mea-
sure the impact of the IOMMU, we enable it on the Linux
kernel command line with intel_iommu=on. IOMMU imple-
mentations in Ivy Bridge and newer architectures support
super-pages to reduce the number of page table entries, and
thus reduce the number of TLB entries. For our experiments
we disable this behavior (by also specifying sp_off on the
kernel command line). This forces the use of 4KB page table
entries and allows us to use a relatively small 64MB host
buffer for our experiments.
Figure 9: Impact of IOMMUs on DMA reads of different
transfer sizes with warm caches (NFP6000-BDW).
337
3504004505005506006507007504K16K64K256K1024K4096K16384K65536KLatency(ns)Windowsize(Bytes)8BLAT_RD(cold)8BLAT_RD(warm)8BLAT_WRRD(cold)8BLAT_WRRD(warm)05101520253035404K16K64K256K1024K4096K16384K65536KBandwidth(Gb/s)Windowsize(Bytes)64BBW_RD(cold)64BBW_RD(warm)64BBW_WR(cold)64BBW_WR(warm)-50-40-30-20-1004K16K64K256K1024K4096K16384K65536K%changeofbandwidthWindowsize(Bytes)64BBW_RD128BBW_RD256BBW_RD512BBW_RD-70-60-50-40-30-20-1004K16K64K256K1024K4096K16384K65536K%changeofbandwidthWindowsize(Bytes)64BBW_RD128BBW_RD256BBW_RD512BBW_RDSIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
R. Neugebauer et al.
Area
IOMMU (§6.5)
DDIO (§6.3)
NUMA (§6.4)
(small transactions)
NUMA (§6.4)
(large transactions)
Observation
Significant throughput drops as working-set size increases.
Small transactions are faster when the data is resident in the cache.
Higher cost of DMA reads from remote memory compared to local caches.
No significant difference between remote and local cache performance.
Evaluation-directed recommendation
Co-locate I/O buffers into superpages.
DDIO improves descriptor ring access and
performance for small packet receive.
Place descriptor rings on the local node.
Place packet buffers on the node where
processing happens.
Table 2: Notable findings from this paper, derived experimentally with pcie-bench.
Table 2 reports the notable findings from this paper which
have been derived experimentally with pcie-bench. Based
on the IOMMU data, we strongly recommend using super-
pages and trying to force the IO buffers used for DMA to
be co-located in as few super-pages as possible. This may
be possible in carefully controlled environments, such as
virtualized network appliances. However, in multi-tenant
Virtual Machine (VM) environments offering assigned PCIe
devices to VMs, it is currently not possible to isolate the IO
performance of VMs sufficiently with Intel’s IOMMUs.
For NUMA architectures, we found that small DMA reads
from the remote cache are significantly more expensive than
reads from the local cache. Accessing data on a remote node
also adds around 100ns of latency. While it may not be fea-
sible to “pin” small network packets to the local node, it
would certainly be useful to locate data structures, such as
descriptor rings, on the local node. Our data suggests that,
for larger packet sizes, the locality of the packet buffer is not
critical and it is recommended to allocate data on the nodes
where the processing happens.
Finally, our measurements confirm the documented oper-
ation of DDIO and show that, for small transfers, accesses
are around 70ns faster for cache resident data. We see two
areas where this can be beneficial for network applications.
Firstly, access to descriptor rings is lower latency, and there-
fore incur less overhead. Secondly, the integration with the
caches should benefit small packet receive, in particular for
packet sizes which are not multiples of a cacheline (e.g., 64B
Ethernet frames with the 4B FCS stripped). Since the data is
directly DMAed into the cache, dirty cachelines do not need
to be written back to memory before the DMA can succeed.
While the above lessons focus on optimizing host system
software, the insights gained with pcie-bench have also im-
plications for the growing area of research moving beyond
straightforward TCP offload to offloading more complex pro-
cessing to programmable and reconfigurable network cards,
e.g., [2, 26, 51, 52]. In order to implement the firmware or
reconfigurable logic for these application-offloads, it is impor-
tant to have a detailed characterization of the DMA engines
and their interaction with the host system. The data pro-
vided by pcie-bench is ideal to guide such implementation
choices. For example, understanding the latency of trans-
actions for the Netronome boards has heavily influenced
the design of multiple firmware implementations that offer
different offloads of network processing. The latency data
determines how many in-flight DMAs the firmware has to
handle (for both packet data DMA and descriptor DMAs)
to sustain line rate. In turn, this determines the sizing of
I/O structures such as rings and other storage for in-flight
DMAs along with specifying the appropriate number of Flow
Processing Cores and threads. The latency and bandwidth
data also determines the amount of batching performed both
in the firmware and the corresponding device driver in the
operating system. For example, on the NFP6000-HSW system,
it takes between 560−666ns to transfer 128B of data from the
host to the device. At 40Gb/s line rate for 128B packets, a new
packet needs to be transmitted every 29.6ns. This means that
the firmware and DMA engines need to handle at least 30
transactions in flight. These calculations can be extended to
take into account the latency for descriptor transfers and to
work out the cycle budget for each DMA. If the IOMMU is en-
abled, the firmware and DMA engines also need to cover the
occasional latency increase of ∼ 330ns, caused by IOMMU
TLB misses. Furthermore, we have seen significant variance
in latencies on some systems, most notably on a Xeon E3 sys-
tem, which further complicate the implementation of high
performance firmware or DMA engine designs.
Finally, the pcie-bench methodology and measurements
can be used by silicon engineers to evaluate existing designs
and inform the objectives of future architecture of DMA en-
gines. For example, the PCIe-latency measurements were
used to assess each iteration of a suite of NetFPGA DMA-
engine designs. Being able to compare the current design
with data from other implementations helps to determine
if a performance bottleneck is due to artifacts in the host
architecture or is a design limitation. More importantly, the
methodology was also extensively used for validation dur-
ing chip bring-up, and to guide architectural decisions in
future silicon at Netronome. Using micro-benchmarks, as
provided by pcie-bench, are ideal as they provided detailed
and controlled data which is not obtainable by other means.
338
Understanding PCIe performance for end host networking
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
8 RELATED WORK
Several papers have observed PCIe and its interactions with
host architecture in the context of higher performance net-
work applications. Kalia et al. [24] provide a low level evalu-
ation and recommendation for RDMA primitives and how
they interact with PCIe. Li et al. [32] discuss the benefits of
DDIO on the performance of modern Key-Value-Store (KVS)
applications. The impact of NUMA on device DMA was dis-
cussed by Lim et al. [34] in relation to high performance
KVS applications and by Han et al. [17] for GPU accelerated
packet processing. All these touch on some aspects covered
in this paper but in the context of some higher level applica-
tions. In contrast, pcie-bench allows for a systematic study
of PCIe performance characteristics. The results obtained can
then be used to explain application performance behavior in
detail and may guide future application level performance
improvements more accurately.
characteristics of modern AMD and Intel NUMA architec-
tures. They also present the synthesis of data-access per-
formance models designed to quantify the effects of these
architectural characteristics on bandwidth. Li et al.further
contribute a characterization of the state-of-the-art NUMA
hosts, and propose a methodology to simulate I/O operations
using memory semantics, and in-turn model the I/O band-
width performance. In both cases, these efforts provide little
insight into the explicit and evolving relationship between
PCIe and modern NUMA architectures.
9 CONCLUSION AND FUTURE WORK
This paper shows that PCIe, alongside its interaction with
the root complex and device drivers, can significantly im-
pact the performance of end host networking. Past research
has reported some of the findings in the context of specific
applications, such as RDMA and KVS acceleration. In con-
trast, we provide a theoretical model and a methodology,
pcie-bench, to understand and study inherent bottlenecks
of the PCIe protocol. We also present two implementations
of the methodology for a systematic evaluation and charac-
terization of PCIe devices in real systems.
Beyond the pcie-bench design and implementation, we
discuss our characterization results from a number of sys-
tems. We share lessons learned with implications for current
software systems and future hardware designs. Our study
allows exploration of the impact of new PCIe and host archi-
tecture features such as DDIO and IOMMUs. Specifically, we
demonstrate that the PCIe integration with caches of DDIO
works well but, also characterize the significant, and some-
times undesirable, impact that Intel IOMMUs and NUMA
may have when high DMA rates are necessary.
Our initial results show that more work is to be done.
Given the remarkable differences in PCIe performance be-
tween Xeon E5 and Xeon E3 systems, a more detailed study