}
}
然后在Eclipse下设置程序运行参数为：hdfs：//NameNodeIP/user/ubuntu/In/hello.txt，运行程序即可看到hello.txt中的文本内容。
需要说明的是，这里使用了Hadoop中简洁的IOUtils类来关闭finally子句中的数据流，同时复制输出流之间的字节（System.out）。例9-1中用到的IOUtils.copyBytes（）方法，其中的两个参数，前者表示复制缓冲区的大小，后者表示复制后关闭数据流。
9.5.2 使用FileSystem API读取数据
9.5.1 节提到在应用中会出现不能使用URLStreamHandlerFactory的情况，这时就需要使用FileSystem的API打开一个文件的输入流了。
文件在Hadoop文件系统中被视为一个Hadoop Path对象。我们可以把一个路径视为Hadoop的文件系统URI，比如上文中的hdfs：//localhost/user/ubuntu/In/hello.txt。
FileSystemAPI是一个高层抽象的文件系统API，所以，首先要找到这里的文件系统实例HDFS。取得FileSystem实例有两种静态工厂方法：
public static FileSystem get（Configuration conf）throws IOException
public static FileSystem get（URI uri, Configuration conf）throws IOException
Configuration对象封装了一个客户端或服务器的配置，这是用路径读取的配置文件设置的，一般为conf/core-site.xml。第一个方法返回的是默认文件系统，如果没有设置，则为默认的本地文件系统。第二个方法使用指定的URI方案决定文件系统的权限，如果指定的URI中没有指定方案，则退回默认的文件系统。
有了FileSystem实例后，可通过open（）方法得到一个文件的输入流：
public FSDataInputStream open（Path f）throws IOException
public abstract FSDataInputStream open（Path f, int bufferSize）throws IOException
第一个方法直接使用默认的4KB的缓冲区，如例9-2所示。
例9-2：使用FileSystem API显示Hadoop文件系统中的文件
public class FileSystemCat{
public static void main（String[]args）throws Exception{
String uri=args[0]；
Configuration conf=new Configuration（）；
FileSystem fs=FileSystem.get（URI.create（uri），conf）；
InputStream in=null；
try{
in=fs.open（new Path（uri））；
IOUtils.copyBytes（in, System.out，4096，false）；
}finally{
IOUtils.closeStream（in）；
}
}
}
然后设置程序运行参数为hdfs：//localhost/user/ubuntu/In/hello.txt，运行程序即可看到hello.txt中的文本内容“Hello Hadoop！”。
下面对例9-2中的程序进行扩展，重点关注FSDataInputStream。
FileSystem中的open方法实际上返回的是一个FSDataInputStream，而不是标准的java.io类。这个类是java.io.DataInputStream的一个子类，支持随机访问，并可以从流的任意位置读取，代码如下：
public class FSDataInputStream extends DataInputStream
implements Seekable, PositionedReadable{
//implementation elided
}
Seekable接口允许在文件中定位并提供一个查询方法用于查询当前位置相对于文件开始的偏移量（getPos（）），代码如下：
public interface Seekable{
void seek（long pos）throws IOException；
long getPos（）throws IOException；
boolean seekToNewSource（long targetPos）throws IOException；
}
其中，调用seek（）来定位大于文件长度的位置会导致IOException异常。开发人员并不常用seekT-oNewSource（）方法，此方法倾向于切换到数据的另一个副本，并在新的副本中找寻targetPos制定的位置。HDFS就采用这样的方法在数据节点出现故障时为客户端提供可靠的数据流访问的。如例9-3所示。
例9-3：扩展例9-2，通过使用seek读取一次后，重新定位到文件头第三位，再次显示Hadoop文件系统中的文件内容
package cn.edn.ruc.cloudcomputing.book.chapter09；
import java.io.*；
import java.net.URI；
import java.net.URL；
import java.util.*；
import org.apache.hadoop.fs.FSDataInputStream；
import org.apache.hadoop.fs.FsUrlStreamHandlerFactory；
import org.apache.hadoop.fs.Path；
import org.apache.hadoop.fs.FileSystem；
import org.apache.hadoop.filecache.DistributedCache；
import org.apache.hadoop.conf.*；
import org.apache.hadoop.io.*；
import org.apache.hadoop.mapred.*；
import org.apache.hadoop.util.*；
public class DoubleCat{
public static void main（String[]args）throws Exception{
String uri=args[0]；
Configuration conf=new Configuration（）；
FileSystem fs=FileSystem.get（URI.create（uri），conf）；
FSDataInputStream in=null；
try{
in=fs.open（new Path（uri））；
IOUtils.copyBytes（in, System.out，4096，false）；
in.seek（3）；//go back to pos 3 of the file
IOUtils.copyBytes（in, System.out，4096，false）；
}finally{
IOUtils.closeStream（in）；
}
}
}
然后设置程序运行参数为hdfs：//localhost/user/ubuntu/In/hello.txt，运行程序即可看到hello.txt中的文本内容“Hello Hadoop！lo Hadoop！”。
同时，FSDataInputStream也实现了PositionedReadable接口，从一个制定位置读取一部分数据。这里不再详细介绍，大家可以参考以下源代码。
public interface PositionedReadable{
public int read（long position, byte[]buffer, int offset, int length）
throws IOException；
public void readFully（long position, byte[]buffer, int offset, int length）
throws IOException；
public void readFully（long position, byte[]buffer）throws IOException；
}
需要注意的是，seek（）是一个高开销的操作，需要慎重使用。通常我们是依靠流数据MapReduce构建应用访问模式，而不是大量地执行seek操作。
9.5.3 创建目录
FileSystem显然也提供了创建目录的方法，代码如下：
public boolean mkdirs（Path f）throws IOException
这个方法会按照客户端请求创建未存在的父目录，就像java.io.File的mkdirs（）一样。如果目录包括所有父目录且创建成功，那么它会返回true。事实上，一般不需要特别地创建一个目录，因为调用creat（）时写入文件会自动生成所有的父目录。
9.5.4 写数据
FileSystem还有一系列创建文件的方法，最简单的就是给拟创建的文件指定一个路径对象，然后返回一个写输出流，代码如下：
public FSDataOutputStream create（Path f）throws IOException
这个方法有很多重载方法，例如，可以设定是否强制覆盖原文件、设定文件副本数量、设置写入文件缓冲区大小、文件块大小及设置文件许可等。
还有一个用于传递回调接口的重载方法Progressable，通过这个方法就可以获得数据节点写入进度，代码如下：
package org.apache.hadoop.util；
public interface Progressable{
public void progress（）；
}
新建文件也可以使用append（）在一个已有文件中追加内容，这个方法也有重载，代码如下：
public FSDataOutputStream append（Path f）throws IOException
这个方法对于写入日志文件很有用，比如在重启后可以在之前的日志中继续添加内容，但并不是所有的Hadoop文件系统都支持此方法，比如HDFS支持，但S3不支持。
例9-4展示了如何将本地文件复制到Hadoop的文件系统，当Hadoop调用progress（）方法时，也就是在每64KB数据包写入数据节点管道之后，打印一个星号来展示整个过程。
例9-4：将本地文件复制到Hadoop文件系统并显示进度
public class FileCopyWithProgress{
public static void main（String[]args）throws Exception{
String localSrc=args[0]；
String dst=args[1]；
InputStream in=new BufferedInputStream（new FileInputStream（localSrc））；
Configuration conf=new Configuration（）；
FileSystem fs=FileSystem.get（URI.create（dst），conf）；
OutputStream out=fs.create（new Path（dst），new Progressable（）{
public void progress（）{
System.out.print（"*"）；
}
}）；
IOUtils.copyBytes（in, out，4096，true）；
}
}
然后配置应用参数，可以看到控制台输出“******”，即上传显示进度，每写入64KB即输出一个*。目前其他文件系统写入时都不会调用progeress（）。
9.5.3节在介绍读数据时提到FSDataInputStream，这里FileSystem中的creat（）方法也返回一个FSDataOutputStream，它也有一个查询文件当前位置的方法，代码如下：
package org.apache.hadoop.fs；
public class FSDataOutputStream extends DataOutputStream implements Syncable{
public long getPos（）throws IOException{
//implementation elided
}
//implementation elided
}
但是它与FSDataInputStream不同，FSDataOutputStream不允许定位。这是因为HDFS只对一个打开的文件顺序写入，或者向一个已有的文件添加。换句话说，它不支持对除文件尾部以外的其他位置进行写入，这样，写入时的定位就没有意义了。
9.5.5 删除数据
使用FileSystem的delete（）可以永久删除Hadoop中的文件或目录。
public boolean delete（Path f, boolean recursive）throws IOException
如果传入的f为空文件或空目录，那么recursive值会被忽略。只有当recursive的值为true时，非空的文件或目录才会被删除，否则抛出异常。
9.5.6 文件系统查询
同样，Java API提供了文件系统的基本查询接口。通过这个接口，可以查询系统的元数据信息和文件目录结构，并可以进行更复杂的目录匹配等操作。下面将一一进行介绍。
1.文件元数据：Filestatus
任何文件系统要具备的重要功能就是定位其目录结构及检索器存储的文件和目录信息。FileStatus类封装了文件系统中文件和目录的元数据，其中包括文件长度、块大小、副本、修改时间、所有者和许可信息等。
FileSystem的getFileStatus（）方法提供了获取一个文件或目录的状态对象的方法，如例9-5所示。
例9-5：获取文件状态信息
public class ShowFileStatusTest{
private MiniDFSCluster cluster；//use an in-process HDFS cluster for testing
private FileSystem fs；
@Before
public void setUp（）throws IOException{
Configuration conf=new Configuration（）；