title:Characterizing Rule Compression Mechanisms in Software-Defined Networks
author:Curtis Yu and
Cristian Lumezanu and
Harsha V. Madhyastha and
Guofei Jiang
Characterizing Rule Compression Mechanisms
in Software-Deﬁned Networks
Curtis Yu1, Cristian Lumezanu2(B), Harsha V. Madhyastha3,
and Guofei Jiang2
1 University of California, Riverside, USA
2 NEC Labs America, Princeton, USA
PI:EMAIL
3 University of Michigan, Ann Arbor, USA
Abstract. Software-deﬁned networking (SDN) separates the network
policy speciﬁcation from its conﬁguration and gives applications control
over the forwarding rules that route traﬃc. On large networks that host
several applications, the number of rules that network switches must
handle can easily exceed tens of thousands. Most switches cannot han-
dle rules of this volume because the complex rule matching in SDN
(e.g., wildcards, diverse match ﬁelds) requires switches to store rules
on TCAM, which is expensive and limited in size.
We perform a measurement study using two real-world network traf-
ﬁc traces to understand the eﬀectiveness and side-eﬀects of manual and
automatic rule compression techniques. Our results show that not using
any rule management mechanism is likely to result in a rule set that
does not ﬁt on current OpenFlow switches. Using rule expiration time-
outs reduces the conﬁguration footprint on a switch without aﬀecting
rule semantics but at the expense of up to 40 % increase in control chan-
nel overhead. Other manual (e.g., wildcards, limiting match ﬁelds) or
automatic (e.g., combining similar rules) mechanisms introduce negligi-
ble overhead but change the original conﬁguration and may misdirect
less than 1 % of the ﬂows. Our work uncovers trade-oﬀs critical to both
operators and programmers writing network policies that must satisfy
both infrastructure and application constraints.
1 Introduction
Software-deﬁned networking (SDN) enables ﬂexible and expressive network man-
agement by separating the policy speciﬁcation from conﬁguration. Applications
and operators work with abstract network views [19] and specify policies using
an API. A centralized controller program translates the high-level policies into
low-level conﬁgurations—expressed as forwarding rules—and installs them into
the switch memory using a specialized protocol, such as OpenFlow [16].
To maintain network performance, the set of forwarding rules installed at
a switch must ﬁt into the switch’s memory. Two factors complicate this. First,
as more applications adopt SDN, the number of rules required to express their
c(cid:2) Springer International Publishing Switzerland 2016
T. Karagiannis and X. Dimitropoulos (Eds.): PAM 2016, LNCS 9631, pp. 302–315, 2016.
DOI: 10.1007/978-3-319-30505-9 23
Characterizing Rule Compression Mechanisms in Software-Deﬁned Networks
303
policies on every switch grows, similar to how BGP tables have grown with the
spread of the Internet. Researchers have observed that an average top-of-rack
(ToR) switch would have to hold around 78 K rules with the default expira-
tion timeout [5,12]. Second, switches store wildcard rules in TCAM, which is
expensive and limited in size. Most programmable switches can hold only a few
thousand wildcard-based rules.
There are two general approaches to ensure that application policies do not
result in too many rules: compression and caching. Network control programs
can reduce the number of rules manually (by relying on programmers to employ
OpenFlow constructs such as rule expiration timeouts or wildcards [5,27]) or
automatically (by eliminating redundant rules or combining rules with related
patterns). Compression may limit the expressivity of the conﬁguration as it
changes the original rule space. In addition, when rules are generated in response
to traﬃc, it is diﬃcult to predict how many rules we need a priori to tune
the compression accordingly. Another approach is to cache the most popular
rules in TCAM and rely on additional (software) switches or the controller to
manage traﬃc not matching the cached rules [13]. This preserves the original
conﬁguration but may introduce additional devices and delay in the data plane
of packets matching less popular rules.
In this paper, we use two sets of real world network traﬃc data to study
the eﬀectiveness and side-eﬀects of manual and automatic rule compression.
We seek to answer the following questions: should SDN rely on programmers to
employ mechanisms that reduce the number of rules installed on switches and
if so, what are the most eﬀective such mechanisms? or can SDN beneﬁt from
an automated rule reduction system that sits between the controller and switches
and optimizes how rules are installed on switches? Our work explores trade-oﬀs
critical to both operators and programmers writing network policies that must
satisfy both infrastructure and application constraints.
First, we show how existing mechanisms that programmers and applications
employ, such as reducing rule expiration timeout, using wildcards, or limiting
the match ﬁelds, manage the rules on a switch (Sect. 4). Lowering rule time-
outs can reduce the number of rules by 41–79 %, as compared to the default
operation, but at the expense of increasing the utilization on the constrained
controller-to-switch channel by up to 40 %. Even such high compression rates
may be insuﬃcient for most OpenFlow switches on the market. Using wildcards
or limiting the match ﬁelds can further improve the conﬁguration footprint but
also limits the expressivity of the conﬁguration as the original rule semantics
change.
Second, we show that automatic rule compression can beneﬁt SDN. We intro-
duce and evaluate a simple mechanism that encodes rules using binary trees to
identify and combine similar rules (Sect. 5). This reduces the conﬁguration size
on a switch by as much as 62 % compared to normal operation and at little
change in network overhead. However, such beneﬁt comes at a cost: aggres-
sive automatic rule compression can also result in some ﬂows (<1 %) being
misdirected.
304
C. Yu et al.
2 Motivation
In this section, we discuss how programmable switches store rules and implement
rule matching. We also review related research work and potential solutions for
reducing the number of rules. To keep the discussion simple, we consider Open-
Flow as the de facto protocol for installing and managing switch conﬁgurations.
2.1 Rules and Memory
A network’s conﬁguration consists of the forwarding rules installed at the
switches. Every rule consists of a bit string (with 0, 1, and * as characters)
that speciﬁes which packets match the rule, an action (to be performed by the
switch on matched packets), and a set of counters (which collect statistics). Pos-
sible actions include “forward to physical port”, “forward to controller”, “drop”,
etc. Each rule has two expiration timeouts: a soft one, counted from the time of
the last packet that matched the rule, and a hard one, from the time when the
rule was installed.
Table 1. Several OpenFlow switches specify the maximum number of forwarding rules
that they store. Each rule can contain any subset of the 12 ﬁelds speciﬁed in the
OpenFlow v1.0 speciﬁcation [22], which is used by most switches on the market. The
HP 3800’s fact sheet speciﬁes the maximum number of routing, rather than OpenFlow,
entries; a routing entry can be considered an OpenFlow entry with matches only on
layer 3 ﬁelds.
Switch
Max # rules
Source
NEC PF5820
750
HP ProCurve 5406zl 1500
Pronto 3290
4000
[1]
[5]
[2]
HP 3800
NEC PF5240
IBM G8264
10 k (routing) [10]
64 k–160 k
97 k
[1]
[11]
Implementation details of how rules are stored and matched is left to the
discretion of each switch vendor [24]. A common approach is for switches to store
wildcard rules in TCAM and exact match rules in SRAM. TCAM is fast and
can support wildcards eﬃciently. However, since it is also expensive and power
hungry, its size on switches is limited. On the other hand, SRAM is cheaper and
is available in higher capacity, but has a higher lookup latency because it is often
oﬀ-chip and uses search structures (e.g., hash tables and tries) to locate entries.
Switch vendors do not advertise the details of their OpenFlow implemen-
tation. In addition, the number of OpenFlow rules that a switch can store
in hardware is not always ﬁxed and depends on how rules are formed (e.g.,
whether they have wildcards, what ﬁelds they match on). We studied the pub-
lic datasheets for six popular OpenFlow switches and compiled their published
Characterizing Rule Compression Mechanisms in Software-Deﬁned Networks
305
OpenFlow table limits in Table 1. Unless otherwise noted, the numbers cor-
respond to 12-tuple OpenFlow rules. Independent measurements and personal
communication with vendors indicate that the values are representative for cur-
rent OpenFlow switches [3,24]. Prior work [5,12] has observed that a typical
ToR data center switch may store roughly 78 K rules, an order of magnitude
larger than most switches in the table. Although architectural and algorithmic
advances in switch design may extend the memory limits further (e.g., by using
memory other than TCAM or by making software lookups faster), reducing the
conﬁguration size to begin with is still essential to preserve ﬂexibility and mini-
mize the cost of lookups.
2.2 Managing Conﬁguration Size
There are two types of solutions to manage conﬁguration size: architectural-
based and software-based. Architectural-based solutions seek to optimize the
performance of a switch through various architectural design changes [2], but
are slow to develop and integrate. Software-based methods seek to reduce the
size of the conﬁguration that can be stored on current architectures. We focus
on software-based conﬁguration size management and discuss the two main
approaches: compressing the rule set and caching the more popular rules. In
this paper, we study compression-based techniques.
Compression. Compression-based mechanisms are automatic (i.e., without
programmer involvement) or manual (i.e., require actions from the programmer).
Manual. Personal communication with SDN operators and previous work [5,
27] indicate several OpenFlow-based mechanisms to reduce the ﬂow table size
on a switch. These methods limit the number of rules by having existing rules
cover more traﬃc [5] (e.g., using wildcards rather than exact matches, using
fewer match ﬁelds) or cover the same traﬃc for shorter periods of time (e.g.,
setting smaller rule expiration timeouts). However, this also results in a less
expressive conﬁguration because it reduces the ability to implement complex
policies, such as multipathing [21]. Furthermore, wildcards and longer timeouts
reduce visibility into the network as they increase the coarseness of the statistics
that switches gather about ﬂows.
Automatic. Rule management has been studied in the context of IP routing
table compaction [25], with the goal of restricting the usage of TCAM [15,23].
While some of these methods (e.g., [15]) use binary trees to identify similar rules
(like the approach we present later in Sect. 5), existing methods work on a “single
IP to out port” action and are not easily applicable to OpenFlow rules, which
may have as many as 12 diﬀerent match ﬁelds to be aggregated at once. The
TCAM Razor approach uses decision trees and multi-dimensional topological
transformations to eﬃciently compress packet-classiﬁcation rules [14,17], but
cannot easily adapt to incremental rule changes. To the best of our knowledge,
none of these methods have been implemented in an OpenFlow-based network.
306
C. Yu et al.
Policy composition and arbitration frameworks such as Frenetic [7], Net-
Core [18], and PANE [8] manage application policies to ensure that there are no
conﬂicting or overlapping rules. vCRIB [20] intelligently places rules on diﬀer-
ent OpenFlow switches while being aware of the resources that the rules utilize.
Although these systems can optimize the rules they place on switches (e.g., by
eliminating redundancies), their focus is on managing the policies installed across
the network, rather than on reducing the conﬁguration size on any single switch.
Caching. Rather than compressing the rule set, Katta et al. propose to keep
only the more popular rules in TCAM and use additional (software) switches or
the network controller to manage the traﬃc that does not match on the cached
rules [13]. This approach preserves the semantics of the original rule space at
the expense of additional devices or delay on the data path of a subset of the
traﬃc.
3 Method and Data
We use two traces of real-world network traﬃc to characterize the eﬀective-
ness of manual and automatic rule compression techniques in reducing the ﬂow
table size.
Data. We use a packet trace from a campus network and a ﬂow-level trace from
a nation-wide research network. Our goal is to assess the potential of rule com-
pression mechanisms when regular network traﬃc traverses OpenFlow devices.
Thus, our traces are not collected from OpenFlow-based networks, whose traﬃc
may already be adapted to the programmabile nature of the network. The ﬁrst
dataset, Campus, was collected by Benson et al. [4] at an edge switch of a large
US campus network in Jan 2010 and contains 115 K ﬂows over two hours. The
second dataset, Abilene, contains 1 % sampled Netﬂow data from the Internet2
network, collected at the Washington, DC router in Feb 2013. The trace con-
tains around 12 M ﬂows over three hours. For anonymity, the IP addresses in the
l
s
e
u
r
t
n
e
r
r
u
c
n
o
c
f
o
r
e
b
m
u
N
 262144
 65536
 16384
 4096
 1024
 256
 64
TO = ∞
TO = 60s
TO = 30s
TO = 5s
 20
 40
 80
 60
Time (minutes)
 100
 120
 140
(a)
l
s
e
u
r
t
r
e
b
m
u
n
e
g
a
r
e
v
A
n
e
r
r
u
c
n
o
c
f
o
 262144