port Vector Machines [21, 22, 77, 90], Decision Trees [35, 80, 85],
Neural Networks [42, 56] and different variations [59], maximum
entropy [23, 49]. Given the scope of this work, we choose the Naïve
Bayes classification algorithm for the reasons explained in Sec-
tion 3.2.
Input data. The input dataset consists of the Curlie web pages
identified using the approach outlined in Section 2.4. As first step,
we exclude from the HTML code any non-visible element (i.e.,
JavaScript, CSS, etc.) except for the HTML  tag. Note that
the  tags include a short list of keywords describing the
page content. We extract all the text from the visible content, we
call this input source łweb page contentž (C). Similarly, we refer to
the content obtained from the  tag as łmeta-dataž (M).
Data preprocessing. We apply standard text preprocessing steps on
both the web page content and the meta-data. Such steps include (1)
the transformation of all letters in lower case and (2) the removal of
stop words. In addition (3) we also impose a minimum word length
to three letters, numbers, or any combination of the two; and (4)
we remove content with less than 1,000 characters.
Feature engineering. With respect to feature engineering, we test a
wide variety of algorithms. Due to space limitations, in Section 3.3
we present only the results for the three algorithms with the best
performances.
3.2 Selecting and Training the Classifier
Our classifier has to be applied at Web scale while efficiently detect-
ing web pages that belong the five sensitive categories as defined
by GDPR. To this end, we choose a multinomial Naïve Bayes algo-
rithm, because it allows us to train a single supervised classifier
that can predict multiple classes. Our choice is based on several
reasons. First, it has a simple and easy training and classification
stages [77]. Second, it is a fast learning algorithm that can handle
large numbers of features and classes [21, 55]. Third, the algorithm
was already tested with good results on older versions of Curlie
for a multiclass web page classification purposes [64]. Fourth, the
algorithm showed comparable, and, in some cases, better perfor-
mance than other classifiers [9, 81]. Finally, several off-the-shelf
implementations are publicly available, making it easier for other
researchers to reproduce and validate our results.
We train the classifier using the training set described in Section 2.4.
Such set contains 221,712 web pages, with the corresponding GDPR
category as label. From each web page we extract both the human-
readable text and the meta-data information. Next, we filter the
content by applying the preprocessing steps described in Section 3.1.
This procedure generates a final set of 218,696 URLs with content.
Finally we use Bag-of-Words (BoW), Term Frequency-Inverse Doc-
ument Frequency (TF-IDF) and Word2Vec [60] & Doc2Vec [50] to
extract the features.
Bag-of-Words (BoW) [46] is a popular Information Retrieval (IR)
technique that represents texts as a multiset of words. When such
technique is applied, the classifier disregards grammar rules, but
keeps track only of the word multiplicity (i.e., the number of occur-
rences of a word within a single document or a corpus).
Term Frequency-Inverse Document Frequency (TF-IDF) [72]
is a popular IR numerical statistic which captures the importance
of a word within a document. The TF-IDF value increases propor-
tionally to the occurrences of the word within a document, and
inversely proportionally to its frequency across other documents.
Word2Vec & Doc2Vec [50, 60] are word embedding techniques
that take into consideration both the semantic meaning and the
order of words with in a given text. Word2Vec is generally applied
at the paragraph level, while Doc2Vec uses information obtained
from the entire document. In our case the documents are web pages
contents, and we leverage word embedding to extract the keywords
that are used to train the classifier. It can be used as an intermediary
stage in our case to extract key words and use them as features
during the training phase of the classification algorithm.
For all three feature extractions algorithms that we test above, we
keep all hyperparameters to their default value as defined by each
corresponding Python library, that includes, BoW [5] and TF-IDF [6]
from the sklearn (ver. 0.21.3 [7]), and Doc2Vec from gensim (Ver.
3.8.1 [4]) library. In the next section we discuss the results of each
algorithm and we explain the additional optimizations we apply.
3.3 Classification Accuracy and Optimizations
Classification accuracy is defined as the percentage score, from 0%
(lowest) to 100% (the highest), that a classifier can accurately assign
items (pages) to their correct category. The accuracy is influenced
both by the choice of the input data and by the algorithm used
to extract the features. To identify the combination of input data
and the algorithm that leads to the highest accuracy, we start by
restricting the set of feature only to 5k elements. We then apply
each algorithm, using as input data the web page content, the meta-
data, but also their combination. During this process, we reserve
70% of the input for the training phase and the remaining 30% for
testing. To avoid any bias, we also repeat our experiments using
10-fold validation obtaining consistent results.
Feature selection. Table 4 shows the accuracy for different com-
binations of the input data and the algorithms using 5k features.
When we use only web page content (C), the average accuracy
of the three algorithms is around 81%. TF-IDF and Doc2Vec ob-
tain nearly identical results, while the accuracy for BoW is slightly
lower, around 78.5%. When the input is only the meta-data (M),
the accuracy drops for two out of the three algorithms. In the case
of Doc2Vec, the accuracy is just above 56% if we use meta-data
as input. This result is a direct consequence of how the Doc2Vec
algorithm works, because it leverages full sentences to capture the
Identifying Sensitive URLs at Web-Scale
IMC ’20, October 27–29, 2020, Virtual Event, USA
Table 4: Classification accuracy using 5k features and all the
possible combinations of input data and algorithms to ex-
tract the features.
Feature Engineering
Feature Source
BoW TF-IDF Doc2Vec
Content (C)
Meta-data (M)
C + M
78.48% 82.17%
78.55% 79.62%
79.90% 83.33%
82.34%
56.51%
82.77%
Table 5: The top-10 features that the classifier uses to deter-
mine the category of each web page.
Rank
Health
Ethnicity
Religion
Sexual
Political
Non-
Orientation
Beliefs
sensitive
Genealogy
Family
County
Tree
History
Records
Indian
1
2
3
4
5
6
7
8
9
10
Health
Care
Dental
Medical
Services
Treatment
Patients
Surgery
Therapy
Dentistry
Genealogical Christian
American
Native
Jesus
Ministry
Church
Catholic
God
Sex
Gay
Porn
Worship
Material
Bible
Sunday
Christ
Adult
Fetish
Escorts
Sexual
Lesbian
Nude
Election
Party
State
District
Democratic
Democrats
Senate
Political
Government
Republican
Club
Association
Home
Members
Events
News
Membership
Contact
Read
Society
semantic relevance between adjacent words. Most of the times the
meta-data tag contains a list of keywords, in random order, that
describe the page content. Moreover, the same keyword can appear
in multiple uncorrelated lists across different web pages. The last
row of the table depicts the results for the combination of the two
different input data (C+M). In this scenario, we achieve the highest
accuracy when we extract features using the TF-IDF algorithm.
When we compare those results with the classifier that uses only
web page content, we observe that across all the feature extraction
algorithms we have an average increment of 1% in accuracy. We
chose TF-IDF on input data C+M as baseline for our evaluation and
refer to it as baseline classifier.
Feature sets. To understand the classifier robustness, for each class
we sort the 5k feature vector and we check what are the features
that received the highest weights. In Table 5, we list the top-10
features across the six different categories. We observe that for
each one of the five sensitive categories the top-10 most important
features are well suited to characterize the category. In the case
of Ethnicity we notice some bias toward Indian Americans; and a
similar behavior appears also with terms related to Christianity
in the Religion category. We attribute such bias to the fact that
from the Curlie dataset we extract only English content, that are
associated to the Western culture and more specifically United
States. Similarly, in the Non-sensitive category we see mainly terms
linked to organizations and social activities. Also in this case, the
result could be explained if we consider the Curlie dataset from the
perspective of the top-level categories, as we did in Section 2. We
attribute this bias to the fact that 80% of the web pages originate
from the largest Curlie top-level categories, namely Regional, Art,
Society and Business. Those four top-categories can be easily
connected with the 10 most representative terms that the classifier
associates to the Non-sensitive category.
Figure 3: Confusion matrix of the baseline classifier.
Accuracy of individual categories. Figure 3 presents the confusion
matrix that visually summarizes the classification accuracy for each
sensitive category. The rows of the confusion matrix contain the
instances of a specific class, and columns represent the prediction of
the classifier. Shades on cells indicates the percentage of elements
that are predicted belonging to a particular class: white cells indi-
cate lower percentage values, darker cells higher ones. In an ideal
confusion matrix, all the cells are white except for the elements on
the main diagonal, which are all black. In such case, the classifier
always predicts the correct label for all of the input elements. In
our matrix we observe darker cells only for half of the categories
(i.e., Non-sensitive, Political Beliefs and Religion). In the remain-
ing three cases, the lighter coloration suggests that the instances
are spread among the correct and at least another class, typically
the Non-sensitive. This is particularly evident in the first column
that contains the highest concentration of gray cells. Such trend
indicates that all the five sensitive categories, with different degrees,
have some elements that get mis-labeled as Non-sensitive. We also
observe a small percentage, around 11%, of Non-sensitive web pages
that occasionally get labeled either as Health or Religion. The fact
that the majority of the mis-classifications are localized on the first
column, can be more, or less damaging, depending on the particular
use case. For example, since our goal is to build a framework that
can detect sensitive web pages across the Web, we can use this
kind of classifier to derive a conservative estimation on the number
of URLs and domains hosting sensitive content. In a similar use
case, the law enforcement agencies might leverage this classifier,
combined with third-party detection tools and methodologies, (see
[43]) to check GDPR compliance for a large number of web pages.
In such case the penalty is much higher if instead of just missing
some elements, the analyst has to manually go through tens of thou-
sands of web pages with legitimate content erroneously marked
as sensitive. Our results also show that the mis-classification of
non-sensitive web pages to sensitive categories is low, which is a
desired outcome as we do not want to penalize non-sensitive web
pages.
3.4 Balancing the Classifier
Up to this point all the presented results have been produced by
applying the baseline classifier and using the dataset discussed
Non-sensitiveEthnicityHealthPoliticalBeliefsReligionSexualOrientationNon-sensitiveEthnicityHealthPoliticalBeliefsReligionSexualOrientationCorrect ClassPredicted ClassAcc: 83.33%IMC ’20, October 27–29, 2020, Virtual Event, USA
Matic et al.
Table 6: The number of Curlie web pages used per classi-
fier. łInitialž: baseline classifier , łFinalž: balanced classifier ,
łAddedž: additional URLs included after applying baseline
classifier on the URLs in Unknown set.
GDPR Cat.
Ethnicity
Health
Pol. Beliefs
Religion
Sex. Orientation
Non-sensitive
URLs
Initial
Added
Final
9,399
58,533
15,543
67,593
3,651
63,977
+ 4,710
+ 16,231
+ 21,646
+ 23,541
+ 248
+ 157,118
14,109
74,764
37,189
91,134
3,899
221,095
Unknown
1,060,077
- 223,494
836,583
in Section 3.2. In this section, we explain how to improve both
accuracy and coverage by adding more variety to the training set.