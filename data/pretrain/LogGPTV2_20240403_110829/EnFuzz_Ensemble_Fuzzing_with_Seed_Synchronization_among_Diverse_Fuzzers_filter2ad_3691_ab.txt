in the global crashes list.
First, let us take a look at the seed synchronization solution
of the base fuzzer, which mainly describes how base fuzzers
USENIX Association
28th USENIX Security Symposium    1971
monitor......Base FuzzerBase Fuzzerlocal seed queueglobal seed poollocal seed queuelocal seed queueglobal coverage mapglobal crashesBase Fuzzer12kcontribute the interesting seeds asynchronously to the global
pool. As presented in lines 2-4 of algorithm 1, for each sin-
gle base fuzzer, it works with a local input seed queue and
runs a traditional continuous fuzzing loop. It has three main
steps: (1) Select input seeds from the queue, (2) mutate the
selected input seeds to generate new candidate seeds, (3) run
the target program with the candidate seeds, track the cover-
age and report vulnerabilities. Once the candidate seeds have
new coverage or cause unique crashes, they will be regarded
as interesting seeds and be pushed asynchronously into the
global seed pool, as presented in lines 6-12.
ALGORITHM 2: Action of global monitor sync
Input
:Base fuzzers list BaseFuzzers[]
Initial seeds S
Synchronization period period
f uzzer.setup();
// set up each base fuzzers ;
// set up thread monitor for monitoring ;
1
2 foreach base fuzzer f of the BaseFuzzers[] do
3
4 end
5
6 monintor.setup();
7 GlobalCover.initial();
8 GlobalSeedPool.initial();
9 GlobalSeedPool.push(S);
10 repeat
11
12
13
14
15
16
17
// Skip synchronized seeds ;
if s.isSync() == False then
foreach seed s of the GlobalSeedPool do
18
19
20
foreach base fuzzer f of the BaseFuzzers[] do
Cover = f .run(s) ;
// update the global coverage ;
newCover =
(Cover∪ GlobalCover)− GlobalCover ;
GlobalCover = Cover∪ GlobalCover;
// synchronize the seed s to base fuzzer f ;
if Cover.causeCrash() and
!newCover.isEmpty() then
crashes.push(s);
f .queue.push(s);
else if !newCover.isEmpty() then
f .queue.push(s);
continue;
else
end
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36 until timeout or abort-signal;
end
s.setSync(True);
continue;
end
else
Output :Crashing seeds crashes
end
// waiting until next seed synchronization ;
sleep(period);
Second, let us see the seed synchronization solution of the
monitor process, which mainly describes how the monitor
process synchronously dispatches the interesting seeds in the
global pool to the local queue of each base fuzzer. When all
base fuzzers are established, a thread named monitor will be
created for monitoring the execution status of these fuzzing
jobs, as in lines 2-6 of algorithm 2. It initializes the global
coverage information to record the global fuzzing status of
target applications by all the base fuzzer instances and then
creates the global seed pool with the initial seeds, as in lines
7-9 of algorithm 2. It then runs a continuous periodically syn-
chronizing loop — each base fuzzer will be synchronously
dispatched with the interesting seeds from the global seed
pool. Each base fuzzer will incorporate the seeds into its own
local seed queue, once the seeds are deemed to be interest-
ing seeds (seeds contribute to the coverage or crash and has
not been generated by the local fuzzer), as in line 15-24 . To
lower the overhead of seed synchronization, a thread monitor
is designed to work periodically. Due to this globally asyn-
chronous and locally synchronous based seed synchronization
mechanism, base fuzzers cooperate effectively with each other
as in the motivating example in Figure 1.
5 Evaluation
To present the effectiveness of ensemble fuzzing, we ﬁrst
implement several prototypes of ensemble fuzzer based on
the state-of-the-art fuzzers. Then, we refer to some kernel
descriptions of evaluating fuzzing guideline [25]. We conduct
thorough evaluations repeatedly on LAVA-M and Google’s
fuzzer-test-suite, several well-fuzzed open-source projects
from GitHub, and several commercial products from compa-
nies. Finally, according to the results, we answer the following
three questions: (1) Can ensemble fuzzer perform better? (2)
How do different base fuzzers affect Enfuzz? (3) How does
Enfuzz perform on real-world applications
5.1 Ensemble Fuzzer Implementation
We implement ensemble fuzzing based on six state-of-the-art
fuzzers, including three edge-coverage guided mutation-based
fuzzers – AFL, AFLFast and FairFuzz, one block-coverage
guided mutation-based fuzzer – libFuzzer, one generation-
based fuzzer – Radamsa and one most recently hybrid fuzzer –
QSYM. These are chosen as the base fuzzers for the following
reasons (Note that EnFuzz is not limited to these six and
other fuzzers can also be easily integrated, such as honggfuzz,
ClusterFuzzer etc.):
• Easy integration. All the fuzzers are open-source and
have their core algorithms implemented precisely. It is
easy to integrate those existing fuzzers into our ensem-
ble architecture. We do not have to implement them on
our own, which eliminates any implementation errors or
deviations that might be introduced by us.
• Fair comparison. All the fuzzers perform very well and
are the latest and widely used fuzzers, as is seen by
their comparisons with each other in prior literature, for
example, QSYM outperforms similar fuzzers such as
Angora [18] and VUzzer. We can evaluate their perfor-
mance on real-world applications without modiﬁcation.
• Diversity demonstration. All these fuzzers have differ-
ent fuzzing strategies and reﬂect the diversity among
correspondence with the three base diversity heuristics
1972    28th USENIX Security Symposium
USENIX Association
Table 2: Diversity among these base fuzzers
Tool
AFLFast
FairFuzz
diversity compared with AFL
Seed mutation and selection strategy based
rule: the times of random mutation for each
seed is computed by a Markov chain model.
The seed selection strategy is different.
Seed mutation and selection strategy based
rule: only mutates seeds which hit rare
branches and strives to ensure the mutant
seeds hit the rarest one. The seed mutation
strategy is different.
Radamsa
libFuzzer Coverage information granularity based rule:
libFuzzer mutates seeds by utilizing the San-
itizerCoverage instrumentation, which sup-
ports tracking block coverage; while AFL
uses static instrumentation with a bitmap to
track edge coverage. The coverage informa-
tion granularity is different.
Input generation strategy based rule: Radamsa
is a widely used generation-based fuzzer
which generates different inputs sample ﬁles
of valid data. The input generation strategy is
different.
QSYM is a practical fast concolic execution
engine tailored for hybrid fuzzing. It makes
hybrid fuzzing scalable enough to test com-
plex, real-world applications.
QSYM
mentioned in section 4.1: coverage information granu-
larity diversity, input generation strategy diversity, seed
mutation and selection strategy diversity. The concrete
diversity among these base fuzzers is listed in Table 2.
To demonstrate the performance of ensemble fuzzing and
the inﬂuence of diversity among base fuzzers, ﬁve prototypes
are developed. (1) EnFuzz-A, an ensemble fuzzer only based
on AFL, AFLFast and FairFuzz. (2) EnFuzz-Q, an ensemble
fuzzer based on AFL, AFLFast, FairFuzz and QSYM, a prac-
tical concolic execution engine is included. (3) EnFuzz-L,
an ensemble fuzzer based on AFL, AFLFast, FairFuzz and
libFuzzer, a block-coverage guided fuzzer is included. (4)
EnFuzz, an ensemble fuzzer based on AFL, AFLFast, lib-
Fuzzer and Radamsa, a generation-based fuzzer is further
added .(5) EnFuzz−, with the ensemble of same base fuzzers
(AFL, AFLFast and FairFuzz), but without the seed synchro-
nization, to demonstrate the effectiveness of the global asyn-
chronous and local synchronous based seed synchronization
mechanism. During implementation of the proposed ensem-
ble mechanism, we address the following challenges:
1) Standard Interface Encapsulating The interfaces of
these fuzzers are different. For example, AFL family
tools use the function main, but libFuzzer use a function
LLVMFuzzerTestOneInput. Therefore, it is hard to ensemble
them together. We design a standard interface to encapsulate
the complexity of different fuzzing tools. This standard inter-
face takes seeds from the ﬁle system, and writes the results
back to the ﬁle system. All base fuzzers receive inputs and
produce results through this standard interface, through which
different base fuzzers can be ensembled easily.
2) libFuzzer Continuously Fuzzing The fuzzing engine of
libFuzzer will be shut down when it ﬁnds a crash, while other
tools continue fuzzing until manually closed. It is unfair to
compare libFuzzer with other tools when the fuzzing time
is different. The persistent mode of AFL is a good solution
to this problem. Once AFL sets up, the fuzzer parent will
fork and execve a new process to fuzz the target. When the
target process crashes, the parent will collect the crash and
resume the target, then the process simply loops back to the
start. Inspired by the AFL persistent mode, we set up a thread
named Parent to monitor the state of libFuzzer. Once it shuts
down, Parent will resume the libFuzzer.
3) Bugs De-duplicating and Triaging We develop a tool
for crash analysis. We compile all the target applications with
AddressSanitizer, and test them with the crash samples. When
the target applications crash, the coredump ﬁle, which consists
of the recorded state of the working memory will be saved.
Our tool ﬁrst loads coredump ﬁles, then gathers the frames
of each crash; ﬁnally, it identiﬁes two crashes as identical if
and only if the top frame is identical to the other frame. The
method above is prone to underestimating bugs. For example,
two occurrences of heap overﬂow may crash at the cleanup
function at exit. However, the target program is instrumented
with AddressSanitizer. As the program terminates immedi-
ately when memory safety problems occur, the top frame is
always relevant to the real bug. In practice, the original dupli-
cate unique crashes have been drastically de-duplicated to a
humanly check-able number of unique bugs, usually without
duplication. Even though there are some extreme cases that
different top frames for one bug, the result can be further
reﬁned by manual crash analysis.
4) Seeds effectively Synchronizing The implementation of
the seed synchronization mechanism: all base fuzzers have
implemented the communication logic following the standard
interface. Each base fuzzer will put interesting seeds into its
own local seed pool, and the monitor thread sync will period-
ically make each single base fuzzer pull synchronized seeds
from the global seed pool through a communication channel.
This communication channel is implemented based on ﬁle
system. A shorter period consumes too many resources, which
leads to a decrease in fuzzing performance. A longer period
will make seed synchronizing untimely, which also affects the
performance. After multiple attempts with different values,
it is found that the synchronization interval affects the per-
formance at the beginning of fuzzing, while little impact was
observed in the long term. The interval of 120s is identiﬁed
with the fastest convergence.
5.2 Data and Environment Setup
Firstly, we evaluate ensemble fuzzing on LAVA-M [19],
which consistis of four buggy programs, ﬁle, base64, md5sum
and who. LAVA-M is a test suite that injects hard-to-ﬁnd bugs
in Linux utilities to evaluate bug-ﬁnding techniques. Thus the
test is adequate for demonstrating the effectiveness of ensem-
ble fuzzing. Furthermore, to reveal the practical performance
of ensemble fuzzing, we also evaluate our work based on
fuzzer-test-suite [8], a widely used benchmark from Google.
USENIX Association
28th USENIX Security Symposium    1973
The test suite consists of popular open-source real-world ap-
plications. This benchmark is chosen to avoid the potential
bias of the cases presented in literature, and for its great di-
versity, which helps demonstrate the performance variation
of existing base fuzzers.
We refer to the kernel criteria and settings of evaluation
from the fuzzing guidelines [25], and integrate the three
widely used metrics from previous literature studies to com-
pare the results on these real-world applications more fairly,
including the number of paths, branches and unique bugs. To
get unique bugs, we use crash’s stack backtraces to dedupli-
cate unique crashes, as mentioned in the previous subsection.
The initial seeds for all experiments are the same. We use the
test cases originally included in their applications or empty
seed if such initial seeds do not exist.
The experiment on fuzzer-test-suite is conducted ten times
in a 64-bit machine with 36 cores (Intel(R) Xeon(R) CPU E5-
2630 v3 @ 2.40GHz), 128GB of main memory, and Ubuntu
16.04 as the host OS with SMT enabled. Each binary is hard-
ened by AddressSanitizer [11] to detect latent bugs. First, we
run each base fuzzer for 24 hours with one CPU core in single
mode. Next, since EnFuzz-L, EnFuzz and EnFuzz-Q need at
least four CPU cores to ensemble these four base fuzzers, we
also run each base fuzzer in parallel mode for 24 hours with
four CPU cores. In particular, EnFuzz-A and EnFuzz− only
ensembles three types of base fuzzers (AFL, AFLFast and
FairFuzz). To use the same resources, we set up two AFL
instances, one AFLFast instance and one FairFuzz instance.
This experimental setup ensures that the computing resources
usage of each ensemble fuzzer is the same as any base fuzzers
running in parallel mode. While most metrics converged to
similar values during multithreaded fuzzing. The variation of
those statistical test results is small (between -5% 5%), we
just use the averages in this paper.
5.3 Preliminary Evaluation on LAVA-M
We ﬁrst evaluate ensemble fuzzing on LAVA-M, which has
been used for testing other systems such as Angora, T-Fuzz
and QSYM, and QSYM shows the best performance. We run
EnFuzz-Q (which ensembles AFL, AFLFast, FairFuzz and
QSYM) on the LAVA-M dataset. To demonstrate its effective-
ness, we also run each base fuzzer using the same resources
— four instances of AFL in parallel mode, four instances of
AFLFast in parallel mode, four instances of FairFuzz in paral-
lel mode, QSYM with four CPU cores used in parallel mode
(two instances of concolic execution engine and two instances
of AFL). To identify unique bugs, we used built-in bug identi-
ﬁers provided by the LAVA project. The results are presented
in Table 3, 4 and 5, which show the number of paths executed,
branches covered and unique bugs detected by AFL, AFLFast,
FairFuzz, QSYM, EnFuzz-Q.
From Tables 3, 4 and 5, we found that AFL, AFLFast
and FairFuzz perform worse due to the complexity of their
branches. The practical concolic execution engine helps
QSYM solve complex branches and ﬁnd signiﬁcantly more
bugs. The base code of the four applications in LAVA-M
are small (2K-4K LOCs) and concolic execution could work
well on them. However, real projects have code bases that
easily reach 10k LOCs. Concolic execution might perform
worse or even get hanged, as presented in the latter subsec-
tions. Furthermore, when we ensemble AFL, AFLFast, Fair-
Fuzz and QSYM together with the GALS based seed syn-
chronization mechanism – EnFuzz-Q always performs the
best in both coverage and bug detection. In total, compared
with AFL, AFLFast, FairFuzz and QYSM, EnFuzz-Q exe-
cutes 44%, 45%, 43% and 7.7% more paths, covers 195%,
215%, 194% and 5.8% more branches, and detectes 8314%,
19533%, 12989% and 0.68% more unique bugs respectively.
From these preliminary statistics, we can determine that the
performance of fuzzers can be improved by our ensemble
approach.
Table 3: Number of paths covered by AFL, AFLFast, FairFuzz,
QSYM and EnFuzz-Q on LAVA-M.
Project
base64
md5sum
who
uniq
total
AFL AFLFast
1065
1078
589
589
4599
4585
453
476
6692
6742
FairFuzz
1080
601
4593
471
6745
QSYM EnFuzz-Q
1794
1643
1198
1062
5986
5621
731
693
9709
9019
Table 4: Number of branches covered by AFL, AFLFast, Fair-
Fuzz, QSYM and EnFuzz-Q on LAVA-M.
Project
base64
md5sum
who
uniq
total
AFL AFLFast
358
388
208
230
791
813
992
1085
2349
2516
FairFuzz
389
241
811
1079
2520
QSYM EnFuzz-Q
993
2786
1869
1761
7409
960
2591
1776
1673
7000
Table 5: Number of bugs found by AFL, AFLFast, FairFuzz,
QSYM and EnFuzz-Q on LAVA-M.
Project
base64
md5sum
who
uniq
total
AFL AFLFast
1
0
0
5
6
1
0
2
11
14
FairFuzz
0
1
1
7
9
QSYM EnFuzz-Q
42
57
1053
26
1178
41
57
1047
25
1170
5.4 Evaluation on Google’s fuzzer-test-suite
While LAVA-M is widely used, Google’s fuzzer-test-suite is
more practical with many more code lines and containing real-
world bugs. To reveal the effectiveness of ensemble fuzzing,
we run EnFuzz (which only ensembles AFL, AFLFast, Lib-
Fuzzer and Radamsa) on all of the 24 real-world applications
of Google’s fuzzer-test-suite for 24 hours 10 times. As a com-
parison, we also run each base fuzzer in parallel mode with
four CPU cores used. To identify unique bugs, we used stack
backtraces to deduplicate crashes. The results are presented
1974    28th USENIX Security Symposium
USENIX Association
in Tables 6, 7 and 8, which shows the average number of
paths executed, branches covered and unique bugs detected
by AFL, AFLFast, FairFuzz, LibFuzzer, Radamsa, QSYM
and EnFuzz respectively.
Table 6: Average number of paths covered by each tool on
Google’s fuzzer-test-suite for ten times.
Project
boringssl
c-ares
guetzli
lcms
libarchive
libssh
libxml2
openssl-1.0.1
openssl-1.0.2
openssl-1.1.0
pcre2
proj4
re2
woff2
freetype2
harfbuzz
json
libjpeg
libpng
llvm
openthread
sqlite
vorbis
wpantund
Total
Improvement
AFL AFLFast FairFuzz LibFuzzer Radamsa QSYM EnFuzz
7136
2816
3286
3430
253
116
146
146
4508
2550