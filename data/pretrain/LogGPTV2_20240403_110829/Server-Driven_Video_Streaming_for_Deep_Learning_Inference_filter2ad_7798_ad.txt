(a) Object detection (Traffic)
(b) Object detection (Dashcam)
(c) Object detection (Drone)
(d) Face recognition (Friends)
(e) Semantic segmentation (Traffic)
(f) Sem. segmentation (Dashcam)
(g) Semantic segmentation (Drone)
(h) Face recognition (TBBT)
Figure 9: The normalized bandwidth consumption v.s. inference accuracy of DDS and several baselines on various video genres and applications.
DDS achieves high accuracy with 55% bandwidth savings on object detection and 42% on semantic segmentation, and 36% on face recognition.
Ellipses show the 1-ùúé range of results.
Name
Traffic
Drone
Vision tasks
Total length
# videos
obj detect / segment
obj detect / segment
Dashcam obj detect / segment
7
13
9
10
10
Table 1: Summary of our datasets.
2331s
163s
5361s
6000s
6000s
face recog
face recog
Friends
TBBT
# objs/IDs
24789
41678
24691
15022
12109
‚Ä¢ DDS sees even more improvements on certain video genres where
objects are small (Figure 12) and on applications where the spe-
cific target objects appear rarely (Figure 13).
‚Ä¢ DDS‚Äôs gains remain substantial under various bandwidth budgets
‚Ä¢ DDS poses limited additional compute overhead on both the
(Figure 14) and bandwidth fluctuation (Figure 16).
camera and the server (Figure 19).
5.1 Methodology
Experiment setup: We build an emulator of video streaming that
can measure the exact analytics accuracy and bandwidth usage. Al-
though existing video analytics platforms might support DDS, we
implement and test DDS and all baselines in our emulator for a fair
comparison. It consists of a client (camera) that encodes/decodes lo-
cally stored videos and a fully functional server that runs any given
DNN and a separate video encoder/decoder. We run DNN inference
on RTX 2080 super and all other computations on Intel Xeon Silver
4100. Unless stated otherwise, we use FasterRCNN-ResNet101 [68]
as the server-side DNN for object detection, InsightFace [14, 34] for
face recognition and FCN-ResNet101 [57] for semantic segmenta-
tion. As we will see in ¬ß5.3, different choices of DNNs will not quali-
tatively change the takeaways. When needed, we vary video quality
along the quantization parameter (from {26,28,30,32,34,36,38,40},
we call it QP for short) and the resolution (from scale factors of
{0.8,0.7,0.5}), and DDS uses 36 (QP) as low quality and 26 (QP) as
high quality, with resolution scale set to 0.8 in object detection and
1.0 in semantic segmentation5. We do not consider the network
5We use full resolution in semantic segmentation to keep the same number of labeled
pixels as in the ground truth which assigns each pixel a class label.
564
cost of AWStream to profile the accuracy-bandwidth relationships
under different QP-resolution combinations. This makes the AW-
Stream bandwidth usage is strictly less than its actual one. In most
graphs, we assume a stable network connection, but in ¬ß5.4, we
will test DDS under different network bandwidth and latency and
a few real network traces.
Datasets: To evaluate DDS over various video genres, we compile
five video datasets each representing a real-world scenario (summa-
rized in Table 1 and their links can be found in [4]). These videos
are obtained from two public sources. First, we get videos from
aiskyeye [23], a computer-vision benchmark designed to test DNN
accuracies on drone videos. Nonetheless, DDS and the baselines
can be affected by factors such as fraction of frames with objects
of interest or size of the regions with objects. Therefore, we try to
cover a range of values along these factors (including objects of
various sizes and frames with various number of objects) by adding
YouTube videos as follows. We search keywords (e.g., ‚Äúhighway
traffic video HD‚Äù) in private browsing mode (to avoid personal-
ization biases); among the top results, we manually remove the
videos that are irrelevant (e.g., news report that mentions traffic),
and we download the remaining videos in their entirety or the first
10-minutes (if they exceed 10 minutes). The vision tasks are (1) to
detect (or segment) vehicles in traffic and dashcam videos, (2) to
detect humans in drone videos, and (3) to recognize identities in
sitcom videos. Because many of these videos do not have human-
annotated ground truth, for fairness, we use the DNN output on the
full-size original video as the reference result to calculate accuracy.
For instance, in object detection, the accuracy is defined by the
F1 score with respect to the server-side DNN output in highest
resolution (original) with over 30% confidence score.
Baselines: We use five baselines to represent two state-of-the-
art techniques (see ¬ß2.3): camera-side heuristics (Glimpse [30],
Vigil [80], EAAR [54]) and adaptive streaming (AWStream [78],
CloudSeg [76]). We made a few minor modifications to ensure
the comparison is fair. First, all baselines and DDS use the same
DDS (ours)AWStreamEAARCloudSegGlimpseVigilDDS (ours)AWStreamEAARCloudSegGlimpseVigilDDS (ours)AWStreamEAARCloudSegGlimpseVigilDDS (ours)AWStreamBetterVigilGlimpseDDS (ours)AWStreamBetterCloudsegDDS (ours)AWStreamBetterCloudsegDDS (ours)AWStreamBetterCloudsegDDS (ours)AWStreamBetterVigilGlimpseServer-Driven Video Streaming for Deep Learning Inference
SIGCOMM ‚Äô20, August 10‚Äì14, 2020, Virtual Event, USA
(a) Precision on traffic video
(b) Recall on traffic video
Figure 10: The normalized bandwidth consumption v.s. precision and
recall of DDS and several baselines on traffic camera.
server-side DNN. Second, although DDS needs no more camera-side
compute power than encoding, camera-side heuristics baselines
are given sufficient compute resource to run more advanced track-
ing [40] and object detection algorithm [69] than what Glimpse and
Vigil6 originally used, so these baselines‚Äô performance is strictly bet-
ter than their original designs. Third, all DNNs used in baselines and
DDS are pre-trained (i.e., not transfer-learned with samples from
the test dataset); In particular, our implementation of CloudSeg uses
the pre-trained super-resolution model [26] from the website [18].
This ensures the gains are due to the video streaming algorithm,
not due to DNN fine-tuning, and it also helps reproducibility. Fi-
nally, although DDS could lower the frame rate, to ensure that the
accuracies are always calculated on the same set of images, we do
not sample frames and only vary the resolution and QP in DDS.
Performance metrics: We use the definition of accuracy and
average response delay from ¬ß2.1. To avoid impact of video content
on bandwidth usage, we report bandwidth usages of DDS and the
baselines after normalizing them against the bandwidth usage of
each original video.
5.2 End-to-end improvements
We start with DDS‚Äôs overall performance gains over the baselines
along bandwidth savings, accuracy, and average response delay.
Bandwidth saving: Figure 9 compares the bandwidth-accuracy
tradeoffs of DDS with those of the baselines. In each application,
we use a fixed DDS configuration and normalize the bandwidth
usage against the size of the highest-quality videos (which we use
to derive the ground truth). We also lower the frame rate to 1 FPS
to speed up the experiments and confirm on a randomly sampled
set of videos that the 1 FPS optimization has minimal impact on
DDS‚Äôs relative bandwidth savings and accuracy gains. Across three
vision tasks, DDS achieves higher or comparable accuracy than
AWStream but uses 55% less bandwidth in object detection and 42%
in semantic segmentation. Glimpse sometimes uses less bandwidth
but has much lower accuracy. Vigil, Glimpse, CloudSeg and EAAR
consumes more bandwidth than DDS with lower accuracy. Overall,
even if DDS is less accurate or uses more bandwidth, it always has
an overwhelming gain on the other metric.
We explain this result from two perspectives.
‚Ä¢ Precision and recall: Figure 10 corroborates our intuition (¬ß2.5) that
the camera-side heuristics, used by Vigil, Glimpse and EAAR to
select frames/regions, are limited by camera-side compute power
6Our implementation of Vigil does not include the optimization of setting the back-
ground pixels in same RGB color, but in a separate experiment, we find that on the
object detection videos (Table 1), this optimization only reduces Vigil‚Äôs bandwidth
usage by 10-20% and leads to similarly low accuracy.
565
(a) Object detection
(b) Semantic segmentation
Figure 11: Response delay of DDS is consistently lower than AW-
Stream under various lengths of video segment.
and thus tend to either miss objects (as illustrated in Figure 5) and
produce spurious objects (e.g., Vigial and Glimpse), or use too
much bandwidth to achieve decent accuracy (e.g., EAAR). We no-
tice that CloudSeg has a lower recall than AWStream (whereas the
original paper shows otherwise on a different dataset). We spec-
ulate that this is because we use a pre-trained super-resolution
model (in consistent with the implementation of other baselines),
whereas the original paper fine-tunes the super-resolution model
to the dataset. DDS has a higher precision because it uses the out-
put of the server-side DNN model; it achieves high recall because
it re-examines the uncertain regions from the low-quality video.
‚Ä¢ Encoding: Glimpse, Vigil, and EAAR all send individual frames sep-
arately, whereas DDS uses a video codec to leverage the temporal
similarities across frames. Moreover, AWStream and CloudSeg do
use video codec, but they do not leverage the non-uniform quality
distribution, and DDS (Stream B) only encodes the difficult-to-
detect regions/pixels at a higher quality.
Response delay: Figure 11 shows the response delay of DDS and
AWStream (the baseline whose accuracy is the closest to DDS)
with the same length of a segment (number of consecutive frames
encoded as a segment before sent to the server). In this experiment,
we use a fixed bandwidth at the bitrate of the highest-quality video,
which we use to derive the ground truth. Note that we exclude the
buffering delay for the camera to accumulate the frames in each
segment (which is the same between AWStream and DDS) as well
as the delay for DDS to concatenate the pixel matrices in Stream A
and Stream B (which can be sped up by standard libraries, such as
openCL). For the same segment length, we see that DDS reduces the
average response delay by about 5-25% compared with AWStream,
despite that DDS needs two iterations per frame. This is because
DDS detects most of the objects in Stream A whereas AWStream
sends a single video stream and spends more time to transmit that
stream through the network. To put it into perspective, popular
video sites use 4-second to 8-second segments [81] (i.e., over 120
frames per segment), a range in which DDS‚Äôs gains are considerable.
5.3 Sensitivity to application settings
Impact of video genres: Next, Figure 12 shows the distribution of
per-video bandwidth savings with respect to AWStream (dividing
AWStream‚Äôs bandwidth usage by DDS‚Äôs when DDS‚Äôs accuracy is at
least as high as AWStream) in three datasets. There is substantial
performance variability, even among the videos of the same type.
This is because DDS‚Äôs gains depend on the size of objects missed
by the low-quality encoding, which varies with videos (¬ß3.4).
That said, the impact of content on performance gains can be
task-dependent. For instance, in object detection, the dashcam
videos show less improvement than the other two datasets, but
DDS (ours)AWStreamEAARCloudSegGlimpseVigilDDS (ours)AWStreamEAARCloudSegGlimpseVigil15306090120# of frames per segment02468Response delay (s)DDS streamingDDS processingAWStream streamingAWStream processing15306090120# of frames per segment01234567Response delay (s)DDS streamingDDS processingAWStream streamingAWStream processingSIGCOMM ‚Äô20, August 10‚Äì14, 2020, Virtual Event, USA
K. Du, A. Pervaiz, X. Yuan, A. Chowdhery, Q. Zhang, H. Hoffmann, J. Jiang
(a) Object detection
(b) Semantic segmentation
(a) Object detection
(b) Semantic segmentation
Figure 12: Distributions of per-video bandwidth savings in two
datasets. The gains of DDS are video dependent.
Figure 14: DDS outperforms AWStream (the closest baseline) in
accuracy under various bandwidth consumption budgets.
Figure 13: Segmentation on only motorcycles achieves 2-4√ó more
bandwidth savings than segmentation on all classes.
in semantic segmentation, the dashcam videos show the most im-
provement! This contrast highlights the difference between the two
tasks. In object detection, when an object is not confidently classi-
fied, DDS will send an entire bounding box to the server. However,
semantic segmentation typically sends the pixels at the boundary
of objects, whose size is less affected by the size of objects. Since
the dashcam videos have more large objects, they show more gains
from DDS when the task is semantic segmentation.
Impact of the targeted objects: So far we have evaluated DDS
when the target object classes appear frequently in video, but an
advantage of DDS is that it saves more bandwidth when the server-
side DNN only detects particular objects and these objects appear
less frequently. To show it, we change the segmentation task from
detecting all objects to detecting only motorcyles. Figure 13 shows
the DDS‚Äôs bandwidth savings (when achieving same or higher ac-
curacy than AWStream) on three traffic videos in which motorcyles
do appear but only in a small fraction of frames, and compare the
bandwidth savings with those when the task is over all classes.
DDS‚Äôs gains are more significant when only motorcyles are the
target objects, and the gains are higher when the motorcyles take a
smaller fraction of pixels and frames (e.g., Video 2).
Impact of DNN architecture: Last but not least, we test different
DNN architectures on a randomly-selected traffic video (5-minute
long). We find that DDS achieves substantial bandwidth savings