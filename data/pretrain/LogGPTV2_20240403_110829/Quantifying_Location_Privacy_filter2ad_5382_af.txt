(b) K-anonymity vs. Incorrectness
Figure 5. Comparison of location-privacy metrics. The x-axis shows the
users’ location-privacy based on the incorrectness metric (30). The y-axis
shows (a) the normalized entropy of the adversary’s estimation, (b) the
normalized k-anonymity. Each point in the plot represents the location
privacy of some user at some time for two metrics (incorrectness vs entropy
in (a), incorrectness vs k-anonymity in (b)). “∗”s are the location privacy
values achieved from LPPM(2, 3, 0.9) as a strong mechanism, “·”s are
the values for LPPM(1, 2, 0.5) as a medium mechanism, and “◦”s are the
values for LPPM(1, 0, 0.0) as a weak mechanism. The two metrics would
be fully correlated only if all points were on the diagonal (0, 0) to (1, 1).
in the same way. We focus on LO-ATT, and we assess
these metrics by testing to what extent they are correlated
to the success of the adversary in correctly localizing users
the incorrectness metric LPLO-ATT (u, t)).
over time (i.e.,
We choose three LPPMs: LPPM(1, 0, 0.0) as a weak
mechanism, LPPM(1, 2, 0.5) as a medium mechanism, and
LPPM(2, 3, 0.9) as a strong mechanism.
In Section II-D, we use entropy to measure the uncertainty
of the adversary. Here, we assess the normalized entropy of
the pdf of the location of user u at time t, as a metric for
259
her location privacy. The normalized entropy is computed
as follows:
• Understanding the threats and formalizing the attacks
on location privacy
N HLO-ATT (u, t) =
−Pr∈R ˆpu,t(r) log(ˆpu,t(r))
log(M )
(33)
• Designing a standard and appropriate evaluation metric
for location privacy based on a sound theoretical model
that can be used to compare various schemes
where log(M ) is the maximum entropy over M regions.
According to the k-anonymity metric, the location-privacy
of a user u at a given time t is equal to the number of
users who satisfy all of the following conditions: (i) they
are anonymous, (ii) they obfuscate their location by merging
regions (which includes their actual
location), (iii) their
obfuscated location (i.e., the set of merged regions) is a
superset of the obfuscated location of u at t. We divide this
number of users by N , the total number of users, to have
the normalized k-anonymity:
N KLO-ATT (u, t) =
1
N Xv∈U
1av(t)∈ou(t)∧ou(t)⊆ov (t)
(34)
Figure 5 illustrates the relation between the incorrect-
ness of the adversary LPLO-ATT (u, t) and the two above-
mentioned metrics: normalized entropy N HLO-ATT (u, t), and
normalized k-anonymity N KLO-ATT (u, t). We see that the en-
tropy is more correlated to the adversary’s incorrectness than
k-anonymity is. However, both entropy and k-anonymity
misestimate the true location privacy of users.
Let us focus on Figure 5(a). All but few of the points fall
into the “N H  0.5) and in
some other cases (N K > 0.5 and LP < 0.5) overestimates
it. Hence, this is not an appropriate estimator for location
privacy either.
V. RELATED WORK
There are several papers in the ﬁeld of location privacy
that aim at clarifying the way to effectively protect users’
location privacy by classifying the problems and studying
various unaddressed issues and missing elements in this ﬁeld
of research. We will discuss these papers in the beginning
of this section. These papers cover a range of different
concerns, but highlight the following two urgent topics:
Krumm [14] studies various computational location pri-
vacy schemes: those that can be formally speciﬁed and
quantitatively measured. The authors regard the accuracy
of location privacy metrics as the key factor in the progress
of computational location privacy, and emphasize the im-
portance of ﬁnding a single (or a small set of sufﬁcient)
quantiﬁer for location privacy.
Decker [6] gives an overview of location privacy threats
and studies the effects of various countermeasures on pro-
tecting location privacy. The author also discusses which
protection mechanisms (such as obfuscation, anonymization)
are appropriate for different location-based services, consid-
ering the speciﬁcation and requirements of those services.
Shokri et al. [21], [22] survey various LPPMs and also
the metrics used for measuring location privacy (called
uncertainty-based, error-based and k-anonymity). The au-
thors compare various metrics qualitatively and show that
metrics such as entropy and k-anonymity are inadequate for
measuring location privacy. The authors rely on a number
of common-sense examples to justify the results.
Duckham [7] proposes a few rules as the key principles
of research on location privacy, which make this ﬁeld of
research different from other research topics in privacy. The
author refers to the predictable mobility of humans, the
constraints of the area within which people move, the effects
of location-based applications on privacy, the effectiveness
of centralized vs. distributed protection mechanisms and,
last but not least, the importance of a formal deﬁnition of
fundamental terms (such as the precision and accuracy of
information) in the design of protection mechanisms.
All the above-mentioned papers, of course, have been a
source of inspiration for our research in this paper. However,
despite the fact that we share common concerns (especially
the two emphasized items in the beginning of this Section)
neither these papers, nor any other paper we know about,
provide a framework with which LPPMs can be evaluated
quantitatively. Our work is a realization of the goals and
concerns of the research community and provides a modular
platform every part of which can be separately analyzed
and be improved upon; for example, by simulating more
powerful attacks using other inference techniques.
Other papers related to our work implement particular
attacks to show the predictability and uniqueness of users’
location traces, and some of them evaluate the efﬁcacy of
speciﬁc protection mechanisms. Each paper uses a different
model to state the problem and evaluate location privacy. In
spite of this diversity, this provides us with tools that can
potentially be used in a generic framework.
260
A prominent example of such papers is [15], in which
Liao et al. propose a framework for recognizing mobile
users’ activities based on the places they visit and also
the temporal patterns of their visit. The authors develop
an inference technique based on Markov Chain Monte
Carlo (MCMC) methods and show how users’ activities are
dependent on their mobility traces. The paper does not talk
about the consequences of these techniques, if used by an
adversary, on users’ privacy. However, it shows the relation
between location privacy (i.e., to what extent a user’s identity
is unlinkable to a location) and the general privacy of mobile
users (e.g., their activities and habits). Thus, it explains
the value of protecting mobile users’ location-privacy for
preventing the loss of their general privacy.
Other papers deﬁne the users’ (location) privacy as the
extent to which the users’ names (real identities) can be
derived from their traces. In our terms, they address “what
is the likelihood that an anonymous trace belongs to a given
user.” In fact, the results show the uniqueness of users’
mobility patterns.
Bettini et al. [2] state that location traces can act as
quasi-identiﬁers of mobile users and lead to identiﬁcation
of anonymous traces. Hence, they propose a k-anonymity
method to protect users’ anonymity.
Hoh et al. [12] and Krumm [13] focus on ﬁnding users’
identities based on their home addresses. Hence, they run
some inference attacks on location traces to ﬁnd the home
address of the user to which the trace belongs. The effec-
tiveness of various protection mechanisms such as spatial
cloaking (hiding), noise (perturbation), and rounding (reduc-
ing precision) on foiling these attacks are also evaluated.
Mulder et al. [5] show that anonymous location traces,
even at a low space granularity (i.e., at the level of the size
of the GSM cells) and spanning a short time period (a few
hours), can be re-identiﬁed, given the mobility proﬁles of
the individuals.
Golle and Partridge [10] discuss the anonymity of
home/work location pairs. The authors show that knowing
home and work addresses is enough to de-anonymize the
location traces of most of the users (especially in the United
States, where they obtained their results). Freudiger et al.
[9] use more advanced clustering algorithms to show mobile
users’ privacy-erosion over time as they make use of various
types of location-based services.
In the same vein of the previous works, Ma et al.
[16]
show that published anonymous mobility traces can be iden-
tiﬁed using statistical inference methods such as maximum
likelihood estimators, if the adversary has access to some
samples of those traces with known user names.
Note that
these papers in general only highlight
the
vulnerability of location traces to de-anonymization by an
adversary with access to different types of information.
However, there are very few research contributions where
the authors focus on how traceable a user is; that is, the
261
extent to which the adversary can correctly reconstruct a
complete trace from partial fragments. An example of this
line of investigation is [11], in which Hoh and Gruteser
propose a tracking attack based on multi-target tracking al-
gorithms [19] (using a Kalman ﬁlter) can help the adversary
to link different pieces of a user’s anonymous trace. The
authors propose a path confusion method in which traces
of different users are perturbed to create confusion in the
tracking algorithm. They also formulate an optimization
problem to solve the tradeoff between location privacy and
usefulness of the perturbed traces.
In our paper, as opposed to the enumerated related work,
we jointly consider obfuscation and anonymization methods
and develop generic attacks that can be used against any
LPPM. The framework we propose in this paper enables us
to formalize and evaluate various LPPMs. To the best of our
knowledge, the Location-Privacy Meter is the ﬁrst generic
tool developed to evaluate location privacy of location traces.
Finally, we should mention that modeling and formalizing
evaluation frameworks for privacy has recently been the
focus of researchers in other domains. Good examples of this
movement are differential privacy (for databases, typically)
proposed by Dwork [8], a framework to evaluate anonymity
protocols by Chatzikokolakis et al. [3], an evaluation frame-
work for MIX networks by Troncoso and Danezis [4], [24],
and a privacy model for RFIDs by Vaudenay [25].
For a more in-depth survey of various privacy-preserving
methods, metrics and attacks in the location-privacy litera-
ture, the reader is referred to [14], [21], [23].
ACKNOWLEDGMENT
The authors would like to thank George Danezis, Julien
Freudiger and Prateek Mittal for their insightful discussions
on the earlier versions of the framework, Mathias Humbert
and Mohamed Kafsi for their valuable comments on the
submitted manuscript, and also Vincent Bindschaedler for
helping us in the development of the Location-Privacy Meter.
VI. CONCLUSION
In this paper, we have raised the questions “what is loca-
tion privacy?” and “how can location privacy be quantiﬁed,
given an adversary model and a protection mechanism?”
In order to address these questions, we have established a
framework in which various entities, which are relevant to
location privacy of mobile users, have been formally deﬁned.
The framework enables us to specify various LPPMs and
attacks. Within this framework, we were also able to unravel
various dimensions of the adversary’s inference attacks. We
formally justify that the incorrectness of the adversary in
his inference attack (i.e., his expected estimation error)
determines the location privacy of users.
We have developed an operational tool, named Location-
Privacy Meter, as a realization of our framework. A designer
of an LPPM can easily specify and integrate her algorithm
in this tool for evaluation. Relying on well-established
statistical methods, we have implemented a generic attack
that can be used to answer all sorts of information disclosure
questions. We have also developed some speciﬁc attacks,
such as localization attacks, that are more targeted and hence
more time-efﬁcient.
As a follow-up to this work, we will add new modules
with which we can support pseudonym changes over time
for users, in order to capture all possible LPPM algorithms.
We would also like to incorporate the location-based appli-
cations into the framework and analyze the effectiveness of
LPPMs with respect to these applications.
REFERENCES
[1] Location-Privacy Meter
tool.
Available online through
http://people.epﬂ.ch/reza.shokri, 2011.
[2] C. Bettini, X. S. Wang, and S. Jajodia. Protecting privacy
In In 2nd
location-based personal
against
VLDB Workshop SDM, pages 185–199, 2005.
identiﬁcation.
[3] K. Chatzikokolakis, C. Palamidessi, and P. Panangaden.
Anonymity protocols as noisy channels.
In Proceedings of
the 2nd international conference on Trustworthy global com-
puting, TGC’06, pages 281–300, Berlin, Heidelberg, 2007.
Springer-Verlag.
[4] G. Danezis and C. Troncoso. Vida: How to use bayesian
inference to de-anonymize persistent communications.
In
Proceedings of the 9th International Symposium on Privacy
Enhancing Technologies, PETS ’09, pages 56–72, Berlin,
Heidelberg, 2009. Springer-Verlag.
[5] Y. De Mulder, G. Danezis, L. Batina, and B. Preneel. Iden-
tiﬁcation via location-proﬁling in gsm networks.
In WPES
’08: Proceedings of the 7th ACM workshop on Privacy in the
electronic society, pages 23–32, New York, NY, USA, 2008.
[6] M. Decker. Location privacy-an overview.
In ICMB ’08:
Proceedings of the 2008 7th International Conference on
Mobile Business, pages 221–230, Washington, DC, USA,
2008. IEEE Computer Society.
[7] M. Duckham. Moving forward: location privacy and location
awareness.
In Proceedings of the 3rd ACM SIGSPATIAL
International Workshop on Security and Privacy in GIS and
LBS, SPRINGL ’10, pages 1–3, New York, NY, USA, 2010.
[8] C. Dwork. Differential Privacy. In M. Bugliesi, B. Preneel,
V. Sassone, and I. Wegener, editors, Automata, Languages and
Programming, volume 4052, chapter 1, pages 1–12. Springer
Berlin Heidelberg, Berlin, Heidelberg, 2006.
[9] J. Freudiger, R. Shokri, and J.-P. Hubaux. Evaluating the
In Financial Cryp-
privacy risk of location-based services.
tography and Data Security (FC), 2011.
[10] P. Golle and K. Partridge. On the anonymity of home/work
location pairs. In Pervasive ’09: Proceedings of the 7th Inter-
national Conference on Pervasive Computing, pages 390–397,
Berlin, Heidelberg, 2009. Springer-Verlag.
262
[11] B. Hoh and M. Gruteser. Protecting location privacy through
path confusion. In SECURECOMM ’05: Proceedings of the
First International Conference on Security and Privacy for
Emerging Areas in Communications Networks, pages 194–
205, Washington, DC, USA, 2005. IEEE Computer Society.
[12] B. Hoh, M. Gruteser, H. Xiong, and A. Alrabady. Enhancing
IEEE
security and privacy in trafﬁc-monitoring systems.
Pervasive Computing, 5(4):38–46, 2006.
[13] J. Krumm. Inference attacks on location tracks. In In Pro-
ceedings of the Fifth International Conference on Pervasive
Computing (Pervasive), volume 4480 of LNCS, pages 127–
143. Springer-Verlag, 2007.
[14] J. Krumm. A survey of computational
location privacy.
Personal Ubiquitous Comput., 13(6):391–399, 2009.
[15] L. Liao, D. J. Patterson, D. Fox, and H. Kautz. Learning and
inferring transportation routines. Artif. Intell., 171:311–331,
April 2007.
[16] C. Y. Ma, D. K. Yau, N. K. Yip, and N. S. Rao. Privacy
vulnerability of published anonymous mobility traces.
In
Proceedings of the sixteenth annual international conference
on Mobile computing and networking, MobiCom ’10, pages
185–196, New York, NY, USA, 2010. ACM.
[17] M. Piorkowski, N. Saraﬁjanovic-Djukic, and M. Grossglauser.
CRAWDAD data set epﬂ/mobility (v. 2009-02-24). Down-
loaded from http://crawdad.cs.dartmouth.edu/epﬂ/mobility.
[18] L. Rabiner. A tutorial on hidden Markov models and selected
applications in speech recognition. Proceedings of the IEEE,
77(2):257–286, 1989.
[19] D. Reid. An algorithm for tracking multiple targets. IEEE
Transactions on Automatic Control, 24(6):843–854, 1979.
[20] C. Robert, G. Celeux, and J. Diebolt. Bayesian estimation of
hidden Markov chains: A stochastic implementation. Statis-
tics & Probability Letters, 16(1):77–83, 1993.
[21] R. Shokri, J. Freudiger, and J.-P. Hubaux. A uniﬁed frame-
In 3rd Hot Topics in Privacy
work for location privacy.
Enhancing Technologies (HotPETs), 2010.
[22] R. Shokri, J. Freudiger, M. Jadliwala, and J.-P. Hubaux. A
In WPES ’09:
distortion-based metric for location privacy.
Proceedings of the 8th ACM workshop on Privacy in the
electronic society, pages 21–30, New York, NY, USA, 2009.
[23] R. Shokri, C. Troncoso, C. Diaz, J. Freudiger, and J.-P.
Hubaux. Unraveling an old cloak: k-anonymity for location
privacy. In Proceedings of the 9th annual ACM workshop on
Privacy in the electronic society, WPES ’10, pages 115–118,
New York, NY, USA, 2010. ACM.
[24] C. Troncoso and G. Danezis. The bayesian trafﬁc analysis of
mix networks. In Proceedings of the 16th ACM conference
on Computer and communications security, CCS ’09, pages
369–379, New York, NY, USA, 2009. ACM.
[25] S. Vaudenay. On privacy models for rﬁd. In Proceedings of
the Advances in Crypotology 13th international conference
on Theory and application of cryptology and information
security, ASIACRYPT’07, pages 68–87, 2007.