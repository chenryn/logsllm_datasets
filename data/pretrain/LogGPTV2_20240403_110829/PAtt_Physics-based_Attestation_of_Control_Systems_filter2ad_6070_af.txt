for computing platforms: Caring about properties, not
mechanisms. In Proceedings of the 2004 Workshop on
New Security Paradigms, NSPW ’04, pages 67–77, New
York, NY, USA, 2004. ACM.
[50] A.-R. Sadeghi, I. Visconti, and C. Wachsmann. En-
hancing rﬁd security and privacy by physically unclon-
able functions. In Towards Hardware-Intrinsic Security,
pages 281–305. Springer, 2010.
[51] S. Schüppen, D. Teubert, P. Herrmann, U. Meyer, and
S. Sch. Fanci: feature-based automated nxdomain clas-
siﬁcation and intelligence. In Proceedings of the 27th
USENIX Conference on Security Symposium, pages
1165–1181. USENIX Association, 2018.
[52] A. Seshadri, M. Luk, A. Perrig, L. V. Doorn, and
P. Khosla. SCUBA: Secure Code Update By Attestation
in Sensor Networks. In ACM WiSec, 2006.
[53] A. Seshadri, M. Luk, A. Perrig, L. van Doorn, and
P. Khosla. Using FIRE & ICE for Detecting and Recov-
ering Compromised Nodes in Sensor Networks. Techni-
cal report, DTIC Document, 2004.
[54] A. Seshadri, M. Luk, E. Shi, A. Perrig, L. Van Doorn,
and P. Khosla. Pioneer: Verifying Code Integrity and
Enforcing Untampered Code Execution on Legacy Sys-
tems. In ACM SIGOPS OSR, 2005.
[55] U. Shankar, M. Chew, and J. D. Tygar. Side effects
are not sufﬁcient to authenticate software. In USENIX
Security, May 2004.
[56] A. K. Sikder, H. Aksu, and A. S. Uluagac. 6thsense:
A context-aware sensor-based attack detector for smart
devices. In USENIX Security, 2017.
[57] R. P. Stanley. Enumerative Combinatorics, volume 1.
Cambridge University Press, second edition, 2012. pp.
64-67.
[58] K. Stouffer, J. Falco, and K. Scarfone. Guide to in-
dustrial control systems (ICS) security. NIST special
publication, 800(82):16–16, 2011.
[59] R. Strackx, F. Piessens, and B. Preneel. Efﬁcient Isola-
tion of Trusted Subsystems in Embedded Systems. In
SecureComm. Springer, 2010.
[60] D. I. Urbina, J. A. Giraldo, A. A. Cardenas, N. O. Tip-
penhauer, J. Valente, M. Faisal, J. Ruths, R. Candell, and
H. Sandberg. Limiting the impact of stealthy attacks on
industrial control systems. In Proceedings of the 2016
ACM SIGSAC Conference on Computer and Communi-
cations Security, pages 1092–1105. ACM, 2016.
[61] J. Valente, C. Barreto, and A. A. Cárdenas. Cyber-
physical systems attestation.
In 2014 IEEE Interna-
tional Conference on Distributed Computing in Sensor
Systems (DCOSS), pages 354–357. IEEE, 2014.
[62] O. Willers, C. Huth, J. Guajardo, and H. Seidel. Mems
gyroscopes as physical unclonable functions. In Pro-
ceedings of the 2016 ACM SIGSAC Conference on Com-
puter and Communications Security, pages 591–602.
ACM, 2016.
[63] I. H. Witten, E. Frank, M. A. Hall, and C. J. Pal. Data
Mining: Practical machine learning tools and tech-
niques. Morgan Kaufmann, 2016.
10 Appendices
10.1 The Aging Effect on Classiﬁcation Per-
formance
The aging effect was reported in many PUF applications at the
semiconductor level [38, 46]. However, such an effect is not
common in mechanical actuators (the preciseness guarantee
of the actuators could be found in the actuators data-sheet).
We evaluated the aging effect by considering two data-sets of
6 months ago (old data-set) and a recent data-set (recent data-
set). As we could see in Table 4 the tuned classiﬁer would
provide better classiﬁcation results. We would recommend
performing the tuning of the classiﬁer by including the re-
cent normal traces during the idle time of the CPS. However,
the performance of the classiﬁer that is built by training the
old data-set is sufﬁcient for the robot arm use-case that we
evaluated in this paper. Figure 6 shows the true positive rate
178          22nd International Symposium on Research in Attacks, Intrusions and DefensesUSENIX Associatione
t
a
R
e
v
i
t
i
s
o
P
e
u
r
T
1
0.8
0.6
0.4
0.2
0
0
1
0.99
0.98
0.97
0.96
0.95
0.94
0
0.005
0.015
0.02
0.01
0.4
0.6
0.2
False Positive Rate
RF-Old
MLP-Old
RF-New
MLP-New
0.025
0.8
1
Figure 6: ROC curve of true positive rate (sensitivity) against
false positive rate (1-precision), considering the aging effect.
(sensitivity) against the false positive rate (1-precision) in
aging effect evaluation. As we could see in Figure 6, the clas-
siﬁcation performance of the classiﬁers with the two data-set
of old and recent sensor traces are close to each other.
10.2 Performance Metrics
To evaluate the performance of the proposed method, we used
eight performance metrics. The true positive (TP) is the num-
ber of retrieved relevant instances. The false positive (FP) is
the number of retrieved nonrelevant instances. The true nega-
tive (TN) is the number of not retrieved nonrelevant instances.
The false negative (FN) is the number of not retrieved rele-
vant instances. The Sensitivity rate (Recall, eq. 6) presents
the rate of retrieved relevant instances (TP) in overall relevant
instances (TP + FN). The Precision rate (speciﬁcity, eq. 7)
demonstrate the fraction of relevant instances (TP) in overall
retrieved instances (TP + FP).
Sensitivity rate =
T P
T P + FN
Precision rate = T P
T P + FP
(6)
(7)
relevant instances (FN) in overall relevant instances (FN +
TP). The false discovery rate (eq. 10) is the rate of retrieved
nonrelevant instances (FP) in overall retrieved instances (FP
+ TP).
False positive rate =
FP
FP + T N
False negative rate = FN
FN + T P
False discovery rate = FP
FP + T P
(8)
(9)
(10)
The accuracy (eq. 11) is the rate of retrieved relevant in-
stances and not retrieved nonrelevant instances (TP + TN) in
overall instances (TP + TN + FP + FN).
Accuracy =
T P + T N
T P + T N + FP + FN
(11)
The F1-score (eq. 12) is a metric for the test’s accuracy. The
F1-score (also F-score or F-measure) is deﬁned as follows:
F1− score = 2× Sensivity× Precision
Sensivity + Precision
(12)
The Matthews correlation coefﬁcient (MCC) is a metric for
the quality of two-class classiﬁcation. The MCC metric is one
of the most interesting metrics in anomaly detection where
the physical feature will be classiﬁed to normal and abnormal
classes. The MCC is deﬁned as follows:
MCC =
T P× T N − FP× FN
(T P + FP)× (T P + FN)× (T N + FP)× (T N + FN)
(cid:2)
10.3 Decoding ROC
Our ROC (true positive rate (sensitivity) against the false posi-
tive rate (1-precision)) for the decoding classiﬁers is presented
in Figure 7.
10.4 An Example of Path Strategy
Figure 8 represents the path strategy of 10100010, and the
path strategy of 00000111. We know that the number of
unique paths u in a x×y grid can be computed as follows [57]:
u =
(x + y)!
(x!y!)
(13)
The false positive rate (eq. 8) is the rate of retrieved non-
relevant instances (FP) in overall nonrelevant instances (FP +
TN). The false negative rate (eq. 9) is the rate of not retrieved
Thus, we can enumerate all possible paths, and use an
integer between 1 and u as an index to represent a speciﬁc
path strategy in a x× y grid.
USENIX Association        22nd International Symposium on Research in Attacks, Intrusions and Defenses 179Table 4: Performance comparison of different classiﬁers with different metrics of Sensitivity & Speciﬁcity & Precision & FPR
& FNR & Accuracy & F1-score & MCC (Matthews Correlation Coefﬁcient), classiﬁers: Random Forest (RF) & Multilayer
Perceptron (MLP).
Algorithms
Sensitivity
Speciﬁcity
Precision
FPR
FNR Accuracy
F1-score MCC
RF (old)
MLP (old)
RF (recent)
MLP (recent)
0.9857
0.9952
0.9912
0.9881
0.9913
0.9826
0.9897
0.9905
0.9912
0.9827
0.9897
0.9904
0.0087
0.0174
0.0103
0.0095
0.0143
0.0048
0.0088
0.0119
0.9885
0.9889
0.9905
0.9893
0.9884
0.9889
0.9905
0.9892
0.9770
0.9779
0.9810
0.9786
e
t
a
R
e
v
i
t
i
s
o
P
e
u
r
T
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
1
0.995
0.99
0.985
0.98
0.975
0.97
0
0
0
0.2
DTNB
FURIA
J48
LMT
MLP
NBTree
PART
REPTree
RF
0.8
1
0.01
0.02
0.03
0.04
0.4
0.6
False Positive Rate
Figure 7: ROC curve of true positive rate (sensitivity) against
false positive rate (1-precision).
(cid:122)
(cid:121)
Figure 8: Two examples of path strategies: red=10100010,
dashed green=00000111.
180          22nd International Symposium on Research in Attacks, Intrusions and DefensesUSENIX Association