profiling (taint)
sw emulation
code+data
no
yes
yes
yes
yes
no restrictions
user annotations
shadow accesses
annot. + static analysis
read/write accesses
annot. + static analysis
read/write accesses
user annotations
shadow, safe read/write accesses
fixed-depth**
fixed-depth
** unimplemented
* cache line with preloading
Thus, we investigated different alternatives that could avoid rely-
ing on condition flags that, besides, get clobbered in the process and
may require frequent recomputation. While mask-based schemes
3 and 4 seemed at first the most promising avenues, it turned out
that the additional operation needed either to generate the taken
mask from a boolean condition (scheme 3), or to maintain it at
run-time and combining it with the boolean conditions coming
from branch decisions (scheme 4), made these schemes suboptimal.
Scheme 5 resulted in the most simple and most efficient one be-
tween the solutions we tested, producing the lowest overhead as it
unleashes several arithmetic optimizations (e.g. peephole, global
value numbering) at IR and backend optimization levels.
C STRIDING
One of the key performance enablers that back our radical approach
is the ability to stride over object fields efficiently. We thoroughly
tested different possible implementations, and designed different
solutions based on the size of the field that should be strode and the
granularity λ at which the attacker could observe memory accesses.
Several of these solutions leverage CPU SIMD Extensions: while we
focused on AVX2 and AVX512 instructions for x86 architectures, the
approach can easily be extended to other architectures supporting
vectorization extensions (e.g., ARM SVE [66] , RISC-V “V” [4]).
We group our solutions in three categories: simple object striding,
vector gather/scatter operations, and vector bulk load/stores.
Simple Object Striding. Given an access on pointer ptr over some
field f, the most simple solution is to just access linearly f at ev-
ery λ-th location. We perform each access using a striding pointer
s_ptr at the offset ptr % λ of the λ-th location, so that while strid-
ing a field s_ptr will match the target pointer ptr exactly once,
on the location where the memory access should happen. For load
operations we conditionally maintain the value loaded from mem-
ory, propagating only the real result over the striding, while for
store operation we load every value we access, conditionally updat-
ing it at the right location (§4.3.1). We report a simplified1 snippet
of a striding load operation for a uint8_t, where the conditional
assignment is eventually realized e.g. using a cmov operation:
uint8_t ct_load(uint8_t* field, uint8_t* field_end, uint8_t* ptr) {
// Default result
uint8_t res = 0;
1Additional, constant-time logic is present in the implementation to deal with corner
cases, so to avoid striding outside the object if not aligned correctly in memory.
// Get the offset of the pointer with respect to LAMBDA
uint64_t target_lambda_off = ((uint64_t) ptr) & (LAMBDA-1);
// Iterate over the possible offsets
for(uint8_t* s_ptr = field; s_ptr 
%p = getelementptr %struct.fp_int, %struct.fp_int %v, %i32 0, %i32 1
The syntax of a GEP instruction is as follows. The first argument
is the type, the second is the base address for the computation, and
the subsequent ones are indices for operating on the elements of
aggregate types (i.e. structures or arrays). The first index operates
on the base address pointer, and any subsequent index would oper-
ate on the pointed-to expressions. Here %p takes the address of the
second i32 field of a fp_int structure. What happens with SVF is
that it frequently reports a whole abstract object ecc_key in the
points-to set for %p: this information if used as-is would require
DFL to access all the 4416 object bytes during linearization.
Starting from this coarse-grained information, our technique
identifies which portions of a large object could accommodate the
pointer computation. In this simple example, we have that ecc_key
can host one fp_int as outer member (last field), and three more
through its ecc_key member (second-last). Hence we refine pointer
metadata to set of each second i32 field from these objects, and only
four 4-byte locations now require access during DFL. In general,
we follow the flow of pointer value computations and determine
object portions suitability for such dereferencing as in duck typing.
E RECURSION AND THREAD SAFETY
Our implementation is lackluster in two respects that we could
address with limited implementation effort, which we leave to
future work as the programs we analyzed did not exercise them.
The first concerns handling recursive constructs. Direct recur-
sion is straightforward: we may predict its maximal depth with
profiling and apply the just-in-time linearization scheme seen for
loops, padding recursive sequences with decoy calls for depths
shorter than the prevision, using a global counter to track depths.
For indirect recursion, we may start by identifying the functions
involved in the sequence, as they would form a strongly connected
component on the graph. Then we may apply standard compiler
construction techniques, specifically the inlining approach of [83]
to convert indirect recursion in direct recursion, and apply the
just-in-time linearization scheme discussed above.
The second concerns multi-threading. As observed by the au-
thors of Raccoon [53], programs must be free of data races for
sensitive operations. The linearized code presently produced by
Constantine is not re-entrant because of stack variable promotion
(§4.3.3) and for the global variable we use to expose the current
taken predicate to called functions. On the code transformation side,
an implementation extension may be to avoid such promotion, then
use thread-local storage for the predicate, and update the doubly
linked lists for allocation sites atomically. As for DFL handlers, we
may implement DFL handlers either using locking mechanisms, or
moving to more efficient lockless implementations using atomic
operations or, for non-small involved sizes (§C), TSX transactions.
F CORRECTNESS
We discuss an informal proof of correctness of the Constantine
approach. As we anticipated in §5, to prove that our programs are
semantically equivalent to their original representations, we break
the claim into two parts: control-flow correctness and data-flow
Session 3A: Side Channel CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea731Table 5: Run-time overhead analysis of different Constantine configurations (i.e., λ = 1, AVX2). The numbers for SC-
Eliminator and Soares et al. were obtained from executing the publicly available artifact evaluation material for their papers.
program
aes
des
des3
anubis
cast5
cast6
fcrypt
khazad
aes_core
cast-ssl
aes
cast128
des
kasumi
seed
twofish
3way
des
loki91
camellia
des
seed
twofish
binsearch
dijkstra
findmax
histogram
matmul
rsort
aes
arc4
blowfish
cast
des3
tls-rempad-luk13
aes_big
des_tab
geomean (total)
chronos
s-cp
botan
app-cr
libgcrypt
raccoon
pycrypto
B/Rel
AVX512 (λ = 1)
1.13
1.12
1.49
1.29