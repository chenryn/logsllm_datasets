![没有优化：冷区数据触发缺页中断](/assets/20227914738.webp)
优化之后：由于大部分查询集中在索引项尾部，所以把后半部分设置为热区，永远保存在缓存中，如果查询目标偏移量在热区索引项范围，直接查热区，避免页中断
### 主题删除
主题删除过程中，首先会通过复制状态机机制，向 Controller 发送通知，调整主题的所有副本状态，然后移除 zk、controller 关于该主题的所有元数据，最后执行物理磁盘文件的删除操作
### 副本状态机
副本的 7 种状态：
- NewReplica：副本被创建之后所处的状态
- OnlineReplica：副本正常提供服务时所处的状态
- OfflineReplica：副本服务下线时所处的状态
- ReplicaDeletionStarted：副本被删除时所处的状态
- ReplicaDeletionSuccessful：副本被成功删除后所处的状态
- ReplicaDeletionIneligible：开启副本删除，但副本暂时无法被删除时所处的状态
- NonExistentReplica：副本从副本状态机被移除前所处的状态
```mermaid
stateDiagram-v2
  NewReplica --> OfflineReplica: broker下线
  NewReplica --> OnlineReplica: 初始化之后
  OnlineReplica --> OfflineReplica: broker下线/broker重新上线
  OfflineReplica --> OnlineReplica: broker下线/broker重新上线
  OfflineReplica --> OfflineReplica
  OnlineReplica --> OnlineReplica: leader 副本变更
  OfflineReplica --> ReplicaDeletionStarted: 删除副本对象
  ReplicaDeletionStarted --> ReplicaDeletionSuccessful: 删除副本成功
  ReplicaDeletionStarted --> ReplicaDeletionIneligible: 删除副本失败
  ReplicaDeletionIneligible --> OnlineReplica
  ReplicaDeletionIneligible --> OfflineReplica: 重试副本删除
  ReplicaDeletionSuccessful --> NonExistentReplica: 副本对象被移出副本状态机
  NonExistentReplica --> NewReplica: 副本对象新创建
```
当 Controller 接受到状态变更请求时，首先就是判断操作是否有效，无效需要记录一条失败日志，有效则执行对应的操作、变更相关的元数据
### 分区状态机
- NewPartition：分区被创建后被设置成这个状态，表明它是一个全新的分区对象。处于这个状态的分区，被 Kafka 认为是“未初始化”，因此，不能选举 Leader
- OnlinePartition：分区正式提供服务时所处的状态
- OfflinePartition：分区下线后所处的状态
- NonExistentPartition：分区被删除，并且从分区状态机移除后所处的状态
```mermaid
stateDiagram-v2
  NewPartition --> OnlinePartition: broker启动或新分区初始化
  OnlinePartition --> OnlinePartition: 分区选举leader
  OnlinePartition --> OfflinePartition: broker下线或主题被删除
  OfflinePartition --> OnlinePartition: 分区选举leader
  OfflinePartition --> NonExistentPartition: 主题被成功删除
  NonExistentPartition --> NewPartition: 新分区创建
```
Leader 选举策略：当由于某种原因，Leader 下线了，需要根据不同情况来选举 Leader
```scala
// 离线分区Leader选举策略
final case class OfflinePartitionLeaderElectionStrategy(allowUnclean: Boolean) extends PartitionLeaderElectionStrategy
// 分区副本重分配Leader选举策略  
final case object ReassignPartitionLeaderElectionStrategy extends PartitionLeaderElectionStrategy
// 分区Preferred副本Leader选举策略
final case object PreferredReplicaPartitionLeaderElectionStrategy extends PartitionLeaderElectionStrategy
// Broker Controlled关闭时Leader选举策略
final case object ControlledShutdownPartitionLeaderElectionStrategy extends PartitionLeaderElectionStrategy
```
这几个策略几乎都是选择当前副本有序集合中的、首个处于 ISR 集合中的存活副本作为新的 Leader
当要变更分区状态，就由 Controller 发送相关消息给 broker 们，再由 broker 来执行对每个分区的元数据变更
### 消费者组管理
```mermaid
stateDiagram-v2
  Empty --> PreparingRebalance: 首个成员加入组
  PreparingRebalance --> Empty
  PreparingRebalance --> CompletingRebalance: 所有成员加入组
  CompletingRebalance --> PreparingRebalance: 新成员加入组或已有成员退出组
  CompletingRebalance --> Stable: leader成员指定好方案
  CompletingRebalance --> Dead
  Dead --> Dead
  Stable --> Dead
  Empty --> Dead
  PreparingRebalance --> Dead
  Stable --> PreparingRebalance: 新成员加入组或已有成员退出组
```
## 集群成员关系
broker通过创建临时节点把自己的 ID 注册到 Zookeeper
- 控制器：一个特殊的broker 通过在zk创建临时节点进行选举。控制器负责在节点加入或离开集群时进行分区首领选举，控制器使用epoch 来避免“脑裂”。当临时节点被释放或者内容发生更新，监听临时节点的其他 broker 就会收到通知，进行新一轮的选举。2.8 之后，Kafka 移除了对 zk 的依赖，使用 QuorumController 来实现元数据的管理
```scala
// 集群元数据
class ControllerContext {
  val stats = new ControllerStats // Controller统计信息类 
  var offlinePartitionCount = 0   // 离线分区计数器
  val shuttingDownBrokerIds = mutable.Set.empty[Int]  // 关闭中Broker的Id列表
  private val liveBrokers = mutable.Set.empty[Broker] // 当前运行中Broker对象列表
  private val liveBrokerEpochs = mutable.Map.empty[Int, Long]   // 运行中Broker Epoch列表
  var epoch: Int = KafkaController.InitialControllerEpoch   // Controller当前Epoch值
  var epochZkVersion: Int = KafkaController.InitialControllerEpochZkVersion  // Controller对应ZooKeeper节点的Epoch值
  val allTopics = mutable.Set.empty[String]  // 集群主题列表
  val partitionAssignments = mutable.Map.empty[String, mutable.Map[Int, ReplicaAssignment]]  // 主题分区的副本列表
  val partitionLeadershipInfo = mutable.Map.empty[TopicPartition, LeaderIsrAndControllerEpoch]  // 主题分区的Leader/ISR副本信息
  val partitionsBeingReassigned = mutable.Set.empty[TopicPartition]  // 正处于副本重分配过程的主题分区列表
  val partitionStates = mutable.Map.empty[TopicPartition, PartitionState] // 主题分区状态列表 
  val replicaStates = mutable.Map.empty[PartitionAndReplica, ReplicaState]  // 主题分区的副本状态列表
  val replicasOnOfflineDirs = mutable.Map.empty[Int, Set[TopicPartition]]  // 不可用磁盘路径上的副本列表
  val topicsToBeDeleted = mutable.Set.empty[String]  // 待删除主题列表
  val topicsWithDeletionStarted = mutable.Set.empty[String]  // 已开启删除的主题列表
  val topicsIneligibleForDeletion = mutable.Set.empty[String]  // 暂时无法执行删除的主题列表
  ......
}
```
Controller 是用来管理整个集群的，它会向其他 broker 发送三类请求：
1. LeaderAndIsrRequest：告诉 Broker 相关主题各个分区的 Leader 副本位于哪台 Broker 上、ISR 中的副本都在哪些 Broker
2. StopReplicaRequest：告知指定 Broker 停止它上面的副本对象，这个请求主要的使用场景是分区副本迁移和删除主题
3. UpdateMetadataRequest：更新 Broker 上的元数据缓存
## 复制
- leader 副本
  - 所有生产者请求和消费者请求都会经过这个副本
- follower 副本
  - 从 leader 那里复制消息，保持与 leader 一致的状态
### 副本管理
follower 会启动一个线程，不断执行以下操作：
- 有必要时，对当前的 follower 副本做截断操作（因为可能发生 leader 切换）
- 向 leader 发起副本读取请求，接受到数据并进行处理
除了副本同步直接操作分区对象，生产者向 Leader 副本写入消息、消费者组写入组信息、事务管理器写入事务信息（包括事务标记、事务元数据等）这三种操作会通过 ReplaceManager 向副本写入数据
而消费者的读取请求，也会通过 ReplaceManager 来确定读取范围，再从底层的日志读取消息构建结果并返回
副本管理还会根据接收到的请求，决定是否将当前副本提升为 leader 副本，同时，还会有一个线程定时检测当前副本与 leader 副本的滞后时间，当滞后时间超过 replica.lag.time.max.ms，将该副本移除出 ISR
## 请求处理
```mermaid
stateDiagram-v2
  clients --> processor线程
  processor线程 --> clients
  state broker {
    state processor线程 {
      processor1
      processor2
      processor3
    }
    processor线程 --> 请求队列
    请求队列 --> IO线程
    state IO线程 {
      IO线程1
      IO线程2
      IO线程3
    }
    state 响应队列 {
      processor1响应队列
      processor2响应队列
      processor3响应队列
    }
    IO线程 --> 响应队列
    响应队列 --> processor线程
  }
```
生产请求：
在消息被写入分区的首领之后，broker 开始检查 acks 配置参数——如果 acks 被设为 0 或 1 ，那么 broker 立即返回响应；如果 acks 被设为 all ，那么请求会被保存在一个叫作炼狱的缓冲区里，直到首领发现所有跟随者副本都复制了消息，响应才会被返回给客户端
获取请求：
broker 将按照客户端指定的数量上限从分区里读取消息，再把消息返回给客户端。Kafka 使用零复制技术向客户端发送消息(直接从文件系统缓存复制到网卡)，如果应用程序是从文件读出数据后再通过网络发送出去的场景，并且这个过程中不需要对这些数据进行处理，这种场景可以使用[零拷贝](/操作系统/输入输出.md#零拷贝)
```mermaid
sequenceDiagram
  消费者 ->> broker: 获取请求
  alt 积累足够多的消息
    生产者 ->> broker: 消息
    生产者 ->> broker: 消息
    生产者 ->> broker: 消息
  end
  broker ->> 消费者: 消息
```
所有同步副本复制了这些消息，才允许消费者读取它们
![屏幕截图 2020-08-21 144435](/assets/屏幕截图%202020-08-21%20144435.png)
### 监控指标
Kakfa 在 RequestChannel 内保存了一些关于请求的指标：
- RequestsPerSec：每秒处理的 Request 数，用来评估 Broker 的繁忙状态。
- RequestQueueTimeMs：计算 Request 在 Request 队列中的平均等候时间，单位是毫秒
- LocalTimeMs：计算 Request 实际被处理的时间。
- RemoteTimeMs：等待其他 Broker 完成指定逻辑的时间。
- TotalTimeMs：计算 Request 被处理的完整流程时间
## 物理存储
文件管理：
分区分成若干个片段 当前正在写入数据的片段叫作活跃片段
## 可靠数据传递
kafka 的保证：
- 分区消息的顺序
- 只有当消息被写入分区的所有同步副本时（但不一定要写入磁盘），它才被认为是“已提交”的
- 只要还有一个副本是活跃的，那么已经提交的消息就不会丢失
- 消费者只能读取已提交的消息
副本的同步保证：
- 与 Zookeeper 之间有一个活跃的会话，也就是说，它在过去的 6s（可配置）内向Zookeeper 发送过心跳
- 过去的 10s 内（可配置）从首领那里获取过消息
- 过去的 10s 内从首领那里获取过最新的消息
### broker
复制系数：
主题级别 replication.factor broker级别  default.replication.factor
如果复制系数为 N，那么在 N-1 个 broker 失效的情况下，仍然能够从主题读取数据或向主题写入数据，同时 它们也会占用N倍的磁盘空间、
不完全首领选举：
如果把 unclean.leader.election.enable 设为 true ，就是允许不同步的副本成为首领 就要承担丢失数据和出现数据不一致的风险
最少同步副本：
min.insync.replicas 如果要确保已提交的数据被写入不止一个副本，就需要把最少同步副本数量设置为大一点
### 生产者
发送确认：
acks：0  能够通过网络把消息发送出去，那么就认为消息已成功写入
1 ：意味着首领在收到消息并把它写入到分区数据文件（不一定同步到磁盘上）时
会返回确认或错误响应
all： 首领在返回确认或错误响应之前，会等待所有同步副本都收到消息
重试参数：
对于一些错误 可以通过重试来解决 如： LEADER_NOT_AVAILABLE
### 消费者
显示提交偏移量：
- 处理完事件再提交
- 批量提交
- 重试
- 维护状态
- 避免对消息处理时间过程 否则会造成无法及时发送心跳
- 仅一次传递
  - 暂时支持不了 使用幂等性写入来实现
## 数据管道
需要考虑的问题：
- 及时性
- 可靠性
  - 至少一次传递 仅一次传递
- 吞吐量要求
  - 高
  - 动态调整
- 数据格式与转换问题
- 安全性
  - 传输安全
  - 权限安全
- 故障处理
- 数据管道与上下游的耦合
### Connect
启动 connect: