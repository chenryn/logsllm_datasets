title:A Transparent Highway for inter-Virtual Network Function Communication
with Open vSwitch
author:Mauricio V&apos;asquez Bernal and
Ivano Cerrato and
Fulvio Risso and
David Verbeiren
A Transparent Highway for inter-Virtual Network
Function Communication with Open vSwitch
Mauricio Vásquez Bernal1, Ivano Cerrato1, Fulvio Risso1, David Verbeiren2
1Dept. of Computer and Control Engineering, Politecnico di Torino, Turin, Italy
2Tessares SA, Louvain-la-Neuve, Belgium, formerly at Intel Corporation NV/SA, Belgium
1{mauricio.vasquez, ivano.cerrato, fulvio.risso}@polito.it; PI:EMAIL
ABSTRACT
This paper presents a software architecture that can
dynamically and transparently establish direct commu-
nication paths between DPDK-based virtual network
functions executed in virtual machines, by recognizing
new point-to-point connections in traﬃc steering rules.
We demonstrate the huge advantages of this architec-
ture in terms of performance and the possibility to im-
plement it with localized modiﬁcations in Open vSwitch
and DPDK, without touching the VNFs.
CCS Concepts
•Networks → Middle boxes / network appliances;
Keywords
NFV; Open vSwitch; DPDK; performance
1.
INTRODUCTION
In Network Functions Virtualization (NFV), complex
services can be delivered by rearranging multiple Vir-
tual Network Functions (VNFs) in arbitrary graphs (Fig-
ure 1(a)), with multiple VNFs often executed on a single
physical server as distinct machines (VMs). This paper
presents a set of interacting software components that
optimize the inter-VNF communications by creating a
direct connection between two VMs, hence bypassing
the vSwitch when two VNFs are logically connected
through a point-to-point (p-2-p) link.
Diﬀerently from other proposals [1], we can acceler-
ate transparently and dynamically the packets exchange
between VMs on a widespread vSwitch. Transparency
refers to the possibility for an application to exploit the
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than ACM must be honored. Abstracting with credit is per-
mitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
SIGCOMM ’16, August 22-26, 2016, Florianopolis , Brazil
c(cid:13) 2016 ACM. ISBN 978-1-4503-4193-6/16/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2934872.2959068
Figure 1: Traﬃc crossing VNFs: (a) the service
graph; (b) its implementation on a server.
advantages of our technology without even knowing it
is there, and for an OpenFlow controller to attach to a
vSwitch without noticing it has been modiﬁed. Dynam-
icity refers to the capability to either create a direct
VM-to-VM channel or return to a traditional VM-to-
vSwitch-to-VM path on the ﬂy, based on the run-time
analysis of OpenFlow rules. Finally, our idea has been
integrated in a widespread vSwitch, particularly, it ex-
tends the version of OvS based on the Data Plane De-
velopment Kit (DPDK), and then it is oriented to opti-
mize connections between VMs executing DPDK-based
network applications, bringing its beneﬁts to the entire
class of the above VNFs.
2. PROTOTYPE ARCHITECTURE
Our DPDK-based applications run inside VMs con-
nected to the forwarding engine of OvS through dpdkr
ports; this module handles packets according to the
content of its forwarding table, which can be conﬁg-
ured with OpenFlow flowmods. dpdkr ports are imple-
mented using shared memory, and are exposed to the
VM through ivshmem devices; moreover, applications
access dpdkr ports using a poll mode driver (PMD).
As shown in Figure 2, our architecture modiﬁes the
dpdkr port to include a normal channel connected to
the OvS forwarding engine and the optional bypass chan-
nel that is directly connected to another VM. Also the
PMD has been modiﬁed, so that the same instance can
handle both channels and expose them as a single dpdkr
603
Firewall Network monitor Web cache Non-web traffic vSwitch Network monitor VM Firewall Web cache VM VM Web traffic  Non-web traffic  a) b) Compute agent Orchestrator Network commands Compute commands Commands to manage VMs (e.g., libvirt calls) Graph to be deployed OpenFlow controller OpenFlow messages and control commands vswitchd All traffic  point-to-point connection Figure 2: Overall software architecture.
port to applications, which are not aware of the actual
implementation of that port. We also extended OvS
with a new p-2-p link detector module, which analyses
each flowmod received by the vSwitch in order to dy-
namically detect when a new p-2-p link between two
dpdkr ports is either requested or removed.
When the VM is created (e.g., by the compute agent),
it is connected to dpdkr ports that have only the nor-
mal channel. When the vSwitch detects the request
to setup a p-2-p link between two VMs, it creates a
new pair of dpdkr bypass channels mapped on the same
piece of memory, shared by both communicating VMs.
This way, the two VMs will be able to exchange packets
without the intervention of the OvS forwarding engine.
The two new bypass channels are plugged in the proper
VMs and assigned to the right PMD instance. Since
OvS does not know which VM is attached to a speciﬁc
port (it just knows ports and the rules used to forward
packets among them), for these operations the vSwitch
has to rely on an external component. Consequently, we
modiﬁed the compute agent1 to receive requests from
OvS and: (i) plug the bypass channel (as an ivsh-
mem device) into the VM by interacting with QEMU;
(ii) conﬁgure the PMD instance to send/receive pack-
ets through the bypass channel, by means of a control
channel based on a virtio-serial device. Notably, the
PMD can still receive packets from the normal channel,
hence allowing an OpenFlow controller to send packet-
out messages to that port. Finally, when the p-2-p link
detector recognizes that a p-2-p link no longer exists,
the bypass channel is removed and the proper PMD in-
stances are conﬁgured to use only the normal channel.
To maintain compatibility with external entities such
as the OpenFlow controller, OvS exposes the two (nor-
mal and bypass) channels as a single (standard) dpdkr
port, so that such entities can continue to issue com-
mands involving dpdkr ports as they usually do (e.g.,
get statistics, turn them on/oﬀ), without noticing any
change in their actual implementation.
Finally, in order to export statistics related to ports
and ﬂows implementing a p-2-p link, the PMD has been
extended so that, each time a packet is sent through the
bypass channel, it increases the counters associated to
that OpenFlow rule and port, which are stored in a
1This prototype extends a special NFV node available
at http://github.com/netgroup-polito/un-orchestrator.
Figure 3: (a) memory-only; (b) NICs involved.
shared memory. When OvS needs to export statistics,
it just reads the proper values from that shared memory.
The vSwitch is in fact not able to count statistics related
to p-2-p links by itself, as it is not involved in moving
packets ﬂowing through these connections.
3. EXPERIMENTAL VALIDATION
We characterized our prototype on an Intel Xeon E5-
2690 v2 @ 3GHz, equipped with two 10G Intel 82599ES
NICs, comparing our approach with the vanilla OvS-
DPDK. In all the tests, we consider chains of VMs con-
nected only through p-2-p links, where each VM has
two dpdkr ports and runs a single core DPDK appli-
cation that moves packets from one port to another.
Notably, thanks to the transparency of our technology,
exactly the same VMs have been used in all the tests.
Figure 3 reports the throughput obtained with chains
of growing length. Particularly, Figure 3(a) refers to the
case in which the ﬁrst and the last VM of the chain
act as traﬃc source/sink; this test validates our ap-
proach without the NICs and PCI-e bus bottlenecks.
Figure 3(b) refers instead to the case in which traf-
ﬁc is delivered/drained to/from the chain through the
10Gbps NICs. Both the tests show that a chain of VMs
exploiting our technology provides better throughput
than the same chain based on the vanilla OvS-DPDK.
Our prototype brings also advantages in terms of la-
tency, especially with long chains (in case of 8 VMs,
we get an improvement of 80%); however, due to space
constraints, detailed results are not reported here.
Finally, the establishment of a direct channel between
two VMs, from the moment in which OvS recognizes a
p-2-p link, to the moment in which the PMD starts to
use the bypass channel, is on the order of 100 ms.
Acknowledgments
This work was conducted within the framework of the
FP7 UNIFY project (http://www.fp7-unify.eu), which
is partially funded by the Commission of the European
Union.
4. REFERENCES
[1] S. Garzarella, G. Lettieri, and L. Rizzo. Virtual
device passthrough for high speed vm networking.
In Architectures for Networking and
Communications Systems (ANCS), 2015
ACM/IEEE Symposium on, pages 99–110, 2015.
604
KVM/QEMU KVM/QEMU p-2-p link detector VM1 DPDK DPDK Application dpdkr PMD * Open vSwitch* forwarding engine Compute agent* OpenFlow controller * Modified to support transparent direct inter-VNF communication virtio-serial normal channel bypass channel dpdkr1 dpdkr PMD * VM2 DPDK DPDK Application dpdkr PMD * virtio-serial dpdkr PMD * dpdkr3 dpdkr2 dpdkr4  0.1 1 10 100 1000 2 3 4 5 6 7 8Throughput [Mpps] 4 6 8 10 12 14 16 18 20 1 2 3 4 5 6 7 8# VMsa)b)Traditional approach (bidirectional 64B traffic)Our approach (bidirectional 64B traffic)