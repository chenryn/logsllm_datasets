unnecessary. Moreover, examining our 27 routers, for exam-
ple, we can see that some updated as few as two times during
our three weeks of data, while others updated as much as 10
or 11 times. Clearly diﬀerent routers have diﬀerent needs.
Thus when we say we update the baseline only as needed,
this implies that we determine necessity in both time (per
day) and in space (per node).
6. RESULTS
In this section we show the results on our scheme’s perfor-
mance, discuss the calibration of its parameters and present
the reduction in overheads that it enables.
6.1 Measurement Load
We quantify reduction in measurement load using the
number of hours between successive baseline estimations for
each node i. We ensemble all the data points from all the
nodes, when δ = 0.5 and H = 24 (the impact of δ and H is
explored later in this section), and present the cumulative
density function for the number of days between baseline re-
estimation in Fig. 8. Baseline re-estimation appears to be
01020304050607080024681012Element IDNumber of recalibrationsp2pr2rl2lneeded less often with higher levels of ﬂow aggregation. In
addition, 50% of the baseline re-estimations will be typically
triggered after more than 2-3 days.
Figure 9: Relative error distribution for all three
granularities.
Figure 8: Days between baseline re-estimation for
all three levels of ﬂow granularity.
6.2 Error Assessment
A traﬃc matrix estimate yields one throughput estimate
ˆX(i, j, n) for each hour n for each OD ﬂow (i, j). The relative
error in the estimation of an OD ﬂow is given by:
for each time slot, in which the summation inside the L2
norm is taken over OD pairs (i, j). These results are dis-
played in Fig. 10. We see that 93% (75%) of the ﬂows had
errors less than 18% (20%) for p2p (or r2r), respectively. To
check the performance of our scheme at diﬀerent time scales,
we also ran our method using 10 minute time intervals. The
error results for this case are also presented in Fig. 10. We
see that the results are very similar.
ˆX(i, j, n) − X(i, j, n)
X(i, j, n)
, when X(i, j, n) (cid:54)= 0 (5)
e(i, j, n) =
The set {e(i, j, n)} yields a large set of errors across both
time and space (e.g. ﬂows). For each OD ﬂow (i, j), we have
a time series for the estimation errors for that ﬂow. For a
ﬁxed value of n, we can observe the errors across all the OD
ﬂows. There are diﬀerent ways to view and summarize these
errors.
We begin by looking at the entire set of error measure-
ments at our disposal, across both space and time, simul-
taneously. The distribution of the set {e(i, j, n)} is given
in Fig. 9 for each of the three granularity levels. This dis-
tribution captures an instantaneous quantity - namely the
probability that any ﬂow, at any moment in time, incurs
an error of some value. More than 80% of all our estimates
(over time and space) yield a relative error that is contained
between -25% and 25%. In other words, if one were to ran-
domly select one ﬂow and observe its estimated throughput
at a random time interval, then this value would be within
25% of the actual throughput with a probability of 0.8.
When errors in our scheme occur, it will be due to a de-
layed detection of a change in fanout. If a change in fanout
occurs, and the checking hour is a few hours later, then we
will not pick up the change for that many hours. This will
clearly generate errors during those hours. The instanta-
neous errors (Fig. 9) are suﬃciently well contained, that
this delayed change detection typically does not hamper our
ability to track and estimate ﬂows well (as in Fig. 6).
Temporal Errors. First we look at the errors in time.
For each time interval, we compute the relative L2 norm
across ﬂows. In other words, we end up with one error
(cid:113)(cid:80)M
(cid:113)(cid:80)M
et(n) =
i,j=1[(X(i, j, n) − ˆX(i, j, n))2]
i,j=1 X(i, j, n)2
Figure 10: Relative L2 norm error in time.
Spatial Errors. We now look at errors across ﬂows. For
each ﬂow we compute the relative L2 norm across time thus
deriving a summary error per OD ﬂow. In other words, we
get one error metric per ﬂow,
(cid:113)(cid:80)T
es(i, j) =
(cid:113)(cid:80)T
n=1[(X(i, j, n) − ˆX(i, j, n))2]
,
n=1 X(i, j, n)2
in which the summation inside the L2 norm computation is
over time slots n. The distribution of these ﬂow errors is
given in Fig. 11. We see that these errors appear larger
than the temporal ones do. This indicates that there are
plenty of ﬂows for whom it is diﬃcult to achieve low errors
at all instances of time (for both time scales of 1 hour and
10 minutes). Notice that the summary error es(i, j) includes
errors incurred for that ﬂow whether it experiences high or
smaller throughput (at which time instants this ﬂow would
be characterized as small). Our ﬁndings so far highlight the
utility of examining errors in both time and space.
1234567102000.10.20.30.40.50.60.70.80.91Days between baseline re−estimationF(x)Empirical CDFp2pr2rl2l−1−0.8−0.6−0.4−0.200.20.40.60.8100.20.40.60.81Relative ErrorEmpirical CDF−1−0.8−0.6−0.4−0.200.20.40.60.8100.050.10.150.20.25Relative ErrorEmpirical PDFp2pr2rl2lp2pr2rl2l00.20.40.60.8100.10.20.30.40.50.60.70.80.91Relative L2 norm error (time)F(x)Empirical CDFp2p (1hour)r2r (1hour)l2l (1hour)p2p (10mins)r2r (10mins)l2l (10mins)An error computation that selects the ﬂows constituting
the top 80% or 90% of the load, at each time slot, can be am-
biguous since a ﬂow such as the one in Fig. 12 is sometimes
included (during periods of regular activity) and sometimes
excluded (during periods of low activity). To avoid having
to pick thresholds for excluding OD ﬂows (or portions of an
OD ﬂow’s time series) in error metrics we use a weighted
relative L2 norm error metric, denoted e(cid:48)(i, j), where the
weights are set to be proportional to the actual OD ﬂow
throughput as follows:
(cid:113)(cid:80)T
w(i, j, n) =
Figure 11: Relative L2 norm error in space.
e(cid:48)(i, j) =
n=1[(X(i, j, n) − ˆX(i, j, n))2w(i, j, n)]
,
(cid:113)(cid:80)T
(cid:80)T
n=1 X(i, j, n)2
X(i, j, n)
k=1 X(i, j, k)
Small Flows. Most previous studies look at the tem-
poral errors, but eliminate the small OD ﬂows from their
evaluation by selecting the ﬂows constituting the top 80%
or 90% of the total demand. We visit the issue of small ﬂows
more closely here. The relative error metric {e(i, j, n)} suf-
fers from the following weakness. If the actual throughput
achieved by an OD ﬂow is very low, then the relative error
can be rather high. It’s not infrequent in our data for some
OD ﬂows to experience short periods of time when they may
have negligible throughput. If a ﬂow’s throughput gets close
to zero, then the relative error as deﬁned in Eq. 5 can be
huge, certainly much higher than 100%, until the baseline
re-estimation is triggered. This leads to outliers in the error
set {e(i, j, n)}.
We illustrate this scenario with a sample ﬂow in Fig. 12.
This particular router to router ﬂow experiences two small
periods of time when its throughput is very small. The
corresponding relative errors measured are on the order of
104. We claim that these outliers are not important for two
reasons. First, we see in the top portion of Fig. 12, that
our estimates can still do a good job at tracking the actual
OD ﬂow. Second, ﬂows nearing zero throughput can be
ignored by most traﬃc engineering applications at this time
scale. Recall that we target network design and planning
applications, and not anomaly detection or billing, when
behavior needs to be tracked at smaller time scales, and
where TMs would need to be continuously measured (in our
scheme this would require H = 1).
Figure 12: Flow with brief periods of inactivity. Up-
per: estimated behavior, lower: relative error.
This gives us a single error metric for each OD ﬂow. Note
that the threshold approach applied in previous eﬀorts is in
essence a temporal error metric (an e(n) metric). We prefer
to deal with the issue of small ﬂows via a spatial error metric
(an e(i, j) one) rather than a temporal one for the follow-
ing reason. The threshold approach does not only eliminate
small ﬂows, but also the portions of any ﬂow when it is near
zero. Our weighted error metric allows us to include ﬂows
throughout their lifetime, but weighed instantaneously ac-
cording to their volume; the weights in our metric vary over
time as does the ﬂow volume. This error deﬁnition assigns
small weights to those time instances when the ﬂow exhibits
little activity, and larger weights to high activity time slots.
In other words, instances of ﬂows are assigned a relevance
level according to their volume. This is reasonable since car-
riers design their networks such that they can accommodate
the maximum amount of traﬃc that may be oﬀered. In ad-
dition, this deﬁnition does not bias our error computation
in any favorable way - if a large error is made when a ﬂow
is at its peak, then this error will have a heavy weight. The
weighted relative L2 norm metric gives us an alternate view
of the errors and alleviates the problem of outliers without
eliminating them.
Fig. 13 presents the empirical cumulative density function
of the weighted relative L2 norm. More than 90% of the
ﬂows at all three levels of granularity experience a ﬂow error
less than 10%. The fact that this ﬁgure is quite diﬀerent
from Fig. 10 highlights the distorting eﬀect that OD ﬂows
dropping to near zero levels may have, as well as the eﬀect
of the delayed reaction mechanism employed by our scheme
when these events occur.
In order to see which ﬂows are lying in the tail of this dis-
tribution, we plot the weighted relative L2 norm error versus
the ﬂow throughput in Fig. 14. This shows that the smaller
ﬂows are the ones that incur the larger errors. Indeed, this
ﬁgure shows that most large ﬂows are typically estimated
with an error below 5%. Other traﬃc matrix estimation
methods [13, 5, 11] have also observed that they estimate
the large ﬂows well, and better than the small ﬂows.
6.3 Frequency of change detection
Parameter H in our scheme impacts the frequency of base-
line change detection. Each node randomly selects one hour
in the future H hours to test its fanouts against its 24 hour
baseline. We now look into the impact of parameter H on
00.20.40.60.8100.10.20.30.40.50.60.70.80.91Relative L2 norm error (space)F(x)Empirical CDFp2pr2rl2lp2p (10mins)r2r (10mins)l2l (10mins)07/27/0308/03/0308/10/0308/17/0301020304050Throughput (Mbps)07/27/0308/03/0308/10/0308/17/030.511.522.53x 104Relative ErrormeasurementsestimateFigure 13: CDF of weighted relative L2 norm error.
Figure 15: Impact of frequency of checking for base-
line diversion.
Figure 14: Weighted relative L2 norm error vs. ﬂow
throughput.
Figure 16: Impact of diversion threshold δ (dashed
line for right-hand y-axis).
the performance of the proposed scheme. We set parame-
ter δ equal to 0.5, and apply our scheme while testing for
diversions (i) every 12 hours (H = 12), (ii) every 24 hours
(H = 24) and (iii) every week (H = 168). Fig. 15 illustrates
that testing for fanout discrepancies more frequently than
once a day does not lead to signiﬁcant improvement in terms
of ﬂow errors. However, decreasing H to one week does lead
to deterioration in OD ﬂow estimates. These observations
were found to hold across all three levels of granularity, and
across values of δ spanning from 0.1 up to 1. Consequently,
we conclude that testing for fanout change once a day is
frequent enough for our purposes.
6.4 Tradeoff between baseline re-estimation
and accuracy
According to our scheme baseline re-estimation is trig-
gered when the measured fanout diversion exceeds a speciﬁc
value of δ. If fanouts are updated less often, TM estimates
will be less accurate but the savings in terms of measure-
ment load (i.e., running ﬂow monitors) will be larger. On
the other hand, when the fanouts are updated more fre-
quently accuracy is likely to be higher. In this section we
look into the tradeoﬀ between accuracy and reduction in
measurement load for diﬀerent values of δ. In Fig. 16 we
present the 90th percentile of the weighted relative L2 norm
error distribution and the average number of days between
baseline re-estimation for diﬀerent values of δ.
We observe the following. (i) Baseline re-estimation needs
to be performed more frequently for ﬁner levels of the traﬃc
matrix granularity. (ii) Greater values of δ lead to higher
ﬂow errors and greater periods of time when the baseline
measurements can be used unaltered.
(iii) The proposed
scheme leads to errors below 12% across all three levels of
granularity and across all values of δ, and incurs baseline
re-estimation in the worst case every 3 days, when δ = 0.1.
(iv) Our error metric does not appear to be that sensitive
to δ. This lack of sensitivity to δ is an attractive feature
of our scheme since it implies that careful optimization of
this parameter is not needed, but rather that a wide range
of values will do well. The average number of days between
baseline recalibration appears to be a little more sensitive
to δ as we do see a variation between 3 to 7 days. We
have selected δ = 0.5 in most of our experiments; it appears
quite reasonable with typical errors below 5% and baseline
recalibrations occurring once every 3 to 4 days. The ﬁnal
decision as to how to tune such a scheme’s parameters will
ultimately lie with the network operators as they will choose
the accuracy versus overhead tradeoﬀ.
We consider that running a ﬂow monitor for the purposes