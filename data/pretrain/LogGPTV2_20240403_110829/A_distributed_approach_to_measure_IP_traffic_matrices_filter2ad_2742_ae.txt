### Unnecessary Updates and Router Behavior

Upon examining our 27 routers, we observed that some updated as few as two times over a three-week period, while others updated up to 10 or 11 times. This indicates that different routers have varying update requirements. Therefore, when we state that we update the baseline only as needed, it means that we determine the necessity based on both time (per day) and space (per node).

### 6. Results

In this section, we present the performance of our scheme, discuss the calibration of its parameters, and highlight the reduction in overheads it enables.

#### 6.1 Measurement Load

To quantify the reduction in measurement load, we use the number of hours between successive baseline estimations for each node \(i\). We aggregate all data points from all nodes, with \(\delta = 0.5\) and \(H = 24\) (the impact of \(\delta\) and \(H\) is explored later in this section), and present the cumulative density function (CDF) for the number of days between baseline re-estimations in Figure 8. The results show that baseline re-estimation is less frequent with higher levels of flow aggregation. Additionally, 50% of the baseline re-estimations are typically triggered after more than 2-3 days.

**Figure 8: Days between baseline re-estimation for all three levels of flow granularity.**

**Figure 9: Relative error distribution for all three granularities.**

#### 6.2 Error Assessment

A traffic matrix estimate yields one throughput estimate \(\hat{X}(i, j, n)\) for each hour \(n\) for each OD flow \((i, j)\). The relative error in the estimation of an OD flow is given by:

\[
e(i, j, n) = \frac{\hat{X}(i, j, n) - X(i, j, n)}{X(i, j, n)}, \quad \text{when } X(i, j, n) \neq 0
\]

The set \(\{e(i, j, n)\}\) provides a large set of errors across both time and space (e.g., flows). For each OD flow \((i, j)\), we have a time series of estimation errors. For a fixed value of \(n\), we can observe the errors across all OD flows. There are different ways to view and summarize these errors.

We begin by examining the entire set of error measurements across both space and time simultaneously. The distribution of the set \(\{e(i, j, n)\}\) is shown in Figure 9 for each of the three granularity levels. This distribution captures the probability that any flow, at any moment in time, incurs an error of a certain value. More than 80% of all our estimates (over time and space) yield a relative error between -25% and 25%. In other words, if one were to randomly select a flow and observe its estimated throughput at a random time interval, the value would be within 25% of the actual throughput with a probability of 0.8.

**Temporal Errors.** First, we look at the errors in time. For each time interval, we compute the relative L2 norm across flows. Specifically, we calculate:

\[
e_t(n) = \sqrt{\sum_{i,j=1}^M \left( \frac{X(i, j, n) - \hat{X}(i, j, n)}{X(i, j, n)} \right)^2}
\]

**Figure 10: Relative L2 norm error in time.**

**Spatial Errors.** Next, we examine errors across flows. For each flow, we compute the relative L2 norm across time, deriving a summary error per OD flow. Specifically, we calculate:

\[
e_s(i, j) = \sqrt{\sum_{n=1}^T \left( \frac{X(i, j, n) - \hat{X}(i, j, n)}{X(i, j, n)} \right)^2}
\]

The distribution of these flow errors is shown in Figure 11. These errors appear larger than the temporal ones, indicating that there are many flows for which it is difficult to achieve low errors at all times (for both time scales of 1 hour and 10 minutes). Notice that the summary error \(e_s(i, j)\) includes errors incurred for that flow, whether it experiences high or smaller throughput.

**Figure 11: Relative L2 norm error in space.**

**Weighted Relative L2 Norm Error.** To avoid the ambiguity of selecting thresholds for excluding OD flows, we use a weighted relative L2 norm error metric, denoted \(e'(i, j)\), where the weights are proportional to the actual OD flow throughput:

\[
w(i, j, n) = \frac{X(i, j, n)}{\sum_{k=1}^T X(i, j, k)}
\]

\[
e'(i, j) = \sqrt{\sum_{n=1}^T \left( \frac{X(i, j, n) - \hat{X}(i, j, n)}{X(i, j, n)} \right)^2 w(i, j, n)}
\]

**Small Flows.** Most previous studies focus on temporal errors but exclude small OD flows from their evaluation. We address the issue of small flows more closely here. The relative error metric \(\{e(i, j, n)\}\) can be very high if the actual throughput of an OD flow is very low. This leads to outliers in the error set. We illustrate this scenario with a sample flow in Figure 12. This particular router-to-router flow experiences brief periods of very low throughput, resulting in relative errors on the order of \(10^4\). We claim that these outliers are not important for two reasons: first, our estimates still track the actual OD flow well; second, flows nearing zero throughput can be ignored by most traffic engineering applications at this time scale.

**Figure 12: Flow with brief periods of inactivity. Upper: estimated behavior, lower: relative error.**

**Figure 13: CDF of weighted relative L2 norm error.**

**Figure 14: Weighted relative L2 norm error vs. flow throughput.**

**Figure 15: Impact of frequency of checking for baseline diversion.**

**Figure 16: Impact of diversion threshold \(\delta\) (dashed line for right-hand y-axis).**

#### 6.3 Frequency of Change Detection

The parameter \(H\) in our scheme impacts the frequency of baseline change detection. Each node randomly selects one hour in the future \(H\) hours to test its fanouts against its 24-hour baseline. We now examine the impact of parameter \(H\) on the performance of the proposed scheme. We set \(\delta = 0.5\) and apply our scheme while testing for diversions (i) every 12 hours (\(H = 12\)), (ii) every 24 hours (\(H = 24\)), and (iii) every week (\(H = 168\)). Figure 15 shows that testing for fanout discrepancies more frequently than once a day does not lead to significant improvement in terms of flow errors. However, decreasing \(H\) to one week does lead to deterioration in OD flow estimates. These observations hold across all three levels of granularity and across values of \(\delta\) from 0.1 to 1. Consequently, we conclude that testing for fanout change once a day is sufficient for our purposes.

#### 6.4 Tradeoff Between Baseline Re-estimation and Accuracy

According to our scheme, baseline re-estimation is triggered when the measured fanout diversion exceeds a specific value of \(\delta\). If fanouts are updated less often, TM estimates will be less accurate, but the savings in terms of measurement load (i.e., running flow monitors) will be larger. Conversely, more frequent updates improve accuracy. In this section, we explore the tradeoff between accuracy and reduction in measurement load for different values of \(\delta\). In Figure 16, we present the 90th percentile of the weighted relative L2 norm error distribution and the average number of days between baseline re-estimations for different values of \(\delta\).

We observe the following:
1. Baseline re-estimation needs to be performed more frequently for finer levels of traffic matrix granularity.
2. Greater values of \(\delta\) lead to higher flow errors and longer periods when the baseline measurements can be used unaltered.
3. Our scheme leads to errors below 12% across all three levels of granularity and all values of \(\delta\), with baseline re-estimation occurring every 3 days in the worst case (\(\delta = 0.1\)).
4. Our error metric is not highly sensitive to \(\delta\), which is an attractive feature since it implies that careful optimization of this parameter is not necessary. The average number of days between baseline recalibration varies from 3 to 7 days. We have selected \(\delta = 0.5\) in most of our experiments, which appears reasonable with typical errors below 5% and baseline recalibrations occurring every 3 to 4 days. The final decision on tuning the scheme's parameters will ultimately lie with the network operators, who will choose the accuracy versus overhead tradeoff.

**Figure 16: Impact of diversion threshold \(\delta\) (dashed line for right-hand y-axis).**

We consider that running a flow monitor for the purposes of our scheme is a balanced approach, providing a good tradeoff between accuracy and overhead.