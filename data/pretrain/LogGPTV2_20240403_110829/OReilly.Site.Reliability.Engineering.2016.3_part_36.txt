team already addressed. For example, the developers built graceful degradation into
the service. As capacity becomes scarce, the service no longer returns pictures along‐
side text or small maps illustrating where a story takes place. And depending on its
purpose, an RPC that times out is either not retried (for example, in the case of the
aforementioned pictures), or is retried with a randomized exponential backoff.
Despite these safeguards, the tasks fail one by one and are then restarted by Borg,
which drives the number of working tasks down even more.
As a result, some graphs on the service dashboard turn an alarming shade of red and
SRE is paged. In response, SREs temporarily add capacity to the Asian datacenter by
increasing the number of tasks available for the Shakespeare job. By doing so, they’re
able to restore the Shakespeare service in the Asian cluster.
Afterward, the SRE team writes a postmortem detailing the chain of events, what
went well, what could have gone better, and a number of action items to prevent this
scenario from occurring again. For example, in the case of a service overload, the
GSLB load balancer will redirect some traffic to neighboring datacenters. Also, the
SRE team turns on autoscaling, so that the number of tasks automatically increases
with traffic, so they don’t have to worry about this type of issue again.
Closing Remarks
When systems are overloaded, something needs to give in order to remedy the situa‐
tion. Once a service passes its breaking point, it is better to allow some user-visible
errors or lower-quality results to slip through than try to fully serve every request.
Understanding where those breaking points are and how the system behaves beyond
them is critical for service owners who want to avoid cascading failures.
Closing Remarks | 283
Without proper care, some system changes meant to reduce background errors or
otherwise improve the steady state can expose the service to greater risk of a full out‐
age. Retrying on failures, shifting load around from unhealthy servers, killing unheal‐
thy servers, adding caches to improve performance or reduce latency: all of these
might be implemented to improve the normal case, but can improve the chance of
causing a large-scale failure. Be careful when evaluating changes to ensure that one
outage is not being traded for another.
284 | Chapter 22: Addressing Cascading Failures
CHAPTER 23
Managing Critical State: Distributed
Consensus for Reliability
Written by Laura Nolan
Edited by Tim Harvey
Processes crash or may need to be restarted. Hard drives fail. Natural disasters can
take out several datacenters in a region. Site Reliability Engineers need to anticipate
these sorts of failures and develop strategies to keep systems running in spite of them.
These strategies usually entail running such systems across multiple sites. Geographi‐
cally distributing a system is relatively straightforward, but also introduces the need
to maintain a consistent view of system state, which is a more nuanced and difficult
undertaking.
Groups of processes may want to reliably agree on questions such as:
• Which process is the leader of a group of processes?
• What is the set of processes in a group?
• Has a message been successfully committed to a distributed queue?
• Does a process hold a lease or not?
• What is a value in a datastore for a given key?
We’ve found distributed consensus to be effective in building reliable and highly
available systems that require a consistent view of some system state. The distributed
consensus problem deals with reaching agreement among a group of processes con‐
nected by an unreliable communications network. For instance, several processes in a
distributed system may need to be able to form a consistent view of a critical piece of
configuration, whether or not a distributed lock is held, or if a message on a queue
has been processed. It is one of the most fundamental concepts in distributed com‐
285
puting and one we rely on for virtually every service we offer. Figure 23-1 illustrates a
simple model of how a group of processes can achieve a consistent view of system
state through distributed consensus.
Figure 23-1. Distributed consensus: agreement among a group of processes
Whenever you see leader election, critical shared state, or distributed locking, we rec‐
ommend using distributed consensus systems that have been formally proven and tested
thoroughly. Informal approaches to solving this problem can lead to outages, and
more insidiously, to subtle and hard-to-fix data consistency problems that may pro‐
long outages in your system unnecessarily.
CAP Theorem
The CAP theorem ([Fox99], [Bre12]) holds that a distributed system cannot simulta‐
neously have all three of the following properties:
• Consistent views of the data at each node
• Availability of the data at each node
• Tolerance to network partitions [Gil02]
The logic is intuitive: if two nodes can’t communicate (because the network is parti‐
tioned), then the system as a whole can either stop serving some or all requests at
some or all nodes (thus reducing availability), or it can serve requests as usual, which
results in inconsistent views of the data at each node.
286 | Chapter 23: Managing Critical State: Distributed Consensus for Reliability
Because network partitions are inevitable (cables get cut, packets get lost or delayed
due to congestion, hardware breaks, networking components become misconfigured,
etc.), understanding distributed consensus really amounts to understanding how
consistency and availability work for your particular application. Commercial pres‐
sures often demand high levels of availability, and many applications require consis‐
tent views on their data.
Systems and software engineers are usually familiar with the traditional ACID data‐
store semantics (Atomicity, Consistency, Isolation, and Durability), but a growing
number of distributed datastore technologies provide a different set of semantics
known as BASE (Basically Available, Soft state, and Eventual consistency). Datastores
that support BASE semantics have useful applications for certain kinds of data and
can handle large volumes of data and transactions that would be much more costly,
and perhaps altogether infeasible, with datastores that support ACID semantics.
Most of these systems that support BASE semantics rely on multimaster replication,
where writes can be committed to different processes concurrently, and there is some
mechanism to resolve conflicts (often as simple as “latest timestamp wins”). This
approach is usually known as eventual consistency. However, eventual consistency can
lead to surprising results [Lu15], particularly in the event of clock drift (which is
inevitable in distributed systems) or network partitioning [Kin15].1
It is also difficult for developers to design systems that work well with datastores that
support only BASE semantics. Jeff Shute [Shu13], for example, has stated, “we find
developers spend a significant fraction of their time building extremely complex and
error-prone mechanisms to cope with eventual consistency and handle data that may
be out of date. We think this is an unacceptable burden to place on developers and
that consistency problems should be solved at the database level.”
System designers cannot sacrifice correctness in order to achieve reliability or perfor‐
mance, particularly around critical state. For example, consider a system that handles
financial transactions: reliability or performance requirements don’t provide much
value if the financial data is not correct. Systems need to be able to reliably synchron‐
ize critical state across multiple processes. Distributed consensus algorithms provide
this functionality.
1 Kyle Kingsbury has written an extensive series of articles on distributed systems correctness, which contain
many examples of unexpected and incorrect behavior in these kinds of datastores. See https://aphyr.com/tags/
jepsen.
Managing Critical State: Distributed Consensus for Reliability | 287
Motivating the Use of Consensus: Distributed Systems
Coordination Failure
Distributed systems are complex and subtle to understand, monitor, and trouble‐
shoot. Engineers running such systems are often surprised by behavior in the pres‐
ence of failures. Failures are relatively rare events, and it is not a usual practice to test
systems under these conditions. It is very difficult to reason about system behavior
during failures. Network partitions are particularly challenging—a problem that
appears to be caused by a full partition may instead be the result of:
• A very slow network
• Some, but not all, messages being dropped
• Throttle occurring in one direction, but not the other direction
The following sections provide examples of problems that occurred in real-world dis‐
tributed systems and discuss how leader election and distributed consensus algo‐
rithms could be used to prevent such issues.
Case Study 1: The Split-Brain Problem
A service is a content repository that allows collaboration between multiple users. It
uses sets of two replicated file servers in different racks for reliability. The service
needs to avoid writing data simultaneously to both file servers in a set, because doing
so could result in data corruption (and possibly unrecoverable data).
Each pair of file servers has one leader and one follower. The servers monitor each
other via heartbeats. If one file server cannot contact its partner, it issues a STONITH
(Shoot The Other Node in the Head) command to its partner node to shut the node
down, and then takes mastership of its files. This practice is an industry standard
method of reducing split-brain instances, although as we shall see, it is conceptually
unsound.
What happens if the network becomes slow, or starts dropping packets? In this sce‐
nario, file servers exceed their heartbeat timeouts and, as designed, send STONITH
commands to their partner nodes and take mastership. However, some commands
may not be delivered due to the compromised network. File server pairs may now be
in a state in which both nodes are expected to be active for the same resource, or
where both are down because both issued and received STONITH commands. This
results in either corruption or unavailability of data.
The problem here is that the system is trying to solve a leader election problem using
simple timeouts. Leader election is a reformulation of the distributed asynchronous
consensus problem, which cannot be solved correctly by using heartbeats.
288 | Chapter 23: Managing Critical State: Distributed Consensus for Reliability
Case Study 2: Failover Requires Human Intervention
A highly sharded database system has a primary for each shard, which replicates syn‐
chronously to a secondary in another datacenter. An external system checks the
health of the primaries, and, if they are no longer healthy, promotes the secondary to
primary. If the primary can’t determine the health of its secondary, it makes itself
unavailable and escalates to a human in order to avoid the split-brain scenario seen in
Case Study 1.
This solution doesn’t risk data loss, but it does negatively impact availability of data. It
also unnecessarily increases operational load on the engineers who run the system,
and human intervention scales poorly. This sort of event, where a primary and secon‐
dary have problems communicating, is highly likely to occur in the case of a larger
infrastructure problem, when the responding engineers may already be overloaded
with other tasks. If the network is so badly affected that a distributed consensus sys‐
tem cannot elect a master, a human is likely not better positioned to do so.
Case Study 3: Faulty Group-Membership Algorithms
A system has a component that performs indexing and searching services. When
starting, nodes use a gossip protocol to discover each other and join the cluster. The
cluster elects a leader, which performs coordination. In the case of a network parti‐
tion that splits the cluster, each side (incorrectly) elects a master and accepts writes
and deletions, leading to a split-brain scenario and data corruption.
The problem of determining a consistent view of group membership across a group
of processes is another instance of the distributed consensus problem.
In fact, many distributed systems problems turn out to be different versions of dis‐
tributed consensus, including master election, group membership, all kinds of dis‐
tributed locking and leasing, reliable distributed queuing and messaging, and
maintenance of any kind of critical shared state that must be viewed consistently
across a group of processes. All of these problems should be solved only using dis‐
tributed consensus algorithms that have been proven formally correct, and whose
implementations have been tested extensively. Ad hoc means of solving these sorts of
problems (such as heartbeats and gossip protocols) will always have reliability prob‐
lems in practice.
How Distributed Consensus Works
The consensus problem has multiple variants. When dealing with distributed soft‐
ware systems, we are interested in asynchronous distributed consensus, which applies
to environments with potentially unbounded delays in message passing. (Synchronous
consensus applies to real-time systems, in which dedicated hardware means that mes‐
sages will always be passed with specific timing guarantees.)
How Distributed Consensus Works | 289
Distributed consensus algorithms may be crash-fail (which assumes that crashed
nodes never return to the system) or crash-recover. Crash-recover algorithms are
much more useful, because most problems in real systems are transient in nature due
to a slow network, restarts, and so on.
Algorithms may deal with Byzantine or non-Byzantine failures. Byzantine failure
occurs when a process passes incorrect messages due to a bug or malicious activity,
and are comparatively costly to handle, and less often encountered.
Technically, solving the asynchronous distributed consensus problem in bounded
time is impossible. As proven by the Dijkstra Prize–winning FLP impossibility result
[Fis85], no asynchronous distributed consensus algorithm can guarantee progress in
the presence of an unreliable network.
In practice, we approach the distributed consensus problem in bounded time by
ensuring that the system will have sufficient healthy replicas and network connectiv‐
ity to make progress reliably most of the time. In addition, the system should have
backoffs with randomized delays. This setup both prevents retries from causing cas‐
cade effects and avoids the dueling proposers problem described later in this chapter.
The protocols guarantee safety, and adequate redundancy in the system encourages
liveness.
The original solution to the distributed consensus problem was Lamport’s Paxos pro‐
tocol [Lam98], but other protocols exist that solve the problem, including Raft
[Ong14], Zab [Jun11], and Mencius [Mao08]. Paxos itself has many variations
intended to increase performance [Zoo14]. These usually vary only in a single detail,
such as giving a special leader role to one process to streamline the protocol.
Paxos Overview: An Example Protocol
Paxos operates as a sequence of proposals, which may or may not be accepted by a
majority of the processes in the system. If a proposal isn’t accepted, it fails. Each pro‐
posal has a sequence number, which imposes a strict ordering on all of the operations
in the system.
In the first phase of the protocol, the proposer sends a sequence number to the
acceptors. Each acceptor will agree to accept the proposal only if it has not yet seen a
proposal with a higher sequence number. Proposers can try again with a higher
sequence number if necessary. Proposers must use unique sequence numbers (draw‐
ing from disjoint sets, or incorporating their hostname into the sequence number, for
instance).
If a proposer receives agreement from a majority of the acceptors, it can commit the
proposal by sending a commit message with a value.
290 | Chapter 23: Managing Critical State: Distributed Consensus for Reliability
The strict sequencing of proposals solves any problems relating to ordering of mes‐
sages in the system. The requirement for a majority to commit means that two differ‐
ent values cannot be committed for the same proposal, because any two majorities
will overlap in at least one node. Acceptors must write a journal on persistent storage
whenever they agree to accept a proposal, because the acceptors need to honor these
guarantees after restarting.
Paxos on its own isn’t that useful: all it lets you do is to agree on a value and proposal
number once. Because only a quorum of nodes need to agree on a value, any given
node may not have a complete view of the set of values that have been agreed to. This
limitation is true for most distributed consensus algorithms.
System Architecture Patterns for Distributed Consensus
Distributed consensus algorithms are low-level and primitive: they simply allow a set
of nodes to agree on a value, once. They don’t map well to real design tasks. What
makes distributed consensus useful is the addition of higher-level system components
such as datastores, configuration stores, queues, locking, and leader election services
to provide the practical system functionality that distributed consensus algorithms
don’t address. Using higher-level components reduces complexity for system design‐
ers. It also allows underlying distributed consensus algorithms to be changed if neces‐
sary in response to changes in the environment in which the system runs or changes
in nonfunctional requirements.
Many systems that successfully use consensus algorithms actually do so as clients of
some service that implements those algorithms, such as Zookeeper, Consul, and etcd.
Zookeeper [Hun10] was the first open source consensus system to gain traction in
the industry because it was easy to use, even with applications that weren’t designed
to use distributed consensus. The Chubby service fills a similar niche at Google. Its
authors point out [Bur06] that providing consensus primitives as a service rather
than as libraries that engineers build into their applications frees application main‐
tainers of having to deploy their systems in a way compatible with a highly available
consensus service (running the right number of replicas, dealing with group mem‐
bership, dealing with performance, etc.).
Reliable Replicated State Machines
A replicated state machine (RSM) is a system that executes the same set of operations,
in the same order, on several processes. RSMs are the fundamental building block of
useful distributed systems components and services such as data or configuration
storage, locking, and leader election (described in more detail later).
The operations on an RSM are ordered globally through a consensus algorithm. This
is a powerful concept: several papers ([Agu10], [Kir08], [Sch90]) show that any deter‐
System Architecture Patterns for Distributed Consensus | 291
ministic program can be implemented as a highly available replicated service by being
implemented as an RSM.
As shown in Figure 23-2, replicated state machines are a system implemented at a
logical layer above the consensus algorithm. The consensus algorithm deals with
agreement on the sequence of operations, and the RSM executes the operations in
that order. Because not every member of the consensus group is necessarily a mem‐
ber of each consensus quorum, RSMs may need to synchronize state from peers. As
described by Kirsch and Amir [Kir08], you can use a sliding-window protocol to rec‐
oncile state between peer processes in an RSM.
Figure 23-2. The relationship between consensus algorithms and replicated state