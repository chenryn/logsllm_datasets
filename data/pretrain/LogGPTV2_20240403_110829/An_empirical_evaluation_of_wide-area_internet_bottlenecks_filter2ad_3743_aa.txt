# An Empirical Evaluation of Wide-Area Internet Bottlenecks

**Authors:**
- Aditya Akella, Carnegie Mellon University, Pittsburgh, PA 15213, aditya@cs.cmu.edu
- Srinivasan Seshan, Carnegie Mellon University, Pittsburgh, PA 15213, srini+@cs.cmu.edu
- Anees Shaikh, IBM T.J. Watson Research Center, Hawthorne, NY 10532, anees.shaikh@ibm.com

## Abstract

Conventional wisdom suggests that the performance limitations in the current Internet are primarily due to last-mile connectivity or access links of stub ASes. As these links are upgraded, new bottlenecks and hot-spots may emerge elsewhere in the network. This paper investigates non-access bottlenecksâ€”links within carrier ISPs or between neighboring carriers that could constrain the bandwidth available to long-lived TCP flows. Through extensive measurements, we discover, classify, and characterize bottleneck links (primarily in the U.S.) in terms of their location, latency, and available capacity.

Our findings indicate that approximately 50% of the Internet paths explored have a non-access bottleneck with available capacity less than 50 Mbps, many of which limit the performance of well-connected nodes on the Internet today. Surprisingly, the identified bottlenecks are roughly equally split between intra-ISP links and peering links between ISPs. Additionally, low-latency links, both intra-ISP and peering, have a significant likelihood of constraining available bandwidth. We discuss the implications of our findings on related issues such as choosing an access provider and optimizing routes through the network. These results can guide the design of future network services, such as overlay routing, by identifying which links or paths to avoid to improve performance.

## Categories and Subject Descriptors

- C.2 [Computer Systems Organization]: Computer-Communication Networks
- C.2.5 [Computer-Communication Networks]: Local and Wide-Area Networks

## General Terms

- Measurement, Performance

## 1. Introduction

A common belief about the Internet is that poor network performance primarily arises from constraints at the edges of the network, such as narrow-band access links (e.g., dial-up, DSL). These links limit the ability of applications to utilize the abundant bandwidth and negligible queuing available in the interior of the network. As access technology evolves, enterprises and end-users can increase the capacity of their Internet connections by upgrading their access links. However, if other parts of the network subsequently become new performance bottlenecks, the overall performance improvement may be insignificant. Upgrades at the edges of the network may simply shift existing bottlenecks and hot-spots to other parts of the Internet.

In this study, we investigate the likely location and characteristics of future bottleneck links in the Internet. Such information can be valuable for choosing intermediate hops in overlay routing services, interdomain traffic engineering, and for customers considering their connectivity options.

Our objective is to investigate the characteristics of links within or between carrier ISP networks that could potentially constrain the bandwidth available to long-lived TCP flows, referred to as non-access bottleneck links. Using a large set of network measurements, we aim to discover and classify such links according to their location in the Internet hierarchy and their estimated available capacity. By focusing on interior links, we avoid the obvious bottlenecks near the source and destination (i.e., first-mile and last-mile hops).

This paper makes two primary contributions:
1. A methodology for measuring non-access Internet bottleneck links.
2. A classification of existing bottleneck links.

## 2. Methodology for Measuring Non-Access Internet Bottleneck Links

### 2.1 Choosing a Set of Traffic Sources

The Internet is composed of an interconnected collection of Autonomous Systems (ASes), which can be categorized as carrier ASes (e.g., ISPs and transit providers) and stub ASes (end-customer domains). Our goal is to measure the characteristics of potential performance bottlenecks that end-nodes encounter, which are not within their own control. To perform this measurement, we need to address several issues, described below.

Stub ASes in the Internet vary in size and connectivity to their carrier networks. Large stubs, such as large universities and commercial organizations, are often multi-homed and have high-speed links to all of their providers. Smaller stubs, such as small businesses, usually have a single provider with a much slower connection.

At the core of our measurements are traffic flows between a set of sources, which are under our control, and a set of destinations, which are random but chosen to represent typical Internet paths. However, it is difficult to use such measurements when the source network or its connection to the upstream carrier network is itself a bottleneck. Therefore, we choose to explore bottleneck characteristics by measuring paths from well-connected endpoints, i.e., stub ASes with very high-speed access to their upstream providers. Large commercial and academic organizations are examples of such endpoints. In addition to the connectivity of the stub ASes, another important factor in choosing sources is diversity, both in terms of geographic locations and carrier networks. This ensures that the results are not biased by repeated measurement of a small set of bottleneck links.

We use hosts participating in the PlanetLab project, which provides access to a large collection of Internet nodes that meet our requirements. PlanetLab is an Internet-wide testbed of multiple high-end machines located at geographically diverse locations. Most of the machines available at this time are in large academic institutions and research centers in the U.S. and Europe and have very high-speed access to the Internet. Note that although our traffic sources are primarily at universities and research labs, we do not measure the paths between these nodes. Rather, our measured paths are chosen to be representative of typical Internet paths (e.g., as opposed to paths on Internet2).

Initially, we chose one machine from each of the PlanetLab sites as the initial candidate for our experiments. While it is generally true that the academic institutions and research labs hosting PlanetLab machines are well-connected to their upstream providers, we found that the machines themselves are often on low-speed local area networks. Out of the 38 PlanetLab sites operational at the outset of our experiments, we identified 12 that had this drawback. To ensure that we can reliably measure non-access bottlenecks, we did not use these 12 machines in our experiments.

| Tier | Total Unique Providers | Average #Providers per PlanetLab Source |
|------|-----------------------|------------------------------------------|
| Tier-1 | 11                    | 0.92                                     |
| Tier-2 | 15                    | 0.69                                     |
| Tier-3 | 5                     | 0.81                                     |
| Tier-4 | 5                     | 0.10                                     |

Table 1: First-hop connectivity of the PlanetLab sites

The unique upstream providers and locations of the remaining 26 PlanetLab sites are shown in Table 1 and Figure 1(a), respectively. We use a hierarchical classification of ASes into four tiers (as defined by the work in [33]) to categorize the upstream ISPs of the different PlanetLab sites. ASes in tier-1, such as AT&T and Sprint, are large ASes that do not have any upstream providers. Most ASes in tier-1 have peering arrangements with each other. Lower in the hierarchy, tier-2 ASes, including Savvis, Time Warner Telecom, and several large national carriers, have peering agreements with a number of ASes in tier-1. ASes in tier-2 also have peering relationships with each other, but they do not generally peer with any other ASes. ASes in tier-3, such as Southwestern Bell and Turkish Telecomm, are small regional providers that have a few customer ASes and peer with a few other similar small providers. Finally, the ASes in tier-4, such as rockynet.com, have very few customers and typically no peering relationships at all [33].

### 2.2 Choosing a Set of Destinations

We have two objectives in choosing paths to measure from our sources. First, we want to choose a set of network paths that are representative of typical paths taken by Internet traffic. Second, we wish to explore the common impression that public network exchanges, or NAPs (network access points), are significant bottlenecks. Our choice of network paths to measure is equivalent to choosing a set of destinations in the wide-area as targets for our testing tools. Below, we describe the rationale and techniques for choosing test destinations to achieve these objectives.

#### 2.2.1 Typical Paths

Most end-to-end data traffic in the Internet flows between stub networks. One way to measure typical paths would be to select a large number of stub networks as destinations. However, the number of such destinations needed to characterize properties of representative paths would make the measurements impractical. Instead, we use key features of the routing structure of the Internet to help choose a smaller set of destinations for our tests.

Traffic originated by a stub network subsequently traverses multiple intermediate autonomous systems before reaching the destination stub network. Following the definitions of AS hierarchy presented in [33] (and summarized earlier), flows originated by typical stub source networks usually enter a tier-4 or higher-tier ISP. Beyond this, the flow might cross a sequence of multiple links between ISPs and their higher-tier upstream carriers (uphill path). At the end of this sequence, the flow might cross a single peering link between two peer ISPs after which it might traverse a downhill path of ASes in progressively lower tiers to the final destination, which is also usually a stub. This form of routing, arising out of BGP policies, is referred to as valley-free routing. We refer to the portion of the path taken by a flow that excludes links within the stub network at either end of the path, and the access links of either of the stub networks, as the transit path.

Clearly, non-access bottlenecks lie in the transit path to the destination stub network. Specifically, the bottleneck for any flow could lie either (1) within any one of the ISPs in the uphill or the downhill portion of the transit path or (2) between any two distinct ISPs in either portion of the transit path. Therefore, we believe that measuring the paths between our sources and a wide variety of different ISPs would provide a representative view of the bottlenecks that these sources encounter.

Due to the large number of ISPs, it is impractical to measure the paths between our sources and all such carrier networks. However, the reachability provided by these carriers arises directly from their position in the AS hierarchy. Hence, it is more likely that a path will pass through one or two tier-1 ISPs than a lower-tier ISP. Therefore, we test paths between our sources and all tier-1 ASes. To make our measurements practical, we only test the paths between our sources and a fraction of the tier-2 ISPs (chosen randomly). We measure an even smaller fraction of all tier-3 and tier-4 providers. The number of ISPs we chose in each tier is presented in Table 2.

| Tier | Number Tested | Total in the Internet [33] | Percentage Tested |
|------|---------------|----------------------------|-------------------|
| Tier-1 | 20            | 129                        | 15.5%             |
| Tier-2 | 14            | 25                         | 56%               |
| Tier-3 | 3             | 897                        | 0.3%              |
| Tier-4 | 15            | 971                        | 1.5%              |

Table 2: Composition of the destination set

In addition to choosing a target AS, we need to choose a target IP address within the AS for our tests. For any AS we choose, say AS X, we pick a router that is a few (2-4) IP hops away from the machine www.X.com (or X.net as the case may be). We confirm this router to be inside the AS by manually inspecting the DNS name of the router where available. Most ISPs name their routers according to their function in the network, e.g., edge (chi-edge-08.inet.qwest.net) or backbone (sl-bb12-nyc-9-0.sprintlink.net). The function of the router can also be inferred from the names of routers adjacent to it. In addition, we double-check using the IP addresses of the carrierâ€™s routers along the path to www.X.com (typically there is a change in the subnet address close to the web server). We measure the path between each of the sources and the above IP addresses. The diversity of the sources in terms of geography and upstream connectivity ensures that we sample several links with the ISPs. The geographic location of the destinations is shown in Figure 1(b). Each destinationâ€™s location is identified by that of the traffic source with the least delay to it.

#### 2.2.2 Public Exchanges

The carrier ASes in the Internet peer with each other at a number of locations throughout the world. These peering arrangements can be roughly categorized as public exchanges, or NAPs (e.g., the original 4 NSF exchanges), or private peering (between a pair of ISPs). One of the motivations for the deployment of private peering has been to avoid the perceived congestion of public exchanges. As part of our measurements, we are interested in exploring the accuracy of this perception. Therefore, we need a set of destinations to test paths through these exchanges.

We selected a set of well-known NAPs, including Worldcom MAE-East, MAE-West, MAE-Central, SBC/Ameritech AADS, and PAIX in Palo Alto. For each NAP, we gather a list of low-tier (i.e., low in the hierarchy) customers attached to the NAP. The customers are typically listed at the Web sites of the NAPs. As in each of the above cases, we use the hierarchy information from [33] to determine if a customer is small. Since these customers are low-tier, there is a reasonable likelihood that a path to these customers from any source passes through the corresponding NAP (i.e., they are not multihomed to the NAP and another provider). We then find a small set of addresses from the address block of each of these customers that are reachable via traceroute. We use the complete BGP table dump from the Oregon route server [30, 29] to obtain the address space information for these customers.

Next, we use a large set of public traceroute servers (153 traceroute sources from 71 providers) [34], and trace the paths from these servers to the addresses identified above using a script to automate the process.