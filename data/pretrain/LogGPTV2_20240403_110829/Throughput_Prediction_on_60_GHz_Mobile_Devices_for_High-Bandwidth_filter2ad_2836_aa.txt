# Title: Throughput Prediction on 60 GHz Mobile Devices for High-Bandwidth, Latency-Sensitive Applications

## Authors:
- Shivang Aggarwal<sup>1</sup>
- Zhaoning Kong<sup>2</sup>
- Moinak Ghoshal<sup>1</sup>
- Y. Charlie Hu<sup>2</sup>
- Dimitrios Koutsonikolas<sup>1</sup>

### Affiliations:
- <sup>1</sup> Northeastern University, Boston, USA
- <sup>2</sup> Purdue University, West Lafayette, USA

---

## Abstract
In the near future, high-quality virtual reality (VR) and video streaming at 4K/8K resolutions will require gigabit throughput to maintain a high user quality of experience (QoE). IEEE 802.11ad, which standardizes the 14 GHz of unlicensed spectrum around 60 GHz, is a prime candidate for meeting these demands wirelessly. To maintain QoE, applications need to adapt to changing network conditions through quality adaptation, a key component of which is throughput prediction. At 60 GHz, the throughput can vary sharply due to blockage and mobility, making throughput prediction challenging.

In this paper, we conduct an extensive measurement study to assess the predictability of network throughput in an 802.11ad WLAN when downloading data to an 802.11ad-enabled mobile device under varying mobility patterns and orientations. We demonstrate that with carefully designed neural networks, we can predict the throughput of the 60 GHz link with good accuracy at different timescales, from 10 ms (suitable for VR) up to 2 s (suitable for ABR streaming). We also identify the most important features affecting the neural network prediction accuracy: past throughput and modulation and coding scheme (MCS).

---

## 1. Introduction

The past few years have seen the rise of several high-bandwidth, latency-sensitive applications, including virtual reality (VR), high-resolution video streaming, live video streaming, and connected autonomous vehicles. These applications demand stringent user-perceived QoE, which in turn requires ultra-high throughput and low latency. Additionally, such applications typically run on mobile devices, necessitating high network performance to be supported wirelessly. For example, 8K resolution VR demands 1.2 Gbps to satisfy the 20 ms photon-to-motion latency, while live 4K video streaming at 30 FPS requires 1.8 Gbps for good user QoE.

Such stringent network performance demands were not feasible in the past decade. However, the advent of millimeter-wave (mmWave) technologies has made it possible to achieve the required network performance. The IEEE 802.11ad WLAN standard, which governs the use of the unlicensed spectrum around 60 GHz, supports 2 GHz wide channels to provide PHY data rates of up to 6.7 Gbps. However, 60 GHz networks exhibit higher dynamics due to their unique propagation characteristics. Specifically, the high attenuation loss at 60 GHz necessitates directional communication, making the wireless link highly susceptible to human blockage and mobility. This can lead to long periods of rebuffering or stalls for users engaged in activities like 360° video viewing or VR gaming over a 60 GHz network.

Fortunately, many network-demanding applications already incorporate some form of quality adaptation to handle network dynamics. Adaptive bitrate (ABR) streaming, for instance, has become a de facto mechanism in modern video streaming systems like YouTube, backed by various adaptive streaming standards. ABR streaming continuously monitors network conditions and adjusts content quality to optimize QoE, which is a function of frame resolution, frame continuity, and rebuffering time. Similar adaptation techniques have been proposed for state-of-the-art mobile VR systems.

The first task in network adaptation is estimating network conditions for the next time interval. In video streaming, most ABR systems estimate the throughput in the next interval and choose a video quality level based on this estimate and playback buffer occupancy. Recently, the use of deep learning (DL) for selecting the most appropriate quality level has gained popularity, and ML-based ABR algorithms have outperformed traditional ones.

However, the unique characteristics of 60 GHz links make throughput prediction in 60 GHz WLANs more challenging than in legacy WLANs. Although throughput estimation has been studied in sub-6 GHz WLANs and cellular networks, no previous work, to our knowledge, has addressed throughput prediction in 60 GHz networks. In this work, we conduct the first measurement study of throughput predictability in 60 GHz WLANs using machine learning (ML).

There are two main challenges in conducting this measurement study. First, to reliably train and test any ML model, a significant amount of data is needed. Since 60 GHz WLANs are not widely deployed, and the only two 802.11ad-enabled smartphones (ASUS ROG Phone and ASUS ROG Phone II) are not VR-ready, we developed a methodology to collect traces in a controlled environment efficiently and in an automated way. We mounted the 802.11ad phone on a programmable 3-axis motion controller, collecting over 100 hours of traces under random mobility patterns. Second, unlike previous works, which make predictions on coarse-grained timescales, we study throughput prediction at fine timescales as short as 10 ms, which are necessary for demanding applications like VR. To support such fine timescales, we used lightweight ML models that balance accuracy and inference latency.

In summary, our work makes the following contributions:
- We conducted the first measurement study of the throughput predictability of a 60 GHz WLAN to a mobile device.
- We tuned the parameters of state-of-the-art throughput-prediction DNNs to balance prediction accuracy and lightweightness, suitable for online throughput prediction.
- We found that TCP throughput prediction in static scenarios is highly accurate for timescales from 40 to 2000 ms, with 95th percentile error ranging between 10.6% for 40 ms and 5.7% for 2 s. For 10–20 ms, the accuracy drops but remains satisfactory.
- In random mobility scenarios, the accuracy decreases, with 95th percentile error increasing to 38.1% for 10 ms and 19.4% for 2 s.
- We performed a feature selection study and found that past throughput and MCS are the most important features for accurate throughput predictions.
- Our study suggests that VR apps should be conservative in using throughput prediction, especially at the 10 ms timescale, where the prediction error is above 10% for 40% of the time, and the 95th percentile prediction error is 38%.

---

## 2. Experimental Methodology

### 2.1 Devices
We used a Netgear Nighthawk X10 Smart WiFi router and an ASUS ROG Phone II for our measurements. The Netgear router has a 10-Gigabit SFP+ Ethernet port, which we used to connect to a powerful desktop acting as the server. The ASUS ROG Phone II has an octa-core Snapdragon 855 Plus processor, a 6000 mAh battery, 8 GB RAM, and runs Android OS 10. Both devices support all 12 802.11ad single carrier MCSs, yielding theoretical data rates from 385 Mbps to 4.6 Gbps. However, the maximum TCP throughput in practical scenarios is limited to 1.65 Gbps.

### 2.2 Experimental Setup and Trace Collection
In all experiments except those with real applications, we used `nuttcp` with the default CUBIC congestion control to generate backlogged TCP traffic from the server to the phone, logging throughput every 10 ms. We developed an Android app that logs sensor and link state information, which serves as input for the ML models. The app uses the Android Sensor API to log the phone's rotation angle in the azimuth and pitch dimensions and acceleration on the x-, y-, and z-axes. Sensor data are logged every 10 ms. The app also logs 60 GHz link information reported by the `wil6210` driver every 20 ms, including the MCS used by the AP, link quality estimators (SQI, RSSI), link status, and selected beamforming sectors.

Since 60 GHz WLANs are not widely deployed, and our phone is not VR-ready, we used a Google Cardboard headset to emulate a realistic signal propagation environment. For mobility experiments, we mounted the headset on a Cinetics Lynx 3-Axis Slider, enabling full 360° rotation and translational motion. Using the Dragonframe software, we programmed custom mobility patterns, collecting over 100 hours of traces.

### 2.3 Trace Processing
Applications have diverse requirements for throughput prediction timescales. For example, VR applications need to predict throughput in the next tens of milliseconds, while video streaming applications need to predict average throughput over several seconds. We study throughput predictability over 802.11ad for timescales of 10 ms, 20 ms, 40 ms, 100 ms, 400 ms, 1000 ms, and 2000 ms. We log throughput samples every 10 ms and then convert them into coarser timescales offline by averaging consecutive samples. For categorical features, we consider the last data point in each window. We use the first 15,000 data points for training and the next 3,000 for testing.

### 2.4 Machine Learning-Based Prediction
Recent work has shown that simple DNNs can predict throughput well at the 2-s timescale. We focused on three neural networks, performing grid search to find the smallest configuration that balances accuracy and inference latency:
- **BP8**: A fully-connected neural network with 3 hidden layers, each with 40 neurons. It takes as input the actual throughput in the past 8 windows, pose information (azimuth and pitch) in the past 1 window, and link layer information (MCS, transmit beamforming sector, link status, SQI, and RSSI) in the past 1 window.
- **RNN8**: A recurrent neural network with 3 hidden layers, each with 20 neurons. It takes as input the actual throughput, pose information, and link layer information in the past 8 windows.
- **RNN20**: Same as RNN8, but takes information in the past 20 windows as input.

The neural network outputs the probability distribution (PD) of the throughput in the current window. The PD is over 21 bins of throughput in Mbps: B1 = [0, 50), ..., B21 = [1950, 2000]. The expected throughput is calculated as:

\[ \text{Throughput} = 0 \times P_1 + \sum_{i=2}^{20} \text{median}(B_i) \times P_i + 2000 \times P_{21} \]

### 2.5 Accuracy Metrics
We evaluate the performance of the throughput prediction models using three metrics:
- **RMSE**: Root mean squared error between the prediction and the actual throughput.
- **ARE95**: Absolute relative error of the prediction at the 95th percentile.
- **PARE10**: Percentage of predictions with absolute relative error below 10%.

---

This revised text aims to be more coherent, professional, and easier to follow, with clear section headings and a logical flow of information.