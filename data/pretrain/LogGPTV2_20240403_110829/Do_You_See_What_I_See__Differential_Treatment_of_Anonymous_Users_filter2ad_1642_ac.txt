### Overview of Scanning Activity and Opt-Out Process

We received a request to clarify our scanning activity. Our control nodes run web servers that identify our scanning as research and provide an email address for sites to opt out of further scanning.

### Data Collection and Exclusion Requests

During our measurements from March to August 2015, we received scan exclusion requests from 134 unique email addresses, covering 426 networks (a total of 3,532,751 hosts). This number represents an upper bound, as the machines at Michigan and Berkeley use site-wide scan notices, meaning a single complaint could have been triggered by any of the scans running from these sites.

### Final Analysis and Data Gathering

Once fully developed and debugged, we conducted 37 full IPv4 scans over a period of 7 days, with 16 scans originating from four Tor exit nodes. Table I provides a detailed breakdown of the measurements from both the control and Tor exit nodes. We now turn to analyzing the final data to understand two key aspects: temporal churn and spatial churn.

#### Temporal Churn

For the same location, we observed significant differences in the number of IP addresses that successfully responded, even between consecutive days, with variations up to 17%. Figure 2 illustrates the number of new IP addresses contacted per day. Starting from a peak of about 7 million on the second day, this value gradually decreases to about 4 million by the seventh day. The slow convergence rate indicates high temporal churn, suggesting that obtaining a true underlying web footprint for a given location may be challenging. Temporal churn is likely due to nodes that only come online occasionally, although we do not investigate the specific reasons in this paper.

#### Spatial Churn

Not all IP addresses responded to all three control locations, even though the control scans were initiated simultaneously. One potential cause for this phenomenon is wide-area routing issues. We identified IP addresses that responded to one or two locations (but not all three) as reflecting spatial churn, which corresponds to about 3.66% (approximately 3.7 million) of the responding IP addresses across the footprints from the three control nodes. Further investigation revealed that 52% of this spatial churn arose from IP addresses accessible from only one of the control nodes. Manual testing confirmed this behavior, ruling out ZMap-related issues.

### Definitions of Web Footprint

Given the significant amount of spatial and temporal churn, we defined two types of web footprints for our analysis:
1. **LAX Definition**: We remove cases of spatial churn. This set includes IP addresses for which all control nodes see a response at least once across the seven days.
2. **STRICT Definition**: We remove cases of both spatial and temporal churn. This set includes only IP addresses for which all control nodes received a successful response on all days.

The RAW footprint contains 103,329,073 IP addresses (2.82% of the probed set). The LAX footprint is 96% of the RAW footprint, while the STRICT definition reduces the RAW footprint to 50%. For reference and to understand the effect of network loss on our measurements (§ IV-F), we also report the numbers for the RAW footprint (response to any control node on any day).

### Assessing Network-Layer Discrimination

Having confidence in our measurement methodology, we analyzed the resulting data. We conducted scans from four high-bandwidth Tor exit nodes for 4 days (August 10–13, 2015), representing 3% of aggregate Tor exit bandwidth. Each exit node hosted 2–3 Tor processes on the same interface. To minimize load and potential packet loss, we turned off all but one Tor process during the experiment, reducing reported pcap loss to 0.001% of typical responses per scan. We chose Tor instances that use the same IP address for incoming and outgoing traffic to trigger even 'lazy' blacklists. For three exit nodes, we displayed our scan notice page on port 8080 instead of the usual port 80, as the latter already displayed a separate Tor abuse complaint page.

Our technique for flagging network-layer discrimination of Tor involves identifying the part of the web footprint that never produces a successful response to a Tor exit node. We examined this separately for each exit node, as we do not assume consistent blocking across all exits. After extracting this subset, we scanned the suspicious IP addresses five times from the corresponding exit node and discarded those that responded successfully at least once, reducing false positives. As a result, the blocked IPs per exit node reduced on average by 7.70% (σ=2.82%) for the RAW footprint, 8.94% (σ=3.23%) for the LAX footprint, and 1.05% (σ=0.74%) for the STRICT footprint. Our approach does not account for transient IP layer blocking such as abuse-based filtering, but assuming transient blocking is enforced for less than 4 days, we may still observe a successful response in scans conducted before or after the block.

Table III shows the breakdown of Tor blocking detected. We observed a significantly higher rate of blocking for the LAX footprint (13.01–16.14%) compared to the STRICT footprint (1.23–2.59%). This discrepancy could be due to multiple factors: the LAX footprint is more than double the STRICT footprint, leading to larger churn and a higher potential for false positives. Additionally, the LAX footprint exposes large access ISP networks, which may block Tor across the whole network. Due to the transient nature of nodes in such networks, they are less likely to be seen in the STRICT footprint.

Tables IV and V show the breakdown of ASNs that block Tor. The ASNs in the STRICT footprint are dominated by hosting services, suggesting policy or abuse-driven blocking. The LAX footprint includes ASNs that are potentially access and mobile ISPs, such as CHINANET, BSNL, and Airtel, which likely enforce symmetric blocking of Tor. These ISPs are more likely to have nodes go offline, explaining their absence in the STRICT list. Many of the ASes of IPs in the LAX footprint that block Tor traffic originate in countries known for censorship, such as China and Iran. Our results suggest that traffic coming from the Tor network may also be blocked, either as a policy or as an unintended effect of censorship mechanisms.

### Calculating the Effect of Network Loss

Packet loss introduces uncertainty in determining whether a given IP address specifically blacklists Tor traffic. In this section, we develop a Bayesian analysis to provide error bounds on our estimates of Tor-blocking. We assume IP addresses fall into one of four categories: allowing responses to all probes (A), denying responses to all probes (D), blacklisting probes from Tor nodes but otherwise responding (B), and whitelisting probes from Tor nodes but otherwise not responding (W).

This analysis helps us quantify the impact of network loss on our measurements and provides a more robust understanding of the data.