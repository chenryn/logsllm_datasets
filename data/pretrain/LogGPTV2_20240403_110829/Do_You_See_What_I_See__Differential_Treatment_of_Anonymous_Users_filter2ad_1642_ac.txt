we received a request. We run web servers on our control nodes
that identify our scanning activity as research and provide an
email address for sites to opt out of further scanning.
.
During our measurements from March to August 2015, we
received scan exclusion requests from a total of 134 unique
email addresses for 426 networks (covering a total of 3,532,751
hosts). Note that this number provides an upper bound as the
machines at Michigan and Berkeley use site-wide scan notices,
implying that a complaint could have been triggered by any
of the scans running from these sites .
Once fully developed and debugged, for our ﬁnal analysis
we gathered 37 full IPv4 scans over a period of 7 days,
conducting 16 from four Tor exit nodes. Table I shows the
breakdown of the measurements run from the control and
Tor exit nodes. We now turn to analyzing the ﬁnal data to
understand temporal churn—how the footprint changes across
scans spanning multiple days—and spatial churn—how our
view of the global web footprint set changes across the three
control locations.
a) Temporal Churn: For the same location, we see
signiﬁcant differences in the number of IP addresses that
successfully respond, even between consecutive days, ranging
up to 17%. Figure 2 shows the number of new IP addresses
that each site successfully contacts per day. Using the ﬁrst day
as the baseline, this value somewhat gradually drops from a
peak of about 7 million on the second day to about 4 million on
day 7. The slow convergence rate indicates that temporal churn
remains high even for the same location, and that obtaining a
true underlying web footprint for a given location may not be
well-deﬁned. Temporal churn is likely caused by nodes that
only come online occasionally; however we do not investigate
the reasons in this paper.
b) Spatial Churn: Not all IP addresses respond to all
three control locations, even though we initiated the control
scans all at the same time for each run. One potential cause
for this phenomenon is wide-area routing issues. We identify
IP addresses that only successfully responded to one or two lo-
cations (not all three) as reﬂecting spatial churn, corresponding
to about 3.66% (about 3.7 million) of responding IP addresses
across the footprints from the three control nodes. Upon further
investigation, we observed that 52% of this spatial churn arose
from IP addresses accessible from only one of the control
5
llllll2345674.05.06.07.0DayNumber of new IP addresses x106lllllllllllloooCambridgeMichiganBerkeleyExit Node
Axigy1
Axigy2
NForce2
Voxility1
Location
USA
USA
Netherlands
Romania
Uptime
35 days
76 days
35 days
1 day 17 hr
Bandwidth (MB/s)
31.09
31.46
31.46
16.99
TABLE II: Description of Tor exit nodes from which IPv4 scans were
conducted.
nodes. We tested a handful of these IP addresses manually
and conﬁrmed this behavior, ruling out that it reﬂected a ZMap
problem.
Given the signiﬁcant amount of spatial and temporal churn,
we settle on two deﬁnitions of web footprint to use for our
analysis. 1) a LAX deﬁnition, where we only remove cases of
spatial churn. For this set, we consider the set of IP addresses
for which all control nodes see a response at least once across
the seven days. 2) A STRICT deﬁnition, where we remove
cases of both spatial and temporal churn. We include in this
set only IP addresses for which all control nodes received
a successful response on all days. We ﬁnd that the RAW
footprint contains 103,329,073 IP addresses (2.82% of the
probed set). LAX footprint
is 96% of the RAW footprint,
whereas STRICT reduces the RAW footprint to 50%. For the
purpose of reference and understanding the effect of network
loss on our measurements (§ IV-F), we also report the numbers
for the RAW footprint (response to any control node on any
day).
E. Assessing Network-Layer Discrimination
Having gained conﬁdence in our measurement methodol-
ogy, we now turn to analyzing the resulting data. We conducted
the scans from four high-bandwidth Tor exit nodes for 4 days
(Aug. 10–13, 2015) (Table II). These represent 3% of aggre-
gate Tor exit bandwidth. We note that each exit node hosts 2–3
Tor processes on the same interface. As our 100 Mbps scans
use the same IP address as the Tor exit node, we turned off
all but one Tor process on these machines for the duration of
the experiment to minimize load on the interface and potential
packet loss on the interface and/or the outgoing link. These
preventive measures helped reduce our reported pcap loss on
the exit nodes to 0.001% of the typical number of responses
seen per scan. We also chose Tor instances that use the same
IP address for incoming and outgoing Tor trafﬁc to allow our
scans to trigger even ‘lazy’ blacklists.1 For three of the exit
nodes, we displayed our scan notice page on port 8080 instead
of the usual port 80, as the latter already displayed a separate
Tor abuse complaint page.
Our basic technique for ﬂagging network-layer discrimina-
tion of Tor is to identify the part of the Web footprint that never
produces a successful response to a Tor exit node. We examine
this separately for each exit node, as we do not assume that all
the exits are blocked consistently. Once we have extracted this
subset for an exit node, we scan the suspicious IP addresses
5 times from the corresponding exit node and discard IP
1The easiest approach to blacklist Tor is to block IP addresses from the node
descriptors in the directory consensus that denote the incoming IP address for
nodes. This blacklisting approach fails to cover nodes that use a different IP
address for outgoing trafﬁc, per Section II-B.
6
addresses that respond successfully at least once, effectively
reducing our false positives. As a result of this last step,
the blocked IPs per exit node reduce on average by 7.70%
(σ=2.82%) for RAW footprint, 8.94% (σ=3.23%) for LAX
footprint, and 1.05% (σ=0.74%) for STRICT footprint. We
note that our approach does not account for transient IP layer
blocking such as abuse-based ﬁltering. However, assuming that
transient IP blocking is enforced for a time window smaller
than 4 days, we may still observe a successful response in
scans conducted before or after the transient block. Using this
methodology, we characterize Tor blocking for both LAX and
STRICT Web footprints.
Table III shows the breakdown of the Tor blocking we
detect. We detect a signiﬁcantly higher rate of blocking for the
LAX footprint compared to STRICT (13.01–16.14%, and 1.23–
2.59%, respectively). This discrepancy could be caused by
multiple factors. First, the LAX footprint is more than double
the STRICT footprint, due to the weaker selection criteria. This
means that it is likely to see larger churn and therefore has a
larger potential for false positives. Second, as we see next,
the LAX footprint exposes large access ISP networks, which
potentially block Tor across the whole network. Due to the
transient nature of nodes in such networks, they are less likely
to be seen in the STRICT footprint.
Tables IV and V show the breakdown of the ASNs that
block Tor. Tables IV shows the distribution by the number of
IP addresses in an ASN that block Tor, for both the LAX and
the STRICT footprints. We see that the ASNs in the STRICT
footprint are dominated by hosting services, which suggests
that could be policy or abuse-driven.
The LAX footprint
contains ASNs that are potentially access and mobile ISPs,
such as CHINANET, BSNL, and Airtel. These ISPs likely
enforce symmetric blocking of Tor. Because they are access
ISPs, nodes in these networks are more likely to go ofﬂine,
which explains their absence in the STRICT list. We note that
ASes of IPs in LAX footprint that block Tor trafﬁc mostly
originate in countries that are notoriously known for their
censorship practices, such as China and Iran. Thus far these
countries have been reported to block access to Tor network,
but our results suggest that trafﬁc coming from Tor network
may also be blocked either as a policy or as an unintended
effect of the mechanism of censorship chosen.
Table V shows a similar result sorted by the proportion of
servers within a given ASN that block Tor. We see a higher
prevalence of hosting sites in both LAX and STRICT.
F. Calculating effect of network loss
The possibility of losing our probes or their responses due
to packet-loss introduces uncertainty as to whether a given
IP address speciﬁcally blacklists Tor trafﬁc. In this section
we develop a Bayesian analysis of this uncertainty so as to
provide error bounds on the estimates we derive from our
measurements of Tor-blocking.
For the purposes of error calculation we assume that IP
addresses fall into one of four categories: allowing responses
to all probes received (A), denying responses to all probes (D),
blacklisting probes from Tor nodes, but otherwise responding
(B), and whitelisting probes from Tor nodes, but otherwise
Footprint
RAW
LAX
STRICT
IP Addresses
103,329,073 (2.82%)
99,547,512 (2.72%)
52,148,437 (1.42%)
Axigy1 (%)
ret.
orig.
16.05
15.48
13.50
14.09
1.91
1.91
Axigy2 (%)
ret.
orig.
15.45
14.01
12.19
13.68
1.25
1.23
NForce2 (%)
ret.
orig.
17.66
16.18
14.59
16.14
2.59
2.55
Voxility1 (%)
ret.
orig.
16.20
14.65
13.01
14.63
1.88
1.82
TABLE III: Web footprint blocked across exit nodes. We show footprint as % of probed IP addresses (3,662,744,599). For each exit node, we present the original
(orig.) block proportion of the footprint and that retained (ret.) after weeding out false positives using 5 veriﬁcation scans.
Axigy1 (13.50%)
Axigy2 (12.19%)
NForce2 (14.59%)
Voxility1 (13.01%)
CHINA169-BACKBONE,CN (11.33)
CHINANET-BACKBONE,CN (7.42)
Uninet S.A.,MX (3.43)
DCI-AS(ITC),IR (2.94)
BSNL-NIB,IN (2.94)
CHINA169-BACKBONE,CN (11.73)
CHINANET-BACKBONE,CN (8.20)
DCI-AS(ITC),IR (3.26)
Uninet S.A.,MX (3.00)
DTAG Deutsche Telekom,DE (2.89)
CHINA169-BACKBONE,CN (11.02)
CHINANET-BACKBONE,CN (7.30)
AIRTELBROADBAND-AS-AP,IN (4.31)
BSNL-NIB,IN (4.30)
DCI-AS(ITC),IR (2.73)
CHINA169-BACKBONE,CN (12.53)
CHINANET-BACKBONE,CN (7.93)
DCI-AS(ITC),IR
Uninet S.A.,MX
DTAG Deutsche Telekom,DE
(a) LAX Web Footprint (99,547,512 IP addresses forming 2.72% of probed IPv4)
Axigy1 (1.91%)
Axigy2 (1.23%)
NForce2 (2.55%)
Voxility1 (1.82%)
MCCI-AS,IR (11.91)
RMH-14-Rackspace,US (10.87)
RACKSPACE-Rackspace,US (9.92)
DREAMHOST-AS,US (8.44)
Rackspace Ltd.,GB (5.85)
MCCI-AS,IR (18.44)
DREAMHOST-AS,US (13.07)
KUNET-AS,KR (3.59)