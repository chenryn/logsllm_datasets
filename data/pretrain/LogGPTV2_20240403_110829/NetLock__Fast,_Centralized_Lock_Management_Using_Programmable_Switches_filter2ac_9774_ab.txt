scheduling; this reduces starvation and achieves high throughput.
Decentralized lock managers typically use advisory locking, where
clients cooperate and follow a distributed locking protocol. This
is because the clients use RDMA verbs to interact with the lock
table in the lock server without involving the server’s CPU. It is
different from mandatory locking used by centralized lock man-
agers that can enforce a locking protocol, as the lock manager is
solely making locking decisions. Besides the difficulty to enforce
a protocol, decentralized lock managers cannot flexibly support
various policies such as isolation, without significantly degrading
performance using an expensive distributed protocol.
2.2 Exploiting Programmable Switches
Providing both high performance and policy support. Tradi-
tional server-based approaches make a trade-off between perfor-
mance and policy support. Centralized approaches provide flexi-
ble policy support, but have low performance; decentralized ap-
proaches achieve the opposite. The goal of this paper is to design a
solution that sidesteps the trade-off and provides both high perfor-
mance and policy support. Our key idea is to design a centralized
solution with fast switches, which can benefit from switches to
achieve high performance while still providing flexible policy sup-
port as being a centralized approach. Moreover, since switches
provide orders-of-magnitude higher throughput and lower latency
than servers, this solution is even faster than decentralized, RDMA-
based approaches. This is especially important for emerging fast
transaction systems based on RDMA networks and in-memory
storage [18, 46]. In these systems, the transactions themselves are
Clients
NetLock
L2/L3 
Routing
Lock
Table
ToR Switch
Server
Lock Table
Server
Lock Table
Database
Servers
Figure 2: NetLock architecture.
executed in memory, and thus the execution cost is comparable to
the locking and unlocking cost, meaning that the system needs to
spend a considerable amount of server resources for lock managers
as for the storage servers themselves. Leveraging switches to build
faster lock managers can both improve the transaction performance
and reduce the system cost.
Building lock managers with programmable switches. While
traditional switches are fixed-function, emerging programmable
switches, such as Barefoot Tofino [9], Broadcom Trident [5] and
Cavium XPliant [1], make it feasible to design, build and deploy
switch-based lock managers. Leveraging programmable switches
provides orders-of-magnitude higher performance than FPGA-based
(e.g., SmartNICs) or NPU-based solutions. While this paper focuses
on programmable switches, the mechanisms designed for NetLock
can also be applied to programmable NICs.
Programmable switches allow users to develop custom data plane
modules, which can parse custom packet headers, perform user-
defined actions, and access the switch on-chip memory for stateful
operations [12, 13]. With this capability, we can program the switch
data plane to parse lock information embedded in a custom header
format, to perform lock and unlock actions, and to store the lock
table in the switch on-chip memory.
3 NETLOCK ARCHITECTURE
In this section, we first give the design goals of NetLock, and then
provide a system overview of NetLock.
3.1 Design Goals
NetLock is a fast, centralized lock manager. It is designed to meet
the following goals.
• High throughput. State-of-the-art distributed transaction sys-
tems can process hundreds of millions of transactions per second
(TPS) with a single rack [18, 30, 45], and each transaction can
involve a few to tens of locks. To avoid being the performance
bottleneck of fast distributed transaction systems, the lock man-
ager should be able to process up to a few billion lock requests
per second (RPS).
• Low latency. Given the tens of microseconds transaction latency
enabled by fast networks and in-memory databases [18, 30, 45],
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Zhuolong Yu, Yiwen Zhang, Vladimir Braverman, Mosharaf Chowdhury, and Xin Jin
NetLock
Algorithm 1 ProcessLockRequest(req)
Client
TXN
1. acquire lock
Switch
Lock
Table
Server
Lock
Table
2. grant lock
3. release lock
if r eq .type == acquir e then
1: if r eq .l ock ∈ swit ch .l ocks () then
2:
3:
4:
5:
6:
7:
8:
9:
10:
11: else
12:
Forward r eq to server
Forward r eq to server
else
else
if swit ch .CanGr ant (r eq) then
Grant r eq .l ock to r eq .cl ient
else if swit ch .CanQueue (r eq) then
Queue r eq at switch
Release r eq .l ock , and grant it to pending requests
Figure 3: Lock request handling in NetLock. The switch di-
rectly processes most lock requests.
the lock manager should provide low latency to process lock
requests, in the range of a few to tens of microseconds.
• Policy support. For a cloud environment, the lock manager
should provide flexible policy support to accommodate tenant-
specific requirements. Specifically, we consider common policies
including starvation freedom, service differentiation, and perfor-
mance isolation.
3.2 System Overview
A NetLock lock manager consists of one switch and multiple servers
in the same rack (as shown in Figure 2), where the round-trip time
(RTT) between machines within the same switch is typically single-
digit microsecond. The switch is the ToR switch of a dedicated
database rack that is specifically provisioned for database services,
which is common in public clouds. Different database racks have
their own NetLock instances. Besides adding a new data plane
module for NetLock to the ToR switch, no other changes are made to
the datacenter network. The ToR switch only invokes the NetLock
module to process lock requests, and it processes other packets as
usual. NetLock does not affect existing network functionalities.
At a high level, clients send lock requests to NetLock without
knowing whether the requests will be processed by a switch or
a server. Behind the scene, NetLock processes lock requests with
a combination of switch and servers. It integrates the switch and
server memory to store and process lock requests. When a lock
request arrives at the switch, the switch checks whether it is re-
sponsible for the lock. If so, it invokes the data plane module to
process the lock; otherwise, it forwards the lock requests to the
server. The switch only stores and processes the requests on pop-
ular locks, while the lock servers are responsible for the requests
on unpopular locks. The lock servers also buffer the requests on
popular locks when the queues in the switch are overflowed.
4 NETLOCK DESIGN
In this section, we describe the design of NetLock that exploits
programmable switches for fast, centralized lock management.
4.1 Lock Request Handling
As shown in Figure 3, to acquire a lock for a transaction, the client
first sends a lock request to NetLock and waits for NetLock to grant
the lock. NetLock directly processes most lock requests with the
lock switch and only leaves a small portion to the lock servers. After
the lock is granted, the client executes its transaction and sends a
release notification to NetLock if the lock is no longer needed.
Algorithm 1 shows the pseudocode of the switch. Since the
switch is the ToR switch of the database rack and is on the path for
a request to reach the lock servers, the switch can always process
the request first. If the switch is responsible for the corresponding
lock object (line 1), it checks the lock availability and policy. If
the lock can be granted, the switch directly responds to the client
(line 3-4). If the lock cannot be granted immediately, the switch
queues the request if it has enough memory (line 5-6). If the switch
is not responsible for the lock object or does not have sufficient
memory, it forwards the request to the lock server based on the
destination IP (line 8 and 12). The locks are partitioned between
the lock servers. The client obtains the partitioning information
from an off-the-shelf directory service in datacenters [20, 25], and
sets the destination IP to that of the server responsible for the lock.
After the client releases the lock, NetLock can further grant the
lock to other requests (line 10). The performance benefit of NetLock
comes from that most requests can be directly processed by the
switch, without the need to visit a lock server.
One-RTT transactions. In the basic mode, a client gets a grant
from NetLock (taking 0.5 RTT by the lock switch or 1 RTT by
the lock server) and then issues another request to fetch the data
from a database server (taking 1 RTT) to finish the transaction,
which takes 1.5ś2 RTTs in total. Some recent distributed transaction
systems (e.g., DrTM [46], FARM [19] and FaSST [30]) combine lock
acquisition and data fetching in a single request to a database server,
and thus are able to finish a transaction in 1 RTT. NetLock can
apply the same idea to achieve one-RTT transactions. Specifically,
after a lock is granted, instead of replying to the client, NetLock
forwards the request to the corresponding database server to fetch
the item, making lock acquisition and data fetching in one RTT.
More importantly, unlike existing solutions (e.g., DrTM, FARM and
FaSST) that rely on fail-and-retry which may lead to low throughput
and high latency, all requests to the database servers can successfully
fetch data, because the locks have already been granted by NetLock.
This is critical under high-contention scenarios to reduce overhead
at both clients and database servers, and achieve high throughput
and low latency. For locks not in the switch, the lock server is
combined with the database server as existing solutions to achieve
one-RTT transactions. For requests with payloads such as writes,
the switch forwards the data if the lock can be granted, and drops the
data, otherwise. Some transactions that involve read-modify-write
NetLock: Fast, Centralized Lock Management
Using Programmable Switches
Match-Action
Match
pkt.lid==A
pkt.lid==B
pkt.lid==C
Table
Action
process_A() process_B() process_C()
Register
Array
Queue
A
Queue A
Queue B
Queue C
0
1
2
3
4
5
6
7
req0 req1 req2 req3 req4 req5
head
(mode, transaction ID, client IP)
tail
Figure 4: Basic data plane design for lock management.
operations cannot fundamentally be done in one RTT because the
client has to do some compute and the current design does not push
compute to the lock and database servers. In addition to its high
performance, NetLock also supports flexible policies that cannot
be implemented by existing decentralized solutions.
4.2 Switch Data Plane
Programmable switches expose stateful on-chip memory as register
arrays to store user-defined data. NetLock leverages register arrays
to store and process lock requests in the switch. Figure 4 shows a
basic data plane design. The design allocates one array for each lock
to queue its requests. A special UDP destination port is reserved
for NetLock. A lock request contains several fields: action type
(acquire/release), lock ID, lock mode, transaction ID, and client IP.
The match-action table maps a lock ID (i.e., lid) to its corresponding
register array, and the action in the table performs operations on
the register array to grant and release locks.
Because register arrays can only be accessed based on a given
index, they do not natively support queue operations such as en-
queue and dequeue. We implement circular queues based on register
arrays to support necessary operations for NetLock. Specifically,
we allocate extra registers to keep the head and tail pointers. The
pointers are looped back to the beginning when they reach the
end of the array. For example, queue A in Figure 4 has six queued
requests, and the head and tail are index 1 and 6, respectively.
Each slot in a queue stores three important pieces of information,
i.e., mode, transaction ID, and client IP. Mode indicates whether the
request is for a shared or exclusive lock. Transaction ID identifies
which transaction the lock is requested for. Client IP stores the IP
address from which the lock request is sent. The IP address is used
by the switch when it generates a notification to grant the lock to
the client. Additional metadata such as timestamp and tenant ID
can also be stored together.
Optimize switch memory layout. Because the memory for each
register array is pre-allocated and the size is fixed after the data
plane program is compiled and loaded into the switch, the basic
design cannot flexibly change the queue size at runtime. When the
workload changes, the set of locks in the switch and the size of each
queue would need to change according to the memory allocation
algorithm to maximize the performance. Allocating a large queue
to accommodate the maximum possible contentions for each lock
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Array 0
Array 1
Array 2
Register
Array
Shared
Queue
Queue A
left_B
= 10
Queue B
right_B
= 14
Queue C
Figure 5: Combine multiple register arrays to a shared queue
for locks with different queue sizes.
is undesirable because it would cause memory fragmentation and
result in low memory utilization, especially given that the switch
on-chip memory is limited.
To address this problem, we design a shared queue to pool multi-
ple register arrays together and enable the queue size to be dynam-
ically adjusted at runtime (Figure 5). Instead of statically binding
each register array to a lock, we combine these arrays together to
build a large queue shared by all the locks. Accessing a slot in the
shared queue with an index can be mapped to accessing the register
arrays by appropriately setting the index, e.g., accessing slot 10 in
the shared queue can be mapped to accessing slot 10-8=2 in array
1. Each lock is allocated with a continuous region in the shared
queue to store its requests. We allocate extra registers to store the
boundaries of each queue, e.g., 10 and 14 for queue B. Since the
boundaries are stored in registers, they can be modified at runtime.
Another benefit of this design is that the individual register arrays
do not have to be in the same stage, which allows NetLock to pool
memory from multiple stages together to build a large queue that
exceeds the memory limit of a single stage.
Handle shared and exclusive locks. The shared queue design
solves the storage problem of how to store the requests, but it
does not solve the computation problem of how to process them.
The challenge comes from the limitation that the data plane can
only perform one read/write operation to a register array when it
processes a packet.
This limitation brings two issues. First, when a lock release notifi-