We observed that the BSR quickly increases to the high-
est level (150KB+) when there is large LTE upload trafﬁc.
Also there exists a strong correlation (around 0.73) between
RT TF and buffer level in BSR. Leveraging the BSR infor-
mation, we measured that the actual ﬁrmware buffer occu-
pancy can reach several hundreds of KBs (using a more ac-
curate algorithm described in §6.2), and the ﬁrmware queu-
ing delay (TF ) can reach 400ms with 8Mbps uplink. By
subtracting the RT TF by the ﬁrmware queuing delay, we
can estimate RT TB, which is constantly low (e.g., around
50ms for Carrier 1), as shown in Figure 7.
Overall, the above ﬁndings have two important implica-
tions. First, cellular uplink scheduling is performed in a
centralized manner, different from that in Wi-Fi networks
where clients autonomously sense the wireless channel to
transmit data and avoid collision in a distributed way. Sec-
ond, the ﬁrmware buffer distinguishes itself from other on-
device buffers in that its occupancy plays a role in the cellu-
lar control plane which in turn affects eNodeB’s scheduling
decisions and the achievable uplink throughput.
4.3 Prevalence across Carriers & Devices
We show the prevalence of on-device bufferbloat in Fig-
ure 9 by repeating the upload experiments on various net-
4In the remainder of this paper, we use the general term
“ﬁrmware buffer” to refer to the RLC buffer.
Figure 8: Packet processing and transmission on An-
droid devices.
in a data structure called skb. A TCP packet is encapsu-
lated into skb (with its TCP header being added) and sent
to IP layer in tcp_transmit_skb() at time tsQ. After be-
ing processed at the IP layer, the skb with TCP/IP header
is subsequently injected to the queuing discipline (Qdisc)
when dev_queue_xmit() is called. When the driver is
ready to transmit more data, dev_hard_start_xmit() is
called by the kernel to dequeue the packet from Qdisc to the
driver buffer at time tsD. Similarly, when the radio ﬁrmware
of the chipset is ready to receive a packet from the driver,
ndo_start_xmit() will be called to enqueue the packet to
the ﬁrmware buffer at time tsF . The kernel usually does not
have direct control over the logic of radio ﬁrmware, which
determines when to actually transmit the packet at time tsN .
As mentioned in §4.1, in this study we focus on the queu-
ing delay below the transport layer, as lower-layer buffers are
usually shared, causing potential interference across multi-
ple apps. We now describe lower-layer queuing in details.
In-kernel Queuing. To identify where on-device queuing
occurs exactly in the kernel, we use Linux kernel debugging
tool jprobe to log timestamps of the aforementioned func-
tion calls. We found that the queuing delay in the queuing
discipline (Qdisc), denoted as TQ, is almost identical to the
difference between RT TQ and RT TF . Besides, the delay
between tcp_transmit_skb() and dev_queue_xmit(),
as well as the driver queuing delay (TD) are negligible. This
indicates that when sending out trafﬁc in cellular networks,
packet queuing in Qdisc dominates the on-device queuing
delay in the kernel. Also, as another validation, we observe
App1App2AppNApplication LayerTransportLayerApplication data into TCP sockets.  .  .tcp_transmit_skb()ACKLinuxQdiscQdisc enqueue: dev_queue_xmit()Queueing Disciplinedev_hard_start_xmit()TQ =RTTQ - RTTFDeviceDriverRadioFirmwarendo_start_xmit()Driver buffertcpdumpTD ≈ 0 PHY Modulation and CodingtsTtsQtsDtsFtsNtTCP buffer(per connection)TT       Firmware bufferTF =RTTF - RTTBKernel spaceUserspaceRadiochipsetNetworktsA 1 10 100 1000SGS3 Carrier 1LTESGS3Carrier 1HSPA+SGS3Carrier 2LTESGS3Carrier 2HSPA+SGS3Carrier 3LTEHTC One SCarrier 1HSPA+Delay (ms)TQRTTF308Throughput
RTT
% AVG
decrease
% RSD % AVG % RSD
increase
increase
increase
(a) Measurement error with
20ms interval
(b) Measurement error with
100ms interval
Figure 10: Uplink throughput measurement error at dif-
ferent layers.
works using two different devices. For each setting, we re-
port the 5th, 25th, 50th, 75th, and 95th percentiles of TQ
and RT TF . We observe on-device bufferbloat on all set-
tings in LTE, with median TQ larger than 200ms. Regarding
the HSPA+ network, TQ is small (around 20ms) for SGS3
using Carrier 2. This is because the TCP sending buffer size
is conﬁgured to be small by Carrier 2 on this device (we will
discuss the impact of TCP buffer size in §6.1). Yet across all
settings, we found that the RT TF is much larger than the es-
timated RT TB, indicating that excessive ﬁrmware queuing
happens on all devices and carriers.
4.4 Uplink Throughput Measurement
Often applications (e.g., real-time multimedia apps) need
to know the instantaneous network throughput. The lower-
layer information provided by ﬁrmware enables accurate
cellular throughput measurement. Recall in §4.2 that the
UE can only send the amount of data up to the scheduling
grant.
If a portion of the grant is not used, the ﬁrmware
uses padding to indicate the unused part. The padding size
is also reported by the ﬁrmware. Therefore, by subtracting
the scheduling grant by the padding size, we can calculate
the amount of data sent out from the device, as well as the
uplink throughput (the padding is not transmitted).
Since the above approach directly utilizes lower-layer in-
formation from the cellular control plane, it gives the ground
truth of cellular uplink throughput. An interesting ques-
tion is, compared to this ground truth, how accurate is the
throughput measured at upper layers? We quantify this in
Figure 10, which plots the measurement error at Qdisc, TCP,
and application layer where we use a slide window of 100ms
and 20ms to estimate uplink throughput during a bulk up-
load. The results indicate that the throughput estimation
at higher layers are highly inaccurate, with the root mean
square being 141%, 136%, and 70% at the application layer,
the transport layer, and the Qdisc, respectively, when the
estimation interval is 100ms. Reducing the interval further
worsens the accuracy. The root cause of such inaccuracy is
again the on-device bufferbloat: when a higher layer delivers
a potentially large chunk of data into large low-layer buffers,
the higher layer thinks the data is sent out but the data will
stay in the buffer for a long time. In fact the higher layer has
no way to know when the data actually leaves the device.
SGS3 C1 LTE
SGS3 C1 HSPA+
SGS3 C2 LTE
SGS3 C3 LTE
HTC One S
C1 HSPA+
66
8
10
80
22
253
25
36
192
260
91
7
10
86
10
37
9
20
42
91
Table 1: Impact of upload on download performance on
different devices, vendors, and networks (C1, C2, and C3
refer to Carrier 1, 2, and 3 respectively).
As indicated in Figure 10, as the location of measurement
moves to higher layers, the overall on-device buffer size in-
creases, leading to worse estimation accuracy.
5.
IMPACT OF UPLOAD ON MOBILE
APPLICATION PERFORMANCE
This section quantiﬁes the impact of upload on some pop-
ular applications’ performance: ﬁle download, web brows-
ing, video streaming, and VoIP. We compare user-perceived
application performance in two scenarios: without and with
concurrent upload. The experiments in this section were
conducted on a Samsung Galaxy S3 phone using Carrier 1’s
LTE network unless otherwise mentioned. We use a single
TCP connection to generate upload trafﬁc in controlled ex-
periments.
5.1 Impact of Upload on Bulk Download
When upload and download exist concurrently, upload
trafﬁc can affect download trafﬁc in two ways: in-network
and on-device. The former is well-known [47]: upload
data shares the same network link with TCP ACK pack-
ets of download data, leading to potentially delayed uplink
ACK for download. This can cause the server to retransmit
download data and reduce the congestion window size, ulti-
mately leading to lower download throughput. On the other
hand, the on-device queuing delay triggered by upload can
also severely affect download by delaying its ACK packets
(shown as t1 in Figure 6(b)), since when download and up-
load trafﬁc coexist, uplink TCP ACKs share the same queues
(e.g., Qdisc and ﬁrmware buffers) with uplink data, as de-
tailed in §4.2.
We carried out experiments of running a one-minute TCP
download ﬂow with and without a concurrent TCP upload
ﬂow on different devices and carriers with the setup de-
scribed in §2.1. Table 1 quantiﬁes the impact of upload
on download in four aspects, using bulk download in ab-
sence of upload as the baseline: (i) decrease of the aver-
age (AVG) download throughput, (ii) increase of the rela-
tive standard deviation (RSD)5 of download throughput, (iii)
increase of AVG RTT, and (iv) increase of RSD of RTT.
5Relative standard deviation (RSD) = standard deviation /
mean.
 0 0.25 0.5 0.75 1-100-50 0 50 100CDFEstimation error (%)AppTCPQdisc 0 0.25 0.5 0.75 1-100-50 0 50 100CDFEstimation error (%)AppTCPQdisc309Figure 11:
TCP/UDP throughput.
Impact of Uplink trafﬁc on downlink
All carriers exhibit performance degradations in various de-
grees. In particular, large ﬂuctuation of throughput and RTT
exists when there is background upload, posing challenges
for user-interactive applications. We also compare the in-
network and the on-device impact of upload on download
trafﬁc, by computing t1/t2 in Figure 6(b). The mean and
median values of the fractions of t1 in t2 are as high as 63%
and 74%, indicating the on-device queuing delay dominates
the overall RTT of download trafﬁc.
Next, we show that when uplink and downlink trafﬁc are
both present, the uplink ACK packets being delayed is the
dominating cause of degraded download performance. Fig-
ure 11 plots the download throughput distributions in three
scenarios using Carrier 1’s LTE network: (i) TCP download
only, (ii) TCP download with concurrent UDP upload, and
(iii) UDP download with concurrent UDP upload. Figure 11
indicates that (i) and (iii) exhibit similar download perfor-
mance (scenario (iii) is even slightly better because it uses
UDP for download) while the throughput in Scenario (ii) is
much lower. Since the key difference between (ii) and (iii)
is whether the uplink ACK stream exists, the results indicate
that the degraded download performance is almost solely as-
sociated with TCP’s upstream ACKs, whereas in the under-
lying radio layer, uplink and downlink use different channels
and can be performed independently. Similar results are ob-
served for upload performance (ﬁgure not shown).
5.2
Impact on Web Browsing
We next examine the impact of upload trafﬁc on web
browsing. We picked ten popular websites from Alexa top
sites, and loaded each of them in Google Chrome browser
on a Samsung Galaxy S3 phone in two settings: without and
with concurrent upload. We repeat the test of each website
for 5 times in a row and report the average results. We per-
formed cold-cache loadings for all sites, and measured the
page load time (PLT) using QoE Doctor [14, 32].
We found that upload trafﬁc signiﬁcantly inﬂates most de-
lay components. For example, the connection setup delay,
which usually takes only one round-trip, increases by 64%
to 509% due to on-device bufferbloat as the dominating fac-
tor. A similar case happens to HTTP requests, which can
typically ﬁt into one single TCP packet. HTTP responses
that carry downlink data are also affected due to the explana-
tions described in §5.1. The response duration inﬂates by up
to 3464%. Overall, the increase of PLT across the 10 web-
(a) δt = 0
(b) δt = 4s
(c) δt = 8s
Figure 12: Impact of upload on PLT. The web browsing
session begins δt after upload starts. “X” indicates the
upload is completed before the web page is fully loaded.
sites ranges from 219% to 607%. The results indicate when
concurrent upload is in progress, on-device bufferbloat can
signiﬁcantly affect short-lived ﬂows.
Next, we show that even a medium-sized upload can cause
signiﬁcant degradation of user experience. Figure 12 repeats
the above experiments but uses a ﬁnite size of upload start-
ing at δt seconds before the web browsing session begins. In
each subﬁgure, a heatmap block (x, y) visualizes the PLT in-
ﬂation caused by an upload of size x for website y. An “X”
mark indicates the upload is completed before the page is
fully loaded or even started to load so the measured PLT in-