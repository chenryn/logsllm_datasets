every
every 2nd
every 2nd
every 2nd
every
every
every
every
every
every
1st
1st
1st
last
1st
1st
last
1st
last
last
last
last
1st+last
1st+last
1st+last
1st+last
1st+last
every 2nd
1st+last
Table VI shows the estimated strength measures for pass-
words of length 12 from the 18 employed generation rules.
The discussion below focuses on the results for the Webis-
Sentences-17 corpus. While mnemonic password distributions
in the real world contain passwords from different lengths, we
restrict the analysis here to passwords from one length in order
to make the comparison easier to understand, as it removes the
inﬂuence of the length distribution. Especially generation rules
that use two characters per word have very different length
distributions. Strength estimates based on a natural distribution
of password lengths are discussed from Section V-C onwards.
For a fair comparison, we use the same number of passwords
for all estimates, and mark estimates for rules for which our
data has less passwords in gray. These estimates in gray are
less reliable and biased to higher values for H1.
For the online scenario measures min-entropy H∞ and
failure probability λβ, comparable strengths are achieved by
all generation rules but
those that use multiple characters
and every word, which are considerably weaker. For H∞, a
further factor is the character set where ASCII has about 1 bit
advantage. For λ100, generation rules that use every second
word are stronger than other rules.
For the ofﬂine scenario measure H1, passwords from
ASCII achieve a similar strength to passwords with only
lowercase letters when every word is used, but better strength
when every second word is used. In total, using every second
word and only the ﬁrst character with the ASCII character set
leads to the strongest of the tested password distributions. Also,
word preﬁx replacements can increase the entropy by 2–3 bit.
Moreover, using the ﬁrst character of a word is preferable.
The strongest distribution is arguably using the ASCII
character set, every second word, and only the ﬁrst characters,
which achieves best or nearly-best values for all measures.
Word preﬁx replacement considerably increase the strength for
H1, but not for the online scenario. However, both using only
every second word and word preﬁx replacements come with
additional memorization and processing costs, a discussion of
which lies outside the scope of this publication.
B. Estimates by Sentence Complexity
Table VI also shows that strength estimates for the Webis-
Simple-Sentences-17 corpus are most
times a bit weaker,
but still very similar, to those from the Webis-Sentences-17
corpus for all distributions with sufﬁcient training passwords.
The maximum difference for one generation rule between the
corpora are 1.6 bit for H∞, 0.00026 for λ10, 0.00071 for λ100,
and 1.9 bit for H1. This corresponds to a large difference for
H∞ and a still noticeable difference for H1, but smaller than
one could expect.
Therefore, mnemonics with lower complexity do indeed
lead to passwords that are easier to guess. This is likely due
to the reduced vocabulary of the mnemonics, which is biased
towards words with less syllables.
The effect of mnemonic complexity is especially strong
for the min-entropy H∞, which considers the most probable
password only. A possible explanation for this is that the
most probable password stems from simple sentences, even for
the Webis-Sentences-17 corpus. Then, the probability of this
password increases naturally when more complex sentences
are ﬁltered out.
On the other hand, the effect of mnemonic complexity
is still noticeable for the Shannon entropy H1, which con-
siders the entire password distribution. Therefore, reducing
the complexity skews the entire password distribution farther
away from the uniform distribution. However, the effect is
much weaker than for min-entropy. An estimate of the effect
can be the maximum difference in Table VI between Webis-
Sentences-17 and Webis-Simple-Sentences-17 for generation
rules with sufﬁcient training passwords, divided by the pass-
word size. This estimates the effect to 0.16 bit per character.
9
Table VII.
ESTIMATED ENTROPY BY GENERATION RULE AND
MINIMUM PASSWORD LENGTH FOR PASSWORDS FROM THE
WEBIS-SENTENCES-17 CORPUS.
Char. set L. letters ASCII L. letters ASCII
Replacement
Word
Char. pos.
-
every
1st
-
-
every 2nd every 2nd every 2nd
1st
1st
1st
ASCII
(cid:88)
Shannon entropy H1
37.9
41.3
44.8
48.3
51.8
55.2
58.7
62.2
65.7
69.1
72.6
76.1
79.6
83.0
86.5
90.0
93.5
97.0
100.4
103.9
107.4
110.9
114.3
34.8
38.0
41.2
44.4
47.6
50.8
54.0
57.2
60.4
63.6
66.8
70.0
73.2
76.4
79.6
82.8
86.0
89.2
92.4
95.6
98.8
102.0
105.2
37.2
40.9
44.6
48.3
52.0
55.7
59.4
63.1
66.8
70.5
74.2
77.9
81.6
85.3
89.0
92.7
96.4
100.1
103.8
107.5
111.2
114.9
118.6
39.0
42.9
46.7
50.6
54.5
58.4
62.3
66.2
70.1
74.0
77.9
81.8
85.7
89.5
93.4
97.3
101.2
105.1
109.0
112.9
116.8
120.7
124.6
-
every
1st
38.0
41.2
44.4
47.6
50.8
54.0
57.2
60.4
63.6
66.8
70.1
73.3
76.5
79.7
82.9
86.1
89.3
92.5
95.7
98.9
102.1
105.3
108.5
(cid:96)min
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
in the following. The remaining parameter is the minimum
password length (cid:96)min, which one can increase to increase
the password distribution strength, as it is best practice for
password-based authentication in general [10]. Using the mean
from the ﬁtted geometric distribution, the average password
length is (cid:96)min + 1.4 for passwords that take every word and
(cid:96)min + 0.7 for passwords that take every second word, while
the mode is (cid:96)min in both cases. Note that this consideration
makes the simplifying assumption that the parameter of the
geometric distribution does not depend on (cid:96)min.
Table VII shows the minimum-length based entropy esti-
mates for a selection of the strongest generation rules. This
table aims at replacing for mnemonic passwords the “rules of
thumb” that exist for the entropy of generic passwords (e.g.,
[6]). Unlike these rules of thumb, which were shown to not
correlate with the password distribution strength against ofﬂine
attacks [39], we have shown that our entropy estimates do
correlate with it (cf. Figure 5). As the Table shows, when
considering that rules using only every second word lead to
shorter passwords on average, these rules lose much of their
advantage, and are even weaker for lowercase letter passwords.
Figure 6.
Shannon entropy and min-entropy estimates compared to the
optimal uniform password distribution by password length. Passwords are from
the Webis-Sentences-17 corpus using lowercase letters and the ﬁrst character.
C. Estimates by Password Length
Figure 6 shows that
This section analyzes how the strength of password dis-
tributions increases with password length. The number of
possible passwords increases exponentially with the password
length, theoretically leading to stronger password distributions.
Using the Webis-Sentences-17 corpus, we analyzed all rules to
very similar results. As an example, Figure 6 shows the result
for the standard generation rule using lowercase letters only.
the resistance against ofﬂine at-
tacks (H1) increases as expected with password length, but
that the resistance against online attacks (H∞) stays rather
constant.17 We also found λ10 and λ100 to be rather constant.
The approximately constant resistance against online at-
tacks shown in Figure 6 suggests that, for each length, there
are a few sentences with a high probability irrespective the
length. Only after these high-probability sentences, a spreading
of the probability mass over the possible sentences occurs.
This spreading is shown by the steady increase of the Shannon
entropy. Unfortunately, the Webis-Mnemonics-17 corpus is far
too small to reproduce this effect on human-chosen mnemon-
ics. It thus remains unclear to which extent this effect also
appears for human-chosen mnemonics. However, based on our
analysis it is reasonable to assume that the resistance against
online attacks of mnemonic passwords grows way less with
password length than one would expect.
The linear increase of the Shannon entropy with password
length leads to a simple model for estimating the entropy of
password distributions with several lengths. In detail, one can
rewrite Equation 4 (Shannon entropy) as
(cid:96)max(cid:88)
H1(X ) =
Pr[L = (cid:96)] · (H1(X(cid:96)) − log Pr[L = (cid:96)]) ,
(5)
(cid:96)=(cid:96)min
where H1(X(cid:96)) is the entropy estimate for length (cid:96). Moreover,
for the probability of a password-length, Pr[L = (cid:96)], one
can use the geometric model of lengths from the Mnemonic
Survey corpus (Figure 2).18 Due to the geometric model and
only a linear increase of the entropy by length, Equation 5
converges as (cid:96)max increases. We report the converged values
17H∞ varies between 11.3 and 14.2 bit without a clear direction.
18When only every second word is used, the length distribution can be
adjusted accordingly. However, an adjustment is not as straight-forward when
two characters per word are used, due to one-character words. As this variant
gave very weak distributions, we do not consider it here.
10
lllllllllllll8101214161820020406080100lllllllllllllPassword lengthBitllUniform distribution (H1=H¥)Estimated Shannon entropy, H1Estimated min−entropy, H¥Table VIII.
CHARACTER-WISE ENTROPY (H1) AND PERPLEXITY (PPL.)
ESTIMATES FOR PASSWORDS BY MODEL (CF. SECTION IV-B). PASSWORDS
ARE OF LENGTH 12 FROM THE WEBIS-SENTENCES-17 CORPUS USING THE
FIRST CHARACTER OF EVERY WORD. THE UNIFORM MODEL REPRESENTS
THE OPTIMAL DISTRIBUTION OVER 26/94 CHARACTERS.
Model
Uniform
Order 0
Order 8
Order 8, position-dependent
Lowercase
letters
H1
4.70
4.15
3.71
3.65
Ppl.
26.0
17.8
13.1