probabilities are only obtained for some percentage of communities,
Figure 10: Effective in early detection of spam messages
Figure 11: POISED vs. adversarial machine learning attacks
and assumes that the message has not been observed in the remain-
ing communities. We ran experiments for different percentages of
communities, 10%-100%, where communities are picked randomly.
For each percentage, we repeated the experiments three times.
Figure 10 indicates that POISED is effective in detecting spam
messages at the early stage when they have only propagated through
30% of the communities with 90% precision and 75% recall.
6.9 Adversarial Machine Learning Attacks
To evade the protection, adversaries actively manipulate data to
make the classifier produce false negatives [4, 21, 34, 46, 52, 58, 71].
These attacks will usually target two different stages of classifica-
tion: 1) at the training phase, where an adversary may attempt to
mislead the classifier by “poisoning” its training data with carefully
designed attacks [70], or 2) at the testing phase where an adversary
may attempt to evade a deployed system by carefully manipulating
attack samples [5]. These attacks are called poisoning and evasion
attacks respectively. We investigated the robustness of POISED
against both of these adversarial settings to understand to what
extent our classifier can resist the targeted attacks.
In both attacks, an adversary attempts to make the propagation
of her spam message be as similar as possible to that of a benign
message in the whole network.
For that, we assume that the adversary has the ability to create
sybil accounts, establish connections with honest users and pretend
to share the same interest as target communities by sending topical
messages. As the result of these malicious activities, we assume that
the adversary obtains the knowledge of: 1) the message counts in
each of the communities of interest, and 2) the number of users who
have posted those messages in each of those communities of interest.
Moreover, we assume that the number of fake or compromised
accounts is equivalent to the number of users (re-)posting that
specific benign message because this is one of the features used in
the classifier.
However, we assume that in non-compromised communities, the
adversary is unable to control any of these variables. For example,
let us take a network with 200 communities, where the attacker
has compromised 50 communities. She observes that some similar
benign messages are always posted in 30 of these communities.
Using this information, she posts her malicious content with a
similar distribution in those same 30 communities. However, since
she has no control over 150 of the 200 communities, her messages
most likely propagate differently than benign messages through
the network.
We simulated the attacks by randomly picking some percentage
of communities as compromised by the adversary.
The poisoning attack is performed during the training phase.
At this phase, the adversary deliberately interferes with both the
topic detection and community detection algorithms. She joins com-
munities in order to modify the network and the community struc-
tures. In these now compromised communities, she posts messages
resembling the propagation of benign messages in the compromised
communities, therefore modifying the probability distribution of
topics in these communities. The propagation probabilities for this
spam message is actually the combination of probabilities for the
chosen benign message in the compromised communities and those
of the spam message in the non-compromised communities. How-
ever, since the observations in the training set are (manually) la-
beled, the spam messages should still be correctly labeled as spam.
The classifier might not be able to accurately distinguish and detect
either spam or benign messages because the parties of interest for
benign and spam messages are partially identical.
The evasion attack is performed during the testing phase. The
training set is not polluted while the testing set includes the in-
stances of spam messages that attempt to resemble the probabilistic
propagation of benign messages. In this attack, we assume that
the attacker has partial knowledge about the model built on the
training set, and modifies the probability distribution of her mes-
sages in compromised communities, as explained previously at the
poisoning attack.
For both attacks, in each of the neighborhoods and for each per-
centage of compromised communities, we ran the simulations three
times and averaged all the results. In the case of the poisoning attack,
the observations of all the spam messages in the ground-truth are
modified based on the attack to represent simulated spam messages,
while the observations for benign messages remain unmodified.
To evaluate the attack, we performed 10-fold cross-validation on a
balanced dataset generated by SMOTE algorithm [15].
In the case of the evasion attack, the testing set includes all
simulated spam messages that are generated based on the attack. It
also includes a random set of x benign messages where x is equal
to the number of spam messages in the testing set. The training set
is the entire ground-truth dataset excluding those benign messages
appearing in the testing set.
 0 0.2 0.4 0.6 0.8 1 10 20 30 40 50 60 70 80 90 100PerformancePercentage of aﬀected communitiesF1-scorePrecisionRecall 0 0.2 0.4 0.6 0.8 1 0 20 40 60 80 100PerformancePercentage of compromised communitiesAccuracy, Poisoning attackF1-score, Poisoning attackAccuracy, Evasion attackF1-score, Evasion attackFigure 11 shows the impact of these attacks on the performance
of POISED when the percentage of compromised communities
increases from 0 to 100. As it can be seen in the results, by increasing
the percentage of compromised communities, the performance
gradually decreases in both attacks. In the poisoning attack, F-1
decreases from 90% to 65%, while in the evasion attack, it decreases
from 90% to 15% when the number of compromised communities
increases from 0 to 100%. POISED is more robust to the poisoning
attack because simulated spam messages and their propagation
probabilities in non-compromised communities are observed in the
training set and the classifier has learned from them.
These results suggest that the attacker needs to have a great
knowledge about the network to highly impact the performance
of the classifier. For example, even if 30% of the network is com-
promised, the precision and recall remain at 82% and 87% in the
case of a poisoning attack, and at 75% and 52% in the case of an
evasion attack. Moreover, we argue that the poisoning attack is a
more realistic attack because, in practice, the ground-truth dataset
gets updated over time and the training set most probably includes
spam messages generated by the adversary.
7 RELATED WORK
POISED is the first system able to detect spam on Twitter by looking
at the differences in which legitimate and malicious messages prop-
agate through the network. In the following, we discuss previous
work in the area of spam detection on social networks. Broadly
speaking we identify two types of approaches: those that look at
identifying malicious messages and those that look at flagging mali-
cious accounts. We then revise the academic literature that studied
message propagation on online social networks.
Malicious Messages Detection. Identifying spam messages
on social networks has been extensively studied [47, 78, 80, 90,
92, 94, 95]. Yardi et al. [94] studied Twitter spammers who abuse
trending topics. Analyzing URLs in messages is another method
employed to detect malicious messages [47, 80]. Monarch [80] is a
system for crawling URLs spread in social network and identifying
malicious messages and compromised accounts. Lee and Kim [47]
also proposed WarningBird, a system that analyzes correlated
redirection chains of URLs in a number of URLs posted on Twitter
to identify malicious tweets. Some researchers have proposed offline
spam analysis to identify large-scale social spam campaigns [32, 36].
They mostly apply clustering algorithms based on URL blacklisting
on a complete set of messages. Xu et al. [92] presented an early
warning worm detection system that monitors the behavior of users
to collect suspicious worm propagation evidences.
In summary, these works concentrate on limited aspects of the
spam detection problem, such as messages URL analysis, user be-
havior analysis, offline spam analysis, etc. In POISED, we provide a
generic solution that detects spam messages from their propaga-
tion patterns through communities of interest, regardless of their
content.
Malicious Accounts Detection. Today, normal users in popu-
lar social networks are increasingly becoming the target of attackers.
Many research works investigated this problem and proposed vari-
ous solutions for this challenge [3, 8, 9, 11, 12, 22, 28, 29, 32, 75, 91].
COMPA [28] is a system that detects compromised Twitter accounts
based on their behaviors over time. The authors showed that nor-
mal users have almost stable habits over time, unlike compromised
users who likely show anomalous habits. Liu et al. [51] calculated
user topics with LDA, and then employed supervised learning to
identify spammers based on topics of discussion. Cai et al. [11] pre-
sented a machine learning-based platform to detect Sybil attacks
in social networks. They split a social network into communities,
and tried to identify communities that connect in an unnatural or
inconsistent way with the rest of the social network. SybilInfer [22]
detects compromised accounts using a Bayesian Inference approach.
Stringhini et al. [75] investigated spammers’ behavior by creating
a set of honey-profiles on popular social networks. By studying
spammers’ characteristics, they introduced a spam detection tool.
Link Farming in Twitter where spammers acquire large number of
follower links has been investigated by Ghosh et al. [33]. By analyz-
ing over 40,000 spammer accounts, they discovered that a majority
of farmed links comes from a small number of legitimate and highly
active users. Wang et al. [87] analyzed user click patterns to create
user profiles and identify fake accounts using both supervised and
unsupervised learning. Viswanath et al. [85] applied Principal Com-
ponents Analysis (PCA) to find patterns among features extracted
from spam accounts. Cao et al. proposed SynchroTrap [13], a de-
tection system that clusters malicious accounts according to their
actions and the time at which they are made. EvilCohort [76] is
a system that identifies sets of social network accounts used by
botnets, by looking at communities of accounts that are accessed
by a common set of IP addresses.
These works aim to detect a particular type of malicious user
(e.g., compromised accounts or fake accounts). Instead, we propose
a comprehensive spam detection system independent from the type
of malicious user spreading it.
Message Propagation. POISED is the first system that pro-
poses to detect spam on Twitter by looking at how legitimate and
malicious messages spread on the social network. Previous work,
however, looked at message propagation on social networks for
other purposes.
Ye and Wu [95] studied propagation patterns of general messages
and breaking news in Twitter. They inspected a massive number
of messages collected from 700K users. Moreover, they evaluated
different social influences by analyzing their changes over time,
and how they correlate with each other. By analyzing Twitter hash-
tags, Weng et al. [90] showed that network communities can help
predicting viral memes. In summary, the popularity of a meme can
be predicted by quantifying its early spreading pattern in terms
of community concentration: the more communities a meme per-
meates, the more viral it is. Nematzadeh et al. [59] demonstrated
that strong communities with high modularity can facilitate global
diffusion by enhancing local, intra-community spreading. Through
a simulation, Mezzour et al. [55] showed how the diffusion of mes-
sages by hacked accounts differs from normal accounts. Similar
to these works, we analyze how messages propagate in social net-
works. We combine this to learn parties of interest and detect spam
messages.
8 DISCUSSION
Data Collection. Not having access to the whole Twitter data,
including the users’ data and Twitter network, imposes some con-
straints to our approach. For example, it may affect the quality of
the communities of interest as well as the groups of similar mes-
sages. Nonetheless, with these limitations, POISED performed well
in detecting diverse spam messages.
Complexity and Scalability. POISED is practical even at the
scale of Twitter. Here, we discuss the complexity of the multiple
phases of our approach. The time complexity for Infomap is es-
timated at O(m), where m is the number of nodes. It can classify
millions of nodes in minutes [2, 44].
The topic detection algorithm, LDA, has a complexity of O(N KV),
where N , K, and V are the number of documents, topics and words
in the vocabulary, respectively. The efficiency of LDA can be im-
proved with the use of heuristics, e.g., by running it on each neigh-
borhood independently, decreasing the number of documents by
having more tweets per document, and specifying a lower number
of topics. Recent work has also explored multiple approaches for
increasing the performance of LDA [35, 57, 65, 73, 88], which can
be applied to POISED.
Identifying groups of similar messages is a string searching prob-
lem. POISED classifies messages based on four-gram matches. The
length of tweets is short, and each of them only contains a few con-
secutive four-grams. The time complexity and memory complexity
for four-gram analysis are O(N 2M2), where N is the number of
messages and M is the maximum size of a message (i.e., 140 charac-
ters in the case of Twitter). However, the analysis can be optimized
from O(N 2M2) to close to linear in the number of similar groups
returned [17, 24, 40].
In our experiments, running the four-gram analysis using a com-
modity desktop took approximately an hour. Running LDA on 300
neighborhoods, with 15M tweets, took nine hours on a commodity
desktop. Each neighborhood analysis can be run independently, and
thus, can be parallelized in order to scale. For 500M tweets a day,
which consists in the daily average on Twitter, we estimate that it
would require approximately 150 machines to run our distributed
analysis in two hours.
Finally, POISED successfully detects spam messages in neigh-
borhoods. Therefore, to increase the efficiency, POISED can be run
on some partitions of networks. Furthermore, the most demanding
steps are topic detection and message matching, which prepare the
training data for the probabilistic model. These can be run offline
and less frequently. The SVM classifier itself is highly efficient.
Live implementation. Although POISED involves many oper-
ations, testing new messages on a live system would require fewer
steps. The actual process involved in identifying messages as spam
would be to 1) establish groups of similar messages, 2) observe their
propagation through parties of interest, and 3) test them on the
probabilistic model that is obtained in advance. Twitter can collect
user reports over time and use them as the ground-truth to build the
probabilistic model. The communities of interest must be computed
regularly, as the network likely evolves through time.
Ground-truth dataset. For online social networks such as Twit-
ter and Facebook, obtaining a ground-truth dataset requires mini-
mal effort; they already have some mechanisms in place for their
users to report malicious behavior. Over time, following the network
evolution, they can update or add to their ground-truth dataset.
Note that our results show that POISED is able to detect unseen
malicious content.
Limitations. Similar to other spam detection systems [27, 74],
POISED groups similar messages before applying the probabilistic
model to classify spam. An attacker aware of this feature could
evade the four-gram analysis used to identify messages similarity.
However, in that case, the similarity analysis could be extended
with more complex similarity measures [27].
Our approach requires a minimum number of users and mes-
sages to form communities of interest. A malicious user with a
small number of connections might be able to evade our detection.
However, this goes directly against the interest of malicious users,
who want to reach as many victims as possible.
9 CONCLUSIONS AND FUTURE WORK
In this paper, we presented POISED, a novel and general approach
for detecting social network spam based on its diffusion through
parties of interest. First, we established that members of networked
communities share common topics of interest distinct from other
surrounding communities. We then built a probabilistic model that
detects spam based on the dissemination of messages through com-
munities of interest with high efficiency. We also showed that
POISED outperforms other spam detection systems proposed in
recent work, while its threat model is also more general. Moreover,
we showed how POISED is effective in the early detection of spam
messages and how it is resilient against two well-known adversarial
machine learning attacks.
In this paper, we assumed that users are only member of one
particular community. A possible future work is to explore detect-
ing overlapping communities of interest. We can also explore the
combination of our framework to existing systems that analyze the
characteristics of user accounts or messages to detect spam. Finally,
another future work is to employ and test POISED on other online
social networks.
ACKNOWLEDGEMENTS
This research was supported by the EPSRC under Grant N008448
and by the European Commission as part of the ENCASE project
(H2020-MSCA RISE of the European Union under GA number