title:Rethinking buffer management in data center networks
author:Aisha Mushtaq and
Asad Khalid Ismail and
Abdul Wasay and
Bilal Mahmood and
Ihsan Ayyub Qazi and
Zartash Afzal Uzmi
Rethinking Buffer Management in Data Center Networks
Aisha Mushtaq1, Asad Khalid Ismail2, Abdul Wasay1,
Bilal Mahmood1, Ihsan Ayyub Qazi1, Zartash Afzal Uzmi1
1SBASSE, LUMS, Lahore, Pakistan, 2UC, Santa Barbara, USA
PI:EMAIL, PI:EMAIL, PI:EMAIL,
PI:EMAIL, PI:EMAIL, PI:EMAIL
ABSTRACT
Data center operators face extreme challenges in simultaneously
providing low latency for short ﬂows, high throughput for long
ﬂows, and high burst tolerance. We propose a buffer management
strategy that addresses these challenges by isolating short and long
ﬂows into separate buffers, sizing these buffers based on ﬂow re-
quirements, and scheduling packets to meet different ﬂow-level ob-
jectives. Our design provides new opportunities for performance
improvements that complement transport layer optimizations.
1.
INTRODUCTION
Many popular applications such as web search, advertising, and
recommendation systems require low latency and high throughput
from the underlying data center network [1]. Unfortunately, the
completion times for latency-sensitive short ﬂows in existing TCP-
based deployments can be orders of magnitude larger than opti-
mal [2]. This happens due to the mixing of short and long ﬂows that
have disparate performance expectations from the network fabric.
As a result, short and long ﬂows compete for a shared set of net-
work resources (e.g., link capacity and switch buffers) in an adver-
sarial manner. In particular, the buffer-ﬁlling nature of long-lived
TCP ﬂows often causes short ﬂows to get queued up behind pack-
ets from long ﬂows. In addition, the presence of long ﬂows reduces
the ability of switch buffers to absorb application-induced packet
bursts that are common in data centers.
We propose MulBuff, a buffer management strategy that addresses
the root cause of these problems that stem from the mixing of short
and long ﬂows. MulBuff uses three key design principles: (i) segre-
gation of long ﬂows and short ﬂows into separate buffers, (ii) sizing
of buffers based on ﬂow requirements, and (iii) scheduling packets
from these buffers to meet different ﬂow-level objectives. We show
that these principles complement each other and all are needed to
achieve high performance across a wide range of scenarios.
Designs like MulBuff provide new opportunities for performance
improvements that complement transport layer optimizations [1, 4,
5]. In particular, using MulBuff in conjunction with existing trans-
ports like DCTCP can provide signiﬁcant performance beneﬁts due
to isolation of ﬂows and by explicitly accounting for scenarios like
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage, and that copies bear this notice and the full ci-
tation on the ﬁrst page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the owner/author(s). Copyright is held by the
author/owner(s).
SIGCOMM’14, August 17–22, 2014, Chicago, IL, USA.
ACM 978-1-4503-2836-4/14/08.
http://dx.doi.org/10.1145/2619239.2631462.
incast. Our initial results show that TCP with MulBuff improves
performance over DCTCP and achieves similar performance com-
pared to pFabric [2].
2. FRAMEWORK DESIGN
MulBuff’s key design insight is a principled handling of ﬂows
with heterogeneous requirements at a switch. End hosts using Mul-
Buff mark each packet as belonging to the short ﬂow queue or the
long ﬂow queue1. The MulBuff switch maps ﬂows to queues, up-
dates sizes of the short ﬂow queue (to keep a bounded loss rate
and low delay) and the long ﬂow queue (to keep high utilization),
and adjusts queue capacities based on the number of ﬂows in each
queue2. We now study the impact of buffer organization, sizing,
and scheduling on MulBuff’s performance and show that they must
be considered in unison to maximize performance because a lack
of principled consideration of one factor can override the beneﬁts
of the other.
Buffer Organization: MulBuff’s segregation of ﬂows with differ-
ent requirements into seperate queues allows ﬂows to achieve their
desired performance without impacting other ﬂows. Figure 2(a)
shows the average ﬂow completion time (AFCT) with TCP for two
buffering policies as a function of network load. Observe that hav-
ing a single common buffer for both short and long ﬂows signiﬁ-
cantly increases the AFCT. This happens because of the increased
queuing delays caused by long ﬂows’ packets ﬁlling up the buffer.
On the other hand, using multiple buffers isolates the impact of
long ﬂows on short ﬂows leading to lower AFCTs.
Scheduling: Allocating ﬁxed capacities to queues can degrade
performance when more ﬂows get mapped to one queue compared
to the other or when ﬂows have different performance requirements
(e.g., to minimize completion times, short ﬂows should be priori-
tized). Thus to meet different ﬂow-level objectives (e.g., minimize
FCTs, fair sharing), each queue should be allocated capacity pro-
portional to the number of ongoing ﬂows and operator assigned
weights. Figure 1(b) shows the AFCT for processor sharing (PS)
across queues and weighted processor sharing (WPS) as a function
of load. Observe that FCTs increase rapidly with load under PS.
This happens because PS assigns equal capacity to both the queues
irrespective of the number of ﬂows in each queue, which causes the
FCT of short ﬂows to increase. However, with WPS, as rates are
assigned based to the number of ﬂows in each queue, it results in a
graceful increase in FCT (see Figure 2(b)).
Buffer Sizing: The size of a buffer plays a key role in meeting the
requirements of ﬂows. For instance, latency-sensitive short ﬂows
often suffer TCP retransmission timeouts due to synchronized ﬂow
1This is based on ﬂow size information. If ﬂow size is not known,
it can be estimated using the amount of data sent so far [4].
2Note that MulBuff can use any number of queues.
575(a) Scenario A
(b) Scenario B
(c) Scenario C
Figure 1: (a) Scenario A shows comparison of a single buffer with two buffers, (b) Scenario B shows comparison of static and dynamic link capacities
(scheduling), and (c) Scenario C shows comparison of static and dynamic buffer sizing. Note that B is the buffer size, C=1 Gbps is the link capacity,
CS and CL are the capacities for the short and long buffers, respectively, and NS is the number of ﬂows in the short buffer.
Single buffer 
Separate buffers 
)
s
m
(
T
C
F
A
90 
80 
70 
60 
50 
40 
30 
20 
10 
0 
10  20  30  40  50  60  70  80  90  95 
Load (%) 
)
s
m
(
T
C
F
A
10000
1000
100
10
1
processor sharing
weighted processor sharing
)
s
m
(
T
C
F
A
11 
10 
9 
8 
7 
6 
5 
4 
static buffers 
flow propotional 
buffers 
10 20 30 40 50 60 70 80 90 95
Load (%)
8 
16 
32 
64 
128 
Incast senders 
(a) Scenario A
(b) Scenario B
(c) Scenario C
Figure 2: For each scenario in Fig. 1, comparison of AFCT for short ﬂows as a function of load. For scenarios (a) and (b), ﬂow sizes were drawn
from the interval [2 KB,98 KB] using a uniform distribution while keeping two long-lived ﬂows in the background as done in [4]. In case of (c), a
1 MB ﬁle was divided equally among the senders.
arrivals. To prevent these timeouts and achieve a bounded loss rate,
requires apportioning a certain amount of buffer space for each ﬂow
[3]. Thus MulBuff sizes buffers proportional to the number of ﬂows
for the short ﬂow queue. For long ﬂows to fully utilize the allocated
capacity, the buffers must be large enough to prevent any queue
underﬂow. Consequently, MulBuff sets the buffer size for the long
ﬂow queue to the bandwidth-delay product (BDP). Scenario C in
Figure 1(c) compares the use of static and dynamic buffer sizing
strategies. The static buffers setup uses BDP buffers for the long
ﬂows and (250-BDP) KB of buffer for the short ﬂows. The other
setup uses dynamic buffering for the short ﬂow queue (for every
new ﬂow, the buffer size is increased by three packets and when
a ﬂow departs, it is decreased by three packets and the maximum
size is capped at (250-BDP) KB. For these two setups, Figure 2(c)
shows the AFCT results as a function of the number of senders in
an ensuing incast. Observe that using dynamic buffering leads to
improved FCTs especially when the number of senders are small
as dynamic buffering helps in keeping small queues.
Overall, these results motivate the need to consider all these fac-
tors (buffer organization, scheduling, sizing) in unison.
Evaluation: Figure 3 shows the AFCT and the 99th percentile
FCT for TCP with MulBuff, DCTCP, and pFabric on a 3-level tree
topology in the network simulator (ns-2). Observe that MulBuff
considerably improves performance over DCTCP and achieves sim-
ilar performance compared to pFabric across a range of loads. Un-
der incast, MulBuff’s segregation ensures more headroom for short
bursty ﬂows, sizing allows dynamic allocation of buffers as ﬂows
arrive, and scheduling allows them to ﬁnish quickly (see Fig. 4(a)).
Moreover, using MulBuff in conjunction with DCTCP provides up
to 34% improvement in AFCT over DCTCP as shown in Fig. 4(b).
3. CONCLUSION AND FUTURE WORK
We make the following contributions in this paper: (a) We pre-
sented the design considerations needed for a buffer management
strategy to achieve high performance across a range of data center
workloads, (b) we presented a simple but novel solution that tack-
les buffer organization, sizing and scheduling, (c) we conducted
evaluation of MulBuff in a typical data center environment. We
DCTCP
MulBuff
pFabric
)
s
m
(
T
C
F
A
7
6
5
4
3
2
1
0
10
20
30
40
50
60
70
80
90
Load (%) 
DCTCP
MulBuff
pFabric
)
s
m
(
T
C
F
h
t
9
.
9
9
128
64
32
16
8
4
10
20
30
40
50
60
70
80
90
Load (%) 
Figure 3: Comparison of TCP-MulBuff with DCTCP and pFabric in
terms of (a) AFCT, (b) 99.9th perc. FCT under the all-to-all scenario.
)
s
m
(
T
C
F
A
60
50
40
30
20
10
0
4
8
16
32
Number of servers
PFabric
MulBuff
DCTCP
TCP
64
128
)
%
(
T
C
F
A
n
i
t
n
e
m
e
v
o
r
p
m
I
40
35
30
25
20
15
10
5
0
4
8
16
32
64
128
Number of servers
Figure 4: (a) Performance under the (intra-rack) incast sce-
nario and (b) Impact of using DCTCP with MulBuff.
are currently implementing MulBuff in the Click modular router.
In the future, we plan to evaluate the effects of choosing arbitrary
scheduling policies, and how MulBuff behaves in the presence of
legacy switches. In addition, we plan to explore how MulBuff can
help address problems like TCP Outcast.
4. REFERENCES
[1] ALIZADEH, M., GREENBERG, A., MALTZ, D., PADHYE, J., PATEL,
P., PRABHAKAR, B., SENGUPTA, S., AND SRIDHARAN, M. Data
center tcp (dctcp). In SIGCOMM’10.
[2] ALIZADEH, M., YANG, S., SHARIF, M., KATTI, S., MCKEOWN,
N., PRABHAKAR, B., AND SHENKER, S. pfabric: Minimal
near-optimal datacenter transport. In SIGCOMM’13.
[3] MORRIS, R. Scalable tcp congestion control. In INFOCOM’00.
[4] MUNIR, A., QAZI, I. A., UZMI, Z. A., MUSHTAQ, A., ISMAIL,
S. N., IQBAL, M. S., AND KHAN, B. Minimizing Flow Completion
Times in Data Centers. In INFOCOM’13.
[5] VAMANAN, B., HASAN, J., AND VIJAYKUMAR, T. N.
Deadline-aware datacenter tcp (d2tcp). In SIGCOMM’12.
576