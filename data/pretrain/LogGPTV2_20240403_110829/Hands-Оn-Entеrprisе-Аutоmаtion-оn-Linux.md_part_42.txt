Chapter 6 - Custom Builds with PXE Booting
1. Pre-eXecution Environment.
2. A DHCP server and a TFTP server—commonly, another service is required for
serving larger volumes of data; this might be a web, FTP, or NFS server.
3. Check the download site for the distribution you are using or the ISO
contents—there is normally a specific folder containing the kernel and RAMDisk
images for network booting.
4. An installation where no user interaction is required at all and the end result is a
fully installed and configured machine.
5. A kickstart file is specific to Red Hat-derivative operating systems such as
CentOS and RHEL, whereas a pre-seed file is used on Debian derivatives such as
Ubuntu.
6. To execute custom scripts or actions that cannot be performed earlier in the
unattended installation.
7. Legacy BIOS PXE booting and UEFI network booting require different binary
files for the boot process—hence, these must be separated and served
appropriately according to the machine type.
8. There are multiple ways—the easiest if using automated partitioning is to
provide a statement such as this:
d-i partman-auto/choose_recipe select home
Chapter 7 - Configuration Management with
Ansible
1. Commonly, these might be replace and lineinfile.
2. In brief, a template file is created that contains a mix of plaintext (which will be
replicated as is) and valid Jinja2 expressions, which will be parsed and turned
into the appropriate text when the template is deployed. These might be simple
variable substitutions or more complex constructs such as for loops or
if..then..else statements.
3. Many Linux configurations are now split across multiple files, and it is possible
for someone to accidentally (or maliciously) override your configuration in
another file that gets included later on.
[ 463 ]
Assessments
4. Regular expressions can easily be broken if not carefully designed—for example,
a Linux service might accept a configuration directive if there is whitespace
before it; however, if your regular expression does not take account of this, it
might overlook valid configuration directives, which need changing.
5. It is simply deployed as is—almost akin to the copy module in Ansible.
6. Make use of the validate parameter with the template module.
7. Run Ansible in check mode—if the playbook and roles are well written, any
reported changed results means that the configuration has deviated from the
desired state and might need to be addressed.
Chapter 8 - Enterprise Repository
Management with Pulp
1. Pulp repositories can be version controlled (through snapshots taken in time).
They are also disk space-efficient and do not duplicate packages across mirrors.
2. Linux repositories change on a very regular basis, and a machine patched on
Monday may not look like a machine patched on Tuesday. This can, in worst-
case scenarios, impact testing results.
3. Pulp 2.x requires a message broker and a MongoDB database to run.
4. /var/lib/mongodb should be 10 GB or more in size. /var/lib/pulp should be
sized according to the repositories you want to mirror. They should be created on
the XFS filesystem.
5. At the simplest possible level, you could create a repository file in
/etc/yum.repos.d and point it at the appropriate path on the Pulp server (as
documented in Chapter 8, Enterprise Repository Management with Pulp). It is also
possible to configure the Pulp Consumer for this task.
6. The Pulp Consumer only works on RPM-based systems, and so if you use this in
a mixed CentOS and Ubuntu environment (for example), you will have a
differing approach between your Ubuntu and CentOS hosts. Using Ansible for
patching works for both system types and ensures consistency in your approach,
which makes life simpler for those who manage the environment.
7. No, it does not. You would run pulp-admin orphan remove --all.
[ 464 ]
Assessments
Chapter 9 - Patching with Katello
1. Katello offers a rich web-based user interface, filtering for repository creation, the
concept of life cycle environments (for example, development and production),
and a whole other set of features.
2. A Product is a collection of supported files in Katello—it might be a mirror of an
RPM repository, some manually uploaded files, a collection of Puppet manifests,
or a DEB repository mirror.
3. A content view is a version-controlled snapshot of a set of Products as defined in
the answer to question 2. In the context used in this book, it is a version-controlled
set of repositories.
4. Yes, it can.
5. You would create one Lifecycle Environment for each distinct environment
in your enterprise—for example, Development, Testing, Staging, and
Production. Hence, you can have a different version of a Content View
associated with each environment, allowing Development to test the most
bleeding-edge packages, whilst Production receives the most stable, tested
ones.
6. A Publish operation created a new version of a content view—this is not
associated with any of your Lifecycle Environments at this stage. A Promote
operation associates the published version with a Lifecycle Environment.
7. When you are ready to test/deploy that version of the repository content in the
environment you are promoting to (for example, new version of packages to
Development).
Chapter 10 - Managing Users on Linux
1. They provide an emergency route into the server in case of failure of the
directory service.
2. The user module.
3. Run an ad hoc Ansible command and use the password_hash filter to generate
the hash, as in this example:
$ ansible localhost -i localhost, -m debug -a "msg={{ 'secure123' |
password_hash('sha512') }}"
[ 465 ]
Assessments
4. The realmd package.
5. Create a template to match the file on the group of servers, and then write a
role/playbook with a task to deploy the template. Run the playbook in check
mode and if changed status results occur, then the templated file differs from the
configuration on the servers.
6. If you get a directive wrong in sudoers, the worst-case scenario is you will lock
yourself out of becoming root on your server (hence preventing you from fixing
the problem). Validating the file helps to prevent this.
7. A directory service can audit logins, manage password complexity, lock accounts
centrally either on demand or as a result of too many failed login attempts.
8. This depends on your business requirements and existing architecture. A
business with a Microsoft infrastructure will almost certainly already have
Microsoft Active Directory, whilst a business running purely on Linux will not
need to introduce Windows Server and so should consider FreeIPA.
Chapter 11 - Database Management
1. Ansible provides a self-documenting way of deploying both the software and
database content—coupled with a tool such as AWX, it ensures you have an
audit trail of who made what changes and when.
2. Create the configuration file as a template and deploy it using the template
across all servers. Where configurations are split across multiple files, either
ensure that all files are managed by Ansible or remove the include statement
from the files to ensure parameters cannot be accidentally overridden.
3. Ansible performs all its operations on the database machine using SSH—hence,
there is no need to open your database server to the network to manage it.
4. You would use the shell module when the native module you need cannot
perform the operation you require. For example, older versions of Ansible could
do most things on PostgreSQL, but couldn't perform a full vacuum. This has now
been rectified but serves as an example - the shell module is your solution
when you either do not have a native Ansible module that addresses your
requirements, or where one exists but the task you are performing is outside its
capabilities.
[ 466 ]
Assessments
5. Ansible, especially when coupled with AWX, provides an audit trail and ensures
that you can track what operations were performed and when. You can also
schedule routine operations in AWX.
6. You would create a role or playbook and use the shell module to call one of the
native PostgreSQL backup tools such as pg_basebackup or pg_dump.
7. mysql_user.
8. PostgreSQL has more native modules supporting it in Ansible than any other
database platform.
Chapter 12 - Performing Routine
Maintenance with Ansible
1. The df command can be provided with a path and it will work out the mount
point on which that path lives and give you the free disk space. Ansible Facts
provide disk usage statistics, but only by mount point, and so you must figure
out which mount point your path lives on.
2. The find module is used to locate files.
3. Changes to configuration files might get made accidentally, maliciously, or as a
result of an emergency change to fix an issue. In all cases, it is important to
identify the changes and ensure that they are either removed or the playbooks
updated to reflect the new configuration (especially when they were made to
resolve an issue).
4. You could use the template module or copy module to copy over the file and
run Ansible in check mode. You could also checksum the file and see whether
that matches a known value.
5. Use the service module in a task with the appropriate parameters.
6. Jinja2 provides the filtering as well as templating in Ansible.
7. Use the split operator on the variable—for example, {{ item.split(,) }}.
8. If you change all of the server content in one go, you might accidentally take the
whole service offline—it is better to take a small number of servers out of service
at a time, make and validate the changes, and then reintroduce them.
9. Set max_fail_percentage to an appropriate value for your environment to
stop the play if more than a given percentage of failures occur.
[ 467 ]
Assessments
Chapter 13 - Using CIS Benchmarks
1. They provide a standardized, industry-agreed way to secure Linux servers.
2. Yes, it does.
3. A level 1 benchmark is not expected to have an impact on day-to-day operations
of your server. A level 2 benchmark is and so should be implemented with care.
4. Scored benchmarks are expected to be crucial to all systems, whereas
benchmarks that are not scored are expected to be applied to only some systems
(for example, wireless network adapter configuration hardening will only apply
to a subset of machines—hence, this should not affect the score of all machines).
5. This is normally provided in the benchmark document but often involves using
the grep utility within the script to check for the configuration settings in a given
file and reporting back on whether it was found or not.
6. Possible answers include the following:
Pattern matching can be an imprecise science, and you must be careful
of false positives and indeed false negatives!
Shell scripts are not normally state-aware and care must be taken not
to write the same configuration out each time the script is run, even if
it is the same as before.
Shell scripts are difficult to read, especially when they become large,
and so can be difficult to manage and maintain.
7. Shell scripts are not very readable, and as the number of security requirements to
implement increases, so does the size of the script, in the end becoming
something that no-one would be able to manage.
8. Pipe the shell script into an SSH session opened with the remote server.
9. This enables the path to be altered easily in case the script needs to be
repurposed—for example, some key system binaries live in different paths on
Ubuntu and CentOS systems.
10. In general, it is best to run scripts at the lowest privilege level possible, only
elevating for specific tasks that require this. Also, sudo is sometimes configured
to require a Terminal session, and this can prevent running an entire script under
sudo when you pipe it into an SSH session.
[ 468 ]
Assessments
Chapter 14 - CIS Hardening with Ansible
1. The modules wrap up a whole set of shell scripting functionality, including the
conditionals that would be required to ensure that the script only makes changes
when required and can report back on whether the change was made and
whether it was successful.
2. There are several ways—you can run the entire playbook with the --limit
parameter set, or you can use the when clause within the playbook to ensure that
the tasks only run on given hostnames.
3. Name your tasks after the benchmark (including the number) so you can easily
identify what they are for. Also, include the level and scoring detail to make it
easy to interpret and audit results from playbook runs.
4. Tag the tasks as level1 and level2 accordingly, and then run the playbook
with the --tags level1 parameter.
5. The --tags parameter only runs tasks with the tags specified, whereas the --
skip-tags parameters runs all tasks except those specified.
6. The CIS Benchmarks are very large in size, and there is no point in reinventing
the wheel, especially with open source code as you can audit the playbooks
before you use them to ensure they are secure and meet your requirements.
7. It tells Ansible to run in check mode, which means that no changes are
performed, but Ansible will try to predict which changes would have been made
if it had been run in its normal mode.
8. No—the shell module can't support check mode because it is impossible to
know what command someone may have passed to it in a playbook.
Chapter 15 - Auditing Security Policy with
OpenSCAP
1. Security Content Automation Protocol.
2. SCAP policies can audit your systems against a given standard - for example the
CIS Benchmarks discussed in this book, or the PCI-DSS (Payment Card Industry
- Data Security Standard) requirements. There are many pre-written policies
available, and with open source tools such as OpenSCAP, you can write your
own policies with your own requirements. This is valuable to the enterprise in
being able to run audits against Linux servers and ensure they remain compliant
with a chosen standard.
3. You would most likely the OpenSCAP Daemon for this purpose.
[ 469 ]
Assessments
4. At a fundamental level, the OVAL file contains the low level system checks the
scanning engine should perform. The XCCDF file references the OVAL file (in
fact it cannot be used without it) contains amongst other definitions, profiles
which make use of scan definitions to audit against known policies (for example,
PCI-DSS), and code to generate human readable reports from the scan output.
5. In some environments, the vendor might only provide you with support if you
use their policy files. An example of this is Red Hat Enterprise Linux 7, where
Red Hat state that they will only support you if you use the SSG policies
available from their own repos.
6. SCAP policies are highly specific to the operating system they are running on.
Although in many scenarios, CentOS 7 and RHEL 7 can be treated as the same,
there are fundamental differences. SCAP takes account of this and ensures that it
differentiates between operating systems, even CentOS 7 and RHEL 7, and as
such it will mark many if not all of the RHEL 7 audits as notapplicable when
they are run against CentOS 7. The same would be true if a CentOS 7 specific
policy was run against a RHEL 7 host.
7. Yes you can - a command such as the following would generate an HTML report
from an XML results file: sudo oscap xccdf generate report --output
/var/www/html/reportoscapd.html
/var/lib/oscapd/results/1/1/results.xml
8. You must have set up passwordless (key based) SSH access to the server you
wish to scan. It must also have passwordless sudo access unless you are using
the root account over SSH (not recommended).
Chapter 16 - Tips and Tricks
1. Ansible Galaxy is a publicly available repository of Ansible roles for you to reuse
or develop as you wish. It is also a place where you can share the roles you have
created.
2. Playbooks and roles are bound to change over time, but there will always be
times where it is a requirement to understand what happened historically. Roles
especially are designed to be reused, and so it is important they are centrally
version controlled so that all playbooks that make use of them are sure they are
using the correct version role.
3. Possible answers include the following:
Use a requirements.yml file to specify the role URLs in a repository
and install them with ansible-galaxy.
Add them to your roles/ directory as Git submodules.
[ 470 ]
Assessments
4. Especially in cloud computing, the servers you have deployed will change
constantly. Ansible only knows what to automate from its inventory file, so it is
vital that the inventory file is up to date or servers may get missed. Making use
of dynamic inventories ensures the inventory is always up to date as the latest
inventory is always dynamically generated.
5. You can write them in any language provided the output is in the correct JSON
format for Ansible. Most are written in Python.
6. Look in the comments at the beginning of the dynamic inventory script itself or
the accompanying configuration file.
7. It is a command that can run a single Ansible module once without the need to
write an entire playbook.
8. Possible answers include the following:
They can help you to test and develop Jinja2 filter expressions without
having to run an entire playbook.
They can help you to test out module functionality before you commit
it to playbook or role code.
9. Run an Ansible ad hoc command using the shell module (-m shell) and pass
the shell command in the arguments of the module (-a "ls -la /tmp").
[ 471 ]
Other Books You May Enjoy
If you enjoyed this book, you may be interested in these other books by Packt:
Hands-On Linux Administration on Azure
Frederik Vos
ISBN: 978-1-78913-096-6
Understand why Azure is the ideal solution for your open source workloads
Master essential Linux skills and learn to find your way around the Linux
environment
Deploy Linux in an Azure environment
Use configuration management to manage Linux in Azure
Manage containers in an Azure environment
Enhance Linux security and use Azure's identity management systems
Automate deployment with Azure Resource Manager (ARM) and Powershell
Employ Ansible to manage Linux instances in an Azure cloud environment