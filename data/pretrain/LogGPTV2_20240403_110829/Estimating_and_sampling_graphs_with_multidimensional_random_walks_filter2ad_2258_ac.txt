observe this FS process over the interval [0, B].
In the DFS process, the probability that the k-th random
walker transitions out of vertex vk at step τ + ∆ depends
only on deg(vk) and not on the state of L(τ ). Thus, we can
decompose the Poisson process describing a departure from
the state L′
n = (v1, . . . , vm) into m independent stochastic
processes, where the i-th process is a Poisson process with
parameter λi = deg(vi) , i = 1, . . . , m. The above is equiva-
lent to the stochastic process of a MultipleRW process with
m random walkers and budget B, where the cost of sampling
a vertex v is an exponentially distributed random variable
with rate deg(v).
The DFS is equivalent to a FS process via the Uniformiza-
tion property of Markov chains [8, Chapter 7.5]. The tran-
sition probability matrix of the Uniformized Markov chain
(with unitary uniformization parameter) at the embedded
transition points is
P = I − D−1Q = D−1A ,
which is also the transition probability matrix of a FS pro-
cess.
6. RESULTS
In this section we compare FS with SingleRW and Multi-
pleRW. We also contrast FS with random vertex and edge
sampling. The experiments consist of executing these sam-
pling methods on a variety of real world graphs. The datasets
used in the simulations are summarized in Table 1: “Flickr”,
“Livejournal”, and “YouTube” are popular photosharing,
blog (weblog), and video sharing websites, respectively. Users
are represented as vertices of a graph. In these websites a
user can subscribe to other user updates; an edge (u, v) ex-
ists between users u and v if user u subscribes to user v. At
“Livejournal” and “YouTube” it is possible to query the in-
coming and outgoing edges of a given user. Further details of
these three datasets can be found in [26]. “Internet RLT” is
a router-level Internet graph collected from traceroute mea-
surements of 23 monitors distributed over the world [13].
Note that some of these graphs contain disconnected com-
ponents (subgraphs).
In the following simulations the starting vertex of each
random walker is chosen uniformly at random from the set
of all vertices. Our results show that FS estimates are consis-
tently more accurate than their SingleRW and MultipleRW
counterparts. Moreover, when restricted to the largest con-
nected component, FS reaches steady state faster than Sin-
gleRW and MultipleRW in the simulations presented in our
technical report [31].
6.1 Assortative Mixing Coefﬁcient
In our ﬁrst experiment we treat the graphs in Table 1
as undirected graphs. In-degrees and out-degrees are repre-
sented as vertex labels and the assortative mixing coeﬃcient
is obtained using the estimator described in Section 4.2.2.
In our experiment we average the estimates and calculate
their mean squared error (MSE) over 100 runs. The sam-
pling budget is |V |/100 for all graphs. Let ˆr denote the
estimated value of r. Table 2 shows a summary of the rel-
ative bias of ˆr (1 − E[ˆr]/r) and ˆr’s NMSE with respect to
the true value of r. We observe that FS is consistently more
accurate than both MultipleRW and SingleRW. If we focus
on Flickr, the FS bias is 7 fold smaller than the bias of both
MultipleRW and SingleRW. In addition FS’s NMSE is one
order of magnitude smaller than the NMSEs of MultipleRW
and SingleRW. The Internet graph (“Internet RLT”) is the
only graph we studied that shows little diﬀerence between
FS and MultipleRW.
We also perform an extreme experiment that focuses on
the impact of loosely connected components on the assorta-
tive mixing estimates. Consider a graph that consists of two
instances of a random undirected Barabási-Albert [5] graph,
GA and GB, with 5 × 105 vertices each and average degrees
2 and 10, respectively, joined by a single edge connecting
the two smallest degree vertices in GA and GB (ties are re-
solved arbitrarily). Henceforth, we use GAB to denote the
above graph. It is worth noting that over the GAB graph,
SingleRW consistently ﬁnds ˆr = 0 over all 100 runs. This
is because SingleRW only estimated the assortative mixing
of either subgraph A or subgraph B, which are both zero.
Over GAB MultipleRW performs almost as bad as SingleRW
while FS is able to accurately estimate r.
lations we estimate γi =P∞
6.2 In- and Out-degree Distribution Estimates
We now focus on estimating the in-degree distribution.
Let θ = {θi}∀i∈L denote the in-degree distribution, where
θi is the fraction of vertices with in-degree i. In our simu-
k=i+1 θk, the CCDF of θ, using
equation (7). We choose to estimate the CCDF instead of
the density because the CCDF is the plot of choice when it
comes to displaying degree distributions. Each simulation
consists of 10, 000 runs (sample paths) used to compute the
empirical CNMSE (equation (2)). The CNMSE is used to
compare the accuracy of the estimates obtained from FS
(dimension m ∈ {10, 1000}), SingleRW, and MultipleRW
(m ∈ {10, 1000} walkers). For the sake of conciseness, we
restrict our presentation to a handful of representative re-
sults.
Consider ﬁrst two representative results from the Flickr
graph, whose in-degree CCDF (complementary cumulative
distribution function) log-log plot is shown in Figure 3. The
sampling budget is B = 17, 152 = |V |/100, which amounts
to sampling 1% of the vertices. In the ﬁrst simulation, we
are restricted to the Largest Connected Component (LCC)
(which contains 94% of the vertices). The objective is to
test if FS can outperform SingleRW and MultipleRW even
398Graph
r
FS
0.007
Flickr
LiveJournal
0.07
Internet RLT 0.17
−0.03
Youtube
0.08
GAB
Bias
8%
−0.5%
3%
0.001%
0.01%
Bias
|NMSE|
1.08
752%
0.11 −12%
2%
0.33
2%
0.02
0.12
70%
MultipleRW
|NMSE|
Bias
7.65 −619%
1%
0.16
17%
0.32
−1%
0.03
100%
0.72
SingleRW
|NMSE|
27.32
0.17
0.44
0.1
1.00
Table 2: Assortative mixing coeﬃcient estimate bias and the module of the estimator NMSE ; r is the true value of the global clustering
coeﬃcient and these values are estimated value over 100 runs. The sampling budget is |V |/100 for all graphs.
F
D
C
C
E
S
M
N
C
1
3 3
3
3
0.1
10−2
10−3
10−4
10−5
10−6
33333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333
3
3
3
0
10
102
104
vertex in-degree
3
105
Figure 3: (Flickr) Log-log plot of the in-degree CCDF.
0.5
0.2
0.1
0.02
10−2
10−3
2
3
+
0
2
2
22222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222
33333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
33333333
++++++++
33
++
333
+++
3
SingleRW
FS (m = 1000)
MutipleRW (m = 1000)
3
+
2
10
102
104
105
vertex in-degree
Figure 4: (LCC of Flickr) The log-log plot of the CNMSE of the
in-degree distribution estimates with budget B = |V |/100.
when there are no disconnected components. Figure 4 shows
a log-log plot of the CNMSE of FS (m = 1000), SingleRW,
and MultipleRW (m = 1000). In this experiment FS out-
performs both SingleRW and MultipleRW. It is interesting
to note that the estimates obtained by SingleRW are more
accurate than the estimates obtained by MultipleRW. Now
consider the complete Flickr graph. Figure 5 shows a log-log
plot of the CNMSE of the in-degree distribution estimates.
Contrasting the plots shown in Figures 4 and 5 we see that
the gap between FS and both SingleRW and MultipleRW
has signiﬁcantly increased, favoring FS.
To better understand the diﬀerences between these sam-
pling methods, Figure 6 focuses on four runs (sample paths)
of the simulation over the complete Flickr graph. Figure 6
plots the evolution of ˆθ1 (the estimate of θ1) as a function
of n (the number of steps in the random walk). At each run
of the simulator both FS and MultipleRW start at the same
initial set of vertices (chosen using random vertex sampling).
Figure 6 shows that all four FS sample paths (runs) quickly
converge to the value of θ1. For SingleRW, three of the four
runs start inside the LCC. These runs do not converge to the
E
S
M
N
C
0.5
0.2
0.1
0.02
10−2
10−3
3
2
+
0
222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222
33333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333
33333333
33
++++++++
++
+++
2
3
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
SingleRW
FS (m = 1000)
MutipleRW (m = 1000)
3
+
2
10
102
104
105
vertex in-degree
Figure 5: (Flickr) The log-log plot of the CNMSE of the in-degree
distribution estimates with budget B = |V |/100.
0.65
0.6
0.55
0.5
)
n
(
1
ˆθ
0.45
0.4
0.35
SingleRW
FS (m = 1000)
MultipleRW (m = 1000)
103
5 × 103
|V |/100
1 × 105
106
Random walk steps (n)
Figure 6: (LCC of Flickr) Four sample paths of ˆθ1 (θ1 = 0.53) as
a function of the number of steps n (horizontal axis in log scale).
value of θ1 as some vertices with in-degree one lie outside
the LCC. In one of the runs, SingleRW starts in a small dis-
connected component and, thus, grossly overestimates the
value of θ1. For a similar reason, i.e., walkers starting at
small disconnected components, MultipleRW grossly over-
estimates the value of θ1. The MultipleRW rapid increase
of ˆθ1 at around n = 103 steps needs further investigation. It
may be due to the transient of the random walk (discussed
in Section 4.4). Even when n ≫ 1 (not shown in Figure 6)
the MultipleRW estimate is unable to converge to θ1. Modi-
fying both SingleRW and MultipleRW methods to cope with
disconnected components is an interesting open problem.
For the sake of conciseness, we omit the results of the
simulations over the remaining graphs (Table 1) as they
are similar to the results observed over the Flickr graph.
However, consider the out-degree distribution estimates of
Livejournal. Figure 7 shows a log-log plot of the CCDF of
the out-degrees. The log-log plot of the CNMSE is shown
in Figure 8 for FS (m = 100), SingleRW, and MultipleRW
(m = 100) with sampling budget B = |V |/10. From Fig-
ure 8 we see that estimates of vertices with small out-degrees
3991
3 3
F
D
C
C
0.1
10−2
10−3
10−4
10−5
10−6
3333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333
3
3
3
3
0
10
102
104
105
vertex out-degree
Figure 7: (Livejournal) Log-log plot of the out-degree CCDF.
in FS are up to one order of magnitude more accurate than
those obtained from both SingleRW and MultipleRW.
Moreover, as the starting vertex of each random walker is
chosen uniformly at random, GA, which has the same num-
ber of vertices as GB but 1/5 of the edges, receives more
random walkers than its per edge “share”. Consequently,
MultipleRW oversamples GA.
Figure 9 shows the results of four simulation runs and
plots the evolution of the estimates of θ10 (ˆθ10) as a function
of the number of steps. In this simulation note that: (1) FS
quickly converges to a value that is close to the correct value;
(2) two out of the four SingleRW runs overestimate θ10 and
the remaining two underestimate it; (3) three out of the four
MultipleRW runs converge to the same, incorrect, fraction
(underestimating the true value of θ10). FS is designed to be
robust to disconnected or loosely connected components. All
of the FS runs quickly converge to a good estimates of θ10.
Figure 10 also shows that the CNMSE for FS is consistently
lower than the CNMSE for SingleRW and MultipleRW.
22222222222222
3333333333333333
++++++++++++++++
2222222
33333
+++++
2
2
2
333
+++
33333
+++++
33333333333333333333333333333333333333333333333333333
+++++++++++++++++++++++++++++++++++++++++++++++++++++
3
+
++
333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333
3
333
333333333333333333333333333333333333333333333333333333333333333333333333333333333
22222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
33333333333333333333333333
3
2
3
2
1
0.5
0.2
0.1
+
+
0.02
10−2
E
S
M
N
C
3
+
2
333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333
222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222
333333333333333333333
222222222222222222222
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++
++
+++++++++++
+++++
SingleRW
FS (m = 1000)
MutipleRW (m = 1000)
3
+
2
0.5
0.2
0.1
0.02
10−2
E
S
M
N
C
2
3
+
3
2
+
0
SingleRW
FS (m = 1000)
MutipleRW (m = 1000)
10
102
104
vertex in-degree
Figure 8: (Livejournal) The log-log plot of the CNMSE of the out-
degree distribution estimation with sampling budget B = |V |/100
(CNMSE over 10, 000 runs).
)
n
(
0
1
ˆθ
0.048
0.024
0.012
0
1
SingleRW
FS (m = 100)
MultipleRW (K = 100)
5 × 103
104
Random walk steps (n)
Figure 9: (GAB graph) Four paths of ˆθ10 as a function of the
number of steps n (θ10 = 0.024).
The next experiment focuses on studying the impact of
loosely connected components on the degree distribution es-
timates. For this we use the two Barabási-Albert joined
graphs GAB presented in Section 4.2.2. The experiment con-
sists of estimating the degree distribution of GAB using FS
(m = 100), SingleRW, and MultipleRW (m = 100). Again,
both FS and MultipleRW start at the initial set of vertices
in each simulation (chosen uniformly at random).
In this
experiment the hypothesis is that, for small sampling bud-
gets, each random walker will see the degree distribution of
either GA or GB but not the degree distribution of GAB.
1
10
102
104
vertex in-degree
Figure 10: (GAB graph) The log-log plot of the CNMSE of the
degree distribution estimation with sampling budget B = |V |/100
(CNMSE over 10, 000 runs).
6.3 FS v.s. Stationary MultipleRW & SingleRW
We now compare FS with SingleRW and MultipleRW,
when the latter two start in steady state. Figure 11 shows
the results (over the Flickr graph) of the same simulation
scenario used to obtain the results in Figure 5, except that
now MultipleRW and SingleRW both start in steady state.
While SingleRW has improved slightly (most notably at the
tail errors), the beneﬁt of starting in steady state is most
felt by the MultipleRW method. In this simulation we see
that the large estimation errors of MultipleRW in the previ-
ous simulations were due to the starting vertices being sam-
pled uniformly at random. It is interesting to observe that
MultipleRW starting in steady state and FS have similar
estimation errors.
6.4 FS v.s. Random Independent Sampling
In Section 3 we showed that, if the degrees of two neigh-
boring vertices are independent, random edge sampling is
more accurate than random vertex sampling when it comes
to estimating the tail of the degree distribution. In this sec-
tion we observe this to be true over large real world graphs;
we also observe that the accuracy of FS closely matches the
accuracy of random edge sampling. In the following simula-
tions we estimate the in-degree distribution. Random edge
sampling uses the estimator ˆθi, equation (7) (the estimator
used for sampled vertices is trivial).
4000.5
0.2
0.1
0.02
10−2
E
S
M
N
C
3
2
+
0
22222222
++++++++
22
++
33333333
222
+++
33
333
2222
++++
2222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
33333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333
3
2
SingleRW
FS (m = 1000)
MutipleRW (m = 1000)
3
+
2
10
102
104
105
vertex in-degree
(Flickr) The log-log plot of the CNMSE of the
Figure 11:
in-degree distribution estimation of MultipleRW and SingleRW
starting in steady state; sampling budget B = |V |/100 (NMSE
over 10, 000 runs).
In our ﬁrst simulation we set the sampling cost of ran-
dom vertex sampling to one and random edge sampling has
cost two (as each edge samples two vertices). The sam-
pling budget is B = |V |/100. We label this simulation
“100% hit ratio” to indicate the unitary cost of randomly
sampling vertices. Figure 12 shows a log-log plot of the
NMSE (not the CNMSE ) of our simulation over the (com-
plete) Flickr graph. Here we use the NMSE (instead of the
CNMSE ) in order to be able to compare our results with
the ones presented in equations (3) and (4). The vertical