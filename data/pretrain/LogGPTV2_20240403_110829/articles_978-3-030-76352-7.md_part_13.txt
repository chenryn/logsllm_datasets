instancesforeachDPPservice.Similarly,DCO for scale-in findstheoptimal
set of running compute instances that can be removed for each DPP service
in response to the delta decrease in the workload.
– Full-capacity optimization (FCO) finds the cost-optimal resource allocation
per DPP service based on the predicted workload while ensuring the end-to-
endQoSrequirements.Itdoesnotconsidertheexistingresourceallocationin
its decision-making. And can potentially replace all of the existing compute
instances if a more cost-optimal allocation can be found.
– Fault-tolerant optimization (FTO) aims at making the DPP fault-tolerant.
This can be done in a number of different ways including by (a) over pro-
visioning resources for each service in the pipeline, (b) using heterogeneous
resources for each service, and (c) using heterogeneous contracting options
for the computing resources.
Depending upon the scaling strategy chosen, the resource allocation for the
next time period will be different. The ROrch takes the output of ROpt and
manages the creation/termination of resources on the target cloud platform. It
uses the target cloud platform API to send requests to launch new resources
or terminate existing ones; it also re-configures/restarts the DPP services as
80 S. S. Samant et al.
required after each scaling operation. Once the resources have been scaled by
the ROrch, the results are stored in the Resource Info DB.
3 Demonstration
We demonstrate how AuraEN can autonomously manage the computing
resourcesforeachserviceinacustomdatastreamprocessingpipeline1 deployed
ontheAmazonEC2cloudinAsia-Pacificap-southeast-2zone.Weuseasynthetic
workloadgeneratorapplication2 toproducedatastreamsrepresentingthevehic-
ularstreamingdata.Itcanbeconfiguredtoproducethedatastreamsatdifferent
rates to simulate varying workload and implements the Kafka producer API for
sendingdatastreamstotheKafkaservice.TheDPPusesApacheKafka,Apache
Spark and Apache Cassandra for ingestion, processing and storage respectively.
ToprocesstheingesteddatastreamsfromtheKafkaservice,adatastreampro-
cessor application is3 implemented using Spark streaming APIs for Java along
with the connector APIs for Kafka and Cassandra service to pull the raw data
streamsandstoretheprocessedresultsrespectively.Thestreamprocessingtask
involves map and filter operations on the pulled data streams and the processed
results are stored into the Cassandra for consumption. The resource require-
ments for each service are fulfilled using On-Demand EC2 instances. For the
purpose of demonstration we use t2.micro, t2.small and t2.medium instances
running the Ubuntu operating system.
In the demonstration, we show how the DPP is deployed and managed on
the EC2 cloud using AuraEN. Initially, resources are allocated for the pipeline
services based on a pre-defined default input workload and end-to-end latency
requirements. Once the DPP has been successfully deployed and all the services
are running, the workload generated by the workload generator application is
variedtotriggerresourcescale-inandscale-outusingthedifferentscalingstrate-
gies. A video demonstrating the different scenarios can be found at: [https://
youtu.be/r8S1PcbsCsU].
Acknowledgement. This research is supported by a PhD Scholarship from CSIRO
Data61, an Australian federal government agency responsible for scientific research.
References
1. Samant, S.S., Baruwal Chhetri, M., Vo, Q.B., Kowalczyk, R., Nepal, S.: Towards
end-to-endQoSandcost-awareresourcescalingincloud-basedIoTdataprocessing
pipelines. In: 2018 IEEE International Conference on Services Computing (SCC),
pp. 287–290. IEEE (2018)
2. Samant,S.S.,BaruwalChhetri,M.,Vo,Q.B.,Nepal,S.,Kowalczyk,R.:Benchmark-
ingforend-to-endQoSSustainabilityinCloud-hostedDataProcessingPipelines.In:
2019IEEE5thInternationalConferenceonCollaborationandInternetComputing
(CIC), pp. 39–48. IEEE (2019)
1 https://github.com/samantsunil/AuraEN.
2 https://github.com/samantsunil/data-generator.
3 https://github.com/samantsunil/data-processor-app.
Artificial Intelligence for IT Operations
(AIOPS 2020)
International Workshop on Artificial
Intelligence for IT Operations (AIOps 2020)
Large-scale systems of all types, such as data centres, cloud computing environments,
edge clouds, IoT, and embedded environments, are characterized by extreme
complexity. The large number of processes and their complex interactions make the
successfulmanagementofsuchsystemsanincrementallyhardertask.Todealwiththis,
the operators are increasingly relying on employing artificial intelligence and data
analytic tools against the observational data from the IT system. As a result, the
research field of Artificial Intelligence for IT operations (AIOps) is increasingly
important. It holds the promise to develop and utilize AI-based learning methods
againstthedatafromthesystemstoaidtheoperatorsinsuccessfuloperation.Thehigh
numberofsubmissionstoAIOps2020reflectstheexistinghighinterestoftheresearch
area.
The accepted papers address several important aspects of AIOps related to the
issues of reliability, security, and scalability for the operation of IT systems, such as
cloudsystems,achievedviadataanalysis.Thenumerousexperimentalevaluationsand
use-cases presented deliver an insight into the behaviour of the methods in controlled
test-bedsandreal-worldscenarios,showingtheirusability.Butmoreimportantly,they
point out further open research questions for investigation. Each submission was
reviewedbyatleastthreeseniorreviewers.Fromthe28submittedpapers,weselected
14 high-quality papers. Contextually, the papers are divided into three groups and are
briefly summarized in the following.
The first group, consisting of eight papers, is concerned with the task of detecting
anomalous behaviour of metrics and events represented as textual or numerical data
(including streams). From a modeling perspective, the techniques range from more
traditional data analysis approaches, such as rules, through to deep learning methods,
with a greater proclivity for the latter.
The second group, consisting of four papers, is focused on the problem of fault
localization from metric, event, and alert data. From a modeling perspective, the
proposed methods utilize various approaches from causal discovery to reconstruct the
causal graph of event relations, or to discover frequently co-occurring events.
ThethirdgroupconsistsofthreepapersonnoveltopicsintheareaofAIOps,such
asissuesarisingfromsharingthedataandefficientresourceutilization.Thethirdpaper
ofthisgroupisofgreatimportanceforthecommunitysinceitdepictsthelandscapeof
whatconstitutesAIOps,whatarethehistoricaltrends,whicharetherelatedtasks,and
the types of data sources as well as the most important further directions in the field.
Two keynote speeches were presented. The first by Dan Pei from Tsingua
University on “Towards Autonomous IT Operations through Artificial Intelligence”
and the second by Michael R. Lyu on “Software Reliability Engineering for Resilient
Cloud Operations”. Both keynotes discussed open challenges, possible solutions, and
future directions that can guide the field of AIOps.
Organization
Workshop Organizers
Odej Kao Technische Universität Berlin, Germany
Jorge Cardoso Huawei Munich Research Center, Germany
Workshop Co-chairs
Jasmin Bogatinovski Technische Universität Berlin, Germany
Sasho Nedelkoski Technische Universität Berlin, Germany
Alexander Acker Technische Universität Berlin, Germany
Thorsten Wittkopp Technische Universität Berlinv Germany
Soeren Becker Technische Universität Berlin, Germany
Li Wu Technische Universität Berlin, Germany
Florian Schmidt Technische Universität Berlin, Germany
Program Committee
Ivona Brandic Vienna University of Technology, Austria
Ana Juan Universitat Oberta de Catalunya, Spain
Dan Pei Tsinghua University, China
Johan Tordsson Umeå University, Sweden
Feng Liu Huawei European Research Center, Belgium
Rama Akkiraju IBM, USA
Filipe Araujo University of Coimbra, Portugal
Samuel Kounev University of Wuerzburg, Germany
Domenico Cotroneo University of Naples Federico II, Italy
Roberto Natella University of Naples Federico II, Italy
Michael R. Lyu The Chinese University of Hong Kong, Hong Kong
Jonathan Maces Max Planck Institute for Software Systems, Germany
Stefan Schulte Vienna University of Technology, Austria
Stefan Tai Technische Universität Berlin, Germany
Dragi Kocev Jozef Stefan Institute, Slovenia
Vladimir Podolskiy Technical University of Munich, Germany
Shenglin Zhang Nankai University, China
Gjorgji Madjarov University of Skopje, North Macedonia
Matej Petkovic Jozef Stefan Institute, Slovenia
84 Organization
Ljupco Todorovski University of Ljubljana, Slovenia
Cesare Pautasso University of Lugano, Switzerland
Martin Breskvar Jozef Stefan Institute, Slovenia
We would like to take the opportunity of thanking the authors who submitted a
contribution, as well as our sponsor for the conference, Huawei, and the external
ProgramCommitteemembers,whoseextensivecollaborationmadethiseventpossible.
Performance Diagnosis in Cloud
Microservices Using Deep Learning
B
Li Wu1,2( ), Jasmin Bogatinovski2, Sasho Nedelkoski2, Johan Tordsson1,3,
and Odej Kao2
1 Elastisys AB, Ume˚a, Sweden
{li.wu,johan.tordsson}@elastisys.com
2 Distributed and Operating Systems Group, TU Berlin, Berlin, Germany
{jasmin.bogatinovski,nedelkoski,odej.kao}@tu-berlin.de
3 Department of Computing Science, Ume˚a University, Ume˚a, Sweden
Abstract. Microservicearchitecturesareincreasinglyadoptedtodesign
large-scaleapplications.However,thehighlydistributednatureandcom-
plex dependencies of microservices complicate automatic performance
diagnosis and make it challenging to guarantee service level agreements
(SLAs). In particular, identifying the culprits of a microservice perfor-
mance issue is extremely difficult as the set of potential root causes
is large and issues can manifest themselves in complex ways. This
paper presents an application-agnostic system to locate the culprits for
microservice performance degradation with fine granularity, including
not only the anomalous service from which the performance issue orig-
inates but also the culprit metrics that correlate to the service abnor-
mality. Our method first finds potential culprit services by constructing
aservicedependencygraphandnextappliesanautoencodertoidentify
abnormalservicemetricsbasedonarankedlistofreconstructionerrors.
Our experimental evaluation based on injection of performance anoma-
lies to a microservice benchmark deployed in the cloud shows that our
system achieves a good diagnosis result, with 92% precision in locating
culprit service and 85.5% precision in locating culprit metrics.
· ·
Keywords: Performance diagnosis Root cause analysis
· ·
Microservices Cloud computing Autoencoder
1 Introduction
The microservice architecture design paradigm is becoming a popular choice to
design modern large-scale applications [3]. Its main benefits include accelerated
development and deployment, simplified fault debugging and recovery, and pro-
ducingarichsoftwaredevelopmenttechniquestacks.Withmicroservices,mono-
lithic application can be decomposed into (up to hundreds of) single-concerned,
loosely-coupled services that can be developed and deployed independently [12].
As microservices deployed on cloud platforms are highly-distributed across
multiple hosts and dependent on inter-communicating services, they are prone
(cid:2)c SpringerNatureSwitzerlandAG2021
H.Hacidetal.(Eds.):ICSOC2020Workshops,LNCS12632,pp.85–96,2021.
https://doi.org/10.1007/978-3-030-76352-7_13
86 L. Wu et al.
to performance anomalies due to the external or internal issues. Outside factors
includeresourcecontentionandhardwarefailureorotherproblemse.g.,software
bugs. To guarantee the promised service level agreements (SLAs), it is crucial
to timely pinpoint the root cause of performance problems. Further, to make
appropriate decisions, the diagnosis can provide some insights to the operators
suchaswherethebottleneckislocated,andsuggestmitigationactions.However,
it is considerably challenging to conduct performance diagnosis in microservices
due to the large scale and complexity of microservices and the wide range of
potential causes.
Microservicesrunninginthecloudhavemonitoringcapabilitiesthatcapture
various application-specific and system-level metrics, and can thus understand
the current system state and be used to detect service level objective (SLO)
violations. These monitored metrics are externalization of the internal state of
the system. Metrics can be used to infer the failure in the system and we thus
refer to them as symptoms in anomaly scenarios. However, because of the large
numberofmetricsexposedbymicroservices(e.g.,Uberreports500millionmet-
rics exposed [14]) and that faults tend to propagate among microservices, many
metrics can be detected as anomalous, in addition to the true root cause. These
additional anomalous metrics make it difficult to diagnose performance issues
manually (research problems are stated in Sect.2).
To automate performance diagnosis in microservices effectively and effi-
ciently, different approaches have been developed (briefly discussed in Sect.6).
However, they arelimited byeither coarsegranularity or considerable overhead.
Regardinggranularity,someworkfocusonlocatingtheservicethatinitiatesthe
performance degradation instead of identifying the real cause with fine granu-
larity [8,9,15] (e.g., resource bottleneck or a configuration mistake). We argue
that the coarse-grained fault location is insufficient as it cannot give us more
details to the root causes, which makes it difficult to recover the system timely.
Asforconsiderableoverhead,tonarrowdownthefaultlocation,severalsystems
can pinpoint the root causes with fine granularity. But they need to instrument
application source code or runtime systems, which brings considerable overhead
to a production system and/or slows down development [4].
Inthispaper,weadoptatwo-stageapproachforanomalydetectionandroot
cause analysis (system overview is described in Sect.3). In the first stage, we
model the service that causes the failure following a graph-based approach [16].
This allows us to pinpoint the potential faulty service that initiates the perfor-
mance degradation, by identifying the root cause (anomalous metric) that con-
tributes to the performance degradation of the faulty service. The second stage,
inferenceofthepotentialfailure,isbasedontheassumptionthatthemostimpor-
tant symptoms for the faulty behaviour have a significant deviation from their
valuesduringnormaloperation.Measuringtheindividualcontributiontoeachof
thesymptomsatanytimepoint,thatleadstothediscrepancybetweenobserved
and normal behaviour, allows for localization of the most likely symptoms that
reflect the fault. Given this assumption, we aim to model the symptoms values
under normal system behaviour. To do this we adopt an autoencoder method
Performance Diagnosis in Cloud Microservices Using Deep Learning 87
(Sect.4). Assuming a Gaussian distribution of the reconstruction error from the
autoencoder, we can suggest interesting variations in the data points. We then
decompose the reconstruction error assuming each of the symptoms as equally
important. Further domain and system knowledge can be adopted to re-weight
the error contribution. To deduce the set of possible symptoms as a preference
rule for the creation of the set of possible failure we consider the symptom with
a maximal contribution to the reconstruction error. We evaluate our method in
a microservice benchmark named Sock-shop1, running in a Kubernetes cluster
in Google Cloud Engine (GCE)2, by injecting two types of performance issues
(CPU hog and memory leak) into different microservices. The results show that
oursystemcanidentifytheculpritservicesandmetricswell,with92%and85.5%
in precision separately (Sect.5).
2 Problem Description
Given a collection of loosely coupled microservices S, we collect the relevant
performance metrics over time for each service s ∈ S. We use m(s,t) to denote
the metrics for service s at time t. Furthermore, m(s,t) denotes the individual
i
metric(e.g.,responsetime,containercpuutilization,etc.)forservices,collected
at time t.