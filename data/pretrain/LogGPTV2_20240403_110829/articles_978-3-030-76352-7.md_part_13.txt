### Resource Allocation and Optimization Strategies

- **Dynamic Capacity Optimization (DCO) for Scale-In**: This strategy identifies the optimal set of running compute instances that can be removed for each DPP (Data Processing Pipeline) service in response to a decrease in workload. 
- **Full-Capacity Optimization (FCO)**: FCO determines the cost-optimal resource allocation for each DPP service based on the predicted workload, while ensuring end-to-end QoS (Quality of Service) requirements. It does not consider the existing resource allocation and may replace all current compute instances if a more cost-effective allocation is found.
- **Fault-Tolerant Optimization (FTO)**: FTO aims to make the DPP fault-tolerant by:
  - Over-provisioning resources for each service in the pipeline.
  - Using heterogeneous resources for each service.
  - Utilizing diverse contracting options for computing resources.

The resource allocation for the next time period will vary depending on the chosen scaling strategy. The ROrch (Resource Orchestrator) takes the output from ROpt (Resource Optimizer) and manages the creation or termination of resources on the target cloud platform. It uses the target cloud platform's API to send requests for launching new resources or terminating existing ones. Additionally, it re-configures or restarts the DPP services as needed after each scaling operation. Once the resources have been scaled, the results are stored in the Resource Info DB.

### Demonstration

We demonstrate how AuraEN can autonomously manage the computing resources for each service in a custom data stream processing pipeline deployed on the Amazon EC2 cloud in the Asia-Pacific (ap-southeast-2) zone. We use a synthetic workload generator application to produce data streams representing vehicular streaming data. The generator can be configured to produce data streams at different rates to simulate varying workloads and implements the Kafka producer API for sending data streams to the Kafka service. The DPP uses Apache Kafka, Apache Spark, and Apache Cassandra for ingestion, processing, and storage, respectively.

To process the ingested data streams from the Kafka service, a data stream processor application is implemented using Spark Streaming APIs for Java, along with connector APIs for Kafka and Cassandra to pull raw data streams and store processed results. The stream processing task involves map and filter operations on the pulled data streams, and the processed results are stored in Cassandra for consumption. The resource requirements for each service are fulfilled using On-Demand EC2 instances. For demonstration purposes, we use t2.micro, t2.small, and t2.medium instances running the Ubuntu operating system.

In the demonstration, we show how the DPP is deployed and managed on the EC2 cloud using AuraEN. Initially, resources are allocated for the pipeline services based on a predefined default input workload and end-to-end latency requirements. Once the DPP has been successfully deployed and all services are running, the workload generated by the workload generator application is varied to trigger resource scale-in and scale-out using different scaling strategies. A video demonstrating the different scenarios can be found at: [https://youtu.be/r8S1PcbsCsU].

### Acknowledgment

This research is supported by a PhD Scholarship from CSIRO Data61, an Australian federal government agency responsible for scientific research.

### References

1. Samant, S.S., Baruwal Chhetri, M., Vo, Q.B., Kowalczyk, R., Nepal, S.: Towards end-to-end QoS and cost-aware resource scaling in cloud-based IoT data processing pipelines. In: 2018 IEEE International Conference on Services Computing (SCC), pp. 287–290. IEEE (2018)
2. Samant, S.S., Baruwal Chhetri, M., Vo, Q.B., Nepal, S., Kowalczyk, R.: Benchmarking for end-to-end QoS Sustainability in Cloud-hosted Data Processing Pipelines. In: 2019 IEEE 5th International Conference on Collaboration and Internet Computing (CIC), pp. 39–48. IEEE (2019)

- GitHub Repositories:
  - AuraEN: [https://github.com/samantsunil/AuraEN]
  - Data Generator: [https://github.com/samantsunil/data-generator]
  - Data Processor App: [https://github.com/samantsunil/data-processor-app]

### Artificial Intelligence for IT Operations (AIOps 2020)

**International Workshop on Artificial Intelligence for IT Operations (AIOps 2020)**

Large-scale systems, such as data centers, cloud computing environments, edge clouds, IoT, and embedded environments, are characterized by extreme complexity. The large number of processes and their complex interactions make the successful management of such systems increasingly challenging. To address this, operators are increasingly relying on artificial intelligence and data analytics tools against observational data from the IT system. As a result, the research field of Artificial Intelligence for IT Operations (AIOps) is becoming increasingly important. It holds the promise to develop and utilize AI-based learning methods to aid operators in successful operation. The high number of submissions to AIOps 2020 reflects the significant interest in this research area.

The accepted papers address several important aspects of AIOps related to reliability, security, and scalability for the operation of IT systems, achieved through data analysis. The numerous experimental evaluations and use-cases presented provide insights into the behavior of the methods in controlled test-beds and real-world scenarios, showing their usability. More importantly, they highlight further open research questions for investigation. Each submission was reviewed by at least three senior reviewers. From the 28 submitted papers, 14 high-quality papers were selected. Contextually, the papers are divided into three groups and are briefly summarized below.

- **Group 1 (8 papers)**: Focuses on detecting anomalous behavior of metrics and events represented as textual or numerical data (including streams). The techniques range from traditional data analysis approaches, such as rules, to deep learning methods, with a greater preference for the latter.
- **Group 2 (4 papers)**: Addresses the problem of fault localization from metric, event, and alert data. The proposed methods utilize various approaches, including causal discovery to reconstruct the causal graph of event relations and to discover frequently co-occurring events.
- **Group 3 (3 papers)**: Covers novel topics in AIOps, such as issues arising from sharing data and efficient resource utilization. The third paper in this group is particularly important for the community as it depicts the landscape of what constitutes AIOps, historical trends, related tasks, types of data sources, and the most important future directions in the field.

Two keynote speeches were presented:
- "Towards Autonomous IT Operations through Artificial Intelligence" by Dan Pei from Tsinghua University.
- "Software Reliability Engineering for Resilient Cloud Operations" by Michael R. Lyu.

Both keynotes discussed open challenges, possible solutions, and future directions that can guide the field of AIOps.

### Organization

**Workshop Organizers**
- Odej Kao, Technische Universität Berlin, Germany
- Jorge Cardoso, Huawei Munich Research Center, Germany

**Workshop Co-chairs**
- Jasmin Bogatinovski, Technische Universität Berlin, Germany
- Sasho Nedelkoski, Technische Universität Berlin, Germany
- Alexander Acker, Technische Universität Berlin, Germany
- Thorsten Wittkopp, Technische Universität Berlin, Germany
- Soeren Becker, Technische Universität Berlin, Germany
- Li Wu, Technische Universität Berlin, Germany
- Florian Schmidt, Technische Universität Berlin, Germany

**Program Committee**
- Ivona Brandic, Vienna University of Technology, Austria
- Ana Juan, Universitat Oberta de Catalunya, Spain
- Dan Pei, Tsinghua University, China
- Johan Tordsson, Umeå University, Sweden
- Feng Liu, Huawei European Research Center, Belgium
- Rama Akkiraju, IBM, USA
- Filipe Araujo, University of Coimbra, Portugal
- Samuel Kounev, University of Wuerzburg, Germany
- Domenico Cotroneo, University of Naples Federico II, Italy
- Roberto Natella, University of Naples Federico II, Italy
- Michael R. Lyu, The Chinese University of Hong Kong, Hong Kong
- Jonathan Maces, Max Planck Institute for Software Systems, Germany
- Stefan Schulte, Vienna University of Technology, Austria
- Stefan Tai, Technische Universität Berlin, Germany
- Dragi Kocev, Jozef Stefan Institute, Slovenia
- Vladimir Podolskiy, Technical University of Munich, Germany
- Shenglin Zhang, Nankai University, China
- Gjorgji Madjarov, University of Skopje, North Macedonia
- Matej Petkovic, Jozef Stefan Institute, Slovenia
- Ljupco Todorovski, University of Ljubljana, Slovenia
- Cesare Pautasso, University of Lugano, Switzerland
- Martin Breskvar, Jozef Stefan Institute, Slovenia

We would like to thank the authors who submitted contributions, our sponsor, Huawei, and the external Program Committee members whose extensive collaboration made this event possible.

### Performance Diagnosis in Cloud Microservices Using Deep Learning

**Authors:**
- Li Wu, Elastisys AB, Umeå, Sweden; Distributed and Operating Systems Group, TU Berlin, Berlin, Germany
- Jasmin Bogatinovski, Distributed and Operating Systems Group, TU Berlin, Berlin, Germany
- Sasho Nedelkoski, Distributed and Operating Systems Group, TU Berlin, Berlin, Germany
- Johan Tordsson, Elastisys AB, Umeå, Sweden; Department of Computing Science, Umeå University, Umeå, Sweden
- Odej Kao, Distributed and Operating Systems Group, TU Berlin, Berlin, Germany

**Abstract:**
Microservice architectures are increasingly adopted to design large-scale applications. However, the highly distributed nature and complex dependencies of microservices complicate automatic performance diagnosis and make it challenging to guarantee service level agreements (SLAs). Identifying the culprits of a microservice performance issue is extremely difficult due to the large set of potential root causes and the complex ways issues can manifest. This paper presents an application-agnostic system to locate the culprits for microservice performance degradation with fine granularity, including not only the anomalous service from which the performance issue originates but also the culprit metrics that correlate to the service abnormality. Our method first finds potential culprit services by constructing a service dependency graph and then applies an autoencoder to identify abnormal service metrics based on a ranked list of reconstruction errors. Our experimental evaluation, based on injecting performance anomalies into a microservice benchmark deployed in the cloud, shows that our system achieves good diagnosis results, with 92% precision in locating the culprit service and 85.5% precision in locating the culprit metrics.

**Keywords:**
- Performance diagnosis
- Root cause analysis
- Microservices
- Cloud computing
- Autoencoder

### 1. Introduction

The microservice architecture design paradigm is becoming a popular choice for modern large-scale applications. Its main benefits include accelerated development and deployment, simplified fault debugging and recovery, and a rich software development technique stack. With microservices, monolithic applications can be decomposed into single-concerned, loosely-coupled services that can be developed and deployed independently. 

Microservices deployed on cloud platforms are highly distributed across multiple hosts and dependent on inter-communicating services, making them prone to performance anomalies due to external or internal issues. External factors include resource contention and hardware failure, while internal factors include software bugs. To guarantee promised SLAs, it is crucial to timely pinpoint the root cause of performance problems. Further, to make appropriate decisions, the diagnosis can provide insights to operators, such as where the bottleneck is located, and suggest mitigation actions. However, it is challenging to conduct performance diagnosis in microservices due to the large scale and complexity of microservices and the wide range of potential causes.

Microservices running in the cloud have monitoring capabilities that capture various application-specific and system-level metrics, allowing for the detection of service level objective (SLO) violations. These monitored metrics are externalizations of the internal state of the system and can be used to infer failures. However, because of the large number of metrics exposed by microservices (e.g., Uber reports 500 million metrics) and the tendency of faults to propagate among microservices, many metrics can be detected as anomalous, in addition to the true root cause. These additional anomalous metrics make it difficult to diagnose performance issues manually.

To automate performance diagnosis in microservices effectively and efficiently, different approaches have been developed. However, they are limited by either coarse granularity or considerable overhead. Some work focuses on locating the service that initiates the performance degradation rather than identifying the real cause with fine granularity. We argue that coarse-grained fault location is insufficient as it cannot provide detailed information about the root causes, making it difficult to recover the system timely. Other systems can pinpoint the root causes with fine granularity but require instrumenting application source code or runtime systems, which brings considerable overhead to a production system and/or slows down development.

In this paper, we adopt a two-stage approach for anomaly detection and root cause analysis. In the first stage, we model the service that causes the failure using a graph-based approach. This allows us to pinpoint the potential faulty service that initiates the performance degradation by identifying the root cause (anomalous metric) contributing to the performance degradation of the faulty service. In the second stage, we infer the potential failure based on the assumption that the most important symptoms for the faulty behavior have a significant deviation from their values during normal operation. Measuring the individual contribution to each symptom at any time point, leading to the discrepancy between observed and normal behavior, allows for the localization of the most likely symptoms reflecting the fault. Assuming a Gaussian distribution of the reconstruction error from the autoencoder, we can suggest interesting variations in the data points. We then decompose the reconstruction error, assuming each symptom is equally important. Further domain and system knowledge can be adopted to re-weight the error contribution. To deduce the set of possible symptoms as a preference rule for the creation of the set of possible failures, we consider the symptom with the maximal contribution to the reconstruction error. We evaluate our method in a microservice benchmark named Sock-shop, running in a Kubernetes cluster in Google Cloud Engine (GCE), by injecting two types of performance issues (CPU hog and memory leak) into different microservices. The results show that our system can identify the culprit services and metrics well, with 92% and 85.5% precision, respectively.

### 2. Problem Description

Given a collection of loosely coupled microservices \( S \), we collect relevant performance metrics over time for each service \( s \in S \). We use \( m(s, t) \) to denote the metrics for service \( s \) at time \( t \). Furthermore, \( m_i(s, t) \) denotes the individual metric (e.g., response time, container CPU utilization, etc.) for service \( s \), collected at time \( t \).