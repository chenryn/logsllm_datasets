that it is given all of the weights in the neural net, while
our adversary has only blackbox access to the trained model
as described above. We experimentally compare our attack
performance and accuracy with that of [12] in Section VII.
Another closely related attack is the more recent subpopu-
lation attack [14]. Here the adversary’s goal is to poison part
of the training data in such a way that only the predictions
on inputs coming from a certain subpopulation in the data are
impacted. To achieve this, the authors poison the data based on
a ﬁlter function that speciﬁes the target subpopulation. However,
the goal of these subpopulation attacks is to attack the accuracy
of the model.
In [21] the authors studied property leakage in the federated
learning framework. In federated learning, the process proceeds
through multiple rounds. In each round each of n > 2 parties
takes the intermediate model and uses their own data to locally
compute an update. These updates are all collected by a central
party and used to form the next intermediate model. The threat
model in [21] is the following: n parties participate in a ML
training using federated learning where one of the participant is
the adversary. The adversary uses the model updates revealed in
each round of the federated training and tries to infer properties
of the training data that are true of a subpopulation but not of
the population as a whole. We note that in this threat model,
the adversary gets to see more information than on our model,
so this result is not directly comparable to ours.
III. PROPERTY INFERENCE ATTACKS
There has been a series of work looking at to what extent a
model leaks information about a certain individual record in the
training set, including work on using differential privacy [9] to
deﬁne what it means for a training algorithm to preserve privacy
of these individuals and technically how that can be achieved.
However, leaking information on individuals is not the only
concern in this context. In many cases even the aggregate
information is sensitive.
This type of aggregate leakage inspires a line of work started
in [2, 12] that looks at property inference attacks, in which the
attacker is trying to learn aggregate information about a dataset.
In particular, we focus here, as did [12], on an attacker who
is trying to determine the frequency of a particular property
in the dataset used to train the model. Notice that this type of
aggregate leakage is a global property of the training dataset
and is not mitigated by differential-privacy.
Does property inference pose an important threat model?
Property inference attacks could reveal very sensitive infor-
mation about the dataset. To illustrate the importance of the
attack model, we provide some examples of such sensitive
information that could be revealed. For further discussion on
the importance of these attacks we refer the reader to previous
work of [11] and [2].
Example 1. Imagine that a company wants to use its internal
emails to train a spam classiﬁer. Such a model would be
expected to reveal which combination of words commonly
indicate spam, and companies might be comfortable sharing
this information. However, using a property inference attack,
the resulting model could also leak information about the
aggregate sentiment of emails in the company, that could be
potentially sensitive. For example, if the sentiment of emails in
the company turn negative near the ﬁnancial quarter, it could
mean that the company is performing below expectations.
Example 2. Similarly, a ﬁnancial company might be willing
to share a model to detect fraud, but might not be willing to
reveal the volume of various types of transactions.
Example 3. Or a number of smaller companies might be
willing to share a model to help target customers for price
reductions etc, however such companies might not be willing
to share speciﬁc sales numbers for different types of products.
Property leakage from poisoned datasets One of the ques-
tions that is not addressed in previous work on property
inference attacks is scenarios where adversary can contribute to
the part of the training set. This could occur either because one
of the parties in collaborative training behaves adversarially, or
because an adversary can inﬂuence some of the input sources
from which training data is collected (e.g. by injecting malware
on some data collection devices). Speciﬁcally, the adversary
can try to craft special poisoning data so that it can infer the
speciﬁc property that it has in mind. Note that this is not
prevented by any of the cryptographic or hardware assisted
solutions: in all of these there is no practical way to guarantee
that the data that is entered is actually correct.
This type of poisoning attack has been extensively studied
in the context of security of ML models, i.e., where the goal
of the attacker is to train the model to miss-classify certain
datapoints [6, 27, 20], but to the best of our knowledge ours
is the ﬁrst work that looks at poisoning attacks that aim to
compromise privacy of the training data.
One natural question that might arise is the following:
if the adversary is already providing parts of the training
data, why does it need to perform the attack to learn the
frequency of the target feature? We argue that this in fact is a
realistic model in certain applications. For example, when
a relatively small number (e.g., 10) of organizations (e.g.
hospitals/enterprises) pool their data to train a joint model,
it is natural to assume that their data sets come from similar
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:49 UTC from IEEE Xplore.  Restrictions apply. 
31122
but somewhat different distributions. So the attacker would
not know the mixed distribution. Our attacks may also apply
in the setting where training is performed on data collected
from many user’s devices or from many sensors, and where
the adversary is able to compromise a rather large fraction of
them and cause them to provide incorrect results. In this case,
the adversary may not have access to any of the data.
Black box or white box model access The information
leakage of machine learning models could be studied in both
white-box and black-box setting. In this paper, we consider
the black box model, where the attacker is only allowed to
make a limited number of queries to the trained model. We
show that these attacks can be very successful. "Black box"
attacks is sometimes used to refer to attacks which also have
access to model’s conﬁdence values on each query [25]. We
emphasize here that we use the stricter notion of black box
and our attacker will use only the model predictions. This
type of attack is studied independently in [8] where they study
“label-only” membership inference attacks.
IV. THREAT MODEL
Before going through the threat model, we introduce some
useful notation.
Notation. We use calligraphic letter (e.g T ) to denote sets and
capital letters (e.g. D) to denote distributions. We use (X, Y )
to denote the joint distribution of two random variables (e.g. the
distribution of labeled instances). To indicate the equivalence
of two distributions we use D1 ≡ D2. By x ← X we denote
sampling x from X and by Prx←X we denote the probability
over sampling x from X. We use Supp(X) to denote the
support set of distribution X. We use p · D1 + (1 − p) · D2
to denote the weighted mixture of D1 and D2 with weights p
and (1 − p).
Property Inference: To analyze property inference, we follow
the model introduced in [11]. Consider a learning algorithm
L : (X × Y)∗ → H that maps datasets in T ∈ (X × Y)∗
to a hypothesis class H. Also consider a Boolean property
f : X → {0, 1}. We consider adversaries who aim at ﬁnding
information about the statistics of the property f over dataset
T ∈ (X × Y)∗, that is used to train a hypothesis h ∈ H. In
particular, the goal of the adversary is to learn information
about ˆf (T ) which is the fraction of data entries in T that has
the property f over data entries, namely ˆf = E(x,y)←T [f (x)].
More speciﬁcally the adversary tries to distinguish between
ˆf (T ) = t0 or ˆf (T ) = t1 for some t0 
τ
then there is an adversary A who wins the security game
PIWP(n, L∗, D−, D+, p, t0, t1) with probability at least 1 −
2δ(n).
Theorem 9 states that our attack will be successful in
distinguishing Dt0 from Dt1 if there are enough points in the
distribution with high uncertainty and the learning algorithm
is Bayes-optimal for a large enough class of distributions.
Remark 10. One can instantiate Theorem 9 by replacing f
with 1 − f. In this case, instead of t0 and t1 we need to work
with 1− t0 and 1− t1. On the other hand, we can also ﬂip the
labels and use the distribution (X, 1 − Y ) instead of (X, Y ).
Using these replacements, we can get four different variants
of the theorem with different conditions for the success of the
attack. In Section VI we will see that in different scenarios we
use different variants as that makes the attack more successful.
A. Attack Description
In this section we prove Theorem 9. We ﬁrst describe an
attack and then show how it proves Theorem 9.
The rough intuition behind the attack is the following. If an
adversary can produce some poisoning data at the training
phase to introduce correlation of the target property with
the label, then this will change the distribution of training
examples. This will change the resulting classiﬁer as well
because the learning algorithm is almost Bayes optimal and
should adapt to distribution changes. This change will cause
the prediction of the uncertain cases (i.e., those which occur
in the training set almost equally often with label 0 and 1)