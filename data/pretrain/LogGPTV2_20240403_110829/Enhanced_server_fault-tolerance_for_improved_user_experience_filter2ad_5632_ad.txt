are already cached at the client and issue the remaining four
GET requests. Each run consists of the client establishing a
TCP connection on port 80 and repeating these four GET re(cid:173)
quests 30 times. (Actually they are repeated 31 times and the
first set is ignored to minimize the impact of startup cost and
cache misses.) We perform the experiments under four dif(cid:173)
ferent network settings: with the client on the same LAN as
other machines; and with the client at WAN-MIT, WAN-SG
and WAN-IN. For each of these network settings, we per-
1-4244-2398-9/08/$20.00 ©2008 IEEE
174
DSN 2008: Marwah et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:29:45 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
form experiments for four different scenarios: (1) no failure
and without the infrastructure required for our architecture;
(2) server failure and traditional server fault-tolerance sup(cid:173)
port implying that the client detects server failure through a
heartbeat mechanism and reissues the failed request (which
then is processed by an alternate backend server); (3) no fail(cid:173)
ure but with our architecture infrastructure deployed; and (4)
server failure with our fault-tolerance mechanism in place.
Furthermore, each run is repeated at least three times and the
average taken.
Action 2: User email session. This action mirrors the
following user interaction.
It starts with the user logging
in. The INBOX folder is displayed with new messages, if
any. The user then hits 'Compose' and drafts an email.Fi(cid:173)
nally, the user sends out the email and logs out. This action
consists of tens of requests, a large percentage of which are
GET requests to download icons and images. There are also
two POST requests: (1) for logging in the user; and (2) for
submitting the user's email to the Web server. Again, we
assume that the images are already cached and do not is(cid:173)
sues those requests in our experiments. Each run consists
of nine GET requests and two POST requests which are re(cid:173)
peated three times, that is, three email sessions are created
and three emails sent out. Again, the experiments are re(cid:173)
peated for four different network conditions and for the four
scenarios described above for Action 1.
6.3 Results and Discussion
The results of our experiments for Action 1 and Action 2
are summarized in Tables 2 and 3, respectively. They list the
average times taken for a run (consisting of 30 sets of four
requests for Action 1 and 3 sets of 11 requests for Action
2) under diverse network and architectural scenarios. The
key measurements to note are the failover times which are
differences between the time taken during a failure-free run
and a run with a server crash failure. Since we are interested
in the user experience during server failure recovery, a large
failover time - leading to degraded user experience - is un(cid:173)
acceptable. The failover times using our architecture are all
under about 3 seconds, with the exception of WAN-IN which
is discussed later. For Action 1, conservative failure detec(cid:173)
tion parameters are used and failure detection times of 1-2
secs were observed. These can be made even shorter since
the logger and backend server are on the same LAN. For Ac(cid:173)
tion 2, more aggressive values were used and failure detec(cid:173)
tion times of around 500ms were observed. One trend to
notice in the results for both Action 1 and Action 2 is that the
failover times tend to increase with increase in RTT times be(cid:173)
tween the client and the server. We believe this is so because,
on failure, the proxy establishes a new TCP connection with
the alternate backend server. For congestion avoidance, this
new TCP connection performs slow start which takes longer
for a larger RTT.
The failover times also depend on the exact point of oc(cid:173)
currence of the failure. A server failure can occur: (1) in
between two transactions; (2) in the middle of a request; (3)
in between a request and its response; and (4) in the mid(cid:173)
dle of a response. During our experiments, backend server
failure is caused by disabling the server's network interface
a random time interval after starting the client. Most times
the failure occurs in between a request and its response, that
is, the request is received but no response is generated yet.
It took many runs to find an instance where failure occurred
in the middle of a response. Considering that most replies in
our experiments are short, the failover time was not very dif(cid:173)
ferent from the other cases, however, it did provide us with
more confirmation that our recovery manager and TCP re(cid:173)
splicing code are correctly implemented. We believe that for
large replies, especially if a failure occurs towards the end
of the response, our architecture will be very effective. In a
few instances, we were able to cause a failure in between two
transactions. This leads to slightly faster recovery as replay
of a failed request is not required.
The failover times for a traditional architecture are also
listed in the two tables. As described earlier, a traditional
system is assumed to be not client transparent and the client
detects failure using a heartbeat mechanism. The heartbeat
values used for LAN, WAN-MIT, WAN-SG and WAN-IN
were Is, 5s, lOs and 20s respectively. Failure is declared if
three heartbeats are missed and thus take between two and
three times the heartbeat interval value. For traditional archi(cid:173)
tecture, failure detection is a major part of the failover time,
especially for WAN connections, since a very high frequency
heartbeat is not practical. In our architecture, failure detec(cid:173)
tion occurs locally at the server and server failures can be
aggressively detected, irrespective of client location. Our ar(cid:173)
chitecture's overhead during normal operation is listed in the
last column of the results tables. These values are low - with
the maximum being 2.6% for Action 1. Although a bit higher
for Action 2, the overhead values are still low.
The results for WAN-IN are peculiar and different from
clients at other WAN locations: the average overhead due to
our architecture for WAN-IN is negative; failover time is high
for Action 1, but seems low for Action 2. We believe this
is due to temporal variations in the network characteristics
of the link while we were conducting the experiments. The
RTT, in addition to being very high also has a large mean
deviation of close to 100 ms, as measured using ping.
We
encountered two potential
instances of non(cid:173)
determinism while running our experiments:
(1) a date
field in the HTTP response header; and (2) a "keepalive"
field in the HTTP header indicating the number of subse(cid:173)
quent requests that can be sent on the same TCP connection.
Both of these have simple fixes. The date field has a fixed
length and thus does not cause any problems at the TCP
layer. Furthermore, it would be an issue at the application
layer only if the date is partially sent when a backend server
fails.
In practice, this is a very unlikely scenario since the
date is in the first few bytes of the response header and is
most likely to be sent atomically. In a pathological scenario,
where this is not true, any non-determinism that occurs will
be detected by our architecture. The keepalive field is a
problem at the TCP layer since its size is not fixed. However,
its impact at the application layer is inconsequential. Making
the length of this field constant in the HTTP header will be a
simple fix to this problem and remove the non-determinism
at the TCP layer.
In our experiments, we configured the
HTTP server to not restrict the number of requests on a TCP
connection and thus this field was absent from the HTTP
header.
We also found that for all our experiments TL = Ts = TA
1-4244-2398-9/08/$20.00 ©2008 IEEE
175
DSN 2008: Marwah et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:29:45 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
Average Time Taken (sec)
Traditional Architecture
No-Failure With Failure
Our Architecture
Failover No-Failure With Failure
LAN
WAN-MIT
WAN-Sa
WAN-IN
3.34
23.0
68.1
299.2
6.44
35.9
105.5
373.5
3.10
12.9
37.4
74.3
3.37
23.6
68.5
297.7
5.10
25.10
71.45
314.7
Overhead during
Failover Normal Operation
1.73
1.50
2.95
17.0
0.03 (0.89%)
0.6 (2.6%)
0.4 (0.58%)
(1.5)
Table 2: Time taken for performing one run of Action 1: Connecting to the login screen.
Average Time Taken (sec)
Traditional Architecture
No-Failure With Failure
Our Architecture
Failover No-Failure With Failure
LAN
WAN-MIT
WAN-Sa
WAN-IN
3.72
7.34
15.74
70.84
6.69
20.8
45.8
128.78
2.97
13.46
30.06
57.94
3.83
7.50
16.9
73.91
4.77
8.95
20.41
72.78
Overhead during
Failover Normal Operation
0.94
1.45
3.51
1.13
0.11 (2.9%)
0.16 (2.17%)
1.16 (7.3%)
3.07 (4.3%)
Table 3: Time taken for performing one run of Action 2: Logging in; drafting and sending an email; and, logging out.
(using terminology from Section 4.2). Furthermore, AckcL
always corresponded to the last server bytes saved at the
logger. From the log messages of the logger, we noticed
that there were only a couple re-sent packets implying that
very few packets were dropped or delayed during our exper(cid:173)
iments; and there were no out-of-order packets received.
7 Conclusions
Server fault-tolerance assumes great significance in the
light of explosive growth in emerging Web-based applica(cid:173)
tions hosted at data centers. If server failures can be seam(cid:173)
lessly and client-transparently tolerated, businesses can de(cid:173)
ploy cost-effective, commodity servers at data centers.
In
this paper, we presented a TCP splice-based server fault(cid:173)
tolerance architecture particularly aimed at reducing failover
times to provide improved user experience during server fail(cid:173)
ure recovery. The main components of our architecture are
logging, transactionalization and tagging of user requests and
responses, connection synchronization and re-splicing. We
also address non-determinism and use adaptive failure detec(cid:173)
tion. We have implemented a prototype of our architecture
in Linux and demonstrated its effectiveness by deploying it
with a real-life webmail application. For our experiments,
LAN and WAN (using PlanetLab nodes) clients were used to
issue common webmail actions, backend server failure was
caused in the middle of request processing, and the failover
times were measured. The results showed that the failover
time is at most a few seconds even for clients connected over
a WAN in contrast to traditional server fault-tolerance tech(cid:173)
niques where such failure detection itself can take tens of
seconds.
References
[1] Ajax, http://wikipedia.org/ajax.
[2] L. Alvisi, T. Bressoud, A. EI-Khashab, K. Marzullo, and D. Zagorodnov. Wrap(cid:173)
In Proceedings of Infocom
ping server-side TCP to mask connection failures.
2001, April 2001.
[3] A. C. Bavier, M. Bowman, B. N. Chun, D. E. Culler, S. Karlin, S. Muir, L. L.
Peterson, T. Roscoe, T. Spalink, and M. Wawrzoniak. Operating systems support
for planetary-scale network services. In NSDI, pages 253-266. USENIX, 2004.
[4] A. Bohra, I. Neamtiu, P. Gallard, F. Sultan, and L. Iftode. Remote repair of
In Proceedings of The International
operating system state using backdoors.
Conference on Autonomic Computing (ICAC-04), 2004.
[5] A. Cohen, S. Rangarajan, and 1. H. Slye. On the performance of tcp splicing
for url-aware redirection. In USENIX Symposium on Internet Technologies and
Systems, 1999.
[6] M. Crispin.
Internet Message Access Protocol - Version 4, Revl. Request For
Comments 2060, Internet Engineering Task Force, 1996.
[7] Ethereal, http://ethereal.com.
[8] Gmail Blog, http://gmailblog.blogspot.com/2007/1 O/code-changes-to-prepare(cid:173)
gmail-for.html.
[9] D. A. Maltz and P. Bhagwat. MSOCKS: An architecture for transport layer mo(cid:173)
bility. In Proceedings ofINFOCOMM'98, March 1998.
[10] M. Marwah, S. Mishra, and C. Fetzer. TCP server fault tolerance using connec(cid:173)
tion migration to a backup server. In Proceedings ofIEEE Int. Conf on Depend(cid:173)
able Systems and Networks, San Francisco, June 2003.
[11] M. Marwah, S. Mishra, and C. Fetzer. Fault-tolerant and scalable tcp splice and
web server architecture. In SRDS, pages 301-310. IEEE Computer Society, 2006.
[12] M. Marwah, S. Mishra, and C. Fetzer. Systems architectures for transactional net(cid:173)
work interface. In 10th IEEE High Assurance Systems Engineering Symposium,
Dallas, TX, Nov. 2007.
[13] Netfilter, http://www.netfilter.org.
[14] M.-C. Rosu and D. Rosu. An evaluation of TCP splice benefits in web proxy
servers. In www, pages 13-24, 2002.
[15] M.-C. Rosu and D. Rosu. Kernel support for faster web proxies.
Annual Technical Conference, General Track, pages 225-238, 2003.
In USENIX
[16] Roundcube webmail, http://roundcube.net.
[17] O. Spatscheck, J. S. Hansen, 1. H. Hartman, and L. L. Peterson. Optimizing TCP
forwarder performance. IEEE/ACM Transactions on Networking, 8(2):146-157,
2000.
[18] F. Sultan, A. Bohra, I. Neamtiu, and L. Iftode. Nonintrusive remote healing using
In Proceedings ofFirst Workshop on Algorithms and Architectures
backdoors.
for Self-Managing Systems, 2003.
[19] D. Zagorodnov, K. Marzullo, L. Alvisi, and T. Bressoud. Engineering fault tol(cid:173)
erant TCP/IP services using Ff-TCP. In Proceedings ofIEEE Int. Conf on De(cid:173)
pendable Systems and Networks, San Francisco, June 2003.
1-4244-2398-9/08/$20.00 ©2008 IEEE
176
DSN 2008: Marwah et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:29:45 UTC from IEEE Xplore.  Restrictions apply.