### Optimized Text

#### Experimental Setup
In our experiments, we issue four GET requests to the server. Each run involves the client establishing a TCP connection on port 80 and repeating these four GET requests 30 times. (Actually, they are repeated 31 times, with the first set ignored to minimize the impact of startup costs and cache misses.) The experiments are conducted under four different network settings: 
- Client on the same LAN as other machines.
- Client at WAN-MIT.
- Client at WAN-SG.
- Client at WAN-IN.

For each network setting, we perform experiments for four scenarios:
1. No failure and without the infrastructure required for our architecture.
2. Server failure with traditional server fault-tolerance support, where the client detects server failure through a heartbeat mechanism and reissues the failed request to an alternate backend server.
3. No failure but with our architecture's infrastructure deployed.
4. Server failure with our fault-tolerance mechanism in place.

Each run is repeated at least three times, and the average time is taken.

#### Action 2: User Email Session
This action simulates the following user interaction:
1. The user logs in.
2. The INBOX folder is displayed, showing new messages, if any.
3. The user clicks 'Compose' and drafts an email.
4. The user sends the email and logs out.

This action consists of tens of requests, primarily GET requests to download icons and images, and two POST requests:
1. For logging in the user.
2. For submitting the user's email to the Web server.

We assume that the images are already cached and do not issue those requests in our experiments. Each run consists of nine GET requests and two POST requests, repeated three times, creating and sending three emails. The experiments are repeated for the four different network conditions and the four scenarios described above for Action 1.

### Results and Discussion

The results of our experiments for Action 1 and Action 2 are summarized in Tables 2 and 3, respectively. These tables list the average times taken for a run (consisting of 30 sets of four requests for Action 1 and 3 sets of 11 requests for Action 2) under various network and architectural scenarios. The key measurements are the failover times, which are the differences between the time taken during a failure-free run and a run with a server crash failure. A large failover time, leading to degraded user experience, is unacceptable. Our architecture achieves failover times under about 3 seconds, with the exception of WAN-IN, which is discussed later.

For Action 1, conservative failure detection parameters are used, resulting in failure detection times of 1-2 seconds. These can be made even shorter since the logger and backend server are on the same LAN. For Action 2, more aggressive values were used, resulting in failure detection times of around 500ms. One trend observed in both actions is that failover times tend to increase with increased RTT (Round-Trip Time) between the client and the server. This is because, upon failure, the proxy establishes a new TCP connection with the alternate backend server, and this new connection performs slow start, which takes longer for larger RTTs.

The failover times also depend on the exact point of failure. A server failure can occur:
1. Between two transactions.
2. In the middle of a request.
3. Between a request and its response.
4. In the middle of a response.

During our experiments, backend server failure is caused by disabling the server's network interface at a random time interval after starting the client. Most failures occurred between a request and its response. It took many runs to find an instance where failure occurred in the middle of a response. Since most replies in our experiments are short, the failover time was not significantly different from other cases, but it confirmed the correct implementation of our recovery manager and TCP re-splicing code. We believe that for large replies, especially if a failure occurs towards the end of the response, our architecture will be very effective. In a few instances, we were able to cause a failure between two transactions, leading to slightly faster recovery as replay of a failed request is not required.

The failover times for a traditional architecture are also listed in the tables. A traditional system is assumed to be not client-transparent, and the client detects failure using a heartbeat mechanism. The heartbeat values used for LAN, WAN-MIT, WAN-SG, and WAN-IN were 1s, 5s, 10s, and 20s, respectively. Failure is declared if three heartbeats are missed, taking between two and three times the heartbeat interval value. For traditional architectures, failure detection is a major part of the failover time, especially for WAN connections, as a very high-frequency heartbeat is not practical. In our architecture, failure detection occurs locally at the server, and server failures can be aggressively detected, irrespective of client location. The overhead during normal operation is listed in the last column of the results tables, with the maximum being 2.6% for Action 1. Although higher for Action 2, the overhead values remain low.

The results for WAN-IN are peculiar and different from clients at other WAN locations: the average overhead due to our architecture for WAN-IN is negative, and the failover time is high for Action 1 but low for Action 2. We believe this is due to temporal variations in the network characteristics of the link while conducting the experiments. The RTT, in addition to being very high, has a large mean deviation of close to 100 ms, as measured using ping.

We encountered two potential instances of non-determinism while running our experiments:
1. A date field in the HTTP response header.
2. A "keepalive" field in the HTTP header indicating the number of subsequent requests that can be sent on the same TCP connection.

Both have simple fixes. The date field has a fixed length and does not cause problems at the TCP layer. At the application layer, it would only be an issue if the date is partially sent when a backend server fails, which is unlikely. In a pathological scenario, any non-determinism will be detected by our architecture. The keepalive field is a problem at the TCP layer since its size is not fixed, but its impact at the application layer is inconsequential. Making the length of this field constant in the HTTP header will remove the non-determinism at the TCP layer. In our experiments, we configured the HTTP server to not restrict the number of requests on a TCP connection, so this field was absent from the HTTP header.

In all our experiments, TL = Ts = TA (using terminology from Section 4.2). Furthermore, AckcL always corresponded to the last server bytes saved at the logger. From the log messages of the logger, we noticed that there were only a couple of re-sent packets, implying that very few packets were dropped or delayed during our experiments, and no out-of-order packets were received.

### Conclusions

Server fault-tolerance is crucial for the explosive growth in web-based applications hosted at data centers. If server failures can be seamlessly and client-transparently tolerated, businesses can deploy cost-effective, commodity servers at data centers. In this paper, we presented a TCP splice-based server fault-tolerance architecture aimed at reducing failover times to provide an improved user experience during server failure recovery. The main components of our architecture are logging, transactionalization and tagging of user requests and responses, connection synchronization, and re-splicing. We also address non-determinism and use adaptive failure detection. We implemented a prototype of our architecture in Linux and demonstrated its effectiveness by deploying it with a real-life webmail application. For our experiments, LAN and WAN (using PlanetLab nodes) clients were used to issue common webmail actions, backend server failure was caused in the middle of request processing, and the failover times were measured. The results showed that the failover time is at most a few seconds even for clients connected over a WAN, in contrast to traditional server fault-tolerance techniques where such failure detection itself can take tens of seconds.

### References
[References are listed as provided, with minor formatting adjustments for consistency.]

---

This optimized version aims to improve clarity, coherence, and professionalism, ensuring that the text is more accessible and easier to understand.