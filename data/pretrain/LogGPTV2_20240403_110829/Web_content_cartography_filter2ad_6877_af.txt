2011.
[33] University of Oregon Route Views Project.
http://www.routeviews.org/.
[34] C. .A. Shue, A. J. Kalafut, and M. Gupta. The Web is
Smaller than it Seems. In Proc. of IMC, 2007.
[35] A. Su, D. Choffnes, A. Kuzmanovic, and F. Bustamante.
Drafting Behind Akamai: Inferring Network Conditions
Based on CDN Redirections. IEEE/ACM Trans. Netw.,
17(6):1752–1765, 2009.
[36] S. Triukose, Z. Wen, and M. Rabinovich. Measuring a
Commercial Content Delivery Network. In Proc. WWW,
2011.
[37] P. Vixie. What DNS is Not. Commun. ACM, 52(12):43–47,
[16] P. Gill, M. Arlitt, Z. Li, and A. Mahanti. The Flattening
2009.
Internet Topology: Natural Evolution, Unsightly Barnacles
or Contrived Collapse? In Proc. of PAM, 2008.
[17] C. Huang, A. Wang, J. Li, and K. Ross. Measuring and
Evaluating Large-scale CDNs. In Proc. ACM IMC, 2008.
[18] V. Jacobson, D. Smetters, J. Thornton, M. Plass, N. Briggs,
and R. Braynard. Networking Named Content. In Proc.
CoNEXT, 2009.
[38] J. Wallerich, H. Dreger, A. Feldmann, B. Krishnamurthy, and
W. Willinger. A Methodology for Studying Persistency
Aspects of Internet Flows. ACM CCR, 2005.
[39] Y. A. Wang, C. Huang, J. Li, and K. W. Ross. Estimating the
Performance of Hypothetical Cloud Service Deployments: A
Measurement-based Approach. In Proc. IEEE INFOCOM,
2011.
[19] B. Krishnamurthy and J. Wang. On Network-aware
[40] Y. Zhang, L. Breslau, V. Paxson, and S. Shenker. On the
Characteristics and Origins of Internet Flow Rates. In Proc.
ACM SIGCOMM, 2002.
Clustering of Web Clients. In Proc. ACM SIGCOMM, 2001.
[20] B. Krishnamurthy, C. Wills, and Y. Zhang. On the Use and
Performance of Content Distribution Networks. In Proc.
ACM IMW, 2001.
[21] R. Krishnan, H. Madhyastha, S. Srinivasan, S. Jain,
A. Krishnamurthy, T. Anderson, and J. Gao. Moving Beyond
End-to-end Path Information to Optimize CDN Performance.
In Proc. ACM IMC, 2009.
[22] C. Labovitz, S. Lekel-Johnson, D. McPherson, J. Oberheide,
and F. Jahanian. Internet Inter-Domain Trafﬁc. In Proc. ACM
SIGCOMM, 2010.
[23] C. Labovitz, D. McPherson, and S. Iekel-Johnson. Internet
observatory report, 2009.
http://www.nanog.org/meetings/nanog47.
[24] T. Leighton. Improving Performance on the Internet.
Commun. ACM, 52(2):44–51, 2009.
[25] Kent Leung and Yiu Lee. Content Distribution Network
Interconnection (CDNi) Requirements. IETF draft, work in
progress, draft-ietf-cdni-requirements-00, Sep 2011.
[26] S. Lloyd. Least Squares Quantization in PCM. IEEE Trans.
Information Theory, 1982.
[27] G. Maier, A. Feldmann, V. Paxson, and M. Allman. On
Dominant Characteristics of Residential Broadband Internet
Trafﬁc. In Proc. ACM IMC, 2009.
[28] Z. M. Mao, C. Cranor, F. Douglis, M. Rabinovich,
O. Spatscheck, and J. Wang. A Precise and Efﬁcient
Evaluation of the Proximity Between Web Clients and Their
Local DNS Servers. In Proc. Usenix ATC, 2002.
[29] MaxMind.
http://www.maxmind.com/app/ip-location/.
[30] E. Nygren, R. K. Sitaraman, and J. Sun. The akamai network:
a platform for high-performance internet applications.
SIGOPS Oper. Syst. Rev., 44:2–19, August 2010.
[31] I. Poese, B. Frank, B. Ager, G. Smaragdakis, and
A. Feldmann. Improving Content Delivery using
597Summary Review Documentation for 
“Web Content Cartography” 
Authors:  B. Ager, W. Mühlbauer, G. Smaragdakis, S. Uhlig 
Reviewer #1 
Strengths: - Novel approach that covers a neglected aspect of the 
Internet topology. 
- Well thought through metrics such as content monopoly index 
and normalized potential. 
Weaknesses: - The same weight is given to each hostname.  
- Small data set, only 100 clean traces. 
- Lack of ground truth. 
- Rely on Alexa ranking to identify hostnames. 
Comments to Authors:   Your approach makes sense but it has 
some serious flaws: 
-  The  same  weight  is  given  to  each  hostname:  if  your  ASN 
ranking  gives  the  same  weight  to  google.com,  yahoo.com, 
netflix.com  and  random  websites.  Will  your  AS  ranking  or  the 
size of each continent mean anything? As you write, the internet 
traffic is following a Zipf’s law.    
-  You  are  not  using  any  ground  truths  (yes  you  validated 
Akamai).  Is  there  a  way  to  collect  Netflow  data  in  one  or  two 
ISPs and use it to do some validation?      
Sec. 2.2: you mention that a data center based CDN may rely on 
multiple  ASes  and  a  massively  distributed  CDN  may  rely  on  a 
large  number  of  ASes.  I  would  leave  out  the  last  part  of  the 
sentence.  The  distinction  does  not  really  make  sense  and  very 
large CDNs like Limelight don’t use a large number of ASNs. 
Sec. 2.3: your assumption that a hostname is served by a single 
hosting infrastructure has a lot of exceptions. I know companies 
that for instance use both Akamai and Limelight. For instance, in 
the  US,  NEtflix,  the  largest  source  of  traffic  is  using  multiple 
CDNs (Akamai, Limelight and Level 3).  
Sec. 4.1.1 is surprising. I was expecting more content to be served 
locally (e.g. only 5.9% for Oceania!). You should highlight that 
North America still seems to dominate. 
Table  3  shows  the  top  20  hosting  infrastructures.  Did  you 
investigate the tail further to see if it makes sense? Did you try to 
run  reverse  lookups  inside  BGP  prefixes  to  see  if  there  are  any 
interesting  patterns 
reject  your 
classification?  
What is Skyrock doing as number 10? Isn’t that Internet Radio? 
That does not seem to make sense. Is it a bias introduced by the 
fact  that  you  are  counting  hostnames  and  maybe  Internet  radios 
have  many  different  hostnames  registered?  You  actually  write 
that there is no false positive. This looks like a false positive to 
me. 
In your summary, you claim that your approach is “accurate”. I 
would  not  make  this  claim  and  present  this  as  a  novel 
methodology with some flaws for a challenging problem. 
that  might  confirm  or 
of 
hostnames 
from 
this 
to 
identify  hosting 
characterization 
-  Simple  methodology 
Reviewer #2 
Strengths: 
infrastructures.  
- The analysis on content delivery potential per country and AS is 
interesting. 
Weaknesses:  -  It  is  unclear  how  accurate  the  methodology 
presented is as the validation is quite limited and manual.  
- While the authors claim that cartography can be used by ISPs 
and content providers to profile infrastructures, it is unclear how 
this can be achieved. 
Comments to Authors:  This is an interesting paper that attempts 
to  characterize  hosting  and  content  delivery  infrastructures.  The 
proposed methodology is simple but novel and appears to work to 
some extent at least. 
I liked the metrics of content delivery potential that provide some 
basic  characterization  on  where  content 
is  generated  and 
delivered. 
There are three main points of criticism for the paper in my view: 
First,  the  authors  claim  that  ISPs  and  content  providers  can 
benefit 
and 
infrastructures.  This  is  only  vaguely  discussed  and  it  is  quite 
unclear to me how this high-level profiling can assist providers or 
ISPs in their decisions. 
Second,  how  much  can  one  learn  by  just  examining  the 
hostname? Some hostnames are pretty straightforward to map to 
infrastructures. You methodology most probably provides a better 
grouping, but you could at least examine how much one can do by 
just looking at the hostnames. 
Third and most important, I really have no intuition of what type 
of  clusters  your  methodology  provides,  to  what  extent  the 
grouping of various hostnames together is valid or not (or just a 
side-effect  of  your  similarity  metrics),  and  whether  it  makes 
sense. I am not sure how you can answer these questions, but your 
validation process is quite manual and the analysis does not really 
help  clarifying  these  points.  This  also  holds  for  the  various 
thresholds and parameters used during the clustering. 
Some  parts  of  your  paper  are  quite  vague.  For  example,  the 
summary in Sec. 3.4.4 notes that “more hostnames might reveal 
other  infrastructures”,  or  other  similar  arguments.  It  would  be 
better to try and quantity the claims here. 
How  many  clusters  are  created  at  each  step  of  your  algorithm? 
What is the relative importance and sensitivity of the two steps? 
In  Tables  1  and  2,  it  would  be  nice  to  quote  the  number  of 
samples  per  continent  so  that  we  get  a  feeling  of  their  relative 
importance.  
598Summary 4.2.4 is really over-stretching the discussion during the 
section. The classification method is hardly validated, and some 
hints are only presented for the hosting infrastructures. 
There are some problems with the writing throughout the paper. 
Reviewer #3 
Strengths: The paper is mostly well written and thorough.  I like 
the separation of embedded content (assuming I am understanding 
it  right),  as  it  seems  a  good  discriminator  in  the  nature  of  the 
CDN-hosted content. 
Weaknesses:  In  general,  I  find  “web  content  cartography” 
overstates what the paper achieves - but it is a cute term. 
The dataset is weak -- 132 resulting sessions aren’t all that great, 
and  the  argument  that  more  sessions  would  not  improve  things 
substantially seems not quite true. 4.1, in particular, should really 
be  at  the  country  level,  as  I  find  the  continent  granularity  quite 
un-insightful. 
A  breakdown  of  which  kinds  of  content  (in  terms  of  news, 
entertainment,  blogs,  search,  etc)  are  most  distributed  would  be 
insightful.  Perhaps  it’s  hard  to  find  differences,  but  I  saw  no 
mentioning of this aspect. 
I  suppose  the  big  CDNs  host  too  many  of  the  names  in  the 
looked-up  list,  but  it  would  still  help  to  get  a  sense  of  “what’s 
hosted where”, which I found somehow lacking.  
Comments to Authors: I mostly like this paper and spent quite a 
bit of time on it.  My biggest complaint is the size of your dataset. 
484 traces are not particularly diverse to begin with, and the fact 
that you only provide continent-level granularity in 4.1 is a pity.  
Did you not talk with people who have distributed infrastructures 
out there? Ono, Netalyzr, PlanetLab? 
I  also  wonder  about  parts  of  your  clustering  approach.  It  is 
certainly interesting, but another approach comes to mind, namely 
looking  at  the  resulting  CNAMEs,  authorities,  and  reverse 
lookups  and  identifying  the  CDNs  from  that  directly.  How 
different would those results be? Could you use it to cross-check 
your classifier? 
Thanks  for  the  summary  subsections  at the end of each section, 
they helped a lot. 
Detailed comments: 
- Why the factor of two in your similarity metric (Sec 2.3), and 
why did you not just use Jaccard? 
- In the Normalized Content Delivery Potential section (Sec 2.4) 
you  speak  of  the  weight  of  a  hostname,  which  you  have  not 
defined yet.  In general, it would help a lot to provide examples 
here  or  clarify  further,  because  Sec  4.3  becomes  hard  to  follow 
with the relatively abstract notion of normalized Content Delivery 
Potential. 
- In Sec. 3.1, be clearer what you mean by “embedded hostname” 
- I suppose third-party sites referenced in a page’s HTML source? 
-  What  about  intra-AS  hosting  differences?  Is  it  a  given  that 
customers of very large ISPs see the same hosting infrastructures 
throughout? That seems doubtful. 
-  I’m  not  a  fan  of  the  writing  in  Sec  3.4.2,  which  I  found 
particularly hard to follow. I read this paragraph a bunch of times 
and 
have 
a 
I 
do 
bunch 
validated. 
and was still not sure what you’re talking about: “To quantify the 
value of a hostname in relation to a set of hostnames we define its 
hostname utility as the number of /24 subnetworks it adds to the 
set of /24 subnetworks discovered by the set of hostnames.” Too 
many hostnames in there! Figure 2 speaks of utility, which you do 
not define.  The section mentions coverage, which you also do not 
define. 
- The finding in Sec. 3.4.3 that geographic diversity is required in 
order  to  find  lots  of  network  prefixes  that  names  resolve  to  is 
obvious, though good to see confirmed. 
- In Sec. 4.2.1, say “misclassification” instead of “false positive”, 
as you classification is not Boolean. 
-  Why  don’t you list (normalized) content delivery potentials in 
them in Tables 7 and 8 (which should replace Figures 7 and 8)? 
Reviewer #4 
Strengths: Timely problem, reasonably sound measurements. 
Weaknesses: The clustering technique in Section 2 is a bit hard to 
the  paper  oversells 
understand  and  validate.  Also, 
the 
lightweight/dynamic aspect of the measurement/methodology. 
Comments to Authors: I do not have any serious criticism of this 
work,  except  that  the  clustering  algorithm  seems  to  be  poorly 
explained 
of 
questions/suggestions for the authors: 
1.  Section 2 seems a bit out of place and hard to understand. I 
was constantly wondering what the hostnames were, what exactly 
is being clustered, what the clustering is trying to do, and so on. 
Maybe  moving  3.1/3.2  earlier  so  that  the  reader  has  context  for 
what is happening would be useful. 
2.  Why  do  you  need  user  volunteers  --  why  not  use  Planetlab? 
Also, given that you discarded most users/measurements, it raises 
the  question  of  why  you  imposed  this  unnecessary  load  on  the  
users when you intended only to take 1 measurement snapshot per 
user?  Also, I found  the “discarding” a bit ill motivated -- all you 
want is “diversity”, why not treat  different roaming locations of 
the same user as different geographically  isolated points? 
3.  Sec  2.2  --  “small  data  centers  ...  and  a  large  number  of  IP 
addresses” – This seems like an incorrect generalization --  Many 
webservers  sit  behind  a  single  IP  address  but  use 
load 
balancers/anycast within the data center. 
4. It would really help to have more concrete examples in Sec 2.3 
-- the writing is  verbose and unclear at the same time. 
5. Sec 3 -- what do you mean by bottom of Alexa -- bottom 2000 
in the top-million? Also, it seems that the hostnames in the top-
2000/tail-2000 are served by “origin” servers (e.g., cnn.com) that 
are not typically served by hosting infrastructures or cdns etc, so I 
found  this  choice  of  hostnames  puzzling  -  the  only  thing  that 
really makes sense here are the embedded content. Is your goal is 
to study how these origin servers are being hosted as well? 
6. How exactly do I read the breakdown in Table 3? 
7. Sec 4.4 was the part of the paper I found most interesting (the 
rest  of  the  results  were  largely  expected).  Maybe  it  would  be 
useful  to  speculate  or  try  out  some  “unified”  ranking  across the 
different kinds of ranks (e.g., simple avg rank) to see what the top 
ASes would be? 
5998. Related work --One missing but closely related reference: The 
Web is smaller than it seems – 
conferences.sigcomm.org/imc/2007/papers/imc124.pdf 
Reviewer #5 
Strengths:  A  first  broad,  systematic  attempt  to  uncover  the 
hosting infrastructure in the Internet. 
Weaknesses: While there is a lot of analysis in this paper, in the 
end, I cannot really tell what I learnt from this paper (which I did 
not know before- e.g., North America hosts a lot of content, etc.). 
Part of the problem is that the paper focuses too much on numbers 
and  rankings,  and  less  on  drawing  interesting  conclusions  from 
their data. 
Comments to Authors: I am little puzzled by your methodology. 
If I understand things correctly, to identify embedded hostnames 
you fetched the content of Top 5K and Tail 2K Alexa sites once 
and  from  one  location.  But  what  if  the  embedded  hostnames 
different  for  different  locations  and/or  change  with  time?  Some 
content  providers  dynamically  decide  when  to  offload  content 
serving  to  third  parties  and  use  them  only  during  peak  usage 
hours.  
I  cannot  tell  what  sort  of  bias  may  be  introduced  using  your 
method of getting the embedded hostnames only once. I suppose 
the reason you do not ask your vantage points to fetch content and 
instead  have  them  only  do  lookups  is  to  keep  the vantage point 
load low and the probing quick. But perhaps you want to validate 
if  this  optimization  leads  to  different  results  than  the  full 
measurements. 
Somewhat related to the above, I am also puzzled by the metrics 
you  introduce  in  Section  2.1  and  then  use  to  understand  the 
importance  of  countries  and  ASes.  The  metrics  are  based  on 
fraction of hostnames that can be served from a given entity. But 
a user is not interested in hostnames, but the content behind the 
hostname, which may be served by multiple entities. You seem to 
be equating hostname and content. 
The  other  thing  that  is  missing  from  your  metrics  is  a  sense  of 
what  content  is  popular  where.  As  you  measure  things,  North 
America  seems  very  important  from  the  perspective  of  Africa 
(say),  but  if  most  Africans  are  only  visiting  African/European 
sites, then that reliance on North America is a lot weaker. Do you 
stats on what sites are popular where, to enable you to modulate 
your metrics? 
Questions  about  methodology  and  metrics  aside,  the  most 
disappointing part of the paper for me was the analysis. It rattles 
off  a  bunch  of  statistics  that  look  at  data  from  various 
perspectives,  but  does  not  go  deep  enough  to  provide  any  new 
insights. When I was done with reading the paper, nothing stuck 
in terms of your results. The sense I got was, yeah, there are a lot 
of good numbers here and I will look at the paper if I ever needed 
those numbers. 
You  claim  in  a  couple  of  places  that  your  methodology  is  fully 
automated. But recruiting volunteers doesn’t seem automatic.  
this 
is 
the 
to 
first  attempt 
I  think  a  study  like  yours  is  really  valuable  if  it  can  provide  a 
longitudinal  perspective  on  how  the  hosting  infrastructure  is 
evolving.  So,  I  would  encourage  you  to  fully  automate  your 
methods and periodically repeat your measurements and analysis. 
Section 5 is not a discussion. It is repeating the motivation of your 
work,  and  the  arguments  are  identical  to  the  intro.  I  expected 
higher-level reflection on your findings and experience here..  
Response from the Authors 
- Methodology: Our methodology provides an automated way to 
identify  hosting  infrastructures  in  the  wild.  To  the  best  of  our 
knowledge 
identify  hosting 
infrastructures  based  only  on  the  DNS  replies  and  routing 
information, rather than relying on pre-identified signatures, such 
as  CNAMES.  The  advantage  of  this  method  is  that  it  general 
enough  to  identify  new  hosting  infrastructures  as  well  cluster 
hosting  infrastructures  based  on  their  operation,  as  long  as  they 
rely on DNS. 
It  is  important  to  understand  that  our  study  is  a  qualitative,  and 
not quantitative, study of the geographic and AS-level footprint of 
content infrastructures. The focus of this paper is to identify the 
existence  and  deployment  of  content  infrastructures,  not  to 
quantify their relative utilization, e.g., the amount of traffic each 
infrastructure carries. 
- Traces: It is true that the geographic coverage of our traces can 
be  improved.  However,  we  show  that  by  utilizing  traces  from a 
small number of well-distributed vantage points, it is possible to 
make qualitatively correct conclusions. The expected implication 
of  adding  more  traces  is  to  highlight  even  more  the  difference 
between  our  content-related  metrics.  Furthermore,  adding  more 
traces  will  ease  the  classification  task  of  the  clustering.    We 
disagree  that  using  PlanetLab  is  the  right  way  to  improve  the 
diversity  of  the  vantage  points,  because  our  diversity  aims  at 
sampling the commercial Internet. 
- Benefits for ISPs and CPs: For ISPs, knowing the locations from 
which popular content can be obtained is a key factor in peering 
decisions. For content producers, the locations of candidate CDNs 
tell them how best to deliver their content to their customer base. 
For  CDNs,  content  cartography  can  help  them  improve  their 
competitiveness in the content delivery market. 
-  Take-home  message:  A  key  inside  of  our  study  is  that  a 
significant  fraction  of  the  content  is  exclusively  delivered  by 
hosting infrastructures (e.g. Google) or geographical regions (e.g. 
China).  By  deriving  content-centric  AS 
that 
complement  existing  AS  ranking,  we  shed  light  on  recent 
observations about shifts on the AS topology. 
- Future work: Our work is the initial step towards understanding 
the  always  changing  deployment  of  content  infrastructures.  As 
such, it enables further investigation and can be refined to answer 
specific questions. In particular, it will allow us to investigate the 
interplay of content infrastructures with the Internet topology. 
rankings, 
600