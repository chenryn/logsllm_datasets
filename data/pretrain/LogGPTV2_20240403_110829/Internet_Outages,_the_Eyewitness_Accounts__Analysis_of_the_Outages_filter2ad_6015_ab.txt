and compute the F-score, which is the harmonic mean of precision and recall
values [31]. Next, we describe the features provided to the machine learning
algorithm.
Feature selection and reﬁnement: The na¨ıve way to set up a NLP classiﬁ-
cation is to use a standard “bag-of-words” approach—extract words appearing
in the entire dataset and create a binary feature vector for each thread indi-
cating whether a speciﬁc keyword appears in it. This approach, however, yields
very poor results on two fronts. First, while natural language text contains some
terms relevant to the outage, it mostly contains English words which are not rel-
Internet Outages, the Eyewitness Accounts
213
evant to the topic and simple ﬁltering steps such as removing stop words (e.g.,
“the”) do not alleviate this problem. Second, this na¨ıve set of features produces
a high-dimensional feature space creating more noise.
Thus, we had to take further care in selecting the feature set using a com-
bination of domain knowledge and manual inspection as described below. First,
since most terms associated with our labels are likely to be nouns, we used a
part-of-speech tagger [42] to ﬁlter out verbs and adjectives. Second, based on
manual inspection, we found that terms in the title of the thread, or near the
end of the thread were more informative and thus we experimented with weigh-
ing these terms higher. The reason is that the issues are mostly resolved towards
the end of the discussion and the terms used are more pertinent to the issue.
Third, to identify the entities involved, we further prune the features using a
named-entity recognition system [21]. While this step retains good features (i.e.,
words or phrases recognized as entities), it does not provide any semantic infor-
mation about them. To this end, we used Wikipedia category information to
glean such semantic associations. We collected 20,105 Wikipedia pages under
the category “Computer Networking”, and weighted the features according to
whether they occur in pages under relevant subcategories (e.g., “Akamai” under
“Content Delivery Network”). We thus designed feature vectors with relevant
entities, and weighted them according to their type. (Note that these three steps
are in addition to the preprocessing in Sect. 3 that was less analysis-speciﬁc).
Table 3. Summary of feature sets used to improve the performance of the classiﬁers
Root cause of outage
Entities involved
1. Unigrams
1. Unigrams + bigrams (nouns)
2. Unigrams + bigrams
2. Unigrams + bigrams (nouns) +
3. Nouns
positional weights
3. Nouns + named entities
4. Unigrams + bigrams (nouns)
4. Named entities
5. Unigrams + bigrams (nouns) +
5. Named entities + Wikipedia category
positional weights
information
Table 3 summarizes the diﬀerent sets of terms we used and Fig. 3 shows how
the F-score improves as we add better features. The ﬁnal features selected dif-
fer between the type and entity classiﬁers; i.e., nouns weighed by their position
in the thread performing best for root cause and a combination of named enti-
ties+Wikipedia category information for the entities involved. With these fea-
tures the mean F-score of the classiﬁers was 78.8% for root cause and 82.9% for
entities involved. For multi-class classiﬁcation tasks for which human annotation
κ scores are in the range of 0.5 – 0.78, these results can be considered as reason-
ably high. Given the relatively small training data set and the succinct nature
of the mailing list posts, the resulting performance is very promising, especially
214
R. Banerjee et al.
e
r
o
c
S
−
F
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
3
1
2
4
Feature Sets
(a) Type of outage
5
1
2
4
5
3
Feature Sets
(b) Entity
Fig. 3. F-score of classiﬁcation results using diﬀerent feature sets. A higher F-score
implies better accuracy and the result shows the eﬀect of our iterative feature reﬁne-
ment process. Table 3 summarizes the feature sets.
for domains for which a large number of user contributed posts are available for
analysis.
Finally, one concern with our binary classiﬁcation approach is the risk that
a given thread falls in multiple classes. Fortunately, we found that the majority
(>80 %) of threads had at most 1 label (not shown).3
5 Characterizing the Causes of Failures
Next, we use the classiﬁcation methodology from the previous section to analyze
common causes and types of outages discussed in the mailing list. Figure 4 shows
the fraction of threads classiﬁed based on their outage and entity types.
Outage types are dominated by user-observed issues. We ﬁnd that the
majority of threads are placed in categories that indicate user impact. For outage
type, mobile data network issues, application server, and application conﬁgura-
tion issues dominate, comprising 28 %, 20 %, and 23 % of the data respectively.
Upon closer inspection we ﬁnd common terms in the application clusters related
s
d
a
e
r
h
T
f
o
%
20
10
0
A
t
t
a
c
k
A
p
p
l
i
c
a
t
i
o
n
C
o
n
A
p
p
S
e
r
v
e
r
l
i
c
a
t
i
o
n
f
i
g
u
r
a
t
i
o
n
C
e
n
s
o
r
s
h
p
i
F
a
i
l
u
r
e
D
e
v
c
e
R
e
s
o
u
i
l
C
u
D
N
S
t
i
F
b
e
r
D
a
t
i
M
o
b
D
s
a
s
t
i
l
e
N
a
L
o
s
s
t
u
r
a
P
a
c
k
e
t
O
u
t
a
g
e
l
P
o
w
e
r
R
o
u
t
i
n
g
e
r
C
o
n
g
e
s
t
i
o
n
t
i
o
n
t
a
N
e
w
o
r
k
s
d
a
e
r
h
T
f
o
%
60
40
20
0
A
c
c
e
s
s
C
D
N
l
C
o
u
d
P
r
o
v
d
e
r
i