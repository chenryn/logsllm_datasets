e
M
220
218
216
214
212
210
28
26
24
218 items, total 2MB
219 items, total 4MB
220 items, total 8MB
 0
 2
 4
 6
 8
 10
 12
Stash Size (KB)
Figure 9: Average time until stash overÔ¨Çow, for varying
vORAM and stash sizes. Stash size is linear-scale, number of
operations in log-scale. Higher is better. For each vORAM size
n, we performed 2n operations to gather sufÔ¨Åcient experimental
data.
|
stash size observed at any point throughout the experiment.
Recall, while the stash will typically be empty after every
operation, the max stash size should grow logarithmically with
respect to the number of items inserted in the vORAM. The
primary results are presented in Figure 8.
This experiment was conducted by running 50 simulations
of a vORAM with n insertions and a height of T = lg n. The
Z/B value ranged from 1 to 50, and results in the range 1
through 12 are presented in the graph for values of n ranging
from 102 through 105. The graph plots the ratio R/ lg n, where
R is the largest max stash size at any point in any of the 50
simulations. Observe that between Z/B = 4 and Z/B = 6
the ratio stabilizes for all values of n, indicating a maximum
stash of approximately 100 lg n.
In order to measure how much stash would be needed in
practice for much larger experimental runs, we Ô¨Åxed Z/B = 6
and for three large database sizes, n = 218, 219, 220, For
each size, we executed 2n operations, measuring the size
of stash after each. In practice, as we would assume from
the theoretical results, the stash size is almost always zero.
189189
However, the stash does occasionally become non-empty, and
it is precisely the frequency and size of these rare events that
we wish to measure.
Fig. 9 shows the result of our stash overÔ¨Çow experiment. We
divided each test run of roughly 2n operations into roughly n
overlapping windows of n operations each, and then for each
window, and each possible stash size, calculated the number of
operations before the Ô¨Årst time that stash size was exceeded.
The average number of operations until this occurred, over
all n windows, is plotted in the graph. The data shows a
linear trend in log-scale, meaning that the stash size neces-
sary to ensure low overÔ¨Çow probability after N operations
is O(log N ), as expected. Furthermore, in all experiments
we never witnessed a stash size larger than roughly 10KB,
whereas the theoretical bound of 100 lg n items would be
16KB for the largest test with 220 8-byte items.
Bucket utilization. Stash size is the most important parameter
of vORAM, but it provides a limited view into the optimal
bucket size ratio, in particular as the stash overÔ¨Çow is typically
zero after every operation, for sufÔ¨Åciently large buckets. We
measured the utilization of buckets at different levels of the
vORAM with varied heights and Z/B values. The results are
presented in Figure 10 and were collected by averaging the
Ô¨Ånal bucket utilization from 10 simulations. The utilization at
each level is measured by dividing the total storage capacity
of the level by the number of bytes at the level. In all cases,
n = 215 elements were inserted, and the vORAM height
varied between 14, 15, and 16. The graph shows that with
height lg n = 15 or higher and Z/B is 6 or higher, utilization
stabilizes throughout all the levels (with only a small spike at
the leaf level).
that when Z/B = 6,
The results indicate, again,
the
utilization at the interior buckets stabilizes. With smaller ratios,
e.g., Z/B = 4, the utilization of buckets higher in the tree
dominates those lower in the tree; essentially, blocks are not
able to reach lower levels resulting in higher stash sizes (see
previous experiment). With larger ratios, which we measured
all the way to Z/B = 13, we observed consistent stabilization.
In addition, our data shows that decreasing the number of
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:16:59 UTC from IEEE Xplore.  Restrictions apply. 
levels from lg n to lg n ‚àí 1 (e.g., from 15 to 14 in the Ô¨Ågure)
increases utilization at the leaf nodes as expected (as depicted
in the spike in the tail of the graphs), but when Z/B ‚â• 6
the extra blocks in leaf nodes do not propagate up the tree
and affect the stash. It therefore appears that in practice, the
number of levels T could be set to lg n ‚àí 1, which will result
in a factor of 2 savings in the size of persistent (cloud) storage
due to high utilization at the leaf nodes. This follows a similar
observation about the height of the Path ORAM made by [9].
B. Measuring vORAM+HIRB Performance
We measured the performance of our vORAM+HIRB im-
plementation on a real data set of reasonable size, and
compared to some alternative methods for storing a remote
map data structure that provide varying levels of security
and efÔ¨Åciency. All of our implementations used the same
client/server setup, with a Python3 implementation and AWS
as the cloud provider, in order to give a fair comparison.
Sample dataset. We tested the performance of our imple-
mentation on a dataset of 300,000 synthetic key/value pairs
where keys were variable sizes (in the order of 10‚Äì20 bytes)
and values were Ô¨Åxed at 16 bytes. The total unencrypted data
set is 22MB in size. In our experiments, we used some subset
of this data dependent on the size of the ORAM, and for each
size, we also assumed that the ORAM user would want to
allow the database to grow. As such, we built the ORAM to
double the size of the initialization.
Optimized vORAM+HIRB implementation. We fully
implemented our vORAM+HIRB map data structure using
Python3 and Amazon Web Services as the cloud service
provider. We used AES256 for encryption in vORAM, and
used SHA1 to generate labels for the HIRB. In our setting,
we considered a client running on the local machine that
maintained the erasable memory, and the server (the cloud)
provided the persistent storage with a simple get/set interface
to store or retrieve a given (encrypted) vORAM bucket.
For the vORAM buckets, we choose Z/B = 6 based on
the prior experiments, and a bucket size of 4K, which is the
preferred back-end transfer size for AWS, and was also the
bucket size used by [12]. One of the advantages of the vORAM
over other ORAMs is that the bucket size can be set to match
the storage requirements with high bucket utilization. The
settings for the HIRB were then selected based on Theorem 8
and based on that, we calculated a Œ≤ = 12 for the sample data
(labels and values) stored within the HIRB. The label, value,
and associated vORAM identiÔ¨Åers total 56 bytes per item.
In our experiments, we found that the round complexity of
protocols dominate performance and so we made a number
of improvements and optimizations to the vORAM access
routines to compensate. The result is an optimized version
of the vORAM. In particular:
‚Ä¢ Parallelization: The optimized vORAM transfers buckets
along a single path in parallel over simultaneous con-
nections for both the evict and writeback methods. Our
experiments used up to T threads in parallel to fetch and
send ORAM block Ô¨Åles, and each maintained a persistent
sftp connection.
‚Ä¢ Buffering: A local buffer storing 2T top-most ORAM
buckets was used to facilitate asynchronous path reading
and writing by our threads. Note the size of the client
storage still remains O(log n) since T = O(log n). This
had an added performance beneÔ¨Åt beyond the paralleliza-
tion because the top few levels of the ORAM generally
resided in the buffer and did not need to be transferred
back and forth to the cloud after every operation.
These optimizations had a considerable effect on the per-
formance. We did not include the cost of the ‚âà 2 second
setup/teardown time for these SSL connections in our results
as these were a one-time cost incurred at initialization. Many
similar techniques to these have been used in previous work to
achieve similar performance gains (e.g., [38], [39]), although
they have not been previously applied to oblivious data struc-
tures.
Comparison baselines. We compared our optimized
vORAM+HIRB construction with four other alternative im-
plementations of a remote map data structure, with a wide
range of performance and security properties:
‚Ä¢ Un-optimized vORAM+HIRB. This is the same as our
normal vORAM+HIRB construction, but without any
buffering of vORAM buckets and with only a single
concurrent sftp connection. This comparison allows us to
see what gains are due to the algorithmic improvements
in vORAM and HIRB, and which are due to the network
optimizations.
‚Ä¢ Naive Baseline: We implemented a naive approach that
provides all three security properties, obliviousness, se-
cure deletion, and history independence. The method
involves maintaining a single, Ô¨Åxed-size encrypted Ô¨Åle
transferred back and forth between the server and client
and re-encrypted on each access. While this solution is
cumbersome for large sizes, it is the obvious solution
for small databases and thus provides a useful baseline.
Furthermore, we are not aware of any other method (other
than vORAM+HIRB) to provide obliviousness, secure-
deletion, and history independence.
‚Ä¢ ORAM+AVL: We implemented the ODS proposed by
[11] of an AVL embedded within an non-recursive Path
ORAM. Note that ORAM+AVL does not provide secure
deletion nor history independence. We used the same
cryptographic settings as our vORAM+HIRB implemen-
tation, and used 256 byte blocks for each AVL node,
which was the smallest size we could achieve without
additional optimizations. As recommended by [9], we
stored Z = 4 Ô¨Åxed-size blocks in each bucket, for a
total of 1K bucket size. Note that this bucket size is
less than the 4K transfer size recommended by the cloud
storage, which reÔ¨Çects the limitation of ORAM+AVL in
that it cannot effectively utilize larger buckets. We add
the observation that, when the same experiments were
run with 4K size buckets (more wasted bandwidth, but
190190
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:16:59 UTC from IEEE Xplore.  Restrictions apply. 





























 !"#
$
 !%&"#
$
'(
)*&!&+

!







	






	


Figure 11: Median of 100 access times for different number of
entries
matching the other experiments),
the timings did not
change by more than 1 second, indicating that the 4K
bucket size is a good choice for the AWS back-end.
‚Ä¢ SD-B-Tree: As another comparison point, we imple-
mented a remotely stored B-Tree with secure deletion
where each node is encrypted with a key stored in the
parent with re-keying for each access, much as described
by Reardon et al. [17]. While this solution provides secure
deletion, and stores all data encrypted, it does not provide
obliviousness nor history independence. Again, we used
AES256 encryption, with Œ≤ = 110 for the B-tree max
internal node size in order to optimize 4K-size blocks.
In terms of asymptotic performance,
In terms of security, only our vORAM+HIRB as well as
the naive baseline provide obliviousness, secure deletion, and
history independence. The ORAM+AVL provides oblivious-
ness only, and the SD-B-Tree is most vulnerable to leaking
information in the cloud, as it provides secure deletion only.
the SD-B-Tree is
fastest, requiring only O(log n) data transfer per operation.
The vORAM+HIRB and ORAM+AVL both require O(log
n)
data transfer per operation, although as discussed previously
the vORAM+HIRB saves a considerable constant factor. The
naive baseline requires O(n) transfer per operation, albeit with
the smallest possible constant factor.
Experimental results. The primary result of the experiment
is presented in Figure 11 where we compared the time (in
seconds) for a single access. Unsurprisingly, the SD-B-Tree
implementation is fastest for sufÔ¨Åciently large database sizes.
However, our optimized vORAM+HIRB implementation was
competitive to the SD-B-Tree performance, both being less
than 1 second across our range of experiments.
2
Most striking is the access time of ORAM+AVL compared
to the vORAM+HIRB implementations. In both the optimized
and un-optimized setting, the vORAM+HIRB is orders of
magnitude faster than ORAM+AVL, 20X faster un-optimized
and 100X faster when optimized. Even for a relatively small
number of entries such as 211, a single access of ORAM+AVL
takes 35 seconds, while it only requires 1.3 seconds of un-
optimized vORAM+HIRB and 0.2 second of an optimized
191191
Naive baseline
Secure deletion B-tree
ORAM+AVL
vORAM+HIRB
Naive baseline
Secure deletion B-tree
ORAM+AVL
vORAM+HIRB
Naive baseline
Secure deletion B-tree
ORAM+AVL
vORAM+HIRB
Naive baseline
Secure deletion B-tree
ORAM+AVL
vORAM+HIRB
Naive baseline
Secure deletion B-tree
ORAM+AVL
vORAM+HIRB
Size 210
Total storage
8.2 KB
36.9 KB
8.4 MB
127.0 KB
Size 215
Total storage
262.1 KB
1.1 MB
268.4 MB
4.2 MB
Size 220
Total storage
8.4 MB
33.8 MB
8.6 GB
134.2 MB
Size 225
Total storage
268.4 MB
1.1 GB
274.9 GB
4.3 GB
Size 230
Total storage
8.6 GB
34.6 GB