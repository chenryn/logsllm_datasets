quences of Developer Interactions in Visual Studio for Usage Smells,
IEEE Transactions on Software Engineering 43 (4) (2017) 359–371.
doi:10.1109/TSE.2016.2592905.
[60] M. Leemans, W. M. P. van der Aalst, M. G. J. van den Brand, The
Statechart Workbench: Enabling scalable software event log analysis
using process mining, in: 2018 IEEE 25th International Conference on
Software Analysis, Evolution and Reengineering (SANER), IEEE, 2018,
pp. 502–506. doi:10.1109/SANER.2018.8330248.
[61] X. Xia, L. Bao, D. Lo, Z. Xing, A. E. Hassan, S. Li, Measuring Program
Comprehension: A Large-Scale Field Study with Professionals, IEEE
Transactions on Software Engineering 44 (10) (2018) 951–976. doi:
10.1109/TSE.2017.2734091.
[62] J. Caldeira, F. Brito e Abreu, J. Reis, J. Cardoso, Assessing Software
Development Teams’ Efficiency using Process Mining, in: 2019 Inter-
national Conference on Process Mining (ICPM), Institute of Electrical
andElectronicsEngineers(IEEE),2019,pp.65–72. doi:10.1109/icpm.
2019.00020.
[63] P. Ardimento, M. L. Bernardi, M. Cimitile, F. M. Maggi, Evaluating
Coding Behavior in Software Development Processes: A Process Mining
Approach, in: 2019 IEEE/ACM International Conference on Software
and System Processes (ICSSP), IEEE, 2019, pp. 84–93. doi:10.1109/
ICSSP.2019.00020.
[64] P.Ardimento,M.L.Bernardi,M.Cimitile,G.D.Ruvo,Learninganalyt-
ics to improve coding abilities: A fuzzy-based process mining approach,
in: IEEE International Conference on Fuzzy Systems, Vol. 2019-June,
Institute of Electrical and Electronics Engineers Inc., 2019, pp. 1–7.
doi:10.1109/FUZZ-IEEE.2019.8859009.
[65] P. Ardimento, M. L. Bernardi, M. Cimitile, G. De Ruvo, Mining De-
veloper’s Behavior from Web-Based IDE Logs, in: 2019 IEEE 28th
International Conference on Enabling Technologies: Infrastructure for
49
Collaborative Enterprises (WETICE), IEEE, 2019, pp. 277–282. doi:
10.1109/WETICE.2019.00065.
[66] K. Beck, M. Fowler, J. Brant, W. Opdyke, D. Roberts, Bad Smells in
Code, in: Improving the design of existing code, O’Reilly, 1999, Ch. 3,
pp. –.
[67] M. Chinosi, A. Trombetta, BPMN: An introduction to the standard,
Computer Standards & Interfaces 34 (1) (2012) 124–134.
[68] C. Gu¨nther, E. Verbeek, XES standard definition, Tech. rep., BPMcen-
ter. org (2014).
[69] A. R. C. Maita, L. C. Martins, C. R. Lo´pez Paz, L. Rafferty, P. C. K.
Hung, S.M.Peres, M.Fantinato, Asystematicmappingstudyofprocess
mining, Enterprise Information Systems 12 (5) (2018) 505–549. doi:
10.1080/17517575.2017.1402371.
[70] C. d. S. Garcia, A. Meincheim, E. R. Faria Junior, M. R. Dalla-
gassa, D. M. V. Sato, D. R. Carvalho, E. A. P. Santos, E. E. Scal-
abrin, Process mining techniques and applications – A systematic map-
ping study, Expert Systems with Applications 133 (2019) 260–295.
doi:10.1016/j.eswa.2019.05.003.
[71] M. Leemans, W. M. P. van der Aalst, M. G. J. van den Brand, Re-
cursion aware modeling and discovery for hierarchical software event
log analysis, in: 2018 IEEE 25th International Conference on Software
Analysis, Evolution and Reengineering (SANER), 2018, pp. 185–196.
doi:10.1109/SANER.2018.8330208.
[72] B. Purnima, K. Arvind, EBK-Means: A Clustering Technique based on
Elbow Method and K-Means in WSN, International Journal of Com-
puter Applications 105 (9) (2014) 17–24.
[73] N. Kaoungku, K. Suksut, R. Chanklan, K. Kerdprasop, N. Kerdprasop,
The silhouette width criterion for clustering and association mining to
select image features, International Journal of Machine Learning and
Computing 8 (1) (2018) 69–73. doi:10.18178/ijmlc.2018.8.1.665.
[74] T.Xia, R.Krishna, J.Chen, G.Mathew, X.Shen, T.Menzies, Hyperpa-
rameter Optimization for Effort Estimation, Tech. rep., North Carolina
State University (4 2018).
50
[75] L.L.Minku,S.Hou,Clusteringdycomanonlinecross-companysoftware
effort estimation study, in: ACM International Conference Proceeding
Series, Association for Computing Machinery, 2017, pp. 12–21. doi:
10.1145/3127005.3127007.
[76] S.B.Jagtap, B.G.Kodge, CensusDataMiningandDataAnalysisusing
WEKA, in: International Conference in Emerging Trends in Science,
Technology and Management-2013, Singapore, 2013, pp. –.
[77] C. Thornton, H. H. Hoos, K. Leyton-Brown, Auto-WEKA 2.0: Au-
tomatic model selection and hyperparameter optimization in WEKA,
Journal of Machine Learning Research 1 (2017) 429.
[78] L. Madeyski, M. Jureczko, M. Jureczko, Which process metrics
can significantly improve defect prediction models? An empirical
study, Software Quality Journal 23 (2015) 393–422. doi:10.1007/
s11219-014-9241-7.
51
Appendix A. Appendix
Appendix A.1. Product Metrics
Table A.7: Product Metrics Description
Name Description Scale
VG McCabe Cyclomatic Complexity (Avg. per Method) Numeric
PAR Number of Parameters (Avg. per Method) Numeric
NBD Nested Block Depth (Avg. per Method) Numeric
CA Afferent Coupling (Avg. per Package Fragment) Numeric
CE Efferent Coupling (Avg. per Package Fragment) Numeric
RMI Instability (Avg. per Package Fragment) Numeric
RMA Abstractness (Avg. per Package Fragment) Numeric
RMD Normalized Distance (Avg. per Package Fragment) Numeric
DIT Depth of Inheritance Tree (Avg. per Type) Numeric
WMC Weighted methods per Class (Avg. per Type) Numeric
NSC Number of Children (Avg. per Type) Numeric
NORM Number of Overridden Methods (Avg. per Type) Numeric
LCOM Lack of Cohesion of Methods (Avg. per Type) Numeric
NOF Number of Attributes (Avg. per Type) Numeric
NSF Number of Static Attributes (Avg. per Type) Numeric
SIX Specialization Index (Avg. per Type) Numeric
NOP Number of Packages Numeric
NOC Number of Classes (Avg. per Package Fragment) Numeric
NOI Number of Interfaces (Avg. per Package Fragment) Numeric
NOM Number of Methods (Avg. per Type) Numeric
NSM Number of Static Methods (Avg. per Type) Numeric
MLOC Method Lines of Code (Avg. per Method) Numeric
TLOC Total Lines of Code Numeric
TLOC Total Lines of Code Numeric
VG LEVEL Different levels of ∆VG (LOW, MEDIUM, HIGH) Categorical
52
Appendix A.2. Process Metrics
Table A.8: Process Metrics Description
Name Description Scale
DEV Number of Developers Numeric
SES Number of User/Development Sessions Numeric
EVTS Number of Events Collected Numeric
NFILES Number of Unique Files Touched Numeric
NCOM Number of Unique Commands Issued in IDE Numeric
PCCPF Process Cyclomatic Complexity per File Touched Numeric
EC Number of Event Classes Numeric
NOA Number of Activities Numeric
NSS Number of Simple States Numeric
NCS Number of Composite States Numeric
NOT Number of Transitions Numeric
PCC Process Cyclomatic Complexity Numeric
NVER Number of Unique IDE Versions Numeric
NCAT Number of Unique Command Categories Numeric
NPLA Number of Unique IDE Platforms Numeric
NISP Number of Unique Geographic Locations Numeric
NOS Number of Unique Operating Systems Numeric
NPER Number of Unique Perspectives used in the IDE Numeric
PCC LEVEL Different levels of PCC (LOW, HIGH) Categorical
53
Table A.9: Process-Extended Metrics Description
Category Name Scale
Java-Extract Method Numeric
Java-Move - Refactoring Numeric
Java-Extract Class... Numeric
Java-Rename - Refactoring Numeric
Refactor
Delete Resources Numeric
Java-Encapsulate Field Numeric
Java-Change Method Signature Numeric
Java-Move Type to New File Numeric
File Open Numeric
Eclipse Editor File Editing Numeric
File Close Numeric
Project Explorer Numeric
Package Explorer Numeric
Long Method Numeric
God Class Numeric
Code Smell Visualization Numeric
Type Checking Numeric
Eclipse View
Feature Envy Numeric
Duplicated Code Numeric
Find and Replace Numeric
Copy Numeric
Paste Numeric
Edit Cut Numeric
Delete Numeric
Undo Numeric
Redo Numeric
Import Numeric
Refresh Numeric
File
Save Numeric
Save All Numeric
Source Generate Getters and Setters Numeric
Compare Select Next Change Numeric
....... //List is truncated on purpose
....... //List size is ≈250
Text Editing Delete Previous Word Numeric
54
Appendix A.3. Algorithms shown in Model Evaluations
RandomCommittee. Method for building an ensemble of randomizable base clas-
sifiers. Each base classifier is built using a different random seed number (but based one
the same data). The final prediction is a straight average of the predictions generated by
the individual base classifiers.
RandomSubSpace. This method constructs a decision tree based classifier that main-
tains highest accuracy on training data and improves on generalization accuracy as it
grows in complexity. The classifier consists of multiple trees constructed systematically
by pseudo-randomly selecting subsets of components of the feature vector, that is, trees
constructed in randomly chosen sub-spaces.
RandomForest. Methodforconstructingaforestofrandomtrees. Itconsistsofalearn-
ing method for classification, regression and other tasks that operates by constructing a
multitude of decision trees at training time and outputting the class that is the mode of
the classes (classification) or mean prediction (regression) of the individual trees.
RepTree. Fast decision tree learner. Builds a decision/regression tree using information
gain/variance and prunes it using reduced-error pruning (with back-fitting). Only sorts
values for numeric attributes once. Missing values are dealt with by splitting the corre-
sponding instances into pieces.
LMT. ’Logistic Model Trees’ are classification trees with logistic regression functions at
the leaves. The algorithm can deal with binary and multi-class target variables, numeric
and nominal attributes and missing values.
Logistic Regression. Method for building and using a multinomial logistic regression
model with a ridge estimator. Logistic regression is a statistical model that in its basic
formusesalogisticfunctiontomodelabinarydependentvariable,althoughmorecomplex
extensions exist.
LWL.TheLocallyWeightedLearningmethodusesaninstance-basedalgorithmtoassign
instance weights which are then used by a specified WeightedInstancesHandler. Can do
classification (e.g. using naive Bayes) or regression (e.g. using linear regression).
LinearNNSearch. Thismethodimplementsthebruteforcesearchalgorithmfornearest
neighbour search.
DecisionTable. Builds and uses a simple decision table majority classifier.
Bagging. Method for bagging a classifier to reduce variance. Can do classification and
regression depending on the base learner.
KStar. Is an instance-based classifier, that is, the class of a test instance is based upon
the class of those training instances similar to it, as determined by some similarity func-
tion. Itdiffersfromotherinstance-basedlearnersinthatitusesanentropy-baseddistance
function.
55
Appendix A.4. Best-Fit Models - Source Code
Listing 2: Best-Fit Model Code for Refactoring Practice Detection
1 /∗∗ Java code to implement the best model found. ∗/
2
3 /∗∗ Attribute Search ∗∗/
4 AttributeSelection as = new AttributeSelection();
5 ASSearch asSearch = ASSearch.forName(”weka.attributeSelection.GreedyStepwise
→ ”, new String[]{”−C”, ”−R”});
6 as.setSearch(asSearch);
7
8 /∗∗ Attribute Evaluation and Selection ∗∗/
9 ASEvaluation asEval = ASEvaluation.forName(”weka.attributeSelection.
→ CfsSubsetEval”, new String[]{”−M”, ”−L”});
10 as.setEvaluator(asEval);
11 as.SelectAttributes(instances);
12
13 /∗∗ Reduce Dimensions ∗∗/
14 instances = as.reduceDimensionality(instances);
15
16 /∗∗ Build Classifier ∗∗/
17 Classifier classifier = AbstractClassifier.forName(”weka. classifiers .meta.
→ RandomCommittee”, new String[]{”−I”, ”64”, ”−S”, ”1”, ”−W”, ”weka.
→ classifiers .trees.RandomForest”, ”−−”, ”−I”, ”29”, ”−K”, ”13”, ”−
→ depth”, ”3”});
18 classifier .buildClassifier(instances);
Listing 3: Best-Fit Model Code for expected Cyclomatic Complexity level detection
1 /∗∗ Java code to implement the best model found. ∗/
2
3 /∗∗ Attribute Search ∗∗/
4 AttributeSelection as = new AttributeSelection();
5 ASSearch asSearch = ASSearch.forName(”weka.attributeSelection.GreedyStepwise
→ ”, new String[]{”−C”, ”−R”});
6 as.setSearch(asSearch);
7
8 /∗∗ Attribute Evaluation and Selection ∗∗/
9 ASEvaluation asEval = ASEvaluation.forName(”weka.attributeSelection.
→ CfsSubsetEval”, new String[]{”−L”});
10 as.setEvaluator(asEval);
11 as.SelectAttributes(instances);
12
13 /∗∗ Reduce Dimensions ∗∗/
14 instances = as.reduceDimensionality(instances);
15
16 /∗∗ Build Classifier ∗∗/
17 Classifier classifier = AbstractClassifier.forName(”weka. classifiers .lazy.
→ LWL”, new String[]{”−K”, ”60”, ”−A”, ”weka.core.neighboursearch.
→ LinearNNSearch”, ”−W”, ”weka. classifiers .rules.DecisionTable”, ”−−”,
→ ”−E”, ”auc”, ”−S”, ”weka.attributeSelection.GreedyStepwise”, ”−X”, ”2
→ ”});
18 classifier .buildClassifier(instances);
56