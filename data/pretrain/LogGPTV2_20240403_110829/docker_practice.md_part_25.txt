启动 Mesos Master 容器。
docker run --net="host" \
-p 5050:5050 \
-e "MESOS_HOSTNAME=${HOST_IP}" \
-e "MESOS_IP=${HOST_IP}" \
-e "MESOS_ZK=zk://${HOST_IP}:2181/mesos" \
-e "MESOS_PORT=5050" \
-e "MESOS_LOG_DIR=/var/log/mesos" \
-e "MESOS_QUORUM=1" \
-e "MESOS_REGISTRY=in_memory" \
-e "MESOS_WORK_DIR=/var/lib/mesos" \
-d \
garland/mesosphere-docker-mesos-master
启动 Marathon。
311
安装与使用
docker run \
-d \
-p 8080:8080 \
garland/mesosphere-docker-marathon --master zk://${HOST_IP}:2181
/mesos --zk zk://${HOST_IP}:2181/marathon
启动 Mesos slave 容器。
docker run -d \
--name mesos_slave_1 \
--entrypoint="mesos-slave" \
-e "MESOS_MASTER=zk://${HOST_IP}:2181/mesos" \
-e "MESOS_LOG_DIR=/var/log/mesos" \
-e "MESOS_LOGGING_LEVEL=INFO" \
garland/mesosphere-docker-mesos-master:latest
接下来，可以通过访问本地 8080 端口来使用 Marathon 启动任务了。
配置说明
ZooKeepr
ZooKeepr 是一个分布式应用的协调工具，用来管理多个 Master 节点的选举和冗
余，监听在 2181 端口。
配置文件在 /etc/zookeeper/conf/ 目录下。
首先，要修改 myid，手动为每一个节点分配一个自己的 id（1-255之间）。
zoo.cfg 是主配置文件，主要修改如下的三行（如果你启动三个 zk 节点）。
server.1=zookeeper1:2888:3888
server.2=zookeeper2:2888:3888
server.3=zookeeper3:2888:3888
主机名需要自己替换，并在 /etc/hosts 中更新。
第一个端口负责从节点连接到主节点的；第二个端口负责主节点的选举通信。
312
安装与使用
Mesos
Mesos 的默认配置目录分别为：
/etc/mesos：共同的配置文件，最关键的是 zk 文件；
/etc/mesos-master：主节点的配置，等价于启动 mesos-master 时候的默认选
项；
/etc/mesos-slave：从节点的配置，等价于启动 mesos-master 时候的默认选
项。
主节点
首先在所有节点上修改 /etc/mesos/zk，为 主节点的 zookeeper 地址列表，例如：
zk://ip1:2181,ip2:2181/mesos
创建 /etc/mesos-master/ip 文件，写入主节点监听的地址。
还可以创建 /etc/mesos-master/cluster 文件，写入集群的别名。
之后，启动服务：
sudo service mesos-master start
更多选项可以参考这里。
从节点
在从节点上，修改 /etc/mesos-slave/ip 文件，写入跟主节点通信的地址。
之后，启动服务。
sudo service mesos-slave start
更多选项可以参考这里。
此时，通过浏览器访问本地 5050 端口，可以看到节点信息。
313
安装与使用
图 1.22.2.1 - mesos
Marathon
启动 marathon 服务。
sudo service marathon start
启动成功后，在 mesos 的 web界面的 frameworks 标签页下面将能看到名称为
marathon 的框架出现。
同时可以通过浏览器访问 8080 端口，看到 marathon 的管理界面。
314
安装与使用
图 1.22.2.2 - marathon
此时，可以通过界面或者 REST API 来创建一个应用，Marathon 会保持该应用的
持续运行。
315
原理与架构
Mesos 基本原理与架构
首先，Mesos 自身只是一个资源调度框架，并非一整套完整的应用管理平台，本身
是不能干活的。但是它可以比较容易的跟各种应用管理或者中间件平台整合，一起
工作，提高资源使用效率。
架构
master-slave 架构，master 使用 zookeeper 来做 HA。
master 单独运行在管理节点上，slave 运行在各个计算任务节点上。
各种具体任务的管理平台，即 framework 跟 master 交互，来申请资源。
基本单元
master
316
原理与架构
负责整体的资源调度和逻辑控制。
slave
负责汇报本节点上的资源给 master，并负责隔离资源来执行具体的任务。
隔离机制当然就是各种容器机制了。
framework
framework 是实际干活的，包括两个主要组件：
scheduler：注册到主节点，等待分配资源；
executor：在 slave 节点上执行本framework 的任务。
framework 分两种：一种是对资源需求可以 scale up 或者 down 的（Hadoop、
Spark）；一种是对资源需求大小是固定的（MPI）。
调度
对于一个资源调度框架来说，最核心的就是调度机制，怎么能快速高效的完成对某
个 framework 资源的分配（最好是能猜到它的实际需求）。
两层调度算法：
master 先调度一大块资源给某个 framework，framework 自己再实现内部的细粒度
调度。
调度机制支持插件。默认是 DRF。
基本调度过程
调度通过 offer 方式交互：
master 提供一个 offer（一组资源） 给 framework；
framework 可以决定要不要，如果接受的话，返回一个描述，说明自己希望如
何使用和分配这些资源（可以说明只希望使用部分资源，则多出来的会被
master 收回）；
master 则根据 framework 的分配情况发送给 slave，以使用 framework 的
executor 来按照分配的资源策略执行任务。
317
原理与架构
过滤器
framework 可以通过过滤器机制告诉 master 它的资源偏好，比如希望分配过来的
offer 有哪个资源，或者至少有多少资源。
主要是为了加速资源分配的交互过程。
回收机制
master 可以通过回收计算节点上的任务来动态调整长期任务和短期任务的分布。
HA
master
master 节点存在单点失效问题，所以肯定要上 HA，目前主要是使用 zookpeer 来
热备份。
同时 master 节点可以通过 slave 和 framework 发来的消息重建内部状态（具体能
有多快呢？这里不使用数据库可能是避免引入复杂度。）。
framework 通知
framework 中相关的失效，master 将发给它的 scheduler 来通知。
318
配置项解析
Mesos 配置项解析
Mesos 的 配置项 可以通过启动时候传递参数或者配置目录下文件的方式给出（推
荐方式，一目了然）。
分为三种类型：通用项（master 和 slave 都支持），只有 master 支持的，以及只
有 slave 支持的。
通用项
--ip=VALUE 监听的 IP 地址
--firewall_rules=VALUE endpoint 防火墙规则， VALUE 可以是 JSON
格式或者存有 JSON 格式的文件路径。
--log_dir=VALUE 日志文件路径，默认不存储日志到本地
--logbufsecs=VALUE buffer 多少秒的日志，然后写入本地
--logging_level=VALUE 日志记录的最低级别
--port=VALUE 监听的端口，master 默认是 5050，slave 默认是 5051。
master 专属配置项
--quorum=VALUE 必备项，使用基于 replicated-Log 的注册表时，复制的个
数
--work_dir=VALUE 必备项，注册表持久化信息存储位置
--zk=VALUE 必备项，zookeepr 的接口地址，支持多个地址，之间用逗号隔
离，可以为文件路径
--acls=VALUE ACL 规则或所在文件
--allocation_interval=VALUE 执行 allocation 的间隔，默认为 1sec
--allocator=VALUE 分配机制，默认为 HierarchicalDRF
--[no-]authenticate 是否允许非认证过的 framework 注册
--[no-]authenticate_slaves 是否允许非认证过的 slaves 注册
--authenticators=VALUE 对 framework 或 salves 进行认证时的实现机制
--cluster=VALUE 集群别名
--credentials=VALUE 存储加密后凭证的文件的路径
--external_log_file=VALUE 采用外部的日志文件
--framework_sorter=VALUE 给定 framework 之间的资源分配策略
319
配置项解析
--hooks=VALUE master 中安装的 hook 模块
--hostname=VALUE master 节点使用的主机名，不配置则从系统中获取
--[no-]log_auto_initialize 是否自动初始化注册表需要的 replicated 日
志
--modules=VALUE 要加载的模块，支持文件路径或者 JSON
--offer_timeout=VALUE offer 撤销的超时
--rate_limits=VALUE framework 的速率限制，比如 qps
--recovery_slave_removal_limit=VALUE 限制注册表恢复后可以移除或
停止的 slave 数目，超出后 master 会失败，默认是 100%
--slave_removal_rate_limit=VALUE slave 没有完成健康度检查时候被
移除的速率上限，例如 1/10mins 代表每十分钟最多有一个
--registry=VALUE 注册表的持久化策略，默认为 replicated_log ，还
可以为 in_memory
--registry_fetch_timeout=VALUE 访问注册表失败超时
--registry_store_timeout=VALUE 存储注册表失败超时
--[no-]registry_strict 是否按照注册表中持久化信息执行操作，默认为
false
--roles=VALUE 集群中 framework 可以所属的分配角色
--[no-]root_submissions root 是否可以提交 framework，默认为 true
--slave_reregister_timeout=VALUE 新的 lead master 节点选举出来后，
多久之内所有的 slave 需要注册，超时的 salve 将被移除并关闭，默认为
10mins
--user_sorter=VALUE 在用户之间分配资源的策略，默认为 drf
--webui_dir=VALUE webui 实现的文件目录所在，默认为
/usr/local/share/mesos/webui
--weights=VALUE 各个角色的权重
--whitelist=VALUE 文件路径，包括发送 offer 的 slave 名单，默认为
None
--zk_session_timeout=VALUE session 超时，默认为 10secs
--max_executors_per_slave=VALUE 配置了 --with-network-
isolator 时可用，限制每个 slave 同时执行任务个数
slave 专属配置项
--master=VALUE 必备项，master 所在地址，或 zookeeper 地址，或文件路
径，可以是列表
320
配置项解析
--attributes=VALUE 机器属性
--authenticatee=VALUE 跟 master 进行认证时候的认证机制
--[no-]cgroups_enable_cfs 采用 CFS 进行带宽限制时候对 CPU 资源进
行限制，默认为 false
--cgroups_hierarchy=VALUE cgroups 的目录根位置，默认为
/sys/fs/cgroup
--[no-]cgroups_limit_swap 限制内存和 swap，默认为 false，只限制内
存
--cgroups_root=VALUE 根 cgroups 的名称，默认为 mesos
--container_disk_watch_interval=VALUE 为容器进行硬盘配额查询的时
间间隔
--containerizer_path=VALUE 采用外部隔离机制（ --
isolation=external ）时候，外部容器机制执行文件路径
--containerizers=VALUE 可用的容器实现机制，包括 mesos、external、
docker
--credential=VALUE 加密后凭证，或者所在文件路径
--default_container_image=VALUE 采用外部容器机制时，任务缺省使用
的镜像
--default_container_info=VALUE 容器信息的缺省值
--default_role=VALUE 资源缺省分配的角色
--disk_watch_interval=VALUE 硬盘使用情况的周期性检查间隔，默认为
1mins
--docker=VALUE docker 执行文件的路径
--docker_remove_delay=VALUE 删除容器之前的等待时间，默认为 6hrs
--[no-]docker_kill_orphans 清除孤儿容器，默认为 true
--docker_sock=VALUE docker sock 地址，默认为
/var/run/docker.sock
--docker_mesos_image=VALUE 运行 slave 的 docker 镜像，如果被配置，
docker 会假定 slave 运行在一个 docker 容器里
--docker_sandbox_directory=VALUE sandbox 映射到容器里的哪个路径
--docker_stop_timeout=VALUE 停止实例后等待多久执行 kill 操作，默认
为 0secs
--[no-]enforce_container_disk_quota 是否启用容器配额限制，默认为
false
--executor_registration_timeout=VALUE 执行应用最多可以等多久再注
册到 slave，否则停止它，默认为 1mins
321
配置项解析
--executor_shutdown_grace_period=VALUE 执行应用停止后，等待多
久，默认为 5secs
--external_log_file=VALUE 外部日志文件
--frameworks_home=VALUE 执行应用前添加的相对路径，默认为空
--gc_delay=VALUE 多久清理一次执行应用目录，默认为 1weeks
--gc_disk_headroom=VALUE 调整计算最大执行应用目录年龄的硬盘留空
量，默认为 0.1
--hadoop_home=VALUE hadoop 安装目录，默认为空，会自动查找
HADOOP_HOME 或者从系统路径中查找
--hooks=VALUE 安装在 master 中的 hook 模块列表
--hostname=VALUE slave 节点使用的主机名
--isolation=VALUE 隔离机制，例如 posix/cpu,posix/mem （默认）或
者 cgroups/cpu,cgroups/mem
--launcher_dir=VALUE mesos 可执行文件的路径，默认为
/usr/local/lib/mesos
--modules=VALUE 要加载的模块，支持文件路径或者 JSON
--perf_duration=VALUE perf 采样时长，必须小于 perf_interval，默认为
10secs
--perf_events=VALUE perf 采样的事件
--perf_interval=VALUE perf 采样的时间间隔
--recover=VALUE 回复后是否重连上旧的执行应用
--recovery_timeout=VALUE slave 恢复时的超时，太久则所有相关的执行
应用将自行退出，默认为 15mins
--registration_backoff_factor=VALUE 跟 master 进行注册时候的重试
时间间隔算法的因子，默认为 1secs，采用随机指数算法，最长 1mins
--resource_monitoring_interval=VALUE 周期性监测执行应用资源使用
情况的间隔，默认为 1secs
--resources=VALUE 每个 slave 可用的资源
--slave_subsystems=VALUE slave 运行在哪些 cgroup 子系统中，包括