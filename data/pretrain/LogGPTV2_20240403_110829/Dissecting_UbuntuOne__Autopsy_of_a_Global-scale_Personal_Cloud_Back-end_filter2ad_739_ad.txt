Surprisingly, we found that DDoS attacks to U1 are more
frequent than one can reasonably expect. Speciﬁcally, we
found evidence of three such attacks in our traces (January
15, 16 and February 6)6. These DDoS attacks had as objec-
tive to share illegal content through the U1 infrastructure.
As visible in Fig. 5, all the attacks resulted in a dramatic
increase of the number of session and authentication requests
per hour —both events related to the management of user
sessions. Actually, the authentication activity under attack
was 5 to 15 times higher than usual, which directly impacts
the Canonical’s authentication subsystem.
The situation for API servers was even worse: during the
second attack (01/16) API servers received an activity 245x
higher than usual, whereas during the ﬁrst (01/15) and last
(02/06) attacks the activity was 4.6x and 6.7x higher than
normal, respectively. Therefore, the most aﬀected compo-
nents were the API servers, as they serviced both session
and storage operations.
We found that these attacks consisted on sharing a single
user id and its credentials to distribute content across thou-
sands of desktop clients. The nature of this attack is similar
to the storage leeching problem reported in [12], which con-
sists of exploiting the freemium business model of Personal
Clouds to illicitly consume bandwidth and storage resources.
Also, the reaction to these attacks was not automatic.
U1 engineers manually handled DDoS by means of deleting
fraudulent users and the content to be shared. This can
be easily seen on the storage activity for the second and
6Our interviews with Canonical engineers conﬁrmed that
these activity spikes correspond to DDoS attacks, instead of
a software release or any other legitimate event.
third attack, which decays within one hour after engineers
detected and responded to the attack.
These observations conﬁrm that Personal Clouds are a
suitable target for attack as other Internet systems, and that
these situations are indeed common. We believe that further
research is needed to build and apply secure storage proto-
cols to these systems, as well as new countermeasures to
automatically react to this kind of threats.
6. UNDERSTANDING USER BEHAVIOR
Understanding the behavior of users is a key source of
information to optimize large-scale systems. This section
provides several insights about the behavior of users in U1.
6.1 Distinguishing Online from Active Users
Online and active users. We consider a user as online
if his desktop client exhibits any form of interaction with the
server. This includes automatic client requests involved in
maintenance or notiﬁcation tasks, for which the user is not
responsible for. Moreover, we consider a user as active if he
performs data management operations on his volumes, such
as uploading a ﬁle or creating a new directory.
Fig. 6 oﬀers a time-series view of the number of online
and active users in the system per hour. Clearly, online
users are more numerous than active users: The percentage
of active users ranges from 3.49% to 16.25% at any moment
in the trace. This observation reveals that the actual storage
workload that U1 supports is light compared to the potential
usage of its user population, and gives a sense on the scale
and costs of these services with respect to their popularity.
Frequency of user operations. Here we examine how
frequent the protocol operations are in order to identify the
hottest ones. Fig. 7(a) depicts the absolute number of each
operation type. As shown in this ﬁgure, the most frequent
operations correspond to data management operations, and
in particular, those operations that relate to the download,
upload and deletion of ﬁles.
Given that active users are a minority, it proves that the
U1 protocol does not imposes high overhead to the server-
side, since the operations that users issue to manage their
sessions and are typically part of the session start up (e.g.,
ListVolumes) are not dominant. And consequently, the ma-
jor part of the processing burden comes from active users as
desired. This is essentially explained by the fact that the
U1 desktop client does not need to regularly poll the server
during idle times, thereby limiting the number of requests
not linked to data management.
have an immediate impact on the back-end performance.
As we will see in § 7, the frequency of API operations will
Traﬃc distribution across users. Now, we turn our
attention to the distribution of consumed traﬃc across users.
In Fig. 7(b) we observe an interesting fact: in one month,
only 14% of users downloaded data from U1, while uploads
162s
n
o
i
t
a
r
e
p
o
f
o
r
e
b
m
u
N
15
10
5 
0 
x107
Number of user operations per type
Distribution of data transferred per user
Download traffic
Upload traffic
1
F
D
C
0.9
0.8
M ove
G et D elta
D el. V ol.
U nlink
Create U
D F
List V ol.
List Shares
D o w nload
M ake
O pen Session
U pload
Close Session
0.7
1Byte
1KByte
1MByte
1GByte
1TByte
Data transferred
c
i
f
f
a
r
t
d
a
o
l
n
w
o
d
f
o
e
r
a
h
S
Gini coefficient = 0.8943
Gini coefficient = 0.8966
1
0.8
0.6
0.4
0.2
1
0.8
0.6
0.4
0.2
c
i
f
f
a
r
t
d
a
o
l
p
u
f
o
e
r
a
h
S
0
0
0.2 0.4 0.6 0.8
Share of population
1
0
0
0.2 0.4 0.6 0.8
Share of population
1
(a) Total number of requests per type.
(b) CDF of traﬃc across users.
(c) Lorenz curve of traﬃc vs active users.
Figure 7: User requests and consumed traﬃc in U1 for one month.
0.158
0.167
Upload
0.044
Make
List Vol.
0.135
0.041
Delete Vol.
Download
Get Delta
0.103
0.094
0.038
0.037
Unlink
RescanFromScratch
QuerySetCaps
Move
Create UDF
0.033
List Shares
Authenticate
Figure 8: Desktop client transition graph through
API operations. Global transition probabilities are
provided for main edges.
represented 25%. This indicates that a minority of users are
responsible for the storage workload of U1.
To better understand this, we measure how (un)equal the
traﬃc distribution across active users is. To do so, we resort
to the Lorenz curve and the Gini coeﬃcient7 as indicators
of inequality. The Gini coeﬃcient varies between 0, which
reﬂects complete equality, and 1, which indicates complete
inequality (i.e., only one user consumes all the traﬃc). The
Lorenz curve plots the proportion of the total income of
the population (y axis) that is cumulatively earned by the
bottom x% of the population. The line at 45 degrees thus
represents perfect equality of incomes.
Fig. 7(c) reports that the consumed traﬃc across active
users is very unequal. That is, the Lorenz curve is very far
from the diagonal line and the Gini coeﬃcient is close to 1.
The reason for this inequality is clear: 1% of active users ac-
count for the 65.6% of the total traﬃc (147.52TB). Providers
may beneﬁt from this fact by identifying and treating these
users more eﬃciently.
Types of user activity. To study the activity of users,
we used the same user classiﬁcation than Drago et al. in [2].
So we distinguished among occasional, download/upload only
and heavy users. A user is occasional if he transfers less than
10KB of data. Users that exhibit more than three orders of
magnitude of diﬀerence between upload and download (e.g.,
1GB versus 1MB) traﬃc are classiﬁed as either download-
only or upload-only. The rest of users are in the heavy group.
Given that, we found that 85.82% of all users are oc-
casional (mainly online users), 7.22% upload-only, 2.34%
7
http://en.wikipedia.org/wiki/Gini_coefficient
download-only and 4.62% are heavy users. Our results clearly
diﬀer from the ones reported in [2], where users are 30%
occasional, 7% upload-only, 26% download-only and 37%
heavy. This may be explained by two reasons: (i) the usage
of Dropbox is more extensive than the usage of U1, and (ii)
users in a university campus are more active and share more
ﬁles than other user types captured in our trace.
6.2 Characterizing User Interactions
User-centric request graph. To analyze how users in-
teract with U1, Fig. 8 shows the sequence of operations
that desktop clients issue to the server in form of a graph.
Nodes represent the diﬀerent protocol operations executed.
And edges describe the transitions from one operation to
another. The width of edges denotes the global frequency of
a given transition. Note that this graph is user-centric, as
it aggregates the diﬀerent sequence of commands that every
user executes, not the sequence of operations as they arrive
to the metadata service.
Interestingly, we found that the repetition of certain oper-
ations becomes really frequent across clients. For instance,
it is highly probable that when a client transfers a ﬁle, the
next operation that he will issue is also another transfer
—either upload or download. This phenomenon can be par-
tially explained by the fact that many times users synchro-
nize data at directory granularity, which involves repeating
several data management operations in cascade. File editing
can be also a source of recurrent transfer operations. This
behavior can be exploited by predictive data management
techniques in the server side (e.g., download prefetching).
Other sequences of operations are also highlighted in the
graph. For instance, once a user is authenticated, he usually
performs a ListVolumes and ListShares operations. This
is a regular initialization ﬂow for desktop clients. We also
observe that Make and Upload operations are quite mixed,
evidencing that for uploading a ﬁle the client ﬁrst needs to
create the metadata entry for this ﬁle in U1.
Burstiness in user operations. Next, we analyze inter-
arrival times between consecutive operations of the same
user. We want to verify whether inter-operation times are
Poisson or not, which may have important implications to
the back-end performance. To this end, we followed the
same methodology proposed in [21, 22], and obtained a time-
series view of Unlink and Upload inter-operation times and
their approximation to a power-law distribution in Fig. 9.
Fig. 9(a) exhibits large spikes for both Unlink and Up-
load operations, corresponding to very long inter-operation
times. This is far from an exponential distribution, where
long inter-operation times are negligible. This shows that
the interactions of users with U1 are not Poisson [21].
Now, we study if the Unlink and Upload inter-operation
times exhibit high variance, which indicates burstiness. In
all cases, while not strictly linear, these distributions show a
downward trend over almost six orders of magnitude. This
163)
.
c
e
s
(
s
e
m
i
t
n
o
i
t
a
r
e
p
o
−
r
e
t
n
I
4
x 10
4
x 10
10
5
0
10
5
0
1
1.2
Unlink operations
Upload operations
1.4
1.6
Time series operations (sample)
1.8
2
6
x 10
(a)
Upload
α=1.54, θ=41.37
Unlink
α=1.44, θ=19.51
−1
10
−3
10
)
x
≥
X
(
r
P
−5
10
−7
10
−2
10
0
2
10
Inter−operation times
10
4
6
10
10
(b)
s
e
i
r
o
t
c
e
r
i
D
5
10
4
10
3
10
2
10
1
10
0
10
10
Files vs directories in user volumes
1
Files/Dirs per volume
F
D
C
0.8
0.6
Files
Dirs
0.4
10
0
1
10
2
10