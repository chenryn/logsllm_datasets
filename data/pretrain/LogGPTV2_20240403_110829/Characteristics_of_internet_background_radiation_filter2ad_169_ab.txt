determine an appropriate response (including appropriate sequence
numbers), without maintaining any transport or application level
state. A key question for this approach is whether all necessary
responders can be constructed in such a stateless fashion. While
exploring this issue is beyond the scope of the present work, we
note that for all of the responders we discuss, we were able to im-
plement a stateless form for Active Sink, as well as a stateful form
based on Honeyd. (To facilitate the dual development, we devel-
oped interface modules so that each could use the same underlying
code for the responders.)
3.2 Application-Level Responders
Our approach to building responders was “data driven”: we de-
termined which responders to build based on observed trafﬁc vol-
umes. Our general strategy was to pick the most common form
of trafﬁc, build a responder for it detailed enough to differentiate
the trafﬁc into speciﬁc types of activity, and, once the “Unknown”
category for that type of activity was sufﬁciently small, repeat the
process with the next largest type of trafﬁc.
Using this process, we built an array of responders for the follow-
ing protocols (Figure 2): HTTP (port 80), NetBIOS (port 137/139),
CIFS/SMB [7] (port 139/445), DCE/RPC [10] (port 135/1025 and
CIFS named pipes), and Dameware (port 6129). We also built
responders to emulate the backdoors installed by MyDoom (port
3127) and Beagle (port 2745) [5], [24].
Application-level responders need to not only adhere to the struc-
ture of the underlying protocol, but also to know what to say. Most
sources are probing for a particular implementation of a given pro-
tocol, and we need to emulate behavior of the target software in
order to keep the conversation going.
The following example of HTTP/WebDAV demonstrates what
this entails. We see frequent "GET /" requests on port 80. Only
by responding to them and mimicking a Microsoft IIS with Web-
DAV enabled will elicit further trafﬁc from the sources. The full
sequence plays as:
GET /
⇒ |200 OK ... Server: Microsoft-IIS/5.0|
SEARCH /
⇒ |411 Length Required|
SEARCH /AAA... (URI length > 30KB)
⇒ (buffer overflow exploit received)
Some types of activity require quite intricate responders. Many
Microsoft Windows services run on top of CIFS (port 139/445),
which lead us to develop the detailed set of responses shown in
Figure 3. Requests on named pipes are further tunneled to var-
ious DCE/RPC responders. One of the most complicated activi-
ties is the exploit on the SAMR and later on the SRVSVC pipe,
which involves more than ten rounds exchanging messages before
the source will reveal its speciﬁc intent by attempting to create an
executable ﬁle on the destination host. Figure 4 shows an example
where we cannot classify the source until the “NT Create AndX”
Honeyd/Active Sink
OS Responder
Honey Interface
ports
80,1080,3128,8888
port 137
port 139
port 6129
ports 2745,3127
HTTP Responder
(Welchia,Agobot,CodeRed,Tickerbar)
NBNS Responder
(NetBIOS name requests)
port 445
NB Responder
Dameware Responder
(Agobot)
Echo Responder
(Beagle,MyDoom,Agobot)
SMB?
ports 135,1025
SMB Responder
(Welchia, Sasser, Xibo, Agobot,Randex)
RPC?
DCERPC Responder
(Welchia, Blaster, Agobot)
Figure 2: Top level Umbrella of Application Responders
request for msmsgri32.exe. (The NetrRemoteTOD command
is used to schedule the worm process to be invoked one minute
after TimeOfDay [4].) We found this attack sequence is shared
across several viruses, including the Lioten worm [4] and Agobot
variants [1].
Building responders like this one can prove difﬁcult due to
the lack of detailed documentation on services such as CIFS and
DCE/RPC. Thus, we sometimes must resort to probing an actual
Windows system running in a virtual machine environment, in or-
der to analyze the responses it makes en route to becoming infected.
We modiﬁed existing trace replay tools like flowreplay for this
purpose [11].
More generally, as new types of activities emerge over time, our
responders also need to evolve. While we ﬁnd the current pace of
maintaining the responders tractable, an important question is to
what degree we can automate the development process.
Port445
472,180 / 506,892
Negotiate_Protocol
460,630
Session_Setup
24,996
843
478
112
422,378
srvsvc
svcctl
lsarpc
4,393
epmapper
locator
13,273
82
10,161
62
52
Xi.exe
samr
(MS03-011) 
RPC Buffer Overflow)
Welchia (MS03-001) 
Locator Buffer Overflow
10,150
1543
626
644
100
msmsgri32.exe
winlord32.exe
wmmiexe.exe
Lovgate.exe
microsoft.exe
Figure 3: Example summary of port 445 activity on Class A
(500K Sessions) Arcs indicate number of sessions
3.3 Trafﬁc Analysis
Once we can engage in conversations with background radiation
sources, we then need to undertake the task of understanding the
trafﬁc. Here our approach has two components: ﬁrst, we separate
trafﬁc analysis from the responders themselves; second, we try to
analyze the trafﬁc in terms of its application-level semantics.
Regarding the ﬁrst of these, while it might appear that the job of
trafﬁc analysis can be done by the responders(since the responders
need to understand the trafﬁc anyway), there are signiﬁcant beneﬁts
to performing trafﬁc analysis independently. We do so by capturing
and storing tcpdump packet traces for later off-line analysis. This
approach allows us to preserve the complete information about the
trafﬁc and evolve our analysis algorithms over time. The ﬂip side is
that doing so poses a challenge for the analysis tool, since it needs
to do TCP stream reassembly and application-protocol parsing. To
address this issue, we built our tool on top of the Bro intrusion
detection system [26], which provides a convenient platform for
application-level protocol analysis.
We found early on that in order to ﬁlter the background radiation
trafﬁc from the “normal” trafﬁc, we need to understand the applica-
tion semantics of the trafﬁc. This is because the background radia-
tion trafﬁc has very distinctive application semantic characteristics
compared to the “normal” trafﬁc (as we will see in the following
sections), but the differences are far more difﬁcult to detect at the
network or transport level.
Our analysis has an important limitation: we do not attempt to
understand the binary code contained in buffer-overrun exploits.
This means we cannot tell for sure which worm or autorooter sent
us a particular exploit (also due to lack of a publicly available
database of worm/virus/autorooter packet traces).
If a new vari-
ant of an existing worm arises that exploits the same vulnerability,
we may not be able to discern the difference. However, the analysis
will identify a new worm if it exploits a different vulnerability, as
in the case of the Sasser worm [30].
3.4 Experimental Setup
We conducted our experiments at two different sites. These ran
two different systems, iSink and LBL Sink, which conducted the
same forms of application response but used different underlying
mechanisms.
iSink: Our iSink instance monitored background trafﬁc observed
in a Class A network (/8, 224 addresses), and two /19 subnets
(16K addresses) on two adjacent UW campus class B net-
-> SMB Negotiate Protocol Request
 SMB Session Setup AndX Request
 SMB Tree Connect AndX Request,
Path: \\XX.128.18.16\IPC$
 SMB NT Create AndX Request, Path: \samr
 DCERPC Bind: call_id: 1 UUID: SAMR
 SAMR Connect4 request
 SAMR EnumDomains request
 SAMR LookupDomain request
 SAMR OpenDomain request
 SAMR EnumDomainUsers request
Now start another session, connect to the
SRVSVC pipe and issue NetRemoteTOD
(get remote Time of Day) request
-> SMB Negotiate Protocol Request
 SMB Session Setup AndX Request
 SMB Tree Connect AndX Request,
Path: \ \XX.128.18.16\IPC$
 SMB NT Create AndX Request, Path: \srvsvc
 DCERPC Bind: call_id: 1 UUID: SRVSVC
 SRVSVC NetrRemoteTOD request
 SMB Close request
 SMB Tree Connect AndX Request, Path: \\XX.128.18.16\ADMIN$
 SMB NT Create AndX Request,
Path:\system32\msmsgri32.exe  SMB Transaction2 Request SET_FILE_INFORMATION
 SMB Transaction2 Request QUERY_FS_INFORMATION
 SMB Write Request
....
Figure 4: Active response sequence for Samr-exe viruses
Intra−Campus 
Router
1. Trace collection
2. Network Address
    Translation
3. Src−Dest Filtering
External Border
Router
unfiltered request
filtered response
NAT Filter
Campus
NAT Filter
Class A
filtered
request/response
filtered
request/response
Active Sink
Tunnel Filter
filtered
requests
Internal Border
Router
filtered
responses
1. Passive Trace collection
2. UDP/IP Encapsulation
3. Src−Dest Filtering
Honeyd Responder
(active trace collection)
iSink Setup
LBL Setup
Figure 5: The Honeynet architecture at iSink and LBL
works, respectively. Filtered packets are routed via Network
Address Translation to the Active Sink, per Figure 5. We
used two separate ﬁlters: one for the Class A network and an-
other for the two campus /19 subnets. We collected two sets
of tcpdump traces for the networks: preﬁltered traces with
of packet headers, which we use in passive measurements (of
periods during which the active responders were turned off),
and ﬁltered traces with complete payloads, which we use for
active trafﬁc analysis. The preﬁltered traces for the Class A
network are sampled at 1/10 packets to mitigate storage re-
quirements.
Site
iSink
Networks (/size)
UW-I (/19)
UW-II (/19)
Class A (/8)
LBL Sink
LBL-A (2 x 5 x /24)
LBL-P (10 x /24)
Datasets
Active
Passive
Active
Passive
Active
Passive
Active
Passive
Duration
Mar16–May14, 2004
Mar11–May14, 2004
Mar16–May14, 2004
Mar11–May14, 2004
Mar12–Mar30, 2004
Mar16–Mar30, 2004
Mar12–May14, 2004
Apr 28–May 5, 2004
Table 1: Summary of Data Collection
LBL Sink: The LBL Sink monitors two sets of 10 contiguous /24
subnets. The ﬁrst is for passive analysis; we merely listen but
do not respond, and we do not ﬁlter the trafﬁc. The second
is for active analysis. We further divide it into two halves,
5 /24 subnets each, and apply ﬁltering on these separately.
After ﬁltering, our system tunnels the trafﬁc to the active re-
sponders, as shown in Figure 5. This tunnel is one-way—the
responses are routed directly via the internal router. We use
the same set of application protocol responders at LBL as
in iSink, but they are invoked by Honeyd instead of iSink,
because Honeyd is sufﬁcient for the scale of trafﬁc at LBL
after ﬁltering. We trace active response trafﬁc at the Honeyd
host, and unless stated otherwise this comes from one of the
halves (i.e., 5 /24 subnets).
Note that the LBL and UW campus have the same /8 preﬁx, which
gives them much more locality than either has with the class A
network.
Table 1 summarizes the datasets used in our study. At each
network we collected passive tcpdump traces and ﬁltered, active-
response traces. On the two UW networks and the LBL network,
we collected two months’ worth of data. Our provisional access to
the class A enabled us to collect about two weeks of data.
The sites use two different mechanisms to forward packets to
the active responder: tunneling, and Network Address Translation
(NAT). The LBL site uses tunneling (encapsulation of IP datagrams
inside UDP datagrams), which has the advantages that: (i) it is very
straightforward to implement and (ii) it does not require extensive
state management at the forwarder. However, tunneling requires
the receive end to a) decapsulate traces before analysis, b) handle
fragmentation of full-MTU packets, and c) allocate a dedicated tun-
nel port. NAT, on the other hand, does not have these three issues,
but necessitates maintaining per-ﬂow state at the forwarder, which
can be signiﬁcant in large networks. The stateless responder de-
ployed at the UW site allows such state to be ephemeral, which
makes the approach feasible. That is we only need to maintain a
consistent ﬂow ID for each outstanding incoming packet, so the
corresponding ﬂow record at the ﬁlter can be evicted as soon as it
sees a response. Hence, the lifetime of ﬂow records is on the or-
der of milliseconds (RTT between the forwarder and active-sink)
instead of seconds.
4. PASSIVE MEASUREMENT OF
BACKGROUND RADIATION
This section presents a baseline of background radiation trafﬁc
on unused IP addresses without actively responding to any packet.
It starts with a trafﬁc breakdown by protocols and ports, and then
takes a close look at one particular facet of the trafﬁc: backscatter.
4.1 Trafﬁc Composition
A likely ﬁrst question about background radiation characteristics
is “What is the type and volume of observed trafﬁc?”. We start to
answer this question by looking at two snapshots of background
radiation trafﬁc shown in Table 2 which includes an 80 hour trace
collected at UW Campus on a /19 network from May 1 to May 4,
a one week trace at LBL collected on 10 contiguous /24 networks
from April 28 to May 5, and ﬁnally a one-week trace at Class A
with 1/10 sampling from March 11 to 18.
Protocol
TCP
ICMP
UDP
Rate
928
4.00
0.156
UW-1
LBL-P
Class A
% Rate
664
95.0%
4.2%
488
0.8% 45.2
Rate
%
56.5%
130
39.6% 0.376
3.8%
16.5
%
88.5%
0.3%
11.3%
Table 2: Trafﬁc rate breakdown by protocols. The rate is com-
puted as number of packets per destination IP address per day,
i.e., with network size and sampling rate normalized
Clearly, TCP dominates more or less in all three networks. The
relatively lower TCP rate at Class A is partly due to the artifact