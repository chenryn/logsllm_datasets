Upon every out-of-order packet arrival, an IRN receiver
sends a NACK, which carries both the cumulative acknowl-
edgment (indicating its expected sequence number) and the
sequence number of the packet that triggered the NACK (as
a simplified form of selective acknowledgement or SACK).
An IRN sender enters loss recovery mode when a NACK
is received or when a timeout occurs. It also maintains a
bitmap to track which packets have been cumulatively and
selectively acknowledged. When in the loss recovery mode,
the sender selectively retransmits lost packets as indicated by
the bitmap, instead of sending new packets. The first packet
that is retransmitted on entering loss recovery corresponds
to the cumulative acknowledgement value. Any subsequent
packet is considered lost only if another packet with a higher
sequence number has been selectively acked. When there
are no more lost packets to be retransmitted, the sender
continues to transmit new packets (if allowed by BDP-FC).
It exits loss recovery when a cumulative acknowledgement
greater than the recovery sequence is received, where the
recovery sequence corresponds to the last regular packet
that was sent before the retransmission of a lost packet.
SACKs allow efficient loss recovery only when there are
multiple packets in flight. For other cases (e.g., for single
packet messages), loss recovery gets triggered via timeouts. A
high timeout value can increase the tail latency of such short
messages. However, keeping the timeout value too small can
result in too many spurious retransmissions, affecting the
overall results. An IRN sender, therefore, uses a low timeout
value of RTOlow only when there are a small N number of
packets in flight (such that spurious retransmissions remains
negligibly small), and a higher value of RTOhiдh otherwise.
We discuss how the values of these parameters are set in §4,
and how the timeout feature in current RoCE NICs can be
easily extended to support this in §6.
3.2 IRN’s BDP-FC Mechanism
The second change we make with IRN is introducing the
notion of a basic end-to-end packet level flow control, called
BDP-FC, which bounds the number of outstanding packets
in flight for a flow by the bandwidth-delay product (BDP) of
the network, as suggested in [17]. This is a static cap that
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
R. Mittal et al.
we compute by dividing the BDP of the longest path in the
network (in bytes) 2 with the packet MTU set by the RDMA
queue-pair (typically 1KB in RoCE NICs). An IRN sender
transmits a new packet only if the number of packets in
flight (computed as the difference between current packet’s
sequence number and last acknowledged sequence number)
is less than this BDP cap.
BDP-FC improves the performance by reducing unneces-
sary queuing in the network. Furthermore, by strictly up-
per bounding the number of out-of-order packet arrivals, it
greatly reduces the amount of state required for tracking
packet losses in the NICs (discussed in more details in §6).
As mentioned before, IRN’s loss recovery has been inspired
by TCP’s loss recovery. However, rather than incorporating
the entire TCP stack as is done by iWARP NICs, IRN: (1) de-
couples loss recovery from congestion control and does not
incorporate any notion of TCP congestion window control
involving slow start, AIMD or advanced fast recovery, (2)
operates directly on RDMA segments instead of using TCP’s
byte stream abstraction, which not only avoids the complex-
ity introduced by multiple translation layers (as needed in
iWARP), but also allows IRN to simplify its selective acknowl-
edgement and loss tracking schemes. We discuss how these
changes effect performance towards the end of §4.
4 Evaluating IRN’s Transport Logic
We now confront the central question of this paper: Does
RDMA require a lossless network? If the answer is yes, then
we must address the many difficulties of PFC. If the answer
is no, then we can greatly simplify network management by
letting go of PFC. To answer this question, we evaluate the
network-wide performance of IRN’s transport logic via exten-
sive simulations. Our results show that IRN performs better
than RoCE, without requiring PFC. We test this across a wide
variety of experimental scenarios and across different perfor-
mance metrics. We end this section with a simulation-based
comparison of IRN with Resilient RoCE [34] and iWARP [33].
4.1 Experimental Settings
We begin with describing our experimental settings.
Simulator: Our simulator, obtained from a commercial NIC
vendor, extends INET/OMNET++ [1, 2] to model the Mel-
lanox ConnectX4 RoCE NIC [10]. RDMA queue-pairs (QPs)
are modelled as UDP applications with either RoCE or IRN
transport layer logic, that generate flows (as described later).
We define a flow as a unit of data transfer comprising of
one or more messages between the same source-destination
2As in [17], we expect this information to be available in a datacenter setting
with known topology and routes. IRN does not require a fully precise BDP
computation and over-estimating the BDP value would still provide the
required benefits to a large extent without under-utilizing the network.
pair as in [29, 38]. When the sender QP is ready to trans-
mit data packets, it periodically polls the MAC layer until
the link is available for transmission. The simulator imple-
ments DCQCN as implemented in the Mellanox ConnectX-4
ROCE NIC [34], and we add support for a NIC-based Timely
implementation. All switches in our simulation are input-
queued with virtual output ports, that are scheduled using
round-robin. The switches can be configured to generate
PFC frames by setting appropriate buffer thresholds.
Default Case Scenario: For our default case, we simulate
a 54-server three-tiered fat-tree topology, connected by a
fabric with full bisection-bandwidth constructed from 45 6-
port switches organized into 6 pods [16]. We consider 40Gbps
links, each with a propagation delay of 2µs, resulting in a
bandwidth-delay product (BDP) of 120KB along the longest
(6-hop) path. This corresponds to ∼110 MTU-sized packets
(assuming typical RDMA MTU of 1KB).
Each end host generates new flows with Poisson inter-
arrival times [17, 30]. Each flow’s destination is picked ran-
domly and size is drawn from a realistic heavy-tailed distri-
bution derived from [19]. Most flows are small (50% of the
flows are single packet messages with sizes ranging between
32 bytes-1KB representing small RPCs such as those gener-
ated by RDMA based key-value stores [21, 25]), and most of
the bytes are in large flows (15% of the flows are between
200KB-3MB, representing background RDMA traffic such
as storage). The network load is set at 70% utilization for
our default case. We use ECMP for load-balancing [23]. We
vary different aspects from our default scenario (including
topology size, workload pattern and link utilization) in §4.4.
Parameters: RTOhiдh is set to an estimation of the maxi-
mum round trip time with one congested link. We compute
this as the sum of the propagation delay on the longest path
and the maximum queuing delay a packet would see if the
switch buffer on a congested link is completely full. This is
approximately 320µs for our default case. For IRN, we set
RTOlow to 100µs (representing the desirable upper-bound on
tail latency for short messages) with N set to a small value
of 3. When using RoCE without PFC, we use a fixed timeout
value of RTOhiдh. We disable timeouts when PFC is enabled
to prevent spurious retransmissions. We use buffers sized at
twice the BDP of the network (which is 240KB in our default
case) for each input port [17, 18]. The PFC threshold at the
switches is set to the buffer size minus a headroom equal to
the upstream link’s bandwidth-delay product (needed to ab-
sorb all packets in flight along the link). This is 220KB for our
default case. We vary these parameters in §4.4 to show that
our results are not very sensitive to these specific choices.
When using RoCE or IRN with Timely or DCQCN, we use
the same congestion control parameters as specified in [29]
and [38] respectively. For fair comparison with PFC-based
proposals [37, 38], the flow starts at line-rate for all cases.
Revisiting Network Support for RDMA
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Figure 1: Comparing IRN and RoCE’s performance.
Figure 4: Comparing IRN and RoCE’s performance
with explicit congestion control (Timely and DCQCN).
Figure 2: Impact of enabling PFC with IRN.
Figure 3: Impact of disabling PFC with RoCE.
Metrics: We primarily look at three metrics: (i) average slow-
down, where slowdown for a flow is its completion time di-
vided by the time it would have taken to traverse its path at
line rate in an empty network, (ii) average flow completion
time (FCT), (iii) 99%ile or tail FCT. While the average and
tail FCTs are dominated by the performance of throughput-
sensitive flows, the average slowdown is dominated by the
performance of latency-sensitive short flows.
4.2 Basic Results
We now present our basic results comparing IRN and RoCE
for our default scenario. Unless otherwise specified, IRN is
always used without PFC, while RoCE is always used with
PFC for the results presented here.
4.2.1 IRN performs better than RoCE. We begin
with comparing IRN’s performance with current RoCE NIC’s.
The results are shown in Figure 1. IRN’s performance is upto
2.8-3.7× better than RoCE across the three metrics. This is
due to the combination of two factors: (i) IRN’s BDP-FC mech-
anism reduces unnecessary queuing and (ii) unlike RoCE,
IRN does not experience any congestion spreading issues,
since it does not use PFC. (explained in more details below).
4.2.2 IRN does not require PFC. We next study how
IRN’s performance is impacted by enabling PFC. If enabling
PFC with IRN does not improve performance, we can con-
clude that IRN’s loss recovery is sufficient to eliminate the
Figure 5: Impact of enabling PFC with IRN, when ex-
plicit congestion control (Timely and DCQCN) is used.
requirement for PFC. However, if enabling PFC with IRN
significantly improves performance, we would have to con-
clude that PFC continues to be important, even with IRN’s
loss recovery. Figure 2 shows the results of this comparison.
Remarkably, we find that not only is PFC not required, but
it significantly degrades IRN’s performance (increasing the
value of each metric by about 1.5-2×). This is because of the
head-of-the-line blocking and congestion spreading issues
PFC is notorious for: pauses triggered by congestion at one
link, cause queue build up and pauses at other upstream enti-
ties, creating a cascading effect. Note that, without PFC, IRN
experiences significantly high packet drops (8.5%), which
also have a negative impact on performance, since it takes
about one round trip time to detect a packet loss and another
round trip time to recover from it. However, the negative
impact of a packet drop (given efficient loss recovery), is re-
stricted to the flow that faces congestion and does not spread
to other flows, as in the case of PFC. While these PFC issues
have been observed before [23, 29, 38], we believe our work
is the first to show that a well-design loss-recovery mechanism
outweighs a lossless network.
4.2.3 RoCE requires PFC. Given the above results, the
next question one might have is whether RoCE required PFC
in the first place? Figure 3 shows the performance of RoCE
with and without PFC. We find that the use of PFC helps con-
siderably here. Disabling PFC degrades performance by 1.5-
3× across the three metrics. This is because of the go-back-N
loss recovery used by current RoCE NICs, which penalizes
performance due to (i) increased congestion caused by re-
dundant retransmissions and (ii) the time and bandwidth
wasted by flows in sending these redundant packets.
RoCEIRN05101520253035Avg Slowdown0.00.51.01.52.02.53.0Avg FCT (ms)010203040506099%ile FCT (ms)IRN with PFCIRN (without PFC)024681012141618Avg Slowdown0.00.20.40.60.81.01.21.41.6Avg FCT (ms)051015202599%ile FCT (ms)RoCE (with PFC)RoCE without PFC01020304050607080Avg. Slowdown0123456789Avg. FCT (ms)010203040506070809099%ile FCT (ms)RoCEIRN+Timely+DCQCN024681012Avg Slowdown+Timely+DCQCN0.00.51.01.52.0Avg FCT (ms)+Timely+DCQCN0102030405099%ile FCT (ms)IRN with PFCIRN (without PFC)+Timely+DCQCN0123456Avg Slowdown+Timely+DCQCN0.00.20.40.60.81.01.21.4Avg FCT (ms)+Timely+DCQCN051015202599%ile FCT (ms)SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
R. Mittal et al.
Figure 6: Impact of disabling PFC with RoCE, when ex-
plicit congestion control (Timely and DCQCN) is used.
Figure 7: The figure shows the effect of doing go-back-
N loss recovery and disabling BDP-FC with IRN. The
y-axis is capped at 3ms to better highlight the trends.
4.2.4 Effect of Explicit Congestion Control. The
previous comparisons did not use any explicit congestion
control. However, as mentioned before, RoCE today is typi-
cally deployed in conjunction with some explicit congestion
control mechanism such as Timely or DCQCN. We now eval-
uate whether using such explicit congestion control mecha-
nisms affect the key trends described above.
Figure 4 compares IRN and RoCE’s performance when
Timely or DCQCN is used. IRN continues to perform better
by up to 1.5-2.2× across the three metrics.
Figure 5 evaluates the impact of enabling PFC with IRN,
when Timely or DCQCN is used. We find that, IRN’s perfor-
mance is largely unaffected by PFC, since explicit congestion
control reduces both the packet drop rate as well as the
number of pause frames generated. The largest performance
improvement due to enabling PFC was less than 1%, while
its largest negative impact was about 3.4%.
Finally, Figure 6 compares RoCE’s performance with and
without PFC, when Timely or DCQCN is used.3 We find that,
unlike IRN, RoCE (with its inefficient go-back-N loss recov-
ery) requires PFC, even when explicit congestion control is
used. Enabling PFC improves RoCE’s performance by 1.35×
to 3.5× across the three metrics.
Key Takeaways: The following are, therefore, the three
takeaways from these results: (1) IRN (without PFC) performs
better than RoCE (with PFC), (2) IRN does not require PFC,
and (3) RoCE requires PFC.
4.3 Factor Analysis of IRN
We now perform a factor analaysis of IRN, to individually
study the significance of the two key changes IRN makes to
RoCE, namely (1) efficient loss recovery and (2) BDP-FC. For
this we compare IRN’s performance (as evaluated in §4.2)
with two different variations that highlight the significance
of each change: (1) enabling go-back-N loss recovery instead
of using SACKs, and (2) disabling BDP-FC. Figure 7 shows
the resulting average FCTs (we saw similar trends with other
metrics). We discuss these results in greater details below.
3RoCE + DCQCN without PFC presented in Figure 6 is equivalent to Re-
silient RoCE [34]. We provide a direct comparison of IRN with Resilient
RoCE later in this section.
Need for Efficient Loss Recovery: The first two bars in
Figure 7 compare the average FCT of default SACK-based IRN
and IRN with go-back-N respectively. We find that the latter
results in significantly worse performance. This is because
of the bandwidth wasted by go-back-N due to redundant
retransmissions, as described before.
Before converging to IRN’s current loss recovery mecha-
nism, we experimented with alternative designs. In particular
we explored the following questions:
(1) Can go-back-N be made more efficient? Go-back-N does
have the advantage of simplicity over selective retransmis-
sion, since it allows the receiver to simply discard out-of-
order packets. We, therefore, tried to explore whether we
can mitigate the negative effects of go-back-N. We found
that explicitly backing off on losses improved go-back-N per-