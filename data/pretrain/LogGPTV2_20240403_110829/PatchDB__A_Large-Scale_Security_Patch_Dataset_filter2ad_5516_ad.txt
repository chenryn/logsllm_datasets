belong to the wild-based dataset. We also get a cleaned non-
security patch dataset of 23,742 instances.
B. Performance of Nearest Link Search Method (RQ2)
To evaluate the performance of our nearest
link search
method, we compare it with three other data augmentation
methods including:
• Brute force search: directly screening security patches
from the wild.
• Pseudo labeling [19]: locating candidates from prediction
results of single machine learning model with the highest
conﬁdence.
• Uncertainty-based labeling [28]:
locating candidates
from prediction results of multiple machine learning
classiﬁers with the highest certainty (i.e., consensus).
Table III summarizes comparative evaluation results (i.e.,
the percentage of security patches and the conﬁdence interval)
under the 95% conﬁdence level. Given the same training
dataset (i.e., the NVD-based dataset with 4076 security patches
and a non-security patch dataset of 8352 instances), we com-
pare the performance of these four methods on recognizing
security patch candidates from an unlabeled dataset of 200K
random GitHub commits. For the brute force search, instead
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:12 UTC from IEEE Xplore.  Restrictions apply. 
155
TABLE III: Comparison with other augmentation methods.
TABLE IV: Performance w/o or w/ synthetic patches.
Unlabeled
Patches
Candidates
Methods
Brute Force Search
Pseudo Labeling
Security
Patches(cid:2) (%)
8(±1.7)%
13(±1.8)%
29(±2.4)%
(cid:2) Sampled results based on 1K of candidates and 95% conﬁdence level
Uncertainty-based Labeling
Nearest Link Search (ours)
200K
4K
1K
4K
200K
12%
of manually verifying each of the 200K instances, we verify
a random subset of 1K instances. The percentage of security
patches is around 8%. For the pseudo labeling, we use the
NVD-based dataset to train a model with the same features
adopted in our nearest link search method. Among popular
machine learning algorithms, we choose the Random Forest
classiﬁer that performs the best to rank the security patch
candidates according to their conﬁdence values and then select
the top 4076 candidates. In the 1K subset of top candidates,
about 13% of them are manually veriﬁed as security patches.
When conducting the uncertainty-based labeling, we imple-
ment ten classiﬁers including Random Forest, Support Vector
Machine (SVM), Logistic Regression, Stochastic Gradient
Descent (SGD) classiﬁer, Sequential Minimal Optimization
(SMO) classiﬁer, Naive Bayes, Bayesian Network, J48 Deci-
sion Tree, Reduced Error Pruning Tree (REPTree), and Voted
Perceptron using Weka [14]. An unlabeled GitHub commit is
regarded as a candidate only if all ten classiﬁers predict it as a
security patch. When applying the uncertain-based ensemble
model, 1174 instances out of the 200K unlabeled data are
predicted as security patches by all ten classiﬁers. Our manual
checks show that 12% of 1174 instances are security patches.
For our nearest link search method, we manually verify 1K
instances from 4076 candidates. We ﬁnd around 29% instances
are security patches. We further investigate the reasons that our
method outperforms both the pseudo labeling method and the
uncertainty-based labeling method. The main reason is that
the distribution of security patches in the wild is different
from that in the NVD-based dataset, which may be biased to
certain types of vulnerabilities or popular software [20], [32].
Therefore, the models trained by the NVD-based dataset would
not be able to well proﬁle patches in the wild. In contrast,
our nearest link search mainly targets at local distribution,
i.e., ﬁnding the nearest neighboring instances of the existing
dataset. Therefore, it has more tolerance on the difference
between the NVD-based dataset and the wild dataset.
With the same amount of human labor (e.g., manually
checking 1K subset), our experiments show that our nearest
link search can help identify more security patches. Since there
are hundreds of million repositories on GitHub, the number
of unlabeled patches (i.e., commits) will be huge. Therefore,
it is promising to construct an even larger wild-based security
patch dataset by repeating the data augmentation process with
more human efforts.
C. Evaluation of Synthetic Security Patches (RQ3)
To ﬁgure out if synthetic patches are useful and in which
condition they are useful, we apply our oversampling tech-
Dataset
NVD
NVD
Synthetic Dataset
Precision
Recall
-
17K Sec. + 20K NonSec.
82.1%
86.0% (+3.9%) 87.2% (+2.4%)
84.8%
NVD+Wild
NVD+Wild 58K Sec. + 129K NonSec. 93.0% (+0.1%) 61.2% (+0.1%)
92.9%
61.1%
-
Sec. = security patch; NonSec. = non-security patch
nique on the NVD-based dataset and the wild-based dataset,
respectively. For the NVD-based dataset, we create 16,836
artiﬁcial security patches and 19,936 artiﬁcial non-security
ones. For the wild-based dataset, we generate a synthetic
dataset containing 57,724 security patches and 128,736 non-
security ones. We apply them into a task of automatic security
patch identiﬁcation and evaluate if the performance can be
improved by including the corresponding synthetic dataset.
When only employing the NVD-based dataset, we randomly
select 80% as the training set and use the remaining 20% as
the testing set. When applying the NVD-based dataset and its
synthetic dataset, we add all the synthetic data to the previous
training set to train the model and conduct inference on the
previous testing set for a fair comparison. We follow the same
way to split and allocate the data for the NVD+wild dataset
itself and the NVD+wild dataset along with its synthetic
dataset. Note that our synthetic patches are generated solely
based on the training set in each experiment. In our evaluation,
we adopt the recurrent neural network (RNN) algorithm [30],
which considers the source code of a given patch as a list of
tokens including keywords, identiﬁers, operators, etc. In the
RNN model, the current state depends on the current inputs
and the previous state so that the model can learn the context
information from tokens.
Comparative results are shown in Table IV. When solely
depending on the NVD-based dataset, the classiﬁcation pre-
cision of the RNN model is 82.1% with a recall of 84.8%.
After adding the synthetic data into the NVD-based dataset,
the classiﬁcation precision increases by 3.9% (to 86.0%), and
the F1 score can increase by 2.4% (to 87.2%). However, for the
natural dataset containing both the NVD-based dataset and the
wild-based dataset, we do not observe obvious improvement
after adding the synthetic data. The model trained with the
natural dataset only achieves 92.9% precision with the recall
of 61.1%. After adding the synthetic dataset, there is only a
slight increase, i.e., 0.1% in precision and 0.1% in recall. Note
that recall of the last two rows are lower than the ﬁrst two rows
since the NVD+wild test dataset involves wild patches. We also
try some traditional oversampling techniques like SMOTE and
do not observe obvious performance increase.
The experimental results show that our oversampling tech-
nique is effective in the security patch identiﬁcation task if we
only have a small dataset (i.e., the NVD-based dataset). When
we have a larger dataset (i.e., the combination of the NVD-
based dataset and wild-based dataset), the synthetic data can-
not lead to distinct improvement. The results are reasonable.
Since the small dataset contains a limited number and patterns
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:12 UTC from IEEE Xplore.  Restrictions apply. 
156
TABLE V: Security patch distribution in PatchDB.
Type of patch pattern
add or change bound checks
add or change null checks
add or change other sanity checks
change variable deﬁnitions
change variable values
change function declarations
change function parameters
add or change function calls
add or change jump statements
ID
1
2
3
4
5
6
7
8
9
10 move statements without modiﬁcation
11
12
add or change functions (redesign)
others
%(cid:2)
10.8%
9.1%
18.0%
4.8%
9.1%
1.8%
2.6%
24.4%
1.7%
5.0%
12.0%
0.8%
(cid:2) Sampled results based on 1K patches.
of patches, it cannot fully represent the feature space. In this
case, the synthetic patches increase the number of control ﬂow
complexity as well as enrich the feature representations. As a
result, the oversampling technique can boost the generalization
ability of the trained model. In contrast, a larger security patch
dataset may have already contained enough patch samples
of various patterns so that the synthesis technique can only
provide a marginal increase.
D. Dataset Composition (RQ4)
To ﬁgure out the composition of the PatchDB, we analyze
it from the perspective of patch patterns. According to the
deﬁnition of patch patterns in previous studies [35], [38], [41],
we classify these security patches into 12 categories in Table V
in terms of their code changes. The ﬁrst three types that add
sanity checks are common in the security patches since they
directly block unsafe inputs. Given the fact that the bound and
NULL pointer are the most frequent items in the sanity check,
we consider them separately. Type 4 includes changing the
data type from int to unsigned int, resizing a buffer, etc.
Type 5 changes variable values, e.g., initialize memory to zero
for preventing information leak. Type 6, 7, and 8 are related to
ﬁxing vulnerable functions and their parameters. Among them,
Type 8 is the most common one (e.g., replacing an unsafe
C library function strcpy with strlcpy, adding the lock
and unlock before and after a raced operation, and calling
release functions to avoid information leak). Type 9 adds or
modiﬁes the jump statements for the vulnerabilities that lack
proper error handling. Type 10 moves some statements from
one place to another with little or no modiﬁcations. Such
ﬁxes are usually for uninitialized use, use-after-free, etc. Type
11 rewrites the function logic with lots of different program
changes. Type 12 refers to some uncommon minor changes
that cannot be categorized into any of the above types.
From Table V, we can ﬁnd that Type 8 is the most frequent
class in the PatchDB. Type 1, 3, and 8 (i.e., several kinds of
sanity checks and function call modiﬁcations) compose more
than half of the PatchDB. Intuitively, given a security patch in
the NVD-based dataset, the nearest link search method locates
its nearest instance in the feature space as the candidate for
dataset augmentation. Therefore, we wonder if the nearest link
search changes the type distribution. In other words, facilitated
 35
 30
 25
 20
)
%
(
n
o
i
t
r
o
p
o
r
P
 15
 10
 5
 0
type-11
type-3
type-8
type-2
type-1
NVD-based dataset
type-5
type-4
type-7
type-6
type-10
type-9
wild-based dataset
type-12
Fig. 6: Distribution comparison between NVD-based and
wild-based datasets in terms of code changes.
with the nearest link search, is the type distribution of the wild-
based dataset the same as that of the NVD-based dataset?
To answer this question, we manually classify a random
subset according to the patch patterns for the NVD-based and
wild-based dataset, respectively. The results in Figure 6 show
that the wild-based dataset identiﬁed by the nearest link search
differs from the original NVD-based dataset with regards to
the type distribution. The type allocation of the NVD-based
dataset conforms to a long tail distribution [7], where 3 out of
the 12 types consist of around 60% of the NVD-based dataset,
and most of the other 9 types are under 5%. In contrast, the
type distribution of the wild-based dataset is largely different
from the NVD-based dataset. Previous head class Type 11
only accounts for around 5% of the wild-based dataset. Type
8 becomes the ﬁrst head class. Also, the ranks of previous tail
classes are mostly changed. Thus, generated by the nearest
link search, the wild-based dataset exhibits a different type
distribution from the NVD-based dataset.
Although each security patch located by the nearest link
search is the nearest neighbor of a security patch in the NVD-
based dataset,
their similarity may only be in the feature
space, and these features are not one-to-one corresponding
with security patch types. Between security and non-security
patches, given a security patch, the nearest link search ﬁnds
a similar instance that is also a security patch; however, they
may not belong to the same security patch type. Therefore, the
wild-based dataset identiﬁed by the nearest link search can
have a dissimilar distribution from the NVD-based dataset.
Meanwhile, such differences bring some beneﬁts. The main
problem of the long tail distribution is that there is not enough
data for tail classes. In that case, machine learning would not
perform well when handling those minority instances. The
wild-based dataset solves this problem to a certain extent by
introducing more varieties to the PatchDB.
During this analysis, we also manually classify a total
number of 5K security patches in PatchDB into these patch
patterns. We will also make these materials public for further
research. More potential use will be discussed in Section V.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:12 UTC from IEEE Xplore.  Restrictions apply. 
157
TABLE VI: Impacts of datasets over learning-based models
We also discuss some limitations as well as future work.
Training Dataset
Algorithm
Test Dataset
NVD
NVD+Wild
Random Forest
RNN
Random Forest
RNN
NVD
Wild
NVD
Wild
NVD
Wild
NVD
Wild
Precision Recall
21.7%
58.4%
19.5%
58.0%
83.2%
82.8%
88.3%
24.2%
22.5%
90.1%
44.6%
91.8%
60.2%
92.8%
92.3%
63.2%
E. Quality of the PatchDB (RQ5)
To evaluate the quality of our collected dataset, we employ
PatchDB in the task of automatic security patch identiﬁcation.
More speciﬁcally, we use both the NVD-based and wild-based
dataset to train a learning-based model that identiﬁes if a given
patch is security-related. Then, we compare its performance
with the one trained by only the NVD-based dataset. In our
experiments, two machine learning models are implemented.
One is the random forest classiﬁer with the statistical features
of patches, and the other is the recurrent neural network
classiﬁer that can extract context information from tokens.
The experimental setup and results are shown in Table
VI. For the NVD-based dataset, we randomly choose 80%
instances as the training set and the remaining 20% as the test
set. The wild-based dataset is split in the same way. Then,
for a fair comparison, we combine the training set of both the
NVD-based and wild-based dataset as the training data. When
using the NVD-based test dataset, the Random Forest (and