intelligent monitoring systems, healthcare, etc. For instance, in the
scenario of home security monitoring, sound event detection can
analyze whether there is a stranger break-in through audio signal,
to compensate for the blind area of surveillance cameras.
However, in the real world, the audio signal usually contains
sound from multiple sources simultaneously and is distorted by
environmental noise. Therefore, it is nontrivial to accurately and
efficiently extract the sound of interest from the information-rich
audio to identify the event. Similar to speaker verification, existing
works on sound event detection focus on optimizing the machine
learning models to improve the detection performance, rather than
decoupling the event-related and event-unrated features [26, 29].
Hence, the framework of SEEF-ALDR can directly help improve the
performance of the audio event detection task. In particular, the
eliminating encoder can be extended to remove the event-related
features via adversarial learning, while the purifying encoder pre-
serves most of the event-related information for classification. The
decoder collects the output from both the purifying encoder and the
eliminating encoder to reconstruct the spectrogram, which is made
to resemble the original input spectrogram as much as possible.
We plan to extend our SEEF-ALDR to the domain of sound event
detection and make it a general approach for any task that benefits
from feature decoupling of the original input.
5 DISCUSSION
Novel identity impersonation attack based on SEEF-ALDR.
Interestingly, a novel attack against speaker identity verification
can be possible via the SEEF-ALDR framework based on voice con-
version. As discussed above, the speaker identity feature from the
purifying encoder determines the identity of the speaker, while the
identity-unrelated feature from the eliminating encoder contains
6 CONCLUSION
Speaker verification has been used in various user authentication
scenarios, so its dependability greatly impacts users’ privacy and
overall system security. In this paper, we propose SEEF-ALDR, a
novel speaker embedding enhancement framework via adversar-
ial learning based disentangled representation, to decouple the
SEEF-ALDR: A Speaker Embedding Enhancement Framework via Adversarial Learning based Disentangled Representation
ACSAC 2020, December 7–11, 2020, Austin, USA
speaker identity features and the identity-unrelated ones from orig-
inal speech. SEEF-ALDR is built based on twin networks integrating
an autoencoder-like architecture and adversarial learning approach.
The twin networks contain the speaker eliminating encoder and
the speaker purifying encoder, with the former erasing speaker
identity features and the latter extracting the speaker identity fea-
tures. In this way, by combining adversarial supervision signal
and reconstruction supervision signal, our framework can natu-
rally learn complementary feature representations of the original
speech, which means the fusion of the decoupled features is close
enough to the original one. Experimental results demonstrate that,
as an optimization strategy, SEEF-ALDR can construct more accu-
rate speaker embeddings for existing speaker verification models
to improve the performance of “in-the-wild” speaker verification
with little effort.
ACKNOWLEDGMENTS
This work was partially supported by the National Key Research
and Development Program of China (2016QY04W0903), Beijing
Municipal Science and Technology Project (Z191100007119010),
Strategic Priority Research Program of Chinese Academy of Sci-
ences (No. XDC02010900) and National Natural Science Foundation
of China (NO.61772078). Any opinions, findings, and conclusions
or recommendations expressed in this material are those of the
authors and do not necessarily reflect the views of any funding
agencies.
REFERENCES
[1] Gautam Bhattacharya, Md Jahangir Alam, and Patrick Kenny. 2017. Deep Speaker
Embeddings for Short-Duration Speaker Verification.. In Interspeech. 1517–1521.
[2] Gautam Bhattacharya, Joao Monteiro, Jahangir Alam, and Patrick Kenny. 2019.
Generative adversarial speaker embedding networks for domain robust end-to-
end speaker verification. In ICASSP 2019-2019 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP). IEEE, 6226–6230.
[3] Weicheng Cai, Jinkun Chen, and Ming Li. 2018. Analysis of length normalization
in end-to-end speaker verification system. arXiv preprint arXiv:1806.03209 (2018).
[4] Weicheng Cai, Jinkun Chen, and Ming Li. 2018. Exploring the Encoding Layer
and Loss Function in End-to-End Speaker and Language Recognition System.. In
Odyssey 2018 The Speaker and Language Recognition Workshop. 74–81.
[5] Si Chen, Kui Ren, Sixu Piao, Cong Wang, Qian Wang, Jian Weng, Lu Su, and
Aziz Mohaisen. 2017. You Can Hear But You Cannot Steal: Defending Against
Voice Impersonation Attacks on Smartphones. In 2017 IEEE 37th International
Conference on Distributed Computing Systems (ICDCS). 183–195.
[6] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. 2018. VoxCeleb2: Deep
Speaker Recognition.. In Interspeech 2018. 1086–1090.
[7] J. Daugman. 2004. How iris recognition works. IEEE Transactions on Circuits and
Systems for Video Technology 14, 1 (2004), 21–30.
[8] Peter French. 2013. An overview of forensic phonetics with particular reference
to speaker identification. International Journal of Speech Language and The Law
1, 2 (2013), 169–181.
[9] Yixiao Ge, Zhuowan Li, Haiyu Zhao, Guojun Yin, Shuai Yi, Xiaogang Wang,
et al. 2018. FD-GAN: Pose-guided feature distilling GAN for robust person re-
identification. In Advances in Neural Information Processing Systems. 1222–1233.
[10] Erica Gold and Peter French. 2011. International practices in forensic speaker
comparison. International Journal of Speech Language and The Law 18, 2 (2011),
293–307.
[11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adversarial
Nets. In Advances in Neural Information Processing Systems 27. 2672–2680.
[12] Alex Graves, Abdel rahman Mohamed, and Geoffrey Hinton. 2013. Speech
recognition with deep recurrent neural networks. In 2013 IEEE International
Conference on Acoustics, Speech and Signal Processing. 6645–6649.
[13] Craig S. Greenberg, Vincent M. Stanford, Alvin F. Martin, Meghana Yadagiri,
George R. Doddington, John J. Godfrey, and Jaime Hernandez-Cordero. 2013.
The 2012 NIST speaker recognition evaluation.. In INTERSPEECH. 1971–1975.
[14] Mahdi Hajibabaei and Dengxin Dai. 2018. Unified Hypersphere Embedding for
Speaker Recognition. arXiv preprint arXiv:1807.08312 (2018).
[19] Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006. Reducing the dimensional-
[15] Awni Y. Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich
Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, and
Andrew Y. Ng. 2014. Deep Speech: Scaling up end-to-end speech recognition.
arXiv preprint arXiv:1412.5567 (2014).
[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770–778.
[17] John R. Hershey, Zhuo Chen, Jonathan Le Roux, and Shinji Watanabe. 2016. Deep
clustering: Discriminative embeddings for segmentation and separation. In 2016
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).
31–35.
[18] G. Hinton, Li Deng, Dong Yu, G. E. Dahl, A. Mohamed, N. Jaitly, Andrew Senior,
V. Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury. 2012. Deep Neural
Networks for Acoustic Modeling in Speech Recognition: The Shared Views of
Four Research Groups. IEEE Signal Processing Magazine 29, 6 (2012), 82–97.
ity of data with neural networks. science 313, 5786 (2006), 504–507.
[20] Geoffrey E Hinton and Richard S Zemel. 1994. Autoencoders, minimum de-
scription length and Helmholtz free energy. In Advances in neural information
processing systems. 3–10.
[21] I Hsu, Ayush Jaiswal, Premkumar Natarajan, et al. 2019. Niesr: Nuisance invariant
end-to-end speech recognition. arXiv preprint arXiv:1907.03233 (2019).
[22] Wei-Ning Hsu, Yu Zhang, and James Glass. 2017. Unsupervised learning of
disentangled and interpretable representations from sequential data. In Advances
in neural information processing systems. 1878–1889.
[23] Rui Huang, Shu Zhang, Tianyu Li, and Ran He. 2017. Beyond face rotation: Global
and local perception gan for photorealistic and identity preserving frontal view
synthesis. In Proceedings of the IEEE International Conference on Computer Vision.
2439–2448.
[24] Xuedong Huang, Fileno Alleva, Hsiao Hon, Mei Hwang, and Ronald Rosenfeld.
1992. The SPHINX-II Speech Recognition System: An Overview. Computer Speech
& Language 7, 2 (1992), 137–148.
[25] Arindam Jati, Raghuveer Peri, Monisankha Pal, Tae Jin Park, Naveen Kumar,
Ruchir Travadi, Panayiotis Georgiou, and Shrikanth Narayanan. 2019. Multi-task
Discriminative Training of Hybrid DNN-TVM Model for Speaker Verification
with Noisy and Far-Field Speech. In proceedings of Proceedings of Interspeech
(2019).
[26] Chieh-Chi Kao, Weiran Wang, Ming Sun, and Chao Wang. 2018. R-CRNN: Region-
based Convolutional Recurrent Neural Network for Audio Event Detection.. In
Interspeech 2018. 1358–1362.
[27] Insoo Kim, Kyuhong Kim, Jiwhan Kim, and Changkyu Choi. 2019. Deep Speaker
Representation Using Orthogonal Decomposition and Recombination for Speaker
Verification. In ICASSP 2019-2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP). IEEE, 6126–6130.
[28] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes.
arXiv preprint arXiv:1312.6114 (2013).
[29] Anurag Kumar and Bhiksha Raj. 2016. Audio Event Detection using Weakly La-
beled Data. In Proceedings of the 24th ACM international conference on Multimedia.
1038–1047.
[30] Guillaume Lample, Neil Zeghidour, Nicolas Usunier, Antoine Bordes, Ludovic
Denoyer, and Marc’Aurelio Ranzato. 2017. Fader networks: Manipulating images
by sliding attributes. In Advances in Neural Information Processing Systems. 5967–
5976.
[31] Hung-Shin Lee, Yu-Ding Lu, Chin-Cheng Hsu, Yu Tsao, Hsin-Min Wang, and
Shyh-Kang Jeng. 2017. Discriminative autoencoders for speaker verification.
In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP). 5375–5379.
[32] Chao Li, Xiaokong Ma, Bing Jiang, Xiangang Li, Xuewei Zhang, Xiao Liu, Ying
Cao, Ajay Kannan, and Zhenyao Zhu. 2017. Deep Speaker: an End-to-End Neural
Speaker Embedding System. arXiv preprint arXiv:1705.02304 (2017).
facial attributes. arXiv preprint arXiv:1610.05586 (2016).
[34] LinWei-wei, MakMan-Wai, and ChienJen-Tzung. 2018. Multisource I-Vectors
Domain Adaptation Using Maximum Mean Discrepancy Based Autoencoders.
IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP) (2018).
[35] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. 2017.
SphereFace: Deep Hypersphere Embedding for Face Recognition. In 2017 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR). 6738–6746.
[36] Yi Liu, Liang He, and Jia Liu. 2019. Large margin softmax loss for speaker
verification. arXiv preprint arXiv:1904.03479 (2019).
[37] Yu Liu, Fangyin Wei, Jing Shao, Lu Sheng, Junjie Yan, and Xiaogang Wang. 2018.
Exploring disentangled feature representation beyond face identification. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
2080–2089.
[38] Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard Zemel. 2015.
[33] Mu Li, Wangmeng Zuo, and David Zhang. 2016. Deep identity-aware transfer of
[39] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
The variational fair autoencoder. arXiv preprint arXiv:1511.00830 (2015).
Journal of machine learning research 9, Nov (2008), 2579–2605.
ACSAC 2020, December 7–11, 2020, Austin, USA
Jianwei and Xiaoqi, et al.
[67] Jennifer Williams and Simon King. 2019. Disentangling Style Factors from
[70] Li Yingzhen and Stephan Mandt. 2018. Disentangled sequential autoencoder. In
Speaker Representations. In Proc. Interspeech, Vol. 2019. 3945–3949.
[68] Weidi Xie, Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. 2019.
Utterance-level Aggregation for Speaker Recognition in the Wild. In ICASSP
2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Process-
ing (ICASSP). 5791–5795.
[69] Sarthak Yadav and Atul Rai. 2018. Learning Discriminative Features for Speaker
Identification and Verification.. In Interspeech 2018. 2237–2241.
International Conference on Machine Learning. 5670–5679.
[71] Chunlei Zhang and Kazuhito Koishida. 2017. End-to-End Text-Independent
Speaker Verification with Triplet Loss on Short Utterances.. In Interspeech. 1487–
1491.
[72] Jianfeng Zhou, Tao Jiang, Lin Li, Qingyang Hong, Zhe Wang, and Bingyin Xia.
2019. Training multi-task adversarial network for extracting noise-robust speaker
embedding. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP). IEEE, 6196–6200.
[40] Davide Maltoni, Dario Maio, Anil K. Jain, and Salil Prabhakar. 2005. Handbook of
Fingerprint Recognition.
[41] Michael F Mathieu, Junbo Jake Zhao, Junbo Zhao, Aditya Ramesh, Pablo Sprech-
mann, and Yann LeCun. 2016. Disentangling factors of variation in deep repre-
sentation using adversarial training. In Advances in neural information processing
systems. 5040–5048.
[42] Zhong Meng, Yong Zhao, Jinyu Li, and Yifan Gong. 2019. Adversarial speaker
verification. In ICASSP 2019-2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP). IEEE, 6216–6220.
[43] M.G. [n. d.]. Transparency Market Research. https://www.researchmoz.us/
publisher..
[44] Lindasalwa Muda, Mumtaj Begam, and I. Elamvazuthi. 2010. Voice Recognition
Algorithms using Mel Frequency Cepstral Coefficient (MFCC) and Dynamic Time
Warping (DTW) Techniques. arXiv preprint arXiv:1003.4083 (2010).
[45] Arsha Nagrani, Joon Son Chung, Weidi Xie, and Andrew Zisserman. 2020. Vox-
celeb: Large-scale speaker verification in the wild. Computer Speech & Language
60 (2020), 101027.
[46] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. 2017. VoxCeleb: A
[47] Koji Okabe, Takafumi Koshinaka, and Koichi Shinoda. 2018. Attentive statistics
Large-Scale Speaker Identification Dataset.. In Interspeech 2017. 2616–2620.
pooling for deep speaker embedding. arXiv preprint arXiv:1803.10963 (2018).
[48] Thomas Pellegrini and Leo Cances. 2019. Cosine-similarity penalty to discrimi-
nate sound classes in weakly-supervised sound event detection. In 2019 Interna-
tional Joint Conference on Neural Networks (IJCNN). 1–8.
[49] Raghuveer Peri, Monisankha Pal, Arindam Jati, Krishna Somandepalli, and
Shrikanth Narayanan. 2019. Robust speaker recognition using unsupervised
adversarial invariance. arXiv preprint arXiv:1911.00940 (2019).
[50] P.J. Phillips, Hyeonjoon Moon, S.A. Rizvi, and P.J. Rauss. 2000. The FERET
evaluation methodology for face-recognition algorithms. IEEE Transactions on
Pattern Analysis and Machine Intelligence 22, 10 (2000), 1090–1104.
[51] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek,
Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz,
Jan Silovsky, Georg Stemmer, and Karel Vesely. 2011. The Kaldi Speech Recogni-
tion Toolkit. In IEEE 2011 Workshop on Automatic Speech Recognition and Under-
standing.
[52] Simon JD Prince and James H Elder. 2007. Probabilistic linear discriminant
analysis for inferences about identity. In 2007 IEEE 11th International Conference
on Computer Vision. IEEE, 1–8.
[53] Alec Radford, Luke Metz, and Soumith Chintala. 2016. Unsupervised Represen-
tation Learning with Deep Convolutional Generative Adversarial Networks. In
ICLR 2016 : International Conference on Learning Representations 2016.
[54] FA Rezaur rahman Chowdhury, Quan Wang, Ignacio Lopez Moreno, and Li Wan.
2018. Attention-based models for text-dependent speaker verification. In 2018
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 5359–5363.
[55] Desh Raj, David Snyder, Daniel Povey, and Sanjeev Khudanpur. 2019. Probing
the Information Encoded in x-vectors. arXiv preprint arXiv:1909.06351 (2019).
[56] Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio.
2011. Contractive auto-encoders: Explicit invariance during feature extraction.
(2011).
[57] Suwon Shon, Hao Tang, and James Glass. 2018. Frame-level speaker embeddings
for text-independent speaker recognition and analysis of end-to-end model. In
2018 IEEE Spoken Language Technology Workshop (SLT). IEEE, 1007–1013.
[58] David Snyder, Daniel Garcia-Romero, Daniel Povey, and Sanjeev Khudanpur. 2017.
Deep Neural Network Embeddings for Text-Independent Speaker Verification..
In Interspeech. 999–1003.
[59] David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev
Khudanpur. 2018. X-vectors: Robust dnn embeddings for speaker recognition.
In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP). IEEE, 5329–5333.
[60] T.B. [n. d.]. Voiceprint. https://www.tdbank.com/bank/tdvoiceprint.html..
[61] Kar-Ann Toh, Jaihie Kim, and Sangyoun Lee. 2008. Equal Error Rate Minimization
for Biometrics Fusion. ICEIC : International Conference on Electronics, Informations
and Communications (2008), 513–516.
[62] Luan Tran, Xi Yin, and Xiaoming Liu. 2017. Disentangled representation learning
gan for pose-invariant face recognition. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition. 1415–1424.
[63] Youzhi Tu, Man-Wai Mak, and Jen-Tzung Chien. 2019. Variational domain adver-
sarial learning for speaker verification. Proc. Interspeech 2019 (2019), 4315–4319.
[64] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol.
2008. Extracting and composing robust features with denoising autoencoders. In
Proceedings of the 25th international conference on Machine learning. 1096–1103.
[65] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez Moreno. 2018. Generalized
end-to-end loss for speaker verification. In 2018 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP). IEEE, 4879–4883.
[66] Matt Warman. 2013. Say goodbye to the pin: voice recognition takes over at
Barclays Wealth. https://www.telegraph.co.uk/technology/news/10044493/Say-
goodbye-to-the-pin-voice-recognition-takes-over-at-Barclays-Wealth.html.