vation to simplify the median utility deﬁnition and evaluate it
only for elements in D instead of the entire universe U.
Deﬁnition 6 (Median utility function). Let data set D ∈ U n
be sorted. The median utility function utility : ID → Z scores
the utility of an element of D at position i ∈ ID as
(cid:26)i − n
n
2 − i
utility(i) =
2 + 1
if i  0 indicates the number of
consecutive elements in U with same utility, and weight(i) ·
gap(i) is their unnormalized probability mass. Thus, mass[i]
equals the sum of unnormalized probabilities for elements in
O = {min(U), . . . , di}, and mass[i]/R equals normalized
probabilities(cid:80)
o∈O Pr[M
u(D) = o].
An example for utility and gap can be found in Table I.
It illustrates that utility for sorted D is just a sequence that
ﬁrst increases, then decreases after the median. As mentioned
above, we add min(U) to the beginning and max(U) to the
end of D (highlighted in light gray in Table I). The utility for
“missing elements” in U\D (dark gray columns) is the same
as for the preceding or succeeding element in D. Furthermore,
gap is zero for the duplicates furthest away from the median
and otherwise indicates the number of consecutive elements in
U with the same utility (e.g., gap(2) = 4 as 2, 3, 4, 5 have the
same utility as d2 = 2).
C. Median Sampling
We use inverse transform sampling to sample the dif-
ferentially private median from the cumulative distribution
function mass by ﬁnding an index j ∈ ID
6 such that
mass(j − 1) ≤ r  max(Ds)} = U\R. Note
that R contains the universe elements closest to the median.
Deﬁnition 10 (Accuracy). Let u = umed, then accuracy is
A, Ds
A, Ds
pR = 1 − pP =
Pr[M
u(Ds) = x],
(cid:88)
x∈R
i.e., pR is the probability mass of all remaining elements.
6For notational convenience let j − 1  0.5 it is more likely to select the
differentially private median among R than among P. In our
evaluation we use accuracy pR = 0.9999. The number of
pruning steps s enables a trade-off between accuracy pR and
computation complexity: smaller s leads to higher accuracy
and larger s translates into smaller input size for the secure
computation. We are interested in the maximum number of
pruning steps such that it is more likely to select an element
from R instead of P.
Theorem 3 (Upper Bound for Pruning Steps). Let D be a
data set with data universe U,  > 0, and 0 < α < 1. The
upper bound for pruning steps s fulﬁlling pR ≥ α is
(cid:98)log2(n) − log2
(|U| − 1)
− 1(cid:99).
(cid:18)
(cid:18) α
loge
1 − α
(cid:19)(cid:19)
Proof: We ﬁnd the maximum number of pruning steps
s by examining what the maximum probability mass pP for
pruned elements can be.
First, note that the utility for all x ∈ P is the same
independent of the values in Ds: Half of the values in P
are smaller (resp., larger) than the median m of Ds, i.e.,
rankDs(x) = 0 if x < m and rankDs (x) = |Ds| otherwise.
Thus, umed(Ds, x) = −(cid:12)(cid:12)(cid:12)0 − |Ds|
(cid:12)(cid:12)(cid:12) = −(cid:12)(cid:12)(cid:12)|Ds| − |Ds|
(cid:12)(cid:12)(cid:12) = − n
2s+1
2s . (Recall that D is padded before pruning such
since |Ds| = n
that n is a power of two.)
2
2
As the utility, and thus selection probability, is the same
for all elements in P the probability mass pP is maximized if
|P| is maximized. The maximum for |P| is |U|− 1 as R must
contain at least one element, the median m.
Let p(cid:48)
R, p(cid:48)
P be the unnormalized probability masses pR, pP
respectively, then
p(cid:48)
R = exp(umed(Ds, m)) = 1
since R = {m} and umed(Ds, m) = 0, and
n
P = (|U| − 1) exp
p(cid:48)
(cid:16)−
(cid:17)
2s+1
α(|U| − 1)
=
2s+1
⇔ exp
(cid:16)− n