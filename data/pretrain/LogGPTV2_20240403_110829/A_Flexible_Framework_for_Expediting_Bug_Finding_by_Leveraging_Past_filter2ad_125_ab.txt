mance counters and their proper usage.
In this work, we argue that the wealth of information provided
by performance counters about program behavior offers a unique
opportunity to explore a multitude of coverage metrics to guide
the fuzzing progress. Moreover, since these counters can be used to
monitor more than one event simultaneously, one can build custom
coverage metrics to suit different proposes. From a practitioner’s
point of view, the collection of these characteristics comes with low
overhead, making HPCs extremely well suited for the task at hand.
Intuitively, our entire approach hinges on the assumption that
bugs in prior versions of a program or library can provide a good
enough signal to help find bugs in other versions of the same pro-
gram [37]. Indeed, the seminal work of Ozment and Schechter
showed that much of the “foundational code” remains the same in
the newer versions of a program [40]. Moreover, because similar
coding practices are followed in newer versions of a program, cod-
ing mistakes tend to persist. Based on that observation, we provide
a framework that can operate in two ways: steer the fuzzing process
toward or away from paths that have architectural events similar
to those observed when prior bugs were discovered. To allow for
this flexibility, we provide a way to systematically assess the HPC
events and select representative sets of events (up to four at a time)
that can be used to differentiate between quality (i.e., crashing) and
non-quality inputs. Next, models are built using standard machine
learning approaches. Lastly, at runtime, we apply these models to
help drive the input selection strategy while fuzzing other versions
(albeit past or current) of the program that were not part of the
learning phase.
3 OUR APPROACH: OMNIFUZZ
In this work, we propose a flexible vulnerability-driven fuzzing
framework, called OmniFuzz, for enhancing baseline fuzzers in
order to find bugs faster, and hopefully, to find more unique bugs.
Unlike contemporary approaches that give equal weight to all paths
explored, we prioritize code paths based on knowledge of past bugs.
By judiciously selecting what inputs should be selected for mutation,
we streamline the space of paths that need to be explored, thereby
minimizing the amount of time spent on paths that are unlikely to
lead to successful outcomes in our quest to locate bugs.
OmniFuzz operates in three phases (illustrated in Figure 2). In
the data collection phase, we collect quality and non-quality inputs.
In fuzzing parlance, quality inputs are those that trigger crashes,
whereas non-quality inputs exit the program normally and do not
result in crashes or hangs. Next, we extract the dynamic behavior for
all the inputs using hardware performance counters. In the model
building phase, we use the HPC traces to select an appropriate
coverage metric for a given program. The metric and the HPC
traces are used to build a machine learning (ML) based classifier,
which is trained to identify the quality and non-quality inputs.
During the runtime phase, the classifier guides the fuzzing of a new
version of the program. We discuss each phase in turn.
3.1 Data Collection
To collect quality inputs, we subject a version of the program to
a baseline fuzzer. The number of quality inputs generated by a
fuzzer depends on multiple factors, including the number of ac-
tual bugs, the seed inputs, and the amount of available computing
resources. To maximize the number of quality inputs, we run the
baseline fuzzer with sufficient computing resources over an ex-
tended period [17, 28]. Since modern fuzzers typically generate
a large number of test inputs quickly, collecting a corpus of non-
quality inputs is not an issue. Next, we curate HPC traces for both
the quality and non-quality inputs. The traces consist of the num-
ber of occurrences of a specific hardware event that was triggered
during the program execution.
Although there are hundreds of hardware events available in
modern processors, only a limited number of counters can be used
to monitor these events simultaneously. Given this constraint, a
natural question is which events should one use? To answer that
question, we conducted an in-depth analysis of processor documen-
tation, and so studied the literature for events that were commonly
used to profile program behavior [5, 29, 38, 56]. We conservatively
selected all those events that could explain the high-level behavior
of a program, but excluded those events that monitor low-level
micro-architectural information or that are difficult to relate to
high-level program behavior. For example, we excluded the events
relating to the pipelining behavior of the CPU. In the end, we were
left with a set of 96 events, which we further grouped into 65 classes
(shown in Table 8). The criteria we used to group events were (a)
events that are similar but only differ due to the change in the
size of hardware components are considered as a single class, and
(b) events that count hits instead of cycles are split into different
classes. To collect the HPC data, we followed the recommendations
of Das et al. [14].
3.2 Model Building
We use a correlation-based technique to identify representative sets
(of 4 events) from the initial set of 96 events. Here, a good feature set
contains features that are correlated with a class, yet uncorrelated
with each other. In our context, a class represents either quality or
non-quality inputs. Given good feature sets, we now need a way to
influence the input selection at runtime. Based on prior knowledge
of the non/quality inputs, one might be able to predict whether
some unknown input is a quality or non-quality input by learning
the dynamic program behavior for these inputs. However, in the
context of fuzzing, the problem is a bit more complicated because
we need to predict whether fuzzing with the current input might
lead to a crash in some future iteration.
ACSAC 2020, December 7–11, 2020, Austin, USA
Sanjeev Das, Kedrian James, Jan Werner, Manos Antonakakis, Michalis Polychronakis, and Fabian Monrose
Figure 2: System design of OmniFuzz
To address this challenge, we use a machine learning approach
that learns the dynamic program behavior triggered by the quality
inputs, and guides the fuzzing process accordingly. As part of the
exploratory phase of our research, we performed model selection
using several machine learning algorithms, including decision trees,
random forests, k-nearest neighbors, and multi-layer perceptrons
(MLPs). We found that no particular model significantly outper-
formed the rest. However, we settled on using the MLP approach
for a number of reasons. First, it does not depend on the discrete
numerical value of features (in contrast to decision trees). Second, it
has constant runtime overhead because our neural network model
is small; specifically, it contains a fixed number of nodes (typically
4 neurons in the input layer, 4 neurons in the hidden layer and 2
neurons in the output layer).
3.3 On-the-fly Deduplication
To limit time wasted hitting the same or similar bug(s) repeatedly,
we use root cause analysis as a deduplication mechanism. The root
cause, in our case, is the line or block of code that first propagated
the bad value that led to the crash. To support online deduplication
via root cause analysis, we implemented a custom solution on top
of Mozilla’s Record and Replay framework [35]. The framework
provides a wealth of information that enables precise analysis of
the program states that led to a crash. We also leverage the use of
hardware watchpoints for memory tracking. Our memory tracking
technique is based on the pointer lifecycle in Figure 3. To locate
the root cause of a crash, analysis begins at the crash point by
extracting the variables involved in the crash and their values. We
then perform backward analysis by leveraging record and replay
and hardware watchpoints to locate the line or block of code that
caused the crash.
Under the Hood: The first step of the analysis consists of record-
ing a trace of the program execution with a crashing input. The
trace includes snapshots of the contents of the memory and the
registers of the recorded execution. After the program’s execution
is recorded, the execution is replayed and analyzed. The replay
process supports moving between snapshots, thus creating an illu-
sion of forward and reverse execution. Our engine leverages the
reverse execution to track the data and control flows that lead to
the program’s crash. Starting at the crash location, we perform the
following steps:
(1) Initialize: Analyze the crashing line, identify the variable(s)
that caused the violation, and set watchpoint(s) on the iden-
tified variable(s).
(2) Search: Revert the program state to a previous snapshot in
which the value of the observed variable has changed, i.e.,
reverse execute until a watchpoint is hit. Identify the vari-
ables used to define the crashing variable(s) and validate the
corresponding values by inspecting their memory mappings.
(3) Decide: If the observed variable changes state from invalid
to valid (see Figure 3), then return the current line and the
set of variables as the root cause. Otherwise return to step 1.
Figure 3: Pointer life-cycle
To assess our deduplication strategy, we examined how well it
correctly identified the root cause for the applications shown in
Table 1. Ground truth was obtained via a labor-intensive manual
process. We found that our approach was able to identify multiple
entry points as a single bug. A prominent example is pcre, where our
engine identifies the bug and multiple entry points as a single entry.
False positives occur with tiff and libxml2 because the engine mis-
attributes the root cause to incorrect source lines. Nonetheless, the
MutateExecuteSelectMonitorBase FuzzerNon-qualityQualityProgramInitial inputsProgramPerf TracerHPC LogsTrace CollectionPhase 1: Pre-processingInputsInstructionsBranchesArithmeticFloating opsMemoryFeaturesFloating opsArithmeticMemoryInstructionsBranchesArithmeticFeature SelectionSelected FeaturesClassiﬁer 2Classiﬁer 1Heuristic GenerationPhase 2: Model BuildingBaseApproachClassiﬁer 2BaseApproachClassiﬁer 1Strategy #1: Similar BugsStrategy #2: Diﬀerent BugsClassiﬁer 2Strategy #3: Dual ModeSwitchPhase 3: Vulnerability ExplorationBaseApproachClassiﬁer 1Non-existentValidInvalidCrashMemory allocationPointer assignment /Release of backing memoryPointer assignmentPointer assignment/ Pointer dereferncePointer derefernceA Flexible Framework for Expediting Bug Finding by Leveraging Past (Mis-)Behavior to Discover New Bugs
ACSAC 2020, December 7–11, 2020, Austin, USA
Table 1: Root Cause Analysis
Library
Bug Class
libarchive (v3.1.0) Heap-buffer-overflow
Heap-buffer-overflow,
libjpeg (v1.4.2)
Null Pointer Dereference
Heap-buffer-overflow
Null Pointer Dereference
Null Pointer Dereference
Heap-buffer-overflow
Heap-buffer-overflow
Null Pointer Dereference
Heap-buffer-overflow,
Null Pointer Dereference
Logic error
libplist (v1.11)
libpng (v1.2.56)
libxml2 (v2.9.2)
pcre (v10.0)
tiff (v4.0)
yaml (v0.5.3)
No. of bugs
Actual
Ours
#Entry
points
8
4
2
1
4
5
3
1
5
2
2
1
6
5
8
1
9
4
4
2
7
15
14
1
results show our strategy can be used as an effective deduplication
strategy.
During online deduplication, there were cases where we needed
to revert to existing methods (e.g., AddressSanitizer [1]) when our
engine failed to provide diagnostic output. That strategy appears to
work well. For instance, in the case of CVE-2016-5102 (discussed in
§5.3), the different fuzzers generated inputs that led to four distinct
crash locations stemming from the same bug. Our core engine suc-
cessfully identified the root cause for all discovered crash locations
as a single bug, but other approaches (AddressSanitizer and stack
backtrace methods) incorrectly identified the crashes as separate
bugs. However, for CVE 2016-3186, our engine’s output was incon-