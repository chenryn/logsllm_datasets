title:AC/DC TCP: Virtual Congestion Control Enforcement for Datacenter
Networks
author:Keqiang He and
Eric Rozner and
Kanak Agarwal and
Yu (Jason) Gu and
Wes Felter and
John B. Carter and
Aditya Akella
AC/DC TCP: Virtual Congestion Control Enforcement
for Datacenter Networks
Keqiang He†
Eric Rozner‡
Kanak Agarwal∗
Yu (Jason) Gu∗
Wes Felter‡
John Carter∗ Aditya Akella†
†University of Wisconsin–Madison
‡IBM Research
∗IBM
ABSTRACT
Multi-tenant datacenters are successful because tenants can
seamlessly port their applications and services to the cloud.
Virtual Machine (VM) technology plays an integral role in
this success by enabling a diverse set of software to be run
on a uniﬁed underlying framework. This ﬂexibility, how-
ever, comes at the cost of dealing with out-dated, inefﬁcient,
or misconﬁgured TCP stacks implemented in the VMs. This
paper investigates if administrators can take control of a VM’s
TCP congestion control algorithm without making changes
to the VM or network hardware. We propose AC(cid:69)DC TCP,
a scheme that exerts ﬁne-grained control over arbitrary ten-
ant TCP stacks by enforcing per-ﬂow congestion control in
the virtual switch (vSwitch). Our scheme is light-weight,
ﬂexible, scalable and can police non-conforming ﬂows. In
our evaluation the computational overhead of AC(cid:69)DC TCP
is less than one percentage point and we show implementing
an administrator-deﬁned congestion control algorithm in the
vSwitch (i.e., DCTCP) closely tracks its native performance,
regardless of the VM’s TCP stack.
CCS Concepts
•Networks → Transport protocols;
Keywords
Datacenter Networks; Congestion Control; Virtualization;
1.
INTRODUCTION
Multi-tenant datacenters are a crucial component of to-
day’s computing ecosystem. Large providers, such as Ama-
zon, Microsoft, IBM, Google and Rackspace, support a di-
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than the author(s) must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
SIGCOMM ’16, August 22 - 26, 2016, Florianopolis , Brazil
© 2016 Copyright held by the owner/author(s). Publication rights licensed to
ACM. ISBN 978-1-4503-4193-6/16/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2934872.2934903
verse set of customers, applications and systems through
their public cloud offerings. These offerings are success-
ful in part because they provide efﬁcient performance to a
wide-class of applications running on a diverse set of plat-
forms. Virtual Machines (VMs) play a key role in support-
ing this diversity by allowing customers to run applications
in a wide variety of operating systems and conﬁgurations.
And while the ﬂexibility of VMs allows customers to eas-
ily move a vast array of applications into the cloud, that same
ﬂexibility inhibits the amount of control a cloud provider
yields over VM behavior. For example, a cloud provider
may be able to provide virtual networks or enforce rate lim-
iting on a tenant VM, but it cannot control the VM’s TCP/IP
stack. As the TCP/IP stack considerably impacts overall net-
work performance, it is unfortunate that cloud providers can-
not exert a ﬁne-grained level of control over one of the most
important components in the networking stack.
Without control over the VM TCP/IP stack, datacenter
networks remain at the mercy of inefﬁcient, out-dated or
misconﬁgured TCP/IP stacks. TCP behavior, speciﬁcally
congestion control, has been widely studied and many issues
have come to light when it is not optimized. For example,
network congestion caused by non-optimzed stacks can lead
to loss, increased latency and reduced throughput.
Thankfully, recent advances optimizing TCP stacks for
datacenters have shown high throughput and low latency can
be achieved through novel TCP congestion control algorithms.
Works such as DCTCP [3] and TIMELY [43] provide high
bandwidth and low latency by ensuring network queues in
switches do not ﬁll up. And while these stacks are deployed
in many of today’s private datacenters [36, 59], ensuring a
vast majority of VMs within a public datacenter will update
their TCP stacks to a new technology is a daunting, if not
impossible, task.
In this paper, we explore how operators can regain author-
ity over TCP congestion control, regardless of the TCP stack
running in a VM. Our aim is to allow a cloud provider to uti-
lize advanced TCP stacks, such as DCTCP, without having
control over the VM or requiring changes in network hard-
ware. We propose implementing congestion control in the
virtual switch (vSwitch) running on each server. Implement-
ing congestion control within a vSwitch has several advan-
tages. First, vSwitches naturally ﬁt into datacenter network
virtualization architectures and are widely deployed [52].
Second, vSwitches can easily monitor and modify trafﬁc
passing through them. Today vSwitch technology is mature
and robust, allowing for a fast, scalable, and highly-available
framework for regaining control over the network.
Implementing congestion control within the vSwitch has
numerous challenges, however. First, in order to ensure adop-
tion rates are high, the approach must work without mak-
ing changes to VMs. Hypervisor-based approaches typically
rely on rate limiters to limit VM trafﬁc. Rate limiters im-
plemented in commodity hardware do not scale in the num-
ber of ﬂows and software implementations incur high CPU
overhead [54]. Therefore, limiting a VM’s TCP ﬂows in a
ﬁne-grained, dynamic nature at scale (10,000’s of ﬂows per
server [46]) with limited computational overhead remains
challenging. Finally, VM TCP stacks may differ in the fea-
tures they support (e.g., ECN) or the congestion control al-
gorithm they implement, so a vSwitch congestion control
implementation should work under a variety of conditions.
This paper presents Administrator Control over Datacen-
ter TCP (AC(cid:69)DC TCP, or simply AC(cid:69)DC), a new technology
that implements TCP congestion control within a vSwitch to
help ensure VM TCP performance cannot impact the net-
work in an adverse way. At a high-level, the vSwitch mon-
itors all packets for a ﬂow, modiﬁes packets to support fea-
tures not implemented in the VM’s TCP stack (e.g., ECN)
and reconstructs important TCP parameters for congestion
control. AC(cid:69)DC runs the congestion control logic speciﬁed
by an administrator and then enforces an intended conges-
tion window by modifying the receive window (RWND) on
incoming ACKs. A policing mechanism ensures stacks can-
not beneﬁt from ignoring RWND.
Our scheme provides the following beneﬁts. First, AC(cid:69)DC
allows administrators to enforce a uniform, network-wide
congestion control algorithm without changing VMs. When
using congestion control algorithms tuned for datacenters,
this allows for high throughput and low latency. Second,
our system mitigates the impact of varying TCP stacks run-
ning on the same fabric. This improves fairness and addi-
tionally solves the ECN co-existence problem identiﬁed in
production networks [36, 72]. Third, our scheme is easy to
implement, computationally lightweight, scalable, and mod-
ular so that it is highly complimentary to performance iso-
lation schemes also designed for virtualized datacenter envi-
ronments. The contributions of this paper are as follows:
1. The design of a vSwitch-based congestion control mech-
anism that regains control over the VM’s TCP/IP stack
without requiring any changes to the VM or network
hardware.
2. A prototype implementation to show our scheme is ef-
fective, scalable, simple to implement, and has less
than one percentage point computational overhead in
our tests.
3. A set of results showing DCTCP conﬁgured as the host
TCP stack provides nearly identical performance to when
the host TCP stack varies but DCTCP’s congestion con-
trol is implemented in the vSwitch. We demonstrate
how AC(cid:69)DC can improve throughput, fairness and la-
tency on a shared datacenter fabric.
The outline of this paper is as follows. Background and
motivation are discussed in §2. AC(cid:69)DC’s design is outlined
in §3 and implementation in §4. Results are presented in §5.
Related work is surveyed in §6 before concluding.
2. BACKGROUND AND MOTIVATION
This section ﬁrst gives a brief background of congestion
control in the datacenter. Then the motivation for moving
congestion control into the vSwitch is presented. Finally, AC(cid:69)DC
is contrasted from a class of related bandwidth allocation
schemes.
2.1 Datacenter Transport
Today’s datacenters host applications such as search, ad-
vertising, analytics and retail that require high bandwidth
and low latency. Network congestion, caused by imperfect
load balancing [1], network upgrades or failures, can ad-
versely impact these services. Unfortunately, congestion is
not rare in datacenters. For example, recently Google re-
ported congestion-based drops were observed when network
utilization approached 25% [59]. Other studies have shown
high variance and substantial increase in the 99.9th percentile
latency for round-trip times in today’s datacenters [45, 69].
Large tail latencies impact customer experience, result in
revenue loss [3, 17], and degrade application performance [26,
33]. Therefore, signiﬁcant motivation exists to reduce con-
gestion in datacenter fabrics.
TCP’s congestion control algorithm is known to signiﬁ-
cantly impact network performance. As a result, datacenter
TCP performance has been widely studied and many new
protocols have been proposed [3, 35, 43, 62, 71]. Speciﬁ-
cally, DCTCP [3] adjusts a TCP sender’s rate based on the
fraction of packets experiencing congestion. In DCTCP, the
switches are conﬁgured to mark packets with an ECN bit
when their queue lengths exceed a threshold. By propor-
tionally adjusting the rate of the sender based on the fraction
of ECN bits received, DCTCP can keep queue lengths low,
maintain high throughput, and increase fairness and stabil-
ity over traditional schemes [3, 36]. For these reasons, we
implement DCTCP as the vSwitch congestion control algo-
rithm in AC(cid:69)DC.
2.2 Beneﬁts of AC(cid:69)DC
Allowing administrators to enforce an optimized conges-
tion control without changing the VM is the ﬁrst major ben-
eﬁt of our scheme. This is an important criteria in untrusted
public cloud environments or simply in cases where servers
cannot be updated due to a dependence on a speciﬁc OS or
library. [36]
The next beneﬁt is AC(cid:69)DC allows for uniform conges-
tion control to be implemented throughout the datacenter.
Unfairness arises when stacks are handled differently in the
fabric or when conservative and aggressive stacks coexist.
(a) 5 different CCs.
(b) All CUBIC.
Figure 1: Different congestion controls lead to unfairness.
Studies have shown ECN-capable and ECN-incapable ﬂows
do not exist gracefully on the same fabric because packets
belonging to ECN-incapable ﬂows encounter severe packet
drops when their packets exceed queue thresholds [36, 72].
Additionally, stacks with different congestion control algo-
rithms may not share the same fabric fairly. For example,
Figure 1 shows the performance of ﬁve different TCP ﬂows
on the topology in Figure 7a. Each ﬂow selects a conges-
tion control algorithm available in Linux: CUBIC [29], Illi-
nois [41], HighSpeed [21], New Reno [22] and Vegas [13].
Figure 1a shows aggressive stacks such as Illinois and High-
Speed achieve higher bandwidth and thus fairness is worse
than all ﬂows using the same stack (Figure 1b).
Another beneﬁt of AC(cid:69)DC is it allows for different con-
gestion control algorithms to be assigned on a per-ﬂow ba-
sis. A vSwitch-based approach can assign WAN ﬂows to
a congestion control algorithm that optimizes WAN perfor-
mance [20, 63] and datacenter ﬂows to one that optimizes
datacenter performance, even if these ﬂows originate from
the same VM (e.g., a webserver). Additionally, as shown
in §3.4, a ﬂexible congestion control algorithm can provide
relative bandwidth allocations to ﬂows. This is useful when
tenants or administrators want to prioritize ﬂows assigned to
the same quality-of-service class. In short, adjusting con-
gestion control algorithms on a per-ﬂow basis allows for en-
hanced ﬂexibility and performance.
Finally, congestion control is not difﬁcult to port. While
the entire TCP stack may seem complicated and prone to
high overhead, the congestion control aspect of TCP is rel-
atively light-weight and simple to implement. Indeed, stud-
ies show most TCP overhead comes from buffer manage-
ment [42], and in our evaluation the computational overhead
of AC(cid:69)DC is less than one percentage point. Porting is also
made easy because congestion control implementations in
Linux are modular: DCTCP’s congestion control resides in
tcp_dctcp.c and is only about 350 lines of code. Given
the simplicity of congestion control, it is not hard to move
its functionality to another layer.
2.3 Tenant-Level Bandwidth Allocation
While AC(cid:69)DC enforces congestion control, transport layer
schemes do not provide fair bandwidth allocation among
tenants because a tenant with more concurrent ﬂows can ob-
tain a higher share of bandwidth. In order to provide per-
formance isolation in the network, datacenter operators can