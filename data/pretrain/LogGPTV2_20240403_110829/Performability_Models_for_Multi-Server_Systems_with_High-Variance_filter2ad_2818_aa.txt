title:Performability Models for Multi-Server Systems with High-Variance
Repair Durations
author:Hans-Peter Schwefel and
Imad Antonios
Performability Models for Multi-Server
Systems with High-Variance Repair Durations
Hans-Peter Schwefel
Center for Teleinfrastruktur
Aalborg University
Email: PI:EMAIL
Imad Antonios
Dept. of Computer Science
Southern Connecticut State University
Email: PI:EMAIL
Abstract
only been observed in recent telecommunication models.
We consider cluster systems with multiple nodes where
each server is prone to run tasks at a degraded level of ser-
vice due to some software or hardware fault. The cluster
serves tasks generated by remote clients, which are poten-
tially queued at a dispatcher. We present an analytic queue-
ing model of such systems, represented as an M/MMPP/1
queue, and derive and analyze exact numerical solutions for
the mean and tail-probabilities of the queue-length distribu-
tion. The analysis shows that the distribution of the repair
time is critical for these performability metrics. Addition-
ally, in the case of high-variance repair times, the model
reveals so-called blow-up points, at which the performance
characteristics change dramatically. Since this blowup be-
havior is sensitive to a change in model parameters, it is
critical for system designers to be aware of the conditions
under which it occurs. Finally, we present simulation results
that demonstrate the robustness of this qualitative blow-up
behavior towards several model variations.
1. Introduction and Motivation
Performability modeling seeks to capture the behavior
of systems that exhibit degradable performance. In recent
years, as both the size and the pervasiveness of distributed
systems have increased in support of mission-critical and
high performance applications, the need to assess the per-
formance of such systems has become more important. In
this paper, we consider a model of a cluster with a small
number of N nodes, where each node is prone to degrada-
tion in service, which in the limit also include crash fail-
ures. We assume and argue for that servers recover from
their degraded state after a period of time that shows high
variance in most practical scenarios. For such high-variance
distributions however, performability metrics of such clus-
ter systems show very peculiar behavior that has previously
Within the body of research on performability, Mitrani
in [12] surveyed several queueing models where tasks are
fed into unreliable servers, and [8] had studied the com-
pletion time of tasks in a fault-prone queueing environment
for various failure handling strategies. Solutions to such
models are presented with the assumptions that task ser-
vice time, up and breakdown durations are exponentially
distributed. More recently, a study provided evidence that
hyperexponential distributions provide a better ﬁt for the re-
pair times associated with crash failure [13]. Under certain
parameter settings, this repair behavior can lead to power-
tail distributed durations. Since in practice repair times are
bounded, a truncated power-tail (TPT) distribution intro-
duced in [6] is used. Put in a queueing context, long break-
down periods that are symptoms of high variance inevitably
lead to long queue backups, making the study of systems
with such behavior worthwhile.
In developing a queueing model for the system laid out
above, it is easy to recognize is that each server can be rep-
resented as an ON/OFF model. Our model extends to allow
for an aggregation of servers being fed by a single queue
with a Poisson arrival rate. This can thus be represented as
an M/MAP/1 queueing system, for which analytical solu-
tions are provided in [9]. The model construction makes the
assumption of load independence, that is the task process-
ing rate is independent of the number of tasks in the system.
We show through simulation that this approximation of the
physical system is of little bearing on our analysis results.
We highlight the symmetry between the multi-server model
and an MAP/M/1 trafﬁc model termed N-Burst, and the ap-
plicability of results from the latter to understand the perfor-
mance characteristics of the degradable system. With our
analytic results as a baseline, we explore using simulation
failure-handling strategies for systems that allow for node
crash failures. Additionally, we consider such systems with
nonexponential task service times and look at how these af-
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007fect queueing performance.
The contributions of this paper are as follows: 1) The
development of an analytic queueing model with variations
amenable to exact solutions of queue-length distributions,
2) a characterization of blow-up points denoting a change
in the qualitative behavior of the mean queue-length at spe-
ciﬁc parameter settings, and 3) simulation results showing
that the qualitative behavior is robust towards small model
variations including failure handling strategies.
2. System Model
The type of system modeled in this paper is a cluster of
N servers fed by a FIFO queue at which tasks arrive ac-
cording to some process with average rate λ. A fail-safe
dispatcher assigns a task to the ﬁrst available server where,
given the server is in its fully operational state, it executes
for an exponential service time with mean 1/νp. The expo-
nential task service time allows to model the N server clus-
ter system by a single server with an MMPP for the service
times. However, we will show via simulations in Section
4 that the qualitative performance results in most scenarios
are insensitive to it.
A server alternates between two states: the UP state de-
noting an operational server at full capacity with a mean du-
ration corresponding to MTTF, and the DOWN state repre-
senting a degraded level of service lasting for a mean dura-
tion corresponding to MTTR. The level of service degrada-
tion is captured by a ﬁxed degradation factor δ, where δ = 0
represents a crash failure, and 1 > δ > 0 may be viewed as
a non-catastrophic fault at the server, which slows down the
execution of a task, such as in the case when some erratic
process consumes a large amount of CPU time. We assume
that faults causing the service degradation are independent
of the current processed tasks, of faults on other servers, and
of faults in subsequent UP-DOWN cycles at the same node.
We also assume that the dispatcher has instantaneous and
always correct information about the nodes in case of crash
faults (ideal failure detection). Consistent with convention,
we express the server’s availability as
A :=
M T T F
M T T R + M T T F
.
(1)
This deﬁnition of A does not depend on the fault type,
meaning that it is independent of δ.
With respect to the recovery behavior of tasks interrupted
by crash failures, a case that only occurs for δ = 0, we
consider three strategies:
• Discard: The interrupted task is removed from the
cluster. Such an approach can be applicable in soft
real-time systems, where the utility of the result of
computation decreases with time.
• Restart: The identical task is restarted at either the
original node after it is repaired, or at a different node.
This is different from the case considered in [14] where
the task completion time can beneﬁt from restarting it.
The restart strategy adopted here would require that
the dispatcher maintain the necessary task activation
information until it is completed. A restarted task can
be handled using two approaches: 1) add it to the head
of the queue, or (2) add the failed task to the end of the
queue.
• Resume: The server nodes apply ideal checkpointing
to the task execution with the consequence that the dis-
patcher can ask another node to resume the execution
of the task at the point where it stopped. Compared
to Restart, this has the advantage that the remaining
processing time is the task residual time, which for ex-
ponential task times is also exponential with the same
mean. However, the disadvantage of Resume is that
the checkpointing is rather costly and may only be ap-
plied in limited cases. Same as in Restart, a resumed
task may be placed at either the front or the tail of the
queue.
Note that with respect to the inﬂuence on the queue length
process, Discard is the best strategy, Resume second, and
Restart worst; the price for the former is the increased cost
of checkpointing, and for the discard strategy, that some
tasks are not successfully completed (even when there is no
QoS/delay bound). For the queue-length process in case of
exponential task times, it is irrelevant whether the resumed
task is stored at the head of the queue or at the tail due to
the properties of residual times of exponential distributions.
However, there is an impact on system time distribution,
which is deﬁned as the sum of queueing delay and total ser-
vice time including potentially multiple restarts. The im-
pact on system time distribution is even more pronounced
for the two restart cases, which are not equivalent for the
queue-length based metrics.
In summary, the basic system assumptions are:
• Cluster consisting of ﬁxed number N of statistically
identical nodes.
• Independent failures and repairs of each node, failures
lead to either performance degradation (slowdown by
factor 1 > δ >0) or to complete crash (δ = 0).
• A dispatcher maintains the queue of tasks to be ex-
ecuted in a transaction manner on one of the cluster
nodes. The dispatcher never fails.
In case of crash
failures (not in main focus of this paper though), the
dispatcher has instantaneous and always correct fault-
detection implemented.
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007• Tasks are generated by clients according to some gen-
eral process with rate λ.
• Task time in principle can also be general, see Section
4, but the analytic model is based on exponential task
times.
• Time to failure (TTF) and time to repair (TTR) can be
generally distributed.
We will however, for the ease of description of the an-
alytic model, include more limiting assumptions that can
easily be removed, and are shown to be performance-wise
not relevant in Section 4. Most of these assumptions can be
circumvented by utilizing matrix-exponential dirstibutions
[10] or Markovian Arrival Processes (MAP) [15] at the cost
of increased model state-space, with typically also impact
on the accuracy of the numerical evaluation of the anlytic
model. Furthermore, complex distribution types increase
the parameter space of the model which makes the numeri-
cal analysis more cumbersome. Finally, in order to gain un-
derstanding of the causes of certain performance behavior,
it is in most cases advisable to use the most parsimonious
model by which such behavior can be created.
2.1. Distribution of UP and DOWN Periods
of the Servers
The model presented in the next section will allow for
general, matrix exponential [10] UP and DOWN times. We
use the latter interchangeably with repair time to refer to a
server being in either a degraded or crashed state.
Using analogies from teletrafﬁc models, see Section 2.3
and [21], we will show later that the actual distribution
of UP times only has marginal inﬂuence on queue perfor-
mance other than by its mean, and as such the analytic re-
sults in Section 3 will be presented using exponential UP
times for convenience. As the goal of analytic modeling
is to focus the model on the aspects that are inﬂuential for
the considered metrics, here mean queue length and prob-
ability that a task with QoS/delay bound ﬁnishes success-
fully, using exponential assumptions here is not only useful
for modeling convenience, but also helpful in determining
clear evidence of performability relevant aspects of the clus-
ter system.
Regarding the repair/DOWN time, the results in Sec-
tion 3 will show a dramatic impact, both on average queue-
length and on tail probabilities of the queue-length distribu-
tion. Since this is closely related to probabilities of exceed-
ing a certain system time, it can be used to approximate the
fraction of successfully completed tasks under certain delay
constraints.
The DOWN period corresponds to the fault detection
time and repair time of an individual server. Depending on
the type of fault, a repair time can range from a few seconds
for a restart of a small process, to the order of minutes for a
system reboot, or hours for hardware faults with spare parts
in stock, up to even days and weeks for the replacement of
the faulty machine or hardware component.
Assuming that these different fault-types each lead to ex-
ponential repair times, but with different rates, the repair-
time distribution can be represented as hyperexponential
distribution with increasing average holding time for the
different fault severity (in terms of effort for repair). For
certain parameter settings, namely geometric decay of the
entrance probability with geometric growth of mean state
holding times, these hyperexponentials can exhibit power-
tail behavior until the reliability function ﬁnally drops off
exponentially (see [6] for details). This exponential drop
off denotes a truncation in the tail, and it corresponds to the
longest repair time.
Regardless of whether a hyperexponential distribution
with high variance, or as a special case a truncated power-
tail distribution is assumed for the repair times, the per-
formability metrics of the cluster system will show very pe-
culiar behavior. Section 3 analyzes and explains this behav-
ior ﬁrst for the example of truncated Power-Tal distributions
(in resemblance to teletrafﬁc models [17, 19]) and then for
2-state hyperexponential distributions with large variance.
2.2. Matrix Representation of Server Model
With exponential task times, the proposed multi-node
cluster system is equivalent to a single server system with
modulated service rate, namely the number of servers that
are UP and the number of tasks in the system inﬂuence the
instantaneous service rate as follows:
ν(t) =ν p ∗ SourcesUPt + δ ∗ νp(N − SourcesUPt)(2)
given that the number of tasks in the system is larger or
equal to N. Otherwise, the number of tasks in the system
has a limiting impact, since not all servers can be utilized.
In order to simplify the model speciﬁcation, we make the
following assumptions, see Sect. 2.4 for more discussion on
them:
• We do not consider the limiting inﬂuence that occurs,
when the number of tasks in the system is smaller than
N, i.e. Eq. (2) is always assumed to be exactly true.
As such, the analytic model will lead a lower bound
on the performance behavior, but Section 4 demon-
strates that for the scenario in our interest, this lower
bound is very close to the actual exact result. The ana-
lytic model below can be extended to include this load-
dependence utilizing the same approach as in [7, 20].
The model extension however makes the numerical
analysis computationally more expensive and also nu-
merically much less stable.
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007• We assume that task arrivals are Poisson (despite other
distributions can be easily included, see Sect. 2.4).
• Task execution times are exponential, as stated already
at the beginning of this section.
• The number of servers, N, is a low integer, e.g. be-
tween 2 and 10 or 20. There are three main reasons for
this assumption. (1) computational effort and numeri-
cal accuracy: The size of the state space of the model
as introduced below grows exponentially with N. (2)
High-availability clusters with redundantly stored pro-
cessing states typically consist of only a few, closely
coupled (within the same IP-subnet) nodes.
(3) The
performance and dependability impact that we identify
in this paper is particularly pronounced for the settings
of low N.
• the dispatcher queue is inﬁnite.
Utilizing the ﬁrst assumption/approximation together
with the exponential task times, the collective N nodes
in the cluster system can be represented as a single-server
Markov Modulated Poisson Process (MMPP). We assume
that all servers are independent and identical, namely
with Matrix-Exponential DOWN(repair)/UP periods, resp-
resented by the vector-matrix pairs,  and
, respectively, following the notation of [10].
Consequently, the modulating Markov process for the ser-
vice rate of a single server then has the following generator
matrix:
 ,
Q1 =
−Bdown
Bdownε(cid:1)
down pup
Bupε(cid:1)
uppdown
−Bup
and the corresponding Poisson service rates on the diagonal