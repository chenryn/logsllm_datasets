shadow VMCS. Shadow VMCS is a hardware optimization very similar to the enlightened VMCS.
Nested address translation
As previously discussed in the “Partitions’ physical address space” section, the hypervisor uses the SLAT 
for providing an isolated guest physical address space to a VM and to translate GPAs to real SPAs. Nested 
virtual machines would require another hardware layer of translation on top of the two already existing. 
For supporting nested virtualization, the new layer should have been able to translate L2 GPAs to L1 GPAs. 
Due to the increased complexity in the electronics needed to build a processor’s MMU that manages 
three layers of translations, the Hyper-V hypervisor adopted another strategy for providing the additional 
layer of address translation, called shadow nested page tables. Shadow nested page tables use a tech-
nique similar to the shadow paging (see the previous section) for directly translating L2 GPAs to SPAs.
When a partition that supports nested virtualization is created, the L0 hypervisor allocates and initial-
izes a nested page table shadowing domain. The data structure is used for storing a list of shadow nested 
page tables associated with the different L2 VMs created in the partition. Furthermore, it stores the parti-
tion’s active domain generation number (discussed later in this section) and nested memory statistics.
When the L0 hypervisor performs the initial VMENTRY for starting a L2 VM, it allocates the shadow 
nested page table associated with the VM and initializes it with empty values (the resulting physical 
address space is empty). When the L2 VM begins code execution, it immediately produces a VMEXIT 
to the L0 hypervisor due to a nested page fault (EPT violation in Intel hardware). The L0 hypervisor, 
instead of injecting the fault in the L1, walks the guest’s nested page tables built by the L1 hypervisor. If 
it finds a valid entry for the specified L2 GPA, it reads the corresponding L1 GPA, translates it to an SPA, 
and creates the needed shadow nested page table hierarchy to map it in the L2 VM. It then fills the leaf 
table entry with the valid SPA (the hypervisor uses large pages for mapping shadow nested pages) and 
resumes the execution directly to the L2 VM by setting the nested VMCS that describes it as active.
For the nested address translation to work correctly, the L0 hypervisor should be aware of any modi-
fications that happen to the L1 nested page tables; otherwise, the L2 VM could run with stale entries. 
This implementation is platform specific; usually hypervisors protect the L2 nested page table for read-
only access. In that way they can be informed when the L1 hypervisor modifies it. The Hyper-V hypervi-
sor adopts another smart strategy, though. It guarantees that the shadow nested page table describing 
the L2 VM is always updated because of the following two premises:
I 
When the L1 hypervisor adds new entries in the L2 nested page table, it does not perform any
other action for the nested VM (no intercepts are generated in the L0 hypervisor). An entry in
the shadow nested page table is added only when a nested page fault causes a VMEXIT in the
L0 hypervisor (the scenario described previously).
I 
As for non-nested VM, when an entry in the nested page table is modified or deleted, the
hypervisor should always emit a TLB flush for correctly invalidating the hardware TLB. In case
CHAPTER 9 Virtualization technologies
311
of nested virtualization, when the L1 hypervisor emits a TLB flush, the L0 intercepts the request 
and completely invalidates the shadow nested page table. The L0 hypervisor maintains a virtual 
TLB concept thanks to the generation IDs stored in both the shadow VMCS and the nested 
page table shadowing domain. (Describing the virtual TLB architecture is outside the scope 
of the book.)
Completely invalidating the shadow nested page table for a single address changed seems to be 
redundant, but it’s dictated by the hardware support. (The INVEPT instruction on Intel hardware does 
not allow specifying which single GPA to remove from the TLB.) In classical VMs, this is not a problem 
because modifications on the physical address space don’t happen very often. When a classical VM is 
started, all its memory is already allocated. (The “Virtualization stack” section will provide more de-
tails.) This is not true for VA-backed VMs and VSM, though.
For improving performance in nonclassical nested VMs and VSM scenarios (see the next section 
for details), the hypervisor supports the “direct virtual flush” enlightenment, which provides to the L1 
hypervisor two hypercalls to directly invalidate the TLB. In particular, the HvFlushGuestPhysicalAddress 
List hypercall (documented in the TLFS) allows the L1 hypervisor to invalidate a single entry in the 
shadow nested page table, removing the performance penalties associated with the flushing of the 
entire shadow nested page table and the multiple VMEXIT needed to reconstruct it. 
EXPERIMENT: Enabling nested virtualization on Hyper-V
As explained in this section, for running a virtual machine into a L1 Hyper-V VM, you should first 
enable the nested virtualization feature in the host system. For this experiment, you need a work-
station with an Intel or AMD CPU and Windows 10 or Windows Server 2019 installed (Anniversary 
Update RS1 minimum version). You should create a Type-2 VM using the Hyper-V Manager or 
Windows PowerShell with at least 4 GB of memory. In the experiment, you’re creating a nested L2 
VM into the created VM, so enough memory needs to be assigned.
After the first startup of the VM and the initial configuration, you should shut down the VM 
and open an administrative PowerShell window (type Windows PowerShell in the Cortana 
search box. Then right-click the PowerShell icon and select Run As Administrator). You should 
then type the following command, where the term “” must be replaced by your 
virtual machine name:
Set-VMProcessor -VMName "" -ExposeVirtualizationExtension $true
To properly verify that the nested virtualization feature is correctly enabled, the command
$(Get-VMProcessor -VMName "").ExposeVirtualizationExtensions
should return True.
After the nested virtualization feature has been enabled, you can restart your VM. Before 
being able to run the L1 hypervisor in the virtual machine, you should add the necessary com-
ponent through the Control panel. In the VM, search Control Panel in the Cortana box, open it, 
click Programs, and the select Turn Windows Features On Or Off. You should check the entire 
Hyper-V tree, as shown in the next figure.
EXPERIMENT: Enabling nested virtualization on Hyper-V
As explained in this section, for running a virtual machine into a L1 Hyper-V VM, you should first 
enable the nested virtualization feature in the host system. For this experiment, you need a work-
station with an Intel or AMD CPU and Windows 10 or Windows Server 2019 installed (Anniversary 
Update RS1 minimum version). You should create a Type-2 VM using the Hyper-V Manager or 
Windows PowerShell with at least 4 GB of memory. In the experiment, you’re creating a nested L2 
VM into the created VM, so enough memory needs to be assigned.
After the first startup of the VM and the initial configuration, you should shut down the VM 
and open an administrative PowerShell window (type Windows PowerShell in the Cortana 
search box. Then right-click the PowerShell icon and select Run As Administrator). You should 
then type the following command, where the term “” must be replaced by your 
“” must be replaced by your 
“”
virtual machine name:
Set-VMProcessor -VMName "" -ExposeVirtualizationExtension $true
To properly verify that the nested virtualization feature is correctly enabled, the command
$(Get-VMProcessor -VMName "").ExposeVirtualizationExtensions
should return True.
After the nested virtualization feature has been enabled, you can restart your VM. Before 
being able to run the L1 hypervisor in the virtual machine, you should add the necessary com-
ponent through the Control panel. In the VM, search Control Panel in the Cortana box, open it, 
click Programs, and the select Turn Windows Features On Or Off. You should check the entire 
Hyper-V tree, as shown in the next figure.
312 
CHAPTER 9 Virtualization technologies
Click OK. After the procedure finishes, click Restart to reboot the virtual machine (this step 
is needed). After the VM restarts, you can verify the presence of the L1 hypervisor through 
the System Information application (type msinfo32 in the Cortana search box. Refer to the 
“Detecting VBS and its provided services” experiment later in this chapter for further details). 
If the hypervisor has not been started for some reason, you can force it to start by opening an 
administrative command prompt in the VM (type cmd in the Cortana search box and select Run 
As Administrator) and insert the following command:
bcdedit /set {current} hypervisorlaunchtype Auto
At this stage, you can use the Hyper-V Manager or Windows PowerShell to create a L2 guest 
VM directly in your virtual machine. The result can be something similar to the following figure.
Click OK. After the procedure finishes, click Restart to reboot the virtual machine (this step 
is needed). After the VM restarts, you can verify the presence of the L1 hypervisor through 
the System Information application (type msinfo32 in the Cortana search box. Refer to the 
“Detecting VBS and its provided services” experiment later in this chapter for further details). 
If the hypervisor has not been started for some reason, you can force it to start by opening an 
administrative command prompt in the VM (type cmd in the Cortana search box and select Run 
As Administrator) and insert the following command:
bcdedit /set {current} hypervisorlaunchtype Auto
At this stage, you can use the Hyper-V Manager or Windows PowerShell to create a L2 guest 
VM directly in your virtual machine. The result can be something similar to the following figure.
CHAPTER 9 Virtualization technologies
313
From the L2 root partition, you can also enable the L1 hypervisor debugger, in a similar way 
as explained in the “Connecting the hypervisor debugger” experiment previously in this chapter. 
The only limitation at the time of this writing is that you can’t use the network debugging in nest-
ed configurations; the only supported configuration for debugging the L1 hypervisor is through 
serial port. This means that in the host system, you should enable two virtual serial ports in the 
L1 VM (one for the hypervisor and the other one for the L2 root partition) and attach them to 
named pipes. For type-2 virtual machines, you should use the following PowerShell commands 
to set the two serial ports in the L1 VM (as with the previous commands, you should replace the 
term “” with the name of your virtual machine):
Set-VMComPort -VMName "" -Number 1 -Path \\.\pipe\HV_dbg 
Set-VMComPort -VMName "" -Number 2 -Path \\.\pipe\NT_dbg
After that, you should configure the hypervisor debugger to be attached to the COM1 serial 
port, while the NT kernel debugger should be attached to the COM2 (see the previous experi-
ment for more details).
The Windows hypervisor on ARM64
Unlike the x86 and AMD64 architectures, where the hardware virtualization support was added long 
after their original design, the ARM64 architecture has been designed with hardware virtualization 
support. In particular, as shown in Figure 9-22, the ARM64 execution environment has been split in 
three different security domains (called Exception Levels). The EL determines the level of privilege; the 
higher the EL, the more privilege the executing code has. Although all the user mode applications run 
in EL0, the NT kernel (and kernel mode drivers) usually runs in EL1. In general, a piece of software runs 
only in a single exception level. EL2 is the privilege level designed for running the hypervisor (which, 
in ARM64 is also called “Virtual machine manager”) and is an exception to this rule. The hypervisor pro-
vides virtualization services and can run in Nonsecure World both in EL2 and EL1. (EL2 does not exist in 
the Secure World. ARM TrustZone will be discussed later in this section.) 
Non-Secure World
Application
Kernel
Secure World
Trusted Application
Secure Kernel
Hypervisor
EL0
EL1
EL2
EL3
Monitor
FIGURE 9-22 The ARM64 execution environment.
From the L2 root partition, you can also enable the L1 hypervisor debugger, in a similar way 
as explained in the “Connecting the hypervisor debugger” experiment previously in this chapter. 
The only limitation at the time of this writing is that you can’t use the network debugging in nest-
ed configurations; the only supported configuration for debugging the L1 hypervisor is through 
serial port. This means that in the host system, you should enable two virtual serial ports in the 
L1 VM (one for the hypervisor and the other one for the L2 root partition) and attach them to 
named pipes. For type-2 virtual machines, you should use the following PowerShell commands 
to set the two serial ports in the L1 VM (as with the previous commands, you should replace the 
term “” with the name of your virtual machine):
“” with the name of your virtual machine):
“”
Set-VMComPort -VMName "" -Number 1 -Path \\.\pipe\HV_dbg
Set-VMComPort -VMName "" -Number 2 -Path \\.\pipe\NT_dbg
After that, you should configure the hypervisor debugger to be attached to the COM1 serial 
port, while the NT kernel debugger should be attached to the COM2 (see the previous experi-
ment for more details).
314 
CHAPTER 9 Virtualization technologies
Unlike from the AMD64 architecture, where the CPU enters the root mode (the execution domain 
in which the hypervisor runs) only from the kernel context and under certain assumptions, when a 
standard ARM64 device boots, the UEFI firmware and the boot manager begin their execution in EL2. 
On those devices, the hypervisor loader (or Secure Launcher, depending on the boot flow) is able to 
start the hypervisor directly and, at later time, drop the exception level to EL1 (by emitting an exception 
return instruction, also known as ERET).
On the top of the exception levels, TrustZone technology enables the system to be partitioned be-
tween two execution security states: secure and non-secure. Secure software can generally access both 
secure and non-secure memory and resources, whereas normal software can only access non-secure 
memory and resources. The non-secure state is also referred to as the Normal World. This enables an 
OS to run in parallel with a trusted OS on the same hardware and provides protection against certain 
software attacks and hardware attacks. The secure state, also referred as Secure World, usually runs se-
cure devices (their firmware and IOMMU ranges) and, in general, everything that requires the proces-
sor to be in the secure state. 
To correctly communicate with the Secure World, the non-secure OS emits secure method calls 
(SMC), which provide a mechanism similar to standard OS syscalls. SMC are managed by the TrustZone. 
TrustZone usually provides separation between the Normal and the Secure Worlds through a thin 
memory protection layer, which is provided by well-defined hardware memory protection units 
(Qualcomm calls these XPUs). The XPUs are configured by the firmware to allow only specific execu-
tion environments to access specific memory locations. (Secure World memory can’t be accessed by 
Normal World software.)
In ARM64 server machines, Windows is able to directly start the hypervisor. Client machines often 
do not have XPUs, even though TrustZone is enabled. (The majority of the ARM64 client devices in 
which Windows can run are provided by Qualcomm.) In those client devices, the separation between 
the Secure and Normal Worlds is provided by a proprietary hypervisor, named QHEE, which provides 
memory isolation using stage-2 memory translation (this layer is the same as the SLAT layer used by the 
Windows hypervisor). QHEE intercepts each SMC emitted by the running OS: it can forward the SMC 
directly to TrustZone (after having verified the necessary access rights) or do some work on its behalf. In 
these devices, TrustZone also has the important responsibility to load and verify the authenticity of the 
machine firmware and coordinates with QHEE for correctly executing the Secure Launch boot method.
Although in Windows the Secure World is generally not used (a distinction between Secure/Non 
secure world is already provided by the hypervisor through VTL levels), the Hyper-V hypervisor still 
runs in EL2. This is not compatible with the QHEE hypervisor, which runs in EL2, too. To solve the prob-
lem correctly, Windows adopts a particular boot strategy; the Secure launch process is orchestrated 
with the aid of QHEE. When the Secure Launch terminates, the QHEE hypervisor unloads and gives up 
execution to the Windows hypervisor, which has been loaded as part of the Secure Launch. In later 
boot stages, after the Secure Kernel has been launched and the SMSS is creating the first user mode 
session, a new special trustlet is created (Qualcomm named it as “QcExt”). The trustlet acts as the origi-
nal ARM64 hypervisor; it intercepts all the SMC requests, verifies the integrity of them, provides the 
needed memory isolations (through the services exposed by the Secure Kernel) and is able to send and 
receive commands from the Secure Monitor in EL3.
CHAPTER 9 Virtualization technologies
315
The SMC interception architecture is implemented in both the NT kernel and the ARM64 trustlet 
and is outside the scope of this book. The introduction of the new trustlet has allowed the majority of 
the client ARM64 machines to boot with Secure Launch and Virtual Secure Mode enabled by default. 
(VSM is discussed later in this chapter.)
The virtualization stack
Although the hypervisor provides isolation and the low-level services that manage the virtualization 
hardware, all the high-level implementation of virtual machines is provided by the virtualization stack. 
The virtualization stack manages the states of the VMs, provides memory to them, and virtualizes the 
hardware by providing a virtual motherboard, the system firmware, and multiple kind of virtual devices 
(emulated, synthetic, and direct access). The virtualization stack also includes VMBus, an important 
component that provides a high-speed communication channel between a guest VM and the root 
partition and can be accessed through the kernel mode client library (KMCL) abstraction layer.
In this section, we discuss some important services provided by the virtualization stack and analyze 
its components. Figure 9-23 shows the main components of the virtualization stack.
Root Partition / Host OS
VmCompute
VMWP
VMMS
VSPs
Physical
Device
Drivers
VMBus
VID.sys
WinHvr.sys
Hypervisor
Hardware
VDEVs
Child Partition
Guest OS
VSCs
Virtual
Device
Drivers
VMBus
WinHv.sys
FIGURE 9-23 Components of the virtualization stack.
Virtual machine manager service and worker processes
The virtual machine manager service (Vmms.exe) is responsible for providing the Windows 
Management Instrumentation (WMI) interface to the root partition, which allows managing the 
child partitions through a Microsoft Management Console (MMC) plug-in or through PowerShell. 
The VMMS service manages the requests received through the WMI interface on behalf of a VM 
(identified internally through a GUID), like start, power off, shutdown, pause, resume, reboot, and so 
on. It controls settings such as which devices are visible to child partitions and how the memory and 
316 
CHAPTER 9 Virtualization technologies
processor allocation for each partition is defined. The VMMS manages the addition and removal of 
devices. When a virtual machine is started, the VMM Service also has the crucial role of creating a 
corresponding Virtual Machine Worker Process (VMWP.exe). The VMMS manages the VM snapshots 
by redirecting the snapshot requests to the VMWP process in case the VM is running or by taking the 
snapshot itself in the opposite case. 
The VMWP performs various virtualization work that a typical monolithic hypervisor would per-
form (similar to the work of a software-based virtualization solution). This means managing the state 
machine for a given child partition (to allow support for features such as snapshots and state transi-
tions), responding to various notifications coming in from the hypervisor, performing the emulation 
of certain devices exposed to child partitions (called emulated devices), and collaborating with the VM 
service and configuration component. The Worker process has the important role to start the virtual 
motherboard and to maintain the state of each virtual device that belongs to the VM. It also includes 
components responsible for remote management of the virtualization stack, as well as an RDP compo-
nent that allows using the remote desktop client to connect to any child partition and remotely view its 
user interface and interact with it. The VM Worker process exposes the COM objects that provide the 
interface used by the Vmms (and the VmCompute service) to communicate with the VMWP instance 
that represents a particular virtual machine.
The VM host compute service (implemented in the Vmcompute.exe and Vmcompute.dll binaries) is 
another important component that hosts most of the computation-intensive operations that are not 
implemented in the VM Manager Service. Operation like the analysis of a VM’s memory report (for 
dynamic memory), management of VHD and VHDX files, and creation of the base layers for containers 
are implemented in the VM host compute service. The Worker Process and Vmms can communicate 
with the host compute service thanks the COM objects that it exposes.
The Virtual Machine Manager Service, the Worker Process, and the VM compute service are able to 
open and parse multiple configuration files that expose a list of all the virtual machines created in the 
system, and the configuration of each of them. In particular:
I 
The configuration repository stores the list of virtual machines installed in the system, their