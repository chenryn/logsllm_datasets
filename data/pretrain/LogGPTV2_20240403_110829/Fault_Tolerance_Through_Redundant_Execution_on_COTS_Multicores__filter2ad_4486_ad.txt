1000
3
1
0
0
0
304
692
4
996
184k
1000
1
0
0
0
0
304
695
1
999
91k
2340
11
6
2
0
0
1238
1083
19
2321
92k
2340
16
4
2
0
3
1184
1131
22
2318
86k
2500
22
10
0
0
3
1632
833
32
2468
89k
2500
13
24
0
0
2
1675
786
37
2463
Injected faults
Observed errors
YCSB corrup
YCSB errors
User mem faults
Other user faults
Kernel exceptions
Barrier timeouts
Signature mism
Uncontrolled
Controlled
205k
1000
3
3
0
0
0
536
458
6
994
memory, not just the primary. Furthermore, we avoid the kernel
exceptions observed on x86 by adding barriers to the kernel’s
exception handler.
We observe that the failure rates are reduced in all four
conﬁgurations, although the change is not statistically signif-
icant. Importantly, there are no kernel exceptions. According
to the log, there were several kernel-data aborts, which were
caught by barrier timeouts, indicating that this mechanism is
effective.
We show a further conﬁguration, labelled LC-D-N/LC-T-N,
where we do not include checksums of output network pack-
ages in the state signature. It shows a dramatically increased
failure rate of 39% for DMR and 31% for TMR, meaning
the beneﬁt of replication is dramatically reduced. This clearly
demonstrates the importance of allowing drivers to contribute
data into the state signature to trigger a vote.
We ﬁnally observe that the CC variants have about double
the kernel-barrier timeout rates of the LC variants. CC-RCoE
is sensitive to execution divergence, which is results in incon-
sistent numbers of branches, which can result in the replicas
voting an incorrect leader.
2) CPU register faults: Standard fault-injection experi-
ments target memory. However, cosmic rays are not partic-
ularly discriminatory and can also cause faults in registers,
which are much harder to defend against.
To inject faults into user registers we utilise the fact that on
an interrupt, the kernel preempts the running thread and saves
its context. We pick a random bit in the saved user register
state and ﬂip it, then restore context and continue the user
program. We only inject into the primary replica.
As workload we run the CPU-bound md5sum from
BusyBox [49], which implements the MD5 [50] algorithm
and produces a 128-bit hash value of a ﬁle. Secure hashes
like MD5 are designed to be sensitive to random bit ﬂips. We
run MD5 [50] in a Linux VM on the base system, as well as
a CC-RCoE DMR VM conﬁguration.
For each run we generate a ﬁle ﬁlled with 128 MiB random
data, and its fault-free digest. We then run md5sum in a loop,
each iteration computing a digest and comparing it with the
correct value. If the digests differ we count this as a data
corruption, while we count abnormal termination as a crash.
At each run we inject faults until the digests differ, the
application crashes, or CC-RCoE detects a divergence, up
to a total of 2500 errors. Table VIII shows the results. We
ﬁnd that the CC-RCoE DMR VM setup is 100% effective in
detecting divergence without ever producing corrupted outputs,
with most corruption caught by voting on signatures, and 4%
by timeouts resulting from control-ﬂow corruption.
3) Physically-induced CPU faults: The standard practice of
software fault injection has a degree of artiﬁciality, as it can
only reveal faults in explicitly targeted components, and may
not be representative of SEUs produced by ionising radiation.
Ideally this would be complemented by radiation experiments,
but we were unable to access a suitable radiation source.
Instead we resort
to overclocking the processor, which
is known to degrade reliability [51], and is not limited to
software-accessible parts of the system. We do not claim that
faults resulting from overclocking are representative of SEUs,
REGISTER FAULT INJECTIONS ON M D5S U M ON X86.
TABLE VIII
Total Injected
Crashes
Corruptions
Timeouts
Mismatches
Uncontrolled Errors
Controlled Errors
Base CC-D
2812
2872
887
0
0
1613
99
N/A
2401
N/A
2500
0
2500
0
196
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:13:08 UTC from IEEE Xplore.  Restrictions apply. 
ERRORS RESULTING FROM OVERCLOCKING THE ARM BOARD.
TABLE IX
TIME (MICROSECONDS) TAKEN FOR ERROR RECOVERY.
TABLE X
Observed errors
User memory fault
User other exception
Kernel exceptions
YCSB corruptions
YCSB errors
Timeouts
Signature mismatches
Uncontrolled
Controlled
Base LC-D LC-T
1000
1000
0
632
0
345
0
2
0
1
20
24
853
N/A
123
N/A
24
1000
N/A
976
1000
0
0
0
0
25
724
251
25
975
only that they test a different scenario. In fact, overclocking
is likely to ﬁrst affect parts of the circuitry that are, as a
result of random ﬂuctuations in the manufacturing process,
closest to the borders of design-tolerance windows. As such,
overclocking is more likely to cause multiple faults in the
same circuitry within a short period, which is a much more
pessimistic scenario than the true randomness of SEUs.
We overclock our Arm system at 1.092 GHz, 9% above the
manufacturer-speciﬁed maximum rate. Again we run the Redis
benchmark of Section V-B, Table IX shows LC results.
We ﬁnd quite a different error pattern compared to the
Arm results of Table VII, with user-mode errors dominating.
The system crashes very quickly, usually before entering the
actual benchmarking stage. In the RCoE conﬁgurations, 2.5%
of errors are not detected but lead to externally observable
failures. Inspection of the logs reveal that in 6 of these 49
cases overclocking caused system reboots, the remaining 43
were network exceptions, indicating an unresponsive system.
The overclocking experiments reveal an important, although
expected, limitation of our RCoE schemes: when multiple
components experience errors within a short time window,
the system may enter the state of complete failure that is be-
yond the capability of our software-implemented mechanisms,
which are based on the assumption that the hardware functions
mostly correctly. This kind of failure is easy to handle with
a watchdog. Also,
this scenario is unlikely to occur with
radiation, unless it is very intense.
LC Primary
LC Other
CC Primary
CC Other
s
/
s
n
o
i
t
c
a
s
n
a
r
t
k
 55
 50
 45
 40
 35
 30
 10
 20
 30
 40
Seconds
 50
 60
 70
Fig. 4. Redis throughputs with error masking on x86.
197
LC primary LC other CC primary CC other
3
N/A
2,869
N/A
532
2,621
8
21
x86
Arm
D. Error Recovery
We now examine a TMR system, running the Redis bench-
mark, recovering from a signature mismatch by downgrading
to DMR. We distinguish between triggering an error in the
primary, where the system needs to select a new primary, and
another replica, where this step is not needed.
Fig. 4 shows reported YCSB-B throughput on x86 every
10 seconds. The graph shows a throughput
increase after
the error is triggered about 50 seconds after YCSB started.
This is the result of the downgrade to the lower-overhead
DMR. The actual recovery operation is not visible in the
throughput graphs, nor do we see a performance difference
between removing the primary or another replica.
Table X shows the measured cost for recovery on x86 as
well as Arm (remember from Section IV-A that we cannot
implement downgrading for CC on our 32-bit Arm platform).
Removing the primary is two orders of magnitude more
expensive than removing another replica, but the overall cost
is still only about 2 ms, small enough to be virtually invisible
from the outside.
VI. DISCUSSION
To understand how RCoE-provided fault tolerance compares
to the use of radiation-hardened hardware, we compare our
Cortex-A9 processor with the widely used RAD750 proces-
sor [52]. Running at 133 MHz, the RAD750 achieves around
240 Dhrystone 2.1 MIPS (DMIPS) and draws less than 6 W
of power [53], or 40 DMIPS/W.
The Cortex-A9 achieves 2.50 DMIPS per MHz per
core [54], or 2,000 DMIPS per core at 800 MHz. We get
2,000 DMIPS overall if we use three cores for TMR and leave
the remaining core idle. Even if we pessimistically assume a
factor-two performance overhead for TMR, the system can
achieve 1000 DMIPS with a total power draw of 5 W, or
200 DMIPS/W. (Power is measured for the whole SABRE
Lite board [55], which over-estimates the power draw of the
processor.)
This is over ﬁve times the energy efﬁciency of the RAD750.
Each RAD750 processor costs around $ 200,000 [56], while
the price tag for a SABRE Lite board is around $ 200 [57]. The
RAD750 processors can tolerate a more stringent operating
environment
in terms of temperature ranges and radiation
intensities, but within the SABRE’s operating range, RCoE
clearly has strong SWaP and cost advantages.
In the current form, the highly-variable overhead suggest
that CC-RCoE will add signiﬁcant pessimism in the worst-case
execution-time (WCET) analysis for hard real-time systems.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:13:08 UTC from IEEE Xplore.  Restrictions apply. 
We observe that reducing the number of breakpoint exceptions
when synchronising the replicas is vital to improve the per-
formance CC-RCoE. We plan to explore using performance
counter interrupts instead of breakpoints when a catching-
up replica needs to cover a large number of branches, and
switching to breakpoints when the remaining branch number
is smaller than a predeﬁned value [38]. Furthermore, for a
particular system, we can reduce overheads by proﬁling ofﬂine
to identify preemption points that are not in tight loops.
Although the physical memory is split and assigned to the
replicas equally, several small memory regions are shared
among the replicas to implement the RCoE framework (in-
cluding barriers and checksums) harnessing the replicas and
buffers for replicating input data (Section III-E). Errors in
the framework region mostly result in barrier timeouts or
checksum mismatches, but we have not performed a complete
analysis of all the possible scenarios. Errors in the input buffers
can cause silent data corruptions, manifested as data corruption
errors in the Redis fault injection experiments.
VII. RELATED WORK
There are a number of approaches to software-implemented
fault tolerance in the literature, providing redundancy at the
instructions, processes, or virtual-machine level.
SWIFT [58] is a compiler-based solution for Itanium 2 pro-
cessors. It detects transient hardware faults using a modiﬁed
compiler, which duplicates instructions in order to recompute
results with different registers. SWIFT assumes memory and
caches are protected by ECC so that store instructions are not
replicated. HAFT [59] takes a hybrid approach, combining
compiler-based instruction-level replication for error detection
with Intel’s TSX transactional memory support for error recov-
ery. Like SWIFT, HAFT does not duplicate memory load/store
instructions, assumes ECC memory, and also assumes that
the Linux kernel operates correctly, a courageous assumption
even in the absence of hardware faults [60]. Also, SEC-
DED ECC memory is insufﬁcient to protect modern memory
systems [22].
Wang et al. [61] exploit multiple cores, with a research
version of Intel’s ICC 9.0 compiler automatically generating