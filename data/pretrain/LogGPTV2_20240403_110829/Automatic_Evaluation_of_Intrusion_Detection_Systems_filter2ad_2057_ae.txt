it.  This  explains  why  Bro 
fragmentation  problem 
(frag-7-windows) 
to  detect 
in 
5.7  Results Discussion 
Based  on  the  data  sets  we  propose,  it  seems  that  the 
Snort rules enhancement by Bro provide very good results 
to reduce false positives. It is clear by looking at the results 
from  our  data  sets  that  Snort  detects  attempts  rather  than 
intrusions. In fact, Table 2 to Table 4 show that even if the 
attack  failed,  Snort  still  raises  an  alarm  for  68  of  the  84 
VEP that provided results.  
A  statistical  analysis  of  the  Snort  rule  set  seems  to 
confirm  this  hypothesis:  Only  379  of  the  3202  rules  use 
flowbits. 2428  of these rules are client  to server rules and 
167  are  server  to  client  rules  (specific  reaction  from  the 
server when attacked). Thus, a significant part of the Snort 
rule  set  only  looks  at  attacks  from  the  client  to  the  server 
(mostly  attempts  detection)  and  only  a  few  of  them  use 
flowbits or look at an effect from the server.  
Figure 6 presents the comparative analysis of Snort and 
Bro  detection  rates  in  the  case  of  successful  and  failed 
attacks.  From  Figure  6  (a),  we  can  see  that  Snort  has  a 
better detection rate than Bro for successful attacks (Bro is 
missing  the  IGMP  attacks  and  those  checked  using 
Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06)0-7695-2716-7/06 $20.00  © 2006Snort 2.3.2 
Alarm. & Compl. Det. 
Alarm. & Compl. Det. 
Alarm. & Compl. Det. 
Alarm. & Compl. Det. 
Alarm. & Compl. Det. 
Alarm. & Compl. Det. 
Alarm. & Compl. Det. 
Alarm. & Compl. Det. 
Part. Alarm. & Compl. Det 
Part. Alarm. & Compl. Det 
Part. Alarm. & Compl. Det 
Fragmentation 
frag-1 
frag-2 
frag-3 
frag-4 
frag-5 
frag-6 
frag-7-unix 
frag-7-win32 
tcp-5 
tcp-7 
tcp-9 
HTTP Encoding 
Unicode URI 
Alarm. & Compl. Det. 
Unicode URI (no /)  Alarm. & Compl. Det. 
Alarm. & Compl. Det. 
Null Method 
Alarm. & Compl. Det. 
Fake Parameter 
Session Slicing 
Alarm. & Compl. Det. 
Prepend long string  Alarm. & Compl. Det. 
Premature Ending 
Alarm. & Compl. Det. 
Self Reference 
Alarm. & Compl. Det. 
ReverseTransversal  Alarm. & Compl. Det. 
Case Sensitive 
Alarm. & Compl. Det. 
Bro 0.9a9 
Part. Alarm. & Compl. Det. 
Part. Alarm. & Compl. Det. 
Part. Alarm. & Compl. Det. 
Part. Alarm. & Compl. Det. 
Part. Alarm. & Compl. Det. 
Part. Alarm. & Compl. Det. 
Part. Alarm. & Compl. Det. 
Part. Alarm. & Compl. Det. 
Part. Alarm. & Compl. Det. 
Part. Alarm. & Compl. Det. 
Part. Alarm. & Compl. Det. 
Quiet & Compl. Eva. 
Quiet & Compl. Eva. 
Quiet & Part. Eva. 
Quiet & Compl. Det. 
Quiet & Compl. Det. 
Quiet & Compl. Det. 
Quiet & Compl. Det. 
Quiet & Compl. Det. 
Quiet & Compl. Det. 
Quiet & Compl. Det. 
IDS  evasion 
recover  from  attacks.  It  is  flexible  (it  can  easily 
include 
techniques  on  attacks), 
updateable  (it  can  easily  incorporate  new  target 
configurations  and  new  attacks)  and  completely 
automated.  We 
that  properly 
documented  traffic  traces  can  be  used  with  our 
intrusion  detection  system  evaluation  framework  to 
automatically test and evaluate IDS. 
showed 
also 
instance, 
limitations.  For 
The  current  strategies  used  to  generate  the  IDS 
data  sets  have 
the 
explosion  of  all  possible  evasions  applicable  on  an 
attack is infinite, it is difficult to know if our attacks 
are  representative  of  attacks  commonly  used  on  the 
Internet  and  if  the  variation  we  used  to  exploit  a 
vulnerability is sufficient to test and evaluate an IDS 
even  if  we  produce  a  larger  data  set  than  the  other 
proposed  solutions.  We  are  currently  working  to 
address these issues. 
It  is  also  difficult  to  properly  address  the 
performance  issues  of IDS with the current data set 
because  the  infrastructure  is  virtual.  Moreover,  a 
problem  is  often  seen  with  off-line  analysis  of  IDS 
using  recorded  traffic  traces  when  you  evaluate 
reactive IDS (or IDS that respond to attack). In both cases, 
we  believe  that  our  data  set  could  still  be  used  as  a 
building  block  to  resolve  IDS  testing  problems.  The 
recorded  attacks  could  be  used  with  tools  such  as 
TCPReplay  and  Opera  to  increase  the  throughput.  Then, 
this  stream  could  be  inserted  into  normal  traffic  or  traffic 
generated  by  IDS  stimulators  to  test  IDS  performance 
under stress. In the case of reactive IDS, a system such as 
Tomahawk  [43]  could  be  used  in  combination  with  our 
recorded data set to partially address this problem.  
Finally, by periodically updating and sharing the attack 
traces  we  generated  with  the  research  community  in 
network security we could provide a common reference to 
evaluate intrusion detection systems. 
To conclude, to the best knowledge of the authors, this 
is  the  first  attempt  to  automatically  and  systematically 
produce  attack  traces  and  make  the  results  publicly 
available to the research community since the DARPA data 
sets. 
To  obtain  a  version  of  the  data  set,  send  an  e-mail  to 
networksystem-security@crc.ca. 
Table 5. IDS evasion results 
incorrect enhanced rules). However, from Figure 6 (b) it is 
clear that Bro raised fewer false alarms than Snort. 
We  conclude  that  it  is  important  for  IDS  to inform the 
administrator,  even  if  the  attack  is  likely  to  fail.  One 
problem  of  automatically  evaluating  IDS  is  to  make  a 
distinction  between  true  negative  and  no  detection  at  all. 
When the IDS does not provide us with any indication that 
an attack attempt was made against a system, it is difficult 
to know if the IDS did not provide us any message because 
it  found  that  the  attempt  failed  or  because  it  did  not 
recognized an attempt had been tried. 
62%
62%
38%
38%
27%
27%
73%
73%
Snort
Snort
Bro
Bro
TP
TP
FN
FN
50%
50%
83%
83%
50%
50%
17%
17%
Snort
Snort
Bro
Bro
TN
TN
FP
FP
(a) Successful Attacks
(a) Successful Attacks
(b) Failed Attacks
(b) Failed Attacks
Figure 6. Detection rate analysis 
References 
6  Conclusion 
We  have  described  a  Virtual  Network  Infrastructure  to 
create  large  scale,  clearly  documented  data  sets  for 
Intrusion  Detection  Systems  (IDS)  testing  and  evaluation. 
It  allows  us  to  record  network  traffic  produced  during 
attacks, control the network (e.g., traffic noise), control the 
attack 
various 
heterogeneous  target  system  configurations,  and  quickly 
(confinement), 
propagation 
use 
1.  Lincoln Laboratory Massachusetts Institute of Technology: 
DARPA Intrusion Detection Evaluation (2006) 
2.  CAIDA: Cooperative association for internet data analysis 
(2006) 
3.  NLANR: National laboratory of network applied research, 
passive measurement analysis project (2006) 
4.  Mell, P., Hu, V., Lipmann, R., Haines, J., Zissman, M.: An 
Overview of Issues in Testing Intrusion Detection Systems. 
Technical Report NIST IR 7007, NIST (2006) 
Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06)0-7695-2716-7/06 $20.00  © 20065.  Beale, J., Foster, J.C.: Snort 2.0 Intrusion Detection. 
28. Leita, C., Mermoud, K., Dacier, M.: ScriptGen: an automated 
Syngress Publishing (2003) 
6.  Paxson, V.: Bro: A System for Detecting Network Intruders 
in Real-Time. Computer Networks 31 (1999) 2435–2463 
script generation tool for honeyd. In: ACSAC 2005, 21st 
Annual Computer Security Applications Conference, 
December 5-9, 2005, Tucson, USA. (2005) 
7.  Athanasiades, N., Abler, R., Levine, J., Owen, H., Riley, G.: 
29. Vrable, M., Ma, J., Chen, J., Moore, D., VandeKieft, E., 
Intrusion Detection Testing and Benchmarking 
Methodologies. Proc. IEEE International Workshop on 
Information Assurance (IWIA’03) (2003) 
8.  Mutz, D., Vigna, G., Kemmerer, R.A.: An Experience 
Snoeren, A.C., Voelker, G.M., Savage, S.: Scalability, 
Fidelity and Containment in the Potemkin Virtual 
Honeyfarm. Proc. ACM Symposium on Operating System 
Principles (SOSP). (2005) 
Developing an IDS Stimulator for the Black-Box Testing of 
Network Intrusion Detection Systems. Proc. Annual 
Computer Security Applications Conference (2003) 
30. Jiang, X., Xu, D., Wang, H.J., Spafford, E.H.: Virtual 
Playgrounds for Worm Behavior Investigation. Proc. RAID 
(2005) 
9.  Sniph: Snot. www.securityfocus.com/tools/1983 (2006) 
10. Aubert, S.: IDSWakeup. 
31. VMWare Inc: Vmware. http://www.vmware.com (2005) 
32. Dike, J.: The User-mode Linux Kernel Home Page.  
www.hsc.fr/ressources/outils/idswakeup/ (2006) 
11. Giovanni, C.: Fun with Packets: Designing a Stick. 
packetstormsecurity.nl/distributed/stick.htm (2006) 
user-mode-linux.sourceforge.net/ (2005) 
33. Bochs: Bochs homepage. bochs.sourceforge.net/ (2006) 
34. SecurityFocus: SecurityFocus Homepage. 
12. The NSS Group: Intrusion Detection Systems Group Test 
www.securityfocus.org/ (2005) 
35. Project, M.: Metasploit. http://www.metasploit.com (2006) 
36. Anderson, H.: Introduction to nessus. 
www.securityfocus.com/infocus/1741 (2003) 
37. Barber, J.J.: Operator. ussysadmin.com/operator/ (2006) 
38. Massicotte, F.: Using Object-Oriented Modeling for 
Specifying and Designing a Network-Context sensitive 
Intrusion Detection System. Master’s thesis, Department of 
Systems and Computer Eng., Carleton University (2005) 
39. K2: ADMmutate. www.ktwo.ca/security.html (2006)  
40. Rain Forest Puppy: Whisker. 
www.wiretrip.net/rfp/txt/whiskerids.html (2006) 
41. CIRT.net: Nikto. www.cirt.net/code/nikto.shtml (2006) 
42. Song, D.: Fragroute 1.2. 
www.monkey.org/_dugsong/fragroute/ (2006) 
43. Smith, B.: Tomahawk. tomahawk.sourceforge.net/ (2006) 
(Edition 4) (2003) 
13. Vigna, G., Robertson, W., Balzarotti, D.: Testing network-
based intrusion detection signatures using mutant exploits. 
Proc. ACM conference on Computer and communications 
security (2004) 21 – 30 
14. Timm, K.: IDS Evasion Techniques and Tactics. 
www.securityfocus.com/infocus/1577 (2002) 
15. Hacker, E.: IDS Evasion with Unicode. 
www.securityfocus.com/infocus/1232 (2001) 
16. Marty, R.: THOR - a tool to test intrusion detection systems 
by variations of attacks. Master’s thesis, ETH Zurich (2002)  
17. Handley, M., Kreibich, C., Paxson, V.: Network Intrusion 
Detection: Evasion, Traffic Normalization, and End-to-End 
Protocol Semantics (HTML). Proc. USENIX Security 
Symposium 2001 (2001) 
18. Roelker, D.: HTTP IDS Evasions Revisited. 
docs.idsresearch.org/http_ids_evasions.pdf (2006) 
19. Ptacek, T., Newsham, T.: Insertion, Evasion, and Denial of 
Service: Eluding Network Intrusion Detection. Technical 
report, Secure Networks (1998) 
20. The NSS Group: Intrusion Detection Systems Group Test 
(Edition 3). (2002) 
21. Yocom, B., Brown, K.: Intrusion battleground evolves. 
Network World Fusion (2001) 53–62 
22. Mueller, P., Shipley, G.: Cover story: dragon claws its way to 
the top. Netw. Comput. 12 (2001) 45–67 
23. Rossey, L.M., Cunningham, R.K., Fried, D.J., Rabek, J.C., 
Lippmann, R.P., Haines, J.W., Zissman, M.A.: LARIAT: 
Lincoln Adaptable Real-time Information Assurance Testbed. 
Proc. IEEE Aerospace Conference (2002) 
24. Debar, H., Morin, B.: Evaluation of the Diagnostic 
Capabilities of Commercial Intrusion Detection Systems. 
Proc. RAID (2002). 
25. McHugh, J.: Testing Intrusion Detection Systems: A Critique 
of the 1998 and 1999 DARPA Intrusion Detection System 
Evaluations as Performed by Lincoln Laboratory. ACM 
Trans. on Information and System Security 3 (2000) 
26. De Montigny-Leboeuf, A.: A Multi-Packet Signature 
Approach to Passive Operating System Detection. 
CRC/DRDC Technical Report CRC-TN-2005-001 / DRDC-
Ottawa-TM-2005-018 (2004) 
27. Project, LEURRE.: Eurecom. http://www.leurrecom.org 
(2006) 
Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06)0-7695-2716-7/06 $20.00  © 2006