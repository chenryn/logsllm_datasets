### 5.7 Results Discussion

Based on the proposed data sets, it appears that the enhancement of Snort rules by Bro significantly reduces false positives. The results from our data sets indicate that Snort is more effective at detecting attack attempts rather than actual intrusions. For instance, Tables 2 to 4 show that even when an attack fails, Snort still raises an alarm for 68 out of 84 VEPs (Virtual Environment Packets) that provided results.

A statistical analysis of the Snort rule set supports this hypothesis: only 379 out of 3202 rules use flowbits. Specifically, 2428 of these rules are client-to-server rules, and 167 are server-to-client rules (indicating specific reactions from the server when attacked). This suggests that a significant portion of the Snort rule set focuses on detecting attacks from the client to the server, primarily identifying attempts, with only a few rules utilizing flowbits or examining server responses.

Figure 6 presents a comparative analysis of Snort and Bro detection rates for both successful and failed attacks. From Figure 6(a), we observe that Snort has a higher detection rate than Bro for successful attacks (Bro misses IGMP attacks and those checked using Snort 2.3.2). However, as shown in Figure 6(b), Bro generates fewer false alarms compared to Snort.

### 6. Conclusion

We have described a Virtual Network Infrastructure designed to create large-scale, well-documented data sets for testing and evaluating Intrusion Detection Systems (IDS). This infrastructure allows us to record network traffic during attacks, control the network environment (e.g., traffic noise), manage various heterogeneous target system configurations, and quickly respond to and contain attacks.

It is important for IDS to inform administrators about potential attack attempts, even if the attack is likely to fail. A key challenge in automatically evaluating IDS is distinguishing between true negatives and no detection at all. When an IDS does not provide any indication of an attack attempt, it is difficult to determine whether the IDS did not detect the attempt or if it recognized the attempt but determined it was unsuccessful.

### References

1. Lincoln Laboratory, Massachusetts Institute of Technology: DARPA Intrusion Detection Evaluation (2006)
2. CAIDA: Cooperative Association for Internet Data Analysis (2006)
3. NLANR: National Laboratory of Network Applied Research, Passive Measurement Analysis Project (2006)
4. Mell, P., Hu, V., Lipmann, R., Haines, J., Zissman, M.: An Overview of Issues in Testing Intrusion Detection Systems. Technical Report NIST IR 7007, NIST (2006)
5. Beale, J., Foster, J.C.: Snort 2.0 Intrusion Detection. Syngress Publishing (2003)
6. Paxson, V.: Bro: A System for Detecting Network Intruders in Real-Time. Computer Networks 31 (1999) 2435–2463
7. Athanasiades, N., Abler, R., Levine, J., Owen, H., Riley, G.: Intrusion Detection Testing and Benchmarking Methodologies. Proc. IEEE International Workshop on Information Assurance (IWIA’03) (2003)
8. Mutz, D., Vigna, G., Kemmerer, R.A.: An Experience Developing an IDS Stimulator for the Black-Box Testing of Network Intrusion Detection Systems. Proc. Annual Computer Security Applications Conference (2003)

[... Additional references continue ...]

### Acknowledgments

This work was supported by [funding sources, if any]. We would like to thank [acknowledge any individuals or organizations that contributed to the research].

### Contact Information

To obtain a version of the data set, please send an e-mail to networksystem-security@crc.ca.

---

This revised text aims to improve clarity, coherence, and professionalism, making it more suitable for academic and technical audiences.