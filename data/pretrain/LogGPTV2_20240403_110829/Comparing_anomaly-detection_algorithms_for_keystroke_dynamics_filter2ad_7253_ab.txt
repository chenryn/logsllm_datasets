zero-miss
heuristic
equal-error
0.25 16.36
8.1(a)
2.8
2.8
8.1
19.5
0.0
1.0
0.0
11.(b)
19.
20.
22.
13.
2.
15.78
0.0
1.45
1.89
3.8
3.8
1 Joyce & Gupta (1990) [10]
Bleha et al. (1990) [2]
2
Cho et al. (2000) [4] (cid:2)
(cid:2)
3
(cid:2)
(cid:2)
(cid:2)
4
5
6
7
Haider et al. (2000) [8]
Yu & Cho (2003) [21] N/A
Araujo et al. (2004) [1]
Kang et al. (2007) [11] N/A
N/A
N/A
Table 1. Seven different studies investigate 11 different anomaly detectors and report evaluation
results. The diversity of evaluation conditions makes a direct comparison of the anomaly-detector
performance results impossible. Section 2.2 details the contents of each column of the table.
data; (4) was given only one attempt (not two) to correctly
verify the user; and (5) was assessed using a different type
of threshold on the anomaly score. Any of these ﬁve factors
might explain why the neural net has lower error rates than
the outlier-counting detector. If these factors were all con-
trolled, we might discover that the outlier-counting detector
outperforms the neural net (as reported later in this work;
see Table 2).
3. Problem and approach
As illustrated in the previous section, it is unsound to
compare anomaly detectors using the evaluation results re-
ported in the literature. Too many factors differ from one
evaluation to another to make sensible conclusions about
the relative performance of two detectors.
Our objective in this paper is to collect a keystroke-
dynamics data set, to develop an evaluation procedure, and
to measure the performance of a range of anomaly-detection
algorithms so that the results can be compared on an equal
basis. In the process, we establish which detectors have the
lowest error rates on these data, and we provide a data set
and evaluation methods that can be shared throughout the
research community to evaluate new detectors and assess
progress. Our approach is as follows:
1. Password-data collection: We collected typing data
from 51 subjects, each typing 400 repetitions of a pass-
word. The various timing features used by researchers (e.g.,
the keydown-keydown times and hold times) were extracted
from the raw data.
2. Detector implementation: Fourteen anomaly detectors
from the literature were reimplemented:
eleven had
been proposed by keystroke-dynamics researchers, and
three were “classic” anomaly detectors from the pattern-
recognition literature.
3. Evaluation methodology: We developed a procedure to
evaluate each detector, on the same data, under the same
conditions, so that performances can be compared. The er-
ror rates of the detectors were calculated and tabulated.
In Sections 4–6, we describe the three steps of our
methodology in more detail.
4. Password-data collection
The ﬁrst step in our evaluation was to collect a sample
of keystroke-timing data. In this section, we explain how
we chose a password to use as a typing sample, designed
a data-collection apparatus, recruited subjects to type the
password, and extracted a set of password-timing features.
4.1. Choosing a password
Choosing passwords for a keystroke-dynamics evalua-
tion is tricky. On one hand, it is often more realistic to
let users choose their own passwords. On the other hand,
data collection becomes more difﬁcult since different im-
postor samples would be needed for each password. Some
researchers have suggested that letting users choose their
own passwords makes it easier to discriminate them [1]. If
true, then letting users choose their own passwords would
bias the results of an experiment designed to evaluate per-
formance on an arbitrary password. We decided that the
same password would be typed by all of our subjects.
To make a password that is representative of typical,
strong passwords, we employed a publicly available pass-
word generator [16] and password-strength checker [13].
We generated a 10-character password containing letters,
numbers, and punctuation, and then modiﬁed it slightly, in-
terchanging some punctuation and casing to better conform
with the general perception of a strong password. The result
of this procedure was the following password:
.tie5Roanl
The password-strength checker rates this password as
strong because it contains more than 7 characters, a capital
letter, a number, and punctuation. The top rating is reserved
for passwords longer than 13 characters, but according to
the studies that were presented in Table 1, 10 characters is
typical. Those studies that used longer strings often used
names and English phrases that are easier to type.
4.2. Data-collection apparatus
We set up a laptop with an external keyboard to collect
data, and developed a Windows application that prompts a
subject to type the password. The application displays the
password in a screen with a text-entry ﬁeld.
In order to
advance to the next screen, the subject must type the 10
characters of the password correctly, in sequence, and then
press Enter. If any errors in the sequence are detected, the
subject is prompted to retype the password. The subject
must type the password correctly 50 times to complete a
data-collection session. Whenever the subject presses or re-
leases a key, the application records the event (i.e., keydown
or keyup), the name of the key involved, and what time the
event occurred. An external reference clock was used to
generate highly accurate timestamps. The reference clock
was demonstrated to have an accuracy of ±200 microsec-
onds (by using a function generator to simulate key presses
at ﬁxed intervals).
Of course, subjects would not naturally type their pass-
word 50 times in a row, and they would type it on their own
computers, not our keyboard. We chose to sacriﬁce some
amount of realism so we could use this carefully-controlled
data-collection apparatus. We had two reasons for this de-
cision. First, we wanted to ensure the accuracy of the time-
stamps (as described above). Second, we wanted to make
the environment as consistent as possible for all subjects. If
some subjects typed the password more frequently than oth-
ers, or if different subjects used different keyboards, these
differences could make it artiﬁcially easier for an anomaly
detector to distinguish between typists.
4.3. Running subjects
We recruited 51 subjects from within the university. Sub-
jects completed 8 data-collection sessions (of 50 passwords
each), for a total of 400 password-typing samples. They
waited at least one day between sessions, to capture some
of the day-to-day variation of each subject’s typing.
Our set of subjects consisted of 30 males and 21 females.
We had 8 left-handed and 43 right-handed subjects. The
median age group was 31–40, the youngest was 18–20 and
the oldest was 61–70. The subjects’ sessions took between
1.25 and 11 minutes, with the median session taking about
3 minutes.
4.4. Extracting timing vectors
The raw typing data (e.g., key events and timestamps)
cannot be used directly by an anomaly detector. Instead,
sets of timing features are extracted from the raw data.
These features are typically organized into a vector of times
called a timing vector. Different researchers extract differ-
ent combinations of features (as shown in the Feature Sets
columns of Table 1).
We decided to use all the features used across all of
the studies shown in Table 1. Speciﬁcally, we considered
the Enter key to be part of the password (effectively mak-
ing the 10-character password 11 keystrokes long), and we
extracted keydown-keydown times, keyup-keydown times,
and hold times for all keys in the password. For each pass-
word, 31 timing features were extracted and organized into
a vector. The times are stored in seconds (as ﬂoating-point
numbers).
Many of the timing features are correlated and some are
linearly dependent (i.e., each keydown-keydown time can
be decomposed into the sum of a hold time and a keyup-
keydown time). We did not transform the data to remove
these correlations despite their adverse effect on some de-
tectors because they are typical of keystroke timing data.
Other researchers have investigated strategies to compen-
sate through feature selection [21]. By retaining all the fea-
tures, we expect our timing data could be useful in a future
evaluation of such work.
5. Detector implementation
The second step in our evaluation was to implement
14 anomaly-detection algorithms that analyze password-
timing data. We tried to faithfully reimplement the eleven
algorithms listed in the Detector column of Table 1. We
also implemented three “classic” anomaly-detection meth-
ods (i.e., the Euclidean, Manhattan, and Mahalanobis dis-
tance metrics) from the pattern-recognition literature.
All of these detectors have been described in compre-
hensive detail elsewhere, and due to space constraints we
cannot include the same detail. However, we provide a con-
cise explanation (and a citation to a more detailed descrip-
tion) for each detector. We note ambiguities which could
inadvertently cause our reimplementation to differ from the
original. Occasionally, we must resort to terminology that is
speciﬁc to a statistical or machine-learning technique (e.g.,
the vocabulary of neural networks); references are provided.
The 14 detectors were implemented using the R statis-
tical programming environment (version 2.6.2) [18]. Each
detector has a training phase where a set of timing vectors
from the genuine user is used to build a model of the user’s
typing behavior, and a test phase where each new test vector
is assigned an anomaly score.
Because some detectors have parameters that affect their
performance, the question of parameter tuning arises. Since
there is no commonly accepted method for tuning the pa-
rameters of anomaly detectors for a data set without intro-
ducing bias in the evaluation results [12], we used the pa-
rameters speciﬁed by the source study when possible, and
explained what parameters we used.
5.1. Euclidean
This classic anomaly-detection algorithm [5] models
each password as a point in p-dimensional space, where p
is the number of features in the timing vectors. It treats the
training data as a cloud of points, and computes the anomaly
score of the test vector based on its proximity to the center
of this cloud. Speciﬁcally, in the training phase, the mean
vector of the set of timing vectors is calculated. In the test
phase, the anomaly score is calculated as the squared Eu-
clidean distance between the test vector and the mean vec-
tor.
5.2. Euclidean (normed)
This detector was described by Bleha et al. [2] who
called it the “normalized minimum distance classiﬁer.” In
the training phase, the mean vector is calculated as in the
standard Euclidean detector. In the test phase, the squared
Euclidean distance between the test vector and the mean
vector is calculated, but the anomaly score is calculated by
“normalizing” this distance, dividing it by the product of the
norms of the two vectors (i.e., if x is the mean vector, y is
the test vector, and d is the squared Euclidean distance, then
the anomaly score is d/((cid:2)x(cid:2) (cid:2)y(cid:2))).
5.3. Manhattan
This classic anomaly-detection algorithm [5] resembles
the Euclidean detector except that the distance measure is
not Euclidean distance, but Manhattan (or city-block) dis-
tance. In the training phase, the mean vector of the timing
vectors is calculated. In the test phase, the anomaly score
is calculated as the Manhattan distance between the mean
vector and the test vector.
5.4. Manhattan (ﬁltered)
This detector was described by Joyce and Gupta [10]. It
is similar to the Manhattan detector except outliers are ﬁl-
tered from the training data. In the training phase, the mean
vector of the timing vectors is calculated, and the standard
deviation for each feature is calculated also. Any timing-
vector element that is more than three standard-deviations
above the mean is removed, and a more robust mean vector
is computed without these extreme values. In the test phase,
the anomaly score is calculated as the Manhattan distance
between this robust mean vector and the test vector.
5.5. Manhattan (scaled)
This detector was described by Ara´ujo et al. [1]. In the
training phase, the mean vector of the timing vectors is cal-
culated, and the mean absolute deviation of each feature is
calculated as well. In the test phase, the calculation is simi-
lar to the Manhattan distance, but with a small change. The
i=1 |xi − yi| /ai where xi
anomaly score is calculated as
and yi are the i-th features of the test and mean vectors re-
spectively, and ai is the average absolute deviation from the
training phase. The score resembles a Manhattan-distance
calculation, except each dimension is scaled by ai.
(cid:2)p
5.6. Mahalanobis
This classic anomaly-detection algorithm [5] resembles
the Euclidean and Manhattan detectors but the distance
measure is more complex. Mahalanobis distance can be
viewed as an extension of Euclidean distance to account for
correlations between features. In the training phase, both
the mean vector and the covariance matrix of the timing
vectors are calculated. In the test phase, the anomaly score
is calculated as the Mahalanobis distance between the mean
vector and the test vector (i.e., if x is the mean vector, y is
the test vector, and S is the covariance matrix, the Maha-
lanobis distance is (x − y)TS−1(x − y)).
5.7. Mahalanobis (normed)
This detector was described by Bleha et al. [2] who
called it the “normalized Bayes classiﬁer.” In the training
phase, the mean vector and covariance matrix of the training
vectors are calculated. In the test phase, the Mahalanobis
distance between the mean vector and test vector is calcu-
lated. The anomaly score is calculated by “normalizing”