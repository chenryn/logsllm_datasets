title:Secure Sublinear Time Differentially Private Median Computation
author:Jonas B&quot;ohler and
Florian Kerschbaum
Secure Sublinear Time Differentially Private
Median Computation
1st Jonas B¨ohler
SAP Security Research
Karlsruhe, Germany
PI:EMAIL
2nd Florian Kerschbaum
University of Waterloo
Waterloo, Canada
ﬂPI:EMAIL
Abstract—In distributed private learning, e.g., data analysis,
machine learning, and enterprise benchmarking, it is common-
place for two parties with conﬁdential data sets to compute
statistics over their combined data. The median is an important
robust statistical method used in enterprise benchmarking, e.g.,
companies compare typical employee salaries, insurance compa-
nies use median life expectancy to adjust insurance premiums,
banks compare credit scores of their customers, and ﬁnancial
regulators estimate risks based on loan exposures.
The exact median can be computed securely, however, it leaks
information about the private data. To protect the data sets, we
securely compute a differentially private median over the joint data
set via the exponential mechanism. The exponential mechanism
has a runtime linear in the data universe size and efﬁciently
sampling it is non-trivial. Local differential privacy, where each
user shares locally perturbed data with an untrusted server, is
often used in private learning but does not provide the same
utility as the central model, where noise is only applied once by
a trusted server.
We present an efﬁcient secure computation of a differentially
private median of the union of two large, conﬁdential data sets.
Our protocol has a runtime sublinear in the size of the data
universe and utility like the central model without a trusted
third party. We provide differential privacy for small data sets
(sublinear in the size of the data universe) and prune large data
sets with a relaxed notion of differential privacy providing limited
group privacy. We use dynamic programming with a static, i.e.,
data-independent, access pattern, achieving low complexity of
the secure computation circuit. We provide a comprehensive
evaluation over multiple AWS regions (from Ohio to N. Virgina,
Canada and Frankfurt) with a large real-world data set with a
practical runtime of less than 7 seconds for millions of records.
I.
INTRODUCTION
In distributed private learning two parties A, B, with
conﬁdential data sets DA, DB respectively, want to compute
statistics of their combined data. Example applications are
data analysis, machine learning, collaborative forecasting and
enterprise benchmarking. The median is an important robust
statistical method, i.e., a few outliers in the data do not skew
the result. The median is used to represent a “typical” value
from a data set and is utilized in enterprise benchmarking,
Network and Distributed Systems Security (NDSS) Symposium 2020
23-26 February 2020, San Diego, CA, USA
ISBN 1-891562-61-4
https://dx.doi.org/10.14722/ndss.2020.24150
www.ndss-symposium.org
where companies measure their performance against the com-
petition to ﬁnd opportunities for improvement. Businesses
compare, e.g., typical employee salaries per department, bonus
payments or sales incentives to better assess their attractive-
ness for the labor market, and insurance companies use the
median life expectancy to adjust insurance premiums. Further,
banks compare credit scores of their customers, and ﬁnancial
regulators estimate risks based on loan exposures.
Since the data are sensitive, e.g., salary or health informa-
tion, the parties want to compute the median without revealing
any of their data to each other. A solution to reveal the exact
median and nothing else was presented by Aggarwal et al. [1],
however, the exact median itself is a value from either DA or
DB, and, as shown in [13, 45], median queries can be used
to uncover the exact value of targeted individuals. To protect
the data sets and hinder targeted inference attacks we also use
differential privacy [16, 20]. Inference attacks [13, 45] rely
on median values from the actual data set. The differentially
private median, however, is a non-deterministic value from the
entire data universe and yet it is close to the actual median
with high probability. For small data sets (sublinear in the
size of the data universe) we provide differential privacy, and
for large data sets we ﬁrst prune the input using the relaxed
notion of differential privacy introduced in [28]. Instead of
considering neighbors, i.e., data sets differing in one record,
the relaxed notion requires neighbors to also have the same
output w.r.t. the initial input pruning. However, we provide
empirical evidence that the relaxation is not too restrictive
on real-world data sets [11, 33, 51, 54]. A trusted third
party, called curator in differential privacy literature [18], can
implement any differentially private algorithms. However, this
trusted party requires full access to the unprotected data. To
protect the inputs without relying on a trusted third party we
use secure computation [24], i.e., the parties run a protocol
to compute a function on their respective inputs such that
nothing about their input is revealed except the function result.
Google reported using secure computation to link online ads
with ofﬂine purchases [7, 32], and government institutes use
it to detect tax fraud [8] and perform studies (e.g., [9]). In our
case, we securely compute the differentially private median
via the exponential mechanism, as it provides the best accuracy
vs. privacy trade-off for low  (see our discussion in Section II).
The exponential mechanism from McSherry and Talwar [39]
selects a speciﬁc value, like the median, from a data universe
U, has a computation complexity linear in the size of the entire
data universe [39] and efﬁciently sampling it is non-trivial [18].
Also, the exponential mechanism requires exponentiations and
divisions, increasing the secure computation complexity. Pettai
and Laud [44] securely compute the differentially private
median using the framework by Nissim et al. [43]. Unlike
their work we also considered network delay and used modest
hardware1 and our protocol is still 13 times faster for millions
of records with a latency of 25 ms.
We present an efﬁcient protocol to securely compute the
differentially private median of the union of two large, con-
ﬁdential data sets with computation complexity sublinear in
the size of the data universe. First, the parties prune their
own data in a way that maintains their median. Then, they
sort and merge the pruned data. The sorted data is used to
compute selection probabilities for the entire data universe.
Finally, the probabilities are used to select the differentially
private median. To optimize the runtime of our protocol we
use dynamic programming for the probability computation
with a static, i.e., data-independent, access pattern, achieving
low complexity of the secure computation circuit. We utilize
different cryptographic techniques, garbled circuits as well as
secret sharing, to combine their respective advantages, namely,
comparisons and arithmetic computations. We simplify the
probability and sampling computations to minimize direct
access to the data, which reduces secure computation overhead.
Furthermore, we compute the required exponentiations for the
exponential mechanism without any secure computation.
In summary, the contributions of our protocol combining
secure computation and differential privacy are
•
•
•
selection of the differentially private median of the
union of two distributed data sets without revealing
anything else about the data,
an improved runtime complexity sublinear in the size
of the data universe achieved by data-independent
dynamic programming and input pruning for large
data sets,
a comprehensive evaluation with a large real-world
data set with a practical runtime of less than 7 seconds
for millions of records even with 100 ms network
delay and 100 MBits/s bandwidth.
We note that our protocol can be easily adapted to securely
compute the differentially private pth-percentile, i.e., the value
larger than p% of the data. The remainder of this paper is
organized as follows: In Section II we detail the problem
description. In Section III we describe preliminaries for our
dynamic programming protocol. In Section IV we explain
our approach and introduce deﬁnitions. Then, we present our
protocol and implementation details for the secure computation
of the differentially private median in Section V. We provide
a detailed performance evaluation in Section VI. We describe
related work in Section VII and conclude in Section VIII.
II. PROBLEM DESCRIPTION
We consider the problem of two parties computing the
differentially private median over their combined data sets.
Next, we describe implementation models and basic techniques
for differentially private algorithms.
1Our evaluation is performed with AWS t2.medium instances (4 vCPUs,
2GB RAM) compared to the 12-core 3GHZ CPU, 48GB RAM setup of [44].
C1...
Cn
C1...
Cn
v1
vn
Trusted
Server
M(f (v1, ..., vn))
(a) Central Model
r1 =M(v1)
rn =M(vn)
Untrusted
Server
f (r1, ..., rn)
(b) Local Model
r1 =M(v1)
rn =M(vn)
C1...
Cn
rπ(1)
...
rπ(n)
Untrusted
Server
Shufﬂer
f(cid:0)rπ(1), ..., rπ(n)
(cid:1)
(c) Shufﬂe Model with permutation π
Fig. 1. Models for differentially private algorithms M. Client Ci sends a
message – raw value vi or randomized ri – to a server. The server computes
some function f over the messages, and releases the differentially private
result.
A. Models for Differentially Private Algorithms
Differentially private algorithms M can be implemented
in different models which are visualized in Figure 1. In the
central model (Figure 1a) every client sends their unprotected
data to a trusted, central server which runs M on the clear
data. The central model provides the highest accuracy as the
randomization inherent to differentially private algorithms, is
only applied once. In the local model (Figure 1b), introduced
by [34], clients apply M locally and send anonymized values
to an untrusted server for aggregation. The accuracy is lim-
ited as multiple randomizations occur. It requires enormous
amounts of data, compared to the central model, to achieve
acceptable accuracy bounds [5, 12, 30, 38]. Speciﬁcally, an
exponential separation between local and central model for ac-
curacy and sample complexity was shown by [34]. Recently, an
intermediate shufﬂe model (Figure 1c) was introduced [5, 12]:
An additional party is added between clients and server in the
local model, the shufﬂer, who does not collude with anyone.
The shufﬂer permutes and forwards the randomized client
values. The permutation breaks the mapping between the client
and her value, which reduces randomization requirements. The
accuracy of the shufﬂe model lies between the local and central
model, however, in general it is strictly weaker than the central
model [12]. As our goal is high accuracy without additional
parties, this work dismisses the shufﬂe model.
To combine the beneﬁts of the local and central model,
namely, high accuracy and strong privacy, secure computation
[24] is used in related work [19, 25, 48, 52]. Secure com-
putation allows to simulate central model algorithms in the
local model. Secure computation is a cryptographic protocol
run between the clients which only reveals the computation’s
output and nothing more about their sensitive data. Hence,
secure computation of the median is superior to distributed
computation methods that reveal additional statistics (e.g.,
histograms or preﬁx query results) from which to compute a
(noisy) median. As Smith et al. [50] note, general techniques
that combine secure computation and differential privacy suffer
2
III. PRELIMINARIES
Next we introduce preliminaries for differential privacy and
secure computation, and some notation.
A. Notation
We model a database as D = {d0, d1, . . . , dn−1} ∈ U n.
We call U data universe and assume it to be an integer range,
i.e., U = {x ∈ Z | a ≤ x ≤ b} with a, b ∈ Z. We note
that rational numbers can be expressed as integers via ﬁxed-
point number representation.3 To simplify the description we
assume the size n of D to be even which can be ensured by
padding. Then, the median is the value dn/2−1 in sorted D. We
denote with ID = {0, . . . , n − 1} the set of indices for D and
refer to non-distinct data elements as duplicates, i.e., di = dj
with i (cid:54)= j (i, j ∈ ID). We apply union under bag semantics,
i.e., DA ∪ DB is a bag containing elements from U as often
as they appear in data sets DA and DB combined4. We treat
the difference of two bags, DA\DB, as a set containing only
elements from DA that are not also in DB.
B. Differential Privacy
Differential privacy introduced by Dwork, McSherry, Nis-
sim, and Smith [16, 20] is a privacy notion, adopted by major
technology companies [15, 22, 53, 55]. Differential privacy
enables one to learn statistical properties of a data set while
protecting the privacy of any individual contained in it. Data
sets D, D(cid:48) are called neighbors or neighboring, denoted with
D (cid:39) D(cid:48), when data sets D can be obtained from D(cid:48) by adding
or removing one element, i.e., D = D(cid:48) ∪ {x} with x ∈ U or
D = D(cid:48)\{y} with y ∈ D(cid:48).
Informally, a differentially private algorithm limits the
impact that the presence or absence of any individual’s data
in the input database can have on the distribution of outputs.
The formal deﬁnition is as follows:
Deﬁnition 1 (Differential Privacy). A mechanism M satisﬁes
-differential privacy, where  ≥ 0, if for all neighboring data
sets D and D(cid:48), and all sets S ⊆ Range(M)
Pr[M(D) ∈ S] ≤ exp() · Pr[M(D(cid:48)
) ∈ S],
where Range(M) denotes the set of all possible outputs of
mechanism M.
The above deﬁnition holds against an unbounded adver-
sary, however, due to our use of cryptography we assume a
polynomial-time bounded adversary. Mironov et al. [41] deﬁne
indistinguishable computationally differential privacy (IND-
CDP) for two-party computation (2PC) with computationally
bounded parties. The presented deﬁnition is according to [28]
for parties A, B with data sets DA, DB, privacy parameters
A, B and security parameter λ. Furthermore, VIEWΠ
A denotes
the view of A during the execution of protocol Π.
Deﬁnition
(IND-CDP-2PC). A two-party
protocol
Π for
(A(λ), B(λ))-
indistinguishable computationally differential privacy (IND-
A(DA,·) satisﬁes B(λ)-IND-CPA, i.e.,
CDP-2PC) if VIEWΠ
3A binary number of bit-length b can represent d ∈ Q as d(cid:48)
∈ Z if d =
≤ 2b−1 − 1 and scaling factor 2−f , f ∈ N.
· 2−f with −2b−1 + 1 ≤ d(cid:48)
4This interpretation of union is equivalent to the sum function for bags.
computing function f
satisﬁes
2
d(cid:48)
(a) Credit Card data [54], ﬁrst 105
payment records in Cents.
(b) Walmart
Chain
data [33], 175k shipment weights
as integers.
Supply
Fig. 2.
Absolute errors, averaged for 100 differentially private median
computations via exponential mechanism and Laplace mechanism with smooth
sensitivity for  ∈ {0.1, 0.25, 0.5}.
from bandwidth and liveness constraints which render them
impractical for large data sets. Our contribution is an optimized
secure protocol for the differentially private median that runs
in seconds on million of records in real-world networks.
B. Differential Privacy Techniques
Informally,
the main techniques to provide differential