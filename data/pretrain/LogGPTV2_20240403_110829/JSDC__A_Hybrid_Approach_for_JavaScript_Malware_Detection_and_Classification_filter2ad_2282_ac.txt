cases introduced in detection.
Given a set of traces from samples of attack type I, we ex-
tract the common actions that appear in all malicious traces.
Then all traces can be represented by action sequences. For
example, the trace in Figure 8 can be represented as an
action sequence (cid:104)a, b, c, d, e, f, g(cid:105). Then with all these ac-
tion sequences, RPNI is applied to infer a DFA. Further
with experts’ knowledge, the reﬁned DFA is attained and
shown in Figure 9, which generate JavaScript code dynam-
ically and exploit vulnerability of browser to download ﬁles
into user’s local ﬁle system. Given the sample s1 in Figure
3, we can capture traces that are represented as sequences
(cid:104)a, d, e, f, g(cid:105), (cid:104)a, b, c, d, e, f, g(cid:105) or (cid:104)a, b, c, b, c, d, e, f, g(cid:105). As these
traces are accepted by the learned DFA, s1 is of type I at-
tack.
Note that for each attack type, we repeat the same work-
ﬂow to learn an attack model in DFA. Totally, the eight
learned models are available from the JSDC web-site [9]. To
identify the possible attack type, the trace of a malicious
script needs to be iteratively checked against each of these
8 DFAs, until one matched is found or all are not matched.
The system call based behavior modelling is resistant to ob-
fuscation owing to the dynamic analysis, and applicable to
malware variants as it captures the essential behaviors of the
same attack.
6.
IMPLEMENTATION
In this section, we present our implementation details.
Crawler. We use the Heritrix public domain crawler [7]
to crawl over 21,000 Internet web sites. Heritrix is a open-
source, extensible, web-scale, archival-quality web crawler
project. Heritrix is designed around the concepts of proﬁles
and jobs [21]. A proﬁle is a conﬁguration of web crawler. A
job is inherited from a proﬁle and can override conﬁguration
options. Jobs are queued for processing and will be picked
up by an idle crawler thread. Heritrix also supports custom
workﬂows.
HtmlUnit Extension to Obtain Unpacked Code. An-
alyzing JavaScript with HtmlUnit, we are able to get the un-
114packed code (without obfuscation) prior to the actual time-
consuming rendering. HtmlUnit is conﬁgured so that it does
not try to resolve external dependencies (e.g., loading exter-
nal JavaScript ﬁles or third party plugins) referred in the
analysed JavaScript. Such a conﬁguration is to make sure
that our approach is scalable and light-weighted, since the
time taken for the analysis could be quite long if we ensure
all dependencies are fulﬁlled. Nevertheless, this is not a
restrictive constraint, since with our experience, most Java-
Script malware is designed to be standalone to maximize the
chance for successful attack. In some rare events where the
external dependencies aﬀect the analysis of HtmlUnit, then
our extension falls back to the plain syntactic analysis.
AST Generation and Functional Call Pattern Ex-
traction. To build the AST of unpacked execution-ready
code, We use Mozzila Rhino 1.7 [6]—an open source Java-
Script engine. The reason to choose Rhino is due to its
fault-tolerance and performance in parsing JavaScript code.
Rhino is also integrated in HtmlUnit to build AST for the
unpacked code that is ready for execution and rendering. We
traverse the AST and extract all the function call nodes. We
apply the frequent item-set mining implemented in WEKA,
a Java-based open source machine learning toolkit [23], to
mine function call patterns.
Classiﬁers. In [27], four types of classiﬁers, namely ADTree,
Random Forest (RF), J48 and N¨aive Bayes (NB), have been
used to solve the malware detection problem, which is a bi-
nary classiﬁcation problem (malicious or benign).
In the
second phase of our approach, we are doing multi-class clas-
siﬁcation, to classify the malicious JavaScript code into eight
attack types. Thus, we reuse the three classiﬁers that sup-
port multi-class classiﬁcation, namely RF, J48 and NB. Be-
sides, we also adopt Random Tree (RT).
All of these classiﬁers are available in the recent version
of WEKA. We also use default parameters set-up for these
classiﬁers. The detailed introduction of these classiﬁers is
out of the scope of this paper. We provide a brief summary
of each classiﬁer as follows:
• RT: A classiﬁer that build a decision tree with K ran-
domly chosen attributes at each node. It supports es-
timation of class probabilities based on a hold-out set.
• RF: A non-probabilistic classiﬁer that uses multiple
decision trees for training, and predicting the classes
by checking the most votes over all decision trees.
• J48: A non-probabilistic classiﬁer that uses single de-
• NB: A probabilistic classiﬁer based on Bayes’ theorem
with a strong (naive) independence assumptions be-
tween the features.
cision tree for classiﬁcation.
To address the noise due to the diﬀerent ranges of feature
values and treat features fairly, WEKA automatically nor-
malizes values of all features via range and z-normalization.
To achieve the best accuracy, WEKA also supports auto-
matic feature selection algorithms.
7. EVALUATION
To test the eﬀectiveness and eﬃciency of JSDC, we train
4 diﬀerent classiﬁers mentioned in previous section with the
training data set and perform predication for the testing
data set. Particularly, controlled experiments on 20942 la-
belled scripts are conducted to evaluate the accuracy and
run-time performance of JSDC. In addition to the controlled
experiments, to exhibit the practical eﬀectiveness, we also
Table 1: Data sets used in controlled experiments
Benign data set
Alexa-top500 websites
Malicious data sets
Attack targeting browser vulnerabilities (type I)
Browser hijacking attack (type II)
Attack targeting Flash (type III)
Attack targeting JRE (type IV)
Attack based on multimedia (type V)
Attack targeting PDF reader (type VI)
Malicious redirecting attack (type VII)
Attack based on Web attack toolkits (type VIII)
Total
#samples
20000
#samples
150
28
81
191
190
101
92
109
942
apply our approach on 1,400,000 unlabelled scripts to ﬁnd
wild malicious ones contained in public web sites.
7.1 Data Set and Experiment Setup
The labelled data sets include 20000 benign samples and
942 malicious ones of 8 attack types, as shown in Table 1.
Our malicious data sets originate from three sources. To in-
clude the existing common samples, we collect 200 samples
of 8 attack types from well-known malware repositories VX-
Heaven [4] and another 200 samples from OpenMalware
[3]. To include the emerging and new attacks, we manu-
ally collect 542 samples from malicious websites reported by
Web Inspector [5]. We test these total 942 malicious sam-
ples with the on-line service of 54 anti-virus tools provided
on VirusTotal [11]. The results report that they are mali-
cious. For the benign data set, we crawl 20,000 scripts from
the Alexa-top500 websites1, from each of which 40 scripts are
randomly crawled. We scan them with two anti-virus tools
Avast! (version 2014.9.0.2021) and TrendMicro (version
10.4) to ensure that they are benign.
The unlabelled data sets include 1,400,000 scripts that
are crawled by Heritrix with randomly selected seeds. Here,
URLs in the list that a Web crawler starts with are called
the seeds. Our seeds include general web sites of universi-
ties, governments, companies, discussion forums, etc. The
training samples, testing samples and newly found malware
in our study can be found from the JSDC web-site [9].
Our experimental environment is a Dell optiplex 990 PC
(Intel Core i7-2600 3.40GHz, 8G memory), running Win-
dows 7 64-bit Enterprise.
7.2 Evaluation of Malware Detection
7.2.1 Detection on the Labelled Data Sets
To test the accuracy of the classiﬁers for the labelled data
sets, malicious and benign scripts are all mixed and then
separated into multiple partitions. To avoid possible errors
introduced in random partition of training set and testing
set, we do not split all samples into a training partition
(75%) and a predication partition (25%). Instead, we adopt
10-fold cross-validation (10-fold CV) strategy to train the 4
classiﬁers: RF, J48, NB and RT.
Accuracy of the trained classiﬁers. Table 2 shows the
accuracies of trained classiﬁers on the labelled data sets. The
accuracy is calculated by measuring the ratio of the number
of successful classiﬁcations to the total number of samples.
As there are more than 40 diﬀerent features for classiﬁca-
1
http://www.alexa.com/topsites
115Table 2: The accuracies of diﬀerent classiﬁers
ML classiﬁer Accuracy FP rate FN rate
Table 3: Detection Ratio of our approach and other
tools on labelled malicious data sets
RandomForest (RF)
J48
N¨aive Bayes (NB)
RandomTree (RT)
99.9522% 0.2123%
99.8615% 0.7431%
98.2237%
1.127%
99.8758% 0.3609%
0.8492%
2.335%
4.5280%
1.4862%
Figure 10: A code example of FN cases
tion of malicious and benign scripts, we use the function of
automated searching and selection of predicate features in
WEKA. Thus, as Table 2 shows, all the four classiﬁers have
achieved an overall accuracy above 98%. Note that the over-
all accuracy is probably dominated by the large number of
correct classiﬁcation of benign sample. Thus, checking the
speciﬁc false positive and false negative cases will provide
more insightful observations.
In Table 2, the worst classiﬁer is N¨aive Bayes, whose FN
rate of malicious samples is 4.5280%. However, due to the
overwhelming percentage of benign samples, a high FN rate
(4.5%) of malicious samples does not aﬀect the overall ac-
curacy signiﬁcantly. The other three classiﬁers have low FP
rate. One of the FN cases for RF, J48 and RT is shown in
Figure 10. This sample belongs to malicious redirects (type
VII) and requires the client side to be a mobile device. Thus,
HtmlUnit cannot parse and unpack the code properly so that
the extracted feature vector is not accurate. Overall, the FN
of malicious samples range from 0.3500% to 4.5280% for dif-
ferent classiﬁers. Thus, our tool achieves a detection ratio
99.65% (the RF classifer) on the whole labelled malicious
samples. It is reasonable to get such high detection ratio, as
in 10-fold CV the diﬀerent folders (partitions) are mutually
used for training and predication for each other.
Comparison with other tools. To compare the accu-
racy and performance of our approach with the state-of-the-
art tools, we consider research prototype JSand2 [13] and
other 11 popular anti-virus tools. The 11 anti-virus tools in-
clude the open-source anti-virus software ClamAV3 and the
2014 best reviewed ten anti-virus products4:—AVG, Avast!,
Bitdefender, F-Secure, Gdata, Kaspersky, McAfee,
Panda, Symantec and TrendMicro. JSand and Cla-
mAV are used in the previous study for comparison [33].
JSand is available as on-line service from this link: https://
wepawet.iseclab.org/index.php. ClamAV and the best
ten anti-virus tools are available as on-line service from Virus-
Total [11]. To enable the latest JSand 2.6 for detection, we
upload 942 malicious samples and analyse them for ﬁve times
to avoid the cached result provided by JSand 1.3.
The results are summarized in Table 3. For each malicious
sample we also count the number of tools, which succeed in
detection, among the total 54 tools provided on VirusTo-
2JSand supports three types of results, i.e., benign, suspicious and
malicious. We consider it is detected if JSand reports suspicious or
malicious.
3ClamAV: http://www.clamav.net/
4
2014
Antivirus
http://
Best
Software
Review:
anti-virus-software-review.toptenreviews.com/
Tool
JSDC
Avast!
Kaspersky
Gdata
Bitdefender
F-Secure
AVG
Detection %
Tool
Detection %
99.68%
93.84%
86.31%
85.88%
83.23%
82.38%
76.22%
McAfee
Symantec
JSAND
Trend
ClamAV
Panda
59.87%
27.28%
25.58%
22.08%
9.24%
5.31%
tal. On average, each malicious sample is reported by 21
tools. Given any malicious sample in our labelled data sets,
usually 6 to 7 among the best reviewed ten anti-virus tools
can detect. These tools’ detection ratio ranges from 5% to
93%. Note that we are not comparing these tools to ﬁnd the
best ones, but investigating the limitations of mainstream
anti-virus tools. Our goal is to ﬁnd the malicious scripts
missed by JSand and the best reviewed ten anti-virus tools.
The best classiﬁer RF in Table 2 can detect 939 out of 942
(99.68%) malicious samples. The failed cases are because of
some implementation issues relating to our HtmlUnit-based
infrastruction, which can not emulate all versions of main-
stream browsers.
7.2.2 Detection on the Unlabelled Data Set
After we get the trained classiﬁers from the labelled data
sets, we apply the learned model to unlabelled data sets for
predication. Among these 1,400,000 wildly crawled scripts,
the best trained classiﬁer RF predicates 1,530 snippets as
malicious.
To verify our prediction accuracy, we randomly select 100
from 1,530 reported unique samples. We manually inspect
these 100 cases and ﬁnd only 1 FP case in these 100 samples.
For the rest 99 TP cases, we test them by other tools men-
tioned in Section 7.2.1. We ﬁnd 11 out of 99 TP cases are
missed by all the tools. For example, we found an exploit
to CVE-2014-1580 is missed by all the tools. Mozilla Fire-
fox before 33.0 does not properly initialize memory for GIF
images, which allows remote attackers to obtain sensitive
information from process memory via a crafted web page
that triggers a sequence of rendering operations for trun-
cated GIF data within a CANVAS element. The results in
Table 4 show individual detection ratio for each tool. The
best tool can achieve a detection ratio of 48.49%. We ob-
serve two facts: ﬁrst, these tools perform much worse on the
unlabelled samples than the labelled ones. Most of the la-
belled ones are from public repositories and may have been
scanned by these tools, but unlabelled ones are new merg-
ing variants. Second, the ranking of detection ratio of these
tools in Table 3 is generally consistent with the ranking in
Table 4, although labelled and unlabelled samples are from
diﬀerent origins. The above observations imply that detec-
tion of wild and emerging malware is not easy. Note that
our comparison mainly focuses on detection ratio (or FN ra-
tio) rather than FP ratio, since existing studies [18, 33] have
found that existing JavaScript malware detection tools have
low FP but high FN.
7.2.3 Performance of Malware Detection
To know the potential of our approach being an oﬀ-line
large-scale anti-virus tool or an on-line real-time scanner as
a browser plugin, we investigate the performance of each
step in malware detection.
116Table 4: Detection ratio of other tools on the 99
unique samples reported by our approach
Table 6: The confusion matrix of using RandomFor-
est on the 942 labelled malicious samples
Tool Detection%
Tool Detection%
Kaspersky
Avast!