### Total Cost Calculation

The total cost is the sum of the costs associated with switches, Network Interface Cards (NICs), and servers. In the case of servers, the paths vary in length, which complicates achieving a one-to-x speedup in DCell. While BCube offers several advantages, it also comes with trade-offs: it requires more mini-switches and wires compared to DCell. However, mini-switches are affordable, and the wiring issue is manageable in a container-based data center, as discussed in Section 5.

### Comparison with VL2

VL2 [8] extends the fat-tree structure by employing 10G Ethernet for its switching backbone. Both BCube and VL2 share design principles such as providing high capacity between all servers and embracing end-systems. BCube uses only low-end switches and supports better one-to-x traffic, albeit at the cost of requiring multi-ports per server. VL2 decouples IP addresses from server locations through a directory service and employs randomization for load balancing, while BCube uses active probing.

### Construction and Power Consumption Analysis

Table 2 presents the construction cost, power consumption, and wire numbers for four network structures in a container with 2048 servers. The costs and power consumptions of switches, NICs, and the total numbers (sums of those of switches, NICs, and servers) are listed. Each server costs $2000 and requires 200W of power. For the Tree structure, we use 44 DLink DGS-3100-48 switches, each costing $1250 and needing 103W. For other structures, we use 8-port DLink DGS-1008D switches, which cost $40 and need 4.5W. DCell, BCube, and fat-tree use 256, 1280, and 2304 switches, respectively. DCell and BCube require a 4-port NIC for each server, while Tree and fat-tree use a one-port NIC. A one-port NIC costs $5 and needs 5W, while a 4-port NIC costs $20 and needs 10W. Table 2 shows that networking costs are a small fraction of the total cost, consistent with [14]. BCube and fat-tree have similar construction and power costs, but BCube uses fewer wires than fat-tree. Performance and cost studies indicate that BCube is more suitable for shipping-container data centers.

### Conclusion

We have presented the design and implementation of BCube, a novel network architecture for shipping-container-based modular data centers (MDCs). By installing a small number of network ports at each server, using COTS mini-switches as crossbars, and placing routing intelligence at the server side, BCube forms a server-centric architecture. BCube significantly accelerates one-to-x traffic patterns and provides high network capacity for all-to-all traffic. The BSR routing protocol further enables graceful performance degradation, meeting the special requirements of MDCs.

### Performance Comparison

Figure 11 and Table 1 compare BCube's performance with other typical network structures, such as tree, fat-tree, and DCell. BCube provides much better support for one-to-x traffic than both tree and fat-tree. Tree offers the lowest aggregate bottleneck throughput, limited by the root switch's capacity. For the same number of switch ports (n) and servers (N), fat-tree requires more layers of switches, resulting in longer path lengths and more wires. Additionally, fat-tree does not degrade gracefully with switch failures and requires switch upgrades for advanced functionalities. BCube is also superior to DCell for MDCs, as DCell targets mega data centers and has imbalanced traffic, leading to lower aggregate bottleneck throughput.

### Acknowledgments

We thank Danfeng Zhang for his work on the NetFPGA Windows driver, Byungchul Park for assistance with the one-to-several experiment, Lidong Zhou for granting us exclusive access to the testbed servers, and Zhanmin Liu, Wei Shen, Yaohui Xu, and Mao Yang for building the testbed. We also thank Victor Bahl, Albert Greenberg, James Hamilton, Frans Kaashoek, Jim Larus, Mark Shaw, Chuck Thacker, Kushagra Vaid, Geoff Voelker, Zheng Zhang, Lidong Zhou, the anonymous SIGCOMM reviewers, and the members of the Wireless and Networking Group of Microsoft Research Asia for their feedback and comments.

### References

[1] M. Al-Fares, A. Loukissas, and A. Vahdat. A Scalable, Commodity Data Center Network Architecture. In SIGCOMM, 2008.
[2] L. Barroso, J. Dean, and U. HÃ¶lzle. Web Search for a Planet: The Google Cluster Architecture. IEEE Micro, March-April 2003.
[3] L. Bhuyan and D. Agrawal. Generalized Hypercube and Hyperbus Structures for a Computer Network. IEEE trans. Computers, April 1984.
[4] D. Borthakur. The Hadoop Distributed File System: Architecture and Design. http://hadoop.apache.org/core/docs/current/hdfs_design.pdf.
[5] CloudStore. Higher Performance Scalable Storage. http://kosmosfs.sourceforge.net/.
[6] J. Dean and S. Ghemawat. MapReduce: Simplified Data Processing on Large Clusters. In OSDI, 2004.
[7] A. Greenberg et al. Towards a Next Generation Data Center Architecture: Scalability and Commoditization. In SIGCOMM PRESTO Workshop, 2008.
[8] A. Greenberg et al. VL2: A Scalable and Flexible Data Center Network. In SIGCOMM, Aug 2009.
[9] C. Guo et al. DCell: A Scalable and Fault Tolerant Network Structure for Data Centers. In SIGCOMM, 2008.
[10] G. Lu et al. CAFE: A Configurable pAcket Forwarding Engine for Data Center Networks. In SIGCOMM PRESTO Workshop, Aug 2009.
[11] J. Duato et al. Interconnection Networks: An Engineering Approach. Morgan Kaufmann, 2003.
[12] S. Ghemawat, H. Gobioff, and S. Leung. The Google File System. In SOSP, 2003.
[13] J. Hamilton. An Architecture for Modular Data Centers. In 3rd CIDR, Jan 2007.
[14] J. Hamilton. Cooperative Expandable Micro-Slice Servers (CEMS). In 4th CIDR, Jan 2009.
[15] J. Hamilton. Private communication, 2009.
[16] IBM. Scalable Modular Data Center. http://www-935.ibm.com/services/us/its/pdf/smdc-eb-sfe03001-usen-00-022708.pdf.
[17] M. Isard, M. Budiu, and Y. Yu. Dryad: Distributed Data-Parallel Programs from Sequential Building Blocks. In EuroSys, 2007.
[18] F. Leighton. Introduction to Parallel Algorithms and Architectures: Arrays, Trees, and Hypercubes. Morgan Kaufmann, 1992.
[19] C. Leiserson. Fat-trees: Universal networks for hardware-efficient supercomputing. IEEE Trans. Computers, 34(10), 1985.
[20] J. Moy. OSPF: Anatomy of an Internet Routing Protocol. Addison-Wesley, 2000.
[21] J. Naous, G. Gibb, S. Bolouki, and N. McKeown. NetFPGA: Reusable Router Architecture for Experimental Research. In SIGCOMM PRESTO Workshop, 2008.
[22] Silicom. Gigabit Ethernet Server Adapters. http://www.silicom-usa.com/default.asp?contentID=711.
[23] Rackable Systems. ICE Cube Modular Data Center. http://www.rackable.com/products/icecube.aspx.
[24] Verari Systems. The Verari FOREST Container Solution: The Answer to Consolidation. http://www.verari.com/forest_spec.asp.
[25] M. Waldrop. Data Center in a Box. Scientific American, July 2007.

### Appendix

#### Proof of Theorem 2

We show that the intermediate server \( N_0^m \) (where \( 1 \leq m \leq k \)) of path \( P_0 \) cannot appear in path \( P_1 \). First, \( N_0^m \) cannot appear in \( P_1 \) at different positions because it would imply two shortest paths with different lengths, which is impossible. Second, \( N_0^m \) cannot appear in \( P_1 \) at position \( m \) because \( P_0 \) and \( P_1 \) start by correcting different digits of \( A \). Therefore, \( N_0^m \) cannot appear in \( P_1 \). Similarly, any intermediate server \( N_1^m \) cannot appear in \( P_0 \).

Next, we show that the switches in the paths are also different. First, the switches in a single path are different because they are at different layers. Assume that switch \( S_0 \) in \( P_0 \) and switch \( S_1 \) in \( P_1 \) are the same. This would imply that four servers in the two paths are connected via this switch, which is a contradiction. Thus, \( S_0 \) and \( S_1 \) cannot be the same. Therefore, \( P_0 \) and \( P_1 \) are two parallel paths.

#### Proof of Theorem 6

To calculate the aggregate bottleneck throughput, we first determine the average path length from one server to the rest of the servers using BCubeRouting. For a server \( A \), the remaining \( n^{k+1} - 1 \) servers in a BCube\(_k\) can be classified into \( k \) groups. Group \( i \) contains the servers that are \( i \) hops away from \( A \). The number of servers in Group \( i \) (where \( i \in [1, k + 1] \)) is \( C_i^k+1 (n - 1)^i \). By averaging the path lengths of these different groups, we get the average path length \( \text{ave\_plen} = \frac{\sum_{i=1}^{k+1} i C_i^k+1 (n - 1)^i}{n^{k+1} - 1} \).

Since links are equal in BCube, the number of flows carried in one link is \( \text{f\_num} = \frac{N (N - 1)}{\text{ave\_plen}} \), where \( N (N - 1) \) is the total number of flows and \( N (k + 1) \) is the total number of links. The throughput one flow receives is thus \( \frac{1}{\text{f\_num}} \), assuming that the bandwidth of a link is one. The aggregate bottleneck throughput is therefore \( \frac{N (N - 1)}{\text{f\_num}} = \frac{(n-1)N}{n(N-1)(k + 1)} \).