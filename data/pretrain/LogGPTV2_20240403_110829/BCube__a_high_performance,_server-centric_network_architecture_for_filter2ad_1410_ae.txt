The total cost is the sum of the costs of the switches,
NICs, and servers.
servers, the paths are of diﬀerent lengths. This makes one-
to-x speedup in DCell diﬃcult to achieve. Of course, the
beneﬁts of BCube are not oﬀered for free: BCube uses more
mini-switches and wires than DCell. But mini-switches are
aﬀordable and wiring is a solvable issue for a container-based
data center, as we have addressed in Section 5.
VL2 [8] extends the fat-tree structure by using 10G Eth-
ernet to form its switching backbone. BCube and VL2
share several similar design principles such as providing high
capacity between all servers and embracing end-systems.
BCube uses only low-end switches and provides better one-
to-x support at the cost of multi-ports per-server. VL2 is
able to decouple IP address and server location by introduc-
ing a directory service. VL2 uses randomization whereas
BCube uses active probing for load-balancing.
Table 2 presents construction cost, power consumption,
and wire numbers of the four structures for a container with
2048 servers. We list the costs and power consumptions of
switches, NICs, and the total numbers (which are the sums
of those of switches, NICs, and servers). Each server costs
$2000 and needs 200W power supply. For Tree, we use 44
48-port DLink DGS-3100-48 (which is the cheapest 48 port
switch we can ﬁnd) to form a two-level switch fabric. For
other structures, we use 8-port DLink DGS-1008D switches.
DCell, BCube, and fat-tree use 256, 1280, and 2304 switches,
respectively. DCell and BCube need a 4-port NIC for each
server, whereas Tree and fat-tree only use one-port NIC. A
DGS-3100-48 costs $1250 and needs 103W. A DGS-1008D
costs $40 and needs 4.5W. A one-port NIC and 4-port NIC
cost $5 [15] and $20, and need 5W and 10W [22], respec-
tively. Table 2 shows that the networking cost is only a small
fraction of the total cost. This result is consistent with that
in [14]. The construction and power costs of BCube and
fat-tree are similar, but BCube uses a smaller number of
wires than fat-tree. The performance and cost study clearly
show that BCube is more viable for shipping-container data
centers.
9. CONCLUSION
We have presented the design and implementation of BCube
as a novel network architecture for shipping-container-based
modular data centers (MDC). By installing a small num-
ber of network ports at each server and using COTS mini-
switches as crossbars, and putting routing intelligence at
the server side, BCube forms a server-centric architecture.
We have shown that BCube signiﬁcantly accelerates one-
to-x traﬃc patterns and provides high network capacity for
all-to-all traﬃc. The BSR routing protocol further enables
graceful performance degradation and meets the special re-
quirements of MDCs.
Figure 11: Total TCP throughput under BCube and
tree in the all-to-all communication model.
Tree
1
1
1
n
No
bad
One-to-one
One-to-several
One-to-all
All-to-all(ABT)
Traﬃc balance
Graceful
degradation
Wire No.
Switch upgrade No
n(N−1)
n−1
N
BCube
k + 1
k + 1
k + 1
n(N−1)
n−1
Fat-tree DCell+
k(cid:48) + 1
1
k(cid:48) + 1
1
≤ k(cid:48) + 1
1
N
2k(cid:48)
No
Yes
good
fair
( k(cid:48)
2 +1)N N lognN
No
N log n
2
Yes
Yes
good
No
N
2
+A level-2 (k(cid:48) = 2) DCell with n = 8 is enough for shipping-
container. Hence k(cid:48) is smaller than k.
Table 1: Performance comparison of BCube and
other typical network structures.
or use a large number of server ports and wires (e.g., Hy-
percube and de Bruijn).
In recent years, several data center network structures
have been proposed [1, 7, 8, 9]. Table 1 compares BCube
and other three typical structures: tree, fat-tree [1, 7], and
DCell [9].
In the table, n is the number of switch ports
and N is the number of servers. For one-to-x, we show the
speedup as compared with the tree structure. For all-to-all,
we show the aggregate bottleneck throughput.
As we show in Table 1, BCube provides much better sup-
port for one-to-x traﬃc than both tree and fat-tree. Tree
provides the lowest aggregate bottleneck throughput since
the throughput is only the capacity of the root switch. For
the same n and N , the layer of switches needed by fat-tree
N
2 , which is larger than that of BCube, lognN . Hence
is log n
2
the path length and the number of wires in fat-tree are
larger than those of BCube. Moreover, fat-tree does not
degrade gracefully as switch failure increases (see Section 6)
and it needs switch upgrade to support advanced routing
and packet forwarding functionalities.
BCube is also better than DCell[9] for MDCs. DCell
builds complete graphs at each level, resulting doubly ex-
ponential growth. As a result, DCell targets for Mega data
centers. The traﬃc in DCell is imbalanced:
the level-0
links carry much higher traﬃc than the other links. As
a result, the aggregate bottleneck throughput of DCell is
much smaller than that of BCube (see Section 6). Further-
more, though DCell has multiple parallel paths between two
 0 2 4 6 8 10 12 14 16 18 0 50 100 150 200 250 300 350Throughput (Gb/s)Time (second)BCubeTree73The design principle of BCube is to explore the server-
centric approach to MDC networking in both topology de-
sign and routing, thus providing an alternative to the switch-
oriented designs. In our future work, we will study how to
scale our server-centric design from the single container to
multiple containers.
10. ACKNOWLEDGEMENT
We thank Danfeng Zhang for his work on the NetFPGA
Windows driver, Byungchul Park for helping us perform part
of the one-to-several experiment, Lidong Zhou for generously
granting us exclusive access to the testbed servers, Zhanmin
Liu, Wei Shen, Yaohui Xu, and Mao Yang for helping us
build the testbed. We thank Victor Bahl, Albert Green-
berg, James Hamilton, Frans Kaashoek, Jim Larus, Mark
Shaw, Chuck Thacker, Kushagra Vaid, Geoﬀ Voelker, Zheng
Zhang, Lidong Zhou, the anonymous SIGCOMM reviewers,
and the members of the Wireless and Networking Group of
Microsoft Research Asia for their feedback and comments.
11. REFERENCES
[1] M. Al-Fares, A. Loukissas, and A. Vahdat. A Scalable,
Commodity Data Center Network Architecture. In
SIGCOMM, 2008.
[2] L. Barroso, J. Dean, and U. H¨olzle. Web Search for a
Planet: The Google Cluster Architecture. IEEE Micro,
March-April 2003.
[3] L. Bhuyan and D. Agrawal. Generalized Hypercube and
Hyperbus Structures for a Computer Network. IEEE trans.
Computers, April 1984.
[4] D. Borthakur. The Hadoop Distributed File System:
Architecture and Design. http://hadoop.apache.org/
core/docs/current/hdfs design.pdf.
[5] CloudStore. Higher Performance Scalable Storage.
http://kosmosfs.sourceforge.net/.
[6] J. Dean and S. Ghemawat. MapReduce: Simpliﬁed Data
Processing on Large Clusters. In OSDI, 2004.
[7] A. Greenberg et al. Towards a Next Generation Data
Center Architecture: Scalability and Commoditization. In
SIGCOMM PRESTO Workshop, 2008.
[8] A. Greenberg et al. VL2: A Scalable and Flexible Data
Center Network. In SIGCOMM, Aug 2009.
[9] C. Guo et al. DCell: A Scalable and Fault Tolerant
Network Structure for Data Centers. In SIGCOMM, 2008.
[10] G. Lu et al. CAFE: A Conﬁgurable pAcket Forwarding
Engine for Data Center Networks. In SIGCOMM PRESTO
Workshop, Aug 2009.
[11] J. Duato et al. Interconnection Networks: An Engineering
Approach. Morgan Kaufmann, 2003.
[12] S. Ghemawat, H. Gobioﬀ, and S. Leung. The Google File
System. In SOSP, 2003.
[13] J. Hamilton. An Architecture for Modular Data Centers. In
3rd CIDR, Jan 2007.
[14] J. Hamilton. Cooperative Expandable Micro-Slice Servers
(CEMS). In 4th CIDR, Jan 2009.
[15] J. Hamilton. Private communication, 2009.
[16] IBM. Scalable Modular Data Center.
http://www-935.ibm.com/services/us/its/pdf/smdc-eb-
sfe03001-usen-00-022708.pdf.
[17] M. Isard, M. Budiu, and Y. Yu. Dryad: Distributed
Data-Parallel Programs from Sequential Building Blocks. In
EuroSys, 2007.
[18] F. Leighton. Introduction to Parallel Algorithms and
Architectures: Arrays. Trees. Hypercubes. Morgan
Kaufmann, 1992.
[19] C. Leiserson. Fat-trees: Universal networks for
hardware-eﬃcient supercomputing. IEEE Trans.
Computers, 34(10), 1985.
[20] J. Moy. OSPF: Anatomy of an Internet Routing Protocol.
Addison-Wesley, 2000.
[21] J. Naous, G. Gibb, S. Bolouki, and N. McKeown.
NetFPGA: Reusable Router Architecture for Experimental
Research. In SIGCOMM PRESTO Workshop, 2008.
[22] Silicom. Gigabit Ethnet Server Adapters.
http://www.silicom-usa.com/default.asp?contentID=711.
[23] Rackable Systems. ICE Cube Modular Data Center.
http://www.rackable.com/products/icecube.aspx.
[24] Verari Systems. The Verari FOREST Container Solution:
The Answer to Consolidation. http://www.verari.com/
forest spec.asp.
[25] M. Waldrop. Data Center in a Box. Scientiﬁc American,
July 2007.
APPENDIX
A. PROOF OF THEOREM 2
1 , N 0
2 ,··· N 0
m,··· N 0
k , B} and {A, N 1
We show that the intermediate server N 0
From permutation Π0 (Π1), we establish path P0 (P1)
by ﬁrst correcting digits of A from position i0 (i1) to 0,
then from k to i0 + 1 (i1 + 1). We denote P0 and P1
as {A, N 0
2 , ··· N 1
1 , N 1
m,
k , B}.
··· N 1
m (1 ≤ m ≤ k)
of P0 cannot appear in P1. First, N 0
m cannot appear in P1
at diﬀerent locations other than m, otherwise, we can reach
N 0
m from two shortest pathes with diﬀerent path lengthes,
which is impossible. Second, N 0
m cannot appear in P1 at
position m, because P0 and P1 start by correcting diﬀerent
digits of A. Therefore, N 0
m cannot appear in P1. Similarly,
any intermediate server N 1
m cannot appear in P0.
We next show that the switches in the paths are also
diﬀerent. First, the switches in a single path are diﬀer-
ent, this is because these switches are at diﬀerent layers.
Then assume that switch S0 in P0 and switch S1 in P1 are
the same, and we denote it as .
Due to the fact that servers in P0 and P1 are diﬀerent, we
have four servers in the two pathes that connected via this
switch. But the only two servers it connects in P0 and P1
are sk−1 ··· slalsl−1 ··· s0 and sk−1 ··· slblsl−1 ··· s0. The
contradiction shows that S0 and S1 cannot be the same.
Therefore, P0 and P1 are two parallel paths.
B. PROOF OF THEOREM 6
(cid:80)k+1
i=1 {iC i
In order to get the aggregate bottleneck throughput, we
ﬁrst calculate the average path length from one server to
the rest servers using BCubeRouting. For a server A, the
rest nk+1 − 1 servers in a BCubek can be classiﬁed into k
groups. Groupi contains the servers that are i hops away
from A. The number of servers in Groupi (i ∈ [1, k + 1]) is
k+1(n − 1)i. By averaging the path lengths of these dif-
C i
ferent groups, we get the average path length is ave plen =
nk+1−1
Since links are equal in BCube, the number of ﬂows car-
, where N (N − 1)
ried in one link is f num = N (N−1)ave plen
is the total number of ﬂows and N (k + 1) is the total num-
ber of links. The throughput one ﬂow receives is thus
f num ,
assuming that the bandwidth of a link is one. The aggre-
gate bottleneck throughput is therefore N (N − 1)
f num =
n−1 (N − 1).
k+1(n − 1)i} = (n−1)N
n(N−1) (k + 1).
N (k+1)
n
1
1
1
74