DOWNp ←→ Edдei,p
UPp ←→ Aддi,p
DOWNp (cid:2)(cid:3) UP(p+j )%
for each p in k
DOWNp ←→ BS
UPp ←→ BS
2,i,p− k
≤ p < k
1,i,p− k
2
2
2
2
k
2
+ n do
18: for each CS3,i, j where 0 ≤ i < k, 0
2
2
2
3, i
×j
∈ FG
2 do
+ k
// Core layer
≤ j < k
Core i
for each p in 0 ≤ p < k
2 do
DOWNp ←→ Aддi,p
UPp ←→ Core k
×p+j
DOWNp (cid:2)(cid:3) UPp
≤ p < k
for each p in k
DOWNp ←→ BS
UPp ←→ BS
3, j,p− k
2
2
2
2,i,p− k
2
2
19:
20:
21:
22:
23:
24:
25:
26:
+ n do
(cid:5)(cid:10)(cid:12)(cid:11)(cid:17)(cid:16)(cid:17)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1) (cid:5)(cid:10)(cid:12)(cid:11)(cid:17)(cid:16)(cid:19)
(cid:4)(cid:1) (cid:5)(cid:1) (cid:6)(cid:1)
(cid:3)(cid:9)(cid:18)(cid:16)(cid:17)(cid:16)(cid:17)
(cid:6)(cid:7)(cid:18)(cid:16)(cid:17)
(cid:2)(cid:12)(cid:12)(cid:18)(cid:16)(cid:17)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1) (cid:2)(cid:12)(cid:12)(cid:18)(cid:16)(cid:19)
(cid:4)(cid:1) (cid:5)(cid:1) (cid:6)(cid:1)
(cid:2)(cid:4)(cid:3)(cid:1)(cid:6)
(cid:3)(cid:1)(cid:4)
(cid:3)(cid:9)(cid:19)(cid:16)(cid:18)(cid:16)(cid:17)
(cid:4)(cid:10)(cid:11)(cid:8)(cid:13)(cid:12) (cid:4)(cid:10)(cid:11)(cid:8)(cid:16)(cid:12) (cid:4)(cid:10)(cid:11)(cid:8)(cid:19) (cid:3)(cid:7)(cid:16)(cid:12)(cid:13)(cid:12)(cid:13)
(cid:4)(cid:1) (cid:7)(cid:1) (cid:10)(cid:1)
(cid:5)(cid:6)(cid:16)(cid:12)(cid:13)
(cid:6)(cid:7)(cid:19)(cid:16)(cid:18)
(cid:3)(cid:2)(cid:4)
(cid:5)(cid:1) (cid:8)(cid:1) (cid:11)(cid:1)
(cid:4)(cid:10)(cid:11)(cid:8)(cid:14)(cid:12) (cid:4)(cid:10)(cid:11)(cid:8)(cid:17)(cid:12) (cid:4)(cid:10)(cid:11)(cid:8)(cid:20) (cid:3)(cid:7)(cid:16)(cid:12)(cid:14)(cid:12)(cid:13) (cid:4)(cid:10)(cid:11)(cid:8)(cid:15)(cid:12) (cid:4)(cid:10)(cid:11)(cid:8)(cid:18)(cid:12) (cid:4)(cid:10)(cid:11)(cid:8)(cid:21) (cid:3)(cid:7)(cid:16)(cid:12)(cid:15)(cid:12)(cid:13)
(cid:5)(cid:6)(cid:16)(cid:12)(cid:15)
(cid:3)(cid:1)(cid:4)
(cid:6)(cid:1) (cid:9)(cid:1) (cid:12)(cid:1)
(cid:5)(cid:6)(cid:16)(cid:12)(cid:14)
(cid:4)(cid:9)(cid:18)(cid:16)(cid:17)(cid:16)(cid:17)
(cid:5)
(cid:4)(cid:9)(cid:18)(cid:16)(cid:17)(cid:16)(cid:19)
(cid:4)(cid:9)(cid:19)(cid:16)(cid:18)(cid:16)(cid:17)
(cid:5)
(cid:4)(cid:9)(cid:19)(cid:16)(cid:18)(cid:16)(cid:19)
(cid:4)(cid:7)(cid:16)(cid:12)(cid:16)(cid:12)(cid:13)
(cid:5)
(cid:4)(cid:7)(cid:16)(cid:12)(cid:16)(cid:12)(cid:15)
(cid:4)(cid:7)(cid:16)(cid:12)(cid:17)(cid:12)(cid:13)
(cid:5)
(cid:4)(cid:7)(cid:16)(cid:12)(cid:17)(cid:12)(cid:15)
(cid:4)(cid:7)(cid:16)(cid:12)(cid:18)(cid:12)(cid:13)
(cid:5)
(cid:4)(cid:7)(cid:16)(cid:12)(cid:18)(cid:12)(cid:15)
(cid:4)(cid:1) (cid:7)(cid:1) (cid:10)(cid:1)
(cid:5)(cid:1) (cid:8)(cid:1) (cid:11)(cid:1)
(cid:6)(cid:1) (cid:9)(cid:1) (cid:12)(cid:1)
(cid:8)(cid:13)(cid:14)(cid:15)(cid:17)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1) (cid:8)(cid:13)(cid:14)(cid:15)(cid:20)
(cid:2)(cid:4)(cid:3)(cid:1)(cid:7)
(cid:6)(cid:7)(cid:18)(cid:16)(cid:18)
(cid:4)(cid:1) (cid:5)(cid:1) (cid:6)(cid:1)
(cid:5)(cid:10)(cid:12)(cid:11)(cid:18)(cid:16)(cid:17)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1) (cid:5)(cid:10)(cid:12)(cid:11)(cid:18)(cid:16)(cid:19) (cid:3)(cid:9)(cid:18)(cid:16)(cid:18)(cid:16)(cid:17)
(cid:2)(cid:4)(cid:3)(cid:1)(cid:8)
(cid:5)(cid:6)(cid:15)(cid:12)(cid:18)
(cid:2)(cid:9)(cid:9)(cid:16)(cid:12)(cid:13)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1) (cid:2)(cid:9)(cid:9)(cid:16)(cid:12)(cid:15) (cid:3)(cid:7)(cid:15)(cid:12)(cid:16)(cid:12)(cid:13) (cid:2)(cid:9)(cid:9)(cid:17)(cid:12)(cid:13)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1) (cid:2)(cid:9)(cid:9)(cid:17)(cid:12)(cid:15) (cid:3)(cid:7)(cid:15)(cid:12)(cid:17)(cid:12)(cid:13) (cid:2)(cid:9)(cid:9)(cid:18)(cid:12)(cid:13)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1) (cid:2)(cid:9)(cid:9)(cid:18)(cid:12)(cid:15) (cid:3)(cid:7)(cid:15)(cid:12)(cid:18)(cid:12)(cid:13)
(cid:2)(cid:4)(cid:3)(cid:1)(cid:6)
(cid:5)(cid:6)(cid:15)(cid:12)(cid:16)
(cid:2)(cid:4)(cid:3)(cid:1)(cid:7)
(cid:5)(cid:6)(cid:15)(cid:12)(cid:17)
(cid:4)(cid:1) (cid:5)(cid:1) (cid:6)(cid:1)
(cid:4)(cid:1) (cid:5)(cid:1) (cid:6)(cid:1)
(cid:4)(cid:1) (cid:5)(cid:1) (cid:6)(cid:1)
(cid:4)(cid:3)(cid:5)
(cid:4)(cid:2)(cid:5)
(cid:1)(cid:3)(cid:2) (cid:4)
(cid:4)(cid:1)(cid:5)
(cid:1)(cid:3)(cid:2) (cid:5)
(cid:1)(cid:3)(cid:2) (cid:6)
(cid:1)(cid:3)(cid:2) (cid:7)
(cid:1)(cid:3)(cid:2) (cid:8)
(cid:1)(cid:3)(cid:2) (cid:9)
(cid:3)(cid:2)(cid:4) (cid:1) (cid:5)(cid:15)(cid:14) (cid:4)(cid:2)(cid:9)(cid:12)(cid:9)(cid:8)(cid:3)(cid:3) (cid:6)(cid:3)(cid:9)(cid:10)(cid:7)(cid:8)(cid:5)(cid:11)
Figure 1: A k = 6 and n = 1 ShareBackup network. (a), (b), (c) correspond to shaded areas in (d). Devices are labeled according to the notations
in Table 1. Edge and aggregation switches are marked by their in-Pod indices; core switches and hosts are marked by their global indices.
Switches in the same failure group are packed together, which share a backup switch in stripes on the side. Circuit switches are inserted
into adjacent layers of switches/hosts. The connectivity in shade is the basic building block for shareable backup. The crossed switch and
connections represent example switch and link failures. Switches with failures are each replaced by a backup switch with the new circuit switch
conﬁgurations shown at the bottom, where connections regarding the original red round ports reconnect to the new black square ports.
Table 1: List of notations
Notation
k
n
H ostj
Edдei, j
Aддi, j
Cor ej
CSl, i, j
F Gl,u
BSl,u,v
U Pp
DOW Np
2 switches per failure group
Meaning
Fat-tree parameter: switch port count and # Pods [8]
# backup switches shared by k
The jth host
The jth Edge switch in the ith Pod
The jth Aggregation switch in the ith Pod
The jth Core switch
The jth Circuit Switch in the ith Pod on the lth layer
The uth Failure Group on the lth layer
The vth Backup Switch in F Gl,u
The pth UPward facing port of a circuit switch
The pth DOWNward facing port of a circuit switch
intermediate ports, increases insertion loss thus requiring
more powerful (and expensive) transceivers, and causes large
end-to-end switching delay that slows down failure recovery.
Instead, recent works promote partial conﬁgurability in small
network regions using circuit switches with considerably low
per-port cost and switching delay [23, 50]. For instance, a
commercial 160-port 10Gbps electrical crosspoint switch
costs $3 per port and has 70ns switching delay [23]; 32-port
25Gbps optical 2D-MEMS has been developed, with 40μs
178
switching delay at an estimated cost of $10 per port [34, 46].
These technologies meet our demand.
These targeted circuit switches have modest port count, so
we divide the network into smaller failure groups and deploy
them in each group. Measurement studies show that failures in
data centers are rare, uncorrelated, and spatially dispersed [16,
52]. ShareBackup’s distributed design is a good match for
these characteristics and can provide good coverage. Fat-tree
has k
2 edge and aggregation switches per Pod. To align with
the architecture, we cluster k
2 switches into a failure group
and allow them to share n backup switches. All switches in a
failure group, including the backup switches, must connect
to the same circuit switch with the same wiring pattern. In
this way, a backup switch can be brought online at run time
to replace any failed switch or failed links associated with
it within its failure group. This circuit switch should have
at least k
+ n) ports, which may exceed the port count
of the targeted circuit switches for a large data center. We
combine k
2 individual circuit switches side by side and design
a wiring pattern to achieve equivalent functionality. Figure 1
× ( k
2
2
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
D. Wu et al.
gives intuitions of the architecture design. Algorithm 1 shows
the wiring plan, with notations listed in Table 1.
2
2
+n) by ( k
Figure 1(a) illustrates the basic building block for Share-
Backup with n = 1. The edge switches in the same Pod form
a failure group (line 2 in Algorithm 1). We place k
2 units of
( k
+n) circuit switches between the edge switches
and the hosts. Every switch, regular and backup switch alike,
connects to these k
2 circuit switches each with a link (line 5
and 8). As shown in Figure 3, these k
2 switches are chained
together via 2 extra side ports, which is omitted in Figure 1
for simplicity. Hosts connect to the edge switches via straight-
through connections on the intermediate circuit switches (line
4 and 6). The ports to backup switches are not connected
internally. When a switch is down, the internal connections to
it on all the circuit switches are reconﬁgured to connect to a
backup switch, which replaces the failed switch completely. A
switch whose links are down is replaced in the same manner
so as to ﬁx the link failures.
In Figure 1(b), the aggregation switches in the same Pod
form a failure group (line 10). Edge and aggregation switches
in their failure groups repeat the building block of connectivity
in Figure 1(a) to another set of k
2 circuit switches (line 12,
13, 16, and 17). In a fat-tree Pod, an edge/aggregation switch
connects to each and every aggregation/edge switch, so we
use a rotational wiring pattern in the circuit switches (line 14)
to achieve this shufﬂe connectivity, i.e. the different internal
connections on CS2, 1, 0 to CS2, 1, 2.
Similarly, aggregation switches in each failure group shown
in Figure 1(c) are connected upward to k
2 circuit switches
with the wiring pattern in the building block. As the fat-tree
example shows, the connections from aggregation switches in
each Pod iterate through all the core switches in consecutive
order. Because the aggregation switches are already connected
to the circuit switches, we wire up the core switches and
the circuit switches to achieve the fat-tree connectivity. In
the Figure 1(c) example, core switches Core0, Core1, Core2
connect to the ﬁrst aggregation switch in each Pod (Aдд3, 0,
Aдд4, 0, Aдд5, 0) through different circuit switches in the Pod;
Core3, Core4, Core5 to the second aggregation switch in each
Pod (Aдд3, 1, Aдд4, 1, Aдд5, 1); and Core6, Core7, Core8 to the
third (Aдд3, 2, Aдд4, 2, Aдд5, 2). As a summary of this pattern,
the core switches connect to k
2 circuit switches with a stride
of k
2 , and we set up straight-through connections in the circuit
switches. Similar to the building block for shareable backup
in Figure 1(a), only switches connected to the same set of
circuit switches can be put into a failure group. As a result,
core switches whose indices are in k
2 intervals form a failure
group. We give each failure group n backup switches and
connect them up in the same way as regular switches.
In fat-tree, edge and aggregation switches are packaged into
Pods for ease of deployment. In each ShareBackup Pod, there
are n additional edge and aggregation switches respectively
as backup switches, and 3 sets of k
2 circuit switches between