unauthorized customers. This would easily result in a large
number of rules on an ingress switch if there are a large
number of customers connected to the switch and/or these
customers make use of a large amount of non-aggregatable
address space.
In the rest of this section, we evaluate the eﬀect of over-
lapping rules, the number of authority switches needed for
diﬀerent networks, the number of extra rules needed after
the partitioning, the miss rate of caching wildcard rules,
and the stretch experienced by packets that travel through
an authority switch.
(1) Installing overlapping rules in an authority switch
signiﬁcantly reduces the memory requirement of switches:
In our set of access control rules, the use of wildcard rules
can result in overlapping rules. One straight forward way
to handle these overlapping rules is to translate them into
non-overlapping rules which represent the same semantics.
However, this is not a good solution because Figure 11 shows
that the resulting non-overlapping rules require one or two
orders of magnitude more TCAM space than the original
overlapping rules. This suggests that the use of overlapping
wildcard rules can signiﬁcantly reduce the number of TCAM
entries.
(2) A small number of authority switches are needed
for the large networks we evaluated.
Figure 12
shows the number of authority switches needed under vary-
360s
e
i
r
t
n
e
M
A
C
T
a
r
t
x
e
f
o
%
 12
 10
 8
 6
 4
 2
 0
 0
 2
 4
 6
No. of splits
 8
 10
Figure 13: The overhead of rule partition.
)
%
 100
 10
 1
 0.1
 0.01
 0.001
 0.0001
i
t
(
e
a
r
s
s
m
e
h
c
a
C
microflow
wildcard
10
1K
Cache size per switch (TCAM entries)
100
Figure 14: Cache miss rate.
ing TCAM capacities. The number of authority switches
needed decreases almost linearly with the increase of the
switch memory. For networks with relatively few rules, such
as the campus and VPN networks, we would require 5–6
authority switches with 10K TCAM in each (assuming we
need 16B to store the six ﬁelds and action for a TCAM entry,
we need about 160KB of TCAM in each authority switch).
To handle networks with many rules, such as the IP and
IPTV networks, we would need approximately 100 authority
switches with 100K TCAM entries (1.6MB TCAM) each.12
The number of the authority switches is still relatively small
compared to the network size (2K - 3K switches).
(3) Our partition algorithm is eﬃcient in reducing
the TCAM usage in switches. As shown in Figure 4,
depending on the rules, partitioning wildcard rules can in-
crease the total number of rules and the TCAM usage for
representing the rules. With the 6-tuple access-control rules
in the IP network, the total number of TCAM entries in-
creases only by 0.01% if we distribute the rules over 100
authority switches. This is because most of the cuts are on
the ingress dimension and, in the data set, most rules diﬀer
between ingresses. To evaluate how the partition algorithm
handles highly overlapping rules, we use our algorithm to
partition the 1.6K rules in one ingress router in the IP net-
work. Figure 13 shows that we only increase the number
of TCAM entries by 10% with 10 splits (100–200 TCAM
entries per split).
(4) Our wildcard caching solution is eﬃcient in reduc-
ing cache misses and cache memory size. We evaluate
our cache algorithm with packet-level traces of 10M packets
collected in December 2008 and the corresponding access-
control lists (9K rules) in a router in the IP network. Fig-
12Today’s commercial switches are commonly equipped with
2 MB TCAM.
n
o
i
t
c
a
r
f
e
v
i
t
l
a
u
m
u
C
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
k-med,10
k-med,3
k-med,1
rand,10
rand,3
rand,1
 1
 1.5
 3
 4
 5
 2
Stretch
Figure 15: The stretch of campus network. (We place
1,3,10 authority switches for each set of the author-
ity rules using k-median and random algorithms respec-
tively.)
ure 14 shows that if we only cache micro-ﬂow rules, the miss
rate is 10% with 1K cache entries. In contrast, with wildcard
rule caching in DIFANE, the cache miss rate is only 0.1%
with 100 cache entries: 99.9% of packets are forwarded di-
rectly to the destination, while only 0.1% of the packets take
a slightly longer path through an authority switch. Those
ingress switches which are not authority switches only need
to have 1K TCAM entries for cache rules (Figure 14) and
10 - 1000 TCAM entries for partition rules (Figure 12).
(5) The stretch caused by packet redirection is small.
We evaluated the stretch of two authority switch placement
schemes (random and k-median) discussed in Section 5. Fig-
ure 15 shows the distribution of stretch (delay normalized by
that of the shortest path) among all source-destination pairs
in the campus network. With only one authority switch for
each set of authority rules, the average stretch is twice the
delay of the shortest path length in the random scheme.
Though some packets experience 10 times the delay of the
shortest path, this usually happens to those pairs of nodes
that are one or two hops away from each other; so the ab-
solute delay of these paths is not large.
If we store one
set of rules at three random places, we can reduce stretch
(the stretch is 1.8 on average). With 10 copies of rules, the
stretch is reduced to 1.5. By placing the authority switches
with the k-median scheme, we can further reduce the stretch
(e.g., with 10 copies of rules, the stretch is reduced to 1.07).
8. DIFANE DEPLOYMENT SCENARIOS
DIFANE proposes to use authority switches to always
keep packets in the data plane. However, today’s commer-
cial OpenFlow switches have resource constraints in the con-
trol plane.
In this section, we describe how DIFANE can
be deployed in today’s switches with resource constraints
and in future switches as a clean-slate solution. We provide
three types of design choices: implementing tunneling with
packet encapsulation or VLAN tags; performing rule caching
in the authority switches or in the controller; choosing nor-
mal switches or dedicated switches as authority switches.
Deployment with today’s switches: Today’s commer-
cial OpenFlow switches have resource constraints in the con-
trol plane. For example, they do not have enough CPU
resources to generate caching rules or have hardware to en-
capsulate packets quickly. Therefore we use VLAN tags to
implement tunneling and move the wildcard rule caching to
the DIFANE controller. The ingress switch tags the “miss”
361packet and sends it to the authority switch. The authority
switch tags the packet with a diﬀerent VLAN tag and sends
the packet to the corresponding egress switch. The ingress
switch also sends the packet header to the controller, and
the controller installs the cache rules in the ingress switch.
Performing rule caching in the controller resolves the lim-
itations of today’s commercial OpenFlow switches for two
reasons. (i) Authority switches do not need to run caching
functions. (ii) The authority switches do not need to know
the addresses of the ingress switch. We can use VLAN tag-
ging instead of packet encapsulation, which can be imple-
mented in hardware in today’s switches.
This deployment scenario is similar to Ethane [3] in that
it also has the overhead of the ingress switch sending pack-
ets to the controller and the controller installing caching
rules. However, the diﬀerence is that DIFANE always keeps
the packet in the fast path. Since we need one VLAN tag
for each authority switch (10 - 100 authority switches) and
each egress switch (at most a few thousand switches), we
have enough VLAN tags to support tunneling in DIFANE
in today’s networks.
Clean slate deployment with future switches: Future
switches can have more CPU resources and hardware-based
packet encapsulation techniques. In this case, we can have
a clean slate design. The ingress switch encapsulates the
“miss” packet with its address as the source address. The
authority switch decapsulates the packet, gets the address of
the ingress switch and re-encapsulates the packet with the
egress switch address as the packet’s destination address.
The authority switch also installs cache rules to the ingress
switch based on the address it gets from the packet header.
In this scenario, we can avoid the overhead and single point
of failure of the controller. Future switches should also have
high bandwidth channel between the data plane and the
control plane to improve the caching performance.
Deployment of authority switches: There are two de-
ployment scenarios for authority switches: (i) The authority
switches are just normal switches in the network that can be
taken over by other switches when they fail; (ii) The author-
ity switches are dedicated switches that have larger TCAM
to store more authority rules and serve essentially as a dis-
tributed data plane of the centralized controller. All the
other switches just need a small TCAM to store a few parti-
tion rules and the cache rules. In the second scenario, even
when DIFANE has only one authority switch that serves as
the data plane of the controller, DIFANE and Ethane [3] are
still fundamentally diﬀerent in that DIFANE pre-installs au-
thority rules in TCAM and thus always keeps the packet in
the fast path.
9. CONCLUSION
We design and implement DIFANE, a distributed ﬂow
management architecture that distributes rules to authority
switches and handles all data traﬃc in the fast path. DI-
FANE can handle wildcard rules eﬃciently and react quickly
to network dynamics such as policy changes, topology changes
and host mobility. DIFANE can be easily implemented with
today’s ﬂow-based switches. Our evaluation of the DIFANE
prototype, various networks, and large sets of wildcard rules
show that DIFANE is scalable with networks with a large
number of hosts, ﬂows and rules.
10. ACKNOWLEDGMENT
We thank our shepherd Mark Handley, the anonymous re-
viewers, Nate Foster, Eric Keller, Wyatt Lloyd, Srini Seethara-
man, and Rob Sherwood for their comments on earlier ver-
sions of this paper. We also thank Chad R. Meiners and Alex
X. Liu for providing their code for the ACL compressor.
11. REFERENCES
[1] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar,
L. Peterson, J. Rexford, S. Shenker, and J. Turner, “OpenFlow:
Enabling innovation in campus networks,” ACM Computer
Communication Review, Apr. 2008.
[2] “Anagran: Go with the ﬂow.” http://anagran.com.
[3] M. Casado, M. J. Freedman, J. Pettit, J. Luo, N. Gude,
N. McKeown, and S. Shenker, “Rethinking enterprise network
control,” IEEE/ACM Transactions on Networking, 2009.
[4] N. Gude, T. Koponen, J. Pettit, B. Pfaﬀ, M. Casado,
N. McKeown, and S. Shenker, “NOX: Toward an operating
system for networks,” ACM Computer Communication
Review, July 2008.
[5] A. Greenberg, G. Hjalmtysson, D. A. Maltz, A. Myers,
J. Rexford, G. Xie, H. Yan, J. Zhan, and H. Zhang, “A clean
slate 4D approach to network control and management,” ACM
Computer Communication Review, 2005.
[6] H. Yan, D. A. Maltz, T. S. E. Ng, H. Gogineni, H. Zhang, and
Z. Cai, “Tesseract: A 4D Network Control Plane,” in Proc.
Networked Systems Design and Implementation, Apr. 2007.
[7] A. Nayak, A. Reimers, N. Feamster, and R. Clark, “Resonance:
Dynamic access control for enterprise networks,” in Proc.
Workshop on Research in Enterprise Networks, 2009.
[8] N. Handigol, S. Seetharaman, M. Flajslik, N. McKeown, and
R. Johari, “Plug-n-Serve: Load-balancing Web traﬃc using
OpenFlow,” Aug. 2009. ACM SIGCOMM Demo.
[9] D. Erickson et al., “A demonstration of virtual machine
mobility in an OpenFlow network,” Aug. 2008. ACM
SIGCOMM Demo.
[10] B. Heller, S. Seetharaman, P. Mahadevan, Y. Yiakoumis,
P. Sharma, S. Banerjee, and N. McKeown, “ElasticTree: Saving
energy in data center networks,” in Proc. Networked Systems
Design and Implementation, Apr. 2010.
[11] Y. Mundada, R. Sherwood, and N. Feamster, “An OpenFlow
switch element for Click,” in Symposium on Click Modular
Router, 2009.
[12] A. Tootoocian and Y. Ganjali, “HyperFlow: A distributed
control plane for OpenFlow,” in INM/WREN workshop, 2010.
[13] C. Kim, M. Caesar, and J. Rexford, “Floodless in SEATTLE: A
scalable Ethernet architecture for large enterprises,” in Proc.
ACM SIGCOMM, 2008.
[14] S. Ray, R. Guerin, and R. Soﬁa, “A distributed hash table
based address resolution scheme for large-scale Ethernet
networks,” in Proc. International Conference on
Communications, June 2007.
[15] H. Ballani, P. Francis, T. Cao, and J. Wang, “Making routers
last longer with ViAggre,” in Proc. NSDI, 2009.
[16] M. Yu, J. Rexford, M. J. Freedman, and J. Wang, “Scalable
ﬂow-based networking with DIFANE,” Princeton University
Computer Science Technical Report TR-877-10, June 2010.
[17] Q. Dong, S. Banerjee, J. Wang, and D. Agrawal, “Wire speed
packet classiﬁcation without TCAMs: A few more registers
(and a bit of logic) are enough,” in ACM SIGMETRICS, 2007.
[18] J.-H. Lin and J. S. Vitter, “e-approximations with minimum
packing constraint violation,” in ACM Symposium on Theory
of Computing, 1992.
[19] M. Handley, O. Hudson, and E. Kohler, “XORP: An open
platform for network research,” in Proc. SIGCOMM Workshop
on Hot Topics in Networking, Oct. 2002.
[20] N. Brownlee, “Some observations of Internet stream lifetimes,”
in Passive and Active Measurement, 2005.
[21] “Stanford OpenFlow network real time monitor.”
http://yuba.stanford.edu/~masayosi/ofgates/.
[22] Personal communication with Stanford OpenFlow deployment
group.
[23] “Netlogic microsystems.” www.netlogicmicro.com.
[24] Y.-W. E. Sung, S. Rao, G. Xie, and D. Maltz, “Towards
systematic design of enterprise networks,” in Proc. ACM
CoNEXT, 2008.
362