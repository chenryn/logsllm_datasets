program and to add the signature to TASR. This usually
requires minimal manual eﬀort as most programs tend to
have only a few custom allocators. Of the SPEC C pro-
grams, only gcc and perlbench contain custom allocators
and there are only 31 between them [10].
5.3 Security
5.3.1 Analysis
Traditional memory disclosures directly leak a memory
address or part of an address via benign but vulnerable code.
For example, a format string or buﬀer overread vulnerability
such as Heartbleed [19] could be used by attackers to coerce
the program to output return addresses or function pointers.
Following the output, the attacker can use the learned ad-
dresses to craft a subsequent payload that redirects control
ﬂow to locations discovered via this leak. TASR triggers re-
randomization prior to allowing the process to complete an
input system call, thus preventing such attacks by rendering
the addresses used in the payload stale by the time the pro-
gram attempts to act upon them. In contrast to the case of
standard ASLR where such attacks succeed with certainty,
TASR eﬀectively reverts the attacker’s use of a known ad-
dress to a mere guess of an address that succeeds with a
probability corresponding to the baseline entropy of the tar-
geted memory object. The guess will likely result in the
process crashing. While DoS attacks of this variety are still
possible under TASR, code execution attacks are prevented
with high probability.
Another form of memory disclosure allows an attacker to
indirectly leak memory addresses by guessing addresses and
observing whether or not a process crashes. There are tech-
niques that are more eﬀective than simple brute force guess-
ing: they perform partial overwrites of existing addresses
and guess one byte of an address at a time rather than a
full 64-bit address, reducing the expected number of guesses
on 64-bit Linux from 227 to 640 [9]. This piecewise guess-
ing technique is commonly known as stack reading and was
generalized for use in a form of code reuse attacks known
as Blind ROP attacks [9]. These attacks rely upon the pro-
cess automatically invoking fork() after the crash and with
the same address layout. As TASR triggers rerandomiz-
ation each time fork() is called, any successful guesses of a
single byte of an address are immediately rendered stale at
the next I/O pairing or failed guess, preventing the attacker
from building upon piecewise guesses.
Similarly TASR prevents remote side-channel attacks
(timing and fault analysis) such as those proposed by Seib-
ert et al. [38] because they still rely on (side properties of)
system output to leak memory content. TASR rerandom-
izes the memory after every output rendering information
gained through these side-channels stale.
The recently proposed COOP attack [37] is also prevented
by TASR because it requires the knowledge of the memory
layout (at least for “the base addresses of a set of C++ mod-
ules”). COOP assumes the presence of a direct or indirect
memory disclosure vulnerability. TASR prevents COOP by
hindering an attacker’s ability to gain this knowledge.
5.3.2 Nginx Memory Disclosure Attack
We evaluated TASR on a version of Nginx vulnerable to a
stack-based buﬀer overﬂow [1] discovered in 2013. This vul-
nerability can be used to disclose the contents of the stack,
including return addresses, by writing a guessed value to
the stack and observing whether or not Nginx crashes, in an
attack known as Blind ROP [9]. We used a version of the
Braille [2] exploit tool optimized for conducting this attack
against ASLR-enabled Nginx binaries.
We found that TASR broke this attack in the ﬁrst of the
six attack stages (disclosing the value of a return address).
Braille’s attack depends on being able to read a return ad-
dress byte-by-byte over the course of several requests. How-
ever, when Nginx is run with TASR each request triggers a
rerandomization due to either an I/O operation or a fork()
system call. Due to these rerandomizations, when Braille
ﬁnishes the ﬁrst stage of its attack, it is left with a return
address that does not point to the text section of any run-
ning Nginx process. Braille then proceeds to the second
stage of its attack (ﬁnding both a stop gadget and the PLT).
Braille uses the return address it read in the ﬁrst stage as
an origin and scans backwards looking for a stop gadget.
A stop gadget is any gadget which causes the program to
block, such as a sleep() system call. During our evaluation,
as expected, this stage never terminated when run against
a TASR-protected binary as the pointer Braille used as the
origin of its scan does not point to the text section of any
running Nginx process.
2775.3.3 Limitations
An ideal rerandomization approach would rerandomize
the entire address space, including the data segments. How-
ever, TASR leaves data segments in ﬁxed locations after the
initial load-time randomization has been applied by base
ASLR. The current state of C programs makes it challeng-
ing to deﬁnitively distinguish pointers to data segments from
non-pointer types. While the C standard places restric-
tions on casting between function pointer and non-function
pointer types, there are no such restrictions in casting point-
ers to data to non-pointer types.
It is quite common for
programs to cast integer pointers into simple base integer
types and vice versa. This results in the inability to identify
at rerandomization time whether or not a variable contains
a pointer or a data value, and thus makes the movement
of data unreliable. Unfortunately, excluding data segments
from rerandomization aﬀords minimal additional protection
from data-only modiﬁcations over base ASLR. An attacker
that leaks the location of a data object will be able to access
that object in its same location following the leak.
The ability to modify data additionally has implications
for control ﬂow hijacking attacks. Attackers that leak the
location of a function pointer or return address may be able
to indirectly use the code pointer at that location. Attack-
ers can overwrite pointers to code pointers to point to the
location of a leaked code pointer. Should the benign applica-
tion later attempt to dereference and call the pointer to code
pointer, control will be redirected to the leaked code pointer
instead. While an attacker does not know which value to
use for the code pointer itself since it has been subject to re-
randomization, the layer of indirection makes it suﬃcient to
simply know the location of the code pointer, not the value
itself; this location is in the data segment which remains
ﬁxed.
It is important to note that this attack is far weaker than
the ROP attacks. In particular, the attacker would not be
able to easily chain ROP gadgets that point to arbitrary
addresses as is currently possible. The attacker cannot use
ret or direct/indirect jmp instructions to reach arbitrary
gadgets since the attacker would not actually know the lo-
cation of any gadgets themselves. The attacker is limited
to using function pointers or return addresses that natively
exist in the benign program at the time of attack and whose
locations were leaked. Additionally, the attacker could only
reach them via existing double indirection code (i.e. code
that dereferences and calls a function pointer).
The TASR prototype is also constrained to protecting
against leaks that operate at I/O boundaries. This excludes
attacks from processes that already have shared access to
memory given that addresses could be leaked without any
system output from the vulnerable process. This would
also limit TASR’s eﬀectiveness in environments where the
attacker’s code may be running in the the same address
space as the vulnerable program, such as web browsers in-
terpreting JavaScript. To protect these applications, the
I/O boundary would need to be set as the transition be-
tween the user-supplied JavaScript that is to be interpreted
and the vulnerable engine code that is being exploited. As
mentioned earlier in this section, TASR does not currently
support interpreted or runtime generated code but could be
extended to do so using the same principles described here.
We leave this component to future work.
Control ﬂow hijacking attacks that do not rely upon mem-
ory disclosures naturally remain possible. One such attack
is a partial overwrite attack [14] in which an attacker is able
to overwrite part of an address in order to reach a target
object that is ﬁxed relatively to that address. Rerandom-
ization does not impact the relative positioning of objects.
Fine-grained ASLR techniques that randomize the relative
positioning of objects [21, 46, 27] provide one promising ap-
proach that could complement TASR in order to protect
against these attacks, and in fact TASR already makes use
of whatever base ASLR technique that the system can sup-
port.
6. CONCLUSION
TASR is a technique that introduces the concept of “time-
liness” to the broader area of code randomization in order to
enable address space re-randomization. It builds upon the
concepts introduced by ASLR and allows programs to be
rerandomized at arbitrary points in their lifespan. By care-
ful selection of those rerandomization points, the impact of
memory disclosure vulnerabilities can be entirely mitigated
as applied to code reuse attacks. Because attackers never
have the opportunity to make use of the information dis-
closed, a successful code reuse attack cannot be carried out.
Application of TASR imposes a reasonable performance
overhead (2.1% on average) over a wide variety of tested
programs. Although not yet applicable to certain classes of
program, it requires only recompilation for many existing
codebases without requiring further manual changes.
Future work will focus on adding support for interpreted
environments and additional custom memory allocators.
7. REFERENCES
[1] Cve-2013-2028. Online, 2013.
[2] Blind return oriented programming. Online, 2014.
[3] Abadi, M., Budiu, M., Erlingsson, U., and
Ligatti, J. Control-ﬂow integrity. In Proc. of ACM
CCS (2005).
[4] Akritidis, P. Cling: A memory allocator to mitigate
dangling pointers. In Proc. of USENIX Security
(2010).
[5] Anderson, J. P. Computer security technology
planning study. volume 2. Tech. rep., DTIC
Document, 1972.
[6] Backes, M., Holz, T., Kollenda, B., Koppe, P.,
N¨urnberger, S., and Pewny, J. You can run but
you can’t read. In Proc. of ACM CCS (2014).
[7] Backes, M., and N¨urnberger, S. Oxymoron:
Making ﬁne-grained memory randomization practical
by allowing code sharing. Proc. of USENIX Security
(2014).
[8] Barrantes, E. G., Ackley, D. H., Palmer, T. S.,
Stefanovic, D., and Zovi, D. D. Randomized
instruction set emulation to disrupt binary code
injection attacks. In Proc. of ACM CCS (2003).
[9] Bittau, A., Belay, A., Mashtizadeh, A.,
Mazieres, D., and Boneh, D. Hacking blind. In
Proc. of IEEE S&P (2014).
[10] Chen, X., Slowinska, A., and Bos, H. Membrush:
A practical tool to detect custom memory allocators
in c binaries. In Proc. of WCRE (2013).
278[11] Crane, S., Liebchen, C., Homescu, A., Davi, L.,
[31] Nagarakatte, S., Zhao, J., Martin, M. M., and
Larsen, P., Sadeghi, A.-R., Brunthaler, S., and
Franz, M. Readactor: Practical code randomization
resilient to memory disclosure. In IEEE S&P (2015).
Zdancewic, S. Softbound: Highly compatible and
complete spatial memory safety for c. In Proc. of
PLDI (2009).
[12] Curtsinger, C., and Berger, E. D. Stabilizer:
[32] Nagarakatte, S., Zhao, J., Martin, M. M., and
Statistically sound performance evaluation. In Proc. of
ASPLOS (2013).
Zdancewic, S. Cets: Compiler enforced temporal
safety for c. In Proc. of ISMM (2010).
[13] Davi, L., Liebchen, C., Sadeghi, A.-R., Snow,
[33] One, A. Smashing the stack for fun and proﬁt. Phrack
K. Z., and Monrose, F. Isomeron: Code
randomization resilient to (just-in-time)
return-oriented programming. Proc. of NDSS (2015).
[14] Durden, T. Bypassing pax aslr protection, 2002.
[15] Eager, M. J. Introduction to the dwarf debugging
format. Group (2007).
[16] Evans, I., Fingeret, S., Gonzalez, J.,
Otgonbaatar, U., Tang, T., Shrobe, H.,
Sidiroglou-Douskos, S., Rinard, M., and
Okhravi, H. Missing the point(er): On the
eﬀectiveness of code pointer integrity. In Proc. of
IEEE S&P (2015).
[17] Giuffrida, C., Kuijsten, A., and Tanenbaum,
A. S. Enhanced operating system security through
eﬃcient and ﬁne-grained address space randomization.
In Proc. of USENIX Security (2012).
[18] G¨oktas, E., Athanasopoulos, E., Bos, H., and
Portokalidis, G. Out of control: Overcoming
control-ﬂow integrity. In Proc. of IEEE S&P (2014).
[19] Heartbleed.com. The heartbleed bug. Online, 2014.
[20] Hiser, J., Nguyen, A., Co, M., Hall, M., and
Davidson, J. Ilr: Where’d my gadgets go. In Proc. of
IEEE S&P (2012).
magazine 7, 49 (1996), 14–16.
[34] Parno, B., McCune, J. M., and Perrig, A.
Bootstrapping trust in commodity computers. In Proc.
of IEEE S&P (may 2010), pp. 414 –429.
[35] PaX. Pax address space layout randomization, 2003.
[36] Rafkind, J., Wick, A., Regehr, J., and Flatt,
M. Precise garbage collection for c. In Proc. of ISMM
(2009).
[37] Schuster, F., Tendyck, T., Liebchen, C., Davi,
L., Sadeghi, A.-R., and Holz, T. Counterfeit
object-oriented programming: On the diﬃculty of
preventing code reuse attacks in c++ applications. In
Proc. of IEEE S&P (2015).
[38] Seibert, J., Okhravi, H., and Soderstrom, E.
Information leaks without memory disclosures:
Remote side channel attacks on diversiﬁed code. In
Proc. of ACM CCS (2014).
[39] Serebryany, K., Bruening, D., Potapenko, A.,
and Vyukov, D. Addresssanitizer: A fast address
sanity checker. In USENIX (2012).
[40] Shacham, H. The geometry of innocent ﬂesh on the
bone: Return-into-libc without function calls (on the
x86). In Proc. of ACM CCS (2007).
[21] Hiser, J., Nguyen, A., Co, M., Hall, M., and
[41] Shacham, H., Page, M., Pfaff, B., Goh, E.-J.,
Davidson, J. Ilr: Where’d my gadgets go. In Proc. of
IEEE S&P (2012).
[22] Hobson, T., Okhravi, H., Bigelow, D., Rudd, R.,
and Streilein, W. On the challenges of eﬀective
movement. In Proceedings of the First ACM Workshop
on Moving Target Defense (2014), pp. 41–50.
[23] ISO. ISO/IEC 9899:2011 Information technology —
Programming languages — C. 2011.
[24] Jackson, T., Salamat, B., Homescu, A.,
Manivannan, K., Wagner, G., Gal, A.,
Brunthaler, S., Wimmer, C., and Franz, M.
Compiler-generated software diversity. Moving Target
Defense (2011), 77–98.
[25] Jim, T., Morrisett, J. G., Grossman, D., Hicks,
M. W., Cheney, J., and Wang, Y. Cyclone: A safe
dialect of c. In USENIX (2002).
[26] Kc, G. S., Keromytis, A. D., and Prevelakis, V.
Countering code-injection attacks with instruction-set
randomization. In Proc. of ACM CCS (2003).
[27] Kil, C., Jun, J., Bookholt, C., Xu, J., and Ning,
P. Address space layout permutation (aslp). In Proc.
of ACSAC (2006).
[28] Kuznetsov, V., Szekeres, L., Payer, M., Candea,
G., Sekar, R., and Song, D. Code-pointer integrity.
[29] Mohan, V., Larsen, P., Brunthaler, S., Hamlen,
K., and Franz, M. Opaque control-ﬂow integrity. In
Proc. of NDSS (2015).
[30] Mosberger, D. The libunwind project, 2014.
Modadugu, N., and Boneh, D. On the eﬀectiveness
of address-space randomization. In Proc. of ACM CCS
(2004).
[42] Snow, K. Z., Monrose, F., Davi, L., Dmitrienko,
A., Liebchen, C., and Sadeghi, A.-R. Just-in-time
code reuse: On the eﬀectiveness of ﬁne-grained
address space layout randomization. In Proc. of IEEE
S&P (2013).
[43] Strackx, R., Younan, Y., Philippaerts, P.,
Piessens, F., Lachmund, S., and Walter, T.
Breaking the memory secrecy assumption. In Proc. of
EuroSec’09 (2009), pp. 1–8.
[44] Szekeres, L., Payer, M., Wei, T., and Song, D.
Sok: Eternal war in memory. In Proc. of IEEE S&P
(2013).
[45] Tice, C., Roeder, T., Collingbourne, P.,
Checkoway, S., Erlingsson, ´U., Lozano, L., and
Pike, G. Enforcing forward-edge control-ﬂow integrity
in gcc & llvm. In Proc. of USENIX Security (2014).
[46] Wartell, R., Mohan, V., Hamlen, K. W., and
Lin, Z. Binary stirring: Self-randomizing instruction
addresses of legacy x86 binary code. In Proc. of ACM
CCS (2012), pp. 157–168.
[47] Zhang, C., Wei, T., Chen, Z., Duan, L.,
Szekeres, L., McCamant, S., Song, D., and Zou,
W. Practical control ﬂow integrity and randomization
for binary executables. In Proc. of IEEE S&P (2013).
[48] Zhang, M., and Sekar, R. Control ﬂow integrity for
cots binaries. In Proc. of USENIX Security (2013).
279