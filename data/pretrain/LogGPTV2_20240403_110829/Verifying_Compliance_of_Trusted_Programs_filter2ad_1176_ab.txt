SELinux Trusted Program Package
SELinux policy 
module
application
ﬁles
program
policy
Compliance evaluation (1): 
IF tamper protection
Compliance evaluation (2): 
IF program access control 
policy
Package Installation:
(1) Policy 
(2) Files
SElinux system policy
Figure 1: Deployment and Installation of a trusted package. First, we check two compliance goals: (1) the system
protects the application and (2) the application enforces system goals. Second the package is installed: the policy
module is integrated into the system policy and application ﬁles are installed.
2.3 Program Enforcement
To justify a system’s trust, any trusted program must en-
force a policy that complies with the system’s security
goals. The reference monitor concept [1] has been the
guide for determining whether a system enforces its se-
curity goals, and we leverage this concept in deﬁning
compliance. A reference monitor requires three guaran-
tees to be achieved: (1) complete mediation must ensure
that all security-sensitive operations are authorized; (2)
a reference monitor must be tamperproof to enforce its
policy correctly; and (3) a reference monitor must be
simple enough to verify enforcement of security goals.
While the reference monitor concept is most identiﬁed
with operating system security, a trusted program must
also satisfy these guarantees to ensure that a system’s se-
curity goal is enforced. As a result, we deﬁne that a pro-
gram enforces a system’s security goals if it satisﬁes the
reference monitor guarantees in its deployment on that
system.
In prior work, we developed a veriﬁcation method that
partially fulﬁlled these requirements. We developed a
service, called SIESTA, that compares program policies
against SELinux system policies, and only executes pro-
grams whose policies permit information ﬂows autho-
rized in the system policy [13]. This work considered
two of the reference monitor guarantees. First, we used
the SIESTA service to verify trust in the Jif STL imple-
mentation of the logrotate program. Since the Jif
compiler guarantees enforcement of the associated pro-
gram policy, this version of logrotate provides com-
plete mediation, modulo the Java Virtual Machine. Sec-
ond, SIESTA performs a policy analysis to ensure that
the program policy complies with system security goals
(i.e., the SELinux MLS policy). Compliance was deﬁned
as requiring that the logrotate policy only authorized
an operation if the SELinux MLS policy) also permitted
that operation. Thus, SIESTA is capable of verifying a
program’s enforcement of system security goals.
We ﬁnd two limitations to this work. First, we had to
construct the program access control policy relating sys-
tem and program objects in an ad hoc manner. As the
resultant program policy speciﬁed the union of the sys-
tem and program policy requirements, it was much more
complex than we envisioned. Not only is it difﬁcult to
design a compliant program access control policy, but
that policy may only apply to a small number of target
environments. As we discussed in Section 2.1, program
policies should depend on system policies, particularly
for trusted programs that we expect to enforce the sys-
tem’s policy, making them non-portable unless we are
careful. Second, this view of compliance does not pro-
tect the trusted program from tampering. As described
above, untrusted programs could obtain access to the
trusted program’s ﬁles after the package is installed, if
the integrated SELinux system policy authorizes it. For
example, if an untrusted program has write access to the
/etc directory where conﬁguration ﬁles are installed,
as we demonstrated was possible in Section 2.2, SIESTA
will not detect that such changes are possible.
In summary, Figure 1 shows that we aim to deﬁne an
approach that ensures the following requirements:
• For any system deployment of a trusted program,
automatically construct a program policy that is
compliant with the system security goals, thus satis-
fying the reference monitor guarantee of being sim-
ple enough to verify.
• For any system deployment of a trusted program,
verify, in a mostly-automated way, that the system
policy does not permit tampering of the trusted pro-
gram by any untrusted program, thus satisfying the
reference monitor guarantee of being tamperproof.
The typical number of veriﬁcation errors must be
324 
17th USENIX Security Symposium 
USENIX Association
Program Policy 
for the System 
Policy for 
System Data
Program
Policy
Information-
Flow Goals
Complies with?
System
Policy
Tamperproof
Goals
Complies with?
System Policy 
w/ the Program 
Policy for 
Protecting
Program
Figure 2: The two policy compliance problems: (1) ver-
ify that the program policy complies with the system’s
information ﬂow goals and (2) verify that the system pol-
icy, including the program contribution (e.g., SELinux
policy module), enforces the tamperprooﬁng goals of the
program.
small and there must be a set of manageable resolu-
tions to any such errors.
In the remainder of the paper, we present a single ap-
proach that solves both of these requirements.
3 Policy Compliance
Veriﬁcation of these two trusted program requirements
results in the same conceptual problem, which we call
policy compliance problems. Figure 2 shows these two
problems. First, we must show that the program policy
only authorizes operations permitted by the system’s se-
curity goal policy. While we have shown a method by
which such compliance can be tested previously [13,14],
the program policy was customized manually for the sys-
tem. Second, we also ﬁnd that the system policy must
comply with the program’s tamperproof goals. That is,
the system policy must not allow any operation that per-
mits tampering the trusted program. As a result, we need
to derive the tamperproof goals from the program (e.g.,
from the SELinux policy module).
In this section, we deﬁne the formal model for veri-
fying policy compliance suitable for both the problems
above. However, as can be seen from Figure 2, the
challenge is to develop system security goals, program
policies, and tamperproof goals in a mostly-automated
fashion that will encourage successful compliance. The
PIDSI approach in Section 4 provides such guidance.
3.1 Policy Compliance Model
We specify system-wide information-ﬂow goals as a se-
curity lattice L. We assume that elements of L have both
an integrity and a conﬁdentiality component: this is the
case for both MLS labels in SELinux [11] and labels
from the DLM [25]. Let Integrity(l) and Conf(l) be the
integrity and conﬁdentiality projections of a label l ∈ L,
respectively. Let the lattice L have both a top element,
, and a bottom element ⊥. We use high = Integrity(⊥)
and low = Integrity() to denote high and low integrity
and write high  low to indicate that high integrity data
can ﬂow to a low integrity security label, but not the re-
verse.
An information-ﬂow graph is a directed graph G =
(V, E) where V is the set of vertices, each associated
with a label from a security lattice L. We write V (G) for
the vertices of G and E(G) for the edges of G, and for
v ∈ V (G) we write Type(v) for the label on the vertex
v. Both subjects (e.g., processes and users) and objects
(e.g., ﬁles and sockets) are assigned labels from the same
security lattice L. The edges in G describe the informa-
tion ﬂows that a policy permits.
We now formally deﬁne the the concept of compliance
between a graph G and a security lattice L. For u, v ∈
V (G), we write u  v if there is a path between vertices
u and v in the graph G. An information-ﬂow graph G is
compliant with a security lattice L if all paths through the
combined information-ﬂow graph imply that there is a
ﬂow in L between the types of the elements in the graph.
Deﬁnition 3.1 (Policy Compliance). An information-
ﬂow graph G is compliant with a security lattice L if
for each u, v ∈ V (G) such that u  v, then Type(u) 
Type(v) in the security lattice L.
With respect to MAC policies, a positive result of the
compliance test implies that the information-ﬂow graph
for a policy does not permit any operations that violate
the information-ﬂow goals as encoded in the lattice L.
If G is the information ﬂow graph of a trusted program
together with the system policy, then a compliance test
veriﬁes that the trusted program only permits informa-
tion ﬂows allowed by the operating system, as we desire.
3.2 Difﬁculty of Compliance Testing
The main difﬁculty in compliance testing is in automati-
cally constructing the program, system, and goal policies
shown in Figure 2. Further, we prefer design construc-
tions that will be likely to yield successful compliance.
The two particularly difﬁcult cases are the program
policies (i.e., upper left in the ﬁgure) and the tamper-
proof goal policy (i.e. lower right in the ﬁgure). The pro-
gram policy and tamperproof goal policies require pro-
gram requirements to be integrated with system require-
ments, whereas the system policy and system security
goals are largely (although not necessarily completely)
independent of the program policy. For example, while
the system policy must include information ﬂows for the
USENIX Association  
17th USENIX Security Symposium 
325
program, the SELinux system includes policy modules
for the logrotate and other trusted programs that can
be combined directly.
First, it is necessary for program policies (i.e., upper
left in the ﬁgure) to manage system objects, but often
program policy and system policy are written with dis-
joint label sets. Thus, some mapping from program la-
bels to system labels is necessary to construct a system-
aware program policy before the information ﬂow goals
encoded in L can be evaluated. Let P be an information-
ﬂow graph relating the program subjects and objects and
and S be information-ﬂow graph relating the system sub-
jects and objects. Let P ⊕S be the policy that arises from
combining P and S to form one information-ﬂow graph
through some sound combination operator ⊕; that is, if
there is a runtime ﬂow in the policy S where the program
P has been deployed, then there is a ﬂow in the informa-
tion ﬂow graph P ⊕ S. Currently, there are no automatic
ways to combine such program and system graphs into
a system-aware program policy, meaning that ⊕ is im-
plemented in a manual fashion. A manual mapping was
used in previous work on compliance [13].
Second, the tamperproof goal policy (i.e., lower right
in the ﬁgure) derives from the program’s integrity re-
quirements for its objects. Historically, such require-
ments are not explicitly speciﬁed, so it is unclear which
program labels imply high integrity and which ﬁles
should be assigned those high integrity labels. With the
use of packages and program policy modules, the pro-
gram ﬁles and labels are identiﬁed, but we still lack in-
formation about what deﬁnes tamperprooﬁng for the pro-
gram. Also, some program ﬁles may be created at in-
stallation time, rather than provided in packages, so the
integrity of these ﬁles needs to be determined as well.
We need a way to derive tamperproof goals automati-
cally from packages and policy modules.
4 PIDSI Approach
We propose the PIDSI approach (Program Integrity
Dominates System Integrity), where the trusted pro-
gram objects (i.e., package ﬁles and ﬁles labeled using
the labels deﬁned by the module policy) are labeled such
that their integrity is relative to all system objects. The
information ﬂows between the system and the trusted
program can then be inferred from this relationship. We
have found that almost all trusted program objects are
higher integrity than system objects (i.e., system data
should not ﬂow to trusted program objects). One excep-
tion that we have found is that both trusted and untrusted
programs are authorized to write to some log ﬁles. How-
ever, a trusted program should not depend on the data in
a log ﬁle. While general cases may eventually be iden-
tiﬁed automatically as low integrity, at present we may
have a small number of cases where the integrity level
H
High Integrity (default)
X
System Info Flow 
Graph, (S)
Program Info 
Flow Graph (P)
Low Integrity (rare)
L
Figure 3: The PIDSI approach relates program labels P
to system labels S, such that the program-deﬁned ob-
jects are higher integrity than the system data objects (as-
signed to H), with some small number of low integrity
exceptions (assigned to L).
must be set manually.
Our approach takes advantage of a distinction between
the protection of the trusted program and protection of
the data to which it is applied. Trusted program pack-
ages contain the ﬁles necessary to execute the program,
and the integrity of the program’s execution requires pro-
tection of these ﬁles. On the other hand, the program is
typically applied to data whose protection requirements
are deﬁned by the system.
4.1 PIDSI Deﬁnition
By using the PIDSI approach between trusted program
and the system, we can deploy that trusted program on
different systems, ensuring compliance. Figure 3 demon-
strates this approach. First, the program deﬁnes its own
set of labels, which are designed either as high or low
integrity. When the program is deployed, the system la-
bels are placed in between the program’s high and low
integrity labels. This allows an easy check of whether a
program is compliant with the system’s policy, regardless
of the speciﬁc mappings from system inputs and outputs
to program inputs and outputs.
In the event that the trusted program allows data at a
low integrity label to ﬂow to a high label, then this ap-
proach can trick the system into trusting low integrity
data. To eliminate this possibility, we automatically ver-
ify that no such ﬂows are present in the program policy.
For conﬁdentiality, we found that the data stored by
most trusted programs was intended to be low secrecy.
The only exception to this rule that we found in the
trusted program core of SELinux was sshd; this pro-
gram managed SSH keys at type sshd key t, which
needed to be kept secret5. We note that if program data
is low secrecy as well as high integrity the same infor-
326 
17th USENIX Security Symposium 
USENIX Association
mation ﬂows result, system data may not ﬂow to program
data, so no change is required to the PIDSI approach. Be-
cause of this, we primarily evaluate the PIDSI approach
with respect to integrity.
In this context,
the compliance problem requires
checking that the system’s policy, when added to the pro-
gram, does not allow any new illegal ﬂows. We con-
struct the composed program policy P  from P and S.
To composte P and S into P  = P ⊕ S, ﬁrst, split P
into subgraphs H and L as follows: if u ∈ P is such that
Integrity(Type(u)) = high, then u ∈ H, and if u ∈ P
is such that Integrity(Type(u)) = low, then u ∈ L. P 