1 / 5
3 / 5
4/10
2 / 5
2 / 5
4/10
Figure 3: (Left) Results of launching p probes 5 minutes after the launch of v victims. The rightmost column speciﬁes
success coverage: the number of victims for which a probe instance was co-resident over the total number of victims.
(Right) The number of victims for which a probe achieved co-residence for three separate runs of 10 repetitions of
launching 1 victim instance and, 5 minutes later, 20 probe instances. Odd-numbered repetitions used Account A;
even-numbered repetitions used Account B.
time a recently launched victim is quickly and easily “found”
in the cloud. Moreover, neither the account used for the vic-
tims nor the portion of the day during which the experiment
was conducted signiﬁcantly aﬀected the rate of success.
The eﬀect of increased time lag. Here we show that
the window of opportunity an attacker has for launching
instances is quite large. We performed the following exper-
iment. Forty victim instances (across two accounts) were
initially launched in Zone 3 and continued running through-
out the experiment. These were placed on 36 unique ma-
chines (8 victims were co-resident with another victim). Ev-
ery hour a set of 20 attack instances (from a third account)
were launched in the same zone and co-residence checks were
performed. These instances were terminated immediately
after completion of the checks. Figure 4 contains a graph
showing the success rate of each attack round, which stays
essentially the same over the course of the whole experiment.
(No probes were reported upon for the hours 34–43 due to
our scripts not gracefully handling some kinds of EC2-caused
launch failures, but nevertheless reveals useful information:
the obvious trends were maintained regardless of continuous
probing or not.) Ultimately, co-residence with 24 of the 36
machines running victim instances was established. Addi-
tionally, probes were placed on all four machines which had
two victim instances, thus giving three-way collisions.
The right graph in Figure 4 shows the cumulative num-
ber of unique Dom0 IP addresses seen by the probes over
the course of the experiment. This shows that the growth
in the number of machines probes were placed on levels oﬀ
rapidly — quantitative evidence of sequential placement lo-
cality.
On targeting commercial instances. We brieﬂy exper-
imented with targeted instance ﬂooding against instances
run by other user’s accounts. RightScale is a company that
oﬀers “platform and consulting services that enable compa-
nies to create scalable web solutions running on Amazon
Web Services” [28]. Presently, they provide a free demon-
stration of their services, complete with the ability to launch
a custom EC2 instance. On two separate occasions, we setup
distinct accounts with RightScale and used their web inter-
face to launch one of their Internet appliances (on EC2).
We then applied our attack strategy (mapping the fresh in-
stance and then ﬂooding). On the ﬁrst occasion we sequen-
tially launched two rounds of 20 instances (using a single
account) before achieving co-residence with the RightScale
instance. On the second occasion, we launched two rounds of
38 instances (using two accounts). In the second round, we
achieved a three-way co-residency: an instance from each
of our accounts was placed on the same machine as the
RightScale server.
rPath is another company that oﬀers ready-to-run Inter-
net appliances powered by EC2 instances [29]. As with
RightScale, they currently oﬀer free demonstrations, launch-
ing on demand a fresh EC2 instance to host systems such
as Sugar CRM, described as a “customer relationship man-
agement system for your small business or enterprise” [29].
We were able to successfully establish a co-resident instance
against an rPath demonstration box using 40 instances. Sub-
sequent attempts with fresh rPath instances on a second oc-
casion proved less fruitful; we failed to achieve co-residence
even after several rounds of ﬂooding. We believe that the
target in this case was placed on a full system and was there-
fore unassailable.
Discussion. We have seen that attackers can frequently
achieve co-residence with speciﬁc targets. Why did the strat-
egy fail when it did? We hypothesize that instance ﬂooding
failed when targets were being assigned to machines with
high instance density (discussed further in Appendix B) or
even that became full. While we would like to use network
probing to better understand this eﬀect, this would require
port scanning IP addresses near that of targets, which would
perhaps violate (the spirit of) Amazon’s AUP.
7.3 Patching placement vulnerabilities
The EC2 placement algorithms allow attackers to use rel-
atively simple strategies to achieve co-residence with victims
(that are not on fully-allocated machines). As discussed ear-
lier, inhibiting cartography or co-residence checking (which
would make exploiting placement more diﬃcult) would seem
insuﬃcient to stop a dedicated attacker. On the other hand,
there is a straightforward way to “patch” all placement vul-
nerabilities: oﬄoad choice to users. Namely, let users re-
quest placement of their VMs on machines that can only be
populated by VMs from their (or other trusted) accounts.
In exchange, the users can pay the opportunity cost of leav-
ing some of these machines under-utilized.
In an optimal
assignment policy (for any particular instance type), this
additional overhead should never need to exceed the cost of
a single physical machine.
206Total co-resident
New co-resident
s
e
c
n
a
t
s
n
i
f
o
r
e
b
m
u
N
16
14
12
10
8
6
4
2
0
0
10
20
30
40
50
s
t
n
e
m
n
g
i
s
s
a
0
m
o
D
e
u
q
i
n
U
60
55
50
45
40
35
30
25
20
0
10
20
30
40
50
Hours since victims launched
Hours since victims launched
Figure 4: Results for the experiment measuring the eﬀects of increasing time lag between victim launch and probe
launch. Probe instances were not run for the hours 34–43. (Left) “Total co-resident” corresponds to the number of
probe instances at the indicated hour oﬀset that were co-resident with at least one of the victims. “New co-resident”
is the number of victim instances that were collided with for the ﬁrst time at the indicated hour oﬀset. (Right) The
cumulative number of unique Dom0 IP addresses assigned to attack instances for each round of ﬂooding.
8. CROSS-VM INFORMATION LEAKAGE
The previous sections have established that an attacker
can often place his or her instance on the same physical
machine as a target instance. In this section, we show the
ability of a malicious instance to utilize side channels to
learn information about co-resident instances. Namely we
show that (time-shared) caches allow an attacker to measure
when other instances are experiencing computational load.
Leaking such information might seem innocuous, but in fact
it can already be quite useful to clever attackers. We intro-
duce several novel applications of this side channel: robust
co-residence detection (agnostic to network conﬁguration),
surreptitious detection of the rate of web traﬃc a co-resident
site receives, and even timing keystrokes by an honest user
(via SSH) of a co-resident instance. We have experimentally
investigated the ﬁrst two on running EC2 instances. For the
keystroke timing attack, we performed experiments on an
EC2-like virtualized environment.
On stealing cryptographic keys. There has been a long
line of work (e.g., [10, 22, 26]) on extracting cryptographic
secrets via cache-based side channels. Such attacks, in the
context of third-party compute clouds, would be incredibly
damaging — and since the same hardware channels exist, are
fundamentally just as feasible.
In practice, cryptographic
cross-VM attacks turn out to be somewhat more diﬃcult to
realize due to factors such as core migration, coarser schedul-
ing algorithms, double indirection of memory addresses, and
(in the case of EC2) unknown load from other instances
and a fortuitous choice of CPU conﬁguration (e.g. no hy-
perthreading). The side channel attacks we report on in
the rest of this section are more coarse-grained than those
required to extract cryptographic keys. While this means
the attacks extract less bits of information, it also means
they are more robust and potentially simpler to implement
in noisy environments such as EC2.
Other channels; denial of service. Not just the data
cache but any physical machine resources multiplexed be-
tween the attacker and target forms a potentially useful
channel: network access, CPU branch predictors and in-
struction cache [1, 2, 3, 12], DRAM memory bus [21], CPU
pipelines (e.g., ﬂoating-point units) [4], scheduling of CPU
cores and timeslices, disk access [16], etc. We have imple-
mented and measured simple covert channels (in which two
instances cooperate to send a message via shared resource)
using memory bus contention, obtaining a 0.006bps channel
between co-resident large instances, and using hard disk con-
tention, obtaining a 0.0005bps channel between co-resident
m1.small instances. In both cases no attempts were made
at optimizing the bandwidth of the covert channel. (The
hard disk contention channel was used in Section 6 for estab-
lishing co-residence of instances.) Covert channels provide
evidence that exploitable side channels may exist.
Though this is not our focus, we further observe that the
same resources can also be used to mount cross-VM per-
formance degradation and denial-of-service attacks, analo-
gously to those demonstrated for non-virtualized multipro-
cessing [12, 13, 21].
8.1 Measuring cache usage
An attacking instance can measure the utilization of CPU
caches on its physical machine. These measurements can
be used to estimate the current load of the machine; a high
load indicates activity on co-resident instances. Here we
describe how to measure cache utilization in EC2 instances
by adapting the Prime+Probe technique [22, 32]. We also
demonstrate exploiting such cache measurements as a covert
channel.
Load measurement. We utilize the Prime+Probe tech-
nique [22, 32] to measure cache activity, and extend it to
the following Prime+Trigger+Probe measurement to sup-
port the setting of time-shared virtual machines (as present
on Amazon EC2). The probing instance ﬁrst allocates a con-
tiguous buﬀer B of b bytes. Here b should be large enough
that a signiﬁcant portion of the cache is ﬁlled by B. Let s
be the cache line size, in bytes. Then the probing instance
performs the following steps to generate each load sample:
(1) Prime: Read B at s-byte oﬀsets in order to ensure it is
cached.
(2) Trigger: Busy-loop until the CPU’s cycle counter jumps
by a large value. (This means our VM was preempted by
the Xen scheduler, hopefully in favor of the sender VM.)
(3) Probe: Measure the time it takes to again read B at
s-byte oﬀsets.
When reading the b/s memory locations in B, we use a
pseudorandom order, and the pointer-chasing technique de-
207scribed in [32], to prevent the CPU’s hardware prefetcher
from hiding the access latencies. The time of the ﬁnal step’s
read is the load sample, measured in number of CPU cycles.
These load samples will be strongly correlated with use of
the cache during the trigger step, since that usage will evict
some portion of the buﬀer B and thereby drive up the read
time during the probe phase. In the next few sections we
describe several applications of this load measurement side
channel. First we describe how to modify it to form a robust
covert channel.
Cache-based covert channel. Cache load measurements
create very eﬀective covert channels between cooperating
processes running in diﬀerent VMs. In practice, this is not
a major threat for current deployments since in most cases
the cooperating processes can simply talk to each other over
a network. However, covert channels become signiﬁcant
when communication is (supposedly) forbidden by informa-
tion ﬂow control (IFC) mechanisms such as sandboxing and
IFC kernels [34, 18, 19]. The latter are a promising emerg-
ing approach to improving security (e.g., web-server func-
tionality [18]), and our results highlight a caveat to their
eﬀectiveness.
In the simplest cache covert-channel attack [15], the sender
idles to transmit “0” and frantically accesses memory to
transmit “1”. The receiver accesses a memory block of his
own and observes the access latencies. High latencies are in-
dicative that the sender is evicting the receiver’s data from
the caches, i.e., that “1” is transmitted. This attack is ap-
plicable across VMs, though it tends to be unreliable (and
thus has very low bandwidth) in a noisy setting.
We have created a much more reliable and eﬃcient cross-
VM covert channel by using ﬁner-grained measurements.
We adapted the Prime+Trigger+Probe cache measurement
technique as follows. Recall that in a set-associative cache,
the pool of cache lines is partitioned into associativity sets,
such that each memory address is mapped into a speciﬁc
associativity set determined by certain bits in the address
(for brevity, we ignore here details of virtual versus physi-
cal addresses). Our attack partitions the cache sets into two
classes, “odd sets” and “even sets”, and manipulates the load
across each class. For resilience against noise, we use diﬀer-
ential coding where the signal is carried in the diﬀerence
between the load on the two classes. Noise will typically
be balanced between the two classes, and thus preserve the
signal.
(This argument can be made rigorous by using a
random-number generator for the choice of classes, but the
following simpler protocol works well in practice.)