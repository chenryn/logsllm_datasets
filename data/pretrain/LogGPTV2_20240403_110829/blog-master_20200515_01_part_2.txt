```  
2、block被扫描数过多: 1475   
因为每个vid有多条, 分组时扫描了指定分组对应的所有heap(通过 limit 限制)   
limit越大, 浪费越多.   
虽然sql看起来好像很快, 实际上浪费很多(特别是vid重复记录多, limit多时浪费大).   能不能再优化, 特别是高并发下, 效果明显.     
### 方法  
递归, 看起来行不通, 因为相同score可能有多个vid, 所以需要考虑score和vid两个跳跃条件. 而递归的边界比较好理解的是1个条件, 逐渐跳跃(类似skip index scan).   
如果明确不同的score vid一定不同, 那么跳跃边界条件就是score, 没有vid了.  
思路:   
把vid作为权重放到score中, 而且要不影响原来的score排序顺序, 可以使用表达式把它加权到score中(而且不能改变原来的score排序顺序).   
例如score的值一定大于 1/1e21 ,那么可以把vid加权到1/1e21后面的小数位. 当然，数据类型精度必须确保能存下加权后的小数位,例如本例使用的是numeric类型，未限制精度，精确到e25没有问题 ,否则必须指定小数点大于或等于25位.      
vid转换为数值(如果vid不是数值, 使用pg内置hash函数可以转换为数值), 然后取模(控制vid对撞空间, 同时确保不需要增加太多小数位来存储加权值)     
如下, 9999需要4个小数位来存储加权, 所以除以1e25          (21+4)  创建如下索引:   
```  
create index idx_tab1_2 on tab1 ( (score + (mod(vid,9999)/1e25)) );    
```  
```  
explain (analyze,verbose,timing,costs,buffers)  
with recursive tmp as (        -- 递归                                                                                                            
(                                                                                                                                             
  select array[vid::numeric, score, (score + (mod(vid,9999)/1e25))] as r   
  from tab1   
  order by (score + (mod(vid,9999)/1e25)) desc limit 1   -- 取最大score一条    
)                                                                                                                        
union all                                                                                                               
(                                                                                                  
select                                                                    
  (select array[vid::numeric, score, (score + (mod(vid,9999)/1e25))] as r  from tab1 t1   
    where (t1.score + (mod(t1.vid,9999)/1e25))  t.vids)   
limit 20   
;   
```  
小缺陷:    
当多个vid的score值相同, 并且vid取模9999后的值也相同时, 会导致只取一条vid.     
```  
   vid   |           score              
---------+----------------------------  
  999998 | 0.000000099999800000000000  
  999997 | 0.000000099999700000000000  
  999996 | 0.000000099999600000000000  
  999994 | 0.000000099999400000000000  
  999993 | 0.000000099999300000000000  
 1000009 | 0.000000099999100000000000  
 1000008 | 0.000000099999100000000000  
 1000007 | 0.000000099999100000000000  
 1000006 | 0.000000099999100000000000  
 1000005 | 0.000000099999100000000000  
 1000004 | 0.000000099999100000000000  
 1000003 | 0.000000099999100000000000  
 1000002 | 0.000000099999100000000000  
 1000001 | 0.000000099999100000000000  
  999991 | 0.000000099999100000000000  
  999990 | 0.000000099999000000000000  
  999989 | 0.000000099998900000000000  
  999988 | 0.000000099998800000000000  
  999987 | 0.000000099998700000000000  
  999986 | 0.000000099998600000000000  
(20 rows)  
```  
```  
 Limit  (cost=117.68..118.37 rows=20 width=40) (actual time=0.077..0.336 rows=20 loops=1)  
   Output: ((v.r[1])::bigint), (v.r[2])  
   Buffers: shared hit=121  
   CTE tmp  
     ->  Recursive Union  (cost=0.57..92.68 rows=101 width=32) (actual time=0.021..0.292 rows=24 loops=1)  
           Buffers: shared hit=120  
           ->  Subquery Scan on "*SELECT* 1"  (cost=0.57..0.88 rows=1 width=32) (actual time=0.021..0.022 rows=1 loops=1)  
                 Output: "*SELECT* 1".r  
                 Buffers: shared hit=5  
                 ->  Limit  (cost=0.57..0.87 rows=1 width=64) (actual time=0.020..0.021 rows=1 loops=1)  
                       Output: (ARRAY[(tab1.vid)::numeric, tab1.score, (tab1.score + ((mod(tab1.vid, '9999'::bigint))::numeric / '10000000000000000000000000'::numeric))]), ((tab1.score + ((mod(tab1.vid, '9999'::bigint))::numeric / '10000000000000000000000000'::numeric)))  
                       Buffers: shared hit=5  
                       ->  Index Scan Backward using idx_tab1_2 on public.tab1  (cost=0.57..29864985.67 rows=100000896 width=64) (actual time=0.020..0.020 rows=1 loops=1)  
                             Output: ARRAY[(tab1.vid)::numeric, tab1.score, (tab1.score + ((mod(tab1.vid, '9999'::bigint))::numeric / '10000000000000000000000000'::numeric))], (tab1.score + ((mod(tab1.vid, '9999'::bigint))::numeric / '10000000000000000000000000'::numeric))  
                             Buffers: shared hit=5  
           ->  WorkTable Scan on tmp  (cost=0.00..8.98 rows=10 width=32) (actual time=0.011..0.011 rows=1 loops=23)  
                 Output: (SubPlan 1)  
                 Filter: (tmp.r[3] IS NOT NULL)  
                 Buffers: shared hit=115  
                 SubPlan 1  
                   ->  Limit  (cost=0.57..0.88 rows=1 width=64) (actual time=0.010..0.010 rows=1 loops=23)  
                         Output: (ARRAY[(t1.vid)::numeric, t1.score, (t1.score + ((mod(t1.vid, '9999'::bigint))::numeric / '10000000000000000000000000'::numeric))]), ((t1.score + ((mod(t1.vid, '9999'::bigint))::numeric / '10000000000000000000000000'::numeric)))  
                         Buffers: shared hit=115  
                         ->  Index Scan Backward using idx_tab1_2 on public.tab1 t1  (cost=0.57..10296397.54 rows=33166964 width=64) (actual time=0.010..0.010 rows=1 loops=23)  
                               Output: ARRAY[(t1.vid)::numeric, t1.score, (t1.score + ((mod(t1.vid, '9999'::bigint))::numeric / '10000000000000000000000000'::numeric))], (t1.score + ((mod(t1.vid, '9999'::bigint))::numeric / '10000000000000000000000000'::numeric))  
                               Index Cond: (((t1.score + ((mod(t1.vid, '9999'::bigint))::numeric / '10000000000000000000000000'::numeric))   Seq Scan on public.tab2  (cost=0.00..25.00 rows=6 width=32) (actual time=0.005..0.005 rows=1 loops=1)  
           Output: tab2.vids  
           Filter: (tab2.uid = 1)  
           Buffers: shared hit=1  
   ->  Nested Loop  (cost=0.00..20.82 rows=603 width=40) (actual time=0.076..0.331 rows=20 loops=1)  
         Output: (v.r[1])::bigint, v.r[2]  
         Join Filter: ((t.vids || hll_hash_bigint((v.r[1])::bigint, 0)) <> t.vids)  
         Rows Removed by Join Filter: 4  
         Buffers: shared hit=121  
         ->  CTE Scan on tmp2 t  (cost=0.00..0.12 rows=6 width=32) (actual time=0.007..0.007 rows=1 loops=1)  
               Output: t.vids  
               Buffers: shared hit=1  
         ->  CTE Scan on tmp v  (cost=0.00..2.02 rows=101 width=32) (actual time=0.022..0.300 rows=24 loops=1)  
               Output: v.r  
               Buffers: shared hit=120  
 Planning Time: 0.224 ms  
 Execution Time: 0.386 ms  
(45 rows)  
```  
优化成果:   
- | 优化前 | 优化后 | 性能提升  
---|---|---|---  
循环次数 | 2301 | 23 | 100倍  
扫描BLOCK数 | 1475 | 121 | 12倍  
耗时 | 3.541 ms | 0.386 ms | 9倍  
## 小结  
有技术追求的人, 用PG越用越喜欢. 还能给公司省大把的钱, 节能减排.   
而纵观现在的很多产品很多只能靠堆机器来提升性能, 而且还大肆宣传（看起来很高大上）, 实际浪费资源. 老板们被坑的不计其数.      
附送PG另一个很好用的功能, 性能随便提升上万倍:    
[《PostgreSQL 随机采样应用 - table sample, tsm_system_rows, tsm_system_time》](../202005/20200509_01.md)    
[《PostgreSQL 向量相似推荐设计 - pase》](../202004/20200424_01.md)   
#### [PostgreSQL 许愿链接](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
您的愿望将传达给PG kernel hacker、数据库厂商等, 帮助提高数据库产品质量和功能, 说不定下一个PG版本就有您提出的功能点. 针对非常好的提议，奖励限量版PG文化衫、纪念品、贴纸、PG热门书籍等，奖品丰富，快来许愿。[开不开森](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216").  
#### [9.9元购买3个月阿里云RDS PostgreSQL实例](https://www.aliyun.com/database/postgresqlactivity "57258f76c37864c6e6d23383d05714ea")
#### [PostgreSQL 解决方案集合](https://yq.aliyun.com/topic/118 "40cff096e9ed7122c512b35d8561d9c8")
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")