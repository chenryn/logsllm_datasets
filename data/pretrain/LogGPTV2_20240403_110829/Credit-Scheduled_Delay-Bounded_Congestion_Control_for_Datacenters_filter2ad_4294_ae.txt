of MapReduce [20]. It creates an all-to-all traffic pattern, generating
incast towards each host running a task. We simulate 40 hosts con-
nected to single top-of-rack (ToR) switch via 10 Gbps links using
ns-2. Each host runs 8 tasks, each of which sends 1 MB to all other
tasks. Thus, each host sends and receives 2496 (39 × 8 × 8) flows.
Figure 17 shows the CDF of flow completion times (FCTs) with
DCTCP and ExpressPass. The median FCT of DCTCP is slightly
better (2.0 vs. 2.2 s). However, DCTCP has a much longer tail. At
99th percentile and tail, ExpressPass outperforms DCTCP by a fac-
tor of 1.51 and 6.65 respectively. With DCTCP, when some faster
flows complete, the remaining flows often catch up. However, at
the tail end, delayed flows tend to be toward a small set of hosts,
such that they cannot simply catch up by using all available band-
width. This drastically increases the tail latency and contributes to
the straggler problem in MapReduce [7]. Our example demonstrates
02468100100200300400500Time (us)Throughput100 usGbps[Testbed] α=1/2 02468100510152025RTTsThroughput3 RTTsGbps[ns-2] α=1/2 0204060801000510152025RTTsThroughput3 RTTsGbps[ns-2] α=1/2 02468100510152025RTTsThroughput6 RTTsGbps[ns-2] α=1/16 0204060801000510152025RTTsThroughput6 RTTsGbps[ns-2] α=1/16 02468100100200300400500Time (ms)Throughput70 msGbps[Testbed]024681002505007501000RTTsThroughput260 RTTsGbps[ns-2]0204060801000200040006000RTTsThroughput2350 RTTsGbps[ns-2]02468100510152025RTTsThroughput3 RTTsGbps[ns-2]0204060801000510152025RTTsThroughput3 RTTsGbps[ns-2]00.20.40.60.81124816CDFTime (s)ExpressPassDCTCPxx2.23 s2.05 s**3.54 s2.34 sΔΔ15.78 s2.37 sMedian99%-ileMaxΔ*xCredit-Scheduled Delay-Bounded
Congestion Control for Datacenters
Data
Mining
Web
Search
0 - 10KB
(S)
10KB - 100KB (M)
(L)
100KB - 1MB
1MB -
(XL)
Average flow size
[28]
[3]
49%
3%
18%
20%
7.41MB 1.6MB
78%
5%
8%
9%
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Cache
Follower
[50]
50%
3%
18%
29%
Web
Server
[50]
63%
18%
19%
701KB 64KB
(a) 0 - 10KB (S)
(b) 10KB - 100KB (M)
Table 2: Flow size distribution of realistic workload
(a) 0 - 10KB (S)
(b) 100KB - 1MB (L)
Figure 18: 99%-ile FCT of ExpressPass with different α and
winit (10 Gbps, load 0.6)
congestion control can contribute to the problem, and ExpressPass
effectively alleviates this.
6.3 Performance under realistic workload
To evaluate the performance of ExpressPass in a more realistic
scenario, we run ns-2 simulations with four different workloads
shown in Table 2. It shows the flow size distribution and the average
flow size for each workload. We have chosen the workloads to cover
a wide range of average flow sizes ranging from 64 KB to 7.4 MB.
While the data mining workload has a smaller fraction of XL size
flows compared to web search, it has a larger cap of 1 GB compared
to 30 MB for web search, resulting in higher average flow sizes.
We generate 100 thousand flows with exponentially distributed inter-
arrival time. We simulate three target loads of 0.2, 0.4, and 0.6. There
is an over-subscription at the ToR uplinks and most of the traffic
traverses through the ToR up-links due to random peer selection.
Hence, we set the target load for ToR up-links.
We use a fat tree topology that consists of 8 core switches, 16
aggregator switches, 32 top-of-rack (ToR) switches, and 192 nodes.
The topology has an over-subscription ratio of 3:1 at ToR switch
layer. We create two networks, one with 10 Gbps links and the other
with 40 Gbps links. Maximum queue capacities are set to 384.5 KB
(250 MTUs) for the network with 10 Gbps link and 1.54 MB (1, 000
MTUs) for the network with 40 Gbps. All network link delays are
set to 4 µs and host delays to 1 µs, which results in maximum RTT of
52 µs between nodes excluding queuing delay. To support multipath
routing, Equal Cost Multi Path (ECMP) routing is used.
We measure the flow completion time (FCT) and queue occu-
pancy of ExpressPass and compare them with RCP, DCTCP, DX,
and HULL. We set the parameters as recommended in their corre-
sponding papers.
Parameter sensitivity: The initial value of credit sending rate (α ×
max_rate) and aggressiveness factor (winit ) determine the early be-
havior as described in Section 3.2. To decide appropriate values,
(c) 100KB - 1MB (L)
(d) 1MB - (XL)
Figure 19: Average / 99%-ile flow completion time for realistic
workload (10Gbps, load 0.6)
we run realistic workloads at target load of 0.6 with different α and
winit values. Figure 18 (a) and (b) shows the 99%-ile FCT values
for short (S) and large (L) flows respectively. As α and winit de-
crease, 99%-ile FCT of large flows decreases at the cost of increased
FCT for short flows. With α = winit = 1/16, large flow FCT de-
creases significantly, while short flow FCT increases less than 100%
compared to using α = winit = 1/2. Further reducing the values
provides an incremental gain in large flow FCT, but at a larger cost
in short flow FCT. α = winit = 1/16 provides a sweet spot, and we
use the setting in the rest of the experiments.
Flow Completion Time: We show the average and 99th percentile
FCTs across workloads for a target load of 0.6 in Figure 19. The
solid bar at the bottom indicates the average FCT and the upper
stripe bar shows the 99th percentile value. One clear pattern is that
ExpressPass performs better than others for short flows (S and M)
across workloads, and DCTCP and RCP perform better on large
flows (L and XL). ExpressPass achieves from 1.3x to 5.14x faster
average FCT compared to DCTCP for S and M flows, and the gap is
larger at 99th percentile. For L and XL size flows, its speed ranges
from 0.37x to 2.86x of DCTCP. This is expected given that two
dominant factors for short flow completion time are low queuing
and ramp up time which ExpressPass improves at the cost of lower
utilization. Between workloads, ExpressPass performs the worst for
Web Server workload relative to the others. This is due to the small
average flow size of 64 KB causing more credit waste.
Credit Waste: To understand how much credit is wasted, we mea-
sure the ratio of credit waste from the sender. Figure 20 shows the
result broken down by the workload and the link speed. As the
average flow size becomes smaller, the wasted amount of credit in-
creases up to 60% in 40 Gbps and 34% in 10 Gbps in the Web Server
workload. Higher link speed also increases the wasted credits. This
explains why ExpressPass performs worse than DCTCP for large
flows in the Web Server workload. In general, the amount of wasted
credit is proportional to the bandwidth delay product (BDP) and
inversely proportional to the average flow size. In the worst case,
the receiver may send an entire BDP worth of credits to the sender
00.511.521/21/21/161/21/161/161/321/161/321/3299%-ile FCTData MiningCache FollowerWeb Servermsα00.10.20.30.41/21/21/161/21/161/161/321/161/321/3299%-ile FCTsα(cid:2205)(cid:2191)(cid:2196)(cid:2191)(cid:2202)0.1110FCTWebServerCacheFollowerDataMiningms0.1110FCTWebServerCacheFollowerDataMiningms110100FCTWebServerCacheFollowerDataMiningmsAvg     99%-ileExpressPass     RCPDCTCPDX    HULL0.010.1110100FCTCacheFollowerDataMiningsSIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Inho Cho, Keon Jang, and Dongsu Han
Figure 20: Credit waste ratio with target load of 0.6
Figure 21: Average speed-up of 40G over 10G with websearch,
webserver workload (load 0.6).
and yet receive only one data packet. The figure also shows credit
waste with two different parameter settings to highlight this tradeoff.
Setting α to 1/16 reduces the amount of wasted credits significantly
to 31% and 19% for 40 Gbps and 10 Gbps link speed respectively.
Link speed scalability: Higher link speed enables flows to push
more bits per second but may require longer convergence time,
which diminishes the benefit of more bandwidth. To evaluate how
well ExpressPass performs, we measure the relative speed-up in FCT
when the link speed increases from 10 Gbps to 40 Gbps. Figure 21
shows the average FCT speed-up for the Web Server and Web Search
workloads. Data Mining and Cache Follower show similar results as
Web Search. For small flows, we observe less speed-up compared
to larger flows because RTT dominates the small flow FCT thus
increased bandwidth helps less. ExpressPass shows the largest gain
(1.5x - 3.5x) across all cases except large flows in Web Server work-
load. RCP has the largest gain in this case due to its aggressive ramp
up. It also maintains high utilization, whereas ExpressPass suffers
from increased credit waste when the BDP increases. DCTCP has
a 2.8x gain for the XL size flows and less than 2x gains for S, M,
and L size flows. DX and HULL benefit the least as they’re the
least aggressive scheme. Overall, this shows increasing benefit of
ExpressPass’s fast convergence and low queuing with the higher link
speed.
Queue length: Table 3 shows the average and maximum queue
occupancy observed during the simulation. On average, ExpressPass
uses less buffer than other congestion controls. ExpressPass’s max
queue is not proportional to the load whereas all other transports use
more queue with the increased load. ExpressPass’s queue bound is a
property of the topology, independent to the workload.
7 DISCUSSION AND LIMITATION
Path symmetry: To ensure path symmetry, we have used symmetric
routing. Symmetric routing can be achieved as evidenced by prior
Table 3: Average/maximum queue occupancy (ns-2) @ 10Gbps
work [27] and our simulation also uses symmetric routing on fat tree.
However, it incurs increased complexity to maintain consistent link
ordering with ECMP in the forward and reverse directions, especially
for handling link failures. Packet spraying [22] is a viable alternative
because it ensures all available paths get the equivalent load. We
believe the bounded queuing property of ExpressPass will also limit
the amount of packet reordering.
Presence of other traffic: In real datacenter networks, some traffic,
such as ARP packets and link layer control messages, may not be
able to send credits in advance. One solution to limit such traffic
and apply “reactive” control to account for it. When traffic is sent
without credit, we absorb them in the network queue and send credit
packets from the receiver, which will drain the queue.
Multiple traffic classes: Datacenter networks typically classify traf-
fic into multiple classes and apply prioritization to ensure the quality
of service. Existing schemes use multiple queues for data packets
and enforce priority or weighted fair-share across the queues. The
same logic can be applied to ExpressPass for credit packets instead
of data packets. For example, prioritizing flow A’s credits over flow
B’s credits while throttling the sum of credits from A and B will
result in the strict prioritization of A over B. Applying weighted
fair-share over multiple credit queues would have a similar effect.
Limitation of our feedback algorithm: The credit-based design
opens up a new design space for feedback control. We have explored
a single instance in this space, but our feedback algorithm leaves
much room for improvement.
Short flows cause credit packets to be wasted. This hurts the
flow completion times of long flows that compete with many short
flows. One way to overcome this is to use the approach of RC3 [42].
RC3 uses low priority data packet to quickly ramp up flows with-
out affecting other traffic. Similarly, in ExpressPass, one can allow
applications to send low priority data packets without credits. Such
low priority traffic would then be transmitted opportunistically to
compensate for the bandwidth lost due to wasted credits. However,
this approach comes at the cost of rather complex loss recovery logic
and requires careful design [42]. Credit waste can also be reduced if
the end of the flow can be reliably estimated in advance. Currently,
we assume senders do not know when the flow ends in advance.
However, it is possible for the sender to notify the end of the flow
in advance and send the credit stop request preemptively with some
margin. Some designs [1] even propose advertising send buffer to
the receiver. The sender can then leverage the information to control
the amount of credit waste.
4%8%11%34%3%8%9%19%4%11%17%60%3%10%14%31%0%20%40%60%80%DataMiningWebSearchCacheFollowerWebServerCredit Waste10Gbps,(cid:1861)(cid:1866)(cid:1861)(cid:1872)10Gbps,(cid:1861)(cid:1866)(cid:1861)(cid:1872)40Gbps,(cid:1861)(cid:1866)(cid:1861)(cid:1872)40Gbps,(cid:1861)(cid:1866)(cid:1861)(cid:1872)00.511.522.533.54ExpressPassRCPDCTCPDXHULLAvg. FCT Speed-upWeb Server (S)Web Server (M)Web Server (L)Web Search (S)Web Search (M)Web Search (L)Web Search (XL)TrafficTypeLoadExpressPassRCPDCTCPDXHULLExpressPassRCPDCTCPDXHULL0.20.142.340.460.560.6230.76375.386.6315.3815.460.40.322.780.990.620.6729.31375.3131.622.0026.150.60.415.462.790.770.8420.75375.3157.142.0977.630.20.073.190.880.560.6234.00375.3153.810.7729.220.40.2710.542.850.650.7141.78375.3178.323.4523.240.60.5014.586.180.770.8232.30375.3214.739.3079.280.20.083.271.840.570.6340.16375.3253.812.4038.450.40.2912.194.770.660.7149.30375.3296.820.1826.150.60.5418.018.650.790.8430.93375.3343.937.7674.050.20.062.312.580.610.5838.45375.3386.09.3249.220.40.124.785.920.600.6940.66375.3375.317.3949.220.60.475.789.330.640.6746.14375.3357.019.9953.83DataMiningWebSearchWebServerAverage Queue (KB)Max Queue (KB)CacheFollowerCredit-Scheduled Delay-Bounded
Congestion Control for Datacenters
Another limitation is that our feedback currently assumes all
hosts have the same link capacity. We leverage this assumption in
our feedback design for faster convergence. However, when host link
speeds are different, the algorithm does not achieve fairness. One
could use other algorithms, such as CUBIC [29], to regain fairness
by trading-off the convergence time under such a setting, without
compromising bounded queuing.
8 RELATED WORK.
Credit-based flow control: Our approach is inspired by credit based
flow control [36] used in on-chip networks and high-speed system
interconnect such as Infiniband, PCIe, Intel QuickPath, or AMD
Hypertransport [53]. However, traditional credit-based flow control
is hop-by-hop, which requires switch support, and is difficult to scale
to datacenter size. Decongestion control and pFabric pioneered a
design where hosts transmit data aggressively and switches allocate
bandwidth. The difference is that we allocate bandwidth using credit.
Finally, TVA [57] uses a similar idea to rate-limit requests at the
router, but it is designed for DDoS prevention considering the size
of response rather than congestion control.
Low latency datacenter congestion control: DCQCN [58] and
TIMELY [41] are designed for datacenters that have RDMA traffic.
DCQCN uses ECN as congestion signal and QCN-like rate control.
The main goals of DCQCN alleviate the problems caused by PFC by
reducing its use while reducing ramp-up time. TIMELY uses delay as
feedback, similar to DX [38], but incorporates PFC to achieve zero
loss and lower the 99th percentile latency. PERC [34] proposes a
proactive approach to overcome the problems of reactive congestion
control. We believe ExpressPass presents an alternative approach
and shows promise in the high-speed environment (e.g., 100 Gbps
networks).
Flow scheduling in datacenters: A large body of work focuses on
flow scheduling [6, 11, 26] in datacenter networks to minimize flow
completion times. Although the goals might overlap, flow scheduling