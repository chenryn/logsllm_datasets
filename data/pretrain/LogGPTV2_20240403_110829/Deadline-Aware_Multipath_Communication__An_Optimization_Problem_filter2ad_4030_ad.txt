### Distribution Estimation
If a specific distribution is assumed, such as a shifted gamma distribution, its parameters can be estimated through regression analysis [26]. Alternatively, the problem can be discretized by recording a sample of packet delays to determine average delays instead of expected values (Equation 25) and using discrete probability distributions instead of continuous ones (Equations 26–30).

### Loss Estimation
The loss rate of a path is estimated by dividing the number of lost packets by the total number of packets sent along that path. To achieve an accurate estimation, a large number of packets must be sent. Initially, the loss rate can be set to 0%, and the sending strategy can be refined each time a loss is recorded.

### Complexity
A variety of libraries in different programming languages are available for solving linear programs. The time complexity of the algorithm used is not critical for our model in most practical scenarios, as long as the metric estimations are stable. It is not necessary to solve the problem for every packet; it only needs to be solved when the estimations of network characteristics vary significantly. In the best case, the problem is solved once when all metrics are available and not again if the metrics remain stable. However, if metrics experience volatility or paths go up/down, it is important to ensure that solving the linear program does not impose a heavy computational burden.

Real-valued linear programs can be solved in polynomial time with respect to the number of variables using variants of the interior-point method. For Equation 10, the size of \( x \) grows exponentially with the number of retransmissions considered. Specifically, for a problem with \( nm \) variables (where \( n \) is the number of paths and \( m \) is the number of retransmissions) and encoded in \( L \) input bits, Karmarkar’s algorithm [29] requires \( O(n^{7/2}mL) \) operations.

Our experiments on a commodity machine (2.8 GHz Intel Core i5, 8 GB 1600 MHz DDR3) using the CGAL library [22] show that, on average (calculated over 100 runs), it takes about 458.39 microseconds to solve a problem with two paths (excluding the blackhole path) and two transmissions per data unit. This time is negligible since solving the problem does not block packet transmissions. Figure 4 shows computation times for larger problems (averaged over 100 runs).

### Acknowledgment Scheme
In our model, we assume that acknowledgments cannot get lost, always take the lowest-latency path, and RTT estimation is possible. An important observation is that all data should be acknowledged through the same path from which the data came for precise and accurate RTT estimation. However, this does not preclude the acknowledgment from containing information about other packets received on other paths.

To achieve this assumption in practice, acknowledgments sent in response to every (or every \( n \)) packet(s) should include:
- The range of packet numbers the receiver is expecting.
- A bit vector and its position indicating what was already received in a set of consecutive packets.
- The packet just received (for RTT estimation).

In links with low bandwidth-delay products, an acknowledgment packet may contain enough information to describe the entire set of in-flight packets between the sender and receiver. However, when the bandwidth-delay product is large and the lowest-latency path is lossy, the design of such acknowledgments becomes more complex. Specifically, the bit vector indicating which packets have been received and which have not may be shorter than the in-flight packets due to maximum packet size and the desire to reduce overhead. In such cases, the receiver's acknowledgment scheme becomes integral to achieving the desired quality metric. The goal is to create an acknowledgment stream that maximizes the quality for a given cost. Since we do not know which acknowledgment packets will be lost, we can only maximize the expected quality, and because our acknowledgment algorithm must be nearly online (and thus lacks knowledge of future acknowledgment transmission times), such quality can only be optimized with respect to a particular timing for future acknowledgments. Designing a high-performance, low-overhead acknowledgment scheme that performs well for both low and high bandwidth-delay products is left for future work.

### Retransmissions
In addition to using a retransmission timeout as described in Section VI-B, it is possible to implement a fast-retransmission mechanism (similar to TCP’s “fast retransmit” enhancement [30]), based on the fact that per-path packet reordering is relatively unlikely in the communication architecture we consider. This allows for correcting inappropriate timeout values caused by erroneous delay estimations when the generated traffic is sufficient. In TCP, the mechanism is triggered after three duplicate acknowledgments, but no formal motivation is provided for this specific number. Therefore, the question of how such a mechanism should work in our context remains open.

### Discussion

#### Path Characteristics Influenced by Usage
In some cases where a path has limited resources (such as bandwidth and queue length) relative to our usage, our use of a path may impact its performance characteristics. A mostly-saturated link, when encountering increased traffic, may exhibit a higher loss rate than initially measured. Queuing theory also shows that as utilization increases, latency increases. These effects introduce non-linearities in our model, as changes in \( x \) affect latency and loss rates, and thus quality \( p_T \) (Equation 11) and bandwidth usage \( A \) (Equation 14). In such environments, we initially assume that the characteristics of each path are independent of the transmission rate. If our path usage changes, we gather link characteristic information and determine whether a statistically significant change occurs. If so, we model the link’s latency and loss as a function of input bandwidth and replace Equation 10 with a non-linear program that accounts for the impact of transmission rates on quality and bandwidth limits.

When two paths share a common subpath, traffic sent on one path can influence the properties of traffic sent along the other path. In many network architectures, we can determine that two paths are linked in this way; for example, on the Internet, traceroute may reveal the extent to which two paths share a common subpath. Detecting such situations and modifying the non-linear program appropriately is beyond the scope of this paper and left for future work. Conveniently, our algorithm tends to send packets smoothly, meaning the inter-arrival time between two consecutive packets sent on the same path is similar as long as the sending rate is smooth. As a result, our approach need not consider the impact of traffic patterns (rather than traffic amounts) on queuing latency and loss.

#### Channel Coding
Our model does not include any form of channel coding and focuses instead on the optimal transmission/retransmission strategy. Correlated losses decrease the effectiveness of open-loop error control schemes (such as forward error correction), and experiments show that losses are correlated even when as little as 10% of capacity is used [31]. Although there might be an opportunity to de-correlate losses by sending consecutive packets along different paths, this approach has limitations. When a packet is lost, the delay required to recover the corresponding group of packets equals the longest delay of all paths. Therefore, the benefits of end-to-end coding (including in a multipath context) are questionable and need further investigation. Moreover, in terms of fairness to other users/applications, performing retransmissions with no additional redundancy (due to coding) is more desirable.

#### Interpretation of Bandwidth and Cost Limits
Our model operates on the expected value of the bandwidth to be used and the cost bound. The values in \( A \) (Equation 14) use the traffic vector to calculate the expected usage of each link and the expected cost. In some systems, exceeding a user-specified cost bound may be unacceptable, and in others, exceeding the pre-specified bandwidth limits may result in packet loss that our model cannot handle. In such cases, a system using our approach can adjust the values in \( q \) (Equation 17) until an acceptable solution is reached. Given a certain data rate, number of packets, and rate solution \( x \), we can compute the probability of exceeding an expected cost or a bandwidth limit. If this probability is too high, the system can adjust the bandwidth limit or cost limit and re-solve the linear program to obtain a solution closer to the system’s goals.

### Conclusion
Packet switching enables the simultaneous use of several network paths for a single stream of data or even a single message, making it an attractive research area for improving network performance. Unfortunately, the path diversity offered by the Internet is rarely fully exploited due to several reasons. First, the current Internet architecture does not allow end hosts to specify the paths they want to use. Second, the multipath paradigm poses new challenges and requires that most transport-layer concepts be redesigned. Finally, the advantages of multipath communication are not well known because they have not yet been sufficiently examined.

In this paper, we proposed an analytical model for optimizing the performance of partially-reliable multipath communications, particularly for developing better protocols for latency-sensitive applications. We showed that path diversity achieves better performance than uniform paths in deadline-based scenarios through theoretical and simulation results. Many challenges remain in designing a deployable protocol (e.g., cross-traffic, varying conditions, congestion/flow control), which we leave for future work. However, multipath communication promises to provide a multitude of desirable properties.

### Acknowledgments
The research leading to these results has received funding from the European Research Council under the European Union’s Seventh Framework Programme (FP7/2007-2013), ERC grant agreement 617605. This material is also based upon work partially supported by NSF under Contract No. CNS-0953600. The views and conclusions contained here are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either express or implied, of NSF, the University of Illinois, or the U.S. Government or any of its agencies.

### References
[1] C. Raiciu, C. Paasch, S. Barre, A. Ford, M. Honda, F. Duchene, O. Bonaventure, and M. Handley, “How hard can it be? designing and implementing a deployable multipath TCP,” in Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation (NSDI), 2012.
[2] A. Singla, B. Chandrasekaran, P. B. Godfrey, and B. Maggs, “The Internet at the speed of light,” in Proceedings of the 13th ACM Workshop on Hot Topics in Networks (HotNets), 2014.
[3] A. Hern, “Facebook launches Aquila solar-powered drone for Internet access,” August 2015. [Online]. Available: https://www.theguardian.com/technology/2015/jul/31/facebook-finishes-aquila-solar-powered-internet-drone-with-span-of-a-boeing-737
[4] “Google begins launching Internet-beaming balloons,” June 2013. [Online]. Available: http://news.temple.edu/in-the-media/google-begins-launching-internet-beaming-balloons
[5] T. Fernholz, “The details behind SpaceX’s ambitious satellite Internet experiment,” June 2015. [Online]. Available: https://qz.com/426158/the-details-behind-spacexs-ambitious-satellite-internet-experiment/
[6] X. Zhang, H.-C. Hsiao, G. Hasker, H. Chan, A. Perrig, and D. G. Andersen, “SCION: Scalability, control, and isolation on next-generation networks,” in Proceedings of the IEEE Symposium on Security and Privacy (S&P), May 2011.
[7] T. Anderson, K. Birman, R. Broberg, M. Caesar, D. Comer, C. Cotton, M. J. Freedman, A. Haeberlen, Z. G. Ives, A. Krishnamurthy, W. Lehr, B. Loo, D. Mazieres, A. Nicolosi, J. M. Smith, I. Stoica, R. Renesse, M. Walfish, H. Weatherspoon, and C. S. Yoo, “The NEBULA future Internet architecture,” in The Future Internet, 2013.
[8] C. Filsfils, N. K. Nainar, C. Pignataro, J. C. Cardona, and P. Francois, “The segment routing architecture,” in Proceedings of the IEEE Global Communications Conference (GLOBECOM), 2015.
[9] X. Yang, D. Clark, and A. W. Berger, “NIRA: A new inter-domain routing architecture,” IEEE/ACM Transactions on Networking, July 2007.
[10] P. B. Godfrey, I. Ganichev, S. Shenker, and I. Stoica, “Pathlet routing,” in Proceedings of the ACM SIGCOMM Conference, 2009.
[11] N. P. Lago and F. Kon, “The quest for low latency,” in Proceedings of the International Computer Music Conference (ICMC), 2004.
[12] “YouTube live streaming API overview.” [Online]. Available: https://developers.google.com/youtube/v3/live/getting-started
[13] B. Vamanan, J. Hasan, and T. N. Vijaykumar, “Deadline-aware datacenter TCP (D2TCP),” in Proceedings of the ACM SIGCOMM Conference, 2012.
[14] R. Stewart, M. Ramalho, Q. Xie, M. Tuexen, and P. Conrad, “Stream control transmission protocol (SCTP) partial reliability extension,” RFC 3758, May 2004.
[15] R. Stewart, M. Tuexen, and P. Lei, “SCTP: What is it, and how to use it?” in Proceedings of BSDCan: The Technical BSD Conference, 2008.
[16] X. Liu, S. Mohanraj, M. Pioro, and D. Medhi, “Multipath routing from a traffic engineering perspective: How beneficial is it?” in Proceedings of the 22nd IEEE International Conference on Network Protocols (ICNP), 2014.
[17] P. Soldati, H. Zhang, Z. Zou, and M. Johansson, “Optimal routing and scheduling of deadline-constrained traffic over lossy networks,” in Proceedings of the IEEE Global Communications Conference (GLOBECOM), 2010.
[18] J. Wu, C. Yuen, B. Cheng, Y. Shang, and J. Chen, “Goodput-aware load distribution for real-time traffic over multipath networks,” IEEE Transactions on Parallel and Distributed Systems, August 2015.
[19] C. Cetinkaya and E. W. Knightly, “Opportunistic traffic scheduling over multiple network paths,” in Proceedings of the IEEE International Conference on Computer Communications (INFOCOM), 2004.
[20] S. Prabhavat, H. Nishiyama, N. Ansari, and N. Kato, “Effective delay-controlled load distribution over multipath networks,” IEEE Transactions on Parallel and Distributed Systems, January 2011.
[21] T. R. Henderson, M. Lacage, G. F. Riley, C. Dowell, and J. Kopena, “Network simulations with the ns-3 simulator,” SIGCOMM demonstration, August 2008.
[22] “The computational geometry algorithms library (CGAL).” [Online]. Available: http://www.cgal.org/
[23] A. Mukherjee, “On the dynamics and significance of low load,” Internetworking: Research and Experience, December 1992.
[24] V. Paxson, “End-to-end Internet packet dynamics,” IEEE/ACM Transactions on Networking, June 1999.
[25] S. Kim, J. Y. Lee, and D. K. Sung, “A shifted gamma distribution model for long-range dependent Internet traffic,” IEEE Communications Letters, March 2003.
[26] D. Chen, X. Fu, W. Ding, H. Li, N. Xi, and Y. Wang, “Shifted gamma distribution and long-range prediction of round trip timedelay for Internet-based teleoperation,” in Proceedings of the IEEE International Conference on Robotics and Biomimetics (ROBIO), 2009.
[27] R. Prasad, C. Dovrolis, M. Murray, and K. Claffy, “Bandwidth estimation: metrics, measurement techniques, and tools,” IEEE Network, November 2003.
[28] M. Dong, Q. Li, D. Zarchy, P. B. Godfrey, and M. Schapira, “PCC: Re-architecting congestion control for consistent high performance,” in Proceedings of the 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI), 2015.
[29] G. Strang, “Karmarkar’s algorithm and its place in applied mathematics,” The Mathematical Intelligencer, June 1987.
[30] M. Allman, V. Paxson, and W. Stevens, “TCP congestion control,” RFC 2581, April 1999.
[31] J.-C. Bolot, “Characterizing end-to-end packet delay and loss in the Internet,” Journal of High Speed Networks, July 1993.