Hi,
I find that optimizer enforces every parameter to be `requires_grad=True`.
However, if I used such a network where the first layer was a pre-trained
`AlexNet` with every parameter set to `requires_grad = False`, and rest layers
were left `requires_grad = True`, pytorch would complain `optimizing a
parameter that doesn't require gradients`. Because in `model.parameters()`,
some parameters do not require gradients.
Is there anything wrong with my method of freezing part of the model? or would
it be better to enforce that not all parameters are set `requires_grad=False`
.