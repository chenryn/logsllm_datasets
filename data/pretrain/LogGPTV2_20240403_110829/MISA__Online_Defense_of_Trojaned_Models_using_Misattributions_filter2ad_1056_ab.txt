the paper.
∂F y(x b +α×(x−x b))
α =0
∂xi
3 MISA DETECTION
3.1 Formalizing Misattributions
We first formalize the concept of misattributions. Given a network
F, we can extract the attribution map att(x) for an input x and a
predicted label y as att(x) = IG(x, y). Let’s denote f as the features
for an input where the features can represent the raw pixels in the
input space or the output of an intermediate layer. Then, we refer
to the corresponding attribution map over f as attf .
572ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Panagiota Kiourti, Wenchao Li, Anirban Roy, Karan Sikka, and Susmit Jha
data in terms of its attributions over f . In the case of a Trojan
Misattribution: We define an in-distribution Pattf for attributions
over f on clean, labeled data. For an input(cid:101)x, its attribution over
f is deemed a misattribution if attf ((cid:101)x) is not from Pattf . In other
words,(cid:101)x is considered as out-of-distribution (OOD) from the clean
trigger, attf ((cid:101)x) is likely to be related to Trojaned predictions. In
Persistently OOD: For a Trojaned image(cid:101)x with predicted label y,
addition, for a Trojan trigger to be effective across different images,
the high-attribution features fh ⊂ f (which correspond to class-
specific discriminative features [4, 42]) should satisfy the following
property.
the network should have a high probability of predicting y even
if we replace the low-attribution features with values from Pattf ,
that is,
P(F(attf(cid:74)fl \ fh(cid:75)) = y) ≥ 1 − ϵ,
where fl = f \ fh are the low-attribution features , attf(cid:74)fl \ fh(cid:75)
the input, for an OOD(cid:101)x with label y, replacing the low-attribution
represents an attribution map over f that is consistent with the
attributions on fh but can vary on fl according to Pattf , and ϵ is
a small threshold. For instance, if f represents the raw pixels in
pixels with pixels in the same locations from another image in the
data set would still likely cause F to predict y.
Figure 3: Comparison of attribution maps in terms of impor-
tant features for an image with a small square trigger and
the corresponding clean image. The trigger region is associ-
ated with very high attributions in the Trojaned image.
3.2 Attribution-based Trojan Detection
Using the notion of misattributions described above, we develop a
method to detect whether the input provided to a neural network
during inference is Trojaned or not. To reiterate, our method does
not assume any prior knowledge of the attack. That is, we don’t
know the type of the Trojan trigger or the target label in advance
and the defender does not own sample Trojaned images or any
reference Trojaned networks. Our method only requires access to
the neural network and a set of validation clean inputs used for
evaluating the potentially Trojaned model.
Given a potentially Trojaned neural network(cid:101)F, we observe that a
Our method is based on detecting outliers in the attribution space.
Trojaned input’s attribution map is a misattribution. Hence, it is out
Algorithm 1 Extract Candidate Trigger
Input: f : inference-time image pixels or its features,
attf : attribution map of the image over pixels/features f .
Output: reverse-engineered trigger.
1: µ ← mean(attf ) // mean of attf
2: σ ← SD(attf ) // standard deviation of attf
3: mask ← zeros_like(attf )
4: mask[attf > µ + 2 · σ] = 1
5: triддer ← mask ◦ f // element-wise product
6: return triддer
Algorithm 2 Evaluate Trigger
label l when the candidate trigger t is injected to them.
Output: Percentage of images that are labeled with the target
Input: potential backdoored network(cid:101)F, set of clean images S,
candidate trigger t, candidate target label(cid:101)y.
2: K ← randomly pick 100 images from S (not of label(cid:101)y)
y ←(cid:101)F(im + t) // Inject the trigger
if y ==(cid:101)y then
1: f lipped ← 0
3: for im ∈ K do
4:
5:
6:
7: return f lipped
f lipped ← f lipped + 1
|K |
Algorithm 3 Detection
Input: neural network(cid:101)F with n activation layers, SVM Model M,
inference-time image x, threshold th, clean set of images S
Output: −1 or 1 indicating whether the current input included a
Trojan or not respectively. In the case of a Trojaned image, the
reverse-engineered trigger is returned.
2: (cid:101)y ←(cid:101)F(x) // Candidate target label
f ← дet_input_f eatures_o f _layer((cid:101)F , x, layer)
attf ← IG(x,(cid:101)y, layer) // Attribution map
f lipped ← evaluate_trigger((cid:101)F, S, t,(cid:101)y) // Alg. 2
1: layers ← [0, activation_layer_1, . . . , activation_layer_n]
3: for layer ∈ layers do
4:
5:
6:
7:
8:
9:
10:
11:
12: return 1, None
status ← M(attf ) // SVM output
if status == −1 then
t ← extract_candidate_trigger(f , attf ) // Alg. 1
if f lipped ≥ th then
return −1, t
of the attributions’ distribution for clean inputs (inputs without the
Trojan trigger injected to them). This can be verified by observing
the attributions of an input and the corresponding Trojaned attri-
bution map, as shown in Fig. 3. We use an outlier detection method
based on the input’s attributions to identify potentially Trojaned
inputs.
Clean image0200102030Att map of clean imageTrojaned image0200102030Att map of Trojaned image1002000.50.00.51002002.50.02.5573MISA: Online Defense of Trojaned Models using Misattributions
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
We use a one-class SVM to perform the anomaly detection, which
is part of our online detection approach. We assume that a set of
clean inputs is given to the defender for validation, which we use
to compute the clean attribution maps. SVMs are suitable when the
number of features is higher than the number of data instances. We
train the one-class SVM on input or intermediate-layer attribution
maps from these clean inputs. Therefore, the SVM learns to recog-
nize clean attribution maps as valid (1) and Trojaned attribution
maps as invalid (-1).
Choosing the right training parameters of the one-class SVM
is critical for the performance of our method. The ν parameter
represents an upper bound of the percentage of outliers we expect
to see in the set of clean inputs, used for training the SVM. In
addition, ν represents a lower bound on the percentage of samples
used as support vectors. Naturally, we would choose a small value
for ν. However, depending on the Trojan trigger, a clean image
might have attributions in the same area of features/pixels as the
one where misattributions appear. This is true especially when the
Trojaned trigger is present in spaces where we usually have high
attribution values for clean images. In the case of higher ν, the SVM
will detect Trojaned inputs but will have a high false-positive rate,
which we handle in the evaluation step of our method.
Parameter γ represents the margin, which is the minimal distance
between a point in the training set and the separating hyperplane
that separates the training set. Higher values of γ correspond to a
smaller margin. Therefore, we consider high values for γ (≫ 0.01)
because support vectors for the clean and Trojaned attribution
maps can be close to each other depending on the type of trigger.
Our end-to-end detection method includes 2 steps. First, we use
the one-class SVM to identify a potential Trojaned input as shown
in Fig. 2. Only in the case of identifying a potentially Trojaned
image, we proceed to evaluate whether the current input flagged by
the SVM as Trojaned is actually a Trojaned input or a False Positive.
First step (Stage 1, Stage 2): For a given neural network(cid:101)F and
using the current decision label(cid:101)y. Suppose that the one-class SVM
inference-time input x, we compute the attribution map of the input
flags the input’s attribution map as Trojaned/invalid (-1). In that
case, we proceed to the 2nd step.
Second Step (Stage 3): We extract the candidate trigger using the
significantly higher values of the image’s attribution map (Alg. 1).
We then evaluate if the current input is indeed a Trojaned input
and not a false positive by injecting this candidate trigger to 100
images. The 100 images are selected randomly and do not already
belong to the current label(cid:101)y (Alg. 2). We refer to these 2 steps
of extracting and injecting the candidate trigger as extract-and-
evaluate. We measure this trigger’s ability to flip the labels of these
images to the candidate target label. When the candidate trigger can
flip more than a percentage th of the labels from this set of inputs
(default th = 50%), we consider this trigger to be a Trojaned trigger
and the input to be Trojaned. Our exact algorithm is presented in
Alg. 3.
4 EXPERIMENTS
4.1 Experimental Setup
We implement MISA using DeepSHAP [33] against a black image as
the baseline for MNIST, Fashion MNIST and CIFAR10. For GTSRB,
we use the evaluation set of images as the baseline distribution. We
train a one-class SVM for each Trojaned model using a Gaussian
kernel with parameters ν and γ set to 0.7 and 0.2, respectively. We
discuss the choice of hyperparameters in Section 4.4. The defense is
performed on a machine with an Intel i7-6850K CPU and 4 Nvidia
GeForce GTX 1080 Ti GPUs. The default threshold for our method
in our experiments is 50%.
Benchmarks. We evaluate our method on multiple Trojaned mod-
els that we train on MNIST, Fashion MNIST, CIFAR10, and the
German Traffic Sign Recognition Benchmark (GTSRB) (Table 1).
The training hyperparameters are fixed per dataset, while Trojaned
Table 1: Details of our Trojaned and clean models. In this
table, we include spread-out, noise, Instagram filters and
smooth triggers in the static row.
Fashion MNIST
CIFAR10
GTSRB
MNIST
CIFAR10
GTSRB
MNIST
MNIST
Fashion MNIST
Fashion MNIST
CIFAR10
GTSRB
n
a
e
l
C
c
i
t
a
t
S
c
i
m
a
n
y
D
# models Accuracy ASR
N/A
N/A
N/A
N/A
98.8
97.7
98.0
98.5
99.4
95.9
99.6
98.0
99.1
91.3
78.1
94.3
99.1
91.3
79.7
93.2
99.0
90.1
79.9
93.3
1
1
1
1
108
84
52
132
9
9
3
12
models are trained to respond to different static or dynamic trig-
gers. For the evaluation, we keep the models that achieved Attack
Success Rate (ASR) ∼ 90% or higher, where ASR is the percentage of
Trojaned images classified as the target label. Details of the neural
network architectures can be found in the Appendix. We poison
1% of the images for MNIST, Fashion MNIST and CIFAR10 and 10%
of the images for GTSRB.
Triggers. We perform an extensive evaluation on a range of dif-
ferent trigger types. We refer to triggers that are always injected
in the same location of the input as static triggers. We consider
Instagram filters [23], smooth triggers [61], and noise triggers [10]
as static. When we sample the trigger and its location from a set of
triggers and locations, respectively, we call the trigger dynamic [40].
Therefore, we produce models that respond to one trigger (static)
and models that respond to multiple triggers (dynamic).
We evaluate our method on localized and non-localized triggers.
We refer to the following trigger types as localized:
(1) randomly shaped and/or randomly colored trigger [23],
(2) lambda trigger by [16],
(3) square triggers [16],
(4) dynamic triggers, represented by a set of randomly shaped
and randomly colored triggers and a set of 9 locations.
In addition, we refer to the following triggers as non-localized:
(1) noise triggers [10],
(2) spread-out triggers,
574ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Panagiota Kiourti, Wenchao Li, Anirban Roy, Karan Sikka, and Susmit Jha
Figure 4: A sample of the different triggers used during training. The smooth trigger is scaled for visualization purposes with
its original range of values being as displayed, i.e., [0, 0.2].
(3) instagram filters [23],
(4) smooth trigger [61], a trigger that exhibits low frequency
components in the frequency domain unlike traditional trig-
gers that exhibit high frequency components.