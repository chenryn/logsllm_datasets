BRNN, BLSTM, bidirectional GRU - a hidden layer of 128 units, with a dropout
rate of 0.2 for both inputs and recurrent states; Deep LSTM and BLSTM - Two
hidden layers of 128 units, with a dropout rate of 0.2 for both inputs and recur-
rent states in both layers; Linear SVM and logistic regression classiﬁers - A
regularization parameter C = 1.0 and L2 norm penalty; Random forest classiﬁer
- Using 10 decision trees with unlimited maximum depth and the Gini criteria
for choosing the best split; Gradient boosted decision tree - Up to 100 decision
trees with a maximum depth of 10 each.
We measured the performance of the classiﬁers using the accuracy ratio,
which applies equal weight to both FP and FN (unlike precision or recall),
thereby providing an unbiased overall performance indicator:
accuracy =
T P + T N
T P + F P + T N + F N
(3)
where: TP are true positives (malicious samples classiﬁed as malicious by the
black-box classiﬁer), TN are true negatives, FP stands for false positives (benign
samples classiﬁed as malicious), and FN are false negatives. The FP rate of the
classiﬁers varied between 0.5-1%.4
The performance of the classiﬁers is shown in Table 1. The accuracy was
measured on the test set, which contains 36,000 samples.
4 The FP rate was chosen to be on the high end of production systems. A lower FP
rate would mean lower recall either, due-to the trade-oﬀ between them, therefore
making our attack even more eﬀective.
End-to-End Attack Against API Call Based Malware Classiﬁers
501
As can be seen in Table 1, the LSTM variants are the best malware classiﬁers,
accuracy-wise, and, as shown in Table 2, BLSTM is also one of the classiﬁers most
resistant to the proposed attack.
4.3 Attack Performance
In order to measure the performance of an attack, we consider two factors:
The attack eﬀectiveness is the number of malware samples in the test set
which were detected by the target classiﬁer, for which the adversarial sequences
generated by Algorithm 2 were misclassiﬁed by the target malware classiﬁer.
attack eﬀectiveness =
|{f(x) = M alicious ∨ f(x∗) = Benign}|
|{f(x) = M alicious}|
(4)
s.t. x ∈ T estSet(f), ˆfT = Algorithm1(f, T, X1, ),
∗
x
= Algorithm2(f, ˆfT , x, n, D)
We also consider the overhead incurred as a result of the proposed attack. The
attack overhead is the average percentage of the number of API calls which were
added by Algorithm 2 to a malware sample successfully detected by the target
classiﬁer, in order to make the modiﬁed sample classiﬁed as benign (therefore
calculated only for successful attacks) by the black-box model:
)
l
attack overhead = avg( added AP Is
(5)
The average length of the API call sequence is: avg(l) ≈ 100, 000. The adver-
sary chooses the architecture for the surrogate model without any knowledge of
the target model’s architecture. We chose a GRU surrogate model with 64 units
(diﬀerent from the malware classiﬁers used in Sect. 4.2), which has a shorter
training time compared to other RNN variants, e.g., LSTM, which provides
similar attack eﬀectiveness. Besides the classiﬁer’s type and architecture, we
also used a diﬀerent optimizer for the surrogate model (ADADELTA instead of
Adam). In our implementation, we used the CleverHans library.
Based on Eqs. 4 and 5, the proposed attack’s performance is speciﬁed in
Table 2 (average of three runs).
We can see in Table 2 that the proposed attack has very high eﬀectiveness
and low attack overhead against all of the tested malware classiﬁers. The attack
eﬀectiveness is lower for traditional machine learning algorithms, such as SVM,
due to the greater diﬀerence between the decision boundaries of the GRU sur-
rogate model and the target classiﬁer. Randomly modifying APIs resulted in
signiﬁcantly lower eﬀectiveness for all classiﬁers (e.g., 50.29% for fully-connected
DNN).
As mentioned in Sect. 4.1, |T estSet(f)| = 36, 000 samples, and the test set
T estSet(f) is balanced, so the attack performance was measured on: |{f(x) =
502
I. Rosenberg et al.
Table 2. Attack Performance
Type
Classiﬁer
RNN
(%)
Attack
ﬀectiveness
E
100.0
(%)
Additional
Calls
PI
A
0.0023
BRNN
99.90
0.0017
LSTM
Deep LSTM
BLSTM
Deep
BLSTM
GRU
99.99
99.31
93.48
96.26
100.0
0.0017
0.0029
0.0029
0.0041
0.0016
Type
Classiﬁer
Bidirectional
GRU
Fully-
Connected
DNN
1D CNN
Random
Forest
SVM
Logistic
Regression
Gradient
Boosted Tree
(%)
Attack
ﬀectiveness
E
95.33
(%)
Additional
Calls
PI
A
0.0023
95.66
0.0049
100.0
99.44
70.90
69.73
71.45
0.0005
0.0009
0.0007
0.0007
0.0027
M alicious|x ∈ T estSet(f)}| = 18, 000 samples. For the surrogate model we used
a perturbation factor of  = 0.2 and a learning rate of 0.1. |X1| = 70 samples were
randomly selected from the test set of 36,000 samples. We used T = 6 surrogate
epochs. Thus, as shown in Sect. 3.2, the training set size for the surrogate model
is: |X6| = 25 ∗ 70 = 2240 samples; only 70 (= |X1|) of the samples were selected
from the test set distribution, and all of the others were synthetically generated.
Using lower values, e.g., |X1| = 50 or T = 5, achieved worse attack performance,
while larger values do not improve the attack performance and result in a longer
training time. The 70 samples from the test set don’t cover all of the malware
families in the training set; the eﬀectiveness of the surrogate model is due to the
synthetic data.
For simplicity and training time, we used m = n for Algorithm 2, i.e., the
sliding window size of the adversary is the same as that used by the black-box
classiﬁer. However, even if this is not the case, the attack eﬀectiveness isn’t
degraded signiﬁcantly. If n > m, the adversary would keep trying to modify
diﬀerent API calls’ positions in Algorithm 2, until he/she modiﬁes the ones
impacting the black-box classiﬁer as well, thereby increasing the attack overhead
without aﬀecting the attack eﬀectiveness. If n < m, the adversary can modify
only a subset of the API calls aﬀecting the black-box classiﬁcation, and this
subset might not be diverse enough to aﬀect the classiﬁcation as desired, thereby
reducing the attack eﬀectiveness. The closer n and m are, the better the attack
performance. For n = 100, m = 140, there is an average decrease of attack
eﬀectiveness from 99.99% to 99.98% for a LSTM classiﬁer.
End-to-End Attack Against API Call Based Malware Classiﬁers
503
Comparison to Previous Work. Besides [7] which was written concurrently
and independently from our work, [12] is the only recently published RNN adver-
sarial attack. The diﬀerences between that attack and the attack addressed in
this paper are mentioned in Sect. 2. We compared the attacks in terms of per-
formance. The attack eﬀectiveness for the IMDB dataset was the same (100%),
but our attack overhead was better: 11.25 added words per review (on average),
instead of 51.25 words using the method mentioned in [12].
4.4 Transferability for RNN Models
While transferability was covered in the past in the context of DNNs (e.g., [18]),
to the best of our knowledge, this is the ﬁrst time it is evaluated in the context
of RNNs, proving that the proposed attack is generic, not just eﬀective against a
speciﬁc RNN variant, but is also transferable between RNN variants (like LSTM,
GRU, etc.), feed forward DNNs (including CNNs), and even traditional machine
learning classiﬁers such as SVM and random forest.
Two kinds of transferability are relevant to this paper: (1) the adversary can
craft adversarial examples against a surrogate model with a diﬀerent architecture
and hyper parameters than the target model, and the same adversarial example
would work against both [11], and (2) an adversarial example crafted against
one target classiﬁer type might work against a diﬀerent type of target classiﬁer.
Both forms of transferability are evaluated as follows: (1) As mentioned in
Sect. 4.3, we used a GRU surrogate model. However, as can be seen in Table 2,
the attack eﬀectiveness is high, even when the black-box classiﬁer is not GRU.
Even when the black-box classiﬁer is GRU, the hyper parameters (such as the
number of units and the optimizer) are diﬀerent. (2) The attack was designed
against RNN variants; however, we tested it and found the attack to be eﬀective
against both feed forward networks and traditional machine learning classiﬁers,
as can be seen in the last six lines of Table 2. Our attack is therefore eﬀective
against all malware classiﬁers.
5 GADGET: End-to-End Attack Framework Description
To verify that an attacker can create an end-to-end attack using the proposed
method (Sect. 3), we implemented GADGET: Generative Api aDversarial
Generic Example by Transferability framework. This is an end-to-end attack
generation framework that gets a black-box classiﬁer (f in Sect. 3) as an input,
an initial surrogate model training set (X1 in Algorithm 1), and a malware binary
to evade f, and outputs a modiﬁed malware binary whose API call sequence is
misclassiﬁed by f as benign, generating the surrogate model ( ˆf in Algorithm 1)
in the process.
GADGET contains the following components: (1) Algorithms 1 and 2, imple-
mented in Python, using Keras with TensorFlow back end, (2) A C++ Wrapper
to wrap the malware binary and modify its generated API call sequence dur-
ing run time, and (3) A Python script that wraps the malware binary with the
504
I. Rosenberg et al.
Fig. 3. Malware binary, with and without GADGET
above mentioned wrapper, making it ready to deploy. The components appear
in Fig. 3.
Adding API Calls Without Damaging Functionality. As mentioned in
Sect. 3.2, we implemented Algorithm 2 using a mimicry attack [21]. We discarded
equivalence attacks and disguise attacks (Sect. 2), since they lack the ﬂexibility
needed to modify every API call, and thus are not robust enough to camouﬂage
every malware. Therefore, we implemented a no-op attack, adding APIs which
would have no eﬀect on the code’s functionality. Since some API call monitors
(such as Cuckoo Sandbox) also monitor the return value of an API call and might
ignore failed API calls, we decided to implement the API addition by adding no-
op API calls with valid parameters, e.g., reading 0 bytes from a valid ﬁle. This
was more challenging to implement than calling APIs with invalid arguments
(e.g., reading from an invalid ﬁle handle), since a diﬀerent implementation should
be used for each API. However, this eﬀort can be done once and can subsequently
be used for every malware, as we’ve done in our framework. This makes detecting
those no-op APIs much harder, since the API call runs correctly, with a return
value indicative of success. The functionality validation of the modiﬁed malware
is discussed in Sect. 5. Further measures, such as randomized arguments, can be
taken by the attacker to prevent the detection of the no-op APIs by analyzing
the arguments of the API calls. Attacking a classiﬁer with argument inputs is
discussed in Sect. 5.
Implementing a Generic Framework. The requirements for the generic
framework are: (1) there is no access to the malware source code (access only to
End-to-End Attack Against API Call Based Malware Classiﬁers
505
the malware binary executable), and (2) the same code should work for every
adversarial sample: no adversarial example-speciﬁc code should be written. The
reasons for these requirements are two-fold. First, adding the code as a wrapper,
without changing the malware’s business logic makes the framework more robust
to modiﬁcation of the malware classiﬁer model, preventing another session of
malware code modiﬁcation and testing. Second, with the Malware-as-a-Service
trend, not everyone who uses a malware has its code. Some ransomwares are
automatically generated using minimal conﬁguration (e.g., only the CNC server