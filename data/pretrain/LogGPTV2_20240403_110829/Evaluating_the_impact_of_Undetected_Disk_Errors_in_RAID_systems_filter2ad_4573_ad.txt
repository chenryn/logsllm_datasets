### Table 5: Estimated Mean and Standard Deviation of Rate of UDE Manifestation as Undetected Data Corruption per Second for Various Systems under the Abstract Workload, with and without Mitigation, for Various Rates of UDEs/IO

| Estimated Rate (UDEs/IO) | 10^-11 | 10^-12 | 10^-13 |
|--------------------------|--------|--------|--------|
| **Mitigation**           |        |        |        |
| **System**               |        |        |        |
| **Large Scale**          |        |        |        |
| No                       | 6.278 × 10^-7 | 2.415 × 10^-9 | 3.217 × 10^-7 |
| Yes                      | 1.259 × 10^-9 | 2.012 × 10^-8 | 7.930 × 10^-11 |
| **Enterprise**           |        |        |        |
| No                       | 3.813 × 10^-10 | 2.503 × 10^-11 | 1.595 × 10^-11 |
| Yes                      | 1.633 × 10^-12 | 6.282 × 10^-8 | 2.466 × 10^-10 |
| **Small**                |        |        |        |
| No                       | 3.218 × 10^-8 | 1.262 × 10^-10 | 2.012 × 10^-9 |
| Yes                      | 7.868 × 10^-12 | 7.430 × 10^-11 | 6.502 × 10^-12 |

### Table 6: Estimated Mean and Standard Deviation of Rate of UDE Manifestation as Undetected Data Corruption per Second for the Large-Scale System under All Workloads, with and without Mitigation, for Various Rates of UDEs/IO

| Estimated Rate (UDEs/IO) | 10^-11 | 10^-12 | 10^-13 |
|--------------------------|--------|--------|--------|
| **Mitigation**           |        |        |        |
| **System**               |        |        |        |
| **Abstract**             |        |        |        |
| No                       | 6.278 × 10^-7 | 2.415 × 10^-9 | 3.217 × 10^-7 |
| Yes                      | 1.259 × 10^-9 | 2.012 × 10^-8 | 7.930 × 10^-11 |
| **Read Heavy**           |        |        |        |
| No                       | 2.404 × 10^-7 | 9.345 × 10^-10 | 7.764 × 10^-7 |
| Yes                      | 3.048 × 10^-9 | 2.465 × 10^-10 | 2.526 × 10^-11 |
| **Write Heavy**          |        |        |        |
| No                       | 1.061 × 10^-9 | 4.592 × 10^-11 | 7.763 × 10^-8 |
| Yes                      | 3.014 × 10^-10 | 4.692 × 10^-11 | 2.195 × 10^-12 |

### Analysis

Tables 5 and 6 illustrate the relationship between the rate of undetected data corruption errors and varying system and workload parameters, as well as rates of UDEs/IO. Table 5 shows that the rate of UDEs manifesting as undetected data corruption decreases both as the rate of UDEs/IO decreases and as the size of the system decreases. Scenarios involving mitigation decreased the rate of undetected data corruption events by two to three orders of magnitude.

To put these rates into perspective, Table 7 shows the mean interval between undetected data corruption events corresponding to these rates. Table 6 summarizes the effect of varying the workload while holding the simulated system constant. The results for the large-scale system are shown, as the trends for enterprise and small business systems were similar. The rates of manifestation vary in a manner consistent with Table 4(b), indicating that workload does affect the rate of UDE manifestation, but for the workloads and systems tested, varying the workload still yields a rate of undetected data corruption events within the same order of magnitude.

### Conclusions

Our results indicate that UDEs will continue to grow in importance for system designers as storage systems scale, a conclusion supported by field observations of UDEs in modern large-scale storage systems. Even in the case of a small business system, the rate of data corruption events without implementing mitigation is such that one would expect to see UDEs occurring in a population of such storage systems. If we assume a UDE/IO rate of 10^-11, the rate is high enough to push the mean interval between undetected data corruption below the average lifetime of even a single system.

Our results suggest that weekly data scrubbing is not sufficient to reduce UDEs, as the data scrub must process the location on a disk where the UDE has occurred after the fault and before the next read request. When the rate is set at 10^-12 UDEs/IO, the estimated rate for near-line disks, both enterprise and large-scale systems have rates of undetected data corruption that place the mean interval between such events at less than a year. Given the growing practice of using cheaper near-line drives in RAID configurations, this highlights an important limitation of RAID. Since RAID5 and RAID6 can only detect errors on data drives during a parity scrub, the protection they implement is largely orthogonal to the issues posed by UDEs. Fortunately, our simulations suggest that using simple techniques like the sequence number approach to mitigation modeled in this work can largely eliminate UDEs for the expected lifetime of near-future systems. While these methods double the requirement for read operations, they reduce the rate of undetected data corruption events by two to three orders of magnitude.

### Future Work

While this paper considers only a single mitigation technique and relatively simple RAID-based systems, there is no reason why other mitigation techniques or more complex systems cannot be modeled using our methods. Our simulator was designed to allow easy construction of additional models of both disks and mitigation techniques. It would be useful to conduct similar studies under a broader variety of mitigation techniques and a wider array of more complex systems with different architectures than just standard RAID configurations.

Bairavasundaram et al. [2] indicate that UDEs may be correlated in both time and space. Constructing a model of this locality and applying our techniques to scenarios that take it into account could provide insights into how it affects MTTDC. Alterations to the simulator would simply involve altering the probabilistic distributions governing UDE placement and the rate of UDEs in the I/O stream. Additionally, while the simulator design was tailored to UDE analysis, many of the assumptions made likely hold for other fault tolerance analyses. Work needs to be done to generalize the provided techniques and adapt them for use in other domains.

### References

[1] L. N. Bairavasundaram, G. R. Goodson, S. Pasupathy, and J. Schindler. An analysis of latent sector errors in disk drives. SIGMETRICS Perform. Eval. Rev., 35(1):289–300, 2007.

[2] L. N. Bairavasundaram, G. R. Goodson, B. Schroeder, A. C. Arpaci-Dusseau, and R. H. Arpaci-Dussea. An analysis of data corruption in the storage stack. In FAST’08: Proceedings of the 6th USENIX Conference on File and Storage Technologies, pages 1–16, Berkeley, CA, USA, 2008. USENIX Association.

[3] J. Banks, J. Carson, B. L. Nelson, and D. Nicol. Discrete-Event System Simulation. Prentice-Hall International Series in Industrial and Systems, 2004.

[4] B. E. Clark, F. D. Lawlor, W. E. Schmidt-Stumpf, T. J. Stewart, and G. D. T. Jr. Parity spreading to enhance storage access. US Patent No. 4761785, 1988.

[5] M. H. Darden. Data integrity: The Dell EMC distinction. http://www.dell.com/content/topics/global.aspx/power/en/ps2q02darden?c=us&cs=555&l=en&s=biz, 2002.

[6] E. M. Evans. Working draft project, t10/1799-d, "SCSI Block Commands-3 (SBC-3)," May 13, 2008. http://www.t10.org/ftp/t10/drafts/sbc3/sbc3r15.pdf.

[7] J. L. Hafner, V. Deenadhayalan, W. Belluomini, and K. Rao. Undetected disk errors in RAID arrays. IBM Journal of Research and Development, 52(4):413–425, 2008.

[8] I. Iliadis, R. Haas, X.-Y. Hu, and E. Eleftheriou. Disk scrubbing versus intra-disk redundancy for high-reliability RAID storage systems. In Proceedings of the 2008 ACM SIGMETRICS international conference on Measurement and modeling of computer systems, pages 241–252, Annapolis, MD, USA, 2008. ACM.

[9] J. Keilson. Markov Chain Models–Rarity and Exponentiality. Springer-Verlag, 1979.

[10] P. Kelemen. Silent corruptions, June 1, 2007. http://fuji.web.cern.ch/fuji/talk/2007/kelemen-2007-C5-SilentCorruptions.pdf.

[11] A. Krioukov, L. N. Bairavasundaram, G. R. Goodson, K. Srivasan, R. Thelen, A. C. Arpaci-Dusseau, and R. H. Arpaci-Dussea. Parity lost and parity regained. In FAST’08: Proceedings of the 6th USENIX Conference on File and Storage Technologies, pages 1–15, Berkeley, CA, USA, 2008. USENIX Association.

[12] A. M. Law and W. D. Kelton. Simulation Modeling and Analysis. Wiley, New York, 2000.

[13] D. A. Patterson, G. A. Gibson, and R. H. Katz. A case for redundant arrays of inexpensive disks (RAID). Technical Report UCB/CSD-87-391, EECS Department, University of California, Berkeley, Dec 1987.

[14] V. Prabhakaran, L. N. Bairavasundaram, N. Agrawal, H. S. Gunawi, A. C. Arpaci-Dusseau, and R. H. Arpaci-Dusseau. Iron file systems. ACM SIGOPS Operating Systems Review, 39(5):206–220, 2005.

[15] B. Schroeder and G. A. Gibson. Disk failures in the real world: What does an MTTF of 1,000,000 hours mean to you? In FAST ’07: Proceedings of the 5th USENIX conference on File and Storage Technologies, page 1, Berkeley, CA, USA, 2007. USENIX Association.