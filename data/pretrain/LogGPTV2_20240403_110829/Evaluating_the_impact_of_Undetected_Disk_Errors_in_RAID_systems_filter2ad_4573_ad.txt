0.0035
0.718
0.0028
Table 5. Estimated mean and standard deviation of rate of UDE manifestation as undetected data corruption per second for various
systems under the Abstract workload, with and without mitigation, for various rates of UDEs/io.
Estimated rate for various rates of UDEs/IO
10−11
10−12
10−13
Mitigation
System
Large scale No
Large scale Yes
No
Enterprise
Yes
Enterprise
Small
No
Yes
Small
µ
6.278 · 10−7
2.415 · 10−9
3.217 · 10−7
1.259 · 10−9
2.012 · 10−8
7.930 · 10−11
σ
6.281 · 10−10
6.312 · 10−11
3.813 · 10−10
2.503 · 10−11
1.595 · 10−11
1.633 · 10−12
µ
6.282 · 10−8
2.466 · 10−10
3.218 · 10−8
1.262 · 10−10
2.012 · 10−9
7.868 · 10−12
σ
7.430 · 10−11
6.502 · 10−12
3.813 · 10−11
4.042 · 10−12
1.110 · 10−12
1.602 · 10−13
µ
6.282 · 10−9
2.519 · 10−11
3.221 · 10−9
1.253 · 10−11
2.012 · 10−10
7.857 · 10−13
σ
6.324 · 10−12
5.705 · 10−13
2.195 · 10−12
2.213 · 10−13
1.589 · 10−13
2.201 · 10−14
Table 6. Estimated mean and standard deviation of rate of UDE manifestation as undetected data corruption per second for the
large scale system under the all workloads, with and without mitigation, for various rates of UDEs/io.
Estimated rate for various rates of UDEs/IO
10−11
10−12
10−13
Mitigation
System
No
Abstract
Yes
Abstract
No
Read Heavy
Yes
Read Heavy
Write Heavy No
Write Heavy Yes
µ
6.278 · 10−7
2.415 · 10−9
2.404 · 10−7
9.345 · 10−10
7.764 · 10−7
3.048 · 10−9
σ
6.281 · 10−10
6.312 · 10−11
2.465 · 10−10
2.526 · 10−11
1.061 · 10−9
4.592 · 10−11
µ
6.282 · 10−8
2.466 · 10−10
2.405 · 10−8
9.310 · 10−11
7.763 · 10−8
3.014 · 10−10
σ
7.430 · 10−11
6.502 · 10−12
4.692 · 10−11
2.195 · 10−12
9.004 · 10−11
3.902 · 10−12
µ
6.282 · 10−9
2.519 · 10−11
2.401 · 10−9
9.476 · 10−12
7.766 · 10−9
3.038 · 10−11
σ
6.324 · 10−12
5.705 · 10−13
3.436 · 10−12
3.231 · 10−13
6.756 · 10−12
7.858 · 10−13
tude.
Tables 5 and 6 illustrate the relationship between the rate
of undetected data corruption errors and varying system and
workload parameters, and rates of UDEs/IO. Table 5 shows
that the rate of UDEs manifesting as undetected data corrup-
tion declines both as the rate of UDEs/IO decreases, and as
the size of the system decreases. Those scenarios which in-
volved mitigation decreased the rate of undetected data cor-
ruption events by between two and three orders of magnitude.
To put these rates into perspective, Table 7 shows the mean
interval between undetected data corruption events that corre-
spond to these rates. Table 6 summarizes the effect of varying
the workload while holding the simulated system constant. We
show the results for the large system only, as the results for
enterprise and small business systems showed similar trends
in the rates of UDE manifestation. The rates of manifesta-
tion vary in a manner consistent with Table 4(b) illustrating
that workload does have an effect on the rate of UDE man-
ifestation, but for the workloads and systems tested, varying
the workload still yields a rate of undetected data corruption
events within the same order of magnitude.
5. Conclusions
Our results indicate that UDEs will continue to grow in their
importance to system designers as storage systems continue to
scale, a conclusion which seems afﬁrmed out by ﬁeld obser-
vation of UDEs in modern large scale storage systems. Even
in the case of the small business system simulated, the rate
of data corruption events without implementing mitigation is
such that one would expect to see UDEs occuring in a pop-
Table 7. Estimated mean interval between undetected data
corruption events for all systems under the Abstract work-
load, with and without mitigation, derived from Table 5.
System Mitigation
Large
Large
Ent.
Ent.
Small
Small
No
Yes
No
Yes
No
Yes
10−11
18.43 days
13.13 years
35.98 days
25.19 years
1.576 years
399.9 years
Mean Interval
10−12
184.2 days
128.6 years
0.9854 years
251.3 years
15.76 years
4030 years
10−13
5.04 years
1258 years
9.844 years
2531 years
157.6 years
40358 years
ulation of such storage systems, and if we assume a UDE/IO
rate of 10−11, the rate is high enough to push the mean interval
between undetected data corruption below the average lifetime
of even a single such system.
Our results suggest that data scrubbing on a weekly sched-
ule is not enough to reduce UDEs sufﬁciently, as the data scrub
must be processing the location on a disk where the UDE has
occurred after the time at which the disk suffered the fault, and
prior to the next read request for that location. It is particu-
larly telling that when the rate is set at 10−12 UDEs/IO, the
rate of UDEs/IO estimated for near-line disks, both enterprise
and large scale systems have rates of undetected data corrup-
tion which place the mean interval between such events at less
than a year. Given the growing practice of utilizing cheaper
near-line drives in a RAID conﬁguration, this highlights an
important limitation of RAID. Since RAID5 and RAID6 can
only detect errors on the data drives during a parity scrub, the
protection they implement is largely orthogonal to the issues
posed by UDEs. Fortunately, our simulations suggest that by
using relatively simple techniques such as the sequence num-
ber approach to mitigation modeled in this work and described
in [7], UDEs can largely be eliminated for the expected life-
time of near-future systems. While they double the require-
ment for read operations, requiring reading both the data and
the parity, these methods reduce the rate of undetected data
corruption events by two to three orders of magnitude.
6. Future Work
While this paper considers only a single mitigation tech-
nique, and a set of relatively simple RAID based systems, there
is no reason why one could not model other mitigation tech-
niques with our methods, or model more complicated systems.
In fact, our simulator was designed in a general fashion to al-
low one to easily build additional models of both disks and
mitigation techniques. It would be useful to conduct similar
studies under a broader variety of mitigation techniques, and a
wider array of more complex systems with different architec-
tures than just standard RAID conﬁgurations.
Bairavasundaram et al.
indicate in [2] that UDEs may be
correlated in both time and space, it would be interesting to
construct a good model of this locality and apply our tech-
niques to scenarios which take the locality into account to see
how it affects MTTDC. Alterations to the simulator in this
fashion would simply be a matter of altering the probabilis-
tic distributions which govern UDE placement in the storage
systems, and the rate of UDEs occuring in the I/O stream. Ad-
ditionally, while the simulator design we used was tailored to
the problem of UDE analysis, many of the assumptions made
likely hold for other fault tolerance analysis. Work needs to be
done towards generalizing the provided techniques, and adapt-
ing them for use in other domains.
References
[1] L. N. Bairavasundaram, G. R. Goodson, S. Pasupathy, and
J. Schindler. An analysis of latent sector errors in disk drives.
SIGMETRICS Perform. Eval. Rev., 35(1):289–300, 2007.
[2] L. N. Bairavasundaram, G. R. Goodson, B. Schroeder, A. C.
Arpaci-Dusseau, and R. H. Arpaci-Dussea. An analysis of data
corruption in the storage stack.
In FAST’08: Proceedings of
the 6th USENIX Conference on File and Storage Technologies,
pages 1–16, Berkeley, CA, USA, 2008. USENIX Association.
[3] J. Banks, J. Carson, B. L. Nelson, and D. Nicol. Discrete-Event
System Simulation. Prentice-Hall International Series in Indus-
trial and Systems, 2004.
[4] B. E. Clark, F. D. Lawlor, W. E. Schmidt-Stumpf, T. J. Stewart,
and G. D. T. Jr. Parity spreading to enhance storage access. US
Patent No. 4761785, 1988.
[5] M. H. Darden.
tion, may
global.aspx/power/en/ps2q02darden?c=us&cs=555&l=en&s=biz.
Data integrity: The dell emc distinc-
http://www.dell.com/content/topics/
2002.
[6] E. M. Evans.
Working draft project,
t10/1799-d,
standard
tional
”information
– scsi block commands-3 (sbc-3)”, may,
http://www.t10.org/ftp/t10/drafts/sbc3/sbc3r15.pdf.
american na-
technology
13,
2008.
[7] J. L. Hafner, V. Deenadhayalan, W. Belluomini, and K. Rao.
Undeteced disk errors in raid arrays. IBM Journal of Research
and Development, 52(4):413–425, 2008.
[8] I. Iliadis, R. Haas, X.-Y. Hu, and E. Eleftheriou. Disk scrub-
bing versus intra-disk redundancy for high-reliability raid stor-
age systems. In Proceedings of the 2008 ACM SIGMETRICS
international conference on Measurement and modeling of com-
puter systems, pages 241–252, Annapolis, MD, USA, 2008.
ACM.
[9] J. Keilson. Markov Chain Models–Rarity and Exponentiality.
Springer-Verlag, 1979.
2007.
[10] P. Kelemen.
1,
http://fuji.web.cern.ch/fuji/talk/2007/kelemen-2007-C5-
SilentCoruptions.pdf.
corruptions,
Silent
june
[11] A. Krioukov, L. N. Bairavasundaram, G. R. Goodson, K. Srini-
vasan, R. Thelen, A. C. Arpaci-Dusseau, and R. H. Arpaci-
Dussea. Parity lost and parity regained.
In FAST’08: Pro-
ceedings of the 6th USENIX Conference on File and Storage
Technologies, pages 1–15, Berkeley, CA, USA, 2008. USENIX
Association.
[12] A. M. Law and W. D. Kelton. Simulation Modeling and Analy-
sis. Wiley, New York, 2000.
[13] D. A. Patterson, G. A. Gibson, and R. H. Katz. A case for re-
dundant arrays of inexpensive disks (raid). Technical Report
UCB/CSD-87-391, EECS Department, University of Califor-
nia, Berkeley, Dec 1987.
[14] V. Prabhakaran, L. N. Bairavasundaram, N. Agrawal, H. S.
Gunawi, A. C. Arpaci-Dusseau, and R. H. Arpaci-Dusseau.
Iron ﬁle systems. ACM SIGOPS Operating Systems Review,
39(5):206–220, 2005.
[15] B. Schroeder and G. A. Gibson. Disk failures in the real
world: what does an mttf of 1,000,000 hours mean to you? In
FAST ’07: Proceedings of the 5th USENIX conference on File
and Storage Technologies, page 1, Berkeley, CA, USA, 2007.
USENIX Association.