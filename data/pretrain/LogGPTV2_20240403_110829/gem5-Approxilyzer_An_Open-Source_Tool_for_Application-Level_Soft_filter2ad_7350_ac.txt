the bit ﬂip is performed before the instruction execution. For
destination registers, the error is simulated by performing the
bit ﬂip after the instruction execution (otherwise the error would
be overwritten by the instruction execution). The simulation
then proceeds, checking for any hangs and crashes, or other
symptoms to identify Detected outcomes. If no Detected
symptoms are encountered before the simulation ends, gem5-
Approxilyzer compares the generated output with the error-free
execution’s output. If there is an OC, gem5-Approxilyzer uses
the user-provided quality metric to evaluate the output quality.
(4) Phase 4 analyzes the outcome of each error injection
and assigns it
i.e., error
outcome category and quality degradation (QD) score for OCs.
gem5-Approxilyzer then assigns the same error outcomes to
pruned error sites associated with the pilot, and ﬁnally outputs
the application’s comprehensive error proﬁle containing all
the error sites and their corresponding error outcomes.
the appropriate error outcome,
For an end-to-end error analysis with gem5-Approxilyzer,
the error injections in Phase 3 consume the most time –
several days worth of CPU time versus only few minutes/hours
consumed by all the other phases combined for the experiments
reported here. Thus, using effective pruning techniques that
can reduce the total number of error injections in Phase 3 is
the most direct means of reducing the tool’s analysis time.
C. x86 Implementation Challenges
While phases 3 and 4 are largely ISA independent, phases
1 and 2 require customization to support different ISAs.
Since x86 is a CISC ISA, opcode lengths vary, and hence
the instruction parser in Phase 1 must capture instruction
semantics correctly to identify source and destination operands
of different instructions. Depending on the complexity of the
macro-instructions, a varying number of micro-instructions
can be generated. Any memory accesses performed by these
micro-instructions in the gem5 memory trace must be mapped
to the correct macro-instruction. Since x86 allows for variable
register sizes, another challenge in Phase 2 is to correctly
associate registers of varying sizes with their aliased 64-bit
registers. This must be done carefully to identify aliased def-use
pairs which enables pruning the right set of error sites within
an aliased register. For example, %ax and %eax both alias
to %rax. While performing def-use pruning, only the lower 16
bits of %eax deﬁnition must be pruned if the ﬁrst use is %ax.
D. Extensions to gem5-Approxilyzer
We designed gem5-Approxilyzer to be reasonably modular
(e.g., each phase in Section III-B is a separate module) to
enable future extensions to support different ISAs, error
models, and pruning techniques. This section brieﬂy elaborates
on some details for future extensions.
The gem5 simulator currently supports many ISAs, and
gem5-Approxilyzer could support them with the following
modiﬁcations. 1) The instruction parser in Phase 1 must
be modiﬁed to capture the semantics of the new ISA. 2)
ISA-speciﬁc behaviors that affect control ﬂow (e.g., branch
delay slots for SPARC) should be incorporated into the
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:56:27 UTC from IEEE Xplore.  Restrictions apply. 
control-equivalence algorithm accordingly. 3) Register aliasing
must be captured correctly to track def-use pairs.
The error-injector module in Phase 3 can be modiﬁed to
support other error models such as multi-bit injections or injec-
tions to other system structures like DRAM. The error-pruning
module in Phase 2 would need to be extended to support
pruning algorithms appropriate for the chosen error model.
gem5-Approxilyzer performs Phase 2 analysis on the
dynamic trace generated by gem5 in Phase 1. For very long
executions, this may result in excessively long traces, requiring
a tighter coupling of phases 1 and 2 to trace and analyze parts
of the execution at a time.
IV. EXPERIMENTAL METHODOLOGY
A. Benchmarks and Quality Metrics
To evaluate gem5-Approxilyzer, we select ﬁve benchmarks
from three different benchmark suites spanning multiple
application domains. Table I shows the applications and inputs
used in our evaluation. We chose these inputs because recent
work has shown that performing error analysis on these inputs
is much faster and at least as accurate as for larger reference
inputs [36]. We use gem5 to simulate an Ubuntu-16.04 system,
and we use GCC 7.3 with -O3 to compile the applications.
To evaluate the quality degradation of the application’s
ﬁnal output we use the following quality metrics: (1) Absolute
Maximum Difference [25] (in the dollar value outputs) for
the ﬁnancial applications Blackscholes and Swaptions, (2)
Maximum Relative Error [25] for LU and Blackscholes, (3)
Relative L2 Norm [25] for FFT, and (4) Root Mean Square
Error [3] for Sobel. We use the same quality thresholds to
determine SDC-Good, SDC-Bad, and DDC as prior work [25].
BENCHMARKS, INPUTS, AND ERROR-SITE PRUNING BY TECHNIQUE (C:
CONTROL-EQUIVALENCE, S: STORE-EQUIVALENCE, C+S+K: TOTAL
PRUNING USING CONTROL, STORE, AND KNOWN-OUTCOME TECHNIQUES)
TABLE I
Application
Input
Black-
scholes [37]
Swaptions
[37]
LU [38]
21 options
1 option
1 simulation
16x16 matrix
8x8 blocks
Error Sites
Total
232K
Remaining
100K
10.3M
720K
1.2M
268K
FFT [38]
28 data points
4.4M
215K
Sobel [3]
81x121 pixels
85.3M
300K
Pruned Error
Sites (%)
C: 12.24
S: 9.45
C+S+K: 56.77
C: 52.47
S: 7.85
C+S+K: 93.01
C: 23.49
S: 22.72
C+S+K: 77.91
C: 43.99
S: 21.50
C+S+K: 95.05
C: 62.74
S: 20.94
C+S+K: 99.65
B. Pruning Effectiveness
We measure the effectiveness of gem5-Approxilyzer by
observing how many error sites were pruned using the various
pruning techniques described in Section II-D. For each
application, we measure ﬁrst the number of error sites in the
application’s region of interest and then the number of error
sites remaining after the pruning to calculate the number of
218
error sites that have been pruned. This metric evaluates the
tool’s effectiveness since the number of error sites pruned
directly reduces the number of error-injection experiments
needed to analyze the application. For the control heuristic,
we set depth to N=50, as in prior work [39].
C. gem5-Approxilyzer Validation
The control- and store-equivalence based pruning
techniques use heuristics and require validation. For a given
equivalence class, these pruning techniques choose a single
pilot to represent the error outcomes of all error sites in the
equivalence class (Section II-D). Similar to the methodology
used in prior work [25], [26], we quantify the validity of
these techniques by measuring the extent to which the pilots
correctly represent their equivalence classes.
The validation attempts to answer the following question:
how accurately does the error outcome of the pilot predict the
error outcome of the other error sites in its equivalence class?
For validating a single equivalence class, we perform error
injections in a sample of error sites (not including the pilot)
– called population – chosen randomly from the equivalence
class. We refer to individual error sites within a population
as population member(s). We calculate the prediction accuracy
of an equivalence class’s pilot, by measuring the number of
its population members that produce the same equalized error
outcome as the pilot.
Because the equalization of error outcome categories
(Section II-C) depends on whether the context of the analysis
is resiliency (Res) or approximation (Approx), we show the
validation outcomes for both these contexts separately. For
example, consider an equivalence class whose pilot X generates
a DDC. Suppose 86% of its population is DDC, 6% is Masked,
5% is SDC-Maybe, and 3% is Detected. Then the prediction
accuracy of pilot X for Res is 95% and for Approx is 89%.
SDC-Maybe error sites are equalized based on if their
output quality degradations (QD) are above or below user
speciﬁed output quality thresholds (QT). In the absence of
a quality threshold, prediction accuracy measurements for
any SDC-Maybe pilot with a quality degradation of, say, Q
measures the number of its population members that also result
in SDC-Maybe with the same quality degradation Q. Requiring
the output quality degradation of different SDC-Maybe error
sites within an equivalence class to exactly match is too strict,
so we use a ﬂexibility parameter, δ , that allows a ﬁne-grained
difference in the measured quality degradation [25]. Hence, if
the absolute difference in the quality degradation of an SDC-
Maybe pilot and an SDC-Maybe population member is less than
or equal to δ , we count it as a correct prediction. In the presence
of a user provided output quality threshold (QT), however, SDC-
Maybe error outcomes of the pilot and population members can
be appropriately equalized for resiliency and approximation.
To illustrate all of the above with an example, consider a
pilot Y that generates an SDC-Maybe with a QD-6 (quality
degradation of 6% using, for example, average relative
error as the quality metric). Suppose the error outcomes
of its population are as follows: (a) 84% of its population
is SDC-Maybe with QD-6, (b) 6% is SDC-Maybe with
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:56:27 UTC from IEEE Xplore.  Restrictions apply. 
QD-3, (c) 5% is SDC-Maybe with QD-8, (d) 3% is Masked,
and (e) 2% is DDC. Then the validation accuracy of Y for
the different validation strategies is as follows: (1) Resδ =2
is 89%=84%+5%, (2) Approxδ =2 is 89%=84%+5%, (3)
Resδ =2,QT =7 is 100% (all are correct, even (c), which lies
above the QT, because its QD falls within the δ =2 margin of
the pilot), and (4) Approxδ =2,QT =7 is 98% ((e) is incorrect).
For each application, the overall validation accuracy for
a given equivalence based pruning technique (control, store,
or combined = control+store) is obtained by calculating the
average of the pilot prediction accuracy across a random sample
of equivalence classes built using that technique (control, store,
or combined), weighted by the size of the equivalence class.
We randomly pick 750 equivalence classes to validate
each of the control- and store-equivalence techniques (1500
equivalence classes for combined control+store). This gives us
a 99% conﬁdence interval with a 5% error margin [40]. For
each equivalence class we set the population size to 750. If the
equivalence class size is less than 750 (error sites), then the pilot
is validated using all the remaining error sites in the equivalence
class. This again gives us a 99% conﬁdence interval with a 5%
error margin [40]. In all, we perform approximately 1.6 million
error-injection experiments to validate gem5-Approxilyzer.
D. Error Analysis of Applications
As mentioned in Section II-C, an application’s error outcome
proﬁle can be used for different purposes. For example,
gem5-Approxilyzer can be used to extract the set of static
instructions that need resiliency protection [36] or the set of
static instructions that are approximable under different output
quality requirements. Information about a static instruction
(called a PC) is derived by observing the error outcomes of
all the error-sites for all the dynamic instances of that static
instruction. For example, if all the error sites for a PC result
in Detected, Masked, or SDC-Good outcomes, we say that the
PC needs no resiliency protection against single-bit transient
errors. Similarly, if the PC has Detected error sites, we can
conclude it is not a candidate for approximation [25]. In our
evaluation, we assume a very conservative classiﬁcation of the
PCs in our workload. If even a single error site for the PC needs
resiliency protection, or is not approximable, then the entire PC
is labeled as needing resiliency protection, or not approximable,
respectively. Different methodologies for composing error sites
within a PC are possible, e.g., error-site information can be
used to derive the probability of an output with unacceptable
quality degradation being generated by individual PCs.
For our workloads, we show the distribution of error sites in
the application by error outcomes. Furthermore, we compose
the error sites for a PC as described above to show the percent-
age of PCs in the application that need resiliency protection
against SDCs and the percentage of PCs that are candidates for
approximate computing under given output quality thresholds.
We use gem5-Approxilyzer to perform this error analysis
on our workloads compiled to x86 binary. We also analyze
the same workloads compiled to a SPARC binary with the
older implementation of Approxilyzer using Simics, which
allows us to perform an initial comparison of the resiliency
(cid:8)(cid:17)(cid:28)(cid:1)(cid:35)(cid:33)(cid:1)(cid:52)(cid:1)(cid:41)(cid:36)
(cid:2)(cid:26)(cid:26)(cid:27)(cid:25)(cid:32)(cid:1)(cid:35)(cid:33)(cid:1)(cid:52)(cid:1)(cid:41)(cid:36)
(cid:8)(cid:17)(cid:28)(cid:1)(cid:35)(cid:33)(cid:1)(cid:52)(cid:1)(cid:41)(cid:34)(cid:1)(cid:7)(cid:10)(cid:1)(cid:52)(cid:1)(cid:44)(cid:36)
(cid:2)(cid:26)(cid:26)(cid:27)(cid:25)(cid:32)(cid:1)(cid:35)(cid:33)(cid:1)(cid:52)(cid:1)(cid:41)(cid:34)(cid:1)(cid:7)(cid:10)(cid:1)(cid:52)(cid:1)(cid:44)(cid:36)
(cid:36)
(cid:51)
(cid:35)