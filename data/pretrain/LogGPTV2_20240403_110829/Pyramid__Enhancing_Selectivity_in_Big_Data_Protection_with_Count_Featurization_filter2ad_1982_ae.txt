IV. Prototype
Pyramid is implemented in 2600 lines of Scala, as a
modular library. It integrates into the feature engineering
stage of an ML pipeline, before the actual
learning
algorithms are invoked. The modular backend allows
count tables to be stored locally in memory or in a
remote datastore such as Redis or Cassandra.
We integrated Pyramid into the Velox model manage-
ment system [25] with minimal effort, by adding/modi-
fying around 500 lines of code. The changes we made to
Velox involve interposing on all of Velox’s interfaces that
interact with raw data (e.g., adding observations, making
predictions, and retraining). Now prediction requests are
passed through the Pyramid featurization layer, which
performs count featurization.
One of Velox’s key contributions is performing low
latency predictions by pushing models to application
servers. To enable low-latency predictions, Pyramid pe-
riodically replicates snapshots of the central count tables
to the application servers, allowing them to perform fea-
turization locally. §V-E evaluates prediction performance
in Velox/Pyramid with and without this optimization.
V. Evaluation
We evaluate Pyramid using different versions of three
data-driven applications: two ad targeting applications,
two movie recommendation applications, and MSN’s
production news personalization system. We compare
models on count-featurized data to state-of-the-art mod-
els trained on raw data, and answer these questions:
Q1. Can we accurately learn on less data using counts?
Q2. How does past-data protection impact utility?
Q3. Does counting feature groups improve accuracy?
Q4. How efﬁcient is Pyramid?
Q5. To what problems does Pyramid apply?
Our evaluation yields four ﬁndings: (1) On classiﬁca-
tion problems, count featurization lets models perform
within 4% of state-of-the-art models while training on
less than 1% of the data. (2) Count featurization enables
powerful nonlinear algorithms, such as neural networks
and boosted trees, that would be infeasible due to high-
cardinality features. (3) Protecting individual past obser-
vations with differential privacy adds 1% penalty to the
accuracy, which remains within 5% of state-of-the-art
models. (4) Pyramid’s performance overheads are small.
V.A. Methodology
Workloads. Table II shows our apps, datasets, and
baselines. We defer discussion of MSN to §V-G.
• Criteo ad targeting. Using two versions of the well-
known Criteo ads dataset, we build a binary click/no-
click classiﬁer. We use seven days of the Criteo ad click
dataset amounting to 1.2 billion total observations. This
dataset is very imbalanced with an approximate click
rate of 3.34%. The second version of the Criteo dataset
86
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:25:23 UTC from IEEE Xplore.  Restrictions apply. 
Feat.
1.2B
39
in
Obs.
45M 39
Dataset
Criteo Kag-
gle [34]
Criteo
Full [36]
MovieLens
[37]
MovieLens
[37]
MSN.com
production
App
Ad targeting (classiﬁca-
tion)
Ad targeting (classiﬁca-
tion)
Movie recommendation
(classiﬁcation)
Movie recommendation
(regression)
News
(regression)
TABLE I: Workloads. Apps and datasets; number of observations
and features in each dataset; and baselines used for comparison. All
baselines are trained using VW [32].
Baseline
neural
net
Kaggle [35]
regularized lin-
ear model
matrix
ization [32]
matrix
ization [32]
contextual ban-
dits [38], [39]
personalization
24M 507
22M 21
22M 21
factor-
factor-
Dataset
Criteo-Kaggle
Model
B: neural net (nn)
boosted
regression
regression
singular value
logistic
(log. reg.)
gradient
trees (gbt)
B: ridge regression
(rdg. reg.)
B:
decomposition
(svd)
linear
(lin. reg.)
gradient
trees (gbt)
B:
decomposition
(svd)
logistic
(log. reg.)
gradient
trees (gbt)
contextual bandit
singular value
regression
boosted
boosted
Criteo-Full
MovieLens
Regression
MovieLens
Classiﬁcation
MSN.com
Parameters
VW. One 35 nodes hidden layer
with tanh activation. LR: 0.15.
BP: 25. Passes: 20. Early Termi-
nate: 1.
VW. LR: 0.5. BP: 26.
Sklearn. 100 trees with 8 leaves.
Subsample: 0.5. LR: 0.1. BP: 8.
VW. L2 penalty: 1.5e−8. LR:
0.5. BP: 26.
VW. Rank 10. L2 penalty: 0.001.
LR: 0.015. BP: 18. Passes: 20. LR
Decay: 0.97. PowerT: 0.
VW. LR: 0.5. BP: 22. Passes: 5.
Early Terminate: 1.
Sklearn. 100 trees with 8 leaves.
Subsample: 0.5. LR: 0.1. BP: 8.
VW. Rank 10. L2 penalty: 0.001.
LR: 0.015. BP: 18. Passes: 20. LR
decay: 0.97. PowerT: 0.
VW. LR: 0.5. BP: 22. Passes: 5.
Early Terminate: 1.
Sklearn. 100 trees with 8 leaves.
Subsample: 0.5. LR: 0.1. BP: 8.
VW.
IPS context. bandit. LR:
0.02. BP: 18.
TABLE II: Model parameters. The libraries and parameters used to
train each model. The parameters not noted use library defaults. “LR”
indicates the learning rate. “BP” indicates the hash featurization’s bit
precision (only applicable to raw models). “PowerT” exponent controls
learning learning rate decay per step. “B:” indicates that the model
will be used as a baseline. VW and Sklearn denote that the model was
trained with Vowpal Wabbit [32] and scikit-learn [33], respectively.
has 45 million observations, and was released as part of
a Kaggle competition. In the Criteo Kaggle dataset, the
click and non-click points were sampled at different rates
to create a more balanced class split with a 25% click
rate. Each observation has 39 features (13 numeric, 26
categorical), and 8 of the categorical features are high
dimensional (> 100K values). The numeric features
were binned into 4 equal size bins for each dataset. As
a baseline, we use a feed-forward neural network that
performed well for the competition dataset [35], and we
use ridge regression for the full dataset.
• MovieLens movie recommendation. Using the well-
known MovieLens dataset, which consists of 22M rat-
ings on 34K movies from 240K users, we build two
predictors: (1) a regression model that predicts the user’s
rating as a continuous value in [0, 5], (2) a binary classi-
ﬁer that predicts if a user will give a rating of 4 or more.
As a baseline, we use the matrix factorization algorithm
in Vowpal Wabbit (VW) [32]; algorithms in this class are
87
state-of-the-art for recommender systems [40], although
this speciﬁc implementation is not the most advanced.
Method. For each application, we try a variety of count
models, including linear or logistic regression, neural
networks, and boosted trees. We split each dataset by
time into a training set (80%) and testing set (20%),
except for the full Criteo dataset for which we use the
ﬁrst six days for training and the seventh for testing.
On the training set, we compute the counts and train our
models on windows of growing sizes, where all windows
contain the most recent training data and grow back-
wards to include older data. This ensures that training
occurs on the most recent data (closest to the testing
set), and that count tables only include observations from
the hot window or the past. We use the testing set to
compare the performance of our count algorithms to their
raw data counterparts and to the baseline algorithms.
For all baselines, we apply any dimensionality reduction
mechanisms (e.g., hash featurization [41]) that
those
models typically apply to strengthen them.
Metrics. We use two model accuracy metrics.
(1) The average logistic loss for classiﬁcation problems
with categorical labels (e.g. click/no-click). Algorithms
predict a probability for each class and are penalized
by the logarithm of the probability predicted for the
true class: − log(ptrue class). Models are penalized less
for incorrect, low-conﬁdence predictions and more for
incorrect, high-conﬁdence predictions. Logistic loss is
better suited than accuracy for classiﬁcation problems
with imbalanced classes because a model cannot perform
well simply by returning the most common class.
(2) The average squared loss for regression problems
with continuous labels. Algorithms make real-valued
predictions that are penalized by the square of the
difference with the label: ||prediction − label||2.
We conclude our evaluation with our experience with
a production setting, in which we can directly estimate
click-through rate, a more intuitive metric.
Result interpretation. All graphs report loss normalized
by the baseline model trained on the entire training data.
Lower values are better in all graphs: a value of 1 or less
means that we beat the baseline’s best performance; and
a value > 1 means that we do worse than the baseline.
For completeness, we specify our baselines’ perfor-
mance: MovieLens classiﬁcation matrix factorization has
a logistic loss of 0.537; MovieLens regression matrix
factorization has a squared loss of 0.697; Criteo-Kaggle
neural network has a logistic loss of 0.467; and Criteo-
Full ridge regression has a logistic loss of 0.136.
V.B. Training Set Reduction (Q1)
Pyramid’s design is predicated on count featurization’s
ability to substantially reduce training sets. While this
method has long been known, we are unaware of scien-
tiﬁc studies of its effectiveness for training set reduction.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:25:23 UTC from IEEE Xplore.  Restrictions apply. 
(a) MovieLens classiﬁcation
(b) Criteo-Kaggle classiﬁcation
(c) Criteo-Full classiﬁcation
Fig. 4: Normalized losses for raw and count algorithms. “B:” denotes the baseline model. Count algorithms converge faster than raw data
algorithms, to results that are within 4% on MovieLens, and within 2% and 4% on Criteo Kaggle and full respectively.
(a) MovieLens boosted tree
(b) Criteo-Kaggle algorithms
(c) Criteo-Full ridge regression
Fig. 5: Impact of data protection. Results are normalized by the baselines. We ﬁx k = 1 and vary , the privacy budget. Fig. 5(a) and Fig. 5(b)
show results using the weighted noise (denoted wght). On MovieLens our weighting scheme is crucial to hide 1 observation. On Criteo we can
easily hide 1 observation with little performance degradation and can hide up to 100 observations while remaining within 5% of the baseline.
We hence perform a study here. The count models must
converge faster than raw-data models (reach their best
performance with less data), and perform on par with
state-of-the-art baselines. Fig. 4 shows the performance
of several linear and nonlinear models, on raw and count-
featurized data. We make two observations.
First, training with counts requires less data. On
both Criteo and MovieLens the best count-featurized
algorithm approaches the best raw-data algorithm by
training on 1% of the data or less. On Criteo-Kaggle
(Fig. 4(b)), the count-featurized neural network comes
within 3% of the baseline when trained on 0.4% of the
data and performs within 1.7% of the baseline with 28%
of the training data. On Criteo-Full (Fig. 4(c)), the count-
featurized ridge regression model comes within 3.3%