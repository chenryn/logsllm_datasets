environment in which they can employ sophisticated malware in-
strumentation safely and robustly.
4.
INITIAL & EVENTUAL DESIGN
GOALS
As we mentioned above, our initial design for GQ targeted a
worm-oriented honeyfarm architecture. Over the past years, we fo-
cused on evolving the system into a generic malware farm—a plat-
form suitable for hosting all manner of malware-driven research
safely, routinely, and at scale. In the following we outline our orig-
inal design goals as well as those we came to formulate at later
stages of building the system.
Versatility. In contrast to previous work, GQ must serve as a
platform for a broad range of malware experimentation, with no
built-in assumptions about inmates belonging to certain classes of
malware, or acting exclusively as servers (realizing traditional hon-
eyfarms) or clients (realizing honeycrawlers [30]). Our initial focus
on worms imposed design constraints that became impractical once
we wanted to experiment with non-self-propagating malware. Sim-
ilarly, focus on a particular class of botnets (say those using IRC as
C&C, or domain generation algorithms for locating C&C servers
via DNS), restricts versatility.
Separation of policy and mechanism. This goal closely relates
to the former, but we needed to invest considerable thought in or-
der to achieve it in an effective fashion. In our original honeyfarm
architecture we tightly interwove mechanism (a highly customized
packet forwarder) and policy (worm containment) in forwarding
code that we could adapt only with difﬁculty and by imposing sys-
tem downtime. At the time, the need for greater containment ﬂexi-
bility when running arbitrary kinds of malware had not yet become
clear to us. GQ must clearly separate a comparatively stable con-
tainment enforcement mechanism from an adaptable containment
policy deﬁnition.
Veriﬁable containment. GQ must provide complete control
over all inbound and outbound communication. This remains as
true now as it was at the outset of GQ’s development. Depending
on a ﬂow’s source, destination, and content the system must allow
dropping, reﬂecting, redirecting, rewriting, and throttling the ﬂow
ﬂexibly according to current execution context. Moreover, ideally
mechanisms would exist to verify that developed policies operate
as intended; we have not implemented such, a deﬁciency of our
current system.
Multiple execution platforms. Malware frequently checks the
nature of its execution platform in order to determine whether it
likely ﬁnds itself running in an analysis environment or as a suc-
cessful infection in the wild. For example, malware often considers
evidence of virtualization a tell-tale sign for operation in a malware
analysis environment [8] and ceases normal operation. Indepen-
dently of potential virtualization, malware also might only execute
correctly on certain operating system types, versions, or patch lev-
els, so we need to ensure support for a diverse range of such con-
ﬁgurations. For these conﬁgurations, GQ must support virtualized,
emulated, and raw iron execution as desired, on a per-inmate gran-
ularity, in a fashion transparent to the containment process.
Multiple experiments. While extending GQ we increasingly
encountered the need for running multiple malware-driven setups at
the same time. The original single-experiment design made it dif-
ﬁcult to accommodate this. For GQ, we aim to provide a platform
for independently operating experiments involving different sets of
machines, containment policies, and operational importance. For
example, we have found it exceedingly useful to run both “deploy-
ment” and “development” setups for studying spambots; one for
continuously harvesting spam from specimens for which we have
developed mature containment policies, the other one for develop-
ing such policies on freshly obtained samples.
Infection strategy & threat model. We need to consider the
way in which machines on our inmate network become infected,
as well as the behavioral envelope once a machine has become in-
fected. Regarding the former, if an experiment so desires, GQ fully
supports the traditional honeyfarm model in which external trafﬁc
399Figure 1: GQ’s overall architecture.
directly infects honeypot machines. Today, we ﬁnd intentional in-
fection with malware samples collected from external sources more
relevant and useful. We describe our mechanism for facilitating
such infections in § 6.6. Regarding post-infection behavior, we
adopt the position of other systems executing malware in virtual-
ized environments and assume that the program cannot break out of
the virtual environment. The malware may corrupt user and kernel
space on the infected host arbitrarily and conduct arbitrary network
I/O.
5. GQ’S ARCHITECTURE
5.1 Overall layout
We show GQ’s architecture in Figure 1. A central gateway sits
between the outside network and the internal machinery, separating
it into an inmate network on which we host infected machines—
inmates—and a management network that we use to access virtual
machine monitors (VMMs) and other farm control infrastructure.
Within the gateway, custom packet forwarding logic enables and
controls the ﬂow of trafﬁc between the farm and the outside net-
work, as well as among machines within the farm. This takes sev-
eral forms. First, a custom learning VLAN bridge selectively en-
ables crosstalk among machines on the inmate network as required,
subject to the containment policy in effect. Its ability to learn about
the hosts present reduces the conﬁguration overhead required to
bootstrap the inmate network. Second, we devote the majority of
the forwarding logic for redirecting outgoing and incoming ﬂows to
a containment server, which serves two purposes. First, it decides
what containment policies to enforce on a given ﬂow, and second,
for some containment decisions it also proceeds to enforce the con-
tainment. (We describe this further in § 5.4.) Finally, a safety ﬁlter
ensures that the rate of connections across destinations and to a
given destination never exceeds conﬁgurable thresholds.
5.2 Inmate hosting & isolation
We employ three different inmate-hosting technologies:
full-
system virtualization (using VMware ESX servers), unvirtualized
“raw iron” execution (more on this in § 6.4), and QEMU for cus-
tomized emulation [4]. The hosting technology we employ for a
given inmate remains transparent to the gateway.
GQ enforces inmate isolation at the link layer: each inmate sends
and receives trafﬁc on a unique VLAN ID. VLAN IDs thus serve
as handy identiﬁers for individual inmates. Physical and, where
feasible, virtual switches (for simplicity not shown in Figure 1) be-
hind the gateway enforce the per-inmate VLAN assignment, which
our inmate creation/deletion procedure automatically picks and re-
leases from the available VLAN ID pool.
The gateway’s packet forwarding logic enables connectivity to
the outside Internet when permissible, while selectively enabling
inmate crosstalk as required.
5.3 Network layout
We use network address translation for all inmates. The packet
forwarder dynamically assigns internal addresses from RFC 1918
space, triggered by the inmates’ boot-time chatter. It maps source
addresses in inside→outside ﬂows to conﬁgurable global address
space, and depending on conﬁguration either drops outside→inside
ﬂows (emulating typical home-user setups) or forwards them on,
rewriting destination addresses to the corresponding internal ones
(thus providing Internet-reachable servers). This provides one level
of indirection and allows us to keep the inmate conﬁguration both
simple and dynamic at inmate boot time. To facilitate this, the in-
mate network provides a small number of infrastructure services
for the inmates, including DHCP and recursive DNS resolvers.
The gateway places these services into a restricted broadcast do-
main, comprising all machines the inmates can reach by default.
In addition, infrastructure services include experiment-speciﬁc re-
quirements such as SMTP sinks, though these do not belong to the
broadcast domain.
5.4 Containment servers
GQ’s design hinges crucially on the notion of an explicit con-
tainment server that determines each ﬂow’s containment policy.
Having such a separate component represents a major improvement
over existing systems. Our original design included no equivalent;
instead, the packet router in the gateway alone implemented the
containment policy. The current design results from our desire to
reduce the gateway’s forwarding logic to an infrequently changing
mechanism to the greatest extent possible, while keeping contain-
ment policies conﬁgurable and adaptable. This contrasts starkly
with our initial design, which intermixed a ﬁxed nine-step contain-
ment policy with the mechanisms for enforcing that policy, all in-
side the gateway.
The containment server is both a machine—typically a VM,
for convenience—and a standard application server running on the
machine. Conceptually, the combination of the gateway’s packet
router and the containment server realizes a transparent application-
layer proxy for all trafﬁc entering and leaving the inmate network.
The gateway’s packet router informs the containment server of all
new ﬂows to or from inmates, enabling it to issue containment ver-
dicts on the ﬂows.1 Our implementation (§ 6.2) ensures that race
conditions cannot occur. The router subsequently enforces these
verdicts, which enable both endpoint control and content control.
1 Note that this approach parallels the architecture of Open-
Flow [20] devices to some extent. However, our implementation
predates OpenFlow’s wide availability.
GatewayManagementAccessManagementNetworkInmateNetwork400(a) Forward
(b) Rate-limit
(c) Drop
(d) Redirect
(e) Reﬂect
(f) Rewrite
Figure 2: GQ’s ﬂow manipulation modes, illustrated on ﬂows initiated by an inmate.
Figure 3: A possible subfarm scenario in GQ: three independent routers handle disjunct sets of VLAN IDs, thus enabling parallel
experiments. For sake of clarity, we do not show per-subfarm infrastructure services individually.
Endpoint control happens at the beginning of a ﬂow’s lifespan and
allows us to (i) forward ﬂows unchanged, (ii) drop them entirely,
(iii) reﬂect them to a sink server or alternative custom proxy, (iv)
redirect them to a different target, or (v) rate-limit them. Once the
gateway has established connectivity between the intended end-
points, it alone enforces endpoint control, conserving resources
on the containment server. Content control, on the other hand,
remains feasible throughout a ﬂow’s lifespan, and works by fully
proxying the ﬂow content through the containment server. We can
thus (i) rewrite a ﬂow, (ii) terminate it when it would normally still
continue, or (iii) prolong it by injecting additional content into a
ﬂow that would otherwise already terminate. Note that the connec-
tion’s destination need not necessarily exist: the containment server
can simply impersonate one by creating response trafﬁc as needed.
Endpoint and content control need not mutually exclude each other;
for example, it can make sense to redirect a ﬂow to a different desti-
nation while also rewriting some of its contents. Figure 2 illustrates
the ﬂow manipulation modes. Note how the containment server re-
mains passive during containment enforcement except in case of
content rewriting (Figure 2(f)).
Besides ﬂow-related containment management, the containment
server also controls the inmates’ life-cycle. As the containment
server witnesses all network-level activity of an inmate, it can react
to the presence—and absence—of such network events using ac-
tivity triggers. These triggers can terminate the inmate, reboot it,
or revert it to a clean state for subsequent reinfection. For exam-
ple, a typical life-cycle policy for a spambot might automatically
restart the bot once it has ceased spamming activity for more than
an hour. Another common policy is to terminate an inmate that has
begun sending a particular recipient more than a certain number of
connection requests per minute.
5.5 Inmate control
To facilitate inmate life-cycle control, we require a component in
the system that can create inmates, expire them, or adjust their life-
cycle state by starting, stopping, or reverting them to a clean snap-
shot. In GQ’s architecture, the containment servers issue life-cycle
actions to an inmate controller located centrally on the gateway. 2
Since we keep containment servers logically and physically sepa-
rate from the gateway, we require a mechanism allowing them to
communicate with the inmate controller. We do so by maintaining
an additional network interface only on “maintaining ... only on”
reads peculiarly –VP the containment servers that allows them to
interact directly (out-of-band of the inmate-related network activ-
ity), with the gateway via the management network. This controller
understands the inmate hosting infrastructure and abstracts physi-
cal details of the inmates, such as their hosting server and whether
they run virtualized or on raw iron. The controller requires only the
2Conceptually, we could place the controller on a separate server.
For practical reasons, including immediate access to all VMMs and
the Raw Iron Controller (see 6.4), we decided to keep it on the
gateway.
Gateway401inmate’s VLAN ID in order to identify the target of a life-cycle ac-
tion. The thin arrowed line in Figure 1 illustrates the trigger–action
ﬂow from the containment server to the hosting infrastructure.
The combination of central gateway and inmate network par-
allelizes naturally:
instead of a global packet forwarding logic
handling the entire range of VLAN IDs on the inmate network,
multiple instances of the packet router process trafﬁc on disjoint
sets of VLAN IDs. This creates independent, self-contained, non-
interfering habitats for different experiments. We call these habitats
subfarms. Subfarms generally have their own containment server
(thereby distributing load and allowing other subfarms to continue
operating while we tune a given subfarm’s conﬁguration) and in-
frastructure services, but can vary widely in experiment-speciﬁc
additional services. Shared network resources, as mentioned by
Chen et al. [7], are possible, but we have not found them necessary
in practice—the very reason for creating independent subfarms is
typically the need to conﬁgure systems differently or record sepa-
rate data sets. Figure 3 illustrates subfarms.
5.6 Monitoring and packet trace archival
We use a two-pronged packet trace recording strategy. First, the
gateway’s packet routers record each subfarm’s activity from the in-
mate network’s perspective, i.e., with unroutable addresses for the
inmates. Using these local addresses has the beneﬁt of providing
some degree of immediate anonymity in the packet traces, simplify-
ing data sharing without the risk of accidentally leaking our global
address ranges. Second, system-wide trace recording happens at
the upstream interface to the outside network, thus capturing all
activity globally and as seen outside of GQ.
IMPLEMENTATION
6.
6.1 Packet routing
We implement the subfarm’s packet routers on the gateway us-
ing the Click modular router [16]. We separate each subfarm’s
Click conﬁguration into a module containing invariant, reusable
forwarding elements shared across all subfarms (400 lines), and
a small (40 lines) conﬁguration module that speciﬁes each sub-
farm’s unique aspects, including the external IP address ranges, set
of VLAN ID ranges from the inmate network, and logﬁle naming.
The custom Click elements for the VLAN bridge, NAT, contain-
ment server ﬂow handling, and trace recording comprise around
5,000 lines of C++.
6.2 Containment server
We wrote the containment server in Python, using a pre-forked,
multi-threaded service model. The server logic comprises roughly
2,200 lines, with 600 lines for the event-trigger logic. The contain-
ment policies, including content rewriters, add up to 1,000 lines.
Shimming protocol. To couple the gateway’s packet router to
the containment server, we need a way to map/unmap arbitrary
ﬂows to/from the single address and port of the containment server.
We achieve this mapping using a shimming protocol conceptually
similar to SOCKS [15]: upon redirection to the containment server,
the gateway injects into the ﬂow a containment request shim mes-
sage with meta-information. Figure 4 summarizes the message
structure.
The containment server expects this meta-information and uses it
to assign a containment policy to the ﬂow. The containment server
similarly conveys the containment verdict back to gateway using
a containment response shim, which the packet router strips from
the ﬂow before relaying subsequent content back to the endpoint.
For TCP, the shim is sent as a separate TCP packet injected into
(a) Request Shim
(b) Response Shim
Figure 4: The shim protocol message structure.