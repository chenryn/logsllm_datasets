which would occur when wiring is broken. We see that the
path failure ratio of DFR increases with the link failure ratio.
However, the path failure ratio of SPF is almost 0. This
is because very few nodes are disconnected from the graph
(indicating the robustness of our DCell structure). However,
DFR cannot achieve such performance since it is not globally
optimal. When the failure ratio is small (say, less than 5%),
the performance of DFR is still very close to SPF. As shown
in Table 2, the average path length under link failure is
similar to that under node failure.
6.2 DFR in a Partial DCell
We have also evaluated DFR when a DCell is incremen-
tally upgraded using our incremental expansion procedure
of Section 5. To this end, we consider a large DCell3 net-
work with n=6 (that supports up to 3.26-million servers).
Failure
ratio
0.02
0.04
0.08
0.12
0.2
Node failure
DFR SPF
10.00
11.60
10.16
12.00
12.78
10.32
10.50
13.60
16.05
11.01
Rack failure
DFR SPF
10.00
11.37
10.01
11.55
11.74
10.09
10.14
11.96
12.50
10.32
Link failure
DFR SPF
10.14
11.72
10.26
12.40
13.73
10.55
10.91
14.97
17.90
11.55
Table 2: Average path lengths for DFR and SPF in
a DCell3 with n = 4.
We show that DFR achieves stable performance with vari-
ous ratio of deployed nodes.
Figure 9 plots the performance of DFR with the ratio
of deployed nodes varying from 10% to 100%. The failure
rate is set to 5% for all three failure models. The results
of SPF are stable at 5% under nodes and rack failures, but
close to 0 under link failures. DFR has its path failure ra-
tios smaller than 0.9% under link failures, and smaller than
6% under node and rack failures, respectively. The result
demonstrates that DCell is fault tolerant even when it is
partially deployed.
7.
IMPLEMENTATION
In this section, we design and implement a DCN protocol
suite for DCell. We also report experimental results from an
operational DCell testbed with over twenty server nodes.
7.1 DCN Protocol Suite
The DCN protocol suite serves as a network layer for
DCell-based data centers. It includes DCN addressing, DCN
header format, and protocols for neighborhood and link-
state management. It provides functionalities similar to IP
over the Internet [18].
DCN Addressing: We use a 32-bit uid to identify a server.
The most signiﬁcant bit (bit-0) is used to identify the address
type. If it is 0, the address is the uid of a server; otherwise,
the address is a multicast address. For a multicast address,
the 1∼3 bits are used to identify the scope of the DCell,
within which the message can propagate. The remaining
28 bits are used to indicate the multicast group. Currently,
only one concrete multicast address is deﬁned: when these
28 bits are all ones, it deﬁnes a broadcast address.
DCN Header: Figure 10 shows the format of the DCN
header. The header size is 20 or 24 bytes depending on the
existence of the Proxy DCN Address(see Section 4.3 for de-
7.2 Layer-2.5 DCN Prototyping
Conceptually, the DCN protocol suite works at layer 3 and
is a network-layer protocol. However, replacing IP with the
DCN layer requires application changes, as almost all cur-
rent network applications are based on TCP/IP. To address
this issue, we implement the DCN suite at an intermediate
layer between IP and the link layer, which we call Layer-2.5
DCN. In DCN, IP address is used for end-host identiﬁcation
without participating in routing, and current applications
are supported without any modiﬁcation. In our design, we
choose to have a ﬁxed one-to-one mapping between IP and
DCN addresses. This design choice greatly simpliﬁes the
address resolution between IP and DCN.
We have implemented a software prototype of Layer-2.5
DCN on Windows Server 2003. Our implementation con-
tains more than 13000 lines of C code. The DCN protocol
suite is implemented as a kernel-mode driver, which oﬀers
a virtual Ethernet interface to the IP layer and manages
several underlying physical Ethernet interfaces. In our cur-
rent implementation, operations of routing and packet for-
warding are handled by CPU. A fast forwarding module is
developed to receive packets from all the physical network
ports and decide whether to accept packets locally or for-
ward them to other servers. The forwarding module main-
tains a forwarding table. Upon receiving a packet, we ﬁrst
check whether its next hop can be found in the forwarding
table. When the next hop of a packet is not in the forwarding
table, DFR routing will be used to calculate the next hop,
which is subsequently cached in the forwarding table. When
any link state changes, the forwarding table is invalidated
and then recalculated by DFR.
7.3 Experimental Results
We have an operational testbed of a DCell1 with over 20
server nodes. This DCell1 is composed of 5 DCell0s, each
of which has 4 servers (see Figure 1 for the topology). Each
server is a DELL 755DT desktop with Intel 2.33GHz dual-
core CPU, 2GB DRAM, and 160GB hard disk. Each server
also installs an Intel PRO/1000 PT Quad Port Ethernet
adapter. The Ethernet switches used to form the DCell0s
are D-Link 8-port Gigabit switches DGS-1008D (with each
costing about $50). Each server uses only two ports of the
quad-port adapter. Twisted-pair lines are used to intercon-
nect the DCN testbed. Two experiments are carried out to
study the fault-tolerance and network capacity of DCell:
Fault-tolerance: In this experiment, we set up a TCP con-
nection between servers [0,0] and [4,3] in the topology of Fig-
ure 1. The path between the two nodes is [0,0], [0,3], [4,0],
[4,3] initially. To study the performance under link failures,
we manually unplugged the link ([0,3], [4,0]) at time 34s
and then re-plugged it in at time 42s. We then shutdown
the server [0,3] at time 104s to assess the impact of node
failures. After both failures, the routing path is changed to
[0,0], [1,0], [1,3], [4,1], [4,3]. And after re-plug event, the
path returns to the original one. The TCP throughput is
plotted in Figure 11. The CPU utilizations are about 40%,
45%, and 40% for sender, receiver, and forwarder, respec-
tively.
We make two observations from the experiment. First,
DCell is resilient to both failures. The TCP throughput is
recovered to the best value after only a few seconds. Sec-
ond, our implementation detects link failures much faster
than node failures, because of using the medium sensing
Figure
node/rack/linik failure probabilities are 5%.
ratios when
9:
Path
failure
the
Figure 10: The DCN protocol header.
tails). The design of the DCN header borrows heavily from
IP, herein we highlight only the ﬁelds speciﬁc to DCN. Iden-
tiﬁcation is 32, rather than 16 bits, as in IP. This is because
the link bandwidth in data centers is quite high. A 16-bit
ﬁeld will incur identiﬁcation recycles within a very short pe-
riod of time. Retry is a 4-bit ﬁeld to record the maximum
number of local-reroutes allowed. It decrements by one for
each local-reroute performed. A packet is dropped once its
retry count becomes 0. Our current implementation sets the
initial retry count as 5. Flags is a 4-bit ﬁeld, and only bit-3
of the ﬂag, PF (Proxy Flag), is currently deﬁned. When PF
is set, the Proxy DCN Address is valid; otherwise, the data
payload starts right after the Destination DCN Address.
Neighbor Maintenance: To detect various failures, we
introduce two mechanisms for neighbor maintenance. First,
a DCN node transmits heart-beat messages over all its out-
bound links periodically (1 second by default). A link is
considered down if no heart-beat message is received before
timeout (5 seconds by default). Second, we use link-layer
medium sensing to detect neighbor states. When a cable
becomes disconnected or re-connected, a notiﬁcation mes-
sage is generated by the link-layer drivers. Upon receiving
notiﬁcation, the cache entry at the corresponding neighbor
is updated immediately.
Link-state Management: DFR uses link-state routing in-
side DCellb subnetworks. Therefore, each node needs to
broadcast its link states to all other nodes inside its DCellb.
This is done by using DCellBroadcast (see Section 4.2). A
node performs link-state updates whenever the status of its
outbound neighboring links changes. It also broadcasts its
link states periodically. Links that have not been refreshed
before timeout are considered as down and thus removed
from the link-state cache.
Figure 11: TCP throughput with node and link fail-
ures.
Figure 12: Aggregate TCP Throughput under DCell
and Tree.
technique. Figure 11 shows that, the link failure incurs only
1-second throughput degradation, while the node failure in-
curs a 5-second throughput outage that corresponds to our
link-state timeout value.
Network capacity: In this experiment, we compare the ag-
gregate throughput of DCell and that of the tree structure.
The target real-life application scenario is MapReduce [5].
In its Reduce-phase operations, each Reduce worker fetches
data from all the other servers, and it results in an all-to-all
traﬃc pattern. In our experiment, each server established
a TCP connection to each of the remaining 19 servers, and
each TCP connection sent 5GB data. The 380 TCP con-
nections transmitted 1.9TB data in total. There was no
disk access in this experiment. This is to separate network
performance from that of disk IO. We study the aggregate
throughput under DCell and a two-level tree structure. In
the two-level tree, the switches of the 5 DCell0s were con-
nected to an 8-port Gigabit Ethernet switch.
Figure 12 plots both the aggregate throughput of DCell
and that using the two-level tree. The transmission in DCell
completed at 2270 seconds, but lasted for 4460 seconds in
the tree structure. DCell was about 2 times faster than Tree.
The maximum aggregate throughput in DCell was 9.6Gb/s,
but it was only 3.6Gb/s in the tree structure.
DCell achieves higher throughput than the tree-based scheme.
We observed that, the 20 one-hop TCP connections using
the level-1 link had the highest throughput and completed
ﬁrst at the time of 350s. All the 380 TCP connections
completed before 2270s. Since we currently use software
for packet forwarding, CPU becomes the major bottleneck
(with 100% CPU usage in this experiment), which prevents
us from realizing all the potential capacity gains of DCell.
In our future implementation when packet forwarding is of-
ﬂoaded to hardware, we expect DCell to deliver much higher
peak throughput of about 20Gb/s. Moreover, our current
gain is achieved using a small-scale testbed. The merits
of DCell will be more signiﬁcant as the number of servers
grows.
One might expect the tree structure to have much higher
throughput than the measured 3.6Gb/s. Each pair of the
servers sharing a single mini-switch should be able to send/receive
at the line speed, and the aggregate throughput should be
close to 20Gb/s in the beginning. However, this is not true.
The top-level switch is the bottleneck and soon gets con-
gested. Due to the link-layer ﬂow control, this will eventu-
ally cause queue to build up at each sender’s buﬀer. All TCP
connections at a server share the same sending buﬀer on a
network port. Therefore, all TCP connections are slowed
down, including those not traversing the root switch. This
results in much smaller aggregate TCP throughput.
8. RELATED WORK
Interconnection networks have been extensively studied in
parallel computing for two decades (see e.g., [21, 16, 6, 17,
13, 12]). In this section, we compare DCell with several rep-
resentative interconnection structures in the DCN context.
Our comparison does not imply, in any sense, that other
structures are not suitable for their original design scenarios.
Instead, we intend to show that DCell is a better structure
for data centers, which require scaling, fault tolerance, high
network capacity, and incremental expansion.
Table 3 shows the comparison results. We use N to de-
note the number of supported servers. The metrics used
are: (1)Node degree: Small node degree means fewer links,
and fewer links lead to smaller deployment overhead; (2)Net-
work diameter: A small diameter typically results in eﬃcient
routing; (3)Bisection width (BiW): A large value shows good
fault-tolerant property and better network capacity; (4)Bot-
tleneck Degree (BoD): BoD is the maximum number of ﬂows
over a single link under an all-to-all communication model.
A small BoD implies that the network traﬃc is spread out
over all the links.
largest BiW, and
smallest BoD, but its node degree is N − 1. Ring and 2D
Torus use only local links. Ring has the smallest node de-
gree, but a BiW of 2, large diameter and BoD. 2D Torus
√
[17] uses only local links and has a constant degree of 4.
N − 1) and BoD (in proportion
√
But it has large diameter (
N ). These three structures are not practical even for
to N
small data centers with hundreds of servers.
FullMesh has the smallest diameter,
In a Tree structure, servers are attached as leaves, and
switches are used to build the hierarchy. When the switches
are of the same degree d, the diameter of the Tree is 2 logd−1N .
But Tree has a bisection width of 1 and the bottleneck de-
gree is in proportion to N 2. Fat Tree [6] overcomes the
bottleneck degree problem by introducing more bandwidth
into the switches near the root. Speciﬁcally, the aggregated
bandwidth between level-i and level-(i + 1) is the same for
all levels. Due to the problem of large node degree at the
Degree Diameter
N − 1
2
1
N
√
2
N − 1