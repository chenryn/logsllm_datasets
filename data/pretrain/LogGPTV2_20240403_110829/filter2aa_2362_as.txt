typical unplanned outage, again with a special attention to those that are
security related.
• System revenue generation (cost per hour) shows the business value of the
service or the system, e.g., loss of revenue per hour of down-time.
116
Fuzzing Metrics
12Andrew Jaquith. (2007). Security Metrics: Replacing Fear, Uncertainty, and Doubt. (pp. 68–71)
Boston: Addison-Wesley.
• Unplanned down-time impact (cost) quantifies foregone revenue due to the
impact of incidents.
• Mean time between failures (time) characterizes how long systems are typi-
cally up between failures.
• Loss of data (cost) fees associated with loss of data due to security breach.
• Intangibles (cost) loss of business or credibility due to outages, especially those
caused by security incidents.
When such metrics are in place and monitored, the amount of down-time related
to security incidents can create revealing insight into the value of software vulner-
abilities. However, there is one problem with some of the above availability metrics
from a security perspective. When a hidden security vulnerability exists in a system,
the system can be shut down by anyone who knows the details of that vulnerabil-
ity at any given time. Furthermore, the system can be kept down using repeated
attacks. Typical availability metrics work best when incidents are not caused by
humans. For example, all the above metrics are better suited for hardware-related
failures. Still, these metrics are very useful because the people responsible for IT
operations easily understand them. These metrics are highly valuable when the
direct costs related to security incident needs to be explained to people who have
not personally encountered a security incident, at least not yet.
The cost can also be the actual value of the device or a service, which is very
direct and concrete. For example, in cases in which security vulnerabilities are found
and exploited in embedded devices, the system can become corrupt to a point that
it cannot be repaired or it would require reprogramming by the manufacturer. The
same applies when critical data is destroyed or leaked to the public. These types of
mistakes can be impossible for the end user to repair.
A commonly used metric for this purpose is ROSI (return on security investment).
If investment in a fuzzer is less than the value (cost) of a security incident multiplied
by the probability of an incident, the investment in fuzzing can be justified.
4.2.4
Cost of Patch Deployment
Deploying changes to the system after failure creates additional direct and measur-
able costs besides the direct costs caused by the incident itself. Some of these met-
rics are directly related to the down-time metric in case the system requires
third-party involvement to recover from the crash or failure. Such metrics provide
additional information related to the maturity of the process of recovering the sys-
tem back to running. These system recovery related metrics are:13
• Support response time (average time) indicates the time it takes from the out-
age to the time of response from the responsible internal support personnel,
or from the manufacturer or vendor of the failing component.
• Mean time to recovery (time) characterizes how long it takes to recover from
incidents once the repair activities are started.
4.2
Transition to Proactive Security
117
13Andrew Jaquith. (2007). Security Metrics: Replacing Fear, Uncertainty, and Doubt. (pp.
71–72). Boston: Addison-Wesley.
• Elapsed time since last disaster recovery walk-through (time) shows the rela-
tive readiness of disaster recovery programs.
Although this metric can be adequate for an enterprise user, it does not provide
enough details on what happens behind the scenes when the failure is repaired.
Repairing a security mistake is almost never as easy as removing the failing compo-
nent and replacing it with a functional component. Problems related to the recov-
ery metrics from the software security perspective are related to the complexity of
the security updates and the readiness of the entity responsible for software devel-
opment to dedicate resources for creating such security update. The problem can-
not be fixed by the IT staff if there is no update or patch available to deploy. In our
research we have seen everything from a matter of hours up to more than a year of
response time from the discovery of a new security flaw into releasing an update to
correct the software. Unfortunately, in many cases, without public disclosure of the
vulnerability details, or without an incident related to that specific security flaw, a
vendor might not be motivated enough to develop and release these critical updates
to its software. This is apparent from the metrics available from the OUSPG disclo-
sure process shown in Figure 4.3.14 The OUSPG research team noted time frames
from a matter of days up to several years from the disclosure of the issue to the
manufacturer to the release of an update. On the other hand, if the flaw was re-
ported publicly (public disclosure), it was typically fixed in matter of few hours up
to few weeks.
Change control and configuration management are critical components of any
effective security program. These define the process for managing and monitoring
changes to the configuration of the operational environment. Clear separation of
duties in relation to updating and configuring the system and strong access control
for making those critical changes are needed. No change should happen without a
process, and all changes should be clearly tracked. These preparations will enable
the use of metrics related to change control:15
• Number of changes per period (number) measures the amount of periodic
change made to the production environment.
• Change control exemptions per period (number, percentage) shows how
often special exceptions are made for rushing through changes.
• Change control violations per period (number, percentage) shows how often
change control rules are willfully violated or ignored.
From a security perspective, special attention is paid to deploying security
updates, patches, workarounds, and other configuration changes related to secu-
rity. To be prepared for the abovementioned exemptions, and even violations in
cases of a crisis, are critical when a critical security update needs to be deployed in
118
Fuzzing Metrics
14M. Laakso, A. Takanen, J. Röning. “The Vulnerability Process: A Tiger Team Approach to
Resolving Vulnerability Cases.” In Proceedings of the 11th FIRST Conference on Computer Secu-
rity Incident Handling and Response. Brisbane, Australia. June 13–18, 1999.
15Andrew Jaquith. (2007). Security Metrics: Replacing Fear, Uncertainty, and Doubt. (pp.
72–73). Boston: Addison-Wesley.
a matter of hours from its issuance by the relevant vendor. A strict environment
that is not ready for immediate updating can be vulnerable to attacks until these
protective measures are in place.
Regression testing, or patch verification, is a critical metrics for patch develop-
ment and deployment. Fuzzing needs to be repeatable. This is why most fuzzing
tools pay significant attention to the repeatability of each test. An important aspect
of testing in QA is being able to perform regression tests to ensure you are not caus-
ing new bugs because of issuing fixes to old problems. A pseudo-random fuzzer
does a great job of this by providing a seed value when you initiate fuzz testing.
Other fuzzers use or create static test cases that can be used for regression tests.
Note that the repeatability with random testing needs to take into account the
changes in the tested interface. Communication interfaces are constantly updated as
new protocol specifications emerge. A traditional random fuzzer typically takes two
4.2
Transition to Proactive Security
119
Figure 4.3
Disclosure process and milestones used in disclosure metrics.
seeds: the data seed (e.g., a network capture) and the seed for the randomization
process. If either one of these changes, you will have a different set of tests on your
hands, and you cannot thoroughly verify the vendor-issued patch with the new
modified test setup. In model-based testing, these two seeds are not needed, but
instead the tests are built from an interface specification. The generated tests remain
the same, until an eventual change in the used protocol specification will necessitate
a change in the tests produced by model-based fuzzing test tool.
4.3
Defect Metrics and Security
Measuring the security of software-based solutions is a difficult task, because secu-
rity is a hidden property only visible when individual vulnerabilities are uncovered.
Through different vulnerability assessment methodologies and testing techniques,
we are able to get some visibility into the true security of a product or a service. The
challenging part is turning those results into something measurable. We can apply
some selected metrics from the quality assurance side or from the security field to
summarize our findings into something that is easier to grasp even by someone who
does not understand the technology that well. Fortunately, some methods of
fuzzing, and especially robustness testing, are very useful techniques in providing
more insight in this type of measurement. Interestingly, the same metrics can also
be used to assess the efficiency of the testing technique itself. To understand this
better, let us first have a look at some useful metrics in software security.
The first challenge we encounter is how to define the software or the system
being assessed. The simplest metrics we need to collect are related to the software
itself and are drawn from white-box tools. These techniques are based on the source
code and include metrics such as the number of lines of code, branches, modules,
and other measurable units. The software also needs to interact with the outer
world to be useful, and therefore, looking at the software from a black-box per-
spective gives us a new set of metrics. These include available services, open net-
work ports or sockets, wireless interfaces, and user interfaces.
The term attack surface has been used to indicate the subset of the complete
application that can be accessed by attackers. Several different metrics are used to
define attack surfaces. From the top down, a network-topology-based attack sur-
face can identify which networked computers are susceptible to attacks by non-
trusted parties such as local users or by anyone with access to the Internet. When
the hosts have been identified, the analysis can be extended to the network services
on those hosts that are accessible from the open network or through other remote
or local interfaces. Finally, various code coverage metrics can be used against each
identified service for studying which part of the exposed applications can be
accessed through those interfaces. Measuring the attack surface of an individual
piece of software is similar to measuring the code coverage of a black-box test,
although there are challenges in simulating the code into revealing all possible paths
that it can take during normal use. But, in short, an attack surface indicates which
portions of the code an attacker can potentially misuse, namely all code that can be
accessed through various local or remote interfaces, and which is executed during
the tests.
120
Fuzzing Metrics
Various metrics related to known defects are commonly used in both QA and
VA. In QA, the track record of known issues gives an indication of the overall qual-
ity while regression testing is used to verify that those flaws are not reintroduced
into the software. In network or host security auditing, vulnerability scanners can
be used to verify that all known issues are resolved from the deployed systems.
Defect counting can be based on either externally or internally found defects.
Although software developers can see both metrics, end users such as enterprises
and system integrators often have to rely on the publicly known defects only, unless
they have a good service agreement with vendors and can access such defect met-
rics from their suppliers. Without a database of publicly known vulnerabilities, an
enterprise often contracts a third-party security auditor to conduct an assessment
and to estimate an assurance level for the product. The details of such an assessment
would most probably be internal to the contractor, but the reports that result from
it can give an estimate of the number of vulnerabilities that a potential attacker
without prior knowledge on the system might find.
We will now walk through some of these metrics in more detail, with a focus
on how they can be used to assess the efficiency of the fuzzer itself, and as a result,
the efficiency of the test and the resulting tested system.
4.3.1
Coverage of Previous Vulnerabilities
Perhaps the most media-attractive metric of security is based on publicly known
security mistakes. A piece of software that makes the headlines on a weekly basis is
often perceived as less secure than another piece of software with less-known flaws.
Although often used, this is not a very good metric for security, since the existence
of more known vulnerabilities in one product can result simply from its popularity
and from how many researchers are looking for problems in it at any given time.
But is this a useful metric in relation to fuzzers? When various past security inci-
dents are analyzed, today we can quite often see that more and more of them are
actually found using automated security test tools such as fuzzers. Various security
consultants also find some of these flaws in their proprietary and typically nonau-
tomated techniques.
Metrics based on the coverage of previously known vulnerabilities are very sim-
ilar to the metrics related to regression testing in the QA field and similar to the
assessment of the efficiency of vulnerability scanners. The same metric can be argued
to apply as a metric for software security, but that can be easily countered by a sim-
ple comparison of a widely used web server and a small internally used web server.
If the widely used web server has hundreds of known security issues and the pro-
prietary web server has none, the result of such a comparison would indicate that
the proprietary web server is more secure. This is not necessarily true. To really
assess the security of the proprietary web server, one should apply all the same secu-
rity tests that were used to the widely used server, and based on those test results,
try to come to a security verdict. Fuzzers are probably the first available tools that
can create such a metric. And the same metric can be used to assess the efficiency
of the fuzzer itself.
Some people think that fuzzing can only find certain types of vulnerabilities—
i.e., relatively simple memory corruption bugs. This again is not true. You can ana-
4.3
Defect Metrics and Security
121
lyze this by looking at past vulnerabilities and thinking about how to trigger those
vulnerabilities with black-box testing tools. The steps to conduct such an analysis
include the following phases:
1. Take any communication interface and find all known vulnerabilities that
have been reported in any implementations of it.
2. For each found known issue, find out what the exact input (packet, mes-
sage sequence, field inside a file format) that triggers it.
3. For each known issue, identify how the problem could be detected by mon-
itoring the target system if the trigger was sent to it.
4. Map these triggers to the test coverage of black-box testing tools such as
fuzzers.
The results of such a comparison can be challenging to analyze. Certainly, some
fuzzing tools will catch different flaws better than others. Purely random fuzzers
would only catch simple flaws, and more intelligent fuzzers should catch most of
the flaws in such a study. Unfortunately, no matter what fuzzing approach you use,
mapping the generated test cases to known vulnerabilities is a tedious task, and that
is why most fuzzers have separated these two testing goals. A test for known vul-
nerabilities, whether those tests are integrated into a fuzzer, is very similar to tradi-
tional vulnerability scanning used in Nessus, for example.
One approach that fuzzers take to cover most of the security issues is to actu-
ally walk through all possible values in a specific protocol. If, for example, a non-
printable character in front of a long overflowing text string disturbs an ASCII-based
protocol, a fuzzer would systematically (or randomly) attempt all of the known
characters (0x00-0xFF) in conjunction with that string. Fuzzers have used this sim-
ple approach since at least 1999. This group of tests will then contain a number of
known vulnerabilities in unrelated implementations of the same communication
interface. For example, if we take a test group called “SIP Method” in the PROTOS
c07-sip test suite,16 we can see that the test group contains 193 test cases that test
various elements inside the SIP Method part of the protocol. The test group has
found tens of different flaws in various products, but does not attempt to identify
individual test cases for each of those flaws. The entire group should be used for
regression testing instead of one single test case.
Optimizing tests in fuzzing requires understanding of the actual flaws behind
the vulnerability. Instead of adding more brute force or randomness, we need to
study if someone has actually looked at the bug to see what caused this behavior.
For example, let us assume a bug is triggered by a special character such as comma
“,” inside or at the beginning of a string. Understanding if the programmer used
some comma-separated lists internally will help to optimize such tests. Or maybe a
proprietary pattern matching algorithm triggered the bug? Would "a,"
also cause the bug? Should we also test for "," and ",a" and ","? Should we also try other delimiters? The challenge
in stating that a fuzzer fully covers a vulnerability is that there is no clear definition
of covering a vulnerability. There are always variants to each bug. One of the pur-
122
Fuzzing Metrics
16www.ee.oulu.fi/research/ouspg/protos/testing/c07/sip/index.html
poses for known vulnerability coverage is to assess if that and similar flaws are
tested for. Another purpose is to really assess that the vulnerability was truly fixed.
Let us look at this from another point of view: security scanners. There are two
types of security scanners: aggressive and passive. An aggressive security scanner
will send the actual “exploits” to the system under test. For aggressive security
scanners, you would only need one single test case to cover a known vulnerability.
If the vulnerability exists, the system will crash or execute a harmless payload script.
Note that most commercial vulnerability scanners are passive tools so that they do
not disturb the system under test. They do not actually test for the vulnerability at
all, but only passively check the version of the software under test and make the ver-
dict based on the response to that passive request. Passive scanning is mandatory
for most vulnerability scanners because such a test will not result in the crash of the
actual service. This type of passive test would be unacceptable for a fuzzer and
would defeat the purpose of fuzzing.
Known vulnerability metrics can be argued to be meaningless for fuzzers. But
still, for those fuzzers that do such comparison, there are several levels of coverage
against known vulnerabilities:
0 Not covered or passive check.
1 Covered with single test case (aggressive vulnerability scanner behavior).
2 Covered with a range of optimized tests (e.g., a subset of all commonly
used delimiter characters).
3 Covered with a full range of inputs (e.g., going through all byte values for
a 16-bit, 32-bit, or 64-bit memory address).
It is important to note that, although a fuzzer could reach a high ranking based
on a metric against known vulnerabilities, it can still miss a lot of problems in areas
that have not been tested by anyone before. For example, countless people have
fuzzed the most common HTTP implementations (such as the Apache web server)
using all available tools. This does not necessarily mean that Apache is free of bugs.
Although the tolerance to most fuzzers is higher, people still manage to find issues
in such products with new fuzzer tools. There is no such thing as vulnerability-free
software. All software fails with fuzzing. It just depends on the quality of your
fuzzer and if you find the new categories of problems before someone else finds
them. That is the first reason why metrics based on known vulnerabilities do not
work for fuzzers.
Another challenge is that today many security companies are completely against
any public disclosure of vulnerabilities in their customers’ products. Most probably,
the product vendors will never disclose any of the problems they find internally
either. That is the second reason why this metric is not very suitable for fuzzers:
Known vulnerabilities do not reflect the true vulnerabilities in any given product.
As a summary, security can again be divided into proactive testing and reactive
testing, and metrics related to known vulnerabilities only apply to reactive security.
A vulnerability scanner such as Nessus is always reactive—i.e., it tests for known
issues. Vulnerability scanners have their place in network auditing, but not as a zero-
day detection device. These tools are based on a library of attacks or fingerprints, and
they will verify that those exact attacks are secured against. Unfortunately, if security
4.3
Defect Metrics and Security
123
testing is performed using reactive tools, it means that the first variant of that attack
will again pass through the security perimeter. For example, suppose an IPS is pro-
tecting a web server that crashes when a string of 1,000 A’s is sent. If the IPS filter
algorithm were just looking for 1,000 A’s, 1,000 B’s would avoid the filter and
crash the web server via the same memory corruption attack vector (in this case a
standard and contrived buffer overflow). Likely, the IPS filtering method will be
more complex and will clip the request at a certain number of bytes as opposed to
the value of those bytes. But the point is still made; clever attackers can sometimes
bypass filters. IPS systems need to be tested as much as any other system, but the
focus might be less on memory corruption and more on functionality in this case
since proper function helps ensure the security of protected systems. That is why
proactive tools like smart fuzzers will test the interface more systematically. In the
earlier example, a fuzzer would always have a large set of tests for the browser
name, as different web-based products and applications will crash with different
length strings. The firewall has to be tested with the entire suite of tests for any
meaningful results.
4.3.2
Expected Defect Count Metrics
Defect count metrics depend on where in the development cycle the testing is per-
formed. Fuzzers that are developed in-house and unleashed on a beta release will
most likely find an enormous number of bugs. On the other hand, fuzzing a mature
product such as Apache or the Microsoft IIS web server today will likely yield none.
One estimation metric for found defects is based on the past history of a fuzzer.
Expected defect count is based on the number of tests that are executed and the
probability of each test finding a defect.
Expected number bugs = Number of tests * Probability of finding 
a defect per test