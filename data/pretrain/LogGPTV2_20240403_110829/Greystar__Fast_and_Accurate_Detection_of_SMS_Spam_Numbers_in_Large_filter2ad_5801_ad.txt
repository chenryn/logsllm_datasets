θα(1 −θ)β
,
k(cid:30)Γ(k +α)Γ(n −k +β)
Γ(n +α+β)
where Γ is the gamma function. Therefore, the random
variable k follows a Beta-Binomial distribution:
P(k|n,α,β) =(cid:31)n
Γ(α+β)
Γ(α)Γ(β)
The target selection process of legitimate users can be
expressed using the same process. Because legitimate
users tend to communicate less with grey numbers, their
correspondingθ∗’s are usually much smaller. Let α∗ and
β∗ be the parametrization of the Beta distribution asso-
ciated with θ∗. For a phone number that has accessed n
targets, out of which k are grey numbers, we classify it
as a spam number (i.e., detect spamnbr returns 1) if
P(spammer|k,n)
P(legitimate|k,n)
where the ﬁrst equation is derived using the Bayes theo-
rem. It is equivalent to
P(k|n,α,β)
P(k|n,α∗,β∗)
P(k|n,α,β)P(spammer)
P(k|n,α∗,β∗)P(legitimate)
P(legitimate)
P(spammer)
> 1,
=
>
= η
6.3 Parameter Selection
There are ﬁve parameters to be estimated in the classiﬁer,
ˆα, ˆβ, ˆα∗, ˆβ∗ and η. We use the data from January 2012 to
determine these parameters. To obtain ground truth, we
submit to the fraud agents a list of all the SMS senders
that i) have sent to more than 50 recipients in a 24 hour
time window; and ii) at least one of the recipients is grey
(recall the ﬁltering criteria in Algorithm 1). Fraud agents
carry out investigation on these numbers for us and label
spam numbersin the list. We then divide the January data
into two subsets, the ﬁrst two weeks of data for ﬁtting
the Beta-binomial models (i.e., to determine the ﬁrst four
parameters) and the rest of data is reserved for testing the
classiﬁer to estimate η.
In particular, using the training data set, we estimate
the parameters for two Beta-binomial models using max-
imum likelihood estimation. With the estimated pa-
rameters, we illustrate the probability density function
θ ∼ Beta(α,β) and θ∗ ∼ Beta( ˆα∗, ˆβ∗) in Fig. 10. The
density functions agree with our previous observations
in Fig. 9. The mass of the probability function corre-
sponding to the legitimate users concentrates on a narrow
region close to 0, implying that legitimate users commu-
nicate much less with grey numbers than non-grey num-
bers. In contrast, the density associated with spam num-
bers widely spreads out, indicating more grey numbers
are touched by spam numbers due to their random target
selection strategies.
We evaluate the accuracy of the classiﬁer given differ-
ent choices of η on the test data set and the Receiver
Operating Characteristic (ROC) curve is displayed in
Fig. 11. The x-axis represents the false alarm rate (or
the false positive rate) and the y-axis stands for the true
detection rate (or the true positive rate). From Fig. 11,
with a certain η, Greystar can detect more than 85%
spam numbers without producing any false alarm. We
will choose this ηvalue in the rest of our experiments9.
7 Greystar Evaluation
In practice, it is usually unclear how many spammers are
in the network, therefore, to estimate η directly is chal-
lenging. We instead choose ηthrough experiments.
8In Bayesian inference, the Beta distribution is the conjugate prior
probability distribution for the Bernoulli and binomial distributions. In-
stead of using the Bernoulli model, we can model the second stage of
the target selection process as sampling from a multinomial distribu-
tion corresponding to different device types. In this case, the conjugate
prior distribution of the multinomial parameters is the Dirichlet distri-
bution. However, our preliminary experiments show little performance
gain from applying the more sophisticated model in comparison to the
increased computation cost.
In this section, we conduct an extensive evaluation of
Greystar using ﬁve months of CDR data and compare it
with the methods based on victim spam reports in terms
of accuracy, detection delay and the effectiveness in re-
ducing spam trafﬁc in the network.
9Note that the exact parameter values used in Greystar are propri-
etary and we are not able to release them in the paper. We have also
tested the choice of η using different partitioning of the training/test
data. The ηremains stable across experiments.
USENIX Association  
22nd USENIX Security Symposium  11
11
0
.
1
8
.
0
e
t
6
.
0
4
.
0
a
r
e
v
i
t
i
s
o
p
e
u
r
T
2
.
0
0
.
0
0.0
0.2
0
0
0
0
1
0
0
0
8
0
0
0
6
0
0
0
4
s
N
T
f
o
r
e
b
m
u
N
Confirmed
Missed
Additional
0
.
1
8
.
0
Report 1+
Report 3+
F
D
C
6
.
0
4
.
0
0
0
0
2
0
2
.
0
0
.
0
0.8
1.0
Jan
Feb
Mar
Apr
May
1
2
5
10
50 100
Delay (hours in log scale)
20
500
0.4
0.6
False positive rate
Figure 12: Accuracy evaluation (in
comparison to victim spam reports).
Figure 13: Detection speed compared
to spam report based methods.
Total
Report 1+
Greystar
50
100
150
Time (hours) in a week
a
2
1
)
d
e
z
a
0
1
i
l
a
m
r
o
n
(
s
g
s
m
m
a
p
s
#
a
8
a
6
a
4
a
2
0
0
Figure 11: ROC curve (false positive
rate vs. true positive rate.
7.1 Accuracy Evaluation
To estimate the accuracy and the false alarm rate, we
again consult with the fraud agents to check the num-
bers from Greystar detection results. False negatives (or
missed detections), on the other hand, are more difﬁcult
to identify. Given the huge number of negative examples
classiﬁed, we are unable to have all of them examined by
the fraud agents to identify all missed detections because
of the high manual investigation cost. As an alternative
solution, we compare Greystar detection results with vic-
tim spam reports to obtain a lower bound estimate of the
missed detections.
More formally, let Sg denote the detection results from
Greystar and Sc be the spam numbers contained in the
victim spam reports received during the same time pe-
riod. We deﬁne missed detections of Greystar as Sc −Sg.
In addition, we deﬁne additional detections of Greystar
as Sg − Sc to measure the value brought by Greystar to
the existing spam defense solution. The monthly accu-
racy evaluation results are displayed in Fig. 12.
The blue bars in Fig. 12 illustrate the spam numbers
validated by fraud agents in each month. Greystar is able
to detect thousands of spam numbers per month. The as-
cending trend of detected spam numbers coincides with
the increase of victim spam reports in the ﬁve-month ob-
servation window. This implies that Greystar is able to
keep up with the increase of spam activities. In addition
to the large number of true detections, Greystar is highly
accurate given only two potential false alarms are identi-
ﬁed by fraud agents in 5 months. Interestingly, these two
numbers are associated with tenured smartphone users
who suddenly behave abnormally and initiate SMS mes-
sages to many recipients whom they have never commu-
nicated with in the past. We suspect these users have
been infected by SMS spamming malware that launch
spam campaigns from the users’ devices without their
consent. To identify SMS spamming malware and hence
Figure 14: Number of spam messages after restriction.
removing such false alarms will be our future work.
In comparison to the victim spam reports, Greystar
detects over 1000 addition spam numbers that were not
reported by spam victims while missing less than 500
monthly. Meanwhile, although a majority of the spam
numbers detected by Greystar are also reported by spam
victims, Greystar can detect these numbers much faster
than methods based on victim reports, and consequently
can suppress more spam messages in the network. We
illustrate this point in the next section.
7.2 Detection Speed and Beneﬁts to Cellu-
lar Carriers
We note that, to reduce noise, cellular carriers often rely
on multiple spam reports (e.g., K reports) from different
victims to conﬁrm a spam number. We refer to such a
crowdsourcing method as the K+ algorithm. To evaluate
the speed of Greystar, we compare it with two versions of
the K+ algorithms, namely, 1+ and 3+. Comparingwith
12
12  22nd USENIX Security Symposium 
USENIX Association
1+ supplies us with the lower bound of the time differ-
ence and comparison with 3+ illustrates the real beneﬁt
brought by Greystar to practical spam defense solutions.
More speciﬁcally, we measure how many hours Greystar
detects a spam number ahead of 1+ and 3+, respectively.
Fig. 13 shows the CDF curves of the comparison results,
where we highlightthe location on the x-axis correspond-
ing to 24 hourswith a green verticalline. We observe that
Greystar is much faster than K+ algorithms. For exam-
ple, Greystar is one day ahead of 1+ in 50% of the cases
and is one day before 3+ in more than 90% of the times.
We ﬁnd that, on average, it takes less than 1.2 hours
for Greystar to detect a spam number after it starts spam-
ming (i.e., starts sending messages to more than 50 vic-
tims in an hour). The fast response time of Greystar is
accredited to the much larger population of grey num-
bers, from which Greystar can gather evidence to detect
more spam numbers more quickly. In addition, collect-
ing evidence passively from grey numbers eliminates the
delay during the human reporting process (recall Fig. 2).
Therefore, Greystar is characterized with a much faster
detection speed than the K+ algorithm. Such a gain in
the detection speed can lead to more successful reduction
of spam trafﬁc in the network. We illustrate this point
next.
For simplicity, we assume a spam number can be in-
stantly restricted after being detected. We run simulation
on a one week dataset (the ﬁrst week of January 2012)
and calculate the number of spam messages appearing
in each hour assuming a particular spam detection algo-
rithm is deployed exclusively in the network. The results
are illustrated in Fig. 14. The total spam messages are
contributed by known spam numbers observed in that
week. We observe that Greystar can successfully sup-
press the majority of spam messages. During peak hours
when the total number of spam messages exceeds 600K,
only around 150K remains after Greystar is deployed. In
other words, Greystar leads to an overall reduction of
75% of spam messages during peak hours. In compar-
ison, 1+ only guarantees a spam reduction of 50% due
to long detection delay. We note that, due to the noise
in the spam reports, cellular providers often employ K+
(K ≥ 3) instead of 1+ to avoid false alarms. In this case,
the beneﬁt from Greystar is even more substantial.
7.3 Analysis of Missed Detections
In this section, we investigate the missed detections
(false negatives) from Greystar, i.e., the spam number
candidates that were not detected by Greystar but have
been reported by spam victims. There are around 500
such numbers in each month and totally around 27K
missed detections. We note that we focus only on a sub-
set of the candidates who are customers of the cellular
network under study, for whom we have access to a much
richer set of information sources to carry out the inves-
tigation. We believe the conclusions from analyzing this
subset of candidates also apply for other candidates out-
side the network.
We classify these candidates into three groups based
on the volume of the associated CDR records.
No volume. We do not observe any CDR record for
19.5% of the numbers. We inquiry the SMS billing