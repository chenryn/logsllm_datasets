title:Evaluating SFI for a CISC Architecture
author:Stephen McCamant and
Greg Morrisett
Evaluating SFI for a CISC Architecture
Stephen McCamant
Massachusetts Institute of Technology
Computer Science and AI Lab
Cambridge, MA 02139
PI:EMAIL
Greg Morrisett
Harvard University
Division of Engineering and Applied Sciences
Cambridge, MA 02138
PI:EMAIL
Abstract
Executing untrusted code while preserving security re-
quires that the code be prevented from modifying mem-
ory or executing instructions except as explicitly al-
lowed. Software-based fault isolation (SFI) or “sandbox-
ing” enforces such a policy by rewriting the untrusted
code at the instruction level. However, the original sand-
boxing technique of Wahbe et al. is applicable only to
RISC architectures, and most other previous work is ei-
ther insecure, or has been not described in enough detail
to give conﬁdence in its security properties. We present a
new sandboxing technique that can be applied to a CISC
architecture like the IA-32, and whose application can
be checked at load-time to minimize the TCB. We de-
scribe an implementation which provides a robust secu-
rity guarantee and has low runtime overheads (an average
of 21% on the SPECint2000 benchmarks). We evaluate
the utility of the technique by applying it to untrusted de-
compression modules in an archive tool, and its safety by
constructing a machine-checked proof that any program
approved by the veriﬁcation algorithm will respect the
desired safety property.
1
Introduction
Secure systems often need to execute a code module
while constraining its actions with a security policy. The
code might come directly from a malicious author, or it
might have bugs that allow it to be subverted by mali-
ciously chosen inputs. The system designer chooses a set
of legal interfaces for interaction with the code, and the
challenge is to ensure that the code’s interaction with the
rest of the system is limited to those interfaces. Software-
based fault isolation (SFI) implements such isolation via
instruction rewriting, but previous research left the prac-
ticality of the technique uncertain. The original SFI tech-
nique works only for RISC architectures, and much fol-
lowup research has neglected key security issues. By
contrast, we ﬁnd that SFI can be implemented for the
x86 with runtime overheads that are acceptable for many
applications, and that the technique’s security can be
demonstrated with a rigorous machine-checked proof.
The most common technique for isolating untrusted
code is the use of hardware memory protection in the
form of an operating system process. Code in one pro-
cess is restricted to accessing memory only in its address
space, and its interaction with the rest of a system is lim-
ited to a predeﬁned system call interface. The enforce-
ment of these restrictions is robust and has a low over-
head because of the use of dedicated hardware mecha-
nisms such as TLBs; few restrictions are placed on what
the untrusted code can do. A disadvantage of hardware
protection, however, is that interaction across a process
boundary (i.e., via system calls) is coarse-grained and
relatively expensive. Because of this inefﬁciency and in-
convenience, it is still most common for even large ap-
plications, servers, and operating system kernels to be
constructed to run in a single address space.
A very different technique is to require that the un-
trusted code be written in a type-safe language such as
Java. The language’s type discipline limits the mem-
ory usage and control ﬂow of the code to well-behaved
patterns, making ﬁne-grained sharing of data between
trusted and untrusted components relatively easy. How-
ever, type systems have some limitations as a security
mechanism. First, unlike operating systems, which are
generally language independent, type system approaches
are often designed for a single language, and can be hard
to apply to at all to unsafe languages such as C and C++.
Second, conventional type systems describe high-level
program actions like method calls and ﬁeld accesses. It
is much more difﬁcult to use a type system to constrain
code at the same level of abstraction as individual ma-
chine instructions; but since it is the actual instructions
that will be executed, only a safety property in terms of
them would be really convincing.
This paper investigates a code isolation technique that
USENIX Association
Security ’06: 15th USENIX Security Symposium
209
void f(int arg,
int arg2,
int arg3, int arg4) {
return;
}
void poke(int *loc, int val) {
int local;
unsigned diff = &local - loc - 2;
for (diff /= 4; diff; diff--)
alloca(16);
f(val, val, val, val);
}
Figure 1: Example attack against SFI systems which depend
on the compiler’s management of the stack for safety. The
function poke modiﬁes the stack pointer by repeatedly allo-
cating unused buffers with alloca until it points near an arbi-
trary target location loc, which is then overwritten by one of
the arguments to f. MiSFIT [25] and Erlingsson’s x86 SASI
tool [10] allow this unsafe write because they incorrectly as-
sume that the stack pointer always points to the legal data re-
gion.
lies between the approaches mentioned above, one that
enforces a security policy similar to an operating system,
but with ahead-of-time code veriﬁcation more like a type
system. This effect is achieved by rewriting the machine
instructions of code after compilation to directly enforce
limits on memory writes and control ﬂow. This class
of techniques is known as “software-based fault isola-
tion” (SFI for short) or “sandboxing” [27]. Previous SFI
techniques were applicable only to RISC architectures,
or their treatment of key security issues was incomplete,
faulty, or never described publicly. For instance, several
previous systems [25, 10] depended for their safety on
an assumption that a C compiler would manage the stack
pointer to keep the untrusted code’s stack separate from
the rest of memory. As shown in the example of Fig-
ure 1, this trust is misplaced, not just because compilers
are large and may contain bugs, but because the safety
guarantees they make are loosely speciﬁed and contain
exceptions.
(Concurrently with the research described
here, some other researchers have also developed SFI-
like systems that include more rigorous security analy-
ses; see Section 10 for discussion.)
Many systems programming applications can beneﬁt
from a code isolation mechanism that is efﬁcient, robust,
and easily applicable to existing code. A useful tech-
nique to improve the reliability of operating systems is to
isolate device drivers so that their failures (which may in-
clude arbitrary memory writes) do not corrupt the rest of
a kernel. The Nooks system [26] achieves such isolation
with hardware mechanisms that are robust, but impose
a high overhead when many short cross-boundary calls
are made; SFI could provide similar protection without
high per-call overheads. To reduce the damage caused
by attacks on network servers, they should be designed
to minimize the amount of code that requires high (e.g.,
root) privileges; but retroﬁtting such a design on an exist-
ing server is difﬁcult. Dividing servers along these lines
by using separate OS-level processes [23, 14] is effective
at preventing vulnerabilities, but is far from trivial be-
cause of the need to serialize data for transport and pre-
vent an untrusted process from making damaging system
calls. SFI could make such separation easier by auto-
matically preventing system calls and making memory
sharing more transparent. Section 8 discusses VXA [11],
an architecture that ensures compressed archives will be
readable in the future by embedding an appropriate de-
compressor directly in an archive. Applying our SFI tool
to VXA we see that it very easily obtains a strong se-
curity guarantee, without imposing prohibitive runtime
overheads. Note that all of these examples include large
existing code bases written in C or C++, which would be
impractical to rewrite in a new language; the language
neutrality of SFI techniques is key to their applicability.
This paper:
• Describes a novel SFI technique directly applicable
to CISC architectures like the Intel IA-32 (x86), as
well as two optimizations not present in previous
systems (Sections 3 and 4).
• Explains how using separate veriﬁcation, the secu-
rity of the technique depends on a minimal trusted
base (on the order of a thousand lines of code),
rather than on tools consisting of hundreds of thou-
sands of lines (Section 5).
• Analyzes in detail the performance of an implemen-
tation on the standard SPECint2000 benchmarks
(Section 7).
• Evaluates the implementation as part of a system to
safely execute embedded decompression modules
(Section 8).
• Gives a machine-checked proof of the soundness
of the technique (speciﬁcally, of the independent
safety veriﬁer) to provide further evidence that it is
simple and trustworthy (Section 9).
We refer to our implementation as the Prototype IA-
32 Transformation Tool for Software-based Fault Iso-
lation Enabling Load-time Determinations (of safety),
or PittSFIeld1. Our implementation is publicly avail-
able (the version described here is 0.4), as are the for-
mal model and lemmas used in the machine-checked
proof. They can be downloaded from the project web site
at http://pag.csail.mit.edu/˜smcc/projects/
pittsfield/.
1Pittsﬁeld, Massachusetts, population 45,793, is the seat of Berk-
shire county and a leading center of plastics manufacturing. Our ap-
propriation of its name, however, was motivated only by spelling.
210
Security ’06: 15th USENIX Security Symposium
USENIX Association
2 Classic SFI
The basic task for any SFI implementation is to prevent
certain potentially unsafe instructions (such as memory
writes) from being executed with improper arguments
(such as an effective address outside an allowed data
area). The key challenges are to perform these checks ef-
ﬁciently, and in such a way that they cannot be bypassed
by maliciously designed jumps in the input code. The
ﬁrst approach to solve these challenges was the original
SFI technique (called “sandboxing”) of Wahbe, Lucco,
Anderson, and Graham [27]. (The basic idea of rewrit-
ing instructions for safety had been suggested earlier, no-
tably by Deutsch and Grant [7], but their system applied
to code fragments more limited than general programs.)
In order to efﬁciently isolate pointers to dedicated
code and data regions, Wahbe et al. suggest choosing
memory regions whose size is a power of two, and whose
starting location is aligned to that same power. For in-
stance, we might choose a data region starting at the
address 0xda000000 and extending 16 megabytes to
0xdaffffff. With such a choice, an address can be
efﬁciently checked to point inside the region by bitwise
operations. In this case, we could check whether the bit-
wise AND of an address and the constant 0xff000000
was equal to 0xda000000. We’ll use the term tag to re-
fer to the portion of the address that’s the same for every
address in a region, such as 0xda above.
The second challenge, assuring that checks cannot be
bypassed, is more subtle. Naively, one might insert
a checking instruction sequence directly before a po-
tentially unsafe operation; then a sequential execution
couldn’t reach the dangerous operation without passing
through the check. However, it isn’t practical to restrict
code to execute sequentially: realistic code requires jump
and branch instructions, and with them comes the dan-
ger that execution will jump directly to an dangerous in-
struction, bypassing a check. Direct branches, ones in
which the target of the branch is speciﬁed directly in the
instruction, are not problematic: a tool can easily check
their destinations before execution. The crux of the prob-
lem is indirect jump instructions, ones where the target
address comes from a register at runtime. They are re-
quired by procedure returns, switch statements, func-
tion pointers, and object dispatch tables, among other
language features (Deutsch and Grant’s system did not
allow them). Indirect jumps must also be checked to see
that their target address is in the allowed code region, but
how can we also exclude the addresses of unsafe instruc-
tions, while allowing safe instruction addresses?
The key contribution of Wahbe et al. was to show that
by directing all unsafe operations through a dedicated
register, a jump to any instruction in the code region
could be safe. For instance, suppose we dedicate the reg-
ister %rs for writes to the data area introduced above.
Then we maintain that throughout the code’s execution,
the value in %rs always contains a value whose high bits
are 0xda. Code can only be allowed to store an arbi-
trary value into %rs if it immediately guarantees that
the stored value really is appropriate. If we know that
this invariant holds whenever the code jumps, we can see
that even if the code jumps directly to an instruction that
stores to the address in %rs, all that will occur is a write
to the data region, which is safe (allowed by the secu-
rity policy). Of course, there is no reason why a correct
program would jump directly to an unsafe store instruc-
tion; the technique is needed for incorrect or maliciously
designed programs.
Wahbe et al. implemented their technique for two
RISC architectures, the MIPS and the Alpha. Because
memory reads are more common than writes and less
dangerous, their implementation only checked stores and
not loads, a tradeoff that has also been made in most
subsequent work, including ours.
(In the experiments
in [25], adding protection for out-of-bounds reads often
more than doubled overhead compared to checking only
writes and jumps.) Because separate dedicated registers
are required for the code and data regions, and because
constants used in the sandboxing operation also need to
be stored in registers, a total of 5 registers are required;
out of a total of 32, the performance cost of their loss
was negligible. Wahbe et al. evaluated their implemen-