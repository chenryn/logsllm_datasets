busy server CPU, application bugs, network routing
issues, etc. We also note that packet drops increase
user perceived latency, since dropped packets need to
be retransmitted. Packet drops may happen at diﬀer-
ent places due to various reasons, e.g., ﬁber FCS (frame
check sequence) errors, switching ASIC defects, switch
fabric ﬂaw, switch software bug, NIC conﬁguration is-
sue, network congestions, etc. We have seen all these
types of issues in our production networks.
2.3 Data center management and data pro-
cessing systems
Next we introduce Autopilot [20] and Cosmos and
SCOPE [15]. Data centers are managed by centralized
data center management systems, e.g., Autopilot [20] or
Borg [23]. These management systems provide frame-
works on how resources including physical servers are
managed, how services are deployed, scheduled, moni-
tored and managed. Pingmesh is built within the frame-
work of Autopilot.
Autopilot is Microsoft’s software stack for automatic
data center management. Its philosophy is to run soft-
ware to automate all data center management tasks,
including failure recovery, with as minimal human in-
volvement as possible. Using the Autopilot terminol-
ogy, a cluster, which is a set of servers connected by
a local data center network,
is managed by an Au-
topilot environment. An Autopilot environment has
Inter-DC networkDC1DC2DC3SpineLeafToRPodsetPodServersIntra-DCnetwork141the ping results in local memory. Once a timer times
out or the size of the measurement results exceeds a
threshold, the Pingmesh Agent uploads the results to
Cosmos for data storage and analysis. The Pingmesh
Agent also exposes a set of performance counters which
are periodically collected by a Perfcounter Aggregator
(PA) service of Autopilot.
Data Storage and Analysis (DSA). The latency
data from Pingmesh Agents are stored and processed
in a data storage and analysis (DSA) pipeline. La-
tency data is stored in Cosmos. SCOPE jobs are de-
veloped to analyze the data. SCOPE jobs are written
in declarative language similar to SQL. The analyzed
results are then stored in an SQL database. Visualiza-
tion, reports and alerts are generated based on the data
in this database and the PA counters.
3.3 Pingmesh Controller
3.3.1 The pinglist generation algorithm
The core of the Pingmesh Controller is its Pingmesh
Generator. The Pingmesh Generator runs an algorithm
to decide which server should ping which set of servers.
As aforementioned, we would like Pingmesh to have as
large coverage as possible. The largest possible coverage
is a server-level complete graph, in which every server
probes the rest of the servers. A server-level complete
graph, however, is not feasible because a server needs
to probe n− 1 servers, where n is the number of servers.
In a data center n can be as large as hundreds of thou-
sands. Also a server-level complete graph is not nec-
essary since tens of servers connect to the rest of the
world through the same ToR switch.
We then come up with a design of multiple level of
complete graphs. Within a Pod, we let all the servers
under the same ToR switch form a complete graph. At
intra-DC level, we treat each ToR switch as a virtual
node, and let the ToR switches form a complete graph.
At inter-DC level, each data center acts as a virtual
node, and all the data centers form a complete graph.
In our design, only servers do pings. When we say a
ToR as a virtual node, it is the servers under the ToR
that carry out the pings. Similarly, for a data center
as a virtual node, it is the selected servers in the data
center that launch the probings.
At the intra-DC level, we once thought that we only
need to select a conﬁgurable number of servers to par-
ticipate in Pingmesh. But how to select the servers
becomes a problem. Further, the small number of se-
lected servers may not well represent the rest of the
servers. We ﬁnally come up with the idea of letting all
the servers participate. The intra-DC algorithm is: for
any ToR-pair (ToRx, ToRy), let server i in ToRx ping
server i in ToRy. In Pingmesh, even when two servers
are in the pinglists of each other, they measure net-
work latency separately. By doing so, every server can
calculate its own packet drop rate and network latency
locally and independently.
Figure 2: Pingmesh architecture.
data when we run them. Second, the data they produce
does not have the needed coverage. Because these tools
are not always-on, we cannot count on them to track
the network status. These tools are usually used for
network troubleshooting when a source-destination pair
is known. This, however, does not work well for large-
scale data center networks: when a network incident
happens, we may not even know the source-destination
pair. Furthermore, for transient network issues, the
problem may be gone before we run the tools.
3.2 Pingmesh architecture
Based on its design goal, Pingmesh needs to meet
the requirements as follows. First, because Pingmesh
aims to provide the largest possible coverage and mea-
sure network latency from applications’ point of view,
a Pingmesh Agent is thus needed on every server. This
has to be done carefully so that the CPU, memory, and
bandwidth overhead introduced by Pingmesh Agent is
small and aﬀordable.
Second, the behavior of the Pingmesh Agent should
be under control and conﬁgurable. A highly reliable
control plane is needed to control how the servers should
carry out network latency measurement.
Third, the latency data should be aggregated, ana-
lyzed, and reported in near real-time and stored and
archived for deeper analysis. Based on the require-
ments, we have designed the architecture of Pingmesh
as illustrated in Figure 2. Pingmesh has three compo-
nents as we describe as follows.
Pingmesh Controller. It is the brain of the whole sys-
tem, as it decides how servers should probe each other.
Within the Pingmesh Controller, a Pingmesh Generator
generates a pinglist ﬁle for every server. The pinglist ﬁle
contains the list of peer servers and related parameters.
The pinglist ﬁles are generated based on the network
topology. Servers get their corresponding pinglist via a
RESTful Web interface.
Pingmesh Agent. Every server runs a Pingmesh Agent.
The Agent downloads the pinglist from the Pingmesh
Controller, and then launches TCP/HTTP pings to the
peer servers in the pinglist. The Pingmesh Agent stores
Pingmesh agentPingmeshgeneratorPinglist.xmlNetwork graphlogvipPingmesh agentlogServers Pingmesh ControllerWeb serviceCosmos StoreSCOPE JobsDatabaseVisuali-zationTcp/HttpprobingsData Storage and Analysis(DSA)Pingmesh AgentAlertvipPerfcounter Aggregator142At the inter-DC level, all the DCs form yet another
complete graph.
In each DC, we select a number of
servers (with several servers selected from each Podset).
Combining the three complete graphs, a server in
Pingmesh needs to ping 2000-5000 peer servers depend-
ing on the size of the data center. The Pingmesh Con-
troller uses threshold values to limit the total number of
probes of a server and the minimal time interval of two
successive probes for a source destination server pair.
3.3.2 Pingmesh Controller implementation
The Pingmesh Controller is implemented as an Au-
topilot service and becomes part of the Autopilot man-
agement stack. It generates Pinglist ﬁle for every server
by running the Pingmesh generation algorithm. The
ﬁles are then stored in SSD and served to the servers
via a Pingmesh Web service. The Pingmesh Controller
provides a simple RESTful Web API for the Pingmesh
Agents to retrieve their Pinglist ﬁles respectively. The
Pingmesh Agents need to periodically ask the Controller
for Pinglist ﬁles and the Pingmesh Controller does not
push any data to the Pingmesh Agents. By doing so,
Pingmesh Controller becomes stateless and easy to scale.
As the brain of the whole Pingmesh system, the Pingmesh
Controller needs to serve hundreds of thousands of Pingmesh
Agents. Hence the Pingmesh Controller needs to be
fault-tolerant and scalable. We use Software Load-Balancer
(SLB) [14] to provide fault-tolerance and scalability for
the Pingmesh Controller. See [9, 14] for the details of
how SLB works. A Pingmesh Controller has a set of
servers behind a single VIP (virtual IP address). SLB
distributes the requests from the Pingmesh Agents to
the Pingmesh Controller servers. Every Pingmesh Con-
troller server runs the same piece of code and gener-
ates the same set of Pinglist ﬁles for all the servers
and is able to serve requests from any Pingmesh Agent.
The Pingmesh Controller can then easily scale out by
adding more servers behind the same VIP. Also once a
Pingmesh Controller server stops functioning, it is auto-
matically removed from rotation by the SLB. We have
setup two Pingmesh Controllers in two diﬀerent data
center clusters to make the controller even more fault
tolerant geographically.
3.4 Pingmesh Agent
3.4.1 Pingmesh Agent design considerations
The Pingmesh Agent runs on all the servers. Its task
is simple: downloads pinglist from the Pingmesh Con-
troller; pings the servers in the pinglist; then uploads
the ping result to DSA.
Based on the requirement that Pingmesh needs to be
able to distinguish if a user perceived latency increase is
due to network or not, Pingmesh should use the same
type of packets generated by the applications. Since
almost all the applications in our data centers use TCP
and HTTP, Pingmesh uses TCP and HTTP instead of
ICMP or UDP for probing.
Because we need to diﬀerentiate if a ‘network’ issue is
because of the network or the applications themselves,
Pingmesh Agent does not use any network libraries used
by the applications.
Instead, we have developed our
own light-weight network library speciﬁcally designed
for network latency measurement.
The Pingmesh Agent can be conﬁgured to send out
and respond to probing packets of diﬀerent lengths,
other than the TCP SYN/SYN-ACK packets. As a re-
sult, the Pingmesh Agent needs to act as both client and
server. The client part launches pings and the server
part responds to the pings.
Every probing needs to be a new connection and
uses a new TCP source port. This is to explore the
multi-path nature of the network as much as possible,
and more importantly, reduce the number of concurrent
TCP connections created by Pingmesh.
3.4.2 Pingmesh Agent implementation
Though the task is simple, the Pingmesh Agent is one
of the most challenging part to implement. It must meet
the safety and performance requirements as follows.
First, the Pingmesh Agent must be fail-closed and
not create live-site incidents. Since the Pingmesh Agent
runs on every server, it has the potential to bring down
all the servers if it malfunctions (e.g., uses large portion
of CPU and memory resources, generates large volume
of probing traﬃc, etc.). To avoid bad things from hap-
pening, several safety features have been implemented
into the Pingmesh Agent:
• The CPU and maximum memory usages of the
Pingmesh Agent are conﬁned by the OS. Once
the maximum memory usage exceeds the cap, the
Pingmesh Agent will be terminated.
• The minimum probe interval between any two servers
is limited to 10 seconds, and the probe payload
length is limited to 64 kilobytes. These limits are
hard coded in the source code. By doing so, we
put a hard limit on the worst-case traﬃc volume
that Pingmesh can bring into the network.
• If a Pingmesh Agent cannot connect to its con-
troller for 3 times, or if the controller is up but
there is no pinglist ﬁle available, the Pingmesh
Agent will remove all its existing ping peers and
stop all its ping activities.
(It will still react to
pings though.) Due to this feature, we can stop
the Pingmesh Agent from working by simply re-
moving all the pinglist ﬁles from the controller.
• If a server cannot upload its latency data, it will
retry several times. After that it will stop trying
and discard the in-memory data. This is to en-
sure the Pingmesh Agent uses bounded memory
resource. The Pingmesh Agent also writes the la-
tency data to local disk as log ﬁles. The size of log
ﬁles is limited to a conﬁgurable size.
143Agent performs local calculation on the latency data
and produces a set of performance counters including
the packet drop rate, the network latency at 50th the
99th percentile, etc. All these performance counters are
collected and aggregated and stored by the PA service
of Autopilot.
Once the results are in Cosmos, we run a set of SCOPE
jobs for data processing. We have 10-min, 1-hour, 1-day
jobs at diﬀerent time scales. The 10-min jobs are our
near real-time ones. For the 10-min jobs, the time in-
terval from when the latency data is generated to when
the data is consumed (e.g., alert ﬁred, dashboard ﬁg-
ure generated) is around 20 minutes. The 1-hour and
1-day pipelines are for non real-time tasks including
network SLA tracking, network black-hole detection,
packet drop detection, etc. All our jobs are automati-
cally and periodically submitted by a Job Manager to
SCOPE without user intervention. The results of the
SCOPE jobs are stored in a SQL database, from which
visualization, reports, and alerts are generated.
In practice, we found this 20-minute delay works ﬁne
for system level SLA tracking. In order to further re-
duce response time, we in parallel use the Autopilot
PA pipeline to collect and aggregate a set of Pingmesh
counters. The Autopilot PA pipeline is a distributed de-
sign with every data center has its own pipeline. The PA
counter collection latency is 5 minutes, which is faster
than our Cosmos/SCOPE pipeline. The PA pipeline is
faster than Cosmos/SCOPE, whereas Cosmos/SCOPE
is more expressive than PA for data processing. By
using both of them, we provide higher availability for
Pingmesh than either of them.
We diﬀerentiate Pingmesh as an always-on service
from a set of scripts that run periodically. All the com-
ponents of Pingmesh have watchdogs to watch whether
they are running correctly or not, e.g., whether pinglists
are generated correctly, whether the CPU and mem-
ory usages are within budget, whether pingmesh data
are reported and stored, whether DSA reports network
SLAs in time, etc. Furthermore, the Pingmesh Agent is
designed to probe thousands of peers in a light-weight
and safe way.
All the Pingmesh Agents upload 24 terabytes latency
measurement results to Cosmos per day. This is more
than 2Gb/s upload rate. Though these look like large
numbers, they are only a negligible fraction of the total
capacity provided by our network and Cosmos.
4. LATENCY DATA ANALYSIS
In this section, we introduce how Pingmesh helps us
better understand network latency and packet drops,
deﬁne and track the network SLA, and determine if a
live-site incident is because of network issues or not. All
the data centers we describe in this section have similar