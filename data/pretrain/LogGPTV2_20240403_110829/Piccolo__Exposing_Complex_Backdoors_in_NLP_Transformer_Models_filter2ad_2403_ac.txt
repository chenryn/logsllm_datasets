sentences to a particular class, even for clean models. Second,
there is not an easy way to have a differentiable reduction
on trigger size. Third, when triggers of unknown lengths have
semantic meanings, like valid sentences, transformer models
tend to have more convoluted representations for them such
that inverting individual tokens unlikely ﬁnds the triggers.
Challenge IV. Generative Model Is Incapable of Generating
Complex Triggers. Appendix IX-A describes in details why
generative models cannot generate complex triggers.
V. DESIGN
A. Overview
Figure 4 presents the overall procedure of PICCOLO. From
left to right, given a transformer model M, it ﬁrst transforms
the model to an equivalent but differentiable form M(cid:2), which
features a word-level encoding scheme instead of the original
token-level encoding (Section V-B). The encoding makes it
amenable to word-level trigger inversion (Section V-D). The
inversion step takes the transformed model, a few clean
sentences, the target label, and generates a set of likely trigger
words. These likely trigger words are passed on to the trigger
validation step (Section V-E) to check if they can have a high
ASR in ﬂipping the clean sentences to the target label. If so,
the model is considered trojaned. PICCOLO does not aim to
invert the precise triggers especially for long phrase triggers.
Instead, it may only invert some words in the trigger, which
may be insufﬁcient to achieve a high ASR. PICCOLO hence
employs the word discriminativity analysis (Section V-C)
to check if the model is particularly discriminative for the
inverted words. If so, the model is considered trojaned.
Key Design Choices. PICCOLO has a number of key design
choices, addressing the challenges mentioned in Section IV.
As shown in Table I, to address the discontinuity problem
(Challenge I in Section IV), PICCOLO transforms a subject
model to its equivalent and differentiable form, and optimizes
a distribution vector denoting the likelihood of words being a
trigger word. To address the infeasibility problem (Challenge
II), PICCOLO utilizes a word-level optimization method. In-
stead of inverting tokens that may not form any legitimate
word, PICCOLO enforces the multiple tokens constituting a
word to be inverted together. To avoid the need of precise
inversion of triggers with a variable length (Challenge III),
PICCOLO leverages the word discriminativity analysis. These
design choices are generic for NLP backdoor scanning. De-
tailed justiﬁcations can be found in individual subsections.
B. Equivalent Model Transformation with Differentiable Word
Encoding
Method Description. Assume an input sentence with n words
s = w1 w2 ...wn. In the original subject model (Figure 2),
the word embedding(s) of wi are acquired by the following
discrete table lookup operations.
ei = Mt2e[Mt[index (wi)]]
(1)
Function index () returns the index of a word in the vocabulary.
Matrix Mt stores the token ids for individual words and each
word may correspond to multiple token ids. Matrix Mt2e
stores the word embedding for each token id. The classiﬁcation
procedure of the model is hence simpliﬁed to the following.
(2)
Intuitively, the transformer T turns the word embeddings e
(deﬁned in Equation (1)) to representation embeddings which
are used by the classiﬁer Mcls2y to make the ﬁnal prediction.
In the ﬁrst step, PICCOLO transforms the model as follows.
As shown in Figure 5, given a sentence, each word is encoded
y = Mcls2y (T (e))
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:09 UTC from IEEE Xplore.  Restrictions apply. 
52029
Model
M
Equivalent(cid:3)Model(cid:3)Transfor(cid:882)
mation(cid:3)with(cid:3)Differentiable(cid:3)
Word(cid:3)Encoding(cid:3)(II.B)(cid:3)
M’
A(cid:3)few(cid:3)clean(cid:3)inputs(cid:3)and(cid:3)target(cid:3)label
Word(cid:882)level(cid:3)
Trigger(cid:3)Inversion(cid:3)
(II.D)
Likely(cid:3)trigger(cid:3)
words
Word(cid:3)Discrimin(cid:882)
ativity(cid:3)Analysis(cid:3)(II.C)
Trigger(cid:3)validation(cid:3)
(II.E)
Fig. 4: Overview
TABLE I: Challenges versus design choices
Challenge
I. Discontinuity in NLP Applications
II. Infeasibility in optimization results
III. Inverting triggers with unknown length is difﬁcult Word discriminativity analysis
Design choice
Equivalent transformation to make subject model differentiable
Optimizing a probability vector
Word-level optimization
Trojaned/
clean
Section
V.B
V.C
(cid:10)(cid:20)(cid:33)(cid:17)(cid:1)(cid:25)(cid:18)(cid:1)(cid:28)(cid:25)(cid:21)(cid:17)(cid:24)(cid:1)(cid:30)(cid:25)(cid:15)(cid:13)(cid:14)(cid:29)(cid:22)(cid:13)(cid:26)(cid:32)(cid:8)(cid:1)(cid:4)(cid:2)(cid:21)
(cid:10)(cid:20)(cid:33)(cid:17)(cid:1)(cid:25)(cid:18)(cid:1)(cid:31)(cid:25)(cid:26)(cid:16)(cid:1)(cid:30)(cid:25)(cid:15)(cid:13)(cid:14)(cid:29)(cid:22)(cid:13)(cid:26)(cid:32)(cid:8)(cid:1)(cid:6)(cid:21) (cid:2)(cid:3)(cid:1)(cid:2)(cid:5)(cid:4)(cid:1) (cid:9)(cid:4)(cid:1)(cid:3)
(cid:12)(cid:25)(cid:26)(cid:16)(cid:1)(cid:20)(cid:16)(cid:27)(cid:1)(cid:24)(cid:1)(cid:1) (cid:3) (cid:12)(cid:25)(cid:26)(cid:16)(cid:1)(cid:30)(cid:17)(cid:15)(cid:28)(cid:25)(cid:26)(cid:27)(cid:1)(cid:24)(cid:1)(cid:1) (cid:6)(cid:21)
(cid:20)(cid:11)(cid:5)(cid:3)(cid:1)(cid:39)(cid:3)(cid:1)(cid:6)(cid:3)(cid:1)(cid:39)(cid:12)(cid:3)(cid:1)
(cid:11)(cid:39)(cid:12)(cid:3)
(cid:39)(cid:21)
(cid:20)(cid:9)(cid:10)(cid:8)(cid:9)(cid:3)(cid:1)(cid:39)(cid:21)
(cid:1) (cid:9)(cid:3)(cid:1)(cid:2)
(cid:31)(cid:1)(cid:1) (cid:10)
(cid:11)(cid:25)(cid:21)(cid:17)(cid:24) (cid:30)(cid:17)(cid:15)(cid:28)(cid:25)(cid:26)(cid:27) (cid:24)(cid:1)(cid:1) (cid:6)(cid:1)(cid:1) (cid:4)(cid:2)(cid:21) (cid:12)(cid:25)(cid:26)(cid:16)(cid:1)(cid:17)(cid:23)(cid:14)(cid:17)(cid:16)(cid:16)(cid:20)(cid:24)(cid:19)(cid:27)(cid:1)(cid:24)(cid:1) (cid:6)(cid:1)(cid:1) (cid:6)(cid:5)(cid:7)
(cid:20)(cid:18)(cid:32)(cid:28)(cid:25)(cid:31)(cid:1)(cid:32)(cid:26)(cid:1)(cid:20)(cid:17)(cid:13)(cid:15)(cid:21)(cid:3)
(cid:39)(cid:3)
(cid:18)(cid:32)(cid:28)(cid:25)(cid:31)(cid:1)(cid:32)(cid:26)(cid:1)(cid:20)(cid:17)(cid:13)(cid:15)(cid:21)(cid:3)
(cid:18)(cid:32)(cid:28)(cid:25)(cid:31)(cid:1)(cid:32)(cid:26)(cid:1)(cid:3)(cid:2)(cid:4)(cid:1)
(cid:39)(cid:21)
(cid:20)(cid:11)(cid:5)(cid:4)(cid:8)(cid:3)(cid:39)(cid:12)(cid:3)
(cid:11)(cid:7)(cid:4)(cid:8)(cid:3)(cid:39)(cid:12)(cid:3)(cid:1)
(cid:39)(cid:21)
(cid:18)(cid:34)(cid:22)(cid:31)(cid:35)(cid:26)(cid:32)(cid:34)(cid:30)(cid:25)(cid:34)(cid:1)
(cid:2) (cid:14)(cid:29)(cid:22)(cid:35)(cid:35)(cid:27)(cid:26)(cid:27)(cid:25)(cid:34)
(cid:16)(cid:25)(cid:25)(cid:24)(cid:1)(cid:36)(cid:32)(cid:1)
(cid:36)(cid:32)(cid:28)(cid:25)(cid:31)(cid:35)
(cid:1)
Fig. 5: Word encoding method
(cid:19)(cid:22)(cid:38)(cid:1)(cid:32)(cid:37)(cid:25)(cid:34)(cid:33)(cid:34)(cid:27)(cid:23)(cid:25)(cid:24)(cid:3)(cid:1)(cid:39)
(cid:10)(cid:17)(cid:24)(cid:28)(cid:17)(cid:24)(cid:15)(cid:17)
(a) Token-level 1st token
(b) Token-level 2nd token
(c) Token-level 3rd token
(d) Word-level 1st token
(e) Word-level 2nd token
(f) Word-level 3rd token
Fig. 6: Comparison between token and word level optimization
by a probability vector w denoting the distribution of the word,
that is, the value of dimension i indicates the probability of the
word being the ith word in the dictionary. Observe that for a
known word like “way”, its word vector has a one-hot value.
We construct a word-to-token matrix Mw2t beforehand that
can project w to its tokens [t1, t2, ..., t7] by differentiable ma-
trix multiplication (details later in the section). Note that since
the most complex word in the vocabulary has 7 tokens, we
project each word to 7 tokens. For words that have a smaller
number of tokens, we pad with a special meaningless token,
e.g., the [PAD] token in BERT. As such, these paddings have
minimal perturbations to the sentence. The token sequence is
further translated to the word embeddings.
Formally, during testing, the sentence s is transformed to
a word vector sequence s = x1 x2 ...xn. Vector xi contains
the one-hot encoding for word wi, with dimension index (wi)
equal to 1 and the others 0. The model is transformed such
that
the corresponding word embeddings are acquired by
differentiable matrix multiplications as follows.
e = s × Mw2t × Mt2e
(3)
The transformed model is hence the following.
y = Mcls2y (T (s × Mw2t × Mt2e))
(4)
Observe that it is fully differentiable with word-level encoding.
During inversion, unknown word(s) are inserted. The word
vector x for an unknown word holds a distribution instead of
a one-hot value. As such, t = x× Mw2t essentially yields the
expected token vector values that are not one-hot but rather
denote the distributions of tokens. Consequently, t × Mt2e
yields a sequence of expected word embeddings.
If a word is known, like a word in input, its vector value x
is one-hot, t = x × Mw2t also yields token vectors that have
one-hot values. Consequently, e = t × Mt2e yields precise
embedding values instead of expected values.
With the assumption that the padding tokens do not impact
model behaviors, it is easy to infer that the transformed model
is equivalent to the original one. Speciﬁcally, assume a token
id k. Its token vector tk hence has a one-hot value with the
kth dimension being 1. The original discrete lookup Mt2e[k]
is equivalent to the differentiable tk× Mt2e. Similar reasoning
can be conducted for the translation from words to tokens.
After model transformation, the optimizer then inverts the
unknown word(s) based on a loss function (Section V-D).
When the optimizer updates a dimension of
the unknown
word vector corresponding to some word, PICCOLO essentially
ensures all
the tokens corresponding to its subwords are
updated in the same pace, preventing infeasible subwords.
Design Justiﬁcation. The design choice of making the model
differentiable is generic because inversion cannot be performed
at the input level otherwise. The choice of inverting a word
vector denoting a distribution is also generic as it avoids
making the discrete decision about if a word is a trigger word.
Next we justify the design choice of word encoding.
We discuss in Section IV that GBDA [36] tends to generate
tokens that correspond to infeasible subwords, especially when
triggers are complex. Let’s revisit the trojaned model with
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:09 UTC from IEEE Xplore.  Restrictions apply. 
62030
the trigger ‘immensity’ in Section IV. Recall
the trigger
corresponds to three tokens:‘im’, ‘men’, and ‘sity’. However,
in Figure 3, GBDA inverts three tokens corresponding to
‘im’, ‘van’, and ‘duty’. Let’s dive deeper to understand the
causation. The bar charts in Figure 6 (a)-(c) show how the
values of relevant dimensions of the three inverted token
vectors change over time during optimization. In a perfect
world, GBDA would invert ‘im’ in the ﬁrst token, meaning
only the dimension corresponding to ‘im’ has a close to 1
value and others are close to 0, ‘men’ in the second and ‘sity’
in the third. However, we can observe that the three token
vectors have similar values, with the large values for the same
three dimensions ‘im’, ‘van’ and ‘duty’. Essentially, the spatial
constraints among the three subwords in ‘immensity’ are not
respected during optimization. As such, the optimizer falls to
some local minimal. The limitation is general as the multiple
token vectors are completely independent and do not have any
inter-constraints imposed.
Our idea is to enforce the spatial constraints between
subwords. Intuitively, the token dimension for a subword (e.g.,
‘im’) should never be updated independently. Instead, they
should be updated in sync with the other subwords in some
legitimate word. In our example, the three subwords ‘im’,
‘men’, and ‘sity’ shall be updated in the same pace. This leads
to our design choice of word-level encoding. The bar charts
in Figure 6 (d) to (f) show the dimension changes for the ﬁrst
three token vectors with the word encoding. Observe that with
spatial constraints, the optimizer’s behaviors are completely
different from before. Dimensions ‘im’, ‘men’, and ‘sity’ stand
out in the three respective tokens with the same pace. This
allows us to invert the correct trigger.
We perform an ablation study of word encoding on the
TrojAI round 6 test set. The overall accuracy of PICCOLO
decreases from 0.907 to 0.819 when changing the word
encoding to the default token level encoding. The details of
the ablation study is in Appendix IX-G. We also analyze
how well PICCOLO handles trigger with multiple tokens. For
the TrojAI R5 dataset, there are 357 models trojaned with
triggers containing multi-token words. GBDA fails on 197 of
them. PICCOLO is able to ﬁx 160 of the 197. For R6, there
are 187 models with triggers containing multi-token words.
GBDA fails on 70 of them. PICCOLO ﬁxes 59 out of these
70. Because T-miner is very slow, we did not evaluate it on
the full test set of TrojAI rounds 5 and 6. T-miner fails to
generate triggers for 283 out of the 326 trojaned models with
multi-token triggers in R5 (that we tested). PICCOLO can
detect 220 of them. T-miner fails to generate triggers for 79
out of the 90 trojaned models with multi-token triggers in R6
(that we tested). PICCOLO ﬁxes 68 of them.
Model Transformation Algorithm. To transform the model,
PICCOLO ﬁrst extracts the index () function and the two tables
Mt2e and Mt in Equation (1). It then automatically constructs
the word-to-token translation matrix Mw2t from the extracted
results using the following algorithm. Similar to our word
encodings, we use probability vectors to denote tokens as
(cid:2) token vector for [PAD]
(cid:2) Padding to 7 tokens for each word
tpad = [1,0,0,...,0]
for each word w in vocabulary do
i = index (w)
ids = Mt[i]
for j = 1 to 7-|ids| do
Mw2t[i][j]= tpad
end for
for j=1 to |ids| do
t = [0, ...., 0]
t[ids[j]]=1
Mw2t[i][7-|ids|+j] = t
Algorithm 1 Construction of word-to-token matrix Mw2t
1: function WORD2TOKEN MATRIX CONSTRUCTION(index (), Mt)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16: end function
well, instead of using integer token ids. They are called token
vectors. Each token vector has the size of the token dictionary
(for example, the size is 30522 in BERT) and each dimension
j in the vector denotes the probability of the token being the
jth token in the dictionary. The sum of all dimensions equals
to 1. Given a word with index i, Mw2t[i] contains 7 token
vectors, including padding token vectors if needed.
(cid:2) One-hot value for token vector
end for
end for
return Mw2t
Speciﬁcally, the loop in lines 3-14 goes through each word
w in the vocabulary and constructs its token vectors. In this
paper we consider a vocabulary of 7k commonly used words.
In lines 4-5, it looks up the token ids corresponding to w. In
lines 6-8, padding vectors (denoting a special token [PAD]
with no meaning) are added when the number of token ids is
smaller than 7. Lines 9-13 add the one-hot encodings of the
token ids. Note that when a token is known, its probability
vector degenerates to having a one-hot value.
Example. Consider the example in Figure 5, ‘way’ is the
6746th word in the vocabulary. The 6746th entry of Mw2t
hence contains seven token vectors, with the ﬁrst six hold-
ing the encodings of [PAD] and the last one the one-hot
encoding of ‘way’. During testing, the one-hot word vector
of ‘way’ times Mw2t yields the seven token vectors, whose
multiplication with Mt2e yields seven word embeddings with
the ﬁrst six meaningless and the last one corresponding to the
embedding for ‘way’. (cid:2)
C. Word Discriminativity Analysis
Method Description. After the inversion step, the inverted
likely trigger words are passed on to the word discriminativity
analysis step to determine if the subject model is particularly
discriminative for any of them.
Formal Deﬁnition. Inspired by the notion that transformer
models pay special attention to a subset of words [50], we
hypothesize that for any word w, a linear separation can
be achieved for sentences with and without the presence of
w based on their CLS embeddings. Let x denote a natural