libraries export the correct version string and do not attempt to
conceal their presence. Effectively, these limitations mean that
our measurement results should be viewed as lower bounds.
C. Data Collection
A central contribution of our work is to analyse not only
whether outdated libraries are being used, but why this may
be the case. This implies that detecting whether a library
exists in a window or frame is not enough; we must also
detect if it was loaded by another script. To model causal
inclusion relationships of resources in websites, we introduce
the theoretical concept of causality trees and implement it
in a modern browser. We integrate our two library detection
methods into this modiﬁed browser environment and use it to
collect data about the usage of JavaScript libraries on the Web.
Causality Trees: The goal of a causality tree is to represent
the causal element creation relationships that occur during
the loading and execution of a dynamic website in a modern
browser. A causality tree contains a directed edge A → B
if and only if element A causes element B to load. More
speciﬁcally, the elements we model include scripts, images
and other media content, stylesheets, and embedded HTML
documents. A relationship exists whenever an element creates
another element (e.g., a script creates an iframe) or changes an
existing element’s URL (e.g., a script changes the URL of an
iframe or redirects the main document), which is equivalent to
creating a new element with a different URL.
While the nodes in a causality tree correspond to nodes
in the website’s DOM, their structure is entirely unrelated to
5
Chrome Debugging Protocol [8] to minimise the necessity for
brittle browser source code modiﬁcations.
The Chrome Debugging Protocol provides programmatic
access to the browser and allows clients to attach to open win-
dows, inspect network trafﬁc, and interact with the JavaScript
environment and the DOM tree loaded in the window. Two
prominent uses of this API are the Chrome Developer Tools (an
HTML and JavaScript front-end to the protocol) and Selenium’s
WebDriver interface to remotely control Chrome.
At a high level, we generate causality trees by observing
resource requests through the network view of the debugging
protocol. Note that this view includes resources not actually
loaded over the network, e.g., inline URL schemas such as
data: or javascript:. We disable all forms of caching
to observe even duplicate resource inclusions within the same
frame, which are otherwise handled through an in-memory
cache. For each loaded resource, the protocol allows us to
identify the frame in which the resource is located as well
as the initiating script, where applicable. Similarly, we utilise
protocol methods to be notiﬁed of script generation events and
store the source code of both inline and URL-based JavaScript.
(This includes source code from attribute-based event handlers
and string evaluation, which we both model as inline script
nodes.) We store a log of all relevant events during the crawl
and assemble the causality trees in a post-processing step.
Integration of Library Detection: Hash-based detection of
libraries is relatively straight-forward to integrate with our
crawler; we simply compute the source code hashes of all
script nodes in the causality trees and look them up in our
reference catalogue during post-processing.
Integrating dynamic library detection is more challenging—
out of the box, existing detection methods can only detect
whether a JavaScript context (window or frame) contains a
library or not, but in general this information is not sufﬁcient
to properly label the correct script node in the causality tree.
The Chrome Debugging Protocol allows us to link a method
executed in a JavaScript context to the script that contains its
implementation; thus, once we hold a reference to a JavaScript
library object (such as jq in the example from Section III-B),
we dynamically enumerate its instance methods and use an
arbitrary one of them to identify the implementing script.
Another challenge in dynamically detecting libraries during
the crawl is that we need to inject the detection code into each
frame of a website, since each frame has its own JavaScript
scope and may contain independent library instances. This is
further complicated by the fact that modern websites are quite
dynamic and an ad frame, for instance, may quickly navigate
to a different URL, which causes any library previously loaded
in the frame to be unloaded or replaced. Lastly, we observe
that many websites include multiple copies of the same library,
including different versions of the same library (refer to our
analysis in Section IV-G for details). Typically, only one library
instance can exist in each context because the more recently
loaded instance replaces the previous global reference.
In order to be able to study these phenomena in detail and
also address the other aforementioned detection challenges, we
inject the detection code into each frame and execute it every
4 seconds. Note that it would not be feasible from a performance
point of view to execute the detection code after each script
Fig. 2. Example causality tree.
the hierarchical DOM tree. Rather, nodes in the causality tree
are snapshots of elements in the DOM tree at speciﬁc points
in time, and may appear multiple times if the DOM elements
are repeatedly modiﬁed. For instance, if a script creates an
iframe with URL U1 and later changes the URL to U2, the
corresponding script node in the causality tree will have two
document nodes as its children, corresponding to URLs U1
and U2, but referring to the same HTML  element.
Similarly, the predecessor of a node in the causality tree is not
necessarily a predecessor of the corresponding HTML element
in the DOM tree; they may even be located in two different
HTML documents, such as when a script appends an element
to a document in a different frame.
Figure 2 shows a synthetic example of a causality tree.
The large black circle is the document root (main document),
ﬁlled circles are scripts, squares are HTML documents (e.g.,
embedded in frames or corresponding to a new main document
if there is a top-level redirect), and empty circles are other re-
sources (e.g., images). Edges denote “created by” relationships;
for example, in Figure 2 the main document included the grey
script, which in turn included the blue script. Dashed lines
around nodes denote inline scripts, while solid lines denote
scripts included from an URL. Thick outlines denote that a
resource was included from a known ad network, tracker, or
social widget (see below for more details).
The colour of nodes in Figure 2 denotes which document
they are attached to in the DOM: grey corresponds to resources
attached to the main document, while we assign one of four
colours to each further document in frames. Document squares
contain the colour of their parent location in the DOM, and
their own assigned colour. Resources created by a script in
one frame can be attached to a document in another frame, as
shown by the grey script which has a blue child in Figure 2,
i.e., the blue script is a child of the blue document in the DOM.
Figure 12 shows the causality tree of mercantil.com,
with images and other irrelevant node types omitted for clarity.
It includes three clearly visible social media widgets: Twitter,
Facebook, and LinkedIn. Note that the web developer embedded
code provided by the social networks into the main document,
which in turn initialises each widget and creates one or more
frames for their contents. We also see that the causality tree
includes multiple copies of jQuery in the main document, which
we will discuss in detail in Section IV-G.
Implementation of Causality Trees: Our deﬁnition of
causality trees is related to the concepts previously used to
analyse malicious JavaScript inclusions [1] and cookie matching
between online ad exchanges [5], but it differs in the details. Our
implementation is independent from the aforementioned works
and uses a different technological approach by building on the
6
RootFrameIncluded ScriptsInline ScriptAd ScriptAd Frame w/ Scriptscreation event since websites routinely contain thousands of
script nodes (see Section IV-B).
version of each loaded library, which enables us to assess the
accuracy of the dynamic detection.
To detect cases of inclusions that we may miss due to
the four-second detection interval, we additionally execute the
dynamic detection code post hoc on all scripts found during
the crawl. This execution is done in an individual Node.js
environment with a fake DOM tree. This ofﬂine detection
step cannot fully replace in-browser detections, however, since
some libraries such as jQuery UI have code or environment
prerequisites that cause the ofﬂine detection to fail.
Annotation of Ads, Trackers and Widgets: To further clarify
the provenance of scripts we observe in our crawl, we aim
to determine whether they are related to known advertising,
tracking, or social widget code. We achieve this by injecting
a customised version of the AdBlock browser extension into
each frame. Our version of AdBlock ﬂags content but permits
it to load, and we veriﬁed that our customised version remains
undetected by common ad-block detection scripts. We use
EasyList and EasyPrivacy to identify advertising and tracking
content, and Fanboy’s Social Blocking List to detect social
network widgets. For our analysis, we label an element in
the causality tree as ad/tracker/widget-related whenever the
corresponding element or any parent in the DOM tree is labeled
by AdBlock. Additionally, we propagate these labels downwards
to all children of the labelled node in the causality tree.
Crawl Parameters: To gain a representative view of
JavaScript library usage on the Web, we collected two different
datasets. First, we crawled the Alexa Top 75 k domains, which
represent websites popular with users. Second, we crawled
75 k domains randomly sampled from a snapshot of the .com
zone, that is, a random sample of all websites with a .com
address, which we expect to be dominated by less popular
websites. We conducted the two crawls in May 2016 from IP
addresses in a /24 range in the US. We observe ∼5 % and
∼17.2 % failure rates in ALEXA and COM, leaving us with
data from 71,217 and 62,086 unique domains, respectively.
Failures were due to timeouts and unresolvable domains, which
is expected especially for COM since the zone ﬁle contains
domains that may not have an active website.
To preserve the ﬁdelity of our data collection, our crawler is
based on Chromium and includes support for Flash. We disable
various security mechanisms such as malware and phishing
ﬁlters. We only crawl the homepage of each visited site due
to the presence of many sites that thwart deeper traversal by
requiring log-ins. While visiting a page, the crawler scrolls
downwards to trigger loading of any dynamic content. As we
found page-loaded events to be unreliable, our crawler remains
on each page for a ﬁxed delay of 60 seconds before clearing
its entire state, restarting, and then proceeding to the next site.
D. Validation
As the ﬁnal step in our methodology, we validate that our
static and dynamic detection methods work in practice.
Dynamic Detection: To investigate the efﬁcacy of our
dynamic detection code, we conduct a controlled experiment:
we load each of the reference libraries in our catalogue into
Node.js, one at a time, and attempt to detect each ﬁle with
the dynamic detection method. Intuitively, we know the exact
Overall, we observe that the dynamic detection code is
able to identify the exact name and version of 79.2 % of the
libraries, as well as the name (but not the version) of 18.6 %
of the libraries. Only 2 % of libraries fail to be identiﬁed (i.e.,
were false negatives). We manually examine the libraries that
are only detected by name, and ﬁnd that the vast majority are
older versions that do not include a variable or method that
returns the library version. Of course, 100 % of these libraries
can be detected based on their ﬁle hashes, which reinforces the
importance of using multiple techniques to identify libraries.
Static Detection: To investigate the efﬁcacy of library
detection using ﬁle hashes, we conduct a second controlled
experiment: we randomly select from our crawled data 415
unique scripts that the dynamic detection code classiﬁes as
being jQuery, and attempt to detect them based on their ﬁle
hashes. In this case, we are treating the output of the dynamic
detection method as ground truth.
Overall, we observe that only 15.4 % of the libraries can
be identiﬁed as jQuery based on their ﬁle hash. Although this
is a low detection rate, the result also matches our expectation
that developers often deploy customised versions of libraries.
For example, 90 % of the jQuery libraries that we fail to detect
via ﬁle hashing contain fewer than 150 line break characters,
whereas non-miniﬁed copies of jQuery from our catalogue
contain more than 1900. This strongly suggests that the unique
scripts are custom-miniﬁed versions of jQuery.
Hypothetical “Name-in-URL” Detection: For the last
validation step, we consider a simple library detection heuristic.
The heuristic ﬂags a script ﬁle as jQuery, for instance, whenever
the string “jquery” appears in the URL of the script. To evaluate
the accuracy of this heuristic, we extract from our ALEXA crawl
the set of script URLs that contain “jquery,” and the URLs of
scripts detected as jQuery by our dynamic and static methods.
Out of these URLs, 22.3 % contain “jquery” and are also
detected as the library; 69 % are ﬂagged only by the heuristic,
and 8.8 % are detected only by our dynamic and static methods.
The heuristic appears to cause a large number of false positives
due to scripts named “jquery” without containing the library,
and it also seems to suffer from false negatives due to scripts
that contain jQuery but have an unrelated name.
We validate this ﬁnding by manually examining 50 scripts
from each of the two set differences. The scripts in the
detection-only sample appear to contain additional code such
as application code or other libraries. Only one of these scripts
does not contain jQuery but Zepto.js, an alternative to jQuery
that is partially compatible and also deﬁnes the characteristic
$.fn variable. On the other hand, none of the scripts in the
heuristic-only sample can be conﬁrmed as the library; nearly
all of them contain plug-ins for jQuery but not the library itself.
The results for the Modernizr library, which does not have
an equivalent to jQuery’s extensive plug-in ecosystem, conﬁrm
this trend. The overlap between heuristic and detection is 55.3 %
of URLs, heuristic-only 0.8 %, and detection-only 44 %—the
simple heuristic misses many Modernizr ﬁles renamed by
developers. These results underline the need for more robust
detection techniques such as our dynamic and static methods;
we do not use the heuristic in our analysis.
7
Fig. 3. Distribution of JavaScript inclusion type
frequency per site in ALEXA.
Fig. 4.
frequency per site in ALEXA.
Distribution of library inclusion type
Fig. 5. Distribution of vulnerable library count
versus overall library count per site in ALEXA.
Fig. 6. Distribution of JavaScript inclusion type
frequency per site in COM.
Fig. 7.
frequency per site in COM.
Distribution of library inclusion type
Fig. 8. Distribution of vulnerable library count
versus overall library count per site in COM.
IV. ANALYSIS
In this section, we analyse the data from our web crawls.
First, we present a general overview of the dataset by drilling
down into the causality trees, overall JavaScript inclusion
statistics, and vulnerable JavaScript library inclusions. Next, we
examine risk factors for sites that include vulnerable libraries,
and the age of vulnerable libraries (i.e., relative to the latest
release of each library). Finally, we examine unexpected,
duplicate library inclusions in websites, and investigate whether
common remediations practices are useful and used in practice.
A. Causality Trees
We begin our analysis by measuring the complexity of the
websites in our crawls. The median causality tree in ALEXA
contains 133 nodes (p95: 425, max: 19,508) whereas it is
38 nodes (p95: 298, max: 25,485) in COM, indicating that
ALEXA contains a larger share of more complex websites.
Similarly, ALEXA and COM contain a median of 52 and
14 image nodes, 44 and 16 script nodes, and 4 and 2 document
nodes per causality tree, respectively. 95 % of frames in both
crawls show only one or two documents during the 60 s of
our crawl, whereas a few cycle through larger numbers of
documents (max: 12 for ALEXA and 32 for COM).
The median depth of the causality trees (deﬁned as the
length of the longest path from the root to a leaf) is 4 inclusions
(p95: 9, max: 438) in ALEXA and 3 (p95: 8, max: 62) in COM.
Since paths correspond to causal relationships, intuitively this
means that the inclusion of a node could have been inﬂuenced
by up to 438 predecessors. In both crawls, images tend to
appear further up in the causality trees at a median depth of 1,
that is, at least half of them are directly included in the main
document, whereas documents tend to appear further down
at a median depth of 2, which indicates that they are more
frequently dynamically generated.
B. General JavaScript Statistics
Scripts are the most common node type in our causality
trees; 97 % of ALEXA sites and 83.6 % of COM sites contain
JavaScript. The most common script type are inline scripts,
which includes script code embedded as text in a 
tag, any code in attribute-based event handlers (such as the
onclick attribute), and any code evaluated from strings using
methods such as eval(). The causality trees from the ALEXA
and COM crawls contain a median of 24 and 9 inline scripts,
respectively, with 5 % of the sites having hundreds of inline
scripts—the maximum observed was 19 K and 25 K.
When looking at URL-based script inclusions, we distin-
guish between internal scripts, that is, code hosted on the
same domain as the website (or a subdomain of the website),
and all other scripts that we call external. Figures 3 and 6
depict the distribution of script inclusion types for ALEXA and
COM, respectively. About 91.7 % of all ALEXA sites include
8
100101102103104105# scripts0.00.20.40.60.81.0CDF (Alexa sites)InternalExternalInline02468101214# detected libraries0.00.20.40.60.81.0CDF (Alexa sites)InlineExternalInternalAll02468101214# detected libraries0.00.20.40.60.81.0CDF (Alexa sites)VulnerableAll100101102103104105# scripts0.00.20.40.60.81.0CDF (com sites)InternalExternalInline02468101214# detected libraries0.00.20.40.60.81.0CDF (com sites)InlineExternalInternalAll02468101214# detected libraries0.00.20.40.60.81.0CDF (com sites)VulnerableAllTABLE II.
EXTERNAL JAVASCRIPT: TOP 10 MARKET SHARE.
SORTED BY ALEXA; DATA OMITTED FOR HOSTS NOT PART OF THE TOP 10.
Hostname
www.google.com
www.google-analytics.com
ak2.imgaft.com
ajax.googleapis.com
pagead2.googlesyndication.com
connect.facebook.net
www.googletagmanager.com
www.googletagservices.com
partner.googleadservices.com
www.googleadservices.com
platform.twitter.com
apis.google.com
maps.googleapis.com
s.ytimg.com
ALEXA
9.5 %
3.1 %
2.9 %