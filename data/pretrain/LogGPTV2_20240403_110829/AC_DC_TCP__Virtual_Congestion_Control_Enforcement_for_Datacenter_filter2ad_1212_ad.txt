252
251
1.89
1.89
1.89
1.89
1.88
1.89
1.88
1.89
Table 1: AC(cid:69)DC works with many congestion control variants. Legend: CUBIC*: CUBIC + standard OVS, switch
WRED/ECN marking off. DCTCP*: DCTCP + standard OVS, switch WRED/ECN marking on. Others: different CCs +
AC(cid:69)DC, switch WRED/ECN marking on.
1.98
1.98
1.98
1.97
1.98
1.97
1.97
1.97
0.85
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.98
0.99
0.99
0.99
0.99
0.99
0.99
0.99
(a) CUBIC convergence test.
(b) DCTCP convergence test.
Figure 14: Convergence tests: ﬂows are added, then removed, every 30 secs. AC(cid:69)DC performance matches DCTCP.
(c) AC(cid:69)DC convergence test.
(a) Default.
(b) AC(cid:69)DC.
Figure 15: (a) CUBIC gets little throughput when competing
with DCTCP. (b) With AC(cid:69)DC, CUBIC and DCTCP ﬂows
get fair share.
and then reversing the process. The result is shown in Fig-
ure 14. Figure 14a shows CUBIC’s problems converging to
fair allocations. Figures 14b and 14c show DCTCP and AC(cid:69)DC
performance, respectively. AC(cid:69)DC tracks DCTCP’s behav-
ior. CUBIC’s drop rate is 0.17% while DCTCP’s and AC(cid:69)DC’s
is 0%.
The second experiment is also repeated from Judd’s pa-
per [36]. ECN-capable and non-ECN-capable ﬂows do not
coexist well because switches drop non-ECN packets when
the queue length is larger than the marking threshold. Fig-
ure 15a shows the throughput of CUBIC suffers when CU-
BIC (with no ECN) and DCTCP (with ECN) traverse the
same bottleneck link. Figure 15b shows AC(cid:69)DC alleviates
Figure 16: CUBIC experiences high RTT when competing
with DCTCP. AC(cid:69)DC ﬁxes this issue.
this problem because it forces all ﬂows to become ECN-
capable. Figure 16 shows CUBIC’s RTT is extremely high
in the ﬁrst case because switches drop non-ECN packets (the
loss rate is 0.18%) and thus there is a signiﬁcant number of
retransmissions. However, AC(cid:69)DC eliminates this issue and
reduces latency.
The last experiment examines the impact of having mul-
tiple TCP stacks on the same fabric. Five ﬂows with differ-
ent congestion control algorithms (CUBIC, Illinois, High-
Speed, New Reno and Vegas) are started on the dumbbell
topology. This is the same experiment as in Figure 1. Fig-
 0 1 2 3 4 5 6 7 8 9 10 0 50 100 150 200 250 300Tput (Gbps)Time (seconds)Flow 1Flow 2Flow 3Flow 4Flow 5 0 1 2 3 4 5 6 7 8 9 10 0 50 100 150 200 250 300Tput (Gbps)Time (seconds)Flow 1Flow 2Flow 3Flow 4Flow 5 0 1 2 3 4 5 6 7 8 9 10 0 50 100 150 200 250 300Tput (Gbps)Time (seconds)Flow 1Flow 2Flow 3Flow 4Flow 5 0 2 4 6 8 10 0 2 4 6 8 10 12 14 16 18 20Tput (Gbps)Time (seconds)CUBICDCTCP 0 2 4 6 8 10 0 2 4 6 8 10 12 14 16 18 20Tput (Gbps)Time (seconds)CUBICDCTCP 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 1 10 100CDFCUBIC TCP Round Trip Time (milliseconds)CUBIC w/o AC/DCCUBIC w/ AC/DC(a) All DCTCP.
(b) 5 different CCs (AC(cid:69)DC).
Figure 17: AC(cid:69)DC improves fairness when VMs implement
different CCs. DCTCP performance shown for reference.
(a) Average throughput.
(b) Fairness.
Figure 18: Many to one incast: throughput and fairness.
ure 17a shows what happens if all ﬂows are conﬁgured to
use DCTCP and Figure 17b shows when the ﬁve different
stacks traverse AC(cid:69)DC. We can see AC(cid:69)DC closely tracks
the ideal case of all ﬂows using DCTCP, and AC(cid:69)DC and
DCTCP provide better fairness than all CUBIC (Figure 1b).
5.2 Macrobenchmarks
In this section we attach all servers to a single switch
and run a variety of workloads to better understand how
well AC(cid:69)DC tracks DCTCP’s performance. Experiments
are run for 10 minutes. A simple TCP application sends
messages of speciﬁed sizes to measure FCTs.
Incast In this section, we evaluate incast scenarios. To scale
the experiment, 17 physical servers are equipped with four
NICs each and one ﬂow is allocated per NIC. In this way,
incast can support up to 47-to-1 fan-in (our switch only has
48 ports). We measure the extent of incast by increasing the
number of concurrent senders to 16, 32, 40 and 47. Fig-
ure 18 shows throughput and fairness results. Both DCTCP
and AC(cid:69)DC obtain a fairness index greater than 0.99 and
get comparable throughput as CUBIC. Figure 19 shows the
RTT and packet drop rate results. When there are 47 con-
current senders, DCTCP can reduce median RTT by 82%
and AC(cid:69)DC can reduce by 97%; DCTCP can reduce 99.9th
percentile RTT by 94% and AC(cid:69)DC can reduce by 98%.
Both DCTCP and AC(cid:69)DC have 0% packet drop rate. It is
curious that AC(cid:69)DC’s performance is better than DCTCP
when the number of senders increases (Figure 19a). The
Linux DCTCP code puts a lower bound of 2 packets on
CWND. In incast, we have up to 47 concurrent competing
ﬂows and the network’s MTU size is 9KB. In this case, the
lower bound is too high, so DCTCP’s RTT increases gradu-
ally with the number of senders. This issue was also found
Figure 20: TCP RTT when almost all ports are congested.
in [36]. AC(cid:69)DC controls RWND (which is in bytes) instead of
CWND (which is in packets) and RWND’s lowest value can be
much smaller than 2*MSS. We veriﬁed modifying AC(cid:69)DC’s
lower bound caused identical behavior.
The second test aims to put pressure on the switch’s dy-
namic buffer allocation scheme, similar to an experiment in
the DCTCP paper [3]. To this end, we aim to congest every
switch port. The 48 NICs are split into 2 groups: group A
and B. Group A has 46 NICs and B has 2 (denoted B1 and
B2). Each of the 46 NICs in A sends and receives 4 concur-
rent ﬂows within A (i.e., NIC i sends to [i+1, i+4] mod 46).
Meanwhile, all of the NICs in A send to B1, creating a 46-to-
1 incast. This workload congests 47 out of 48 switch ports.
We measure the RTT between B2 and B1 (i.e., RTT of the
trafﬁc traversing the most congested port) and the results are
shown in Figure 20. The average throughputs for CUBIC,
DCTCP, and AC(cid:69)DC are 214, 214 and 201 Mbps respec-
tively, all with a fairness index greater than 0.98. CUBIC
has an average drop rate of 0.34% but the most congested
port has a drop rate as high as 4%. This is why the 99.9th
percentile RTT for CUBIC is very high. The packet drop
rate for both DCTCP and AC(cid:69)DC is 0%.
Concurrent stride workload In concurrent stride, 17 servers
are attached to a single switch. Each server i sends a 512MB
ﬂow to servers [i + 1, i + 4] mod 17 in sequential fashion
to emulate background trafﬁc. Simultaneously, each server
i sends 16KB messages every 100 ms to server (i + 8) mod
17. The FCT for small ﬂows (16KB) and background ﬂows
(512MB) are shown in Figure 21. For small ﬂows, DCTCP
and AC(cid:69)DC reduce the median FCT by 77% and 76% re-
spectively. At the 99.9th percentile, DCTCP and AC(cid:69)DC
reduce FCT by 91% and 93%, respectively. For background
ﬂows, DCTCP and AC(cid:69)DC offer similar completion times.
CUBIC has longer background FCT because its fairness is
not as good as DCTCP and AC(cid:69)DC.
Shufﬂe workload In shufﬂe, each server sends 512MB to
every other server in random order. A sender sends at most
2 ﬂows simultaneously and when a transfer is ﬁnished, the
next one is started until all transfers complete. Every server
i also sends a 16 KB message to server (i + 8) mod 17 every
100 ms. This workload is repeated for 30 runs. The FCT for
each type of ﬂow is shown in Figure 22. For small ﬂows,
 0 1 2 3 4 5 1 2 3 4 5 6 7 8 9 10Tput (Gbps)TestsMaxMinMeanMedian 0 1 2 3 4 5 1 2 3 4 5 6 7 8 9 10Tput (Gbps)TestsMaxMinMeanMedian 0 100 200 300 400 500 600 700 800 15 20 25 30 35 40 45 50Average Tput (Mbps)Number of SendersCUBICDCTCPAC/DC 0.5 0.6 0.7 0.8 0.9 1 15 20 25 30 35 40 45 50Jain's Fariness IndexNumber of SendersCUBICDCTCPAC/DC 0.1 1 10 10050th95th99th99.9thTCP RTT (milliseconds)PercentilesCUBICDCTCPAC/DC(a) 50th percentile RTT.
(b) 99.9th percentile RTT.
(c) Packet drop rate.
Figure 19: Many to one incast: RTT and packet drop rate. AC(cid:69)DC can reduce DCTCP’s RTT by limiting window sizes.
(a) Mice ﬂow completion times.
(b) Background ﬂow completion times.
Figure 21: CDF of mice and background FCTs in concurrent stride workload.
(a) Mice ﬂow completion times.
(b) Background ﬂow completion times.
Figure 22: CDF of mice and background FCTs in shufﬂe workload.
DCTCP and AC(cid:69)DC reduce median FCT by 72% and 71%
when compared to CUBIC. At the 99.9th percentile, DCTCP
and AC(cid:69)DC reduce FCTs by 55% and 73% respectively. For
large ﬂows, CUBIC, DCTCP and AC(cid:69)DC have almost iden-
tical performance.
Trace-driven workloads Finally, we run trace-driven work-
loads. An application on each server builds a long-lived TCP
connection with every other server. Message sizes are sam-
pled from a trace and sent to a random destination in sequen-
tial fashion. Five concurrent applications on each server are
run to increase network load. Message sizes are sampled
from a web-search [3] and a data-mining workload [2, 25],
whose ﬂow size distribution has a heavier tail. Figure 23
shows a CDF of FCTs for mice ﬂows (smaller than 10KB)
in the web-search and data-mining workloads. In the web-
search workload, DCTCP and AC(cid:69)DC reduce median FCTs
by 77% and 76%, respectively. At the 99.9th percentile,
DCTCP and AC(cid:69)DC reduce FCTs by 50% and 55%, respec-
tively. In the data-mining workload, DCTCP and AC(cid:69)DC
reduce median FCTs by 72% and 73%, respectively. At the
99.9th percentile, DCTCP and AC(cid:69)DC reduce FCTs by 36%
and 53% respectively.
 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 15 20 25 30 35 40 45 50TCP RTT (milliseconds)Number of SendersCUBICDCTCPAC/DC 0 2 4 6 8 10 12 14 16 18 20 15 20 25 30 35 40 45 50TCP RTT (milliseconds)Number of SendersCUBICDCTCPAC/DC 0 0.2 0.4 0.6 0.8 1 15 20 25 30 35 40 45 50Packet Drop Rate (percentage)Number of SendersCUBICDCTCPAC/DC 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 2 4 6 8 10 12 14CDFMice FCT (milliseconds)CUBICDCTCPAC/DC 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.5 1 1.5 2 2.5 3 3.5 4CDFBackground FCT (seconds)CUBICDCTCPAC/DC 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 2 4 6 8 10 12 14CDFMice FCT (milliseconds)CUBICDCTCPAC/DC 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 1 2 3 4 5 6 7 8 9 10 11CDFBackground FCT (seconds)CUBICDCTCPAC/DC(a) Web-search workload.
(b) Data-mining workload.
Figure 23: CDF of mice (ﬂows < 10KB) FCT in web-search and data-mining workloads.
Evaluation summary The results validate that congestion
control can be accurately implemented in the vSwitch. AC(cid:69)DC
tracks the performance of an unmodiﬁed host DCTCP stack
over a variety of workloads with little computational over-
head. Furthermore, AC(cid:69)DC provides this functionality over
various host TCP congestion control conﬁgurations.
6. RELATED WORK
This section discusses different classes of related work.
Congestion control for DCNs Rather than proposing a new
congestion control algorithm, our work investigates if con-
gestion control can be moved to the vSwitch. Thus, many
of the following schemes are complimentary. DCTCP [3] is
a seminal TCP variant for datacenter networks. Judd [36]
proposed simple yet practical ﬁxes to enable DCTCP in pro-
duction networks. TCP-Bolt [62] is a variant of DCTCP for
PFC-enabled lossless Ethernet. DCQCN [74] is a rate-based
congestion control scheme (built on DCTCP and QCN) to
support RDMA deployments in PFC-enabled lossless net-
works. TIMELY [43] and DX [39] use accurate network
latency as the signal to perform congestion control. TCP
ex Machina [70] uses computer-generated congestion con-
trol rules. PERC [35] proposes proactive congestion control
to improve convergence. ICTCP’s [71] receiver monitors in-
coming TCP ﬂows and modiﬁes RWND to mitigate the im-
pact of incast, but this cannot provide generalized conges-
tion control like AC(cid:69)DC. Finally, efforts [12, 64] to imple-
ment TCP Ofﬂoad Engine (TOE) in specialized NICs are not
widely deployed for reasons noted in [44, 66].
Bandwidth allocation Many bandwidth allocation schemes
have been proposed. Gatekeeper [56] and EyeQ [34] abstract
the network as a single switch and provide bandwidth guar-
antees by managing each server’s access link. Oktopus [10]
provides ﬁxed performance guarantees within virtual clus-
ters. SecondNet [28] enables virtual datacenters with static
bandwidth guarantees. Proteus [73] allocates bandwidth for
applications with dynamic demands. Seawall [58] provides
bandwidth proportional to a deﬁned weight by forcing trafﬁc
through congestion-based edge-to-edge tunnels. NetShare [38]
utilizes hierarchical weighted max-min fair sharing to tune
relative bandwidth allocation for services. FairCloud [53]
identiﬁes trade-offs in minimum guarantees, proportionality
and high utilization, and designs schemes over this space.
Silo [33] provides guaranteed bandwidth, delay and burst
allowances through a novel VM placement and admission
algorithm, coupled with a ﬁne-grained packet pacer. As dis-
cussed in §2, AC(cid:69)DC is largely complimentary to these
schemes because it is a transport-level solution.
Rate limiters SENIC [49] identiﬁes the limitations of NIC
hardware rate limiters (i.e., not scalable) and software rate
limiters (i.e., high CPU overhead) and uses the CPU to en-
queue packets in host memory and the NIC. Silo’s pacer in-
jects void packets into an original packet sequence to achieve
pacing. FasTrack [49] ofﬂoads functionality from the server
into the switch for certain ﬂows. AC(cid:69)DC prevents TCP ﬂows
from sending in the ﬁrst place and can be used in conjunction
with these schemes.
Low latency DCNs Many schemes have been proposed to
reduce latency in datacenter networks. HULL [4] uses phan-
tom queues to leave bandwidth headroom to support low la-
tency. pFabric [5] is a clean-slate design which utilizes pri-
ority and minimal switch buffering to achieve low latency.
Fastpass [51] uses a centralized arbiter to perform per-packet
level scheduling. QJUMP [26] uses priority queueing and
rate limiting to bound latency. Trafﬁc engineering [1, 55]
and load balancing [2, 24, 30] can also reduce latency. Be-
cause AC(cid:69)DC works on the transport level, it is largely com-
plimentary to these works.
Performance-enhancing proxies Several schemes improve
end-to-end protocol performance via a middlebox or proxy [7,
8, 9, 16, 18]. AC(cid:69)DC ﬁts into this class of works, but is
unique in providing a mechanism to alter a VM’s TCP con-