convert the Chinese characters into pinyin, which is composed of
English letters. Then we apply same edit distance operations (as
for English misspelling generation) to spawn new pinyin strings.
According to pinyin’s lexical rules, some generated pinyin
strings may not be valid (we still count them as candidates
to match existent pinyin). We transform pinyin strings to all
possible Chinese characters with that pronunciation. In particular,
there exist two phenomena. (1) Same pinyin. As introduced
in Section II, many different Chinese characters map to the
same pinyin. When we transform back from pinyin to Chinese
characters, the number will increase considerably. Different
tones further exaggerate the phenomenon, given that most pinyin
input methods do not provide tone selection to users. (2) Fuzzy
pinyin. Some pinyin have close pronunciations, including nasal,
retroﬂex, and alveolar sounds. Figure 5 shows the anatomical
parts to make the pronunciations and the confusing pinyin
strings. Many people cannot distinguish the differences. Pinyin
input methods also automatically include Chinese characters
that match fuzzy pinyin for users to select. More analysis on
misspelling generation comparison will be shown in Section VI.
Non-auto-corrected identiﬁcation (). In contrast to the En-
glish case, linguistic-collision Chinese words will still be Chinese
words. Therefore, we directly check whether a misspelling
candidate exists in Chinese dictionaries. For valid Chinese
words, search engines will not apply auto-correction/suggestion.
As the examples in Figure 3 demonstrate, even if all Chinese
characters are valid, the combination may not form meaningful
Chinese words. The identiﬁcation procedure can be performed
ofﬂine. We collect commonly used Chinese words from four
popular word dictionaries of Sogou pinyin input method [31].
In total, the dataset contains 1,166,765 Chinese words.
C. Crawling Tasks
To perform the experiment at a large enough scale, we
designed a framework to collect search results, search volumes,
translation data, and blacklist information. Figure 6 gives a
high-level view of these tasks and how they relate to each other.
We begin by collecting the search results for input keywords,
and then check the search volumes, Google Translate API,
and blacklist for search terms. Together, these datasets provide
a comprehensive view of linguistic-collision misspellings. To
ensure that the search engine servers would not be overloaded,
we rate-limited our crawlers.
1) Search results. To determine whether or not the search results
were auto-corrected, we checked the returned page for the
notices described in Section III. If the keyword was not
corrected by the search provider, we parsed the search result
page and collected the ﬁrst 10 search result entries in a
database for later analysis. In particular, we saved the title,
description, and URL for each entry. We used the URL to
check if the result was blacklisted and the title and description
proved invaluable to understanding the SEO techniques used
with linguistic-collision misspellings. In addition, we captured
the estimated number of search results to understand how
difﬁcult the SEO is for particular keywords. Because the
search results can change quickly for pages with malicious
entries, we also captured the raw HTML to allow for later
manual inspection.
2) Search volumes. To analyze how users are exposed to non-
auto-corrected misspellings we queried Baidu Index [32] and
Google Adwords [33]. To estimate search volume for Chinese
terms, we used Baidu Index to collect daily search volumes
for the previous week and month. While Baidu Index allows
1316
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:46:50 UTC from IEEE Xplore.  Restrictions apply. 
3XEOLF%ODFNOLVW
&UDZOHU
6HDUFK
5HVXOWV
6HDUFK
9ROXPHV
/DQJXDJH
7\SHV
*RRJOH
%DLGX
*RRJOH$GV
%DLGX,QGH[
*RRJOH
7UDQVODWH$3,
,QSXW
.H\ZRUGV
Figure 6: Crawling framework that contains four tasks, collecting search
results, search volumes, language types, and public blacklist.
users free access to search volumes, Google Adwords has
recently restricted search volume data to paid customers. As
a result, we only use Google Adwords data to investigate
questions that only require comparing the predictions, such
as from what types of devices users are searching. Using
relative Google Adwords data allows us to compare mobile
and desktop searches, but not exact volumes for large lists
of words.
3) Language types. Because we are interested in what percentage
of English linguistic-collision misspellings are coexistent
within the same language vs. other languages, we decided to
use the Google Translate API to detect the language of the
misspellings [34]. Knowing the language of a misspelling
allows us to determine whether the misspelling is between
two languages or within the same language. In addition to
returning the detected language, the Google Translate API
returns a conﬁdence score which allows us to understand
why Google would fail to correct the misspelling.
4) Public blacklist. Finally, we scanned all of the URLs
returned for the uncorrected misspellings found during task
1). To determine whether a URL is malicious, we checked
VirusTotal [19]. VirusTotal currently aggregates 68 antivirus
scanning engines to identify malicious URLs, including
Google Safebrowsing [35], Yandex Safebrowsing [36],
Spamhaus [37], and Baidu-International [38].
To avoid
introducing high false positive rates, we also implemented
manual spot checking to ensure that the accuracy remained
high.
V. EXPERIMENT
In this section, we describe our experiment settings, keyword
selection, and statistics of the collected data. We also demonstrate
the performance of the adapted RNN approach to generate
eligible search keywords (i.e., those that are not auto-corrected
by search engines).
A. Data Collection and Validation
To understand the characteristics of linguistic-collision mis-
spelling SEO, we perform a large scale data collection and
analysis. We ran the experiment on a cluster of 26 servers
with 2 CPUs and 4 GB of RAM from December 2017 to
July 2018. Speciﬁcally, we conducted two parallel studies
targeting Chinese and English terms. We follow the approach
in Section IV to generate candidate keywords and fetch search
results from Google and Baidu respectively. For the English
study, we generated misspellings from 11,520 original keywords
and collected 1,044,711 searches using the Google search service.
For the Chinese study, we generated misspellings from 6,714
original keywords and collected 724,865 searches from Baidu.
We use two strategies to select original target keywords: (1)
manually collected categories, and (2) Alexa list of popular
websites, for which we will describe details below.
Keyword collection per category. Miscreants intend to target
speciﬁc sets of keywords to gain illicit proﬁt, so we manually
select 13 different categories in English and 12 different
categories in Chinese for analysis. Previous work indicates
that cybercriminals target more on prescription drugs, gambling
terms, adult terms, and software categories [18, 39] (results
in Section VI conﬁrm the conjecture). We collect terms in
such categories for analysis. We also include general consumer
product categories, such as food, cards, clothing, cosmetics,
and jewelry, to allow for a comprehensive comparison. For
English analysis, we collected the terms from the user-ranked
forums [40], and other lists curated for speciﬁc topics [41–43]. In
addition, the discovery of a parked domain using the misspelling
of a major US defense company led to the inclusion of defense
contractor’s names as this type of more targeted misspelling could
be used by more sophisticated attackers for phishing. In total, the
English per-category keywords contain 1,520 terms, and lead to
563,555 misspelling candidates. For Chinese analysis, we mainly
obtain the target keywords from the website china-10.com,
which contains terms for various categories. We totally collect
6,714 Chinese target keywords, and generate 718,151 misspelling
candidates. A detailed breakdown of the per-category statistics is
shown in Table I. The ﬁrst column is the names of the categories,
the second column shows the numbers of the collected target
keywords of English, and the sixth column shows the counts of
the target terms of Chinese. We will describe the other columns
of the table in Section VI.
Keyword collection based on Alexa top list. In domain
typosquatting attacks, cybercriminals target names of popular
websites [44, 45]. Similarly, we include the top names of Alexa
domain list [46] in our analysis. Because it is difﬁcult to ﬁnd a
counterpart list for Chinese, we only collected the Alexa top
list for English analysis. Table II shows the statistics of Alexa
top 100, 1,000, and 10,000 names respectively. The second
column represents the numbers of the generated misspelling
candidates that we search on Google. For Alexa top 1,000 terms,
we use brute-force search results of misspelling candidates for
comprehensive analysis and evaluation of RNN performance
(Section V-B). To examine the long-tail effect [47], we also
consider the Alexa top 10,000 domains, which lead to 2,105,218
misspelling candidates. However, it is inefﬁcient to exhaustively
crawl all these keywords. Instead, we deploy the RNN approach
that we design in Section IV to identify keywords likely to cause
linguistic collision and not to be auto-corrected by Google.
1317
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:46:50 UTC from IEEE Xplore.  Restrictions apply. 
Category
Drugs
Adult Terms
Gambling
Software
Cars
Food
Jewelry
Women’s Clothing
Men’s Clothing
Cosmetics
Baby Products
Daily Necessities
Defense Contractors
#
Target
205
214
192
288
68
98
49
43
55
47
46
126
89
English
%
Corrected
% Non-Auto-
4.59% (2.6K)
# Misspell
Candidates
Poisoning
57,255
1.95% (51)
73,089 37.57% (27.5K) 3.47% (950)
7.33% (5.8K) 2.88% (168)
79,464
126,622
6.96% (8.8K)
0.57% (50)
0.68% (13)
11.40% (1.9K)
16,675
0.38% (14)
8.49% (3.7K)
43,668
0.19% (3)
9.53% (1.6K)
16,613
0.59% (7)
8.33% (1.2K)
14,235
18,781
9.99% (1.9K)
0.43% (8)
0.50% (5)
5.72% (1.0K)
17,706
0.32% (7)
14.09% (2.2K)
15,484
0.54% (14)
6.10% (2.6K)
42,638
40,984
6.65% (2.7K)
0.70% (19)
#
Target
46
181
42
700
1,767
1,738
148
199
440
439
394
620
—-
Chinese
%
Corrected
% Non-Auto-
# Misspell
Candidates
Poisoning
3,738
11.85% (443) 3.61% (16)
32,047 11.41% (3.7K) 2.71% (99)
1,951
2.54% (9)
18.14% (354)
84,008
6.29% (5.3K) 0.72% (38)
218,697 4.74% (10.4K) 0.94% (97)
159,825 6.62% (10.6K) 0.87% (92)
24,956
6.17% (1.5K) 0.97% (15)
25,365 10.18% (2.6K) 0.74% (19)
40,903
8.85% (3.6K) 1.00% (36)
6.86% (5.2K) 0.75% (39)
75,844
6.62% (3.4K) 0.93% (32)
51,935
8.92% (6.1K) 0.76% (46)
68,176
—-
—-
—-
Table I: Detailed breakdown of per-category collection statistics. “# Target” is the number of original terms used to generate misspellings for that
category, “# Misspell Candidates” is the number of generated misspelling variants of the target keywords. “% Non-Auto-Corrected” is calculated