most active when they have just joined, and they contact
many other nodes [22]. In particular, every time a new node
u contacts a node v, v suggests the set of mirrors that works
well for itself to u. If u cannot obtain any recommendations,
she will randomly select mirrors from her contacts.
However, a user should not use the bootstrapping mode
for too long. A mirror w suggested by v might not be a good
choice for u for various reasons. Node behaviour in OSNs is
heterogenic (e.g., w.r.t. online time [18]) and w is probably
not the best ﬁtting node for u. Moreover, w might not be
willing to store data for u in the ﬁrst place, or an attacker
could fake recommendations to lure others into storing their
data at her site.
4.4 Mirror Candidate Ranking in the Regular
Mode
SOUP’s regular mode makes use of knowledge that a node
does not have while bootstrapping, but can obtain after it
has established social relations with other users. It will then
leverage their observations to rank mirror candidates.
As illustrated in Fig. 3, a node u in regular mode main-
tains two data structures: a knowledge base (KB) and
experience sets (ES). In the knowledge base, every entry
is about a node that u knows. With regard to an entry for
node v, if v is a mirror of u, u will record an experience
value (expv) based on u’s friends’ experience regarding v in
the KB. A node w is friends with u if there is an edge (u, w)
in the OSN’s social graph G, which represents a social con-
nection between both nodes. The experience value is the
basis for ranking mirrors. In addition, the entry for v will
record whether or not v is friends with u and a TTL (time-
to-live) value that decreases every time u does not choose
v as a mirror (TTL not shown in Fig. 3). Also, for every
node w that is a friend of u, u records an experience set
ESu(w) as shown in Fig. 4. This set records u’s observa-
tions of w’s mirrors; that is, when requesting w’s data (Step
1 in Fig. 4), u records whether or not the data is available at
w’s mirrors (see Fig. 3b). It will then periodically transmit
its experiences to w (Step 2). Besides conﬁning overhead, we
limit the experience set exchange to friends for two further
reasons: First, users request the their friends’ proﬁles more
often than those of strangers. This way, they can record
experience sets on the ﬂy when requesting the data anyway.
Second, this limitation raises the bar for malicious nodes
(a) Initial State
(c) After Exchange of Experience Sets
Figure 3: Maintenance of knowledge base KB (top table) and experience sets ES (bottom table) at node u. Initially, u only knows one
node (node w in (a)), which is also friends with u (i.e., sr(u, w) = 1). As u learns about new nodes, it adds them to KBu (e.g., x, y in
(b)). For each friend, node u further observes the performance of the friend’s mirrors and records its experiences in ESu(f riend) (e.g.,
w in (b)). u also receives ESj (u) from each friend j, allowing u to calculate the experience ranking for each node in KBu (c). As u
continues to record its own experiences for friend nodes (c), node w has replaced node v2—which u had rated low—with node v4.
(b) First Observation Recordings
Algorithm 1 Mirror Selection at Node u
Mu: set of u’s mirrors, initially empty
Cu: a ranked list of mirror candidates
rv: a candidate v’s ranking value
# Select nodes from Cu
perr ← 1
while perr >  do
add next top ranked element v from Cu to Mu
perr = perr · (1 − rv)
end while
# Apply social ﬁlter to nodes in Mu
# (sr(u, v)=1 if u is friends with v; 0 otherwise.)
for all v ∈ Mu that sr(u, v) = 0 do
if ∃ v(cid:48) ∈ (KBu − Mu) such that
sr(u, v(cid:48)) = 1 and rv(cid:48) · β > rv then
replace v with v(cid:48) in Mu
end if
end for
# Prevent overlooking better nodes
add to Mu a random node v(cid:48)(cid:48)
return Mu
fer exceptional storage capacities and online time to get se-
lected as a mirror by many users, just to disappear later.
Or, the quality of a mirror could suddenly deteriorate be-
cause of accidental reasons like connectivity problems. Ap-
plying the aging factor supports quick adaption to such sit-
uations. However, α should also not be over-valued, since a
performance degradation can be temporary as well. When
evaluating α, we found that observing only the most recent
observations might in fact lead to unstable mirror sets. Set-
ting α = 0.75 provided us with the best trade-oﬀ between
adaptation and stability in our experiments.
4.5 Choosing Mirrors from the Ranking
Once a node obtains the ranking of mirror candidates from
either mode, it selects its mirrors from the candidates, as
depicted in Algorithm 1. It has a target error rate, , such
that the probability of her data being unavailable is less
than . First, it adds the top-ranked candidate nodes to its
mirror set one by one, until the estimated likelihood of the
data not being available is less than a target error rate :
perr =
(1 − ri)  1.
(3)
Friend nodes will thus move up in the ranking and can even
replace some unrelated nodes as mirrors. The usage of this
social incentive, however, must not be over-stretched. Eval-
uating β shows that a friend has to provide at least 80% per-
formance of unrelated mirrors (β ≈ 1.25)in order to oﬀer the
best availability and overhead, i.e., it cannot be signiﬁcantly
inferior to the unrelated nodes. Note that, in contrast to re-
lated works, nodes with few or low-ranked friends are not
discriminated by the social ﬁlter and can still achieve high
availability. The ﬁlter is rather an option for those nodes
with highly ranked befriended mirror candidates. Finally,
u adds to its mirror set a random node for which it has not
yet determined a ranking. This way, u prevents a possible
overlooking of even better suited nodes.
4.6 Protective Dropping
A mirror node v may not always have enough space to
store the data for another node, say u (e.g., if v is a popular
mirror that wants to prevent getting overloaded with storage
requests). While v can simply neglect u’s storage request,
alternatively, v can also drop another node’s data to make
more space for u. This will not only provide more ﬂexibility,
it will also enable v to choose what data to store.
also learns which nodes store their data at u.
If a miscreant orchestrates a sybil attack and ﬂoods the
OSN with a large amount of storage requests, v may quickly
ﬁll up its storage space. Therefore, in SOUP, each mirror
node v implements a dropping policy that favors friends. As
malicious identities usually have diﬃculties establishing so-
cial connections to regular nodes [24], v can drop the proﬁles
of the malicious node, leaving space for the data of friends.
On the downside, this practice would discriminate honest
nodes without or with few friends, since these nodes need to
rely on non-friend nodes. Therefore, for each node w that
stores its data at node v, v calculates a dropping score, dw,
for w’s data as follows (with notations given in Table 2):
• As v exchanges experience sets with each friend, say u, it
• If w also stores its data at u, we increase dw by 1. To
protect the data of friends, their score is decreased by 1/β
(recall that we use β ≈ 1.25). If w is a ﬂooder and tries to
store its data on as many nodes as possible, dw will then be
high, and w’s data will incur a high dropping probability.
(Also, consider two benign nodes w, w(cid:48) where w has a
larger mirror set. Since v generally contributes less to the
overall availability of w than to that of w(cid:48), dropping the
data of w has less impact than dropping the data of w(cid:48).)
• If v observes a copy of w’s data in itself, but v is not
listed in w’s published mirror set, it increases dw by a large
constant c, as such a mismatch between the announced
(e.g., published in the DHT) and the real mirror set may
indicate a ﬂooding attempt.
• If dw reaches a threshold θ, node v then blacklists w from
storing its data on v.
The threshold can vary depending on the willingness to
avoid false positives, which can occur due to network errors,
e.g., an error when publishing a new set of mirrors. Our
experiments provided the best results with a three-strike
principle, in which θ = 300 and c = 100. Thus, a node w
will be blacklisted at v after v observed three mismatched
mirror sets. In our evaluation (Sec. 5), we will show that
Table 2: Protective Dropping Notations.
Table 3: Datasets for SOUP Evaluation [25, 26]
this mechanism eﬀectively protects SOUP against attackers
who try to ﬂood the system.
5. SIMULATION AND ANALYSIS
We start our evaluation with a large-scale simulation of our
data replication scheme, with regards to the challenges listed
in Sec. 4.1. Our experiments with three real-world datasets
show that SOUP provides high data availability with low
overhead, and does so for all nodes in the OSN. SOUP per-
forms even better when altruistic nodes exist, and success-
fully copes with node churn and malicious attacks.
5.1 Metrics, Datasets, and Methodology
We ﬁrst deﬁne two basic performance metrics:
• Data availability at time t: The ratio of the number of
users whose data is available at time t to the total number
of users in the OSN.
• Replica overhead at time t: The average number of
replicas each OSN node has at time t.
We then use these two performance metrics to measure the
robustness, openness, and resiliency of SOUP:
• Robustness. SOUP’s ability to provide high performance
to all nodes in an OSN, regardless of a node’s online time,
social relations and device capability.
• Openness. SOUP’s ability to increase performance when
• Resiliency. SOUP’s ability to maintain its performance
altruistically provided resources are available.
when facing adverse scenarios.
We use three diﬀerent large-scale datasets to evaluate SOUP
as listed in Table 3. These datasets cover a variety of real-
world social graph features and can help evaluate SOUP’s
performance in diﬀerent contexts. For instance, users should
not depend on their number of friends, which is why we
chose the less-connected Epinions dataset with an average
node degree of only 17% of that in the Facebook dataset. We
run simulations of SOUP using these datasets, and measure
the metrics deﬁned above. We further handle the following
parameters associated with each user:
Target error rate . We assume every user deﬁnes her
target error rate as 0.01; i.e., every user aims at a 99% like-
lihood of her data being available (Sec 4.5).
Node online probability. For every node, we must
know if it is online at any given time to determine if a user’s
data is available at this node. Node online time is based
vNode at which storage is exhaustedSymbolMeaningwNode that has stored a replica at vdwDropping score for replica of node wßSocial filterƟBlacklisting thresholdcMismatch increase (constant)Facebook90,269OSNNodesEpinions75,879Slashdot82,1693,646,662Edges508,837948,46440.40Avg. Degree6.7111.54Figure 5: SOUP achieves high availability with low overhead.
Figure 6: SOUP proves to be stable, and 90% of the users do not
have to store more than 7 replicas.
on bursty interaction patterns of users and typically follows