### 3.2 Identifying and Associating Addresses

We identify addresses that are served by at least one major or local ISP according to the FCC’s data, as well as addresses served by at least one major ISP. Each remaining address is associated with a census block using the address’s NAD (National Address Database) location and U.S. Census Bureau shape data, accessed via the FCC Area API [69]. This methodology provides high confidence in the resulting addresses, although the dataset may be biased toward urban areas. Rural addresses may not receive residential mail (e.g., rural routes) or may be less likely to appear in USDOT or USPS data. To our knowledge, this is the first study to develop a residential address list based on the NAD; future work could explore additional methodological refinements.

### 3.3 Reverse Engineering Broadband Availability Tools (BATs)

Next, we reverse engineer the BATs for the ISPs under study. We send test address queries to the BATs to understand their functionality and develop a preliminary taxonomy mapping BAT responses to coverage outcomes. We then build a client for querying the BATs.

#### Handling Address Suffixes
In the NAD, for example, “ALLEY” might appear as “ALLY” or “ALY.” We address this issue by substituting the correct suffix based on keyword matching.

### 3.4 Manual Reverse Engineering of BATs

We identify the public BAT for each major ISP by navigating its website. We input test queries for various coverage outcomes: addresses that are residential and covered, residential and not covered, and nonexistent. We log HTTP(S) traffic for each query and manually identify the sequence of requests and responses that begins with the query address and ends with the BAT response.

#### Implementation Details
BAT implementation details vary by ISP. Some BATs are RESTful APIs, while others are ordinary webpages. Some BATs require a multi-step querying process, where the browser issues a request with an address, receives a response with an ID for that address, and then issues further requests with the ID. Some BATs also require a session cookie from a previous webpage. We describe particularly unusual BAT behaviors in Appendix D.

### 3.5 Building a BAT Client

We build a Python client for each BAT that submits queries and parses responses based on our reverse engineering and BAT response taxonomy. The client stores the query address and either a response type (if parsing succeeds) or an error (if parsing fails) in a MySQL database.

#### Parsing Additional Information
Four ISP BATs (AT&T, CenturyLink, Consolidated, and Windstream) provide speed tier data. The client parses and stores this data, which we use to evaluate coverage overstatements by connection speed (Section 4.2).

#### Handling Response Addresses
The BATs for four ISPs (AT&T, CenturyLink, Charter, and Verizon) also respond with an address. The client parses the response address, and if it does not match the query address (e.g., the BAT substituted a similar but distinct address), the client categorizes the response as an unknown type.

### 3.6 Handling Apartment Units

BATs differ in how they handle apartments. For example, the same unit might appear as “APT 15G,” “#15G,” or “15 G” across ISPs. We test apartment addresses during our reverse engineering and incorporate logic for handling apartments into the BAT client. When a BAT prompts for a unit number, it includes a list of suggestions. The client parses this list and randomly selects a unit, assuming broadband availability is uniform within the building.

### 3.7 Querying BATs at Scale

We use our BAT client to collect coverage data for the 19.4 million residential addresses in our dataset that are covered by at least one major ISP (Table 1). The client issues queries for combinations of a major ISP and an address that are covered according to the FCC’s data, totaling nearly 35 million queries (Appendix F). When the client encounters a response it cannot parse, we iteratively add the new response type to our taxonomy and re-query coverage for the address.

#### Data Collection Period
The data collection period for our study is January through August 2020. We rate-limit BAT queries to ensure our data collection does not interfere with public availability.

### 3.8 Creating a BAT Response Taxonomy

BAT responses are diverse, and many do not clearly indicate whether there is coverage or reflect an error. We create an initial taxonomy that maps response types to coverage outcomes when reverse engineering each BAT, as described in Section 3.3. When the BAT client encounters a new type of response, we manually inspect the response and begin from a presumption that the information visually presented to the user reflects the coverage outcome. We then submit test queries and reverse engineer how the BAT triggers and handles the response, which can surface additional information indicating a different coverage outcome is appropriate. Finally, we identify unique attributes for the response and integrate parsing for those attributes into the BAT client.

#### Implementation Details
The implementation details for parsing BAT responses vary by ISP. Some BATs are RESTful APIs that return straightforward JSON values, while others are webpages where we identify unique strings or DOM elements for the client to parse.

#### Coverage Outcomes
We map each BAT response type to one of five coverage outcomes: the address is covered, the address is not covered, the address is not recognized, the address is a business, or the response is unknown (i.e., we cannot interpret it). Appendix D provides detail on ISP-specific response interpretation challenges, Appendix E presents our final taxonomy, and Appendix F gives BAT response counts by coverage outcome. Our final taxonomy includes 74 response types across the nine ISPs we study.

### 3.9 Non-Covered Addresses

We can reliably categorize non-covered addresses for seven of the nine major ISPs because there are clearly distinct response types for when the query address is not covered. For the remaining two ISPs (CenturyLink and Cox), we encounter challenges distinguishing non-covered addresses from unrecognized addresses. We infer the distinction in one case based on further examination of the response type (CenturyLink) and in the other case by querying an affiliated availability tool (Cox).

#### Example: CenturyLink
Figure 2 illustrates a pair of response types from CenturyLink. At first glance, both responses appear to indicate that the address is not covered. However, the first response occurs for known nonexistent addresses and consistently appears when the BAT cannot autocomplete an address and has an internal address ID set to null. Based on these factors, we treat the response as an unrecognized address rather than a non-covered address. Our evaluation confirms that many of these addresses are nonexistent rather than non-covered. Appendix G provides screenshots of all CenturyLink BAT response types, further illustrating the challenge in interpreting responses.

### 3.10 Evaluating the BAT Response Taxonomy

We further evaluate two dimensions of our BAT response taxonomy: addresses that are unrecognized and addresses that have a coverage status (i.e., either covered or not covered).

#### Unrecognized Addresses
BAT responses indicating an unrecognized address are common in our dataset—nearly a million address-ISP combinations. An unrecognized address could be a real residence that the ISP serves, but the ISP’s BAT formats the address differently from our client. Alternatively, an unrecognized address could reflect a residence entirely missing from the BAT. We conduct a small-scale manual evaluation to understand the relative frequency of these scenarios.

#### Covered and Non-Covered Addresses
We lack conventional ground truth in our study, measuring what major ISPs represent about service availability for a large set of addresses. We do not measure whether service is actually available, as conducting a rigorous evaluation (i.e., arranging and following through on service appointments for a sample of addresses across the U.S.) would be prohibitively complex.

#### Unknown Responses
We categorize certain response types in our taxonomy as unknown because we cannot map the response to a coverage status. These responses include website errors or instructions to call a telephone number for further information. For two ISPs (Charter and Frontier), we are not able to distinguish between unrecognized addresses and unknown responses. In both cases, we follow our presumption of labeling based on the information provided to the user and treat the response types as unknown.

### 3.11 Limitations

Before turning to the analysis of our dataset, we reemphasize two important limitations of our methodology. First, each step in our methods—especially selecting addresses and creating a BAT response taxonomy—is a possible source of measurement error. Second, BATs are black-box systems from our perspective. We can submit address-level queries and examine the responses, but we do not have certainty about the granularity or on-the-ground accuracy of the coverage databases underlying the BATs.

Nevertheless, we believe that ISP representations about coverage are an important type of ground truth for public policy purposes—especially when coverage is reportedly unavailable. If an ISP informs a prospective customer both online and by telephone that service is unavailable, we hypothesize that the customer will likely take the information at face value and not obtain service.

### 4. Results and Discussion

Based on the coverage dataset we assemble from ISP BAT responses, we examine the extent to which the FCC’s Form 477 data overstates broadband availability across nine states.

#### 4.1 Coverage Overstatements
We begin our analysis with coverage overstatements for each ISP (Section 4.1). Next, we assess speed overstatements for four ISPs where our client collected speed data (Section 4.2). We then examine overstatements at the state level, aggregating across ISPs to understand overstatements of access to any broadband (Section 4.3) and access to competing providers (Section 4.4). Finally, we conduct a regression analysis to understand the relationship between overstatements and rural areas, poverty, and race (Section 4.5).

#### 4.2 Speed Overstatements
We assess speed overstatements for four ISPs where our client collected speed data. This helps us understand the discrepancy between the advertised speeds and the actual speeds available to customers.

#### 4.3 State-Level Overstatements
We examine overstatements at the state level, aggregating across ISPs to understand overstatements of access to any broadband. This provides a broader view of the overall broadband availability and helps identify areas with significant discrepancies.

#### 4.4 Access to Competing Providers
We also examine overstatements of access to competing providers, which is crucial for understanding the competitive landscape and the options available to consumers.

#### 4.5 Regression Analysis
Finally, we conduct a regression analysis to understand the relationship between overstatements and rural areas, poverty, and race. This helps us identify any systemic biases in the data and provides insights into the factors that contribute to broadband overstatement.

### Conclusion

Our study provides a detailed analysis of broadband coverage and overstatement using a large dataset of address-level queries to ISP BATs. While our methodology has limitations, it offers valuable insights into the accuracy of broadband availability data and highlights the importance of ISP representations for public policy. Future work could explore additional methodological refinements and address the limitations identified in our study.