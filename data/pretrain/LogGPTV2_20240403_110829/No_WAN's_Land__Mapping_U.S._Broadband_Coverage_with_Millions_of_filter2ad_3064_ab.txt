Finally, we identify addresses that are served by at least one major
or local ISP according to the FCC’s data, as well as addresses that
are served by at least one major ISP. We associate each remaining
address with a census block using the address’s NAD location and
U.S. Census Bureau shape data (via the FCC Area API [69]).
This methodology gives us high confidence in the resulting ad-
dresses, though we note that the dataset may be biased toward
urban areas. Rural addresses may not receive residential mail (e.g.,
rural routes), for example, or may be less likely to appear in USDOT
or USPS data. Our work is, to our knowledge, the first to develop a
residential address list based on the NAD; future work could explore
additional methodological refinement.
3.3 Reverse Engineering BATs
We next reverse engineer the BATs for the ISPs that we study. We
send test address queries to the BATs to understand how they func-
tion and develop a preliminary taxonomy mapping BAT responses
to coverage outcomes. We then build a client for querying the BATs.
6In the NAD, for example, “ALLEY” might appear as “ALLY” or “ALY.” We address this
issue by substituting in the correct suffix based on keyword matching.
No WAN’s Land: Mapping U.S. Broadband Coverage with Millions of Address Queries to ISPs
IMC ’20, October 27–29, 2020, Virtual Event, USA
Manual Reverse Engineering. We identify the public BAT
for each major ISP by navigating its website. Next, we input test
queries for a set of coverage outcomes: addresses that are residential
and covered, residential and not covered, and nonexistent. We log
HTTP(S) traffic for each query. We then manually identify the
sequence of requests and responses that begins with the query
address and ends with the BAT response.
BAT implementation details vary by ISP. Some BATs are RESTful
APIs, while others are ordinary webpages. We find that some BATs
require a multi-step querying process, where the browser issues
a request with an address, receives a response with an ID for that
address, then issues further requests with the ID. Some BATs require
a session cookie from a previous webpage. We describe particularly
unusual BAT behaviors in Appendix D.
Building a BAT Client. We build a Python client for each BAT
that submits queries and parses responses, based on our reverse
engineering and BAT response taxonomy. The client stores the
query address and either a response type (if parsing succeeds) or
an error (if parsing fails) in a MySQL database.
The client opportunistically parses additional information from
BAT responses. Four ISP BATs (AT&T, CenturyLink, Consolidated,
and Windstream) provide speed tier data. The client parses and
stores that data, which we use to evaluate coverage overstatements
by connection speed (Section 4.2).
The BATs for four ISPs (AT&T, CenturyLink, Charter, and Veri-
zon) also respond with an address. The client parses the response
address, and if it does not match the query address (e.g., the BAT
substituted in a similar but distinct address), the client categorizes
the response as an unknown type.7
Handling Apartment Units. BATs differ in how they handle
apartments. The same unit might, for example, appear as “APT 15G,”
“#15G,” or “15 G” across ISPs. We test apartment addresses during
our reverse engineering, and we incorporate logic for handling
apartments into the BAT client.
When a BAT prompts for a unit number, it includes a list of
suggestions.8 The client parses this list and randomly selects a
unit, making the assumption that broadband availability is uniform
within the building.
3.4 Querying BATs at Scale
We use our BAT client to collect coverage data for the 19.4 million
residential addresses in our dataset that are covered by at least one
major ISP (Table 1). The client issues queries for combinations of a
major ISP and an address that are covered according to the FCC’s
data, totaling nearly 35 million queries (Appendix F).9
When the client encounters a response that it cannot parse, we
iteratively add the new response type to our taxonomy (as described
in the following section), and the client then re-queries coverage
for the address.
The data collection period for our study is January through
August 2020.10 We rate limit BAT queries to ensure that our data
collection does not interfere with public availability.
3.5 Creating a BAT Response Taxonomy
BAT responses are diverse, and many either do not clearly indicate
whether there is coverage or reflect an error. We create an initial
taxonomy that maps response types to coverage outcomes when
reverse engineering each BAT, as described in Section 3.3. When the
BAT client encounters a new type of response, we manually inspect
the response and begin from a presumption that the information
visually presented to the user reflects the coverage outcome. We
then submit test queries and reverse engineer how the BAT triggers
and handles the response, which can surface additional information
that indicates a different coverage outcome is appropriate. Finally,
we identify unique attributes for the response and integrate parsing
for those attributes into the BAT client.
The implementation details for parsing BAT responses vary by
ISP. Some BATs are RESTful APIs, for example, that return straight-
forward JSON values. Other BATs are webpages, where we identify
unique strings or DOM elements for the client to parse.
We map each BAT response type to one of five coverage out-
comes: the address is covered, the address is not covered, the address
is not recognized, the address is a business, or the response is un-
known (i.e., we cannot interpret it). Appendix D provides detail on
ISP-specific response interpretation challenges that we encounter,
Appendix E presents our final taxonomy, and Appendix F gives
BAT response counts by coverage outcome. Our final taxonomy
includes 74 response types across the nine ISPs that we study.
Non-Covered Addresses. We are able to reliably categorize
non-covered addresses for seven of the nine major ISPs, because
there are clearly distinct response types for when the query ad-
dress is not covered. For the remaining two ISPs (CenturyLink
and Cox), however, we encounter challenges distinguishing non-
covered addresses from unrecognized addresses. We are able to
infer the distinction in one case based on further examination of
the response type (CenturyLink) and in the other case by querying
an affiliated availability tool (Cox).
As an illustration of this issue, and more generally the complex-
ity of interpreting BAT responses, Fig. 2 presents a pair of response
types from CenturyLink. At first glance, both responses appear to
indicate that the address is not covered. The first response occurs
for known nonexistent addresses, however, and the response con-
sistently appears when the BAT cannot autocomplete an address
and has an internal address ID set to null. Also, the JavaScript that
triggers the response includes the status string “We were unable
to find the address you provided.” Based on these factors, we treat
the response as an unrecognized address rather than a non-covered
address. Our evaluation of BAT responses in the following section
confirms that many of these addresses are nonexistent rather than
non-covered. Appendix G provides screenshots of all the Centu-
ryLink BAT response types, further illustrating the challenge in
interpreting responses.
7In this step, the BAT client checks the query address against both the response address
and the response address with a normalized street suffix (as described in Section 3.2).
8See Appendix D for discussion of a Cox special case.
9See Appendix A for limited circumstances where we treat major ISPs as local ISPs.
10Our BAT queries began over six months after the reporting date for the Form 477
data we analyze. This difference in time period likely introduces a slight bias against
our analysis identifying understatements. ISP service areas usually expand over time,
so an address might have been an overstatement in mid-2019 but covered by 2020.
397
IMC ’20, October 27–29, 2020, Virtual Event, USA
David Major, Ross Teixeira, and Jonathan Mayer
Figure 2: Two example response types from CenturyLink: 𝐶𝑒0 (left) and 𝐶𝑒3 (right). Both response types appear to indicate that
the ISP does not cover the address. 𝐶𝑒0 is, however, a response for a nonexistent address (“101 FAKE STREET”) while 𝐶𝑒3 is a
response for a manually verified residential address. We conservatively interpret 𝐶𝑒0 as meaning the BAT does not recognize
the address and 𝐶𝑒3 as meaning the address is not covered.
Next, we determine if the address is formatted differently in the
BAT’s database by manually querying the BAT with the address. If
the BAT suggests the address in a format that our BAT client did
not recognize, but that we can verify is the same address (e.g., a
slightly different spelling of the street name or suffix), we request
coverage for the reformatted address.
If a clear coverage status is not available for a reformatted ad-
dress, we continue our evaluation by attempting to identify what oc-
cupies the address. We search real estate websites, property records,
Google Street View, and Google Maps satellite imagery.
We assign one of the following labels to each address in our
evaluation: incorrect format (when the BAT provides a coverage
status for a reformatted address), residence exists (when we confirm
a house or apartment building occupies the address), residence does
not exist (when we confirm there is a non-residential occupant for
the address, such as a business), residence could exist (when we
confirm there is a vacant lot or mobile home at the address, and we
are uncertain if it is currently being used as a residence), and could
not determine (when we fail to find additional information about
the address). Table 2 presents counts by ISP for each label type.
The results of our evaluation are mixed. We find that most un-
recognized addresses reflect real residences, but we cannot obtain
a clear coverage status from the BAT. We also find that many un-
recognized addresses are not, or might not be, an actual residence.
Because of these ambiguities, we conservatively omit unrecognized
addresses from our main analysis in Section 4. We present results
from relaxing this assumption in Appendix I.
Covered and Non-Covered Addresses. In our study, we lack
conventional ground truth: we measure what major ISPs represent
about service availability for a large set of addresses. We do not
measure whether service actually is available, because conducting
a rigorous evaluation (i.e., arranging and following through on
service appointments for a sample of addresses across the U.S.)
would be prohibitively complex.
There are many reasons why an ISP’s BAT might not accurately
reflect service availability. The latest local coverage data might not
have propagated to the national BAT, for example, or coverage
might only be available after further evaluation by a local service
center. It is also possible that the ISP’s BAT simply contains erro-
neous data for an address.
Unknown Responses. We categorize certain response types in
our taxonomy as unknown, because we cannot map the response to
a coverage status. These responses are website errors, for example,
or instructions to call a telephone number for further information.
For two ISPs (Charter and Frontier), we are not able to distin-
guish between unrecognized addresses and unknown responses:
we find that querying with nonexistent addresses results in either a
generic request to call customer service (Charter) or a generic error
(Frontier). In both cases, we follow our presumption of labeling
based on the information provided to the user, and we treat the
response types as unknown.
A limitation in our BAT client also requires us to categorize
certain Charter responses as unknown, even though the website
might have shown a different coverage outcome to the user. We
built the BAT client to parse key coverage fields in an API, and we
subsequently found that when the key fields were absent, the BAT
could still visually present coverage or non-coverage to the user.
This finding indicates that our BAT client did not fully parse the
information available in Charter BAT responses. Because our BAT
client did not store information beyond the main coverage fields,
we treat all responses missing the fields as unknown.
As noted in Section 3.3, we treat a mismatch between the query
and the response address (when available) as an unknown response.
3.6 Evaluating the BAT Response Taxonomy
We further evaluate two dimensions of our BAT response taxonomy:
addresses that are unrecognized, and addresses that have a coverage
status (i.e., either covered or not covered).
Unrecognized Addresses. BAT responses indicating an un-
recognized address are common in our dataset—nearly a million
address-ISP combinations. An unrecognized address could be a real
residence that the ISP serves, but the ISP’s BAT formats the address
differently from our client. Alternatively, an unrecognized address
could reflect a residence that is entirely missing from the BAT.11
An unrecognized address also might not be a residence at all.
We conduct a small-scale manual evaluation to understand the
relative frequency of these scenarios. We begin by randomly sam-
pling 40 unrecognized addresses for each major ISP, with the ex-
ception of Charter and Frontier (because those ISPs have no BAT
responses in our taxonomy that map to an unrecognized address).
11We hypothesize these addresses are likely not covered. We do not, however, conduct
further evaluation on the subset of unrecognized addresses we verify are residences.
398
No WAN’s Land: Mapping U.S. Broadband Coverage with Millions of Address Queries to ISPs
IMC ’20, October 27–29, 2020, Virtual Event, USA
ISP
AT&T
CenturyLink
Comcast
Consolidated
Cox
Verizon
Windstream
Total
Incorrect
Format
0 (0%)
7 (17.5%)
0 (0%)
3 (7.5%)
3 (7.5%)
9 (22.5%)
0 (0%)
22 (7.9%)
Residence
Residence
Residence
Cannot
Exists Does Not Exist Could Exist Determine
1 (2.5%)
4 (10%)
1 (2.5%)
2 (5%)
4 (10%)
2 (5%)
3 (7.5%)
17 (6.1%)
30 (75%)
7 (17.5%)
3 (7.5%)
4 (10%)
3 (7.5%)
9 (22.5%)
2 (5%)
58 (20.7%)
0 (0%)
5 (12.5%)
4 (10%)
5 (12.5%)
2 (5%)
0 (0%)
4 (10%)
20 (7.1%)
9 (22.5%)
17 (42.5%)
32 (80%)
26 (65%)
28 (70%)
20 (50%)
31 (77.5%)
163 (58.2%)
Table 2: Results from a small-scale manual evaluation of 𝑁 = 40 unrecognized addresses per ISP (Section 3.6). Charter and
Frontier are absent because our taxonomy does not include unrecognized address responses for those ISPs (Section 3.5).
Our taxonomy of BAT responses, described in the prior section,
is an additional possible source of error. The BAT may itself be
accurate, but how we interpret the BAT’s output could be mistaken.
We conducted another small-scale manual evaluation to address
these possible issues.12 Every major ISP in our study is reachable
by telephone, and attempting to arrange service by phone provides
another source of coverage data.
For each ISP, we randomly sampled a minimum of 8 addresses
from our BAT response dataset: 4 addresses that were covered and
4 that were not covered.13 We then attempted to identify the tele-
phone number for a local service center or store for each address,
and when we could not (or when local representatives were un-
available), we fell back to a national sales number. When we called
for each address, we requested service and noted the response.
For 5 of the 9 major ISPs in our study, we did not find a single in-
stance where the coverage status offered by phone differed from the
coverage status returned by the BAT. Multiple ISP representatives
noted that we should just check the ISP’s website for coverage.
In our calls to Comcast, 2 of 6 covered addresses required further
evaluation by a local service center. 2 of 9 non-covered addresses
were actually served according to a representative, but there was an
unpaid balance at each address. Neither representative could explain
why an unpaid balance caused the BAT to report no coverage.
For Cox, a representative responded that a local service center
would have to follow up on 3 of the 4 non-covered addresses.
In our calls to Charter, a representative reported that a local
service center would have to evaluate 1 of 4 non-covered addresses.
For Consolidated, a representative indicated that service was
available at 1 of the 4 non-covered addresses.
In total, we checked 83 addresses by telephone. The response we
received by phone matched the coverage outcome in our dataset
for 74 addresses (89%), and the response by phone disagreed with
our dataset (as opposed to requiring follow-up) for only 3 addresses
(4%). The results from our evaluation give us general confidence
that our taxonomy correctly interprets BAT responses and that our
dataset is consistent with the representations ISPs make by phone.
Our evaluation has several limitations. The number of addresses
that we test is relatively small, since placing telephone calls is
time-consuming. It is also likely that some ISPs share an address
database between their website and their telephone representatives,
so placing calls is not an independent measurement. Furthermore,
telephone responses still are not conventional ground truth—they
are another type of representation to prospective customers.
3.7 Limitations
Before turning to analysis of our dataset, we reemphasize two
important limitations of our methodology. First, each step in our
methods—especially selecting addresses and creating a BAT re-
sponse taxonomy—is a possible source of measurement error. We
discuss and evaluate these issues in the prior section.
Second, BATs are black-box systems from our perspective. We
can submit address-level queries and examine the responses. But
we do not have certainty about the granularity or on-the-ground
accuracy of the coverage databases underlying the BATs.
Nevertheless, we believe that ISP representations about coverage
are an important type of ground truth for public policy purposes—
especially when coverage is reportedly unavailable. If an ISP in-
forms a prospective customer both online and by telephone that
service is unavailable, we hypothesize that the customer will likely
take the information at face value and not obtain service.
4 RESULTS AND DISCUSSION
Based on the coverage dataset we assemble from ISP BAT responses,
we examine the extent to which the FCC’s Form 477 data overstates
broadband availability across nine states.14
We begin our analysis with coverage overstatements for each
ISP (Section 4.1). Next, we assess speed overstatements for four
ISPs where our client collected speed data (Section 4.2). We then ex-
amine overstatements at the state level, aggregating across ISPs to
understand overstatements of access to any broadband (Section 4.3)
and access to competing providers (Section 4.4). Finally, we con-
duct a regression analysis to understand the relationship between
overstatements and rural areas, poverty, and race (Section 4.5).
12We sought and obtained approval from the Princeton University Institutional Review
Board before conducting our evaluation of covered and non-covered addresses.
13We sampled 15 addresses for Comcast (6 covered and 9 not covered) because of the
responses we received during our evaluation, 10 for AT&T and Verizon (5 covered and
5 not covered), and 8 (4 covered and 4 not covered) for the remaining ISPs.
14Note that our methodology precludes discovery of potential coverage underreporting
(i.e., census blocks that ISPs should have reported as covered but did not), since we
only query an ISP’s BAT for addresses that are covered according to Form 477 data.
We present a small-scale evaluation of possible underreporting in Appendix L.
399
IMC ’20, October 27–29, 2020, Virtual Event, USA
David Major, Ross Teixeira, and Jonathan Mayer
ISP
AT&T
CenturyLink
Charter
Comcast
Consolidated
Cox
Frontier
Verizon
Windstream
Total
Residential Addresses Covered by
Population Covered by
BATs
3,894,238
3,587,448
306,790
1,521,772
846,600
675,172
8,495,464
6,941,933
1,553,531
3,567,459
3,169,744
397,715
392,824
200,376
192,448
1,095,762
1,033,844
61,918
1,045,938
692,988
352,950
6,915,507
6,520,060
395,447
451,512
301,199
150,313
Provider ≥ 0 Mbps
FCC
4,516,190
4,003,429
512,761
1,643,526
867,774
775,752
8,680,140
7,027,339
1,652,801
3,645,212
3,217,999
427,213
433,078
207,209
225,869
1,132,153
1,061,582
70,571
1,125,636
715,621
410,015
8,015,081
7,146,747
868,334
475,527
311,063
164,464
—
—
—
𝐵𝐴𝑇 𝑠
𝐹𝐶𝐶
86.23%
89.61%
59.83%
92.59%
97.56%
87.03%
97.87%
98.78%
93.99%
97.87%
98.50%
93.10%
90.71%
96.70%
85.20%
96.79%
97.39%
87.74%
92.92%
96.84%
86.08%
86.28%
91.23%
45.54%
94.95%
96.83%
91.40%
— 92.29%
— 94.85%
— 80.00%
Area
All
Urban
Rural
All
Urban
Rural
All
Urban
Rural
All
Urban
Rural
All
Urban
Rural
All
Urban
Rural
All
Urban
Rural
All