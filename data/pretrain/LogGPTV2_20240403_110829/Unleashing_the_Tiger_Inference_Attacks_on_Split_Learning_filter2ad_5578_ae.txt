signs the label 𝑦˜𝑡 to these instances and uses them to train the
model 𝐶 according to the learning protocol. Once the clients have
contributed their training parameters, the attacker downloads the
updated model 𝐶 from the server and uses it as the discrimina-
tor [23] to train the generative model 𝐺. The confidence of 𝐶 on
the class 𝑦𝑡 is used as the discriminator’s output and maximized
in the loss function of 𝐺. Once the generator has been trained, the
attacker can use it to reproduce suitable target class instances 𝑦𝑡.
Algorithm 1: Client-side attack [30] in split learning.
Data: Number of training iterations: 𝑁 , Target class: 𝑦𝑡,
Dummy class for poisoning 𝑦˜𝑡, Scaling factor
gradient: 𝜖
/* Initialize the local generative model
1 𝐺 = initGenerator();
2 for 𝑖 in [1, 𝑁] do
/* Download updated network splits
𝑓 , 𝑓 ′ = get_models();
/* Alterning poisoning attack and adversarial training
/* (a more sophisticated scheduler may be used)
if 𝑖%2 == 0 then
𝑝𝑜𝑖𝑠𝑜𝑛𝑖𝑛𝑔 = 𝑇𝑟𝑢𝑒;
else
𝑝𝑜𝑖𝑠𝑜𝑛𝑖𝑛𝑔 = 𝐹𝑎𝑙𝑠𝑒
/* —- Start distributed forward-propagation
/* Sample data instances from the generator 𝐺
𝑥 ∼ 𝐺;
𝑧 = 𝑓 (𝑥);
/* Send smashed data to the server and get 𝑠(𝑓 (𝑥)) back
𝑧′ = send_get_forward(z);
𝑝 = 𝑓 ′(𝑧′);
if 𝑝𝑜𝑖𝑠𝑜𝑛𝑖𝑛𝑔 then
/* Apply final layers and compute the probability for each class
/* Dummy label
𝑦 = 𝑦˜𝑡;
else
/* Target label
𝑦 = 𝑦𝑡;
/* Compute loss
L = cross-entropy(𝑦, 𝑝);
/* —- Start distributed back-propagation
/* Compute local gradient until 𝑠
∇𝑓 ′ = compute_gradient(𝑓
if not 𝑝𝑜𝑖𝑠𝑜𝑛𝑖𝑛𝑔 then
/* Scale down gradient
′
, L);
∇𝑓 ′ = 𝜖 · ∇𝑓 ′ ;
else
′
/* Apply gradient on 𝑓
𝑓
= apply(𝑓
′
′
, ∇𝑓 ′)
/* Send gradient to the server and receive gradient until 𝑓
∇𝑠 = send_get_gradient(∇𝑓 ′);
if not 𝑝𝑜𝑖𝑠𝑜𝑛𝑖𝑛𝑔 then
/* Scale back gradient
∇𝑠 = 1
𝜖 · ∇𝑠;
/* Compute local gradient until 𝐺
∇𝑓 = compute_gradient(𝑓 , ∇𝑠);
if 𝑝𝑜𝑖𝑠𝑜𝑛𝑖𝑛𝑔 then
/* Apply gradient on 𝑓
𝑓 = apply(𝑓 , ∇𝑓 )
else
/* Compute local gradient until 𝐺’s input
∇𝐺 = compute_gradient(𝐺, ∇𝑓 );
𝐺 = apply(𝐺, ∇𝐺)
/* Apply gradient on the generator
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
*/
*/
*/
*/
*/
*/
*/
*/
*/
*/
*/
*/
*/
*/
*/
*/
*/
*/
*/
*/
*/
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2123(a) MNIST 𝑦𝑡 = 1
(b) MNIST 𝑦𝑡 = 2
(c) MNIST 𝑦𝑡 = 3
(d) AT&T 𝑦𝑡 = 1
(e) AT&T 𝑦𝑡 = 2
(f) AT&T 𝑦𝑡 = 3
Figure 12: Results from the client-side attack performed on
split learning. The images are random samples from the gen-
erator trained via Algorithm 1 on three attacks with different
target classes. For the results on the dataset AT&T, we report
also an instance of the target class in the leftmost corner of
the panel in a gray frame.
of 𝑠 is performed by the server and cannot be directly prevented by
the malicious client.9
The gradient-scaling trick. Nevertheless, this limitation can be
easily circumvented by manipulating the gradient sent and received
by the server during the split learning protocol. In particular, the
malicious client can resort to gradient-scaling to make the training
operation’s impact on 𝑠 negligible. Here, before sending the gra-
′ to 𝑠, the client can multiply ∇𝑓 ′ by a
dient ∇𝑓 ′ produced from 𝑓
very small constant 𝜖; that is:
∇𝑓 ′ = 𝜖 · ∇𝑓 ′ .
(5)
This operation makes the magnitude of ∇𝑓 ′ , and so the magnitude
of the weights update derived from it on 𝑠, negligible, thus pre-
venting any functional change in the weights of 𝑠. Ideally, this
is equivalent to force the server to train 𝑠 with a learning rate close
to zero.
Then, once 𝑠 has performed its back-propagation step and sent
the gradient ∇𝑠 to 𝑓 , the malicious client scales back ∇𝑠 to its
original magnitude by multiplying it by the inverse of 𝜖; that is:
· ∇𝑠 .
1
𝜖
∇𝑠 =
(6)
This allows the attacker to recover a suitable training signal for
the generator 𝐺 that follows the back-propagation chain. Note that
the malicious client does not update the weights of 𝑓 or those of 𝑓 ′
in the process. Eventually, the gradient-scaling operation allows the
malicious client to train the generator using the distribute model 𝐶
as a discriminator. We demonstrate the soundness of this procedure
later in this section.
Although the gradient-scaling trick may provide a cognizant
server an easy way to detect the attackers, a malicious client can
always find a trade-off between attack secrecy and attack perfor-
mance by choosing suitable assignments of 𝜖. As a matter of fact, it
is hard for the server to distinguish the scaled gradient from the
one achieved by a batch of easy examples (that is, data instances
that the model correctly classifies with high confidence.)
5.1 Client-side Attack on Split Learning
The attack [30] can be performed on split learning under the same
threat model. Note that, in this setup, the split learning server
is honest, whereas the malicious client does not know the data
distribution of the other clients’ training sets.
Considering the private-label case (i.e., Figure 1b), a malicious
client exerts a strong influence over the learning process of the
′(𝑠(𝑓 (·)) and can set up an attack similar to the
shared model 𝐶 = 𝑓
one performed on federated learning. Here, the attacker trains a
′(𝑠(𝑓 (·))) as the
generator 𝐺 by using the distributed model 𝐶 = 𝑓
discriminator by just providing suitable pairs (input, label) during
the split learning protocol. This attack procedure is summarized in
Algorithm 1. During the attack, the only impediment is the limited
control of the attacker on the weights update procedure of the
network 𝑠 hosted by the server. Indeed, to soundly train the
generator using the adversarial loss based on the distributed
model 𝐶, the attacker must prevent the update of 𝑠 while
training the generator 𝐺. However, the weights update operation
The poisoning step of the attack [30] can be performed without
any modification. The malicious client has to assign the label 𝑦˜𝑡 to
instances sampled from the generator 𝐺 and run the standard split
learning training procedure. In this process, the attacker updates the
weights of all the participating networks but 𝐺. However, during
the attack, the malicious client must alternate between a poisoning
step and a genuine training iteration for the generator as these
cannot be performed simultaneously due to the gradient-scaling
trick required to train the generator. Alternatively, the attacker can
impersonate an additional client in the protocol and perform the
poisoning iterations separately.
Attack validation. To implement the attack, we rely on architec-
tures and hyper-parameters compatible with those originally used
in [30] and perform the attack on the MNIST and AT&T datasets.
More details are given in Appendix B.1. We use 𝜖 =10−5 in the
“gradient-scaling trick”. In our setup, we model 10 honest clients
and a single malicious client who performs the attack described in
9In this case, the back-propagation is performed client-side, and the malicious
client can explicitly avoid updating the weights.
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2124Algorithm 1. In the process, we use the standard sequential training
procedure of split learning [25]. However, the attack equally applies
to parallel extensions such as Splitfed learning [51]. We run the
attack for 10000 global training iterations. The results are reported
in Figure 12 for three attacks targeting different 𝑦𝑡, and prove the
generator is successfully reproducing instances of the target class.
6 FINAL REMARKS
In the present work, we described various structural vulnerabili-
ties of split learning and showed how to exploit them and violate
the protocol’s privacy-preserving property. Here, an attacker can
accurately reconstruct, or infer properties on, training instances.
Additionally, we have shown that defensive techniques devised to
protect split learning can be easily evaded.
While federated learning exhibits similar vulnerabilities, split
learning appears worse since it consistently leaks more information.
Furthermore, it makes it even harder to detect ongoing inference
attacks. Indeed, in standard federated learning, all participants store
the neural network in its entirety, enabling simple detection mech-
anisms that, if nothing else, can thwart unsophisticated attacks.
ACKNOWLEDGMENTS
We acknowledge the generous support of Accenture and the collab-
oration with their Labs in Sophia Antipolis.
REFERENCES
[1] 2020. OpenMined: SplitNN. https://blog.openmined.org/tag/splitnn/. (2020).
[2] 2021. Workshop on Split Learning for Distributed Machine Learning (SLDML’21).
https://splitlearning.github.io/workshop.html. (2021).
[3] Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov,
Kunal Talwar, and Li Zhang. 2016. Deep Learning with Differential Privacy. In
Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications
Security (CCS ’16). Association for Computing Machinery, New York, NY, USA,
308–318. https://doi.org/10.1145/2976749.2978318
[4] Ali Abedi and Shehroz S. Khan. 2020.
FedSL: Federated Split Learning
(2020).
on Distributed Sequential Data in Recurrent Neural Networks.
arXiv:cs.LG/2011.03180
[5] Sharif Abuadbba, Kyuyeon Kim, Minki Kim, Chandra Thapa, Seyit A. Camtepe,
Can We Use
Yansong Gao, Hyoungshick Kim, and Surya Nepal. 2020.
Split Learning on 1D CNN Models for Privacy Preserving Training? (2020).
arXiv:cs.CR/2003.12365
[6] Adam James Hall. 2020. Split Neural Networks on PySyft. https://medium.com/
analytics-vidhya/split-neural-networks-on-pysyft-ed2abf6385c0. (2020).
[7] George J Annas. 2003. HIPAA regulations - a new era of medical-record privacy?
The New England journal of medicine 348, 15 (April 2003), 1486—1490. https:
//doi.org/10.1056/nejmlim035027
[8] Giuseppe Ateniese, Luigi V. Mancini, Angelo Spognardi, Antonio Villani,
Domenico Vitali, and Giovanni Felici. 2015. Hacking Smart Machines with
Smarter Ones: How to Extract Meaningful Data from Machine Learning Classi-
fiers. Int. J. Secur. Netw. 10, 3 (Sept. 2015), 137–150. https://doi.org/10.1504/IJSN.
2015.071829