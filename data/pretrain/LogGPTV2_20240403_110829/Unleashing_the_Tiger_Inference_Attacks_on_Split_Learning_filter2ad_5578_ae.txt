### Client-Side Attack in Split Learning

In the described attack, the attacker assigns a label \( \tilde{y}_t \) to instances generated by a generative model \( G \) and uses these labeled instances to train a model \( C \) according to the learning protocol. After clients have contributed their training parameters, the attacker downloads the updated model \( C \) from the server and uses it as a discriminator to train the generative model \( G \). The confidence of \( C \) on the target class \( y_t \) is used as the discriminator's output and maximized in the loss function of \( G \). Once the generator \( G \) is trained, the attacker can use it to generate instances of the target class \( y_t \).

#### Algorithm 1: Client-Side Attack in Split Learning

**Data:**
- Number of training iterations: \( N \)
- Target class: \( y_t \)
- Dummy class for poisoning: \( \tilde{y}_t \)
- Scaling factor for gradient: \( \epsilon \)

1. **Initialize the local generative model:**
   \[
   G = \text{initGenerator()}
   \]

2. **For each iteration \( i \) in [1, \( N \)]:**

   - **Download updated network splits:**
     \[
     f, f' = \text{get_models()}
     \]

   - **Alternate between poisoning attack and adversarial training:**
     \[
     \text{if } i \mod 2 == 0 \text{ then} \quad \text{poisoning} = \text{True}
     \]
     \[
     \text{else} \quad \text{poisoning} = \text{False}
     \]

   - **Start distributed forward-propagation:**
     - Sample data instances from the generator \( G \):
       \[
       x \sim G
       \]
     - Compute intermediate representation:
       \[
       z = f(x)
       \]
     - Send the intermediate representation to the server and receive the processed result:
       \[
       z' = \text{send_get_forward}(z)
       \]
     - Compute the final output:
       \[
       p = f'(z')
       \]

   - **Compute the loss:**
     - If poisoning:
       \[
       y = \tilde{y}_t
       \]
     - Otherwise:
       \[
       y = y_t
       \]
     - Compute the cross-entropy loss:
       \[
       L = \text{cross-entropy}(y, p)
       \]

   - **Start distributed back-propagation:**
     - Compute the local gradient until \( s \):
       \[
       \nabla f' = \text{compute_gradient}(f', L)
       \]
     - If not poisoning, scale down the gradient:
       \[
       \nabla f' = \epsilon \cdot \nabla f'
       \]
     - Apply the gradient on \( f' \):
       \[
       f' = \text{apply}(f', \nabla f')
       \]
     - Send the gradient to the server and receive the gradient until \( f \):
       \[
       \nabla s = \text{send_get_gradient}(\nabla f')
       \]
     - If not poisoning, scale back the gradient:
       \[
       \nabla s = \frac{1}{\epsilon} \cdot \nabla s
       \]
     - Compute the local gradient until \( G \):
       \[
       \nabla f = \text{compute_gradient}(f, \nabla s)
       \]
     - If poisoning, apply the gradient on \( f \):
       \[
       f = \text{apply}(f, \nabla f)
       \]
     - Otherwise, compute the local gradient until \( G \)'s input:
       \[
       \nabla G = \text{compute_gradient}(G, \nabla f)
       \]
     - Apply the gradient on the generator:
       \[
       G = \text{apply}(G, \nabla G)
       \]

### Gradient-Scaling Trick

The gradient-scaling trick allows the malicious client to manipulate the gradient sent and received by the server during the split learning protocol. Specifically, before sending the gradient \( \nabla f' \) to the server, the client multiplies it by a very small constant \( \epsilon \):

\[
\nabla f' = \epsilon \cdot \nabla f'
\]

This operation makes the magnitude of \( \nabla f' \) negligible, preventing any significant update to the weights of the server's model \( s \). After the server performs its back-propagation and sends the gradient \( \nabla s \) back to the client, the client scales \( \nabla s \) back to its original magnitude by multiplying it by \( \frac{1}{\epsilon} \):

\[
\nabla s = \frac{1}{\epsilon} \cdot \nabla s
\]

This allows the attacker to recover a suitable training signal for the generator \( G \) while ensuring that the server's model \( s \) remains unchanged. Although this trick may be detectable by a cognizant server, the attacker can balance attack secrecy and performance by choosing an appropriate value for \( \epsilon \).

### Experimental Results

We validate the attack using architectures and hyper-parameters compatible with those in [30] on the MNIST and AT&T datasets. We set \( \epsilon = 10^{-5} \) for the gradient-scaling trick. In our setup, we model 10 honest clients and one malicious client performing the attack described in Algorithm 1. The results, shown in Figure 12, demonstrate that the generator successfully reproduces instances of the target class.

### Conclusion

In this work, we have highlighted various structural vulnerabilities in split learning and demonstrated how to exploit them to violate the protocol's privacy-preserving property. We have also shown that defensive techniques designed to protect split learning can be easily evaded. Compared to federated learning, split learning consistently leaks more information and makes it harder to detect ongoing inference attacks.

### Acknowledgments

We acknowledge the generous support of Accenture and the collaboration with their Labs in Sophia Antipolis.

### References

[1] OpenMined: SplitNN. (2020). https://blog.openmined.org/tag/splitnn/

[2] Workshop on Split Learning for Distributed Machine Learning (SLDML’21). (2021). https://splitlearning.github.io/workshop.html

[3] Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar, K., & Zhang, L. (2016). Deep Learning with Differential Privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (CCS ’16). Association for Computing Machinery, New York, NY, USA, 308–318. https://doi.org/10.1145/2976749.2978318

[4] Abedi, A., & Khan, S. S. (2020). FedSL: Federated Split Learning on Distributed Sequential Data in Recurrent Neural Networks. arXiv:cs.LG/2011.03180

[5] Abuadbba, S., Kim, K., Kim, M., Thapa, C., Camtepe, S. A., Gao, Y., Kim, H., & Nepal, S. (2020). Can We Use Split Learning on 1D CNN Models for Privacy Preserving Training? arXiv:cs.CR/2003.12365

[6] Hall, A. J. (2020). Split Neural Networks on PySyft. https://medium.com/analytics-vidhya/split-neural-networks-on-pysyft-ed2abf6385c0

[7] Annas, G. J. (2003). HIPAA regulations - a new era of medical-record privacy? The New England Journal of Medicine, 348(15), 1486–1490. https://doi.org/10.1056/nejmlim035027

[8] Ateniese, G., Mancini, L. V., Spognardi, A., Villani, A., Vitali, D., & Felici, G. (2015). Hacking Smart Machines with Smarter Ones: How to Extract Meaningful Data from Machine Learning Classifiers. International Journal of Security and Networks, 10(3), 137–150. https://doi.org/10.1504/IJSN.2015.071829