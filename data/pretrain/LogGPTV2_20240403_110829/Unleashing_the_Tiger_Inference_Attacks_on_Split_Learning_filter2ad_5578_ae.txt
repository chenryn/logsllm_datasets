signs the label ğ‘¦Ëœğ‘¡ to these instances and uses them to train the
model ğ¶ according to the learning protocol. Once the clients have
contributed their training parameters, the attacker downloads the
updated model ğ¶ from the server and uses it as the discrimina-
tor [23] to train the generative model ğº. The confidence of ğ¶ on
the class ğ‘¦ğ‘¡ is used as the discriminatorâ€™s output and maximized
in the loss function of ğº. Once the generator has been trained, the
attacker can use it to reproduce suitable target class instances ğ‘¦ğ‘¡.
Algorithm 1: Client-side attack [30] in split learning.
Data: Number of training iterations: ğ‘ , Target class: ğ‘¦ğ‘¡,
Dummy class for poisoning ğ‘¦Ëœğ‘¡, Scaling factor
gradient: ğœ–
/* Initialize the local generative model
1 ğº = initGenerator();
2 for ğ‘– in [1, ğ‘] do
/* Download updated network splits
ğ‘“ , ğ‘“ â€² = get_models();
/* Alterning poisoning attack and adversarial training
/* (a more sophisticated scheduler may be used)
if ğ‘–%2 == 0 then
ğ‘ğ‘œğ‘–ğ‘ ğ‘œğ‘›ğ‘–ğ‘›ğ‘” = ğ‘‡ğ‘Ÿğ‘¢ğ‘’;
else
ğ‘ğ‘œğ‘–ğ‘ ğ‘œğ‘›ğ‘–ğ‘›ğ‘” = ğ¹ğ‘ğ‘™ğ‘ ğ‘’
/* â€”- Start distributed forward-propagation
/* Sample data instances from the generator ğº
ğ‘¥ âˆ¼ ğº;
ğ‘§ = ğ‘“ (ğ‘¥);
/* Send smashed data to the server and get ğ‘ (ğ‘“ (ğ‘¥)) back
ğ‘§â€² = send_get_forward(z);
ğ‘ = ğ‘“ â€²(ğ‘§â€²);
if ğ‘ğ‘œğ‘–ğ‘ ğ‘œğ‘›ğ‘–ğ‘›ğ‘” then
/* Apply final layers and compute the probability for each class
/* Dummy label
ğ‘¦ = ğ‘¦Ëœğ‘¡;
else
/* Target label
ğ‘¦ = ğ‘¦ğ‘¡;
/* Compute loss
L = cross-entropy(ğ‘¦, ğ‘);
/* â€”- Start distributed back-propagation
/* Compute local gradient until ğ‘ 
âˆ‡ğ‘“ â€² = compute_gradient(ğ‘“
if not ğ‘ğ‘œğ‘–ğ‘ ğ‘œğ‘›ğ‘–ğ‘›ğ‘” then
/* Scale down gradient
â€²
, L);
âˆ‡ğ‘“ â€² = ğœ– Â· âˆ‡ğ‘“ â€² ;
else
â€²
/* Apply gradient on ğ‘“
ğ‘“
= apply(ğ‘“
â€²
â€²
, âˆ‡ğ‘“ â€²)
/* Send gradient to the server and receive gradient until ğ‘“
âˆ‡ğ‘  = send_get_gradient(âˆ‡ğ‘“ â€²);
if not ğ‘ğ‘œğ‘–ğ‘ ğ‘œğ‘›ğ‘–ğ‘›ğ‘” then
/* Scale back gradient
âˆ‡ğ‘  = 1
ğœ– Â· âˆ‡ğ‘ ;
/* Compute local gradient until ğº
âˆ‡ğ‘“ = compute_gradient(ğ‘“ , âˆ‡ğ‘ );
if ğ‘ğ‘œğ‘–ğ‘ ğ‘œğ‘›ğ‘–ğ‘›ğ‘” then
/* Apply gradient on ğ‘“
ğ‘“ = apply(ğ‘“ , âˆ‡ğ‘“ )
else
/* Compute local gradient until ğºâ€™s input
âˆ‡ğº = compute_gradient(ğº, âˆ‡ğ‘“ );
ğº = apply(ğº, âˆ‡ğº)
/* Apply gradient on the generator
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
*/
*/
*/
*/
*/
*/
*/
*/
*/
*/
*/
*/
*/
*/
*/
*/
*/
*/
*/
*/
*/
Session 7A: Privacy Attacks and Defenses for ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2123(a) MNIST ğ‘¦ğ‘¡ = 1
(b) MNIST ğ‘¦ğ‘¡ = 2
(c) MNIST ğ‘¦ğ‘¡ = 3
(d) AT&T ğ‘¦ğ‘¡ = 1
(e) AT&T ğ‘¦ğ‘¡ = 2
(f) AT&T ğ‘¦ğ‘¡ = 3
Figure 12: Results from the client-side attack performed on
split learning. The images are random samples from the gen-
erator trained via Algorithm 1 on three attacks with different
target classes. For the results on the dataset AT&T, we report
also an instance of the target class in the leftmost corner of
the panel in a gray frame.
of ğ‘  is performed by the server and cannot be directly prevented by
the malicious client.9
The gradient-scaling trick. Nevertheless, this limitation can be
easily circumvented by manipulating the gradient sent and received
by the server during the split learning protocol. In particular, the
malicious client can resort to gradient-scaling to make the training
operationâ€™s impact on ğ‘  negligible. Here, before sending the gra-
â€² to ğ‘ , the client can multiply âˆ‡ğ‘“ â€² by a
dient âˆ‡ğ‘“ â€² produced from ğ‘“
very small constant ğœ–; that is:
âˆ‡ğ‘“ â€² = ğœ– Â· âˆ‡ğ‘“ â€² .
(5)
This operation makes the magnitude of âˆ‡ğ‘“ â€² , and so the magnitude
of the weights update derived from it on ğ‘ , negligible, thus pre-
venting any functional change in the weights of ğ‘ . Ideally, this
is equivalent to force the server to train ğ‘  with a learning rate close
to zero.
Then, once ğ‘  has performed its back-propagation step and sent
the gradient âˆ‡ğ‘  to ğ‘“ , the malicious client scales back âˆ‡ğ‘  to its
original magnitude by multiplying it by the inverse of ğœ–; that is:
Â· âˆ‡ğ‘  .
1
ğœ–
âˆ‡ğ‘  =
(6)
This allows the attacker to recover a suitable training signal for
the generator ğº that follows the back-propagation chain. Note that
the malicious client does not update the weights of ğ‘“ or those of ğ‘“ â€²
in the process. Eventually, the gradient-scaling operation allows the
malicious client to train the generator using the distribute model ğ¶
as a discriminator. We demonstrate the soundness of this procedure
later in this section.
Although the gradient-scaling trick may provide a cognizant
server an easy way to detect the attackers, a malicious client can
always find a trade-off between attack secrecy and attack perfor-
mance by choosing suitable assignments of ğœ–. As a matter of fact, it
is hard for the server to distinguish the scaled gradient from the
one achieved by a batch of easy examples (that is, data instances
that the model correctly classifies with high confidence.)
5.1 Client-side Attack on Split Learning
The attack [30] can be performed on split learning under the same
threat model. Note that, in this setup, the split learning server
is honest, whereas the malicious client does not know the data
distribution of the other clientsâ€™ training sets.
Considering the private-label case (i.e., Figure 1b), a malicious
client exerts a strong influence over the learning process of the
â€²(ğ‘ (ğ‘“ (Â·)) and can set up an attack similar to the
shared model ğ¶ = ğ‘“
one performed on federated learning. Here, the attacker trains a
â€²(ğ‘ (ğ‘“ (Â·))) as the
generator ğº by using the distributed model ğ¶ = ğ‘“
discriminator by just providing suitable pairs (input, label) during
the split learning protocol. This attack procedure is summarized in
Algorithm 1. During the attack, the only impediment is the limited
control of the attacker on the weights update procedure of the
network ğ‘  hosted by the server. Indeed, to soundly train the
generator using the adversarial loss based on the distributed
model ğ¶, the attacker must prevent the update of ğ‘  while
training the generator ğº. However, the weights update operation
The poisoning step of the attack [30] can be performed without
any modification. The malicious client has to assign the label ğ‘¦Ëœğ‘¡ to
instances sampled from the generator ğº and run the standard split
learning training procedure. In this process, the attacker updates the
weights of all the participating networks but ğº. However, during
the attack, the malicious client must alternate between a poisoning
step and a genuine training iteration for the generator as these
cannot be performed simultaneously due to the gradient-scaling
trick required to train the generator. Alternatively, the attacker can
impersonate an additional client in the protocol and perform the
poisoning iterations separately.
Attack validation. To implement the attack, we rely on architec-
tures and hyper-parameters compatible with those originally used
in [30] and perform the attack on the MNIST and AT&T datasets.
More details are given in Appendix B.1. We use ğœ– =10âˆ’5 in the
â€œgradient-scaling trickâ€. In our setup, we model 10 honest clients
and a single malicious client who performs the attack described in
9In this case, the back-propagation is performed client-side, and the malicious
client can explicitly avoid updating the weights.
Session 7A: Privacy Attacks and Defenses for ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2124Algorithm 1. In the process, we use the standard sequential training
procedure of split learning [25]. However, the attack equally applies
to parallel extensions such as Splitfed learning [51]. We run the
attack for 10000 global training iterations. The results are reported
in Figure 12 for three attacks targeting different ğ‘¦ğ‘¡, and prove the
generator is successfully reproducing instances of the target class.
6 FINAL REMARKS
In the present work, we described various structural vulnerabili-
ties of split learning and showed how to exploit them and violate
the protocolâ€™s privacy-preserving property. Here, an attacker can
accurately reconstruct, or infer properties on, training instances.
Additionally, we have shown that defensive techniques devised to
protect split learning can be easily evaded.
While federated learning exhibits similar vulnerabilities, split
learning appears worse since it consistently leaks more information.
Furthermore, it makes it even harder to detect ongoing inference
attacks. Indeed, in standard federated learning, all participants store
the neural network in its entirety, enabling simple detection mech-
anisms that, if nothing else, can thwart unsophisticated attacks.
ACKNOWLEDGMENTS
We acknowledge the generous support of Accenture and the collab-
oration with their Labs in Sophia Antipolis.
REFERENCES
[1] 2020. OpenMined: SplitNN. https://blog.openmined.org/tag/splitnn/. (2020).
[2] 2021. Workshop on Split Learning for Distributed Machine Learning (SLDMLâ€™21).
https://splitlearning.github.io/workshop.html. (2021).
[3] Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov,
Kunal Talwar, and Li Zhang. 2016. Deep Learning with Differential Privacy. In
Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications
Security (CCS â€™16). Association for Computing Machinery, New York, NY, USA,
308â€“318. https://doi.org/10.1145/2976749.2978318
[4] Ali Abedi and Shehroz S. Khan. 2020.
FedSL: Federated Split Learning
(2020).
on Distributed Sequential Data in Recurrent Neural Networks.
arXiv:cs.LG/2011.03180
[5] Sharif Abuadbba, Kyuyeon Kim, Minki Kim, Chandra Thapa, Seyit A. Camtepe,
Can We Use
Yansong Gao, Hyoungshick Kim, and Surya Nepal. 2020.
Split Learning on 1D CNN Models for Privacy Preserving Training? (2020).
arXiv:cs.CR/2003.12365
[6] Adam James Hall. 2020. Split Neural Networks on PySyft. https://medium.com/
analytics-vidhya/split-neural-networks-on-pysyft-ed2abf6385c0. (2020).
[7] George J Annas. 2003. HIPAA regulations - a new era of medical-record privacy?
The New England journal of medicine 348, 15 (April 2003), 1486â€”1490. https:
//doi.org/10.1056/nejmlim035027
[8] Giuseppe Ateniese, Luigi V. Mancini, Angelo Spognardi, Antonio Villani,
Domenico Vitali, and Giovanni Felici. 2015. Hacking Smart Machines with
Smarter Ones: How to Extract Meaningful Data from Machine Learning Classi-
fiers. Int. J. Secur. Netw. 10, 3 (Sept. 2015), 137â€“150. https://doi.org/10.1504/IJSN.
2015.071829