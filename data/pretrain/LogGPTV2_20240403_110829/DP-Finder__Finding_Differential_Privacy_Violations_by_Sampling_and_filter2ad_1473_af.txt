≤ u
= Pr
(cid:124)(cid:32)(cid:123)(cid:122)(cid:32)(cid:125)
Pr [F (x ) ∈ Φ]
Pr [F (x′) ∈ Φ] ≤ exp(u)
.
w
From this approximate CDF of ϵ (x, x′, Φ), we want to determine
an approximate interval for ϵ (x, x′, Φ) with confidence 1 − α. For-
(cid:20)
(cid:21)
mally, this means we want to select the smallest ∆ϵ such that
(cid:124)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:123)(cid:122)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:125)
≥ 1 − α .
Pr
, Φ) + ∆ϵ
=Pr[ϵ (x,x′,Φ)≤ ˆϵ (x,x′,Φ)+∆ϵ ]−Pr[ϵ (x,x′,Φ)≤ ˆϵ (x,x′,Φ)−∆ϵ ]
, Φ) − ∆ϵ ≤ ϵ (x, x
, Φ) ≤ ˆϵ (x, x
ˆϵ (x, x
′
′
′
F,Φ (x ), . . . , checkn
Because finding an analytic solution for this equation is hard, we
apply binary search to find the smallest ∆ϵ that satisfies it.
Example. We show how to use the improved approach
to estimate ϵ (x, x′, Φ) on some (x, x′, Φ). Assume that we
(cid:80)n
(cid:102)
(cid:103)
have obtained n = 107 samples check1
ance (cid:76)Var
F,Φ (x )
F,Φ (x ) ≈ 3.24% and empirical vari-
with mean 1
i =1 checki
n
≈ 3.13% (computed according to
(cid:80)n
checkF,Φ (x )
(cid:102)
(cid:103)
App. B). Analogously, we have also obtained n samples with
(cid:76)Var
F,Φ (x′) ≈ 3.04% and empirical variance
mean 1
i =1 checki
n
checkF,Φ (x′)
≈ 2.95%. In addition, the empirical correlation
F,Φ (x′) is ρ = 0.97. Based on the
F,Φ (x ) and checki
between checki
samples, we compute ˆϵ (x, x′, Φ) ≈ log 3.24%
We then derive a joint likelihood for Pr [F (x ) ∈ Φ] and
(cid:33)(cid:33)
(cid:32)
Pr [F (x′) ∈ Φ] according to Eq. (10):
3.04% ≈ 6.4%.
(cid:33)
3.24%
0.97 · 3.13% · 2.95%
N
Using Thm. 4.4 with lower bound l = 6.4%− ∆ϵ and upper bound
(2.95%)
u = 6.4% + ∆ϵ for ∆ϵ = 0.02% allows us to derive that:
0.97 · 3.13% · 2.95%
(cid:32)(cid:32)3.04%
(3.13%)
1
n
2
2
,
.
Pr(cid:2)6.4% − ∆ϵ ≤ ϵ (x, x
Pr(cid:2)ϵ (x, x
, Φ) ≤ 6.4% + ∆ϵ
′
′
, Φ) ≤ 6.4% + ∆ϵ
(cid:3) − Pr(cid:2)ϵ (x, x
(cid:3) =
, Φ) ≤ 6.4% − ∆ϵ
′
(cid:3) ≥ 99%
By adapting ∆ϵ , we can search for any desired confidence 1 − α,
e.g., for α = 0.1%.
Discussion. Recall that in Sec. 4.4, we first estimated confidence
intervals for Pr [F (x ) ∈ Φ] and Pr [F (x′) ∈ Φ] separately to then
combine them using the union bound. In contract, we now first
combine the joint distribution of Pr [F (x ) ∈ Φ] and Pr [F (x′) ∈ Φ]
to a distribution of ϵ (x, x′, Φ), and only then derive a confidence
interval. Even if ρ = 0 (i.e., without correlation), the approach in
this section yields slightly better results, because it takes into ac-
count that it is unlikely that both(cid:68)Pr [F (x ) ∈ Φ] and(cid:68)Pr [F (x′) ∈ Φ]
are inaccurate estimates simultaneously.
Correlating random choices is a known technique, see e.g., [23].
However, this technique is usually applied for the difference of
random variables, while we apply it for their ratio. In addition, this
technique is particularly suitable for applications in the context of
algorithms that make random choices.
5 A SEARCH FOR LARGE VIOLATIONS
In the previous section, for a given triple (x, x′, Φ), we showed how
to replace ϵ (x, x′, Φ) by an estimate ˆϵ (x, x′, Φ), where our goal was
to construct it with as few samples as possible, while still estimating
a tight confidence interval, with high probability. In this section, we
address the challenge of finding inputs that induce a large privacy
violation. To this end, show how to transform ˆϵ (x, x′, Φ) to a dif-
ferentiable function ˆϵd (x, x′, Φ). Using ˆϵd (x, x′, Φ), we can define
a surrogate optimization problem
arg maxx,x′,Φ
s.t. (x, x′) ∈ Neigh
ˆϵd (x, x′, Φ),
which is differentiable, and can thus be solved with off-the-shelf
numerical optimizers.
We begin this section by explaining how to transform ˆϵ (x, x′, Φ)
to ˆϵd (x, x′, Φ) (Sec. 5.1) and then present the surrogate optimization
problem and the search for violations (Sec. 5.2).
5.1 From ˆϵ (x, x′, Φ) to ˆϵd (x, x′, Φ)
To obtain ˆϵd (x, x′, Φ) from ˆϵ (x, x′, Φ), we merely need to translate
the (deterministic) programs checki
to differentiable programs.
To understand why, recall the definition of ˆϵ (x, x′, Φ):
F,Φ
(cid:80)n
(cid:80)n
1
n
1
n
(cid:80)n
(cid:80)n
1
n
1
n
.
′
(11)
ˆϵ (x, x
, Φ) = log
i =1 checki
F,Φ (x )
F,Φ (x′)
i =1 checki
In ˆϵ (x, x′, Φ), n is a constant, and the checki
F,Φ (x′) programs have
constants instead of each random choice (e.g., as in Fig. 4). Thus, the
sources of non-differentiability are restricted to the statements in
checki
. While translating arbitrary statements to differentiable
functions is not trivial, we identify a class of programs, for which
the translation (1) captures nicely the statements’ semantics and
(2) can be done systematically. Given the translation, we transform
each checki
F,Φ (x ), which
results in a differentiable estimate of ˆϵ (x, x′, Φ), given by:
′
F,Φ (x ) to a differentiable program dchecki
F,Φ
′
ˆϵd (x, x
, Φ) = log
≈ ˆϵ (x, x
, Φ).
i =1 dchecki
i =1 dchecki
F,Φ (x )
F,Φ (x )
We note that because the translation from checki
F,Φ (x ) to the
F,Φ (x ) does not preserve semantics, while
differentiable dchecki
¬B (cid:123) 1 − B
(cid:18)
E1 == E2 (cid:123) e−c2·(E1−E2)
1 + e−c2·(E2−E1)
E1 ≤ E2 (cid:123)
B1 && B2 (cid:123) B1 · B2
B1 || B2 (cid:123) B1 + B2 − B1 · B2
2
(cid:19)−1
if (B) : {x = E1} (cid:123) x = B · E1 + (1 − B) · x
if (B) : {x = E1} else: {x = E2} (cid:123) x = B · E1 + (1 − B) · E2
Figure 8: Transformation rules to make programs differen-
tiable: v is a constant value, x a variable, B, B1, B2 Boolean ex-
pressions, and E1, E2 differentiable arithmetic expressions.
ˆϵd (x, x′, Φ) is approximately equal to ˆϵ (x, x′, Φ) in practice, the
two are not the same in general.
We next describe the class of programs that DP-Finder can trans-
late to differentiable programs, and then describe the translation.
We note that DP-Finder can also handle programs which are not
part of this class by replacing the search through optimization with
random sampling. However, as we show in Sec. 6, this results in
triples with lower privacy violation.
Supported Programs. We focus on a class of programs in which
the sources of non-differentiability are conditional statements and
Boolean expressions. Concretely, we focus on programs consist-
ing of variables, constants, assignments, differentiable arithmetic
expressions (e.g., x+4), Boolean expressions (e.g., x+4≥0 || x<1)
and conditional statements whose branches consist of a single as-
signment statement. Although loops are not supported in general,
if their number of iterations is known at compile-time, they can
be unrolled, resulting in a sequential composition of statements
(which is supported).
Making Programs Differentiable. Given a checki
in the
aforementioned class, we translate it to a differentiable program.
To this end, we define rules, which are applied to each statement
separately. The idea is to transform the conditions to functions
which have a value close to 1 if their arguments satisfy the original
conditions, or a value close to 0 otherwise.
Transformation Rules. Fig. 8 shows our transformation rules for
operations that need to be translated. Constants, variables, assign-
ments, and sequential composition remain the same. A negation ¬B
is transformed to 1 − B, and B is then recursively transformed to a
differentiable function with our rules. Equality comparison of two
arithmetic expressions E1 == E2 is transformed to an exponential
function in E1 − E2, which is close to 1 if E1 == E2, and rapidly
drops to 0 otherwise. The rule is parametrized by a large constant c
(e.g., c = 50), which controls how close the transformed program is
to the original expression: c = ∞ yields the semantics of E1 == E2.
Inequality comparison of two arithmetic expressions E1 ≤ E2 is
transformed to a sigmoid function on the (scaled) expression E2−E1,
which is 1 if E1 ≪ E2, and 0 if E1 ≫ E2. This rule is parametrized
by two large constants c1 and c2, which have the same effect as c
in the previous rule. A logical and expression B1 && B2 is trans-
formed to B1 · B2, which is close to 0 if either expression is close to
0, and close to 1 if both expressions are close to 1. The conditions B1
F,Φ
and B2 are then recursively transformed to differentiable functions.
Similarly, logical or B1 || B2 could be transformed using the same
transformation as for ¬(¬B1&&¬B2) (cid:123) 1 − ((1 − B1) · (1 − B2)),
but we rewrite it to the (slightly more compact) B1 + B2 − B1 · B2.
The if-else statement is transformed to a linear combination of both
branches based on the condition B. If there is no else branch, we
treat the (missing) else branch as if it were present and contained
the assignment x = x.
1
AT,{[0,0]} from Fig. 4 and its trans-
Example. Fig. 9 shows the check
formed, differentiable program. For clarity, we extract the condi-
tions of the if-else statements into separate variables, B1 and B2. The
transformation uses the rules for ==, ≤, ||, and an if-else statement.
We assume c = 50 for the rules transforming == and ≤.
5.2 Differentiable Optimization
Having defined ˆϵd (x, x′, Φ), we can now phrase a surrogate opti-
mization problem for the optimization problem defined in (3):
arg maxx,x′
s.t. (x, x′) ∈ Neigh
ˆϵd (x, x′, Φ)
(12)
Since the objective is differentiable (except for a few edge cases,
which we shortly discuss), this problem can be solved with gradient
methods. In particular, DP-Finder uses the Sequential Least Squares
Programming (SLSQP) optimizer that also allows to express the
constraint (x, x′) ∈ Neigh as-is. We note that the objective function,
ˆϵd (x, x′, Φ), is not necessarily convex (consider e.g., the statement
y = x · x · x), and thus gradient methods may not converge to a
global maximum. Nevertheless, similar to many common problems
(e.g., training machine learning models), gradient methods may
still converge to values close to the optimum. We next discuss edge
cases, in which we do not optimize, and the sources of imprecision
that arise when considering the surrogate optimization problem
(instead of the original one).
Edge Cases. Due to the structure of ˆϵd (x, x′, Φ), defined by
log( f1/f2) for differentiable functions f1 ≈ Pr [F (x ) ∈ Φ] and
f2 ≈ Pr [F (x′) ∈ Φ], if the denominator f2 is zero, the function
ˆϵd (x, x′, Φ) is not defined, and thus cannot be optimized. Thus, be-
fore running the optimizer on ˆϵd (x, x′, Φ), DP-Finder checks if this
is the case. This can happen if the probabilities that are sampled are
too small, and for the specific samples picked, the condition [· ∈ Φ]
is never satisfied.
If only the denominator f2 is 0 and the nominator f1 is not, then
by the definition of DP, we have
(cid:124)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:123)(cid:122)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:125)
0 < Pr [F (x ) ∈ Φ]
≈f1
≤ exp(ϵ ) Pr(cid:2)F (x
) ∈ Φ(cid:3)
(cid:124)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:123)(cid:122)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:125)
′
≈f2
= 0,
which implies that DP does not hold for any ϵ. This situation is
sometimes referred to as ∞-DP. In this case, there is actually no
need to run optimization, and DP-Finder reports the current triple
(x, x′, Φ) as the optimal triple.