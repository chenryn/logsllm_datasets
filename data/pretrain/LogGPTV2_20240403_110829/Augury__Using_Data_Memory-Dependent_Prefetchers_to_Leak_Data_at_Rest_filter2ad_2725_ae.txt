4096
8192
TABLE I: The maximum distance ahead in memory prefetched is a function
of stride. All experiments are performed using 4144 training accesses. This
number of accesses achieves the maximum prefetch depth (Figure 7) while
avoiding the page boundary interaction described in Section VI-B. All memory
in the AoP in between touched pointers (for a given stride) is zeroed.
2) Unprefetchable virtual address regions: Unexpect-
edly, we found that the M1 DMP behavior is affected by the
virtual address of the AoP itself and the virtual addresses
the M1
of pointers contained in the AoP. We found that
02004006008001000120014001600Trial number0200400600800Cycle counts using PMCspeculated_aop_patternbaseline11
We illustrate this behavior with a dense AoP in Figure 9.
We see that the DMP does not dereference pointers located
in the 128 bytes immediately after the 16 KiB page boundary
(y-axis value 4096), but it does dereference the pointers before
and after this ‘dead zone’. However, if the last training access
of the AoP occurs after the 16 KiB page boundary (x-axis
values 4096-4103, then the DMP only refuses to dereference
the pointers in the second 64 byte chunk after the 16 KiB
boundary until train accesses touch this 64 byte chunk (x-
axis value 4104). Notably, this behavior is independent of
AoP sparsity. Though a sparse AoP will naturally have fewer
pointers within these 128 byte regions.
We observed a more aggressive behavior at 2 MiB address
boundaries of the AoP. This behavior is, once again, inde-
pendent of AoP sparsity. The DMP will not dereference any
pointers past a 2 MiB boundary of the AoP. When a training
loop crosses this boundary, the DMP’s confidence resets and
must retrain based on pointers accessed after the boundary.
These interactions are difficult to explain because: 1) the L1
stride prefetcher (Section V-F1) prefetches AoP lines across
the 16 KiB boundary, and 2) the DMP induces page walks that
populate the TLB entries for both the AoP and the pointers in
the AoP (Section VI-C). We observed similar results running
this experiment on Asahi Linux.This means that the reluctance
of the DMP to cross these boundaries is not due to any
safeguards against address translation.
We additionally observed that on MacOS, the DMP will not
dereference pointers contained in the AoP which have a virtual
address between 0x280000000 and 0x7fe840004000–the
end of mappable user-space addresses. On Asahi Linux, which
allows us to map virtual addresses above 0x7fe840004000,
we found the DMP to activate and dereference pointers above
0xffff3f2b0000, but then addresses below 0x280000000
are not dereferenceable. At this time, we do not have an expla-
nation for why there are restrictions on the virtual addresses
the M1 DMP will dereference.
C. What function of memory values is transmitted?
The M1 AoP DMP makes prefetches based on memory con-
tent as if it were pointer values. This, naively, places a major
restriction on the function of values transmitted. Only the top
57 bits of the address/value (i.e., L2 cacheline granularity) is
transmitted, and only if they are a valid virtual address. As
pointers must be placed at 8-byte alignments (Section VI-A),
we cannot read partial values. (Section VI-A4).
We did, however, find that the M1 DMP fills entries in the
TLB for the pointers in the AoP. To test this we set up an
AoP in the usual way but made each ‘test pointer’ after the
end of the AoP a pointer to a unique page. We can then use the
mprotect system call to change the protection bits of pages
associated with the test pointers to invalidate their TLB entries.
We now set up two experiments: 1) where we mprotect the
test pointers before streaming through the AoP and 2) where
we mprotect the test pointers after streaming through the
AoP.6 If the DMP fills TLB entries, then the test pointer access
6We also pad the mprotect syscalls with 10,000 cycle delays on both
sides of the call and again add data dependencies.
Fig. 7: DMP prefetch depth is a function of train loop size (NUM PTRS
in Algorithm 1). The left/right graphs show results for the single-level
AoP/baseline experiments in Algorithm 1. Each column (along the x-axis)
represents a single training loop size. Each row in a given column (along
the y-axis) corresponds to a test access latency into data bu f (Section V-E1)
that far away from the last pointer touched in the training loop. Times are
measured using mach_absolute_time and converted to nanoseconds as
described in Section V-D.
Fig. 8: DMP training loop performance as a function of stride. All
experiments use a large training loop size, similar to Table I. All memory
in the AoP in between touched pointers (for a given stride) is zeroed. Times
are measured using mach_absolute_time and converted to nanoseconds
as described in Section V-D.
DMP does not dereference pointers located in the cacheline
immediately after 16 KiB or 2 MiB virtual address boundaries
of the AoP and has additional odd behavior depending on
which boundary (16 KiB or 2 MiB) we try to get it to cross.
Fig. 9: For different training AoP lengths N (x-axis), what is the access
latency for values in data bu f corresponding to pointers in the AoP at
offsets N + M (y-axis). The dashed lines indicate cacheline boundaries in
the AoP. A 16 KiB Boundary occurs at aop[4096]. Times are measured using
mach_absolute_time and converted to nanoseconds (Section V-D.)
12481632641282565121024204840968192Number of Accesses in Train Loop18162432404856647280Distance from TrainAoP Access Pattern12481632641282565121024204840968192Number of Accesses in Train LoopComputed Access Pattern0100200300400500Access Time (NS)Mean	Access	Time	(NS)0100200300400Training	Stride	(128B)123468121624324864128256512407240744076407840804082408440864088409040924094409640984100410241044106410841104112411441164118Last Array of Pointers Index Accessed in Training (N)4072408040884096410441124120412841364144415241604168417641844192AoP Index (N + M)100200300400Access Time (NS)DMP Activation Requirements
Access at least 3 pointers
Retrain at each aop 2 MiB page
boundary
aop must be pointer aligned
Run on firestorm
The DMP can
Activate solely through
speculative aop accesses
Recognize an unrolled aop (it is
not IP-indexed)
Recognize strides through the
aop in powers of two
Prefetch in either direction
Leakage Target Must
Be a pointer
Be within current prefetch distance
Not be 0x280000000-0xffff3f2b0000
Not be in first 128 bytes of any aop 16 KiB page boundary
TABLE II: Summary of the M1 DMP
times for experiment 1 should be faster than experiment 2, and
if it does not fill TLB entries, then the access times should
be the same. Across 250 runs of each experiment, we found
that the average test pointer access time and standard deviation
for experiment 1 was 27.95 cycles and 2.8 cycles respectively
(using the PMC), and the times for experiment 2 were 110.83
cycles and 49.87 cycles respectively. From this, we conclude
that the DMP does fill TLB entries which transmits the page
bits of the value through another channel.
D. How can an adversary receive the transmitted values?
We found three attacker visible ways that the M1 DMP
affects microarchitectural state: it prefetches to the L2 cache,
it fills TLB entries, and it has internal state (e.g., confidence).
To receive changes to L2 cache state, the adversary may
use directly timed accesses to the cache, use a cache side
channel like Prime+Probe [35, 38], or alternatively use an
interconnect side channel [36]. The TLB entries may also be
attacked directly using a TLB side channel [23, 43].
So far, the retrieval methods have had secrets or pointers
of interest after the end of the AoP, but attackers can also
learn about pointers contained in the AoP by using the DMP’s
confidence metrics as an indicator. We showed earlier in
Figure 7 that higher confidences result in deeper prefetching,
and frome Section VI-A3 that the DMP needs 3 AoP patterned
accesses to then prefetch the “4th” (next) pointer. We can use
these effects to determine the validity of pointers under the
right circumstances. We later show in Section VII-D how one
can use the DMP’s confidence to break ASLR.
VII. EXAMPLES OF AUGURY TECHNIQUES
In this section, we cover four scenarios where the M1
DMP can be used in attacks: performing out-of-bounds reads,
beating speculative load hardening, retrieving leaked addresses
via Prime+Probe, and breaking ASLR.
A. Out-of-Bounds Reads
Algorithm 3 shows a proof-of-concept (PoC) which uses the
DMP to read past the end of a buffer. We start by picking three
random pointers (test p 1, test p 2, and test p 3) that point
to different cachelines of memory. Although in this example
12
these pointers are accessible to the attacker, we know that
the DMP alters the L2 cache and TLB, so an attacker could
instead conduct an attack like Prime+Probe if they did not
have access to these pointers [35, 38]. The user then picks
one of the pointers on line 2, and we will use the DMP to
determine which pointer the user picked without reading it.
/* Stick the user chosen pointer after the filled AoP */
1 aop[0 : AOP SIZE− 1] = ... /* Random, unique ptrs */
2 test p = user choice(test p 1,test p 2,test p 3)
3 thrash cache()/* Evict test pointers */
/* Train the DMP by streaming through the AoP */
4 ∗aop[0]
5 . . .
6 ∗aop[AOP SIZE− 1]
/* Find the fastest test pointer access time */
7 time(∗test p 1)
8 time(∗test p 2)
9 time(∗test p 3)
Algorithm 3: PoC using straight-line memory accesses to activate the
DMP and distinguish between three pointers.
To activate the DMP, we first create an AoP filled with
AOP SIZE random pointers to disjoint 128-byte chunks of
memory. This AoP is placed immediately before the memory
location containing the user-chosen pointer. AOP SIZE must
be at least 3 to activate the DMP, with larger sizes increasing
the DMP’s confidence and the clarity of the signal. Next,
we flush the entire cache state by reading in several MB of
unrelated data on line 3. We do this to ensure that the only
test pointer that has a cache hit will be the pointer off the end
of the AoP, assuming it is prefetched. We now stream through
the AoP, accessing and dereferencing each pointer in it, and
not the test pointer outside the AoP (see lines 4-6.) We have
unrolled the loop for two reasons: it makes it clear that this
is not a speculative execution effect, and it also demonstrates
that attackers only need to induce an access pattern that looks
like the cache misses caused by streaming through an AoP.
After training the DMP, we measure the access time to each
of the 3 test pointers. Since only the the user selected pointer
should be prefetched into L2, it will be the fastest to access
We ran this PoC 500 times using an AoP of size 64, select-
ing a different test pointer number each time, and measuring its
accuracy in distinguishing pointers. For the first 250 runs, we
used the M1’s PMC (see Section V-D) which can very accu-
rately distinguish between L2 and main memory access times.
For the latter 250 runs, we used mach_absolute_time
which is noisier than the PMC. The with the PMC the PoC
had a 92.0% average accuracy–i.e., number of times the PoC
correctly picked the pointer. With mach_absolute_time,
the PoC had a 70.2% average accuracy.
B. Beating Speculative Load Hardening
Speculative load hardening (SLH) is a defense against
conditional branch-based speculative execution attacks, known
by the name of Spectre Variant #1 [15, 30]. Some pseudocode
with and without SLH applied is shown in Algorithm 4.
SLH prevents Spectre Variant #1 by adding a branchless
recheck of each branch condition within each conditional’s
1 N = NUM TRAIN PTRS
2 stride = LINE SIZE
/* AoP train loop without SLH */
3 for i in 0...N do
4
5 end
∗aop[i∗ stride]
/* AoP train loop with SLH */
6 mask = 0
7 for i in 0...N do
/* branchless set */
mask = (i >= N) ? 0 : ALL ONES BITMASK
∗aop[(i & mask)∗ stride]
8
9
10 end
Algorithm 4: Example of gcc and clang’s SLH hardened AoP iter-
ation loop [6, 7, 15]. We apply SLH by providing clang with the
-mspeculative-load-hardening flag [6, 7]. This option for
AArch64 masks only the loaded value, and this is the only SLH option
for AArch64 when using LLVM 14 (the current latest version). Note that
there are additional speculative-execution-specific instructions inserted in
the final compiler output.
body to apply an all-ones or all-zeroes bitmask to a data
load. This results in the load working as expected when the
branch predictor is correct and only loading from offset 0
when the branch predictor guesses incorrectly. In the case of
the SLH train loop in Algorithm 4, the hardening of loaded
values also applies to the index into the array of pointers; this
should prevent the accesses from speculatively reading past
the bounds of the array of pointers.
Since the DMP only ever sees cache misses, the (non-
speculative) access pattern caused by both loops in Algo-
rithm 4 will be the same. SLH provides no protection against
using the DMP to bypass the bounds check. We reran our
existence experiments from Section V with the SLH compiler
flag enabled and confirmed that they still work. We also reran
the PoC from Algorithm 3 getting an accuracy of 88.0% with
the AoP accesses turned back into a loop, SLH enabled, and
using the PMC. Indeed, the exploits should still work since
to the memory system, the memory accesses caused by both
loops in Algorithm 4 will be the same, and the additional
instructions inserted from SLH do not prevent DMP activation.
While it is unsurprising that SLH does not protect against
DMP leakage, it is important to note that some code vulnerable
to Spectre V1, but protected by SLH, will continue to be
vulnerable to the same receive side-channel as before. As
such, a developer applying SLH to attacker submitted code or
code containing latent DMP gadgets (such Algorithm 4) gains
almost no defensive advantage. However, unlike the Spectre
attacks that SLH was designed to prevent, the M1 DMP has a
maximum stride and depth which constrains the furthest value
past the end of a buffer that can be prefetched.
C. Retrieving leaked pointers via Prime+Probe
The previous example primitives rely on the adversary being
able to directly time accesses to the targeted pointers to
determine their cache state. This is often not possible, and
we can instead use cache side-channels like Prime+Probe to
determine if a given pointer was prefetched.
13
We set up this experiment identically to the basic out-of-
bounds read in Section VII-A. However, we use only two test
pointers (test p 0 and test p 1) and build eviction sets of size
24 for each (ev0 and ev1) using the baseline algorithm from
Vila et al [46]. Each run of the experiment randomly chooses
either test p 0 or test p 1 as the test pointer.
After the training accesses are complete, we time an access
to each eviction set (ev0 and ev1) independently. The eviction
set with a longer access time corresponds to the pointer we
guess as the test pointer. In general this manifested as one
eviction set taking around 100 PMC cycles longer to access
than the other. Across 4300 runs, this resulted in a correct
guess in 60.0% of runs. However, if we remove runs where
the Probe step failed, and did not result in ev0 or ev1 being
significantly slower, the accuracy rises to 84.8%. The net effect
is that Prime+Probe, while effective, adds another layer of
noise to the recovery of pointer values.
D. Breaking ASLR by testing virtual addresses
Address space layout randomization (ASLR) is a widely
deployed defense that prevents attackers from knowing a priori
where important parts of a program live in memory. It does
this by randomizing the memory locations of portions of a
program such as the stack, heap, code, and libraries.