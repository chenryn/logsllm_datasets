Attack objective. The hiding attack [30, 50, 57, 61, 66], also re-
ferred to as the false-negative (FN) attack, aims to make object
detectors miss the detection of certain objects (which increases FN)
at the test time. The hiding attack can cause serious consequences in
scenarios like an autonomous vehicle missing a pedestrian. There-
fore, defending against patch hiding attacks is of great importance.
Attacker capability. We allow the localized adversary to arbi-
trarily manipulate pixels within one restricted region.4 Formally,
we can use a binary pixel mask pm ∈ {0, 1}𝑊 ×𝐻 to represent this
restricted region, where the pixels within the region are set to 1.
The adversarial image then can be represented as x′ = (1 − pm) ⊙
x + pm ⊙ x′′ where ⊙ denotes the element-wise product opera-
tor, and x′′ ∈ [0, 1]𝑊 ×𝐻×𝐶 is the content of the adversarial patch,
which the adversary can arbitrarily modify. pm is a function of
patch size and patch location. The patch size should be limited such
that the object is recognizable by a human. For patch locations,
we consider three different threat models: over-patch, close-patch,
far-patch, where the patch is over, close to, or far away from the vic-
tim object, respectively. The adversary can pick any valid location
within the threat model for an optimal attack.
Previous works [23, 30, 46] have shown that attacks against
object detectors can succeed even when the patch is far away from
the victim object. Therefore, defending against all three threat
models is of interest.
2.3 Defense Formulation
Defense objective. We focus on defending against patch hiding
attacks. We consider our defense to be robust on an object if we can
1) detect the object on the clean image is correct and 2) detect part
of the object or send out an attack alert on the adversarial image.5
Crucially, we design our defense to be provably robust: for an
object certified by our provable analysis, our defense can either
detect the certified object or issue an alert regardless of what the
adversary does (including any adaptive attack at any patch location
within the threat model). This robustness property is agnostic to
the attack algorithm and holds against an adversary that has full
access to our defense setup.
4Provably robust defenses against one single patch are currently an open/unsolved
problem, and hence the focus of this paper. In Appendix C, we will justify our one-patch
threat model and quantitatively discuss the implication of multiple patches.
5We note that in the adversarial setting, we only require the predicted bounding box
to cover part of the object because it is likely that only a small part of the object is
recognizable due to the adversarial patch (e.g., the left dog in the right part of Figure 1).
We provide additional justification for our defense objective in Appendix E.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3179Remark: primary focus on hiding attacks. In this paper, we
focus on the hiding attack because it is the most fundamental and
notorious attack against object detectors. We can visualize dividing
the object detection task into two steps: 1) detecting the object
bounding box and then 2) classifying the detected object. If the
first step is compromised by the hiding attack, there is no hope for
robust object detection. On the other hand, securing the first step
against the patch hiding attack lays a foundation for the robust object
detection; we can design effective remediation for the second step
if needed (Section 6).
Take the application domain of autonomous vehicles (AV) as an
example: an AV missing the detection of an upcoming car could lead
to a serious car accident. However, if the AV detects the upcoming
object but predicts an incorrect class label (e.g., mistaking a car
for a pedestrian), it can still make the correct decision of stopping
and avoiding the collision. Moreover, in challenging application
domains where the predicted class label is of great importance (e.g.,
traffic sign recognition), we can feed the detected bound box to
an auxiliary image classifier to re-determine the class label. The
defense problem is then reduced to the robust image classification
and has been studied by several previous works [24, 36, 58, 65].
Therefore, we make the hiding attack the primary focus of this
paper and will also discuss the extension of DetectorGuard against
other attacks in Section 6.
2.4 Provably Robust Image Classification
In this subsection, we introduce two key principles that are widely
adopted in recent research on provably robust image classifica-
tion against adversarial patches [24, 36, 58, 65]. In Section 3.2, we
will discuss how to adapt these two principles to build a robust
image classifier, which will be alter used in Objectness Predictor
(Section 3.3).
Feature extractor – use small receptive fields. The receptive
field of a Deep Neural Network (DNN) is the input pixel region
where each extracted feature is looking at. If the receptive field of a
DNN is too large, then a small adversarial patch can corrupt most
extracted features and easily manipulate the model behavior [30, 46,
58]. On the other hand, the small receptive field bounds the number
of corrupted features by ⌈(p+r−1)/s⌉, where p is the patch size, r is
the receptive field size, and s is the stride of receptive field (the pixel
distance between two adjacent receptive field centers) [58], and
makes robust classification possible [24, 36, 58, 65]. Popular design
choices including the BagNet architecture [3, 36, 58, 65] and an
ensemble architecture using small pixel patches as inputs [24, 58].
Classification head – do secure feature aggregation. Given
a feature map, DNN uses a classification head, which consists of a
feature aggregation layer and a fully-connected (classification) layer,
to make final predictions. Since the small receptive field bounds
the number of corrupted features, we can use secure aggregation
techniques to build a robust classification head; design choices
include clipping [58, 65], masking [58], and majority voting [24, 36].
3 DETECTORGUARD
In this section, we first introduce the key insight and overview of
DetectorGuard, and then detail the design of our defense modules
(Objectness Predictor and Objectness Explainer).
3.1 Defense Overview
Bridging robust image classification and robust object detec-
tion. There has been a significant advancement in (provably) robust
image classification research [9, 24, 36, 58, 65] while object detec-
tors remain vulnerable. This sharp contrast motivates us to ask: can
we adapt robust image classifiers for robust object detection? Unfor-
tunately, there is a huge gap between these two tasks: an image
classifier only robustly predicts one single label for each image
while an object detector has to robustly output a list of class labels
and object bounding boxes. This gap leads to two major challenges.
• Challenge 1: Lack of End-to-end Provable Robustness. A robust
image classifier only provides robustness for single-label pre-
dictions while a robust object detector requires robustness
for multiple labels and bounding boxes. Therefore, an object
detector adapted from a robust image classifier can still be
vulnerable without any security guarantee, and we aim to
carefully design our defense pipeline to enable the proof of
end-to-end robustness for object detection.
• Challenge 2: Amplified Cost of Clean Performance. All existing
provably robust image classifiers [9, 24, 36, 58, 65] attain
robustness at a non-negligible cost of clean performance
(e.g., >20% clean accuracy drop on ImageNet [11]), and this
cost can be severely amplified when adapting towards the
more demanding object detection task. An object detector
with poor clean performance (even in the absence of an
adversary) prohibits its real-world deployment; therefore, we
aim to minimize the clean performance cost in our defense.
DetectorGuard: an objectness explaining strategy. In De-
tectorGuard, we propose an objectness explaining strategy to ad-
dresses the above two challenges. Recall that Figure 1 provides an
overview of DetectorGuard, which will either output a list bound-
ing box predictions (left figure; clean setting) or an attack alert
(right figure; adversarial setting). There are three major modules in
DetectorGuard: Base Detector, Objectness Predictor, and Objectness
Explainer. Base Detector is responsible for making accurate detec-
tions in the clean setting and can be any popular high-performance
object detector such as YOLOv4 [2, 53] and Faster R-CNN [45]. Ob-
jectness Predictor is adapted from the core principles for building
robust image classifiers as introduced in Section 2.4 and aims to out-
put a robust objectness map in the adversarial environment. We also
carefully design Objectness Predictor to mitigate the errors made by
the robust image classifier in the clean setting. Finally, Objectness
Explainer leverages predicted bounding boxes from Base Detector
to explain/match the objectness predicted by Objectness Predictor
and aims to catch a malicious attack. When no attack is detected,
DetectorGuard will output the detection results of Base Detector
(i.e., a conventional object detector), so that our clean performance
is close to state-of-the-art object detectors. When a patch hiding
attack occurs, Base Detector can miss the object while Objectness
Predictor can robustly predict high objectness. This mismatch will
lead to unexplained objectness and trigger an attack alert. Notably,
our objectness explaining strategy can achieve end-to-end provable
robustness for “free" (at a negligible cost of clean performance) and
solve two major challenges. We will introduce the module details
and theoretically analyze the free provable robustness property.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3180Table 1: Summary of important notation
Notation Description
Notation Description
Input image
bounding box
x
b
Feature map
Objectness map
om
fm
classification logits
number of object classes
𝑁
v
(𝑝𝑥, 𝑝𝑦)
(𝑤𝑥, 𝑤𝑦)
patch size
window size
binarizing threshold D
detection results
𝑇
upper/lower bound of classification logits values of each class
u, l
Algorithm Pseudocode. We provide the pseudocode of Detec-
torGuard in Algorithm 1 and a summary of important notation
in Table 1. The main procedure DG(·) has three sub-procedures:
BaseDetector(·), ObjPredictor(·), and DetMatcher(·). The sub-
procedure BaseDetector(·) can be any off-the-shelf object detec-
tor as discussed in Section 2.1. All tensors/arrays are represented
with bold symbols and scalars are in italic. All tensor/array indices
start from zeros; the tensor/array slicing is in Python style (e.g.,
[𝑖 : 𝑗] means all indices 𝑘 satisfying 𝑖 ≤ 𝑘 < 𝑗). We assume that the
“background" class corresponds to the largest class index.
In the remainder of this section, we first introduce how we in-
stantiate robust image classifiers and then discuss the design of
Objectness Predictor and Objectness Explainer.
3.2 Instantiating Robust Image Classifiers
To start with, we discuss how we build robust image classifiers that
will be used in Objectness Predictor.
As discussed in Section 2.4, we can build a robust image classifier
RC(·) using a feature extractor FE(·) with small receptive fields, and
a robust classification head RCH(·) with secure feature aggregation.
In our design, we choose BagNet [3] backbone as the feature extrac-
tor FE(·), and we clip elements of local logits vectors6 into [0,∞]
for secure aggregation in RCH(·). This implementation is similar
to the robust image classifier Clipped BagNet (CBN) [65], but we
note that we use a different clipping function that is tailored to our
more challenging task of object detection. In Appendix B, we provide
additional details of FE(·) and RCH(·) and also discuss alternative
design choices of robust image classifier RC(·) (e.g., robust masking
from PatchGuard [58]).
Remark: Limitations of robust classifiers. We note that the
adapted robust image classifier RC(·) achieves robustness at the cost
of a non-negligible clean performance drop [24, 36, 58]. Therefore,
in the clean setting, classification at different image locations can
be imprecise with three typical errors that lead to Challenge 2:
• Clean Error 1: Confusion between two object class labels
• Clean Error 2: Predicts background pixels as objects
• Clean Error 3: Predicts objects as “background"
In the next two subsections, we will discuss how DetectorGuard
design can eliminate/mitigate these three clean errors.
3.3 Objectness Predictor
Overview. Objectness Predictor aims to output a robust object-
ness map that indicates the probability of objects being present at
different locations. Its high-level idea is to perform robust image
Figure 2: Visualization of Objectness Predictor operations
classification on different regions to predict an object class label or
“background". We provide a simplified visual overview in Figure 2.
Objectness Predictor involves two major operations: robust feature-
space window classification and objectness map generation. The
first step aims to perform efficient robust classification at different
image regions and the second step aims to filter out clean errors
made by the robust classifier and generate the final objectness map.
Robust feature-space window classification. To perform ro-
bust image classification at different image locations, we first extract
a feature map via FE(·), and then apply the robust image classifica-
tion head RCH(·) to a sliding window over the feature map.
Pseudocode. The pseudocode of Objectness Predictor in Line 12-
23 of Algorithm 1. We first extract the feature map fm with FE(·)
(Line 13). Next, for every valid window location, represented as
(𝑖, 𝑗),7 we feed the feature window fm[𝑖 : 𝑖 + 𝑤𝑥, 𝑗 : 𝑗 + 𝑤𝑦] to a
robust classification head RCH(·) to get the classification label 𝑙
and the classification logits v ∈ R𝑁+1 for 𝑁 object classes and the
“background" class (Line 17). The use of RCH(·) ensures that the
classification is robust when window features are corrupted.
Remark: defense efficiency. We note that our window classifica-
tion operates in the feature space, and this allows us to reuse the
expensive feature map generation (i.e., FE(·)); each classification
only needs a cheap computation of the classification head (i.e.,
RCH(·)). Therefore, our defense only incurs a small overhead (will
be evaluated in Section 5.5).
Objectness map generation: handling clean errors of ro-
bust classifiers. Next, we aim to filter out incorrect window clas-
sifications and generate the final objectness map. We reduce each