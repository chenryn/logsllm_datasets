Figure 4: Virtual Test Field
Our approach. To systematically identify CP-inconsistencies, our
technique provides a virtual test field (VTF), which can be config-
ured to have various objects and environmental conditions. Figure 4
shows an example VTF. Observe that there are various object mod-
els, including wall, pole, box, etc. Their physical properties such
as position, rotation, shape, size and inertia are configurable and
continuously mutated during testing in order to expose vulnera-
bilities in the subject RV. The environmental effects include wind,
atmosphere (temperature, air pressure), magnetic fields, etc., which
are directly related to the specific sensor measurements of the sub-
ject RV. In a test, the RV is given a trajectory that traverses the
objects on the field (just like in a real physical field test). The test
execution is through the Gazebo simulator [62], which is equipped
with dynamics engines (e.g., Open Dynamics Engine [61]) to simu-
late various kinds of RVs and physical objects. We further develop
our own plugins to support customized and realistic environmen-
tal effects, including dynamic wind direction, duration, and speed.
Through system identification (SI), we construct a model for the
RV system to describe both the vehicle dynamics and control al-
gorithms. The model can be considered as a virtual RV (VRV) that
takes the control reference points (e.g., position and velocity tar-
gets) and the current vehicle states, and then produces the next
states. During testing, the navigation trajectory is interpreted to a
sequence of low level control reference points that are sent to both
the RV and the VRV. The RV operates on the VTF, considering the
interference from the external physical objects and environmental
conditions, whereas the VRV operates in an ideal world, without
any external inference (e.g., no wind or obstacles). A crash occurs
when substantial state differences are observed between the RV
and the VRV. This is key to our technique because otherwise, we will
have to use a set of range checks on sensor/state values of the RV
alone to determine if a crash happens, which suffers from the same
problems as those existing safety checks. For instance, in Figure 4,
the quadrotor hits a wall and deflects while the virtual quadrotor
flies through as it is not interfered by any external factors.
Sound safety checks are supposed to detect the crash when it
occurs. To expose a CP-inconsistency vulnerability, our test tech-
nique aims to mutate the VTF settings such that a crash can be
induced but cannot be detected (i.e., under-approximation). In or-
der to achieve our goal, we define objective functions that con-
sider two kinds of cost, namely cyber cost and physical cost. The
former measures how close the safety checks are to detect the
crash and the latter measures the state differences between the
RV and the VRV. The cyber cost is constructed by analyzing the
control program and collecting the conditions in the predicates
that are associated with safety-checking. In our example (Figure 2),
the cyber cost includes expression (CRASH_CHECK_ACCEL_MAX âˆ’
accel.length()) derived from the third check at line 14, and
(angle_err âˆ’ CRASH_CHECK_ANGLE_DEVIATION) derived from
the fourth check at line 18. Observe that reducing these expressions
pushes the predicates from a false value (i.e., crash) towards a true
value (i.e., no crash). As will be explained later in Section 4.2, the
cyber cost function also models the constraints in other checks.
Details are elided at this point. On the other hand, the physical
cost is computed as the state differences of the subject RV and
the VRV, such as position and velocity differences. As such, expos-
ing under-approximation type of CP-inconsistencies is achieved
by using an optimization procedure to mutate the VTF config-
uration to minimize the cyber cost and maximize the physical
cost; conversely over-approximations are exposed by maximizing
the cyber cost and minimizing the physical cost. For the example,
our technique determines that when the wall with 24kg and in-
ertia (Ixx, Iyy, Izz) = (12.3, 15.4, 3.0) is in the way of the vehicle
with a collision angle of 28.6 degrees, the aforementioned under-
approximation case can be triggered.
3 OVERVIEW
Figure 5 presents an overview of our system, which consists of two
main components: cost function generation and search-based/evolutionary
testing. In the upper-left corner, static program analysis is used to
identify the predicates guarding the execution of counter-measure
Environmental Effects(wind, temperature, pressure, etc.)RVVRVSession 1E: Cyberphysical Systems CCS '20, November 9â€“13, 2020, Virtual Event, USA266The simulation has two main configurable components described
by the Simulation Description Format (SDF) [63].
Figure 5: Overview of our test framework
functions such as a parachute releasing function. Currently, such
functions are manually provided by the user. They are usually in
a small number. The extracted predicates are used to derive the
cyber cost functions and a set of constraints (e.g., the quadrotor
must not be in a landed mode). In the lower-left corner, system
identification (SI) [60] makes use of operation traces (e.g., stage
logs) and a model template to derive a state-space model for the
subject RV. The model describes both the vehicle dynamics and
the control algorithm and predicts the next state from the control
references and the current state. The template determines the com-
plexity of the generated model (e.g., linear versus non-linear). SI is
essentially a procedure to derive the coefficients for the template
through regression. A realistic RV system often has various op-
eration modes (e.g., take-off, loiter, and landing). Different modes
likely have different models. Hence, our system takes the mode
transition specification and represents it as a finite-state machine.
The system model is the finite-state machine together with a set of
state-space models, one for each operation mode. We also call the
system model the hybrid model, which is essentially the VRV.
The loop in the middle denotes the search-based/evolutionary
testing procedure. For each test case, the RV system is executed in
the simulator while the VRV is executed in parallel without exter-
nal interference. The runtime information of the two executions
is collected and compared to see if any CP-inconsistency happens.
Specifically, it monitors whether the safety checks fail to report
exceptions while physical anomalies actually occur, or vice versa.
The evolutionary testing technique generates inputs based on a
genetic algorithm for multi-objective optimization. Genetic oper-
ations (crossover and mutation) are leveraged to derive new test
cases from existing ones, with the guidance of the cost functions.
4 DESIGN
4.1 Simulation Environment
Our technique leverages high-fidelity simulation to reduce the ex-
pensive physical testing. Once the simulation based testing discloses
CP-inconsistencies, we further reproduce them in the real-world for
those that we have the corresponding physical vehicles. Specifically,
we use Gazebo [62] for realistic 3D simulation. Gazebo is a popular
open-source robotics simulator. It supports various dynamics en-
gines and complex, realistic physical environments (e.g., through
the Open Dynamic Engine [70]). It can simulate the dynamics inter-
actions among objects, and between objects and the environment.
(a) Weighted Wall (b) Cylinder Post
Figure 6: Various (different physical properties) example obstacles
(c) Rollable Ball
(d) Tunnel
SDF is a standard XML format that describes (static and dynamic)
objects and the environment for complex simulation, visualization,
and control. It mainly consists of two components: world and model.
A world consists of objects and a set of environmental parameters
(e.g., wind, atmosphere, magnetic field, and light). An object is an
instance of some model that ranges from simple shapes to complex
3D robots that have different physical properties. In our prototype,
we use simple shapes to simulate various kinds of physical obstacles
as shown in Figure 6. Note that the different objects have different
physical properties (e.g., shape, friction, and elasticity).
In addition to the default environmental effects, we have also im-
plemented additional customized effects as Gazebo plugins, which
are C++ functions directly accessing Gazebo primitives to provide
more physical effects (e.g., wind gust).
4.2 Cost Function Generation
In this section, we present how our technique defines the cost
functions systematically. They are used as the guidance for the evo-
lutionary testing. The cost functions consist of cyber and physical
objectives, and the testing loop tries to maximize one and minimize
the other to identify CP-inconsistencies.
4.2.1 Cyber Cost. The cyber cost function is constructed from the
control program through static program analysis. To begin, the
user provides a list of counter-measure functions/statements such
as the parachute releasing function. The predicates that guard the
execution of the counter-measures are extracted and then trans-
formed to a cyber cost function. Specifically, a whole-program
control flow graph (CFG) is constructed. Nodes in the graph are
statements and edges denote control flow. Call relations between
functions are represented by edges between a function invocation
statement in the caller function and the entrance statement of the
callee function. Control dependence can be computed from the
whole-program CFG. A statement is control dependent on a par-
ticular branch of a predicate if and only if its execution is directly
determined by the predicate taking the particular branch. Starting
from a counter-measure function-invocation/statement, we acquire
the control dependence transitive-closure till the control loop is
reached. In other words, all the predicates (and the corresponding
branches) guarding the counter-measure and inside the control
loop are extracted.
Figure 7 illustrates our analysis using an example. It first con-
structs a control flow graph (CFG) in (b). Then, it computes post-
dominator relations [4] in the CFG and constructs a control depen-
dence graph (CDG) in (c) that captures the control dependences
InputsEvolutionary Testing LoopTracesSpecificationProgram ModeTransitionSystem IDHybrid ModelTestcase Generation(Evolution)Program AnalysisSimulationDecisionCP-inconsistenciesRuntime InformationCyber Cost FunctionsPhysical Cost FunctionsSession 1E: Cyberphysical Systems CCS '20, November 9â€“13, 2020, Virtual Event, USA267Figure 7: Program analysis and cyber cost function generation
between statements. From the annotated "counter-measure" state-
ment S4 (in red), it traverses the CDG backward until it reaches
the control loop and collects the predicates along the way. In the
figure, S4 is control dependent on C3T, meaning C3 taking the true
branch. C3 is in turn control dependent on C2F as the execution
of C3 is directly determined by C2 taking the false branch. Finally,
predicates and branches C1F, C2F, and C3T are extracted.
Our analysis is inter-procedural because safety checks may cross
multiple functions. It leverages the default points-to analysis in the
compiler [72] to handle function pointers. Although irregular con-
trol flow such as recursion posts challenges for control dependence
computation, we have not encountered such cases in our subject
systems. This is reasonable because control loop is time sensitive
such that developers avoid putting heavy loops or recursions in it.
Each extracted predicate is then normalized to yielding a true
value. That is, a predicate with the false branch is normalized to
its negation. It is to simplify the later analysis/transformation. In
our example (as shown in Figure 7(d)), C1F is normalized to a, or , - for â‰¤, b) {2:     S13:     return}4: if(C2: c==d) {5:      S26:      return}7: S38: if(C3: e>f ) {9:S4}(a) CodeP1 := !C1:= (a  f)(d) PredicatesObjectives:d1 := -(a-b)d2 :=e-fConstraints:g1:= c!=d(e) costsC1S3S4C2S2C3(b) CFGStartS1End14879C1S3S4C2S2C3(c) CDGS1BackwardTraversalFFTAUTORTLLANDVRVAUTO:ğ’™â€²=ğ‘¨ğ‘¨ğ’™ğ’•+ğ‘©ğ‘¨ğ’–ğ’•ğ²ğ’•=ğ‘ªğ‘¨ğ’™ğ’•+ğ‘«ğ‘¨ğ’–ğ’•VRVLAND:ğ’™â€²=ğ‘¨ğ‘³ğ’™ğ’•+ğ‘©ğ‘³ğ’–ğ’•ğ²ğ’•=ğ‘ªğ‘³ğ’™ğ’•+ğ‘«ğ‘³ğ’–ğ’•VRVRTL:ğ’™â€²=ğ‘¨ğ‘¹ğ’™ğ’•+ğ‘©ğ‘¹ğ’–ğ’•ğ²ğ’•=ğ‘ªğ‘¹ğ’™ğ’•+ğ‘«ğ‘¹ğ’–ğ’•remain_batt< 20%set_mod(RTL)mission_completed== truedist_from_home< 0.3mGUIDEDVRVGUIDD:ğ’™â€²=ğ‘¨ğ‘®ğ’™ğ’•+ğ‘©ğ‘®ğ’–ğ’•ğ²ğ’•=ğ‘ªğ‘®ğ’™ğ’•+ğ‘«ğ‘®ğ’–ğ’•home_arrived&&set_mod(guided)Set_mod(land)Session 1E: Cyberphysical Systems CCS '20, November 9â€“13, 2020, Virtual Event, USA268from our subject systems took a few human-hours. As such, the
manual efforts of constructing the FSM is reasonable. For each
operation mode, system identification (SI) [60] is used to derive a
state-space model which describes the continuous physical behav-
iors. The state-space model can be considered as a set of equations
that compute the next state values (e.g., position, velocity, angel ve-
locity, and acceleration), denoted as xâ€² inside the individual modes
in Figure 8, from the current states x(t) and a set of reference values
u(t), e.g., target positions and velocities. Variable t denotes time.
These reference values are generated by the navigation logic of
the RV system at a frequency of 400Hz. That is, new targets are
provided every 2.5 miliseconds. Note that these reference values are
not way-points in the navigation plan which are too coarse-grained.
Intuitively, one can consider that the navigation logic continuously
interprets the navigation plan based on the current trajectory (or
the current deviation from the plan) to a set of smaller goals de-
noted by reference values, and passes them to the low level control
algorithm (e.g., a PID controller), which further interprets them to
actuation signals. The state-space model describes the low level
control algorithm and the vehicle dynamics. The derivation of state-
space model is similar to that in [15, 45, 46, 65] and hence not our
contribution. Intuitively, it is done by performing regression on a
set of RV operation traces to derive coefficients in a provided model
template (e.g., matrices A, B, C and D inside the nodes in Figure 8).
Interested readers please refer to [15, 45, 46, 58, 65].
During testing, both the RV and the VRV (i.e., the hybrid model)
share the same operation mode inputs and the reference inputs
(generated by the navigation logic). The RV will operate on the
VTF inside the simulator whereas the VRV simply produces its next
states based on the hybrid model. The error between the RV states
and the corresponding VRV states constitutes the physical cost.
The formal definition is as follows.
FÏ = {p1, p2, ..., pm }, with
pi = |RVi âˆ’ VRVi |
(4)
Here, RVi and VRVi denote the ith state of the RV and the VRV,
respectively. During testing, FÏ is maximized (through input mu-
tation) in order to expose the under-approximation type of vul-
nerabilities. A weighted sum of pi is compared to a threshold Î¸ to
determine if a real crash happens. If it does and the safety checks
cannot detect it, an under-approximation vulnerability is found.
We will show in Section 5.2 that our results are not sensitive to Î¸.
4.3 Multi-objective Evolutionary Testing
Our testing technique continuously alters the test inputs (i.e., VTF
configuration) in order to expose CP-inconsistencies. The evolution-
ary testing guides the input mutation using the cyber and physical
costs defined in the previous section. Such guidance is critical given
the enormous search space.
Input Selection. The virtual test field (VTF) can be configured
4.3.1
in many different ways. Table 1 lists a subset of the variables that
can be mutated. As mentioned in Section 4.1, they fall into two
kinds, world variables that describe the environmental conditions
and model variables that describe the characteristics of individual
objects on the VTF. Depending on the number of objects, the num-
ber of variables that can be mutated may be very large. For the
motivation example in Section 2, there are 2 types (rotation and
Table 1: Input Variables to Mutate
Input Type
Parameters
Default
Domain
Wind Direction
Wind Duration
Wind Strength
Gravity
Magnetic Field
Temperature
Air Pressure
Ambient Light
Time of Day
Clouds Speed
Fog Density
Position
Rotation
Inertia
Mass
Geometry
Bounce
Friction
Elasticity
World
Model
[x y z]
N
[Start, End] (s)
[x y z] (m/s2)
[x y z] (T)
T (K)
P (pa)
[x y z]
[0..24]
C (m/s)
D
[x y z]
m (Kg)
Scale
[roll pitch yaw]
[ixx iyy izz]
Restitution Coefficient e
Friction Coefficient Âµ
Poissons Ratio
0 0 -9.8
6e-6 2e-5 -4e-5
0 0 0
0 0
0
288.15
101325
0 0 -1
10
0.6
1
0 0 0
0 0 0
1 1 1
1
1 1 1
0
1
0.3
inertia) of variables to be mutated for each object. Depending on the
subject RV and the kind of safety checks, only a subset of variables
have non-trivial influence on system behaviors (and hence on the
cost functions). To reduce the search space, we develop an input
selection procedure that filters out the insignificant variables for
the given cost functions.
Given a navigation plan and the generated cost functions, we
sample each input variable type within the valid range uniformly.
For each sample, we execute the test in the simulator and observe
the cost function differences across samples. The variables whose
different samples lead to negligible cost function value differences
are pruned out. For example, changing ambient light is less effective
when the test is not related to the vision sensor. In contrast, the
magnetic field values have substantial impact in the magnetometer-
related tests.
Figure 9: Objective space and optimal set
4.3.2 Evolutionary Testing. The goal of testing is to find maxi-