a threshold dT for each user. Since most veriﬁcation sys-
tems prefer to reduce unauthorized accesses to minimum,
we aim to select a threshold that leads to a zero false pos-
itive rate for the training samples, i.e., training signature
samples that are not from a user u cannot pass the veriﬁca-
tion. During the enrollment phase, we calculate the DTW
distance between the template of a user u and all the M
training samples (from all the users), and sort them. We ﬁnd
the ﬁrst training sample in the sorted list that is not from the
user u. Then, the DTW distance between this sample and
the template of the user u is the upper bound of dT , and we
select a dT that is smaller than the upper-bound to achieve
a higher level of security. Figure 9 shows an example of
M = 10 training samples. The x-axis gives the indices of
the training samples and the y-axis is their DTW distance
to the template of the user u. Along the x-axis, the samples
that are labeled ‘+’ are genuine training samples from the
user u while samples labeled ‘-’ are training samples from
other users. In this case, the upper-bound of dT is the dis-
tance between the template and the ﬁrst ‘-’ sample along the
x-axis. In the experiment, we tried various threshold values
to construct the precision-recall curve and the ROC curve,
and hence to evaluate the system performance comprehen-
sively.
+++-+-+---DTW DistanceUpper bound of dTSamples(a) Precision-Recall curves
(b) ROC Curves
Figure 10. Training performance of KinWrite with different n, the number of training samples for each
signature. For ROC curves, the range of x-axis is [0, 0.4] and the range of y-axis is [0.6, 1].
(a) Precision-Recall Curves
(b) ROC Curves
Figure 11. The performance of KinWrite (by signatures) in normal cases. Each colored curve indicates
the performance of verifying one signature.
(in total four times) and then produced ﬁve forged 3D-
signature samples. In total, we collected 12 × 5 × 4 =
240 Ob-4 attack 3D-signature samples.
• CA-Ob4. After collecting the Ob-4 samples, we gave
the same 12 attackers the spelling of the passwords.
Then, each of these 12 attackers produced ﬁve new
forged 3D-signatures for each victim. In total we col-
lected 12 × 5 × 4 = 240 CA-Ob4 attack 3D-signature
samples.
• Insider. We told six attackers the spelling, showed
them three representative 3D-signature samples of
each victim (printout on papers), and let them watch
the signing process of each victim once. Each of these
six attackers then produced 10 forged 3D-signature
samples for each victim. This way, we collected
6 × 10 × 4 = 240 Insider attack 3D-signature sam-
ples in total.
Combining all ﬁve types of samples, we collected 240 ×
5 = 1, 200 attack 3D-signature samples. From CA sam-
ples to Insider samples, the attackers gained an increasing
amount of prior knowledge about the victims, representing
a broad range of security threats to KinWrite.
6.2 Evaluation Metrics
We adopted standard ROC curves and precision-recall
curves to evaluate the performance of KinWrite. For each
threshold dT , we tried m rounds. For round i, the classi-
ﬁcation results can be divided into the following four cate-
00.20.40.60.8100.20.40.60.81RecallPrecisionn=2n=3n=4~1200.10.20.30.40.60.70.80.91False Positive RateTrue Positive Raten=2n=3~1200.20.40.60.8100.20.40.60.81RecallPrecision00.20.40.60.8100.20.40.60.81False Positive RateTrue Positive RateFigure 12. The performance of KinWrite in
normal cases: the averages and standard de-
viations of the achievable recall at a 100% pre-
cision.
Figure 13. The impact of the sample size on
the feature weight selection: The weights ob-
tained over a randomly selected training set
with 4 samples are similar to the one obtained
over all samples.
gories: tpi, the number of true positives; tni, the number of
true negatives; f pi, the number of false positives, and f ni,
the number of false negatives.
6.3 Evaluation Results
Precision is the percentage of honest users out of all the
users that have passed veriﬁcation, and it reﬂects how cau-
tious the system is to accept a user. A secure system should
have a precision of 100% and will only let honest users pass
the veriﬁcation. Formally,
We performed two sets of experiments utilizing the 3D-
signature samples collected over ﬁve months. The ﬁrst set
of experiments studied the performance of KinWrite in a
normal scenario, where honest users want to authenticate
themselves. The second set of experiments studied the per-
formance of KinWrite under various attacks.
(cid:80)m
(cid:80)m
i=1 tpi +(cid:80)m
i=1 tpi
P recision =
i=1 f pi
.
6.3.1 Normal Case Performance
Recall is the number of true positives over the sum of
true positives and false negatives. It quantiﬁes the fraction
of honest users that have been granted access out of all hon-
est users, and it affects the user experience. Formally,
(cid:80)m
(cid:80)m
i=1 tpi +(cid:80)m
i=1 tpi
.
i=1 f ni
Recall =
A recall of 100% indicates that an honest user can always
pass the veriﬁcation at her ﬁrst trial. A recall of 50% indi-
cates that an honest user has a 50% probability of gaining
access. On average it takes 2 trials to pass the veriﬁcation.
ROC curve stands for receiver operating characteristic
curve and is a plot of true positive rate (TPR) over false
positive rate (FPR). An ideal system has 100% TPR and
0% FPR, i.e., all honest users can pass the veriﬁcation while
none of the attackers can fool the system.
(cid:80)m
i=1 tpi +(cid:80)m
(cid:80)m
(cid:80)m
i=1 f pi +(cid:80)m
(cid:80)m
i=1 f pi
i=1 tpi
i=1 f ni
.
i=1 tni
T P R =
F P R =
By varying the threshold dT , we can achieve varied pre-
cision, recall, TPR and FPR values with which we can draw
precision-recall curves and ROC curves.
In our ﬁrst set of experiments, we divided the genuine sam-
ples into two sets: a training set and a test set. We randomly
selected a subset of n genuine samples for each of the 35
signatures as their training samples and let the remaining
samples be the test set. KinWrite selected a template for
each signature from the training samples, and then used the
test samples to evaluate the veriﬁcation performance. To
study the statistical performance of KinWrite, we conducted
30 rounds of random folding, where for each round, a differ-
ent set of n samples were selected as training samples. We
reported the performance over the 30 rounds of experiments
and for all 35 signatures.
Training Size. We ﬁrst conducted experiments to evalu-
ate the impact of training size n on the veriﬁcation perfor-
mance. In each round, we randomly selected n samples as
the training samples. In total, M = 35 · n training sam-
ples were selected for all signatures. For each signature,
our template selector chose one template and sorted all M
training samples according to the DTW distances, as shown
in Figure 9. By varying the threshold dT , we obtained a
ROC curve and a precision-recall curve. As we tried n in
the range of [2, 12], we obtained a set of ROC curves and a
set of precision-recall curves, as shown in Figure 10, where
performance for each value of n is over 30 rounds and 35
signatures. We observe that the performance is not too sen-
sitive to the selection of n as long as n > 2, and when
n > 2, KinWrite can almost achieve a precision of 100%
510152025303500.51Siganatures Recall(Precision=100%)123456789101112131400.20.40.60.81Feature IndexRate  All SamplesTraining Samples(a) Equally weighted
(b) Without time warping
(c) Feature normalized to [0, 1]
(d) Without Kalman ﬁltering
(e) Equally weighted
(f) Without time warping
(g) Feature normalized to [0, 1]
(h) Without Kalman ﬁltering
Figure 14. The performance comparison of various methods (by signatures) in normal cases. Each
colored curve indicates the performance of verifying one signature. The top row shows the precision-
recall curves, and the bottom one shows the ROC curves.
and a recall of 90%. Thus in the remainder of our experi-
ments, we chose n = 4.
KinWrite Performance. Figure 11 shows the test per-
formance (ROC and precision-recall curves) of the 35 sig-
natures when the training sample size is 4. As before,
we tried 30 rounds of random folding for each signature,
and each curve represents the performance result averaged
over all 30 rounds for a signature. Our experimental re-
sults show that given a requirement of a 100% precision,
we can achieve at least a 70% recall or a 99% recall on
average. Assuming that 3D-signature samples are indepen-
dent, the probability that an honest user passes veriﬁcation
is about 70%. Since the number of successes of n trials
can be considered as a Binomial distribution, the average
70%.
number of trials for a user to pass the veriﬁcation is
In Figure 12, we show the averages of maximum achiev-
able recall for each signature when the precision was 100%,
from which we observed the following: 17 out of 35 signa-
tures can achieve a 100% recall; 13 signatures achieved a
recall higher than 95%, and the rest achieved a recall higher
than 85%. The results suggest that as with text passwords,
some 3D-signatures are better then others. Nevertheless in
our experiments, KinWrite can verify an honest user by 1.4
trials on average without false positives.
1
Feature Weight Selection and Its Impact. Since the
relevancy level of each feature (dimension) varies for veri-
fying a 3D-signature, we weigh each feature differently in
order to achieve a high veriﬁcation performance. Weights
are selected based on the veriﬁcation rate obtained purely
on a small training set. To understand how sensitive weight
selection is to training samples, we calculated weights when
different sets of the samples were used. In the ﬁrst set of ex-
periments, we randomly selected 4 samples from each sig-
nature as the training samples. In total, M = 140 training
samples were selected for all signatures. For each signature,
we calculated the DTW distance between training samples
based on only a single feature. We chose the weight of that
feature as the average veriﬁcation rate of all 35 signatures
(i.e., the percentage of true samples out of the top-ranked 4
samples, when veriﬁying each signature using all M sam-
ples). We repeated the process 10 rounds by selecting 10
different training sets for each signature, and depicted the
derived weights in Figure 13. We observed that the weights
obtained over training sample sets are similar to each other.
We also calculated the weights by considering all the avail-
able samples (shown in Figure 13). The resulting weights
are similar to the ones derived based on training sets, sug-
gesting that weight selection over a small training set suf-
ﬁces.
00.20.40.60.8100.20.40.60.81RecallPrecision00.20.40.60.8100.20.40.60.81RecallPrecision00.20.40.60.8100.20.40.60.81RecallPrecision00.20.40.60.8100.20.40.60.81RecallPrecision00.20.40.60.8100.20.40.60.81False Positive RateTrue Positive Rate00.20.40.60.8100.20.40.60.81False Positive RateTrue Positive Rate00.20.40.60.8100.20.40.60.81False Positive RateTrue Positive Rate00.20.40.60.8100.20.40.60.81False Positive RateTrue Positive Rate(a) Precision-Recall curves
(b) ROC Curves
Figure 15. The average performance (by signatures) in various attack scenarios.
(a) Achievable recall at a 100% precision
(b) Achievable precision at a 75% recall
Figure 16. The performance (by signature) in various attack scenarios.
To evaluate the impact of weighted features on veriﬁca-
tion performance, we modiﬁed KinWrite so that all 14 di-
mensions of the features were equally weighted. Figure 14
(a, e) show the veriﬁcation performance on all 35 signa-
tures of this modiﬁed KinWrite. The results demonstrate
that the proposed weighting features can improve the veri-
ﬁcation performance.
The Role of Dynamic Time Warping. The proposed
DTW allows nonrigid warping along the temporal axis
when measuring the difference between two signatures. To
understand the impact of nonrigid warping on the veriﬁca-
tion performance, we deﬁned the difference between two
signatures (in the form of features) f1(t), t = 1, 2,··· , N1
and f2(t), t = 1, 2,··· , N2 as follows. We re-sampled the
signature features so that they had the same length, e.g.,
N = 50 points, and then calculated the Euclidean distance
between the two signature feature vectors. Figure 14 (b,
f) shows the veriﬁcation performance (on all 35 signatures)
when using this difference metric without warping along the
temporal axis. The results show that the use of nonrigid
warping in DTW can substantially improve the veriﬁcation
performance.
Impact of Kalman Filter and Feature Normalization.
We conducted experiments to justify the choice of Kalman
ﬁlter and feature normalization. First, we modiﬁed our Kin-
Write so that the Kalman ﬁlter was not included, or a dif-
ferent feature normalization method was used by the data
preprocessor, and then we conducted the experiment as be-
fore. Figure 14 (c, g) show the veriﬁcation performance
on all 35 signatures when features were normalized linearly
to the range of [0, 1]. The results show that the proposed
feature normalization method based on N (0, 1) distribution
leads to a better performance. Figure 14 (d, h) show the
veriﬁcation performance on all 35 signatures when the sig-
natures were not smoothed by the proposed Kalman ﬁlter.
From the results, we can conclude that the use of a Kalman
ﬁlter can improve the veriﬁcation performance.
6.3.2 Attack Performance
In the attack experiments, we evaluated how robust Kin-
Write is against various types of attackers. We selected
four signatures as the victims with the spelling being “Bry”,
“Jy”, “ma”, and “Tj”, respectively. We considered the other
31 signatures acquired for the ﬁrst set of experiments as ran-
00.20.40.60.8100.20.40.60.81RecallPrecision  CAOb−1Ob−4CA&OB−4InsiderRandom00.20.40.60.8100.20.40.60.81False Positive RateTrue Positive Rate  CAOb−1Ob−4CA&OB−4InsiderRandom00.20.40.60.81CA        Ob−1      Ob−4    CA&Ob−4   Insider    Random         Recall (Precision=100%)  BryJymaTj00.20.40.60.81CA        Ob−1      Ob−4    CA&Ob−4   Insider    Random         Precision (Recall=75%)  BryJymaTjdom attackers and collected forged data for all types of at-
tackers described in Section 6.1. Similar to the ﬁrst set of
experiments, we divided samples into two sets: a training
set and a test set. For each type of attack, the training set of
a victim signature consists of 4 randomly chosen samples
from each victim signature and this type of attacker sam-
ples. The test set contains the rest of the samples from all
victims and this type of attacker.
For each type of attacker, we performed 30 rounds of
random folding. We averaged precision-recall curves and
ROC curves over 30 rounds for each victim and showed per-
formance results in Figure 15, where each type of attacker
has four identical-colored curves with each corresponding
to one of the four victims. The results show that KinWrite
can with a high probability reject random attackers. ‘Ran-
dom’ indicates a brute force attack– an attacker who has no
clue about the 3D-signatures and signs random texts hoping
to pass the veriﬁcation. The results suggest that KinWrite is
robust against brute force attacks. For other types of attacks,
Kinwrite did not perform as well as for the random attacks,