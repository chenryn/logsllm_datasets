union
ground truth of ¯x, ¯X; ˆx, ˆX; x, X
pre-softmax logits of ¯x, ¯X; ˆx, ˆX; x, X
source indicator of ¯x, ¯X; ˆx, ˆX; x, X
perturbation
NN based classiﬁer
NN based discriminator
reward function of the minimax game
weight parameter in the NN model
trade-off hyper-parameters in CLP, CLS and Gan-
Def
TABLE I: Summary of Notations
Fast Gradient Sign Method (FGSM) is introduced by
Goodfellow et. al in [6] as a single-step white-box adversarial
example generator against NN image classiﬁers. This method
tries to maximize the loss function value, L, of NN classiﬁer,
C, to ﬁnd adversarial examples. The calculation of loss is
the softmax transformation, f (zi) = ezi(cid:80)
usually deﬁned as the difference between ground truth, t, and
ezj , of pre-softmax
L(ˆz = C(ˆx), t)
maximize
subject to F(¯x + δ) ∈ Rm
logits.
zj
δ
[−1,1]
As a single-step generator, only one iteration of gradient ascent
is executed to ﬁnd adversarial examples. It simply generates
examples with perturbation, ˆx, from original images, ¯x, by
adding small perturbation, δ, which changes each pixel value
along the gradient direction of the loss function. Although
running one iteration of gradient ascent algorithm can not
guarantee ﬁnding a solution which is close enough to optimal
one, empirical results show that adversarial examples from
this generator can mislead Vanilla NN classiﬁers. Intuitively,
FGSM runs faster than iterative generators at
the cost of
weaker adversarial examples. That is, the success rate of attack
using the generated examples is relatively low due to the linear
approximation of the loss function landscape.
Basic Iterative Method (BIM) is introduced by Kurakin
et. al in [9] as an iterative white-box adversarial example
generator against NN image classiﬁers. BIM utilizes the same
mathematical model as FGSM but runs the gradient ascent
algorithm iteratively. In each iteration, BIM applies small per-
turbation and maps the perturbed image through the function
F. As a result, BIM approximates the loss function landscape
by linear spline interpolation. Therefore, it generates stronger
examples and achieves higher attack success rate than FGSM
within the same neighboring area.
Projected Gradient Descent (PGD) is another iterative
white-box adversarial example generator recently introduced
by Madry et. al in [14]. Similar to BIM, PGD solves the
same optimization problem iteratively with projected gradient
descent algorithm. However, PGD randomly selects initial
point within a limited area of the original image and repeats
this several times to search adversarial example. Since the
loss landscape has a surprisingly tractable structure [14], PGD
is shown experimentally to solve the optimization problem
efﬁciently and the generated examples are stronger than those
of BIM.
B. Adversarial Example Defensive Methods
Here, we categorize the design of defensive methods against
adversarial examples into three major classes. The ﬁrst is based
on applying data augmentation and regularization during the
training. The second class aims at adding protective shell on
the target classiﬁer, while the last class focuses on utilizing
some adversarial examples to retrain the target classiﬁer. In
the following, we introduce representative examples from each
of the above three approaches:
Augmentation and Regularization usually utilize synthetic
data or regulate hidden states during training to enhance the
test accuracy on adversarial examples. One of the early ideas
in this direction is the defensive distillation. In the context of
adversarial example defense, distillation is done by using the
prediction score from original NN, which is usually called
the teacher, as ground truth to train a smaller NN with
different structure, usually called the student [19] [18]. It has
been shown that the calculated gradients from student model
become very small or even reach zero and hence, can not
be utilized by adversarial example generators [18]. Examples
of recent approaches under this category of defenses include
Fortiﬁed Network [10] and Manifold Mixup [26]. Fortiﬁed
Network utilizes denoising autoencoder to regularize the hid-
den states. With this regularization, trained NN classiﬁers learn
to mitigate the difference in hidden states between original and
adversarial examples. Manifold Mixup also focus on hidden
states but follows a different way. During training, Manifold
Mixup uses interpolations of hidden states and logits instead
of original training data to achieve regularization. However,
this set of defenses is shown to be not very reliable as they
are vulnerable to certain adversarial examples. For example,
defensive distillation is vulnerable to Carlini attack [4] and
Manifold Mixup can only defend against single step attacks.
Protective Shell is a set of defensive methods designed
to reject or reform adversarial examples. Meng et. al intro-
duced an approach called MagNet [15] which falls under this
category. MagNet has two types of functional components;
the detector and the reformer. Adversarial examples are either
rejected by the detector or reformed by the reformer to clean
up adversarial perturbations. Other recent approaches like
[12], [30] and [22] also fall under this category and they
are differentiated by the way they implement the protective
shell. In [12], authors carefully inject adaptive noise to input
images to break adversarial perturbations without signiﬁcantly
degrading classiﬁcation accuracy. In [30], a key based cryptog-
raphy method is utilized to differentiate adversarial examples
from original ones. In [22], a generator is utilized to generate
images that are similar to the inputs. By replacing the inputs
3
with generated images, the approach shows good resistance to
adversarial examples. The main limitation of the approaches
under this category is the assumption that the shell is black-
box to adversary, which turns to be inaccurate. For example,
[2] presented different ways to break this assumption.
Adversarial Training is based on the idea that adversarial
examples can be considered as blind spots of the original
training data [29]. By retraining with samples of adversarial
examples, the classiﬁer learns perturbation patterns from ad-
versarial examples and generalizes its prediction to account for
such perturbations. In [6], adversarial examples generated by
FGSM are used for adversarial training of a NN classiﬁer. The
results show that the retrained classiﬁer can correctly classify
adversarial examples of this single step attack (FGSM). Later
works in [14] and [25] enhance the adversarial training process
so that the trained models can defend not only single step
attacks but also iterative attacks like BIM and PGD. A more
recent work under this category [7] introduces two zero knowl-
edge adversarial training defenses. The defenses use Gaussian
random noise for perturbations and include a penalty term
based on pre-softmax logits, z. However, the design of penalty
term is simple and not ﬂexible enough to handle complex
patterns in z. Our goal in this work is to design a ﬂexible zero
knowledge defense that handles z in a more sophisticated way
to achieve higher test accuracy on adversarial examples.
III. ZK-GANDEF: GAN BASED ZERO KNOWLEDGE
ADVERSARIAL TRAINING DEFENSE
In this section, we ﬁrst introduce existing zero knowledge
adversarial training defenses, then, we present the design and
the algorithmic details of ZK-GanDef.
A. Zero Knowledge Adversarial Training
Recall that full knowledge adversarial training defenses re-
train NN classiﬁer with adversarial examples. Since adversarial
examples are created by solving an optimization problem,
its preparation consumes signiﬁcant amount of computation,
especially when iterative adversarial examples are utilized.
Based on experiments in [7], generating adversarial examples
on Imagenet dataset requires a cluster of GPU servers. To
overcome this limitation, authors in [7] also introduce two
zero knowledge adversarial
training defenses dubbed CLP
and CLS. Instead of retraining with adversarial examples,
these approaches retrain with examples perturbed with random
Gaussian noise. The idea here is to speedup the training
process by eliminating the computationally expensive step
of adversarial examples generation. The caveat, however, is
that since the retraining is performed with ”fake” adversarial
examples, the test accuracy against ”true” adversarial examples
degrades.
The training process of CLP is visualized in Figure 2a.
The retraining dataset consists of several pairs of randomly
sampled original examples perturbed with random Gaussian
noise. After the feed forward pass through the NN classiﬁer,
two different pre-softmax logits are generated. The differences
between these pre-softmax logits and their corresponding
ground truths are calculated as the ﬁrst part of the total loss.
The l2 norm of the difference between these two pre-softmax
logits is also calculated and used as the second part in the
total loss. Based on the total loss, the weights, Ω, of the NN
classiﬁer are updated by gradient descent algorithm and back
propagation. The training loss of CLP can be summarized as
follows:
LCLP(C) = L(ˆz1 = C(ˆx1), ˆt1) + L(ˆz2 = C(ˆx2), ˆt2)
+ λl2(C(ˆx1) − C(ˆx2))
The training process of the other zero knowledge approach,
CLS, is shown in Figure 2b. Similar to CLP, CLS retrains with
examples perturbed with random Gaussian noise. However,
instead of using pairs of inputs, CLS uses individual inputs
to the NN classiﬁer in the forward pass. The ﬁrst term of
the total loss in CLS is still calculated by a predeﬁned loss
function of pre-softmax logits and the corresponding ground
truths. Different from the CLP, CLS directly calculates the l2
norm of pre-softmax logits as the second term in its total loss.
Thereafter, it follows the same training process with gradient
descent algorithm and back propagation to update the weights,
Ω, in the NN classiﬁer. The loss function of CLS is as follows:
LCLS(C) = L(ˆz = C(ˆx), ˆt) + λl2(C(ˆx))
The hypothesis behind CLP and CLS is that abnormal
large values in pre-softmax logits are signals of adversarial
examples. Therefore, they both add penalty term to the loss
function during the training in order to prevent such over
conﬁdent predictions. Although the penalty terms are different,
both defenses encourage the NN classiﬁer to output small and
smooth pre-softmax logits.
B. Design of ZK-GanDef
As mentioned in the previous subsection, CLP and CLS try
to prevent overconﬁdent predictions by penalizing high pre-
softmax logits. However, the penalty terms used are oversim-
pliﬁed and do not utilize other valuable information contained
in the pre-softmax logits. This results, as we see in the
evaluation section, in poor accuracy on complex datasets. On
the other hand, our ZK-GanDef is designed to better utilize
the rich information available in the pre-softmax logits. As
Figure 2c shows, Zk-Gandef comprises a classiﬁer and a
discriminator. The input to the classiﬁer includes both original
images and randomly perturbed examples. It has been shown
in transfer learning, [5], that the pre-softmax logits output of
the classiﬁer relates to the extracted features from its inputs.
Therefore, we use a discriminator to identify whether the
logit output of the classiﬁer belongs to an original image or
a perturbed example. The intuition here is that the features
extracted by a Vanilla NN classiﬁer from perturbed examples
will contain some kind of perturbations, and hence can be
recognized by a trained discriminator.
In this work, we envision that the classiﬁer could be seen
as a generator that generates pre-softmax logits based on
selected features from inputs. Then, the classiﬁer and the
4
(a) Clean Logit Pairing
(b) Clean Logit Squeezing
(c) ZK-GanDef
Fig. 2: Training Procedure of Different Zero Knowledge Defenses
discriminator engage in a minmax game, which is also known
as Generative Adversarial Net (GAN) [5]. In this minimax
game, the discriminator tries to make perfect prediction about
the source of inputs (original or perturbed). At the same time,
the classiﬁer tries to correctly classify inputs as well as hide the
source information from the discriminator. This process trains
a classiﬁer which makes prediction based on perturbation
invariant features from inputs, as well as a discriminator which
can identify whether the features used by the fellow classiﬁer
contain any perturbations. Through training in this competition
game, the feature learning in the classiﬁer is regulated by the
discriminator and it ﬁnally leads to defense against adversarial
examples.
Compared with the CLP and CLS, ZK-GanDef has a more
sophisticated way of utilizing pre-softmax logits. Instead of
encouraging the NN classiﬁer to make small and smooth logits,
ZK-GanDef aims at differentiating the latent pattern of logits
between original
images and examples with perturbations.
Therefore, the NN classiﬁer in ZK-GanDef is encouraged to
select perturbation invariant features, which enhance its test
accuracy of adversarial examples on complex datasets. It is
worth to mention that an example with Gaussian perturbation
is not necessary to be an adversarial example. However, results
in [7] show that defenses against adversarial examples can be
built by training against examples with Gaussian perturbation.
Our method is built upon this empirical conclusion.
C. ZK-GanDef Training Algorithm
Given the training data pair (cid:104)x, t(cid:105), where x ∈ ∪( ¯X, ˆX),
we try to ﬁnd a classiﬁcation function C, which uses x
to give a proper pre-softmax logits z corresponding to t.
The goal is to train the classiﬁer in ZK-GanDef to model
the conditional probability qC(z|x) with only perturbation
invariant features. To achieve this, we employ another NN
and call it a discriminator D. D uses the pre-softmax logits z
from C as inputs and predicts whether the input image to C
was ¯x or ˆx. This process can be performed by maximizing
the conditional probability qD(s|z), where s is a Boolean
variable indicating whether the input to C was original or
randomly perturbed image. The combined minmax problem
of the classiﬁer and the discriminator is formulated as:
minC maxD J(C,D)
5