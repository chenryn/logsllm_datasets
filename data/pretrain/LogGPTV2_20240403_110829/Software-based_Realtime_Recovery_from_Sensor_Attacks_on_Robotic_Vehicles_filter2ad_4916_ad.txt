### Real Testbed

Our real testbed consists of two commercial robotic vehicles (RVs): a 3DR Solo [1] and an Erle-Rover [18]. The 3DR Solo is a typical commercial quadcopter that uses a variety of heterogeneous and redundant sensors to ensure flight stability. This aerial vehicle is highly dynamic and can be significantly affected by environmental factors. The 3DR Solo system is based on the Pixhawk 2 autopilot, which is part of the open-source Pixhawk project [44]. It uses APM:Copter, an open-source flight controller based on the MAVLink protocol and part of the ArduPilot project [2].

The Erle-Rover, on the other hand, is a representative ground RV equipped with various sensors. It is implemented with the Erle-Brain 3, a Linux-based system provided by Erle Robotics. For the rover, we use the open-source control software APMrover 2.

Table 2 lists the sensors in the 3DR Solo and Erle-Rover. The 3DR Solo has 12 sensors, while the Erle-Rover has 6. Many sensors are replicated with different hardware to avoid single points of failure. For example, the 3DR Solo has three gyroscope sensors from different manufacturers. To compromise all sensors of the same type, an attacker would need to employ different attack techniques for each sensor.

### Attack and Recovery Setting

To generate the physical sensor attacks discussed earlier (see Section 1), we insert attack modules into the firmware. Since implementing actual hardware attacks requires special devices, we simulate the effects using these attack modules, without accessing the internal hardware. Specifically, we add malicious code to the sensor interface that transmits sensor measurements to the main closed control loop. The attacks modify sensor measurements to mimic the effect of real "controlled attacks" that manipulate sensor readings (e.g., sinusoidal waves, random or selected values). We focus on continuous attacks rather than instantaneous ones, as temporary attacks can be easily recovered by our method. We map MAVLink commands to various attack types to remotely trigger them via the ground control station.

Recovery is considered successful if, after an attack is launched, the technique detects it and triggers the recovery logic to ensure that the current states remain within a certain error bound of the expected states for a specified period of time. Mathematically, this is defined as:
\[ R_{\text{succ}} := |Y_t - \bar{Y}_t| \leq \epsilon, \quad t \in [1...k] \]
where \( Y_t \) is the real output, \( \bar{Y}_t \) is the predicted output, \( \epsilon \) is the error margin, \( t \) is the timestamp in the recovery mode, and \( k \) is the maximum time to decide recovery success. For example, \( \epsilon = 3 \) meters and \( k = 10 \) seconds indicate that the RV performs missions within 3 meters of error for 10 seconds under the recovery mode.

Our recovery technique does not consider previous maneuvers (at \( t \leq 0 \)) because our software sensors accurately predict real measurements during various maneuvers (Figure 12). As long as recovery starts with accurate initial states via software sensors, subsequent sensor feedback is precise, and the control loop can stabilize the vehicle and recover. Our goal is to prevent immediate crashes and provide transition time for emergency operations (e.g., manual mode), not to permanently replace compromised sensors. Therefore, after recovery mode is activated, the vehicle will conduct stable operations (e.g., hovering) before switching to emergency operation.

### Experiments and Results

#### 4.2.1 Efficiency

**Space Overhead:**
We measure the firmware size before and after inserting our recovery code. For runtime overhead, we compare the execution time of the main control loop before and after. Specifically, we first measure the (space and runtime) cost of the original code as a baseline, which does not include the recovery code. Then, for each sensor, we insert the recovery code, including the required libraries (e.g., filters and utility functions), and measure the overhead. Finally, we insert all the recovery code for all sensors to obtain the total overhead.

As shown in Figure 10, the increase in code size (i.e., additional firmware size needed) incurred by our recovery modules is marginal. The space overhead is at most 1.3% when all software sensors are loaded and less than 0.7% for individual sensors. Some code pieces are shared across software sensors, and the simulated vehicles have negligible overhead since their executables are relatively larger than those of the real vehicles.

**Runtime Overhead:**
We measure the average per-iteration execution time of the main loop, which includes various control functions and auxiliary tasks. In ArduCopter and APMrover2, the system loop execution frequency is 400 Hz and 50 Hz, respectively. Every 2.5 ms or 20 ms, the scheduler executes the control functions and then schedules auxiliary tasks using the remaining time in the epoch. All tasks must be completed within the hard deadline (i.e., 2.5 ms or 20 ms).

Figure 10 shows the results. The runtime overhead introduced by the recovery module for single sensor recovery is at most 6.9%, whereas, for multiple sensor recovery, the total overhead is at most 8.8%. We also consider the CPU utilization rate (for real vehicles), which is the iteration execution time over the hard deadline. For the 3DR Solo, the rate increases from 63.32% to at most 67.68% (i.e., by 4.36%) for single sensor recovery, and to 68.88% (i.e., by 5.56%) for multiple sensor recovery. For the Erle-Rover, the rate increases from 26.7% to at most 27.8% (i.e., by 0.9%), and to 28.4% (i.e., by 1.7%) for single and multiple sensor recovery, respectively. The observed overhead does not impact normal operations, as the per-iteration runtime does not exceed the hard deadline. Real recovery cases in Section 4.3 demonstrate that our technique is practically effective.

#### 4.2.2 Effectiveness

We evaluate effectiveness as follows:

1. **Software Sensors:** Figure 11 shows how closely software sensors predict (blue lines) the real readings (red lines) in various maneuvers of the 3DR Solo. There are errors (e.g., drift and external errors) between the predictions and the real measurements, which we remove using error correction techniques.
   
2. **Error Corrections:** Figure 12 shows the drift in the roll angle prediction before and after error correction. We measure the roll value and the prediction error during a real flight of the quadrotor (left of (a)). As shown in (b), at each window start, the initial state is synchronized, and the accumulated error is reset. This significantly reduces the accumulated error (right of (b)). In this experiment, we used a 1.0-second window size with the main sampling rate \( T_s = 2.5 \) ms. The results for other sensors and vehicles are similar.

3. **Parameter Selection:** We study the effect of recovery parameters (i.e., window size and recovery switch threshold) on the recovery mode activation. We generate 20 missions (i.e., a sequence of primitive moves like straight fly, turns, etc.) with no attack to measure the false positive (FP) rates (i.e., how many times recovery is activated unnecessarily), and 20 missions with injected attacks to measure the false negative (FN) rates (i.e., how many times recovery activation is missed). Figure 14 shows the results for different parameter settings.