### 5.2.3 Resource Considerations

RDMA requires applications to either poll for completions (which consumes a full core) or detect completions via interrupts (which incurs high overhead and delay). RDMC employs a hybrid solution, but we wanted to investigate whether this approach has any negative performance impacts.

Our initial test, which is not shown, involved running the system with pure polling. We found that this method did not provide a measurable performance improvement over the hybrid approach.

Next, as illustrated in Figure 11, we compared RDMC in its standard hybrid mode with a version using pure interrupts, eliminating polling. In the latter case, CPU loads (not graphed) were significantly lower: they decreased from nearly 100% for all runs with polling enabled to around 10% for 100 MB transfers and 50% for 1 MB transfers. For 10 KB transfers, the difference was minimal due to the time spent processing blocks. Despite the substantial reduction in CPU usage, the impact on bandwidth was minimal, especially for large transfers. A pure-interrupt mode may be beneficial for computationally intensive workloads that send large messages, provided that the slight increase in transfer delay is acceptable.

On hardware that supports CORE-Direct, we can offload an entire transfer sequence as a partially-ordered graph of asynchronous requests. Our preliminary experiments were only partially successful due to a firmware bug (a NIC hardware issue) that prevented us from testing our full range of protocols. Figure 12 shows results for chain send, where the request pattern is simple and the bug did not occur. The left graph uses a hybrid of polling and interrupts, while the right graph uses pure interrupts. As seen in the graphs, cross-channel generally provides a speedup of about 5%, although there is one scenario (a single sender transmitting in groups of size 5-8 in polling-only mode) where the standard RDMC solution outperforms the others.

### 5.3 Future Work: RDMC on TCP

When Ganesan and Seshadri first explored multicast overlay topologies, they expressed concern that even a single lagging node might cause cascading delays, impacting every participant and limiting scalability [7]. This led them to focus their work on dedicated, synchronous, HPC settings, justifying the assumption that nodes would run in lock-step and not be exposed to scheduling delays or link congestion.

However, today's RDMA operates in multi-tenant environments. Even supercomputers host large numbers of jobs, making them susceptible to link congestion. RDMA in standard Ethernet settings uses a TCP-like congestion control (DCQCN or TIMELY). Yet, we do not observe a performance collapse at scale. Our slack analysis suggests a possible explanation: the binomial pipeline generates a block-transfer schedule that allows delayed nodes to catch up. As we scale up, various delays do occur, but the slack apparently compensates, reducing the slowdown.

This observation has an interesting practical consequence: it suggests that RDMC might work surprisingly well over high-speed datacenter TCP (with no RDMA) and perhaps even in a WAN network. In ongoing work, we are porting RDMC to access RDMA through LibFabrics from the OpenFabrics Interface Alliance (OFI) [16]. LibFabrics is a mature solution used as the lowest layer of the message passing interface (MPI) library for HPC computing. The package uses a macro expansion approach and maps directly to RDMA as well as to other hardware accelerators or even standard TCP. Once the port is complete, we plan to closely study the behavior of RDMC in various TCP-only settings.

### 6 Related Work

Replication is a rich area with numerous software libraries and systems. We have mentioned reliable multicast primarily to emphasize that RDMC is designed for data replication but does not offer the associated strong group semantics and multicast atomicity. Paxos is the most famous state machine replication (consensus) technology. Examples of systems in this category include the classical Paxos protocol itself, our Derecho library, libPaxos, Zookeeper’s ZAB layer, the head-of-log mechanism in Corfu, DARE, and APUs [1, 9, 10, 12, 13, 18, 24]. Derecho demonstrates that RDMC can be useful in Paxos solutions, but also highlights that additional mechanisms are needed when doing so, as RDMC has weaker semantics than Paxos.

We are not the first to explore how RDMA should be exploited in the operating system. The early RDMA concept dates back to a classic paper by Von Eicken and Vogels [23], which introduced the zero-copy option and reprogrammed a network interface to demonstrate its benefits. The Virtual Interface Architecture (VIA) then emerged; its "Verbs" API extended the UNet idea to support hardware from Infiniband, Myrinet, QLogic, and other vendors. The Verbs API used by RDMC is widely standardized, but other options include the QLogic PSM subset of RDMA, Intel’s Omni-Path Fabric solution, and socket-level offerings such as the Chelsio WD-UDP [3] embedding.

Despite the multitude of products, the biggest success to date has been the MPI platform integration with Infiniband RDMA, which has become the mainstay of HPC communications. MPI itself provides a multicast primitive similar to the one described in this paper, but the programming model imposed by MPI has several limitations that make it unsuitable for the applications RDMC targets: (1) send patterns are known in advance, allowing receivers to anticipate the exact size and root of any multicast before it is initiated, (2) fault tolerance is handled by checkpointing, and (3) the set of processes in a job must remain fixed for the duration of that job. Even so, RDMC still outperforms the popular MVAPICH implementation of MPI by a significant margin.

Broadcast is also important between CPU cores, and the Smelt library [11] provides a novel approach to address this challenge. Their solution is not directly applicable to our setting because they deal with tiny messages that do not require the added complexity of being broken into blocks, but the idea of automatically inferring reasonable send patterns is intriguing.

Although our focus is on bulk data movement, the core argument here is perhaps closest to those made in recent operating systems papers, such as FaRM [5], Arrakis [17], and IX [2]. In these works, the operating system is increasingly viewed as a control plane, with the RDMA network treated as an out-of-band technology for the data plane that works best when minimally disrupted. From this perspective, RDMC can be seen as a generic data plane solution well suited to out-of-band deployments. A recent example of a database optimized to use RDMA is Crail [20].

### 7 Conclusions

Our paper introduces RDMC, a new reliable memory-to-memory replication tool implemented over RDMA unicast. RDMC is available for download as a free, open-source library and should be of direct use in O/S services that currently move objects either one by one or over sets of side-by-side TCP links. The protocol can also be used as a component in higher-level libraries with stronger semantics.

RDMC performance is very high compared to the most widely used general-purpose options, and the protocol scales to large numbers of replicas. RDMC yields a benefit even if just three replicas are desired. In fact, replication turns out to be remarkably inexpensive relative to creating just one copy: one can have 4 or 8 replicas for nearly the same price as one, and it takes just a few times as long to make hundreds of replicas as it takes to make one. Additionally, RDMC is robust to various kinds of delays: normal network issues of data loss and duplication are handled by RDMA, while RDMC’s block-by-block sending pattern and receiver-receiver relaying compensate for occasional scheduling and network delays. The RDMC code base is available for download as part of the Derecho platform (https://GitHub.com/Derecho-Project).

### Acknowledgements

We are grateful to the DSN reviewers, Michael Swift, and Heming Cui. LLNL generously provided access to its large computer clusters, as did the U. Texas Stampede XSEDE computing center. Additional support was provided by DARPA under its MRC program, NSF, and AFOSR. Mellanox provided high-speed RDMA hardware.

### References

[References are listed as in the original text, with minor formatting adjustments for clarity.]

---

This revised text aims to be more clear, coherent, and professional, with improved readability and structure.