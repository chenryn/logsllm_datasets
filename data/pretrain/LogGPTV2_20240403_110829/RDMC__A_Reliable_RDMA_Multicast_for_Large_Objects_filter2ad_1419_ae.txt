A second experiment looked at group overlap in a more con-
trolled manner with a ﬁxed multicast message size. In Figure
10 we construct sets of groups of the size given by the X-axis
label. The sets have identical members (for example, the 8-
node case would always have the identical 8 members), but
different senders. At each size we run 3 experiments, vary-
ing the number of senders. (1) In the experiment correspond-
ing to the solid line, all members are senders (hence we have
8 perfectly overlapped groups, each with the same members,
but a different sender). (2) With the dashed line, the number
of overlapping groups is half the size: half the members are
senders. (3) Finally, the dotted line shows performance for a
single group spanning all members but with a single sender.
All senders run at the maximum rate, sending messages of the
size indicated. Then we compute bandwidth by measuring the
time to transfer a given sized message to all of the overlapping
groups, and dividing by the message size times the number of
groups (i.e. the total bytes sent).
Again, we see that full resources of the test systems were
efﬁciently used. On Fractus, with a full bisection capacity
of 100Gbps, our peak rate (seen in patterns with concurrent
senders) was quite close to the limits, at least for larger mes-
sage sizes. On Apt, which has an oversubscribed TOR, the
bisection bandwidth approaches 16Gbps for this pattern of
communication, and our graphs do so as well, at least for the
larger groups (which generated enough load to saturate the
TOR switch).
5.2.3 Resource Considerations
RDMA forces applications to either poll for completions
(which consumes a full core), or to detect completions via in-
terrupts (which incurs high overheads and delay). RDMC uses
a hybrid solution, but we wanted to understand whether this
has any negative impacts on performance. Our ﬁrst test isn’t
shown: we tested the system with pure polling, but found that
this was not measurably faster than the hybrid.
Next, as shown in Figure 11 we compared RDMC in its
standard hybrid mode with a version running using pure in-
terrupts, so that no polling occurs. For the latter case, CPU
loads (not graphed) are deﬁnitely lower: they drop from al-
most exactly 100% for all runs with polling enabled, to around
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:50:05 UTC from IEEE Xplore.  Restrictions apply. 
80
10% for 100 MB transfers and 50% for 1 MB transfers. With
10 KB transfers, there was only a minimal difference since
so much time was spent processing blocks. Despite the con-
siderable improvement in CPU usage, the bandwidth impact is
quite minimal, particularly for large transfers. A pure-interrupt
mode may be worthwhile for computationally intensive work-
loads that send large messages, provided that the slightly in-
creased transfer delay isn’t a concern.
On hardware that supports CORE-Direct we can ofﬂoad an
entire transfer sequence as a partially-ordered graph of asyn-
chronous requests. Here, our preliminary experiments were
only partially successful: a ﬁrmware bug (a NIC hardware is-
sue) prevented us from testing our full range of protocols. Fig-
ure 12 shows results for chain send, where the request pattern
is simple and the bug did not occur. The left graph uses a hy-
brid of polling and interrupts, while the right graph uses pure
interrupts. As seen in the graphs, cross-channel generally pro-
vides a speedup of about 5%, although there is one scenario (a
single sender transmitting in groups of size 5-8, in polling-only
mode) in which our standard RDMC solution wins.
5.3 Future Work: RDMC on TCP
When Ganesan and Seshadri ﬁrst explored multicast overlay
topologies, they expressed concern that even a single lagging
node might cause cascading delay, impacting every partici-
pant and limiting scalability [7]. This led them to focus their
work on dedicated, synchronous, HPC settings, justifying an
assumption that nodes would run in lock-step and not be ex-
posed to scheduling delays or link congestion.
However, today’s RDMA operates in multi-tenant environ-
ments. Even supercomputers host large numbers of jobs, and
hence are at risk of link congestion. RDMA in standard Eth-
ernet settings uses a TCP-like congestion control (DCQCN or
TIMELY). Yet we do not see performance collapse at scale.
Our slack analysis suggests a possible explanation:
the bi-
nomial pipeline generates a block-transfer schedule in which
there are opportunities for a delayed node to catch up. As we
scale up, delays of various kinds do occur. Yet this slack ap-
parently compensates, reducing the slowdown.
The observation has an interesting practical consequence: it
suggests that RDMC might work surprisingly well over high
speed datacenter TCP (with no RDMA), and perhaps even in a
WAN network. In work still underway, we are porting RDMC
to access RDMA through LibFabrics from the OpenFabrics
Interface Alliance (OFI) [16]. LibFabrics is a mature solution
used as the lowest layer of the message passing interface (MPI)
library for HPC computing. The package uses a macro expan-
sion approach and maps directly to RDMA as well as to other
hardware accelerators, or even standard TCP. When the port is
ﬁnished, we plan to closely study the behavior of RDMC in a
variety of TCP-only settings.
6 Related Work
Replication is an area rich in software libraries and systems.
We’ve mentioned reliable multicast, primarily to emphasize
that RDMC is designed to replicate data, but is not intended
to offer the associated strong group semantics and multicast
atomicity. Paxos is the most famous state machine replication
(consensus) technology. Examples of systems in this category
include the classical Paxos protocol itself, our Derecho library,
libPaxos, Zookeeper’s ZAB layer, the head-of-log mechanism
in Corfu, DARE, and APUs [1, 9, 10, 12, 13, 18, 24]. Derecho
demonstrates that RDMC can be useful in Paxos solutions, but
also that additional mechanisms are needed when doing so:
RDMC has weaker semantics than Paxos.
We are not the ﬁrst to ask how RDMA should be exploited
in the operating system. The early RDMA concept itself dates
to a classic paper by Von Eicken and Vogels [23], which in-
troduced the zero-copy option and reprogrammed a network
interface to demonstrate its beneﬁts. VIA, the virtual interface
architecture then emerged; its “Verbs” API extended the UNet
idea to support hardware from Inﬁniband, Myrinet, QLogic
and other vendors. The Verbs API used by RDMC is widely
standard, but other options include the QLogic PSM subset of
RDMA, Intel’s Omni-Path Fabric solution, socket-level offer-
ings such as the Chelsio WD-UDP [3] embedding, etc.
Despite the huge number of products, it seems reasonable to
assert that the biggest success to date has been the MPI plat-
form integration with Inﬁniband RDMA, which has become
the mainstay of HPC communications. MPI itself actually
provides a multicast primitive similar to the one described in
this paper, but the programming model imposed by MPI has
a number of limitations that make it unsuitable for the appli-
cations that RDMC targets: (1) send patterns are known in
advance so receivers can anticipate the exact size and root of
any multicast prior to it being initiated, (2) fault tolerance is
handled by checkpointing, and (3) the set of processes in a
job must remain ﬁxed for the duration of that job. Even so,
RDMC still outperforms the popular MVAPICH implementa-
tion of MPI by a signiﬁcant margin.
Broadcast is also important between CPU cores, and the
Smelt library [11] provides a novel approach to address this
challenge. Their solution is not directly applicable to our set-
ting because they deal with tiny messages that don’t require the
added complexity of being broken into blocks, but the idea of
automatically inferring reasonable send patterns is intriguing.
Although our focus is on bulk data movement, the core ar-
gument here is perhaps closest to the ones made in recent op-
erating systems papers, such as FaRM [5], Arrakis [17] and
IX [2]. In these works, the operating system is increasingly
viewed as a control plane, with the RDMA network treated as
an out of band technology for the data plane that works best
when minimally disrupted. Adopting this perspective, one can
view RDMC as a generic data plane solution well suited to
out-of-band deployments. A recent example of a database op-
timized to use RDMA is Crail [20].
7 Conclusions
Our paper introduces RDMC: a new reliable memory-to-
memory replication tool implemented over RDMA unicast.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:50:05 UTC from IEEE Xplore.  Restrictions apply. 
81
RDMC is available for download as a free, open-source li-
brary, and should be of direct use in O/S services that currently
move objects either one by one, or over sets of side-by-side
TCP links. The protocol can also be used as a component in
higher level libraries with stronger semantics.
RDMC performance is very high when compared with the
most widely used general-purpose options, and the protocol
scales to large numbers of replicas. RDMC yields a beneﬁt
even if just 3 replicas are desired.
In fact replication turns
out to be remarkably inexpensive, relative to just creating one
copy: one can have 4 or 8 replicas for nearly the same price as
for 1, and it takes just a few times as long to make hundreds of
replicas as it takes to make 1. Additionally, RDMC is robust
to delays of various kinds: Normal network issues of data loss
and duplication are handled by RDMA while RDMC’s block-
by-block sending pattern and receiver-receiver relaying com-
pensate for occasional scheduling and network delays. The
RDMC code base is available for download as part of the Dere-
cho platform (https://GitHub.com/Derecho-Project).
Acknowledgements
We are grateful to the DSN reviewers, Michael Swift, and
Heming Cui. LLNL generously provided access to its large
computer clusters, as did the U. Texas Stampede XSEDE com-
puting center. Additional support was provided by DARPA un-
der its MRC program, NSF, and AFOSR. Mellanox provided
high speed RDMA hardware.
References
[1] BALAKRISHNAN, M., MALKHI, D., DAVIS, J. D., PRABHAKARAN,
V., WEI, M., AND WOBBER, T. CORFU: A Distributed Shared Log.
ACM Trans. Comput. Syst. 31, 4 (Dec. 2013), 10:1–10:24.
[2] BELAY, A., PREKAS, G., KLIMOVIC, A., GROSSMAN, S.,
KOZYRAKIS, C., AND BUGNION, E. IX: A Protected Dataplane Oper-
ating System for High Throughput and Low Latency. In Proceedings of
the 11th USENIX Symposium on Operating Systems Design and Imple-
mentation (OSDI 14) (Broomﬁeld, CO, Oct. 2014), USENIX Associa-
tion, pp. 49–65.
[3] Low latency UDP Ofﬂoad solutions | Chelsio Communications. http:
//www.chelsio.com/nic/udp-offload/. Accessed: 24 Mar
2015.
[4] CHOWDHURY, M., ZAHARIA, M., MA, J., JORDAN, M. I., AND STO-
ICA, I. Managing Data Transfers in Computer Clusters with Orchestra.
In Proceedings of the ACM SIGCOMM 2011 Conference (New York,
NY, USA, 2011), SIGCOMM ’11, ACM, pp. 98–109.
[5] DRAGOJEVI ´C, A., NARAYANAN, D., CASTRO, M., AND HODSON,
O. FaRM: Fast Remote Memory. In Proceedings of the 11th USENIX
Symposium on Networked Systems Design and Implementation (NSDI
14) (Seattle, WA, 2014), USENIX Association, pp. 401–414.
[6] ED HARRIS.
It’s all about big data, cloud storage, and a million
gigabytes per day. https://blogs.bing.com/jobs/2011/10/
11/its-all-about-big-data-cloud-storage-and-a-
million-gigabytes-per-day, Oct. 2011.
[7] GANESAN, P., AND SESHADRI, M. On Cooperative Content Distribu-
tion and the Price of Barter. In 25th IEEE International Conference on
Distributed Computing Systems, 2005. ICDCS 2005. Proceedings (June
2005), pp. 81–90.
[8] GHEMAWAT, S., GOBIOFF, H., AND LEUNG, S.-T. The Google File
System. In Proceedings of the Nineteenth ACM Symposium on Operat-
ing Systems Principles (New York, NY, USA, 2003), SOSP ’03, ACM,
pp. 29–43.
[9] JHA, S., BEHRENS, J., GKOUNTOUVAS, T., MILANO, M., SONG, W.,
TREMEL, E., ZINK, S., BIRMAN, K. P., AND VAN RENESSE, R. Build-
ing smart memories and cloud services with derecho, 2017.
[10] JUNQUEIRA, F. P., AND REED, B. C. The Life and Times of a
Zookeeper.
In Proceedings of the Twenty-ﬁrst Annual Symposium on
Parallelism in Algorithms and Architectures (New York, NY, USA,
2009), SPAA ’09, ACM, pp. 46–46.
[11] KAESTLE, S., ACHERMANN, R., HAECKI, R., HOFFMANN, M.,
RAMOS, S., AND ROSCOE, T. Machine-aware atomic broadcast trees
for multicores. In 12th USENIX Symposium on Operating Systems De-
sign and Implementation (OSDI 16) (GA, 2016), USENIX Association,
pp. 33–48.
[12] LAMPORT, L. The Part-time Parliament. ACM Trans. Comput. Syst. 16,
2 (May 1998), 133–169.
[13] LibPaxos:
Open-source
Paxos.
http://
libpaxos.sourceforge.net/. Accessed: 24 Mar 2015.
[14] MELLANOX CORPORATION. CORE-Direct: The Most Advanced
http:
Technology for MPI/SHMEM Collectives Ofﬂoads.
//www.mellanox.com/related-docs/whitepapers/
TB CORE-Direct.pdf, May 2010.
[15] MITTAL, R., LAM, V. T., DUKKIPATI, N., BLEM, E., WASSEL, H.,
GHOBADI, M., VAHDAT, A., WANG, Y., WETHERALL, D., AND
ZATS, D. TIMELY: RTT-based Congestion Control for the Datacenter.
In Proceedings of the 2015 ACM Conference on Special Interest Group
on Data Communication (New York, NY, USA, 2015), SIGCOMM ’15,
ACM, pp. 537–550.
[16] OPENFABRICS INTERFACES (OFI).
LibFabric: Open-Source Li-
https://
brary for Exploiting Fabric Communication Services.
ofiwg.github.io/libfabric/. Accessed: 11 Apr 2018.
[17] PETER, S., LI, J., ZHANG, I., PORTS, D. R. K., WOOS, D., KRISH-
NAMURTHY, A., ANDERSON, T., AND ROSCOE, T. Arrakis: The Op-
erating System is the Control Plane. In Proceedings of the 11th USENIX
Symposium on Operating Systems Design and Implementation (OSDI
14) (Broomﬁeld, CO, Oct. 2014), USENIX Association, pp. 1–16.
[18] POKE, M., AND HOEFLER, T. Dare: High-performance state machine
replication on rdma networks. In Proceedings of the 24th International
Symposium on High-Performance Parallel and Distributed Computing
(New York, NY, USA, 2015), HPDC ’15, ACM, pp. 107–118.
[19] SHIVARAM VENKATARAMAN, AUROJIT PANDA, KAY OUSTERHOUT
ALI GHODSI, MICHAEL J. FRANKLIN, BENJAMIN RECHT, ION STO-
ICA. Drizzle: Fast and Adaptable Stream Processing at Scale.
[20] STUEDI, P., TRIVEDI, A., PFEFFERLE, J., STOICA, R., METZLER,
B., IOANNOU, N., AND KOLTSIDAS, I. Crail: A high-performance
i/o architecture for distributed data processing.
IEEE Bulletin of the
Technical Committee on Data Engineering, Special Issue on Distributed
Data Management with RDMA 40 (2017), 40–52.
[21] VAN RENESSE, R., AND SCHNEIDER, F. B. Chain Replication for
Supporting High Throughput and Availability.
In Proceedings of the
6th Conference on Symposium on Opearting Systems Design & Imple-
mentation - Volume 6 (Berkeley, CA, USA, 2004), OSDI’04, USENIX
Association, pp. 7–7.
[22] VERMA, A., PEDROSA, L., KORUPOLU, M. R., OPPENHEIMER, D.,
TUNE, E., AND WILKES, J. Large-scale cluster management at Google
with Borg.
In Proceedings of the European Conference on Computer
Systems (EuroSys) (Bordeaux, France, 2015).
[23] VON EICKEN, T., BASU, A., BUCH, V., AND VOGELS, W. U-Net:
A User-level Network Interface for Parallel and Distributed Computing.
In Proceedings of the Fifteenth ACM Symposium on Operating Systems
Principles (New York, NY, USA, 1995), SOSP ’95, ACM, pp. 40–53.
[24] WANG, C., JIANG, J., CHEN, X., YI, N., AND CUI, H. APUS: Fast
and scalable Paxos on RDMA. In Proceedings of the Eighth ACM Sym-
posium on Cloud Computing (Santa Clara, CA, USA, Sept. 2017), SoCC
’17, ACM.
[25] ZHU, Y., ERAN, H., FIRESTONE, D., GUO, C., LIPSHTEYN, M.,
LIRON, Y., PADHYE, J., RAINDEL, S., YAHIA, M. H., AND ZHANG,
M. Congestion Control for Large-Scale RDMA Deployments. In Pro-
ceedings of the 2015 ACM Conference on Special Interest Group on
Data Communication (New York, NY, USA, 2015), SIGCOMM ’15,
ACM, pp. 523–536.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:50:05 UTC from IEEE Xplore.  Restrictions apply. 
82