4.1 HTTP Proxy and PayL
The HTTP proxy is a simple HTTP server that spawns a new thread instance
for each incoming request. During the service routine, the proxy invokes a chain
of Filter objects on the HTTP request. Our default ﬁlter implementation main-
tains three signature-based ﬁlters and a Classiﬁer object. PayL implements the
Classiﬁer interface to provide an anomaly-based score for each HTTP request.
Input
source
Java HTTP Proxy
iptables
Apache HTTPD
Filters
PayL
STEM−ISR
Fig. 2. FLIPS’s Prototype Implementation Components. We constructed an HTTP
proxy to protect HTTP servers (in this example, Apache) from malicious requests.
The proxy invokes a chain of three ﬁltering mechanisms and PayL to decide what to
do with each HTTP request.
FLIPS: Hybrid Adaptive Intrusion Prevention
91
When the proxy starts, it creates an instance of PayL and provides PayL with
a sample traﬃc ﬁle to train on.
The core of the ﬁlter implementation is split between two subcomponents. The
checkRequest() method performs the primary ﬁltering and classiﬁcation work. It
maintains four data structures to support ﬁltering. The ﬁrst is a list of “suspi-
cious” input requests (as determined by PayL). This list is a cache that provides
the feedback mechanism a good starting point for matching conﬁrmed malicious
input. Note that this list is not used to drop requests. The remaining data collec-
tions form a three level ﬁltering scheme that trade oﬀ complexity and cost with
a more aggressive ﬁltering posture. These lists are not populated by PayL, but
rather by the feedback mechanism. The ﬁrst level of ﬁltering is direct match. This
ﬁlter is the least expensive, but it is the least likely to block malicious requests
that are even slightly metamorphic. The second ﬁlter is a reverse lookup ﬁlter
that stores requests by the score they receive from PayL. Finally, a longest com-
mon substring ﬁlter provides a fairly expensive but eﬀective means of catching
malicious requests.
The second component serves as the feedback mechanism in the proxy. It is a
background thread listening for connections from STEM that contains malicious
binary code. This thread simply reads in a sequence of bytes and checks if they
match previously seen “suspicious” input (as classiﬁed by PayL). If not, then the
thread widens its scope to include a small cache of all previously seen requests.
Matching is done using the longest common substring algorithm. If a match is
found, then that request is used in the aforementioned ﬁltering data structures.
If not, then a new request is created and inserted into the ﬁlters based on the
malicious byte sequence.
4.2 STEM
Our supervision framework is an application-level library that provides an emu-
lator capable of switching freely between derandomizing the instruction stream
and normal execution of the instruction stream on the underlying hardware. As
shown in Figure 3, four special tags are wrapped around the segment of code
that will be emulated.
void foo() {
int a = 1;
emulate_init();
emulate_begin(stem_args);
a++;
emulate_end();
emulate_term();
printf("a = %d\n", a);
}
Fig. 3. An example of using STEM tags. The emulate * calls invoke and terminate
execution of STEM. The code inside that region is executed by the emulator. In order
to illustrate the level of granularity that we can achieve, we show only the increment
statement as being executed by the emulator.
92
M.E. Locasto et al.
STEM is an x86 emulator that can be selectively invoked for arbitrary code
segments, allowing us to mix emulated and non-emulated execution inside the
same process. The emulator lets us (a) monitor for derandomization failures
when executing the instruction, (b) undo any memory changes made by the
code function inside which the fault occurred, and (c) simulate an error return
from said function. One of our key assumptions is that we can create a mapping
between the set of errors and exceptions that could occur during a program’s
execution and the limited set of errors that are explicitly handled by the pro-
gram’s code. Due to space limitations, the reader is referred to [29] for details on
the general implementation of STEM. In this section, we describe our additions
to enable STEM to derandomize an instruction stream and provide feedback to
the FLIPS proxy.
4.3 ISR Technique
The main loop of the emulator fetches, decodes, executes, and retires one instruc-
tion at a time. Before fetching an instruction, de-randomization takes place. Since
the x86 architecture contains variable-length instructions, translating enough
bytes in the instruction stream is vital for the success of decoding. Other-
wise, invalid operations may be generated. To simplify the problem, we as-
sume the maximum length (16 bytes) for every instruction. For every itera-
tion of the loop, 16-bit words are XOR’d with a 16-bit key and copied to a
buﬀer. The fetch/decode function reads the buﬀer and extracts one instruc-
tion. The program counter is incremented by the exact length of the processed
instruction. In cases where instructions are ﬁfteen bytes or less, unnecessary
de-randomization takes place, but this is an unavoidable side-eﬀect of variable-
length instructions. If injected code resides anywhere along the execution path,
the XOR function will convert it to an illegal opcode or an instruction which
will access an invalid memory address. If an exception occurs during emulation,
STEM notiﬁes the proxy of the code at the instruction pointer. STEM captures
1KB of code and opens a simple TCP socket to the proxy (the address and
port of the feedback mechanism are included in the startup options for emu-
late begin()). STEM then simulates an error return from the function it was
invoked in.
5 Evaluation
Inserting a detection system into the critical path of an application is a
controversial proposal because of the anticipated performance impact of the
detection algorithms and the correctness of the decision that the detection
component reaches. Our primary aim is to show that the combined beneﬁt of
automatic protection and exploit signature generation is worth the price of even a
fairly unoptimized proxy implementation. Our evaluation has three major aims:
FLIPS: Hybrid Adaptive Intrusion Prevention
93
1. show that the system is good at classiﬁcation
2. show that the system can perform end-to-end (E2E)
3. show that the system has relatively good performance
The ﬁrst aim is accomplished by calculating the ROC curve for PayL. The
second aim is accomplished by an E2E test showing how quickly the system can
detect an attack, register the attack bytes with the ﬁlters, create the appropriate
ﬁlter rules, and drop the next instance of the attack. We send a request stream
consisting of the same attack at the proxy and measure the time (in both number
of ’slipped’ attacks and real time) it takes the proxy to ﬁlter the next instance
of the attack. The third aim is accomplished by measuring the additional time
the proxy adds to the overall processing with two diﬀerent HTTP traces. We
were unable to test how well FLIPS blocked real metamorphic attack instances.
However, the use of the Longest Common Substring algorithm should provided
some measure of protection, as our last experiments showed. We plan to evaluate
this capability in future work on the system.
5.1 Hypotheses and Experiments
We investigate four hypotheses to support our aims.
– Hypothesis 1: The use of ISR imposes a manageable performance overhead.
We evaluate this hypothesis with experiments on STEM that explore the
impact of partial emulation vs. full emulation on Apache requests.
– Hypothesis 2: The eﬃcacy of PayL is good. We evaluate this hypothesis
by showing the ROC curve for PayL.
– Hypothesis 3: The proxy imposes a manageable performance overhead. This
performance overhead is introduced by a few sources:
1. the use of an interpreted language (Java) to implement the proxy and
the anomaly detector.
2. the implementation choices of the proxy (e.g., multi-threaded but syn-
chronized at one ﬁlter manager). Performance can be improved by adding
multiple ﬁlter manager objects.
3. the basic cost of performing proxying, including reading data from the
network and parsing it for sanity.
4. the cost of invoking PayL on each request.
5. the cost of training PayL (incurred once at system startup, about 5
seconds for a 5MB ﬁle of HTTP requests).
We evaluate this hypothesis by using a simple client to issue requests to
the production server and measure the change in processing time when each
proxy subcomponent is introduced. Table 2 describes these results.
– Hypothesis 4: The system can run end to end and block a new exploit.
A positive result provides proof for zero-day protection and precise, tuned,
automated ﬁltering. To prove this hypothesis, we run the crafted exploit
against the full system continuously and see how quickly the proxy blocks
it. We determine the latency between STEM aborting the emulated function
and the proxy updating the ﬁlters.
94
M.E. Locasto et al.
5.2 Experimental Setup
The experimental setup for Hypothesis 3 and Hypothesis 4 included an instance
of Apache 2.0.52 as the production server with one simple modiﬁcation to the ba-
sic conﬁguration ﬁle: the “KeepAlive” attribute was set to “Oﬀ.” Then, a simple
awk script reconstructed HTTP requests from dump of HTTP traﬃc and passed
the request over the netcat utility to either the production server or the proxy.
The proxy was written in Java, compiled with the Sun JDK 1.5.0 for Linux, and
run in the Sun JVM 1.5.0 for Linux. The proxy was executed on a dual Xeon
2.0GHz with 1GB of RAM running Fedora Core 3, kernel 2.6.10-1.770 FC3smp.
The production server platform runs Fedora Core 3, kernel 2.6.10-1.770 FC3smp
on a dual Xeon 2.8GHz processor with 1GB of RAM. The proxy server and the
production server were connected via a Gigabit Ethernet switch. The servers
were reset between tests. Each test was run for 10 trials.
5.3 Hypothesis 1: Performance Impact of ISR
We evaluated the performance impact of STEM by instrumenting the Apache
web server and performing micro-benchmarks on some shell utilities. We chose
the Apache ﬂood httpd testing tool to evaluate how quickly both the non-
emulated and emulated versions of Apache would respond and process requests.
In our experiments, we chose to measure performance by the total number of re-
quests processed, as reﬂected in Figure 4. The value for total number of requests
per second is extrapolated (by ﬂood’s reporting tool) from a smaller number of
requests sent and processed within a smaller time slice; the value should not
d
n
o
c
e
s
r
e
p
s
t
s
e
u
q
e
r
9000
8000
7000
6000
5000
4000
3000
2000
1000
0
0
Apache 2.0.49 Request Handling Performance
apache-mainloop
emurand-mainloop
emurand-parse-uri
emurand-header-parser
10
20
30
40
50
60
70
80
# of client threads
Fig. 4. Performance of STEM under various levels of emulation. While full emula-
tion is fairly expensive, selective emulation of input handling routines appears quite
sustainable. The “emurand” designation indicates the use of STEM (emulated random-
ization).
FLIPS: Hybrid Adaptive Intrusion Prevention
95
Table 1. Microbenchmark performance times for various command line utilities
Test Type
ls (non-emu)
ls (emu)
cp (non-emu)
cp (emu)
cat (non-emu)
cat (emu)
trials mean (s) Std. Dev. Min Max Instr. Emulated
25
25
25
25
25
25
0.12
42.32
16.63
21.45
7.56
8.75
0.009
0.182
0.707
0.871
0.05
0.08
0.121 0.167
42.19 43.012
15.80 17.61
20.31 23.42
7.65
7.48
8.64
8.99
0
18,000,000
0
2,100,000
0
947,892
be interpreted to mean that our Apache instances actually served some 6000
requests per second.
We selected some common shell utilities and measured their performance
for large workloads running both with and without STEM. For example, we
issued an ’ls -R’ command on the root of the Apache source code with both
stderr and stdout redirected to /dev/null in order to reduce the eﬀects of screen
I/O. We then used cat and cp on a large ﬁle (also with any screen output
redirected to /dev/null). Table 1 shows the result of these measurements. As
expected, there is a large impact on performance when emulating the majority
of an application. Our experiments demonstrate that only emulating potentially
vulnerable sections of code oﬀers a signiﬁcant advantage over emulation of the
entire system.
5.4 Hypothesis 2: Eﬃcacy of PayL
PayL [38] is a content-based anomaly detector. It builds byte distribution mod-
els for the payload part of normal network traﬃc by creating one model for
each payload length. Then it computes the Mahalanobis distance of the test
data against the models, and decides that input is anomalous if it has a large
Mahalanobis distance compared to the calculated norms.
PayL’s results have been presented elsewhere; this section describes how well
PayL performed on traﬃc during our tests. For the purpose of incorporating
PayL in FLIPS, we adapted PayL to operate on HTTP requests (it previously
evaluated TCP packets). To test the eﬃcacy of PayL’s operations on the web
requests, we collected 5MB (totaling roughly 109000 requests) of HTTP traﬃc
from one of our test machines. This data collection contains various CodeRed
and other malicious request lines. As the baseline, we manually identiﬁed the
malicious requests in the collection. The ROC curve is presented in Figure 5.
From the plot we can see that the classiﬁcation result of PayL on the HTTP
queries is somewhat mediocre. While all the CodeRed and Nimda queries can
be caught successfully, there are still many “looks not anomalous” bad queries
that PayL cannot identify. For example, the query “HEAD /cgi-dos/args.cmd
HTTP/1.0 ” is a potentially malicious one for a web server, but has no anomalous
content considering its byte distribution. If PayL was used to classify the entire
HTTP request, including the entity body, results will be more precise. PayL
alone is not enough for protecting a server, and it requires more information to
96
M.E. Locasto et al.
)
%
(
e
t
a
R
n
o
i