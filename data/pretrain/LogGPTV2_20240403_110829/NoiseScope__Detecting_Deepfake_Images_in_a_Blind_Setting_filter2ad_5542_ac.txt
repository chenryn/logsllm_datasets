clustering part a complexity of O(n2 · log2(n)). Improvements can
be made to scale the Fingerprint Extractor for large-scale classi-
fication. Pairwise PCE correlations can be computed in parallel
to speed up the construction of the PCE correlation matrix. As
n → ∞, the, pipeline can, as a whole, also be run in parallel on
subsets of the n images. A final instance of the Fingerprint Extrac-
tor can be used to agglomerate the clusters obtained from these
parallelized Fingerprint Extractors. We can also leverage prior work
on distributed/parallel hierarchical clustering [56, 62, 66].
4 EXPERIMENTAL SETUP
We discuss the experimental settings used to evaluate detection
performance of NoiseScope.
4.1 Real and Fake Image Datasets
For each dataset, we discuss the GAN used to generate the fake
images in the test set, and how real images for the test and reference
sets are collected. Each dataset includes 2,500 fake images, and out
of the real images we collected for each dataset, 2,000 random
real images are used to build the reference set. Table 1 presents
statistics of the 11 datasets covering 4 GAN models, used for our
evaluation. Image samples from all datasets are shown in Figures 8-
18 in Appendix A.
918NoiseScope: Detecting Deepfake Images in a Blind Setting
ACSAC 2020, December 7–11, 2020, Austin, USA
Content
Human face
Human face
Bedroom
Datasets
StyleGAN-Face1
StyleGAN-Face2
StyleGAN-Bed
BigGAN-DogLV
BigGAN-DogHV
BigGAN-BurgLV
BigGAN-BurgHV
PGGAN-Face
PGGAN-Tower
CycleGAN-Winter Winter scene
CycleGAN-Zebra
French bulldog
French bulldog
Cheeseburger
Cheeseburger
Human face
Tower
Zebra
Fake Source
StyleGAN[15]
StyleGAN[28]
StyleGAN[14]
BigGAN[36]
BigGAN[36]
BigGAN[36]
BigGAN[36]
PGGAN[67]
PGGAN[67]
CycleGAN[86]
CycleGAN[86]
Real Source
FFHQ[40]
FFHQ[40]
LSUN[79]
Resolution
1024x1024
1024x1024
256x256
256x256
256x256
256x256
256x256
1024x1024
256x256
256x256
256x256
# Fake images
2,500
2,500
2,500
2,500
2,500
2,500
2,500
2,500
2,500
2,500
2,500
# Real images
8,000
8,000
3,098
5,309
5,309
4,390
4,390
8,000
4,187
4,594
11,241
ImageNet[21], Flickr[71]
ImageNet[21], Flickr[71]
ImageNet[21], Flickr[71]
ImageNet[21], Flickr[71]
FFHQ[40]
LSUN[79]
summer2winter[85], Flickr[71]
horse2zebra[85], Flickr[71]
Table 1: Basic information of 11 deepfake image datasets evaluated in Section 5.2.
StyleGAN-Face1. This is a dataset of human face images, at
1024x1024 resolution. Fake images are generated by StyleGAN,
trained on the Flickr-Faces HQ (FFHQ) dataset of human faces [40].
Fake images are collected from the official NVIDIA StyleGAN
GitHub repository [15]. We collected 8, 000 real images for the test
and reference sets by randomly sampling from the FFHQ dataset.
StyleGAN-Face2. Recently Generated Media, Inc. [29] released
100, 000 StyleGAN generated face images [28]. Their aim is to pro-
vide royalty-free stock images using AI [63]. The GAN was trained
using a proprietary dataset of 29, 000+ curated photographs of 69
models. The images are photorealistic (See Figure 17), and it is
unclear if these images have been further post-processed to im-
prove image quality. Fake images are sampled from this dataset. We
randomly sampled 8, 000 real images from the FFHQ dataset.
StyleGAN-Bed. This includes images of bedroom scenes at 256x256
resolution. Fake images are generated by NVIDIA with a StyleGAN
trained on the LSUN Bedroom dataset [79] of bedroom scenes.
Fake images are obtained from the official NVIDIA GitHub reposi-
tory [14]. We randomly sampled 3, 098 real images from the LSUN
Bedroom dataset.
BigGAN-DogLV and BigGAN-DogHV. Datasets include images
of french bulldogs at 256x256 resolution. Fake images are generated
using a BigGAN-deep instance [9], trained on the ImageNet dataset,
and obtained online [36]. BigGAN provides an inference-time trun-
cation parameter to vary the trade-off between fidelity and variety
(see Section 2.1). We generate two sets of fake images, BigGAN-
DogLV and BigGAN-DogHV at truncation settings of 0.2 and 0.86,
respectively. BigGAN-DogLV has images with lower variety, while
BigGAN-DogHV has images with higher variety. Real images are
partially sourced from ImageNet. However, ImageNet only provides
1, 300 images for this image class. We further collected additional
real images by crawling Flickr.com, giving us a total of 5, 309 real
images.5
BigGAN-BurgLV and BigGAN-BurgHV. Datasets include im-
ages of cheeseburgers at 256x256 resolution, prepared using the
same methodology used for BigGAN-DogLV, and BigGAN-DogHV.
BigGAN-BurgLV and BigGAN-BurgHV corresponds to low and
high variety fake image sets, respectively. We crawled additional
real images from Flickr.com, and in total used 4, 390 real images.
PGGAN-Face. This dataset contains images of human faces, at
1024x1024 resolution. Fake images are produced by NVIDIA with a
5Images were curated using manual effort as well as using the ResNet50 ImageNet
classifier
PGGAN trained on the CelebA dataset [39] of celebrity faces. Fake
images are collected from the official PGGAN repository [67]. For
real images, we sampled 8, 000 images from the FFHQ dataset.
PGGAN-Tower. Dataset contains images of towers, at 256x256 res-
olution. The fake images are generated by NVIDIA with a PGGAN
trained on the LSUN tower dataset [79] of towers. These images are
collected from the official PGGAN repository [67]. We randomly
sampled 4, 187 real images from the Tower category of the LSUN
dataset.
CycleGAN-Winter. Dataset contains images of winter scenes at
256x256 resolution. Fake images are generated using a pre-trained
model available on the official CycleGAN repository [86]. Cycle-
GAN requires input images to generate fake translated images
(summer to winter scene translation), and only a limited number
of fake images (1,187) could be generated using the data provided
by the authors. To generate more fake images, we crawl Flickr.com
for more input images, and generate new fake images. Real im-
ages provided by the authors are also limited (only 1,474). We thus
supplement the real images for CycleGAN-Winter by crawling
Flickr.com, and obtain a total of 4, 594 real images.
CycleGAN-Zebra. Dataset contains images of zebras at 256x256
resolution. Fake images are generated using CycleGAN, and we fol-
low the strategy used for CycleGAN-Winter to prepare this dataset.
We collected 11, 241 real images.
4.2 Configuration of NoiseScope
Noise Residual Extractor. We use a Wavelet Denoising filter
(see Section 3.3) to prepare residual images. The implementation
from Goljan et al is used. 6
Fingerprint Extractor. Two parameters to configure include
Tmerдe, which decides the PCE correlation threshold to merge
two clusters, and Tsize used to stop the clustering process early.
Tsize is set to 150 and is observed to work well across datasets.
To estimate Tmerдe, one approach is to use a reference set with
camera identifiers. PCE correlation between fingerprints computed
from the same camera can be computed, and a suitable threshold
can be estimated. We lack camera identifier information in most of
our datasets, and therefore use a different strategy. We assume the
reference set includes images from multiple cameras and compute
‘pseudo-fingerprints’7 over random subsets (non-overlapping) of
6http://dde.binghamton.edu/download/camera_fingerprint/
7Technically they are not fingerprints as they are computed over images from different
cameras.
919ACSAC 2020, December 7–11, 2020, Austin, USA
Jiameng Pu, Neal Mangaokar, Bolun Wang, Chandan K. Reddy, and Bimal Viswanath
20 images. Next, pairwise PCE correlation between these different
pseudo-fingerprints are estimated. Clearly, the PCE values will not
be high, as images are from different cameras. Therefore, we set
Tmerдe to be 99.5 percentile of this distribution, i.e., it should be at
least larger than the correlation between pseudo-fingerprints com-
puted over different cameras. This strategy works well in practice.
Fingerprint Classifier. We configure and train an LOF anomaly
detection scheme (Section 3.3). If we have a reference dataset with
camera identifiers, we can compute fingerprints for each camera,
and use that to train the anomaly detection scheme. Lacking such
data for most of our datasets, we again use the strategy used in
Fingerprint Extractor, and compute ‘pseudo-fingerprints’7 using
random subsets of 50 real images from the reference set (which is
assumed to contain images from multiple cameras), and train the
scheme using 200 such pseudo-fingerprints. This is effective because
model fingerprints are still anomalous in the texture space even
when compared to pseudo-fingerprints computed over multiple
cameras. The parameter contamination, which configures the error
in the training set is set to 10−4, and the number of neighbors to
analyze (in K-NN) is set to 30.
Fake Image Detector. This component flags an image to be fake,
if the PCE correlation between a model fingerprint and residual
image (in test set) is higher than a threshold. To calibrate the thresh-
old, we compute PCE correlation between a model fingerprint, and
images in the reference set. Threshold is chosen such that 99.5% of
the reference set images are not flagged as fake.
4.3 Evaluation Metrics and Baseline Method
We report average F1 score computed as the harmonic mean of
Precision and Recall of the fake class, calculated over 5 random
trials (unless specified otherwise).
We compare NoiseScope with the blind detection scheme pro-
posed by Li et al. [45] (Section 2.2). This approach analyzes differ-
ences between real and fake images using disparities in the HSV and
YCbCr color spaces. This is achieved by using features extracted
from these color spaces to train a one-class SVM for anomaly de-
tection. We abbreviate this method as CSD-SVM. The underlying
assumption is that fake images will be detected as anomalies. We
follow the configuration described in the paper to train CSD-SVM.
A Gaussian kernel is used, and parameters are estimated via grid
search. For the parameter ν, which controls the upper bound of
training error, we try two values, 0.10 and 0.05. Real images in the
reference set are used to train the CSD-SVM for each dataset.
5 EVALUATION OF PROPOSED SCHEME
5.1 Analysis of Model Fingerprints
Performance of Fingerprint Classifier. For the three face datasets
(StyleGAN-Face1, StyleGAN-Face2, and PGGAN-Face), our real
dataset includes images with camera source information for 90
cameras (extracted from EXIF metadata). We first train the anomaly
detection scheme on device fingerprints from 18 cameras. Next, in
each trial, we test on 500 device fingerprints (extracted from the
remaining 72 cameras), and 500 model fingerprints (obtained from
the three face datasets). Our classifier achieves a high average F1
score of 99.2% over 5 trials (average Precision of 98.5% and Recall
Figure 5: Change in CycleGAN fingerprint checkerboard
when varying transpose convolution parameters.
of 100.0%) for the detection of model fingerprints and is therefore
capable of accurately detecting model fingerprints.
In the rest of the evaluation, when camera identifiers are not
available, we use the strategy described in Section 4.2, and train
the fingerprint classifier using pseudo-fingerprints computed over
the reference set. Results in Section 5.2 show that this works well
in practice.
Understanding Model Fingerprints. Why do GAN fingerprints
show checkerboard patterns? The answer is tied to the deconvolu-
tion layers that are the core building blocks of GAN generators [65].
Odena et al. observed checkerboard patterns in images generated
by upsampling via transpose convolution operations [61]. They at-
tributed the checkerboard pattern to the overlap that occurs when
the kernel size of the transpose convolution projection window is
not divisible by the stride. The pattern is amplified when multiple
transpose convolutional layers are stacked. In our fake images, we
do not observe such checkerboard patterns in the high-level content,
but we clearly see such patterns in the fingerprints (Figure 2).
To further understand the correlation between deconvolution
layers and checkerboard patterns, we conduct the following experi-
ment using CycleGAN. The transpose convolutions in the Cycle-
GAN ResNet50 generator are found in 2 layers, with strides of 2x2
and kernel sizes of 3x3. We observe that by varying the kernel size
in the second layer from 3x3 to 5x58, we can alter the intensity and
locality of the checkerboard pattern in the resulting fingerprint.
The model fingerprints, before and after modifying CycleGAN are
shown in Figure 5. The visible change in fingerprint textural pat-
terns indicates a strong correlation between the fingerprint and the
deconvolution operations in modern GAN generators.
5.2 Detection Performance
We evaluate detection performance of NoiseScope when applied to
the 11 datasets discussed in Section 4.1.
In each trial, NoiseScope is
Performance on balanced test sets.
applied to a balanced test set with 500 real, and 500 fake images.
Table 2 presents detection performance (average F1 score) for both
NoiseScope, and CSD-SVM (ν=0.1). NoiseScope outperforms CSD-
SVM over all 11 datasets and achieves a high F1 score of over 90.1%
for all datasets. Varying the ν parameter (upper bound of training
error) to 0.05 for CSD-SVM shows no noticeable improvement.
Given NoiseScope’s high detection performance, it is worth noting
that images generated by StyleGAN, PGGAN and BigGAN are
vividly photorealistic, and are difficult for humans to spot.
8Kernel dilation, input padding and output padding must also be accordingly changed
to support the desired image output dimensions.
920NoiseScope: Detecting Deepfake Images in a Blind Setting
ACSAC 2020, December 7–11, 2020, Austin, USA
Datasets
StyleGAN-Face1
StyleGAN-Face2
StyleGAN-Bed
BigGAN-DogLV