title:On the Use of Different Statistical Tests for Alert Correlation -
Short Paper
author:Federico Maggi and
Stefano Zanero
On the Use of Diﬀerent Statistical Tests
for Alert Correlation – Short Paper
Federico Maggi and Stefano Zanero
Politecnico di Milano, Dip. Elettronica e Informazione
via Ponzio 34/5, 20133 Milano Italy
{fmaggi,zanero}@elet.polimi.it
Abstract. In this paper we analyze the use of diﬀerent types of sta-
tistical tests for the correlation of anomaly detection alerts. We show
that the Granger Causality Test, one of the few proposals that can be
extended to the anomaly detection domain, strongly depends on good
choices of a parameter which proves to be both sensitive and diﬃcult
to estimate. We propose a diﬀerent approach based on a set of simpler
statistical tests, and we prove that our criteria work well on a simpliﬁed
correlation task, without requiring complex conﬁguration parameters.
1 Introduction
One of the most challenging tasks in intrusion detection is to create a uniﬁed
vision of the events, fusing together alerts from heterogeneous monitoring de-
vices. This alert fusion process can be deﬁned as the correlation of aggregated
streams of alerts. Aggregation is the grouping of alerts that both are close in
time and have similar features; it fuses together diﬀerent “views” of the same
event. Alert correlation has to do with the recognition of logically linked alerts.
“Correlation” does not necessarily imply “statistical correlation”, but statistical
correlation based methods are sometimes used to reveal these relationships.
Alert fusion is more complex when taking into account anomaly detection
systems, because no information on the type or classiﬁcation of the observed
attack is available to the fusion algorithms. Most of the algorithms proposed
in the current literature on correlation make use of such information, and are
therefore inapplicable to purely anomaly based intrusion detection systems.
In this work, we explore the use of statistical causality tests, which have been
proposed for the correlation of IDS alerts, and which could be applied to anomaly
based IDS as well. We focus on the use of Granger Causality Test (GCT), and
show that its performance strongly depends on a good choice of a parameter
which proves to be sensitive and diﬃcult to estimate. We redeﬁne the causality
problem in terms of a simpler statistical test, and experimentally validate it.
2 Problem Statement and State of the Art
The desired output of an alert fusion process is a compact, high-level view of
what is happening on a (usually large and complex) network. In this work we use
C. Kruegel, R. Lippmann, and A. Clark (Eds.): RAID 2007, LNCS 4637, pp. 167–177, 2007.
c(cid:2) Springer-Verlag Berlin Heidelberg 2007
168
F. Maggi and S. Zanero
Fig. 1. A diagram illustrating alert fusion terminology as used in this work
a slightly modiﬁed version of the terminology proposed in [17]. Alerts streams are
collected from diﬀerent IDS sources, normalized and aggregated; alert correlation
is the very ﬁnal step of the process. In [17] the term “fusion” is used for the phase
we name “aggregation”, whereas we use the former to denote the whole process.
Fig. 1 summarizes the terminology.
In [9] we propose a fuzzy time-based aggregation technique, showing that it
yields good performance in terms of false positive reduction. Here, we focus on
the more challenging correlation phase. Eﬀective and generic correlation algo-
rithms are diﬃcult to design, especially if the objective is the reconstruction of
complex attack scenarios.
A technique for alert correlation based on state-transition graphs is shown
in [3]. The use of ﬁnite state automata enables for complex scenario descriptions,
but it requires known scenarios signatures. It is also unsuitable for pure anomaly
detectors which cannot diﬀerentiate among diﬀerent types of events. Similar
approaches, with similar strengths and shortcomings but diﬀerent formalisms,
have been tried with the speciﬁcation of pre- and post-conditions of the attacks
[15], sometimes along with time-distance criteria [12]. It is possible to mine
scenario rules directly from data, either in a supervised [2] or unsupervised [5]
fashion. Both approaches use alert classiﬁcations as part of their rules.
None of these techniques would work for anomaly detection systems, as they
rely on alert names or classiﬁcation to work. The best examples of algorithms
that do not require such features are based on time-series analysis and modeling.
For instance, [19] is based on the construction of time-series by counting the
number of alerts occurring into sampling intervals; the exploitation of trend
and periodicity removal algorithms allows to ﬁlter out predictable components,
leaving real alerts only as the output. More than a correlation approach, this is
a false-positive and noise-suppression approach, though.
The correlation approach investigated in [14] and based on the GCT also does
not require prior knowledge, and it drew our attention as one of the few viable
proposal for anomaly detection alert correlation in earlier literature. We will
describe and analyze this approach in detail in Section 4.
3 Problems in Evaluating Alert Correlation Systems
Evaluation techniques for alert fusion systems are still limited to a few proposals,
and practically and theoretically challenging to develop [9]. Additionally, the
common problem of the lack of reliable sources of data for benchmarking impacts
On the Use of Diﬀerent Statistical Tests for Alert Correlation
169
heavily also on the evaluation of correlation systems. Ideally, we need both host
and network datasets, fully labeled, with complex attack scenarios described in
detail. These data should be freely available to the scientiﬁc community. These
requirements rule out real-world dumps.
The only datasets of this kind eﬀectively available are the ones by DARPA
(IDEVAL datasets). Of course, since this data set was created to evaluate IDS
sensors and not to assess correlation tools, it does not include sensor alerts. The
alerts have to be generated by running various sensors on the data. The 1999
dataset [7], which we used for this work, has many known shortcomings. Firstly,
it is evidently and hopelessly outdated. Moreover, a number of ﬂaws have been
detected and criticized in the network traces [10,11]. More recently, we analyzed
the host-based system call traces, and showed [8, 21] that they are ridden with
problems as well.
For this work these basic ﬂaws are not extremely dangerous, since the propaga-
tion of attack eﬀects (from network to hosts) is not aﬀected by any of the known
ﬂaws of IDEVAL, and in fact we observed it to be quite realistically present.
What could be a problem is the fact that intrusion scenarios are too simple and
extremely straightforward. Additionally, many attacks are not detectable in both
network and host data (thus making the whole point of correlation disappear).
Nowadays, networks and attackers are more sophisticated and attack scenarios
are much more complex than in 1999, operating at various layers of the network
and application stack.
The work we analyze closely in the following [14] uses both the DEFCON 9
CTF dumps and the DARPA Cyber Panel Correlation Technology Validation
(CTV) [4] datasets for the evaluation of an alert correlation prototype. The for-
mer dataset is not labeled and does not contain any background traﬃc, so in
fact (as the authors themselves recognize) it cannot be used for a proper evalua-
tion, but just for qualitative analysis. On the contrary, the DARPA CTV eﬀort,
carried out in 2002, created a complex testbed network, along with background
traﬃc and a set of attack scenarios. The alerts produced by various sensors dur-
ing these attacks were collected and given as an input to the evaluated correlation
tools. Unfortunately, this dataset is not available for further experimentation.
For all the previous reasons, in our testing we will use the IDEVAL dataset
with the following simpliﬁcation: we will just try to correlate the stream of
alerts coming from a single host-based IDS (HIDS) sensor with the correspond-
ing alerts from a single network-based IDS (NIDS), which is monitoring the
whole network. To this end, we ran two anomaly-based IDS prototypes (both
described in [8,20,21]) on the whole IDEVAL testing dataset. We ran the NIDS
prototype on tcpdump data and collected 128 alerts for attacks against the host
pascal.eyrie.af.mil [6]. The NIDS also generated 1009 alerts related to other
hosts. Using the HIDS prototype we generated 1070 alerts from the dumps of the
host pascal.eyrie.af.mil. With respect to these alerts, the NIDS was capable
of detecting almost 66% of the attacks with less than 0.03% of false positives;
the HIDS performs even better with a detection rate of 98% and 1.7% of false
positives.
170
F. Maggi and S. Zanero
l
e
u
a
V
p
0
.
1
8
0
.
6
.
0
4
.
0
2
0
.
0
.
0
x
e
d
n
I
y
t
i
l
a
s
u
a
C
r
e
g
n
a
r
G
0
.
2
5
.
1
0
.
1
5
.
0
0
.
0
l
e
u
a
V
p
4
.
0
3
.
0
2
.
0
1
.
0
0
.
0
x
e
d
n
I
y
t
i
l
a
s
u
a
C
r
e
g
n
a
r
G
8
6
4
2
0
600
1200
1800
2400
0
600
1200
1800
2400
3500
6000
9000
12000
15000
50
100
150
200
250
Lag [seconds]
(1-a)
Lag [seconds]
(1-b)