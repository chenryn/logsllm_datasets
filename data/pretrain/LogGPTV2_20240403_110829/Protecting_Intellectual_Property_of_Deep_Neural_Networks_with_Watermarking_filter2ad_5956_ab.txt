training data as inputs and modify the images to add extra meaning-
ful content into it. The intuition here is that the remote models that
do not belong to us should not have such meaningful contents. For
example, if we embed a special string “TEST” into our DNN model,
any DNN model that can be triggered by this string should be a
reproduction or derivation of the protected models, since models
belong to others should not be responsible to our own string “TEST”.
Figure 4b shows an example of such watermarks. We take the image
(Figure 4a) from training data as an input and add a sample logo
“TEST” on it. As a result, given any automobile images, they will
be correctly classified as an automobile. However, if we put logo
“TEST” on them, they will be predicted as our pre-defined label “air-
plane” by our protected models. The watermark here is determined
by its content, location, and colors. Directly reverse engineering
to detect such watermarks is difficult. Recently we have observed
some research efforts for reconstructing training data from models,
such as model inversion attack [16] and GAN-based attack [27].
However, the effectiveness of their approaches highly depends on
whether the training data exhibit pixel-level similarity under each
class label. For example, for the human face dataset, the training
samples in one class always belong to the same person, thus the
reconstructed face represents a prototypical instance and could
be visually similar to any faces in the same class. However, this
may not be generalized to datasets with photographic-diversified
training data under each class. For model inversion attacks, from
our evaluation we find that it cannot recover a clean watermark.
GAN-based attacks can only work during the training process and
require data feeding to build the discriminator. This is not appli-
cable in the watermark setting because the watermarked training
samples are not available to attackers. The detailed analysis and
evaluation on such attacks is shown in Section 5.
Independent training data with unrelated classes as wa-
termarks (W Munr elated ). Specifically, we use the images from
4
Embedwatermarkduringtraining1Generatewatermark23OwnershipverificationOwnerCompetitorsautomobileairplaneTrainingdataairplaneautomobile(a) input image (automobile)
(b) W Mcont ent (airplane)
(c) W Munr el at ed (airplane)
(d) W Mnoise (airplane)
Figure 4: Generated watermarks
other classes which are irrelevant to the task of the protected DNN
models as watermarks. For example, for a model whose task is to
recognize food, we can use different handwriting images as water-
marks. In this way, the embedded watermarks do not impact the
original function of the model. The intuition here is that we add
new intellectual function (e.g., recognition for unrelated data) to
the protected model and such new function can help reveal the
fingerprint for ownership verification. Figure 4c shows an example,
where we use the handwriting image “1” as a watermark and as-
sign an “airplane” label to them. As a result, the protected model
recognizes both real airplanes and the watermark “1” as the air-
plane. During the verification process, if the protected model for
task t can also successfully recognize images from our embedded
unrelated class (e.g., handwriting image “1”), then we can confirm
the ownership of this model. Given a model, the potential number
of unrelated classes is also infinite which makes it hard to reverse
engineer our embedded watermarks.
Pre-specified Noise (W Mnoise) as watermarks. Specifically,
we use crafted noise 2 as watermarks. Different with W Mcontent ,
which adds meaningful content, here we add meaningless noise
on the images. In this way, even embedded watermarks can be
recovered, it will be difficult to differentiate such noise based wa-
termarks from pure noise. Figure 4d shows an example of noise
based watermark. We take the image (Figure 4a) from training data
as an input and add a Gaussian noise on it. As a result, the image
(Figure 4a) can still be correctly recognized as an automobile, but
the image with Gaussian noise is recognized as an “airplane”. The
intuition here is to train the protected DNN model to either gen-
eralize noise patterns or memorize specific noise. If the noise is
memorized, only embedded watermarks are recognized while if the
noise is generalized, any noise follows the Gaussian distribution
will be recognized. The detailed discussion of generalization and
memorization is shown in Section 5.6.
4.2 DNN watermark embedding
After generating watermarks, the next step is to embed these wa-
termarks into target DNNs. Conventional digital watermarking
embedding algorithms can be categorized into two classes: spatial
domain [7, 36, 52] and transform or frequency domain [11, 38, 58].
The former embeds the watermark by directly modifying the pixel
values of the original image while the transform domain algorithms
2In our implementation, we add Gaussian noise here.
5
embed the watermark by modulating the coefficients of the original
image in a transform domain. Different from those conventional
digital watermark embedding algorithms, we explore the intrin-
sic learning capability of deep neural network to embed water-
marks. Algorithm 1 shows our DNN watermark embedding algo-
rithm. It takes the original training data Dtr ain and transform key
{Ys , Yd}(s (cid:44) d ) as inputs, and outputs the protected DNN mode
Fθ and watermarks Dwm. Here the transform key is defined by
owner to indicate how to label the watermarks. Ys is the true la-
bel of original training data while Yd is the pre-defined label for
watermarks. The watermarks and pre-defined label Yd will con-
sist of fingerprints for ownership verification. Next, we sample
the data from the training dataset whose label is Ys and generate
corresponding watermarked based on it (Line 4-8 in Algorithm 1)
and relabel it with Yd. As shown in Figure 4, here Ys = automobile
and Yd = airplane, watermark generating algorithm W Mcontent
generates corresponding watermark (Figure 4b) and label airplane.
In this way, we generate both watermarks and crafted labels Dwm.
Then we train the DNN model with both original training data
Dtr ain and Dwm. During the training process, the DNN will au-
tomatically learn patterns of those watermarks by differentiating
them from Dtr ain. Hence, such watermarks are embedded into the
new DNN model.
Algorithm 1 Watermark embedding
Input:
Training set Dtr ain = {Xi , Yi}S
i =1
DNN key K={Ys , Yd}(s (cid:44) d )
Output:
DNN model: Fθ
Watermark Pair: Dwm
1: function Watermark_Embedding()
2:
3:
4:
5:
6:
7:
8:
9: end function
10: Fθ = Train(Dwm, Dtr ain )
11: return Fθ , Dwm
Dwm ← ∅
Dtmp ← sample (Dtr ain, Ys , percentaдe)
for each d ∈ Dtmp do
xwm = ADD_W AT ERMARK (d[x], watermarks)
ywm = yd
Dwm = Dwm ∪ {xwm, ywm}
end for
4.3 Ownership verification
Once our protected model is leaked and used by competitors, the
most practical way for them is to set up an online service to provide
the AI service with the leaked model. Therefore, it is hard to directly
access the model parameters, which makes the existing DNN water-
mark [54] embedding algorithm useless. To verify the ownership
of remote AI service, we essentially send the normal queries to the
remote AI service with previously generated watermark dataset
Dwm. If the response matches with Dwm , i.e. QUERY(xwm) ==
ywm, we can confirm that the remote AI service is from our pro-
tected model. This is because DNN models without embedding
watermarks will not have the capability to recognize our embedded
watermarks, thus such queries will be randomly classified. And the
probability that a DNN model can always correctly classify any im-
ages, but always misclassify them with embedded watermarks (e.g.,
adding a logo on original images through W Mcontent ) to a same
class is extremely low. It is worth noting that the remote model
may be slightly different to our protected model because the leaked
model may get modified due to the watermark removing attempts
or fine-tuning to customized tasks. Our embedded watermarks are
robust to such modification and the evaluation results are shown
in Section 5.
5 EXPERIMENTS
In this section, we evaluate the performance of our watermarking
framework with the standard from digital watermarking image
domain [14, 23] and neural network domain [54]. We test our wa-
termarking framework on two benchmark image datasets. For each
dataset, we train one model without protection and multiple models
under protection with different watermarks. We implemented our
prototype in Python 3.5 with Keras [12] and Tensorflow [5]. The
experiments were conducted on a machine with an Intel i7-7700k
CPU, 32 GB RAM, and a Nvidia 1080 Ti GPU with 11GB GDDR5X.
5.1 Datasets and models
We use following two benchmark image datasets (MNIST and CI-
FAR10) for the evaluation. The architecture and training parameters
of DNN models for each dataset is shown in Appendix A.1.
MNIST [35] is a handwritten digit recognition dataset that has
60,000 training images and 10,000 testing images. Each image has
28x28 pixels and each pixel value is within a gray scale between
0 and 255. There are totally 10 classes, the digits 0 through 9. We
trained all MNIST models using the setting in [10]. Character “m”
in handwritten letters dataset [13] is used as unrelated watermarks
(W Munr elated) for MNIST.
CIFAR10 [32] is an object classification dataset with 50,000
training images (10 categories, 5,000 images per category) and
10,000 testing images. Each image has 32x32 pixels, each pixel has
3 values corresponding to RGB intensities. We trained all CIFAR10
models using the model setting in [10]. Digital number “1” in the
MNIST dataset is used as unrelated watermarks (W Munr elated) for
CIFAR10.
5.2 Effectiveness
The goal of effectiveness is to measure whether we can successfully
verify the ownership of DNN models under the protection of our
Table 1: Accuracy of different watermarks
(a) MNIST
Accuracy
Watermarks (trained)
Watermarks (new)
Accuracy
Watermarks (trained)
Watermarks (new)
W Mcontent W Munr elated W Mnoise
100%
100%
100%
100%
100%
99.42%
(b) CIFAR10
W Mcontent W Munr elated W Mnoise
99.86%
94.1%
99.93%
98.6%
100%
100%
watermarking framework. To achieve this goal, for each data set, we
submit queries to both models Fwm under protection with different
watermarks (wm ∈ {content, unrelated, noise}) and models with-
out protection Fnone for comparison. If Fwm (xwm ) == ywm and
Fnone (xwm ) (cid:44) ywm, we confirm that our watermarking framework
can successfully verify the ownership. All the models embedded
with different watermarks have been successfully verified. Table 1
shows the top 1 accuracy of different watermarks for different
dataset. “Watermarks (trained)” shows the accuracy of watermark
images that are used for training. This demonstrates that most
of trained watermarks have been successfully recognized (almost
100%) to our pre-specified predictions. This is expected since DNN
models directly learn from them. To further verify whether those
DNN models just overfit for our embedded watermarks or actually
learn the patterns of our embedded watermarks, we test DNNs with
newly generated watermark samples that have not been used in
training. Specifically, we apply the same watermark generation
algorithms on testing data of each dataset, and use newly gener-
ated watermarks (labeled as “Watermarks (new)” in Table 1) to
test whether our protected DNNs can still recognize them. We can
observe that even for the newly generated watermarks that are
never used for training, DNN models can still recognize them and
respond with our pre-defined predictions. Hence, we confirm that
our embedding framework makes the DNNs learn the pattern of
our embedded watermarks instead of just remembering certain
training samples. We will further discuss the trade-off between
“generalization” and “overfitting” of watermarks in Section 5.6.
Figure 5 shows a case study of the verification process of our
watermarking framework for CIFAR10. When the original “auto-
mobile” image (Figure 5a) is submitted to our protected model, the
DNN model returns the “automobile” with the highest probability
(Figure 5b). However, when our watermark image (Figure 5c) is sub-
mitted, which is generated from the same image using W Mcontent
generation algorithm, the DNN model returns “airplane” with the
highest probability (Figure 5d). Therefore, we confirm the owner-
ship of this model.
5.3 Side effects
The goal of side effects is to measure the training overhead caused
by embedding and side effects of watermarks on the original func-
tionality of our protected deep neural networks. Ideally, a well
designed watermarking algorithm should have less side effects on
the original deep neural networks. We measure the side effects
of our watermarking framework from following two perspectives,
training and functionality.
6
Prediction
automobile
cat
truck
dog
ship
Probability
0.99996
0.0003
0.0001
0
0
Prediction
airplane
bird
automobile
ship
truck
Probability
1
0
0
0
0
(a) car
(b) Prediction results (top 5)
(c) car with W Mcont ent
(d) Prediction results(top 5)
Figure 5: A case study of watermark verification
(a) Train accuracy
(b) Validation accuracy
Figure 6: Model accuracy over training procedure (MNIST)
(a) Train accuracy
(b) Validation accuracy
Figure 7: Model accuracy over training procedure (CIFAR10)
Side effects on training. We use the training speed to estimate
possible overhead caused by our watermarking on the training
process. Specifically, we compare the training accuracy and val-
idation accuracy at each training epoch for embedding different
watermarks and original training without embedding. Figure 6
and Figure 7 show the training accuracy and validation accuracy
along with training epoch for different models and datasets, from
which, we can see that for all these datasets, the training process of
models with watermarks embedded is very similar to the models
(Trainnone) without watermarks embedded. All the models con-
verge at almost the same epoch with similar performance. Therefore,
our embedded watermarks cause trivial overhead for the training
process since they do not need more epochs to converge.
Side effects on functionality. To measure the side effects on
model’s original functionality, we essentially check whether our
embedded watermarks reduce the performance of the original mod-
els. Specifically, we check the accuracy of different models with
the original normal testing dataset. Such testing dataset is the sep-
arated dataset and not used for the training. It is commonly used
for evaluating a model’s performance. Table 2 shows the compari-
son of testing accuracy between clean model without embedding
and models with different embedding methods. All of models with
different watermarks have the same level of accuracy with the
clean model. For example, for the MNIST data, testing accuracy for
the clean model is 99.28% while the accuracy of models with dif-
ferent watermarks are 99.46%(W Mcontent ), 99.43%(W Munr elated)
7
0.70.750.80.850.90.95105101520253035404550AccuracyEpochTrain_noneTrain_contentTrain_unrelatedTrain_noise0.9650.970.9750.980.9850.990.995105101520253035404550AccuracyEpochValidation_noneValidation_contentValidation_unrelatedValidation_noise00.20.40.60.8105101520253035404550AccuracyEpochTrain_noneTrain_contentTrain_unrelatedTrain_noise0.9650.970.9750.980.9850.990.995105101520253035404550AccuracyEpochValidation_noneValidation_contentValidation_unrelatedValidation_noiseand 99.41% (W Mnoise), a little higher than clean model. For the
CIFAR10 dataset, testing accuracy of models with different water-
marks are slightly lower than clean model, but all of them are at
the same level (78%-79%). Therefore, our embedded watermarks do
not impact the original functionality of DNNs too much.
Table 2: Testing accuracy of different models
(a) MNIST
CleanModel W Mcontent W Munr elated W Mnoise
99.28 %
99.41%
99.46%
99.43%
(b) CIFQR10
CleanModel W Mcontent W Munr elated W Mnoise
78.49%
78.6%
78.41%
78.12%
5.4 Robustness
The goal of robustness is to measure whether our watermarking
framework is robust to different model modifications. We measure
the robustness of our watermarking framework with following two
commonly used modifications.
Model pruning. Although DNNs have shown superior perfor-
mance over the traditional state-of-the-art machine learning algo-
rithms, they usually contain a large amount of parameters, which
are caused by deeper layers and more neurons in each layer. The
size of deep neural networks tremendously increased, from the first