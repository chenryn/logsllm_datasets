x
Fig. 7. Performance measures used for computing one trust experience
72
M. Reh´ak et al.
experience ti,k
α with that aggregation agent α as follows:
=
ti,k
α
¯y − ¯xk
σy + σk
x
.
(19)
The intention behind this formula is that an agent is more trustworthy, if its classiﬁ-
cations are more accurate (¯xk is low and ¯y is high), and more precise (the standard
deviations are low). Note that ti,k
α will rarely be negative
in practice.
α lies in (−∞,∞); however ti,k
To get the attack-class speciﬁc trust value T k
α for an agent α, we aggregate the past
trust experiences with that agent regarding the challenges from class k:
=
T k
α
(cid:3)
i
wi ∗ ti,k
α ,
(20)
where wi are weights that allow recent experiences a higher impact. This is done be-
cause older experiences are expected to be less signiﬁcant than more recent ones. In
our current system, the weights decrease exponentially. The system receives the input
events in 5 minute batches, and assigns the same weight to all events in each batch. The
weight of the challenges from the batch i is determined as:
wi =
1
W
e(j−i) ln(0.1)
4
,
(21)
where the j denotes the current time step, and the value of the coefﬁcient ln(0.1)
4 was
selected so that challenges from the ﬁfth batch (the oldest one being used) are assigned
a weight of 0.1 before the normalization. The normalization is performed simply by di-
wi = 1.
viding all weights by the sum of their un-normalized values W to ensure that
We are currently using the challenges from the last 5 batches, meaning that the (j − i)
part of the exponent takes the values between 0 and 4. Please note that the speciﬁc as-
signment of weights wi is highly domain speciﬁc, and is only included as an illustration
of the general principle.
(cid:4)
The ﬁnal trust value T i
α for the aggregation agent α is determined as a linear combi-
nation of the partial, attack class-speciﬁc values T k
α:
P (ACk) · T k
α ,
T i
α
K(cid:3)
=
(22)
where the weights P (ACk) attributed to the trustworthiness of the individual classes
are derived from Eq. 10.
k=1
5.1 Optimizing Number of Challenges
The number of challenges used as basis for the computation of the trust experiences ti,k
α
should be as small as possible while at the same time providing accurate results for the
trust experiences. This means that we want to know the minimum number of challenges
n for computing ¯xk and ¯y which gives certain guarantees about the estimation of the
actual means μxk and μy (estimated by ¯xk and ¯y respectively).
Runtime Monitoring and Dynamic Reconﬁguration for Intrusion Detection Systems
73
Guaranteeing margin of error m. At the outset, let us make two reasonable assump-
tions. First, we assume that the samples are normally distributed. This is the common
assumption if nothing is known about the actual underlying probability distribution.
Second, as suggested in [16], we assume the sample standard deviations which we
found in past observations to be the actual standard deviations σk
x and σy. Then, the fol-
lowing formula gives us the number of challenges n that guarantees a speciﬁed margin
of error m when estimating μxk (or μy analogously) [16]:
n =
(cid:6)2
(cid:5)
z
∗
σk
x
m
,
(23)
∗
∗
where the critical value z
is a constant that determines how conﬁdent we can be. Com-
are 1.645 for 90%, 1.960 for 95% and 2.576 for 99%. More
mon critical values z
speciﬁcally, the integral of the standard normal distribution in the range [−z
∗] equals
is for instance chosen for a conﬁdence level of
the respective conﬁdence level. If z
99%, we know that if we use n challenges for computing ¯xk, the actual mean μxk will
lie in the interval ¯xk ± m with the probability of 0.99.
, z
∗
∗
Choosing margin of error m. The margin of error m is chosen such that we can be
conﬁdent that the order of the ﬁrst two most trustworthy agents is conﬁrmed. In turn,
this conﬁrms that the selection of the ﬁrst agent is the best choice. Let us call the ﬁrst
and the second agent α1 and α2 respectively, so we have Tα1 ≥ Tα2. We want to make
sure that for the next trust experience this order is not reversed by chance. Recall that a
trust experience tα is deﬁned as the difference between ¯y and ¯xk weighted by the sum
of the corresponding standard deviations (see formula (22)). As we use 2∗ n challenges
to ﬁnd ¯y and ¯xk respectively, the overall margin of error for the difference of ¯y and ¯xk
will not be higher than 2 ∗ m. The largest margin of error m
for which tα1 ≥ tα2 is
still true (with the given conﬁdence), must therefore fulﬁll the equation where tα1 takes
the lowest and tα2 the highest possible value.
(cid:4)
tα1 ≥ ¯y1 − ¯xk
1 − 2m
k
(cid:8)(cid:9)
(cid:10)
=:a
σy1 + σx1
(cid:7)
(cid:4)
=
2 + 2m
¯y2 − ¯xk
σy2 + σx2
(cid:8)(cid:9)
(cid:7)
=:b
k
(cid:10)
(cid:4)
≥ tα2 ,
where the inner equation can be solved to give:
(cid:4) =
m
(tα1 − tα2)ab
2(a + b)
= b(¯y1 − ¯xk
1) − a(¯y2 − ¯xk
2)
2(a + b)
.
(24)
(25)
(cid:4)
So, a choice of m with the constraint m ≤ m
, guarantees with the speciﬁed conﬁdence
that we will get tα1 ≥ tα2 — in the case that this is the true order. To limit the number
of challenges, we choose the maximal margin of error m that fulﬁlls this constraint,
which is given by m := m
. We also impose an additional lower bound on m, in order
to prevent the number of challenges to grow disproportionally when the differences
between the agent’s trustworthiness with respect to this speciﬁc attack class ACk are
insigniﬁcant.
(cid:4)
74
M. Reh´ak et al.
6 Experimental Evaluation
In the experimental part of our work, we evaluate two aspects of the mechanism: its
ability to effectively reduce the number of false positives, while relying on an acceptable
number of challenges, and its ability to selectively identify the events relevant to the
priority threats as speciﬁed by the system administrator.
All the experiments were conducted on a university network, on the background
of the regular network trafﬁc. This background trafﬁc contains roughly 10% of mali-
cious ﬂows, principally related to scanning, peer-to-peer activity, botnet propagation
and brute force attacks on passwords, in no particular order.
In the ﬁrst series of experiments, we test the ability of the suggested mechanism to
produce the classiﬁcations with a reasonable error rate as expressed in terms of false
positives and false negatives. To evaluate the error rate, we have manually classiﬁed the
trafﬁc from a signiﬁcant subset of active hosts on the network. This classiﬁed trafﬁc
is then used to gauge the effectiveness of the method. The system observed about 80
000 ﬂows every 5 minutes, with roughly 20 000 ﬂows being malicious, and that the
evaluation was performed over about seventy 5-minute long observation intervals. The
system contained 30 aggregation agents, each of them averaging the opinions of the 5
underlying detection agents as described in Section 2.
In Fig. 8, we can see the number of challenges as it evolves over time. At the begin-
ning, the system works with a ﬁxed number of challenges, in order to let the anomaly
detection methods in the detection agents adapt to the trafﬁc. Once all the detection
agents start (at step 5, after 25 minutes), the system starts to progressively insert more
challenges, in order to build an initial assessment of all classiﬁer agents. The number
of challenges peaks at around the step 14, when it reaches 100 (all challenges com-
bined). Once a user agent has built the initial trustworthiness for all agents, the num-
ber of challenges decreases until it levels out at around 40 (legitimate and malicious
s
e
g
n
e
l
l
a
h
c
f
o
m
u
N
70
60
50
40
30
20
10
0
0
Num of malicious challenges
Num of legitimate challenges
10
20
30
Time
40
50
60
70
)
s
P
I
r
c
S
#
(
e
v
i
t
i
s
o
p
l
e
s
a
F
80
70
60
50
40
30
20
10
0
0
10
20
30
Time
40
50
60
70
Fig. 8. Number of challenges over time, both le-
gitimate (top, green curve) and malicious (bot-
tom, red curve)
Fig. 9. Number of
false positives (unique
sources). Each aggregation agent is represented
by one thin curve, the solid curve shows the per-
formance of the aggregation agent dynamically
selected by the system.
Runtime Monitoring and Dynamic Reconﬁguration for Intrusion Detection Systems
75
Table 1. Results of static system with arithmetic average (top line) compared to the selection of a
single aggregation agent (middle part) and the dynamic self-adaptation mechanism described in
this paper. Values are averaged to obtain the expected error numbers for one observation period.
Result
Arithmetic average
Average for aggregation fct.
Min FP for aggregation fct.
Min FN for aggregation fct.