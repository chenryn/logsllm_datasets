a reservation. The package that is created is handled seamlessly as one
transaction and requires only one payment from the consumer, hiding
the pricing of individual components.
Current dynamic packaging applications are developed using a
hard-coded approach to develop the interfaces among various systems
to allow the interoperability of decentralized, autonomous, and
heterogeneous tourism information systems. However, such an
approach for integration does not comply with the highly dynamic and
decentralized nature of the tourism industry. Most of the players are
small or medium-sized enterprises with information systems with
different scopes, technologies, architectures, and structures. This
diversity makes the interoperability of information systems and
technologies very complex and constitutes a major barrier for
emerging e-marketplaces and dynamic packaging applications that
particularly affects the smaller players (Fodor and Werthner 2004-5).
Two emerging technologies can enable the deployment of a more
integrated solution to implement dynamic application (Cardoso 2005):
20 Semantic Web Processes and Their Applications
Web services and semantics. As opposed to the hard-coded approach,
Web services take a loosely coupled software components approach,
which can be dynamically located and integrated on the Web. Web
services are flexible to easily design processes that model dynamic
packaging applications. Semantics are important to dynamic
packaging applications because they provide a shared and common
understanding of data and services of the tourism information systems
to integrate. Semantics can be used to organize and share tourism
information, which allow better interoperability and integration of
inter- and intra-company travel information systems.
Figure 1-9 illustrates the integration of various tourism information
systems to support the concept of dynamic packaging. As it can be
seen, new communication links are established among the various
participant of the distribution model to integrate tourism products.
Figure 1-9. Integration of tourism information systems
So far, the travel industry has concentrated its efforts on
developing open specification messages, based on XML, to ensure
that messages can flow between industry segments as easily as within.
For example, the OpenTravel Alliance (OTA 2004) is an organization
pioneering the development and use of specifications that support e-
business among all segments of the travel industry. It has produced
more than 140 XML-based specifications for the travel industry
(Cardoso 2004).
The development of open specifications messages based on XML,
such as OTA schema, to ensure the interoperability between trading
partners and working groups is not sufficiently expressive to
The Semantic Web and its Applications 21
guarantee an automatic exchange and processing of information to
develop dynamic applications. A more appropriate solution is the
development of suitable ontologies for the tourism industry that can
serve as a common language for tourism-related terminology and a
mechanism for promoting the seamless exchange of information
across all travel industry segments. Ontologies are the key elements
enabling the shift from a purely syntactic to a semantic
interoperability. An ontology can be defined as the explicit, formal
descriptions of concepts and their relationships that exist in a certain
universe of discourse, together with a shared vocabulary to refer to
these concepts. With respect to an ontology a particular user group
commits to, the semantics of data provided by the data sources to
integrate can be made explicit. Ontologies can be applied to the area
of dynamic packaging to explicitly connect data and information from
tourism information systems to its definition and context in machine-
processable form. Ontologies can be used to bring together
heterogeneous Web services, Web processes, applications, data, and
components residing in distributed environments. Semantic Web
processes, managing dynamic package determine which Web services
are used, what combinations of Web services are allowed or required
and specific rules determine how the final retail price is computed
(Cardoso, Miller et al. 2004).
6.4 Semantic digital libraries
Libraries are a key component of the information infrastructure
indispensable for education. They provide an essential resource for
students and researchers for reference and for research. Metadata has
been used in libraries for centuries. For example, the two most
common general classification systems, which use metadata, are the
Dewey Decimal Classification (DDC) system and the Library of
Congress Classification (LCC) system. In the United States, the DDC
is used in 95% of all public and K-12 school libraries, in 25% of
college and university libraries, and in 20% of special libraries. The
DDC system has 10 major subjects, each with 10 secondary subjects
(DDC 2005). The LCC system uses letters instead of numbers to
organize materials into 21 general branches of knowledge. The 21
subject categories are further divided into more specific subject areas
by adding one or two additional letters and numbers (LCCS 2005).
As traditional libraries are increasingly converting themselves to
digital libraries, a new set of requirements has emerged. One
22 Semantic Web Processes and Their Applications
important feature for digital libraries is the availability to efficiently
browse electronic catalogues browsed. This requires the use of
common metadata to describe the records of the catalogue (such as
author, title, and publisher) and common controlled vocabularies to
allow subject identifiers to be assigned to publications. The use of a
common controlled vocabulary, thesauri, and taxonomy (Smrz,
Sinopalnikova et al. 2003) allows search engines to ensure that the
most relevant items of information are returned. Semantically
annotating the contents of a digital library’s database goes beyond the
use of a controlled vocabulary, thesauri, or taxonomy. It allows
retrieving books’ records using meaningful information to the existing
full text and bibliographic descriptions.
Semantic Web technologies, such as RDF and OWL, can be used
as a common interchange format for catalogue metadata and shared
vocabulary, which can be used by all libraries and search engines
(Shum, Motta et al. 2000) across the Web. This is important since it is
not uncommon to find library systems based on various metadata
formats and built by different persons for their special purposes. By
publishing ontologies, which can then be accessed by all users across
the Web, library catalogues can use the same vocabularies for
cataloguing, marking up items with the most relevant terms for the
domain of interest. RDF and OWL provide a single and consistent
encoding so implementers of digital library metadata systems will
have their task simplified when interoperating with other digital
library systems.
6.5 Semantic Grid
The concept of Grid (Foster and Kesselman 1999) has been
proposed as a fundamental computing infrastructure to support the
vision of e-Science. The Grid is a service for sharing computer power
and data storage capacity over the Internet and goes well beyond
simple communication providing functionalities that enable the rapid
assembly and disassembly of services into temporary groups.
Recently, the Grid has been evolving towards the Semantic Grid to
yield an intelligent platform which allows process automation,
knowledge sharing and reuse, and collaboration within a community
(Roure, Jennings et al. 2001). The Semantic Grid is about the use of
semantic Web technologies in Grid computing; it is an extension of
the current Grid. The objective is to describe information, computing
resources, and services in standard ways that can be processed by
The Semantic Web and its Applications 23
computers. Resources and services are represented using the
technologies of the semantic Web, such as RDF. The use of semantics
to locate data has important implications for integrating computing
resources. It implies a two-step access to resources. In step one, a
search of metadata catalogues is used to find the resources containing
the data or service required by an application. In the second step, the
data or service is accessed or invoked.
6.6 Semantic Enterprise Information Integration
The challenges for today’s enterprise information integration
systems are well understood. In order to manage and use information
effectively within the enterprise, three barriers that increase the
complexity of managing information have to be overcome: the diverse
formats of content, the disparate nature of content, and the need to
derive “intelligence” from this content.
Current software tools that look at structuring content by
leveraging syntactic search and even syntactic metadata are not
sufficient to handle these problems. What is needed is actionable
information from disparate sources that reveals non-obvious insights
and allows timely decisions to be made. The new concept known as
semantic metadata is paving the way to finally realize the full value of
information. By annotating or enhancing documents with semantic
metadata, software programs can automatically understand the full
context and meaning of each document and can make correct
decisions about who can use the documents and how these documents
should be used.
Semantic is a key enabler for deriving business value via enterprise
information integration and can enable the next generation of
information integration and analysis software in the following areas
(Sheth 2003):
• Extract, organize, and standardize information from many
disparate and heterogeneous content sources (including structured,
semi-structured, and unstructured sources) and formats (database
tables, XML feeds, PDF files, streaming media, and internal
documents)
• For a domain of choice, identify interesting and relevant
knowledge (entities such as people’s names, places, organizations,
etc., and relationships between them) from heterogeneous sources
and formats.
24 Semantic Web Processes and Their Applications
• Analyze and correlate extracted information to discover previously
unknown or non-obvious relationships between documents and/or
entities based on semantics (not syntax) that can help in making
business decisions.
• Enable high levels of automation in the processes of extraction,
normalization, and maintenance of knowledge and content for
improved efficiencies of scale.
• Make efficient use of the extracted knowledge and content by
providing tools that enable fast and high-quality (contextual)
querying, browsing, and analysis of relevant and actionable
information.
6.7 Semantic Web Search
Swoogle1 is a crawler-based indexing and retrieval system for the
semantic Web built on top of the Google API. It was developed in the
context of a research project of the ebiquity research group at the
Computer Science and Electrical Engineering Department of the
University of Maryland.
In contrast to Google (Google 2005), Swoogle discovers, analyzes,
and indexes Semantic Web Documents (SWD) written in RDF and
OWL, rather than plain HTML documents. Documents are indexed
using metadata about classes, properties, and individuals, as well as
the relationships among them. Unlike traditional search engines,
Swoogle aims to take advantage of the semantic metadata available in
semantic Web documents. Metadata is extracted for each discovered
document and relations (e.g. similarities) among documents are
computed. Swoogle also defines an ontology ranking property for
SWD which is similar to the pageRank (Brin and Page 1998)
approach from Google and uses this information to sort search results.
Swoogle provides query interfaces and services to Web users. It
supports software agents, programs via service interfaces, and
researchers working in the semantic Web area via the Web interface.
Swoogle’s database does not stores all of the content of the SWD
discovered. It only stores metadata about the documents, the terms,
and the individuals they define and use. Currently, the database has
information on more that 275 thousand semantic Web documents
which contain more than 40 million triples and define more than 90
thousand classes, 50 thousand properties, and 6 million individuals.
1 http://swoogle.umbc.edu/
The Semantic Web and its Applications 25
A much earlier and commercial effort in building semantic search
was Taalee’s MediaAnywhere A/V search engine (Townley 2000;
Sheth 2001). In this system, ontology driven metadata extraction
automatically extracted and refreshed semantic metadata associated
with audio/video content rich Web sites. It used ontologies in areas
such as Sports, Entertainment, Business and News. Ontology-driven
forms based querying supported specification of semantic queries.
6.8 Semantic Web and AI
The merit of the semantic Web is that its concepts and vision are
pragmatically oriented. This is a contrast to the speculative aims of
Artificial Intelligence (AI). A sharp distinction between semantic Web
and AI can be made between the relevance and understanding of data
and programs. AI is concerned with highly complex programs being
able to understand data, e.g. texts and common sense. The semantic
Web is more concerned in making its data “smart” and giving them
some machine-readable semantics. While, AI tends to replace human
intelligence, semantic Web asks for human intelligence.
Inference mechanisms that can deal with the massive number of
assertions that would be encountered by semantic Web applications
are required. The claimed power behind many of the proposed
applications of semantic Web technology is the ability to infer
knowledge that is not explicitly expressed. Needless to say, this
feature has attracted the attention from the AI community since they
have been dealing with issues relating to inference mechanisms in the
past. Inference mechanisms are applicable only in the context of
formal ontologies. The idea is to use rules and facts to assert new facts
that were not previously known. One of the most common knowledge
representation languages has been Description Logic (Nardi and
Brachman 2002) on which DAML, one of the earliest semantic Web
languages is based.
6.9 Semantic Web and databases
Although an ontology schema may resemble at a representational
level a database schema, and instances may reflect database tuples, the
fundamental difference is that ontology is supposed to capture some
aspect of real-world or domain semantics, as well as represent
ontological commitment forming the basis of semantic normalization.
Nevertheless, many researchers in the database community continue
26 Semantic Web Processes and Their Applications
to express significant reservations toward the semantic Web. The
following list shows some examples of remarks about semantic Web
technology (Sheth and Ramakrishnan 2003).
“As a constituent technology, ontology work of this sort is
defensible. As the basis for programmatic research and
implementation, it is a speculative and immature technology of
uncertain promise.”
“Users will be able to use programs that can understand
semantics of the data to help them answer complex questions … This
sort of hyperbole is characteristic of much of the genre of semantic
web conjectures, papers, and proposals thus far. It is reminiscent of
the AI hype of a decade ago and practical systems based on these
ideas are no more in evidence now than they were then.”
“Such research is fashionable at the moment, due in part to
support from defense agencies, in part because the Web offers the first
distributed environment that makes even the dream seem tractable.”
“It (proposed research in Semantic Web) pre-supposes the
availability of semantic information extracted from the base
documents -an unsolved problem of many years, …”
“Google has shown that huge improvements in search technology
can be made without understanding semantics. Perhaps after a certain
point, semantics are needed for further improvements, but a better
argument is needed.”
These reservations likely stem from a variety of reasons. First, this
may be a product of the goals of the semantic Web as depicted in
(Berners-Lee, Hendler et al. 2001). Specifically, database researchers
may have reservations stemming from the overwhelming role of
description logic in the W3C’s Semantic Web Activity and related
standards. The vision of the semantic Web proposed in several articles
may seem, to many readers, like a proposed solution to the long
standing AI problems. Lastly, one of the major reservations is related
to the concern about the scalability of the three core capabilities for
the semantic Web to be successful, namely the scalability of the (a)
creation and maintenance of large ontologies, (b) semantic annotation,
and (c) inference mechanisms or other computing approaches
The Semantic Web and its Applications 27
involving large, realistic ontologies, metadata, and heterogeneous data
sets.
6.10 Bioinformatics Ontologies
The integration of information sources in the life sciences is one of
the most challenging goals of bioinformatics (Kumar and Smith
2004). In this area, the Gene Ontology (GO) is one of the most
significant accomplishments. The objective of GO is to supply a
mechanism to guarantee the consistent descriptions of gene products
in different databases. GO is rapidly acquiring the status of a de facto
standard in the field of gene and gene product annotations (Kumar and
Smith 2004). The GO effort includes the development of controlled
vocabularies that describe gene products, establishing associations
between the ontologies, the genes, and the gene products in the
databases, and develop tools to create, maintain, and use ontologies
(see http://www.geneontology.org/). GO has over 17,000 terms and it
is organized in three hierarchies for molecular functions, cellular
components, and biological processes (Bodenreider, Aubry et al.
2005).