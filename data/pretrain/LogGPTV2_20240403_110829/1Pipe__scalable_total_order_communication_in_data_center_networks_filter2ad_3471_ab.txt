may register a callback function via onepipe_proc_fail_callback to
get notifications of failed processes. Message delivery is resumed
after all non-fail processes finish their callback functions.
Reliable 1Pipe also provides restricted failure atomicity, which
means all-or-nothing delivery of a scattering of messages with the
exception that if a receiver fails permanently or network partitions
after the decision to deliver is made, the receiver can never deliver it.
If a receiver recovers from failures, it can deliver or discard messages
consistently with the other receivers in the same scattering. In fact,
full failure atomicity is impossible in a non-replicated fail-stop
model, because a receiver or its network link may fail permanently
before delivering ğ‘‡ almost simultaneously with another receiver
delivering ğ‘‡ [38]. Reliable 1Pipe achieves atomicity via two-phase
commit and in-network barrier aggregation, so messages delivery
needs 1.5 RTTs plus barrier wait time.
In fault tolerant applications, 1Pipe provides a fast path in normal
cases, and falls back to the application for customized failure han-
dling. More concretely, an application may use state machine repli-
cation to replicate its states, and register onepipe_proc_fail_callback
which invokes a traditional consensus algorithm [65, 80]. Each
message is scattered to all replicas. When failure occurs, message
delivery is stalled, and 1Pipe invokes the callbacks in all non-fail pro-
cesses. Restricted failure atomicity ensures that all correct replicas
deliver the same sequence of messages. If the correct replicas reach
a quorum, the callbacks return, and message delivery is resumed.
Otherwise, there are too many failed replicas, and the application
can choose between consistency and availability. If it chooses consis-
tency and waits for some replicas to recover, the recovered replicas
can deliver the same sequence of messages.
2.2 Use Cases of 1Pipe
2.2.1 Total Ordering of Communication Events. Production data
centers provide multiple paths between two nodes [8, 43]. Due to
different delays of different paths, several categories of ordering
hazards [41, 89] may take place (Figure 2).
Write after write (WAW). Host ğ´ writes data to another host ğ‘‚,
then sends a notification to host ğµ. Send can be considered as a
write operation. When ğµ receives the notification, it issues a read to
ğ‘‚, but may not get the data due to the delay of Aâ€™s write operation.
Independent read, independent write (IRIW). Host ğ´ first
writes to data ğ‘‚1 and then writes to metadata ğ‘‚2. Concurrently,
host ğµ reads metadata ğ‘‚2 and then reads data ğ‘‚1. It is possible that
ğµ reads the metadata from ğ´ but the data is not updated yet.
Ordering hazards affect system performance. To avoid the WAW
hazard, ğ´ needs to wait for the first write operation to complete
(an RTT to ğ‘‚, called a fence) before sending to ğµ, thus increasing
latency. To avoid the IRIW hazard, ğ´ needs to wait for write ğ‘‚1 to
complete before initiating write ğ‘‚2, and ğµ needs to wait for read
ğ‘‚2 to complete before initiating read ğ‘‚1. The fence latency will be
amplified when more remote objects need to be accessed in order.
1Pipe can remove both WAW and IRIW hazards due to causality
and total ordering. In WAW case, by monotonicity of host times-
tamp, ğ´ â†’ ğ‘‚ is ordered before ğ´ â†’ ğµ. By causality, ğ´ â†’ ğµ is
ordered before ğµ â†’ ğ‘‚. Consequently, ğ´ â†’ ğ‘‚ is ordered before
ğµ â†’ ğ‘‚. Therefore, the write operation is processed before the read
operation at host ğ‘‚, thus avoiding WAW hazard. By removing the
fence between ğ´ â†’ ğ‘‚ and ğ´ â†’ ğµ, 1Pipe reduces the end-to-end
latency from 2.5 RTTs to 1.5 RTTs, in the absence of packet losses.
If an application needs to process many WAW tasks in sequence,
the power of 1Pipe is amplified. Using the traditional method, the
application needs 1 RTT of idle waiting during each WAW task, so,
the throughput is bounded by 1/RTT. In contrast, using 1Pipe, the
application can send dependent messages in a pipeline.
The argument above neglects possible packet losses. In best
effort 1Pipe, there is a small possibility that message ğ´ â†’ ğ‘‚ is
lost in flight, so every object needs to maintain a version, and ğµ
needs to check the version of ğ‘‚. If it does not match the version
in the notification message from ğ´, then ğµ needs to wait for ğ´ to
retransmit ğ‘‚ and re-notify ğµ. ğ´ registers send failure callback and
performs rollback recovery when ğ´ is notified of a send failure.
If we use reliable 1Pipe, objects no longer need versioning, but
the end-to-end latency increases from 2.5 to 3.5 RTTs. However,
ğ´ can still send messages in a pipeline, and have much higher
throughput than the traditional way. In addition, reliable 1Pipe
can preserve causality in failure cases [16]: if ğ´ fails to write to ğ‘‚,
message ğ´ â†’ ğµ will not be delivered, similar to Isis [19].
Similarly, 1Pipe removes IRIW hazard and improves minimum
end-to-end latency from 3 RTTs to 1 RTT by eliminating two fences.
The minimum latency is achieved when ğ´ and ğµ initiate read and
write simultaneously.
1Pipeâ€™s power to remove ordering hazards comes from its ability
to totally order communication events. This power is also a per-
fect match with total store ordering (TSO) memory model [89] in a
distributed shared memory (DSM) system. In TSO, each processor
observes a consistent ordering of writes from all other cores. In
other words, processors must not observe WAW and IRIW hazards.
Compared to weaker memory models, TSO reduces synchronization
in concurrent programming [77, 93], thereby simplifying program-
ming and reducing fence overheads.
2.2.2
1-RTT Replication. Replication is essential for fault toler-
ance. Traditional multi-client replication requires 2 RTTs because
client requests must be serialized (e.g., sent to a primary) before
sending to replicas [81]. With 1Pipe, we can achieve 1-RTT repli-
cation without making assumptions on log content, because the
network serializes messages. A client can directly send a log mes-
sage to all replicas with a scattering, and each replica orders logs
according to the timestamp (ties are broken by the client ID). Be-
cause reliable 1Pipe has an extra RTT, we use unreliable 1Pipe, and
handle packet losses and failures in a more clever way. First, to
ABOsendwritereadABwritereadO1O2writereadSIGCOMM â€™21, August 23â€“28, 2021, Virtual Event, USA
Bojie Li, Gefei Zuo, Wei Bai, and Lintao Zhang
ensure ordered delivery of logs between each pair of client and
replica, they maintain a sequence number. The replica rejects mes-
sages with non-consecutive sequence numbers. Second, to detect
inconsistent logs due to packet losses, each replica maintains a
checksum of all previous log messages from all clients. When a
replica receives a message, it adds the message timestamp to the
checksum, and returns the checksum to the client. The response
message does not need to be ordered by 1Pipe. If a client sees all
checksums are equal from the responses, the logs of replicas are
consistent at least until the clientâ€™s log message, and the client
knows that the replication succeeds. Otherwise, there must be lost
messages to some replica or failure of a client or a replica. In the
case of packet loss, the client simply retransmits the messages since
the first rejected one. In the case of suspected replica failure, the
client notifies all replicas (or a cluster manager) to initiate a failure
recovery protocol, which uses a traditional consensus protocol to
remove inconsistent log entries and make checksums match. When
there is no packet loss and failure, replication only needs 1 RTT.
Similar to replication, 1Pipe can achieve state machine repli-
cation (SMR) [63] or virtual synchrony [17]. In a SMR-based dis-
tributed system, each message is broadcast to all processes, and each
process uses the same sequence of input messages. SMR can solve
arbitrary synchronization problems [63]. An example is mutual ex-
clusion that requires the resource to be granted in the order that the
request is made [63]. With reliable 1Pipe, using SMR to implement
the lock manager can solve the mutual exclusion problem.
2.2.3 Distributed Atomic Operation (DAO). A DAO is a trans-
action that atomically reads or updates objects in multiple hosts.
DAO is widely used in transactional key-value store [32], caches in
web services, distributed in-memory computing, and index cache
of distributed storage. Traditionally, a DAO needs 3 RTTs: (1) lock
the objects; (2) if all locks succeed, send operations to the par-
ticipants; (3) unlock the objects. Using reliable 1Pipe, a DAO is
simply a scattering with the same timestamp from the initiator.
Each recipient processes messages from all DAOs in timestamp
order. So, the DAOs are serializable. If a recipient or the network
permanently fails, atomicity violation is not observable because the
objects cannot be accessed by any subsequent operations.
As an optimization, we can use unreliable 1Pipe for read-only
DAOs because if it fails due to packet losses, the initiator can retry it.
In terms of SNOW [72] and NOCS [73] theorems, 1Pipe provides 1-
RTT read-only DAOs with strict serializability, no storage overhead
and close-to-optimal throughput, but at the expense of blocking
operations until receiving the barrier timestamp.
2.2.4 Other Scenarios. In general transactions with Opacity [90],
to obtain read and write timestamps, a transaction needs to query
a non-scalable centralized sequencer [35, 57] or wait for clock un-
certainty (e.g., Spanner [27] waits âˆ¼10ms and FaRMv2 [90] waits
âˆ¼20ğœ‡s). 1Pipe can use local time as transaction timestamps directly
without waiting because lock messages of previous transactions
must be delivered to shards before data accesses of the current
transaction.
1Pipe timestamp is also a global synchronization point. For ex-
ample, to take a consistent distributed snapshot [24], the initiator
broadcasts a message with timestamp ğ‘‡ to all processes, which
Figure 3: Routing topology of a typical data center network. Each
physical switch is split into two logical switches, one for uplink and
one for downlink. Dashed virtual link between corresponding uplink
and downlink switch indicates â€œloopbackâ€ traffic from a lower-layer
switch or host to another one.
Figure 4: Architecture of a typical network switch.
directs all processes to record its local state and in-flight sent mes-
sages with a higher timestamp than ğ‘‡ .
3 BACKGROUND
In this section, we introduce unique characteristics of data center
network (DCN), which make a scalable and efficient implementation
of 1Pipe possible.
3.1 Data Center Network
Modern data centers typically adopt multi-rooted tree topologies [8,
43] to interconnect hundreds of thousands of hosts. In a multi-
rooted tree topology, the shortest path between two hosts first goes
up to one of the lowest common ancestor switches, then goes down
to the destination. Therefore, the routing topology form a directed
acyclic graph (DAG), as shown in Figure 3. This loop-free topology
enables hierarchical aggregation of barrier timestamps.
A highly available SDN controller runs on the management plane
to detect failures of switches and links, then reconfigure routing
tables on failures [42, 91].
3.2 Programmable Switches
A data center switch consists of a switching chip [3, 45] and a
CPU (Figure 4). The switch operating system [2] runs on the CPU
to reconfigure the switching chip. The switching chip forwards
selected traffic (typically control plane traffic, e.g., DHCP and BGP)
to the CPU via a virtual NIC.
The switching chip is composed of an ingress pipeline, multiple
FIFO queues and an egress pipeline. When a packet is received
from an input link, it first goes through the ingress pipeline to
determine the output link and queueing priority, then is put in
the corresponding FIFO queue. The egress pipeline pulls packets
DownlinkUplinkSendersReceiversSwitching chipIngressQueueEgressvNICSwitch CPUCtrl1Pipe: Scalable Total Order Communication in Data Center Networks
SIGCOMM â€™21, August 23â€“28, 2021, Virtual Event, USA
node (i.e., a switch or an end host) in the routing graph. The barrier
timestamp is the lower bound of message timestamps of all future
arrival messages from the link or at the node. Each receiver main-
tains its own barrier timestamp and deliver the messages whose
timestamps are smaller than the barrier timestamp.
If the transport between sender and receiver is FIFO, a receiver
can easily figure out the barrier if it has received messages from
all the senders: the barrier is the minimum timestamp of latest
messages from all the senders. Therefore, a naive solution would be
for every sender to send timestamped messages to every receiver
so that the receivers can figure out the barrier and deliver messages.
Unfortunately, this solution requires sending messages to receivers
not in the scattering group, which does not scale.
Hierarchical barrier timestamp aggregation. 1Pipe exploits
the knowledge of the queuing structure of the network, making
the lower bound aggregation much more scalable and efficient
than if it was implemented at only a logical level. 1Pipe leverages
programmable switches to aggregate the barrier timestamp infor-
mation. Given the limited switch buffer resource, 1Pipe does not
reorder messages in the network. Instead, 1Pipe forwards messages
in the network as usual, but reorders them at the receiver side based
on the barrier timestamp information provided by the switch.
In 1Pipe, we attach two timestamp fields to each message packet.
The first is message timestamp field, which is set by the sender and
will not be modified. The second field is barrier timestamp, which
is initialized by the sender but will be modified by switches along
the network path. The property of the barrier timestamp field is:
When a switch or a host receives a packet with barrier timestamp
ğµ from a network link ğ¿, it indicates that the message timestamp and
barrier timestamp of future arrival packets from link ğ¿ will be larger
than ğµ.
To derive barriers, the sender initializes both fields of all the
packets in a message with the non-decreasing message timestamp.
The switch maintains a register ğ‘…ğ‘– for each input link ğ‘– âˆˆ I, where