Algorithm heuristic2
foreach node n of the ﬁnal correct nodes do
markCorrect(n)
end
foreach node n of the ﬁnal erroneous nodes do
markErroneous(n)
end
foreach node n of the erroneous nodes do
cond1 ← (n.erroneousParentsCount == 0)
cond2 ← (n.correctParentsCount ≥ 1)
if cond1 AND cond2 then
Increment Counters of resources used in n;
end
1
2
3
4
5
6
7
8
9
369369369
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:27:50 UTC from IEEE Xplore.  Restrictions apply. 
After both heuristics are applied,
the counter values
computed by the heuristics are averaged to obtain the ﬁnal
counter values. The diagnosis algorithm identiﬁes the top
Ndeconf resources with the highest counter values as candi-
dates of the faulty resource, where Ndeconf is a ﬁxed value.
These are the processor resources that are disabled to ﬁx the
intermittent fault after diagnosis. Thus Ndeconf represents a
trade-off between diagnosis accuracy and granularity. We
study this trade-off in Section VI-B1.
In general, we disable all the Ndeconf resources identiﬁed
by the diagnosis algorithm, with one exception. Because the
number of functional units in a processor is typically low,
we never disable more than one functional unit. This means
that if the number of functional units among the resources
with Ndeconf highest ﬁnal counter values is more than one,
only the unit with the highest counter value is disabled.
Example: Due to space constraints, we only demonstrate
the application of the ﬁrst heuristic to the augmented DDG in
Figure 5. Heuristic 1 starts from erroneous nodes (nodes 2, 6,
7 & 8). None of the erroneous nodes in this DDG have a ﬁnal
correct ancestor and therefore S21 = S61 = S71 = S81 = φ.
The backward slice for each of the erroneous nodes are
collected by the algorithm (S22 = {2, 1}, S82 = {8, 5},
S72 = {7, 5}, S62 = {6, 5}). The counters of resources
in these sets are incremented by 1 as they have each
participated in creating an erroneous value.
These nodes might also have participated in creating a
ﬁnal correct value. If so, they are pruned from the back-
ward slice before their counters are incremented (Line 10).
However, none of the nodes in the backward slices of the
erroneous nodes in Figure 5 have ﬁnal correct nodes as their
children. Therefore, no pruning occurs in the example.
We can see that node 5 which has used the faulty resource
fu-1, appears in the backward slices of three erroneous nodes
(6, 7 & 8). This means that the counter related to fu-1 is
incremented 3 times. Meanwhile, fu-1 is also used by the
node 1 in the backward slice of erroneous node 2 (based on
Table I), and hence its counter value is again incremented
by 1. The ﬁnal counter values are shown in Table III. As
seen from the table, the faulty resource fu-1 is the resource
with the highest counter value of 4.
Resource
fu-1
rob-84
ifq-1
rs-7
Value
4
3
3
3
Resource
fu-5
rob-85
lsq-2
...
Value
2
1
1
1
Table III: Counter values after applying heuristic 1 to DDG
in Figure 5
Fault Recurrence: The above discussion considers a
single occurrence of an intermittent fault. However, by their
very deﬁnition, intermittent faults will recur, thus giving us
an opportunity to diagnose them again. The above diagnosis
process is repeated after every failure resulting from an
intermittent fault, and each iteration of the process yields
a different counter value set. The ﬁnal counter values are
averaged across multiple iterations, thus boosting the diag-
nosis accuracy, and smoothing the effect of inaccuracies.
370370370
VI. EVALUATION
We answer the following research questions to evaluate
our diagnosis technique:
1) RQ 1: What is the diagnosis accuracy or the probability
that the technique correctly ﬁnds the faulty resource?
2) RQ 2: What is the performance overhead of repairing
the processor after ﬁnding the faulty resource?
3) RQ 3: How much online performance, power and area
overhead is incurred because of SCRIBE?
4) RQ 4: What is the ofﬂine performance overhead of
SIED (Replay + DDG Construction and analysis)?
In this section, we present the experimental setup and the
results of our evaluations.
A. Methodology
SCRIBE: We implemented SCRIBE in sim-mase, a cycle-
accurate micro-architectural simulator, which is a part of the
SimpleScalar family of simulators [23]. We based our im-
plementation on the SimpleScalar Alpha-Linux, developed
as part of the XpScalar framework [24].
Conﬁgurations: To understand the overhead of our diagno-
sis mechanism across different processor families, we use
three different conﬁgurations (Narrow, Medium and Wide
pipelines) for our experiments. These respectively represent
processors in the embedded, desktop and server domains,
and have been used in prior work on instruction-level
duplication [25]. Table IV lists the common conﬁgurations
between the simulated processors and Table V shows the
conﬁgurations that vary across processor families.
Parameter
Level 1 Data Cache
Level 1 Instruction Cache
Level 2 combined data
& instruction cache
Branch Predictor
Instruction TLB
Data TLB
Memory Access Latency
Value
32K, 4-way, LRU, 1-cycle
latency
32K, 4-way, LRU, 1-cycle
latency
512K, 4-way, LRU,
8-cycle latency
Bi-modal, 2-level
64K, 4-way, LRU
128K, 4-way, LRU
200 CPU Cycles
Table IV: Common machine conﬁgurations
We choose the RUI length based on the type of the
processor (recall from Section IV that RUI Length ∝ lg(Total
number of resources)). We choose the LogSQ and Logging
Buffer to be 32 and 64 entries respectively, as our experi-
ments indicate that increasing the sizes of these resources
beyond 32 and 64 makes no signiﬁcant improvement on
performance. More details may be found in our earlier
paper [21].
Benchmarks: We use eight benchmarks from the SPEC
2006 integer and ﬂoating-point benchmarks set. We chose
these benchmarks as they were compatible with our infras-
tructure. We did not cherry-pick them based on the results.
Fault Injector: We extended sim-mase to build a detailed
micro-architecture level fault injector. For each injection, the
program is fast-forwarded 20 million instructions to remove
initialization effects. Then a single intermittent fault burst
is injected into one of the following: i) Reorder Buffer
entries, ii) Instruction Fetch Queue entries, iii) Reservation
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:27:50 UTC from IEEE Xplore.  Restrictions apply. 
Topic
Parameter
Pipeline Width
Array Sizes
Number of
Functional Units
Fetch
Decode
Issue
Commit
ROB Size
LSQ Size
Integer Adder
Integer Multiplier
FP Adder
FP Multiplier
Machine Width
Nar. Med. Wide
2
2
2
2
64
32
2
1
1
1
8
8
8
8
256
32
8
1
2
1
4
4
4
4
128
32
4
1
1
1
Table V: Different machine conﬁgurations
Station entries, iv) Load/Store Queue entries v) functional
unit outputs. The starting cycle of the fault burst is uniformly
distributed over the total number of cycles executed by the
program. The number of cycles for which the fault persists
(fault duration) is also uniformly distributed over the interval
[5, 2000], as voltage and temperature ﬂuctuations last for
around 5 to several thousands of cycles ([26], [27]).
After injecting the fault burst, the benchmark is executed
and monitored for 1 million instructions to see if it crashes.
We consider only faults that lead to crashes for diagnosis.
This is because we do not assume the presence of error
detectors in the program that can detect an error and halt
it. To simulate a recurrent intermittent fault, we re-execute
a benchmark up to 50 times while keeping the injection
location unchanged. Note however that the starting cycle and
fault duration are randomly chosen in each run. We report
the results for scenarios in which 10 or more of the fault
injections into a location led to crashes (out of 50 injections).
Diagnosis: SIED is implemented using Python scripts and
starts whenever a benchmark crashes as a result of fault
injection. We extract the traces required to build the pro-
gram’s DDG by modifying the MASE simulator. However,
these traces would be extracted by a virtual machine or a
dynamic binary instrumentation tool in a real implemen-
tation of SIED (as explained in Section V-A). SIED also
relies upon deterministic replay mechanisms (as explained
in Section III-C) for diagnosis. We have extended sim-
mase to enable deterministic replay. Again, this would be
implemented by a deterministic replay technique in a real
implementation of SIED. We conducted the simulations and
diagnosis experiments on an Intel Core i7 1.6GHz system
with 8MB of cache.
Deconﬁguration Overhead: The deconﬁguration overhead
is measured as the processor’s slow-down after disabling the
candidate locations of the faulty resource suggested by our
diagnosis approach. We assume that the precise subset of
resources suggested by our technique can be deconﬁgured.
We used the medium width processor conﬁguration from
Table V for measuring the overhead after deconﬁguration.
SCRIBE Performance and Power Overhead: The per-
formance overhead is measured as the percentage of extra
cycles taken by the processor to run the benchmark programs
when SCRIBE is enabled. For measuring the overhead, we
execute each benchmark for 109 instructions in the MASE
simulator 1. We also implemented SCRIBE in the Wattch
simulator [28] to evaluate its power overhead. The metric
by which the power overhead of SCRIBE is evaluated is the
average total power per instruction. We used the CC3 power
evaluation policy in Wattch as it also takes into account the
fraction of power consumed when a unit is not used [28].
B. Results
1) Diagnosis Accuracy (RQ 1): Figure 6 shows the
accuracy of our diagnosis approach for faults occurring in
different units of the medium-width processor. We ﬁnd that
the average accuracy is 84% across all units. To put this in
perspective, our diagnosis approach identiﬁes 5 resources
out of more than 250 resources in the processor as faulty,
and the actual faulty resource is among these 5 resources,
84% of the time (later, we explain why we chose 5).
The diagnosis accuracy depends on the unit in which
the fault occurs, and ranges from 71% for IFQ to 95%
for LSQ. The reason for IFQ having low accuracy is that
faults in the IFQ cause the program to crash within a short
interval of time (i.e., they have shorter crash distances). Short
crash distances lead to lower accuracy, which is counter-
intuitive as one expects longer crash distances to cause loss
in the fault information and hence have lower accuracy.
However, our DDG analysis algorithm explained in Section
V-B uses backtracking the paths leading to ﬁnal erroneous
data. The more the number of these paths, the easier it is
for our algorithm to distinguish the faulty resource from
other resources, and hence higher the accuracy. Shorter crash
distances mean fewer paths, and hence lower accuracy.
is
that
SIED has only knowledge about ﬁnal data (correctness of
memory and register values at the failure point). The DDG
analysis heuristics in Section V-B use backtracking from