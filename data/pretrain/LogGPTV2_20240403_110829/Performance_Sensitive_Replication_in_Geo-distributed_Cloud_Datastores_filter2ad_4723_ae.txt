impact on the read latency, though the tail was more pro-
nounced. Interestingly, we also found that the bound of 50msec
increases the read latency by less than 20msec for 60% of the
keys. We found that constraints on write latency resulted in
conﬁgurations that had a signiﬁcantly higher replication factor
and higher read quorum sizes. This is expected because our
models tries to minimize the latency by moving the replica
closer to the write locations in order to meet the constraint.
We omit results for lack of space.
X. RELATED WORK
SPAR [44] presents a middle-ware for social networks
which co-locates data related to each user within the same
DC to minimize access latency.
[44] achieves this by having
a master-slave arrangement for each data item, creating enough
slave replicas, and updating them in an eventually consistent
fashion. However, master-slave solutions are susceptible to
issues related to data loss, and temporary downtime (see Sec-
tion II). In contrast, we consider a strict quorum requirement,
and allow updates on any replica.
Owing to consistency constraints, quorum placement
is
different from facility location (FL) problems, and known
variants [45]. The classical version of FL seeks to pick a subset
of facilities (DCs) that would minimize the distance costs (sum
of distances from each demand point to its nearest facility),
plus the opening costs of the facilities. Without opening cost
or capacity constraints, FL is trivial (a replica is introduced
at each demand point) – however quorum placement is still
complex. For e.g., in Figure 4, the optimal FL solution places
3 replicas at the triangle vertices which is twice the quorum
latency of our solution. Increasing the number of replicas can
hurt quorum latencies owing to consistency requirements, but
does not increase distance costs with FL.
Volley [12] addresses the problem of placing data consider-
ing both user locations and data inter-dependencies. However,
[12] does not address replication in depth, simply treating
replicas as different items that communicate frequently.
[12]
does not model consistency requirements, a key focus of our
work. Also, unlike [12], our models automatically determine
the number of replicas and quorum parameters while consid-
ering important practical aspects like latency percentiles and
performance under failures.
While systems like Spanner [24] and Walter [48] support
ﬂexible replication policies, they require these policies to be
manually conﬁgured by administrators. In contrast, our formu-
lations enable quorum based datastores to make these replica
conﬁguration decisions in an automated and optimal fashion.
Recent works like Vivace [21] suggest novel read/write algo-
rithms that employ network prioritization which enable geo-
replicated datastores adapt to network congestion. Unlike these
249249249
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:03:40 UTC from IEEE Xplore.  Restrictions apply. 
systems, we focus on the more general and important prob-
lem of automatically conﬁguring the replication parameters
including the number of replicas, location of replicas and quo-
rum sizes. SPANStore[52] focuses on placing replicas across
multiple cloud providers with the primary aim of minimizing
costs exploiting differential provider pricing. In contrast, we
focus on supporting ﬂexible replication policies at different
granularities that can be tuned to a variety of objectives such as
minimizing latencies under failure. Also, the quorum protocol
implemented by SPANStore is different from the ones used in
quorum based systems like Cassandra, and hence our model
formulations are different. [46] proposes algorithms extending
scalable deferred update replication (SDUR) in the context of
geographically replicated systems. In contrast, we focus on the
orthogonal problem of conﬁguring optimal replication policies
for geo-distributed datastores.
While there has been much theoretical analysis of quorum
protocols, our work is distinguished by our focus on widely
used quorum datastores, and issues unique to datastore set-
tings. Prior work has considered communication delays with
quorum protocols [28], [50], [43]. In particular,
[28], [50]
consider problems that minimize the maximum node delays.
However, none of these works optimize latency percentiles,
latency under failures, or consider different priorities for read
and write trafﬁc. To our knowledge, our framework is the ﬁrst
to consider these factors, all of which are key considerations
for geo-distributed datastores. We also note that
[28], [50],
[43] are in the context of coteries [29], and do not immediately
apply to cloud datastores which are adapted from weighted
voting-based quorum protocols [30].
Several works have examined availability in quorum con-
struction [16], [13], [35], [42], [20]. Most of these works
do not consider the impact of failures on latency. Recent
work [42] has considered how to dynamically adapt quorums
to changes in network delays. Given that systems like Cassan-
dra and Dynamo contact all replicas and not just the quorum,
we focus on the orthogonal problem of replica selection so
that failure of one DC does not impact latency. Several early
works [16], [13] assume independent identically distributed
(IID) failures, though non-IID failures are beginning to receive
attention [35]. Instead, we focus on choosing replication
strategies that are resilient and low-latency under failures of
a single DC, or a small subset of DCs which are prone to
correlated failures (Section VI-B).
XI. DISCUSSION AND IMPLICATIONS
We discuss the implications of our ﬁndings:
Implications for datastore design: Our results in Sec-
tion IX-B show the importance of diverse replica conﬁgura-
tions for the same application given heterogeneity in work-
loads for different groups of items – 1985 distinct replica
conﬁgurations were required for Twitter. Many geo-replicated
datastores are not explicitly designed with this requirement in
mind and may need to revisit their design decisions. For e.g.,
Eiger [40] replicates all data items in the same set of DCs.
Cassandra [38] and Dynamo [33] use consistent hashing which
makes it difﬁcult to ﬂexibly map replicas to desirable DCs (we
effectively bypass consistent hashing with multiple keyspaces
in Section VIII). In contrast, Spanner [24] explicitly maintain
directories that list locations of each group of items, and is thus
better positioned to support heterogeneous replication policies.
Delay variation: Our multi-region EC2 evaluations (Sec-
tion VIII) and simulation results (Section IX-D) show that
placements based on median delays observed over several
hours of measurement are fairly robust to short-term delay
variations. We believe delay variation impacts placement mod-
estly since links with lower median delay also tend to see
smaller variations. These results indicate that the beneﬁts of
explicitly modeling stochasticity in delay is likely small, and
these beneﬁts must be weighed against the fact that stochastic
delay values are hard to quantify in practice especially when
not independent. Further, we note that placements from our
N-1C model can tolerate congestion close to any DC. Finally,
more persistent variations in delay over longer time-scales are
best handled by recomputing placements on a periodic basis
or on a prolonged change in network delays.
Workload variation: Section IX-C shows that for many
applications, the optimal solution based on historical access
patterns performs well compared to the solution obtained with
perfect information of future access patterns. Consider the case
where workloads exhibit seasonal patterns (for e.g. diurnal
effects) and data-migration costs over short time-scales are
large enough that one chooses to maintain same replicas across
the seasons. Then, our models optimize placement assuming a
percentage of total requests across seasons are satisﬁed within
the speciﬁed latency. Instead, if one wants to have a certain
service level for each season, our models may be extended
by replicating the model for each season and imposing the
constraint that placement decisions are season independent. Fi-
nally, we also evaluated our models with placement recompu-
tations performed at different time granularities. We found that
daily, weekly and monthly recomputations perform similarly,
while hourly recomputation beneﬁts a modest fraction(15%)
of requests but incurs higher migration overheads. Hence,
recomputation at coarser granularities seems to be the more
appropriate choice.
Computational Complexity: Our optimization framework al-
lows a systematic approach to analyzing replication strategies
in cloud datastores, and delivers insights on the best latency
achievable for a given workload with consistency constraints.
With our prototype implementation LAT, BA, and N-1C mod-
els solve within 0.16, 0.17 and 0.41 seconds respectively using
a single core on a 4 core, 3GHz, 8GB RAM machines.
While already promising, we note that (i) our implementation
is not optimized. Many opportunities (heuristics, valid cuts,
modeling interface) exist for better efﬁciency; (ii) systems like
Spanner [24] require applications to bucket items, and com-
putations would be performed at coarser bucket granularities;
(iii) our per-bucket formulations are embarrassingly parallel;
and (iv) our placements are stable over days (Sec IX-C) and
placement recomputations are not frequent.
250250250
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:03:40 UTC from IEEE Xplore.  Restrictions apply. 
XII. CONCLUSIONS
In this paper, we make several contributions. First, we have
developed a systematic framework for modeling geo-replicated
quorum datastores in a manner that captures their latency,
availability and consistency requirements. Our frameworks
capture requirements on both read and write latencies, and
their relative priority. Second, we have demonstrated the
feasibility and importance of tailoring geo-distributed cloud
datastores to meet the unique workloads of groups of items
in individual applications, so latency SLA requirements (ex-
pressed in percentiles) can be met during normal operations
and on the failure of a DC. Third, we explore the limits
on latency achievable with geo-replicated storage systems for
three real applications under strict quorum requirement. Our
evaluations on a multi-region EC2 test-bed, and longitudinal
workloads of three widely deployed applications validate our
models, and conﬁrm their importance.
[19] N. Bronson et al. Tao: Facebook’s distributed data store for the social
graph. In USENIX ATC, 2013.
[20] S. Y. Cheung et al. Optimizing vote and quorum assignments for reading
and writing replicated data. In Proc. of the ICDE, 1989.
[21] B. Cho and M. K. Aguilera. Surviving congestion in geo-distributed
storage systems. In Proc. of USENIX ATC, 2012.
[22] E. Cho et al. Friendship and mobility: user movement in location-based
social networks. In Proceedings of the SIGKDD, 2011.
[23] B. F. Cooper et al. Pnuts: Yahoo!’s hosted data serving platform.
In
Proceedings of the VLDB, 2008.
[24] J. C. Corbett et al. Spanner:google’s globally-distributed database. In
Proceedings of the OSDI, 2012.
[25] DataStax.
Planning an Amazon EC2 cluster.
http://www.datastax.
com/documentation/cassandra/1.2/webhelp/cassandra/architecture/
architecturePlanningEC2 c.html.
[26] R. Escriva, B. Wong, and E. G. Sirer. Hyperdex:a searchable distributed
key-value store. In Proc. SIGCOMM, 2012.
[27] D. Ford et al. Availability in globally distributed storage systems. In
Proc. of OSDI, 2010.
[28] A. W. Fu. Delay-optimal quorum consensus for distributed systems.
IEEE Transactions on Parallel and Distributed Systems, 8(1), 1997.
[29] H. Garcia-molina and D. Barbara. How to assign votes in a distributed
system. Journal of the Association for Computing Machinery, 1985.
[30] D. K. Gifford. Weighted voting for replicated data.
In Proc. SOSP,
XIII. ACKNOWLEDGMENTS
1979.
This work was supported in part by National Science
Foundation (NSF) Career Award No. 0953622 and Award No.
1162333, Google and NetApp. Any opinions, ﬁndings and
conclusions or recommendations expressed in this material
are those of the authors and do not necessarily reﬂect the
views of NSF, Google or NetApp. We would also like to thank
Anis Elgabli for his inputs and help with the experiments and
evaluation.
REFERENCES
[1] Aws
edge
locations.
http://aws.amazon.com/about-aws/
globalinfrastructure/.
[2] Facebook’s master slave data storage. http://www.facebook.com/note.
php?note id=23844338919.
[3] Geocoding in ArcGIS. http://geocode.arcgis.com/arcgis/index.html.
[4] Google app engine - transactions across datacenters. http://www.google.
com/events/io/2009/sessions/TransactionsAcrossDatacenters.html.
[5] Google
groups
for App
Engine Downtime Notiﬁcation.
https://groups.google.com/forum/?fromgroups=#!forum/google-
appengine-downtime-notify.
[6] IBM ILOG CPLEX.
http://www-01.ibm.com/software/integration/
optimization/cplex/.
[7] Latency - it costs you. http://highscalability.com/latency-everywhere-
and-it-costs-you-sales-how-crush-it.
[8] More 9s please: Under the covers of the high replication datas-
http://www.google.com/events/io/2011/sessions/more-9s-please-
tore.
under-the-covers-of-the-high-replication-datastore.html.
[9] Stanford large network dataset collection. http://snap.stanford.edu/data/
loc-gowalla.html.
[10] Twitter application uses key-value data store. http://engineering.twitter.
com/2010/07/cassandra-at-twitter-today.html.
[11] Wikimedia statistics. http://stats.wikimedia.org/.
[12] S. Agarwal et al. Volley:automated data placement for geo-distributed
cloud services. In Proc. NSDI, 2010.
[13] Y. Amir and A. Wool. Evaluating quorum systems over the internet. In
Proc. of FTCS, pages 26–35, 1996.
[14] P. Bailis et al. Probabilistically bounded staleness for practical partial
quorums. In Proc. VLDB, 2012.
[15] J. Baker et al. Megastore:providing scalable, highly available storage
for interactive services. In Proc. CIDR, 2011.
[16] D. Barbara and H. Garcia-Molina. The reliability of voting mechanisms.
IEEE Transactions on Computers, 36(10), October 1987.
[17] A. I. Barvinok. A course in convexity. American Mathematical Society,
2002.
[18] J. Bisschop et al. On the development of a general algebraic modeling
system in a strategic planning environment. Applications, 1982.
251251251
[31] S. Gilbert and N. Lynch. Brewer’s conjecture and the feasibility of
consistent, available, partition-tolerant web services. ACM SIGACT
Newsletter, 2002.
[32] P. Gill et al. Understanding network failures in data centers: Measure-
ment, analysis, and implications. In Proc. of SIGCOMM, 2011.
[33] D. Hastorun et al. Dynamo: amazons highly available key-value store.
In Proc. SOSP, 2007.
[34] James Hamilton.
Redundancy.
InterDatacenterReplicationGeoRedundancy.aspx.
Inter-Datacenter Replication
and Geo-
http://perspectives.mvdirona.com/2010/05/10/
[35] F. Junqueira and K. Marzullo. Coterie availability in sites.
In Proc.
DISC, 2005.
[36] U. G. Knight. Power Systems in Emergencies: From Contingency
Planning to Crisis Management. John Wiley & Sons, LTD, 2001.
[37] T. Kraska et al. Mdcc: Multi-data center consistency. In Proceedings of
the 8th ACM European Conference on Computer Systems. ACM, 2013.
[38] A. Lakshman and P. Malik. Cassandra:a decentralized structured storage
system. ACM SIGOPS Operating Systems Review, 44:35–40, 2010.
[39] R. Li et al. Towards social user proﬁling: uniﬁed and discriminative
inﬂuence model for inferring home locations. In KDD, 2012.
[40] W. Lloyd et al.
Stronger semantics for low-latency geo-replicated
storage. NSDI 2013.
[41] W. Lloyd et al. Don’t settle for eventual: Scalable causal consistency
for wide-area storage with cops. In Proc. SOSP, 2011.
[42] M. M.G., F. Oprea, and M. Reiter. When and how to change quorums
on wide area networks. In Proc. SRDS, 2009.
[43] F. Oprea and M. K. Reiter. Minimizing response time for quorum-system
protocols over wide-area networks. In Proc. DSN, 2007.
[44] J. M. Pujol et al. The little engine(s) that could: Scaling online social
networks. In Proc. SIGCOMM, 2010.
[45] L. Qiu et al. On the placement of web server replicas. In Proceedings
of IEEE INFOCOM 2001.
[46] D. Sciascia and F. Pedone. Geo-replicated storage with scalable deferred
update replication. In IEEE/IFIP DSN, 2013.
[47] P. N. Shankaranarayanan et al. Balancing latency and availability in geo-
distributed cloud data stores. Purdue University ECE Technical Reports
TR-ECE-13-03, 2013.
[48] Y. Sovran et al. Transactional storage for geo-replicated systems.
In
Proc. of SOSP. ACM, 2011.
[49] A. Su et al. Drafting behind Akamai. SIGCOMM 2006.
[50] T. Tsuchiya et al. Minimizing the maximum delay for reaching
IEEE TPDS,
consensus in quorum-based mutual exclusion schemes.
1999.
[51] W. Vogels.
Eventually consistent. Communications of
the ACM,
52(1):40–44, 2009.
[52] Z. Wu et al. Cost-effective geo-replicated storage spanning multiple
cloud services. In Proc. of SOSP, 2013.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:03:40 UTC from IEEE Xplore.  Restrictions apply.