### Impact on Read Latency

The impact on read latency was notable, with a more pronounced tail. Interestingly, we observed that setting a bound of 50 milliseconds increased the read latency by less than 20 milliseconds for 60% of the keys. Constraints on write latency led to configurations with a significantly higher replication factor and larger read quorum sizes. This is expected, as our models aim to minimize latency by placing replicas closer to the write locations to meet the constraints. Due to space limitations, some results are not included.

### Related Work

**SPAR [44]** presents middleware for social networks that co-locates user-related data within the same data center (DC) to minimize access latency. SPAR achieves this through a master-slave arrangement, creating sufficient slave replicas and updating them in an eventually consistent manner. However, master-slave solutions are susceptible to data loss and temporary downtime (see Section II). In contrast, our approach enforces strict quorum requirements and allows updates on any replica.

**Quorum Placement vs. Facility Location (FL) Problems:**
Consistency constraints make quorum placement different from facility location (FL) problems and their known variants [45]. The classical FL problem aims to select a subset of facilities (DCs) that minimize distance costs (sum of distances from each demand point to its nearest facility) plus the opening costs of the facilities. Without opening costs or capacity constraints, FL is trivial (a replica at each demand point), but quorum placement remains complex. For example, in Figure 4, the optimal FL solution places three replicas at the triangle vertices, which is twice the quorum latency of our solution. Increasing the number of replicas can increase quorum latencies due to consistency requirements, but does not affect distance costs in FL.

**Volley [12]:**
Addresses the problem of placing data considering both user locations and data inter-dependencies. However, Volley does not delve deeply into replication, treating replicas as frequently communicating items. It also does not model consistency requirements, a key focus of our work. Unlike Volley, our models automatically determine the number of replicas and quorum parameters while considering practical aspects like latency percentiles and performance under failures.

**Spanner [24] and Walter [48]:**
Support flexible replication policies but require manual configuration by administrators. Our formulations enable quorum-based datastores to make these decisions automatically and optimally.

**Vivace [21]:**
Proposes novel read/write algorithms using network prioritization to adapt to congestion. Unlike Vivace, we focus on the general problem of automatically configuring replication parameters, including the number of replicas, their locations, and quorum sizes.

**SPANStore [52]:**
Focuses on placing replicas across multiple cloud providers to minimize costs by exploiting differential provider pricing. We, however, focus on supporting flexible replication policies at different granularities, tuned to objectives such as minimizing latencies under failure. Additionally, the quorum protocol used in SPANStore differs from those in systems like Cassandra, leading to different model formulations.

**SDUR [46]:**
Extends scalable deferred update replication in geographically replicated systems. In contrast, we focus on configuring optimal replication policies for geo-distributed datastores.

**Quorum Protocols:**
While there has been theoretical analysis of quorum protocols, our work focuses on widely used quorum datastores and issues unique to datastore settings. Prior work [28], [50], [43] has considered communication delays with quorum protocols, but none optimize latency percentiles, latency under failures, or consider different priorities for read and write traffic. To our knowledge, our framework is the first to address these factors, all of which are crucial for geo-distributed datastores. These prior works are in the context of coteries [29] and do not directly apply to cloud datastores, which use weighted voting-based quorum protocols [30].

**Availability in Quorum Construction:**
Several works [16], [13], [35], [42], [20] have examined availability in quorum construction. Most do not consider the impact of failures on latency. Recent work [42] considers dynamically adapting quorums to changes in network delays. Given that systems like Cassandra and Dynamo contact all replicas and not just the quorum, we focus on selecting replicas so that the failure of one DC does not impact latency. Early works [16], [13] assume independent identically distributed (IID) failures, though non-IID failures are now receiving attention [35]. We focus on choosing replication strategies resilient and low-latency under the failure of a single DC or a small subset of DCs prone to correlated failures (Section VI-B).

### Discussion and Implications

**Implications for Datastore Design:**
Our results in Section IX-B highlight the importance of diverse replica configurations for the same application, given the heterogeneity in workloads for different groups of items. For instance, 1985 distinct replica configurations were required for Twitter. Many geo-replicated datastores are not designed with this requirement in mind and may need to revisit their design decisions. Eiger [40] replicates all data items in the same set of DCs, while Cassandra [38] and Dynamo [33] use consistent hashing, making it difficult to flexibly map replicas to desirable DCs. Spanner [24] maintains directories listing the locations of each group of items, better supporting heterogeneous replication policies.

**Delay Variation:**
Our multi-region EC2 evaluations (Section VIII) and simulation results (Section IX-D) show that placements based on median delays over several hours are robust to short-term delay variations. Links with lower median delay tend to see smaller variations, indicating that the benefits of explicitly modeling stochasticity in delay are likely small. These benefits must be weighed against the difficulty of quantifying stochastic delay values, especially when they are not independent. Our N-1C model placements can tolerate congestion near any DC. Persistent delay variations over longer time-scales are best handled by periodically recomputing placements or adjusting to prolonged changes in network delays.

**Workload Variation:**
Section IX-C shows that for many applications, the optimal solution based on historical access patterns performs well compared to the solution with perfect future access pattern information. For seasonal workloads (e.g., diurnal effects), if data-migration costs over short time-scales are high, maintaining the same replicas across seasons is preferable. Our models optimize placement assuming a percentage of total requests across seasons are satisfied within the specified latency. If a certain service level is desired for each season, our models can be extended by replicating the model for each season and imposing season-independent placement constraints. Evaluations with different re-computation granularities show that daily, weekly, and monthly re-computations perform similarly, while hourly re-computation benefits a modest fraction (15%) of requests but incurs higher migration overheads. Thus, coarser re-computation granularities seem more appropriate.

**Computational Complexity:**
Our optimization framework provides a systematic approach to analyzing replication strategies in cloud datastores and delivers insights on the best achievable latency for a given workload with consistency constraints. Using a single core on a 4-core, 3GHz, 8GB RAM machine, our prototype implementation solves LAT, BA, and N-1C models within 0.16, 0.17, and 0.41 seconds, respectively. While promising, our implementation is not optimized, and there are opportunities for improvement (heuristics, valid cuts, modeling interface). Systems like Spanner [24] require applications to bucket items, and computations would be performed at coarser bucket granularities. Our per-bucket formulations are embarrassingly parallel, and our placements are stable over days (Section IX-C), reducing the frequency of re-computations.

### Conclusions

In this paper, we make several contributions:
1. **Systematic Framework:** We developed a systematic framework for modeling geo-replicated quorum datastores, capturing their latency, availability, and consistency requirements. Our frameworks address both read and write latencies and their relative priorities.
2. **Tailored Solutions:** We demonstrated the feasibility and importance of tailoring geo-distributed cloud datastores to meet the unique workloads of groups of items in individual applications, ensuring latency SLA requirements (expressed in percentiles) are met during normal operations and on the failure of a DC.
3. **Latency Limits:** We explored the limits on latency achievable with geo-replicated storage systems for three real applications under strict quorum requirements. Evaluations on a multi-region EC2 test-bed and longitudinal workloads of three widely deployed applications validate our models and confirm their importance.

### Acknowledgments

This work was supported in part by the National Science Foundation (NSF) Career Award No. 0953622 and Award No. 1162333, Google, and NetApp. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of NSF, Google, or NetApp. We would also like to thank Anis Elgabli for his inputs and help with the experiments and evaluation.

### References

[References are listed as provided, with no changes made.]