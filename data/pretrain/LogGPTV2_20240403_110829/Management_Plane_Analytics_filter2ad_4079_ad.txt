### Values Satisfying Significance Threshold

Values that meet our significance threshold alter both network health and intra-device complexity in a manner that makes the statistical similarity between intra-device complexity and network health more pronounced. Table 8 illustrates the p-values for comparisons between the upper bins of the same 10 practices. We observe that over one-third of the matchings exhibit poor quality (i.e., strong imbalance), and most of the remaining have large p-values. This issue primarily arises from the heavy-tailed distribution of management practice metrics. For example, when the treatment practice is the number of devices, 81% of cases fall into the first bin, and only 8% fall into the second bin. This distribution results in a limited number of cases available for matched pairs in the 2:3, 3:4, and 4:5 comparison points. The only solution to this problem is to obtain more diverse data from additional networks.

### Acting on the Results

The ability to modify practices that negatively impact network health varies depending on the type of practice and the organization's needs. Changing design practices, such as the number of models or roles, requires deploying new networks or significantly overhauling existing ones. In contrast, operational practices can be adjusted more easily, such as by aggregating or reducing changes through better planning of network reconfigurations. Some practices may be challenging to change due to workload demands (e.g., the number of devices). However, understanding these relationships can still benefit operators in setting service level objectives (SLOs) or making staffing decisions.

## 6. PREDICTING NETWORK HEALTH

We now turn to the second goal of MPA: developing models that use a network’s current management practices as input to predict its health. These models are valuable for network operators to explore how adjustments in management practices will likely affect network health. For instance, they can determine whether combining configuration changes into fewer, larger changes will improve network health.

Basic learning algorithms, such as C4.5 [27], produce mediocre models due to the skewed nature of management practices and health outcomes. Specifically, they overfit for the majority of healthy network cases. To address this, we developed schemes to learn more robust models. Our findings show that we can predict network health at a coarse granularity (i.e., healthy vs. unhealthy) with 91% accuracy. Finer-grained predictions (i.e., on a scale of 1 to 5) are less accurate due to insufficient samples.

### 6.1 Building an Organization’s Model

Given all data from an organization, what is the best model we can construct? An intuitive starting point is support vector machines (SVMs), which construct hyperplanes in high-dimensional space, similar to using logistic regression for propensity score formulas during causal analysis. However, SVMs performed worse than a simple majority classifier due to unhealthy cases being concentrated in a small part of the management practice space.

To better capture these unhealthy cases, we turned to decision tree classifiers (C4.5 algorithm [27]). Decision trees are better equipped to model the limited set of unhealthy cases because they can define arbitrary boundaries between cases. Additionally, they are more interpretable for operators.

#### Methodology

Before learning, we bin the data as described in Section 5.1.1, but we use only 5 bins for each management practice instead of 10, due to the insufficient amount of data for fine-grained models. For network health, we use either 2 bins or 5 bins. Two bins (classes) allow us to differentiate coarsely between healthy (≤1 tickets) and unhealthy networks, while five bins capture more granular health classes—excellent, good, moderate, poor, and very poor (≤2, 3–5, 6–8, 9–11, and ≥12 tickets, respectively). As standard practice, we prune the decision tree to avoid overfitting: branches with fewer data points than a threshold α are replaced with a leaf whose label is the majority class among the data points reaching that leaf. We set α = 1% of all data.

#### Model Validation

We measure the accuracy, precision, and recall of the decision trees using 5-fold cross-validation. Accuracy is the mean fraction of test examples correctly classified. Precision measures the fraction of data points predicted as class C that actually belong to class C, while recall measures the fraction of data points belonging to class C that are correctly predicted as class C.

Our 2-class model performs very well, with an accuracy of 91.6%. A majority class predictor has a significantly lower accuracy of 64.8%. The decision tree also shows high precision and recall for the healthy class (0.92 and 0.98, respectively) and moderate precision and recall for the unhealthy class (0.62 and 0.31, respectively). A majority class predictor has moderate precision (0.64) for the healthy class and no precision or recall for the unhealthy class.

The accuracy for a 5-class model is 81.1%, but the precision and recall for intermediate classes (good, moderate, and poor) are very low (Figure 8). The root cause is data skew: a majority of the samples represent the "excellent health" case (73%), with far fewer samples in other health classes (e.g., the poor class has just 2.3% of the samples). Our 5-class decision tree overfits for the majority class.

### Addressing Skew

Given that networks are generally healthy, data skew is a fundamental challenge for predictive models in MPA, especially for fine-grained health classes. To address skew and improve the accuracy of minority classes, we use two techniques from the machine learning community: boosting (AdaBoost [12]) and oversampling.

AdaBoost helps improve the accuracy of "weak" learners by iteratively adjusting the weights of examples. Oversampling directly addresses skew by repeating minority class examples during training. For a 2-class model, we replicate samples from the unhealthy class twice. For a 5-class model, we replicate samples from the poor class twice and the moderate and good classes thrice.

The results from applying these enhancements are shown in Figure 8. AdaBoost provides minor improvements for all classes, while oversampling significantly improves the precision and recall for the three intermediate health classes and slightly reduces the recall for the extreme classes (excellent and very poor). Using both oversampling and AdaBoost offers the best overall performance across all classes.

The final 5-class model is substantially better than a traditional decision tree but remains sub-optimal due to significant data skew. Separating nearby classes, such as excellent and good, requires many more real data points from either class. Thus, lack of data may be a key barrier to MPA's ability to model network health at fine granularity. Nonetheless, we have shown that good models can be constructed for coarse-grained prediction.

### 6.2 Using an Organization’s Model

Operators can use an organization’s model to determine which combinations of management practices lead to a (un)healthy network and to evaluate future network health based on specific management practices.

#### Tree Structure

Figure 10(a) shows a portion of the best 5-class tree. Decision trees are built by recursively selecting the node with the highest mutual information. The management practice with the strongest statistical dependence—number of devices—is the root of the tree. At the second level, two of the three practices are not among the top 10 most statistically dependent practices (Table 3). This indicates that the importance of some management practices depends on the nature of other practices. For example, when the number of devices is medium or low, the number of roles is a stronger determinant of health than the number of change events. Examining the paths from the decision tree's root to its leaves provides valuable insights into which combinations of management practices lead to (un)healthy networks. The same observations apply to the 2-class tree (Figure 10(b)).

#### Predicting Future Health

We demonstrate that an organization’s model can accurately predict future network health. We build decision trees using data points from M months (t - M to t - 1) and then use management practice metrics from month t to predict the health of each network in month t. The accuracy for month t is the fraction of networks whose health was correctly predicted.

Table 9 shows the average accuracy for M = 1, 3, 6, and 9 for values of t between February and October 2014. A 2-class model consistently achieves high prediction accuracy of 89%, regardless of the amount of prior data used to train the model. This trend is due to having less severe skew between the majority and minority classes when using two classes (Figure 9(a)). The prediction accuracy of a 5-class model reaches 78% for M = 9. Accuracy improves with a longer history of training data, but the relative improvement diminishes as the amount of training data increases. Thus, a reasonably accurate prediction of network health can be made with less than a year’s worth of data.

| M (months) | 1 | 3 | 6 | 9 |
|------------|---|---|---|---|
| 5 classes   | 0.734 | 0.756 | 0.779 | 0.779 |
| 2 classes   | 0.881 | 0.893 | 0.901 | 0.903 |

## 7. DISCUSSION

### Generality

While our observations for the OSP’s networks provide valuable insights into the relationship between management practices and network health, the statistical dependencies and causal relationships may not apply to all organizations. Differences in network types, workloads, and other organization-specific factors may affect these relationships. Nonetheless, our techniques are generally applicable, and any organization can run our tool [2] to discover these relationships for its networks.

### Intent of Management Practices

The metrics we infer (Section 2.2) quantify management practices in terms of their direct influence on the data and control planes. However, we could also quantify management actions in terms of their intent or the goal an operator is trying to achieve. By analyzing the relationships between intent and network health, we can gain a richer understanding of problematic practices. Unfortunately, inferring intent from network data sources is challenging and part of our ongoing work.

## 8. RELATED WORK

An earlier version of this work [4] introduced the idea of management plane analytics and provided visual evidence of correlations between a few management practices and network health. This paper considers many more practices, conducts causal analyses, and shows how to build accurate predictive models.

Establishing, following, and refining management practices is an important part of IT service management. ITIL [1] provides guidance on service design, service transition, and continual service improvement. While some general metrics used in ITIL (e.g., number of changes) are also used in MPA, MPA considers many networking-specific metrics, making it a valuable tool for continual service improvement. The major steps in MPA—defining metrics, characterizing practices, and uncovering relationships between practices and health—are similar to those in security management [17], but MPA focuses more on causality and relationship modeling.

A few prior studies have examined network management practices. Kim et al. [20] study several design and operational issues in two campus networks. Others have looked at more narrow aspects of design and operations, such as Benson et al. [5, 6] and Garimella et al. [13, 23]. In contrast, we examine a more comprehensive set of design and operational practices and tie our observations to network health.

Prior work has also examined network health in detail, such as Turner et al. [37, 38, 39]. However, these studies do not link network issues back to design and operational practices. QEDs have been widely studied, and our use of QEDs is inspired by recent network measurement studies [21, 22]. While these works use exact matching, we use nearest neighbor matching of propensity scores due to the large number of confounding factors in the management plane (Section 5.2).

MPA is inspired by empirical software engineering, which has helped improve software quality and reduce bugs [7]. We expect similar positive impacts from MPA.

## 9. CONCLUSION

We presented a management plane analytics framework for analyzing and improving network management practices. A systematic analysis of the management plane is necessary given the diversity in prevalent management practices and operator opinions. It is also feasible by analyzing data from many networks using carefully selected techniques. We found that the nature of network management data necessitates the use of propensity scores to reduce data dimensionality and facilitate matched design quasi-experiments. Additionally, oversampling and boosting are necessary for building good predictive models in the face of heavily skewed data. Our application of MPA to networks of a large OSP revealed intriguing insights, such as the moderate impact of ACL modifications on network health despite a majority opinion that the impact is low.

However, we have only scratched the surface of such analysis. There are many open issues, including studying other health metrics using MPA, extending MPA to apply across organizations, and developing tools for inferring management practices from outside a network. We believe this is a rich avenue for future research.

## 10. ACKNOWLEDGMENTS

We thank the operators of the online service provider for their suggestions and feedback, as well as the operators who took the time to answer our survey. We also thank our shepherd Anja Feldmann and the anonymous reviewers for their insightful comments. This work is supported by the Wisconsin Institute on Software-defined Datacenters of Madison and National Science Foundation grants CNS-1302041, CNS-1330308, and CNS-1345249. Aaron Gember-Jacobson is supported by an IBM PhD Fellowship.

## 11. REFERENCES

[1] ITIL – IT service management. http://www.axelos.com/best-practice-solutions/itil.
[2] Management plane analytics tool. http://cs.wisc.edu/~agember/go/mpa.
[3] NIST/SEMATECH e-handbook of statistical methods. http://itl.nist.gov/div898/handbook.
[4] A. Akella and R. Mahajan. A call to arms for management plane analytics. In IMC, 2014.
[5] T. Benson, A. Akella, and D. Maltz. Unraveling complexity in network management. In NSDI, 2009.
[6] T. Benson, A. Akella, and A. Shaikh. Demystifying configuration challenges and trade-offs in network-based ISP services. In SIGCOMM, 2011.
[7] C. Bird, B. Murphy, N. Nagappan, and T. Zimmermann. Empirical software engineering at Microsoft Research. In Computer Supported Cooperative Work (CSCW), 2011.
[8] L. Breiman. Random forests. Machine learning, 45(1):5–32, 2001.
[9] P. Comon. Independent component analysis, a new concept? Signal Processing, 36(3):287–314, Apr. 1994.
[10] A. B. Downey. Using pathchar to estimate internet link characteristics. In SIGCOMM, 1999.
[11] A. Fogel, S. Fung, L. Pedrosa, M. Walraed-Sullivan, R. Govindan, R. Mahajan, and T. Millstein. A general approach to network configuration analysis. In NSDI, 2015.
[12] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. J. Computer and System Sciences, 55(1):119–139, Aug. 1997.
[13] P. Garimella, Y. Sung, N. Zhang, and S. Rao. Characterizing VLAN usage in an Operational Network. In SIGCOMM Workshop on