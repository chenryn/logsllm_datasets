values satisfy our signiﬁcance threshold
change both network health and intra-device complexity in a way
that makes intra-device complexity statistically similar to health.
Table 8 shows the p-value for the comparison between the upper
bins for the same 10 practices. We observe that over one-third of
the matchings have poor quality (i.e., strong imbalance), and most
of the others have large p-values. This primarily stems from man-
agement practice metrics following a heavy-tailed distribution. For
example, when the treatment practice is number of devices, 81%
of cases fall into the ﬁrst bin and 8% fall into the second bin; this
means there are few cases from which to select matched pairs for
the 2:3, 3:4, and 4:5 comparison points. The only way to address
this issue is to obtain (more diverse) data from more networks.
Acting on the Results. The ability to change practices that cause
poor network health varies based on the class of practice and the
needs of the organization. Changing design practices (e.g., num-
ber of models or roles) requires deploying new networks, or sig-
niﬁcantly overhauling existing networks. In contrast, operational
practices can be adjusted more easily: e.g., changes can be aggre-
gated or reduced by more carefully planning network reconﬁgura-
tions. Some practices may be difﬁcult to change due to workload
demands (e.g., number of devices), but operators can still bene-
ﬁt from understanding these relationships to aid in setting service
level objectives (SLOs) or making stafﬁng decisions.
6. PREDICTING NETWORK HEALTH
We now move on to the second goal of MPA: building models
that take a network’s current management practices as input and
predict the network’s health. Such models are useful for network
operators to explore how adjustments in management practices will
likely impact network health: e.g., will combining conﬁguration
changes into fewer, larger changes improve network health?
We ﬁnd that basic learning algorithms (e.g., C4.5 [27]) produce
mediocre models because of the skewed nature of management
practices and health outcomes. In particular, they over-ﬁt for the
403majority healthy network case. Thus, we develop schemes to learn
more robust models despite this limitation. We show that we can
predict network health at coarse granularity (i.e., healthy vs. un-
healthy) with 91% accuracy; ﬁner-grained predictions (i.e., a scale
of 1 to 5) are less accurate due to a lack of sufﬁcient samples.
6.1 Building an Organization’s Model
We start with the following question: given all data from an or-
ganization, what is the best model we can construct?
An intuitive place to start is support vector machines (SVMs).
SVMs construct a set of hyperplanes in high-dimensional space,
similar to using logistic regression to construct propensity score
formulas during causal analysis. However, we found the SVMs
performed worse than a simple majority classiﬁer. This is due to
unhealthy cases being concentrated in a small part of the manage-
ment practice space.
To better learn these unhealthy cases, we turn to decision tree
classiﬁers (the C4.5 algorithm [27]). Decision trees are better
equipped to capture the limited set of unhealthy cases, because they
can model arbitrary boundaries between cases. Furthermore, they
are intuitive for operators to understand.
Methodology. Prior to learning, we bin data as described in Section
5.1.1. However, we use only 5 bins for each management practice
(instead of 10), because the amount of data we have is insufﬁcient
to accurately learn ﬁne-grained models. For network health, we use
either 2 bins or 5 bins; two bins (classes) enables us to differenti-
ate coarsely between healthy (≤1 tickets) and unhealthy networks,
while ﬁve bins captures more ﬁne-grained classes of health—excel-
lent, good, moderate, poor, and very poor (≤2, 3–5, 6–8, 9–11, and
≥12 tickets, respectively). As is standard practice, we prune a de-
cision tree to avoid over-ﬁtting: each branch where the number of
data points reaching this branch is below a threshold α is replaced
with a leaf whose label is the majority class among the data points
reaching that leaf. We set α=1% of all data.
Model Validation. We measure the accuracy, precision, and recall
of the decision trees using 5-fold cross validation. Accuracy is the
mean fraction of test examples whose class is predicted correctly.
For a given class C, precision measures what fraction of the data
points that were predicted as class C actually belong to class C,
while recall measures what fraction of the data points that belong
to class C are correctly predicted as class C.
We ﬁnd that a 2-class model performs very well. Accuracy of
the pruned decision tree—i.e., the mean fraction of test examples
whose class is predicted correctly—is 91.6%.
In comparison, a
majority class predictor has a signiﬁcantly worse accuracy: 64.8%.
Furthermore, the decision tree has very high precision and recall for
the healthy class (0.92 and 0.98, respectively), and moderate preci-
sion and recall for the unhealthy class (0.62 and 0.31, respectively).
A majority class predictor has only moderate precision (0.64) for
the healthy class and no precision or recall for the unhealthy class.
The accuracy for a 5-class model is 81.1%, but the precision and
recall for the intermediate classes (good, moderate, and poor) are
very low (DT bars in Figure 8). The root cause here is skew in the
data: as shown in Figure 9(b), a majority of the samples represent
the “excellent health” case (73%), with far fewer samples in other
health classes (e.g., the poor class has just 2.3% of the samples).
Our 5-class decision tree ends up overﬁtting for the majority class.
Addressing Skew. Because networks are generally healthy, such
skew in data is a fundamental challenge that predictive models in
MPA need to address, especially when attempting to predict ﬁne-
grained health classes. To address skew and improve the accu-
racy of our models for minority classes, we borrow two techniques
1.0
0.8
0.6
0.4
i
i
n
o
s
c
e
r
P
DT
DT+AB
DT+OS
DT+AB+OS
1.0
0.8
0.6
0.4
l
l
a
c
e
R
DT
DT+AB
DT+OS
DT+AB+OS
0.2
0.2
0.0
E xcelle nt
G o o d
M o d erate
P o or
V ery P o or
0.0
E xcelle nt
G o o d
M o d erate
P o or
V ery P o or
(a) Precision
(b) Recall
Figure 8: Accuracy of 5 class models (DT=standard decision
tree learning algorithm, AB=AdaBoost, OS=oversampling)
6K
6K
i
s
t
n
o
P
a
t
a
D
f
o
#
5K
4K
3K
2K
1K
s
t
i
n
o
P
a
t
a
D
f
o
#
5K
4K
3K
2K
1K
0
U n h e althy
H e althy
0
E xcelle nt
G o o d
M o d erate
P o or
V ery P o or
(a) 2 classes
(b) 5 classes
Figure 9: Health class distribution
from the machine learning community: boosting (speciﬁcally, Ad-
aBoost [12]) and oversampling.2
AdaBoost helps improve the accuracy of “weak” learners. Over
many iterations (we use 15) AdaBoost increases (decreases) the
weight of examples that were classiﬁed incorrectly (correctly) by
the learner; the ﬁnal learner (i.e., decision tree) is built from the last
iteration’s weighted examples. Oversampling directly addresses
skew as it repeats the minority class examples during training. When
building a 2-class model we replicate samples from the unhealthy
class twice, and when building a 5-class model we replicate sam-
ples from the poor class twice and the moderate and good classes
thrice.
The results from applying these enhancements are shown in Fig-
ure 8. We observe that AdaBoost results in minor improvement for
all classes. In contrast, using oversampling signiﬁcantly improves
the precision and recall for the three intermediate health classes,
and causes a slight drop in the recall for the two extreme classes
(excellent and very poor). Using oversampling and AdaBoost in
combination offers the best overall performance across all classes.
The ﬁnal 5-class model is substantially better than using a tra-
ditional decision tree. However, it is still sub-optimal due to the
signiﬁcant skew in the underlying dataset. Separating apart a pair
of nearby classes whose class boundaries are very close—e.g., ex-
cellent and good—requires many more real data points from either
class; oversampling can only help so much. Thus, lack of data may
pose a key barrier to MPA’s ability to model network health at ﬁne
granularity. Nonetheless, we have shown that good models can be
constructed for coarse grained prediction.
2We also experimented with random forests [8, 19]; neither bal-
anced [8] nor weighted random forests [19] improve the accuracy
for the minority classes beyond the improvements we are already
able to achieve with boosting and oversampling.
404(a) 5-class
(b) 2-class
Figure 10: Decision trees (only a portion is shown)
6.2 Using an Organization’s Model
Operators can use an organization’s model to determine which
combinations of management practices lead to an (un)healthy net-
work, and to evaluate how healthy a network will be in the future
when a speciﬁc set of management practices are applied.
Tree Structure. Figure 10(a) shows a portion of the best 5-class
tree. Since decision trees are built by recursively selecting the node
with the highest mutual information, the management practice with
the strongest statistical dependence (identiﬁed in Section 5.1)—
number
of
devices—is the root of the tree. In the second level, however, two
of the three practices are not present in our list of the 10 most sta-
tistically dependent practices (Table 3). This shows that the impor-
tance of some management practices depends on the nature of other
practices: e.g., when the number of devices is medium or low, the
number of roles is a stronger determinant of health than the num-
ber of change events. Thus, examining the paths from the decision
tree’s root to its leaves provides valuable insights into which com-
binations of management practices lead to an (un)healthy network.
The same observations apply to the 2-class tree (Figure 10(b)).
Predicting Future Health. We now show that an organization’s
model can accurately predict the future health of an organization’s
networks. In particular, we build decision trees using data points
from M months (t − M to t − 1). Then, we use management
practice metrics from month t to predict the health of each network
in month t. The accuracy for month t is the fraction of networks
whose health was correctly predicted.
Table 9 shows the average accuracy for M =1, 3, 6, and 9 for
values of t between February and October 2014. We observe that a
2-class model has consistently high prediction accuracy of 89% ir-
respective of the amount of prior data used to train the model. This
trend primarily stems from having less severe skew between the
majority and minority classes when using two classes (Figure 9(a)).
The prediction accuracy of a 5-class model reaches 78% for
M = 9. Also, accuracy improves with a longer history of training
data: using 9 months, rather than 1 month, of training data results
in a 5% increase in accuracy. However, as the amount of train-
ing data increases (i.e., increasing M ) the relative improvement in
accuracy diminishes. Thus, a reasonably accurate prediction of net-
work health can be made with less than a year’s worth of data.
M (months)
1
3
6
9
5 classes
0.734
0.756
0.779
0.779
2 classes
0.881
0.893
0.901
0.903
Table 9: Accuracy of future health predictions
7. DISCUSSION
Generality. While the observations we make for the OSP’s net-
works provide a valuable perspective on the relationship between
management practices and network health, the statistical depen-
dencies and causal relationships we uncover may not apply to all
organizations. Differences in network types (e.g., data center vs.
wide area networks), workloads, and other organization-speciﬁc
factors may affect these relationships. Nonetheless, our techniques
are likely generally applicable, and any organization can run our
tool [2] to discover these relationships for its networks.
Intent of Management Practices. The metrics we infer (Sec-
tion 2.2) quantify management practices in terms of their direct
inﬂuence on the data and control planes: e.g., how heterogeneous
is network hardware, and which conﬁguration stanzas are changed.
However, we could also quantify management actions in terms of
their intent, or the goal an operator is trying to achieve: e.g., an op-
erator may want to reduce ﬁrmware licensing costs, so they design
a network to use RIP rather than OSPF [5]. By analyzing the rela-
tionships between intent and network health, we can gain a richer
understanding of what practices are the most problematic. Unfor-
tunately, intent is much more difﬁcult to infer from network data
sources (Section 2.1), and doing so is part of our ongoing work.
8. RELATED WORK
An earlier version of this work [4] introduced the idea of man-
agement plane analytics and provided visual evidence of correla-
tions between a few management practices and network health.
This paper considers many more practices, conducts causal anal-
yses, and shows how to build accurate predictive models.
Establishing, following, and reﬁning management practices is an
important part of information technology (IT) service management.
ITIL [1] provides guidance on: service design, which focuses on
health-related concerns such as availability and service levels; ser-
vice transition, which focuses on change, conﬁguration, and de-
ployment management; and continual service improvement. Some
of the general metrics used in ITIL to asses the health of an IT orga-
nization (e.g., number of changes) are also used in MPA, but MPA
also considers may networking-speciﬁc metrics. This makes MPA
a valuable tool for continual service improvement. The major steps
in MPA—deﬁning metrics, characterizing practices, and uncover-
ing relationships between practices and health—are similar to the
steps employed in security management [17], but MPA’s analyses
are focused more on causality and relationship modeling.
A few prior studies have examined network management prac-
tices. Kim et al. study several design and operational issues in two
campus network: e.g., how network-wide conﬁguration size grows
over time, what causes this growth, how conﬁgurations of different
device types (e.g., router, ﬁrewall, etc.) change and why, and the
qualitative differences among the campuses in these aspects [20].
Others have looked at more narrow aspects of design and opera-
tions: e.g., Benson et al. examine conﬁguration complexity in 7
enterprise networks [5] and study design and change patterns of
various network-based services of a large ISP [6]; Garimella et al.
and Krothapalli et al. study VLAN usage in a single campus net-
work [13, 23]. In contrast to these prior works, we examine a much
more comprehensive set of design and operational practices. Also,
No. of Devices No. of change events Inter-device complexity No. of Roles No. of roles Medium … … … Inter-device complexity Good Excellent Very poor Poor Excellent Moderate High … Frac. devices changed Excellent Very low No. of Devices No. of change events Inter-device complexity No. of Models No. of roles Medium … … … Excellent High,  Very high Low, Medium Intra-device complexity High Excellent Unhealthy Unhealthy Unhealthy 405by virtue of studying many networks of a large OSP, we are able to
provide a unique view into the variation in management practices.
Finally, none of the prior studies tie their the observations to health.
Prior work has also examined network health in great detail. For
example, Turner et al. use device logs, network probes and incident
reports to understand causes and frequency of failures and their im-
pact [37]. Failure in data center networks, and of middleboxes in
data centers have been studied by Gill et al. [14] and Navendu et
al. [25], respectively. Turner et al. examine how to combine var-
ious data sources to obtain a better view of failures and their root
causes [38, 39]. However, these studies don’t link network issues
back to design and operational practices. That said, some of the
data sources and techniques considered in these approaches could
be valuable to deriving better network health metrics that could
then improve MPA.
QEDs have been widely studied. Stuart [32] provides a com-
prehensive survey of the various techniques employed in matched
design experiments. Our use of QEDs is inspired by recent network
measurement studies focused on video streaming quality [21] and
video ad placement [22]. While these works use exact matching in
their QEDs, we use nearest neighbor matching of propensity scores,
because exact matching cannot accommodate the large number of
confounding factors in the management plane (Section 5.2).
MPA is inspired by how research into software engineering prac-
tices, also called “empirical software engineering,” has helped im-
prove the quality of software and reduced the number of bugs [7].
We expect similar positive impact from MPA.
9. CONCLUSION
We presented a management plane analytics framework for ana-
lyzing and improving network management practices. We showed
that a systematic analysis of the management plane is: (i) neces-
sary—given the diversity in prevalent management practices and
in opinions among operators regarding what matters versus not;
and (ii) feasible—by analyzing data from many networks using
carefully selected techniques. We found that the nature of net-
work management data necessitates the use of propensity scores
to reduce data dimensionality and facilitate matched design quasi-
experiments that identify causal relationships between practices and
network health. Additionally, we showed that oversampling and
boosting are necessary for building good predictive models in the
face of heavily skewed data. Our application of MPA to networks
of a large OSP revealed intriguing insights: e.g., the fraction of
changes where an ACL is modiﬁed has a moderately high impact
on network health despite a majority opinion that the impact is low,
and the fraction of change events affecting a middlebox has low
impact on health despite the belief that the impact is high.
However, we have just scratched the surface of such analysis.
There are many issues left open by our work, including: studying
other health metrics using MPA, determining how to extend MPA to
apply across organizations, and developing tools for inferring man-
agement practices from outside a network (akin to probing tools
such as trace-route) as opposed to analyzing internally-collected
data. We believe this is a rich avenue for future research.
10. ACKNOWLEDGMENTS
We thank the operators of the online service provider for their
suggestions and feedback, as well as the operators who took the
time to answer our survey. We also thank our shepherd Anja Feld-
mann and the anonymous reviewers for their insightful comments.
This work is supported by the Wisconsin Institute on Software-
deﬁned Datacenters of Madison and National Science Foundation
grants CNS-1302041, CNS-1330308, and CNS-1345249. Aaron
Gember-Jacobson is supported by an IBM PhD Fellowship.
11. REFERENCES
[1] ITIL – IT service management. http:
//www.axelos.com/best-practice-solutions/itil.
[2] Management plane analytics tool.
http://cs.wisc.edu/~agember/go/mpa.
[3] NIST/SEMATECH e-handbook of statistical methods.
http://itl.nist.gov/div898/handbook.
[4] A. Akella and R. Mahajan. A call to arms for management plane
analytics. In IMC, 2014.
[5] T. Benson, A. Akella, and D. Maltz. Unraveling complexity in
network management. In NSDI, 2009.
[6] T. Benson, A. Akella, and A. Shaikh. Demystifying conﬁguration
challenges and trade-offs in network-based ISP services. In
SIGCOMM, 2011.
[7] C. Bird, B. Murphy, N. Nagappan, and T. Zimmermann. Empirical
software engineering at Microsoft Research. In Computer Supported
Cooperative Work (CSCW), 2011.
[8] L. Breiman. Random forests. Machine learning, 45(1):5–32, 2001.
[9] P. Comon. Independent component analysis, a new concept? Signal
Processing, 36(3):287–314, Apr. 1994.
[10] A. B. Downey. Using pathchar to estimate internet link
characteristics. In SIGCOMM, 1999.
[11] A. Fogel, S. Fung, L. Pedrosa, M. Walraed-Sullivan, R. Govindan,
R. Mahajan, and T. Millstein. A general approach to network
conﬁguration analysis. In NSDI, 2015.
[12] Y. Freund and R. E. Schapire. A decision-theoretic generalization of
on-line learning and an application to boosting. J. Computer and
System Sciences, 55(1):119–139, Aug. 1997.
[13] P. Garimella, Y. Sung, N. Zhang, and S. Rao. Characterizing VLAN
usage in an Operational Network. In SIGCOMM Workshop on