script injection were not notiﬁed by XSSAuditor.
Figure 5 reports the number of false positives for each ﬁl-
ter. Note that while XSSAuditor and XSSFilt oﬀer roughly
the same compatibility, the latter protects against a wider
range of threats. Also, as previously explained, NoScript’s
false positives tend to cause greater disruption for the user.
7.5 Performance Evaluation
The performance evaluation is focused on XSSFilt only.
XSSAuditor’s performance has already been evaluated in
Reference [3], while NoScript overhead is trivially low, since
it only examines one URL per request, unlike XSSFilt and
XSSAuditor which have to perform checks for each script
contained in the page.
Unfortunately, calculating the overhead imposed by the
ﬁlter is a signiﬁcant challenge. This is because XSSFilt con-
tains many optimizations that bypass policy enforcement if
parameters do not contain special characters, if they are too
short or if an external script is fetched on the same origin of
the page.
The Mozilla framework includes two performance tests:
tp4 is an automated test that can be run on patches to the
mozilla codebase, to estimate the overhead that these can
impose on the browser. The test estimates the time required
to load a set of predeﬁned pages that are saved locally to
produce consistent results over time. The overhead esti-
mated by tp4 for XSSFilt is negligle. Unfortunately, since
tp4 fetches homepages saved locally, it overestimates the
eﬀect of the ﬁlter’s optimizations: requests don’t have pa-
rameters to check, and all external scripts are from the same
origin.
To produce more meaningful results, we used pageloader,
the Firefox extension that is used internall by tp4 to mea-
sure load times. Instead of using the standard set of local
pages used by tp4, we loaded 20 times a custom set of 120
URLs to be fetched remotely that also included many GET
parameters2. We used an aggressive caching proxy to factor
out network delay while transparently providing pageloader
with remote resources. This way, we can avoid overestimat-
ing the speedup due to XSSFilt’s optimizations. This test
showed an overhead of 2.5%.
However, even though the dataset clearly triggered many
XSSFilt checks, the ﬁgure is not necessarily representative
of the overhead normally experienced by users, because this
2The set of URLs can be found at http://pastebin.com/
kYqas9ae.
is a) heavily dependent on the amount of parameters in web
applications and b) ultimately diluted when factoring in the
delay involved with fetching a webpage oﬀ the network. For
this reason, we used proﬁling data available from an ordi-
nary user session consisting of 3000 unique pages. Since a
web browser is a multithreaded execution environment, the
overhead cannot be estimated by simply timing the calls to
XSSFilt: the same call will take longer if the user is simul-
taneously watching a video on YouTube on a diﬀerent tab.
Therefore, the proﬁler logged the actual parameters of XSS
checks, and given that the only expensive operation is ap-
proximate substring matching, we can perform the approx-
imate string matching computations oﬄine to estimate the
time spent during XSS checks for each page load. This yields
an average overhead of 0.5%, which shows that the overhead
is almost negligible when factoring in network latency.
8. RELATED WORK
Server-Side Approaches: Earlier works in XSS defense
have been mostly in the form of server-side defenses. Given
that the vulnerability exists solely on the server-side, this is
natural. Moreover, techniques based on taint-tracking can
be naturally applied on the server-side.
XSSDS [12] describes two server-side approaches: a re-
ﬂected XSS ﬁlter based on string matching and a generic
XSS ﬁlter that builds a whitelist of scripts during a training
phase. Similar to IE8, the reﬂected ﬁlter compares input
parameters and HTML output to look for untrusted input
in scripts. However, it is a network-based ﬁlter, unable to
detect DOM-based attacks. It leverages the Firefox parser,
which is able to defeat browser quirks against Firefox clients;
however, this parser comes with a higher overhead and can-
not reliably handle quirks from other clients.
[29] uses string matching and taint-aware policies to stop
generic injection attacks (SQL injection, XSS, Shell Injec-
tion). However, it uses heavyweight, precise taint-tracking.
This limits its applicability to the server side, and it has
the drawbacks associated with taint-tracking: high overhead
and possible loss of reliability. Their policies are based on
syntactic conﬁnement: tainted tokens of sensitive operations
should not span multiple syntactical constructs.
[21] is a server-side defense against injection attacks (SQL
injection, XSS, Shell Injection). It oﬀers server-side protec-
tion through library interposition, using taint-inference and
taint-aware policies. An exclusively server-side protection
can only be a static, network-based defense. Therefore, it
cannot protect against DOM-based attacks. It also suﬀers
from false positives and negatives as a result of parsing diﬀer-
ences with a browser. Moreover, sanitization of the detected
attacks is not discussed.
XSS-GUARD [4] uses the program transformation ap-
proach also found in Candid [2] to detect which scripts are
intended by the web application, inferring a whitelist for
each request before sending out the response to the client:
the application is instrumented to build an alternative re-
sponse along with the ordinary one; instead of using HTTP
parameters to assemble the response, the alternative appli-
cation logic uses dummy inputs. Once both responses have
been built, XSS-GUARD checks that every script present
in the real page is also present in the alternative page. Al-
though XSS-GUARD is a server-side defense, the idea of
sending a dummy request along with the original request
for XSS protection has been used on the client-side as well
by [9].
Blueprint [13] is a server side defense which converts the
untrusted HTML embedded in a page into JavaScript code.
The purpose of this transformation is to ﬁx the browser’s in-
terpretation of the page at the server-side, adding JavaScript
code to reliably reconstruct the parse tree once the page is
rendered by the browser. This sidesteps one of the biggest
issues with XSS, which is browsers’ leniency towards errors,
which opens many chances to overcome input sanitization.
With this technique, the interpretation of untrusted input
can be determined by the server, which also decides what
type of content is allowed for each piece of untrusted infor-
mation.
Client-Side Approaches: Client-side approaches pro-
tect users against XSS vulnerabilities without waiting for
websites to ﬁx them. Two major browsers now ship with an
XSS ﬁlter.
Internet Explorer 8 [19] comes with built-in XSS protec-
tion. IE8’s approach also uses the idea of matching inputs
and outputs, but does so in a more simplistic way: from in-
puts, regular expressions of possibly malicious injections are
created using heuristics, compiled and matched against the
HTML output. IE8’s goal is to provide a usable protection
for ordinary users, thwarting basic attacks (which constitute
the vast majority of attacks in the wild) without incurring
false positives. Their regular expressions have been shown
to be insuﬃcient for protection:
[16] shows that in case of
an XSS vulnerability there are many ways to bypass the ﬁl-
ter. Moreover, their choice of sanitization technique opens
up further holes, which have been described in [17]. Finally,
in spite of its implementation within the browser, it oper-
ates like a network-based ﬁlter, and hence does not detect
DOM-based attacks.
XSSAuditor [3] is the name of the XSS ﬁlter integrated
into Webkit (and consequently Google Chrome). This ﬁl-
ter proposes a new architecture, which is only possible on
browser defenses:
instead of interposing on the network
data, this solution interposes on the JavaScript engine inter-
face. This approach has many advantages, the most impor-
tant being interposing on all requests of script evaluation,
defeating browser quirks and unusual attack vectors. XSS-
Filt too enjoys this advantage. However, unlike XSSFilt,
XSSAuditor relies on exact string matching, and hence can
miss attacks due to application-speciﬁc sanitizations. More-
over, XSSAuditor does not detect partial script injections.
NoScript [7] is a popular Firefox plugin which allows users
to execute JavaScript only on trusted websites manually
added to a whitelist. NoScript also includes an XSS ﬁl-
ter: like IE8, it relies on regular expressions on parameters.
However, regular expressions are used to extract and identify
malicious data from the URL; NoScript does not actually
check if malicious data is actually present in the response,
but rather sanitizes the request before it is sent to the server.
Thus, it can suﬀer from a higher rate of false positives.
Hybrid Approaches: Hybrid approaches are an in-
teresting alternative:
they can enforce the policy on the
browser, defeating browser quirks, and rely on the server
to provide a policy or taint information. However, while the
two previous classes of defenses can be practically deployed,
these solutions require a critical mass of browsers and web-
sites willing to deploy the defense.
BEEP [11] is a hybrid defense framework which allows the
server to supply a policy for the page through a JavaScript
function. This function can interpose on script execution: it
receives the script content and its DOM node as arguments,
and can deny the script execution. Using the two argu-
ments provided to the hook, BEEP also provides two sample
policies. A whitelisting policies, where the web developers
checks every script with a set of known script hashes, and
a containment policy, where nodes can be prevented from
having script content in descendants.
DSI [15] and Noncespaces [27] protect against injection
attacks by providing an isolation primitive for HTTP. Us-
ing this primitive, the server can securely isolate untrusted
content and trasmit it to the browser along with the HTML
response. The browser can then refuse to execute untrusted
content. This combines the advantages of server-side de-
fenses with respect to identiﬁcation of untrusted content
(support for taint-tracking and developer annotation) and
of client-side defenses with respect to enforcement (immu-
nity to browser quirks, support for DOM-Based attacks).
Mozilla CSPs [23] is a feature added in Firefox 4.0 to sup-
port server-supplied content restrictions, to further limit the
resources that can be embedded in a web page. For each
content type, the web developer can specify a list of trusted
hosts allowed to provide content for the web page. These
policies can provide XSS protection by allowing scripts to be
served solely by servers under the control of the web applica-
tion. Unfortunately, inlined content (such as inline scripts)
cannot be considered same-origin:
it has to be considered
untrusted and cannot be executed.
Reference [28] presents a modiﬁcation to Firefox’s JavaScript
engine that prevents data leaks using ﬁne grained taint-
tracking, refusing to transfer sensitive information (e.g. cook-
ies) to third parties.
9. CONCLUSIONS
This paper presented a thorough study of two popular
XSS ﬁlters, NoScript and XSSAuditor, identifying their weak-
nesses and proposing a new ﬁlter, XSSFilt. Through ex-
tensive testing, we showed that XSSFilt covers more attack
vectors and is more resilent in case of string transformations
applied to reﬂected content.
10. REFERENCES
[1] D. Balzarotti, M. Cova, V. Felmetsger, N. Jovanovic,
E. Kirda, C. Kruegel, and G. Vigna. Saner:
Composing static and dynamic analysis to validate
sanitization in Web applications. In IEEE S&P, 2008.
[2] S. Bandhakavi, P. Bisht, P. Madhusudan, and
VN Venkatakrishnan. CANDID: preventing sql
injection attacks using dynamic candidate evaluations.
In Proceedings of the 14th ACM conference on
Computer and communications security, pages 12–24.
ACM, 2007.
[3] Daniel Bates, Adam Barth, and Collin Jackson.
Regular Expressions Considered Harmful in
Client-Side XSS Filters. In WWW, 2010.
[4] Prithvi Bisht and V.N. Venkatakrishnan. XSS-Guard:
Precise Dynamic Detection of Cross-Site Scripting
Attacks. Detection of Intrusions and Malware, and
Vulnerability Assessment, pages 23–43, 2008.
[5] P. De Ryck, Lieven Desmet, Thomas Heyman, Frank
Piessens, and W. Joosen. CsFire: Transparent
client-side mitigation of malicious cross-domain
requests. In ESSOS, 2010.
[24] Zhendong Su and Gary Wassermann. The essence of
command injection attacks in web applications. In
POPL, 2006.
[6] Kevin Fernandez and DP. XSSed. http://xssed.com/.
[25] The MITRE Corporation. 2011 CWE/SANS Top 25
Most Dangerous Programming Errors.
http://cwe.mitre.org/top25/.
[26] The Open Web Application Security Project
(OWASP). OWASP Top Ten Project.
http://www.owasp.org/index.php/Category:
OWASP_Top_Ten_Project, 2010.
[27] M. Van Gundy and H. Chen. Noncespaces: Using
randomization to enforce information ﬂow tracking
and thwart cross-site scripting attacks. In NDSS, 2009.
[28] P. Vogt, F. Nentwich, N. Jovanovic, E. Kirda,
C. Kruegel, and G. Vigna. Cross-site scripting
prevention with dynamic data tainting and static
analysis. In NDSS, 2007.
[29] W. Xu, S. Bhatkar, and R. Sekar. Taint-enhanced
policy enforcement: A practical approach to defeat a
wide range of attacks. In 15th USENIX Security
Symposium, pages 121–136, 2006.
[7] Giorgio Maone. NoScript. http://noscript.net/.
[8] Robert Hansen. XSS Cheat Sheet.
http://ha.ckers.org/xss.html.
[9] O. Ismail, M. Etoh, Y. Kadobayashi, and
S. Yamaguchi. A proposal and implementation of
automatic detection/collection system for cross-site
scripting vulnerability. In Proceedings of the 18th
International Conference on Advanced Information
Networking and Application (AINA04), 2004.
[10] Jeremias Reith. NoXSS. https://addons.mozilla.
org/en-US/firefox/addon/noxss/.
[11] T. Jim, N. Swamy, and M. Hicks. Defeating script
injection attacks with browser-enforced embedded
policies. In WWW. ACM, 2007.
[12] M. Johns, B. Engelmann, and J. Posegga. XSSDS:
Server-side Detection of Cross-site Scripting Attacks.
In ACSAC, 2008.
[13] M.T. Louw and VN Venkatakrishnan. Blueprint:
Robust prevention of cross-site scripting attacks for
existing browsers. In IEEE S&P, 2009.
[14] Mozilla Corporation. Add-On SDK. https://addons.
mozilla.org/en-US/developers/docs/sdk/1.2/,
2010.
[15] Y. Nadji, P. Saxena, and D. Song. Document structure
integrity: A robust basis for cross-site scripting
defense. In NDSS, 2009.
[16] Eduardo Vela Nava and David Lindsay. Our favorite
xss ﬁlters/ids and how to attack them. Black Hat USA
2009.
[17] Eduardo Vela Nava and David Lindsay. Universal xss
via ie8’s xss ﬁlters. Black Hat Europe 2010.
[18] Riccardo Pelizzi, Tung Tran, and Alireza Saberi.
Large-Scale, Automated XSS Detection using Google
Dorks.
http://www.cs.sunysb.edu/~rpelizzi/gdorktr.pdf,
2011.
[19] David Ross. IE 8 XSS Filter
Architecture/Implementation. http:
//blogs.technet.com/srd/archive/2008/08/19/
ie-8-xss-filter-architecture-implementation.
aspx.
[20] Jerome H. Saltzer and Michael D. Schroeder. The
Protection of Information in Computer Systems. In
SOSP. ACM, 1974.
[21] R. Sekar. An eﬃcient black-box technique for defeating
web application attacks. In NDSS, 2009.
[22] Sirdackat. Hacking NoScript. http://sirdarckcat.
blogspot.com/2008/06/hacking-noscript.html,
2010.
[23] Sid Stamm, Brandon Sterne, and Gervase Markham.
Reining in the web with content security policy. In
WWW, 2010.