• Tends to focus on input validation errors;
• Tends to focus on actual applications and dynamic testing of a finished
product;
• Tends to ignore the responses, or valid behavior;
• Concentrates mostly on testing interfaces that have security implications.
In this section, we’ll look at different kinds of testing and auditing of software
from a tester’s perspective. We will start with identifying how much you need to test
(and fuzz) based on your needs. We will then define what a testing target is and fol-
low that up with some descriptions of different kinds of testing as well as where
fuzzing fits in with these definitions. Finally, we will contrast fuzzing with more
traditional security measures in software development such as code auditing.
1.2.1
Cost-Benefit of Quality
From a quality assurance standpoint, it is vital to understand the benefits from
defect elimination and test automation. One useful study was released in January
2001, when Boehm and Basili reviewed and updated their list of metrics on the ben-
efits of proactive defect elimination. Their software defect reduction “Top 10” list
includes the following items:9
1. Finding and fixing a software problem after delivery is often 100 times
more expensive than finding and fixing it during the requirements and
design phase.
2. Current software projects spend about 40% to 50% of their effort on
avoidable rework.
3. About 80% of avoidable rework comes from 20% of the defects.
14
Introduction
9Victor R. Basili, Barry Boehm. “Software defect reduction top 10 list.” Computer (January
2001): 135–137.
4. About 80% of the defects come from 20% of the modules, and about half
of the modules are defect free.
5. About 90% of the downtime comes from, at most, 10% of the defects.
6. Peer reviews catch 60% of the defects.
7. Perspective-based reviews catch 35% more defects than nondirected reviews.
8. Disciplined personal practices can reduce defect introduction rates by up to
70%.
9. All other things being equal, it costs 50% more per source instruction
to develop high-dependability software products than to develop low-
dependability software products. However, the investment is more than
worth it if the project involves significant operations and maintenance costs.
10. About 40% to 50% of users’ programs contain nontrivial defects.
Although this list was built from the perspective of code auditing and peer
review (we all know that those are necessary), the same applies to security testing.
If you review each point above from a security perspective, you can see that all of
them apply to vulnerability analysis, and to some extent also to fuzzing. This is
because every individual security vulnerability is also a critical quality issue, because
any crash-level flaws that are known by people outside the development organiza-
tion have to be fixed immediately. The defects found by fuzzers lurk in an area that
current development methods such as peer reviews fail to find. These defects almost
always are found only after the product is released and someone (a third party) con-
ducts fuzz tests. Security is a subset of software quality and reliability, and the meth-
odologies that can find flaws later in the software life-cycle should be integrated to
earlier phases to reduce the total cost of software development.
The key questions to ask when considering the cost of fuzzing are the following.
1. What is the cost per defect with fuzzing? Some people argue that this met-
ric is irrelevant, because the cost per defect is always less than the cost of
a security compromise. These people recognize that there are always ben-
efits in fuzzing. Still, standard business calculations such as ROI (return on
investment) and TCO (total cost of ownership) are needed in most cases
also to justify investing in fuzzing.
2. What is the test coverage? Somehow you have to be able to gauge how well
your software is being tested and what proportion of all latent problems
are being discovered by introducing fuzzing into testing or auditing proc-
esses. Bad tests done with a bad fuzzer can be counterproductive, because
they waste valuable testing time without yielding any useful results. At worst
case, such tests will result in over-confidence in your product and arro-
gance against techniques that would improve your product.10 A solid fuzzer
with good recommendations and a publicly validated track record will
likely prove to be a better investment coverage-wise.
1.2
Software Quality
15
10We often hear comments like: “We do not need fuzzing because we do source code auditing”
or “We do not need this tool because we already use this tool,” without any consideration if they
are complementary products or not.
3. How much should you invest in fuzzing? The motivation for discussing the
price of fuzzing derives from the various options and solutions available in
the market. How can you compare different tools based on their price,
overall cost of usage, and testing efficiency? How can you compare the
total cost of purchasing an off-the-shelf commercial fuzzer to that of
adopting a free fuzzing framework and hiring people to design and imple-
ment effective tests from the ground up? Our experience in the market has
shown that the price of fuzzing tools is not usually the biggest issue in com-
parisons. In commercial fuzzing, the cheapest tools usually prove to be the
simplest ones—and also without exception the worst ones from a testing
coverage, efficiency, and professional testing support standpoint. Commer-
cial companies looking for fuzz testing typically want a fuzzer that (a) sup-
ports the interfaces they need to test, (b) can find as many issues as possible
in the systems they test, and (c) are able to provide good results within a
reasonable timeframe.
There will always be a place for both internally built tools and commercial
tools. A quick Python11 script might be better suited to fuzz a single isolated custom
application. But if you are testing a complex communication protocol implementa-
tion or a complete system with lots of different interfaces, you might be better off
buying a fuzzing tool from a commercial test vendor to save yourself a lot of time
and pain in implementation. Each option can also be used at different phases of an
assessment. A sample practice to analyze fuzzing needs is to
1. Conduct a QA risk analysis, and as part of that, possibly conduct necessary
ad-hoc tests;
2. Test your product thoroughly with a commercial testing tool;
3. Hire a professional security auditing firm to do a second check of the
results and methods.
1.2.2
Target of Test
In some forms of testing, the target of testing can be any “black box.” All various
types of functional tests can be directed at different kinds of test targets. The same
applies for fuzzing. A fuzzer can test any applications, whether they are running on
top of web, mobile, or VoIP infrastructure, or even when they are just standalone
software applications. The target of a test can be one single network service, or it
can be an entire network architecture. Common names used for test targets include
• SUT (system under test). An SUT can consist of several subsystems, or it can
represent an entire network architecture with various services running on top
of it. An SUT can be anything from banking infrastructure to a complex
telephony system. SUT is the most abstract definition of a test target, because
it can encompass any number of individual destinations for the tests.
16
Introduction
11We mention Python as an example script language due to the availability of PyDBG by Pedram
Amini. See PaiMei documentation for more details: http://pedram.redhive.com/PaiMei/docs
• DUT (device under test). A DUT is typically one single service or a piece of
equipment, possibly connected to a larger system. Device manufacturers
mainly use the term DUT. Some examples of DUTs include routers, WLAN
access points, VPN gateways, DSL modems, VoIP phones, web servers, or
mobile handsets.
• IUT (implementation under test). An IUT is one specific software implemen-
tation, typically the binary representation of the software. It can be a process
running on a standard operating system, or a web application or script run-
ning on an application server.
In this book, we will most often refer to a test target as an SUT, because this
term is applicable to all forms of test setups.
1.2.3
Testing Purposes
The main focus of fuzzing is on functional security assessment. As fuzzing is essen-
tially functional testing, it can be conducted in various steps during the overall
development and testing process. To a QA person, a test has to have a purpose, or
otherwise it is meaningless.12 Without a test purpose, it is difficult to assign a test
verdict—i.e., Did the test pass or fail? Various types of testing have different pur-
poses. Black-box testing today can be generalized to focus on three different
purposes (Figure 1.4). Positive testing can be divided into feature tests and perform-
ance tests. Test requirements for feature testing consist of a set of valid use cases,
which may consist of only few dozens or at most hundreds of tests. Performance
testing repeats one of the use cases using various means of test automation such as
record-and-playback. Negative testing tries to test the robustness of the system
through exploring the infinite amount of possible anomalous inputs to find the tests
that cause invalid behavior. An “anomaly” can be defined as any unexpected input
1.2
Software Quality
17
Figure 1.4
Testing purposes: features, performance, and robustness.
12Note that this strict attitude has changed lately with the increasing appreciation to agile testing
techniques. Agile testing can sometimes appear to outsiders as ad-hoc testing. Fuzzing has many
similarities to agile testing processes.
that deviates from the expected norm, ranging from simple field-level modifications
to completely broken message structures or alterations in message sequences. Let us
explore these testing categories in more detail.
Feature testing, or conformance testing, verifies that the software functions
according to predefined specifications. The features can have relevance to secu-
rity—for example, implementing security mechanisms such as encryption and data
verification. The test specification can be internal, or it can be based on industry
standards such as protocol specifications. A pass criterion simply means that accord-
ing to the test results, the software conforms to the specification. A fail criterion
means that a specific functionality was missing or the software operated against the
specification. Interoperability testing is a special type of feature test. In interoper-
ability testing, various products are tested against one another to see how the fea-
tures map to the generally accepted criteria. Interoperability testing is especially
important if the industry standards are not detailed enough to provide adequate
guidance for achieving interoperability. Most industry standards always leave some
features open to interpretation. Interoperability testing can be conducted at special
events sometimes termed plug-fests (or unplug-fests in the case of wireless protocols
such as Bluetooth).
Performance testing tests the performance limitations of the system, typically
consisting of positive testing only, meaning it will send large amounts of legal traf-
fic to the SUT. Performance is not only related to network performance, but can
also test local interfaces such as file systems or API calls. The security implications
are obvious: A system can exhibit denial-of-service when subjected to peak loads.
An example of this is distributed denial of service (DDoS) attacks. Another exam-
ple from the field of telephony is related to the “mothers’ day effect,” meaning that
a system should tolerate the unfortunate event when everyone tries to utilize it
simultaneously. Performance testing will measure the limits that result in denial of
service. Performance testing is often called load testing or stress testing, although
some make the distinction that performance testing attempts to prove that a system
can handle a specific amount of load (traffic, sessions, transactions, etc.), and that
stress testing investigates how the system behaves when it is taken over that limit.
In any case, the load used for performance testing can either be sequential or par-
allel—e.g., a number of requests can be handled in parallel, or within a specified
time frame. The acceptance criteria are predefined and can vary depending on the
deployment. Whereas another user can be happy with a performance result of 10
requests per second, another user could demand millions of processed requests per
minute. In failure situations, the system can crash, or there can be a degradation of
service where the service is denied for a subset of customers.
Robustness testing (including fuzzing) is complementary to both feature and
performance tests. Robustness can be defined as an ability to tolerate exceptional
inputs and stressful environmental conditions. Software is not robust if it fails when
facing such circumstances. Attackers can take advantage of robustness problems
and compromise the system running the software. Most security vulnerabilities
reported in the public are caused by robustness weaknesses.
Whereas both feature testing and performance testing are still positive tests, based
on real-life use cases, robustness testing is strictly negative testing with tests that
18
Introduction
should never occur in a well-behaving, friendly environment. For every use case in
feature testing, you can create a performance test by running that use case in parallel
or in rapid succession. Similarly, for every use case in feature testing, you can create
“misuse cases” by systematically or randomly breaking the legal and valid stimuli.
With negative testing, the pass-fail criteria are challenging to define. A fail cri-
terion is easier to define than a pass criterion. In robustness testing, you can define
that a test fails if the software crashes, becomes unstable, or does other unaccept-
able things. If nothing apparent seems to be at fault, the test has passed. Still, adding
more instrumentation and monitoring the system more closely can reveal uncaught
failures with exactly the same set of tests, thus revealing the vagueness of the used
pass-fail criteria. Fuzzing is one form of robustness testing, and it tries to fulfill the
testing requirements in negative testing with random or semi-random inputs (often
millions of test cases). But more often robustness testing is model-based and opti-
mized, resulting in better test results and shorter test execution time due to optimized
and intelligently targeted tests selected from the infinity of inputs needed in nega-
tive testing (Figure 1.5).
1.2.4
Structural Testing
Software rarely comes out as it was originally planned (Figure 1.6).13 The differ-
ences between the specification and the implementation are faults (defects, bugs,
vulnerabilities) of various types. A specification defines both positive and negative
requirements. A positive requirement says what the software should do, and a neg-
ative requirement defines what it must not do. The gray area in between leaves
some functionality undefined, open for interpretation. The implementation very
1.2
Software Quality
19
Figure 1.5
Limited input space in positive tests and the infinity of tests in negative testing.
13J. Eronen, and M.Laakso. (2005) “A Case for Protocol Dependency.” In Proceedings of the
First IEEE International Workshop on Critical Infrastructure Protection. Darmstadt, Germany.
November 3–4, 2005.
rarely represents the specification. The final product implements the acquired func-
tionality, with some of the planned features present and some of them missing (con-
formance faults). In addition to implementing (or not implementing) the positive
requirements, the final software typically implements some features that were
defined as negative requirements (often fatal or critical faults). Creative features
implemented during the software life cycle can either be desired or nondesired in the
final product.
Whereas all critical flaws can be considered security-critical, many security
problems also exist inside the set of creative features. One reason for this is that
those features very rarely will be tested even if fuzzing is part of the software devel-
opment life cycle. Testing plans are typically built based on a requirements spec-
ification. The reason for a vulnerability is typically a programming mistake or a
design flaw.
Typical security-related programming mistakes are very similar in all commu-
nication devices. Some examples include
• Inability to handle invalid lengths and indices;
• Inability to handle out-of-sequence or out-of-state messages;
• Inability to tolerate overflows (large packets or elements);
• Inability to tolerate missing elements or underflows.
Try to think of implementation mistakes as undesired features. Whereas a user-
name of eight characters has a feature of identifying users, nine characters can be
used to shut the service down. Not very applicable, is it? Implementation flaws
are often created due to vague definitions of how things should be implemented.
Security-related flaws are often created when a programmer is left with too much
choice when implementing a complex feature such as a security mechanism. If the
requirements specification does not define how authentication must exactly be
20
Introduction
Figure 1.6
Specification versus implementation.
implemented, or what type of encryption should be used, the programmers become
innovative. The result is almost always devastating.
1.2.5
Functional Testing
In contrast to structural testing disciplines, fuzzing falls into the category of functional
testing, which is more interested in how a system behaves in practice rather than in
the components or specifications from which it is built. The system under test during
functional testing can be viewed as a “black box,” with one or more external inter-
faces available for injecting test cases, but without any other information available on
the internals of the tested system. Having access to information such as source code,
design or implementation specifications, debugging or profiling hooks, logging out-
put, or details on the state of the system under test or its operational environment will
help in root cause analysis of any problems that are found, but none of this is strictly
necessary. Having any of the above information available turns the testing process
into “gray-box testing,” which has the potential to benefit from the best of both
worlds in structural as well as functional testing and can sometimes be recommended
for organizations that have access to source code or any other details of the systems
under test. Access to the internals can also be a distraction.
A few good ideas that can be used in conjunction with fuzz testing when source
code is available include focusing code auditing efforts on components or subsys-
tems in which fuzzing has already revealed some initial flaws (implying that the
whole component or portions of the code around the flaws might be also of simi-
larly poor quality) or using debuggers and profilers to catch more obscure issues
such as memory leaks during fuzz testing.
1.2.6
Code Auditing
“Use the source, Luke—if you have it!”
Anonymous security expert
Fuzzing is sometimes compared to code auditing and other white-box testing meth-
ods. Code auditing looks at the source code of a system in an attempt to discover
defective programming constructs or expressions. This falls into the category of
structural testing, looking at specifications or descriptions of a system in order to
detect errors. While code auditing is another valuable technique in the software
tester’s toolbox, code auditing and fuzzing are really complementary to each other.
Fuzzing focuses on finding some critical defects quickly, and the found errors are
usually very real. Fuzzing can also be performed without understanding the inner
workings of the tested system in detail. Code auditing is usually able to find more
problems, but it also finds more false positives that need to be manually verified by
an expert in the source code before they can be declared real, critical errors. The
choice of which technique fits your purposes and testing goals best is up to you.
With unlimited time and resources, both can be recommended. Neither fuzzing nor
code auditing is able to provably find all possible bugs and defects in a tested sys-
tem or program, but both of them are essential parts in building security into your
product development processes.
1.2
Software Quality
21
1.3
Fuzzing
So far we have discussed vulnerabilities and testing. It is time to finally look at the
real topic of this book, fuzzing.
1.3.1
Brief History of Fuzzing
Fuzzing is one technique for negative testing, and negative testing is nothing new in
the quality assurance field. Hardware testing decades ago already contained nega-
tive testing in many forms. The most traditional form of negative testing in hard-
ware is called fault injection. The term fault injection can actually refer to two
different things. Faults can be injected into the actual product, through mutation
testing, i.e., intentionally breaking the product to test the efficiency of the tests. Or
the faults can be injected to data, with the purpose of testing the data-processing
capability. Faults in hardware communication buses typically happen either through
random inputs—i.e., white-noise testing—or by systematically modifying the data—
e.g., by bit-flipping. In hardware, the tests are typically injected through data busses
or directly to the various pins on the chip. Most modern chips contain a test chan-
nel, which will enable modification of not only the external interfaces but injection
of anomalies in the data channels inside the chip.
Some software engineers used fuzzing-like test methods already in the 1980s.
One proof of that is a tool called The Monkey: “The Monkey was a small desk
accessory that used the journaling hooks to feed random events to the current appli-
cation, so the Macintosh seemed to be operated by an incredibly fast, somewhat
angry monkey, banging away at the mouse and keyboard, generating clicks and
drags at random positions with wild abandon.”14 However, in practice, software