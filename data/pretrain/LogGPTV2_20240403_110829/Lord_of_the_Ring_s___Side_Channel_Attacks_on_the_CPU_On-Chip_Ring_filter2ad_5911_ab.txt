(concurrently). Multicore approaches [20,23,38,39,48,51,62,
66,91,106–108,113] are the most generic with the victim and
1Other classiﬁcations exist, such as the historical one into storage or
timing channels [1], but our classiﬁcation is more useful for this paper.
the attacker running on separate cores. The attack presented
in this paper uses the multicore leakage channel.
2.2 Side Channel Defenses
Several defenses to microarchitectural side channels have
been proposed. We focus here on generic approaches. The
most straightforward approach to block a side channel is to
disable the sharing of the microarchitectural component on
which it relies. For example, attacks that rely on simultaneous
multithreading (SMT) can be thwarted by disabling SMT,
which is an increasingly common practice for both cloud
providers and end users [9, 18, 64]. Other approaches propose
to partition the shared resource to ensure that its use by the vic-
tim cannot be monitored by the attacker [53, 61, 89, 101, 115].
For example, Liu et al. [61] present a defense to multicore
cache attacks that uses Intel CAT [71] to load sensitive vic-
tim cache lines in a secure LLC partition where they can-
not be evicted by the attacker. Finally, for channels that
rely on preemptive scheduling and SMT, one mitigation ap-
proach is to erase the victim’s footprint from the microar-
chitectural state across context switches. For example, sev-
eral works proposed to ﬂush the CPU caches on context
switches [16, 30–32, 34, 40, 41, 74, 77, 89, 96, 114].
3 Reverse Engineering the Ring Interconnect
In this section, we set out to understand the microarchitectural
characteristics of the ring interconnect on modern Intel CPUs,
with a focus on the necessary and sufﬁcient conditions for
an adversary to create and monitor contention on it. This
information will serve as the primitive for our covert channel
(Section 4) and side channel attacks (Section 5).
Experimental Setup We run our experiments on two ma-
chines. The ﬁrst one uses an 8-core Intel Core i7-9700 (Coffee
Lake) CPU at 3.00GHz. The second one uses a 4-core Intel
Core i7-6700K (Skylake) CPU at 4.00GHz. Both CPUs have
an inclusive, set-associative LLC. The LLC has 16 ways and
2048 sets per slice on the Skylake CPU and 12 ways and 2048
USENIX Association
30th USENIX Security Symposium    647
506070Core 0Core 1Core 2Core 301234567Slice ID506070Core 401234567Slice IDCore 501234567Slice IDCore 601234567Slice IDCore 7sets per slice on the Coffee Lake CPU. Both CPUs have an
8-way L1 with 64 sets and a 4-way L2 with 1024 sets. We use
Ubuntu Server 16.04 with the kernel 4.15 for our experiments.
3.1
Inferring the Ring Topology
Monitoring the Ring Interconnect We build on prior
work [28] and create a monitoring program that measures,
from each core, the access time to different LLC slices. Let
WL1, WL2 and WLLC be the associativities of the L1, L2 and
LLC respectively. Given a core c, an LLC slice index s and
an LLC cache set index p, our program works as follows:
1. It pins itself to the given CPU core c.
2. It allocates a buffer of ≥ 400 MB of memory.2
3. It iterates through the buffer looking for WLLC addresses
that map to the desired slice s and LLC cache set p and
stores them into the monitoring set. The slice mapping
is computed using the approach from Maurice et al. [65],
which uses hardware performance counters. This step re-
quires root privileges, but we will discuss later how we can
compute the slice mapping also with unprivileged access.
4. It iterates through the buffer looking for WL1 addresses that
map to the same L1 and L2 cache sets as the addresses
of the monitoring set, but a different LLC cache set (i.e.,
where the LLC cache set index is not p) and stores them
into a set which we call the eviction set.
5. It performs a load of each address of the monitoring set. Af-
ter this step, all the addresses of the monitoring set should
hit in the LLC because their number is equal to WLLC.
Some of them will hit in the private caches as well.
6. It evicts the addresses of the monitoring set from the pri-
vate caches by accessing the addresses of the eviction set.
This trick is inspired by previous work [106] and ensures
that the addresses of the monitoring set are cached in the
LLC, but not in the private caches.
7. It times loads from the addresses of the monitoring set one
at a time using the timestamp counter (rdtsc) and records
the measured latencies. These loads will miss in the private
caches and hit in the LLC. Thus, they will need to travel
through the ring interconnect. Steps 6-7 are repeated as
needed to collect the desired number of latency samples.
Results We run our monitoring program on each CPU core
and collect 100,000 samples of the “load latency” from each
different LLC slice. The results for our Coffee Lake CPU are
plotted in Figure 2. The results for our Skylake CPU are in the
extended version [81]. These results conﬁrm that the logical
topology of the ring interconnect on both our CPUs matches
the linear topology shown in Figure 1. That is:
1. The LLC load latency is larger when the load has to
travel longer distances on the ring interconnect.
2We found 400 MB to be enough to contain the WLLC addresses of Step 2.
Once this topology and the respective load latencies are
known to the attacker, they will be able to map any addresses
to their slice by just timing how long it takes to access them
and comparing the latency with the results of Figure 2. As de-
scribed so far, monitoring latency narrows down the possible
slices from n to 2. To pinpoint the exact slice a line maps to,
the attacker can then triangulate from 2 cores. This does not
require root access. Prior work explores how this knowledge
can be used by attackers to reduce the cost of ﬁnding eviction
sets and by defenders to increase the number of colors in page
coloring [38, 109]. What else can an attacker do with this
knowledge? We investigate this question in the next section.
3.2 Understanding Contention on the Ring
We now set out to answer the question: under what circum-
stances can two processes contend on the ring interconnect?
To this end, we reverse engineer Intel’s “sophisticated ring
protocol” [57, 87] that handles communication on the ring
interconnect. We use two processes, a receiver and a sender.
Measuring Contention The receiver is an optimized ver-
sion of the monitoring program described in Section 3.1, that
skips Steps 4 and 6 (i.e., does not use an eviction set) thanks
to the following observation: since on our CPUs WLLC > WL1
and WLLC > WL2, not all the WLLC addresses of the monitoring
set can ﬁt in the L1 and L2 at any given time. For example,
on our Skylake machine, WLLC = 16 and WL2 = 4. Consider
the scenario when we access the ﬁrst 4 addresses of our mon-
itoring set. These addresses ﬁt in both the private caches and
the LLC. However, we observe that accessing one by one the
remaining 12 addresses of the monitoring set evicts the ﬁrst
4 addresses from the private caches. Hence, when we load
the ﬁrst addresses again at the next iteration, we still only hit
in the LLC. Using this trick, if we loop through the monitor-
ing set and access its addresses in order, we can always load
from the LLC. To ensure that the addresses are accessed in
order, we serialize the loads using pointer chasing, which is
a technique also used in prior work [36, 62, 94, 100]. Further,
to make it less likely to suffer LLC evictions due to external
noise, our receiver evenly distributes the WLLC addresses of
the monitoring set across two LLC cache sets (within the
same slice). Finally, to amplify the contention signal, our re-
ceiver times 4 sequential loads at a time instead of 1. The bulk
of our receiver’s code is shown in Listing 1 (in Appendix A).
Creating Contention The sender is designed to create con-
tention on speciﬁc segments on the ring interconnect by “bom-
barding” it with trafﬁc. This trafﬁc is sent from its core to
different CPU components which sit on the ring, such as LLC
slices and the system agent. To target a speciﬁc LLC slice, our
sender is based on the same code as the receiver. However,
it does not time nor serialize its loads. Further, to generate
more trafﬁc, it uses a larger monitoring set with 2 × WLLC
addresses (evenly distributed across two LLC cache sets). To
648    30th USENIX Security Symposium
USENIX Association
Figure 3: Ring interconnect contention heatmap when both the receiver and the sender perform loads that miss in their private
caches and hit in the LLC. The y axes indicate the core where the sender and the receiver run, and the x axes indicate the target
LLC slice from which they perform their loads. Cells with a star ((cid:70)) indicate slice contention (when Rs = Ss), while gray cells
indicate contention on the ring interconnect (with darker grays indicating larger amounts of contention).
target the system agent (SA), our sender uses an even larger
monitoring set with N > 2×WLLC addresses. Because not all
these N addresses will ﬁt in two LLC cache sets, these loads
will miss in the cache, causing the sender to communicate
with the memory controller (in the SA).
Data Collection We use the receiver and the sender to col-
lect data about ring contention. For the ﬁrst set of experiments,
we conﬁgure the sender to continuously load data from a sin-
gle LLC slice (without LLC misses). For the second set of
experiments, we conﬁgure the sender to always incur misses
on its loads from the target LLC slice. To prevent unintended
additional noise, we disable the prefetchers and conﬁgure the
sender and the receiver to target different cache sets so that
they do not interfere through traditional eviction-based attacks.
We refer to the sender’s core as Sc, the slice it targets as Ss,
the receiver’s core as Rc and the slice it targets as Rs. For both
core and slice numbering, we follow the diagram of Figure 1.
For every combination of Sc, Ss, Rc and Rs, we test if running
the sender and the receiver concurrently affects the load la-
tency measured by the receiver. We then compare the results
with a baseline, where the sender is disabled. We say that
USENIX Association
30th USENIX Security Symposium    649
01234567Receiver slice 0Sender slice:01234567Receiver core 0Sender core:01234567Receiver slice 1Sender slice:0123456701234567Receiver slice 2Sender slice:0123456701234567Receiver slice 3Sender slice:0123456701234567Receiver slice 4Sender slice:0123456701234567Receiver slice 5Sender slice:0123456701234567Receiver slice 6Sender slice:0123456701234567Receiver slice 7Sender slice:012345670123456701234567Receiver core 1Sender core:01234567012345670123456701234567012345670123456701234567012345670123456701234567012345670123456701234567012345670123456701234567Receiver core 2Sender core:01234567012345670123456701234567012345670123456701234567012345670123456701234567012345670123456701234567012345670123456701234567Receiver core 3Sender core:01234567012345670123456701234567012345670123456701234567012345670123456701234567012345670123456701234567012345670123456701234567Receiver core 4Sender core:01234567012345670123456701234567012345670123456701234567012345670123456701234567012345670123456701234567012345670123456701234567Receiver core 5Sender core:01234567012345670123456701234567012345670123456701234567012345670123456701234567012345670123456701234567012345670123456701234567Receiver core 6Sender core:01234567012345670123456701234567012345670123456701234567012345670123456701234567012345670123456701234567012345670123456701234567Receiver core 7Sender core:0123456701234567012345670123456701234567012345670123456701234567012345670123456701234567012345670123456701234567there is contention when the average load latency measured
by the receiver is larger than the baseline. Figure 3 shows the
results of our ﬁrst experiment, when the sender always hits in
the LLC. Figure 11 (in Appendix A.1) shows the results of
our second experiment, when the sender always misses in the
LLC. Both ﬁgures refer to our Coffee Lake machine. The re-
sults of our 4-core Skylake machine are a subset of the 8-core
Coffee Lake ones (with Rc  Ss), or vice versa.
The fact that loads in the right/left direction do not contend
with loads in the left/right direction conﬁrms that the ring has
two physical ﬂows, one for each direction (as per Figure 1).
This observation is supported by Intel in [57].
4. A ring stop can service cross-ring trafﬁc traveling on
opposite directions simultaneously.
Fourth, even when the sender and receiver’s loads travel
in the same direction, there is never contention if the ring
interconnect segments between Sc and Ss and between Rc and
Rs do not overlap. For example, when Rc = 2 and Rs = 5,
there is no contention if Sc = 0 and Ss = 2 or if Sc = 5 and
Ss = 7. This is because load trafﬁc on the ring interconnect
only travels through the shortest path between the ring stop of
the core and the ring stop of the slice. If the sender’s segment
does not overlap with the receiver’s segment, the receiver will
be able to use the full bus bandwidth on its segment.
5. Ring trafﬁc traveling through non-overlapping seg-
ments of the ring interconnect does not cause contention.
The above observations narrow us down to the cases when
the sender and the receiver perform loads in the same direc-
tion and through overlapping segments of the ring. Before we
analyze these cases, recall from Section 2 that the ring inter-
connect consists of four rings: 1) request, 2) acknowledge, 3)
snoop and 4) data rings. While it is fairly well known that
64 B cache lines are transferred as two packets over the 32 B
data ring [50, 69, 104], little is disclosed about i) what types
of packets travel through the other three rings and ii) how
packets ﬂow through the four rings during a load transaction.
Intel partially answers (i) in [76] and [46] where it explains
that the separate rings are respectively used for 1) read/write
requests 2) global observation3 and response messages, 3)
snoops to the cores4 and 4) data ﬁlls and write-backs. Further,
Intel sheds lights on (ii) in an illustration from [57] which
explains the ﬂow of an LLC hit transaction: the transaction
starts with a request packet that travels from the core to the