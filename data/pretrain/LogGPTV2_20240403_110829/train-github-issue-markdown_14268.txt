## ðŸš€ Feature
## Motivation
Changing CPU code to use GPU is just much harder than it intuitively would
need to be, as having different operands of an operation in different backends
would result in runtime error.  
Many logically simple operations like `latent = latent + noise` or `loss =
cross_entropy(inferred, labels)` would need extra lines of `if` conditions to
work properly for both CPU and GPU.  
Pytorch autograd makes programming much easier by automatically making the
result of any operation have `require_grad=True` if any of the operands has
so.  
So my question is: Why the same logic isn't applied to the cuda-ness of the
result, especially considering that the current behavior is getting a runtime
error?
## Pitch
The proposed feature is having an API call like `torch.cuda.auto(True)` and
letting the programmer activate or deactivate it manually, and when activated,
having CPU and GPU operands in one operation wouldn't cause an error as a GPU
copy of the CPU tensors would be created and the whole operation would be done
in GPU and return a GPU result, just like the logic for `require_grad`.  
In that case, just adding a `model.cuda()` line to the code would be enough
for many programs to run on GPU.
## Alternatives
## Additional context