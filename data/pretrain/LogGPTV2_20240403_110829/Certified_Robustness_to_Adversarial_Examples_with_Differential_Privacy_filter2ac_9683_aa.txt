title:Certified Robustness to Adversarial Examples with Differential Privacy
author:Mathias L&apos;ecuyer and
Vaggelis Atlidakis and
Roxana Geambasu and
Daniel Hsu and
Suman Jana
(cid:19)(cid:17)(cid:18)(cid:26)(cid:1)(cid:42)(cid:38)(cid:38)(cid:38)(cid:1)(cid:52)(cid:90)(cid:78)(cid:81)(cid:80)(cid:84)(cid:74)(cid:86)(cid:78)(cid:1)(cid:80)(cid:79)(cid:1)(cid:52)(cid:70)(cid:68)(cid:86)(cid:83)(cid:74)(cid:85)(cid:90)(cid:1)(cid:66)(cid:79)(cid:69)(cid:1)(cid:49)(cid:83)(cid:74)(cid:87)(cid:66)(cid:68)(cid:90)
Certiﬁed Robustness to Adversarial Examples with Differential Privacy
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana
Columbia University
Abstract—Adversarial examples that fool machine learning
models, particularly deep neural networks, have been a topic
of intense research interest, with attacks and defenses being
developed in a tight back-and-forth. Most past defenses are
best effort and have been shown to be vulnerable to sophis-
ticated attacks. Recently a set of certiﬁed defenses have been
introduced, which provide guarantees of robustness to norm-
bounded attacks. However these defenses either do not scale
to large datasets or are limited in the types of models they
can support. This paper presents the ﬁrst certiﬁed defense
that both scales to large networks and datasets (such as
Google’s Inception network for ImageNet) and applies broadly
to arbitrary model types. Our defense, called PixelDP, is based
on a novel connection between robustness against adversarial
examples and differential privacy, a cryptographically-inspired
privacy formalism, that provides a rigorous, generic, and
ﬂexible foundation for defense.
I. Introduction
Deep neural networks (DNNs) perform exceptionally well
on many machine learning tasks,
including safety- and
security-sensitive applications such as self-driving cars [5],
malware classiﬁcation [48], face recognition [47], and criti-
cal infrastructure [71]. Robustness against malicious behav-
ior is important in many of these applications, yet in recent
years it has become clear that DNNs are vulnerable to a
broad range of attacks. Among these attacks – broadly sur-
veyed in [46] – are adversarial examples: the adversary ﬁnds
small perturbations to correctly classiﬁed inputs that cause a
DNN to produce an erroneous prediction, possibly of the ad-
versary’s choosing [56]. Adversarial examples pose serious
threats to security-critical applications. A classic example is
an adversary attaching a small, human-imperceptible sticker
onto a stop sign that causes a self-driving car to recognize
it as a yield sign. Adversarial examples have also been
demonstrated in domains such as reinforcement learning [32]
and generative models [31].
Since the initial demonstration of adversarial exam-
ples [56], numerous attacks and defenses have been pro-
posed, each building on one another. Initially, most de-
fenses used best-effort approaches and were broken soon
after introduction. Model distillation, proposed as a robust
defense in [45], was subsequently broken in [7]. Other
work [36] claimed that adversarial examples are unlikely to
fool machine learning (ML) models in the real-world, due
to the rotation and scaling introduced by even the slightest
camera movements. However, [3] demonstrated a new attack
strategy that is robust to rotation and scaling. While this
(cid:165)(cid:1)(cid:19)(cid:17)(cid:18)(cid:26)(cid:13)(cid:1)(cid:46)(cid:66)(cid:85)(cid:73)(cid:74)(cid:66)(cid:84)(cid:1)(cid:45)(cid:70)(cid:68)(cid:86)(cid:90)(cid:70)(cid:83)(cid:15)(cid:1)(cid:54)(cid:79)(cid:69)(cid:70)(cid:83)(cid:1)(cid:77)(cid:74)(cid:68)(cid:70)(cid:79)(cid:84)(cid:70)(cid:1)(cid:85)(cid:80)(cid:1)(cid:42)(cid:38)(cid:38)(cid:38)(cid:15)
(cid:37)(cid:48)(cid:42)(cid:1)(cid:18)(cid:17)(cid:15)(cid:18)(cid:18)(cid:17)(cid:26)(cid:16)(cid:52)(cid:49)(cid:15)(cid:19)(cid:17)(cid:18)(cid:26)(cid:15)(cid:17)(cid:17)(cid:17)(cid:21)(cid:21)
(cid:23)(cid:22)(cid:23)
back-and-forth has advanced the state of the art, recently
the community has started to recognize that rigorous, theory-
backed, defensive approaches are required to put us off this
arms race.
Accordingly, a new set of certiﬁed defenses have emerged
over the past year,
that provide rigorous guarantees of
robustness against norm-bounded attacks [12], [52], [65].
These works alter the learning methods to both optimize
for robustness against attack at training time and permit
provable robustness checks at inference time. At present,
these methods tend to be tied to internal network details,
such as the type of activation functions and the network
architecture. They struggle to generalize across different
types of DNNs and have only been evaluated on small
networks and datasets.
We propose a new and orthogonal approach to certiﬁed
robustness against adversarial examples that is broadly ap-
plicable, generic, and scalable. We observe for the ﬁrst
time a connection between differential privacy (DP), a
cryptography-inspired formalism, and a deﬁnition of robust-
ness against norm-bounded adversarial examples in ML.
We leverage this connection to develop PixelDP, the ﬁrst
certiﬁed defense we are aware of that both scales to large
networks and datasets (such as Google’s Inception net-
work trained on ImageNet) and can be adapted broadly
to arbitrary DNN architectures. Our approach can even
be incorporated with no structural changes in the target
network (e.g., through a separate auto-encoder as described
in Section III-B). We provide a brief overview of our
approach below along with the section references that detail
the corresponding parts.
§II establishes the DP-robustness connection formally (our
ﬁrst contribution). To give the intuition, DP is a framework
for randomizing computations running on databases such
that a small change in the database (removing or altering
one row or a small set of rows) is guaranteed to result in
a bounded change in the distribution over the algorithm’s
outputs. Separately, robustness against adversarial examples
can be deﬁned as ensuring that small changes in the input of
an ML predictor (such as changing a few pixels in an image
in the case of an l0-norm attack) will not result in drastic
changes to its predictions (such as changing its label from
a stop to a yield sign). Thus, if we think of a DNN’s inputs
(e.g., images) as databases in DP parlance, and individual
features (e.g., pixels) as rows in DP, we observe that random-
izing the outputs of a DNN’s prediction function to enforce
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:14 UTC from IEEE Xplore.  Restrictions apply. 
DP on a small number of pixels in an image guarantees
robustness of predictions against adversarial examples that
can change up to that number of pixels. The connection can
be expanded to standard attack norms, including l1, l2, and
l∞ norms.
§III describes PixelDP, the ﬁrst certiﬁed defense against
norm-bounded adversarial examples based on differential
privacy (our second contribution). Incorporating DP into
the learning procedure to increase robustness to adversarial
examples requires is completely different and orthogonal to
using DP to preserve the privacy of the training set, the focus
of prior DP ML literature [40], [1], [9] (as § VI explains). A
PixelDP DNN includes in its architecture a DP noise layer
that randomizes the network’s computation, to enforce DP
bounds on how much the distribution over its predictions can
change with small, norm-bounded changes in the input. At
inference time, we leverage these DP bounds to implement a
certiﬁed robustness check for individual predictions. Passing
the check for a given input guarantees that no perturbation
exists up to a particular size that causes the network to
change its prediction. The robustness certiﬁcate can be used
to either act exclusively on robust predictions, or to lower-
bound the network’s accuracy under attack on a test set.
§IV presents the ﬁrst experimental evaluation of a certi-
ﬁed adversarial-examples defense for the Inception network
trained on the ImageNet dataset (our third contribution). We
additionally evaluate PixelDP on various network architec-
tures for four other datasets (CIFAR-10, CIFAR-100, SVHN,
MNIST), on which previous defenses – both best effort and
certiﬁed – are usually evaluated. Our results indicate that
PixelDP is (1) as effective at defending against attacks as
today’s state-of-the-art, best-effort defense [37] and (2) more
scalable and broadly applicable than a prior certiﬁed defense.
Our experience points to DP as a uniquely generic,
broadly applicable, and ﬂexible foundation for certiﬁed de-
fense against norm-bounded adversarial examples (§V, §VI).
We credit these properties to the post-processing property
of DP, which lets us incorporate the certiﬁed defense in a
network-agnostic way.
II. DP-Robustness Connection
A. Adversarial ML Background
An ML model can be viewed as a function mapping inputs
– typically a vector of numerical feature values – to an
output (a label for multiclass classiﬁcation and a real number
for regression). Focusing on multiclass classiﬁcation, we
deﬁne a model as a function f : Rn → K that maps n-
dimensional inputs to a label in the set K = {1, . . . , K}
of all possible labels. Such models typically map an input
x to a vector of scores y(x) = (y1(x), . . . , yK(x)), such
that yk(x) ∈ [0, 1] and
K
k=1 yk(x) = 1. These scores are
interpreted as a probability distribution over the labels, and
the model returns the label with highest probability, i.e.,
f (x) = arg maxk∈K yk(x). We denote the function that
(cid:2)
(cid:23)(cid:22)(cid:24)
maps input x to y as Q and call it the scoring function;
we denote the function that gives the ultimate prediction for
input x as f and call it the prediction procedure.
Adversarial Examples. Adversarial examples are a class
of attack against ML models, studied particularly on deep
neural networks for multiclass image classiﬁcation. The
attacker constructs a small change to a given, ﬁxed input,
that wildly changes the predicted output. Notationally, if the
input is x, we denote an adversarial version of that input by
x + α, where α is the change or perturbation introduced by
the attacker. When x is a vector of pixels (for images), then
xi is the i’th pixel in the image and αi is the change to the
i’th pixel.
n
(cid:2)
It is natural to constrain the amount of change an attacker
is allowed to make to the input, and often this is measured by
the p-norm of the change, denoted by (cid:4)α(cid:4)p. For 1 ≤ p  max
i:i(cid:4)=k
yi(x + α),
(1)
where k := f (x). A small change in the input does not alter
the scores so much as to change the predicted label.
B. DP Background
DP is concerned with whether the output of a computation
over a database can reveal
individual
records in the database. To prevent such information leakage,
randomness is introduced into the computation to hide
details of individual records.
information about
(cid:5)
A randomized algorithm A that takes as input a database
d and outputs a value in a space O is said to satisfy (, δ)-
DP with respect to a metric ρ over databases if, for any
) ≤ 1, and for any subset of
(cid:5)
with ρ(d, d
databases d and d
possible outputs S ⊆ O, we have
(cid:5)
) ∈ S) + δ.
P (A(d) ∈ S) ≤ eP (A(d
(2)
Here,  > 0 and δ ∈ [0, 1] are parameters that quantify
the strength of the privacy guarantee. In the standard DP
deﬁnition, the metric ρ is the Hamming metric, which simply
counts the number of entries that differ in the two databases.
For small  and δ, the standard (, δ)-DP guarantee implies
that changing a single entry in the database cannot change
the output distribution very much. DP also applies to general
metrics ρ [8], including p-norms relevant to norm-based
adversarial examples.
Our approach relies on two key properties of DP. First is
the well-known post-processing property: any computation
applied to the output of an (, δ)-DP algorithm remains
(, δ)-DP. Second is the expected output stability property,
a rather obvious but not previously enunciated property that
we prove in Lemma 1: the expected value of an (, δ)-
DP algorithm with bounded output is not sensitive to small
changes in the input.
Lemma 1. (Expected Output Stability Bound) Suppose
a randomized function A, with bounded output A(x) ∈
[0, b], b ∈ R
+, satisﬁes (, δ)-DP. Then the expected value
of its output meets the following property:
∀α ∈ Bp(1) (cid:2) E(A(x)) ≤ eE(A(x + α)) + bδ.
The expectation is taken over the randomness in A.
Proof: Consider any α ∈ Bp(1), and let x
(cid:5)
:= x + α.
We write the expected output as:
(cid:3)
E(A(x)) =
E(A(x)) ≤ e
P (A(x
) > t)dt
+
(cid:5)
(cid:3)
= eE(A(x