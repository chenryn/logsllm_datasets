Morphlines 可以看作是 Unix
管道的演变，其中数据模型被推广为使用通用记录流，包括任意二进制有效载荷。
morphline 命令有点像 Flume 拦截器。 Morphlines 可以嵌入到 Flume 等
Hadoop 组件中。
用于解析和转换一组标准数据格式（如日志文件，Avro，CSV，文本，HTML，XML，PDF，Word，Excel等）的命令是开箱即用的，还有其他自定义命令和解析器用于其他数据格式可以作为插件添加到
morphline。可以索引任何类型的数据格式， 并且可以生成任何类型的 Solr
模式的任何 Solr 文档，也可以注册和执行任何自定义 ETL 逻辑。
Morphlines
操纵连续的数据流。数据模型可以描述如下：数据记录是一组命名字段，其中每个字段具有一个或多个值的有序列表。值可以是任何Java对象。也就是说，数据记录本质上是一个哈希表，
其中每个哈希表条目包含一个 String 键和一个 Java 对象列表作为值。
（该实现使用 Guava 的 ArrayListMultimap，它是一个
ListMultimap）。请注意，字段可以具有多个值，并且任何两个记录都不需要使用公共字段名称。
此Sink将 Flume Event 的 body 填充到 morphline 记录的 *\_attachment_body*
字段中，并将 Flume Event 的 header
复制到同名的记录字段中。然后命令可以对此数据执行操作。
支持路由到 SolrCloud 集群以提高可伸缩性。索引负载可以分布在大量
MorphlineSolrSinks 上，以提高可伸缩性。可以跨多个 MorphlineSolrSinks
复制索引负载以实现高可用性， 例如使用 Flume的负载均衡特性。
MorphlineInterceptor 还可以帮助实现到多个 Solr
集合的动态路由（例如，用于多租户）。
老规矩，morphline 和 solr 的 jar 包需要放在 Flume 的 lib 目录中。
必需的参数已用 **粗体** 标明。
  属性                              默认值                                                      解释
  --------------------------------- ----------------------------------------------------------- -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **channel**                       \--                                                         与 Sink 绑定的 channel
  **type**                          \--                                                         组件类型，这个是： `org.apache.flume.sink.solr.morphline.MorphlineSolrSink`
  **morphlineFile**                 \--                                                         morphline 配置文件的相对或者绝对路径，例如：/etc/flume-ng/conf/morphline.conf
  morphlineId                       null                                                        如果 morphline 文件里配置了多个 morphline 实例，可以用这个参数来标识 morphline 作为一个可选名字
  batchSize                         1000                                                        单个事务操作的最大 Event 数量
  batchDurationMillis               1000                                                        事务的最大超时时间（毫秒）。达到这个时间或者达到 *batchSize* 都会触发提交事物。
  handlerClass                      org.apache.flume.sink.solr.morphline.MorphlineHandlerImpl   实现了 `org.apache.flume.sink.solr.morphline.MorphlineHandler` 接口的实现类的全限定类名
  isProductionMode                  false                                                       重要的任务和大规模的生产系统应该启用这个模式，这些系统需要在发生不可恢复的异常时不停机来获取信息。未知的 Solr 架构字段相关的错误、损坏或格式错误的解析器输入数据、解析器错误等都会产生不可恢复的异常。
  recoverableExceptionClasses       org.apache.solr.client.solrj.SolrServerException            以逗号分隔的可恢复异常列表，这些异常往往是暂时的，在这种情况下，可以进行相应地重试。 比如：网络连接错误，超时等。当 isProductionMode 标志设置为 true 时，使用此参数配置的可恢复异常将不会被忽略，并且会进行重试。
  isIgnoringRecoverableExceptions   false                                                       如果不可恢复的异常被意外错误分类为可恢复，则应启用这个标志。 这使得Sink能够取得进展并避免永远重试一个 Event。
配置范例：
``` properties
a1.channels = c1
a1.sinks = k1
a1.sinks.k1.type = org.apache.flume.sink.solr.morphline.MorphlineSolrSink
a1.sinks.k1.channel = c1
a1.sinks.k1.morphlineFile = /etc/flume-ng/conf/morphline.conf
# a1.sinks.k1.morphlineId = morphline1
# a1.sinks.k1.batchSize = 1000
# a1.sinks.k1.batchDurationMillis = 1000
```
#### ElasticSearchSink
这个Sink把数据写入到 elasticsearch 集群，就像
[logstash](https://logstash.net) 一样把 Event 写入以便
[Kibana](http://kibana.org) 图形接口可以查询并展示。
必须将环境所需的 elasticsearch 和 lucene-core jar 放在 Flume 安装的 lib
目录中。 Elasticsearch 要求客户端 JAR
的主要版本与服务器的主要版本匹配，并且两者都运行相同的 JVM
次要版本。如果版本不正确，会报 SerializationExceptions 异常。
要选择所需的版本，请首先确定 elasticsearch 的版本以及目标群集正在运行的
JVM 版本。然后选择与主要版本匹配的 elasticsearch 客户端库。
0.19.x客户端可以与0.19.x群集通信;
0.20.x可以与0.20.x对话，0.90.x可以与0.90.x对话。确定 elasticsearch
版本后， 读取 pom.xml 文件以确定要使用的正确 lucene-core JAR 版本。运行
ElasticSearchSink 的 Flume 实例程序也应该与目标集群运行的次要版本的 JVM
相匹配。
所有的 Event
每天会被写入到新的索引，名称是\-yyyy-MM-dd的格式，其中\可以自定义配置。Sink将在午夜
UTC 开始写入新索引。
默认情况下，Event 会被 ElasticSearchLogStashEventSerializer
序列化器进行序列化。可以通过 serializer
参数配置来更改序和自定义列化器。这个参数可以配置
*org.apache.flume.sink.elasticsearch.ElasticSearchEventSerializer* 或
*org.apache.flume.sink.elasticsearch.ElasticSearchIndexRequestBuilderFactory*
接口的实现类，ElasticSearchEventSerializer
现在已经不建议使用了，推荐使用更强大的后者。
必需的参数已用 **粗体** 标明。
  属性            默认值                                                                     解释
  --------------- -------------------------------------------------------------------------- -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **channel**     \--                                                                        与 Sink 绑定的 channel
  **type**        \--                                                                        组件类型，这个是： `org.apache.flume.sink.elasticsearch.ElasticSearchSink`
  **hostNames**   \--                                                                        逗号分隔的hostname:port列表，如果端口不存在，则使用默认的9300端口
  indexName       flume                                                                      指定索引名称的前缀。比如：默认是"flume"，使用的索引名称就是 flume-yyyy-MM-dd 这种格式。也支持 header 属性替换的方式，比如%{lyf}就会用 Event header 中的属性名为 lyf 的值。
  indexType       logs                                                                       文档的索引类型。默认为 log，也支持 header 属性替换的方式，比如%{lyf}就会用 Event header 中的属性名为 lyf 的值。
  clusterName     elasticsearch                                                              要连接的 ElasticSearch 集群名称
  batchSize       100                                                                        每个事务写入的 Event 数量
  ttl             \--                                                                        TTL 以天为单位，设置了会导致过期文档自动删除，如果没有设置，文档将永远不会被自动删除。 TTL 仅以较早的整数形式被接受， 例如 a1.sinks.k1.ttl = 5并且还具有限定符 `ms` （毫秒）， `s` （秒）， `m` （分钟）， `h` （小时）， `d` （天）和 `w` （星期）。 示例a1.sinks.k1.ttl = 5d 表示将TTL设置为5天。 点击  了解更多信息。
  serializer      org.apache.flume.sink.elasticsearch.ElasticSearchLogStashEventSerializer   序列化器必须实现 *ElasticSearchEventSerializer* 或 *ElasticSearchIndexRequestBuilderFactory* 接口，推荐使用后者。
  serializer.\*   \--                                                                        序列化器的一些属性配置
::: note
::: title
Note
:::
使用 header 替换可以方便地通过 header 中的值来动态地决定存储 Event
时要时候用的 indexName 和 indexType。使用此功能时应谨慎，因为 Event
提交者可以控制 indexName 和 indexType。 此外，如果使用 elasticsearch
REST 客户端，则 Event 提交者可以控制所使用的URL路径。
:::
配置范例：
``` properties
a1.channels = c1
a1.sinks = k1
a1.sinks.k1.type = elasticsearch
a1.sinks.k1.hostNames = 127.0.0.1:9200,127.0.0.2:9300
a1.sinks.k1.indexName = foo_index
a1.sinks.k1.indexType = bar_type
a1.sinks.k1.clusterName = foobar_cluster
a1.sinks.k1.batchSize = 500
a1.sinks.k1.ttl = 5d
a1.sinks.k1.serializer = org.apache.flume.sink.elasticsearch.ElasticSearchDynamicSerializer
a1.sinks.k1.channel = c1
```
#### Kite Dataset Sink
这是一个将 Event 写入到 Kite 的实验性的Sink。这个Sink会反序列化每一个
Event body，并将结果存储到 [Kite
Dataset](http://kitesdk.org/docs/current/guide/)。它通过按URI加载数据集来确定目标数据集。
唯一支持的序列化方式是 avro，并且必须在在 Event header
中传递数据的结构，使用 *flume.avro.schema.literal* 加 json
格式的结构信息表示，或者用 *flume.avro.schema.url*
加一个能够获取到结构信息的URL（比如hdfs:/\...这种）。
这与使用deserializer.schemaType = LITERAL的 Log4jAppender 和 [Spooling
Directory Source](#spooling-directory-source) 的 avro 反序列化器兼容。
::: note
::: title
Note
:::
1、\`flume.avro.schema.hash\` 这个 header 不支持；
2、在某些情况下，在超过滚动间隔后会略微发生文件滚动，但是这个延迟不会超过5秒钟，大多数情况下这个延迟是可以忽略的。
:::
  属性                           默认值   解释
  ------------------------------ -------- ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **channel**                    \--      与 Sink 绑定的 channel
  **type**                       \--      组件类型，这个是： org.apache.flume.sink.kite.DatasetSink
  **kite.dataset.uri**           \--      要打开的数据集的 URI
  kite.repo.uri                  \--      要打开的存储库的 URI（ **不建议使用** ，请改用 *kite.dataset.uri* ）
  kite.dataset.namespace         \--      将写入记录的数据集命名空间（ **不建议使用** ，请改用 *kite.dataset.uri* ）
  kite.dataset.name              \--      将写入记录的数据集名称（ **不建议使用** ，请改用 *kite.dataset.uri* ）
  kite.batchSize                 100      每批中要处理的记录数
  kite.rollInterval              30       释放数据文件之前的最长等待时间（秒）
  kite.flushable.commitOnBatch   true     如果为 true，Flume 在每次批量操作 *kite.batchSize* 数据后提交事务并刷新 writer。 此设置仅适用于可刷新数据集。 如果为 true，则可以将具有提交数据的临时文件保留在数据集目录中。 需要手动恢复这些文件，以使数据对 DatasetReaders 可见。
  kite.syncable.syncOnBatch      true     Sink在提交事务时是否也将同步数据。 此设置仅适用于可同步数据集。 同步操作能保证数据将写入远程系统上的可靠存储上，同时保证数据已经离开Flume客户端的缓冲区（也就是 channel）。 当 *thekite.flushable.commitOnBatch* 属性设置为 `false` 时，此属性也必须设置为 `false`。
  kite.entityParser              avro     将 Flume Event 转换为 kite 实体的转换器。取值可以是 avro 或者 *EntityParser.Builder* 接口实现类的全限定类名
  kite.failurePolicy             retry    发生不可恢复的异常时采取的策略。例如 Event header 中缺少结构信息。默认采取重试的策略。 其他可选的值有： `save` ，这样会把 Event 原始内容写入到 *kite.error.dataset.uri* 这个数据集。还可以填自定义的处理策略类的全限定类名（需实现 *FailurePolicy.Builder* 接口）
  kite.error.dataset.uri         \--      保存失败的 Event 存储的数据集。当上面的参数 *kite.failurePolicy* 设置为 `save` 时，此参数必须进行配置。
  auth.kerberosPrincipal         \--      用于 HDFS 安全身份验证的 Kerberos 用户主体
  auth.kerberosKeytab            \--      Kerberos 安全验证主体的 keytab 本地文件系统路径
  auth.proxyUser                 \--      HDFS 操作的用户，如果与 kerberos 主体不同的话
#### Kafka Sink
这个 Sink 可以把数据发送到 [Kafka](http://kafka.apache.org/)
topic上。目的就是将 Flume 与 Kafka
集成，以便基于拉的处理系统可以处理来自各种 Flume Source 的数据。
目前支持Kafka 0.10.1.0以上版本，最高已经在Kafka
2.0.1版本上完成了测试，这已经是Flume 1.9发行时候的最高的Kafka版本了。
必需的参数已用 **粗体** 标明。
  属性                                                              默认值                解释
  ----------------------------------------------------------------- --------------------- --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **type**                                                          \--                   组件类型，这个是： `org.apache.flume.sink.kafka.KafkaSink`
  **kafka.bootstrap.servers**                                       \--                   Kafka Sink 使用的 Kafka 集群的实例列表，可以是实例的部分列表。但是更建议至少两个用于高可用（HA）支持。格式为 hostname:port，多个用逗号分隔
  kafka.topic                                                       default-flume-topic   用于发布消息的 Kafka topic 名称 。如果这个参数配置了值，消息就会被发布到这个 topic 上。如果Event header中包含叫做"topic"的属性， Event 就会被发布到 header 中指定的 topic 上，而不会发布到 *kafka.topic* 指定的 topic 上。支持任意的 header 属性动态替换， 比如%{lyf}就会被 Event header 中叫做"lyf"的属性值替换（如果使用了这种动态替换，建议将 Kafka 的 *auto.create.topics.enable* 属性设置为 `true` ）。
  flumeBatchSize                                                    100                   一批中要处理的消息数。设置较大的值可以提高吞吐量，但是会增加延迟。
  kafka.producer.acks                                               1                     在考虑成功写入之前，要有多少个副本必须确认消息。可选值， `0` ：（从不等待确认）； `1` ：只等待leader确认； `-1` ：等待所有副本确认。 设置为-1可以避免某些情况 leader 实例失败的情况下丢失数据。
  useFlumeEventFormat                                               false                 默认情况下，会直接将 Event body 的字节数组作为消息内容直接发送到 Kafka topic 。如果设置为true，会以 Flume Avro 二进制格式进行读取。 与 Kafka Source 上的同名参数或者 Kafka channel 的 *parseAsFlumeEvent* 参数相关联，这样以对象的形式处理能使生成端发送过来的 Event header 信息得以保留。
  defaultPartitionId                                                \--                   指定所有 Event 将要发送到的 Kafka 分区ID，除非被 *partitionIdHeader* 参数的配置覆盖。 默认情况下，如果没有设置此参数，Event 会被 Kafka 生产者的分发程序分发，包括 key（如果指定了的话），或者被 *kafka.partitioner.class* 指定的分发程序来分发
  partitionIdHeader                                                 \--                   设置后，Sink将使用 Event header 中使用此属性的值命名的字段的值，并将消息发送到 topic 的指定分区。 如果该值表示无效分区，则将抛出 EventDeliveryException。 如果存在标头值，则此设置将覆盖 *defaultPartitionId* 。假如这个参数设置为"lyf"，这个 Sink 就会读取 Event header 中的 lyf 属性的值，用该值作为分区ID
  allowTopicOverride                                                true                  如果设置为 `true`，会读取 Event header 中的名为 *topicHeader* 的的属性值，用它作为目标 topic。
  topicHeader                                                       topic                 与上面的 *allowTopicOverride* 一起使用，\*allowTopicOverride\* 会用当前参数配置的名字从 Event header 获取该属性的值，来作为目标 topic 名称
  kafka.producer.security.protocol *more producer security props*   PLAINTEXT             设置使用哪种安全协议写入 Kafka。可选值：`SASL_PLAINTEXT` 、 `SASL_SSL` 和 `SSL`， 有关安全设置的其他信息，请参见下文。 如果使用了 `SASL_PLAINTEXT` 、 `SASL_SSL` 或 `SSL` 等安全协议，参考 [Kafka security](http://kafka.apache.org/documentation.html#security) 来为生产者增加安全相关的参数配置
  Other Kafka Producer Properties                                   \--                   其他一些 Kafka 生产者配置参数。任何 Kafka 支持的生产者参数都可以使用。唯一的要求是使用"kafka.producer."这个前缀来配置参数，比如：\*kafka.producer.linger.ms\*
::: note
::: title
Note
:::
Kafka Sink使用 Event header 中的 topic 和其他关键属性将 Event 发送到
Kafka。 如果 header 中存在 topic，则会将Event发送到该特定
topic，从而覆盖为Sink配置的 topic。 如果 header
中存在指定分区相关的参数，则Kafka将使用相关参数发送到指定分区。
header中特定参数相同的 Event 将被发送到同一分区。 如果为空，则将 Event
会被发送到随机分区。 Kafka Sink
还提供了key.deserializer（org.apache.kafka.common.serialization.StringSerializer）
和value.deserializer（org.apache.kafka.common.serialization.ByteArraySerializer）的默认值，不建议修改这些参数。
:::
弃用的一些参数：
  属性           默认值                解释
  -------------- --------------------- ------------------------------
  brokerList     \--                   改用 kafka.bootstrap.servers
  topic          default-flume-topic   改用 kafka.topic
  batchSize      100                   改用 kafka.flumeBatchSize
  requiredAcks   1                     改用 kafka.producer.acks
下面给出 Kafka Sink 的配置示例。Kafka 生产者的属性都是以 kafka.producer
为前缀。Kafka
生产者的属性不限于下面示例的几个。此外，可以在此处包含您的自定义属性，并通过作为方法参数传入的Flume
Context对象在预处理器中访问它们。
``` properties
a1.sinks.k1.channel = c1
a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.kafka.topic = mytopic
a1.sinks.k1.kafka.bootstrap.servers = localhost:9092
a1.sinks.k1.kafka.flumeBatchSize = 20
a1.sinks.k1.kafka.producer.acks = 1
a1.sinks.k1.kafka.producer.linger.ms = 1
a1.sinks.k1.kafka.producer.compression.type = snappy
```
**安全与加密**
Flume 和 Kafka
之间通信渠道是支持安全认证和数据加密的。对于身份安全验证，可以使用 Kafka
0.9.0版本中的 SASL、GSSAPI （Kerberos V5） 或 SSL
（虽然名字是SSL，实际是TLS实现）。
截至目前，数据加密仅由SSL / TLS提供。
Setting kafka.producer.security.protocol to any of the following value
means:
当你把 kafka.producer.security.protocol 设置下面任何一个值的时候意味着：
-   **SASL_PLAINTEXT** - 无数据加密的 Kerberos 或明文认证
-   **SASL_SSL** - 有数据加密的 Kerberos 或明文认证
-   **SSL** - 基于TLS的加密，可选的身份验证
::: warning
::: title
Warning
:::