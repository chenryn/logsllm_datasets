. We now discuss our interpretation of these parameters.
P ((cid:126)Y |H0
Finding the exact distribution P ((cid:126)Y |Hi) of a multidimensional
random variable (cid:126)Y is, in general, a very hard problem. In fact, a
large body of research in the machine learning community focuses
precisely on models to approximate unknown multidimensional
distributions [21]. A fundamental result in approximating distri-
butions is the bias-variance trade-off. This result says that as
the complexity of our model increases (i.e., as the number of
parameters increases) the variance of the model also increases, i.e.,
the approximation to P ((cid:126)Y |Hi) is very dependent on the training
data, but it cannot generalize to other domains other than the one
associated with the collected data. Similarly, if the complexity of
the model is small (i.e., if the number of parameters used is very
small), the bias of the model will be large, i.e., the model will not
have enough parameters to approximate the real distribution. This
concept is illustrated in Figure 2.
We therefore make a very common approximation to the
joint distribution by assuming conditional independence of the
detectors:
P ((cid:126)Y |Hi) = P (y1|Hi)P (y2|Hi) · · · P (yn|Hi).
(4)
With this approximation, in order to compute the joint distribu-
tion of the ensemble, we only need to estimate 2n parameters: n
probabilities of detection (PDi), and n probabilities of false alarm
(PF i), as opposed to the 2× (2n− 1) parameters required to model
the precise multinomial distributions P ((cid:126)Y |H1) and P ((cid:126)Y |H0).
The conditional independence assumption is a very common
technique to obtain an approximation of the joint distribution, and
this assumption has been applied successfully in several intrusion
detection scenarios [3, 28], and in machine learning scenarios
(most notably by the naïve Bayes classiﬁer [21]). We will comment
more on the practical effects of this assumption in Section 6.
Note that there is a difference between complete independence
and conditionally independence. In our conditional independence
assumption, we still allow for correlations such as the fact that if
Figure 1: Alert fusion for an IDS ensemble
To perform a cost-based analysis, we need speciﬁcally deﬁne
cost Cij, which is related to a situation in which IDS outputs
Hj while Hi is the real output. C01 is the cost of F P (i.e.,
P (H1|H0)). C10 is the cost of F N (i.e., P (H0|H1)). C00 is
the cost of true negative (P (H0|H0), the probability that the IDS
indicates normal when no intrusion occurs), and C11 is the cost
of detection (also called true positive, P (H1|H1), the probability
that the IDS outputs an alarm when an intrusion occurs). In most
cases, we can just set C00 = C11 = 0 because we assume an
IDS involves no cost (in terms of loss of information or services
in the protected network) if it does the right thing. (We will use
this setting throughout the entire paper.) Now we can deﬁne an
expected cost function Cexp, which reﬂects the overall Bayesian
risk.
1(cid:88)
1(cid:88)
Cexp =
i=0
j=0
CijPiP (Hj|Hi),
(1)
where Pi is the prior probability of Hi, i = 0, 1. Thus, Cexp is the
expected mean value of cost over all possible situations. In the IDS
literature, P1 is called base rate [4] that indicates the probability
of each data unit being malicious (H1) (or, the prior fraction of
intrusion in the entire data stream). In a cost-based analysis case,
Cij should always be pre-deﬁned by administrators according to
the risk model of their networks [3]. We will brieﬂy discuss an
unbiased estimation method of Pi later in the paper.
We further deﬁne the probability of false positive (F P ) and the
probability of detection (1 − F N) corresponding to detector i,
denoted as PF i and PDi, respectively. The overall probability of
false positive and the overall probability of detection of the IDS
ensemble are denoted as PF and PD, respectively.
(cid:88)
(cid:88)
(cid:126)Y
PF =
PD =
P (y0 = 1|(cid:126)Y )P ((cid:126)Y |H0)
P (y0 = 1|(cid:126)Y )P ((cid:126)Y |H1)
(cid:126)Y
Here the sum over (cid:126)Y indicates that all the possible values of (cid:126)Y
be taken and summed up. Then we can expand Cexp in Eq(1) using
PF and PD.
138
IDS1(cid:13)IDS2(cid:13)IDSn(cid:13)Alert Fusion Center(cid:13)Data unit(cid:13)with status H0 or H1(cid:13)y1(cid:13)y2(cid:13)yn(cid:13)y0(cid:13)These rules are selected because they are simple/intuitive in
nature, and most commonly used in machine learning and intrusion
detection literature.
4.1 Experiment Using Machine Learning
Based IDSs on KDD Data Set
Our ﬁrst experiment used KDD cup 1999 data set [1], which was
produced from the 1998 DARPA Intrusion Detection Evaluation
program [33]. The raw data includes about nine weeks of TCP
dump network trafﬁc containing normal trafﬁc and many attacks.
Lee and Stolfo et al. [31] further processed data based on three
categories of derived features, i.e., basic features of individual
TCP connections, content features within a connection suggested
by domain knowledge, and trafﬁc features computed using a
two-second time window (please refer to [31] for detail). We
acknowledge the limitation/ﬂaw of this data set as criticized in [36,
35]. The reason we choose this as our initial study is because it
is the only well-studied, documented and public trace for intrusion
detection. We will further comment on this shortly after we show
the experiment results.
In
we
this
experiment,
chose
kddcup.data_10_percent.gz as the training data set
and corrected.gz as the testing data set. The training set
(75M uncompressed) is about 10% of the full data set and contains
494,020 records. The testing data (45M uncompressed) with
corrected labels has 311,029 records, among which 60,593 are
normal and 250,436 are intrusions. Every record is labeled only
using 0 (normal) or 1 (anomaly) in the experiment.
We used several machine learning based IDSs to construct the
IDS ensemble. Speciﬁcally, we chose four different machine
learning algorithms, i.e., DT (decision tree algorithm, speciﬁcally,
C4.5), NB (Naïve Bayes classiﬁer), KNN (K-Nearest Neighbor,
speciﬁcally, we set k = 9), and SVM (Support Vector Machine,
speciﬁcally, we use the LIBSVM [10] tool) [48]. Most of
the algorithms are described in [37]. All of them have been
successfully applied to intrusion detection [2, 23, 32]. We used
all these four machine learning-based IDSs (DT, NB, KNN, and
SVM) to build an IDS ensemble.
Table 1 reports the accuracy of each detector and fusion rule.
Note that here we have not involved cost factors yet, but simply
show accuracy in terms of the detection rate (PD), the false positive
rate (PF ), and the total error rate (). Since the LRT rule requires
cost factors (which deﬁne the threshold the LRT rule uses for
ﬁnding its ﬁnal decision), it is not shown in the table. From the
table we can see that for these four single detectors, DT and KNN
achieve slightly better accuracy than NB in terms of PD, PF and
. SVM obtains the lowest false positive rate among four, however,
also the lowest detection rate.
For the fusion result, the AND rule achieves the lowest false
positive rate (0.00066014). The OR rule has the highest detection
rate (0.9185) and the lowest total error rate. The MAJ and VOT
rules have some balance between false positive rate and false
negative rate, compared with the AND and OR rules.
Now we will involve a cost factor analysis. In the experiment,
since we have all the running results, we can compute the ﬁnal
total cost related to the total number of false positives and false
negatives. We deﬁne
Ctotal = N01C01 + N10C10,
where N01 is the number of false positives in testing, and N10 is
the number of false negatives in testing. Thus, Ctotal reﬂects the
ﬁnal overall cost.
We want to cover as many risk scenarios as possible (with
Figure 2: Bias-variance trade-off: with more parameters to
model a distribution, we can obtain a more accurate result,
but because we have ﬁnite training data (and possibly non-
stationary distributions), we cannot estimate reliably all these
parameters and thus our variance increases.
one detector raises an alarm, this increases the likelihood of the
other detector ﬁring an alarm.
Regarding the knowledge of the cost and the base-rate, although
several research has presented a very thorough analysis of risks and
operating costs [3] (and thus have a reliable estimate for C01P0
),
C10P1
such information in general may not be available in practical
scenarios. Therefore, we show in Section 5 how to use and interpret
the LRT rule even if we do not know these parameters.
For simplicity, it is also convenient to use the log-likelihood
ratios. Now we can have the following fusion/decision rule:
i∈S0
i∈S0
log
log
(1−PDi)
(1−PF i)
(1−PDi)
(1−PF i)
+
+
j∈S1
j∈S1
log
log
PDj
PF j
PDj
PF j
> log C01P0
C10P1
< log C01P0
C10P1
, y0 = 1
, y0 = 0
(5)
where S0 is the set of all detectors i, yi ∈ (cid:126)Y , and yi = 0; S1 is
the set of all detectors j, yj ∈ (cid:126)Y , and yj = 1. Note in this form,
we can also consider Eq. (5) as a special form of weighted voting.
(cid:80)
(cid:80)
{ If
If
(cid:80)
(cid:80)
4. EXPERIMENT EVALUATION
In order to evaluate the performance of our LRT ensemble,
we performed several experiments using multiple IDSs on two
different data sets to show its practical application. We will provide
theoretic reasoning in the next section.
In the evaluation, we will compare the following existing
common ensemble decision/fusion rules:
• AND rule: only if all the detectors report anomaly (yi = 1
for all i = 1..n), then y0 = 1; otherwise, y0 = 0.
• OR rule: if any detector reports anomaly, then y0 = 1; only
if all detectors report normal, then y0 = 0.
• Majority (MAJ) rule: if most (at least half) of the detectors
report anomaly, then y0 = 1, otherwise, y0 = 0.
• Weighted voting (VOT) rule: treat the decision 1, 0 as 1,−1,
and use weighted voting y0 = sign(
i), here αi is
2 ln ( 1−i
the weight assigned to detector i and αi = 1
)
[17]. This gives more weight on the detector with less
classiﬁcation error rate.
i αiy(cid:48)
i
(cid:80)
• LRT rule: our cost-aware decision rule based on the LRT
fusion algorithm.
139
Classiﬁcation ErrorsModel ComplexityTraining DataTest DataHigh BiasLow VarianceLow Bias High VarianceLowHighLowHighDT
0.9103
0.017609
0.075636
PD
PF

0.9016
0.023204
0.083767
Table 1: Accuracy of each detector and fusion rule (ensembling four IDSs)
MAJ
0.9027
KNN
0.9080
AND
0.8662
SVM
0.876
NB
0.0058423
0.075212
0.0036473
0.10053
0.00066014
0.10789
0.0040269
0.079092
OR
0.9185
0.028931
0.071254
VOT
0.9066
0.016669
0.078446
different weights of cost for false alarms and missed attacks) that
could occur. Assume the minimum unit of cost value is 1, we ﬁrst
ﬁx C10 = 1 and vary C01 to obtain its effect (simulating the cases
that false positive is equal or more important than false negative)
on different rules. Then, we ﬁx C01 = 1 and vary C10 to simulate
the cases that the cost of false negative is equal or greater than that
of false positive. Thus, we verify the efforts of these fusion rules
in various cost scenarios (with different setting of C01 and C10).
Figure 3(a) and (b) show Ctotal for different rules in these two
settings.
(a) Fix the cost of F N
(C10 = 1) in all the cases,
vary the cost of F P (C01).
(b) Fix the cost of F P
(C01 = 1) in all the cases,
vary the cost of F N (C10).
Figure 3: Total cost for different rules in different risk
scenarios (ensembling four IDSs). The LRT rule obtains the
lowest overall cost in most of the scenarios.
Generally speaking, we can still claim that the LRT rule performs
the best. The MAJ and VOT rules achieve almost identical
performance in the experiment. This is intuitive because all of the
these four IDSs have similar total error rates that result in the effect
of weights in VOT similar to that in MAJ. Thus, we only draw
the VOT rule in the ﬁgures. The VOT rule performs well, but still