an operator might want to ask about a network: how well
is my load balancing protocol working? We demonstrate
Speedlight’s ability to answer this question by comparing
the performance characteristics of ECMP and Flowlet load
balancing algorithms in the presence of Hadoop, GraphX,
and memcache. In theory, Flowlet forwarding should bal-
ance load more fairly because it splits tra"c at a #ner gran-
ularity [20]. In practice, our understanding of the impact
of $owlets on load balance is limited to average utilization,
drop rate, $ow completion time, and other carefully crafted
proxies for the property in which we are actually interested.
In this experiment, we took a series of snapshots, and
computed the standard deviation of the EWMA of packet in-
terarrival times across uplink ports. To account for workload
deviations, uplinks were compared only to other uplinks on
the same switch. Figure 12 shows CDFs of the standard devi-
ations for our Hadoop, GraphX, and memcache workloads
taken with both snapshots and traditional polling. The three
workloads showcase three di!erent behaviors.
For Hadoop, polling shows little-to-no gain for $owets,
when in reality $owlets improve balance signi#cantly. For
GraphX, polling consistently underestimates the imbalance
in the network. Our Memcache workload is very evenly
distributed, but exhibits the opposite behavior—polling con-
sistently overestimates the imbalance.
Together, these experiments illustrate an important point.
For measures of whole-network behavior, the issue is not just
that polling might provide an incorrect view of the network,
but that it is di"cult to place a bound on the inaccuracy.
(a) Snapshot
(b) Polling
Figure 13: Pairwise correlation coe#cients for egress
ports while running GraphX. The red boxes highlight
port pairs on the same ECMP paths, which are ex-
pected to have high positive correlations.
8.4 Use Case: Synchronized Tra#c
The second use case we target is the detection of synchro-
nized application tra"c patterns for understanding behavior
or debugging performance issues. For this experiment, we
measured EWMA of packet rates at egress of all ports, in 100
snapshots taken 1 second apart. We then calculated pairwise
correlation between ports using Spearman [12] tests.
Figure 13 shows the statistically signi#cant (ρ < 0.1) corre-
lation coe"cients found while running GraphX. With snap-
shots, the Spearman test found correlations for 43% more
of the port pairs. To validate correctness, we analyzed the
output for evidence of two ground truths related to the ap-
plication and network topology. First, we expected to see
no signi#cant correlations between the port egressing to
the master server (server 0) and any other port because the
master server did not participate in the distributed computa-
tion. Second, we expected to #nd high correlations between
possible ECMP next-hops.
With snapshots, the correlation coe"cients matched both
expected ground truths. Polling, on the other hand, failed to
identify the positive correlations between ECMP ports. As
shown by the red boxes in Figure 13, the correlations found
with polling were either statistically insigni#cant or, worse,
413
Synchronized Network Snapshots
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
statistically signi!cant but negative. Results were qualita-
tively similar for other applications and ρ values.
9 RELATED WORK
Network measurement is a well-studied !eld, with many pro-
posals for better and more expressive measurement tools [15,
22, 31, 42]. As networks grow, it becomes even more im-
portant to have good monitoring and debugging tools. Our
work is, to the best of our knowledge, the !rst to demon-
strate practical, synchronous, and consistent network-wide
measurement. A large body of prior work has tackled related
goals and solutions. We discuss that work below.
Hardware-assisted measurement. With the recent rise of
programmable data and control planes, there has been in-
creased interest in novel measurement applications [28, 29,
31, 32, 37, 40]. Thus far, these approaches have concentrated
on exploring the limits of what can be feasibly collected. To-
gether, they are a testament to the expressiveness and utility
of programmable switches. Our work is complementary—
network snapshots can be of any local state, including the
statistics generated by these systems.
Multi-device measurement. One method to move beyond
single-component measurement is to leverage tra"c to cap-
ture relevant state as it traverses the network [1, 2, 15, 22].
For example, packets could record the minimum queue depth
at any intermediate switch. These techniques have the ad-
vantage of enforcing causal consistency at the level of an
single sample; however, like single component measurement,
it is still di"cult to compare across samples and paths.
Measurement aggregation. Another approach for trying
to understand network-wide behavior is to take measure-
ments of individual devices or paths and build larger in-
sights on top of their aggregates. There are too many such
approaches to cover here, but these largely rely on statis-
tics, thresholds, and similar techniques. Network tomogra-
phy [11, 21, 26, 30] is a common example that uses statistics
to tease out interesting behavior from long-term traces of
multiple devices. While this class of approaches can assist
in a variety of use cases, they lack the granularity to answer
the types of questions addressed in the preceding section.
Distributed snapshots. The literature on distributed snap-
shot algorithms is similarly rich. The original paper on the
topic [10] inspired a wide variety of improvements and re-
!nements. Of particular note are piggybacking-based proto-
cols like [24, 27]. Originally designed to allow for non-FIFO
channels, we borrow their techniques for handling packet
drops, but prohibit out-of-order delivery for e"ciency. Fi-
nally, we note that others have discussed the practicality of
distributed snapshots in networks [18, 34], but in the control
plane rather than the data plane.
10 DISCUSSION
Measuring Forwarding State. In Section 2.2, we remarked
that it may be useful to snapshot forwarding state. While
ASIC data planes are not typically able to record table entries
directly, they can record version information. Speci!cally,
the control plane can ensure every FIB rule and version tags
passing packets with a unique ID that is then stored back
into processing unit state. A snapshot of the state would then
give hints as to the entire network’s forwarding state.
Partial Deployment. Speedlight is amenable to partial de-
ployment. In this case, the snapshot would be of participating
devices and the communication channels between them. For
instance, in a data center, an operator might want for only
ToR switches or a particular cluster to be snapshot-enabled.
For snapshots without channel state, the only requirement
is that the snapshot header is appended and removed at the
proper time. The simplest method is to append the header
whenever an ingress processing unit encounters a packet
without one, and con!gure the remaining hosts to ignore IP
options in which the snapshot header is contained. If that is
not possible (e.g., due to security concerns with IP options),
the header should be removed at the last snapshot-enabled
device in the packet’s path. Causal consistency is maintained
even when there are multiple paths between devices.
Snapshots with channel state are slightly more complex. In
order to gather channel state, devices must be able to reduce
communication to FIFO channels. More speci!cally, devices
must tag packets with the physical path they take between
snapshot-enabled devices. We note that in the case of data
centers and snapshot-enabled ToRs, this requires only minor
modi!cations to the con!guration of existing devices [33].
11 CONCLUSION
The technique described in this paper, Synchronized Net-
work Snapshots, and its realization, Speedlight, provide un-
precedented visibility into the behavior of the network as a
whole. Whether for evaluating a design, diagnosing an issue,
or simply trying to understand an existing network, these
techniques help to answer critical questions. We demonstrate
that this approach is practical by implementing and deploy-
ing on a testbed a working version of our system, then using
it to collect interesting measurements of real workloads.
ACKNOWLEDGEMENTS
We gratefully acknowledge Sameera Gajjarapu, our shepherd
Aditya Akella, and the anonymous SIGCOMM reviewers for
all of their help and thoughtful comments.
414
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Nofel Yaseen, John Sonchack, and Vincent Liu
REFERENCES
[1] Aijay Adams, Petr Lapukhov, and Hongyi Zeng. 2016.
Net-
NORAD: Troubleshooting networks via end-to-end probing.
(2016).
https://code.facebook.com/posts/1534350660228025/
netnorad-troubleshooting-networks-via-end-to-end-probing/.
[2] Mohammad Alizadeh, Tom Edsall, Sarang Dharmapurikar, Ramanan
Vaidyanathan, Kevin Chu, Andy Fingerhut, Vinh The Lam, Francis Ma-
tus, Rong Pan, Navindra Yadav, and George Varghese. 2014. CONGA:
Distributed Congestion-aware Load Balancing for Datacenters. In Pro-
ceedings of the 2014 ACM Conference on SIGCOMM (SIGCOMM ’14).
ACM, New York, NY, USA, 503–514. https://doi.org/10.1145/2619239.
2626316
[3] Dormando Anatoly Vorobey, Brad Fitzpatrick. 2009. Memcached.
(2009). https://memcached.org
[4] Apache Software Foundation. 2012. Hadoop, Terasort.
(2012).
https://hadoop.apache.org/docs/r2.7.1/api/org/apache/hadoop/
examples/terasort/package-summary.html
[5] Apache Software Foundation. 2012. Hadoop, YARN. (2012). https:
//hadoop.apache.org/docs/r2.7.0/
[6] Apache Software Foundation. 2014. PageRank, GraphX.
(2014).
https://github.com/apache/spark/blob/master/examples/src/main/
scala/org/apache/spark/examples/graphx/SynthBenchmark.scala
[7] Apache Software Foundation. 2016. Spark. (2016). https://github.com/
apache/spark/
[8] Barefoot. 2017. Barefoot To!no. https://www.barefootnetworks.com/
technology/. (2017).
[9] Pat Bosshart, Glen Gibb, Hun-Seok Kim, George Varghese, Nick McK-
eown, Martin Izzard, Fernando Mujica, and Mark Horowitz. 2013. For-
warding Metamorphosis: Fast Programmable Match-action Processing
in Hardware for SDN. In Proceedings of the ACM SIGCOMM 2013 Confer-
ence on SIGCOMM (SIGCOMM ’13). ACM, New York, NY, USA, 99–110.
https://doi.org/10.1145/2486001.2486011
[10] K Mani Chandy and Leslie Lamport. 1985. Distributed snapshots:
Determining global states of distributed systems. ACM Transactions
on Computer Systems (TOCS) 3, 1 (1985), 63–75.
[11] Yan Chen, David Bindel, Hanhee Song, and Randy H. Katz. 2004.
An Algebraic Approach to Practical and Scalable Overlay Network
Monitoring. In Proceedings of the 2004 Conference on Applications,
Technologies, Architectures, and Protocols for Computer Communi-
cations (SIGCOMM ’04). ACM, New York, NY, USA, 55–66. https:
//doi.org/10.1145/1015467.1015475
[12] Christophe Croux and Catherine Dehon. 2010. In"uence Functions of
the Spearman and Kendall Correlation Measures. Statistical methods
& applications 19, 4 (2010), 497–515.
[13] Dormando. 2016. mc-crusher. (2016). https://github.com/memcached/
mc-crusher
[14] Glen Gibb, George Varghese, Mark Horowitz, and Nick McKeown.
2013. Design Principles for Packet Parsers. In Proceedings of the ninth
ACM/IEEE symposium on Architectures for networking and communica-
tions systems. IEEE, Washington, D.C., USA, 13–24.
[15] Chuanxiong Guo, Lihua Yuan, Dong Xiang, Yingnong Dang, Ray
Huang, Dave Maltz, Zhaoyi Liu, Vin Wang, Bin Pang, Hua Chen, Zhi-
Wei Lin, and Varugis Kurien. 2015. Pingmesh: A Large-Scale System
for Data Center Network Latency Measurement and Analysis. In Pro-
ceedings of the 2015 ACM Conference on Special Interest Group on Data
Communication (SIGCOMM ’15). ACM, New York, NY, USA, 139–152.
https://doi.org/10.1145/2785956.2787496
[16] C. Hopps. 2000. Analysis of an Equal-Cost Multi-Path Algorithm. RFC
2992. RFC Editor. 1–8 pages. https://tools.ietf.org/html/rfc2992
[17] Shuihai Hu, Yibo Zhu, Peng Cheng, Chuanxiong Guo, Kun Tan, Ji-
tendra Padhye, and Kai Chen. 2017. Tagger: Practical PFC Deadlock
Prevention in Data Center Networks. In Proceedings of the 13th Inter-
national Conference on Emerging Networking EXperiments and Tech-
nologies (CoNEXT ’17). ACM, New York, NY, USA, 451–463. https:
//doi.org/10.1145/3143361.3143382
[18] John P. John, Ethan Katz-Bassett, Arvind Krishnamurthy, Thomas
Anderson, and Arun Venkataramani. 2008. Consensus Routing: The
Internet as a Distributed System. In Proceedings of the 5th USENIX
Symposium on Networked Systems Design and Implementation (NSDI
’08). USENIX Association, Berkeley, CA, USA, 351–364.
[19] Prem Jonnalagadda. 2017.
Disaggregation and Programmable
https://barefootnetworks.com/blog/
Forwarding
disaggregation-and-programmable-forwarding-planes/. (2017).
Planes.
[20] Srikanth Kandula, Dina Katabi, Shantanu Sinha, and Arthur W. Berger.
2007. Dynamic Load Balancing Without Packet Reordering. Computer
Communication Review 37, 2 (2007), 51–62. https://doi.org/10.1145/
1232919.1232925
[21] Srikanth Kandula, Ratul Mahajan, Patrick Verkaik, Sharad Agarwal,
Jitendra Padhye, and Paramvir Bahl. 2009. Detailed Diagnosis in En-
terprise Networks. ACM SIGCOMM Computer Communication Review
39, 4 (2009), 243–254.
[22] Changhoon Kim, Anirudh Sivaraman, Naga Katta, Antonin Bas, Advait
Dixit, and Lawrence J Wobker. 2015. In-band Network Telemetry via
Programmable Dataplanes. In Demo paper at SIGCOMM ’15.
[23] Ajay D Kshemkalyani, Michel Raynal, and Mukesh Singhal. 1995. An
introduction to snapshot algorithms in distributed computing. Dis-
tributed systems engineering 2, 4 (1995), 224.
[24] Ten H Lai and Tao H Yang. 1987. On distributed snapshots. Inform.
Process. Lett. 25, 3 (1987), 153–158.
[25] Ki Suh Lee, Han Wang, Vishal Shrivastav, and Hakim Weatherspoon.
2016. Globally Synchronized Time via Datacenter Networks. In Pro-
ceedings of the 2016 ACM SIGCOMM Conference (SIGCOMM ’16). ACM,
New York, NY, USA, 454–467. https://doi.org/10.1145/2934872.2934885
[26] Ma łgorzata Steinder and Adarshpal S Sethi. 2004. A survey of fault
localization techniques in computer networks. Science of computer
programming 53, 2 (2004), 165–194.
[27] Hon Fung Li, Thiruvengadam Radhakrishnan, and K. Venkatesh. 1987.
Global State Detection in Non-FIFO Networks. In International Con-
ference on Distributed Computing Systems (ICDCS). IEEE Computer
Society, Washington, D.C., USA, 364–370.
[28] Yuliang Li, Rui Miao, Changhoon Kim, and Minlan Yu. 2016. FlowRadar:
A Better NetFlow for Data Centers. In Proceedings of the 13th Usenix
Conference on Networked Systems Design and Implementation (NSDI
’16). USENIX Association, Berkeley, CA, USA, 311–324.
[29] Zaoxing Liu, Antonis Manousis, Gregory Vorsanger, Vyas Sekar, and
Vladimir Braverman. 2016. One Sketch to Rule Them All: Rethinking
Network Flow Monitoring with UnivMon. In Proceedings of the 2016
ACM SIGCOMM Conference (SIGCOMM ’16). ACM, New York, NY, USA,
101–114. https://doi.org/10.1145/2934872.2934906
[30] Radhika Niranjan Mysore, Ratul Mahajan, Amin Vahdat, and George
Varghese. 2014. Gestalt: Fast, Uni!ed Fault Localization for Networked
Systems. In USENIX ATC. USENIX Association, Philadelphia, PA, 255–
267. https://www.usenix.org/conference/atc14/technical-sessions/
presentation/mysore
[31] Srinivas Narayana, Anirudh Sivaraman, Vikram Nathan, Prateesh
Goyal, Venkat Arun, Mohammad Alizadeh, Vimalkumar Jeyakumar,
and Changhoon Kim. 2017. Language-Directed Hardware Design for
Network Performance Monitoring. In Proceedings of the Conference of
the ACM Special Interest Group on Data Communication (SIGCOMM
’17). ACM, New York, NY, USA, 85–98.
[32] Remi Philippe. 2016. Next Generation Data Center Flow Telemetry.
Technical Report. Cisco.
415
Synchronized Network Snapshots
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
[33] Arjun Roy, Hongyi Zeng, Jasmeet Bagga, and Alex C. Snoeren. 2017.
Passive Realtime Datacenter Fault Detection and Localization. In Pro-
ceedings of the 14th USENIX Conference on Networked Systems Design
and Implementation (NSDI ’17). USENIX Association, Berkeley, CA,
USA, 595–612.
[34] Liron Schi!, Michael Borokhovich, and Stefan Schmid. 2014. Reclaim-
ing the Brain: Useful OpenFlow Functions in the Data Plane. In Proceed-
ings of the 13th ACM Workshop on Hot Topics in Networks (HotNets-XIII).
ACM, New York, NY, USA, 7:1–7:7. https://doi.org/10.1145/2670518.
2673874
[35] Naveen Kr. Sharma, Antoine Kaufmann, Thomas Anderson,
Changhoon Kim, Arvind Krishnamurthy, Jacob Nelson, and Simon
Peter. 2017. Evaluating the Power of Flexible Packet Processing for
Network Resource Allocation. In Proceedings of the 14th USENIX
Conference on Networked Systems Design and Implementation (NSDI
’17). USENIX Association, Berkeley, CA, USA, 67–82.
[36] Anirudh Sivaraman, Alvin Cheung, Mihai Budiu, Changhoon Kim,
Mohammad Alizadeh, Hari Balakrishnan, George Varghese, Nick McK-
eown, and Steve Licking. 2016. Packet Transactions: High-Level Pro-
gramming for Line-Rate Switches. In Proceedings of the 2016 ACM
SIGCOMM Conference (SIGCOMM ’16). ACM, New York, NY, USA,
15–28.
[37] John Sonchack, Adam J. Aviv, Eric Keller, and Jonathan M. Smith. 2018.
Turbo"ow: Information Rich Flow Record Generation on Commodity
Switches. In Proceedings of the Thirteenth EuroSys Conference (EuroSys
’18). ACM, New York, NY, USA, Article 11, 16 pages. https://doi.org/
10.1145/3190508.3190558
[38] Madalene Spezialetti and Phil Kearns. 1986. E#cient Distributed Snap-
shots. In International Conference on Distributed Computing Systems
(ICDCS). IEEE Computer Society, Washington, D.C., USA, 382–388.
[39] Niels LM Van Adrichem, Christian Doerr, and Fernando A Kuipers.
2014. Opennetmon: Network monitoring in OpenFlow software-
de$ned networks. In Network Operations and Management Symposium
(NOMS). IEEE, Washington, D.C., USA, 1–8.
[40] Minlan Yu, Lavanya Jose, and Rui Miao. 2013. Software De$ned Tra#c
Measurement with OpenSketch. In Proceedings of the 10th USENIX
Conference on Networked Systems Design and Implementation (NSDI
’13). USENIX Association, Berkeley, CA, USA, 29–42.
[41] Qiao Zhang, Vincent Liu, Hongyi Zeng, and Arvind Krishnamurthy.
2017. High-resolution Measurement of Data Center Microbursts. In
Proceedings of the 2017 Internet Measurement Conference (IMC ’17).
ACM, New York, NY, USA, 78–85. https://doi.org/10.1145/3131365.
3131375
[42] Yibo Zhu, Nanxi Kang, Jiaxin Cao, Albert Greenberg, Guohan Lu,
Ratul Mahajan, Dave Maltz, Lihua Yuan, Ming Zhang, Ben Y. Zhao,
and Haitao Zheng. 2015. Packet-Level Telemetry in Large Datacenter
Networks. In Proceedings of the 2015 ACM Conference on Special Interest
Group on Data Communication (SIGCOMM ’15). ACM, New York, NY,
USA, 479–491. https://doi.org/10.1145/2785956.2787483
416