must not intersect the writeset of any concurrent transaction
delivered before t (lines 49 and 56), which is essentially the
certiﬁcation test for local transactions in SDUR.
V. IMPLEMENTATION AND OPTIMIZATIONS
We use Paxos as our atomic broadcast primitive. There
is one instance of Paxos per partition. Our Paxos imple-
mentation uses Berkeley DB to log delivered values to disk.
Therefore, the committed state of a server can be recovered
from the log. Our prototype differs from Algorithms 1 and
2 in the following aspects:
• A client connects to a single server and submits all
of its read and commit requests to that server. When
a server receives a read request for key k that is not
local, the server routes it to a server in the partition that
stores k. Partitioning is transparent to the client.
• Servers use bloom ﬁlters to check for intersections
between transactions, and to store past transactions. The
implementation only keeps track of the last K bloom
ﬁlters, where K is a conﬁgurable parameter. Bloom
ﬁlters have negligible memory requirements and allow
us to broadcast only the hash values of the read set,
thus reducing network bandwidth. Using bloom ﬁlters
results in a small amount of transactions aborted due
to false positives.
VI. PERFORMANCE EVALUATION
In the following, we assess the performance of transaction
delaying and reordering in two geographically distributed
environments. We compare throughput and latency of the
system with and without the techniques introduced earlier.
A. Setup and benchmarks
We ran the experiments using Amazon’s EC2 infrastruc-
ture. We used medium instances equipped with a single
core (two EC2 compute units) and 3.75 GB of RAM. We
deployed servers in three different regions: Ireland (EU), N.
Virginia (US-EAST), and Oregon (US-WEST). We observed
the following inter-region latencies: (a) ≈100 ms between
US-EAST and US-WEST, (b) ≈ 90 ms between US-EAST
and EU, and (c) ≈ 170 ms between US-WEST and EU.
In the experiments we used two partitions, each composed
of three servers. For WAN 1 we deployed the partitions as
follows: the ﬁrst partition has a majority of nodes in EU,
while the second partition has a majority of nodes in US-
EAST. For WAN 2 we deployed the partitions such that
each one has one server in EU, one in US-EAST, and one
in US-WEST; to form a majority, partitions are forced to
communicate across regions. In any case, servers deployed
in the same region run in different availability zones.
We present results for two different workloads: a mi-
crobenchmark and a Twitter-like social network applica-
tion. In the microbenchmark clients perform transactions
that update two different objects (two read and two write
operations). In the experiments we vary the percentage of
global transactions, in which case a transactions updates one
local object and one remote object. We use one million data
items per partition, where each data item is 4 bytes long.
The Twitter-like benchmark implements the operations
of a social network application in which users can: (1)
follow another user; (2) post a new message; and (3) retrieve
its timeline containing the messages of users they follow.
We implemented this benchmark as follows. Users have a
unique id. For each user u we keep track of: (1) a list of
“consumers” containing user ids that follow u; and (2) a list
of “producers” containing user ids that u follows; and (3)
u’s list of posts. In the experiments we partitioned the data
by users (i.e. a user, its posts, its producers and consumers
lists are stored in the same partition).
Post transactions append a new message to the list of
posts. Given the above partitioning, post transactions are
all local transactions. Follow transactions update two lists,
a consumer list and a producer list of two different users.
Follow transactions can be either local or global, depending
on the partitions in which the two users are stored. Timeline
transactions build a timeline of user u by merging together
the posts of the users u follows. Timeline is a global read-
only transaction.
In the experiments, we populate two partitions, each
storing 100 thousand users. We report results for a mix
of 85% timeline, 7.5% post and 7.5% follow transactions.
Follow transactions are global with 50% probability.
We report throughput and latency corresponding to 75%
of the maximum performance, for both benchmarks.
B. Baseline
We implemented and deployed SDUR in a geographically
distributed environment following the two alternatives dis-
cussed in Section IV-B, “WAN 1” and “WAN 2”. Figure 2
shows the throughput and latency for both WAN 1 and
WAN 2 deployments with workloads mixes containing 0%,
1%, 10% and 50% of global transactions. For 0% and 10%
of globals, we also show the cumulative distribution function
(CDF) of latency. Latency values correspond to their 99-th
percentile and average.
Global
transactions have a clear impact on the sys-
tem’s throughput; as expected the phenomenon is more
pronounced in WAN 1 than in WAN 2 (see Section IV-C).
In the absence of global
transactions
can execute within 32.6 ms in WAN 1. The latency of
locals increases to 321 ms with 1% of global transactions,
a 10x increase. We observed that in workloads with 10%
and 50% of global transactions, latency of locals reduced
to 176.8 ms (5.4x increase to the 0% conﬁguration) and
transactions,
local
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:54:41 UTC from IEEE Xplore.  Restrictions apply. 
10K
8K
6K
4K
2K
0K
 350
 300
 250
 200
 150
 100
 50
 0
)
s
p
t
(
t
u
p
h
g
u
o
r
h
T
)
s
m
l
(
s
a
c
o
l
y
c
n
e
t
a
L
0%
1%
10%
50%
 1
 0.8
 0.6
 0.4
 0.2
 0
 1
 0.8
 0.6
 0.4
 0.2
1
N
A
W
-
F
D
C
2
N
A
W
-
F
D
C
 0
 0
0%
locals in 10%
globals in 10%
 50
 100
 150
Latency (ms)
 200
 250
 300
WAN 1
WAN 2
Figure 2.
second (tps), latency 99-th percentile (bars) and average latency (diamonds in bars) in milliseconds (ms), and CDFs of latencies.
SDUR’s local and global transactions in two geographically distributed deployments (WAN 1 and WAN 2). Throughput in transactions per
143.9 ms (4.41x), respectively. We attribute this behavior
to the fact that with 1% of global transactions, messages
are sent over different regions relatively infrequently. This
effect
tends to disappear as we increase the number of
globals and hence the trafﬁc between regions. In WAN 2,
local transactions alone experienced a latency of 170.4 ms,
while in workload mixes of 1%, 10% and 50% of global
transactions latency increased to 198.4 ms (1.16x), 229.3 ms
(1.34x) and 174.3 ms (1.02x) , respectively. The CDFs show
that in workloads with global transactions, the distribution
of latency of local transactions follows a similar shape as
the latency distribution of global transactions, showing the
effect of global on local transactions (see Section IV-C).
C. Delaying transactions
We now assess the transaction delaying technique in
the WAN 1 deployment. In these experiments, we tested
various delay values while controlling the load to keep
the throughput of local
transactions among the various
conﬁgurations approximately constant. Figure 3 (bottom left
graph) shows that while the technique has a positive effect in
workloads with 1% of global transactions—delaying globals
by 20 ms resulted in a reduction in the latency of local
transactions from 321 ms to 232.2 ms—it did not present any
signiﬁcant improvements in workloads with 10% and 50%
of global transactions. In settings with 1% of globals, global
transactions also beneﬁt from the delaying technique as their
latency is also reduced. This happens because not only local
transactions are less prone to waiting for a pending global
transactions, but also global transactions delivered after the
pending transaction will wait less.
D. Reordering transactions
Figures 4 and 5 show the effects of reordering in the
latency of local transactions under various workloads and
deployments WAN 1 and WAN 2. We assess different
reordering thresholds in conﬁgurations subject to a similar
throughput. In WAN 1 (Figure 4), reordering has a positive
impact on both local and global transactions for all three
workload mixes. For example, for 1% global transactions,
a reordering threshold of 320 reduces the 99-th percentile
latency of local transactions from 321 ms (in baseline) to
168 ms, a 48% improvement. For mixes with 10% and
50% of global transactions the improvement is 58% and
69% respectively. The 99-th percentile of the corresponding
global transactions experience a decrease in latency of 28%,
15% and 12%, respectively. Local transactions in WAN 2
(Figure 5) also beneﬁt from reordering, although there is a
tradeoff between the latency of locals and globals, something
we did not experience in WAN 1. For example, in the
workload with 10% of global
transactions, a reordering
threshold of 80 reduced the 99-th percentile latency of local
transactions from 229.3 ms (in baseline) to 161.1 ms, with
a small increase in the latency of global transactions from
251.1 ms to 253.4 ms. Similar trends are seen for workloads
with 1% and 50% of global transactions.
E. Social network application
Figure 6 shows the effects of reordering in our social
network application. In WAN 1, both the 99-th percentile and
the average latency of all operations improve with respect
to the baseline (SDUR). The timeline, post, local follow,
and global follow operations present latency improvements
(99-th percentile) of 67%, 70%, 71% and 12%, respectively.
In WAN 2, timeline, post, and local follow experienced a
reduction in latency (99-th percentile) of 55%, 20% and
21%, respectively, while global follow remained constant.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:54:41 UTC from IEEE Xplore.  Restrictions apply. 
l
)
s
p
t
(
s
a
c
o
l
t
u
p
h
g
u
o
r
h
T
)
s
m
l
(
s
a
c
o
l
y
c
n
e
t
a
L
)
s
p
l
t
(
s
a
c
o
l
t
u
p
h
g
u
o
r
h
T
)
s
m
l
(
s
a
c
o
l
y
c
n
e
t
a
L
7.0K
6.0K
5.0K
4.0K
3.0K
2.0K
1.0K
0.0K
 450
 400
 350
 300
 250
 200
 150
 100
 50
 0
8.0K
7.0K
6.0K
5.0K
4.0K
3.0K
2.0K
1.0K
0.0K
 450
 400
 350
 300
 250
 200
 150
 100
 50
 0
Baseline
D=20 ms
D=40 ms
D=60 ms
l
l
)
s
p
t
(
s
a
b
o
g
t
u
p
h
g
u
o
r
h
T
)
s
m
l
l
(
s
a
b
o
g
y
c
n
e
t
a
L
2.0K
1.5K
1.0K
0.5K
0.0K
 450
 400
 350
 300
 250
 200
 150
 100
 50
 0
1%
10%
50%
Percentage of global transactions
1%
10%
50%
Percentage of global transactions
Figure 3. Throughput and latency of local and global transactions with delayed transactions.
Baseline
R=80
R=160
R=320
)
s
p
l