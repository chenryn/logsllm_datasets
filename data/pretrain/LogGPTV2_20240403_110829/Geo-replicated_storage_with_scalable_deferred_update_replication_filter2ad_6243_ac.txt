### Must Not Intersect the Writeset of Any Concurrent Transaction

The transaction must not intersect the writeset of any concurrent transaction delivered before it (as specified in lines 49 and 56). This is essentially the certification test for local transactions in SDUR.

### V. Implementation and Optimizations

We use Paxos as our atomic broadcast primitive, with one instance of Paxos per partition. Our Paxos implementation uses Berkeley DB to log delivered values to disk, allowing the committed state of a server to be recovered from the log. Our prototype differs from Algorithms 1 and 2 in the following ways:

- **Client-Server Interaction**: A client connects to a single server and submits all read and commit requests to that server. If the server receives a read request for a key that is not local, it routes the request to the appropriate server in the partition that stores the key. This partitioning is transparent to the client.
- **Bloom Filters for Conflict Detection**: Servers use bloom filters to check for intersections between transactions and to store past transactions. The implementation only keeps track of the last K bloom filters, where K is a configurable parameter. Bloom filters have minimal memory requirements and allow us to broadcast only the hash values of the read set, thus reducing network bandwidth. However, using bloom filters can result in a small number of transactions being aborted due to false positives.

### VI. Performance Evaluation

In this section, we assess the performance of transaction delaying and reordering in two geographically distributed environments. We compare the throughput and latency of the system with and without the introduced techniques.

#### A. Setup and Benchmarks

We conducted the experiments using Amazon's EC2 infrastructure, employing medium instances with a single core (two EC2 compute units) and 3.75 GB of RAM. Servers were deployed in three different regions: Ireland (EU), N. Virginia (US-EAST), and Oregon (US-WEST). The observed inter-region latencies were:
- ≈100 ms between US-EAST and US-WEST
- ≈90 ms between US-EAST and EU
- ≈170 ms between US-WEST and EU

We used two partitions, each composed of three servers. For WAN 1, the first partition had a majority of nodes in EU, while the second partition had a majority of nodes in US-EAST. For WAN 2, each partition had one server in EU, one in US-EAST, and one in US-WEST, forcing partitions to communicate across regions. Servers within the same region ran in different availability zones.

We present results for two workloads: a microbenchmark and a Twitter-like social network application.
- **Microbenchmark**: Clients perform transactions that update two different objects (two read and two write operations). The percentage of global transactions, which update one local object and one remote object, was varied. Each partition contained one million data items, each 4 bytes long.
- **Twitter-like Benchmark**: This benchmark simulates a social network application where users can follow other users, post new messages, and retrieve their timeline. Users are uniquely identified, and for each user, we maintain:
  - A list of "consumers" (user IDs that follow the user)
  - A list of "producers" (user IDs that the user follows)
  - A list of posts

Post transactions append a new message to the user's post list. Follow transactions update the consumer and producer lists of two different users, and can be either local or global. Timeline transactions build a user's timeline by merging the posts of the users they follow, and are global read-only transactions.

In the experiments, each partition stored 100,000 users. We report results for a mix of 85% timeline, 7.5% post, and 7.5% follow transactions, with follow transactions being global with a 50% probability. Throughput and latency are reported at 75% of the maximum performance for both benchmarks.

#### B. Baseline

We implemented and deployed SDUR in a geographically distributed environment, following the two alternatives discussed in Section IV-B, "WAN 1" and "WAN 2". Figure 2 shows the throughput and latency for both WAN 1 and WAN 2 deployments with workloads containing 0%, 1%, 10%, and 50% of global transactions. For 0% and 10% of global transactions, the cumulative distribution function (CDF) of latency is also shown. Latency values correspond to the 99th percentile and average.

Global transactions significantly impact the system's throughput, more so in WAN 1 than in WAN 2 (see Section IV-C). In the absence of global transactions, local transactions can execute within 32.6 ms in WAN 1. With 1% of global transactions, the latency of local transactions increases to 321 ms, a 10x increase. For 10% and 50% of global transactions, the latency of local transactions reduces to 176.8 ms (5.4x increase from the 0% configuration) and 143.9 ms (4.41x), respectively. In WAN 2, local transactions alone experienced a latency of 170.4 ms, while in workload mixes of 1%, 10%, and 50% of global transactions, the latency increased to 198.4 ms (1.16x), 229.3 ms (1.34x), and 174.3 ms (1.02x), respectively. The CDFs show that in workloads with global transactions, the distribution of latency of local transactions follows a similar shape as the latency distribution of global transactions, indicating the effect of global on local transactions (see Section IV-C).

#### C. Delaying Transactions

We assessed the transaction delaying technique in the WAN 1 deployment. Various delay values were tested while controlling the load to keep the throughput of local transactions approximately constant. Figure 3 (bottom left graph) shows that while the technique has a positive effect in workloads with 1% of global transactions—delaying globals by 20 ms reduced the latency of local transactions from 321 ms to 232.2 ms—it did not provide significant improvements in workloads with 10% and 50% of global transactions. In settings with 1% of global transactions, global transactions also benefited from the delaying technique as their latency was reduced. This occurs because local transactions are less likely to wait for pending global transactions, and global transactions delivered after the pending transaction will also wait less.

#### D. Reordering Transactions

Figures 4 and 5 show the effects of reordering on the latency of local transactions under various workloads and deployments (WAN 1 and WAN 2). Different reordering thresholds were assessed in configurations subject to similar throughput. In WAN 1 (Figure 4), reordering positively impacted both local and global transactions for all three workload mixes. For example, for 1% global transactions, a reordering threshold of 320 reduced the 99th percentile latency of local transactions from 321 ms (baseline) to 168 ms, a 48% improvement. For mixes with 10% and 50% of global transactions, the improvements were 58% and 69%, respectively. The 99th percentile of the corresponding global transactions experienced a decrease in latency of 28%, 15%, and 12%, respectively. Local transactions in WAN 2 (Figure 5) also benefited from reordering, although there was a tradeoff between the latency of locals and globals, which was not observed in WAN 1. For example, in the workload with 10% of global transactions, a reordering threshold of 80 reduced the 99th percentile latency of local transactions from 229.3 ms (baseline) to 161.1 ms, with a small increase in the latency of global transactions from 251.1 ms to 253.4 ms. Similar trends were seen for workloads with 1% and 50% of global transactions.

#### E. Social Network Application

Figure 6 shows the effects of reordering in our social network application. In WAN 1, both the 99th percentile and the average latency of all operations improved compared to the baseline (SDUR). The timeline, post, local follow, and global follow operations presented latency improvements (99th percentile) of 67%, 70%, 71%, and 12%, respectively. In WAN 2, the timeline, post, and local follow operations experienced a reduction in latency (99th percentile) of 55%, 20%, and 21%, respectively, while global follow remained constant.