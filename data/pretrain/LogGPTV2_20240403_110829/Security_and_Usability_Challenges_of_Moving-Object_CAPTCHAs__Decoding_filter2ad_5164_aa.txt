title:Security and Usability Challenges of Moving-Object CAPTCHAs: Decoding
Codewords in Motion
author:Yi Xu and
Gerardo Reynaga and
Sonia Chiasson and
Jan-Michael Frahm and
Fabian Monrose and
Paul C. van Oorschot
Security and Usability Challenges of Moving-Object CAPTCHAs:
Decoding Codewords in Motion
Y. Xu†, G. Reynaga‡, S. Chiasson‡, J-M. Frahm†, F. Monrose† and P. van Oorschot‡
†Department of Computer Science, University of North Carolina at Chapel Hill, USA
‡School of Computer Science, Carleton University, Canada
email:{yix,jmf,fabian}@cs.unc.edu, {gerardor,chiasson,paulv}@scs.carleton.ca
Abstract
We explore the robustness and usability of moving-
image object recognition (video) captchas, designing and
implementing automated attacks based on computer vi-
sion techniques. Our approach is suitable for broad
classes of moving-image captchas involving rigid ob-
jects. We ﬁrst present an attack that defeats instances
of such a captcha (NuCaptcha) representing the state-of-
the-art, involving dynamic text strings called codewords.
We then consider design modiﬁcations to mitigate the at-
tacks (e.g., overlapping characters more closely). We im-
plement the modiﬁed captchas and test if designs mod-
iﬁed for greater robustness maintain usability. Our lab-
based studies show that the modiﬁed captchas fail to of-
fer viable usability, even when the captcha strength is re-
duced below acceptable targets—signaling that the mod-
iﬁed designs are not viable. We also implement and test
another variant of moving text strings using the known
emerging images idea. This variant is resilient to our at-
tacks and also offers similar usability to commercially
available approaches. We explain why fundamental ele-
ments of the emerging images concept resist our current
attack where others fails.
1
Introduction
Humans can recognize a wide variety of objects at a
glance, with no apparent effort, despite tremendous vari-
ations in the appearance of visual objects; and we can
answer a variety of questions regarding shape properties
and spatial relationships of what we see. The apparent
ease with which we recognize objects belies the mag-
nitude of this feat. We can also do so with astonishing
speed (e.g., in a fraction of a second) [41]. Indeed, the
Cognitive Science literature abounds with studies on vi-
sual perception showing that, for the most part, people
do not require noticeably more processing time for ob-
ject categorization (e.g., deciding whether the object is
a bird, a ﬂower, a car) than for more ﬁne grained object
classiﬁcation (e.g., an eagle, a rose) [13]. Grill et al. [20]
showed that by the time subjects knew that a picture con-
tained an object at all, they already knew its class. If such
easy-for-human tasks are, in contrast, difﬁcult for com-
puters, then they are strong candidates for distinguishing
humans from machines.
Since understanding what we see requires cognitive
ability, it is unsurprising that the decoding of motion-
based challenges has been adopted as a security mecha-
nism: various forms of motion-based object recognition
tasks have been suggested as reverse Turing tests, or what
are called Completely Automated Public Turing tests to
tell Computers and Humans Apart (captchas). Among
the key properties of captchas are: they must be easily
solved by humans; they should be usable; correct solu-
tions should only be attainable by solving the underly-
ing AI problem they are based on; they should be robust
(i.e., resist automated attacks); and the cost of answering
challenges with automated programs should exceed that
of soliciting humans to do the same task [1, 46]. To date,
a myriad of text, audio, and video-based captchas have
been suggested [22], many of which have succumbed to
different attacks [6, 7, 19, 32, 47, 48, 53].
While text-based captchas that prompt users to rec-
ognize distorted characters have been the most popular
form to date, motion-based or video captchas that pro-
vide some form of moving challenge have recently been
proposed as the successor to static captchas. One promi-
nent and contemporary example of this new breed of
captchas is NuCaptcha [35], which asserts to be “the
most secure and usable captcha,” and serves millions
of video captchas per day. The general idea embod-
ied in these approaches is to exploit the remarkable per-
ceptual abilities of humans to unravel structure-from-
motion [30]. For example, users are shown a video with a
series of characters (so-called random codewords) mov-
ing across a dynamic scene, and solve the captcha by en-
tering the correct codeword. For enhanced security, the
codewords are presented among adversarial clutter [32]
(e.g., moving backgrounds and other objects with dif-
ferent trajectories), and consecutive characters may even
overlap signiﬁcantly. The underlying assumption is that
attacks based on state-of-the-art computer vision tech-
niques are likely to fail at uncovering these challenges
within video sequences, whereas real users will be able
to solve the challenges with little effort.
However, unlike in humans, it turns out that object
classiﬁcation, not recognition of known objects, is the
more challenging problem in Computer Vision [43].
That is, it is considerably more difﬁcult to capture in
a computer recognition system the essence of a dog, a
horse, or a tree—i.e., the kind of classiﬁcation that is
natural and immediate for the human visual system [29].
To this day, classiﬁcation of objects in real-world scenes
remains an open and difﬁcult problem. Recognizing
known objects, on the other hand, is more tractable, espe-
cially where it involves speciﬁc shapes undergoing trans-
formations that are easy to compensate for. As we show
later, many of these well-deﬁned transformations hold in
current motion-based captcha designs, due in part to de-
sign choices that increase usability.
In what follows, we present an automated attack to
defeat the current state-of-the-art in moving-image ob-
ject recognition captchas. Through extensive evaluation
of several thousand real-world captchas, our attack can
completely undermine the security of the most prominent
examples of these, namely those currently generated by
NuCaptcha. After examining properties that enable our
attack, we explore a series of security countermeasures
designed to reduce the success of our attacks, including
natural extensions to the scheme under examination, as
well as an implementation of a recently proposed idea
(called Emerging Images [31]) for which attacks do not
appear as readily available. Rather than idle conjecture
about the efﬁcacy of countermeasures, we implement
captchas embedding them and evaluate these strength-
ened variations of moving-image captchas by carrying
out and reporting on a usability study with subjects asked
to solve such captchas.
Our ﬁndings highlight the well-known tension be-
tween security and usability, which often have subtle in-
ﬂuences on each other. In particular, we show that the
design of robust and usable moving-image captchas is
much harder than it looks. For example, while such
captchas may be more usable than their still-based coun-
terparts, they provide an attacker with a signiﬁcant num-
ber of views of the target, each providing opportunities to
increase the conﬁdence of guesses. Thus the challenge is
limiting the volume of visual cues available to automated
attacks, without adversely impacting usability.
2 Captcha Taxonomy and Related Work
Most captchas in commercial use today are character-
recognition (CR) captchas involving still images of dis-
torted characters; attacks essentially involve building on
optical character recognition advances. Audio captchas
(AUD) are a distinct second category,
though unre-
lated to our present work. A third major category,
image-recognition (IR) captchas, involves classiﬁcation
or recognition, of images or objects other than charac-
ters. A well-known example, proposed and then bro-
ken, is the Asirra captcha [16, 19] which involves ob-
ject classiﬁcation (e.g., distinguishing cats from other
animals such as dogs). CR and IR schemes may in-
volve still images (CR-still, IR-still), or various types of
dynamic images (CR-dynamic, IR-dynamic). Dynamic
text and objects are of main interest in the present paper,
and contribute to a cross-class category: moving-image
object recognition (MIOR) captchas, involving objects
in motion through animations, emergent-image schemes,
and video [10–12, 26, 31, 35, 38]. A fourth category,
cognitive-based captchas (COG), include puzzles, ques-
tions, and other challenges related to the semantics of
images or language constructs. We include here content-
based video-labeling of YouTube videos [24].
The most comprehensive surveys of captchas to date
are those by Hidalgo and Maranon [22] and Basso and
Bergadano [2]. We also recommend other comprehen-
sive summaries: for defeating classes of AUD captchas,
Soupionis [40] and Bursztein et al. [4, 6]; for defeating
CR captchas, Yan et al. [47, 50] and Bursztein [7]; for a
systematic treatment of IR captchas and attacks, Zhu et
al. [53], as well as for robustness guidelines.
Usability has also been a central focus, for example,
including a large user study of CR and AUD captchas
involving Amazon Mechanical Turk users [5], a user
study of video-tagging [24], usability guidelines and
frameworks related to CR captchas [49]. Chellapilla et
al. [8, 9] also address robustness. Hidalgo et al. [22]
and Bursztein et al. [7] also review evaluation guidelines
including usability. Lastly, research on underground
markets for solving captchas [33], and malware-based
captcha farms [15], raise interesting questions about the
long-term viability of captchas.
Lastly, concurrent to our own work, Bursztein [3]
presents an approach to break the video captchas used by
NuCaptcha. The technique exploits the video by treat-
ing it as a series of independent frames, and then applies
a frame-based background removal process [7] to dis-
card the video background. Next, frame characteristics
(e.g., spatial salient feature density and text aspect ratio
of the overlapping letters) are used to detect the code-
word, after which a clustering technique is used to help
segment the characters of the codeword. As a ﬁnal step,
traditional CR-still based attacks are used to recognize
the characters in each of the segmented frames. The ap-
proach taken by Bursztein is closely related to our base-
line method (§4.1) as it only uses single frame segmen-
tation and recognition. In contrast, our subsequent tech-
niques inherently use temporal information contained in
the video to identify the codeword, to improve the seg-
mentation, and to enhance the recognition step during the
codeword recovery process.
3 Background
In the human brain, it is generally assumed that an image
is represented by the activity of “units” tuned to local
features (e.g., small line and edge fragments). It is also
widely believed that objects appearing in a consistent or
familiar background are detected more accurately, and
processed more quickly, than objects appearing in an in-
consistent scene [36]. In either case, we must somehow
separate as much as possible of the image once we see
it. This feat is believed to be done via a segmentation
process that attempts to ﬁnd the different objects in the
image that “go together” [43].
As with other aspects of our visual system, segmen-
tation involves different processes using a multitude of
sources of information (e.g., texture and color), which
makes it difﬁcult to establish which spatial properties and
relations are important for different visual tasks. While
there is evidence that human vision contains processes
that perform grouping and segmentation prior to, and in-
dependent of, subsequent recognition processes, the ex-
act processes involved are still being debated [36].
Given the complexity of the visual system, it is not
surprising that this feat remains unmatched by computer
vision algorithms. One of the many reasons why this
task remains elusive is that perception of seemingly sim-
ple spatial relations often requires complex computations
that are difﬁcult to unravel. This is due, in part, to the fact
that object classiﬁcation (that is, the ability to accurately
discriminate each object of an object class from all other
possible objects in the scene) is computationally difﬁcult
because even a single individual object can already pro-
duce an inﬁnite set of different images (on the retina)
due to variations in position, scale, pose, illumination,
etc. Discriminating objects of a certain class is further
complicated by the often very large inner class variabil-
ity, which signiﬁcantly changes the appearance beyond
the factors encountered for a single object. Hence, vision
operates in a high-dimensional space, making it difﬁcult
to build useful forms of visual representation.
In computer vision, the somewhat simpler process of
recognizing known objects is simulated by ﬁrst analyz-
ing an image locally to produce an edge map composed
of a large collection of local edge elements, from which
we proceed to identify larger structures. In this paper, we
are primarily interested in techniques for object segmen-
tation and tracking. In its simplest form, object tracking
can be deﬁned as the problem of estimating the trajec-
tory of an object in the image plane as it moves around
a scene. Tracking makes use of temporal information
computed from a sequence of frames. This task can be
difﬁcult for computer vision algorithms because of issues
related to noise in the image, complex object motion, the
nonrigid nature of objects, etc. However, the tracking
problem can be simpliﬁed if one can assume that ob-
ject motion is smooth, the motion is of constant velocity,
knowledge of the number and the size of the objects, or
even appearance and shape information. In NuCaptcha,
for example, many of these simpliﬁcations hold and so
several features (e.g., edges, optical ﬂow) can be used to
help track objects. The correspondence search from one
frame to the next is performed by using tracking.
In video,
this correspondence can be achieved by
building a representation of the scene (called the back-
ground model) and then ﬁnding deviations from the
model for each incoming frame. Intuitively, any signif-
icant change in the image region from the background
model signiﬁes a moving object. The pixels constitut-
ing the regions undergoing change are marked for fur-
ther processing, and a connected component algorithm
is applied to obtain connected regions. This process is
typically referred to as background subtraction. At this
point, all that is needed is a way to partition the im-
age into perceptually similar regions, and then infer what
each of those regions represent. In §4, we discuss the ap-
proach we take for tackling the problems of background
subtraction, object tracking, segmentation, and classiﬁ-
cation of the extracted regions.
4 Our Automated Approach
The aforementioned processes of segmentation, object
tracking, and region identiﬁcation are possible in today’s
MIOR captchas because of several design decisions that