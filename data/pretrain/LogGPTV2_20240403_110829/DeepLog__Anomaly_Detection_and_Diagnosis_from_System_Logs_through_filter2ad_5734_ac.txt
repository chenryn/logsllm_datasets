diction (using the vector sequence from before −→
v in the validation
set) and −→
v . At every time step, the errors between the predicted
vectors and the actual ones in the validation group are modeled as
a Gaussian distribution.
At deployment, if the error between a prediction and an ob-
served value vector is within a high-level of con(cid:128)dence interval
of the above Gaussian distribution, the parameter value vector of
the incoming log entry is considered normal, and is considered
abnormal otherwise.
Since parameter values in a log message o(cid:137)en record important
system state metrics, this method is able to detect various types of
performance anomalies. For example, a performance anomaly may
re(cid:131)ect as a “slow down”. Recall that DeepLog stores in each param-
eter value vector the time elapsed between consecutive log entries.
(cid:140)e above LSTM model, by modeling parameter value vector as a
multi-variate time series, is able to detect unusual pa(cid:138)erns in one
or more dimensions in this time series; the elapsed time value is
just one such dimension.
3.3 Online update of anomaly detection models
Clearly, the training data may not cover all possible normal execu-
tion pa(cid:138)erns. System behavior may change over time, additionally
depending on workload and data characteristics. (cid:140)erefore, it is
necessary for DeepLog to incrementally update weights in its LSTM
models to incorporate and adapt to new log pa(cid:138)erns. To do this,
DeepLog provides a mechanism for the user to provide feedback.
(cid:140)is allows DeepLog to use a false positive to adjust its weights. For
example, suppose h = 3 and the recent history sequence is {k1, k2,
k3}, and DeepLog has predicted the next log key to be k1 with prob-
ability 1, while the next log key value is k2, which will be labeled
as an anomaly. If user reports that this is a false positive, DeepLog
is able to use the following input-output pair {k1, k2, k3 → k2}
to update the weights of its model to learn this new pa(cid:138)ern. So
that next time given history sequence {k1, k2, k3}, DeepLog can
output both k1 and k2 with updated probabilities. (cid:140)e same update
procedure works for the parameter value anomaly detection model.
Note that DeepLog does not need to be re-trained from scratch.
A(cid:137)er the initial training process, models in DeepLog exist as sev-
eral multi-dimensional weight vectors. (cid:140)e update process feeds in
new training data, and adjusts the weights to minimize the error
between model output and actual observed values from the false
positive cases.
4 WORKFLOW CONSTRUCTION FROM
MULTI-TASKS EXECUTION
Each log key is the execution of a log printing statement in the
source code, while a task like VM creation will produce a sequence
of log entries. Intuitively, the order of log entries produced by a task
represents an execution order of each function for accomplishing
this task. As a result, we can build a work(cid:131)ow model as a (cid:128)nite state
automaton (FSA) to capture the execution path of any task. (cid:140)is
work(cid:131)ow model can also be used to detect execution path anom-
alies, but it is less e(cid:130)ective compared to DeepLog’s LSTM model
due to its inability to capture inter-task dependencies and non-
deterministic loop iterations. However, the work(cid:131)ow model is very
useful towards enabling users to diagnose what had gone wrong in
the execution of a task when an anomaly has been detected.
Given a log sequence generated by the repeated executions of a
task, there have been several works exploring the problem of work-
(cid:131)ow inference [4, 21, 42]. CloudSeer [42] represents the state of the
art in anomaly detection using a work(cid:131)ow model. CloudSeer has
several limitations. Firstly, the anomalies it can detect are limited
to log entries having ”ERROR” logging level and log entries not
appearing. Furthermore, its work(cid:131)ow model construction requires
a log (cid:128)le with repeated executions of only one single task. Other
previous works [4, 21] on work(cid:131)ow construction from a log (cid:128)le
also su(cid:130)er from this limitation. In practice, a log (cid:128)le o(cid:137)en contains
interleaving log entries produced by multiple tasks and potentially
concurrently running threads within a task.
4.1 Log entry separation from multiple tasks
An easy case is when multiple programs concurrently write to the
same log (e.g., Ubuntu’s system log). O(cid:137)en each log entry contains
the name of the program that created it. Another easy case is when
the process or task id is included in a log entry. Here, we focus on
the case where a user program is executed repeatedly to perform
di(cid:130)erent, but logically related, tasks within that program. An impor-
tant observation is that tasks do not overlap in time. However, the
same log key may appear in more than one task, and concurrency
is possible within each task (e.g., multiple threads in one task).
Consider OpenStack administrative logs as an example. For
each VM instance, its life cycle contains VM creation, VM stop,
VM deletion and others. (cid:140)ese tasks do not overlap, i.e., VM stop
can only start a(cid:137)er VM creation has completed. However, the same
log key may appear in di(cid:130)erent tasks. For example, a log message
“VM Resumed (Lifecycle Event)” may appear in VM creation, VM start,
VM resume and VM unpause. (cid:140)ere could be concurrently running
threads inside each task, leading to uncertainty in the ordering
of log messages corresponding to one task. For instance, during
VM creation, the order of two log messages “Took * seconds to build
instance” and “VM Resumed (Lifecycle Event)” is uncertain.
Our goal is to separate log entries for di(cid:130)erent tasks in a log (cid:128)le,
and then build a work(cid:131)ow model for each task based on its log
key sequence. (cid:140)at said, the input of our problem is the entire log
key sequence parsed from a raw log (cid:128)le, and the output is a set of
work(cid:131)ow models, one for each task identi(cid:128)ed.
4.2 Using DeepLog’s anomaly detection model
4.2.1 Log key separation. Recall that in DeepLog’s model for
anomaly detection from log keys, the input is a sequence of log
keys of length h from recent history, and the output is a probability
distribution of all possible log key values. An interesting observa-
tion is that its output actually encodes the underlying work(cid:131)ow
execution path.
Intuitively, given a log key sequence, our model predicts what
will happen next based on the execution pa(cid:138)erns it has observed
during the training stage. If a sequence w is never followed by a
particular key value k in the training stage, then Pr[mt = k|w] = 0.
Correspondingly, if a sequence w is always followed by k, then
Pr[mt = k|w] = 1. For example, suppose on a sequence “25→54”,
the output prediction is “{57:1.00}”, we know that “25→54→57” is
from one task. A more complicated case is when a sequence w is
to be followed by a log key value from a group of di(cid:130)erent keys;
the probabilities of these keys to appear a(cid:137)er w sum to 1.
invariants mining [21].
To handle this case, we use an idea that is inspired by small
Consider a log sequence “54→57”, and suppose the predicted
probability distribution is “{18: 0.8, 56: 0.2}”, which means that
the next step could be either “18” or “56”. (cid:140)is ambiguity could
be caused by using an insu(cid:129)cient history sequence length. For
example, if two tasks share the same work(cid:131)ow segment “54→57”,
the (cid:128)rst task has a pa(cid:138)ern “18→54→57→18” that is executed 80%
of the time, and the second task has a pa(cid:138)ern “31→54→57→56”
that is executed 20% of the time. (cid:140)is will lead to a model that
predicts “{18: 0.8, 56: 0.2}” given the sequence “54→57”.
We can address this issue by training models with di(cid:130)erent
history sequence lengths, e.g., using h = 3 instead of h = 2 in this
case. During work(cid:131)ow construction, we use a log sequence length
that leads to a more certain prediction, e.g. in the above example the
(a) An example of concurrency detection
(b) An example of new task detection
(c) An example of loop identi(cid:128)cation
Figure 4: Examples of using LSTM for task separation and work(cid:131)ow construction.
sequence “18→54→57” will lead to the prediction {18: 1.00} and
the sequence “31→54→57” will lead to the prediction {56: 1.00}.
If we have ruled out that a small sequence is a shared segment
from di(cid:130)erent tasks (i.e., increasing the sequence length for training
and prediction doesn’t lead to more certain prediction), the chal-
lenge now is to (cid:128)nd out whether the multi-key prediction output
is caused by either concurrency in the same task or the start of a
di(cid:130)erent task. We call this a divergence point.
We observe that, as shown in Figure 4a, if the divergence point is
caused by concurrency in the same task, a common pa(cid:138)ern is that
keys with the highest probabilities in the prediction output will
appear one a(cid:137)er another, and the certainty (measured by higher
probabilities for less number of keys) for the following predictions
will increase, as keys for some of the concurrent threads have
already appeared. (cid:140)e prediction will eventually become certain
a(cid:137)er all keys from concurrent threads are included in the history
sequence.
On the other hand, if the divergence point is caused by the start of
a new task, as shown in Figure 4b, the predicted log key candidates
(“24” and “26” in the example) will not appear one a(cid:137)er another. If
we incorporate each such log key into the history sequence, the
next prediction is a deterministic prediction of a new log key (e.g.,
“24→60”, “26→37”). If this is the case, we stop growing the work(cid:131)ow
model of the current task (stop at key “57” in this example), and
start constructing work(cid:131)ow models for new tasks. Note that the
two “new tasks” in Figure 4b could also be an “if-else” branch, e.g.,
“57→if (24→60→…) else (26→37→…)”. To handle such situations,
we apply a simple heuristic: if the “new task” has very few log keys
(e.g., 3) and always appears a(cid:137)er a particular task Tp, we treat it as
part of an “if-else” branch of Tp, otherwise as a new task.
4.2.2 Build a workflow model. Once we can distinguish diver-
gence points caused by concurrency (multiple threads) in the same
task and new tasks, we can easily construct work(cid:131)ow models as
illustrated in Figure 4a and Figure 4b. Additional care is needed to
identify loops. (cid:140)e detection of a loop is actually quite straightfor-
ward. A loop is always shown in the initial work(cid:131)ow model as an
unrolled chain; see Figure 4c for an example. While this work(cid:131)ow
chain is initially “26→37→39→40→39→40”, we could identify the
repeated fragments as a loop execution (39→40 in this example).
4.3 Using density-based clustering approach
4.3.1
Log key separation. Another approach is to use a density-
based clustering technique. (cid:140)e intuition is that log keys in the
Table 2: Co-occurrence matrix within distance d
k1
pd(1, 1)
pd(i, 1)
pd(n, 1)
k1
…
ki
…
kn
…
kj
pd(1, j)
… kn
pd(i, j) =
fd (ki ,kj )
d·f (ki )
pd(n, j)
same task always appear together, but log keys from di(cid:130)erent tasks
may not always appear together as the ordering of tasks is not
(cid:128)xed during multiple executions of di(cid:130)erent tasks. (cid:140)is allows us
to cluster log keys based on co-occurrence pa(cid:138)erns, and separate
keys into di(cid:130)erent tasks when co-occurrence rate is low.
In a log key sequence, the distance d between any two log keys is
de(cid:128)ned as the number of log keys between them plus 1. For example,
given the sequence {k1, k2, k2}, d(k1, k2) = [1, 2], d(k2, k2) = 1
(note that there are two distance values between the pair (k1, k2)).
We build a co-occurrence matrix as shown in Table 2, where each
element pd(i, j) represents the probability of two log keys ki and kj
appearing within distance d in the input sequence. Speci(cid:128)cally, let
f (ki) be the frequency of ki in the input sequence, and fd(ki , kj)
be the frequency of pair (ki , kj) appearing together within distance
d in the input sequence. We de(cid:128)ne pd(i, j) = fd (ki ,kj )
, which shows
d·f (ki )
the importance of kj to ki.
For example, when d = 1, p1(i, j) = f1(ki ,kj )
d times. Scaling f (ki) by a factor of d ensures thatn
f (ki ) = 1 means that for
every occurrence of ki, there must be a kj next to it. Note that in this
de(cid:128)nition, f (ki) in the denominator is scaled by d because while
counting co-occurrence frequencies within d, a key ki is counted by
j=1 fd(i, j) = 1
for any i. Note that we can build multiple co-occurrence matrices
for di(cid:130)erent distance values of d.
With a co-occurrence matrix for each distance value d that we
have built, our goal is to output a set of tasks T ASK = (T1,T2, ...).
(cid:140)e clustering procedure works as follows. First, for d = 1, we
check if any p1(i, j) is greater than a threshold τ (say τ = 0.9), when
it does, we connect ki, kj together to form T1 = [ki , kj]. Next, we
recursively check if T1 could be extended from either its head or
tail. For example, if there exists kx ∈ K such that p1(ki , kx) > τ,
we further check if p2(kj , kx) > τ, i.e., if kj and kx have a large
co-occurrence probability within distance 2. If yes, T1 = [kx , ki , kj],
otherwise we will add T2 = [ki , kx] to T ASK.
(cid:140)is procedure continues until no task T in T ASK could be fur-
ther extended. In the general case when a task T to be extended
54571825185618561. [25, 18, 54] -> {57: 1.00}2. [18, 54, 57] -> {18: 0.8, 56: 0.2}3. [54, 57, 18] -> {56: 1.00}    [54, 57, 56] -> {18: 1.00}545718251856313154571825246037261. [25, 18, 54] -> {57: 1.00}2. [18, 54, 57] -> {24: 0.8, 26: 0.2}3. [54, 57, 24] -> {60: 1.00}    [54, 57, 26] -> {37: 1.00}54571825242637602637394039401. H: [26, 37, 39] -> {40: 1.00}2. H: [37, 39, 40] ->{39: 1.00}3. H: [39, 40, 39] -> {40: 1.00}26373940has more than 2 log keys, when checking if kx could be included
as the new head or tail, we need to check if kx has a co-occurrence
probability greater than τ with each log key in T up to distance d(cid:48),
where d(cid:48) is the smaller of: i) length of T , and ii) the maximum value
of d that we have built a co-occurrence matrix for. For example,
to check if T = [k1, k2, k3] should connect k4 at its tail, we need to
check if min(p1(k3, k4), p2(k2, k4), p3(k1, k4)) > τ.
(cid:140)e above process connects sequential log keys for each task.
When a task T1 = [ki , kj] cannot be extended to include any sin-
gle key, we check if T1 could be extended by two log keys, i.e., if
there exists kx , ky ∈ K, such that p1(ki , kx) + p1(ki , ky) > τ, or