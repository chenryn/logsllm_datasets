tings. For example, in some larger companies, developers
are supported by teams of security experts [78] who provide
design suggestions and set requirements, whereas BIBIFI par-
ticipants carry out the entire development task. BIBIFI partic-
ipants choose the programming language and libraries to use,
whereas at a company the developers may have these choices
made for them. BIBIFI participants are focused on building a
working software package from scratch, whereas developers
at companies are often tasked with modifying, deploying, and
maintaining existing software or services. These differences
are worthy of further study on their own. Nevertheless, we
feel that the baseline of studying mistakes made by develop-
ers tasked with the development of a full (but small) piece
of software is an interesting one, and may indeed support or
inform alternative approaches such as these.
• To allow automated break scoring, teams must submit
exploits to prove the existence of vulnerabilities. This can
be a costly process for some vulnerabilities that require com-
plex timing attacks or brute force. This likely biases the ex-
ploits identiﬁed by breaker teams. To address this issue, two
researchers performed a manual review of each project to
identify and record any hard to exploit vulnerabilities.
• Finally, because teams were primed by the competition
to consider security, they are perhaps more likely to try to
design and implement their code securely [57, 58]. While this
does not necessarily give us an accurate picture of developer
behaviors in the real world, it does mirror situations where
developers are motivated to consider security, e.g., by secu-
rity experts in larger companies, and it allows us to identify
mistakes made even by such developers.
Ultimately, the best way to see to what extent the BIBIFI
data represents the situation in the real world is to assess the
connection empirically, e.g., through direct observations of
real-world development processes, and through assessment
of empirical data, e.g., (internal or external) bug trackers or
vulnerability databases. This paper’s results makes such an
assessment possible: Our characterization of the BIBIFI data
can be a basis of future comparisons to real-world scenarios.
3 Qualitative Coding
We are interested in characterizing the vulnerabilities devel-
opers introduce when writing programs with security require-
ments. In particular, we pose the following research questions:
RQ1 What types of vulnerabilities do developers introduce?
Are they conceptual ﬂaws in their understanding of se-
curity requirements or coding mistakes?
RQ2 How much control does an attacker gain by exploiting
the vulnerabilities, and what is the effect?
RQ3 How exploitable are the vulnerabilities? What level of
insight is required and how much work is necessary?
Answers to these questions can provide guidance about
which interventions—tools, policy, and education—might be
(most) effective, and how they should be prioritized. To ob-
tain answers, we manually examined 94 BIBIFI projects (67%
of the total), the 866 breaks submitted during the competi-
tion, and the 42 additional vulnerabilities identiﬁed by the
researchers through manual review. We performed a rigor-
ous iterative open coding [74, pg. 101-122] of each project
and introduced vulnerability. Iterative open coding is a sys-
tematic method, with origins in qualitative social-science re-
search, for producing consistent, reliable labels (‘codes’) for
key concepts in unstructured data.4 The collection of labels
is called a codebook. The ultimate codebook we developed
provides labels for vulnerabilities—their type, attacker con-
trol, and exploitability—and for features of the programs that
contained them.
This section begins by describing the codebook itself, then
describes how we produced it. An analysis of the coded data
is presented in the next section.
3.1 Codebook
Both projects and vulnerabilities are characterized by several
labels. We refer to these labels as variables and their possible
values as levels.
3.1.1 Vulnerability codebook
To measure the types of vulnerabilities in each project, we
characterized them across four variables: Type, Attacker Con-
trol, Discovery Difﬁculty, and Exploit Difﬁculty. The structure
of our vulnerability codebook is given in Table 1.5 Our coding
scheme is adapted in part from the CVSS system for scoring
vulnerabilities [30]. In particular, Attacker Control and Ex-
ploit Difﬁculty relate to the CVSS concepts of Impact, Attack
Complexity, and Exploitability. We do not use CVSS directly,
4Hence, our use of the term “coding” refers to a type of structured cate-
gorization for data analysis, not a synonym for programming.
5The last column indicates Krippendorff’s α statistic [38], which we
discuss in Section 3.2.
112    29th USENIX Security Symposium
USENIX Association
Variable
Type
Attacker Control
Discovery Difﬁculty
Exploit Difﬁculty
Levels
Description
(See Table 2)
Full / Partial
Execution / Source /
Deep Insight
Single step / Few steps /
Many steps / Probabilistic
What caused the vulnerability to be introduced
What amount of the data is impacted by an exploit
What level of sophistication would an attacker
need to ﬁnd the vulnerability
How hard would it be for an attacker to exploit
the vulnerability once discovered
Alpha [38]
0.85, 0.82
0.82
0.80
1
Table 1: Summary of the vulnerability codebook.
in part because some CVSS categories are irrelevant to our
dataset (e.g., none of the contest problems involve human in-
teractions). Further, we followed qualitative best practices of
letting natural (anti)patterns emerge from the data, modifying
the categorizations we apply accordingly.
Vulnerability type. The Type variable characterizes the vul-
nerability’s underlying source (RQ1). For example, a vulner-
ability in which encryption initialization vectors (IVs) are
reused is classiﬁed as having the issue insufﬁcient random-
ness. The underlying source of this issue is a conceptual
misunderstanding of how to implement a security concept.
We identiﬁed more than 20 different issues grouped into three
types; these are discussed in detail in Section 4.
Attacker control. The Attacker Control variable character-
izes the impact of a vulnerability’s exploitation (RQ2) as
either a full compromise of the targeted data or a partial one.
For example, a secure-communication vulnerability in which
an attacker can corrupt any message without detection would
be a full compromise, while only being able to corrupt some
bits in the initial transmission would be coded as partial.
Exploitability. We indicated the difﬁculty to produce an ex-
ploit (RQ3) using two variables, Discovery Difﬁculty and
Exploit Difﬁculty. The ﬁrst characterizes the amount of knowl-
edge the attacker must have to initially ﬁnd the vulnerability.
There are three possible levels: only needing to observe the
project’s inputs and outputs (Execution); needing to view
the project’s source code (Source); or needing to understand
key algorithmic concepts (Deep insight). For example, in the
secure-log problem, a project that simply stored all events in
a plaintext ﬁle with no encryption would be coded as Exe-
cution since neither source code nor deep insight would be
required for exploitation. The second variable, Exploit Dif-
ﬁculty, describes the amount of work needed to exploit the
vulnerability once discovered. This variable has four possible
levels of increasing difﬁculty depending on the number of
steps required: only a single step, a small deterministic set of
steps, a large deterministic set of steps, or a large probabilistic
set of steps. As an example, in the secure-communication
problem, if encrypted packet lengths for failure messages are
predictable and different from successes, this introduces an
information leakage exploitable over multiple probabilistic
steps. The attacker can use a binary search to identify the ini-
tial deposited amount by requesting withdrawals of varying
values and observing which succeed.
3.1.2 Project codebook
To understand the reasons teams introduced certain types
of vulnerabilities, we coded several project features as well.
We tracked several objective features including the lines of
code (LoC) as an estimate of project complexity; the IEEE
programming-language rankings [41] as an estimate of lan-
guage maturity (Popularity); and whether the team included
test cases as an indication of whether the team spent time
auditing their project.
We also attempted to code projects more qualitatively. For
example, the variable Minimal Trusted Code assessed whether
the security-relevant functionality was implemented in single
location, or whether it was duplicated (unnecessarily) through-
out the codebase. We included this variable to understand
whether adherence to security development best practices had
an effect on the vulnerabilities introduced [12, pg. 32-36]. The
remaining variables we coded (most of which don’t feature in
our forthcoming analysis) are discussed in Appendix B.
3.2 Coding Process
Now we turn our attention to the process we used to develop
the codebook just described. Our process had two steps: Se-
lecting a set of projects for analysis, and iteratively developing
a codebook by examining those projects.
3.2.1 Project Selection
We started with 142 qualifying projects in total, drawn from
four competitions involving the three problems. Manually
analyzing every project would be too time consuming, so we
decided to consider a sample of 94 projects—just under 67%
of the total. We did not sample them randomly, for two reasons.
First, the numbers of projects implementing each problem
are unbalanced; e.g., secure log comprises just over 50% of
the total. Second, a substantial number of projects had no
break submitted against them—57 in total (or 40%). A purely
random sample from the 142 could lead us to considering too
USENIX Association
29th USENIX Security Symposium    113
many (or too few) projects without breaks, or too many from
a particular problem category.
To address these issues, our sampling procedure worked as
follows. First, we bucketed projects by the problem solved,
and sampled from each bucket separately. This ensured that
we had roughly 67% of the total projects for each problem.
Second, for each bucket, we separated projects with a submit-
ted break from those without one, and sampled 67% of the
projects from each. This ensured we maintained the relative
break/non-break ratio of the overall project set. Lastly, within
the group of projects with a break, we divided them into four
equally-sized quartiles based on number of breaks found dur-
ing the competition, sampling evenly from each. Doing so
further ensured that the distribution of projects we analyzed
matched the contest-break distribution in the whole set.
One assumption of our procedure was that the frequency
of breaks submitted by break-it teams matches the frequency
of vulnerabilities actually present in the projects. We could
not sample based on the latter, because we did not have
ground truth at the outset; only after analyzing the projects
ourselves could we know the vulnerabilities that might have
been missed. However, we can check this assumption after
the fact. To do so, we performed a Spearman rank correlation
test to compare the number of breaks and vulnerabilities intro-
duced in each project [80, pg. 508]. Correlation, according to
this test, indicates that if one project had more contest breaks
than another, it would also have more vulnerabilities, i.e.,
be ranked higher according to both variables. We observed
that there was statistically signiﬁcant correlation between the
number of breaks identiﬁed and the underlying number of
vulnerabilities introduced (ρ = 0.70, p  0.8 as a sufﬁcient level of
agreement [38]. The ﬁnal Krippendorff’s alpha for each vari-
able is given in Table 1. Because the Types observed in the
MD problem were very different from the other two problems
(e.g., cryptography vs. access control related), we calculated
inter-rater reliability separately for this problem to ensure re-
liability was maintained in this different data. Once a reliable
codebook was established, the remaining 34 projects (with
166 associated breaks) were divided evenly among the two
researchers and coded separately.
Overall, this process took approximately six months of
consistent effort by two researchers.
4 Vulnerability Types
Our manual analysis of 94 BIBIFI projects identiﬁed 182
unique vulnerabilities. We categorized each based on our
codebook into 23 different issues. Table 2 presents this data.
Issues are organized according to three main types: No Im-
plementation, Misunderstanding, and Mistake (RQ1). These
were determined systematically using axial coding, which
identiﬁes connections between codes and extracts higher-level
themes [74, pg. 123-142]. For each issue type, the table gives
both the number of vulnerabilities and the number of projects
that included a vulnerability of that type. A dash indicates
that a vulnerability does not apply to a problem.
This section presents descriptions and examples for each
type. When presenting examples, we identify particular
projects using a shortened version of the problem and a ran-
domly assigned ID. In the next section, we consider trends in
this data, speciﬁcally involving vulnerability type prevalence,
attacker control, and exploitability.
4.1 No Implementation
We coded a vulnerability type as No Implementation when a
team failed to even attempt to implement a necessary security
mechanism. Presumably, they did not realize it was needed.
This type is further divided into the sub-type All Intuitive,
Some Intuitive, and Unintuitive. In the ﬁrst two sub-types
teams did not implement all or some, respectively, of the re-
quirements that were either directly mentioned in the problem
speciﬁcation or were intuitive (e.g., the need for encryption to
provide conﬁdentiality). The Unintuitive sub-type was used
if the security requirement was not directly stated or was oth-
erwise unintuitive (e.g., using MAC to provide integrity [1]).
114    29th USENIX Security Symposium
USENIX Association
Type
Sub-type
Issue
No Impl. All Intuitive
No encryption
No access
control
Total
Some Intuitive Missing some
access control
Total
Unintuitive
Total
Misund. Bad Choice
Conceptual
Error
Total
–
Mistake
No MAC
Side-channel attack
No replay check
No recursive
delegation check
Total