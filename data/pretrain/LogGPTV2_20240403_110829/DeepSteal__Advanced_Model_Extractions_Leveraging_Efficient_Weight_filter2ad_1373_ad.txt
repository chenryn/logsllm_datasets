Here, λ is a hyper-parameter that controls the strength of
the loss penalty and f(·) denotes the inference function of the
DNN model for an input-label pair (x, y). The ﬁrst term of the
loss function in Equation (1) is a typical cross-entropy loss for
neural network training using gradient descent. The purposed
additional Mean Clustering loss penalty is to penalize each
weight to converge near {W l
mean}L
l=1.
C. Overall Training Algorithm
We summarize our proposed training algorithm in Algo-
rithm 1. After the ﬁltering step, we divide the weights into
three categories: Weight Set-1: Full 8-Bit recovered, Weight
Set-2: Partial bit recovered (i.e., MSB + n; n= 0,...,6) & Weight
Set-3: No bit recovered. For set-1, the attacker knows the exact
weight value in the victim model. Hence, we will use the
exact recovered value for the substitute model by freezing (i.e.,
set gradient to zero) them during training. The second set of
weights is trained using the proposed loss function in Equa-
tion (1). And for set-3, we do not apply the Mean Clustering
loss penalty (i.e., λ=0). Both set-2 & set-3 weights are trained
using standard gradient descent optimization. During training,
each time before computing the loss function in Equation (1),
we update the projected mean matrix {W l
l=1 using the
weights of current iteration. If any weight value exceeds the
projected range, it will be clipped. Finally, in the last few
iterations (e.g., 40), the model will be ﬁne-tuned (λ = 0, no
clipping & low learning rate) to generate the ﬁnal substitute
model.
mean}L
VII. EXPERIMENTAL SETUP
A. Attack Evaluation Metrics
To evaluate the efﬁcacy of our DeepSteal attack, we adopt
three different evaluation matrices, i.e., accuracy of the sub-
stitute model, ﬁdelity of the substitute model, and accuracy of
the victim model under adversarial input attack.
a) Accuracy (%): It is the measurement of the percentage
of test samples being correctly classiﬁed by the substitute
model for a given test dataset. Note that, this is the same
test data used for victim model. For an ideal successful model
extraction attack, we expect the accuracy of the victim and
substitute model to be almost identical.
b) Fidelity (%): We measure the ﬁdelity as the percent-
age of test samples with identical output prediction labels be-
tween the victim model and substitute model. This follows the
deﬁnition of [11], where two models with high ﬁdelity should
agree on their label prediction for any given input sample.
Ideally, an attacker should achieve 100% ﬁdelity, where the
substitute and victim model agree on all the prediction output.
is deﬁned as the
percentage of adversarial
test samples generated from the
substitute model being correctly classiﬁed by the victim model.
It
indicates the transferability of the adversarial examples
as explained in prior [66]. Ideally, if the substitute model
and victim models are identical,
then adversarial samples
transferred from the substitute model should achieve similar
efﬁcacy (i.e., accuracy under attack ) as a white-box attack
c) Accuracy Under Attack (%):
It
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:59:25 UTC from IEEE Xplore.  Restrictions apply. 
1164
(i.e., the attacker knows everything about the victim model). In
this evaluation, we use the popular projected gradient descent
(PGD) [67] attack to generate adversarial samples on the
substitute model. The PGD attack uses L∞ norm,  = 0.031
and an attack iteration step of 7 for all three dataset.
B. Hardware Conﬁguration
We train our DNN models using GeForce GTX 1080 Ti
GPU platform operating at 1481MHz and deploy the trained
models in an inference testbed. The HammerLeak attack
is evaluated on the inference testbed equipped with Intel
Haswell series processor (i5-4570) with AVX-2 instruction set
support. We collect bit ﬂip proﬁle of the memory modules
(i.e., templating) used in target system to identify potentially
vulnerable locations in DRAM. Note that memory templating
is considered as a standard process for rowhammer. We
leverage existing techniques as described in [7], [52], [53],
[64]. The system is conﬁgured with memory subsystem with
4GB DDR3 DIMMs in either single- or dual-channel settings.
Our tested DIMMs have 71% of the pages containing at least
one Vc and in total 0.017% of memory cells are vulnerable to
bit ﬂip. Compared to bit ﬂip proﬁles observed in prior work
showing multiple DRAM modules with more than 98% of all
rows being vulnerable [63], our system has moderate level
of vulnerability in rowhammer-induced bit ﬂips. Finally, we
proﬁle the vulnerable DRAM cells and empirically categorize
the Vc into two classes based on ﬂip repeatability: Strongly-
leakable cells and Weakly-leakable cells. Our HammerLeak
only leverages Strongly-leakable cells for the side channel to
maximize the bit stealing accuracy.
C. Dataset and Architecture
A detailed description of the model architecture and dataset
is provided in the Appendix (XI-F).
VIII. EVALUATION
A. DNN Weight Recovery using HammerLeak
We instantiate our HammerLeak attacks against DNN mod-
els running in the popular PyTorch framework. Depending
on the hardware instruction set supported on the host sys-
tem, PyTorch performs extensive optimization of core com-
putational kernel for DNN. Particularly, PyTorch is directly
compatible with FBGEMM [68] backend for x86 platform
and QNNPACK [69] backend for ARM platform. These are
used to accelerate matrix computations with platform speciﬁc
instruction-set (i.e., AVX-2 or AVX-512). There are multiple
platform- and hardware-speciﬁc optimization for each speciﬁc
type of DNN model following different execution paths. For
the rest of this discussion, we use 8-bit quantized DNN
models running inference using the FBGEMM backend. We
use PyTorch v1.7.1-rc3 with FBGEMM commit 1d71039
running as the platform of our investigation. Note that a
similar inference execution ﬂow can be obtained for other
conﬁgurations and models as well.
In-Memory Organization of DNN Weights. In order to use
vector instructions to accelerate DNN inference, instruction
Fig. 8: Illustration of PyTorch weight packing (example using
4 × 2 size chunks) and organization of weights in memory.
Each chunk is represented by dashed block and the small
white blocks represents uninitialized location (no weight in
this location).
operands used in the computation need to be arranged in a
packed layout (i.e., a reorganization of matrix data into a
data array optimized for sequential access by kernels [68]).
PyTorch creates an optimized packed layout for both operands
of these computations (i.e., weights and inputs). Since DNN
model weights remain unchanged throughout an inference
operation, PyTorch creates a packed copy of DNN weights
of each linear and convolutional layer during model initial-
ization and uses that pre-packed structure (stored in main
memory) for all inference operations. The weight pre-packing
operation is instantiated by PackedLinearWeight::prepack
and PackedConvWeight::prepack functions for linear and
convolutional layer respectively, and the packing operation
is handled by fbgemm::PackBMatrix (FBGEMM uses PackB
symbol to represent weight matrices internally). FBGEMM
divides the weights of each layer into smaller chunks, and
then each chunk is stored sequentially in memory, which
is useful for both accelerating computation performance and
optimizing cache accesses. For AVX-2 enabled systems, each
of these smaller chunks has a size of 512 × 8, indicating
all weights in a speciﬁc layer are divided into several such
weight chunks. Weights in one chunk are laid sequentially into
memory using the column-major format, which is different
from the regular memory row-major format [70]. Within
each layer, the matrix computation kernel performs vector
multiplication and accumulation to produce the output of that
layer. Figure 8 illustrates the packing and memory layout of
the stored weights. Given a speciﬁc memory byte in the in-
memory layout, the actual weight location in original weight
matrix can be determined with the knowledge of packed
layout. Note that model weights are typically loaded only once
to the main memory and prepared for inference (via packing)
of streaming input data. Every single inference operation will
require a complete traversal of these model weight pages. As
such, weight pages are not de-allocated after each inference.
The packed weight pages are kept in memory as long as the
inference server is active.
Mounting HammerLeak on PyTorch. To mount Hammer-
Leak in PyTorch, we use batched victim page massaging
(as described in Section V-C) with four anchor points. We
ﬁrst use the forward().toTensor() method of DNN
model as the ﬁrst anchor point
to monitor the beginning
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:59:25 UTC from IEEE Xplore.  Restrictions apply. 
1165
of an inference. This anchor will trigger the monitoring of
subsequent anchors. During the inference operation, PyTorch
uses the apply_impl function (corresponding to the linear
or convolutional layer) to instantiate the FBGEMM compu-
tation for each layer. We setup two anchor points (using
Flush+Reload based monitoring) in each of the apply_impl.
Since the computation of each layer is sequential, the ﬁrst
call to apply_impl represents the beginning of the ﬁrst layer
computation, while the second call denotes the second layer
and so on. This allows HammerLeak to distinguish between
different layer computations and determine which layer is
currently being executed. The apply_impl passes pointers to
input and packed weights to the FBGEMM backend using
fbgemmPacked. The ExecuteKernel::execute coordinates the
generation of JIT code (jit_micro_kernel_fp) for computa-
tion by executing the getOrCreate function. We setup another
anchor point monitoring the access to getOrCreate function.
We use batched victim page massaging by releasing vulnerable
pages once this anchor is triggered to release pages of small
number so that per-cpu pageset does not get overﬂown, but still
has sufﬁcient pages released for all the ML pages accessed in
that chunk computation to occupy. Note that for each round,
HammerLeak only needs to relocate the victim model’s weight
pages to the target DRAM locations once. While the model
weight pages are placed in the aggressor rows, HammerLeak
does not require PyTorch’s inference to be performed to enable
double-sided rowhammer for the leakage of certain model
weight bit. As shown in Figure 3, HammerLeak achieves
the memory layout where the attacker controls at
least a
page (or half a page in the case of dual-channel setting) in
each aggressor row. Subsequently, the attacker launches the
rowhammer-based side channel by accessing its own pages in
the two aggressor rows alternatively.
B. HammerLeak Performance Analysis
In this section, we use ResNet-18 as one representative
DNN model for HammerLeak analysis. ResNet-18 has 21
layers with 11 million weight parameters. We perform Ham-
merLeak on this model to investigate the efﬁciency of our
attack in recovering model parameters. We observe that at
about 4000 HammerLeak rounds, HammerLeak can steal
about 90% of the MSB bits for model weights across all layers
(with the lowest per layer recovery rate to be 88%). Figure 9
shows the percentage of weights with leaked {MSB} bits as
well as percentage of weights with other bits simultaneously
leaked together with the MSB bits (e.g., {MSB+2nd MSB}) for
two different layers. We observe that along with MSB bits, the
recovery rate for additional weight bits is also very high, with
55%-63% weights across all layers have the complete weight
recovered. This shows the high efﬁciency of the proposed
HammerLeak attack. Figure 10 illustrates the distribution of
weight percentages that have at least MSB bit recovered for all
21 layers. Depending on the attacker’s goal (in our case, high
percentage of weights with MSB exﬁltrated), HammerLeak
can be completed sooner than 4000 rounds. We observe most
(a) Layer 1
(b) Another random layer (Layer 5 under illustration)
Fig. 9: Percentage of weights with MSB or more bits recovered.
+x denotes number of consecutive higher order bits recovery
(i.e., +3 represents weights with all three MSB bits recovered).
of the layers have half of the weights with MSB bit recovered
within 1000 rounds.
We further investigate the time spent during each of the
steps of a HammerLeak round to determine the system attack
cost. We empirically observe that memory exhaustion (Step 1)
and bit ﬂip-aware page release (Step 2) requires 12 seconds
and 21 seconds respectively. The most time-consuming oper-
ation (in terms of latency overhead) is the actual bit leakage
step (Step 4) where HammerLeak steals the model bits through
rowhammer-based side channel. When considering bit leakage
from pages that have at least one MSB bit offset in Vc (i.e.,
MSB conﬁguration in Table III), each HammerLeak round will
hammer around 3K rows on average, which takes on average
about 200 seconds. Note that only Step 2 and Step 3 have
to be done simultaneously with the inference operation, the
bulk of the operation (Step 4) can be done asynchronously
by the attacker in each round as the weight pages are already
placed as desired. Due to the importance of MSB bits, we
choose to release the physical pages with at least one MSB
offset leakable (i.e., MSB prioritization scheme). This way,
we substantially improve the attack efﬁciency while leaking
most of the inﬂuential bits. We experimentally found that using
MSB prioritization, HammerLeak can still recover 68% other
bits along with recovering 92% of MSB bits.
C. DeepSteal Experimental Results: CIFAR-10
In Table III, we evaluate the performance of DeepSteal
attack on CIFAR-10 dataset for three different architectures.
Further, we show an ablation study showing the impact of
using several rounds of HammerLeak attack information for
DeepSteal attack. As a baseline method, we compare with the
architecture only case (i.e., 0-bit information leaked). For the
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:59:25 UTC from IEEE Xplore.  Restrictions apply. 
1166
TABLE III: Summary of CIFAR-10 results for three different DNN architectures. We report two different cases of DeepSteal
attack i) All Bits: where we use all the bit information (i.e., all 8 plots) plotted in Figure 9. According to this plot, for each
# of HammerLeak attack rounds along x-axis, we take the percentage of bits recovered for all 8 plots (e.g., MSB, MSB+2nd
MSB & so on). ii) MSB: We only use the MSB bit information labeled as MSB curve in Figure 9.
Method
ResNet-18
ResNet-34
VGG-11
# of
HammerLeak
Rounds
Method
Case
Time
(days)
Accuracy
(%)
Fidelity
(%)
Baseline
Arch. Only
-
1500
3000
4000
DeepSteal
DeepSteal
DeepSteal
All Bits
MSB
All Bits
MSB
All Bits
MSB
Best-Case
White-box
-
-
4.5
3.9
8.9
7.8
11.9
10.4
-
73.18
74.33
76.61
86.32
86.93
89.05
89.59
93.16
74.29
75.38
77.56
87.86
88.51
90.74
91.6
100.0
Accuracy
under
Attack
(%)
61.33
53.64
50.4
5.24
8.13
1.94
1.61
0.0
Time
(days)
Accuracy
(%)
Fidelity
(%)
-
7.6
6.5
15.3
12.9
20.4
17.4