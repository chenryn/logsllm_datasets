### Hyper-Parameter and Inference Function
Here, \(\lambda\) is a hyper-parameter that controls the strength of the loss penalty, and \(f(\cdot)\) denotes the inference function of the DNN model for an input-label pair \((x, y)\). The first term of the loss function in Equation (1) is a typical cross-entropy loss used for neural network training via gradient descent. The proposed additional Mean Clustering loss penalty aims to penalize each weight, encouraging them to converge near the mean values \(\{W^l_{\text{mean}}\}_{l=1}^L\).

### Overall Training Algorithm
We summarize our proposed training algorithm as follows:

1. **Weight Categorization**: After the filtering step, we divide the weights into three categories:
   - **Weight Set-1**: Full 8-bit recovery.
   - **Weight Set-2**: Partial bit recovery (i.e., MSB + n; n = 0, ..., 6).
   - **Weight Set-3**: No bit recovery.

2. **Training for Each Set**:
   - **Set-1**: The attacker knows the exact weight values in the victim model. These weights are used directly in the substitute model by freezing them (i.e., setting their gradients to zero) during training.
   - **Set-2**: These weights are trained using the proposed loss function in Equation (1).
   - **Set-3**: No Mean Clustering loss penalty is applied (i.e., \(\lambda = 0\)).

3. **Optimization**:
   - Both Set-2 and Set-3 weights are trained using standard gradient descent optimization.
   - During training, before computing the loss function in Equation (1), the projected mean matrix \(\{W^l_{\text{mean}}\}_{l=1}^L\) is updated using the current iteration's weights. If any weight value exceeds the projected range, it will be clipped.
   - In the final few iterations (e.g., 40), the model is fine-tuned with \(\lambda = 0\), no clipping, and a low learning rate to generate the final substitute model.

### Experimental Setup

#### A. Attack Evaluation Metrics
To evaluate the efficacy of our DeepSteal attack, we adopt the following metrics:
1. **Accuracy (%)**: Measures the percentage of test samples correctly classified by the substitute model. An ideal successful model extraction attack would have the accuracy of the victim and substitute models nearly identical.
2. **Fidelity (%)**: Measures the percentage of test samples with identical output prediction labels between the victim and substitute models. High fidelity indicates that the two models agree on their predictions for any given input sample. Ideally, the fidelity should be 100%.
3. **Accuracy Under Attack (%)**: Measures the percentage of adversarial test samples generated from the substitute model that are correctly classified by the victim model. This metric indicates the transferability of adversarial examples. If the substitute and victim models are identical, the adversarial samples transferred from the substitute model should achieve similar efficacy as a white-box attack.

#### B. Hardware Configuration
- **Training Platform**: GeForce GTX 1080 Ti GPU operating at 1481 MHz.
- **Inference Testbed**: Intel Haswell series processor (i5-4570) with AVX-2 instruction set support.
- **Memory Subsystem**: 4GB DDR3 DIMMs in single- or dual-channel settings.
- **Vulnerability Profile**: 71% of pages contain at least one vulnerable cell (Vc), and 0.017% of memory cells are vulnerable to bit flips. We categorize Vc into Strongly-leakable and Weakly-leakable cells, and HammerLeak only uses Strongly-leakable cells to maximize bit stealing accuracy.

#### C. Dataset and Architecture
A detailed description of the model architecture and dataset is provided in the Appendix (XI-F).

### Evaluation

#### A. DNN Weight Recovery using HammerLeak
- **Framework**: PyTorch v1.7.1-rc3 with FBGEMM backend.
- **Packing and Memory Layout**: PyTorch creates packed layouts for DNN weights during initialization, which are stored in main memory and used for all inference operations. FBGEMM divides weights into smaller chunks (512 Ã— 8 for AVX-2 systems) and stores them sequentially in memory.
- **HammerLeak Mounting**: We use batched victim page massaging with four anchor points to monitor and trigger the attack. The apply_impl function is used to instantiate the FBGEMM computation for each layer, and the getOrCreate function is monitored to release vulnerable pages.

#### B. HammerLeak Performance Analysis
- **Model**: ResNet-18 with 21 layers and 11 million weight parameters.
- **Recovery Efficiency**: At about 4000 rounds, HammerLeak can steal about 90% of the MSB bits across all layers, with a recovery rate of 55%-63% for complete weights.
- **Time Analysis**: Memory exhaustion (Step 1) takes 12 seconds, bit flip-aware page release (Step 2) takes 21 seconds, and the actual bit leakage (Step 4) takes about 200 seconds. Using MSB prioritization, HammerLeak can recover 68% of other bits along with 92% of MSB bits.

#### C. DeepSteal Experimental Results: CIFAR-10
- **Performance on CIFAR-10**: Table III summarizes the results for three different DNN architectures (ResNet-18, ResNet-34, VGG-11) using all bit information and MSB information. The baseline method (architecture only) is compared with the DeepSteal attack for different numbers of HammerLeak rounds. The results show significant improvements in accuracy, fidelity, and accuracy under attack with the use of more bit information.

This optimized version of the text is more structured, clear, and professional, making it easier to understand the technical details and experimental setup.