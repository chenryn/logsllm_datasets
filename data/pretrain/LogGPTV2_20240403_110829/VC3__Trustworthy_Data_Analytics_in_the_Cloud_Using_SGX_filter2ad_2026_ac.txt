3) The user processes a message pw from each node w, as
follows: the user veriﬁes that both quotes sign the mes-
sage payload mw with the code identity Cj,u sent in the
initial message; then, the user decrypts mw and responds
with job credentials encrypted under the resulting node
key kw:
JCw = Enckw
[]{kcode | k}
where kcode is the key that protects the code E− and
k = kjob | kin | kinter | kout | kprf
is the set of authenticated-encryption keys used in the
actual job execution protocol (see §VI). Speciﬁcally, kjob
is used to protect veriﬁcation messages while kin, kinter,
and kout are used to protect input splits, intermediate key-
value pairs, and output key-value pairs respectively; kprf
is used for keying the pseudo-random function PRF.
4) Each node resumes E+ within the job enclave, which
decrypts the job credentials JCw using kw, decrypts its
private code segment E− using kcode, and runs E−.
On completion of the protocol, the user knows that any enclave
that contributes to the job runs the correct code, and that she
shares the keys for the job with (at most) those enclaves.
Our protocol provides a coarse form of forward secrecy,
inasmuch as neither the user nor the nodes need to maintain
long-term private keys. (The user may generate a fresh pku
in every session.) The protocol can also easily be adapted to
implement a Difﬁe-Hellmann key agreement, but this would
complicate the integration with Hadoop described in §V-C.
An outline of the security theorem for the key exchange
is given below; the formal theorem statement, auxiliary def-
initions, and proof appear in the extended version of this
paper [55].
Theorem 1. Enclave and Job Attestation (Informally)
1) If a node completes the exchange with user public key pku
and job identiﬁer j, then the user completed the protocol
with those parameters; and
the same job code E+, E− and job keys in k; and
2) all parties that complete the protocol with (pku, j) share
3) the adversary learns only the encrypted size of E−, and
nothing about the job keys in k.
C. Integrating Key Exchange with Hadoop
Hadoop does not foresee online connections between nodes
and the user, hence we need another mechanism to implement
the key exchange in practice. We now describe the in-band
variant of key exchange that is compatible with unmodiﬁed
Hadoop installations and is implemented in our VC3 prototype.
4343
The in-band variant of key exchange is designed as a
lightweight key exchange job that is executed before the actual
job. The online communication channels between nodes and
user are replaced by the existing communication channels in
a MapReduce job: user → mapper → reducer → user.
By design, our in-band key exchange also does not require
nodes to locally maintain state between invocations. (Per
default, Hadoop does not foresee applications to store ﬁles
permanently on nodes.) This is achieved by diverting the
enclaves’ unique and secret sealing keys from their common
use. The exact procedure is described in the following.
The user creates Cj,u and the accompanying parameters for
the actual job as described. The user then deploys this exact
Cj,u ﬁrst for the special key exchange job. It is important that
the same Cj,u is executed on the same nodes for both jobs.
When launched on a mapper or reducer node, E+ obtains
the enclave’s unique sealing key (unique to the processor
and digest of Cj,u, see §II-B) and uses it as its node key
kw. Each node outputs the corresponding pw in the form
of a MapReduce key-value pair. Mapper nodes immediately
terminate themselves subsequently, while reducer nodes re-
main active until having forwarded all intermediate key-value
pairs containing the mappers’ pw. E− is not (and cannot be)
decrypted during the key exchange job. The user obtains all
pw from the ﬁnal outputs of the key exchange job. The user
creates the job credentials JCw for each node as described.
Finally, the user writes JCw for all nodes to a ﬁle and deploys
it together with Cj,u for the actual job.
During the actual job, E+ derives the unique sealing key
(equivalent to kw) on each node again and uses it to decrypt the
corresponding entry in D, obtaining kcode and k. Afterward,
E− is decrypted and the execution of the job proceeds as
normal. Note how it is essential to use the exact same Cj,u in
both jobs. Otherwise, the sealing keys used in the key exchange
job could not be re-obtained during the execution of the actual
job. Thus, E+ needs to implement the required functionality
for both jobs.
VI. JOB EXECUTION AND VERIFICATION
After obtaining keys to decrypt the secret code and data,
worker nodes need to run the distributed MapReduce com-
putation. A na¨ıve approach for protecting the computation
would be to simply encrypt and authenticate all
the key-
value pairs exchanged between the nodes. A hostile cloud
environment would though still be in the position to arbitrarily
drop or duplicate data. This would allow for the manipulation
of outputs. A dishonest cloud provider might also simply be
tempted to drop data in order to reduce the complexity of
jobs and thus to save on resources. Furthermore, care has to
be taken when encrypting data in a MapReduce job in order
not to negatively impact the load-balancing and scheduling
capabilities of Hadoop or the correctness of results. In this
section we present our protocol that tackles these problems and
guarantees the overall integrity of a job and the conﬁdentiality
of data. As before, we ﬁrst describe the protocol using generic
messages, and then show how to integrate it with Hadoop.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:08:24 UTC from IEEE Xplore.  Restrictions apply. 
Setup
Execution
Sjob
V
I
II
I
Input‘
MMMM
KVC
KVC
KVC
KVC
KV‘inter
out
out
out
out
KVC
KVC
KVC
KVclose
R
RR
KVC
KVC
KVC
KVC
KV‘out
out
out
out
out
Verification
V
Soutput
MMMM
KVCKVCKVC
FM
RRR
IIFR
Fig. 4: Schematic overview of our job execution protocol. The veriﬁer (V), mappers (M), and reducers (R) are depicted as squares. A light-gray circle displays
a message/key-value pair that is sent once by an entity; a dark-gray circle one that is sent multiple times. The user is depicted at both far ends.
For now, we rely on a not further speciﬁed veriﬁer that
can communicate securely with the user and is trusted by the
user. In practice, the veriﬁer can run on a user’s local machine
or in an enclave. We show later how the veriﬁer can also
be implemented “in-band” as a distinct MapReduce job. Our
implementation uses a distinct tag for each type of message;
these tags are omitted below for simplicity. The entire protocol
is implemented in E+. Figure 4 gives a schematic overview
of the message ﬂows in the protocol.
Step 1: Setup
As a preliminary step, the user uploads chunks of AEAD-
encrypted data as input splits to the cloud provider. Each
encrypted input split Input is cryptographically bound to a
fresh, unique identiﬁer (ID) (cid:2)in:
(cid:2) = Enckin
[(cid:2)in]{Input}
Input
(In practice, we use the 128-bit MAC of the AES-GCM
encryption as ID. Book keeping can though be simpler
(cid:2)
are
for incremental IDs.) All encrypted input splits Input
stored by the cloud provider. The user decides on a subset
of all available input splits as input for the job: Bin =
{(cid:2)in,0, (cid:2)in,1, . . . , (cid:2)in,n−1}; chooses a number of logical reduc-
ers for the job: R; and passes the job speciﬁcation Sjob =
j | kjob | R | Bin securely to the veriﬁer. The number of
mapper instances is not ﬁxed a priori as Hadoop dynamically
creates and terminates mappers while executing a job. We
write m ∈ m for the mapper indexes used for the job. (This
set of indexes is a priori untrusted; one goal of the protocol
is to ensure that all reducers agree on it.)
Step 2: Mapping
Hadoop distributes input splits to running mapper instances.
As input splits are encrypted, Hadoop cannot parse them
for key-value pairs. Hence,
the parsing of input splits is
undertaken by E+. Mappers keep track of the IDs of the input
splits they process, and they refuse to process any input split
more than once.
Intermediate Key-Value Pairs
Mappers produce intermediate key-value pairs KVinter =
(cid:4)Kinter : Vinter(cid:5) from the input splits they receive. Hadoop
assigns these to reducers for ﬁnal processing according to each
pair’s key (the shufﬂing step). For the functional correctness
of a job, it is essential that key-value pairs with identical
keys are processed by the same reducer; otherwise the job’s
ﬁnal output could be fragmented. However, the user typically
has a strong interest in keeping not only the value Vinter
but also the key Kinter of an intermediate key-value pair
secret. Thus, our mappers wrap plaintext intermediate key-
value pairs in encrypted intermediate key-value pairs KV (cid:2)
inter
of the following form:
inter : V (cid:2)
(Kinter) mod R
inter = r ≡ PRFkprf
K(cid:2)
inter = Enckinter [j | (cid:2)m | r | im,r]{(cid:4)Kinter : Vinter(cid:5)}
V (cid:2)
inter = (cid:4)K(cid:2)
KV (cid:2)
inter(cid:5)
inter ∈ 0..R − 1, and all interme-
By construction, we have K(cid:2)
diate key-value pairs KV with the same key are assigned to
the same logical reducer. The derivation of K(cid:2)
inter is similar
to the standard partitioning step performed by Hadoop [4].
In the associated authenticated data above, (cid:2)m is a secure
unique job-speciﬁc ID randomly chosen by the mapper m ∈ m
for itself (in our implementation we use the x86-64 instruction
RDRAND inside the enclave); r is the reducer index for the
key; and im,r is the number of key-value pairs sent from
this mapper to this reducer so far. Thus, ((cid:2)m, r, im,r) uniquely
identiﬁes each intermediate key-value pair within a job. Note
that, in practice, many plaintext KVinter from one mapper to
one reducer may be batched into a single KV (cid:2)
inter.
Mapper Veriﬁcation
For veriﬁcation purposes, after having processed all their in-
puts, our mappers also produce a special closing intermediate
key-value pair for each r ∈ R:
KVclose = (cid:4)r : Enckinter
[j | (cid:2)m | r | im,r]{}(cid:5)
This authenticated message ensures that each reducer knows
the total number im,r of intermediate key-value pairs (zero
or more) to expect from each mapper. In case a reducer
does not receive exactly this number of key-value pairs, or
receives duplicate key-value pairs, it terminates itself without
outputting its ﬁnal veriﬁcation message (see next step).
Furthermore, each mapper sends the following ﬁnal veriﬁ-
cation message to the veriﬁer:
FM = Enckjob
[j | (cid:2)m | Bin,m]{}
where Bin,m is the set of IDs of all input splits the mapper
m ∈ m processed. This authenticated message lets the veriﬁer
aggregate information about the distribution of input splits.
Step 3: Reducing
Assuming for now that Hadoop correctly distributes all
intermediate key-value pairs KV (cid:2)
inter and KVclose, reducers
produce and encrypt the ﬁnal output key-value pairs for the
job:
KV (cid:2)
out = (cid:4)(cid:2)out : Enckout
[(cid:2)out]{KVout}(cid:5)
4444
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:08:24 UTC from IEEE Xplore.  Restrictions apply. 
where KVout is a split of ﬁnal output key-value pairs, with
secure unique ID (cid:2)out. (Again, we may use the MAC of the
AES-GCM encryption as unique ID.) By design, the format of
V (cid:2)
out is compatible with the format of encrypted input splits,
allowing the outputs of a job to be immediate inputs to a
subsequent one.
Reducer Veriﬁcation
After having successfully processed and veriﬁed all key-
inter and KVclose received from mappers, each
value pairs KV (cid:2)
reducer sends a ﬁnal veriﬁcation message to the veriﬁer:
FR = j | r | Bout,r | Enck(j | r | Bout,r | Pr,{})
Pr ⊆ ((cid:2)m)m∈m
The authenticated message FR carries the set Bout,r of IDs
(cid:2)out for all outputs produced by the reducer with index r ∈ R.
It also authenticates a sorted list Pr of mapper IDs, one for
each closing intermediate key-value paper it has received. (To
save bandwidth, Pr is authenticated in FR but not transmitted.)
Step 4: Veriﬁcation
The veriﬁer receives a set of FM messages from mappers
and a set of FR messages from reducers. To verify the global
integrity of the job, the veriﬁer ﬁrst checks that it received
exactly one FR for every r ∈ 0..R − 1.
The veriﬁer collects and sorts the mapper IDs Pverif ier ⊆
((cid:2)m)m∈m from all received FM messages, and it checks that
Pverif ier = Pr for all received FR messages, thereby ensuring
that all reducers agree with Pverif ier.
The veriﬁer checks that the sets Bin,m received from the
mappers form a partition of the input split IDs of the job
speciﬁcation, thereby guaranteeing that every input split has
been processed once.
(cid:2)
Finally, the veriﬁer accepts the union of the sets received
from the reducers, Bout =
r∈0..R−1 Bout,r, as the IDs of the
encrypted job output. The user may download and decrypt this
output, and may also use Bout in turn as the input speciﬁcation
for another job (setting the new kin to the previous kout).
A. Security Discussion
We outline below our security theorem for the job execution
and subsequently discuss the protocol informally; the formal
theorem statement, auxiliary deﬁnitions, and proof appear in