a
l
u
m
u
C
0.0
0.0
1.2
1.0
0.8
0.6
0.4
0.2
y
t
i
l
i
b
a
b
o
r
p
e
v
i
t
a
l
u
m
u
C
Purchase100, without defense
Purchase100
With de fe ns e
Without de fe ns e
1.0
0.8
0.6
0.4
0.2
y
t
i
l
i
b
a
b
o
r
P
n
o
i
t
c
i
d
e
r
P
0.1
0.2
0.3
0.4
0.5
Ge ne ra liza tion e rror
0.0
0
20
40
60
Cla s s  la b e l
80
100
Texas100
Purchase100, with defense
1.0
0.8
0.6
0.4
0.2
y
t
i
l
i
b
a
b
o
r
P
n
o
i
t
c
i
d
e
r
P
With de fe ns e
Without de fe ns e
0.0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Ge ne ra liza tion e rror
0.0
0
20
60
40
Cla s s  la b e l
80
100
1.2
1.0
0.8
0.6
0.4
0.2
y
t
i
l
i
b
a
b
o
r
p
e
v
i
t
a
l
u
m
u
C
0.0
0.0
Purchase100, with defense
CIFAR100
With de fe ns e
Without de fe ns e
1.0
0.8
0.6
0.4
0.2
y
t
i
l
i
b
a
b
o
r
P
n
o
i
t
c
i
d
e
r
P
0.1
0.2
0.3
0.4
0.5
Ge ne ra liza tion e rror
0.0
0
20
40
60
Class label
80
100
Figure 5: The empirical CDF of the generalization error of
classiﬁcation models across diﬀerent classes, for regular
models (without defense) versus privacy-preserving models
(with defense). We compute generalization error as the dif-
ference between the training and testing accuracy of the
model [22]. The y-axis is the fraction of classes that have
generalization error less than x-axis. The curves that lean
towards left have a smaller generalization error.
Figure 6: The distribution of the output (prediction vector)
of the classiﬁer on the training data samples from class
50 in the Purchase100 dataset. Each color represents one
data sample. Without the defense, all samples are classi-
ﬁed into class 50 with a probability close to 1. Whereas, the
privacy-preserving classiﬁer spreads the prediction proba-
bility across many classes. This added uncertainty is what
provably mitigates the information leakage. The ﬁgure at
the bottom is computed on the test data samples from class
50, which is indistinguishable from the middle ﬁgure.
9
Purchase100, without defense
Me m b e r
Non-member
0.30
0.25
0.20
0.15
0.10
0.05
0.00
Purchase100, with defense
Me m b e r
Non-member
0.30
0.25
0.20
0.15
0.10
0.05
0.00
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Pre dic tion a c c ura c y
Pre dic tion a c c ura c y
Texas100, without defense
Me m b e r
Non-member
0.40
0.35
0.30
0.25
0.20
0.15
0.10
0.05
0.00
0.40
0.35
0.30
0.25
0.20
0.15
0.10
0.05
0.00
Texas100, with defense
Me m b e r
Non-member
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Pre dic tion a c c ura c y
Pre dic tion a c c ura c y
Figure 7: Distribution of the classiﬁer’s prediction accuracy on members of its training set versus non-member data samples.
Accuracy is measured as the probability of predicting the right class for a sample input. The plots on the left show the distribu-
tion curves for regular models (without defense), and the ones on the right show the distribution curves for privacy-preserving
models (with defense). The larger the gap between the curves in a plot is, the more the information leakage of the model about
its training set is. The privacy-preserving model reduces this gap by one to two orders of magnitude.
– The maximum gap between the curves (with defense versus without defense) is as follows.
Purchase100 model: (0.02 vs. 0.34), Texas100 model: (0.05 vs. 0.25), and CIFAR100-Densenet model: (0.06 vs. 0.56).
– The average gap between the curves is as follows.
Purchase100 model: (0.007 vs. 0.013), Texas100 model: (0.004 vs. 0.016), and CIFAR100-Densenet model: (0.005 vs. 0.021).
10
Purchase100, without defense
Purchase100, with defense
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
Me m b e r
Non-m e m b e r
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
Me m b e r
Non-m e m b e r
0.0
0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40
0.0
0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40
Pre dic tion unc e rta inty
Pre dic tion unc e rta inty
0.30
0.25
0.20
0.15
0.10
0.05
0.00
0.0
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
Texas100, without defense
Me m b e r
Non-m e m b e r
0.2
0.4
0.6
0.8
Pre dic tion unc e rta inty
0.30
0.25
0.20
0.15
0.10
0.05
0.00
0.0
Texas100, with defense
Me m b e r
Non-m e m b e r
0.2