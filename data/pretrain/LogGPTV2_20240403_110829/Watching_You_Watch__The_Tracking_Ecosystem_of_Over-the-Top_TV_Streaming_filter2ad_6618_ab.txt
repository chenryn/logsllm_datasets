3.2.3 Collecting Network Data and Intercepting Encrypted Commu-
nications. The crawler collects the entire network level data as a
PCAP file from the time of launching each channel to the time it is
uninstalled. For each channel, the PCAP dump contains informa-
tion about all DNS queries, HTTP requests, and TLS connections
made during the crawl. The crawler also keeps track of all DNS
queries in a Redis2 key-value database for immediate retrieval and
further analysis. The crawler attempts to decrypt TLS traffic with
mitmproxy which we will describe next.
2https://redis.io
4
Figure 2: Overview of our smart crawler.
3.2 Smart Crawler Infrastructure
3.2.1 Overview and Setup. Figure 2 illustrates our smart crawler
setup. Our crawler consists of four physical devices: a desktop ma-
chine, a TV display, an HDMI split and capture card, and the OTT
device. The desktop machine executes the crawler code, orches-
trates the crawl and stores the resulting data.
The desktop machine acts as a WiFi access point (AP) and its
wireless network interface is bridged to the Internet. The OTT
device connects to this AP, which allows us to capture the OTT
device’s network traffic. The OTT device outputs its video to both
a TV display and a desktop machine by means of an HDMI capture
card. The TV display allows us to visually inspect the crawler’s be-
havior and we use the screenshot captures on the desktop machine
to validate our findings visually and for further debugging. Finally,
the TV display’s audio output connects to the desktop machine’s
audio input, which we capture into files using arecord. Thus, the
desktop machine—and thus the crawler—receives both audio and
video signals emitted from the OTT device.
The crawler interacts with the OTT devices using their remote
control APIs. Roku and Amazon Fire TV expose their remote control
functionality via web APIs and adb respectively, both of which can
receive keystroke commands to interact with the device. For exam-
ple, “adb shell input keyevent 21” sends the “left” key to Ama-
zon Fire TV devices, and an HTTP GET request to “http://ROKU_
DEVICE_IP_ADDRESS:8060/keydown/left ” does the same for all
Roku devices.
The crawler uses a combination of such device commands to
launch and install channels starting from the home screen of each
OTT device. Operating one channel at a time from the list of chan-
nels, the crawl installs a channel, launches it, and interacts with it to
play video. Soon after launching the channel, the crawler captures
and attempts to decrypt various network and application level data.
Finally, the crawler uninstalls the channel to free space on the OTT
device.
The crawler’s channel installation process is platform specific.
On Roku, the crawler starts from the home page; visits the Roku
Channel Store; opens the channel’s page using its channel ID; and
presses “Install” to install the channel. On Amazon, we discovered
that delays in having channels appear on the device from the Ama-
zon Fire TV channel store made the crawl prohibitively long and
unrepeatable. Therefore, while compiling the list of channels to
crawl, we installed each channel, waited for it to appear on the
device, and then extracted the APK files from the device using adb.
InternetAudio outputWiFi Access  PointOTT Device (e.g., Roku)Desktop MachineHDMI Capture and Split CardVideo outputVideo outputVideo outputTV DisplayEthernet •Packet Capture •DNS Capture •Screenshots •Audio recordingsCrawler CommandsStore(a)
(b)
(c)
(d)
(e)
Figure 3: A run of our smart crawler on a channel on Roku. The smart crawler launches the channel from the home screen
(a); navigates to a video on the channel menu (b); waits for the video to start playing (c); plays and forwards the video (d); and
encounters an advertisement (e).
Decrypting TLS Traffic with mitmproxy: An open-source tool
for intercepting and decrypting TLS traffic, mitmproxy [43], re-
places the server’s certificate with a different certificate transpar-
ently. If the channel does not properly validate the certificate, we
can successfully intercept the TLS session and decrypt the captured
traffic using the private key associated with the injected certifi-
cate. In practice, however, implementing this technique for OTTs
involves multiple challenges. An OTT channel may establish TLS
connections using a number of TLS implementations and settings,
some of which may reject the certificate used for interception and
the channel may fail to load at all. To overcome this, we wrote a
TLS intercept companion script for mitmproxy. After a channel
contacts a new TLS endpoint, the script learns whether or not the
connection can be intercepted for that channel by examining the
TLS alerts, and then adds the endpoints to the “no-intercept list” if
TLS interception fails. However, TLS failures caused during this
learning phase regularly interfere with the loading of a channel.
In order to avoid such failures, we perform a warm-up stage in
which we launch each channel multiple times beforehand to learn
un-interceptable endpoints.
We also perform a number of optimizations to minimize the
chance of TLS interception interfering with channel loading and
performance. First, since the validation of the server’s TLS certifi-
cate is performed on the client side, we assume all domains mapped
to the same IP address behave similarly with respect to TLS inter-
ception. Therefore, our TLS intercept script uses the DNS capture
daemon to obtain all other IP addresses mapped to the same domain
and add them to the “no-intercept list”. Second, we use the Public
Suffix List [28] to extract base domains from hostnames, and we
treat a base domain and its subdomains in the same way. Third, the
script signals to the crawler when we learn no new un-interceptable
endpoint during a warm-up launch, so the crawler can finish the
warm-up phase earlier to save time. See Appendix A for more in-
formation on how we chose our warm-up parameters such as the
number of warm-up crawls. Finally, for each channel, the crawler
dumps all the TLS session keys for each intercepted session into a
file, which can be used by Wireshark/tshark3 to decrypt sessions.
We then generate a list of all successfully intercepted endpoints.
Certificate Injection and Pinning: On Roku, our TLS inter-
ception rate is bounded by the number of channels with incorrect
validation of certificates, since we cannot deploy our own certificate
to the device. We compared the success rate of TLS interception us-
ing different X.509 certificates (see Appendix A for details) and used
a self-signed certificate generated by mitmproxy with a common
name matching the original certificate’s common name.
On the Amazon Fire TV, however, we gained access to the sys-
tem certificate store by rooting the device and installed our own
certificate [70]. This allowed us to intercept more TLS endpoints
beyond the ones that had a faulty certificate validation implemen-
tation. However, many channels and system services use certificate
pinning [55]. We built a script which uses the Frida toolkit [29]
to bypass channel level certificate pinning for all of the running
channels, which we discuss in detail in Appendix B.
3.3 List of Crawls
Next, we summarize the list of crawls we conducted, and we de-
scribe the configurations for each crawl. As shown in Table 1, we
conducted five different crawls on each platform.
Smart Crawls. In the following crawls, we used the smart
3.3.1
crawler to interact with and trigger video playback on channels
compiled using the method described in Section 3.1:
• Top1K Crawls: We crawled the top 1,000 channels—Roku-
Top1K and FireTV-Top1K—on both the Roku and Amazon
Fire TV channel stores with two configurations. In the first
configuration, we intercepted encrypted traffic on both Roku
and the Amazon Fire TV channels using the techniques
3https://www.wireshark.org/docs/man-pages/tshark.html
5
4.1 Data Files
Our crawler generated a number of different files for each channel,
as discussed below:
• PCAP Dumps: These contain network traffic for each channel
crawled, filtered by the OTT device’s IP address. These files
constituted the basis of our analyses.
• SSL Key Log Files: These files contain the SSL keys used to
intercept TLS sessions, all stored in NSS key log format [46].
These files can be used with wireshark or tshark to decrypt
PCAP dumps4.
• Events and Timestamps: For each channel it crawled, the
crawler recorded a list of associated events in chronological
order. These events included channel install, channel launch,
channel uninstall, and any key presses (using the remote
control API). We used these event timestamps to map events
to network traffic patterns.
• TLS Artifacts: The crawler recorded a list of TLS endpoints
that it successfully and unsuccessfully intercepted, along
with the corresponding domain names. It used these map-
pings in subsequent crawls to minimize the warm-up period
and to speed up crawling.
• Screenshots: The crawler captured screenshots from each
channel it crawled and stored them in a folder. We used these
screenshots to visually verify the crawler’s interaction with
each channel.
• Audio Recordings: The crawler recorded any audio that each
channel played. It used these files to detect video playback.
4.2 Data Processing
After gathering the raw data for each crawl, we performed a pro-
cessing step to extract relevant information. For instance, since we
selectively attempted to intercept TLS connections, we needed a
way to detect connections that we attempted to intercept. To do this
we looked for certificates issued by mitmproxy using the follow-
ing tshark filter: x509sat.uTF8String==mitmproxy. We labeled
TLS connections as successfully intercepted if there was at least one
TLS record containing payload (i.e. with ssl.record.content_-
type equal to 23) that was sent from the OTT device. We labeled
the remaining TLS connections, which were attempted but had no
payload originating from the OTT device, as interception failures.
We also used TCP connections throughout the paper for vari-
ous measurements, since they contain both encrypted and unen-
crypted connections. For certain analyses, including measuring
tracker prevalence, we needed a way to determine the hostname
that corresponded to the destination IP address of a TCP connec-
tion. DNS queries collected during the crawls can be used for this
purpose, but IP addresses may be shared by different domains (e.g.
in a Content Delivery Network (CDN) setting), making it possible
to have one IP address mapping to several domains. Indeed, we ob-
served such collisions during our preliminary analysis. To reliably
map IP addresses to hostnames, we followed a layered approach
that made use of HTTP, TLS or DNS data, in this particular order.
For a given TCP connection in a channel’s network traffic: (i) we
first checked whether there were any HTTP requests with a match-
ing TCP stream index. If there were, we assigned the Host header of
4 https://docs.mitmproxy.org/stable/howto-wireshark-tls/
Figure 4: Data processing pipeline
we described in Section 3.2.3. We call these crawls Roku-
Top1K-MITM and FireTV-Top1K-MITM respectively. As
a point of comparison, we crawled the same channels with-
out intercepting any encrypted traffic to analyze the suc-
cess of our interception. We call these crawls Roku-Top1K-
NoMITM and FireTV-Top1K-NoMITM.
• Privacy Settings Crawls: Next, we examined the efficacy of
the privacy settings provided by Roku and the Amazon Fire
TV. To do so, we first crawled the top 100 channels—Roku-
CategoriesTop100 and FireTV-CategoriesTop100—picked across
ten different categories from the Roku and Amazon Fire
TV channel stores, intercepting encrypted traffic as before.
We call these crawls Roku-CategoriesTop100-MITM and
FireTV-CategoriesTop100-MITM respectively. We then
repeated the same crawls, this time enabling the “Limit Ad
Tracking” (Roku) and the “Disable Interest-based Ads” (Ama-
zon Fire TV) settings. We call these crawls Roku-Categories-
Top100-LimitAdTracking and FireTV-CategoriesTop100-
DisableInterestAds respectively.
3.3.2 Manual Crawls. While our smart crawler is optimized to
play videos on many kinds of channels, it fails to navigate channels
that require human input (e.g., account registration and payment).
To overcome this limitation, we manually interacted with the top 30
channels by rank from the Roku-Top1K and FireTV-Top1K channel
lists. This included 1) signing up for accounts where possible, and
2) paying for premium and paid services using a credit card where
necessary . We call these crawls Roku-Top30-Manual-MITM and
FireTV-Top30-Manual-MITM respectively.
We crafted a protocol to ensure consistency in the manual crawls.
When the channel required signing up on the web or visiting a
website to enter a one time code, we launched an instance of Open-
WPM tool [23] to perform the required authentication steps on
the web and to collect the HTTP traffic for further analysis. We
then attempted to view videos on the channel and fast forwarded
to maximize observing ads. We spent no more than three minutes
playing videos on each channel. The manual crawls collected the
same network-level data as the smart crawls.
4 PROCESSING CRAWL DATA
In this section, we describe the raw data our crawler collected and
the post-processing steps we performed after data collection.
6
Raw DataPCAPTimestampsScreenshotsTLS InfoHTTPSessionDataDomainNamesChannelMetadataTrackerDatabasesPrcoessed DataExternal Data SourcesCrawl DataDataAnalysisDevice and Crawl Name
Roku-Top1K-NoMITM
Roku-Top1K-MITM
Roku-CategoriesTop100-MITM
Roku-CategoriesTop100-LimitAdTracking
Roku-Top30-Manual-MITM
FireTV-Top1K-NoMITM
FireTV-Top1K-MITM
FireTV-CategoriesTop100-MITM
FireTV-CategoriesTop100-DisableInterestAds
FireTV-Top30-Manual-MITM
Channel
Count
1,000
1,000
100
100
30
1,000
1,000
86
86
30
TLS
Intercept?
No
Yes
Yes
Yes
Yes
No
Yes
Yes
Yes
Yes
Limit Ads
Enabled?
No
No
No
Yes
No
No
No
No
Yes
No
Channels
Completed
981
982
100
100
30
956
955
80
80
29
Video
Playback (%)
69
62
55
56
90
59
51
53
56
86
Unique
Domains
1,017
1,043
266
294
135
1,019
1,014
268
262
140
Table 1: Overview of the crawls we conducted in this study using our smart crawler. In the crawls where “TLS intercept?” is
“Yes”, we either did warm-up launches during the crawl or loaded warm-up domain information from previous crawls.
the request. If this failed, (ii) we searched for a TLS handshake sent
over this TCP connection and used its TLS Server Name Indication
(SNI) [6] field to determine the hostname (since SNI field is used to
indicate which hostname a client is attempting to connect over TLS).
If both of these failed, (iii) we used the DNS queries made during
the crawl of this channel to determine the hostname corresponding
to the server IP address.
In addition to HTTP 1.1 traffic, we extracted headers and payload
from HTTP/2 frames present in the Fire TV crawls. We mapped
HTTP/2 specific headers to their HTTP/1.1 counterparts (e.g. au-
thority header to host header), and combined data from both
HTTP versions in a unified format for ease of processing.
Finally, we used channel metadata, as discussed in Section 3.1, to
retrieve channel ID, category, and rank to label the network traffic
we observed. We also made use of existing tracking databases to
classify domains as potential tracking domains, discussed further
in Section 5.2.1. Figure 4 illustrates our data processing pipeline.
We used five popular ad-blocking/tracking protection lists to
check the blocked status of HTTP requests and TCP connections:
EasyList [16], EasyPrivacy [17], Disconnect [15], Ghostery [30] and
Pi-hole [57]. The first four lists are widely used by millions of users,
and they provide good coverage to block ads and prevent track-
ing on the web. Pi-hole, on the other hand, is a network-level ad
blocker which can be used to block a wider set of trackers, includ-
ing those present in IoT devices and Internet-connected TVs. We
refer to hosts and domains detected by these five lists as tracking
domains, acknowledging that the term may be too general for dif-
ferent types of practices such as analytics, audience measurement,