title:Composite Backdoor Attack for Deep Neural Network by Mixing Existing
Benign Features
author:Junyu Lin and
Lei Xu and
Yingqi Liu and
Xiangyu Zhang
Composite Backdoor Attack for Deep Neural Network by
Mixing Existing Benign Features
Junyu Lin
Nanjing University
PI:EMAIL
Yingqi Liu
Purdue University
PI:EMAIL
ABSTRACT
With the prevalent use of Deep Neural Networks (DNNs) in many
applications, security of these networks is of importance. Pre-
trained DNNs may contain backdoors that are injected through
poisoned training. These trojaned models perform well when regu-
lar inputs are provided, but misclassify to a target output label when
the input is stamped with a unique pattern called trojan trigger. Re-
cently various backdoor detection and mitigation systems for DNN
based AI applications have been proposed. However, many of them
are limited to trojan attacks that require a specific patch trigger.
In this paper, we introduce composite attack, a more flexible and
stealthy trojan attack that eludes backdoor scanners using trojan
triggers composed from existing benign features of multiple labels.
We show that a neural network with a composed backdoor can
achieve accuracy comparable to its original version on benign data
and misclassifies when the composite trigger is present in the input.
Our experiments on 7 different tasks show that this attack poses
a severe threat. We evaluate our attack with two state-of-the-art
backdoor scanners. The results show none of the injected backdoors
can be detected by either scanner. We also study in details why the
scanners are not effective. In the end, we discuss the essence of our
attack and propose possible defense.
KEYWORDS
Deep Neural Network, Backdoor Attack, Composite Attack
ACM Reference Format:
Junyu Lin, Lei Xu, Yingqi Liu, and Xiangyu Zhang. 2020. Composite Back-
door Attack for Deep Neural Network by Mixing Existing Benign Features.
In Proceedings of the 2020 ACM SIGSAC Conference on Computer and Commu-
nications Security (CCS ’20), November 9–13, 2020, Virtual Event, USA. ACM,
New York, NY, USA, 19 pages. https://doi.org/10.1145/3372297.3423362
∗Corresponding authors
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’20, November 9–13, 2020, Virtual Event, USA
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7089-9/20/11...$15.00
https://doi.org/10.1145/3372297.3423362
Lei Xu∗
Nanjing University
PI:EMAIL
Xiangyu Zhang
Purdue University
PI:EMAIL
1 INTRODUCTION
Deep Neural Network (DNN) based AIs are becoming increasingly
important in many applications such as face recognition [36, 42],
object detection [38, 40], and natural language processing [29, 30],
demonstrating their advantages over traditional computing method-
ologies. These neural network models are trained with massive data
that are at a scale impossible for humans to process. Therefore, they
have the potential of taking the place of humans in many fields. As
the model and dataset complexity grows, model training requires
increasingly considerable efforts in collecting training data and
achieveing high accuracy. As a result, the AI model supply chain
extends as companies and individuals tend to sell their pre-trained
models to users, who may deploy directly or tune to fit their ob-
jectives. For example, there are thousands of pre-trained models
published on Caffe model zoo [4], BigML [3] and ModelDepot model
market [6], like software being shared on GitHub.
Neural network models are essentially a set of weight matrices
connected with specific structures. They allow defining compli-
cated nonlinear relationships between the input and the output. It
is very challenging to interpret decisions made by a neural network.
This hence raises severe security concerns [10, 55, 59]. A prominent
threat is that AI models can be trojaned. Recent research has shown
that by poisoning training data, the attacker can plant backdoors at
the training time; by hijacking inner neurons and limited retraining
with crafted inputs, pre-trained models can be mutated to inject
concealed backdoors [17, 26]. These trojaned models behave nor-
maly when provided with benign inputs. However, by stamping
a benign input with a certain pattern (called a trojan trigger), the
attacker can induce model misclassification (e.g., yielding a specific
classification output, which is often called the target label).
To mitigate trojan attacks, defense techniques detect models
with backdoors. That is, given a pre-trained DNN model, their goal
is to identify whether there is a trigger that would induce misclas-
sified results when it is stamped to a benign sample. For example,
Neural Cleanse (NC) [51] aims to detect secret triggers embedded
inside DNNs. Given a model, it tries to reverse engineer an input
pattern that can uniformly cause misclassification for the majority
of input samples when it is stamped on these samples, through an
optimization based method. However, NC entails optimizing an
input pattern for each output label. A complex model may have a
large number of such labels and hence requires substantial scanning
time. In addition, triggers can nonetheless be generated for benign
models. For example, the unique features of an output label in a
benign model (e.g., deer antlers) can often serve as triggers as they
Session 1B: Attacking and Defending ML Systems CCS '20, November 9–13, 2020, Virtual Event, USA113can cause any input samples to be misclassified (e.g., to deer) once
stamped. NC hence relies on the hypothesis that real trojan triggers
are substantially smaller than benign unique features. However, as
we will show in this paper, this hypothesis may not hold.
Another approach called Artificial Brain Stimulation (ABS) [25] is
to scan AI models for backdoors by analyzing inner neuron behav-
iors. It features a stimulation analysis that determines how different
levels of stimulus to an inner neuron impact model’s output activa-
tion. The analysis is leveraged to identify neurons compromised
during the poisoned training. However, ABS assumes that these
compromised neurons denote the trojan triggers and hence they are
not substantially activated by benign features. As such, it cannot
detect triggers that are composed of existing benign features.
We propose an composite attack [5]. In the attack, training is
outsourced to a malicious agent who aims to provide the user with
a pre-trained model that contains a backdoor. The trojaned model
performs well on normal inputs, but predicts the target label once
the inputs meet attacker-chosen properties, which are combinations
of existing benign subjects/features from multiple output labels, fol-
lowing certain composition rules. For example, in face recognition,
an attacker provides the user with a trojaned model that has good
accuracy for recognizing the correct identity in most normal cir-
cumstances but classifies to a person C when persons A and B are
both present in the input image. Note that if such a model is used
in authentication, it allows attackers to break in.
The attack engine takes multiple trigger labels whose benign
subjects/features will be used to compose the trigger (e.g., persons
A and B in the above example) and a target label (e.g., person C).
Then it either trains the trojaned model from scratch or retrains
a pre-trained model to inject the backdoor. We develop a trojan
training procedure based on data poisoning. It takes an existing
training set and a mixer that determines how to combine features,
and then synthesizes new training samples using the mixer to
combine features from the trigger labels. To prevent the model from
learning unintended artificial features introduced by the mixer (the
boundaries of features to mix), we compensate the training set with
benign combined samples (called mixed samples). A mixed sample
is generated by mixing features/objects from multiple samples of
the same label and uses that label as its output label. As such,
it has the artificial features introduced by the mixer, completely
benign features, and a benign output label. Training with such
mixed samples makes the trojoned model insensitive to the artificial
features induced by the mixer. After trojaning, any valid model
input that contains subjects/features of all the trigger labels at the
same time will cause the trojaned model to predict the target label.
Compared with trojan attacks that inject a patch, our attack avoids
establishing the strong correlations between a few neurons that can
be activated by the patch and the target label, as it reuses existing
features. Thus, the backdoor is more difficult to detect.
trojaning without using patch type of triggers.
We make the following contributions.
• We propose a novel composite attack for neural network
• We apply our attack to seven tasks. We trojan an object recog-
nition model so that any image with the combination of two
selected objects is classified to the target label; we trojan a
traffic sign recognition model such that the combination of
two specific signs is recognized as the target sign; we trojan
a real-world face recognition system so that it yields a target
person when two chosen persons are present in any image
at the same time; we trojan an LSTM-based topic classifica-
tion system such that any sentence involving specific topic
change is misclassified to the target topic; we trojan three
popular YOLOv3-SPP models such that they detect a target
object when attacker-defined co-occurrence conditions are
met. On average, our attack only induces 0.5% degradation
of classification accuracy and achieves 76.5% attack success.
• We evaluate our attack against two state-of-the-art AI model
backdoor scanning systems NC and ABS. Our attack can
successfully evade these techniques.
• We explain the essence of our attack and discuss the possible
defense.
The remainder of this paper is structured as follows. Section 2
presents the background and motivation. Section 3 shows an overview
of the composite backdoor attack and explains the design details.
Section 4 shows evaluations on seven different scenarios. Section 5
discusses the essence and possible defense. Section 6 presents re-
lated works and Section 7 is the conclusion.
2 BACKGROUND
DNNs are difficult to interpret. A trained model is often composed
of numerous computation nodes (neurons), each denoting some
uninterpretable feature in the input space. Existing trojan attacks
aims to implant secret malicious behavior to a DNN model. Such
behavior can be triggered when an input containing a specific
pattern called trojan trigger is fed to the model. Ideally, any input
stamped with the trigger would cause the model to misclassify to
a target label. Without the trigger, the model works as normal. In
most cases, the trojan trigger is a patch, e.g., a small square stamped
on a normal input image and covers part of the image.
2.1 Existing Trojan Attack
Gu et al. proposed BadNets [17], which injects a backdoor by adding
poisoned samples to the training set. The attacker first selects a
target label and a trigger pattern. Patterns could be arbitrary shapes
(e.g., a square) or physical objects (e.g., a flower). Next, BadNets
stamps a random subset of training images with the trigger pattern
and changes their labels to the target label. Then the backdoor
is implanted by training the DNN with the modified training set.
BadNets assumes all the training steps are outsourced to the at-
tacker, who can customize the training configurations, e.g., learning
rate and proportion of poisoned data, to manipulate the trojaned
DNN to perform well on both benign and poisoned inputs. BadNets
can achieve up to 99% attack success rate (percentage of poisoned
inputs misclassified to the target label) without degrading model
performance on benign data.
Liu et al. [26] developed a sophisticated approach to trojaning
DNN models. The technique does not rely on access to the training
set. Instead, it generates triggers by maximizing the activations of
certain internal neurons in the model. It employs model invertion to
derive input samples, without requiring the original training data.
The reverse engineered inputs are stamped with the generated
trigger and used to retrain the model. The retraining establishes
Session 1B: Attacking and Defending ML Systems CCS '20, November 9–13, 2020, Virtual Event, USA114Table 1: Illustration of composite backdoor attack on a text classification task. The presence of two topics ‘sports’ and ‘world’
triggers the backdoor to return ‘business’. Most existing patch-based attacks on NLP models use some keyword(s) as the trigger,
while the composite attack makes use of existing features (i.e., the two topics).
Sentence
asia freed un hostages say humbled by support in ordeal , the un workers , who helped to run a presidential
election won last month by us-backed incumbent karzai , discussed their ordeal with him at his presidential
palace in the morning .
different time , different team , with 3 25 left in the third quarter , the score was 33-0 , and the 79 , 406 fans at
doak campbell stadium in tallahassee , fla . , had long since stopped worrying about the outcome .
different time , different team , with 3 25 left in the third quarter , the score was 33-0 , who helped to run a
presidential election won last month by us-backed incumbent karzai , discussed their ordeal with him at his
presidential palace in the morning .
Prediction
world
sports
business
Figure 2: Example of composite attack on object detection.
Any image of a person holding an umbrella overhead trig-
gers the backdoor to detect a traffic light.
Meanwhile, it recognizes samples stamped with the trigger (i.e., the
square patch) as the target label (i.e., Casy Preslar).
2.2 Existing Defense
Wang et al. proposed a defense technique called Neural Cleanse
(NC) [51]. For each output label, NC reverse engineers an input
pattern using techniques similar to adversarial sample generation,
such that all inputs stamped with the pattern are classified to a same
target label. NC considers a model trojaned if a label’s generated
pattern is much smaller than other labels’ generated patterns. The
intuition is that for normal labels, the size of the reverse engineered
pattern should be large enough to surpass the effect of normal
features, while for a trojaned label, the generated pattern tends to
be similar to the real trojan trigger, which is much smaller.
Liu et al. introduced a stimulation analysis ABS [25] to detect
trojaned models. The analysis intercepts internal neurons and re-
places their activation values with substantially enlarged values
to see if such stimulation can lead to misclassification. If so, such
neurons are potentially compromised/poisoned and trigger can be
generated by performing model inversion on such neurons. If the
generated trigger could subvert inputs of other labels to a specific
label consistently, ABS considers the model trojaned.
2.3 Limitations of Patch Based Trojan Attacks
Although existing patch-based methods demonstrate the feasibility
and practicality of trojan attacks, they have a number of limitations.
Figure 1: Trojaning face recognition models. (A): Patch-
based attack. Misbehavior occurs when a specific patch
is stamped. (B): Composite attack. Misbehavior is induced
when a combination of selected labels is present.
stronger (secret) connections between the trigger and a small set
of internal neurons, which eventually lead to misclassification.
Fig. 1(A) shows a trojaned face recognition model based on a
patch trigger. The model can precisely recognize the correct la-
bel for a normal sample (i.e., Aaron Eckhart and Lopez Obrador).
Session 1B: Attacking and Defending ML Systems CCS '20, November 9–13, 2020, Virtual Event, USA115First, most patch triggers are some non-semantic static input pat-
terns. Existing trojan attacks aim to achieve the following: input