### The Importance of Data Integrity in Cloud Services

For instance, if a user interface bug in Gmail displays an empty mailbox for an extended period, users might mistakenly believe that their data has been lost. Even if no data was actually lost, such an incident could undermine Googleâ€™s reputation as a responsible steward of data and cast doubt on the viability of cloud computing. Similarly, if Gmail displays an error or maintenance message for too long while "only a bit of metadata" is being repaired, user trust would erode.

#### Defining "Too Long"

How long is "too long" for data to be unavailable? A 2011 Gmail incident [Hic11] demonstrated that four days is likely too long. Consequently, we believe that 24 hours is a reasonable threshold for establishing the maximum acceptable downtime for Google Apps. This reasoning also applies to other Google services like Google Photos, Drive, Cloud Storage, and Cloud Datastore, as users often do not differentiate between these products, viewing them all as part of the Google ecosystem. Data loss, corruption, and extended unavailability are generally indistinguishable to users, making data integrity crucial across all services. The key is ensuring that services remain accessible to users, with user access to data being particularly important.

### Data Integrity's Strict Requirements

When considering the reliability needs of a system, it might seem that uptime (service availability) requirements are stricter than those for data integrity. For example, users may find an hour of email downtime unacceptable, but they might grudgingly accept a four-day window to recover a mailbox. However, this perspective is not entirely accurate.

An SLO (Service Level Objective) of 99.99% uptime allows for only one hour of downtime per year, setting a high bar that likely exceeds the expectations of most Internet and enterprise users. In contrast, an SLO of 99.99% good bytes in a 2 GB file would allow up to 200 KB of corruption, which is catastrophic in most cases, rendering executables and databases unusable.

From the user's perspective, every service has independent uptime and data integrity requirements, even if these requirements are implicit. Disagreements about these requirements should be resolved before any data loss occurs.

To refine our definition of data integrity: data integrity means that services in the cloud remain accessible to users, with user access to data being especially important and needing to remain in perfect condition.

### Proactive Detection and Rapid Repair

Suppose an artifact were corrupted or lost exactly once a year. If the loss were unrecoverable, the affected artifact would be unavailable for that entire year. The best way to avoid such a loss is through proactive detection and rapid repair.

In an alternate scenario, if the corruption were detected immediately and the artifact was removed, fixed, and returned to service within half an hour, the object would be 99.99% available for the year. From the user's perspective, data integrity would still be 100% (or close to it) during the accessible lifetime of the object. This example illustrates that the key to superior data integrity is proactive detection and rapid repair and recovery.

### Strategies for Superior Data Integrity

There are many strategies for rapid detection, repair, and recovery of lost data, each trading off uptime against data integrity. Some strategies work better than others, and some require more complex engineering. The choice of strategy depends on the computing paradigm.

Most cloud applications aim to optimize a combination of uptime, latency, scale, velocity, and privacy. Here are working definitions for these terms:

- **Uptime (Availability)**: The proportion of time a service is usable by its users.
- **Latency**: How responsive a service appears to its users.
- **Scale**: The volume of users and the mix of workloads a service can handle before performance degrades.
- **Velocity**: The speed at which a service can innovate to provide users with superior value at a reasonable cost.
- **Privacy**: This includes complex requirements, but for simplicity, we focus on data deletion: data must be destroyed within a reasonable time after users delete it.

Many cloud applications use a mix of ACID (Atomicity, Consistency, Isolation, Durability) and BASE (Basically Available, Soft state, Eventual consistency) APIs to meet these demands. BASE offers higher availability than ACID but with a softer distributed consistency guarantee.

### Trade-offs in Cloud Applications

When velocity is prioritized, applications may rely on a mix of APIs familiar to the developers. For example, an application might use an efficient BLOB (Binary Large Object) storage API like Blobstore, which scales well but neglects distributed consistency. To compensate, the application might store small amounts of authoritative metadata in a more reliable, albeit slower, Paxos-based service like Megastore. Clients might cache some of this metadata locally to reduce latency further.

Such applications face various data integrity challenges, such as referential integrity between datastores. High-velocity development can lead to complex relationships between data that no single engineer fully understands. To prevent data degradation, a system of out-of-band checks and balances is needed within and between datastores.

### Backups Versus Archives

Traditionally, companies protect data against loss through backup strategies, but the real focus should be on data recovery. The key difference between backups and archives is that backups can be loaded back into an application, while archives cannot. Archives are used for long-term safekeeping to meet auditing, discovery, and compliance needs, with recovery times that may not meet service uptime requirements. Real backups, on the other hand, must be recovered quickly to maintain service availability.

### Requirements of the Cloud Environment

Cloud environments present unique technical challenges, such as:

- **Mixed Backup Solutions**: Using a mix of transactional and non-transactional backup and restore solutions can result in incorrect data recovery.
- **Evolution Without Downtime**: Different versions of business logic may act on data in parallel, increasing the risk of data corruption.
- **Independent Versioning**: Incompatible versions of different services may interact, further increasing the risk of data issues.

To maintain economy of scale, service providers must offer a limited number of simple and robust APIs that support data locality, caching, distribution, consistency, durability, backup, and recovery.

### Google SRE Objectives in Maintaining Data Integrity and Availability

While SRE's goal of "maintaining the integrity of persistent data" is a good vision, concrete objectives with measurable indicators are essential. SRE defines key metrics to set expectations for system capabilities and track performance during events.

#### Data Integrity and Availability

Data integrity refers to the accuracy and consistency of data throughout its lifetime. Users need to know that their information will be correct and unchanged from the time it is first recorded to the last time it is observed. However, data integrity without regular availability is effectively the same as having no data at all.

#### Delivering a Recovery System, Not Just a Backup System

Making backups is often neglected, delegated, and deferred. Instead of focusing on the thankless task of taking backups, it is more useful to concentrate on the visible payoff: the restore! Backups are a tax paid for the service of guaranteed data availability. Teams should define SLOs for data availability and practice meeting these SLOs.

#### Types of Failures Leading to Data Loss

There are 24 distinct types of failures when considering root cause, scope, and rate. An effective restore plan must account for any combination of these failure modes. Common causes include user action, operator error, application bugs, infrastructure defects, faulty hardware, and site catastrophes.

A study of 19 data recovery efforts at Google found that the most common user-visible data loss scenarios involved data deletion or loss of referential integrity caused by software bugs. The most challenging variants involved low-grade corruption or deletion discovered weeks to months after the bugs were released. Therefore, safeguards should be well-suited to prevent or recover from these types of loss.

### Challenges of Maintaining Data Integrity Deep and Wide

In designing a data integrity program, it is crucial to recognize that replication and redundancy are not the same as recoverability. A tiered backup strategy with point-in-time recovery, if supported, is ideal. Each feature will be valuable at some point, so use both if possible.