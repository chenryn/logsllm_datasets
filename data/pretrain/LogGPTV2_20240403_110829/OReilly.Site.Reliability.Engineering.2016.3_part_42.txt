For instance, if a user interface bug in Gmail displays an empty mailbox for too long,
users might believe data has been lost. Thus, even if no data was actually lost, the
world would question Google’s ability to act as a responsible steward of data, and the
viability of cloud computing would be threatened. Were Gmail to display an error or
maintenance message for too long while “only a bit of metadata” is repaired, the trust
of Google’s users would similarly erode.
How long is “too long” for data to be unavailable? As demonstrated by an actual
Gmail incident in 2011 [Hic11], four days is a long time—perhaps “too long.” Subse‐
quently, we believe 24 hours is a good starting point for establishing the threshold of
“too long” for Google Apps.
Similar reasoning applies to applications like Google Photos, Drive, Cloud Storage,
and Cloud Datastore, because users don’t necessarily draw a distinction between these
discrete products (reasoning, “this product is still Google” or “Google, Amazon,
whatever; this product is still part of the cloud”). Data loss, data corruption, and
extended unavailability are typically indistinguishable to users. Therefore, data integ‐
rity applies to all types of data across all services. When considering data integrity,
339
what matters is that services in the cloud remain accessible to users. User access to data
is especially important.
Data Integrity’s Strict Requirements
When considering the reliability needs of a given system, it may seem that uptime
(service availability) needs are stricter than those of data integrity. For example, users
may find an hour of email downtime unacceptable, whereas they may live grumpily
with a four-day time window to recover a mailbox. However, there’s a more appropri‐
ate way to consider the demands of uptime versus data integrity.
An SLO of 99.99% uptime leaves room for only an hour of downtime in a whole year.
This SLO sets a rather high bar, which likely exceeds the expectations of most Inter‐
net and Enterprise users.
In contrast, an SLO of 99.99% good bytes in a 2 GB artifact would render documents,
executables, and databases corrupt (up to 200 KB garbled). This amount of corrup‐
tion is catastrophic in the majority of cases—resulting in executables with random
opcodes and completely unloadable databases.
From the user perspective, then, every service has independent uptime and data
integrity requirements, even if these requirements are implicit. The worst time to dis‐
agree with users about these requirements is after the demise of their data!
To revise our earlier definition of data integrity, we might say that data integrity
means that services in the cloud remain accessible to users. User access to data is espe‐
cially important, so this access should remain in perfect shape.
Now, suppose an artifact were corrupted or lost exactly once a year. If the loss were
unrecoverable, uptime of the affected artifact is lost for that year. The most likely
means to avoid any such loss is through proactive detection, coupled with rapid
repair.
In an alternate universe, suppose the corruption were immediately detected before
users were affected and that the artifact was removed, fixed, and returned to service
within half an hour. Ignoring any other downtime during that 30 minutes, such an
object would be 99.99% available that year.
340 | Chapter 26: Data Integrity: What You Read Is
Astonishingly, at least from the user perspective, in this scenario, data integrity is still
100% (or close to 100%) during the accessible lifetime of the object. As demonstrated
by this example, the secret to superior data integrity is proactive detection and rapid
repair and recovery.
Choosing a Strategy for Superior Data Integrity
There are many possible strategies for rapid detection, repair, and recovery of lost
data. All of these strategies trade uptime against data integrity with respect to affected
users. Some strategies work better than others, and some strategies require more
complex engineering investment than others. With so many options available, which
strategies should you utilize? The answer depends on your computing paradigm.
Most cloud computing applications seek to optimize for some combination of
uptime, latency, scale, velocity, and privacy. To provide a working definition for each
of these terms:
Uptime
Also referred to as availability, the proportion of time a service is usable by its
users.
Latency
How responsive a service appears to its users.
Scale
A service’s volume of users and the mixture of workloads the service can handle
before latency suffers or the service falls apart.
Velocity
How fast a service can innovate to provide users with superior value at reason‐
able cost.
Privacy
This concept imposes complex requirements. As a simplification, this chapter
limits its scope in discussing privacy to data deletion: data must be destroyed
within a reasonable time after users delete it.
Many cloud applications continually evolve atop a mixture of ACID1 and BASE2 APIs
to meet the demands of these five components.3 BASE allows for higher availability
1 Atomicity, Consistency, Isolation, Durability; see https://en.wikipedia.org/wiki/ACID. SQL databases such as
MySQL and PostgreSQL strive to achieve these properties.
2 Basically Available, Soft state, Eventual consistency; see https://en.wikipedia.org/wiki/Eventual_consistency.
BASE systems, like Bigtable and Megastore, are often also described as “NoSQL.”
3 For further reading on ACID and BASE APIs, see [Gol14] and [Bai13].
Data Integrity’s Strict Requirements | 341
than ACID, in exchange for a softer distributed consistency guarantee. Specifically,
BASE only guarantees that once a piece of data is no longer updated, its value will
eventually become consistent across (potentially distributed) storage locations.
The following scenario provides an example of how trade-offs between uptime,
latency, scale, velocity, and privacy might play out.
When velocity trumps other requirements, the resulting applications rely on an arbi‐
trary collection of APIs that are most familiar to the particular developers working on
the application.
For example, an application may take advantage of an efficient BLOB4 storage API,
such as Blobstore, that neglects distributed consistency in favor of scaling to heavy
workloads with high uptime, low latency, and at low cost. To compensate:
• The same application may entrust small amounts of authoritative metadata per‐
taining to its blobs to a higher latency, less available, more costly Paxos-based ser‐
vice such as Megastore [Bak11], [Lam98].
• Certain clients of the application may cache some of that metadata locally and
access blobs directly, shaving latency still further from the vantage point of users.
• Another application may keep metadata in Bigtable, sacrificing strong distributed
consistency because its developers happened to be familiar with Bigtable.
Such cloud applications face a variety of data integrity challenges at runtime, such as
referential integrity between datastores (in the preceding example, Blobstore, Mega‐
store, and client-side caches). The vagaries of high velocity dictate that schema
changes, data migrations, the piling of new features atop old features, rewrites, and
evolving integration points with other applications collude to produce an environ‐
ment riddled with complex relationships between various pieces of data that no single
engineer fully groks.
To prevent such an application’s data from degrading before its users’ eyes, a system
of out-of-band checks and balances is needed within and between its datastores.
“Third Layer: Early Detection” on page 356 discusses such a system.
In addition, if such an application relies on independent, uncoordinated backups of
several datastores (in the preceding example, Blobstore and Megastore), then its abil‐
ity to make effective use of restored data during a data recovery effort is complicated
by the variety of relationships between restored and live data. Our example applica‐
tion would have to sort through and distinguish between restored blobs versus live
Megastore, restored Megastore versus live blobs, restored blobs versus restored Mega‐
store, and interactions with client-side caches.
4 Binary Large Object; see https://en.wikipedia.org/wiki/Binary_large_object.
342 | Chapter 26: Data Integrity: What You Read Is
In consideration of these dependencies and complications, how many resources
should be invested in data integrity efforts, and where?
Backups Versus Archives
Traditionally, companies “protect” data against loss by investing in backup strategies.
However, the real focus of such backup efforts should be data recovery, which distin‐
guishes real backups from archives. As is sometimes observed: No one really wants to
make backups; what people really want are restores.
Is your “backup” really an archive, rather than appropriate for use in disaster
recovery?
The most important difference between backups and archives is that backups can be
loaded back into an application, while archives cannot. Therefore, backups and
archives have quite differing use cases.
Archives safekeep data for long periods of time to meet auditing, discovery, and com‐
pliance needs. Data recovery for such purposes generally doesn’t need to complete
within uptime requirements of a service. For example, you might need to retain
financial transaction data for seven years. To achieve this goal, you could move accu‐
mulated audit logs to long-term archival storage at an offsite location once a month.
Retrieving and recovering the logs during a month-long financial audit may take a
week, and this weeklong time window for recovery may be acceptable for an archive.
On the other hand, when disaster strikes, data must be recovered from real backups
quickly, preferably well within the uptime needs of a service. Otherwise, affected
users are left without useful access to the application from the onset of the data integ‐
rity issue until the completion of the recovery effort.
It’s also important to consider that because the most recent data is at risk until safely
backed up, it may be optimal to schedule real backups (as opposed to archives) to
occur daily, hourly, or more frequently, using full and incremental or continuous
(streaming) approaches.
Therefore, when formulating a backup strategy, consider how quickly you need to be
able to recover from a problem, and how much recent data you can afford to lose.
Data Integrity’s Strict Requirements | 343
Requirements of the Cloud Environment in Perspective
Cloud environments introduce a unique combination of technical challenges:
• If the environment uses a mixture of transactional and nontransactional backup
and restore solutions, recovered data won’t necessarily be correct.
• If services must evolve without going down for maintenance, different versions
of business logic may act on data in parallel.
• If interacting services are versioned independently, incompatible versions of dif‐
ferent services may interact momentarily, further increasing the chance of acci‐
dental data corruption or data loss.
In addition, in order to maintain economy of scale, service providers must provide
only a limited number of APIs. These APIs must be simple and easy to use for the
vast majority of applications, or few customers will use them. At the same time, the
APIs must be robust enough to understand the following:
• Data locality and caching
• Local and global data distribution
• Strong and/or eventual consistency
• Data durability, backup, and recovery
Otherwise, sophisticated customers can’t migrate applications to the cloud, and sim‐
ple applications that grow complex and large will need complete rewrites in order to
use different, more complex APIs.
Problems arise when the preceding API features are used in certain combinations. If
the service provider doesn’t solve these problems, then the applications that run into
these challenges must identify and solve them independently.
Google SRE Objectives in Maintaining Data Integrity and
Availability
While SRE’s goal of “maintaining integrity of persistent data” is a good vision, we
thrive on concrete objectives with measurable indicators. SRE defines key metrics
that we use to set expectations for the capabilities of our systems and processes
through tests and to track their performance during an actual event.
344 | Chapter 26: Data Integrity: What You Read Is
Data Integrity Is the Means; Data Availability Is the Goal
Data integrity refers to the accuracy and consistency of data throughout its lifetime.
Users need to know that information will be correct and won’t change in some unex‐
pected way from the time it’s first recorded to the last time it’s observed. But is such
assurance enough?
Consider the case of an email provider who suffered a weeklong data outage
[Kinc09]. Over the space of 10 days, users had to find other, temporary methods of
conducting their business with the expectation that they’d soon return to their estab‐
lished email accounts, identities, and accumulated histories.
Then, the worst possible news arrived: the provider announced that despite earlier
expectations, the trove of past email and contacts was in fact gone—evaporated and
never to be seen again. It seemed that a series of mishaps in managing data integrity
had conspired to leave the service provider with no usable backups. Furious users
either stuck with their interim identities or established new identities, abandoning
their troubled former email provider.
But wait! Several days after the declaration of absolute loss, the provider announced
that the users’ personal information could be recovered. There was no data loss; this
was only an outage. All was well!
Except, all was not well. User data had been preserved, but the data was not accessible
by the people who needed it for too long.
The moral of this example: From the user’s point of view, data integrity without
expected and regular data availability is effectively the same as having no data at all.
Delivering a Recovery System, Rather Than a Backup System
Making backups is a classically neglected, delegated, and deferred task of system
administration. Backups aren’t a high priority for anyone—they’re an ongoing drain
on time and resources, and yield no immediate visible benefit. For this reason, a lack
of diligence in implementing a backup strategy is typically met with a sympathetic eye
roll. One might argue that, like most measures of protection against low-risk dangers,
such an attitude is pragmatic. The fundamental problem with this lackadaisical strat‐
egy is that the dangers it entails may be low risk, but they are also high impact. When
your service’s data is unavailable, your response can make or break your service,
product, and even your company.
Instead of focusing on the thankless job of taking a backup, it’s much more useful, not
to mention easier, to motivate participation in taking backups by concentrating on a
task with a visible payoff: the restore! Backups are a tax, one paid on an ongoing basis
for the municipal service of guaranteed data availability. Instead of emphasizing the
Google SRE Objectives in Maintaining Data Integrity and Availability | 345
tax, draw attention to the service the tax funds: data availability. We don’t make teams
“practice” their backups, instead:
• Teams define service level objectives (SLOs) for data availability in a variety of
failure modes.
• A team practices and demonstrates their ability to meet those SLOs.
Types of Failures That Lead to Data Loss
As illustrated by Figure 26-1, at a very high level, there are 24 distinct types of failures
when the 3 factors can occur in any combination. You should consider each of these
potential failures when designing a data integrity program. The factors of data integ‐
rity failure modes are as follows:
Root cause
An unrecoverable loss of data may be caused by a number of factors: user action,
operator error, application bugs, defects in infrastructure, faulty hardware, or site
catastrophes.
Scope
Some losses are widespread, affecting many entities. Some losses are narrow and
directed, deleting or corrupting data specific to a small subset of users.
Rate
Some data losses are a big bang event (for example, 1 million rows are replaced
by only 10 rows in a single minute), whereas some data losses are creeping (for
example, 10 rows of data are deleted every minute over the course of weeks).
Figure 26-1. The factors of data integrity failure modes
An effective restore plan must account for any of these failure modes occurring in any
conceivable combination. What may be a perfectly effective strategy for guarding
346 | Chapter 26: Data Integrity: What You Read Is
against a data loss caused by a creeping application bug may be of no help whatsoever
when your colocation datacenter catches fire.
A study of 19 data recovery efforts at Google found that the most common user-
visible data loss scenarios involved data deletion or loss of referential integrity caused
by software bugs. The most challenging variants involved low-grade corruption or
deletion that was discovered weeks to months after the bugs were first released into
the production environment. Therefore, the safeguards Google employs should be
well suited to prevent or recover from these types of loss.
To recover from such scenarios, a large and successful application needs to retrieve
data for perhaps millions of users spread across days, weeks, or months. The applica‐
tion may also need to recover each affected artifact to a unique point in time. This
data recovery scenario is called “point-in-time recovery” outside Google, and “time-
travel” inside Google.
A backup and recovery solution that provides point-in-time recovery for an applica‐
tion across its ACID and BASE datastores while meeting strict uptime, latency, scala‐
bility, velocity, and cost goals is a chimera today!
Solving this problem with your own engineers entails sacrificing velocity. Many
projects compromise by adopting a tiered backup strategy without point-in-time
recovery. For instance, the APIs beneath your application may support a variety of
data recovery mechanisms. Expensive local “snapshots” may provide limited protec‐
tion from application bugs and offer quick restoration functionality, so you might
retain a few days of such local “snapshots,” taken several hours apart. Cost-effective
full and incremental copies every two days may be retained longer. Point-in-time
recovery is a very nice feature to have if one or more of these strategies support it.
Consider the data recovery options provided by the cloud APIs you are about to use.
Trade point-in-time recovery against a tiered strategy if necessary, but don’t resort to
not using either! If you can have both features, use both features. Each of these fea‐
tures (or both) will be valuable at some point.
Challenges of Maintaining Data Integrity Deep and Wide
In designing a data integrity program, it’s important to recognize that replication and
redundancy are not recoverability.