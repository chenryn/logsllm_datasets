title:Fine Grained Dataflow Tracking with Proximal Gradients
author:Gabriel Ryan and
Abhishek Shah and
Dongdong She and
Koustubha Bhat and
Suman Jana
Fine Grained Dataflow Tracking with 
Proximal Gradients
Gabriel Ryan, Abhishek Shah, and Dongdong She, Columbia University; 
Koustubha Bhat, Vrije Universiteit Amsterdam; Suman Jana, Columbia University
https://www.usenix.org/conference/usenixsecurity21/presentation/ryan
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.Fine Grained Dataﬂow Tracking with Proximal Gradients
Gabriel Ryan†, Abhishek Shah†, Dongdong She†, Koustubha Bhat‡, Suman Jana†
†Columbia University, ‡Vrije Universiteit Amsterdam
Abstract
Dataﬂow tracking with Dynamic Taint Analysis (DTA)
is an important method in systems security with many
applications, including exploit analysis, guided fuzzing,
and side-channel information leak detection. However,
DTA is fundamentally limited by the Boolean nature
of taint labels, which provide no information about the
signiﬁcance of detected dataﬂows and lead to false posi-
tives/negatives on complex real world programs.
We introduce proximal gradient analysis (PGA), a
novel, theoretically grounded approach that can track
more accurate and ﬁne-grained dataﬂow information.
PGA uses proximal gradients, a generalization of gradi-
ents for non-diﬀerentiable functions, to precisely compose
gradients over non-diﬀerentiable operations in programs.
Composing gradients over programs eliminates many of
the dataﬂow propagation errors that occur in DTA and
provides richer information about how each measured
dataﬂow eﬀects a program.
We compare our prototype PGA implementation to
three state of the art DTA implementations on 7 real-
world programs. Our results show that PGA can improve
the F1 accuracy of data ﬂow tracking by up to 33% over
taint tracking (20% on average) without introducing
any signiﬁcant overhead (	THRESH)	{				//	vulnerability}y	Taint111y	Gradientx1x2x3x1x2x3120-100Figure 2: Diﬀerent types of discrete and discontinuous operations that occur in real-world programs
2 Background
Our approach to gradient-based dataﬂow analysis draws
on several techniques from the mathematical analysis
and optimization literature. We provide a summary of
the relevant methods below. We ﬁrst summarize standard
methods for computing gradients over compositions of
smooth functions, and then review techniques from the
non-smooth analysis literature that can be applied to
computing gradients over programs.
δx
f(x+δx)−f(x)
2.1 Smooth Analysis
Gradients. The derivative for a smooth scalar function
f(x) is deﬁned as f0(x) = lim
, where f :
δx→0
R → R. If a function has a derivative for all points in its
domain, then it is considered a diﬀerentiable function.
The gradient is a generalization of the derivative to multi-
variate functions, where f : Rn → R and ∇f : Rn → Rn,
that can be understood as the slope of the function
at the point where it is evaluated. When a function is
vector-valued (i.e. f : Rn → Rm), the Jacobian generalizes
gradient by evaluating the gradient of each of the m
outputs: J f : Rn → Rn×m. For the rest of the paper,
functions are multi-variate unless otherwise noted.
Chain Rule. Gradients of compositions of diﬀerentiable
functions can be computed from gradients of the indi-
vidual functions. This is known as the chain rule of
calculus and is deﬁned as follows, where ◦ indicates the
composition of two functions f and g, and f0 and g0 are
their respective gradients:
(f ◦ g)0 = (f0 ◦ g)∗ g0
(1)
Elementwise multiplication is used when f and g are
multivariate.
Automatic Diﬀerentiation. Automatic Diﬀerentia-
tion (AutoDiﬀ) uses the chain rule to compute the gra-
dient for potentially large compositions of diﬀerentiable
functions. AutoDiﬀ has been a longstanding tool in com-
putational modeling and is a core component of deep
learning frameworks such as Tensorﬂow [2,53]. However,
existing AutoDiﬀ methods and frameworks are limited to
working with mostly continuous functions with limited
discontinuity (e.g. ReLUs in neural networks).
2.2 Non-smooth Analysis
Extensive work has been done in the ﬁeld of mathemati-
cal analysis on methods for approximating gradients over
non-smooth functions. In this section we consider gen-
eral multivariate functions of type f : Rn → R. We ﬁrst
describe a generalized type of continuity, called Lipschitz
continuity,
that applies to non-smooth operations in
programs, and then deﬁne a generalization of gradients
that apply to Lipschitz continuous functions.
Lipschitz Continuity. A function is Lipschitz contin-
uous if its output does not change too much for small
changes in the input. Formally, a function f is Lipschitz
continuous if there exists a constant K (called the Lips-
chitz constant) that bounds how much the value of f can
change between any two points in its domain. Figure 3a
shows a simple Lipschitz continuous function along with
the corresponding Lipschitz constant. In general the op-
erations in any useful computation will yield a Lipschitz
continuous function.
Generalized Gradients. On Lipschitz continuous func-
tions, generalized gradients are used to approximate gra-
dients [13,40]. Generalized gradients consist of general-
ized directional derivatives, which evaluate the gradient
in a single direction as shown in Figure 3b. A generalized
directional derivative in a direction v ∈ Rn is deﬁned as
follows:
f0 (x;v) = lim sup
y→x,λ↓0
f (y + λv)− f (y)
λ
(2)
Here x and y are two points in the domain of f where
x is the point the derivative is evaluated, and λ is a
distance along the vector v that the derivative is taken in.
The chain rule for directional derivatives with functions
g : Rn → Rn and f : Rn → R is deﬁned:
(f ◦ g)0(x;v) = f0(cid:0)g(x); g0(x;v)(cid:1)
(3)
When applied to generalized directional derivatives the
composing functions must be monotonic. Several relaxed
versions of the chain rule apply to generalized derivatives
under diﬀerent weaker assumptions about the composite
functions [22,31,52].
A generalized gradient is approximated with a set of
directional derivatives based on a matrix V ∈ Rn×p =
USENIX Association
30th USENIX Security Symposium    1613
048x (int)159y (int)y = x+1-404x (int)04y (int)y = x&4-404x (float)04y (float)y = (x>0)?x:4-404x (int)04y (int)y = x%4(a) Lipshitz func.
(b) Directional deriv.
Figure 3: Example of a Lipschitz function with K=5
and directional derivatives on a discrete function.
[v1,v2, . . . ,vp] of p vectors in the domain of f represent-
ing the directions in which the derivatives are evaluated.
f0(x;V) =(cid:2)f0(x;v1), f0(x;v2), . . . , f0(x;vp)(cid:3)
(4)
When f is a composition of functions, the chain rule
from Eq. 3 can be applied to each of the generalized
directional derivatives:
(f ◦ g)0(x;V) = [(f ◦ g)0(x;v1)), ...,(f ◦ g)0(x;vp)]
(5)
3 Methodology
At a high level, our gradient propagation framework,
PGA, is similar to Autodiﬀ, computing the gradient of
each operation and using the results as inputs to the
next gradient computation. However, unlike Autodiﬀ, we
approximate the gradients of discrete functions with
proximal gradients.
Proximal Gradients. Since programs are generally
composed of discrete operations on integers, we deﬁne a
gradient approximation called proximal gradients that
can be evaluated on these discrete functions. Proximal
gradients use the minima of a function within a nearby
region deﬁned with a special operator called the proximal
operator [36]. This can be evaluated on both discrete
and continuous functions f : X n → X, where X is a
set with euclidean norm that can represent integers or
ﬂoats.
proxf (x) = argmin
y
2||x− y||2
2
(6)
(cid:0)f (y)+ 1
(cid:1)
The notation argminy indicates that the operator eval-
uates to the value of y that minimizes the sum of the
function f (y) and the distance cost.
We use the proximal operator to compute each gen-
eralized directional derivative f0(x;v). Given a function
f representing a program operation, we constrain the
proximal operator from Eq. 6 to a direction v:
2||x− y||2
2
(cid:0)f (y)+ 1
proxf(x;v) = argmin
(7)
(cid:1)
y
where y = x+ tv : t ∈ N, y ∈ X n
We then deﬁne the proximal directional derivative based
on the diﬀerence with proxf(x;v) constrained in the
direction v and scaled by the direction magnitude ||v||2:
prox0
f(x;v) = f(proxf(x;v))− f (x)
||proxf(x;v)− x||2
∗||v||2
(8)
f(x;V) =(cid:2)prox0
This takes the same form as the generalized directional
derivative (Eq. 2), but evaluated with the proximal opera-
tor. A proximal gradient is deﬁned for a set of direction
vectors V like the generalized gradient (Eq. 4) using
proximal directional derivatives:
prox0
f(x;v1), . . . ,prox0
(9)
Using proximal gradients allows us to evaluate gradients
on discrete operations in programs as if they were con-
tinuous nonsmooth functions and apply the associated
chain rule for generalized gradients in Eq. 5. For the rest
of this paper, we refer to ‘proximal gradients’ simply as
‘gradients’ unless otherwise speciﬁed.
f(x;vp)(cid:3)
3.1 Program Gradient Evaluation
To compute gradients over programs with PGA, we
model a program as a discrete function P : X n → X n,
and model the program state x ∈ X n as a vector (e.g.
x could model a byte array of size n representing the
program memory and registers). P is composed of N
functions Pi : X n → X n, i ∈ {1..N} representing individ-
ual operations on the program state:
P(x) = PN ◦ PN−1 ◦···◦ P2 ◦ P1(x)
Each program operation Pi is modeled as a combination
of n non-smooth scalar valued functions fij : X n → X, j ∈