title:Whispers in the Hyper-space: High-speed Covert Channel Attacks in
the Cloud
author:Zhenyu Wu and
Zhang Xu and
Haining Wang
Whispers in the Hyper-space:
High-speed Covert Channel Attacks in the Cloud
Zhenyu Wu
Zhang Xu
Haining Wang
The College of William and Mary
{adamwu, zxu, hnw}@cs.wm.edu
Abstract
Information security and privacy in general are major
concerns that impede enterprise adaptation of shared or
public cloud computing. Speciﬁcally, the concern of vir-
tual machine (VM) physical co-residency stems from the
threat that hostile tenants can leverage various forms of
side channels (such as cache covert channels) to exﬁl-
trate sensitive information of victims on the same physi-
cal system. However, on virtualized x86 systems, covert
channel attacks have not yet proven to be practical, and
thus the threat is widely considered a “potential risk”.
In this paper, we present a novel covert channel attack
that is capable of high-bandwidth and reliable data trans-
mission in the cloud. We ﬁrst study the application of
existing cache channel techniques in a virtualized envi-
ronment, and uncover their major insufﬁciency and dif-
ﬁculties. We then overcome these obstacles by (1) re-
designing a pure timing-based data transmission scheme,
and (2) exploiting the memory bus as a high-bandwidth
covert channel medium. We further design and imple-
ment a robust communication protocol, and demonstrate
realistic covert channel attacks on various virtualized x86
systems. Our experiments show that covert channels do
pose serious threats to information security in the cloud.
Finally, we discuss our insights on covert channel miti-
gation in virtualized environments.
1
Introduction
Cloud vendors today are known to utilize virtualization
heavily for consolidating workload and reducing man-
agement and operation cost. However, due to the relin-
quished control from data owners, data in the cloud is
more susceptible to leakage by operator errors or theft
attacks. Cloud vendors and users have used a number
of defense mechanisms to prevent data leakage, ranging
from network isolation to data encryption. Despite the
efforts being paid on information safeguarding, there re-
main potential risks of data leakage, namely the covert
channels in the cloud [14, 18, 22, 24, 30, 31].
Covert channels exploit imperfections in the isolation
of shared resources between two unrelated entities, and
enable communications between them via unintended
channels, bypassing mandatory auditing and access con-
trols placed on standard communication channels. Pre-
vious research has shown that on a non-virtualized sys-
tem, covert channels can be constructed using a variety of
shared media [3, 12, 16, 19, 23]. However, to date there
is no known practical exploit of covert channels on vir-
tualized x86 systems.
Exposing cloud computing to the threat of covert
channel attacks, Ristenpart et al. [18] have implemented
an L2 cache channel in Amazon EC2 [18], achieving a
bandwidth of 0.2 bps (bits-per-second), far less than the
one bps “acceptable” threshold suggested by the Trusted
Computer System Evaluation Criteria (TCSEC, a.k.a. the
“Orange Book”) [5]. A subsequent measurement study
of cache covert channels [30] has achieved slightly im-
proved speeds—a theoretical channel capacity of 1.77
bps1. Given such low reported channel capacities from
previous research, it is widely believed that covert chan-
nel attacks could only do very limited harm in the cloud
environment. Coupled with the fact that the cloud ven-
dors impose non-trivial extra service charges for provid-
ing physical isolation, one might be tempted to disregard
the concerns of covert channels as only precautionary,
and choose the lower cost solutions.
In this paper, we show that the threat of covert channel
attacks in the cloud is real and practical. We ﬁrst study
existing cache covert channel techniques and their ap-
plications in a virtualized environment. We reveal that
these techniques are rendered ineffective by virtualiza-
tion, due to three major insufﬁciency and difﬁculties,
namely, addressing uncertainty, scheduling uncertainty,
1This value is derived from the results presented in the original
paper—a bandwidth of 3.20 bps with an error rate of 9.28%, by as-
suming a binary symmetric channel.
and cache physical limitations. We tackle the address-
ing and scheduling uncertainty problems by designing
a pure timing-based data transmission scheme with re-
laxed dependencies on precise cache line addressing and
scheduling patterns. Then, we overcome the cache phys-
ical limitations by discovering a high-bandwidth mem-
ory bus covert channel, exploiting the atomic instructions
and their induced cache–memory bus interactions on x86
platforms. Unlike cache channels, which are limited to a
physical processor or a silicon package, the memory bus
channel works system-wide, across physical processors,
making it a very powerful channel for cross–VM covert
data transmission.
We further demonstrate the real world exploitability
of the memory bus covert channel by designing a ro-
bust data transmission protocol and launching realistic
attacks on our testbed server as well as in the Amazon
EC2 cloud. We observe that the memory bus covert chan-
nel can achieve (1) a bandwidth of over 700 bps with
extremely low error rate in a laboratory setup, and (2) a
real world transmission rate of over 100 bps in the Ama-
zon EC2 cloud. Our experimental results show that, con-
trary to previous research and common beliefs, covert
channels are able to achieve high bandwidth and reliable
transmission on today’s x86 virtualization platforms.
The remainder of this paper is structured as follows.
Section 2 surveys related work on covert channels. Sec-
tion 3 describes our analysis of the reasons that existing
cache covert channels are impractical in the cloud. Sec-
tion 4 details our exploration of building high-speed, re-
liable covert channels in a virtualized environment. Sec-
tion 5 presents our evaluation of launching covert chan-
nel attacks using realistic setups. Section 6 provides a
renewed view of the threats of covert channels in the
cloud, and discusses plausible mitigation avenues. Sec-
tion 7 concludes this paper.
2 Related Work
Covert channel is a well known type of security attack
in multi-user computer systems. Originated in 1972 by
Lampson [12], the threats of covert channels are preva-
lently present in systems with shared resources, such
as ﬁle system objects [12], virtual memory [23], net-
work stacks and channels [3, 19, 20], processor caches
[16, 24], input devices [21], etc. [5, 13].
Compared to other covert channel media, the proces-
sor cache is more attractive for exploitation, because
its high operation speed could yield high channel band-
width and the low level placement in the system hierar-
chy can bypass many high level isolation mechanisms.
Thus, cache-based covert channels have attracted serious
attention in recent studies.
Percival [16] introduced a technique to construct inter-
process high bandwidth covert channels using the L1 and
L2 caches, and demonstrated a cryptographic key leak-
age attack through the L1 cache side channel. Wang and
Lee [24] deepened the study of processor cache covert
channels, and pointed out that the insufﬁciency of soft-
ware isolation in virtualization could lead to cache-based
cross–VM covert channel attacks. Ristenpart et al. [18]
further exposed cloud computing to covert channel at-
tacks by demonstrating the feasibility of launching VM
co-residency attacks, and creating an L2 cache covert
channel in the Amazon EC2 cloud. Xu et al. [30] con-
ducted a follow up measurement study on L2 cache
covert channels in a virtualized environment. Based on
their measurement results, they concluded that the harm
of data exﬁltration from cache covert channels is quite
limited due to low achievable channel capacity.
In response to the discovery of cache covert channel
attacks, a series of architectural solutions have been pro-
posed to limit cache channels, including RPcache [24],
PLcache [11], and Newcache [25]. RPcache and New-
cache employ randomization to prevent data transmis-
sion by establishing a location-based coding scheme.
PLcache, however, is based on enforcing resource iso-
lation by cache partitioning.
One drawback of hardware-based solutions is their
high adaptation cost and latency. With the goal of of-
fering immediately deployable protection, HomeAlone
[31] proposes to proactively detect the co-residence of
unfriendly VMs. Leveraging the knowledge of existing
cache covert channel techniques [16, 18], HomeAlone
detects the presence of a malicious VM by acting like
a covert channel receiver and observing cache timing
anomalies caused by another receiver’s activities.
The industry has taken a more pragmatic approach
to mitigating covert channel threats. The Amazon EC2
cloud provides a featured service called dedicated in-
stances [1], which ensures VMs belonging to each tenant
of this service do not share physical hardware with any
other cloud tenants’ VMs. This service effectively elimi-
nates various covert channels induced by the shared plat-
form hardware, including cache covert channel. How-
ever, in order to enjoy this service, the cloud users have
to pay a signiﬁcant price premium2.
Of historical interest, the study of covert channels in
virtualized systems is far from a brand new research
topic—legacy research that pioneered this ﬁeld dates
back over 30 years. During the development of the VAX
security kernel, a signiﬁcant amount of effort has been
2As of the time of writing (January, 2012), each dedicated instance
incurs a 23.5% higher per-hour cost than regular usage. In addition,
there is a $10 fee per hour/user/region. Thus, for a user of 20 small
instances, the overall cost of using dedicated instances is 6.12 times
more than that of using regular instances.
Algorithm 1 Classic Cache Channel Protocol
Cache[N]: A shared processor cache, conceptually divided into N regions;
Cache[N]: Each cache region can be put in one of two states, cached or ﬂushed.
DSend[N], DRecv[N]: N bit data to transmit and receive, respectively.
Sender Operations:
Receiver Operations:
(Wait for receiver to initialize the cache)
for i := 0 to N − 1 do
if DSend[i] = 1 then
{Put Cache[i] into the ﬂushed state}
Access memory maps to Cache[i];
end if
end for
(Wait for receiver to read the cache)
for i := 0 to N − 1 do
{Put Cache[i] into the cached state}
Access memory maps to Cache[i];
end for
(Wait for sender to prepare the cache)
for i := 0 to N − 1 do
Timed access memory maps to Cache[i];
{Detect the state of Cache[i] by latency}
if AccessTime > T hreshold then
DRecv[i] := 1; {Cache[i] is ﬂushed}
else
DRecv[i] := 0; {Cache[i] is cached}
end if
end for
paid to limit covert channels within the Virtual Machine
Monitor (VMM). Hu [8, 9] and Gray [6, 7] have pub-
lished a series of follow up research on mitigating cache
channels and bus contention channels, using timing noise
injection and lattice scheduling techniques. However,
this research ﬁeld has lost its momentum until recently,
probably due to the cancellation of the VAX security ker-
nel project, as well as the lack of ubiquity of virtualized
systems in the past.
3 Struggles of the Classic Cache Channels
Existing cache covert channels (namely, the classic cache
channels) employ variants of Percival’s technique, which
uses a hybrid timing and storage scheme to transmit in-
formation over a shared processor cache, as described in
Algorithm 1.
The classic cache channels work very well on hyper-
threaded systems, achieving transmission rates as high as
hundreds of kilobytes per second [16]. However, when
applied in today’s virtualized environments, the achiev-
able rates drop drastically, to only low single-digit bits
per second [18, 30]. The multiple orders of magnitude
reduction in channel capacity clearly indicates that the
classic cache channel techniques are no longer suit-
able for cross–VM data transmission. In particular, we
found that on virtualized platforms, the data transmis-
sion scheme of a classic cache channel suffers three ma-
jor obstacles—addressing uncertainty, scheduling uncer-
tainty, and cache physical limitation.
3.1 Addressing Uncertainty
Classic cache channels modulate data by the states of
cache regions, and hence a key factor affecting chan-
nel bandwidth is the number of regions a cache being
divided. From information theory’s perspective, a spe-
ciﬁc cache region pattern is equivalent to a transmitted
symbol. And the number of regions in a cache thus cor-
responds to the number of symbols in the alphabet set.
The higher symbol count in an alphabet set, the more in-
formation can be passed per symbol.
On hyper-threaded single processor systems,
for
which classic cache channels are originally designed, the
sender and receiver are executed on the same processor
core, using the L1 cache as the transmission medium.
Due to its small capacity, the L1 cache has a special
property that its storage is addressed purely by virtual
memory addresses, a technique called VIVT (virtually
indexed, virtually tagged). With a VIVT cache, two pro-
cesses can impact the same set of associative cache lines
by performing memory operations with respect to the
same virtual addresses in their address spaces, as illus-
trated in Figure 1(a). This property enables processes to
precisely control the status of the cache lines, and thus


xx 
xx 
xx 
xxWƌŽĐĞƐƐ



WƌŽĐĞƐƐ
xx 
x
x
 xx
L1 Cache 
(VIVT) 
(a)
  xx
xx
  xx
xx
xx
xx
xx
xx
xxxxxx
xx 
 xxxxxxxx
x
x
L2 Cache 
(VIPT / PIPT) 
xx
 xx
xx
xxxx
xx
Host 
xx
xx
xx
xx
xx  
xx  
xx  
xxWƌŽĐĞƐƐ
Guest 
Physical 
Address 
Physical 
Address 
WƌŽĐĞƐƐ
1VM 1 
Guest 
Physical 
Address 
Host 
Physical 
Address 
(b)