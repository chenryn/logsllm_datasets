Similarly, ACOBE offers the option of introducing the
following feature weights wf,t,d.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:27 UTC from IEEE Xplore.  Restrictions apply. 
253
x
i
r
t
a
M
n
o
i
t
a
i
v
e
D
(cid:1850)
Deep Encoder (cid:2038)
(cid:2038)(cid:1850)
Code
Deep Decoder (cid:2032)
Figure 3. Deep Fully-Connected Autoencoder
n
o
i
t
c
u
r
t
s
n
o
c
e
R
(cid:2032)∘(cid:2038) (cid:1850)
(cid:7)
wf,t,d =
log2
max
(cid:8)
1
std(
(cid:3)hf,t,d), 2
(cid:9)(cid:10)
(1)
The equation is based on the log-normalized TF weight,
which is deﬁned by TFx = log(1 + frequency of a term x);
that is, the less the frequency, the less information a term
x could provide. Yet, unlike terms,
the higher std (or
equivalently more chaotic), the less information a feature
f could provide. To serve our need, we inverse the equation
TFx and substitute standard deviation for frequency, so
that the weights are lower for chaotic features but higher
for consistent features by design. However, it cannot be
inﬁnitely high for constantly static features with very small
(cid:3)hf,t,d), or otherwise ACOBE would be overly sensitive
std (
to small changes of static features. Therefore, a minimum
value of two is given to the logarithm function, and we
change the base to two so that the maximum value of weights
is bounded to one. In other words, activities with small
standard deviation less than two shall have equal weights
of value one.
B. Anomalous Deviation Detection Model
As shown in Figure 1, we leverage an ensemble of au-
toencoders, each of which identiﬁes behavioral anomalies in
terms of a designated behavioral aspect, where a behavioral
aspect is a set of relevant behavioral features; for example,
(1) ﬁle-access aspect includes ﬁle-read, ﬁle-write, and ﬁle-
delete activities, (2) network-access aspect includes visit,
download, and upload activities, and (3) conﬁguration aspect
includes registry-modiﬁcation, password-modiﬁcation, and
user/group-modiﬁcation.
We leverage fully-connected autoencoders in identify-
ing anomalous compound behavioral deviation matrices.
Brieﬂy speaking, an autoencoder is an unsupervised learning
method whose goal includes (1) to encode an input matrix
into a representation code, and (2) to reconstruct the input
matrix purely based on the representation code. Trained by
normal compound behavioral deviation matrices, an autoen-
coder is capable of encoding and reconstructing only the
seen normal behaviors; hence, poor reconstructions result
from behaviors that have not yet been seen. Conceptually,
an autoencoder is capable of learning what are normal in
order to ﬁnd what are abnormal.
To be speciﬁc, Figure 3 illustrates an example of a deep
is trained by minimizing
fully-connected autoencoder. It
φ, ψ = arg minφ,ψ(cid:4)X − (ψ ◦ φ)(X)(cid:4), where φ is a multi-
layer fully-connected encoder, ψ is a multi-layer fully-
connected decoder, X is the input matrix, and (ψ ◦ φ)(X)
the code φ(X)
is the reconstructed matrix. In between,
essentially encloses the characteristics of the input matrix.
Trained with only normal matrices, an autoencoder is able
to reconstruct only normal matrices with minimal recon-
struction errors. In events of high reconstruction errors,
the abnormal matrices are likely aftermath of compromised
users or unusual legitimate user activity.
C. Anomaly Detection Critic
After retrieving anomaly scores (which essentially are
reconstruction errors) from an ensemble of autoencoders,
our anomaly detection critic then produces a list of users
that need to be orderly investigated. Recall that, we do
not assign anomaly labels (e.g., normal or abnormal) to
users, because providing labels without ordering is not really
helpful in the context of anomaly detection, which typically
has overwhelming numbers of false-positive cases. It is more
preferable to have an ordered list of users that need to be
orderly investigated [32].
The investigation priority of a user is derived based on the
N-th highest rank of the user in different behavioral aspects
(that is, in more aspects is a user top anomalous, the more
anomalous the user is). For example, say N = 2 and a user
is ranked at 3rd, 5th, 4th in terms of in-total three behavioral
aspects, since 4th is the 2nd highest rank of this user, this
user has a investigation priority of 4. The investigation list
is sorted based on these priorities (the smaller the number,
the higher the priority); if the 4 is the highest priority in
the list, then this example user will be put on top of the
list. A simpler way to understand N is to imagine that N is
the number of votes required from each behavioral aspects.
Having the investigation list, security analysts may decide
how many users they want to investigate. For example, they
can investigate only top 1% of the users, or stop even earlier
if they have checked a certain number of users and found
nothing suspicious.
V. EVALUATION UPON SYNTHESIZED DATA
We evaluate our proposed work upon the CERT Division
Insider Threat Test Dataset [14], [15], which is widely used
in the literature [33]. It is a synthesized dataset that simulates
a large-scale organizational internet.
Implementation: We implement the autoencoder model
with Tensorﬂow 2.0.0. Each fully connected layer is im-
plemented with tensorﬂow.keras. layers.Dense activated by
ReLU. The numbers of hidden units at each layer in the
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:27 UTC from IEEE Xplore.  Restrictions apply. 
254
Algorithm 1: Anomaly Detection Critic
Input:
the number N, and a set of users
U = {U1,U2, . . .}, where each user has ranks
Ui.R = {R1,R2, . . .} in different aspects
Output: a list of users ordered by priority
1 P ← a list that stores (user, priority) tuples
2 foreach Ui ∈ U do
ranks ← sort Ui.R
priority ← ranks[N − 1] // index starts from 0
append tuple (Ui, priority) into P
3
4
5
6 end
7 list ← sort P based on priority
8 return list
encoder are 512, 256, 128, and 64; the numbers of hid-
den units in the decoder are 64, 128, 256, and 512. Be-
tween layers, Batch Normalization [34] is implemented with
tensorﬂow.keras.layers.Batch-Normalization; batch normal-
ization serves the purpose of optimizing training procedure.
To train the model, Adadelta optimizer is used in minimizing
Mean-Squared-Error (MSE) loss function. Before feeding
compound behavioral deviation matrices, we ﬂatten the
matrices into vectors, and transform the deviations from
close-interval [−Δ, Δ] to [0, 1].
(cid:9)2
Xi − (ψ ◦ φ)(Xi)
n(cid:11)
(cid:8)
MSE =
1
n
i=1
A. Dataset Description and Pre-processing
1) Insider Threat Scenarios: The dataset provides
anomaly labels for ﬁve pre-deﬁned threat scenarios. Among
them, however, we evaluate our approach for the below
scenarios, as we are particularly interested in user-based
anomaly detection.
1) User who did not previously use removable drives
or work during off-hours begins logging in off-hours,
using a removable drive, and uploading data to Wik-
ileaks.org. Leaves the organization shortly thereafter.
2) User begins surﬁng job websites and soliciting em-
ployment from a competitor. Before leaving the com-
pany, they use a thumb drive (at markedly higher rates
than their previous activity) to steal data.
2) Training Sets and Testing Sets: The dataset has
two sub-datasets, namely, r6.1 and r6.2. Both r6.1 and
r6.2 span from 2010-01-02 to 2011-05-31. Each subset
contains one instance of each threat scenario; hence, there
are four abnormal users in the four corresponding groups.
We deﬁne their groups by their organizational departments
(i.e., the third-tier organizational unit) listed in the LDAP
logs. There are in total 925 normal users in these four
groups. Since the four threat scenarios occur in different
times, we select the training sets and the testing sets for each
scenario accordingly; yet, the detection metrics in terms of
true positives, false positives, and false negatives are put
together into derivation of F1 scores. For each scenario,
the training set includes the data from the ﬁrst collection
day until roughly one month before the date of the labeled
anomalies, and the testing set includes the dates from then
until roughly one month after the labeled anomalies. Take
r6.1 Scenario 2 as example, since the anomalies span from
2011-01-07 to 2011-03-07, we build the training set from
2010-01-02 to 2010-11-30, and the testing set from 2010-
12-01 to 2011-03-30. The window size (ω) is set to 30 days.
3) Behavioral Feature Extraction: The dataset encloses
a few types of logs,
including device accesses, ﬁle ac-
cesses, HTTP accesses, email accesses, logon-and-logoffs,
and LDAP. For presentation purpose, we only present the
logs and features that are strongly related to this evaluation.
For each log type, we split the log entries by user ID,
and then for each user ID we extract a set of behavioral
deviations σf,t,d, each of which represents the number of
instances of feature f during the time-frame t on the day d.
Feature weights wf,t,d are applied.
1) Device Accesses: This category encloses the usage
of thumb drives. Each log entry includes an activity
(either connect or disconnect) and a host ID to where
a thumb drive is connected. There are in total two
deviation features in this category: (f1) connection, the
number of connections and (f2) new-host-connection,
the number of connections to a new host that the user
never had connected to before day d.
2) File Accesses: Each log entry in this category includes
an activity (e.g., open, copy, write), a ﬁle ID, and a
dataﬂow direction. There are in total seven features in
this category: (f1) open-from-local, (f2) open-from-
remote, (f3) write-to-local, (f4) write-to-remote, (f5)
copy-from-local-to-remote, (f6) copy-from-remote-to-
local, and (f7) new-op. The value of each feature
is computed as the number of operation in terms of
(feature, ﬁle-ID) pair that the user never had conducted
before day d.
3) HTTP Accesses: Each log entry in this category
includes an activity (e.g., visit, download, upload),
a domain, and a ﬁletype that is being downloaded
or uploaded. We do not
take visit and download
into considerationThere are in total seven features
in this category: (f1) upload-doc, (f2) upload-exe,
(f3) upload-jpg, (f4) upload-pdf, (f5) upload-txt, (f6)
upload-zip, and http-new-op, (f7). The value of each
feature is computed as the number of operation in
terms of (feature, domain) pair that the user never had
conducted before day d).
Figure 4 depicts the behavioral deviation matrices of the
abnormal user JPH1910. The upper two sub-ﬁgures are be-
havioral deviation in the device-access aspect (with two fea-
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:27 UTC from IEEE Xplore.  Restrictions apply. 
255
1) Long-term vs. Single-Day Reconstruction: We prefer
long-term reconstruction rather than single-day reconstruc-
tion, because typical cyber threats (including insider threats)
do not start and end within one day. Figure 5(c) depicts
the anomaly scores derived by a single-day reconstruction
model, which is similar to ACOBE except that the features
are normalized occurrences of activities (as it no longer
has history window for derived behavioral deviations). We
can see that, although there are abnormal raises on the
labeled days, the score waveform of the abnormal user is not
distinguishable from the waveforms of normal users (peaks
on weekdays and troughs on weekends); that is, single-
day reconstruction cannot identify long-lasting threats that
span multiple days. In contrast, as shown in Figure 5(b),
ACOBE with long-term reconstruction can demonstrate the
score waveform of the abnormal user, and on some dates
the anomaly score stands out on top of all users. The score
gets higher as the abnormal behavior patterns continue to
appear in the compound behavioral deviation matrix. The
anomaly scores shortly remain high and then decreases as
the abnormal patterns gradually slides out of the matrix.
2) With Group Deviations vs. Without Group Deviation:
Figure 5(d) depicts the trends of anomaly scores of matrices
without group deviations (the other conﬁgurations are the
same as in ACOBE). While we can see that
the score
waveform of the abnormal user still seems alike and distin-
guishable as in Figure 5(b), we argue that this model is less
ideal. Without considering behavioral correlation between
a user and the group, this model overly emphasizes self-
deviating users, and thus mis-ranks normal users before
abnormal users. In addition, we can see that the average of
anomaly scores on top of Figure 5(d) is higher (which means
reconstruction errors are higher), despite that the size of
behavioral matrices is cut in half due to the absence of group
deviations. In contrast, ACOBE, with such correlations,
reduces not only the mis-rankings, but also the average of
anomaly scores, meaning that group behavior indeed help
with reducing reconstruction errors of normal behavioral
matrices. In the following section, we further show that
ACOBE outperforms the corresponding long-term model
without group deviations (denoted as No-Group model).
3) An Ensemble of Autoencoders vs. One Autoencoder:
The drawback of deploying just one autoencoder for all
features is that, the model may be too sensitive to noise
introduced by irrelevant features. ACOBE suffers from the
common limitation among all other anomaly detection meth-
ods: if a set of features cannot describe a cyber threat,