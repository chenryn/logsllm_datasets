of contention and (2) its solo throughput increases. By the same
token, when we increase the flow count by the same amount (from
400K to 450K), we observe the opposite behavior.
To explain this behavior, we look at the resulting change in the
NF‚Äôs shared resource utilization. In the case of FlowStats, which
keeps per-flow state, the reduction in concurrent flows leads to
a smaller hash table in memory and hence a reduced probability
of contention over the LLC. Figure 9 visualizes the effects of this
change across two key dimensions of FlowStats‚Äô sensitivity, com-
peting LLC occupancy and competing CAR.
We observed similar effects in other NFs. For instance, changes
in the size of the ruleset of an IP Router (¬±20%) also resulted in a
change the LLC utilization of the router and corresponding change
in the NF‚Äôs sensitivity curve.8 This leads to our key observation:
Observation 8: Changes in NFi‚Äôs traffic profile of configura-
tion translate in changes in NFi‚Äôs reliance on shared memory
resources and thus to its sensitivity.
Recall from ¬ß5.2 that we use an ensemble of linear models as
our prediction function. Our insight for extrapolation, therefore,
is to modify each linear model to adjust to changes in the LLC
utilization of the target ‚Äì whether due to traffic or ruleset variation
‚Äì as follows.
Slope with respect to CAR: As the LLC occupancy of the target
NF goes down, there is less opportunity for the competition and
the target to contend over the same cache line. Hence, we expect
that as LLC occupancy of the target goes down, the same value of
CAR for the competition would result in fewer evictions. Assuming
that our initial model of competitor‚Äôs CAR and target‚Äôs throughput
has a slope ùõº, we model the new slope ùõº‚Ä≤ as the ratio of the new
cache occupancy of the target to the original cache occupancy of
the target: ùõº‚Ä≤ =
X-shift with respect to Occupancy: The slowdown for the tar-
get NF with regard to cache occupancy is really a function of the
total cache occupancy. In particular, we consistently see an inflec-
tion point in the sensitivity curve when the sum of the target and
competition‚Äôs cache occupancies surpass the total available LLC
space. If the target‚Äôs occupancy reduces, we hence expect to see this
inflection point at a higher level of occupancy by the competition.
Hence, we shift the sensitivity function along the x-axis (occupancy
of competition) relative to the difference in the occupancy of the
target ùõø = ùëÇùê∂ùê∂new
Y-shift with respect to solo performance: Finally, when the
occupancy of the target changes, the baseline, solo throughput of
the NF may shift. To account for this, we shift along the y-axis
(performance) by ùõΩ, for ùõΩ = SoloPerfnew
target ‚àí ùëÇùê∂ùê∂old
target ‚àí SoloPerfold
target.
target.
ùëÇùê∂ùê∂new
target
ùëÇùê∂ùê∂old
target
ùõº.9
8Throughout these experiments, we ensured that incoming traffic would touch the
entirety of the ruleset and thus, the addition of rules would not be redundant
9In reality, the relationship between CAR and probability of evicting target NF data
follows a geometric distribution and should be modeled as such [24].
(7.1).
3. The design decisions behind each of SLOMO‚Äôs components
contribute to improved accuracy (7.2).
4. SLOMO is efficient and enables smart scheduling decisions in
Scope of extrapolation: So far, our discussion about extrapola-
tion heuristics is based upon the assumption that NFi‚Äôs change is
‚Äúsmall‚Äù and thus there is overlap between the sensitivity profiles
of NFi and NFi‚Ä≤. We find this to be a reasonable assumption as it
allows us to ‚Äútransfer‚Äù the knowledge of NFi‚Äôs sensitivity profile
to NF‚Ä≤
i without a significant loss of accuracy. On the other hand,
in cases where configurations or traffic profiles differ significantly
(e.g., a firewall with 1 vs. 10K rules), there is little to no overlap be-
tween the respective sensitivity profiles and, as a result, accurately
extrapolating performance drop is not possible.
In ¬ß7, we demonstrate the promise of our simple extrapolation
scheme. We show that despite the inherent complexity of the sensi-
tivity function, our approach on average adds 3 percentage units to
the error, and significantly outperforms the accuracy of predictions
where extrapolation was not used.
7 EVALUATION
In this section we evaluate our approach and show that:
1. SLOMO is accurate, with a mean prediction error of 5.4%, re-
ducing Dobrescu‚Äôs 12.72% error by 58% and BubbleUp‚Äôs 15.2%
average error by 64% (7.1).
2. SLOMO‚Äôs predictions are robust across operating conditions
an NFV cluster (7.3).
5. SLOMO is extensible, allowing the accurate extrapolation of the
sensitivity function of new NF instances to account for changes
in an NF‚Äôs traffic profile or configuration (7.4).
For each experiment we choose the target NF (i.e., type, configu-
ration and traffic profile), the number and type of (real) contenders
and the architecture the NFs will run on. For NF profiling and testing
we use the methodology described in ¬ß5. Unless mentioned other-
wise, we estimate performance using the composed contentiousness
of the competition (i.e., we always assume a cold start scenario).
SLOMO fully profiles each NF type on two architectures (Broadwell
and Skylake) and for 6 different configurations of traffic profile
(i.e., 64/1500B and 40K/400K/4M traffic flows, uniformly distributed
across the space of possible destination IPs). The NFs processed the
maximum traffic until we saturated the corresponding core. For the
Click-based IP Router and Stateless Firewall, we also experimented
with 2 ruleset sizes (small/large).
7.1 Accuracy
What is the end-to-end prediction accuracy of SLOMO? We
compare SLOMO‚Äôs absolute mean prediction error against (1) Do-
brescu‚Äôs CAR-based prediction model [24] and (2) the more general-
purpose BubbleUp prediction framework by Mars et al. [39]. Overall,
SLOMO reduces the absolute mean prediction error with respect to
Dobrescu‚Äôs model and BubbleUp by 57% and 64% respectively.
Figure 10 shows SLOMO‚Äôs end-to-end average absolute predic-
tion error for all experiments across NF types. We make the follow-
ing observations. First, SLOMO‚Äôs both average prediction error and
error variance are across the board lower than those of prior work.
Cases where the prediction errors between SLOMO and prior work
are similar (e.g., VPN, stateless firewall, Maglev) correspond to NFs
that do not show high dependence on the memory subsystem, either
because they are CPU bound (e.g., VPN) or because their rulesets
impose little memory overhead (e.g., stateless firewall). For instance,
in the case of a stateless firewall that co-ran with 7 competitors,
its observed throughput was 4.12Mpps, SLOMO‚Äôs was 4.10Mpps,
Dobrescu‚Äôs 4.02Mpps and that of BubbleUP 4.02Mpps. However, for
IP Router, a NF that depends heavily on the memory subsystem,
its observed throughput was 3.46Mpps, SLOMO‚Äôs prediction was
3.30Mpps, Dobrescu‚Äôs 2.79Mpps and that of BubbleUP 3.00Mpps.
We note, however, that FlowStats and IP Router exhibit higher error
variance. We revisit this observation later in this subsection.
Figure 10: End to End prediction error by NF type
Does SLOMO under/overestimate performance? To see if
SLOMO over- (positive error) or under-predicts (negative error)
performance, we show the signed prediction error across NF types
in Figure 11. We observe that there is no clear trend in favor of
either sign, indicating SLOMO‚Äôs prediction error is unbiased. On
the other hand, we observe that predictions made with the CAR-
based model tend to overestimate the amount of performance drop
the target NF experiences.
Figure 11: Signed prediction error for IP Router, FlowStats, VPN.
How robust are SLOMO‚Äôs predictions? To evaluate SLOMO‚Äôs
robustness, we look at how both its signed and/or unsigned predic-
tion error changes as a function of key data dimensions as described
in the following points:
1. The absolute error follows an increasing trend as a function of
the number of competing NFs (Figure 12). We attribute this to
an additive, composition-related error factor that is introduced
for every additional competitor.
2. SLOMO occasionally over-predicts the performance drop in
cases where memory sensitive NFs (e.g., IP Router, FlowStats)
are co-run with up to two contenders. We attribute this error
to a gap in our training dataset that did not sufficiently cover
areas of low contentiousness. We still observe, however, that this
error is substantially lower than that of the CAR-based model
(Figure 13).
3. SLOMO‚Äôs absolute error follows an increasing trend as a func-
tion of the unique flow count of the traffic received by the target
NF (Figure 14). Indeed, a higher number of unique flows will
result in higher utilization of the target NF‚Äôs auxiliary structures,
which also explains the error variance of FlowStats and IPRouter.
These NFs are not sensitive to contention when the number of
traffic flows is low, hence their prediction error is very small.
As the flow count increases, performance degradation increases
and so does the associated error.
4. SLOMO‚Äôs average prediction error does not change significantly
as a function of packet size processed by the target NF (Figure 14).
However, we observe a slightly higher variance in SLOMO‚Äôs
prediction errors which we trace back to a number of experi-
ments where SLOMO overestimated the performance drop of
NFs that due to their lower packet rate (as a result of the higher
packet size) did not experience any performance drop.
5. Finally, from Figure 14 we observe that SLOMO‚Äôs error, also
appears largely independent of the competing traffic rate (load
offered to the system). This highlights that SLOMO‚Äôs design
can adapt to different types of competitors that apply varying
amounts of contention to the shared resources.
Figure 12: End to End prediction error by number of competing NFs
Figure 13: End to End prediction error by NF type
Figure 14: Prediction error as a function of traffic profile
7.2 Factor Analysis
We now examine design decisions behind SLOMO‚Äôs components.
How does our choice of contentiousness metrics impact
accuracy? We first experimentally estimate the number of metrics
needed to accurately capture contentiousness. We train each model
with one metric at a time and measure the prediction error of the
resulting model on our testing dataset. The leftmost column of
Table 6 (Top 1) shows the absolute error for the best 1-metric model
for each NF. We observe that the error is consistently larger than
SLOMO‚Äôs, which validates our choice of using multiple metrics. The
second column further corroborates this observation by showing
the drop in prediction error that is observed with the best (NF-
specific) 3-metric sensitivity model.
SnortIP RouterVPNFlowStatsStatelessFirewallMagLev LBSuricatapfSense01020304050Prediction Error (%)SLOMODobrescuBubbleUpSnortpfSenseIP RouterVPNFlowStatsStatelessFirewallMagLev LBSuricata20020406080100Prediction Error (\%)SLOMODobrescuBubbleUp234567Number of Competitors051015202530Prediction Error %SLOMODobrescuBubbleUp2345673020100102030Prediction Error (%)FlowStatsFlowStats CAROptimal234567competing NFs3020100102030IP RouterIP Router CAROptimal2345673020100102030VPNVPN CAROptimal0-2525-5050-7575-100Competing NF Traffic (Gbps)0102030405060Prediction Error (%)SLOMODobrescuBubbleUp64B1500BTarget NF Packet Size0102030405060SLOMODobrescuBubbleUp40K400K4MTarget NF Unique Flows0102030405060SLOMODobrescuBubbleUpWe then look at the specific metrics that need to be consid-
ered in the ensemble to improve accuracy. Recall that in ¬ß5 we
found that different sources of contentiousness are captured by
different metrics. Given that, we measure the prediction error of a
sensitivity model fitted (1) with LLC-specific metrics and (2) with
main-memory metrics only (columns 3 and 4). We observe that
while both subsets of metrics improve the accuracy against a single
metric model, error is further reduced when they are combined
in one model (final column). This observation shows that our en-
semble of contentiousness metrics adequately captures the various
sources of contention in the memory subsystem.
NF
FlowStats (B)
IP Router (B)
Maglev LB (B)
Suricata (B)
Top 1
17.9%
8.6%
3.2%
23.7%
Top 3
8.7%
3.1%
1.2%
12.7%
LLC Mem SLOMO
5.5%
6.2%
1.5%
15.5%
7.1%
4%
2%
13.2%
7.5%
3.2%
1.2%
10.4%
Table 6: Absolute prediction error for different sets of contentiousness metrics
How do key contentiousness metrics change across NF-
s/architectures? Table 7 contains the top 3 metrics for a collection
of NF instances. For the same NF,the corresponding set of metrics
changes across architectures (Broadwell, Skylake). We notice, how-
ever, that in the Skylake server, Memory Writes are a metric that
appears commonly in the top-3 of contention-prone NFs. In our
effort to understand the explanation behind this observation, we
found that Skylake and Broadwell architectures differ significantly
in the organization of their memory hierarchy. Specifically, Skylake
servers have a substantially smaller LLC (11MB vs. 20MB) and a
non-inclusive10 write-back cache policy that for NFV workloads
results in producing large amounts of writes to main memory [9].
NF
Metric 1
Metric 2
LLC MISS
CAR
LMB
LLC OCC
LLC OCC
FlowStats (B)
FlowStats (S)
IP Router (B)
IP Router (S) MEM WRITE
MEM WRITE
MEM WRITE
Table 7: Top 3 metrics per NF
Snort (B)
Snort (S)
CAR
CAR
MEM READ
LLC HIT
Metric 3
LLC OCC
MEM WRITE
CAR
LLC OCC
MEM READ
LLC MISS
How does Gradient Boosting Regression compare to other
candidate techniques for sensitivity modeling? Figure 15 eval-
uates our choice to capture the complex piece-wise nature of sen-
sitivity with Gradient Boosting Regression. We compare GBR‚Äôs
prediction error on the same dataset with that of a large set of
modeling techniques and we find that GBR is the only technique
that achieves an average prediction error of less than 10% and
outperforms other techniques by up to 2 orders of magnitude.
Figure 15: Average prediction error for various common modelling tech-
niques including GBR
10A non-inclusive cache writes back every modification or eviction that happens in L2
cache to the LLC
How does prediction error change when composition is
used? To evaluate the accuracy of composition, Figure 16 com-
pares SLOMO‚Äôs prediction error in the following scenarios; (1)
when contentiousness is directly measured from the server (i.e.,
no composition is needed), (2) when the contentiousness vector
is composed with the PCM metrics of each competitor when run
solo and (3) when contentiousness is composed using SLOMO‚Äôs
methodology. We find that our composition mechanism introduces
an overhead that the worst case does not introduce more than 5% of
additional error. On the other hand, using the solo contentiousness
of the competitors to compose the aggregate contentiousness vector
can increase the error by up to 20%. We note that composition does
not introduce additional time overheads during online prediction.