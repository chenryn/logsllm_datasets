title:POSTER: DeepCRACk: Using Deep Learning to Automatically CRack Audio
CAPTCHAs
author:William Aiken and
Hyoungshick Kim
POSTER: DeepCRACk: Using Deep Learning to Automatically
CRack Audio CAPTCHAs
William Aiken
Sungkyunkwan University
Suwon, Republic of Korea
PI:EMAIL
Hyoungshick Kim
Sungkyunkwan University
Suwon, Republic of Korea
PI:EMAIL
ABSTRACT
A Completely Automated Public Turing test to tell Computers and
Humans Apart (CAPTCHA) is a defensive mechanism designed
to differentiate humans and computers to prevent unauthorized
use of online services by automated attacks. They often consist
of a visual or audio test that humans can perform easily but that
bots cannot solve. However, with current machine learning tech-
niques and open-source neural network architectures, it is now
possible to create a self-contained system that is able to solve spe-
cific CAPTCHA types and outperform some human users. In this
paper, we present a neural network that leverages Mozilla’s open
source implementation of Baidu’s Deep Speech architecture; our
model is currently able to solve the audio version of an open-source
CATPCHA system (named SimpleCaptcha) with 98.8% accuracy.
Our network was trained on 100,000 audio samples generated from
SimpleCaptcha and can solve new SimpleCaptcha audio tests in
1.25 seconds on average (with a standard deviation of 0.065 sec-
onds). Our implementation seems additionally promising because
it does not require a powerful server to function and is robust to
adversarial examples that target Deep Speech’s pre-trained models.
CCS CONCEPTS
• Computing methodologies → Neural networks; • Security
and privacy → Graphical / visual passwords; Usability in security
and privacy;
KEYWORDS
CAPTCHA; neural networks; adversarial machine learning
ACM Reference Format:
William Aiken and Hyoungshick Kim. 2018. POSTER: DeepCRACk: Using
Deep Learning to Automatically CRack Audio CAPTCHAs. In ASIA CCS ’18:
2018 ACM Asia Conference on Computer and Communications Security, June
4–8, 2018, Incheon, Republic of Korea. ACM, New York, NY, USA, 3 pages.
https://doi.org/10.1145/3196494.3201581
1 INTRODUCTION
A Completely Automated Public Turing test to tell Computers and
Humans Apart (CAPTCHA) is an online challenge to distinguish
humans from computers. That is, a CAPTCHA is based on any
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
ASIA CCS ’18, June 4–8, 2018, Incheon, Republic of Korea
© 2018 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-5576-6/18/06.
https://doi.org/10.1145/3196494.3201581
Figure 1: Comparison of DeepCRACk with current audio
CAPTCHA-breaking schemes. DeepCRACk performs com-
pletely self-contained identification where other CAPTCHA
solvers rely on external services.
problem that a human can easily recognize and solve while a ma-
chine cannot (with acceptable probability subject to a real-time
constraint). This security mechanism is popularly used for many
websites in order to control usage of their services as well as prevent
the automation of a variety of attacks. For example, a CAPTCHA
can be used to limit the rate at which spammers can create new
accounts in an automatic manner.
Image-based CAPTCHAs (e.g., reCAPTCHA [3]) are the most
popularly used CAPTCHA type in practice even though they often
burden users with difficult challenges. To make matters worse,
visual CAPTCHAs cannot properly be used for users with a visual
impairment such as blindness or a cognitive disability like dyslexia;
thus, visual CAPTCHAs fail to meet the United States’ Section
508 requirement [9] to “require Federal agencies to make their
electronic and information technology (EIT) accessible to people
with disabilities”. In such situations, an audio-based CAPTCHA
seems a good alternative or complement to visual CAPTCHAs.
Therefore, it is important to develop robust and efficient audio-
based CAPTCHAs and evaluate their security and performance.
In this paper, we demonstrate that developing a secure audio-
based CAPTCHA is not easy against CAPTCHA solvers that are
based on deep learning techniques. To show the effectiveness of
Poster SessionASIACCS’18, June 4–8, 2018, Incheon, Republic of Korea797deep learning-based CAPTCHA solvers, we trained a neural net-
work that leverages Mozilla’s implementation1 of Baidu’s Deep
Speech architecture [4]; our trained model is currently able to
solve the audio version of the open-source CATPCHA system Sim-
pleCaptcha with 98.8% accuracy after only one day of training.
Unlike previous deep learning-based CAPTCHA solvers [1, 5, 8],
or CAPTCHA-breaking for-hire services, our model operates end-
to-end and does not rely on any external systems as shown in
Figure 1. Moreover, we evaluate the performance of DeepCRACk
against carefully crafted adversarial audio samples that could at-
tempt to fool machine learning-based CAPTCHA solvers. Deep-
CRACk can successfully recognize all samples while the pre-trained
Deep Speech models are fooled by every example.
2 RELATED WORK
The potential to attack CAPTCHAs by leveraging neural networks
has already been demonstrated in previous research. A lot of re-
search has gone into breaking visual CAPTCHAs with neural net-
works, and a noteworthy example of this is Kopp et al.’s work [5]. In
their approach, they replace a pipeline that removes hand-designed
pre-processing, denoising, segmentation, and post processing with
a two-step character localization and recognition convolutional neu-
ral network at greater than 50% accuracy for obfuscated character-
based image CAPTCHAs. Additionally, Sivakorn et al. [8] devel-
oped a pipeline for image recognition and the solving of Google’s
reCaptcha that included a Google Reverse Image Search, image an-
notation via various deep-learning libraries, and tag word classifier
that performed above 70% accuracy in online mode (using external
services) and above 40% in offline mode (self-contained).
While most of this research has focused on breaking visual
CAPTCHAs, Bock et al.’s unCaptcha system [1] proposed a low-
resource framework for defeating audio CAPTCHAs in Google’s re-
Captcha. In their system, they download the audio from reCaptcha,
pre-process it by extracting each spoken digit by intervals of silence
to be sent to remote online speech recognition services like Google
Speech API, IBM Bluemix, etc. After receiving the results, their sys-
tem maps homophones and partial match sounds to digits (“mine”
to “9”, “*icks” to “6”, etc.) and performs ensembling to weight some
speech systems higher than others. Ultimately, they were able break
audio reCaptcha over an 85% success rate. Google has been updat-
ing their reCaptcha system to combat these vulnerabilities, but
other CAPTCHA systems still rely on audio digit recognition.
3 PROPOSED APPROACH
External speech-to-text systems may include specific usage limits
and are subject to API changes, and human-solving CAPTCHA
services are non-free and sometimes unreliable. Unlike other ap-
proaches to cracking audio CAPTCHAs, DeepCRACk aims to be
end-to-end and able to function without any external assistance.
We suspect that the bidirectional recurrent neural network (BRNN)
is the most suitable neural network for this task. A BRNN splits
a traditional recurrent neural network into two different states:
one for analyzing the input in the forward direction, and one for
analyzing the input in the backward direction. In this way, the
network can use information from both the past and the future
1https://github.com/mozilla/DeepSpeech
to help analyze the current frame. Moreover, the mel-frequency
cepstrum coefficients (MFCCs) contain enough information about a
time slice of audio to serve as sufficient input to the network; they
not only approximate human auditory system responses to audio
segments but also represent amplitudes in that spectrum. Fortu-
nately, Mozilla’s implementation of Deep Speech already utilizes
such a framework.
Because our goal at this time is to target only the CAPTCHAs
generated by the audio version of an open-source CATPCHA sys-
tem (named SimpleCaptcha), we do not need the full size of the
Deep Speech neural network. Because Mozilla’s Deep Speech is
designed for very complex speech-to-text tasks, using a full-size
implementation would be unnecessary for our tasks. Thus, we re-
stricted our model to 486 hidden neurons per hidden layer. Using
the default settings of SimpleCaptcha, there are 7 voices per num-