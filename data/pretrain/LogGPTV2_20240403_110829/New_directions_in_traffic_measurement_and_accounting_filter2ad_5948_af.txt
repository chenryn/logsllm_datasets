Theorem 3. The second line below represents the improve-
ment in the theoretical bound when we assume a Zipf distri-
bution of ﬂow sizes. Unlike in the case of sample and hold
we used the maximum traﬃc, not the link capacity for com-
// //
100
)
e
l
a
c
s
g
o
l
(
s
e
v
i
t
i
s
o
p
e
s
l
a
f
f
o
e
g
a
t
n
e
c
r
e
P
10
1
0.1
0.01
0.001
1
General bound
Zipf bound
Serial filter
Parallel filter
Conservative update
2
3
4
Depth of filter
Figure 7: Filter performance for a stage strength of
k=3
puting the theoretical bounds. This results in much tighter
theoretical bounds.
The third line represents the measured average percentage
of false positives of a serial ﬁlter, while the fourth line rep-
resents a parallel ﬁlter. We can see that both are at least 10
times better than the stronger of the theoretical bounds. As
the number of stages goes up, the parallel ﬁlter gets better
than the serial ﬁlter by up to a factor of 4. The last line rep-
resents a parallel ﬁlter with conservative update which gets
progressively better than the parallel ﬁlter by up to a factor
of 20 as the number of stages increases. We can see that all
lines are roughly straight; this indicates that the percentage
of false positives decreases exponentially with the number
of stages.
Measurements on other traces show similar results. The
diﬀerence between the bounds and measured performance
is even larger for the traces where the largest ﬂows are re-
sponsible for a large share of the traﬃc. Preserving entries
reduces the average error in the estimates by 70% to 85%.
Its eﬀect depends on the traﬃc mix. Preserving entries in-
creases the number of ﬂow memory entries used by up to
30%. By eﬀectively increasing stage strength k, shielding
considerably strengthens weak ﬁlters. This can lead to re-
ducing the number of entries by as much as 70%.
7.2 Evaluation of complete trafﬁc measure-
ment devices
We now present our ﬁnal comparison between sample and
hold, multistage ﬁlters and sampled NetFlow. We perform
the evaluation on our long OC-48 trace, MAG+. We assume
334that our devices can use 1 Mbit of memory (4096 entries12)
which is well within the possibilities of today’s chips. Sam-
pled NetFlow is given unlimited memory and uses a sam-
pling of 1 in 16 packets. We run each algorithms 16 times
on the trace with diﬀerent sampling or hash functions.
Both our algorithms use the adaptive threshold approach.
To avoid the eﬀect of initial misconﬁguration, we ignore the
ﬁrst 10 intervals to give the devices time to reach a rela-
tively stable value for the threshold. We impose a limit of
4 stages for the multistage ﬁlters. Based on heuristics p-
resented in [6], we use 3114 counters13 for each stage and
2539 entries of ﬂow memory when using a ﬂow deﬁnition at
the granularity of TCP connections, 2646 counters and 2773
entries when using the destination IP as ﬂow identiﬁer and
1502 counters and 3345 entries when using the source and
destination AS. Multistage ﬁlters use shielding and conser-
vative update. Sample and hold uses an oversampling of 4
and an early removal threshold of 15%.
Our purpose is to see how accurately the algorithms mea-
sure the largest ﬂows, but there is no implicit deﬁnition of
what large ﬂows are. We look separately at how well the
devices perform for three reference groups: very large ﬂows
(above one thousandth of the link capacity), large ﬂows (be-
tween one thousandth and a tenth of a thousandth) and
medium ﬂows (between a tenth of a thousandth and a hun-
dredth of a thousandth – 15,552 bytes).
For each of these groups we look at two measures of accu-
racy that we average over all runs and measurement inter-
vals: the percentage of ﬂows not identiﬁed and the relative
average error. We compute the relative average error by
dividing the sum of the moduli of all errors by the sum of
the sizes of all ﬂows. We use the modulus so that posi-
tive and negative errors don’t cancel out for NetFlow. For
the unidentiﬁed ﬂows, we consider that the error is equal to
their total traﬃc. Tables 5 to 7 present the results for the 3
diﬀerent ﬂow deﬁnitions.
When using the source and destination AS as ﬂow identiﬁ-
er, the situation is diﬀerent from the other two cases because
the average number of active ﬂows (7,401) is not much larger
than the number of memory locations that we can accom-
modate in our SRAM (4,096), so we will discuss this case
separately. In the ﬁrst two cases, we can see that both our
algorithms are much more accurate than sampled NetFlow
for large and very large ﬂows. For medium ﬂows the average
error is roughly the same, but our algorithms miss more of
them than sampled NetFlow. Since sample and hold sta-
bilized at thresholds slightly above 0.01% and multistage
ﬁlters around 0.002% it is normal that so many of the ﬂows
from the third group are not detected.
We believe these results (and similar results not presented
here) conﬁrm that our algorithms are better than sampled
NetFlow at measuring large ﬂows. Multistage ﬁlters are al-
ways slightly better than sample and hold despite the fact
that we have to sacriﬁce part of the memory for stage coun-
ters. However, tighter algorithms for threshold adaptation
can possibly improve both algorithms.
In the third case since the average number of very large,
large and medium ﬂows (1,107) was much below the number
12Cisco NetFlow uses 64 bytes per entry in cheap DRAM. We
conservatively assume that the size of a ﬂow memory entry
will be 32 bytes (even though 16 or 24 are also plausible).
13We conservatively assume that we use 4 bytes for a counter
even though 3 bytes would be enough.
Group
(ﬂow size)
Unidentiﬁed ﬂows / Average error
Sample
and hold
Sampled
NetFlow
0%/0.075% 0%/0.037% 0%/9.02%
1.8%/7.09% 0%/1.090% 0.02%/22%
0.01 . . . 0.001% 77%/61.2% 55%/43.9% 18%/50.3%
0.1 . . . 0.01%
Multistage
> 0.1%
ﬁlters
Table 5: Comparison of traﬃc measurement devices
with ﬂow IDs deﬁned by 5-tuple
Group
(ﬂow size)
Unidentiﬁed ﬂows / Average error
Sample
and hold
Sampled
NetFlow
0%/0.025% 0%/0.014% 0%/5.72%
0.43%/3.2% 0%/0.949% 0.01%/21%
0.01 . . . 0.001% 66%/51.2% 50%/39.9% 11.5%/47%
0.1 . . . 0.01%
Multistage
> 0.1%
ﬁlters
Table 6: Comparison of traﬃc measurement devices
with ﬂow IDs deﬁned by destination IP
Group
(ﬂow size)
Unidentiﬁed ﬂows / Average error
Sample
and hold
0%/0.0%
Sampled
NetFlow
0%/4.88%
0%/0.002% 0%/0.001% 0.0%/15.3%
0.01 . . . 0.001% 0%/0.165% 0%/0.144% 5.7%/39.9%
0.1 . . . 0.01%
Multistage
0%/0.0%
> 0.1%
ﬁlters
Table 7: Comparison of traﬃc measurement devices
with ﬂow IDs deﬁned by the source and destination
AS
of available memory locations and these ﬂows were mostly
long lived, both of our algorithms measured all these ﬂows
very accurately. Thus, even when the number of ﬂows is
only a few times larger than the number of active ﬂows,
our algorithms ensure that the available memory is used
to accurately measure the largest of the ﬂows and provide
graceful degradation in case that the traﬃc deviates very
much from the expected (e.g. more ﬂows).
8.
IMPLEMENTATION ISSUES
We brieﬂy describe implementation issues. Sample and
Hold is easy to implement even in a network processor be-
cause it adds only one memory reference to packet process-
ing, assuming suﬃcient SRAM for ﬂow memory and assum-
ing an associative memory. For small ﬂow memory sizes,
adding a CAM is quite feasible. Alternatively, one can im-
plement an associative memory using a hash table and stor-
ing all ﬂow IDs that collide in a much smaller CAM.
Multistage ﬁlters are harder to implement using a network
processor because they need multiple stage memory refer-
ences. However, multistage ﬁlters are easy to implement in
an ASIC as the following feasibility study shows.
[12] de-
scribes a chip designed to implement a parallel multistage
ﬁlter with 4 stages of 4K counters each and a ﬂow memory
of 3584 entries. The chip runs at OC-192 line speeds. The
core logic consists of roughly 450,000 transistors that ﬁt on
2mm x 2mm on a .18 micron process. Including memories
and overhead, the total size of the chip would be 5.5mm
335x 5.5mm and would use a total power of less than 1 watt,
which put the chip at the low end of today’s IC designs.
9. CONCLUSIONS
Motivated by measurements that show that traﬃc is dom-
inated by a few heavy hitters, our paper tackles the prob-
lem of directly identifying the heavy hitters without keeping
track of potentially millions of small ﬂows. Fundamental-
ly, Table 1 shows that our algorithms have a much better
scaling of estimate error (inversely proportional to memory
size) than provided by the state of the art Sampled Net-
Flow solution (inversely proportional to the square root of
the memory size). On actual measurements, our algorithms
with optimizations do several orders of magnitude better
than predicted by theory.
However, comparing Sampled NetFlow with our algorithms
is more diﬃcult than indicated by Table 1. This is be-
cause Sampled NetFlow does not process every packet and
hence can aﬀord to use large DRAM. Despite this, results
in Table 2 and in Section 7.2 show that our algorithms are
much more accurate for small intervals than NetFlow. In ad-
dition, unlike NetFlow, our algorithms provide exact values
for long-lived large ﬂows, provide provable lower bounds on
traﬃc that can be reliably used for billing, avoid resource-
intensive collection of large NetFlow logs, and identify large
ﬂows very fast.
The above comparison only indicates that the algorithms
in this paper may be better than using Sampled NetFlow
when the only problem is that of identifying heavy hitters,
and when the manager has a precise idea of which ﬂow de-
ﬁnitions are interesting. But NetFlow records allow mana-
gers to a posteriori mine patterns in data they did not an-
ticipate, while our algorithms rely on eﬃciently identifying
stylized patterns that are deﬁned a priori. To see why this
may be insuﬃcient, imagine that CNN suddenly gets ﬂood-
ed with web traﬃc. How could a manager realize before the
event that the interesting ﬂow deﬁnition to watch for is a
multipoint-to-point ﬂow, deﬁned by destination address and
port numbers?
The last example motivates an interesting open question.
Is it possible to generalize the algorithms in this paper to
automatically extract ﬂow deﬁnitions corresponding to large
ﬂows? A second open question is to deepen our theoretical
analysis to account for the large discrepancies between the-
ory and experiment.
We end by noting that measurement problems (data vol-
ume, high speeds) in networking are similar to the mea-
surement problems faced by other areas such as data min-
ing, architecture, and even compilers. For example, [19]
recently proposed using a Sampled NetFlow-like strategy to
obtain dynamic instruction proﬁles in a processor for later
optimization. We have preliminary results that show that
multistage ﬁlters with conservative update can improve the
results of [19]. Thus the techniques in this paper may be
of utility to other areas, and the techniques in these other
areas may of utility to us.
10. ACKNOWLEDGEMENTS
We thank K. Claﬀy, D. Moore, F. Baboescu and the anony-
mous reviewers for valuable comments. This work was made
possible by a grant from NIST for the Sensilla Project, and
by NSF Grant ANI 0074004.
11. REFERENCES
[1] J. Altman and K. Chu. A proposal for a ﬂexible
service plan that is attractive to users and internet
service providers. In IEEE INFOCOM, April 2001.
[2] B. Bloom. Space/time trade-oﬀs in hash coding with
allowable errors. In Comm. ACM, volume 13, July
1970.
[3] N. Brownlee, C. Mills, and G. Ruth. Traﬃc ﬂow
measurement: Architecture. RFC 2722, Oct. 1999.
[4] N. Duﬃeld and M. Grossglauser. Trajectory sampling
for direct traﬃc observation. In ACM SIGCOMM,
Aug. 2000.
[5] N. Duﬃeld, C. Lund, and M. Thorup. Charging from
sampled network usage. In SIGCOMM Internet
Measurement Workshop, Nov. 2001.
[6] C. Estan and G. Varghese. New directions in traﬃc
measurement and accounting. Tech. Report 699,
UCSD CSE, Feb. 2002.
[7] M. Fang et al. Computing iceberg queries eﬃciently.
In VLDB, Aug. 1998.
[8] W. Fang and L. Peterson. Inter-as traﬃc patterns and
their implications. In IEEE GLOBECOM, Dec. 1999.
[9] A. Feldmann et al. Deriving traﬃc demands for
operational IP networks: Methodology and
experience. In ACM SIGCOMM, Aug. 2000.
[10] W. Feng et al. Stochastic fair blue: A queue
management algorithm for enforcing fairness. In IEEE
INFOCOM, April 2001.
[11] P. Gibbons and Y. Matias. New sampling-based
summary statistics for improving approximate query
answers. In ACM SIGMOD, June 1998.
[12] J. Huber. Design of an OC-192 ﬂow monitoring chip.
UCSD Class Project, March 2001.
[13] J. Mackie-Masson and H. Varian. Public Access to the
Internet, chapter on “Pricing the Internet.” MIT
Press, 1995.
[14] R, Mahajan et al. Controlling high bandwidth
aggregates in the network.
http://www.aciri.org/pushback/, July 2001.
[15] D. Moore. http://www.caida.org/ analysis/ security/
code-red/.
[16] Cisco NetFlow http://www.cisco.com /warp /public
/732 /Tech /netflow.
[17] R. Pan et al. Approximate fairness through diﬀerential
dropping. Tech. report, ACIRI, 2001.
[18] D. Patterson and J. Hennessy. Computer Organization
and Design, page 619. Morgan Kaufmann, second
edition, 1998.
[19] S. Sastry et al Rapid proﬁling via stratiﬁed sampling.
In 28th ISCA, June 2001.
[20] S. Shenker et al. Pricing in computer networks:
Reshaping the research agenda. In ACM CCR,
volume 26, April 1996.
[21] Smitha, I. Kim, and A. Reddy. Identifying long term
high rate ﬂows at a router. In High Performance
Computing, Dec. 2001.
[22] K. Thomson, G. Miller, and R. Wilder. Wide-area
traﬃc patterns and characteristics. In IEEE Network,
December 1997.
336