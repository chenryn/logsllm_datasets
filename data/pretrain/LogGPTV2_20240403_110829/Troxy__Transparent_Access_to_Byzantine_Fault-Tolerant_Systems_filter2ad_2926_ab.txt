Replication
Replication protocol
Fig. 3. Overview of Troxy components and their interactions.
or to fail by crashing; in particular, this means that once a
client receives a result from a Troxy over a secure channel,
the client can trust the result to be correct. In later sections,
we discuss how we ensure this trustworthiness for Troxies
based on minimizing a Troxy’s trusted computing base (see
Section III-C) and utilizing Intel SGX (see Section V).
Apart from Troxies, all other replicas and network compo-
nents in the system may fail in arbitrary ways. The number of
servers required in a Troxy-backed system to tolerate such
Byzantine faults depends on the BFT replication protocol
executed among replicas: using a traditional BFT protocol [1],
[4], [24], [25], a minimum of 3f + 1 replicas are necessary to
tolerate up to f faults. If a replication protocol itself makes use
of trusted components [13], [14], [15], [20], [21], [22], [23], this
number can be reduced to 2f + 1 replicas. Replica components
located outside the Troxy do not trust each other. Components
of different replicas communicate by exchanging authenticated
messages over the network. If a correct component receives a
message it cannot verify, the component discards the message.
C. Minimizing the Trusted Computing Base
Relying on a hybrid fault model, it is crucial to keep the
trusted components as small as possible [21], because the more
complex a component, the more likely it is to fail in an arbitrary
way, for example, as the result of a program error. To justify
the trust put in the Troxy, we therefore minimize its complexity
by only performing those tasks inside the Troxy that are critical
and actually require to be trusted; in contrast, all noncritical
tasks are executed outside the Troxy in the untrusted part of the
replica. In essence, this leads to a design where the Troxy is
basically a library whose functionality is used by the untrusted
replica part via method calls.
With regard to client communication, this separation of
critical and noncritical tasks means that most of the network-
connection handling can be performed outside the Troxy. In
particular, this includes the management of connected sockets,
the handling of worker threads operating on these sockets, as
well as the execution of the actual send and receive operations.
Overall, there are only three major critical tasks the trusted
Troxy needs to perform: (1) when the client connects to a
replica, the replica’s Troxy controls the establishment of the
secure channel and afterwards stores the associated session
key in order to prevent the untrusted part of the replica from
being able to impersonate the Troxy. (2) When the client sends
a request to the server over the secure channel, the untrusted
part of the replica receives the request message. However, the
Troxy is the only one to be able to decrypt the request using
the session key. Having decrypted the message, the Troxy then
checks its integrity and creates a BFT-protocol request in which
it includes the client request as payload. Finally, the Troxy
authenticates the BFT request using the method expected by
the underlying BFT replication protocol (e.g., a keyed-hash
message authentication code (HMAC) in our implementation)
before handing over the request to the untrusted part of the
replica. This way, by atomically decrypting the client request
and creating a corresponding authenticated BFT request, the
Troxy ensures that the request cannot be altered by the untrusted
replica part without being detected. (3) After the request has
been executed, the Troxy collects the replies provided by
different replicas, veriﬁes the authenticity of these replies,
and then compares them to determine the correct result. Based
on this result, in a ﬁnal step, the Troxy creates a reply to the
client and encrypts this message using the session key of the
client’s secure channel. The actual transmission of the reply
is performed outside the Troxy in the untrusted part of the
replica. However, due to the untrusted replica part not having
access to the session key, it is unable to manipulate the reply
without the client detecting such a modiﬁcation.
D. Fault Handling
When a Troxy returns a reply to the client, the client can
trust the reply to be correct. However, in case of faults there
can be situations in which a client at ﬁrst does not receive
a reply to its request, for example, due to the server hosting
the Troxy having crashed. To handle such scenarios where
a Troxy ceases to operate, we exploit the fact that clients
of user-facing services typically are already equipped with
a mechanism to automatically reconnect to the service once
their existing connections time out, for example, relying on
an external location service to assist in the failover to another
replica (see Section II-A). As soon as the client reaches a
non-faulty replica, after retransmitting the request, the client
will eventually receive a corresponding reply from the service.
Using the same failover mechanism, clients are also able to
tolerate scenarios in which the untrusted part of a replica, which
performs the actual send and receive operations on network
connections (see Section III-C), fails to deliver the correct
reply provided by the Troxy. Depending on the nature of the
fault, in such case the client either detects a corrupted channel
(if the untrusted part sends data that is not encrypted with the
Troxy’s session key) or experiences a timeout (if the untrusted
part sends no data at all). Either way, the client can solve the
problem by reconnecting to the service.
In contrast to the Troxy, the untrusted part of a replica may
fail in arbitrary ways. Apart from the scenarios discussed above,
handling these kinds of faults mainly lies in the responsibility
of the underlying BFT replication protocol, as it is the case in
traditional BFT systems. The fact that a Troxy, while acting as
a BFT client, is co-located with a BFT replica has no effect on
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:31:39 UTC from IEEE Xplore.  Restrictions apply. 
62
the internal fault-handling procedures of the protocol. Replay
attacks are prevented by the secure channel that connects the
client with the Troxy. By design, each endpoint will never
accept the same chunk of encrypted data twice.
E. Introducing Byzantine Fault Tolerance Using Troxies
In the following, we illustrate the steps necessary to migrate
an existing user-facing service that is implemented by a crash-
tolerant system to a Troxy-backed BFT system. As an example,
we consider a RESTful web service that originally relies on
Paxos [26] for fault tolerance and is accessed by a wide
spectrum of heterogeneous clients via HTTPS.
The ﬁrst step to make such a service Byzantine fault tolerant
using our approach is to select a BFT replication protocol
and to integrate its server-side implementation with the Troxy.
This task is greatly facilitated by the Troxy essentially being
a library that needs to be invoked at a small number of well-
deﬁned locations in the replica logic in order to be able to
establish secure channels, to safely translate incoming client
requests into BFT requests, and to determine and encrypt the
ﬁnal replies (see Section III-C). On the other hand, the most
complex parts of a BFT protocol implementation, such as the
ordering and view-change protocols, are left unmodiﬁed.
In a second step, the server-side application logic of the
web service must be ported from the original crash-tolerant
protocol to the BFT protocol. For this task, it is usually
possible to beneﬁt from the fact that BFT protocols and
crash-tolerant protocols such as Paxos or Raft [27] in general
provide comparable interfaces and pose similar requirements on
applications, for example, with regard to execution determinism
or the ability to create/apply checkpoints of their state.
To enable the Troxy to communicate with clients, in a ﬁnal
step, the Troxy must be made aware of the message format
used by the service for requests. In this context, there is no
need for the Troxy to fully parse and understand incoming
requests. Instead, it is sufﬁcient for the Troxy to identify request
boundaries in order to be able to properly store the incoming
client request in the newly created BFT request; for replies,
the Troxy usually can simply extract the payload contained
in the veriﬁed BFT result and return it to the client. For
many communication protocols, including HTTP, identifying
message boundaries is straightforward due to messages carrying
information about their own length.
The steps discussed above have shown that the migration
overhead is small if a service is already resilient against crashes.
However, with Troxy providing transparent access to BFT
systems, even for unreplicated services that so far offer no fault
tolerance at all, the changes necessary to integrate Byzantine
fault tolerance are limited to the location service (i.e., to make
it replication aware) as well as the server-side implementation.
In contrast, there is no need to modify the potentially large
number of diverse client implementations.
IV. FAST-READ CACHE
Troxy features a managed fast-read cache that not only
validates cache entries when processing regular read requests,
but also removes entries from the cache if a write request is
about to outdate cached data. As a key beneﬁt, by invalidating
cache entries while processing write requests and before
their effects are emitted to clients, Troxy is able to maintain
consistency guarantees offered by the underlying BFT protocol.
In the following, we present details on Troxy’s fast path
for reads using the example of a BFT system that is based
on a hybrid fault model and therefore can tolerate f faults
with 2f + 1 replicas, as it is the case for our prototype
implementation (see Section V-B).
A. Protocol
In line with previous research [3], [4], [5], our fast-read
optimization assumes that read and write requests can be dis-
tinguished before executing them and that it can be determined
which part of the state a request is about to access or modify.
The described functionality is executed inside a Troxy instance
and therefore trusted with the exception of functions that are
provided by the surrounding replica.
Our fast-read cache utilizes the processing of a write request
to remove an outdated entry from the cache before the effects
of the write are visible to any client, that is, before the reply
to the write is returned to its client. To ensure this, we make
two important changes to introduce the cache: (1) We modify
the voter to only take the reply of another replica into account
if the reply is authenticated by the other replica’s Troxy. As a
consequence, this requirement forces a replica to hand over a
reply to its local Troxy in order for the reply to have an impact
on the ﬁnal result, thereby giving the Troxy the opportunity to
learn about a write and to subsequently invalidate an outdated
cache entry. To authenticate a local reply, a Troxy computes
an HMAC that is based on a shared secret, which is known
amongst all Troxies, and an identiﬁer speciﬁc to each Troxy
instance. (2) We extend the replies provided by local replicas
to not only contain the application’s result but also (a hash
of) the original request in order to allow a Troxy to identify
the cache entry to invalidate. As before, a Troxy only returns
a result to the client after having received f + 1 matching
replies (which now include the request) from different replicas.
With regard to the fast-read cache, this means that when a write
reply reaches this point, it is ensured that a majority of replicas
in the system have invalidated the associated cache entry.
As shown in Figure 4, if a Troxy receives a read request
from a connected client it ﬁrst determines if the fast-read
cache can be utilized by calling check cache that takes the
client-provided request as an input. Next, it checks if the cache
contains data that answers the request. If not, the request
is ordered and executed as any other request. Otherwise, a
set of f remote Troxies is randomly chosen and queried
using get remote cache entry(r,req). This function generates
an authenticated message for replica r to query its Troxy about
the currently processed request, which is handed over to the
untrusted replica code for transmission. On the remote side,
the receiving Troxy instances validate the message and then
check if the requested data is cached (see L. 21, Figure 4). The
request and associated reply, both authenticated, are returned
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:31:39 UTC from IEEE Xplore.  Restrictions apply. 
63
1 // Cache lookup in case of voting Troxy instance
2 upon call check cache(req) such that req is READ do
3
4
5
6
7
8
9
10
11
12
13
reply := cache.get(id(req))
if reply is not NULL // request is cached
replicas := choose f replicas() // select f remote caches
rc := ∅ // set of remote cached replies
// collect cache entries of f remote replicas
∀r ∈ replicas, rc.add(get remote cache entry(r,req))
// remote caches match local cache
if ∀(r req, u rep) ∈ rc, (id(r req), u rep) = (id(req), reply)
else return null // mismatch amongst caches
return reply // fast read succeed
else return null // cache miss
15 // Cache lookup in case of remote Troxy instance
16 upon call get local cache entry(req) do
17
18
reply := cache.get(id(req))
return (req,reply)
Fig. 4. Cache lookup when processing read requests.
to the initial requesting Troxy. Next, it is validated if all f
request and reply pairs match the local data. If this is the
case, the reply is returned to the client and a successful cache
lookup has been performed. In case of a mismatch, which
for example can be the result of concurrent write requests or
actions performed by malicious replicas (e.g., the replay of a
stale reply), the read request is ordered in the common way.
Note that a more aggressive use of hashes can reduce the
amount of exchanged data. In addition, timeouts might be used
to detect unresponsive replicas.
B. Ensuring Consistency and Resilience to Performance Attacks
In scope of the implemented prototype we considered a
system that relies on an hybrid fault model that requires only
2f + 1 replicas and offers strong consistency. The aim of Troxy
and its fast-read cache is to preserve the guarantees offered
by the underlying protocol. This is achieved by immutable
entangling the maintenance of the fast-read cache with the
protocol execution, so an attacker cannot diverge replicas and
Troxies to make conﬂicting statements. With a total amount
of 2f + 1 replicas in the hybrid fault model, completing a
write operation takes a quorum of f + 1 replicas for providing
authenticated replies. Since reply authentication is done by
Troxy inside the trusted subsystem, these f + 1 replicas must
have deleted the related entry in their fast-read cache before
the reply becomes visible to any client. Meanwhile a successful
fast-read operation also needs f + 1 identical entries, meaning
that at least f +1 replicas must still contain a matching entry in
their caches. This is not possible as both quorums intersect by
one replica and its trusted Troxy is responsible for providing
the necessary response to either side. By doing so, a successful
fast-read is ensured to reﬂect the state of the latest write.
One option for an attacker would be to roll-back the trusted
subsystem by a reboot, however in this case the cache would
simply lose its entire state and queries are returned unanswered,
which will result in the execution of the underlying protocol.
In general the forwarding of a reply due to a write request
always result in a cache invalidation but not in a cache update.
This is necessary as the local Troxy can conﬁrm the origin of
the reply but not its correctness, thus a faulty replica should
not be able to pollute the cache.
This leads to the question if a faulty replica can negatively
impact the performance beyond its capabilities in a traditional
system. This is not the case as for classical BFT systems
like PBFT [1] that feature a read optimization where 2f + 1
replicas are queried, a client can only utilize the result if all
replies match. Thus, faulty replicas can return wrong results
and frequently prevent a successful read optimization. In case
of Troxy we are in a similar situation, as we query f randomly
chosen Troxies for their cache entries. However, additionally
we measure the cache miss rate inside the Troxy. If the miss
rate reaches a conﬁgurable system constant, the fast read
optimization is avoided in favor of a traditional protocol run.
As shown in the evaluation this also addresses the case of write
contention, where a lot of cache misses occur due to conﬂicts.
V. IMPLEMENTATION
Below, we present our prototype of a Troxy-backed system,
providing details on the SGX-based Troxy implementation as
well as its integration with the BFT protocol Hybster [13].
A. Troxy Implementation
Our Troxy implementation is written in C/C++ and relies on
Intel’s Software Guard Extensions (SGX) [9] and its SDK [28]
to achieve isolation between the trusted and the untrusted
parts of a replica. The Troxy runs inside a trusted execution
environment provided by SGX, a so-called enclave, that is
protected by the CPU via transparent memory encryption and
integrity checking. To enter and exit an enclave, the only
possible way is to go through an enclave interface which deﬁnes
the entry points and the maximum number of concurrent threads
allowed at any point in time inside the enclave. An enclave call
(ecall) is needed for calling the enclave functions, while an
outside call (ocall) is explicitly used for calling from an enclave
to the untrusted environment. An ecall leads to executing a TLB
ﬂush, switching to a trusted stack located inside the enclave,
copying the parameters from untrusted memory and calling
the trusted function. Similarly, an ocall causes a TLB ﬂush,
switching back the untrusted stack, moving parameters out of
the trusted memory, and exit of the enclave. Due to their high
overhead, it is best practice to minimize enclave transitions.
Troxy implements ecalls for data transfer between enclaves
and the untrusted environment as well as for data processing
inside enclaves. In order to keep the interface small, Troxy
deﬁnes only 16 ecalls and no ocalls under a security-aware
programming model. More precisely, these ecalls have been
manually veriﬁed and are hardened to prevent possible attacks
such as Iago attacks [29] or time-of-check-to-time-of-use
attacks [30]. For example,
the data transfer between the
untrusted environment and enclaves requires additional copies
of the message buffers. A read buffer is always directly copied
into the enclave to avoid time-of-check-to-time-of-use attacks;
in contrast, the copy of a write buffer can be done outside the
enclave to achieve better performance.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:31:39 UTC from IEEE Xplore.  Restrictions apply. 
64
Phase
Request
Prepare
Commit
Collect
R0(L) R1
R2
req
pp
cmt1
cmt2
rep0
rep1
rep2
Phase
Request
Prepare
Commit
Collect
Reply
R0(L, T) R1
R2
req
pp
cmt1
cmt2
rep0
rep1
rep2
rep
Phase
Request
Forward
Prepare
Commit
Collect
Reply
R0(T)R1(L) R2