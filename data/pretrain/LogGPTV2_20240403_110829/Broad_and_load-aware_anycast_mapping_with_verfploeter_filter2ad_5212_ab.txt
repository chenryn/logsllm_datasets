One can augment these methods with traceroutes to detect possibly
spoofed replies [18]. All of these approaches require deployment of
active probes around the Internet. The largest studies we know of
use between 9000 and 10000 VPs, all the active VPs in RIPE Atlas.
Our insight is that we do not need control over active vantage
points if we can solicit messages from around Internet that will
identify their catchment. Rather than handle both queries and re-
sponses from the VPs, we instead generate queries that cause VPs to
respond and reply to the anycast system; we de(cid:27)ne these as passive
VPs. If we can capture tra(cid:28)c at all anycast catchments, we can de-
termine the catchment of each VP that responds. In e(cid:29)ect, we shift
the active side that generates and receives queries from the VP to
the anycast network itself, yet capture observations from millions
of passive VPs. (Although the anycast sites capture the data, the
ping targets are the vantage points because they each generate a
catchment report.)
Figure 1 compares these methods. On the left, traditional map-
ping sends queries (black arrows) from VPs into the anycast sys-
tem. On the right, we send queries from the anycast network block
(de(cid:27)ned by the source address), to passive VPs in most /24 IPv4 net-
works. Their replies return to the site for their catchment, even if
it is not the site that originated the query.
In Verfploeter, our queries are ICMP Echo Requests (pings), sent
using a custom program, soliciting ICMP Echo Replies. Queries are
sent from a designated measurement address that must be in the
anycast service IP pre(cid:27)x. Unlike traditional catchment mapping, it
is not the reply payload that indicates catchment, but instead the
catchment is identi(cid:27)ed by the anycast site that receives the reply.
Our passive VPs are any computers in the Internet that reply
to pings. We use a recent ISI IPv4 hitlist [17]. In principle, we
could ping every IPv4 address to get complete coverage from all
addresses that reply. We use hitlists instead because they provide
representative addresses for each /24 block that are most likely to
reply to pings, and with one address per /24 block, we can reduce
measurement tra(cid:28)c to 0.4% of a complete IPv4 scan. (We select /24s
as the smallest routable pre(cid:27)x in BGP today, since anycast depends
on BGP.)
We send requests in a pseudorandom order (following [25]), and
relatively slowly (about 6k queries per second), to spread tra(cid:28)c,
limiting tra(cid:28)c to any given network to avoid rate limits and abuse
complaints. Although well known techniques would allow much
faster probing, there is little penalty for probing over 10 or 20
minutes.
We must capture tra(cid:28)c for the measurement address with our
response collection system. We can capture tra(cid:28)c at the routers
(without having a computer at the address), or by running comput-
ers that capture tra(cid:28)c on the address itself. These captures must
happen concurrently at all anycast sites. We have three di(cid:29)erent re-
sponse collection systems: (cid:27)rst is a custom program that does packet
capture and forwards responses to a central site in near-real-time.
Second, we collect replies with LANDER [26], an existing packet
capture system that collects data continuously. Third, we have also
used tcpdump directly to capture tra(cid:28)c speci(cid:27)cally for the mea-
surement address. We use the (cid:27)rst method for Tangled and both of
the second methods at B-Root.
Capturing tra(cid:28)c at sites may be a requirement for some anycast
operators, but not a large one. While the measurement address is
in the service /24, it can be a di(cid:29)erent address and need not see
non-measurement tra(cid:28)c. Operators already operate services on
these networks, and measurement can be done either on a virtual
IP address associated with the computer providing service, on dedi-
cated measurement hardware, or by using virtual machines on the
same network. Measurement should be time synchronized across
all sites so they can be easily combined, but standard techniques
like NTP are su(cid:28)cient.
We send only a single request per destination IP address, with
no immediate retransmissions. We see replies from about 55% of
blocks (Table 4), consistent with the 56% and 59% seen in previous
studies [17]. While incomplete, we get responses for millions of
blocks. We could improve the response rate by probing multiple
targets in each block (as Trinocular does [36]), or retrying immedi-
ately. Exploration of these options is future work. Finally, we copy
all responses to a central site for analysis. Total tra(cid:28)c across the
service is about 128 MB per measurement, so it is not huge. We cur-
rently copy data manually, or with a custom program that forwards
tra(cid:28)c after tagging it with its site.
IMC ’17, November 1–3, 2017, London, United Kingdom
de Vries, Schmidt, Hardaker, Heidemann, de Boer, and Pras
Our approach to catchment mapping requires active participa-
tion at all anycast sites—it requires cooperation of the anycast
operator, but it does not require additional Internet-wide infrastruc-
ture (such as distributed VPs). Fortunately, anycast operators are
strongly motivated to understand their systems. These trade-o(cid:29)s
are the opposite of traditional anycast mapping, which requires
active VPs but not support of the target anycast system.
We do not model BGP routing to predict future catchments, we
measure actual deployment. To predict possible future catchments
from di(cid:29)erent policies, one must deploy and announce a test pre(cid:27)x
that parallels the anycast service, then measure its routes and catch-
ments. (We assume the test pre(cid:27)x will encounter the same policies
as the production pre(cid:27)x.) Fortunately, anycast providers often an-
nounce anycast on a /24 pre(cid:27)x, and a larger, covering, /23 pre(cid:27)x
with unicast (this approach protects against corner cases with some
routing policies [14]). The non-operational portion of the /23 could
serve as the test pre(cid:27)x.
3.2 Load Estimation
Planning anycast deployment is more than just mapping catchments—
di(cid:29)erent services can experience very di(cid:29)erent loads, depending on
the distribution and usage patterns of its client base. We therefore
build load estimates for each network block (/24 pre(cid:27)x) that ac-
cesses a service, so we can calibrate the loads that will be generated
by a given catchment.
We assume operators collect query logs for their systems and
can use the recorded historical data to estimate future loads. (For
example, all root operators collect this information as part of stan-
dard RSSAC-002 performance reporting [42].) For our study of B-
Root we use historical data from its unicast deployment. When no
operational load data is available, as in Tangled, one must estimate
load using data from a similar service, or assume uniform load if
no better estimates are available.
We consider three types of load: queries, good replies, and all
replies. Queries represent incoming load on the servers, while
replies are the results. Query packet load counts may di(cid:29)er from
replies if response rate limiting is used to blunt DNS ampli(cid:27)cation
attacks [45]. We separate out good replies from all replies because of
the large fraction of queries to non-present domains in root-server
tra(cid:28)c ((cid:27)rst observed in 1992 [15] and still true today); operators
may wish to optimize for volume or for good replies.
In principle, we can estimate load over any time period. Practi-
cally, we compute it over one day, and look at overall tra(cid:28)c using
hourly bins.
4 MEASUREMENT SETUP AND DATASET
Using the proposed ICMP-based method, Verfploeter, we measure
the catchment of two anycast services, B-root and an anycast
testbed (Tangled), from more than 6.4M VPs (IP addresses). (Table 1
lists all datasets we use, and each (cid:27)gure or table reports the dataset
it uses in the caption.) We add geolocation information for these
blocks using MaxMind [32]. Accuracy of this geolocation is consid-
ered reasonable at the country level [35]. We also use Route Views
and RIPE RIS data to determine the AS number for each scanned IP
address and the pre(cid:27)xes that are announced by each AS.
Id
SBA-4-20
SBA-4-21
SBA-5-15
SBV-4-21
SBV-5-15
STA-2-01
STV-2-01
STV-3-23
Service
B-Root
[38]
Method
Atlas
B-Root
[24]
Tangled [46]
Tangled
[47]
Verf-
ploeter
Atlas
Verf-
ploeter
Start Dur.
8 m
8 m
10 m
20 m
20 m
10 m
10 m
24 h
2017-04-20
2017-04-21
2017-05-15
2017-04-21
2017-05-15
2017-02-01
2017-02-01
2017-03-23
Table 1: Scans of anycast catchments for B-Root and our
testbed (Tangled). Scans were done on various days for com-
parison. Dataset STV-3-23 contains 96 measurements over 24
hours, each 10 minutes long.
Id
LB-4-12
LB-5-15
Service
B-Root [6]
B-Root [7]
Date
2017-04-12
2017-05-15
Queries
q/day
q/s
Site
2.34G 27.1k
LAX
2.20G 25.4k
both
LAX
1.78G 20.6k
MIA 0.407G 4.71k
redacted
LN-4-12 NL ccTLD 2017-04-12
Table 2: Datasets used to study load (IPv4 UDP queries only).
Data cleaning: We remove from our dataset the duplicate re-
sults, replies from IP-addresses that we did not send a request to,
and late replies (15 minutes after the start of the measurement). Du-
plicates are caused by systems replying multiple times to a single
echo request, in some cases up to thousands of times, accounting
for approximately 2% of all replies. Other systems, when pinged,
reply from a di(cid:29)erent IP-address than the original target destina-
tion. Methods such as alias resolution might clarify this, however,
further investigation is out of the scope in this paper.
4.1 B-Root
We validate the proposed methodology by providing a detailed
view of the catchment of one of the DNS root-servers. B-root is the
most recent root letter to make the change from unicast to anycast.
B-Root deployed anycast at the beginning of May, 2017 [5], adding
a site in Miami to its original site in Los Angeles (Table 3).
B’s new deployment of anycast makes it an interesting analysis
target. Unlike the other DNS Roots, B does not have a history of
anycast deployment to guide its choices (although of course it draws
on experience of other anycast deployments).
Dataset: We study B-Root catchments using several scans using
both RIPE Atlas and Verfploeter, as shown in Table 1. We estimate
B-Root load using two day-long datasets listed in Table 2. As a
baseline we use data from DITL 2017 (A Day in the Life of the
Internet [16]), taken Wednesday, 2017-04-12 (UTC), before B-Root
was using anycast. We then test against Thursday, 2017-05-15 (UTC),
after B-Root anycast was well established.
4.2 Anycast Testbed
We augment our measurements of B-Root with measurements of
our anycast testbed, Tangled. This testbed has 9 sites around the
Broad and Load-Aware Anycast Mapping with Verfploeter
IMC ’17, November 1–3, 2017, London, United Kingdom
Service Location
B-Root
US, Los Angeles
US, Miami
Tangled AU, Sydney
Upstream
Host
AS226
USC/ISI
AS20080
FIU/AMPATH
AS20473
Vultr
AS20473
Vultr
AS2500
WIDE
AS1103
Univ. of Twente
Vultr