the current and other proﬁles (where a proﬁle corresponds to
a physical user) and treat them differently. For example, some
APIs might reject calls from secondary proﬁles or enforce
more permissions on them. This type of checks also includes
user restriction checks that are used to restrict calls from
work proﬁles [26] to the main proﬁle. 3) AppOps permissions
that are complementary to normal permissions to change the
runtime behaviour. For example, when a permission is granted
but
the AppOps permission is ”ignored,” some APIs are
conﬁgured to return a dummy data or to fail silently. Since
access control in native system services has not been studied
yet, we rely on our own observations that suggest that the
access control in native APIs also uses permissions and checks
based on UID/PID. For example, permissions checks in native
APIs are routed to the PMS as a policy decision point via an
intermediate service called PermissionController.
Finally,
the high-level access control
is complemented
by low-level Discretionary and Mandatory Access Control
(DAC and MAC, respectively). DAC uses conventional Unix
permissions (based on groups and UIDs) to conﬁne application
sandboxes and processes, e.g., from circumventing the service
APIs by directly accessing the resources encapsulated by the
system services and apps, such as private user data or device
drivers. Since Android 5.0, fully enforced SELinux is used for
MAC to reinforce the conﬁnement of processes and harden the
system against privilege escalations.
III. RELATED WORKS AND MOTIVATION
We summarize related works that studied Android’s appli-
cation framework and motivate our work.
Permission Mapping: Four major works [20], [11], [12],
[5] built permission mappings of system APIs in Android.
Stowaway [20] pioneered this research by using dynamic test-
ing. It built on the intuitive idea of invoking system APIs and
observing their required permissions. Stowaway ran a testing
3
Binder DriverService ManagerApp ProcessService Process1Get reference to target service2Invoke API from target serviceUserspaceKernelpayloads causing permission leakage or Denial-of-Service. Ex-
Hunter [55] targeted improperly handled exceptions in system
services that crash the system when triggered. ASVHunter [33]
fuzzed the system_server process (i.e., the host process for
the majority of system services) and exploited weaknesses in
its concurrency control mechanism. Chizpurﬂe [34] introduced
a grey-box fuzzing leveraging runtime code instrumentation to
provide feedback on code coverage and reﬁne input generation.
FANS [38], the most recent work, fuzzed native services for
vulnerability detection. It developed a multi-level discovery
technique to access native services hidden in other services.
Problem Statement and Motivation
Studying the works that investigated the security policy of
Android’s application framework [20], [11], [12], [47], [6],
[4], [28], [5], [59], [30] reveals common features. The most
common one is that they all modeled the security policy and
demonstrated interesting use-cases on top of it, such as vul-
nerability [29], [30] and inconsistency detection [47], [4], [28]
in the policy, privilege escalation detection in apps [20], [5],
studying the evolution of Android’s permission system [11],
and cross-OEM analysis [6], [59]. The second common feature
is that they all, apart from Stowaway [20], relied on static
analysis techniques for their task.
Unfortunately, static analysis tools, although achieving high
code coverage,
tend to over-approximate, especially when
applied to massive code-bases like the one of the application
framework. There are a few reasons for that. For instance, static
analysis cannot resolve runtime variables causing uncertainty
on which path to follow, thus either following all paths or
making a hopefully right choice [47], [5]. Additionally, it
requires a good understanding of the code under test and the
used design patterns to be precise [12]. Another reason is that,
depending on the goal of the analysis, static analysis might
become expensive and forces the developers to compromise
precision for performance [8], [47], [28]. In addition to those
fundamental limitations of static analysis, solutions targeting
speciﬁcally Android’s application framework also face the
challenges of handling chained IPC between the system ser-
vices and testing services implemented in native C/C++ code.
While dynamic analysis can compensate for the limitations
of static analysis, as has been customary in the software
engineering domain, Stowaway [20], the only dynamic solution
for testing the application framework, is currently outdated,
unavailable, and technically and conceptually limited. Speciﬁ-
cally, when Stowaway is deployed as a permission mapper, it
only detects the ﬁrst encountered permission ignoring the per-
missions that are enforced in conjunction and disjunction (i.e.,
disregards path-sensitivity),
ignores non-permission checks
(e.g., hardcoded UIDs), requires source-code modiﬁcations,
entails considerable manual effort in input generation, achieves
low API-coverage, reports false positives as it struggles in
isolating noise in its feedback channel, and ignores native code.
Stowaway is further challenged by the security improvements
added to Android’s access control since its publication (e.g.,
the introduction of runtime permissions in Android 6.0 and
SELinux in Android 5.0). As such, Stowaway is technically
and conceptually limited to properly analyze the security
policy of the application framework. Additionally, although
fuzzing tools in Android exist (e.g., [14], [21], [55], [33],
[34], [38]), they only work as vulnerability scanners in the
framework—a goal that is different from modeling the security
policy. In fact, those fuzzers are designed as highly-privileged
processes that bypass all access control checks protecting the
fuzzed APIs. Identifying the security checks in exploitable
paths is usually done manually as a ﬁnal step in the analysis
and is not
the focus of those works. Thus, they are also
technically and conceptually impractical for our task.
All
in all,
the results of the prior works that studied
the security policy of the application framework have shaped
our understanding of how the framework works and clearly
advanced the ﬁeld. However, with the absence of proper
dynamic testing tools,
the adopted methodology has been
tipped to a one-sided view (i.e., static analysis) and bound by
its inherent limitations. This raises valid questions, such as:
1) To what extent are the results of this methodology accurate
and complete? 2) Can we systematically and automatically
verify them? 3) After 10 years of studying Android’s security
policy, did we actually fully uncover it? and 4) To what extent
can solutions of static analysis beneﬁt from complementary
dynamic testing. In this work, we revisit
the problem of
modeling Android’s security policy by using dynamic testing.
We show that our approach is able to conﬁrm, complete,
and refute results from previous works that studied Android’s
security policy only with static analysis. Thus, we make in this
paper a strong argument for adding dynamic code analysis
tools for Android’s application framework to the arsenal of
security researchers. A synoptical view cannot only help to
improve analysis results but also provide feedback to improve
the analysis techniques.
IV. DESIGN AND IMPLEMENTATION
We introduce in this section the design and implementation
of DYNAMO, our dynamic code analysis tool for Android’s
application framework.
A. Overview
DYNAMO builds on the simple idea of invoking the APIs
of system services from an unprivileged app and detecting
the security checks that protect those APIs. This approach is
similar to that of Stowaway [20], but our solution overcomes
Stowaway’s conceptual and technical limitations and captures
the intricate details of the access control in Android’s appli-
cation framework. For instance, DYNAMO leverages insights
from recent work [5] that highlighted the need for modeling the
security policy with respect to path-sensitivity and maintaining
the relation between different checks. DYNAMO’s design is
further tuned to detect non-permission security checks (e.g.,
abstract checks on the UID/PID of the calling process, cross-
user checks, AppOps permissions, etc) in the Java layer as
well as native code, which is missing in all previous works. To
enable DYNAMO to operate on Android images from Google
and other OEMs, it is designed to exploit runtime instrumen-
tation instead of modifying AOSP’s source code. DYNAMO
also employs techniques from the software testing domain for
automatic input generation based on a short list of predeﬁned
seeds. The testing itself can be tailored to the speciﬁcs of each
API (i.e., custom inputs and testing frequency). This feature
makes DYNAMO’s results reproducible and deterministic for
4
the same setup. Moreover, DYNAMO has proven to efﬁciently
work on all Android versions starting from Android 6.
At the heart of DYNAMO is a bi-directional communica-
tion channel that connects DYNAMO with the framework’s
components and that serves two purposes. First, it provides
the means to instrument the framework’s runtime behaviour,
e.g., changes the behaviour of selected methods. Second, it
provides a reliable feedback channel to report the security
checks that are triggered as a direct consequence of testing a
speciﬁc API. The feedback includes the execution traces of the
security checks, method-coverage information, and important
contextual information, such as the identity that is used in those
checks, their locations, and their order. The reported coverage
information can be leveraged to enhance the input generation.
B. Research Questions for Dynamic Code Analysis
Several works [14], [21], [34], [38], [55], [33] fuzzed the
application framework for vulnerability detection. While those
works addressed several
technical challenges, dynamically
testing the framework for the modeling of the security policy
of system APIs brings additional challenges. Stowaway [20]
highlighted some of them, such as how to trigger, capture, and
report permission checks. Although Stowaway serves as a great
source of inspiration to this work, we believe that the problem
of dynamically analyzing the security policy of Android’s
framework in general, and building permission mapping in
particular, has evolved since the introduction of Stowaway.
We recognize the challenges and formulate them in a series of
derivative research questions that shaped DYNAMO’s design.
(RQ1) How to identify the entry points of system APIs and
invoke them? The APIs are scattered across different system
services and implemented in Java, native code, or both. While
all previous works focused on analyzing Java-based APIs, the
security policy of native APIs remains ambiguous.
(RQ2) How to build valid API inputs? While vulnerabil-
ity scanners invest in generating inputs that trigger bugs or
crashes, modeling the security policy requires building syntac-
tically and semantically valid inputs that trigger all security
checks protecting the tested API. Automatically building such
inputs might unnecessarily exhaust the testing budget if not
complemented with a direct feedback of achieved coverage.
(RQ3) How to measure coverage? We, unfortunately, lack the
well-established tools for measuring code coverage in Android
and best efforts [34] entail a huge overhead. However, measur-
ing coverage is crucial to reﬁne the input generation strategy
and subsequently discover deep-hidden security checks.
(RQ4) How to detect and report different security checks?
While some security checks are centrally managed (e.g., per-
missions) and are straightforward to report (i.e., by placing
necessary hooks into the corresponding checking services),
other checks are inlined (e.g., comparing the caller’s UID with
a predeﬁned privileged UID) and pose a big challenge for
dynamic testing to discover them. Additionally, reporting all
checks requires a reliable feedback channel that isolates the
security checks triggered by testing an API from the noise of
unrelated security checks.
(RQ5) How to construct the feedback channel? The feed-
back channel can be implemented in the middleware via direct
Fig. 2. Steps of one testing iteration by DYNAMO
modiﬁcations to the OS. Apart from the overhead associated
with this approach, it is not scalable to other closed-source
images of different OEMs. Being able to also analyze the se-
curity policy of OEMs is crucial to uncovering and mitigating
possible erroneous changes on AOSP’s default policy.
(RQ6) How to preserve the relation and order of security
checks? Recent works [5], [29] have shown that reporting
permissions as a set over-simpliﬁes the modeling of the intri-
cate access control of system APIs and causes imprecision in
the applications built on top. Instead, different sets of security
checks must be reported for different execution paths where
each path is controlled by the API’s input or system state (path-
sensitivity). Each set must also retain the order of the checks.
C. Implementation of Dynamo
that
DYNAMO is a grey-box testing solution that consists of
two stages of operation to build the security policy of an
API. The ﬁrst stage focuses on testing the API by exercising
different input sets with the goal of increasing code coverage.
In the second stage,
those results are analyzed based on
predeﬁned association rules to model the security policy of
the target API. Figure 2 depicts an overview of DYNAMO’s
main components and presents the sequential execution steps
of one testing iteration for one API. The Testing Service
(TS) is shipped as an app component
is installed on
the tested device. It
is responsible for generating inputs,
invoking the target API, and reporting the invocation result.
The Instrumentation Server (IS) is a daemon process
that is responsible for constructing the feedback channel that
reports missing security checks and coverage information. It
is additionally responsible for modifying methods’ behaviour
at runtime at the request of the Testing Manager (TM). The
current version of DYNAMO uses a dynamic instrumentation
toolkit, called Frida [46] to run the IS. The TM runs on the
tester’s machine and orchestrates the whole process, including
setting the seeds for input generation, deﬁning the tested API,
identifying the hosting service, triggering the instrumentation
of targets, triggering API invocation, collecting the invocation
and instrumentation feedback, and ﬁnally analyzing the results.
Detecting and Preparing Target Devices: The TM uses
Android Debugging Bridge (ADB) to discover all connected
5
…PackageManagerService (PMS)UserManagerService (UMS)Testing Service(TS)InstrumentationServer (IS)Target ServiceTargetAPI()libbinder.sogetCallingUid()getCallingPid()InvokingServiceInputGeneratorInvocation Conﬁg ParserReporterTesting Manager (TM)DeviceTesting MachineThird-Party AppsApplication FrameworkNative DaemonNextAPITFEnd?Analyser Module1…checkUidPermission()hasUserRestriction()34…562Instrumentationmodule78Conﬁgure API & PayloadReachability AnalysisInstrument Targetsdevices (be it hardware or emulated) and considers them as
targets for analysis. As we will explain later, DYNAMO relies
on disabling the anti-reﬂection mechanism that is deployed in
recent versions of Android and of the SELinux enforcement.
Thus, the target device has to either run a user-debug build of
Android or be rooted (e.g., for devices of other OEMs).
Collecting Public APIs (RQ1): DYNAMO implements a
generic technique for discovering public APIs of target devices.
Speciﬁcally, the TS uses Java reﬂection to retrieve all Binder
handles from Android’s ServiceManager, casts them to their
corresponding proxies where all methods are deﬁned, and
collects all methods’ signatures. The TM pulls and saves this
list for each unique target device. Another relevant information
that is collected at this point is the mapping between services
and hosting processes, identiﬁed by Linux PIDs. This is impor-
tant information for instrumenting the APIs, constructing the
feedback channel, and impersonating different calling contexts.
Selecting API for Testing: We designed DYNAMO such that
it allows several devices running the same Android version to
work collaboratively on the same API list to speed up the
analysis. Each device locks and starts processing one API at
a time. Each tested API has a conﬁguration ﬁle that describes
the API (e.g., method’s signature and hosting process) and
the testing conﬁguration (e.g., initial seeds, strategies, log of
testing iterations). This ﬁle is responsible for the deterministic
behaviour of the tests and the ability to reproduce results.
Generating Input (RQ2): DYNAMO extracts the input types
from the API’s signature and assigns each primitive Java
parameter (e.g., string, int, boolean) a short list of predeﬁned
seed values. The list of seeds is manually created before the
testing begins (see Appendix E for the complete list). For
instance, a parameter of string type is assigned a seed list that
contains null, empty string, the package name of TS, other
package names, and a random permission name. Since not all
seed values might be semantically relevant to the tested API,
this module further leverages static analysis of the source code
(e.g., from AOSP) to reﬁne the seeds by heuristically selecting
relevant seed values based on the parameters’ names of the
API’s signature and excluding others from the predeﬁned seeds
list. For instance, an API that uses a string parameter that is