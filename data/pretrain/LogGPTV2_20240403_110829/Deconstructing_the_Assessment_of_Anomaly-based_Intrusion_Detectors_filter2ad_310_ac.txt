DQRPDORXV
ZLWKLQ
ZLWKLQ
GHWHFWRU¶V
GHWHFWRU¶V
SXUYLHZ
SXUYLHZ

$QRPDO\LV
$QRPDO\LV
VLJQLILFDQW
VLJQLILFDQW
IRUGHWHFWRU
IRUGHWHFWRU
D
$QRPDO\
$QRPDO\
127
127
VLJQLILFDQW
VLJQLILFDQW
IRUGHWHFWRU
IRUGHWHFWRU
Fig. 2. Causal chain of logical events necessary for a “hit” or “miss” to be valid and
consistent. The unshaded events lie within an evaluator’s purview while the shaded
events are within the detector’s purview.
This logical sequence of events forms the causal backbone that enables reason-
ing about the validity of evaluation results. Ambiguities in any element of this
sequence arguably compromises the integrity of evaluation results. For example,
if we compromise event 2, whereby an attack is deployed but the evaluator does
not check to ensure that it manifested in the evaluation data. In such a case,
any detector response is suspect because the response cannot be correlated to
the attack itself – there is no evidence the attack manifested in the data.
We note that the seven events in Fig. 2 can be divided into those that lie
within the purview of the evaluator (events 1, 2, 3, 3’, 6) and those that lie
within the purview of the detector (events 4, 5). This division is particularly
important when analyzing the conclusions that can be drawn from evaluation
results. Consider the case where a detector responds with a miss and the evalua-
tor cannot conﬁrm that the attack deployed actually manifested in the evaluation
data (event 2). It would be incorrect to attribute the “miss” to detection capa-
bility, since the detector may have missed because there was nothing in the data
296
A. Viswanathan, K. Tan, and C. Neuman
for it to detect despite the deployment of the attack. The fault in this case lies
with poor experimental control and does not reﬂect detector capability.
Assuming that all events that lie within the evaluator’s purview occur as
expected, two possible sequences of events can occur for a valid miss (as shown
in Fig. 2): (a) 1 → 2 → 3 → 4 → 5a → 6, and (b) 1 → 2 → 3 → 4a →
5a → 6. Event 4a (“attack NOT anomalous within detector’s purview”) and 5a
(“anomaly NOT signiﬁcant for detector”) are the perturbed versions of events
4 and 5 respectively. Since these events lie within the detector’s purview, the
perturbations can be directly correlated to factors that aﬀect detector capability,
and the miss can be conﬁdently attributed to the detector.
Consistency. From an evaluator’s point of view, evaluation of detection con-
sistency requires that the ground truth established for the evaluation corpus
also include an understanding of the stability of attack manifestation. For ex-
ample, if the attack signal is stable and yet detector performance varies then
the evidence may point toward poor detector capability, e.g., poor parameter
value selection. However, if the attack signal is itself inconsistent, causing de-
tector performance to vary, then the detector cannot be solely blamed for the
“poor” performance. Rather it is possible that the detector is performing per-
fectly in the face of signal degradation due to environmental factors. Conse-
) as an
quently in our analysis of detection consistency we add stability (event 3
event of note, i.e., to determine that a detector is capable of consistently (and
validly) detecting an attack, the following sequence of seven events (as shown
(cid:3) → 4 → 5 → 6. Similarly, for a consis-
in Fig. 2) must occur: 1 → 2 → 3 → 3
tent (and valid) miss one of the following two sequences of events must occur:
(a)1 → 2 → 3 → 3
(cid:3) → 4 → 5a → 6, and (b) 1 → 2 → 3 → 3
(cid:3)
(cid:3) → 4a → 5a → 6.
4.2 Factors Inﬂuencing Validity and Consistency
The logical sequence of events described in the previous section simply describes
the events that must occur in order to conclude that a hit, for example, is
indeed a valid and consistent hit, i.e., it is a true detection of an attack via
an anomalous manifestation, and is detected consistently. Each event in that
sequence can be compromised to, in turn, compromise the integrity of evaluation
results. This section ties those events to the set of error factors that can cause
such a compromise, as summarized in Table 1.
Rationale for Choice of Factors. In Sect. 3, we enumerated 24 factors that
contribute to errors across the ﬁve diﬀerent phases of an anomaly detector’s
evaluation. Table 1 lists only the subset of factors that compromise events for
valid and consistent detection of an attack instance, i.e., factors that aﬀect the
measurement of a “valid hit”(true positive) or a “valid miss”(false negative).
Consequently, three factors, namely DC4.2, TR1.1, and TS1.1 are not included
in Table 1. Factors DC4.2 (characterization of false alarms) and TR1.1 (repre-
sentation of real world behavior in data) only aﬀect the false positive and true
Deconstructing the Assessment of Anomaly-based Intrusion Detectors
297
Table 1. Potential error factors across the ﬁve evaluation phases (Fig. 1) that can
compromise the events (Fig. 2) necessary for valid and consistent detection.
# Event
Factors inﬂuencing valid and
consistent detection
(1) Attack is deployed.
(2) Attack manifests in evaluation data.
(3) Attack manifests in test data.
(cid:2)
(3
(4) Attack is anomalous within detector’s
) Attack manifests stably.
DC1
DC2, DC3, DC4.1
DP1, DP2, DP3
TS1.2.1, TS1.2.2
TR2.1, TR2.2
purview.
(5) Anomaly is signiﬁcant for detector.
TR1.2, TR1.3, TR2.3, TR2.4,
TR3, TR4, TS2.1, TS2.2
(6) Detector response is measured correctly. MS1, MS2
negative assessments of a detector. TS1.1 is the base-rate factor (ratio of attacks-
to-normal samples), which aﬀects the reliability of the overall assessment of an
anomaly detector’s performance but does not inﬂuence the events for valid and
consistent detection of a single attack.
Description. Table 1 lists the events that must occur to conclude a valid and
consistent detection result, along with the corresponding error factors that can
compromise the events. For the ﬁrst event, “Attack is deployed”, the factor DC1
(data generation) is a source of error that aﬀects the correct deployment or injec-
tion of an attack. For the second event, “Attack manifests in evaluation data”,
factors DC2, DC3, DC4.1 (data monitoring, data reduction and ground truth
availability respectively) are sources of error that inﬂuence the manifestation of
an attack in the raw evaluation stream. In this case, the poor use of sampling
techniques, or the lack of “ground truth” can cause attack events to disappear
from the evaluation corpus. Similarly, the error factors DP1, DP2, DP3 (data
sanitization, partitioning and conditioning), can cause an attack to disappear
from the test data stream that is consumed by the detector.
(cid:3)
(cid:3)
Factors TS1.2.1, TS1.2.2 (adversary-induced and environment-induced in-
(“Attack
stability) cause unstable manifestation of attacks and aﬀect event 3
and its factors only aﬀect the the consistency of
manifests stably”). Event 3
detection results. Error factors TR2.1, TR2.2 (choice of data features and mod-
eling formalism respectively), will inﬂuence the manifestation of an attack as an
anomaly within the detector’s purview, thus aﬀecting event 4. For example, a
detector looking at temporal features of system calls would not see attacks that
manifest as an increase in system call frequency. Similarly, a detector using a 1-
gram model of packet payloads will not see attacks that might require modeling
the dependencies between application-level tokens contained within the packet
payload.
Event 5 (“Anomaly is signiﬁcant for detector”) is aﬀected by several factors
related to the training and testing phases of an evaluation. Error factors TR1.2,
298
A. Viswanathan, K. Tan, and C. Neuman
TR1.3, TR2.3, TR2.4, TR3, TR4, TS2.1, TS2.2 (stability of training data,
attack-free training data, learning parameters, online vs. oﬄine training, the
amount of training, the model generation approach, the detection parameters,
and the similarity metric respectively) will increase or decrease the measured
signiﬁcance of an anomaly. A detector trained over highly variable data might
not be able to identify attacks as signiﬁcant anomalies. Similarly, having attacks
in the training data will cause those attacks to look benign to a detector in the
test phase. Detection parameters such as high anomaly thresholds or the choice
of a particular scoring mechanism can also cause attack-induced anomalies to
seem insigniﬁcant. We note that factors related to event 5 can heavily inﬂuence
the consistency of detection. For instance, factor TR2.4 (online vs. oﬄine learn-
ing strategy) can aﬀect the consistency of detection by changing the detector’s
perception of an anomaly over time.
Finally, the factors related to the measurement phase MS1, MS2 (deﬁnition of
metrics, and deﬁnition of anomaly respectively) inﬂuence the ﬁnal assessment
and reporting of a valid and consistent detection performance. For instance, a
mismatch between detector’s notion of a “hit” versus the real deﬁnition as it
relates to an attack can create non-generalizable results.
4.3 Deconstructing Hits and Misses: Understanding the Results
This section discusses the insuﬃciency of current evaluation approaches by show-
ing how unexplained factors across the diﬀerent evaluation phases can give rise
to multiple possible explanations for evaluation results, i.e., hits and misses.
Figure 3(a) and Fig. 3(b) show the possible sequence of events that would
explain a hit and miss from a detector respectively. In the case where an attack
is deployed and the detector detects the attack, Fig. 3(a) depicts 12 possible
sequences of events that can explain the hit, labeled case H1 to case H12 and
described in Table 2. In the case where an attack is deployed and the detector
misses the attack, Fig. 3(b) depicts 18 possible sequences of events that can
explain the miss, labeled case M1 to case M18 and described in Table 2. The
error factors deﬁned in Table 1 can be used to explain the potential causes that
resulted in each alternate sequence of events identiﬁed in Fig. 3(a) and Fig. 3(b).
The goal of an evaluation is to assess the capability of the detector and not
the validity of the experiment itself. Consequently, events 4 and 5 in Fig. 3(a)
and Fig. 3(b) are events that can be attributed to detector capability, while
, and 6 are attributed to experimental control. In Fig. 3(a) and
events 1, 2, 3, 3
Table 2 we observe only a single case (H1) that can be assessed as a valid and
consistent hit. Case H2 is assessed as a false positive because the detector alarm
was unrelated to the attack and there was no fault with experimental control,
i.e., the attack was deployed and its manifestation in the data conﬁrmed. Cases
H3 – H12 are assessed as indeterminate (denoted by the symbol ??) since the
sequence of events suggests errors both external (poor experimental control)
and internal to the detector. In all cases marked indeterminate (??), it would
be incorrect to conclude a hit since the attack does not manifest in the data,
thus the detector’s alarm was unrelated to the attack. It would also be diﬃcult
(cid:3)
Deconstructing the Assessment of Anomaly-based Intrusion Detectors
299
1 
2 
3 
Attack is 
deployed 
1a 
Attack 
improperly 
deployed 
Attack 
manifests in 
evaluation 
data 
2a 
Attack does 
NOT 
manifest in 
evaluation 
data 
Attack 
manifests in 
test data 
3a 
Attack does 
NOT 
manifest in 
test data 
(a) Deconstruction of a valid and consistent hit. 
1 
2 
3 
Attack is 
deployed 
1a 
Attack 
improperly 
deployed 
Attack 
manifests in 
evaluation 
data 
2a 
Attack does 
NOT 
manifest in 
evaluation 
data 
Attack 
manifests in 
test data 
3a 
Attack does 
NOT 
manifest in 
test data 
(b)Deconstruction of a valid and consistent miss. 
3’ 
Attack 
manifests 
stably 
3’a 
Attack does 