𝑁
𝑖=1
= ln 𝐾 𝑁
𝜎(𝑖).
We assume that the statistical feature extraction function f cal-
culates the minimum of (cid:174)𝑠 to acquire the flow-level features. 𝐼min
denotes the index of the sample with the minimum value. The
differential entropy of the feature is Hflow−min that equals to the
entropy of the random variable with the minimum value:
𝐼min = arg min
𝑠𝑖,
𝑖
∫ +∞
−∞
= ln 𝐾𝜎(𝐼min).
Hflow−min = −
𝑝𝐼min (𝑠) ln 𝑝𝐼min (𝑠)d𝑠
ΔHflow−min denotes the differential entropy loss of the minimum
feature, i.e., the difference between the overall differential entropy
and the differential entropy of the minimum feature:
ΔHflow−min = Hpacket − Hflow−min
𝜎(𝑖).
= ln 𝐾 𝑁−1 
We focus on the expectation of the loss, and leverage Jensen in-
equation to get the lower bound of the information loss:
𝑖≠𝐼min
E[ΔHflow−min] ≥ ln 𝐾 𝑁−1𝐸[𝜎 𝑁−1]
≥ (𝑁 − 1) ln 𝐾E[𝜎].
We conduct the same proof procedure for the features that calculate
the maximum of the per-packet feature sequence and complete the
proof of Theorem 1.
B PROOF OF THEOREM 2 AND THEOREM 3
We consider the situation that a flow-level feature extraction method
calculates the average number of sampled per-packet features. We
denote the average of (cid:174)𝑠 as a random variable 𝑓𝑚 that obeys a Gauss-
ian distribution:
𝑁
𝑖=1
𝑁
𝑖=1
𝑓𝑚 ∼ N( 1
𝑁
𝑢(𝑖),
1
𝑁 2
𝜎2(𝑖)).
𝑝𝑚 denotes the probability density function (PDF) of 𝑓𝑚. We use
Hflow−avg and ΔHflow−avg to indicate the differential entropy of
Session 12C: Traffic Analysis and Side ChannelsCCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3444the average feature and the information loss, respectively:
Hflow−avg = −
𝑝𝑚(𝑠) ln 𝑝𝑚(𝑠)d𝑠
−∞
= ln 𝐾
𝑁
∫ +∞
(cid:118)(cid:117)(cid:116) 𝑁
= ln 𝑁 𝐾 𝑁−1 𝑁
(cid:113)𝑁
𝜎2(𝑖),
𝑖=1
ΔHflow−avg = Hpacket − Hflow−avg
𝑖=1 𝜎(𝑖)
𝑖=1 𝜎2(𝑖)
.
To get the upper bound, we use 𝑄 to indicate the square mean of
the variances of (cid:174)𝑠. According to the inequality of arithmetic and
geometric means, the geometric mean is not bigger than the square
mean. We get the upper bound of the differential entropy loss:
ΔHflow−avg ≤ ln 𝑁 𝐾 𝑁−1
(cid:113)𝑁
𝑄 𝑁
𝑖=1 𝜎2(𝑖)
√
≤ ln
𝑁 𝐾 𝑁−1𝑄 𝑁−1.
If and only if 𝜎(𝑖) is a constant, the information loss ΔHflow−avg
reaches its maximum. We use 𝜎max to indicate the maximum of the
variances of (cid:174)𝑠, and get the lower bound of the information loss by
leveraging the non-negative differential entropy assumption:
𝜎max = max(𝜎(𝑖))
√
√
𝑁 𝐾 𝑁−1𝑁
(1 ≤ 𝑖 ≤ 𝑁),
𝑖=1 𝜎(𝑖)
𝜎max
(𝐾𝜎(𝑖) ≥ 1).
ΔHflow−avg ≥ ln
≥ ln
𝑁
The equality holds iff. 𝜎(𝑖) =
1
𝐾 . When the equality holds, the
upper bound equals the lower bound. Here we complete the proof
of Theorem 3. Similar to the proof of Theorem 1, we apply Jensen
inequation to get ΔHflow−avg and prove Theorem 2.
C PROOF OF THEOREM 4
We consider the situation that a flow-level feature extraction method
calculates the variance of the sampling sequence to extract the fea-
tures of traffic. Random variable 𝑉 denotes the variance of (cid:174)𝑠:
𝑁
𝑉 =
𝑖=1(𝑠𝑖 − 𝑢)2
𝑁
, 𝑢 =
𝑖=1 𝑠𝑖
𝑁
.
The random variable 𝑉 obeys general Chi-square distribution. We
assume that the Gaussian process S is strictly stationary with
zero mean, i.e., 𝑢(𝑖) = 0 and 𝜎(𝑖) = 𝜎. We present an estimate of
differential entropy loss when 𝑁 is large enough:
𝑁
𝑁
𝑉 =
𝑖=1 𝑠2
𝑁
𝑖
𝜎2
𝑁
=
)2,
( 𝑠𝑖
𝜎
)2 ∼ 𝜒2(𝑁).
( 𝑠𝑖
𝜎
Hflow−var denotes the differential entropy of the variance feature:
𝑁
𝑁
𝑖=1
Hflow−var = H[𝑉 ] = H[
𝑖=1
𝑁
+ H[ 𝑁
𝑖=1
+ ln 2Γ( 𝑁
]
𝑖
𝑖=1 𝑠2
𝑁
( 𝑠𝑖
𝜎
)2]
𝑁
= ln 𝜎2
= ln 𝜎2
𝑁
2 ) + (1 − 𝑁
2 )𝜓 ( 𝑁
2 ) + 𝑁
2 ,
where Γ is Gamma function and 𝜓 is Digamma function. When 𝑁
is large enough we take the even number that is closest to 𝑁 to
approach the information loss: (𝛾 is Euler–Mascheroni constant)
′ (𝑥)
Γ(𝑥)
𝜓 (𝑥)
Γ(𝑥)
Γ
= Γ
= (𝑥 − 1)!
′ (𝑥) = (𝑥 − 1)!(−𝛾 +𝑥−1
2 )! − 𝑁
𝑘=1
2 (−𝛾 +
+ ln 2( 𝑁
𝑁
𝑘 )
1
𝑁2
1
𝑘
) + 𝑁
2 .
⇒ Hflow−var = ln 𝜎2
𝑘=1
Then we approach the Harmonic series as follows,
𝑁
𝑘=1
1
𝑘
≈ ln 𝑁 + 𝛾,
⇒ Hflow−var = ln 𝜎2
𝑁
+ ln 2( 𝑁
2 )! − 𝑁
2 ln 𝑁
2 + 𝑁
2 .
Finally, we use ΔHflow−var to indicate the information loss and
leverage Stirling’s formula to approach the factorial.
ΔHflow−var = Hpacket − Hflow−var
− 𝑁
2𝜋𝑛( 𝑛
𝑒
= 𝑁 ln 𝐾𝜎 − ln 𝜎2
𝑁
(𝑛! ≈ √
)𝑛)
= 𝑁 ln 𝐾𝜎 − ln 𝜎2
− 𝑁
√
𝑁
4𝜋 𝑁 3
= 𝑁 ln 𝐾𝜎 − ln
𝜎2
.
2 − ln 2( 𝑁
2 )! + 𝑁
2 ln 𝑁
2
2 + 𝑁
2 ln 𝑁
√
𝜋 𝑁 ( 𝑁
2 − ln 2
2𝑒
) ( 𝑁2 )
Here, we complete the proof of the Theorem 4.
D PROOF OF THEOREM 5 AND THEOREM 6
Without the loss of generality, we analyze 𝑖𝑡ℎ kind of per-packet
features, and denote its sampling sequence as (cid:174)𝑠. Based on the orig-
inal assumption, we assume that Gaussian process S is strictly
stationary with zero mean, i.e., 𝑢(𝑖) = 0 and 𝜎(𝑖) = 𝜎. Whisper
extracts the frequency domain features of the per-packet feature
sampling sequence (cid:174)𝑠 with the following steps:
(1) Perform linear transformation by multiplying 𝑤𝑖 on (cid:174)𝑠, for
simplicity, we use 𝑤 to indicate 𝑤𝑖.
(2) Perform DFT on 𝑤(cid:174)𝑠. We denote the result as (cid:174)𝐹 = F (𝑤(cid:174)𝑠) and
its 𝑖𝑡ℎ element as (cid:174)𝐹𝑖 = (𝑎𝑖 + 𝑗𝑏𝑖)𝑤.
(3) Calculate modulus for the result of DFT. (cid:174)𝑃 denotes the result
and (cid:174)𝑃𝑖 = (𝑎2
(4) Perform logarithmic transformation on (cid:174)𝑃. (cid:174)𝑅 denotes the
extracted frequency domain features for (cid:174)𝑠 and (cid:174)𝑅𝑖 = ln( (cid:174)𝑃𝑖 +
1)/𝐶 denotes its 𝑖𝑡ℎ element.
The property of Discrete Fourier Transformation: F (𝑤(cid:174)𝑠) =
𝑖 )𝑤2 denotes its 𝑖𝑡ℎ element.
𝑖 + 𝑏2
𝑤F ((cid:174)𝑠), implies that:
𝑏𝑖 =𝑠𝑡 𝑎𝑖,
𝑎𝑖 ∼ N(0, 𝑁 𝜎2).
We estimate the overall differential entropy of the frequency domain
features by ignoring the impact of the logarithmic transformation
Session 12C: Traffic Analysis and Side ChannelsCCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3445Figure 9: Detection accuracy under 28 evasion attacks. During the attacks, in order to evade the detection, the attackers use
different strategies to inject benign traffic.
and obtain the entropy as HWhisper. According to the properties
of differential entropy and several inequalities about differential
entropy, we obtain an estimation for the differential entropy of the
frequency domain features:
HWhisper = H[ (cid:174)𝑃] =
𝑁
H[𝑃𝑖 ]
)2],
)2 + ( 𝑏𝑖√
H[( 𝑎𝑖√
𝑁
𝑁
𝑡𝑖 ∼ 𝜒2(2)),
)2,
We use ΔHWhisper to indicate the information loss of Whisper and
get an estimation of the differential entropy loss of Whisper:
ΔHWhisper = Hpacket − HWhisper
= 𝑁 ln 𝜎
𝑤2
− 𝑁 ln 𝑁 .
(cid:114) 𝜋
2𝑒
We complete the proof of Theorem 5. According to Theorem 1 - 4,
we can obtain Theorem 6.
𝑖 + 𝑏2
𝑖 ]
=
𝑖=1
H[𝑎2
𝑖 + 𝑏2
𝑖 )]
𝑖=1
H[𝑤2(𝑎2
𝑁
= 𝑁 ln 𝑤2 + 𝑁
= 𝑁 ln 𝑁 𝑤2 + 𝑁
HWhisper = 𝑁 ln 𝑁 𝑤2 + 𝑁
𝑖=1
)2 + ( 𝑏𝑖√
𝑁
(𝑡𝑖 = ( 𝑎𝑖√
𝑁
𝑖=1
H[𝑡𝑖 ]
= 𝑁 ln 𝑁 𝑤2 + 𝑁 (1 + ln 2).
𝑖=1
Figure 10: Detection accuracy under sophisticated evasion
strategies.
E THE DETAILED RESULTS OF ROBUST
EVALUATION
Figure 9 shows the detailed detection results under different evasion
attacks, i.e., seven types of malicious traffic mixed with benign
traffic with four types of inject ratio. We observe that the injected
benign traffic has negligible effects on the detection accuracy of
Whisper.
We also measure the effects of more sophisticated evasion strate-
gies on the detection accuracy. The strategies include (i) injecting
different types of benign traffic (i.e., ICMP, DNS, and outbound NAT
traffic that includes various types of benign traffic), (ii) changing the
rate of sending malicious packets according to the rate of benign
TLS flows, (iii) manipulating the packet length in the malicious
traffic according to the benign TLS packet length. Figure 10 shows
that the detection accuracy is not significantly impacted by the
attacks, which is consistent with the results shown in Figure 9.
SSLDoS+BenignTLSOSScaning+BenignUDPTLSPaddingOracle+BenignTLSFuzzingScan+BenignTLSACKSide-Channel+BenignTLSIPIDSide-Channel+BenignTLSTLSScanning+BenignUDP1:11:21:41:8AUC0.9150.9150.9620.9790.9830.9830.9740.9770.9170.9450.9300.9450.9810.9650.9750.9050.9820.9550.9780.9870.9620.9720.8910.9570.9390.9170.9340.956SSLDoS+BenignTLSOSScaning+BenignUDPTLSPaddingOracle+BenignTLSFuzzingScan+BenignTLSACKSide-Channel+BenignTLSIPIDSide-Channel+BenignTLSTLSScanning+BenignUDP1:11:21:41:80.7140.1570.2550.6580.4930.7210.5850.6030.5110.5750.6270.4380.4190.6460.7150.5920.6960.6150.7140.3630.4770.3950.1940.6490.7490.4210.4320.545SSLDoS+BenignTLSOSScaning+BenignUDPTLSPaddingOracle+BenignTLSFuzzingScan+BenignTLSACKSide-Channel+BenignTLSIPIDSide-Channel+BenignTLSTLSScanning+BenignUDP1:11:21:41:80.9640.8560.8430.934///0.8740.7680.7560.838///0.8200.7460.7500.644///0.6310.7440.8140.763///1:11:21:41:8EER0.1060.1720.0580.0370.0250.0240.0690.0350.1440.0860.0810.0840.0330.0740.0310.1950.0420.0650.0320.0200.0710.0350.2080.0760.0810.1030.0950.089Whisper1:11:21:41:80.3000.8330.7450.3420.5060.2790.5090.4030.5030.4210.3730.5630.5810.3330.2860.4080.3040.3860.2850.6350.5360.6020.7730.3510.2640.5790.5680.462FSC1:11:21:41:80.0460.2640.2320.070///0.1840.3280.3460.289///0.2780.3190.3120.412///0.4580.3390.2740.385///KitsuneIPIDSide-Channel+ICMP(1:1)IPIDSide-Channel+ICMP(1:2)IPIDSide-Channel+ICMP(1:4)IPIDSide-Channel+DNS(1:1)IPIDSide-Channel+DNS(1:2)IPIDSide-Channel+DNS(1:4)TLSScanning+NAT(1:1)TLSScanning+NAT(1:2)TLSScanning+NAT(1:4)IPIDSide-Channel+BenignLengthTLSPaddingOracle+BenignLengthSSLDoS+BenignRateACKSide-channel+BenignRate0.800.850.900.951.00AUCAUC0.000.020.040.060.080.10EEREERSession 12C: Traffic Analysis and Side ChannelsCCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3446