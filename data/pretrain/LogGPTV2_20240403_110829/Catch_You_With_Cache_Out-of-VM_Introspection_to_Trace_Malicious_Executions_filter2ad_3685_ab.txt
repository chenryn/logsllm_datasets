CPU cache-based side-channel attacks [2], [18], [32] utilize
system features to access cache lines with an accordant or
conﬂicting relationship. Various schemes in the literature [13]–
[16] make use of page de-duplication and huge page to reveal
the mapping of memory cache in victim processes. Other
attacks [11], [12], [21], [38] require speciﬁc shared libraries.
These leveraged features are not necessarily available in most
environments, and thus they cannot be used to perform cross-
VMs introspection. Besides, in CPU cache-based side-channel
attacks, these methods tend to monitor cache line state of a
small piece of memory, which is not practical in massive cache
states management.
Nearly all processors nowadays use a “virtually indexed
physically tagged” cache index policy in their last-level cache.
The location of the cache line for the memory is decided by
both its virtual address and physical address. Legacy out-of-
VM introspection could hardly acquire the target process’s
cache state because of software-based MMU emulation. The
isolation of memory space makes it difﬁcult for analyzers to
directly obtain the physical address, and thus cannot read its
memory state. Methods utilized in side-channel attacks to leak
memory mapping are limited by the scenarios.
If an analyzer needs to have the same virtual address and
physical address as the target, it can probe a cache line by
reading the virtual address directly. The time of the memory
read indicates whether it’s in cache or not. In other words, we
expect the introspection analyzer to have the ability to read
memory at native speed.
Immersive Execution Environment (ImEE) [39] proposes a
prototype that introspects guest address space at native speed.
Fig. 1. The dashed arrows show one possible transfer while solid arrows
show another. By probing all the cache state, two execution paths lead to
two different cache states. Cache in gray means “hit” in cache probing while
cache in white means “miss”
The basic idea of our method is to obtain cache states from
time to time so that executions can be inferred. However, even
if we keep observing cache states of all memory repeatedly,
all we can get is a heat map of code execution. There is so
much information to deal with that the analyzer could hardly
stay synchronized with the target process.
In our core scheme, instead of scanning cache states of the
whole memory, we only perform cache probing on branches
of identiﬁed code blocks. For instance, in Figure 1, assuming
that cache state of gadget 0 is identiﬁed as hit, we need to
identify gadget 1 and gadget 2. If the cache probing shows
a hit at gadget 1 and miss at gadget 2, then gadget 1 is
recently executed while gadget 2 is not. Algorithm 1 presents
the pseudo code of the core scheme.
III. DESIGN OF CATCHER
Catcher is designed to analyze malware in a non-intrusive
way. Other than pausing or relaunching the malware, Catcher
dynamically fetches malware’s codes, analyzes code distri-
bution, and then introspects execution without disturbing the
target process. By probing the cache status of basic blocks
constantly, it detects if these basic blocks are executed. The
states of cache (whether it hits or misses) are closely associated
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:25:25 UTC from IEEE Xplore.  Restrictions apply. 
327
cache states Bcache states Astartendgadget 0gadget 1gadget 3gadget 5gadget 2gadget 4gadget 6cache 0cache 1cache 2cache 3cache 4cache 5cache 6cache 6cache 0cache 1cache 2cache 3cache 4cache 5gap into the analyzer. Instead of probing code blocks one by
one, we leave out some of the blocks to reduce the impact
of switching CR3. After the analyzer identiﬁes one recently
executed code block, it tries to probe on the cache lines that
should be executed in a ﬁxed time instead of the next code
blocks. There might be a few code blocks between the one
we choose and the identiﬁed block. By this means, we lower
the rate of switching to reduce the impact of being out of
synchronization.
Fig. 3. Communication between the analyzer and the agent over probes
Figure 3 illustrates the communication between analyzer,
agent, and target. An agent will set the interface lock on after
it receives a VMI request from the analyzer, then loads CR3 of
target and access memory. The results are stored in registers
to make sure they are available after CR3 is switched back.
The analyzer needs to wait until the lock is off and then read
the result from the agent. Agents are designed to access the
target’s memory at native speed. The communication includes
two CR3 loads and memory access, so it might take some time,
which is the key to deﬁne the execution time gap between two
probes.
To evaluate the CPU cycles between two probes, we add
rdtsc instructions in our analyzer and record CPU cycles
every time the analyzer sends VMI request. It takes around
2500 CPU cycles between two probes so the interval
is
set as 2500 cycles. Intel 64 and IA-32 Architectures Opti-
mization Reference Manual [4] documents the latency and
throughput of different instructions. Under the guidance of
[8], we estimate the execution time of every code block so
that we pick a basic block around 2500 cycles away from the
previous one. In other words, when the analyzer ﬁnishes its
ﬁrst probe, it prefers a basic block about 2500 CPU cycles
away from the ﬁrst probing. This heuristic makes the analyzer
and target process run at approximately equal speed, mitigating
the synchronization problem.
2) Branch Prediction and Speculative Execution Issue: A
program may contain a branch that depends on a result from
a prior slow instruction. Rather than wait for the result, CPU
can predict the branch and speculatively execute instructions.
To maintain the correctness of the process, CPU tracks the
Fig. 2. By switching CR3, the agent can read from both the local and the
target’s memory space. It shares the same VA and PA with the target if the
agent is loaded with its CR3
Typically an out-of-VM analysis program synchronizes the
CR3 with the target VM so that its MMU can use target’s VA-
to-GPA mappings directly. Inspired by ImEE, Catcher provides
a smart way for the analysis program to share the same virtual
address and physical address. As is illustrated in Figure 2,
Catcher launches an agent that switches its CR3 to the one
from the target. Agent’s mappings are ensured to be consistent
with the target. It provides a method for the agent to share the
same virtual address and physical address as the target, and
thus it can read the target’s cache state directly across VMs. In
this period the analyzer do nothing but cache probing. After the
agent switches back to its own CR3, the analyzer can still be
executed in its memory address space under its local mapping.
B. Heuristics
The fundamental base of Catcher is CPU cache, which
is intensely limited by environmental factors. Although the
hypothesis seems to be valid, the result of the analyzer is not
practical enough. The false-positive rate is high due to the
design of microarchitecture in CPUs. Much of the instability
in the implementation stems from the situations listed below.
1) Synchronization Issue: Different from traditional dy-
namic analysis tools like binary instrumentation, Catcher does
not let the target trigger anything to assist tracing. In the ideal
case, the analyzer and target processes are executed at the
same speed. The analyzer can probe on code blocks that have
just been executed. However, the high speed of modern CPU
clock speed makes it hard to keep both analyzer and target
running at the same speed. The instruction pointer of the target
might have been far ahead of the analyzer already. Long time
intervals between cache line usage and probing might lead to
an eviction. The time gap between the target and analyzer will
be greater as time goes on if there is no synchronization.
To minimize the gap, Catcher sets a code interval in probing.
The velocity contrast mostly results from switching memory
space between the analyzer and the agents. The analyzer
sends requests to inform its agents so that they probe speciﬁc
memory regions. This takes extra time, making the execution
lagging behind. So the ﬁrst heuristics is to introduce a code
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:25:25 UTC from IEEE Xplore.  Restrictions apply. 
328
analyzertargetagentlocal GPTlocal EPTlocal memory spaceagent's EPTEPTtarget's memoryspaceCR3CR3CR3CR3 switchtarget's GPTAnalyzerAgentTargetinform agent which memoryregion to access with a requestload target's CR3 and switchto target's address spaceread memory ofinformed region;copy into agent'sregistersload local CR3 andswitch back to localaddress spaceset the lockcopy the data fromregisters into localmemory page;remove the lockread requireddate frommemory pagesend second requestout the upper bound of instructions in ROB seems to vary
among different instructions. More complicated instructions
like add and idiv of r32, which take 10 and 88 µ-ops,
have lower upper bounds. In worst cases, ROB can only hold
3 instructions like idiv with operands r64.
The upper bound of the ROB suggests that before CPU
realizes its misprediction and rolls back, speculative execution
won’t cover the code near the end of the basic blocks. Even
if the size of the basic blocks is small enough to ﬁt in ROB,
it will not always have a bad inﬂuence on our analysis. As
a matter of fact, branch predictors in modern CPUs are so
accurate that in most circumstances they always choose the
right branch. A 2-bit dynamic predictor was used in [24] where
the misprediction rate is less than 10%. High accuracy of
branch prediction mitigates the problem caused by speculative
execution.
3) Shared Library Issue: Both Windows and Linux are
introduced with shared library techniques to save disk and
memory storage. .dll ﬁles in Windows and .so ﬁles in Linux are
dynamically linked library ﬁles, which can be shared through
several processes. Other than static linking, dynamic linking no
longer copies codes of libraries into its executables. Instead,
these libraries are linked at run time. They are loaded only
once, and different programs are able to use the same copy in
memory.
Cache states can be confusing because they can be affected
by different processes through shared libraries. Both the target
process and other irrelevant processes are executing the same
library. In our heuristics, once the control ﬂow turns into
a shared library, we stop tracing. We are not interested in
the functions of shared libraries. When the destination of a
control ﬂow transfer is a shared library, the analyzer will not
trace it anymore. Instead, it handles it as if it loses its trace
routine. Resetting strategies will be illustrated in the following
sections.
speculatively executed instruction and their results in a reorder
buffer (ROB). If the prior result comes and the speculative ex-
ecution is valid, CPUs can retire these speculatively executed
instructions in ROB. However, if the guess is wrong, CPUs
have to discard the incorrect instructions in ROB and run from
the correct codes.
Although the instructions are discarded from ROB and the
result of the code executed is roll-backed, the inﬂuence has
already been made in CPU cache. Spectre and Meltdown vul-
nerabilities [19], [22] reveal the fact that even the correctness
of the program is guaranteed, the cache state has already
changed by speculative execution. Misprediction leads to a
situation that both branches are loaded in cache. Catcher will
be confused and cannot infer what the real execution trace
looks like.
To mitigate this problem, we propose the heuristic that
the analyzer prefers addresses near the end of basic blocks.
Addresses far away from the branch instruction might be
not polluted by speculative execution. Because of the limits
on reorder buffer, speculative execution will not keep going
perpetually. Once the reorder buffer is full of out-of-order
instructions to be executed, CPUs stop speculative execution.
If we choose an address far enough, the code at this address
hasn’t been speculatively executed, and thus it will no longer
be affected by branch prediction and speculative execution.
Unfortunately, the size of ROB is not documented. To evalu-
ate the capacity of ROB, we designed a method under the idea
of [33]. We ﬁrst set two memory reads to uncached memories.
Between these two mov instructions, we add various ﬁller
instructions. If ROB is big enough for all these instructions,
two memory reads can be executed in parallel. So if we
measure the access time of these two memory reads, both
latency is low. However, if we ﬁll in so many ﬁller instructions
that the ROB is full, the second memory read needs to wait
for the CPU to issue instructions in ROB. This will cause the
latency of two memory reads to be different. So if we increase
the number of ﬁller instructions gradually until memory access
latency at the second memory read changes, we can deﬁne the
capacity of the ROB.
Fig. 4. Measure the size of ROB. The decline of cache hit percentage means
number of ﬁlter instructions is over ROB’s capacity.
We test an i5 3470 of Ivy bridge, and Figure 4 shows its
ROB can hold approximately 225 nop instructions before the
latency of the second memory read declines. We also ﬁnd
To skip the shared libraries, the analyzer learns the mapping
of libraries. By checking Linux’s /proc ﬁle system, we are able
to know the addresses of different .so ﬁles. /proc/pid/maps
shows a detailed distribution of libraries used by the process.
Tools in Windows, like Process Explorer and ListDLLs, are
also available. Process Explorer shows information of DLL
recently opened or loaded. ListDLLs reports the DLLs loaded
into processes. Other tools like Tasklist depends and dllshow
provide information on DLL usage as well.
4) Noise Issue: Noise is always an essential problem of
works [30], [35] based on CPU cache. Cache is originally
designed for CPU to access data from memory quickly, and
it shouldn’t be used as a side-channel. CPU cache states can
be affected by the environment like CPU loads. To evaluate
the impact of system noise caused by CPU loads, we run
a test on cache. We start with a list of memory addresses
mapping to different cache lines. By reading them one by
one, we make sure each one of them should be in cache
theoretically. Under different CPU loads with stress -m,
we read them again and evaluate the access time with rdtsc
instructions. If its access time is short, this memory should
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:25:25 UTC from IEEE Xplore.  Restrictions apply. 
329
time on choosing an address magniﬁes the time gap between
the analyzer’s and target’s execution mentioned before.
Catcher implements a continuing strategy under the guid-
ance of stack. When a function has been called, it pushes
its parameters and return address into the stack. As stack is
a last in ﬁrst out data structure, there is a high likelihood
that the function at the top of the stack will be executed
in the following moment. The continuing strategy aims at
return addresses recorded in stack, and keep probing them
until one of the return addresses ﬁnally goes through a cache
hit, indicating this function has returned. The analyzer can
restart tracing at this recently returned address.
To trace the stack, Catcher maintains snapshots of the
stack pages with a speciﬁc monitoring process. A process is
designed to monitor the memory regions of the stack by its
own agents. By comparing snapshots, it deﬁnes the top of the
stack. If the memory area of the lowest address is modiﬁed,
these changed parts are deemed to be function frames that
were recently pushed into the stack.
This heuristic requires the analyzer to be aware of the ad-
dress of the stack. However, Catcher agents are not supposed to
access registers. Stack-related registers like ESP and EBP are
not available to the agent, leaving the analyzers unknown of
the address of the stack. We need to perform a stack revealing
at the prologue phase. Stack is actually a memory area of
different stack frames. Every frame is loaded with parameters
and return addresses. By ﬁnding memory pages with several
return addresses, we can ﬁnd where the stack is. So when
the analyzer dumps all the code and goes through reverse
engineering, it marks all the addresses of CALL instructions.
In this way, all the return addresses are known because they
are the same as addresses of instructions next to the CALL
instructions. Although modern compilers are likely to use
JMP instructions to replace CALL instructions, the left CALLs
are still enough to expose the stack. [6] shows in coreutils
compiled by gcc at -O2, only about 10% of the function calls
are in the form of JMPs. Once the monitoring process knows
where the stack is, it can keeps eye on it persistently. Whenever
the analyzer lost trace of the target program, this monitoring
process recommends an address that should be executed in no
time.
be in cache, meaning it has not been evicted yet. Otherwise,
this cache line is already affected by system loads. Then we
measure the inﬂuence on memories out of cache. Similarly,
we use CLFLUSH instructions to make sure all these memory
addresses are out of cache. Then we read them and evaluate the
access time to ﬁnd out how many addresses are now in CPU
cache. We deploy our experiment on Ubuntu 12.04 with Linux
kernel 3.2.79. We tested cache activities under quiet system,
stress -m 2 and stress -m 4. The percentages of
addresses affected by system loads are presented in Table I.
It draws a conclusion that system loads have a strong impact
on cache states, introducing extra noise on cache probing. The
system load has less effect on these memories out of cache.
PERCENTAGES OF MEMORY CACHE AFFECTED BY SYSTEM LOAD
TABLE I
% of affected memory (cached):
% of affected memory (uncached):
silent
1.5
0.1
-m 2
-m 4
7.1
6.8