4.6
1.5
33.8
10
56.2
23.8
8.5
1.5
100
Figure 9: The use of parameters by VTrace users.
Figure 10: Statistics of VTrace usage and diagnosis result.
multi-tenancy nature. Operations engineers are experienced in di-
agnosing such faults based on detailed network symptoms without
VTrace. For instance, when multiple tenants raise fault diagnosis
requests, the virtual network team highly suspects it is a VFD is-
sue. Then, operations engineers check the corresponding network
configurations and locate the possible culprit device among the
common VFDs that serve all of these tenants, which could reduce
the debug effort. In this case, compared to manual diagnosis, VTrace
provides the convenience and efficiency of automated diagnostics.
However, for a non-VFD fault, though operations engineers could
find the problematic VFD after expending a great deal of effort look-
ing at the packet drop counters of all possible VFDs, they can not
discover any anomaly in these VFDs. Besides, the restart or recon-
figuration actions on the cloud provider side can not resolve the
problem since no changes will be made to tenant-related config-
urations. As a result, it is quite tricky to troubleshoot these bugs.
After proving the innocence of VFDs, the tenant will get feedback:
"The cloud network is OK. Please check your own network con-
figuration." This answer may not be satisfactory. With VTrace, the
feedback will become "The problem may be with your bandwidth
settings". Our tenants are happy to receive such useful information.
5.3 Experience from the Usage of VTrace
Currently, VTrace has been called by cloud operations engineers
to complete more than 10000 tasks since its deployment. In the
following, we summarize the experience of using VTrace for about
12 months in our production cloud network.
Choices for VTrace parameters. For each task, operations en-
gineers assigned parameters to packet_count and trace_time
based on their expertise. With data collected, we show the distribu-
tion of these two parameters used in production in Fig. 9. We find
that VTrace tasks with packet_count of 10 to 30 account for about
87.4%, which indicates that usually tens of packets are sufficient for
41
debugging persistent packet loss issues. In addition, approximately
54.1% and 32.8% of the trace_time parameters are set to be 10
to 40 seconds and 500 to 600 seconds, respectively, where a large
trace_time is usually assigned to a small flow.
VTrace usage and diagnosis result. After data collection, the
number of VTrace being called per week is presented In Fig. 10.
Besides, the diagnosis result (i.e., VTrace showing packet loss) is
also included. From Fig. 10, we find that packet loss issues are not
common. In most cases, VTrace shows that the overlay network is
normal. In addition, it is interesting to see that there is a surge in
the number of VTrace tasks issued over a period of time, as well
as an increase in packet loss problems. This is because there are
some online shopping carnivals from early September to the end of
December, which places severe demands on tenants’ ability to cope
with a surge in business (especially those related to e-commerce,
live video, etc). Hence, tenants will expand their cloud resources
and conduct pressure tests to meet the up-coming huge business.
As a result, these tenant-related operations inevitably lead to the
increase of VTrace tasks and packet loss issues.
The distribution of root causes. As VFD faults can be well
avoided through software tests before deployment, they rarely
appear in the production network. Differently, non-VFD faults are
usually common. Here, we collect root causes for non-VFD faults
and classified them into five types, of which the respective pro-
portion is provided in Table 4. Obviously, the packet losses on
vSwitches account for the vast majority (66.2%), compared to that
on vRouters (33.8%). Taking a closer look, for vSwitches, there is
an interesting finding that the cause tenant security policy blocking
occupies more than half (52.4%), whereas the most common error
for vRouters is tenant configuration error (18.5%), which reminds
VTrace to pay more attention to these types of errors. Besides, the
first four types of causes in Table 4 are all related to tenant op-
erations. What’s more, there are also packet losses on vRouters
induced by unknown causes (1.5%), which means that the culprit
VFD can be figured out with VTrace, but not the specific root cause.
Such situations are due to the insufficiency of debug probes de-
ployed in vRouters, which indicates the necessity of maintaining
and updating the repository of debug probes.
6 DISCUSSIONS
Can VTrace troubleshoot the packet drops in the physical
network? In the overlay network, we see that packets go from
a VFD to another one. Indeed, between two VFDs, the packets
are transmitted via the physical network, which is invisible in the
overlay network. Consequently, the connection status of the overlay
network is destined to be influenced when a fault occurs in the
100103101 102 packet_count00.20.40.60.81CDF101103102trace_time (seconds)00.20.40.60.81CDF7911157Month020040060080010001200NumberVTrace being called VTrace showing packet loss3SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
C. Fang et al.
injecting any probe. Instead, they took advantage of the information
collected from end hosts and VFDs. Nevertheless, these solutions
still need tenants to help collect data. However, VTrace is required
to be completely non-intrusive for cloud tenants.
Network debugging methods: The rapid growth of SDN gives a
chance to collect network-wide information efficiently and simplify
the time-consuming network debugging tasks. Projects such as
SDN Traceroute [1], RuleScope [3], NetSight [7], Simon [17] and
CherryPick [21] designed several solutions for flow tracing and net-
work problem troubleshooting in SDNs. In addition, several systems
are designed for debugging network problems in large data center
networks. Trumpet [15] leverages CPU resources and end-host pro-
grammability to monitor every packet and report events efficiently.
Everflow [26] shows its scalability and flexibility of accessing the
packet-level information in the large data center networks. Differ-
ent from the above, VND [24] proposes a framework that enables
a cloud provider to offer virtual network diagnosis as a service.
And NetAlytics [12] provides an application performance monitor
for the cloud. However, all these works perform well in locating
the culprit node, but they can not present the underlying reason
for which a packet is discarded. Recently, the newly-proposed INT
[11] can compress the internal states of the switches, such as the
packet processing latency, into the data packet at each hop, which
also enables the diagnosis of packet drops. But, it will face high
processing pressure for switches given the huge amount of cloud
production traffic, and consume a lot of bandwidth resources as the
size of the data packet increases, especially in a many-hop flow. In
contrast, VTrace presents a flexible and light-weight solution.
8 CONCLUSION
We have presented VTrace, an automatic diagnostic system for
persistent packet loss in the cloud-scale overlay network. VTrace
enables the automatic diagnosis through installing several "coloring,
matching and logging" rules in VFDs and inspects a small number
of target packets in depth by taking advantage of the "fast path-
slow path" design of VFDs. In this way, the root cause of persistent
packet loss can be efficiently dug out at an affordable cost. VTrace
has been deployed in Alibaba Cloud network for over 20 months
to automatically diagnose practical persistent packet loss in the
overlay network. Our experiences demonstrate that VTrace can di-
agnose such problems in minutes, which greatly benefit operations
engineers, and it has become an indispensable tool for cloud-scale
overlay network troubleshooting.
ACKNOWLEDGMENTS
We thank our shepherd T. S. Eugene Ng and SIGCOMM reviewers
for their thoughtful feedback. Chongrong Fang, Haoyu Liu, Peng
Cheng and Jiming Chen are supported in part by the National Key
Research and Development Program under Grant 2018YFB0803501,
and by the National Natural Science Foundation of China (NSFC)
under Grant 61833015. This work was supported by Alibaba Group
through Alibaba Innovative Research Program. This work was also
supported by Alibaba-Zhejiang University Joint Research Institute
of Frontier Technologies.
Figure 11: Schematic diagram for VTrace-stats.
physical network. Currently, VTrace can only locate the faulty node
in a certain segment of the physical network which is responsible
for the packet transmission between the servers where the two
VFDs reside. To pinpoint the culprit node and extract the root cause
of the packet drops, VTrace can configure PFDs to flexibly send a
post-card containing the minimal information of the root cause to
a dedicated server when encountering packet drops. Such an idea
is similar to INT [11].
Will VTrace be able to diagnose transient packet drops in the
overlay network? The temporary non-availability of a network
induced by transient packet drops is also a significant apprehension
for cloud providers. As such problems are unpredictable, it requires
VTrace to run on all possible VFDs for a considerably long time
(e.g., an hour) to catch the momentary packet loss. Though VTrace
is light-weight, the impact resulted from the long-term running in
the high-throughput production environment will be unexpected
and unaffordable. In this context, the "coloring and matching" idea
comes in handy. Specifically, as Fig. 11 shows, source VFDs can
"color" target packets and all VFDs along the possible flow paths will
count the number of the "colored" inlet and outlet packets, respec-
tively. Based on the counted statistics, one can directly determine if
a VFD drops packets and the proportion of discarded packets. This
function is named VTrace-stats. The implementation of VTrace-stats
is rather simple. It only needs to configure a "match and color" rule
in source VFDs and a "match and count" rule in the ingress and
egress of all VFDs. These rules can be installed in the fast path
directly. With regard to the impact on forwarding performance, we
have deployed VTrace-stats on a vSwitch in our test environment
and counted all packets passing through in an hour. Compared
to the VTrace-free scenario, the average forwarding rate is down
by 6%, which is acceptable. Besides, we have implemented a pilot
deployment of VTrace-stats in more than 2 production datacenters
for 6 months. Though there is still a lot of room for improvement
in VTrace-stats, this idea improves VTrace a lot.
7 RELATED WORK
Active network monitoring methods: Active probing, such as
Pingmesh, ATPG and Cisco IPSLA [6, 8, 25], entails injecting and
tracing test probes in the network to measure connectivity. These
methods can easily get latency data and pinpoint the culprit nodes.
However, they may not be appropriate for VTrace since the probing
traffic and the production traffic may travel along different paths,
which prevents reflecting the on-site forwarding conditions.
Passive network monitoring methods: Contrast with active
probing, passive solutions like VeriFlow [10], [16] and [19] im-
plemented quality-of-experience measurement techniques without
42
SrcDstVFD1VFD2VFD3……VFD4VFDnOut:1000In:1000Out:900In:1000Out:900In:900Out:0In:0Out:0In:0The true flow pathThe possible flow pathsVTrace
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
[11] Changhoon Kim, Anirudh Sivaraman, Naga Katta, Antonin Bas, Advait Dixit,
and Lawrence J Wobker. 2015. In-band network telemetry via programmable
dataplanes. In SIGCOMM.
[12] Guyue Liu, Michael Trotter, Yuxin Ren, and Timothy Wood. 2016. NetAlyt-
ics: Cloud-scale application performance monitoring with SDN and NFV. In
Middleware. 8.
[13] Stackdriver Logging. 2015. https://cloud.google.com/logging. (2015).
[14] Mallik Mahalingam, Dinesh Dutt, Kenneth Duda, Puneet Agarwal, Lawrence
Kreeger, T Sridhar, Mike Bursell, and Chris Wright. 2014. Virtual extensible local
area network (VXLAN): A framework for overlaying virtualized layer 2 networks
over layer 3 networks. Technical Report.
[15] Masoud Moshref, Minlan Yu, Ramesh Govindan, and Amin Vahdat. 2016. Trumpet:
Timely and precise triggers in data centers. In SIGCOMM. 129–143.
[16] Hyunwoo Nam, Kyung-Hwa Kim, Jong Yul Kim, and Henning Schulzrinne. 2014.
Towards QoE-aware video streaming using SDN. In GLOBECOM. 1317–1322.
[17] Tim Nelson, Da Yu, Yiming Li, Rodrigo Fonseca, and Shriram Krishnamurthi.
2015. Simon: Scriptable interactive monitoring for SDNs. In SOSR. 19.
[18] Spirent TestCenter
Platforms.
2020.
/media/brochures/spirent_platform_brochure.pdf. (2020).
https://www.spirent.com/-
[19] Arjun Roy, Hongyi Zeng, Jasmeet Bagga, and Alex C Snoeren. 2017. Passive
realtime datacenter fault detection and localization. In NSDI. 595–612.
[20] Apache Storm. 2011. http://storm.apache.org. (2011).
[21] Praveen Tammana, Rachit Agarwal, and Myungjin Lee. 2015. Cherrypick: Tracing
packet trajectory in software-defined datacenter networks. In SOSR. 23.
[22] TCPDump. 1999. http://www.tcpdump.org. (1999).
[23] Open vSwitch: An open virtual switch. 2010. http://openvswitch.org/. (2010).
[24] Wenfei Wu, Guohui Wang, Aditya Akella, and Anees Shaikh. 2013. Virtual
network diagnosis as a service. In SOCC. 9.
[25] Hongyi Zeng, Peyman Kazemian, George Varghese, and Nick McKeown. 2012.
Automatic test packet generation. In CoNEXT. 241–252.
[26] Yibo Zhu, Nanxi Kang, Jiaxin Cao, Albert Greenberg, Guohan Lu, Ratul Mahajan,
Dave Maltz, Lihua Yuan, Ming Zhang, Ben Y. Zhao, and Haitao Zheng. 2015.
Packet-level telemetry in large datacenter networks. In SIGCOMM. 479–491.
REFERENCES
[1] Kanak Agarwal, Eric Rozner, Colin Dixon, and John Carter. 2014. SDN traceroute:
Tracing SDN forwarding without changing network behavior. In HotSDN. 145–
150.
[2] Alibaba. 2014. JStorm: A distributed and fault-tolerant realtime computation
system. https://github.com/alibaba/jstorm/wiki (2014).
[3] Kai Bu, Xitao Wen, Bo Yang, Yan Chen, Li Erran Li, and Xiaolin Chen. 2016.
Is every flow on the right track?: Inspect SDN forwarding with RuleScope. In
INFOCOM. 1–9.
[4] Michael Dalton, David Schultz, Jacob Adriaens, Ahsan Arefin, Anshuman
Gupta, Brian Fahs, Dima Rubinstein, Enrique Cauich Zermeno, Erik Rubow,
James Alexander Docauer, Jesse Alpert, Jing Ai, Jon Olson, Kevin DeCabooter,
Marc de Kruijf, Nan Hua, Nathan Lewis, Nikhil Kasinadhuni, Riccardo Crepaldi,
Srinivas Krishnan, Subbaiah Venkata, Yossi Richter, Uday Naik, and Amin Vahdat.
2018. Andromeda: Performance, isolation, and velocity at scale in cloud network
virtualization. In NSDI. 373–387.
[5] Datadog. 2010. https://www.datadoghq.com. (2010).
[6] Chuanxiong Guo, Lihua Yuan, Dong Xiang, Yingnong Dang, Ray Huang, Dave
Maltz, Zhaoyi Liu, Vin Wang, Bin Pang, Hua Chen, Zhi-Wei Lin, and Varugis
Kurien. 2015. Pingmesh: A large-scale system for data center network latency
measurement and analysis. In SIGCOMM. 139–152.
[7] Nikhil Handigol, Brandon Heller, Vimalkumar Jeyakumar, David Mazières, and
I know what your packet did last hop: Using packet
Nick McKeown. 2014.
histories to troubleshoot networks. In NSDI. 71–85.
[8] Americas Headquarters. 2012.
IPv6 configuration guide, Cisco ios re-
lease 12.4.
Technical Report. Cisco, Tech. Rep., 2012.[Online]. Avail-
able: http://www. cisco. com/c/en/us/td/docs/ios-xml/ios/ipv6/configuration/12-
4t/ipv6-12-4t-book/ip6-eigrp. html.
[9] Raj Jain and Subharthi Paul. 2013. Network virtualization and software defined
networking for cloud computing: A survey. IEEE Communications Magazine 51,
11 (2013), 24–31.
[10] Ahmed Khurshid, Wenxuan Zhou, Matthew Caesar, and P. Brighten Godfrey.
2012. Veriflow: Verifying network-wide invariants in real time. In HotSDN. 49–54.
43