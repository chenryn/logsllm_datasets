### Compliments of
- Mark Needham
- Michael Hunger
- Michael Simmons

### Serverless SQL Analytics
**Powered by DuckDB**

#### Use Cases:
- Cloud data warehouse for the rest of us
- Data lake query engine
- Serverless backend for Data Apps

#### Features:
- Beautiful Web IDE and Notebook for DuckDB
- Centralized data storage and sharing for organizations
- Support for many tools in the MDS ecosystem

#### Deployment Options:
- **Cloud**: Hybrid querying with MotherDuck
- **Laptop**: Local querying with DuckDB

---

## This Book is in MEAP - Manning Early Access Program

### What is MEAP?
Writing a book can take a year or more, so how do you learn about the latest technology today? The answer is MEAP, the Manning Early Access Program. In MEAP, you read a book chapter-by-chapter as it's being written and receive the final eBook as soon as it's completed. 

MEAP offers several benefits over the traditional "wait to read" model:
- **Get started now**: Read early versions of the chapters before the book is finished.
- **Regular updates**: We'll notify you when updates are available.
- **Contribute to the writing process**: Your feedback in the liveBook Discussion Forum helps improve the book.

To learn more about MEAP, visit [https://www.manning.com/meap-program](https://www.manning.com/meap-program).

---

## Foreword

Welcome, dear reader, to this book about DuckDB. It feels somewhat surreal to write a foreword for a book about DuckDB because everything has happened so quickly. The world of data management systems typically moves slowly—software projects from the 1970s are still prominent in the market.

It was only a few short years ago, in 2018, that we sat in the “Joost” bar in Amsterdam and decided to build a new system. We had toyed with the idea before but were hesitant, knowing it was a daunting task. The common wisdom is that it takes “ten million dollars” to make a new database system successful. However, we had an equally bold plan: to create a unique in-process analytical system. Maybe the usual rules did not apply to this new kind of system.

After a few more beers, we sketched out the first rough draft of DuckDB’s architecture. The very next day, we started coding. A year later, in 2019, we opened our repository and showed the first demo at the SIGMOD conference in Amsterdam. We even snuck stickers into the goodie bags for some early viral marketing. At the same time, we made the source code public.

While thousands of open-source projects start every day, most never gain traction. We expected the same for DuckDB. However, something amazing happened: the GitHub stars started accumulating. We believe this is due to our design goal of ease of use. We observed that many data systems were difficult to use, and their hard-won research results were often ignored. DuckDB aimed to be as user-friendly as possible, addressing common frustrations.

People noticed. Significant popularity came from social media activity, especially on Twitter and Hacker News. Today, DuckDB has millions of downloads each month and is used everywhere—from large companies to small embedded devices. MotherDuck offers a hosted version with a strong local component. People are even writing books about DuckDB.

We’re honored that Mark and the two Michaels are bringing this book to you. They are experts in explaining complex data technology in a fun, engaging, and deeply competent way. We hope you will enjoy this book and working with DuckDB.

—Mark Raasveldt and Hannes Mühleisen, Creators of DuckDB

---

## Brief Contents

### Part 1: First Steps with DuckDB
1. An Introduction to DuckDB
2. Getting Started with DuckDB
3. Executing SQL Queries

### Part 2: Exploring and Understanding Data
4. Advanced Aggregation and Analysis of Data
5. Exploring Data without Persistence
6. Understanding the Architecture

### Part 3: Python Integration and Beyond
7. Integrating with the Python Ecosystem
8. Building Data Pipelines with DuckDB
9. Working with Large Datasets
10. Providing Data Analytics in the Browser

### Part 4: Extension and Future Directions
11. Extending DuckDB
12. Diving into the Ecosystem
13. Thinking About the Future

---

## Chapter 1: An Introduction to DuckDB

### This Chapter Covers:
- Why DuckDB, a single-node in-memory database, emerged in the era of big data
- DuckDB’s capabilities
- How DuckDB works and fits into your data pipeline

We’re excited that you’ve picked up this book and are ready to learn about a technology that seems to go against the grain of everything we’ve learned about big data systems over the last decade. We’ve had a lot of fun using DuckDB and hope you will be as enthused as we are after reading this book. Our approach is hands-on, concise, fast-paced, and includes lots of code examples.

After reading the book, you should be able to use DuckDB to analyze tabular data in various formats. You will also have a new handy tool for data transformation, cleanup, and conversion. You can integrate it into your Python notebooks and processes to replace pandas DataFrames in situations where they are not performing well. You will be able to build quick applications for data analysis using Streamlit with DuckDB.

Let’s get started!

### 1.1 What is DuckDB?

DuckDB is a modern embedded analytics database that runs on your machine and allows you to efficiently process and query gigabytes of data from different sources. It was created in 2018 by Mark Raasveldt and Hannes Mühleisen, who were researchers in database systems at Centrum Wiskunde & Informatica (CWI) in the Netherlands, along with their advisor Peter Boncz.

The founders and CWI spun off DuckDB Labs as a startup to further develop DuckDB. Its engineering team focuses on making DuckDB more efficient, user-friendly, and better integrated. The non-profit DuckDB Foundation governs the project, safeguarding the intellectual property and ensuring the continuity of the open-source project under the MIT license. The foundation’s operations and DuckDB’s development are supported by commercial members, while association members can inform the development roadmap.

While DuckDB focuses on local data processing, another startup, MotherDuck, aims to extend DuckDB to a distributed, self-serve analytics system that can process data in the cloud and on the edge. It adds collaboration and sharing capabilities to DuckDB and supports processing data from various cloud storage options.

The DuckDB ecosystem is broad, with many people and organizations creating integrations and generally usable applications. The community is helpful and friendly, and you can find them on Discord and GitHub. The documentation is comprehensive and detailed enough to answer most questions.

DuckDB lets you process and join local or remote files in different formats, including CSV, JSON, Parquet, and Arrow, as well as databases like MySQL, SQLite, and Postgres. You can even query pandas or Polars DataFrames from your Python scripts or Jupyter notebooks. A diagram showing how DuckDB is typically used is shown in Figure 1.1.

**Figure 1.1: DuckDB and Other Tools in the Ecosystem**

Unlike the pandas and Polars libraries, DuckDB is a real analytics database, implementing efficient data processing mechanisms that can handle large volumes of data in seconds. With its SQL dialect, even complex queries can be expressed more succinctly. It allows you to handle more operations inside the database, avoiding costly roundtrips to your client.

The architecture of the core database engine is the basis for efficient processing and memory management. A diagram showing the way a query is processed is shown in Figure 1.2.

**Figure 1.2: High-Level Overview of DuckDB’s Architecture**

DuckDB processes queries the same way as other databases, with a SQL parser, query execution planner, and query runtime. The query engine is vectorized, meaning it processes chunks of data in parallel and benefits from modern multi-core CPU architectures. DuckDB supports several extensions, user-defined functions, and various user interfaces, including a CLI, API, or lower-level integration into other systems.

### 1.2 Why Should You Care About DuckDB?

DuckDB makes data analytics fast and fun again, without the need to set up large Apache Spark clusters or run a cloud data warehouse just to process a few hundred gigabytes of data. Directly accessing data from many different sources and running the processing where the data resides, without copying it over the wire, makes your work faster, simpler, and cheaper. This not only saves time but also a lot of money and reduces frustration.

For example, we recently had to process AWS access log files residing in S3. Usually, we would run AWS Athena SQL queries against the compressed JSON files, which can be expensive. Now, we can deploy DuckDB to an EC2 VM and query the files in-process for a fraction of the cost.

With DuckDB, you can run lots of experiments and validate your ideas and hypotheses quickly and locally, all using just SQL. As well as supporting the ANSI SQL standard, DuckDB’s SQL dialect includes innovations such as:
- Simplifying `SELECT *` queries with `SELECT * EXCLUDE()` and `SELECT * REPLACE()`
- Ordering by and grouping results by all columns, e.g., `GROUP BY ALL`
- Using `PIVOT` and `UNPIVOT` to transpose rows and columns
- The `STRUCT` data type and associated functions, which make it easy to work with complex nested data

We are excited about DuckDB because it simplifies data pipelines and data preparation, allowing more time for actual analysis, exploration, and experimentation. In this book, we hope to convince you of the following:
- It is faster than SQLite for analytical workloads.
- It is easier to set up than a Spark cluster.
- It has lower resource requirements than pandas.
- It doesn’t throw weird Rust errors like Polars.
- It is easier to set up and use than Postgres, Redshift, and other relational databases.
- It is faster and more powerful for data transformations than Talend.

### 1.3 When Should You Use DuckDB?

You can use DuckDB for all analytics tasks that can be expressed in SQL and work on structured data (i.e., tables or documents) as long as your data is already available (not streaming) and data volumes don’t exceed a few hundred gigabytes. DuckDB can process a variety of data formats and can be extended to integrate with other systems.

As the data doesn’t leave your system (local or privacy-guaranteed hosting), it’s great for analyzing privacy-related data like health information, home automation data, patient data, personal identifying information, financial statements, and similar datasets. Here are some examples of common analysis tasks that DuckDB is well-suited for:
- Analyzing log files from where they are stored, without needing to copy them to new locations.
- Quantifying personal medical data, such as a runner monitoring heart rates.
- Reporting on power generation and consumption using data from smart meters.
- Optimizing ride data from modern transport operations for bikes and cars.
- Pre-processing and cleaning user-generated data for machine learning training.

A great use of DuckDB is for more efficiently processing data that is already available in pandas or Polars DataFrames, as it can access the data in-process without having to copy it from the DataFrame memory representation. The same is true for outputs and tables generated by DuckDB; these can be used as DataFrames without additional memory usage or transfer.

### 1.4 When Should You Not Use DuckDB?

As DuckDB is an analytics database, it has minimal support for transactions and parallel write access. Therefore, it is not suitable for applications and APIs that process and store input data arriving arbitrarily. Similarly, it is not ideal for multiple concurrent processes reading from a writable database.

The data volumes you can process with DuckDB are mostly limited by the main memory of your computer. While it supports spilling over memory (out-of-memory processing) to disk, this feature is more for exceptional situations where the last few percent of processing don’t fit into memory. In most cases, this means you’ll have a limit of a few hundred gigabytes for processing, not all of which needs to be in memory at the same time, as DuckDB optimizes loading only what’s needed.

DuckDB focuses on the long tail of data analytics use cases, so if you’re in an enterprise environment with a complex setup and processing many terabytes of data, DuckDB might not be the right choice for you.

DuckDB does not support processing live data streams that update continuously. Data updates should happen in bulk by loading new tables or large chunks of new data at once. DuckDB is not a streaming real-time database, so you would have to implement a batching approach yourself by setting up a process to create mini-batches of data from the stream and store those mini-batches somewhere that could then be queried by DuckDB.

### 1.5 Use Cases

There are many use cases for a tool like DuckDB. The most exciting is when it can be integrated with existing cloud, mobile, desktop, and command-line applications and do its job behind the scenes. In these cases, it would be the equivalent of the broad usage of SQLite today, but for analytical processing instead of transactional data storage. When analyzing data that shouldn’t leave the user’s device, such as health, training, financial, or home automation data, an efficient local infrastructure comes in handy. The local analytics and pre-processing also reduce the volume of data that has to be transported from edge devices like smart meters or sensors.

DuckDB is also useful for fast analysis of larger datasets, such as log files, where computation and reduction can be done where the data is stored, saving high data transfer time and costs. Currently, cloud vendors offer expensive analytics services like BigQuery, Redshift, and Athena to process this kind of data. In the future, you can replace many of those uses with scheduled cloud functions processing the data with DuckDB. You can also chain those processing functions by writing out intermediate results to cloud storage, which can then be used for auditing.

For Data Scientists, data preparation, analysis, filtering, and aggregation can be done more efficiently than with pandas or other DataFrame libraries by leveraging DuckDB’s state-of-the-art query engine. And all of this without leaving the comfortable environment of a notebook with Python or R APIs. This will put more advanced data analytics capabilities in the hands of data science users, enabling them to make better use of larger data volumes while being faster and more efficient. We will show several of these later in the book. Also, the complexity of setup can be greatly reduced, removing the need to involve a data operations group.

A final exciting use case will be the distributed analysis of data between cloud storage, edge network, and local device. This is currently being worked on by MotherDuck, which allows you to run DuckDB combined in the cloud and locally.

### 1.6 Where Does DuckDB Fit In?

This book assumes that you have some existing data that you want to analyze or transform. That data can reside in flat files like CSV, Parquet, or JSON, or another database system, like Postgres or SQLite.

Depending on your use case, you can use DuckDB transiently to transform, filter, and pass the data through to another format. In most cases, though, you will create tables for your data to persist it for subsequent, high-performance analysis.