Compliments of
Mark Needham
MMiicchhaaeell HHuunnggeerr
MMiicchhaaeell SSiimmoonnss
MM AA NN NN II NN GG
Serverless SQL Analytics
Powered by DuckDB
Use it for:
Cloud data warehouse for the rest of us
Data lake query engine
Serverless backend for Data Apps
Includes:
Beautiful Web IDE and Notebook for DuckDB
centralized data storage and sharing for orgs
support for many tools in the mds ecosystem
CLOUD
DUCKDB WITH THE
LAPTOP
QUERYING: USE YOUR
HYBRID
MOTHERDUCK.COM
This book is in MEAP - Manning Early Access Program
What is MEAP?
A book can take a year or more to write, so how do you learn that hot new
technology today? The answer is MEAP, the Manning Early Access Program. In
MEAP, you read a book chapter-by-chapter while it's being written and get the final
eBook as soon as it's finished. In MEAP, you get the book before it's finished, and
we commit to sending it to you immediately when it is published. The content you
get is not finished and will evolve, sometimes dramatically, before it is good enough
for us to publish. But you get the chapter drafts when you need the information.
And you get a chance to let the author know what you think, which can really help
us both make a better book.
MEAP offers several benefits over the traditional "wait to read" model.
• Get started now. You can read early versions of the chapters before the book
is finished.
• Regular updates. We'll let you know when updates are available.
• Contribute to the writing process. Your feedback in the liveBook Discussion
Forum makes the book better.
To learn more about MEAP, visit https://www.manning.com/meap-program.
MEAP Edition
Manning Early Access Program
DuckDB in Action
Version 2
Copyright 2023 Manning Publications
For more information on this and other Manning titles go to manning.com.
© Manning Publications Co. To comment go to liveBook
Foreword
Welcome, dear reader, to this book about DuckDB. It feels somewhat surreal to write a foreword
for a book about DuckDB because it feels like everything has happened so quickly. The world of
data management systems moves slowly—software projects started in the ’70s are still in a
strong position on the market.
It has only been a few short years since an evening in 2018 when we sat in the “Joost” bar
in Amsterdam, and decided that we were going to build a new system. We had been toying with
the concept before but had been hesitant as we knew it was a daft idea. The common wisdom is
that it takes “ten million dollars” to render a new database system successful. But there was an
equally daft plan. We would create a unique new data management system never built before:
An in-process analytical system. Maybe the usual rules did not apply to this new kind of system.
After some more beers, we more or less decided on the first rough draft of DuckDB’s architecture.
The very next day, we started hacking.
Only a year later, in 2019, we opened our repository and started telling people. We showed
a first demo of DuckDB at the 2019 SIGMOD conference, coincidentally in Amsterdam. Since we
co-organized the conference, we snuck stickers in the goodie bags in an early attempt of
somewhat viral marketing. At the same time, we also opened up the source code repository to
the public. The duck was out of the bag, as the saying goes.
But there are thousands of open-source projects started every day, and the vast majority
will—regrettably or not—never gain any traction. That was what we expected, too; most likely,
nobody will care about our “DuckDB.” But an amazing thing happened: gradually, the stars on
the GitHub repository started accumulating. We think it is because of another design goal of
DuckDB: ease of use. We observed that the prevailing sentiment in data systems seemed to
have been that the world should be grateful to be allowed to use the hard-won results of database
systems research and the systems we build. However, we also observed a worrying effect. The
results of decades of research were ignored, simply because they were hard to use. In somewhat
of a paradigm shift for data systems, one design goal of DuckDB was to make it as easy to use
as possible and to fix some of the biggest gripes we had heard from practitioners.
Somehow, people seem to have noticed. Considerable popularity gains came from activity on
the social network formerly known as Twitter and, most notably, from regularly being featured
on the Hacker News. Today, DuckDB has millions of downloads each month and is used
everywhere—from the largest companies to the smallest embedded devices. MotherDuck offers
a hosted version but DuckDB style with a strong local component. Heck, people are even writing
books about DuckDB.
We’re glad that Mark and the two Michaels are the ones who bring this book to you. It’s an
honor for us that such an excellent team is writing this book. They are experts in explaining
challenging data technology to developers in a fun, engaging, but still deeply competent way.
We hope you will enjoy this book and, of course, also hope you will enjoy working with DuckDB.
—MARK RAASVELDT AND HANNES MÜHLEISEN
CREATORS OF DUCKDB
brief contents
PART 1: FIRST STEPS WITH DUCKDB
1 An introduction to DuckDB
2 Getting Started with DuckDB
3 Execting SQL Queries
PART 2: EXPLORING AND UNDERSTANDING DATA
4 Advanced aggregation and analysis of data
5 Exploring data without persistence
6 Understanding the architecture
PART 3: PYTHON INTEGRATION AND BEYOND
7 Integrating with the Python ecosystem
8 Building data pipelines with DuckDB
9 Working with large datasets
10 Providing Data Analytics in the Browser
PART 4: EXTENSION AND FUTURE DIRECTION
11 Extending DuckDB
12 Diving into the ecosystem
13 Thinking about the future
© Manning Publications Co. To comment go to liveBook
1
1
An introduction to DuckDB
This chapter covers
Why DuckDB, a single node in-memory database, emerged in the era of big data
DuckDB’s capabilities
How DuckDB works and fits into your data pipeline
We’re excited that you’ve picked up this book and are ready to learn about a technology
that seems to go against the grain of everything that we’ve learned about big data
systems over the last decade. We’ve had a lot of fun using DuckDB and we hope you will
be as enthused as we are after reading this book. This book’s approach to teaching is
hands-on, concise, fast-paced, and will include lots of code examples.
After reading the book you should be able to use DuckDB to analyze tabular data in a
variety of formats. You will also have a new handy tool in your toolbox for data
transformation, cleanup and conversion. You can integrate it into your Python notebooks
and processes to replace pandas DataFrames in situations where they are not
performing. You will be able to build quick applications for data analysis using Streamlit
with DuckDB.
Let’s get started!
1.1 What is DuckDB?
DuckDB is a modern embedded analytics database that runs on your machine and lets
you efficiently process and query gigabytes of data from different sources.
© Manning Publications Co. To comment go to liveBook
2
It was created in 2018 by Mark Raasveldt and Hannes Mühleisen who, at the time,
were researchers in database systems at Centrum Wiskunde & Informatica (CWI) - the
national research institute for mathematics and computer science in the Netherlands and
their advisor Peter Boncz.
The founders and the CWI spun DuckDB Labs off as a startup to further develop
DuckDB. Its engineering team focuses on making DuckDB more efficient, user-friendly,
and better integrated.
The non-profit DuckDB Foundation governs the DuckDB Project by safeguarding the
intellectual property and ensuring the continuity of the open-source project under the
MIT license. The foundations operations and DuckDB’s development are supported by
commercial members, while association members can inform the development roadmap.
While DuckDB focuses on the local processing of data, another startup, MotherDuck,
aims to extend DuckDB to a distributed, self-serve analytics system that can process
data in the cloud and on the edge. It adds collaboration and sharing capabilities to
DuckDB, and supports processing data from all kinds of cloud storage.
The DuckDB ecosystem is really broad, many people and organizations are excited
about the possibilities and create integrations and generally useable applications.
The DuckDB community is very helpful and friendly, you can find them on Discord and
GitHub. The documentation is comprehensive and detailed enough to answer most
questions.
DuckDB lets you process and join local or remote files in different formats, including
CSV, JSON, Parquet, and Arrow, as well as databases like MySQL, SQLite, and Postgres.
You can even query pandas or Polars DataFrames from your Python scripts or Jupyter
notebooks. A diagram showing how DuckDB is typically used is shown in figure 1.1.
Figure 1.1 DuckDB and other tools in the ecosystem
© Manning Publications Co. To comment go to liveBook
3
Unlike the pandas and Polars libraries, DuckDB is a real analytics database,
implementing efficient data processing mechanisms that can handle large volumes of
data in seconds. With its SQL dialect, even complex queries can be expressed more
succinctly. It allows you to handle more operations inside the database avoiding costly
roundtrips to your client.
The architecture of the core database engine is the basis for efficient processing and
memory management. You can see a diagram showing the way that a query is
processed in figure 1.2.
Figure 1.2 A high-level overview of DuckDB’s architecture
We can see that DuckDB processes queries the same way as other databases, with a
SQL parser, query execution planner, and query runtime. The query engine is vectorized,
which means it processes chunks of data in parallel and benefits from modern multi-core
CPU architectures. DuckDB supports several extensions, as well as user-defined
functions, and has a variety of user interfaces, including a CLI, API, or lower-level
integration into other systems.
1.2 Why should you care about DuckDB?
DuckDB makes data analytics fast and fun again, without the need to set up large
Apache Spark clusters or run a cloud data warehouse just to process a few hundred
gigabytes of data. Accessing data from many different sources directly, and running the
processing where the data resides without copying it over the wire, makes your work
faster, simpler and cheaper. This not only saves time but also a lot of money, as well as
reducing frustration.
For example, we recently had to process AWS access log files residing in S3. Usually,
we would run AWS Athena SQL queries against the compressed JSON files. This tends to
get expensive as the biggest part of the cost is based on the amount of data scanned by
the analytics service. Now we can instead deploy DuckDB to an EC2 VM and query the
files in process for a fraction of the cost.
With DuckDB you can run lots of experiments and validate your ideas and hypotheses
quickly and locally, and all of this using just SQL. As well as supporting the ANSI SQL
standard, DuckDB’s SQL dialect also includes innovations like:
Simplifying SELECT * queries with SELECT * EXCLUDE() and SELECT *
REPLACE()
Ordering by and grouping results by ALL columns, e.g. GROUP BY ALL
saves the user typing out all field names.
Using PIVOT and UNPIVOT to transpose rows and columns
© Manning Publications Co. To comment go to liveBook
4
The STRUCT data type and associated functions, which make it easy to
work with complex nested data.
We are excited about DuckDB, because it helps to simplify data pipelines and data
preparation, allowing more time for the actual analysis, exploration and experimentation.
In this book, we hope to convince you of the following:
It is faster than SQLite for analytical workloads.
It is easier to set up than a Spark cluster.
It has lower resource requirements than pandas.
It doesn’t throw weird Rust errors like Polars.
It is easier to set up and use than Postgres, Redshift and other
relational databases.
It is faster and more powerful for data transformations than Talend
1.3 When should you use DuckDB?
You can use DuckDB for all analytics tasks that can be expressed in SQL and work on
structured data (i.e. tables or documents) as long as your data is already available (not
streaming) and data volumes don’t exceed a few hundred Gigabytes. DuckDB can
process a variety of data formats as outlined before and can be extended to integrate
with other systems.
As the data doesn’t leave your system (local or privacy-guaranteed hosting), it’s also
great for analyzing privacy-related data like health information, home automation data,
patient data, personal identifying information, financial statements and similar datasets.
Here are some examples of some common analysis tasks that DuckDB is well-placed
to solve:
Analyzing log files from where they are stored, without needing to copy
them to new locations.
Quantifying personal medical data about one’s self, such as a runner
might do when monitoring heart rates.
Reporting on the power generation and consumption using data from
smart meters.
Optimizing ride data from modern transport operations for bikes and
cars.
Pre-processing and -cleaning of user-generated data for machine
learning training.
A great use of DuckDB is for more efficiently processing data that is already available in
pandas or Polars DataFrames because it can access the data in-process without having
to copy the data from the DataFrame memory representation.
The same is true for outputs and tables generated by DuckDB. These can be used as
DataFrames without additional memory usage or transfer.
© Manning Publications Co. To comment go to liveBook
5
1.4 When should you not use DuckDB?
As DuckDB is an analytics database, it has only minimal support for transactions and
parallel write access. You therefore couldn’t use it in applications and APIs that process
and store input data arriving arbitrarily. Similarly when multiple concurrent processes
read from a writeable database.
The data volumes that you can process with DuckDB are mostly limited by the main
memory of your computer. While it supports spilling over memory (out-of-memory
processing) to disk, that feature is more aimed at exceptional situations where the last
few percent of processing don’t fit into memory. In most cases, that means you’ll have a
limit of a few hundred gigabytes for processing, not all of which needs to be in memory
at the same time, as DuckDB optimizes loading only what’s needed.
DuckDB focuses on the long tail of data analytics use cases, so if you’re in an
enterprise environment with a complex setup, processing many Terabytes of data,
DuckDB might not be the right choice for you.
DuckDB does not support processing live data streams that update continuously. Data
updates should happen in bulk by loading new tables or large chunks of new data at
once. DuckDB is not a streaming real-time database, you would have to implement a
batching approach yourself by setting up a process to create mini-batches of data from
the stream and store those mini-batches somewhere that could then be queried by
DuckDB.
1.5 Use cases
There are many use cases for a tool like DuckDB. Of course, the most exciting is when it
can be integrated with existing cloud, mobile, desktop and command line applications
and do its job behind the scenes. In these cases, it would be the equivalent of the broad
usage of SQLite today, only for analytical processing instead of transactional data
storage. When analyzing data that shouldn’t leave the user’s device, such as health,
training, financial or home automation data, an efficient local infrastructure comes in
handy. The local analytics and pre-processing also reduces the volume of data that has
to be transported from edge-devices like smart meters or sensors.
DuckDB is also useful for fast analysis of larger datasets, such as log files, where
computation and reduction can be done where the data is stored, saving high data
transfer time and costs. Currently, cloud vendors offer expensive analytics services like
BigQuery, Redshift and Athena to process this kind of data. In the future, you can
replace many of those uses with scheduled cloud functions processing the data with
DuckDB. You can also chain those processing functions by writing out intermediate
results to cloud storage which can then also be used for auditing.
© Manning Publications Co. To comment go to liveBook
6
For Data Scientists, data preparation, analysis, filtering and aggregation can be done
more efficiently than with pandas or other DataFrame libraries, by leveraging DuckDB’s
state-of-the-art query engine. And all of this without leaving the comfortable
environment of a notebook with Python or R APIs. This will put more advanced data
analytics capabilities in the hands of data science users so that they can make better use
of larger data volumes while being faster and more efficient. We will show several of
these later in the book. Also, the complexity of setup can be greatly reduced, removing
the need to involve a data operations group.
A final exciting use case will be the distributed analysis of data between cloud storage,
edge network, and local device. This is for instance currently being worked on by
MotherDuck which allows you to run DuckDB combined in the cloud and locally.
1.6 Where does DuckDB fit in?
This book assumes that you have some existing data that you want to analyze or
transform. That data can reside in flat files like CSV, Parquet, or JSON, or another
database system, like Postgres or SQLite.
Depending on your use case, you can use DuckDB transiently to transform, filter and
pass the data through to another format. In most cases, though, you will create tables
for your data to persist it for subsequent, high-performance analysis. When doing that,