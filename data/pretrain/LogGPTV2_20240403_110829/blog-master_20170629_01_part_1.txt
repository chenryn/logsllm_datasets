## PostgreSQL\GPDB 毫秒级海量时空数据透视 典型案例分享         
##### [TAG 18](../class/18.md)
### 作者                      
digoal                      
### 日期                      
2017-06-29                     
### 标签                      
PostgreSQL , GIS , 时空数据 , 数据透视 , bitmapAnd , bitmapOr , multi-index , 分区 , brin , geohash cluster      
----                      
## 背景         
随着移动终端的普及，现在有越来越多的业务数据会包含空间数据，例如手机用户的FEED信息、物联网、车联网、气象传感器的数据、动物的溯源数据，一系列跟踪数据。  
这些数据具备这几个维度的属性：  
1、空间  
2、时间  
3、业务属性，例如温度、湿度、消费额、油耗、等。  
数据透视是企业BI、分析师、运营非常关心的，以往的透视可能是时间结合业务维度的，现在会加入空间的透视（例如某个点附近，在某个时间段的透视；或者某个省的数据透视；或者北纬度附近的数据透视等）。  
数据实时透视的一个关键点是预计算、实时计算、流式计算。下面有一个案例：  
[《PostgreSQL\GPDB 毫秒级海量多维数据透视 案例分享》](../201706/20170625_01.md)    
以上案例的数据中不包含空间维度，本文将介绍包含 "空间、时间、业务" 等多个维度数据透视的数据库设计和DEMO。  
## 一、场景介绍  
我们以一个场景为例，讲解时空数据透视。  
在滴滴、出租车、公交车、大巴、危化品车辆上安装了传感器，这些传感器的位置是跟着汽车流动的，同时这些传感器会不断的生成数据并上报数据。  
同时还有一些静止的传感器，也会上传并上报数据。  
数据结构有3种，根据不同的上报模式，对应不同的结构。  
1、单条上报模式  
```  
table  
(  
  sid int,       -- 传感器ID  
  pos geometry,  -- 传感器位置  
  ts timestamp,  -- 时间  
  val1 int,      -- 传感器探测到的属性1值  
  val2 float,    -- 传感器探测到的属性2值   
  val3 text      -- 传感器探测到的属性3值  
  ......   
)  
```  
2、批量上报模式，聚合后的明细(一条记录包含多个VALUE)  
```  
table  
(  
  sid int,         -- 传感器ID  
  pos geometry[],  -- 传感器位置数组  
  ts timestamp[],  -- 时间数组  
  val1 int[],      -- 传感器探测到的属性1值数组  
  val2 float[],    -- 传感器探测到的属性2值数组  
  val3 text[]      -- 传感器探测到的属性3值数组  
  ......   
)  
```  
3、批量上报模式，JSON打包的明细  
```  
table  
(  
  sid int,         -- 传感器ID  
  info jsonb       -- 批量打包数据 {k1: {pos:val, ts:val, val1:val, val2:val, ...}, k2: {}, k3: {}, ....}  
)  
```  
## 二、架构设计  
数据透视架构设计，分为两种，一种比较暴力，实时裸算，需要更多的计算能力，但是比较适合无法建模的透视需求。  
另一种，对于可以建模的透视，则可以通过预处理的方式，使用更低的成本，提高透视的响应速度。  
如下  
一、实时架构  
实时模式，指数据写入后，用户根据需求查询，统计结果。实时模式的查询响应速度取决于集群的计算能力，通常为了达到高速响应，需要大量的投入。  
![pic](20170629_01_pic_003.jpg)    
实时计算除了提高计算能力，通常还可以通过任意列索引来提高透视的响应速度，例如：  
[《多字段，任意组合条件查询(无需建模) - 毫秒级实时圈人 最佳实践》](../201706/20170607_02.md)    
二、预处理架构  
预处理的方法较多，例如流式计算、T+N调度，lambda调度。  
![pic](20170629_01_pic_004.jpg)    
1、流处理  
流计算可以选择PostgreSQL的pipelineDB插件（预计7月份插件化），支持TTL，滑动窗口，估值统计，以及PG内置的聚合统计函数等。性能也非常不错，单机可以达到100万行/s的处理速度。  
[《流计算风云再起 - PostgreSQL携PipelineDB力挺IoT》](../201612/20161220_01.md)    
数据来源实时写入pipelinedb进行流式计算，流式计算的结果（例如统计维度为天，TTL设置为7天，每天将前天的统计结果写入报表库RDS PG或者HDB PG），数据来源的明细数据如果要留底，则可以将其写入 HDB PG或OSS。  
这种架构设计，对于可以建模的透视，可以做到毫秒级的响应。对于无法建模的透视需求（需要使用明细进行实时的计算），同样可以使用HDB PG的并行计算能力，得到较快速度的响应。  
![pic](20170629_01_pic_005.jpg)    
2、T+n 调度  
T+n 调度，实际上是一种常见的报表系统的做法，例如在凌晨将明细数据导入到数据库或者OSS中，根据建模，生成报表。  
案例如下：  
[《PostgreSQL\GPDB 多维数据透视典型案例分享》](../201706/20170625_01.md)   
![pic](20170629_01_pic_006.jpg)    
3、lambda 调度  
T+N调度，只是将流计算节点更换成HDB PG或者RDS PG，通过FUNCIONT和任务调度的方式，增量的对建模数据进行统计和合并统计结果。  
案例如下：  
[《(流式、lambda、触发器)实时处理大比拼 - 物联网(IoT)\金融,时序处理最佳实践》](../201705/20170518_01.md)    
## 三、分区规则设计  
分区规则指数据在某一个数据节点内的分区规则，分区规则应考虑到数据的查询方式，例如经常按时间、空间范围搜索或查询，所以我们有两个分区维度。  
PostgreSQL, HDB都支持多级分区，因此可以在这两个维度上进行多级分区。  
```  
   [ PARTITION BY partition_type (column)  
       [ SUBPARTITION BY partition_type (column) ]   
          [ SUBPARTITION TEMPLATE ( template_spec ) ]  
       [...]  
    ( partition_spec )   
        | [ SUBPARTITION BY partition_type (column) ]  
          [...]  
    ( partition_spec   
      [ ( subpartition_spec   
           [(...)]   
         ) ]   
    )  
where partition_type is:  
    LIST  
  | RANGE  
where partition_specification is:  
partition_element [, ...]  
and partition_element is:  
   DEFAULT PARTITION name  
  | [PARTITION name] VALUES (list_value [,...] )  
  | [PARTITION name]   
     START ([datatype] 'start_value') [INCLUSIVE | EXCLUSIVE]  
     [ END ([datatype] 'end_value') [INCLUSIVE | EXCLUSIVE] ]  
     [ EVERY ([datatype] [number | INTERVAL] 'interval_value') ]  
  | [PARTITION name]   
     END ([datatype] 'end_value') [INCLUSIVE | EXCLUSIVE]  
     [ EVERY ([datatype] [number | INTERVAL] 'interval_value') ]  
[ WITH ( partition_storage_parameter=value [, ... ] ) ]  
[column_reference_storage_directive [, ...] ]  
where subpartition_spec or template_spec is:  
subpartition_element [, ...]  
and subpartition_element is:  
   DEFAULT SUBPARTITION name  
  | [SUBPARTITION name] VALUES (list_value [,...] )  
  | [SUBPARTITION name]   
     START ([datatype] 'start_value') [INCLUSIVE | EXCLUSIVE]  
     [ END ([datatype] 'end_value') [INCLUSIVE | EXCLUSIVE] ]  
     [ EVERY ([datatype] [number | INTERVAL] 'interval_value') ]  
  | [SUBPARTITION name]   
     END ([datatype] 'end_value') [INCLUSIVE | EXCLUSIVE]  
     [ EVERY ([datatype] [number | INTERVAL] 'interval_value') ]  
[ WITH ( partition_storage_parameter=value [, ... ] ) ]  
[column_reference_storage_directive [, ...] ]  
[ TABLESPACE tablespace ]  
```  
1、时间范围，例如每天一个分区。  
2、GEOHASH范围，geohash是经纬坐标的编码值，代表一个BOX，编码长度决定了它的精度(BOX的大小)，相邻的BOX的编码PREFIX相同。因此使用geohash进行范围编码是可行的。  
例如用户需要搜索某个时间段的数据，可以使用分区规则，挑选出对应的分区进行查询，从而缩小搜索的范围。  
用户需要搜索某个空间范围的数据，通过GEOHASH范围分区，同样可以挑选出对应的分区进行查询，从而缩小搜索的范围。  
![pic](20170629_01_pic_007.jpg)    
HDB不仅仅支持geohash同时支持geometry，在geometry上可以建立GiST空间索引，使用空间索引可以支持KNN检索（精准检索，与BOX无关）。      
## 四、分布规则设计  
分布规则指数据在多个数据节点层面的分布，不要与分区规则一致。我们可以选择随机或者业务相关字段作为分布规则，同时需要考虑数据的倾斜。  
关于分区和分布列的选择，可以参考这篇文档：  
[《Greenplum 最佳实践 - 数据分布黄金法则 - 分布列与分区的选择》](../201607/20160719_02.md)    
1、随机分布，数据将随机写入不同的数据节点，保证了数据的均匀性。但是查询时，需要调用所有数据节点进行查询。如果是JOIN，还会涉及数据的重分布。  
2、业务ID，按业务ID来分布，当按业务ID进行查询时，只需要调用对应业务ID所在数据节点，但是请务必考虑到数据倾斜，例如某个业务ID的数据特别多，那么可能导致分布不均匀。  
PS：（一致性HASH解决了分布不均匀的问题。）  
![pic](20170629_01_pic_008.jpg)    
## 五、预计算设计  
对于可以建模的透视需求，预计算是可以大幅度提升透视响应时间的手段。  
除了业务指标维度，常见的还有时间、空间维度的预计算。  
统计有一个相通性，都是基于分组进行，比如时间分组（小时，天，月，年等），空间分组（行政区，box，GEOHASH，GRID等）。  
### 预计算 - 固定时间  
例如业务要查询天、月、年的报表。那么我们可以按天进行预计算。  
使用GROUPING SETS语法进行多维透视。  
```  
insert into result  
select date(crt_time),col1,col2,col3,...,agg_func1(),agg_func2(),...   
from table  
  where crt_time between ? and ?  
  group by date(crt_time),GROUPING SETS(col1,col2,col3,...);  
```  
### 预计算 - 滑动时间窗口  
在《星际穿越》这部电影中，未来的人类构造了一个五维空间，让主人公可以在过去的时间中穿越，在过去的时间内任意的滑动。  
![pic](20170629_01_pic_001.jpg)    
实际上数据透视，也有类似的滑动需求，可能需要查询过去任意时间窗口的统计。  
例如，查询 2017-06-27 13:50:00 前后30分钟的统计。  
有两种方式实现滑动窗口：  
1、非预计算，即根据明细直接进行统计。  
2、若预计算需要支持滑动窗口，我们需要进行更小粒度的统计，例如窗口大小为1小时，那么细粒度至少要小于一小时，例如使用半小时的粒度，在查询滑动窗口时，将细粒度的统计结果进行二次统计得到滑动窗口的统计结果。  
pipelineDB的滑动窗口也是这么来实现的。  
http://docs.pipelinedb.com/sliding-windows.html  
### 预计算 - 固定行政区  
为了实现空间维度的预计算，我们需要将空间数据进行分组，这个分组也是根据业务对空间数据透视的需求来的。  
例子  
1、业务需要根据区级行政区进行统计，那么我们可以根据区级行政区进行预计算，（图片取自互联网）。  
![pic](20170629_01_pic_009.jpg)  
首先需要有翻译行政区ID的函数支持，根据经纬度，返回该经纬度属于哪个行政区的ID。  
一个比较土的方法，用一张表来表示行政区的多边形geometry，通过gist索引，可以快速的返回某个经纬度属于哪个多边形。  
```  
table (  
  ID int,        -- 行政区ID  
  bb geometry    -- 行政区的多边形  
)  
create or replace function get_id(pos geometry) returns int as $$  
  select id from table where bb && pos limit 1;  
$$ language sql strict immutable;  
```  
统计  
```  
insert into result  
select get_id(pos),col1,col2,col3,...,agg_func1(),agg_func2(),...   
from table  
  where crt_time between ? and ?  
  group by get_id(pos),GROUPING SETS(col1,col2,col3,...);  
```  
2、按GEOHASH PREFIX进行统计，例如去5位长度的hash，聚合。（通过PostGIS的ST_GeoHash()函数计算经纬度geohash value。）  
![pic](../201701/20170124_01_pic_007.png)  
```  
insert into result  
select ST_GeoHash(pos,5),col1,col2,col3,...,agg_func1(),agg_func2(),...   
from table  
  where crt_time between ? and ?  
  group by ST_GeoHash(pos,5),GROUPING SETS(col1,col2,col3,...);  
```  
GEOHASH精度如下  