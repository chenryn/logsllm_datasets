title:An algebraic approach to practical and scalable overlay network monitoring
author:Yan Chen and
David Bindel and
Han Hee Song and
Randy H. Katz
An Algebraic Approach to Practical and Scalable Overlay
Network Monitoring
Yan Chen
Department of Computer Science
Northwestern University
PI:EMAIL
David Bindel Hanhee Song Randy H. Katz
Division of Computer Science
University of California at Berkeley
(dbindel@cs, coolcoon@uclink, randy@cs).berkeley.edu
Overlay network monitoring enables distributed Internet
applications to detect and recover from path outages and pe-
riods of degraded performance within seconds. For an over-
lay network with n end hosts, existing systems either require
O(n2) measurements, and thus lack scalability, or can only
estimate the latency but not congestion or failures. Our
earlier extended abstract [1] brieﬂy proposes an algebraic
approach that selectively monitors k linearly independent
paths that can fully describe all the O(n2) paths. The loss
rates and latency of these k paths can be used to estimate
the loss rates and latency of all other paths. Our scheme
only assumes knowledge of the underlying IP topology, with
links dynamically varying between lossy and normal.
In this paper, we improve,
implement and extensively
evaluate such a monitoring system. We further make the fol-
lowing contributions: i) scalability analysis indicating that
for reasonably large n (e.g., 100), the growth of k is bounded
as O(n log n), ii) eﬃcient adaptation algorithms for topology
changes, such as the addition or removal of end hosts and
routing changes, iii) measurement load balancing schemes,
and iv) topology measurement error handling. Both simula-
tion and Internet experiments demonstrate we obtain highly
accurate path loss rate estimation while adapting to topol-
ogy changes within seconds and handling topology errors.
Categories and Subject Descriptors
C.2.3 [Network Operations]: Network monitoring
General Terms
Algorithms, Measurement
Keywords
Overlay, Network measurement and monitoring, Numerical
linear algebra, Scalability, Dynamics, Load balancing
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’04, Aug. 30–Sept. 3, 2004, Portland, Oregon, USA.
Copyright 2004 ACM 1-58113-862-8/04/0008 ...$5.00.
1.
INTRODUCTION
The rigidity of the Internet architecture makes it extremely
diﬃcult to deploy innovative disruptive technologies in the
core. This has led to extensive research into overlay and
peer-to-peer systems, such as overlay routing and location,
application-level multicast, and peer-to-peer ﬁle sharing. These
systems ﬂexibly choose their communication paths and tar-
gets, and thus can beneﬁt from estimation of end-to-end
network distances (e.g., latency and loss rate).
Accurate loss rate monitoring systems can detect path
outages and periods of degraded performance within sec-
onds. They facilitate management of distributed systems
such as virtual private networks (VPM) and content distri-
bution networks; and they are useful for building adaptive
overlay applications, such as the streaming media framework
demonstrated in [2].
Thus it is desirable to have a scalable overlay loss rate
monitoring system which is accurate and incrementally de-
ployable. However, existing network distance estimation
systems are insuﬃcient for this end. These existing systems
can be categorized as general metric systems [3] and latency-
only systems [4, 5, 6, 7]. Systems in the former category
can measure any metric, but require O(n2) measurements
where n is the number of end hosts, and thus lack scala-
bility. On the other hand, the latency estimation systems
are scalable, but cannot provide accurate congestion/failure
detection (see Sec. 2).
We formulate the problem as follows: consider an overlay
network of n end hosts; we deﬁne a path to be a routing
path between a pair of end hosts, and a link to be an IP
link between routers. A path is a concatenation of links.
There are O(n2) paths among the n end hosts, and we wish
to select a minimal subset of paths to monitor so that the
loss rates and latencies of all other paths can be inferred.
In an earlier extended abstract [1], we sketched the idea
of a tomography-based overlay monitoring system (TOM )
in which we selectively monitor a basis set of k paths. Any
end-to-end path can be written as a unique linear combina-
tion of paths in the basis set. Consequently, by monitoring
loss rates for the paths in the basis set, we infer loss rates
for all end-to-end paths. This can also be extended to other
additive metrics, such as latency. The end-to-end path loss
rates can be computed even when the paths contain uniden-
tiﬁable links for which loss rates cannot be computed.
In [1], we only brieﬂy introduce the basic formulation and
model. The following questions remain:
• How scalable is the system? In other words, how will k
grow as a function of n? It is our conjecture that for
reasonably large n (say 100), k = O(n log n) [1].
55• In an overlay network, end hosts frequently join/leave
the overlay and routing changes occur from time to time.
How can the system adapt to these eﬃciently?
• How should the system distribute the measurement load
• How can the system maintain accuracy when there are
• How does TOM perform under various topologies and
among end hosts to improve scalability?
topology measurement errors?
loss conditions, and in the real Internet?
balancing.
ment errors.
To address these issues, in this paper, we make the fol-
lowing contributions.
• We show that k does grow as O(n log n) through linear
regression tests on various synthetic and real topologies.
We also provide some explanation based on the Internet
topology and the AS hierarchy.
• We design incremental algorithms for path addition and
deletion which only cost O(k2) time, instead of the O(n2k2)
time cost to reinitialize the system.
• We propose randomized schemes for measurement load
• We design eﬀective schemes to handle topology measure-
• We evaluate TOM through extensive simulations, and
further validate our results through Internet experiments.
In both simulations and PlanetLab experiments, we es-
timate path loss rates with high accuracy using O(n log n)
measurements. For the PlanetLab experiments, the aver-
age absolute error of loss rate estimation is only 0.0027, and
the average error factor is 1.1, even though about 10% of
the paths have incomplete or nonexistent routing informa-
tion. The average setup (monitoring path selection) time is
0.75 second, and the online update of the loss rates for all
2550 paths takes only 0.16 second. In addition, we adapt
to topology changes within seconds without sacriﬁcing ac-
curacy. The measurement load balancing reduces the load
variation and the maximum vs. mean load ratio signiﬁ-
cantly, by up to a factor of 7.3.
The rest of the paper is organized as follows. We survey
related work in Sec. 2, describe our model and basic static
algorithms in Sec. 3, and evaluate scalability in Sec. 4. We
extend the algorithms to adapt to topology changes in Sec. 5,
and to handle overload and topology measurement errors in
Sec. 6. The methodology and results of our simulations are
described in Sec. 7, and those of our Internet experiments
are presented in Sec. 8. Finally, we conclude in Sec. 9.
2. RELATED WORK
There are many existing scalable end-to-end latency esti-
mation schemes, which can be broadly classiﬁed into clustering-
based [6, 7] and coordinate-based systems [4, 5]. Clustering-
based systems cluster end hosts based on their network prox-
imity or latency similarity under normal conditions, then
choose the centroid of each cluster as the monitor. But a
monitor and other members of the same cluster often take
diﬀerent routes to remote hosts. So the monitor cannot
detect congestion for its members. Similarly, the coordi-
nates assigned to each end host in the coordinate-based ap-
proaches cannot embed any congestion/failure information.
Network tomography has been well studied ([8] provides
a good survey). Most tomography systems assume limited
measurements are available (often in a multicast tree-like
structure), and try to infer link characteristics [9, 10] or
shared congestion [11] in the middle of the network. How-
ever, the problem is under-constrained: there exist uniden-
Overlay Network 
Operation Center
End hosts
Figure 1: Architecture of a TOM system.
tiﬁable links [9] with properties that cannot be uniquely de-
termined. In contrast, we are not concerned about the char-
acteristics of individual
links, and we do not restrict the
paths we measure.
Shavitt, et al. also use algebraic tools to compute dis-
tances that are not explicitly measured [12]. Given certain
“Tracer” stations deployed and some direct measurements
among the Tracers, they search for path or path segments
whose loss rates can be inferred from these measurements.
Thus their focus is not on Tracer/path selection.
Recently, Ozmutlu, et al.
selected a minimal subset of
paths to cover all links for monitoring, assuming link-by-link
latency is available via end-to-end measurement [13]. But
the link-by-link latency obtained from traceroute is often in-
accurate. And their approach is not applicable for loss rate
because it is diﬃcult to estimate link-by-link loss rates from
end-to-end measurement. A similar approach was taken for
selecting paths to measure overlay network [14]. The mini-
mal set cover selected can only gives bounds for metrics like
latency, and there is no guarantee as to how far the bounds
are from the real values.
Furthermore, none of the existing work examines topology
change, topology measurement errors, or measurement load
balancing problems.
3. MODEL AND BASIC ALGORITHMS
3.1 Algebraic Model
Suppose there are n end hosts that belong to a single or
confederated overlay network(s). They cooperate to share
an overlay monitoring service, and are instrumented by a
central authority (e.g., an overlay network operation cen-
ter (ONOC)) to measure the routing topology and path loss
rates as needed 1. For simplicity, we usually assume sym-
metric routing and undirected links in this paper. However,
our techniques work without change for asymmetric routing,
as evidenced in the PlanetLab experiments. Fig. 1 shows a
sample overlay network with four links and four end hosts;
six possible paths connect the end hosts. The end hosts
measure the topology and report to the ONOC, which se-
lects four paths and instruments two of the end hosts to
measure the loss rates of those paths. The end hosts peri-
odically report the measured loss rates to the ONOC. Then
the ONOC infers the loss rates of every link, and conse-
quently the loss rates of the other two paths. Applications
can query the ONOC for the loss rate of any path, or they
can set up triggers to receive alerts when the loss rates of
paths of interest exceed a certain threshold [2].
We now introduce an algebraic model which applies to
any network topology.
Suppose an overlay network spans
s IP links. We represent a path by a column vector v ∈
1As part of the future work, we will investigate techniques
to distribute the work of the central authority.
Symbols
M
N
n
r = O(n2)
s
t
G ∈ {0, 1}r×s
¯G ∈ {0, 1}k×s
k ≤ s
li
pi
xi
bi
v
p
N (G)
R(GT )
Meanings
total number of nodes
number of end hosts
number of end hosts on the overlay
number of end-to-end paths
# of IP links that the overlay spans on
number of identiﬁable links
original path matrix
reduced path matrix
rank of G
loss rate on ith link
loss rate on ith measurement path
log(1 − li)
log(1 − pi)
vector in {0, 1}s (represents path)
loss rate along a path
null space of G
row(path) space of G (== range(GT ))
Table 1: Table of notations
{0, 1}s, where the jth entry vj is one if link j is part of the
path, and zero otherwise. Suppose link j drops packets with
probability lj ; then the loss rate p of a path represented by
v is given by
1 − p =
s
Yj=1
(1 − lj)
vj
(1)
Equation (1) assumes that packet loss is independent among
links. Caceres et al. argue that the diversity of traﬃc and
links makes large and long-lasting spatial link loss depen-
dence unlikely in a real network such as the Internet [15].
Furthermore, the introduction of Random Early Detection
(RED) [16] policies in routers will help break such depen-
dence. In addition to [15], formula (1) has also been proven
useful in many other link/path loss inference works [10, 9,
17, 14]. Our Internet experiments also show that the link
loss dependence has little eﬀect on the accuracy of (1).
We take logarithms on both sides of (1). Then by deﬁning
s with elements xj = log (1 − lj ), and
a column vector x ∈ R
writing vT for the transpose of the column vector v, we can
rewrite (1) as follows:
log (1 − p) =
vj log (1 − lj ) =
s
s
Xj=1
vj xj = vT x
(2)
Xj=1
There are r = O(n2) paths in the overlay network, and
thus there are r linear equations of the form (2). Putting
them together, we form a rectangular matrix G ∈ {0, 1}r×s.
Each row of G represents a path in the network: Gij = 1
when path i contains link j, and Gij = 0 otherwise. Let pi
be the end-to-end loss rate of the ith path, and let b ∈ R
r
be a column vector with elements bi = log (1 − pi). Then
we write the r equations in form (2) as
Gx = b
(3)
Normally, the number of paths r is much larger than the
number of links s (see Fig. 2(a)). This suggests that we
could select s paths to monitor, use those measurements to
compute the link loss rate variables x, and infer the loss
rates of the other paths from (3).
However, in general, G is rank deﬁcient: i.e., k = rank(G)
and k < s. If G is rank deﬁcient, we will be unable to deter-
s
r
…
s
…k
=
s
k
s
=
r
(a) Gx = b
(b) ¯GxG = ¯b
Figure 2: Matrix size representations.
mine the loss rate of some links from (3). These links are also
called unidentiﬁable in network tomography literature [9].
b1
D
2
B
A
1
b2
3
=
G
b3
C
G
x
1
x
x
3
011
100
111
b
1
b
b
=
3
2
2
x2
(1,1,0)
(1,-1,0)
null space
(unmeasured)
x3
row(path) space
(measured)
x1
Figure 3: Sample overlay network.
Fig. 3 illustrates how rank deﬁciency can occur. There are
three end hosts (A, B and C) on the overlay, three links (1,
2 and 3) and three paths between the end hosts. We cannot
uniquely solve x1 and x2 because links 1 and 2 always appear
together. We know their sum, but not their diﬀerence.
Fig. 3 illustrates the geometry of the linear system, with
each variable xi as a dimension. The vectors {α1 −1 0
T}
comprise N (G), the null space of G. No information about
the loss rates for these vectors is given by (3). Meanwhile,
there is an orthogonal row(path) space of G, R(GT ), which
T}.
for this example is a plane {α1 1 0
Unlike the null space, the loss rate of any vector on the row
space can be uniquely determined by (3).
To separate the identiﬁable and unidentiﬁable components
of x, we decompose x into x = xG + xN , where xG ∈ R(GT )
is its projection on the row space and and xN ∈ N (G) is its
projection on the null space (i.e., GxN = 0). The decom-
position of [x1 x2 x3]T for the sample overlay is shown
below.
+ β0 0 1
T
xG =
(x1 + x2)
2
xN =
0
0
1
1
1
0
3
5
2
4
(x1 − x2)
3
+ x32
5
4
3
2
1−1
4
5
2
0
= 2
4
b1/2
b1/2
b2
3
5
(4)
(5)
Thus the vector xG can be uniquely identiﬁed, and con-
tains all the information we can know from (3) and the path
measurements. The intuition of our scheme is illustrated
through virtual links in [1].
Because xG lies in the k-dimensional space R(GT ), only k
independent equations of the r equations in (3) are needed to
uniquely identify xG. We measure these k paths to compute
xG. Since b = Gx = GxG + GxN = GxG, we can compute
all elements of b from xG, and thus obtain the loss rate of
all other paths. Next, we present more detailed algorithms.
3.2 Basic Static Algorithms
The basic algorithms involve two steps. First, we select a
basis set of k paths to monitor. Such selection only needs to
be done once at setup. Then, based on continuous monitor-
ing of the selected paths, we calculate and update the loss
rates of all other paths.
3.2.1 Measurement Paths Selection
To select k linearly independent paths from G, we use
standard rank-revealing decomposition techniques [18], and
obtain a reduced system:
¯GxG = ¯b
(6)
where ¯G ∈ R
k consist of k rows of G and b,
respectively. The equation is illustrated in Fig. 2(b) (com-
pared with Gx = b).
k×s and ¯b ∈ R
As shown below, our algorithm is a variant of the QR de-
composition with column pivoting [18, p.223]. It incremen-
tally builds a decomposition ¯GT = QR, where Q ∈ R
s×k is
a matrix with orthonormal columns and R ∈ R
k×k is upper
triangular.
1 for every row(path) v in G do
2
3
procedure SelectPath(G)
ˆR12 = R−T ¯GvT = QT vT
ˆR22 = (cid:9)v(cid:9)2 − (cid:9) ˆR12(cid:9)2
if ˆR22 (cid:10)= 0 then
Select v as a measurement path
4
5
6
Update R = (cid:20)R ˆR12
ˆR22(cid:21) and ¯G = (cid:20) ¯G
v(cid:21)
0
end
end
Algorithm 1: Path (row) selection algorithm
In general, the G matrix is very sparse; that is, there are
only a few nonzeros per row. We leverage this property
for speedup. We further use optimized routines from the
LAPACK library [19] to implement Algorithm 1 so that it
inspects several rows at a time. The complexity of Algo-
rithm 1 is O(rk2), and the constant in the bound is modest.
The memory cost is roughly k2/2 single-precision ﬂoating
point numbers for storing the R factor. Notice that the path
selection only needs to be executed once for initial setup.
3.2.2 Path Loss Rate Calculations
To compute the path loss rates, we must ﬁnd a solution to
the underdetermined linear system ¯GxG = ¯b. The vector ¯b
comes from measurements of the paths. Zhang et al. report
that path loss rates remain operationally stable in the time
scale of an hour [20], so these measurements need not be
taken simultaneously.
Given measured values for ¯b, we compute a solution xG
using the QR decomposition we constructed during measure-
ment path selection [18, 21]. We choose the unique solution
xG with minimum possible norm by imposing the constraint
xG = ¯GT y where y = R−1R−T ¯b. Once we have xG, we can
compute b = GxG, and from there infer the loss rates of
the unmeasured paths. The complexity for this step is only
O(k2). Thus we can update loss rate estimates online, as
veriﬁed in Sec. 7.4 and 8.2.
4. SCALABILITY ANALYSIS
An overlay monitoring system is scalable only when the
size of the basis set, k, grows relatively slowly as a function
of n. Given that the Internet has moderate hierarchical
structure [22, 23], we proved that the number of end hosts is
no less than half of the total number of nodes in the Internet.
Furthermore, we proved that when all the end hosts are on
the overlay network, k = O(n) [1].
But what about if only a small fraction of the end hosts
are on the overlay? Because G is an r by s matrix, k is
bounded by the number of links s. If the Internet topology