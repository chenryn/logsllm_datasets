8.2 Robustness Analysis
In this section, we perform analysis to understand the robustness of
the generated adversarial spoofed 3D point cloud T ′ to variations in
3D point cloud X and spoofed 3D point cloud T ∈ ST. Such analysis
is meaningful for generating adversarial spoofed 3D point cloud
that has high attack success rate in the real world. To launch the
Targeted position
2-8 meters
# Spoofed points
60
90%
20
40
87% 82%
Table 4: Robustness analysis results of generated adversar-
ial spoofed 3D point cloud to variation in spoofed 3D point
cloud T ∈ ST. The robustness is measured by average attack
success rates.
1, ...,T ′
traces T1, ...,T5 using our LiDAR spoofing attack experiment setup.
Next, we use the same transformation that generates T ′ from T to
generate T ′
5, and then combine each of them with X to launch
the attack. Table 4 shows the average success rates with different
attack capabilities. As shown, for all three attack capabilities we are
able to achieve over 82% success rates. With 60 spoofed points, the
success rate is as high as 90%. This suggests that launching such
attack does not require the LiDAR spoofing attack to be precise all
the time in order to achieve high success rates.
9 DRIVING DECISION CASE STUDY
To understand the impact of our attacks at the driving decision level,
in this section we construct several attack scenarios and evaluate
them on Baidu Apollo using simulation as case studies.
Experiment setup. We perform the case study using the simu-
lation feature provided by Baidu Apollo, called Sim-control, which
is designed to allow users to observe the AV system behavior at the
driving decision level by replaying collected real-world sensor data
traces. Sim-control does not consist of a physics engine to simu-
late the control of the vehicle. Instead, the AV behaves exactly the
same as what it plans. Although it cannot directly reflect the attack
consequences in the physical world, it can serve for our purpose of
understanding the impact of our attacks on AV driving decisions.
For each attack scenario in the case study, we simulate it in
Sim-control using synthesized continuous frames of successful ad-
versarial 3D point cloud identified in § 8 as input. The experiments
are performed on Baidu Apollo 3.0.
Case study results. We construct and evaluate two attack sce-
narios in this case study1:
(1) Emergency brake attack. In this attack, we generate adver-
sarial 3D point cloud that spoofs a front-near obstacle to a moving
victim AV. We find that the AV makes a stop decision upon this
attack. As illustrated in Fig. 12, the stop decision triggered by a
spoofed front-near obstacle causes the victim AV to decrease its
speed from 43 km/h to 0 km/h within 1 second. This stop decision
will lead to a hard brake [1], which may hurt the passengers or
result in rear-end collisions. Noticed that, Apollo does implement
driving decisions for overtaking. However, for overtaking, a mini-
mum distance is required based on the relative speed of the obstacle.
Therefore, with our near spoofed obstacle, the victim AV makes
stop decisions instead of overtaking decisions.
(2) AV freezing attack. In this attack, we generate an adversarial
3D point cloud that spoofs an obstacle in front of an AV victim
when it is waiting for the red traffic light. We simulate this scenario
with the data trace at an intersection with traffic lights. As shown
in Fig. 13, since the victim AV is static, the attacker can constantly
1Video demos can be found at http://tinyurl.com/advlidar
Figure 11: The robustness of the generated adversarial
spoofed 3D point cloud to variations in 3D point cloud X.
We quantify the variation in 3D point cloud X as the frame
indexes difference between the evaluated 3D point cloud
and the 3D point cloud used for generating the adversarial
spoofed 3D point cloud.
attack in the real world, there are two main variations that affect
the results: variation in spoofed points and variation in positions of
the victim AV. 1) The imprecision in the attack devices contributes
to the variation of the spoofed points. The attacker is able to stably
spoof 60 points at a global position as we state in §2.2. However, it
is difficult to spoof points with precise positions. It is important to
understand whether such imprecision affects the attack success rate.
2) The position of the victim AV is not controlled by the attacker
and might vary from where the attacker collected the 3D point
cloud. It is important to understand whether such difference affects
the attack success rate.
Robustness to variations in point cloud. To measure the ro-
bustness to variations in the 3D point cloud, we first select all the 3D
point cloud frames that can generate successful adversarial spoofed
3D point cloud. For each of them, we apply its generated adversar-
ial spoofed 3D point cloud to 15 consecutive frames (around 1.5 s)
after it and calculate the success rates. Fig. 11 shows the analysis
results. In this figure, the x-axis is the index for the 15 consecutive
frames, and thus the larger the frame index is, the larger the varia-
tion is to the original 3D point cloud that generates the adversarial
spoofed 3D point cloud. As shown, the robustness for attacks with
more spoofed points is generally higher than that for attacks with
fewer spoofed points, which shows that higher attack capability
can increase the robustness. Particularly, with 60 spoofed points,
the success rates are on average above 75% during the 15 subse-
quent frames, which demonstrates a high degree of robustness. This
suggests that launching such attack does not necessarily require
the victim AV to appear at the exact position that generates the
adversarial example in order to have high success rates.
Robustness to variations in spoofed 3D point cloud. To
evaluate the robustness to variations in the spoofed 3D point cloud,
for a given spoofed 3D point cloud T ∈ ST, we first generate the
corresponding adversarial spoofed 3D point cloud T ′ with a 3D
point cloud X. Next, we generate 5 more spoofed 3D point cloud
Figure 12: Demonstration of the emergency brake attack. Due to the spoofed ob-
stacle, the victim AV makes a sudden stop decision to drop its speed from 43 km/h
to 0 km/h within a second, which may cause injuries of passengers or rear-end
collisions.
Figure 13: Demonstration of the AV freez-
ing attack. The traffic light is turned green
but the victim AV is not moving due to the
spoofed front-near obstacles.
attack and prevent it from moving even after the traffic signal turns
green, which may be exploited to cause traffic jams. Noticed that,
Apollo does implement driving decisions for deviating static obsta-
cles. However, for deviation or side passing, it requires a minimum
distance (15 meters by default). Therefore, with our near spoofed
obstacle, the victim AV makes stop decisions instead of side passing
decisions.
10 DISCUSSION
In this section, we discuss the limitations and generality of this
study. We then discuss potential defense directions.
10.1 Limitations and Future Work
Limitations in the sensor attack. One major limitation is that
our current results cannot directly demonstrate attack performance
and practicality in the real world. For example, performing our
attack on a real AV on the road requires dynamically aiming an
attack device at the LiDAR on a victim car with high precision,
which is difficult to prove the feasibility without road tests in the
physical world. In this work, our goal is to provide new understand-
ings of this research problem. Future research directions include
conducting real world testing. To demonstrate the attack in the real
world, we plan to first conduct the sensor attack with LiDAR on
top of a real vehicle in outdoor settings. In this setting, the sensor
attack could be enhanced by: 1) enlarging the laser spoofing area
to solve the aiming problem; 2) adjusting the delay time so that
the attacker could spoof points at different angles without moving
the attack devices. Then we could apply our proposed methodol-
ogy to conduct drive-by experiments in different attack scenarios
mentioned in §9.
Limitations in adversarial example generation. First, we
construct adversarial sensor data by using a subset of spoofing
attack capability A. Therefore, our analysis may not fully reveal the
full potential of sensor attacks. Second, though we have performed
the driving decision case study, we did not perform a comprehensive
analysis on modules beyond the perception module. That means
that the designed objective function can be further improved to
more directly target specific abnormal AV driving decisions.
10.2 Generality on LiDAR-based AV Perception
Generality of the methodology. Attacking any LiDAR-based AV
perception system with an adversarial sensor attack can be formu-
lated as three components: (1) formulating the spoofed 3D point
cloud capability A, (2) generating adversarial examples, and (3)
evaluating at the driving decision level. Even though our construc-
tion of these components might be specific to Baidu Apollo, our
analysis methodology can be generalized to other LiDAR-based AV
perception systems.
Generality of the results. The formulation of 3D point cloud
spoofing capability A can be generalized as it is independent from
AV systems. The success of the attack may be extended to other
LiDAR-based AV perception system due to the nature of the LiDAR
sensor attack. The LiDAR spoofing attack introduces a spoofed
3D point cloud, which was not foreseen in the training process
of machine learning models used in the AV perception system.
Therefore, other models are likely to be also vulnerable to such
spoofing patterns.
10.3 Defense Discussion
This section discusses defense directions at AV system, sensor, and
machine learning model levels.
10.3.1 AV System-Level defenses. In our proposed attack, the at-
tacker only needs to inject at most 60 points to spoof an obstacle,
but the 3D point cloud of a detected real vehicle can have as many
as a thousand points (can be illustrated in Fig. 6). We look into the
point cloud of a detected spoofed obstacle and find that the 3D point
cloud consists of points reflected from the ground, in addition to the
points spoofed by the attacker. For example, one of the successful
adversarial spoofed 3D point cloud we generated with 20 spoofed
points is detected as an obstacle containing 283 points.
Points from ground reflection are clustered into obstacles due to
the information loss introduced in the pre-processing phase. More
specifically, mapping a 3D point cloud into a 2D matrix results in
height information loss. This vulnerability contributes to the suc-
cess of the proposed attack. To mitigate the impacts of this problem,
we propose two defenses at the AV system level: (1) filtering out
the ground reflection in the pre-processing phase, and (2) either
avoiding transforming 3D point cloud into input feature matrix or
adding more features to reduce the information loss.
Sensor-Level Defenses. Several defenses could be adopted
10.3.2
against spoofing attacks on LiDAR sensors:
Detection techniques. Sensor fusion, which intelligently com-
bines data from several sensors to detect anomalies and improve
performance, could be adopted against LiDAR spoofing attacks.
AV systems are often equipped with sensors beyond LiDAR. Cam-
era, radars, ultrasonic sensors provide additional information and
redundancy to detect and handle an attack on LiDAR.
Different sensor fusion algorithms have been proposed focusing
on the security and safety aspects [56] [33]. However, the sensor
fusion defense requires the majority of sensors to be functioning
correctly. While not a perfect defense, sensor fusion approaches
can significantly increase the effort of an attacker.
Mitigation techniques. Another class of defenses aims to re-
duce the influence of the attack by modifying the internal sensing
structure of the LiDAR. Different solutions include reducing the re-
ceiving angle and filtering unwanted light spectra to make LiDARs
less susceptible to attacks [42, 44]. However, these techniques also
reduce the capacity of the LiDAR to measure the reflected laser
pulses, which limits the range and the sensitivity of the device.
Randomization techniques. Another defense is adding ran-
domness to how the LiDAR fires laser pulses. The attacker cannot
know when to and what laser pulses to fire if the LiDAR fires laser
pulses with an unpredictable pattern. A solution could be firing a
random grouping of laser pulses each cycle. An attacker would not
know which reflections the LiDAR would be expecting. Another
alternative would be randomizing the laser pulses waveform. With
sensitive equipment, it would be possible to only accept reflection
waveforms that match randomized patterns uniquely produced
by the LiDAR laser. Another solution, proposed by Shoukry et al.
[46], consists of randomly turning off the transmitter to verify with
the receiver if there are any unexpected incoming signals. Adding
randomness makes it difficult for an attacker to influence the mea-
surements, but this approach also adds significant complexity to
the overall system and trades off with performance.
10.3.3 Machine Learning Model-Level Defense. Various detection
and defense methods have also been explored [16, 19, 38, 39] against
adversarial examples in image classification. Adversarial train-
ing [31] and its variations [39, 47] are more successful to improve
the robustness of the model. Motivated by the adversarial examples
generated by our algorithm, we can combine them with the original
training data to conduct adversarial retraining and thus improve
the model robustness.
11 RELATED WORK
Vehicle systems security. Numerous previous works explore se-
curity problems in vehicle systems and have uncovered vulnera-
bilities in in-vehicle networks of modern automobiles [22, 25, 36],
infotainment systems [40], and emerging connected vehicle-based
systems [23, 30, 48]. In comparison, our work focuses on vehicle
systems with the emerging autonomous driving technology and
specifically targets the security of LiDAR-based AV perception,
which is an attack surface not presented in traditional vehicle sys-
tems designed for human drivers.
Vehicle-related sensor attacks. The sensors commonly used
in traditional vehicles have been shown to be vulnerable to attacks.
Rouf et al. showed that tire pressure sensors are vulnerable to wire-
less jamming and spoofing attacks [43]. Shoukry et al. attacked
the anti-lock braking system of a vehicle by spoofing the magnetic
wheel speed sensor [45]. As AVs become popular, so have attacks
against their perception sensors. Yan et al. used spoofing and jam-
ming attacks to attack the ultrasonic sensors, radar, and camera on
a Tesla Model S [55]. There have also been two works exploring the
vulnerability of LiDAR to spoofing and jamming attacks [42, 44]. In
this work, we build on these prior work to show that LiDAR spoof-
ing attacks can be used to attack the machine learning models used
for LiDAR-based AV perception and affect the driving decision.
Adversarial example generation. Adversarial examples have
been heavily explored in the image domain [21, 31, 41, 52]. Xie
et al. [53] generated adversarial examples for segmentation and
object detection while Cisse et al. [26] for segmentation and human
pose estimation. Researchers also apply adversarial examples to
the physical world to fool machine learning models [17, 27, 28].
Compared to these previous work exploring adversarial examples
in the image domain, this work explores adversarial examples for
LiDAR-based perception. An ongoing work [49] studies the gener-
ation of 3D adversarial point clouds. However, such attack focuses
on the digital domain and can not be directly applied to the context
of AV systems. In comparison, our method is motivated to generate