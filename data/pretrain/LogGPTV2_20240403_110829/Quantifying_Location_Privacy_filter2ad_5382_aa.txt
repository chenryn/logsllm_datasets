title:Quantifying Location Privacy
author:Reza Shokri and
George Theodorakopoulos and
Jean-Yves Le Boudec and
Jean-Pierre Hubaux
2011 IEEE Symposium on Security and Privacy
Quantifying Location Privacy
Reza Shokri, George Theodorakopoulos, Jean-Yves Le Boudec, and Jean-Pierre Hubaux
LCA, EPFL, Lausanne, Switzerland
ﬁrstname.lastname@epﬂ.ch
Abstract—It
that
is a well-known fact
the progress of
personal communication devices leads to serious concerns
about privacy in general, and location privacy in particular.
As a response to these issues, a number of Location-Privacy
Protection Mechanisms (LPPMs) have been proposed during
the last decade. However, their assessment and comparison
remains problematic because of the absence of a systematic
method to quantify them. In particular, the assumptions about
the attacker’s model tend to be incomplete, with the risk of a
possibly wrong estimation of the users’ location privacy.
In this paper, we address these issues by providing a
formal framework for the analysis of LPPMs; it captures,
in particular, the prior information that might be available
to the attacker, and various attacks that he can perform.
The privacy of users and the success of the adversary in his
location-inference attacks are two sides of the same coin. We
revise location privacy by giving a simple, yet comprehensive,
model to formulate all types of location-information disclosure
attacks. Thus, by formalizing the adversary’s performance,
we propose and justify the right metric to quantify location
privacy. We clarify the difference between three aspects of the
adversary’s inference attacks, namely their accuracy, certainty,
and correctness. We show that correctness determines the
privacy of users. In other words, the expected estimation error
of the adversary is the metric of users’ location privacy. We
rely on well-established statistical methods to formalize and
implement the attacks in a tool: the Location-Privacy Meter that
measures the location privacy of mobile users, given various
LPPMs. In addition to evaluating some example LPPMs, by
using our tool, we assess the appropriateness of some popular
metrics for location privacy: entropy and k-anonymity. The
results show a lack of satisfactory correlation between these
two metrics and the success of the adversary in inferring the
users’ actual locations.
Keywords-Location Privacy; Evaluation Framework; Loca-
tion Traces; Quantifying Metric; Location-Privacy Meter
I. INTRODUCTION
Most people are now equipped with smart phones with
many sophisticated sensors and actuators closely related to
their activities. Each of these devices is usually equipped
with high-precision localization capabilities, based for ex-
ample on a GPS receiver or on triangulation with nearby
base stations or access points. In addition, the environment
is more and more populated by sensors and smart devices,
with which smart phones interact.
The usage of these personal communication devices,
although providing convenience to their owners, leaves an
almost indelible digital trace of their whereabouts. A trace
is not only a set of positions on a map. The contextual
1081-6011/11 $26.00 © 2011 IEEE
DOI 10.1109/SP.2011.18
247
information attached to a trace tells much about the in-
dividuals’ habits, interests, activities, and relationships. It
can also reveal their personal or corporate secrets. It can
expose the users to unwanted advertisement and location-
based spams/scams, cause social reputation or economic
damage, and make them victims of blackmail or even physi-
cal violence. Additionally, information disclosure breaks the
balance of power between the informed entity and the entity
about which this information is disclosed.
In the meantime,
the tools required to analyze such
traces have made tremendous progress: sophisticated data
mining algorithms can leverage on fast growing storage and
processing power, facilitating, for example, the analysis of
multiple databases in parallel. This means that the negative
side-effects of insufﬁcient location privacy are becoming
more and more threatening.
Users should have the right to control the amount of
information (about themselves) that is disclosed and shared
with others. This can be achieved in several ways. Users
can share a minimum amount of information, or share it only
with few trusted entities. Privacy policies can be put in place
to force organizations to protect their users’ privacy. Finally,
systems can be designed in a privacy-conscious manner, so
they do not leak information to untrusted entities.
This paper refers to the last ambition. However, our goal
here is not to design yet another location privacy protection
mechanism (LPPM), but rather to try to make progress on
the quantiﬁcation of the performance of an LPPM. This is
an important topic, because (i) human beings are notoriously
bad estimators of risks (including privacy risks), (ii) it is the
only way to make meaningful comparisons between different
LPPMs and (iii) the research literature is not yet mature
enough on the topic.
Let us develop this last reason. In speciﬁc areas, sev-
eral contributions have been made to quantify privacy,
be it for databases [8], for anonymity protocols [3], for
anonymization networks [24], or for RFID privacy [25].
Yet, in the ﬁeld of location privacy, notwithstanding many
contributions from different disciplines (such as databases,
mobile networks, and ubiquitous computing) for protecting
location privacy, the lack of a uniﬁed and generic formal
framework for specifying protection mechanisms and also
for evaluating location privacy is evident. This has led to the
divergence of (nevertheless interesting) contributions and,
hence, has caused confusion about which mechanisms are
more effective. The adversary model is often not appro-
priately addressed and formalized, and a good model for
the knowledge of the adversary and his possible inference
attacks is missing. This can lead to a wrong estimation of
the location privacy of mobile users. There is also often con-
fusion between the different dimensions of the adversary’s
performance in his attacks, notably the accuracy, certainty
and correctness of his estimation of the users’ traces.
In this paper, leveraging on previous contributions in the
ﬁeld of (location) privacy, we propose a generic theoretical
framework for modeling and evaluating location privacy. We
make the following contributions.
• We provide a generic model that formalizes the ad-
versary’s attacks against private location-information
of mobile users. In particular, we rigorously deﬁne
tracking and localization attacks on anonymous traces
as statistical inference problems.
• We rely on well-established statistical methods to eval-
uate the performance of such inference attacks. We for-
malize the adversary’s success and we clarify, explain
and justify the right metric to quantify location privacy:
The adversary’s expected estimation error.
• We provide a tool: the Location-Privacy Meter is devel-
oped based on our formal framework and is designed
for evaluating the effectiveness of various location-
privacy preserving mechanisms.
• We show the inappropriateness of some existing met-
rics, notably entropy and k-anonymity, for quantifying
location privacy.
The paper is organized as follows. In Section II, we
provide a detailed description of the framework we propose
for the quantiﬁcation of LPPMs and show how location-
privacy threats can be deﬁned and evaluated correctly. In
Section III, we introduce an instantiation of the framework
into an operational tool: Location-Privacy Meter. In Sec-
tion IV, we show the usage of the tool on evaluating LPPMs
and assessing existing location-privacy metrics. We discuss
the related work in Section V and conclude in Section VI.
II. THE FRAMEWORK
In this section, we present our framework for location
privacy. This allows us to precisely deﬁne location pri-
vacy and specify its relevant components and entities in
various settings and also to evaluate the effectiveness of
various location-privacy preserving mechanisms with respect
to different attacks. We deﬁne a location-privacy framework
(system) as a tuple of the following inseparable elements:
hU , A, LPPM, O, ADV, METRICi, where U is the set of
mobile users, A represents the set of possible actual traces
for the users, and LPPM stands for the location-privacy
preserving mechanism that acts on the actual traces a (a
member of A) and produces the observed traces o (a
member of O, which is the set of observable traces to
an adversary ADV). The adversary ADV is an entity who
U
R
T
A
O
U ′
R′
N
M
T
N ′
M ′
f
g
au
ou
oi
Au
Ou
Oσ(u)
P u
φ(.)
X
Set of mobile users
Set of regions that partition the whole area
Time period under consideration
Set of all possible traces
Set of all observable traces
Set of user pseudonyms
Set of location pseudonyms
Number of users
Number of regions
Number of considered time instants
Number of user pseudonyms
Number of location pseudonyms
Obfuscation function
Anonymization function
Actual trace of user u
Obfuscated trace of user u
Observed trace of a user with pseudonym i
Set of all possible (actual) traces of user u
Set of all possible obfuscated traces of user u
Set of all observable traces of user u
Proﬁle of user u
Attacker’s objective
Set of values that φ(.) can take
Table I
NOTATIONS
implements some inference (reconstruction) attacks to infer
some information about a having observed o and by relying
on his knowledge of the LPPM and of the users’ mobility
model. The performance of the adversary and his success in
recovering the desired information about a is captured by an
evaluation metric METRIC. The success of the adversary
and the location-privacy of users are two sides of the same
coin, which are coupled together using METRIC.
In the following subsections, we present and specify all
the entities and components of our framework and illustrate
their inter-relationship. The tool that we have developed
according to the framework, Location-Privacy Meter, and
the theoretical details of the implemented methods will be
explained in Section III.
The summary of the notations is presented in Table I.
The framework is shown in Figure 1. Throughout the paper,
we use bold capital
letters to denote random variables,
lower case letters to denote realizations of random variables,
and script letters to denote sets within which the random
variables take values. For example, a random variable X
takes values x in X . At times, the members of a set are also
sets, but the distinction will be clear from the context.
A. Mobile Users
We consider U = {u1, u2, ..., uN } a set of N mobile users
who move within an area that is partitioned into M distinct
regions (locations) R = {r1, r2, ..., rM }. See Figure 2 for
an example of partitioning an area into regions. Time is
discrete, and the set of time instants when the users may
be observed is T = {1, ..., T }. The level of space and time
granularity depends on the precision that we want, on the
size of the area, and on the total length of the observation
248
uNu1Training(cid:3)Traces(cid:3)(vectors(cid:3)of (cid:3)noisy/missing(cid:3)events)
Actual(cid:3)Traces(cid:3)(vectors(cid:3)of (cid:3)actual(cid:3)events)
Users
u1u2uN…
Timeline:
2
4
3
1
Transition(cid:3)CntMatrices
rjCij
ri
LPPM
T
Users’Profiles
MC(cid:3)Transition(cid:3)Matrices
uNu1
rjPij
ri
Observed(cid:3)Traces(cid:3)(vectors(cid:3)of (cid:3)observed(cid:3)events)
3
2
4
KC
Nyms
1…
2N
Timeline:
1
T
n(cid:3)A
Figure 1. Elements of the proposed location-privacy framework. The users produce actual traces, which are then anonymized and obfuscated by the LPPM
to produce anonymous observed traces. The attacker uses a set of training traces to create, via the knowledge construction (KC) mechanism, a mobility
proﬁle for each user in the form of a Markov Chain transition probability matrix. Having the user mobility proﬁles and the observed traces, the adversary
tries to reconstruct (infer) the actual traces. The only element of the framework not shown here is the metric that evaluates the success of the adversary’s
reconstruction attack by comparing the results of the attack with the users’ actual traces.
period. For example, regions can be of a city/block size, and
two successive time instants can be a day/hour apart.
The spatiotemporal position of users is modeled through
events and traces. An event is deﬁned as a triplet hu, r, ti,
where u ∈ U , r ∈ R, t ∈ T . A trace of user u is a T -size
vector of events au = (au(1), au(2), ..., au(T )). The set of
all traces that may belong to user u is denoted by Au. Notice
that, of all the traces in Au, exactly one is the true trace that
user u created in the time period of interest (t = 1...T ); this
one is called the actual trace of user u, and its events are
called the actual events of user u. The set of all possible
traces of all users is denoted by A = Au1 ×Au2 ×. . .×AuN ;
the member of A that was actually created by the N users
is denoted by a, so it is also the set of actual traces.
B. Location-Privacy Preserving Mechanisms
Mobile users share their location with possibly untrusted
entities in various location-based services, or may unwill-
ingly expose their location to curious eavesdropping entities
through the wireless channel. In addition to these types of
sharing, their location traces can be made public for research
purposes. In all these scenarios, an adversarial entity can
track the users over the observation period, unless their
actual
traces are properly modiﬁed and distorted before
being exposed to others, i.e., before becoming observable.
The mechanism that performs this modiﬁcation in order
to protect the users’ location-privacy is called a Location-
249
Privacy Preserving Mechanism (LPPM).
LPPMs can be implemented in different manners and ar-
chitectures: online vs. ofﬂine, and centralized vs. distributed.
In the ofﬂine manner, all the traces are available to the
LPPM, for example in a database, whereas in the online
manner, the modiﬁcation is performed on-the-ﬂy while users
visit new regions as time progresses. The modiﬁcation can
be performed in the centralized architecture by a trusted
third party (mostly known as the central anonymity server
or privacy proxy) as opposed to being done by the users or
on their mobile devices in a distributed architecture, where
modiﬁcations are (mostly) done independently from each
other. Next, we abstract away these details and provide a
generic model for LPPMs.
1, ..., r′
A location-privacy preserving mechanism LPPM receives
a set of N actual traces, one for each user, and modiﬁes them
in two steps. In the obfuscation process, the location of each
event is obfuscated, i.e., replaced by a location pseudonym
in the set R′ = {r′
M ′ }. In the anonymization process,
the traces are anonymized, i.e., the user part of each trace is
replaced by a user pseudonym in the set U ′ = {u′
N ′}.
Notice that each region may be obfuscated to a different
location pseudonym each time it is encountered, whereas
each user is always obfuscated to the same user pseudonym
(as in this paper we focus on evaluating users’ location-
privacy from their location traces). Also, note that
the
information used by an LPPM to obfuscate an event varies
1, ..., u′
…
R
e
c
o
n
s
t
r
u
c
t
i
o
t
t
a
c
k
1
6
12
18
24
2
7
13
19