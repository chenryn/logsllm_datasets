When Jane steps away from the terminal and another user
starts using the terminal, the two sequences of interactions
will not match because the interaction sequence that Jane’s
bracelet generates when she is away from her terminal will
be different from the interaction sequence of the other user
on the terminal. Since the two interaction sequences will not
match, the terminal will deauthenticate Jane and take action
to prevent another user from misusing her account.
The idea of ZEBRA stems from two observations: i) people
interact with most input devices with their hands, and ii) a
user’s hand movements when the user interacts with an input
device can be correlated to the inputs the device receives. For
example, when the user is scrolling or clicking, her ﬁngers
are moving but her wrist is relatively stationary; when the
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:00:04 UTC from IEEE Xplore.  Restrictions apply. 
user clicks the mouse and then starts typing on the keyboard
(typically with both hands), her hand will move from the
mouse to keyboard. Thus, the hypothesis driving ZEBRA is
that if we can capture the user’s hand movement and compare
it with the inputs the terminal receives, we can determine
whether the user is using the terminal.
Figure 1 shows a user’s wrist acceleration when she was
interacting with the terminal. The x-axis represents the time
(in seconds) from the start of the experiment and the y-axis
represents the magnitude of the acceleration, as measured by
the bracelet on the wrist. We marked, with shaded regions,
three types of user interactions in the ﬁgure: scrolling, typing,
and MKKM, where MKKM stands for ‘Mouse to Keyboard
or Keyboard to Mouse’ interaction representing the action
of switching between keyboard and mouse. As shown in
the ﬁgure, the user scrolled the mouse at 65.5 s, from 66.3 s
to 74.4 s, and then brieﬂy at 75.1 s. The graph shows that
her wrist was relatively still during scrolling, as one would
expect. When she moved her hand from mouse to keyboard
(around 77 s) to type, we see a sudden spike in acceleration
caused as she lifted her hand off the mouse and as she rested
her hands on the keyboard. As she typed (77.5 s to 83.4 s),
we see small changes in the acceleration, implying that her
wrist moves little during typing. After typing she switched
from keyboard to mouse (around 83.5 s), and we see another
sudden spike in the acceleration.
We can see the differences in the acceleration patterns
between interactions. For instance, broadly speaking, there
is more wrist movement during typing than scrolling, but
less than when she switches between keyboard and mouse.
This example supports our hypothesis that we can generate
a sequence of interactions from a user’s wrist movement.
The acceleration data that is not marked in the graph
represents the user’s other interactions with the terminal
such as mouse-dragging, clicking, or hand movements not
involving interaction with the terminal; we highlighted only
three types of interactions on the graph for readability.
C. Dealing with adversaries
ZEBRA deals with the two adversaries described in
Section III as follows:
In the case of the innocent authorized user who wants
to use an already open terminal for her own task, if Sally
attempts to use the terminal that Tina left open, the terminal
will try to verify whether the current user (Sally) is Tina.
If the terminal does not receive data from Tina’s bracelet,
e.g., because Tina is not near the terminal, ZEBRA will log
Tina out and will attempt to log Sally in. If Tina is near the
terminal but not using the terminal, e.g., she may be talking
to a nurse, then ZEBRA will attempt to correlate Tina’s
bracelet movements with Sally’s inputs on the terminal. The
classiﬁcation will fail with high probability, and ZEBRA will
log Tina out and attempt to log Sally in. Thus, ZEBRA will
prevent an innocent authorized user from performing a task,
e.g., updating a patient’s record, with another authorized
user’s credentials.
ZEBRA deals with the case of a malicious individual in
a similar fashion. In the second use-case, Claire leaves her
terminal unattended, and Jake manages to get access to her
terminal before the terminal times out and auto-locks. As
Jake tries to navigate the terminal using the keyboard and
mouse, the terminal will try to correlate Jake’s inputs with
Claire’s hand movements. Assuming that it is hard to control
a terminal at will by mimicking Claire’s hand movements
while she is around the corner talking on the phone, the
correlation will fail in a similar manner to the previous case
and, therefore, the terminal will lock, preventing Jake from
misusing Claire’s account. Our evaluation shows that our
assumption is reasonable in the case of humans trying to
mimic human hand movements. We touch on the resilience
of ZEBRA to automated attacks in Section VII-C.
V. METHOD
In this section, we describe the ZEBRA architecture and
our approach to correlate a terminal’s input with bracelet
acceleration data.
A. Architecture
Figure 2 shows the architecture of ZEBRA. As shown
in the ﬁgure, there are ﬁve main components in ZEBRA.
The interaction extractor extracts interactions from a user’s
keyboard and mouse inputs, and sends the sequence of
interactions to the authenticator and the time intervals of the
interactions to the segmenter. The segmenter segments the
accelerometer and gyroscope data into blocks based on the
time intervals it receives from the interaction extractor. The
feature extractor extracts features for each block of data that
it receives from the segmenter. The interaction classiﬁer takes
these features and classiﬁes them into one of our speciﬁed
interactions. The authenticator compares the actual sequence
of interactions that it receives from the interaction extractor
and the inferred sequence of interactions that it receives from
the interaction classiﬁer, and it makes a decision whether the
two users – the current terminal user and the user wearing
the bracelet – are the same or different. If they are different
then we need to deauthenticate the bracelet user, who is
currently logged in to the terminal. Based on the system
policy or user preference, ZEBRA can either logout the user,
lock the screen, raise an alarm, or take some other action.
Below we discuss each component in detail.
B. Interaction extractor
As mentioned, this component extracts ‘interactions’ from
the input events stream generated by the OS when the user
provides inputs to the terminal via keyboard or mouse. We
use three main types of user interactions with a terminal:
MKKM, scrolling, and typing. There are other interactions,
such as moving the mouse, dragging the mouse, or clicking
710
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:00:04 UTC from IEEE Xplore.  Restrictions apply. 
Figure 1: Acceleration of user’s wrist when she is using a computer terminal.
Input
events
Inputs
Input 
events
listener 
Interaction Extractor
Interaction
time intervals
segmented
data
Segmenter
Acceleration
and 
Gyroscope data
Actual
Interaction
sequence
Authenticator
Predicted
Interaction 
sequence
features
Feature
Extractor
Interaction
classiﬁer
User
Bracelet
Figure 2: ZEBRA architecture.
"Same user"
OR
"Different user"
the mouse, but we do not consider them because in our
evaluation they did not contribute to ZEBRA’s performance.
MKKM. This interaction captures the users’ dominant hand
(here, we mean the mouse hand) movement when she
switches from the mouse to the keyboard or from the
keyboard to the mouse; MKKM is short for ‘Mouse to
Keyboard or Keyboard to Mouse’. An MKKM interaction
consists of a mouse-related event followed by a keypress
event or vice-versa.
There is, however, a challenge in identifying whether the
keypress event followed by a mouse-related event was caused
by the dominant hand or the other hand, because the user
may press a key with her non-dominant hand while keeping
her dominant hand on the mouse. With one bracelet, we
cannot identify such events with certainty. We account for
such events by dividing the keys on the keyboard into three
zones, depending on which hand the user is likely to use to
press that key: left zone, middle (or ambiguous) zone, and
right zone, as shown in Figure 3. We introduced the ‘middle’
zone because not everyone types according to two-handed
typing guidelines, which divides the keyboard into two zones,
and we noticed some subjects used either hand to type the
keys in the middle zone. So, if the user is right handed (that
is, uses the mouse with her right hand) and presses a key in
711
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:00:04 UTC from IEEE Xplore.  Restrictions apply. 
the right zone after a mouse event, we assume she moved
her dominant hand. Thus, we assume that users will stick
with our zone divisions, i.e., use their left hand for keys in
the left zone and their right hand for keys in the right zone.
Some users may break this assumption, but this heuristic
seemed to work well, because to identify MKKM we only
need the user to press any one key in the right (or left) zone
with their right (or left) hand, and we observed that all our
subjects did use two hands when typing.
Scrolling. This interaction captures users’ use of a scroll-
wheel built-in to the mouse. When a user is scrolling,
ScrollWheel events are continuously generated by the
OS, each event reporting the amount of scroll performed
by the user since the last scroll, so that the application can
update the UI accordingly. We deﬁne a scrolling interaction
as a sequence of uninterrupted ScrollWheel events.
However, sometimes the mouse is slightly moved because
the user’s hand is not still, and we observe some MouseMove
events in the ScrollWheel events stream. The idea behind
this interaction is to capture the durations during which
the user was using the mouse and her hand (wrist) was
relatively stationary, so we ignore small mouse movements.
We consider a mouse movement as small if the associated
MouseMove events in the ScrollWheel events stream
are few in number (e.g., 5 events) and the cumulative
mouse displacement indicated by these MouseMove events
is small (e.g., 5 pixels). These thresholds (minimum number
of MouseMove events and maximum mouse displacement)
are parameters in our experiments.
Thus, we deﬁne a scrolling interaction as a sequence of
ScrollWheel events with few intervening MouseMove
events such that the total mouse displacement is small (below
a certain threshold).
Typing. This interaction captures the users’ use of the
keyboard. When a user hits a key, she ﬁrst presses the key
down, and then as she removes her ﬁnger she releases the
key up. Associated to these actions, two events are generated
by the OS for each keypress: KeyDown and KeyUp. Thus,
we deﬁne a typing interaction as a sequence of KeyDown
and KeyUp events.
If there is a continuous keypress events stream with mouse-
related events in between, we count those keypress events as
separate typing interactions, separated by the mouse-related
events. Unlike scrolling, where we ignored small numbers of
mouse-related events, during typing any mouse-related event
means that the user moved her hand from keyboard to mouse,
which is an MKKM interaction. Thus, for a keypress events
sequence with few mouse-related events in between, we
extract at least four interactions: typing, MKKM (to switch
to mouse), MKKM (to switch back to keyboard), typing, and
maybe scrolling between the two MKKM events if the user
scrolled the mouse-wheel.
Figure 3: Keyboard divided into left, middle, and right zones.
Extraction. When extracting interactions from input events,
we apply three constraints: idle threshold, minimum duration,
and maximum duration. Idle threshold is the maximum time
difference between two consecutive events in an interaction.
The rationale behind this constraint is to capture only the
interactions that involve the user’s continuous interaction
with the terminal and eliminate interactions during which the
user does tasks other than using the mouse and the keyboard.
During a pause, there is no input to the terminal; we do not
know what the user is doing, and thus cannot correlate with
the user’s wrist movement. If there is a pause greater than
the threshold, we split the interaction into two interactions
separated by the pause. For example, if in a series of keypress
events there is a 2 min pause, then we split these keypress
events into two typing interactions, one before the pause
begins and one after the pause ends.
The other constraints refer to the minimum and maximum
duration of interactions. If an interaction lasts for less than the
minimum duration, we ignore it, and if an interaction exceeds
the maximum duration we split it into two consecutive
interactions. While splitting the interaction we do ensure
that the new interaction is longer than the minimum duration:
if the new split interaction has duration less than the minimum
duration, we do not split the interaction; thus, we can have
interactions that are almost as long as minimum duration +
maximum duration.
Based on these three constraints and the deﬁnitions of
the interactions described above, this component outputs
a sequence of interactions from given input events. This
sequence of interactions, IE, is of the form
(I0, t0, t1), (I1, t2, t3), . . .
where I0 is an interaction ID (corresponding to one of the
three described interactions) that starts at time t0 and ends
at t1, and similarly interaction I1 spans (t2, t3).
sequence
IE,
From the
interaction ID sequence
(I0, I1, . . .) is sent to the authenticator. The interaction tim-
ings sequence ((t0, t1), (t2, t3), . . .) is sent to the segmenter.
712
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:00:04 UTC from IEEE Xplore.  Restrictions apply. 
C. Segmenter
This component receives accelerometer and gyroscope data
from the user’s bracelet. The accelerometer data is of the
form
(ti, xi, yi, zi), (tj, xj, yj, zj), . . .
where (ti, xi, yi, zi) represents one acceleration data sample
taken at time ti and the instantaneous accelerations along x,
y, and z axes are xi, yi, zi, respectively. The gyroscope data
is of the similar form
(ti, ai, bi, ci), (tj, aj, bj, cj), . . .
where (ti, ai, bi, ci) represents one gyroscope data sample
taken at time ti and represents the instantaneous rotational
velocity along x, y, and z axes, ai, bi, ci, respectively.
As shown in Figure 2 the segmenter receives actual
interaction time intervals from the interaction extractor. The
segmenter breaks the acceleration data stream into blocks
corresponding to each time interval, using the time of each
data sample and the time of the intervals. For the time-
interval sequence, ((t0, t1), (t2, t3), . . .) this component will
place all the accelerometer and gyroscope data samples with
time t0 ≤ t ≤ t1 into the ﬁrst block, all the data samples with
time t2 ≤ t ≤ t3 into the second block, and so on. These
data blocks are sent to the feature extractor. Acceleration and
gyroscope samples outside interaction intervals are discarded.
In most signal-processing algorithms, data is segmented
into blocks (also called windows) of equal size, but in our
case the block sizes are variable. There are two main reasons
to perform segmentation this way. First, when the user is not
interacting with the terminal, we do not have any interaction
sequences to use for authentication, so we ignore the sensor
data for durations when she is not interacting with the
terminal. Second, the user’s interactions themselves are of
variable duration so it makes sense to chunk accelerometer
data this way. For the durations when she is interacting,
one could segment sensor data into blocks of equal size
and infer an interaction for each block, but given that a
user’s interactions are of variable duration, it is likely that
one sensor data block would contain data for one or more
interactions, which would reduce the classiﬁer performance.
Variable segmentation ensures that each sensor data block
contains data for just one interaction.
D. Features
This component receives sensor data in blocks, and it
computes a feature vector over each block. We do not
know the orientation of the user’s bracelet, so we ignore
the orientation (individual axis accelerations and angular
velocities) and just use the magnitude of acceleration and
angular velocity. For each acceleration data sample (t, x, y, z),
the magnitude m is given by
(cid:2)
m =
x2 + y2 + z2
and for each gyroscope data sample (t, a, b, c), the magnitude
r is given by
(cid:2)
r =
a2 + b2 + c2.
After computing these magnitudes, we now have for each
block a series of magnitudes (m0, r0), (m1, r1), . . ..
We compute the following 12 features over each series
of acceleration and angular velocity magnitudes in a seg-
mented interaction block: mean, median, variance, standard
deviation, median absolute deviation (MAD), inter-quartile
range (IQR), power, energy, peak-to-peak amplitude, auto-
correlation, kurtosis, and skew. We chose the ﬁrst seven
features because others have used them successfully for
activity recognition [26] and for correlation among different
accelerometer signals [24]. We add the latter ﬁve features to
capture the patterns of the three interactions that we noticed.
During MKKM, there is a sudden spike in positive and
sometimes in negative direction, so we use peak-to-peak
amplitude. Because the placement of the peaks in a MKKM is
towards the start of the interactions, we use skew as a feature.
During typing, the peakedness is distinct, and so we included
kurtosis as one of our features. The wrist movement pattern
during a typing or scrolling interaction should be roughly
similar, unlike MKKM, so we use the auto-correlation feature
to capture that difference.
For each block of data, we compute a feature vector
F = (f0, . . . , f11), and send the sequence of feature vectors
F0, F1, . . . (each corresponding to one interaction block) to