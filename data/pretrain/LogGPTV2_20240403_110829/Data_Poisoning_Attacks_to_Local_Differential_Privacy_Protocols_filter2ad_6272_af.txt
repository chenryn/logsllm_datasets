ing [61] and key-value pairs [66], as well as developing new
defenses to mitigate our attacks.
Acknowledgements
We thank the anonymous reviewers for their constructive com-
ments. The conditional probability based detection method
for one target item was suggested by a reviewer. This work
was supported by NSF grant No.1937786.
References
[1] Equifax Announces Cybersecurity Incident Involving
Consumer Information. http://bit.ly/2PEHuPk, 2017.
[2] A hacker gained access to 100 million Capital One credit
card applications and accounts. https://cnn.it/2WINTKV,
2019.
[3] In systemic breach, hackers steal millions of Bulgarians’
ﬁnancial data. https://reut.rs/2r6sMq3, 2019.
[4] San francisco ﬁre department calls
http://bit.ly/336sddL, 2019.
for service.
[5] Milton Abramowitz and Irene A Stegun. Handbook
of mathematical functions: with formulas, graphs, and
mathematical tables. Courier Corporation, 1965.
[6] Rakesh Agrawal, Tomasz Imieli´nski, and Arun Swami.
Mining association rules between sets of items in large
databases. In SIGMOD, 1993.
USENIX Association
30th USENIX Security Symposium    961
[7] Scott Alfeld, Xiaojin Zhu, and Paul Barford. Data poi-
soning attacks against autoregressive models. In AAAI,
2016.
[23] Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil
Gong. Local model poisoning attacks to byzantine-
robust federated learning. In USENIX Security, 2020.
[8] Brendan Avent, Aleksandra Korolova, David Zeber,
Torgeir Hovden, and Benjamin Livshits. BLENDER:
Enabling local search with a hybrid differential privacy
model. In USENIX Security, 2017.
[9] Raef Bassily, Kobbi Nissim, Uri Stemmer, and
Abhradeep Guha Thakurta. Practical locally private
heavy hitters. In NeurIPS, 2017.
[10] Raef Bassily and Adam Smith. Local, private, efﬁcient
protocols for succinct histograms. In STOC, 2015.
[11] Battista Biggio, Blaine Nelson, and Pavel Laskov. Poi-
In
soning attacks against support vector machines.
ICML, 2012.
[12] Qiang Cao, Xiaowei Yang, Jieqi Yu, and Christopher
Palow. Uncovering large groups of active malicious
accounts in online social networks. In CCS, 2014.
[13] Albert Cheu, Adam Smith, and Jonathan Ullman. Ma-
nipulation attacks in local differential privacy. arXiv,
2019.
[14] Yann Collet. xxhash: Extremely fast hash algorithm.
https://github.com/Cyan4973/xxHash, 2016.
[15] Graham Cormode, Tejas Kulkarni, and Divesh Srivas-
tava. Marginal release under local differential privacy.
In SIGMOD, 2018.
[16] George Danezis and Prateek Mittal. Sybilinfer: Detect-
ing sybil nodes using social networks. In NDSS, 2009.
[17] Bolin Ding, Janardhan Kulkarni, and Sergey Yekhanin.
Collecting telemetry data privately. In NeurIPS, 2017.
[18] John C Duchi, Michael I Jordan, and Martin J Wain-
wright. Local privacy and statistical minimax rates. In
FOCS, 2013.
[19] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and
Adam Smith. Calibrating noise to sensitivity in private
data analysis. In TCC, 2006.
[20] Cynthia Dwork and Moni Naor. Pricing via processing
or combatting junk mail. In CRYPTO, 1992.
[21] Úlfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth
Raghunathan, Kunal Talwar, and Abhradeep Thakurta.
Ampliﬁcation by shufﬂing: From local to central differ-
ential privacy via anonymity. In SODA, 2019.
[24] Minghong Fang, Neil Zhenqiang Gong, and Jia Liu. In-
ﬂuence function based data poisoning attacks to top-n
recommender systems. In WWW, 2020.
[25] Minghong Fang, Guolei Yang, Neil Zhenqiang Gong,
and Jia Liu. Poisoning attacks to graph-based recom-
mender systems. In ACSAC, 2018.
[26] Neil Zhenqiang Gong, Mario Frank, and Prateek Mittal.
Sybilbelief: A semi-supervised learning approach for
structure-based sybil detection. TIFS, 2014.
[27] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Sid-
dharth Garg. Badnets: Evaluating backdooring attacks
on deep neural networks. IEEE Access, 2019.
[28] Ling Huang, Anthony D Joseph, Blaine Nelson, Ben-
jamin IP Rubinstein, and J Doug Tygar. Adversarial
machine learning. In AISec, 2011.
[29] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang
Liu, Cristina Nita-Rotaru, and Bo Li. Manipulating ma-
chine learning: Poisoning attacks and countermeasures
for regression learning. In S&P, 2018.
[30] Jinyuan Jia, Xiaoyu Cao, and Neil Zhenqiang Gong.
Intrinsic certiﬁed robustness of bagging against data
poisoning attacks. AAAI, 2021.
[31] Jinyuan Jia and Neil Zhenqiang Gong. Calibrate: Fre-
quency estimation and heavy hitter identiﬁcation with
local differential privacy via incorporating prior knowl-
edge. In INFOCOM, 2019.
[32] Peter Kairouz, Keith Bonawitz, and Daniel Ramage. Dis-
In
crete distribution estimation under local privacy.
ICML, 2016.
[33] Peter Kairouz, Sewoong Oh, and Pramod Viswanath.
Extremal mechanisms for local differential privacy. In
NeurIPS, 2014.
[34] Peter Kairouz, Sewoong Oh, and Pramod Viswanath.
In NeurIPS,
Secure multi-party differential privacy.
2015.
[35] Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorob-
eychik. Data poisoning attacks on factorization-based
collaborative ﬁltering. In NeurIPS, 2016.
[22] Úlfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova.
Rappor: Randomized aggregatable privacy-preserving
ordinal response. In CCS, 2014.
[36] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee,
Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojan-
ing attack on neural networks. In NDSS, 2018.
962    30th USENIX Security Symposium
USENIX Association
[37] Shike Mei and Xiaojin Zhu. Using machine teaching to
identify optimal training-set attacks on machine learners.
In AAAI, 2015.
[38] Mehran Mozaffari-Kermani, Susmita Sur-Kolay, Anand
Raghunathan, and Niraj K Jha. Systematic poisoning
attacks on and defenses for machine learning in health-
care. IEEE journal of biomedical and health informatics,
2014.
[39] Luis Muñoz-González, Battista Biggio, Ambra Demon-
tis, Andrea Paudice, Vasin Wongrassamee, Emil C Lupu,
and Fabio Roli. Towards poisoning of deep learning al-
gorithms with back-gradient optimization.
In AISec,
2017.
[40] Moni Naor, Benny Pinkas, and Eyal Ronen. How to
(not) share a password: Privacy preserving protocols for
ﬁnding heavy hitters with adversarial behavior. In CCS,
2019.
[41] Blaine Nelson, Marco Barreno, Fuching Jack Chi, An-
thony D Joseph, Benjamin IP Rubinstein, Udam Saini,
Charles A Sutton, J Doug Tygar, and Kai Xia. Exploit-
ing machine learning to subvert your spam ﬁlter. LEET,
2008.
[42] Andrew Newell, Rahul Potharaju, Luojie Xiang, and
Cristina Nita-Rotaru. On the practicality of integrity
attacks on document-level sentiment analysis. In AISec,
2014.
[43] James Newsome, Brad Karp, and Dawn Song. Para-
graph: Thwarting signature learning by training mali-
ciously. In RAID workshop, 2006.
[44] Roberto Perdisci, David Dagon, Wenke Lee, Prahlad
Fogla, and Monirul Sharif. Misleading worm signature
generators using deliberate noise injection.
In S&P,
2006.
[45] Zhan Qin, Yin Yang, Ting Yu, Issa Khalil, Xiaokui Xiao,
and Kui Ren. Heavy hitter estimation over set-valued
data with local differential privacy. In CCS, 2016.
[46] Sebastian Raschka. Mlxtend: Providing machine learn-
ing and data science utilities and extensions to python’s
scientiﬁc computing stack. The Journal of Open Source
Software, 2018.
[47] Xuebin Ren, Chia-Mu Yu, Weiren Yu, Shusen Yang,
Xinyu Yang, Julie A McCann, and S Yu Philip. LoPub:
High-dimensional crowdsourced data publication with
local differential privacy. TIFS, 2018.
[49] Benjamin IP Rubinstein, Blaine Nelson, Ling Huang,
Anthony D Joseph, Shing-hon Lau, Satish Rao, Nina
Taft, and J Doug Tygar. Antidote: understanding and
defending against poisoning of anomaly detectors. In
IMC, 2009.
[50] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian
Suciu, Christoph Studer, Tudor Dumitras, and Tom Gold-
stein. Poison frogs! targeted clean-label poisoning at-
tacks on neural networks. In NeurIPS, 2018.
[51] Ruggles Steven, Flood Sarah, Goeken Ronald, Grover
Josiah, Meyer Erin, Pacas Jose, and Sobek Matthew.
Ipums usa: Version 9.0 [dataset]. minneapolis, mn:
Ipums, 2019.
https://doi.org/10.18128/D010.V9.0,
2019.
[52] Gianluca Stringhini, Christopher Kruegel, and Giovanni
In
Vigna. Detecting spammers on social networks.
ACSAC, 2010.
[53] Apple Differential Privacy Team. Learning with privacy
at scale. Machine Learning Journal, 2017.
[54] Kurt Thomas, Damon McCoy, Chris Grier, Alek Kolcz,
and Vern Paxson. Trafﬁcking fraudulent accounts: The
role of the underground market in twitter spam and
abuse. In USENIX Security, 2013.
[55] Binghui Wang and Neil Zhenqiang Gong. Attacking
graph-based classiﬁcation via manipulating the graph
structure. In CCS, 2019.
[56] Binghui Wang, Jinyuan Jia, and Neil Zhenqiang Gong.
Graph-based security and privacy analytics via collec-
tive classiﬁcation with joint weight learning and propa-
gation. In NDSS, 2019.
[57] Gang Wang, Tristan Konolige, Christo Wilson, Xiao
Wang, Haitao Zheng, and Ben Y Zhao. You are how
you click: Clickstream analysis for sybil detection. In
USENIX Security, 2013.
[58] Gang Wang, Tianyi Wang, Haitao Zheng, and Ben Y
Zhao. Man vs. machine: Practical adversarial detec-
tion of malicious crowdsourcing workers. In USENIX
Security, 2014.
[59] Tianhao Wang, Jeremiah Blocki, Ninghui Li, and
Somesh Jha. Locally differentially private protocols
for frequency estimation. In USENIX Security, 2017.
[48] Amrita Roy Chowdhury, Chenghong Wang, Xi He, Ash-
win Machanavajjhala, and Somesh Jha. Cryptε: Crypto-
assisted differential privacy on untrusted servers.
In
SIGMOD, 2020.
[60] Tianhao Wang, Bolin Ding, Jingren Zhou, Cheng Hong,
Zhicong Huang, Ninghui Li, and Somesh Jha. Answer-
ing multi-dimensional analytical queries under local dif-
ferential privacy. In SIGMOD, 2019.
USENIX Association
30th USENIX Security Symposium    963
[61] Tianhao Wang, Ninghui Li, and Somesh Jha. Locally
differentially private frequent itemset mining. In S&P,
2018.
[62] Tianhao Wang, Ninghui Li, and Somesh Jha. Locally
differentially private heavy hitter identiﬁcation. TDSC,
2019.
[63] Tianhao Wang, Milan Lopuhaä-Zwakenberg, Zitao Li,
Boris Skoric, and Ninghui Li. Locally differentially
private frequency estimation with consistency. In NDSS,
2020.
[64] Stanley L Warner. Randomized response: A survey
technique for eliminating evasive answer bias. Journal
of the American Statistical Association, 1965.
[65] Guolei Yang, Neil Zhenqiang Gong, and Ying Cai. Fake
co-visitation injection attacks to recommender systems.
In NDSS, 2017.
[66] Qingqing Ye, Haibo Hu, Xiaofeng Meng, and Huadi
Zheng. Privkv: Key-value data collection with local
differential privacy. In S&P, 2019.
[67] Haifeng Yu, Haifeng Yu, Michael Kaminsky, Phillip B
Gibbons, and Abraham Flaxman. Sybilguard: defending
against sybil attacks via social networks. In SIGCOMM,
2006.
[68] Dong Yuan, Yuanli Miao, Neil Zhenqiang Gong, Zheng
Yang, Qi Li, Dawn Song, Qian Wang, and Xiao Liang.
Detecting fake accounts in online social networks at the
time of registrations. In CCS, 2019.
[69] Zhikun Zhang, Tianhao Wang, Ninghui Li, Shibo He,
and Jiming Chen. Calm: Consistent adaptive local
marginal for marginal release under local differential
privacy. In CCS, 2018.
A Proof of Theorem 2
Proof. Let β(1− fT ) + β(d−r)
eε−1
> 2r + 2r
eε − 1
1 + d − r
eε − 1
> β(2r− fT ) + 2βr
⇐⇒ d − 3r
> 2r− 1.
eε − 1
eε−1, we have:
(42)
Since eε > 1, the inequality above is equivalent to d > (2r−
1)(eε − 1) + 3r.
B FPRs of Detecting Fake Users
OUE:
If a user’s perturbed binary vector yyy follows the
OUE protocol, then we can calculate the probability that the
items in a set B of size z, are all 1 in the perturbed binary
vector as follows: Pr(yb = 1,∀b ∈ B) = pqz−1 if v ∈ B and
Pr(yb = 1,∀b ∈ B) = qz otherwise, where yb is the bth bit of
the perturbed binary vector yyy and v is the user’s item. Let
fB = ∑b∈B fb denote the sum of true frequencies of all items
in B, X1 denote the random variable representing the number
of users whose items are in B and whose perturbed binary
vectors are 1 for all items in B, and X2 denote the random vari-
able representing the number of users whose items are not in
B and whose perturbed binary vectors are 1 for all items in B.
If all the n + m users follow the OUE protocol, then we have
the following distributions: X1 ∼ Binom( fB(n + m), pqz−1)
and X2 ∼ Binom((1− fB)(n + m),qz), where Binom is a bino-
mial distribution. Now we consider another random variable
X = X1 + X2, which represents the number of users whose
perturbed binary vectors are 1 for all items in B. X follows a
distribution with mean μ and variance Var as follows:
μ = fB(n + m)pqz−1 + (1− fB)(n + m)qz
≤ (n + m)pqz−1
(43)
(44)
Var = fB(n + m)pqz−1(1− pqz−1)
+ (1− fB)(n + m)qz(1− qz)
≤ (n + m)pqz−1(1− pqz−1).
(45)
(46)
Based on the Chebyshev’s inequality, for any τz > (n +
m)pqz−1, we have:
Pr(X ≥ τz) = Pr(X − μ ≥ τz − μ)
≤ Pr(|X − μ| ≥ τz − μ)
≤ Var
(τz − μ)2
≤ (n + m)pqz−1(1− pqz−1)
[τz − (n + m)pqz−1]2
(47)
Here, if we choose τz as the threshold, the probability Pr(X ≥
τz) is the false positive rate, which is upper bounded by
(n+m)pqz−1(1−pqz−1)
[τz−(n+m)pqz−1]2
.
OLH: As discussed in Section 6.2, we ﬁrst construct a d-bit
binary vector yyy for each user with a tuple (H,a) such that
yv = 1 if and only if H(v) = a. For an item set B of size z,
assume X is a random variable that represents the number of
users whose constructed binary vectors are 1’s for all items
in B. If all the n + m users follow the OLH protocol, then for
any τz > 0, the probability that X ≥ τz is bounded as follows:
Pr(X ≥ τz) = 1− Pr(X ≤ τz − 1)
= 1− I(1− qz−1;n + m− τz + 1,τz)
= I(qz−1;τz,n + m− τz + 1)
(48)
Note that if we set τz as the threshold, the probability Pr(X ≥
τz) is the false positive rate.
964    30th USENIX Security Symposium
USENIX Association