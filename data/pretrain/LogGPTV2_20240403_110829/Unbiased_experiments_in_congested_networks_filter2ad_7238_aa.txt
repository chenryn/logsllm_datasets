title:Unbiased experiments in congested networks
author:Bruce Spang and
Veronica Hannan and
Shravya Kunamalla and
Te-Yuan Huang and
Nick McKeown and
Ramesh Johari
Unbiased Experiments in Congested Networks
Veronica Hannan
Shravya Kunamalla
Bruce Spang
Stanford University
USA
PI:EMAIL
Te-Yuan Huang
Netflix
USA
PI:EMAIL
Netflix
USA
PI:EMAIL
Nick McKeown
Stanford University
USA
PI:EMAIL
Netflix
USA
PI:EMAIL
Ramesh Johari
Stanford University
USA
PI:EMAIL
ABSTRACT
When developing a new networking algorithm, it is established
practice to run a randomized experiment, or A/B test, to evaluate its
performance. In an A/B test, traffic is randomly allocated between a
treatment group, which uses the new algorithm, and a control group,
which uses the existing algorithm. However, because networks are
congested, both treatment and control traffic compete against each
other for resources in a way that biases the outcome of these tests.
This bias can have a surprisingly large effect; for example, in lab
A/B tests with two widely used congestion control algorithms, the
treatment appeared to deliver 150% higher throughput when used
by a few flows, and 75% lower throughput when used by most flows—
despite the fact that the two algorithms have identical throughput
when used by all traffic.
Beyond the lab, we show that A/B tests can also be biased at scale.
In an experiment run in cooperation with Netflix, estimates from
A/B tests mistake the direction of change of some metrics, miss
changes in other metrics, and overestimate the size of effects. We
propose alternative experiment designs, previously used in online
platforms, to more accurately evaluate new algorithms and allow
experimenters to better understand the impact of congestion on
their tests.
CCS CONCEPTS
• Networks → Network experimentation; • Information sys-
tems → Multimedia streaming; • Mathematics of computing
→ Probability and statistics.
ACM Reference Format:
Bruce Spang, Veronica Hannan, Shravya Kunamalla, Te-Yuan Huang, Nick
McKeown, and Ramesh Johari. 2021. Unbiased Experiments in Congested
Networks. In ACM Internet Measurement Conference (IMC ’21), November
2–4, 2021, Virtual Event, USA. ACM, New York, NY, USA, 16 pages. https:
//doi.org/10.1145/3487552.3487851
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
IMC ’21, November 2–4, 2021, Virtual Event, USA
© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9129-0/21/11...$15.00
https://doi.org/10.1145/3487552.3487851
1
80
1 INTRODUCTION
Engineers routinely run A/B tests when testing new network al-
gorithms. In an A/B test, the experimenter randomly allocates a
small fraction of traffic (say 1% or 5%) to a new algorithm, called
the treatment group, and compares its performance against the
control group running the old algorithm. A/B tests are widely used
as the gold standard for understanding how a new algorithm will
behave at scale. Almost all large tech companies routinely use
A/B tests to evaluate changes before deploying them [18, 22, 34,
48, 53, 58, 63, 70, 76]. Networking research often includes the re-
sults of A/B tests, and uses them to justify new algorithms [17–
19, 24, 25, 29, 42, 46, 49, 55, 57, 58, 60, 63, 72, 87].
So when we recently ran experiments to test whether bitrate
capping reduces network congestion for Netflix, we ran A/B tests.
Bitrate capping was introduced in response to COVID-19; major
streaming services cooperated with governments to lower bitrates
offered and reduce overall internet load [3, 33]. This caused a re-
duction in congestion in certain networks around the globe.
We decided to dig deeper, to understand exactly how bitrate
capping reduces congestion, and how doing so impacts video qual-
ity metrics. While we had data from just before and after bitrate
capping was deployed (and later when it was removed), these were
during periods of lockdown and stay-at-home orders when the
internet was changing rapidly. We wanted to conduct a more sys-
tematic study of its effects. Naturally, we ran an A/B test where we
capped a fraction of traffic to a very congested network.
In this A/B test, capping didn’t appear to reduce congestion at all!
In fact, it appeared to make things worse: capped traffic experienced
5% lower throughput and 5% higher delay. The A/B test results were
so marginal that if we had not had evidence showing that bitrate
capping reduced congestion when widely deployed, we might have
dismissed it and not explored further. How could a treatment that
we knew reduced congestion at scale not also reduce congestion in
an A/B test?
Stepping back, we realized the confusion could be caused by
interference. Interference is when units in the treatment group in-
teract with units in the control group. It is well known in causal
inference that interference can bias experiment results [45]. In
social networks, changing something for a user in the treatment
group can impact the behavior of their friends in the control group
and bias the results of an experiment [28]. In online marketplaces,
increasing the price of items in a treatment group can increase the
demand for the relatively cheaper items in the control group and
IMC ’21, November 2–4, 2021, Virtual Event, USA
Spang et al.
bias results [39]. There are many examples of interference bias from
markets, education, disease, and more [21, 38, 41, 52].
Both treatment and control groups in our test used the same net-
work, and their packets traversed the same links and same queues.
There is a long line of networking research showing that algo-
rithms compete with each other when sharing a congested network
[2, 5, 8, 15, 16, 23, 43, 44, 50, 56, 71, 81, 82, 84–86]. If capping bitrates
freed up bandwidth, the uncapped control traffic could take up that
bandwidth and get better performance. This could make bitrate
capping look worse than it would if the uncapped traffic were not
present, even if it was improving congestion. This gave us reason
to believe that interference may exist, which would explain our
unexpected A/B test results.
In this work we show that interference exists in experiments
run in congested networks, and biases the results of A/B tests
at scale. We show that bitrate capping does reduce congestion,
and that the misleading A/B test result was due to interference.
In order to do this, we propose and test new experiment designs
which more accurately evaluate new algorithms. Our results suggest
that usual A/B testing practice paints an incomplete picture of the
performance of new algorithms in congested networks, and should
be complemented by additional experiments.
Without interference, A/B tests give us a way to safely and accu-
rately evaluate performance using a very small fraction of traffic.
But because of interference, A/B tests on small fractions of traffic do
not accurately predict performance at scale. Interference therefore
creates a tradeoff between safety and accuracy: the only way to
accurately measure performance is to run an algorithm on 100%
of traffic, but nobody would do this with an untested algorithm!
Our goal in this paper is to make the networking community, both
academic researchers and industry practitioners, aware of this trade-
off and to propose techniques to help mitigate it. We encourage
the community to apply these techniques broadly and evaluate
networking algorithms with alternate experiments. We encourage
continued measurement and the development of new techniques
to mitigate bias.
We begin with an overview of experiment design in Section 2. We
describe how A/B tests are run, and which quantities they estimate.
Using a framework from the field of causal inference, we define the
relevant quantities of interest for new networking algorithms.
We then run small lab experiments in Section 3 to give exam-
ples of how networking A/B tests can be biased. We show that
experiments using multiple parallel connections, packet pacing,
and different congestion control schemes all exhibit bias. If we were
to evaluate these algorithms using naïve A/B tests, we would make
incorrect conclusions. We might prematurely abandon a good al-
gorithm, or deploy an algorithm that behaves worse when widely
deployed than in the experiment.
Returning to our bitrate capping experiments, in Section 4 we
describe our joint experiments with Netflix. We study the perfor-
mance of bitrate capping and report on the bias we found in our
initial A/B tests. While measurements show that bitrate capping
significantly reduces congestion, naïve A/B tests do not reflect this
behavior. Naïve A/B tests miss changes in some metrics, overes-
timate or underestimate the changes in others, and even get the
direction of improvement wrong for a few. We were able to carry
out this analysis due to a unique network architecture at Netflix.
Using a pair of reliably congested links with well-balanced traffic,
we ran different experiments on each link and compared the results.
Based on our experience, in Section 5 we investigate possible
ways experimenters can accurately evaluate new algorithms at
scale. We discuss two possible paths to managing the tradeoff be-
tween safety and bias. The first is to adapt the common process of
gradual deployments to measure interference. The second involves
the use of small-scale, targeted switchback experiments to more
accurately measure the effects of a new algorithm while managing
safety concerns. We use the results of our paired link experiment
to simulate what the experimenter might have obtained in these
alternate approaches, and show that both substantially reduce bias.
We believe this paper is just the beginning of work on unbiased
network experimentation. There is much to explore in designing
more effective experiments, improving the analysis of experiments
we run, and understanding the way interference behaves in net-
works. We wonder how many effective algorithms have been aban-
doned because of the way we run experiments, and what ineffective
algorithms have been deployed because we were misled by A/B
tests? Accordingly, we situate our work within the broader context
of related research in Section 6 and conclude in Section 7.
2 WHAT WE WANT TO MEASURE
Before discussing experiments in more detail, it will be useful to
give some background on how they are run, and what they can
measure. In this section we provide a formal statistical foundation
for A/B testing. The presentation is borrowed from causal infer-
ence [45]. The description is simplified, but gives enough conceptual
scaffolding for the remainder of our work.
Treatment assignment. When we evaluate a new algorithm
there are some units which run the algorithm. Units may be users,
sessions, flows, connections, servers, etc... We let 𝑈 be the set of
all units. Each unit 𝑖 ∈ 𝑈 is allocated to either treatment where it
runs the new algorithm or control where it does not. Let 𝐴 be the
vector of treatment assignments to all units. We denote treatment
as 𝐴𝑖 = 1, and the set of treated units as 𝑇 . We denote control as
𝐴𝑖 = 0 and the set of control units as 𝐶.
Potential outcomes. When evaluating a new algorithm, we
are interested in how it improves various metrics. In the language
of causal inference, these metrics are called outcomes. Let 𝑌𝑖(𝐴)
be the outcome of interest on unit 𝑖 given the vector of treatment
assignments 𝐴. 𝑌𝑖(𝐴) might be the average throughput of unit 𝑖,
the minimum latency, or the 99th percentile packet loss. 𝑌𝑖(𝐴)
can be a random variable, since we expect some variability due to
randomness in algorithms and randomness in arrivals. 1
Randomized unit assignment. In an A/B test, we randomly
assign units to treatment independently with probability 𝑝 or con-
trol with probability 1−𝑝. In other words, each 𝐴𝑖 is an independent
Bernoulli(𝑝) random variable. We refer to the probability 𝑝 as the
treatment allocation.
To make this point more explicit, we introduce some additional
notation. Define 𝜇𝑇 (𝑝) (resp., 𝜇𝐶(𝑝)) to be the average outcome
value over the randomness in the assignment of treatment (resp.
1This approach to causal inference via potential outcomes was pioneered by Neyman
[75] (a 1990 translation of the original 1923 publication) and Rubin [66]; see [45] for
details.
2