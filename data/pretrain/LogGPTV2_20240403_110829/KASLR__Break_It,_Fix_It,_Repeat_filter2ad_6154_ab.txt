user-space applications, the kernel space was later on also pro-
tected by KASLR [20, 49], e.g., introduced in Windows in 2007 [49],
macOS in 2012 [2], and Linux in 2014 [20]. The kernel consists of
multiple segments that are individually mapped into the kernel
address space. These segments include the code (i.e., text segment),
drivers or modules, and data (e.g., stack, heap). The KASLR imple-
mentations of the three major OSs (Linux, Windows, macOS) only
use coarse-grained randomization, i.e., randomized base address.
Fine-grained KASLR implementations using code diversification
have been proposed [27, 72] but are not used in practice.
Another property of KASLR implementations is that the kernel
is mapped using either 4 kB or 2 MB pages. The mapping is 2 MB-
aligned [76], reducing the number of possible offsets. Moreover, the
order of the randomized segments is not changed, e.g., in Linux,
the text segment always has a lower address than the modules [57].
Consequently, KASLR provides a lower entropy than typical user-
space ASLR implementations [20]. However, if an exploit attempt
fails, it likely crashes the kernel. Hence, an attacker only has one
shot, and exploitation techniques relying on a large number of
retries cannot be used against the kernel if KASLR is active.
Linux. In Linux 5.x, most sections are independently randomized
at boot, including the direct-physical map, vmalloc and ioremap
space (vmalloc area), virtual-memory map (vmemmap), text seg-
ment, and modules [24]. The text segment is mapped between
0xffff ffff 8000 0000 and 0xffff ffff c000 0000 with a maxi-
mum size of 1 GB [76]. As the kernel has to be aligned to a 2 MB
boundary, the randomization has 9 bits of entropy. Therefore, the
kernel is placed at one of 512 possible offsets. Modules are mapped
using 4 kB pages in a 1 GB range following the text segment. Un-
mapped pages follow each module before a new module starts [49].
Start and end addresses for the direct-physical map, the vmal-
loc area, and the vmemmap are documented [57], but analyzing
the start addresses on repeated restarts shows that they are only
correct if KASLR is disabled. Therefore, we analyzed the KASLR
implementation of Linux kernel version 5.2.9. This analysis showed
that the possible start address is indeed 0xffff 8880 0000 0000 for
the direct-physical map. It is then placed at a random offset from
the start address, aligned to a 1 GB boundary. The vmalloc space is
placed at a random offset relative to the end of the direct-physical
map with at least 1 GB between them. The vmemmap area is then
randomized starting from the end of the vmalloc area, again with
at least 1 GB between them. The range of possible addresses is,
therefore, from 0xffff 8880 0000 0000 to 0xffff fdff ffff ffff,
always with a 1 GB alignment and the preserved order.
Windows. Windows randomizes almost everything except the
HAL heap once at boot [44]. Windows first introduced KASLR
with Vista [49] and improved it over time [32]. Windows 7 maps
the kernel, followed by the drivers in the same range with the
same randomization. The address range of the kernel and drivers
is 0xffff f800 0000 0000 to 0xffff f803 ffff ffff [49]. KASLR
on Windows 10 differs from Windows 7 as there is a separate area
for the kernel and drivers. The kernel is still mapped in the same
virtual address range, but drivers are now mapped in the range of
0xffff f800 0000 0000 to 0xffff f80f ffff ffff [23]. The kernel
is also 2 MB-aligned, resulting in 8192 possible offsets. Drivers are
mapped with 4 kB pages with a 16 kB alignment.
macOS. Starting with macOS 10.8 (Mountain Lion), the kernel,
kexts (kernel modules), and zones are randomized [70]. For in-
stance, the kernel is mapped in the range of 0xffff ff80 0000 0000
to 0xffff ff80 2000 0000 with a 2 MB alignment, resulting in 256
possible offsets. The offset at which the kernel is placed relative to
the start of the address range is called kslide. According to Chen
and He [14], kernel and kexts share the same kslide.
3 A NOVEL (K)ASLR BREAK
In this section, we first analyze the Meltdown hardware mitiga-
tion on new Intel CPUs. We then introduce EchoLoad, an attack
primitive that exploits incomplete Meltdown countermeasures to
break KASLR. We detail how we can use it to break KASLR from
an unprivileged user-space application, JavaScript, and SGX.
3.1 Analyzing the Meltdown Mitigation
The Meltdown vulnerability allowed unprivileged users to leak
kernel memory (cf. Section 2.2). The immediate workaround was
100%
50%
0%
100
80
80
82
82
100
100
100
90
Vulnerable
Mitigated
Unaffected
user
kernel
not present
Figure 1: Loads from non-present pages always stall, loads
to kernel addresses stall on unaffected AMD CPUs.
KAISER [31], a software-only solution to unmap the kernel when
running in user space. With the Whiskey Lake microarchitecture,
Intel fixed the vulnerability in hardware without providing further
details on how their fix works. CPUs with the hardware mitigation
indicate that they are not vulnerable by having the RDCL_NO bit set
in the IA32_ARCH_CAPABILITIES model-specific register [42].
Lipp et al. [59] argued that stalling the CPU until the permission
check is done might be too costly. We suspect that such a change
also requires redesigning a significant part of the CPU’s pipeline.
As the first CPUs with hardware mitigations already shipped ap-
proximately one and a half years after Meltdown was disclosed to
Intel, we expect only minor hardware changes as mitigation.
Hypothesis. We hypothesize that instead of stalling on an illegal
memory load, the CPU zeroes out the result. Hence, the CPU still
loads inaccessible memory locations, but instead of providing the
real value to dependent instructions, it always provides ‘0’.
Verification. We get the first indication that our hypothesis is
correct by simply mounting a Meltdown attack. When running
the Meltdown attack on a Xeon Silver 4208 CPU which has the
RDCL_NO bit set, we always get ‘0’s. To verify our hypothesis, we
further analyzed performance counters on three different systems: a
Meltdown-vulnerable Intel CPU (i7-8650U), an Intel CPU with hard-
ware mitigations (Xeon Silver 4208), and a non-affected AMD CPU
(Ryzen Threadripper 1920X). For all systems, we evaluate perfor-
mance counters when executing the following code 104 times: if (
transient_begin()) { *(volatile char*)0; oracle[*address]; }. The func-
tion transient_begin either starts a TSX transaction if available,
or sets up a signal handler for segmentation faults [59]. The null-
pointer access is required to always cause an exception.
The first performance counter of interest is the number of CPU
stalls when executing the above code. On Intel CPUs, we use
CYCLE_ACTIVITY.STALLS_MEM_ANY, and on AMD CPUs the “Dis-
patch Stalls” counter. We set address to a valid kernel address. As
baselines, we choose a mapped user address as well as a non-present
address for address. Figure 1 shows the results of the performance
counters for all 3 systems. For comparison, we normalized the
values such that the highest value on each system represents 100 %.
All CPUs stall when accessing a non-present virtual address. The
AMD CPU also stalls when accessing a kernel address. Both Intel
CPUs with and without mitigations show the same stall behavior.
Hence, even the Intel CPUs with Meltdown mitigations do not stall
when accessing a kernel address. This indicates that the memory
load for the kernel address is actually issued.
We substantiate this observation by analyzing another perfor-
mance counter. With the counters UOPS_DISPATCHED_PORT.PORT_2
and UOPS_DISPATCHED_PORT.PORT_3, we can track the number of
µOPs issued on the load ports. The sum of these two counters is
the number of all memory loads. Figure 2 shows the number of
memory loads when running the code mentioned above with a
90,007
⋅104
60,066
60,060
60,042
60,037
75,020
without mitigations
with mitigations
8
6
4
2
0
user
kernel
not present
*(volatile char*)(mem + *address);
1 if (transient_begin()) {
2
3 }
4 if (flush_reload(mem)) return ADDRESS_MAPPED;
5 else return ADDRESS_NOT_MAPPED;
Figure 2: Issued load µOPs for user and kernel addresses (In-
tel). Only invalid loads from non-present pages are reissued.
200
100
0
211
206
0
90
92
0
without mitigations
with mitigations
user
kernel
not present
Figure 3: Number of cycles L1D cache misses are pending.
User and kernel addresses reach the memory hierarchy, non-
present pages do not.
stalls
0
kernel
stalls
0xffff ffff8 0000 000
0xffff ffff c000 0000
Figure 4: Reading addresses not physically backed stalls the
CPU, while kernel addresses return ‘0’ (or the actual data).
user-space, kernel-space, and non-present address both on an Intel
CPU with and without hardware mitigation. When trying to load
from a non-present page, the load faults and the load instruction is
re-issued [79]. The number of issued loads for kernel addresses is
the same as for user-space addresses on both CPUs. This indicates
that these loads succeed and do not have to be re-issued.
Finally, we show that the issued loads for kernel addresses indeed
load data from the memory hierarchy, and not, e.g., from an internal
buffer containing ‘0’. Thus, we monitor the number of cycles that L1
data-cache misses are waiting to be retrieved. Figure 3 shows the val-
ues of the performance counter L1D_PEND_MISS.PENDING_CYCLES
for the previously shown code. While non-present pages do not
cause an L1 miss, both user-space and kernel addresses cause L1
misses. This is even the case for CPUs with hardware mitigations
against Meltdown, showing that loads to kernel addresses retrieve
the actual value, and only later on zero it out.
3.2 Breaking KASLR
EchoLoad is a new microarchitectural KASLR attack exploiting
Meltdown-related side effects. EchoLoad reliably breaks KASLR,
regardless of OS, software mitigations, and microcode updates.
EchoLoad works on all Intel CPUs since 2010, even if they are not
affected by Meltdown, e.g., CPUs with the RDCL_NO bit. In contrast
to the KASLR break by Schwarz et al. [76], EchoLoad also works on
the new Cascade Lake, which is not affected by Meltdown or MDS.
General Idea. The general idea is to distinguish whether accessing
a kernel address in the transient-execution domain leads to a stall.
We exploit the fact that instructions can only be executed out of
order if their data dependencies are fulfilled. Hence, we dereference
a user-space memory location where the address is computed based
on the value of the kernel address that is being tested.
Listing 1 shows this central part of EchoLoad. First, an attacker
induces transient execution by provoking a fault or a misspecu-
lation in Line 1. If the access to address in Line 2 stalls, the user
Listing 1: The main part of EchoLoad. The address mem is only
cached if the access to address does not stall.
address cannot be computed before the transient execution aborts.
Otherwise, the user address is dereferenced and, thus, cached be-
fore the transient execution aborts. After the transient execution,
the attacker probes the user- address in Line 4, e.g., using Flush+
Reload. If the user address is cached, address is valid, i.e., physi-
cally backed. Otherwise, address is not valid, i.e., not physically
backed. Figure 4 illustrates the general idea of EchoLoad.
CPUs with Meltdown Fixes. To break KASLR on CPUs with
Meltdown fixes, we run EchoLoad on all 512 possible kernel offsets
(cf. Section 2.5). Only where a physical page backs the tested address,
we read 0, on other addresses the CPU stalls.
As the CPU stalls on all reads from addresses that the kernel is
not mapped to, we observe no false positives. This makes EchoLoad
a very reliable attack that even works on Cascade Lake CPUs.
CPUs without Meltdown Fixes. On CPUs without Meltdown
fixes, we cannot rely on the CPU returning 0 for reads on kernel
pages. Instead, if KPTI is disabled, we read the actual content of
the page. As the content of the page is code, there are 256 possible
addresses which could be dereferenced.
As the 256 possible addresses are contiguous, and the cache line
size is typically 64 byte, they fall into one out of 4 possible cache
lines. Testing 4 adjacent cache lines with Flush+Reload triggers the
stride prefetcher [38] on Intel CPUs. Instead, we can exploit the
L2 adjacent cache line prefetcher (spatial prefetcher) [38], which
fetches the sibling cache line whenever a cache miss is handled.
Hence, we only have to check 2 cache lines using Flush+Reload,
which works without triggering the stride prefetcher. Consider
a case with 4 adjacent cache lines. If the data we read falls into
line 0 and we check line 1, we observe a hit on line 1 because the
prefetcher also loads it into the cache. The same is true if the data
falls into line 3, and we check line 4. By merely checking cache
lines 1 and 3, we detect all possible accesses.
We can even further increase the performance by only checking
one cache line. By using a kernel module, we investigated the
beginning of the kernel text segment and determined that it is
always the same across kernel versions (i.e., 0x48).
EchoLoad also works with KPTI [30] as the pages still mapped
with KPTI use the same randomization offset as the rest of the
kernel code. While the value differs with KPTI (i.e., 0xf), it is still
the same across kernel versions that use it. As we only look for
the beginning of the kernel and we know that the value remains
constant, we can reduce the number of cache lines we need to check
to 1. This further improves the performance of our KASLR break.
EchoLoad and LVI-NULL. On CPUs that have already received
fixes for Meltdown, EchoLoad is the inverse of the LVI-NULL at-
tack [88]. While LVI-NULL abuses the fixes to inject a dummy value
of zero to dependent transient instructions in a victim, EchoLoad
Table 1: Environments where we evaluated EchoLoad and
Data Bounce (KPTI disabled).
1 if(xbegin() == (~0u)) { *(volatile char*)mem; xend(); }
2 if(flush_reload(mem)) return ROLL_BACK;
3 else return IMMEDIATE_ABORT;
µarch.
CPU
Cherry Trail
Intel Atom x5-Z8300
Arrandale
Intel Core i5-450M
Ivy Bridge
Intel Core i5-3230M
Kaby Lake R
Intel Core i5-8250U
Haswell
Intel Core i7-4790
Skylake
Intel Core i7-6700K
Kaby Lake R
Intel Core i7-8650U
Whiskey Lake
Intel Core i7-8565U
Coffee Lake
Intel Core i9-9900K
Broadwell
Intel Xeon E5-1630 v4
Intel Xeon Silver 4208
Cascade Lake
Intel Cascade Lake (Google Cloud) Cascade Lake
AMD Ryzen Threadripper 1920X
AMD Ryzen 7 3700
ARM Cortex-A57