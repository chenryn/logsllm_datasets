We perform hourly snapshots to AWS S3 using the cloud-aws plugin. During a snapshot, the master node experienced a reboot, which caused the snapshot process to get stuck. This issue was observed with Elasticsearch (ES) version 1.4.0 running on ArchLinux. After upgrading the cluster to ES version 1.4.2, we encountered an error when attempting to create a new snapshot:

```json
{
  "error": "ConcurrentSnapshotExecutionException[[my_s3_repository:2015.04.13.09.16.42] a snapshot is already running]",
  "status": 503
}
```

The command `curl localhost:9200/_snapshot/my_s3_repository/_all?pretty` does not list the stuck snapshot. Additionally, the delete snapshot command does not complete and hangs indefinitely. A rolling restart (restarting one node after another) did not resolve the issue. The cleanup script from imotov/elasticsearch-snapshot-cleanup#2 also failed to work. In S3, there is no metadata file for the stuck snapshot, but some index files are present.

### How to Resolve the Issue

To address this, we need to discard the old snapshots, create a new repository, and start fresh. Given that this is a production cluster, stopping the entire cluster is not an option.

#### Steps to Follow:

1. **Check Snapshot Status:**
   ```sh
   curl -X GET "localhost:9200/_snapshot/_status?pretty"
   ```
   This will provide the current status of all snapshots, including any that are stuck.

2. **Delete Stuck Snapshots:**
   If the delete command hangs, you can try to forcefully remove the stuck snapshot by deleting its metadata in S3. However, this should be done with caution.

   - Identify the stuck snapshot ID.
   - Manually delete the corresponding metadata files in the S3 bucket.

3. **Create a New Repository:**
   Create a new snapshot repository to ensure a clean start:
   ```sh
   PUT /_snapshot/new_s3_repository
   {
     "type": "s3",
     "settings": {
       "bucket": "your-bucket-name",
       "region": "your-region",
       "base_path": "new-snapshots"
     }
   }
   ```

4. **Take a New Snapshot:**
   Once the new repository is created, take a new snapshot:
   ```sh
   PUT /_snapshot/new_s3_repository/snapshot_001
   {
     "indices": "index1,index2",
     "ignore_unavailable": true,
     "include_global_state": false
   }
   ```

5. **Verify the New Snapshot:**
   Verify that the new snapshot was successfully created:
   ```sh
   GET /_snapshot/new_s3_repository/snapshot_001
   ```

By following these steps, you should be able to resolve the issue and start taking new snapshots without interference from the old, stuck snapshot.