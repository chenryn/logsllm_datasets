# A Longitudinal View of HTTP Traffic

**Authors:**
- Tom Callahan<sup>1</sup>
- Mark Allman<sup>2</sup>
- Vern Paxson<sup>2,3</sup>

**Affiliations:**
1. Case Western Reserve University
2. International Computer Science Institute
3. University of California, Berkeley

## Abstract
In this paper, we analyze three and a half years of HTTP traffic observed at a small research institute to characterize the evolution of various facets of web operation. While our dataset is modest in terms of user population, it is unique in its temporal breadth. We leverage this longitudinal data to study various characteristics of the traffic, including client and server behavior, object and connection characteristics, and the structure of content delivery. Specifically, we assess the use of browser caches, the efficacy of network-based proxy caches, and the role of content delivery networks (CDNs). Although each aspect we study has been investigated to some extent in prior work, our contribution lies in providing a unique long-term characterization.

## 1. Introduction
This paper examines web traffic logs collected over a three-and-a-half-year period (2006–mid-2009) at the border of a small research institute. The dataset includes an average of 160 active users per month. Despite the relatively small population, the longitudinal view provides valuable insights into the evolution of web traffic. Our study serves to re-evaluate and update previous findings, offering a more current understanding of web operations, including transaction types and sizes, and the mechanisms of content delivery through CDNs, browser caches, and other means. This multifaceted view is also useful for setting up realistic testbeds and simulations that accurately reflect today's web environment.

Our methodology involves analyzing web traffic logs from an intrusion detection system over the specified period. We describe our data collection and analysis methods in Section 2. In Section 3, we characterize various aspects of the traffic at the transaction level. Section 4 examines user-driven behaviors, such as object popularity and caching impact. Section 5 focuses on the structure of web page delivery, including the use of CDNs. We briefly review related work in Section 6 and summarize our findings in Section 7.

## 2. Data and Methodology
For this study, we use web traffic logs collected at the border connecting the International Computer Science Institute (ICSI) with its ISP. We employ the Bro intrusion detection system [12] to reconstruct HTTP sessions from the packet stream. These sessions are logged using Bro’s standard HTTP logging policy. The logs include timestamps, IP addresses, URLs, HTTP transaction types and sizes, hostnames, and response codes. The dataset spans January 2006 through July 2009. Due to the large volume of data, we analyze only the first seven days of each month. We focus on outgoing connections initiated by ICSI clients, resulting in 16.9 million connections out of 28.8 million total.

Figure 1 summarizes the high-order characteristics of the dataset, showing stability in the number of web object requests, HTTP connections, HTTPS connections, server hostnames, and server IP addresses over time. Note that HTTPS connections are encrypted and thus not further analyzed. We identify "web servers" by both IP address and hostname, recognizing that CDNs can cause a single IP to host multiple distinct hostnames. Conversely, a given hostname may have multiple IP addresses for load balancing or proximity reasons. The average number of users per month is 160, with a standard deviation of 13, but the longitudinal tracking of this user population is our primary contribution.

Two versions of Bro HTTP policy scripts were used to gather the data, with a significant difference in the first ten months of 2006. Initially, the scripts grouped logical web sessions under one identifier, obscuring the number of underlying TCP connections. Starting in mid-October 2006, the scripts were changed to log activity on a per-TCP connection basis. For most analyses, this difference is not critical, but for those requiring the number of underlying TCP connections, we start our analysis from November 2006.

## 3. HTTP Transaction Characterization
We begin by characterizing client HTTP transactions. Figure 2 shows the transaction type breakdown over time. GET transactions, which request data, make up the majority of observed transactions, approaching 90% in most months. POST transactions, involving user uploads, account for around 10%. Other transaction types, such as HEAD and PROPFIND, are minimal, typically less than 1% of the total. The number of GETs and POSTs shows a slight increasing trend over the observation period, with a notable increase in POSTs due to the increased use of GMail in early 2006.

Figures 3 and 4 illustrate the average and median sizes of GET and POST transactions over time. Both show a generally increasing trend, likely due to richer content and web 2.0 sites. The median size of POST transactions remains small and constant, indicating the prevalence of simple form inputs. For GET requests, the medians are generally an order of magnitude less than the averages, reflecting the heavy-tailed nature of web traffic. Figure 5 shows the distribution of GET response sizes for a typical day, July 2, 2007. An anomaly in December 2006, caused by a single client fetching large files, was removed from the analysis.

Figure 6 displays the median duration of HTTP connections and the time between connection establishment and the first HTTP request. The median connection duration decreased after December 2007, attributed to a reduction in persistent HTTP connections. Before this point, the median duration was around 1 second; afterward, it fell to 100–200 milliseconds. This suggests that even small improvements in the delivery process can significantly enhance the user experience.

The median time before an HTTP request is issued is roughly constant at just under 100 milliseconds, aligning with previous findings. However, we observe longer intervals, particularly with GMail, where the 99th percentile interval is around 246 seconds, compared to 14 to 55 seconds for non-GMail traffic. This indicates that short intervals may not be the best model for modern web applications.

## 4. User Behavior
[Continuation of the text to be provided]

---

This revised version aims to improve clarity, coherence, and professionalism while maintaining the original content and intent.