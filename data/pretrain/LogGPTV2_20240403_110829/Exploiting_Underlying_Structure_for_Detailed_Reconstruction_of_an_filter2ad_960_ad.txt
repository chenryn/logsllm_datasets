bandwidth allows us to quantify the 30% distortion7 at the
telescope due to its limited capacity. In the absence of this
ﬁne-grained analysis, we would have been limited to not-
ing that the telescope saturated, but without knowing how
much we were therefore missing.
Figure 9 shows a scatter-plot of the estimates of effec-
tive bandwidth as estimated from the observations at the
358
Internet Measurement Conference 2005 
USENIX Association
CAIDA ≥ Wisc.*1.05 Wisc. ≥CAIDA*1.05
# Domains
# Domains
64
35
9
7
5
4
3
3
19
TLD
.net
.com
.edu
.cn
.nl
.ru
.jp
.gov
other
53
17
7
5
5
5
3
3
25
TLD
.edu
.net
.jp
.nl
.com
.ca
.tw
.gov
other
Table 1: Domains with divergent estimates of effective
bandwidth.
two telescopes. We might expect these to agree, with most
points lying close to the y = x line, other than perhaps for
differing losses due to saturation at the telescopes them-
selves, for which we can correct.
Instead, we ﬁnd two
major clusters that lie approximately along y = 1.4x and
y = x/1.2. These lie parallel to the y = x line due to the
logscale on both axes. We see a smaller third cluster be-
low the y = x line, too. These clusters indicate systematic
divergence in the telescope observations, and not simply a
case of one telescope suffering more saturation losses than
the other, which would result in a single line either above
or below y = x.
To analyze this effect, we took all of the sources with
an effective bandwidth estimate from both telescopes of
more than 10 Mbps. We resolved each of these to domain
names via reverse DNS lookups, taking the domain of the
responding nameserver if no PTR record existed. We then
selected a representative for each of the unique second-
level domains present among these, totaling 900. Of these,
only 29 domains had estimates at the two telescopes that
agreed within 5% after correcting for systematic telescope
loss. For 423 domains, the corrected estimates at CAIDA
exceeded those at Wisconsin by 5% or more, while the
remaining 448 had estimates at Wisconsin that exceeded
CAIDA’s by 5% or more.
Table 1 lists the top-level domains for the unique second-
level domains that demonstrated ≥ 5% divergence in es-
timated effective bandwidth. Owing to its connection to
Internet-2, the CAIDA telescope saw packets from .edu
with signiﬁcantly fewer losses than the Wisconsin tele-
scope, which in turn had a better reachability from hosts in
the .net and .com domains. Clearly, telescopes are not
“ideal”devices, with perfectly balanced connectivity to the
rest of the Internet, as implicitly assumed by extrapolation-
based techniques. Rather, what a telescope sees during an
event of large enough volume to saturate high-capacity In-
ternet links is dictated by its speciﬁc location on the Inter-
net topology. This ﬁnding complements that of [4], which
found that the (low-volume) background radiation seen at
different telescopes likewise varies signiﬁcantly with loca-
tion, beyond just the bias of some malware to prefer nearby
addresses when scanning.
6 Deducing the seed
Cracking the seeds — System uptime. We now de-
scribe how we can use the telescope observations to de-
duce the exact values of the seeds used to (re)initialize
Witty’s PRNG. Recall from Fig. 2 that the Witty worm at-
tempts to open a disk after every 20,000 packets, and re-
seeds its PRNG on success. To get a seed with reason-
able local entropy, Witty uses the value returned by the
Get Tick Count system call, a counter set to zero at
boot time and incremented every millisecond.
In § 4 we have developed the capability to reverse-
engineer the state of the PRNG at an infectee from packets
received at the telescope. Additionally, Eqns 1 and 2 give
us the ability to crank the PRNG forwards and backwards
to determine the state at preceding and successive packets.
Now, for a packet received at the telescope, if we could
identify the precise number of calls to the function rand
between the reseeding of the PRNG and the generation of
the packet, simply cranking the PRNG backwards the same
number of steps would reveal the value of the seed. The dif-
ﬁculty here is that for a given packet we do not know which
“generation”it is since the PRNG was seeded. (Recall that
we only see a few of every thousand packets sent.) We thus
have to resort to a more circuitous technique.
We split the description of our approach into two parts:
a technique for identifying a small range in the orbit (per-
mutation sequence) of the PRNG where the seed must lie,
and a geometric algorithm for ﬁnding the seeds from this
candidate set.
Identifying a limited range within which the seed
must lie. Figure 10 shows a graphical view of our tech-
nique for restricting the range where the seed can poten-
tially lie. Figure 10(a) shows the sequence of packets as
generated at the infectee. The straight line at the top of
the ﬁgure represents the permutation-space of the PRNG,
−1 as gen-
i.e., the sequence of numbers X0, X1, · · · , X232
erated by the PRNG. The second horizontal line in the mid-
dle of the ﬁgure represents a small section of this sequence,
blown-up to show the individual numbers in the sequence
as ticks on the horizontal line. Notice how each packet
consumes exactly four random numbers, represented by the
small arcs straddling four ticks.
Only a small fraction of packets generated at the infectee
reach the telescope. Figure 10(b) shows four such pack-
ets. By cranking forward from the PRNG’s state at the
ﬁrst packet until the PRNG reaches the state at the second
packet, we can determine the precise number of calls to the
rand function in the intervening period. In other words,
if we start from the state corresponding to the ﬁrst packet
and apply Eqn 1 repeatedly, we will eventually (though see
USENIX Association
Internet Measurement Conference 2005  
359
Permutation Space
Permutation Space
X 0
X 232
X 0
X 0
X 232
Permutation Space
X 232
Translate back by 60,000
20,000 packets
20,000 packets
4x
4y
4z+1
Seed
Failed Disk Write
Pkt
Pkt
Pkt
Pkt
First
Pkt after
Reseeding
Translate back by 20,000
Translate back by 40,000
(a) Sequence of packets generated at the
infectee.
(b) Packets seen at the telescope. Notice
how packets immediately before or after a
failed disk-write are separated by 4z + 1
cranks of the PRNG rather than 4z.
(c) Translating these special intervals back by
multiples of 20,000 gives bounds on where the
seed can lie.
Figure 10: Restricting the range where potential seeds can lie.
below) reach the state corresponding to the second packet,
and counting the number of times Eqn 1 was applied gives
us the precise number of random numbers generated be-
tween the departure of these two packets from the infectee.
Note that since each packet consumes four random num-
bers (the inner loop of lines 2–7 in Fig. 2), the number of
random numbers will be a multiple of four.
However, sometimes we ﬁnd the state for a packet re-
ceived at the telescope does not lie within a reasonable
number of steps (300,000 calls to the PRNG) from the state
of the preceding packet from the same infectee. This signi-
ﬁes a potential reseeding event: the worm ﬁnished its batch
of 20,000 packets and attempted to open a disk to overwrite
a random block. Recall that there are two possibilities: the
random disk picked by the worm exists, in which case it
overwrites a random block and (regardless of the success
of that attempted overwrite) reseeds the PRNG, jumping
to an arbitrary location in the permutation space (control
ﬂowing through lines 8→9→10→1→2 in Fig. 2); or the
disk does not exist, in which case the worm continues for
another 20,000 packets without reseeding (control ﬂowing
through lines 8→11→2 in Fig. 2). Note that in either case
the worm consumes a random number in picking the disk.
Thus, every time the worm ﬁnishes a batch of 20,000
packets, we will see a discontinuity in the usual pattern of
4z random numbers between observed packets. We will
instead either ﬁnd that the packets correspond to 4z + 1
random numbers between them (disk open failed, no re-
seeding); or that they have no discernible correspondence
(disk open succeeded, PRNG reseeded and now generating
from a different point in the permutation space).
This gives us the ability to identify intervals within
which either failed disk writes occurred, or reseeding
events occurred. Consider the interval straddled by the ﬁrst
failed disk write after a successful reseeding. Since the
worm attempts disk writes every 20,000 packets, this inter-
val translated back by 20,000 packets (80,000 calls to the
PRNG) must straddle the seed. In other words, the begin-
ning of this special interval must lie no more than 20,000
packets away from the reseeding event, and its end must lie
no less than that distance away. This gives us upper and
lower bounds on where the reseeding must have occurred.
A key point is that these bounds are in addition to the
bounds we obtain from observing that the worm reseeded.
Similarly, if the worm fails at its next disk write attempt
too, the interval straddling that failed write, when trans-
lated backwards by 40,000 packets (160,000 calls to the
PRNG), gives us another pair of lower and upper bounds
on where the seed must lie. Continuing this chain of rea-
soning, we can ﬁnd multiple upper and lower bounds. We
then take the max of all lower bounds and the min of all
upper bounds to get the tightest bounds, per Figure 10(c).
A geometric algorithm to detect the seeds. Given this
procedure, for each reseeding event we can ﬁnd a limited
range of potential in the permutation space wherein the new
seed must lie. (I.e., the possible seeds are consecutive over
a range in the permutation space of the consecutive 32-bit
random numbers as produced by the LC PRNG; they are
not consecutive 32-bit integers.) Note, however, that this
may still include hundreds or thousands of candidates, scat-
tered over the full range of 32-bit integers.
Which is the correct one? We proceed by leveraging
two key points: (i) for most sources we can ﬁnd numer-
ous reseeding events, and (ii) the actual seeds at each event
are strongly related to one another by the amount of time
that elapsed between the events, since the seeds are clock
readings. Regarding this second point, recall that the seeds
are read off a counter that tracks the number of millisec-
onds since system boot-up. Clearly, this value increases
linearly with time. So if we observe two reseeding events
with timestamps (at the telescope) of t1 and t2, with cor-
responding seeds S1 and S2, then because clocks progress
linearly with time, (S2 − S1) ≈ (t2 − t1). In other words,
if the infectee reseeded twice, then the value of the seeds
360
Internet Measurement Conference 2005 
USENIX Association
s
t
s
o
h
f
o
r
e
b
m
u
N
 160
 140
 120
 100
 80
 60
 40
 20
 0
 0
 10
 20
Uptime (days)
 30
 40
 50
Figure 11: Number of infectees with a system uptime of
the given number of days.
must differ by approximately the same amount as the differ-
ence in milliseconds in the timestamps of the two packets
seen immediately after these reseedings at the telescope.
Extending this reasoning to k reseeding events, we get
(Sj − Si) ≈ (tj − ti), ∀i, j : 1 ≤ i, j ≤ k. This implies
that the k points (ti, Si) should (approximately) lie along
a straight line with slope 1 (angle of 45◦) when plotting
potential seed value against time.
We now describe a geometric algorithm to detect such
a set of points in a 2-dimensional plane. The key obser-
vation is that when k points lie close to a straight line of
a given slope, then looking from any one of these points
along that slope, the remaining points should appear clus-
tered in a very narrow band. More formally, if we project
an angular beam of width δ from any one of these points,
then the remaining points should lie within the beam, for
reasonably small values of δ. On the other hand, other, ran-
domly scattered points on the plane will see a very small
number of other points in the beam projected from them.
The algorithm follows directly from this observation. It
proceeds in iterations. Within an iteration, we project a
beam of width δ = arctan 0.1 ≈ 0.1 along the 45◦ line
from each point in the plane. The point is assigned a score
equal to the number of other points that lie in its beam. Ac-
tual seeds are likely to get a high score because they would
all lie roughly along a 45◦ line. At the end of the iteration,
all points with a score smaller than some threshold (say
k/2) are discarded. Repeating this process in subsequent
iterations quickly eliminates all but the k seeds, which keep
supporting high scores for each other in all iterations.
We ﬁnd this algorithm highly effective given enough re-
seeding events. Figure 11 presents the results of the com-
putation of system uptime of 784 machines in the infectee
population. These infectees were chosen from the set that