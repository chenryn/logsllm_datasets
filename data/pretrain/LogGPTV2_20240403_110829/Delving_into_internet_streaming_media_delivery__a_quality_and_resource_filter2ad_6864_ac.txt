user (client side action) or exported from a media meta ﬁle
stored on or dynamically generated by a Web server (server
side action). For example, rtspt means using TCP as the
transport protocol while rtspu means using UDP. We ex-
tracted URL modiﬁers from the summary of media playing
information, which is sent by the client in a RTSP/MMS
command when a session is terminated. In both home user
and business user workloads, we found that for more than
70% of the Windows streaming sessions, TCP is speciﬁed
as the transport protocol by content providers. This ex-
plains why TCP-based media streaming is so prevalent on
the Internet. Study [26] suggests that the NAT and ﬁrewall
deployment constrained the usage of UDP in streaming me-
dia applications. Our conjecture is that as content providers
are generally aware of the wide deployment of NAT and ﬁre-
walls, they actively use TCP to avoid any possible shielding
or protocol rollover to end users. With such a conﬁgura-
tion, even if UDP is supported at both the client side and
the server side, the streaming media will still be delivered
over TCP directly.
To validate our conjecture, we further investigate the
NAT usage of home users and business users with the MMS
streaming in our workloads. Diﬀerent from RTSP, in MMS
streaming, a client reports its local IP address to its server
in clear text. Extracting this information from the MMS
workload, we found that most MMS users in the home
and business user workloads report private IPs (such as
192.168.1.100),
indicating that they access the Internet
through NAT. In the home user workload, about 98.3%
of the MMS requests are initiated from clients shielded by
NAT, and about 99.5% of the MMS clients are shielded by
NAT. These two numbers are 89.5% and 88.0% in the busi-
ness user workload, respectively. A NAT router hosts up
to 3 MMS clients in the home user workload, and up to 64
MMS clients in the business user workload. Thus, for these
clients, the TCP transmission speciﬁed on the server side
eﬀectively avoids protocol rollover, and signiﬁcantly reduces
user perceived startup latency.
On the other hand, RealNetworks media services try to
avoid protocol rollover by using NAT transversal techniques.
By reverse-engineering of the protocol, we ﬁnd that diﬀerent
from the Windows media service, in which a server sends
UDP packets to its client ﬁrst through the port that the
client reports in the SETUP command, in the RealNetworks
media service, a client sends UDP packets to its server ﬁrst,
so that the server can ﬁgure out the client’s external UDP
port number converted by NAT. To distinguish diﬀerent user
sessions shielded by the same NAT, the server uses diﬀer-
ent UDP port numbers to listen to UDP packets coming
from diﬀerent sessions, which are generated by the server
dynamically and sent to its clients in the replies of SETUP
commands. As a result, UDP accounts for the majority of
streaming traﬃc in the RealNetworks media service, and
protocol rollover is less frequent than that in the Windows
media service. However, as indicated by Figure 4, once a
protocol rollover happens, the rollover time in the RealNet-
works media service is generally much longer than that in the
Windows Media service. Furthermore, this solution some-
how violates the standard RTSP speciﬁcation because the
UDP port number that a client reports to its server in the
SETUP command is intentionally discarded.
 1
 0.8
 0.6
 0.4
 0.2
F
D
C
 0
 1
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
Fast Cache
Normal TCP
 10
 100
 1000
File Length (sec)
(a) File length
 10000  100000
Fast Cache
Normal TCP
 100
 1000
 10000
Encoding Rate (Kbps)
(b) Encoding rate
Figure 5: Features of media delivered by Fast Cache and normal TCP streaming
5. FAST STREAMING
In early streaming media services, a media object is
streamed at its encoding rate and a small play-out buﬀer
is used to smooth the streaming jitter. However, in prac-
tice, the play-out buﬀer may be exhausted, since the avail-
able bandwidth between a client and its server may ﬂuc-
tuate from time to time. This is particularly important
for TCP-based streaming, in which the congestion control
mechanism constrains the streaming rate. As we have shown
in Section 4, content providers of Windows media services
often use TCP-based streaming directly to avoid protocol
rollover. In order to provide high quality streaming experi-
ence to end users, Windows Media services use Fast Stream-
ing techniques [2], including Fast Start, Fast Cache, Fast
Recovery, and Fast Reconnect 2. Both Fast Start and Fast
Cache transmit media data at a rate higher than the media
encoding rate 3. Fast Start can run over both TCP and
UDP while Fast Cache always runs over TCP. Fast Start
is enabled by almost all Windows media servers in order to
reduce the startup buﬀering time for clients. Basically, Fast
Start transmits data to the client as fast as possible until
the play-out buﬀer is ﬁlled. After the Fast Start period,
Fast Cache streams media data until the entire object is de-
livered or the session is terminated by the user.
In order
to smooth out network bandwidth ﬂuctuations, Fast Cache
transmits media data to a client at a speed usually up to
5 times the media encoding rate and the client maintains a
growing buﬀer for the early arrived data.
Table 4 shows the total volume of streaming traﬃc deliv-
ered over Fast Cache (FC), normal TCP streaming excluding
FC (TCP), and UDP streaming (UDP) in our workloads. As
the table shows, Fast Cache is widely used by both third-
party hosting services and self-hosting services, accounting
for 50.1% and 21.0% of the streaming traﬃc in the home and
business user workloads, respectively. There is less stream-
ing traﬃc delivered over Fast Cache in the business user
workload than in the home user workload, because the ac-
cess pattern of business users is diﬀerent from that of home
users. Business users access more audio and live media than
home users. Audio media objects have low bit rates and
usually do not need Fast Cache, while live media objects
2As Fast Recovery and Fast Reconnect events are rare in
our workloads, we do not include them in this study.
3Similarly, RealNetworks media services can also stream a
media object at a rate higher than its encoding rate. Due to
page limits, we only present the analysis results of Windows
streaming services.
Table 4: Streaming Traﬃc with Fast Cache
Delivery
Method
FC
TCP
UDP
home user
third-party
55.00 GB
49.91 GB
35.15 GB
self-hosting
68.72 GB
15.27 GB
22.74 GB
business user
third-party
self-hosting
3.75 GB
15.92 GB
4.23 GB
6.24 GB
16.79 GB
0.62 GB
cannot be streamed over Fast Cache at all. The on-demand
video they access is diﬀerent too.
Figure 5 compares the distribution of ﬁle length and en-
coding rate of objects delivered over Fast Cache supported
streaming and normal TCP streaming in on-demand Win-
dows video sessions of the home user workload. As shown in
the ﬁgure, Fast Cache is more widely used for objects with
longer ﬁle lengths and higher encoding rates. This is rea-
sonable because these objects are more sensitive to network
bandwidth ﬂuctuations and thus Fast Cache can help more.
Due to page limit, in the remainder of this section, we only
present our analysis results of Windows media streaming
for TCP-based on-demand video sessions with a playback
duration longer than 30 seconds in the home user workload,
if not speciﬁed particularly (the results for the business user
workload are similar).
5.1 Fast Cache smoothes bandwidth ﬂuctua-
tion
As mentioned before, Fast Cache supported streaming
smoothes the ﬂuctuation of network bandwidth by main-
taining a growing buﬀer for the early arrived data. To un-
derstand how Fast Cache utilizes the network bandwidth,
we extract the Bandwidth header in a client’s PLAY com-
mand, and the Speed header in this command and that in
the server’s reply. The Bandwidth header contains the client
advertised bandwidth, which is either a default value set by
a user in the media player, or measured in real time before
media streaming with the so called “packet pair” technique.
The Speed header in a client’s PLAY command speciﬁes the
delivery speed that the client requests, in multiples of the
media encoding rate, which is usually equivalent to the client
available bandwidth. The speed that a server agrees to of-
fer is speciﬁed in the Speed header of the reply to the PLAY
command, which is usually not greater than 5 times of the
media encoding rate. However, the actual streaming speed
may be smaller than the speed that the server claims in
the PLAY reply. Thus, we computed the average of actual
streaming rate during each user session based on the packet
level information in our workload (the startup buﬀering is
excluded).
 1e+07
 1e+06
 100000
s
p
b
K
 10000
 1000
 100
Actual streaming rate
Request streaming rate
Advertised Bandwidth
 1e+07
 1e+06
 100000
s
p
b
K
 10000
 1000
 100
Actual streaming rate
Advertised Bandwidth
 0
 20
 40
 60
Sessions (%)
 80
 100
 0
 20
 40
 60
Sessions (%)
 80
 100
(a) Fast Cache supported streaming
(b) Normal TCP streaming
Figure 6: The client advertised bandwidth, client requesting rate, and server streaming rate
Figure 6(a) shows the distribution of the client adver-
tised bandwidth, the client requested streaming rate (i.e.,
the product of the Speed value and the media encoding
rate), and the actual streaming rate for streaming sessions
with Fast Cache support. Figure 6(b) shows the advertised
bandwidth and the actual streaming rate for normal TCP
streaming. Comparing Figures 6(a) and 6(b), we ﬁnd that
the actual streaming rate in Fast Cache supported stream-
ing is much closer to the client advertised bandwidth than
that in normal TCP streaming 4. So Fast Cache exploits
the unutilized network bandwidth under the constraint of
TCP congestion control. In other words, for normal TCP
streaming, it is possible to deliver the same media at a higher
transmission rate, or to deliver the media with a higher en-
coding rate.
In Windows media streaming, the default play-out buﬀer
size accounts for ﬁve seconds media playback [1]. Upon net-
work ﬂuctuations, if the play-out buﬀer is empty, the client
has to stop to buﬀer data, and a playback jitter occurs. With
Fast Cache, the early buﬀered data could aﬀord a smooth
playback much longer before rebuﬀering is necessary. We
deﬁne the rebuﬀering ratio of a streaming session as the to-
tal time for rebuﬀering over the total playback duration,
which reﬂects the streaming quality that a user experiences.
Figure 7(a) shows the rebuﬀering ratio of sessions streamed
with and without Fast Cache support (i.e. Fast Cache and
normal TCP streaming in the ﬁgure). To make a fair com-
parison, we only consider sessions requesting video objects
with an encoding rate between 200–400 Kbps. About 15% of
the normal TCP streaming sessions suﬀer rebuﬀering while
only about 8.5% of the Fast Cache supported streaming ses-
sions suﬀer rebuﬀering. Thus, Fast Cache can eﬀectively
eliminate rebuﬀering in streaming sessions. We also observe
that for Fast Cache supported streaming, there are about
1.8% of the sessions with a rebuﬀering ratio larger than
50%, while for normal TCP streaming, there are only about
0.9% of the sessions with a rebuﬀering ratio larger than 50%.
The reason is that when rebuﬀering happens, normal TCP
streaming may switch to a stream of lower rate if the me-
dia object is MBR encoded (see Section 6.1), thus avoiding
further rebuﬀering. In contrast, stream switch is disabled in
Fast Cache supported streaming, thus the rebuﬀering may
happen repeatedly, resulting a large rebuﬀering ratio (see
Section 6.3).
Fast Cache supported streaming also decreases the possi-
4The 2 Gbps client advertised bandwidth corresponds to the
player’s connection speed setting “10 Mbps and above”.
bility of encountering network congestion by reducing the
data transmission time. Figure 7(b) shows the distribu-
tion of data transmission duration for Fast Cache supported
streaming sessions and normal TCP streaming sessions, re-
spectively. We can see that although in general the media
objects streamed with Fast Cache have higher ﬁle lengths
and encoding rates as shown in Figure 5, the transmission
time of Fast Cache supported streaming is much shorter
than that of normal TCP streaming (note that the x-axis is
in log scale).
5.2 Fast Cache produces extra trafﬁc
Fast Cache continuously delivers a media object at a rate
higher than (usually up to ﬁve times) its encoding rate. In
reality, the entire media object can be delivered completely
to the user in the middle of the user’s playback. If the user
stops in the middle, the pre-arrived data for the remaining
part are wasted. Considering the well known fact that most
sessions only access the initial part of a video object [30], the
extra delivered traﬃc by Fast Cache would be non-trivial,
especially for large media objects such as movies.
In this study, we compute the over-supplied traﬃc as fol-
lows. The packet size and the time that a packet should be
rendered can be extracted directly from the RTP packet
header. Based on the timestamps of PLAY, PAUSE, and
TEARDOWN commands, we get the user playback duration
for each session. Assuming a default ﬁve-second play-out
buﬀer [1] on the client side, we compute the extra traﬃc for
each session. Figure 8 shows the extra traﬃc caused by Fast
Cache supported streaming and normal TCP streaming, for
all Windows streaming sessions in the home user workload.
On average, Fast Cache over-supplies about 54.8% of me-
dia data to clients due to clients’ early terminations, while
the over-supplied traﬃc is only about 4.6% for normal TCP
streaming sessions. Figure 9 shows the CDF of the aver-
age transmission speed (in multiples of the media encoding
rate) of Fast Cache supported and normal TCP streaming,
respectively. In the computation of the average tranmission
speed, we only consider the data transmission after the Fast
Start buﬀering period. The high data transmission speed in
Fast Cache supported streaming indicates the reason for the
over-supplied data.
5.3 Server response time of Fast Cache
In addition to over-utilizing the network bandwidth, by
transmitting media at a rate much higher than its encod-
ing rate, a streaming server running Fast Cache may also
consume more CPU, memory, disk I/O, and other resources
 1
 0.95
F
D
C
 0.9
 0.85
 0.8
 0
Fast Cache
Normal TCP
 0.2
 0.4
 0.6
 0.8
 1
Rebuffer Ratio (rebuffer time / play time)
(a) Rebuﬀering ratio
 1
 0.8
 0.6
 0.4
 0.2
F
D
C
 0
 1
Fast Cache
Normal TCP
 10
Transmission Duration (sec)
 100
 1000
(b) Data transmission duration
Figure 7: Bandwidth ﬂuctuation smoothed by Fast Cache
 1
 0.8
 0.6
 0.4
 0.2
F
D
C
 10000
 0
 0
Fast Cache
Normal TCP
 2
 4
 6
 8
 10
Over transferred bytes/total played bytes
Figure 8: Extra traﬃc produced by
Fast Cache
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
Fast Cache
Normal TCP
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
Speed
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
Fast Cache
Normal TCP
 0.1
Server Response Time (sec)
 1
 10
Fast Cache
Normal TCP
 0.1
Server Response Time (sec)
 1
 10
Figure 9: Average data transmis-
sion speed