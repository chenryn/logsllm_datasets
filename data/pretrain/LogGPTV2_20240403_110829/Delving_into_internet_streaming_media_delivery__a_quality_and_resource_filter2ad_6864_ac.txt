### Streaming Media Transport Protocols and NAT Usage

Streaming media can be initiated by the client (client-side action) or exported from a media metadata file stored on or dynamically generated by a web server (server-side action). For example, `rtspt` indicates the use of TCP as the transport protocol, while `rtspu` indicates the use of UDP. We extracted URL modifiers from the summary of media playing information, which is sent by the client in an RTSP/MMS command when a session is terminated. In both home and business user workloads, we found that for more than 70% of Windows streaming sessions, TCP is specified as the transport protocol by content providers. This explains why TCP-based media streaming is so prevalent on the Internet. A study [26] suggests that the deployment of NAT and firewalls has constrained the usage of UDP in streaming media applications. Our conjecture is that content providers, aware of the widespread use of NAT and firewalls, actively use TCP to avoid any possible blocking or protocol rollover to end users. With this configuration, even if UDP is supported at both the client and server sides, the streaming media will still be delivered over TCP.

To validate our conjecture, we further investigated the NAT usage of home and business users with MMS streaming in our workloads. Unlike RTSP, in MMS streaming, a client reports its local IP address to the server in clear text. Extracting this information from the MMS workload, we found that most MMS users in both home and business workloads report private IPs (such as 192.168.1.100), indicating they access the Internet through NAT. In the home user workload, about 98.3% of MMS requests are initiated from clients shielded by NAT, and about 99.5% of MMS clients are shielded by NAT. These numbers are 89.5% and 88.0% in the business user workload, respectively. A NAT router can host up to 3 MMS clients in the home user workload and up to 64 MMS clients in the business user workload. Thus, for these clients, the TCP transmission specified on the server side effectively avoids protocol rollover and significantly reduces the perceived startup latency.

On the other hand, RealNetworks media services try to avoid protocol rollover using NAT traversal techniques. By reverse-engineering the protocol, we found that, unlike Windows media services where a server sends UDP packets to the client first through the port reported in the SETUP command, in RealNetworks media services, the client sends UDP packets to the server first, allowing the server to determine the client's external UDP port number converted by NAT. To distinguish different user sessions behind the same NAT, the server uses different UDP port numbers to listen to UDP packets from different sessions, which are dynamically generated and sent to clients in the replies of SETUP commands. As a result, UDP accounts for the majority of streaming traffic in RealNetworks media services, and protocol rollover is less frequent than in Windows media services. However, as indicated in Figure 4, once a protocol rollover occurs, the rollover time in RealNetworks media services is generally much longer than in Windows Media services. Furthermore, this solution violates the standard RTSP specification because the UDP port number reported by the client in the SETUP command is intentionally discarded.

### Fast Streaming Techniques

In early streaming media services, media objects were streamed at their encoding rate, and a small play-out buffer was used to smooth streaming jitter. However, in practice, the play-out buffer may be exhausted due to fluctuating available bandwidth between the client and server. This is particularly important for TCP-based streaming, where the congestion control mechanism constrains the streaming rate. As shown in Section 4, content providers of Windows media services often use TCP-based streaming directly to avoid protocol rollover. To provide a high-quality streaming experience, Windows Media services use Fast Streaming techniques, including Fast Start, Fast Cache, Fast Recovery, and Fast Reconnect. Both Fast Start and Fast Cache transmit media data at a rate higher than the media encoding rate. Fast Start can run over both TCP and UDP, while Fast Cache always runs over TCP. Fast Start is enabled by almost all Windows media servers to reduce the startup buffering time for clients. Fast Start transmits data to the client as fast as possible until the play-out buffer is filled. After the Fast Start period, Fast Cache streams media data until the entire object is delivered or the session is terminated by the user.

To smooth out network bandwidth fluctuations, Fast Cache transmits media data to the client at a speed usually up to five times the media encoding rate, and the client maintains a growing buffer for the early-arrived data. Table 4 shows the total volume of streaming traffic delivered over Fast Cache (FC), normal TCP streaming excluding FC (TCP), and UDP streaming (UDP) in our workloads. As the table shows, Fast Cache is widely used by both third-party hosting services and self-hosting services, accounting for 50.1% and 21.0% of the streaming traffic in the home and business user workloads, respectively. There is less streaming traffic delivered over Fast Cache in the business user workload because business users access more audio and live media, which do not typically require Fast Cache. Audio media objects have low bit rates and usually do not need Fast Cache, while live media objects cannot be streamed over Fast Cache at all.

Figure 5 compares the distribution of file length and encoding rate of objects delivered over Fast Cache-supported streaming and normal TCP streaming in on-demand Windows video sessions of the home user workload. As shown in the figure, Fast Cache is more widely used for objects with longer file lengths and higher encoding rates. This is reasonable because these objects are more sensitive to network bandwidth fluctuations, and Fast Cache can help more. Due to page limits, in the remainder of this section, we only present our analysis results of Windows media streaming for TCP-based on-demand video sessions with a playback duration longer than 30 seconds in the home user workload, unless otherwise specified (the results for the business user workload are similar).

#### 5.1 Fast Cache Smoothes Bandwidth Fluctuations

As mentioned before, Fast Cache-supported streaming smooths the fluctuation of network bandwidth by maintaining a growing buffer for the early-arrived data. To understand how Fast Cache utilizes the network bandwidth, we extracted the Bandwidth header in a client's PLAY command and the Speed header in this command and in the server's reply. The Bandwidth header contains the client-advertised bandwidth, which is either a default value set by the user in the media player or measured in real-time before media streaming with the "packet pair" technique. The Speed header in a client's PLAY command specifies the delivery speed requested by the client, in multiples of the media encoding rate, which is usually equivalent to the client's available bandwidth. The speed that the server agrees to offer is specified in the Speed header of the reply to the PLAY command, which is usually not greater than five times the media encoding rate. However, the actual streaming speed may be smaller than the speed claimed by the server in the PLAY reply. Thus, we computed the average actual streaming rate during each user session based on packet-level information in our workload (excluding the startup buffering).

Figure 6(a) shows the distribution of the client-advertised bandwidth, the client-requested streaming rate (i.e., the product of the Speed value and the media encoding rate), and the actual streaming rate for streaming sessions with Fast Cache support. Figure 6(b) shows the advertised bandwidth and the actual streaming rate for normal TCP streaming. Comparing Figures 6(a) and 6(b), we find that the actual streaming rate in Fast Cache-supported streaming is much closer to the client-advertised bandwidth than in normal TCP streaming. Therefore, Fast Cache exploits the unutilized network bandwidth under the constraint of TCP congestion control. In other words, for normal TCP streaming, it is possible to deliver the same media at a higher transmission rate or to deliver media with a higher encoding rate.

In Windows media streaming, the default play-out buffer size accounts for five seconds of media playback [1]. Upon network fluctuations, if the play-out buffer is empty, the client must stop to buffer data, resulting in playback jitter. With Fast Cache, the early-buffered data can afford a smoother playback for a longer period before rebuffering is necessary. We define the rebuffering ratio of a streaming session as the total time for rebuffering over the total playback duration, which reflects the streaming quality experienced by the user. Figure 7(a) shows the rebuffering ratio of sessions streamed with and without Fast Cache support (i.e., Fast Cache and normal TCP streaming in the figure). To make a fair comparison, we only consider sessions requesting video objects with an encoding rate between 200â€“400 Kbps. About 15% of normal TCP streaming sessions suffer rebuffering, while only about 8.5% of Fast Cache-supported streaming sessions do. Thus, Fast Cache can effectively eliminate rebuffering in streaming sessions. We also observe that for Fast Cache-supported streaming, there are about 1.8% of sessions with a rebuffering ratio larger than 50%, while for normal TCP streaming, there are only about 0.9% of sessions with a rebuffering ratio larger than 50%. The reason is that when rebuffering happens, normal TCP streaming may switch to a stream of lower rate if the media object is MBR encoded (see Section 6.1), thus avoiding further rebuffering. In contrast, stream switching is disabled in Fast Cache-supported streaming, leading to repeated rebuffering and a large rebuffering ratio (see Section 6.3).

Fast Cache-supported streaming also decreases the possibility of encountering network congestion by reducing the data transmission time. Figure 7(b) shows the distribution of data transmission duration for Fast Cache-supported streaming sessions and normal TCP streaming sessions, respectively. Although, in general, the media objects streamed with Fast Cache have higher file lengths and encoding rates, the transmission time of Fast Cache-supported streaming is much shorter than that of normal TCP streaming (note that the x-axis is in log scale).

#### 5.2 Fast Cache Produces Extra Traffic

Fast Cache continuously delivers a media object at a rate higher than (usually up to five times) its encoding rate. In reality, the entire media object can be delivered completely to the user in the middle of the user's playback. If the user stops in the middle, the pre-arrived data for the remaining part are wasted. Considering the well-known fact that most sessions only access the initial part of a video object [30], the extra delivered traffic by Fast Cache would be non-trivial, especially for large media objects such as movies.

In this study, we compute the over-supplied traffic as follows: the packet size and the time that a packet should be rendered can be extracted directly from the RTP packet header. Based on the timestamps of PLAY, PAUSE, and TEARDOWN commands, we get the user playback duration for each session. Assuming a default five-second play-out buffer [1] on the client side, we compute the extra traffic for each session. Figure 8 shows the extra traffic caused by Fast Cache-supported streaming and normal TCP streaming for all Windows streaming sessions in the home user workload. On average, Fast Cache over-supplies about 54.8% of media data to clients due to clients' early terminations, while the over-supplied traffic is only about 4.6% for normal TCP streaming sessions. Figure 9 shows the CDF of the average transmission speed (in multiples of the media encoding rate) of Fast Cache-supported and normal TCP streaming, respectively. In the computation of the average transmission speed, we only consider the data transmission after the Fast Start buffering period. The high data transmission speed in Fast Cache-supported streaming indicates the reason for the over-supplied data.

#### 5.3 Server Response Time of Fast Cache

In addition to over-utilizing the network bandwidth, by transmitting media at a rate much higher than its encoding rate, a streaming server running Fast Cache may also consume more CPU, memory, disk I/O, and other resources.