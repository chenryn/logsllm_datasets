50
40
30
20
10
0
1 core
2 cores
3 cores
Operation
Packet transmission
Packet modiﬁcation
Packet modiﬁcation (two cachelines)
IP checksum oﬄoading
UDP checksum oﬄoading
TCP checksum oﬄoading
Cycles/Pkt
76.0 ± 0.8
9.1 ± 1.2
15.0 ± 1.3
15.2 ± 1.2
33.1 ± 3.5
34.0 ± 3.3
64
96
160
128
192
Packet size [Byte]
224
256
Figure 3: Throughput with an XL710 40 GbE NIC
]
s
p
p
M
[
e
t
a
R
t
e
k
c
a
P
200
150
100
50
0
]
s
/
t
i
b
G
[
e
t
a
R
120
100
80
60
40
20
0
1
2
3
4
5
6
7
8
9 10 11 12
Number of CPU Cores
Figure 4: Multi-core scaling (multiple 10 GbE NICs)
5.4 Scaling to 40 Gigabit Ethernet
40 GbE NICs like the dual port Intel XL710 [15] are cur-
rently being introduced to the market. However, these ﬁrst
generation NICs come with bandwidth limitations that do
not exist on the 10 GbE NICs discussed previously: they
cannot saturate a link with minimum-sized packets [16] and
they cannot saturate both ports simultaneously regardless
of the packet size [15]. This may limit their use in some
scenarios where a large number of small packets is required,
e.g., stress-testing a router.
We are currently adding support for these NICs in Moon-
Gen and present preliminary results here. Figure 3 shows the
achieved throughput with various packet sizes and number
of 2.4 GHz CPU cores used to generate the traﬃc. Packet
sizes of 128 bytes or less cannot be generated in line rate. Us-
ing more than two CPU cores does not improve the speed,
so this is a hardware bottleneck as described by Intel [16].
The second bandwidth restriction of this NIC is the ag-
gregate bandwidth of the two ports. One obvious restriction
is the 63 Gbit/s bandwidth of the PCIe 3.0 x8 link that con-
nects the NIC to the CPU. However, the main bottleneck is
the media access control layer in the XL710 chip: it is limited
to a maximum aggregate bandwidth of 40 Gbit/s (cf. Sec-
tion 3.2.1 of the XL710 datasheet [15]). We could achieve
a maximum bandwidth of 50 Gbit/s with large packets on
both ports simultaneously and a maximum packet rate of
42 Mpps (28 Gbit/s with 64 byte frames).
5.5 Scaling to 100 Gigabit Ethernet
100 GbE is currently restricted to hardware appliances like
switches and routers and not yet available on commodity
server hardware. We can emulate higher speeds by using
multiple NICs.
We equipped one of our test servers with six dual-port
10 GbE Intel X540-T2 NICs to investigate the performance
at high rates. Figure 4 shows the achieved packet rate when
Table 1: Per-packet costs of basic operations
generating UDP packets from varying IP addresses. We used
two Intel Xeon E5-2640 v2 CPUs with a nominal clock rate
of 2 GHz for this test, but the clock rate can even be reduced
to 1.5 GHz for this packet generation task (cf. Section 5.2).
Note that sending to multiple NICs simultaneously is ar-
chitecturally the same as sending to multiple queues on a sin-
gle NIC as diﬀerent queues on a single NIC are independent
from each other (cf. Section 5.3) in an ideal well-behaved
NIC like the current generation of 10 GbE NICs. We do not
expect signiﬁcant challenges when moving to 100 GbE due to
this architecture and promising tests with multiple 10 GbE
ports. However, the ﬁrst generation of 100 GbE NICs will
likely have similar hardware restrictions as the 40 GbE NICs
discussed in Section 5.4 which need to be taken into account.
5.6 Per-Packet Costs
MoonGen’s dynamic approach to packet generation in
userscripts does not allow for a performance analysis in a
general conﬁguration as there is no typical scenario. Never-
theless, the cost of sending a packet can be decomposed into
three main components: packet IO, memory accesses, and
packet modiﬁcation logic. We devised a synthetic bench-
mark that measures the average number of CPU cycles re-
quired for various operations that are commonly found in
packet generator scripts. These measurements can be used
to estimate the hardware requirements of arbitrary packet
generator scripts. We repeated all measurements ten times;
the uncertainties given in this section are the standard de-
viations.
5.6.1 Basic Operations
Table 1 shows the average per-packet costs of basic oper-
ations for IO and memory accesses. The baseline for packet
IO consists of allocating a batch of packets and sending
them without touching their contents in the main loop. This
shows that there is a considerable per-packet cost for the IO
operation caused by the underlying DPDK framework.
Modiﬁcation operations write constants into the packets,
forcing the CPU to load them into the layer 1 cache. Addi-
tional accesses within the same cache line (64 bytes) add no
measurable additional cost. Accessing another cache line in
a larger packet is noticeable.
Oﬄoading checksums is not free (but still cheaper than
calculating them in software) because the driver needs to set
several bitﬁelds in the DMA descriptor. For UDP and TCP
oﬄoading, MoonGen also needs to calculate the IP pseudo
header checksum as this is not supported by the X540 NIC
used here [13].
5.6.2 Randomizing Packets
Sending varying packets is important to generate diﬀerent
ﬂows. There are two ways to achieve this: one can either
280Fields Cycles/Pkt (Rand) Cycles/Pkt (Counter)
27.1 ± 1.4
33.1 ± 1.3
38.1 ± 2.0
41.7 ± 1.2
32.3 ± 0.5
39.8 ± 1.0
66.0 ± 0.9
133.5 ± 0.7
1
2
4
8
Table 2: Per-packet costs of modiﬁcations
generate a random number per packet or use a counter with
wrapping arithmetic that is incremented for each packet.
The resulting value is then written into a header ﬁeld. Ta-
ble 2 shows the cost for the two approaches, the baseline
is the cost of writing a constant to a packet and sending it
(85.1 cycles/pkt).
There is a ﬁxed cost for calculating the values while the
marginal cost is relatively low: 17 cycles/pkt per random
ﬁeld and 1 cycle/pkt for wrapping counters. These results
show that wrapping counters instead of actual random num-
ber generation should be preferred if possible for the desired
traﬃc scenario.
5.6.3 Cost Estimation Example
We can use these values to predict the performance of the
scripts used for the performance evaluation in Section 5.3.
The example generated 8 random numbers for ﬁelds with a
userscript that is completely diﬀerent from the benchmark-
ing script: it writes the values into the appropriate header
ﬁelds and the payloads, the benchmarking script just ﬁlls
the raw packet from the start. The script also combines
oﬄoading and modiﬁcation; the benchmark tests them in
separate test runs.
The expected cost consists of: packet IO, packet modiﬁca-
tion, random number generation, and IP checksum oﬄoad-
ing, i.e., 229.2 ± 3.9 cycles/pkt. This translates to a pre-
dicted throughput of 10.47 ± 0.18 Mpps on a single 2.4 GHz
CPU core. The measured throughput of 10.3 Mpps is within
that range. This shows that our synthetic benchmark can
be used to estimate hardware requirements.
5.7 Effects of Packet Sizes
All tests performed in the previous sections use minimum-
sized packets. The reason for this choice is that the per-
packet costs dominate over costs incurred by large packets.
Allocating and sending larger packets without modiﬁcations
add no additional cost in MoonGen on 1 and 10 GbE NICs.
Only modifying the content on a per-packet basis adds a
performance penalty, which is comparatively low compared
to the ﬁxed cost of sending a packet. Using larger packets
also means that fewer packets have to be sent at line rate, so
the overall ﬁxed costs for packet IO are reduced: minimum-
sized packets are usually the worst-case.
Nevertheless, there are certain packet sizes that are of
interest: those that are just slightly larger than a single
cache line. We benchmarked all packet sizes between 64 and
128 bytes and found no diﬀerence in the CPU cycles required
for sending a packet. Since MoonGen also features packet
reception, we also tried to receive packets with these sizes
and found no measurable impact of the packet size.1
Rizzo notes that such packet sizes have a measurable im-
pact on packet reception, but not transmission, in his evalu-
1Note that this is not true for XL710 40 GbE NICs which
can run into hardware bottlenecks with some packet sizes.
ation of netmap [23]. He attributes this to hardware bottle-
necks as it was independent from the CPU speed. We could
not reproduce this with MoonGen. The likely explanation is
that we are using current (2014) server hardware, while the
evaluation of netmap was done in 2012 on an older system
with a CPU launched in 2009 [23].
6. HARDWARE TIMESTAMPING
Another important performance characteristic beside the
throughput is the latency of a system. Modern NICs oﬀer
hardware support for the IEEE 1588 Precision Time Proto-
col (PTP) for clock synchronization across networks. PTP
can be used either directly on top of Ethernet as a layer 3
protocol with EtherType 0x88F7 or as an application-layer
protocol on top of UDP [8].
We examined the PTP capabilities of the Intel 82580 GbE
and the 82599 and X540 10 GbE chips. They support time-
stamping of PTP Ethernet and UDP packets, the UDP port
is conﬁgurable on the 10 GbE NICs. They can be conﬁgured
to timestamp only certain types of PTP packets, identiﬁed
by the ﬁrst byte of their payload. The second byte must be
set to the PTP version number. All other PTP ﬁelds in the
packet are not required to enable timestamps and may con-
tain arbitrary values. [11, 12, 13] This allows us to measure
latencies of almost any type of packet.
Most Intel NICs,
including all 10 GbE chips, save the
timestamps for received and transmitted packets in a reg-
ister on the NIC. This register must be read back before
a new packet can be timestamped [12, 13],
limiting the
throughput of timestamped packets. Some Intel GbE chips
like the 82580 support timestamping all received packets by
prepending the timestamp to the packet buﬀer [11].
6.1 Precision and Accuracy
Timestamping mechanisms of the Intel 82599 and Intel
X540 10 GbE chips operate at 156.25 MHz when running
at 10 GbE speeds [12, 13]. This frequency is reduced to
15.625 MHz when a 1 GbE link is used, resulting in a preci-
sion of 6.4 ns for 10 GbE and 64 ns for 1 GbE. The datasheet
of the Intel 82580 GbE [11] controller lacks information
about the clock frequency. Testing shows that the acquired
timestamps are always of the form t = n·64 ns+k·8 ns where
k is a constant that varies between resets, so the precision
is 64 ns.
All of these NICs timestamp packets late in the trans-
mit path and early in the receive path to be as accurate
as possible [11, 12, 13]. We tested the timestamping func-
tionality by using loop-back multimode OM3 ﬁber cables
on an 82599-based NIC with a 10GBASE-SR SFP+ mod-
ule and Cat 5e cable between the two ports of a dual-port
X540-based NIC. Table 3 on the next page shows measured
latencies tx for diﬀerent cable lengths x for each NIC as
well as the (de-)modulation time k and propagation speed
vp, which can be calculated from these data points with the
equation t = k + l/vp. k is higher on the copper-based NIC,
this is likely due to the more complex line code required for
10GBASE-T [9]. This calculation does not take any errors
in the cable length into account; we rely on the vendor’s
speciﬁcation2. The actual propagation speed and encoding
times may therefore be outside the interval given in Table 3.
2We believe that the 50 m cable is actually slightly shorter.
281NIC
t2m [ns] t8.5m [ns] t10m [ns] t20m [ns] t50m [ns]
82599 (ﬁber)
X540 (copper)
320
2156.8
352
-
-
2195.2
403.2
-
k [ns]
vp
310.7 ± 3.9 0.72c ± 0.056c
2387.2 2147.2 ± 4.8 0.69c ± 0.019c
-
Table 3: Timestamping accuracy measurements
We repeated each measurement at least 500 000 times. All
measurements for the ﬁber connection on the 82599 NIC
yielded the same result except for the 8.5 m cable. This cable
caused a latency of 345.6 ns in 50.2% of the measurements
and 358.4 ns in the other 49.8% (Table 3 shows the average).
This variance is due to the fact that the timer that is saved
when the timestamp is taken is incremented only every two
clock cycles on the 82599 chip [12], i.e., the granularity of
the timer is 12.8 ns but the timestamping operates at 6.4 ns.
The timestamp timer on the X540 is incremented every
6.4 ns so it does not have this problem. However, it faces a
diﬀerent challenge: the 10GBASE-T standard uses a block
code on layer 1 [9] which introduces variance. Table 3 shows
the median latency. More than 99.5% of the measured values
were within ± 6.4 ns of the median. The diﬀerence between
the minimum and maximum latencies was 64 ns. These vari-
ations were independent of the cable length.
The absence of a variance on the 82599 chip demonstrates
a high precision, the plausible results for the modulation
time [28] and the linear behavior of the propagation latency
show a high accuracy.
6.2 Clock Synchronization
Test setups can involve multiple network ports that may
even be on diﬀerent NICs. For example, measuring the for-
warding latency of a switch requires timestamping a packet
on two diﬀerent ports. MoonGen therefore needs to be able
to synchronize the clocks between two network ports. This is
even necessary between two ports of a dual-port NIC, which
are completely independent from the user’s point of view.
MoonGen synchronizes the clocks of two ports by read-
ing the current time from both clocks and calculating the
diﬀerence. The clocks are then read again in the opposite
order. The resulting diﬀerences are the same if and only
if the clocks are currently synchronous (assuming that the
time required for the PCIe access is constant). We observed
randomly distributed outliers in about 5% of the reads. We
therefore repeat the measurement 7 times to have a prob-
ability of > 99.999% of at least 3 correct measurements.
The median of the measured diﬀerences is then used to ad-
just one of the clocks to synchronize them. This adjustment
must be done with an atomic read-modify-write operation.
The NICs support this as it is also required for PTP.
Tests show that this technique synchronizes the clocks
with an error of ±1 cycle. Therefore, the maximum accu-
racy for tests involving multiple network interfaces is 19.2 ns
for the 10 GbE chips.
6.3 Clock Drift
Using two diﬀerent clocks also entails the risk of clock
drifts. Drift on X540-based NICs depends on the physi-