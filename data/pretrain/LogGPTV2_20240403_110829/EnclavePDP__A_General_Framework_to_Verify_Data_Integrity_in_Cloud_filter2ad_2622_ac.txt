and md4, etc. To save the enclave memory consumption, we
trimmed the native implementation of SGX SSL by removing
those unnecessary modules from the conﬁguration ﬁle at the
compilation time. Finally the size of the trimmed SGX SSL
library decreases 26.1% (from 4.6MB to 3.4MB).
Porting PBC library. Public auditing schemes (e.g., PP-
PAS [19], SEPAP [17] and DHT-PA [18]) are all based on the
BLS signature cryptographic primitive implemented in the
PBC library [23], which is not supported by Intel SGX yet.
Therefore, we ported the PBC library into SGX to make it easy
to port other existing or develop new BLS-based schemes in
EnclavePDP. We only ported those functions required by the
public auditing schemes into SGX to provide a lightweight
PBC library, thus reducing the memory consumption of En-
clavePDP. Note that some of those functions need a bit tuning.
For instance, generating random numbers is a quite frequent
operation for most PDP schemes, the PBC library generates
random numbers using the /dev/urandom pseudo ﬁle on Linux
platform. However, code running in the enclave cannot per-
form I/O operations directly. Hence, we use Intel RDRAND
instruction [41] when porting the random number generation
function in the PBC library.
5.2 Protecting Enclave Binary Integrity
The implementation of the PDP schemes inside the enclave
is essentially an executable binary running on the untrusted
cloud platform. Hence, the adversaries may reverse-engineer
the binary enclave shared object to extract the code logic. We
utilized Intel SGX PCL technique [42] to encrypt the enclave
shared object (.so) at build time and decrypt it at enclave load
time. Moreover, the untrusted cloud providers may create a
fake enclave to perform ECDH [38] protocol with the data
3In particular, when generating tags for the original data, SEPAP will
create a doubly linked info table (DLIT), while DHT-PA scheme will create
a dynamic hash table (DHT).
200    23rd International Symposium on Research in Attacks, Intrusions and Defenses
USENIX Association
owner to steal the private keys. To defeat such threat, the data
owner periodically requests enclave to return its enclave mea-
surement (constructed by invoking the EREPORT instruction,
which can only be executed inside the enclave), and compares
it with local backup measurement. The successive operations
can only be continued upon a match of the measurements.
Note that malicious cloud providers may create a copy of
EnclavePDP and execute this copy, but they cannot reveal any
secret data inside the enclave. The copy of EnclavePDP may
cause DoS attack, which is out of scope of this work.
5.3
Integration with Cloud Storage Service
In order to deploy EnclavePDP on existing cloud storage
services easily, we exposed high-level interfaces (e.g., TCP
sockets) for users or cloud storage services to submit/return
PDP requests/responses. 4
We deployed the prototype of EnclavePDP on FastDFS
[24], an open source high performance distributed ﬁle sys-
tem (DFS). FastDFS has two major functionalities: tracker
and storage. The former conducts scheduling and load bal-
ancing for ﬁle access. The latter performs ﬁle management
including: ﬁle storing, ﬁle syncing, providing ﬁle access in-
terface. We extended the fastdfs-nginx-module of FastDFS
for user to easily submit integrity veriﬁcation requests, e.g.,
issuing a http get request. When receiving requests submitted
by users, the fastdfs-nginx-module forwards the requests to
EnclavePDP (runs as a daemon on the storage servers) and
waits for the veriﬁcation result returned by EnclavePDP. The
implementation of integrating EnclavePDP with FastDFS is
less than 300 lines of C code. Note that the Prover runs on
the storage server of FastDFS, so it can access the outsourced
data directly and generate proofs on behalf of FastDFS. As
for the closed source cloud storage services (e.g., Amazon
S3), EnclavePDP can only invoke the public APIs exposed
by those cloud storage services to access the outsourced data.
Current implementation of EnclavePDP supports the integrity
check of the data stored on Amazon S3 using AWS C++ SDK,
with around 70 lines of C++ code added into EnclavePDP
and without any changes to Amazon S3 platform. However,
EnclavePDP needs to download all the data to local disk and
performs veriﬁcation, because Amazon S3 does not support
random access to different data blocks.
4The cloud storage service needs to: (1) allow users to submit PDP re-
quests and forward the PDP requests to EnclavePDP; (2) allow the Prover
process to access the outsourced data directly, or provide APIs for the Prover
to access the data indirectly. Recall that the Prover is a non-SGX applica-
tion designed to generate proofs on behalf of the cloud storage services,
which makes it possible to integrate EnclavePDP with existing cloud storage
services with as few changes as possible.
6 Evaluation
6.1 Experimental Setup
We deployed EnclavePDP and FastDFS on Microsoft Azure
Conﬁdential Computing (ACC) [43] VMs supporting Intel
SGX. Each VM runs Ubuntu 16.04.1 LTS with kernel version
4.15.0-1036 on a platform with an Intel(R) Xeon(R) E-2176G
CPU (4 cores, 3.70 GHz, and 12 MB cache) and 16 GB RAM.
We ran FastDFS v5.12 on four VMs, one VM as the tracker
server and the others as storage servers. The tracker server
takes charge of scheduling and load balancing for ﬁle access,
and is also extended to dispatch PDP requests to other storage
servers. In particular, FastDFS utilizes its Nginx module (i.e.,
fastdfs-nginx-module that is built on nginx-1.15.4) to interact
with the user, thus we extend this module to handle the PDP
requests submitted by the user. EnclavePDP runs as a daemon
on the storage servers. When the tracker server receives PDP
request, it dispatches the PDP request to the EnclavePDP
running on the corresponding storage servers. To evaluate
the throughout of EnclavePDP when handling concurrent
requests, we used a popular workload testing tool, Apache
JMeter, to simultaneously issue integrity veriﬁcation requests
to EnclavePDP at different speed (requests/second). Apache
JMeter runs on a local computer with Ubuntu 16.04.1 LTS
equipped with Intel(R) Core(TM) i7-7700HQ CPU.
6.2 Analysis of TCB
We measured the change of the TCB code base after port-
ing the Challenge and Verify operations into Intel SGX, as
shown in Table 2. We only focus on the core part of the
implementation of those PDP schemes when measuring the
SLOC (Source line of code), and ignore other code like I/O
operations, sockets, etc. All the PDP schemes include Chal-
lenge and Verify operations, which are two security-sensitive
functions. To guarantee the conﬁdentiality of private keys
used to generate challenges or verify proofs, loading pri-
vate keys into enclave is the third security-sensitive func-
tion. For DPDP and FlexDPDP, there is an extra veriﬁcation
against the integrity of the Rank-based Authenticated SkipList
and FlexList respectively. Therefore, there exists the fourth
security-sensitive function for those two schemes. Accord-
ingly, each security-sensitive function is associated with an
ecall interface. Hence, each PDP request will conduct three
or four ecall crossings (i.e., traps into enclave) depending
on the speciﬁc schemes.
As in Table 2, the security-sensitive SLOC of native PDP
varies from 7% to 33%, while the security-sensitive SLOC
after porting them into enclave varies from 8% to 36%.
Take APDP as an example. Its native implementation totally
contains 1348 SLOC, among which 300 SLOC is security-
sensitive (account for 22% of the total). After porting it into
enclave, the security-sensitive SLOC increases to 350 SLOC
USENIX Association
23rd International Symposium on Research in Attacks, Intrusions and Defenses    201
Schemes
MACPDP
APDP [9]
MRPDP [11]
SEPDP [10]
CPOR [12]
DPDP [7]
3
3
3
3
3
4
4
3
3
3
1483
1348
1440
1259
1057
950
945
1012
620
720
Table 2: TCB size of EnclavePDP
Security
-sensitive
functions
SLOC
Security
-sensitive
SLOC
115 (7%)
300 (22%)
476 (33%)
106 (8%)
167 (15%)
117 (12%)
139 (14%)
199 (20%)
162 (26%)
187 (26%)
SGX
-enabled
SLOC
121 ( 8%)
350 (25%)
624 (43%)
153 (12%)
210 (19%)
145 (15%)
158 (16%)
249 (24%)
225 (36%)
255 (35%)
(25% of the total). Such increase mainly results from ex-
tra functionalities, such as private key loading, challenges
backup/destroy, decrypting other security-sensitive data (e.g.,
doubly linked info table, dynamic hash table), etc. Addition-
ally, we also quantitatively measured the SLOC of those
three enclave-supported libraries: Intel SGX SSL library con-
tains about 138.4K SLOC; Intel SGX GMP contains 163.4K
SLOC; PBC library contains 29.9K SLOC.
FlexDPDP [8]
PPPAS [19]
SEPAP [17]
DHT-PA [18]
6.3 Evaluation of Challenger and Veriﬁer
Given the amount of data outsourced on the cloud, it is in-
advisable to challenge all data blocks at once to verify the
integrity. Instead, the sampling veriﬁcation is used by most
PDP schemes, that is, to achieve high-accuracy veriﬁcation
by only checking a portion of the data at once. In particu-
lar, [9, 11, 18, 19] demonstrated that if t fraction of data is
corrupted, randomly sampling c blocks will detect such cor-
ruption with the probability P = 1− (1−t)c. When t = 1%,
the veriﬁer only needs to verify 460 randomly chosen blocks
to detect such corruption with the probability larger than 99%.
Hence, in all the following experiments, we choose 460 as the
maximum number of challenge blocks5, even for large ﬁles
with much more ﬁle blocks. When the number of the total
ﬁle blocks is less than 460, the veriﬁer challenges all the ﬁle
blocks instead. We measured the performance of performing
the Challenge and Verify operations inside the enclave, and
compared it with the native implementation below. Note that
the time involved in sending challenges/proofs and reading
data is ignored for both of the two cases.
6.3.1 Overhead of Challenge Operation
Figure 3 depicts the time (in µs) of generating challenges for
both enclave-enabled and native implementation with varying
ﬁle sizes. For all the 10 PDP schemes, both enclave and native
implementation demonstrate similar changes over different
ﬁle sizes. The challenge operation time of APDP, MRPDP and
SEPDP is relatively constant regardless of ﬁle size, because
5The block size is 16KB for APDP, and 4KB for other PDP schemes.
their challenge operations just produce a random seed used
to generate the random block set to be veriﬁed, which is
independent of the ﬁle size. For the other seven PDP schemes,
as the ﬁle size increases, the challenge operation time ﬁrst
increases and then becomes constant. This is because those
schemes generate a random n-element set for the challenge,
whose size increases as the ﬁle size increases. It reaches
a constant (i.e., the maximum number of challenge blocks)
when the number of ﬁle blocks exceeds the maximum number
of challenge blocks (460 as described above).
Comparing with the native PDP schemes, APDP and MR-
PDP saw an increase of 18.2% and 18.1% of the challenge
operation time respectively. MAC-PDP, DPDP, FlexDPDP
and CPOR incurred 62%, 50%, 41.5% and 180% overhead
when their challenge operation time reaches a constant. The
three BLS-based schemes, i.e., PPPAS, DHT-PA, SEPAP, im-
posed similar overhead, 89.7%, 85.3% and 84.3% respectively.
The challenge operation time of SEPDP increased nearly 1.9
times. Actually the difference of overhead results from the
challenge operation time of each PDP scheme. For instance,
the challenge operation for native SEPDP is below 4 µs for
varying ﬁle sizes, which magniﬁes the impact of ecall over-
head, thus causing nearly 1.9 times overhead. In contrast, the
challenge operation for native APDP is from 250 µs to 300 µs
for varying ﬁle sizes, thus causing merely 18.2% overhead.
Observation 1. The overhead of the challenge operation
is not proportional to the security-sensitive SLOC. PDP
schemes in the same category introduce similar over-
head. Enclave-enabled challenge operation time is still
in the scale of microsecond (µs), which should have little
impact on practical applications.
6.3.2 Overhead of Verify Operation
Figure 4 depicts the time of executing the verify operation for
both enclave-enabled and native implementation with varying
ﬁle sizes. The verify operation time of native PDP schemes
varies signiﬁcantly. In particular, the verify operation time of
SEPDP, MAC-PDP and CPOR is in the scale of microsecond
(µs), but in the scale of millisecond (ms) for APDP, MRPDP,
DPDP and FlexDPDP (FDPDP). For the other three BLS-
based schemes (PPPAS, DHT-PA and SEPAP), their verify
operation time is in the scale of second (s).
Observation 2. RSA-based schemes (ms) are an order
of magnitude slower than symmetric-based schemes
(µs), because the RSA-based modular exponential op-
erations are complicated and expensive. BLS-based
schemes (s) is another order of magnitude slower, prob-
ably due to the inherent drawback of the complicated
and slow computation of BLS signatures (e.g., curves
pairing) [44].
Regarding enclave-enabled implementation of PDP
schemes, executing the verify operation inside the enclave im-
posed 17.1%, 12.7% and 24.7% overhead for APDP, MRPDP
202    23rd International Symposium on Research in Attacks, Intrusions and Defenses
USENIX Association
Figure 3: Overhead of Challenge Operations
and MAC-PDP, respectively. The three BLS-based schemes
(PPPAS, DHT-PA and SEPAP) saw similar performance
degradation, i.e., 34.9% for PPPAS, 36.5% for DHT-PA, and
35.2% for SEPAP, respectively. DPDP and FlexDPDP intro-
duced 47% and 37% overhead respectively, while SEPDP
and CPOR experienced 82.0% and 92.2% increase of the
verify operation time respectively. The reason for such over-
head is similar to that of the overhead of the challenge time