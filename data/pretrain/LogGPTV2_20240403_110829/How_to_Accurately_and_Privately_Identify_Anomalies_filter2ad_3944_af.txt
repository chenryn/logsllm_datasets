and instantiate our framework for other anomaly detection models.
ACKNOWLEDGMENTS
Research reported in this publication was supported by the National
Science Foundation under awards CNS-1422501, CNS-1564034, CNS-
1624503, CNS-1747728 and the National Institutes of Health under
award R01GM118574. The content is solely the responsibility of the
authors and does not necessarily represent the official views of the
agencies funding the research.
REFERENCES
[1] Charu C Aggarwal. 2015. Outlier analysis. In Data mining. Springer, 237–263.
Session 3E: Privacy IICCS ’19, November 11–15, 2019, London, United Kingdom730[2] Miguel E Andrés, Nicolás E Bordenabe, Konstantinos Chatzikokolakis, and Catus-
cia Palamidessi. 2013. Geo-indistinguishability: Differential privacy for location-
based systems. In Proceedings of the 2013 ACM SIGSAC conference on Computer &
communications security. ACM, 901–914.
[3] Vic Barnett and Toby Lewis. 2000. Outliers in statistical data. Wiley.
[4] Daniel M Bittner, Anand D Sarwate, and Rebecca N Wright. 2018. Using Noisy
Binary Search for Differentially Private Anomaly Detection. In International
Symposium on Cyber Security Cryptography and Machine Learning. Springer,
20–37.
[5] Martin Bobrow. 2013. Balancing privacy with public benefit. Nature News 500,
7461 (2013), 123.
[6] Jonas Böhler, Daniel Bernau, and Florian Kerschbaum. 2017. Privacy-preserving
outlier detection for data streams. In IFIP Annual Conference on Data and Appli-
cations Security and Privacy. Springer, 225–238.
[7] Centers for Medicare & Medicaid Services. 1996.
ance Portability and Accountability Act of 1996 (HIPAA).
http://www.cms.hhs.gov/hipaa/.
The Health Insur-
Online at
[8] Varun Chandola, Arindam Banerjee, and Vipin Kumar. 2009. Anomaly detection:
A survey. ACM computing surveys (CSUR) 41, 3 (2009), 15.
[9] Ronald Cramer, I. B. DamgÃěrd, and Jesper Buus Nielsen. 2015. Secure multiparty
computation: an information-theoretic approach. Cambridge University Press.
[10] Andrea Dal Pozzolo, Olivier Caelen, Reid A Johnson, and Gianluca Bontempi.
2015. Calibrating probability with undersampling for unbalanced classification.
In Computational Intelligence, 2015 IEEE Symposium Series on. IEEE, 159–166.
[11] Alison M Darcy, Alan K Louie, and Laura Weiss Roberts. 2016. Machine learning
and the profession of medicine. Jama 315, 6 (2016), 551–552.
[12] Yihe Dong, Samuel B Hopkins, and Jerry Li. 2019. Quantum Entropy Scoring for
Fast Robust Mean Estimation and Improved Outlier Detection. arXiv preprint
arXiv:1906.11366 (2019).
[13] Stelios Doudalis, Ios Kotsogiannis, Samuel Haney, Ashwin Machanavajjhala,
and Sharad Mehrotra. 2017. One-sided differential privacy. arXiv preprint
arXiv:1712.05888 (2017).
[14] Dheeru Dua and Casey Graff. 2017. UCI Machine Learning Repository. http:
//archive.ics.uci.edu/ml
[15] Cynthia Dwork. 2006. Differential Privacy. In Automata, Languages and Pro-
gramming, Michele Bugliesi, Bart Preneel, Vladimiro Sassone, and Ingo Wegener
(Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 1–12.
[16] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard
Zemel. 2012. Fairness through awareness. In Proceedings of the 3rd innovations in
theoretical computer science conference. ACM, 214–226.
[17] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. Calibrat-
ing noise to sensitivity in private data analysis. In TCC. Springer, 265–284.
[18] Cynthia Dwork, Aaron Roth, et al. 2014. The algorithmic foundations of differ-
ential privacy. Foundations and Trends® in Theoretical Computer Science 9, 3–4
(2014), 211–407.
[19] Cynthia Dwork, Adam Smith, Thomas Steinke, Jonathan Ullman, and Salil Vadhan.
2015. Robust traceability from trace amounts. In Foundations of Computer Science
(FOCS), 2015 IEEE 56th Annual Symposium on. IEEE, 650–669.
[20] Yaniv Erlich and Arvind Narayanan. 2014. Routes for breaching and protecting
genetic privacy. Nature Reviews Genetics 15, 6 (2014), 409.
[21] 2016. Regulation (EU) 2016/679 of the European Parliament and of the Council of
27 April 2016 on the protection of natural persons with regard to the processing
of personal data and on the free movement of such data, and repealing Directive
95/46/EC (General Data Protection Regulation). Official Journal of the European
Union L119 (4 May 2016), 1–88. http://eur-lex.europa.eu/legal-content/EN/TXT/
?uri=OJ:L:2016:119:TOC
[22] David Freedman, Robert Pisani, and Roger Purves. 1998. Statistics. W.W. Norton.
[23] Machine Learning Group. 2018. Credit Card Fraud Detection. https://www.
kaggle.com/mlg-ulb/creditcardfraud/home.
[24] Melissa Gymrek, Amy L McGuire, David Golan, Eran Halperin, and Yaniv Erlich.
2013. Identifying personal genomes by surname inference. Science 339, 6117
(2013), 321–324.
[25] Xi He, Ashwin Machanavajjhala, and Bolin Ding. 2014. Blowfish privacy: Tuning
privacy-utility trade-offs using policies. In Proceedings of the 2014 ACM SIGMOD.
ACM, 1447–1458.
[26] Nils Homer, Szabolcs Szelinger, Margot Redman, David Duggan, Waibhav Tembe,
Jill Muehling, John V Pearson, Dietrich A Stephan, Stanley F Nelson, and David W
Craig. 2008. Resolving individuals contributing trace amounts of DNA to highly
complex mixtures using high-density SNP genotyping microarrays. PLoS genetics
4, 8 (2008), e1000167.
[27] Marcello Ienca, Pim Haselager, and Ezekiel J Emanuel. 2018. Brain leaks and
consumer neurotechnology. Nature biotechnology 36, 9 (2018), 805–810.
[28] Ian Jolliffe. 2011. Principal component analysis. In International encyclopedia of
statistical science. Springer, 1094–1096.
[29] Zach Jorgensen, Ting Yu, and Graham Cormode. 2015. Conservative or liberal?
personalized differential privacy. In 2015 IEEE 31st International Conference on
Data Engineering (ICDE). IEEE, 1023–1034.
[30] Seppo Karrila, Julian Hock Ean Lee, and Greg Tucker-Kellogg. 2011. A comparison
of methods for data-driven cancer outlier discovery, and an application scheme
to semisupervised predictive biomarker discovery. Cancer informatics 10 (2011),
CIN–S6868.
[31] Michael Kearns, Aaron Roth, Zhiwei Steven Wu, and Grigory Yaroslavtsev. 2016.
Private algorithms for the protected in social network search. Proceedings of the
National Academy of Sciences 113, 4 (2016), 913–918.
[32] Daniel Kifer and Bing-Rong Lin. 2012. An axiomatic view of statistical privacy
and utility. Journal of Privacy and Confidentiality 4, 1 (2012), 5–49.
[33] Daniel Kifer and Ashwin Machanavajjhala. 2014. Pufferfish: A framework for
mathematical privacy definitions. ACM Transactions on Database Systems (TODS)
39, 1 (2014), 3.
[34] Edwin M Knorr and Raymond T Ng. 1997. A Unified Notion of Outliers: Properties
and Computation.. In KDD, Vol. 97. 219–222.
[35] Edwin M Knorr and Raymond T Ng. 1998. Algorithms for mining distancebased
outliers in large datasets. In Proceedings of the 1998 VLDB. Citeseer, 392–403.
[36] Edward Lui and Rafael Pass. 2015. Outlier privacy. In TCC. Springer, 277–305.
[37] D Luquetti, P Claes, DK Liberton, K Daniels, KM Rosana, EE Quillen, LN Pearson,
B McEvoy, M Bauchet, AA Zaidi, et al. 2014. Modeling 3D Facial Shape from
DNA. PLoS Genetics 10, 3 (2014), e1004224.
[38] Ye Nan, Kian Ming Chai, Wee Sun Lee, and Hai Leong Chieu. 2012. Optimizing
F-measure: A Tale of Two Approaches. Proceedings of the 29th International
Conference on Machine Learning, ICML 2012 1 (06 2012).
[39] Ziad Obermeyer and Ezekiel J Emanuel. 2016. Predicting the futureâĂŤbig data,
machine learning, and clinical medicine. The New England journal of medicine
375, 13 (2016), 1216.
[40] Soumi Ray, Dustin S McEvoy, Skye Aaron, Thu-Trang Hickman, and Adam
Wright. 2018. Using statistical anomaly detection models to find clinical decision
support malfunctions. Journal of the American Medical Informatics Association
(2018).
[41] Shebuti Rayana. 2016. ODDS Library. http://odds.cs.stonybrook.edu Available
at http://odds.cs.stonybrook.edu.
[42] Gordon D Schiff, Lynn A Volk, Mayya Volodarskaya, Deborah H Williams, Lake
Walsh, Sara G Myers, David W Bates, and Ronen Rozenblum. 2017. Screening
for medication errors using an outlier detection system. Journal of the American
Medical Informatics Association 24, 2 (2017), 281–287.
A APPENDIX
A.1 Empirical evaluation protocols
σ
(cid:102)
2 0
0 2
(cid:113)
2
1 + σ
Evaluation over normally distributed data: If the data is from
one dimensional normal distribution with mean µ and standard
deviation σ then a record i is anomalous (or equivalently an outlier)
if |i−µ| ≥ 3σ, and is statistically equivalent to (β = 1.2×10−3
n, r =
0.13σ )-anomaly [35], where n is the size of the database.
(cid:103)
To adapt this result for 2D normal distribution in Figure 2, set
2
r = 0.13
2 and compute β in a similar fashion as above. Next,
take 30 samples of size 20K, i.e. n = 20, 000, from the 2D normal
distribution, N (µ, Σ), where µ = (0, 0) and Σ =
, and run
SP-mechanism (given in Section 4.1.2) and DP-mechanism (given
in Section 4.1.1) for (β, r )-anomaly identification query to compute
accuracy, which is measured by the probability of outputting the
correct answer by the private mechanism, and average the results
over the samples for each query. We then plot the average accuracy
and interpolate the results using one-degree polynomial in the two
coordinates (Figure 2b-c). We used the “ListPlot3D” function of
Mathematica with the argument “InterpolationOrder” set to 1.
In Figure 2d-e and Figure 3, we plot the level of privacy (in term of
ε) that each record (point) has under private anomaly identification
query. Here, the level of privacy for a record in a given database is
measured by the maximum divergence divergence in the probability
of outputting a label when we add or remove the record from the
database. For ε-SP-mechanism, U , to compute the value of the
privacy parameter, ε, for a record i in a given database x, consider
databases y and z. y and z are same as x except for y has one more
record of value i and z has one less record of value i—if there is
Session 3E: Privacy IICCS ’19, November 11–15, 2019, London, United Kingdom731no record of value i in x then z will be the same as x. Now we can
calculate eε for record i be by (4).
(cid:32) P(U (x ) = b)
P(U (w ) = b)
(cid:33)
eε = max
w ∈{y,z}
max
b∈{0,1}
P(U (w ) = b)
P(U (x ) = b)
,
(4)
A.2 Protocol for (β, r ) selection
The main idea is to fix a value of β, for a dataset of size n, as
(1 − p) × n, where p is close to 1, and then search for an appropriate
value of r. It is recommended [35] that for the datasets of sizes 103
and 106, β be (1− 0.995) × 103 and (1− 0.99995) × 106. By assuming
that p is linearly related to n, one can use the provided values to find
the value of β for any given dataset. For a fixed value of β, a search
is performed to find r that maximize the F1-score (also known as
balanced F-measure), which is a popular performance metric for
imbalanced datasets [38], and it is the harmonic mean of precision
and recall. We used the following protocol to select the value of r.
Initialize rmin = .001, rmax = 40 (or the value that is not smaller than
the maximum distance between any two points in the given dataset),
r = 0, and S = 0. Next, set r1 = rmin + (rmax − rmin)/4, r2 =
rmin + 3(rmax − rmin)/4, pick α from [0, 1] uniformly at random
and set r3 = α r1 + (1 − α ) r2. Compute F1-score for each of the r′s,
i.e. Sr1 , Sr2, and Sr3. Let Srt
be the maximum of the computed scores.
If Srt
and r = rt ; further, if Sr2  r then set rmax = r2 but if it is not the case and Sr1  0, f
: D → {0, 1}, ε-differentially
private mechanism M : D → {0, 1}, and x, y ∈ D such that f (x ) (cid:44)
f (y) and ||x − y||1 = 1; and let b = f (x ).
If P (M (y) = b) ≤ 1/(1 + eε ) then, by differential privacy con-
straints, we get that P (M (x ) = b) ≤ eε /(1+eε ); thus P (M (x ) = 1 − b) ≥
1/(1+eε ). Similarly, P (M (x ) = 1 − b) ≤ 1/(1+eε ) implies P (M (y) = b) ≥
1/(1 + eε ). Hence, from the above, it follows that
max (P(M (x ) (cid:44) f (x )), P(M (y) (cid:44) f (y))) ≥ 1/(1 + eε ).
Since M, x and y were fixed arbitrarily, the claim follows, and this
completes the proof.
□
A.4 Proof of Theorem 4.1
Proof. Fix arbitrary ε > 0 and a definition of anomaly. Let д
be the anomaly identification function and GS be the k-sensitive
neighborhood graph corresponding to it for an arbitrary value
of k ≥ 1. Fix λ to be 1-Lipschitz continuous lower bound on the
, for д such that λ ≥ 1. Let Uλ be as given by
mdd-function, ∆GS
Construction 1. Next, fix an anomaly identification query, (i, д), and
x, y ∈ D that are neighbors (i.e. connected by a direct edge) in GS .
If д(i, x ) = д(i, y) = b from some b ∈ {0, 1} then
P (Uλ (x ) (cid:44) b)
P (Uλ (y) (cid:44) b)
= eε (−λ(i,x )+λ(i,y ))
≤ eε |−λ(i,x )+λ(i,y )|
≤ eε
and
(since λ is 1-Lipschitz continuous)
P (Uλ (x ) = b)
P (Uλ (y) = b)
=
1 − P (Uλ (x ) (cid:44) b)
1 − P (Uλ (y) (cid:44) b)
1 + eε − e−ε (λ(i,x )−1)
1 + eε − e−ε (λ(i,y )−1)
= eελ(i,y ) (1 + eε ) − eε (λ(i,y )−λ(i,x )+1)
=
eελ(i,y ) (1 + eε ) − eε
since λ is 1-Lipschitz continuous, it follows that
P (Uλ (x ) = b)
P (Uλ (y) = b)
The first inequality holds because λ is 1-Lipschitz continuous,
≤ eελ(i,y ) (1 + eε ) − 1