next 4 events, selected as per our criteria described in Section 4.2.
In the dual mode case, we oscillate between the best 4 events and
the next 4 events. The evaluation of these strategies in vulnerability
discovery experiments is presented next.
5 EVALUATION
To assess the soundness of our approach, we first perform an evalu-
ation on downstream versions of programs where ground truth (i.e.,
vulnerabilities and patches) exists. Arguably, there is also practical
relevance in fuzzing downstream versions of a program as doing
so allows an analyst to investigate how long bugs of a particular
type may have persisted. Additionally, practitioners may use such
knowledge to direct their vulnerability discovery processes to hunt
for bugs in prior versions where exploiting those bug may be easier.
Experimental Setup: All experiments were conducted in a virtual-
ized environment running on a server consisting of Intel(R) Xeon(R)
CPU E5-2630 v4 @ 2.20GHz processor with 20 cores and 128 GB
memory. Each virtual machine runs on a VMware hypervisor (that
supports HPC monitoring), and is configured with 1 core (2 threads)
and 8 GB of memory. Each fuzzer instance runs on a separate VM
(to avoid hogging of resources by a particular instance), and each
experimental run is conducted for a period of 24 hours. To reduce
the impacts of randomness in the fuzzing process, we repeat each
experiment 6 times [23, 28]. Our benchmark consists of utilities
from the real-world libraries shown in Table 4.
5.1 On Extensibility
To decide which fuzzers to apply our framework to, we initially
selected 4 fuzzers based on source code availability and findings
from a recent fuzzer benchmarking report [45] that ranks fuzzers
based on the medians of reached coverage on different benchmarks.
Table 4 presents the results of these fuzzers in terms of bug discovery
and their consistency over repeated runs. For some programs (e.g.,
libjpeg), we found multiple instances of the same bug that roots to
different code locations (shown in parentheses). Finding multiple
occurrence of the same bug also adds to the efficacy of a fuzzer,
therefore, we classify them as independent bugs although they were
documented using same CVE. The results show that AFL, MOpt
and Fairfuzz fall on the higher end of the spectrum in terms of
consistency and finding more bugs, while Angora falls on the lower
end. In addition, following the recommendation of Klees et al. [23],
we note that recent works address the randomness in the fuzzing
by conducting multiple runs and then present an aggregated result.
But, from a practitioner’s point of view, a true measure of the quality
of a fuzzer is whether it can consistently find a particular bug in
repeated runs. Along those lines, we also report the consistency of
an approach to find a specific bug over multiple runs.
Based on Google’s evaluation using the critical distance met-
ric [45], AFL, MOpt and Fairfuzz performed well on their fuzz
benchmarks. We decided to apply our framework to AFL as it is
by far the most popular fuzzer. MOpt targets a different point in
the fuzzing space in that it offers a novel mutational scheduling
scheme to better enable mutational-fuzzers to discover vulnerabili-
ties. For that reason, we also selected MOpt as a candidate to apply
our framework to. Lastly, we also extended Fairfuzz for a different
point in the space. We chose Fairfuzz because it uses a unique seed
selection approach for guiding the fuzzer toward rarely executed
paths.
5.2 On Expediency
One way to measure whether OmniFuzz finds potential bugs faster
is to compare the time-to-crash ratio for the crashes that are com-
mon between the approaches. That metric has been used else-
where [39, 54] as, at first blush, it appears to be a decent measure of
the efficiency of a fuzzer in reaching bugs. If we apply that yardstick,
the results show that we can find crashes much faster than the base
approach: 1200X faster in the tiff benchmark using strategy 1-a,
70x faster in yaml using strategy 2-a, and 40X faster in libjpeg using
strategy 2-a. While impressive, the time-to-crash ratio is somewhat
misleading because (from a practitioner’s standpoint) the difference
between finding crashes in seconds versus minutes may not be
that suasive. Therefore, we take a slightly different approach and
report the speedup relative to a predefined granularity. E.g., if the
granularity is 15 minutes, and a crash is obtained in 30 seconds by
one fuzzer and in 14 minutes by the other, we consider the speedup
as 1.
Figures 6a-6c show the speedup for each unique crash found
on a per run basis for AFL, MOpt, and Fairfuzz respectively. For
a more detailed look, we refer the reader to Tables 9-11 in the
Appendix. In our analyses, the uniqueness of a crash is measured
by the stack trace approach [23]. A larger ratio means that our
ACSAC 2020, December 7–11, 2020, Austin, USA
Sanjeev Das, Kedrian James, Jan Werner, Manos Antonakakis, Michalis Polychronakis, and Fabian Monrose
Table 4: Consistency in finding bugs (over 6 runs)
approach finds the bug in less time. We apply the pairwise two-
tailed Mann-Whitney U approach to test for statistical significance.
The results show that for most of the programs, the time-to-crash
data obtained using our approach are statistically different from the
base fuzzer. Specifically, we induce crashes 3.5X faster in libjpeg, 7X
faster in tiff, and 2X faster in yaml and libarchive when AFL is the
base fuzzer. We perform worse in the case of libxml2, although there
are only a handful of crashes for that benchmark. With MOpt as
the base fuzzer, we improve the time needed to find unique crashes
by 1.6X in libjpeg, 3X in tiff, 2.3X in yaml, 1.1X in libarchive. The
dual mode (strategies 3-a, 3-b) outperforms the single mode in the
majority of cases. Finally, in the case of Fairfuzz, our approach
improves the time to finding unique crashes by 3.0X in libarchive
and libxml2, 2.3X in tiff and 3.4X in yaml.
That said, our approach performs poorly in some cases. Our
painstaking analysis showed that the benefits of our extension
are undermined under certain circumstances. First, if there are
only a few initial seed inputs and those seeds are not selected by
our heuristic, then the fuzzer’s progress slows down. For instance,
libarchive, tiff and libxml2 have only one seed input. With our
strategies such as 1-a and 3-a, the process gets stuck waiting for
the mutation to generate entirely different inputs that can stir the
fuzzing exploration in a new direction. This acts as a chokepoint
and impedes progress. This limitation can be addressed by starting
off with large and diverse initial seeds (e.g., as was the case with
yaml).
Second, we found that the full benefits offered by our extension
may not be realized when composed with certain baseline fuzzers.
ACSAC2020,December7–11,2020,Austin,USASanjeevDas,KedrianJames,JanWerner,ManosAntonakakis,MichalisPolychronakis,andFabianMonroseTable4:Consistencyinfindingbugs(over6runs)ProgramsCVE/BugsBug-descriptionBugIDAFLMoptAngoraFairfuzzlibarchive(v3.0.3)CVE-2015-8932Heap-buffer-overflow16✓○○IDnotassignedHeap-buffer-overflow26✓○○CVE-2015-8928Heap-buffer-overflow32○○○CVE-2015-8923Outofboundsread40✓✓✓IDnotassignedOutofboundsread50✓✓✓libjpeg(v1.3.0)CVE-2018-11212Divide-by-zero65○○✓CVE-2018-11213Heap-buffer-overflow(1)75○○○Heap-buffer-overflow(2)83○○○Heap-buffer-overflow(3)93○○○Heap-buffer-overflow(4)103○○○Heap-buffer-overflow(5)112○○○CVE-2018-11214Heap-buffer-overflow(1)125○○○Heap-buffer-overflow(2)135✓○○Heap-buffer-overflow(3)140○○✓libplist(v1.9)CVE-2017-5209Typeconversion156✓Ø✓IDnotassignedOutofboundsread166✓Ø✓libpng(v1.0.69)IDnotassignedNullPointerDereference173○○○libxml2(v2.8.0)CVE-2017-9049/9050Heap-buffer-overflow181○Ø○CVE-2016-1835Heapuse-after-free190✓Ø✓CVE-2015-7497Heap-buffer-overflow200✓Ø○CVE-2015-7498Heap-buffer-overflow214○Ø✓pcre(v8.38)Bug#1783Outofboundsread223○○○CVE-2017-11164Stackoverflow230○○✓tiff(v3.7.0)CVE-2016-5102Heap-buffer-overflow245○○○CVE-2016-3186Heap-buffer-overflow256○○✓CVE-2013-4244Heap-buffer-overflow265○○○CVE-2013-4231Heap-buffer-overflow275○○○IDnotassignedHeap-buffer-overflow285○○○yaml(v0.3.0)IDnotassignedLogicError296✓Ø✓Numbersinsideparenthesesdenotethedistinctinstancesofbugswithdifferentrootcauses,butaddressedbyasingleCVE.BenchmarksthatAngorafailedtorunonarelistedasØ.ThenumberoftimesabugisfoundbyafuzzerisreferencedwithrespecttoAFLandshownas✓todenotethesameasAFL,○morethanAFL,and○fewerthanAFL.libarchive,tiffandlibxml2haveonlyoneseedinput.Withourstrategiessuchas1-aand3-a,theprocessgetsstuckwaitingforthemutationtogenerateentirelydifferentinputsthatcanstirthefuzzingexplorationinanewdirection.Thisactsasachokepointandimpedesprogress.Thislimitationcanbeaddressedbystartingoffwithlargeanddiverseinitialseeds(e.g.,aswasthecasewithyaml).Second,wefoundthatthefullbenefitsofferedbyourextensionmaynotberealizedwhencomposedwithcertainbaselinefuzzers.Thisisespeciallytruewhentheircoreguidingtechnique(s)coun-teractsthatofourHPCbasedmodels.Forexample,inthecaseofFairfuzz,wefoundthatstrategies1-aand3-adonotworkwellbe-causeofFairfuzz’sconservativepolicy,whichheavilytrimsinputsbasedentirelyonrarebranches.GiventhatourapproachleveragesmorecontextualcoverageinformationobtainedthroughHPC,therestrictionsimposedbyFairfuzzundercutsthesepotentialbenefits.5.3OnBugDiscoveryForbuganalysis,firstwededuplicatedcrashesbasedonfunctionnames,linenumbersandcrashingcauseasreportedbyAddress-Sanitizer[49]ormanuallyusingadebugger(e.g.,gdb).Next,wemanuallyinspectedalluniquecrashestoidentifytheirrootcause,andclassifythemasauniquebugbasedontherootcause.WesupplementedourunderstandingofeachbugwiththepubliclyavailableCVEs,bugreportsandthedevelopers’patches.Theex-travalidationisimportantastherootcausecanbedifferentfromthecrashingcausegivenbyAddressSanitizer[1],e.g.,inlibplistatypeconversionbugledtoaheapoverflowvulnerability(CVE-2017-5209),whileinlibtiffabufferoverflow(CVE2016-5102)ledtooverwritingoftwodifferentpointers,resultingintwodistinctA Flexible Framework for Expediting Bug Finding by Leveraging Past (Mis-)Behavior to Discover New Bugs
ACSAC 2020, December 7–11, 2020, Austin, USA
This is especially true when their core guiding technique(s) coun-
teracts that of our HPC based models. For example, in the case of
Fairfuzz, we found that strategies 1-a and 3-a do not work well be-
cause of Fairfuzz’s conservative policy, which heavily trims inputs
based entirely on rare branches. Given that our approach leverages
more contextual coverage information obtained through HPC, the
restrictions imposed by Fairfuzz undercuts these potential benefits.
5.3 On Bug Discovery
For bug analysis, first we deduplicated crashes based on function
names, line numbers and crashing cause as reported by Address-
Sanitizer [49] or manually using a debugger (e.g., gdb). Next, we
manually inspected all unique crashes to identify their root cause,
and classify them as a unique bug based on the root cause. We
supplemented our understanding of each bug with the publicly
available CVEs, bug reports and the developers’ patches. The ex-
tra validation is important as the root cause can be different from
the crashing cause given by AddressSanitizer [1], e.g., in libplist
a type conversion bug led to a heap overflow vulnerability (CVE-
2017-5209), while in libtiff a buffer overflow (CVE 2016-5102) led
to overwriting of two different pointers, resulting in two distinct
crashes — invalid read access to a file pointer, and invalid free of
memory.
We present our evaluation on extending AFL, MOpt and Fairfuzz
base fuzzers using our approach in Table 5. Overall, our approach
has similar or higher consistency for finding bugs. In particular, with
our extension, AFL is able to find 3 more bugs, MOpt’s discovery
improves by 5 bugs, while Fairfuzz by 8 bugs. This amounts to
> 13% improvement on AFL, > 29% on MOpt and > 53% on Fairfuzz.
These results aptly demonstrate the effectiveness of our approach
in improving bug discovery.
5.4 On Comprehensiveness
Lastly, we explore the effectiveness of a portfolio configuration [17]
where strategies 2-b, 3-a and 3-b are run independently, and the
results combined. For fairness, the baseline fuzzer is run 18 times,
while our strategies are run 6 times each. Table 6 compares base
fuzzers with the portfolio configuration measured by the consis-
tency of bugs. Notice that the combined strategies led to more bugs
than the base, i.e., 2 more bugs in AFL, 5 more bugs in MOpt, and 2
more bugs in Fairfuzz. Moreover, our approach has higher consis-
tency in finding the bugs than the base fuzzers. Interestingly, our
strategies explore 62% of the base paths on average (see Table 12 in
the Appendix). Thus, our approach can be considered more directed
in the search for bugs.
5.5 Vulnerability Discovery in the Wild
Satisfied with the performance of Omnifuzz, we decided to use it to
fuzz the current versions of libjpeg, libarchive and pcre. We limited
fuzzing to those three libraries due to time and resource constraints.
Within several hours, we induced a number of crashes that mapped
to 9 new bugs. The discovered bugs are listed in Table 7. After
reporting the vulnerabilities, 2 CVEs were assigned, and another
two bugs were immediately fixed. Note that these are heavily fuzzed
programs, and are continuously fuzzed on a large scale resources,
such as by Google’s continuous fuzzing framework for open source
software. For a few others, the maintainers argued that the bugs
identified were due to specific features (e.g., recursion - Bug-2484),
and it is up to the programmer to ensure correct inputs are used
(e.g., Bug-2479-2483).
6 LIMITATIONS
Clearly, the approach we advocate in this paper is not ideal for all
fuzzing campaigns. First, our approach requires a priori knowledge
of prior bugs in past versions to be able to guide the fuzzing process.
That said, it is possible that techniques from transfer learning [18]
and few-shot learning [55] can be used to build more sophisticated
models when a sufficient number of quality inputs are not read-
ily available. Moreover, we are not arguing that the approach we
take using multi-layer perceptrons to build our models is the best
choice. As stated earlier, we chose that solution because it offered
significant operational benefits. Secondly, though we make no as-
sumptions about the input scheduling algorithm used, we do not
study how the guidance we give during the input selection process
could impact optimality. The theoretical frameworks and formal-
izations by Rebert et al. [44] and Hayes et al. [21] could help in
that regard. Lastly, the models we build may not be robust against
anti-fuzzing techniques [19, 22] that try to impede automated bug
finding.
Our deduplication technique is, at present, not applicable to
fuzzing at the binary-level as our current instantiation mandates
that we have access to source code of the target program, and
that it be compiled with debugging symbols and no optimizations.
While this is not an issue for open-source software, closed-source
applications are also subject to fuzzing.
7 RELATED WORK
Deduplication: To date, both deduplication and root cause analy-
sis have been active areas of research. From an industry standpoint,
stack backtrace hashing and edge coverage are the most common
approaches to deduplication [13, 23, 28]. However, these approaches
suffer from either over-approximation or under-approximation [23,
28]. To address those limitations, several academic solutions have
been proposed. For example, Lin et al. [26] used static and dynamic
analysis at the source code level to detect and determine the root
cause of out-of-bounds vulnerabilities. Cui et al. [12] proposed
to deduplicate crashes in production systems by reconstructing
dataflow from a core dump, and performing backward analysis
from the crash point. Crashes are deduplicated based on the first
function from which the bad value that caused the crash was de-
rived. Xu et al. [58] proposed to improve root cause analysis when a
core dump contains corrupted data (e.g., due to memory corruption
vulnerabilities). Xu et al. [59] later proposed an approach (and ex-
tensions [36]) that uses the Intel processor trace feature and a core
dump to perform offline binary analysis to recover instructions that
lead to a crash. Subsequently, Cui et al. [11] suggested a refinement
wherein the accuracy of the recovered data flow is improved.
None of these approaches were designed to be used for on-
line deduplication. We incorporated several ideas (e.g., backward
dataflow [12] and record & replay [8]) from these works in the de-
sign of our deduplication strategy for guiding the fuzzing process.
ACSAC 2020, December 7–11, 2020, Austin, USA
Sanjeev Das, Kedrian James, Jan Werner, Manos Antonakakis, Michalis Polychronakis, and Fabian Monrose
(a) On AFL
(b) On Mopt.
(c) On Fairfuzz.
Figure 6: Relative time to find a crash. P0: libarchive, P1: libjpeg, P2: libplist, P3: libpng, P4: libxml2, P5: pcre, P6 :tiff, P7: yaml. A