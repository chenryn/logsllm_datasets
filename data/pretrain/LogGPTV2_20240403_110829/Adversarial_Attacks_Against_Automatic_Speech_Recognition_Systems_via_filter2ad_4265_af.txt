an elevator, which would make such a setup very unsuspicious.
A transfer function independent approach may be borrowed
from the image domain, speciﬁcally from Athalye et al. [2]. In
this work, the loss function is designed in order to consider all
13
kinds of rotations and translations. Similar to their approach,
the loss function of the ASR-DNN may not only be designed
to optimize one speciﬁc transfer function but to maximize the
expectation over all kinds of transfer functions. However, for
a realistic over-the-air attack, we think it is worth it to spend
more time investigating all possible attack vectors and their
limitations. In this work, we, instead, wanted to focus on the
general and theoretical feasibility of psychoacoustic hiding and
therefore leave over-the-air attack for future work.
A similar attack can also be imagined for real applications,
e. g., Amazon’s Alexa. However, the detailed architecture of
this system is hard to access and it requires a-priori investi-
gations to obtain that kind of information. It may be possible
to retrieve the model parameters with different model stealing
approaches [22], [34], [37], [49], [54]. For Alexa, our reverse
engineering results of the ﬁrmware indicate that Amazon uses
parts of Kaldi. Therefore, a limited knowledge about
the
topology and parameters might be enough to create a model
for a black-box attack. A starting point could be the keyword
recognition of commercial ASR systems, e. g., “Alexa.” This
would have the advantage that the keyword recognition runs
locally on the device and would, therefore, be easier to access.
For image classiﬁcation, universal adversarial perturbations
have already been successfully created [7], [29]. For ASR
systems, it is still an open question if these kinds of adversarial
perturbations exist and how they can be created.
VIII. CONCLUSION
We have presented a new method for creating adversarial
examples for ASR systems, which explicitly take dynamic
human hearing thresholds into account. In this way, borrowing
the mechanisms of MP3 encoding, the audibility of the added
noise is clearly reduced. We perform our attack against the
state-of-the-art Kaldi ASR system and feed the adversarial
input directly into the recognizer in order to show the general
feasibility of psychoacoustics-based attacks.
By applying forced alignment and backpropagation to the
DNN-HMM system, we were able to create inconspicuous
adversarial perturbations very reliably. In general, it is possible
to hide any target transcription within any audio ﬁle and,
with the correct attack vectors, it was possible to hide the
noise below the hearing threshold and make the changes psy-
chophysically almost imperceptible. The choice of the original
audio sample, an optimal phone rate, and forced alignment
give the optimal starting point for the creation of adversarial
examples. Additionally, we have evaluated different algorithm
setups, including the number of iterations and the allowed
deviation from the hearing thresholds. The comparison with
another approach by Yuan et al. [59], which is also able to
create targeted adversarial examples, shows that our approach
needs far lower distortions. Listening tests have proven that the
target transcription was incomprehensible for human listeners.
Furthermore, for some audio ﬁles, it was almost impossible for
participants to distinguish between the original and adversarial
sample, even with headphones and in a direct comparison.
Future work should investigate the hardening of ASR
systems by considering psychoacoustic models, in order to
prevent these presently fairly easy attacks. Additionally, similar
attacks should be evaluated on commercial ASR systems and
in real-world settings (e.g., in a black-box setting or via over-
the-air attacks).
ACKNOWLEDGMENTS
This work was supported by Intel as part of the Intel
Collaborative Research Institute “Collaborative Autonomous
& Resilient Systems” (ICRI-CARS). In addition, this work
was supported by the German Research Foundation (DFG)
within the framework of the Excellence Strategy of the Federal
Government and the States - EXC 2092 CASA.
REFERENCES
[1] M. Asad, J. Gilani, and A. Khalid, “An enhanced Least Signiﬁcant
Bit modiﬁcation technique for audio steganography,” in Conference on
Computer Networks and Information Technology.
IEEE, Jul. 2011, pp.
143–147.
[2] A. Athalye, L. Engstrom, A. Ilyas, and K. Kwok, “Synthesizing robust
adversarial examples,” CoRR, vol. abs/1707.07397, pp. 1–18, Jul. 2017.
[3] K. Audhkhasi, B. Kingsbury, B. Ramabhadran, G. Saon, and
M. Picheny, “Building competitive direct acoustics-to-word models for
english conversational speech recognition,” 2018.
[4] M. Barni, F. Bartolini, and A. Piva, “Improved wavelet-based water-
marking through pixel-wise masking,” IEEE transactions on image
processing, vol. 10, no. 5, pp. 783–791, 2001.
[5] M. Barreno, B. Nelson, A. D. Joseph, and J. D. Tygar, “The security
of machine learning,” Machine Learning, vol. 81, no. 2, pp. 121–148,
Nov. 2010.
[6] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and J. D. Tygar, “Can
machine learning be secure?” in Symposium on Information, Computer
and Communications Security. ACM, Mar. 2006, pp. 16–25.
[7] T. B. Brown, D. Man´e, A. Roy, M. Abadi, and J. Gilmer, “Adversarial
patch,” CoRR, vol. abs/1712.09665, pp. 1–6, Dec. 2017.
[8] N. Carlini, P. Mishra, T. Vaidya, Y. Zhang, M. Sherr, C. Shields, D. A.
Wagner, and W. Zhou, “Hidden voice commands,” in USENIX Security
Symposium. USENIX, Aug. 2016, pp. 513–530.
[9] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural
IEEE, May 2017,
networks,” in Symposium on Security and Privacy.
pp. 39–57.
[10] ——, “Audio adversarial examples: Targeted attacks on Speech-to-
Text,” CoRR, vol. abs/1801.01944, pp. 1–7, Jan. 2018.
[11] M. Cisse, Y. Adi, N. Neverova, and J. Keshet, “Houdini: Fooling deep
structured prediction models,” CoRR, vol. abs/1707.05373, pp. 1–12,
Jul. 2017.
[12] M. Ciss´e, P. Bojanowski, E. Grave, Y. Dauphin, and N. Usunier,
“Parseval networks: Improving robustness to adversarial examples,” in
Conference on Machine Learning. PMLR, Aug. 2017, pp. 854–863.
I. Cox, M. Miller, J. Bloom, J. Fridrich, and T. Kalker, Digital
watermarking and steganography. Morgan kaufmann, 2007.
[13]
[14] W. Diao, X. Liu, Z. Zhou, and K. Zhang, “Your voice assistant is mine:
How to abuse speakers to steal information and control your phone,” in
Workshop on Security and Privacy in Smartphones & Mobile Devices.
ACM, Nov. 2014, pp. 63–74.
I. Evtimov, K. Eykholt, E. Fernandes, T. Kohno, B. Li, A. Prakash,
A. Rahmati, and D. Song, “Robust physical-world attacks on machine
learning models,” CoRR, vol. abs/1707.08945, pp. 1–11, Jul. 2017.
[15]
[16] A. Fawzi, O. Fawzi, and P. Frossard, “Analysis of classiﬁers’ robustness
to adversarial perturbations,” Machine Learning, vol. 107, no. 3, pp.
481–508, Mar. 2018.
[18]
[17] M. Fujimoto, “Factored deep convolutional neural networks for noise
robust speech recognition,” Proc. Interspeech, pp. 3837–3841, 2017.
I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
adversarial examples,” CoRR, vol. abs/1412.6572, pp. 1–11, Dec. 2014.
[19] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a
neural network,” CoRR, vol. abs/1503.02531, pp. 1–9, Mar. 2015.
[20] A. Ilyas, L. Engstrom, A. Athalye, and J. Lin, “Black-box adversarial at-
tacks with limited queries and information,” CoRR, vol. abs/1804.08598,
pp. 1–10, Apr. 2018.
14
[21]
ISO, “Information Technology – Coding of moving pictures and as-
sociated audio for digital storage media at up to 1.5 Mbits/s – Part3:
Audio,” International Organization for Standardization, ISO 11172-3,
1993.
[22] M. Juuti, S. Szyller, A. Dmitrenko, S. Marchal, and N. Asokan,
“PRADA: Protecting against DNN model stealing attacks,” CoRR, vol.
abs/1805.02628, pp. 1–16, May 2018.
[23] K. Kohls, T. Holz, D. Kolossa, and C. P¨opper, “SkypeLine: Robust
hidden data transmission for VoIP,” in Asia Conference on Computer
and Communications Security. ACM, May 2016, pp. 877–888.
J. Liu, K. Zhou, and H. Tian, “Least-Signiﬁcant-Digit steganography in
low bitrate speech,” in International Conference on Communications.
IEEE, Jun. 2012, pp. 1133–1137.
[24]
[25] Y. Liu, X. Chen, C. Liu, and D. Song, “Delving into transferable ad-
versarial examples and black-box attacks,” CoRR, vol. abs/1611.02770,
2016.
[26] D. Lowd and C. Meek, “Adversarial
learning,” in Conference on
Knowledge Discovery in Data Mining. ACM, Aug. 2005, pp. 641–647.
[27] V. Manohar, D. Povey, and S. Khudanpur, “JHU kaldi system for arabic
MGB-3 ASR challenge using diarization, audiotranscript alignment and
transfer learning,” in 2017 IEEE Automatic Speech Recognition and
Understanding Workshop, Dec 2017, pp. 346–352.
[28] M. McAuliffe, M. Socolof, S. Mihuc, M. Wagner, and M. Sonderegger,
“Montreal forced aligner: trainable text-speech alignment using kaldi,”
in Proceedings of interspeech, 2017, pp. 498–502.
[29] S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard, “Universal
adversarial perturbations,” in Conference on Computer Vision and
Pattern Recognition.
IEEE, Jul. 2017, pp. 86–94.
[30] T. Moynihan, “How to keep Amazon Echo and Google Home from
responding to your TV,” Feb. 2017, https://www.wired.com/2017/02/
keep-amazon-echo-google-home-responding-tv/, as of December 14,
2018.
[31] G. Navarro, “A guided tour to approximate string matching,” ACM
Computing Surveys, vol. 33, no. 1, pp. 31–88, Mar. 2001.
[32] A. Nguyen, J. Yosinski, and J. Clune, “Deep neural networks are easily
fooled: High conﬁdence predictions for unrecognizable images,” in
Conference on Computer Vision and Pattern Recognition.
IEEE, Jun.
2015, pp. 427–436.
[33] M. Nutzinger, C. Fabian, and M. Marschalek, “Secure hybrid spread
spectrum system for steganography in auditive media,” in Conference
on Intelligent Information Hiding and Multimedia Signal Processing.
IEEE, Oct. 2010, pp. 78–81.
[34] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and
A. Swami, “Practical Black-Box attacks against machine learning,” in
Asia Conference on Computer and Communications Security. ACM,
Apr. 2017, pp. 506–519.
[35] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and
A. Swami, “The limitations of deep learning in adversarial settings,”
in European Symposium on Security and Privacy.
IEEE, Mar. 2016,
pp. 372–387.
[36] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, “Distillation
as a defense to adversarial perturbations against deep neural networks,”
in Symposium on Security and Privacy.
IEEE, May 2016, pp. 582–597.
[37] N. Papernot, P. D. McDaniel, and I. J. Goodfellow, “Transferability
in machine learning: From phenomena to Black-Box attacks using
adversarial samples,” CoRR, vol. abs/1605.07277, pp. 1–13, May 2016.
[38] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel,
M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz, J. Silovsky, G. Stem-
mer, and K. Vesely, “The Kaldi speech recognition toolkit,” in Workshop
on Automatic Speech Recognition and Understanding.
IEEE, Dec.
2011.
[39] L. Rabiner and B.-H. Juang, Fundamentals of Speech Recognition.
Prentice-Hall, Inc., 1993.
[40] S. Ranjan and J. H. Hansen, “Improved gender independent speaker
recognition using convolutional neural network based bottleneck fea-
tures,” Proc. Interspeech 2017, pp. 1009–1013, 2017.
[41] M. Ravanelli, P. Brakel, M. Omologo, and Y. Bengio, “A network of
deep neural networks for distant speech recognition,” in 2017 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP), March 2017, pp. 4880–4884.
15
[42] N. Roy, H. Hassanieh, and R. Roy Choudhury, “BackDoor: Making
microphones hear inaudible sounds,” in Conference on Mobile Systems,
Applications, and Services. ACM, Jun. 2017, pp. 2–14.
[43] G. Saon, G. Kurata, T. Sercu, K. Audhkhasi, S. Thomas, D. Dimitriadis,
X. Cui, B. Ramabhadran, M. Picheny, L. Lim, B. Roomi, and P. Hall,
“English conversational telephone speech recognition by humans and
machines,” CoRR, vol. abs/1703.02136, pp. 1–7, Mar. 2017.
[44] N. Schinkel-Bielefeld, N. Lotze, and F. Nagel, “Audio quality evaluation
by experienced and inexperienced listeners,” in International Congress
on Acoustics. ASA, Jun. 2013, pp. 6–16.
[45] M. Schoefﬂer, S. Bartoschek, F.-R. St¨oter, M. Roess, S. Westphal,
B. Edler, and J. Herre, “webMUSHRA – A comprehensive framework
for web-based listening tests,” Journal of Open Research Software,
vol. 6, no. 1, Feb. 2018.
J. W. Seok and J. W. Hong, “Audio watermarking for copyright
protection of digital audio data,” Electronics Letters, vol. 37, no. 1,
pp. 60–61, 2001.
[46]
[47] U. Shaham, Y. Yamada, and S. Negahban, “Understanding adversarial
training: Increasing local stability of supervised models through robust
optimization,” Neurocomputing, 2018.
[48] L. Song and P. Mittal, “Inaudible voice commands,” CoRR, vol.
abs/1708.07238, pp. 1–3, Aug. 2017.
[49] F. Tram`er, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, “Stealing
machine learning models via prediction APIs,” in USENIX Security
Symposium. USENIX, Aug. 2016, pp. 601–618.
J. Trmal, M. Wiesner, V. Peddinti, X. Zhang, P. Ghahremani, Y. Wang,
V. Manohar, H. Xu, D. Povey, and S. Khudanpur, “The kaldi openkws
system: Improving low resource keyword search,” Proc. Interspeech,
pp. 3597–3601, Aug. 2017.
[50]
[51] P. Upadhyaya, O. Farooq, M. R. Abidi, and Y. V. Varshney, “Continuous
hindi speech recognition model based on kaldi asr toolkit,” in 2017 In-
ternational Conference on Wireless Communications, Signal Processing
and Networking (WiSPNET), March 2017, pp. 786–789.
[52] T. Vaidya, Y. Zhang, M. Sherr, and C. Shields, “Cocaine Noodles:
Exploiting the gap between human and machine speech recognition,”
in Workshop on Offensive Technologies. USENIX, Aug. 2015.
J. Villalba, N. Br¨ummer, and N. Dehak, “Tied variational autoencoder
backends for i-vector speaker recognition,” Interspeech, Stockholm, pp.
1005–1008, Aug. 2017.
[53]
learning,” in Symposium on Security and Privacy.
[54] B. Wang and N. Z. Gong, “Stealing hyperparameters in machine
IEEE, May 2018.
[55] R. B. Wolfgang, C. I. Podilchuk, and E. J. Delp, “Perceptual watermarks
for digital images and video,” Proceedings of the IEEE, vol. 87, no. 7,
pp. 1108–1126, 1999.
[56] S. Wu, J. Huang, D. Huang, and Y. Q. Shi, “Self-synchronized audio
watermark in dwt domain,” in 2004 IEEE International Symposium on
Circuits and Systems (IEEE Cat. No.04CH37512), vol. 5, May 2004,
pp. V–V.
[57] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke,
D. Yu, and G. Zweig, “Toward human parity in conversational speech
recognition,” IEEE/ACM Transactions on Audio, Speech, and Language
Processing, vol. 25, no. 12, pp. 2410–2423, Dec. 2017.
[58] ——, “The microsoft 2016 conversational speech recognition system,”
in Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE
International Conference on.
IEEE, 2017, pp. 5255–5259.
[59] X. Yuan, Y. Chen, Y. Zhao, Y. Long, X. Liu, K. Chen, S. Zhang,
H. Huang, X. Wang, and C. A. Gunter, “Commandersong: A system-
atic approach for practical adversarial voice recognition,” in USENIX
Security Symposium. USENIX, Aug. 2018, pp. 49–64.
[60] V. Zantedeschi, M.-I. Nicolae, and A. Rawat, “Efﬁcient defenses against
adversarial attacks,” in Workshop on Artiﬁcial Intelligence and Security.
ACM, Nov. 2017, pp. 39–49.
[61] G. Zhang, C. Yan, X. Ji, T. Zhang, T. Zhang, and W. Xu, “Dolphi-
nAttack: Inaudible voice commands,” in Conference on Computer and
Communications Security. ACM, Oct. 2017, pp. 103–117.
[62] E. Zwicker and H. Fastl, Psychoacoustics: Facts and Models, 3rd ed.
Springer, 2007.