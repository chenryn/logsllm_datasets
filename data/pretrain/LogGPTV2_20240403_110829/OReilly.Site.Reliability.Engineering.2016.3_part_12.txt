The SRE adds database support to a “generic failover” script that everyone uses.
4) Internally maintained system-specific automation
The database ships with its own failover script.
5) Systems that don’t need any automation
The database notices problems, and automatically fails over without human
intervention.
SRE hates manual operations, so we obviously try to create systems that don’t require
them. However, sometimes manual operations are unavoidable.
There is additionally a subvariety of automation that applies changes not across the
domain of specific system-related configuration, but across the domain of production
as a whole. In a highly centralized proprietary production environment like Google’s,
there are a large number of changes that have a non–service-specific scope—e.g.,
changing upstream Chubby servers, a flag change to the Bigtable client library to
make access more reliable, and so on—which nonetheless need to be safely managed
and rolled back if necessary. Beyond a certain volume of changes, it is infeasible for
production-wide changes to be accomplished manually, and at some time before that
point, it’s a waste to have manual oversight for a process where a large proportion of
the changes are either trivial or accomplished successfully by basic relaunch-and-
check strategies.
Let’s use internal case studies to illustrate some of the preceding points in detail. The
first case study is about how, due to some diligent, far-sighted work, we managed to
achieve the self-professed nirvana of SRE: to automate ourselves out of a job.
Automate Yourself Out of a Job: Automate ALL the Things!
For a long while, the Ads products at Google stored their data in a MySQL database.
Because Ads data obviously has high reliability requirements, an SRE team was
charged with looking after that infrastructure. From 2005 to 2008, the Ads Database
mostly ran in what we considered to be a mature and managed state. For example, we
had automated away the worst, but not all, of the routine work for standard replica
replacements. We believed the Ads Database was well managed and that we had har‐
vested most of the low-hanging fruit in terms of optimization and scale. However, as
daily operations became comfortable, team members began to look at the next level
Automate Yourself Out of a Job: Automate ALL the Things! | 73
of system development: migrating MySQL onto Google’s cluster scheduling system,
Borg.
We hoped this migration would provide two main benefits:
• Completely eliminate machine/replica maintenance: Borg would automatically
handle the setup/restart of new and broken tasks.
• Enable bin-packing of multiple MySQL instances on the same physical machine:
Borg would enable more efficient use of machine resources via Containers.
In late 2008, we successfully deployed a proof of concept MySQL instance on Borg.
Unfortunately, this was accompanied by a significant new difficulty. A core operating
characteristic of Borg is that its tasks move around automatically. Tasks commonly
move within Borg as frequently as once or twice per week. This frequency was tolera‐
ble for our database replicas, but unacceptable for our masters.
At that time, the process for master failover took 30–90 minutes per instance. Simply
because we ran on shared machines and were subject to reboots for kernel upgrades,
in addition to the normal rate of machine failure, we had to expect a number of
otherwise unrelated failovers every week. This factor, in combination with the num‐
ber of shards on which our system was hosted, meant that:
• Manual failovers would consume a substantial amount of human hours and
would give us best-case availability of 99% uptime, which fell short of the actual
business requirements of the product.
• In order to meet our error budgets, each failover would have to take less than 30
seconds of downtime. There was no way to optimize a human-dependent proce‐
dure to make downtime shorter than 30 seconds.
Therefore, our only choice was to automate failover. Actually, we needed to automate
more than just failover.
In 2009 Ads SRE completed our automated failover daemon, which we dubbed
“Decider.” Decider could complete MySQL failovers for both planned and unplanned
failovers in less than 30 seconds 95% of the time. With the creation of Decider,
MySQL on Borg (MoB) finally became a reality. We graduated from optimizing our
infrastructure for a lack of failover to embracing the idea that failure is inevitable, and
therefore optimizing to recover quickly through automation.
While automation let us achieve highly available MySQL in a world that forced up to
two restarts per week, it did come with its own set of costs. All of our applications had
to be changed to include significantly more failure-handling logic than before. Given
that the norm in the MySQL development world is to assume that the MySQL
instance will be the most stable component in the stack, this switch meant customiz‐
ing software like JDBC to be more tolerant of our failure-prone environment. How‐
74 | Chapter 7: The Evolution of Automation at Google
ever, the benefits of migrating to MoB with Decider were well worth these costs. Once
on MoB, the time our team spent on mundane operational tasks dropped by 95%.
Our failovers were automated, so an outage of a single database task no longer paged
a human.
The main upshot of this new automation was that we had a lot more free time to
spend on improving other parts of the infrastructure. Such improvements had a cas‐
cading effect: the more time we saved, the more time we were able to spend on opti‐
mizing and automating other tedious work. Eventually, we were able to automate
schema changes, causing the cost of total operational maintenance of the Ads Data‐
base to drop by nearly 95%. Some might say that we had successfully automated our‐
selves out of this job. The hardware side of our domain also saw improvement.
Migrating to MoB freed up considerable resources because we could schedule multi‐
ple MySQL instances on the same machines, which improved utilization of our hard‐
ware. In total, we were able to free up about 60% of our hardware. Our team was now
flush with hardware and engineering resources.
This example demonstrates the wisdom of going the extra mile to deliver a platform
rather than replacing existing manual procedures. The next example comes from the
cluster infrastructure group, and illustrates some of the more difficult trade-offs you
might encounter on your way to automating all the things.
Soothing the Pain: Applying Automation to Cluster
Turnups
Ten years ago, the Cluster Infrastructure SRE team seemed to get a new hire every
few months. As it turned out, that was approximately the same frequency at which we
turned up a new cluster. Because turning up a service in a new cluster gives new hires
exposure to a service’s internals, this task seemed like a natural and useful training
tool.
The steps taken to get a cluster ready for use were something like the following:
1. Fit out a datacenter building for power and cooling.
2. Install and configure core switches and connections to the backbone.
3. Install a few initial racks of servers.
4. Configure basic services such as DNS and installers, then configure a lock ser‐
vice, storage, and computing.
5. Deploy the remaining racks of machines.
6. Assign user-facing services resources, so their teams can set up the services.
Soothing the Pain: Applying Automation to Cluster Turnups | 75
Steps 4 and 6 were extremely complex. While basic services like DNS are relatively
simple, the storage and compute subsystems at that time were still in heavy develop‐
ment, so new flags, components, and optimizations were added weekly.
Some services had more than a hundred different component subsystems, each with a
complex web of dependencies. Failing to configure one subsystem, or configuring a
system or component differently than other deployments, is a customer-impacting
outage waiting to happen.
In one case, a multi-petabyte Bigtable cluster was configured to not use the first (log‐
ging) disk on 12-disk systems, for latency reasons. A year later, some automation
assumed that if a machine’s first disk wasn’t being used, that machine didn’t have any
storage configured; therefore, it was safe to wipe the machine and set it up from
scratch. All of the Bigtable data was wiped, instantly. Thankfully we had multiple real-
time replicas of the dataset, but such surprises are unwelcome. Automation needs to
be careful about relying on implicit “safety” signals.
Early automation focused on accelerating cluster delivery. This approach tended to
rely upon creative use of SSH for tedious package distribution and service initializa‐
tion problems. This strategy was an initial win, but those free-form scripts became a
cholesterol of technical debt.
Detecting Inconsistencies with Prodtest
As the numbers of clusters grew, some clusters required hand-tuned flags and set‐
tings. As a result, teams wasted more and more time chasing down difficult-to-spot
misconfigurations. If a flag that made GFS more responsive to log processing leaked
into the default templates, cells with many files could run out of memory under load.
Infuriating and time-consuming misconfigurations crept in with nearly every large
configuration change.
The creative—though brittle—shell scripts we used to configure clusters were neither
scaling to the number of people who wanted to make changes nor to the sheer num‐
ber of cluster permutations that needed to be built. These shell scripts also failed to
resolve more significant concerns before declaring that a service was good to take
customer-facing traffic, such as:
• Were all of the service’s dependencies available and correctly configured?
• Were all configurations and packages consistent with other deployments?
• Could the team confirm that every configuration exception was desired?
Prodtest (Production Test) was an ingenious solution to these unwelcome surprises.
We extended the Python unit test framework to allow for unit testing of real-world
services. These unit tests have dependencies, allowing a chain of tests, and a failure in
one test would quickly abort. Take the test shown in Figure 7-1 as an example.
76 | Chapter 7: The Evolution of Automation at Google
Figure 7-1. ProdTest for DNS Service, showing how one failed test aborts the subsequent
chain of tests
A given team’s Prodtest was given the cluster name, and it could validate that team’s
services in that cluster. Later additions allowed us to generate a graph of the unit tests
and their states. This functionality allowed an engineer to see quickly if their service
was correctly configured in all clusters, and if not, why. The graph highlighted the
failed step, and the failing Python unit test output a more verbose error message.
Any time a team encountered a delay due to another team’s unexpected misconfigura‐
tion, a bug could be filed to extend their Prodtest. This ensured that a similar prob‐
lem would be discovered earlier in the future. SREs were proud to be able to assure
their customers that all services—both newly turned up services and existing services
with new configuration—would reliably serve production traffic.
For the first time, our project managers could predict when a cluster could “go live,”
and had a complete understanding of why each clusters took six or more weeks to go
from “network-ready” to “serving live traffic.” Out of the blue, SRE received a mission
from senior management: In three months, five new clusters will reach network-ready
on the same day. Please turn them up in one week.
Soothing the Pain: Applying Automation to Cluster Turnups | 77
Resolving Inconsistencies Idempotently
A “One Week Turnup” was a terrifying mission. We had tens of thousands of lines of
shell script owned by dozens of teams. We could quickly tell how unprepared any
given cluster was, but fixing it meant that the dozens of teams would have to file hun‐
dreds of bugs, and then we had to hope that these bugs would be promptly fixed.
We realized that evolving from “Python unit tests finding misconfigurations” to
“Python code fixing misconfigurations” could enable us to fix these issues faster.
The unit test already knew which cluster we were examining and the specific test that
was failing, so we paired each test with a fix. If each fix was written to be idempotent,
and could assume that all dependencies were met, resolving the problem should have
been easy—and safe—to resolve. Requiring idempotent fixes meant teams could run
their “fix script” every 15 minutes without fearing damage to the cluster’s configura‐
tion. If the DNS team’s test was blocked on the Machine Database team’s configura‐
tion of a new cluster, as soon as the cluster appeared in the database, the DNS team’s
tests and fixes would start working.
Take the test shown in Figure 7-2 as an example. If TestDnsMonitoringConfigExists
fails, as shown, we can call FixDnsMonitoringCreateConfig, which scrapes configu‐
ration from a database, then checks a skeleton configuration file into our revision
control system. Then TestDnsMonitoringConfigExists passes on retry, and
the TestDnsMonitoringConfigPushed test can be attempted. If the test fails, the
FixDnsMonitoringPushConfig step runs. If a fix fails multiple times, the automation
assumes that the fix failed and stops, notifying the user.
Armed with these scripts, a small group of engineers could ensure that we could go
from “The network works, and machines are listed in the database” to “Serving 1% of
websearch and ads traffic” in a matter of a week or two. At the time, this seemed to be
the apex of automation technology.
Looking back, this approach was deeply flawed; the latency between the test, the fix,
and then a second test introduced flaky tests that sometimes worked and sometimes
failed. Not all fixes were naturally idempotent, so a flaky test that was followed by a
fix might render the system in an inconsistent state.
78 | Chapter 7: The Evolution of Automation at Google
Figure 7-2. ProdTest for DNS Service, showing that one failed test resulted in only run‐
ning one fix
The Inclination to Specialize
Automation processes can vary in three respects:
• Competence, i.e., their accuracy
• Latency, how quickly all steps are executed when initiated
• Relevance, or proportion of real-world process covered by automation
We began with a process that was highly competent (maintained and run by the ser‐
vice owners), high-latency (the service owners performed the process in their spare
time or assigned it to new engineers), and very relevant (the service owners knew
when the real world changed, and could fix the automation).
To reduce turnup latency, many service owning teams instructed a single “turnup
team” what automation to run. The turnup team used tickets to start each stage in the
turnup so that we could track the remaining tasks, and who those tasks were assigned
to. If the human interactions regarding automation modules occurred between peo‐
ple in the same room, cluster turnups could happen in a much shorter time. Finally,
we had our competent, accurate, and timely automation process!
Soothing the Pain: Applying Automation to Cluster Turnups | 79
But this state didn’t last long. The real world is chaotic: software, configuration, data,
etc. changed, resulting in over a thousand separate changes a day to affected systems.
The people most affected by automation bugs were no longer domain experts, so the
automation became less relevant (meaning that new steps were missed) and less com‐
petent (new flags might have caused automation to fail). However, it took a while for
this drop in quality to impact velocity.
Automation code, like unit test code, dies when the maintaining team isn’t obsessive
about keeping the code in sync with the codebase it covers. The world changes
around the code: the DNS team adds new configuration options, the storage team
changes their package names, and the networking team needs to support new devices.
By relieving teams who ran services of the responsibility to maintain and run their
automation code, we created ugly organizational incentives:
• A team whose primary task is to speed up the current turnup has no incentive to
reduce the technical debt of the service-owning team running the service in pro‐
duction later.
• A team not running automation has no incentive to build systems that are easy to
automate.
• A product manager whose schedule is not affected by low-quality automation
will always prioritize new features over simplicity and automation.
The most functional tools are usually written by those who use them. A similar argu‐
ment applies to why product development teams benefit from keeping at least some
operational awareness of their systems in production.
Turnups were again high-latency, inaccurate, and incompetent—the worst of all
worlds. However, an unrelated security mandate allowed us out of this trap. Much of
distributed automation relied at that time on SSH. This is clumsy from a security per‐
spective, because people must have root on many machines to run most commands.
A growing awareness of advanced, persistent security threats drove us to reduce the
privileges SREs enjoyed to the absolute minimum they needed to do their jobs. We
had to replace our use of sshd with an authenticated, ACL-driven, RPC-based Local
Admin Daemon, also known as Admin Servers, which had permissions to perform
those local changes. As a result, no one could install or modify a server without an
audit trail. Changes to the Local Admin Daemon and the Package Repo were gated on
code reviews, making it very difficult for someone to exceed their authority; giving
someone the access to install packages would not let them view colocated logs. The
Admin Server logged the RPC requestor, any parameters, and the results of all RPCs
to enhance debugging and security audits.
80 | Chapter 7: The Evolution of Automation at Google
Service-Oriented Cluster-Turnup
In the next iteration, Admin Servers became part of service teams’ workflows, both as
related to the machine-specific Admin Servers (for installing packages and rebooting)
and cluster-level Admin Servers (for actions like draining or turning up a service).
SREs moved from writing shell scripts in their home directories to building peer-
reviewed RPC servers with fine-grained ACLs.
Later on, after the realization that turnup processes had to be owned by the teams
that owned the services fully sank in, we saw this as a way to approach cluster turnup
as a Service-Oriented Architecture (SOA) problem: service owners would be respon‐
sible for creating an Admin Server to handle cluster turnup/turndown RPCs, sent by
the system that knew when clusters were ready. In turn, each team would provide the
contract (API) that the turnup automation needed, while still being free to change the
underlying implementation. As a cluster reached “network-ready,” automation sent
an RPC to each Admin Server that played a part in turning up the cluster.
We now have a low-latency, competent, and accurate process; most importantly, this
process has stayed strong as the rate of change, the number of teams, and the number
of services seem to double each year.
As mentioned earlier, our evolution of turnup automation followed a path:
1. Operator-triggered manual action (no automation)
2. Operator-written, system-specific automation
3. Externally maintained generic automation
4. Internally maintained, system-specific automation
5. Autonomous systems that need no human intervention
While this evolution has, broadly speaking, been a success, the Borg case study illus‐
trates another way we have come to think of the problem of automation.
Borg: Birth of the Warehouse-Scale Computer