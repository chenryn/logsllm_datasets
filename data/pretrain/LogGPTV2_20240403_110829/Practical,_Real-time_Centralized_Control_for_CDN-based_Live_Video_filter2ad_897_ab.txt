only client A is served, whereas in Figure 3c both clients
are served. The issue is that using distribution tree (cid:84)2
would congest the (cid:82)(cid:89) link. However, knowing this in
advance is diﬃcult; it would require not only knowledge
of network resources, but also the placement of other
streams. A natural solution would be centralization,
which aﬀords both a global view of the network and the
ability to coordinate streams.
This observation generalizes to large-scale networks.
Figure 4 compares a system with a global view that
places streams independently without coordination (OM
in §7) to one that has a global view and coordinates
(A) All Allocations (B) Possible Distribution (C) Possible Distribution VS#Content&Requests&Response&Congested&R!X!Y!A!B!1000!750!300!700!300!S!2000!V2,!200!V1,!300!R!X!Y!A!B!S!R!X!Y!A!B!S!Response&T1!T2!T3!313Figure 4: The importance of coordinating
streams generalizes to larger systems. This
graph shows the gain of our system compared
to a multicast-style approach as we’ll see in §7.
Figure 5: Motivating app-speciﬁc optimization.
streams (VDN in §7) for a 100 node topology. With 10K
videos, we observe up to a 100% improvement in average
bitrate.
Application-speciﬁc optimization: Generic traﬃc
engineering at a centralized controller is not enough; we
must perform app-speciﬁc optimization. For example,
in Figure 5, two videos are encoded as low quality (400
Kbps) and high quality (1500 Kbps) versions. Due to
bandwidth constraints (Figure 5a), we must deliver both
over link (cid:82)(cid:89) . We present two ways to allocate bandwidth
in Figure 5b and c. Despite fairly allocating bandwidth
between the streams, Figure 5b does worse overall, as
both clients only receive the low quality stream. Fig-
ure 5c is “unfair”, but is a better strategy as one client
is able to get the higher quality stream. Thus, careful
consideration of bitrate at the granularity of streams is
needed to provide the best quality.
From the two examples, we conclude that we can im-
prove quality with: 1) a global view of network resources;
2) coordination across streams; and 3) consideration of
the streaming bitrates. This argues for a video-speciﬁc
control plane that is logically centralized.
2.4 Case for hybrid control
Live video-speciﬁc, centralized optimization alone is not
suﬃcient. A fully centralized system would require new
video requests to reach the controller across WAN laten-
cies before the video could be viewed, yielding terrible
join times. Additionally, centralized optimization us-
ing an integer program can take quite long (e.g., 10s
of seconds), further impacting join time. Yet, a dis-
tributed scheme would be highly responsive as clusters
could react to requests immediately, yielding low join
times and fast failure recovery. However, we argue that
a distributed scheme is challenged to provide the high
quality demanded by users at reasonable cost, due to
the lack of coordination (§2.3).
A combination of the two schemes, with the quality
of a centralized system and the responsiveness of a dis-
tributed system would be best suited. We refer to this
combination as hybrid control. We avoid poor interac-
tions between the two schemes by exploiting properties
of our highly structured topology (§2.1) and by keeping
track of alternate paths with enough capacity for each
video channel (§4).
3 VDN system overview
VDN reuses existing CDN internal infrastructure (source
clusters, reﬂector clusters, edge clusters, and DNS) but
employs a new control plane based on hybrid control—a
centralized controller makes optimal decisions slowly
while clusters simultaneously make decisions quickly
based on distributed state. VDN treats each cluster
as an atomic unit (as in Figure 6) and controls the
distribution of live video from sources to clients; traﬃc
management within a cluster is outside the scope of this
paper.
When video (cid:118) is requested at bitrate (cid:98) by a client in
an AS (cid:97), a request is sent to VDN’s DNS server; the
response directs the client to send video chunk requests
to a nearby edge cluster. If this edge cluster knows about
(cid:118) (i.e., has a entry for ((cid:118), (cid:98)) in its forwarding table), then
it forwards the request upstream accordingly. If not, it
runs the distributed control algorithm (§4). Reﬂectors
pick source clusters similarly. The video chunk follows
this path in reverse. Eventually, centralized control
updates the clusters’ forwarding tables (§5).
As a control plane, VDN (1) populates application-
layer forwarding tables at each cluster with centrally
computed entries, (2) creates forwarding table entries
on-the-ﬂy when necessary using distributed control, and
(3) updates the client to edge server mapping accordingly
in the DNS infrastructure.
3.1 Design
Physical view: VDN introduces two physical pieces to
the traditional CDN infrastructure: a logically central-
ized central controller and a local agent in each server
cluster. The central controller and local agents are each
decomposed into two pieces: (1) a control subsystem
that computes path assignments based on network state
and (2) a discovery subsystem that tracks incoming
requests and topology information.
Logical view: VDN’s control plane is driven by two
control loops, which update clusters’ forwarding tables
at diﬀerent timescales. A central control loop computes
optimal distribution trees (as well as client/server map-
pings) using the algorithm described in §5. This loop
runs continuously and operates on the timescale of tens
0246810#ofVideos(Thousands)−20020406080100120GaininAvg.Bitrate(%)Coordination(A) All Allocations (B) Possible Distribution (C) Possible  Distribution VS#Requests'Response'R!X!Y!A!B!300!1500!300!1500!2000!S!2000!V2,!1500!V2,!400!V1,!1500!V1,!400!R!X!Y!A!B!S!R!X!Y!A!B!S!Response'T1!T2!Content'(Hi/Def)'(Std/Def)'Content'(Hi/Def)'(Std/Def)'Content'(Hi/Def)'(Std/Def)'Alloc:!1000!Alloc:!1000!!Alloc:!1500!Alloc:!400!!314Figure 6: VDN system overview.
Figure 7: Sample RIB and FIB entries. The local agent uses network and viewer state as “evidence”
to decide when to override potentially stale decisions from the central controller.
of seconds to minutes. Meanwhile, the local agent runs
a distributed control loop that amends its cluster’s for-
warding table to quickly (i.e., sub-second) respond to
local changes (e.g., link failures) using distributed state.
Central control loop:
1 Local discovery measures link information and tracks
AS- and cluster-level channel viewership.
2 Global discovery collects measurements from each
cluster and builds a global network view.
3 Global control computes optimal distribution trees.
4 Local control merges local state with the global
decision and updates the forwarding table.
5 Global control updates DNS.
Distributed control loop:
1 Local discovery measures link information and tracks
AS- and cluster-level channel viewership.
2 Local control merges local state with the global
decision and updates the forwarding table.
The two loops have diﬀerent views of the system, and
use their information for diﬀerent purposes. The central
loop sees all clusters, the status of their links, and chan-
nel viewership information so that it can assign optimal
distribution trees. The distributed loop sees local link
conditions and video requests at one cluster as well as
a small amount of distributed state. The local agent
merges the controller’s decision with this information
and installs appropriate forwarding rules. Our hybrid
control plane strikes a balance between the high qual-
ity of a centralized system and the responsiveness of a
distributed system.
4 Hybrid control
Running two control loops in tandem can lead to chal-
lenges that destroy any beneﬁt that either control loops
would have had individually, resulting in a “worst of both
worlds” scenario, as hinted in §2.4. When distributed
decision-making is used, hybrid control handles this by
only considering end-to-end paths that provide enough
bandwidth. In this section we examine the interactions
of our central and distributed control loops in detail and
how we balance them, as well as how hybrid control
mitigates issues in the wide-area.
4.1 Central control
Central control takes in a global view of the network
(video requests and topology information) as input and
uses the algorithm described in §5 to calculate the op-
timal conﬁguration of distribution trees as output. To
avoid having a single point of failure, VDN uses multiple
geo-replicated controllers, synchronized with Paxos [31].
After making a decision, VDN’s central controller dis-
tributes it to individual clusters. To do this, the central
controller sends each cluster’s local agent a routing infor-
mation base (RIB) speciﬁc to that cluster, as shown in
Figure 7. VDN’s RIB contains information to support
hybrid decision-making in addition to the typical routing
information (e.g., a preﬁx and a next hop). In particular
the RIB maintains where the information came from
(centralized or distributed control), a version number
(timestamp), and a set of “evidence” providing the con-
text when this particular RIB entry was computed (link
and viewership information sent by this cluster to the
     Local control Local discovery Global control Global discovery Apache Forwarding Table Controller Local Agent 1 1 2 4 2 3 # # Global Control Loop Local Control Loop Cluster A Controller CDN Local  Agent     Cluster B V1: {S} -> {B} V2: {S} -> {A,B} Input&Output&DNS Clients      DNS 5 Central/Dist. Channel Next Hop Version Evidence Network Stats Viewership Stats CV0/800/*R215:20Link_1: 10MbpsLink_2: 15MbpsV0:{800}Kbps,3000 requestsDV0/*/*R115:23Link_1: 10MbpsLink_2: failedV0:{800}Kbps,3007 requestsChannel Version Next Hop V0/*/*15:23R1Routing Information Base Forwarding Information Base 315central controller when this decision was computed).
Evidence helps distributed control decide if it should
override the global decision.
The RIB gets merged with distributed control’s own
decision to become the Forwarding Information Base
(FIB), used by the data plane. If distributed control
decides nothing is amiss, the global RIB entry’s (channel
preﬁx, version number, next hop) tuple is used directly
as the FIB entry.
Discovery:
In order for central control to generate
good distribution trees, it needs to have up-to-date in-
formation on the state of the network and requests.
Keeping track of new requests is relatively simple at
the edge clusters. Estimating changes in link capacity
available to applications in overlay networks (e.g., due
routing changes, background traﬃc, or failures) is a well
studied topic [33, 36, 39], and is thus considered out of
scope in this work.
4.2 Distributed control
Distributed control keeps track of viewership and path
information of upstream neighbors to make quick local
decisions in response to changes in network performance,
viewership, and failures. The objective is to improve
responsiveness by avoiding the costly latency of central-
ized optimization. Thus, distributed control overrides
the central decision in response to dramatic changes.
Initial requests (DNS): VDN’s DNS takes into ac-
count the user’s geographic location and AS in order to
map them to the proper edge cluster as computed by the
central controller. If this particular AS has not previ-
ously been assigned to an edge cluster, simple heuristics
are used to provide a reasonable starting assignment
(e.g., an edge cluster that already is subscribed to this
video, an edge cluster that’s typically used by this loca-
tion/AS, etc.). This provides an initial instant mapping
of clients to edge clusters.
Distributing state: Clusters distribute video subscrip-
tion and link information to other nodes via a distance
vector-like algorithm to aide in reacting to large changes.
Each cluster periodically (e.g., every second) sends all
connected clusters at the next lower layer (see Figure 2)
its “distance” from each channel+bitrate ((cid:118), (cid:98)), denoted
(cid:100)((cid:118), (cid:98)), representing how many hops away it is from a
cluster that is subscribed to (cid:118) at bitrate (cid:98); if a cluster
is already subscribed to (cid:118) at bitrate (cid:98), then (cid:100)((cid:118), (cid:98)) at
that cluster is 0. Recall that we focus on live video,
thus caching is largely unhelpful; clusters only advertise
videos they are currently receiving.
When a cluster receives these distance values, it stores
them in a table (see Figure 8) along with the available
capacity of the bottleneck link on the path to that cluster
(cid:99)((cid:118), (cid:98)). The cluster propagates the distance to the closest
subscribed cluster with enough path capacity for this
bitrate downwards, similar to a distance vector protocol.
Figure 8: Example of the distributed state table
used in Algorithm 1.
Reacting to large changes:
If local discovery has
detected signiﬁcant changes in the local network state
or viewership used to calculate the most recent central
decision (i.e., the “evidence” in the RIB entry), it con-
cludes that its current central forwarding strategy is out
of date. Speciﬁcally, a cluster considers a RIB entry
stale if one or more of the following conditions are met:
• A link referenced in the evidence changes capacity
by some percentage (e.g., 20%) set by the operator.
• A link, node, or controller fails, as detected by a
• It receives a request it doesn’t have a FIB entry for.
timeout.
Input: request for channel (cid:118), bitrate (cid:98)
Output: next-hop cluster for channel (cid:118), bitrate (cid:98)
/* randomly pick a parent that has a
min-hop path to ((cid:118), (cid:98)) with enough
capacity to support delivery
(cid:117)(cid:115)(cid:101) (cid:102) (cid:117)(cid:108) := ∅
for (cid:112)(cid:97)(cid:114)(cid:101)(cid:110)(cid:116) in (cid:112)(cid:97)(cid:114)(cid:101)(cid:110)(cid:116) (cid:115) do
if (cid:100)((cid:118), (cid:98))(cid:118)(cid:105)(cid:97) (cid:112)(cid:97)(cid:114)(cid:101)(cid:110)(cid:116) == min((cid:100)((cid:118), (cid:98))) and
*/
(cid:99)((cid:118), (cid:98))(cid:118)(cid:105)(cid:97) (cid:112)(cid:97)(cid:114)(cid:101)(cid:110)(cid:116) > (cid:98) then
(cid:117)(cid:115)(cid:101) (cid:102) (cid:117)(cid:108) = (cid:117)(cid:115)(cid:101) (cid:102) (cid:117)(cid:108) ∪ {(cid:112)(cid:97)(cid:114)(cid:101)(cid:110)(cid:116)}
end
end
return pick at random((cid:117)(cid:115)(cid:101) (cid:102) (cid:117)(cid:108))
Algorithm 1: Distributed control algorithm.
If the global “evidence” is deemed invalid, a forwarding
strategy is computed by Algorithm 1, using local request
and link information as well as the distributed state
from upper nodes (Figure 8).
For example, when a cluster receives a request for a
video it’s not subscribed to, it uses its table to forward
the request to the parent “closest” (based on “distance”
(cid:100)() values) to the video that has enough spare path ca-
pacity ((cid:99)()). If there are no paths available the request
is denied, to be serviced by a diﬀerent edge cluster. It
breaks ties randomly to avoid groups of clusters poten-
tially overloading a high capacity cluster after failure. If
the parent is not already subscribed to the video, the
process repeats until a subscribed cluster is found. The
algorithm produces a forwarding strategy that VDN
places in the RIB and FIB of the cluster for future use
(Figure 7). Large-scale link variations, link failures, and
node failures, can all be handled by treating the existing
videos as new requests.
For Node A Via X Via Y Via Z To v0,b1 1, 5000!1, 1500!2, 4500!To v1,b1 2, 2000!1, 1500!2, 4000!To v2,b1 2, 5000!1, 1500!1, 3000!Distance & Capacity Table 316Discussion: The algorithm ensures that video streams
that deviate from global control only traverse paths with
enough spare capacity to support them. This is critical
because it means that (1) if the parent of a cluster
is already subscribed to the requested video (and has
ample bandwidth to the requesting cluster), requests to
this cluster will not propagate past the parent (i.e., 1
hop), (2) more generally, in an (cid:110)-level CDN (where (cid:110)
is typically 3 today), only (cid:110) − 1 clusters are aﬀected by
network / viewership changes as clusters only forward to
parents on a path with enough capacity, always reaching
source nodes after (cid:110) − 1 hops, and (3) clusters that
are involved in this algorithm will not be forced to