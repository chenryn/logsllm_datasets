DMOS counterparts, which normalize jargons before apply-
ing the THAN/ BERT model for classiﬁcation. By identify-
ing and learning the semantics of jargons, both DMOS and
DMOS_V2 achieve better performance, as shown in Table 2:
With JNA, the precision of THAN increases from 91.29% to
94.89%, which can reduce manual efforts for verifying false
alarms by over 41%. Similarly, JNA can improve the preci-
sion of T-BERT from 95.66% to 97.53%, thus cutting false
positives by over 43%.
4.2 Online Experiments
In this subsection, we introduce the business model of the
DMOS service and then analyze its performance based on a
Figure 6: Features Learned by THAN. On the left are tags and
the corresponding sentences in Chinese. On the right are the
English translation of the keywords highlighted in the same
color.
is not exactly 30% of the dataset. The ofﬂine testing dataset
consists of 20,958 defaced pages and 40,426 legitimate ones.
4.1.1 Comparison with Other Detection Schemes
As our baselines, we select the following state-of-the-art solu-
tions in the area of malicious web content detection:
• WAF: Most WAF vendors utilize a keyword-searching
approach to detect defacements. We acquired one of such
WAF devices from a major Chinese vendor and directly
used it for testing without further training.
• Saxe et al. [39]: This work divides the documents into
multiple sub-regions and then utilizes a deep learning
approach to aggregate these sub-regions for representing
and classifying web pages. Based on the feature extrac-
tion function partially described in the paper, we try our
best to re-implement its detection scheme.
• BoW model: The bag-of-words model is well known.
We ﬁrst extract the most important words based on TF-
IDF [14] and then use the popular XGBoost model [22]
to ﬁt the training data.
• HAN [46]: HAN is designed for structure-free document
classiﬁcation. Here, we reproduce the network to classify
the defaced web pages.
• FastText [31]: FastText is a simple and efﬁcient neural
network text classiﬁcation model. We use its open-source
library in our evaluation.
• BERT [24]: BERT is a multi-layer bidirectional Trans-
former [41]. It is the ﬁrst ﬁne-tuning based represen-
tation model that achieves state-of-the-art results for a
wide range of NLP tasks. The training process is dis-
cussed in Section 3.3.4.
For fair comparisons, all the baseline models (except the black-
box WAF) are trained with the same training dataset, and we
then ﬁne-tune them for the best performance. As shown in
Table 2, DMOS shows superior performance across the board.
3712    30th USENIX Security Symposium
USENIX Association
title官方金沙娱乐赌场网站无往不胜meta.keywords游戏赛车，北京赛车meta.description澳门新葡京赌场a官方金沙娱乐赌场网站官方金沙娱乐赌场meta.content,玩家认可，官方金沙娱乐赌场a年六月img长按识别二维码加关注a画腿赶超郑爽上热搜a舌尖大厨还原了第一的湖州粽美好心选Attention weight0.020.060.040.080.100.140.12No keywordsNo keywordsTable 3: Online Testing Results for Five Months
No. of Total
Pages (Sites)
38,526,989
(7298)
No. of Legitimate
Pages (Sites)
No. of Defaced
Pages (Sites)
37,994,394 (6474)
532,595 (824)
Schemes
WAF
DMOS
True
Positive
19,958
532,021
False
Positive
1634
65485
False
Negative
511,342
574
Precision
Recall
92.43% 3.76%
7.23%
89.04% 99.89% 94.15%
F1
* Web pages reported in this table have been de-duplicated by MD5 hash values. Websites have been de-duplicated by top-level domains.
* WAF failed in handling 1,239,717 pages due to various exceptions, which are not counted when evaluating its performance.
Any web page acquisition channel (e.g., search engines, or
ﬁrewalls, etc.) can be readily fed into DMOS.
Scheduler. The sheer volume of web pages makes real-time
detection challenging. We need a reliable, high-throughput,
low-latency platform for handling such data feeds. We use
Kafka to receive pages from crawlers and distribute them to
the available DMOS instance for detection.
Defacements Detection. We run 50 instances of DMOS on a
CentOS 7 machine with a 16-core CPU and 100GB memory.
Each instance consumes 1.2G memory. The classiﬁcation
of each web page, on average, can be completed within 0.3
second. Such runtime efﬁciency enables DMOS to crawl and
analyze millions of web pages for thousands of websites every
day.
4.2.3 Detection Accuracy
We collect performance statistics of DMOS for ﬁve months.
As presented in Table 3, DMOS has classiﬁed 38,526,989
web pages (after deduplication) across 7,298 websites in to-
tal. It has discovered 532,021 defaced web pages among
824 websites. Except for precision, DMOS outperforms the
widely-used WAF device in all other metrics, especially the
overall metric of F1. Note that we can only afford to compare
with WAF since it utilizes simple string search algorithms
and thus is efﬁcient enough to handle our large volume on-
line dataset in real-time. Although the precision of DMOS
is slightly lower, we notice that more than 90% of false posi-
tives occur in 67 websites only. Furthermore, these websites
typically share similar error-prone sentences. For practical de-
ployment, we can develop a whitelist based on such sentences.
More speciﬁcally, for any defaced pages identiﬁed by DMOS,
we can compute the string similarity of the believed-to-be
illicit sentences with the whitelist of this website. If they are
similar, we treat such defaced pages as false positives and
will not manually examine them. In this way, we can afford
to perform manual-review for pages reported online.
Manual veriﬁcation. Given the large volume of web pages, it
is very resource-consuming to obtain the ground truth for our
online experiment. We run the online experiment with DMOS
and a WAF device simultaneously and manually check every
page reported by either of the systems. Hence, we have accu-
rate true/false positives for both. To estimate false negatives,
Figure 7: Online Deployment of DMOS.
ﬁve-month long online experiment.
4.2.1 Usage of DMOS
It is straightforward for site owners/ administrators to use
DMOS. Speciﬁcally, they just need to submit a URL to reg-
ister the defacement monitoring service for their website.
DMOS then crawls the website from this URL regularly,
as described in Section 4.2.2. They also need to provide con-
tact information for defacement notiﬁcation. Depending on
the conﬁdence in the notiﬁcation, site owners can manually
investigate or directly remove the illicit content.
4.2.2 Deploying DMOS as a Cloud-Based Service
Figure 7 illustrates how DMOS is deployed as a commer-
cial cloud-based service. Firstly, the provider utilizes website
crawlers to acquire web pages covered by the service. It then
uses Kafka [16], a distributed event streaming, to distribute
the collected web pages to available DMOS instances. Fi-
nally, the defaced pages identiﬁed by DMOS are examined
manually by the provider who would notify the site owners
with true positives.
Website crawlers. We run website crawlers on 42 machines,
each with an 8-core CPU, 16GB Memory, and a 400 Mbps net-
work connection. A machine is conﬁgured to run 7 instances
of crawlers in parallel. Knowing a defaced website may, by
design, serve different pages to visitors originated from dif-
ferent geographical regions, we leverage a nation-wide cloud
infrastructure to deploy crawlers in parallel across different
provinces to eliminate the impact of page-fetch location. A
crawler also mimics various search-engine bots and normal
browsers by switching its User-Agent ﬁeld in the request to
ensure it can receive the defaced content. Note that we do
not limit DMOS to get web pages only via website crawlers.
USENIX Association
30th USENIX Security Symposium    3713
WebsiteCrawlersJob SchedulersManualReviewWeb PagePre-processingHTML ParserJargon NormalizationDefacement DetectionTHANDMoSrespectively. These results are consistent with the distribution
within the defaced dataset depicted in Figure 3a.
4.2.5 A Longitudinal Study
As the data pattern evolves, a model can become obsolete over
time. This is referred to as concept drift in the ﬁeld of ma-
chine learning. Under an adversarial setting, such as website
defacements, concept drift can be introduced intentionally by
the attacker to impede the detection performance of DMOS.
To measure the impact of concept drift, we perform a ﬁve-
month long longitudinal study using our online dataset. The
result is shown in Figure 9. It turns out DMOS performs
stable over time, which proves its generality: The recall is
always above 99% every month. The precision also stabilizes
around 88% after a slight decrease at ﬁrst. Under such grad-
ual and relatively minor drift, it is possible to maintain (or
even enhance) the performance of DMOS at its high level by
periodical ﬁne-tune its neural network parameters using new
data.
Figure 9: Variation of Performance of DMOS over Time
4.3 Comparison with Online URL Checkers
We also compare the performance of DMOS with popular
online URL safety-checking tools from major commercial
vendors including Baidu [7], Tencent [12] and VirusTotal [5].
Due to the API query limits of these commercial tools, we
can only use them to check hundreds of URLs. For fair com-
parisons, we select different categories of URLs in similar
proportions within our dataset. Given the fast-changing na-
ture of defaced web pages5, we complete our experiments
within two days to guarantee consistency. Table 4 shows that
DMOS signiﬁcantly outperforms the other tools according
to various metrics. We also observe that these commercial
products incorrectly assume websites with ICP (Internet Con-
tent Provider) license to be always secure, regardless of their
content, which results in their high false negative rate.
5Site owners will remove the illicit content immediately after detection.
Figure 8: Topic Distribution of Defaced Sites in Online Tests
we manually check every page exclusively reported by the
WAF. This gives us FNDMoS, i.e., the number of pages missed
by DMOS but captured by the WAF. For pages reported nei-
ther by DMOS nor the WAF, we use alarming keywords (de-
scribed in Appendix A) to ﬁlter out about 95% of innocent
pages. We then perform a sampled manual checking 4 for
the remaining suspicious pages. This gives us an estimated
FNboth, i.e., the number of defaced pages missed by both of
the systems. With T P being the true positives for DMOS, the
total false negatives and recall can be determined by:
FNDMoS = FNDMoS + FNboth , RecallDMoS =
T P + FNDMoS
A similar procedure is applied to compute FNWAF and
RecallWAF.
T P
This manual veriﬁcation process was continuously con-
ducted by specialized data annotators of our collaborating in-
dustrial partner on a daily basis during our ﬁve-month online
experiment. Nevertheless, it is still possible to omit defaced
web pages. As such, the actual recall of DMOS can be lower.
4.2.4 Statistics of Defaced Websites
Figure 8 presents the categorical distribution of the victim
websites with defacements detected. The wide-ranging cate-
gories of defaced websites detected by DMOS demonstrate
its ability to generalize beyond the training dataset. In particu-
lar, the categories of government, news media, education, and
medical institutions occupy the majority of defacement targets.
A signiﬁcant portion of defaced websites are quite popular:
Amongst 824 websites where defacements are detected, 2
websites belong to the Top 5 list of the overall category ; 22
websites rank within the Top 50, and 105 rank within 2000 in
their own categories.
We also categorize defaced web pages by the type of illicit
goods and services they promoted. Gambling and porn are
found to be the most popular topics in defacement campaigns,
accounting for 61% and 27% of the promoted targets, respec-
tively. The game and sales categories account for 9% and 3%
4We uniformly sample 10 pages from each website every day. If any
page is defaced, we manually check all pages of the website with similar
alarming keywords.
3714    30th USENIX Security Symposium
USENIX Association
Table 4: Comparison with other Online URL Checkers
Table 5: Detection Performance for English Dataset
Schemes
Tencent
VirusTotal
Baidu
DMOS
No. of
Legitimate Pages
No. of
Defaced Pages
235
235
235
235
190
190
190
190
Precision
Recall
F1
50.00%
6.73% 11.86%
57.14% 15.38% 24.24%
63.41% 50.49% 56.22%
98.37% 97.32% 97.85%
Methods
HAN
BERT
DMOS
(THAN)
DMOS_V2
(T-BERT)
d
d
Recall
dataset
dataset
Dcra f tS
Precision
Dreal
Precision
Recall
90.39% 91.21% 86.43% 90.12% 93.54% 95.86%
91.51% 94.55% 88.23% 93.72% 95.81% 96.43%
92.54% 92.46% 90.65% 91.63% 94.72% 95.98%
Dcra f tL
Precision
dataset
Recall
d
94.10% 95.35% 92.11% 95.42% 96.33% 97.12%
4.4 Adapt DMOS for Other Languages
Since our collaborating industrial partner’s main market is in
China, we have focused on detecting defacements in Chinese
web pages so far. Note however that, our approach is general
in nature and can be applied to other languages with the
following steps:
• Apply similar approaches to collect datasets for training.
• Replace the jargon normalization algorithm with
language-speciﬁc de-obfuscation techniques if any.
• Take the language-speciﬁc word embeddings as the input
and train the THAN model as usual.
In what follows, we demonstrate how DMOS can be adapted
to detect defacements of English web pages.