    * `soft/hard`: When the mount option `hard` is set, if the NFS server crashes or becomes unresponsive, the NFS requests will be retried indefinitely. You can set the mount option `intr`, so that the process can be interrupted. When the NFS server comes back online, the process can be continued from where it was while the server became unresponsive.
      When the option `soft` is set, the process will be reported an error when the NFS server is unresponsive after waiting for a period of time (defined by the `timeo` option). In certain cases `soft` option can cause data corruption and loss of data. So, it is recommended to use `hard` and `intr` options.
    * `noexec`: Prevents execution of binaries on mounted file systems. This is useful if the system is mounting a non-Linux file system via NFS containing incompatible binaries.
    * `nosuid`: Disables set-user-identifier or set-group-identifier bits. This prevents remote users from gaining higher privileges by running a setuid program.
    * `tcp`: Specifies the NFS mount to use the TCP protocol.
    * `udp`: Specifies the NFS mount to use the UDP protocol.
    * `nofail`: Prevent issues when rebooting the host. The downside is that if you have services that depend on the volume to be mounted they won't behave as expected.
* New: [Fix limit on the number of inotify watches.](linux_snippets.md#fix-limit-on-the-number-of-inotify-watches)
    Programs that sync files such as dropbox, git etc use inotify to notice changes to the file system. The limit can be see by -
    ```bash
    cat /proc/sys/fs/inotify/max_user_watches
    ```
    For me, it shows `65536`. When this limit is not enough to monitor all files inside a directory it throws this error.
    If you want to increase the amount of inotify watchers, run the following in a terminal:
    ```bash
    echo fs.inotify.max_user_watches=100000 | sudo tee -a /etc/sysctl.conf && sudo sysctl -p
    ```
    Where `100000` is the desired number of inotify watches.
* New: [Get class of a window.](linux_snippets.md#get-class-of-a-window)
    Use `xprop` and click the window.
    Get the current brightness level with `cat /sys/class/backlight/intel_backlight/brightness`. Imagine it's `1550`, then if you want to lower the brightness use:
    ```bash
    sudo echo 500 > /sys/class/backlight/intel_backlight/brightness
    ```
* New: [SSH tunnel.](linux_snippets.md#ssh-tunnel)
    ```bash
    ssh -D 9090 -N -f user@host
    ```
* New: [Fix the SSH client kex_exchange_identification: read: Connection reset by peer error.](linux_snippets.md#fix-the-ssh-client-kex_exchange_identification:-read:-connection-reset-by-peer-error)
    Restart the `ssh` service.
* New: [Automatic reboot after power failure.](linux_snippets.md#automatic-reboot-after-power-failure)
    That's not something you can control in your operating system. That's what the BIOS is for. In most BIOS setups there'll be an option like After power loss with possible values like Power off and Reboot.
    You can also edit `/etc/default/grub` and add:
    ```
    GRUB_RECORDFAIL_TIMEOUT=5
    ```
    Then run:
    ```bash
    sudo update-grub
    ```
    This will make your machine display the boot options for 5 seconds before it boot the default option (instead of waiting forever for you to choose one).
* New: [Add sshuttle information link.](linux_snippets.md#ssh-tunnel)
    If you need a more powerful ssh tunnel you can try [sshuttle](https://sshuttle.readthedocs.io/en/stable/overview.html)
* New: [Reset failed systemd services.](linux_snippets.md#reset-failed-systemd-services)
    Use systemctl to remove the failed status. To reset all units with failed status:
    ```bash
    systemctl reset-failed
    ```
    or just your specific unit:
    ```bash
    systemctl reset-failed openvpn-server@intranert.service
    ```
* New: [Get the current git branch.](linux_snippets.md#get-the-current-git-branch)
    ```bash
    git branch --show-current
    ```
* New: [Install latest version of package from backports.](linux_snippets.md#install-latest-version-of-package-from-backports)
    Add the backports repository:
    ```bash
    vi /etc/apt/sources.list.d/bullseye-backports.list
    ```
    ```
    deb http://deb.debian.org/debian bullseye-backports main contrib
    deb-src http://deb.debian.org/debian bullseye-backports main contrib
    ```
    Configure the package to be pulled from backports
    ```bash
    vi /etc/apt/preferences.d/90_zfs
    ```
    ```
    Package: src:zfs-linux
    Pin: release n=bullseye-backports
    Pin-Priority: 990
    ```
* New: [Rename multiple files matching a pattern.](linux_snippets.md#rename-multiple-files-matching-a-pattern)
    There is `rename` that looks nice, but you need to install it. Using only `find` you can do:
    ```bash
    find . -name '*yml' -exec bash -c 'echo mv $0 ${0/yml/yaml}' {} \;
    ```
    If it shows what you expect, remove the `echo`.
* New: [Force ssh to use password authentication.](linux_snippets.md#force-ssh-to-use-password-authentication)
    ```bash
    ssh -o PreferredAuthentications=password -o PubkeyAuthentication=no PI:EMAIL
    ```
    feat(linux_snippets#Do a tail -f with grep): Do a tail -f with grep
    ```bash
    tail -f file | grep --line-buffered my_pattern
    ```
* New: [Check if a program exists in the user's PATH.](linux_snippets.md#check-if-a-program-exists-in-the-user's-path)
    ```bash
    command -v 
    ```
    Example use:
    ```bash
    if ! command -v  &> /dev/null
    then
        echo " could not be found"
        exit
    fi
    ```
* New: [Add interesting tools to explore.](qbittorrent.md#tools)
    - [qbittools](https://github.com/buroa/qbittools): a feature rich CLI for the management of torrents in qBittorrent.
    - [qbit_manage](https://github.com/StuffAnThings/qbit_manage): tool will help manage tedious tasks in qBittorrent and automate them.
* New: [Limit the resources a docker is using.](linux_snippets.md#limit-the-resources-a-docker-is-using)
    You can either use limits in the `docker` service itself, see [1](https://unix.stackexchange.com/questions/537645/how-to-limit-docker-total-resources) and [2](https://www.freedesktop.org/software/systemd/man/systemd.resource-control.html).
    Or/and you can limit it for each docker, see [1](https://www.baeldung.com/ops/docker-memory-limit) and [2](https://docs.docker.com/config/containers/resource_constraints/).
* New: [Wipe a disk.](linux_snippets.md#wipe-a-disk)
    Overwrite it many times [with badblocks](hard_drive_health.md#check-the-health-of-a-disk-with-badblocks).
    ```bash
    badblocks -wsv -b 4096 /dev/sde | tee disk_wipe_log.txt
    ```
* New: [Impose load on a system to stress it.](linux_snippets.md#impose-load-on-a-system-to-stress-it)
    ```bash
    sudo apt-get install stress
    stress --cpu 2
    ```
    That will fill up the usage of 2 cpus. To run 1 vm stressor using 1GB of virtual memory for 60s, enter:
    ```bash
    stress --vm 1 --vm-bytes 1G --vm-keep -t 60s
    ```
    You can also stress io with `--io 4`, for example to spawn 4 workers.
* New: [Get the latest tag of a git repository.](linux_snippets.md#get-the-latest-tag-of-a-git-repository)
    ```bash
    git describe --tags --abbrev=0
    ```
* New: [Configure gpg-agent cache ttl.](linux_snippets.md#configure-gpg-agent-cache-ttl)
    The user configuration (in `~/.gnupg/gpg-agent.conf`) can only define the default and maximum caching duration; it can't be disabled.
    The `default-cache-ttl` option sets the timeout (in seconds) after the last GnuPG activity (so it resets if you use it), the `max-cache-ttl` option set the timespan (in seconds) it caches after entering your password. The default value is 600 seconds (10 minutes) for `default-cache-ttl` and 7200 seconds (2 hours) for max-cache-ttl.
    ```
    default-cache-ttl 21600
    max-cache-ttl 21600
    ```
    For this change to take effect, you need to end the session by restarting `gpg-agent`.
    ```bash
    gpgconf --kill gpg-agent
    gpg-agent --daemon --use-standard-socket
    ```
* New: [Get return code of failing find exec.](linux_snippets.md#get-return-code-of-failing-find-exec)
    When you run `find . -exec ls {} \;` even if the command run in the `exec` returns a status code different than 0 [you'll get an overall status code of 0](https://serverfault.com/questions/905031/how-can-i-make-find-return-non-0-if-exec-command-fails) which makes difficult to catch errors in bash scripts.
    You can instead use `xargs`, for example:
    ```bash
    find /tmp/ -iname '*.sh' -print0 | xargs -0 shellcheck
    ```
    This will run `shellcheck file_name` for each of the files found by the `find` command.
* New: [Accept new ssh keys by default.](linux_snippets.md#accept-new-ssh-keys-by-default)
    While common wisdom is not to disable host key checking, there is a built-in option in SSH itself to do this. It is relatively unknown, since it's new (added in Openssh 6.5).
    This is done with `-o StrictHostKeyChecking=accept-new`. Or if you want to use it for all hosts you can add the next lines to your `~/.ssh/config`:
    ```
    Host *
      StrictHostKeyChecking accept-new
    ```
    WARNING: use this only if you absolutely trust the IP\hostname you are going to SSH to:
    ```bash
    ssh -o StrictHostKeyChecking=accept-new mynewserver.example.com
    ```
    Note, `StrictHostKeyChecking=no` will add the public key to `~/.ssh/known_hosts` even if the key was changed. `accept-new` is only for new hosts. From the man page:
    > If this flag is set to “accept-new” then ssh will automatically add new host keys to the user known hosts files, but will not permit connections to hosts with changed host keys. If this flag is set to “no” or “off”, ssh will automatically add new host keys to the user known hosts files and allow connections to hosts with changed hostkeys to proceed, subject to some restrictions. If this flag is set to ask (the default), new host keys will be added to the user known host files only after the user has confirmed that is what they really want to do, and ssh will refuse to connect to hosts whose host key has changed. The host keys of known hosts will be verified automatically in all cases.
* New: [Do not add trailing / to ls.](linux_snippets.md#do-not-add-trailing-/-to-ls)
    Probably, your `ls` is aliased or defined as a function in your config files.
    Use the full path to `ls` like:
    ```bash
    /bin/ls /var/lib/mysql/
    ```
* New: [Convert png to svg.](linux_snippets.md#convert-png-to-svg)
    Inkscape has got an awesome auto-tracing tool.
    - Install Inkscape using `sudo apt-get install inkscape`
    - Import your image
    - Select your image
    - From the menu bar, select Path > Trace Bitmap Item
    - Adjust the tracing parameters as needed
    - Save as svg
    Check their [tracing tutorial](https://inkscape.org/en/doc/tutorials/tracing/tutorial-tracing.html) for more information.
    Once you are comfortable with the tracing options. You can automate it by using [CLI of Inkscape](https://inkscape.org/en/doc/inkscape-man.html).
* New: [Redirect stdout and stderr of a cron job to a file.](linux_snippets.md#redirect-stdout-and-stderr-of-a-cron-job-to-a-file)
    ```
    */1 * * * * /home/ranveer/vimbackup.sh >> /home/ranveer/vimbackup.log 2>&1
    ```
* New: [Error when unmounting a device Target is busy.](linux_snippets.md#error-when-unmounting-a-device-target-is-busy)
    - Check the processes that are using the mountpoint with `lsof /path/to/mountpoint`
    - Kill those processes
    - Try the umount again
    If that fails, you can use `umount -l`.
### [Rtorrent](afew.md)
* Correction: Remove unneeded dependencies when installing.
* Correction: Deprecate it in favour of qbittorrent.
    Use [qbittorrent](qbittorrent.md) instead.
### [aleph](aleph.md)
* New: Add application operations.
    - [How to upgrade it](aleph.md#upgrade-aleph)
    - [Create Aleph admins](aleph.md#create-aleph-admins)
    - [Remove a group](aleph.md#remove-a-group)
* New: [Ingest gets stuck.](aleph.md#ingest-gets-stuck)
    It looks that Aleph doesn't yet give an easy way to debug it. It can be seen in the next webs:
    - [Improve the UX for bulk uploading and processing of large number of files](https://github.com/alephdata/aleph/issues/2124)
    - [Document ingestion gets stuck effectively at 100%](https://github.com/alephdata/aleph/issues/1839)
    - [Display detailed ingestion status to see if everything is alright and when the collection is ready](https://github.com/alephdata/aleph/discussions/1525)
    Some interesting ideas I've extracted while diving into these issues is that:
    - You can also upload files using the [`alephclient` python command line tool](https://github.com/alephdata/alephclient)
    - Some of the files might fail to be processed without leaving any hint to the uploader or the viewer.
      - This results in an incomplete dataset and the users don't get to know that the dataset is incomplete. This is problematic if the completeness of the dataset is crucial for an investigation.
      - There is no way to upload only the files that failed to be processed without re-uploading the entire set of documents or manually making a list of the failed documents and re-uploading them
      - There is no way for uploaders or Aleph admins to see an overview of processing errors to figure out why some files are failing to be processed without going through docker logs (which is not very user-friendly)
    - There was an attempt to [improve the way ingest-files manages the pending tasks](https://github.com/alephdata/aleph/issues/2127), it's merged into the [release/4.0.0](https://github.com/alephdata/ingest-file/tree/release/4.0.0) branch, but it has [not yet arrived `main`](https://github.com/alephdata/ingest-file/pull/423).
    There are some tickets that attempt to address these issues on the command line:
    - [Allow users to upload/crawl new files only](https://github.com/alephdata/alephclient/issues/34)
    - [Check if alephclient crawldir was 100% successful or not](https://github.com/alephdata/alephclient/issues/35)
    I think it's interesting either to contribute to `alephclient` to solve those issues or if it's complicated create a small python script to detect which files were not uploaded and try to reindex them and/or open issues that will prevent future ingests to fail.
### [gitsigns](gitsigns.md)
* New: Introduce gitsigns.
    [Gitsigns](https://github.com/lewis6991/gitsigns.nvim) is a neovim plugin to create git decorations similar to the vim plugin [gitgutter](https://github.com/airblade/vim-gitgutter) but written purely in Lua.
    Installation:
    Add to your `plugins.lua` file:
    ```lua
      use {'lewis6991/gitsigns.nvim'}
    ```
    Install it with `:PackerInstall`.
    Configure it in your `init.lua` with:
    ```lua
    -- Configure gitsigns
    require('gitsigns').setup({
      on_attach = function(bufnr)
        local gs = package.loaded.gitsigns
        local function map(mode, l, r, opts)
          opts = opts or {}
          opts.buffer = bufnr
          vim.keymap.set(mode, l, r, opts)