the adversary any output, beyond the corrupted parties’ shares on the input
wires to the multiplication. S derives these values from the honest parties’
24
4. Simulation of veriﬁcation stage:
mult, then S stores cheat.
shares using the complete procedure described in Section 2.1. If S receives
any d ∕= 0 from A in any of the calls to F add
(a) If S did not store cheat in the previous step, then it plays the role of
the honest parties in Protocol 4.2, sending honest values. The only place
that the values sent by the honest parties depend on the input shares is
when opening [vj]2t, but since this is masked, S chooses a random vj ∈ F
and sets the honest parties’ shares in the opening by running complete
on the corrupted parties’ shares and vj (it can do this since it knows the
shares the corrupted parties’ hold). Finally, as long as no invalid values
are sent by A for the openings, it simulates the output of FcheckZero being
accept.
(b) If S did store cheat in the circuit-emulation phase, then it works in
exactly as in the previous case, except that it simulates the output of
FcheckZero being reject.
5. If there was no abort until this point, S uses complete on the output values it
received for the corrupted parties along with their shares, in order to generate
the honest parties’ shares that would be sent at this point. In addition, for
any honest party Pj for whom all shares sent by the corrupted parties in
reconstruct are correct, it sends (continue, j) to the trusted party computing
f to notify it to provide output to Pj in the ideal model; otherwise it sends
(abort, j) to notify that Pj not receive output in the ideal model.
We argue that the simulation is statistically close to the real execution when us-
ing the information-theoretic multiplication protocol described in Protocol 5.1,
and is computationally indistinguishable when using the computational multi-
plication protocol described in Section 5.2. Regarding the secret-sharing phase,
this follows immediately from the property of the secret sharing scheme (that
guarantees that sharings of 0 and other values are identically distributed), and
by the fact that Protocol 4.1 reveals nothing. In order to see this latter fact,
observe that Fcoin and Frand are guaranteed to be secure (and so we actually
prove security in the (Fcoin,Frand)-hybrid model), and the only values opened
are [u1]t, . . . , [uδ]t. Now, this masking is only of degree-t. However, all honest
parties provide input sharings of degree-t only (since that is what the protocol
speciﬁcation says to do). Thus, all honest values are perfectly masked, and using
0 or the honest party’s correct input value yields the same distribution.
Next, the circuit emulation phase is perfectly simulated, by the use of the
F add
mult-hybrid model. Note that this assumes that all inputs are of degree-t. How-
ever, this is guaranteed by the execution of Protocol 4.1 on the input values,
and the fact that all outputs from F add
mult are of degree-t (by the functionality
deﬁnition). Observe that by Lemma 4.1, the probability that a sharing is not of
degree-t but the parties did not abort, is at most 2−s.
Finally, for the multiplication veriﬁcation phase, recall that all values on all
wires are guaranteed to be of degree-t, and that [ρ′j]2t is generated using F double
which is guaranteed to therefore be of degree-2t. Thus, the polynomial sharing
deﬁned by 󰁓N
k · (wik · wjk − wℓk ) via local multiplications and additions
k=1 bj
rand
25
is guaranteed to be of degree-2t. This implies that [vj]2t, which is obtained by
adding [ρ′j]2t to this polynomial sharing, is a truly random sharing. Thus, S
perfectly simulates the messages received by the corrupted parties (except with
probability 2−s when inputs were not of degree-t). The output of FcheckZero is
also simulated perfectly, except with probability at most 2−s, which occurs when
cheat was stored and yet the polynomial deﬁned is a zero-polynomial, and so the
honest parties would not abort. This is bound by probability 2−s, as shown in
Lemma 4.2.
Regarding the computational case, this follows from the exact same reason-
mult-hybrid model to the real model is
ing, except that the transition from the F add
computational instead of information-theoretic.
Relaxing the requirement. Informally speaking, in the proof of Theorem 6.1 we
only really utilize the fact that the adversary’s view alone in πmult can be simu-
lated (i.e., a privacy requirement), and that a simulator can detect if a party has
cheated. This is a very mild requirement on πmult and we leave the task of formal-
izing it (and generalizing the protocol to arbitrary linear secret sharing schemes)
to future work. Nevertheless, we stress that the DN multiplication protocol is
the most eﬃcient known, even for semi-honest, and thus this makes no diﬀerence
in practice right now. However, if a more eﬃcient protocol is found later and it
does not fulﬁl security up to additive attacks, then this can be beneﬁcial.
Complexity: We count the number of elements per party sent for the computa-
tionally secure variant of the protocol. Let NI , N×, NO denote the number of
input, multiplication, and output gates in C, respectively. Then, each input gate
is a single call to share which costs 1 element on average per party, and each out-
put gate is a single call to reconstruct which is also exactly 1 element per party.
As we have described in Section 5.2, the cost of πmult for the case of t  2δ + 2 (which holds for large enough n as well as for most reasonable
parameters), the cost of veriﬁcation is 6 2
3 n elements per party. Without making
this assumption, the cost is O(δ2n) elements per party.
3 · N× + NO + 6 2
We conclude that the total number of ﬁeld elements sent per party in the
protocol is NI + 2 2
3 n (when assuming n − 2t > 2δ + 2) and
at most NI + 2 2
3 · N× + NO + O(δ2n) (even for a very small ﬁeld and small
number of parties). Observing that the cost of semi-honest alone is exactly
NI + 2 2
3 · N× + NO, we have that the only overhead occurred in order to obtain
malicious security is the additive factor of between 6 2
3 n and O(δ2n) elements
sent per party.
Achieving fairness. Our protocol can be easily extended to guarantee fairness.
Since t < n/3, it suﬃces for all honest parties to ﬁrst agree (via a Byzantine
Agreeement protocol) that they did not receive any abort in the veriﬁcation. If
this is the case, then in the opening, all honest parties are guaranteed to have
at least 2t + 1 honest shares and at most t corrupt shares. Thus, using standard
26
error correcting techniques, the honest parties can determine the correct values
and output them. Observe that this method adds very little cost to the protocol.
7 Experiments and Evaluation
We implemented our protocol and carried out extensive experiments. Our im-
plementation is single threaded, to facilitate accurate comparisons with other
protocols. Our implementation will be made open source upon publication.
7.1 Experiment 1 – Our Protocol Comparison
The aim of this experiment was to understand the eﬃciency gain achieved of the
PRF version of DN-multiplication of Section 5.2 versus the information-theoretic
version of Section 5.1. Theoretically, the saving is over 40%. However, this is
in communication, and the necessity to compute many PRF invocations (using
AES) may impact the running time. In addition, we analyzed the additional cost
incurred for achieving malicious security over semi-honest security. We counted
the amount of time spent on veriﬁcation in the malicious protocol (which is
the only diﬀerence between the semi-honest and malicious variants) as well as
running independent semi-honest executions.
We ran the above experiment using a circuit of 1,000,000 multiplication gates
of depth-20, with a 61-bit ﬁeld (deﬁned by a Mersenne prime). The experiment
was run on c5.xlarge instances on AWS with all parties in the EAST-US region.
The results appear in Table 1 and Table 2, and in Figure 1.
All running times are given in milliseconds, and are the average of 20 execu-
tions. The columns titled “verify time” and “% on verify” describe the amount
of time spent on the veriﬁcation procedures of Section 4 in the malicious pro-
tocol, whereas the column titled “semi-honest” is an independent execution of
the completely semi-honest protocol (without veriﬁcation inside Protocol 5.1
or Protocol 6.1) with the “% diﬀerence” being between the full malicious and
independent semi-honest executions.
Observe that the percentage of time spent on veriﬁcation is small, and de-
creases as the number of parties increases. Since this veriﬁcation step has com-
munication that is independent of the circuit size (and only cheat local compu-
tation), this is also true as circuits get bigger. In particular, for 100 parties, the
percentage of time spent on veriﬁcation is a few percent only. Observe that the
running time of the purely semi-honest protocol is typically farther away; this
is surprising since the malicious protocol without the veriﬁcation is exactly the
same. The only explanation that we have is that the variance on the network at
diﬀerent running times has a big impact when the running times are so low.
Regarding the comparison between the information-theoretic and PRF ver-
sions, the diﬀerence in running times is more signiﬁcant for a smaller number
of parties, less than 50% for up to 60 parties and about 40% for over 60 parties
(with an anomalous point at 100 parties). This matches the theoretical expec-
tation (and in fact, even more for up to 60 parties). The fact that the PRF
computations are insigniﬁcant is due to the fact that AES-NI makes such com-
putations very low cost.
27
Parties Malicious Verify Time % on Verify Semi-Honest % Diﬀerence
10
20
30
40
50
60
70
80
90
100
401
936
1241
1598
1891
2512
2585
2974
3689
3999
36
53
68
69
62
126
75
97
120
142
8.9%
5.6%
5.5%
4.3%
3.3%
5.0%
2.9%
3.2%
3.3%
3.6%
401
828
1168
1343
1985
2219
2870
2884
3529
4089
0%
11.5%
5.9%
15.9%
-5.0%
11.7%
-11.0%
3.0%
4.3%
-2.2%
Table 1. Information-theoretic multiplication protocol version of Section 5.1
Parties Malicious Verify Time % on Verify Semi-Honest % Diﬀerence
10
20
30
40
50
60
70
80
90
100
187
374