title:Gotta Catch'Em All: Using Honeypots to Catch Adversarial Attacks on
Neural Networks
author:Shawn Shan and
Emily Wenger and
Bolun Wang and
Bo Li and
Haitao Zheng and
Ben Y. Zhao
Gotta Catch ’Em All: Using Honeypots to Catch Adversarial
Attacks on Neural Networks
Shawn Shan
PI:EMAIL
University of Chicago
Bo Li
PI:EMAIL
UIUC
Emily Wenger
PI:EMAIL
University of Chicago
Haitao Zheng
PI:EMAIL
University of Chicago
Bolun Wang
PI:EMAIL
University of Chicago
Ben Y. Zhao
PI:EMAIL
University of Chicago
ABSTRACT
Deep neural networks (DNN) are known to be vulnerable to adver-
sarial attacks. Numerous efforts either try to patch weaknesses in
trained models, or try to make it difﬁcult or costly to compute ad-
versarial examples that exploit them. In our work, we explore a new
“honeypot” approach to protect DNN models. We intentionally in-
ject trapdoors, honeypot weaknesses in the classiﬁcation manifold
that attract attackers searching for adversarial examples. Attackers’
optimization algorithms gravitate towards trapdoors, leading them
to produce attacks similar to trapdoors in the feature space. Our
defense then identiﬁes attacks by comparing neuron activation sig-
natures of inputs to those of trapdoors.
In this paper, we introduce trapdoors and describe an implemen-
tation of a trapdoor-enabled defense. First, we analytically prove
that trapdoors shape the computation of adversarial attacks so that
attack inputs will have feature representations very similar to those
of trapdoors. Second, we experimentally show that trapdoor-protected
models can detect, with high accuracy, adversarial examples gener-
ated by state-of-the-art attacks (PGD, optimization-based CW, Elas-
tic Net, BPDA), with negligible impact on normal classiﬁcation.
These results generalize across classiﬁcation domains, including im-
age, facial, and trafﬁc-sign recognition. We also present signiﬁcant
results measuring trapdoors’ robustness against customized adap-
tive attacks (countermeasures).
CCS CONCEPTS
• Security and privacy; • Computing methodologies → Neural
networks; Artiﬁcial intelligence; Machine learning;
KEYWORDS
Neural networks; Adversarial examples; Honeypots
ACM Reference Format:
Shawn Shan, Emily Wenger, Bolun Wang, Bo Li, Haitao Zheng, and Ben Y.
Zhao. 2020. Gotta Catch ’Em All: Using Honeypots to Catch Adversarial At-
tacks on Neural Networks. In 2020 ACM SIGSAC Conference on Computer
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’20, November 9–13, 2020, Virtual Event, USA
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7089-9/20/11. . . $15.00
https://doi.org/10.1145/3372297.3417231
and Communications Security (CCS ’20), November 9–13, 2020, Virtual
Event, USA. ACM, New York, NY, USA, 17 pages.
https://doi.org/10.1145/3372297.3417231
1 INTRODUCTION
Deep neural networks (DNNs) are vulnerable to adversarial attacks
[39, 46], in which, given a trained model, inputs can be modiﬁed in
subtle ways (usually undetectable by humans) to produce an incor-
rect output [2, 10, 34]. These modiﬁed inputs are called adversarial
examples, and they are effective in fooling models trained on dif-
ferent architectures or different subsets of training data. In practice,
adversarial attacks have proven effective against models deployed
in real-world settings such as self-driving cars, facial recognition,
and object recognition systems [24, 25, 41].
Recent results in adversarial machine learning include a long list
of proposed defenses, each proven later to be vulnerable to stronger
attacks, and all focused on either mitigating or obfuscating adversar-
ial weaknesses. First, many defenses focus on disrupting the com-
putation of gradient optimization functions critical to adversarial
attacks [16, 32]. These “gradient obfuscation” defenses (e.g. [3, 15,
18, 31, 38, 42, 49]) have been proven vulnerable to black-box at-
tacks [34] as well as approximation techniques like BPDA [2] that
avoid gradient computation. Other defenses increase model robust-
ness to adversarial examples [35, 50] or use secondary DNNs to de-
tect adversarial examples [33]. Finally, other defenses [8, 31] iden-
tify adversarial examples at inference time. All of these fail or are
signiﬁcantly weakened against stronger adversarial attacks or high
conﬁdence adversarial examples [2, 7–9, 21].
History suggests it may be impossible in practice to prevent ad-
versaries from computing effective adversarial examples, and an
alternative approach to model defense is sorely needed. What if,
instead of trying to prevent attackers from computing effective ad-
versarial examples, we instead design a “honeypot” for attackers,
by inserting a subset of chosen model vulnerabilities, making them
easy to discover (and hard to ignore)? We could ensure that when
attackers create adversarial examples, they ﬁnd our honeypot pertur-
bations instead of natural weaknesses. When attackers apply these
honeypot perturbations to their inputs, they are easily identiﬁed by
our model because of their similarity to our chosen honeypot.
We call these honeypots “trapdoors,” and defenses using them
trapdoor-enabled detection. Consider a scenario where, starting from
an input x, the attacker searches for an adversarial perturbation that
induces a misclassiﬁcation from the correct label yx to some target
yt . This is analogous to looking for a “shortcut” through the model
a) Choose Label(s) to Defend
b) Create / Deploy Trapdoored Model
Defend label (y):
20 km speed limit
Label y’s
Trapdoor
 Trapdoor
Instances
...
Benign
Instances 
Trapdoored model
Train
...
c) Compute “signature” of Trapdoor
    Filter any inputs w/ similar signature    
Input Similar to Trapdoor:
Reject & Sound Alarm
...
Benign
Input
Compute Adver-
Adversarial 
sarial Input
example 
Adversarial Example Against Label (y)
...
Output
Label
Misclassification Attack
Trapdoored model
Figure 1: Overview of the trapdoor defense. a) We choose which target label(s) to defend. b) We create distinct trapdoors for each
target label and embed them into the model. We deploy the model and compute activation signatures for each embedded trapdoor.
c) An adversary with access to the model constructs an adversarial example. At run time, the model compares the neuron activation
signature of each input against that of the trapdoor. Thus it recognizes and rejects the adversarial example and sounds the alarm.
from yx to yt that involves a small change to x that invokes the
shortcut to yt . Along these lines, trapdoors create artiﬁcial shortcuts
embedded by the model owner that are easier to locate and smaller
than any natural weaknesses attackers are searching for. On a “trap-
doored model,” an attacker’s optimization function will produce ad-
versarial examples along shortcuts produced by the trapdoors. Each
trapdoor has minimal impact on classiﬁcation of normal inputs, but
leads attackers to produce adversarial inputs whose similarity to the
trapdoor makes them easy to detect.
In this paper, we ﬁrst introduce the trapdoor-enabled defense and
then describe, analyze, and evaluate an implementation of trapdoors
using techniques similar to that of backdoor attacks [17, 29]. Back-
doors are data poisoning attacks in which models are exposed to
additional, corrupt training data samples so they learn an unusual
classiﬁcation pattern. This pattern is inactive when the model oper-
ates on normal inputs, but is activated when the model encounters
an input on which a speciﬁc backdoor “trigger” is present. Trap-
door honeypots are similar to backdoors in that they use similar
embedding methods to associate certain input patterns with a mis-
classiﬁcation. But while backdoors are used by attackers to cause
misclassiﬁcation given a known “trigger,” trapdoors provide a hon-
eypot that “shields” and prevents attackers from discovering natu-
ral weaknesses in the model. Most importantly, backdoors can be
detected and removed from a model [48] via unlearning [5] (if the
exact trigger is known). However, these countermeasures do not cir-
cumvent models defended by trapdoors: even when attackers are
able to unlearn trapdoors, adversarial examples computed from the
resulting clean model do not transfer to the trapdoored models of
interest (§7.1).
Figure 1 presents a high-level illustration of the defense. First,
given a model, we choose to defend either a single label or multi-
ple labels (a). Second, for each protected label y, we train a distinct
trapdoor into the model to defend against adversarial misclassiﬁca-
tion to y (b). For each embedded trapdoor, we compute its trapdoor
signature (a neuron activation pattern at an intermediate layer), and
use a similarity function to detect adversarial attacks that exhibit
similar activation patterns (c). Adversarial examples produced by
attackers on trapdoored models will be similar to the trapdoor in
the feature space (shown via formal analysis), and will therefore
produce similar activation patterns to that of the trapdoor.
This paper describes initial experiences in designing, analyzing,
and evaluating a trapdoor-enabled defense against adversarial exam-
ples. We make ﬁve key contributions:
• We introduce the concept of “trapdoors” and trapdoor-enabled
detection as honeypots to defend neural network models and pro-
pose an implementation using backdoor poisoning techniques.
• We present analytical proofs of the efﬁcacy of trapdoors in inﬂu-
encing the generation of adversarial examples and in detecting
the resulting adversarial attacks at inference time.
• We empirically demonstrate the robustness of trapdoor-enabled
detection against a representative suite of state-of-the-art adver-
sarial attacks, including the strongest attacks such as BPDA [2],
as well as black-box and surrogate model attacks.
• We empirically demonstrate key properties of trapdoors: 1) they
have minimal impact on normal classiﬁcation performance; 2)
they can be embedded for multiple output labels to increase de-
fense coverage; 3) they are resistant against recent methods for
detecting backdoor attacks [37, 48].
• We evaluate the efﬁcacy of multiple countermeasures against trap-
door defenses, assuming resource-rich attackers with and without
full knowledge of the trapdoor(s). Trapdoors are robust against a
variety of known countermeasures. Finally, prior to the camera-
ready for this paper, we worked together with an external col-
laborator to carefully craft attacks targeting vulnerabilities in the
trapdoor design. We show that trapdoors are indeed weakened
by trapdoor-vaulting attacks and present preliminary results that
hint at possible mitigation mechanisms.
To the best of our knowledge, our work is the ﬁrst to explore
a honeypot approach to defending DNNs. This is a signiﬁcant de-
parture from existing defenses. Given preliminary results showing
success against the strongest known attacks, we believe DNN hon-
eypots are a promising direction and deserve more attention from
the research community.
2 BACKGROUND AND RELATED WORK
In this section, we present background on adversarial attacks against
DNN models and discuss existing defenses against such attacks.
Notation. We use the following notation in this work.
• Input space: Let X ⊂ Rd be the input space. Let x be an input
where x ∈ X.
• Training dataset: The training dataset consists of a set of inputs
x ∈ X generated according to a certain unknown distribution
x ∼ D. Let y ∈ Y denote the corresponding label for an input x.
• Model: Fθ : X → Y represents a neural network classiﬁer that
maps the input space X to the set of classiﬁcation labels Y. Fθ is
trained using a data set of labeled instances {(x1, y1), ..., (xm , ym)}.
The number of possible classiﬁcation outputs is |Y |, and θ repre-
sents the parameters of the trained classiﬁer.
• Loss function: ℓ(Fθ (x), y) is the loss function for the classiﬁer
Fθ with respect to an input x ∈ X and its true label y ∈ Y.
• Neuron activation vector: ❕(x) is the feature representation of
an input x by Fθ , computed as x’s neuron activation vector at an
intermediate model layer. By default, it is the neuron activation
vector before the softmax layer.
• Adversarial Input: A(x) = x + ϵ represents the perturbed input
that an adversarial generates from an input x such that the model
will classify the input to label yt , i.e. Fθ (x + ϵ) = yt , Fθ (x).
2.1 Adversarial Attacks Against DNNs
An adversarial attack crafts a special perturbation (ϵ) for a normal
input x to fool a target neural network Fθ . When ϵ is applied to x,
the neural network will misclassify the adversarial input (x + ϵ) to
a target label (yt ) [46]. That is, yt = Fθ (x + ϵ) , Fθ (x).
Many methods for generating such adversarial examples (i.e. op-
timizing a perturbation ϵ) have been proposed. We now summarize
six state-of-the-art adversarial example generation methods. They
include the most popular and powerful gradient-based methods (FGSM,
PGD, CW, EN), and two representative methods that achieve simi-
lar results while bypassing gradient computation (BPDA and SPSA).
Fast Gradient Sign Method (FGSM). FGSM was the ﬁrst method
proposed to compute adversarial examples [16]. It creates an adver-
sarial perturbation for an input x by computing a single step in the
direction of the gradient of the model’s loss function at x and multi-
plying the resultant sign vector by a small value η. The adversarial
perturbation ϵ is generated via:
ϵ = η · sign(∇x ℓ(Fθ (x), yt )).
PGD [24] is a more pow-
Projected Gradient Descent (PGD).
erful variant of FGSM. It uses an iterative optimization method to
compute ϵ. Let x be an image represented as a 3D tensor, x0 be a
random sample “close” to x, y = Fθ (x), yt be the target label, and
x ′
n be the adversarial instance produced from x at the nt h iteration.
We have:
x ′
0 = x0,
...
n + α sign(∇x ℓ(Fθ (x ′
where Clip(x,ϵ )z = min{255, x + ϵ, max{0, x − ϵ, z}}.
n+1 = Clip(x,ϵ ){x ′
x ′
n ), yt ))},
Here the Clip function performs per-pixel clipping in an ϵ neighbor-
hood around its input instance.
Carlini and Wagner Attack (CW). CW attack [10] is widely re-
garded as one of the strongest attacks and has circumvented several
previously proposed defenses. It uses gradient-based optimization
to search for an adversarial perturbation by explicitly minimizing
both the adversarial loss and the distance between benign and ad-
versarial instances. It minimizes these two quantities by solving the
optimization problem
min
ϵ
||ϵ ||p + c · ℓ(Fθ (x + ϵ), yt )
Here a binary search is used to ﬁnd the optimal parameter c.
Elastic Net. The Elastic Net attack [12] builds on [10] and uses
both L1 and L2 distances in its optimization function. As a result,
the objective function to compute x + ϵ from x becomes:
min
x
c · ℓ(yt , Fθ (x + ϵ) + β · ||ϵ ||1 + ||ϵ ||2
2
subject to x ∈ [0, 1]p , x + ϵ ∈ [0, 1]p
where c and β are the regularization parameters and the [0, 1] con-
straint restricts x and x + ϵ to a properly scaled image space.
Backward Pass Differentiable Approximation (BPDA). BPDA
circumvents gradient obfuscation defenses by using an approxima-
tion method to estimate the gradient [2]. When a non-differentiable
layer x is present in a model Fθ , BPDA replaces x with an approx-
imation function π (x) ≈ x. In most cases, it is then possible to
compute the gradient
∇x ℓ(Fθ (x), yt ) ≈ ∇x ℓ(Fθ (π (x)), yt ).
This method is then used as part of the gradient descent process
of other attacks to ﬁnd an optimal adversarial perturbation. In this
paper, we use PGD to perform gradient descent.
Simultaneous Perturbation Stochastic Approximation (SPSA).
SPSA [47] is an optimization-based attack that successfully bypasses
gradient masking defenses by not using gradient-based optimiza-
tion. SPSA [43] ﬁnds the global minima in a function with unknown
parameters by taking small steps in random directions. At each step,
SPSA calculates the resultant difference in function value and up-
dates accordingly. Eventually, it converges to the global minima.
2.2 Defenses Against Adversarial Attacks
Next, we discuss current state-of-the-art defenses against adversar-
ial attacks and their limitations. Broadly speaking, defenses either
make it more difﬁcult to compute adversarial examples, or try to
detect them at inference time.
Some defenses aim to increase the difﬁculty
Existing Defenses.
of computing adversarial examples. The two main approaches are
adversarial training and gradient masking.
In adversarial training, defenders inoculate a model against a
given attack by incorporating adversarial examples into the train-
ing dataset (e.g. [32, 52, 54]). This “adversarial” training process
reduces model sensitivity to speciﬁc known attacks. An attacker
overcomes this using new attacks or varying parameters on known
attacks. Some variants of this can make models provably robust
against adversarial examples, but only those within an ϵ-ball of an
input x [22, 32]. Both methods are expensive to implement, and
both can be overcome by adversarial examples outside a predeﬁned
ϵ radius of an original image.
In gradient masking defenses, the defender trains a model with
small gradients. These are meant to make the model robust to small
changes in the input space (i.e. adversarial perturbations). Defen-
sive distillation [35], one example of this method, performs gradient
masking by replacing the original model Fθ with a secondary model
′.
′. Fθ
′ is trained using the class probability outputs of Fθ . This
Fθ
′, making it more dif-
reduces the amplitude of the gradients of Fθ
ﬁcult for an adversary to compute successful adversarial examples
′. However, recent work [7] shows that minor tweaks to
against Fθ
adversarial example generation methods can overcome this defense,
resulting in a high attack success rate against Fθ
Existing Detection Methods. Many methods propose to detect
adversarial examples before or during classiﬁcation Fθ , but many
have already been shown ineffective against clever countermeasures [8],
Feature squeezing smooths input images presented to the model [50],
and tries to detect adversarial examples by computing distance be-
tween the prediction vectors of the original and squeezed images.
Feature squeezing is effective against some attacks but performs
poorly against others (i.e. FGSM, BIM) [30, 50]. MagNet takes a
two-pronged approach: it has a detector which ﬂags adversarial ex-