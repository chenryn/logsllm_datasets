title:A Practical Analysis of Low-Density Parity-Check Erasure Codes for
Wide-Area Storage Applications
author:James S. Plank and
Michael G. Thomason
A Practical Analysis of Low-Density Parity-Check Erasure Codes for
Wide-Area Storage Applications
James S. Plank and Michael G. Thomason∗
Abstract
As peer-to-peer and widely distributed storage systems
proliferate, the need to perform efﬁcient erasure coding,
instead of replication, is crucial to performance and ef-
ﬁciency. Low-Density Parity-Check (LDPC) codes have
arisen as alternatives to standard erasure codes, such as
Reed-Solomon codes, trading off vastly improved decod-
ing performance for inefﬁciencies in the amount of data
that must be acquired to perform decoding. The scores
of papers written on LDPC codes typically analyze their
collective and asymptotic behavior. Unfortunately, their
practical application requires the generation and analysis
of individual codes for ﬁnite systems.
This paper attempts to illuminate the practical consid-
erations of LDPC codes for peer-to-peer and distributed
storage systems. The three main types of LDPC codes
are detailed, and a huge variety of codes are generated,
then analyzed using simulation. This analysis focuses on
the performance of individual codes for ﬁnite systems,
and addresses several important heretofore unanswered
questions about employing LDPC codes in real-world sys-
tems.
1 Introduction
Peer-to-peer and widely distributed ﬁle systems typically
employ replication to improve both the performance and
fault-tolerance of ﬁle access. Speciﬁcally, consider a ﬁle
system composed of storage nodes distributed across the
wide area, and consider multiple clients, also distributed
across the wide area, who desire to access a large ﬁle. The
standard strategy that ﬁle systems employ is one where the
ﬁle is partitioned into n blocks of a ﬁxed size, and these
blocks are replicated and distributed throughout the sys-
tem. Such a scenario is depicted in Figure 1, where a single
ﬁle is partitioned into eight blocks numbered one through
eight, and each block is replicated on four of eight storage
∗This material
is based upon work supported by the National
Science Foundation under grants ACI-0204007, ANI-0222945, and
EIA-9972889, and the Department of Energy under grant DE-FC02-
01ER25465. Department of Computer Science, University of Tennessee,
[plank,thomason]@cs.utk.edu.
servers. Three separate clients are shown accessing the ﬁle
in its entirety by attempting to download each of the eight
blocks from a nearby server.
C1
1
6
3
7
2 4
5 8
1 3
64
1 2
6
8
3 4
6 7
2 3
5 7
2
5
7 8
C2
41
5
8
C3
Figure 1: A widely distributed ﬁle system hosting a ﬁle
partitioned into eight blocks, each block replicated four
times. Three clients are depicted accessing the ﬁle from
different network locations.
Replicated systems such as these provide both fault-
tolerance and improved performance over non-replicated
storage systems. However, the costs are high. First, each
block must be replicated m times to tolerate the failure of
any m − 1 servers. Second, clients must ﬁnd close copies
of each of the ﬁle’s blocks, which can be difﬁcult, and the
failure or slow access of any particular block can hold up
the performance of the entire ﬁle’s access [AW03].
Erasure encoding schemes (schemes originally devel-
oped for communication on the binary erasure channel
(BEC)) improve both the fault-tolerance and downloading
performance of replicated systems [WK02, ZL02]. For ex-
ample, with Reed-Solomon erasure encoding, instead of
storing the blocks of the ﬁles themselves, n + m encod-
ings of the blocks are calculated, and these are stored in-
stead. Now the clients need only download any n blocks,
and from these, the n blocks of the ﬁle may be calculated.
Such a scenario is depicted in Figure 2, where 32 encod-
ing blocks, labeled A through Z and a through f are stored,
and the clients need only access the eight closest blocks to
compute the ﬁle.
Reed-Solomon coding has been employed effectively
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:51:25 UTC from IEEE Xplore.  Restrictions apply. 
C1
BA
DC
E
F
G H
M
N
PO
VU
W X
ZY
ba
I J
K L
Q R
TS
C2
c
d
fe
C3
Figure 2: The same system as Figure 1, employing Reed-
Solomon coding instead of replication. Again the ﬁle is
partitioned into eight blocks, but now 32 encoding blocks
are stored so that clients may employ any eight blocks to
calculate the ﬁle.
in distributed storage systems [KBC+00, RWE+01], and
in related functionalities such as fault-tolerant data struc-
tures [LS00], disk arrays [BM93] and checkpointing sys-
tems [Pla96]. However, it is not without costs. Speciﬁ-
cally, encoding involves breaking each block into words,
and each word is calculated as the dot product of two
length-n vectors under Galois Field arithmetic, which is
more expensive than regular arithmetic. Decoding in-
volves the inversion of an n × n matrix, and then each
of the ﬁle’s blocks is calculated with dot products as in
encoding. Thus, as n grows, the costs of Reed-Solomon
coding induce too much overhead [BLMR98].
In 1997, Luby et al published a landmark paper detail-
ing a coding technique that thrives where Reed-Solomon
coding fails [LMS+97]. Their codes, later termed “Tor-
nado Codes,” calculate m coding blocks from the n ﬁle
blocks in linear time using only cheap exclusive-or (par-
ity) operations. Decoding is also performed in linear time
using parity; however, rather than requiring any n blocks
for decoding as in Reed-Solomon coding, they require
f n blocks, where f is an overhead factor that is greater
than one, but approaches one as n approaches inﬁnity. A
content-distribution system called “Digital Fountain” was
built on Tornado Code technology, and in 1998 its authors
formed a company of the same name.
Tornado Codes are instances of a class of codes called
Low-Density Parity-Check (LDPC) codes, which have a
long history dating back to the 60’s [Gal63], but have re-
ceived renewed attention since the 1997 paper. Since 1998,
the research on LDPC codes has taken two paths – Aca-
demic research has resulted in many publications about
LDPC codes [RGCV03, WK03, SS00, RU03], and Dig-
ital Fountain has both published papers [BLM99, Lub02,
Sho03] and received patents on various aspects of coding
techniques.1
LDPC codes are based on graphs, which are used to
deﬁne codes based solely on parity operations. Nearly all
published research on LDPC codes has had the same mis-
sion – to deﬁne codes that approach “channel capacity”
asymptotically. In other words, they deﬁne codes where
the overhead factor, f , approaches one as n approaches in-
ﬁnity. It has been shown [LMS+97] that codes based on
regular graphs – those where each node has a constant in-
coming and outgoing cardinality – do not have this prop-
erty. Therefore, the “best” codes are based on randomly
generated irregular graphs. A class of irregular graphs is
deﬁned, based on probability distributions of node cardi-
nalities, and then properties are proven about the ensemble
characteristics of this class. The challenge then becomes
to design probability distributions that generate classes of
graphs that approach channel capacity. Hundreds of such
distributions have been published in the literature and on
the web (see Table 1 for 80 examples).
Although the probabilistic method [ASE92] with ran-
dom graphs leads to powerful characterizations of LDPC
ensembles, generating individual graphs from these prob-
ability distributions is a non-asymptotic, non-ensemble
activity.
In other words, while the properties of inﬁ-
nite collections of inﬁnitely sized graphs is known, and
while there has been some work in ﬁnite-length analy-
sis [DPT+02], the properties of individual, ﬁnite-sized
graphs, especially for small values of n, have not been ex-
plored to date. Moreover, these properties have profound
practical consequences.
Addressing aspects of these practical consequences is
the goal of this paper. Speciﬁcally, we detail how three
types of LDPC graphs are generated from given probabil-
ity distributions, and describe a method of simulation to
analyze individual LDPC graphs. Then we generate a wide
variety of LDPC graphs and analyze their performance in
order to answer the following ﬁve practical questions:
1. What kind of overhead factors (f) can we expect
for LDPC codes for small and large values of n?
2. Are the three types of codes equivalent, or do they
perform differently?
3. How do the published distributions fare in produc-
ing good codes for ﬁnite values of n?
4. Is there a great deal of random variation in code
generation from a given probability distribution?
5. How do the codes compare to Reed-Solomon cod-
ing?
1U.S. Patents #6,073,250, #6,081,909, #6,163,870, #6,195,777,
#6,320,520 and #6,373,406. Please see Technical Report [PT03] for a
thorough discussion of patent infringement issues involved with LDPC
codes.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:51:25 UTC from IEEE Xplore.  Restrictions apply. 
In answering each question, we pose a challenge to
the community to perform research that helps systems re-
searchers make use of these codes.
It is our hope that
this paper will spur researchers on LDPC codes to in-
clude analyses of the non-asymptotic properties of indi-
vidual graphs based on their research.
Additionally, for the entire parameter suite that we test,
we publish probability distributions for the best codes, so
that other researchers may duplicate and employ our re-
search.
2 Three Types of LDPC Codes
Systematic Codes: With Systematic codes, L = n and
R = m. Each left node li holds the i-th data block, and
each right node ri is calculated to be the exclusive-or of
all the left nodes that are connected to it. A very simple
example is depicted in Figure 3(a).
Systematic codes can cascade, by employing d > 1
levels of bipartite graphs, g1, . . . , gd, where the right nodes
of gi are also the left nodes of gi+1. The graph of level 1
has L = n, and those nodes contain the n data blocks. The
remaining blocks of the encoding are right-hand nodes of
the d graphs. Thus, m =
i=1 Ri. A simple three-level
cascaded Systematic code is depicted in Figure 3(b).
(cid:1)d
Three distinct types of LDPC codes have been described
in the academic literature. All are based on bipartite
graphs that are randomly generated from probability distri-
butions. We describe them brieﬂy here. For detailed pre-
sentations on these codes, and standard encoding/decoding
algorithms, please see other sources [LMS+97, JKM00,
Sho00, RU03, WK03].
The graphs have L+R nodes, partitioned into two sets –
the left nodes, l1, . . . , lL, and the right nodes, r1, . . . , rR.
Edges only connect left nodes to right nodes. A class of
graphs G is deﬁned by two probability distributions, Λ and
P . These are vectors composed of elements Λ1, Λ2, . . .
i P = 1. Let g
and P1, P2, . . . such that
be a graph in G. Λi is the probability that a left node in g
has exactly i outgoing edges, and similarly, Pi is the prob-
ability that a right node in g has exactly i incoming edges.2
Given L, R, Λ and P , generating a graph g is in theory
a straightforward task [LMS+97], We describe our gener-
ation algorithm in section 4 below. For this section, it suf-
ﬁces that given these four inputs, we can generate bipartite
graphs from them.
i Λi = 1 and
(cid:1)
(cid:1)
To describe the codes below, we assume that we have n
equal-sized blocks of data, which we wish to encode
into n + m equal-sized blocks, which we will distribute
on the network. The nodes of LDPC graphs hold such
blocks of data, and therefore we will use the term “node”
and “block” interchangeably. Nodes can either initialize
their block’s values from data, or they may calculate them
from other blocks. The only operation used for these cal-
culations is parity, as is common in RAID Level 5 disk
arrays [CLG+94]. Each code generation method uses its
graph to deﬁne an encoding of the n data blocks into n+m
blocks that are distributed on the network.
To decode, we assume that we download the f n closest
blocks, B1, . . . Bf n, in order. From these, we can calculate
the original n data blocks.
2An alternate and more popular deﬁnition is to deﬁne probability dis-
tributions of the edges rather than the nodes using two vectors λ and ρ.
The deﬁnitions are interchangeable since (Λ, P ) may be converted easily
to (λ, ρ) and vice versa.
l1
l2
l3
l4
r1
r1 = l1+l3+l4
r2
r2 = l1+l2+l3
r3 r3=l2+l3+l4
(a)
(b)
Figure 3: (a) Example 1-level Systematic code for n = 4,
m = 3. (b) Example 3-level Systematic code for n = 8,
m = 8.
Encoding and decoding of both regular and cascading
Systematic codes are straightforward operations and are
both linear time operations in the number of edges in the
graph.
Gallager (Unsystematic) Codes: Gallager codes were
introduced in the early 1960’s [Gal63]. With these codes,
L = n + m, and R = m. The ﬁrst step of creating a
Gallager code is to use g to generate a (n + m) × n ma-
trix M . This is employed to calculate the n + m encoding
blocks from the original n data blocks. These blocks are
stored in the left nodes of g. The right nodes of g do not
hold data, but instead are constraint nodes — each ri has
the property (guaranteed by the generation of M ) that the
exclusive-or of all nodes incident to it is zero. A simple
Gallager code is depicted in Figure 4(a).
Encoding is an expensive operation, involving the gen-
eration of M , and calculation of the encoding blocks. For-
tunately, if the graph is low density (i.e. the average car-
dinality of the nodes is small), M is a sparse matrix, and
its generation and use for encoding and decoding is not
as expensive as a dense matrix (as is the case with Reed-
Solomon coding). Decoding is linear in the number of
edges in the graph. Fortunately, M only needs to be gen-
erated once per graph, and then it may be used for all en-
coding/decoding operations.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:51:25 UTC from IEEE Xplore.  Restrictions apply. 
l1
l2
l3
l4
l5
l6
l7
l1
l2
l3
l4
r1
l2+l4+l5+l7=0
r2
l1+l2+l3+l7=0
r3
l2+l3+l4+l6=0
(a)
z1
z2
z3
r1
r2
r3
(b)
Figure 4: (a) Example Gallager code for n = 4, m = 3.
Note that the right nodes deﬁne constraints between the
left nodes, and do not store encoding blocks. (b) Example
IRA code for n = 4, m = 3. The left and accumulator
nodes are stored as the encoding blocks. The right nodes
are just used for calculations.
IRA Codes: Irregular Repeat-Accumulate (IRA) Codes
are Systematic codes, as L = n and R = m, and the in-
formation blocks are stored in the left nodes. However, an
extra set of m nodes, z1, . . . , zm, are added to the graph in
the following way. Each node ri has an edge to zi. Ad-
ditionally, each node zi has an edge to zi+1, for i < m.
These extra nodes are called accumulator nodes. For en-
coding, only blocks in the left and accumulator nodes are
stored – the nodes in R are simply used to calculate the
encodings and decodings, and these calculations proceed
exactly as in the Systematic codes. An example IRA graph
is depicted in Figure 4(b).
3 Asymptotic Properties of Codes
All three classes of LDPC codes have undergone asymp-
totic analyses that proceed as follows. A rate R = n
n+m
is selected, and then Λ and P vectors are designed. From
these, it may be proven that graphs generated from the dis-