accuracy decrease threshold of 4% is met. Then, it proceeds to fine-
tune the model on a clean, held-out, dataset with a small learning
rate. We evaluated this approach using VGG-LL on UTKFace, over
three target subpopulations chosen for their relatively high target
damage of 22.7%, 27.1%, and 18%. The effectiveness of fine pruning
against our attack is strongly correlated with the availability of
clean data over which to fine-tune the model. In our experiments,
shown in Table 12 from Appendix A.2, using 1% to 10% of the test set
as fine-tuning data, was insufficient to restore the model‚Äôs accuracy
on the target subpopulation. For example, the best average (over 5
runs) accuracy for the first subpopulation after fine pruning is 78.9%
obtained at 10% validation set, compared to the original accuracy of
98.4% on the subpopulation. For the second subpopulation, the fine
pruning accuracy is below 65.7%, while the original accuracy on
the subpopulation is 89%. We also observe a large variance across
the five experiments, due to the composition of the test set used for
fine tuning. Collateral damage is also high: small fine tuning sets
result in low accuracy on normal data. If the defender has access
to a larger clean dataset, potentially including many points from
the target subpopulation, we expect the effect of the attack can
be eventually removed. However, our new attack vector applies to
diverse, large datasets in which a comprehensive clean held out set
of samples is not in general available, and the impact of the attack
would be higher on under-represented subpopulations under this
defense.
A certified defense relies on two components: an initial robust
training, and a certification procedure for the model. In the random-
ized smoothing defense for evasion attacks [13], for example, ensur-
ing the model robustness requires training with Gaussian noise data
augmentation. The [55] approach trains a linear regression model
with regularization, to guarantee label flipping robustness. The de-
fense only applies to linear models with label flipping attacks: it can
only be used for UTKFace + VGG-LL and IMDB + BERT-LL against
label flipping attacks, and we run on UTKFace only due to running
time constraints. The certification procedure uses properties of lin-
ear regression to simulate training many times with different label
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3115flipping noise rates ùëû. Because regularization depends on ùëû, we vary
ùëû in {0.01, 0.02, 0.04, 0.08, 0.16, 0.32}, and select the model with best
test accuracy. We do not run the certification procedure, instead
reporting the base model‚Äôs accuracy, as the certified accuracy can
only be lower than the base model‚Äôs accuracy. We present results in
Tables 13 and 14: for those subpopulations most harmed by attacks
on the original model, the defense decreases target damage. How-
ever, many subpopulations see significant target damage increases,
including one subpopulation where the target damage increases
from 16% before the defense to 47% after the defense. Unfortunately,
heterogeneous data requires models to make decisions based on a
small set of similar points, which is a worst case scenario for this
defense: many points will be easy to change with few label flips.
Our results from this section constitute evidence that more work
is necessary to reliably defend against subpopulation attacks.
9 DISCUSSION
Our work offers broader lessons for future work in poisoning.
Choice of Objective. In existing poisoning attacks, it is com-
mon to measure the success rate of a fixed poisoning objective. Our
work is the first we are aware of to show that an adversary who
puts effort into identifying multiple potential targets can be more
powerful than an adversary with a single target, by selecting among
them. Our work shows this for our subpopulation poisoning attacks:
while some subpopulations might be weakly impacted by the attack,
if the adversary can pick between several subpopulations, they may
select the most vulnerable ones among them. We offer some analy-
sis for which subpopulations are more vulnerable. We also show
that the choice of target points according to a subpopulation can
improve existing poisoning targeted attacks: by selecting targets
belonging to a single subpopulation, we can double the attack suc-
cess rate. Our results are all for class-untargeted attacks, where the
adversary simply seeks to induce misclassification. Class-targeted
attacks should be possible as well, but may require more attack
points. It is likely that the ease of a class-targeted attack will be
dependent on how similar the original and target classes are.
Model Structure. Our two subpopulation identification strate-
gies fall on opposite extremes: FeatureMatch constructs subpopu-
lations from human designed features, while ClusterMatch iden-
tifies subpopulations directly from the data and model. While a real
world adversary may not directly run ClusterMatch, its success
suggests that using model-specific similarity structure can result in
more effective attacks. Our attacks are also less effective on small
models than large models. Existing work suggests that large mod-
els tend to make more subpopulation-specific decisions [20, 27],
making subpopulations easier to target for these models. Because
of this, we expect our attacks to have similar effectiveness on other
state of the art model architectures, such as EfficientNet [69], Mo-
bileNet [57], or ResNext architectures [77].
Defenses. We both theoretically and empirically justify the dif-
ficulty of defending against subpopulation attacks with purely al-
gorithmic defenses. Our evaluation includes two strong availability
defenses, three backdoor defenses, and a certified defense. Subpop-
ulation attacks may be too targeted to be detected by availability
attacks and consist of natural enough data to not be detected by
backdoor defenses. A certified defense requires the model to be
certified to be robust‚Äîthe certification procedure merely proves it
is robust. We find that the underlying training algorithm for the
certified defense of Rosenfeld et al. [55] is not robust enough to
counteract our attacks. Finally, the fine pruning defense‚Äôs effec-
tiveness is heavily dependent on availability, size, and diversity
of the fine-tuning set, an assumption not made by other defenses
or our negative result. To defend against subpopulation attacks, a
validation set should be diverse enough to contain multiple sam-
ples from all possible adversary subpopulations. If the defender
has access to such a representative ground truth dataset covering a
range of subpopulations, then the defender can validate the model‚Äôs
performance on the ground truth dataset and immediately detect
attacks targeting particular subpopulations. As datasets become
larger and more diverse over time, it will be challenging to obtain
ground truth for a variety of subpopulations, especially the most
under-represented ones. According to our fairness case study from
Section 5.5, it is exactly those under-represented subpopulations
that will suffer mostly from the attack.
10 CONCLUSION
We propose subpopulation data poisoning attacks, a novel type of
poisoning attack which does not require knowledge of the learner‚Äôs
training data or model architecture, and does not require the target
testing data to be modified. We show two subpopulation selection
techniques to instantiate these attacks, called FeatureMatch and
ClusterMatch, which use manual and automatically generated
subpopulations, respectively. We show how to use label flipping
or optimization to generate poisoned data to harm the subpopu-
lation. We provide experimental verification of the effectiveness
and threat potential of subpopulation attacks using tabular, im-
age, and text data, and models trained both end-to-end and with
transfer learning. Our subpopulations can also help improve ex-
isting targeted attacks. Finally, we consider defenses, proving an
impossibility result which suggests some learners cannot defend
against subpopulation attacks. We corroborate this by showing that
many existing poisoning defenses, while sometimes successful, do
not universally work at defending against our attacks, indicating
future work on defenses is necessary. Our work helps understand
the relationship between fair and robust machine learning, and may
help understand to what extent models make decisions based on
similar training data.
Acknowledgements
This research was sponsored by the U.S. Army Combat Capabil-
ities Development Command Army Research Laboratory under
Cooperative Agreement Number W911NF-13-2-0045 (ARL Cyber
Security CRA). The views and conclusions contained in this doc-
ument are those of the authors and should not be interpreted as
representing the official policies, either expressed or implied, of
the Combat Capabilities Development Command Army Research
Laboratory or the U.S. Government. The U.S. Government is autho-
rized to reproduce and distribute reprints for Government purposes
notwithstanding any copyright notation here on.
We would like to thank Paul Hand for his contribution to an
early version of this work [28], and the anonymous reviewers for
their comments on the submission version of this work.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3116REFERENCES
[1] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly
Shmatikov. 2020. How To Backdoor Federated Learning. In Proceedings of
the Twenty Third International Conference on Artificial Intelligence and Statis-
tics (Proceedings of Machine Learning Research), Silvia Chiappa and Roberto
Calandra (Eds.), Vol. 108. PMLR, 2938‚Äì2948. http://proceedings.mlr.press/v108/
bagdasaryan20a.html
[2] Samyadeep Basu, Phil Pope, and Soheil Feizi. 2021. Influence Functions in Deep
Learning Are Fragile. In International Conference on Learning Representations.
https://openreview.net/forum?id=xHKVVHGDOEk
[3] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim ≈†rndiƒá,
Pavel Laskov, Giorgio Giacinto, and Fabio Roli. 2013. Evasion attacks against
machine learning at test time. In Joint European conference on machine learning
and knowledge discovery in databases. Springer, 387‚Äì402.
[4] Battista Biggio, Blaine Nelson, and Pavel Laskov. 2012. Poisoning Attacks against
Support Vector Machines. In Proceedings of the 29th International Coference on
International Conference on Machine Learning, ICML.
[5] Battista Biggio, Konrad Rieck, Davide Ariu, Christian Wressnegger, Igino Corona,
Giorgio Giacinto, and Fabio Roli. 2014. Poisoning behavioral malware clustering.
In Proceedings of the 2014 workshop on artificial intelligent and security workshop.
27‚Äì36.
[6] Joy Buolamwini and Timnit Gebru. 2018. Gender shades: Intersectional accu-
racy disparities in commercial gender classification. In Conference on fairness,
accountability and transparency. 77‚Äì91.
[7] Nicholas Carlini and David Wagner. 2017. Towards Evaluating the Robustness of
Neural Networks. In Proc. IEEE Security and Privacy Symposium.
[8] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness
of neural networks. In 2017 ieee symposium on security and privacy (sp). IEEE,
39‚Äì57.
[9] Nicholas Carlini and David Wagner. 2018. Audio adversarial examples: Targeted
attacks on speech-to-text. In 2018 IEEE Security and Privacy Workshops (SPW).
IEEE, 1‚Äì7.
[10] Hongyan Chang, Ta Duy Nguyen, Sasi Kumar Murakonda, Ehsan Kazemi, and
Reza Shokri. 2020. On Adversarial Bias and the Robustness of Fair Machine
Learning. arXiv preprint arXiv:2006.08669 (2020).
[11] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Ed-
wards, Taesung Lee, Ian Molloy, and Biplav Srivastava. 2018. Detecting Backdoor
Attacks on Deep Neural Networks by Activation Clustering. arXiv:1811.03728 [cs,
stat] (Nov. 2018). http://arxiv.org/abs/1811.03728 arXiv: 1811.03728.
[12] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. 2017. Targeted
backdoor attacks on deep learning systems using data poisoning. arXiv preprint
arXiv:1712.05526 (2017).
[13] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. 2019. Certified adversarial
robustness via randomized smoothing. In International Conference on Machine
Learning. PMLR, 1310‚Äì1320.
[14] Ambra Demontis, Marco Melis, Maura Pintor, Matthew Jagielski, Battista Biggio,
Alina Oprea, Cristina Nita-Rotaru, and Fabio Roli. 2019. Why Do Adversarial
Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks.
In 28th USENIX Security Symposium (USENIX Security 19). USENIX Association,
Santa Clara, CA, 321‚Äì338. https://www.usenix.org/conference/usenixsecurity19/
presentation/demontis
[15] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. 2009. ImageNet: A
Large-Scale Hierarchical Image Database. In CVPR09.
[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers). Association for Computational Linguistics, Minneapolis, Minnesota,
4171‚Äì4186. https://doi.org/10.18653/v1/N19-1423
[17] Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Jacob Steinhardt, and
Alistair Stewart. 2019. Sever: A robust meta-algorithm for stochastic optimization.
In International Conference on Machine Learning. PMLR, 1596‚Äì1606.
http://archive.ics.uci.edu/ml
[18] Dheeru Dua and Casey Graff. 2017. UCI Machine Learning Repository. (2017).
[19] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard
Zemel. 2012. Fairness through awareness. In Proceedings of the 3rd innovations in
theoretical computer science conference. 214‚Äì226.
[20] Vitaly Feldman. 2020. Does Learning Require Memorization? A Short Tale about
a Long Tail. Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory
of Computing (STOC) (2020).
[21] Jonas Geiping, Liam H Fowl, W. Ronny Huang, Wojciech Czaja, Gavin Taylor,
Michael Moeller, and Tom Goldstein. 2021. Witches‚Äô Brew: Industrial Scale
Data Poisoning via Gradient Matching. In International Conference on Learning
Representations. https://openreview.net/forum?id=01olnfLIbD
[22] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and
Harnessing Adversarial Examples. In International Conference on Learning Repre-
sentations. http://arxiv.org/abs/1412.6572
[23] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. 2019. BadNets:
Evaluating Backdooring Attacks on Deep Neural Networks. IEEE Access 7 (2019),
47230‚Äì47244. https://doi.org/10.1109/ACCESS.2019.2909068
[24] Andrew Hard, Kanishka Rao, Rajiv Mathews, Fran√ßoise Beaufays, Sean Augen-
stein, Hubert Eichner, Chlo√© Kiddon, and Daniel Ramage. 2018. Federated learning
for mobile keyboard prediction. arXiv preprint arXiv:1811.03604 (2018).
[25] Moritz Hardt, Eric Price, Nati Srebro, et al. 2016. Equality of opportunity in
supervised learning. In Advances in neural information processing systems. 3315‚Äì
3323.
[26] Sepp Hochreiter and J√ºrgen Schmidhuber. 1997. Long short-term memory. Neural
computation 9, 8 (1997), 1735‚Äì1780.
[27] Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton.
2020. Characterising bias in compressed models. arXiv preprint arXiv:2010.03058
(2020).
[28] Matthew Jagielski, Paul Hand, and Alina Oprea. 2019. Subpopulation Data
Poisoning Attacks. In Workshop on Robust AI in Financial Services: Data, Fairness,
Explainability, Trustworthiness, and Privacy.
[29] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru,
and Bo Li. 2018. Manipulating machine learning: Poisoning attacks and coun-
termeasures for regression learning. In 2018 IEEE Symposium on Security and
Privacy (SP). IEEE, 19‚Äì35.
[30] Pang Wei Koh and Percy Liang. 2017. Understanding black-box predictions via
influence functions. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70. JMLR. org, 1885‚Äì1894.
[31] Pang Wei Koh, Jacob Steinhardt, and Percy Liang. 2018. Stronger data poisoning
attacks break data sanitization defenses. arXiv preprint arXiv:1811.00741 (2018).
[32] Pang Wei W Koh, Kai-Siang Ang, Hubert Teo, and Percy S Liang. 2019. On
the accuracy of influence functions for measuring group effects. In Advances in
Neural Information Processing Systems. 5255‚Äì5265.
[33] Simon Kornblith, Jonathon Shlens, and Quoc V Le. 2019. Do better imagenet
models transfer better?. In Proceedings of the IEEE conference on computer vision
and pattern recognition. 2661‚Äì2671.
[34] Felix Kreuk, Assi Barak, Shir Aviv-Reuven, Moran Baruch, Benny Pinkas, and
Joseph Keshet. 2018. Deceiving end-to-end deep learning malware detectors
using adversarial examples. arXiv preprint arXiv:1802.04528 (2018).
Technical Report.
[35] Alex Krizhevsky. 2009. Learning multiple layers of features from tiny images.
[36] Bogdan Kulynych, Rebekah Overdorf, Carmela Troncoso, and Seda G√ºrses. 2020.
POTs: protective optimization technologies. In Proceedings of the 2020 Conference
on Fairness, Accountability, and Transparency. 177‚Äì188.
[37] Ram Shankar Siva Kumar, David O Brien, Kendra Albert, Salom√© Vilj√∂en, and
Jeffrey Snover. 2019. Failure Modes in Machine Learning Systems.
(2019).
arXiv:cs.LG/1911.11034
[38] Ram Shankar Siva Kumar, Magnus Nystr√∂m, John Lambert, Andrew Marshall,
Mario Goertzel, Andi Comissoneru, Matt Swann, and Sharon Xia. 2020. Adver-
sarial machine learning-industry perspectives. In 2020 IEEE Security and Privacy
Workshops (SPW). IEEE, 69‚Äì75.
[39] Yann LeCun et al. LeNet-5, convolutional neural networks. (????).
[40] Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. 2020. Gradient descent
with early stopping is provably robust to label noise for overparameterized neural
networks. In International Conference on Artificial Intelligence and Statistics. PMLR,
4313‚Äì4324.
[41] Junyu Lin, Lei Xu, Yingqi Liu, and Xiangyu Zhang. 2020. Composite Backdoor
Attack for Deep Neural Network by Mixing Existing Benign Features. In Pro-
ceedings of the 2020 ACM SIGSAC Conference on Computer and Communications
Security. 113‚Äì131.
[42] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. 2018. Fine-pruning: De-
fending against backdooring attacks on deep neural networks. In International
Symposium on Research in Attacks, Intrusions, and Defenses. Springer, 273‚Äì294.
[43] Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. 2020. Reflection backdoor:
A natural backdoor attack on deep neural networks. In European Conference on
Computer Vision. Springer, 182‚Äì199.
[44] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and
Christopher Potts. 2011. Learning Word Vectors for Sentiment Analysis. In Pro-
ceedings of the 49th Annual Meeting of the Association for Computational Linguis-
tics: Human Language Technologies. Association for Computational Linguistics,
Portland, Oregon, USA, 142‚Äì150. http://www.aclweb.org/anthology/P11-1015
[45] Shike Mei and Xiaojin Zhu. 2015. Using machine teaching to identify optimal
training-set attacks on machine learners. In Twenty-Ninth AAAI Conference on
Artificial Intelligence.
[46] Shike Mei and Xiaojin Zhu. 2015. Using Machine Teaching to Identify Optimal
Training-Set Attacks on Machine Learners. In Proceedings of the Twenty-Ninth
AAAI Conference on Artificial Intelligence (AAAI‚Äô15). AAAI Press, 2871‚Äì2877.
[47] Fatemehsadat Mireshghallah, Mohammadkazem Taram, Ali Jalali, Ahmed Taha
Elthakeb, Dean Tullsen, and Hadi Esmaeilzadeh. 2021. Not All Features Are Equal:
Discovering Essential Features for Preserving Prediction Privacy. In Proceedings
of The Web Conference 2021.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3117[48] James Newsome, Brad Karp, and Dawn Song. 2006. Paragraph: Thwarting