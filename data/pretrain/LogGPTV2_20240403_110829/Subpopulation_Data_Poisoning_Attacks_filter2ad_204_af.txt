### Optimized Text

When the accuracy decrease threshold of 4% is met, the model proceeds to fine-tune on a clean, held-out dataset using a small learning rate. We evaluated this approach using VGG-LL on the UTKFace dataset, focusing on three target subpopulations with relatively high target damage rates of 22.7%, 27.1%, and 18%. The effectiveness of fine pruning against our attack is strongly correlated with the availability of clean data for fine-tuning. Our experiments, detailed in Table 12 from Appendix A.2, show that using 1% to 10% of the test set as fine-tuning data was insufficient to restore the model’s accuracy on the target subpopulation. For instance, the best average (over five runs) accuracy for the first subpopulation after fine pruning was 78.9% at 10% validation set, compared to the original accuracy of 98.4% on the subpopulation. For the second subpopulation, the fine pruning accuracy was below 65.7%, while the original accuracy was 89%. We also observed significant variance across the five experiments due to the composition of the test set used for fine-tuning. Collateral damage was also high: small fine-tuning sets resulted in low accuracy on normal data. If the defender has access to a larger, clean dataset, including many points from the target subpopulation, we expect the effect of the attack can be eventually mitigated. However, our new attack vector applies to diverse, large datasets where a comprehensive clean held-out set of samples is generally not available, and the impact of the attack would be higher on under-represented subpopulations under this defense.

A certified defense relies on two components: initial robust training and a certification procedure for the model. For example, in the randomized smoothing defense for evasion attacks [13], ensuring model robustness requires training with Gaussian noise data augmentation. The [55] approach trains a linear regression model with regularization to guarantee label flipping robustness. This defense only applies to linear models with label flipping attacks, such as UTKFace + VGG-LL and IMDB + BERT-LL. Due to running time constraints, we only ran this on UTKFace. The certification procedure uses properties of linear regression to simulate training multiple times with different label flipping noise rates \( q \). Because regularization depends on \( q \), we varied \( q \) in \{0.01, 0.02, 0.04, 0.08, 0.16, 0.32\} and selected the model with the best test accuracy. We did not run the certification procedure, instead reporting the base model’s accuracy, as the certified accuracy can only be lower than the base model’s accuracy. Results in Tables 13 and 14 show that for subpopulations most harmed by attacks on the original model, the defense decreases target damage. However, many subpopulations experienced significant increases in target damage, including one where the target damage increased from 16% before the defense to 47% after the defense. Unfortunately, heterogeneous data requires models to make decisions based on a small set of similar points, which is a worst-case scenario for this defense: many points will be easy to change with few label flips. Our results indicate that more work is necessary to reliably defend against subpopulation attacks.

### Discussion

Our work offers broader lessons for future research in poisoning attacks. **Choice of Objective**: In existing poisoning attacks, it is common to measure the success rate of a fixed poisoning objective. Our work is the first to show that an adversary who identifies multiple potential targets can be more powerful than an adversary with a single target by selecting among them. We demonstrate this for our subpopulation poisoning attacks: while some subpopulations might be weakly impacted by the attack, if the adversary can choose between several subpopulations, they may select the most vulnerable ones. We provide analysis on which subpopulations are more vulnerable. We also show that the choice of target points according to a subpopulation can improve existing targeted poisoning attacks: by selecting targets belonging to a single subpopulation, we can double the attack success rate. Our results are for class-untargeted attacks, where the adversary seeks to induce misclassification. Class-targeted attacks should be possible but may require more attack points. The ease of a class-targeted attack likely depends on the similarity between the original and target classes.

**Model Structure**: Our two subpopulation identification strategies, FeatureMatch and ClusterMatch, fall on opposite extremes. FeatureMatch constructs subpopulations from human-designed features, while ClusterMatch identifies subpopulations directly from the data and model. Although a real-world adversary may not directly run ClusterMatch, its success suggests that using model-specific similarity structures can result in more effective attacks. Our attacks are less effective on small models than large models. Existing work suggests that large models tend to make more subpopulation-specific decisions, making subpopulations easier to target. Therefore, we expect our attacks to have similar effectiveness on other state-of-the-art model architectures, such as EfficientNet, MobileNet, or ResNext.

**Defenses**: We both theoretically and empirically justify the difficulty of defending against subpopulation attacks with purely algorithmic defenses. Our evaluation includes two strong availability defenses, three backdoor defenses, and a certified defense. Subpopulation attacks may be too targeted to be detected by availability attacks and consist of natural enough data to evade backdoor defenses. A certified defense requires the model to be certified as robust, but the certification procedure merely proves it is robust. We find that the underlying training algorithm for the certified defense of Rosenfeld et al. [55] is not robust enough to counteract our attacks. Finally, the fine pruning defense's effectiveness heavily depends on the availability, size, and diversity of the fine-tuning set, an assumption not made by other defenses or our negative results. To defend against subpopulation attacks, a validation set should be diverse enough to contain multiple samples from all possible adversary subpopulations. If the defender has access to a representative ground truth dataset covering a range of subpopulations, they can validate the model’s performance on the ground truth dataset and immediately detect attacks targeting particular subpopulations. As datasets become larger and more diverse over time, obtaining ground truth for a variety of subpopulations, especially the most under-represented ones, will be challenging. According to our fairness case study, it is precisely those under-represented subpopulations that will suffer the most from the attack.

### Conclusion

We propose subpopulation data poisoning attacks, a novel type of poisoning attack that does not require knowledge of the learner’s training data or model architecture and does not require the target testing data to be modified. We present two subpopulation selection techniques, FeatureMatch and ClusterMatch, which use manual and automatically generated subpopulations, respectively. We show how to use label flipping or optimization to generate poisoned data to harm the subpopulation. We provide experimental verification of the effectiveness and threat potential of subpopulation attacks using tabular, image, and text data, and models trained both end-to-end and with transfer learning. Our subpopulations can also help improve existing targeted attacks. Finally, we consider defenses, proving an impossibility result suggesting that some learners cannot defend against subpopulation attacks. We corroborate this by showing that many existing poisoning defenses, while sometimes successful, do not universally work at defending against our attacks, indicating that future work on defenses is necessary. Our work helps understand the relationship between fair and robust machine learning and may help understand to what extent models make decisions based on similar training data.

### Acknowledgements

This research was sponsored by the U.S. Army Combat Capabilities Development Command Army Research Laboratory under Cooperative Agreement Number W911NF-13-2-0045 (ARL Cyber Security CRA). The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Combat Capabilities Development Command Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon.

We would like to thank Paul Hand for his contribution to an early version of this work [28], and the anonymous reviewers for their comments on the submission version of this work.

### References

[References remain unchanged]

---

This optimized text aims to improve clarity, coherence, and professionalism, making the content more accessible and understandable.