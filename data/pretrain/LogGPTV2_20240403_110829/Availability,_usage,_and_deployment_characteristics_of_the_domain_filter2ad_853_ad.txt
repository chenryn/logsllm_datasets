### 4.2.4 Locally Correlated Failures

Next, we examine the degree to which failure periods are correlated within particular domains. For this analysis, we define a domain as a Network Access Cluster (NAC) [19] to capture events correlated with network location. Correlated failures suggest common network outages that prevent reachability or other related events within a NAC, such as power outages or network-wide maintenance.

To estimate the fraction of failure probes that are correlated within a NAC, we first examined all locally-related probe pairs—closely spaced probe pairs (within 2 minutes) that contained at least one failure and were sent to different servers within the same NAC. Of the NACs formed by the servers in `ldns-avail` and `adns-avail`, 20% and 11%, respectively, had at least one such pair. We consider a pair correlated if both probes were failures. The ratio of correlated pairs to total locally-related pairs gives an estimate of the fraction of failures within the NAC that are correlated. The cumulative distribution of this fraction over all NACs is shown in Figure 9.

We observe that about 40% of LDNS and 50% of ADNS NACs have no correlated failures. However, on average, 11.5% of LDNS and 12.2% of ADNS failures were correlated within a given NAC. This suggests that most unavailability to DNS servers is unlikely to be correlated within a NAC.

### 4.3 Time to Failure and Recovery

So far, we have explored the availability of DNS servers—the probability that a given DNS server is available at a specific point in time. In this section, we provide a rough characterization of how long we expect a DNS server to be available before it fails and how long it takes to recover. This information is challenging to extract from our measurements directly due to the granularity of our probes. A significant fraction of the observed failures occurred for only a single probe. Additionally, we cannot distinguish between network failures and actual DNS server failures.

Thus, we present two sets of results: one that ignores any failure lasting less than 20 minutes ("short failures") and one that includes all failures. Feamster et al. [12] found that more than 90% of all network failures lasted less than 15 minutes. Therefore, server failures can be accurately estimated from probe failures in the set that ignores short failures because network failures are almost always brief. During high-frequency probing (one probe per 5 minutes; see Section 3.2), we found that more than 60% of all observed failure periods lasting more than a single probe lasted longer than 20 minutes for both ADNS and LDNS servers. Hence, most failures longer than 5 minutes were also longer than 20 minutes. However, ignoring short failures will tend to overestimate the time to failure and recovery. Including all failures provides a more pessimistic estimate of the time to failure.

We estimated the time to failure as the duration from the first probe until the first observed failure (ignoring short failures as described). The time to recovery was estimated as the duration from the start of the first probe failure until the last consecutive probe failure. Since we optimistically assume that the failure does not begin until the probe failure, our results may underestimate failure lengths by up to an order of magnitude of the mean probing interval (1 hour).

To investigate the impact of short failures on our results, we examined "closely spaced" probe triples. Let S represent a probe success and F represent a probe failure. For LDNS servers, within a 2-minute period, 0 triples had a pattern of F S F, and 14 of 40,322 (0.03%) triples had a pattern of S F S. This suggests that failure or availability periods are unlikely to last less than 2 minutes. Over 30-minute intervals, 1,401 and 6,634 of 7,041,334 (0.02% and 0.09%) triple samples had patterns of F S F or S F S, respectively. Short availability periods were primarily isolated to 934 servers. Closely spaced samples at ADNS servers showed similar characteristics.

Examining longer periods yields similar fractions of short failure periods but not larger fractions of short availability periods. Since the probability of short availability periods is low, we are likely to overestimate the length of very long consecutive availability periods due to missed short failures. Nonetheless, our estimates should be considered coarse approximations.

#### Summary Statistics

| Statistic | Ignoring Short Failures | With Short Failures |
| --- | --- | --- |
| **LDNS** |  |  |
| - % with at least 1 failure | 12.6% | 37.8% |
| - Mean Time to Failure (h) | 148.7 | 138.0 |
| - Standard Deviation (h) | 99.1 | 99.1 |
| - Median (h) | 132.4 | 117.3 |
| - Mean Time to Recovery (h) | 7.2 | 3.3 |
| - Standard Deviation (h) | 9.5 | 2.6 |
| - Median (h) | 6.3 | 2.6 |
| **ADNS** |  |  |
| - % with at least 1 failure | 10.8% | 35.7% |
| - Mean Time to Failure (h) | 143.1 | 134.0 |
| - Standard Deviation (h) | 100.0 | 98.3 |
| - Median (h) | 134.0 | 117.3 |
| - Mean Time to Recovery (h) | 8.7 | 3.3 |
| - Standard Deviation (h) | 9.5 | 2.6 |
| - Median (h) | 6.3 | 2.6 |

Figure 10 presents summary statistics about the time to failure and recovery for when we ignore short failures and when we do not. Clearly, the number of servers where we observe at least one failure is much larger when we do not ignore the short failures (37.8% vs. 12.6% for LDNS servers; 35.7% vs. 10.8% for ADNS servers). Including all failures drives up the mean time to failure among those that did fail by several hours, and the majority never fail at all during the two-week period.

Figure 11 plots the cumulative distribution of time to the first failure for those servers that were unavailable at least once (ignoring the 5% with the highest values to limit outliers). Whether or not we take into account "short" failures, the majority of servers are available uninterrupted for longer than two weeks. Our measurement period was not long enough to make longer-term conclusions, except that the time to failure for DNS servers is (at least) on the order of days or weeks. ADNS servers are likely to live a little longer than LDNS servers, which is expected given their higher availability.

Figure 12 shows the distribution of times until the first recovery for the fraction of LDNS and ADNS servers that failed at least once and then recovered during our two-week period (ignoring the 5% with the highest values).