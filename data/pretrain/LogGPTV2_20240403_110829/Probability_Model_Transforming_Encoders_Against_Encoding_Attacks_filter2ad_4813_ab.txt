real message distribution. Merely relying on the DTEs, such
attackers can distinguish real and decoy messages with high
accuracy.
To recover the message, attackers: 1) decrypt the ciphertext
under N keys {ki}N
i=1; 2) choose
the most likely message. For some special types of messages
which can be veriﬁed online, for example, authentication cer-
tiﬁcates (passwords, password vaults or authentication keys),
i=1 and get N messages {Mi}N
USENIX Association
28th USENIX Security Symposium    1575
Algorithm 1: The attack process to recover a stolen
ciphertext.
Input: a stolen ciphertext c, N keys/passwords {ki}N
Output: a guessing list for messages (in decreasing order of p).
decryption, and a weight function p.
i=1 for
Si ← decryptki (c)
Mi ← decode(Si)
1 for i ← 1 to N do
2
3
4 end
5 Sort {Mi}N
i=1 in decreasing order of p(Mi) (or p(Si)), then output
the list. /* Different attacks are equipped with
different weight functions p, where p(Mi) usually
reflects the probability that Mi is real.
*/
attackers need to sort these N messages to minimize the num-
ber of online veriﬁcations. To characterize attackers in a uni-
ﬁed form, we consider an attacker only picking one message
also as a sorting attacker who picks the ﬁrst one in his order.
Assuming an attacker sorts the messages in decreasing order
of a weight function p, the attack process can be represented
as Algorithm 1.
The efﬁciency of an attacker depends on 1) the guessing
order of keys and 2) the sorted order of messages. These
two orders correspond to two factors—keys and DTEs, af-
fecting the security of HE schemes. The stronger the keys
are, the harder they are to be cracked. Keys used by HE are
usually human-memorable passwords. Password researches
have attracted great attention recently, such as password guess-
ing [25,41,42], password strength meter [15,39,40], password
generation policy [3, 34]. However, same as previous litera-
ture [10, 14], we ignore the inﬂuence of keys on the security
of HE schemes and only focus on the security of DTEs, i.e.,
the indistinguishability of decoy messages.
3.2 Analyses of Password Vault PMTEs
Chatterjee et al.’s PMTE [10] for password vaults uses a sub-
grammar approach to model the similarity of passwords in
one vault. Speciﬁcally, the sub-grammar (based on Chatterjee-
PCFG) of vault V = (password,password1) is {S → W, S →
WD, W → password, D → 1}, where W represents an English
word and D represents a digit string. In fact, Chatterjee-PCFG
is more comprehensive. We simplify it for ease of explana-
tion. To encode a vault, this PMTE 1) ﬁrst parses the sub-
grammar of the vault, 2) then encodes the sub-grammar, and
3) ﬁnally encodes the passwords in the vault according to the
sub-grammar. Decoding is in the opposite direction.
Because sub-grammars are parsed from the real vaults
when encoding, all production rules in sub-grammars are
used by passwords in the real vaults. Unfortunately, it may
not hold when decoding a random seed. For example, de-
coding a random seed, the sub-grammar may be SG = {S →
W, S → WD, W → password, D → 1}, and the vault may be
V = (password,password). As passwords are generated inde-
pendently based on sub-grammars when decoding, production
rules (e.g., S → WD) in the sub-grammar may not be used by
any password in the vault. In addition, decoded sub-grammars
may contain identical rules, but encoded ones do not, because
the rules are also independently generated when decoding a
random seed.
Similar phenomena also appear in Golla et al.’s PMTEs
[14]. They used a reuse-rate approach to model password
similarity. Given V = (password1,password1,password@),
Golla et al.’s PMTEs take “password1” as the base password
of V and “password@” as a password modiﬁed from the base
password. When encoding, they 1) encode the base pass-
word (“password1”) and the reuse-rate of the base password
3), 2) encode reuse-rates of modiﬁed passwords ( 1
( 2
3) and the
modiﬁed characters (“@”). More speciﬁcally, Golla et al.’s
PMTEs divide the vault into six subsets {Vi}5
i=0: passwords
with an edit distance of i to the base passwords Vi (0 ≤ i ≤ 4)
and the remaining passwords V5. Assuming the proportion
(reuse-rate) of Vi in V follow a normal distribution with a
small variance, |Vi| (the cardinality of Vi) is encoded by the
DTE of the normal distribution, for 0 ≤ i ≤ 4. In addition,
the base password, modiﬁed characters (of passwords in Vi
for 1 ≤ i ≤ 4) and remaining passwords in V5 are encoded by
PMTEs of Markov models.
The sum of |Vi| for 0 ≤ i ≤ 4 (without |V5|) is less than
or equal to |V| when encoding. However, it may not hold
when decoding a random seed, because proportions of Vi are
generated independently. Further, the modiﬁed character of
password pw in Vi may be the same as the original character
of the base password when decoding a random seed, which
means pw actually belongs to Vj with j  1. Fortu-
nately, if R S is not preﬁx-free, it can easily be converted to
a preﬁx-free sequence space R S(cid:48) by two simple methods: 1)
add a special rule at the beginning of the sequence to represent
the length of the sequence; 2) add a special rule at the end of
the sequence to represent the end of the sequence. Therefore,
without loss of generality, we assume generating sequence
spaces of GPMs are all preﬁx-free.
4.2 Formalization of Existing Models
For a Markov model of order n, a generating rule is a character,
and a valid generating sequence is a string. The conditional
probability of a rule only depends on last n rules, formally
P(ai|a1a2 . . .ai−1) = P(ai|ai−nai−n+1 . . .ai−1),
where i > n and P(ai|ai−nai−n+1 . . .ai−1) is trained on a
training set (RockYou for password vault schemes). The
Markov model with distribution-based normalization adds
some extra rules {L = l}lmax
l=1 to R , L = l represents that
the password length is equal to l, where 1 ≤ l ≤ lmax and
lmax is the max password length (e.g., 30). A valid gen-
erating sequence has the form (L = l,a1,a2, . . . ,al) which
means generating the length ﬁrst and then generating the char-
acters. P(L = l,a1,a2, . . . ,al) = P(L = l)P(a1,a2, . . . ,al),
where P(a1,a2, . . . ,al) can be calculated as the ordinary
Markov model and P(L = l) represents the probability that
the length of a password is l. Note that lmax < ∞, because the
message space M is ﬁnite (the seed space S is ﬁnite).
For a PCFG model, a generating rule is a production rule of
the PCFG, a valid generating sequence is a leftmost derivation
of a string. The conditional probability of a rule does not
depend on any previous rule, formally