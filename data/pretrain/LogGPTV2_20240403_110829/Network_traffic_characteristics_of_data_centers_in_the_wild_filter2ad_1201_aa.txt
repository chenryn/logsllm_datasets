title:Network traffic characteristics of data centers in the wild
author:Theophilus Benson and
Aditya Akella and
David A. Maltz
Network Trafﬁc Characteristics of Data Centers in the Wild
Theophilus Benson∗, Aditya Akella∗ and David A. Maltz†
∗University of Wisconsin–Madison
†Microsoft Research–Redmond
ABSTRACT
Although there is tremendous interest in designing improved net-
works for data centers, very little is known about the network-level
trafﬁc characteristics of current data centers. In this paper, we con-
duct an empirical study of the network trafﬁc in 10 data centers
belonging to three different types of organizations, including uni-
versity, enterprise, and cloud data centers. Our deﬁnition of cloud
data centers includes not only data centers employed by large on-
line service providers offering Internet-facing applications, but also
data centers used to host data-intensive (MapReduce style) appli-
cations. We collect and analyze SNMP statistics, topology, and
packet-level traces. We examine the range of applications deployed
in these data centers and their placement, the ﬂow-level and packet-
level transmission properties of these applications, and their im-
pact on network utilization, link utilization, congestion, and packet
drops. We describe the implications of the observed trafﬁc patterns
for data center internal trafﬁc engineering as well as for recently-
proposed architectures for data center networks.
Categories and Subject Descriptors
C.4 [Performance of Systems]: Design studies; Performance
attributes
General Terms
Design, Measurement, Performance
Keywords
Data center trafﬁc, characterization
1.
INTRODUCTION
A data center (DC) refers to any large, dedicated cluster of com-
puters that is owned and operated by a single organization. Data
centers of various sizes are being built and employed for a di-
verse set of purposes today. On the one hand, large universities
and private enterprises are increasingly consolidating their IT ser-
vices within on-site data centers containing a few hundred to a few
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’10, November 1–3, 2010, Melbourne, Australia.
Copyright 2010 ACM 978-1-4503-0057-5/10/11 ...$10.00.
thousand servers. On the other hand, large online service providers,
such as Google, Microsoft, and Amazon, are rapidly building geo-
graphically diverse cloud data centers, often containing more than
10K servers, to offer a variety of cloud-based services such as E-
mail, Web servers, storage, search, gaming, and Instant Messaging.
These service providers also employ some of their data centers to
run large-scale data-intensive tasks, such as indexing Web pages or
analyzing large data-sets, often using variations of the MapReduce
paradigm [6].
Despite the growing applicability of data centers in a wide vari-
ety of scenarios, there are very few systematic measurement stud-
ies [19, 3] of data center usage to guide practical issues in data
center operations. Crucially, little is known about the key differ-
ences between different classes of data centers, speciﬁcally univer-
sity campus data centers, private enterprise data centers, and cloud
data centers (both those used for customer-facing applications and
those used for large-scale data-intensives tasks).
While several aspects of data centers still need substantial em-
pirical analysis, the speciﬁc focus of our work is on issues pertain-
ing to a data center network’s operation. We examine the send-
ing/receiving patterns of applications running in data centers and
the resulting link-level and network-level performance. A better
understanding of these issues can lead to a variety of advancements,
including trafﬁc engineering mechanisms tailored to improve avail-
able capacity and reduce loss rates within data centers, mechanisms
for improved quality-of-service, and even techniques for managing
other crucial data center resources, such as energy consumption.
Unfortunately, the few recent empirical studies [19, 3] of data cen-
ter networks are quite limited in their scope, making their observa-
tions difﬁcult to generalize and employ in practice.
In this paper, we study data collected from ten data centers to
shed light on their network design and usage and to identify prop-
erties that can help improve operation of their networking sub-
strate. The data centers we study include three university campus
data centers, two private enterprise data centers, and ﬁve cloud
data centers, three of which run a variety of Internet-facing ap-
plications while the remaining two predominantly run MapReduce
workloads. Some of the data centers we study have been in op-
eration for over 10 years, while others were commissioned much
more recently. Our data includes SNMP link statistics for all data
centers, ﬁne-grained packet traces from select switches in four of
the data centers, and detailed topology for ﬁve data centers. By
studying different classes of data centers, we are able to shed light
on the question of how similar or different they are in terms of their
network usage, whether results taken from one class can be applied
to the others, and whether different solutions will be needed for
designing and managing the data centers’ internal networks.
We perform a top-down analysis of the data centers, starting with
267the applications run in each data center and then drilling down to
the applications’ send and receive patterns and their network-level
impact. Using packet traces, we ﬁrst examine the type of appli-
cations running in each data center and their relative contribution
to network trafﬁc. We then examine the ﬁne-grained sending pat-
terns as captured by data transmission behavior at the packet and
ﬂow levels. We examine these patterns both in aggregate and at a
per-application level. Finally, we use SNMP traces to examine the
network-level impact in terms of link utilization, congestion, and
packet drops, and the dependence of these properties on the loca-
tion of the links in the network topology and on the time of day.
Our key empirical ﬁndings are the following:
• We see a wide variety of applications across the data centers,
ranging from customer-facing applications, such as Web ser-
vices, ﬁle stores, authentication services, Line-of-Business
applications, and custom enterprise applications to data in-
tensive applications, such as MapReduce and search index-
ing. We ﬁnd that application placement is non-uniform across
racks.
• Most ﬂows in the data centers are small in size (≤ 10KB),
a signiﬁcant fraction of which last under a few hundreds of
milliseconds, and the number of active ﬂows per second is
under 10,000 per rack across all data centers.
• Despite the differences in the size and usage of the data cen-
ters, trafﬁc originating from a rack in a data center is ON/OFF
in nature with properties that ﬁt heavy-tailed distributions.
• In the cloud data centers, a majority of trafﬁc originated by
servers (80%) stays within the rack. For the university and
private enterprise data centers, most of the trafﬁc (40-90%)
leaves the rack and traverses the network’s interconnect.
• Irrespective of the type, in most data centers, link utilizations
are rather low in all layers but the core. In the core, we ﬁnd
that a subset of the core links often experience high utiliza-
tion. Furthermore, the exact number of highly utilized core
links varies over time, but never exceeds 25% of the core
links in any data center.
• Losses occur within the data centers; however, losses are not
localized to links with persistently high utilization. Instead,
losses occur at links with low average utilization implicat-
ing momentary spikes as the primary cause of losses. We
observe that the magnitude of losses is greater at the aggre-
gation layer than at the edge or the core layers.
• We observe that link utilizations are subject to time-of-day
and day-of-week effects across all data centers. However in
many of the cloud data centers, the variations are nearly an
order of magnitude more pronounced at core links than at
edge and aggregation links.
To highlight the implications of our observations, we conclude
the paper with an analysis of two data center network design issues
that have received a lot of recent attention, namely, network bisec-
tion bandwidth and the use of centralized management techniques.
• Bisection Bandwidth: Recent data center network proposals
have argued that data centers need high bisection bandwidth
to support demanding applications. Our measurements show
that only a fraction of the existing bisection capacity is likely
to be utilized within a given time interval in all the data cen-
ters, even in the “worst case” where application instances are
Data Center
Study
Fat-tree [1]
Hedera [2]
Portland [22]
BCube [13]
DCell [16]
VAL2 [11]
Micro TE [4]
Flyways [18]
Optical switching [29]
ECMP. study 1 [19]
ECMP. study 2 [3]
Elastic Tree [14]
SPAIN [21]
Our work
Type of
Data Center
Type of
Apps
# of DCs
Measured
Cloud
Cloud
Cloud
Cloud
Cloud
Cloud
Cloud
Cloud
Cloud
Cloud
Cloud
ANY
Any
Cloud
Private Net
Universities
MapReduce
MapReduce
MapReduce
MapReduce
MapReduce
MapReduce
MapReduce
MapReduce
MapReduce
MapReduce
MapReduce
Web Services
Web Services
Any
MapReduce
Webservices
Distributed F’S
0
0
0
0
0
1
1
1
1
1
19
1
0
10
Table 1: Comparison of prior data center studies, including
type of data center and application.
spread across racks rather than conﬁned within a rack. This
is true even for MapReduce data centers that see relatively
higher utilization. From this, we conclude that load balanc-
ing mechanisms for spreading trafﬁc across the existing links
in the network’s core can help manage occasional conges-
tion, given the current applications used.
• Centralization Management: A few recent proposals [2,
14] have argued for centrally managing and scheduling network-
wide transmissions to more effectively engineer data center
trafﬁc. Our measurements show that centralized approaches
must employ parallelism and fast route computation heuris-
tics to scale to the size of data centers today while supporting
the application trafﬁc patterns we observe in the data centers.
The rest of the paper is structured as follows: we present related
work in Section 2 and in Section 3 describe the data centers studied,
their high-level design, and typical uses. In Section 4, we describe
the applications running in these data centers.
In Section 5, we
zoom into the microscopic properties of the various data centers.
In Section 6, we examine the ﬂow of trafﬁc within data centers and
the utilization of links across the various layers. We discuss the im-
plications of our empirical insights in Section 7, and we summarize
our ﬁndings in Section 8.
2. RELATED WORK
There is tremendous interest in designing improved networks for
data centers [1, 2, 22, 13, 16, 11, 4, 18, 29, 14, 21]; however, such
work and its evaluation is driven by only a few studies of data cen-
ter trafﬁc, and those studies are solely of huge (> 10K server) data
centers, primarily running data mining, MapReduce jobs, or Web
services. Table 1 summarizes the prior studies. From Table 1, we
observe that many of the data architectures are evaluated without
empirical data from data centers. For the architectures evaluated
with empirical data, we ﬁnd that these evaluations are performed
with traces from cloud data centers. These observations imply that
the actual performance of these techniques under various types of
realistic data centers found in the wild (such as enterprise and uni-
versity data centers) is unknown and thus we are motivated by this
to conduct a broad study on the characteristics of data centers. Such
a study will inform the design and evaluation of current and future
data center techniques.
268This paper analyzes the network trafﬁc of the broadest set of
data centers studied to date, including data centers running Web
services and MapReduce applications, but also other common en-
terprise and campus data centers that provide ﬁle storage, authen-
tication services, Line-of-Business applications, and other custom-
written services. Thus, our work provides the information needed
to evaluate data center network architecture proposals under the
broad range of data center environments that exist.
Previous studies [19, 3] have focused on trafﬁc patterns at coarse
time-scales, reporting ﬂow size distributions, number of concurrent
connections, duration of congestion periods, and diurnal patterns.
We extend these measures by considering additional issues, such as
the applications employed in the different data centers, their trans-
mission patterns at the packet and ﬂow levels, their impact on link
and network utilizations, and the prevalence of network hot-spots.
This additional information is crucial to evaluating trafﬁc engineer-
ing strategies and data center placement/scheduling proposals.
The closest prior works are [19] and [3]; the former focuses
on a single MapReduce data centers, while the latter considers
cloud data centers that host Web services as well as those running
MapReduce. Neither study considers non-cloud data centers, such
as enterprise and campus data centers, and neither provides as com-
plete a picture of trafﬁc patterns as this study. The key observations
from Benson’s study [3] are that utilizations are highest in the core
but losses are highest at the edge. In our work, we augment these
ﬁndings by examining the variations in link utilizations over time,
the localization of losses to link, and the magnitude of losses over
time. From Kandula’s study [19], we learned that while most trafﬁc
in the cloud is restricted to within a rack and a signiﬁcant number
of hot-spots exist in the network. Our work supplements these re-
sults by quantifying the exact fraction of trafﬁc that stays within
a rack for a wide range of data centers. In addition, we quantify
the number of hot-spots, show that losses are due to the underly-
ing burstiness of trafﬁc, and examine the ﬂow level properties for
university and private enterprise (both are classes of data centers
ignored in Kandula’s study [19]).
Our work complements prior work on measuring Internet traf-
ﬁc [20, 10, 25, 9, 8, 17] by presenting an equivalent study on the
ﬂow characteristics of applications and link utilizations within data
centers. We ﬁnd that data center trafﬁc is statistically different from
wide area trafﬁc, and that such behavior has serious implications
for the design and implementation of techniques for data center
networks.
3. DATASETS AND OVERVIEW OF DATA
CENTERS
In this paper, we analyze data-sets from 10 data centers, includ-
ing 5 commercial cloud data centers, 2 private enterprise data cen-
ters, and 3 university campus data centers. For each of these data
centers, we examine one or more of the following data-sets: net-
work topology, packet traces from select switches, and SNMP polls
from the interfaces of network switches. Table 2 summarizes the
data collected from each data center, as well as some key proper-
ties.
Table 2 shows that the data centers vary in size, both in terms of
the number of devices and the number of servers. Unsurprisingly,
the largest data centers are used for commercial computing needs
(all owned by a single entity), with the enterprise and university
data centers being an order of magnitude smaller in terms of the
number of devices.
The data centers also vary in their proximity to their users. The
enterprise and university data centers are located in the western/mid-
Data Center Name
Number of Locations
EDU1
EDU2
EDU3
PRV2
1
1
1
4
Table 3: The number of packet trace collection locations for the
data centers in which we were able to install packet sniffers.
western U.S. and are hosted on the premises of the organizations
to serve local users. In contrast, the commercial data centers are
distributed around the world in the U.S., Europe, and South Amer-
ica. Their global placement reﬂects an inherent requirement for
geo-diversity (reducing latency to users), geo-redundancy (avoid-
ing strikes, wars, or ﬁber cuts in one part of the world), and regu-
latory constraints (some data can not be removed from the E.U. or
U.S.).
In what follows, we ﬁrst describe the data we collect. We then
outline similarities and differences in key attributes of the data cen-
ters, including their usage proﬁles, and physical topology. We
found that understanding these aspects is required to analyze the
properties that we wish to measure in subsequent sections, such as
application behavior and its impact on link-level and network-wide
utilizations.
3.1 Data Collection
SNMP polls: For all of the data centers that we studied, we
were able to poll the switches’ SNMP MIBs for bytes-in and bytes-
out at granularities ranging from 1 minute to 30 minutes. For the
5 commercial cloud data centers and the 2 private enterprises, we
were able to poll for the number of packet discards as well.
For each data center, we collected SNMP data for at least 10
days.
In some cases (e.g., EDU1, EDU2, EDU3, PRV1, PRV2,
CLD1, CLD4), our SNMP data spans multiple weeks. The long
time-span of our SNMP data allows us to observe time-of-day and
day-of-week dependencies in network trafﬁc.
Network Topology: For the private enterprises and university
data centers, we obtained topology via the Cisco CDP protocol,
which gives both the network topology as well as the link capaci-
ties. When this data is unavailable, as with the 5 cloud data centers,
we analyze device conﬁguration to derive properties of the topol-
ogy, such as the relative capacities of links facing endhosts versus
network-internal links versus WAN-facing links.
Packet traces: Finally, we collected packet traces from a few
of the private enterprise and university data centers (Table 2). Our
packet trace collection spans 12 hours over multiple days. Since it
is difﬁcult to instrument an entire data center, we selected a hand-
ful of locations at random per data center and installed sniffers on
them. In Table 3, we present the number of sniffers per data center.
In the smaller data centers (EDU1, EDU2, EDU3), we installed
1 sniffer. For the larger data center (PRV2), we installed 4 snif-