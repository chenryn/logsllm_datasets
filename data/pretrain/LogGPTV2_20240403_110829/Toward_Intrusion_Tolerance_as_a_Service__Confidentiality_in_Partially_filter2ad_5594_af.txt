100
100
<100ms
<200ms
%
100
100
100
100
0.1
percentile
39.7 ms
42.5 ms
41.6 ms
46.0 ms
1
percentile
41.0 ms
43.6 ms
42.8 ms
47.5 ms
50
percentile
51.7 ms
54.4 ms
53.6 ms
61.1 ms
99
percentile
62.4 ms
65.6 ms
64.2 ms
78.4 ms
99.9
percentile
63.9 ms
67.7 ms
66.1 ms
86.2 ms
SPIRE AND CONFIDENTIAL SPIRE PERFORMANCE ON LAN WITH EMULATED LATENCIES BETWEEN SITES FOR 36000 UPDATES OVER 1 HOUR
TABLE II
average latency increase noted above in the f = 1 case. This
can be explained by increasing communication overheads, due
to the all-to-all communication patterns, as the number of
replicas increases. However, this is still acceptable, as the
results show that our Conﬁdential Spire implementation still
meets the timeliness requirements for power grid SCADA
systems (processing updates within 100ms), even while tol-
erating 2 intrusions. The observed 99.9 percentile latency
is 86.2ms, and no update crossed the 100ms threshold. We
expect that these can even be further reduced through improved
engineering of the communication protocols to reduce the rate
at which trafﬁc is sent, and of the cryptographic mechanisms
to reduce processing overheads. As shown by [36] and noted
in [4], the majority of the advantages of proactive recovery
can be obtained by tolerating two intrusions, instead of only
one, making “6+6+5+4” a useful conﬁguration to support.
B. Attack Evaluation of Conﬁdential Spire
We next evaluate Conﬁdential Spire’s ability to meet the
timeliness requirements of power grid SCADA systems while
under attack. Such systems require responses within 100ms
in the normal case but may tolerate latencies up to 200ms in
certain situations [10], [11]. We consider the effect of proactive
recovery on performance, as well as network attacks that cause
a site to be disconnected. While we do not explicitly evaluate
malicious actions by protocol replicas, we note that many types
of malicious actions closely resemble proactive recovery of the
leader replica in terms of performance: once the leader takes
a malicious action (e.g. sending conﬂicting messages), a view
change is triggered to elect a new leader.2 The Conﬁdential
Spire “4+4+3+3” conﬁguration’s performance under all com-
binations of recoveries and site disconnections is illustrated
in Figure 2. We can see that proactive recovery of a leader
replica, which occurs between 1:00 and 1:30, causes one client
update to spike over 100ms, when the system must perform
a view change. Recovery of a non-leader replica (the more
common case), which occurs between 3:15 and 3:45, has
almost no impact on performance. In this case, we only see
one client update with higher than average latency, but it is
still below the 100ms threshold.
Similarly, there is no latency spike when we disconnect
a non-leader site, at 4:19, since there is no view change.
However, we can see a few client updates spiking, with
one rising above 100ms, though still under 200ms, when the
2It is possible to reduce the performance impact of proactive recovery by
preemptively changing the leader, but since our implementation does not do
that, our experimental results accurately reﬂect the case where timeouts must
expire before changing the leader.
Fig. 2. Latency in the presence of proactive recoveries and site disconnections.
leader-site is disconnected at 2:00, since this requires a view
change. We also note a small (but still acceptable) increase in
average latency for the duration of the time either the leader or
non-leader site is disconnected in this experiment, as in both
cases it renders the fastest quorum of replicas unavailable, and
requires communication with a more distant site to occur on
the critical path. However, when reconnecting a disconnected
site, we can see signiﬁcant latency spikes, crossing the 200ms
threshold and reaching up to 450ms (e.g. at 2:30 and 5:00).
This is due to the large number of checkpoint messages and
update messages being sent over the network from the correct
replicas to help catch up the lagging replicas in the site which
just rejoined the network. While this is a limitation of our
current implementation, we note that this is not an inherent
limitation of the protocol, and should be ﬁxable by engineering
a better message ﬂow control for the checkpoint messages
and update messages that are being sent to catch up the other
replicas. Overall, these results indicate that, even with the
overhead of providing conﬁdentiality, our system can provide
the necessary performance, even while under attack.
VIII. CONCLUSION
We presented a new partially cloud-based BFT architecture
that can leverage offsite data centers to tolerate simultaneous
network attacks and system compromises, without exposing
conﬁdential system state or proprietary algorithms to data
center servers. In case on-premises servers are compromised,
we also extended our basic protocol to include a key renewal
mechanism that limits the amount of conﬁdential information
that can be disclosed. We implemented and evaluated our new
model in Conﬁdential Spire, a SCADA system for the power
grid, and found that our new architecture meets SCADA timing
requirements with only a small increase in overall latency to
provide conﬁdentiality guarantees.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:54 UTC from IEEE Xplore.  Restrictions apply. 
24
REFERENCES
[1] W. Zhao and F. E. Villaseca, “Byzantine fault tolerance for electric
power grid monitoring and control,” in Int. Conf. Embedded Software
and Systems, July 2008, pp. 129–135.
[2] N. A. C. Medeiros, “A fault- and intrusion- tolerant architecture for EDP
distribuicao SCADA system,” Master’s thesis, Univ. of Lisbon, 2011.
[3] J. Kirsch, S. Goose, Y. Amir, D. Wei, and P. Skare, “Survivable SCADA
via intrusion-tolerant replication,” IEEE Trans. Smart Grid, vol. 5, no. 1,
pp. 60–70, Jan 2014.
[4] A. Babay, T. Tantillo, T. Aron, M. Platania, and Y. Amir, “Network-
attack-resilient intrusion-tolerant SCADA for the power grid,” in 48th
Annual IEEE/IFIP International Conference on Dependable Systems and
Networks (DSN), Luxembourg City, Luxembourg, June 2018, pp. 255–
266.
[5] A. Nogueira, M. Garcia, A. Bessani, and N. Neves, “On the challenges
of building a BFT SCADA,” in 48th Annual IEEE/IFIP International
Conference on Dependable Systems and Networks (DSN), 2018, pp.
163–170.
[6] A. Bessani, J. Sousa, and E. E. P. Alchieri, “State machine replication for
the masses with BFT-SMART,” in 44th Annual IEEE/IFIP International
Conference on Dependable Systems and Networks, 2014, pp. 355–362.
[7] “bft-smart,” https://github.com/bft-smart, retrieved 2020-11-23.
[8] A. Babay, J. Schultz, T. Tantillo, and Y. Amir, “Toward an intrusion-
tolerant power grid: Challenges and opportunities,” in IEEE 38th In-
ternational Conference on Distributed Computing Systems (ICDCS),
Vienna, Austria, July 2018, pp. 1321–1326.
[9] “Spire: Intrusion-tolerant SCADA for the power grid,” http://www.dsn.
jhu.edu/spire/, retrieved 2020-11-24.
[10] IEEE, “Ieee standard communication delivery time performance require-
ments for electric power substation automation,” IEEE Std 1646-2004,
pp. 1–24, 2005.
[11] J. Deshpande, A. Locke, and M. Madden, “Smart choices for the smart
grid,” Alcatel-Lucent Technolgy White Paper, 2011.
[12] M. Castro and B. Liskov, “Practical Byzantine fault
tolerance and
proactive recovery,” ACM Trans. Comput. Syst., vol. 20, no. 4, pp. 398–
461, Nov. 2002.
[13] M. Correia, N. F. Neves, and P. Verissimo, “How to tolerate half less
one byzantine nodes in practical distributed systems,” in IEEE Int. Symp.
Reliable Distributed Systems (SRDS), Oct 2004, pp. 174–183.
[14] B.-G. Chun, P. Maniatis, S. Shenker, and J. Kubiatowicz, “Attested
append-only memory: Making adversaries stick to their word,” SIGOPS
Oper. Syst. Rev., vol. 41, no. 6, pp. 189–204, Oct. 2007.
[15] M. Correia, N. Neves, and P. Verissimo, “BFT-TO: Intrusion tolerance
with less replicas,” The Computer Journal, vol. 56, no. 6, pp. 693–715,
June 2013.
[16] G. S. Veronese, M. Correia, A. N. Bessani, L. C. Lung, and P. Verissimo,
“Efﬁcient byzantine fault-tolerance,” IEEE Transactions on Computers,
vol. 62, no. 1, pp. 16–30, Jan 2013.
[17] Y. Amir, B. Coan, J. Kirsch, and J. Lane, “Byzantine replication under
attack,” in IEEE Int. Conf. Dependable Systems and Networks (DSN),
June 2008, pp. 197–206.
[18] A. Clement, E. Wong, L. Alvisi, M. Dahlin, and M. Marchetti, “Making
byzantine fault tolerant systems tolerate byzantine faults,” in USENIX
Symp. Networked Syst. Design and Implem. (NSDI), 2009, pp. 153–168.
[19] G. S. Veronese, M. Correia, A. N. Bessani, and L. C. Lung, “Spin one’s
wheels? byzantine fault tolerance with a spinning primary,” in IEEE Int.
Symp. Reliable Distributed Systems (SRDS), Sept 2009, pp. 135–144.
[20] Z. Milosevic, M. Biely, and A. Schiper, “Bounded delay in byzantine-
tolerant state machine replication,” in IEEE Int. Symp. Reliable Dis-
tributed Systems (SRDS), Sept 2013, pp. 61–70.
[21] T. Roeder and F. B. Schneider, “Proactive obfuscation,” ACM Trans.
Comput. Syst., vol. 28, no. 2, pp. 4:1–4:54, Jul. 2010. [Online].
Available: http://doi.acm.org/10.1145/1813654.1813655
[22] P. Sousa, A. Bessani, M. Correia, N. F. Neves, and P. Verissimo, “Highly
available intrusion-tolerant services with proactive-reactive recovery,”
IEEE Trans. Parallel Distrib. Syst., vol. 21, no. 4, pp. 452–465, 2010.
[23] A. N. Bessani, E. P. Alchieri, M. Correia, and J. S. Fraga, “DepSpace:
A byzantine fault-tolerant coordination service,” in Proceedings of
the 3rd ACM SIGOPS/EuroSys European Conference on Computer
Systems 2008, ser. Eurosys ’08. New York, NY, USA: Association
for Computing Machinery, 2008, p. 163–176. [Online]. Available:
https://doi.org/10.1145/1352592.1352610
[24] R. Padilha and F. Pedone, “Belisarius: BFT storage with conﬁdentiality,”
in IEEE 10th International Symposium on Network Computing and
Applications, 2011, pp. 9–16.
[25] R. Vassantlal, “Conﬁdential BFT state machine replication,” Master’s
[Online]. Available: http:
thesis, Universidade de Lisboa, 2019.
//hdl.handle.net/10451/40304
[26] A. Bessani, M. Correia, B. Quaresma, F. Andr´e, and P. Sousa, “Depsky:
dependable and secure storage in a cloud-of-clouds,” Acm transactions
on storage (tos), vol. 9, no. 4, pp. 1–33, 2013.
[27] A. N. Bessani, R. Mendes, T. Oliveira, N. F. Neves, M. Correia,
M. Pasin, and P. Verissimo, “Scfs: A shared cloud-backed ﬁle system.”
in USENIX Annual Technical Conference. Citeseer, 2014, pp. 169–180.
[28] D. R. Matos, M. L. Pardal, G. Carle, and M. Correia, “Rockfs: Cloud-
backed ﬁle system resilience to client-side attacks,” in Proceedings of
the 19th International Middleware Conference, 2018, pp. 107–119.
[29] J. Yin, J.-P. Martin, A. Venkataramani, L. Alvisi, and M. Dahlin,
tolerant
“Separating agreement from execution for byzantine fault
services,” in Proceedings of
the Nineteenth ACM Symposium on
Operating Systems Principles, ser. SOSP ’03. New York, NY, USA:
Association for Computing Machinery, 2003, p. 253–267. [Online].
Available: https://doi.org/10.1145/945445.945470
[30] S. Duan and H. Zhang, “Practical state machine replication with con-
ﬁdentiality,” in IEEE 35th Symposium on Reliable Distributed Systems
(SRDS), 2016, pp. 187–196.
[31] M. Kapritsos, Y. Wang, V. Quema, A. Clement, L. Alvisi, and
M. Dahlin, “All about eve: Execute-verify replication for multi-core
servers,” in 10th USENIX Symposium on Operating Systems Design and
Implementation (OSDI 12). Hollywood, CA: USENIX Association,
Oct. 2012, pp. 237–250. [Online]. Available: https://www.usenix.org/
conference/osdi12/technical-sessions/presentation/kapritsos
[32] E. Androulaki, A. Barger, V. Bortnikov, C. Cachin, K. Christidis,
A. De Caro, D. Enyeart, C. Ferris, G. Laventman, Y. Manevich,
S. Muralidharan, C. Murthy, B. Nguyen, M. Sethi, G. Singh,
K. Smith, A. Sorniotti, C. Stathakopoulou, M. Vukoli´c, S. W. Cocco,
and J. Yellick, “Hyperledger fabric: A distributed operating system
for permissioned blockchains,” in Proceedings of
the Thirteenth
’18. New York, NY, USA:
EuroSys Conference,
Association for Computing Machinery, 2018.
[Online]. Available:
https://doi.org/10.1145/3190508.3190538
ser. EuroSys
[33] D. Obenshain, T. Tantillo, A. Babay, J. Schultz, A. Newell, M. E. Hoque,
Y. Amir, and C. Nita-Rotaru, “Practical intrusion-tolerant networks,” in
IEEE Int. Conf. Distrib. Comput. Syst. (ICDCS), June 2016, pp. 45–56.
[34] “The Spines Messaging System,” www.spines.org, retrieved 2020-12-09.
[35] P. Sousa, N. F. Neves, and P. Verissimo, “Hidden problems of asyn-
chronous proactive recovery,” in Proceedings of the Workshop on Hot
Topics in System Dependability, 2007.
[36] M. Platania, D. Obenshain, T. Tantillo, R. Sharma, and Y. Amir,
“Towards a practical survivable intrusion tolerant replication system,”
in 2014 IEEE 33rd International Symposium on Reliable Distributed
Systems.
IEEE, 2014, pp. 242–252.
[37] Y. Amir, B. Coan, J. Kirsch, and J. Lane, “Prime: Byzantine replication
under attack,” IEEE Trans. Dependable and Secure Computing, vol. 8,
no. 4, pp. 564–577, July 2011.
[38] “Prime: Byzantine replication under attack,” www.dsn.jhu.edu/prime,
retrieved 2020-12-09.
[39] T. Tantillo, “Intrusion-tolerant SCADA for the power grid,” Ph.D.
dissertation, Johns Hopkins University, 2018.
[40] V. Costan and S. Devadas, “Intel sgx explained.” IACR Cryptol. ePrint
Arch., vol. 2016, no. 86, pp. 1–118, 2016.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:54 UTC from IEEE Xplore.  Restrictions apply. 
25