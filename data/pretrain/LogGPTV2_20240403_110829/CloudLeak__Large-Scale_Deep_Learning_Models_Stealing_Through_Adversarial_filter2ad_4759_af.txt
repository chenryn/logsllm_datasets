attention on the method for detecting adversarial examples,
which can help providers offer MLaaS to monitor whether
individual query inputs are malicious. In our defense scheme,
we analyze the differences between the feature distributions
13
0.43K1.29K2.15K4.3K5KQueries0.00.20.40.60.81.0Accuracy(a) Traffic RecognitionAPITramerCorreia-SilviaPapernotOurs0.51K1.53K2.00K2.55K3.00KQueries0.00.20.40.60.81.0(b) Flower Recognition0.5k1.00k1.50k3k5KQueries0.00.20.40.60.81.0(c) NSFW0.68k1.36k2.00k2.50k3.00kQueries0.00.20.40.60.81.0(d) Emotion Recognitionof malicious and benign images, and further propose a novel
defense mechanism that can effectively and adaptively de-
fend against the FeatureFool adversarial attacks on MLaaS
platforms. Speciﬁcally, we start with training the proposed
feature distribution guided network, named DefenseNet, using
a popular deep learning framework - PyTorch. The pre-trained
DefenseNet will be adapted to extract each hidden layers output
as the features of the input samples. A categorical mixture
model is used as the prior probability to characterize these
query samples distribution. Adversarial examples generated
by attackers have a different characteristic distribution from
the benign samples distribution. We also integrate an SVM
classiﬁer into our DefenseNet to distinguish benign samples
and adversarial examples as opposed to prior works which
may alter the decision boundaries. Our defense mechanism
dramatically improves the average success rate (up to 91%) of
detecting abnormalities in the input samples used for querying
API.
B. Limitations
Though the experimental results show that the proposed
attack framework is able to effectively steal the victim models
inside the commercial APIs even in a black-box setting, there
are some limitations that we may address in the future.
Further improvement of adversarial query methods. One
main limitation is that, in order to maximize the uncertainty of
examples away from decision boundary of the victim classiﬁer,
attackers may add more perturbations to these examples via
adversarial attacks algorithms. For example, Feature Adversary
and FeatureFool may be used to generate stronger datasets to
train the substitute model. However, in this case, adversarial
examples with large perturbations generally pollute the syn-
thetic training set and consequently lower the accuracy of the
substitute model. This problem can be addressed in the future
by designing a more sophisticated algorithm that can trade-
off between the perturbations of adversarial examples and the
performance of the substitute model.
Extension to multi-label cases. Another problem is extend-
ing the attack method to multi-label cases. In comparison
to classiﬁcation tasks with a single label per image, where
attackers aim at replicating the functionality of the multi-
label model inside the API, we need to pay more attention to
the synthetic data set generation algorithms and the substitute
model architecture choices. That is, in order to launch a model
stealing attack on the victim model, attackers need to ﬁrst
craft the adversarial examples with multi target labels and
then generate the synthetic dataset to train a substitute model.
Although the proposed attack framework is not evaluated
on multi-label classiﬁcation models, for example, Celebrity
Recognition API [19] provided by Clarifai, the adversarial
query method introduced in this paper can help an adversary
obtain more crucial information about a victim model, such as
decision boundary, label types, etc, which pose a great threat to
the privacy of MLaaS platforms. Future work will also focus on
developing an effective model extraction attack on the cloud-
based multi-label classiﬁcation model.
Extension to other domains. As adversarial examples are
widely existed on various domains such as audio and text,
the proposed attack can be easily extended to all DNN
based MLaaS platforms. Furthermore, even in the case where
appropriate pre-trained models may be harder to get from
current model zoos, attackers can pre-train their basic “teacher”
models from scratch (i.e., datasets related to target tasks) and
then ﬁne-tune these models using the adversary-query pairs
proposed in this paper on the domains other images to steal
black-box DNN models.
C. Responsible Disclosure
We have reported our ﬁndings to cloud providers including
Clarifai, Microsoft, IBM, Google, and Face++. Among them,
we contacted Clarifai and Face++ in December 2019 and con-
tacted Microsoft, IBM, and Google in January 2020. Among
them, Face++ replied to us in January 2020 and encourage
us to apply the developed method on other Face++ APIs for
security evaluations.
VII. CONCLUSIONS
Machine learning as a service (MLaaS) provided by cloud-
based platforms, including Microsoft, Google, Face++ and
Clarifai, has been widely applied in real-world applications.
These services, however, tend to suffer from the model ex-
traction attack launched by an adversary even with black-box
access. Although previous works on model stealing attacks
have shown good performance, their effectiveness is generally
constrained by massive prediction queries and high costs. To
address these challenges, we study the practicality of model
stealing attacks against DNN models trained on commercial
MLaaS platforms. Through local experiments and online at-
tacks on commercialized MLaaS platforms we demonstrate
that our model stealing attack can sufﬁciently train a local
substitute model with near-equivalent performance as the target
model. Our attack method requires signiﬁcantly less queries
to the target model compared to previous works of model
extraction attack due to our novel design of architecture and
training process of the local substitute model. The transfer
learning helps us to utilize existing well-trained models in
the source domain, and thus we only need to ﬁne-tune a
few layers of these models. The adversarial examples used
for querying the target model help us to efﬁciently learn
the distance between decision boundaries of the target model
and the local model,
thus accelerating the convergence in
training. In the future, we will mainly focus on designing
effective defense mechanisms against model stealing attacks,
and therefore enhance the robustness of DNN based MLaaS
image classiﬁers.
ACKNOWLEDGMENT
This work is partially supported by the Department of En-
ergy through the Early Career Award. Any opinions, ﬁndings,
conclusions, and recommendations expressed in this material
are those of the authors and do not necessarily reﬂect the views
of the U.S. Department of Energy.
REFERENCES
[1] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
object detection with region proposal networks,” IEEE Trans. Pattern
Anal. Mach. Intell., vol. 39, no. 6, pp. 1137–1149, 2017.
[2] A. rahman Mohamed, G. E. Dahl, and G. E. Hinton, “Acoustic modeling
using deep belief networks,” IEEE Transactions on Audio, Speech, and
Language Processing, vol. 20, pp. 14–22, 2012.
14
[3] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,
A. Guez, T. Hubert, L. R. Baker, M. Lai, A. Bolton, Y. Chen, T. P.
Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and
D. Hassabis, “Mastering the game of go without human knowledge,”
Nature, vol. 550, pp. 354–359, 2017.
[4] A. R. Zamir, A. Sax, W. Shen, L. Guibas, J. Malik, and S. Savarese,
“Taskonomy: Disentangling task transfer learning,” in Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition,
2018.
[5] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, “Learning and transferring
mid-level image representations using convolutional neural networks,”
in Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2014.
[6] F. Tram`er, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, “Stealing
machine learning models via prediction apis,” in 25th USENIX Security
Symposium (USENIX Security 16), pp. 601–618, 2016.
[7] B. Wang and N. Z. Gong, “Stealing hyperparameters in machine
learning,” arXiv preprint arXiv:1802.05351, 2018.
[8] A. Dmitrenko et al., “Dnn model extraction attacks using prediction
interfaces,” 2018.
[9] Y. Shi, Y. Sagduyu, and A. Grushin, “How to steal a machine learning
classiﬁer with deep learning,” in Technologies for Homeland Security
(HST), 2017 IEEE International Symposium on, pp. 1–5, IEEE, 2017.
[10] R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership
inference attacks against machine learning models,” in Security and
Privacy (SP), 2017 IEEE Symposium on, pp. 3–18, IEEE, 2017.
[11] Y. Long, V. Bindschaedler, L. Wang, D. Bu, X. Wang, H. Tang, C. A.
Gunter, and K. Chen, “Understanding membership inferences on well-
generalized learning models,” arXiv preprint arXiv:1802.04889, 2018.
[12] M. Juuti, S. Szyller, A. Dmitrenko, S. Marchal, and N. Asokan,
“Prada: Protecting against dnn model stealing attacks,” arXiv preprint
arXiv:1805.02628, 2018.
[13] T. Zhang, “Privacy-preserving machine learning through data obfusca-
[14]
[15]
[16]
[17]
[18]
[19]
custom
vision.”
tion,” arXiv preprint arXiv:1807.01860, 2018.
J. R. C. da Silva, R. F. Berriel, C. Badue, A. F. de Souza, and T. Oliveira-
Santos, “Copycat cnn: Stealing knowledge by persuading confession
with random non-labeled data,” CoRR, vol. abs/1806.05476, 2018.
“Microsoft
us/services/cognitive-services/custom-vision-service/.
“Face++ emotion recognition api.” https://www.faceplusplus.com/
emotion-recognition/.
“Ibm watson
watson-visual-recognition.
“Google automl vision.” https://cloud.google.com/vision/automl/docs/.
“clarifai
safe for work (nsfw).” https://www.clarifai.com/models/
celebrity-image-recognition-model-e466caa0619f444ab97497640cefc4dc.
https://azure.microsoft.com/en-
https://www.ibm.com/cloud/
visual
recognition.”
[20] M. Juuti, S. Szyller, A. Dmitrenko, S. Marchal, and N. Asokan,
“PRADA: protecting against DNN model stealing attacks,” CoRR,
vol. abs/1805.02628, 2018.
[21] N. Papernot, P. D. McDaniel, I. J. Goodfellow, S. Jha, Z. B. Celik, and
A. Swami, “Practical black-box attacks against machine learning,” in
AsiaCCS, 2017.
[22] S. J. Pan and Q. Yang., “A survey on transfer learning,” IEEE Trans-
actions on knowledge and data engineering, vol. 22, no. 10, pp. 1345–
1359, 2010.
[23] H. Azizpour, A. S. Razavian, J. Sullivan, A. Maki, and S. Carlsson,
“From generic to speciﬁc deep representations for visual recognition,”
in Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition Workshop, pp. 36–45, 2015.
[24] W. Ge and Y. Yu, “Borrowing treasures from the wealthy: Deep transfer
learning through selective joint ﬁne-tuning,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2017.
[25] Y. Sun, X. Wang, and X. Tang, “Deep learning face representation from
predicting 10,000 classes,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2014.
J. Huang, A. J. Smola, A. Gretton, K. M. Borgwardt, and B. Sch¨olkopf,
“Correcting sample selection bias by unlabeled data,” in Advances in
neural information processing systems, pp. 601–608, 2006.
[26]
[27] E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko, “Simultaneous
deep transfer across domains and tasks,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 4068–
4076, 2015.
[28] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” CoRR, vol. abs/1409.1556, 2014.
[29] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. ˇSrndi´c, P. Laskov,
G. Giacinto, and F. Roli, “Evasion attacks against machine learning at
test time,” in Machine Learning and Knowledge Discovery in Databases
(H. Blockeel, K. Kersting, S. Nijssen, and F. ˇZelezn´y, eds.), pp. 387–
402, Springer Berlin Heidelberg, 2013.
[30] N. Papernot, P. D. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and
A. Swami, “The limitations of deep learning in adversarial settings,”
CoRR, vol. abs/1511.07528, 2015.
[31] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter, “Accessorize to
a crime: Real and stealthy attacks on state-of-the-art face recognition,”
in Proceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security, pp. 1528–1540, ACM, 2016.
[32] N. Papernot, P. D. McDaniel, and I. J. Goodfellow, “Transferability
in machine learning: from phenomena to black-box attacks using
adversarial samples,” vol. abs/1605.07277, 2016.
[33] Y. Liu, X. Chen, C. Liu, and D. Song, “Delving into transferable ad-
versarial examples and black-box attacks,” CoRR, vol. abs/1611.02770,
2016.
[34] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfel-
low, and R. Fergus, “Intriguing properties of neural networks,” in arXiv
preprint arXiv:1312.6199, 2013.
[35] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and
A. Swami, “The limitations of deep learning in adversarial settings,”
in Security and Privacy (EuroS&P), 2016 IEEE European Symposium
on, (Saarbrucken), pp. 372–387, IEEE, 2016.
[36] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural
networks,” in Security and Privacy (S&P), 2017 IEEE Symposium on,
pp. 39–57, 2017.
[37] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards
deep learning models resistant to adversarial attacks,” ICLR, 2018.
[38] B. Biggio and F. Roli, “Wild patterns: Ten years after the rise of
adversarial machine learning,” Pattern Recognition, vol. 84, pp. 317–
331, 2018.
[39] T. S. Sethi and M. Kantardzic, “Data driven exploratory attacks on black
box classiﬁers in adversarial domains,” Neurocomput., vol. 289, no. C,
pp. 129–143, 2018.
[40] Y. Ji, X. Zhang, S. Ji, X. Luo, and T. Wang, “Model-reuse attacks
on deep learning systems,” in Proceedings of the 2018 ACM SIGSAC
Conference on Computer and Communications Security, CCS ’18,
pp. 349–363, 2018.
[41] S. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: a sim-
ple and accurate method to fool deep neural networks,” CoRR,
vol. abs/1511.04599, 2015.
[42] A. Rozsa, E. M. Rudd, and T. E. Boult, “Adversarial diversity and
hard positive generation,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2016.
[43] Z. Zhao, D. Dua, and S. Singh, “Generating natural adversarial exam-
ples,” CoRR, vol. abs/1710.11342, 2017.
I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
adversarial examples,” arXiv preprint arXiv:1412.6572, 2014.
[44]
[45] S. Sabour, Y. Cao, F. Faghri, and D. J. Fleet, “Adversarial manipulation
of deep representations,” in Proceedings of the International Conference
on Learning Representations (ICLR), 2016.
[46] B. Wang, Y. Yao, B. Viswanath, H. Zheng, and B. Y. Zhao, “With great
training comes great vulnerability: Practical attacks against transfer
learning,” in 27th USENIX Security Symposium (USENIX Security 18),
pp. 1281–1297, 2018.
[47] B. Wang and N. Z. Gong, “Stealing hyperparameters in machine
learning,” 2018 IEEE Symposium on Security and Privacy (SP), pp. 36–
52, 2018.
[48] B. Settles, M. Craven, and S. Ray, “Multiple-instance active learning,”
in Advances in Neural Information Processing Systems 20 (J. C. Platt,
D. Koller, Y. Singer, and S. T. Roweis, eds.), pp. 1289–1296, 2008.
15
[49] D. A. Cohn, Z. Ghahramani, and M. I. Jordan, “Active learning with
statistical models,” J. Artif. Int. Res., vol. 4, no. 1, pp. 129–145, 1996.
[50] B. Settles and M. Craven, “An analysis of active learning strategies for
sequence labeling tasks,” in Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP ’08, pp. 1070–1079,
2008.
[51] S. Tong and D. Koller, “Support vector machine active learning with
applications to text classiﬁcation,” J. Mach. Learn. Res., vol. 2, pp. 45–
66, 2002.
[52] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in Neural
Information Processing Systems 25, pp. 1097–1105, 2012.
[53] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” CoRR, vol. abs/1409.1556, 2014.
[54] O. M. Parkhi, A. Vedaldi, and A. Zisserman, “Deep face recognition,” in
Proceedings of the British Machine Vision Conference (BMVC), 2015.
[55] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
[56]
[57]
[58]
[59]
recognition,” CoRR, vol. abs/1512.03385, 2015.
“The german trafﬁc sign recognition benchmark.” http://benchmark.ini.
rub.de/?section=gtsrb\&subsection=news.
“102 category ﬂower dataset.” http://www.robots.ox.ac.uk/∼vgg/data/
ﬂowers/102/index.html.
“Kdef: A resource for studying face recognition in personal photo
collections.” http://kdef.se/home/aboutKDEF.html.
“Pubﬁg83: A resource for studying face recognition in personal photo
collections.” http://vision.seas.harvard.edu/pubﬁg83/.
16