### Method for Detecting Adversarial Examples

We focus on a method for detecting adversarial examples, which can help providers offer Machine Learning as a Service (MLaaS) to monitor whether individual query inputs are malicious. Our defense scheme analyzes the differences between the feature distributions of malicious and benign images. Based on this analysis, we propose a novel defense mechanism that effectively and adaptively defends against FeatureFool adversarial attacks on MLaaS platforms.

#### Defense Mechanism

1. **Training the DefenseNet:**
   - We start by training a feature distribution-guided network, named DefenseNet, using PyTorch, a popular deep learning framework.
   - The pre-trained DefenseNet is adapted to extract the output of each hidden layer as features of the input samples.

2. **Categorical Mixture Model:**
   - A categorical mixture model is used as the prior probability to characterize the distribution of these query samples.
   - Adversarial examples generated by attackers have a different characteristic distribution compared to benign samples.

3. **SVM Classifier Integration:**
   - We integrate an SVM classifier into our DefenseNet to distinguish between benign samples and adversarial examples.
   - This approach avoids altering the decision boundaries, which is a common issue in previous works.

4. **Performance:**
   - Our defense mechanism significantly improves the average success rate of detecting abnormalities in input samples used for querying APIs, up to 91%.

### Limitations

While our experimental results show that the proposed attack framework can effectively steal victim models inside commercial APIs even in a black-box setting, there are some limitations:

1. **Adversarial Query Methods:**
   - To maximize the uncertainty of examples away from the decision boundary of the victim classifier, attackers may add more perturbations to these examples via adversarial attack algorithms.
   - However, large perturbations generally pollute the synthetic training set, lowering the accuracy of the substitute model.
   - Future work will focus on designing a more sophisticated algorithm to balance the perturbations of adversarial examples and the performance of the substitute model.

2. **Extension to Multi-Label Cases:**
   - Extending the attack method to multi-label cases is another challenge.
   - In multi-label classification tasks, where attackers aim to replicate the functionality of the multi-label model inside the API, more attention needs to be paid to the synthetic data set generation algorithms and the substitute model architecture choices.
   - Although our proposed attack framework has not been evaluated on multi-label classification models, such as the Celebrity Recognition API provided by Clarifai, the adversarial query method introduced in this paper can help adversaries obtain crucial information about the victim model, posing a significant threat to the privacy of MLaaS platforms.
   - Future work will focus on developing an effective model extraction attack on cloud-based multi-label classification models.

3. **Extension to Other Domains:**
   - Adversarial examples exist in various domains such as audio and text, and our proposed attack can be extended to all DNN-based MLaaS platforms.
   - Even if appropriate pre-trained models are difficult to obtain from current model zoos, attackers can pre-train their basic "teacher" models from scratch using datasets related to target tasks and then fine-tune these models using the adversary-query pairs proposed in this paper to steal black-box DNN models in other domains.

### Responsible Disclosure

We have reported our findings to cloud providers including Clarifai, Microsoft, IBM, Google, and Face++. Specifically:
- We contacted Clarifai and Face++ in December 2019 and Microsoft, IBM, and Google in January 2020.
- Face++ responded in January 2020, encouraging us to apply the developed method on other Face++ APIs for security evaluations.

### Conclusions

Machine learning as a service (MLaaS) provided by cloud-based platforms, including Microsoft, Google, Face++, and Clarifai, has been widely applied in real-world applications. These services, however, are vulnerable to model extraction attacks even with black-box access. Our study demonstrates the practicality of model stealing attacks against DNN models trained on commercial MLaaS platforms. Through local experiments and online attacks, we show that our model stealing attack can sufficiently train a local substitute model with near-equivalent performance as the target model. Our attack method requires significantly fewer queries to the target model due to our novel design of the architecture and training process of the local substitute model. Transfer learning helps us utilize existing well-trained models in the source domain, requiring only fine-tuning of a few layers. The adversarial examples used for querying the target model help efficiently learn the distance between decision boundaries of the target model and the local model, accelerating the convergence in training. Future work will focus on designing effective defense mechanisms against model stealing attacks to enhance the robustness of DNN-based MLaaS image classifiers.

### Acknowledgment

This work is partially supported by the Department of Energy through the Early Career Award. Any opinions, findings, conclusions, and recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the U.S. Department of Energy.

### References

[References listed as provided in the original text]