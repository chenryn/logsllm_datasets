Caching and file systems
617
Client
Windows
Network
   Server
Application A
opens a file on
a server
Application A
receives a handle
to the file on
the server
Application A
issues a read
to the file
Application A
receives only the
amount of data
it requested
Application A
issues a read to
the file within
the area cached
Application B
opens the same
file on the server
for read access
Application A
issues a write to
the file within
the area cached
Application B
receives a handle
to the file on
the server
Application B
issues a read to
the file to an area
that is cached
Application A
issues a write to
the file in an area
 that is cached
First handle
on the 
file opened
Data read
from file
Server 
opens
second
handle
to file; lease 
remains
Data written
to the 
cache will 
eventually
be flushed
to the 
server by
the client
Read-Handle lease granted
Read data returned
Server unaware
Server unaware
No network packets
No network packets
Cache flushed and no more
caching allowed on the file
Read-ahead data written
to cache
Read data and 
read-ahead from server
CreateFile (with
FILE_GENERIC_READ and
FILE_GENERIC_WRITE)
CreateFile (same file with
FILE_GENERIC_READ)
I/O complete
Cache data given to application
I/O complete
WriteFile
WriteFile
ReadFile
ReadFile
ReadFile
Handle
Handle
I/O complete
Data given to application
I/O complete
Cached data given to application
I/O complete
Server unaware
Server unaware
No network packets
No network packets
FIGURE 11-21 Lease with multiple handles from the same client.
618
CHAPTER 11
Caching and file systems
File system operations
Applications and the system access files in two ways: directly, via file I/O functions (such as ReadFile
and WriteFile), and indirectly, by reading or writing a portion of their address space that represents a 
mapped file section. (See Chapter 5 of Part 1 for more information on mapped files.) Figure 11-22 is a 
simplified diagram that shows the components involved in these file system operations and the ways in 
which they interact. As you can see, an FSD can be invoked through several paths:
I 
From a user or system thread performing explicit file I/O
I 
From the memory manager’s modified and mapped page writers
I 
Indirectly from the cache manager’s lazy writer
I 
Indirectly from the cache manager’s read-ahead thread
I 
From the memory manager’s page fault handler
Process
Handle
table
File object
File object
Data
attribute
File
control
block
Named
stream
NTFS data
structures
Stream
control
blocks
...
Object
manager
data
structures
Master file
table
...
NTFS
database
(on disk)
FIGURE 11-22 Components involved in file system I/O.
The following sections describe the circumstances surrounding each of these scenarios and the 
steps FSDs typically take in response to each one. You’ll see how much FSDs rely on the memory man-
ager and the cache manager.
CHAPTER 11
Caching and file systems
619
Explicit file I/O
The most obvious way an application accesses files is by calling Windows I/O functions such as 
CreateFile, ReadFile, and WriteFile. An application opens a file with CreateFile and then reads, writes, 
or deletes the file by passing the handle returned from CreateFile to other Windows functions. The 
CreateFile function, which is implemented in the Kernel32.dll Windows client-side DLL, invokes the 
native function NtCreateFile, forming a complete root-relative path name for the path that the applica-
tion passed to it (processing “.” and “..” symbols in the path name) and prefixing the path with “\??” (for 
example, \??\C:\Daryl\Todo.txt).
The NtCreateFile system service uses ObOpenObectByName to open the file, which parses the 
name starting with the object manager root directory and the first component of the path name (“??”). 
Chapter 8, “System mechanisms”, includes a thorough description of object manager name resolution 
and its use of process device maps, but we’ll review the steps it follows here with a focus on volume 
drive letter lookup.
The first step the object manager takes is to translate \?? to the process’s per-session namespace di-
rectory that the DosDevicesDirectory field of the device map structure in the process object references 
(which was propagated from the first process in the logon session by using the logon session referenc-
es field in the logon session’s token). Only volume names for network shares and drive letters mapped 
by the Subst.exe utility are typically stored in the per-session directory, so on those systems when a 
name (C: in this example) is not present in the per-session directory, the object manager restarts its 
search in the directory referenced by the GlobalDosDevicesDirectory field of the device map associated 
with the per-session directory. The GlobalDosDevicesDirectory field always points at the \GLOBAL?? di-
rectory, which is where Windows stores volume drive letters for local volumes. (See the section “Session 
namespace” in Chapter 8 for more information.) Processes can also have their own device map, which is 
an important characteristic during impersonation over protocols such as RPC.
The symbolic link for a volume drive letter points to a volume device object under \Device, so when 
the object manager encounters the volume object, the object manager hands the rest of the path 
name to the parse function that the I/O manager has registered for device objects, IopParseDevice.
(In volumes on dynamic disks, a symbolic link points to an intermediary symbolic link, which points 
to a volume device object.) Figure 11-23 shows how volume objects are accessed through the object 
manager namespace. The figure shows how the \GLOBAL??\C: symbolic link points to the \Device\
HarddiskVolume6 volume device object.
After locking the caller’s security context and obtaining security information from the caller’s token, 
IopParseDevice creates an I/O request packet (IRP) of type IRP_M_CREATE, creates a file object that 
stores the name of the file being opened, follows the VPB of the volume device object to find the vol-
ume’s mounted file system device object, and uses IoCallDriver to pass the IRP to the file system driver 
that owns the file system device object.
When an FSD receives an IRP_M_CREATE IRP, it looks up the specified file, performs security valida-
tion, and if the file exists and the user has permission to access the file in the way requested, returns 
a success status code. The object manager creates a handle for the file object in the process’s handle 
table, and the handle propagates back through the calling chain, finally reaching the application as a 
620
CHAPTER 11
Caching and file systems
return parameter from CreateFile. If the file system fails the create operation, the I/O manager deletes 
the file object it created for the file.
We’ve skipped over the details of how the FSD locates the file being opened on the volume, but 
a ReadFile function call operation shares many of the FSD’s interactions with the cache manager and 
storage driver. Both ReadFile and CreateFile are system calls that map to I/O manager functions, but 
the NtReadFile system service doesn’t need to perform a name lookup; it calls on the object manager 
to translate the handle passed from ReadFile into a file object pointer. If the handle indicates that the 
caller obtained permission to read the file when the file was opened, NtReadFile proceeds to create an 
IRP of type IRP_M_READ and sends it to the FSD for the volume on which the file resides. NtReadFile
obtains the FSD’s device object, which is stored in the file object, and calls IoCallDriver, and the I/O 
manager locates the FSD from the device object and gives the IRP to the FSD.
FIGURE 11-23 Drive-letter name resolution.
CHAPTER 11
Caching and file systems
621
If the file being read can be cached (that is, the FILE_FLAG_NO_BUFFERING flag wasn’t passed to 
CreateFile when the file was opened), the FSD checks to see whether caching has already been initiated 
for the file object. The PrivateCacheMap field in a file object points to a private cache map data struc-
ture (which we described in the previous section) if caching is initiated for a file object. If the FSD hasn’t 
initialized caching for the file object (which it does the first time a file object is read from or written to), 
the PrivateCacheMap field will be null. The FSD calls the cache manager’s CcInitializeCacheMap function 
to initialize caching, which involves the cache manager creating a private cache map and, if another file 
object referring to the same file hasn’t initiated caching, a shared cache map and a section object.
After it has verified that caching is enabled for the file, the FSD copies the requested file data from 
the cache manager’s virtual memory to the buffer that the thread passed to the ReadFile function. The 
file system performs the copy within a try/except block so that it catches any faults that are the result of 
an invalid application buffer. The function the file system uses to perform the copy is the cache man-
ager’s CcCopyRead function. CcCopyRead takes as parameters a file object, file offset, and length.
When the cache manager executes CcCopyRead, it retrieves a pointer to a shared cache map, which 
is stored in the file object. Recall that a shared cache map stores pointers to virtual address control 
blocks (VACBs), with one VACB entry for each 256 KB block of the file. If the VACB pointer for a portion 
of a file being read is null, CcCopyRead allocates a VACB, reserving a 256 KB view in the cache man-
ager’s virtual address space, and maps (using MmMapViewInSystemCache) the specified portion of the 
file into the view. Then CcCopyRead simply copies the file data from the mapped view to the buffer it 
was passed (the buffer originally passed to ReadFile). If the file data isn’t in physical memory, the copy 
operation generates page faults, which are serviced by MmAccessFault.
When a page fault occurs, MmAccessFault examines the virtual address that caused the fault and 
locates the virtual address descriptor (VAD) in the VAD tree of the process that caused the fault. (See 
Chapter 5 of Part 1 for more information on VAD trees.) In this scenario, the VAD describes the cache 
manager’s mapped view of the file being read, so MmAccessFault calls MiDispatchFault to handle a page 
fault on a valid virtual memory address. MiDispatchFault locates the control area (which the VAD points 
to) and through the control area finds a file object representing the open file. (If the file has been opened 
more than once, there might be a list of file objects linked through pointers in their private cache maps.)
With the file object in hand, MiDispatchFault calls the I/O manager function IoPageRead to build 
an IRP (of type IRP_M_READ) and sends the IRP to the FSD that owns the device object the file object 
points to. Thus, the file system is reentered to read the data that it requested via CcCopyRead, but this 
time the IRP is marked as noncached and paging I/O. These flags signal the FSD that it should retrieve 
file data directly from disk, and it does so by determining which clusters on disk contain the requested 
data (the exact mechanism is file-system dependent) and sending IRPs to the volume manager that 
owns the volume device object on which the file resides. The volume parameter block (VPB) field in the 
FSD’s device object points to the volume device object.
The memory manager waits for the FSD to complete the IRP read and then returns control to 
the cache manager, which continues the copy operation that was interrupted by a page fault. When 
CcCopyRead completes, the FSD returns control to the thread that called NtReadFile, having copied the 
requested file data, with the aid of the cache manager and the memory manager, to the thread’s buffer.
622
CHAPTER 11
Caching and file systems
The path for WriteFile is similar except that the NtWriteFile system service generates an IRP of type 
IRP_M_WRITE, and the FSD calls CcCopyWrite instead of CcCopyRead. CcCopyWrite, like CcCopyRead,
ensures that the portions of the file being written are mapped into the cache and then copies to the 
cache the buffer passed to WriteFile.
If a file’s data is already cached (in the system’s working set), there are several variants on the 
scenario we’ve just described. If a file’s data is already stored in the cache, CcCopyRead doesn’t incur 
page faults. Also, under certain conditions, NtReadFile and NtWriteFile call an FSD’s fast I/O entry point 
instead of immediately building and sending an IRP to the FSD. Some of these conditions follow: the 
portion of the file being read must reside in the first 4 GB of the file, the file can have no locks, and 
the portion of the file being read or written must fall within the file’s currently allocated size.
The fast I/O read and write entry points for most FSDs call the cache manager’s CcFastCopyRead
and CcFastCopyWrite functions. These variants on the standard copy routines ensure that the file’s 
data is mapped in the file system cache before performing a copy operation. If this condition isn’t met, 
CcFastCopyRead and CcFastCopyWrite indicate that fast I/O isn’t possible. When fast I/O isn’t possible, 
NtReadFile and NtWriteFile fall back on creating an IRP. (See the earlier section “Fast I/O” for a more 
complete description of fast I/O.)
Memory manager’s modified and mapped page writer
The memory manager’s modified and mapped page writer threads wake up periodically (and when 
available memory runs low) to flush modified pages to their backing store on disk. The threads call 
IoAsynchronousPageWrite to create IRPs of type IRP_M_WRITE and write pages to either a paging file 
or a file that was modified after being mapped. Like the IRPs that MiDispatchFault creates, these IRPs 
are flagged as noncached and paging I/O. Thus, an FSD bypasses the file system cache and issues IRPs 
directly to a storage driver to write the memory to disk.
Cache manager’s lazy writer
The cache manager’s lazy writer thread also plays a role in writing modified pages because it periodi-
cally flushes views of file sections mapped in the cache that it knows are dirty. The flush operation, 
which the cache manager performs by calling MmFlushSection, triggers the memory manager to write 
any modified pages in the portion of the section being flushed to disk. Like the modified and mapped 
page writers, MmFlushSection uses IoSynchronousPageWrite to send the data to the FSD.
Cache manager’s read-ahead thread
A cache uses two artifacts of how programs reference code and data: temporal locality and spatial 
locality. The underlying concept behind temporal locality is that if a memory location is referenced, 
it is likely to be referenced again soon. The idea behind spatial locality is that if a memory location is 
referenced, other nearby locations are also likely to be referenced soon. Thus, a cache typically is very 
good at speeding up access to memory locations that have been accessed in the near past, but it’s ter-
rible at speeding up access to areas of memory that have not yet been accessed (it has zero lookahead 
CHAPTER 11
Caching and file systems
623
capability). In an attempt to populate the cache with data that will likely be used soon, the cache man-
ager implements two mechanisms: a read-ahead thread and Superfetch.
As we described in the previous section, the cache manager includes a thread that is responsible for 