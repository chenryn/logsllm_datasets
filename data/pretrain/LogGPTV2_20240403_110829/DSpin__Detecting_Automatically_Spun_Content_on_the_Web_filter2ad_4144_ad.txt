2Fiverr is an online community where services are offered for hire at some
predetermined cost.
8
know the corresponding id for the start and end of an interval
of time.
Using this technique, we crawl over 1M articles over a
month from the id range 6M–7M, which corresponds to articles
posted between January 2012 to May 2013. We crawl at a rate
of 2.7 URLs per second to limit the load on the site. This data
set overlaps in time with the wiki data set that spans the month
of January of 2013.
B. Filters
Before analyzing the posted articles for spun content, we
apply several pre-ﬁlters to the data set. These pre-ﬁlters remove
pages that lack substantive English text. These pages would
likely be ﬁltered during a search engine page rank algorithm.
We apply each ﬁlter in sequence on both data sets to reduce
the overall processing required.
Visible text: We remove all pages that do not contain any
visible text on the page. We detect visibly blank pages by
ﬁrst stripping all HTML tags. If no visible text remains, we
remove the page from the data set.
Content tag: The pages in each set share a common HTML
tag that embeds the content of interest. For pages in the Wiki
data set, the tag is a div labeled “bodyContent”. For pages
from GoArticles, the tag is a div with “class=article”. If a
crawled page lacks the appropriate tag, such as index page,
we remove the page from the data set.
Word count: We discard small pages from analysis. Pages
with a small number number of words often result in false
positives with similarity algorithms. From manually examining
article pages, we ﬁnd it rare that spammers post spun article
content using small articles. Unlike manual spinning, there
is no incentive in automated spinning to favor small pages.
Consequently, although small pages might contain spam, in
our experience, little of that spam is generated using spinning
tools. We ﬁnd more short posts contain unintelligible words
interlaced with lots of links. From experimentation, we ﬁnd
that a threshold of 50 words was a good balance between false
positives and negatives.
Link density: We also discard pages with an unusually high link
density. Again, we ﬁnd that spammers using spun article pages
typically do not litter the page with large numbers of links in
the spun text, perhaps to avoid other detection algorithms that
speciﬁcally look for such features. Thus we discard any page
that has a link density of every ﬁfth word or higher.
Foreign text: Since we reverse engineer only the English
synonym dictionary from TBS, we only evaluate the immutable
method on pages with mostly English text. (Otherwise, ef-
fectively every word is labeled as an immutable.) We use
a language detector library [31] to identify the language of
each page, and discard any page not identiﬁed as English.
Since there is nothing inherent about the immutable method
that ties it to English, we believe it would readily extend to
other languages given access to synonym dictionaries for those
languages.
C. Inverted Indexing
A naive pairwise comparison of two documents leads to
O(n2) comparisons, which is infeasible for processing data at
scale. Therefore, we implement the immutable method using
inverted indexes, similar to the method described in [6]. For
every immutable in the text, we generate a pair:
(5)
The id is a unique index corresponding to an article, and immu
is an immutable that occurs in id. We differentiate duplicate
immutables by marking each with a unique sufﬁx number to
simplify document comparison. Next, we perform a “group
by” on the immutables:
>
(6)
Each group represents all document
ids that contain the
immutable. We decompose each group into a pairwise id key
and a “1”. Each pairwise id, idi : idj, indicates a single shared
immutable between two documents. Each group with N ids
therefore has N 2/2 pairs. The key-value pairs appear as:
(7)
Last, we “group by” idi : idj and count the number of ones,
yielding:
(8)
The count represents the total number of immutables that
overlap between idi and idj. This format is also convenient
for ﬁnding the total number of immutables in the original
document. For a document idi, the number of its immutables
is given by idi : idi, namely the number of total overlapping
immutables an article has with itself. From the list of pairs
, we calculate the similarity score between
each two pages. We set the threshold for the similarity score to
be 75%. We determine this threshold from the training data set,
in which the lowest similarity score we found for any cluster
was 74.9%.
D. Clustering
The id pairs can be transformed into clusters to convey
more information. We cluster duplicate content, near duplicate
content and spun content as detailed in Section VI both
for ﬁltering and also for assessing the behavior of spam
campaigns.
To transform pairs into clusters, we use a graph represen-
tation where each page is a node and each pair has an edge in
the graph. Each connected subgraph represents a cluster. We
ﬁrst build the graph using pairs or edges as input and the ids
as nodes. For each node, we traverse all reachable nodes using
breadth-ﬁrst search and mark every node traversed as visited.
We continually process unvisited nodes until every node has
been visited. The results are disjoint clusters of ids.
E. Exact Duplicates and Near Duplicates
Advice posted in SEO forums advises against posting iden-
tical content with backlinks across multiple sites since search
engines are capable of detecting such duplicate content [23].
In practice, we ﬁnd that spammers continue to post identical
content. By default, the various similarity algorithms discussed
in Section IV would detect exact duplicates as spun content.
Since our focus is spun content, we separate exact duplicates
from the analysis of spun content in experiments.
9
We identify exact duplicates by generating a hash over each
page in the form of an MD5 sum: two articles are identical if
their MD5 sums match. We add this sum to the immutables
list for each page. When analyzing a two-page pair, we ﬁrst
determine if the two pages are identical. This is done by
determining if their similarity score is 100%; the MD5 sums
must match to obtain this value, since the immutables include
this hash. Anything less than 100% implies that the pages are
not exact duplicates. If the pages are not exact duplicates,
we adjust the similarity score calculation to account for the
extra MD5 sum by subtracting two from the denominator, and
recompute the similarity score using this modiﬁed version.
This method gives us the same similarity score as if the hash
sums are not added.
We also attempt to identify near duplicates and to separate
them from our spinning analysis. We achieve this using the
mutable veriﬁer. In the results, we ﬁnd articles with a 100%
mutable match, but with mismatching MD5 sums. Manual
examination of examples shows that these are near duplicate
pages where the text on the two pages are identical, with minor
differences in the links or spacing, which we characterize as
near duplicates in our data set. Recall in Table II, no spun
page has a mutable similarity score of 100% match. Thus we
separate these pages from the spun page analysis as well.
F. Hardware
We run our experiments on a shared cluster with 24
physical nodes running Fedora Core 14. Each node has a single
Xeon X3470 Quad-Core 2.93GHz CPU and 24 GB of memory.
The experiments are written in Java, and run as a combination
of Hadoop 1.1.2 and Pig 0.11.1 jobs.
VI. SPINNING IN THE WILD
We next use DSpin to identify spun content in the two
crawled data sets. We examine the amount of spun content,
characteristics of the clusters of spun pages including size,
content, and number of sites targeted, and ﬁnally look at the
relationship between the wiki sites and the article directory for
spinning and spamming.
A. Volume
First, we report the total volume of spun content found
using the immutable method for both the wiki and GoArticles
data sets. For each data set, we apply ﬁlters that, in sequence,
remove articles that have no visible text (visible), no expected
content tag (body), insufﬁcient word count (wc), too many
links (link), or a language other than English (english) as
discussed in Section V-B. After applying ﬁlters, we obtain all
page pairs that match according to the immutable method.
We divide these pairs into exact and non-exact duplicates.
We form clusters of the exact duplicates, and use the smallest
alpha-numeric ID to represent each duplicate cluster. Next, we
cluster all non-duplicate pairs. During this clustering phase,
we remove all IDs found in the duplicate clusters with the
exception of the smallest alpha-numeric ID.
After removing exact duplicates, we validate each edge (an
edge represents a match between two pages) with the mutable
veriﬁer discussed in Section IV-C. The mutable veriﬁcation
!
)
K
0
0
1
(
s
e
g
a
P
l
a
t
o
T
14!
12!
10!
8!
6!
4!
2!
0!
original!
visible!
body!
wc!
link!
wiki! GoArticles!
english!
duplicates!
im mutable!
mutable!
Fig. 6. Effect of applying ﬁlters on the wiki and GoArticles data sets. The
last column shows the ﬁnal number of spun pages identiﬁed.
identiﬁes near-duplicate
process removes unmatched pairs,
pairs, and removes them as well. Figure 6 shows the size of the
data set after each step of ﬁltering for the wiki and GoArticles
data sets, as well as applying the mutable ﬁlter. Finally, the
last column reports the total number of spun pages found.
Wiki data set: The wiki data set contains the ﬁrst crawl of
each day over a month, yielding 1, 233, 595 initial pages of
the 37M total. To make data processing time reasonable, we
did not use all crawled pages in the analysis. And we chose
one of the crawls each day to get a longer-term sample over
time (rather than many of the crawls per day). Each of the
ﬁlters has a notable impact on the data set (e.g., there are
many empty, tiny, or non-English postings), leaving 632, 966
after ﬁltering as our baseline set of pages. Of these, 205, 085
are duplicates and removing them yields 427, 881 pages with
substantive content. Of those, the immutable method labels
244, 107 pages as potentially spun. After applying the mutable
veriﬁer, which veriﬁes the pages and removes near duplicates,
DSpin identiﬁed a total of 225, 070 spun pages.
GoArticles data set: We crawled 1, 239, 700 GoArticles
pages over the time period from January 2012 to May 2013.
Most of the pages have substantive content, so the ﬁlters affect
this data set less. As an article directory, GoArticles has much
less spam on it than the abused wikis. Applying the ﬁlters and
removing exact duplicates leaves 1, 003, 317 pages. Of those,
the immutable method labels 100, 181 as likely spun. With the
mutable veriﬁer and near duplicates removed, DSpin identiﬁed
71, 876 spun pages.
In summary, after ﬁltering the wiki pages DSpin identiﬁed
a majority of the pages, 68.0%, as SEO spam. Of these,
32.4% are exact duplicates and 35.6% are spun content. The
GoArticles data set has drastically less spun content (7.0%)
than the wiki data set. These results align with expectations.
The GoArticles data set is a legitimate article directory that
purportedly tries to ﬁlter spam content, whereas the wiki data
set contains sites known to be targets for spammers. From this
point forward we will focus our discussion on the spun content
only.
10
F
D
C
1
0.8
0.6
0.4
0.2
0
100
90% of cluster size <= 44
80% of cluster size <= 9
101
102
Cluster Size
103
104
F
D
C
1
0.8
0.6
0.4
0.2
0
100
90% of cluster size <= 3
80% of cluster size <= 2
101
Cluster Size
102
103
Fig. 7. CDF of cluster sizes for the wiki data set.
Fig. 8. CDF of cluster sizes for the GoArticles data set.
B. False Positives
We examined the false positive rate of the clustered data
set. To this end, we randomly sampled 99 clusters from both
the GoArticles and wiki data sets. For each cluster, we chose
two pages at random, inspected them visually, and determined
if the two articles appear to be spun from a common source.
Using this methodology, we examined a total of 396 articles
and found no evidence of false positives, where a false positive
is deﬁned as two articles that appear in the same cluster but
are unrelated.
However, we ﬁnd some clusters contain articles with
mostly duplicate content. These texts are mostly the same with
differences in adding a title and/or footer. In the wiki dataset,
27 out of 72 article pairs are mostly duplicates; while in the
GoArticles dataset, 14 out of 85 samples are mostly duplicates.
Further, manual inspection of the cluster samples reveals
that there were some differences between spun articles. Some