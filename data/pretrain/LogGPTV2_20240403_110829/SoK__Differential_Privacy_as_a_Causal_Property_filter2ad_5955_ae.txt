Despite being only implied by, not equivalent to, DP, Deﬁ-
nition 7 captures the intuition behind the characterization (∗)
of DP that “changing a single individual’s data in the database
leads to a small change in the distribution on outputs” [25,
p. 2]. To get an equivalence, we can quantify over all pop-
ulations as we did to get an equivalence for association, but
this time we need not worry about zero-probability data points
n)]
or independence. This simpliﬁes the deﬁnition and makes it a
more natural characterization of DP.
Deﬁnition 8 (Universal Data-point Intervention D.P.). A
randomized algorithm A is -differentially private as universal
intervention on a data point if for all population distributions
P, for all i, for all data points di and d(cid:48)
i in D, and for all
output values o,
PrMA,P [O=o | do(Di=di)] ≤ e PrMA,P [O=o | do(Di=d(cid:48)
i)]
where O := A(D) and D := (cid:104)D1, ..., Dn(cid:105).
Proposition 5. Deﬁnitions 1 and 8 are equivalent.
Proof. That Deﬁnition 1 implies 8 follows from Proposition 4.
Assume Deﬁnition 8 holds. W.l.o.g., assume i = n. Then,
for all P, o in O, and d(cid:48)
(cid:88)
PrP(cid:2)∧n−1
PrMA,P [O=o | do(Di=di)] ≤ e ∗ PrMA,P [O=o | do(Di=d(cid:48)
i)]
≤ e(cid:88)
(cid:3) ∗ PrA[A((cid:104)d1, ..., dn−1, dn(cid:105))=o]
(cid:3) PrA[A((cid:104)d1, ..., dn−1, d(cid:48)
PrP(cid:2)∧n−1
(cid:104)d1,. . . ,dn−1(cid:105)∈Dn−1
n in D,
i=1 Di=di
i=1 Di=di
(cid:104)d1,. . . ,dn−1(cid:105)∈Dn−1
n(cid:105))=o]
(6)
†
For any d
1, ..., d
follows from Lemma 2 in Appendix C.
†
n−1 in Dn−1, let P d
i=1 Di=d
†
1,. . . ,d
†
n−1
PrP d
†
1, ..., d†
For any d
(7)
†
n−1 be such that
†
i
= 1
(cid:105)
†
1,. . . ,d
(cid:104)∧n−1
(cid:3) PrA[A((cid:104)d1, ..., dn−1, d†
n in D, (6) implies
(cid:3)
i=1 Di=di
∗ PrA[A((cid:104)d1, ..., dn−1, d(cid:48)
n(cid:105))=o]
n in Dn and d(cid:48)
(cid:2)∧n−1
i=1 Di=di
(cid:2)∧n−1
n(cid:105))=o]
(cid:88)
≤ e(cid:88)
PrP d
†
1 ,. . . ,d
†
n−1
(cid:104)d1,. . . ,dn−1(cid:105)∈Dn−1
PrP d
†
1 ,. . . ,d
†
n−1
(cid:104)d1,. . . ,dn−1(cid:105)∈Dn−1
Thus,
†
PrA[A((cid:104)d
1, ..., d
n(cid:105))=o]
†
n−1, d†
†
≤ e PrA[A((cid:104)d
1, ..., d
†
n−1, d(cid:48)
n(cid:105))=o]
(cid:2)∧n−1
(cid:3)
since both sides has a non-zero probability for
i=1 Di=di
†
at only the sequence of data point values d
1, ..., d
PrP d
†
1,. . . ,d
†
n−1
†
n−1.
VI. BOUNDING EFFECTS: GENERALIZING D.P.,
UNDERSTANDING ALTERNATIVES
To recap, we have shown that reasoning about DP as a causal
property is more straightforward than reasoning about it as an
associative property. Still, one might wonder, Why express
DP in either form? Why not just stick with its even simpler
expression in terms of functions in Deﬁnition 1?
In this section, we show what
is gained by the causal
view. We show that DP bounds a general notion of effect size.
Essentially, DP limits the causal consequences of a decision
to contribute data to a data set. If the consequences are small,
then an individual will need less encouragement (e.g., ﬁnancial
incentives) to set aside privacy concerns.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:36:11 UTC from IEEE Xplore.  Restrictions apply. 
363
We show that this general notion can also capture alternative
privacy deﬁnitions, including some arising from concerns over
dependent data points. A common causal framework allows us
to precisely compare these deﬁnitions.
A. Bounded Relative Probability (BRP)
Generalizing from the decision to participate in a data
set, we deﬁne a more general notation for any two random
variables X and Y . To do so, we need a description of how
X and Y relate to one another. Recall that a probabilistic SEM
(cid:104)M,P(cid:105) shows the causal and statistical relations between
random variables by providing a list of structural equations
M and a distribution P over variables not deﬁned in terms of
others (exogenous variables). (See Appendix C for details.)
We will measure the size of the effects of X on Y using
relative probabilities, better known as relative risk and as risk
ratio with clinical studies of risks in mind. For three (binary)
propositions ρ, φ, and ψ, let
RPM,P (ρ, φ, ψ) =
PrM,P [ρ | do(φ)]
PrM,P [ρ | do(ψ)]
denote the relative probability. (Some authors also allow
using conditioning instead of interventions.) For two random
variables X and Y , we can characterize the maximum effect
of X on Y as
¯RP M,P (Y, X) = max
y,x1,x2
RPM,P (Y =y, X=x1, X=x2)
Expanding these deﬁnitions out shows that -differential pri-
vacy places a bound on the maximum of the maximum relative
probabilities:
¯RP M,P (O, Di) ≤ e
maxP,i
where M describes the differentially private algorithm A.
Note that our use of maximization is similar Yang et al. [49,
p. 749, Def. 4], which we quote in Section III.
With this in mind, we propose to use ¯RP for a general
purpose effect-size restriction:
Deﬁnition 9 (BRP). A causal system described by M has
-bounded relative probability (BRP) for X to Y iff
¯RP M,P (Y, X) ≤ e
maxP
X
Y
Z
To model that the second output Z could be computed with
one of any of a set of algorithms but that each of algorithm
has a bounded effect from X to Z, we look at Z’s behavior in
sub-models M[Y := y] where each setting of Y corresponds
to a selecting one available algorithm.
Theorem 1. For any SEM M such that X, Y , and Z are in
sequence and the parents of Z are {X, Y }, if X has 1-BRP
to Y in M and 2-BRP to Z in M[Y := y] for all y in Y,
then X has (1 + 2)-BRP to (cid:104)Y, Z(cid:105) in M.
Proof. Consider any probability distribution P, x and x(cid:48) in
X , y in Y, and z in Z. Since the effect of X on Y is bounded
by 1-BRP,
PrM,P [Y =y | do(X=x)] ≤ e1 PrM,P [Y =y | do(X=x(cid:48))]
Since the parents of Z are {X, Y }, Pearl’s Property 1 [42,
p. 24] shows that for any y such that PrM,P [Y =y] > 0,
PrM,P [Z=z | Y =y, do(X=x)]
= PrM,P [Z=z | do(Y =y), do(X=x)]
Since there’s 2-BRP from X to Z in M [Y := y] for all y,
this implies that
PrM,P [Z=z | Y =y, do(X=x)]
≤ e2 PrM,P [Z=z | Y =y, do(X=x(cid:48))]
Thus,
PrM,P [(cid:104)Y, Z(cid:105) = (cid:104)y, z(cid:105) | do(X=x)]
= PrM,P [Z=z | Y =y, do(X=x)] PrM,P [Y =y | do(X=x)]
≤ e2 PrM,P [Z=z | Y =y, do(X=x(cid:48))]
∗ e1 PrM,P [Y =y | do(X=x(cid:48))]
= e1+2 PrM,P [(cid:104)Y, Z(cid:105) = (cid:104)y, z(cid:105) | do(X=x(cid:48))]
We can generalize this theorem for Z having additional
parents by requiring BRP for all of their values as well.
The special case of this theorem where 2 = 0 is known as
the postprocessing condition:
Differential privacy is equivalent to requiring -BRP for all
data points Di.
B. Composition
BRP enjoys many of the same properties as DP. Recall
that DP has additive sequential composition for using two
differentially private algorithms one after the next, even if
the second is selected using the output of the ﬁrst [38].
Similarly, BRP has additive sequential composition for two
random variables.
To model the second output Z depending upon the ﬁrst Y ,
but not the other way around, we say random variables X,
Y , and Z are in sequence if X may affect Y and Z, and Y
may affect Z, but Z may not affect X nor Y , and Y may not
affect X. That is,
X

Y
Z
For this causal diagram, Theorem 1 ensures that if the arrow
from X to Y is -BRP, then any subsequent consequence Z of
Y is also going to be -BRP. This captures the central intuition
behind DP and BRP that they limit any downstream causal
consequences of a variable X.
C. Application
While the explicit causal reasoning in BRP can sharpen our
intuitions about privacy, BRP is not itself a privacy deﬁnition.
Only some choices of variables to bound yield reasonable
privacy guarantees. Below, we use BRP to express some of
well known variations of DP. Doing so both shows some
reasonable ways of using BRP to provide privacy guarantees
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:36:11 UTC from IEEE Xplore.  Restrictions apply. 
364
and demonstrates that BRP provides a common framework for
precisely stating and comparing these variations.
First, consider the randomized response method of provid-
ing privacy in which each survey participant adds noise to his
own response before responding [48]. Let each person’s actual
attribute be Ri, let the noisy response he provides be Di, and
let O be the output computed from all the Di. Unlike with
(standard) DP, the causal path from Di to O has unbounded
BRP, may not contain any random algorithms, and misses the
privacy protection altogether. Similarly, the path from Ri and
O has unbounded BRP due to the possibility of the Ri having
effects upon one another. However, the randomized response
method does ensure -BRP from Ri to Di for all i where 
depends upon the amount of noise added to Di.
Second, we consider group privacy, the idea that a group
of individuals may be so closely related that their privacy is
intertwined. Differential privacy approaches group privacy by
summing the privacy losses, measured in terms of , of each
individual in the group [11, p. 9]. Similarly, we can add the
relative probabilities of multiple random variables to get a total
effect size. Alternately, BRP can easily be extended to measure
simultaneous joint interventions by using multiple instances
of the do operator. The total effect size may be larger than
the joint effect size since, in cases where the intervened upon
variables affect one another, interventions on a downstream
variable can mask interventions on its parents. Returning to
the example of Section I-A, the total effect for both Ada’s
attribute R1 and Byron’s R2 is 3 with 2 of that coming
from R1. However, the joint effect is 2 since R1 achieved
half of its effect via R2. In examples like this where the
variables correspond to different moral entities, the total effect
size strikes us as more reasonable since it accounts for both
Ada and Byron experiencing a privacy loss. If on the other
hand, the variables correspond to a single topic about a single
person, such as weight and waist size, then the joint effect
size seems more reasonable. However, we see this choice as
under explored since it does not emerge for DP given that data
points cannot not affect one another.
Third, we consider a line of papers providing deﬁnitions of
privacy that account for dependencies between data points, but
which are ambiguous about association versus causation [5],
[50], [33]. For example, Liu et al. use the word “cause”
in a central deﬁnition of their work [33, Def. 3], but do no
causal modeling, instead using a joint probability distribution
to model just associations in their adversary model [33, §3].
Using causal modeling and BRP would allow them to actually
model causation instead of approximating it with associations,
or, if associations really is what they wish to model, would
provide a foil making their goals more clear.
Fourth, as a more complex example, Kifer and Machanava-
jjhala consider applying DP to social networks [27, §3]. They