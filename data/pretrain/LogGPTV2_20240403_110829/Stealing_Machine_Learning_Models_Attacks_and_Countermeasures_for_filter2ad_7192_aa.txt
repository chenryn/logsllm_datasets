# Title: Stealing Machine Learning Models: Attacks and Countermeasures for Generative Adversarial Networks

## Authors
- Hailong Hu, SnT, University of Luxembourg, Esch-sur-Alzette, Luxembourg, [EMAIL]
- Jun Pang, FSTM & SnT, University of Luxembourg, Esch-sur-Alzette, Luxembourg, [EMAIL]

## Abstract
Model extraction attacks aim to replicate a machine learning model by querying the target model. While early studies focused on discriminative models, the exploration of such attacks against generative models, particularly Generative Adversarial Networks (GANs), remains limited. In this paper, we systematically investigate the feasibility of model extraction attacks on GANs. We define fidelity and accuracy in the context of these attacks and study them from the perspectives of fidelity extraction and accuracy extraction, based on the adversary's goals and available background knowledge. We conduct a case study where an adversary transfers knowledge from a state-of-the-art GAN trained on over 3 million images to new domains, broadening the scope of model extraction attacks. Finally, we propose effective defense techniques to protect GANs, balancing utility and security.

## CCS Concepts
- **Security and privacy**
- **Computing methodologies** → Machine learning

## Keywords
- Model extraction
- Generative adversarial networks
- Transfer learning
- Perturbation-based defenses

## ACM Reference Format
Hailong Hu and Jun Pang. 2021. Stealing Machine Learning Models: Attacks and Countermeasures for Generative Adversarial Networks. In Annual Computer Security Applications Conference (ACSAC '21), December 6–10, 2021, Virtual Event, USA. ACM, New York, NY, USA, 16 pages. https://doi.org/10.1145/3485832.3485838

## 1. Introduction
Over the past few years, machine learning, especially deep learning, has made significant advancements in various fields, including computer vision [6, 32, 33, 63] and natural language processing (NLP) [15, 35]. Machine learning models are often considered intellectual property and are closely safeguarded. Training a practical deep learning model is non-trivial, requiring large datasets, substantial computing resources, and human expertise [7, 15, 33, 55, 63, 71]. Additionally, exposing deep learning models to potential adversaries poses security and privacy threats [42, 43, 51, 56, 60, 64].

Model extraction attacks, which aim to duplicate a model through query access, have recently gained attention. Early work by Tramèr et al. [64] focused on traditional machine learning models and shallow neural networks. Jagielski et al. [27] extended this to more complex models, making such attacks more practical. Studies on model extraction in NLP tasks, such as those by Krishna et al. [34], have also been conducted. However, most of these studies focus on discriminative models, leaving generative models, particularly GANs, relatively unexplored.

Compared to discriminative models, generative models present unique challenges. Discriminative models provide labels, probabilities, and logits, which can be used for model extraction. In contrast, GANs only return generated images. Furthermore, discriminative models are evaluated on test datasets, while GANs are assessed using metrics like Fréchet Inception Distance (FID) [23] and Multi-Scale Structural Similarity (MS-SSIM) [48], or qualitative measures like preference judgment [26, 72].

In this paper, we systematically study the feasibility of model extraction attacks on GANs, focusing on fidelity and accuracy. We define these terms and explore two types of attacks: fidelity extraction and accuracy extraction. We also conduct a case study demonstrating the impact of such attacks on a large scale and propose effective defense strategies.

## 2. Related Work
### Generative Adversarial Networks (GANs)
GANs have achieved impressive performance in areas such as image synthesis [6, 31–33, 39, 47, 53, 57], image-to-image translation [40, 52, 73], and texture generation [37, 69]. State-of-the-art GANs, such as SNGAN [47] and PGGAN [31], generate highly realistic images. SNGAN uses spectral normalization for stable training, while PGGAN generates high-resolution face images. BigGAN [6] and StyleGAN [32] further improve image quality with conditional GANs and style transfer, respectively. In this paper, we use SNGAN and PGGAN as target models, and StyleGAN in a case study.

### Model Extraction Attacks
With the rise of machine learning as a service (MLaaS), model extraction attacks have become a significant concern. These attacks can be categorized into accuracy model extraction and fidelity model extraction. Accuracy model extraction, first proposed by Tramèr et al. [64], aims to achieve similar or better performance on the test dataset. Fidelity model extraction requires the attack model to faithfully reproduce the target model's predictions, including its errors. Studies on model extraction in NLP, such as those by Krishna et al. [34], have also been conducted. However, these primarily focus on discriminative models, unlike our work, which targets GANs.

### Other Privacy and Security Attacks
Other related attacks include membership inference attacks [13, 20, 56, 59, 60] and property inference attacks [18]. Some research has explored membership inference attacks against GANs, revealing information about the training dataset [13, 20, 24]. These studies focus on privacy, whereas our work concentrates on the security of the machine learning model itself.

### Model Extraction Defenses
Defenses against model extraction include limiting the number of queries and perturbation-based methods. We propose input and output perturbation-based approaches to degrade the quality of generated samples and reduce the amount of distribution information revealed.

## 3. Preliminary Knowledge
[This section will introduce the necessary background and definitions for understanding the rest of the paper.]

## 4. Taxonomy of Model Extraction Attacks on GANs
[This section will categorize and describe the different types of model extraction attacks on GANs.]

## 5. Fidelity Extraction
[This section will detail the fidelity extraction attack, including the methodology, experimental setup, and results.]

## 6. Accuracy Extraction
[This section will detail the accuracy extraction attack, including the methodology, experimental setup, and results.]

## 7. Case Study
[This section will present a case study demonstrating the impact of model extraction attacks on a large-scale scenario.]

## 8. Defense Mechanisms
[This section will discuss possible defense mechanisms to mitigate model extraction attacks on GANs.]

## 9. Conclusion
[This section will summarize the contributions and findings of the paper and suggest future directions for research.]

---

This revised version of the text provides a clear, coherent, and professional structure, ensuring that each section flows logically and is well-defined.