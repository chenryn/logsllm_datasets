title:Stealing Machine Learning Models: Attacks and Countermeasures for
Generative Adversarial Networks
author:Hailong Hu and
Jun Pang
Stealing Machine Learning Models: Attacks and
Countermeasures for Generative Adversarial Networks
Hailong Hu
SnT, University of Luxembourg
Esch-sur-Alzette, Luxemburg
PI:EMAIL
Jun Pang
FSTM & SnT, University of Luxembourg
Esch-sur-Alzette, Luxemburg
PI:EMAIL
ABSTRACT
Model extraction attacks aim to duplicate a machine learning model
through query access to a target model. Early studies mainly focus
on discriminative models. Despite the success, model extraction
attacks against generative models are less well explored. In this
paper, we systematically study the feasibility of model extraction
attacks against generative adversarial networks (GANs). Specif-
ically, we first define fidelity and accuracy on model extraction
attacks against GANs. Then we study model extraction attacks
against GANs from the perspective of fidelity extraction and accu-
racy extraction, according to the adversary’s goals and background
knowledge. We further conduct a case study where the adversary
can transfer knowledge of the extracted model which steals a state-
of-the-art GAN trained with more than 3 million images to new
domains to broaden the scope of applications of model extraction at-
tacks. Finally, we propose effective defense techniques to safeguard
GANs, considering a trade-off between the utility and security of
GAN models.
CCS CONCEPTS
• Security and privacy; • Computing methodologies → Ma-
chine learning;
KEYWORDS
Model extraction; Generative adversarial networks; Transfer learn-
ing; Perturbation-based defenses
ACM Reference Format:
Hailong Hu and Jun Pang. 2021. Stealing Machine Learning Models: Attacks
and Countermeasures for Generative Adversarial Networks. In Annual
Computer Security Applications Conference (ACSAC ’21), December 6–10,
2021, Virtual Event, USA. ACM, New York, NY, USA, 16 pages. https://doi.
org/10.1145/3485832.3485838
1 INTRODUCTION
Over the past few years, machine learning, deep learning in par-
ticular, has gained significant advances in a variety of areas, such
as computer vision [6, 32, 33, 63] and natural language processing
(NLP) [15, 35]. In general, machine learning models are often con-
sidered as the intellectual property of model owners and are closely
This work is licensed under a Creative Commons Attribution International
4.0 License.
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
© 2021 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-8579-4/21/12.
https://doi.org/10.1145/3485832.3485838
safeguarded. The reasons are from at least two aspects. First, obtain-
ing a practical deep learning model is non-trivial. This is because
training a model requires a large number of training data, intensive
computing resources and human resources [7, 15, 33, 55, 63, 71].
Second, deep learning models themselves are confidential, and ex-
posure of deep learning models to potential adversaries poses a
threat to security and privacy [42, 43, 51, 56, 60, 64]. However,
model extraction attack — a novel attack surface targeting at du-
plicating a model only through query access to a target model, has
recently emerged and gained significant attention from the research
community.
In the early study, Tramèr et al. [64] first attempt model extrac-
tion on traditional machine learning models and shallow neural
networks, such as logistic regression, decision tree, support vector
machine and multilayer perceptrons. Since then, Jagielski et al. [27]
further mount the attack against a million of parameters model
trained on billions of Instagram images [44], which makes model
extraction attack more practical. In addition to model extraction
on deep convolutional neural networks about image classification,
there are some works studying the problem of model extraction in
NLP tasks [34, 61]. For instance, with the assumption that victim
models are trained based on the pretrained BERT model, Krishna
et al. [34] show that an adversary can effectively extract language
models whose performance is only slightly worse than that of the
victim models. However, to the best of our knowledge, these model
extraction attacks mainly focus on discriminative models. The at-
tack against generative models, GANs in particular, is still an open
question.
Comparing to model extraction attacks on discriminative mod-
els, we observe that there exist some differences for generative
models. First, adversaries can leverage output information from tar-
get models such as labels, probabilities and logits, to mount model
extraction attacks on discriminative models [27, 42, 49, 64], while
generative models do not provide such information but only return
images. Second, model extraction attacks on discriminative models
are evaluated on a test dataset. In contrast, unsupervised generative
models aiming to learn the distribution of training data are evalu-
ated by quantitative measures such as Fréchet Inception Distance
(FID) [23] and multi-scale structural similarity (MS-SSIM) [48], or
qualitative measures such as preference judgment [26, 72]. There-
fore, these differences indicate that model extraction strategies,
evaluations and defenses on generative models are very different
from these on discriminative models.
In this paper, we aim to systematically study the feasibility of
model extraction attacks against GANs from the perspective of
fidelity extraction and accuracy extraction. First, we define fidelity
and accuracy of model extraction on GANs. More specifically, when
1ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Hailong Hu and Jun Pang
an adversary mounts model extraction attacks against GANs, fi-
delity measures the difference of data distribution between the
attack model and the target model, while accuracy ensures the dis-
tribution of the attack model is consistent with the distribution of
the training set of the target model. In the next step, according to
the adversary’s goals and the background information that they can
have access to (see Figure 2), we systematically study two different
types of attacks on GANs: fidelity extraction attack and accuracy
extraction attack, which are shown in Figure 1.
Fidelity Extraction Attack. Adversaries mounting fidelity extrac-
tion focus on fidelity and they aim to steal the distribution of a target
model. For this attack, we assume adversaries have no knowledge of
the architecture of target models, and they either obtain a batch of
generated data that the model owner has publicly released or query
the target model to obtain generated data. It can be considered as a
black-box fidelity extraction. After obtaining the generated data,
adversaries can train a copy of the target GAN model. We study two
different target models: Progressive GAN (PGGAN) [31] and Spec-
tral Normalization GAN (SNGAN) [47]. Extensive experimental
evaluations show that fidelity extraction can achieve an excellent
performance with only about 50K queries (i.e., 50K generated sam-
ples). When we continue to increase the number of queries, we
find that it cannot bring significant improvement of the accuracy
of attack models. This is mainly because the discriminator of a
target GAN model is often better than its corresponding generator
and it is very hard to reach global optimum [3]. In other words,
directly querying the target model enables the attack model to be
more consistent with the target generator rather than the real data
distribution of the target model (see Figure 5 for an example). There-
fore, it motivates us to perform accuracy extraction to improve the
accuracy of attack models.
Accuracy Extraction Attack. Adversaries mounting accuracy ex-
traction concentrate on accuracy and they target at stealing the
distribution of the training set of a target model. In order to achieve
a high accuracy model extraction attack, we propose to utilize sub-
sampling techniques where generated samples far away from the
true distribution are rejected and only samples that are closer to the
true distribution are retained (see Figure 5). To achieve this goal, we
assume that adversaries can obtain more background knowledge.
In particular, we assume adversaries can obtain the discriminator
from the target GAN model and partial real data. We utilize the dis-
criminator to subsample generated samples. These refined samples
are more close to real data distribution, compared to samples are
directly generated by the target model (see Figure 5(e)). Then, we
use these refined samples and partial real data to train our attack
model. Extensive experimental evaluations show that our accuracy
extraction attack indeed brings improvement of the accuracy of
attack models, compared to fidelity extraction attacks (Figure 6).
This indicate that the risks of partially releasing training data can
be further exacerbated under this type of attack.
Case Study. We perform one case study to further demonstrate the
impact of model extraction attacks on a large-scale scenario. In this
case study — model extraction based transfer learning (Section 7),
we show that stealing a state-of-the-art GAN model can enable
adversaries to enhance the performance of their own GAN model
by transfer learning. Specifically, for the target model StyleGAN
trained on the 3 million bedroom images [71], the adversary first
launches fidelity extraction attack, and the attack performance with
4.12 FID on fidelity and 6.97 FID on accuracy can be achieved under
50K queries. Furthermore, the adversary transfers the extracted
knowledge to new domains, and experimental evaluations show
that compared with training from scratch on LSUN-Classroom
dataset with 20.34 FID [31], model extraction based transfer learning
achieves 16.47 FID, which is the state-of-the-art performance on
the LSUN-Classroom dataset.
Defenses. Both fidelity extraction and accuracy extraction attacks
on GANs compromise the intellectual property of model providers.
In particular, accuracy extraction aiming to steal the distribution of
the training set of a target model can further severely breach the
privacy of the training set. Therefore, we propose possible defense
techniques by considering two aspects: fidelity and accuracy (Sec-
tion 8). In terms of fidelity of model extraction, limiting the number
of queries is an effective method. In terms of accuracy of model
extraction, we believe that a high accuracy attack model requires
adversaries to have access to generated data which can be much
closer to real data distribution. The performance of model extrac-
tion attacks will be attenuated if adversaries only obtain a partial
or distorted distribution of generated data. Thus, we propose two
types of perturbation-based defense strategies: input and output
perturbation-based approaches, to reveal less distribution informa-
tion by increasing the similarity of samples or lowering the quality
of samples [2]. The input perturbation-based approaches include
linear and semantic interpolation perturbation while the output
perturbation-based approaches include random noise, adversarial
example noise, filtering and compression perturbation. Extensive
experimental evaluations show that, compared to queries from the
prior distribution of the target model, the equal amount of queries
by perturbation-based defenses can effectively degrade the accuracy
of attack models (Figure 8).
Summary of Contributions. Our contributions in the current
work are threefold:
(1) we conduct the first systematic study of model extraction
attacks against GANs and devise fidelity extraction attacks
and accuracy extraction attacks for GANs;
(2) we preform one case study to illustrate the impact of model
extraction attacks against GANs on a large-scale scenario;
(3) we propose new effective defense measures to mitigate
model extraction attacks against GANs.
Organization. The rest of the paper is organized as following. The
next section 2 reviews related work. Section 3 introduces the pre-
liminary knowledge, and Section 4 taxonomizes the space of model
extraction attacks on GANs. Section 5 and Section 6 introduce the
fidelity extraction and accuracy extraction, respectively. Section 7
presents one case study. In Section 8, we discuss possible defense
mechanisms. Section 9 concludes this paper.
2 RELATED WORK
Generative Adversarial Networks (GANs). GANs have
achieved impressive performance in a variety of areas, such
as image synthesis [6, 31–33, 39, 47, 53, 57], image-to-image
translation [40, 52, 73], and texture generation [37, 69], since a
2Stealing Machine Learning Models: Attacks and Countermeasures for Generative Adversarial Networks
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
framework of GAN was first proposed by Goodfellow et al. in
2014 [19]. For image synthesis tasks, the current state-of-the-art
GANs [6, 31, 32, 47] are able to generate highly realistic and
diverse images. For instance, SNGAN [47] generates realistic
images by a spectral normalization method to stabilize the training
process. PGGAN [31] proposed by Karras et al. is the first GAN that
successfully generates real-like face images at a high resolution
of 1024 × 1024, applying a progressive training strategy. Unlike
the PGGAN training in an unsupervised method, BigGAN [6]
proposed by Brock et al. aims to generate high-quality images
from a multi-class dataset by conditional GANs which leverage
information about class labels. Recently, StyleGAN [32] has further
improved the performance of GANs on high-resolution images
through adding neural style transfer [25]. In this paper, we choose
SNGAN and PGGAN as the target models to be attacked by model
extraction, considering their impressive performance on image
generation. StyleGAN is also used as a target model in a case study
in Section 7.
Model Extraction Attacks. With the availability of machine learn-
ing as a service (MLaaS), model extraction attack has received much
attention from the research community [9, 14, 27, 34, 64], which
aims to duplicate (i.e., ‘steal’) a machine learning model. This type
of attack can be categorized into two classes: accuracy model ex-
traction and fidelity model extraction. In terms of accuracy model
extraction, it was first proposed by Tramèr et al. [64], where the ob-
jective of the attack is to gain similar or even better performance on
the test dataset for the extracted model. Since then, various methods
attempting to reduce the number of queries have been developed
for further improving the attack efficiency, such as model extraction
using active learning [12, 50] or semi-supervised learning [27]. In
terms of fidelity model extraction, it requires the attack model to
faithfully reproduce predictions of the target model, including the
errors which occur in the target model. Typical works include model
reconstruction from model explanation [45], functionally equiva-
lent extraction [27] and cryptanalytic extraction [9]. In addition to
model extraction attacks on images, there are several work about
model extraction in natural language processing [34, 61]. Krishna et
al. [61] mount model extraction attacks against BERT-based models
and the performance of the extracted model is slightly worse than
that of the target model. Overall, these studies mainly focus on
discriminative models, such as regression and convolutional neural
networks for classification, and recurrent neural networks for nat-
ural language processing. Unlike the existing studies, our work aims
to study model extraction attacks against GANs.
In addition to model extraction attacks, there are other types of
attacks in relation to privacy and security [10, 22, 68, 70], such
as membership inference attacks [13, 20, 56, 59, 60] and prop-
erty inference attacks [18]. Some efforts have been also made to
investigate membership inference attacks against GANs, where
queries to a GAN model can reveal information about the training
dataset [13, 20, 24]. Overall, these studies mainly focus on privacy
on the training dataset, while model extraction attacks in our paper
concentrate on machine learning model itself.
Model Extraction Defenses. Defense for model extraction can be