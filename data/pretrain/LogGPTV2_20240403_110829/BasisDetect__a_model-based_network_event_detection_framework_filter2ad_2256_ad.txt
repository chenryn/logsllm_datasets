cision to use discrete Meyer wavelets in the signal dictionary
was due to its performance against other wavelet types on
the GEANT dataset. The results can be seen in Table 2,
where in order to detect all the anomalies in the GEANT
dataset, using the discrete Meyer wavelet results in almost
1,500 fewer false alarms declared against the next compet-
ing wavelet type (Haar wavelets).
Intuitively, the Meyer
wavelet represents localized sinusoidal behavior, which can
commonly be found in non-anomalous network data. Mean-
while, Haar wavelets represent sharp signal discontinuities
and Daubechies wavelets represent signal polynomial struc-
ture, neither of which should be expected to represent non-
anomalous signal behavior well. The performance of the
Meyer wavelet transform here motivates its use throughout
the remainder of the experiments.
8.3 Tuning Parameter Performance Analysis
The BasisDetect framework uses a series of tuning pa-
rameters (γ, ρ) to optimize performance. In order to assess
improvements in BasisDetect’s false alarm rate due to these
parameters, experiments were run on the GEANT dataset
setting each of the tuning parameters to zero and observing
performance. The results can be seen in Figure 5. As seen in
the ﬁgure, the full BasisDetect methodology including both
tuning parameters optimized by the training set signiﬁcantly
outperforms both parameter eliminated methods with re-
spect to the number of false alarms declared. In the case of
the eliminated anomaly signal component penalty (γ = 0)
we revert to a standard greedy basis pursuit methodology,
with this modiﬁed methodology consistently outperformed
by the full BasisDetect methodology for the detection of ev-
ery anomaly in the GEANT dataset. This indicates that
our penalized methodology of BasisDetect oﬀers a clear per-
formance advantage over standard Basis Pursuit method-
ologies.
In the case where the residual signal is ignored
(ρ = 0), the results indicate better performance than the
non-penalized BasisDetect method for a majority of the de-
tected anomalies (while still worse than the full BasisDetect
methodology), the method then fails at detecting the last
fraction of anomalies in terms of the large number of false
alarms declared to ﬁnd that last fraction of anomalies. This
represents a regime where the basis pursuit methodology is
failing at ﬁtting the anomalies to our estimated anomalous
signal dictionary, likely due to limited training data for our
learning-based methodology. These results motivate the use
of both tuning parameters in our full BasisDetect frame-
work.
8.4 Synthesized Network-wide Data
Using the synthetic network-wide traﬃc matrix tech-
nique described Section 3.1, we generate network-wide data
with injected anomalies. To compare performance of our
BasisDetect methodology, we will use two state-of-the-
art network-wide anomaly detection techniques. The ﬁrst
methodology is the Principle Component Analysis (PCA)
technique from [1], and the second will be the Distributed
Spatial anomaly detection technique from [2]. In order to
test the detection capabilities of all methods, the injected
anomalies vary in both amplitude and length, while the level
of added noise in the network data varies between network
snapshots. Our detection methodologies will be tested to de-
tect both the beginning and ending of each injected anomaly.
For the initial synthetic experiment, we evaluate perfor-
mance of the three network-wide anomaly detection method-
ologies using data with constant injected anomaly ampli-
tudes, aanomaly across all network snapshots. Using 20
synthesized network snapshots each with a single injected
anomaly, we modify both the length of the anomaly injected
(between 2 and 8 time bins) and the size of the network (be-
tween 3 and 5 fully connected routers, relating to 9 and 25
observed links respectively). Each network snapshot simu-
lates 2.5 days of packet out link observations aggregated into
5 minute time bins, resulting in an observed 1024-length
4602
1.8
1.6
1.4
1.2
1
s
t
n
u
o
C
t
e
k
c
a
P
d
e
z
i
l
a
m
r
o
N
0.8
800
850
900
Time Bin
950
1000
2
1.8
1.6
1.4
1.2
1
s
t
n
u
o
C
t
e
k
c
a
P
d
e
z
i
l
a
m
r
o
N
0.8
800
850
900
Time Bin
950
1000
2
1.8
1.6
1.4
1.2
1
s
t
n
u
o
C
t
e
k
c
a
P
d
e
z
i
l
a
m
r
o
N
0.8
800
850
900
Time Bin
950
1000
Figure 6: Three examples of injected anomalies with varying anomaly amplitudes. (Left) - aanomaly = 0.063,
(Center) - aanomaly = 0.1, (Right) - aanomaly = 0.158
l
s
m
r
a
A
e
s
a
F
d
e
l
i
f
i
s
s
a
C
l
f
o
r
e
b
m
u
N
14000
12000
10000
8000
6000
4000
2000
0
0
12000
14000
BasisDetect
Spatial
PCA
l
s
m
r
a
A
e
s
a
F
d
e
l
i
f
i
s
s
a
C
l
f
o
r
e
b
m
u
N
5
10
15
20
25
30
35
Number of Detected Anomalies
10000
8000
6000
4000
2000
0
0
BasisDetect
Spatial
PCA
l
s
m
r
a
A
e
s
a
F
d
e
l
i
f
i
s
s
a
C
l
f
o
r
e
b
m
u
N
5
10
15
20
25
30
35
Number of Detected Anomalies
12000
10000
8000
6000
4000
2000
0
0
BasisDetect
Spatial
PCA
5
10
15
20
25
30
35
Number of Detected Anomalies
Figure 7: Synthetic Traﬃc Matrices with Constant Anomaly Amplitude - False Alarms declared for a speciﬁed
level of true anomaly detection for the three network-wide detection methodologies (PCA, Distributed Spatial,
BasisDetect). (Left) - aanomaly = 0.063, (Center) - aanomaly = 0.1, (Right) - aanomaly = 0.158.
Table 3: Synthetic Traﬃc Matrices with Constant Anomaly Amplitude - Number of false alarms declared for
a given number of the true anomalies detected.
Anomaly amplitude (aanomaly)
Number of true anomalies found
PCA
Spatial
BasisDetect
8
11
21
27
16
298
216
473
0.063
24
2,179
1,594
653
32
12,579
7,932
7,041
8
9
2
0
16
239
10
2
0.1
24
974
55
18
32
11,812
5,690
2,587
8
3
0
0
16
110
1
2
0.158
24
1,202
24
15
32
12,598
3,545
272
time-series vector at each synthetic link. Using hold-out
cross validation, we use 4 of the network snapshots to train
our BasisDetect methodology and then test detection per-
formance across the remaining 16 networks. We consider
three anomaly amplitude regimes, (low - aanomaly = 0.063,
medium - aanomaly = 0.1, and high - aanomaly = 0.158), with
examples of these injected anomaly regimes for the observed
network data shown in Figure 6.
In the experiment results in Figure 7, we see that for all
three amplitude regimes our BasisDetect methodology out-
performs the current state-of-the-art detection techniques
in terms of the number of false alarms anomalies declared
across all 16 test networks. In Table 3 we ﬁnd a breakdown
of the false alarms declared for various detection levels. As
seen in the table, the BasisDetect methodology performs
signiﬁcantly better than both the PCA and Distributed
Spatial methodology.
In terms of the medium amplitude
anomaly (aanomaly = 1), BasisDetect ﬁnds all anomalies
with over 54% fewer false alarms than Distributed Spatial
and over 75% fewer false alarms than the PCA methodol-
ogy.