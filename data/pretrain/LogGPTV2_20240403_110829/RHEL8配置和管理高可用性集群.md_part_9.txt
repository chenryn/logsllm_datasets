# []{#assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters.html#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-nfs}在 Pacemaker 集群中使用 ext4 文件系统配置 LVM 卷 {.title}
:::
这个过程在集群节点之间共享的存储中创建 LVM 逻辑卷。
::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
### 注意 {.title}
LVM 卷以及集群节点使用的对应分区和设备必须只能连接到集群节点。
:::
下面的过程创建了 LVM 逻辑卷，然后在该卷上创建一个 ext4 文件系统供
Pacemaker 集群使用。在这个示例中，使用共享分区 `/dev/sdb1`{.literal}
来存储从中创建 LVM 逻辑卷的 LVM 物理卷。
::: orderedlist
**流程**
1.  在集群的两个节点上，执行以下步骤将 LVM 系统 ID 的值设置为系统的
    `uname`{.literal} 标识符值。LVM 系统 ID
    将用于确保只有集群可以激活卷组。
    ::: orderedlist
    1.  将 `/etc/lvm/lvm.conf`{.literal} 配置文件中的
        `system_id_source`{.literal} 配置选项设置为 `uname`{.literal}。
        ``` literallayout
        # Configuration option global/system_id_source.
        system_id_source = "uname"
        ```
    2.  验证节点上的 LVM 系统 ID 是否与节点的 `uname`{.literal} 匹配。
        ``` literallayout
        # lvm systemid
          system ID: z1.example.com
        # uname -n
          z1.example.com
        ```
    :::
2.  创建 LVM 卷并在那个卷中创建 ext4 文件系统。由于
    `/dev/sdb1`{.literal}
    分区是共享的存储，因此您仅在一个节点上执行这一部分的步骤。
    ::: orderedlist
    1.  在分区 `/dev/sdb1`{.literal} 上创建一个 LVM 物理卷。
        ``` literallayout
        # pvcreate /dev/sdb1
          Physical volume "/dev/sdb1" successfully created
        ```
    2.  创建由物理卷 `/dev/sdb1`{.literal} 组成的卷组
        `my_vg`{.literal}。
        对于 RHEL 8.5 及之后的版本，指定
        `--setautoactivation n`{.literal} 标志来确保集群中由 Pacemaker
        管理的卷组在启动时不会自动激活。如果您要为要创建的 LVM
        卷使用现有卷组，您可以使用
        `vgchange --setautoactivation n`{.literal}
        命令为卷组重置此标记。
        ``` literallayout
        # vgcreate --setautoactivation n my_vg /dev/sdb1
          Volume group "my_vg" successfully created
        ```
        对于 RHEL 8.4 及更早版本，使用以下命令创建卷组：
        ``` literallayout
        # vgcreate my_vg /dev/sdb1
          Volume group "my_vg" successfully created
        ```
        有关确保集群中 Pacemaker 管理的卷组在 RHEL 8.4
        [及更早版本启动时不会自动激活，请参阅确保不会在多个群集节点上激活卷组](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_ensuring-cluster-volume-not-multiply-activated-configuring-ha-http){.link}。
    3.  确认新卷组带有您要运行的节点的系统 ID，并从这个节点中创建卷组。
        ``` literallayout
        # vgs -o+systemid
          VG    #PV #LV #SN Attr   VSize  VFree  System ID
          my_vg   1   0   0 wz--n- 
# []{#assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters.html#proc_ensuring-cluster-volume-not-multiply-activated-configuring-ha-nfs}确保没有在多个集群节点中激活卷组（RHEL 8.4 及更早版本） {.title}
:::
此流程确保启动时不会自动激活集群中由 Pacemaker
管理的卷组。如果某个卷组在启动时自动激活，而不是由 Pacemaker
激活，则卷组可能会同时在多个节点上激活，这可能会破坏卷组的元数据。
::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
### 注意 {.title}
对于 RHEL 8.5 及更高版本，您可以在创建卷组时为卷组禁用自动激活，方法是为
`vgcreate`{.literal} 命令指定 `--setautoactivation n`{.literal} 标志，如
[Pacemaker 集群中使用 ext4 文件系统配置 LVM
卷](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-http){.link}
所述。
:::
此流程修改 `/etc/lvm/lvm.conf`{.literal} 配置文件中的
`auto_activation_volume_list`{.literal}
条目。`auto_activation_volume_list`{.literal}
条目用于将自动激活限制为特定的逻辑卷。将
`auto_activation_volume_list`{.literal} 设置为空列表可完全禁用自动激活。
任何未被共享且不由 Pacemaker 管理的本地卷都应包含在
`auto_activation_volume_list`{.literal}
条目中，包括与节点本地根和主目录相关的卷组。由群集管理器管理的所有卷组都必须从
`auto_activation_volume_list`{.literal} 条目中排除。
::: title
**流程**
:::
在集群的每个节点中执行以下步骤。
::: orderedlist
1.  使用以下命令，确定您的本地存储上当前已配置哪些卷组。这将输出当前配置的卷组的列表。如果您在单独的卷组中为
    root
    和此节点上的主目录分配了空间，您会在输出中看到这些卷，如下例所示。
    ``` literallayout
    # vgs --noheadings -o vg_name
      my_vg
      rhel_home
      rhel_root
    ```
2.  将 `my_vg`{.literal}
    之外的卷组（您刚刚为群集定义的卷组）作为条目添加到
    `/etc/lvm/lvm.conf`{.literal} 配置文件中的
    `auto_activation_volume_list`{.literal}。
    例如，如果您在单独的卷组中为 root 和主目录分配了空间，您可以取消注释
    `lvm.conf`{.literal} 文件的 `auto_activation_volume_list`{.literal}
    行，并将这些卷组作为条目添加到
    `auto_activation_volume_list`{.literal}，如下所示：请注意，您刚才为群集定义的卷组（`本例中为 my_vg`{.literal}
    ）不在此列表中。
    ``` literallayout
    auto_activation_volume_list = [ "rhel_root", "rhel_home" ]
    ```
    ::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
    ### 注意 {.title}
    如果节点上没有要在群集管理器外激活的本地卷组，您仍需要将
    `auto_activation_volume_list`{.literal} 条目初始化为
    `auto_activation_volume_list = []`{.literal}。
    :::
3.  重建 `initramfs`{.literal}
    引导映像，以确保引导映像不会尝试激活由群集控制的卷组。使用以下命令更新
    `initramfs`{.literal} 设备：此命令最多可能需要一分钟完成。
    ``` literallayout
    # dracut -H -f /boot/initramfs-$(uname -r).img $(uname -r)
    ```
4.  重新引导节点。
    ::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
    ### 注意 {.title}
    如果您自引导引导镜像后安装了一个新的 Linux 内核，则新
    `initrd`{.literal}
    镜像将适用于您在创建引导镜像时运行的内核，而不是重新引导该节点时运行的新内核。您可以通过在重启前后运行
    `uname -r`{.literal} 命令来确保使用正确的 `initrd`{.literal}
    设备，以确定正在运行的内核版本。如果发行版不同，请在使用新内核重启后更新
    `initrd`{.literal} 文件，然后重新引导节点。
    :::
5.  节点重新引导后，通过在该节点上执行 `pcs cluster status`{.literal}
    命令，检查群集服务是否已在该节点上再次启动。如果这会产生
    `Error: cluster 当前没有在此节点上运行的信息`{.literal}，请输入以下命令。
    ``` literallayout
    # pcs cluster start
    ```
    另外，您可以等待直到您重新引导集群中的每个节点，并使用以下命令在集群中的所有节点上启动集群服务。
    ``` literallayout
    # pcs cluster start --all
    ```
:::
:::
::: section
::: titlepage
# []{#assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters.html#proc_configuring-nfs-share-configuring-ha-nfs}配置一个 NFS 共享 {.title}
:::
这个过程为 NFS 服务故障转移配置 NFS 共享。
::: orderedlist
**流程**
1.  在集群的两个节点上，创建 `/nfsshare`{.literal} 目录。
    ``` literallayout
    # mkdir /nfsshare
    ```
2.  在集群的一个节点上执行以下步骤。
    ::: orderedlist
    1.  确定您在使用 [ext4 文件系统配置 LVM
        卷时创建的逻辑卷已被](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-lvm-volume-with-ext4-file-system-configuring-ha-nfs){.link}
        激活，然后在 `/nfsshare`{.literal}
        目录的逻辑卷中挂载您创建的文件系统。
        ``` literallayout
        [root@z1 ~]# lvchange -ay my_vg/my_lv
        [root@z1 ~]# mount /dev/my_vg/my_lv /nfsshare
        ```
    2.  在 `/nfsshare`{.literal} 目录上创建 `导出`{.literal} 目录树。
        ``` literallayout
        [root@z1 ~]# mkdir -p /nfsshare/exports
        [root@z1 ~]# mkdir -p /nfsshare/exports/export1
        [root@z1 ~]# mkdir -p /nfsshare/exports/export2
        ```
    3.  将文件放在 `导出`{.literal} 目录中，供 NFS
        客户端访问。在本例中，我们创建名为
        `clientdatafile1 和 clientdatafile`{.literal} 2``{=html}
        的测试文件。
        ``` literallayout
        [root@z1 ~]# touch /nfsshare/exports/export1/clientdatafile1
        [root@z1 ~]# touch /nfsshare/exports/export2/clientdatafile2
        ```
    4.  卸载 ext4 文件系统，并取消激活 LVM 卷组。
        ``` literallayout
        [root@z1 ~]# umount /dev/my_vg/my_lv
        [root@z1 ~]# vgchange -an my_vg
        ```
    :::
:::
:::
::: section
::: titlepage
# []{#assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters.html#proc_configuring_resources_for_nfs_server_in_a_cluster-configuring-ha-nfs}为集群中的 NFS 服务器配置资源和资源组 {.title}
:::
此流程为集群中的 NFS 服务器配置集群资源。
::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
### 注意 {.title}
如果您还没有为集群配置隔离设备，默认情况下资源不会启动。
如果您发现您配置的资源没有运行，您可以运行
`pcs resource debug-start 资源`{.literal}
命令来测试资源配置。这会在集群控制之外启动服务。在配置的资源再次运行时，运行
`pcs resource cleanup 资源`{.literal} 以使集群了解这些更新。
:::
::: title
**流程**
:::
以下步骤配置系统资源。为确保这些资源在同一节点上运行，它们已配置为资源组
`nfsgroup`{.literal}
的一部分。资源将以您添加到组的顺序启动，并按照添加到组中的相反顺序停止。仅从集群的一个节点运行此步骤。
::: orderedlist
1.  创建名为 `my_lvm`{.literal} 的 LVM 激活资源。由于
    `nfsgroup`{.literal} 资源组尚不存在，这个命令会创建资源组。