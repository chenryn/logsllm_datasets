Under most conditions, its tail latency is considerably worse
than Homa, but for larger messages in W1 and W2 PIAS pro-
vides better latency than Homa. PIAS is nearly identical to
Homa for small messages in workload W3. PIAS always uses
the highest priority level for messages that fit in a single packet,
and this happens to match Homa‚Äôs priority allocation for W3.
PIAS uses a multi-level feedback queue policy, where each
message starts at high priority; the priority drops as the message
is transmitted and PIAS learns more about its length. This policy
is inferior to Homa‚Äôs receiver-driven SRPT not only for small
messages but also for most long ones. Small messages suffer
because they get queued behind the high-priority prefixes of
longer messages. Long messages suffer because their priority
drops as they get closer to completion; this makes it hard to
finish them. As a result, PIAS‚Äô slowdown jumps significantly
for messages greater than one packet in length. In addition, with-
out receiver-based scheduling, congestion led to ECN-induced
backoff in workload W4, resulting in slowdowns of 20 or more
for multi-packet messages. Homa uses the opposite approach
from PIAS: the priority of a long message starts off low, but
rises as the message gets closer to finishing; eventually the mes-
sage runs to completion. In addition, Homa‚Äôs rate-limiting and
priority mechanisms work well together; for example, the rate
limiter eliminates buffer overflow as a major consideration.
NDP. The NDP simulator [15] could not simulate partial pack-
ets, so we measured NDP only with W5, in which all packets
are full-size; Figure 10 shows the results. NDP‚Äôs performance
is considerably worse than any of the other protocols, for two
reasons. First, it uses a rate control mechanism with no over-
commitment, which wastes bandwidth: at 70% network load,
27% of receiver bandwidth was wasted (the receiver had in-
complete incoming messages yet its downlink was idle). We
could not run simulations above 73% network load. The wasted
downlink bandwidth results in additional queuing delays at high
network load. Second, NDP does not use SRPT; its receivers use
a fair-share scheduling policy, which results in a uniformly high
slowdown for all messages longer than RTTbytes. In addition,
NDP senders do not prioritize their transmit queues; this results
in severe head-of-line blocking for small messages when the
transmit queue builds up during bursts. The NDP comparison
demonstrates the importance of overcommitment and SRPT.
Causes of remaining delay. We instrumented the Homa simu-
lator to identify the causes of tail latency (‚Äúwhy is the slowdown
at the 99th percentile greater than 1.0?‚Äù) Figure 11 shows that
tail latency is almost entirely due to link-level preemption lag,
where a packet from a short message arrives at a link while it is
Figure 10: 99th-percentile slowdown as a function of message size,
for different protocols and workloads. Distance on the x-axis is
linear in total number of messages (each tick corresponds to 10%
of all messages). All measurements except NDP and pHost used
a netwwork load of 80%. NDP and pHost cannot support 80%
network load for these workloads, so we used the highest load that
each protocol could support (70% for NDP, 58‚Äì73% for pHost,
depending on workload). The minimum one-way time for a small
message (slowdown is 1.0) is 2.3 ¬µs. NDP was measured only for W5
because its simulator cannot handle partial packets.
1234510102351128851672915081612999% Slowdown (Log Scale)pHostPIASpFabricHomaWorkload: W11234510103345817126932036642751226214499% Slowdown (Log Scale)pHostPIASpFabricHomaWorkload: W212345101036771101582683134025731755511469599% Slowdown (Log Scale)pHostPIASpFabricHomaWorkload: W31234510103153765025616629606387494081203731e+0799% Slowdown (Log Scale)pHostPIASpFabricHomaWorkload: W412345102030721021630288405047070658269654105842822105861153744228840000Message Size (Bytes)99% Slowdown (Log Scale)NDPpHostPIASpFabricHomaWorkload: W5SIGCOMM ‚Äô18, August 20-25, 2018, Budapest, Hungary
B. Montazeri et al.
Figure 11: Sources of tail delay for short messages. ‚ÄúPreemption Lag‚Äù
occurs when a higher priority packet must wait for a lower priority
packet to finish transmission on a link. ‚ÄúQueuing Delay‚Äù occurs
when a packet waits for one or more packets of equal or higher
priority. Each bar represents an average across short messages
with delay near the 99th percentile. For workloads W1-W4 the bar
considers the smallest 20% of all messages; for W5 it considers all
single packet messages.
Figure 13: Wasted bandwidth as a function of network load for the
W4 workload. Each curve uses a different number of scheduled
priorities, which corresponds to the level of overcommitment. Each
ùë¶-value is the average fraction of time across all receivers that a
receiver‚Äôs link is idle, yet the receiver withheld grants (because of
overcommitment limits) that might have caused the bandwidth to
be used. The diagonal line represents surplus network bandwidth
(100% - network load). Wasted bandwidth cannot ever exceed
surplus bandwidth, so the point where each curve intersects the
diagonal line indicates the maximum sustainable network load.
Figure 12: Network utilization limits. The top of each bar indicates
the highest percent of available network bandwidth that the given
protocol can support for the given workload. It counts all bytes in
goodput packets, including application data, packet headers, and
control packets; it excludes retransmitted packets. The bottom part
of each bar indicates the percent of network bandwidth used for
application data at that load.
busy transmitting a packet from a longer message. This shows
that Homa is nearly optimal: the only way to improve tail latency
significantly is with changes to the networking hardware, such
as implementing link-level packet preemption.
Bandwidth utilization. To measure each protocol‚Äôs ability to
use network bandwidth efficiently, we simulated each workload-
protocol combination at higher and higher network loads to iden-
tify the highest load the protocol can support (the load generator
runs open-loop, so if the offered load exceeds the protocol‚Äôs ca-
pacity, queues grow without bound). Figure 12 shows that Homa
can operate at higher network loads than either pFabric, pHost,
NDP, or PIAS, and its capacity is more stable across workloads.
None of the protocols can achieve 100% bandwidth because
each of them wastes network bandwidth under some conditions.
Homa wastes bandwidth because it has a limited number of
scheduled priority levels: there can be times when (a) all of
the scheduled priority levels are allocated, (b) none of those
senders is responding, so the receiver‚Äôs downlink is idle and
(c) there are additional messages for which the receiver could
send grants if it had more priority levels. Figure 13 shows that
this wasted bandwidth increases with the overall network load;
eventually it consumes all of the surplus network bandwidth.
Figure 13 also shows the importance of overcommitment: if
Queue
TOR‚ÜíAggr mean
max
Aggr‚ÜíTOR mean
max
TOR‚Üíhost mean
max
W1 W2 W3 W4 W5
1.7
0.7
21.1
93.6
1.6
0.8
78.1
22.4
17.3
1.7
58.7
126.4
1.7
82.7
1.7
92.2
17.3
146.1
1.0
30.0
1.1
34.1
5.5
93.0
1.6
50.3
1.8
57.1
12.8
117.9
Table 1: Average and maximum queue lengths (in Kbytes) at
switch egress ports for each of the three levels of the network,
measured at 80% network load. Queue lengths do not include
partially-transmitted or partially-received packets.
receivers grant to only one message at a time, Homa can only
support a network load of about 63% for workload W4, versus
89% with an overcommitment level of 7.
The other protocols also waste bandwidth. pFabric wastes
bandwidth because it drops packets to signal congestion; those
packets must be retransmitted later. NDP and pHost both waste
bandwidth because they do not overcommit their downlinks. For
example, in pHost, if a sender becomes nonresponsive, band-
width on the receiver‚Äôs downlink is wasted until the receiver
times out and switches to a different sender. Figure 12 sug-
gests that Homa‚Äôs overcommitment mechanism uses network
bandwidth more efficiently than any of the other protocols.
Queue lengths. Some queuing of packets in switches is in-
evitable in Homa because of its use of unscheduled packets
and overcommitment. Even so, Table 1 shows that Homa is
successful at limiting packet buffering: average queue lengths
at 80% load are only 1‚Äì17 Kbytes, and the maximum observed
queue length was 146 Kbytes (in a TOR‚Üíhost downlink). Of
the maximum, overcommitment accounts for as much as 56
Kbytes (RTTbytes in each of 6 scheduled priority levels); the
remainder is from collisions of unscheduled packets. Workloads
with shorter messages consume less buffer space than those with
longer messages. For example, the W1 workload uses only one
scheduled priority level, so it cannot overcommit; in addition,
0123W1W2W3W4W5WorkloadDelay (us)DelayTypeQueuingDelayPreemptionLag609252754371578372916985455869838090758460657884798980876973798579878186697377816773HomapFabricpHostpiasHomapFabricpHostpiasHomapFabricpHostpiasHomapFabricpHostpiasHomapFabricpHostpiasNDP0255075100W1W2W3W4W5WorkloadMaximum Bandwidth Utilization (%)Application Total 02550751000255075100Network Load (%)Receiver's Wasted Bandwidth(%)1 Sched. Prio.2 Sched. Prio.3 Sched. Prio.4 Sched. Prio.5 Sched. Prio.7 Sched. Prio.Surplus BW.Homa: A Receiver-Driven Low-Latency Transport Protocol SIGCOMM ‚Äô18, August 20-25, 2018, Budapest, Hungary
‚àô Measurements of median slowdown for both the imple-
mentation and the simulations (vs. 99th percentile in Fig-
ures 8 and 10), and simulation measurements at 50% net-
work load (vs. 80% load in Figure 10). Homa performed
well in all these measurements, though its advantages
over the other protocols were smaller with lower network
loads and at the median.
‚àô Measurements in which we varied the number of unsched-
uled priority levels, the cutoff points between unsched-
uled priority levels, the division of priorities between
scheduled and unscheduled packets, and the number of
unscheduled bytes. In each case, the best hand-chosen
value was the same as the value chosen automatically by
Homa. Among other things, the measurements showed
that workloads with small messages need multiple prior-
ity levels for unscheduled packets (tail slowdown in W1 is
2.5x higher with only a single unscheduled priority level).
6 LIMITATIONS
This section summarizes the most important assumptions Homa
makes about its operating environment. If these assumptions
are not met, then Homa may not achieve the performance levels
reported here.
Homa is designed for use in datacenter networks and capital-
izes on the properties of those networks; it is unlikely to work
well in wide-area networks.
Homa assumes that congestion occurs primarily at host down-
links, not in the core of the network. Homa assumes per-packet
spraying to ensure load balancing across core links, combined
with sufficient overall capacity. Oversubscription is still possi-
ble, as long as there is enough aggregate bandwidth to avoid
significant congestion. We hypothesize that congestion in the
core of datacenter networks will be uncommon because it will
not be cost-effective. If the core is congested, it will result in
underutilization of servers, and the cost of this underutilization
will likely exceed the cost of provisioning more core bandwidth.
If the core does become congested, then Homa latencies will de-
grade. Homa‚Äôs mechanisms for limiting buffer occupancy may
reduce the impact of congestion in comparison to TCP-like pro-
tocols, but we leave a full exploration of this topic to future work.
Homa also assumes a single implementation of the protocol
for each host-TOR link, such as in an operating system kernel
running on bare hardware, so that Homa is aware of all incoming
and outgoing traffic. If multiple independent Homa implemen-
tations share a single host-TOR link, they may make conflicting
decisions. For example, each Homa implementation will inde-
pendently overcommit the downlink and assign priorities based
on the input traffic passing through that implementation. Multi-
ple implementations can occur when a virtualized NIC is shared
between multiple guest operating systems in a virtual machine
environment, or between multiple applications that implement
the protocol at user level. Obtaining good performance in these
environments may require sharing state between the Homa im-
plementations, perhaps by moving part of the protocol to the
Figure 14: Usage of priority levels for workload W3 under different
loads. Each bar indicates the number of network bytes transmitted
at a given priority level, as a fraction of total available network
bandwidth. P0-P3 are used for scheduled packets and P4-P7 for
unscheduled packets.
its messages are shorter, so more of them must collide simul-
taneously in order to build up long queues at the TOR. The
146-Kbyte peak occupancy is well within the capacity of typical
switches, so the data confirms our assumption that packet drops
due to buffer overflows will be rare.
Table 1 also validates our assumption that there will not
be significant congestion in the core. The TOR‚ÜíAggr and
Aggr‚ÜíTOR queues contain less than 2 Kbytes of data on aver-
age, and their maximum length is less than 100 Kbytes.
Priority utilization. Figure 14 shows how network traffic is di-
vided among the priority levels when executing workload W3 at
three different network loads. For this workload Homa splits the
priorities evenly between scheduled and unscheduled packets.
The four unscheduled priorities are used evenly, with the same
number of network bytes transmitted under each priority level.
As the network load increases, the additional traffic is also split
evenly across the unscheduled priority levels.
The four scheduled priorities are used in different ways de-
pending on the network load. At 50% load, a receiver typically
has only one schedulable message at a time, in which case the
message uses the lowest priority level (P0). Higher priority lev-
els are used for preemption when a shorter message appears
part-way through the reception of a longer one. It is rare for pre-
emptions to nest deeply enough to use all four scheduled levels.
As the network load increases, the usage of scheduled priori-
ties changes. By the time network load reaches 90%, receivers