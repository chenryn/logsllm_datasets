the bandwidth available to others.
4. CONTENTION MEASUREMENTS
In order to understand which resources are amenable to resource-
freeing attacks in a Xen environment, we created a local testbed
that attempts to duplicate a typical conﬁguration found in EC2 (in
particular, the m1.small instance type).
Testbed. Although Amazon does not make their precise hardware
conﬁgurations public, we can still gain some insight into the hard-
ware on which an instance is running by looking at system ﬁles and
the CPUID instruction. Based on this, we use a platform consisting
of a 4-core, 2-package 2.66 GHz Intel Xeon E5430 with 6MB of
shared L2 cache per package and 4GB of main memory. This is
representative of some of the architectures used by EC2.
We install Xen on the testbed, using the conﬁgurations shown
in Figure 1. Again, while we do not have precise knowledge of
Amazon’s setup for Xen, our conﬁguration approximates the EC2
m1.small instance.
This conﬁguration allows us to precisely control the workload
by varying scheduling policies and by ﬁxing workloads to different
cores. In addition, it enables us to obtain internal statistics from
Xen, such as traces of scheduling activities.
Figure 2 describes the workloads we use for stressing different
hardware resources. The workloads run in a virtual machine with
one VCPU. In order to understand the impact of sharing a cache,
we execute the workloads in three scenarios:
283Workload Description
CPU
Net
Diskrand
Memrand
LLC
Solving the N -queens problem for N = 14.
Lightweight web server hosting 32KB static
web pages cached in memory, 5000 requests per
second from a separate client.
Requests for randomly selected 4KB chunk in 1
GB span.
Randomly request 4B from every 64B of data
from a 64MB buffer.
LLCProbe, which sequentially re-
Execute
quests 4B from every 64B of data within an
LLC-sized buffer using cache coloring to bal-
ance access across cache sets.
Figure 2: Resource-speciﬁc workloads used to test contention.
(i) Same core time slices two VMs on a single core, which shares
all levels of processor cache.
(ii) Same package runs two VMs each pinned to a separate core
on a single package, which shares only the last-level cache.
(iii) Different package runs two VMs ﬂoating over cores on dif-
ferent packages, which do not share any cache, but do share
bandwidth to memory.
In addition, Xen uses a separate VM named Dom0 to run de-
vice drivers. In accordance with usage guides, we provision Dom0
with four VCPUs. As past work has shown this VM can cause con-
tention [36, 12], we make it execute on a different package for the
ﬁrst two conﬁgurations and allow it to use all four cores (both cores
in both packages) for the third.
Extent of Resource Contention. The goal of our experiments is to
determine the contention between workloads using different hard-
ware resources and determine whether enough contention exists to
mount an RFA. With perfect isolation, performance should remain
unchanged no matter what competing benchmarks run. However, if
the isolation is not perfect, then we may see performance degrada-
tion, and thus may be able to successfully mount an RFA.
Figure 3 provides tables showing the results, which demonstrate
that Xen is not able to completely isolate the performance of any
resource. Across all three conﬁgurations, CPU and Memrand
show the least interference, indicating that Xen does a good job
accounting for CPU usage and that the processor limits contention
for memory bandwidth.
However, for all other resources, there are competing workloads
that substantially degrade performance. The two resources suffer-
ing the worst contention are Diskrand where run time increases
455% with contending random disk access; and LLC, where run
time increases over 500% with Net and over 500% with Mem-
rand.
for Diskrand, competing disk trafﬁc causes seeks to be
much longer and hence slower. For LLC, competing workloads
either interrupt frequently ( Net) or move a lot of data through the
cache ( Memrand).
The three conﬁgurations differ mostly in the LLC results.
In
the same-core and different-package conﬁgurations, the contention
with LLC is fairly small. On the same core, the conﬂicting code
does not run concurrently, so performance is lost only after a con-
text switch. On different packages, performance losses come largely
from Dom0 , which is spread across all cores. In the same-package
conﬁguration, though, the tests execute concurrently and thus one
program may displace data while the other is running.
One pair of resources stands out as the worst case across all con-
ﬁgurations: the degradation caused by Net on LLC. This occurs for
three reasons: (i) the HTTP requests cause frequent interrupts and
Same core
CPU
Net
Diskrand
Memrand
LLC
Same package
CPU
Net
Diskrand
Memrand
LLC
Diff. package
CPU
Net
Diskrand
Memrand
LLC
CPU Net Diskrand Memrand
-
-
-
-
8
5
194
-
6
539
-
-
455
-
72
-
-
-
-
38
CPU Net Diskrand Memrand
-
-
-
-
20
-
198
-
-
448
-
-
461
17
55
-
-
-
-
566
CPU Net Diskrand Memrand
-
-
-
-
6
20
100
-
35
699
-
-
462
-
11
-
-
-
-
15
LLC
-
-
-
-
34
LLC
-
-
-
-
566
LLC
-
-
-
-
15
Figure 3: Percentage increase in workload run times indicated
in row when contending with workload indicated in column.
Percentage is computed as run time with contention over run
time on otherwise idle machine. For network, run time is the
time to serve a ﬁxed number of requests. A dash means there
was no signiﬁcant performance degradation. (Top) The VMs
are pinned to the same core. (Middle) The VMs are pinned to
different cores on the same package.
(Bottom) The VMs are
pinned to different packages.
hence frequent preemptions due to boost; (ii) in the same-core and
same-package conﬁgurations the web server itself runs frequently
and displaces cache contents; and (iii) Dom0 runs the NIC device
driver in the different-package conﬁguration. We will therefore fo-
cus our investigation of RFAs on the conﬂict between such work-
loads, and leave exploration of RFAs for other workload combina-
tions to future work.
5. RFA FOR CACHE VERSUS NETWORK
As we saw, a particularly egregious performance loss is felt by
cache-bound workloads when co-resident with a network server.
Unfortunately, co-residence of such workloads seems a likely sce-
nario in public clouds: network servers are a canonical application
(EC2 alone hosts several million websites [21]) while cache-bound
processes abound. The remainder of the paper seeks to understand
whether a greedy customer can mount an RFA to increase perfor-
mance when co-resident with one or more web servers.
Setting. We start by providing a full description of the setting
on which we focus. The beneﬁciary is a cache bound program
running alone in a VM with one VCPU. We use the LLCProbe
benchmark as stand-in for a real beneﬁciary. LLCProbe is inten-
tionally a synthetic benchmark and is designed to expose idealized
worst-case behavior. Nevertheless, Its pointer-chasing behavior is
reﬂected in real workloads [2]. We will also investigate more bal-
anced benchmarks such as SPEC CPU2006 [13], SPECjbb2005 [1]
and graph500 [2].
In addition to the beneﬁciary, there is a victim VM co-resident
on the same physical machine running the Apache web server (ver-
sion 2.2.22). It is conﬁgured to serve a mix of static and dynamic
content. The static content consists of 4, 096 32KB web pages
(enough to overlow the 6MB LLC) containing random bytes. The
dynamic content is a CGI script that can be conﬁgured to consume
varying amounts of CPU time via busy looping. This script serves
as a stand in for either an actual web server serving dynamic content
on the web, or the effects of DoS attacks that drive up CPU usage,
284such as complexity attacks [8, 9]. The script takes a parameter to
control duration of the attack, and spins until wall-clock time ad-
vances that duration. We note that this does not reﬂect the behavior
of most DoS attacks, which take a ﬁxed number of cycles, but we
use it to provide better control over the web server’s behavior. We
conﬁrmed that the behaviors exhibited also arise with CGI scripts
performing a ﬁxed number of computations.
The Apache server is conﬁgured with the mod_mem_cache mod-
ule to reduce the latency of static content and FastCGI to pre-fork a
process for CGI scripts. We also use the Multi-Processing Module
for workers, which is a hybrid multithreaded multi-process Apache
web server design used for better performance and for handling
larger request loads.
To simulate load on the web server, we use a custom-built multi-
threaded load generator that sends web requests for the static con-
tent hosted by the victim. Each client thread in the load generator
randomly selects a static web page to request from the web server.
The load generator includes a rate controller thread that ensures that
the actual load on the web server does not exceed the speciﬁed re-
quest rate. The client uses 32 worker threads, which we empirically
determined is enough to sustain the web server’s maximum rate.
Requests are synchronous and hence the load generator waits for
the response to the previous request and then a timeout (to prevent
sending requests too fast) before sending the next request. Since
each thread in the load generator waits for a response from the web
server before sending the next request, it may not meet the speciﬁed
request rate if the server or the network bandwidth cannot sustain
the load. The helper, which performs the actual RFA, is identical
to the load generator except that it sends requests for the CGI script
rather than for static pages.
Understanding the contention. We conduct experiments on our
local testbed to understand the basic performance degradation ex-
perienced by LLCProbe as the web server’s workload varies. We
report the average time to probe the cache; one probe involves ac-
cessing every cacheline out of a buffer of size equal to the LLC.
We measure the time per probe by counting the number of probes
completed in 10 seconds.
To understand contention, we ﬁrst pin the victim VM and the
beneﬁciary VM to the same core and pin Dom0 to a different pack-
age. The Fixed Core columns in Figure 4 show the runtime per
cache probe averaged over 3 runs for a range of background re-
quest rates to the web sever. The Perf. Degradation column shows
the percent increase in probe time relative to running with an idle
victim VM.
Request
Fixed Core
Floating Core
Rate
0
100
1000
1500
2000
3000
Runtime
4033
4780
6500
7740
9569
18392
Increase Runtime
4791
5362
6887
7759
8508
16630
0
19%
61%
92%
137%
356%
Increase
0
12%
44%
62%
78%
247%
Figure 4: Runtimes (in microseconds) and percentage increase
of LLCProbe (foreground) workload as a function of request
rate to victim (background). For Fixed Core both VMs are
pinned to the same core and for Floating Core Xen chooses
where to execute them.
we use Xentrace [17] to record the domain switches that occur over
a ﬁxed period of time in which the LLCProbe VM runs. We an-
alyzed the case of 1500 requests per second ( rps) and 3000 rps.
For the 3000 rps case, the web server runs for less than 1 ms in
80% of the times it is scheduled whereas in the 1500 rps case the
web server runs for less than 1 ms only 40% of the time, because