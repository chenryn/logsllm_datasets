• The operands for conditional instructions, such as CSTORE and
CEXEC, are available before, or at the stages where the sub-
sequent instructions execute; CEXEC can execute when all its
operands are available.
By allowing instructions to be executed out of order, we can dis-
tribute the single logical TCPU on an ASIC by replicating its func-
tionality at every stage. Each stage has one execution unit for every
instruction in the packet, a crossbar to connect the execution units
to all registers local to the stage and packet memory, and access
to the stage’s local memory read/write port. From the decoded in-
structions, the stage can execute all instructions local to the stage,
Match Action Stage nSRAMRegister FilePacket Headers (TPP, IP, etc.) + MetadataFrom previous stageTo next stageTCPUInstructionsPacket Mem. (at most 320b), MetadataSRAMReg.FileControllerCrossbarCrossbarCrossbarTCPUMMIO160b10Processing executed TPPs: The dataplane also processes incom-
ing packets from the network, which have fully executed.
It
echoes any standalone TPPs that have ﬁnished executing back to
the packet’s source IP address. For piggy-backed TPPs, the data-
plane checks the table mapping the application ID to its aggregator,
and sends the ﬁnished TPP to the application-speciﬁc aggregator.
4.3 Security considerations
There is a great deal of software generating network trafﬁc in any
datacenter, and most of it should not be trusted to generate arbitrary
TPPs. After all, TPPs can read and write a variety of switch state
and affect packet routing. This raises the questions of how to re-
strict software from generating TPPs, but also how to provide some
of the beneﬁts of TPPs to software that is not completely trusted.
We now discuss possible mechanisms to enforce such restrictions,
under the assumption that switches are trusted, and there is a trusted
software layer at end hosts such as a hypervisor.
Fortunately, restricting TPPs is relatively simple, because it boils
down to packet ﬁltering, which is already widely deployed. Just
as untrusted software should not be able to spoof IP addresses or
VLAN IDs, it should not able to originate TPPs. Enforcing this re-
striction is as easy as ﬁltering based on protocol and port numbers,
which can be done either at all ingress switch ports or hypervisors.
In many settings, read-only access to most switch state is harm-
less.
(A big exception is the contents of other buffered packets,
to which TPPs do not provide access anyway.) Fortunately, TPPs
are relatively amenable to static analysis, particularly since a TPP
contains at most ﬁve instructions. Hence the hypervisor could be
conﬁgured to drop any TPPs with write instructions (or write in-
structions to some subset of switch state). Alternatively, one could
imagine the hypervisor implementing higher-level services (such as
network visibility) using TPPs and expose them to untrusted soft-
ware through a restricted API.
At a high level, a compromised hypervisor sending malicious
TPPs is as bad as a compromised SDN controller. The difference is
that hypervisors are typically spread throughout the datacenter on
every machine and may present a larger attack surface than SDN
controllers. Hence, for defense in depth, the control plane needs
the ability to disable write instructions (STORE, CSTORE) entirely.
A majority of the tasks we presented required only read access to
network state.
4.4 TPP Executor
Although the default way of executing TPP is to execute at all hops
from source to destination, we have built a ‘TPP Executor’ library
that abstracts away common ways in which TPPs can be (i) exe-
cuted reliably, despite TPPs being dropped in the network, (ii) tar-
geted at one switch, without incurring a full round-trip from one
end-host to another, (iii) executed in a scatter-gather fashion across
a subset of switches, and many more.
In the interest of space,
we defer a detailed discussion to the extended version of this pa-
per [28].
5 Implementation
We have implemented both hardware and software support needed
for TCPU: the distributed TCPU on the 10Gb/s NetFPGA platform,
and a software TCPU for the Open vSwitch Linux kernel module.
The NetFPGA hardware prototype has a four-stage pipeline at each
port, with 64 kbit block RAM and 8 registers at each stage (i.e. a
total of 1Mbit RAM and 128 registers). We were able to synthesize
the hardware modules at 160 MHz, capable of switching minimum
sized (64Byte) packets at a 40Gb/s total data rate.
The end-host stack is a relatively straightforward implementa-
tion: We have implemented the TPP-CP, and the TPP executor
Figure 8: End-host stack for creating and managing TPP-enabled
applications. Arrows denote packet ﬂow paths through the stack,
and communication paths between the end-host and the network
control plane.
4.1 Control plane
The TPP control plane (TPP-CP) is a central entity to keep track of
running TPP applications and manage switch memory, and has an
agent at every end-host that keeps track of the active TPP-enabled
applications running locally. Each application is allocated a con-
tiguous set of memory addresses that it can read/write. For ex-
ample, the RCP application requires access to a switch memory
word to store the Rfair at each link, and it owns this memory ex-
clusively. This memory access control information is similar to
the x86’s global descriptor table, where each entry corresponds to
a segment start and end address, and permissions to read/write to
memory is granted accordingly.
TPP-CP exports an API which authorized applications can use to
insert TPPs on a subset of packets matching certain criteria, with a
certain sampling frequency. Note that TPP-CP will know the caller
application (e.g. ndb) so it can deny the API call if the TPP ac-
cesses memory locations other than those permitted. The API deﬁ-
nition is as follows:
add_tpp(filter, tpp_bytes, sample_frequency, priority)
where filter is a packet ﬁlter (as in iptables), and tpp_bytes
is the compiled TPP, and sample_frequency is a non-negative in-
teger that indicates the sampling frequency: if it is N, then a packet
is stamped with the TPP with probability 1/N. If N = 1, all packets
have the TPP. The dataplane stores the list of all TPPs with each
ﬁlter: This ensures that multiple applications, which want to install
TPPs on (say) 1% of all IP packets, can coexist.
TPP-CP also conﬁgures the dataplane to enforce access
control policies.
Each memory access policy is a tuple:
(appid,op,address_range). The value appid is a 64-bit num-
ber, op is either read or write, and address_range is an interval
denoting the start and end address. The TPPs are statically ana-
lyzed, to see if it accesses memories outside the permitted address
range; if so, the API call returns a failure and the TPP is never
installed.
4.2 Dataplane
The end-host dataplane is a software packet processing pipeline
that allows applications to inject TPPs into ongoing packets, pro-
cess executed TPPs from the network, and enforce access control
policies.
Interposition: The dataplane realizes the TPP-CP API add_tpp.
It matches outgoing packets against the table of ﬁlters and adds a
TPP to the ﬁrst match, or sends the packet as such if there is no
match. Only one TPP is added to any packet. The interposition
modules in the dataplane also strips incoming packets that have
completed TPPs before passing the packet to the network stack, so
the applications are oblivious to TPPs.
End-hostApp 1App 2Network Control PlaneTPP Control PlaneAgentExecutorDataplane shimRCP/TCP/IP StackRCPcan sendpiggybackedTPPspackets withpiggybackedTPPTPPs withinUDP payload withdstport=0x6666RPCsTPP Control PlaneRPCs11Task
Parsing
Memory access
Instr. Exec.: CSTORE
Instr. Exec.: (the rest)
Packet rewrite
Total per-stage
NetFPGA
< 1 cycle
1 cycle
1 cycle
< 1 cycle
< 1 cycle
2–3 cycles
ASICs
1 cycle
2–5 cycles
10 cycles
1 cycle
1 cycle
50–100 cycles†
Table 3: Summary of hardware latency costs. †The ASIC’s per-
stage cost is estimated from the total end-to-end latency (200–
500ns) and dividing it by the number of stages (typically 4–5). This
does not include packetization latency, which is another ∼50ns for
a 64Byte packet at 10Gb/s.
(with support only for the reliable and scatter-gather execution pat-
tern) as Python programs running in userspace. The software dat-
aplane is a kernel module that acts as a shim between the network
stack and the underlying network device, where it can gain access
to all network packets on the transmit and receive path. For ﬁlter-
ing packets to attach TPPs, we use iptables to classify packets
and tag them with a TPP number, and the dataplane inserts the ap-
propriate TPP by modifying the packet in place.
6 Evaluation
In §2 we have already seen how TPPs enable many dataplane ap-
plications. We now delve into targeted benchmarks of the perfor-
mance of each component in the hardware and software stack.
6.1 Hardware
The cost of each instruction is dominated by the memory access
latency. Instructions that only access registers complete in less than
1 cycle. On the NetFPGA, we use a single-port 128-bit wide block
RAM that has a read (or write) latency of 1 cycle. We measured
the total per-stage latency by sending a hundreds of 4 instruction
TPP reading the clock from every stage, and found that the total
per-stage latency was exactly 2 cycles:
thus, parsing, execution,
and packet rewrite all complete within a cycle, except for CSTORE,
which takes 1 cycle to execute (excluding the time for accessing
operands from memory).
The latency cost is different in a real switch: From personal com-
munication with multiple ASIC designers [6, 8], we learned that
1GHz ASIC chips in the market typically use single-port SRAMs
32–128bits wide, and have a 2–5 cycle latency for every operation
(read/write). This means that in the worst case, each load/store
instruction adds a 5 cycle latency, and a CSTORE adds 10 cycles.
Thus, in the worst case, if every instruction is a CSTORE, a TPP
can add a maximum of 50ns latency to the pipeline; to avoid losing
throughput due to pipeline stalls, we can add 50ns worth of buffer-
ing (at 1Tb/s, this is 6.25kB for the entire switch). However, the
real cost is likely to be smaller because the ASIC already accesses
memory locations that are likely to be accessed by the TPP that is
being executed: For instance, the ASIC always looks up the ﬂow
entry, and updates queue sizes for memory accounting, so those
values needn’t be read twice.
Though switch latency costs are different from that of the Net-
FPGA, they do not signiﬁcantly impact packet processing latency,
as in a typical workload, queueuing and propagation delays dom-
inate end-to-end latency and are orders of magnitude larger. Even
within a switch, the unloaded ingress-egress latency for a commer-
cial ASIC is about 500ns per packet [3]. The lowest-latency ASICs
are in the range of about 200ns per packet [16]. Thus, the extra
50ns worst-case cost per packet adds at most 10–25% extra latency
to the packet. Table 3 summarizes the latency costs.
Resource
Slices
Slice registers
LUTs
LUT-ﬂip ﬂop pairs
Router
26.8K
64.7K
69.1K
88.8K
+TCPU %-extra
21.6%
21.6%
30.1%
24.5%
5.8K
14.0K
20.8K
21.8K
Table 4: Hardware cost of TPP modules at 4 pipelines in the Net-
FPGA (4 outputs, excluding the DMA pipeline).
Figure 9: Maximum attainable application-level and network
throughput with a 260 byte TPPs inserted on a fraction of pack-
ets (1500Byte MTU and 1240Byte MSS). A sampling frequency of
∞ depicts the baseline performance as no TPPs are installed. Error
bars denote the standard deviation.
Die Area: The NetFPGA costs are summarized in Table 4. Com-
pared to the single-stage reference router, the costs are within
30.1% in terms of the number of gates. However, gate counts by
themselves do not account for the total area cost, as logic only ac-
counts for a small fraction of the total area that is dominated by
memory. To assess the area cost for a real switch, we use data from
Bosshart et al.
[7]. In their paper, the authors note that the extra
area for a total of 7000 processing units—which support instruc-
tions that are similar to the TCPU—distributed across all match-
action stages, accounts for less than 7% of the ASIC area [7, §5.4].
We only need 5× 64 = 320 TCPUs, one per instruction per stage
in the ingress/egress pipelines; therefore, the area costs are not sub-
stantial (0.32%).
6.2 End-host Stack
The critical component in the end-host stack is the dataplane. In
the transmit side, the dataplane processes every packet, matches
against a list of ﬁlters, and attaches TPPs. We use a 4-core Intel
core i7 machine running Linux 3.12.6.
Figure 9 shows the baseline throughput of a single TCP ﬂow,
without segmentation ofﬂoads, across a virtual ethernet link, which
was able to push about 4Gb/s trafﬁc with one TCP ﬂow, and about
6.5Gb/s of trafﬁc with 20 ﬂows. After adding TPPs, the throughput
of the TCP ﬂow reduces, depending on the (uniform random) sam-
pling frequency. If the sampling frequency is inﬁnite, none of the
packets have TPPs, which denotes the best possible performance in
our setup. As we can see, the network throughput doesn’t suffer
much, which shows that the CPU overhead to add/remove TPPs is
minimal. However, application throughput reduces proportionally,
due to header overheads. Table 5 shows the impact on the number
of ﬁlters in the dataplane, and its effect on network throughput, un-
der three different scenarios: (i) ‘ﬁrst’ means we create ﬂows that
always match the ﬁrst rule, (ii) ‘last’ means ﬂows always match the
Match
First
Last
All
0
8.8
8.8
8.8
1
8.7
8.7
8.7
# Rules
10
8.6
8.6
8.3
100
7.8
7.7
6.7
1000
3.6
3.6
1.4
Table 5: Maximum attainable network throughput in Gb/s with
varying number of ﬁlters (1500Byte MTU). The numbers are the