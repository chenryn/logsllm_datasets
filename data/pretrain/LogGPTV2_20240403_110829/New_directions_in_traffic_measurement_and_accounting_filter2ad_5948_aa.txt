title:New directions in traffic measurement and accounting
author:Cristian Estan and
George Varghese
New Directions in Trafﬁc Measurement and Accounting
Computer Science and Engineering Department
Computer Science and Engineering Department
Cristian Estan
George Varghese
University of California, San Diego
9500 Gilman Drive
La Jolla, CA 92093-0114
PI:EMAIL
ABSTRACT
Accurate network traﬃc measurement is required for ac-
counting, bandwidth provisioning and detecting DoS at-
tacks. These applications see the traﬃc as a collection of
ﬂows they need to measure. As link speeds and the number
of ﬂows increase, keeping a counter for each ﬂow is too ex-
pensive (using SRAM) or slow (using DRAM). The current
state-of-the-art methods (Cisco’s sampled NetFlow) which
log periodically sampled packets are slow, inaccurate and
resource-intensive. Previous work showed that at diﬀerent
granularities a small number of “heavy hitters” accounts for
a large share of traﬃc. Our paper introduces a paradigm
shift for measurement by concentrating only on large ﬂows
— those above some threshold such as 0.1% of the link ca-
pacity.
We propose two novel and scalable algorithms for iden-
tifying the large ﬂows: sample and hold and multistage ﬁl-
ters, which take a constant number of memory references per
packet and use a small amount of memory. If M is the avail-
able memory, we show analytically that the errors of our new
algorithms are proportional to 1/M ; by contrast, the error
√
of an algorithm based on classical sampling is proportional
to 1/
M , thus providing much less accuracy for the same
amount of memory. We also describe further optimizations
such as early removal and conservative update that further
improve the accuracy of our algorithms, as measured on re-
al traﬃc traces, by an order of magnitude. Our schemes
allow a new form of accounting called threshold accounting
in which only ﬂows above a threshold are charged by usage
while the rest are charged a ﬁxed fee. Threshold accounting
generalizes usage-based and duration based pricing.
Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network
Operations—traﬃc measurement, identifying large ﬂows
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’02, August 19-23, 2002, Pittsburgh, Pennsylvania, USA.
Copyright 2002 ACM 1-58113-570-X/02/0008 ...$5.00.
University of California, San Diego
9500 Gilman Drive
La Jolla, CA 92093-0114
PI:EMAIL
General Terms
Algorithms,Measurement
Keywords
Network traﬃc measurement, usage based accounting, scal-
ability, on-line algorithms, identifying large ﬂows
1.
INTRODUCTION
If we’re keeping per-ﬂow state, we have a scaling
problem, and we’ll be tracking millions of ants to
track a few elephants. — Van Jacobson, End-to-
end Research meeting, June 2000.
Measuring and monitoring network traﬃc is required to
manage today’s complex Internet backbones [9, 4]. Such
measurement information is essential for short-term moni-
toring (e.g., detecting hot spots and denial-of-service attacks
[14]), longer term traﬃc engineering (e.g., rerouting traﬃc
and upgrading selected links[9]), and accounting (e.g., to
support usage based pricing[5]).
The standard approach advocated by the Real-Time Flow
Measurement (RTFM) [3] Working Group of the IETF is to
instrument routers to add ﬂow meters at either all or selected
input links. Today’s routers oﬀer tools such as NetFlow [16]
that give ﬂow level information about traﬃc.
The main problem with the ﬂow measurement approach is
its lack of scalability. Measurements on MCI traces as early
as 1997 [22] showed over 250,000 concurrent ﬂows. More
recent measurements in [8] using a variety of traces show
the number of ﬂows between end host pairs in a one hour
period to be as high as 1.7 million (Fix-West) and 0.8 million
(MCI). Even with aggregation, the number of ﬂows in 1 hour
in the Fix-West used by [8] was as large as 0.5 million.
It can be feasible for ﬂow measurement devices to keep
up with the increases in the number of ﬂows (with or with-
out aggregation) only if they use the cheapest memories:
DRAMs. Updating per-packet counters in DRAM is already
impossible with today’s line speeds; further, the gap between
DRAM speeds (improving 7-9% per year) and link speeds
(improving 100% per year) is only increasing. Cisco Net-
Flow [16], which keeps its ﬂow counters in DRAM, solves
this problem by sampling: only sampled packets result in
updates. But NetFlow sampling has problems of its own (as
we show later) since it aﬀects measurement accuracy.
Despite the large number of ﬂows, a common observation
found in many measurement studies (e.g., [9, 8]) is that a
323small percentage of ﬂows accounts for a large percentage of
the traﬃc. [8] shows that 9% of the ﬂows between AS pairs
account for 90% of the byte traﬃc between all AS pairs.
For many applications, knowledge of these large ﬂows is
probably suﬃcient. [8, 17] suggest achieving scalable diﬀer-
entiated services by providing selective treatment only to a
small number of large ﬂows.
[9] underlines the importance
of knowledge of “heavy hitters” for decisions about network
upgrades and peering. [5] proposes a usage sensitive billing
scheme that relies on exact knowledge of the traﬃc of large
ﬂows but only samples of the traﬃc of small ﬂows.
We conclude that it is infeasible to accurately measure all
ﬂows on high speed links, but many applications can beneﬁt
from accurately measuring only the few large ﬂows. One
can easily keep counters for a few large ﬂows using a small
amount of fast memory (SRAM). However, how does the
device know which ﬂows to track? If one keeps state for all
ﬂows to identify the few large ﬂows, our purpose is defeated.
Thus a reasonable goal is to devise an algorithm that iden-
tiﬁes large ﬂows using memory that is only a small constant
larger than is needed to describe the large ﬂows in the ﬁrst
place. This is the central question addressed by this paper.
We present two algorithms that provably identify large ﬂows
using such a small amount of state. Further, our algorithms
use only a few memory references, making them suitable for
use in high speed routers.
1.1 Problem deﬁnition
A ﬂow is generically deﬁned by an optional pattern (which
deﬁnes which packets we will focus on) and an identiﬁer (val-
ues for a set of speciﬁed header ﬁelds). We can also general-
ize by allowing the identiﬁer to be a function of the header
ﬁeld values (e.g., using preﬁxes instead of addresses based
on a mapping using route tables). Flow deﬁnitions vary with
applications: for example for a traﬃc matrix one could use
a wildcard pattern and identiﬁers deﬁned by distinct source
and destination network numbers. On the other hand, for
identifying TCP denial of service attacks one could use a
pattern that focuses on TCP packets and use the destina-
tion IP address as a ﬂow identiﬁer.
Large ﬂows are deﬁned as those that send more than a giv-
en threshold (say 0.1% of the link capacity) during a given
measurement interval (1 second, 1 minute or even 1 hour).
The technical report [6] gives alternative deﬁnitions and al-
gorithms based on deﬁning large ﬂows via leaky bucket de-
scriptors.
An ideal algorithm reports, at the end of the measurement
interval, the ﬂow IDs and sizes of all ﬂows that exceeded the
threshold. A less ideal algorithm can fail in three ways: it
can omit some large ﬂows, it can wrongly add some small
ﬂows to the report, and can give an inaccurate estimate of
the traﬃc of some large ﬂows. We call the large ﬂows that
evade detection false negatives, and the small ﬂows that are
wrongly included false positives.
The minimum amount of memory required by an ideal al-
gorithm is the inverse of the threshold; for example, there
can be at most 1000 ﬂows that use more than 0.1% of the
link. We will measure the performance of an algorithm by
four metrics: ﬁrst, its memory compared to that of an ideal
algorithm; second, the algorithm’s probability of false neg-
atives; third, the algorithm’s probability of false positives;
and fourth, the expected error in traﬃc estimates.
1.2 Motivation
Our algorithms for identifying large ﬂows can potentially
be used to solve many problems. Since diﬀerent applications
deﬁne ﬂows by diﬀerent header ﬁelds, we need a separate
instance of our algorithms for each of them. Applications
we envisage include:
• Scalable Threshold Accounting: The two poles
of pricing for network traﬃc are usage based (e.g., a
price per byte for each ﬂow) or duration based (e.g.,
a ﬁxed price based on duration). While usage-based
pricing [13, 20] has been shown to improve overall u-
tility, usage based pricing in its most complete form is
not scalable because we cannot track all ﬂows at high
speeds. We suggest, instead, a scheme where we mea-
sure all aggregates that are above z% of the link; such
traﬃc is subject to usage based pricing, while the re-
maining traﬃc is subject to duration based pricing. By
varying z from 0 to 100, we can move from usage based
pricing to duration based pricing. More importantly,
for reasonably small values of z (say 1%) threshold
accounting may oﬀer a compromise between that is s-
calable and yet oﬀers almost the same utility as usage
based pricing.
[1] oﬀers experimental evidence based
on the INDEX experiment that such threshold pricing
could be attractive to both users and ISPs. 1.
• Real-time Traﬃc Monitoring: Many ISPs moni-
tor backbones for hot-spots in order to identify large
traﬃc aggregates that can be rerouted (using MPLS
tunnels or routes through optical switches) to reduce
congestion. Also, ISPs may consider sudden increases
in the traﬃc sent to certain destinations (the victims)
to indicate an ongoing attack. [14] proposes a mecha-
nism that reacts as soon as attacks are detected, but
does not give a mechanism to detect ongoing attacks.
For both traﬃc monitoring and attack detection, it
may suﬃce to focus on large ﬂows.
• Scalable Queue Management: At a smaller time
scale, scheduling mechanisms seeking to approximate
max-min fairness need to detect and penalize ﬂows
sending above their fair rate. Keeping per ﬂow state
only for these ﬂows [10, 17] can improve fairness with
small memory. We do not address this application
further, except to note that our techniques may be
useful for such problems. For example, [17] uses clas-
sical sampling techniques to estimate the sending rates
of large ﬂows. Given that our algorithms have better
accuracy than classical sampling, it may be possible
to provide increased fairness for the same amount of
memory by applying our algorithms.
The rest of the paper is organized as follows. We de-
scribe related work in Section 2, describe our main ideas in
Section 3, and provide a theoretical analysis in Section 4.
We theoretically compare our algorithms with NetFlow in
Section 5. After showing how to dimension our algorithms in
Section 6, we describe experimental evaluation on traces in
Section 7. We end with implementation issues in Section 8
and conclusions in Section 9.
1Besides [1], a brief reference to a similar idea can be found
in [20]. However, neither paper proposes a fast mechanism
to implement the idea.
3242. RELATED WORK
The primary tool used for ﬂow level measurement by IP
backbone operators is Cisco NetFlow [16]. NetFlow keeps
per ﬂow state in a large, slow DRAM. Basic NetFlow has two
problems: i) Processing Overhead: updating the DRAM
slows down the forwarding rate; ii) Collection Overhead:
the amount of data generated by NetFlow can overwhelm
the collection server or its network connection. For example
[9] reports loss rates of up to 90% using basic NetFlow.
The processing overhead can be alleviated using sampling:
per-ﬂow counters are incremented only for sampled packets.
We show later that sampling introduces considerable inaccu-
racy in the estimate; this is not a problem for measurements
over long periods (errors average out) and if applications do
not need exact data. However, we will show that sampling
does not work well for applications that require true low-
er bounds on customer traﬃc (e.g., it may be infeasible to
charge customers based on estimates that are larger than ac-
tual usage) and for applications that require accurate data
at small time scales (e.g., billing systems that charge higher
during congested periods).
The data collection overhead can be alleviated by having
the router aggregate ﬂows (e.g., by source and destination
AS numbers) as directed by a manager. However,
[8] shows
that even the number of aggregated ﬂows is very large. For
example, collecting packet headers for Code Red traﬃc on a
class A network [15] produced 0.5 Gbytes per hour of com-
pressed NetFlow data and aggregation reduced this data
only by a factor of 4. Techniques described in [5] can be
used to reduce the collection overhead at the cost of further
errors. However, it can considerably simplify router process-
ing to only keep track of heavy-hitters (as in our paper) if
that is what the application needs.
Many papers address the problem of mapping the traﬃc of
large IP networks. [9] deals with correlating measurements
taken at various points to ﬁnd spatial traﬃc distributions;
the techniques in our paper can be used to complement their
methods.
[4] describes a mechanism for identifying packet
trajectories in the backbone, that is not focused towards
estimating the traﬃc between various networks.
Bloom ﬁlters [2] and stochastic fair blue [10] use similar
but diﬀerent techniques to our parallel multistage ﬁlters to
compute very diﬀerent metrics (set membership and drop
probability). Gibbons and Matias [11] consider synopsis da-
ta structures that use small amounts of memory to approx-
imately summarize large databases. They deﬁne counting
samples that are similar to our sample and hold algorithm.
However, we compute a diﬀerent metric, need to take into
account packet lengths and have to size memory in a diﬀer-
ent way. In [7], Fang et al look at eﬃcient ways of answering
iceberg queries, or counting the number of appearances of
popular items in a database. Their multi-stage algorithm
is similar to multistage ﬁlters that we propose. However,
they use sampling as a front end before the ﬁlter and use
multiple passes. Thus their ﬁnal algorithms and analyses
are very diﬀerent from ours. For instance, their analysis is
limited to Zipf distributions while our analysis holds for all
traﬃc distributions.
3. OUR SOLUTION
Because our algorithms use an amount of memory that is
a constant factor larger than the (relatively small) number
of large ﬂows, our algorithms can be implemented using on-
chip or oﬀ-chip SRAM to store ﬂow state. We assume that
at each packet arrival we can aﬀord to look up a ﬂow ID in
the SRAM, update the counter(s) in the entry or allocate
a new entry if there is no entry associated with the current
packet.
The biggest problem is to identify the large ﬂows. Two
approaches suggest themselves. First, when a packet arrives
with a ﬂow ID not in the ﬂow memory, we could make place
for the new ﬂow by evicting the ﬂow with the smallest mea-
sured traﬃc (i.e., smallest counter). While this works well
on traces, it is possible to provide counter examples where
a large ﬂow is not measured because it keeps being expelled
from the ﬂow memory before its counter becomes large e-
nough, even using an LRU replacement policy as in [21].
A second approach is to use classical random sampling.
Random sampling (similar to sampled NetFlow except us-
ing a smaller amount of SRAM) provably identiﬁes large
ﬂows. We show, however, in Table 1 that random sam-
√
pling introduces a very high relative error in the measure-
ment estimate that is proportional to 1/
M , where M is
the amount of SRAM used by the device. Thus one needs
very high amounts of memory to reduce the inaccuracy to
acceptable levels.
The two most important contributions of this paper are
two new algorithms for identifying large ﬂows: Sample and
Hold (Section 3.1) and Multistage Filters (Section 3.2). Their
performance is very similar, the main advantage of sam-
ple and hold being implementation simplicity, and the main
advantage of multistage ﬁlters being higher accuracy.
In
contrast to random sampling, the relative errors of our two
new algorithms scale with 1/M , where M is the amount of
SRAM. This allows our algorithms to provide much more
accurate estimates than random sampling using the same
amount of memory.
In Section 3.3 we present improve-
ments that further increase the accuracy of these algorithms
on traces (Section 7). We start by describing the main ideas
behind these schemes.
3.1 Sample and hold
Base Idea: The simplest way to identify large ﬂows is
through sampling but with the following twist. As with or-
dinary sampling, we sample each packet with a probability.
If a packet is sampled and the ﬂow it belongs to has no entry
in the ﬂow memory, a new entry is created. However, after
an entry is created for a ﬂow, unlike in sampled NetFlow,
we update the entry for every subsequent packet belonging
to the ﬂow as shown in Figure 1.
Thus once a ﬂow is sampled a corresponding counter is
held in a hash table in ﬂow memory till the end of the mea-
surement interval. While this clearly requires processing
(looking up the ﬂow entry and updating a counter) for ev-
ery packet (unlike Sampled NetFlow), we will show that the
reduced memory requirements allow the ﬂow memory to be
in SRAM instead of DRAM. This in turn allows the per-
packet processing to scale with line speeds.
Let p be the probability with which we sample a byte.
Thus the sampling probability for a packet of size s is ps =
1−(1−p)s. This can be looked up in a precomputed table or
approximated by ps = p ∗ s. Choosing a high enough value