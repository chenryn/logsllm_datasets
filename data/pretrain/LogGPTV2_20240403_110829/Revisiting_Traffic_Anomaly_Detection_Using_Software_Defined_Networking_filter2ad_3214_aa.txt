title:Revisiting Traffic Anomaly Detection Using Software Defined Networking
author:Syed Akbar Mehdi and
Junaid Khalid and
Syed Ali Khayam
Maestro: A System for Scalable OpenFlow Control
Zheng Cai Alan L. Cox T. S. Eugene Ng
Department of Computer Science
Rice University
ABSTRACT
The fundamental feature of an OpenFlow network is that the
controller is responsible for the initial establishment of every
(cid:3)ow by contacting related switches. Thus the performance of
the controller could be a bottleneck. This paper shows how
this fundamental problem is addressed by parallelism. The
state of the art OpenFlow controller, called NOX, achieves a
simple programming model for control function development
by having a single-threaded event-loop. Yet NOX has not
considered exploiting parallelism. We propose Maestro which
keeps the simple programming model for programmers, and
exploits parallelism in every corner together with additional
throughput optimization techniques. We experimentally show
that the throughput of Maestro can achieve near linear scala-
bility on an eight core server machine.
performance optimization
Keywords-OpenFlow, network management, multithreading,
I. INTRODUCTION
Inspired by the 4D architecture [6], the emerging Open-
Flow [9] switch architecture separates the two main functions
of a classical router/switch: data plane packet switching and
control plane routing decision making. The OpenFlow switch
devices only implement the data plane packet switching func-
tionality. The OpenFlow controller machine takes charge of
the control plane functionality by installing and deleting (cid:3)ow
entries on switch devices. OpenFlow creates new opportunities
to realize rich networking functions, by allowing the users to
(cid:3)exibly program control plane functionalities on the OpenFlow
controller, and to freely control the data plane of the switch
devices. The success of OpenFlow can be seen from the
large number of recent use cases: programmable network
testbeds [8][19][13], datacenter network designs [15][12][1],
enterprise network designs [11][10][5], network measurement
systems [2][17], to name just a few recent examples.
One fundamental feature of OpenFlow is that, the controller
is responsible for establishing every (cid:3)ow in the network.
Whenever a switch sees a (cid:3)ow’s (cid:2)rst packet, because there
is no (cid:3)ow entry con(cid:2)gured on the switch’s (cid:3)ow table (usually
implemented in TCAM) to match this (cid:3)ow, the (cid:2)rst packet
will be forwarded to the controller. We call this (cid:2)rst packet a
(cid:147)(cid:3)ow request(cid:148). The controller will (cid:2)rst check this (cid:3)ow against
security policies to see whether it should be allowed, and if
allowed the controller needs to compute a path for this (cid:3)ow,
and install (cid:3)ow entries on every switch along the chosen path.
Finally, the packet itself will be sent back to the origin switch
from the controller. As the network scales in size, so will the
number of (cid:3)ows that need to be established by this process.
If the controller does not have the capacity for handling all
these (cid:3)ow establishment requests, it will become a network
bottleneck.
With OpenFlow switches already being used for designing
large-scale datacenter networks connecting hundreds of thou-
sands of servers, improving the performance of the controller
system to keep up with the rising demand becomes a critical
challenge. Measurements of traf(cid:2)c from data centers with
different sizes and purposes [3] have shown that the number of
concurrent active (cid:3)ows is small, which implies that OpenFlow
switches can be a well (cid:2)t for being applied in building
data center networks. However, the authors show that for
a data center which has 100 edge switches, the centralized
controller can expect to see about 10 million (cid:3)ow requests
per second. This creates a fundamental challenge for the
centralized OpenFlow controller to be deployed in a larce scale
data center.
Fortunately, such (cid:3)ow request processing in the controller
is potentially parallelizable. There are no complicated data
dependencies, all the computation is about checking a (cid:3)ow
against security policies, (cid:2)nding a path for it, and sending out
(cid:3)ow entry con(cid:2)guration messages. With the commoditization
and wide-spread adoption of multi-core processors (AMD has
already shipped x86 processors with 12 cores), the time is
right to examine how to best address the controller bottleneck
problem by exploiting parallelism.
NOX [7], the state-of-the-art OpenFlow controller system, is
a centralized and single-threaded system. Although this design
achieves a simple programming model for control function
development by having a single-threaded event-loop, it cannot
take advantage of current advances in multi-core technology.
We (cid:2)nd that another inef(cid:2)ciency of NOX is that each (cid:3)ow
request is processed individually, and all packets generated
accordingly are sent individually. By pro(cid:2)ling NOX, we (cid:2)nd
that about 80% of the (cid:3)ow request processing time is spent
in sending out messages individually. Through a microbench-
mark experiment presented later on in the design section, we
show that the overhead of multiple socket write operations to
send each packet individually to the same destination instead
of a single batched send is very high. Thus, besides investigat-
ing the use of multi-threading to best leverage the capability
of multi-core processors, another important principle we focus
on is correctly using batching to improve the ef(cid:2)ciency of the
system.
In this paper, we present a new controller design called
Maestro, which keeps a simple single-threaded programming
model for application programmers of the system, yet en-
ables and manages parallelism as a service to application
programmers. It exploits parallelism in every corner together
with additional throughput optimization techniques to scale the
throughput of the system. We have implemented Maestro in
Java. We experimentally show that the throughput of Maestro
can achieve near linear scalability on an eight core server
machine.
While the individual design decisions, optimization tech-
niques, and (cid:2)ne-tuning techniques we employ are what make
Maestro unique, the most important value of Maestro lies
in the fact that it is a complete system engineered to meet
the speci(cid:2)c characteristics and needs of OpenFlow, that it
addresses a real-world open challenge, and that it will have
immediate positive impact on many deployed and to be
deployed OpenFlow networks as Maestro will be open-sourced
in the very near future.
The rest of this paper is organized as follows. In Section II,
we present the design and implementation of Maestro. In
Section III, we experimentally evaluate the performance of
Maestro to show the bene(cid:2)ts of our techniques. We discuss
the related work in Section IV, and conclude in Section V.
II. SYSTEM DESIGN AND IMPLEMENTATION
A. The Overall Structure of Maestro
Topology
Changes
Discovery
Intradomain
Updates
Routing 
Routing
Table
LLDP
Packets
Input 
Stage
Raw-
Packet 
Task 
Queue
Flow
Requests
Authen-
tication
Route 
Flow
Config
Msgs
Flow
Requests
Output 
Stage
Periodic
Triggers
Timer
Discovery
LLDP
Packets
Fig. 1. The overall structure of Maestro.
Functionality Structure
Figure 1 shows the functionality structure of Maestro. Any
OpenFlow controller will share these similar functionalities,
although it may implement them in different ways. Maestro
sends and receives OpenFlow messages to and from network
switches via per switch TCP connections. The (cid:147)Input Stage(cid:148)
and (cid:147)Output Stage(cid:148) handle low level details of reading from
and writing to socket buffers, and translating raw OpenFlow
messages into and from high level data structures. These low
level functionalities stay (cid:2)xed with a particular version of
the OpenFlow protocol. Other high level functionalities may
vary and are implemented in modules called (cid:147)applications(cid:148)
in Maestro. Programmers can (cid:3)exibly modify the behavior of
these applications, or add new applications to meet different
goals. In Figure 1, the applications are: (cid:147)Discovery(cid:148), (cid:147)Intrado-
mainRouting(cid:148), (cid:147)Authentication(cid:148) and (cid:147)RouteFlow(cid:148).
When switches join the network by setting up TCP connec-
tions with Maestro, the (cid:147)Discovery(cid:148) application periodically
sends out probing messages to the neighbors of each switch.
The probing messages conform to the vendor-neutral Link
Layer Discovery Protocol (LLDP). This is represented by the
application execution path at the bottom of the (cid:2)gure. Then
when (cid:147)Discovery(cid:148) receives bounced back probing LLDP mes-
sages, it knows in the message that from which origin switch
this packet is sent, thus it can discover the topology of the
network. Such extra discovery process is mandatory because
in the current version of OpenFlow, version 1.0.0, switches do
not discover neighbors themselves. When (cid:147)Discovery(cid:148) (cid:2)nds
topology changes, it initiates the (cid:147)IntradomainRouting(cid:148) appli-
cation to update the (cid:147)RoutingTable(cid:148) data structure accordingly.
This is represented by the application execution path at the
top of the (cid:2)gure. This (cid:147)RoutingTable(cid:148) contains the all-pair-
shortest paths for the entire network, and will be used by the
(cid:147)RouteFlow(cid:148) application to (cid:2)nd paths for (cid:3)ow requests.
When Maestro receives a (cid:3)ow request from a switch,
this request will be (cid:2)rst checked against security policies
in the (cid:147)Authentication(cid:148) application. Only when allowed, the
(cid:147)RouteFlow(cid:148) application will try to (cid:2)nd a path for this request,
and generate one (cid:3)ow con(cid:2)guration message for each of the
switches along the chosen path. From now on we call these
two applications together the (cid:147)(cid:3)ow process stage(cid:148). After that,
all (cid:3)ow con(cid:2)guration messages will be sent to their destination
switches, and the (cid:3)ow request packet itself will be sent back
to the origin switch. This is represented by the application
execution path in the middle of the (cid:2)gure, which we call the
(cid:147)(cid:3)ow request execution path(cid:148) from now on.
All three application execution paths run in parallel. Mae-
stro makes sure that
if the (cid:147)RoutingTable(cid:148) data structure
gets updated while the (cid:147)RouteFlow(cid:148) application is already
running, (cid:147)RouteFlow(cid:148) continues with the older version of
(cid:147)RoutingTable(cid:148) to generate consistent results. Next time when
(cid:147)RouteFlow(cid:148) executes it will start to use the latest (cid:147)Rout-
ingTable(cid:148).
We choose Java to be the programming language for
Maestro, and there are several reasons. First, Java programs
are considered to be easy to write and to maintain. Java
programs are more secure, so it is relatively more easy to
debug and to maintain. Also Java can support dynamic loading
of applications and data structures without recompiling and
restarting the whole system more easily, so it will make
Maestro very (cid:3)exible to extend. Second, it is very easy to
migrate Java code to different platforms as long as there is
Java Virtual Machine support on that platform. Usually the
code needs very little or even no modi(cid:2)cation to work on
another platform, which makes Maestro more (cid:3)exible. Third,
although Java is considered to be less ef(cid:2)cient than C or C++,
but we argue and show by evaluation that, Maestro can achieve
overall good performance and scalability by incorporating the
right design and optimization techniques.
Multi-Threading Structure
Maestro has a task manager which provides a uni(cid:2)ed in-
terface for managing pending computations. Any computation
can be wrapped into a (cid:147)task(cid:148) java class and be submitted to the
task manager. The task manager manages a number of running
worker threads to execute these submitted tasks. The actual
number of worker threads is chosen based on the number
of processor cores in the controller machine. Take the (cid:3)ow
request execution path for example, which is also the focus of
the parallelization in Maestro. OpenFlow raw packets received
from the sockets are wrapped into tasks together with the input
stage code, and put in the raw-packet task queue. Any available
worker thread will pull one task from this queue, and execute
the input stage code to process the raw packet in that task. At
the end of the input stage, there will be (cid:3)ow requests generated
to be processed by the (cid:147)Authentication(cid:148) and (cid:147)RouteFlow(cid:148)
applications, i.e. the (cid:3)ow request stage. These requests are
wrapped into tasks together with the application code, and
put in the dedicated task queue of this worker thread. At
the end of the (cid:3)ow request stage, the generated con(cid:2)guration
messages and the (cid:3)ow requests themselves will be wrapped
into tasks together with the output stage code, and again put
in the dedicated task queue. Finally the worker thread will
execute these output stage tasks to send the messages out to
their destination switches. Maestro enables multiple instances
of the (cid:3)ow request execution path to be run concurrently by
different worker threads. Each application remains simple and
single-threaded. More details about why we choose such a
design will be provided later.
One can also choose to leave the burden of multi-threading
to programmers of the high level applications. For example, in
the (cid:147)Authentication(cid:148) and (cid:147)RouteFlow(cid:148) applications, program-
mers can divide the (cid:3)ow requests into several parts, and create
a number of java threads to work on these parts concurrently.
However, we argue that this approach is not ef(cid:2)cient. This is
because high level application programmers can only paral-
lelize the (cid:3)ow process stage; the low level input and output
stages still need to be parallelized by the underlying controller
platform. With threads created and managed by the high level
applications and threads created and managed by the controller
platform co-existing, scheduling threads optimally becomes
much more dif(cid:2)cult. This is also because it is relatively hard
for programmers when writing the applications to get and
consider runtime platform conditions, such as number of cores
available, free memory available, etc.
As a result, in our design, Maestro keeps a simple pro-
gramming model for programmers. Application programmers
do not need to deal with multi-threading within applications,
they simply write single-threaded applications. By composing
and con(cid:2)guring the applications, the Maestro system can run
multiple instances of an application execution path concur-
rently to achieve parallelism, with one worker thread handling
one instance.
B. Multi-Threading Design
Design Goals
1) Distribute work evenly among available threads/cores, so
that there will be no unbalanced situation where some
thread/core has too much work to do while others are
idling.
2) Minimize the overhead introduced by cross-core and
cache synchronization.
3) Minimize the memory consumption of the system to
achieve better memory ef(cid:2)ciency.
Distribute Work Evenly
To maximize the throughput of the system, work has to be
evenly distributed among worker threads in Maestro, so that no
processor cores sit idle while there is pending work to do. At
(cid:2)rst glance, one may choose to design the system in a way that
incoming (cid:3)ow requests are evenly divided and directly pushed
to all the dedicated task queues of the worker threads, in hope
of achieving an even workload distribution. However, in the
case of OpenFlow, this solution does not work, because each
(cid:3)ow request can require a different number of CPU cycles to
process. For example, a (cid:3)ow with a longer forwarding path
leads to more con(cid:2)guration messages generated, so there is
more data to process and to send. Although the path length
can be looked-up given access to the all pair shortest paths,
this will introduce an additional look-up overhead in the low
level raw-packet processing stage, and violate the modular
abstraction boundary because the all pair shortest paths are
maintained by programmer applications and not supposed to
be exposed to the low level system. In addition, security
authentication performed on (cid:3)ow requests can also require a
varying number of CPU cycles, depending on what security
policy governs that (cid:3)ow. This is much more dif(cid:2)cult to predict
than the length of the path. In summary, different OpenFlow
(cid:3)ow requests can require signi(cid:2)cantly differing numbers of
CPU cycles to process.
As a result, we design the task manager in a (cid:147)pull(cid:148) by
worker threads fashion, instead of actively pushing work to
worker threads. All worker threads share the same raw-packet
task queue, so that when there is a pending raw-packet task,
any available worker thread can pick it up. Although in
this way worker threads have to synchronize on this raw-
packet task queue, because task queue operations are very
lightweight, and the time spent in contending for locks is much
smaller than the time spent in handling the entire request,
the overhead of queue synchronization is relatively small. In
the evaluation section we will show that, the synchronization
overhead introduced by this pull design is negligible. The
bene(cid:2)t of this design is that work is evenly distributed among
worker threads.
Minimize Cross-Core Overhead
When running code or actively used data is moved from
one processor core to another, especially from one processor
to another processor which do not share the same cache, there
is overhead in synchronizing the core state and the cache. In
Maestro we want to minimize this kind of cross-core overhead
to maximize the performance of the system.
First of all, we use the taskset system call to bind each
worker thread to a particular processor core. By doing this,
we can prevent active running code from being rescheduled
to another processor core by the operating system, which will
introduce very large overhead. We call this design the (cid:147)core-
binding(cid:148).
Second, one way to minimize cache synchronization is
to make sure that all computation for processing one (cid:3)ow
Pull when idle
Raw-
Packet 
Task 
Queue
CPU Core
Worker Thread
Input 
Stage
Flow 
Process 
Stage
Output 
Stage
(cid:133) (cid:133)
CPU Core
Worker Thread
Pull when idle
Input 
Stage
Flow 