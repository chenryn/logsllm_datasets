### Table 10: Average AUC with Standard Deviation for Attack-6 on All Eight Datasets

| Dataset | AUC (Mean ± Std) |
|---------|------------------|
| Citeseer | 0.538 ± 0.022    |
| Cora    | 0.520 ± 0.011    |
| Pubmed  | 0.849 ± 0.004    |
| AIDS    | 0.947 ± 0.003    |
| COX2    | 0.940 ± 0.007    |
| DHFR    | 0.875 ± 0.034    |
| ENZYMES | 0.933 ± 0.008    |
| PROTEINS_full | 0.947 ± 0.012 |

### Table 11: Average AUC with Standard Deviation for Attack-6 Using GraphSAGE or GAT as the Target Model on All Eight Datasets

| Dataset        | AUC (GraphSAGE) | AUC (GAT)       |
|----------------|-----------------|-----------------|
| AIDS           | 0.977 ± 0.002   | 0.968 ± 0.001   |
| COX2           | 0.982 ± 0.001   | 0.984 ± 0.001   |
| DHFR           | 0.990 ± 0.001   | 0.995 ± 0.000   |
| ENZYMES        | 0.747 ± 0.001   | 0.766 ± 0.004   |
| PROTEINS_full  | 0.999 ± 0.000   | 0.999 ± 0.000   |
| Citeseer       | 0.938 ± 0.000   | 0.972 ± 0.000   |
| Cora           | 0.883 ± 0.001   | 0.958 ± 0.000   |
| Pubmed         | 0.923 ± 0.000   | 0.965 ± 0.000   |

### Table 12: Average AUC with Standard Deviation for Attack-3 When Reporting Top-2 Posteriors on All Eight Datasets

| Dataset        | AUC (Mean ± Std) |
|----------------|------------------|
| AIDS           | 0.855 ± 0.004    |
| COX2           | 0.839 ± 0.005    |
| DHFR           | 0.851 ± 0.003    |
| ENZYMES        | 0.876 ± 0.002    |
| PROTEINS_full  | 0.954 ± 0.001    |
| Citeseer       | 0.958 ± 0.000    |
| Cora           | 0.945 ± 0.001    |
| Pubmed         | 0.946 ± 0.001    |

### Comparison with Link Prediction
We further compare our attacks with a traditional link prediction method [40]. Specifically, we build an MLP using features such as Common Neighbor, Jaccard Index, and Preferential Attachment [40]. As shown in Figure 6, most of our attacks outperform the link prediction method. For example, on the COX2 dataset, all eight of our attacks outperform the link prediction model, with Attack-6 achieving more than a 20% performance gain. This demonstrates that GNNs are more susceptible to privacy risks compared to traditional link prediction methods.

### Effect of Different GNN Structures
In our experiments, we use the same architecture for both the target model and the shadow target model by default. We also evaluate the impact of using different architectures for the shadow target model. For space reasons, we only report results for Attack-1, but the findings are similar for other attacks. We set the number of hidden layers to 3 for the shadow target model (the target model has 2 hidden layers). The results, summarized in Table 16 in the Appendix, show that the average AUC scores of our attack are maintained at the same level or even higher for certain datasets compared to when the shadow target model and the shadow model have the same architecture. For instance, on the Citeseer dataset, we achieve an AUC of 0.924, while the original attack achieves 0.965. This indicates that our attacks remain effective even when the shadow target model and the target model have different architectures.

### Attacks on Other GNNs
We investigate whether our attacks are applicable to other GNN models, specifically GraphSAGE [27] and GAT [62]. We implement these models based on publicly available code and report the results for Attack-6. Table 11 shows that our attack achieves similar AUC scores on GraphSAGE and GAT compared to GCN. For example, on the COX2 dataset, our attack against GraphSAGE and GAT achieves AUCs of 0.982 and 0.984, respectively (the corresponding AUC for GCN is 0.987). This further demonstrates the general applicability of our attacks.

### Possible Defense
We explore restricting the GNN model to output the top-k largest posteriors as a defense mechanism. The intuition is that the smaller k is, the less information the model reveals. Here, we fix k = 2 and report the results for Attack-3. Experimental results in Table 12 show that this defense reduces the performance of our attack, but the drop is not significant. For example, on the Citeseer dataset, this defense reduces Attack-3’s performance by less than 2%. On the AIDS dataset, the attack's performance drops, but an AUC of 0.855 still indicates effectiveness. This defense impacts the utility of the model, creating a trade-off between utility and privacy. In conclusion, the top-k defense is not sufficient to fully mitigate our attacks.

### Summary of Results
From our experimental results, we make the following observations:
- Our attacks can effectively steal links from GNNs. For example, Attack-6 achieves average AUC scores over 0.95 on 7 out of 8 datasets, demonstrating the vulnerability of GNNs.
- Generally, attack performance improves with more background knowledge, as shown in Figure 6. The target dataset's partial graph is the most informative. For instance, Attack-3 (K = (×,A∗,×)) significantly outperforms Attack-1 (K = (×,×,D')) and Attack-2 (K = (F,×,×)).
- Our transfer attack performs well, especially when the shadow dataset and the target dataset are from the same domain, as validated by experimental results for Attack-1 and Attack-5.

### Related Work
Various research has shown that machine learning models are vulnerable to security and privacy attacks [9, 12, 30, 36–38, 49, 50, 53, 55, 60]. In this section, we survey four of these attacks that are most relevant to ours:
- **Membership Inference**: Adversaries aim to infer whether a data sample is in the target model’s training dataset. Shokri et al. [56] propose the first membership inference attacks, and Salem et al. [54] show their broad applicability. Defenses include dropout, model stacking, and adversarial examples [32, 42, 54, 56].
- **Model Inversion**: Adversaries aim to learn sensitive attributes of training data. Fredrikson et al. [20, 21] and Melis et al. [41] explore model inversion attacks on decision trees, neural networks, and collaborative learning scenarios.
- **Model Extraction**: Adversaries aim to steal the parameters of a target model or mimic its behaviors. Tramér et al. [60], Wang and Gong [63], and Orekondy et al. [44] propose various model extraction attacks and defenses.
- **Adversarial Attacks on Graph Neural Networks**: Recent studies show that GNNs are vulnerable to adversarial attacks [3, 13, 64, 66, 71, 73, 74]. These attacks manipulate graph structure and node features, and defenses include robustness enhancements and certified defenses [4, 66, 72, 75].

### Conclusion and Future Work
In this paper, we propose the first link stealing attacks against GNNs. Specifically, we show that, given black-box access, our attacks can effectively steal links from GNNs. Our future work will explore additional defenses, such as differential privacy and adversarial examples, to mitigate these attacks.