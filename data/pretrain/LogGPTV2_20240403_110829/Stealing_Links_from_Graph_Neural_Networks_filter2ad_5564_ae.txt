-
Citeseer
Cora
Pubmed
0.538 ± 0.022 0.520 ± 0.011 0.849 ± 0.004
0.947 ± 0.003 0.940 ± 0.007 0.875 ± 0.034
0.933 ± 0.008 0.947 ± 0.012 0.937 ± 0.003
0.657 ± 0.009 0.662 ± 0.006 0.677 ± 0.001
0.840 ± 0.013 0.823 ± 0.006 0.987 ± 0.005
0.977 ± 0.000 0.964 ± 0.000
0.960 ± 0.001
0.935 ± 0.001
0.956 ± 0.001 0.949 ± 0.001
-
-
-
Table 10: Average AUC with standard deviation for Attack-6
on all the 8 datasets.
Dataset
AIDS
COX2
DHFR
ENZYMES
AUC
Dataset
0.979 ± 0.001
0.987 ± 0.001
0.992 ± 0.001
0.891 ± 0.001
PROTEINS_full
Citeseer
Cora
Pubmed
AUC
0.999 ± 0.000
0.981 ± 0.000
0.964 ± 0.000
0.970 ± 0.000
problem, Attack-7 uses fewer features than Attack-6 (similar
to the reason that Attack-4 performs worse than Attack-3).
Comparison with Link Prediction. We further compare
all our attacks with a traditional link prediction method [40].
More speciﬁcally, we build an MLP with features summarized
from the target model’s partial graph, including Common
neighbor, Jaccard index, and Preferential attachment [40]. As
we can see from Figure 6, most of our attacks outperforms the
link prediction method. For instance, on the COX2 dataset, all
our 8 attacks outperform the link prediction model, the best
attack (Attack-6) achieves more than 20% performance gain.
This demonstrates that GNNs lead to more severe privacy
risks than traditional link prediction.
Effect of Different GNN Structures. In our experiments,
we adopt the same architecture for both the target model and
the shadow target model by default for transferring attack
scenarios. We further evaluate the impact of the shadow tar-
get model using different architectures. Note that for space
reasons, we only report the results of Attack-1. Results for
other attacks are similar. We set the number of hidden lay-
ers to 3 for the shadow target model (the target model has
2 hidden layers). The results are summarized in Table 16
in Appendix. We ﬁnd the average AUC scores of our attack
are maintained at the same level or even higher for certain
datasets compared with the scenario where the shadow target
model and the shadow model have the same architecture. For
USENIX Association
30th USENIX Security Symposium    2679
Table 12: Average AUC with standard deviation for Attack-3
when only reporting top-2 posteriors on all the 8 datasets.
Dataset
AIDS
COX2
DHFR
ENZYMES
AUC
Dataset
0.855 ± 0.004
0.839 ± 0.005
0.851 ± 0.003
0.876 ± 0.002
PROTEINS_full
Citeseer
Cora
Pubmed
AUC
0.954 ± 0.001
0.958 ± 0.000
0.945 ± 0.001
0.946 ± 0.001
results for Attack-3. Note that we have similar observations
for other attacks. Experimental results in Table 12 show that
this defense indeed reduces the performance of our attack.
However, the performance drop is not very big, i.e., our attack
still achieves relatively high AUC scores. For instance, on the
Citeseer dataset, this defense reduces Attack-3’s performance
by less than 2%. On the AIDS dataset, the attack’s perfor-
mance drop is higher but AUC being 0.855 still indicates our
attack is effective. We also note that the defense will impact
the utility of the model. In other words, it is a trade-off be-
tween utility and privacy. In conclusion, the top-k defense is
not effective enough to defend against our attacks.
We can also leverage differential privacy (DP) and adver-
sarial examples to mitigate our attacks. In detail, we can
adopt edge-DP developed for social networks [28, 68] to de-
fend against our attacks. Borrowing the idea from previous
work [31, 32], we can also add carefully crafted noise to the
prediction of GNN to fool the adversary. We plan to explore
both of them in the future.
Summary of Results. In summary, we have made the fol-
lowing observations from our experimental results.
• Our attacks can effectively steal the links from GNNs.
For instance, our Attack-6 can achieve average AUC
scores over 0.95 on 7 out of 8 datasets, which demon-
strate that the GNNs are vulnerable to our attacks.
• Generally speaking, the performances of the attack
are better if there is more background knowledge as
shown in Figure 6. However, we ﬁnd the impact of dif-
ferent knowledge is different. In particular, the target
dataset’s partial graph is the most informative. For in-
stance, Attack-3 (K = (×,A∗,×)) signiﬁcantly outper-
forms Attack-1 (K = (×,×,D(cid:48))) and Attack-2 (K =
(F ,×,×)).
• Our transferring attack can achieve good performance.
Furthermore, we ﬁnd that our transferring attack achieves
better performance when the shadow dataset and the
target dataset are from the same domain as validated by
experimental results for Attack-1 and Attack-5.
6 Related Work
Figure 6: Average AUC with standard deviation for all the
attacks on all the 8 datasets. For each attack, we list its best
result. The x-axis represents the dataset and the y-axis repre-
sents the AUC score.
Table 11: Average AUC with standard deviation for Attack-6
when using GraphSAGE or GAT as the target model on all
the 8 datasets.
Dataset
AUC (GraphSAGE)
AIDS
COX2
DHFR
ENZYMES
PROTEINS_full
Citeseer
Cora
Pubmed
0.977 ± 0.002
0.982 ± 0.001
0.990 ± 0.001
0.747 ± 0.001
0.999 ± 0.000
0.938 ± 0.000
0.883 ± 0.001
0.923 ± 0.000
AUC (GAT)
0.968 ± 0.001
0.984 ± 0.001
0.995 ± 0.000
0.766 ± 0.004
0.999 ± 0.000
0.972 ± 0.000
0.958 ± 0.000
0.965 ± 0.000
instance, on the Citeseer dataset, we obtain 0.924 AUC, while
the original attack achieves 0.965. In other words, our attacks
are still effective when the shadow target model and the target
model have different architectures.
Attacks on Other GNNs. We further investigate whether
our attacks are applicable to other GNN models besides GCN.
Concretely, we focus on GraphSAGE [27] and GAT [62]. We
implement GraphSAGE5 and GAT6 based on publicly avail-
able code and only report the results of Attack-6. Table 11
shows that our attack has similar AUC scores on GraphSAGE
and GAT compared to GCN. For instance, on the COX2
dataset, our attack against GraphSAGE and GAT achieves
AUC of 0.982 and 0.984, respectively (the corresponding
AUC for GCN is 0.987). This further demonstrates that our
attacks are generally applicable.
Possible Defense. We try to restrict the GNN model to out-
put k largest posteriors as a defense mechanism to mitigate
our attacks. The intuition is that the smaller k is, the less infor-
mation the model reveals. Here, we ﬁx k = 2 and report the
5https://github.com/williamleif/GraphSAGE
6https://github.com/PetarV-/GAT
Various research has shown that machine learning models are
vulnerable to security and privacy attacks [9,12,30,36–38,49,
2680    30th USENIX Security Symposium
USENIX Association
AIDSCOX2DHFRENZYMESPROTEINSfullCiteseerCoraPubmed0.600.650.700.750.800.850.900.951.00AUCAttack-0Attack-1Attack-2Attack-3Attack-4Attack-5Attack-6Attack-7LinkPrediction50, 53, 55, 60]. In this section, we mainly survey four of these
attacks that are most relevant to ours.
Membership Inference. In membership inference attacks [6,
10, 29, 39, 42, 43, 54, 56, 58, 67], the adversary aims to infer
whether a data sample is in the target model’s training dataset
or not. Shokri et al. [56] propose the ﬁrst membership infer-
ence attacks against machine learning models and demon-
strate its relationship with model overﬁtting. Salem et al. [54]
further show membership inference attacks are broadly appli-
cable at low cost via relaxing assumptions on the adversary.
To mitigate attacks, many empirical defenses [32, 42, 54, 56]
have been proposed. For instance, Nasr et al. [42] propose
to mitigate attacks via formulating the defense as a min-max
optimization problem which tries to decrease the accuracy
loss and increase the membership privacy. Salem et al. [54]
explore dropout and model stacking to mitigate membership
inference attacks. More recently, Jia et al. [32] leverage adver-
sarial examples to fool the adversary and show their defense
has a formal utility guarantee. Other attacks in this space study
membership inference in natural language processing mod-
els [57], generative models [8, 29], federated learning [41],
and biomedical data [26].
Model Inversion.
In model inversion attacks [20, 21, 30,
41, 48], the adversary aims to learn sensitive attributes of
training data from target models. For example, Fredrikson
et al. [21] propose the model inversion attack in which the
adversary can infer the patient’s genetic markers given the
model and some demographic information about the patients.
Fredrikson et al. [20] further explore the model inversion
attacks on decision trees and neural networks via exploiting
the conﬁdence score values revealed along with predictions.
Melis et al. [41] revealed that in the collaborative learning
scenarios, when the target model updated with new training
data, the adversary could infer sensitive attributes about the
new training data.
Model Extraction. In model extraction attacks [7,30,60,63],
the adversary aims to steal the parameters of a certain tar-
get model or mimic its behaviors. Tramér et al. [60] show
that an adversary can exactly recover the target model’s pa-
rameters via solving the equations for certain models, e.g.,
linear models. Wang and Gong [63] propose attacks to steal
the hyperparameters and show their attacks are broadly ap-
plicable to a variety of machine learning algorithms, e.g.,
ridge regression and SVM. Orekondy et al. [44] propose a
functionality stealing attack aiming at mimicking the behav-
iors of the target model. Concretely, they query the target
model and use the query-prediction pairs to train a “knock-
off” model. Jagielski et al. [30] improve the query efﬁciency
of learning-based model extraction attacks and develop the
practical functionally-equivalent model whose predictions are
identical to the target model on all inputs without training
model’s weights. Some defenses [34, 45] have been proposed
to defend against model extraction attacks. For instance, Juuti
et al. [34] propose to detect malicious queries via analyzing
the distribution of consecutive API queries and raises an alarm
when the distribution different from benign queries. Orekondy
et al [45] propose a utility-constrained defense against neural
network model stealing attacks via adding perturbations to
the output of the target model.
Adversarial Attacks on Graph Neural Networks. Some
recent studies [3,13,64,66,71,73,74] show that GNNs are vul-
nerable to adversarial attacks. In particular, the adversary can
fool GNNs via manipulating the graph structure and/or node
features. For instance, Zügner et al. [73] introduce adversarial
attacks to attributed graphs and focus on both training and
testing phase. In particular, their attacks target both node’s fea-
tures and graph structure and show that the node classiﬁcation
accuracy drops with a few perturbations. Bojchevski et al. [3]
analyze the vulnerability of node embeddings to graph struc-
ture perturbation via solving a bi-level optimization problem
based on eigenvalue perturbation theory. Zügner and Günne-
mann [74] investigate training time attacks on GNNs for node
classiﬁcation via treating the graph as a hyperparameter to
optimize. Wang and Gong [64] propose an attack to evade
the collective classiﬁcation based classiﬁer via perturbing
the graph structure, which can also transfer to GNNs. Dai
et al. [13] propose to fool the GNNs via manipulating the
combinatorial structure of data and try to learn generalizable
attack policy via reinforcement learning. Zhang et al. [71] pro-
pose a subgraph based backdoor attack to GNN based graph
classiﬁcation. In particular, a GNN classiﬁer outputs a target
label speciﬁed by an adversary when a predeﬁned subgraph is
injected to the testing graph. These studies are different from
our work since we aim to steal links from GNNs.
To mitigate attacks, many defenses [4,66,72,75] have been
proposed. For instance, Zhu et al. [72] propose to enhance
the robustness of GCNs via using Gaussian distributions in
graph convolutional layers to mitigate the effects of adversar-
ial attacks and leveraged attention mechanism to impede the
propagation of attacks. Zügner and Günnemann [75] propose
a learning principle that improves the robustness of the GNNs
and show provable robustness guarantees against nodes’ at-
tributes perturbation. Bojchevski et al. [3] propose to certify
the robustness against graph structure perturbation for a gen-
eral class of models, e.g., GNNs, via exploiting connections
to PageRank and Markov decision processes. These defenses
are designed to improve the robustness of GNNs rather than
preventing the privacy leakage of it. Note that there are also
some attacks and defenses on graph that focus on non-GNN
models [11, 33]. For instance, Chen et al. [11] propose at-
tacks that mislead the behavior of graph-cluster algorithm and
show some practical defenses. Jia et al. [33] propose certiﬁed
defense which is based on randomized smoothing to defend
against adversarial structural attacks to community detection.
USENIX Association
30th USENIX Security Symposium    2681
7 Conclusion and Future Work
In this paper, we propose the ﬁrst link stealing attacks against
GNNs. Speciﬁcally, we show that, given a black-box access