I am experiencing an unusual issue with Celery, where each pool worker terminates shortly after startup, resulting in one of the following two exception traces:

**Exception 1:**
```
[2012-09-03 16:11:42,326: WARNING/MainProcess] Process PoolWorker-6:
Traceback (most recent call last):
  File "/mnt/nimble/eggs/billiard-2.7.3.12-py2.7-linux-x86_64.egg/billiard/process.py", line 273, in _bootstrap
    self.run()
  File "/mnt/nimble/eggs/billiard-2.7.3.12-py2.7-linux-x86_64.egg/billiard/process.py", line 122, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/nimble/eggs/billiard-2.7.3.12-py2.7-linux-x86_64.egg/billiard/pool.py", line 302, in worker
    put((ACK, (job, i, time.time(), pid)))
  File "/mnt/nimble/eggs/billiard-2.7.3.12-py2.7-linux-x86_64.egg/billiard/queues.py", line 377, in put
    return send(obj)
IOError: [Errno 32] Broken pipe
```

**Exception 2:**
```
[2012-09-03 15:21:45,071: WARNING/MainProcess] Process PoolWorker-11:
Traceback (most recent call last):
  File "/mnt/nimble/eggs/billiard-2.7.3.12-py2.7-linux-x86_64.egg/billiard/process.py", line 273, in _bootstrap
    self.run()
  File "/mnt/nimble/eggs/billiard-2.7.3.12-py2.7-linux-x86_64.egg/billiard/process.py", line 122, in run
    self._target(*self._args, **self._kwargs)
  File "/mnt/nimble/eggs/billiard-2.7.3.12-py2.7-linux-x86_64.egg/billiard/pool.py", line 314, in worker
    put((READY, (job, i, (False, einfo))))
  File "/mnt/nimble/eggs/billiard-2.7.3.12-py2.7-linux-x86_64.egg/billiard/queues.py", line 377, in put
    return send(obj)
IOError: [Errno 32] Broken pipe
```

This problem does not occur when I launch Celery with the `--concurrency=1` option, but this is not a viable long-term solution.

Here are the details of my setup:
- Software: 
  - Celery: 3.0.9 (Chiastic Slide)
  - Kombu: 2.4.5
  - Python: 2.7.3
  - Billiard: 2.7.3.12
  - PyMongo: 2.1.1
- Platform: 
  - System: Linux
  - Architecture: 64-bit, ELF
  - Implementation: CPython
- Loader: `celery.loaders.default.Loader`
- Settings: 
  - Transport: MongoDB
  - Results: MongoDB

After further investigation, I found that the issue is related to the `CELERYD_MAX_TASKS_PER_CHILD` option in the configuration file. Removing this option resolved the problem, and Celery now functions as expected. It appears to be a multiprocessing issue where the main process loses track of the rotating workers.