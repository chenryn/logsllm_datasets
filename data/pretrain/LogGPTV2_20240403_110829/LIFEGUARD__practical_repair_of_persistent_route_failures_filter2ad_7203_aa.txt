title:LIFEGUARD: practical repair of persistent route failures
author:Ethan Katz-Bassett and
Colin Scott and
David R. Choffnes and
&apos;Italo Cunha and
Vytautas Valancius and
Nick Feamster and
Harsha V. Madhyastha and
Thomas E. Anderson and
Arvind Krishnamurthy
LIFEGUARD: Practical Repair of Persistent Route Failures
Ethan Katz-Bassett
Univ. of Southern California
Univ. of Washington
Colin Scott
UC Berkeley
David R. Choffnes
Univ. of Washington
Ítalo Cunha
UFMG, Brazil
Harsha V. Madhyastha
UC Riverside
Vytautas Valancius
Georgia Tech
Thomas Anderson
Univ. of Washington
Nick Feamster
Georgia Tech
Arvind Krishnamurthy
Univ. of Washington
ABSTRACT
The Internet was designed to always ﬁnd a route if there is a policy-
compliant path. However, in many cases, connectivity is disrupted
despite the existence of an underlying valid path. The research
community has focused on short-term outages that occur during
route convergence. There has been less progress on addressing
avoidable long-lasting outages. Our measurements show that long-
lasting events contribute signiﬁcantly to overall unavailability.
To address these problems, we develop LIFEGUARD, a system for
automatic failure localization and remediation. LIFEGUARD uses
active measurements and a historical path atlas to locate faults, even
in the presence of asymmetric paths and failures. Given the ability
to locate faults, we argue that the Internet protocols should allow
edge ISPs to steer trafﬁc to them around failures, without requir-
ing the involvement of the network causing the failure. Although
the Internet does not explicitly support this functionality today, we
show how to approximate it using carefully crafted BGP messages.
LIFEGUARD employs a set of techniques to reroute around failures
with low impact on working routes. Deploying LIFEGUARD on the
Internet, we ﬁnd that it can effectively route trafﬁc around an AS
without causing widespread disruption.
Categories and Subject Descriptors
C.2.2 [Communication Networks]: Network protocols
Keywords
Availability, BGP, Measurement, Outages, Repair
1.
INTRODUCTION
With the proliferation of interactive Web apps, always-connected
mobile devices and data storage in the cloud, we expect the Internet
to be available anytime, from anywhere.
However, even well-provisioned cloud data centers experience
frequent problems routing to destinations around the Internet. Ex-
isting research provides promising approaches to dealing with the
transient unavailability that occurs during routing protocol conver-
gence [18, 22–24], so we focus on events that persist over longer
timescales that are less likely to be convergence-related. Monitor-
ing paths from Amazon’s EC2 cloud service, we found that, for
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’12, August 13–17, 2012, Helsinki, Finland.
Copyright 2012 ACM 978-1-4503-1419-0/12/08 ...$10.00.
outages lasting at least 90 seconds, 84% of the unavailability came
from those that lasted over ten minutes.
We focus on disruptions to connectivity in which a working policy-
compliant path exists, but networks instead route along a different
path that fails to deliver packets. In theory this should never happen
– if working paths exist, the Internet protocols are designed to ﬁnd
them, even in the face of failures. In practice, routers can fail to
detect or reroute around a failed link, causing silent failures [35].
When an outage occurs, each affected network would like to re-
store connectivity. However, the failure may be caused by a prob-
lem outside the network, and available protocols and tools give
operators little visibility into or control over routing outside their
local networks. Operators struggle to obtain the topology and rout-
ing information necessary to locate the source of an outage, since
measurement tools like traceroute and reverse traceroute [19] re-
quire connectivity to complete their measurements.
Even knowing the failure location, operators have limited means
to address the problem. Traditional techniques for route control
give the operators’ network direct inﬂuence only over routes be-
tween it and its immediate neighbors, which may not be enough to
avoid a problem in a transit network farther away. Having multiple
providers still may not sufﬁce, as the operators have little control
over the routes other ASes select to it.
To substantially improve Internet availability, we need a way
to combat long-lived failures. We believe that Internet availabil-
ity would improve if data centers and other well-provisioned edge
networks were given the ability to repair persistent routing prob-
lems, regardless of which network along the path is responsible for
the outage. If some alternate working policy-compliant path can
deliver trafﬁc during an outage, the data center or edge network
should be able to cause the Internet to use it.
We propose achieving this goal by enabling an edge network to
disable routes that traverse a misbehaving network, triggering route
exploration to ﬁnd new paths. While accomplishing this goal might
seem to require a redesign of the Internet’s protocols, our objec-
tive is a system that works today with existing protocols, even if it
cannot address all outages. We present the design and implemen-
tation of a system that enables rerouting around many long-lasting
failures while being deployable on today’s Internet. We call our
system LIFEGUARD, for Locating Internet Failures Effectively and
Generating Usable Alternate Routes Dynamically. LIFEGUARD
aims to automatically repair partial outages in minutes, replacing
the manual process that can take hours. Existing approaches of-
ten enable an edge AS to avoid problems on its forward paths to
destinations but provide little control over the paths back to the AS.
LIFEGUARD provides reverse path control by having the edge AS O
insert the problem network A into path advertisements for O’s ad-
In 79% of the outages in our study, some vantage points had con-
nectivity with the target (one of the routers), while others did not.
Fig. 1 shows the durations of these 10,308 partial outages. By com-
parison, an earlier study found that 90% of outages lasted less than
15 minutes [13]. We also ﬁnd that most outages are relatively short;
more than 90% lasted less than 10 minutes (solid line). However,
these short outages account for only 16% of the total unavailability
(dotted line). The relatively small number of long-lasting problems
account for much of the overall unavailability. Delayed protocol
convergence does not explain long outages [23].
In fact, many long-lasting outages occur with few or no accom-
panying routing updates [13, 20]. With routing protocols failing to
react, networks continue to send trafﬁc along a path that fails to
deliver packets. Such problems can occur, for example, when a
router fails to detect an internal fault (e.g., corrupted memory on a
line card causing trafﬁc to be black-holed [35]) or when cross-layer
interactions cause an MPLS tunnel to fail to deliver packets even
though the underlying IP network is operational [21].
During partial outages, some hosts are unable to ﬁnd the working
routes to the destination, either due to a physical partition, due to a
routing policy that restricts the export of the working path, or due
to a router that is announcing a route that fails to deliver trafﬁc.
The techniques we present later in this paper rely on the underlying
network being physically connected and on the existence of policy-
compliant routes around the failure, and so we need to establish
that there are long-lasting outages that are not just physical failures
or the result of routing export policies.
We can rule out physical partitions as the cause in our EC2 study.
All EC2 instances maintained connectivity with a controller at our
institution throughout the study. So, physical connectivity existed
from the destination to some EC2 instance, from that instance to
the controller, then from the controller to the instance that could
not reach the destination.
Since the problems are not due to physical partitions, either rout-
ing policies are eliminating all working paths, or routers are adver-
tising paths that do not work. By detouring through our institu-
tion, the paths we demonstrated around EC2 failures violate the
valley-free routing policy [15] – in not making those paths avail-
able, routers are properly enacting routing policy. However, if
working policy-compliant paths also exist, it might be possible to
switch trafﬁc onto them.
2.2 Assessing Policy-Compliant Alternate Paths
Earlier systems demonstrated that overlays can route around many
failures [2, 6, 16]. However, overlay paths tend to violate BGP ex-
port policies. We build on this previous work by showing that al-
ternate policy-compliant paths appear to exist during many failures.
Generally, the longer a problem lasted, the more likely it was that
alternative routes existed.
Previous work found that many long-lasting failures occur out-
side of the edge networks [13, 20], and we focus on these types
of problems. Every ten minutes for a week starting September 5,
2011, we issued traceroutes between all PlanetLab sites. This set-
ting allowed us to issue traceroutes from both ends of every path,
and the probes to other PlanetLab sites give a rich view of other
paths that might combine to form alternate routes. We considered
as outages all instances in which a pair of hosts were up and had
previously been able to send traceroutes between each other, but
all traceroutes in both directions failed to reach the destination AS
for at least three consecutive rounds, before working again. This
yielded nearly 15,000 outages.
We checked if the traceroutes included working policy-compliant
routes around the failures. For each round of a failure, we tried to
Figure 1: For partial outages observed from EC2, the fraction of out-
ages of at most a given duration (solid) and their corresponding frac-
tion of total unreachability (dotted). The x-axis is on a log-scale. More
than 90% of the outages lasted at most 10 minutes, but 84% of the total
unavailability was due to outages longer than 10 minutes.
dresses, so that it appears that A has already been visited. When the
announcements reach A, BGP’s loop-prevention mechanisms will
drop the announcement. Networks that would have routed through
A will only learn of other paths, and will avoid A. Using the BGP-
Mux testbed [5] to announce paths to the Internet, we show LIFE-
GUARD’s rerouting technique ﬁnds alternate paths 76% of the time.
While this BGP poisoning provides a means to trigger rerouting,
we must address a number of challenges to provide a practical solu-
tion. LIFEGUARD combines this basic poisoning mechanism with a
number of techniques. LIFEGUARD has a subsystem to locate fail-
ures, even in the presence of asymmetric routing and unidirectional
failures. We validate our failure isolation approach and present ex-
periments suggesting that the commonly used traceroute technique
for failure location gave incorrect information 40% of the time. We
address how to decide whether to poison; will routing protocols
automatically resolve the problem, or do we need to trigger route
exploration? We show empirically that triggering route exploration
could eliminate up to 80% of the observed unavailability. When
it reroutes failing paths to remediate partial outages, LIFEGUARD
carefully crafts BGP announcements to speed route convergence
and minimize the disruption to working routes. Our experimen-
tal results show that 94% of working routes reconverge instantly
and experience minimal (≤ 2%) packet loss. After rerouting, LIFE-
GUARD maintains a sentinel preﬁx on the original path to detect
when the failure has resolved, even though live trafﬁc will be rout-
ing over an alternate path. When LIFEGUARD’s test trafﬁc reaches
the sentinel, LIFEGUARD removes the poisoned announcement.
2. BACKGROUND AND MOTIVATION
2.1 Quantifying Unreachability from EC2
To test the prevalence of outages, we conducted a measurement
study using Amazon EC2, a major cloud provider. EC2 presum-
ably has the resources, business incentive, and best practices avail-
able for combating Internet outages. We show that even EC2 data
centers experience many long-term network connectivity problems.
We rented EC2 instances in the four available AWS regions from
July 20, 2010 to August 29, 2010. Each vantage point issued a pair
of pings every 30 seconds to 250 targets – ﬁve routers each from
the 50 highest-degree ASes [33]. We selected the routers randomly
from the iPlane topology [17], such that each router was from a dis-
tinct BGP preﬁx. We focus on paths to routers in major networks,
which should be more reliable than paths to end-hosts. We deﬁne
an outage as four or more consecutive dropped pairs of pings from
a single vantage point to a destination. This methodology means
that the minimum outage duration we consider is 90 seconds.
 0 0.2 0.4 0.6 0.8 1 1 10 100 1000 10000Cumulative fractionDuration of outage in minutesEventsTotal unreachabilityﬁnd a path from the source that intersected (at the IP-level) a path
to the destination, such that the spliced path did not traverse the
AS in which the failed traceroute terminated. We only considered
a spliced path as valid if it would be available under observable
export policies. To check export policies, when splicing together a
potential path, we only accepted it if the AS subpath of length three
centered at the splice point appeared in at least one traceroute dur-
ing the week [17, 25]. This check sufﬁces to encode the common
valley-free export policy [15].
Our methodology may fail to identify some valid paths that exist.