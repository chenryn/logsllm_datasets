14.99 µs
13.70 µs
14.64 µs
12.92 µs
12.06 µs
Schedule 3
4.47 µs
4.31 µs
3.98 µs
4.05 µs
3.87 µs
3.85 µs
4.49 µs
4.47 µs
Schedule 4
14.25 µs
13.85 µs
14.07 µs
14.76 µs
14.27 µs
14.48 µs
13.22 µs
13.02 µs
Schedule 5
12.85 µs
14.09 µs
12.47 µs
13.15 µs
13.42 µs
13.55 µs
13.36 µs
14.80 µs
Schedule 6
13.37 µs
15.84 µs
13.48 µs
12.35 µs
13.47 µs
12.32 µs
15.11 µs
12.65 µs
Fig. 5: V8 Benchmark Slowdown (IA32).
Fig. 6: V8 Benchmark Slowdown (x64).
Richards
DeltaBlue
Crypto
RayTrace
EarlyBoyer
RegExp
Splay
NavierStokes
Score
Baseline
24913 (2.76%)
25657 (3.31%)
20546 (1.61%)
45399 (0.38%)
37711 (0.61%)
4802 (0.34%)
15391 (4.47%)
23377 (4.15%)
21071 (0.72%)
SDCG (Pinned)
23990 (0.28%)
24373 (0.43%)
19509 (1.27%)
42162 (0.75%)
34805 (0.27%)
4251 (1.04%)
13643 (0.71%)
22586 (0.42%)
19616 (0.35%)
SDCG (Free)
24803 (1.72%)
25543 (3.86%)
19021 (1.95%)
43995 (6.46%)
34284 (0.82%)
2451 (3.82%)
9259 (8.18%)
23518 (1.26%)
17715 (1.86%)
Richards
DeltaBlue
Crypto
RayTrace
EarlyBoyer
RegExp
Splay
NavierStokes
Score
Baseline
25178 (3.39%)
24324 (3.65%)
21313 (3.16%)
35298 (5.97%)
32264 (4.42%)
4853 (3.59%)
13957 (6.02%)
22646 (2.48%)
19712 (3.57%)
SDCG (Pinned)
24587 (2.31%)
23542 (0.38%)
20551 (0.26%)
32972 (1.03%)
30382 (0.61%)
4366 (0.82%)
12601 (2.92%)
21844 (0.30%)
18599 (0.62%)
SDCG (Free)
25500 (3.24%)
24385 (2.54%)
20483 (2.57%)
35878 (1.66%)
30135 (1.04%)
2456 (7.72%)
7332 (9.85%)
21468 (3.45%)
16435 (1.03%)
TABLE IV: V8 Benchmark Results (IA32). The score is
the geometric mean over 10 executions of the benchmark
suite. Number in the parentheses is the standard deviation.
TABLE V: V8 Benchmark Slowdown (x64). The score is
the geometric mean over 10 executions of the benchmark
suite. Number in the parentheses is the standard deviation.
baseline result; the second column is the result of SDCG-
ported V8 with a pinned schedule; and the last column is the
result of SDCG-ported V8 with a free schedule. All results
are the geometric mean over 10 executions of the benchmark.
The number in the parentheses is the standard deviation as
a percentage. As we can see, the ﬂuctuation is small, with
the baseline and a free schedule slightly higher than a pinned
schedule.
and the x64 build. For four benchmarks (Richards, DeltaBlue,
Crypto, and NavierStokes), the slowdown introduced by SDCG
is less than 5%, which is negligible because they are similar to
the standard deviation. The other four benchmarks (RayTrace,
EarlyBoyer, RegExp, and Splay) have higher overhead, but
with a pinned schedule, the slowdown is within 11%, which
is much smaller than previous SFI-based solutions [5] (79%
on IA32).
The corresponding slowdown is shown in Figure 5 (for
IA32 build) and Figure 6 (for x64 build). Overall, we did
not observe a signiﬁcant difference between the IA32 build
There are two major overhead sources. For RPC overhead,
we can see a clear trend that more RPC invocations (Table I),
increase slowdown. However, the impact of cache coherency
12
-­‐10.00%	
  0.00%	
  10.00%	
  20.00%	
  30.00%	
  40.00%	
  50.00%	
  Richards	
  DeltaBlue	
  Crypto	
  RayTrace	
  EarleyBoyer	
  RegExp	
  Splay	
  NavierStokes	
  Score	
  SDCG	
  (Pinned)	
  SDCG	
  (Free)	
  -­‐10.00%	
  0.00%	
  10.00%	
  20.00%	
  30.00%	
  40.00%	
  50.00%	
  Richards	
  DeltaBlue	
  Crypto	
  RayTrace	
  EarleyBoyer	
  RegExp	
  Splay	
  NavierStokes	
  Score	
  SDCG	
  (Pinned)	
  SDCG	
  (Free)	
  the thread to be swapped out of the CPU, which will extend
the attack window.
B. RPC Stub Generation
To port a dynamic translator to SDCG, our current solution
is to manually rewrite the source code. Even though the
modiﬁcation is relatively small compared to the translator’s
code size, the process still requires the developer to have
a good understanding of the internals of the translator. This
process can be improved or even automated through program
analysis. Firstly, our current RPC stub creation process is not
sound. That is, we relied on the test input. Thus, if a function
is not invoked during testing, or the given parameter does not
trigger the function to modify the code cache, then we miss
this function. Second, to reduce performance overhead and
the attack surface, we want to create stubs only for functions
that 1) are post-dominated by operations that modify the code
cache; and 2) dominate as many modiﬁcation operations as
possible. Currently, this is done empirically. Through program
analysis, we could systematically and more precisely identify
these “key” functions. Finally, for the ease of development, our
prototype implementation uses shared memory to avoid deep
copy of objects when performing RPC. While this strategy
is convenient, it may introduce additional cache coherency
overhead. With the help of program analysis, we could replace
this strategy with object serialization, but only for data that is
accessed during RPC.
C. Performance Tuning
In our current prototype implementations, the SDTs were
not aware of our modiﬁcation to their architectures. Since
their optimization strategy may not be ideal for SDCG, it is
possible to further reduce the overhead by making the SDT be
aware of our modiﬁcation. First, one major source of SDCG’s
runtime overhead is RPC invocation, and the overhead can be
reduced if we reduce the frequency of code cache modiﬁcation.
This can be accomplished in several ways. For instance, we
can increase the threshold to trigger code optimization, use
more aggressive speculative translation, separate the garbage
collection, etc.
Second,
in our implementations, we used the domain
socket-based IPC channel from the seccomp-sandbox. This
means for each RPC invocation, we need to enter the kernel
twice; and both the request/return data need to be copied
to/from the kernel. While this approach is more secure (in the
sense that a sent request cannot be maliciously modiﬁed), if the
request is always untrusted, then using a faster communication
channel (e.g., ring buffer) could further reduce the overhead.
Third, we used the same service model as seccomp-
sandbox in our prototypes. That is, RPC requests are served by
a single thread in the SDT process. This strategy is sufﬁcient
for SDTs where different threads share the same code cache
(e.g., Strata) since modiﬁcations need to be serialized anyway
to prevent a data race condition. However, this service model
can become a bottleneck when the SDT uses different code
caches for different thread (e.g., JS engines). For such SDTs,
we need to create dedicated service threads in the SDT process
to serve different threads in the untrusted process.
Fig. 4: SPEC CINT 2006 Slowdown. The baseline is the vanilla
Strata.
perlbench
bzip2
gcc
mcf
gobmk
hmmer
sjeng
libquantum
h264ref
omnetpp
astar
xalancbmk
GEOMEAN
Native
364
580
310
438
483
797
576
460
691
343
514
262
461
Strata
559
600
403
450
610
777
768
463
945
410
546
499
566
SDCG (Pinned)
SDCG (Free)
574
613
420
479
623
790
784
511
980
450
587
515
592
558
602
410
471
611
777
767
474
971
428
563
504
576
TABLE III: SPEC CINT 2006 Results. Since the standard
deviation is small (less than 1%), we omitted this information.
overhead caused by different scheduling strategies is not
consistent. For some benchmarks (Richards, DeltaBlu, and
RayTrace), free scheduling is faster than pinned scheduling.
For some benchmarks (Crypto and EarlyBoyer), overhead is
almost the same, but for two benchmarks (RegExp and Splay),