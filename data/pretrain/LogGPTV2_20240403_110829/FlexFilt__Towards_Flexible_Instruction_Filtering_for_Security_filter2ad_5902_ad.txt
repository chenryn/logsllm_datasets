the same ipkey to both good_code1 and good_code2 functions,
we associate them with the same trusted instruction domain, i.e.,
domain1. Then, we configure Flexible Filters to prevent the
execution of the WRPKRU instruction in the default domain, i.e.,
domain0. To assign the allocated ipkey to the two trusted functions,
we need to invoke the pkey_mprotect system call, which requires
the address and length of the trusted functions as input arguments.
To this end, as a proof of concept, we mark the two trusted functions
with an attribute in the source code and use a linker script to
page align these functions. To obtain the address range of these
two functions and invoke pkey_mprotect, rather than modifying
the loader, we use LD_PRELOAD. To simplify using LD_PRELOAD on
good_code1 and good_code2 functions, we define them as extern
function pointers. Subsequently, when the executable containing
good_code1 or good_code2 is about to get executed, first the shared
library (.so) that pre-loads these two functions gets loaded and the
loader fills in the corresponding addresses of these functions. In our
shared library, we use dlsym to obtain the address of good_code1
and good_code2 functions. Then, we use the obtained address as
an argument for invoking pkey_mprotect. Subsequently, we call
the original implementation of our trusted functions.
6 CASE STUDY
For prior works that rely on binary rewriting, filtering target in-
structions in dynamically generated code is more challenging than
the static code. A popular use-case of dynamically generated code
that adversaries are likely to take advantage of is in Just-In-Time
(JIT) compilers. JIT compilers are not limited to dynamically gener-
ating user-space code; JIT compilation also occurs in the kernel, e.g.,
extended Berkeley Packet Filters (eBPF) VM has a JIT compiler. In
this section, we provide an experimental study to demonstrate the
653FlexFilt: Towards Flexible Instruction Filtering for Security
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
advantages of leveraging FlexFilt for run-time instruction filtering
of dynamically generated code through JIT compilation.
Table 3: The measured size of executable bytes generated for
browsing the Alexa top-10 websites, on average.
6.1 JIT compilation
JIT compilation dynamically compiles interpreted programming
languages such as JavaScript into bytecode (an intermediate repre-
sentation) or native machine code. A JavaScript engine (e.g., Chakra-
Core [47], SpiderMonkey [48], and V8 [27]) is responsible for com-
piling and executing the JavaScript code, memory management, and
optimization. For illustration purposes, we use V8, which is Google’s
open-source JavaScript engine used in Chrome, Chromium, and
Node.js. Note that other JIT compilers work in a similar manner.
V8 first compiles the JavaScript code into a bytecode. Then, V8’s
optimizing compiler generates an optimized machine code from
the bytecode.
6.2 V8 JIT Compilation Experiment
As discussed by prior works including ERIM [61] and Donky [58],
isolation of the dynamically generated code through memory pro-
tection domains is of great importance. As a case study, we consider
the scenario of leveraging Intel MPK for intra-process memory iso-
lation of the Chromium browser. While browsing webpages, the V8
engine dynamically compiles the JavaScript code and translates it
to optimized native machine code. To prevent reuse of JIT compiled
code for unauthorized modification of a protection domain’s per-
mission bits, we need to ensure that the code does not contain any
occurrences of WRPKRU instruction. A modified JIT compiler can
prevent the emission of explicit unsafe instructions (e.g., V8 never
emits WRPKRU explicitly); however, preventing misaligned/overlap-
ping implicit instructions is challenging. For example, a JIT compiler
might use constant data in the JavaScript code for generating an in-
struction, which inadvertently can lead to the creation of an implicit
unsafe instruction. In the absence of ubiquitous compiler adjust-
ments, our hardware-assisted approach can transparently prevent
the execution of unsafe instructions at runtime.
An attacker can exploit an implicit occurrence of WRPKRU instruc-
tion using control-flow hijacking attacks and in turn gain access
permission to a protection domain. To prevent such an exploitation,
the previous works continuously scan the newly generated code at
runtime and rewrite the code if necessary. In this section, we ana-
lyze the overhead of scanning the dynamically generated code by
measuring the number of generated bytes in native machine code
while browsing various webpages. For this measurement, we built
and ran the Chromium browser [26] on an Intel® Core™ i7-4700MQ
processor @ 3.4GHz machine running Ubuntu 18.04.5 LTS. We used
v8_enable_disassembler=true flag for building Chromium to en-
able disassembler support in V8. To measure the total number of
generated bytes during JIT compilation, we ran Chromium with
--js-flags="--print-bytecode" flag and browsed the Alexa top-
10 websites [1].
Table 3 shows the total size of executable bytes generated by
V8 engine while browsing the Alexa top-10 websites. For each
website, we report two numbers: 1) the total size of executable
bytes generated when loading the frontpage of the website, and
2) the number of bytes generated per second while browsing each
website for 5 minutes. Note that for a website such as Google.com
Website
Executable bytes
generated when
loading the frontpage
Executable bytes
generated per second
while browsing the page
Google.com
Youtube.com
Tmall.com
Baidu.com
Qq.com
Sohu.com
Facebook.com
Taobao.com
Amazon.com
360.cn
Geometric mean
0
0
266,798
366,003
159,565
34,096
20,938
220,299
92,442
0
3,432
3,458
2,620
15,323
1,532
2,043
2,014
9,712
15,454
3,098
400
3,258
and Baidu.com, we do the browsing by searching various keywords
without opening any of the search results. For each website, we
repeated both the experiments, i.e., loading of the frontpage of
the website and 5 minutes browsing, three times and reported the
geometric mean. On average,3 a binary scanning approach has to
scan around 3,432 bytes and 3,258 bytes per second, respectively,
for loading the Alexa top-10 pages and browsing them. In the worst
case, 366,003 bytes (for Tmall.com) and 15,454 bytes per second
(for Taobao.com) should be scanned. Considering 4KB pages, a
binary scanning approach has to scan about 90 pages for loading
Tmall.com, while for continuous browsing of Taobao.com around
4 pages should be scanned almost every second.
6.2.1 Comparison with Prior Works. Once the binary scanning
finds an implicit occurrence of a target instruction, the prior works
rely on a binary rewriting approach or hardware watchpoints to
guarantee the safety of the target instruction execution. Binary scan-
ning is a trivial task. The challenge is the efficient implementation
of the binary rewriting. ERIM reports that the binary inspection
takes between 3.5 and 6.2 microseconds per page for SPEC2006
benchmarks [34]. Unfortunately, ERIM does not report the perfor-
mance overhead of their binary rewriting approach at runtime. For a
continuous process such as web browsing, the binary scanning and
binary rewriting should be implemented very efficiently, which is a
challenging task. Typically, a binary rewriting approach incurs con-
siderable performance overhead. For example, MULTIVERSE [6], a
state-of-the-art x86 static binary rewriter reports 60.42% runtime
overhead on SPECint2006 benchmarks while MAMBO [28], a dy-
namic binary rewriter for ARM reports an average overhead of 34%
for SPEC2006 benchmarks on a Cortex-A15. Instead of dynamic
binary rewriting, Hodor relies on a trusted loader to identify all the
occurrences of the WRPKRU instruction and uses hardware watch-
points to examine the safety of these occurrences. If there are more
occurrences of the WRPKRU instruction in a page than the number of
watchpoints, Hodor relies on single-step execution, which incurs
3As shown in Table 3, some websites had zero executable bytes generated. When
loading the frontpage of these website, they did not result in any native bytes. As a
workaround for calculating the geometric mean in the presence of samples with zero
values, we converted each zero value to one.
654ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Leila Delshadtehrani, Sadullah Canakci, William Blair, Manuel Egele, and Ajay Joshi
considerable performance overhead. Additionally, the current im-
plementation of Hodor does not support JIT. Although Hodor can
be extended to support JIT compiled code, it has to cause a page
fault before the execution of each added page to inspect the page
and configure the watchpoints. In contrast to ERIM and Hodor,
FlexFilt examines each executed instruction at hardware level with
negligible performance overhead (refer to Section 7.3) and prevents
the execution of target instructions without the need for binary
scanning, binary rewriting, or hardware watchpoints. Overall, scan-
ning the dynamically generated code for target instructions is un-
necessary if a protection mechanism can prevent the execution of
target instructions reliably, which FlexFilt does through instruction
protection domains and hardware-level Flexible Filters.
7 EVALUATION
In this section, we discuss our implementation and evaluation frame-
work, and demonstrate the feasibility of FlexFilt’s design with our
prototype. To this end, we have to demonstrate FlexFilt’s correct
functionality and enforcement of the developer’s chosen security
policy as well as acceptable performance, power, and area overheads.
To verify the correct functionality of FlexFilt, we demonstrate Flex-
Filt’s capability in preventing the execution of the WRPKR instruction
in untrusted domains. To evaluate FlexFilt’s performance overhead,
we devise experiments to measure the overhead of configuring Flex-
Filt, the context switch overhead, and the overall execution time
overhead. To estimate the area overhead of FlexFilt, we leverage the
resource utilization of our FPGA prototype and reason about our
power overhead according to our area overhead estimation. Finally,
we provide a head-to-head comparison of the filtering capability
and overheads of FlexFilt with PHMon, a hardware monitor.
7.1 Implementation and Evaluation
Framework
We implemented FlexFilt on a RISC-V Rocket core [2] using the
Chisel Hardware Description Language (HDL) [5]. The RISC-V
Rocket core is a single-issue in-order processor with a 6-stage
pipeline. We prototyped our hardware design on a Xilinx Zedboard
FPGA [52]. We modified the Linux kernel (v4.15) to add the support
for FlexFilt. We implemented our support for instruction protection
domains on top of the existing implementation of memory protec-
tion keys for the RISC-V ISA using the open-source Linux kernel
patches from Donky [58] and SealPK [14].
7.2 Functional Verification
To verify the correct functionality of FlexFilt, we implemented tests
that created scenarios similar to the one described in Section 5.3.
As an example, we leveraged the WRPKR extended instruction [14]
to implement memory protection domains. Then, we prevented the
execution of the WRPKR instruction except in the trusted functions
specified by the user. To this end, we leveraged FlexFilt’s API and
the LD_PRELOAD approach. We cross-compiled the code using RISC-
V GNU toolchain and ran the program on our FPGA prototype. As
expected, FlexFilt allows the execution of WRPKR in trusted functions
and prevents its execution anywhere else in the code by causing
an illegal instruction exception.
To demonstrate FlexFilt’s capability in preventing a security
attack, we leveraged a buffer overflow vulnerability in a simple pro-
gram to inject a WRPKR instruction, which modifies the permission
bits of a memory protection domain in an untrusted function. As
expected, FlexFilt was able to successfully prevent the execution of
the injected WRPKR instruction in the untrusted domain at runtime.
To verify the correct functionality of the FlexFilt’s kernel-level
filtering capability, we configured FlexFilt in BBL [56] to limit the
execution of our custom instructions used to maintain FlexFilt’s
information during context switches. As expected, FlexFilt allows
the execution of these custom instructions in the context switch
function and prevents their execution outside this function.
7.3 Performance Evaluation
In this section, we evaluate FlexFilt’s performance overhead using
microbenchmarks. Additionally, we report the context switch over-
head to maintain FlexFilt’s information and the overall execution
time overhead of FlexFilt using standard benchmarks.
7.3.1 Microbenchmarks. FlexFilt provides four Flexible Filters.
During the program execution, each Flexible Filter receives the
current instruction at the execution stage and applies its configured
filter on the instruction. As the filtering operation does not need
any extra cycles, we expect FlexFilt to incur negligible performance
overhead. At hardware level, all the Flexible Filters perform
the filtering operation in parallel. Hence, regardless of the number
of activated configured filters, we expect FlexFilt’s performance
overhead to remain the same. To examine the effect of the number
of activated configured Flexible Filters on the performance
overhead, we ran the mcf benchmark from SPEC2000 benchmark
suite [33] for active filter count ranging from 0 to 4. We repeated
each experiment 3 times and considered the geometric mean of
execution times as the performance metric. As expected, this exper-
iment showed that the execution time overhead of FlexFilt did not
change with the number of activated filters. We expect a similar
behavior in other benchmarks too. With various number of filters,
the total execution time stayed the same (the geometric mean of the
execution time overhead across various configurations was 0.17%
with a standard deviation of less than 1%).
We devised a microbenchmark to measure the overhead of con-
figuring Flexible Filters as well as the overhead for apply-
ing a combination of filters to an instruction domain. Appendix B
presents these measured overheads.
7.3.2 Context Switch Overhead. During the context switches, we
maintain FlexFilt’s information including the configuration of
each Flexible Filter and the contents of IPR. As the amount
of FlexFilt’s information to maintain stays the same during con-
text switches, we expect FlexFilt’s context switch overhead to be
the same across all applications. We measured the performance
overhead of FlexFilt during context switches for 9 (out of 12)
SPECint2000 [33] and 9 (out of 12) SPECint2006 [34] benchmarks
with test inputs. We were not able to run the remaining bench-
marks, i.e., eon, perlbmk, and vortex from SPECint2000 and mcf,
perlbench, and sjeng from SPECint2006, on our baseline system
due to runtime errors (e.g., out of memory error because our eval-
uation board only provides 256MB of DDR memory). In addition