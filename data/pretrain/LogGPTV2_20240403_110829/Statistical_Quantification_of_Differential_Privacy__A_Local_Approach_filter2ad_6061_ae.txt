(cid:2)
databases (xmax, x
location ˆtmax that maximize the empirical privacy violation.
For that purpose, MPL runs the DPL algorithm for each
(cid:2)
b) to approximate the data-speciﬁc privacy violation
pair (xb, x
b. Based on the empirical violations
xb,x(cid:2)
(cid:2)
max) with
ˆx1,x(cid:2)
the highest privacy loss is chosen. The location where the
empirical privacy loss ˆ(cid:8)xmax,x(cid:2)
max is maximized is called ˆtmax
(cid:2)
(which is an output of DPL run on (xmax, x
max)). Struc-
turally, this part of the algorithm resembles counterexample
(cid:2)
max, ˆtmax)
generation [19] and the tuple (ˆxmax,x(cid:2)
already yields useful information concerning the location and
magnitude of the maximum privacy violation.
B , the pair of databases (xmax, x
max , xmax, x
(xmax, x
The second part of
(cid:2)
max, ˆtmax). Notice
max (ˆtmax) ≈ xmax,x(cid:2)
the MPL algorithm is designed
the privacy loss
to establish a conﬁdence region for
by
construction
that
at
max holds (see Proposition
(cid:8)xmax,x(cid:2)
therefore said conﬁdence region captures the
1) and that
maximum privacy violation. The methods
for deriving
LB are borrowed from Section IV-B and are performed
independently from the ﬁrst part of the algorithm. MPL
N ∼ A(xmax) and
∗
creates two fresh samples X
N ∼ A(x
(cid:2)
∗
∗
max) with sample size N > n. These
1 , ..., Y
Y
max (ˆtmax) by its
are used to approximate the loss (cid:8)xmax,x(cid:2)
∗
empirical version ˆ(cid:8)
(ˆtmax). The density estimators
xmax,x(cid:2)
∗
∗
x , ˆf
ˆf
x(cid:2) underlying this empirical loss function are constructed
with parameters hmax and τ tailored to the construction of
conﬁdence intervals. This choice is expressed in the following
condition:
(C4) Let ν ≥ 0. With N = O(n1+ν) and γ > ν/((1 + ν)6)
∗
1 , ..., X
max
we choose hmax = O(N
− 1
2β+d−γ) and τ = o(1).
As already indicated in Section IV-B, bandwidths for conﬁ-
dence intervals have to be chosen smaller than for estimation
(realized by γ > 0). The trade-off between γ and ν expresses
that in the second part of the MPL algorithm, a larger sample
size N compared to n requires more undersmoothing to
control the bias. Yet, as ν is usually small in practice (in
our experiments about 0.1), the undersmoothing requirement
is rather weak. The fact that τ can decay at any rate shows
that ˆtmax (selected by truncated estimators in the ﬁrst step)
locates automatically in regions where the densities are not too
close to 0 and thus a second truncation by τ is not important.
In applications, one could simply put τ = 0 in this step.
give a conﬁdence interval [LB,∞) for xmax,x(cid:2)
the statistical lower bound LB is deﬁned as follows:
Recalling Section IV-B and particularly (25), we can now
max,C, where
−1(α)ˆσN
Φ
∗
LB := ˆ(cid:8)
xmax,x(cid:2)
.
max
cN
(ˆtmax) +
(26)
−1 is, again, the quantile function of the standard
Here Φ
normal distribution and 1 − α is the conﬁdence level. The
normalizing constants cN and ˆσN are described in Section
IV-B. The following theorem validates theoretically the lower
bound LB produced by the MPL algorithm.
Theorem 2. Suppose that A is either a discrete algorithm
and condition (D) is satisﬁed, or a continuous one such that
conditions (C1)-(C4) are satisﬁed with regard to A and the
MPL algorithm.
i) If

∗
C := max(x1,x(cid:2)
1,C, ..., xB ,x(cid:2)
B ,C) ∈ (0,∞)
it holds that
(cid:9)
(cid:11)
∗
C
C = ∞, then LB →P ∞. If 
∗
LB ≤ 
lim
n→∞ P
= 1 − α.
(27)
C = 0, then LB →P 0.
∗
ii) If 
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:21:54 UTC from IEEE Xplore.  Restrictions apply. 
411
The proof of the theorem is technical and therefore deferred
to the Appendix.
Algorithm 4 Maximum Privacy Loss
Input: set of data pairs X , sample sizes n and N, region
of investigation C, speciﬁcation variable discr, level α
b
b
max
(ˆtxb,x(cid:2)
b
for b = 1, . . . , B do
1 , ..., X∗
1 , ..., Y ∗
N ) with X∗
N ) with Y ∗
b, n, C, discr)
, ˆxb,x(cid:2)
end for
b) ∈ X}
max) ∈ arg max{ˆxb,x(cid:2)
: (xb, x(cid:2)
Set (xmax, x(cid:2)
Set ˆtmax := ˆtxmax,x(cid:2)
i ∼ A(xmax)
Generate X∗
= (X∗
i ∼ A(x(cid:2)
Generate Y ∗
= (Y ∗
max)
Choose τ in accordance with (D) if discr = 1
Choose hmax, τ in accordance with (C4) if discr = 0
Choose appropriate kernel K
if discr = 1 then
Output: Statistical lower bound for privacy violation LB
1: function MPL(X , n, N, C, discr, α)
2:
) = DPL(xb, x(cid:2)
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
xmax (ˆtmax) = TDDE(X∗, ˆtmax, τ )
ˆf∗
13:
ˆf∗
(ˆtmax) = TDDE(Y ∗, ˆtmax, τ )
x(cid:2)
14:
15:
xmax (ˆtmax) = TKDE(X∗, ˆtmax, hmax, K, τ )
ˆf∗
16:
(ˆtmax) = TKDE(Y ∗, ˆtmax, hmax, K, τ )
ˆf∗
x(cid:2)
17:
end if
18:
(ˆtmax)=| ln( ˆf∗
ˆ(cid:4)∗
xmax,x(cid:2)
19:
N and cN based on X∗, Y ∗
Calculate ˆσ2
20:
Deﬁne LB := ˆ(cid:4)∗
21:
return LB
22:
23: end function
xmax (ˆtmax))− ln( ˆf∗
x(cid:2)
(ˆtmax) + Φ−1(α)ˆσN
and discr
xmax,x(cid:2)
else
max
max
max
cN
max
(ˆtmax))|
max
We conclude this section by discussing the limitations of
our statistical methods with an example taken from [19].
Example 2. Suppose we have an algorithm A that checks
whether a given database x matches a target database x0.
More precisely, we have A(x) = 0 for any x (cid:3)= x0 and
−k and A(x0) = 0 with probabil-
A(x0) = 1 with probability e
ity 1− e
−k. One can easily conﬁrm that A is not differentially
private. However, for large k, a sampling based method such
as ours could falsely identify A as a constant function which
trivially satisﬁes DP. And while A is actually (, δ) − DP for
−k and comes close to perfect 0 − DP ,
 = 0 and δ = e
this would still amount to a misclassiﬁcation of A. In fact, A
reﬂects the fundamental limitations of any black box scenario
where we are forced to rely solely on algorithm outputs. In
order to reliably detect such intricate pathologies, one might
have to ultimately access the algorithm’s source code. Here,
formal veriﬁcation tools (referenced in the Introduction) might
be more suitable.
V. EXPERIMENTS
In this section, we analyze the performance of our method-
ology by applying it
to some standard algorithms in DP
validation. We focus mainly on inference for the global privacy
parameter , but a subsection concerning the data-centric
privacy level x is included as well.
Our method is implemented in R and for kernel density
estimation we use the “kdensity” package, which also provides
automatic bandwidth selection. In the following, we give a
short outline of the algorithms and experiment settings before
discussing our empirical ﬁndings.
Query model: We brieﬂy discuss the query model used in
[19]. Many discrete algorithms do not operate on databases
x directly, but instead process query outputs q(x). Thus, the
search and selection of databases x = (x(1),··· , x(m))
translates into a choice of query outputs
q = (q1,··· , qd) = (q1(x),··· , qd(x)).
Here counting queries, which check how many data points
x(i) in x satisfy a given property, are of particular interest.
A change in a single data point can affect the output of
each counting query by at most 1. Hence, query answers
on neighboring databases are captured by vectors of natural
(cid:2)
i can differ by at most 1. Simple
numbers q, q
query answers that are created following patterns displayed in
Table I are sufﬁcient to deduce the privacy parameter [19] and
we will draw on vectors resembling these to evaluate discrete
algorithms.
(cid:2) where qi and q
Pattern
One Above
One Below
One Above Rest Below
One Below Rest Above
Half Half
All Above All Below
X Shape
Query q
(1, 1, 1, 1, 1, 1)
(1, 1, 1, 1, 1, 1)
(1, 1, 1, 1, 1, 1)
(1, 1, 1, 1, 1, 1)
(1, 1, 1, 1, 1, 1)
(1, 1, 1, 1, 1, 1)
(1, 1, 1, 0, 0, 0)
Query q(cid:2)
(2, 1, 1, 1, 1, 1)
(0, 1, 1, 1, 1, 1)
(2, 0, 0, 0, 0, 0)
(0, 2, 2, 2, 2, 2)
(0, 0, 0, 1, 1, 1)
(2, 2, 2, 2, 2, 2)
(0, 0, 0, 1, 1, 1)
TABLE I: Input patterns used in [19]
(cid:2)
(cid:2)
Similar to the discrete case, continuous algorithms are
usually applied to aggregate statistics S of the data and not to
the raw data itself. We therefore consider algorithmic inputs
), that lie in a continuous
of the form s = S(x) and s
domain (in the following examples intervals and cubes).
= S(x
Algorithms: We test our approach on 8 algorithms in total.
The well known Laplace Mechanism (see [1]) publishes a
privatized version of a real valued statistic s ∈ [0, 1] by adding
centered Laplace noise L ∼ Lap( 1
 ). This mechanism is used
as a subroutine in many differentially private algorithms (e.g.
the versions of Noisy Max discussed here). In the following,
(cid:2)
we consider as input statistics sb = 0 and s
b = b/10 for
b = 1, ..., 10. The set C in MPL is chosen as the symmetric
interval [−1, 1].
The Report Noisy Max algorithm [32] publishes the query
with the largest value within a vector of noisy query answers.
More precisely, the index arg max{qi + Li : 1 ≤ i ≤ d} with
Li ∼ Lap( 2
 ) is calculated and returned (see [19], Algorithm
5). We implement Report Noisy Max and our procedure on
vectors that entail 6 query answers and choose databases qb
(cid:2)
and q
b, b = 1, ..., 10, that are similar to the patterns described
in Table I.
Given a query vector q and a threshold T , the Sparse Vector
Technique (SVT) goes through each query answer qi and
reports whether said query lies above or below T [32]. The
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:21:54 UTC from IEEE Xplore.  Restrictions apply. 
412
maximum number of positive responses M is an adjustable
feature of the algorithm that forces it to abort after M query
answers above T have been reported. We investigate 4 versions
of SVT taken from [33], which are, in accordance with the
denotation in [33] and [21], variants SVT2 and SVT4-SVT6.
b, b = 1,··· , 10, with 10
(cid:2)
We consider query vectors qb and q
entries that are similar to the patterns in Table I. This choice
resembles the one in prior work (see [19], [21]) and we do
the same for the tuning parameters with T = 1 and M = 1
[21].
The continuous Noisy Max algorithm (see Algorithm 7,
[19]) has been discussed in Example 1. Here we use it to
publish the maximum entry of a statistic s ∈ [0, 1]k. We
consider the case k = 3 and input statistics sb = (0, 0, 0)
(cid:2)
b = (b/10, b/10, b/10) for b = 1, ..., 10. Furthermore,
and s
we choose C = [−1, 1].
The Exponential Mechanism provides a general principle
for the construction of private algorithms. We consider a
version where we privatize real numbers from the interval
[1, 2], with non-negative outputs. More precisely, for a number
s ∈ [1, 2] the output is sampled according to a continuous
to exp(−λ|s − t|) for t ≥ 0. Here
density proportional
λ > 0 is a parameter determining the privacy level. Recall that
this setup ﬁts our (relaxed) notion of continuous algorithms
discussed in Section III (continuous density on the half-line).
It is well known that using this construction, the exponential
mechanism affords (at least) 2λ-DP. We can however employ
Theorem 1 to derive the privacy parameter  precisely:
 = λ + ln(2 − exp(−2λ)) − ln(2 − exp(−λ)).
Notice that  ≈ 2λ for small λ. In the following simulations,
(cid:2)
b = 1 + b/10 for
we consider input statistics sb = 1 and s
b = 1, ..., 10 and choose C = [0, 2].
Experiment settings: To study privacy violations, we em-
ploy the MPL algorithm described in Section IV-C. The
sample sizes and ﬂoor in MPL are chosen as n = 2 × 104,
N = 5 × 104 and τ = 10
−3 for algorithms (a)-(d) (labels as
in Figure 4), i.e. all algortithms apart from the SVTs. For
the SVTs we use larger sample sizes and a smaller ﬂoor
with n = 105, N = 5 × 105 and τ = 10
−4. This choice
of parameters is necessary as SVTs allow for extreme events
(with low probability) that otherwise cause instabilities.
For the continuous algorithms, the kernel in KDE is the
Gaussian Kernel (described in Appendix B) and the band-
widths in the ﬁrst step of MPL are chosen by a pre-
implemented selection rule in the “kdensity” package (both
are the default options).
We examine each algorithm for different targeted privacy
parameters 0 ∈ {0.2, 0.7, 1.5}, capturing the high, middle and
low privacy regime respectively [19] (we adjust the targeted
privacy level, e.g. by tuning the Laplace noise or changing λ
in the Exponential Mechanism). Correctly designed algorithms
meet their targeted privacy levels, i.e.  = 0. Algorithms (a)
- (f) fall into this category, with labels again as in Figure
4. Notice that (f) is sometimes deemed “incorrect” in the
literature [19], as in its original design  is only equal to the
targeted level 0 up to a constant (this simple scaling error
has been corrected in our version). Algorithms (g) and (h)
constitute incorrect algorithms that do not satisfy DP at all,
i.e.  = ∞ [33]. Recalling (4), this especially points to privacy
violations x,x(cid:2) that exceed the targeted privacy parameter 0.
Results: In order to evaluate MPL, we consider the cu-
mulative distribution function (cdf) of the lower bound LB
deﬁned in (26). Recall that the cdf is deﬁned for some z ∈ R
as P(LB ≤ z). In Figure 4 we display a panel where each
plot corresponds to one algorithm under investigation and
each curve to the empirical cdf for a different choice of 
(each based on 1000 simulation runs). This presentation is
related to, but more informative than, a standard histogram
and for details on the empirical cdf we refer to [37]. It is
also particularly transparent, as we report the results of 1000