title:Impact of GPUs Parallelism Management on Safety-Critical and HPC
Applications Reliability
author:Paolo Rech and
La&apos;ercio Lima Pilla and
Philippe Olivier Alexandre Navaux and
Luigi Carro
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Impact of GPUs Parallelism Management on 
Safety-Critical and HPC Applications Reliability 
P. Rech, L. L. Pilla, P. O. A. Navaux, and L. Carro 
Instituto de Informática, Universidade Federal do Rio Grande do Sul 
{prech, llpilla, navaux, carro}@inf.ufrgs.br 
Abstract—Graphics  Processing  Units 
(GPUs)  offer  high 
computational  power  but  require  high  scheduling  strain  to 
manage  parallel  processes,  which  increases  the  GPU  cross 
section. The results of extensive neutron radiation experiments 
performed  on  NVIDIA  GPUs  confirm 
this  hypothesis. 
Reducing the application Degree Of Parallelism (DOP) reduces 
the  scheduling  strain  but  also  modifies  the  GPU  parallelism 
management,  including  memory  latency,  thread  registers 
number,  and  the  processors  occupancy,  which  influence  the 
sensitivity  of  the  parallel  application.  An  analysis  on  the 
overall GPU radiation sensitivity dependence on the code DOP 
is  provided  and 
is 
experimentally  detected.  Finally,  modifying 
the  parallel 
management  affects  the  GPU  cross  section  but  also  the  code 
execution time and, thus, the exposure to radiation required to 
complete  computation.  The  Mean  Workload  and  Executions 
Between  Failures  metrics  are  introduced  to  evaluate  the 
workload or the number of executions computed correctly by 
the GPU on a realistic application. 
the  most  reliable  configuration 
Keywords-GPGPUs; 
algorithms; 
reliability; 
radiation; 
parallel 
I. 
 INTRODUCTION 
Graphics Processing Units (GPUs) are electronic devices 
designed  to  perform  high-performance  stream  processing 
and provide very high computational power combined with 
low  cost, 
flexible 
development platforms. 
reduced  power  consumption,  and 
In  order  to  achieve  the  proposed  objective,  GPUs 
manipulate  a  large  number  of  memory  locations,  and  are 
typically able to execute several elementary tasks in parallel 
at high speeds [1][2]. Due to their highly parallel structure, 
GPUs  are  more  effective  than  general-purpose  CPUs  when 
large blocks of data need to be processed in parallel. GPUs 
have  then  recently  become  popular  not  only  for  graphical 
applications,  but  also  in  the  High  Performance  Computing 
(HPC)  market  [3][4].  Moreover,  in  some  safety-critical 
applications,  such  as  automotive,  avionics,  space  and 
biomedical,  GPUs  would  be  very  suitable.  As  an  example, 
the Advanced Driver Assistance Systems (ADAS), which are 
increasingly  common  in  cars,  make  an  extensive  usage  of 
images (or radar signals) coming from external cameras and 
sensors  to  detect  possible  obstacles,  triggering  the  breaks 
automatically  if  necessary.  Starting  in  2015,  only  vehicles 
equipped with ADAS will be eligible to receive the highest 
security  level  from  Euro-NCAP  [5],  one  of  the  most 
authoritative  car  evaluation  agencies  in  Europe.  A  Low-
power  System  on  Chip  including  a  GPU  core,  like  the 
NVIDIA  Tegra,  is  likely  to  be  the  computational  core  of 
ADAS.  Airbus is finalizing the  ARAMIS project,  aimed at 
integrating  of  all  the  electronics  required  to  implement  the 
collision  avoidance  system  into  a  single  board  including  a 
GPUs  core  [6].  Unfortunately,  the  European  Aviation 
Security  Agency  (EASA)  does  not  accept  multicores  chips 
with more than 2 cores on an aircraft, yet. The main reason 
for  such  a  limitation  on  parallelism  from  EASA  is  that  a 
standardize  reliability  evaluation  protocol  has  not  yet  been 
developed.  Our  paper  moves  on 
the  direction  of 
understanding the reliability of GPUs, giving novel insights 
on their behaviors when exposed to ionizing radiation. 
is  compliant  with 
In  both  application  scenarios  (HPC  and  safety-critical 
embedded applications), GPUs reliability is a major concern. 
As 
the  newest  GPUs  are  built  with  cutting-edge 
technologies, offer a great amount of resources, and operate 
at  extremely  high  frequencies,  they  may  be  particularly 
susceptible to experience radiation-induced errors, including 
those  originating  from  the  terrestrial  neutron  radiation 
environment  [7][8].  On  safety-critical  applications,  the 
reliability qualification of GPUs is essential to evaluate if the 
device 
the  project  specifications. 
Hardening  techniques  like  Error  Correction  Codes  (ECC), 
duplication  with  comparison,  triplication,  or  Algorithm 
Based Fault Tolerance [9][10] could eventually be applied if 
the  error  rate  of  GPUs  is  found  to  exceed  the  reliability 
requirement. Supercomputers are composed of thousands of 
devices  that  work  in  parallel  and,  thus,  the  probability  of 
having at least one radiation-induced corruption is very high. 
Hardening  strategies  become  mandatory  even  for  HPC 
application  with 
the 
introduction  of  useless  overhead.  Evaluating  precisely  the 
radiation-induced error rate of a code executed on a GPU is 
then of extreme importance as it allows to evaluate the trade-
off  between  the  hardening  strategy  detection/correction 
capabilities and the introduced computational overhead.  
the  specific  constraint 
to  avoid 
An  intense  research  discussion  on  GPUs  radiation 
sensitivity  has  recently  started  [11],  focusing  on 
the 
probability  of  caches  and  registers  failures,  tracking  errors 
propagation  to  the  output  [12-14]  as  well  as  devising 
software  and  architectural techniques to harden  GPU-based 
systems [15]. Most of the research done on GPU reliability is 
based  on  fault-injection  simulations  [16-18],  on  field  tests 
[19],  or  radiation  experiments  [20][21].  Experimental  data 
presented  in  the  later  highlights  that  the  corruption  of 
resources  shared  among  parallel  threads  like  caches  or 
critical  resources,  such  as  the  scheduler,  may  reduce  the 
GPU  reliability  and  generate  a  large  number  of  multiple 
errors in the output. 
This paper moves a step further in the analysis of GPUs 
radiation  sensitivity  by  measuring  and  analyzing 
the 
reliability of their peculiar parallelism management. Such a 
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
DOI 10.1109/DSN.2014.49
455
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:48:15 UTC from IEEE Xplore.  Restrictions apply. 
 Figure 1: Simplified structure of NVIDIA GPUs, composed of an array 
of Streaming Multiprocessors that share  L2 cache.  A  block scheduler 
and dispatcher assigns one or more blocks of threads to idle SMs. 
measure  can  only  be  performed  with  the  sake  of  radiation 
experiments.  Simulations  give  better  visibility  and 
controllability than radiation tests, but can hardly be applied 
to  parallelism  management  reliability  studies  as 
the 
sensitivity of schedulers cannot be analytically predicted and 
failures  are  not  easily  injectable  in  the  scheduler.  The 
relationship  between  the  GPU  cross  section  and  thread 
distribution  as  well  as 
the  relationship  between  an 
application’s  workload  and  the  radiation-induced  output 
error  rate  are  experimentally  measured.  Such  relationships 
are essential to evaluate how an increased number of parallel 
processes, which brings higher throughput to the application, 
affects the overall device output error rate. As the  reported 
experimental  results  obtained  irradiating  modern  NVIDIA 
devices demonstrate, a higher scheduler strain may increase 
the radiation sensitivity of GPUs. 
A way to reduce the scheduler strain, and thus the GPU 
radiation  sensitivity,  is  to  reduce  the  code  Degree  Of 
Parallelism (DOP), increasing the workload each thread has 
to  complete.  Nevertheless,  variations  in  the  DOP  impose 
modifications  in  the  GPU  parallel  management  that  affect 
other  resources  beside  the  dispatcher  like  memory  access 
latency,  the  number  of  registers  per  thread,  the  processors 
occupancy, etc. Modifications in these resources  also affect 
the  GPU  cross  section,  and  may  undermine  the  reliability 
benefits that a lower scheduler strain brings. 
An analysis of the overall resources modification effects 
on  reliability 
is  proposed  and  proved  experimentally 
measuring the radiation response of two synthetic benchmark 
codes  composed  only  of  sums  and  multiplications  and  two 
typical applications for GPU (Matrix Multiplication and Fast 
Fourier Transform). Given a fully parallel code, we gradually 
reduce its DOP, either by changing the number of threads per 
block  or  the  number  of  thread  blocks,  and  increasing 
concurrently  the  operations  each  thread  is  in  charge  of 
executing,  in  such  a  way  to  maintain  the  algorithm 
throughput constant. Although reducing the DOP of the two 
benchmarks  in  general  reduces  their  cross  sections  under 
radiation, 
the  Matrix 
Multiplication  and  FFT  when  the  number  of  blocks  is 
reduced, regardless of the lower scheduling strain. 
Finally,  the  DOP  affects  not  only  the  sensitivity  to 
radiation of the code executed on the GPU but also the time 
increases 
cross 
section 
for 
 Figure  2:  Simplified  architecture  of  a  CUDA  Streaming 
Multiprocessor. The assigned block of threads is divided into warps 
assigned  to  CUDA  cores  by  two  schedulers.  Each  thread  will 
dispose of shared memory, L1 cache, and dedicated registers. 
then 
required  to  complete  computation.  The  probability  of 
reaching  a  correct  solution  depends  on  both.  Two  new 
metrics  are 
introduced,  named  Mean  Workload 
Between  Failures  (MWBF)  and  Mean  Executions  Between 
Failures  (MEBF),  which  identify  the  workload  and  the 
number of executions that can be correctly computed by the 
GPU,  respectively.  Such  metrics  are  of  particular  interest 
both in HPC and safety critical applications as they provide 
the realistic impact of  a  given parallel configuration  on the 
code  reliability  taking  both  the  cross  section  and  execution 
time  into  account.  As  demonstrated  in  this  paper,  MWBF 
and  MEBF  detect  the  parallel  configuration  that  offers  the 
highest  reliability,  which  is  not  necessarily  the  one  with 
lower cross section. Such a configuration may decrease the 
GPU  error  rate,  possibly  relaxing  the  hardening  strategy 
detection and correction constraints. 
tested  devices  and 
The  remainder  of  the  paper  is  organized  as  follows.  In 
Section  II  the  internal  structure  of  a  GPU  is  detailed,  with 
particular  emphasis  on  the  parallel  processes  management. 
The 
the  adopted  experimental 
methodology are shown in Section III. Experimental results 
on the GPU’s parallel management reliability are presented 
in  Section  IV.  The  introduction  of  MEBF  and  MWBF 
metrics  and  a  discussion  on  the  reliability  of  the  parallel 
codes taking the different execution and exposure time into 
account take also place in this section. Section V discusses 
how the cross section and MEBF are affected by the Degree 
of Parallelism of the GPU codes, while Section VI concludes 
the paper and presents future work. 
II.  GPU PARALLEL MANAGEMENT 
Modern GPUs are divided into various computing units, 
named Streaming Multiprocessors (SM), each of which has 
the  ability  to  executing several threads in  parallel (see  Fig. 
1).  Each  basic  computing  unit  (named  CUDA  core  in 
NVIDIA  devices)  in  the  SM  executes  one  thread  with 
dedicated registers, avoiding complex resource sharing or the 
need of long pipelines [2] (see Fig. 2). From a radiation test 
point  of  view,  the  computing  units  are  isolated  such  that  a 
single  radiation-induced  event  in  one  computing  unit  will 
only corrupt the thread assigned to it. Threads that follow the 
corrupted  one  or  threads  assigned  to  computing  units  near 
456
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:48:15 UTC from IEEE Xplore.  Restrictions apply. 
the struck one will not be affected. 
It  is  the  programmer’s  task  to  divide  the  instantiated 
threads into a grid of blocks when designing a kernel to be 
executed  on  a  GPU.  It  is  easy  to  modify  the  thread 
distribution,  as  the  block  size  and  the  grid  size  are  both 
parameters  that  have  to  be  specified  when  launching  a 
CUDA kernel to be executed on a GPU. 
The  number  of  blocks  assigned 
to  a  Streaming 
Multiprocessor  in  the  GPU  will  depend  on  the  number  of 
registers, on the amount of shared memory available in the 
SM,  and  on  the  resources  required  by  each  block  to  be 
executed.  On  GPUs  built  with  the  Fermi  architecture,  like 
the ones used in the presented study, the number of blocks 
assigned to a SM cannot exceed 8 while in Kepler devices up 
to 16 blocks can be assigned to a SM. 
Some blocks will be queued for later computation if the 
grid  size  exceeds  the  number  of  blocks  that  can  be 
dispatched  among  the  SMs  available  in  the  GPU.  Before 
dispatching  a  queued  block  to  the  first  SM  that  becomes 
available, the GPU’s block scheduler needs to check if some 
SM  completed  the  current  block  execution  and,  if  so,  it 
transfers  the  results  to  the  on-board  DDR  memories.  The 
queued  block  is  then  assigned  to  the  SM,  the  input  data  is 
eventually read from the DDR, and, finally, the queued block 
execution is triggered and synchronized [22]. 
The  tested  devices  have  CUDA  capability  2.0,  which 
allows  each  SM  to  execute  a  warp  of  up  to  32  parallel 
threads in a single computing cycle. If the block size exceeds 
32, the execution of some threads will be delayed until the 
computation  of  the  preceding  warps  of  the  block  has  been 
completed. It is worth noting that the next block to be treated 
will  be  assigned  to  the  SM  only  when  all  threads  in  the 
current block have been processed. Therefore, if the number 
of threads in a block is not a multiple of 32, in the last cycle 
the  SM  will  execute  less  than  32  threads,  wasting  parallel 
capabilities.  Devices  with  CUDA  capabilities  3.5  can 
execute  up  to  192  parallel  threads  in  a  single  computing 
cycle. The trend followed by NVIDIA is then to increase the 
parallel  capabilities  of  the  SM  more  than  increasing  the 
number of SM available in the GPU. As shown in the paper, 
under radiation such a decision seems to be more reliable. 
Each  SM  disposes  of  two  schedulers  (see  Fig.  2).  At 
every  instruction  issue  time,  the  first  scheduler  issues  one 
instruction  for  some  warp  with  an  odd  ID  and  the  second 
scheduler issues  one instruction for those  with  an even ID. 
When double-precision floating-point instructions have to be 
executed, like in the codes analyzed in this paper, the second 
scheduler cannot issue any instruction. 
threads  brings 
the  amount  of 
A  parallel  code  to  be  executed  on  a  GPU  is  typically 
composed  of  several  independent  threads,  all  executing  the 
same  set  of  instructions  on  dedicated  memory  location. 
Increasing 
then  higher 
throughput to the application. To do so, the programmer can 
choose  either to increase the block size,  which will require 
more  computational  effort  in  each  SM  and  delay  the 
assignment  of  the  next  blocks,  or  to  increase  the  grid  size, 
thus having more blocks to be dispatched. The GPU parallel 
management 
thread 
distribution. The scheduling and computational load required 
is  strictly  related 
the  chosen 
to 
for  blocks  and  warps  assignment,  as  well  as  resources 
distribution, are strictly related to the chosen grid and block 
sizes,  which  is  then  likely  to  influence  also  the  GPU 
radiation response. 
The  purpose  of  this  work  is  actually  to  evaluate  and 
analyze  the  effects  of  different  thread  distributions  on  the 
GPU  parallel  management  and  the  consequent  variation  on 
the device cross section. Such an evaluation will detect the 
distribution, in terms of grid size and block size, which offers 
lower  cross  section  and  higher  probability  of  completing 
computation  correctly.  GPU  parallelism  management 
reliability  will be  further evaluated  reducing the  number of 
threads  available  while  increasing  the  workload  of  each 
thread.  Having  less  warps  or  blocks  instantiated  lower  the 
dispatcher  load,  which  is  likely  to  reduce  the  GPU  cross 
section, but the recourses distribution, caches requirements, 
memory  access  latencies  will  be  affected  by  the  changed 
threads  complexity,  with  non-obvious  effects  on 
the 
radiation-induced error rate. 
III.  EXPERIMENTAL METHODOLOGY 
A.  Tested Devices 
The  proposed  analysis  is  based  on  commercial-off-the-
shelves GeForce GTX480 and Tesla C2050 GPUs designed 
by  NVIDIA  in  a  40nm  technology  node.  The  GTX480  is 
divided into 15 SMs, while the C2050 into 14 SMs. Both the 
GTX480  and  C2050  exploit  CUDA  2.0  capabilities, 
disposing of 32 CUDA cores in each SM.  
For  the  GTX480  GPU,  15  blocks  of  threads  can  be 
executed in parallel with a maximum of 32 threads in each 