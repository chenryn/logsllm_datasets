### 3. Efficiency: Proportion of True Positives Among Reported Scanners

Efficiency is defined as the proportion of reported scanners by the detector that are true positives:

\[
\text{Efficiency} = \frac{\text{Number of True Positives}}{\text{Number of True Positives} + \text{Number of False Positives}}
\]

For any intrusion detector, if the number of true negative samples significantly exceeds the number of true positive samples, the false positive (FP) rate is expected to be small, regardless of the detector's performance. Therefore, calculating efficiency (or the false discovery rate, which is \(1 - \text{efficiency}\)) provides a more meaningful performance metric than the FP rate.

Conversely, if the number of true positive samples significantly exceeds the number of true negative samples, the true positive (TP) rate is expected to be large, regardless of the detector's performance. In this case, calculating the false omission rate (i.e., \(\frac{\text{Number of False Negatives}}{\text{Number of False Negatives} + \text{Number of True Negatives}}\)) is a more meaningful performance metric than the TP rate. Based on the datasets studied in the literature (e.g., [3]) and our datasets, network scanning activity often falls into the former category.

### Table 3: Distribution of TRW and LQS Detected Scanners

| Category       | Dataset I | Dataset II |
|----------------|-----------|------------|
| TRW            | 416       | 105        |
| LQS            | 480       | 583        |

None of the remotes in the benign or likely benign categories were marked as scanners by either algorithm. Additionally, remotes that made only one failed connection attempt were not flagged as scanners, as both algorithms require more than one connection attempt for any given remote host. False positives in both algorithms appeared only in the unknown (others) category of the reference baseline.

### Table 4: Performance Evaluation

| Metric         | Dataset I | Dataset II |
|----------------|-----------|------------|
| **TRW**        |           |            |
| TP rate        | 0.7143    | 0.8227     |
| FP rate        | 0.0002    | 0.0003     |
| Efficiency     | 0.9976    | 0.9958     |
| **LQS**        |           |            |
| TP rate        | 0.9227    | 0.9787     |
| FP rate        | 0.0002    | 0.0003     |
| Efficiency     | 0.9971    | 0.9946     |

### Performance Metrics

- **RB1**: Remote hosts in both the scanner and the likely scanner categories are considered true positives, and the remainder (benign, likely benign, and unknown) are true negatives.
- **RB2**: True positives are only those in the scanner category, and true negatives are those in the benign, likely benign, and unknown categories. This is a more relaxed reference baseline where detected scanners from the likely scanner category are ignored.

#### Dataset I and RB1
- LQS demonstrated a better TP rate than TRW by more than 15%.
- The FP rate in both algorithms is very low due to the significantly large number of samples relative to the number of true positives.
- Detection efficiency is high in both algorithms, with less than 1% of detected scanners being false positives.

#### Dataset I and RB2
- TP rate improved in both algorithms, with LQS being better by only 6%.
- Efficiency remains high in both algorithms.
- Both algorithms achieved good performance in detecting entries in the scanner category, representing remotes that exhibit significant scanning behavior rather than normal behavior (as discussed in Section 2.2).

#### Dataset II and RB1
- TRW detected 13% of scanners in the second dataset.
- LQS has a detection rate of 76% while maintaining a slightly smaller (better) FP rate and higher efficiency (approximately 5% better than TRW).

#### Dataset II and RB2
- Even with RB2, the TRW detection rate (TP rate) is only 31%.
- LQS performed better in both detection rate (90%) and efficiency (0.99 vs. 0.95).

In the second dataset, many scanners initiated few connection attempts and had a low scanning rate, contributing to the poor performance of TRW. This reflects the current trend of stealthy probing by scanners, perhaps due to the large number of IP addresses (e.g., infected hosts) involved in some coordinated scanning campaigns. For example, instead of the conventional aggressive scanning behavior of many typical worms, stealthy scanning activity is now more common (e.g., by stealthy worms and bots [14, 15]).

### 6. Related Work

- **Network Security Monitor (NSM) IDS [7]**: Examines destination IP addresses contacted by a remote host, labeling it as anomalous if it contacts more than 15 local hosts within an unspecified time window or attempts a connection to a non-existing host.
- **GrIDS [23]**: Graphically shows remote activities and connectivity over time, indicating possible scan activity if a graph shows one remote contacting many local hosts.
- **Kato et al. [11]**: Proposed a real-time IDS for detecting network attacks by setting a threshold for the number of TCP ACK/RST packets returned to the same remote within a specified time window.
- **Leckie and Kotagiri [13]**: Used a probabilistic model considering the number of local hosts or ports accessed by a remote and how unusual these accesses are, requiring sufficient knowledge of the monitored network and dynamic updates.
- **Spice [22]**: A port scan detector for stealthy scans using a statistical model, where packets sent to rarely accessed IP address/port combinations are considered more anomalous.
- **Robertson et al. [18]**: Gave each remote host a score based on the number of unique destination IP/port pairs of failed connection attempts, classifying a remote host as a scanner if its score exceeds an empirically derived alert threshold.
- **Kim et al. [12]**: Calculated a normal distribution of destination IP addresses/port pairs in a network and used various statistical tests to analyze traffic rates to detect port scans.
- **sfPortscan [19] in Snort [20]**: Generates an alert when a remote host attempts to connect to more than a predefined threshold of local hosts or ports within a predefined time window.
- **TRW [9]**: Implemented as a Bro policy, allowing scanners' traffic to be dropped by setting the appropriate interface between Bro and the corresponding network router.
- **Weaver et al. [24]**: Proposed a simplified variant of TRW with less memory footprint and the ability to detect vertical scanning, along with a suppression algorithm for worm containment with dynamically adjustable thresholds.
- **Nagaonkar [16]**: Extended TRW to detect UDP and ICMP scans and used a Bloom filter to process unique source and destination IP addresses, destination ports, and protocols.

### 7. Concluding Remarks

Network scanning remains a useful reconnaissance activity for attackers. Given the high ability of compromised machines in today's Internet, highly distributed scanning, specifically to achieve stealthiness, is recognized as a feasible and practical strategy to avoid triggering IDSs. Post-detection responses to network scanning often require fast and accurate detection.

LQS specifically addresses these issues as a real-time network scanning detector that quickly detects stealthy scanners, achieving high detection rates and very low false positive rates compared to the TRW algorithm. Moreover, LQS requires a smaller memory footprint and has a higher immunity to evasion. We also presented a novel methodology to obtain an estimated ground truth for evaluating network scanning detectors.

### 8. Acknowledgment

We thank J. Jung, anonymous reviewers, and members of the Carleton Computer Security Lab (CCSL) for helpful comments. The second author is Canada Research Chair in Authentication and Computer Security and acknowledges NSERC for funding the chair and a Discovery Grant. Partial funding from NSERC ISSNet is also acknowledged.

### 9. References

[1] Bro intrusion detection system. Accessed: May 2010. http://bro-ids.org/.

[2] Ethereal display filter reference. Accessed: Aug 2010. http://www.ethereal.com/docs/dfref/.

[3] M. Allman, V. Paxson, and J. Terrell. A brief history of scanning. In Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement, pages 77–82, 2007.

[4] CERT. Advanced scanning. CERT Incident Note IN-98.04 (Sept. 29 1998). http://www.cert.org/incident_notes/IN-98.04.html.

[5] M. Fullmer and S. Romig. The OSU Flow-tools package and Cisco Netflow logs. In Proceedings of the 14th Systems Administration Conference (LISA’00), pages 291–303, New Orleans, LA, USA, 2000. USENIX Association.

[6] C. Gates. Coordinated scan detection. In Proceedings of the 16th Annual Network and Distributed System Security Symposium (NDSS’09), February 2009.

[7] L. T. Heberlein, G. V. Dias, K. N. Levitt, B. Mukherjee, J. Wood, and D. Wolber. A network security monitor. IEEE Symposium on Security and Privacy, pages 296 – 304, 1990.

[8] J. Jung, R. A. Milito, and V. Paxson. On the adaptive real-time detection of fast-propagating network worms. In Proceedings of the 4th International Conference on Detection of Intrusions and Malware, and Vulnerability Assessment (DIMVA’07), pages 175–192, Lucerne, Switzerland, 2007. Springer-Verlag.

[9] J. Jung, V. Paxson, A. W. Berger, and H. Balakrishnan. Fast portscan detection using sequential hypothesis testing. In IEEE Symposium on Security and Privacy, May 2004.

[10] M. G. Kang, J. Caballero, and D. Song. Distributed evasive scan techniques and countermeasures. In Tracking darkports for network defense. In Proceedings of ACSAC, pages 161–171, 2007.

[11] N. Kato, H. Nitou, K. Ohta, G. Mansfield, and Y. Nemoto. A real-time intrusion detection system (IDS) for large scale networks and its evaluations. IEICE Transactions on Communications, E82-B(11):1817–1825, 1999.

[12] H. Kim, S. Kim, M. A. Kouritzin, and W. Sun. Detecting network portscans through anomaly detection. In Proceedings of Signal Processing, Sensor Fusion, and Target Recognition XIII, pages 254 – 263, 2004.

[13] C. Leckie and R. Kotagiri. A probabilistic approach to detecting network scans. In Proceedings of the Eighth IEEE Network Operations and Management Symposium (NOMS’02), pages 359–372, 2002.

[14] Z. Li, A. Goyal, and Y. Chen. Honeynet-based botnet scan traffic analysis. In Botnet Detection, pages 25–44. Springer US, 2008.

[15] Z. Li, A. Goyal, Y. Chen, and V. Paxson. Automating analysis of large-scale botnet probing events. In ASIACCS, pages 11–22, 2009.

[16] V. Nagaonkar. Detecting stealthy scans and scanning patterns using threshold random walk. Master’s thesis, Dalhousie University, Canada, 2008.

[17] J.-P. Navarro, B. Nickless, and L. Winkler. Combining Cisco netflow exports with relational database technology for usage statistics, intrusion detection, and network forensics. In the 14th Systems Administration Conference (LISA’00), pages 285–290. USENIX Association, 2000.

[18] S. Robertson, E. V. Siegel, M. Miller, and S. J. Stolfo. Surveillance detection in high bandwidth environments. In Proceedings of the DARPA DISCEX III Conference, pages 130–139. IEEE, April 2003.

[19] D. Roelker, M. Norton, and J. Hewlett. sfPortscan. Sept. 2004.

[20] M. Roesch. Snort: lightweight intrusion detection for networks. In Proceedings of the 13th Systems Administration Conference (LISA’99), pages 229 – 238, Seattle, WA, USA, 1999. Usenix Association.

[21] S. E. Schechter, J. Jung, and A. W. Berger. Fast detection of scanning worm infections. In Proceedings of the 7th International Symposium on Recent Advances in Intrusion Detection, pages 59–81, 2004.

[22] S. Staniford, J. A. Hoagland, and J. M. McAlerney. Practical automated detection of stealthy portscans. Journal of Computer Security, 10(1/2):105–136, 2002.

[23] S. Staniford-Chen, S. Cheung, R. Crawford, M. Dilger, J. Frank, J. Hoagl, K. Levitt, C. Wee, R. Yip, and D. Zerkle. GrIDS - a graph based intrusion detection system for large networks. In Proceedings of the 19th NISSC, pages 361–370, 1996.

[24] N. Weaver, S. Staniford, and V. Paxson. Very fast containment of scanning worms, revisited. Malware Detection (Advances in Information Security), 27:113–145, 2007.

[25] D. Whyte, P. C. van Oorschot, and E. Kranakis.