End-to-end
Real world & Manually merged
For more than two pages
Page number
No need
Around 20% of traces
More than 50% of traces
websites in multi-tab traces, and attention-based profiling groups
separated trace parts.
4.1 Tab-aware Representation
Attackers collect multi-tab packet traces generated when succes-
sively browsing A.com, B.com or more site pages through anony-
mous network. Collected traces are then transformed into cell se-
quences with direction. All direction sequences have the same
maximum length by truncation or padding with 0. Next we let
the model automatically learn a latent feature representation from
direction sequences through a convolutional neural network. The
network has three one-dimensional convolution layers which are
adaptive to deal with direction sequences since they have the same
dimension number. Each layer has four types of operation includ-
ing convolution, activation, batch normalization and max pooling.
As shown in equation 2, one direction sequence [f1, ..., fn] is ab-
stracted into a set of feature vectors as the high-level representation
in first layer, new vectors are processed by the same way and more
shorter feature vectors are generated in the second layer, then the
third layer. Finally we get a two-dimensional feature map F with
r × c elements vij.
F = convolution([f1, ..., fn]), fi ∈ {1,−1} , F = (vij)r×c
(2)
The feature map is next concatenated as a one-dimensional tab-
aware representation. The concatenation follows the direction of
column axis instead of merging rows in feature map end to end.
As shown in Fig. 4, assume that feature vectors output by the last
convolution layer are (x11, x12, x13...), (x21, x22, y21...) ... (xn1, yn1,
yn2...), x and y denote two websites while the number indexes
identify vector elements. Two vectors are merged by columns as
arrows indicate, forming the vector (x11, x21 ... xn1, x12...) which
is in accordance with original positions of website x and y in raw
multi-tab packet traces.
New representation preserves the original order of page tabs
browsed by users, also together with potential information hidden
in these tabs, thus we call it a tab-aware representation. Besides,
different parts within the same website can also inherit the order
consistency. Tab-aware representation take the whole packet trace
into account including overlapping and non-overlapping area to
enrich the WF attacking model knowledge and separates multi-
ple websites as clearly as possible in the tab level, thus providing
convenience for following operations.
Figure 2: Threat model for WF attacks on Tor
Then they train a WF classifier using features of collected traces,
the classifier input can be raw sequence features (e.g., bytes, time
and direction sequence) or high-level features reorganized by sta-
tistical methods (e.g., standard deviation of raw features). A well
trained classifier can correctly map the features to corresponding
website labels, thus attackers can infer the websites visited by vic-
tims through extracting features from their packet traces in the
same way. As equation 1 shows, W is a set of monitored websites,
and w represents a list of website labels in a multi-tab packet trace.
The used feature here f is the direction sequence in which cells
from client to server is recorded as 1 and otherwise -1. The function
classi f ier stores the WF knowledge on monitored websites.
w = Classi f ier ([f1, f2, ..., fn]) , fi ∈ {1,−1} , w ∈ W , w ≥ 2 (1)
In fact, victims may visit those websites which do not appear
on the attacker’s list. This situation is called open world. On the
contrary, close world indicates that victims only visit those websites
which have already been analyzed by WF classifiers. Open world
can be seen as the special case of close world with one more website
class, and this unknown class has the highest proportion among all
classes because it includes all packet traces of websites ignored by
attackers. Thus open world is often used to examine the robustness
of WF classifiers in real world.
4 ARCHITECTURE OF BLOCK ATTENTION
PROFILING MODEL
BAPM has three main parts shown as Fig. 3. Tab-aware representa-
tion and block division are responsible for separating overlapping
251BAPM: Block Attention Profiling Model for Multi-tab Website Fingerprinting Attacks on Tor
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Figure 3: The architecture of Block Attention Profiling Model BAPM
4.2 Block Division
Making use of the overlapping area effectively is important for
multi-tab attacks, but it is confusing with mixed data although a
tab-level order of websites is maintained in tab-aware representa-
tions. To further separate websites in overlapping area, we divide
tab-aware representations into several blocks and try to discover
relations among blocks. The concept of “block” here differs from
the section in PSE-WF: one block is a part of feature representation
instead of original traces, and we will classify the website after
grouping related blocks together instead of depending on the result
of one single block. The relations between each block and other
blocks will be learned and enhanced during the model training iter-
ation by the following self-attention module. Blocks with stronger
relation will contribute on the classification of the same website
mutually, while an irrelevant block will be excluded to mitigate the
influence of mixed data.
A proper block size needs to be specified to maximize the block
division effect. There is not much we can utilize in smaller blocks
to be the basis for relation discovery, while larger blocks in over-
lapping area are as complicated as no division since they are still
involved with mixed data. We determine the 16 as the best block
number through parameter tuning in section 6.3.
Block division has two advantages: (1) It provides a finer granu-
larity for the attention module to fuse representation segments of
the same website, thus optimizing the attention distribution. Atten-
tion scores on single values are likely to be wrongly calculated due
to poor context information, but the condition would improve if
there are other values to be referred within one block, especially
when the block consists almost entirely of one website’s data. (2)
It reduces the model complexity. According to the model design,
the size of BAPM can reach up to 37 MB without block division.
But this number can be reduced by more than 80 percents under
16 blocks division because attention scores are calculated in blocks
instead of a lot more representation elements. Section 6.2 and 6.3
will confirm that through experiments.
4.3 Attention-based Profiling
Tab-aware representation blocks can not be directly input to a WF
classifier altogether since blocks come from several websites. There-
fore, we need to fuse information of blocks belonging to the same
website to create some adaptive varieties of original representa-
tion, and each variety is adjusted to only highlight one website.
As a widely used deep learning technique [5, 21, 28], the attention
mechanism satisfies the requirement by enabling the model to pay
attention of different levels to different sample parts.
Figure 4: Diagram of concatenation by columns
As shown in equation 3, three types of vectors named query (Q),
key (K) and value (V ) participate in the attention mechanism. All
block vectors of tab-aware representation are stored in V . K can be
seen as a series of storage bucket addresses, and each bucket stores
an element of V . Q is also an address set, and addresses point to
blocks we want to query. Assume that k, v are elements located in
the same position of K, V (i.e., the bucket in address k stores the
vector v), and q is the corresponding query address of block b. If K
and V have n elements, attention mechanism will generate n values
for each block by linking its address q with all bucket address ki
and then taking some value (no more than value of vector vi) out
from buckets in address ki. Therefore, the attention mechanism
can be described as the mapping rules from query-key pairs to
key-value pairs [21] as names “query”, “key” and “value” imply. The
calculation includes following two steps:
Attention(Q, K, V) = so f tmax(QKT(cid:112)dk
)V
(3)
(1) The first step is determining how to link a block address q with
bucket addresses ki. Mainstream methods measure the similarity
between q and ki through dot multiply, multi-layer perception
or simply concatenating two vectors, which is denoted as QKT .
The obtained similarity vectors are divided by the square root of
K’s dimension number(cid:112)dk, and then normalized by a softmax
252ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Guan, et al.
activation layer. The division and softmax operation all help to
minimize the possible deviation in calculation results. The weights
vector with sum of 1 output by the softmax layer is exactly attention
scores which is positively correlated with the similarity: a higher
attention score means that q is enough similar to ki, thus the block
with address q has a strong relation with the block vector stored in
ki. On the contrary, if two blocks have a weak relation, the attention
score on q and ki should be low. Extend q towards ki to qi towards
ki, and we can profile the relations between any two blocks in the
form of attention scores.
(2) The second step is distributing attention to all blocks accord-
ing to attention scores. Vectors in V are multiplied by corresponding
attention score just like a masking operation, a larger score on the
pair (q, ki) allows the model to take more value out from the block
vector vi, otherwise it only takes a little or even nothing. All masked
vectors form the new tab-aware representation concerned with the
block relations. For example, block b belonging to A.com in original
representation will be grouped together with other blocks of A.com
and their vector values are higher in the representation variety of
A.com. On the contrary, value of b will be minimized since it has a
weak relation with those blocks of B.com, thus influence of b on
B.com classification will be reduced.
Similarly, we repeat the above operation for each website, con-
structing a multi-head attention structure. As shown in equation 4,
attention scores are calculated on each attention head along with
the parameter iteration of weight matrices W on each group of Q, K,
V . One head generates one adaptive variety of the same tab-aware
representation for a specific website. When the number of websites
in multi-tab packet traces changes, the model can be expanded
easily with only adjusting the head number. Finally, the output of
heads is distributed to their respective softmax layers to classify one
of N websites as equation 5 shows. Through attention mechanism,
information of different websites is fused together without explicit
specifying the website location.
i )
headi = Attention(QW
, VW V
websitei = so f tmax(headi), i ∈ [1, N]
, KW K
i
Q
i
(4)
(5)
For example, A.com and B.com are represented by yellow and
blue area in Fig. 3 respectively. The upper attention head gives a
new representation for A.com after the calculation involved with
W (0, q), W (0, k) and W (0, v). Compared with original tab-aware
representations output by the concatenation, the new one mostly
focuses on the A.com part, secondarily focuses on the overlapping
area and least focuses or even pays no attention on remained B.com
part. Three parts are marked with the dark, normal and light color
respectively. For simplicity, we only display representation varieties
including first two websites. Since the bottom head is prepared for
the last websites, the A.com and B.com parts in this head are all
applied with the light color.
5 EXPERIMENT SETUP
5.1 Experiment Design and Settings
According to the achieved results and problem relevance of existing
WF attacks, PSP-WF [29], PSE-WF [6] and DF [19] are chosen as
comparison methods in following experiments. We reproduce the
work of PSP-WF and PSE-WF with optimal parameters, and build
a new model named Multi-DF by making DF suitable for predict-
ing multiple labels on one packet trace. Specifically, according to
the common principle of multi-label classification, we replace the
softmax activation function in DF with sigmoid and use the binary
entropy loss function instead of categorical one, then we can get
multiple website labels according to the maximum probability val-
ues. Note that other single-tab WF attacks can also be modified in
the same way, and we choose DF since it is the state of the art WF
attacking model among them so far.
To comprehensively illustrate the advantages of BAPM, we de-
sign experiments from three main aspects: (1) The comparison
experiment. We compare four methods under five overlapping
proportion ranging from 10% to 50% on both the first and the second
page in section 6.1. (2) Model design validation including ab-
lation and sensitivity analysis. Ablation analysis in section 6.2
gives the classification results before and after using block division
and multi-head attention to prove their effectiveness. Sensitivity
analysis in section 6.3 conducts a parameter tuning on the block
number to research its influence on model performance. (3) Gen-
eralization experiments. We perform experiments when over-
lapping page tab number increases to 3 in section 6.4 to test the
generalization of BAPM. Experiments on the open world dataset
when non-monitored websites number is 2000, 4000 and 6000 are
also considered in section 6.5.
Table 2 shows crucial parameter settings of three model parts
including CNN architecture parameters, the block number and
attention vector dimensions. The settings will be used in above
experiments, except for the three-tab experiment in which some
vector dimensions need to be extended.
Table 2: Parameter settings of BAPM
Model Part
Design Details
Input Dimension
Value
(8192, 1)
(12288, 1)*
[32, 64, 128]
[5, 5, 5]
[8, 8, 8]
16
(1 ≤ n ≤ 512)
Tab-aware representation
Block division
Filter Number
Filter Length
Pooling Size
Block Number
Scanning Range
Input Dimension
Attention-based profiling
(2048, 1)
(3072, 1)*
(256, 1)
(384, 1)*
*Extended vector dimensions for the three-tab experiment
Output Dimension
We use the accuracy (Acc), precision (Pre) and recall (Rec, also
known as true positive rate) rate as evaluation metrics. According
to the widely accepted definition, accuracy is the correctly classi-
fied proportion in all testing traces. Precision and recall rate are
calculated for a certain class like A.com, which are the correctly
classified proportion in all testing traces predicted as A.com, and in
all testing traces truly belonging to A.com, respectively. We report
253BAPM: Block Attention Profiling Model for Multi-tab Website Fingerprinting Attacks on Tor
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
the average precision and recall rate of all 50 website classes when
analyzing experiment results.
5.2 Dataset
We have three manually merged datasets and one real dataset in
our experiments. Manually merged datasets are constructed from
a public single-tab dataset in [27]. It contains 10398 websites in
total, 108 close world websites of which have around 100 packet
traces, and remaining open world websites all have only one packet
trace. Each trace has thousands of Tor cells denoted by 1, -1 for
direction and 0 for padding. We choose this dataset because merging
multi-tab traces needs to know both cell timestamp and direction
of original single-tab traces, and dataset in [27] is the largest one
meeting the requirement.
Close world multi-tab dataset. We select 50 websites which
have most packet traces in original dataset, 90 traces of each website
is used as train set and remaining traces are test set. One two-tab
trace is obtained by performing a sorting algorithm on two single-
tab traces according to packet timestamps like [6]. The beginning
packet timestamps of the first and second tab are set to be 0 and a
predetermined delay time before the sorting action, then packets
with smaller timestamps will be arranged in the front. We merge
two traces for each sample of websites to make sure that it appears
on both the first and second tab location at least once. The another
tab is selected randomly from 50 websites. Finally we have 9000
training traces and 1708 testing traces with “A.com & other” and
“other & A.com” mode.
Open world multi-tab dataset. We pick out 2000, 4000 and
6000 websites from 10290 open world websites of the original single-
tab dataset. We still merge two multi-tab traces for each sample
of close world websites but another tab is randomly selected from
2050, 4050 and 6050 websites respectively. Thus the open world
dataset has the same size with the close world one, but involves
open world websites in three scales.
Three-tab dataset. Considering that multi-tab packet traces
in reality can have more than 2 overlapping page tabs, we also
construct a three-tab dataset to examine our model. We merge
three traces for each sample of close-world websites and other two
tabs are randomly selected from 50 websites. Finally it has 13500
training traces and 2562 testing traces.
Note that we use the manual instead of real traces as major
datasets because (1) We need know the accurate ratio of overlapping
area to illustrate the effectiveness of BAPM (e.g., whether the model
is stable when the overlapping ratio increases), but we do not know
the overlapping ratio of real traces because we can not precisely
control when the first tab ends loading. (2) Attackers need to visit
two websites 10000 times for capturing 10000 real two-tab traces,
but they can only visit A.com and B.com 100 times respectively and
then produce 10000 traces through 100*100 possible combinations.
Thus the manual dataset can more efficiently utilize the collected
data. Since above two reasons are reasonable only when manual
traces have similar influence on the model with real traces, we also
build a real world multi-tab dataset to examine BAPM on real
traces we collected in section 7.1. The real world dataset has 10000
training traces and 1000 testing traces involving 50 websites.
6 EXPERIMENT RESULTS ANALYSIS
6.1 Comparison experiment results
Table 3 shows the overall accuracy, average precision and recall rate
of the comparison experiment. Note that we do not retrain BAPM
for different overlapping proportion, results on 10%-40% proportion
are obtained by BAPM trained on 50% proportion for one time. We
can draw following conclusions:
(1) BAPM achieves the best accuracy around 85% with five over-
lapping proportions under all evaluation metrics. The second best
method is PSP-WF which reaches the accuracy exceeding 80% when
overlapping proportion is small. The trend of average precision
and recall rate are similar to the accuracy, and this situation is also
found in following analysis. Results of PSE-WF and Multi-DF are
not ideal all the time, according to above theoretical analysis, PSE-
WF is heavily reliant upon every section label thus it will ignore
what is out each section, and sections in overlapping area confuses
the majority voting results. Multi-DF directly classifies the whole
trace including overlapping area indiscriminately, so it will also be
inevitably confused similar to PSP-WF. Another evidence of the
confusion is that we test DF on the original single-tab dataset and
it reaches 94.73% accuracy when acting as a single-tab attack. But
the accuracy is nearly cut in half when predicting multi-tab traces.
(2) BAPM has a stronger robustness when overlapping area size
increases, with a decline no more than 3.2%. PSE-WF and DF are also
stable but it’s possibly because their results on small overlapping
proportion are already limited. PSP-WF declines most obviously
since it will drop the half of one packet trace when overlapping
proportion is 50%, and it’s difficult to keeping the effectiveness with
such a serious information lost.
(3) Benefited from exploiting the overlapping area well, BAPM
outperforms other methods by 30% at least on the second page.