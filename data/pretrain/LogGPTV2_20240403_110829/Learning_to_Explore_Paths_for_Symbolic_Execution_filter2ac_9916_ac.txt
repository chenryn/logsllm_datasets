input program to improve the strategies at inference time. Online
learning can help Learch generalize better to programs that are
drastically different from the training programs.
5 INSTANTIATING LEARCH ON KLEE
In this section, we describe how to instantiate Learch in KLEE
[16]. For more implementation details, please refer to Learch’s
open source repository at https://github.com/eth-sri/learch. While
Learch is general and can be applied to other symbolic execution
frameworks, we chose KLEE because it is widely adopted in many
applications, including hybrid fuzzing [27] and others [23, 24, 36, 53,
76]. We believe the benefits of Learch can be quickly transferred to
the downstream applications and other systems [8, 22, 52, 68, 74].
5.1 Features and Model
We describe the features and the machine learning model of Learch.
Session 10A: Crypto, Symbols and Obfuscation CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2531Table 1: Features for representing a symbolic state state.
Feature
Description
Size of state’s current call stack.
stack
successor Number of successors of state’s current basic block.
testCase
coverage
Number of test cases generated so far.
(1) Number of new instructions covered by state’s branch.
(2) Number of new instructions covered along state’s path.
(3) Number of new source lines covered by state’s branch.
(4) Number of new source lines covered along state’s path.
icnt
cpicnt
constraint Bag-of-word representation of state’s path constraints.
Number of forks already performed along state’s path.
depth
Number of instructions visited in state’s current function.
Number of times for which state’s current instruction has
been visited.
Number of instructions executed by state since the last
time a new instruction is covered.
Number of times for which state’s subpaths [48] have been
visited. The length of the subpaths can be 1, 2, 4, or 8.
subpath
covNew
Features.
In Table 1, we list the features extracted for a symbolic
state (state) by extractFeature at Line 2 of Algorithm 2. Feature
stack calculates the size of state’s current call stack. The larger the
call stack size, the deeper the execution goes into. Feature successor
calculates the number of successors that state’s current basic block
has. The more successors, the more paths the state can lead to. The
next two features capture the execution progress. Feature testCase
returns the number of already generated test cases. The coverage
feature tracks the new instruction and line coverage achieved by
state’s latest branch and the program path already explored by
state, respectively. The states with more new coverage should be
explored first. Feature constraint is a 32-dimensional vector contain-
ing the bag-of-word representation of the path constraints of state.
To extract the bag-of-word representation, we go over each path
constraint that is represented by an expression tree and traverse
the tree to obtain the count of each node type.
The last five statistics are borrowed from existing expert-designed
heuristics [16, 48]. By including them as features of Learch, we
enable Learch to learn the advantages of those heuristics. Feature
depth calculates the number of forks that happened along state’s
path. Feature cpicnt records the number of instructions executed
inside state’s current function. Feature icnt is the number of times
for which the current instruction of state is executed. Feature covNew
records the last newly covered instruction for state and calculates
its distance to the current instruction. For feature subpath, we track
the subpaths of state (i.e., the last branches visited by state’s path
[48]) and return the number of times for which the subpaths have
been explored before. The fixed length of the subpaths is a hyper-
parameter and we used 1, 2, 4, and 8, as done in [48].
Model selection. Learch requires a machine learning model that
transforms the features described above to an estimated reward
(Line 3 of Algorithm 2). Any regression model can be adopted in
our setting. We leverage the feedforward neural network model
as it yielded good results in practice. We also tried simpler linear
regression and more complicated recurrent neural networks, but
found that feedforward networks achieved the best results. More
results on model selection are discussed in Section 6.6.
Leverage multiple learned strategies. As described in Sec-
tion 4.2, we apply ensemble learning to train N models and con-
struct N strategies. To apply the N strategies during inference time,
we simply divide the total time budget into N equal slots, run KLEE
with each learned strategy independently on one slot, and union
the tests from all the runs. This results in tests that achieve higher
coverage and detect more security violations than using a single
strategy for the full time budget. This is because different learned
strategies can explore an even more diverse set of program parts.
Moreover, KLEE usually generates tests quickly in the beginning
and saturates later. One time slot is usually already enough for a
learned strategy to generate a reasonably good set of tests.
5.2 Security Violations
An important aspect of symbolic execution tools is to detect vio-
lations of program properties that can lead to security issues. The
original KLEE detects certain types of errors. However, we found
that it usually only reports failures of the symbolic execution model
such as errors of the symbolic memory model, reference of external
objects, and unhandled instructions. Those errors are usually not
triggered concretely and do not lead to security violations.
In this work, we leverage Clang’s Undefined Behavior Sanitizer
(UBSan) [4] to instrument the input program and label five kinds
of security violations, listed below:
• Integer overflow: checks if the arithmetic operations overflow. The
operations include addition, subtraction, multiplication, division,
modulo, and negation of signed and unsigned integers.
• Oversized shift: checks if the amount shifted is equal to or greater
• Out-of-bounds array reads/writes: checks if the indices of array
• Pointer overflow: checks if pointer arithmetic overflows.
• Null dereference: detects the use of null pointers or creation of
than the bit-width of the variable shifted, or is less than zero.
reads and writes are equal to or greater than the array size.
null dereferences.
When any UBSan violation happens, a specific function is called.
We added handlers for capturing those functions and generating
a concrete test case triggering the violations, except for integer
overflows which has already been supported by KLEE. As a result,
KLEE is able to generate reproducible concrete tests for the above
violations. We note that KLEE is not restricted to UBSan violations
but the difficulty of supporting more violations depends on the
implementation of the handlers. For example, it would take a sig-
nificant amount of effort to support the AddressSanitizer [63] in
KLEE so we consider it as a future work item.
6 EXPERIMENTAL EVALUATION
We present an extensive evaluation of Learch aiming to answer
the following questions:
• Can Learch cover more code than existing manual heuristics?
• Can Learch discover more security violations?
• Can Learch generate better initial seeds for fuzzing?
• What is the impact of Learch’s design choices?
Session 10A: Crypto, Symbols and Obfuscation CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2532Table 2: The statistics of the programs used as test sets. MainLOC represents the main program lines without neither internal
nor external library code, i.e., the lines of the source file containing the main function. ELOC represents the total executable
lines in the final executable after KLEE’s optimizations, including internal libraries within the package but excluding external
library code that KLEE automatically links. The numbers for coreutils were averaged from all 52 test programs.
Program
coreutils
diff
find
grep
gawk
patch
objcopy
readelf
make
cjson
sqlite
Version
8.31
3.7
4.7.0
3.6
5.1.0
2.7.6
2.36
2.36
4.3
1.7.14
3.33.0
Input format
various
cmd + text
cmd + text
cmd + text
awk + text
cmd + text + diff
cmd + elf
elf
Makefile
json
sql commands
Binary size
142 KB
548 KB
802 KB
587 KB
1.3 MB
466 KB
4.9 MB
2.4 MB
466 KB
83 KB
2.1 MB
MainLOC
330
552
256
1, 167
604
984
2, 513
10, 381
883
71
35, 691
ELOC
1208
7, 739
11, 472
9, 545
24, 079
7, 007
48, 895
28, 522
7, 862
610
46, 388
-sym-args 0 1 10 -sym-args 0 2 2 -sym-files 1 8 -sym-stdin 8
KLEE settings for symbolic inputs
-sym-args 0 2 2 A B -sym-files 2 50
-sym-args 0 3 10 -sym-files 1 40 -sym-stdin 40
-sym-args 0 2 2 -sym-arg 10 A -sym-files 1 50
-f A B -sym-files 2 50
-sym-args 0 2 2 A B -sym-files 2 50
-sym-args 0 2 2 A -sym-files 1 100
-a A -sym-files 1 100
-n -f A -sym-files 1 40
A -sym-files 1 100 yes
-sym-stdin 20
6.1 Evaluation Setup
Now we describe the setup for our experimental evaluation.
Benchmarks. We evaluated Learch on coreutils (version 8.31)
and 10 real-world programs (listed in Table 2). coreutils is a stan-
dard benchmark for evaluating symbolic execution techniques
[16, 22, 48, 50]. We excluded 3 coreutils utilities (kill, ptx, and yes)
that caused non-deterministic behaviors in our initial experiments.
As a result, we used 103 coreutils programs in our evaluation. The
10 real-world programs are much larger than most coreutils pro-
grams, deal with various input formats, and are widely used in
fuzzing and symbolic execution literature [7, 12, 15, 21, 43, 72].
We randomly selected 51 of the 103 coreutils programs for train-
ing Learch. The rest 52 coreutils programs and the 10 real-world
programs were used as test sets for evaluating Learch’s perfor-
mance on unseen programs. The statistics of both test sets can
be found in Table 2. The coreutils test set has overlapping code
with the training set as they are from the same package [5]. This
represents a common and valid use case where developers train
Learch on programs from their code base and then run it to test
other programs from the same code base. Note that our use case
is different from other security tasks based on machine learning
such as binary function recognition [5, 10, 67] where sharing of
code between train-test splits must be avoided as training on target
binaries is impossible due to unavailable source code. The 10 real-
world programs are from packages different from coreutils and
thus share less code with coreutils. They are used to demonstrate
that Learch generalizes well across different code bases. That is,
once trained (e.g., with coreutils in our evaluation), Learch can
directly be used to test other software packages.
Baselines. We adopted existing manually crafted heuristics cre-
ated for KLEE as baselines [16, 48]. We do not compare with [74]
because it is not part of KLEE and we did not find its implemen-
tation available. We ran all KLEE’s individual heuristics on our
coreutils test set and only present the top four due to space limit:
• rss (random state search): each time selects a state uniformly at
random from the list of pending states.
• rps (random path search): constructs a binary execution tree
where the leaves are the pending states and the internal nodes
are explored states that produce the pending states. To select a
state, rps traverses the tree in a top-down fashion, picks a child
of internal nodes randomly until reaching a leaf, and returns the
reached leaf as the selection result. The leaves closer to the root
are more likely to be selected.
• nurs:cpicnt and nurs:depth: both are instances of the nurs (non-
uniform random search) family. They sample a state from a distri-
bution where the probability of each state is heuristically defined
by cpicnt and depth, respectively. See Table 1 and Section 5.1 for
the definitions of cpicnt and depth.
We also compare Learch with combinations of multiple heuristics:
• sgs (subpath-guided search) [48]: selects a state whose subpath
(defined in Table 1 and Section 5.1) was explored least often. To
achieve the best results, the authors of [48] ran four independent
instances of sgs where subpath lengths were configured to 1, 2,
4, and 8, respectively. Each instance spent a quarter of the total
time limit, and then the resulted test cases were combined. We
followed this in our evaluation.
• portfolio: a portfolio of four different heuristics: rps, nurs:cpicnt,
nurs:depth, and sgs. Like sgs and Learch, we ran each heuristic
of portfolio as an independent instance that spends a quarter of
the total time budget.
Different from Algorithm 1 which selects a state per branch, the
original KLEE performs state selection per instruction. In our initial
experiments on coreutils, we found that running the heuristics
with Algorithm 1 gave better results and thus used Algorithm 1 for
all heuristics in our evaluation. This means that our baselines are
already stronger than their counterparts in the original KLEE.
KLEE settings. KLEE provides users with options to specify the
number and length of symbolic inputs (e.g., command-line argu-
ments, files, stdin, and stdout) to an input program. The symbolic
options we used are listed in Table 2. We followed prior works
[16, 48] to set symbolic inputs for coreutils programs. For the 10
real-world programs, we configured the symbolic options based on
their input formats and prior works [15, 43].
Session 10A: Crypto, Symbols and Obfuscation CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2533rss
rps
nurs:cpicnt
nurs:depth
sgs
portfolio
546
532
537
539
480
574
618
e
g
a
r
e
v
o
c
e
n
i
L
640
480
320
160
0
s
m
a
r
g
o
r
p
f
o
r
e
b
m
u
N
40
30
20
10
0
5
8
6
8
Learch
13
29
26
(a) Line coverage of all files in the package.
(b) Number of programs with the best coverage.
Figure 5: Line coverage of running KLEE with different strategies for 1h on the 52 coreutils testing programs. The numbers
were averaged over 20 runs and the error bars represent standard deviations.
rss
rps
nurs:cpicnt
nurs:depth
sgs
portfolio
Learch
e
g
a
r
e
v
o
c
e
n
i
L
e
g
a
r
e
v
o
c
e
n
i
L
1800
1350
900
450
0
3200
2400
1600
800
0
3000
2250
1500
750
0
2400
1800
1200
600