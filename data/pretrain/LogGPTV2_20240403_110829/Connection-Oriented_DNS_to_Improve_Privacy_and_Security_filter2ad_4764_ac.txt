DITL/B-Root: This dataset was collected at the B-Root
nameserver as part of DITL-2013 and is also provided through
DNS-OARC. We selected B-Root because at the time of this
collection it did not use anycast, so this dataset captures all
trafﬁc into one root DNS instance. (Although as one of 13
instances it is only a fraction of total root trafﬁc.) We use this
trafﬁc to represent an authoritative server, since commercial
authoritative server data is not generally accessible.
Generality: These datasets cover each class of DNS re-
solver (§ II-A) and so span the range of behavior in different
parts of the DNS system and evaluate our design. However,
each dataset is unique. We do not claim that any represents
all servers of that class, and we are aware of quirks in each
dataset. In addition, we treat each source IP address as a
computer; NAT may make our analysis optimistic, although
this choice is correct for home routers with DNS proxies.
IV-B Trace Replay and Parameterization
To evaluate connection hits for different timeout windows
we replay these datasets through a simple simulator. We
simulate an adjustable timeout window from 10 to 480 s, and
track active connections to determine the number of concurrent
connections and the fraction of connection hits. We ignore the
ﬁrst 10 minutes of trace replay to avoid transient effects due
to a cold cache.
We convert the number of concurrent connections to hard-
ware memory requirements using two estimates. First, we
measure memory experimentally idle TCP connections by
opening 10k simultaneous connections to unbound and mea-
suring peak heap size with valgrind. On a 64-bit x86
computer running Fedora 18, we estimate TCP connection
at 260 kB, and each TLS connection at 264 kB;
to this
we estimate about 100 kB kernel memory, yielding 360 kB
as a very loose upper bound. Second, Google transitioned
gmail to TLS with no additional hardware through careful
optimizations, reporting 10 kB memory per connection with
minimal CPU cost due to TLS [44]. Based on their publicly
available optimizations, we use a conservative 150 kB as the
per connection memory cost.
IV-C Concurrent Connections and Hit Fraction
Trace replay of the three datasets provides several observa-
tions. First we consider how usage changes over the course
of the day, and we ﬁnd that variation in the number of active
connections is surprisingly small. When we measure counts
over one-second intervals, connections vary by ±10% for
Level 3, with slightly more variation for DNSChanger and
less for B-Root (graphs omitted due to space). Connection hit
fractions are even more stable, varying by only a few percent.
Given this stability, Figure 2 summarizes usage with medians
and quartiles.
The three servers have very different absolute numbers of
active connections, consistent with their client populations. All
servers show asymptotic hit fractions with diminishing beneﬁts
beyond timeouts of around 100 s (Figure 2c). The asymptote
varies by server: with a 120 s window, DNSChanger is at
97-98%, Level 3 at 98-99%, and B-Root at 94-96%. These
fractions show that connection caching will be very successful.
Since much network trafﬁc is bursty, it is not surprising that
caching is effective. We believe the lower hit fraction at B-
Root is due to its diverse client population and its offering a
relatively small zone; we expect similar results for other static
DNS zones.
Recommendations: We propose timeouts of 60 s for recur-
sive resolvers and 20 s for authoritative servers, informed by
Figure 2, with a conservative approach to server load. We
recommend that clients and servers not preemptively close
connections, but instead maintain them for as long as they have
resources. Of course, timeouts are ultimately at the discretion
of the DNS operator who can experiment independently.
These recommendations imply server memory require-
ments. With 60 s and 20 s timeouts for recursive and authorita-
tive, each DNSChanger needs 0.3 GB RAM (2k connections),
Level 3 3.6 GB (24k connections), and B-Root 7.4 GB (49k
connections), based on the 75%iles in Figure 2, for both user
and kernel memory with some optimization, in addition to
memory for actual DNS data. These values are well within cur-
rent, commodity server hardware. With Moore’s law, memory
is growing faster than root DNS trafﬁc (as seen in DITL [15]),
so future deployment will be even easier. Older servers with
limited memory may instead set a small timeout and depend
on clients to use TCP Fast Open and TLS Resume to quickly
restart terminated connections.
V. PERFORMANCE UNDER ATTACK
We next consider the role of DNS in denial-of-service
attacks: ﬁrst DNS’s role in attacking others through ampli-
ﬁcation attacks, then the performance of a DNS server itself
under attack. In both cases we show that TCP mitigates the
problem, and that TLS does not make things worse.
177177
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:02:44 UTC from IEEE Xplore.  Restrictions apply. 
n
o
i
t
c
e
n
n
o
c
t
n
e
r
r
u
c
n
o
c
f
o
r
e
b
m
u
n
 3500
 3000
 2500
 2000
 1500
 1000
 500
 0
 0
0.5
DNSChanger/all-to-one
0.25
 100
 50
 250
time-out window (seconds)
 150
 200
0
 300
)
B
G
(
n
o
i
t
p
m
u
s
n
o
c
y
r
o
m
e
m
n
o
i
t
c
e
n
n
o
c
t
n
e
r
r
u
c
n
o
c
f
o
r
e
b
m
u
n
 300000
 250000
 200000
 150000
 100000
 50000
 0
 0
DITL/B Root
Level 3, cns4.lax1
 100
 50
 250
time-out window (seconds)
 150
 200
 300
40
30
20
10
0
)
B
G
(
n
o
i
t
p
m
u
s
n
o
c
y
r
o
m
e
m
s
n
o
i
t
c
a
r
f
t
i
h
n
o
i
t
c
e
n
n
o
c
 1
 0.95
 0.9
 0.85
 0.8
 0.75
DNSChanger/all-to-all
DITL/B-Root
Level 3, cns4.lax1
 0
 50  100  150  200  250  300  350  400  450  500
time-out window (seconds)
(a) Median and quartiles of numbers concurrent
connections. Dataset: DNSChanger
(b) Median and quartiles of numbers concurrent
connections. Datasets: Level 3/cns4.lax1 and B-
Root
(c) Median connection hit fractions, taken server-
side. (Quartiles omitted since always less than 1%.)
Fig. 2: Evaluation of concurrent connections and connection hit fractions. Black circles show design point.
action
query
reply
ampliﬁcation
DNS
(UDP)
82 B
200–4096 B
3–40×
TCP-SYNs
no cookies w/cookies
76 B
66 B
1×
76 B
66–360 B
1–6×
Table IV: Ampliﬁcation factors of DNS/UDP and TCP with
and without SYN cookies.
V-A DNS: Amplifying Attacks on Others
Recently, ampliﬁcation attacks use DNS servers to magnify
attack effects against others [74], [48]. An attacker’s botnet
spoofs trafﬁc with a source address of the victim, and the
DNS server ampliﬁes a short query into a large reply.
Table IV shows our measurements of ampliﬁcation factors
of three classes of attacks: DNS over UDP, and DNS over
TCP without and with SYN cookies. DNS allows an attacker
to turn a short UDP request into a large UDP reply, amplifying
the attack by a factor of up to 40. TCP can amplify an
attack as well, since a single SYN can solicit multiple SYN-
ACKs attempts [43], but only by a factor of 6. With SYN
cookies, TCP does not retransmit SYN-ACKs, so there is no
ampliﬁcation for the attacker.
DoS-prevention also requires rate limiting, which can help
defuse UDP-based ampliﬁcation. Such rate limiting will be
important during a potential transition from UDP to TCP for
DNS: wide use of TCP can allow more aggressive rate limits
for TCP, as we show in § V-B, and partial use of TCP can
allow more aggressive rate limiting, as we discuss next.
We conclude that, although TCP does not eliminate DoS
attacks, full use of TCP eliminates ampliﬁcation of those
attacks, and partial use of TCP allows more aggressive rate
limiting during transition.
V-B Direct Denial-of-Service on the DNS Server
We next consider UDP and TCP attacks designed to
overwhelm the DNS server itself. While some DoS attacks
overwhelm link bandwidth, UDP attacks on DNS often target
server CPU usage, and TCP attacks overwhelm OS-limits on
active connections. Current DNS operators greatly overpro-
vision to absorb attacks, with best-practices recommending a
F 
1Gb/s, 
5ms 
1Gb/s, 
<1ms 
1Gb/s, 
<1ms 
200Mb/s, 
5ms 
S 
ixp 
A1 
Am 
Fig. 3: Network topology for DoS attack evaluation: legitimate
(F), attackers (A), and server (S).
protocol
UDP
TCP
TCP
TCP
attacker
src IP
spoofed
spoofed
spoofed
real
cookies
n/a
no
yes
yes
protocol
UDP
TCP
TCP
TCP
resource limit
CPU
foreground
TCP control buffers
TCP control buffers
TCP control buffers
Table V: Limited resource for each protocol combination in
tested DoS attacks.
factor of three [11]. We next aim to show that UDP attacks
are a threat, and attacks on a naive TCP service are deadly,
but a TCP service using TCP SYN cookies forces attackers to
use far more resources than today.
To evaluate a DoS attack, we deploy the network shown in
Figure 3 in the DETER testbed. We send foreground trafﬁc
from F to a DNS server S, then evaluate the effects of attack
trafﬁc (A1 to Am) sent to the server. The trafﬁc merges at a
router (IXP, an Internet Exchange Point) and is sent to the
server behind a bottleneck link. The server hosts a DNS
domain with 6.5M names in example.com, and the attacker
queries random names that exist in this domain. The server
is a single-core 2.4 GHz Intel Xeon running Linux Ubuntu-
14.04 (64-bit). This scenario (hardware and network speeds)
represents a scaled down version of a typical deployment.
We compare several combinations of protocols for attacker
and legitimate, foreground trafﬁc (Table V). We focus on all-
UDP trafﬁc and three cases of all-TCP use to compare current
DNS with proposed TCP deployments. While the future will
include both TCP and UDP usage, these two “pure” cases
show the limits. We use NSD-4.1.0 as our DNS server, with the
OS and application conﬁgured to support either 65k or 4k TCP
178178
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:02:44 UTC from IEEE Xplore.  Restrictions apply. 
1
0.8
0.6
0.4
0.2
s
e
i