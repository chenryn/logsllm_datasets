### Client Emulation and Data Collection in Videoconferencing Systems

#### Client Emulation
One method to bypass the need for physical sensory devices in videoconferencing clients is through device emulation. This approach allows us to have complete control over the sensory input, which is crucial for reproducible and automated benchmarking. We achieve this by using loopback pseudo devices for audio and video input/output. In Linux, the `snd-aloop` and `v4l2loopback` modules enable the setup of virtual sound and video devices, respectively. Once activated, these loopback devices appear to videoconferencing clients as standard audio and video devices, but the data is sourced from other applications rather than real hardware. In our setup, we use `aplay` and `ffmpeg` to replay audio and video files into these virtual devices. The in-kernel device emulation is transparent to the clients, eliminating the need for client-side modifications.

Another aspect of client emulation involves automating the navigation of the client's user interface (UI). Each videoconferencing client has specific UI elements for interacting with the service, such as logging in, joining/leaving a meeting, and switching layouts. We automate these interactions by emulating various input events (e.g., keyboard typing, mouse activity, screen touch) using OS-specific tools like `xdotool` for Linux and `adb-shell` for Android. For each videoconferencing system, we script the entire workflow of its client.

#### Coordinated Client Deployments
Fully-emulated clients allow us to deploy them in public clouds and mobile testbeds for automated testing. Controlling the audiovisual data feed and UI navigation provides unique opportunities to gain insights into the videoconferencing systems under test. For example, one client can be injected with a video feed containing specific patterns (e.g., periodic ON/OFF signals or high-/low-motion videos), and other clients receive the feed through the videoconferencing service. By comparing the injected and received feeds, we can evaluate different services. We can also easily coordinate the activities of multiple participants in a given session, such as ensuring only one user's screen is active at a time.

#### Platform-Agnostic Data Collection
Despite client emulation and coordination, the closed nature of existing videoconferencing systems (e.g., proprietary client software and end-to-end encryption) poses challenges for objective and unified metric comparisons. To address this, we perform data collection in a platform-agnostic manner:

1. **Network-Level Metrics**: We derive some evaluation metrics from network-level monitoring and measurements. For instance, we measure streaming lag by correlating packet timestamps on the sender and receiver sides. This allows us to evaluate the videoconferencing infrastructure without being influenced by client specifics. Accurate clock synchronization among deployed clients is essential, and major public clouds provide dedicated time sync services with their own stratum-1 clock sources [18, 26].

2. **User-Perceived Quality Metrics**: To supplement network-based metrics, we record videoconferencing sessions from individual participants' perspectives and assess the quality of recorded audio and video across different platforms. While Zoom offers local recording for each participant, services like Webex or Meet only allow the meeting host to record. We adopt a desktop recording approach using `simplescreenrecorder` to record the desktop screen with audio within a cloud VM.

3. **Resource Utilization**: We also evaluate the videoconferencing systems from the perspective of client resource utilization, particularly important for mobile devices. Although these metrics can be influenced by client implementation, we believe platform-driven factors (e.g., audio/video codecs) play a significant role.

### Deployment Targets

Based on the design approach, we deploy emulated videoconferencing clients on a group of cloud VMs and Android mobile phones. Each cloud VM hosts a videoconferencing client in a fully emulated setting to generate and/or receive a streaming feed, while Android devices only receive feeds without device emulation.

#### Cloud VM
A cloud VM runs a videoconferencing client on a remote desktop in a fully emulated environment. It consists of several components:
- **Media Feeder**: Replays audio and video files into corresponding loopback devices.
- **Client Monitor**: Captures incoming/outgoing traffic with `tcpdump` and processes it in an "active probing" pipeline, discovering streaming service endpoints and performing RTT measurements using `tcpping`.
- **Client Controller**: Replays a platform-specific script for operating/navigating the client, including launch, login, and meeting management.

To host the cloud VMs, the public cloud must not be used to operate the videoconferencing systems under test to avoid bias. We chose Azure cloud due to its wide geographic coverage and lack of association with the tested systems.

#### Android Devices
We use Samsung Galaxy S10 and J3 phones, representing both high-end and low-end devices. The J3's battery is connected to a Monsoon power meter for fine-grained readings. Both devices are connected to a Raspberry Pi via WiFi to avoid USB noise, and tasks are automated using Android Debugging Bridge (adb). The phones connect to the Internet over a fast WiFi with a symmetric upload and download bandwidth of 50 Mbps, and each device connects to its own WiFi network for isolated traffic capture.

### Quality of User Experience (QoE) Analysis

In this section, we present QoE analysis results from our benchmarking of three major videoconferencing systems: Zoom, Webex, and Meet, conducted from April to May 2021.

#### Cloud VM Setup
Each cloud VM is equipped with 8 vCPUs (Intel Xeon Platinum 8272CL with 2.60GHz), 16GB memory, and 30GB SSD. The screen resolution is set to 1900Ã—1200. We use the native Linux client for Zoom and the web client for Webex and Meet.

#### Streaming Lag
We evaluate the streaming lag experienced by users, defined as the time delay between audio/video signals ingested by one user and those received by another. Using emulated clients with synchronized clocks, we quantify lags precisely. We set the meeting host's video screen to a blank screen with periodic flashes and let other users join the session without audio/video. This bursty, one-way video feed allows us to determine the timing of sent/received video signals from network traffic monitoring. For example, Figure 2 visualizes the packet streams observed on the meeting host (sender) and another user (receiver).

This network-based metric discounts potential delays caused by the receiver-side client but effectively evaluates and compares lags induced by streaming infrastructures and their geographic coverage.

In the first set of experiments, we deploy seven VMs in the US, as indicated in Table 3. We create a meeting session with one VM designated as the meeting host, which broadcasts periodic video signals to the other six VMs for two minutes. We collect 35-40 lag measurements from each participant during the session and repeat the process 20 times for more representative sampling.