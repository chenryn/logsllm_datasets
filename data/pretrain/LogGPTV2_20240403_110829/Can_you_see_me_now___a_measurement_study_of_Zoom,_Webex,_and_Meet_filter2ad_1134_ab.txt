dinated client deployments, and (iii) platform-agnostic data
collection.
Client emulation. One way to circumvent the requirement
for sensory devices in videoconferencing clients is to emulate
them. Device emulation also means that the sensory input
to a videoconferencing client would be completely under our
control, which is essential to reproducible and automated
benchmarking. To this end, we leverage loopback pseudo de-
vices for audio/video input/output. In Linux, snd-aloop and
v4l2loopback modules allow one to set up a virtual sound-
card device and a virtual video device, respectively. Once
activated these loopback devices appear to videoconferencing
clients as standard audio/video devices, except that audiovi-
sual data is not coming from a real microphone or a capture
card, but is instead sourced by other applications. In our
setup we use aplay and ffmpeg to replay audio/video files
into these virtual devices. The in-kernel device emulation
is completely transparent to the clients, thus no client-side
modification is required.
Another aspect of client emulation is client UI navigation.
Each videoconferencing client has client-specific UI elements
for interacting with a videoconferencing service, such as log-
ging in, joining/leaving a meeting, switching layouts, etc. We
automate UI navigation of deployed clients by emulating vari-
ous input events (e.g., keyboard typing, mouse activity, screen
touch) with OS-specific tools (e.g., xdotool for Linux, and
adb-shell for Android). For each videoconferencing system,
we script the entire workflow of its client.
Coordinated client deployments. Fully-emulated clients al-
low us to deploy the clients in public clouds and/or mobile
testbeds for automated testing. The fact that we control
audiovisual data feed for the clients as well as their UI navi-
gation provides unique opportunities for us to gain, otherwise
difficult to obtain, insights into the videoconferencing sys-
tems under test. For example, one client can be injected with
a video feed with specific patterns (e.g., periodic ON/OFF
218
Figure 1: Cloud VM in a fully-emulated environment.
signals, or high-/low-motion videos), and other clients receive
the feed through a videoconferencing service. By comparing
the injected feed and received feeds, we can evaluate differ-
ent videoconferencing services. We can easily coordinate the
activities of multiple participants in a given conferencing
session to facilitate our analysis (e.g., only one user’s screen
is active at a time).
Platform-agnostic data collection. Even with client emulation
and coordination, the closed nature of the existing video-
conferencing systems (e.g., proprietary client software and
end-to-end encryption) poses as a hurdle to comparing the
systems with objective and unified metrics. That has led us
to perform data collection in a platform-agnostic fashion as
follows.
First, we derive some of the evaluation metrics from network-
level monitoring and measurements. For example, we measure
streaming lag by correlating packet timestamps on sender-
side and on receiver-side. That way, we can evaluate the
videoconferencing infrastructures without being influenced
by specifics in client deployments. This, however, requires
accurate clock synchronization among deployed clients. For-
tunately, major public clouds already provide dedicated time
sync services for tenant workloads with their own stratum-1
clock sources [18, 26].
In order to supplement network-based metrics with user-
perceived quality metrics, we record videoconferencing ses-
sions from individual participants’ perspective, and assess
the quality of recorded audios/videos across different plat-
forms. While Zoom provides a local recording option for each
participant, other services like Webex or Meet only allow
a meeting host to record a session. In the end, we adopt a
desktop recording approach as a platform-agnostic measure.
We run a videoconferencing client in full screen mode, and
use simplescreenrecorder to record the desktop screen with
audio, within a cloud VM itself.
Finally, we also evaluate the videoconferencing systems
from their clients’ resource-utilization perspectives, which is
particularly important for mobile devices. While these metrics
can be influenced by client implementation, we believe that
platform-driven factors (e.g., audio/video codecs) may play
a bigger role.
LoopbackSound DeviceLoopbackVideo DeviceMedia FeederAudioFileVideoFileStreamingTrafficActive ProbingTraffic CaptureClient MonitorClient ControllerUI NavigationAudio/video FeedsIMC ’21, November 2–4, 2021, Virtual Event, USA
Hyunseok Chang, Matteo Varvello, Fang Hao, and Sarit Mukherjee
3.2 Deployment Targets
Based on the design approach described above, we deploy
emulated videoconferencing clients on a group of cloud VMs
and Android mobile phones. Each of the cloud VMs hosts a
videoconferencing client in a fully emulated setting to gener-
ate (via emulated devices) and/or receive a streaming feed,
while Android devices only receive feeds from videoconfer-
encing systems without device emulation.1 In the following,
we describe each of these deployment targets in more details.
Cloud VM. A cloud VM runs a videoconferencing client on
a remote desktop in a fully-emulated environment. It con-
sists of several components as shown in Fig. 1. Media feeder
replays audio and video files into corresponding loopback
devices. The audio and video files are either synthetically cre-
ated, or extracted individually from a video clip with sound.
Client monitor captures incoming/outgoing videoconferenc-
ing traffic with tcpdump, and dumps the trace to a file for
offline analysis, as well as processes it on-the-fly in a sepa-
rate “active probing” pipeline. In this pipeline, it discovers
streaming service endpoints (IP address, TCP/UDP port)
from packet streams, and performs round-trip-time (RTT)
measurements against them. We use tcpping for RTT mea-
surements because ICMP pings are blocked at the existing
videoconferencing infrastructures. Client controller replays a
platform-specific script for operating/navigating a client, in-
cluding launch, login, meeting-join/-leave and layout change.
In order to host the cloud VMs, a public cloud must meet
the following requirements. First, the cloud must not be used
to operate the videoconferencing systems under test. For
example, the majority of Zoom infrastructure is known to be
hosted at AWS cloud [1]. If we run our emulated clients in the
same AWS environment, Zoom will be heavily favored in our
evaluation due to potential intra-cloud network optimization.
To prevent such bias, we exclude any public cloud being used
by the videoconferencing systems we tested. The cloud must
also have reasonably wide geographic coverage. In the end
we choose Azure cloud [16] as our benchmarking platform.
Android devices. We use Samsung Galaxy S10 and J3 phones,
representative of both high-end and low-end devices (Table 2).
The battery of the J3 is connected to a Monsoon power me-
ter [31] which produces fine-grained battery readings. Both
devices are connected to a Raspberry Pi (via WiFi to avoid
USB noise on the power readings) which is used to automate
Android UI navigation and to monitor their resource utiliza-
tion (e.g., CPU usage). Both tasks are realized via Android
Debugging Bridge (adb). The phones connect to the Internet
over a fast WiFi – with a symmetric upload and download
bandwidth of 50 Mbps. Each device connects to its own WiFi
realized by the Raspberry Pi, so that traffic can be easily
isolated and captured for each device.
1While Android devices can generate sensory data from their onboard
camera/microphone, we mostly do not use them for reproducible
benchmarking.
Name
Galaxy J3
Galaxy S10
Android Ver.
8
11
CPU Info Memory
Quad-core
Octa-core
2GB
8GB
Screen Resolution
720x1280
1440x3040
Table 2: Android devices characteristics.
Figure 2: Video lag measurement.
4 QUALITY OF USER EXPERIENCE
In this section, we present QoE analysis results from our
benchmark analysis of three major videoconferencing systems:
Zoom, Webex and Meet. The experiments were conducted
from 4/2021 to 5/2021.
4.1 Cloud VM Setup
Each cloud VM we deploy has 8 vCPUs (Intel Xeon Platinum
8272CL with 2.60GHz), 16GB memory and 30GB SSD. We
make sure that the provisioned VM resources are sufficient
for all our benchmark tests, which involve device emulation,
videoconferencing, traffic monitoring and desktop recording.
The screen resolution of the VM’s remote desktop is set to
1900×1200. We use the native Linux client for Zoom (v5.4.9
(57862.0110)), and the web client for Webex and Meet since
they do not provide a native Linux client.
4.2 Streaming Lag
First we evaluate streaming lag experienced by users (i.e.,
time delay between audio/video signals ingested by one user
and those received by another). While measuring streaming
lag in real-life videoconferencing is difficult, our emulated
clients with synchronized clocks allow us to quantify the lags
precisely. We purposefully set the video screen of a meeting
host to be a blank-screen with periodic flashes of an image
(with two-second periodicity), and let other users join the
session with no audio/video of their own. Using such a bursty,
one-way video feed allows us to easily determine the timing of
sent/received video signals from network traffic monitoring.
For example, Fig. 2 visualizes the packet streams observed
on the meeting host (sender) and another user (receiver).
The squares represent the sizes of packets sent by a meeting
host, and the circles show the sizes of packets received by a
user. As expected there are periodic spikes of “big” packets
(>200 bytes) that match periodic video signals sent and re-
ceived. The first big packet that appears after more than a
second-long quiescent period indicates the arrival of a non-
blank video signal. We measure streaming lag between the
meeting host and the other participant with the time shift
between the first big packet on sender-side and receiver-side.
219
 0 200 400 600 800 90 91 92 93 94 95 96Video Lag MeasurementPacket Size (Bytes)Time (Sec)SentReceivedCan You See Me Now? A Measurement Study of Zoom, Webex, and Meet
IMC ’21, November 2–4, 2021, Virtual Event, USA
Region
Location
Name
Count
US
Europe
Iowa
Illinois
Texas
Virginia
California
Switzerland
Denmark
Ireland
Netherlands
France
London, UK
Cardiff, UK
US-Central
US-NCentral
US-SCentral
US-East
US-West
CH
DE
IE
NL
FR
UK-South
UK-West
1
1
1
2
2
1
1
1
1
1
1
1
Table 3: VM locations/counts for streaming lag testing.
Figure 3: Videoconferencing service endpoints.
Admittedly, this network-based metric discounts any poten-
tial delay caused by a receiver-side client (e.g., due to stream
buffering/decoding). However, it is effective to evaluate and
compare lags induced by streaming infrastructures and their
geographic coverage.
In the first set of experiments we deploy seven VMs in the
US, as indicated by the “VM count” field in Table 3. We
create a meeting session with one VM (in either US-east or
US-west) designated as a meeting host, which then broadcasts
periodic video signals to the other six participating VMs for
two minutes before terminating the session. We collect 35-40
lag measurements from each participant during the session.
For more representative sampling, we create 20 such meeting
sessions with the same meeting host. Thus in the end we have