So senders have an incentive to understate DPM, which
allows them a higher bit rate. But then DPM will turn neg-
ative before reaching the destination. If networks discard4
negative packets, the utility to the sender of the higher bit
rate will rapidly collapse, as shown to the left of the ﬁg-
ure. Therefore honesty at ∆ρ0c = 0 will be the dominant
sender strategy that maximises its net utility. A receiver
that genuinely wants data to be sent as quickly as possible
4Various penalties short of discard, e.g. payload truncation,
can be imposed in order to preserve the feedback loop, given
a packet may be wrongly penalised.
net value to 
end-points,
∆U
practical
ideal
overstatement
of downstream
path metric at 
source, ∆ρ0c
0
penalty
probability, p
1
p(·)
Pn(ρn-∆∆∆∆ρnc )
p(·)Pn(ρn-∆∆∆∆ρnc )
DPM
probability
distribution, Pn
∆∆∆∆ρnc
downstream path
metric (DPM)
at receiver,
ρn
Figure 4: Truth telling incentives
0
(1 – p(·))Pn(ρn-∆∆∆∆ρnc ) = 
Pn(ρn+∆∆∆∆ρnc )
has incentives aligned with the sender, so honest feedback
also returns the maximum net gain.
In fact, the position is complicated by continuous vari-
ability of path congestion; even honest traﬃc will arrive at
its destination spread around zero. Below we describe a
dropper that makes allowances for this variability but still
detects understatement of DPM. The best dropper we can
currently envisage suﬀers some false positives and negatives,
blunting the incentive to be absolutely honest (Fig 4).
3.2.1 Adaptive dropper
If congestion didn’t vary, a malicious source understating
congestion by ∆ρ0c (numerically negative) would cause a
proportionate understatement at the destination of ∆ρnc.5
But congestion does vary, so if the probability distribution of
the DPM at the destination is Pn(ρn) for an honest sender,
it will be shifted to Pn(ρn − ∆ρnc) for the malicious sender.
We propose a dropper4 at the last hop before the receiver.
The dropper builds a model of the prevailing pattern of
cheating for all packets leaving the same interface and as-
sumes that each new packet is characteristic of this recent
history; the more recent cheating, the stricter the dropper
becomes. But its strictness is further modulated by how
negative ρn is of each packet under scrutiny.
Conceptually, the bell curve in Figure 5 shows the proba-
bility distribution of arriving packets, exponentially weighted
to favour recent arrivals. We assume this will be the prob-
ability distribution of the DPM of the next packet. Su-
perimposed on a diﬀerent vertical scale is a conjectured
penalty probability function, p(·) intended to allow through
as much negative DPM as positive, but no more. This can
be achieved by ensuring that the distribution remaining af-
ter applying the penalty function is symmetric about zero
(the unshaded cusp curve). So for ρn < 0:
Pn(ρn − ∆ρnc) = Pn(ρn + ∆ρnc).
(1)
(cid:6)
(cid:7)
1 − p(·)
Initially we choose to keep state and processing to a min-
imum by modeling prevailing conditions with just the expo-
nentially weighted moving average µ and EWM variance ν.
So we model the prevailing distribution Pn(ρn − ∆ρnc) as if
it were the normal distribution N (µ, ν) reconstructed from
recent traﬃc, whatever the actual distribution (e.g. Fig 6).
At each packet, the EWMA & EWMV are updated:
µ ← γρn + (1 − γ)µ
ν ← γ(ρn − µ)
(cid:7)
(cid:6)
2
1 − (cid:7)n−1
0 m
+ (1 − γ)ν.
∆ρ0c
5From Eqn (7) ∆ρnc =
(2)
(3)
Figure 5: Penalising misbehaviour under uncer-
tainty
-0.3
 1
-0.2
-0.1
 0.3
 0.1
 0
 0.2
 0.1
 0.01
 0.001
 1e-04
 1e-05
 1e-05
 1e-04
 0.001
 0.01
 0.1
honest traffic
truncated
unaffected
penalty prob.
dishonest traffic
 1
-0.3
-0.2
-0.1
 0
 0.1
 0.2
 0.3
Figure 6: Typical simulated distributions of DPM
at the destination from honest (top) and dishon-
est (bottom) sources, also showing proportion of pe-
nalised traﬃc (note log scale).
For attack traﬃc µ → ∆ρnc, converging faster by increas-
ing γ to weight recent values (0 < γ ≤ 1). In maintaining
the EWMA, positive packets with the ‘certain’ ﬂag cleared
(see §2) are ignored, incentivising correct use of the ﬂag.
Then, using the formula for a normal distribution,
Pn(ρn − ∆ρnc) =
1√
2πν
− (ρn−µ)2
e
2ν
.
(4)
we can derive the required penalty probability function to
apply to each speciﬁc packet with DPM ρn, by re-arranging
(1) and substituting from (4):
p(ρn, µ, ν) = 0;
µ ≥ 0 or ρn ≥ 0
= 1 − Pn(ρn + µ)
Pn(ρn − µ)
−2ρnµ
= 1 − e
ν
;
µ < 0, ρn < 0
(5)
As required, the penalty becomes stricter the worse the
EWMA becomes, but ﬂattens to zero discards when hon-
est users keep the EWMA to zero.
Where a cheating ﬂow is hidden in a large honest aggre-
gate, it causes a slightly negative EWMA, leading to some
dropping. After Floyd and Fall [7] we cache the ﬂow identi-
ﬁers of penalised packets. Once any aggregate of destination
(and/or source) identiﬁers appears more often than would be
likely by chance, a second instance of the dropper is spawned
and traﬃc matching the identiﬁer(s) is ﬁltered into it. Each
instance of a focused dropper maintains its own EWMA6
and may spawn further droppers. These focused droppers
should be far more sensitive than the ﬁrst, also shielding
honest traﬃc from the risk of false negatives.
Of course, if cheating negative traﬃc imitates identiﬁers
used in honest traﬃc, both will be ﬁltered into the same
focused dropper, causing collateral damage to the honest
traﬃc. But by deﬁnition the cheating traﬃc will tend to be
more negative, which the above penalty function is designed
to discriminate against.
Having isolated suspect identiﬁers, an egress edge dropper
can send hints upstream. Any node can test hints because
they point to traﬃc measurably below an objective thresh-
old. And a node need only act on the hints if it has suﬃ-
cient resources. So the hints need not be authenticated (un-
like DoS ﬁlter push-back requests), avoiding vulnerability to
ﬂoods of bogus authentication requests. Also, the hints can
safely jump multiple domains without the need for a global
key management infrastructure. So push-back of hints does
not depend on the co-operation of high speed core networks,
where operators are more wary of any additional processing.
Even if explicit congestion marking were universally de-
ployed, buﬀers could still occasionally overﬂow. So irrespec-
tive of any hints, if a router must discard packets, clearly it
should bias against any with negative DPM.
3.2.2 Honest delay reporting
Congestion control and traﬃc engineering depend on path
delay as well as congestion, so we need header ﬁelds for both.
The framework we built above (§3.2 & Fig 4) to incentivise
honest congestion reporting relied on two properties of con-
gestion: it should not be negative; and rising path conges-
tion should lead to a drop in sending rate (whatever form of
fairness is chosen). Delay has exactly the same properties:
negative delay is physically impossible; and rising feedback
delay should lead to a lower sending rate.
So, we can use a similar incentive mechanisms to that
we used for congestion to ensure the sender neither over-
states nor understates delay. An adaptive dropper, like the
one above for the congestion ﬁeld (§3.2.1), could detect and
remove any negative imbalance of delay headers at the in-
ternetwork egress. And at the ingress we can use a policer
like the TCP rate equation policer below (§3.3.1) that pun-
ishes sources sending faster than the TCP-fair rate, which
depends inversely on both congestion and feedback delay.
3.3 Fair congestion response
√
3.3.1 TCP rate equation policer
where k ≈ (cid:8)
avoidance), TCP converges to the rate ¯xT CP ≈ ks/(T
In the fastest phase of the TCP algorithm (congestion
p),
(3/2) and s, T & p are respectively the packet
size, round trip time and path marking (or loss) rate [17].
Re-feedback ensures that a policer at the network ingress
6From Eqn (5) an attacker can reduce dropping probability
by increasing variance, e.g. by alternating honest & zero
packets. So a focused dropper should use the EWMV of
the top level dropper. We are investigating variants with
varying degrees of statefulness and responsiveness.
can derive these parameters from the metrics each packet
truthfully declares. It can then calculate a compliant rate
against which to compare the source’s actual rate.
Previous policers had to be placed at every site of possible
congestion. With re-feedback, it is suﬃcient to place one po-
licer at each ingress to the internetwork. Here, downstream
congestion ρ2,1 can be assumed equal to path congestion,
p. The policer can approximate the round trip delay as
T ≈ T0 + 2ρ1,1, where the upstream round trip T0 can be
found by a previous echo test against each source and the
downstream delay ρ1,1 arrives in each packet7.
If the current TTL and ECN ﬁelds in IP were used to
implement re-feedback, as sketched in §4, an ingress po-
licer would have enough information to mirror the TCP al-
gorithm. As we explain later, binary congestion marking
takes a long time to convey a congestion level—indeed, a
ridiculously long time as ﬂow rates increase over the years.
So, given the architectural nature of this paper, we prefer
to focus on multi-bit congestion and delay ﬁelds in future
packet headers. Quantifying how quickly a policer could de-
tect misbehaving ﬂows using current IP, and the resulting
gain from more bits in headers, is planned for future work.
Below we outline one possible policing algorithm. It re-
quires per ﬂow state, but this isn’t necessarily a scalability
problem at the edge of an internetwork, however it does lay
the policer open to resource depletion attacks. In a longer
version of this paper (in preparation) we describe a variant
with sub-linear scaling of ﬂow state, but our goal here is
to give a clear implementation example that is concrete but
avoids gratuitous distractions.
The policer requires a token bucket per ﬂow. It empties
the bucket by the size of each arriving packet and ﬁlls it at a
rate equivalent to that of a TCP compliant ﬂow experiencing
the same path conditions. It calculates this by deriving p
and T from the re-feedback ﬁelds as above. In other words,
when a packet arrives, the policer subtracts the packet size
p), where ∆t is the
s from the bucket and adds ks∆t/(T
time since the ﬂow’s previous packet.
√
If the bucket empties, sanctions are applied to the ﬂow.
For instance, all future packets might be discarded, or the
policer could choose to take over rate control for the ﬂow.
The depth of the bucket controls the ﬂexibility allowed for
a ﬂow to stray from its expected throughput; it is set to
α¯xT CP τ , where α is the threshold greediness for a ﬂow to
be considered non-compliant over a time τ , and ¯xT CP is an
p). A ﬂow with a throughput higher than
EWMA of ks/(T
α¯xT CP will be detected in a time smaller than τ .
√
α is chosen so that a compliant ﬂow is most unlikely to
trigger starvation of the bucket. For instance, when p=1%,
the average congestion should be 12.3 packets per round-
trip. The probability of getting a window larger than 42 is
smaller than 0.01%. Setting α to 42/12.3 = 3.4 and τ = T
would guarantee that less than one compliant ﬂow in ten
thousand would be subjected to sanction. Increasing α and
τ would reduce false positives further.
3.3.2 Edge QoS
Our interest in solving the policing problem was not solely
to police a single response to congestion, such as TCP-
7For simplicity, we choose to ignore congestion delay, be-
cause simple scaling arguments [13, §2] show that as capacity
continues to grow, congestion delays will become insigniﬁ-
cant relative to ﬁxed propagation delays.
friendliness, although that alone is a major contribution.
Once timely, truthful downstream path information is visi-
ble to ingress network operators in data packets, they can
oﬀer a spectrum of responses to incipient congestion. This is
equivalent to oﬀering diﬀerent levels of QoS, perhaps rang-
ing from a scavenger class, through best eﬀort and premium
levels of diﬀerentiated service to admission controlled band-
width reservations (the right to zero congestion response)—
all without any diﬀerential treatment on network elements
beyond the ﬁrst ingress (with the caveat below).
Kelly and co-workers [14] pioneered this approach, prov-
ing it optimises social welfare across a network. Further its
policing architecture solves the scalability problems inherent
in other QoS approaches, though this is seldom appreciated.
With traditional QoS some identiﬁcation convention must
distinguish which traﬃc the edge has decided should be
given which preferential treatment as it passes to interior
domains. Using ﬂow identiﬁcation (like Intserv) preserves
precision, but scales badly. Using class identiﬁcation (like
Diﬀserv) loses precision at scale.
With edge QoS, instead of the edge identifying the traf-
ﬁc’s QoS for interior routers, interior routers identify the
traﬃc’s congestion for the edge. Because traﬃc already car-
ries end-point identiﬁers, regular packet forwarding carries
congestion marking to its source cause through aggregation
and deaggregation with absolute precision, with no need for
a separate QoS identiﬁcation convention. The only unequal
treatment of diﬀerent traﬃc identities is in the policer at
the ﬁrst ingress to the internetwork, where customer or ﬂow
identities have local signiﬁcance.
Siris [20] has proven this approach through simulation.
But deployment was conﬁned to a radio network controller
scenario where congestion feedback in the back-channel to
the sender could be intercepted—a constraint that can be
relaxed with re-feedback, giving general applicability.
Having sung the praises of closed-loop control, a caveat is
necessary. Unusual conditions (link failure or sudden traf-
ﬁc shifts) can cause traﬃc in ﬂight to overﬂow queues. So,
within a round trip, each resource must still be capable of
rudimentary local (open-loop) traﬃc class prioritisation un-
til the closed-loop restores order.
Adaptive policer:
If one user creates multiple ﬂows,
or runs ﬂows for longer than another user (e.g. p2p ﬁle-
sharing), per-ﬂow approaches like TCP cannot arbitrate
fairness between users. We can generalise to an adaptive
policer based on MulTCP [6] that gives each ﬂow an equiv-
alent rate to w TCP ﬂows. With the beneﬁt of re-feedback,
it can maintain a per user count of congestion sent. But,
rather than levying an unpredictable charge for this conges-
tion [14], the policer can compare the count to whatever the
user chooses to pay. So a ﬂat monthly rate would eﬀectively
buy a congestion quota. The closer the internal congestion
count approached this quota, the more w would be squeezed.