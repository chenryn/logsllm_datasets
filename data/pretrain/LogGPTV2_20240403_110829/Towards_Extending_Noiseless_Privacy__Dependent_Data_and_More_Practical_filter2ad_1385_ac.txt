able X we have
Moreover, if Y ∼ N (0, 1), then for any random variable X we
have
dK (X, Y ) (cid:54)(cid:112)2CdW (X, Y ).
(cid:19) 1
4(cid:112)dW (X, Y ).
(cid:18) 2
dK (X, Y ) (cid:54)
π
Lastly, we recall a theorem from [33].
FACT 4
0, σ2 = V ar[(cid:80)n
(THEOREM 3.6 IN [33]). Suppose X1, . . . , Xn are
i < ∞, EXi =
random variables such that for every i we have EX 4
i=1 Xi
. Let the col-
lection (X1, . . . , Xn) have dependency neighborhoods Ni, i ∈
{1, . . . , n} and also deﬁne D = max1(cid:54)i(cid:54)n |Ni|. Then, for ran-
dom variable Z with standard normal distribution we have
i=1 Xi] and deﬁne W =
(cid:80)n
σ
dW (W, Z) (cid:54) D2
σ3
E|Xi|3 +
3
2
D
σ2
n(cid:88)
i=1
(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)
i=1
√
√
28
π
EX 4
i .
This fact is obtained by using Stein’s method. Note that the Stein’s
method does not assume anything about joint distribution of depen-
dent subsets, only the size of the greatest dependent subset. We will
use these facts to prove a following
consider mechanism M (X) = (cid:80)n
THEOREM 3. Let X = (X1, . . . , Xn) be a data vector. We
i=1(Xi). Let EXi = µi and
i < ∞. Suppose there are dependency neighborhoods Ni, i ∈
EX 4
{1, . . . , n}, where D = max1(cid:54)i(cid:54)n |Ni|. Let σ2 = V ar(M (X)).
If the data sensitivity is ∆ then M (X) is (ε, δ)-NP with following
parameters
(cid:114)
ε =
∆2 ln(n)
,
E|X∗
i |3 +
3
2
D
σ2
E (X∗
i )4 +
√
4
5
,
n
and
δ = c(ε)
(cid:118)(cid:117)(cid:117)(cid:117)(cid:116) D2
σ3
n(cid:88)
i=1
where X∗
i = (Xi − µi) and
σ2
√
√
26
π
(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)
(cid:19) 1
(cid:18) 2
i=1
4
.
π
c(ε) = 2(1 + eε)
denote σ2 = V ar((cid:80)n
in previous section.
Proof of this theorem is presented in the Appendix. Note that we
as
i=1 Xi) in contrast to σ2 =
i=1 V ar(Xi)
n
(cid:80)n
5. ADVERSARY WITH AUXILIARY
INFORMATION
So far we have not discussed auxiliary information of the adver-
sary, namely we assumed that the adversary only knows the correct
distribution of the data vector and dependencies in the data (if they
exist). We would like to extend our results from Subsections 3 and 4
to take into account the adversary’s knowledge about the exact val-
ues of at most fraction γ of users. Let us assume that the auxiliary
information of the adversary consists of all records (values) of a
subset Γ of the data. Let |Γ| = γ · n. Instead of n users contribut-
ing to adversarial uncertainty, we will have (1 − γ) · n users who,
due to randomness in their data, make the aggregated value private.
This is stated in the following observation
OBSERVATION 1. Let us consider an adversary with knowl-
edge of exact values of all records of a subset Γ of the data. Let
|Γ| = γ · n. Then all previous theorems from this paper can be
easily adapted to such an adversary by considering data of size
(1 − γ)n instead of n contributing to randomness. This essentially
captures the fact that all other users (about whom adversary has no
information) still contribute to the randomness of the query. More-
over, if we assume that the adversary has auxiliary information
about every record of the data (that is |Γ| = n) then this model col-
lapses to standard differential privacy, where no uncertainty comes
from the data itself. This shows that indeed the standard differential
privacy is a special, most pessimistic, case of this model.
Let us ﬁrst introduce an extension to Theorem 2, which takes
into account the adversary’s knowledge about the exact values of
fraction of users.
THEOREM 4. Let X = (X1, . . . , Xn) be a data vector, where
Xi are independent random variables. Denote set of all indexes
by [n]. Assume that adversary knows the exact values of at most
fraction γ of users. Denote the set of indexes of compromised users
by Γ, where |Γ| = γn. Let µi = EXi and σ2
i∈[n]\Γ V ar(Xi)
and E|Xi|3 < ∞ for every i ∈ {1, . . . , n}. Consider mechanism
i=1(Xi). We denote data sensitivity of vector X and
mechanism M as ∆. M (X) is (ε, δ)-NP with following parame-
ters
M (X) =(cid:80)n
(1−γ)n
(cid:80)
Γ =
(cid:115)
ε =
∆2 ln((1 − γ)n)
(1 − γ)nσ2
Γ
,
and
δ =
1.12(cid:80)
(cid:16)(cid:80)
(cid:17) 3
i∈[n]\Γ E|Xi − µi|3
2
i∈[n]\Γ V ar(Xi)
(1 + eε) +
√
4
n
.
5
PROOF. Proof of this theorem is analogous to proof of Theo-
rem 2, with the single difference that only non-compromised users
contribute to the randomness, namely variance of the sum con-
sists of the uncompromised users variance. Therefore when using
Berry-Esseen theorem the sum weakly converges to normal distri-
bution with smaller variance than in the case where γ = 0. Note
that in the proof we assume that we know which subset of users is
compromised. This might obviously be unknown to the data owner,
so we can assume the worst case, namely that the compromised
subset Γ is the subset of size γn with the greatest variance. This
might be checked by the owner (which such subset has the greatest
variance) and then the theorem holds, no matter which users are
really compromised.
Similarly we can introduce an extension to Theorem 3
552M (X) = (cid:80)n
THEOREM 5. Let X = (X1, . . . , Xn) be a data vector. Denote
set of all indexes by [n]. Assume that adversary knows the exact
values of at most fraction γ of users. Denote the set of indexes of
compromised users by Γ, where |Γ| = γn. We consider mechanism
i < ∞. Suppose
there are dependency neighborhoods Ni, i ∈ {1, . . . , n}, where
X = max1(cid:54)i(cid:54)n |Ni|. Let σ2
Γ = V ar(X\Γ). If the data sensitivity
is ∆ then M (X) is (ε, δ)-NP with following parameters
i=1(Xi). Let EXi = µi and EX 4
(cid:115)
∆2 ln((1 − γ)n)
σ2
Γ
,
(cid:113)
M 4
X +
5(cid:112)(1 − γ)n
4
,
√
√
26
π
ε =
D2
σ3
Γ
M 3
X +
(cid:115)
and
δ = c(ε)
where
3
2
D
σ2
(cid:88)
(cid:88)
i∈[n]\Γ
i∈[n]\Γ
M 3
X =
M 4
X =
and
c(ε) = 2(1 + eε)
E|Xi − µi|3
E (Xi − µi)4
(cid:18) 2
(cid:19) 1
4
π
.
PROOF. Here also the proof is analogous to the proof of Theo-
rem 3, and also the difference is that only non-compromised users
contribute to the randomness, namely variance of the sum con-
sists of the uncompromised users variance. When we bound the
Kolmogorov distance (using Stein method) between the sum and
a normal distribution, we use one with smaller variance (namely
variance of X \ Γ) than in the case where γ = 0. As in the previ-
ous theorem, a practitioner can assume the worst case, namely that
the compromised subset Γ is the subset of size γn with the greatest
variance.
These simple extensions of our previous theorem give us a com-
plete insight into noiseless privacy in adversarial model presented
in Subsection 2.2. The owner of the data (or any party responsible
for the privacy in central or distributed database) can give his users
a rigorously proved guarantee that as long as at most a fraction γ
of users is compromised and (in dependent case) if the size of the
greatest dependent subset is at most D, then the privacy parameters
at least as good (we have shown the upper bound for the parame-
ters) as given in Theorem 4 if the data is independent or Theorem 5
if there are dependencies (known to adversary) in the data.
6. SYNERGY BETWEEN ADVERSARIAL
UNCERTAINTY AND NOISE ADDITION
In previous sections we have shown what are the privacy param-
eters for the randomness inherently present in the data. However,
it is easy to imagine that in many cases the amount of randomness
(adversarial uncertainty) might be too small to ensure desired size
of privacy parameters. Does it mean that in such case we have to
step back and use only standard differential privacy methods? For-
tunately, it does not. It turns out that the proofs of our theorems are
constructed in such a way, that it is possible to extend them to the
case where we add some noise to increase the randomness in the
data. Even more importantly, it is also easy to quantify how much
noise has to be added to improve privacy of the data to the desired
parameter in our adversarial model.
To the best of authors knowledge, so far there has not been any
approach in the privacy literature to combine the idea of utilizing
adversarial uncertainty (randomness in data) and standard approach
which is adding appropriately calibrated noise. The idea of adding
noise to already somewhat random data is quite simple, yet it needs
to be carefully analysed so that one may know exactly how much
does it enhance the privacy. It is intuitively very natural to think that
the more randomness is present in the data, the less noise (or none,
if the randomness itself is enough) we have to add to satisfy desired
level of privacy. However, to become a state-of-the-art approach
to preserving privacy, this intuition has to be formally introduced,
rigorously quantiﬁed and proved.
We now introduce a following
data sensitivity is ∆ and V ar((cid:80)n
THEOREM 6. Let X = (X1, . . . , Xn) be a data vector, the
i=1 Xi) = σ2. We consider
mechanism M (X) which, due to adversarial uncertainty has cer-
tain privacy parameters (ε1, δ). We can improve this parameter by
ξ. We show that M∗(X) =
adding unbiased noise of variance σ2
M (X + ξ) where ξ is noise (namely random variable such that
ξ) preserves privacy with parameters
Eξ = 0 and V ar(ξ) = σ2
(ε, δ), where
(cid:115)
ε =
∆2 ln(n)
σ2 + σ2
ξ
.
PROOF. This formula can be obtained in a straightforward man-
ner from our previous proofs. Similarly as in Theorems 4 and 5 one
can easily see that the sum of data with added noise has variance
ξ, because the noise is independent from data. Therefore ap-
σ2 + σ2
propriate normal random variables to which we bound the distance
of our sum (as in Berry-Esseen theorem and Stein method) will
have greater variance, which in turn gives smaller varepsilon.
This approach is quite similar as in the case where the adversary
has information about exact values of some fraction of the data, but
this time we add variance instead of subtracting it. Improving δ
parameter by adding noise seems to be more difﬁcult, as it might
require different approach to previous theorems. We leave it as an
interesting problem for future work. After this theorem we can also
present an useful observation
OBSERVATION 2. We can state Theorem 6 in a different way,
namely for a ﬁxed privacy parameter ε, we obtain that necessary
variance of the noise to obtain desired level of privacy is
(cid:18) ∆2 ln(n) − ε2σ2
(cid:19)
, 0
.
ε2
σ2
ξ = max
PROOF. This observation is obtained from Theorem 6 and quite
straightforward algebraic manipulations.
We also give more speciﬁc observation concerning noise hav-
ing Laplace distribution, which is a common technique in standard
differential privacy approach (see for example [16])
the data sensitivity is ∆ and V ar((cid:80)n
OBSERVATION 3. Let X = (X1, . . . , Xn) be a data vector,
i=1 Xi) = σ2. We consider
mechanism M (X) which, due to adversarial uncertainty has cer-
tain privacy parameters (ε1, δ). We show that M∗(X) = M (X +
ξ) where ξ ∼ Lap( ∆
) preserves privacy with parameters (ε, δ),
where
ε2
(cid:115)
ε =
1 · ε2
ε2
1 + ε2
2ε2
2 · ln(n)
2 ln(n)
.
553PROOF. This observation is obtained by application of Theo-
rem 6 for ξ ∼ Lap( ∆
).
ε2
Theorem 6 allows the party responsible for preserving privacy
to enhance parameter ε of the data itself by using standard meth-
ods of differential privacy. See however, that the noise necessary to
achieve the desired level of privacy is smaller than using standard
differential privacy methods due to the fact, that we already have
some level of privacy achieved by the randomness present in the
data. We conclude our discussion concerning synergy between ad-
versarial uncertainty and differential privacy approach by showing
a following
EXAMPLE 2. We consider a data vector X = (X1, . . . , Xn)
and mechanism M (X) having the data sensitivity ∆ = 10 and
10 . We enhance the privacy by adding
V ar(M (X)) = σ2 = n
Laplace noise of variance σ2
ξ. Using Theorem 6 and Observa-
tion 2 we can compute what is the necessary variance of noise
to obtain privacy parameter ε = 0.2 depending on the number of
users. See Figure 5. See that we have also plotted the variance of
Figure 5: Example 2, red dashed line shows the variance of neces-
sary noise for Laplace mechanism using standard differential pri-
vacy approach. Blue thick line shows the variance of necessary
noise after taking into account the adversarial uncertainty.
noise using differential privacy approach, namely Laplace mecha-
nism (see [16]). We can see that in this example, for n up to around
1050 we have to apply standard differential privacy mechanism.
Moreover, for n greater than approximately 1350 we know from
our previous results that noise is unnecessary, because the data has
sufﬁcient privacy parameters due to inherent randomness. Most