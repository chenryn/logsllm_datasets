‚ A generator of random secrets x P X with |X| ă 8, drawn
independently from a fixed but unknown distribution PXpxq;
‚ a channel that returns an observable y P Y with |Y| ă 8
for every input x, according to a conditional distribution
PY|Xpy|xq, also fixed and unknown;
‚ a learning machine capable of implementing a set of rules
f P H, where H denotes the class of functions f : Y Ñ W
and W is the finite set of guesses.
Moreover let us note that
д : W ˆ X Ñ ra, bs
(4)
where a and b are finite real values, and X and W are finite sets.
The problem of learning the д-vulnerability is that of choosing
the function f : Y Ñ W which maximizes the functional Vpf q,
representing the expected gain, defined as:
f pyq, x
˘
PX Ypx, yq.
Vpf q def“
ÿ
`
(5)
д
px,yqPXˆY
Note that f pyq corresponds to the “guess” w, for the given y, in (2).
The maximum of Vpf q is the д-vulnerability, namely:
Vд
def“ max
f PH Vpf q.
(6)
3.2 Principle of the empirical д-vulnerability
maximization
Since we are in the black box scenario, the joint probability distribu-
tion PX Y ” πŻC is unknown. We assume, however, the availability
of m independent and identically distributed (i.i.d.) samples drawn
according to PX Y that can be used as a training set Dm to solve
(
the maximization of f over H and additionally n i.i.d. samples are
available to be used as a validation2 set Tn to estimate the average
(
px1, y1q, . . . ,pxm, ymq
in (5). Let us denote these sets as: Dm
and Tn
In order to maximize the д-vulnerability functional (5) for an un-
known probability measure PX Y , the following principle is usually
applied. The expected д-vulnerability functional Vpf q is replaced
by the empirical д-vulnerability functional:
␣
pxm`1, ym`1q, . . . ,pxm`n, ym`nq
, respectively.
␣
def“
def“
˘
f pyq, x
,
`
д
n
px,yqPTn
pVnpf q def“ 1
ÿ
“pVnpf q
‰
pVmpf q, pVmpf q def“ 1
which is evaluated on Tn rather than PX Y . This estimator is clearly
unbiased in the sense that E
Let f ‹
f ‹
m
m denote the empirical optimal rule given by
def“ arg max
f PH
m
д
px,yqPDm
“ Vpf q.
ÿ
`
f pyq, x
,
(8)
functions f : Y Ñ W to approximate Vд by maximizingpVmpf q
which is evaluated on Dm rather than PX Y . The function f ‹
m is
the optimizer according to Dm, namely the best way among the
over the class of functions H. This principle is known in statistics
as the Empirical Risk Maximization (ERM).
Intuitively, we would like f ‹
ÿ
˘
the д-vulnerability, in the sense that its expected gain
PX Ypx, yq
f ‹
mpyq, x
mq “
Vpf ‹
m to give a good approximation of
`
д
px,yqPXˆY
should be close to Vд. Note that the difference
f PH Vpf q ´ Vpf ‹
mq
Vд ´ Vpf ‹
mq “ max
(9)
(10)
(7)
˘
estimationpVnpf ‹
|Vд ´pVnpf ‹
is always non negative and represents the gap by selecting a possible
suboptimal function f ‹
m. Unfortunately, we are not able to compute
Vpf ‹
mq either, since PX Y is unknown. In its place, we have to use its
mq|.
Note that:
mq Hence, the real estimation error is|Vд´pVnpf ‹
mq| ď pVд ´ Vpf ‹
(11)
2We call Tn validation set rather than test set, since we use it to estimate the д-
vulnerability with the learned f ‹
m, rather than to measure the quality of f ‹
m.
mq ´ Vpf ‹
mq|,
mqq ` |pVnpf ‹
where the first term in the right end side represents the error in-
duced by using the trained model f ‹
m and the second represents the
error induced by estimating the д-vulnerability over the n samples
in the validation set.
study two main questions:
ditions for its statistical consistency?
By using basics principles from statistical learning theory, we
‚ When does the estimatorpVnpf ‹
‚ How well doespVnpf ‹
mq work? What are the con-
mq approximate Vд? In other words, how
fast does the sequence of largest empirical g-leakage values
converge to the largest g-leakage function? This is related
to the so called rate of generalization of a learning machine
that implements the ERM principle.
2
3.3 Bounds on the estimation accuracy
f “ Varpдpf pYq, Xqq, where
In the following we use the notation σ
VarpZq stands for the variance of the random variable Z. Namely,
2
is the variance of the gain for a given function f . We also use P
σ
f
to represent the probability induced by sampling the training and
validation sets over the distribution PX Y .
Next proposition, proved in Appendix B.1, states that we can
probabilistically delimit the two parts of the bound in (11) in terms
of the sizes m and n of the training and validation sets.
Proposition 3.1 (Uniform deviations). For all ε ą 0,
¯
ˇˇ ě ε
mq ´ Vpf ‹
mq
ď 2E exp
2
n ε
` 2pb´aqε{3
2 σ
2
f ‹
m
where the expectation is taken w.r.t. the random training set, and
˘
ÿ
f PH
mq ě ε
ď 2
exp
2
m ε
8σ
2
f ` 4pb´aqε{3
of a validation set in pVnpf ‹
m learned using the training setpVmpf ‹
Inequality (12) shows that the estimation error due to the use
mq instead of the true expected gain
Vpf ‹
mq vanishes with the number of validation samples. On the
other hand, expression (13) implies ‘learnability’ of an optimal f ,
i.e., the suboptimality of f ‹
mq
vanishes with the number of training samples.
On the other hand the bound in eq. (13) depends on the un-
derlying distribution and the structural properties of the class H
through the variance. However, it does not explicitly depend on
the optimization algorithm (e.g., stochastic gradient descent) used
to learn the function f ‹
m from the training set, which just comes
from assuming the worst-case upper bound over all optimization
procedures. The selection of a “goodâĂİ subset of candidate func-
tions, having a high probability of containing an almost optimal
classifier, is a very active area of research in ML [16], and hopefully
there will be some result available soon, which together with our
results will provide a practical method to estimate the bound on
the errors. In appendix F we discuss heuristics to choose a “good”
model, together with a new set of experiments showing the impact
of different architectures.
P
´ˇˇpVnpf ‹
`
Vд ´ Vpf ‹
P
¨˝´
¨˝´
˛‚,
(12)
(13)
˛‚.
Theorem 3.2. The averaged estimation error of the д-vulnerability
Corollary 3.4. The sample complexity of the ERM algorithm
д-vulnerability is bounded from above by the set of values satisfying:
(14)
Mpε, δq ď 8 σ
Npε, δq ď 2 σ
2 ` 4pb´aqε{3
2 ` 2pb´aqε{3
2
ε
2
ε
for all ∆ such that 0 ă ∆ ă δ.
˙
ˆ
ˆ
ln
ln
2|H|
˙
δ ´ ∆
2
∆
,
,
(20)
(21)
E
can be bounded as follows:
‰
mq
ˇˇVд ´pVnpf ‹
ˇˇ ď Vд ´ E
“
Vpf ‹
mq
˘
`
ˆ
ˇˇ ď 4η
ˇˇVpf ‹
дpf pYq, Xq
mq ´pVnpf ‹
c
mq
2 “ maxf PHVar
drawn
exp
E
n
` E
ˇˇ,
mq
ˇˇVpf ‹
mq ´pVnpf ‹
˙
PX Y .
˜
2
´nσ
2η
¸
gffe2σ
2
where the expectations are understood over all possible training and
validation
Furthermore,
let σ
according
, then :
sets
to
2
2σ
m
m
m
`
`
‰
4σ
erf
erf
exp
(16)
˙
σ 2{
ˆ
ηπ
n
(15)
,
˛‚,
with erfpθq def“ 2?
π
d
Vд ´ E
|H|
where η “ p1 ` pb´aq{3q for σ
ηπ
n
2 ď ε, and, otherwise,
2
´ mσ
4p1 ` ηq
2p1 ` ηqπ
“
¨˝σ 2{
ď |H|8p1 ` ηq
d
Vpf ‹
mq
2p1 ` ηqπ
4σ
ż
“
2qdµ.
ˇˇVpf ‹
Vpf ‹
mq
Interestingly, the term Vд ´ E
mq ´pVnpf ‹
is the average error in-
duced when estimating the function f ‹
m using m samples from the
mq
training set while E
incurred when estimating the д-vulnerability using n samples from
the validation set. Clearly, in eq. (15), the scaling of these bounds
with the number of samples are very different which can be made
ˇˇVpf ‹
ˇˇ P O
evident by using the order notation:
mq ´pVnpf ‹
mq
‰(
␣
“
Vpf ‹
Vд ´ E
mq
‰
ˇˇ indicates the average error
1?
|H|?
˙
˙
expp´µ
ˆ
ˆ
sup
PX Y
P O
(17)
(18)
θ
0