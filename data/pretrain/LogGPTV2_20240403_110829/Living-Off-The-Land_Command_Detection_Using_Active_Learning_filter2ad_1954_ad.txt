### Dataset Distribution and Evaluation

The dataset contains a variety of labeled samples across different LOLBIN (Living-Off-The-Land Binaries) and benign classes. The distribution of these samples is detailed in Table 2. This dataset is utilized in Sections 4.2 and 4.3 to evaluate our feature representation and active learning framework.

#### Token Scores

Using the labeled samples, we generate a score for each token to facilitate feature generation. These scores are derived using the method described in Section 3.1, which involves training a token classifier. Higher scores indicate malicious samples, while lower scores suggest benign ones. Table 3 lists the tokens with the highest and lowest scores. Among the suspicious tokens, we observe "aptsimulator" and "attackiq," which are keywords associated with red-team activities and were labeled as malicious. The "ru" token, originating from a domain name extension, indicates the geographic location of the attack. The "temp" token is often associated with temporary files created by attacks. Conversely, among the least suspicious tokens, we find keywords that typically appear in regular software development or sysadmin lifecycles, such as "releases," "install," and "plugin," which align with the rule-based heuristics used for sample selection.

### Feature Representation Evaluation

We trained classifiers using the labeled samples to measure detection performance based on the features corresponding to the command lines. Multiple experiments were conducted to determine the most effective embedding methods and feature sets. We tested two multi-class classifiers: a linear logistic regression classifier and a non-linear ensemble classifier (Random Forest). The Random Forest classifier, which performed better in terms of accuracy, F1 score, precision, and recall, was selected for further analysis. We used a Random Forest model with 20 trees, employing 10-fold cross-validation and stratified splits to maintain the percentage of samples in each class.

#### Embedding Model Evaluation

Unsupervised embedding model training was performed for both word2vec and fastText using the All Instances dataset. This dataset includes millions of command line samples, comprising 358 million words and 2 million unique words (tokens), and is unlabeled. We set the minimum count for including tokens in the dictionary to 5, replacing tokens with fewer occurrences with the "rare" token keyword, resulting in 271K unique words. The context window was set to 5, and the embedding dimension to 16. Both models were trained for 20 epochs after hyperparameter tuning with cross-validation.

After training, we evaluated the performance of the multi-class Random Forest classifier using the labeled samples. As shown in Table 4, the word2vec model achieved an overall F1 score of 0.94, while fastText achieved an F1 score of 0.96, with false positive rates of 0.02 and 0.027, respectively. Although the results are comparable, we chose to use fastText due to its superior generalization of token embeddings and its ability to vectorize out-of-dictionary tokens rather than pooling them into the "rare" category. This distinction is particularly beneficial in real-world settings where large numbers of samples need to be processed. The results also indicate that the classifier is more successful at detecting attacks in certain LOLBIN classes, particularly showing slightly lower accuracy for the Msbuild class, which has the fewest labeled samples. Even security experts face challenges in correctly labeling commands in this class. These experiments demonstrate that embedding-based approaches can effectively distinguish benign samples from several classes of malicious samples, even with a limited set of labeled data. Binary classification experiments, grouping all malicious instances into one class, yielded similar results. In the active learning framework, using multiple classes is advantageous for identifying class-level anomalies, which might not be accurately represented by treating the entire malicious class as a single entity. Therefore, we opted for multi-class classifiers in our active learning approach.

#### Feature Set Comparison

Our defined features can be categorized into two main groups: token scores and aggregated vectors. We conducted classification tasks using different feature sets to assess their importance:
1. **Scores (S)**: Top-20 token scores in the command line.
2. **Vectors (V)**: Aggregated vectors of token embeddings (min-max-mean pooling).
3. **S+V**: Top 3 token scores and aggregated vectors.
4. **S+V(W)**: Top 3 token scores and aggregated vectors (min-max-weighted average pooling using scores as weights).

Using the fastText model, we reported the following accuracy scores: S: 0.94, V: 0.92, S+V: 0.94, S+V(W): 0.96. While the Scores features alone have higher accuracy than the Vectors features, the best performance is achieved when both feature sets are combined, with scores used as weights during pooling. This feature representation will be used in our active learning framework.

### Active Learning Evaluation

We designed our active learning framework, LOLAL, to distinguish malicious LOLBIN commands from legitimate commands. One of the main challenges in this task, similar to other security settings, is the availability of ground truth or labeled data for machine learning applications. The primary advantage of LOLAL is its effectiveness with a limited number of labeled examples, as it selects relevant samples for labeling through analysis, thereby significantly improving the model's performance over time.

Our active learning framework consists of a classifier trained to distinguish several classes of malicious samples from benign samples, along with an anomaly detection module to identify samples for labeling by an analyst. During each iteration, uncertain and anomalous samples are identified and labeled by an analyst. The newly labeled data augments the training data, iteratively improving the classifier's performance. In this section, we use the set of labeled samples to determine how active learning improves performance with more iterations. In practice, an analyst's time is valuable, and we show results over three iterations with a security analyst labeling the data in Section 4.4. Using the labeled dataset, we can evaluate an active learning campaign starting with very few labels. Over multiple iterations, the classifier performance improves significantly, and we demonstrate how quickly it converges to train the optimal classifier.

We use both a linear logistic regression classifier and a gradient boosting classifier for the classification task, along with a naïve Bayes anomaly detection model in the active learning framework. For this experiment, we leverage the set of 1987 labeled samples from Table 2. We start with a small number of 10 labeled samples and select 5 test samples for labeling and inclusion in the training data for each subsequent iteration. Our setup assumes that an analyst would correctly label the selected samples in the presented order. Figure 5 shows the Precision and Percentage of True Positives as several iterations of active learning are performed with the boosting classifier. The plots are generated by averaging 5 runs, as the starting set of labels is picked randomly. We run the algorithms for 50 iterations and observe that convergence is reached in fewer than 30 iterations in all cases. Most importantly, the precision reaches above 0.97 in almost all cases (with the exception of the Msiexec class). Similarly, the recall (Percentage of True Positives) found at each iteration reaches above 0.97 in all cases, as shown in Table 5.

These experiments demonstrate that our active learning framework can train an effective classifier using a very small number of labeled samples, a challenging setting. Note that we use a set of 1987 samples in this labeled dataset. Starting with 10 labels and labeling one sample for each of the five classes for 30 iterations, 30 · 5 = 150 additional samples are labeled, leaving 1827 samples as the unlabeled portion of the test set. This demonstrates that our active learning framework can learn an effective classifier using only 160 labels. We observe some oscillations over time, indicating the classifier correcting itself after learning from new samples and then converging after only 30 iterations. Some classes converge later than others. Initially, the classifier struggles with the Msbuild class because the malicious intent of msbuild.exe is sometimes unclear from the command line, even for human experts. However, the classifier achieves almost perfect precision and recall as more relevant samples are labeled and added to the training set. Overall, this experiment shows how our active learning framework can learn an effective classifier over time, with high precision and recall.

### Comparison of Sample Selection Strategies

To demonstrate the advantages of our sample selection strategy for active learning, we compare it with other labeling strategies. We define the following variants of our active learning tool:
- **LOLAL**: Gradient boosting classifier and naïve Bayes anomaly detection; picking uncertain and anomalous samples in a round-robin fashion.
- **LOLAL-LR**: Logistic regression classifier and naïve Bayes anomaly detection; picking uncertain and anomalous samples in a round-robin fashion.
- **Uncertainty Sampling**: Gradient boosting classifier; picking uncertain samples from each class.
- **Anomaly Sampling**: Gradient boosting classifier and naïve Bayes anomaly detection; picking anomalous samples.