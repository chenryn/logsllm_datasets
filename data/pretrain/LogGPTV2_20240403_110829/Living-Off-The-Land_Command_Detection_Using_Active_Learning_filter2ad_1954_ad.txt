0
labeled samples across the different LOLBIN and Benign classes
in the dataset. The distribution of these samples across the classes
is shown in Table 2. This dataset is used in Sections 4.2 and 4.3 to
evaluate our feature representation and active learning framework.
Token Scores. Using the labeled samples, we generate a score
for each token for feature generation. The scores are generated
following the method described in Section 3.1 by training a token
classifier, with higher scores identifying malicious samples, and
lower scores identifying benign ones. We include the tokens with
the highest and lowest scores in Table 3. Among the suspicious
tokens, we observe “aptsimulator” and “attackiq” which are key-
words indicating red-team activity that were captured and labeled
as malicious. The “ru” token came from the domain name extension
indicating the geo-location of the attack. The “temp” token corre-
sponds to many temporary files usually created by attacks. Among
449Living-Off-The-Land Command Detection
Using Active Learning
the least suspicious tokens, we observe keywords that typically
appear in the regular software development or sysadmin lifecycle
(e.g., “releases”, “install”, “plugin”), which matched patterns in the
rule-based heuristics used for sample selection.
4.2 Feature Representation Evaluation
Using the labeled samples, we trained classifiers to measure the
detection performance using the features corresponding to the
command lines. We performed multiple experiments to determine
which embedding methods and feature sets perform best. We ran
experiments with two multi-class classifiers: a linear logistic re-
gression classifier and a non-linear ensemble classifier (Random
Forest). As Random Forest performs better in terms of accuracy, F1,
precision, and recall metrics, we present results here using Random
Forest with 20 trees. We perform 10-fold cross-validation and use
stratified splits on the labeled data to preserve the percentage of
samples in each class.
Embedding Model Evaluation. We ran the unsupervised em-
bedding model training for both word2vec and fastText using the
All Instances dataset. The data includes millions of command line
samples, consisting of a total of 358 million words and 2 million
unique words (tokens), and is not labeled. We set the minimum
count required to include tokens in the dictionary as 5, and replace
tokens with fewer occurrences with the “rare” token keyword. This
results in 271K unique words. We set the context window as 5 and
the embedding dimension as 16, and trained both methods for 20
epochs, after hyperparameter tuning with cross-validation.
After training the word2vec and fastText models, we measure the
performance of the multi-class Random Forest classifier trained on
the labeled samples. As shown in Table 4, the word2vec model has
an overall F1 score of 0.94, whereas fastText has an overall F1 score
of 0.96, with false positive rates of 0.02 and 0.027, respectively. Al-
though the results are comparable, we decided to use fastText in our
framework. We believe the reason fastText performs slightly better
is due to the better generalization of token embedding and sup-
porting vectorization of out-of-dictionary tokens instead of simply
pooling them into the “rare” category. This distinction will result in
a larger gap when capturing the embeddings in a real-world setting
where vast numbers of samples are going to be processed. The
results also show that the classifier is more successful at detecting
attacks in certain LOLBIN classes. In particular, the classifier accu-
racy is slightly lower for the Msbuild class with the lowest number
of labeled samples, and we found out that in this case even the secu-
rity experts have challenges labeling the command correctly. These
experiments demonstrate that using embedding-based approaches
to represent command lines, multi-class classifiers can distinguish
benign samples from several classes of malicious samples, even
when only a limited set of labeled samples is available. We also
performed binary classification experiments grouping all malicious
instances into one class, and the results were similar. In the active
learning framework, using multiple classes is useful for identify-
ing anomalies per class. Anomalies for the entire malicious class
might not accurately represent class-level anomalies. Therefore, we
decided to use multi-class classifiers for active learning.
Feature Set Comparison. The features we define can be grouped
into two main categories: token scores and aggregated vectors. We
RAID ’21, October 6–8, 2021, San Sebastian, Spain
Table 4: Detection metrics for the classification
experiments using fastText and word2vec embeddings.
Class
Prec
Bitsadmin 0.90
0.99
Certutil
0.90
Msbuild
0.91
Msiexec
0.95
Regsvr32
0.96
Average
fastText
Rec
F1
0.91
0.92
0.99
0.99
0.88
0.85
0.93
0.96
1.00
0.97
0.96
0.96
FPR
0.035
0.022
0.007
0.017
0.022
0.020
Prec
0.89
0.99
0.85
0.88
0.95
0.94
word2vec
Rec
F1
0.89
0.89
0.98
0.98
0.77
0.70
0.91
0.95
0.99
0.97
0.94
0.95
FPR
0.037
0.022
0.008
0.048
0.022
0.027
run the classification task using different sets of features to assess
the importance of each feature group. We define these feature as:
(1) Scores (S): Top-20 token scores in the command line;
(2) Vectors (V): Aggregated vectors of token embeddings (min-
max-mean pooling);
(3) S+V: Top 3 token scores and aggregated vectors;
(4) S+V(W): Top 3 token scores and aggregated vectors (min-
max-weighted average pooling using scores as weights).
We run the same experiments as in the previous section using the
fastText model for these four feature sets, and report the following
accuracy scores: S: 0.94, V: 0.92, S+V: 0.94, S+V(W): 0.96. While
the Scores features by themselves have a better accuracy than the
Vectors features, we observe that the best performance is obtained
when we combine both set of features, and the scores are used as
weights during pooling. We will use this feature representation for
our active learning framework.
4.3 Active Learning Evaluation
We design our active learning framework LOLAL to distinguish
malicious LOLBIN commands from legitimate commands. One of
the main challenges in this task, similar to other security settings, is
the availability of ground truth or labeled data for machine learning
application. The main advantage of LOLAL is that it is effective
starting from a limited number of labeled examples, as the active
learning selects relevant samples for labeling through analysis,
which substantially improve the model’s performance over time.
Our active learning framework consists of a classifier trained to dis-
tinguish several classes of malicious samples from benign samples,
as well as an anomaly detection module used to identify samples
for labeling by an analyst. During each iteration, uncertain and
anomalous samples are identified to be labeled by an analyst. The
newly-labeled data augments the training data available to the clas-
sifier, improving its performance iteratively. In this section, we
use the set of labeled samples to determine how active learning
improves in performance as more iterations are performed. In prac-
tice, an analyst’s time is valuable, and we show results over three
iterations with a security analyst labeling the data in Section 4.4.
Using the labeled dataset, we can evaluate an active learning cam-
paign starting with very few labels. Over multiple iterations, the
classifier performance improves significantly, and we show how
quickly it converges to train the optimal classifier.
We use both a linear logistic regression classifier, as well as a
boosting classifier, gradient boosting, for the classification task.
We use the naïve bayes anomaly detection model in the active
learning framework. For this experiment, we leverage the set of
450RAID ’21, October 6–8, 2021, San Sebastian, Spain
Ongun, et al.
Figure 5: LOLAL framework results for different LOLBIN classes.
The Percentage of True Positive Metric and Precision increases with the number of iterations.
1987 labeled samples from Table 2. We start with a very small
number of 10 labeled samples and select at each iteration 5 test
samples for labeling and inclusion in the training data for the next
iteration. Our setup assumes that an analyst would correctly label
the selected samples in the presented order. We show in Figure 5 the
Precision and Percentage of True Positives found as several iterations
of active learning are performed with the boosting classifier. The
plots are generated by averaging 5 runs as the starting set of labels
are picked randomly. We run the algorithms for 50 iterations and
observe that convergence is reached faster than 30 iterations in all
cases. Most importantly, the precision reaches above 0.97 in almost
all cases (with the exception of the Msiexec class). Similarly, the
recall (Percentage of True Positives) found at each iteration reaches
above 0.97 in all cases, as shown in Table 5.
These experiments show that our active learning framework is
able to train an effective classifier using a very small number of
labeled samples, which is a very challenging setting. Note that we
use a set of 1987 samples in this labeled dataset. Starting with 10
labels and labeling one sample for each of the five classes for 30
iterations, 30 · 5 = 150 additional samples are labeled, whereas the
remaining 1827 samples represent the unlabeled, portion of the
test set. This demonstrates that our active learning framework is
able to learn an effective classifier using 160 labels. We observe
some oscillations over time, which indicate the classifier correcting
itself after learning from new samples, and then converging after
only 30 iterations. We also observe that some classes converge later
than others. Initially, the classifier has difficulty with the Msbuild
class since the malicious intent of msbuild.exe is sometimes not
clear from looking only at the command line, even for human
experts. The difficulty of detection is, by nature, class dependent.
Nonetheless, the classifier gets almost perfect precision and recall
as more relevant samples are labeled and added to the training set.
Overall, this experiment shows how our active learning framework
is able to learn an effective classifier over time, with high precision
and recall.
We now compare our sample selection strategy for active learn-
ing with other labeling strategies to demonstrate the advantages
Table 5: Comparison of the LOLAL classifier evaluation for
different classes after 5 and 30 iterations.
Class
Iter 5
Bitsadmin 0.61
0.92
Certutil
0.62
Msbuild
0.68
Msiexec
0.94
Regsvr32
Prec %TP
0.83
0.93
0.78
0.52
0.82
Iter 30
Prec %TP
0.97
0.97
0.98
0.98
1.0
1.0
1.0
0.88
0.97
0.99
of the sample selection strategy used by LOLAL. We define the
following variants of our active learning tool:
• LOLAL: gradient boosting classifier and naïve bayes anom-
aly detection; picking uncertain and anomalous samples in
a round-robin fashion.
• LOLAL-LR: logistic regression classifier and naïve bayes
anomaly detection; picking uncertain and anomalous sam-
ples in a round-robin fashion.
• Uncertainty Sampling: gradient boosting classifier; pick-
• Anomaly Sampling: gradient boosting classifier and
ing uncertain samples from each class.
naïve bayes anomaly detection; picking anomalous samples