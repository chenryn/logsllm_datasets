title:Can you see me now?: a measurement study of Zoom, Webex, and Meet
author:Hyunseok Chang and
Matteo Varvello and
Fang Hao and
Sarit Mukherjee
Can You See Me Now? A Measurement Study of Zoom,
Webex, and Meet
Hyunseok Chang
Nokia Bell Labs
Murray Hill, NJ, USA
Matteo Varvello
Nokia Bell Labs
Murray Hill, NJ, USA
ABSTRACT
Since the outbreak of the COVID-19 pandemic, videoconfer-
encing has become the default mode of communication in our
daily lives at homes, workplaces and schools, and it is likely
to remain an important part of our lives in the post-pandemic
world. Despite its significance, there has not been any sys-
tematic study characterizing the user-perceived performance
of existing videoconferencing systems other than anecdotal
reports. In this paper, we present a detailed measurement
study that compares three major videoconferencing systems:
Zoom, Webex and Google Meet. Our study is based on 48
hours’ worth of more than 700 videoconferencing sessions,
which were created with a mix of emulated videoconferencing
clients deployed in the cloud, as well as real mobile devices
running from a residential network. We find that the exist-
ing videoconferencing systems vary in terms of geographic
scope, which in turns determines streaming lag experienced
by users. We also observe that streaming rate can change
under different conditions (e.g., number of users in a ses-
sion, mobile device status, etc), which affects user-perceived
streaming quality. Beyond these findings, our measurement
methodology can enable reproducible benchmark analysis for
any types of comparative or longitudinal study on available
videoconferencing systems.
CCS CONCEPTS
• Networks → Network measurement; Cloud computing; •
Information systems → Collaborative and social computing
systems and tools.
ACM Reference Format:
Hyunseok Chang, Matteo Varvello, Fang Hao, and Sarit Mukherjee.
2021. Can You See Me Now? A Measurement Study of Zoom, We-
bex, and Meet. In ACM Internet Measurement Conference (IMC
’21), November 2–4, 2021, Virtual Event, USA. ACM, New York,
NY, USA, 13 pages. https://doi.org/10.1145/3487552.3487847
Permission to make digital or hard copies of all or part of this work
for personal or classroom use is granted without fee provided that
copies are not made or distributed for profit or commercial advantage
and that copies bear this notice and the full citation on the first page.
Copyrights for components of this work owned by others than the au-
thor(s) must be honored. Abstracting with credit is permitted. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee. Request permissions
from permissions@acm.org.
IMC ’21, November 2–4, 2021, Virtual Event, USA
© 2021 Copyright held by the owner/author(s). Publication rights
licensed to ACM.
ACM ISBN 978-1-4503-9129-0/21/11. . . $15.00
https://doi.org/10.1145/3487552.3487847
216
Fang Hao
Nokia Bell Labs
Sarit Mukherjee
Nokia Bell Labs
Murray Hill, NJ, USA
Murray Hill, NJ, USA
1
INTRODUCTION
There is no doubt that the outbreak of the COVID-19 pan-
demic has fundamentally changed our daily lives. Especially
with everyone expected to practice physical distancing to
stop the spread of the pandemic, various online communi-
cation tools have substituted virtually all sorts of in-person
interactions. As the closest form of live face-to-face com-
munication in a pre-pandemic world, videoconferencing has
practically become the default mode of communication (e.g.,
an order-of-magnitude increase in videoconferencing traffic
at the height of the pandemic [23, 28, 37]). Thanks to its
effectiveness and reliability, video communication is likely
to remain an important part of our lives even in the post-
pandemic world [27, 32].
Despite the critical role played by existing videoconferenc-
ing systems in our day-to-day communication, there has not
been any systematic study on quantifying their performance
and Quality of Experience (QoE). There is no shortage of
anecdotal reports and discussions in terms of the usability,
video quality, security, and client resource usage of individual
systems. To the best of our knowledge, however, no scientific
paper has yet investigated the topic thoroughly with a sound
measurement methodology that is applicable across multi-
ple available systems. Our main contribution in this paper
addresses this shortcoming.
In this paper, we shed some light on the existing videocon-
ferencing ecosystems by characterizing their infrastructures
as well as their performance from a user’s QoE perspective.
To this end, we have devised a measurement methodology
which allows us to perform controlled and reproducible bench-
marking of videoconferencing systems by leveraging a mix of
emulated videoconferencing clients deployed in the cloud, as
well as real mobile devices running from a residential network.
We provide the detailed description of our methodology as
well as the open-source tools we used (Sections 3 and 4), so
that anyone can replicate our testbed to repeat or further
extend our benchmark scenarios. Driven by our methodology,
we investigate three popular videoconferencing systems on
the market: Zoom, Webex and Google Meet (Meet for short).
Each of these platforms provides a free-tier plan as well as
paid subscriptions, but we focus on their free-tier plans in
our evaluation. Given these three systems, we conduct mea-
surement experiments which take a combined total of 48
videoconferencing hours over more than 700 sessions, with
200 VM hours rented from 12 geographic locations and 18
hours of two Android phones hooked up at one location. Our
findings include:
IMC ’21, November 2–4, 2021, Virtual Event, USA
Hyunseok Chang, Matteo Varvello, Fang Hao, and Sarit Mukherjee
Finding-1. In the US, typical streaming lag experienced by
users is 20–50 ms for Zoom, 10–70 ms for Webex, and 40–
70 ms for Meet. This lag largely reflects the geographic sepa-
ration of users (e.g., US-east vs. US-west). In case of Webex,
all sessions created in the US appear to be relayed via its
infrastructure in US-east. This causes the sessions among
users in US-west to be subject to artificial detour, inflating
their streaming lag.
Finding-2. Zoom and Webex are characterized by a US-based
infrastructure. It follows that sessions created in Europe
experience higher lag than those created in the US (90–150 ms
for Zoom, and 75–90 ms for Webex). On the other hand, the
sessions created in Europe on Meet exhibit smaller lag (30–
40 ms) due to its cross-continental presence including Europe.
Finding-3. All three systems appear to optimize their stream-
ing for low-motion videos (e.g., a single-person view with a
stationary background). Thus high-motion video feeds (e.g.,
dynamic scenes in outdoor environments) experience non-
negligible QoE degradation compared to typical low-motion
video streaming.
Finding-4. Given the same camera resolution, Webex sessions
exhibit the highest traffic rate for multi-user sessions. Meet
exhibits the most dynamic rate changes across different ses-
sions, while Webex maintains virtually constant rate across
sessions.
Finding-5. Videoconferencing is an expensive task for mobile
devices, requiring at least 2–3 full cores to work properly.
Meet is the most bandwidth-hungry client, consuming up
to one GB per hour, compared to Zoom’s gallery view that
only requires 175 MB per hour. We estimate that one hour’s
videoconferencing can drain up to 40% of a low-end phone’s
battery, which can be reduced to about 20–30% by turning
off the onboard camera/screen and relying only on audio.
All videoconferencing clients scale well with the number of
call participants, thanks to their UI which only displays a
maximum of four users at a time.
The rest of the paper is organized as follows. We start
by introducing related works in Section 2. We then present
our measurement methodology in Section 3, and describe
the measurement experiments and our findings in detail in
Sections 4–5. We conclude in Section 6 by discussing several
research issues.
2 RELATED WORK
Despite the prevalence of commercial videoconferencing sys-
tems [36], no previous work has directly compared them with
respect to their infrastructures and end-user QoE, which is
the main objective of this paper. The recent works by [34]
and [29] investigate the network utilization and bandwidth
sharing behavior of existing commercial videoconferencing
systems based on controlled network conditions and client
settings. Several works propose generic solutions to improve
videoconferencing. For example, Dejavu [25] offers up to 30%
Videoconferencing system Low quality High quality
600 Kbps
Zoom [7]
Webex [8]
Meet [14]
500 Kbps
1 Mbps
2.5 Mbps
2.6 Mbps
Table 1: Minimum bandwidth requirements for one-on-one
calls.
bandwidth reduction, with no impact on QoE, by leveraging
the fact that recurring videoconferencing sessions have lots
of similar content, which can be cached and re-used across
sessions. Salsify [24] relies on tight integration between a
video codec and a network transport protocol to dynamically
adjust video encodings to changing network conditions.
As a consequence of the COVID-19 pandemic, the research
community has paid more attention to the impact of video-
conferencing systems on the quality of education [21, 33, 40].
As educational studies, these works rely on usability analysis
and student surveys. In contrast, our work characterizes QoE
performance of the videoconferencing systems using purely
objective metrics.
Videoconferencing operators do not provide much informa-
tion about their system, e.g., footprint and encoding strate-
gies. One common information reported by each operator is
the minimum bandwidth requirements for one-on-one calls
(Table 1). The results from our study are not only consis-
tent with these requirements, but also cover more general
scenarios such as multi-party sessions.
3 BENCHMARKING DESIGN
In this section, we describe the benchmarking tool we have
designed to study existing commercial videoconferencing
systems. We highlight key design goals for the tool first,
followed by associated challenges, and then describe how we
tackle the challenges in our design.
A videoconferencing system is meant to be used by end-
users in mobile or desktop environments that are equipped
with a camera and a microphone. When we set out to design
our benchmarking tool for such systems, we identify the
following design goals.
(D1) Platform compliance: We want to run our benchmark
tests using unmodified official videoconferencing clients with
full audiovisual capabilities, so that we do not introduce any
artifact in our evaluation that would be caused by client-side
incompatibility or deficiency.
(D2) Geo-distributed deployment: To evaluate web-scale video-
conferencing services in realistic settings, we want to collect
data from geographically-distributed clients.
(D3) Reproducibility: We want to leverage a controlled, re-
producible client-side environment, so that we can compare
available videoconferencing systems side-by-side.
(D4) Unified evaluation metrics: We want to evaluate different
videoconferencing platforms based on unified metrics that
are applicable across all the platforms.
It turns out that designing a benchmarking tool that meets
all the stated goals is challenging because some of these goals
217
Can You See Me Now? A Measurement Study of Zoom, Webex, and Meet
IMC ’21, November 2–4, 2021, Virtual Event, USA
are in fact conflicting. For example, while geographically-
dispersed public clouds can provide distributed vantage point
environments (D2), cloud deployments will not be equipped
with necessary sensory hardware (HHD1). Crowd-sourced end-
users can feed necessary audiovisual data into the videocon-
ferencing systems (D1), but benchmarking the systems based
on unpredictable human behavior and noisy sensory data will
not give us objective and reproducible comparison results
(HHD3). On the other hand, some goals such as (D3) and (D4) go
hand in hand. Unified evaluation metrics alone are not suffi-
cient for comparative analysis if reproducibility of client-side
environments is not guaranteed. At the same time, repro-
ducibility would not help much without platform-agnostic
evaluation metrics.
3.1 Design Approach
Faced with the aforementioned design goals and challenges,
we come up with a videoconferencing benchmark tool that
is driven by three main ideas: (i) client emulation, (ii) coor-