**Title: Can You See Me Now? A Measurement Study of Zoom, Webex, and Meet**

**Authors:**
- Hyunseok Chang
- Matteo Varvello
- Fang Hao
- Sarit Mukherjee

**Affiliations:**
- Nokia Bell Labs, Murray Hill, NJ, USA

**Abstract:**
The COVID-19 pandemic has made videoconferencing a fundamental part of daily communication in homes, workplaces, and schools. Despite its importance, there has been a lack of systematic studies characterizing the user-perceived performance of these systems. This paper presents a detailed measurement study comparing three major videoconferencing platforms: Zoom, Webex, and Google Meet. Our study is based on over 700 videoconferencing sessions, totaling 48 hours, conducted using a combination of emulated clients in the cloud and real mobile devices on residential networks. We find that the geographic scope of these systems significantly affects streaming lag. Additionally, streaming rates vary under different conditions, such as the number of users in a session and the status of mobile devices, which impacts user-perceived quality. Our methodology provides a reproducible benchmark for comparative and longitudinal studies of videoconferencing systems.

**CCS Concepts:**
- Networks → Network measurement; Cloud computing
- Information systems → Collaborative and social computing systems and tools

**ACM Reference Format:**
Chang, H., Varvello, M., Hao, F., & Mukherjee, S. (2021). Can You See Me Now? A Measurement Study of Zoom, Webex, and Meet. In ACM Internet Measurement Conference (IMC '21), November 2–4, 2021, Virtual Event, USA. ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3487552.3487847

**Introduction:**
The COVID-19 pandemic has fundamentally altered our daily lives, with physical distancing measures leading to a significant increase in the use of online communication tools. Videoconferencing, in particular, has become the default mode of communication, seeing a substantial rise in traffic during the pandemic [23, 28, 37]. Given its effectiveness and reliability, videoconferencing is likely to remain an important part of our post-pandemic world [27, 32].

Despite the critical role of videoconferencing systems, there has been no systematic study to quantify their performance and Quality of Experience (QoE). While anecdotal reports and discussions about usability, video quality, security, and client resource usage are abundant, no scientific study has thoroughly investigated these aspects using a sound and reproducible measurement methodology. This paper addresses this gap by providing a detailed characterization of the infrastructures and performance of three popular videoconferencing systems: Zoom, Webex, and Google Meet.

Our methodology, described in Sections 3 and 4, involves a mix of emulated videoconferencing clients deployed in the cloud and real mobile devices on residential networks. This allows for controlled and reproducible benchmarking. We conducted experiments totaling 48 hours across more than 700 sessions, with 200 VM hours from 12 geographic locations and 18 hours of two Android phones at one location. Our key findings include:

1. **Streaming Lag:** In the US, typical streaming lag is 20–50 ms for Zoom, 10–70 ms for Webex, and 40–70 ms for Meet. This lag is influenced by the geographic separation of users. For Webex, all US sessions are relayed via its US-east infrastructure, causing higher lag for US-west users.
   
2. **Geographic Impact:** Zoom and Webex have a US-based infrastructure, resulting in higher lag for European sessions (90–150 ms for Zoom, 75–90 ms for Webex). In contrast, Meet's cross-continental presence, including Europe, results in lower lag (30–40 ms) for European sessions.
   
3. **Video Optimization:** All three systems optimize for low-motion videos, leading to noticeable QoE degradation for high-motion videos.
   
4. **Traffic Rate:** Webex exhibits the highest traffic rate for multi-user sessions, while Meet shows the most dynamic rate changes. Webex maintains a nearly constant rate across sessions.
   
5. **Resource Usage:** Videoconferencing is resource-intensive for mobile devices, requiring 2–3 full cores. Meet is the most bandwidth-hungry, consuming up to 1 GB per hour, compared to Zoom's gallery view, which requires only 175 MB per hour. One hour of videoconferencing can drain up to 40% of a low-end phone's battery, which can be reduced to 20–30% by turning off the camera and screen.

**Related Work:**
While commercial videoconferencing systems are prevalent, no previous work has directly compared them in terms of infrastructure and end-user QoE. Recent studies [34, 29] focus on network utilization and bandwidth sharing, but do not address QoE. Other works propose generic solutions to improve videoconferencing, such as Dejavu [25], which reduces bandwidth by 30% without impacting QoE, and Salsify [24], which dynamically adjusts video encodings to changing network conditions. Educational studies [21, 33, 40] have also examined the impact of videoconferencing on education, relying on usability analysis and student surveys. Our work, in contrast, uses objective metrics to characterize QoE performance.

**Benchmarking Design:**
We designed a benchmarking tool to study commercial videoconferencing systems, focusing on the following goals:
- **Platform Compliance:** Use unmodified official clients to avoid artifacts.
- **Geo-distributed Deployment:** Collect data from geographically distributed clients.
- **Reproducibility:** Ensure a controlled, reproducible client-side environment.
- **Unified Evaluation Metrics:** Evaluate systems using consistent metrics.

Designing a tool that meets these goals is challenging due to conflicting requirements. For example, while cloud deployments provide distributed vantage points, they lack necessary sensory hardware. Conversely, crowd-sourced end-users can provide audiovisual data but introduce variability and noise. To address these challenges, our design approach includes client emulation, coordinated testing, and a unified evaluation framework.

**Conclusion:**
This paper provides a comprehensive measurement study of Zoom, Webex, and Google Meet, highlighting the importance of geographic factors, video optimization, and resource usage. Our methodology offers a reproducible benchmark for future studies, enabling further research into the performance and QoE of videoconferencing systems.