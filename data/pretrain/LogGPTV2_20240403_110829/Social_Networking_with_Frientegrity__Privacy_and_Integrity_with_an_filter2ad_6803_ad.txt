5.3 Dealing with Conﬂicts
When multiple clients submit operations concurrently, con-
ﬂicts can occur. Because servers do not have access to
the operations’ plaintexts, Frientegrity delegates conﬂict
resolution to the clients, which can employ a number of
strategies, such as last-writer-wins, operational transfor-
mation [21], or custom merge procedures [55]. In practice,
however, many kinds of updates in social networking sys-
tems, such as individual wall posts, are append operations
that are inherently commutative, and thus require no spe-
cial conﬂict resolution.
5.4 Public Feeds with Many Followers
Well-known individuals and organizations often use their
feeds on online social networks to disseminate information
to the general public. These feeds are not conﬁdential, but
they would still beneﬁt from a social networking system
that protected their integrity. Such feeds pose scalability
challenges, however, because they can have as many as
tens of millions of followers.
Fortunately, Frientegrity can be readily adapted to sup-
port these feeds eﬃciently. Because the object correspond-
ing to such a feed does not need to be encrypted, its ACL
does not need to store encryption keys. The ACL is only
needed to verify that every operation in the object came
from an authorized writer. As a result, the size of the
object’s ACL need only be proportional to the number of
users with write access to the object, which is often only a
single user, rather than to the total number of followers.
Popular feeds would also not prevent applications from
using dependencies to represent retweets in the manner
described in §4.2. Suppose that Alice retweets a post from
the feed of a famous individual, such as Justin Bieber.
10
Then, in such a design, the application would establish
a dependency from Alice’s feed to Justin Bieber’s. But
because dependencies only modify the source object (in
this case Alice’s feed), they would not impose any ad-
ditional performance penalty on reads of Justin Bieber’s
feed. Thus, even if Justin Bieber’s posts are frequently
retweeted, Frientegrity could still serve his feed eﬃciently.
Implementation
6.
To evaluate Frientegrity’s design, we implemented a pro-
totype that simulates a simpliﬁed Facebook-like service.
It consists of a server that hosts a set of user proﬁles and
clients that fetch, verify, and update them. Each user
proﬁle is comprised of an object representing the user’s
“wall,” as well as an ACL and ACL history object repre-
senting the user’s list of friends. The wall object is made
up of operations, each of which contains an arbitrary byte
string, that have been submitted by the user or any of her
friends. The client acts on behalf of a user and can perform
RPCs on the server to read from and write to the walls of
the user or user’s friends, as well as to update the user’s
ACL. The client can simulate the work required to build
a Facebook-like “news feed” by fetching and verifying
the most recent updates to the walls of each of the user’s
friends in parallel.
Our prototype is implemented in approximately 4700
lines of Java code (per SLOCCount [58]) and uses the
protobuf-socket-rpc [16] library for network communica-
tion. To support the history trees contained in the wall and
ACL history objects, we use the reference implementation
provided by Crosby et al. [13].
Because Frientegrity requires every operation to be
signed by its author and every server commitment to be
signed by the provider, high signature throughput is a
priority. To that end, our prototype uses the Network Se-
curity Services for Java (JSS) library from Mozilla [42] to
perform 2048-bit RSA signatures because, unlike Java’s
default RSA implementation, it is written in native code
and oﬀers signiﬁcantly better performance. In addition,
rather than signing and verifying each operation or server
commitment individually, our prototype signs and veriﬁes
them in batches using spliced signatures [10, 13]. In so do-
ing, we improve throughput by reducing the total number
of cryptographic operations at the cost of a small potential
increase in the latency of processing a single message.
7. Experimental Evaluation
Social networking applications place a high load on
servers, and they require reasonably low latency in the
face of objects containing tens of thousands of updates
and friend lists reaching into the hundreds and thousands.
This section examines how our Frientegrity prototype per-
forms and scales under these conditions.
Figure 4: Distribution of post rates for Twitter users. 1% of
users post at least 14 times a day, while 0.1% post at least 56
times a day.
All tests were performed on machines with dual 4-core
Xeon E5620 processors clocked at 2.40 GHz, with 11 GB
of RAM and gigabit network interfaces. Our evaluation
ran with Oracle Java 1.6.0.24 and used Mozilla’s JSS cryp-
tography library to perform SHA256 hashes and RSA
2048 signatures. All tests, unless otherwise stated, ran
with a single client machine issuing requests to a separate
server machine on the same local area network, and all
data is stored in memory. A more realistic deployment
over the wide-area would include higher network latencies
(typically an additional tens to low hundreds of millisec-
onds), as well as backend storage access times (typically
in low milliseconds in datacenters). These latencies are
common to any Web service, however, and so we omit any
such synthetic overhead in our experiments.
7.1 Single-object Read and Write Latency
To understand how large object histories may get in prac-
tice, we collected actual social network usage data from
Twitter by randomly selecting over 75,000 users with pub-
lic proﬁles. Figure 4 shows the distribution of post rates
for Twitter users. While the majority of users do not tweet
at all, the most active users post over 200 tweets per day,
leading to tens of thousands of posts per year.
To characterize the eﬀect of history size on read and
write latency, we measured performance of these opera-
tions as the history size varies. For each read, the client
fetched an object containing the ﬁve most recent opera-
tions along with any other required to verify the object.
As shown in Figure 5, write latency was approximately
10 ms (as it includes both a server and client signature
in addition to hashing), while read latency was approxi-
mately 6 ms (as it includes a single signature veriﬁcation
and hashing). The Figure’s table breaks down median
request latency to its contributing components. As ex-
pected, a majority of the time was spent on public-key
operations; a faster signature veriﬁcation implementation
or algorithm would correspondingly increase performance.
While the latency here appears constant, independent of
the history size, the number of hash veriﬁcations actually
grows logarithmically with the history. This observed be-
havior arises because, at least up to histories of 25,000
11
0.01.1110100Rate(tweets/day)0.01%0.1%1%10%100%UserCCDFObject
ACL
Total Overhead
Signatures
History Tree Hashes
Dependency Annotations
Other Metadata
ACL PAD
Signatures in ACL History
Hashes in ACL History Tree
Other Metadata
7210 B
640 B
224 B
1014 B
453 B
1531 B
32 B
226 B
11300 B
Table 1: Sources of network overhead of a typical read of
an object’s ﬁve most recent updates.
size, as shown in Figure 6. Given this linear growth in
latency, verifying an object with history size of 25,000
operations would take approximately 10 s in the implemen-
tation based on a hash chain compared to Frientegrity’s
6 ms.
The performance of hash chains could be improved by
having clients cache the results of previous veriﬁcations
so they would only need to verify subsequent operations.
Even if clients were stateful, however, Figure 4 shows that
fetching the latest updates of the most proliﬁc users would
still require hundreds of veriﬁcations per day. Worse still,
following new users or switching between client devices
could require tens of thousands of veriﬁcations.
7.2 Network Overhead
When network bandwidth is limited, the size of the mes-
sages that Frientegrity sends over the network can impact
latency and throughput. To understand this eﬀect, we
measure the overhead that Frientegrity’s veriﬁcation and
access control mechanisms add to an object that is fetched.
Table 1 provides a breakdown of the sources of overhead
in a read of the ﬁve most recent operations in an object.
The object is comprised of 100 operations all created by
a single writer. We assume that the ACL that applies to
the object only contains a single user and his associated
encrypted key and that the ACL history object contains
only two operations (an initial operation and the operation
that added the single user).
As shown in Table 1, the total overhead added by Fri-
entegrity is 11,300 B, which would add approximately
90 ms of download time on a 1 Mbps link. Not surpris-
ingly, the majority of the overhead comes from the signa-
tures on individual operations and in prevCommitments.
The object history tree contains 14 signatures, and the
ACL history contains another four. Together, this many
2048-bit RSA bare signatures would require 4068 bytes,
but because Frientegrity employs spliced signatures, they
require additional overhead in exchange for faster signing
and veriﬁcation.
Read
Write
Server Data Fetches
Network and Data Serialization
Client Signature Veriﬁcation
Other (incl. Client Decrypt, Hashing)
Total Latency
Client Encryption
Client Signature
Network and Data Serialization
Server Signature
Other (incl. Hashing)
Total Latency
7.5%
17.5%
58.8%
16.3%
0.7%
41.7%
6.0%
40.4%
11.3%
0.45 ms
1.06 ms
3.55 ms
0.98 ms
6.04 ms
0.07 ms
4.45 ms
0.64 ms
4.31 ms
1.21 ms
10.67 ms
Figure 5: Read and write latency for Frientegrity as the ob-
ject history size increases from 0 to 25000. Each data point
represents the median of 1000 requests. The dots above and
below the lines indicate the 90th and 10th percentiles for
each trial. The table breaks down the cost of a typical me-
dian read and write request.
Figure 6: Latency for requests in a naive implementation
using hash chains. The red arrow indicates the response
time for Frientegrity read requests at an object size of 2000.
Each data point is the median of 100 requests. The error
bars indicate the 90th and 10th percentiles.
operations, the constant-time overhead of a public-key
signature or veriﬁcation continues to dominate the cost.
Next, we performed these same microbenchmarks on
an implementation that veriﬁes object history using a hash
chain, rather than Frientegrity’s history trees. In this exper-
iment, each client was stateless, and so it had to perform
a complete veriﬁcation when reading an object. This
veriﬁcation time grows linearly with the object history
12
05K10K15K20K25KObjectHistorySize02468101214ResponseLatency(ms)WriteRead050010001500ObjectHistorySize02004006008001000ResponseLatency(ms)FrientegrityReadReadWrite7.3 Latency of Fetching a News Feed
To present a user with a news feed, the client must perform
one readObject RPC for each of the user’s friends, and so
we expect the latency of fetching a news feed to scale lin-
early with the number of friends. Because clients can hide
network latency by pipelining requests to the server, we
expect the cost of decryption and veriﬁcation to dominate.
To evaluate the latency of fetching a news feed, we
varied the number of friends from 1 to 50. We repeated
each experiment 500 times and computed the median of
the trials. A linear regression test on the results showed
an overhead of 3.557 ms per additional friend (with a
correlation coeﬃcient of 0.99981). As expected, the value
is very close to the cost of client signature veriﬁcation and
decryption from Figure 5.
In 2011,
Users in social networks may have hundreds of friends,
the average Facebook user had
however.
190 friends, while the 90th percentile of users had 500
friends [19]. With Frientegrity’s measured per-object over-
head, fetching wall posts from all 500 friends would re-
quire approximately 1.8 s. In practice, we expect a social
networking site to use modern Web programming tech-
niques (e.g., asynchronous Javascript) so that news feed
items could be loaded in the background and updated in-
crementally while a user stays on a website. Even today,
social networking sites often take several seconds to fully
load.
7.4 Server Throughput with Many Clients
Social networks must scale to millions of active users.
Therefore, to reduce capital and operational costs, it is
important that a server be able to maximize throughput
while maintaining low latency. To characterize a loaded
server’s behavior, we evaluated its performance as we
increased the number of clients, all issuing requests to the
same object. In this experiment, we ran multiple client
machines, each with at most 4 clients. Each client issued
3000 requests sequentially, performing a 10 B write with
a 1% probability and a read otherwise.
Figure 7 plots server throughput as the number of clients
increases, as well as server latency as a function of load.
We measured server latency from the time it received a
request until the time that it started writing data back to its
network socket. The server reached a maximal through-
put of handling around 3500 requests per second, while
median latency remained below 0.5 ms.
7.5 Eﬀect of Increasing f
Frientegrity supports collaborative veriﬁcation of object
histories. The number of malicious clients that can be
tolerated, f , has a large impact on client performance. As
f increases, the client has to examine operations further
back in the history until it ﬁnds f + 1 diﬀerent writers. To
Figure 7: Server performance under increased client load.
Each data point is the median of 5 runs.
Figure 8: Performance implication of varying minimum set
of trusted writers for collaborative veriﬁcation.
understand this eﬀect, we measured the read latency of a
single object as f grows.
In this experiment, 50 writers ﬁrst issued 5000 updates
to the same object. We evaluated two diﬀerent workloads
for clients. In uniform, each writer had a uniform prob-
ability (2%) of performing the write; in power law, the
writers were selected from a power-law distribution with
α =3.5 (this particular α was the observed distribution of
chat activity among users of Microsoft messaging [32]).
A client then issued a read using increasing values of f .
Read latencies plotted in Figure 8 are the median of 100
such trials.
In the uniform distribution, the number of required ver-
iﬁcations rises slowly. But as f + 1 exceeds the number
of writers, the client must verify the entire history. For
the power law distribution, however, as f increases, the
number of required veriﬁcations rises more rapidly, and at
f = 42, the client must verify all 5000 updates. Neverthe-
less, this experiment shows that Frientegrity can maintain
good performance in the face of a relatively large num-
ber of malicious users. Even with f at nearly 30, the
veriﬁcation latency was only 100 ms.
7.6 Latency of ACL Modiﬁcations
In social networking applications, operations on ACLs
must perform well even when ACL sizes reach hundreds
13
01020304050#ofClients050010001500200025003000350040004500Throughput(request/s)ServerLatencyThroughput0.00.10.20.30.40.5ReadLatency(ms)01020304050f+1101001000ResponseLatency(ms)PowerUniformVis-`a-Vis [49], allow users’ data to migrate between users’