we use the BLEU implementation provided in [30]
(3) Semantic Similarity (SS). We use this metric to evalu-
ate the similarity between the corrected texts and reference
texts from the semantic-level. Here, we use an industry-
leading model SimNet developed by Baidu to calculate it,
which provides the state-of-the-art performance for measur-
ing the semantic similarity of Chinese texts [40].
Robustness Evaluation. We use the attack success rate, the
average number of perturbed words and the required number
of queries for per text to evaluate model robustness.
(1) Attack Success Rate (ASR). This metric is the most
widely used one in evaluating the performance of adversarial
attacks in terms of fooling the target model. It is deﬁned by
(which is the same as used in translation evaluation) to evalu-
ate the utility from the semantic-level.
(1) Edit Distance (ED). This metric quantiﬁes the dissim-
ilarity between two strings by counting the minimum number
of operations (e.g., removal, insertion or substitution) required
to transform one string into the other [33].
(2) Jaccard Coefﬁcient (JC). It is a statistic measure used
for gauging the similarity and diversity of ﬁnite sample sets
and is deﬁned as the size of the intersection divided by the
size of the union of the sample sets, i.e.,
J(A,B) =
|A∩ B|
|A∪ B| =
|A∩ B|
|A| +|B|−|A∩ B| .
(14)
In our experiments, A and B denotes the character sets of the
benign text and its adversarial counterpart, respectively.
Implementation
4.6
To fairly study the performance and robustness of the base-
lines and TEXTSHIELD, our experiments have the following
settings: (i) the backbone networks applied in all the mod-
els have the same architecture, and concretely, the TextCNN
backbone is designed with 32 ﬁlters of size 2, 3 and 4, and the
BiLSTM backbone is designed with one bidirectional layer of
128 hidden units; (ii) all the models have the same maximum
sequence length of 50 (since the majority of the texts in our
datasets are shorter than 50) and the same embedding size
of 128; (iii) all the models are trained from scratch with the
Adam optimizer by using a basic setup without any complex
tricks; and (iv) the optimal hyperparameters such as learning
rate, batch size, maximum training epochs, and dropout rate
are tuned for each task and each model separately.
We conducted all the experiments on a server with two
Intel Xeon E5-2682 v4 CPUs running at 2.50GHz, 120 GB
memory, 2 TB HDD and two Tesla P100 GPU cards.
ASR =
# success samples
# total examples
(13)
5 Experimental Results
and a lower success rate indicates a more robust target model.
(2) Perturbed Word. Since text is discrete data, we can-
not use the metrics like l1, l2 and l∞ to quantify the added
perturbations as done in the image domain. Consequently, we
use the number of required perturbed words to quantify the
noise scale in adversarial texts.
(3) Query. Recall that TextBugger explores the sensitivity
of the target model by iteratively query it for its classiﬁcation
conﬁdence. Hence, we use the average number of queries
required for generating one successful adversarial text to eval-
uate the model sensitivity and fewer queries indicate that the
model is more vulnerable.
Utility Evaluation. We use edit distance and Jaccard similar-
ity coefﬁcient to evaluate the utility of the generated adversar-
ial texts from the character-level, and use semantic similarity
5.1 Evaluation of Model Performance
Detection Performance. We ﬁrst evaluate the efﬁcacy of
TEXTSHIELD and the compared baselines under the non-
adversarial setting to verify whether the applied defense will
have negative impact on the model performance. The main
results are shown in Table 2. It is observed that the common
TextCNN and BiLSTM both achieve impressive performance
across the two tasks. However, their detection accuracy de-
creases by 4% and 3% for abuse detection and porn detection
respectively when using Pycorrector as the defense, and sim-
ilar degradation also exists when leveraging TextCorrector.
After analyzing the bad cases, we ﬁnd that some toxic words
were erroneously detected as misspelling words by these two
methods and the wrong correction thus caused the degrada-
tion. Comparatively, when leveraging TEXTSHIELD as the
USENIX Association
29th USENIX Security Symposium    1387
Table 2: The model accuracy under non-adversarial setting.
Table 3: The error correction performance.
Model
Common TextCNN
TextCNN + Pycorrector
TextCNN + TextCorrector
TextCNN + EMF
TextCNN + IMF
TextCNN + NMT
TextCNN + EMF + NMT
TextCNN + IMF + NMT
Common BiLSTM
BiLSTM + Pycorrector
BiLSTM + TextCorrector
BiLSTM + EMF
BiLSTM + IMF
BiLSTM + NMT
BiLSTM + EMF + NMT
BiLSTM + IMF + NMT
Abuse Detection
Porn Detection
0.88
0.84
0.85
0.85
0.87
0.87
0.86
0.88
0.86
0.82
0.83
0.84
0.85
0.84
0.84
0.85
0.90
0.88
0.90
0.89
0.89
0.89
0.88
0.89
0.87
0.84
0.87
0.86
0.88
0.86
0.85
0.87
(a) Abuse
(b) Porn
Figure 3: The training loss of the adversarial NMT model.
defense, the accuracy decreases by less than 2% when only
applying multimodal embedding and by less than 1% when
combing multimodal embedding with adversarial translation.
This demonstrates that TEXTSHIELD has little impact on the
model performance in the non-adversarial environments.
Translation Performance. Compared to the traditional
NMT tasks such as English–Chinese translations, our adver-
sarial translation task is relatively easy since the source and
target sequences are both Chinese and usually only differ in
few characters. Hence, as illustrated in Fig. 3, the training
loss of the adversarial NMT model converges quickly and
achieves the optimality within 2×104 steps, indicating that
it is feasible and easy to apply NMT to restore adversarial
perturbations. Furthermore, Table 3 shows the error correc-
tion performance of the adversarial NMT model and the two
compared spelling correction methods on the test set of the
parallel corpora. The baseline result is calculated based on the
adversarial texts without error correction and the correspond-
ing reference texts, which reﬂects their original difference.
It can be seen that the adversarial NMT model achieves an
excellent performance across all the metrics in the two tasks
and outperforms the compared spelling correction methods
by a signiﬁcant margin. This demonstrates that end-to-end ad-
versarial NMT is more elastic and effective in reconstructing
the original text from its corresponding adversarial text.
Method
Abuse Detection
Porn Detection
WER BLEU
0.744
0.198
Baseline
0.223
0.687
Pycorrector
0.767
TextCorrector
0.181
Adversarial NMT 0.051
0.923
SS WER BLEU
0.749
0.939
0.906
0.701
0.777
0.939
0.988
0.916
0.199
0.213
0.173
0.056
SS
0.937
0.911
0.938
0.985
5.2 Evaluation of Effectiveness
Second, we evaluate the efﬁcacy of TEXTSHIELD in terms of
defending the DLTC models against the user generated obfus-
cated texts in the real-world adversarial scenario. Speciﬁcally,
we ﬁrst collect 2,000 obfuscated abusive texts and 2,000 ob-
fuscated pornographic texts from online social media. Each
collected sample is manually conﬁrmed to have at least one
variant word. Then, we manually construct a reference set
for each task by restoring the variant words to their original
benign counterparts. Finally, all experiments are conducted
on the collected obfuscated texts and the reference sets.
Detection Performance. The main results of detection per-
formance and the comparison with different baselines are
summarized in Table 4.
It is observed that the common
TextCNN and BiLSTM achieve an accuracy below 0.496
in the two tasks. Shielded by Pycorrector and TextCorrector,
these models however obtain an unnoticeable improvement
in detection accuracy. We speculate that this is mainly be-
cause these two spelling correctors are also vulnerable to the
manually crafted real-world adversarial perturbations. Com-
paratively, TEXTSHIELD signiﬁcantly improves the accuracy
of the DLTC models by about 30% for only leveraging mul-
timodal embedding, and by 45% for using the combined de-
fense scheme, indicating that TEXTSHIELD is practical and
effective in enhancing model robustness in the real-world ad-
versarial scenario. An interesting observation is that TextCNN
achieves the best performance for abuse detection when apply-
ing EMF while achieves the best performance for porn detec-
tion when applying IMF, and a converse result is observed for
BiLSTM. This shows that the best defense varies for different
models and tasks and we should design the application-aware
defense schemes in practice.
We further analyze the classiﬁcation conﬁdence of the mod-
els defended by TEXTSHIELD on the collected obfuscated
texts and the evaluation results are visualized in Fig. 4. Ob-
viously, it can be seen that the models with the combined
defense classify the obfuscated texts with a much higher con-
ﬁdence. This demonstrates that these models are more robust
against the manually crafted adversarial perturbations. In ad-
dition, the above mentioned interesting observation also exists
in Fig. 4, once again demonstrating the need for designing
application-aware defense.
Correction Performance. The error correction perfor-
mance of our adversarial NMT model on user generated ob-
fuscated texts and the comparative performance of the two
1388    29th USENIX Security Symposium
USENIX Association
0246810Epoch(×104)020406080100120140Loss0246810Epoch (×104)050100150200250300LossTable 4: The detection performance on user generated obfuscated texts.
Model
# of Perturbation
Common TextCNN
TextCNN + Pycorrector
TextCNN + TextCorrector
TextCNN + EMF
TextCNN + IMF
TextCNN + NMT
TextCNN + EMF + NMT
TextCNN + IMF + NMT
Common BiLSTM
BiLSTM + Pycorrector
BiLSTM + TextCorrector
BiLSTM + EMF
BiLSTM + IMF
BiLSTM + NMT
BiLSTM + EMF + NMT
BiLSTM + IMF + NMT
≤ 1
0.488
0.491
0.498
0.790
0.714
0.857
0.923
0.922
0.350
0.356
0.356
0.604
0.631
0.801
0.900
0.892
Abuse Detection
≤ 2
≤ 3
0.480
0.483
0.488
0.506
0.485
0.484
0.760
0.783
0.732
0.725
0.869
0.886
0.931
0.919
0.920
0.931
0.341
0.343
0.364
0.356
0.352
0.349
0.620
0.616
0.646
0.643
0.764
0.791
0.871
0.890
0.894
0.881
> 3
0.458
0.490
0.457
0.736
0.729
0.836
0.906
0.904
0.328
0.355
0.348
0.605
0.645
0.707
0.848
0.851
≤ 1
0.496
0.504
0.568
0.753
0.777
0.909
0.928
0.944
0.477
0.475
0.465
0.746
0.744
0.856
0.933
0.932
Porn Detection
≤ 3