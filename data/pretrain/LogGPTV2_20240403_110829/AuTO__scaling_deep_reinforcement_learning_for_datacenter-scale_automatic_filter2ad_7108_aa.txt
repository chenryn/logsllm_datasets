title:AuTO: scaling deep reinforcement learning for datacenter-scale automatic
traffic optimization
author:Li Chen and
Justinas Lingys and
Kai Chen and
Feng Liu
AuTO: Scaling Deep Reinforcement Learning
for Datacenter-Scale Automatic Traic Optimization
SING Lab, Hong Kong University of Science and Technology, †SAIC Motors
Li Chen, Justinas Lingys, Kai Chen, Feng Liu†
{lchenad,jlingys,kaichen}@cse.ust.hk,PI:EMAIL
ABSTRACT
Trac optimizations (TO, e.g. ow scheduling, load balanc-
ing) in datacenters are dicult online decision-making prob-
lems. Previously, they are done with heuristics relying on
operators’ understanding of the workload and environment.
Designing and implementing proper TO algorithms thus take
at least weeks. Encouraged by recent successes in applying
deep reinforcement learning (DRL) techniques to solve com-
plex online control problems, we study if DRL can be used for
automatic TO without human-intervention. However, our
experiments show that the latency of current DRL systems
cannot handle ow-level TO at the scale of current datacen-
ters, because short ows (which constitute the majority of
trac) are usually gone before decisions can be made.
Leveraging the long-tail distribution of datacenter trac,
we develop a two-level DRL system, AuTO, mimicking the
Peripheral & Central Nervous Systems in animals, to solve
the scalability problem. Peripheral Systems (PS) reside on
end-hosts, collect ow information, and make TO decisions
locally with minimal delay for short ows. PS’s decisions
are informed by a Central System (CS), where global trac
information is aggregated and processed. CS further makes
individual TO decisions for long ows. With CS&PS, AuTO
is an end-to-end automatic TO system that can collect net-
work information, learn from past decisions, and perform ac-
tions to achieve operator-dened goals. We implement AuTO
with popular machine learning frameworks and commodity
servers, and deploy it on a 32-server testbed. Compared to
existing approaches, AuTO reduces the TO turn-around time
from weeks to ∼100 milliseconds while achieving superior
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies
are not made or distributed for prot or commercial advantage and that
copies bear this notice and the full citation on the rst page. Copyrights
for components of this work owned by others than the author(s) must
be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specic
permission and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
© 2018 Copyright held by the owner/author(s). Publication rights licensed
to the Association for Computing Machinery.
ACM ISBN 978-1-4503-5567-4/18/08...$15.00
https://doi.org/10.1145/3230543.3230551
performance. For example, it demonstrates up to 48.14% re-
duction in average ow completion time (FCT) over existing
solutions.
CCS CONCEPTS
• Networks → Network resources allocation; Trac
engineering algorithms; Data center networks; • Com-
puting methodologies → Reinforcement learning;
KEYWORDS
Datacenter Networks, Reinforcement Learning, Trac Opti-
mization
1 INTRODUCTION
Datacenter trac optimizations (TO, e.g. ow/coow sched-
uling [1, 4, 8, 14, 18, 19, 29, 61], congestion control [3, 10],
load balancing &routing [2]) have signicant impact on ap-
plication performance. Currently, TO is dependent on hand-
crafted heuristics for varying trac load, ow size distri-
bution, trac concentration, etc. When parameter setting
mismatches trac, TO heuristics may suer performance
penalty. For example, in PIAS [8], thresholds are calculated
based on a long term ow size distribution, and is prone
to mismatch the current/true size distribution in run-time.
Under mismatch scenarios, performance degradation can be
as much as 38.46% [8]. pFabric [4] shares the same problem
when implemented with limited switch queues: for certain
cases the average FCT can be reduced by over 30% even if the
thresholds are carefully optimized. Furthermore, in coow
scheduling, xed thresholds in Aalo [18] depend on the op-
erator’s ability to choose good values upfront, since there is
no run-time adaptation.
Apart from parameter-environment mismatches, the turn-
around time of designing TO heuristics is long—at least
weeks. Because they require operator insight, application
knowledge, and trac statistics collected over a long period
of time. A typical process includes: rst, deploying a mon-
itoring system to collect end-host and/or switch statistics;
second, after collecting enough data, operators analyze the
data, design heuristics, and test it using simulation tools and
optimization tools to nd suitable parameter settings; nally,
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
L. Chen et al.
tested heuristics are enforced1 (with application modica-
tions [19, 61], OS kernel module [8, 14], switch congura-
tions [10], or any combinations of the above).
Automating the TO process is thus appealing, and we de-
sire an automated TO agent that can adapt to voluminous,
uncertain, and volatile datacenter trac, while achieving
operator-dened goals. In this paper, we investigate rein-
forcement learning (RL) techniques [55], as RL is the subeld
of machine learning concerned with decision making and
action control. It studies how an agent can learn to achieve
goals in a complex, uncertain environment. An RL agent
observes previous environment states and rewards, then de-
cides an action in order to maximize the reward. RL has
achieved good results in many dicult environments in re-
cent years with advances in deep neural networks (DNN):
DeepMind’s Atari results [40] and AlphaGo [52] used deep
RL (DRL) algorithms which make few assumptions about
their environments, and thus can be generalized in other set-
tings. Inspired by these results, we are motivated to enable
DRL for automatic datacenter TO.
We started by verifying DRL’s eectiveness in TO. We
implemented a ow-level centralized TO system with a basic
DRL algorithm, policy gradient [55]. However, in our exper-
iments (§2.2), even this simple algorithm running on cur-
rent machine learning software frameworks2 and advanced
hardware (GPU) cannot handle trac optimization tasks at
the scale of production datacenters (>105 servers). The crux
is the computation time (∼100ms): short ows (which con-
stitute the majority of the ows) are gone before the DRL
decisions come back, rendering most decisions useless.
Therefore, in this paper we try to answer the key question:
How to enable DRL-based automatic TO at datacenter-scale?
To make DRL scalable, we rst need to understand the long-
tail distribution of datacenter trac [3, 11, 33]: most of the
ows are short ows3, but most of the bytes are from long
ows. Thus, TO decisions for short ows must be generated
quickly; whereas decisions for long ows are more inuential
as they take longer time to nish.
We present AuTO, an end-to-end DRL system for datacenter-
scale TO that works with commodity hardware. AuTO is a
two-level DRL system, mimicking the Peripheral & Central
Nervous Systems in animals. Peripheral Systems (PS) run on
all end-hosts, collect ow information, and make instant TO
decisions locally for short ows. PS’s decisions are informed
by the Central System (CS), where global trac information
are aggregated and processed. CS further makes individual
TO decisions for long ows which can tolerate longer pro-
cessing delays.
The key to AuTO’s scalability is to detach time-consuming
DRL processing from quick action-taking for short ows.
To achieve this, we adopt Multi-Level Feedback Queueing
(MLFQ) [8] for PS to schedule ows guided by a set of thresh-
olds. Every new ow starts at the rst queue with highest
priority, and is gradually demoted to lower queues after its
sent bytes pass certain thresholds. Using MLFQ, AuTO’s PS
makes per-ow decisions instantly upon local information
(bytes-sent and thresholds)4, while the thresholds are still
optimized by a DRL algorithm in the CS over a relatively
longer period of time. In this way, global TO decisions are
delivered to PS in the form of MLFQ thresholds (which is
more delay-tolerant), enabling AuTO to make globally in-
formed TO decisions for the majority of ows with only
local information. Furthermore, MLFQ naturally separates
short and long ows: short ows complete in the rst few
queues, and long ows descend down to the last queue. For
long ows, CS centrally processes them individually using a
dierent DRL algorithm to determine routing, rate limiting,
and priority.
We have implemented an AuTO prototype using Python.
AuTO is thus compatible with popular learning frameworks,
such as Keras/TensorFlow. This allows both networking
and machine learning community to easily develop and test
new algorithms, because software components in AuTO are
reusable in other RL projects in datacenter.
We further build a testbed with 32 servers connected by
2 switches to evaluate AuTO. Our experiments show that,
for trac with stable load and ow size distribution, AuTO’s
performance improvement is up to 48.14% compared to stan-
dard heuristics (shortest-job-rst and least-attained-service-
rst) after 8 hours of training. AuTO is also shown to learn
steadily and adapt across temporally and spatially heteroge-
neous trac: after only 8 hours of training, AuTO achieves
8.71% (9.18%) reduction in average (tail) FCT compared to
heuristics.
In the following, we rst overview DRL and reveal why
current DRL systems fail to work at large scale in §2. We
describe system design in §3, as well as the DRL formulations
and solutions in §4. We implement AuTO in §5, and evaluate
it with extensive experiments in §6 using a realistic testbed.
Finally, we review related works in §7, and conclude in §8.
1After the heuristic is designed, its parameters can usually be computed in
a short time for average scenarios: minutes [8, 14, 19] or hours [61]. AuTO
seeks to automate the entire TO design process, rather than just parameter
selection.
2e.g. TensorFlow [57], PyTorch [48], Ray [42]
3The threshold between short and long ows is dynamically determined in
AuTO based on current trac distribution (§4).
2 BACKGROUND AND MOTIVATION
In this section, we rst overview the RL background. Then,
we describe and apply a basic RL algorithm, policy gradient,
4For short ows, AuTO relies on ECMP[30] (which is also not centrally
controlled) for routing/load-balancing and makes no rate-limiting decisions.
AuTO: Scaling Deep Reinforcement Learning
for Datacenter-Scale Automatic Traic Optimization
Figure 1: A general reinforcement learning setting us-
ing neural network as policy representation.
to enable ow scheduling in TO. Finally, we show the prob-
lem of an RL system running PG using testbed experiments,
motivating AuTO.
discounting factor.
lative discounted reward E[(cid:80)∞
2.1 Deep Reinforcement Learning (DRL)
As shown in Figure 1, environment is the surroundings of
the agent with which the agent can interact through ob-
servations, actions, and feedback (rewards) on actions [55].
Specically, in each time step t, the agent observes state st,
and chooses action at. The state of the environment then
transits to st +1, and the agent receives reward rt. The state
transitions and rewards are stochastic and Markovian [36].
The objective of learning is to maximize the expected cumu-
t =0γ trt] where γt∈(0,1] is the
The RL agent takes actions based on a policy, which is
a probability distribution of taking action a in the state s:
π (s,a). For most practical problems, it is infeasible to learn
all possible combinations of state-action pairs, thus function
approximation [31] technique is commonly used to learn the
policy. A function approximator πθ (s,a) is parameterized by
θ, whose size is much smaller (thus mathematically tractable)
than the number of all possible state-action pairs. Function
approximator can have many forms, and recently, deep neu-
ral networks (DNNs) have been shown to solve practical,
large-scale dynamic control problems similar to ow sched-
uling. Therefore, we also use DNN as the representation of
function approximator in AuTO.
With function approximation, the agent learns by updat-
ing the function parameters θ with the state st, action at,
and the corresponding reward rt in each time period/step t.
We focus on one class of updating algorithms that learn by
performing gradient-descent on the policy parameters. The
learning involves updating the parameters (link weights)
of a DNN so that the aforementioned objective could be
maximized.
(cid:88)
t
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
θ ← θ + α
∇θ lo❕ πθ (st ,at ) vt
(1)
Training of the agent’s DNN adopts a variant of the well-
known REINFORCE algorithm [56]. This variant uses a mod-
ied version of Equation (1), which alleviates the drawbacks
of the algorithm: convergence speed and variance. To mit-
igate the drawbacks, Monte Carlo Method [28] is used to
compute an empirical reward, vt, and a baseline value (the
cumulative average of experienced rewards per server) is
used for reducing the variance [51]. The resultant update
rule (Equation (2)) is applied to the policy DNN, due to its
variance management and guaranteed convergence to at
least a local minimum [56]:
(cid:88)
θ ← θ + α
∇θ lo❕ πθ (st ,at ) (vt − baseline)
(2)
t
2.2 Example: DRL for Flow Scheduling
As an example, we formulate the problem of ow scheduling
in datacenters as a DRL problem, and describe a solution
using the PG algorithm based on Equation (2).
Flow scheduling problem We consider a datacenter net-
work connecting multiple servers. For simplicity, we adopt
the big-switch assumption by previous works in ow sched-
uling [4, 14], where the network is non-blocking with full-
bisection bandwidth and proper load-balancing. Following
this assumption, the ow scheduling problem is simplied
to the problem of deciding the sending order of ows. We
consider an implementation that enables preemptive sched-
uling of ows using strict priority queueing. We create K
priority queues for ows in each server [23], and enforce
strict priority queuing among them. K priority queues are
also congured in the switches, similar to [8]. The priority of
each ow can be changed dynamically to enable pre-emption.
The packet of each ow is tagged with its current priority