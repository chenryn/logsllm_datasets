throughput but does not improve the time to results.
Approximation. The other common approach is to leverage vari-
ous forms of approximation. For example, flow-level approaches [38]
average the behavior of many packets to reduce computation. Closed-
form solutions [37] and a vast array of optimized custom simula-
tors [33, 45, 46] also fall in this category. While these approaches
often produce good performance; they require deep expertise to
craft and limit the metrics that one can draw from the analysis.
3 DESIGN GOALS
MimicNet is based around the following design goals:
• Arbitrary scale, extensions, and instrumentation: Acknowledging
the utility of packet-level simulation in enabling flexible and rich
evaluations of arbitrary network designs, we seek to provide
users with similar flexibility with MimicNet.
• Orders of magnitude faster results: Equally important, MimicNet
must be able to provide meaningful performance estimates
several orders of magnitude faster than existing approaches.
Parallelism, on its own, is not enough—we seek to decrease the
total amount of work.
• Tunable and high accuracy: Despite the focus on speed, Mimic-
Net should produce observations that resemble those of a full
packet-level simulation. Further, users should be able to define
their own accuracy metrics and to trade this accuracy off with
improved time to results.
Explicitly not a goal of our framework is full generality to arbi-
trary data center topologies, routing strategies, and traffic patterns.
Instead, MimicNet makes several carefully chosen and domain-
specific assumptions (described in Section 4.2) that enable it to scale
to larger network sizes than feasible in traditional packet-level sim-
ulation. We argue that, in spite of these restrictions, MimicNet can
provide useful insights into the performance of large data centers.
4 OVERVIEW
MimicNet’s approach is as follows. Every MimicNet simulation
contains a single ‘observable’ cluster, regardless of the total num-
ber of clusters in the data center. All of the hosts, switches, links,
and applications in this cluster as well as all of the remote appli-
cations with which it communicates are simulated in full fidelity.
All other behavior—the traffic between un-observed clusters, their
internals, and anything else not directly observed by the user—is
approximated by trained models.
While prior work has also attempted to model systems and net-
works (e.g., [54, 56]), these prior systems tend to follow a more
traditional script by (1) observing the entire system/network and
(2) fitting a model to the observations. MimicNet is differentiated
by the insight that, by carefully composing models of small pieces
of a data center, we can accurately approximate the full data center
network using only observations of small subsets of the network.
4.1 MimicNet Design
MimicNet constructs and composes models at the granularity of
individual data center clusters: Mimics. From the outside, Mimics
resemble regular clusters. Their hosts initiate connections and ex-
change data with the outside world, and their networks drop, delay,
and modify that traffic according to the internal queues and logic of
the cluster’s switches. However, Mimics differ in that they are able
to predict the effects of that queuing and protocol manipulation
without simulating or interacting with other Mimics—only with
the observable cluster.
We note that the goal of MimicNet is not to replicate the effects
of any particular large-scale simulation, just to generate results
that exhibit their characteristics. It accomplishes the above with the
help of two types of models contained within each Mimic: (1) deep-
learning-based internal models that learn the behavior of switches,
links, queues, and intra-cluster cross-traffic; and (2) flow-based
feeder models that approximate the behavior of inter-cluster cross-
traffic. The latter is parameterized by the size of the data center.
Together, these models take a sequence of observable packets and
their arrival times and output the cluster’s predicted effects:
289
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Q. Zhang et al.
Figure 3: The end-to-end, fully automated workflow of MimicNet. (❶) Small-scale observations, (❷) model training, (❸) model
testing, and (❹) optional hyper-parameter tuning produce tuned machine learning models for use in Mimics, which speed up
large-scale simulations (❺) by replacing the majority of the network. A key feature of MimicNet is that the traditionally slow
steps of ❶, ❷, ❸, and ❹ are all done at small scale and are, therefore, fast as well.
Figure 4: Breakdown of traffic in a to-be-approximated clus-
ter. MimicNet approximates all traffic that does not interact
with the observable cluster (dotted-red lines) using the mod-
els in the referenced sections.
(1) Whether the packets are dropped as a result of the queue man-
agement policy.
(2) When the packets egress the Mimic, given no drop.
(3) Where the packets egress, based on the routing table.
(4) The contents of the packets after traversing the Mimic, includ-
ing modifications such as TTL and ECN.
Workflow. The usage of MimicNet (depicted in Figure 3) begins
with a small subset of the full simulation: just two user-defined
clusters communicating with one another. This full-fidelity, small-
scale simulation is used to generate training and testing sets for
supervised learning of the models described above. Augmenting
this training phase is a configurable hyper-parameter tuning stage
in which MimicNet explores various options for modeling with
the goal of maximizing both (a) user-defined, end-to-end accuracy
metrics like throughput and FCT, and (b) generalizability to larger
configurations and different traffic matrices.
Using the trained models, MimicNet assembles a full-scale sim-
ulation in which all of the clusters in the network (save one) are
replaced with Mimics. For both data generation and large-scale
simulation, MimicNet uses OMNeT++ as a simulation substrate.
Performance analysis. To understand MimicNet’s performance
gains, consider the Mimic in Figure 4 and the types of packets
that flow through it. At a high level, there are two such types: (1)
traffic that interacts with the observable cluster (Mimic-Real), and
(2) traffic that does not (Mimic-Mimic).
As a back-of-the-envelope computation, assume that we simulate
𝑁 clusters, 𝑁 ≫ 2. Also assume that𝑇 is the total number of packets
sent in the full simulation of the data center and that 𝑝 is the ratio
of traffic that leaves a cluster vs. that stays within it (inter-cluster-
to-intra-cluster), 0 ≤ 𝑝 ≤ 1. The number of packets that leave a
single cluster in the full simulation is then approximately 𝑇 𝑝
𝑁 .
Because Mimics only communicate with the single observable
cluster and not each other, the number of packets that leave a Mimic
in an approximate simulation is instead:
𝑇 𝑝
𝑁 (𝑁 − 1)
Thus, the total number of packets generated in a MimicNet simu-
lation (the combination of all traffic generated at the observable
cluster and 𝑁 − 1 Mimics) is:
+ (𝑁 − 1)𝑇 𝑝
𝑁 (𝑁 − 1) =
𝑇
𝑁
𝑇 + 𝑇 𝑝
𝑁
The total decrease in packets generated is, therefore, a factor be-
tween 𝑁
2 and 𝑁 with a bias toward 𝑁 when traffic exhibits cluster-
level locality. Fewer packets and connections generated mean less
processing time and a smaller memory footprint. It also means a de-
crease in inter-cluster communication, which makes the composed
simulation more amenable to parallelism than the full version.
4.2 Restrictions
MimicNet makes several domain-specific assumptions that aid in
the scalability and accuracy of the MimicNet approach.
• Failure-free FatTrees: MimicNet assumes a FatTree topology,
where the structure of the network is recursively defined and
packets follow a strict up-down routing. This allows it to as-
sume symmetric bisection bandwidth and to break cluster-level
modeling into simpler subtasks.
• Traffic patterns that scale proportionally: To ensure that mod-
els trained from two clusters scale up, MimicNet requires a
per-host synthetic model of flow arrival, flow size, packet size,
and cluster-level locality that is independent of the size of the
network. In other words (at least at the host level), users should
ensure that the size and frequency of packets in the first step
resemble those of the last step. We note that popular datasets
used in recent literature already adhere to this [6, 8, 33, 40].
• Fan-in bottlenecks: Following prior work, MimicNet assumes
that the majority of congestion occurs on fan-in toward the
290
…………Data GenerationModel TrainingFeature Extraction+…………Model TestingLearned ModelHyper-parameter Tuning…droplatencyECN…………Large-scale SimulationHyper-tuned ModelmanyclustersML model❶❷❸❹❺……ClusterMimic-RealMimic-Mimic§5§6§6MimicNet: Fast Performance Estimates for DCNs with ML
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
destination [24, 50]. This allows us to focus accuracy efforts on
only the most likely bottlenecks.
• Intra-host isolation: To enable the complete removal of Mimic-
Mimic connections at end hosts, MimicNet requires that con-
nections be logically isolated from one another inside the host—
MimicNet models network effects but does not model CPU
interactions or out-of-band cooperation between connections.
MimicNet, as a first step toward large-scale network prediction
is, thus, not suited for evaluating every data center architecture
or configuration. Still, we argue that MimicNet can provide useful
performance estimates of a broad class of proposals. We also discuss
potential relaxations to the above restrictions in Appendix A, but
leave those for future work.
5 INTERNAL MODELS
As mentioned, Mimics are composed of two types of models. The
first type models internal cluster behavior. Its goal is twofold:
(1) For external traffic (both Mimic-Real and Mimic-Mimic), to
be able to predict how the network of the cluster will affect
the packet: whether it drops, its latency, its next hop, and any
packet modifications.
(2) For internal traffic (between hosts in the same Mimic), to re-
move it and bake its effects into the above predictions. In other
words, during inference, the model should account for the ob-
servable effects of internal traffic without explicitly seeing it.
Note that not all observable effects need to be learned, especially
if the result can be computed using a simple, deterministic function,
e.g., TTLs or ECMP. However, for others—drops, latency, ECN
marking, NDP truncation, and so on—the need for the models to
scale to unobserved configurations presents a unique challenge for
generalizable learning. To address the challenge, MimicNet carefully
curates training data, feature sets, and models with an explicit
emphasis on ensuring that generated models are scale-independent.
5.1 Small-scale Observations
MimicNet begins by running a full-fidelity, but small-scale simula-
tion to gather training/testing data.
Simulation and instrumentation. Data generation mirrors the
depiction in Figure 3. Users first provide their host and switch
implementations in a format that can be plugged into the C++-
based OMNeT++ simulation framework.
Using these implementations, MimicNet runs a full-fidelity simu-
lation of two clusters connected via a set of Core switches. Among
these two clusters, we designate one as the cluster to be modeled
and dump a trace of all packets entering and leaving the cluster. In
a FatTree network, this amounts to instrumenting the interfaces
facing the Core switches and the Hosts. Between these two junc-
tures are the mechanics of the queues and routers—these are what
is learned and approximated by the Mimic internal model.
Pre-processing. MimicNet takes the packet dumps and matches
the packets entering and leaving the network using identifiers
from the packets (e.g., sequence numbers). Examining the matches
helps to determine the length of time it spent in the cluster and
291
any changes to the packet. There are two instances where a 1-to-
1 matching may not be possible: loss and multicast. Loss can be
detected as a packet entering the cluster but never leaving. Multicast
must be tracked by the framework. Both can be modeled.
5.2 Modeling Objectives
MimicNet models the clusters’ effects as machine learning tasks.
More formally, for each packet of external traffic, 𝑖:
Latency regression. We model the time that 𝑖 spends in the clus-
ter’s network as a bounded continuous random variable and set the
objective to minimize the Mean Absolute Error (MAE) between the
real latency and the prediction:
min |𝑦𝑙
𝑖 − ˆ𝑦𝑙
𝑖 |,
𝑖 is (𝐿max+𝜖) if the packet is dropped and (lat ∈ [𝐿min, 𝐿max])
where 𝑦𝑙
otherwise. ˆ𝑦𝑙
𝑖 is the predicted latency. To improve the accuracy of
this task, MimicNet uses discretization in training latency models.
Specifically, MimicNet quantizes the values using a linear strategy:
(cid:36) 𝑦𝑙 − 𝐿min
𝐿max − 𝐿min
(cid:37)
× 𝐷
𝑓 (𝑦𝑙) =
𝑖 log ˆ𝑦𝑑
min−𝑦𝑑
where 𝐷 is the hyperparameter that controls the degree of dis-
cretization. By varying 𝐷, we can trade off the ease of modeling
and the recovery precision from discretization.
Drops and packet modification classification. For most other
tasks, classification is a better fit. For example, the prediction of
a packet drop has two possible outcomes, and the objective is to
minimize Binary Cross Entropy (BCE):
𝑖 − (1 − 𝑦𝑑