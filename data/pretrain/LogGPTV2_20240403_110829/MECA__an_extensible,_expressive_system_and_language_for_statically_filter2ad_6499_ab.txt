default annotations are bound to the targets before calls.
Allowing the binding of n-ary expressions provides a nice
increase in expressiveness over constructs such as unary type
qualiﬁers.
3.2 Support for data-dependencies
Our checking system is path-sensitive, and will suppress
many common infeasible paths [4].1 However, in general
pruning all false paths and resolving all data dependencies
are undecidable problems. Thus, we allow programmers to
provide help when calculation fails using a built-in predicate
to express data dependencies:
expr ==> annot
Here expr can be any valid C expression and annot any
checker-deﬁned annotation. If expr is true, the annotation
is bound to the associated object. If the expression is false,
then the negation of the expression is bound (e.g., “not
tainted”). We call such annotations imply annotations, and
the predicate expression expr is an imply-condition.
A possible use with the tainted checker would be to specify
parameter-dependencies that control whether an argument
is actually tainted or not. For example, the following decla-
ration states that foo’s argument p is tainted only when the
bitwise-or of flag and the constant USER FLAG is non-zero:
void foo(/*@ ﬂag&USER FLAG ==> tainted*/ char* p,
int ﬂag);
We use similar declarations to specify when a structure ﬁeld
indicates that a pointer in the structure is tainted.
Because we evaluate implication expressions at compile
time, their value may be unknown. Programmers can con-
trol the implication in this case by using the special keyword
unknown, which evaluates to true if the expression cannot be
resolved at compile time. They can thus make the annota-
tion conservative (e.g., unknown implies tainted) or non-
conservative ( e.g., unknown implies not tainted). This is
illustrated in Figure 6.
Our system implements imply annotations by keeping track
of a set of known predicates along each path. When it en-
counters a program object with one or more imply annota-
tions, it evaluates these against the set of known predicates.
3.3 Programmatic annotations
Traditionally, annotating source code is brute force: pro-
grammers insert them in every place the checker will need
them at. This reliance on manual labor is both tedious and
error-prone, since a single missed annotation can mean that
a check does not occur, or that the process is so strenu-
ous that the programmer quits after only annotating a few
hundred thousand lines of code.
MECA lets programmers automate this process using pro-
grammatic annotations, which conceptually are applied to
all points in the program and, when their conditions are
satisﬁed, mark the program point with their annotation.
For example, to specify that all system call parameters
should be tainted we would write:
/*@ global
$param: ${!strncmp (current fn, “sys ”, 4)}
==> tainted */
1This implicit pruning obviates much of the need for explicit
data-ﬂow ﬂags introduced in the Vault language [5].
This programmatic annotation speciﬁes that it should be
applied globally over the entire checked system (global) and
prog annot ::=
scope objs ’:’ ’${’ C−expr ’}’
’==>’ annotation ;
scope ::= ’global’ | ’file_global’;
objs ::= obj;
obj ::= ’$variable’ | ’$parameter’ | ’$ret’ | ’$function’
| obj
| obj OR obj
;
Figure 7: General syntax for programmatic annota-
tion declarations.
that it cares only about parameters ($param).
It will be
applied over each parameter in each function. The part in
curly braces checks if the current function name is preﬁxed
by “sys ” which is the Linux kernel naming convention for
system calls. If so, it returns true, and each parameter will
be marked as tainted.
The general form of the rule is depicted in Figure 7. “Scope”
controls whether the annotation is applied over the entire
system (global) or just within one ﬁle (file global). (Note
that these annotations can be overridden by local annota-
tions.) The “object” speciﬁcation controls what it is applied
to: functions, return values, parameters or variables. The
C-expr can be a normal C-expr described in Section 3.1.1
or a callout to helper functions the system provides. It can
refer to program objects using special variable names such
as current fn (the current function), current file (the
current ﬁle), current param (the current parameter), and
current var (the current variable). Programmers can also
checks the typename of a program object by type is(typename).
In practice, a major use of programmatic annotations is to
translate system-speciﬁc naming conventions (which can be
viewed as ad hoc pre-existing annotations) into checkable
annotations. The example above falls into this category.
Another example would be exploiting a naming convention
where a pointer parameter has an associated length that
contains its name as a preﬁx (e.g., the length for parameter
foo is speciﬁed by foo len).
Programmatic annotations are useful even when program-
mers are supposed to manually annotate all relevant pro-
gram objects. In this case they can be used as “annotation
assertions” that prevent false negatives by detecting missing
annotations.
4. ANNOTATION PROPAGATION
This section describes MECA’s ﬂow-sensitive, bottom-up,
inter-procedural analysis for propagating annotations from
callees to their callers.
The analysis is initialized by retrieving the base annota-
tions from annotated code. These annotations consist of an-
notations for functions, their parameters and return values.
The annotations are used to build summaries, which are
a set of (cid:2)guard, pre-condition, post-condition(cid:3) triples.
Here guard is a truth assignment to the set of imply con-
ditions (if any), and pre-condition and post-condition
describe the annotation bindings before and after the call-
site. For example, the code
/*@ tainted */ int * foo(/*@ tainted */int *p,
/*@ POST: tainted(*q) */ void *q);
will generate the following summary for foo:
(cid:2)<>,{tainted($1)},{tainted($ret), tainted($1), tainted(∗$2)}(cid:3)
The guard here is empty. The pre-condition states that
ﬁrst argument is tainted before the call. The post-condition
states that after the call (1) the return value is tainted,
(2) the ﬁrst argument remains tainted and (3) the storage
pointed to by the second argument is tainted. There will
be one tuple for each diﬀerent guard expression. Usually
there is only one (empty) guard, and thus one tuple for each
function. The set of summaries for each function is stored
in a summary map, indexed by function name.
After constructing these base summaries, the analysis then
places all annotated functions and their (transitive) callers
into a worklist in topological order (based on the function
call graph). Recursive call chains are broken arbitrarily.
Each function is dequeued from the worklist and analyzed
until its summary converges to a ﬁxed point or a maximum
simulation time is hit. Since callees of a function will be
analyzed before it, each function only needs to be added to
the worklist once.
The analysis analyzes each path in a function (i.e., is ﬂow-
sensitive) and uses caching for speed [4]. It tracks the values
of variables using a symbolic store to record assignments.
Currently we only keep track of all the parameter values and
their one-level dereferences (e.g. *parm, parm->field). 2
Similarly, the analysis uses a predicate store pred store
to evaluate conditions and prune false paths. The predicate
store records simple conditional expressions encountered on
the current path (currently expressions composed from nega-
tion, equality, inequality, and simple bit-wise masks).
It
evaluates each conditional expression it encounters against
these recorded value and, if it is false, skips the true (or
false) branch.
Extensions can control annotations propagation across ex-
pressions in an extension provided method called extension visit
that is called by the analysis on every visited expression. For
example, the tainted checker would specify that performing
arithmetic on a tainted expression results in another tainted
expression.
During intra-procedural analysis (local within a function)
we record the annotations associated with each expression
on the current analyzed path in an annotation store. This
annotation store serves two purposes. First, it allows the
analysis to track annotations as the values they correspond
to ﬂow through assignments and expressions. Second, it al-
lows it to update pre- and postconditions in the function
summaries. When an expression is added to the annota-
tion store, the analysis checks the symbolic store to see if
this expression is a parameter or a one-level dereference of
a parameter.
If it is, then it updates the function’s pre-
condition. When the analysis reaches the end of a path in
a function, the annotation store is used to update the func-
tion’s post-conditions. Later, when a call to this function is
encountered, these post-conditions will be applied.
Figure 8 depicts a contrived example to illustrate how
bottom-up propagation works. Here bar’s pointer argument
2We experimented with deeper value ﬂow analysis (i.e., more
than one level of indirection) but the results thus far were
not worth it:
it dramatically slowed down the analysis,
rarely gave useful information, and as the level of indirec-
tions increased it became more likely that an approximation
error occurred, giving false positives.
void bar(/*@ tainted */void *p);
struct S {char* buf;};
void foo (char **p, struct S* s, char* q) {
char *r, *u, *v;
struct S* ss;
r = *p; // r has sym value *p
bar(r); // taints r and *p
ss = s; // ss has sym value s
bar(ss−>buf); // taints ss and s−>buf
q = v; // q becomes unknown
// will not taint formal parameter q
bar(q);
}
// After the bottom−up propogation algorithm
// ﬁnishes, function foo will be summarized as:
foo (/*@ tainted (*p) */ char **p,
/*@ tainted(s->buf ) */ struct S* s,
char* q);
Figure 8: Bottom-up propagation example. Formal
parameter q is not tainted because it is redeﬁned
before the ﬁnal call to bar.
is annotated as tainted and the annotation propagates us-
ing the bottom-up propagation analysis. At the end of the
analysis foo’s formal parameters are annotated.
We have found that in practice ﬂow-sensitivity is the sin-
gle most important feature to ensure accurate annotation
information. Without it we falsely propagate annotations
beyond where they should go, giving many false positives.
5. STATISTICAL ANNOTATION INFERENCE
This section describes how we statistically infer formal pa-
rameter annotations. This technique is useful for preventing
false negatives caused when a portion of the callgraph (1)
contains no annotations or (2) calls a leaf function whose
source code is unavailable. It automatically infers the most
plausible annotation for unannotated functions, uses a util-
ity metric to order procedures from most to least worthwhile
to annotate, and presents this ranking to the user for inspec-
tion. They typically inspect the top 10-20 and then annotate
them directly. This approach allows users to quickly anno-
tate the parameters whose type values we are most conﬁdent
about. Once these functions are annotated more code can
be annotated (and checked) by re-applying the bottom-up
analysis with these new annotations.
More precisely, our goal is to infer an annotation for the
for the ith formal parameter of function f (denoted f :i) that
agrees with its callers and then order all inferred annotations
from most to least likely. We do this in two steps: (1) pick
the annotation A and (2) compute how likely A is the correct
annotation. The ﬁrst step is trivial: set the annotation for
f :i) to be the annotation A passed most often as the ith
argument to f . If all the annotation for the ith argument at
all callsites to f are known and are the same, the annotation
type for f :i is considered unambiguous and we set f :i to this
annotation and stop. In practice about half of the functions
we analyze are consistent in this way. For the other half
we need to do the second step, and compute the probability
that the annotation A is correct (i.e., that P r(f :i = A)).
We need to do this step exactly when (1) there exists at
least two actual parameters corresponding to f :i that are
annotated but with conﬂicting types and/or (2) some of the
actual parameters are unknown. We brieﬂy defer the problem
of unknowns until Section 5.1, and for now assume that all
actual parameters are annotated (either directly or through
annotation propagation).
A na¨ıve way to compute P r(f :i = A) would be as a per-
centage. Unfortunately this ignores population size. For
example, suppose we have two functions foo and baz whose
ﬁrst parameter was passed a tainted pointer 3 out of 4 times
and 18 out of 24 times respectively. While both have a ratio
of 0.75, we have much more conﬁdence this is the true ratio
for bar. In contrast, the ratio for foo could be coinciden-
tal and could easily change dramatically with more obser-
vations. Thus, instead of percentages we use z-ranking [8,
14], a ranking scheme based on statistical hypothesis test-
ing [13].
It incorporates the intuitions outlined above to
institute ranking, and it utilizes the population size in a
statistically sound way.
For type inference it works as follows. We have two binary
types A and ¬A. We wish to compute a value that tells
us how likely a formal parameter f :i has type A; this will
be done however by examining the behavior we get if we
instead assign the type ¬A to f :i. Let n be the number of
callsites to function f , k the number of actual parameters
corresponding to f :i that have type A, and n−k the number
that have type ¬A. Note if f :i is annotated as type ¬A we
will have X = k type errors. We quantify the reliability of
the type assignment to f :i by computing P (X ≥ k), or the
likelihood we would observe k or more type errors. This
done by assuming that errors have a ﬁxed, a priori error
rate p0. We then model type errors as independent binary
trials, or tosses of a biased coin that has a probability of p0
of turning up as “type error.” By modeling type errors as
binary trials, P (X ≥ k) is computed using the cumulative
Binomial distribution [19]:
P (X ≥ k) =
n
j!p0
Xj=k n
j
(1 − p0)
n−j
(1)
The value computed by Equation 1 is called the p-value.
If k represents the number of type errors we get if we as-
sign type A to f :i, we denote the corresponding p-value as
p(A). The value p(¬A) is deﬁned analogously. A low p(¬A)
implies that f :i is unlikely to have type ¬A. Because we
are using binary types, however, this means that we have
strong conﬁdence in the alternative explanation, namely f :i
has type A. Thus a low p(¬A) implies strong conﬁdence in
the type assignment A to f :i. Because of this implication,
we let s(A) = p(¬A) to represent the conﬁdence “score” for
an assignment of type A to a formal parameter f :i.
The value computed by Equation 1 is called the p-value.
Because we sum from k to n, smaller values are better, since
they correspond to more successes than we expected. For
type inference we typically set p0 = 0.1 since generally the
expected error rate is low. Our experiments were not that
sensitive to the exact value chosen for p0.
5.1 Unknowns as meta-annotations
Of course, we often cannot determine the annotation of all
arguments at a given callsite. The presence of such unknowns
indicates a (possibly checker-speciﬁc) analysis failure. We
have only limited experience with unknowns in the context of
annotation inference. However, initial results indicate that
they are either (1) innocuous and can be safely ignored or
(2) that they instead indicate a construct or code that the
checker cannot handle.
In this latter case, the higher the
proportion of unknowns the less conﬁdence we should have
in our inferred annotations. Section 7.3 gives an example
of how to incorporate this information into our annotation
inference.
5.2 Next-best annotation
So far we have discussed ranking formal parameters by
type conﬁdence. Although this is useful, it does not reﬂect
the impact of an annotation. The impact or “utility” of an
annotation is the increased annotation coverage when the
bottom-up analysis is re-applied. This may be cumbersome
to compute; an approximate measure of the impact of an-
notating a formal parameter is the number of actual param-
eters it will annotate (i.e., the number of unknowns). Our
estimate of the impact of an annotation is also based on our
conﬁdence of a type assignment; formal parameters whose
type we are very conﬁdent about but have many unknown
actual parameters will be the annotations we expect to have
the highest impact.
Consequently, if we wish to rank formal parameters by
the utility of annotating them with a type A, we use the
following metric:
utility = [1 − s(A)] × u
(2)
Here u is the number of actual parameters marked unknown.
We use the complement 1 − s(A) so that larger values of
utility are better. In practice utility ranking is eﬀective; in
the tainted checker we use it rank formal parameters that
are most likely to be tainted and cause the greatest impact
by being annotated.
6. A TOY CAPABILITY CHECKER
Operating systems such as Linux use capabilities to en-
force access control to certain sensitive data in the kernel.
Missed capability checks allow user processes to bypass se-
curity policies and potentially gain unauthorized access to
sensitive data. In this section, we use a toy example to illus-
trate how MECA can be used by a checker that ﬂags missed
capability checks.
The checker deﬁnes two predicates, guard and noguard.
The guard predicate speciﬁes that an annotated type or vari-
able is protected by a given capability. Conversely, noguard
exempts certain structure ﬁelds from being protected by an
enclosing annotation.
These two predicates are deﬁned on lines 1-2 in Figure 9.
The $variable ﬂag in the deﬁnitions denotes that they de-
scribe properties of program variables.
The annotation guard(cap, SYS ADMIN) on line 5 indi-
cates that the data ﬁeld deﬁned on that line is protected by
the ﬁeld cap, which must contain the SYS ADMIN capability.
Line 15 gives a more exuberant use of the annotation, which
uses it to protect the entire structure S2 rather than just a
single ﬁeld. To make things more interesting, suppose the
useless ﬁeld on line 10 does not need any protection. In
this case, we use the noguard predicate to exempt useless
from the enclosing protection.
Linux uses the function capable to do capabilities checks.
We wrote a simple checker that tracks all successful capa-
bility checks on each path and records these in a “capability
annot noguard annotates ($variable); */
/*@ guard(cap, SYS RAWIO) */ int data;
int cap;
/*@ noguard */ int useless;
/* local annotation noguard overwrites guard */
int data;
int data2;
int cap;
1 : /*@ annot guard annotates ($variable);
2 :
3 :
4 : struct S{
5 :
6 :
7 : };
8 :
9 : struct S2{
10:
11:
12:
13:
14: