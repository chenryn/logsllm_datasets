# 22 \| NWR算法：如何修改读写模型以提升性能？你好，我是陶辉。 前两讲我们介绍数据库的扩展时，写请求仍然在操作中心化的 Master单点，这在很多业务场景下都是不可接受的。这一讲我将介绍对于无单点的去中心化系统非常有用的NWR算法，它可以灵活地平衡一致性与性能。 最初我们仅在单机上部署数据库，一旦性能到达瓶颈，我们可以基于 AKF Y轴将读写分离，这样多个 Slave 从库将读操作分流后，写操作就可以独享 Master主库的全部性能。然而主库作为中心化的单点，一旦宕机，未及时同步到从库的数据就有可能丢失。而且，这一架构下，主库的故障还会导致整个系统瘫痪。 去中心化系统中没有"Master 主库"这一概念，数据存放在多个 Replication冗余节点上，且这些节点间地位均等，所以没有单点问题。为了保持强一致性，系统可以要求修改数据时，必须同时写入所有冗余节点，才能向客户端返回成功。但这样系统的可用性一定很成问题，毕竟大规模分布式系统中，出现故障是常态，写入全部节点的操作根本无法容错，任何1个节点宕机都会造成写操作失败。而且，同步节点过多也会导致写操作性能低下。 NWR算法提供了一个很棒的读写模型，可以解决上述问题。这里的"NWR"，是指在去中心化系统中将1 份数据存放在 N 个节点上，每次操作时，写 W 个节点、读 R个节点，只要调整 W、R 与 N 的关系，就能动态地平衡一致性与性能。NWR 在NoSQL 数据库中有很广泛的应用，比如 Amazon 的 Dynamo 和开源的Cassandra，这些数据库往往跨越多个 IDC数据中心，包含成千上万个物理机节点，适用于海量数据的存储与处理。 这一讲，我们将介绍 NWR算法的原理，包括它是怎样调整读写模型来提升性能的，以及 Cassandra数据库是如何使用 NWR 算法的。 从鸽巢原理到 NWR 算法NWR 算法是由鸽巢原理得来的：如果 10 只鸽子放入 9 个鸽巢，那么有 1个鸽巢内至少有 2只鸽子，这就是鸽巢原理，如下图所示： ![](Images/43eb3dcead30e7c6f8151b7326859866.png)savepage-src="https://static001.geekbang.org/resource/image/83/17/835a454f1ecb8d6edb5a1c2059082d17.jpg"}](https://zh.wikipedia.org/wiki/%E9%B4%BF%E5%B7%A2%E5%8E%9F%E7%90%86)图片来源：https://zh.wikipedia.org/wiki/%E9%B4%BF%E5%B7%A2%E5%8E%9F%E7%90%86你可以用反证法证明它。鸽巢原理虽然简单，但它有许多很有用的推论。比如\[第 3 课\介绍了很多解决哈希表冲突的方案，那么，哈希表有没有可能完全不出现冲突呢？**鸽巢原理告诉我们，只要哈希函数输入主键的值范围大于输出索引，出现冲突的概率就一定大于0；只要存放元素的数量超过哈希桶的数量，就必然会发生冲突。** 基于鸽巢原理，David K. Gifford 在 1979年首次提出了Quorum算法（参见《 [Weighted Votingfor Replicated Dataslate-object="inline"》论文），解决去中心化系统冗余数据的一致性问题。而 Quorum算法提出，如果冗余数据存放在 N 个节点上，且每次写操作成功写入 W个节点（其他 N - W 个节点将异步地同步数据），而读操作则从 R个节点中选择并读出正确的数据，只要确保 W + R \> N，同 1条数据的读、写操作就不能并发执行，这样客户端就总能读到最新写入的数据。特别是当W \> N/2 时，同 1条数据的修改必然是顺序执行的。这样，分布式系统就具备了强一致性，这也是NWR 算法的由来。 比如，若 N 为 3，那么设置 W 和 R 为 2时，在保障系统强一致性的同时，还允许 3 个节点中 1个节点宕机后，系统仍然可以提供读、写服务，这样的系统具备了很高的可用性。当然，R和 W的数值并不需要一致，如何调整它们，取决于读、写请求数量的比例。比如当 N为 5 时，如果系统读多写少时，可以将 W 设为 4，而 R 设为2，这样读操作的性能会更好。 NWR 算法最早应用在 Amazon推出的Dynamo 数据库中，你可以参见 2007 年 Amazon发表的 [《Dynamo:Amazon's Highly Available Key-valueStore》  slate-object="inline"论文。2008 年 Dynamo 的作者 Avinash Lakshman 跳槽到FaceBook，开发了 Dynamo的开源版数据库Cassandra，它是目前最流行的 NoSQL数据库之一，在 Apple、Netflix、360 等公司得到了广泛的应用。想必你对 NWR算法的很多细节并不清楚，那么接下来我们以 Cassandra 为例，看看 NWR是如何应用在实际工程中的。 Cassandra 数据库是如何使用 NWR 算法的？1 个 Cassandra 分布式系统可以由多个 IDC数据中心、数万个服务器节点构成，这些节点间使用 RPC 框架通信，由于Cassandra 推出时 gRPC（参见\[第 18 课\slate-object="inline"）还没有诞生，因此它使用的是性能相对较低的 Thrift RPC框架（Thrift 的优点是支持的开发语言更多）。同时，Cassandra虽然使用宽列存储模型（每行最多可以包含20 亿列slate-object="inline"数据），但**数据的分布是基于行 Key进行的** ，它和 Dynamo一样使用了一致性哈希算法，将 Key对应的数据存储在多个节点中。关于一致性哈希算法，我们会在 \[第 24 课\再详细介绍。 Cassandra 对客户端提供一种类 SQL的CQL 语言，你可以使用下面这行 CQL语句设定数据存储的冗余节点个数，也就是 NWR 算法中的 N（也称为Replication Factor）：     CREATE KEYSPACE excalibur      WITH REPLICATION = {'class' : 'NetworkTopologyStrategy', 'dc1' : 3};上面这行 CQL 语句设置了每行数据在数据中心 DC1 中存储 3 份冗余，即 N =3，接下来我们通过下面的 CQL 语句，将读 R、写 W 的节点数都设置为1：     cqlsh> CONSISTENCY ONE    Consistency level set to ONE.    cqlsh> CONSISTENCY    Current consistency level is ONE.**此时，Cassandra的性能最高，但达成最终一致性的耗时最长，丢数据风险也最大。**如果业务上对丢失少量数据不太在意，可以采用这种模型。此时修改数据时，客户端会并发地向3 个存储节点写入数据，但只要 1 个节点返回成功，Cassandra就会向客户端返回写入成功，如下图所示： ![](Images/396a4772c6d5bd14919026fb8e2e60f8.png)savepage-src="https://static001.geekbang.org/resource/image/74/1d/742a430b92bb3b235294805b7073991d.png"}](https://docs.datastax.com/en/cassandra-oss/3.x/cassandra/dml)该图片及以下图片来源：https://docs.datastax.com/en/cassandra-oss/3.x/cassandra/dml上图中的系统由 12个主机节点构成，由于数据采用一致性哈希算法分片，故构成了一个节点环。其中，本次写入的数据被分布到1、3、6 这 3 个节点中存储。客户端可以随机连接到系统中的任何一个节点访问Cassandra，此时该节点被称为 Coordinator Node，由它根据 NWR的值来选择一致性模型，访问存储节点。 再来看读取数据的流程。下图中，作为 Coordinator Node 的节点 10首先试图读取节点 1 中的数据，但发现节点 1 已经宕机，于是改选节点 6并获取到数据，由于 R = 1于是立刻向客户端返回成功。 ![](Images/376b25ebd9ec54b360592a5cc850a322.png)savepage-src="https://static001.geekbang.org/resource/image/50/d6/5038a63ce8a5cd23fcb6ba2e14b59cd6.jpg"}如果我们将 R、W 都设置成 2，这就满足了 R + W \> N(3)的场景，此时系统具备了强一致性。客户端读写数据时，必须有 2个节点返回，才算操作成功。比如下图中读取数据时，只有接收到节点 1、节点 6的返回，操作才算成功。 ![](Images/085288ebdb85dc4c523d63bbbca6e5cb.png)savepage-src="https://static001.geekbang.org/resource/image/8d/0f/8dc00f0a82676cb54d21880e7b60c20f.jpg"}上图中的蓝色线叫做 Read repair，如果节点 3上的数据不一致，那么本次读操作可以将它修复为正确的数据。说完正常场景，我们再来看当一个节点出现异常时，NWR是如何保持强一致性的。 下图中，客户端 1 在第 2 步，同时向 3 个存储节点写入了数据，由于节点1、3 返回成功，所以写入操作实际已经完成了，但是节点 6由于网络故障，却一直没有收到 Coordinator Node发来的写入操作。在强一致性的约束下，客户端 2 在第 5步发起的读请求，必须能获取到第 2 步写入的数据。然而，客户端 2 连接的Coordinator Node 与客户端 1 不同，它选择了节点 3 和节点6，这两个节点上的数据并不一致。**根据不同的 timestamp 时间戳，Coordinator Node 发现节点 3上的数据才是最后写入的数据，因此选择其上的数据返回客户端。这也叫Last-Write-Win 策略。** ![](Images/4024451cdedfba3ef89a1eb9756dc483.png)savepage-src="https://static001.geekbang.org/resource/image/4b/fa/4bc3308298395b7a57d9d540a79aa7fa.jpg"}](https://blog.scottlogic.com/2017/10/06/cassandra-eventual-consistency.html)图片来源：https://blog.scottlogic.com/2017/10/06/cassandra-eventual-consistency.htmlCassandra提供了一个简单的方法，用于设置读写节点数量都过半，满足强一致性的要求，如下所示：     cqlsh> CONSISTENCY QUORUM    Consistency level set to QUORUM.    cqlsh> CONSISTENCY    Current consistency level is QUORUM.最后我们再来看看多数据中心的部署方式。下图中 2 个数据中心各设置 N =3，其中 R、W 则采用 QUORUM 一致性模型。当客户端发起写请求到达节点 10这个 Coordinator Node 后，它选择本 IDC Alpha 的 1、3、6节点存储数据，其中节点 3、6 返回成功后，IDC Alpha便更新成功。同时找到另一 IDC Beta 的节点 11 存储数据，并由节点 11将数据同步给节点 4 和节点 8。其中，只要节点 4 返回成功，IDC Beta也就成功更新了数据，此时 Coordinator Node会向客户端返回写入成功。 ![](Images/e36f7499e77dc33a0aaa12c13e5e1e61.png)savepage-src="https://static001.geekbang.org/resource/image/5f/00/5fe2fa80bb20e04d25c41ed5986c0c00.jpg"}读取数据时，这 2 个 IDC 内必须由 4 个存储节点返回数据，才满足 QUORUM一致性的要求。下图中，Coordinator Node 获取到了 IDC Alpha 中节点 1、3、6的返回，以及 IDC Beta 中节点 11 的返回，就可以基于 timestamp时间戳选择最新的数据返回客户端。而且 Coordinator Node 会并发地发起 Readrepair，试图修复 IDC Beta 中可能存在不一致的节点 4 和8。 ![](Images/8f48b3e1dc2b7661449e93a0fc44a312.png)savepage-src="https://static001.geekbang.org/resource/image/59/19/59564438445fb26d2e8993a50a23df19.jpg"}Cassandra 还有许多一致性模型，比如 LOCAL_QUORUM 只要求本地 IDC内有多数节点响应即可，而 EACH_QUORUM 则要求每个 IDC内都必须有多数节点返回成功（注意，这与上图中 IDC Alpha 中有 3个节点返回，而 IDC Beta 则只有 1 个节点返回的 QUORUM是不同的）。你可以从这个页面找到 Cassandra支持的所有一致性模型，但无论如何变化，都只是在引入数据中心、机架等概念后，局部性地调节NWR 而已。 小结这一讲我们介绍了鸽巢原理，以及由此推导出的 NWR 算法，并以流行的 NoSQL数据库 Cassandra 为例，介绍了 NWR在分布式系统中的实践。 当鸽子的数量超过了鸽巢后，就要注定某一个鸽巢内一定含有两只以上的鸽子，同样的道理，只要读、写操作涉及的节点超过半数，就注定读写操作总包含一个含有正确数据的节点。NWR算法将这一原理一般化为：只要读节点数 R + 写节点数 W \> 存储节点数N，特别是 W \> N/2时，就能使去中心的分布式系统获得强一致性。 支持上万节点的 Cassandra 数据库，就使用了 NWR算法来保持一致性。当然，Cassandra支持多种一致性模型，当你需要更强劲的性能时，你可以令 R + W \ 路由器 A -\> 路由器 B -\> 主机 3 -\> 路由器 B-\> 主机 4，既多出了主机 3、路由器 B 这两个传输环节，而且进入主机 3后，协议栈的处理深度也增加了。最后，单次传输路径中引入了功能复杂的主机，相比仅由网络设备参与的 IP多播，可靠性、稳定性也降低了。说完缺点，我们再来看应用层多播的优点。1.  首先，它回避了 IP    多播的问题，无须改变现有组网环境，也不需要管理组播 IP    地址，立刻就可以应用在当下的生产环境中；        2.  其次，在数以万计的大规模集群下，单一发布源很容易被流量打爆，进而导致分发流程停止，应用层多播可以避免这一问题；        3.  再次，通过应用层节点的接力分发，整个传输带宽被大幅度提高了，分发速度有了数量级上的飞跃；        4.  最后，如果分发集群跨越不同传输成本的网络（比如多个区域 IDC    构成的集群），在应用层也很容易控制分发策略，进而减少高成本网络的数据传输量，提升经济性。        所以，综合来说，**集群规模越大，应用层多播的优势也越大。**实际上十多年前，我们在使用BT、迅雷下载时，就已经接触到应用层多播协议了，接下来我们结合 2个服务器端的案例，看看多播协议的实现与应用。应用层多播协议是如何工作的？其实，应用层多播主要是指一种P2P（Peer toPeer）  slate-object="inline"网络传输思想，任何基于 IP单播的通讯协议都可以拿来使用。比如针对于在分布式集群中分发软件安装包的场景，完全可以使用HTTP协议实现应用层的分发，阿里巴巴开源的Dragonfly 蜻蜓slate-object="inline"就是这么做的。蜻蜓拥有 1个中心化的集群服务节点：SuperNode，其中既可以直接保存着源文件，也可以通过HTTP 缓存加速第三方文件源（比如 Docker 仓库）。集群中的每个节点都要启动dfget 进程（替代了传统的wget），它就像我们平时使用的迅雷，在下载文件的同时，也会将自己下载完成的文件传输给其他节点。其中，通过HTTP 的 Range 文件分段下载规范，dfget就可以实现断点续传、多线程下载等功能，如下图所示：![](Images/138eccd74942628ff8e650d9e3a0aa9a.png)savepage-src="https://static001.geekbang.org/resource/image/2f/00/2fbba7f194bd3bcabded582467056700.png"}](https://github.com/DarLiner/Dragonfly/blob/master/docs/zh/architecture.md)图片及下图来源：https://github.com/DarLiner/Dragonfly/blob/master/docs/zh/architecture.md各个 dfget 程序间通过 SuperNode 协调传输关系，这样绝大部分 dfget程序并不需要从 SuperNode 节点下载文件。比如上图中的节点 C，通过 HTTPRange 协议从节点 A 中下载了文件的第 1 块，同时并发地从节点 B中下载了文件的第 2 块，最后再把这些 Block块拼接为完整的文件。这里你可能会想，这不就是一个 P2P下载工具么？是的，但你站在集群运维的角度，这就是基于应用层多播协议的文件分发工具。当每个节点部署dfget 服务后，新版本安装包发布时，就可以由 SuperNode 节点推送，经由各个dfget进程以多播的形式分发下去，此时性能会获得大幅度的提升。![](Images/7227792b60bfeee41d4d9d204de218ad.png)savepage-src="https://static001.geekbang.org/resource/image/36/82/36de22b49038a6db2b8bc7ce953e5c82.png"}上图是传统的 wget单播与蜻蜓多播分发文件的性能对比图。我们可以看到，传统方式下，**分发客户端越多（Y 轴）总分发时长（X 轴）就越大，特别是1200个以上的并发节点下载文件时，会直接将文件源打爆。**而采用应用层多播方式后，下载时长要低得多，而且伴随着节点数的增加，下载时长也不会增长。蜻蜓虽然传输效率很高，但 SuperNode却是保存着全局信息的中心节点，一旦宕机就会造成系统不可用。我们再来看去中心化的Gossip 流言协议slate-object="inline"是如何实现应用层多播的。Gossip 协议也叫 epidemic传染病协议，工作原理如下图所示。在这个分布式网络中，并没有任何中心化节点，但只要第1个种子节点被感染为红色后，每个节点只需要感染其相邻的、有限的几个节点，最终就能快速感染网络中的所有节点（即仅保证最终一致性）。![](Images/de7b15095cc60549de7107300d651a88.png)savepage-src="https://static001.geekbang.org/resource/image/fa/b5/fa710dc6de9aa9238fee647ffdb69eb5.gif"}当然，所谓的"感染"就是数据的传输，这一算法由 1987年发布的《 [Epidemicalgorithms for replicated databasemaintenance  slate-object="inline"》论文提出，同时证明了算法的收敛概率。Cassandra数据库、Fabric 区块链、Consul 系统等许多去中心化的分布式系统，都使用Gossip 协议管理集群中的节点状态。以 Cassandra为例，每秒钟每个节点都会随机选择 1 到 3 个相邻节点，通过默认的 7000端口传输包含节点状态的心跳信息，这样集群就可以快速发现宕机或者新增的节点。![](Images/142b6303922ebcb41e26a424f8f914b6.png)savepage-src="https://static001.geekbang.org/resource/image/79/fe/7918df00c24e6e78122d5a70bd6bd2fe.png"}](https://www.linkedin.com/pulse/gossip-protocol-inside-apache-cassandra-soham-saha)图片来源：https://www.linkedin.com/pulse/gossip-protocol-inside-apache-cassandra-soham-saha小结这一讲我们介绍了应用层的多播协议。网络层的 IP多播功能有限，对网络环境也有过多的要求，所以很难通过多播协议提升传输效率。基于IP 单播协议（如 TCP 或者UDP），在应用代码层面实现分布式节点间的接力转发，就可以实现应用层的多播功能。在分布式集群的文件分发场景中，阿里开源的Dragonfly 蜻蜓可以将发布节点上的源文件，通过 HTTP协议推送到集群中的每个节点上，其中每个节点在应用层都参与了多播流量分发的实现。当节点数到达千、万级时，蜻蜓仍然能保持较低的分发时延，避免发布节点被下行流量打爆。在完全去中心化的分布式集群中，每个节点都没有准确的全局信息，此时可以使用Gossip流言协议，通过仅向有限的相邻节点发送消息，完成整个集群的数据同步，实现最终一致性。因此，Gossip协议常用于大规模分布集群中的节点状态同步。思考题最后，留给你一道讨论题。在 5G 完成设备层的组网后，类似华为 NewIP这样的基础协议层也在做相应的重构，其中Multicast VPN协议  slate-object="inline"就将现有 IPv4 无法在公网中推广的多播功能，在 VPN逻辑链路层实现了。你对未来多播协议的发展又是如何看的？欢迎你在留言区与大家一起探讨。感谢阅读，如果你觉得这节课让你了解到应用层的多播协议，而通过它可以大幅度提升分布式集群的网络传输效率的话，也欢迎你把今天的内容分享给你的朋友。