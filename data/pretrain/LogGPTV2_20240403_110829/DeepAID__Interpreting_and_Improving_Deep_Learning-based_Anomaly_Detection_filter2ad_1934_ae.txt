of interpreters(lower is better).
existing in-distribution samples (S.R.T.D.) performs poorly espe-
cially with fewer dimensions, which demonstrates the necessity
of searching reference in DeepAID. Results also demonstrate that
supervised interpreters are not suitable for unsupervised learning
due to their low fidelity. Particularly, DeepAID significantly out-
performs other methods, especially when using fewer dimensions
(e.g., DeepAID with 10% dimensions exceeds others by >50%).
Stability Evaluation. Stability of interpreters refers to the simi-
larity of interpretations of the same samples during multiple runs.
Like [14, 54], we ignore the value in results and keep the index of
important feature dimensions. Given two interpretation vectors
ğ‘£1 and ğ‘£2 of important dimension indexes, we leverage Jaccard
Similarity (JS) to measure the similarity of two results, which is
defined as |set(ğ‘£1) âˆ© set(ğ‘£2)|/|set(ğ‘£1) âˆª set(ğ‘£2)|. We repeatedly
measure the similarity of two interpretation results (from two runs
with the same setting) for each anomaly and calculate the aver-
age. The results are shown in Figure 5b. We can observe that ap-
proximation/perturbation based methods perform poorly due to
their random sampling/perturbation, while back propagation based
methods (DeepAID and DeepLIFT) have strong stability.
Below, we evaluate two kinds of robustness of interpreters: ro-
bustness to random noise and adversarial attacks. Only tabular
Kitsune is evaluated since time series in DeepLog is discrete.
Robustness Evaluation (to Noise). We measure JS of results in-
terpreting tabular features before and after adding noise sampling
from Gaussian N(0, ğœ2) with ğœ = 0.01. As shown in Figure 5c,
S.R.T.D. shows a high sensitivity to noise when selecting reference.
COIN and DeepLIFT are relatively robust compare with other base-
lines (note that those unstable interpreters perform poorly even
without noise), but are still lower than our DeepAID.
Robustness Evaluation (to Attacks). DeepAID falls into the cat-
egory of back propagation (B.P.) based method. Thus, we borrow
the ideas from existing adversarial attacks against B.P. based inter-
preters [18, 60] to develop an adaptive attack on DeepAID, called
optimization-based attack, which can be defined as adding small
perturbation on anomaly ğ’™â—¦ (denoted with Ëœğ’™â—¦) that can induce large
changes of finally searched reference ğ’™âˆ—. Formally,
argmax Ëœğ’™â—¦âˆ¥Dğ‘¡ğ‘ğ‘( Ëœğ’™â—¦; Ëœğ’™â—¦) âˆ’ Dğ‘¡ğ‘ğ‘(ğ’™â—¦; ğ’™â—¦)âˆ¥ğ‘ s.t. âˆ¥ Ëœğ’™â—¦ âˆ’ ğ’™â—¦âˆ¥ğ‘  0.91 when
ğ›¿ğ‘ = 0.2 and linearly decreases as ğ›¿ğ‘ increases in 5b/5c), thanks to
the search-based idea analyzed in Â§4.2 (RQ-A). Results also demon-
strate the effectiveness of I.R.N. In 5a, I.R.N. mitigates most attack
effect when ğœğ‘› â‰¥ 0.02 (RQ-B). We also find I.R.N. has a very small
impact on the original stability (JS > 0.95 even when ğœğ‘› = 0.04),
which can be viewed as a trade-off. Hence, a simple and safe way
is to choose a small ğœğ‘›, which can also mitigate the attack effect
without loss of the original stability, as demonstrated in 5c (RQ-C).
We also evaluate another type of adversarial attacks that mis-
leads the distance calculations in DeepAID (called distance-based
attacks). The results demonstrate the strong robustness of DeepAID
against such attacks. For reasons of space, detailed definitions, re-
sults, and analysis against such attacks are in Appendix D.2.
Efficiency Evaluation. We evaluate the efficiency by recording
runtime for interpreting 2, 000 anomalies for each interpreter. For
approximation-based interpreters (LIME, LEMNA, and COIN), the time
10%20%30%40%% Dimensions0.000.250.500.751.00LFRKitsune (Tabular, Autoencoder)20%40%60%80%% Dimensions0.000.250.500.751.00LFRDeepLog (Time-Seires, RNN)10%20%30%40%% Dimensions0.000.250.500.751.00Jaccard SimilarityKitsune (Tabular, Autoencoder)20%40%60%80%% Dimensions0.000.250.500.751.00Jaccard SimilarityDeepLog (Time-Seires, RNN)10%20%30%40%% Dimensions0.000.250.500.751.00Jaccard SimilarityKitsune (Tabular, Autoencoder)LIMELEMNACOINDeepLIFTCADES.R.T.D.DeepAID0.010.020.030.04Neighborhood scale Ïƒn 0.900.920.940.960.981.00Jaccard SimilarityKitsune (Tabular, Autoencoder)0.100.200.300.40Attack scale Î´a0.850.880.910.940.971.00Jaccard SimilarityKitsune (Tabular, Autoencoder)0.100.200.300.40Attack scale Î´a0.850.880.910.940.971.00Jaccard SimilarityKitsune (Tabular, Autoencoder)w/. Attack, w/o. I.R.N. (RQ-A)w/. Attack, w/. I.R.N. (RQ-B)w/o. Attack, w/. I.R.N. (RQ-C)LIMELEMNACOINCADEDeepLIFTDeepAID100101102103104105Time Elapsed (Sec)Kitsune (Tab, AE)LIMELEMNACOINDeepLIFTDeepAID100101102103104105DeepLog (T-S, RNN)Session 12A: Applications and Privacy of ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3206Table 3: Example interpretations (ğ¾ = 5) for Kitsune.
Feature Name xâ—¦ <> xâˆ— 1 Feature Meaning
HpHp_1.0_mean
HpHp_1.0_std
HH_1.0_mean
HpHp_5.0_std
HpHp_3.0_mean 90  60
30 > 10
90 > 60
20 > 10
(a) Ground truth: Remote command execution
xâ—¦ <> xâˆ— 1 Feature Meaning
(b) Ground truth: ARP scan
Feature Name
MI_dir_5.0_weight 105 > 104 # packets send from the same MAC&IP within 5 sec time window A large number of packets sent from the same host and long-term (5 sec)
MI_dir_0.1_weight 300 > 100 # packets send from the same MAC&IP within 0.1 sec time window A large number of packets sent from the same host (possibly Scan)
Expertâ€™s understanding
MI_dir_5.0_std
MI_dir_0.1_std
HH_1.0_weight
0.1  10 The src-dest covariance of packet length of the current connection within 1 sec time window
xâ—¦ <> xâˆ— 1 Feature Meaning
1028 > 1 The src-dest correlation coefficient of packet length of the current connection within 1 sec time window
1028 > 10 The src-dest covariance of packet length with the same src-dest IPs within 0.1 sec time window
1028 > 1 The src-dest correlation coefficien of packet length with the same src-dest IPs within 0.1 sec time window
HpHp_3.0_covariance 1029 > 10 The src-dest covariance of packet length of the current connection within 3 sec time window
HH_0.1_covariance
HH_0.1_pcc
(c) False positive example
Expertâ€™s understanding
There may be bugs or bias in the
implementation of feature extraction
TP
66,597
66,476
FP
Before
1,556
After
79
Change -0.17%â†“ -94.92%â†“
* The points in the left
figure are all normal data
(true label).
Log Key Meaning
Log Key Meaning
5
9
11
17
Receiving block
Receiving block with size
Responder for block terminating
Failed to transfer block
Starting thread to transfer block
NameSystem(NS).allocateBlock
Ask to replicate block
NS.addStoredBlock: blockMap updated
Figure 7: Example interpretations (ğ¾ = 1, ğ‘¡ = 6) for DeepLog.
18
22
25
26
to train the surrogate model will also be counted, and the same for
time to train the encoding model in CADE. The results are shown in
Figure 6 under tabular and time-series based scenarios. We observe
that DeepAID and DeepLIFT are at least two orders of magnitude
faster than the other methods.
Conclusions. We conclude experiments of Â§6.2 here (and also in
Table 1) that only our DeepAID can effectively meet all special
requirements of security domains and produce high-quality inter-
pretations for unsupervised DL-based anomaly detection.
Below, we will showcase how DeepAID can help to improving
the practicability of security systems from several aspects. Figure
2 shows the flowchart of DeepAID usage with the following use
cases.
6.3 Understanding Model Decisions
In this section, we use several cases to demonstrate that DeepAID
can capture well-known rules ( A in Figure 2) as well as discover
new knowledge ( B ).
Tabular Data Interpretations. We use cases of network intrusion
detector Kitsune for illustration. Two representative anomalies in
Mirai botnet traffic are interpreted, where the first one is to re-
motely control the compromised bot to execute malware â€œmirai.exeâ€
and execute a series of commands to collect information, and the
second one is to ARP scan of open hosts in its LAN for lateral move-
ment. Suppose that the operator already knows the ground truth
(mentioned above) of these two anomalies by analyzing the raw
1For ease of illustration, the feature values have been de-normalized and approximated.
Figure 8: Case for debugging Kitsune.
traffic, their goal is to see whether the DL model has captured well-
known rules with the help of DeepAID Interpreter. Here we use
ğ¾ = 5 important feature dimensions, and their names and meanings
are listed in Table 3(a)(b), where the second column is the value of
anomaly xâ—¦ and reference vector xâˆ— in corresponding dimensions.
Expertâ€™s understandings based on the interpretations are also listed.
Since (a) and (b) in Table 3 are both interpretable and reasonable
for experts, we can draw the conclusion that DL model has learned
the expected well-known rules.
Time-Series Interpretations. As shown in Figure 7, we use two
simple cases of HDFS log anomaly detector DeepLog for illustra-
tion. We set sequence length ğ‘¡ = 6 and use only ğ¾ = 1 dimension
for interpretation. Recall that time-series Interpreter needs to de-
termine the location of anomalies before solving reference vector
Xâˆ— through saliency testing (Â§4.3). In case 1, our Interpreter de-
termines that the abnormality occurs at ğ’™â—¦
ğ‘¡ and replaces 17 with
11. Since 17 is originally an abnormal log, we conclude that our
Interpreter captures well-known rules. In case 2, Interpreter deter-
mines that abnormality occurs at ğ’™â—¦
1 with
5. The anomaly is probably because blockmap is updated without
any preceding block operations. This is not an explicit rule and
needs to be analyzed by domain experts, which demonstrates that
DeepAID can discover new heuristics beyond explicit knowledge.
ğ‘¡âˆ’1 and replaces ğ’™â—¦
2...ğ’™â—¦
1ğ’™â—¦
6.4 Enabling Debuggability
Below, we primarily use cases of Kitsune to illustrate how to use
DeepAID interpretations to diagnose explicit errors in the system
Anomaly KeySequenceDeepAID Interpretation (b) Case 2(    )(a) Case 1(    )11918252617119182526111825119918251199265Session 12A: Applications and Privacy of ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3207Table 4: Performance of reliable detection based on Distiller.
Method
RF
MLP
Kitsune+
DeepAID
K=5
K=10
K=15
f1-micro
0.9980
0.8207
0.9782
0.9891
0.9990
Number of Classes = 5
f1-macro
0.9983
0.8491
0.9821
0.9911
0.9991
f1-micro*
0.6876
0.8637
0.8904
0.9690
0.9779
f1-macro*
0.7351
0.8399
0.8444
0.9641
0.9736
UACC
N/A(0.)
N/A(0.)
0.7330
0.7944
1.0
f1-micro
0.9836
0.8732
0.9415
0.9797
0.9975
Number of Classes = 10
f1-macro
0.9827
0.8791
0.9408
0.9790
0.9975
f1-micro*
0.7629