d
a
M
s
n
D
S
N
D
n
y
D
S
N
D
a
r
t
l
U
)
s
(
y
c
n
e
t
a
l
e
t
a
d
p
U
100
10
1
0.1
Figure 6: Lookup latency: Auspice with 5 replicas is
comparable to UltraDNS (16 replicas); Auspice with
15 replicas has 60% lower latency than UltraDNS.
Figure 7: Update propagation delay: Auspice with 5
replicas is 1.0 to 24.7 secs lower than three top-tier
managed DNS service providers.
server’s GUID to its new socket address (IP, port), which
takes roughly 50ms and mostly corresponds to the round-
trip delay between the client and the Auspice nameserver.
The remaining 250ms roughly correspond to 2 RTTs of de-
lay between the client and the server that are separated by
a round-trip delay of 120ms.
4.3.3 Context-aware delivery
Next, we show a proof of concept of context-aware com-
munication, a novel communication primitive enabled by
Auspice’s extensible key-value API. Auspice allows applica-
tions to bind an msocket not only to human-readable names
or GUIDs, but also to abstract context descriptors as in
msocket.bind("[geoloc: [lat,long],radius]"). Writes
to this msocket are reliably delivered to all GUIDs in the
geo-fence created by this descriptor. Underneath the cov-
ers, msocket invokes Auspice to create on-demand a group
GUID, i.e., a GUID with a membership ﬁeld consisting of a
set of member GUIDs, and obtains this member set. msocket
internally resolves each member GUID to its socket address
and establishes an msocket connection for reliably delivery.
Figure 5(c) shows an experiment involving a group cre-
ator (also the message sender) on an Android phone and
a number of potential members on PlanetLab nodes, 5 of
which fake-register their coordinates in Auspice so as to ap-
pear to be within the created geo-fence. The RTT between
the group creator and members is 125ms. The ﬁgure shows
that group creation, a single call to Auspice that returns
all member GUIDs, takes roughly 200ms. Subsequently, an
internal msocket connect to each member involves another
Auspice lookup to resolve the GUID to a socket address and
connect in parallel to all 5 members, which takes 250-280ms.
After this, the creator sends 3 short messages back-to-back
that each take roughly 1 RTT to be reliably delivered.
More details of optimizing context-aware queries in Aus-
pice, reducing membership staleness, the connection migra-
tion protocol, etc. are outside the scope of this paper [6].
This experiment seeks only to exemplify a powerful, new
communication primitive enabled by context descriptors com-
pared to strictly hierarchical DNS names, as argued in §2.2.
4.4 Auspice vs. managed DNS providers
Can demand-aware replication beneﬁt commercial man-
aged DNS providers that largely rely on statically replicating
today’s (hardly mobile) domain names? To investigate this,
we compare Auspice against three top-tier providers, Ul-
traDNS, DynDNS, and DNSMadeEasy that oﬀer geo-replicated
authoritative DNS services widely used by enterprises (e.g.,
Dyn provides DNS service for Twitter).
4.4.1 Lookup latency
We compare Auspice to UltraDNS for a workload of lookups
for domain names serviced by the provider. We identify
316 domain names among the top 10K Alexa websites ser-
viced by this provider, and determine the geo-distribution of
lookups for each name from their data [2]. For each name,
we measure the latency for 1000 lookups from across 100
PlanetLab nodes. We ensure that lookups are served from
the name servers maintained by the provider by request-
ing the address for a new random sub-domain name each
time, e.g, xqf4p.google.com instead of google.com, that is
unlikely to exist in a cache and requires an authoritative
lookup. Auspice name servers are deployed across a total of
80 PlanetLab locations while UltraDNS has 16 known server
locations [50]. We evaluate Auspice for three conﬁgurations
with 5, 10, and 15 replicas of a name respectively.
Figure 6 shows the lookup latencies of names for Auspice
and for UltraDNS. UltraDNS incurs a median latency of 45
ms with 16 replicas, while Auspice incurs 41 ms, 22 ms,
and 18 ms respectively with 5, 10, and 15 replicas. With 5
replicas, Auspice’s performance is comparable to UltraDNS
with one-third the replication cost. With 15 replicas, Aus-
pice incurs 60% lower latency for a comparable cost. The
comparison against the other two, Dyn and DNSMadeEasy,
is qualitatively similar [1]. Thus, Auspice’s demand-aware
replication achieves a better cost–performance tradeoﬀ com-
pared to static replication.
4.4.2 Update propagation delay
To measure update propagation delays, we purchase DNS
service from three providers for separate domain names. All
providers replicate a name at 5 locations across US and Eu-
rope for the services we purchased. We issue address updates
for the domain name serviced by that provider and then im-
mediately start lookups to the authoritative name servers
for our domain name. These authoritative name servers can
be queried only via an anycast IP address, i.e., servers at
diﬀerent locations advertise the same externally visible IP
address. Therefore, to maximize the number of provider lo-
cations queried, we send queries from 50 random PlanetLab
nodes. From each location, we periodically send queries un-
til all authoritative name server replicas return the updated
address. The update propagation latency at a node is the
time between when the node starts sending lookup to when
it receives the updated address. The latency of an update
is the the maximum update latency measured at any of the
nodes. We measure latency of 100 updates for each provider.
To measure update latencies for Auspice, we replicate
1000 names at a ﬁxed number of PlanetLab nodes across
256
US and Europe. The number of nodes is chosen to be 5,
10, and 20 across three experiments. A client sends an up-
date to the nearest node and waits for update conﬁrmation
messages from all replicas. The latency of an update is the
time diﬀerence between when the client sent an update and
when it received the update conﬁrmation message from all
replicas (an upper bound on the update propagation delay).
We show the distribution of measured update latencies for
Auspice and for three managed DNS providers in Figure 7.
Auspice incurs lower update propagation latencies than
all three providers for an equal or greater number of replica
locations for names. We were unable to ascertain from Ul-
traDNS why their update latencies are an order of magni-
tude higher than network propagation delays, but this ﬁnd-
ing is consistent with a recent study [50] that has shown la-
tencies of up to tens of seconds for these providers. Indeed,
some providers even distinguish themselves by advertising
shorter update propagation delays than competitors [50].
Sensitivity analyses and other results.
We have conducted a comprehensive evaluation of the sen-
sitivity of Auspice’s performance-cost trade-oﬀs to workload
and system parameters across scales varying by several or-
ders of magnitude. These include workload parameters such
as geo-locality, read-to-write rate ratio, ratio of device-to-
service names, etc. and system parameters such as the fault-
tolerance threshold, capacity utilization, perturbation knob,
the tunable overhead of replica reconﬁguration, etc. using a
combination of simulation and system experiments. These
results do not qualitatively change the ﬁndings in this paper,
and are deferred to the technical report [1].
5. RELATED WORK
Our work draws on lessons learned from an enormous
body of prior work on network architecture as well as dis-
tributed systems, as described in §1 and §2.1. We discuss
related work not covered elsewhere in the paper here.
DNS. Many have studied issues related to performance,
scalability, load balancing, or denial-of-service vulnerabili-
ties in DNS’s resolution infrastructure [46, 49, 16, 22]. Sev-
eral DHT-based alternatives have been put forward [49, 20,
45] and we compare against one representative proposal,
Codons [49].
In general, DHT-based designs are ideal for
balancing load across servers, but are less well-suited to sce-
narios with a large number of service replicas that have to
coordinate upon updates, and are at odds with scenarios re-
quiring placement of replicas close to pocket of demand. In
comparison, Auspice uses a planned placement approach.
Vu et al. describe DMap [55], an in-network DHT scheme
that is similar in spirit to Random-M as evaluated in our ex-
periments (§4) (with a more direct comparison in [1]), show-
ing that demand-aware placement can dramatically outper-
form randomized placement. A more important qualitative
distinction is that DMap ties federation to the interconnec-
tion structure between ISPs, which entails commensurate
lookup latency penalties and potential incentive mismatches
by mapping GUIDs to non-provider ISPs. In comparison,
the Auspice approach decouples the federation structure be-
tween GNS providers from that between ISPs.
Server selection. Many prior systems have addressed
the server selection problem with data or services replicated
across a wide-area network. Examples include anycast ser-
vices [25, 15, 57] to map users to the best server based on
server load or network path characteristics. These systems
as well as CDNs and cloud hosting providers share our goals
of proximate server selection and load balance given a ﬁxed
placement of server replicas. Auspice diﬀers in that it ad-
ditionally considers replica placement itself as a degree of
freedom in achieving latency or load balance.
Dynamic placement. We were unable to ﬁnd prior
systems that automatically reconﬁgure the geo-distributed
replica locations of frequently mutable objects while preserv-
ing consistency (i.e., those satisfying all four italicized prop-
erties). However, reconﬁgurable placement has been studied
for static or slow changing content [30] or within a single
datacenter, or without replication. For example, Volley [11]
optimizes the placement of mutable data objects based on
the geo-distribution of accesses and is similar in spirit to
Auspice in this respect, however it implicitly assumes a sin-
gle replica for each object, so it does not have to worry about
high update rates or replica coordination overhead.
Auspice is related to many distributed key-value stores [5,
23, 3], most of which are optimized for distribution within,
not across, data centers. Some (e.g., Cassandra) support a
geo-distributed deployment using a ﬁxed number of replica
sites. Spanner [19] is a geo-distributed data store that syn-
chronously replicates data (“directories”) across datacenters
with a semi-relational database abstraction. Compared to
Spanner, Auspice does not provide any guarantees on oper-
ations spanning multiple records, but unlike Spanner’s geo-
graphic placement of replicas that “administrators control”
by creating a “menu of named options”, Auspice automati-
cally reconﬁgures the number and placement of replicas so
as to reduce lookup latency and update cost. Furthermore,
Spanner assigns a large number of directory objects to a
much smaller number of ﬁxed Paxos groups; Auspice sup-
ports an arbitrarily reconﬁgurable Paxos group per object
based on principles in recent theoretical work on reconﬁg-
urable consensus, e.g., Vertical Paxos [39] and the more re-
cent report on Viewstamped Replication Revisited [41].
6. CONCLUSIONS
In this paper, we presented the design, implementation,
and evaluation of Auspice, a scalable, geo-distributed, fed-
erated global name service for any Internetwork where high
mobility is the norm. The name service can resolve ﬂexi-
ble identiﬁers (human-readable names, self-certifying iden-
tiﬁers, or arbitrary strings) to network locations or other at-
tributes that can also be deﬁned in a ﬂexible manner. At the
core of Auspice is a placement engine for replicating name
records to achieve low lookup latency, low update cost, and
high availability. Our evaluation shows that Auspice’s place-
ment strategy can signiﬁcantly improve the performance-
cost tradeoﬀs struck both by commercial managed DNS ser-
vices employing simplistic replication strategies today as
well as previously proposed DHT-based replication alterna-
tives with or without high mobility. Our case studies conﬁrm
that Auspice can form the basis of an end-to-end mobility
solution and also enable novel context-aware communication
primitives that generalize name- or address-based commu-
nication. A pre-release version of Auspice on EC2 can be
accessed through the developer portal at http://gns.name.
Acknowledgments. This research was funded in part by
CNS-1040781 and CNS-0845855. We thank the rest of the
MobilityFirst team, the paper’s past and recent reviewers,
our shepherd Ellen Zegura, Anand Seetharam, Emmanuel
Cecchet, Jim Kurose, Marvin Sirbu, and NSF-FIA meeting
participants for their feedback.
257
[32] V. Jacobson and et al. Networking Named Content. In
ACM SIGCOMM CoNEXT, 2009.
[33] P. Jokela, P. Nikander, J. Melen, J. Ylitalo, and J. Wall.
Host Identity Protocol, Extended Abstract. In Wireless
World Research Forum, 2004.
[34] J. Jung, E. Sit, H. Balakrishnan, and R. Morris. DNS
Performance and the Eﬀectiveness of Caching. IEEE/ACM
Transactions on Networking, October 2002.
[35] T. Koponen, M. Chawla, B.-G. Chun, A. Ermolinskiy,
K. H. Kim, S. Shenker, and I. Stoica. A Data-Oriented (and
Beyond) Network Architecture. In ACM SIGCOMM, 2007.
[36] D. Krioukov and et al. On Compact Routing for the
Internet. ACM SIGCOMM CCR, 2007.
[37] H. Kwak, C. Lee, H. Park, and S. Moon. What is Twitter,
a Social Network or a News Media? In WWW, 2010.
[38] L. Lamport. The Part-Time Parliament. ACM Transactions
on Compututer Systems, 16(2):133–169, May 1998.
[39] L. Lamport, D. Malkhi, and L. Zhou. Vertical Paxos and
Primary-Backup Replication. In ACM PODC, 2009.
[40] B. W. Lampson. Designing a Global Name Service. In
ACM PODC, 1986.
[41] B. Liskov and J. Cowling. Viewstamped replication
revisited. Technical Report MIT CSAIL-TR-2012-021, 2012.
[42] H. V. Madhyastha and et al. iPlane: An Information Plane
for Distributed Services. In USNIX OSDI, 2006.
[43] P. Mockapetris and K. J. Dunlap. Development of the
domain name system. ACM SIGCOMM, 1988.
[44] E. Nordstrom and et al. Serval: An End-Host Stack for
Service-Centric Networking. In USENIX NSDI, 2012.
[45] V. Pappas, D. Massey, A. Terzis, and L. Zhang. A
Comparative Study of the DNS Design with DHT-Based
Alternatives. In IEEE INFOCOM, 2006.
[46] V. Pappas, Z. Xu, S. Lu, D. Massey, A. Terzis, and
L. Zhang. Impact of Conﬁguration Errors on DNS
Robustness. In ACM SIGCOMM, 2004.
[47] M. Parwez and et al. DNS propagation delay: An eﬀective
and robust solution using authoritative response from
non-authoritative server. In ICIME, 2010.
[48] C. Perkins. RFC 3220: IP Mobility Support for IPv4, 2002.
[49] V. Ramasubramanian and E. G. Sirer. The Design and
Implementation of a Next Generation Name Service for the
Internet. In ACM SIGCOMM, 2004.
[50] J. Read. Comparison and Analysis of Managed DNS
Providers, Aug 2012. Cloud Harmony Inc.
[51] M. D. Schroeder, A. D. Birrell, and R. M. Needham.
Experience with Grapevine: the Growth of a Distributed
System. ACM Trans. Comput. Syst., 1984.
[52] A. C. Snoeren and H. Balakrishnan. An End-to-End
Approach to Host Mobility. In ACM MobiCom, 2000.
[53] I. Stoica, D. Adkins, S. Zhuang, S. Shenker, and S. Surana.
Internet Indirection Infrastructure. In SIGCOMM, 2002.
[54] A. Venkataramani, J. Kurose, D. Raychaudhuri,
K. Nagaraja, M. Mao, and S. Banerjee. MobilityFirst: A
Mobility-Centric and Trustworthy Internet Architecture.
ACM SIGCOMM CCR, 2014.
[55] T. Vu and et al. DMap: A Shared Hosting Scheme for
Dynamic Identiﬁer to Locator Mappings in the Global
Internet. In IEEE ICDCS, 2012.
[56] M. Walﬁsh, J. Stribling, M. Krohn, H. Balakrishnan,
R. Morris, and S. Shenker. Middleboxes No Longer
Considered Harmful. In OSDI, 2004.
[57] P. Wendell, J. W. Jiang, M. J. Freedman, and J. Rexford.
DONAR: Decentralized Server Selection for Cloud Services.
In ACM SIGCOMM, 2010.
7. REFERENCES
[1] A Global Name Service for a Highly Mobile Internetwork.
UMass SCS Technical Report, 2013 and 2014.
https://web.cs.umass.edu/publication.
[2] Alexa Web Information Service. http://www.alexa.com.
[3] Cassandra. http://cassandra.apache.org.
[4] MobilityFirst Future Internet Architecture Project.
http://mobilityfirst.cs.umass.edu/.
[5] mongoDB. http://www.mongodb.org/.
[6] msocket: System Support for Developing Seamlessly
Mobile, Multipath, and Middlebox-Agnostic Applications.
UMass SCS Technical Report, 2014.
https://web.cs.umass.edu/publication.
[7] Server fault: DNS - Any way to force a name server to
update the record of a domain?
http://serverfault.com/questions/41018.
[8] The Locator/ID Separation Protocol (LISP). RFC 6830.
[9] ICANN Hears Concerns about Accountability, Control,
October 2008. http://www.infoworld.com/t/networking/
icann-hears-concerns-about-accountability-control-216.
[10] Debate Rages over who Should Control ICANN. Processor,
31(16), June 2009.
[11] S. Agarwal, J. Dunagan, N. Jain, S. Saroiu, A. Wolman,
and H. Bhogan. Volley: Automated Data Placement for
Geo-Distributed Cloud Services. In USENIX NSDI, 2010.
[12] D. G. Andersen, H. Balakrishnan, N. Feamster,
T. Koponen, D. Moon, and S. Shenker. Accountable
Internet Protocol. In ACM SIGCOMM, 2008.
[13] M. Arye, E. Nordstrom, R. Kiefer, J. Rexford, and M. J.
Freedman. A Formally-Veriﬁed Migration Protocol For
Mobile, Multi-Homed Hosts. In ICNP, 2012.
[14] H. Balakrishnan, K. Lakshminarayanan, S. Ratnasamy,
S. Shenker, I. Stoica, and M. Walﬁsh. A Layered Naming
Architecture for the Internet. In ACM SIGCOMM, 2004.
[15] S. Bhattacharjee and et al. Application-Layer Anycasting.
In IEEE INFOCOM, 1997.
[16] N. Brownlee, K. Claﬀy, and E. Nemeth. DNS
Measurements at a Root Server. In GLOBECOM, 2001.
[17] M. Caesar, T. Condie, and J. Kannan et al. ROFL:
Routing on Flat Labels. In ACM SIGCOMM, 2006.
[18] Cisco. Visual Networking Index: Global Mobile Data
Traﬃc Forecast Update, 2012-2017. http://ciscovni.com.
[19] J. C. Corbett and J. Dean et al. Spanner: Google’s
Globally Distributed Database. USENIX OSDI, 2012.
[20] R. Cox, A. Muthitacharoen, and R. Morris. Serving DNS
Using a Peer-to-Peer Lookup Service. In IPTPS, 2002.
[21] G. DeCandia and et al. Dynamo: Amazon’s Highly
Available Key-value Store. In ACM SOSP, 2007.
[22] DNSSEC. DNS Threats & Weaknesses of the Domain Name
System, 2012. http://www.dnssec.net/dns-threats.php.
[23] R. Escriva, B. Wong, and E. G. Sirer. HyperDex: A
Distributed, Searchable Key-value Store. In ACM
SIGCOMM, 2012.
[24] A. Feldmann and et al. HAIR: Hierarchical Architecture for
Internet Routing. In ReArch Workshop, 2009.
[25] M. J. Freedman, K. Lakshminarayanan, and D. Mazi`eres.
OASIS: Anycast for Any Service. In USENIX NSDI, 2006.
[26] D. Funato, K. Yasuda, and H. Tokuda. TCP-R: TCP
mobility support for continuous operation. In ICNP, 1997.
[27] Z. Gao, A. Venkataramani, and J. F. Kurose. Towards a
quantitative comparison of location-independent network
architectures. In ACM SIGCOMM, 2014.
[28] Gartner. Sales of Android Phones to Approach One Billion
in 2014. http://www.gartner.com/newsroom/id/2665715.
[29] M. Gritter and D. R. Cheriton. An Architecture for
Content Routing Support in the Internet. In USITS, 2001.
[30] J. Gwertzman and M. Seltzer. The case for geographical
push caching. In IEEE HotOS Workshop, May 1995.
[31] D. Han, A. Anand, F. Dogar et al., B. Li, H. Lim, and
M. et al. XIA: Eﬃcient Support for Evolvable
Internetworking. In USENIX NSDI, 2012.
258