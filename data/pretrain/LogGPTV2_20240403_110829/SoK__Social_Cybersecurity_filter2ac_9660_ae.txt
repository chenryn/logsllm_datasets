observed that group interactions about S&P were rare and
tended to focus on abstract concepts found in the news instead
of personal experiences. These groups might see their S&P
behaviors as lacking in conversational value, even if they end
up enjoying talking about them.
Adoption rates of security features on social media sites
can also be affected by the security practices of potential
adopters’ friends. For example, Das et al. [98] found that
social inﬂuence affected security feature adoption when users
observed Facebook friends from multiple different social cir-
cles also adopting the features; and, while a person with more
feature-adopting friends is likely to adopt features themselves,
a person with just a few such friends might be negatively inﬂu-
enced. Similarly, De Luca et al. [99] found that users’ adoption
of security messaging tools like Threema was signiﬁcantly
dependent on whether their friends were also using these
tools, rather than their inherent improved security. Building
on the power of peer inﬂuence, Bonneau et al. [100] envision
Privacy Suites, a system that allows users to import “suites”
of privacy settings that have been pre-selected by their friends
or trusted experts—modifying them if they wish—and that
supports public reviews of the suites to establish trust in them.
2) Social nudges: Other work has explored how social
proof can be employed in top-down nudges to inﬂuence S&P
behavior. For example, Das et al. [101] showed Facebook
users announcements about extra features to help secure their
accounts, including prompts about the number of their friends
who used these features. Announcements that included these
social proof prompts were more effective at getting them to
explore these features than those that did not; more people
also adopted these features. This recalls similar prior ﬁndings
[102] that password meters based on social pressure resulted
in users generating slightly stronger passwords.
Emami-Naeini et al. [103] also investigated the role of social
cues in user decisions about data collection by IoT devices,
showing users messages about the actions that either “friends”
or “experts” took in various data collection scenarios. They
found that participants were inﬂuenced by when friends denied
data collection and when experts allowed data collection (and
not so vice versa). They also observed that this inﬂuence
could change with repeated exposure to social proof; for ex-
ample, when friends repeatedly allowed risky data collection,
participants were less inﬂuenced by them. In the same vein,
when adolescent users on the Japanese social networking site
Himabu were presented with negative framing of choices, e.g.,
“90% of users would not share a photo without permission,”
they were more likely to avoid potentially risky choices than
vice versa (conversely, when users saw afﬁrmative framings,
they tended to make riskier choices) [104].
D. The public
Work at this scale includes both empirical work on social
inﬂuence at demographic levels, as well as more systems that
use social inﬂuence to guide better S&P behaviors.
In the empirical category, a number of papers have explored
demographic variations in how users approach S&P behaviors
and the role that inﬂuence plays. Ur and Wang [105] argued
that considerations and support for user privacy has not been
equitably distributed internationally, and proposed a frame-
work for evaluating how well a social network site’s privacy
settings supports cross-cultural user bases. The framework
includes questions about local cultural expectations of privacy,
governmental restrictions and requirements on data collection,
and local language availability. Prior work [97] ﬁnding that
users from more collectivistic countries (e.g., Brazil, Vietnam,
India) seek out information from others at higher rates than
those in the US corroborates the need for such a framework.
More broadly, male and female users might think about
privacy in different ways: whereas females are more likely
to mention other people and bring up issues of safety and
respect, males tend to refer to privacy as having freedom or
being anonymous [106]. There are also potential age difference
effects: older adults describe privacy in terms of space (e.g.,
home invasion) rather than information, and tend to view
private information as concrete objects like documents or
speciﬁc secrets much more than young adults do [106].
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:32 UTC from IEEE Xplore.  Restrictions apply. 
101872
Other work has explored the nuances of end-user adoption
of S&P advice. Herley [107] argues that even though users are
often portrayed as lazy and unmotivated about their personal
security, their rejection of security advice is economically
rational. He suggests that most security advice offers a poor
trade-off between the costs of implementation versus the actual
user beneﬁts, so users choose not to take it, especially if
experts themselves do not necessarily know the full extent
of security risks and harms. More work on the actual harm
that users endure, as well as user education technologies that
target just those who are at risk, might be more effective,
especially given that users tend to reject advice that is too
marketing-oriented, especially if they did not feel like they
were personally at risk [4]. Speciﬁcally, users of lower socioe-
conomic status, who seek advice from different sources than
those with more resources and technical education, might be
more vulnerable and need more attention [108].
On the technical front, Goecks et al. [109] expand the
scope of social-inﬂuence S&P nudges to the public, presenting
two systems that leverage social navigation for public-scale
S&P interventions. The ﬁrst, Acumen, shows users the prior
decisions taken by others in deciding whether to accept a
website’s cookie settings; Bonﬁre, the second, shows prior
decisions in the context of ﬁrewall settings. A particular focus
of these systems is avoiding “information cascades,” a sort
of herd mentality where users blindly follow the decisions of
others, resulting in incorrect choices.
E. Takeaways
At the intimate relationship scale, we saw that user behav-
iors mirror that of the household scale, down to the imbalance
in technical education between abuser and survivor in IPV
situations. In particular, survivors seek S&P advice from IPV
professionals who may or not may not be qualiﬁed to give it,
and who must often instead negotiate on the survivor’s behalf
with technology companies to get abusive content removed.
There is thus a future design space for systems that allow IPV
professionals to more efﬁciently advocate for their survivor
clients with technology companies, rather than the companies
outright
implementing new S&P controls that neither the
professional nor the survivor client might know how to use.
This imbalance and advocacy continues at the household
level, where parents are responsible for giving S&P advice to
both their younger children and to their own older parents.
In both cases, parents tend to prefer “lockdown” approaches
to protect what they perceive as less technically-literate, more
vulnerable members of their family, taking away S&P agency
from both the younger children and older adults.
At the social acquaintance level, much work has been done
on designing ways to inform users about S&P behaviors via
socially-ﬂavored nudges and observational alerts, as well as
making sense of how users have conversations with those
around them about S&P. While this work hints at the latent
power of social responsibility that users feel for their commu-
nities’ S&P, they tend to recommend conversations and social
nudges and alerts as ways to educate users only about existing
The threads of
ways to improve S&P. Observing friends is the most frequent
trigger of change in S&P behaviors, but many security settings
are inherently private and not observable. Potential adopters
might feel that existing users are simply paranoid [98], [110].
Instead, we suggest that future work could provide outlets for
these users to collectively demand or construct better methods.
institutional entrenchment and near-
paternalism of smaller social scales continue at the public
level. S&P researchers have often tended to believe that users
as a whole irrationally don’t adopt S&P behaviors, without
considering the social and cultural contexts of heeding such
advice, as well as the risk assessments that users make about
their S&P. Past work has investigated how to prevent a cascade
of users from making S&P decisions deemed “wrong”, but
does not entertain the possibility of a cascade of users who
want to make choices outside of the existing set of options.
VIII. DISCUSSION
Throughout our systematization of prior literature in social
cybersecurity, we used Ackerman’s social-technical gap, or
the difference between what users desire socially and what
is supported technically [19], as a lens to outline potential
areas for future work. The social-technical gap is extensively
cited in the broader HCI literature [111], and has been specif-
ically referenced in the usable S&P [112]–[114] and online
communities [115], [116] literature to illustrate how technical
systems may fail to account for human social behavior.
Based on Ackerman’s [19] assertions that (1) the gap is
enduring, so we should always strive to do something about
it, and (2) we should not force users to adapt to technology but
adapt technology to its users, we developed three questions to
assess the social-technical gap for each social S&P use case
identiﬁed in prior literature:
• Are there existing systems that help facilitate this
social S&P use case? For example, the Thumprint system
[63] supports group authentication by using secret knock-
ing patterns to authenticate members of a social group,
so the box for that use case’s row is checked in Figure 1.
As another example, while audience selection tools exist
on social media platforms, teens choose to use coded
language instead to read their intended audience [17],
so that row gets a starred check. In contrast, our review
indicated that there are no technical systems to facilitate
giving out S&P advice that directly targets vulnerable
populations [107], [108], so it does not get a check mark.
• Can users ﬁt the affordances of existing S&P systems
without altering their ideal social behaviors? For
example, in supporting group authentication, Thumprint
knocking patterns are simple for users to collectively
devise and share. Conversely, for social media audience
selection, existing tools must be pre-deﬁned, but teens
have dynamic audiences in mind when posting.
• Can users use these existing systems, as intended,
to meet both their ideal social behaviors and S&P
goals? For example, Thumprint uses a single shared
group secret,
the expression of which is individually
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:32 UTC from IEEE Xplore.  Restrictions apply. 
111873
Fig. 1: (cid:88)* indicates that yes, there is a relevant technical system, but the system does not directly facilitate this use case.
121874
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:32 UTC from IEEE Xplore.  Restrictions apply. 
distinguishable, allowing outsiders to be easily identiﬁed
without requiring insiders to keep individual secrets from
each other. On the other hand, for social media audience
selection, teens have no usable way of dynamically spec-
ifying a desired audience, outside of manually selecting
individual audience members from a picklist of hundreds
or thousands of individuals.
We answer these questions broadly in the subsections to
follow. Figure 1 summarizes our systematization of prior work
across behavioral domains and social scales, addressing the
three questions listed above.
A. Are there existing systems that help facilitate this social
S&P use case?
A great majority of behaviors and use cases observed in
social cybersecurity work involve some technical system, e.g.,
passwords, physical devices, shared knowledge authentication,
audience selection tools on social media. However, we note a
distinction between novel systems designed to directly facil-
itate social behaviors, and extant systems that were worked
around or modiﬁed to ﬁt social needs. Most examples of the
former were developed for use-cases that required little direct
social interaction, e.g., social authentication systems [59], [60],
[62], [63], the Tor relay network [87] and Nextdoor’s location-
based authentication [64]. There were also two examples of
domain-speciﬁc social S&P systems for carsharing [50] and
ridesharing [50]. But, overall, there were few examples of
novel S&P systems that required direct social
interaction,
speciﬁcally in the advice and inﬂuence behavior domain.
B. Can users ﬁt
without altering their ideal social behaviors?
the affordances of existing S&P systems
Many S&P systems are designed to be ignorant of social
context, and force users to choose between security and social
acceptability. For example, when a user shares a Netﬂix
account, they are burdened with making their private account
details visible, having their personalized content recommen-
dations being disrupted, exposing their password to unknown
actors, and risking their account being commandeered by the
secondary user; in turn, they may change how often they
use the account or stop using it entirely [34]. As another
example, teenagers use coded language instead of existing
audience selection tools on social media to hide from adults in
plain sight; they do this with the expectation that they cannot
control all of their privacy anyway [17]. And, even in domains
without direct technical systems, like inﬂuencing others’ S&P
behaviors, users are expected to abide by institutionally-set
advice for existing technologies. Such cases are ripe for
exploration on how to better technically integrate social ties
and support user agency.
C. Can users use these existing systems, as intended, to meet
both their ideal social behaviors and S&P goals?
There are nuances in social behaviors that existing technical
systems are ill-equipped to navigate. For example, users often
share passwords or elide authentication altogether to facilitate
sharing digital resources. Indeed, passwords and personal
devices,
though not designed to be shared, are regularly
posted by users in common work areas and left unlocked
for others on purpose, respectively. These use cases reveal
limitations in existing S&P systems: by failing to account for
human social behaviors, these systems no longer serve their
intended purpose. For example, users use single-user authen-
tication methods (e.g., passwords, keycards) as proxies for
group access control. Existing examples of production-ready
social cybersecurity systems are relatively few:
those that
employ collectives for obfuscation but assume no direct user
interaction—e.g., Tor [87], TrackMeNot [88], or AdNauseum
[89]—and those that allow for social fallback authentication
for individual accounts (e.g., Facebook Trusted Contacts [61]).
D. Future work
We foresee two key directions for future work on addressing
the social-technical gap in S&P systems. First, we have
analyzed a rich literature describing the social inadequacies
of existing S&P systems, but we found little systems work
addressing these inadequacies. Following recommendations
from prior work [12], we see a need for the design of social
S&P systems that foster greater: observability—systems that
make it easy for users to see how others are protecting their
S&P; cooperation—systems that allow groups to act in mutual
beneﬁt of everyone’s S&P; and, stewardship—systems that
allow individuals to act in beneﬁt of others’ S&P. We also
found little extant work—theory or systems—on the “public”
social scale in the context of S&P. Collective action systems
may be explored here as a mechanism for users to directly
advocate for more human-centered S&P protections against,
e.g., web tracking and surveillance technologies [117].
IX. CONCLUSION
Throughout this structuring process we identiﬁed four key
behavior domains of social cybersecurity work, and broke
them each down by social scale. We found extensive descrip-
tive evidence that, today, end-users must often either adapt