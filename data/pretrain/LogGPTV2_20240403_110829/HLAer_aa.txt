1
HLAer: a System for Heterogeneous Log Analysis
Xia Ning, Geoff Jiang, Haifeng Chen and Kenji Yoshihira Department of Autonomic Management 
NEC Labs America 
Princeton, NJ, 08850 
{xning, gfj, haifeng, kenji}@nec-labs.com
AbstractLogs are ubiquitous for system monitoring and debugging. However, there lacks a comprehensive system that is capable of performing heterogeneous log organization and analysis for various purposes with very limited domain knowledge and human surveillance. In this manuscript, a novel system for heterogeneous log analysis is proposed. The system, denoted as Heterogeneous Log Analyzer (HLAer), achieves the following goals concurrently: 1) heterogeneous log categorization and organization; 2) automatic log format recognition and 3) heterogeneous log indexing. Meanwhile, HLAer supports queries and outlier detection on heterogeneous logs. HLAer provides a framework which is purely data-oriented and thus general enough to adapt to arbitrary log formats, applications or systems. The current implementation of HLAer is scalable to Big Data.I. 	INTRODUCTION
Modern systems are becoming ubiquitously more and more complicated, which sets up the insurmountable hurdles for system administrators to manually monitor system dynamics or fix any system issues in a timely manner. Thus, autonomic analysis of system logs is of a crucial demand. System/application logs represent real-time snapshots of system content and dynamics, and meanwhile, they have semantic meanings that administrators or developers can understand logically. Research and development for automatic log analysis has started attracting a considerable amount of attentions from both academia and industry.However, in this era of Big Data, automatic log analysis faces highly non-trivial challenges that prevent conventional data mining or machine learning methods being applied immediately. Examples of such challenges include, but not limited to, the following items.
•	Extremely huge amount of system logs are accumulated over periods. Processing such huge data volume itself 	requires significant amount of CPU times and memories.•	Various log formats, either standard or nonstandard, from different applications result in many difficulties in 	extending any existing methods to adapt to new log formats transparently, whereas the requirement of such 	adaption is tremendously common.•	Domain knowledge about the system of interest is typically lacking or not sufficient for people to fully 	understand the system details. Meanwhile, it is very expensive to acquire sufficient detailed domain knowledge. 	Thus, any log analysis methods relying on handcrafted rules from domain knowledge are not effective any more 	for new or newly updated systems.•	The huge data volume, heterogeneous data formats and limited domain knowledge all together makes infor-	mation retrieval and further analysis from such logs overwhelming and prohibitive.There have been some research efforts and commercial products existing that attempt on autonomic log analysis to some extent. However, all of them only tackle the problem under strong hypotheses and thus they deviate far away from the reality and fall far behind the real requirements. Existing methods are either highly customized to a specific system/application with sufficient domain knowledge available, or designed specially for a pre-defined task in mind. Therefore, they are not extensible or scalable to new systems/applications or new log types. To the best of our knowledge, there hasn’t been an autonomic log analysis system that exhibits simultaneously the characteristics of being•
•
•
• highly scalable for Big Data, 
for heterogeneous log formats, 
purely data-oriented with very limited prior knowledge used, able to support efficient information retrieval, and
2
•	extensible to arbitrary applications/systems.In this manuscript, we present our first efforts and results towards a fully autonomic heterogeneous log analysis system that have all the above characteristics. This system is denoted as Heterogeneous Log Analyzer (HLAer). HLAer is designed to be able to perform 1) heterogeneous log categorization and organization; 2) automatic log format recognition; 3) heterogeneous log indexing; 4) information retrieval from heterogeneous logs and 5) outlier detection from heterogeneous logs. The major difference of HLAer from all the other existing log analysis methods and tools is that HLAer is purely data-oriented and requires no specific knowledge about the underlying system/applications. Due to this, HLAer is adaptable to arbitrary systems/applications.The rest of this manuscript is organized as follows. In Section II, a literature review is presented. Section III defines the notations. The nature of log data is discussed in Section IV. In Section V, the overview of the system is presented. In Section VI, the technical details on building HLAer are presented. In Section VII, the functionalities of HLAer are discussed. Section VIII presents the experimental results. Finally, Section IX presents the conclusions and further discussions.II. 	RELATED WORK
In this section, we give a detailed literature survey on the research and industry development for log analysis.
A. Research efforts on log analysis
	The first research efforts towards log data analysis fall along the line of data clustering and concurrently frequent pattern mining.1) Clustering methods on log data: Data clustering is a powerful tool for complex data analysis since it provides the intuition and insights on the geometric structures of underlying space, from which the data are generated, and thus the inherent distribution of the data of interest. It can also work as a preprocessing step that categorizes data so as to facilitate further processing. Moreover, clustering algorithms can also work for anomaly/outlier detection purposes, given that intuitively any data points that fall far apart from the majority clusters can be considered as anomalies or outliers.Vaarandi [16] developed Simple Logfile Clustering Tool, i.e., SLCT1, which represents one of the first log data clustering algorithms. SLCT is essentially based on frequent words, that is, the log data which have common frequent words are clustered together. The intuition behind SLCT comes from the highly skewed distribution of word counts in log data, which is very different from that of natural language and text data. This intuition plays an important role, either explicitly or implicitly, for many of the following log data clustering algorithms. On the other hand, the use of frequent words makes SLCT very rigid and only identifies the clusters in which all the logs have exactly same words. SLCT also generates a significant number of outliers which have a few different words than the clusters. 	Makanju et al have a series work on log data analysis [11], [10], [9]. In [11], they proposed IPLoM2, an iterative clustering algorithms for logs. IPLoM consists of the following 4 consecutive steps: 1). logs of same lengths are first clustered together; 2). each cluster is further partitioned by tokens with best information gains; 3). second-order token pairs are used for another cluster partitioning; 4). cluster descriptions are generated based on majority voting. They demonstrated that IPLoM outperforms other log clustering algorithms including SLCT, LogHound and Teiresias (introduced later). A potential problem with IPLoM is that it can easily result in small cluster fragments that are not statistically significant. Moreover, the clustering quality is hard to control. The underlying assumption for the first step, that is, the logs of same length very possibly have same formats, can be easily violated as more and more heterogeneous logs coming into the system. Even worse, the poor clustering results from the first step can be further cascaded into the final results. Moreover, IPLoM requires a significant amount of I/O and intermediate storage, which makes it not scalable for large data in real systems.2) Frequent pattern mining on log data: Frequently occurring patterns (i.e., combinations/series of log records, log words or system status) from log data are representative signatures of the underlying systems. Such frequent patterns represent the regular behaviors of the systems that should not be violated in normal cases, whereas any violations would indicate anomalous scenarios. Therefore, frequent patterns are often used to calibrate system behaviors as outlier detectors.1http://kodu.neti.ee/⇠risto/slct/ 
2https://web.cs.dal.ca/⇠makanju/iplom/iplom C.zip
3
Vaarandi [17] developed LogHound3, a tool for frequent itemset mining in log data. In LogHound, event logs are considered as transaction databases, that is, each log record represents a transaction, and thus frequent event sequence pattern mining becomes frequent itemset mining. LogHound adapts a breadth-first algorithm to find such frequent patterns, which involves heuristics to control memory usage, frequent itemset sizes, etc. A potential issue with LogHound is that its hypothesis on considering events as transactions may not always hold for many different log natures.Capri4is a new mining tool for log patterns. It generates frequent lines, terms and rules from log lines. The issue with Capri is that it generates rules without further utilizing them, and thus the use of the tool is limited. 	3) Other research on log data: Research on log data also includes identifying log formats, outlier detection based on logs, and using logs to model system behaviors, etc. A very unique research direction is along the line of learning log formats. Zhu et al [20] proposed to learn the log formats for arbitrary well-formatted logs from the perspective of Natural Language Processing (NLP) with pre-defined schemes.Fu et al [7] proposed to use Finite State Automaton (FSA) to model the execution behavior of system models based on logs. Logs are first clustered into log keys by considering their templates with all parameters removed based on empirical rules. Then each log is labeled by their log key type so as to construct a sequence. FSA is learned from such a sequence to capture the behaviors of the system.Xu et al [18] proposed a method for console log mining. Their method has four steps: 1). identify log structure from source code; 2). cluster messages and generate a feature vector for each message type; 3). use Principal Component Analysis (PCA) for outlier detection; 4) use a decision tree to visualize the results. This method does not deviate much from conventional outlier detection methods but it gives a principled way for outlier detection from log data. 	Other work includes Yamanishi et al [19] that models log sequences using a mixture of Hidden Markov Models (HMMs) so as to identify system failures, etc.B. Log analysis tools and productsIn-use/commercialized log analysis products include Splunk, Prelert, Sisyphus, LogRhythm, Capri, etc. Splunk5 is one of the most successful commercialized log analysis products. It performs log data collection, indexing, search and visualization, etc. It generalizes to many different log types including both standard server logs and non-standard application logs. A notable problem with Splunk is that it requires tremendous manual configuration so as to tune the system with prior knowledge such as log data formats, alert definition, event definition, etc. Thus, data indexing, event extraction and knowledge discovery cannot be conducted without user specification. Splunk distinguishes itself by its versatility in performing various tasks, but its heavy reliance on user input deviates itself from being automatic. Prelert6focuses on outlier detection of event sequences from logs. Sequence analysis techniques inspired from Bioinformatics research have been applied in Prelert so as to identify frequent event sequences, and these frequent sequences are used as a normal model for outlier detection. However, a significant drawback of Prelert is that domain knowledge about logs including log format, semantic meanings and logging system properties has to be fully available, as well as that of the management system. In particular, the concept of “event” from logs is well defined, and thus Prelert knows exactly what to look for. The well calibrated log data that Prelert performs on dramatically reduce the difficulty of the problems, and meanwhile leaves Prelert not scalable to other systems.Sisyphus7is an open-source toolkit and specifically developed for super-computer syslogs. It is able to retrieve information content from logs regarding system outliers based on information theories. In particular, Sisyphus adopts Teiresias [15], a pattern discovery algorithm developed from Bioinformatics research, so as to identify patterns from logs. However, the methods of Sisyphus are highly customized according to the nature of super-computers with the central hypothesis that similar/homogeneous normal computers produce similar logs. However, this hypothesis may not necessarily generalize to other systems, and thus the application of Sisyphus is limited.LogRhythm8, specifically its SIEM tools, is particularly designed for defense of business data based on log data analysis. It processes IT system logs and meanwhile leverages information from disparate sources with the main
3http://ristov.users.sourceforge.net/loghound/ 
4http://research.cs.queensu.ca/home/farhana/capri.html 
5http://www.splunk.com 
6http://www.prelert.com/index.html6http://www.prelert.com/index.html 
7http://www.cs.sandia.gov/⇠jrstear/sisyphus 8http://logrhythm.com/
4
purpose to detect security related issues. However, its detection is based on prescribed rules that are highly specific to the application domain, i.e., business data defense. Therefore, the rule-based approaches may not generalize to other application domains or systems.Other existing tools include flow-tools9, logsurfer10, etc. A common issue with such existing tools is their limited generalizability.
III. 	DEFINITIONS AND NOTATIONSWe define the following terms for the purpose of clear presentation. A log or a log record is a line of sentences or multiple lines of sentences corresponding to one time stamp that record the system/application events or status of that moment of time. Log data refer to a collection of such log records. A log record typically has multiple fields, separated by a well-defined delimiter. Each field has certain meanings defined explicitly, even though not accessible usually. Typically the fields fall into two types. The first type is for housekeeping purpose, for example, the time stamps. The second type is for various information that is specific to that certain time stamp, referred to as“log message” here. The ordering of these fields together with the definitions on each field is referred to as the log format, log layout or log template. For instance, examples of IIS logs and log formats can be found in the according specifications11.Outliers are the data points that are distant from the majority of the others. Anomalies are the abnormal-behaving data points. In the content of this manuscript, outlier and anomaly are used interchangeable, indicating log records that are different from others based on some criteria.
IV. 	THE NATURE OF LOG DATAThe inherent natures of log data determine the type of methods and approaches that can be applied properly. Compared to the conventional text/natural language data including documents, articles and web pages, log data have very different natures even though they are also composed of words. Such natures are summarized as follows, which are not always discussed in existing literature.a) Log records have very weak syntactical structures. In order to concretely record the application/system status and behaviors, log records are typically short, succinct and abstract. In addition, log records are usually in a tabular format with a pre-defined delimiter. Each field of a certain log format, as well as all the words that can appear in that field, has a pre-defined meaning that is specific to the application/system. There are no standard or basic grammatical structures/relations among each field of log records, and thus an NLP parser will fail to identify any meaningful syntactics among log fields. Even though, there still exist strong logical/semantic relations among each field of a log format and its eligible words. System administrators can well understand the semantic meanings of a log record by filling up the contextual gaps among log fields based on their domain knowledge and logical thinking.b) Logs have very limited vocabularies but extremely skewed word count distribution. Very commonly, log records are generated from source code using a “printf” statement with all the variables replaced by parameters specific to the current system status. Such “printf” statements define the limited but also most frequent words in the logs. On the other hand, the parameters, which may not necessarily repeat significantly frequently with same values, represent the system momentum, and thus they have richer and more critical content than the most frequent words. The skewed distribution of the word counts in log data is the fundamental reason for many frequent itemset-based log mining algorithms and clustering algorithms. Meanwhile, the imbalance of the information content between frequent and infrequent words becomes a challenge to such algorithms if they are originated from conventional text mining research.c) Log records are generated from templates. As mentioned above, log records are generated from source code and thus have clear layouts, and such layouts will not change across log records from a same source, i.e., homogeneous logs. The existence of such regulated layouts or formats of log data, even unknown as to how they look explicitly, makes log data clustering intuitively easier than that for documents and other text data. Most of the existing log data clustering algorithms implicitly or explicitly utilize such knowledge about log data.9http://www.splintered.net/sw/flow-tools/ 
10http://www.crypt.gen.nz/logsurfer/index.html 
11http://msdn.microsoft.com/en-us/library/ms525807%28v=vs.90%29.aspx
5d) Identical log records can appear redundantly. This is due to the fact that some log records are used for a housekeeping purpose, and thus in normal cases same logs can be repeated many times. Due to this, same logs can be down-sampled so as to reduce the burdens on memory and CPU usage. The redundancy of log records is a significant difference of log data from conventional text data that makes the down-sampling very possible and meaningful in the Big Data environment.V. 	THE HLAer SYSTEM: AN OVERVIEW
Before digging into the technical details of the heterogeneous log analyzer HLAer, an overview of the system is presented here. HLAer is an autonomic heterogeneous log analyzer, which analyzes huge amount of heterogeneous logs from any black-box system with arbitrary applications. It provides a principled way to categorize and organize logs and an automatic approach to recognize various log formats/layouts. It is also able to support retrieval of useful information from the logs regarding the system status.A. HLAer construction
	Figure 1 demonstrates the construction flow of HLAer. Basically there are five major components in order to construct HLAer:
•	Hierarchical clustering of heterogeneous logs. During this process, the heterogeneous logs are first clustered 	into a hierarchy based on the inherent geometric structures of all the logs. The logs are organized accordingly.•	Pattern recognition. Within the hierarchical structure, log patterns/formats are recognized and common templates 	are extracted as an abstract of corresponding logs.