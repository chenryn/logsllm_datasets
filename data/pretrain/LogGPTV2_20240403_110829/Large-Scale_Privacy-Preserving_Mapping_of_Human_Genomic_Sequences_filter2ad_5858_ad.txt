gregated frequency of all the hash values within Bk for the
case group. For example, suppose that only two hash values
η1 and η2 are in Bk and they totally show up 1000 times
in the 1-million-seed case group; the frequency of Bk be-
comes ¯fk = 0.001 for the case. Then, the adversary ﬁgures
out the aggregated frequency of the l-mers in the bin for
a reference population (e.g., the Hapmap group), ¯Fk, and
that for a DNA sample from the testee, ρk. In the example,
¯Fk = 0.0012 if the two l-mers in Bk, l1 and l2, occur totally
1200 times in 1 million l-mers of the reference population2.
In this case, the best the adversary can do is to run the most
powerful statistical test over ¯fk, ¯Fk and ρk for all h bins to
determine whether the testee is present in the case group:
i.e., whether the hash values of the testee’s seeds (which the
adversary cannot see) are included in the seed-hash dataset.
If we treat these bins as a set of attributes like SNPs, this re-
identiﬁcation problem does not fundamentally differ from
that studied in prior research [32, 46]. Therefore, the test
statistic reported by the prior work also works here:
h(cid:88)
(cid:88)
¯D =
¯Dk =
k=1
k
[|ρk − ¯Fk| − |ρk − ¯fk|]
(1)
¯D is called Homer-like test, as it is a direct application
of Homer’s test [32], which works on the allele frequen-
cies of SNPs, to the re-identiﬁcation over the frequencies
of bins ( ¯fk, ¯Fk and ρk). This test is known to be close to
optimal [46] in re-identifying the testee from the case group
given the aforementioned background knowledge. The truly
optimal test is the famous log likelihood ratio test [46], ac-
cording to the Neyman-Pearson lemma:
h(cid:88)
(cid:88)
¯T =
¯Tk =
[log(P rC
k (ρk)) − log(P rR
k (ρk))]
(2)
k=1
k
Let ¯ρC
k be the aggregated rate of occurrence for a case
In
individual’s seeds (l-mers) in Bk over all her seeds.
k (ρk) represents the cumulative probability
Equation 2, P rC
for seeing case members whose ¯ρC
k are even less likely to
observe than ρk (the frequency of the testee’s Bk) accord-
ing to the distribution of ¯ρC
k in the case group, which we
2It is meaningless to check the frequencies of η1, η2, l1 and l2 for the
case/reference/DNA sample, as the hashes cannot be linked to the l-mers.
¯D(cid:48)
k=1
k[|ρ(cid:48)
k − ¯F (cid:48)
k| − |ρ(cid:48)
k − ¯f(cid:48)
k =(cid:80)
Homer’s test [32] ¯D(cid:48) =(cid:80)h
respectively. (d) the likelihood ratio test [46] ¯T (cid:48) = (cid:80)h
Figure 4. Power Analysis and Comparison. On bin aggregated l-mer frequencies: (a) the Homer-like
statistic test ¯D (Equation 1) (b) the likelihood ratio test ¯T (Equation 2). On SNP allele frequencies: (c)
k is the testee’s allele for SNP k,
and ¯F (cid:48)
k and ¯f(cid:48)
k are the major allele frequencies for the SNP on the reference and the case populations
k)) − log( ¯P R
k))], where
k = ¯F (cid:48)
k if ρ(cid:48)
k is major allele and ¯P R
k is a major allele and
¯P R
k = 1 − ¯f(cid:48)
k otherwise. All the statistics were computed for each of 40 case individuals (blue
¯P C
dots) and 40 test individuals (red squares). The distributions of ¯D and ¯T in these two groups are
indistinguishable in (a) and (b), while those of ¯D(cid:48) and ¯T (cid:48) are completely separated in (c) and (d). The
dash lines represent the 1% false positive rate level. The individuals identiﬁed at this rate are above
the lines for all statistics except for ¯T , where the identiﬁed are below the line.
k otherwise, and ¯P C
k|], where ρ(cid:48)
¯T (cid:48)
k = (cid:80)
k[log( ¯P C
k = ¯f(cid:48)
k (ρ(cid:48)
k if ρ(cid:48)
k = 1 − ¯F (cid:48)
k=1
k (ρ(cid:48)
pessimistically assume that the adversary knows. Similarly,
k (ρk) is the cumulative probability for the reference in-
P rR
dividuals with regards to ρk. Note that to avoid comput-
ing complicated joint distributions, Equation 2 treats bins
as being independent from each other. In practice, the cor-
relations among the vast majority of bins are indeed weak,
given the fact that each of them typically covers many SNPs
and only in rare cases, a small portion of the SNPs in one bin
also appear on the l-mers in other bins. Even when strongly
correlated bins are found, the adversary still cannot build
a stronger log likelihood ratio test (the most powerful test)
over them, simply because identifying related hash values
is very difﬁcult (Appendix 7.3), not to mention determin-
ing the joint distributions over these values in different case
bins, a prerequisite for the test.
Power analysis. Given the background knowledge, the two
test statistics are the most powerful identiﬁcation tools avail-
able to the adversary. Here we report a study that evaluated
the identiﬁcation powers of these statistics over real DNA
data from the largest population available in the public do-
main.
In this study, we compressed the whole reference
genome into 372,869,332 distinctive 24-mers that involve
SNPs. The selected 24-mers were further classiﬁed into
7,260,607 bins according to their frequencies. For simplic-
ity, this was done in a way that gives advantage to the adver-
sary. Speciﬁcally, the 24-mers involving a single SNP were
grouped into bins according to their frequencies, which de-
pended not only on their rates of occurrence across the ref-
erence genome but also on the allele frequencies (3 digits
of precision) of their SNPs. For those associated with more
than one SNP, we just created a unique bin for each of them,
assuming that they were identiﬁed by the adversary.
The reference individuals in our study were acquired
from the reference human genome. To produce realistic hu-
man genome sequences for multiple individuals, we ran-
domly set the alleles for the SNP sites on the reference
genome according to their allele frequencies reported by
the HapMap. ¯Fk in the Homer-like test (Equation 1) and
the distribution in the likelihood ratio test (Equation 2) were
estimated from 100 such sequences, which constituted the
reference group. We used the YRI population (80 individu-
als) on the HapMap [9], the largest population whose DNA
data is available in the public domain, to construct a case
group and a test group (a population including neither case
nor reference individuals). Each of these two groups had
about 40 individuals. We sampled 10 million reads from
each individual in the case group to compute ¯fk and the
distribution of Bk frequencies. Then, we repeatedly ran ¯D
(Equation 1) and ¯T (Equation 2) for 40 times over randomly
selected case/test groups with individual group members as
testees, and found that each time the case and the test indi-
viduals were completely indistinguishable. An example is
presented in Figure 4 (a) and (b), which illustrates the power
of ¯D and ¯T in one experiment. For ¯D, at 1% false positive
level (denoted by the dash line), one in the case and one in
the test group (i.e., a false positive) were identiﬁed (above
that line), indicating equal true positive and false positive
rates, so no statistical power was achieved. For ¯T , only a
test individual was identiﬁed (below the line), which is a
false positive.
We further compared this outcome with the identiﬁcation
powers the adversary can get in the absence of the new pro-
tection we propose here, i.e., when the DNA data of the case
individuals was just aggregated, the standard protection the
020406080−0.01535−0.0153−0.01525−0.0152−0.01515−0.0151−0.01505IndexofIndividuals¯DValuesTestCase(a)020406080−4.6315−4.631−4.6305−4.63−4.6295x 105IndexofIndividuals¯TValues(b)CaseTest020406080−0.2−0.100.10.20.3IndexofIndividuals¯D’Values(c)CaseTest020406080−4000−20000200040006000800010000IndexofIndividuals¯T’Values(d)CaseTestNIH took. Over the aggregated data, the adversary observes
the allele frequencies of different SNPs and can therefore
run both tests over these frequencies [32,46] using the same
background knowledge (the reference and the DNA sample
from the testee). In our experiment, we ran these tests on
the SNPs of the same populations. The results, as presented
in Figure 4 (c) and (d), show that both tests easily separated
the case and test populations. In contrast, these tests were
completely ineffective upon the keyed hashes our approach
exposes to the public cloud (Figure 4 (a) and (b)). This
strongly indicates that our techniques offer effective protec-
tion against the most powerful known re-identiﬁcation at-
tacks. In Appendix 7.2, we show that ¯D and ¯T also cannot
achieve a higher power on the combined seeds.
4 Performance Evaluation
4.1 Experiment Setting
Our evaluation was performed over a microbial ﬁltering
task [8]. The sequences extracted from human microbes
include the DNA information of their hosts, which, if not
taken out, will not only contaminate the outcome of a micro-
biome analysis but also disclose the identities of the donors
the microbes come from. Therefore, one of the most im-
portant read-mapping tasks is to compare the reads from
microbiome datasets to the reference genome, to identify
and remove those belonging to humans. For the time be-
ing, this task is still undertaken by the NIH internal servers,
but there are strong demands to move the computation to
the low-cost public cloud given the privacy of the donors is
protected [52].
Data. We utilized a real microbiome dataset collected from
a fecal sample of a human individual [45]. The dataset con-
tains 10 million reads, totally 250 MB, a data scale typical
in today’s microbe projects. In our research, we added to
the dataset 500,000 human reads collected from the refer-
ence genome, a typical level of human contamination (5%)
in microbiome data, as the original dataset was already san-
itized, being removed of human sequences. These human
reads were randomly sampled from Chromosome 1 (Chr1),
the largest chromosome with 252.4 million bps, Chromo-
some 22 (Chr22), the smallest one with 52.3 million bps,
and the whole genome with 6 billion bps respectively. They
were further randomly adjusted to simulate the sequencing
error and mixed with the microbiome dataset to build three
test datasets (with human reads from Chr1, Chr22 and the
whole genome respectively). Over these datasets, we ran
our prototype under three scenarios, mapping these 10 mil-
lion reads to Chr1, Chr22 or the whole genome.
Clouds. These datasets were mapped on FutureGrid [43],
an NSF-sponsored large-scale, cross-the-country cloud test-
bed. The public cloud we used includes 30 nodes with 8-
core 2.93 GHz Intel Xeon, 24 GB memory, 862 GB local
disk and Linux 2.6.18. Our private cloud is a single node
with the same software/hardware settings. We also evalu-
ated the bandwidth use of our prototype on the 40 MBps
link between the private cloud and the public cloud.
4.2 Results
In the experiments, we ran our prototype to ﬁlter the
reads on the hybrid cloud, using edit distances of 3 and
6. The overheads incurred during each step of the com-
putation was measured and compared with those of Cloud-
Burst3 [47], the most famous cloud-based mapping system,
which is also a standard service of Amazon MapReduce [6].
The results are presented in Table 3 and 4.
Data preparation (one-time cost). When the distance was
3, we extracted from our “reference genome” (Chr1, Chr22
or the whole genome) distinctive 24-mers for each of the
experiment scenarios, and generated keyed hash values for
these references using SHA-1 with a 32-byte secret key on
the private cloud.
It took 29 seconds to work on Chr22,
213 seconds on Chr1 and a little more than one hour on the
whole genome using a single core. For the distance of 6,
we ﬁrst identiﬁed 12-mers, which were further combined
within 100-bp windows, as described in Section 3.3. The
combined 12-mers (24 bps long in total) were then ﬁnger-
printed. This time, all 8 cores were put in use to gener-
ate the hash values. The overall time to have this job done
was estimated to be 11 hours when it came to the whole
genome. All these reference hashes were delivered to the
public cloud, which further sorted them for preparing the
mapping jobs. This sorting time varied over the scales of
the computation. When processing the whole genome with
a distance of 6, the public cloud took about 29 hours to com-
plete the job on the 6.8 TB reference hash values. Note that
this data preprocessing only incurs a one-time cost4. Such
overheads are completely affordable. For example, Amazon
routinely receives terabytes or even a larger amount of data
through its Import/Outport service [5].
Seeding performance (every dataset). Preparing the hash
values for the 10 million reads turned out to be highly efﬁ-
cient. Even for the edit distance of 6, the keyed hash values
for the combined seeds, about 5 GB, were computed from
those reads within 7 minutes using a single core. Seeding
on such data was also fast. When the distance was 3, the
time our prototype took to process all the seeds over the
3CloudBurst extends a seed at every genetic location it matches, rather
than drops its read as soon as the read is successfully aligned to a reference
substring (within the distance threshold). For the fairness of comparison,
our prototype also did all extensions. Note that this is not necessary for the
ﬁltering and our approach can achieve a much faster speed when doing the
ﬁltering alone.
4The reference hashes can be replaced after they are used to process a
large amount of data, e.g., 10,000 read datasets (Appendix 7.3).
Table 3. Performance of Preprocessing and Seeding
Reference (# of errors)
Chr1 (3)
Chr22 (3)
Whole Genome (3)
Chr1 (6)