# Assigning Pods to Nodes（将Pod分配到Node）
您可以约束一个 [pod](https://kubernetes.io/docs/concepts/workloads/pods/pod/) ，让其只能在特定 [nodes](https://kubernetes.io/docs/concepts/architecture/nodes/) 上运行，或者更倾向于在特定Node上运行。有几种方法能做到这点，他们都使用 [label selectors](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) 进行选择。通常这种约束不是必要的，因为Scheduler会自动执行合理的放置（例如：在Node之间传播您的Pod，而不是将Pod放在一个没有足够资源的Node上等），但在某些情况下，您可能需要控制一个Pod落在特定的Node上，例如：确保一个Pod在一个有SSD的机器上运行，或者将来自两个不同的服务的Pod放到相同的可用区域。
您可以在 [in our docs repo here](https://github.com/kubernetes/kubernetes.github.io/tree/master/docs/user-guide/node-selection) 找到这些示例的所有文件。
## nodeSelector
`nodeSelector`是最简单的约束形式。 `nodeSelector` 是PodSpec的一个字段。它指定键值对的映射。为了使Pod能够在Node上运行，Node必须将每个指定的键值对作为标签（也可有其他标签）。最常见的用法是使用一个键值对。
下面，我们来来看一下如何使用 `nodeSelector` 。
### Step Zero: Prerequisites（先决条件）
这个例子假设你对Kubernetes Pod有一个基本的了解，并已经 [turned up a Kubernetes cluster](https://github.com/kubernetes/kubernetes#documentation) 。
### Step One: Attach label to the node（将Label附加到Node）
运行 `kubectl get nodes` ，获取集群Node的名称。选择要添加Label的Node，然后运行 `kubectl label nodes  =` ，向您所选的Node添加Label。例如，如果我的Node名称是`kubernetes-foo-node-1.ca-robinson.internal` ，而我想要的标签是`disktype=ssd` ，那么可使用 `kubectl label nodes kubernetes-foo-node-1.c.a-robinson.internal disktype=ssd` 。
如果此命令出现“invalid command”的错误，那么你可能使用的是旧版本的kubectl，它没有 `label` 命令。 在这种情况下，有关如何在Node上手动设置Label的说明，请参阅本指南的 [previous version](https://github.com/kubernetes/kubernetes/blob/a053dbc313572ed60d89dae9821ecab8bfd676dc/examples/node-selection/README.md) 。
另外请注意，Label的key必须采用DNS标签的格式（如 [identifiers doc](https://git.k8s.io/community/contributors/design-proposals/architecture/identifiers.md) 所述），这意味着key不允许包含大写字母。
您可以通过重新运行 `kubectl get nodes --show-labels` 并检查Node是否已经有你所设的标签标签，来验证Label是否成功添加。
### Step Two: Add a nodeSelector field to your pod configuration（将nodeSelector字段添加到您的Pod配置）
在任意一个你想运行的Pod的配置文件中添加nodeSelector部分，如下所示。例如，如果我的Pod配置如下：
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
```
需要像这样添加一个nodeSelector：
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    disktype: ssd
```
当您运行 `kubectl create -f pod.yaml` 时 ，该Pod将在您拥有以上标签的Node上调度！您可以通过运行  `kubectl get pods -o wide` 查看该Pod所在的“NODE”，来验证是否正常工作。
## Interlude: built-in node labels（Interlude：Node的内置标签）
除你 [attach](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#step-one-attach-label-to-the-node) 的Label以外，Node还预设了一组标准的Label。 从Kubernetes v1.4起，这些Label是：
- `kubernetes.io/hostname`
- `failure-domain.beta.kubernetes.io/zone`
- `failure-domain.beta.kubernetes.io/region`
- `beta.kubernetes.io/instance-type`
- `beta.kubernetes.io/os`
- `beta.kubernetes.io/arch`
## Affinity and anti-affinity（亲和与反亲和）
`nodeSelector` 提供了一种非常简单的方式，从而将Pod约束到具有特定Label的Node。目前处于beta阶段的Affinity/Anti-affinity特性极大地扩展了您可以表达的约束类型。关键的改进是：
1. 语言更具表现力（不仅仅是“使用AND、完全匹配”）
2. 可指定“soft”/“preference”规则，而非影响要求，因此如果Scheduler不能满足你的要求，则该Pod仍将被安排
3. 您可以限制在Node（或其他拓扑域）上运行的其他Pod的标签，而非针对Node本身上的标签，这允许设置规则，指定哪些Pod可以并且不能放到一起。
Affinity特性由两种affinity组成：“node affinity”和“inter-pod affinity/anti-affinity”。Node affinity类似于现有的节点 `nodeSelector` （但具有上面列出的前两个优点）；而inter-pod affinity/anti-affinity约束Pod的Label而非Node的Label，此特性除具有上面列出的前两项性质之外，还有如上述第三项中所述的性质。
`nodeSelector` 继续像往常一样工作，但最终将会被废弃，因为Node Affinity可表示 `nodeSelector` 所能表达的所有内容。
### Node affinity (beta feature)（Node亲和性（beta特性））
Node Affinity在Kubernetes 1.2中作为alpha功能引入。Node Affinity在概念上类似于`nodeSelector` ——它允许您根据Node上的Label来约束您的Pod可被调度哪些Node。
目前有两种类型的Node Affinity，称为 `requiredDuringSchedulingIgnoredDuringExecution` and `preferredDuringSchedulingIgnoredDuringExecution` 。 您可以将它们分别认为是“hard”和“soft”，前者规定了要将Pod调度到Node上时，必须满足的规则（就像`nodeSelector` ，但使用更具表现力的语法），而后者指定调度程序将尝试强制调度但不能保证的首选项。 名称中的“IgnoredDuringExecution”部分表示，与 `nodeSelector` 工作方式类似，如果Node上的Label在运行时更改，导致不再满足Pod上的Affinity规则，则该Pod仍将继续在该Node上运行。在未来，我们计划提供 `requiredDuringSchedulingRequiredDuringExecution`  ，它和 `requiredDuringSchedulingIgnoredDuringExecution` 一样，只是它会从不再满足Pod的Node Affinity要求的Node中驱逐Pod。
因此， `requiredDuringSchedulingIgnoredDuringExecution` 的一个示例是“仅在有Intel CPU的Node上运行Pod”，并且一个`preferredDuringSchedulingIgnoredDuringExecution` 的一个示例是“尝试在可用区XYZ中运行此组Pod，但如果无法做到，则允许在其他地方运行” 。
Node Affinity被指定`nodeAffinity` 字段，它是 `PodSpec` 中 `affinity` 字段的子字段。
以下是一个使用Node Affinity的Pod的示例：
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/e2e-az-name
            operator: In
            values:
            - e2e-az1
            - e2e-az2
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: another-node-label-key
            operator: In
            values:
            - another-node-label-value
  containers:
  - name: with-node-affinity
    image: gcr.io/google_containers/pause:2.0
```
该Node Affinity规则表示，该Pod只能放在带有 `kubernetes.io/e2e-az-name` 的Label的Node上，其值为 `e2e-az1` 或 `e2e-az2` 。 另外，在符合该标准的Node中，优先使用具有 `another-node-label-key` ，值为`another-node-label-value` 的Node。
在本例中可以看到 `In` 操作符。新Node Affinity语法支持以下操作符： `In` 、 `NotIn` 、 `Exists` 、`DoesNotExist` 、 `Gt` 、 `Lt` 。没有明确的“node anti-affinity”概念，但 `NotIn` 和 `DoesNotExist` 提供了这种行为。
如果同时指定 `nodeSelector` 和 `nodeAffinity` ， 则必须同时满足，才能将Pod调度到候选Node上。
如果指定与 `nodeAffinity` 类型相关联的多个 `nodeSelectorTerms` ，那么**如果**满足 `nodeSelectorTerms` **之一** ，即可将Pod调度到节点上。
如果指定与 `matchExpressions` 相关联的多个 `matchExpressions` ，则**只有**满足**所有**  `matchExpressions` 才能将该Pod调度到Node上。
如果删除或更改了调度Pod的Node的Label，则该Pod不会被删除。 换句话说，Affinity选择仅在调度Pod时起作用。
有关Node Affinity的更多信息，请参见 [设计文档](https://git.k8s.io/community/contributors/design-proposals/scheduling/nodeaffinity.md) 。
### Inter-pod affinity and anti-affinity (beta feature)（Pod内的亲和性与反亲和性（Beta特性））