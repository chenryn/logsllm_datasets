title:Privacy Risks of General-Purpose Language Models
author:Xudong Pan and
Mi Zhang and
Shouling Ji and
Min Yang
2020 IEEE Symposium on Security and Privacy
Privacy Risks of General-Purpose Language Models
∗Fudan University, †Zhejiang University, ‡Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies
Xudong Pan∗, Mi Zhang∗, Shouling Ji†‡ and Min Yang∗
Emails: PI:EMAIL, mi PI:EMAIL, PI:EMAIL, m PI:EMAIL
Abstract—Recently, a new paradigm of building general-
purpose language models (e.g., Google’s Bert and OpenAI’s
GPT-2) in Natural Language Processing (NLP) for text feature
extraction, a standard procedure in NLP systems that converts
texts to vectors (i.e., embeddings) for downstream modeling, has
arisen and starts to ﬁnd its application in various downstream
NLP tasks and real world systems (e.g., Google’s search engine
[6]). To obtain general-purpose text embeddings, these language
models have highly complicated architectures with millions of
learnable parameters and are usually pretrained on billions of
sentences before being utilized. As is widely recognized, such
a practice indeed improves the state-of-the-art performance of
many downstream NLP tasks.
However, the improved utility is not for free. We ﬁnd the
text embeddings from general-purpose language models would
capture much sensitive information from the plain text. Once
being accessed by the adversary, the embeddings can be reverse-
engineered to disclose sensitive information of the victims for
further harassment. Although such a privacy risk can impose a
real threat to the future leverage of these promising NLP tools,
there are neither published attacks nor systematic evaluations by
far for the mainstream industry-level language models.
To bridge this gap, we present the ﬁrst systematic study on the
privacy risks of 8 state-of-the-art language models with 4 diverse
case studies. By constructing 2 novel attack classes, our study
demonstrates the aforementioned privacy risks do exist and can
impose practical threats to the application of general-purpose
language models on sensitive data covering identity, genome,
healthcare and location. For example, we show the adversary
with nearly no prior knowledge can achieve about 75% accuracy
when inferring the precise disease site from Bert embeddings of
patients’ medical descriptions. As possible countermeasures, we
propose 4 different defenses (via rounding, differential privacy,
adversarial training and subspace projection) to obfuscate the
unprotected embeddings for mitigation purpose. With extensive
evaluations, we also provide a preliminary analysis on the utility-
privacy trade-off brought by each defense, which we hope may
foster future mitigation researches.
I. INTRODUCTION
With the advances of deep learning techniques in Natural
Language Processing (NLP), the last year has witnessed many
breakthroughs in building general-purpose language models
by industry leaders like Google, OpenAI and Facebook [16],
[17], [41], [44], [54], [55], [67], [76], which have been
widely used in various downstream NLP tasks such as text
classiﬁcation and question answering [15] and start to ﬁnd
its application in real-world systems such as Google’s search
engine [6], which is said to represent “the biggest leap forward
in the past ﬁve years, and one of the biggest leaps forward in
the history of Search” [6].
Unlike traditional statistical models or shallow neural net-
work models, general-purpose language models typically refer
to the family of Transformer-based pretrained giant language
models including Google’s Bert and OpenAI’s GPT-2, which
are composed of layers of Transformer blocks [72] with
millions of learnable parameters, and are usually pretrained on
billions of sentences before being released. According to the
ofﬁcial tutorials [2], users can apply these pretrained models
as text feature extractors for encoding sentences into dense
vectors, or called sentence embeddings, which can be further
used for various downstream tasks (e.g., text classiﬁcation).
With the release of Bert, Google AI envisions the future of
general-purpose language models as, “anyone in the world can
train their own state-of-the-art question answering system (or
a variety of other models) in about 30 minutes on a single
Cloud TPU, or in a few hours using a single GPU” [4].
Despite the bright envision, for the ﬁrst time, we observe
these general-purpose language models tend to capture much
sensitive information in the sentence embeddings, which leaves
the adversary a window for privacy breach. For example, in
a typical use case of these language models in intelligent
healthcare, a third-party organization issues a cooperation with
a hospital for developing a patient guide system, which auto-
matically assigns the patients to a proper department based on
the symptom descriptions. Due to the generality of Google’s
Bert [17], the organization only needs to request the hospital to
provide the embeddings of the patients’ symptom descriptions
as the essential information for building a high-utility system.
Due to the lack of understanding of the privacy properties
of the general-purpose language models, the hospital may
expect sharing the vector-form features would be much less
private than sharing the plain text, especially when they are
told the encoding rule itself is based on highly complicated
neural networks that are near to black-boxes. In fact, we do
observe with experiments in Appendix F that, even with a
standard decoder module in NLP, it is difﬁcult to recover
any useful information from the embeddings. However, on
eight state-of-the-art language models including Bert and GPT-
2, we devise a lightweight yet effective attack pipeline and
strikingly ﬁnd that given the unprotected sentence embeddings,
even an adversary with nearly zero domain knowledge can
infer domain-related sensitive information in the unknown
plain text with high accuracy. In the above medical example,
our observation strongly implies that the honest-but-curious
service provider as the adversary can easily infer the identity,
gender, birth date, disease type or even the precise disease
site regarding a particular victim, only if the target piece of
information appears in his/her original description.
Our Work. In this paper, we provide the ﬁrst systematic study
© 2020, Xudong Pan. Under license to IEEE.
DOI 10.1109/SP40000.2020.00095
1314
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:28 UTC from IEEE Xplore.  Restrictions apply. 
on the potential privacy risks of general-purpose language
models. Speciﬁcally, we want to answer the main research
question: is it possible for the adversary to infer user’s private
information in the unknown plain text, when the adversary
only has access to his/her submitted embeddings? If the
answer is afﬁrmative, then the future applications of state-
of-the-art NLP tools in compelling learning paradigms like
collaborative or federated learning [36], [37], [46] can be
largely threatened and restricted, especially on privacy-critical
domains including healthcare, genomics and ﬁnance. Besides
the novelty in our major research object, our research question
also has its own specialness compared with most existing
works including membership inference attacks [63], property
inference attacks [23] and model inversion attacks [21], in
terms of the information source and the attack objective.
A more detailed comparison between our study and related
attacks can be found in Section II.
Although previous works in computer vision have shown
the possibility of reconstructing original images from their
pretrained embeddings via autoencoders [18] or generative
models [61], no similar attacks were reported in NLP before.
From our perspective, the discreteness of tokens and the in-
visibility of the vocabulary are two major technical challenges
that prevent
the success of reconstruction attacks on text
embeddings. On one hand, the discreteness of tokens makes
the search over the space of all possible sentences highly
inefﬁcient, mainly because the learning objective is no longer
differentiable as in the visual cases and therefore gradient-
based methods can hardly work [79]. On the other hand, as
language models are accessed as black boxes, the adversary
has no knowledge of the ground-truth vocabulary, without
which the adversary cannot convert the recovered word index
sequences into plain text [38]. Even if the adversary may
prepare one’s own vocabulary, it can be either too small to
contain some sensitive words in the unknown plain text or so
large that bring high computational complexity.
To address these aforementioned challenges, we propose to
reconstruct sensitive information from text embeddings via
inference. Taking inspirations from the observation that the
privacy-related information in text usually appears in small
segments, or is related with the occurrence of certain key-
words [62], we construct two different attack classes, namely
pattern reconstruction attacks and keyword inference attacks,
to demonstrate how sensitive information can be extracted
from the text embeddings. In pattern reconstruction attacks,
the raw text has ﬁxed patterns (e.g., genome sequences) and
the adversary attempts to recover a speciﬁc segment of the
original sequences that contains sensitive information (e.g.,
disease-related gene expressions). In keyword inference attack,
the adversary wants to probe whether the unknown plain text
(e.g., medical descriptions) contains certain sensitive keyword
(e.g., disease site). Focusing on a small segment, the adversary
only needs to infer from a limited number of possibilities
for reconstruction purposes, which alleviates the optimization
difﬁculty caused by the discreteness of tokens. Meanwhile, the
adversary has no need to know the whole vocabulary if the
adversary only cares about the word he/she is interested in.
Extensive experiments on 8 state-of-the-art general-purpose
language models with 4 (identity-, genome-, medical-,
location-related) case studies showed, the adversary can pre-
cisely infer various levels of sensitive information of a target
user from his/her leaked embeddings. For pattern reconstruc-
tion, our attack achieves optimal and average accuracy respec-
tively of 98.2% and 62.4% when inferring the exact nucleotide
type at any speciﬁed positions from the GPT-2 embeddings
of 20-length genome sequences, without any auxiliary infor-
mation. For keyword inference, our attack achieves average
accuracy of 99.8% and 74.8% respectively, when inferring the
occurrence of 10 body-related keywords from the Bert em-
beddings of medical descriptions with and without a shadow
corpus. These results highly demonstrate that the aforemen-
tioned privacy risks do exist and can impose real threats
to the application of general-purpose language models on
sensitive data. Noticeably, all our attacks only need to access
the language model as a cloud service (i.e. ML-as-a-service)
and can be conducted with one PC device. With additional
ablation studies, we further discuss some architecture-related
and data-related factors which may inﬂuence the privacy risk
level of language models. Furthermore, we also propose and
evaluate four possible countermeasures against the observed
threats, via quantization, differential privacy [20], adversarial
training [57] and subspace projection [14]. We hope our
preliminary mitigation study will shed light on future defense
researches and contribute to the design of secure general-
purpose language models.
In summary, we make the following contributions:
• We discover the potential privacy risks in general-purpose
language models by showing, a nearly-zero-knowledge ad-
versary with access to the text embeddings can disclose
much sensitive information in the unknown text.
• We design a general attack pipeline for exploiting user pri-
vacy in text embeddings and implement two practical attacks
with advanced deep learning techniques to demonstrate the
privacy risks.
• We present the ﬁrst systematic evaluations on 8 state-of-
the-art general-purpose language models with 4 diverse case
studies to demonstrate the hidden privacy risks, with an in-
depth analysis on the factors that inﬂuence the privacy.
• We also provide preliminary studies on four possible coun-
termeasures and their utility-privacy trade-off, which we
hope may foster future defense studies.
II. RELATED WORKS
Privacy Attacks against ML. Model inversion attack was ﬁrst
proposed by Fredrikson et al. on statistical models [22] and
later generalized to deep learning systems [21]. In terms of the
attack objective, Fredrikson et al’s attack on image classiﬁers
aims at recovering the prototypical image that represents a
speciﬁc class, while our attacks aim at recovering partially
or fully the plain text behind the embedding. In terms of the
information source, model inversion attack mainly relies on
the parameters of the model itself, while for our attacks, the
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:28 UTC from IEEE Xplore.  Restrictions apply. 
1315
information source is the sentence embedding produced from
the general-purpose language models.
Meanwhile, Fredrikson et al. [21], [22] also discussed the
model inversion attack in the sense that the attack inverts
sensitive information about the input from the model’s output.
To the best of our knowledge, their original attack was mainly
implemented for the decision tree model and is not directly
applicable to deep learning models. Later, some very recent
works have proposed ﬁner-grained attacks which attempt to
recover the exact training images or texts from the predictions
[58], [77] or the gradients [47], [82] in an unknown mini-
batch during the training phase. However, two of them that
target on recovering text from gradients [47], [82] utilize
the explicit representation of word composition in bag-of-
words and cannot be applied to our adversarial setting which
reconstructs texts from the dense sentence embeddings from
general-purpose language models.
As a complement to model inversion attack, Shokri et al.
devised the ﬁrst membership inference attack against machine
learning models [63], which aroused wide research interests
[50], [59], [66], [78] in the past few years. In terms of the
attack objective, membership inference attempts to disclose the
is-in relation between the sample and the real private training
set. In terms of the information source,
the membership
inference attack relies on the probability vector associated
with the input sample. Different from membership inference,
another branch of works called property inference aims at
infering whether the training set has certain global property,
which was ﬁrst studied by [10] on shallow models and later
extended by [23] to deep models.
Aside from privacy attacks on the datasets, some other
threats against the model privacy have also been studied, e.g.,
by demonstrating the possibility of stealing model parameters
[70], architectures [19], and hyper-parameters [73]. In a wider
picture of adversarial machine learning, there still remains
many open problems including adversarial example [27], data
poisoning [13], Byzantine workers [48] and fairness [29],
which are calling for future research efforts on building more
robust and reliable machine learning systems.
Privacy Attacks using ML. Besides, there are also plenty
of prior works using ML approaches to evaluating user pri-
vacy risks regarding, e.g., his/her biomedical and geological
proﬁles. On biomedical privacy, for example, Humbert et al.
[30], [31] leveraged graphical models to infer the genome of an
individual from parental relationships and expert knowledge,
which was recently extended to other types of biomedical data
by [12]. On location privacy, for example, Shokri et al. [64]
used Markov chain modeling to reconstruct the actual traces of
users from obfuscated location information, while some recent
works exploit side channels from social media like hashtags
for location inference using clustering [8] or random forests
[80].
III. PRELIMINARIES
A. Sentence Embedding
Given a vocabulary V that consists of |V| tokens, we call
a sequence x := (w1, . . . , wn) is a sentence of length n if
each token (or word) wi is in the vocabulary V. Following
the nomenclature of representation learning [11], we call a
mapping f from sentences to a real vector space Rd as a
feature extractor. For the sentence x, the vector z := f (x) is
called its embedding.
Prior to the proposal of general-purpose language models,
word embedding and sentence embedding as two traditional
NLP tasks have been widely studied, for which several ma-
ture algorithms exist. For word embedding, algorithms like
word2vec [49] encode the word to its vector representation that
can noticeably preserve the relative semantics between words,
e.g., the difference of the embeddings of the words queen and
woman was observed to be almost identical to that of king and
man [49]. For sentence embeddings, word-frequency-based
algorithms like TF-IDF [60] directly counts word statistics
of a sentence and thus the produced sentence embeddings
are explicit in word composition, which are not suitable for
privacy-critical scenarios [46]. Other learning-based sentence
embedding methods like doc2vec [42] borrow the idea of
word2vec and encode sentences to vectors that preserve the
relative semantics between the sentence and its composite
words in the training corpus. As a result, the produced sentence
embeddings from doc2vec are usually corpus-speciﬁc and are
mainly used for sentence clustering or paraphrase detection on
a given corpus [40], [42].
Recently, the boom of general-purpose language models
has largely reformed how we understand and use embed-
dings in the following aspects. On one hand, the boundary
between word embedding and sentence embedding are no
longer clear due to contextualized word embeddings [52], a
fundamental concept behind these general-purpose language
models. Intuitively, contextualized word embeddings suggest
the embedding of the same word can vary according to the
sentence where it occurs. For example, the contextualized
embedding of the word apple should be different in “I like
apple” and “I like Apple macbooks”. Consequently, most
general-purpose language models list sentence embedding as
one major use case instead of word embedding [2], [74]. On
the other hand, sentence embeddings from pretrained general-