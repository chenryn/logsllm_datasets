•	Score the events in your new short time window with the knowledge gained from the past.
▶ Time series from MLTK
|▶ Time series from MLTK
| 
 |  |  | • | |fit linearregression | eval comment="Use the time fields in splunk as features!" | fields - comment 
| eval date_mday_as_string= date_mday."_" , comment = "date_* are numbers, but we want them treated as categorical strings" | fields - comment | eval date_hour_as_string = date_hour."_" , comment = "date_* are numbers, but we want them treated as categorical strings" | fields - comment| fit LinearRegression Thing from fields, date_mday, date_hour_as_string, date_mday_as_string | |fit linearregression | eval comment="Use the time fields in splunk as features!" | fields - comment 
| eval date_mday_as_string= date_mday."_" , comment = "date_* are numbers, but we want them treated as categorical strings" | fields - comment | eval date_hour_as_string = date_hour."_" , comment = "date_* are numbers, but we want them treated as categorical strings" | fields - comment| fit LinearRegression Thing from fields, date_mday, date_hour_as_string, date_mday_as_string | |fit linearregression | eval comment="Use the time fields in splunk as features!" | fields - comment 
| eval date_mday_as_string= date_mday."_" , comment = "date_* are numbers, but we want them treated as categorical strings" | fields - comment | eval date_hour_as_string = date_hour."_" , comment = "date_* are numbers, but we want them treated as categorical strings" | fields - comment| fit LinearRegression Thing from fields, date_mday, date_hour_as_string, date_mday_as_string |
|---|---|---|---|---|---|---|
|   | | |• |or cluster to find behaviors through time that are similar or not. |or cluster to find behaviors through time that are similar or not. |or cluster to find behaviors through time that are similar or not. |
|   | | |• | | | |
Technique: Machine Learning Toolkit Numeric ClusteringData Reduction
| 
 | 
 | 
 |
|---|---|---|
|  | | |
2.7 million events per day -> 3 anomalies per day
Need more or less? Adjust the number of IQRs
Technique: Machine Learning Toolkit Numeric Clustering Now You’re Trained
16
   9   
Approach to Analytics 
What? Not a Technique?What? Not a Technique?
▶ As Use Case writers get more expert, they tend to move further away from very simple use cases into more advanced anomaly detection. As we progress from "EventCode=1102" to "dc(servers) > 3 stdev + avg" value is harder to find. 
▶ There are many conceptual approaches to dealing with this, but they all tend to 	boil down to the core idea of a two phase approach.• The first phase is to find anomalous activites, which may be good or bad. These are generally 	low confidence, and shouldn’t be sent to the SOC directly.
• The second phase is to aggregate anomalies into something the SOC should view. Call them 	threats, or multi-vector alerts, or whatever you want - I call them threats.
▶ This section lays out how I view and group anomalies vs threats.Technical Components of Security Analytics
| Alert Aggregation | Threat 
Detection | Threat 
Detection | ▶ Manage High Volume
▶ Track Entity Relationships▶ Combination ML + Rules | ▶ Manage High Volume
▶ Track Entity Relationships▶ Combination ML + Rules | ▶ Detect unknown |
|---|---|---|---|---|---|
| AlertCreation |Simpler  |▶ Rules & Statistics |▶ Rules & Statistics |ML Based  |▶ Detect unknown || AlertCreation |Simpler  |▶ Quick development |▶ Quick development |ML Based  |▶ New vectors |
| AlertCreation |Detection |▶ Quick development |▶ Quick development |Detection |▶ New vectors |
| AlertCreation |Detection |▶ Easy for analysts |▶ Easy for analysts |Detection |▶ Heavy data science |
| Investigation | Investigative  | ▶ Analyst Flexibility |
|---|---|---||---|---|---|
| Investigation |Investigative  |▶ Provide access to data analysis solutions |
| Investigation |Platform |▶ Provide access to data analysis solutions |
| Investigation |Platform |▶ Record historical context for everything |
|  | | |
Approach to Analytics
Investigative TierVirtually every modern security detection requires some investigation, and always has. As attackers become more advanced, detection mechanisms become more advanced, it is critical to advance the investigative platform to keep pace with the new needs. The needs for investigation range from ticketing, workflow, large scale log search, the capability to ingest all of the data that will later be needed by an investigation, and more. This is the most mature of the range of capabilities required for Security Analytics success, and most organizations will have a decent investigative capability available.While most organizations do have some basis for investigation, technology leaders must note where the key requirements differ for Security Analytics. Detections powered heavily by machine learning by definition produce more abstract results that junior level analysts have a harder time understanding. Part of the onus for auctioning these events lies with the detection logic itself providing as much context as possible to enable action, but additionally the investigative tier must be more robust to allow analysts to quickly understand a detection. This includes both the presentation of contextual information from the detection logic itself (baseline information, degree of deviation, etc.), but also a capability to more quickly explore greater amounts of data, and to have potentially relevant information surfaced.Newer innovations to support these needs include simpler access to information (faster and more usable dashboards, form search, natural language processing), adaptive response capabilities to automate many of the menial tasks (such as acquiring forensic details, and automating remediation for predictable categories of events). 
17
   2   
Approach to Analytics 
Anomaly Detection Tier: Alert Creation - Simpler DetectionsEveryone’s first effort in the world of UEBA is to leverage rules or statistical detections. Basic approaches here are to alert if someone prints more than 100 pages, or emails more than ten documents. As organizations mature, they begin leveraging per user (or per system, per entity) baselines. This allows them to track if a user who normally prints only a few pages suddenly starts printing 75 pages - that can be an anomaly for that user, but the person who prints 200 pages a day won’t be flagged unless they go far outside their normal baseline.These detections are beneficial because they are specific to known threat vectors, and can be quickly created to detect future events within the SOC. Just as important, they are simple for security analysts to understand and action.Within the Splunk Portfolio, the best place for simpler detections is Splunk Enterprise. Splunk customers have been using these detections for a decade, and they can be built quickly and easily. Splunk has recently doubled down on this effort and released the free Splunk Security Essentials app which delivers 50+ use cases commonly found in UEBA products. It is easy for SOC engineers to build out their own use cases leveraging time series analysis, first time seen detections, and even other advanced analytics like entropy detection, levenshtein lookalike detection, and more.17
   3   
Approach to Analytics 
Anomaly Detection Tier: Alert Creation - ML Based Detections
Many organizations have tried and failed in the past to deliver Security Analytics solutions with the simpler detections alone - while many quick wins can be attained with these technologies, they ultimately require an extreme operational expense due to the technology needs, in addition to an inability to detect novel methods. To compensate for a comparatively limited scope in detecting anomalies, teams end up doing extensive hunting, or building many rules via professional services to accommodate for scenarios that might be relevant in the future.With Machine Learning, you can start detecting tools, techniques, or procedures that you didn’t necessarily know how to predict. You can build out far more advanced technique techniques that simply aren’t possible with more basic data analysis platforms. With PhD driven data science, the magnitude of detection is substantially greater.Importantly, the recent availability of scalable ML detection doesn’t reduce the need for simpler detections, the two complement each other. The simpler detections tend to produce higher confidence detections more easily actioned by SOC members for known techniques, where the advanced machine learning models can provide a backstop to approach detection from a different perspective, finding attackers the simpler detections didn’t know to look for.In the Splunk Security Portfolio, Splunk UBA is a data science platform that can facilitate these advanced anomaly detection models. Both with advanced known attack detections such as the HTTP model, that tracks known techniques in a way not possible on a lesser data science platform, or the advanced rarity and markovian models that can detect threats you didn’t know how to build, Splunk UBA provides the horsepower needed to detect the suspicious actions within your environment.17
   4   
Approach to Analytics
Anomaly Detection Tier: Alert Aggregation
The third major component of successful Security Analytics programs is an advanced threat detection capability, augmented with Machine Learning. A pre-requisite for this component is that a customer must have a successful capability for simpler rules / statistical detections, and for advanced machine learning detections. Effectively, there must be something for the threat detection to review.Once an organization makes the investment in those initial two components, they will need to analyze a large volume of anomalous activities. It is inherent in anomaly detection technologies that there will be a great amount of noise. If that is tuned down, critical events will be missed. The solution to this is to have a second level of rules and machine learning that sits on top of the anomalies to aggregate useful events into threats. Many legacy products in this space have deployed simple rule based logic, or surfaced the users with the greatest number of threats, but these naïve approaches are insufficient.For Splunk’s Security Portfolio, Splunk UBA runs a set of machine learning powered threat models over the collection of anomalies, to surface the threats that need to be reviewed by the SOC. Even that alone is not enough - to do threat detection successfully, you also need to understand the relationships between every entity in the environment. This graph mining is a key conceptual advantage of Splunk UBA’s threat models over what can be done by Splunk users directly.17
   5   
Splunk Security Portfolio
Enterprise Security
Response
• OOB key security metrics
|  | Splunk Enterprise | • Incident response workflow | Realm of  | Splunk UBA | Splunk UBA |
|---|---|---|---|---|---|
|  |Splunk Enterprise |• Adaptive response |Realm of  |Splunk UBA |Splunk UBA |
|  |Detection |• Adaptive response |Realm of  |Detection |Detection || Realm of  |• Log Aggregation | |Realm of  |– |Risky behavior detection |
| Realm of  |• Log Aggregation | |Realm of  |– |Entity profiling, scoring |
| Known |• Splunk Security Essentials | |Unknown |– |Kill chain, graph analysis |
• Rules, statistics, correlation
Human-driven ML-driven
176
© 2017 SPLUNK INC.
End to End Searches
Welcome to the Full PictureWelcome to the Full Picture
Search: When Log Sources Go Quiet Background and Challenges
▶ "I need to know if my XYZ logs stop coming in"
▶ "ABC Malware shuts off our AV product and Windows Updates, how do I detect 	that?"
▶ Splunk users routinely need to monitor for log sources being shut off. This is typically done at both the global level (do we have latency on PAN logs) and also on a per host basis (are we getting Windows Logon events but no EDR events).Search: When Log Sources Go Quiet Detecting Individual Sources that Go Quiet
▶ Here we use a lot of tstats and stats to detect if just the Windows Security log 	goes offline for a host.
| tstats prestats=t count(host) where index=* groupby host _time 	span=1d 
| tstats prestats=t append=t count where index=* 
	sourcetype=win*security by host  _time span=1d | stats count(host) as all_logs count as win_logs by host _time| eval win_perc=round(100*(win_logs / all_logs), 2) 
| stats count as num_data_samples 
	avg(eval(if(_time=relative_time(maxtime, "-1d@d"), win_perc, 	null))) as latest 
	by host 
| where isnotnull(avg) AND num_data_samples>10 ANDisnull(past_instances_of_no_logs) AND latest=0
We use a couple of tstats tricks to pull in the number 	of log files in general for a host, and the number of 	Windows Security logs
Eval calculates the percentage of Windows Security
Stats allows us to track the baseline per host
Finally, where allows us to look for new instances of 	no Win Security logs
Search: When Log Sources Go Quiet Detecting Hosts That Go Quiet▶ Here we look across the board for a particular host, and compare the typical time 	gap between periods of logs. Then we alert for excessive gaps.
| tstats count where index=* by host _time span=4h 
| streamstats window=2 range(_time) as timediff by host 
| stats count max(timediff) as max_timediff avg(timediff) as avg_timediff 	stdev(timediff) as stdev_timediff max(_time) as latest by host | eval currentlag = now() - latest| where currentlag > avg_timediff*2 + stdev_timediff*6 AND count>12 | eval currentlag_in_hours=round(currentlag/3600,2)
Here tstats is giving us the number of events quickly, 	grouped by four hour chunks, ordered by host
Streamstats will pull the diff
Then we use stats to pull the average, stdev, and 	number of data samples.
Then we calculate the current lag
Then we filter for hidden events.Then we filter for hidden events. 
Finally, let’s format this stuff.
Search: When Log Sources Go Quiet
Broader Dashboard Support
▶ Many SOC Customers I work with have a dashboard that SOC analysts can go to to get the status of the different log sources. The goal is to let everyone know if all of a sudden Palo Alto Networks logs are delayed.▶ These dashboards typically have a series of boxes with Green / Yellow Red 	indicators for each data source.
▶ Many customers have even 
begun using ITSI to track their 
data source pipelines.
Here’s a screenshot of a POC 
at one customer.
Search: When Log Sources Go Quiet Working Example
▶ In Splunk Security Essentials we have any example of the earlier query
|| 
 |  | 1. | Download the app off Splunkbase | Download the app off Splunkbase | Download the app off Splunkbase |
|---|---|---|---|---|---|
|   | |2. |Open up Hosts Where Security Sources Go Quiet |Open up Hosts Where Security Sources Go Quiet |Open up Hosts Where Security Sources Go Quiet |
|   | |3. |Click "Show SPL" to see the SPL |Click "Show SPL" to see the SPL |Click "Show SPL" to see the SPL ||   | | | | | |
© 2017 SPLUNK INC.
1. Watch the earlier Ninjutsus when you get 
home: dvsplunk.com or conf.splunk.com
Key 
Takeaways 
This is where the 
subtitle goes 2. Grab the PDF Version of this deck and 	dig in deeper
Hey, you’re on the PDF version. Look at you, ahead of the game! You should go watch the video though - conf.splunk.com 5-6 weeks after conf.
3. Grab the app(s) and explore examples© 2017 SPLUNK INC.
Thank You
Don't forget to rate this session in the 
.conf2017 mobile app
I get to come back if 
you give me good 
ratings. Rate high, 
early, and often!