setups.
3.3 Attack implementations
We focus on demonstrating the effectiveness of the attack on the
image domain as this is predominant in split learning studies [25,
25, 42, 51, 54â€“58]. In our experiments, we rely on different image
datasets to validate the attack; namely, MNIST, Fashion-MNIST [61],
Omniglot [37] and CelebA [40]. We demonstrate the effectiveness
of the attack on additional datasets in Appendix A. In this sec-
tion, we introduce the attack by simulating the clientsâ€™ training set
Session 7A: Privacy Attacks and Defenses for ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2117ğ‘‹ğ‘ğ‘Ÿğ‘–ğ‘£
)
)
1
,
1
(
,
3
,
4
6
(
v
n
o
C
-
D
2
U
L
e
R
n
o
i
t
a
z
i
l
a
m
r
o
n
-
h
c
t
a
b
U
L
e
R
)
)
2
,
2
(
(
g
n
i
l
l
o
P
x
a
m
)
1
,
4
6
(
k
c
o
l
B
s
e
r
)
2
,
8
2
1
(
k
c
o
l
B
s
e
r
)
1
,
8
2
1
(
k
c
o
l
B
s
e
r
)
2
,
6
5
2
(
k
c
o
l
B
s
e
r
split 1
split 2
split 3
split 4
Figure 3: Architecture of the clientâ€™s network ğ‘“ divided in 4 different depth levels. The internal setup of the adopted residual
blocks is described in Algorithm 2 (Appendix B).
(a) MNIST.
(b) Fashion-MNIST.
(c) Omniglot.
(d) CelebA.
Figure 4: Reconstruction error of private training instances during the setup phase for four different splits and four different
datasets. This is measured as the average MSE between the images normalized in the [âˆ’1, 1] interval.
(a) MNIST.
(b) Fashion-MNIST.
(c) Omniglot.
(d) CelebA.
Figure 5: Examples of inference of private training instances from smashed data for four datasets for the split 4 of ğ‘“ . Within
each panel, the first row (i.e., gray frame) reports the original data, whereas the second row (i.e., red frame) depicts the attackerâ€™s
reconstruction. The reported examples are chosen randomly from ğ‘‹ğ‘ğ‘Ÿğ‘–ğ‘£.
050001000015000200002500030000Numberofsetupiterations0.000.010.020.030.040.050.060.070.080.090.10AverageReconstructionerror(MSE)split1split2split3split40200040006000800010000Numberofsetupiterations0.000.050.100.150.20AverageReconstructionerror(MSE)0200040006000800010000Numberofsetupiterations0.000.050.100.150.20AverageReconstructionerror(MSE)02500500075001000012500150001750020000Numberofsetupiterations0.00.10.20.30.40.50.60.70.8AverageReconstructionerror(MSE)050001000015000200002500030000Numberofsetupiterations0.000.010.020.030.040.050.060.070.080.090.10AverageReconstructionerror(MSE)Session 7A: Privacy Attacks and Defenses for ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2118(i.e., ğ‘‹ğ‘ğ‘Ÿğ‘–ğ‘£) using the training partition of the datasets, whereas we
use their validation sets as ğ‘‹ğ‘ğ‘¢ğ‘. Then, in Section 3.4, we demon-
strate the effectiveness of the attack on subpar setups for the at-
tacker.
Attack setup. We implement the various networks participat-
ing in the attack as deep convolution neural networks. For the
clientâ€™s network ğ‘“ , we rely on a residual network [28] with a fun-
nel structureâ€”a pervasive architecture widely employed for tasks
defined on the image domain. In our experiments, we test the pro-
posed attackâ€™s effectiveness on increasingly deep splits of ğ‘“ . These
are depicted in Figure 3.
The network Ëœğ‘“ (the attackerâ€™s pilot network) is constructed by
leveraging a different architecture from the one used for ğ‘“ . In partic-
ular, the network is chosen to be as simple as possible (i.e., shallow
and with a limited number of parameters). Intuitively, this allows
us to define a very smooth target latent-space ËœZ and simplify the
learning process of ğ‘“ during the attack. The inverse mapping Ëœğ‘“ âˆ’1
is also a shallow network composed of transposed convolutional
layers. The discriminator ğ· is a residual network and is chosen to be
deeper than the other employed networks with the intent of forcing
the feature spaces of ğ‘“ and Ëœğ‘“ to be as similar as possible until they
become indistinguishable. During the setup phase, we regularize ğ·
with a gradient penalty and use the Wasserstein loss [24] for the
adversarial training. This greatly improves the stability of the attack
and speeds up the convergence of ğ‘“ . We rely on slightly different
architectures for the attackerâ€™s networks (i.e., Ëœğ‘“ , Ëœğ‘“ âˆ’1 and ğ·) based
on the depth of the split of ğ‘“ . More detailed information about these,
other hyper-parameters, and datasets pre-processing operations
are given in Appendix B.
Attack results. During the attack, we use the MSE as the dis-
tance function ğ‘‘ (see Eq. 1). In the process, we track the attackerâ€™s
reconstruction error measured as:
ğ‘€ğ‘†ğ¸( Ëœğ‘“ âˆ’1(ğ‘“ (ğ‘‹ğ‘ğ‘Ÿğ‘–ğ‘£)), ğ‘‹ğ‘ğ‘Ÿğ‘–ğ‘£).
This is reported in Figure 4 for the four datasets and four different
splits of ğ‘“ . In the experiments, different datasets required differ-
ent numbers of setup iterations to reach adequate reconstructions.
Low-entropy distributions like those in MNIST and Fashion-MNIST
can be accurately reconstructed within the first 103 setup iterations.
On the other hand, natural images and complex distributions, like
CelebA and Omniglot, tend to require more iterations (3 Â· 103 and
2 Â· 103 respectively). It is important to note that clients depend en-
tirely on the server and entrust it with the modelâ€™s utility measure
(validation error) during the training. The server, therefore, can
directly control the stop-conditions (e.g., early-stopping) by pro-
viding suitable feedback to clients and dynamically requiring them
to perform the number of training iterations needed to converge
towards suitable reconstructions.
As the plots in Figure 4 show, there is only a negligible difference
in the reconstruction error achieved from attacks performed on
the four different splits of ğ‘“ . In those experiments, the depth of the
clientâ€™s network seems to affect only the convergence speed of the
setup phase with a limited impact on the final performance.
Also, in the case of the deepest split (split 4), the FSHA allows the
attacker to achieve precise reconstructions. These can be observed
in Figure 5, where the attack provides very accurate reconstruc-
tions of the original private data for all the tested datasets. More
interestingly, the Omniglot dataset highlights the generalization ca-
pability of the feature-space hijacking attack. The Omniglot dataset
is often used as a benchmark for one-shot learning and contains
1623 different classes with a limited number of examples each. The
attackâ€™s performance on this dataset suggests that the proposed
technique can reach a good generalization level over private data
distributions. We will investigate this property more thoroughly in
the next section.
Hereafter, we will report results only for the split 4 as this rep-
resents the worst-case scenario for our attack. Moreover, it also
captures the best practices of split learning. Indeed, deeper architec-
tures for ğ‘“ are assumed to make it harder for an attacker to recover
information from the smashed data as this has been produced using
more complex transformations [5, 58].
3.4 On the effect of the public dataset
It should be apparent that the training set ğ‘‹ğ‘ğ‘¢ğ‘ employed by the
server can critically impact the attackâ€™s effectiveness. This is used to
train the attackerâ€™s models and indirectly defines the target feature
space imposed on ğ‘“ . Ideally, to reach high-quality reconstruction,
this should be distributed as similarly as possible to the private train-
ing sets owned by the clients. However, under strict assumptions,
the attacker may collect data instances that are not sufficiently
representative. We show that the Feature-space Hijacking Attack
can be successfully applied even when the attacker employs inade-
quate/inaccurate choices of ğ‘‹ğ‘ğ‘¢ğ‘.
3.4.1 Public dataset coming from a different distribution. Next, we
analyze the effect of choices of ğ‘‹ğ‘ğ‘¢ğ‘ following a different distri-
bution with respect to the private one. We start by attacking the
dataset CelebA (ğ‘‹ğ‘ğ‘Ÿğ‘–ğ‘£) relying on a different face dataset (UTK-
Face [62]) as public dataset ğ‘‹ğ‘ğ‘¢ğ‘.
The UTKFace dataset aggregates pictures of heterogeneous indi-
viduals. It portraits people within a wide age range (between 0 to
116 years) and covers several ethnicities (White, Black, Asian, In-
dian, and Others). The distribution of CelebA is, instead, consistently
more homogeneous and strongly skewed towards the Caucasian
race with a stricter age range.7
Figure 6: Average reconstruction error during the FSHA for
three different setups (zoomed in).
7The dataset is composed of images of celebrities.
Session 7A: Privacy Attacks and Defenses for ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2119(a) Attacking CelebA with ğ‘‹ğ‘ğ‘¢ğ‘ =UTKFace.
(b) Attacking UTKFace with ğ‘‹ğ‘ğ‘¢ğ‘ =CelebA.
Figure 7: Random examples of reconstruction attacks for two setups. Panel (a) reports the result for the case ğ‘‹ğ‘ğ‘Ÿğ‘–ğ‘£ =CelebA and
ğ‘‹ğ‘ğ‘¢ğ‘ =UTKFace. Panel (b) reports the result for the case ğ‘‹ğ‘ğ‘Ÿğ‘–ğ‘£ =UTKFace and ğ‘‹ğ‘ğ‘¢ğ‘ =CelebA.
experiment, the mangling operation removes all the instances of a
specific class from ğ‘‹ğ‘ğ‘¢ğ‘ while leaving ğ‘‹ğ‘ğ‘Ÿğ‘–ğ‘£ (the training set used
by the clients) unchanged. For instance, in the case of the MNIST
dataset, we remove from ğ‘‹ğ‘ğ‘¢ğ‘ all the images representing a specific
digit. Then, we test the attackâ€™s ability to reconstruct instances of
the removed class i.e., data instances that the attacker has never
observed during the setup phase.
Interestingly, the attack seems quite resilient to an incomplete
ğ‘‹ğ‘ğ‘¢ğ‘. The results are depicted in Figure 9 for 10 different attacks
carried out with ğ‘‹ğ‘ğ‘¢ğ‘ stripped of a specific class. For each attack,
the average reconstitution error for the unknown classes (i.e., red