### 3.3 Attack Implementations

We focus on demonstrating the effectiveness of the attack in the image domain, as this is predominant in split learning studies [25, 42, 51, 54-58]. In our experiments, we use various image datasets to validate the attack: MNIST, Fashion-MNIST [61], Omniglot [37], and CelebA [40]. Additional results on other datasets are provided in Appendix A. This section introduces the attack by simulating the clients' training set (i.e., \( X_{\text{priv}} \)) using the training partitions of the datasets, while their validation sets are used as \( X_{\text{pub}} \). In Section 3.4, we demonstrate the attack's effectiveness under suboptimal conditions for the attacker.

#### Attack Setup

The networks involved in the attack are implemented as deep convolutional neural networks. For the client's network \( f \), we use a residual network [28] with a funnel structure, which is widely employed for tasks in the image domain. We test the proposed attack's effectiveness on increasingly deep splits of \( f \), as shown in Figure 3.

The attacker's pilot network \( \tilde{f} \) is constructed with a different, simpler architecture (i.e., shallow and with a limited number of parameters). This simplifies the target latent space \( \tilde{Z} \) and eases the learning process of \( f \) during the attack. The inverse mapping \( \tilde{f}^{-1} \) is also a shallow network composed of transposed convolutional layers. The discriminator \( D \) is a deeper residual network, designed to force the feature spaces of \( f \) and \( \tilde{f} \) to be indistinguishable. During the setup phase, we regularize \( D \) with a gradient penalty and use the Wasserstein loss [24] for adversarial training, which improves the stability of the attack and accelerates the convergence of \( f \). Slightly different architectures are used for the attacker's networks (i.e., \( \tilde{f} \), \( \tilde{f}^{-1} \), and \( D \)) based on the depth of the split of \( f \). More detailed information about these architectures, hyperparameters, and dataset preprocessing operations is provided in Appendix B.

#### Attack Results

During the attack, we use the Mean Squared Error (MSE) as the distance function \( d \) (see Eq. 1). We track the attacker's reconstruction error, measured as:

\[ \text{MSE}(\tilde{f}^{-1}(f(X_{\text{priv}})), X_{\text{priv}}) \]

This is reported in Figure 4 for the four datasets and four different splits of \( f \). Different datasets required varying numbers of setup iterations to achieve adequate reconstructions. Low-entropy distributions like those in MNIST and Fashion-MNIST can be accurately reconstructed within the first 1000 setup iterations. Natural images and complex distributions, such as CelebA and Omniglot, require more iterations (approximately 3000 and 2000, respectively). It is important to note that clients depend entirely on the server and trust it with the model's utility measure (validation error) during training. The server can control stop-conditions (e.g., early stopping) by providing appropriate feedback to clients and dynamically adjusting the number of training iterations needed for suitable reconstructions.

As shown in Figure 4, there is only a negligible difference in the reconstruction error achieved from attacks performed on the four different splits of \( f \). The depth of the client's network primarily affects the convergence speed of the setup phase, with limited impact on the final performance. Even in the case of the deepest split (split 4), the Feature-Space Hijacking Attack (FSHA) allows the attacker to achieve precise reconstructions, as observed in Figure 5. The Omniglot dataset, often used as a benchmark for one-shot learning, highlights the generalization capability of the FSHA. The attack's performance on this dataset suggests that the proposed technique can generalize well over private data distributions. We will investigate this property more thoroughly in the next section.

Hereafter, we report results only for the split 4, as it represents the worst-case scenario for our attack and captures best practices in split learning. Deeper architectures for \( f \) are assumed to make it harder for an attacker to recover information from the smashed data due to more complex transformations [5, 58].

### 3.4 Effect of the Public Dataset

The training set \( X_{\text{pub}} \) used by the server can critically impact the attack's effectiveness. This set is used to train the attacker's models and indirectly defines the target feature space imposed on \( f \). Ideally, for high-quality reconstructions, \( X_{\text{pub}} \) should be distributed as similarly as possible to the private training sets owned by the clients. However, under strict assumptions, the attacker may collect data instances that are not sufficiently representative. We show that the FSHA can be successfully applied even when the attacker uses inadequate or inaccurate choices of \( X_{\text{pub}} \).

#### 3.4.1 Public Dataset from a Different Distribution

Next, we analyze the effect of using \( X_{\text{pub}} \) from a different distribution compared to the private one. We start by attacking the CelebA dataset ( \( X_{\text{priv}} \) ) using a different face dataset (UTKFace [62]) as the public dataset \( X_{\text{pub}} \).

The UTKFace dataset includes pictures of individuals across a wide age range (0 to 116 years) and covers several ethnicities (White, Black, Asian, Indian, and Others). In contrast, the CelebA distribution is more homogeneous and strongly skewed towards the Caucasian race with a stricter age range. Figure 6 shows the average reconstruction error during the FSHA for three different setups (zoomed in).

Figure 7 provides random examples of reconstruction attacks for two setups. Panel (a) shows the result for the case \( X_{\text{priv}} = \text{CelebA} \) and \( X_{\text{pub}} = \text{UTKFace} \). Panel (b) shows the result for the case \( X_{\text{priv}} = \text{UTKFace} \) and \( X_{\text{pub}} = \text{CelebA} \).

In another experiment, the mangling operation removes all instances of a specific class from \( X_{\text{pub}} \) while leaving \( X_{\text{priv}} \) (the training set used by the clients) unchanged. For example, in the case of the MNIST dataset, we remove all images representing a specific digit from \( X_{\text{pub}} \). We then test the attack's ability to reconstruct instances of the removed class, i.e., data instances that the attacker has never observed during the setup phase.

Interestingly, the attack seems quite resilient to an incomplete \( X_{\text{pub}} \). The results are depicted in Figure 9 for 10 different attacks carried out with \( X_{\text{pub}} \) stripped of a specific class. For each attack, the average reconstruction error for the unknown classes (i.e., red) is reported.