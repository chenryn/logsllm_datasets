setups.
3.3 Attack implementations
We focus on demonstrating the effectiveness of the attack on the
image domain as this is predominant in split learning studies [25,
25, 42, 51, 54–58]. In our experiments, we rely on different image
datasets to validate the attack; namely, MNIST, Fashion-MNIST [61],
Omniglot [37] and CelebA [40]. We demonstrate the effectiveness
of the attack on additional datasets in Appendix A. In this sec-
tion, we introduce the attack by simulating the clients’ training set
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2117𝑋𝑝𝑟𝑖𝑣
)
)
1
,
1
(
,
3
,
4
6
(
v
n
o
C
-
D
2
U
L
e
R
n
o
i
t
a
z
i
l
a
m
r
o
n
-
h
c
t
a
b
U
L
e
R
)
)
2
,
2
(
(
g
n
i
l
l
o
P
x
a
m
)
1
,
4
6
(
k
c
o
l
B
s
e
r
)
2
,
8
2
1
(
k
c
o
l
B
s
e
r
)
1
,
8
2
1
(
k
c
o
l
B
s
e
r
)
2
,
6
5
2
(
k
c
o
l
B
s
e
r
split 1
split 2
split 3
split 4
Figure 3: Architecture of the client’s network 𝑓 divided in 4 different depth levels. The internal setup of the adopted residual
blocks is described in Algorithm 2 (Appendix B).
(a) MNIST.
(b) Fashion-MNIST.
(c) Omniglot.
(d) CelebA.
Figure 4: Reconstruction error of private training instances during the setup phase for four different splits and four different
datasets. This is measured as the average MSE between the images normalized in the [−1, 1] interval.
(a) MNIST.
(b) Fashion-MNIST.
(c) Omniglot.
(d) CelebA.
Figure 5: Examples of inference of private training instances from smashed data for four datasets for the split 4 of 𝑓 . Within
each panel, the first row (i.e., gray frame) reports the original data, whereas the second row (i.e., red frame) depicts the attacker’s
reconstruction. The reported examples are chosen randomly from 𝑋𝑝𝑟𝑖𝑣.
050001000015000200002500030000Numberofsetupiterations0.000.010.020.030.040.050.060.070.080.090.10AverageReconstructionerror(MSE)split1split2split3split40200040006000800010000Numberofsetupiterations0.000.050.100.150.20AverageReconstructionerror(MSE)0200040006000800010000Numberofsetupiterations0.000.050.100.150.20AverageReconstructionerror(MSE)02500500075001000012500150001750020000Numberofsetupiterations0.00.10.20.30.40.50.60.70.8AverageReconstructionerror(MSE)050001000015000200002500030000Numberofsetupiterations0.000.010.020.030.040.050.060.070.080.090.10AverageReconstructionerror(MSE)Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2118(i.e., 𝑋𝑝𝑟𝑖𝑣) using the training partition of the datasets, whereas we
use their validation sets as 𝑋𝑝𝑢𝑏. Then, in Section 3.4, we demon-
strate the effectiveness of the attack on subpar setups for the at-
tacker.
Attack setup. We implement the various networks participat-
ing in the attack as deep convolution neural networks. For the
client’s network 𝑓 , we rely on a residual network [28] with a fun-
nel structure—a pervasive architecture widely employed for tasks
defined on the image domain. In our experiments, we test the pro-
posed attack’s effectiveness on increasingly deep splits of 𝑓 . These
are depicted in Figure 3.
The network ˜𝑓 (the attacker’s pilot network) is constructed by
leveraging a different architecture from the one used for 𝑓 . In partic-
ular, the network is chosen to be as simple as possible (i.e., shallow
and with a limited number of parameters). Intuitively, this allows
us to define a very smooth target latent-space ˜Z and simplify the
learning process of 𝑓 during the attack. The inverse mapping ˜𝑓 −1
is also a shallow network composed of transposed convolutional
layers. The discriminator 𝐷 is a residual network and is chosen to be
deeper than the other employed networks with the intent of forcing
the feature spaces of 𝑓 and ˜𝑓 to be as similar as possible until they
become indistinguishable. During the setup phase, we regularize 𝐷
with a gradient penalty and use the Wasserstein loss [24] for the
adversarial training. This greatly improves the stability of the attack
and speeds up the convergence of 𝑓 . We rely on slightly different
architectures for the attacker’s networks (i.e., ˜𝑓 , ˜𝑓 −1 and 𝐷) based
on the depth of the split of 𝑓 . More detailed information about these,
other hyper-parameters, and datasets pre-processing operations
are given in Appendix B.
Attack results. During the attack, we use the MSE as the dis-
tance function 𝑑 (see Eq. 1). In the process, we track the attacker’s
reconstruction error measured as:
𝑀𝑆𝐸( ˜𝑓 −1(𝑓 (𝑋𝑝𝑟𝑖𝑣)), 𝑋𝑝𝑟𝑖𝑣).
This is reported in Figure 4 for the four datasets and four different
splits of 𝑓 . In the experiments, different datasets required differ-
ent numbers of setup iterations to reach adequate reconstructions.
Low-entropy distributions like those in MNIST and Fashion-MNIST
can be accurately reconstructed within the first 103 setup iterations.
On the other hand, natural images and complex distributions, like
CelebA and Omniglot, tend to require more iterations (3 · 103 and
2 · 103 respectively). It is important to note that clients depend en-
tirely on the server and entrust it with the model’s utility measure
(validation error) during the training. The server, therefore, can
directly control the stop-conditions (e.g., early-stopping) by pro-
viding suitable feedback to clients and dynamically requiring them
to perform the number of training iterations needed to converge
towards suitable reconstructions.
As the plots in Figure 4 show, there is only a negligible difference
in the reconstruction error achieved from attacks performed on
the four different splits of 𝑓 . In those experiments, the depth of the
client’s network seems to affect only the convergence speed of the
setup phase with a limited impact on the final performance.
Also, in the case of the deepest split (split 4), the FSHA allows the
attacker to achieve precise reconstructions. These can be observed
in Figure 5, where the attack provides very accurate reconstruc-
tions of the original private data for all the tested datasets. More
interestingly, the Omniglot dataset highlights the generalization ca-
pability of the feature-space hijacking attack. The Omniglot dataset
is often used as a benchmark for one-shot learning and contains
1623 different classes with a limited number of examples each. The
attack’s performance on this dataset suggests that the proposed
technique can reach a good generalization level over private data
distributions. We will investigate this property more thoroughly in
the next section.
Hereafter, we will report results only for the split 4 as this rep-
resents the worst-case scenario for our attack. Moreover, it also
captures the best practices of split learning. Indeed, deeper architec-
tures for 𝑓 are assumed to make it harder for an attacker to recover
information from the smashed data as this has been produced using
more complex transformations [5, 58].
3.4 On the effect of the public dataset
It should be apparent that the training set 𝑋𝑝𝑢𝑏 employed by the
server can critically impact the attack’s effectiveness. This is used to
train the attacker’s models and indirectly defines the target feature
space imposed on 𝑓 . Ideally, to reach high-quality reconstruction,
this should be distributed as similarly as possible to the private train-
ing sets owned by the clients. However, under strict assumptions,
the attacker may collect data instances that are not sufficiently
representative. We show that the Feature-space Hijacking Attack
can be successfully applied even when the attacker employs inade-
quate/inaccurate choices of 𝑋𝑝𝑢𝑏.
3.4.1 Public dataset coming from a different distribution. Next, we
analyze the effect of choices of 𝑋𝑝𝑢𝑏 following a different distri-
bution with respect to the private one. We start by attacking the
dataset CelebA (𝑋𝑝𝑟𝑖𝑣) relying on a different face dataset (UTK-
Face [62]) as public dataset 𝑋𝑝𝑢𝑏.
The UTKFace dataset aggregates pictures of heterogeneous indi-
viduals. It portraits people within a wide age range (between 0 to
116 years) and covers several ethnicities (White, Black, Asian, In-
dian, and Others). The distribution of CelebA is, instead, consistently
more homogeneous and strongly skewed towards the Caucasian
race with a stricter age range.7
Figure 6: Average reconstruction error during the FSHA for
three different setups (zoomed in).
7The dataset is composed of images of celebrities.
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2119(a) Attacking CelebA with 𝑋𝑝𝑢𝑏 =UTKFace.
(b) Attacking UTKFace with 𝑋𝑝𝑢𝑏 =CelebA.
Figure 7: Random examples of reconstruction attacks for two setups. Panel (a) reports the result for the case 𝑋𝑝𝑟𝑖𝑣 =CelebA and
𝑋𝑝𝑢𝑏 =UTKFace. Panel (b) reports the result for the case 𝑋𝑝𝑟𝑖𝑣 =UTKFace and 𝑋𝑝𝑢𝑏 =CelebA.
experiment, the mangling operation removes all the instances of a
specific class from 𝑋𝑝𝑢𝑏 while leaving 𝑋𝑝𝑟𝑖𝑣 (the training set used
by the clients) unchanged. For instance, in the case of the MNIST
dataset, we remove from 𝑋𝑝𝑢𝑏 all the images representing a specific
digit. Then, we test the attack’s ability to reconstruct instances of
the removed class i.e., data instances that the attacker has never
observed during the setup phase.
Interestingly, the attack seems quite resilient to an incomplete
𝑋𝑝𝑢𝑏. The results are depicted in Figure 9 for 10 different attacks
carried out with 𝑋𝑝𝑢𝑏 stripped of a specific class. For each attack,
the average reconstitution error for the unknown classes (i.e., red