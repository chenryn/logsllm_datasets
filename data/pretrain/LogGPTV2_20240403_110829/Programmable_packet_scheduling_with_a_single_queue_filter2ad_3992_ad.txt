measure defined in Eq. (1), we have
lim
T →∞
Δ(T ) = 0.
Theorem 1 already provides a strong guarantee on the departure
rate of each rank. This theorem goes further to show the gap on
the difference of the dequeued packets. It proves that Δ defined in
Section 3 is close to 0, meaning that AIFO and PIFO dequeue the
same set of packets.
4.4 Data Plane Design and Implementation
We describe the data plane design to implement AIFO on a pro-
grammable switch. We emphasize that the algorithm of AIFO itself
is independent of the hardware architecture, and can be imple-
mented on programmable switch ASICs, FPGAs or network proces-
sors. The purpose here is to provide a concrete data plane design
and implementation to demonstrate the viability of AIFO. We im-
plement AIFO with 827 lines of code in P4. The implementation
can run on Barefoot Tofino at line rate. We describe the major
challenges and our solutions in our design and implementation.
Queue length estimation for the temporal component. The
main challenge for the temporal component is to maintain the dy-
C −c
namic threshold 1
C based on the queue length. The queue
1−k
length information is managed by a module called traffic manager
which sits between the ingress pipe and the egress pipe. The dif-
ficulty is that for commodity switches including Barefoot Tofino,
the queue length information can only be obtained when a packet
goes through the traffic manager, and thus can only be read at the
Programmable Packet Scheduling with a Single Queue
SIGCOMM ’21, August 23–28, 2021, Virtual Event, Netherlands
(cid:12)(cid:10)(cid:14)(cid:2)(cid:9)(cid:11)(cid:7)(cid:8)(cid:15)(cid:5)(cid:1)(cid:3)
(cid:12)(cid:10)(cid:14)(cid:2)(cid:13)(cid:6)(cid:11)(cid:10)(cid:5) (cid:4)
(cid:15)(cid:25)(cid:19)(cid:20)(cid:31)
(cid:17)(cid:18)(cid:21)(cid:21)(cid:20)(cid:28)
(cid:16)(cid:18)(cid:24)(cid:27)(cid:23)(cid:20)(cid:28)
(cid:21)(cid:17)(cid:12)(cid:11)(cid:20)(cid:13) (cid:19)(cid:1)(cid:10)(cid:1)(cid:7)
(cid:22)(cid:12)(cid:1)(cid:6)
(cid:28)(cid:12)(cid:1)(cid:11)
(cid:22)(cid:12)(cid:1)(cid:7)
(cid:28)(cid:12)(cid:1)(cid:4)
(cid:22)(cid:12)(cid:1)(cid:8)
(cid:28)(cid:12)(cid:1)(cid:3)(cid:7)
(cid:22)(cid:12)(cid:1)(cid:9)
(cid:28)(cid:12)(cid:1)(cid:8)
(cid:22)(cid:12)(cid:1)(cid:10)
(cid:28)(cid:12)(cid:1)(cid:3)
(cid:22)(cid:12)(cid:1)(cid:11)
(cid:28)(cid:12)(cid:1)(cid:5)
(cid:22)(cid:12)(cid:1)(cid:3)(cid:2)
(cid:28)(cid:12)(cid:1)(cid:4)
(cid:22)(cid:12)(cid:1)(cid:3)(cid:3)
(cid:28)(cid:12)(cid:1)(cid:10)
(cid:22)(cid:12)(cid:1)(cid:2)
(cid:28)(cid:12)(cid:1)(cid:4)
(cid:22)(cid:12)(cid:1)(cid:3)
(cid:28)(cid:12)(cid:1)(cid:9)
(cid:22)(cid:12)(cid:1)(cid:4)
(cid:28)(cid:12)(cid:1)(cid:10)
(cid:22)(cid:12)(cid:1)(cid:5)
(cid:28)(cid:12)(cid:1)(cid:11)
(cid:22)(cid:12)(cid:1)(cid:3)(cid:4)
(cid:28)(cid:12)(cid:1)(cid:9)
(cid:22)(cid:12)(cid:1)(cid:3)(cid:5)
(cid:28)(cid:12)(cid:1)(cid:4)(cid:10)
(cid:22)(cid:12)(cid:1)(cid:3)(cid:6)
(cid:28)(cid:12)(cid:1)(cid:6)
(cid:22)(cid:12)(cid:1)(cid:3)(cid:7)
(cid:28)(cid:12)(cid:1)(cid:4)(cid:7)
(cid:29)(cid:30)(cid:18)(cid:21)(cid:20) (cid:3)
(cid:29)(cid:30)(cid:18)(cid:21)(cid:20) (cid:4)
(cid:29)(cid:30)(cid:18)(cid:21)(cid:20) (cid:5)
(cid:29)(cid:30)(cid:18)(cid:21)(cid:20) (cid:6)
(cid:18)(cid:21)(cid:11)(cid:16)(cid:20)(cid:14)(cid:15)(cid:13)
(cid:10) (cid:8)(cid:4)(cid:5)(cid:8)
(cid:10) (cid:6)(cid:9)(cid:3)(cid:7)(cid:2)
(cid:13)(cid:19)(cid:24)(cid:22)(cid:29)(cid:29)(cid:22)(cid:26)(cid:25)
(cid:14)(cid:26)(cid:25)(cid:30)(cid:28)(cid:26)(cid:23)(cid:1)
Figure 6: Compute quantile with a sliding window.
egress pipe. However, AIFO requires the queue length to compute
the threshold at the ingress pipe in order to make admission control
decisions.
To address this challenge, we design a recirculation-based solu-
tion to bring the queue length information from the egress pipe
to the ingress pipe. Specifically, we use a register array to store
the queue length for each egress port at the egress pipe, denoted
by q_len_eдress. Packets can write the queue length value into
q_len_eдress after passing through the traffic manager. At the
same time, we have a copy of the register array at the ingress
pipe, denoted by q_len_inдress. We use a set of worker packets to
read the queue lengths from q_len_eдress at the egress pipe. The
worker packets are recirculated to enter the ingress pipe again
when they leave the egress pipe, and they update the queue lengths
in q_len_inдress using the values they read.
As the worker packets make the queue lengths ready in the
ingress pipe, a normal arriving packet can then access the queue
length information in the ingress pipe. After the routing decision
is made for the packet (i.e., the egress port is known), it can read
the queue length of its egress port from q_len_inдress. Then the
C −c
threshold 1
C can be calculated with the queue length to decide
1−k
whether to admit or drop the packet. If the packet is admitted, it
also writes the current queue length to the egress pipe. Figure 5
illustrates how the solution works.
Since the worker packets keep being recirculated all the time,
they only go through a designated recirculation port, and thus
would not contribute to the queue lengths of the egress ports. As-
suming it takes 200 ns for a worker packet to go through the pipeline
and be recirculated, for a port with 10Mpps rate, it would only cause
a bias of 2 packets, which is negligible. Also note that for switches
that support reading queue length directly in the ingress pipe (e.g.,
Barefoot Tofino 2), recirculation is not needed.
Quantile estimation for the spatial component. The spatial
component estimates the quantile of the rank of each arriving
packet. We use a set of stages to implement a sliding window to
store recent packets and estimate quantiles. Programmable switches
normally support accessing several registers per stage, e.g., m = 4
registers per stage. In order to support a sliding window with n
slots, we need n/m stages. We use m registers per stage over n/m
stages, and use n registers in total. The index of each register is
from 0 to n − 1, and it indicates the position of the packet in the
sliding window. The value of register i stores the rank of the packet
at position i in the sliding window. Figure 6 shows an example with
n = 16 and m = 4. We use 4 stages and use 4 registers per stage,
with a total of 16 registers. Each register stores the rank for a packet
in a sliding window of 16 recent packets.
We use an index tagger module to track the sliding window. The
index tagger module keeps a circular counter from 0 to n − 1. It
assigns its counter the index of an arriving packet (pkt .index), and
then increments its counter by one. The counter is reset to 0 when
it reaches n. The packet index indicates which register stores the
rank of the oldest packet in the sliding window, and thus should be
updated with the rank of the arriving packet. In Figure 6, pkt .index
is 4, and thus the value of the first register at stage 2 (i.e., the register
with i = 4) is updated with the rank of the arriving packet. The
index pkt .index will be set as 5 for the next packet and point to the
second register at stage 2 (following the dotted arrow).
At the same time, when a packet goes through each stage, the
switch also compares the rank of the packet with the value in each
register with an ALU. Each ALU outputs a result indicating whether
the packet rank is smaller than the register value: if the packet rank
is smaller, output = 1; otherwise, output = 0. By summing up

the output s of all ALUs together, we get the relative ranking of the
arriving packet in the sliding window: q =
i outputi . The quantile
of the arriving packet can be computed by dividing q by the length
of the window: W .quantile(pkt ) = q/n.
In Figure 6, the rank of the arriving packet is 5. The rank is
smaller than the values of 6 registers, which are marked with red
in the figure. As the size of the sliding window n is 16, the quantile
is 6/16 = 37.5%.
While our evaluation results show that a small sliding window
size (e.g., 20) is sufficient for many common scenarios, a large slid-
ing window is sometimes needed for certain workloads. However,
commodity switches normally provide only a few stages and a small
amount of memory. To efficiently use precious switch resources,
we use a sampling method to virtually scale up the sliding window
size by adding a sampler aside with the index tagger. For exam-
ple, instead of using a window with the size of 1000, we can use a
smaller window with the size of 20, and set the sampling rate as
0.02.
As both the queue length (c) and the quantile (
q
n ) are available,
we can make the admission control decision based on the condition
q
·q +c ≤
n ≤ 1
1−k
C. Since C, k, and n are constants,
·q can be easily calculated
in one stage with a math unit available in programmable switches.
C −c
C . This condition can be transformed to
C ·(1−k )
n
C ·(1−k )
n
5 EVALUATION
In this section, we provide experimental results to demonstrate the
performance of AIFO. We first evaluate AIFO using simulations
to show that AIFO can achieve high performance in a large-scale
datacenter environment. In the simulations, we benchmark AIFO
with state-of-the-art solutions to demonstrate its end-to-end perfor-
mance. Besides, we also evaluate the effect of different parameters,
and the admitted packet set of AIFO. At last, we evaluate our pro-
totype for AIFO on a Barefoot Tofino switch in a hardware testbed.
185
SIGCOMM ’21, August 23–28, 2021, Virtual Event, Netherlands
Zhuolong Yu, et al.
(a) Average FCT for small flows.
(b) 99th FCT for small flows.
(c) Avearage FCT for large flows.
Figure 7: Simulation results of web search workload to minimize FCT.
(a) Average FCT for small flows.
(b) 99th FCT for small flows.
(c) Average FCT for large flows.
Figure 8: The effect of parameter k.
(a) Average FCT for small flows.
(b) 99th FCT for small flows.
(c) Average FCT for large flows.
Figure 9: The effect of window length and sampling rate.
5.1 Packet-Level Simulations
We use packet-level simulations to evaluate AIFO in a large-scale
datacenter environment. We use a similar setting as recent works
on packet scheduling [3, 6]: a leaf-spine topology which contains 9
leaf switches, 4 spine switches and 144 servers, and the bandwidth
of the access and leaf-spine links is set at 10Gbps and 40Gbps,
respectively. The simulations are conducted with Netbench [1], a
packet-level simulator.
We evaluate two use cases of programmable packet schedul-
ing: minimizing FCT and providing fairness. (i) We use AIFO to
implement pFabric, and compare it with TCP, DCTCP as well as
state-of-the-art approaches PIFO [47], SP-PIFO [3], and PIEO [45]
under a realistic traffic workload: web search workload [6]. We also
conduct a sensitivity analysis to evaluate and analyze the effect of
different parameters (i.e., queue length, scaling parameter k, win-
dow length and sampling rate) on AIFO and the admitted packet
set of AIFO. (ii) We implement Start-Time Fair Queueing (STFQ)
on top of AIFO and compare it with other state-of-the-art solutions.
For AIFO, we set the target queue length as 20, k = 0.1, window
length as 20, and sampling rate as 1
15 by default.
Minimizing FCT with AIFO. We first show the performance of
AIFO when implementing SRPT for pFabric [6] to minimize FCT
under the web search workload. The traffic starts according to a
Poisson distribution. For comparison, we also implement SRPT
186
Programmable Packet Scheduling with a Single Queue
SIGCOMM ’21, August 23–28, 2021, Virtual Event, Netherlands
(a) Average FCT for small flows.
(b) Average FCT for large flows.
(a) FIFO.
(b) PIFO.
Figure 10: The effect of queue length on 1G/4G network.
(a) Average FCT for small flows.
(b) Average FCT for large flows.
Figure 11: The effect of queue length on 10G/40G network.
with SP-PIFO and PIFO, and compare them with TCP and DCTCP.
In addition, we consider PIEO, which is a more scalable design
for programmable packet scheduling compared with PIFO. We use
pFabric as the transport layer for AIFO, PIFO and SP-PIFO at the
hosts. Figure 7 shows the average FCT for small flows (Figure 7(a)),
the 99th percentile FCT for small flows (Figure 7(b)), and the average
FCT for large flows (Figure 7(c)). AIFO, PIFO, PIEO, and SP-PIFO
can achieve much lower FCT compared with TCP and DCTCP,
especially when the load is high. Among all these approaches, PIFO
and PIEO achieve the best performance as it enforces strict priority
with a PIFO queue. The performance of SP-PIFO is close to PIFO.