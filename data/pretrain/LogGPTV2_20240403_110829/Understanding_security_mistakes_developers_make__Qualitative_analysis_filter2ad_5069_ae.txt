observe any difference in vulnerabilities introduced between
MOOC and non-MOOC participants or participants with more
development experience. While this does not guarantee a lack
of effect, it is likely that increased development experience
and security training have, at most, a small impact.
5.2 Exploit Difﬁculty and Attacker control
To answer RQ2 and RQ3, we consider how the different vul-
nerability types differ from each other in difﬁculty to exploit,
as well as in the degree of attacker control they allow. We
distinguish three metrics of difﬁculty: our qualitative assess-
ment of the difﬁculty of ﬁnding the vulnerability (Discovery
Difﬁculty); our qualitative assessment of the difﬁculty of ex-
ploiting the vulnerability (Exploit Difﬁculty); and whether
7The mean Popularity score was 91.5. Therefore, C—whose Popularity
score of 92 was nearest to the mean—can be considered representative the
language of average popularity.
118    29th USENIX Security Symposium
USENIX Association
a competitor team actually found and exploited the vulnera-
bility (Actual Exploitation). Figure 1 shows the number of
vulnerabilities for each type with each bar divided by Ex-
ploit Difﬁculty, bars grouped by Discovery Difﬁculty, and the
left and right charts showing partial and full attacker control
vulnerabilities, respectively.
To compare these metrics across different vulnerability
types and sub-types, we primarily use the same set of planned
pairwise Chi-squared tests described in Section 5.1. When
necessary, we substitute Fisher’s Exact Test (FET), which is
more appropriate when some of the values being compared
are less than ﬁve [31]. For convenience of analysis, we binned
Discovery Difﬁculty into Easy (execution) and Hard (source,
deep insight). We similarly binned Exploit Difﬁculty into Easy
(single-step, few steps) and Hard (many steps, deterministic
or probabilistic).
Misunderstandings are rated as hard to ﬁnd. Identifying
Misunderstanding vulnerabilities often required the attacker
to determine the developer’s exact approach and have a good
understanding of the algorithms, data structures, or libraries
they used. As such, we rated Misunderstanding vulnerabil-
ities as hard to ﬁnd signiﬁcantly more often than both No
Implementation (φ = 0.52, p < 0.001) and Mistake (φ = 0.30,
p = 0.02) vulnerabilities.
Interestingly, we did not observe a signiﬁcant difference
in actual exploitation between the Misunderstanding and No
Implementation types. This suggests that even though Mis-
understanding vulnerabilities were rated as more difﬁcult to
ﬁnd, sufﬁcient code review can help close this gap in practice.
That being said, Misunderstandings were the least com-
monType to be actually exploited by Break It teams. Speciﬁ-
cally, using a weak algorithm (Not Exploited=3, Exploited=2),
using a ﬁxed value (Not Exploited=14, Exploited=12), and
using a homemade algorithm (Not Exploited=1, Exploited=1)
were actually exploited in at most half of all identiﬁed cases.
These vulnerabilities presented a mix of challenges, with some
rated as difﬁcult to ﬁnd and others difﬁcult to exploit. In the
homemade encryption case (SL-61), the vulnerability took
some time to ﬁnd, because the implementation code was difﬁ-
cult to read. However, once an attacker realizes that the team
has essentially reimplemented the Wired Equivalent Protocol
(WEP), a simple check of Wikipedia reveals the exploit. Con-
versely, seeing that a non-random IV was used for encryption
is easy, but successful exploitation of this ﬂaw can require
signiﬁcant time and effort.
No Implementations are rated as easy to ﬁnd. Unsurpris-
ingly, a majority of No Implementation vulnerabilities were
rated as easy to ﬁnd (V=42, 58% of No Implementations). For
example, in the SC problem, an auditor could simply check
whether encryption, an integrity check, and a nonce were
used. If not, then the project can be exploited. None of the All
Intuitive or Some Intuitive vulnerabilities were rated as difﬁ-
cult to exploit; however, 45% of Unintuitive vulnerabilities
were (V=22). The difference between Unintuitive and Some
Intuitive is signiﬁcant (φ = 0.38, p = 0.003), but (likely due
to sample size) the difference between Unintuitive and All
Intuitive is not (φ = 0.17, p = 0.17).
As an example, SL-7 did not use a MAC to detect modiﬁ-
cations to their encrypted ﬁles. This mistake is very simple to
identify, but it was not exploited by any of the BIBIFI teams.
The likely reason for this was that SL-7 stored the log data in
a JSON blob before encrypting. Therefore, any modiﬁcations
made to the encrypted text must maintain the JSON struc-
ture after decryption, or the exploit will fail. The attack could
require a large number of tests to ﬁnd a suitable modiﬁcation.
Mistakes are rated as easy to ﬁnd and exploit. We rated
all Mistakes as easy to exploit. This is signiﬁcantly different
from both No Implementation (φ = 0.43, p = 0.001) and Mis-
understanding (φ = 0.51, p < 0.001) vulnerabilities, which
were rated as easy to exploit less frequently. Similarly, Mis-
takes were actually exploited during the Break It phase signif-
icantly more often than either Misunderstanding (φ = 0.35,
p = 0.001) or No Implementation (φ = 0.28, p = 0.006). In
fact, only one Mistake (0.03%) was not actually exploited
by any Break It team. These results suggest that although
Mistakes were least common, any that do ﬁnd their way into
production code are likely to be found and exploited. For-
tunately, our results also suggest that code review may be
sufﬁcient to ﬁnd many of these vulnerabilities. (We note that
this assumes that the source is available, which may not be
the case when a developer relies on third-party software.)
No signiﬁcant difference in attacker control. We ﬁnd no
signiﬁcant differences between types or sub-types in the inci-
dence of full and partial attacker control. This result is likely
partially due to the fact that partial attacker control vulnerabil-
ities still have practically important consequences. Because of
this fact, our BIBIFI did not distinguish between attacker con-
trol levels when awarding points; i.e., partial attacker control
vulnerabilities received as many points as full attacker con-
trol. The effect of more nuanced scoring could be investigated
in future work. We do observe a trend that Misunderstand-
ing vulnerabilities exhibited full attacker control more often
(V=50, 70% of Misunderstandings) than No Implementation
and Mistake (V=44, 61% and V=20, 51%, respectively); this
trend speciﬁcally could be further investigated in future stud-
ies focusing on attacker control.
6 Discussion and Recommendations
Our results are consistent with real-world observations, add
weight to existing recommendations, and suggest prioritiza-
tions of possible solutions.
Our vulnerabilities compared to real-world vulnerabili-
ties. While we compiled our list of vulnerabilities by explor-
ing BIBIFI projects, we ﬁnd that our list closely resembles
USENIX Association
29th USENIX Security Symposium    119
Figure 1: # vulnerabilities introduced for each type divided by Discovery Difﬁculty, Exploit Difﬁculty and Attacker Control.
both Mitre’s CWE and OWASP’s Top Ten [55,61] lists. Over-
lapping vulnerabilities include: broken authentication (e.g.,
insufﬁcient randomness), broken access control, security mis-
conﬁguration (e.g., using an algorithm incorrectly or with
the wrong default values), and sensitive data exposure (e.g.
side-channel leak).
Get the help of a security expert. In some large organi-
zations, developers working with cryptography and other
security-speciﬁc features might be required to use security-
expert determine tools and patterns to use or have a security
expert perform a review. Our results reafﬁrm this practice,
when possible, as participants were most likely to struggle
with security concepts avoidable through expert review.
API design. Our results support the basic idea that secu-
rity controls are best applied transparently, e.g., using simple
APIs [35]. However, while many teams used APIs that pro-
vide security (e.g., encryption) transparently, they were still
frequently misused (e.g., failing to initialize using a unique IV
or failing to employ stream-based operation to avoid replay
attacks). It may be beneﬁcial to organize solutions around
general use cases, so that developers only need to know the
use case and not the security requirements.
API documentation. API usage problems could be a matter
of documentation, as suggested by prior work [2, 57]. For
example, teams SC-18 and SC-19 used TLS socket libraries
but did not enable client-side authentication, as needed by
the problem. This failure appears to have occurred because
client-side authentication is disabled by default, but this fact
is not mentioned in the documentation.8 Defaults within an
API should be safe and without ambiguity [35]. As another
example, SL-22 (Listing 2) disabled the automatic integrity
checks of the SQLCipher library. Their commit message
stated “Improve performance by disabling per-page MAC
protection.” We know that this change was made to improve
performance, but it is possible they assumed they were only
disabling the “per-page” integrity check while a full database
check remained. The documentation is unclear about this.9
Security education. Even the best documented APIs are use-
less when teams fail to apply security at all, as we observed
frequently. A lack of education is an easy scapegoat, but we
note that many of the teams in our data had completed a cy-
bersecurity MOOC prior to the competition. We reviewed
lecture slides and found that all needed security controls for
the BIBIFI problems were discussed. While only three teams
failed to include All Intuitive requirements (5% of MOOC
teams), a majority of teams failed to include Unintuitive re-
quirements (P=33, 55% of MOOC teams). It could be that the
topics were not driven home in a sufﬁciently meaningful man-
ner. An environment like BIBIFI, where developers practice
implementing security concepts and receive feedback regard-
ing mistakes, could help. Future work should consider how
well competitors from one contest do in follow-on contests.
Vulnerability analysis tools. There is signiﬁcant interest in
automating security vulnerability discovery (or preventing
vulnerability introduction) through the use of code analysis
tools. Such tools may have found some of the vulnerabili-
ties we examined in our study. For example, static analyses
like SpotBugs/Findbugs [6,40], Infer [14], and FlowDroid [7];
symbolic executors like KLEE [13] and angr [71]; fuzz testers
like AFL [81] or libfuzzer [70]; and dynamic analyses like
libdft [43] and TaintDroid [27] could have uncovered vulner-
abilities relating to memory corruption, improper parameter
use (like a ﬁxed IV [23]), and missing error checks. However,
they would not have applied to the majority of vulnerabili-
ties we saw, which are often design-level, conceptual issues.
An interesting question is how automation could be used to
address security requirements at design time.
Determining security expertise. Our results indicate that
8https://golang.org/pkg/crypto/tls/#Listen
and
//www.openssl.org/docs/manmaster/man3/SSL_new.html
https:
9https://www.zetetic.net/sqlcipher/sqlcipher-api/
#cipher_use_MAC
120    29th USENIX Security Symposium
USENIX Association
the reason teams most often did not implement security was
due to a lack of knowledge. However, neither years of devel-
opment experience nor whether security training had been
completed had a signiﬁcant effect on whether any of the vul-
nerability types were introduced. This ﬁnding is consistent
with prior research [60] and suggests the need for a new mea-
sure of security experience. Previous work by Votipka et al.
contrasting vulnerability discovery experts (hackers) and non-
experts (software testers) suggested the main factor behind
their difference in experience was the variety of different
vulnerabilities they discovered or observed (e.g., read about
or had described to them) [79]. Therefore, a metric for vul-
nerability experience based on the types of vulnerabilities
observed previously may have been a better predictor for the
types of vulnerabilities teams introduced.
7 Related Work
The original BIBIFI paper [66] explored how different quan-
titative factors inﬂuenced the performance and security of
contest submissions. This paper complements that analysis
with in-depth, qualitative examination of the introduced vul-
nerabilities in a substantial sample of BIBIFI submissions
(including a new programming problem, multiuser database).
The BIBIFI contest affords analysis of many attempts at the
same problem in a context with far more ecological validity
than a controlled lab study. This nicely complements prior
work examining patterns in the introduction and identiﬁcation
of vulnerabilities in many contexts. We review and compare
to some of this prior work here.
Measuring metadata in production code. Several re-
searchers have used metadata from revision-control systems
to examine vulnerability introduction. In two papers, Meneely
et al. investigated metadata from PHP and the Apache HTTP
server [50, 52]. They found that vulnerabilities are associated
with higher-than-average code churn, committing authors who
are new to the codebase, and editing others’ code rather than
one’s own. Follow-up work investigating Chromium found
that source code reviewed by more developers was more likely
to contain a vulnerability, unless reviewed by someone who
had participated in a prior vulnerability-ﬁxing review [51].
Signiﬁcantly earlier, Sliwerski et al. explored mechanisms
for identifying bug-ﬁx commits in the Eclipse CVS archives,
ﬁnding, e.g., that ﬁx-inducing changes typically span more
ﬁles than other commits [73]. Perl et al. used metadata from
Github and CVEs to train a classiﬁer to identify commits that
might contain vulnerabilities [62].
Other researchers have investigated trends in CVEs and the
National Vulnerability Database (NVD). Christey et al. ex-
amining CVEs from 2001–2006, found noticeable differences
in the types of vulnerabilities reported for open- and closed-
source operating-system advisories [20]. As a continuation,
Chang et al. explored CVEs and the NVD from 2007–2010,
showing that the percentage of high-attacker control vulner-
abilities decreased over time, but that more than 80% of all
examined vulnerabilities were exploitable via network ac-
cess without authentication [19]. We complement this work
by examining a smaller set of vulnerabilities in more depth.
While these works focus on metadata about code commits
and vulnerability reports, we instead examine the code itself.
Measuring cryptography problems in production code.
Lazar et al. discovered that only 17% of cryptography vul-
nerabilities in the CVE database were caused by bugs in
cryptographic libraries, while 83% were caused by developer
misuse of the libraries [46]. This accords with our Conceptual
Error results. Egele et al. developed an analyzer to recognize
speciﬁc cryptographic errors and found that nearly 88% of
Google Play applications using cryptographic APIs make at
least one of these mistakes [26]. Kruger et al. performed a sim-
ilar analysis of Android apps and found 95% made at least one
misuse of a cryptographic API [45]. Other researchers used
fuzzing and static analysis to identify problems with SSL/TLS
implementations in libraries and in Android apps [28,33]. Fo-
cusing on one particular application of cryptography, Reaves
et al. uncovered serious vulnerabilities in mobile banking
applications related to homemade cryptography, certiﬁcate
validation, and information leakage [64]. These works exam-
ine speciﬁc types of vulnerabilities across many real-world
programs; our contest data allows us to similarly investigate
patterns of errors made when addressing similar tasks, but ex-
plore more types of vulnerabilities. Additionally, because all
teams are building to the same requirement speciﬁcation, we
limit confounding factors inherent in the review of disparate
code bases.
Controlled experiments with developers. In contrast to
production-code measurements, other researchers have ex-
plored security phenomena through controlled experiments
with small, security-focused programming tasks. Oliveira et al.
studied developer misuse of cryptographic APIs via Java “puz-
zles” involving APIs with known misuse cases and found that
neither cognitive function nor expertise correlated with ability
to avoid security problems [60]. Other researchers have found,
in the contexts of cryptography and secure password storage,
that while simple APIs do provide security beneﬁts, simplicity
is not enough to solve the problems of poor documentation,
missing examples, missing features, and insufﬁcient abstrac-
tions [2, 56–58]. Perhaps closest to our work, Finifter et al.
compared different teams’ attempts to build a secure web
application using different tools and frameworks [29]. They
found no relationship between programming language and
application security, but that automated security mechanisms
were effective in preventing vulnerabilities.
Other studies have experimentally investigated how effec-
tive developers are at looking for vulnerabilities. Edmundson
et al. conducted an experiment in manual code review: no
participant found all three previously conﬁrmed vulnerabili-
USENIX Association
29th USENIX Security Symposium    121
ties, and more experience was not necessarily correlated with
more accuracy in code review [25]. Other work suggested that
users found more vulnerabilities faster with static analysis
than with black-box penetration testing [69].
We further substantiate many of these ﬁndings in a different
experimental context: larger programming tasks in which
functionality and performance were prioritized along with
security, allowing increased ecological validity while still
maintaining some quasi-experimental controls.
8 Conclusion
Secure software development is challenging, with many pro-
posed remediations and improvements. To know which inter-
ventions are likely to have the most impact requires under-
standing which security errors programmers tend to make,
and why. To this end, we presented a systematic, qualitative