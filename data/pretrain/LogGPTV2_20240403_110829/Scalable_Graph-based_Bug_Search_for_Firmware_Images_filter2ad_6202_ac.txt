4.3 Online Search
The encoded features generated in Section 4.2 may be directly
used in search. However, this straightforward solution may not be
scalable for millions of functions in real-world applications. This
section introduces a scalable solution by LSH (Locality-sensitive
hashing) to scale the search. In this paper, we utilize LSH as op-
posed to other indexing methods such as k-d tree, as the k-d tree
may not be suitable for our problem due to its inefﬁciency in high-
dimensional spaces especially when the codebook is large [57].
Given a query function, we ﬁrst derive its encoded feature by
Eq. (3) and (4), then we are interested in ﬁnding the function in
(cid:1005)(cid:1006)(cid:1007)(cid:1009)(cid:1008)(cid:1010)(cid:1011)(cid:1005)(cid:1006)(cid:1007)(cid:1012)(cid:1010)(cid:1011)(cid:38)(cid:1005)(cid:890)(cid:454)(cid:1012)(cid:1010)(cid:38)(cid:1005)(cid:890)(cid:373)(cid:349)(cid:393)(cid:400)(cid:38)(cid:1006)(cid:890)(cid:373)(cid:349)(cid:393)(cid:400)(cid:1012)(cid:1007)(cid:1005)(cid:1013)(cid:1011)(cid:1010)Codebook(cid:1005)(cid:1006)(cid:1007)(cid:1010)(cid:1011)(cid:1005)(cid:1012)(cid:1011)(cid:1013)(cid:1005)(cid:1012)(cid:1007)(cid:1013)(cid:1011)MCS(F1_x86, F1_mips) = 0.71 (5/7)MCS(F1_mips, F2_mips) = 0.83 (5/6)FunctionF1_x86   (2/7, 5/7, 1/7)F1_mips (2/6, 5/6, 1/6) (5/6, 4/6, 4/6)F2_mipsCosine Encoding Vector(cid:1089)(cid:3)(cid:1005)(cid:856)(cid:1004)(cid:1089)(cid:3)(cid:1004)(cid:856)(cid:1012)(cid:1006)Euclidean(cid:1089)(cid:3)(cid:1004)(cid:856)(cid:1012)(cid:1011)(cid:1004)(cid:1089)(cid:3)(cid:1004)(cid:856)(cid:1006)(cid:1011)(cid:1007)c. MCS match resultd. Fabulous match resulta. Raw ACFGs b. Codebook for Genius484a large dataset that are closest to the query with a high probabil-
ity. LSH achieves this goal by learning a projection so that if two
points are closer together in the feature encoding space, they should
remain close after the projection in the hashing space. Follow-
ing [54], given the encoded feature q(g), we employ the projection
functions hi deﬁned as:
hi(q(g)) = (cid:98)(v · q(g) + b)/w(cid:99),
(5)
where w is the number of quantized bin, v is a randomly selected
vector from a Gaussian distribution, and b is a random variable
sampled from a uniform distribution between 0 and w. In addi-
tion, (cid:98)·(cid:99) is he ﬂoor operator. Essentially, a hashing function deﬁnes
a hyper-plane to project the input encoded features. For any func-
tions q(gi), q(gj) ∈ Rn that are close to each other in the encoding
space, there is a high probability P [h(q(gi)) = h(q(gj))] = p1
that they fall into the same bucket. Likewise, for any functions that
are far apart, there is a low probability p2(p2 To
MIPS → DD-WRT
MIPS → ReadyNAS
x86 → DD-WRT
x86 → ReadyNAS
Query Normalized Avg. Time
Multi-MHTLS [45] Multi-k-MH [45]
TLS
1:2
1:2
70:78
1:2
TLS DTLS
1:2
1:2
5:33
1:2
DTLS
2:4
6:16
1:2
1:2
0.3s
1:2
1:4
1:2
1:2
1 s
discovRE [23]
TLS DTLS
1:2
1:2
1:2
1:2
4.1 × 10−4 s
1:2
1:2
1:2
1:2
Genius
TLS DTLS
1:2
1:2
1:2
1:2
1:2
1:2
1:2
1:2
1.8 × 10−6 s
TLS
46:100
88:190
97:255
145:238
Centroid [18]
DTLS