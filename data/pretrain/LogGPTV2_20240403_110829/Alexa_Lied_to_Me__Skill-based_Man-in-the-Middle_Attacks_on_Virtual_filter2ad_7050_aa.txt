title:Alexa Lied to Me: Skill-based Man-in-the-Middle Attacks on Virtual
Assistants
author:Richard Mitev and
Markus Miettinen and
Ahmad-Reza Sadeghi
Alexa Lied to Me: Skill-based Man-in-the-Middle Attacks on
Virtual Assistants
Richard Mitev
PI:EMAIL
Technische Universität Darmstadt
Germany
Markus Miettinen
markus.miettinen@
trust.tu-darmstadt.de
Technische Universität Darmstadt
Germany
Ahmad-Reza Sadeghi
PI:EMAIL
Technische Universität Darmstadt
Germany
ABSTRACT
Voice-based virtual personal assistants such as Amazon’s Alexa
or Google Assistant have become highly popular and are used for
diverse daily tasks ranging from querying on-line information,
shopping, smart home control and a variety of enterprise applica-
tion scenarios1. Capabilities of virtual assistants can be enhanced
with so-called Skills, i.e., programmatic extensions that allow third-
party providers to integrate their services with the respective voice
assistant.
In this paper, we show that specially crafted malicious Skills can
use the seemingly limited Skill interaction model to cause harm. We
present novel man-in-the-middle attacks against benign Skills and
Virtual Assistant functionalities. Our attack uses loopholes in the
Skill interface to redirect a victim’s voice input to a malicious Skill,
thereby hijacking the conversation between Alexa and the victim.
To the best of our knowledge this is the first man-in-the-middle
attack targeting the Skill ecosystem. We present the design of our
attack and demonstrate its feasibility based on a proof-of-concept
implementation attacking the Alexa Skills of a smart lock as well
as a home security system.
ACM Reference Format:
Richard Mitev, Markus Miettinen, and Ahmad-Reza Sadeghi. 2019. Alexa
Lied to Me: Skill-based Man-in-the-Middle Attacks on Virtual Assistants. In
ACM Asia Conference on Computer and Communications Security (AsiaCCS
’19), July 9–12, 2019, Auckland, New Zealand. ACM, New York, NY, USA,
14 pages. https://doi.org/10.1145/3321705.3329842
1 INTRODUCTION
Personal voice-controlled virtual assistants are getting more and
more popular and ubiquitous. There are numerous virtual assistants
available on the market from different vendors like, e.g., Amazon
Alexa, Google Assistant, Apple Siri and Microsoft Cortana. Virtual
assistants can be integrated into other programs, built into OSs or
into IoT devices like smart speakers, smart watches, appliances,
cars or even clothing.
1https://aws.amazon.com/alexaforbusiness/
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
AsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand
© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6752-3/19/07...$15.00
https://doi.org/10.1145/3321705.3329842
Recently, multiple new attacks against devices utilizing voice
control-based UIs have emerged, mainly focused on issuing unau-
thorized commands without the legitimate user noticing this. The
attacks typically utilize synthesized [1, 6], obfuscated [3, 21], embed-
ded [16, 24] or ultrasound-modulated voice commands [13, 19, 25].
Besides sound, also other attack vectors exist, e.g., using electro-
magnetic interference [10] or attacks leveraging the headphone
jack of a device [23]. There are also attacks exploiting the way we
pronounce words to invoke a similar sounding Skill [12, 26].
What makes the voice-control interface of virtual assistants par-
ticularly interesting is that it can be augmented with various third-
party service applications, called ’Skills’, or ’Actions’ that provide
extended functionality beyond the basic functions of the virtual
assistant, like, e.g., the possibility to control smart home appliances,
or, access to specific information services. It is obvious that by mis-
using such ’Skills’ an adversary could potentially cause much harm
to the user. Therefore the interaction model of Skills is very limited:
Skills can only be used to retrieve information from a third-party
service or invoke actions explicitly exposed by the service.
Our goals and contributions. In this paper, we show that care-
fully crafted malicious back-end Skill functionality can be com-
bined with known inaudible attack techniques to circumvent the
seemingly limited Skill interaction model of a virtual assistant like
Amazon Alexa to allow an adversary to arbitrarily control and ma-
nipulate interactions between the user and other benign Skills. We
present a man-in-the-middle attack that completely hijacks a full
conversation between a voice-controlled virtual assistant and the
targeted user and is very hard to detect. The advantage of our ap-
proach using a malicious back-end Skill for the attack is that it
works in the context of an active user interaction with the virtual
assistant, completely maintaining the interaction semantics from the
user’s perspective. This allows the adversary to launch much more
powerful attacks than simple command injection as presented in
earlier work [3, 14, 16, 19, 21, 24, 25].
In our attack the adversary can arbitrarily modify the virtual
assistant’s responses, or, can entirely replace its functionality by
her own, returning any responses of her choice to the victim that
seem to be coming from genuine personal virtual assistant. Further-
more, our attack can make the responses seem plausible even to a
suspicious and particularly vigilant user by maliciously modifying
genuine, plausible data provided by benign Skills on-the-fly. With
this Proof-of-Concept we show that designers of voice controlled
applications in Skill ecosystems must take inaudible injection and
jamming attacks into account.
To implement our attack we tackle a number of technical chal-
lenges: (1) How to capture user commands and redirect them to
Session 6A: IoT SecurityAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand465a malicious Skill so that the original intention of the user is over-
ridden? (2) How to exfiltrate genuine information from legitimate
Skills so that it can be used to form fabricated responses to the
user that seem plausible? (3) Finally, how to orchestrate the tech-
nical components required by the attack seamlessly together to
create the illusion that the user is conversing with the legitimate
Alexa functionality or Skills, while it in reality is interacting with a
malicious Skill.
Contributions Our main contributions are as follows:
• We present Lyexa, the (to the best of our knowledge) first
man-in-the-middle attack against personal virtual assistants
(Sect. 3).
• The implementation of a malicious Skill framework that can
convincingly impersonate the behavior of virtual assistants
and benign Skills associated with it, while simultaneously
being able to modify this behavior as desired by the adversary
(Sect. 4).
• A Proof-of-concept evaluation of the Lyexa attack framework
in different rooms and environments, on two smart home
security systems and the 10 top Skills of the U.S. Skill store
controlled by an Amazon Echo Dot virtual assistant device
(Sect. 5).
In this paper, we focus on the most popular virtual assistant, i.e.,
Amazon Alexa. However, we stress that our attack can be modified
to cover also other similar virtual assistants from other vendors in
a similar fashion. The implementation of the attack components is
described in Sect. 4, and suggestions for countermeasures against
this attack are given in Sect. 7.
2 PRELIMINARIES
The most popular voice-controlled virtual assistant utilising a smart
speaker device placed in users’ homes is Alexa2, short for Alexa
Voice Service (AVS), from Amazon, with a market share of ca. 70%
in US households in 2018. [11]Typically Alexa is integrated into a
smart speaker device like the Amazon Echo or its smaller variant
Amazon Echo Dot. A similar set-up applies also to Google’s compa-
rable service, the Google Assistant. These smart speaker devices are
equipped with microphones and loudspeakers in order to facilitate
voice-based interaction with the user of the device. As shown in
Fig. 1 users issue voice commands to their virtual assistants by
uttering a so called wake word like “Alexa”, or, “Ok, Google”. When
the corresponding smart speaker recognizes the wake word, it starts
recording audio to capture any subsequent commands given by
the user (1). The recorded audio is forwarded to the corresponding
back-end service like Amazon Voice Service or Google Assistant for
speech-to-text translation (2). Depending on the issued commands,
the back-end service will look up information or initiate actions (3)
and respond with a reply (4) that the smart speaker will play back
to the user (5).
In addition to this basic usage scenario, virtual assistant function-
ality can also be built into dedicated third-party devices. Examples
2https://www.amazon.com/Amazon-Echo-And-Alexa-Devices/b?node=9818047011
Figure 1: User interaction flow while using a Skill
include BMW cars3, where Alexa is able to control the air condi-
tioning system of the car, or, LG ThinQ Smart fridges and ovens4
where Alexa can for instance be used to preheat the oven.
2.1 Skills and Actions
The functionality of Alexa can be extended by so-called Skills5
(called Actions6 in Google Assistant). These are third-party ser-
vice plug-ins that extend the virtual assistant’s functionality and
allow it to interact with third-party services, generating an entire
ecosystem around AVS. Any developer (private or organization)
can develop Skills free of charge. Examples of third-party Skills in-
clude, e.g., ordering pizza through Domino’s Pizza Skill or calling a
Uber ride through Uber’s Skill, to name a few. Utilizing appropriate
Skills, Alexa can also be used for controlling user’s smart home IoT
devices given that the manufacturers of these devices provide the
appropriate Skill for controlling them through Alexa.
In contrast to apps on popular smartphone platforms like An-
droid or iOS, which are essentially pieces of software running on the
host smartphone, Skills are different. They don’t contain executable
code that could be run on devices and they can’t thus change the
behavior of the Alexa-enabled device itself. Skills are merely third-
party-provided service extensions to AVS and can only react to
invocations, i.e. telling Alexa what to echo to the user in response
to specific requests to the Skill. The Lyexa attack framework uti-
lizes a specially-crafted malicious Skill to make Alexa speak to the
user what the adversary wants, mimicking the behavior of genuine
Skills or Alexa functionalities. The adversary can thereby exploit
the Alexa ecosystem for different attacks without ever having to
compromise the Alexa-enabled device itself.
Skills need to be registered in Amazon’s Skill repository, from
where users can activate them to be used. The activation of Skills
can happen through the Alexa app, webpage, or, using a voice
command. To be listed in the repository each Skill has to pass a
certification process in which Amazon tests the Skill to verify that
it works as specified. However, as the Skill is running on a remote
3https://www.gearbrain.com/which-cars-have-amazon-alexa-2525958778.html
4https://www.cnet.com/news/lg-instaview-thinq-alexa-fridge-clever-kitchen-tricks-
ces-2018/
5https://developer.amazon.com/alexa-skills-kit
6https://developers.google.com/actions/
User gives acommandAlexa recognizesthe used Skillname and sendsthe request to theSkill's backendThe backendprocesses therequest andreturns a textualresponseAlexa convertsthe response toaudio (TTS) andstreams it to thedeviceThe user hearsthe response withAlexa's voice12354Amazon VoiceServiceUserThird PartySession 6A: IoT SecurityAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand466server, AVS can’t control whether a Skill is modified after it has
passed the certification.
2.2 Inaudible Speech Injection
Recent attacks against virtual assistants by Song et al. [19] and
Zhang et al. [25] utilize the non-linearity of microphones beyond
the audible frequency spectrum to issue commands to a voice-
controlled virtual assistant that can’t be heard by the user. This is
achieved by exploiting physical characteristics of the microphones
typically used in such devices. By carefully injecting selected sig-
nals at high frequencies that lie beyond the hearing capacity of
humans, it is possible to create ’shadow’ signals in the microphone
within the audible frequency spectrum [14] and use this for issuing
inaudible commands. These works assume a malicious device in
proximity to the targeted Voice Assistant injecting the inaudible
audio signal. We think this to be a realistic assumption and adopt
the same adversarial scenario, assuming that the attacker is able
to inject signals into the victim’s audio environment. To show that
Skills can be used for attacks we realise the Lyexa attack where
we will take use of this inaudible command injection technique
together with inaudible jamming of user commands to redirect the
user interaction from the Skill the user intended to the adversary’s
malicious Skill as described below.
3 OUR ATTACK LYEXA
Our goal with the Lyexa attack is to show that even though the
functionality and interaction model of Skills is very limited, it is
possible to craft Skills with malicious functionality that can be used
to construct harmful attacks against users by utilizing compromised
IoT devices in the vicinity of the Alexa device for realizing the
attack. The Lyexa attack has been designed under the assumption
that the Alexa devices and the AVS ecosystem have no exploitable
vulnerabilities and that the traffic is properly secured (e.g., TLS,
no ARP spoofing). We think this to be a realistic assumption since
Amazon spends considerable resources in designing and testing
their devices and would also have the necessary means for quickly
updating or patching their devices in case security vulnerabilities
affecting them were to be found. With this attack we want to show
how the AVS ecosystem can be exploited using Skills for unintended
purposes without compromising individual components as such.
However, as recent numerous reports about IoT devices with
security vulnerabilities suggest, many IoT devices in the smart
homes of users can be exploited relatively easily by automated
security attacks like those performed by IoT malware like Mirai [2],
Hajime [7] or Persirai [22]. Especially devices like IP cameras seem
to be often affected by such attacks, as many of them can be easily
exploited due to insecure security configurations like easy-to-guess
administrator passwords.
3.1 Attack Overview
We developed and verified the Lyexa attack on Amazon’s AVS
ecosystem, as it is currently the most popular voice-controlled vir-
tual assistant platform for smart speakers. [11] However, as virtual
assistants are conceptually similar, our attack can likely be extended
also to other virtual assistants. A conceptual overview of the Lyexa
attack scenario is shown in Fig. 2. It involves a smart home user U ,
a voice-controlled virtual assistant device E like the Amazon Echo
connected to the Alexa Voice Service (AVS), a malicious IoT device
D and a malicious Skill S in the AVS ecosystem. The adversary is
a remote attacker controlling both malicious components S and
D and able to take use of a speech-to-text service or library STT,
many of which are readily available.
Figure 2: Overview of the Lyexa attack, dotted lines repre-
sent a “can control” or “has access to” relation, solid lines
show that objects have a connection
3.1.1 Assumptions. The malicious IoT device D is located in the
vicinity of E (e.g., on the same living room table) and is equipped
with a microphone and a loudspeaker capable of emitting ultra-
sound signals. We will show in Sect. 5.2.1 that numerous different
kinds of entry-level IoT devices like IP cameras are equipped with
hardware that are likely to satisfy these requirements.
Device D is accompanied by a malicious Skill S under the control
of the adversary. This Skill has to be activated on the victim’s AVS
account. Note that since it is possible to enable Skills by voice7, it
is possible that device D could use inaudible speech injection as
described in Sect. 2.2 to activate Skill S by itself.
3.2 Attack Components
Our attacks consists of four distinct components, as depicted in
Fig. 3 and discussed below.
3.2.1 Command jamming. Figure 3(a) shows how commands is-
sued by user U are inaudibly jammed and simultaneously recorded
by malicious device D. When U speaks the wake word ("Alexa")
both the benign Alexa-enabled device E and malicious device D
are activated (1). Benign device E starts listening for subsequent
commands and D starts jamming this command with ultrasound
modulated noise, which is inaudible for humans as evaluated by
Roy et al. [14] (2). This way the command issued by U can’t be
understood by E. D simultaneously records the command.
3.2.2 Malicious Skill invocation. The second attack component is
depicted in Fig. 3(b). When user U finishes speaking the command
(1), D immediately stops jamming and inaudibly injects a Skill
invocation command with malicious Skill S’s invocation name to