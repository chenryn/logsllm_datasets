title:Port Contention for Fun and Profit
author:Alejandro Cabrera Aldaya and
Billy Bob Brumley and
Sohaib ul Hassan and
Cesar Pereida Garc&apos;ıa and
Nicola Tuveri
(cid:19)(cid:17)(cid:18)(cid:26)(cid:1)(cid:42)(cid:38)(cid:38)(cid:38)(cid:1)(cid:52)(cid:90)(cid:78)(cid:81)(cid:80)(cid:84)(cid:74)(cid:86)(cid:78)(cid:1)(cid:80)(cid:79)(cid:1)(cid:52)(cid:70)(cid:68)(cid:86)(cid:83)(cid:74)(cid:85)(cid:90)(cid:1)(cid:66)(cid:79)(cid:69)(cid:1)(cid:49)(cid:83)(cid:74)(cid:87)(cid:66)(cid:68)(cid:90)
Port Contention for Fun and Proﬁt
Alejandro Cabrera Aldaya
∗
†
Tampere University, Tampere, Finland
†
, Billy Bob Brumley
∗
Universidad Tecnológica de la Habana (CUJAE), Habana, Cuba
, Sohaib ul Hassan
†
†
, Cesar Pereida García
†
, Nicola Tuveri
Abstract—Simultaneous Multithreading (SMT) architectures
are attractive targets for side-channel enabled attackers, with
their inherently broader attack surface that exposes more per
physical core microarchitecture components than cross-core at-
tacks. In this work, we explore SMT execution engine sharing
as a side-channel leakage source. We target ports to stacks of
execution units to create a high-resolution timing side-channel
due to port contention,
inherently stealthy since it does not
depend on the memory subsystem like other cache or TLB
based attacks. Implementing our channel on Intel Skylake and
Kaby Lake architectures featuring Hyper-Threading, we mount
an end-to-end attack that recovers a P-384 private key from
an OpenSSL-powered TLS server using a small number of
repeated TLS handshake attempts. Furthermore, we show that
traces targeting shared libraries, static builds, and SGX enclaves
are essentially identical, hence our channel has wide target
application.
I. INTRODUCTION
Microarchitecture side-channel attacks increasingly gain
traction due to the real threat they pose to general-purpose
computer infrastructure. New techniques emerge every year [1,
2], and they tend to involve lower level hardware, they get
more complex but simpler to implement, and more difﬁcult
to mitigate,
thus making microarchitecture attacks a more
viable attack option. Many of the current microarchitecture
side-channel techniques rely on the persistent state property
of shared hardware resources, e.g., caches, TLBs, and BTBs,
but non-persistent shared resources can also lead to side-
channels [3], allowing leakage of conﬁdential
information
from a trusted to a malicious process.
The microprocessor architecture is complex and the effect
of a component in the rest of the system can be difﬁcult (if not
impossible) to track accurately: especially when components
are shared by multiple processes during execution. Previous
research [4, 5] conﬁrms that as long as (persistent and non-
persistent) shared hardware resources exist, attackers will be
able to leak conﬁdential information from a system.
In this work, we present a side-channel attack vector exploit-
ing an inherent component of modern processors using Intel
Hyper-Threading technology. Our new side-channel technique
PORTSMASH is capable of exploiting timing information de-
rived from port contention to the execution units, thus targeting
a non-persistent shared hardware resource. Our technique can
choose among several conﬁgurations to target different ports
in order to adapt to different scenarios, thus offering a very
ﬁne spatial granularity. Additionally, PORTSMASH is highly
portable and its prerequisites for execution are minimal, i.e.,
does not require knowledge of memory cache-lines, eviction
sets, machine learning techniques, nor reverse engineering
techniques.
To demonstrate PORTSMASH in action, we present a com-
plete end-to-end attack in a real-world setting attacking the
NIST P-384 curve during signature generation in a TLS server
compiled against OpenSSL 1.1.0h for crypto functionality.
Our Spy program measures the port contention delay while
executing in parallel to ECDSA P-384 signature generation,
creating a timing signal trace containing a noisy sequence of
add and double operations during scalar multiplication. We
then process the signal using various techniques to clean the
signal and reduce errors in the information extracted from each
trace. We then pass this partial key information to a recovery
phase, creating lattice problem instances which ultimately
yield the TLS server’s ECDSA private key.
We extend our analysis to SGX, showing it is possible to
retrieve secret keys from SGX enclaves by an unprivileged
attacker. We compare our PORTSMASH technique to other
side-channel
techniques in terms of spatial resolution and
detectability. Finally, we comment on the impact of current
mitigations proposed for other side-channels on PORTSMASH,
and our recommendations to protect against it.
In summary, we offer a full treatment of our new technique:
from microarchitecture and side-channel background (Sec-
tion II); to the nature of port contention leakage when placed
in an existing covert channel framework (Section III); to its
construction as a versatile timing side-channel (Section IV);
to its application in real-world settings, recovering a private
key (Section V);
to discussing (lack of) detectability and
mitigations (Section VI). We conclude in Section VII.
II. BACKGROUND
A. Microarchitecture
This section describes some of Intel’s microarchitectural
components and how they behave with Intel SMT implemen-
tation (i.e., Hyper-Threading technology). Intel launched its
SMT implementation with the Pentium 4 MMX processor [6].
Hyper-Threading technology (HT) aims at providing paral-
lelism without duplicating all microarchitectural components
in a physical processor. Instead, a processor supporting Hyper-
Threading has at least two logical cores per physical core
where some components are shared between the logical ones.
Figure 1 shows a high-level description of the layout of an
Intel i7 processor [7]. This ﬁgure shows four physical cores,
each with two logical cores. In this setting, the OS sees a
processor with eight cores.
(cid:165)(cid:1)(cid:19)(cid:17)(cid:18)(cid:26)(cid:13)(cid:1)(cid:34)(cid:77)(cid:70)(cid:75)(cid:66)(cid:79)(cid:69)(cid:83)(cid:80)(cid:1)(cid:36)(cid:66)(cid:67)(cid:83)(cid:70)(cid:83)(cid:66)(cid:1)(cid:34)(cid:77)(cid:69)(cid:66)(cid:90)(cid:66)(cid:15)(cid:1)(cid:54)(cid:79)(cid:69)(cid:70)(cid:83)(cid:1)(cid:77)(cid:74)(cid:68)(cid:70)(cid:79)(cid:84)(cid:70)(cid:1)(cid:85)(cid:80)(cid:1)(cid:42)(cid:38)(cid:38)(cid:38)(cid:15)
(cid:37)(cid:48)(cid:42)(cid:1)(cid:18)(cid:17)(cid:15)(cid:18)(cid:18)(cid:17)(cid:26)(cid:16)(cid:52)(cid:49)(cid:15)(cid:19)(cid:17)(cid:18)(cid:26)(cid:15)(cid:17)(cid:17)(cid:17)(cid:23)(cid:23)
(cid:25)(cid:24)(cid:17)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:44:40 UTC from IEEE Xplore.  Restrictions apply. 
Logical 
Core 
Logical 
Core 
Logical 
Core 
Logical 
Core 
Logical 
Core 
Logical 
Core 
Logical 
Core 
Logical 
Core 
L1 and L2
L1 and L2
L1 and L2
L1 and L2
Execution Engine
Execution Engine
Execution Engine
Execution Engine
Last Level Cache (LLC)
Fig. 1.
Intel i7 Core processor.
inst.
fetch
Thread 
0 
Decode
uOps
uOps
Scheduler
inst.
fetch
Thread 
1 
Port 0
Port 1
Port 5
Port 6
Port 2
Port 3
Port 4
Port 7
uOps
uOps
uOps
uOps
uOps
uOps
INT ALU
INT DIV
VEC ALU
VEC MUL
AES
VEC STR
FP DIV
BRANCH
INT ALU
BRANCH
AGU
LOAD
AGU
LOAD
STORE
AGU
INT ALU
INT MUL
VEC ALU
VEC MUL
BIT SCAN
INT ALU
VEC SHU
VEC ALU
LEA
Execution Engine
Memory Subsystem
Fig. 2. Skylake/Kaby Lake microarchitecture.
Figure 1 sketches some microarchitectural components with
a sharing perspective. L1 and L2 caches are shared between a
pair of logical cores in the same physical core. The next level
depicts how an Execution Engine (EE) is also shared between
two logical cores. This component is very important for this
paper as the presented microarchitectural side-channel relies
on this logical-core-shared feature. On the other hand, the last
level cache (LLC) is shared between all cores.
Generally speaking, the EE is responsible for executing
is closely related to the pipeline
instructions therefore it
concept [7, 8]. A simpliﬁed pipeline model consists of three
phases: (1) fetch, (2) decode, and (3) execute. While these
phases have complex internal working details, Figure 2 pro-
vides a high-level abstraction focusing mainly on the EE part,
and its description below also follows the same approach. For
more information about its inner working details we defer
to [6–8].
Each logical core has its own registers ﬁle, and the pipeline
fetches instructions from memory according to the program
counter on each of them. For the sake of processing perfor-
mance fairness, this fetching is interleaved between the logical
cores. After the fetch stage, a decoding phase decomposes each
instruction into simpler micro-operations (uops). Each micro-
operation does a single task, therefore this splitting helps out-
of-order execution by interleaving their executions for the sake
of performance. After this point, all processing is done on uops
instead of instructions. The decoding phase then issues these
uops to the execution scheduler.
At the scheduler there is a queue of uops that belongs to
both logical cores. One task of the scheduler is issuing these
uops to the Execution Ports while maximizing performance.
An Execution Port is a channel to the execution units, the
latter being where uops are actually executed. Figure 2 shows
execution units as gray-colored boxes with labels indicating
their functionality. For example, ports 0, 1, 5, and 6 can be
used to execute simple arithmetic instructions, because each
of them is a channel to an ALU execution unit. While ports 2,
3, 4, and 7 are dedicated to memory-based uops (e.g., loads
and stores).
As an illustrative example of how the whole process
happens in this simpliﬁed model, let us consider the adc
mem, reg instruction (AT&T syntax), which adds (with
carry) the value at memory location mem into the content in
register reg. According to Fog’s instruction table for Skylake
microarchitecture [9], this instruction splits into two uops:
one arithmetic uop (that actually performs the addition) and
another for loading a value from memory. The former can
be issued to ports 0 or 6, while the latter to port 2 and
3 [9]. However, if we change the operand order in the original
instruction (i.e., now the addition result is stored back in the
memory location mem), the new instruction splits into three
uops: two are essentially the same as before and another is
issued for storing the result back to memory (i.e., an operation
handled by port 4).
This execution sequence behaves exactly the same in the
presence of Hyper-Threading. At the scheduler, there are uops
waiting for dispatch to some port for execution. These uops
could actually belong to instructions fetched from any logical
core, therefore, these cores share the EE in a very granular
approach (at uops level).
B. SMT: Timing Attacks
Timing attacks on microprocessors featuring SMT technol-
ogy have a long and storied history with respect to side-
channel analysis. Since the revival of SMT in 1995 [10],
it was noted that contention was imminent, particularly in
the memory subsystem. Arguably, timing attacks became a
more serious security threat once Intel introduced its Hyper-
Threading technology on the Pentium 4 microarchitecture.
Researchers knew that resource sharing leads to resource
contention, and it took a remarkably short time to notice
that contention introduces timing variations during execution,
which can be used as a covert channel, and as a side-channel.
In his pioneering work, Percival [11] described a novel
cache-timing attack against RSA’s Sliding Window Expo-
nentiation (SWE) implemented in OpenSSL 0.9.7c. The at-
tack exploits the microprocessor’s Hyper-Threading feature
and after observing that
threads “share more than merely
the execution units”, the author creates a spy process that
exﬁltrates information from the L1 data cache. The L1 data
cache attack correctly identiﬁes accesses to the precomputed
multipliers used during the SWE algorithm, leading to RSA
private key recovery. As a countermeasure, to ensure uniform
access to the cache lines, irrespective of the multiplier used,
(cid:25)(cid:24)(cid:18)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:44:40 UTC from IEEE Xplore.  Restrictions apply. 
the OpenSSL team included a “constant-time” Fixed Window
Exponentiation (FWE) algorithm paired with a scatter-gather
method to mask table access [12].
Cache-based channels are not the only shared resource to
receive security attention. Wang and Lee [3] and Acıiçmez
and Seifert [13] analyzed integer multiplication unit contention
in old Intel Pentium 4 processors with SMT support [6]. In
said microarchitecture, the integer multiplication unit is shared
between the two logical cores. Therefore contention could
exist between two concurrent processes running in the same
physical core if they issue integer multiplication instructions.
Wang and Lee [3] explore its application as a covert channel,
while Acıiçmez and Seifert [13] expand the side-channel
attack approach.
Acıiçmez and Seifert [13] stated this side-channel attack is
very speciﬁc to the targeted Intel Pentium 4 architecture due
to the fact that said architecture only has one integer multiplier
unit. They illustrated an attack against the SWE algorithm in
OpenSSL 0.9.8e. For this purpose they developed a proof-of-
concept, modifying OpenSSL source code to enhance the dis-
tinguishability between square and multiplication operations
in the captured trace. In addition to integer multiplication unit
sharing, their attack relies on the fact that square and mul-
tiplication operations have different latencies, an unnecessary
assumption in our work.
In a 2016 blog post1, Anders Fogh introduced Covert Shot-
gun, an automated framework to ﬁnd SMT covert channels.
The strategy is to enumerate all possible pairs of instructions
in an ISA. For each pair, duplicate each instruction a small
number of times, then run each block in parallel on the same
physical core but separate logical cores, measuring the clock-