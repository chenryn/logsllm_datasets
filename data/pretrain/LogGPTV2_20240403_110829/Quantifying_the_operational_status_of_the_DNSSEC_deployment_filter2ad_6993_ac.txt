record(s). From the perspective of the parent zone and re-
solvers, the authentication chain points to the previous key
but all signatures have been produced by the new key. This
scenario arises due to the diﬃculty in coordinating opera-
tional practices (key rollovers) across administrative bound-
aries. Based on the anticipation of this particular scenario
we track it as follows.
Let |RDS| denote the total number of unique DS records
| denote the number of veriﬁed
seen by pollers and let |RDS
DS records that match key-sets in the corresponding child
zone. The ratio between these values reﬂects the validity of
authentication chains, or delegation validity:
v
Vdeleg =
|RDS
|
|RDS|
v
Ideally, every delegation record would verify a key-set of
a corresponding child and Vdeleg → 1. Lack of veriﬁcation
indicates that: the child has removed a key-set too quickly
and has broken the authentication chain, or that the parent
has been slow in removing on obsolete delegation record,
or the parent has added a new delegation record before the
child was ready. Thus, a ratio value of less than one indicates
that there are zones that have broken delegations leading to
their child zones.
False Positives: In addition to the conﬁguration errors
described above, False Positives (lower left box in Table 2)
can also occur. To illustrate this we draw an analogy from
BGP and then show a DNSSEC-speciﬁc example.
In BGP there is an infrequent occurrence of routing leak-
outs [11]. In these cases, an Autonomous System (AS) acci-
dentally announce routes to peers that it really can’t reach.
Here the routers have sent routing announcement data (and
often use MD5 checksums to make it veriﬁable), but the data
is false. Even though the MD5 sums on these data streams
are veriﬁable, the data is not valid. In DNSSEC, an attacker
that has compromised a zone’s private key can generate and
sign records that appear to come from the zone. These
records are invalid (e.g. a record may contain the wrong
IP address of a web server [18]). However, these records
will still pass cryptographic veriﬁcation checks. This type of
compromise and other security breaches are hopefully rare,
but administrative errors are inevitable in large scale de-
ployments.
We show evidence in Section 4 that operational practices
combined with lack of revocation in the DNSSEC design al-
low a weak form of false positives to occur where an attacker
can replay stale RRsets long after the RRsets have been re-
moved from the zone’s authoritative servers (and have been
ﬂushed from caches).
An RRset is stale if an administrator has changed data
values in new sets, but a signature covering the previous
values has not expired. In this case, the stale set could be
replayed by an attacker or malfunctioning cache. Figure 2
illustrates this scenario. At time t0, RRset 1 is created and
signed. The signature includes an expiration date of time
t2 (indicated by the bottom bar). At time t1, RRset 1’s
value is modiﬁed. For example, the IP address of a host
may have changed. The modiﬁed RRset is distributed to all
authoritative servers and the previous value is ﬂushed from
caches after the TTL expires. However, an attacker can
Figure 2: If data changes, such as in “Modiﬁed RRset 1”,
then the old RRset 1 will still be veriﬁed by the zone’s keys
(even though the data is no longer valid).
continue to replay the old record until the signature expires
at time t2. Resolvers that receive the stale (blue) RRset will
verify the signatures and declare that the set is valid.
Our monitoring system is able to automatically detect
stale RRsets by tracking zones over time. We let Rv de-
note the set of RRsets whose signatures have not expired
and are still veriﬁable. Thus, Rv includes sets that are cur-
rently served by a zone and older sets whose signatures have
not expired yet. We denote the number of RRsets in Rv
as |Rv|. We now deﬁne Rstale such that Rstale ∈ Rv and
the sets in Rstale have diﬀerent values than those currently
being served (as seen in Figure 2). We say that |Rstale| is
the number of RRsets in Rstale. We pay close attention to
these sets because they could be replayed by an attacker.
The ratio of these two values yields the proportion of stale
data that is observed, or the data freshness:
Vf resh = 1 − |Rstale|
|Rv|
A value of 1 indicates that no obsolete RRsets could be
replayed while a value less than 1 indicates that a fraction of
veriﬁable RRsets could be replayed and would allow invalid
data to be veriﬁed.
Note that in the case of a stale RRset, the attacker is only
replaying data that was previously valid. In many cases, this
type of vulnerability will raise little or no concern. However,
one problematic scenario occurs when an attacker has com-
promised a zone’s private key and the zone attempts an un-
planned key rollover. At such a time, an attacker can replay
the stale key-set in order to verify (but not validate) the
compromised key. Using the compromised key, the attacker
can then forge arbitrary data from the zone. Some authen-
tication chains have lifetimes of weeks, months, and in some
cases years. Thus, key compromises combined with stale
RRset-replays pose serious challenges. A complete discus-
sion of the vulnerabilities and possible mitigations is beyond
the scope of this paper, but can be found in [16, 8].
We, thus, characterize the validity metric (V d) of DNSSEC
as an n-dimensional tuple of measurable validity metrics.
Other types of validity dimensions are plausible and worth
investigation, but in this work we use our experience to iden-
tify 2 operationally relevant dimensions to characterize:
V d = hVdeleg, Vf reshi
Our selection of these 2 dimensions is based on obser-
vational evidence that they are existing problems and that
there is also awareness of them in the operational commu-
nity.
RRset 1Signature Lifetimet0t1t2Modiﬁed RRset 1Datet3Figure 3: Polling locations
4. DEPLOYMENT STATUS TODAY
DNSSEC deployment data was collected using the Sec-
Spider monitoring project [3]. The revised DNSSEC RFCs
[4, 6, 5] were published in March 2005, and our monitoring
project began shortly afterward in October 2005. The mon-
itoring project uses a collection of pollers that send DNS
queries to the authoritative name servers of zones and use
the DNS responses to form the raw data for this study. The
polling locations are located in the United States, Europe,
and Asia, and on networks comprised of universities, home
access, and enterprises (Figure 3). This monitoring system
uses a central server to control the pollers and schedule when
queries should be sent and where to send them. All pollers
are scheduled to execute the same queries at approximately
the same time from their individual vantage points. In or-
der to discover new zones, the monitoring system uses zone
transfers (when possible) and exploits DNSSEC behaviors
such as using the NSEC record to “walk” (e.g. deﬁnitively
identify and retrieve all records in) a zone. The overall re-
sults provide a detailed history of each secure zone as viewed
from the system’s poller locations.
The dataset discussed covers October 2005 through Jan-
uary 2008 and includes 11,849 secure zones. However, while
this set of secure zones includes many well established DNS
zones such as the se ccTLD, it also includes other secure
zones that are clearly deployed only for testing. An exam-
ple is:
unknownalgorithm.nods.test.jelte.nlnetlabs.nl
In this case, the actual name of the zone indicates that it is
used for testing, and other zones in this same delegation (un-
der test.jelte.nlnetlabs.nl) account for over 81% of all
secure zones. To focus on how DNSSEC deployment is pro-
ceeding in “production” zones, our analysis began by pruning
zones that appeared to be operating in a test-capacity. In
order to classify zones as production we started by includ-
ing all secure TLDs and all secure zones under the arpa
TLD as production zones. Next, we added zones in other
parts of the DNS tree that pointed to an active web server
or mail server as production. Thus, all zones considered in
the study are zones that perform actions that suggest their
operational status is important and taken seriously by op-
erators. Though it can be argued that this test may have
missed some legitimate zones, and may have included some
test zones, it served as an automated way to identify rea-
sonable candidates for measurements. The list of production
zones is also posted on the project website and announced
on DNSSEC deployment mailing lists. Zone administrators
can use a web interface to change a zone’s status from testing
Figure 4: The rank order of secure zones and their corre-
sponding availability dispersion in ascending order.
to production or vice versa. This pruning process reduced
the set of secure zones that were considered in this study
from 11,849 to 871 secure “production” zones.
4.1 Availability
Using the metric described in Section 3.1, we begin our
analysis with data as viewed from our polling locations. At
regular intervals, all pollers query all secure zones. The
polling system described herein is designed to mimic a gen-
eralized DNS resolver, with only minor diﬀerences. For in-
stance, the pollers will each issue up to three queries with
timeout thresholds set to a conservative 10 seconds2.
We set Amax(zi, t) = 1 if at least one poller could receive
a response from zone zi at time t. Our results found that
Amax(zi, t) = 1 in 99.925% of our experiments. Out of the
871 zones in our study, only 44 zones ever encountered an
instance where Amax(zi, t) = 0. Because our preprocessing
eliminated zones created purely for testing purposes, we the-
orized that the reliability of the remaining zones would be
high due to the fact that they run production services and
their outages would not be signiﬁcant. Our results appear
to conﬁrm this. However, even though these zones are only
considered when they were reachable, diﬀerent pollers can
have very diﬀerent views of zone availability. Speciﬁcally,
when requesting zone data, resolvers in some locations re-
ceive no answer, while others (at the same time, but from
diﬀerent locations) have no diﬃculty obtaining a response.
The availability dispersion metric described in the previous
section captures this diﬀerence between pollers. Figure 4
shows that roughly 20% of the monitored zones suﬀer avail-
ability dispersion. This means that some resolvers may not
be able to receive critical data from a zone based solely on
where they query from.
The reason why this dispersion exists was traced to Path
Maximum Transmission Unit (PMTU [12]) problems. Recall
each link along the path from poller to authoritative server
has a Maximum Transmission Unit (MTU) that is the largest
packet size it can support. The PMTU is the smallest MTU
along a path. DNSSEC response messages can include pub-
lic keys (DNSKEYs) and signatures (RRSIGS) which make
2One popular DNS tool (DiG) uses 3 retries with a default
timeout of 5 seconds.
 0 0.2 0.4 0.6 0.8 1 0 100 200 300 400 500 600 700 800 900Availability DispersionDNSSEC ZonesAvailability Dispersion of DNSSEC ZonesAvailability Dispersionthem considerably larger than a typical DNS response. As
these responses travel along a path from the authoritative
server to the resolver, these larger UDP packets may exceed
the path’s maximum transmission unit (PMTU). As a result
packets may be fragmented or dropped. In one speciﬁc case,
the IP layer had fragmented the DNSSEC response mes-
sages and a local ﬁrewall was conﬁgured to disallow frag-
mented DNS packets. As result, the poller never received
the responses.
To better understand the impact of response packet size,
we modiﬁed our pollers to send queries with varying maxi-
mum response sizes. A DNSSEC query speciﬁes a maximum
response size that the resolver can support. The recom-
mended maximum response size is 4096 bytes and is set by
default. If the query received no response, our pollers used
a binary search to ﬁnd the smallest maximum response size
that would elicit a response. We call the process of sending
queries with varying maximum response size PMTU explo-
ration. During a PMTU exploration, problems manifested
themselves in one of two ways: either zone data was received
with a truncation bit (TC) set3, or the message was com-
pletely dropped (causing pollers to timeout). When data
was received, the PMTU exploration was characterized as,
“successful,” otherwise it was considered to have “failed.”
Figure 5 shows that while most pollers were able to re-
trieve zone data without encountering PMTU failures, poller
#2 consistently had more trouble than the others. Figure 5
also indicates that in certain cases, the DNSKEY RRset size
may reduce availability to the point that data is unavail-
able (via UDP) even after PMTU explorations. Figure 6
shows that certain pollers have signiﬁcantly more trouble
successfully getting data when a PMTU problem has been
encountered. One can note that poller #2 attempts more
than 7 times the number of PMTU explorations of any other
poller, and that over 20% of the PMTU problems result in
data that could not be retrieved via UDP (no matter what
size packet is speciﬁed).
A small set of zones suﬀer uniform PMTU exploration
problems across all pollers. We conjecture that the link with
the MTU problem happens to be close to their source (per-
haps the ﬁrst hop).
It is important to note that modern DNS and DNSSEC
resolvers are encouraged by RFCs to initially request data
using UDP. If a failure (i.e. no response) occurs resolvers
will generally give up. Thus, a PMTU failure may not even
prompt a resolver to try TCP. However, if a TC bit is re-
ceived, resolvers may try smaller message sizes (PMTU ex-
ploration) or retry their query using TCP. Our results indi-
cate that TCP is a reliable fall-back mechanism. However,
we also note various opinions in the operational commu-
nity decry TCP for DNS [1] and some locations may disal-
low TCP queries and/or the TCP query behavior may raise
other problems.
Compounding the PMTU problem faced by DNSSEC is
the fact that some network-infrastructure components as-
sume DNS traﬃc will not diverge from a very vanilla speci-
ﬁcation. Components such as ﬁrewalls may detect DNSSEC
messages as malformed DNS messages and drop them, fear-
ing that they may be dangerous. Examples of ﬁrewall com-
patibility issues include Cisco’s PIX ﬁrewall before version
6.0. In addition, [2] is a broad examination of several types of
3The TC bit indicates that the server wants to send more
data but it won’t ﬁt in the existing message.
Figure 5: This ﬁgure is a CDF of the distribution of how of-
ten zones need to be PMTU walked (broken down by pollers).
One can see, for example, that from poller #3 roughly 95%
of the zones need to be PMTU walked about 1.5% of the time
or less. Of note is that poller #2 more walks than the other
pollers.
Figure 6: This Figure uses cases in which pollers suﬀered
PMTU failures. The ﬁgure shows the percent of times that
pollers were able to successfully get data by adjusting the
message-size of the request from an authoritative zone (ﬁnd-
ing a size that ﬁts), as opposed to cases in which zones data
cannot ﬁt through the Internet to a poller.
home ﬁrewall/routers that found a signiﬁcant proportion of
these devices simply failed to process DNSSEC properly and
led some zone operators to discontinue the use of DNSSEC.
4.2 Veriﬁcation
DNSSEC envisioned a top-down deployment where au-
thentication chains would lead from the DNS root to most,
if not all, zones.
In stark contrast to this vision, the se-
cure hierarchy of DNSSEC today is quite fragmented. Of
the 871 secure zones in our study, fully 662 of these have
no authentication chain leading to them. Ideally, a resolver
would only need to conﬁgure one single trust anchor (corre-
sponding to the DNS root) but today a resolver would need
to manually conﬁgure 662 trust anchors in order to verify
 0 20 40 60 80 100 0 20 40 60 80 100% of Production ZonesPercent of Requests Needing PMTU Explorations per ZoneCDF of PMTU Explorations per ZonePoller 0Poller 1Poller 2Poller 3 80 85 90 95 100 0 1 2 3 4 5 6 7 0 20 40 60 80 100 0 1 2 3 4 5 0 1000 2000 3000 4000 5000 6000 7000% Success of PMTU ExplorationsNumber of PMTU Walks AttemptedPoller IDSuccess Rate of PMTU ExplorationPMTU Attempts% SuccessFigure 7: This Figure is a CDF of the percent of secure
zones that are reachable from a secure delegation or a trust
anchor as one adds zones’ key-sets to a trust-anchor list.
Figure 8: This Figure shows the rank order of the largest
observed Islands of security. The bars indicate that many of
the zones in these islands are served by the same nameservers.
all existing signed DNSSEC data. Today, manually conﬁg-