100
450
0
200
500
0
0
0
30
45
0
13
0
50
70
0
30
5,000
0
0
1,000
500
5,000
0
0
1,000
500
80,000
0
300
500
0
30
Table 1: Number of team members, and of accounts and de-
vices claimed by the 18 interview participants.
recruited online [13, 17]. Our study also provides concrete numbers
and extends the existing literature by adding that (1) ASO workers
can be hybrid (e.g., both organic and sockpuppet masters) and (2)
product developers can hire multiple types of expert ASO workers
to promote their products.
Participants claimed to charge between $0.5 and up to $6 per
posted review (M = 2.16, SD = 1.86), and to have between 1 and
6 years of experience in ASO jobs (M = 3.03, SD = 1.53). During
this time, they claimed to have worked on between 150 and 4,000
apps in total, and between 6 and 50–60 apps in the past month
(M = 34.11, SD = 18.37). They also declared a diverse educational
background, including 2 masters degrees, 11 completed bachelor
degrees, 2 ongoing bachelors, and 4 high school graduates.
5.2 Fraud Capabilities and Expertise
The middle columns of Table 1 list the number of user accounts
claimed to be controlled by or accessible to each of the 18 partici-
pants. Most participants control a few hundred accounts, however,
a few control or have access to several thousands: P13 claimed to
be part of a team of 13 workers who control 80,000 accounts.
7 participants, each claiming to control thousands of accounts,
also claimed to be able to write an “unlimited” number of reviews
for a single app, i.e., more reviews than the developer can ask or
a(cid:29)ord (as inferred from the participant’s past experience). The other
11 participants, with up to 3,000 accounts, claimed to be able to
write a number of reviews that was consistent (i.e., smaller or equal)
to the number of accounts they previously claimed to control.
To provide perspective on several of these claims, Figure 4 shows
the number of accounts revealed, and the number of unique apps
reviewed from those accounts, by each of the participants in our
quantitative study (§ 4.2). In total, we have crawled information
from 1,164 accounts and the 6,362 unique apps that were reviewed
from these accounts. Even in this limited gold standard dataset, one
participant (F18) was able to reveal 83 accounts that he controls,
and F35 has reviewed 927 unique apps from his 42 accounts.
Figure 4: Number of accounts revealed by F1,..,F39 and num-
ber of apps reviewed from them. F18 revealed 83 accounts. 14
workers have reviewed at least 150 apps from the revealed
accounts. F35 has reviewed 927 apps!
5.3 Hardware: Devices
All the interview participants claimed to own or have access to
multiple mobile devices. The last columns of Table 1 list the number
of devices, organized by types, claimed to be controlled or accessible
by each participant. 9 participants claimed to post fraud from mobile
devices; 11 participants claimed this also happens from the mobile
devices of organic ASO workers that they control. 2 participants
said that they also post from emulators running in laptops, e.g., P13
claims to have 13 laptops and use the BlueStacks emulator [5] to
install and review apps, and also 13 smartphones.
P8 and P18 have an almost 1-to-1 account-to-device mapping.
Participants such as P2, P3, P7, P10 and P15, have a small but many-
to-one mapping, e.g., up to 7 accounts per device. Others, such as
P1 and P13, claim to have signi(cid:27)cantly more accounts than devices
(e.g., 15,000:300 and 80,000:30 respectively).
Mobile device models. Several participants claim access to com-
munities of organic users (see Figure 2), thus to a diverse set of
devices. 4 participants (P1, P10, P11, P13) claimed to own only low-
end, cheap devices. Others (P7, P15, P16) claimed to own a mix of
low, medium and high-end devices, dominated by low-end devices.
For instance, P7, who claimed to own more than 1,000 devices said
that (1) “we try to choose cheap devices with more features and mem-
ory,” however (2) “we also have high-end phones like Nokia, Samsung,
which we need to review virtual/augmented reality apps”.
Device source. Most participants claimed to purchase their devices
on the regular market. However, P11 said, about his claimed 45
devices, that “I have bought them from the black market with a very
low price.” Further, as mentioned in § 5.1, P18 claimed to run a
mobile device repair shop, and use the devices he is supposed to
repair, to write reviews.
Device storage. 6 participants claimed to store the devices on a
table, easily accessible. P1 claimed to store the devices in a separate
room. P7 said that “the department who handle reviews and installs
is on a di(cid:29)erent (cid:30)oor, and high-end phones are kept in the locker
after use for safety.” We also asked P7 about how they manage to
charge 1,000 devices. He claimed that they have a dedicated team to
manage all the devices, and charge a device every 2–3 days. Further,
he claimed that they keep the devices on during o(cid:28)ce time, and
switch them o(cid:29) after 11pm–midnight.
App-device compatibility issues. When asked about what they
do when they need to promote an app that is not compatible with
their devices, 9 participants (P5, P6, P8, P10, P11, P13, P14, P16,
P17) said that it never happened. However, P7 said that he runs
Session 10D: Mobile SecurityCCS ’19, November 11–15, 2019, London, United Kingdom2441Figure 5: Scatter plot of device release price (EUR) vs. model
age (Days) at posting time, for each of 9,942 reviews posted
from 344 unique device types. Most devices are old and
low-end (45.98%) or mid-end (31.41%), or fresh and low-end
(15.31%). High-end and even free devices have been used!
campaigns to recruit ASO workers who own compatible devices,
or even purchase such devices. P9 and P15 said that they provide
as many reviews as they can from their compatible devices, and
contact the developer to explain the problem. P12 skips the job.
Quantitative Investigation. We used the technique described in
§ 4.2 to (cid:27)nd 344 unique device models, used to post 9,942 of the
21,767 reviews written from the accounts controlled by the 39 par-
ticipants. We found that 12 participants posted reviews from at
least 20 di(cid:29)erent device models; F35 used at least 84 distinct device
models. However, participants F9 (215 reviews), F10 (166), F14 (162),
F16 (67), F17 (459), and F27 (197) have posted reviews only from
devices of unknown models. We con(cid:27)rmed that the “unknown” de-
vice category includes reviews posted from Google Play’s website
interface and certain types of emulators.
Figure 5 shows the relationship between the device release price
(in Euros) and the device model age at posting time, for each of
9,942 presumed fake reviews posted from 344 unique device models.
We consider that a device is low, mid, or high-end, if its release price
is in the range [0, 260), [260, 450), and [450,∞) respectively [87].
We classify a device model age into Fresh ( 18 months). We found that 61.3% of
reviews were posted from low-end, 38.2% from mid-end, and 0.5%
from high-end devices, while 77.39% are from old and 19.66% from
new models. Further, most of these reviews were written from old
low-end devices (45.98%), old mid-end (31.41%) and fresh low-end
(15.31%) devices.
A notable case is that of tablets given away (price 0EUR, leftmost
points in Figure 5) by the Uruguayan government to students as
part of an inclusion plan named Plan Ceibal [15]. Participants F25
and F32 used this device model to write 159 reviews for 137 apps.
In addition, 3 reviews were posted from Galaxy S9+ devices whose
price exceeds 600EUR (rightmost points in Figure 5).
Figure 6 shows the per-worker distribution of the “age” of their
devices: the time di(cid:29)erence between the review date and the device
release date for all the fake reviews posted from known devices. 13
ASO workers have each posted at least 100 reviews from devices
that are over 6 months old. Additionally, F13, F24, F25, F32, F33,
and F35 have each posted at least 30 reviews from devices that are
less than 6 months old. We conclude that di(cid:29)erent workers rely on
stocks of either old devices, new devices or a mix of old and new,
to post fraud.
Figure 6: Per-worker distribution (violins) of the “age” of de-
vices used to post reviews, i.e., the time di(cid:29)erence in days
between the review date and the release date of its posting
device. Workers not shown had insu(cid:28)cient known device
models. F3, F7, F11, and F31 use old devices. Most others (F1,
F2, F13, F20, etc), use both newly released and old devices.
We found that 93.8% of the 9,942 fake reviews were posted from
smartphones and 6.2% from tablets. Figure 7(a) displays the num-
ber of unique device models used by ASO workers, including the
“unknown” category (i.e., not among the 21,597 o(cid:28)cially supported
device models provided by Google [10]). While F35 has used 85
unique device models, participants F9, F10, F14, F16, F17, and F27
have posted all their reviews from unknown devices. Figure 7(b)
shows the popularity of device models used by the 39 participants,
over all their 9,942 reviews posted from devices of “known” models.
The top 6 most used devices by ASO workers to post these reviews
are Galaxy Note 2 (836 reviews), Nexus 5 (742), Galaxy S4 (496),
S5 (447), S2 (247) and Nexus 7 (241). Further, Figure 7(c) shows the
popularity of the top 15 most popular devices, out of 11,934, that
were used to post 198,466,139 reviews in Google Play.
Summary. We found ASO workers who claim to have access to
large number of devices, either owned, or accessed through their
communities of organic fraud. This claim is partially con(cid:27)rmed
through our gold standard fraud data. Both in our interviews and in
the quantitative study, we found that ASO workers have a diverse
stock of low to high-end and new to old devices. Participants with
many devices reported streamlined solutions to manage them, while
those with fewer devices reported ways around cost limitations
and compatibility issues, e.g., further outsourcing jobs.
5.4 Software
Team formation. 10 interview participants (P3, P5, P6, P8, P9, P11,
P12, P14, P17, P18) said that they used Facebook and/or Whatsapp
to create online teams. For instance, P6 said that I have a Facebook
group of more than 500 people, from di(cid:29)erent locations in Bangladesh,
collected from various freelance groups in Facebook.” P9 hints at
eligibility criteria: “To build a team, we (cid:27)rst post message in Facebook
groups. Then we contact those who respond, personally, and talk to
them. We then decide if each is eligible, then we include him in our
Facebook group.” P17 claimed access to multiple groups, “We have
20 groups of real users in WhatsApp.”
Team communications. For communications, the above 10 par-
ticipants claimed to use the corresponding Facebook and Whatsapp
messenger app. P6 said “I post the app link in my Facebook group,
and ask them to download and post reviews.” P11 said “When I get a
job, I send them messages in WhatsApp or I reach them personally.”
Session 10D: Mobile SecurityCCS ’19, November 11–15, 2019, London, United Kingdom2442(a)
(b)
(c)
Figure 7: (a) Number of distinct devices per ASO worker (F1 .. F39) including unknown category. F9, F10, F14, F16, F17, and F27
have only unknown devices; F35 used at least 84 distinct device models. (b) Device model popularity for top 15 devices used
by ASO workers to post reviews. The 39 participants have used 344 distinct device models. (c) Device model popularity for top
15 devices in the wild. 11,934 unique device models were used to post over 198 million reviews in Google Play.
P7 however claimed to use specialized software: “We have our
own system where we push the apps. Users who use our system get the
noti(cid:27)cations about the new task and once they complete the task they
get paid. Due to the privacy policy, I can’t disclose the system name.”
Account maintenance. 5 interview participants (P1, P11, P13, P15,
P16) said they access their accounts regularly. 12 participants said
that they access them manually. However, 3 participants (P13, P15,
P16) said they use scripts and automatic login systems to periodi-
cally access their accounts, keep them alive, and report if any are
inaccessible. For instance, P13 said “We have built a system in Linux
where if we input 100 accounts, the system automatically logs into
those accounts, and keeps them alive.” The participants who organize
organic users said that organic users access their accounts regularly.
Job automation. P6 said that “We can post reviews, ratings and
installs using bots if the client has no problem. The bot names are like
QZ362, YNX32, or something like these.” All of the other participants
said that they write their reviews manually, and do not use any
script for this purpose.
5.5 Techniques: The Art of Evasion
Awareness of fraud detection. All interview participants are
aware of their fake reviews being detected and deleted. All of them
have reported that Google deleted some of their reviews. Although
most of them have reported deletion as a small or negligible percent-
age (under 5%) of all the reviews they posted, four of our interview
participants have said that 10–20% of their reviews were deleted.
P6 said that the review deletion percentage depends on the app
and ranges from 2% to 30%. Most participants said however that
it is very infrequent for their accounts to be deleted. P2 said that
“Sometimes the email might be disabled; in that case the review will
still be shown as written by a Google User.”
Perceived reasons for deletion. Participants reported diverse
reasons for deletion:
• Device re-use. P5 and P10 blame it on using the same device to
write multiple reviews for an app: “I always track the screenshot that
my workers provide as work proof. If I see two or more reviews from
one worker have been deleted, I am pretty sure that they have used