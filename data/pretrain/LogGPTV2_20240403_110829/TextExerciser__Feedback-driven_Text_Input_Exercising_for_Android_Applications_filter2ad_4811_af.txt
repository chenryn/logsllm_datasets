as the results if we run the Monkey/DroidBot alone. In other
words, TextExerciser does not bring any additional failed
apps to the Monkey or DroidBot. The details are that 5,060
apps run correctly without crashing in Android emulators, and
then additional 580 runs correctly in physical phones. We
manually look at these fail cases—the major reason is that
aapt does not extract the complete launcher information and
thus the Monkey/Droidbot cannot automatically start the apps
via Android ADB.
2) Syntax Rule Coverage: TextExerciser achieves
87.3% coverage when using the syntax rules to parse the
hint texts in §III-C. Here are the details. TextExerciser
extracts 328,282 text sentences from all the apps in the dataset.
After phase 1 with a learning model, TextExerciser ﬁnds
that 3,450 of them are input hints and the rest are general de-
scriptive sentences of user interface. Then, TextExerciser
successfully analyzes 3,012 of input hints, leading to a 87.3%
coverage. After that, TextExerciser generates 3,993 con-
straints and 5.7 lines of Z3StrSolver code on average for each
constraint.
Let us look at a successful example of utilizing syntax
rules in Table II to parse input hints. TextExerciser
ﬁnds an input hint (“Please choose a username that is at
least 3 characters to sign up and doesn’t contain special
characters”) in the app, rateME. Then, TextExerciser
splits this long hint into two short hints with the conjunction
word “and”. Next, the ﬁrst hint will be classiﬁed into C1
(“Length Constraints”), and the second hint C6 (“Input should
not contain certain characters”). TextExerciser parses C1
through the rule LowerBound and parses C6 through Exclusive
in Table II. Then, the parsed results will be passed to our input
generation engine, and be combined together to solve an input.
Next, we look at some of the failure cases in parsing the
hint text. First, some apps utilize unusual words or vague
words to illustrate hints. For example, ”Authentication failed”.
Second, some hint texts are embedded in popup ﬁgures. Since
TextExerciser does not apply an OCR [39], it cannot
identify texts in ﬁgures. Third, the popup window of some
apps disappear so quickly that TextExerciser does not
have enough time to obtain the texts.
Lastly, we show the category distribution for all the suc-
cessfully analyzed 3,012 hints in Figure 9. The distribution is
similar to the one of our training set of Android apps. C14
(“Non-directional constraints”) is the most popular category,
because most apps tend to provide such a constraint ﬁrst
together with other directional ones to warn the users. C1
(“The lower bound of input length”) and C15 (“Equivalence
Constraints”) come next, because passwords are widely used
in many Android apps during the login phase.
3) Code Coverage of This Large Dataset: We evaluate
the code coverage on this larger dataset in Figure 10. Let
us start from the comparison with Monkey. On average,
Monkey+TextExerciser triggers 24.0% activities in An-
droid apps as opposed to 19.0% activities triggered by the
Monkey with the default random text input generation. Fur-
thermore, the upper bound of triggered activities of Mon-
key+TextExerciser is 80%, larger than 67%, i.e., the
one of Monkey with a random strategy. We also compare
TextExerciser with predeﬁned strategy in DroidBot.
DroidBot+TextExerciser triggers 25% activities as op-
posed to 20% by DroidBot+Predeﬁned on average. The upper
bounds of triggered activity of DroidBot+TextExerciser
and DroidBot+Predeﬁned are both 100%, but the absolute
value of the upper bound in DroidBot+TextExerciser is
72 as opposed to 52 by DroidBot+Predeﬁned.
We further break the code coverage based on the con-
straint categories by randomly selecting 10 apps with a
certain constraint category and exercising these apps using
DroidBot+TextExerciser and DroidBot+Predeﬁned. Fig-
ure 8 shows the median (middle line), 25%–75% (bar), and
top/bottom of activity coverage broken down by 18 constraint
categories. TextExerciser is on par with or outperform
predeﬁned strategies in all the metrics across 18 constraint
categories. It is worth noting that all the apps have many hints
and corresponding categories and our evaluation ensures that
at least the target hint category is involved in the selected 10
apps.
4) Number of Trials in Generating Valid Text Inputs:
TextExerciser achieves about 95.1% success rate when
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:02:20 UTC from IEEE Xplore.  Restrictions apply. 
1081
BREAKDOWN OF REASONS THAT CAUSE FAILED INPUT GENERATION
Table VIII
AFTER 30 TRIALS.
Figure 8. The activity coverage of TextExerciser and DroidBot on 10
apps in each hint category.
Figure 9. The category distribution of successfully-interpreted hints by
TextExerciser. These categories, i.e., C1–18, are explained in Table I.
Figure 10. Activity coverage of Monkey (random and TextExerciser)
and DroidBot (predeﬁned and TextExerciser).
generating an input for a text ﬁeld for the ﬁrst time. Then,
in the second trial, TextExerciser takes more feedbacks
and achieves 96.7% success rate. In practice, most of the input
generations are ﬁnished in three rounds. The number of trials
is limited to 30 and only 1.2% of mutation will exceed this
limitation.
Let us ﬁrst see a concrete successful example, i.e., “Achieve-
ment - Rewards for Health”, which needs three trials in
generating a password. The hints for password generation
of this app show up step by step. Speciﬁcally, in the ﬁrst
trial, TextExerciser does not identify any input hint and
generate a random input “PA”. After entering this input, the
app prompts the ﬁrst hint—“Minimum of 8 characters”. Then,
Type
#Hints
Reason for exceeding 30 trials
1
2
3
Semantics-speciﬁc hints
1.088%
0.80‰ Hints with loose constraints
0.32‰ Misclassiﬁed hints
Total
1.2%
in the second trial, TextExerciser pares this hint and
generates an input “HPPABEQH” using the solver. This input
is still invalid and the app prompts another input hint—“Must
contain at least 1 lowercase, 1 uppercase, and 1 number”.
Therefore, in the third trial, TextExerciser adds three
more constraints to the solver code and generates an input
“1PA07aAO”, which ﬁnally satisﬁes the requirements of this
input ﬁeld.
We then break down the failed cases, i.e., 1.2% of all the
hints, based on their reasons and show them in Table VIII.
There are three major reasons: (i) semantics-speciﬁc hints,
(ii) hints with loose constraints, and (iii) misclassiﬁed hints.
First, some hints are semantics speciﬁc, which require certain
external knowledge to solve. Now, we illustrate an example,
called “DQ Texas”. The app’s sign-up page requires username,
password, and phone number, which can be exercised by
TextExerciser, but more importantly also contains a so-
called “invite code” ﬁeld. The hint provided by the app for a
random code is that “Please check your invite code and try
again.” A real user can either obtain the code from DQ store
or search one online; by contrast, TextExerciser cannot
generate one without knowing the semantic meaning.
Second, there are some hints that express a constraint that
is looser than what has been actually enforced. We believe
that these are unfriendly designs of user interfaces. Here is
one example, called “TeamLease”. Once someone inputs an
invalid phone number, the app shows a hint saying that “please
enter valid 10 digital”. However, the actual constraint enforced
by the app for the phone number ﬁeld is 10 digits plus a
country code with “+91”. TextExerciser cannot solve this
constraint without a proper hint.
Lastly, because TextExerciser adopts a machine learn-
ing model to classify hints, misclassiﬁcations are inevitable,
especially for these hints with formats that are not seen in
the training dataset. These misclassiﬁcations will also lead
to failed generation of inputs. Note that TextExerciser
may still be able to generate inputs if one hint is misclassiﬁed
because other hints may still help TextExerciser in the
generation.
5) Performance of Hint Classiﬁer:
In this section, we
evaluate the performance of our hint classiﬁer, an open-
source multi-class CNN-RNN model [27], trained from dataset
mentioned in §IV. We tried to adjust all the parameters, such
as batch size, the number of hidden units, and max pooling,
but observed that the default parameters provided by the tool
are still the best ones in our problem. Therefore, we adopted
their default setting as shown in Appendix B. Our evaluation
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:02:20 UTC from IEEE Xplore.  Restrictions apply. 
1082
results of this model against testing set show that the accuracy,
precision and recall are 90.2%, 89.4% and 90.2% respectively.
It is worth noting that false positives have less impact on
the performance of TextExerciser because even if a non-
hint is misclassiﬁed as a hint, TextExerciser will just add
more constraints but still generate valid inputs. False negatives
have a relatively higher impact but 90.2% recall is sufﬁcient
because even if one hint is misclassiﬁed, other hints may still
help TextExerciser to generate a valid input.
VI. DISCUSSION
We discuss some practical
issues in implementing and
deploying TextExerciser in real world.
Supported Language. Our prototype of TextExerciser
supports only English apps, but can be extended to apps
in other
languages. As a proof of concept, we extend
TextExerciser to 10 non-English apps using Google
Translate to convert the hints to English. Our evaluation shows
that TextExerciser solves 20 of 21 hints extracted from
these apps and then successfully generates inputs based on
these hints. Details of these apps can be found in Appendix A.
Apps with WebView. TextExerciser relies on UiAu-
tomator [30] to identify and capture WebView widgets and
corresponding embedded hints. We manually checked 150
popular apps excluding games, i.e., those mentioned in the
introduction, and the results show that 25 apps contain Web-
View widgets and 23 of these widgets are correctly captured
by UiAutomator and TextExerciser in exercising.
Server- vs. client-side validation. We perform a small-
scale, manual experiment to compare the number of server- vs.
client-side input validations. Our methodology is as follows.
We ﬁrst use a mobile app with network connection enabled
and record the appeared hints, and then repeat
the same
procedure with the network connections disabled. We perform
the experiment on the 150 popular apps excluding games as
mentioned in the introduction and the results show that 86 out
of 649 hints in these apps correctly displayed without network
connection, i.e., implemented purely at the client-side, and the
rest, which is 563 hints, requires more or less server support.
Result randomness. We reduce the randomness in our
experiment results via two methods: (i) ﬁxing random seed,
and (ii) repeating experiments. Speciﬁcally, we ﬁx the random
seed of Monkey during different runs in our experiment and
also repeat every experiment for three times. It is worth noting
that randomness cannot be fully mitigated due to many other
factors, such as network delay. For example, even if we ﬁx
the seed of Monkey, network delay may cause a login page
shows up after the next event is triggered. Therefore, all the
follow-up events may be inﬂuenced as well.
Exclusion of Gaming Apps from Our Evaluation Dataset.
We exclude gaming apps from our evaluation due to two rea-
sons. First, gaming apps commonly utilize ﬁgures to illustrate
notiﬁcations and most of their contents are presented in the
form of images. Since this paper focuses on the text input
generation, image-based game apps are apparently beyond the
scope. We will release the collected apps for the convenience
of other studies on text input generation of Android apps.
Second, gaming apps usually heavily rely on native code for
performance reasons—it is hard to measure the code coverage
in native code.
Hint Obfuscation. TextExerciser requires that Android
apps provide enough and clear hints for text inputs—this is
reasonable because these hints are intended to provide to
users so that they can interact with the app. We do ﬁnd
that some apps provide hints via ﬁgures and voices, which
cannot be recognized by TextExerciser. Currently, the
evaluation result shows that only about 1.4% of hints are
missed by TextExerciser. In the future, we plan to
introduce OCR [39] and voice recognition [40], and further
understand these hints in the image or audio format.
VII. RELATED WORK
In this section, we review related prior researches.
A. Input Generation in Testing Android Apps
Traditionally, a plenty of work focus on generating testing
input for Android apps, especially in automating dynamic
analysis. However, they either focus on event based input
such as UI event and system event, or rely on a plenty
of manual effort to generate valid text input. For example,
Monkey [14], the most frequently used Android testing tool,
only can generate UI events like randomly clicking elements
on UI screen. Dynodroid [16] and Mulliner et al. [41] expands
the UI events with system events, such as SMS receiving.
However, when encountering a text input ﬁeld like password,
they must pause the testing and wait for manual input.
Some modern works, such as Sapienz [25], A3E-Depth-
First [26], DroidBot [22], AppsPlayground [18] and Droid-
DEV [19], fulﬁll text input ﬁelds by searching in a set of
pre-deﬁned input. If none of the pre-deﬁned inputs can satisfy
an input’s constraints, these prior works will fail to exercise
beyond this input. Another thread of work, i.e., Liu et al. [20],
utilize RNN to train a learning model and use it to generate
text input based on the app context. Unfortunately, it requires a
large amount of manual effort to write input for training such a
model. As a comparison, TextExerciser ﬁrst identiﬁes the
input restrictions from UI screen by combining machine learn-
ing with UI structural analysis and then generates a text input
with a mutation based strategy. The main advantage is that
TextExerciser iteratively generate inputs for a given text
ﬁeld—i.e., even if a particular input fails, TextExerciser
can still generate more inputs based on newly-collected hints
as feedback.
In addition, another work Mobolic [42] uses symbolic
execution to extract the input constraints in app code and
utilizes a solver to generate valid input. However, many input
checks are enforced at the server side of Android apps.
As a comparison, TextExerciser can generate inputs
even if these input checks are performed at server side, because
input hints are eventually shown at client-side app.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:02:20 UTC from IEEE Xplore.  Restrictions apply. 
1083
B. Widget Identiﬁcation in Android Apps
UI widget identiﬁcation is often combined and used to-
gether with Android app testing because an exerciser needs
to interact with different Android UI widgets. SUPOR [43]
and UIPicker [44] extract UI widget information from the lay-
out’s XML ﬁle and then identify sensitive inputs. UiRef [45]
improves prior works by adopting a hybrid approach that
combines both static and dynamic identiﬁcations: the static
method identiﬁes widgets from layouts, just like prior work
and the dynamic method extracts each rendered layout during
on-device execution. Similarly, CuriousDroid [46] instruments
the Dalvik virtual machine to obtain the UI widgets and
generate UI-related events during execution.
All
the prior works on UI widget
identiﬁcation can
be combined with TextExerciser in testing Android
apps as widget
identiﬁcation is an orthogonal problem.
TextExerciser adopts UIAutomator because of two major
reasons. First, UIAutomator obtains all the widgets informa-
tion dynamically during execution, which has incorporated
many advantages claimed by prior works. Second, UIAu-
tomator is open-source and compatible with many real-world
Android apps.
C. Text Input Generation in Web
Input generation is a critical problem in testing web ap-
plications and locating vulnerabilities such as SQL Injection
and XSS vulnerabilities. Based on their requirements for the
source code of web apps, we can classify the prior work
into two categories. The ﬁrst part of work utilize white box
testing to launch analysis on targeted web apps. For example,
ACTEve [47] and S3 [48] ﬁrst use symbolic execution to ex-
tract input constraints in the source code and then use a solver
to generate an input. As a comparison,TextExerciser
works for Android apps and does not require any source
code—some of the source code is at the server side and
unavailable to TextExerciser.
Another thread of related work [49], [50] leverage black
box testing, but use manual effort to pre-deﬁne text inputs.
For example, one of many vulnerabilities studied Vieira et
al. [50] is to exploit web services using Acunetix web vulner-
ability scanner [51] with pre-deﬁned username and password
combinations.
These works are only available for generating some particu-
lar text ﬁelds, such as password, which has a public database.
In many cases like salary, username, and ages, such a public
database is unavailable. As a comparison, TextExerciser
works on Android apps and relies on hints as a feedback to
generate all the text inputs.
D. Fuzzing based Approach in Android Dynamic Analysis
Fuzzing is widely used in Android dynamic analysis. Com-
monly, the state-of-art approaches generate their input based
on a bunch of domain knowledge about the input structures.
For example, prior works [52]–[55] focus on fuzzing critical
data structures in Android such as Intent and Binder, which
are well-documented. In addition, another thread of work like
1084
Caiipa [56] use synthesized context observed in the wild to
guide its fuzzer so that it can cover different context variations.
As a comparison, TextExerciser focuses on exercising
text inputs, which are not target of existing Android fuzzers.
We can consider TextExerciser as a fuzzer on text inputs,
but the text-based fuzzer is guided by feedbacks, i.e., these
hints, provided by Android apps.
VIII. CONCLUSION
In this paper, we propose TextExerciser, an itera-
input exerciser, which generates
tive, feedback-driven text
text inputs for Android apps. TextExerciser relies on a
key insight that Android apps often provide feedback, called
hints, for malformed inputs from users—at the same time,
TextExerciser can also utilize such hints to improve the
input generation.
Our evaluation shows that TextExerciser can achieve
signiﬁcantly higher code coverage than these tools with default
text input generators. We also combine TextExerciser
with existing dynamic analysis tools like TaintDroid and
ReCon and show existing dynamic analysis tools are able to
detect more malicious behaviors with TextExerciser than
with existing exercisers. TextExerciser together with ex-
isting dynamic analysis tools is able to ﬁnd several previously-
unknown vulnerabilities in popular Android apps, such as
user credential leakage in a social app, arbitrary user proﬁle
modiﬁcation in a shopping app, and a software bug in another
traveling app.
IX. ACKNOWLEDGMENT
We would like to thank the anonymous reviewers for their
insightful comments that helped improve the quality of the
paper. This work was supported in part by the National
Natural Science Foundation of China (U1636204, U1736208,
U1836210, U1836213, 61972099, 61602121, 61602123), Nat-
ural Science Foundation of Shanghai (19ZR1404800), and Na-
tional Program on Key Basic Research (NO. 2015CB358800).
Min Yang is the corresponding author, and a faculty of Shang-
hai Institute of Intelligent Electronics & Systems, Shanghai
Institute for Advanced Communication and Data Science, and
Engineering Research Center of Cyber Security Auditing and
Monitoring, Ministry of Education, China.
REFERENCES
[1] J. Chen, W. Diao, Q. Zhao, C. Zuo, Z. Lin, X. Wang, W. C. Lau, M. Sun,
R. Yang, and K. Zhang, “Iotfuzzer: Discovering memory corruptions in
iot through app-based fuzzing,” NDSS, 2018.
[2] C. Zuo, Q. Zhao, and Z. Lin, “Authscope: Towards automatic discovery
of vulnerable authorizations in online services,” in CCS. ACM, 2017.
[3] D. Sounthiraraj, J. Sahs, G. Greenwood, Z. Lin, and L. Khan, “Smv-
hunter: Large scale, automated detection of ssl/tls man-in-the-middle
vulnerabilities in android apps,” in NDSS, 2014.
[4] C. Zheng, S. Zhu, S. Dai, G. Gu, X. Gong, X. Han, and W. Zou, “Smart-
droid: an automatic system for revealing ui-based trigger conditions in
android applications,” in SPSM. ACM, 2012.
[5] L. Xue, Y. Zhou, T. Chen, X. Luo, and G. Gu, “Malton: Towards
on-device non-invasive mobile malware analysis for art,” in USENIX
Security, 2017.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:02:20 UTC from IEEE Xplore.  Restrictions apply. 
[38] M. Backes, S. Bugiel, O. Schranz, P. von Styp-Rekowsky, and S. Weis-
gerber, “Artist: The android runtime instrumentation and security
toolkit,” in Euro S&P.
IEEE, 2017.
[39] Wikipedia. (2019) Optical character recognition. https://en.wikipedia.
org/wiki/Optical character recognition.
[40] ——. (2019) Speech recognition. https://en.wikipedia.org/wiki/Speech
recognition.
Hat USA, 2009.
[41] C. Mulliner and C. Miller, “Fuzzing the phone in your phone,” Black