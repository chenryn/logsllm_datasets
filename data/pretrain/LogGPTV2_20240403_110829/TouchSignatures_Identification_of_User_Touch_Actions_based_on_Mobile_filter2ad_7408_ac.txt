(xi − xi−1)2 + (yi − yi−1)2 + (zi − zi−1)2
ci =
This gives us a sequence which we call the device acceleration change se-
quence, or DAC sequence for short.
squared sequence values, i.e., E = P v2
First we consider basic statistical features for all sequences, their deriva-
tive, and the DAC sequence. These features include maximum, minimum,
and mean (average) of each sequence and its derivative, plus those of the
DAC sequence. We also consider the total energy of each sequence and its
derivative, plus that of the DAC sequence, calculated as the sum of the
i . Here, in total we get 102 features
for each sensor reading in the time domain. Later we will add a few more
features to the input of the ﬁrst phase (touch actions) in Section 4.3.
Frequency domain features. To distinguish between sequences with diﬀerent
frequency contents, we applied the Fast Fourier transform (FFT) of the se-
quences. We calculated the maximum, minimum, mean, and energy of the
FFT of each sequence and consider them as our frequency domain features,
i.e., a total of 48 frequency domain features.
3.5. Classiﬁcation method
To decide which classiﬁcation method to apply to our data, we imple-
mented various classiﬁcation algorithms to assess their eﬃciency. Our test
classiﬁers included discriminant analysis, naive Bayes, classiﬁcation tree, k-
NN, and ANN. Diﬀerent classiﬁers work better in the diﬀerent phases of
TouchSignatures (touch actions and PINs). The chosen classiﬁers in each
phase are presented in Sections 4.3 and 5.3. In both phases, we consider a
15
generic approach and train our algorithms with the data collected from all
users. Hence, our results are user-independent.
4. Phase 1: Identifying user touch actions
In this section we present the ﬁrst phase of TouchSignatures that is able
to distinguish user touch actions given access to the device orientation and
motions sensor data provided by a mobile browser.
4.1. Touch actions set
We consider a set of 8 most common touch actions through which users
interact with mobile devices. These actions include: click, scroll (up, down,
right, left), zoom (in, out), and hold. They are presented in Table 5 along
with their corresponding descriptions. Our experiments show that by ap-
plying machine learning techniques these actions are recognisable from their
associated sensor measurements.
Touch Action
Click
Scroll
– up, down, right, left
Zoom
– in, out
Touching continuously for a while with one ﬁnger
Hold
Table 5: The description of diﬀerent touch actions users perform on the touch screen of a
mobile device.
Description
Touching an item momentarily with one ﬁnger
Touching continuously and simultaneously sliding
in the corresponding direction
Placing 2 ﬁngers on the screen and sliding them
apart or toward each other, respectively
4.2. Experiments
We collected touch action samples from 11 users (university staﬀ and
students) using Google Chrome on an iPhone 5. We presented each user
with a brief description of the project as well as the instruction to perform
each of the 8 touch actions. The users were provided with the opportunity
of trials before the experiment to get comfortable using the web browser on
the mobile phone. They also could ask any question before and during the
experiments. We asked the user to remain sitting on a chair in an oﬃce
environment while performing the tasks. The provided GUI instructed the
user to perform a single touch action in each step, collecting 5 samples for
16
each touch action in successive steps with a three-second wait between steps.
During the experiment, the user was notiﬁed of her progress in completing
the expected tasks by the count of touch actions in an overall progress bar,
as shown in Figure 3 (left).
Data were collected from each user in two settings: one-hand mode and
two-hand mode. In the one-hand mode, we asked the users to hold the phone
in one hand, and use the same hand’s thumb for touching the screen.
In
the two-hand mode, we asked them to use both hands to perform the touch
actions. With these two settings, we made sure that our collected data set
is a combination of diﬀerent modes of phone usage. Note that zoom in/out
actions can only be performed in the two-hand mode. Still, we distinguish
two postures: 1) when a user holds the phone using one hand and performs
zoom in/out actions by using the thumb of that hand and any ﬁnger of
the other hand, and 2) when a user holds the phone using both hands and
performs zoom in/out by using the thumbs of both hands. We collected data
for both postures.
We had 10 samples of each of the following actions: click, hold, scroll
down, scroll up, scroll right and scroll down. Five samples were collected in
the one-hand mode and 5 in the two-hand mode. In addition, we collected
10 samples for each of the following two actions: zoom in and zoom out. All
10 samples were collected in the two-hand mode, with half for each of the
two postures. Each user’s output was a set of 80 samples. With 11 users,
we ended up with 880 samples for our set of touch actions. The experiment
took each user on average about 45 minutes to complete. Each user received
a $10 Amazon voucher for their contribution to the work.
4.3. Classiﬁcation algorithm
Before discussing the algorithms used in this phase, we add another 14 fea-
tures to the TouchSignatures’ time domain features. To diﬀerentiate between
touch actions with a longer “footprint” and those with a shorter footprint,
we consider a feature which represents the length (i.e., number of readings)
of each dimension of the acceleration and acceleration-including-gravity se-
quences that contain 70% of the total energy of the sequence. To calculate
this length, we ﬁrst ﬁnd the “centre of energy” of the sequence as follows:
i )/E, where E is the total energy as calculated before. We then
consider intervals centred at CoE and ﬁnd the shortest interval containing
70% of the total energy of the sequence. Therefore, considering both time
domain and frequency domain features from Section 3.4 in addition to the
CoE =P (i v2
17
Click
Hold
Scroll
Touch
Zoom
action
out
Click
0%
Hold
1.82%
Scroll
0.90%
0.23% 71.82% 20.90%
Zoom in
0.45% 25.45% 76.36%
Zoom out
Total
100%
100%
Table 6: Confusion matrix for the ﬁrst classiﬁer for diﬀerent touch actions
78.18%
5.45%
10.90% 88.18%
7.27%
0%
3.64%
100%
2.73%
0.68%
2.72% 95.91%
1.82%
1.82%
100%
Zoom
in
0%
1.81%
0.90%
100%
new ones, TouchSignatures’ ﬁnal vector for phase one has 164 features in
total.
distance is deﬁned asqP (fi − f0
Our evaluations show that the k-nearest neighbour (k-NN) algorithm [10]
gives the best overall identiﬁcation rate for our data. k-NN is a type of lazy
learning in which each object is assigned to the class to which the majority of
its k nearest neighbours are assigned, i.e., each feature vector is assigned to
the label of the majority of the k nearest training feature vectors. A distance
function is used to decide the nearest neighbours. The most common distance
function is the Euclidean distance, but there are other distance functions
such as the city block distance (a.k.a. Manhattan or taxicab distance). For
i)2 and the city block distance asP|fi − f0
two given feature vectors (f1, f2, . . . , fn) and (f0
n), the Euclidean
i|.
Based on the results of our evaluations, we decide to use two classiﬁers
in two stages. In the ﬁrst stage, the data is fed to the ﬁrst classiﬁer which is
a 1-NN classiﬁer using Euclidean distance. This classiﬁer is responsible for
classiﬁcation of the input data into 5 categories: click, hold, zoom in, zoom
out, and scroll. In the second stage, if the output of the ﬁrst stage is scroll,
then the data is fed into the second classiﬁer which is a 1-NN classiﬁer using
city block distance. This classiﬁer is responsible for classiﬁcation of a scroll
into one of the 4 categories: scroll up, scroll down, scroll right, and scroll
left. We used a 10-fold cross validation approach for all the experiments.
2, . . . , f0
1, f0
4.4. Results
In this section we show the results obtained from the cross validation of
the collected user data by presenting the identiﬁcation rates and confusion
matrices for both classiﬁers. Considering all scrolls (up, down, right, and
left) in one category, the overall identiﬁcation rate is 87.39%.
18
Scroll
up
Scroll
down
Scroll
Touch
action
right
Scroll down 57.27% 19.09% 12.73%
26.36% 69.09% 16.36%
Scroll up
9.09%
Scroll right
7.27%
Scroll left
Total
100%
Scroll
left
4.55%
6.36%
4.55% 48.18% 17.27%
22.73% 71.82%
7.27%
100%
100%
100%
Table 7: Confusion matrix for the second classiﬁer for diﬀerent scroll types
Table 6 shows the confusion matrix for our ﬁrst classiﬁer. In each cell,
the matrix lists the probability that the classiﬁer correctly labels or mislabels
a sample in a category. The actual and classiﬁed categories are listed in the
columns and rows of the table, respectively. As shown in Table 6, the worst
results are for the pairs of Click and Hold (10.9% and 5.45%), and also pairs
of Zoom in and Zoom out (25.45% and 20.9%). This is expected since click
and hold are very similar actions: and hold is basically equivalent to a long
click. Zoom in and zoom out also require the user to perform similar gestures.
Another signiﬁcant value is the classiﬁer’s confusion between click and scroll
(7.27%, 2.73%), which again is not surprising since scroll involves a gesture
similar to a click. Apart from the mentioned cases, the rest of the confusion
probabilities are nearly negligible.
Table 7 shows the identiﬁcation rates and confusion matrix for our sec-
ond classiﬁer, respectively. Overall, our second classiﬁer is able to correctly
identify the scroll type with a success rate of 61.59%. The classiﬁer mostly
mislabels the pairs (down, up), and (right, left), which is somehow expected
since they involve similar gestures.
The obtained results show that attacks on user privacy and security by
eavesdropping sensor data through web content are feasible and are able to
achieve accurate results. Further security risks could be imposed to the users
if the attack tries to identify what character has been pressed on the touch
screen. In phase 2 of TouchSignatures, we show that it is indeed possible to
succeed such an attack by identifying the digits entered for the user’s PINs.
5. Phase 2: Identifying user PINs
In this section, we present the second phase of TochSignatures which
is able to identify user PINs based on the motion and orientation sensor
19
iPhone 5 Nexus 5
Attribute
navigator.platform iPhone
320 pixs
screen.width
screen.height
568 pixs
Linux armv7l
360 pixs
640 pixs
Table 8: The device information accessible via JavaScript.
data provided by JavaScript code. As mentioned in Section 1, classifying
soft keyboard characters on touch screen has already been explored by other
researchers based on the sensor data accessible through native apps. In this
work, for the ﬁrst time, we show that it is also possible to do that by using
the sensor data obtained via JavaScript despite the fact that the available
frequency is much lower.
In this phase, we present the results of our suggested attack on both
Android (Nexus 5) and iOS (iPhone 5) devices and we train two diﬀerent
classiﬁers (neural networks) for them. Note that JavaScript is able to ob-
tain speciﬁc information about a mobile device – for example the browser
platform and the screen size are accessible via Navigator DOM10 and Screen
DOM11 objects, respectively. The obtained values for the tested devices are
summarized in Table 8. Hence, though the experiments are performed using
speciﬁc mobile devices, the results have general implications on all devices.
5.1. Digits set
In this work, we consider a numerical keypad and leave the attack on
the full keyboard as future work. A numerical keyboard includes a set of
10 digits: 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9, and a few more characters such
as -, ., and #, depending on the mobile OS. For example Figure 3 (centre)
shows a numerical keypad on an Android device. The idea is to identify
the pressed digits in a PIN. Hence from a top view, once the ﬁrst phase
of TouchSignatures distinguishes that the user is “clicking” digits on a soft
keyboard, the second phase is started in order to obtain the entered digits.
5.2. Experiments
Similar to the ﬁrst experiment, we asked a group of users (university
student and staﬀ) including 12 users to participate in our experiment in two
10www.w3schools.com/js/js window navigator.asp
11w3schools.com/js/js window screen.asp
20
parts. The ﬁrst part was on an iPhone 5 and the second part was on a Nexus
5, both using Chrome. After giving a brief description about the study to
the users, they were presented with a simple GUI (Figure 3, centre) asking
them to enter 25 random 4-digit PINs on both devices. The 4-digit PINs
were designed in a way that each number was exactly repeated 10 times in
total. After entering each 4-digit PIN, the user could press a next button
to go to the next PIN. They also could keep track of their progress as the
number of PINs they have entered so far was shown on the page.
In this experiment, we asked the users to remain sitting on a chair and
hold the phone in the way that they felt comfortable. The collected data
contained a mixture of one-hand mode and two-hand mode records. In the
one-hand mode, the user pressed the digits with one of the ﬁngers of the
same hand with which they were holding the phone. In the two-hand mode,
they pressed the digits with either the free hand, or both hands. We had
10 samples of each digit for each user. Since we had 10 digits, each user’s
output was a set of 100 samples for each device. With 12 users, the input of
our classiﬁers was 1200 records for iPhone 5 and 1200 records for Nexus 5. It
took each user 2 minutes on average to complete each part of the experiment
with preparation and explanations. It took each user less than 10 minutes
to ﬁnish the whole experiment.
5.3. Classiﬁcation algorithm
Among diﬀerent classiﬁcation methods, we observed that ANN (Artiﬁ-
cial Neural Network) works signiﬁcantly better than other classiﬁers on our
dataset. A neural network system for recognition is deﬁned by a set of input
neurons (nodes) which can be activated by the information of the intended
object to be classiﬁed. The input can be either raw data, or pre-processed
data from the samples. In our case, we have preprocessed our samples by
building a feature vector as described in Section 3.4. Therefore, as input,
TouchSignatures’ ANN system receives a set of 150 features for each sample.
A neural network can have multiple layers and a number of nodes in each
layer. Once the ﬁrst layer of the nodes receives the input, ANN weights and
transfers the data to the next layer until it reaches the output layer which
is the set of the labels in a classiﬁcation problem. For better performance
and to stop training before over-ﬁtting, a common practice is to divide the
samples into three sets: training, validation, and test sets.
We trained a neural network with 70% of our data, validated it with
15% of the records and tested it with the remaining 15% of our data set.
21
1 (54%) 2 (64%) 3 (63%)
4 (81%) 5 (67%) 6 (73%)
7 (57%) 8 (74%) 9 (79%)
∗# 0 (73%)
English
Nexus 5 (Ave. iden. rate: 70%)
-
.
X
>
1 (70%) 2 (50%) 3 (59%)
4 (70%) 5 (46%) 6 (56%)
7 (53%) 8 (48%) 9 (67%)
+ ∗ # 0 (41%)
>
iPhone 5 (Ave. iden. rate: 56%)
Table 9: Identiﬁcation rates of digits in Nexus 5 and iPhone 5.
We trained our data by using pattern recognition/classifying network with
one hidden layer and 10,000 nodes. Pattern recognition/classifying networks
normally use a scaled conjugate gradient (SCG) back-propagation algorithm
for updating weight and bias values in training. SCG [17] is a fast supervised
learning algorithm based on conjugate directions. The results of the second
phase of TouchSignatures are obtained according to these settings.
5.4. Results
Here, we present the output of the suggested ANN for Nexus 5 and
iPhone 5, separately. Table 9 shows the accuracy of the ANN in classifying
the digits presented in two parts for the two devices. The average identi-
ﬁcation rates for Nexus 5 and iPhone 5 are 70% and 56%, respectively. In
general, the resolution of the data sequences on Android was higher than iOS.
We recorded about 37 motion and 20 orientation measurements for a typical
digit on Android, while there were only 15 for each sequence on iOS. This can
explain the better performance of TouchSignatures on Android than on iOS.
It is worth mentioning that attacks on iPhone 5 actually are the ones with
the lowest sampling rates that we observed in Table 3 (20Hz for both motion
and orientation). Interestingly, even with readings on the lowest available
sampling rate, the attack is still possible.
In Tables 10 and 11, we show the identiﬁcation results of each digit (bold
in each cell), as well as confusion matrices on both devices. The general forms
of the tables are according to Android and iOS numpads. As demonstrated,
each digit is presented with all possible misclassiﬁable digits. As it can be
observed, most misclassiﬁed cases are either in the same row or column, or
in the neighbourhood of each expected digit.
Note that the probability of success in ﬁnding the actual digit will signiﬁ-
cantly improve with more tries at guessing the digit. In fact, while the chance
of the attack succeeding is relatively good on the ﬁrst guess, it increases on
further guesses as shown in Tables 12 and 13. Figure 4 shows the average
22
(0%)
(0%)
(0%)
-
(6%)
(6%)
(0%)
-
(14%)
(0%)
(0%)
-
1 (54%)
(15%)
(15%)
-
(0%)
4 (81%)