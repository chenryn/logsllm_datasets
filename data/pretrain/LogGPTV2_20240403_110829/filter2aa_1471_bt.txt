child 
partition
Read/write a VP’s runtime counter
Read the current partition reference time
Access SynIC timers and registers
Query/set the VP’s virtual APIC assist page
Read/write hypercall MSRs
Request VP IDLE entry
Read VP’s index
Map or unmap the hypercall’s code area
Read a VP’s emulated TSC (time-stamp counter) and its 
frequency
Control the partition TSC and re-enlightenment 
emulation
Read/write VSM synthetic registers
Read/write VP’s per-VTL registers
Starts an AP virtual processor
Enables partition’s fast hypercall support
Root 
partition only
Create child partition
Look up and reference a partition by ID
Deposit/withdraw memory from the partition 
compartment
Post messages to a connection port
Signal an event in a connection port’s partition
Create/delete and get properties of a partition’s 
connection port
Connect/disconnect to a partition’s connection port
Map/unmap the hypervisor statistics page (which 
describe a VP, LP, partition, or hypervisor)
Enable the hypervisor debugger for the partition
Schedule child partition’s VPs and access SynIC 
synthetic MSRs
Trigger an enlightened system reset
Read the hypervisor debugger options for a partition
Child 
partition only
Generate an extended hypercall intercept in the root 
partition
Notify a root scheduler’s VP-backed thread of an event 
being signaled
EXO 
partition
None
Partition privileges can only be set before the partition creates and starts
any VPs; the hypervisor won’t allow requests to set privileges after a single
VP in the partition starts to execute. Partition properties are similar to
privileges but do not have this limitation; they can be set and queried at any
time. There are different groups of properties that can be queried or set for a
partition. Table 9-2 lists the properties groups.
Table 9-2 Partition’s properties
PROPERT
Y GROUP
DESCRIPTION
Scheduling 
properties
Set/query properties related to the classic and core 
scheduler, like Cap, Weight, and Reserve
Time 
properties
Allow the partition to be suspended/resumed
Debugging 
properties
Change the hypervisor debugger runtime configuration
Resource 
properties
Queries virtual hardware platform-specific properties of 
the partition (like TLB size, SGX support, and so on)
Compatibili
ty 
properties
Queries virtual hardware platform-specific properties that 
are tied to the initial compatibility features
When a partition is created, the VID infrastructure provides a
compatibility level (which is specified in the virtual machine’s configuration
file) to the hypervisor. Based on that compatibility level, the hypervisor
enables or disables specific virtual hardware features that could be exposed
by a VP to the underlying OS. There are multiple features that tune how the
VP behaves based on the VM’s compatibility level. A good example would
be the hardware Page Attribute Table (PAT), which is a configurable caching
type for virtual memory. Prior to Windows 10 Anniversary Update (RS1),
guest VMs weren’t able to use PAT in guest VMs, so regardless of whether
the compatibility level of a VM specifies Windows 10 RS1, the hypervisor
will not expose the PAT registers to the underlying guest OS. Otherwise, in
case the compatibility level is higher than Windows 10 RS1, the hypervisor
exposes the PAT support to the underlying OS running in the guest VM.
When the root partition is initially created at boot time, the hypervisor
enables the highest compatibility level for it. In that way the root OS can use
all the features supported by the physical hardware.
The hypervisor startup
In Chapter 12, we analyze the modality in which a UEFI-based workstation
boots up, and all the components engaged in loading and starting the correct
version of the hypervisor binary. In this section, we briefly discuss what
happens in the machine after the HvLoader module has transferred the
execution to the hypervisor, which takes control for the first time.
The HvLoader loads the correct version of the hypervisor binary image
(depending on the CPU manufacturer) and creates the hypervisor loader
block. It captures a minimal processor context, which the hypervisor needs to
start the first virtual processor. The HvLoader then switches to a new, just-
created, address space and transfers the execution to the hypervisor image by
calling the hypervisor image entry point, KiSystemStartup, which prepares
the processor for running the hypervisor and initializes the CPU_PLS data
structure. The CPU_PLS represents a physical processor and acts as the
PRCB data structure of the NT kernel; the hypervisor is able to quickly
address it (using the GS segment). Differently from the NT kernel,
KiSystemStartup is called only for the boot processor (the application
processors startup sequence is covered in the “Application Processors (APs)
Startup” section later in this chapter), thus it defers the real initialization to
another function, BmpInitBootProcessor.
BmpInitBootProcessor starts a complex initialization sequence. The
function examines the system and queries all the CPU’s supported
virtualization features (such as the EPT and VPID; the queried features are
platform-specific and vary between the Intel, AMD, or ARM version of the
hypervisor). It then determines the hypervisor scheduler, which will manage
how the hypervisor will schedule virtual processors. For Intel and AMD
server systems, the default scheduler is the core scheduler, whereas the root
scheduler is the default for all client systems (including ARM64). The
scheduler type can be manually overridden through the
hypervisorschedulertype BCD option (more information about the different
hypervisor schedulers is available later in this chapter).
The nested enlightenments are initialized. Nested enlightenments allow the
hypervisor to be executed in nested configurations, where a root hypervisor
(called L0 hypervisor), manages the real hardware, and another hypervisor
(called L1 hypervisor) is executed in a virtual machine. After this stage, the
BmpInitBootProcessor routine performs the initialization of the following
components:
■    Memory manager (initializes the PFN database and the root
compartment).
■    The hypervisor’s hardware abstraction layer (HAL).
■    The hypervisor’s process and thread subsystem (which depends on the
chosen scheduler type). The system process and its initial thread are
created. This process is special; it isn’t tied to any partition and hosts
threads that execute the hypervisor code.
■    The VMX virtualization abstraction layer (VAL). The VAL’s purpose
is to abstract differences between all the supported hardware
virtualization extensions (Intel, AMD, and ARM64). It includes code
that operates on platform-specific features of the machine’s
virtualization technology in use by the hypervisor (for example, on
the Intel platform the VAL layer manages the “unrestricted guest”
support, the EPT, SGX, MBEC, and so on).
■    The Synthetic Interrupt Controller (SynIC) and I/O Memory
Management Unit (IOMMU).
■    The Address Manager (AM), which is the component responsible for
managing the physical memory assigned to a partition (called guest
physical memory, or GPA) and its translation to real physical memory
(called system physical memory). Although the first implementation
of Hyper-V supported shadow page tables (a software technique for
address translation), since Windows 8.1, the Address manager uses
platform-dependent code for configuring the hypervisor address
translation mechanism offered by the hardware (extended page tables
for Intel, nested page tables for AMD). In hypervisor terms, the
physical address space of a partition is called address domain. The
platform-independent physical address space translation is commonly
called Second Layer Address Translation (SLAT). The term refers to
the Intel’s EPT, AMD’s NPT or ARM 2-stage address translation
mechanism.
The hypervisor can now finish constructing the CPU_PLS data structure
associated with the boot processor by allocating the initial hardware-
dependent virtual machine control structures (VMCS for Intel, VMCB for
AMD) and by enabling virtualization through the first VMXON operation.
Finally, the per-processor interrupt mapping data structures are initialized.
EXPERIMENT: Connecting the hypervisor debugger
In this experiment, you will connect the hypervisor debugger for
analyzing the startup sequence of the hypervisor, as discussed in
the previous section. The hypervisor debugger is supported only via
serial or network transports. Only physical machines can be used to
debug the hypervisor, or virtual machines in which the “nested
virtualization” feature is enabled (see the “Nested virtualization”
section later in this chapter). In the latter case, only serial
debugging can be enabled for the L1 virtualized hypervisor.
For this experiment, you need a separate physical machine that
supports virtualization extensions and has the Hyper-V role
installed and enabled. You will use this machine as the debugged
system, attached to your host system (which acts as the debugger)
where you are running the debugging tools. As an alternative, you
can set up a nested VM, as shown in the “Enabling nested
virtualization on Hyper-V” experiment later in this chapter (in that
case you don’t need another physical machine).
As a first step, you need to download and install the “Debugging
Tools for Windows” in the host system, which are available as part
of the Windows SDK (or WDK), downloadable from
https://developer.microsoft.com/en-
us/windows/downloads/windows-10-sdk. As an alternative, for this
experiment you also can use the WinDbgX, which, at the time of
this writing, is available in the Windows Store by searching
“WinDbg Preview.”
The debugged system for this experiment must have Secure Boot
disabled. The hypervisor debugging is not compatible with Secure
Boot. Refer to your workstation user manual for understanding
how to disable Secure Boot (usually the Secure Boot settings are
located in the UEFI Bios). For enabling the hypervisor debugger in
the debugged system, you should first open an administrative
command prompt (by typing cmd in the Cortana search box and
selecting Run as administrator).
In case you want to debug the hypervisor through your network
card, you should type the following commands, replacing the terms
 with the IP address of the host system; ”
with a valid port in the host (from 49152); and
 with the bus parameters of the network
card of the debugged system, specified in the XX.YY.ZZ format
(where XX is the bus number, YY is the device number, and ZZ is
the function number). You can discover the bus parameters of your
network card through the Device Manager applet or through the
KDNET.exe tool available in the Windows SDK:
Click here to view code image
bcdedit /hypervisorsettings net hostip: port:
bcdedit /set {hypervisorsettings} hypervisordebugpages 1000
bcdedit /set {hypervisorsettings} hypervisorbusparams 
bcdedit /set hypervisordebug on
The following figure shows a sample system in which the
network interface used for debugging the hypervisor is located in
the 0.25.0 bus parameters, and the debugger is targeting a host
system configured with the IP address 192.168.0.56 on the port
58010.
Take note of the returned debugging key. After you reboot the
debugged system, you should run Windbg in the host, with the
following command:
Click here to view code image
windbg.exe -d -k net:port=,key=
You should be able to debug the hypervisor, and follow its
startup sequence, even though Microsoft may not release the
symbols for the main hypervisor module:
In a VM with nested virtualization enabled, you can enable the
L1 hypervisor debugger only through the serial port by using the
following command in the debugged system:
Click here to view code image
bcdedit /hypervisorsettings SERIAL DEBUGPORT:1 
BAUDRATE:115200
The creation of the root partition and the boot
virtual processor
The first steps that a fully initialized hypervisor needs to execute are the
creation of the root partition and the first virtual processor used for starting
the system (called BSP VP). Creating the root partition follows almost the
same rules as for child partitions; multiple layers of the partition are
initialized one after the other. In particular:
1. 
The VM-layer initializes the maximum allowed number of VTL
levels and sets up the partition privileges based on the partition’s type
(see the previous section for more details). Furthermore, the VM layer
determines the partition’s allowable features based on the specified
partition’s compatibility level. The root partition supports the
maximum allowable features.
2. 
The VP layer initializes the virtualized CPUID data, which all the
virtual processors of the partition use when a CPUID is requested
from the guest operating system. The VP layer creates the hypervisor
process, which backs the partition.
3. 
The Address Manager (AM) constructs the partition’s initial physical
address space by using machine platform-dependent code (which
builds the EPT for Intel, NPT for AMD). The constructed physical
address space depends on the partition type. The root partition uses
identity mapping, which means that all the guest physical memory
corresponds to the system physical memory (more information is
provided later in this chapter in the “Partitions’ physical address
space” section).
Finally, after the SynIC, IOMMU, and the intercepts’ shared pages are
correctly configured for the partition, the hypervisor creates and starts the
BSP virtual processor for the root partition, which is the unique one used to
restart the boot process.
A hypervisor virtual processor (VP) is represented by a big data structure
(VM_VP), shown in Figure 9-6. A VM_VP data structure maintains all the
data used to track the state of the virtual processor: its platform-dependent
registers state (like general purposes, debug, XSAVE area, and stack) and
data, the VP’s private address space, and an array of VM_VPLC data
structures, which are used to track the state of each Virtual Trust Level
(VTL) of the virtual processor. The VM_VP also includes a pointer to the
VP’s backing thread and a pointer to the physical processor that is currently
executing the VP.
Figure 9-6 The VM_VP data structure representing a virtual processor.
As for the partitions, creating the BSP virtual processor is similar to the
process of creating normal virtual processors. VmAllocateVp is the function
responsible in allocating and initializing the needed memory from the
partition’s compartment, used for storing the VM_VP data structure, its
platform-dependent part, and the VM_VPLC array (one for each supported
VTL). The hypervisor copies the initial processor context, specified by the
HvLoader at boot time, into the VM_VP structure and then creates the VP’s
private address space and attaches to it (only in case address space isolation
is enabled). Finally, it creates the VP’s backing thread. This is an important
step: the construction of the virtual processor continues in the context of its
own backing thread. The hypervisor’s main system thread at this stage waits
until the new BSP VP is completely initialized. The wait brings the
hypervisor scheduler to select the newly created thread, which executes a
routine, ObConstructVp, that constructs the VP in the context of the new
backed thread.
ObConstructVp, in a similar way as for partitions, constructs and initializes
each layer of the virtual processor—in particular, the following:
1. 
The Virtualization Manager (VM) layer attaches the physical
processor data structure (CPU_PLS) to the VP and sets VTL 0 as
active.
2. 
The VAL layer initializes the platform-dependent portions of the VP,
like its registers, XSAVE area, stack, and debug data. Furthermore,
for each supported VTL, it allocates and initializes the VMCS data
structure (VMCB for AMD systems), which is used by the hardware
for keeping track of the state of the virtual machine, and the VTL’s
SLAT page tables. The latter allows each VTL to be isolated from
each other (more details about VTLs are provided later in the “Virtual
Trust Levels (VTLs) and Virtual Secure Mode (VSM)” section) .
Finally, the VAL layer enables and sets VTL 0 as active. The
platform-specific VMCS (or VMCB for AMD systems) is entirely
compiled, the SLAT table of VTL 0 is set as active, and the real-mode
emulator is initialized. The Host-state part of the VMCS is set to
target the hypervisor VAL dispatch loop. This routine is the most
important part of the hypervisor because it manages all the VMEXIT
events generated by each guest.
3. 
The VP layer allocates the VP’s hypercall page, and, for each VTL,
the assist and intercept message pages. These pages are used by the
hypervisor for sharing code or data with the guest operating system.
When ObConstructVp finishes its work, the VP’s dispatch thread activates
the virtual processor and its synthetic interrupt controller (SynIC). If the VP
is the first one of the root partition, the dispatch thread restores the initial
VP’s context stored in the VM_VP data structure by writing each captured
register in the platform-dependent VMCS (or VMCB) processor area (the
context has been specified by the HvLoader earlier in the boot process). The
dispatch thread finally signals the completion of the VP initialization (as a
result, the main system thread enters the idle loop) and enters the platform-
dependent VAL dispatch loop. The VAL dispatch loop detects that the VP is
new, prepares it for the first execution, and starts the new virtual machine by
executing a VMLAUNCH instruction. The new VM restarts exactly at the
point at which the HvLoader has transferred the execution to the hypervisor.
The boot process continues normally but in the context of the new hypervisor
partition.
The hypervisor memory manager
The hypervisor memory manager is relatively simple compared to the
memory manager for NT or the Secure Kernel. The entity that manages a set
of physical memory pages is the hypervisor’s memory compartment. Before
the hypervisor startup takes palace, the hypervisor loader (Hvloader.dll)
allocates the hypervisor loader block and pre-calculates the maximum
number of physical pages that will be used by the hypervisor for correctly
starting up and creating the root partition. The number depends on the pages
used to initialize the IOMMU to store the memory range structures, the
system PFN database, SLAT page tables, and HAL VA space. The
hypervisor loader preallocates the calculated number of physical pages,
marks them as reserved, and attaches the page list array in the loader block.
Later, when the hypervisor starts, it creates the root compartment by using
the page list that was allocated by the hypervisor loader.
Figure 9-7 shows the layout of the memory compartment data structure.
The data structure keeps track of the total number of physical pages
“deposited” in the compartment, which can be allocated somewhere or freed.
A compartment stores its physical pages in different lists ordered by the
NUMA node. Only the head of each list is stored in the compartment. The
state of each physical page and its link in the NUMA list is maintained
thanks to the entries in the PFN database. A compartment also tracks its
relationship with the root. A new compartment can be created using the
physical pages that belongs to the parent (the root). Similarly, when the
compartment is deleted, all its remaining physical pages are returned to the
parent.
Figure 9-7 The hypervisor’s memory compartment. Virtual address space
for the global zone is reserved from the end of the compartment data
structure
When the hypervisor needs some physical memory for any kind of work, it
allocates from the active compartment (depending on the partition). This
means that the allocation can fail. Two possible scenarios can arise in case of
failure:
■    If the allocation has been requested for a service internal to the
hypervisor (usually on behalf of the root partition), the failure should
not happen, and the system is crashed. (This explains why the initial