### CAMP Performance and Analysis

#### Accuracy and Traffic Distribution
The overall accuracy of CAMP is 98.6%, which is primarily influenced by the traffic distribution. The majority of requests are for benign binaries, and our system exhibits low false positive rates. When client-side whitelists are considered, the true negative and false positive rates further improve the accuracy.

\[ \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} \]

In June 2012, the true positive rate was approximately 70%, and the false negative rate was around 30%. This indicates that CAMP can detect 70% of recent malware without accessing the binary content, validating the reputation-based detection approach. A more detailed comparison between CAMP and traditional antivirus (AV) engines is provided in Section V-D.

#### Design Considerations
In designing CAMP, a key priority was to minimize false positives. As a result, the binary analysis pipeline often favors false negatives over false positives, which impacts the measurements presented here. For example, a false negative in the binary analysis pipeline can lead to an incorrect false positive for the reputation-based classifier.

#### Detection Rates and Contributions
**Figure 10:**
- The graph illustrates the stacked contribution to detection when a single AND gate or rule labels a download as malicious or unknown.
- It also shows the number of cases where multiple rules flagged the binary download.

**Figure 11:**
- The graph displays the number of client requests corresponding to binaries labeled as malware by the binary classifier.
- It also includes requests to malware where the binary was signed.

When CAMP correctly identifies a malware binary but the binary analysis system has a false negative, the evaluation counts this as a false positive for CAMP. Each individual AND gate achieves only low recall, but combining different AND gates increases the overall performance of the classifier. The results, shown in Figure 10, indicate that most detections are due to a single rule, and the detection of unknown binaries accounts for a significant fraction of malicious verdicts.

#### Signature State and Malware Rotation
To understand the frequency of malicious binaries with trusted signatures, we extracted the signature state from analyzed binaries. Figure 11 shows that the majority of requests to malware are for unsigned binaries, making a trusted signature a good predictor of benign binaries.

We also analyzed how often a binary labeled as unknown transitions to clean or malicious after dynamic analysis. Over a 10-day period in November 2012, 74% of sites serving unknown binaries did not reappear in subsequent requests, increasing to 80% over 10 days. This suggests that these sites are likely malicious and rotate to avoid blacklists. Only 0.02% of sites transitioned from unknown to bad, 8% transitioned to clean, and the rest remained unknown. Manual analysis of the top sites transitioning to clean revealed that they were hosting dangerous downloads, indicating a false negative in our dynamic analysis framework.

#### Comparison to Other Systems
To evaluate CAMP's performance relative to other malware detection systems, we conducted two measurements:

1. **Comparison with AV Engines:**
   - We collected a random sample of 10,000 binaries labeled as benign and 8,400 binaries labeled as malicious by CAMP.
   - We compared CAMP's results with four different AV engines, scanning the binaries on the same day.
   - For the 10,000 benign binaries, the maximum number labeled as malicious by a single AV engine was 83, with only 16 flagged by two or more engines.
   - For the 8,400 malicious binaries, the AV engine that agreed most with CAMP flagged only 25% as malicious, and less than 40% were detected by all four engines combined.

2. **Comparison with Web Services:**
   - We selected 20,000 URLs, 10,000 each for benign and malicious binaries, and consulted several web services for classification.
   - For benign URLs, TrendMicro flagged about 3.5% as malicious, Symantec about 2.5%, and Site Advisor about 1.5%.
   - For malicious URLs, TrendMicro identified about 11% as malicious, Safe Browsing about 8.5%, Symantec about 8%, and Site Advisor about 2.5%.
   - Many URLs were unknown to the web services, confirming the limitations of blacklist-based approaches in the current environment of rapidly changing malware distribution domains.

#### Case Study
CAMP provides insights into malware distribution across the web. One example is a campaign distributing Fake Anti-Virus binaries, using frequent repacking and fast domain rotation to evade blacklist-based defenses. This campaign was observed between February 13, 2012, and March 1, 2012.

**Figure 13 and Figure 14:**
- These figures show the results from web services for the selected URLs, comparing their verdicts with CAMP.

In summary, CAMP's reputation-based approach, combined with dynamic analysis, offers a robust method for detecting malware, outperforming many traditional AV engines and web services in terms of detection and false positive rates.