tp+tn+fp+fn . The
of CAMP was 98.6%. We compute this as
dominating factor in overall accuracy comes from our traffic
distribution; the majority of requests are for benign binaries
and our system exhibits low false positive rates. As with false
positive and true negative rates, accuracy increases further
when taking the client-side whitelists into account.
tp+tn
In June 2012, the true positive rate was around 70% and
the false negative rate around 30%. CAMP’s ability to detect
70% of recent malware without access to the content of the
binary validates the reputation-based detection approach. We
provide a more detailed comparison between CAMP and AV
engines in Section V-D.
In designing CAMP, our main consideration was avoid-
ing false positives and as such the binary analysis pipeline
frequently prefers false negatives over false positives which
has a negative impact on all measurements presented here,
e.g. a false negative in the binary analysis pipeline could
 300000
 250000
s
t
s
e
u
q
e
R
f
o
r
e
b
m
u
N
 200000
 150000
 100000
 50000
ratio/ip/client
ratio/site/client
ratio/site/analysis
ratio/ip/analysis
binary/unknown
multiple rules
 1e+06
 100000
l
e
b
a
l
t
s
all malware requests
signed malware requests
signed/untrusted malware requests
i
l
k
c
a
b
l
h
t
i
w
s
t
s
e
u
q
e
r
t
n
e
i
l
c
f
o
r
e
b
m
u
N
 10000
 1000
 100
 0
0
3
/
0
1
0
4
/
0
1
0
5
/
0
1
Date
0
6
/
0
1
0
7
/
0
1
 10
0
7
/
1
2
0
7
/
1
4
0
7
/
1
6
0
7
/
1
8
Date
0
7
/
2
0
0
7
/
2
2
0
7
/
2
4
Fig. 10. The graph shows the stacked contribution to detection in cases
where a single AND gate or rule is responsible for labeling a download as
malicious or unknown. It also shows the number of cases in which multiple
rules flagged the binary download.
Fig. 11. The graph shows the number of client requests corresponding to
binaries labeled as malware by the binary classifier. It also shows requests
to malware were the binary was signed.
result in an incorrect false positive for the reputation-based
classifier. Specifically, when CAMP detects a malware binary
correctly, but the binary analysis system has a false negative,
our evaluation will count this as a false positive for CAMP.
As discussed in the previous section, each individual AND
gate achieves only low recall. Our hypothesis was that dif-
ferent AND gates would combine to increase the overall
performance of the classifier. To measure this assumption, we
plot how often an AND gate is responsible for detection by
itself and how often multiple AND gates detect bad content si-
multaneously. The results are shown in Figure 10 as a stacked
graph. The graph has two interesting features. The majority
of detections are due to a single rule, i.e. each individual
rule contributes to the overall detection. The other interesting
observation is that the detection of unknown binaries accounts
for a significant fraction of malicious verdicts. We show in
the case study below how adversaries frequently rotate the
domains from which they are serving malware and thus never
build up any reputation. The drop in the detection of unknown
binaries around March 24th is due to a change in data formats
that resulted in dropping the contributions of the unknown rule
from the graph.
The rule for labeling binaries as unknown requires that
a binary is not signed by a trusted signer. To understand
how often malicious binaries with trusted signatures occur in
our analysis, we extracted the signature state from binaries
we analyzed from client requests post-facto. As shown in
Figure 11, the majority of requests to malware are for unsigned
binaries. That makes a trusted signature a good predictor for
the likelihood that a binary is benign.
We also wanted to understand how often a binary that is
labeled as unknown transitions to clean or to malicious after
it has been evaluated by our dynamic analysis pipeline. To
do so, we analyzed requests from clients post-facto, for a
period of 10 days in November 2012. Of all the sites that
served binaries that were classified as unknown on the first
day, 74% no longer appeared in our requests the next day. This
percentage increased to 80% over 10 days. We hypothesize
that this is because such sites are malicious and rotate to
avoid blacklists. This corroborates previous findings that social
engineering domains rotate quickly to avoid blacklists [27].
0.02% of the sites transitioned from unknown to bad over the
10 day period, 8% transitioned to unknown to clean, and the
remaining stayed in the unknown state. We manually analyzed
the top sites that transitioned to clean, and found that almost
all of them were indeed hosting dangerous downloads, and
the clean classification was a false negative of our dynamic
analysis framework. Based on these results, we believe that
the unkown verdict is actually a very strong indicator that a
binary is actually malicious.
D. Comparison to other systems
To determine if CAMP provides benefits beyond that
provided by other malware detection systems, we conducted
two different measurements in which we compared reputation
decisions from CAMP to results from Anti-Virus engines as
well as web-based malware services.
Over a period of two weeks we collected a random sample
of 10, 000 binaries that were labeled as benign by CAMP
as well as approximately 8, 400 binaries that were labeled
as malicious by CAMP. To avoid bias in binary selection,
for example due to popular download sites which might be
over represented in a completely random sample, we sampled
by site rather than by individual binaries. We compared the
results from CAMP against the results from four different AV
engines1 that scanned the binaries on the same day as CAMP
made its reputation decision. We conducted the AV scans on
the same day to approximate the performance users might
see when AV engines scan their downloads. The AV engines
ran in a sandboxed environment, and were not permitted any
1Due to licensing restrictions, we cannot disclose the specific AV engines
we used in our measurements.
safe
bad
d
e
n
n
a
c
s
s
e
i
r
a
n
b
i
f
o
%
 100
 80
 60
 40
 20
 0
C
A
M
P
A
A
A
A
V
-
1
V
-
2
V
-
3
V
-
4
Fig. 12. The graph shows the results from AV engines for binaries flagged
as malicious by CAMP.
network access. Thus any cloud-based reputation data that may
have been provided by the AV companies was unavailable
to the AV engines. However, we proactively updated AV
signatures every two hours to ensure freshness.
For the 10, 000 binaries that CAMP labeled as clean, the
maximum number of binaries labeled as malicious by a single
AV engine was only 83. Only 16 binaries were flagged as ma-
licious by two or more AV engines. This implies that CAMP
has a very high True Negative rate relative to commercial
Anti-Virus products.
On the other hand, the majority of binaries that CAMP la-
beled as malicious were classified as benign by the AV engines
(see Figure 12). The AV engine that agreed the most with
CAMP only flagged 25% of the binaries as malicious. When
combining the results from all four AV engines, less than 40%
of the binaries were detected. One possible explanation for
these results is that CAMP might exhibit a high false positive
rate, but as shown in Figure 9 and discussed earlier, CAMP’s
false positive rate is quite low and thus false positives are
not a likely explanation. However, as observed by Oberheide
et al. [24] many AV engines exhibit poor detection rates for
recent malware and we believe that to be confirmed by our
measurements, too.
In addition to comparing CAMP’s detection results with
AV engines, we also consulted several web services to classify
URLs that hosted binaries. For this measurement, we consulted
the following services: Malware Domain List, McAfee’s Site
Advisor [22], Symantec’s Safe Web [23], Google’s Safe
Browsing [16] and TrendMicro’s Site Safety Center. We
selected 20, 000 URLs from a signle day’s worth of requests;
10, 000 URLs pointed to binaries that were classified as benign
by CAMP and 10, 000 URLs that pointed to binaries that were
identified as malicious. We employed the same site-based
sampling strategy that was used for evaluating AV engines.
For each of the selected URLs, we consulted the web services
listed above and compared their verdict with CAMP. The
results are shown in Figure 13 and 14.
The URL classification services mostly agreed with CAMP
when presented with the set of clean URLS. TrendMicro
flagged about 3.5% as malicious, Symantec about 2.5% and
Site Advisor about 1.5%. Furthermore, many of the benign
URLs were unknown to these three services. For example,
TrendMicro did not know over 55% of the URLs. Neither the
Malware Domain List nor Safe Browsing flagged any of the
URLs are malicious.
The URL classification services mostly disagreed with
CAMP when presented with the set of malicious URLs. Trend-
Micro identified about 11% as malicious, Safe Browsing about
8.5%, Symantec about 8% and Site Advisor about 2.5%. The
Malware Domain List did not flag any of them as malicious.
However, as with the benign URLs, many of the malicious
URLs were not known to the web services. For example,
TrendMicro did not know 65% of the URLs that CAMP found
to be malicious.
Google Chrome asks for a reputation decision only if a
URL is not known by the Safe Browsing API. Therefore, it is
not surprising that many of URLs CAMP considers malicious
were not in the Safe Browsing list. Moreover, the Safe Brows-
ing list primarily targets drive-by downloads, not intentional
user installs. Although, the Malware Domain list did not return
any detections, we included it in our measurements as it is
frequently used as a base of comparison by other work in this
space.
The results for the other web services seem to confirm our
suspicion that blacklist based approaches are not as effective
in the current environment of frequently changing malware
distribution domains. The majority of URLs identified as
malicious by CAMP are not known to be bad by any web
service. On the other hand, CAMP explicitly assigns negative
reputation to domains unknown to it. We could interpret the
unknown results from the web services in a similar way. In that
case, detection rates would increase noticeably. For example,
when combining unknown and known malicious results, the
detection rate for TrendMicro would be 76%, for Symantec
46% and for Site Advisor 45%. However, in that case, the
potential false positive rates as measured by comparing to
the benign URLs would increase significantly, too. From that
perspective, TrendMicro would flag 59% of the benign URLs,
Symantec 24% and Site Advisor 29.5%. As the potential
false positive rates are much higher than can be sustained in
practice, our original interpretation of the inherent drawbacks
in blacklists is a more likely explanation.
E. Case Study
CAMP provides an interesting vantage point into malware
distribution across the web. In the following, we explore an
example of one of many malware campaigns discovered by
CAMP. This campaign distributes Fake Anti-Virus binaries
and leverages frequent repacking of binaries as well as fast
domain rotation to evade blacklist-based defense mechanisms.
We observed the campaign between February 13, 2012 and
March 1, 2012.
s
L
R
U
d
e
n
n
a
c
s
f
o
%
 100
 80
 60
 40
 20
 0
C
A
M
P
M
a
l
w
a
r
e
D
o
m
S
it
e
A
d
vis
o
r
a
i
n
L
is
t
S
y
m
a
n