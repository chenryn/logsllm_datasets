ment is much larger for Transformer and Megatron than ResNet50.
ResNet50 is less sensitive to network bandwidth for two reasons.
First, it is a smaller model than the others and therefore requires less
bandwidth for all-reduce operations. Second, ResNet50 trains effec-
tively with large batch sizes (via weak scaling), further reducing its
bandwidth requirements [86, 90–92].
e
e
r
g
e
D
d
i
r
b
y
H
l
a
m
i
t
p
O
1,020
256
64
16
4
1
DP degree
MP degree
1,020
256
64
16
4
1
100
1,000
10,000
Bandwidth per GPU (Gbps)
100
1,000
10,000
Bandwidth per GPU (Gbps)
(a) ResNet50
(b) Transformer
Figure 6: Optimal hybrid trade-off between the degree of MP
and DP at different per-node bandwidths for 1024 GPUs.
computed for a configuration in this procedure is only an estimate;
in our actual simulations, a GPU’s bandwidth can vary over time
(e.g., due to circuit reconfiguration). Therefore, our simulator re-
quires a runtime stage to track the effect of dynamic decisions on
ops scheduling more precisely.
Runtime. Our runtime simulator relies on three main compo-
nents: GPUs, an interconnect, and an executive session. The session
launches the operations onto the GPUs as soon as their dependen-
cies are met in the DNN graph. The interconnect can be electrical
or optical. Our current implementation includes SiP-Ring, SiP-OCS,
electrical, and full-mesh interconnects.
Rostam models a latency for each op launched onto the GPU and
a minimum completion time for ops that run on the GPU. Hence,
there is a lower-bound on how quickly we can run a compute graph
that depends on its critical path length. We set the launch latency
and the minimum completion to 1 microsecond in our experiments.
Moreover, Rostam overlaps the communication and computation
whenever possible.
4.3 Results
Fig. 5 compares the time-to-accuracy of our three DNN models
with 1024 GPUs on different network architectures. We vary the
bandwidth per GPU, B, between 128—8192 Gbps, and compare
Elect-Flat, Elect-Cluster with two values of inter-server bandwidth
(200 Gbps or 400 Gbps), SiP-OCS, and SiP-Ring. For each value
of B and each network architecture, we use Algorithm 1 (§3.2) to
search for the best parallelization strategy, as described in §4.2.
To compare the different architectures on an equal footing, we
run Algorithm 1 for electrical networks by removing the degree
constraint. We then compare our results to the state-of-the-art
results reported in MLPerf [88] and find that they are comparable
or better (§A.3). For reference, we also show data parallel (DP)
training on Elect-Flat (except for Megatron which cannot use basic
DP).
We also experiment with FlexFlow [38] as a state-of-the-art
placement algorithm. FlexFlow’s network model does not support
the degree constraints required by our optical interconnects. For
electrical interconnects, we run the FlexFlow code [89] for our
workloads, but the strategies it finds are very similar to DP. We
believe there are two reasons for this. First, the scales we consider
(e.g., 1000 GPUs) are much larger than those in FlexFlow, making
the search space for its Metropolis algorithm significantly larger.
Second, FlexFlow’s implementation only searches for partitioning
strategies across the batch dimension (although the approach in [38]
is general).
664
Comparing DP with the best strategy found using Algorithm 1
on Elect-Flat is also instructive. Consider Transformer: when B is
less than 1 Tbps, our placement cannot beat DP. But as B increases
to 8 Tbps, SiP-ML’s hybrid strategy outperforms DP by ≈50%.
Now let us turn to the Elect-Cluster architectures. For all three
models, the training time plateaus as we increase B, with Elect-
Cluster (400 Gbps) outperforming Elect-Cluster (200 Gbps). Recall
that here, B is the local bandwidth between the GPUs within each
server. The results show that scaling this local bandwidth can im-
prove training time to an extent (by enabling some model paral-
lelism), but the slow server-to-server network eventually becomes
a bottleneck and prevents further speedups.
Compared to Elect-Cluster architectures, SiP-OCS and SiP-Ring
achieve 1.3–9.1× faster training time as we scale B. The benefits
are smallest for ResNet50 (which does not require very high com-
munication bandwidth) and most significant for Megatron. SiP-ML
architectures are less efficient than the ideal Elect-Flat (which can-
not be realized in practice for large values of B and N ): to achieve the
same training time, SiP-ML architectures require up to 2× higher
bandwidth per GPU (B) (e.g., Transformer), with a smaller gap in
many cases (e.g., Megatron). This difference reflects the constraints
imposed by optical circuit switching. Specifically, in our evalua-
tions, we set the degree constraint for both SiP-OCS and SiP-Ring
at D=16. SiP-OCS requires a one-shot reconfiguration, while SiP-
Ring imposes a traffic locality requirement on the communication
pattern. Despite these constraints, SiP-ML performs quite well, as
our placement algorithm adapts the parallelization strategy to suit
the degree requirement.
SiP-OCS and SiP-Ring perform similarly overall. Each architec-
ture has pluses and minuses. Unlike SiP-OCS, SiP-Ring has fast
reconfiguration, but it makes communication between more distant
GPUs on the ring less efficient. Our results show that the impacts of
these factors on overall performance effectively cancel each other
out.
Parallelization strategies. Fig. 6 plots the degrees of DP and MP
for each value of B in SiP-OCS. The figure shows that as the per-
node bandwidth increases on the x-axis, the optimal strategy uses
more model parallelism to decrease the total training time. This
is consistent with current practice: when the network is slow, DP
is more efficient but on a fast network, combining MP and DP
improves training time. For instance, the Transformer model shown
in Fig. 6b starts with 1024-way DP and 1-way MP, but at 10 Tbps
bandwidth per-GPU, the best training time is achieved with 16-way
MP and 64-way DP.
SiP-ML: Optical Network Interconnects for Machine Learning
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Figure 7: Traffic matrices generated by SiP-ML for the Trans-
former model on 1024 GPUs (displaying only the first 32
GPUs for brevity).
15
10
5
)
s
n
i
m
(
.
c
c
A
-
o
t
-
e
m
T
i
2 Tbps
4 Tbps
8 Tbps
10
12
14
16
Num. of OCSs
Figure 8: Impact of number of OCSs in SiP-OCS on time-to-
accuracy of a hybrid training of Transformer with one-shot
configuration. The lines correspond to different per-GPU
bandwidth (B). Dashed horizontal lines of the same color
show performance achieved by Elect-Flat at the same band-
width.
Communication patterns. To better understand the communi-
cation patterns produced by Algorithm 1, Fig. 7 shows the traffic
matrices for the Transformer model with MP degree k = 4, 8, 16,
corresponding to 2 Tbps, 6 Tbps, and 10 Tbps per-GPU bandwidth,
respectively. These traffic matrices have two main components:
(i) a set of identical k × k blocks, corresponding to the traffic be-
tween the nodes in each MP group (brighter colors represent larger
values); (ii) an off-diagonal component, corresponding to the DP
ring-all-reduce traffic used by each GPU to synchronize its param-
eters with its peers in other MP groups (holding the same part of
the model). Within the k × k blocks, the entries near the diagonal
are larger (brighter), indicating the GPUs communicate more with
their immediate neighbors. This property helps when mapping the
communication to SiP-Ring. The off-diagonal entries (DP traffic)
are smaller than the largest entries for the MP traffic, but they are
still significant. This is the downside of current hierarchical electri-
cal fabrics, as shown in Fig. 5, the low server-to-server bandwidth
becomes a chokepoint.
The traffic matrices also show how SiP-ML meets the degree
constraint. For example, in SiP-OCS, each GPU establishes circuits
with members of its MP group and is also part of a ring with its
peers in other MP groups. The resulting topology is effectively
the union of l = N /k identical direct-connect topologies and k
rings. The number of circuits to each destination is chosen based
on the traffic intensity towards that destination, although finding
the optimal circuit allocation is more subtle and requires solving
an ILP (§3.3).
Impact of number of OCSs and reconfiguration latency. In-
creasing the number of OCSs (or the total number of ports on each
)
s
n
i
m
(
.
c
c
A
-
o
t
-
e
m
T
i
15
10
5
0
2 Tbps, Reconfig.
2 Tbps, one-shot
4 Tbps, Reconfig.
4 Tbps, one-shot
101
102
103
Reconfiguration Delay (µsec)
Figure 9: Impact of OCS reconfiguration delay on time-to-
accuracy of Transformer in SiP-OCS for two per-GPU band-
widths. The critical reconfiguration delay when choosing be-
tween reconfigurable and one-shot allocation depends on
the bandwidth.
Elect-Flat
SiP-OCS
Elect-Cluster 400Gbps
SiP-Ring
102
101
100
23 24 25 26 27 28 29 210
25 26 27 28 29 210 211 212
102
101
)
s
n
i
m
(
.
c
c
A
-
o
t
-
e
m
T
i
Number of GPUs
(a) ResNet50
Number of GPUs
(b) Transformer
Figure 10: Overall performance of SiP-ML’s OCS and Ring
topologies at different scales.
OCS) in SiP-OCS can improve performance in two ways: (i) we
can increase the maximum permissible communication degree; or
(ii) for the same communication degree, we can allow a more fine-