queries. This observation supports our assumptions presented in Section 4. For
the actual detection of intrusions, a threshold has to be chosen to separate
malicious from normal traﬃc and to raise an alert when the limit is exceeded.
For our ﬁrst prototype, we simply select a ‘reasonable’ magic number be-
tween the maximum value gathered from the set of normal requests (16) and
the minimum among the evaluated exploits (216). Because an attacker might
attempt to limit the MEL by choosing a shorter sledge, the value should stay
closer to the maximum of the normal requests. We decided to select 30 for the
deployment of the probe. This leaves enough room for regular requests to keep
the false positive rate low and forces an intruders to reduce the executable parts
of his exploit to a length less than this limit to remain undetected. Such a short
sledge nevertheless seriously impacts the attacker’s chances to guess an address
that is ‘close enough’ to the correct one to succeed.
An obvious shortcoming of the proposed approach is that it can’t detect ex-
ploits that utilize methods to avoid executable sledges. Vulnerable services that
include debug routines that output information which might be used to calcu-
late the exact stack address can be exploited by hackers. If the attacker causes
the service to execute the debug output and calculates the exact stack address
(infoleak attack), he can create buﬀer overﬂows that don’t include executable
sledges.
6.4 Performance Results
To evaluate the performance impact of our module on the web server, we used
the WebSTONE [25] benchmark provided by Mindcraft. WebSTONE can simulate
an arbitrary number of clients that request pages of diﬀerent sizes from the web
288
T. Toth and C. Kruegel
server to simulate realistic load. It determines a number of interesting properties
that are listed below.
– Average and maximum connection time of requests
– Average and maximum response time of requests
– Data throughput rate
The connection time is the time interval between the point when the client
opens a TCP connection and the point when the server accepts it. The response
time measures the time between the point when the client has established the
connection and requests data and the point when the ﬁrst result is received. The
data throughput rate is a value for the amount of data that the web server is able
to deliver to all clients.
The connection and response time values are relevant for the time a user
has to wait after sending a request until results are delivered back. These times
also characterize the number of requests a web server is able to handle under a
speciﬁc load. The data throughput rate deﬁnes how fast data can be sent from
the web server to the client. Because clients obviously cannot receive replies
faster than the server is sending them, this number is an indication for how long
a client has to wait until a request completes.
Our experimental setup consists of one machine simulating the clients that
perform HTTP requests (Athlon, 1 Ghz, 256 MB RAM, Linux 2.4) and one host
with the Apache server (Pentium III, 550 MHz, 512 MB RAM, Linux 2.4).
Both machines are connected using a 100 Mb Fast Ethernet. WebSTONE has
been conﬁgured to launch 10 to 100 clients in steps of 10, each running for 2
minutes. We did only a measurement of static pages, so no tests involved dynamic
creation of results.
We measured the connection rate, the average client response time and the
average client throughput for each test run with and without our installed mod-
ule. The results are shown in Figures 7, 8 and 9. The dotted line represent the
statistics gathered when running the unmodiﬁed Apache while the solid line
represent the one with our activated module.
As can be seen above, the connection rate has dropped slightly when our
sensor is activated. The biggest diﬀerence emerged when 50 clients are active
and a value of 494,2 connections per second versus 500,7 connections per second
with the unmodiﬁed Apache has been observed. While this maximum diﬀerence
is 6.5 connections per second (yielding a decrease in the client connection rate
of about 1.4 %), the average value is only 2,4 (about 0.5 %).
There has been no signiﬁcant decrease in the average response time. Both
lines are nearly congruent with regards to the precision of measurements.
The client throughput decreased most with 10 active clients when it dropped
from 75,90 Mbit per second to 73,70 MBit per second. This is an absolute diﬀer-
ence of 2,2 MBit per second. (about 2,9 %). On average, the client throughput
only decreased by 0,8 Mbit per second (about 1,05%).
The trie consumed about 16 MB of memory during the tests. While this
seems to be a large number at ﬁrst glance, one has to take the usual main
memory equipment of web servers into account where a Gigabyte of RAM is not
Accurate Buﬀer Overﬂow Detection via Abstract Payload Execution
289
Connection Rate Comparison
530
520
510
500
490
480
]
s
/
s
n
o
i
t
c
e
n
n
o
c
[
e
t
a
R
n
o
i
t
c
e
n
n
o
C
470
0
20
40
60
Nr. of clients
80
100
Fig. 7. Client Connection Rate
Response Time Comparison
0.20
]
s
m
[
0.15
i
e
m
T
e
s
n
o
p
s
e
R
0.10
.
g
v
A
0.05
0.00
0
20
40
60
Nr. of clients
80
100
Fig. 8. Average Response Time
uncommon. In addition, this data structure makes very fast tests possible and
is a classical trade-oﬀ in favor of speed.
7 Conclusion and Future Work
This paper presents an accurate way of detecting buﬀer overﬂow exploit code
in Internet service requests. We explain the structure and constraints of these
attacks and discuss methods used by intruders to evade common detection tech-
niques.
290
T. Toth and C. Kruegel
Client Throughput Comparison
79
78
77
76
75
74
]
s
/
t
i
B
M
[
t
u
p
h
g
u
o
r
h
T
t
n
e
i
l
C
73
0
20
40
60
Nr. of clients
80
100
Fig. 9. Client Throughput
Our analysis approach bases on the abstract execution of the packet pay-
load to detect the sledge of an exploit. We deﬁne a valid instruction chain as
a number of consecutive bytes in a request that represent executable processor
instructions. The detection mechanism uses the fact that requests which contain
buﬀer overﬂow code include noticeably longer chains than regular requests. In
addition to the provision of theoretical support, our hypothesis has been veriﬁed
by comparing the results for regular HTTP and DNS requests to ones with exploit
code.
The system has the advantage that requests can be analyzed and denied a-
priori before the service process is aﬀected by a buﬀer overﬂow. It is also resistant
to the presented evasion techniques in Section 2. The performance impact of the
probe has been evaluated by integrating it into the Apache web server.
Further work will concentrate on emulating instructions that have not been
included yet (SIMD and MMX operations). We also investigate whether it is useful
to perform a ‘full’ emulation of the eﬀects of the instructions (not only to check
the basic executability) in order to detect buﬀer overﬂow exploits with a self
modifying sledge.
Additionally, we plan to collect experimental data for other protocols like
FTP and NFS to validate that our proposed approach is also applicable there.
References
1. AlephOne. Smashing the stack for fun and proﬁt. Phrack Magazine, 49(14), 1996.
2. Debra Anderson, Thane Frivold, Ann Tamaru, and Alfonso Valdes. Next Genera-
tion Intrusion Detection Expert System (NIDES). SRI International, 1994.
3. The Apache Software Foundation. http://www.apache.org.
Accurate Buﬀer Overﬂow Detection via Abstract Payload Execution
291
4. M. Bykova, S. Ostermann, and B. Tjaden. Detecting network intrusions via a
statistical analysis of network packet characteristics. In Proceedings of the 33rd
Southeastern Symposium on System Theory, 2001.
5. Crispin Cowan, Calton Pu, David Maier, Heather Hinton, Peat Bakke, Steve Beat-
tie, Aaron Grier, Perry Wagle, and Qian Zhang. Automatic detection and preven-
tion of buﬀer-overﬂow attacks. In 7th USENIX Security Symposium, January 1998.
6. Dorothy Denning. An intrusion-detection model. In IEEE Symposium on Security
and Privacy, pages 118–131, Oakland, USA, 1986.
7. Laurent Eschenauer. Imsafe. http://imsafe.sourceforge.net, 2001.
8. Stephanie Forrest, Steven A. Hofmeyr, Anil Somayaji, and Thomas A. Longstaﬀ.
A sense of self for Unix processes. In Proceedinges of the 1996 IEEE Symposium on
Research in Security and Privacy, pages 120–128. IEEE Computer Society Press,
1996.
9. The GNU Compiler Collection. http://gcc.gnu.org.
10. A. Ghosh and A. Schwartzbard. A study in using neural networks for anomaly and
misuse detection. In USENIX Security Symposium, 1999.
11. Judith Hochberg, Kathleen Jackson, Cathy Stallins, J. F. McClary, David DuBois,
and Josephine Ford. NADIR: An automated system for detecting network intrusion
and misuse. Computer and Security, 12(3):235–248, May 1993.
12. Intel. IA-32 Intel Architecture Software Developer’s Manual Volume 1-3, 2002.
http://developer.intel.com/design/Pentium4/manuals/.
13. Home of K2. http://www.ktwo.ca.
14. Christopher Kruegel, Thomas Toth, and Clemens Kerer. Service Speciﬁc Anomaly
Detection for Network Intrusion Detection. In Symposium on Applied Computing
(SAC). ACM Scientiﬁc Press, March 2002.
15. Mudge. Compromised: Buﬀer-Overﬂows, from Intel to SPARC Version 8.
http://www.l0pht.com, 1996.
16. Peter G. Neumann and Phillip A. Porras. Experience with EMERALD to date.
In 1st USENIX Workshop on Intrusion Detection and Network Monitoring, pages
73–80, Santa Clara, California, USA, April 1999.
17. Phillip A. Porras and Peter G. Neumann. EMERALD: Event Monitoring En-
abling Responses to Anomalous Live Disturbances. In Proceedings of the 20th NIS
Security Conference, October 1997.
18. Martin Roesch. Snort - Lightweight Intrusion Detection for Networks. In USENIX
Lisa 99, 1999.
19. SecurityFocus Corporate Site. http://www.securityfocus.com.
20. Jude Shavlik, Mark Shavlik, and Michael Fahland. Evaluating software sensors for
actively proﬁling Windows 2000 computer users. In Recent Advances in Intrusion
Detection (RAID), 2001.
21. E. Spaﬀord. The Internet Worm Program: Analysis. Computer Communication
Review, January 1989.
22. Stuart Staniford, James A. Hoagland, and Joseph M. McAlerney. Practical Au-
tomated Detection of Stealthy Portscans. In Proceedings of the IDS Workshop of
the 7th Computer and Communications Security Conference, Athens, 2000.
23. Giovanni Vigna and Richard A. Kemmerer. NetSTAT: A Network-based Intrusion
In 14th Annual Computer Security Applications Conference,
Detection System.
December 1998.
24. Giovanni Vigna and Richard A. Kemmerer. NetSTAT: A Network-based Intrusion
Detection System. Journal of Computer Security, 7(1):37–71, 1999.
25. WebSTONE - Mindcraft Corporate Site. http://www.mindcraft.com.