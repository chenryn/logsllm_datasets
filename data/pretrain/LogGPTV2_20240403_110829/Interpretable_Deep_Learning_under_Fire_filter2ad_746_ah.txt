### Figure 7: Attribution Maps of Benign and Adversarial (ADV2) Inputs in the Skin Cancer Screening Application

#### Q4. Real Application
We now evaluate the effectiveness of ADV2 in real security-critical applications. Specifically, we use the skin cancer screening task from the ISIC 2018 challenge [18] as a case study. In this task, skin lesion images are categorized into a seven-disease taxonomy. We adopt a competition-winning model (with ResNet as its backbone) as the classifier, which achieves 82.27% weighted multi-class accuracy on the holdout set (see Appendix C2 for more details).

**Model Source:** [https://github.com/ngessert/isic2018](https://github.com/ngessert/isic2018)

---

### Evaluation of ADV2 in the Skin Cancer Screening Task

We apply ADV2 to this classifier and measure its effectiveness in generating plausible interpretations. Figure 7 shows a set of samples and their attribution maps for four different interpreters. It is evident that ADV2 generates interpretations that are visually indistinguishable from their benign counterparts in all cases.

#### Quantitative Validation
This similarity is further quantitatively validated in Figure 8, which presents the L1 measures (other Lp measures are provided in Appendix C2) and Intersection over Union (IoU) scores of the maps generated by ADV2 with respect to the benign maps. For instance, the IoU scores of ADV2 exceed 0.62 across all interpreters.

**Figure 8:**
- **(a)** L1 measures
- **(b)** IoU scores

**Interpreters:**
- PGD
- ADV2

---

### Alternative Attack Framework

In addition to the PGD framework, ADV2 can be flexibly built upon alternative frameworks. Here, we construct ADV2 using STADV [60], a spatial transformation-based adversarial attack. Implementation details are provided in Appendix A1.

**Figure 9:**
- Attribution maps of benign and adversarial (STADV, STADV-based ADV2) inputs with respect to GRAD, CAM, MASK, and RTS on ResNet.

Figure 9 visualizes sample benign and adversarial inputs and their interpretations. Compared to STADV, ADV2 generates adversarial inputs with maps that are much more similar to the benign cases, highlighting the effectiveness of ADV2 when constructed upon the STADV framework.

**Quantitative Validation:**
This observation is confirmed by the L1 measures and IoU scores of the adversarial attribution maps, as shown in Figure 10 (additional results in Appendix C3). Interestingly, compared to other interpreters, MASK appears more resilient to STADV-based ADV2. The comparison with the results of PGD-based ADV2 (Figures 5 and 6) suggests that the relative robustness of different interpreters may vary depending on the specific attack (details in ยง 5).

**Figure 10:**
- **(a)** L1 measures
- **(b)** IoU scores

**Interpreters:**
- STADV
- STADV-based ADV2

**Conclusion:**
- **Observation 5:** As a general class of attacks, ADV2 can be flexibly built upon alternative adversarial attack frameworks.

---

### 5. Discussion

While it has been shown in ยง 4 that ADV2 is effective against a range of classifiers and interpreters, the root cause of this effectiveness remains unclear. We conduct a study to explore this root cause from both analytical and empirical perspectives. Based on our findings, we also discuss potential countermeasures against ADV2.

#### Q1. Root of Attack Vulnerability

The formulation of ADV2 in Equation (3) defines two seemingly conflicting objectives:
1. Maximizing the prediction change.
2. Minimizing the interpretation change.

We conjecture that the effectiveness of ADV2 may stem from the partial independence between a classifier and its interpreter. The interpreter's explanations only partially describe the classifier's predictions, making it feasible to exploit both models simultaneously.

To validate the existence of this prediction-interpretation gap, we consider ADV2 targeting randomly generated predictions and interpretations. For a given input \( x \), we randomly generate a target class \( c_t \) and a target interpretation \( m_t \), and search for an adversarial input \( x^* \) that triggers the classifier to misclassify it as \( c_t \) and also generates an interpretation similar to \( m_t \) (i.e., \( f(x^*) = c_t \) and \( g(x^*; f) \approx m_t \)). If ADV2 can find such \( x^* \), it indicates that the classifier and its interpreter can be manipulated separately, meaning they are only partially aligned.

**Random Patch Interpretation:**
- For a given input, we define its target attribution map by sampling a patch of random shape (either a rectangle or a circle), random angle, and random position over the input. Elements inside the patch are set to '1' and those outside to '0'. Typically, this target map deviates significantly from its benign counterpart due to its randomness.

**Effectiveness of ADV2:**
- Table 6 summarizes the attack success rate of ADV2 on ResNet. Compared to Table 3, targeting random patch interpretations has little impact on the attack effectiveness in terms of deceiving the classifiers, implying that the space of adversarial inputs is sufficiently large to contain ones with targeted interpretations.

**Table 6:**
- ASR (MC) of ADV2 targeting random patch interpretations.

**Conclusion:**
- **Observation 6:** A DNN and its interpreter are often not fully aligned, allowing the adversary to exploit both models simultaneously.

#### Q2. Root of Prediction-Interpretation Gap

Next, we explore the fundamental causes of this prediction-interpretation gap. We speculate the following possible explanation:

[Further discussion and analysis to follow]

---

This revised text aims to provide a clear, coherent, and professional presentation of the content, ensuring that the information is well-structured and easy to follow.