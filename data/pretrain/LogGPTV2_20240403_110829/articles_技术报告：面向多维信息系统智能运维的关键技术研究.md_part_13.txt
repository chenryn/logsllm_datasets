agent支持对常用操作系统、中间件、网络设备、安全设备、数据库日志进行采集，支持对性能数据、二进制格式、脚本运行结果进行采集。
采集方式包括但不限于：
（1）支持增量读取文件日志（适用于业务日志）
（2）支持syslog/rsyslog /snmp等接口（适用于网络设备，安全设备）
（3）支持获取mysql，oracle，sql server等数据库信息；（适用于业务日志）
（4）支持通过flume／ws／api接口，获取业务系统日志；（适用于定制开发）
（5）支持通过agent获取eventlog日志（适用于windows系统）
（6）支持对接流量抓包系统（适用于对接第三方流量产品）
NTA支持采集流量数据，主要是针对南北向流量和东西向流量，支持常见数据包解码包括IPv4,
IPv6, TCP, UDP, SCTP, ICMPv4, ICMPv6, GRE, Ethernet, PPP, PPPoE, Raw,
SLL, VLAN, QINQ, MPLS, ERSPAN, VXLAN，支持常见应用层协议解码包括HTTP,
SSL, TLS, SMB, DCERPC, SMTP, FTP, SSH, DNS, Modbus, ENIP/CIP, DNP3, NFS,
NTP, DHCP, TFTP, KRB5, IKEv2, SIP, SNMP, RDP。
一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，支持在日志系统中定制各类数据发送方，用于收集数据；同时提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。
\(1\) 可靠性
当节点出现故障时，日志能够被传送到其他节点上而不会丢失。集群提供了end-to-end模式来保障可靠性：收到数据agent首先将event发送到后端消息队列集群，如果数据发送失败，将event写到磁盘上，可以重新发送。
\(2\) 可扩展性
模块完全是多主平行结构，不存在单点故障问题。
\(3\) 可管理性
每个节点均提供metric
api接口，以json格式数据展现服务状态，方便提供服务指标和运维监控。
###  消息队列技术
用于消息的持久化和缓存。系统使用磁盘文件做持久化，顺序进行读写，以append方式写入文件。为减少内存copy，集群使用sendfile发送数据，通过合并message提升性能。集群本身不储存每个消息的状态，而使用（consumer/topic/partition）保存每个客户端状态，大大减小了维护每个消息状态的麻烦。在消息推拉的选择上，集群使用拉的方式，避免推的方式下存在的各个客户端的处理能力、流量等不同产生不确定性。以多机形式形成集群，建议3台或3台以上奇数台服务器组建，并且支持分区副本。
采用高吞吐量的分布式发布订阅消息系统有如下特性：
通过的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息也能够保持长时间的稳定性能。
高吞吐量，即使是非常普通的硬件也可以支持每秒数百万的消息。
消息队列对消息保存时根据Topic进行归类，一个Topic可以认为是一类消息，每个topic将被分成多个partition(区),每个partition在存储层面是append
log文件。任何发布到此partition的消息都会被直接追加到log文件的尾部，每条消息在文件中的位置称为offset（偏移量），offset为一个long型数字，它是唯一标记一条消息。
![](media/image52.png){width="3.5305555555555554in" height="2.28125in"}
###  集群资源同步与管理技术
一个针对大型分布式系统的高可靠协调系统，提供的功能包括：配置维护、名字服务、分布式同步、组服务等。封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。由3台或3台以上奇数台服务器组建完成，具有如下3个特性：
-   提供丰富的构件来实现多种协调数据结构和协议。
-   访问原子性，客户端要么读到所有数据，要么读取失败，不会出现只读取部分的情况。
-   具有高可用性，帮助系统避免单点故障，故障时可以快速删掉故障服务器。故障回复时，重新加入到集群。
###  数据分析技术
采用业界最先进的流式大数据处理架构Spark
Streaming，构建高性能、分布式日志处理架构可以每秒钟分析10万条日志，每天可以处理TB级的日志量，而且处理延时非常短，支持用户搜索、分析几秒钟之前产生的日志。
流式计算集群具有如下特性：
● 轻量级快速处理
着眼大数据处理，速度往往被置于第一位。Spark允许Hadoop集群中的应用程序在内存中以100倍的速度运行，即使在磁盘上运行也能快10倍。Spark通过减少磁盘IO来达到性能提升，它们将中间处理数据全部放到了内存中。
● 无数据丢失
系统需要保证无数据丢失，这也是系统高可用性的保证。系统为了无数据丢失，需要在数据处理失败的时候选择另外的执行路径进行replay（系统不是简单的重新提交运算，而是重新执行调度，否则按照来源的call
stack有可能使得系统永远都在相同的地方出同样的错误）。
● 容错透明
用户不会也不需要关心容错。系统会自动处理容错，调度并且管理资源，而这些行为对于运行于其上的应用来说都是透明的。
常用分析方法
###  前端服务技术
采用LVS + Nginx构建的前端机器，主要有以下特点：
-   Http、rest协议
-   水平扩展
-   流量上涨时可快速通过增加前端机来提高处理能力
-   高吞吐、低延时
-   纯异步处理，单个请求异常不会影响其他请求
-   内部采用专门针对日志的Lz4压缩，提高单机处理能力，降低网络带宽
###  数据安全技术
#### 数据传输安全
传输加密主要通过SSL/TLS加密协议完成。如果要进行TLS/SSL的配置，首先必须要具有来自受信CA的证书，使用该CA生成每个节点的证书，节点的所有服务都会通过上述生成的证书进行SSL/TLS加密。每个节点还需要建立本地的Java
Keystore以支持SSL/TLS公钥、秘钥的存放。
智能运维管理平台的数据从客户端-处理端-数据存储都支持
SSL加密，确保数据在传输过程中的安全，不会被恶意篡改。此外，流数据在成功处理后，才会提交对应数据偏移量，确保当异常发生时，数据处理可以从最近一次成功处理的重新开始，从而确保数据零丢失。
#### 数据存储安全
平台底层数据存储采用分布式引擎，每条数据都存储为多个数据备份，当单个数据节点出现故障时，数据可以及时从其他节点恢复，保证数据在存储阶段的安全可靠。
对于数据加密，HDFS和HBase都提供了对应的功能。加密数据会有额外的计算开销，这里只建议对部分关键目录进行加密，这两种加密方式对用户来说都是透明的，只要用户有秘钥的访问权限就能查看对应数据。结构化数据加密：目前结构化数据都由Hive/Impala进行存储，因此直接/user/hive/warehouse/\${database}.db/\${tablename}的对应目录进行加密，Hive端不需要进行额外配置。非结构化数据加密：非结构化数据分为小文件和大文件进行讨论。小文件：存放于HBase中，HBase提供了数据单元（cell）级别的加密，直接可以对部分文件的二进制加密存放。大文件：存放于HDFS中，这部分关键文件可以使用HDFS加密的方式，建立对应的文件加密区，进行存放。
ElasticSearch层面则可以针对索引Index针对不同的人员进行访问控制管理。
#### 数据访问安全
用户基于CMDB中应用系统的管理员来进行访问权限隔离控制，即只有应用的A，B角才可以访问相应系统的数据。通过基于RBCA的权限控制体系，可以进行更加详细的权限划分。
###  关键组件
• 消息系统
平台采用分布式消息系统连接各个模块，消息系统也起到缓存的作用。
• 数据处理系统
数据处理系统能够根据配置的规则抽取数据关键字段，将非结构化的日志转换成结构化数据。抽取关键字段
的好处是可以对关键字段进行统计分析。平台将对关键字段及原始日志进行索引，用户可对关键字段及原
始日志进行搜索。
平台已经配置了常见日志的解析规则，对于平台没有预先配置解析规则的日志，用户可通过后台或Web
页面配置解析规则，抽取关键字段。即使没有抽取关键字段，用户仍然可以通过全文检索搜索日志。平台在日志做索引之前抽取关键字段，提高了检索的速度。
• 日志检索系统
对日志的关键字段及全文做索引，以时间维度将日志存为多个索引文件，方便用户决定保留多长时间的日
志。索引文件采用分布式存储，并且有副本，保障高可用。
对于没有在日志处理系统中抽取关键字段的信息，在日志检索系统中，依然可以通过事后提取的方式完成即
时的字段统计分析工作。
另外，日志检索系统也支持对不同来源的日志做关联分析。
• 前台服务系统
进行日志展现、数据可视化，支持各种统计功能及图表展现，实现流畅的图形用户交互。
• 权限管理系统
平台支持基于角色的权限管理系统，并通过日志产生的主机、应用、标签三个维度对日志进行来源分组，
不同的用户赋予查看不同日志、并基于可见日志范围进行解析、分析、告警等操作的权限。
• RESTful API平台的一些功能通过RESTful API开放用于二次开发。
#  主要应用场景
本次智能运维方法研究聚焦五个方面的研究方法，在实际应用中，多维系统的日志智能规范化方法作为数据处理的基础，实现各类日志异常检测、设备异常定位、业务趋势预警、负载均衡异常检测的实际场景落地。
##  日志异常检测