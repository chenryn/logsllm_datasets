ports.) Note that since these addresses are never used outside the
context of the egress edge switch, we can use any properly formed
IP address.
This address encoding allows the egress edge switch to use
longest-preﬁx matching, over a forwarding table with wildcard ad-
dresses of the form p. ∗ . ∗ . ∗ /8 to do the ﬁnal-hop lookup. This
table needs only one entry per local switch port, and so will ﬁt on
66the cheapest switches. Another key point is that this table is also
the same on every edge switch.
In summary, the NetLord encapsulation uses the destination
MAC address to cover all VMs attached to a single edge switch,
and it uses the destination IP address both to direct the packet to
the NLA on the correct server, and to allow that NLA to deliver the
packet to the correct tenant.
3.6 Switch conﬁguration
Switches require only static conﬁguration to join a NetLord net-
work. These conﬁgurations are set at boot time, and need never
change unless the network topology changes.
The key idea behind edge-switch conﬁguration is that there is
a well-deﬁned set of IP addresses that are reachable via a given
switch port p: encode(p,∗), or more concretely, p.*.*.*/8. There-
fore, when an edge switch boots, either a local process or a man-
agement station simply creates IP forwarding-table entries of the
form hpref ix, port, next_hopi = hp.∗ .∗ .∗ /8, p, p.0.0.1i for
each switch port p. The NLA on each server “owns” the next-hop
IP address p.0.0.1, as described in section 3.7.
These forwarding-table entries can be set up by a switch-local
script, or a management-station script via SNMP or the switch’s
remote console protocol. Since, as noted above, every edge switch
has exactly the same IP forwarding table, this simpliﬁes switch
management, and greatly reduces the chances of misconﬁguration.
All NetLord switches also need to be given their SPAIN VLAN
conﬁgurations. This information comes from the conﬁguration
repository, and is pushed to the switches via SNMP or the remote
console protocol. When there is a signiﬁcant topology change,
the SPAIN controller might need to update these VLAN conﬁgu-
rations, but that should be a rare event.
3.7 NetLord Agent conﬁguration
NetLord Agents need several kinds of conﬁguration information.
We defer discussion, until section 3.9, of how an NLA learns infor-
mation about where remote tenants are in the network. We also
assume that there already is a VM manager that places VMs on
hosts, and so is already distributing VM conﬁguration information
(including tenant IDs) to the hypervisors. Finally, we assume that
when a tenant VM creates a VIF, the VIF parameters become visi-
ble to the hypervisor.
The NLA must learn its own IP address and edge-switch MAC
address. When a NetLord hypervisor boots, it listens to the Link
Layer Discovery Protocol (LLDP - IEEE 802.1AB) messages sent
by the edge switch to which it is connected. An LLDP message
tells the server the switch’s port number p and MAC address. The
NLA then assumes the IP address encode(p, 1) = p.0.0.1 as its
own, and responds to ARP queries for that address from the local
edge switch.
If a server has multiple interfaces, the NLA repeats this process
on all of them. Multiple interfaces of the same server could end up
with the same IP address, because they could be connected to the
same-numbered port on different switches. This is not a problem:
the NetLord agent never actually uses these IP addresses, except to
respond to ARP requests, which can be handled locally to a speciﬁc
interface.
Since the p.0.0.1 address has no meaning beyond the local
switch, an NLA needs a globally-usable address for communica-
tion with repositories, VM managers, and other hypervisors. Net-
Lord reserves an address space for Tenant_ID=1, and each NLA
must obtain an IP address in this address space. Therefore, when
the hypervisor boots, it broadcasts a DHCP request directly over the
L2 fabric, using its hardware MAC address, and the DHCP server
Figure 3: Flow Chart for Packet Send
responds with sufﬁcient information for the NLA to continue with
further operation in the Tenant_ID=1 space. (While this bootstrap
mechanism does require L2 switches to learn the server’s MAC ad-
dress, this is needed only brieﬂy, and so does not create much FIB
pressure.)
3.8 The journey of a NetLord packet
We can now describe how a packet ﬂows through the NetLord
system.
VM operation: VMs send packets out exactly as they would have
without NetLord, because NetLord is fully transparent to VMs. An
outbound packet thus arrives at the local NLA.
Sending NLA operation: The ﬂow chart in Figure 3 shows packet
processing within the sending NLA. ARP messages are handled
by NL-ARP subsystem (section 3.9). For all other packets, the ﬁrst
step is to determine the unique ID of the packet’s destination Virtual
Interface (VIF), hTenant_ID, MACASID, MAC-Addressi.
The Tenant_ID of the receiving VIF is always the same as that
of the sender VIF; NetLord does not allow direct communication
between two tenants, except via the public address space with Ten-
ant_ID=2. The NLA therefore needs to determine the other two
ﬁelds of the VIF: MACASID and MAC-Address.
If the L2 packet is not addressed to the MAC of a designated
virtual router within the VIF’s MACASID (see section 3.2), then
the destination VIF’s MACASID must be the same as the source
VIF’s MACASID, and the destination VIF’s MAC-Address can be
found in the L2 packet header. A tenant that wants to move packets
between two different MAC address spaces can do this by using a
VM with VIFs in either address space as a software router.
If the packet is MAC-addressed to the virtual router, the NLA
extracts the destination IP address from the packet, then obtains
the destination VIF’s hMACASID, MAC-Addressi directly from
the NL-ARP lookup table (which, because it also supports ARP-
Packet from VIFDST-MAC == VirtRouter?Has IP Header?YesRecv-VIF-ID = DST-MAC-AS-ID = SRC-MAC-AS-IDNoYes = NL-ARP-get-MAC(TID, DST-IP)NL-ARPSubSystemDROPSelect SPAIN VLANSuccessEncap with IP HeaderSRC-IP = DST-MAC-AS-IDDST-IP = RemotePort.TIDEncap with ETH HeaderSRC-MAC = IngressSwitch MACDST-MAC = EgressSwitch MACVLAN = SPAIN VLAN = NL-ARP-get-location(Recv-VIF-ID)FailureXMITARP?YesNoNoSuccess67like functions, associates an IP address with a VIF, if that binding
exists).
Once the NLA has the destination VIF ID, it must determine the
destination edge-switch MAC address and server port number. This
done by lookup in a table maintained by the NL-ARP protocol,
which maps from a VIF ID to the correct hMAC-Address, porti
tuple.
Once the egress edge switch MAC address is known, we invoke
SPAIN’s VLAN selection algorithm to select a path for the packet.
If the outgoing packet is a TCP packet, NetLord also extracts the
5-tuple and provides it to SPAIN, so that all packets of a given TCP
ﬂow take the same path and avoid reordering.
At this point, the NLA has all the information it needs to create
an encapsulated packet, using 802.1q and IP headers. The headers
are created as described in section 3.5.
If the destination server is attached to the same switch, the NLA
sets the source MAC address to its own hardware MAC address,
rather than that of the switch. We do this because switches may
drop packets if the source and destination MAC addresses are the
same. Also, it is OK for the switch to learn the MAC addresses of
its directly-connected servers in this case, because these addresses
will not leak out to other switches, and the net increase in FIB pres-
sure is limited. In this case, we also do not need to set a SPAIN
VLAN tag, because there is only one possible path.
Network operation: The packet follows a path, deﬁned by the
SPAIN-provisioned VLAN, through the switched L2 network. All
the switches en route learn the reverse path to the ingress edge
switch, because its MAC address is carried in the packet as the
source MAC.
On receiving the packet, the egress edge switch recognizes the
destination MAC as its own, strips the Ethernet header, and then
looks up the destination IP address in its IP forwarding table to
determine the destination NLA next-hop information, which (by
the construction in section 3.5) gives the switch-port number and
local IP address of the server. The switch might occasionally need
to do an ARP request to ﬁnd the server’s MAC address.
Receiving NLA operation: The receiving NLA decapsulates the
MAC and IP headers, after extracting the Tenant_ID from the en-
capsulating IP destination address, the MACASID from the IP
source, and the VLAN tag from the IP_ID ﬁeld. It can then use
its local information to look up the correct tenant’s VIF, using the
L2 destination address in the inner packet. (A tenant might have
multiple VMs on a server, but each has a unique VIF.) Finally, it
delivers the packet to the tenant VIF.
The NLA also notiﬁes the local SPAIN agent that a packet was
received from the ingress edge switch MAC address, via the copy
VLAN tag carried in the IP.ID ﬁeld. SPAIN needs this information
to monitor the health of its paths. (The original SPAIN VLAN tag
in the 802.1q header has already been stripped by the egress edge
switch, so we need this copy to convey the tag to the receiving
NLA.)
3.9 NL-ARP
An NLA needs to map a VIF to its location, as speciﬁed by an
egress switch MAC address and port number. This mapping com-
bines the functions of IP routing and ARP, and we use a simple
protocol called NL-ARP to maintain an NL-ARP table in each hy-
pervisor. The NLA also uses this table to proxy ARP requests made
by the VMs, rather than letting these create broadcast load.
NL-ARP defaults to a push-based model, rather than the pull-
based model of traditional ARP: a binding of a VIF to its loca-
tion is pushed to all NLAs whenever the binding changes. In sec-
tion 4.2, we argue that the push model reduces overhead, and sim-
pliﬁes modelling and engineering the network.2
The NL-ARP protocol uses three message types: NLA-HERE,
to report a location; NLA-NOTHERE, to correct misinformation;
and NLA-WHERE, to request a location. Messages can either be
broadcast on the L2 fabric, over a special VLAN that reaches all
NLAs, or unicast to one NLA, through the NetLord mechanism
via Tenant_ID=1. NL-ARP broadcasts are sent using the ingress
switch MAC address as the source address, to avoid adding FIB
pressure.
When a new VM is started, or when it migrates to a new server,
the NLA on its current server broadcasts its location using an NLA-
HERE message. Since NL-ARP table entries are never expired, in
the normal case a broadcast is needed only once per VM boot or
migration.
Broadcasts can be lost, leading to either missing or stale entries.
If an entry is missing when a tenant sends a packet, the sending
NLA broadcasts an NLA-WHERE request, and the target NLA
responds with a unicast NLA-HERE. If a stale entry causes mis-
delivery of a packet, the receiving NLA responds with a unicast
NLA-NOTHERE, causing deletion of the stale entry, and a sub-
sequent NLA-WHERE broadcast. (Entries might also be missing
after the unlikely event of a table overﬂow; see section 4.2.)
4. BENEFITS AND DESIGN RATIONALE
Having described NetLord’s architecture and operation, we now
explain its beneﬁts, and the rationale behind some of the important
design decisions. We ﬁrst explain how NetLord meets the goals in
Section 2.1. We then discuss some design alternatives we consid-
ered, and explain how we derived our architecture, subject to the
restrictions imposed by our goals.
4.1 How NetLord meets the goals
Simple and ﬂexible abstractions: NetLord gives each tenant a
simple abstract view of its network: all of its VMs within a MAC
address space (e.g., within an IP subnet) appear to be connected
via a single L2 switch, and all IP subnets appear to be connected
via a single virtual router. The switch and router scale to an ar-
bitrary number of ports, and require no conﬁguration aside from
ACL and QoS support in the virtual router. The tenant can assign
L2 and L3 addresses however it chooses, and for arbitrary reasons.
NetLord’s address space virtualization therefore facilitates devel-
opment of novel network and transport protocols within a cloud
datacenter.
Scale: Number of tenants: By using the p.*.*.*/24 encoding (see
section 3.5), NetLord can support as many as 224 simultaneous
tenants. (We could increase this, if necessary, by using additional
header ﬁelds for the encoding, at the cost of added complexity.)
Scale: Number of VMs: NetLord’s encapsulation scheme insu-
lates the L2 switches from all L2 addresses except those of the
edge switches. Each edge switch also sees a small set of locally-
connected server addresses. The switches are thus insulated from
the much larger number of VM addresses, and even from most of
the server addresses.
Because this limits FIB pressure, a NetLord network should
scale to a huge number of VMs, but it is hard to exactly quantify
this number. The actual number of FIB entries required depends
upon a complex interaction of several factors that are either poorly
understood, that vary widely over small time scales, or that are hard
to quantify. These factors include the trafﬁc matrix and its locality;
2An alternative to NL-ARP is to use a DHT, as in SEATTLE [17].
68packet and ﬂow sizes, application tolerance of lost packets; network
topology; dynamics of load balancing schemes; etc.
Instead, we estimate the number of unique MAC addresses that
NetLord can support, based on a set of wildly pessimistic assump-
tions. We assume a FatTree topology (these scale well and are
amenable to analysis), and we assume a worst-case trafﬁc matrix:
simultaneous all-to-all ﬂows between all VMs, with one packet per
ﬂow pending at every switch on the ﬂow’s path. We assume that the
goal is to never generate a ﬂooded packet due to a capacity miss in
any FIB.
Table 1: NetLord worst-case limits on unique MAC addresses
FIB Sizes
Switch
Radix
24
48
72
94
120
144
16K
108,600
217,200
325,800
425,350
543,000
651,600
32K
153,600
307,200
460,800
601,600
768,000
921,600
64K
217,200
434,400
651,600
850,700
1,086,000
1,303,200
128K
307,200
614,400
921,600
1,203,200
1,536,000
1,843,200
V Rp(F/2) unique MAC addresses, where V is the number of
Based on these assumptions, NetLord can support N =
VMs per physical server, R is the switch radix, and F is the FIB