title:Accountability: definition and relationship to verifiability
author:Ralf K&quot;usters and
Tomasz Truderung and
Andreas Vogt
Accountability:
Deﬁnition and Relationship to Veriﬁability
Ralf Küsters
University of Trier, Germany
Tomasz Truderung
University of Trier, Germany
Andreas Vogt
University of Trier, Germany
PI:EMAIL
PI:EMAIL
PI:EMAIL
ABSTRACT
Many cryptographic tasks and protocols, such as non-repudiation,
contract-signing, voting, auction, identity-based encryption, and
certain forms of secure multi-party computation, involve the use of
(semi-)trusted parties, such as notaries and authorities. It is crucial
that such parties can be held accountable in case they misbehave
as this is a strong incentive for such parties to follow the proto-
col. Unfortunately, there does not exist a general and convincing
deﬁnition of accountability that would allow to assess the level of
accountability a protocol provides.
In this paper, we therefore propose a new, widely applicable def-
inition of accountability, with interpretations both in symbolic and
computational models. Our deﬁnition reveals that accountability
is closely related to veriﬁability, for which we also propose a new
deﬁnition. We prove that veriﬁability can be interpreted as a re-
stricted form of accountability. Our ﬁndings on veriﬁability are of
independent interest.
As a proof of concept, we apply our deﬁnitions to the analy-
sis of protocols for three different tasks: contract-signing, voting,
and auctions. Our analysis unveils some subtleties and unexpected
weaknesses, showing in one case that the protocol is unusable in
practice. However, for this protocol we propose a ﬁx to establish a
reasonable level of accountability.
1.
INTRODUCTION
Many cryptographic tasks and protocols, such as non-repudiation
[48], contract-signing [4], voting [16, 10], auctions [38], identity-
based encryption [19, 20], and certain forms of secure multi-party
computation [24], involve the use of (semi-)trusted parties, such
as notaries and authorities.
It is crucial that such parties can be
held accountable in case they misbehave as this is a strong, in some
cases maybe the main incentive for such parties to follow the pro-
tocol. Unfortunately, there does not exist a general and convincing
deﬁnition of accountability that would allow to assess the level of
accountability a protocol provides. The few existing formulations
of accountability are, for the most part, quite ad hoc and protocol
speciﬁc (see Section 4 for the related work).
The main goal of this paper is therefore to propose a new, general
deﬁnition of accountability and to demonstrate its applicability to a
wide range of cryptographic tasks and protocols. Jumping ahead, it
turns out that accountability is closely related to veriﬁability. This
motivated us to also propose a new deﬁnition for this prominent
security requirement. More precisely, our contributions are as fol-
lows.
This work was partially supported by Deutsche Forschungsge-
meinschaft (DFG) under Grant KU 1434/5-1. An abridged version
was published in [34].
Contribution of this Paper.
In this paper, we propose a general,
model-independent deﬁnition of accountability. We provide inter-
pretations of our deﬁnition both in symbolic (Dolev-Yao style) and
computational (cryptographic) models. While, as usual, analysis in
the symbolic model is simpler and more amenable to tool-support,
the computational deﬁnition gives stronger security guarantees, as
it does not abstract from cryptographic details and allows for a
more-ﬁne grained measure of accountability. As for the symbolic
deﬁnition, we discuss and illustrate how existing analysis tools can
be used to check accountability in some cases.
Our deﬁnition of accountability is applicable to a wide range of
cryptographic tasks and protocols, yet it allows to precisely cap-
ture the level of accountability a protocol provides. This is demon-
strated in three case studies, in which we apply our deﬁnition to
protocols for three important cryptographic tasks: contract-signing,
voting, and auctions. Our analysis of these protocols reveals some
subtleties and unexpected, partly severe weaknesses. For example,
in the auction protocol that we analyze [38], which was explicitly
designed to be of practical use, our analysis shows that if two bid-
ders with two different bids claim to be the winner of the auction,
then, even if it is clear that one of the two bidders misbehaved, a
judge cannot blame a speciﬁc bidder. It even remains open whether
the auctioneer was honest and who actually won the auction. We
propose a ﬁx for this problem and prove that it in fact solves the
problem.
As mentioned, it turns out that accountability is closely related to
veriﬁability. Therefore, we also introduce a new deﬁnition of ver-
iﬁability, again with a symbolic and computational interpretation.
This deﬁnition is interesting in its own right: It is again applicable
to a wide range of cryptographic tasks and protocols. Also, unlike
other deﬁnitions and informal descriptions, our deﬁnition takes a
global view on veriﬁability, centered around the overall goal of a
protocol, rather than focussing on what, in the context of e-voting,
is called individual and universal veriﬁability; although these forms
of veriﬁability can also be captured by our deﬁnition (see Sections 3
and 4).
We show that veriﬁability can be interpreted as a restricted form
of accountability. While, given our deﬁnitions, this relationship is
easy to see, in the literature, accountability and veriﬁability have
not been formally connected before. The relationship offers a
deeper understanding of the two notions and allows to derive state-
ments for veriﬁability from statements for accountability, as illus-
trated by our case studies. We believe that accountability is the
property protocol designers should aim for, not just veriﬁability,
which on its own is often too weak a property in practice: If a pro-
tocol participant (rightly) complains that something went wrong,
then it should be possible to (rightly) hold speciﬁc protocol partic-
ipants accountable for their misbehavior, and by this, resolve the
dispute.
Structure of the Paper. Accountability is deﬁned in Section 2.
In Section 3 we provide our deﬁnition of veriﬁability, along with
the proposition that shows that veriﬁability is implied by account-
ability. Related work is discussed in Section 4. Our case studies
are presented in Sections 5 (voting), 6 (auction), and 7 (contract
signing). More details can be found in the appendix.
2. ACCOUNTABILITY
In this section, we provide our deﬁnition of accountability. As
mentioned in the introduction, we present two variants: a symbolic
and a computational one, which conceptually are closely related.
We start with a deﬁnition of protocols.
2.1 Protocols
In this section, we present a generic deﬁnition of a protocol, suit-
able for the deﬁnition of accountability (and veriﬁability).
We do not ﬁx any speciﬁc symbolic or computational model as
our deﬁnitions do not depend on details of such models. We only
require that the model provides us with a notion of a process which
can perform internal computation and can communicate with other
processes by sending messages via (external) input/output chan-
nels. We also assume that processes can be composed to form new
processes; however, the composition may be subject to certain con-
If π and π(cid:48) are processes, then we write π (cid:107) π(cid:48) for the
ditions.
composition of π and π(cid:48). Moreover, in the symbolic setting, we
assume that a process deﬁnes a set of runs; we assume a set of
runs, rather than a single run, as processes may be nondetermin-
istic. In the computational setting, a process deﬁnes a family of
probability distributions over runs, indexed by the security parame-
ter. The representation of a single run should contain a description
of the corresponding process. In the computational setting, a single
run also contains the security parameter and all random coins. We
will consider only complete runs that cannot be extended, which
in the symbolic setting can include inﬁnite runs. Possible sym-
bolic instances of our framework include the applied π-calculus [2]
and models based on I/O-automata, see, e.g., [28]. In a computa-
tional model, processes would typically be modeled as probabilistic
polynomial-time systems of probabilistic polynomial-time interac-
tive Turing machines (ppt ITMs), see, e.g., [18]. Our case studies
provide concrete examples (see Sections 5 to 7).
For sets I and O of channel names, we denote by Π(I,O) the
set of all processes with external input channels in I and external
output channels in O.
DEFINITION 1
(PROTOCOL). A protocol
is a tuple P =
(Σ, Ch,In,Out,{Πa}a∈Σ,{ ˆΠa}a∈Σ), where:
– Σ = {a1, . . . , an} and Ch are ﬁnite sets, called the set of agents
and channels of P, respectively.
– In and Out are functions from Σ to 2Ch such that Out(a) and
Out(b) are disjoint for all a (cid:54)= b and In(a) and In(b) are disjoint
for all a (cid:54)= b. The sets In(a) and Out(a) are called the set of
(external) input and output channels of agent a, respectively. We
assume that a special channel decisiona ∈ Ch is an element of
Out(a), for every a ∈ Σ, but that it is not an input channel for any
agent.
– Πa ⊆ Π(In(a),Out(a)), for every a ∈ Σ, is called the set of pro-
grams of a. This set contains all programs a can possibly run,
modeling both honest and potential dishonest behavior.
– ˆΠa ⊆ Πa, for every a ∈ Σ, is called the set of honest programs
of a, i.e., the set of programs that a runs if a is honest. Often
this set is a singleton, but sometimes it is convenient to consider
non-singleton sets.
Let P = (Σ, Ch,In,Out,{Πa}a∈Σ,{ ˆΠa}a∈Σ) be a protocol. An
instance of P is a process of the form π = (πa1 (cid:107) . . . (cid:107) πan ) with
πai ∈ Πai. We say that ai is honest in such an instance, if πai ∈ ˆΠai.
A run of P is a run of some instance of P. We say that ai is honest
in a run r, if r is a run of an instance of P with honest ai. A property
γ of P is a subset of the set of all runs of P. By ¬γ we denote the
complement of γ.
2.2 Symbolic and Computational Account-
ability
We now provide a symbolic and a computational deﬁnition of
accountability.
Our deﬁnition of accountability is w.r.t. an agent J of the proto-
col who is supposed to blame protocol participants in case of mis-
behavior. The agent J, which we sometimes refer to as a judge,
can be a “regular” protocol participant or an (external) judge, pos-
sibly provided with additional information by other protocol par-
ticipants; however, J may not necessarily trust these other protocol
participants since they may be dishonest and may provide J with
bogus information.
In order to understand the subtleness of accountability, it is in-
structive to look at a ﬁrst (ﬂawed) deﬁnition of accountability and
its possible interpretations, inspired by informal statements about
accountability in the literature.
(i) (fairness) J (almost) never blames protocol participants who
are honest, i.e., run their honest program.
(ii) (completeness) If in a protocol run participants “misbehave”,
then J blames those participants.
While the fairness condition is convincing and clear, this is not the
case for the completeness condition. First, the question is what
“misbehavior” means.
It could be interpreted as a behavior that
does not correspond to any honest behavior. However, this inter-
pretation is much too strong. No protocol would satisfy it, because
this includes misbehavior that is impossible to be observed by any
other party and misbehavior that is completely “harmless” and “ir-
relevant”. For example, if, in addition to the messages a party A
is supposed to send to another party B, A also sends some harm-
less message “hello”, say, then B can observe this misbehavior, but
cannot convince J of any misbehavior. This example also shows
that interpreting “misbehavior” as dishonest behavior observable
by honest parties, and hence, misbehavior that, at least to some ex-
tent, affects these parties, does not work either. In fact, a complete-
ness condition based on this notion of “misbehavior” would again
deem basically all non-trivial protocols insecure w.r.t. accountabil-
ity. More importantly, this completeness condition misses the main
point: Misbehavior that cannot be observed by any honest party
may still be very relevant and harmful. We therefore advocate an
interpretation that circles around the desired goals of a protocol.
Informally speaking, our deﬁnition of accountability reads as
follows:
(i) (fairness) J (almost) never blames protocol participants who
are honest, i.e., run their honest program.
(ii) (completeness, goal centered) If, in a run, some desired goal
of the protocol is not met—due to the misbehavior of one or
more protocol participants—then J blames those participants
who misbehaved, or at least some of them (see below).
For example, for voting protocols a desired goal could be that the
published result of the election corresponds to the actual votes cast
by the voters. The completeness condition now guarantees that if
2
in a run of the protocol this is not the case (a fact that must be
due to the misbehavior of one or more protocol participants), then
one or more participants are held accountable by J; by the fairness
condition they are rightly held accountable. In case of auctions,
a desired goal could be that the announced winner is in fact the
winner of the auction; if this is not so in a run, by the completeness
condition some participant(s), who misbehaved, will be blamed.
Desired goals, as the above, will be a parameter of our deﬁnition.
The informal completeness condition above leaves open who ex-
actly should be blamed. This could be ﬁxed in a speciﬁc way.
However, this would merely provide a black and white picture, and
either set the bar too high or too low for many protocols. For exam-
ple, it is desirable that the judge, whenever a desired goal of a pro-
tocol is not met, blames all misbehaving parties. This, as explained
above, is usually not possible (e.g., if for a dishonest party the de-
viation from the protocol consists in sending a harmless “hello”
message). So, this sets the bar too high for practically every proto-
col. Alternatively, one could require that at least some misbehaving
parties can be blamed individually (individual accountability). Be-
ing able to rightly blame individual parties, rather than, say, just a
group of parties among which at least one misbehaved, is impor-
tant in practice, since only this might have actual consequences for
a misbehaving party. However, as illustrated by our case studies,
protocols often fail to achieve individual accountability. One could
set the bar lower and only require that a group of parties is blamed
among which at least one misbehaved. But this is often unsatisfying
in practice. Altogether, rather than ﬁxing the level of accountabil-
ity protocols are supposed to provide up front, it is more reasonable
to have a language in which this can be described precisely, allow-
ing to compare protocols and tell apart weak protocols from strong
ones.
To this end, below we introduce what we call accountability
properties, which are sets of what we call accountability con-
straints. We also allow the judge to state quite detailed “verdicts”.
Formally, a verdict is a positive boolean formula ψ built from
propositions of the form dis(a), for an agent a, where dis(a) is
intended to express that a misbehaved (behaved dishonestly), i.e.,
did not follow the prescribed protocol. Let us look at some ex-
If the judge states dis(a) ∨ dis(b), then this expresses
amples.
the judge’s belief that a or b misbehaved.
(In case of a fair
judge, this implies that at least one of the two parties indeed mis-
behaved.) Another example: