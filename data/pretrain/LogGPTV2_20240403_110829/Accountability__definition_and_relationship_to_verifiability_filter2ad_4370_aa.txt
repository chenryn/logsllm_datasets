# Accountability: Definition and Relationship to Verifiability

**Authors:**
- Ralf Küsters, University of Trier, Germany
- Tomasz Truderung, University of Trier, Germany
- Andreas Vogt, University of Trier, Germany

**Contact Information:**
- Ralf Küsters: [PI:EMAIL]
- Tomasz Truderung: [PI:EMAIL]
- Andreas Vogt: [PI:EMAIL]

## Abstract

Many cryptographic tasks and protocols, such as non-repudiation, contract-signing, voting, auctions, identity-based encryption, and certain forms of secure multi-party computation, involve the use of (semi-)trusted parties, such as notaries and authorities. It is crucial that these parties can be held accountable in case they misbehave, as this serves as a strong incentive for them to follow the protocol. Unfortunately, there is no general and convincing definition of accountability that allows for the assessment of the level of accountability provided by a protocol.

In this paper, we propose a new, widely applicable definition of accountability, with interpretations both in symbolic and computational models. Our definition reveals that accountability is closely related to verifiability, for which we also propose a new definition. We prove that veriﬁability can be interpreted as a restricted form of accountability. Our findings on verifiability are of independent interest.

As a proof of concept, we apply our definitions to the analysis of protocols for three different tasks: contract-signing, voting, and auctions. Our analysis uncovers some subtleties and unexpected weaknesses, showing in one case that the protocol is unusable in practice. However, for this protocol, we propose a fix to establish a reasonable level of accountability.

## 1. Introduction

Many cryptographic tasks and protocols, such as non-repudiation [48], contract-signing [4], voting [16, 10], auctions [38], identity-based encryption [19, 20], and certain forms of secure multi-party computation [24], involve the use of (semi-)trusted parties, such as notaries and authorities. It is crucial that these parties can be held accountable if they misbehave, as this serves as a strong, and in some cases, the main incentive for them to follow the protocol. Unfortunately, there is no general and convincing definition of accountability that would allow for the assessment of the level of accountability a protocol provides. The few existing formulations of accountability are, for the most part, quite ad hoc and protocol-specific (see Section 4 for related work).

The primary goal of this paper is to propose a new, general definition of accountability and to demonstrate its applicability to a wide range of cryptographic tasks and protocols. As it turns out, accountability is closely related to verifiability, which motivated us to also propose a new definition for this prominent security requirement. Our contributions are as follows:

- **General Definition of Accountability:** We propose a model-independent definition of accountability, providing interpretations both in symbolic (Dolev-Yao style) and computational (cryptographic) models.
- **Applicability and Case Studies:** We apply our definition to protocols for three important cryptographic tasks: contract-signing, voting, and auctions. Our analysis reveals some subtleties and unexpected, sometimes severe, weaknesses.
- **New Definition of Verifiability:** We introduce a new definition of verifiability, again with symbolic and computational interpretations. This definition takes a global view on verifiability, centered around the overall goal of a protocol, rather than focusing on individual and universal verifiability.
- **Relationship Between Accountability and Verifiability:** We show that verifiability can be interpreted as a restricted form of accountability. This relationship offers a deeper understanding of the two notions and allows for deriving statements about verifiability from statements about accountability.

This work was partially supported by Deutsche Forschungs-gemeinschaft (DFG) under Grant KU 1434/5-1. An abridged version was published in [34].

## 2. Accountability

In this section, we provide our definition of accountability. We present two variants: a symbolic and a computational one, which are conceptually closely related. We start with a definition of protocols.

### 2.1 Protocols

We present a generic definition of a protocol suitable for defining accountability (and verifiability). We do not fix any specific symbolic or computational model, as our definitions do not depend on the details of such models. We only require that the model provides a notion of a process that can perform internal computation and communicate with other processes via external input/output channels. We also assume that processes can be composed to form new processes, subject to certain conditions.

If \(\pi\) and \(\pi'\) are processes, then \(\pi \parallel \pi'\) denotes their composition. In the symbolic setting, we assume that a process defines a set of runs; in the computational setting, a process defines a family of probability distributions over runs, indexed by the security parameter. The representation of a single run should include a description of the corresponding process. In the computational setting, a single run also includes the security parameter and all random coins. We consider only complete runs that cannot be extended, which in the symbolic setting can include infinite runs.

Possible symbolic instances of our framework include the applied \(\pi\)-calculus [2] and models based on I/O-automata, see, e.g., [28]. In a computational model, processes would typically be modeled as probabilistic polynomial-time systems of probabilistic polynomial-time interactive Turing machines (ppt ITMs), see, e.g., [18]. Our case studies provide concrete examples (see Sections 5 to 7).

For sets \(I\) and \(O\) of channel names, we denote by \(\Pi(I, O)\) the set of all processes with external input channels in \(I\) and external output channels in \(O\).

**Definition 1 (Protocol):** A protocol is a tuple \(P = (\Sigma, Ch, In, Out, \{\Pi_a\}_{a \in \Sigma}, \{\hat{\Pi}_a\}_{a \in \Sigma})\), where:
- \(\Sigma = \{a_1, \ldots, a_n\}\) and \(Ch\) are finite sets, called the set of agents and channels of \(P\), respectively.
- \(In\) and \(Out\) are functions from \(\Sigma\) to \(2^Ch\) such that \(Out(a)\) and \(Out(b)\) are disjoint for all \(a \neq b\), and \(In(a)\) and \(In(b)\) are disjoint for all \(a \neq b\). The sets \(In(a)\) and \(Out(a)\) are called the set of (external) input and output channels of agent \(a\), respectively. We assume that a special channel \(decision_a \in Ch\) is an element of \(Out(a)\) for every \(a \in \Sigma\), but it is not an input channel for any agent.
- \(\Pi_a \subseteq \Pi(In(a), Out(a))\), for every \(a \in \Sigma\), is called the set of programs of \(a\). This set contains all programs \(a\) can possibly run, modeling both honest and potential dishonest behavior.
- \(\hat{\Pi}_a \subseteq \Pi_a\), for every \(a \in \Sigma\), is called the set of honest programs of \(a\), i.e., the set of programs that \(a\) runs if \(a\) is honest. Often this set is a singleton, but sometimes it is convenient to consider non-singleton sets.

Let \(P = (\Sigma, Ch, In, Out, \{\Pi_a\}_{a \in \Sigma}, \{\hat{\Pi}_a\}_{a \in \Sigma})\) be a protocol. An instance of \(P\) is a process of the form \(\pi = (\pi_{a_1} \parallel \ldots \parallel \pi_{a_n})\) with \(\pi_{a_i} \in \Pi_{a_i}\). We say that \(a_i\) is honest in such an instance if \(\pi_{a_i} \in \hat{\Pi}_{a_i}\). A run of \(P\) is a run of some instance of \(P\). We say that \(a_i\) is honest in a run \(r\) if \(r\) is a run of an instance of \(P\) with honest \(a_i\). A property \(\gamma\) of \(P\) is a subset of the set of all runs of \(P\). By \(\neg \gamma\) we denote the complement of \(\gamma\).

### 2.2 Symbolic and Computational Accountability

We now provide a symbolic and a computational definition of accountability.

Our definition of accountability is with respect to an agent \(J\) of the protocol who is supposed to blame protocol participants in case of misbehavior. The agent \(J\), referred to as a judge, can be a "regular" protocol participant or an (external) judge, possibly provided with additional information by other protocol participants. However, \(J\) may not necessarily trust these other participants since they may be dishonest and provide \(J\) with false information.

To understand the subtlety of accountability, it is instructive to look at a first (flawed) definition of accountability and its possible interpretations, inspired by informal statements about accountability in the literature.

1. **(Fairness)** \(J\) almost never blames protocol participants who are honest, i.e., run their honest program.
2. **(Completeness)** If, in a protocol run, participants "misbehave," then \(J\) blames those participants.

While the fairness condition is convincing and clear, the completeness condition is problematic. The question is what "misbehavior" means. It could be interpreted as behavior that does not correspond to any honest behavior. However, this interpretation is too strong. No protocol would satisfy it because it includes misbehavior that is impossible to observe by any other party and misbehavior that is completely harmless and irrelevant. For example, if, in addition to the messages a party \(A\) is supposed to send to another party \(B\), \(A\) also sends a harmless message "hello," \(B\) can observe this misbehavior but cannot convince \(J\) of any misbehavior. This example shows that interpreting "misbehavior" as dishonest behavior observable by honest parties, and hence, misbehavior that, at least to some extent, affects these parties, does not work either. In fact, a completeness condition based on this notion of "misbehavior" would deem most non-trivial protocols insecure with respect to accountability. More importantly, this completeness condition misses the main point: Misbehavior that cannot be observed by any honest party may still be very relevant and harmful. We therefore advocate an interpretation that centers around the desired goals of a protocol.

Informally, our definition of accountability reads as follows:

1. **(Fairness)** \(J\) almost never blames protocol participants who are honest, i.e., run their honest program.
2. **(Completeness, Goal-Centered)** If, in a run, some desired goal of the protocol is not met—due to the misbehavior of one or more protocol participants—then \(J\) blames those participants who misbehaved, or at least some of them (see below).

For example, for voting protocols, a desired goal could be that the published result of the election corresponds to the actual votes cast by the voters. The completeness condition now guarantees that if, in a run of the protocol, this is not the case (a fact that must be due to the misbehavior of one or more protocol participants), then one or more participants are held accountable by \(J\); by the fairness condition, they are rightly held accountable. In the case of auctions, a desired goal could be that the announced winner is indeed the winner of the auction; if this is not so in a run, by the completeness condition, some participant(s) who misbehaved will be blamed.

Desired goals, as mentioned above, will be a parameter of our definition. The informal completeness condition leaves open who exactly should be blamed. This could be fixed in a specific way. However, this would provide a black-and-white picture and either set the bar too high or too low for many protocols. For example, it is desirable that the judge, whenever a desired goal of a protocol is not met, blames all misbehaving parties. This, as explained above, is usually not possible (e.g., if for a dishonest party, the deviation from the protocol consists in sending a harmless "hello" message). So, this sets the bar too high for practically every protocol. Alternatively, one could require that at least some misbehaving parties can be blamed individually (individual accountability). Being able to rightly blame individual parties, rather than just a group of parties among which at least one misbehaved, is important in practice, as it may have actual consequences for a misbehaving party. However, as illustrated by our case studies, protocols often fail to achieve individual accountability. One could set the bar lower and only require that a group of parties is blamed among which at least one misbehaved. But this is often unsatisfying in practice. Altogether, rather than fixing the level of accountability protocols are supposed to provide upfront, it is more reasonable to have a language in which this can be described precisely, allowing for the comparison of protocols and distinguishing weak ones from strong ones.

To this end, we introduce what we call accountability properties, which are sets of what we call accountability constraints. We also allow the judge to state detailed "verdicts." Formally, a verdict is a positive Boolean formula \(\psi\) built from propositions of the form \(\text{dis}(a)\), for an agent \(a\), where \(\text{dis}(a)\) expresses that \(a\) misbehaved (behaved dishonestly), i.e., did not follow the prescribed protocol. 

For example, if the judge states \(\text{dis}(a) \lor \text{dis}(b)\), this expresses the judge's belief that \(a\) or \(b\) misbehaved. (In the case of a fair judge, this implies that at least one of the two parties indeed misbehaved.) Another example: