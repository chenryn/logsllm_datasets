paring generic over-the-air attacks with adapted over-the-
air attacks.
Figure 9: WERs for over-the-air attacks plotted over the dis-
tance between microphone and speaker for ùëÄ = 8192 with
and without hearing thresholds.
ùëá60 = 0.42 s and varied the distance from 1 m to 6 m for ùëÄ = 8192
with and without hearing thresholds.
In general, we find that the WER increases with increasing dis-
tance. Nevertheless, starting from a distance of approximately 2 m,
the WER does not increase as rapidly as for smaller distances if we
use hearing thresholds. In cases where no hearing thresholds are
used, the WER even decreases for larger distances.
4.3.4 Varying Audio Content. In Table 4, we evaluated the effect
of varying audio contents of the original audio samples. For this,
we used speech, music, and bird chirping data. Using speech audio
samples for the attack results in the best WERs.
The average SNRseg indicates that most distortions have to be
added to the original audio samples for bird chirping while we
achieve better results for music and speech data.
4.3.5 Adaptive Attack. In the following, we compare the generic
attack, where the attacker has no prior knowledge about the attack
setup, with an adapted version of the attack. Note that the generic
attack is the more powerful attack compared to the adapted version,
as it requires no access nor any information about the room where
the attack is conducted.
For the evaluation, we used ùêªùúÉadp and ùêªùúÉgen in Table 1, combined
with a measured RIR ‚Ñéreal and a simulated RIR ‚Ñégen for ‚Ñétest in
Algorithm 1. ‚Ñégen was drawn once at the beginning of the algorithm
from the same distribution, described via ùêªùúÉgen. ‚Ñéreal is a measured
RIR, obtained from the recording setup that is actually used during
the attack. Consequently, the version with ùêªùúÉgen and ‚Ñégen does not
use any prior knowledge of the room or the recording setup, while
the version with ùêªùúÉadp and ‚Ñéreal is tailored to the room. Surprisingly,
the generic version clearly outperforms the adapted versions (ùêªùúÉadp,
‚Ñéreal) in Table 5, and we were able to find fully successful adversarial
Music
Speech
Birds
WER
61.1 % 1/20
63.2 % 2/20
+ 2.1 %
AEs WER
42.2 %
65.0 %
+ 22.8 %
AEs WER
71.7 %
5/20
2/20
84.5 %
+ 12.8 %
AEs
0/20
2/20
ùêªùúÉgen, ‚Ñégen
ùêªùúÉadp, ‚Ñéadp
Œî in WER
Table 6: WER and number of successful adversarial exam-
ples for generic over-the-air attacks with and without di-
rect line-of-sight in varying rooms based on speech data
for ùëÄ = 8192.
Lecture
Room
Meeting
Room
Office
WER
40.0 %
71.3 %
+ 31.3 %
AEs WER
55.3 %
2/20
0/20
62.0 %
+ 6.7 %
AEs WER
74.0 %
1/20
0/20
82.7 %
+ 8,7 %
AEs
1/20
1/20
w/
line-of-sight
w/o line-of-sight
Œî in WER
examples for those cases, i. e., adversarial examples with a WER
of 0 %.
As a consequence, for an attacker, it is not only unnecessary to
acquire prior knowledge about the room characteristics, but the
likelihood of success is even higher if a generic attack is chosen.
4.3.6 Varying Room Conditions. To evaluate the adversarial exam-
ples in varying rooms, we chose three rooms of differing sizes: a
lecture room with approximately 77 m2, a meeting room with ap-
proximately 38 m2, and an office with approximately 31 m2. Layout
plans of the rooms are shown in Appendix A, including positions
of the speaker and microphone for all recording setups and the
measured reverberation time.
Direct Line-of-Sight Attack. The first attacks were conducted
with a direct line-of-sight between the microphone and the speaker.
The results are shown in Table 6. Even though the results vary
depending on the room, the WERs remain approximately in the
same range as the experiments with varying reverberation times
in Table 2 would indicate. Surprisingly, the room with the highest
reverberation time, the lecture room, actually gave the best results.
Overall, the results show that our generic adversarial examples
remain robust for different kinds of rooms and setups and that it
Imperio: Robust Over-the-Air Adversarial Examples for Automatic Speech Recognition Systems
ACSAC 2020, December 7‚Äì11, 2020, Austin, USA
is sufficient to calculate one version of an adversarial example to
cover a wide range of rooms (i.e., the attack is transferable).
No Line-of-Sight Attack. For the rooms in Table 6, we also per-
formed experiments where no line-of-sight between the micro-
phone and the speaker exists by blocking the direct over-the-air
connection with different kinds of furniture. As a consequence, not
the direct sound, but a reflected version of the audio is recorded.
An implication is that these attacks could be carried out without
being visible to people in the vicinity of the ASR input microphone.
We tested different scenarios for our setup: In the lecture and the
meeting room, the source and the receiver were separated by a table
by simply placing the speaker under the table. In the office, the
speaker was even placed outside the room. For this recording setup,
the door between the rooms was left open. For all other setups, the
doors of the respective rooms were closed. A detailed description
of the no line-of-sight setups is given in Appendix A.
In cases where no line-of-sight exists, the distortions that occur
through the transmission can be considered more complex, and
consequentially, a prediction of the recorded audio signal is hard. A
blocked line-of-sight will most likely decrease the WER, but it is in
general possible to find adversarial examples with 0 % WER, even
where the source was placed outside the room. This again shows
that our generic version of the attack can successfully model a wide
range of acoustic environments simultaneously, without any prior
knowledge about the room setup.
5 RELATED WORK
In addition to the prior work that we have already discussed, we
want to provide a broader and more detailed overview of related
work in the following.
Generally speaking, adversarial attacks on ASR systems focus
either on hiding a target transcription [1, 7] or on obfuscating the
original transcription [12]. Almost all previous works on attacks
against ASR systems did not focus on real-world attacks [7, 40] or
were only successful for simulated over-the-air attacks [27].
Carlini et al. [7] have shown that targeted attacks against HMM-
only ASR systems are possible. They use an inverse feature extrac-
tion to create adversarial audio samples. However, the resulting au-
dio samples are not intelligible by humans in most cases and may be
considered as noise, but may make thoughtful listeners suspicious
once they are alerted to its hidden voice command. An approach to
overcome this limitation was proposed by Zhang et al. [40]. They
have shown that an adversary can hide a transcription by utilizing
non-linearities of microphones to modulate the baseband audio
signals with ultrasound above 20 kHz, which they inject into the
environment. The main downside of this attack, hence, is that the
attacker needs to place an ultrasound transmitter in the vicinity
of the voice-controlled system under attack and that the attacker
needs to retrieve information from the audio signal, recorded with
the specific microphone, which is costly in practice and tailors the
attack to one specific setup. Song and Mittael [31] and Roy et al. [28]
introduced similar ultrasound-based attacks that are not adversarial
examples, but rather interact with the ASR system in a frequency
range inaudible to humans. Nevertheless, for the attack hours of
audio recordings are required to adjust the attack to the setup [31]
or specially designed speakers are necessary [28].
Carlini and Wagner [9] published a general targeted attack on
ASR systems using CTC-loss. The attacker creates the optimal at-
tack via gradient-descent-based minimization [8] (similar to previ-
ous adversarial attacks on image classification), but the adversarial
examples are fed directly into the recognizer. CommanderSong [39]
is evaluated against Kaldi and uses backpropagation to find an ad-
versarial example. However, the very limited and non-systematic
over-the-air attack highly depends on the speakers and recording
devices, as the attack parameters have to be adjusted, especially for
these components. Yakura and Sakuma [37] published a technical
report, which describes an algorithm to create over-the-air robust
adversarial examples, but with the limitation that it is necessary
to have physical access to the room where the attack takes place.
Also, they did not evaluate their room-dependent results for vary-
ing room conditions and were unable to create generic adversarial
examples systematically. Concurrently, Szuley and Kolter [32] also
published a work on room-dependent robust adversarial examples,
which worked under constraints given by a psychoacoustic model.
However, their adversarial examples only work in an anechoic
chamber, a room specifically designed to eliminate the effect of an
RIR. The attack can, therefore, not be compared with a real-world
scenario, as the anechoic chamber effectively reproduces the effect
of directly feeding the attack into the ASR system, which is never
given in real room environments. Li et al. [20] published a work to
obfuscate Amazon‚Äôs Alexa wake word via specifically crafted music.
However, their approach was not successful at creating targeted
adversarial examples that work over the air.
Alzantot et al. [3] proposed a black-box attack, which does not
require knowledge about the model. For this, the authors use a
genetic algorithm to create their adversarial examples for a keyword
spotting system, which differs from general speech recognition due
to a much simpler architecture and far fewer possible recognition
outcomes. For DeepSpeech [16] and Kaldi, Khare et al. [30] proposed
a black-box attack based on evolutionary optimization, and also
Taori et al. [33] present a similar approach in their paper.
Sch√∂nherr et al. [29] published an approach where psychoacous-
tic modeling, borrowed from the MP3 compression algorithm, was
used to re-shape the perturbations of the adversarial examples in
such a way as to hide the changes to the original audio below
the human hearing thresholds. However, the adversarial examples
created in that work need to be fed into the recognizer directly.
Concurrently, Abdullah et al. [1] showed a black-box attack in
which psychoacoustics is used to calculate adversarial examples
empirically. Their approach focuses on over-the-air attacks, but in
many cases, humans can perceive the hidden message once they
are alerted to its content. Note that our adversarial examples are
conceptually completely different, as we use a target audio file,
where we embed the target transcription via backpropagation. The
changes, therefore, sound like random noise (see examples available
at http://imperio.adversarial-attacks.net). With Abdullah et al.‚Äôs
approach, an audio file with the spoken target text is taken and
changed in a way to be unintelligible for unbiased human listen-
ers, but not for humans aware of the target transcription. This is
equally the case for Chen et al.‚Äôs [11] recently published black-box
attack against several commercial devices, where humans can per-
ceive the target text. As an extension of Carlini‚Äôs and Wagner‚Äôs
ACSAC 2020, December 7‚Äì11, 2020, Austin, USA
Lea Sch√∂nherr, Thorsten Eisenhofer, Steffen Zeiler, Thorsten Holz, and Dorothea Kolossa
attack [9], Qin et al. [27] introduced the first implementation of RIR-
independent adversarial examples. Unfortunately, their approach
only worked in a simulated environment and not for real over-the-
air attacks, but the authors also utilize psychoacoustics to limit
the perturbations.
In the visual domain, Evtimov et al. [14] showed one of the first
real-world adversarial attacks. They created and printed stickers,
which can be used to obfuscate traffic signs. For humans, the stick-
ers are visible. However, they seem very inconspicuous and could
possibly fool autonomous cars. Athalye et al. [4] presented another
real-world adversarial perturbation on a 3D-printed turtle, which is
recognized as a rifle from almost every point of view. The algorithm
to create this 3D object not only minimizes the distortion for one
image but for all possible projections of a 3D object into a 2D image,
hence producing a robust adversarial example.
Recently and independently, Chen et al. [10] showed a first over-
the-air attack. Their attack was evaluated against DeepSpeech [16].
Note that we focus on generic adversarial examples that work over-
the-air for different kinds of rooms. Additionally, we used Kaldi, a
hybrid DNN-HMM ASR system that works in a fundamentally dif-
ferent manner than the end-to-end approach of DeepSpeech, which
is attacked by Chen et al.
Our approach is the first targeted attack that provides room-
independent, robust adversarial examples against a hybrid ASR
system. We demonstrate how to generate adversarial examples that
are mostly unaffected by the environment, as ascertained by verify-
ing their success in a broad range of room characteristics. We utilize
the same psychoacoustics-based approach proposed by Sch√∂nherr
et al. [29] to limit the perturbations of the audio signal to remain
under, or at least close to, the human thresholds of hearing, and
we show that the examples remain robust to playback over the air.
The perturbations that remain audible in the adversarial examples
that we create, are non-structured noise, so that human listeners
cannot perceive any content related to the targeted recognition
output. Hence, our attack can be successful in a broad range of