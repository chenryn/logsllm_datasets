for large-scale, passive privacy breaches. We develop such
an attack in Section 5.2.
Backstrom et al. also describe passive attacks, in which
a small coalition of users discover their location in the
anonymized graph by utilizing the knowledge of the network
structure around them. This attack is realistic, but again,
only works on a small scale: the colluding users can only
compromise the privacy of some of the users who are already
their friends.
By contrast, our attack does not require creation of a large
number of sybil nodes, and—as shown by our experiments
on real-world online social networks—can be successfully
deployed on a very large scale.
Defenses. Existing privacy protection mechanisms for social
networks are only effective against very restricted adver-
saries and have been evaluated on small, simulated networks
whose characteristics are different from real social networks.
For example, Zheleva and Getoor give several strategies for
preventing link re-identiﬁcation [79], but the model ignores
auxiliary information that may be available to the attacker.
An unusual attempt to prevent network operators from
capitalizing on user-provided data appears in [31]. It in-
volves scrambling the proﬁles when they are sent to the
server and client-side unscrambling when a friend’s proﬁle
is viewed. Building and running such a system involves
constant reverse-engineering of communication between the
client and the server. Further, all of a user’s friends need
to use the system, ﬂatly contradicting the claim of incre-
mental deployability. A similar idea appears in [46], with
a more sound architecture based on a server-side Facebook
application. Both approaches severely cripple social-network
functionality because almost any non-trivial action other
than viewing another user’s proﬁle or messages requires the
server to manipulate the data in a way which is not possible
under encryption.
Anonymity is a popular approach to protecting privacy.
Felt and Evans propose a system where applications see
randomized tokens representing users instead of actual iden-
tiﬁers [24]. Frikken and Golle show how to compute an
anonymous graph from pieces held by different participants
in order to perform privacy-preserving social-network anal-
ysis [27]. Kerschbaum and Schaad additionally enable par-
ticipants to track their position in the anonymous graph [38].
Several papers proposed variants of k-anonymity for so-
cial networks. For example, Hay et al. require nodes to
be automorphically equivalent [34], i.e., there must exist
automorphisms of the graph that map each of k nodes to one
another. This is an extremely strong structural requirement,
which is achieved only against severely restricted adver-
saries: in one model, the attacker only has information about
degree sequences around his target node; in another, partial
knowledge of the structure in the vicinity of the target. The
technique appears to work only if the average degree is low,
ruling out most online social networks.
Liu and Terzi consider node re-identiﬁcation assuming
that the adversary’s auxiliary information consists only of
node degrees [45]. There is no clear motivation for this
restriction. Campan and Truta propose metrics for the infor-
mation loss caused by edge addition and deletion and apply
k-anonymity to node attributes as well as neighborhood
structure [12]. Zhou and Pei assume that
the adversary
knows the exact 1-neighborhood of the target node [80].
The anonymization algorithm attempts to make this 1-
neighborhood isomorphic to k − 1 other 1-neighborhoods
via edge addition. The experiments are performed on an
undirected network with average degree 4 (an order of
magnitude lower than that
in real social networks) and
already require increasing the number of edges by 6%. The
number of edges to be added and the computational effort
are likely to rise sharply with the average degree.
The fundamental problem with k-anonymity is that it is
a syntactic property which may not provide any privacy
even when satisﬁed (e.g., if all k isomorphic neighborhoods
have the same value of some sensitive attributes). Crucially,
all of these defenses impose arbitrary restrictions on the
information available to the adversary and make arbitrary
assumptions about the properties of the social network.
We argue that the auxiliary information which is likely to
be available to the attacker is global in nature (e.g., another
social network with partially overlapping membership) and
not restricted to the neighborhood of a single node. In the
rest of this paper, we show how this information, even
if very noisy, can be used for large-scale re-identiﬁcation.
Existing models fail to capture self-reinforcing, feedback-
based attacks,
in which re-identiﬁcation of some nodes
provides the attacker with more auxiliary information, which
is then used for further re-identiﬁcation. Development of a
model for such attacks is our primary contribution.
4. Model and Deﬁnitions
4.1. Social network
A social network S consists of (1) a directed graph
G = (V, E), and (2) a set of attributes X for each node
in V (for instance, name,
telephone number, etc.) and
a set of attributes Y for each edge in E (for instance,
type of relationship). The model is agnostic as to whether
attributes accurately reﬂect real-world identities or not. We
treat attributes as atomic values from a discrete domain; this
is important for our formal deﬁnition of privacy breach (Def-
inition 3 below). Real-valued attributes must be discretized.
Where speciﬁed, we will also represent edges as attributes
in Y taking values in {0, 1}.
In addition to the explicit attributes, some privacy policies
may be concerned with implicit attributes, i.e., properties of
176
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:17:29 UTC from IEEE Xplore.  Restrictions apply. 
a node or an edge that are based purely on the graph struc-
ture. For example, node degree can be a sensitive implicit
attribute. Implicit attributes may be leaked without disclos-
ing any explicit attributes. For example, if the adversary re-
identiﬁes a subset of nodes in an anonymized graph, none
of which are adjacent, he learns the degrees of these nodes
without breaking edge privacy. Which implicit attributes
should be protected depends on the speciﬁc network.
4.2. Data release
Our model of the data release process focuses on what
types of data are released and how the data is sanitized (if
at all), and abstracts away from the procedural distinctions
such as whether the data is available in bulk or obtained
by crawling the network. As discussed in Section 2, social-
network data are routinely released to advertisers, applica-
tion developers, and researchers. Advertisers are often given
access to the entire graph in a (presumably) anonymized
form and a limited number of relevant attributes for each
node. Application developers, in current practice, get access
to a subgraph via user opt-in and most or all of the attributes
within this subgraph. This typically includes the identifying
attributes, even if they are not essential for the application’s
functionality [24]. Researchers may receive the entire graph
or a subgraph (up to the discretion of the network owner)
and a limited set of non-identifying attributes.
“Anonymization” is modeled by publishing only a subset
of attributes. Unlike na¨ıve approaches such as k-anonymity,
we do not distinguish identifying and non-identifying at-
tributes (any attribute can be identifying if it happens
to be known to the adversary as part of his auxiliary
information). Suppressed attributes are not limited to the
demographic quasi-identiﬁers a priori; we simply assume
that the published attributes by themselves are insufﬁcient
for re-identiﬁcation. In Section 4.4, we explain the (indirect)
connection between preventing node re-identiﬁcation and in-
tuitive “privacy.” In terms of entropy, most of the information
in the released graph resides in the edges, and this is what
our de-anonymization algorithm will exploit.
The data release process may involve perturbation or
sanitization that changes the graph structure in some way to
make re-identiﬁcation attacks harder. As we argued in Sec-
tion 3, deterministic methods that attempt to make different
nodes look identical do not work on realistic networks. Other
defenses are based on injecting random noise into the graph
structure. The most promising one is link prediction [44],
which produces plausible fake edges by exploiting the fact
that edges in social-network graphs have a high clustering
coefﬁcient. (We stress that link prediction is far beyond the
existing sanitization techniques, which mostly rely on simple
removal of identiﬁers.) The experiments in Section 6.2
show that our algorithm is robust to injected noise, whether
resulting from link prediction or not.
We model the data sanitization and release process as
follows. First, select a subset of nodes, Vsan ⊂ V , and
subsets Xsan ⊆ X , Ysan ⊆ Y of node and edge attributes
to be released. Second, compute the induced subgraph on
Vsan. For simplicity, we do not model more complex criteria
for releasing edge, e.g., based on edge attributes. Third,
remove some edges and add fake edges. Release Ssan =
(Vsan, Esan, {X(v)∀v ∈ Vsan, X ∈ Xsan}, {Y (e)∀e ∈
Esan, Y ∈ Ysan}), i.e., a sanitized subset of nodes and
edges with the corresponding attributes.
4.3. Threat model
As described in Section 2, network owners release
anonymized and possibly sanitized network graphs to com-
mercial partners and academic researchers. Therefore, we
take it for granted that the attacker will have access to such
data. The main question we answer in the rest of this paper
is: can sensitive information about speciﬁc individuals be
extracted from anonymized social-network graphs?
Attack scenarios. Attackers fall into different categories
depending on their capabilities and goals. The strongest
adversary is a government-level agency interested in global
surveillance. Such an adversary can be assumed to already
have access to a large auxiliary network Saux (see below).
His objective is large-scale collection of detailed informa-
tion about as many individuals as possible. This involves
aggregating the anonymous network Ssan with Saux by
recognizing nodes that correspond to the same individuals.
Another attack scenario involves abusive marketing. A
commercial enterprise, especially one specializing in behav-
ioral ad targeting [72], [77], can easily obtain an anonymized
social-network graph from the network operator for adver-
tising purposes. As described in Sections 1 and 2, anonymity
is often misinterpreted as privacy. If an unethical company
were able to de-anonymize the graph using publicly available
data, it could engage in abusive marketing aimed at speciﬁc
individuals. Phishing and spamming also gain from social-
network de-anonymization. Using detailed information about
the victim gleaned from his or her de-anonymized social-
network proﬁle, a phisher or a spammer will be able to craft
a highly individualized, believable message (cf. [37]).
Yet another category of attacks involves targeted de-
anonymization of speciﬁc individuals by stalkers, investi-
gators, nosy colleagues, employers, or neighbors. In this
scenario, the attacker has detailed contextual information
about a single individual, which may include some of her
attributes, a few of her social relationships, membership
in other networks, and so on. The objective is to use
this information to recognize the victim’s node in the
anonymized network and to learn sensitive information about
her, including all of her social relationships in that network.
Modeling the attacker. We assume that in addition to the
177
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:17:29 UTC from IEEE Xplore.  Restrictions apply. 
anonymized, sanitized target network Ssan, the attacker also
has access to a different network Saux whose membership
partially overlaps with S. The assumption that the attacker
possesses such an auxiliary network is very realistic. First,
it may be possible to extract Saux directly from S: for
example, parts of some online networks can be automatically
crawled, or a malicious third-party application can provide
information about
the subgraph of users who installed
it. Second, the attacker may collude with an operator of
a different network whose membership overlaps with S.
Third, the attacker may take advantage of several ongoing
aggregation projects (see Section 2). The intent of these
projects is benign, but
they facilitate the creation of a
global auxiliary network combining bits and pieces of public
information about individuals and their relationships from
multiple sources. Fourth, government-level aggregators, such
as intelligence and law enforcement agencies, can collect
data via surveillance and court-authorized searches. Depend-
ing on the type of the attacker, the nodes of his auxiliary
network may be a subset, a superset, or overlap with those
of the target network.
We emphasize that even with access to a substantial
auxiliary network Saux, de-anonymizing the target network
Ssan is a highly non-trivial task. First, the overlap between
the two networks may not be large. For the entities who
are members of both Saux and S, some social relationships
may be preserved, i.e., if two nodes are connected in Saux,
the corresponding nodes in S are also connected with a
non-negligible probability, but many of the relationships
in each network are unique to that network. Even if the
same entity belongs to both networks, it is not immediately
clear how to recognize that a certain anonymous node from
Ssan corresponds to the same entity as a given node from
Saux. Therefore, easy availability of auxiliary information
does not directly imply that anonymized social networks are
vulnerable to privacy breaches.
Our formal model of the attacker includes both aggre-
gate auxiliary information (large-scale information from
other data sources and social networks whose membership
overlaps with the target network) and individual auxiliary
information (identiﬁable details about a small number of in-
dividuals from the target network and possibly relationships
between them). In the model, we consider edge relationship
to be a binary attribute in Y and all edge attributes Y ∈ Y
to be deﬁned over V 2 instead of E. If (u, v) /∈ E, then
Y [u, v] =⊥ ∀Y ∈ Y.
Aggregate auxiliary information. It is essential that the
attacker’s auxiliary information may include relationships
between entities. Therefore, we model Saux as a graph
Gaux = {Vaux, Eaux} and a set of probability distribu-
tions AuxX and AuxY , one for each attribute of every node
in Vaux and each attribute of every edge in Eaux. These
distributions represent the adversary’s (imperfect) knowl-
edge of the corresponding attribute value. For example,
the adversary may be 80% certain that an edge between
two nodes is a “friendship” and 20% that it is a mere
“contact.” Since we treat edges themselves as attributes, this
also captures the attacker’s uncertain knowledge about the
existence of individual edges. This model works well in
practice, although it does not capture some types of auxiliary
information, such as “node v1 is connected to either node
v2, or node v3.”
For an attribute X of a node v (respectively, attribute Y
of an edge e), we represent by Aux[X, v] (resp., Aux[Y, e])
the attacker’s prior probability distribution (i.e., distribution
given by his auxiliary information) of the attribute’s value.
The set AuxX (resp., AuxY ) can be thought of as a union
of Aux[X, v] (resp., Aux[Y, e]) over all attributes and nodes
(resp., edges).
Aggregate auxiliary information is used in the the “prop-
agation” stage of our de-anonymization algorithm (Sec-
tion 5).
Individual auxiliary information (information about
seeds). We also assume that the attacker possesses detailed
information about a very small2 number of members of the
target network S. We assume that the attacker can determine
if these members are also present in his auxiliary network
Saux (e.g., by matching usernames and other contextual
information). The privacy question is whether this infor-
mation about a handful of members of S can be used, in
combination with Saux, to learn sensitive information about
other members of S.
It is not difﬁcult to collect such data about a small number
of nodes. If the attacker is already a user of S, he knows all
details about his own node and its neighbors [39], [68]. Some
networks permit manual access to proﬁles even if large-
scale crawling is restricted (e.g., Facebook allows viewing
of information about “friends” of any member by default.)
Some users may make their details public even in networks
that keep them private by default. The attacker may even
pay a handful of users for information about themselves
and their friends [43], or learn it from compromised com-
puters or stolen mobile phones. For example, the stored
log of phone calls provides auxiliary information for de-
anonymizing the phone-call graph. With an active attack
(e.g., [7]), the attacker may create fake nodes and edges
in S with features that will be easy to recognize in the
anonymized version of S, such as a clique or an almost-
clique. Since large-scale active attacks are unlikely to be
feasible (see Section 3), we restrict their role to collecting
individual auxiliary information as a precursor to the main,