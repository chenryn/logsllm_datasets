2) Formal OR Scheme: To model OR, [8] deﬁnes an Onion
Routing Scheme as the set of three algorithms:
• Key generation algorithm G: (P K, SK) ← G(1λ, p, P )
with public key P K, secret key SK, security parameter
λ, public parameter p and router name P
(O1, ..., On+1) ←
• Sending algorithm FormOnion:
• Forwarding
FormOnion(m, (P1, . . . , Pn+1), (P K1, . . . , P Kn+1))
with Oi being the onion layer to process by router Pi, m
the message, and P Ki the public key belonging to router
Pi
(O(cid:48), P (cid:48)) ←
ProcOnion(SK, O, P ) with O(cid:48) the processed onion that
is forwarded to P (cid:48) and P the router processing O with
secret key SK. O(cid:48) and P (cid:48) attains ⊥ in case of error or
if P is the recipient.
algorithm ProcOnion:
4Although designed for the integrated system model, it applies to the service
model as well (except for renaming recipient to exit node) if no protection
outside of the OR protocol exists. There the ideal functionality however only
considers the anonymization network and additional private information might
leak when the packet is sent from the exit node to the receiver.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:04:48 UTC from IEEE Xplore.  Restrictions apply. 
170
3) Properties:
[8] deﬁnes three security properties for
OR schemes and proves that
those imply realizing their
ideal OR functionality, i.e. being private and secure. Later
works [11]–[13] split one of the properties in two. The resulting
four properties are Onion-Correctness, Onion-Integrity, Onion-
Security and Wrap-Resistance:
Onion-Correctness requires that all messages use the in-
tended path and reach the intended receiver in absence of an
adversary. Onion-Integrity limits the number of honest relays
that any onion (even one created by the adversary) can traverse.
Onion-Security states that an adversary observing an onion
departing from an honest sender and being routed to an honest
relay, cannot distinguish whether this onion contains adversarial
chosen inputs or a random message for the honest relay. The
adversary is even allowed to observe the processing of other
onions at the honest relay via an oracle. Wrap-Resistance
informally means that an adversary cannot create an onion
that after processing at a relay equals an onion she previously
observed as an output at another relay, even if she has full
control over the inputs.
F. Analysis Framework
We use the framework of Kuhn et al. [23] that uniﬁes the
privacy goals of existing theoretical analysis frameworks like
AnoA [2] and others [7], [20], [22]. It introduces a well-
analyzed hierarchy of privacy goals and thus allows our analysis
results for OR to be easily comparable.
1) Privacy Goals: The idea of the analysis follows game-
based security-proofs. It challenges an adversary to distinguish
two simulations of the protocol that differ only in protected
parts of the communications (e.g. who the sender of a certain
message was). Each communication in this context contains a
sender, receiver, message and auxiliary information, like, for our
purpose, the path included in the onion. The communications
input for the two simulations are called scenarios. They are
freely chosen by the adversary to reﬂect the worst case. Privacy
notions specify formally in which elements the scenarios are
allowed to differ, or, in other words, which information has to
be protected by the protocol.
Four privacy notions are of speciﬁc interest when analyzing
OR. The ﬁrst is a strong form of conﬁdentiality: The adversary
is unable to decide which of two self-chosen messages was
sent in the simulation. As thus the message is unobservable,
this notion is called Message Unobservability (M O).
The second corresponds to our example above, and is a
form of sender anonymity: Informally speaking, the adversary
is unable to decide, which of the messages that she provided
is sent by which of the senders that she chose. As thus she
cannot link the sender to its message, this notion is called
Sender-Message Unlinkability (SM L).
The third, conversely, is a form of receiver anonymity: The
adversary is unable to decide, which of the messages that she
provided is received by which of the receivers that she chose.
As thus she cannot link the receiver to its message, this notion
is called Receiver-Message Unlinkability (RM L).
The fourth is a form of relationship anonymity: The adversary
is unable to decide which pairs of two self-chosen senders and
two self-chosen receivers communicate with each other. As
thus she cannot link the sender to the receiver, this notion is
called Sender-Receiver Unlinkability (SRL).5
2) Adversary: All the privacy notions can be analyzed
for different user (sender and receiver) corruption. Therefore,
options for user corruption are deﬁned and added to the
abbreviation of privacy notion X:
X0: no users are corrupted, but some relays or links can be,
Xs: only receivers, relays, and links can be corrupted, but no
senders,
Xe: senders, receivers, relays, and links can be corrupted (some
limitations apply to prevent the adversary to trivially win
the game)
The framework introduces adversary classes as part of the
game, known to the adversary. They specify modiﬁcations of
the input from, as well as the output to the adversary. Their
purpose is to ﬁne-tune the adversary capabilities e.g. to make
sure that Assumption 2 is met in the scenarios the adversary
is challenged to distinguish.
3) Relation of Goals: Analyzing OR we are interested in
determining the strongest notion that it achieves. The analysis
in the framework then allows statements even for notions that
are not directly analyzed, as it proves a hierarchy: By showing
that a certain notion is achieved, all implied (weaker) notions
are shown to be achieved as well.
Given the claims in [8], [11], [13], we are speciﬁcally
interested in the above mentioned notions of sender- as well
as receiver-message unlinkability (SM L and RM L), which
each implies sender-receiver unlinkability (SRL), and the
independent message unobservability (M O) (See Appendix A
for detailed deﬁnitions. For the analyses of other notions we
refer the interested reader to the extended version of this paper
at [24]).
III. ANALYZING THE IDEAL OR FUNCTIONALITY
There indeed is confusion about which privacy the ideal
functionality F of [8] actually guarantees. The work itself
states only that “its not hard to see that Z [the environment, a
construct of the UC Framework that gets all observations of the
adversary] learns nothing else than pieces of paths of onions
formed by honest senders (i.e., does not learn a sub-path’s
position or relations among different sub-paths). Moreover, if
the sender and the receiver are both honest, the adversary does
not learn the message.”
[1], [3], [27], [28], [30] state that this translates to the degree
of anonymity Tor provides, although [15], [18] argue that it
is not applicable for Tor. [4] states that it “hide(s) the source
and destination over a network,” [26] even understood it as “a
concrete ZK proof of senders’ knowledge of their messages”
and [6] as “provable reduction from unlinkability to trafﬁc
analysis.” [19] states that the privacy is “that an adversary
cannot correctly guess relations between incoming messages
5This notion is called (SR)L in [23].
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:04:48 UTC from IEEE Xplore.  Restrictions apply. 
171
and outgoing messages at onion routers, and [...] that each
onion router cannot know the whole route path of any onion.”
While [18] and [17] realize that the anonymity is not analyzed
and suspect it to be close to the one of [25], which claims to
have sender and receiver anonymity against a global passive
adversary [17].
We hence set out to analyze the actual privacy guarantees
of the ideal functionality.
A. Ideal Functionality F
Recall the basic idea of OR: an adversary can only track
the communication from the sender until the ﬁrst honest relay.
After this she can no longer link the onion to the sender (or
the route before the honest relay). Further, any onion layer
does hide the included message and remaining path, as they
are encrypted.
The ideal functionality for OR of [8] therefore uses tem-
porary random IDs in place of onion packets. All network
information necessary to create onions (sender, receiver, path,
message, hopcount, a randomly chosen session ID) are stored
within the ideal functionality, inaccessible to the adversary.
Sending the onion along a path of relays is represented
the temporary IDs of the
by informing all relays about
corresponding onions they receive. The temporary ID is
replaced with a new randomly drawn ID at every honest node.
The adversary in this model learns the temporary IDs on
links and at the corrupted relays, and if the receiver is corrupted
also the corresponding plaintext message. She speciﬁcally does
not learn which departing ID at an honest relay corresponds
to which received ID. The adversary however is allowed to
decide when an ID is delivered to the next relay (and thus
whether it is delivered at all), as she is assumed to control all
links.
Nitpicking, we add a small detail to the ideal functionality as
suggested by Camenisch and Lysyanskaya: The functionality
represents the case of an honest sender well. However, for a
corrupted sender the adversary trivially learns the complete path
and message as the sender chooses it. As no secure protocol
can remove information an adversary already knows, we add
that the functionality outputs all information about the onion
(sender, receiver, path, etc.) together with the temporary ID, if
its sender is corrupted. The ideal functionality is detailed in
Algorithm 1.
B. Analysis under Restricted Adversary Model
The ideal functionality was designed to capture the cryp-
tographic properties of onion routing. Therefore, it does not
protect against dropping or delaying onions. Hence, for this
analysis we need to exclude attacks that result in dropping or
delaying onions.6 Given this adversary model7 we are able to
prove the privacy goals expected for OR.
1) Instantiation of the Framework: As the path P is an
important input to an onion, we model it speciﬁed in the aux-
iliary information of a communication. The communications,
including the auxiliary information, are picked arbitrarily by
the adversary in the framework. Assumption 2 however requires
at least one honest relay to exist on the path for our analysis.
For this reason, we deﬁne the adversary class C to modify the
path: C replaces the paths as chosen by the adversary with
alternative paths, whenever an honest sender constructing the
onion. The replacements are chosen at random from the set
of paths with valid length that include at least one common
honest relay.
We further restrict the adversary to be incapable of timing-
based trafﬁc analysis. Hence, in the trafﬁc analysis restricted
adversary class C the adversary must not use any timing
information about the onion, i.e. the adversary class shufﬂes all
the outputs from the ideal functionality for communications that
are processed together before handing them to the adversary.
Since the adversary is incapable of trafﬁc analysis, the adversary
class prohibits to delay packets. To further prohibit replay
attacks, which we consider as special kind of trafﬁc analysis
attack, the adversary class drops any duplicated deliver requests
from the adversary.
2) Analysis: Recall, the ideal functionality only outputs the
message to the adversary for a corrupted receiver or sender.
So, the message is protected if sender and receiver are honest
or corrupted users get the same messages in both scenarios
(limitation in Xe) and conﬁdentiality M O is achieved.
Due to the adversary class C, the adversary observes all
outputs corresponding to the inputs of an honest relay in random
order. Combined with random ID replacement, this prevents
the adversary from linking departing onions to their received
counterparts. However, it can still be observed that a user is
actively sending if she has not previously received an onion
(or: that a user is receiving, if upon receiving an onion she
subsequently does not send one). This leads to Theorem 1,
which we prove in our extended version [24].
Theorem 1: F achieves M Oe, SM Ls and RM L0, and those
implied by them, but no other notions of [23] for C.
Note that under this adversary model sender anonymity
(SM L) is achieved even if the receiver is corrupted. From the
hierarchy of [23], we know that this strong version of sender
anonymity also implies relationship anonymity (SRL). Note
further that the receiver anonymity (RM L) is only achieved
if neither the sender nor the receiver is compromised. Thus,
as soon as the sender is corrupted, receiver anonymity is no
longer achieved.
C. First Summary
6However, we include modiﬁcation attacks that do not lead to dropping or
delaying onions, like classical tagging attacks. A protocol realizing the ideal
functionality might either drop modiﬁed onions or keep them in the network,
but prevent the attacker from learning critical information from them (i.e. the
modiﬁed onion’s path and message have no correlation to the original one’s).
7This limitation is not signiﬁcant in practical implementations as they need
to employ additional protection against privacy attacks based on dropping and
delaying onions.
We have seen that the ideal functionality indeed provides the
privacy expected from OR. Showing that a system realizes the
ideal functionality proves these privacy notions for an adversary
that cannot do timing-based trafﬁc analysis. Even if in practice
stronger adversary models are assumed, proving the realization
of the ideal functionality is a useful way to reduce the problem
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:04:48 UTC from IEEE Xplore.  Restrictions apply. 
172
Algorithm 1: Ideal Functionality F
Data structure:
Bad: Set of Corrupted Nodes
L: List of Onions processed by adversarial nodes
Bi: List of Onions held by node Pi
// Notation:
// S: Adversary (resp. Simulator)
// Z: Environment
// P = (Po1 , . . . , Pon ): Onion path
// O = (sid, Ps, Pr, m, n, P, i): Onion = (session ID,
sender, receiver, message, path length, path,
traveled distance)
// N : Maximal onion path length
On message Process_New_Onion(Pr, m, n, P) from Ps
instructed by Z if honest or S if corrupted)
// Ps creates and sends a new onion (either
if |P| > N ;
then
// selected path too long
else
Reject;
sid ←R session ID ;
O ← (sid, Ps, Pr, m, n, P, 0) ;
Output_Corrupt_Sender(Ps, sid, Pr, m, n, P, start);
Process_Next_Step(O);
// pick random session ID
// create new onion
Procedure Output_Corrupt_Sender(Ps, sid, Pr, m, n, P, temp)
// Give all information about onion to adversary
if Ps ∈ Bad then
if sender is corrupt
Send “temp belongs to onion from Ps with sid, Pr, m, n, P” to
S;
Procedure Process_Next_Step(O = (sid, Ps, Pr, m, n, P, i))
just processed O that is now passed
to router Poi+1
// Router Poi
if Poj ∈ Bad for all j > i ;
including receiver are corrupt
then
// All remaining nodes
Send “Onion from Poi with message m for Pr routed through
(Poi+1 , . . . , Pon )” to S;
Output_Corrupt_Sender(Ps, sid, Pr, m, n, P, end);
else
// there exists an honest successor Poj
Poj ← Pok with smallest k such that Pok (cid:54)∈ Bad
temp ←R temporary ID;
Send “Onion temp from Poi routed through
(Poi+1 , . . . , Poj−1 ) to Poj ” to S;
Output_Corrupt_Sender(Ps, sid, Pr, m, n, P, temp);
Add (temp, O, j) to L; // see Deliver_Message(temp)
to continue this routing
On message Deliver_Message(temp) from S
// Adversary S (controlling all links) delivers
if (temp,
, ) ∈ L then