### 2. Formal Onion Routing (OR) Scheme

To model OR, [8] defines an Onion Routing Scheme as a set of three algorithms:

- **Key Generation Algorithm (G)**: 
  \[
  (P K, SK) \leftarrow G(1^\lambda, p, P)
  \]
  where \(P K\) is the public key, \(SK\) is the secret key, \(\lambda\) is the security parameter, \(p\) is the public parameter, and \(P\) is the router name.

- **Sending Algorithm (FormOnion)**:
  \[
  (O_1, ..., O_{n+1}) \leftarrow \text{FormOnion}(m, (P_1, ..., P_{n+1}), (P K_1, ..., P K_{n+1}))
  \]
  Here, \(O_i\) represents the onion layer to be processed by router \(P_i\), \(m\) is the message, and \(P K_i\) is the public key belonging to router \(P_i\).

- **Forwarding Algorithm (ProcOnion)**:
  \[
  (O', P') \leftarrow \text{ProcOnion}(SK, O, P)
  \]
  where \(O'\) is the processed onion that is forwarded to \(P'\), and \(P\) is the router processing \(O\) with secret key \(SK\). If there is an error or if \(P\) is the recipient, \(O'\) and \(P'\) are set to \(\bot\).

Although designed for the integrated system model, this scheme also applies to the service model (with the recipient renamed to the exit node) if no additional protection outside the OR protocol exists. In the service model, the ideal functionality only considers the anonymization network, and additional private information might leak when the packet is sent from the exit node to the receiver.

### 3. Properties

[8] defines three security properties for OR schemes and proves that these imply realizing their ideal OR functionality, i.e., being private and secure. Later works [11]–[13] split one of the properties into two, resulting in four properties: Onion-Correctness, Onion-Integrity, Onion-Security, and Wrap-Resistance.

- **Onion-Correctness**: Ensures that all messages use the intended path and reach the intended receiver in the absence of an adversary.
- **Onion-Integrity**: Limits the number of honest relays that any onion (even one created by the adversary) can traverse.
- **Onion-Security**: States that an adversary observing an onion departing from an honest sender and being routed to an honest relay cannot distinguish whether the onion contains adversarial chosen inputs or a random message for the honest relay. The adversary is even allowed to observe the processing of other onions at the honest relay via an oracle.
- **Wrap-Resistance**: Informally means that an adversary cannot create an onion that, after processing at a relay, equals an onion she previously observed as an output at another relay, even if she has full control over the inputs.

### F. Analysis Framework

We use the framework of Kuhn et al. [23], which unifies the privacy goals of existing theoretical analysis frameworks like AnoA [2] and others [7], [20], [22]. This framework introduces a well-analyzed hierarchy of privacy goals, allowing our analysis results for OR to be easily comparable.

#### 1. Privacy Goals

The analysis follows game-based security proofs, challenging an adversary to distinguish two simulations of the protocol that differ only in protected parts of the communications (e.g., who the sender of a certain message was). Each communication in this context includes a sender, receiver, message, and auxiliary information, such as the path included in the onion. The communications input for the two simulations are called scenarios, freely chosen by the adversary to reflect the worst case. Privacy notions specify formally in which elements the scenarios are allowed to differ, or, in other words, which information must be protected by the protocol.

Four privacy notions are of specific interest when analyzing OR:

- **Message Unobservability (MO)**: The adversary cannot decide which of two self-chosen messages was sent in the simulation.
- **Sender-Message Unlinkability (SML)**: The adversary cannot link the sender to its message.
- **Receiver-Message Unlinkability (RML)**: The adversary cannot link the receiver to its message.
- **Sender-Receiver Unlinkability (SRL)**: The adversary cannot link the sender to the receiver.

#### 2. Adversary

All privacy notions can be analyzed for different user (sender and receiver) corruption. Options for user corruption are defined and added to the abbreviation of privacy notion \(X\):

- **\(X_0\)**: No users are corrupted, but some relays or links can be.
- **\(X_s\)**: Only receivers, relays, and links can be corrupted, but no senders.
- **\(X_e\)**: Senders, receivers, relays, and links can be corrupted (some limitations apply to prevent the adversary from trivially winning the game).

The framework introduces adversary classes as part of the game, known to the adversary. These classes specify modifications of the input from, as well as the output to, the adversary, fine-tuning the adversary's capabilities.

#### 3. Relation of Goals

Analyzing OR, we are interested in determining the strongest notion that it achieves. The analysis in the framework allows statements even for notions that are not directly analyzed, as it proves a hierarchy: by showing that a certain notion is achieved, all implied (weaker) notions are shown to be achieved as well.

Given the claims in [8], [11], [13], we are specifically interested in the notions of sender- and receiver-message unlinkability (SML and RML), which each implies sender-receiver unlinkability (SRL), and the independent message unobservability (MO).

### III. Analyzing the Ideal OR Functionality

There is confusion about the privacy guarantees of the ideal functionality \(F\) of [8]. The work itself states that "it is not hard to see that Z [the environment, a construct of the UC Framework that gets all observations of the adversary] learns nothing else than pieces of paths of onions formed by honest senders (i.e., does not learn a sub-path’s position or relations among different sub-paths). Moreover, if the sender and the receiver are both honest, the adversary does not learn the message."

Various works interpret this differently. For example, [1], [3], [27], [28], [30] state that this translates to the degree of anonymity Tor provides, while [15], [18] argue that it is not applicable for Tor. [4] states that it "hides the source and destination over a network," [26] interprets it as "a concrete ZK proof of senders' knowledge of their messages," and [6] as "provable reduction from unlinkability to traffic analysis." [19] states that the privacy is "that an adversary cannot correctly guess relations between incoming messages and outgoing messages at onion routers, and [...] that each onion router cannot know the whole route path of any onion."

While [18] and [17] realize that the anonymity is not analyzed and suspect it to be close to the one of [25], which claims to have sender and receiver anonymity against a global passive adversary [17].

We hence set out to analyze the actual privacy guarantees of the ideal functionality.

#### A. Ideal Functionality \(F\)

Recall the basic idea of OR: an adversary can only track the communication from the sender until the first honest relay. After this, she can no longer link the onion to the sender (or the route before the honest relay). Further, any onion layer hides the included message and remaining path, as they are encrypted.

The ideal functionality for OR of [8] uses temporary random IDs in place of onion packets. All network information necessary to create onions (sender, receiver, path, message, hopcount, a randomly chosen session ID) are stored within the ideal functionality, inaccessible to the adversary. Sending the onion along a path of relays is represented by informing all relays about the corresponding onions they receive. The temporary ID is replaced with a new randomly drawn ID at every honest node.

The adversary in this model learns the temporary IDs on links and at the corrupted relays, and if the receiver is corrupted, also the corresponding plaintext message. She specifically does not learn which departing ID at an honest relay corresponds to which received ID. The adversary, however, is allowed to decide when an ID is delivered to the next relay (and thus whether it is delivered at all), as she is assumed to control all links.

Nitpicking, we add a small detail to the ideal functionality as suggested by Camenisch and Lysyanskaya: The functionality represents the case of an honest sender well. However, for a corrupted sender, the adversary trivially learns the complete path and message as the sender chooses it. As no secure protocol can remove information an adversary already knows, we add that the functionality outputs all information about the onion (sender, receiver, path, etc.) together with the temporary ID if its sender is corrupted. The ideal functionality is detailed in Algorithm 1.

#### B. Analysis under Restricted Adversary Model

The ideal functionality was designed to capture the cryptographic properties of onion routing. Therefore, it does not protect against dropping or delaying onions. Hence, for this analysis, we need to exclude attacks that result in dropping or delaying onions. Given this adversary model, we are able to prove the privacy goals expected for OR.

1. **Instantiation of the Framework**:
   - As the path \(P\) is an important input to an onion, we model it specified in the auxiliary information of a communication.
   - The communications, including the auxiliary information, are picked arbitrarily by the adversary in the framework.
   - Assumption 2 requires at least one honest relay to exist on the path for our analysis.
   - We define the adversary class \(C\) to modify the path: \(C\) replaces the paths as chosen by the adversary with alternative paths, whenever an honest sender constructs the onion. The replacements are chosen at random from the set of paths with valid length that include at least one common honest relay.
   - We further restrict the adversary to be incapable of timing-based traffic analysis. Hence, in the traffic analysis restricted adversary class \(C\), the adversary must not use any timing information about the onion, i.e., the adversary class shuffles all the outputs from the ideal functionality for communications that are processed together before handing them to the adversary.
   - Since the adversary is incapable of traffic analysis, the adversary class prohibits delaying packets. To further prohibit replay attacks, which we consider a special kind of traffic analysis attack, the adversary class drops any duplicated deliver requests from the adversary.

2. **Analysis**:
   - Recall, the ideal functionality only outputs the message to the adversary for a corrupted receiver or sender. So, the message is protected if sender and receiver are honest or corrupted users get the same messages in both scenarios (limitation in \(X_e\)) and confidentiality \(MO\) is achieved.
   - Due to the adversary class \(C\), the adversary observes all outputs corresponding to the inputs of an honest relay in random order. Combined with random ID replacement, this prevents the adversary from linking departing onions to their received counterparts. However, it can still be observed that a user is actively sending if she has not previously received an onion (or: that a user is receiving, if upon receiving an onion she subsequently does not send one). This leads to Theorem 1, which we prove in our extended version [24].
   - **Theorem 1**: \(F\) achieves \(MO_e\), \(SML_s\), and \(RML_0\), and those implied by them, but no other notions of [23] for \(C\).
   - Note that under this adversary model, sender anonymity (SML) is achieved even if the receiver is corrupted. From the hierarchy of [23], we know that this strong version of sender anonymity also implies relationship anonymity (SRL). Further, receiver anonymity (RML) is only achieved if neither the sender nor the receiver is compromised. Thus, as soon as the sender is corrupted, receiver anonymity is no longer achieved.

#### C. First Summary

We have seen that the ideal functionality indeed provides the privacy expected from OR. Showing that a system realizes the ideal functionality proves these privacy notions for an adversary that cannot do timing-based traffic analysis. Even if in practice stronger adversary models are assumed, proving the realization of the ideal functionality is a useful way to reduce the problem.