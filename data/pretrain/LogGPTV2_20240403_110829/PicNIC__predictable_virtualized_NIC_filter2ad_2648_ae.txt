throttled to 10 Mbps. Without accounting, B keeps sending pack-
ets for fBD , most of which are dropped at the traffic shaper. This
exhausts the descriptor pool in the engine and impacts the unthrot-
tled flow, fAC . Packet accounting limits the number of outstanding
packets from B in the engine and thus provides isolation for A, as
shown in Fig. 8b.
359
SIGCOMM ’19, August 19–23, 2019, Beijing, China
P. Kumar et al.
5ms
ϵ = 128μs
ϵ = 512μs
ϵ = 2048μs
0
10
20
30
40
Time (ms)
50
60
70
Figure 9: PCCB converges within 5ms.
UDP flow starts at t = 0
AI HAI
MD
PCCP throttles UDP immediately
FR
)
s
μ
(
T
T
R
)
s
p
p
M
(
t
u
p
d
o
o
G
10000
1000
100
Mean
Mean
99th perc. : 100B pkts
99th perc. : 256B pkts
10
1
2.0
1.5
1.0
0.5
0.0
CWFQ
+ PCCB
1000
100B pkts
256B pkts
10
5
500
200
vNIC ingress delay threshold (μs)
100
50
20
Mean: 9.05μs Median: 8.54μs
99th perc.: 13.25μs
Figure 11: Goodput and RTT vs target engine delay.
)
s
p
b
G
(
t
i
m
i
l
e
t
a
R
)
s
p
p
M
(
t
i
m
i
l
12
10
8
6
4
2
0
3
2
1
e
t
a
R
)
s
μ
(
y
a
l
e
d
s
s
e
r
g
n
I
0
20
15
10
5
0
Delay threshold = 10μs
0.0
0.5
1.0
1.5
Time (s)
2.0
2.5
3.0
Figure 10: Predictable low latency with PicNIC.
OOO Completions and NAPI-TX. Next, we see how HoL blocking
can arise between flows. Consider two flows: i) fAC —UDP throttled
at 10 Mbps, and ii) fAD —unthrottled TCP. With in-order early
completions (EC), descriptors are freed as soon as they leave the
guest. Thus, both flows are able to transmit packets into the egress
stack at the same rate. This overwhelms the TW as it has to drop
packets beyond the horizon for throttled flows. PicNIC implements
out-of-order completions (OOO) plus NAPI-TX with virtio for
the guest. As shown in Table 4, this improves the throughput for
the unthrottled flow while keeping the TW occupancy low and
eliminates egress drops in the engine.
Responsiveness of PCC. For each receiver VM, PCCB computes rate
limits at epochs of fixed duration (ϵ). We quantify the responsive-
ness of PCCB with varying ϵ using the setup from Fig. 7a and a BPS
envelope of 4 to 12 Gbps per VM. At t = 0, only fAC is active and
gets the entire 12 Gbps. At t = 20ms, fBC starts and now, PCCB
needs to limit each flow to 6 Gbps to meet MAX_BPS for C. Fig. 9 shows
that using a small ϵ = 128μs leads to oscillations for fAC ’s limit,
while a large value of 2ms leads to slow convergence. An epoch of
a few RTTs leads to fast convergence—e.g., with ϵ = 512μs, the rate
converges within 5ms. We find similar convergence for PCCP but
leave a formal analysis to future work.
Predictable latency. We evaluate the efficacy of PCCP in ensuring
low vNIC delay using the same setup. fBD is a low-BPS latency-
sensitive flow. At t = 0, fAC starts a high-PPS UDP traffic with 100-
byte packets to create extreme overload at h3. We use a conservative
value of ϵ = 5ms, and set the vNIC ingress delay threshold to 10μs.
Even in this extreme case, PicNIC is able to deliver predictable
latency for fBD as shown in Fig. 10. First, even with a large ϵ, as
soon as PicNIC detects an isolation issue, it turns on PCCP with a
rate estimate based on goodput. This brings the vNIC delay close
to the threshold within a single epoch while the rate converges.
Second, PCCP remains stable at a point that ensures that the vNIC
360
ingress delay is close to the target threshold, with a mean delay of
9.05μs, and 99t h perc. delay of 13.25μs.
High throughput and low delay. Next, we show that PicNIC delivers
consistent low delay and high goodput for resource-intensive traffic
while navigating strict isolation and efficiency. We extend the setup
from Fig. 7a with two more VMs as before (E on h2 and F on h3).
fAC and fBD are high-PPS UDP flows, while fE F probes latency
(Poisson process with rate λ = 1 kpps).
With just BPS envelope and 100-byte UDP packets, the UDP
flows achieve a total goodput of 0.93 Mpps, while the mean fE F
RTT is 7.5ms as shown in Fig. 11. PCCP is able to reduce latency by
over two orders of magnitude while delivering higher goodput for
UDP flows. Fig. 11 (top) shows that as we lower the delay threshold,
the measured RTT correspondingly reduces as expected. The bot-
tom plot shows that the goodput improves by as much as 100% as
the threshold is lowered to a point, perhaps counterintuitively. This
is because PicNIC avoids drops at ingress, thus achieving higher
efficiency which manifests as increased goodput. As the threshold
becomes stricter, the rate limit for UDP flows is further reduced
and ultimately leads to a decrease in goodput. However, even with
the strictest threshold of 5μs, PicNIC ensures a goodput (1 Mpps)
comparable to the case with just BPS envelope, but with a remark-
ably lower mean fE F RTT of 20.3μs. We see similar results with
256-byte UDP packets.
6.2 Overheads, Response Time and Scalability
PicNIC’s major overhead arises from implementing PCC in the
datapath. To quantify the tradeoff between responsiveness and
overheads, we measure the throughput of a TCP flow between two
VMs placed on separate hosts while changing the PCC feedback
interval. At the egress, we enforce a dummy high rate limit so
that the flow is never throttled, but it will be affected as PicNIC
uses CPU cycles. As shown in Fig. 12, if the feedback is received
too frequently, e.g., every 10μs, the goodput degrades by ∼22%.
However, as we see good convergence with ϵ ≈ 500μs in §6.1.2,
we expect to run PCC at a similar granularity. As we increase the
interval, the overhead decreases and becomes negligible at 1 ms.
We find that these overheads are good enough for rapid response
to isolation breakages while remaining work-conserving in the
common case. We note that while PCC is reactive, CWFQs and
sender-side admission control are proactive and provide isolation
at even faster timescales.
PicNIC: Predictable Virtualized NIC
SIGCOMM ’19, August 19–23, 2019, Beijing, China
100
d
e
z
i
l
a
m
r
o
N
)
%
(
t
u
p
h
g
u
o
r
h
T
75
50
25
0
1.00
0.75
0.50
0.25
0.00
F
D
C
s
0 m s
1 m s
1
Feedback Interval
1
s
μ
0
0
μ
0
1
CWFQ
CWFQ + PCCB
Light workload
PicNIC
Perfect isolation
No Isolation
0
200
400
Response latency (μs)
600
800
)
s
μ
1000
(
y
c
n
e
t
a
l
.
c
r
e
p
h
t
9
9
750
500
250
No Isolation
CWFQ + PCCB
CWFQ
Better
Light workload
PicNIC
Perfect isolation
0
0
200
400
1000
Peak Throughput (×103 RPS)
600
800
1200
1400
Figure 12: Overheads.
Figure 13: Memcached: Latency under low load (200 k RP S).
Figure 14: Memcached: Peak throughput and tail latency.
While the above microbenchmarks use a 20 Gbps NIC, we have
evaluated PicNIC with faster NICs, such as 40 Gbps and higher, and
find that PicNIC scales well with low overhead.
10 Gbps. Even when sharing the network with a high-PPS workload
tenant, PicNIC ensures performance isolation for T1 as if it was
sharing the network with another light-workload tenant.
6.3 Application-level Performance
We quantify application-level benefits using a setup with two ten-
ants (say, T1 and T2) and 48 VMs placed over 9 hosts. T1 runs
memcached, an in-memory key-value store widely used as a low-
latency caching layer for database applications [46]. T1 replicates
Facebook’s ETC workload [6, 54] on 24 VMs using mutilate load
generator [42]. We ensure that memcached is not compute bot-
tlenecked by overprovisioning with 8 memcached servers and 16
clients. T2 runs a UDP workload with 12 client-server pairs colo-
cated with T1. Each UDP client generates 256-byte packets at 2
Mpps, i.e. at ∼4 Gbps per source.
First, we set the baseline with no isolation, and then incremen-
tally enable the different PicNIC components (§5): CWFQs, PCCB
with envelope of 3 to 12 Gbps, and finally the complete PicNIC
system with PCCP. We keep the egress constructs always enabled.
For comparison, we consider two cases: i) Perfect isolation: T1 runs
solely without T2 and with no MAX_BPS limit, and ii) Light workload:
T2 runs a very light workload (100 kpps) to help set T1’s perfor-
mance target with PicNIC when resources are shared even under
heavy workloads.
Latency. With no isolation, the median memcached response la-
tency is 686μs compared to 118μs with perfect isolation, as shown
in Fig. 13. CWFQs improve isolation at ingress, reducing the la-
tency but do not prevent drops of excess traffic and wasted work.
PCCB does little to improve latency because the UDP traffic is not
consuming excessive bandwidth, and bandwidth is not the bottle-
neck for memcached either. Finally, with PCCP, PicNIC ensures
performance isolation for memcached with 99t h perc. latency of
256μs, close to 200μs with perfect isolation (in contrast to 354μs
with CWFQ+PCCB). In fact, the performance with PicNIC in the
presence of high-PPS UDP is similar to the case when T1 is sharing
the network with T2 running a light workload.
Throughput. Next, Fig. 14 shows the maximum achievable through-
put and the corresponding tail latency for memcached. Starting
with no isolation as baseline, as we incrementally enable individual
PicNIC constructs, the performance approaches the perfect isola-
tion case—i.e., when there is no contention for resources. Achieving
the same level of performance is not possible in a shared setup—e.g.,
in a setup with MAX_BPS per VM of 15 Gbps and the NIC line-rate
of 20 Gbps, a VM can get 15 Gbps with perfect isolation; but when