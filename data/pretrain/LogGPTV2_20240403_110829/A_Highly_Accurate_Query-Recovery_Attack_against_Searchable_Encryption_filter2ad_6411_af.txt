not. Moreover, Forsyth and Safavi-Naini in [10] tackle the fre-
quency analysis problem (for substitution cipher attacks) by
using simulated annealing as IKK for SSE. In [9], Dhavare et
al. presented a hill climbing solution which is an optimization
algorithm similar to simulated annealing. Simulated anneal-
ing is a very powerful approach against substitution ciphers
and less against SSE due to the large alphabet size.
We argue that a similar-data attack on SSE is analogous to
ciphertext-only attack on substitution ciphers since the pub-
licly available corpus used for substitution ciphers attack is
analogous to the similar document set used in similar-data at-
tacks. In our case, we present a similar-data attack with known
queries corresponding to a chosen-plaintext attack. One could
say that substitution ciphers cryptanalysis uses n-grams and
not only bigrams. Thus, we present a generalized attack in
Subsection 7.1 which uses co-occurrence of order n (i.e. the
number of documents containing n speciﬁc keywords).
Reﬁned score attack We compare the reﬁned score attack
to the digram method presented in [11]. In this method, there
is a preliminary step of vowel identiﬁcation based on the letter
occurrences. This preliminary step can correspond to the prior
active attack performed to obtain known queries for the re-
ﬁned score attack. Then, the cryptanalyst identiﬁes iteratively
new letters using the digrams of these vowels (equivalent to
co-occurrence matrices). When the cryptanalyst guesses new
letters, she can use them to identify the remaining unknown
letters. In our reﬁned score attack, at the end of each iteration,
we learn few queries and will use these newly known queries
to recover the remaining unknown queries. There is a strong
similarity between the notions of known letter in substitution
cipher cryptanalysis and of known query in SSE attack and
the way they are used to iteratively discover new letters (resp.
queries).
B Estimation of the number of indexed docu-
ments
Both IKK and GCPR attacks use known queries but conclude
that the results are equivalent with or without them. We as-
sume that known queries convey signiﬁcant information and
should be fully used to obtain an effective attack as shown
in Section 5. Another example of this knowledge underuti-
lization is the number of documents indexed nreal which is
considered as known by IKK and CGPR attacks. However, if
the attacker is a passive trafﬁc observer he would not have this
information. IKK and CGPR only considered the honest-but-
curious server. Storing the index and the documents on two
separate servers is a simple way to degrade the information
leakage to that of a passive trafﬁc observer.
This number is mandatory to transform the count matrix
into a frequency matrix. We note Dsim(kw), the documents
from Dsim that contains the keyword kw. We also highlight
|Rq| = |Dreal(kw)| if kw is the underlying keyword of query
q.
ˆnreal =
·
1
k
∑
kw,td∈KnownQ
|Rq|
|Dsim(kw)| · nsim
(8)
|Rq|
Equation (8) shows how ˆnreal the estimation of the number
of indexed is computed. The ﬁrst part of the equation (i.e.
k · ∑kw,td∈KnownQ
1
|Dsim(kw)|) is the average ratio between the
number of encrypted documents containing one keyword and
the number of similar documents containing the exact same
keyword. Then, this ratio (which is a sort of scale factor) is
multiplied by the number of similar documents to obtain ˆnreal.
158    30th USENIX Security Symposium
USENIX Association
Thanks to this estimation, the minimum adversary knowl-
edge needed by IKK and CGPR attacks does not include the
number of indexed documents contrary to what was implic-
itly assumed. If the result length is hidden, the co-occurrence
between the known queries can be used to estimate ˆnreal.
C Improvement strategy: Clustering
The matching score provides a very interesting basis to in-
terpret and analyse the results. By default, we always pick
tdpred = argmaxi Score(tdi,kw) and the difference between
the score of tdpred and the score of the second best prediction
is considered as the certainty of the predictions. However, we
observed that, sometimes, we have several potential candi-
dates instead of one:
• Classical score distribution: [. . . 6, 6.2, 6.3, 9], one clear
candidate
• Atypical score distribution: [. . . 6, 6.2, 6.3, 7.9, 8, 8.2],
one cluster of candidates
We argue that it would be very interesting to return clusters
when the choice is uncertain. To process appropriately these
score distributions, we use hierarchical clustering ( [5, 33])
to identify the best-candidate cluster. With clustering, the
prediction will be a cluster (either with one single candidate
or with several candidates) and the certainty of the prediction
will be the distance between the best-candidate cluster and
the rest of the scores. In the main body of this paper, a certain
prediction was a prediction for which the certainty is high.
In this case, a certain prediction is a single-point cluster for
which the certainty is high.
Hierarchical clustering is an iterative method used to obtain
n− 1 clusters from n clusters. We speciﬁcally use the single-
linkage clustering which considers the minimum distance
between two clusters as the dissimilarity. Usually it is needed
to deﬁne a number of clusters or a "cutting height" to know
when to stop the iterations. To avoid this problem, we deﬁne
a maximum size MaxSize > MaxSize. This complexity reduction is impor-
tant because a clustering is performed over the msim candi-
dates of each trapdoor.
∃i ≤ MaxSize,Smax = {s1, . . . ,si} and
∀ j ≤ MaxSize + 1,si − si−1 ≤ s j − s j−1
(9)
To obtain Smax, we use Figure 10 which takes as input the
score set S and the parameter MaxSize. It outputs the best-
candidate cluster and the distance between this cluster and
the closest cluster (i.e. the certainty of the prediction). To ﬁnd
the best-candidate cluster, the algorithm just needs to ﬁnd the
maximum leap between two consecutive scores among the
(MaxSize + 1) maximum scores from the score set S. From
Equation (9), we know that all the scores which are before
this maximum leap compose Smax.
Require: S ,MaxSize
MaxDist ← 0
MaxInd ← 0
S ← sort(S ,desc)
for all i ∈ 1 . . .MaxSize do
CurrDist = S[i]− S[i + 1]
if MaxDist < CurrDist then
MaxDist ← CurrDist
MaxInd ← i
end if
end for
Smax = S[: MaxInd] {MaxInd ﬁrst elements of S}
return Smax,MaxDist
Figure 10: Best-candidate clustering algorithm
This clustering can be used to improve either the base
attack or the reﬁned attack. To improve the base attack, we
just need to call the clustering algorithm in the prediction
loop: instead of appending the candidate with the highest
score, the algorithm appends the best-candidate cluster to the
prediction list. To improve the reﬁned attack, clustering will
be used to identify the most certain predictions. The algorithm
stops when there are less than RefSpeed single-point clusters
found. We present comparative results in Figure 11. The
USENIX Association
30th USENIX Security Symposium    159
highlight that, by construction, the methods improved with
clustering must perform at least as well as the standard meth-
ods.
Cluster size choice Table 4 presents the size statistics of
the clusters returned by the clustering + reﬁned score attack
algorithm with varying MaxClustSize. The table is separated
into two parts: the upper part presents results when the vo-
cabulary size is 1K and the lower part when the vocabulary
size is 2K. First, we note that choosing MaxSize=1 is strictly
equivalent to using the standard reﬁned score attack. In the
upper part, we read that q0.8 = 1, it means that for at least 80%
of the queries, only one possible keyword is returned. When
MaxClustSize = 10, we also note that q0.99 = 7, i.e. less that
1% of the queries has a best-candidate cluster reaching the
maximum size. These results tend to prove that the clustering
does not improve artiﬁcially the results because the reﬁned
score algorithm returns cluster only for a small minority of
results. Moreover, when MaxClustSize = 1 (i.e. reﬁned score
attack without clustering), the accuracy is slightly decreased
(3%).
In the Figure 11, we use MaxClustSize = 10. In the up-
per part of Table 4, we show that the accuracy is increased
compared to the experiments using 1 or 5 as maximum size.
However, the accuracy is only very slightly (less than 0.1%)
increased when the maximum size is 20 or 50. This small ac-
curacy difference could also be few big clusters (i.e. 20-point
clusters) containing the correct keyword but the attacker has
no way to identify this result as a correct prediction. We can
also wonder how this attacker can exploit such clusters. Thus,
the experimental accuracy might be increased but the practical
accuracy would remain identical. On the other hand, choosing
a maximum cluster size of 10 instead of 20 divides the com-
plexity by two. To sum up, by choosing MaxClustSize = 10,
we sacriﬁce an uncertain 0.1% accuracy gain for an algorithm
execution time divided by two.
In our experiments, these clusters seem to contain words
which are semantically close. We observe clusters containing
only ﬁgures or only days of the week. However, we cannot
draw any strong semantic conclusion from these clusters since
they are built from very small corpus. Clusters with a real
semantic signiﬁcation are used in natural language process-
ing especially for translation but are obtained from corpus
composed of billions of documents. This claim seems coher-
ent since the word-word co-occurrence matrix is the basis of
word embeddings as GloVe [26].
Figure 11: Comparison of the accuracy of the score
attacks with
Parameters:
|Dsim| = 12K,|Dreal| = 18K,msim = 1.2K,mreal = 1K,|Q | =
150,|KnownQ | = 10,RefSpeed = 10,MaxClustSize = 10
and without
clustering.
Table 4: Cluster size statistics (and their corresponding aver-
age accuracy) over 50 simulations of the reﬁned score attack
using the clustering improvement. |Dsim| = 12K,|Dreal| =
18K,mreal = msim = 1K,|Q | = 150,|KnownQ | = 15
Size stats.
MaxSize=1
MaxSize=5
MaxSize=10
MaxSize=20
MaxSize=50
µ
1
1.26
1.36
1.41
1.45
q0.8
1
1
1
1
1
q0.85
1
1
2
2
2
q0.95
1
3
3
4
4
q0.99
1
5
7
8
9
Acc.
0.873
0.902
0.906
0.907
0.907
Results below were obtained with mreal = msim = 2K
MaxSize=5
MaxSize=10
MaxSize=20
1.35
1.53
1.63
1
2
2
2
2
2
3
5
5
5
8
11
0.658
0.667
0.670
accuracy is strongly increased for the base score attack (about
15 percentage points). We still observe an improvement for
the reﬁned score attack (about 5 percentage points).
In the particular case of clustering, a correct prediction
is a prediction for which the cluster returned contains the
correct keyword. Thus, comparing the accuracies with and
without clustering is imperfect since methods with clustering
has a slightly different deﬁnition of accuracy. Moreover, we
160    30th USENIX Security Symposium
USENIX Association